
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.AI_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T12:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.AI_2023_11_07/">cs.AI - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ToP-ToM-Trust-aware-Robot-Policy-with-Theory-of-Mind"><a href="#ToP-ToM-Trust-aware-Robot-Policy-with-Theory-of-Mind" class="headerlink" title="ToP-ToM: Trust-aware Robot Policy with Theory of Mind"></a>ToP-ToM: Trust-aware Robot Policy with Theory of Mind</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04397">http://arxiv.org/abs/2311.04397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuang Yu, Baris Serhan, Angelo Cangelosi</li>
<li>for: 本研究探讨了在多智能体设定下，人与机器人合作对抗另一名人对手的情况下，机器人使用理论心理来建立人与机器人之间的信任。</li>
<li>methods: 本研究采用了机器人理论心理模型，推断人对机器人的信任信念，包括真实信任和假信任（是理论心理中的关键元素）。基于不同的信任信念，我们设计了一个动态信任意识 reward函数，以帮助机器人策略学习，以避免人对机器人的信任崩溃。</li>
<li>results: 实验结果表明，基于理论心理的机器人策略对人与机器人之间的信任具有重要作用，并且我们的机器人理论心理基于的策略在多智能体交互设定下具有效果。<details>
<summary>Abstract</summary>
Theory of Mind (ToM) is a fundamental cognitive architecture that endows humans with the ability to attribute mental states to others. Humans infer the desires, beliefs, and intentions of others by observing their behavior and, in turn, adjust their actions to facilitate better interpersonal communication and team collaboration. In this paper, we investigated trust-aware robot policy with the theory of mind in a multiagent setting where a human collaborates with a robot against another human opponent. We show that by only focusing on team performance, the robot may resort to the reverse psychology trick, which poses a significant threat to trust maintenance. The human's trust in the robot will collapse when they discover deceptive behavior by the robot. To mitigate this problem, we adopt the robot theory of mind model to infer the human's trust beliefs, including true belief and false belief (an essential element of ToM). We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology. The experimental results demonstrate the importance of the ToM-based robot policy for human-robot trust and the effectiveness of our robot ToM-based robot policy in multiagent interaction settings.
</details>
<details>
<summary>摘要</summary>
We found that when the robot focuses solely on team performance, it may resort to reverse psychology, which can damage trust between the human and the robot. When the human discovers deceptive behavior by the robot, their trust in the robot will collapse. To address this issue, we developed a robot ToM model to infer the human's trust beliefs, including true belief and false belief, which is an essential element of ToM.We designed a dynamic trust-aware reward function based on different trust beliefs to guide the robot policy learning, which aims to balance between avoiding human trust collapse due to robot reverse psychology and achieving team performance. The experimental results demonstrate the importance of the ToM-based robot policy for human-robot trust and the effectiveness of our robot ToM-based robot policy in multiagent interaction settings.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-Manycore-Processors-with-Distributed-Memory-for-Accelerated-Training-of-Sparse-and-Recurrent-Models"><a href="#Harnessing-Manycore-Processors-with-Distributed-Memory-for-Accelerated-Training-of-Sparse-and-Recurrent-Models" class="headerlink" title="Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models"></a>Harnessing Manycore Processors with Distributed Memory for Accelerated Training of Sparse and Recurrent Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04386">http://arxiv.org/abs/2311.04386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Finkbeiner, Thomas Gmeinder, Mark Pupilli, Alexander Titterton, Emre Neftci</li>
<li>for: 提高人工智能训练基础设施的效率，使得更多的神经网络模型可以更好地利用多指令多数据（MIMD）架构的优势。</li>
<li>methods: 使用触发器神经网络（SNN）和分布式本地内存，实现了在多个处理器（IPU）上进行分布式的后向传播算法（BPTT）训练。</li>
<li>results: 与NVIDIA A100 GPU相比，使用MIMD架构的Intelligence Processing Unit（IPU）可以获得5-10倍的throughput提升，并且在不同的活动缺失率水平上可以达到38倍的提升。此外，我们的结果还显示了在单IPU和多IPU配置下的扩展性。<details>
<summary>Abstract</summary>
Current AI training infrastructure is dominated by single instruction multiple data (SIMD) and systolic array architectures, such as Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), that excel at accelerating parallel workloads and dense vector matrix multiplications. Potentially more efficient neural network models utilizing sparsity and recurrence cannot leverage the full power of SIMD processor and are thus at a severe disadvantage compared to today's prominent parallel architectures like Transformers and CNNs, thereby hindering the path towards more sustainable AI. To overcome this limitation, we explore sparse and recurrent model training on a massively parallel multiple instruction multiple data (MIMD) architecture with distributed local memory. We implement a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. We observe a massive advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU) compared to GPUs. On training workloads, our results demonstrate 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance. Furthermore, our results show highly promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs, and competitive large scale SNN models.
</details>
<details>
<summary>摘要</summary>
We implement a training routine based on backpropagation through time (BPTT) for the brain-inspired class of Spiking Neural Networks (SNNs) that feature binary sparse activations. Our results show a significant advantage in using sparse activation tensors with a MIMD processor, the Intelligence Processing Unit (IPU), compared to GPUs. On training workloads, we observe 5-10x throughput gains compared to A100 GPUs and up to 38x gains for higher levels of activation sparsity, without a significant slowdown in training convergence or reduction in final model performance.Our results also show promising trends for both single and multi IPU configurations as we scale up to larger model sizes. Our work paves the way towards more efficient, non-standard models via AI training hardware beyond GPUs and competitive large-scale SNN models.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Malware-Detection-by-Integrating-Machine-Learning-with-Cuckoo-Sandbox"><a href="#Enhancing-Malware-Detection-by-Integrating-Machine-Learning-with-Cuckoo-Sandbox" class="headerlink" title="Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox"></a>Enhancing Malware Detection by Integrating Machine Learning with Cuckoo Sandbox</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04372">http://arxiv.org/abs/2311.04372</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amaal F. Alshmarni, Mohammed A. Alliheedi</li>
<li>for: 本研究旨在透过机器学习算法进行遗传ware检测，以应对现代化时代内部网络中的遗传ware问题。</li>
<li>methods: 本研究使用的机器学习算法包括CNN（数值对应域几何网络）和RNN（回传神经网络），以检测API呼叫序列中的遗传ware。</li>
<li>results: 本研究发现，使用深度学习技术可以大幅提高检测精度，达到99%的准确率。<details>
<summary>Abstract</summary>
In the modern era, malware is experiencing a significant increase in both its variety and quantity, aligning with the widespread adoption of the digital world. This surge in malware has emerged as a critical challenge in the realm of cybersecurity, prompting numerous research endeavors and contributions to address the issue. Machine learning algorithms have been leveraged for malware detection due to their ability to uncover concealed patterns within vast datasets. However, deep learning algorithms, characterized by their multi-layered structure, surpass the limitations of traditional machine learning approaches. By employing deep learning techniques such as CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network), this study aims to classify and identify malware extracted from a dataset containing API call sequences. The performance of these algorithms is compared with that of conventional machine learning methods, including SVM (Support Vector Machine), RF (Random Forest), KNN (K-Nearest Neighbors), XGB (Extreme Gradient Boosting), and GBC (Gradient Boosting Classifier), all using the same dataset. The outcomes of this research demonstrate that both deep learning and machine learning algorithms achieve remarkably high levels of accuracy, reaching up to 99% in certain cases.
</details>
<details>
<summary>摘要</summary>
现代时期，恶意软件的种类和量在不断增加，与数字世界的普及相应。这种增长对网络安全领域带来了严重的挑战，导致了许多研究和贡献，以解决这个问题。机器学习算法在恶意软件检测中得到了广泛的应用，因为它们能够找到含有掩饰的模式。然而，深度学习算法，具有多层结构，超越了传统机器学习方法的限制。本研究使用深度学习技术，如卷积神经网络（CNN）和循环神经网络（RNN），对从API调用序列中提取的恶意软件进行分类和识别。这些算法的性能与传统机器学习方法，包括支持向量机（SVM）、随机森林（RF）、最近邻居（KNN）、极限梯度提升（XGB）和梯度提升分类器（GBC）进行比较，使用同一个数据集。研究结果表明，深度学习和机器学习算法均能达到非常高的准确率，达到99%以上在某些情况下。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-the-Effectiveness-of-Retrieval-Augmented-Large-Language-Models-in-Scientific-Document-Reasoning"><a href="#Evaluating-the-Effectiveness-of-Retrieval-Augmented-Large-Language-Models-in-Scientific-Document-Reasoning" class="headerlink" title="Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning"></a>Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04348">http://arxiv.org/abs/2311.04348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Munikoti, Anurag Acharya, Sridevi Wagle, Sameera Horawalavithana</li>
<li>for: 本研究旨在评估回备式语言模型在科学文献理解任务中的表现。</li>
<li>methods: 本研究使用了多种变体的回备式语言模型，并对其进行了科学化的 instrucion 以便在科学文献理解任务中进行评估。</li>
<li>results: 研究发现，使用回备式语言模型可能会导致模型提供的解释不准确，而且使用科学文献作为预训练数据不能减少这种风险。<details>
<summary>Abstract</summary>
Despite the dramatic progress in Large Language Model (LLM) development, LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations. Retrieval-augmented LLMs provide a non-parametric approach to solve these issues by retrieving relevant information from external data sources and augment the training process. These models help to trace evidence from an externally provided knowledge base allowing the model predictions to be better interpreted and verified. In this work, we critically evaluate these models in their ability to perform in scientific document reasoning tasks. To this end, we tuned multiple such model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark for the usefulness of the retrieved document passages. Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.
</details>
<details>
<summary>摘要</summary>
In this study, we critically evaluated the performance of these models in scientific document reasoning tasks. We tuned multiple model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark. Our findings suggest that the models often justify their predictions with fabricated evidence, and using a scientific corpus as pretraining data does not eliminate the risk of evidence fabrication.
</details></li>
</ul>
<hr>
<h2 id="A-Taxonomy-of-Rater-Disagreements-Surveying-Challenges-Opportunities-from-the-Perspective-of-Annotating-Online-Toxicity"><a href="#A-Taxonomy-of-Rater-Disagreements-Surveying-Challenges-Opportunities-from-the-Perspective-of-Annotating-Online-Toxicity" class="headerlink" title="A Taxonomy of Rater Disagreements: Surveying Challenges &amp; Opportunities from the Perspective of Annotating Online Toxicity"></a>A Taxonomy of Rater Disagreements: Surveying Challenges &amp; Opportunities from the Perspective of Annotating Online Toxicity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04345">http://arxiv.org/abs/2311.04345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbo Zhang, Hangzhi Guo, Ian D Kivlichan, Vinodkumar Prabhakaran, Davis Yadav, Amulya Yadav</li>
<li>for: 本研究目的是分析在 онлайн审核 зада务中出现的评审者分歧的原因，并提出一种细化的分类系统来解决这些分歧。</li>
<li>methods: 本研究基于大量的人工标注数据，通过分析审核者之间的分歧，提出了一种细化的分类系统，并评估了这种系统的性能。</li>
<li>results: 研究发现了在在线审核任务中出现的主要原因，并提出了一些解决这些问题的可能性。同时，还发现了一些未解决的问题，它们可能会推动未来的研究发展。<details>
<summary>Abstract</summary>
Toxicity is an increasingly common and severe issue in online spaces. Consequently, a rich line of machine learning research over the past decade has focused on computationally detecting and mitigating online toxicity. These efforts crucially rely on human-annotated datasets that identify toxic content of various kinds in social media texts. However, such annotations historically yield low inter-rater agreement, which was often dealt with by taking the majority vote or other such approaches to arrive at a single ground truth label. Recent research has pointed out the importance of accounting for the subjective nature of this task when building and utilizing these datasets, and this has triggered work on analyzing and better understanding rater disagreements, and how they could be effectively incorporated into the machine learning developmental pipeline. While these efforts are filling an important gap, there is a lack of a broader framework about the root causes of rater disagreement, and therefore, we situate this work within that broader landscape. In this survey paper, we analyze a broad set of literature on the reasons behind rater disagreements focusing on online toxicity, and propose a detailed taxonomy for the same. Further, we summarize and discuss the potential solutions targeting each reason for disagreement. We also discuss several open issues, which could promote the future development of online toxicity research.
</details>
<details>
<summary>摘要</summary>
online spaces中的恶意问题正在不断增加，因此过去的一个 décennial的机器学习研究推力在计算机测检和消除在社交媒体文本中的恶意内容。这些努力依赖于人工标注数据，以识别不同类型的社交媒体文本中的恶意内容。然而，这些标注历史上的间合率往往低，通常通过大多数票选或类似方法来到达唯一的真实标签。最近的研究表明，需要考虑这些任务的主观性，在建立和使用这些数据时。这些努力正在填补重要的空白，但是还缺乏一个更广泛的框架，描述投票分歧的根本原因。在这篇survey paper中，我们分析了在线恶意问题中投票分歧的广泛文献，并提出了详细的分类。此外，我们还总结了对每个分歧原因的解决方案，并讨论了未来研究的一些开放问题。
</details></li>
</ul>
<hr>
<h2 id="Sub-Sentence-Encoder-Contrastive-Learning-of-Propositional-Semantic-Representations"><a href="#Sub-Sentence-Encoder-Contrastive-Learning-of-Propositional-Semantic-Representations" class="headerlink" title="Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations"></a>Sub-Sentence Encoder: Contrastive Learning of Propositional Semantic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04335">http://arxiv.org/abs/2311.04335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/schen149/sub-sentence-encoder">https://github.com/schen149/sub-sentence-encoder</a></li>
<li>paper_authors: Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou, Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang, Dan Roth, Dong Yu</li>
<li>for: 本研究旨在提出一种基于对比学习的上下文嵌入模型，用于精细 semantic representation of text。</li>
<li>methods: 该模型使用对比学习来学习不同文本序列中的 atomic propositions 的上下文嵌入，并通过推断性学习来认识这些提positions 的 semantic equivalence。</li>
<li>results: 实验表明，使用 sub-sentence encoder 可以在 text attribution 和 conditional semantic similarity 等应用中 достичь同等效果，而且具有同样的推断成本和空间复杂度。<details>
<summary>Abstract</summary>
We introduce sub-sentence encoder, a contrastively-learned contextual embedding model for fine-grained semantic representation of text. In contrast to the standard practice with sentence embeddings, where the meaning of an entire sequence of text is encoded into a fixed-length vector, the sub-sentence encoder learns to produce distinct contextual embeddings corresponding to different atomic propositions, i.e. atomic units of meaning expressed within a text sequence. The sub-sentence embeddings are contrastively learned to recognize (inferred) semantic equivalence between propositions across different text sequences. Our experiments show the effectiveness of sub-sentence encoders in applications, such as retrieving supporting facts for fine-grained text attribution or recognizing the conditional semantic similarity between texts. In practice, we demonstrate that sub-sentence encoders keep the same level of inference cost and space complexity compared to sentence encoders.
</details>
<details>
<summary>摘要</summary>
我们介绍了下属句编码器，一种基于对比学习的上下文嵌入模型，用于精细表示文本中的Semantic meaning。与标准做法不同，即将整个文本序列编码为固定长度向量，我们的下属句编码器学习生成不同原子提POSITION的上下文嵌入，即在文本序列中表达的精细意义单元。这些下属句嵌入通过对推理结果进行对比学习来认识（推理出）文本序列之间的含义相似性。我们的实验表明，使用下属句编码器可以在文本检索、细化文本责任等应用中 достичь更高的效果，而且在实践中，下属句编码器与句子编码器的推理成本和空间复杂度相同。
</details></li>
</ul>
<hr>
<h2 id="Educating-for-AI-Cybersecurity-Work-and-Research-Ethics-Systems-Thinking-and-Communication-Requirements"><a href="#Educating-for-AI-Cybersecurity-Work-and-Research-Ethics-Systems-Thinking-and-Communication-Requirements" class="headerlink" title="Educating for AI Cybersecurity Work and Research: Ethics, Systems Thinking, and Communication Requirements"></a>Educating for AI Cybersecurity Work and Research: Ethics, Systems Thinking, and Communication Requirements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04326">http://arxiv.org/abs/2311.04326</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sorin Adam Matei, Elisa Bertino<br>for:* The paper explores managerial and instructor perceptions of freshly employed cybersecurity workers’ preparedness to work effectively in a changing cybersecurity environment that includes AI tools.methods:* The study uses a survey to gather perceptions of technical preparedness and non-technical skill sets (ethical, systems thinking, and communication skills) among managers and professors.results:* The study finds that preparedness to use AI tools in cybersecurity is significantly associated with all three non-technical skill sets, with ethics being the most important factor. Additionally, professors over-estimate students’ preparedness for ethical, system thinking, and communication abilities compared to IT managers’ perceptions of their newly employed IT workers.Here is the same information in Simplified Chinese text:for:* 这个研究探讨管理者和教师对新招聘的网络安全工作者或学生的技能准备能力是否足够在不断变化的网络安全环境中使用人工智能工具。methods:* 这个研究使用问卷调查管理者和教师对技术准备和非技术素质（伦理、系统思维和沟通技能）的看法。results:* 研究发现，使用人工智能工具在网络安全中的准备度与非技术素质（伦理、系统思维和沟通技能）密切相关，其中伦理素质最为重要。此外，教师对学生的技能准备有所过分估计，与实际上的IT管理者对新招聘工作者的评估不符。<details>
<summary>Abstract</summary>
The present study explored managerial and instructor perceptions of their freshly employed cybersecurity workers' or students' preparedness to work effectively in a changing cybersecurity environment that includes AI tools. Specifically, we related perceptions of technical preparedness to ethical, systems thinking, and communication skills. We found that managers and professors perceive preparedness to use AI tools in cybersecurity to be significantly associated with all three non-technical skill sets. Most important, ethics is a clear leader in the network of relationships. Contrary to expectations that ethical concerns are left behind in the rush to adopt the most advanced AI tools in security, both higher education instructors and managers appreciate their role and see them closely associated with technical prowess. Another significant finding is that professors over-estimate students' preparedness for ethical, system thinking, and communication abilities compared to IT managers' perceptions of their newly employed IT workers.
</details>
<details>
<summary>摘要</summary>
本研究 investigate 管理人员和教师对新招募的黑客工作者或学生的准备度，以及这些工作者在变化的黑客环境中使用人工智能工具时的效果。特别是，我们关注技术准备与伦理、系统思维和communication skills的关系。我们发现，管理人员和教师认为使用人工智能工具的准备度与三种非技术能力 closely related。其中，伦理是网络关系中的明显领导者。与预期相反，教育高等教育和管理人员看到了伦理的重要性，并认为它与技术能力密切相关。另外，教师对学生的准备度过高于IT管理人员对新入职IT工程师的评估。
</details></li>
</ul>
<hr>
<h2 id="Extending-Machine-Learning-Based-Early-Sepsis-Detection-to-Different-Demographics"><a href="#Extending-Machine-Learning-Based-Early-Sepsis-Detection-to-Different-Demographics" class="headerlink" title="Extending Machine Learning-Based Early Sepsis Detection to Different Demographics"></a>Extending Machine Learning-Based Early Sepsis Detection to Different Demographics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04325">http://arxiv.org/abs/2311.04325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surajsinh Parmar, Tao Shan, San Lee, Yonghwan Kim, Jang Yong Kim</li>
<li>for: 这个研究旨在比较两种ensemble学习方法 LightGBM 和 XGBoost，使用公共的 eICU-CRD 数据集和私人的韩国圣玛利亚医院数据集，以解决医疗数据的不均衡问题并提高 septic shock 的检测精度。</li>
<li>methods: 这个研究使用了两种ensemble学习方法 LightGBM 和 XGBoost，并对两种方法进行比较分析，以挖掘它们在医疗数据中的应用前提和优势。</li>
<li>results: 研究发现 LightGBM 在计算效率和扩展性方面有一定的优势，并且能够有效地解决医疗数据中的不均衡问题，从而提高 septic shock 的检测精度。<details>
<summary>Abstract</summary>
Sepsis requires urgent diagnosis, but research is predominantly focused on Western datasets. In this study, we perform a comparative analysis of two ensemble learning methods, LightGBM and XGBoost, using the public eICU-CRD dataset and a private South Korean St. Mary's Hospital's dataset. Our analysis reveals the effectiveness of these methods in addressing healthcare data imbalance and enhancing sepsis detection. Specifically, LightGBM shows a slight edge in computational efficiency and scalability. The study paves the way for the broader application of machine learning in critical care, thereby expanding the reach of predictive analytics in healthcare globally.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。<</SYS>> septic需要紧急诊断，但研究主要集中在西方数据集上。在这项研究中，我们对公共eICU-CRD数据集和私人韩国圣玛利亚医院数据集进行比较分析，使用两种ensemble学习方法：LightGBM和XGBoost。我们的分析发现这两种方法在医疗数据异质问题上能够具有效果，提高 septic检测。特别是LightGBM在计算效率和可扩展性方面表现略微优势。这项研究为医疗预测分析在全球扩展开创了道路。
</details></li>
</ul>
<hr>
<h2 id="Improved-Child-Text-to-Speech-Synthesis-through-Fastpitch-based-Transfer-Learning"><a href="#Improved-Child-Text-to-Speech-Synthesis-through-Fastpitch-based-Transfer-Learning" class="headerlink" title="Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning"></a>Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04313">http://arxiv.org/abs/2311.04313</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishabh Jain, Peter Corcoran</li>
<li>for: 本研究旨在开发高质量的人工婴儿语音生成技术，以满足媒体和互联网等领域的需求。</li>
<li>methods: 本研究使用了Fastpitch TTS模型，并通过转移学习训练技术来适应婴儿语音特点。使用了公共可用的MyST数据集（55小时）进行训练实验，并同时释放了一个 synthetic speech 示例数据集和模型代码，以支持后续研究。</li>
<li>results: 研究表明，通过使用预训练的 MOSNet，可以对真实婴儿语音和人工生成的语音进行对比，并显示了 significante 的相关性。此外，通过自动语音识别（ASR）模型对比真实婴儿语音和人工生成的语音的单词错误率（WER），可以证明生成的语音具有高度的智能性。同时，使用预训练的 speaker encoder 来测量真实婴儿语音和生成的语音之间的发音相似性，也得到了正面的结果。<details>
<summary>Abstract</summary>
Speech synthesis technology has witnessed significant advancements in recent years, enabling the creation of natural and expressive synthetic speech. One area of particular interest is the generation of synthetic child speech, which presents unique challenges due to children's distinct vocal characteristics and developmental stages. This paper presents a novel approach that leverages the Fastpitch text-to-speech (TTS) model for generating high-quality synthetic child speech. This study uses the transfer learning training pipeline. The approach involved finetuning a multi-speaker TTS model to work with child speech. We use the cleaned version of the publicly available MyST dataset (55 hours) for our finetuning experiments. We also release a prototype dataset of synthetic speech samples generated from this research together with model code to support further research. By using a pretrained MOSNet, we conducted an objective assessment that showed a significant correlation between real and synthetic child voices. Additionally, to validate the intelligibility of the generated speech, we employed an automatic speech recognition (ASR) model to compare the word error rates (WER) of real and synthetic child voices. The speaker similarity between the real and generated speech is also measured using a pretrained speaker encoder.
</details>
<details>
<summary>摘要</summary>
文本生成技术在最近几年内得到了 significiant 进步，使得可以生成自然和表情强的synthetic 语音。一个特定的研究领域是生成synthetic 孩子语音，这个领域存在独特的声音特征和发展阶段。本文提出了一种新的方法，利用Fastpitch 文本到语音（TTS）模型来生成高质量的synthetic 孩子语音。这个研究使用了传输学习训练管道。我们使用了公共可用的MyST dataset（55小时）进行了 fine-tuning 实验。我们还发布了一个原型数据集，包含本研究生成的synthetic 语音样本以及模型代码，以支持进一步的研究。通过使用预训练的MOSNet，我们进行了一个 объектив评估，显示了real 和synthetic 孩子声音之间的strong 相关性。此外，为了验证生成的语音可以理解，我们使用了自动语音识别（ASR）模型来比较实际和synthetic 孩子声音的单词错误率（WER）。生成的语音和实际孩子声音之间的 speaker 相似性也被使用预训练的 speaker 编码器测量。
</details></li>
</ul>
<hr>
<h2 id="Class-Incremental-Continual-Learning-for-General-Purpose-Healthcare-Models"><a href="#Class-Incremental-Continual-Learning-for-General-Purpose-Healthcare-Models" class="headerlink" title="Class-Incremental Continual Learning for General Purpose Healthcare Models"></a>Class-Incremental Continual Learning for General Purpose Healthcare Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04301">http://arxiv.org/abs/2311.04301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amritpal Singh, Mustafa Burak Gurbuz, Shiva Souhith Gantha, Prahlad Jasti</li>
<li>for: 这篇论文旨在探讨医疗机构面临时间变化的资料，使用深度学习模型进行预测，并考虑这些模型在不同预设下的执行。</li>
<li>methods: 这篇论文使用了多种恒常学习方法，包括对应式学习、优先级学习和对应式优先级学习，以评估其在不同医疗团队和医院中的表现。</li>
<li>results: 研究结果显示，单一模型可以顺利地学习新任务，并在不同的医疗专长和医院中获得相似的表现，这显示了模型的可 reuse 和共享性。<details>
<summary>Abstract</summary>
Healthcare clinics regularly encounter dynamic data that changes due to variations in patient populations, treatment policies, medical devices, and emerging disease patterns. Deep learning models can suffer from catastrophic forgetting when fine-tuned in such scenarios, causing poor performance on previously learned tasks. Continual learning allows learning on new tasks without performance drop on previous tasks. In this work, we investigate the performance of continual learning models on four different medical imaging scenarios involving ten classification datasets from diverse modalities, clinical specialties, and hospitals. We implement various continual learning approaches and evaluate their performance in these scenarios. Our results demonstrate that a single model can sequentially learn new tasks from different specialties and achieve comparable performance to naive methods. These findings indicate the feasibility of recycling or sharing models across the same or different medical specialties, offering another step towards the development of general-purpose medical imaging AI that can be shared across institutions.
</details>
<details>
<summary>摘要</summary>
医疗机构常常面临变化的数据，由于患者人口、治疗策略、医疗设备和疾病趋势的变化。深度学习模型可能会受到恶性忘记，导致在已经学习的任务上表现不佳。连续学习允许学习新任务无需影响之前学习的任务表现。在这项工作中，我们研究了不同医疗专业和医院的十种分类数据集的四个医学影像场景，并应用了不同的连续学习方法。我们评估了这些场景中的表现，并发现一个单一的模型可以顺序学习不同的专业任务，并与预期的方法相当。这些结果表明了将模型重用或共享的可能性，这将是医学影像人工智能的另一步发展，可以在机构之间共享。
</details></li>
</ul>
<hr>
<h2 id="CRAB-Assessing-the-Strength-of-Causal-Relationships-Between-Real-world-Events"><a href="#CRAB-Assessing-the-Strength-of-Causal-Relationships-Between-Real-world-Events" class="headerlink" title="CRAB: Assessing the Strength of Causal Relationships Between Real-world Events"></a>CRAB: Assessing the Strength of Causal Relationships Between Real-world Events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04284">http://arxiv.org/abs/2311.04284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelika Romanou, Syrielle Montariol, Debjit Paul, Leo Laugier, Karl Aberer, Antoine Bosselut</li>
<li>for: 评估大型自然语言处理模型在叙述中的 causal 理解能力</li>
<li>methods: 使用 CRAB 数据集，一个基于实际新闻事件时间轴的 fine-grained  causality 注释 benchmark，评估多种语言模型在 causal 理解任务中的表现</li>
<li>results: 研究发现，大多数语言模型在 causal 理解任务中表现不佳，而且模型在 causal 结构较复杂的事件组合中表现更差。<details>
<summary>Abstract</summary>
Understanding narratives requires reasoning about the cause-and-effect relationships between events mentioned in the text. While existing foundation models yield impressive results in many NLP tasks requiring reasoning, it is unclear whether they understand the complexity of the underlying network of causal relationships of events in narratives. In this work, we present CRAB, a new Causal Reasoning Assessment Benchmark designed to evaluate causal understanding of events in real-world narratives. CRAB contains fine-grained, contextual causality annotations for ~2.7K pairs of real-world events that describe various newsworthy event timelines (e.g., the acquisition of Twitter by Elon Musk). Using CRAB, we measure the performance of several large language models, demonstrating that most systems achieve poor performance on the task. Motivated by classical causal principles, we also analyze the causal structures of groups of events in CRAB, and find that models perform worse on causal reasoning when events are derived from complex causal structures compared to simple linear causal chains. We make our dataset and code available to the research community.
</details>
<details>
<summary>摘要</summary>
理解叙述需要关于文本中事件之间的 causal 关系的理解。 existing foundation models 在许多 NLP 任务中表现出色，但是是 unclear  Whether they 理解叙述中事件的下面网络 causal 关系的复杂性。 在这项工作中，我们提出了 CRAB，一个新的 Causal Reasoning Assessment Benchmark，用于评估事件叙述中 causal 理解。 CRAB 包含了 ~2.7K 对实际新闻事件时间线（如 Elon Musk 收购 Twitter）的细腻、 Contextual causality 注释。 使用 CRAB，我们测量了多种大语言模型的性能，发现大多数系统在这个任务中表现出低水平。 针对古典 causal 原则，我们也分析了 CRAB 中事件群体的 causal 结构，发现模型在复杂 causal 结构下的事件 derivation 时表现更差。 我们将数据集和代码提供给研究社区。
</details></li>
</ul>
<hr>
<h2 id="OtterHD-A-High-Resolution-Multi-modality-Model"><a href="#OtterHD-A-High-Resolution-Multi-modality-Model" class="headerlink" title="OtterHD: A High-Resolution Multi-modality Model"></a>OtterHD: A High-Resolution Multi-modality Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04219">http://arxiv.org/abs/2311.04219</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang, Fanyi Pu, Ziwei Liu</li>
<li>for: 本研究专页探讨了一种新型多modal模型OtterHD-8B，它是基于Fuyu-8B的创新模型，能够准确地处理高分辨率的视觉输入。</li>
<li>methods: 该模型不同于传统模型，不受固定大小视觉编码器的限制，可以适应不同的数据类型和视觉输入大小。</li>
<li>results: 我们的比较分析发现，与现今领先的模型不同，OtterHD-8B在高分辨率直接处理视觉输入时表现出色，与其他模型之间有substantial margin的差异。<details>
<summary>Abstract</summary>
In this paper, we present OtterHD-8B, an innovative multimodal model evolved from Fuyu-8B, specifically engineered to interpret high-resolution visual inputs with granular precision. Unlike conventional models that are constrained by fixed-size vision encoders, OtterHD-8B boasts the ability to handle flexible input dimensions, ensuring its versatility across various inference requirements. Alongside this model, we introduce MagnifierBench, an evaluation framework designed to scrutinize models' ability to discern minute details and spatial relationships of small objects. Our comparative analysis reveals that while current leading models falter on this benchmark, OtterHD-8B, particularly when directly processing high-resolution inputs, outperforms its counterparts by a substantial margin. The findings illuminate the structural variances in visual information processing among different models and the influence that the vision encoders' pre-training resolution disparities have on model effectiveness within such benchmarks. Our study highlights the critical role of flexibility and high-resolution input capabilities in large multimodal models and also exemplifies the potential inherent in the Fuyu architecture's simplicity for handling complex visual data.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了OtterHD-8B，一种创新的多modal模型，由Fuyu-8B进行演化，专门为高分辨率视觉输入提供精细精度的解释。与传统模型一样，OtterHD-8B不受固定大小视觉编解码器的限制，因此它在不同的推理需求下表现出高度的灵活性。此外，我们还提出了MagnifierBench评估框架，用于评测模型对小 объек的细节和空间关系的解释能力。我们的比较分析表明，当直接处理高分辨率输入时，OtterHD-8B般比其他对手提供了substantial的优势。这些发现探讨了不同模型在视觉信息处理中的结构差异，以及视觉编解码器的预训练分辨率差异对模型在如此 benchmark中的效果的影响。我们的研究强调了大型多modal模型中的灵活性和高分辨率输入能力的重要性，同时也表明了Fuyu架构的简单性可以处理复杂的视觉数据。
</details></li>
</ul>
<hr>
<h2 id="Towards-Garment-Sewing-Pattern-Reconstruction-from-a-Single-Image"><a href="#Towards-Garment-Sewing-Pattern-Reconstruction-from-a-Single-Image" class="headerlink" title="Towards Garment Sewing Pattern Reconstruction from a Single Image"></a>Towards Garment Sewing Pattern Reconstruction from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04218">http://arxiv.org/abs/2311.04218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lijuan Liu, Xiangyu Xu, Zhijie Lin, Jiabin Liang, Shuicheng Yan</li>
<li>for: 本研究旨在使用日常照片中恢复服装缝纫图，以推进服装设计、虚拟试穿和数字人物等应用。</li>
<li>methods: 我们首先生成了一个多样化的数据集，名为SewFactory，该数据集包含约1M张图像和真实缝纫图数据，用于模型训练和评估。我们还提出了一个两级转换器网络，名为Sewformer，该网络能够显著提高缝纫图预测性能。</li>
<li>results: 我们的实验表明，提posed框架可以有效地恢复缝纫图和在习惯性拍摄的人像照片中具有良好的泛化能力。代码、数据集和预训练模型可以在<a target="_blank" rel="noopener" href="https://sewformer.github.io/">https://sewformer.github.io/</a> obtain。<details>
<summary>Abstract</summary>
Garment sewing pattern represents the intrinsic rest shape of a garment, and is the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. Code, dataset, and pre-trained models are available at: https://sewformer.github.io.
</details>
<details>
<summary>摘要</summary>
裤装 Patterns represent the intrinsic rest shape of a garment, and are the core for many applications like fashion design, virtual try-on, and digital avatars. In this work, we explore the challenging problem of recovering garment sewing patterns from daily photos for augmenting these applications. To solve the problem, we first synthesize a versatile dataset, named SewFactory, which consists of around 1M images and ground-truth sewing patterns for model training and quantitative evaluation. SewFactory covers a wide range of human poses, body shapes, and sewing patterns, and possesses realistic appearances thanks to the proposed human texture synthesis network. Then, we propose a two-level Transformer network called Sewformer, which significantly improves the sewing pattern prediction performance. Extensive experiments demonstrate that the proposed framework is effective in recovering sewing patterns and well generalizes to casually-taken human photos. codes, datasets, and pre-trained models are available at: https://sewformer.github.io.
</details></li>
</ul>
<hr>
<h2 id="Wearable-data-from-subjects-playing-Super-Mario-sitting-university-exams-or-performing-physical-exercise-help-detect-acute-mood-episodes-via-self-supervised-learning"><a href="#Wearable-data-from-subjects-playing-Super-Mario-sitting-university-exams-or-performing-physical-exercise-help-detect-acute-mood-episodes-via-self-supervised-learning" class="headerlink" title="Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning"></a>Wearable data from subjects playing Super Mario, sitting university exams, or performing physical exercise help detect acute mood episodes via self-supervised learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04215">http://arxiv.org/abs/2311.04215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filippo Corponi, Bryan M. Li, Gerard Anmella, Clàudia Valenzuela-Pascual, Ariadna Mas, Isabella Pacchiarotti, Marc Valentí, Iria Grande, Antonio Benabarre, Marina Garriga, Eduard Vieta, Allan H Young, Stephen M. Lawrie, Heather C. Whalley, Diego Hidalgo-Mazzei, Antonio Vergari</li>
<li>for: 这个论文旨在检测抑郁症（MDs）的酷热话论文，使用个人感知技术，收集来自病人的穿戴式设备上的数据，以实时监测病人的情绪状况。</li>
<li>methods: 这个论文使用的方法包括自动学习（SSL）技术，使用无标注数据来学习个人感知数据的表示，并将其应用于检测MDs的酷热话论文。</li>
<li>results: 研究发现，使用SSL技术可以高效地检测MDs的酷热话论文，并且可以在64名病人中 correctly classified recording segments的率为81.23%。此外，研究还发现，使用不同的代理任务进行预训练可以强调SSL的性能。<details>
<summary>Abstract</summary>
Personal sensing, leveraging data passively and near-continuously collected with wearables from patients in their ecological environment, is a promising paradigm to monitor mood disorders (MDs), a major determinant of worldwide disease burden. However, collecting and annotating wearable data is very resource-intensive. Studies of this kind can thus typically afford to recruit only a couple dozens of patients. This constitutes one of the major obstacles to applying modern supervised machine learning techniques to MDs detection. In this paper, we overcome this data bottleneck and advance the detection of MDs acute episode vs stable state from wearables data on the back of recent advances in self-supervised learning (SSL). This leverages unlabelled data to learn representations during pre-training, subsequently exploited for a supervised task. First, we collected open-access datasets recording with an Empatica E4 spanning different, unrelated to MD monitoring, personal sensing tasks -- from emotion recognition in Super Mario players to stress detection in undergraduates -- and devised a pre-processing pipeline performing on-/off-body detection, sleep-wake detection, segmentation, and (optionally) feature extraction. With 161 E4-recorded subjects, we introduce E4SelfLearning, the largest to date open access collection, and its pre-processing pipeline. Second, we show that SSL confidently outperforms fully-supervised pipelines using either our novel E4-tailored Transformer architecture (E4mer) or classical baseline XGBoost: 81.23% against 75.35% (E4mer) and 72.02% (XGBoost) correctly classified recording segments from 64 (half acute, half stable) patients. Lastly, we illustrate that SSL performance is strongly associated with the specific surrogate task employed for pre-training as well as with unlabelled data availability.
</details>
<details>
<summary>摘要</summary>
个人感知，通过在患有抑郁症（MD）患者身边采集和分析穿戴式设备数据，可以识别抑郁症的危机性状态和稳定状态。但是收集和标注穿戴式数据具有极高的人员和资源成本，这限制了使用现代监督学习技术来识别抑郁症的应用。在这篇论文中，我们超越了这种数据瓶颈，通过自我监督学习（SSL）技术，使用无标注数据来学习表示，并将其应用于抑郁症的危机性状态和稳定状态的识别。我们收集了多个不同任务的开放数据集，包括辨别超级马里奥玩家的情感和快速识别大学生的压力，并设计了一个预处理管道，包括在/离体检测、睡眠唤醒检测、分割和（可选）特征提取。通过161名E4记录的主题，我们介绍了E4SelfLearning数据集，这是目前最大的开放访问数据集。我们显示，使用SSL技术可以高效地超过完全监督的架构，包括我们的新型E4特рансформа器架构（E4mer）和经典基eline XGBoost。在64名患者（每个患者有半个危机性状态和半个稳定状态）的记录段中，我们得到了81.23%的正确率，比XGBoost和E4mer的75.35%和72.02%高。最后，我们发现了SSL性能与具体的代理任务和无标注数据的可用性有很强的相关性。
</details></li>
</ul>
<hr>
<h2 id="Rephrase-and-Respond-Let-Large-Language-Models-Ask-Better-Questions-for-Themselves"><a href="#Rephrase-and-Respond-Let-Large-Language-Models-Ask-Better-Questions-for-Themselves" class="headerlink" title="Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves"></a>Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04205">http://arxiv.org/abs/2311.04205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uclaml/Rephrase-and-Respond">https://github.com/uclaml/Rephrase-and-Respond</a></li>
<li>paper_authors: Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu</li>
<li>for: 提高大语言模型（LLM）的性能，使其更好地理解人类的问题并提供有用的回答。</li>
<li>methods: 提出了一种名为“重新phrase和回答”（Rephrase and Respond，简称RaR）的方法，允许人类提问和LLM重新表达问题，并在单个提问中获得回答。这种方法简单且有效地提高了不同任务的性能。</li>
<li>results: 通过实验表明，我们的方法可以在各种任务上提高不同模型的性能，并且与Chain-of-Thought（CoT）方法相比，RaR是一种可以补充CoT的方法，可以在不同的任务上实现更好的性能。<details>
<summary>Abstract</summary>
Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models (LLMs). Such discrepancies can make LLMs interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by LLMs, a systematic method for crafting questions that LLMs can better comprehend is still underdeveloped. In this paper, we present a method named `Rephrase and Respond' (RaR), which allows LLMs to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of RaR, where a rephrasing LLM first rephrases the question and then passes the original and rephrased questions together to a different responding LLM. This facilitates the effective utilization of rephrased questions generated by one LLM with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive comparison between RaR and the popular Chain-of-Thought (CoT) methods, both theoretically and empirically. We show that RaR is complementary to CoT and can be combined with CoT to achieve even better performance. Our work not only contributes to enhancing LLM performance efficiently and effectively but also sheds light on a fair evaluation of LLM capabilities. Data and codes are available at https://github.com/uclaml/Rephrase-and-Respond.
</details>
<details>
<summary>摘要</summary>
人类和大语言模型（LLM）之间的误解不仅存在于人际communication中，还可能出现在LLM与人类之间的交互中。这些差异可能使LLM解释看起来很明确的问题时，提供错误的回答。虽然广泛认可的提示质量对LLM提供回答的质量产生了很大的影响，但是一种系统的方法 для编写LLM可以更好地理解的提示还没有得到开发。在这篇论文中，我们提出了一种方法 named “Rephrase and Respond”（RaR），该方法允许LLM将人类提出的问题重新表述并提供回答在单个提示中。这种方法可以作为一种简单 yet effective的提示方法，以提高性能。我们还介绍了一种两步变体的RaR方法，其中一个重新表述LLM首先重新表述问题，然后将原始和重新表述的问题一起传递给另一个回答LLM。这种方法使得可以有效地利用重新表述的问题，并且可以与另一个LLM结合使用。我们的实验表明，我们的方法可以在不同任务上提高不同模型的性能。我们还提供了对RaR和popular Chain-of-Thought（CoT）方法的比较，包括理论和实验比较。我们表明，RaR和CoT是可 complementary的，可以将它们结合使用以实现更好的性能。我们的工作不仅有助于提高LLM性能，还有助于评估LLM的能力。数据和代码可以在https://github.com/uclaml/Rephrase-and-Respond上获取。
</details></li>
</ul>
<hr>
<h2 id="JPAVE-A-Generation-and-Classification-based-Model-for-Joint-Product-Attribute-Prediction-and-Value-Extraction"><a href="#JPAVE-A-Generation-and-Classification-based-Model-for-Joint-Product-Attribute-Prediction-and-Value-Extraction" class="headerlink" title="JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction"></a>JPAVE: A Generation and Classification-based Model for Joint Product Attribute Prediction and Value Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04196">http://arxiv.org/abs/2311.04196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhongfendeng/jpave">https://github.com/zhongfendeng/jpave</a></li>
<li>paper_authors: Zhongfen Deng, Hao Peng, Tao Zhang, Shuaiqi Liu, Wenting Zhao, Yibo Wang, Philip S. Yu</li>
<li>for: 本文提出了一种基于多任务学习的产品特征值EXTRACTION模型，以解决电商中产品搜索和推荐等下游应用中的产品特征值EXTRACTION问题。</li>
<li>methods: 本文提出了一种基于多任务学习的产品特征值EXTRACTION模型，包括值生成&#x2F;分类和属性预测三个任务。模型使用了值生成器和值分类器，并具有复制机制和价值注意力模块，以 Addressing data discrepancy issue和提高零例能力。</li>
<li>results: 实验结果表明，相比强基eline，本文提出的模型在一个公共数据集上表现出优于其他模型，并且在新值预测中具有较高的预测精度。<details>
<summary>Abstract</summary>
Product attribute value extraction is an important task in e-Commerce which can help several downstream applications such as product search and recommendation. Most previous models handle this task using sequence labeling or question answering method which rely on the sequential position information of values in the product text and are vulnerable to data discrepancy between training and testing. This limits their generalization ability to real-world scenario in which each product can have multiple descriptions across various shopping platforms with different composition of text and style. They also have limited zero-shot ability to new values. In this paper, we propose a multi-task learning model with value generation/classification and attribute prediction called JPAVE to predict values without the necessity of position information of values in the text. Furthermore, the copy mechanism in value generator and the value attention module in value classifier help our model address the data discrepancy issue by only focusing on the relevant part of input text and ignoring other information which causes the discrepancy issue such as sentence structure in the text. Besides, two variants of our model are designed for open-world and closed-world scenarios. In addition, copy mechanism introduced in the first variant based on value generation can improve its zero-shot ability for identifying unseen values. Experimental results on a public dataset demonstrate the superiority of our model compared with strong baselines and its generalization ability of predicting new values.
</details>
<details>
<summary>摘要</summary>
产品特征值EXTRACTION是电商中重要的任务，可以帮助多个下游应用程序，如产品搜索和推荐。现有的大多数模型都使用序列标签或问答方法来处理这个任务，这些方法受到文本中值的顺序位置信息的限制，容易受到训练和测试数据的不一致问题，这限制了它们在实际场景中的泛化能力。此外，这些模型也具有有限的零容量能力，无法处理新的值。在这篇论文中，我们提出了一种多任务学习模型，名为JPAVE，可以预测值不需要文本中值的顺序位置信息。此外，模型中的复制机制和价值注意模块帮助我们解决数据不一致问题，只关注输入文本中相关的部分，忽略其他信息。此外，我们还设计了两种模型的变种，一种适用于开放世界场景，另一种适用于关闭世界场景。实验结果表明，我们的模型在公共数据集上比强基eline模型表现出色，并且具有泛化能力预测新的值。
</details></li>
</ul>
<hr>
<h2 id="Selective-Visual-Representations-Improve-Convergence-and-Generalization-for-Embodied-AI"><a href="#Selective-Visual-Representations-Improve-Convergence-and-Generalization-for-Embodied-AI" class="headerlink" title="Selective Visual Representations Improve Convergence and Generalization for Embodied AI"></a>Selective Visual Representations Improve Convergence and Generalization for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04193">http://arxiv.org/abs/2311.04193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ainaz Eftekhar, Kuo-Hao Zeng, Jiafei Duan, Ali Farhadi, Ani Kembhavi, Ranjay Krishna</li>
<li>for: 这 paper 的目的是提出一种 parameter-efficient 方法，用于过滤embodied AI 模型中的视觉噪声，以提高模型的表现。</li>
<li>methods: 这 paper 使用了一种小型可学习的 codebook 模块，来实现任务条件的瓶颈。 codebook 被同时训练，以优化任务奖励，并作为任务条件的选择性筛选器对视觉观察进行过滤。</li>
<li>results: 这 paper 的实验结果表明，使用这种方法可以在 5 个 benchmark 上达到 state-of-the-art 表现，包括 ProcTHOR、ArchitecTHOR、RoboTHOR、AI2-iTHOR 和 ManipulaTHOR。 filtered 表示也能够更好地泛化和更快地融合到其他 simulated environment 中，如 Habitat。 qualitative 分析表明，使用这种方法可以让代理人更有效地探索其环境，并保留任务相关的信息，如目标物体识别，而忽略其他物体的信息。<details>
<summary>Abstract</summary>
Embodied AI models often employ off the shelf vision backbones like CLIP to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans-the process through which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code and pretrained models are available at our project website: https://embodied-codebook.github.io.
</details>
<details>
<summary>摘要</summary>
embodied AI模型经常使用可购买的视觉脊梁如CLIP来编码其视觉观察。尽管这些通用的表示编码了场景中的语法和 semantics信息，但大多数这些信息对特定任务没有直接关系。这会导致学习过程中的噪音和扰动agent的注意力，使其忽略任务相关的视觉cue。 Drawing inspiration from human selective attention-the process by which people filter their perception based on their experiences, knowledge, and the task at hand-we introduce a parameter-efficient approach to filter visual stimuli for embodied AI. Our approach uses a small learnable codebook module to induce a task-conditioned bottleneck. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR, and ManipulaTHOR. The filtered representations produced by the codebook are also able to generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects. Code and pretrained models are available at our project website: <https://embodied-codebook.github.io>.
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Anomaly-Detection-with-Graph-Networks-for-Data-Quality-Monitoring-of-the-Hadron-Calorimeter"><a href="#Spatio-Temporal-Anomaly-Detection-with-Graph-Networks-for-Data-Quality-Monitoring-of-the-Hadron-Calorimeter" class="headerlink" title="Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter"></a>Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04190">http://arxiv.org/abs/2311.04190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mulugeta Weldezgina Asres, Christian Walter Omlin, Long Wang, David Yu, Pavel Parygin, Jay Dittmann, Georgia Karapostoli, Markus Seidel, Rosamaria Venditti, Luka Lambrecht, Emanuele Usai, Muhammad Ahmad, Javier Fernandez Menendez, Kaori Maeshima, the CMS-HCAL Collaboration</li>
<li>for: 这个研究旨在开发一种基于三维幂点映射数据的半监督空间时间异常检测系统，以检测CMS实验室中有机器错误的可能性。</li>
<li>methods: 该研究使用了卷积神经网络和图神经网络来学习探测器中的本地空间特征，以及背后的回路连接和仪器箱的全局行为。它还使用回归神经网络来捕捉扫描出的特征的时间演化。</li>
<li>results: 该研究通过使用LHC Run-2的粒子撞击数据集来验证 GraphSTAD 系统的准确性，并证明了该系统在捕捉多种探测器错误类型时的产生性。此外，该研究还对alternative指标模型进行了量化比较，以验证 GraphSTAD 系统的扩展优势。<details>
<summary>Abstract</summary>
The compact muon solenoid (CMS) experiment is a general-purpose detector for high-energy collision at the large hadron collider (LHC) at CERN. It employs an online data quality monitoring (DQM) system to promptly spot and diagnose particle data acquisition problems to avoid data quality loss. In this study, we present semi-supervised spatio-temporal anomaly detection (AD) monitoring for the physics particle reading channels of the hadronic calorimeter (HCAL) of the CMS using three-dimensional digi-occupancy map data of the DQM. We propose the GraphSTAD system, which employs convolutional and graph neural networks to learn local spatial characteristics induced by particles traversing the detector, and global behavior owing to shared backend circuit connections and housing boxes of the channels, respectively. Recurrent neural networks capture the temporal evolution of the extracted spatial features. We have validated the accuracy of the proposed AD system in capturing diverse channel fault types using the LHC Run-2 collision data sets. The GraphSTAD system has achieved production-level accuracy and is being integrated into the CMS core production system--for real-time monitoring of the HCAL. We have also provided a quantitative performance comparison with alternative benchmark models to demonstrate the promising leverage of the presented system.
</details>
<details>
<summary>摘要</summary>
大 compact氢离子探测器（CMS）实验是一个高能碰撞的通用侦测器，位于欧洲核子研究所（CERN）的大对核子碰撞机（LHC）上。它使用线上数据质量监控（DQM）系统，以及时发现和诊断数据收集问题，以避免数据质量损失。在这篇研究中，我们提出了半监督空间时间异常检测（AD）监控系统，用于CMSPhysics Particle Reading Channels的半导体探测器（HCAL）。我们提出了具有卷积神经网和图形神经网的GraphSTAD系统，用于学习探测器中的本地空间特征，以及分布式后端电路和封包盒的共享特征。循环神经网 capture了检测器中的时间演化。我们已经验证了提案的AD系统可以 Capture多种通道缺陷类型的数据集，使用LHC Run-2碰撞数据集。GraphSTAD系统在生产环境中已经 дости得高精度，并且在CMS核心生产系统中进行实时监控HCAL。我们也提供了一个量化性能比较，以展示提案的系统具有优秀的应用前景。
</details></li>
</ul>
<hr>
<h2 id="On-Leakage-in-Machine-Learning-Pipelines"><a href="#On-Leakage-in-Machine-Learning-Pipelines" class="headerlink" title="On Leakage in Machine Learning Pipelines"></a>On Leakage in Machine Learning Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04179">http://arxiv.org/abs/2311.04179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Leonard Sasse, Eliana Nicolaisen-Sobesky, Juergen Dukart, Simon B. Eickhoff, Michael Götz, Sami Hamdan, Vera Komeyer, Abhijit Kulkarni, Juha Lahnakoski, Bradley C. Love, Federico Raimondo, Kaustubh R. Patil</li>
<li>for: 本研究旨在扩展对MLipeline中泄漏的理解，以便更好地设计、实现和评估MLipeline。</li>
<li>methods: 本研究使用了具体的例子，对MLipeline中的多种泄漏类型进行了全面的介绍和讨论。</li>
<li>results: 本研究揭示了MLipeline中泄漏的多种类型，并提供了一些解决方案，以便更好地设计、实现和评估MLipeline。<details>
<summary>Abstract</summary>
Machine learning (ML) provides powerful tools for predictive modeling. ML's popularity stems from the promise of sample-level prediction with applications across a variety of fields from physics and marketing to healthcare. However, if not properly implemented and evaluated, ML pipelines may contain leakage typically resulting in overoptimistic performance estimates and failure to generalize to new data. This can have severe negative financial and societal implications. Our aim is to expand understanding associated with causes leading to leakage when designing, implementing, and evaluating ML pipelines. Illustrated by concrete examples, we provide a comprehensive overview and discussion of various types of leakage that may arise in ML pipelines.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-LLM-Intelligence-with-ARM-RAG-Auxiliary-Rationale-Memory-for-Retrieval-Augmented-Generation"><a href="#Enhancing-LLM-Intelligence-with-ARM-RAG-Auxiliary-Rationale-Memory-for-Retrieval-Augmented-Generation" class="headerlink" title="Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation"></a>Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for Retrieval Augmented Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04177">http://arxiv.org/abs/2311.04177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Melz</li>
<li>for: 这篇论文目的是探讨如何通过增强问题解决能力来提高大语言模型（LLMs）的智能水平。</li>
<li>methods: 这篇论文使用了Retrieval Augmented Generation（RAG）技术，并提出了一种名为Auxiliary Rationale Memory（ARM）的新系统，该系统可以从成功中学习而不需要高度的训练成本。</li>
<li>results: 研究发现，通过存储和后续检索解释链可以提高grade-school math问题的解决能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g., (Bubeck et al., 2023)) on modern LLMs have shown that they are capable of performing amazing tasks typically necessitating human-level intelligence. However, unlike humans, frozen LLMs do not improve over time; they neither acquire new knowledge nor learn from their successes or failures. Some approaches to improving the intelligence of LLMs include fine-tuning models based on problem-solving performance (Zelikman et al., 2022), and building bigger and more sophisticated models (Bubeck et al., 2023). However, these methods have the drawback of requiring substantial data and computational resources to retrain existing models. In this paper, we explore the use of Retrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to improve problem-solving performance. We propose ARM-RAG (Auxiliary Rationale Memory for Retrieval Augmented Generation), a system that learns from its successes without incurring high training costs. We demonstrate that the storage and subsequent retrieval of reasoning chains have a positive influence on performance in grade-school math problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HADES-Fast-Singularity-Detection-with-Local-Measure-Comparison"><a href="#HADES-Fast-Singularity-Detection-with-Local-Measure-Comparison" class="headerlink" title="HADES: Fast Singularity Detection with Local Measure Comparison"></a>HADES: Fast Singularity Detection with Local Measure Comparison</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04171">http://arxiv.org/abs/2311.04171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uzu Lim, Harald Oberhauser, Vidit Nanda</li>
<li>for: 检测数据中的点 singularity</li>
<li>methods: 使用kernel好好 fit测试，比 existed topology-based方法更快和可扩展</li>
<li>results: 在synthetic数据、路网数据、分子结构空间和图像数据中recover singularities with high probability<details>
<summary>Abstract</summary>
We introduce Hades, an unsupervised algorithm to detect singularities in data. This algorithm employs a kernel goodness-of-fit test, and as a consequence it is much faster and far more scaleable than the existing topology-based alternatives. Using tools from differential geometry and optimal transport theory, we prove that Hades correctly detects singularities with high probability when the data sample lives on a transverse intersection of equidimensional manifolds. In computational experiments, Hades recovers singularities in synthetically generated data, branching points in road network data, intersection rings in molecular conformation space, and anomalies in image data.
</details>
<details>
<summary>摘要</summary>
我团队介绍了冥王（Hades）算法，用于不监督的数据中点检测缺陷。该算法使用一种层次质量适应测试，因此比现有的几何学基础的方法更快速和可扩展。通过几何学和优化运输理论的工具，我们证明了冥王可以高可能性地检测数据样本生成的缺陷，当数据样本居于等维度抽象 manifold 的横向交会处时。在计算实验中，冥王成功地恢复了 synthetic 数据中的缺陷、路网数据中的分支点、分子结构空间中的交叉环和图像数据中的异常。
</details></li>
</ul>
<hr>
<h2 id="Outliers-with-Opposing-Signals-Have-an-Outsized-Effect-on-Neural-Network-Optimization"><a href="#Outliers-with-Opposing-Signals-Have-an-Outsized-Effect-on-Neural-Network-Optimization" class="headerlink" title="Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization"></a>Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04163">http://arxiv.org/abs/2311.04163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elan Rosenfeld, Andrej Risteski</li>
<li>for: 这个论文旨在描述 neural network 优化中新现象，即深度和自然数据中特殊重 tailed 结构之间的交互作用。</li>
<li>methods: 本文使用实验和理论分析来描述这种新现象，并提供了一种新的解释方式。</li>
<li>results: 研究发现，在训练数据中存在对抗信号的对组，导致早期优化进入窄谷，并且随着优化的深化，这些对组的损失快速增长，oscillating  между高和低。这些结果提供了新的质量预测，并且通过对 Adam 和 SGD 的比较，探讨了现代训练方法的改进。<details>
<summary>Abstract</summary>
We identify a new phenomenon in neural network optimization which arises from the interaction of depth and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several previously reported observations about network training dynamics. In particular, it implies a conceptually new cause for progressive sharpening and the edge of stability; we also highlight connections to other concepts in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization.   Experimentally, we demonstrate the significant influence of paired groups of outliers in the training data with strong opposing signals: consistent, large magnitude features which dominate the network output throughout training and provide gradients which point in opposite directions. Due to these outliers, early optimization enters a narrow valley which carefully balances the opposing groups; subsequent sharpening causes their loss to rise rapidly, oscillating between high on one group and then the other, until the overall loss spikes. We describe how to identify these groups, explore what sets them apart, and carefully study their effect on the network's optimization and behavior. We complement these experiments with a mechanistic explanation on a toy example of opposing signals and a theoretical analysis of a two-layer linear network on a simple model. Our finding enables new qualitative predictions of training behavior which we confirm experimentally. It also provides a new lens through which to study and improve modern training practices for stochastic optimization, which we highlight via a case study of Adam versus SGD.
</details>
<details>
<summary>摘要</summary>
我们发现一种新现象在神经网络优化中， arise from 自然数据中的特殊重 tailed 结构和深度的交互作用。我们的结果提供了直觉关于 Several 已经报告的网络训练动态观察结果。特别是，它隐含了一个新的原因，进步的减须和稳定边缘;我们也发现了与其他优化和泛化概念相关的 grokking、简单偏好和 Sharpness-Aware Minimization。实验中，我们证明了训练数据中的对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对称对�
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Interpretable-Transformer-for-Fine-Grained-Image-Classification-and-Analysis"><a href="#A-Simple-Interpretable-Transformer-for-Fine-Grained-Image-Classification-and-Analysis" class="headerlink" title="A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis"></a>A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04157">http://arxiv.org/abs/2311.04157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/imageomics/intr">https://github.com/imageomics/intr</a></li>
<li>paper_authors: Dipanjyoti Paul, Arpita Chowdhury, Xinqi Xiong, Feng-Ju Chang, David Carlyn, Samuel Stevens, Kaiya Provost, Anuj Karpatne, Bryan Carstens, Daniel Rubenstein, Charles Stewart, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao</li>
<li>For: The paper is written for those interested in image classification and interpretability, specifically in using Transformers to make image classification more interpretable.* Methods: The paper uses a Transformer encoder-decoder architecture, inspired by DETR, to learn “class-specific” queries that allow each class to localize its patterns in an image via cross-attention.* Results: The paper shows that INTR intrinsically encourages each class to attend distinctively, and the cross-attention weights provide a faithful interpretation of the prediction. Additionally, the paper demonstrates that INTR can identify different “attributes” of a class, making it particularly suitable for fine-grained classification and analysis on eight datasets.<details>
<summary>Abstract</summary>
We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully-connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. We realize this idea via a Transformer encoder-decoder inspired by DEtection TRansformer (DETR). We learn ``class-specific'' queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. We name our approach INterpretable TRansformer (INTR), which is fairly easy to implement and exhibits several compelling properties. We show that INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Interestingly, via ``multi-head'' cross-attention, INTR could identify different ``attributes'' of a class, making it particularly suitable for fine-grained classification and analysis, which we demonstrate on eight datasets. Our code and pre-trained model are publicly accessible at https://github.com/Imageomics/INTR.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的应用方法，使用变换器来让图像分类变得可解释。不同于主流的分类器，我们在最后一层完全连接层之前就将类信息integrated到predictions中，而我们在这里investigate一种积极的方法，让每个类在图像中搜寻自己的特征。我们通过基于DEtection TRansformer（DETR）的Transformer encoder-decoder来实现这个想法，并学习每个类的“特定”的查询（一个查询对每个类），使每个类在图像中localize自己的特征viacross-attention。我们称之为可解释变换器（INTR），它易于实现并具有许多吸引人的性能。我们表明，INTR会自然地让每个类强调独特的注意力，因此交叉关注权重提供了可靠的预测解释。甚至via“多头”交叉关注，INTR可以识别不同的“特性”，使其特别适合细化分类和分析，我们在八个数据集上进行了示例。我们的代码和预训练模型可以在https://github.com/Imageomics/INTR上获取。
</details></li>
</ul>
<hr>
<h2 id="Contactless-Fingerprint-Biometric-Anti-Spoofing-An-Unsupervised-Deep-Learning-Approach"><a href="#Contactless-Fingerprint-Biometric-Anti-Spoofing-An-Unsupervised-Deep-Learning-Approach" class="headerlink" title="Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach"></a>Contactless Fingerprint Biometric Anti-Spoofing: An Unsupervised Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04148">http://arxiv.org/abs/2311.04148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Banafsheh Adami, Nima Karimian</li>
<li>For: 提高用户 COMFORT 和解决卫生问题，实现更高效的生物metric系统。* Methods: 使用无接触指纹识别技术，并将自动编码器和卷积束注意模块结合使用，以满足不同类型的投映攻击测试。* Results: 实现了0.96%的BPCER和1.6%的APCER，表明该方法可以有效地抵御不同类型的投映攻击。<details>
<summary>Abstract</summary>
Contactless fingerprint recognition offers a higher level of user comfort and addresses hygiene concerns more effectively. However, it is also more vulnerable to presentation attacks such as photo paper, paper-printout, and various display attacks, which makes it more challenging to implement in biometric systems compared to contact-based modalities. Limited research has been conducted on presentation attacks in contactless fingerprint systems, and these studies have encountered challenges in terms of generalization and scalability since both bonafide samples and presentation attacks are utilized during training model. Although this approach appears promising, it lacks the ability to handle unseen attacks, which is a crucial factor for developing PAD methods that can generalize effectively. We introduced an innovative anti-spoofing approach that combines an unsupervised autoencoder with a convolutional block attention module to address the limitations of existing methods. Our model is exclusively trained on bonafide images without exposure to any spoofed samples during the training phase. It is then evaluated against various types of presentation attack images in the testing phase. The scheme we proposed has achieved an average BPCER of 0.96\% with an APCER of 1.6\% for presentation attacks involving various types of spoofed samples.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Locating-Cross-Task-Sequence-Continuation-Circuits-in-Transformers"><a href="#Locating-Cross-Task-Sequence-Continuation-Circuits-in-Transformers" class="headerlink" title="Locating Cross-Task Sequence Continuation Circuits in Transformers"></a>Locating Cross-Task Sequence Continuation Circuits in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04131">http://arxiv.org/abs/2311.04131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Lan, Fazl Barez</li>
<li>for: 本研究旨在探讨 transformer 模型在语言任务中的强大能力，以及它们的复杂结构使得它们难以解释。</li>
<li>methods: 本研究使用了将 transformer 模型转换成可读的电路表示，以实现人类可读的算法函数。</li>
<li>results: 研究发现，相似序列续写任务中的电路具有共同的计算结构，这些结构在具体的序列中扮演着相似的角色。通过分析电路的技术，我们可以更好地预测模型的行为，识别错误和进行安全的编辑。这种机制性的理解对于建立更加稳定、一致和可解释的语言模型是非常重要的一步。<details>
<summary>Abstract</summary>
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.
</details>
<details>
<summary>摘要</summary>
transformer 模型在语言任务中表现出色，但它们的复杂架构使其难以解释。 recent work 旨在将 transformer 模型转换成可读的人类可读的表示形式，称为Circuit。我们在这些研究的基础上进一步分析和比较 Circuit 在相似序列续写任务中的表现，包括增加数字、数字词和月份。通过Circuit分析技术，我们确定了检测序列成员的关键子Circuit和预测下一个序列成员的Circuit。我们的分析发现，semantically related sequences 使用共享的Circuit子图，具有相似的角色。总的来说，记录共享的计算结构，可以更好地预测模型的行为，发现错误和安全地编辑过程。这种机制的理解是建立更加稳定、对接和可解释的语言模型的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-Safety-Vulnerabilities-of-Large-Language-Models"><a href="#Unveiling-Safety-Vulnerabilities-of-Large-Language-Models" class="headerlink" title="Unveiling Safety Vulnerabilities of Large Language Models"></a>Unveiling Safety Vulnerabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04124">http://arxiv.org/abs/2311.04124</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, Eitan Farchi</li>
<li>for: 这篇论文是为了研究大语言模型的可能的危险或不适应应答而设计的。</li>
<li>methods: 该论文使用了一个唯一的数据集，称为AttaQ，这些数据集包含了诱导大语言模型生成危险或不适应应答的问题。 authors还介绍了一种自动化的方法，可以识别和命名模型受到攻击后可能生成危险输出的输入 semantic area。</li>
<li>results: 该论文通过分析不同模型在AttaQ数据集上的漏洞，证明了AttaQ数据集的有用性。 authors还发现了一些可能导致模型生成危险输出的输入 semantic area，并通过特殊的聚类技术来识别这些区域。<details>
<summary>Abstract</summary>
As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern. This paper introduces a unique dataset containing adversarial examples in the form of questions, which we call AttaQ, designed to provoke such harmful or inappropriate responses. We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it. Additionally, we introduce a novel automatic approach for identifying and naming vulnerable semantic regions - input semantic areas for which the model is likely to produce harmful outputs. This is achieved through the application of specialized clustering techniques that consider both the semantic similarity of the input attacks and the harmfulness of the model's responses. Automatically identifying vulnerable semantic regions enhances the evaluation of model weaknesses, facilitating targeted improvements to its safety mechanisms and overall reliability.
</details>
<details>
<summary>摘要</summary>
As large language models become more prevalent, their possible harmful or inappropriate responses are a cause for concern. This paper introduces a unique dataset containing adversarial examples in the form of questions, which we call AttaQ, designed to provoke such harmful or inappropriate responses. We assess the efficacy of our dataset by analyzing the vulnerabilities of various models when subjected to it. Additionally, we introduce a novel automatic approach for identifying and naming vulnerable semantic regions - input semantic areas for which the model is likely to produce harmful outputs. This is achieved through the application of specialized clustering techniques that consider both the semantic similarity of the input attacks and the harmfulness of the model's responses. Automatically identifying vulnerable semantic regions enhances the evaluation of model weaknesses, facilitating targeted improvements to its safety mechanisms and overall reliability.Here's the translation in Traditional Chinese:随着大型语言模型的普及，它们可能会产生不适当或有害的回应，这篇论文提出了一个唯一的测试集，名为AttaQ，用于触发这些不适当或有害的回应。我们通过将不同模型应用到这个测试集上，进行模型的脆点分析。此外，我们还提出了一个新的自动方法，可以自动识别和命名模型中的具有危险性的Semantic Region，即输入Semantic Area中的具有危险性的输入攻击。这是通过特殊的集群技术，考虑了对Input攻击的Semantic similarity和模型的回应危险性。自动识别具有危险性的Semantic Region可以增强模型的评估，促进了模型的安全机制和整体可靠性。
</details></li>
</ul>
<hr>
<h2 id="ETDPC-A-Multimodality-Framework-for-Classifying-Pages-in-Electronic-Theses-and-Dissertations"><a href="#ETDPC-A-Multimodality-Framework-for-Classifying-Pages-in-Electronic-Theses-and-Dissertations" class="headerlink" title="ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations"></a>ETDPC: A Multimodality Framework for Classifying Pages in Electronic Theses and Dissertations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04262">http://arxiv.org/abs/2311.04262</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lamps-lab/ETDMiner">https://github.com/lamps-lab/ETDMiner</a></li>
<li>paper_authors: Muntabir Hasan Choudhury, Lamia Salsabil, William A. Ingram, Edward A. Fox, Jian Wu</li>
<li>for: 这个论文旨在为电子硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件硬件�<details>
<summary>Abstract</summary>
Electronic theses and dissertations (ETDs) have been proposed, advocated, and generated for more than 25 years. Although ETDs are hosted by commercial or institutional digital library repositories, they are still an understudied type of scholarly big data, partially because they are usually longer than conference proceedings and journals. Segmenting ETDs will allow researchers to study sectional content. Readers can navigate to particular pages of interest, discover, and explore the content buried in these long documents. Most existing frameworks on document page classification are designed for classifying general documents and perform poorly on ETDs. In this paper, we propose ETDPC. Its backbone is a two-stream multimodal model with a cross-attention network to classify ETD pages into 13 categories. To overcome the challenge of imbalanced labeled samples, we augmented data for minority categories and employed a hierarchical classifier. ETDPC outperforms the state-of-the-art models in all categories, achieving an F1 of 0.84 -- 0.96 for 9 out of 13 categories. We also demonstrated its data efficiency. The code and data can be found on GitHub (https://github.com/lamps-lab/ETDMiner/tree/master/etd_segmentation).
</details>
<details>
<summary>摘要</summary>
电子论文和硬件（ETD）已经被提议、推广和生成了超过25年。尽管ETD被主机商业或机构数字图书馆存储，但它们仍然是未经研究的学术大数据的一种，其中一部分原因是它们通常比会议论文和杂志长得多。 segmenting ETDs 可以让研究人员研究 sectional content。读者可以导航到特定页码的兴趣，发现和探索ETD中埋藏的内容。现有的文档页面分类框架都是为普通文档设计，对ETD表现不佳。在这篇论文中，我们提出ETDPC。它的核心是一个两�ream multimodal模型，包括一个 crossed attention网络，用于分类ETD页面到13类中。为了解决少数类样本的权重不均问题，我们增强了数据，并使用堆式分类器。ETDPC在所有类别中都超过了状态之前的模型，实现了 F1 0.84-0.96 的值。我们还证明了它的数据效率。代码和数据可以在 GitHub 上找到（https://github.com/lamps-lab/ETDMiner/tree/master/etd_segmentation）。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Multimodal-Prompted-Training-for-Interactive-Embodied-Task-Completion"><a href="#Multitask-Multimodal-Prompted-Training-for-Interactive-Embodied-Task-Completion" class="headerlink" title="Multitask Multimodal Prompted Training for Interactive Embodied Task Completion"></a>Multitask Multimodal Prompted Training for Interactive Embodied Task Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04067">http://arxiv.org/abs/2311.04067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georgios Pantazopoulos, Malvina Nikandrou, Amit Parekh, Bhathiya Hemanthage, Arash Eshghi, Ioannis Konstas, Verena Rieser, Oliver Lemon, Alessandro Suglia</li>
<li>for: 本研究旨在解决现有视觉语言（VL）模型中的两个基本挑战，即将语言脱离到行为和观察的轨迹中，以及参照扩展。</li>
<li>methods: 我们提出了一个多模态代理（EMMA），它是一个统一的编码器-解码器模型，可以对图像和轨迹进行理解，并将动作预测转换为多模态文本生成。通过将所有任务视为文本生成，EMMA学习了一种行为语言，从而实现了任务之间的传送。与之前的模块化方法不同，我们使用了一个单一的多任务模型，每个任务都会帮助完成目标。</li>
<li>results: EMMA在多个VL准 benchmark上表现了与类似模型一样的性能，并在对话导向任务完成率（DTC）上达到了新的州OF-the-art表现（36.81%成功率）。<details>
<summary>Abstract</summary>
Interactive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNInteractive and embodied tasks pose at least two fundamental challenges to existing Vision & Language (VL) models, including 1) grounding language in trajectories of actions and observations, and 2) referential disambiguation. To tackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a unified encoder-decoder model that reasons over images and trajectories, and casts action prediction as multimodal text generation. By unifying all tasks as text generation, EMMA learns a language of actions which facilitates transfer across tasks. Different to previous modular approaches with independently trained components, we use a single multitask model where each task contributes to goal completion. EMMA performs on par with similar models on several VL benchmarks and sets a new state-of-the-art performance (36.81% success rate) on the Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided agents in the Alexa Arena.中文简体版：存在至少两个基本挑战对现有视觉语言（VL）模型：1）将语言融入行为和观察的轨迹中，和2）参照歧义。为解决这些挑战，我们提出了一个具有多modal的Embodied MultiModal Agent（EMMA）：一个统一的编码器-解码器模型，可以对图像和轨迹进行理解，并将动作预测转换为多modal文本生成。通过将所有任务都作为文本生成，EMMA学习了一种动作语言，从而促进了任务之间的传递。与之前独立训练的组件不同，我们使用了一个单一的多任务模型，每个任务都可以帮助完成目标。EMMA在多个VL标准准测上与类似模型表现相当，并在Dialog-guided Task Completion（DTC）标准准测上设置了新的状态纪录（36.81%成功率），这是一个用于评估对话导向Agent的Alexa Arena中的 benchmark。
</details></li>
</ul>
<hr>
<h2 id="Can-CLIP-Help-Sound-Source-Localization"><a href="#Can-CLIP-Help-Sound-Source-Localization" class="headerlink" title="Can CLIP Help Sound Source Localization?"></a>Can CLIP Help Sound Source Localization?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04066">http://arxiv.org/abs/2311.04066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020">https://github.com/chrisneagu/FTC-Skystone-Dark-Angels-Romania-2020</a></li>
<li>paper_authors: Sooyoung Park, Arda Senocak, Joon Son Chung</li>
<li>for: 本研究应用大规模预训Image-text模型（specifically CLIP）到 зву源定位领域，并不需要文本输入。</li>
<li>methods: 我们引入一个框架，将音频讯号转换为CLIP的文本Encoder兼容的token，从而生成音频驱动的Embeddings。我们直接使用这些Embeddings，生成音频驱动的面积图，提取音频驱动的图像特征，并使用音频视觉对匹配目标来对它们进行对齐。</li>
<li>results: 我们的方法比过去的方法表现出更好的完整性和紧密性，并且在实验中表现出了优秀的性能。<details>
<summary>Abstract</summary>
Large-scale pre-trained image-text models demonstrate remarkable versatility across diverse tasks, benefiting from their robust representational capabilities and effective multimodal alignment. We extend the application of these models, specifically CLIP, to the domain of sound source localization. Unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.
</details>
<details>
<summary>摘要</summary>
大规模预训练图像文本模型在多种任务上显示了惊人的多样性，受到其强大的表示能力和有效的多媒体对应的推动。我们将这些模型，特别是CLIP，应用到声源 localization 领域中。 unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.Here's the translation in Traditional Chinese:大规模预训练图像文本模型在多种任务上显示了惊人的多样性，受到其强大的表示能力和有效的多媒体对应的推动。我们将这些模型，特别是CLIP，应用到对话 localization 领域中。 unlike conventional approaches, we employ the pre-trained CLIP model without explicit text input, relying solely on the audio-visual correspondence. To this end, we introduce a framework that translates audio signals into tokens compatible with CLIP's text encoder, yielding audio-driven embeddings. By directly using these embeddings, our method generates audio-grounded masks for the provided audio, extracts audio-grounded image features from the highlighted regions, and aligns them with the audio-driven embeddings using the audio-visual correspondence objective. Our findings suggest that utilizing pre-trained image-text models enable our model to generate more complete and compact localization maps for the sounding objects. Extensive experiments show that our method outperforms state-of-the-art approaches by a significant margin.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Causal-Representation-Learning-with-Partial-Observability"><a href="#Multi-View-Causal-Representation-Learning-with-Partial-Observability" class="headerlink" title="Multi-View Causal Representation Learning with Partial Observability"></a>Multi-View Causal Representation Learning with Partial Observability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04056">http://arxiv.org/abs/2311.04056</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingling Yao, Danru Xu, Sébastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kügelgen, Francesco Locatello</li>
<li>for: 这 paper 旨在研究同时观察到的视图中学习的表示性的可识别性。</li>
<li>methods: 该 paper 使用了对各个视图的强制学习和单一编码器来学习表示性。</li>
<li>results: 该 paper 提供了一个统一的框架和理论结论，可以在部分可见情况下学习多视图非线性混合的下标。它们还提供了一些图理论来判断哪些隐藏变量可以通过简单的规则来标识。<details>
<summary>Abstract</summary>
We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities. We allow a partially observed setting in which each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets. Further, we demonstrate that the performance of prior methods is recovered in different special cases of our setup. Overall, we find that access to multiple partial views enables us to identify a more fine-grained representation, under the generally milder assumption of partial observability.
</details>
<details>
<summary>摘要</summary>
我们提出一个统一的框架，用于研究同时观察到的视图中学习的表示可否被识别。我们允许部分观察的设置，在每个视图中，每个下标变量可能是非线性混合的一部分。我们证明了，通过对所有视图的共享信息进行对比学习，可以在单个编码器每个视图中学习到所有下标变量的信息，并且这种学习可以保持一定的平滑比例。我们还提供了一些图形标准，用于判断哪些含义可以通过简单的规则来识别，我们称之为可识别代数。我们的总框架和理论结果综合和扩展了多视图非线性ICA、解体和 causal 表示学习的先前研究。我们在数学、图像和多模态数据集上进行了实验 validate 我们的laims，并且我们发现，在不同的特殊情况下，先前的方法的性能可以通过我们的框架来回归。总之，我们发现，通过访问多个部分视图，可以 identific a 更细致的表示，只需要在通常较弱的部分可见性假设下进行学习。
</details></li>
</ul>
<hr>
<h2 id="Causal-Discovery-Under-Local-Privacy"><a href="#Causal-Discovery-Under-Local-Privacy" class="headerlink" title="Causal Discovery Under Local Privacy"></a>Causal Discovery Under Local Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04037">http://arxiv.org/abs/2311.04037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rūta Binkytė, Carlos Pinzón, Szilvia Lestyán, Kangsoo Jung, Héber H. Arcolezi, Catuscia Palamidessi</li>
<li>For: 本研究旨在探讨本地隐私机制在 causal discovery 任务中的选择。* Methods: 本文考虑了多种常见的本地隐私机制，并对这些机制对 causal learning 算法生成的 causal 结构的准确性和隐私水平进行了比较。* Results: 研究发现，不同的本地隐私机制会对 causal 结构生成的准确性产生不同的影响，而且这些影响可能是负面的。本文提供了关于选择合适的本地隐私协议的有价值信息。<details>
<summary>Abstract</summary>
Differential privacy is a widely adopted framework designed to safeguard the sensitive information of data providers within a data set. It is based on the application of controlled noise at the interface between the server that stores and processes the data, and the data consumers. Local differential privacy is a variant that allows data providers to apply the privatization mechanism themselves on their data individually. Therefore it provides protection also in contexts in which the server, or even the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility of the data, particularly by distorting the correlations between individual data components. This distortion can prove detrimental to tasks such as causal discovery. In this paper, we consider various well-known locally differentially private mechanisms and compare the trade-off between the privacy they provide, and the accuracy of the causal structure produced by algorithms for causal learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee that our findings will aid researchers and practitioners in conducting locally private causal discovery.
</details>
<details>
<summary>摘要</summary>
differential privacy 是一种广泛采用的框架，旨在保护数据提供者在数据集中的敏感信息。它基于数据处理和存储服务器和数据消费者之间应用控制的噪声的方法。本地异 diferencial privacy 是一种变体，允许数据提供者自己应用隐私机制于其数据。因此，它可以在服务器或数据收集者不可靠的情况下提供保护。噪声的引入，然而，必然影响数据的使用价值，特别是对个体数据组件之间的相关性进行扭曲。这种扭曲可能对 causal discovery 任务构成负面影响。在这篇论文中，我们考虑了多种广泛known的本地异 diferencial privacy 机制，并比较这些机制对 privacy 和 causal learning 算法应用于扭曲后的数据中的准确性的负面。我们的分析获得了有价值的洞察，可以帮助研究者和实践者在本地私有的 causal discovery 任务中选择合适的异 diferencial privacy 协议。我们预期，我们的发现将帮助研究者和实践者在本地私有的 causal discovery 任务中进行更加有效的工作。
</details></li>
</ul>
<hr>
<h2 id="Impact-of-HPO-on-AutoML-Forecasting-Ensembles"><a href="#Impact-of-HPO-on-AutoML-Forecasting-Ensembles" class="headerlink" title="Impact of HPO on AutoML Forecasting Ensembles"></a>Impact of HPO on AutoML Forecasting Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04034">http://arxiv.org/abs/2311.04034</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Hoffmann</li>
<li>for: 这个论文的目的是提出一种基于多种优化策略的深度学习模型 ensemble，用于解决多种问题的地方和全球一元预测。</li>
<li>methods: 该论文使用了多种方法，包括MQ-CNN、DeepAR、Prophet、NPTS、ARIMA和ETS，以及不同的超参数优化策略，如搜索和权重学习。</li>
<li>results: 研究发现，在这种设置中，添加超参数优化可以提高准确性，最终设置的准确性提升9.9%，相比于基eline ensemble无HPO。此外，该组合还比州Of The Art的商业自动学习预测解决方案Amazon Forecast下降3.5%的错误率和16.0%的综合集成延迟时间。<details>
<summary>Abstract</summary>
A forecasting ensemble consisting of a diverse range of estimators for both local and global univariate forecasting, in particular MQ-CNN,DeepAR, Prophet, NPTS, ARIMA and ETS, can be used to make forecasts for a variety of problems. This paper delves into the aspect of adding different hyperparameter optimization strategies to the deep learning models in such a setup (DeepAR and MQ-CNN), exploring the trade-off between added training cost and the increase in accuracy for different configurations. It shows that in such a setup, adding hyperparameter optimization can lead to performance improvements, with the final setup having a 9.9 % percent accuracy improvement with respect to the avg-wQL over the baseline ensemble without HPO, accompanied by a 65.8 % increase in end-to-end ensemble latency. This improvement is based on an empirical analysis of combining the ensemble pipeline with different tuning strategies, namely Bayesian Optimisation and Hyperband and different configurations of those strategies. In the final configuration, the proposed combination of ensemble learning and HPO outperforms the state of the art commercial AutoML forecasting solution, Amazon Forecast, with a 3.5 % lower error and 16.0 % lower end-to-end ensemble latency.
</details>
<details>
<summary>摘要</summary>
一个包含多种估计器的预测集群，包括MQ-CNN、DeepAR、Prophet、NPTS、ARIMA和ETS，可以用于解决多种问题的预测。这篇论文探讨了将不同的超参数优化策略添加到深度学习模型中的影响，包括加入超参数优化后 ensemble 的性能提升。结果显示，在这种情况下，加入超参数优化可以提高准确性，最终集群与avg-wQL比较基eline ensemble无HPO后提高9.9%，同时end-to-end ensemble延迟提高65.8%。这个提升基于不同的 ensemble 管道和优化策略的实际分析，包括 Bayesian Optimization 和 Hyperband。最终配置中，提档 ensemble 学习和HPO的组合超过了现有的商业AutoML预测解决方案 Amazon Forecast，错误率下降3.5%，ensemble 延迟下降16.0%。
</details></li>
</ul>
<hr>
<h2 id="IoT-Based-Environmental-Control-System-for-Fish-Farms-with-Sensor-Integration-and-Machine-Learning-Decision-Support"><a href="#IoT-Based-Environmental-Control-System-for-Fish-Farms-with-Sensor-Integration-and-Machine-Learning-Decision-Support" class="headerlink" title="IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support"></a>IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04258">http://arxiv.org/abs/2311.04258</a></li>
<li>repo_url: None</li>
<li>paper_authors: D. Dhinakaran, S. Gopalakrishnan, M. D. Manigandan, T. P. Anish</li>
<li>For: 本研究旨在开发一个基于物联网（IoT）的环境控制系统，帮助推动鱼塘的可持续发展。* Methods: 本研究使用了无线感应器网络，进行了实时环境数据的收集和预测，并应用了四种机器学习算法：Random Forests、Support Vector Machines（SVMs）、Gradient Boosting Machines（GBMs）和神经网络。* Results: 本研究发现，这个环境控制系统可以帮助鱼塘的环境条件与预定的条件保持一致，提高鱼的健康和生产力，同时降低资源浪费和环境影响，实现可持续的鱼塘经营。<details>
<summary>Abstract</summary>
In response to the burgeoning global demand for seafood and the challenges of managing fish farms, we introduce an innovative IoT based environmental control system that integrates sensor technology and advanced machine learning decision support. Deploying a network of wireless sensors within the fish farm, we continuously collect real-time data on crucial environmental parameters, including water temperature, pH levels, humidity, and fish behavior. This data undergoes meticulous preprocessing to ensure its reliability, including imputation, outlier detection, feature engineering, and synchronization. At the heart of our system are four distinct machine learning algorithms: Random Forests predict and optimize water temperature and pH levels for the fish, fostering their health and growth; Support Vector Machines (SVMs) function as an early warning system, promptly detecting diseases and parasites in fish; Gradient Boosting Machines (GBMs) dynamically fine-tune the feeding schedule based on real-time environmental conditions, promoting resource efficiency and fish productivity; Neural Networks manage the operation of critical equipment like water pumps and heaters to maintain the desired environmental conditions within the farm. These machine learning algorithms collaboratively make real-time decisions to ensure that the fish farm's environmental conditions align with predefined specifications, leading to improved fish health and productivity while simultaneously reducing resource wastage, thereby contributing to increased profitability and sustainability. This research article showcases the power of data-driven decision support in fish farming, promising to meet the growing demand for seafood while emphasizing environmental responsibility and economic viability, thus revolutionizing the future of fish farming.
</details>
<details>
<summary>摘要</summary>
为了应对全球增长的海鲜需求以及护养鱼场的挑战，我们介绍了一个创新的互联网器件（IoT）基础的环境控制系统，该系统集成了感知技术和高级机器学习决策支持。在鱼场中部署了无线传感器网络，持续收集鱼场实时环境参数的数据，包括水温、pH值、湿度和鱼Behavior。这些数据进行了严格的处理和验证，以确保其可靠性，包括填充、异常检测、特征工程和同步。针对鱼场环境控制，我们采用了四种不同的机器学习算法：Random Forests预测和优化鱼场水温和pH值，以促进鱼的健康和生长；Support Vector Machines（SVMs）作为早期警报系统，快速检测鱼中的疾病和寄生虫；Gradient Boosting Machines（GBMs）动态调整饲料时间表，根据实时环境条件，提高鱼场资源效率和鱼产量；Neural Networks控制鱼场关键设备，如水泵和加热器，以保持鱼场所需环境条件，从而确保鱼场环境控制的正确性。这些机器学习算法合作实时决策，以确保鱼场环境控制与预定标准符合，从而提高鱼的健康和产量，同时减少资源浪费，从而提高利润和可持续性。本文显示了数据驱动的决策支持在鱼养中的力量，承诺满足全球增长的海鲜需求，同时强调环境责任和经济可持续性，以改变未来的鱼养业。
</details></li>
</ul>
<hr>
<h2 id="Expressivity-of-ReLU-Networks-under-Convex-Relaxations"><a href="#Expressivity-of-ReLU-Networks-under-Convex-Relaxations" class="headerlink" title="Expressivity of ReLU-Networks under Convex Relaxations"></a>Expressivity of ReLU-Networks under Convex Relaxations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04015">http://arxiv.org/abs/2311.04015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Baader, Mark Niklas Müller, Yuhao Mao, Martin Vechev</li>
<li>for: 论文主要探讨了训练和证明具有可证明安全性的神经网络所使用的凸 relaxation 技术的限制。</li>
<li>methods: 作者使用了多种常用的凸 relaxation 技术进行研究，包括 IBP relaxation 和更高级的 relaxation。</li>
<li>results: 研究发现，使用更高级的 relaxation 技术可以更好地表示一些不可达的函数，但是对于多变量函数来说，无论使用最精确的单 neuron relaxation，都无法构建可 precisely 分析的 ReLU 网络。<details>
<summary>Abstract</summary>
Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work investigating this question focused on the simple and widely used IBP relaxation. It revealed that some univariate, convex, continuous piecewise linear (CPWL) functions cannot be encoded by any ReLU network such that its IBP-analysis is precise. To explore whether this limitation is shared by more advanced convex relaxations, we conduct the first in-depth study on the expressive power of ReLU networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable ReLU networks, (ii) more precise relaxations can allow exponentially larger solution spaces of ReLU networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable ReLU networks that express multivariate, convex, monotone CPWL functions.
</details>
<details>
<summary>摘要</summary>
“凸松动是训练和证明可靠神经网络的关键组成部分。然而，尽管已经取得了 significativ progress，但是还存在一个宽泛不了解的精度差异，这引起了whether this is due to fundamental limitations of convex relaxations的 вопро题。初步的研究发现，IBP relaxation可以不能正确地分析一些单变量、凸、连续piecewise linear（CPWL）函数。为了检查这种限制是否被更高级的凸松动所共享，我们进行了第一次对所有常用凸松动中ReLU网络的表达力进行深入研究。我们发现了以下结论：（i）更高级的松动可以让更多的单变量函数被ReLU网络表示为正确分析的，（ii）更精度的松动可以让ReLU网络表示的解空间规模增加 exponential，（iii）即使使用最精度的单 neuron松动，也不可以构建ReLU网络来表示多变量、凸、 monotone CPWL函数。”
</details></li>
</ul>
<hr>
<h2 id="A-Method-to-Improve-the-Performance-of-Reinforcement-Learning-Based-on-the-Y-Operator-for-a-Class-of-Stochastic-Differential-Equation-Based-Child-Mother-Systems"><a href="#A-Method-to-Improve-the-Performance-of-Reinforcement-Learning-Based-on-the-Y-Operator-for-a-Class-of-Stochastic-Differential-Equation-Based-Child-Mother-Systems" class="headerlink" title="A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems"></a>A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04014">http://arxiv.org/abs/2311.04014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Yin, Yi Chen</li>
<li>for: 提高actor-critic(AC)基于游戏学习的控制性能，用于解决由随机 diffeq Equations(SDEs) governs的系统。</li>
<li>methods:  introduce a novel operator called Y operator，integrate child-mother system的随机性 into critic network的损失函数，提高RL算法的控制性能。</li>
<li>results: Y operator reformulates solving partial differential equations for state-value function as a parallel problem for drift and diffusion functions within SDEs, demonstrates superiority over existing methods through linear and nonlinear numerical examples.<details>
<summary>Abstract</summary>
This paper introduces a novel operator, termed the Y operator, to elevate control performance in Actor-Critic(AC) based reinforcement learning for systems governed by stochastic differential equations(SDEs). The Y operator ingeniously integrates the stochasticity of a class of child-mother system into the Critic network's loss function, yielding substantial advancements in the control performance of RL algorithms.Additionally, the Y operator elegantly reformulates the challenge of solving partial differential equations for the state-value function into a parallel problem for the drift and diffusion functions within the system's SDEs.A rigorous mathematical proof confirms the operator's validity.This transformation enables the Y Operator-based Reinforcement Learning(YORL) framework to efficiently tackle optimal control problems in both model-based and data-driven systems.The superiority of YORL is demonstrated through linear and nonlinear numerical examples showing its enhanced performance over existing methods post convergence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Energy-Prediction-Smart-Meter-Dataset-Analysis-of-Previous-Competitions-and-Beyond"><a href="#The-Energy-Prediction-Smart-Meter-Dataset-Analysis-of-Previous-Competitions-and-Beyond" class="headerlink" title="The Energy Prediction Smart-Meter Dataset: Analysis of Previous Competitions and Beyond"></a>The Energy Prediction Smart-Meter Dataset: Analysis of Previous Competitions and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04007">http://arxiv.org/abs/2311.04007</a></li>
<li>repo_url: None</li>
<li>paper_authors: Direnc Pekaslan, Jose Maria Alonso-Moral, Kasun Bandara, Christoph Bergmeir, Juan Bernabe-Moreno, Robert Eigenmann, Nils Einecke, Selvi Ergen, Rakshitha Godahewa, Hansika Hewamalage, Jesus Lago, Steffen Limmer, Sven Rebhan, Boris Rabinovich, Dilini Rajapasksha, Heda Song, Christian Wagner, Wenlong Wu, Luis Magdalena, Isaac Triguero</li>
<li>for: 这 paper 主要是为了提供一个实际世界智能仪表数据集，并对 Energy Prediction Technical Challenges 中的解决方案进行分析，特别是在 2020 年 IEEE Computational Intelligence Society（IEEE-CIS）技术挑战（名为 EP）和 2021 年 IEEE International Conference on Fuzzy Systems（FUZZ-IEEE）的跟进挑战（名为 XEP）中。</li>
<li>methods: 这 paper 使用了几种方法，包括 Energy Prediction 技术挑战中的竞赛方法，以及对实际世界智能仪表数据集的分析和评价。</li>
<li>results: 这 paper 的结果显示，通过使用不同的方法和技术，可以准确预测家庭能源消耗，并且可以提高预测的解释性。此外，paper 还提出了一些可能的应用场景，例如能源分解、需求回应计划和行为改变等。<details>
<summary>Abstract</summary>
This paper presents the real-world smart-meter dataset and offers an analysis of solutions derived from the Energy Prediction Technical Challenges, focusing primarily on two key competitions: the IEEE Computational Intelligence Society (IEEE-CIS) Technical Challenge on Energy Prediction from Smart Meter data in 2020 (named EP) and its follow-up challenge at the IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) in 2021 (named as XEP). These competitions focus on accurate energy consumption forecasting and the importance of interpretability in understanding the underlying factors. The challenge aims to predict monthly and yearly estimated consumption for households, addressing the accurate billing problem with limited historical smart meter data. The dataset comprises 3,248 smart meters, with varying data availability ranging from a minimum of one month to a year. This paper delves into the challenges, solutions and analysing issues related to the provided real-world smart meter data, developing accurate predictions at the household level, and introducing evaluation criteria for assessing interpretability. Additionally, this paper discusses aspects beyond the competitions: opportunities for energy disaggregation and pattern detection applications at the household level, significance of communicating energy-driven factors for optimised billing, and emphasising the importance of responsible AI and data privacy considerations. These aspects provide insights into the broader implications and potential advancements in energy consumption prediction. Overall, these competitions provide a dataset for residential energy research and serve as a catalyst for exploring accurate forecasting, enhancing interpretability, and driving progress towards the discussion of various aspects such as energy disaggregation, demand response programs or behavioural interventions.
</details>
<details>
<summary>摘要</summary>
The dataset includes 3,248 smart meters with varying data availability, ranging from one month to one year. This paper explores the challenges, solutions, and evaluation criteria for assessing interpretability in the provided real-world smart meter data. Additionally, it discusses opportunities for energy disaggregation and pattern detection applications at the household level, the significance of communicating energy-driven factors for optimized billing, and the importance of responsible AI and data privacy considerations. These aspects provide insights into the broader implications and potential advancements in energy consumption prediction.Overall, these competitions provide a dataset for residential energy research and serve as a catalyst for exploring accurate forecasting, enhancing interpretability, and driving progress towards discussions of various aspects, such as energy disaggregation, demand response programs, or behavioral interventions.
</details></li>
</ul>
<hr>
<h2 id="Foundational-propositions-of-hesitant-fuzzy-sets-and-parameter-reductions-of-hesitant-fuzzy-information-systems"><a href="#Foundational-propositions-of-hesitant-fuzzy-sets-and-parameter-reductions-of-hesitant-fuzzy-information-systems" class="headerlink" title="Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems"></a>Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04256">http://arxiv.org/abs/2311.04256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shizhan Lu</li>
<li>for: 本研究探讨了不确定和半确定的情况下的软集的定义和应用。</li>
<li>methods: 本文提出了基于不确定软集的分割的多种包含关系，并提出了软集系统的基本定义和家族。</li>
<li>results: 本文提出了基于不确定软集的参数缩放问题的一些基本定理和算法，并给出了一个示例和算法来解释过程。<details>
<summary>Abstract</summary>
Hesitant fuzzy sets are widely used in the instances of uncertainty and hesitation. The inclusion relationship is an important and foundational definition for sets. Hesitant fuzzy set, as a kind of set, needs explicit definition of inclusion relationship. Base on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed. And then some foundational propositions of hesitant fuzzy sets and the families of hesitant fuzzy sets are presented. Finally, some foundational propositions of hesitant fuzzy information systems with respect to parameter reductions are put forward, and an example and an algorithm are given to illustrate the processes of parameter reductions.
</details>
<details>
<summary>摘要</summary>
“抽象不确定集”在不确定和犹豫的场景中广泛使用。确定关系是确定集的基本定义。“抽象不确定集”作为一种集合，需要明确的确定关系定义。基于抽象不确定会员度的离散形式，对抽象不确定集提出了多种包含关系。然后，对抽象不确定集和其家族进行了一些基本提示，并对不确定信息系统参数缩小进行了一些基本提示。最后，给出了一个示例和一个算法，以示 parameter reductions 的过程。Note: "抽象不确定集" (hesitant fuzzy set) is a term used in fuzzy set theory to describe a set with unclear or uncertain boundaries. The term "不确定" (uncertain) is used to emphasize the uncertainty of the set's boundaries, rather than the traditional "抽象" (abstract) used in the term "抽象集" (abstract set).
</details></li>
</ul>
<hr>
<h2 id="Human-AI-Collaboration-in-Thematic-Analysis-using-ChatGPT-A-User-Study-and-Design-Recommendations"><a href="#Human-AI-Collaboration-in-Thematic-Analysis-using-ChatGPT-A-User-Study-and-Design-Recommendations" class="headerlink" title="Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations"></a>Human-AI Collaboration in Thematic Analysis using ChatGPT: A User Study and Design Recommendations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03999">http://arxiv.org/abs/2311.03999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lixiang Yan, Vanessa Echeverria, Gloria Fernandez Nieto, Yueqiao Jin, Zachari Swiecki, Linxuan Zhao, Dragan Gašević, Roberto Martinez-Maldonado</li>
<li>for: 这些研究旨在了解研究者与生成人工智能（GenAI）在质量研究中的合作方式。</li>
<li>methods: 研究者使用了ChatGPT进行主题分析，并发现其可以提高编码效率，帮助初步数据探索，提供细腻的量化预测，以及帮助非native语言speaker和非专家理解。</li>
<li>results: 研究发现了GenAI在质量研究中的价值，但也存在不信任和精度、可靠性和一致性的问题，以及更广泛的研究社区的acceptance。Here’s the full text in Simplified Chinese:</li>
<li>for: 这些研究旨在了解研究者与生成人工智能（GenAI）在质量研究中的合作方式。</li>
<li>methods: 研究者使用了ChatGPT进行主题分析，并发现其可以提高编码效率，帮助初步数据探索，提供细腻的量化预测，以及帮助非native语言speaker和非专家理解。</li>
<li>results: 研究发现了GenAI在质量研究中的价值，但也存在不信任和精度、可靠性和一致性的问题，以及更广泛的研究社区的acceptance。Please note that the text is in Simplified Chinese, and the translation may not be perfect.<details>
<summary>Abstract</summary>
Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers' perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exploration, offering granular quantitative insights, and assisting comprehension for non-native speakers and non-experts. Yet, concerns about its trustworthiness and accuracy, reliability and consistency, limited contextual understanding, and broader acceptance within the research community persist. We contribute five actionable design recommendations to foster effective human-AI collaboration. These include incorporating transparent explanatory mechanisms, enhancing interface and integration capabilities, prioritising contextual understanding and customisation, embedding human-AI feedback loops and iterative functionality, and strengthening trust through validation mechanisms.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Learned-Causal-Method-Prediction"><a href="#Learned-Causal-Method-Prediction" class="headerlink" title="Learned Causal Method Prediction"></a>Learned Causal Method Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03989">http://arxiv.org/abs/2311.03989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shantanu Gupta, Cheng Zhang, Agrin Hilmkil</li>
<li>for: 这个论文是为了解决选择适合的 causal inference 方法问题。</li>
<li>methods: 这个论文使用了 CAusal Method Predictor (CAMP) 框架，该框架可以预测最佳方法 для给定的数据集。</li>
<li>results: CAMP 可以高效地预测最佳方法，并且在实验中表现出色，可以普遍应用于不同的数据集。<details>
<summary>Abstract</summary>
For a given causal question, it is important to efficiently decide which causal inference method to use for a given dataset. This is challenging because causal methods typically rely on complex and difficult-to-verify assumptions, and cross-validation is not applicable since ground truth causal quantities are unobserved. In this work, we propose CAusal Method Predictor (CAMP), a framework for predicting the best method for a given dataset. To this end, we generate datasets from a diverse set of synthetic causal models, score the candidate methods, and train a model to directly predict the highest-scoring method for that dataset. Next, by formulating a self-supervised pre-training objective centered on dataset assumptions relevant for causal inference, we significantly reduce the need for costly labeled data and enhance training efficiency. Our strategy learns to map implicit dataset properties to the best method in a data-driven manner. In our experiments, we focus on method prediction for causal discovery. CAMP outperforms selecting any individual candidate method and demonstrates promising generalization to unseen semi-synthetic and real-world benchmarks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化字的中文。<</SYS>>为给定的 causal 问题，选择合适的 causal inference 方法是关键。这是因为 causal 方法通常基于复杂且难以验证的假设，而 cross-validation 不适用，因为真实的 causal 量未知。在这种情况下，我们提出了 CAusal Method Predictor (CAMP)，一个框架用于预测给定数据集最佳的方法。为此，我们生成了一系列来自多种 sintetic causal 模型的数据集，评分候选方法，并使用一个模型直接预测该数据集中最高分方法。然后，我们通过在 dataset 假设上定义自我超vised 预训练目标，以减少高效的标注数据的需求，并提高训练效率。我们的策略学习将数据集的隐藏特性映射到最佳方法中，以数据驱动的方式。在我们的实验中，我们关注 causal discovery 方法预测。CAMP 在选择任何个人候选方法时表现出优异，并在未见 semi-synthetic 和实际世界 benchmark 中展现了良好的普适性。
</details></li>
</ul>
<hr>
<h2 id="Its-All-Graph-To-Me-Foundational-Topology-Models-with-Contrastive-Learning-on-Multiple-Domains"><a href="#Its-All-Graph-To-Me-Foundational-Topology-Models-with-Contrastive-Learning-on-Multiple-Domains" class="headerlink" title="Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains"></a>Its All Graph To Me: Foundational Topology Models with Contrastive Learning on Multiple Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03976">http://arxiv.org/abs/2311.03976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex O. Davies, Riku W. Green, Nirav S. Ajmeri, Telmo M. Silva Filho</li>
<li>for: 这篇论文的目的是提出一种基于对抗对抗学习的图数据表示和嵌入模型，以解决现有模型是域特定的问题。</li>
<li>methods: 这篇论文使用了对抗对抗学习方法，只在结构上进行了训练，而不是在数据上进行训练。</li>
<li>results: 论文表明，使用这种方法可以获得比单域或非预训练模型更好的表示，并且在多种下游任务中表现更好。<details>
<summary>Abstract</summary>
Representations and embeddings of graph data have been essential in many domains of research.   The principle benefit of learning such representations is that the pre-trained model can be fine-tuned on smaller datasets where data or labels are scarse.   Existing models, however, are domain specific; for example a model trained on molecular graphs is fine-tuned on other molecular graphs.   This means that in many application cases the choice of pre-trained model can be arbitrary, and novel domains may lack an appropriate pre-trained model.   This is of particular issue where data is scarse, precluding traditional supervised methods.   In this work we use adversarial contrastive learning to present a \method, a model pre-trained on many graph domains.   We train the model only on topologies but include node labels in evaluation.   We evaluate the efficacy of its learnt representations on various downstream tasks.   Against baseline models pre-trained on single domains, as well as un-trained models and non-transferred models, we show that performance is equal or better using our single model.   This includes when node labels are used in evaluation, where performance is consistently superior to single-domain or non-pre-trained models.
</details>
<details>
<summary>摘要</summary>
研究领域中的图数据表示和嵌入已经是不可或缺的。这些表示的主要优点是可以通过先经训练的模型进行精度调整，以便在数据或标签稀缺的情况下进行学习。现有的模型却是域特定的，例如一个基于分子图的模型只能在其他分子图上进行精度调整。这意味着在许多应用场景中，选择预训练模型的问题可能是随意的，而新的域可能缺乏适当的预训练模型。这对于数据稀缺的情况是特别问题。在这种情况下，我们使用对抗性强化学习方法来提出一种\method，一个在多个图域上预训练的模型。我们只在结构上训练这个模型，而不包括节点标签在评估中。我们对这种学习得到的表示进行评估，并与基eline模型、未经训练的模型和非转移模型进行比较。我们发现，使用我们的单一模型，表示性能与基eline模型或未经训练的模型相当或更好，尤其是在节点标签被用于评估时。
</details></li>
</ul>
<hr>
<h2 id="An-Expectation-Realization-Model-for-Metaphor-Detection"><a href="#An-Expectation-Realization-Model-for-Metaphor-Detection" class="headerlink" title="An Expectation-Realization Model for Metaphor Detection"></a>An Expectation-Realization Model for Metaphor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03963">http://arxiv.org/abs/2311.03963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oseremen O. Uduehi, Razvan C. Bunescu</li>
<li>for: 本研究旨在提出一种基于两个主要模块的μETD概念检测架构，以优化μETD的表达能力。</li>
<li>methods: 该架构包括一个期望组件，用于估计上下文中Literal word的表达，以及一个现实组件，用于计算上下文中Actual word的表达。整个架构通过学习期望-现实（ER）模式来学习μETD的概念用语。</li>
<li>results: 对于三个μETD数据集（ dentro分布、外部分布和新型μETD泛化）的评估，提出的方法能够取得与现有状态艺术的或更好的结果。此外，通过ER模型 ensemble的方式，进一步提高μETD检测精度。<details>
<summary>Abstract</summary>
We propose a metaphor detection architecture that is structured around two main modules: an expectation component that estimates representations of literal word expectations given a context, and a realization component that computes representations of actual word meanings in context. The overall architecture is trained to learn expectation-realization (ER) patterns that characterize metaphorical uses of words. When evaluated on three metaphor datasets for within distribution, out of distribution, and novel metaphor generalization, the proposed method is shown to obtain results that are competitive or better than state-of-the art. Further increases in metaphor detection accuracy are obtained through ensembling of ER models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于两个主要模块的比喻检测架构：一个预期部分，用于在上下文中计算 Literal 词意 Representatives，以及一个实现部分，用于在上下文中计算实际词义 Representatives。整个架构在学习预期-实现（ER）模式上进行训练，以捕捉比喻用语的含义。在三个比喻数据集上进行评估，我们的方法在 dentro 分布、out of distribution 和新比喻泛化方面具有竞争或更好的 результаados。进一步的增加比喻检测精度可以通过 ER 模型的ensemble。Note: " dentro 分布" means "within distribution" in Chinese, "out of distribution" means "外部分布" in Chinese, and "novel metaphor generalization" means "新比喻泛化" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Elastic-Information-Bottleneck"><a href="#Elastic-Information-Bottleneck" class="headerlink" title="Elastic Information Bottleneck"></a>Elastic Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03955">http://arxiv.org/abs/2311.03955</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nyyxxx/elastic-information-bottleneck">https://github.com/nyyxxx/elastic-information-bottleneck</a></li>
<li>paper_authors: Yuyan Ni, Yanyan Lan, Ao Liu, Zhiming Ma</li>
<li>for: 这篇论文的目的是解释深度学习算法中的表示机制，以及两种信息瓶颈方法（IB和DIB）的泛化能力。</li>
<li>methods: 这篇论文使用了两种信息瓶颈方法（IB和DIB），并对它们进行了 theoretically和实验性的分析。</li>
<li>results: 研究发现，IB和DIB在不同的泛化场景下的表现不同，而EIB可以在这些场景下实现更好的泛化效果。<details>
<summary>Abstract</summary>
Information bottleneck is an information-theoretic principle of representation learning that aims to learn a maximally compressed representation that preserves as much information about labels as possible. Under this principle, two different methods have been proposed, i.e., information bottleneck (IB) and deterministic information bottleneck (DIB), and have gained significant progress in explaining the representation mechanisms of deep learning algorithms. However, these theoretical and empirical successes are only valid with the assumption that training and test data are drawn from the same distribution, which is clearly not satisfied in many real-world applications. In this paper, we study their generalization abilities within a transfer learning scenario, where the target error could be decomposed into three components, i.e., source empirical error, source generalization gap (SG), and representation discrepancy (RD). Comparing IB and DIB on these terms, we prove that DIB's SG bound is tighter than IB's while DIB's RD is larger than IB's. Therefore, it is difficult to tell which one is better. To balance the trade-off between SG and the RD, we propose an elastic information bottleneck (EIB) to interpolate between the IB and DIB regularizers, which guarantees a Pareto frontier within the IB framework. Additionally, simulations and real data experiments show that EIB has the ability to achieve better domain adaptation results than IB and DIB, which validates the correctness of our theories.
</details>
<details>
<summary>摘要</summary>
信息瓶颈是一种信息理论的学习原理，旨在学习最紧凑的表示，保持标签信息的最多。在这个原理下，两种不同的方法得到了提案，即信息瓶颈（IB）和决定性信息瓶颈（DIB），在解释深度学习算法的表示机制方面取得了显著进步。然而，这些理论和实验成功假设训练和测试数据是从同一个分布中采样，这并不符合现实世界中许多应用场景。在这篇论文中，我们研究IB和DIB在转移学习场景中的泛化能力，将目标错误分解为三个组分，即源Empirical error、源泛化差（SG）和表示差（RD）。对比IB和DIB，我们证明DIB的SG bound更紧，而DIB的RD更大。因此，无法判断哪一个更好。为了平衡SG和RD之间的负担，我们提出了灵活信息瓶颈（EIB）来 interpolate IB和DIB正则化， garantía IB frameworks 中的Pareto frontier。此外，实验和实际数据表明，EIB可以在适应领域上达到IB和DIB所不能达到的更好的结果，这证明了我们的理论的正确性。
</details></li>
</ul>
<hr>
<h2 id="The-Music-Meta-Ontology-a-flexible-semantic-model-for-the-interoperability-of-music-metadata"><a href="#The-Music-Meta-Ontology-a-flexible-semantic-model-for-the-interoperability-of-music-metadata" class="headerlink" title="The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata"></a>The Music Meta Ontology: a flexible semantic model for the interoperability of music metadata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03942">http://arxiv.org/abs/2311.03942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacopo de Berardinis, Valentina Anita Carriero, Albert Meroño-Peñuela, Andrea Poltronieri, Valentina Presutti</li>
<li>for: 这篇论文的目的是为了创建一个基于semantic metadata的音乐数据集，以便进行信息检索和知识发现。</li>
<li>methods: 该论文使用了extreme设计方法和数据工程学最佳实践，以满足不同参与者（音乐学家、图书馆员、数据工程师等）的需求，并采用ontology设计模式和证据跟踪。</li>
<li>results: 该论文介绍了Music Meta ontology，一个rich和flexible的semantic模型，用于描述音乐元数据，包括艺术家、作品、演奏、录音等方面。同时，论文还进行了首次评估和其他schema（Music Ontology、DOREMUS、Wikidata）的对接，以及数据转换的支持。<details>
<summary>Abstract</summary>
The semantic description of music metadata is a key requirement for the creation of music datasets that can be aligned, integrated, and accessed for information retrieval and knowledge discovery. It is nonetheless an open challenge due to the complexity of musical concepts arising from different genres, styles, and periods -- standing to benefit from a lingua franca to accommodate various stakeholders (musicologists, librarians, data engineers, etc.). To initiate this transition, we introduce the Music Meta ontology, a rich and flexible semantic model to describe music metadata related to artists, compositions, performances, recordings, and links. We follow eXtreme Design methodologies and best practices for data engineering, to reflect the perspectives and the requirements of various stakeholders into the design of the model, while leveraging ontology design patterns and accounting for provenance at different levels (claims, links). After presenting the main features of Music Meta, we provide a first evaluation of the model, alignments to other schema (Music Ontology, DOREMUS, Wikidata), and support for data transformation.
</details>
<details>
<summary>摘要</summary>
《音乐元数据 semantics 描述是创建可以协调、集成和搜索信息的音乐集成数据的关键要求。然而，这是一个开放的挑战，因为音乐概念来自不同的流派、风格和时期，具有不同的复杂性。为了解决这个问题，我们介绍了音乐元 ontology，一种rich和灵活的semantic模型，用于描述音乐元数据相关的艺术家、作品、表演、录音和链接。我们遵循extreme设计方法和数据工程学的最佳实践，将各种参与者的视角和需求反映到模型的设计中，同时采用ontology设计模式和考虑多维 provinicial（声明、链接）。文章后续介绍了音乐元的主要特点，与其他架构（音乐 ontology、DOREMUS、Wikidata）的对alignment，以及数据转换的支持。
</details></li>
</ul>
<hr>
<h2 id="Everything-of-Thoughts-Defying-the-Law-of-Penrose-Triangle-for-Thought-Generation"><a href="#Everything-of-Thoughts-Defying-the-Law-of-Penrose-Triangle-for-Thought-Generation" class="headerlink" title="Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation"></a>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04254">http://arxiv.org/abs/2311.04254</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang</li>
<li>For: The paper aims to enhance the capabilities of Large Language Models (LLMs) by introducing a novel thought prompting approach called “Everything of Thoughts” (XoT) to improve their ability to generalize to unseen problems efficiently.* Methods: The approach leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, and autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions.* Results: The approach enables LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions.Here’s the simplified Chinese text for the three information points:* For: 这篇论文目的是提高大语言模型（LLM）的能力，通过引入“Everything of Thoughts”（XoT）思维推动方法，使其更好地处理未经见过的问题。* Methods: XoT方法利用预训练的奖励学习和 Monte Carlo Tree Search（MCTS），将外部领域知识integrated into思维，并通过LLM自动生成高质量的全面认知地图，减少LLM与人类交互。* Results: XoT方法使LLM可以进行不受限制的思考，允许它们对多个解决方案进行灵活的认知映射。<details>
<summary>Abstract</summary>
Recent advancements in Large Language Models (LLMs) have revolutionized decision-making by breaking down complex problems into more manageable language sequences referred to as ``thoughts''. An effective thought design should consider three key perspectives: performance, efficiency, and flexibility. However, existing thought can at most exhibit two of these attributes. To address these limitations, we introduce a novel thought prompting approach called ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle of existing thought paradigms. XoT leverages pretrained reinforcement learning and Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge into thoughts, thereby enhancing LLMs' capabilities and enabling them to generalize to unseen problems efficiently. Through the utilization of the MCTS-LLM collaborative thought revision framework, this approach autonomously produces high-quality comprehensive cognitive mappings with minimal LLM interactions. Additionally, XoT empowers LLMs to engage in unconstrained thinking, allowing for flexible cognitive mappings for problems with multiple solutions.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:最近的大语言模型（LLMs）革命化了决策，将复杂问题分解成更容易处理的语言序列，称为“思想”。一个有效的思想设计应考虑三个关键方面：性能、效率和灵活性。然而，现有的思想只能展现两个特征。为了解决这些限制，我们介绍了一种新的思想推荐方法called“everything of thoughts”（XoT），以推翻现有思想 парадигмы的“彭罗斯三角形法则”。XoT利用预训练的奖励学习和Monte Carlo Tree Search（MCTS），将外部领域知识 integrate into thoughts，从而提高LLMs的能力和通用性。通过MCTS-LLM共同思想修订框架，这种方法可以自动生成高质量的完整认知地图，并且减少LLM的交互次数。此外，XoT赋予LLMs无约束的思维能力，允许它们为多解问题生成灵活的认知地图。
</details></li>
</ul>
<hr>
<h2 id="MixtureGrowth-Growing-Neural-Networks-by-Recombining-Learned-Parameters"><a href="#MixtureGrowth-Growing-Neural-Networks-by-Recombining-Learned-Parameters" class="headerlink" title="MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters"></a>MixtureGrowth: Growing Neural Networks by Recombining Learned Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04251">http://arxiv.org/abs/2311.04251</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/mixturegrowth">https://github.com/chaudatascience/mixturegrowth</a></li>
<li>paper_authors: Chau Pham, Piotr Teterwak, Soren Nelson, Bryan A. Plummer</li>
<li>for: 这 paper 是为了解决深度神经网络在不同网络结构下进行训练的问题，以及在提高网络大小时不需要从 scratch 重新训练的问题。</li>
<li>methods: 这 paper 使用了 MixtureGrowth 方法，这是一种基于 linear combination 的方法，通过将每层的 parameter templates 组合成新的 linear combination，以生成新层的 weight。这种方法可以减少 initialize 过程中的噪声，并且可以充分利用已经学习过的 weight。</li>
<li>results: 这 paper 的实验结果表明，MixtureGrowth 方法可以在 CIFAR-100 和 ImageNet  datasets 上提高 top-1 准确率，而且与 fewer FLOPs 的情况下和一个从 scratch 训练的大网络相比，性能相似。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/chaudatascience/mixturegrowth">https://github.com/chaudatascience/mixturegrowth</a> 上获取。<details>
<summary>Abstract</summary>
Most deep neural networks are trained under fixed network architectures and require retraining when the architecture changes. If expanding the network's size is needed, it is necessary to retrain from scratch, which is expensive. To avoid this, one can grow from a small network by adding random weights over time to gradually achieve the target network size. However, this naive approach falls short in practice as it brings too much noise to the growing process. Prior work tackled this issue by leveraging the already learned weights and training data for generating new weights through conducting a computationally expensive analysis step. In this paper, we introduce MixtureGrowth, a new approach to growing networks that circumvents the initialization overhead in prior work. Before growing, each layer in our model is generated with a linear combination of parameter templates. Newly grown layer weights are generated by using a new linear combination of existing templates for a layer. On one hand, these templates are already trained for the task, providing a strong initialization. On the other, the new coefficients provide flexibility for the added layer weights to learn something new. We show that our approach boosts top-1 accuracy over the state-of-the-art by 2-2.5% on CIFAR-100 and ImageNet datasets, while achieving comparable performance with fewer FLOPs to a larger network trained from scratch. Code is available at https://github.com/chaudatascience/mixturegrowth.
</details>
<details>
<summary>摘要</summary>
大多数深度神经网络在固定网络架构下训练，需要重新训练当网络架构发生变化时。如果需要扩大网络的大小，则需要从头开始训练，这是昂贵的。为了避免这个问题，一些先前的方法是通过逐渐添加随机权重来慢慢地实现目标网络大小。然而，这种简单的方法在实践中失败了，因为它会带来太多的噪声。先前的工作是通过使用已经学习过的权重和训练数据来生成新的权重，进行计算昂贵的分析步骤。在这篇论文中，我们介绍了 MixtureGrowth，一种新的网络增长方法，可以绕过先前的初始化开销。在我们的模型中，每个层都是通过线性组合参数模板来生成。新增的层权重是通过使用新的线性组合已有层的参数模板来生成的。一方面，这些模板已经被训练了任务，可以提供强大的初始化。另一方面，新的系数提供了对新增层权重进行学习的灵活性。我们表明，我们的方法可以在CIFAR-100和ImageNet datasets上提高顶部1的准确率，相比之下，需要更多的FLOPs来训练一个从头开始的更大的网络。代码可以在https://github.com/chaudatascience/mixturegrowth上找到。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Graph-Representation-Learning-with-Adaptive-Augmentation-Contrastive"><a href="#Temporal-Graph-Representation-Learning-with-Adaptive-Augmentation-Contrastive" class="headerlink" title="Temporal Graph Representation Learning with Adaptive Augmentation Contrastive"></a>Temporal Graph Representation Learning with Adaptive Augmentation Contrastive</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03897">http://arxiv.org/abs/2311.03897</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongjiang Chen, Pengfei Jiao, Huijun Tang, Huaming Wu</li>
<li>for: 本文旨在提出一种Temporal Graph representation learning方法，用于生成低维度的动态节点嵌入，以捕捉时间信息以及结构和属性信息。</li>
<li>methods: 本方法使用 adaptive augmentation contrastive 对时间图进行增强，并定义了扩展的对比目标函数，以适应时间依赖的噪声。</li>
<li>results: 对多个实际网络进行了广泛的实验，并证明了该方法能够超过其他时间图表示学习方法。<details>
<summary>Abstract</summary>
Temporal graph representation learning aims to generate low-dimensional dynamic node embeddings to capture temporal information as well as structural and property information. Current representation learning methods for temporal networks often focus on capturing fine-grained information, which may lead to the model capturing random noise instead of essential semantic information. While graph contrastive learning has shown promise in dealing with noise, it only applies to static graphs or snapshots and may not be suitable for handling time-dependent noise. To alleviate the above challenge, we propose a novel Temporal Graph representation learning with Adaptive augmentation Contrastive (TGAC) model. The adaptive augmentation on the temporal graph is made by combining prior knowledge with temporal information, and the contrastive objective function is constructed by defining the augmented inter-view contrast and intra-view contrast. To complement TGAC, we propose three adaptive augmentation strategies that modify topological features to reduce noise from the network. Our extensive experiments on various real networks demonstrate that the proposed model outperforms other temporal graph representation learning methods.
</details>
<details>
<summary>摘要</summary>
现代 temporal graph 表示学习的目标是生成低维度的动态节点嵌入，以捕捉 temporal 信息以及结构和属性信息。现有的 temporal network 表示学习方法通常强调细化信息，可能导致模型捕捉Random 噪音而不是重要的semantic信息。而graph contrastive learning 已经在 dealing  with noise 方面表现出了 promise,但它只适用于静止图或快照，可能不适用于处理时间相关的噪音。为了解决这一挑战，我们提出了一种新的 Temporal Graph 表示学习 with Adaptive augmentation Contrastive (TGAC) 模型。在 TGAC 模型中，我们通过结合先前知识与时间信息来实现可变的扩充，并定义了扩充后的对比对象函数。此外，我们还提出了三种适应的扩充策略，可以通过修改 topological 特征来减少网络中的噪音。我们对多种实际网络进行了广泛的实验，结果显示，我们提出的模型在 temporal graph 表示学习方法中占据了优势。
</details></li>
</ul>
<hr>
<h2 id="Unifying-Structure-and-Language-Semantic-for-Efficient-Contrastive-Knowledge-Graph-Completion-with-Structured-Entity-Anchors"><a href="#Unifying-Structure-and-Language-Semantic-for-Efficient-Contrastive-Knowledge-Graph-Completion-with-Structured-Entity-Anchors" class="headerlink" title="Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors"></a>Unifying Structure and Language Semantic for Efficient Contrastive Knowledge Graph Completion with Structured Entity Anchors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04250">http://arxiv.org/abs/2311.04250</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Je, Wontae Choi, Kwangjin Oh</li>
<li>for: 这篇论文的目的是提出一种能够有效地结合结构信息和语言 semantics的方法，以提高知识图（KG）Completion 的性能。</li>
<li>methods: 本文提出的方法使用了预训练语言模型（PLM），并将结构信息和语言表示相结合，以学习一种统一的表示。此外，该方法还使用了随机负样本，可以在每个小批量中进行对照学习，以学习一种通用的实体表示。</li>
<li>results: 经过多种实验和分析，本文证明了该方法在标准的链接预测任务中的表现，超过了现有的最佳知识图完成（KGC）模型。尤其是在 FB15K-237 上，该方法的表现和结构基础 KGC 方法相当。<details>
<summary>Abstract</summary>
The goal of knowledge graph completion (KGC) is to predict missing links in a KG using trained facts that are already known. In recent, pre-trained language model (PLM) based methods that utilize both textual and structural information are emerging, but their performances lag behind state-of-the-art (SOTA) structure-based methods or some methods lose their inductive inference capabilities in the process of fusing structure embedding to text encoder. In this paper, we propose a novel method to effectively unify structure information and language semantics without losing the power of inductive reasoning. We adopt entity anchors and these anchors and textual description of KG elements are fed together into the PLM-based encoder to learn unified representations. In addition, the proposed method utilizes additional random negative samples which can be reused in the each mini-batch during contrastive learning to learn a generalized entity representations. We verify the effectiveness of the our proposed method through various experiments and analysis. The experimental results on standard benchmark widely used in link prediction task show that the proposed model outperforms existing the SOTA KGC models. Especially, our method show the largest performance improvement on FB15K-237, which is competitive to the SOTA of structure-based KGC methods.
</details>
<details>
<summary>摘要</summary>
目标是完成知识图（KG），预训练语言模型（PLM）基本方法可以利用文本和结构信息，但表现落后于现有的结构基本方法或一些方法在结构嵌入文本编码器时失去了推理能力。在这篇论文中，我们提出了一种新的方法，可以有效地融合结构信息和语言 semantics，不失 induction 推理能力。我们采用实体锚点，并将 KG 元素的文本描述和实体锚点一起 fed 到 PLM 基本编码器中，以学习统一表示。此外，我们还使用随机负样本，可以在每个 mini-batch 中重复使用，以在对比学习中学习通用实体表示。我们通过多种实验和分析证明了方法的有效性。实验结果表明，我们提出的方法在标准的链接预测任务中表现出色，尤其是在 FB15K-237 上，与结构基本方法具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Tool-Discovery-and-Tool-Innovation-Using-Active-Inference"><a href="#Understanding-Tool-Discovery-and-Tool-Innovation-Using-Active-Inference" class="headerlink" title="Understanding Tool Discovery and Tool Innovation Using Active Inference"></a>Understanding Tool Discovery and Tool Innovation Using Active Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03893">http://arxiv.org/abs/2311.03893</a></li>
<li>repo_url: None</li>
<li>paper_authors: Poppy Collis, Paul F Kinghorn, Christopher L Buckley</li>
<li>for: 本研究旨在探讨人工智能代理者如何创造新工具，以便在动态和新环境中解决问题。</li>
<li>methods: 本研究使用活动推断 formalism 进行工具发现和工具创新的分类，并在代理者的潜在生成模型中引入工具可用性的概念，以便通过离线推导实现工具的发明。</li>
<li>results: 本研究预示了在潜在生成模型中引入工具可用性的概念可以使代理者不仅发现工具，而且还能够创造新的工具。<details>
<summary>Abstract</summary>
The ability to invent new tools has been identified as an important facet of our ability as a species to problem solve in dynamic and novel environments. While the use of tools by artificial agents presents a challenging task and has been widely identified as a key goal in the field of autonomous robotics, far less research has tackled the invention of new tools by agents. In this paper, (1) we articulate the distinction between tool discovery and tool innovation by providing a minimal description of the two concepts under the formalism of active inference. We then (2) apply this description to construct a toy model of tool innovation by introducing the notion of tool affordances into the hidden states of the agent's probabilistic generative model. This particular state factorisation facilitates the ability to not just discover tools but invent them through the offline induction of an appropriate tool property. We discuss the implications of these preliminary results and outline future directions of research.
</details>
<details>
<summary>摘要</summary>
人类的问题解决能力中，发明新工具的能力被认为是一项重要的特征。虽然人工智能代理人使用工具是一项复杂的任务，但是对于代理人发明新工具的研究远未得到广泛的探讨。在这篇论文中，我们（1）将工具发现和工具创新两个概念进行了明确的分别，通过活动推断 formalism 的最小描述。然后我们（2）将这个描述应用到构建了一个简单的工具创新模型，通过引入工具可用性的概念来扩展代理人的生成模型中的隐藏状态。这种状态分解使得代理人不仅能够发现工具，还能通过离线推导出适当的工具性质来创造新的工具。我们讨论了这些初步结果的意义和未来研究的方向。
</details></li>
</ul>
<hr>
<h2 id="Formulating-Discrete-Probability-Flow-Through-Optimal-Transport"><a href="#Formulating-Discrete-Probability-Flow-Through-Optimal-Transport" class="headerlink" title="Formulating Discrete Probability Flow Through Optimal Transport"></a>Formulating Discrete Probability Flow Through Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03886">http://arxiv.org/abs/2311.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pangzecheung/discrete-probability-flow">https://github.com/pangzecheung/discrete-probability-flow</a></li>
<li>paper_authors: Pengze Zhang, Hubery Yin, Chen Li, Xiaohua Xie</li>
<li>for: 本研究旨在建立抽象扩散模型中的概率流动理论。</li>
<li>methods: 我们首先证明了连续概率流动是 Монже优化运输图的某种情况，并在相应的条件下提供了对应的证明。然后，我们定义了离散概率流动，并基于这些定义提出了一种新的采样方法。</li>
<li>results: 我们通过对synthetic toy dataset和CIFAR-10 dataset的广泛实验 validated了我们提出的离散概率流动的效果。代码可以在<a target="_blank" rel="noopener" href="https://github.com/PangzeCheung/Discrete-Probability-Flow%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/PangzeCheung/Discrete-Probability-Flow中找到。</a><details>
<summary>Abstract</summary>
Continuous diffusion models are commonly acknowledged to display a deterministic probability flow, whereas discrete diffusion models do not. In this paper, we aim to establish the fundamental theory for the probability flow of discrete diffusion models. Specifically, we first prove that the continuous probability flow is the Monge optimal transport map under certain conditions, and also present an equivalent evidence for discrete cases. In view of these findings, we are then able to define the discrete probability flow in line with the principles of optimal transport. Finally, drawing upon our newly established definitions, we propose a novel sampling method that surpasses previous discrete diffusion models in its ability to generate more certain outcomes. Extensive experiments on the synthetic toy dataset and the CIFAR-10 dataset have validated the effectiveness of our proposed discrete probability flow. Code is released at: https://github.com/PangzeCheung/Discrete-Probability-Flow.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译为简化中文。<</SYS>>连续扩散模型通常被认为展示束定概率流，而离散扩散模型则不然。在这篇论文中，我们目标是建立离散概率流的基本理论。 Specifically，我们首先证明了连续概率流是在某些条件下的蒙日最优运输地图，并同时提供了对应的离散情况证明。基于这些发现，我们然后可以定义离散概率流，与最优运输原理相符。最后，我们基于我们 newly established definitions，提出了一种新的采样方法，能够更加准确地生成结果。 extend 的实验在 sintetic 玩具数据集和 CIFAR-10 数据集上验证了我们的提议的离散概率流的效果。 Code 可以在：https://github.com/PangzeCheung/Discrete-Probability-Flow 中找到。
</details></li>
</ul>
<hr>
<h2 id="Mini-but-Mighty-Finetuning-ViTs-with-Mini-Adapters"><a href="#Mini-but-Mighty-Finetuning-ViTs-with-Mini-Adapters" class="headerlink" title="Mini but Mighty: Finetuning ViTs with Mini Adapters"></a>Mini but Mighty: Finetuning ViTs with Mini Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03873">http://arxiv.org/abs/2311.03873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iemprog/mimi">https://github.com/iemprog/mimi</a></li>
<li>paper_authors: Imad Eddine Marouf, Enzo Tartaglione, Stéphane Lathuilière</li>
<li>for: 这个论文主要是为了提出一种能够减少精度转移学习的批处理方法，以提高计算机视觉任务中的模型表现。</li>
<li>methods: 这个论文使用了一种名为MiMi的培训框架，该框架可以自动调整精度转移学习中adapter的维度，以达到最佳的质量和精度之间的平衡。</li>
<li>results: 根据实验结果，MiMi方法可以在3个数据集上 benchmark（DomainNet、VTAB和Multi-task）上对29个数据集进行最佳的质量和精度之间的平衡。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have become one of the dominant architectures in computer vision, and pre-trained ViT models are commonly adapted to new tasks via fine-tuning. Recent works proposed several parameter-efficient transfer learning methods, such as adapters, to avoid the prohibitive training and storage cost of finetuning. In this work, we observe that adapters perform poorly when the dimension of adapters is small, and we propose MiMi, a training framework that addresses this issue. We start with large adapters which can reach high performance, and iteratively reduce their size. To enable automatic estimation of the hidden dimension of every adapter, we also introduce a new scoring function, specifically designed for adapters, that compares the neuron importance across layers. Our method outperforms existing methods in finding the best trade-off between accuracy and trained parameters across the three dataset benchmarks DomainNet, VTAB, and Multi-task, for a total of 29 datasets.
</details>
<details>
<summary>摘要</summary>
视Transformers（ViTs）已经成为计算机视觉领域的主导体系，而预训练ViT模型通常通过精度调整来适应新任务。最近的工作提出了一些精度效率的传输学习方法，如适配器，以避免训练和存储成本过高。在这种情况下，我们发现适配器的维度很小时表现不佳，我们提出了MiMi，一个培训框架，解决这个问题。我们从大的适配器开始，然后逐渐减小它们，以达到高性能。为了自动估计每个适配器的隐藏维度，我们还引入了一个专门设计 для适配器的新评分函数，用于比较层次中的神经元重要性。我们的方法在DomainNet、VTAB和多任务三个数据集上的29个数据集上超过了现有方法，在找到最佳的准确率和训练参数之间的折衔。
</details></li>
</ul>
<hr>
<h2 id="FD-MIA-Efficient-Attacks-on-Fairness-enhanced-Models"><a href="#FD-MIA-Efficient-Attacks-on-Fairness-enhanced-Models" class="headerlink" title="FD-MIA: Efficient Attacks on Fairness-enhanced Models"></a>FD-MIA: Efficient Attacks on Fairness-enhanced Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03865">http://arxiv.org/abs/2311.03865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huan Tian, Guangsheng Zhang, Bo Liu, Tianqing Zhu, Ming Ding, Wanlei Zhou</li>
<li>for: 这个研究旨在测试满足平等需求的模型对于特定子集的攻击敏感性。</li>
<li>methods: 该研究使用了分别对于原始模型和平等增强模型进行预测分析，以探索攻击者可能对于这些模型发动的攻击方法。</li>
<li>results: 研究发现，对于平等增强模型，攻击者无法成功发动攻击，因为这些模型在预测分析中具有较高的防护性。同时，该研究发现，平等方法通常会导致训练资料中主要子集的预测性下降，这使得攻击更加困难且预测距离增加。建基于这些见解，该研究提出了一种高效的攻击方法，基于平等违背结果（FD-MIA），并考虑了防止隐私泄露的策略。实验结果证明了这些发现和方法的有效性。<details>
<summary>Abstract</summary>
Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon these insights, we propose an efficient MIA method against fairness-enhanced models based on fairness discrepancy results (FD-MIA). It leverages the difference in the predictions from both the original and fairness-enhanced models and exploits the observed prediction gaps as attack clues. We also explore potential strategies for mitigating privacy leakages. Extensive experiments validate our findings and demonstrate the efficacy of the proposed method.
</details>
<details>
<summary>摘要</summary>
Our research shows that these score-based MIAs are not effective against fairness-enhanced models in binary classifications. The attack models become too simple and do not work well. We also find that fairness methods can lead to a decrease in prediction performance for the majority group in the training data. This makes it harder for the attacker to succeed and increases the difference between the predictions for members and non-members.Based on these findings, we propose a new method for membership inference attacks (MIAs) against fairness-enhanced models. Our method uses the difference in predictions from the original and fairness-enhanced models to identify potential members. We also explore ways to reduce the risk of privacy leaks.We conducted extensive experiments to test our method and found that it is effective in identifying members. Our results show that the proposed method can be used to launch successful membership inference attacks against fairness-enhanced models.
</details></li>
</ul>
<hr>
<h2 id="Aspects-of-human-memory-and-Large-Language-Models"><a href="#Aspects-of-human-memory-and-Large-Language-Models" class="headerlink" title="Aspects of human memory and Large Language Models"></a>Aspects of human memory and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03839">http://arxiv.org/abs/2311.03839</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmldj/memory-llm-paper">https://github.com/rmldj/memory-llm-paper</a></li>
<li>paper_authors: Romuald A. Janik</li>
<li>for: 这个论文 investigate LLMs 的内存特性，了解它们是如何模拟人类内存的。</li>
<li>methods: 该论文使用 LLMs 来生成文本，并通过分析其内存特性来探讨人类内存的特征。</li>
<li>results: 研究发现，LLMs 的内存特性与人类内存有 surprisingly 的相似之处，这表明人类内存的特征对文本结构产生了深刻的影响。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are huge artificial neural networks which primarily serve to generate text, but also provide a very sophisticated probabilistic model of language use. Since generating a semantically consistent text requires a form of effective memory, we investigate the memory properties of LLMs and find surprising similarities with key characteristics of human memory. This result strongly suggests that the biological features of human memory leave an imprint on the way that we structure our textual narratives.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Reducing-Spatial-Fitting-Error-in-Distillation-of-Denoising-Diffusion-Models"><a href="#Reducing-Spatial-Fitting-Error-in-Distillation-of-Denoising-Diffusion-Models" class="headerlink" title="Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models"></a>Reducing Spatial Fitting Error in Distillation of Denoising Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03830">http://arxiv.org/abs/2311.03830</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sainzerjj/SFERD">https://github.com/Sainzerjj/SFERD</a></li>
<li>paper_authors: Shengzhe Zhou, Zejian Lee, Shengyuan Zhang, Lefan Hou, Changyuan Yang, Guang Yang, Lingyun Sun</li>
<li>For: 增强 diffusion models 的图像生成质量* Methods: 使用 attention 导航和设计的 semantics 梯度预测器来减少学生模型的适应错误* Results: 实验表明，我们提出的模型可以在几个函数评估中生成高质量图像，FID 值为 5.31 和 9.39，超越现有的扩散方法。<details>
<summary>Abstract</summary>
Denoising Diffusion models have exhibited remarkable capabilities in image generation. However, generating high-quality samples requires a large number of iterations. Knowledge distillation for diffusion models is an effective method to address this limitation with a shortened sampling process but causes degraded generative quality. Based on our analysis with bias-variance decomposition and experimental observations, we attribute the degradation to the spatial fitting error occurring in the training of both the teacher and student model. Accordingly, we propose $\textbf{S}$patial $\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction $\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD utilizes attention guidance from the teacher model and a designed semantic gradient predictor to reduce the student's fitting error. Empirically, our proposed model facilitates high-quality sample generation in a few function evaluations. We achieve an FID of 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\times$64 with only one step, outperforming existing diffusion methods. Our study provides a new perspective on diffusion distillation by highlighting the intrinsic denoising ability of models.
</details>
<details>
<summary>摘要</summary>
difang de denoising diffusion models 有非常出色的� image generation 能力。然而，生成高质量样本需要很多迭代。知识储存法为 diffusion models 是一种有效的方法，可以缩短样本生成过程，但会导致生成质量下降。根据我们的分析和实验观察，我们认为这种下降是在教师和学生模型的训练中发生的空间适应错误。因此，我们提出了 $\textbf{S}$patial $\textbf{F}$itting-$\textbf{E}$rror $\textbf{R}$eduction $\textbf{D}$istillation model ($\textbf{SFERD}$). SFERD 使用教师模型的注意力引导和设计的 semantics 梯度预测器来减少学生模型的适应错误。我们的提出的模型可以在几个函数评估中生成高质量样本，我们在 CIFAR-10 和 ImageNet 64$\times$64 上达到了 FID 5.31 和 9.39，比现有的扩散方法更高。我们的研究提供了一个新的� diffusion distillation 的视角，强调模型的内在杂净能力。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-and-Improving-Multi-task-Learning-for-End-to-end-Speech-Translation"><a href="#Rethinking-and-Improving-Multi-task-Learning-for-End-to-end-Speech-Translation" class="headerlink" title="Rethinking and Improving Multi-task Learning for End-to-end Speech Translation"></a>Rethinking and Improving Multi-task Learning for End-to-end Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03810">http://arxiv.org/abs/2311.03810</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaozhang521/imtl">https://github.com/xiaozhang521/imtl</a></li>
<li>paper_authors: Yuhao Zhang, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, Jingbo Zhu</li>
<li>for: 这篇论文主要用于探讨多任务学习在端到端语音翻译（ST）中的应用，以及这种方法在ST任务中是否真正有助于提高翻译质量。</li>
<li>methods: 这篇论文使用了多任务学习方法，包括文本编码器和语音编码器之间的交互，以及对不同时间和模块进行研究。</li>
<li>results: 研究发现，文本编码器主要帮助实现跨Modal的转换，但是在语音中存在噪声会降低语音和文本表示之间的一致性。 authors也提出了一种改进的多任务学习方法（IMTL），可以减轻模式差异和表示差异，从而提高翻译质量。 experiments在MuST-C数据集上进行，结果显示，我们的方法可以达到领先的 result，而且在添加更多数据后，我们的方法在MuST-C英语到西班牙语任务上达到了新的SOTA result，只用20.8%的训练时间。<details>
<summary>Abstract</summary>
Significant improvements in end-to-end speech translation (ST) have been achieved through the application of multi-task learning. However, the extent to which auxiliary tasks are highly consistent with the ST task, and how much this approach truly helps, have not been thoroughly studied. In this paper, we investigate the consistency between different tasks, considering different times and modules. We find that the textual encoder primarily facilitates cross-modal conversion, but the presence of noise in speech impedes the consistency between text and speech representations. Furthermore, we propose an improved multi-task learning (IMTL) approach for the ST task, which bridges the modal gap by mitigating the difference in length and representation. We conduct experiments on the MuST-C dataset. The results demonstrate that our method attains state-of-the-art results. Moreover, when additional data is used, we achieve the new SOTA result on MuST-C English to Spanish task with 20.8% of the training time required by the current SOTA method.
</details>
<details>
<summary>摘要</summary>
significan mejora en la traducción de speech a extremo (ST) se ha logrado a través de la aprendizaje de tareas múltiples. Sin embargo, no se ha estudiado en profundidad el extent to which las tareas auxiliares son consistentes con la tarea ST, y cómo realmente ayudan. En este artículo, investigamos la consistencia entre diferentes tareas, considerando diferentes momentos y módulos. Encontramos que el encoder textual principal facilita la conversión cross-modal, pero la presencia de ruido en el speech impide la consistencia entre las representaciones de texto y speech. Además, propusimos un enfoque de aprendizaje de múltiples tareas mejorado (IMTL) para la tarea ST, que reduce la brecha modal al mitigar la diferencia de longitud y representación. Realizamos experimentos en el conjunto de datos MuST-C. Los resultados demuestran que nuestro método logra resultados estatales del arte. Además, cuando se utiliza más datos, podemos alcanzar un nuevo resultado de SOTA en la tarea de inglés a español de MuST-C con solo el 20.8% del tiempo de entrenamiento requerido por el método SOTA actual.
</details></li>
</ul>
<hr>
<h2 id="Scene-Driven-Multimodal-Knowledge-Graph-Construction-for-Embodied-AI"><a href="#Scene-Driven-Multimodal-Knowledge-Graph-Construction-for-Embodied-AI" class="headerlink" title="Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI"></a>Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03783">http://arxiv.org/abs/2311.03783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Yaoxian, Sun Penglei, Liu Haoyu, Li Zhixu, Song Wei, Xiao Yanghua, Zhou Xiaofang</li>
<li>for: 提高机器人智能，增强机器人在 varied open world 中的决策能力</li>
<li>methods: 组合传统知识工程和大型自然语言模型，构建场景驱动多Modal知识图（Scene-MMKG）</li>
<li>results: 比较研究表明，我们的实现的 ManipMob-MMKG 在数据采集效率和知识质量方面具有广泛的优势，可以提高基于知识的机器人任务性能。<details>
<summary>Abstract</summary>
Embodied AI is one of the most popular studies in artificial intelligence and robotics, which can effectively improve the intelligence of real-world agents (i.e. robots) serving human beings. Scene knowledge is important for an agent to understand the surroundings and make correct decisions in the varied open world. Currently, knowledge base for embodied tasks is missing and most existing work use general knowledge base or pre-trained models to enhance the intelligence of an agent. For conventional knowledge base, it is sparse, insufficient in capacity and cost in data collection. For pre-trained models, they face the uncertainty of knowledge and hard maintenance. To overcome the challenges of scene knowledge, we propose a scene-driven multimodal knowledge graph (Scene-MMKG) construction method combining conventional knowledge engineering and large language models. A unified scene knowledge injection framework is introduced for knowledge representation. To evaluate the advantages of our proposed method, we instantiate Scene-MMKG considering typical indoor robotic functionalities (Manipulation and Mobility), named ManipMob-MMKG. Comparisons in characteristics indicate our instantiated ManipMob-MMKG has broad superiority in data-collection efficiency and knowledge quality. Experimental results on typical embodied tasks show that knowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the performance obviously without re-designing model structures complexly. Our project can be found at https://sites.google.com/view/manipmob-mmkg
</details>
<details>
<summary>摘要</summary>
人工智能中的具体AI是现代人工智能和机器人学的一个最受欢迎的研究领域，可以有效提高真实世界中的代理人（即机器人）的智能水平。场景知识是一个代理人理解围场和做出正确决策的关键。现有的知识库 для具体任务缺失，大多数现有工作使用通用知识库或预训练模型来提高代理人的智能水平。传统的知识库缺乏、容易受到数据收集成本的限制，而预训练模型则面临知识不确定性和维护困难。为了突破场景知识的挑战，我们提出了场景驱动多Modal知识图（Scene-MMKG）建构方法，结合传统知识工程和大语言模型。我们引入了一个统一的场景知识注入框架，以便知识表示。为了评估我们提出的方法的优势，我们实例化Scene-MMKG，并考虑典型的室内机器人功能（操作和移动），称之为ManipMob-MMKG。对比特点表明，我们的实例化ManipMob-MMKG在数据采集效率和知识质量方面有广泛的优势。实验结果表明，使用我们的实例化ManipMob-MMKG可以明显提高代理人的性能，而不需要复杂地重构模型结构。我们的项目可以在https://sites.google.com/view/manipmob-mmkg 找到。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Textual-and-Structure-Based-Models-for-Knowledge-Graph-Completion"><a href="#Ensembling-Textual-and-Structure-Based-Models-for-Knowledge-Graph-Completion" class="headerlink" title="Ensembling Textual and Structure-Based Models for Knowledge Graph Completion"></a>Ensembling Textual and Structure-Based Models for Knowledge Graph Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03780">http://arxiv.org/abs/2311.03780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananjan Nandi, Navdeep Kaur, Parag Singla, Mausam</li>
<li>for: 这个论文主要用于研究知识图补充（KGC）领域中，两种流行的方法：文本模型和结构基于模型。</li>
<li>methods: 这个论文使用了两种方法：文本模型和结构基于模型。文本模型利用知识图中实体描述文本，而结构基于模型则利用知识图中的结构特征。</li>
<li>results: 这个论文的实验结果表明，这两种方法有补充性的优势：结构基于模型在知识图中找到答案的路径较短，而文本模型可以通过描述文本来提高完tenancy。 ensemble weights by using the distributions of scores assigned by individual models to all candidate entities. Our ensemble baseline achieves state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR and 8.3 pt Hits@1 gains over best individual models.<details>
<summary>Abstract</summary>
We consider two popular approaches to Knowledge Graph Completion (KGC): textual models that rely on textual entity descriptions, and structure-based models that exploit the connectivity structure of the Knowledge Graph (KG). Preliminary experiments show that these approaches have complementary strengths: structure-based models perform well when the gold answer is easily reachable from the query head in the KG, while textual models exploit descriptions to give good performance even when the gold answer is not reachable. In response, we explore ensembling as a way of combining the best of both approaches. We propose a novel method for learning query-dependent ensemble weights by using the distributions of scores assigned by individual models to all candidate entities. Our ensemble baseline achieves state-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR and 8.3 pt Hits@1 gains over best individual models.
</details>
<details>
<summary>摘要</summary>
我们考虑了两种受欢迎的知识图完成（KGC）方法：文本模型，它们基于知识图中实体描述文本，以及结构基于模型，它们利用知识图中实体之间的连接结构。我们的初步实验表明，这两种方法具有补做的优势：结构基于模型在知识图中可以快速到达答案，而文本模型可以通过描述来提供良好的性能，即使答案不可达。因此，我们研究 ensemble 的方式来结合这两种方法。我们提出了一种基于查询的 ensemble 学习方法，使用各个模型对所有候选实体分配得分的分布来学习查询取向的 ensemble 权重。我们的ensemble基线达到了三个标准 KGC 数据集的state-of-the-art Result，与最佳个体模型相比，提高了6.8pt MRR和8.3pt Hits@1。
</details></li>
</ul>
<hr>
<h2 id="PT-Tuning-Bridging-the-Gap-between-Time-Series-Masked-Reconstruction-and-Forecasting-via-Prompt-Token-Tuning"><a href="#PT-Tuning-Bridging-the-Gap-between-Time-Series-Masked-Reconstruction-and-Forecasting-via-Prompt-Token-Tuning" class="headerlink" title="PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning"></a>PT-Tuning: Bridging the Gap between Time Series Masked Reconstruction and Forecasting via Prompt Token Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03768">http://arxiv.org/abs/2311.03768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Liu, Jinrui Gan, Xiaoxuan Fan, Yi Zhang, Chuanxian Luo, Jing Zhang, Guangxin Jiang, Yucheng Qian, Changwei Zhao, Huan Ma, Zhenyu Guo</li>
<li>for:  bridging the gap between time series masked reconstruction and forecasting</li>
<li>methods:  reserving pre-trained mask tokens during fine-tuning stage and using prompt token tuning (PT-Tuning)</li>
<li>results:  state-of-the-art performance compared to representation learning and end-to-end supervised forecasting methodsHere’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在 bridging the gap between 时间序列做masked reconstruction和预测</li>
<li>methods: 在练习阶段保留预训decoder，并使用prompt token tuning (PT-Tuning)</li>
<li>results: 比 represenation learning和端到端supervised forecasting方法 obtener了state-of-the-art的性能I hope that helps!<details>
<summary>Abstract</summary>
Self-supervised learning has been actively studied in time series domain recently, especially for masked reconstruction. Most of these methods follow the "Pre-training + Fine-tuning" paradigm in which a new decoder replaces the pre-trained decoder to fit for a specific downstream task, leading to inconsistency of upstream and downstream tasks. In this paper, we first point out that the unification of task objectives and adaptation for task difficulty are critical for bridging the gap between time series masked reconstruction and forecasting. By reserving the pre-trained mask token during fine-tuning stage, the forecasting task can be taken as a special case of masked reconstruction, where the future values are masked and reconstructed based on history values. It guarantees the consistency of task objectives but there is still a gap in task difficulty. Because masked reconstruction can utilize contextual information while forecasting can only use historical information to reconstruct. To further mitigate the existed gap, we propose a simple yet effective prompt token tuning (PT-Tuning) paradigm, in which all pre-trained parameters are frozen and only a few trainable prompt tokens are added to extended mask tokens in element-wise manner. Extensive experiments on real-world datasets demonstrate the superiority of our proposed paradigm with state-of-the-art performance compared to representation learning and end-to-end supervised forecasting methods.
</details>
<details>
<summary>摘要</summary>
自我监督学习在时间序列领域已经广泛研究，特别是对于压缩重建。大多数这些方法采用"预训练+精度调整"模式，在这种模式下，一个新的解码器取代预训练的解码器，以适应特定下游任务，导致上游和下游任务的不一致。在这篇论文中，我们首先指出了融合任务目标和适应任务难度是bridging the gap between time series masked reconstruction and forecasting的关键因素。在练习阶段，保留预训练的假token，使得预测任务可以被视为时间序列压缩重建的特殊情况，将未来值视为假值，并根据历史值进行重建。这 garantizestask objective的一致性，但还存在一定的任务难度差异。因为压缩重建可以利用 contextual information，而预测只能使用历史信息来重建。为了进一步减少现有的差异，我们提议了一种简单 yet effective的 prompt token tuning（PT-Tuning）方法，在这种方法中，所有的预训练参数都被冻结，只有一些可变的提示token被添加到元素级别，以扩展假token。我们对实际世界数据进行了广泛的实验， demonstrate了我们提议的方法的优越性，与代表学习和端到端无监督预测方法相比。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Radio-Signals-with-Wavelet-Transform-for-Deep-Learning-Based-Modulation-Recognition"><a href="#Augmenting-Radio-Signals-with-Wavelet-Transform-for-Deep-Learning-Based-Modulation-Recognition" class="headerlink" title="Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition"></a>Augmenting Radio Signals with Wavelet Transform for Deep Learning-Based Modulation Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03761">http://arxiv.org/abs/2311.03761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Chen, Shilian Zheng, Kunfeng Qiu, Luxin Zhang, Qi Xuan, Xiaoniu Yang</li>
<li>for: 这 paper 是为了提高 radio modulation recognition 的准确率而写的。</li>
<li>methods: 这 paper 使用的方法包括使用 discrete wavelet transform  decomposed detail coefficients 来生成新的样本，以增加训练集的多样性和量。</li>
<li>results: 实验结果表明，这 paper 提出的方法可以significantly outperform 其他数据扩展方法。<details>
<summary>Abstract</summary>
The use of deep learning for radio modulation recognition has become prevalent in recent years. This approach automatically extracts high-dimensional features from large datasets, facilitating the accurate classification of modulation schemes. However, in real-world scenarios, it may not be feasible to gather sufficient training data in advance. Data augmentation is a method used to increase the diversity and quantity of training dataset and to reduce data sparsity and imbalance. In this paper, we propose data augmentation methods that involve replacing detail coefficients decomposed by discrete wavelet transform for reconstructing to generate new samples and expand the training set. Different generation methods are used to generate replacement sequences. Simulation results indicate that our proposed methods significantly outperform the other augmentation methods.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习在无线模式识别中得到了广泛应用。这种方法可以自动提取大量数据集中的高维特征，使得无线模式的准确分类成为可能。然而，在实际应用场景中，可能无法在先前收集足够的训练数据。数据扩展是一种解决这个问题的方法，它可以增加训练集的多样性和量，同时降低数据稀缺和不均衡。在本文中，我们提出了基于离散波лет变换的细节系数替换法，用于生成新的样本并扩展训练集。不同的生成方法用于生成替换序列。实验结果表明，我们提出的方法可以显著超越其他扩展方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-Decentralized-Traffic-Signal-Controllers-with-Multi-Agent-Graph-Reinforcement-Learning"><a href="#Learning-Decentralized-Traffic-Signal-Controllers-with-Multi-Agent-Graph-Reinforcement-Learning" class="headerlink" title="Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning"></a>Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03756">http://arxiv.org/abs/2311.03756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Zhang, Zhiwen Yu, Jun Zhang, Liang Wang, Tom H. Luan, Bin Guo, Chau Yuen</li>
<li>for: 这 paper 考虑了智能城市中的优化交通信号控制问题，它被看作是一个复杂的网络系统控制问题。</li>
<li>methods: 我们采用了多代理学习（MARL）算法，但现有的 MARL 算法忽略了有效信息集成，这是改进减少代理间学习能力的关键。</li>
<li>results: 我们提出了一种新的分布式控制架构，包括一种基于 topology 的信息集成策略，以及一种基于扩散过程的卷积扩散模块，这些方法可以帮助代理学习有效地捕捉空间-时间相关性。我们在实验中发现，我们的提议在实验室和实际数据上都有优于现有分布式算法。<details>
<summary>Abstract</summary>
This paper considers optimal traffic signal control in smart cities, which has been taken as a complex networked system control problem. Given the interacting dynamics among traffic lights and road networks, attaining controller adaptivity and scalability stands out as a primary challenge. Capturing the spatial-temporal correlation among traffic lights under the framework of Multi-Agent Reinforcement Learning (MARL) is a promising solution. Nevertheless, existing MARL algorithms ignore effective information aggregation which is fundamental for improving the learning capacity of decentralized agents. In this paper, we design a new decentralized control architecture with improved environmental observability to capture the spatial-temporal correlation. Specifically, we first develop a topology-aware information aggregation strategy to extract correlation-related information from unstructured data gathered in the road network. Particularly, we transfer the road network topology into a graph shift operator by forming a diffusion process on the topology, which subsequently facilitates the construction of graph signals. A diffusion convolution module is developed, forming a new MARL algorithm, which endows agents with the capabilities of graph learning. Extensive experiments based on both synthetic and real-world datasets verify that our proposal outperforms existing decentralized algorithms.
</details>
<details>
<summary>摘要</summary>
To address this limitation, we propose a new decentralized control architecture with improved environmental observability. Our approach involves extracting correlation-related information from unstructured data gathered in the road network using a topology-aware information aggregation strategy. Specifically, we convert the road network topology into a graph shift operator by creating a diffusion process on the topology, which enables the construction of graph signals. We then develop a diffusion convolution module, which endows agents with the ability to learn from graph signals.Extensive experiments based on both synthetic and real-world datasets demonstrate that our proposed method outperforms existing decentralized algorithms.
</details></li>
</ul>
<hr>
<h2 id="COOL-A-Constraint-Object-Oriented-Logic-Programming-Language-and-its-Neural-Symbolic-Compilation-System"><a href="#COOL-A-Constraint-Object-Oriented-Logic-Programming-Language-and-its-Neural-Symbolic-Compilation-System" class="headerlink" title="COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System"></a>COOL: A Constraint Object-Oriented Logic Programming Language and its Neural-Symbolic Compilation System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03753">http://arxiv.org/abs/2311.03753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jipeng Han</li>
<li>for: 本研究探讨了神经网络与逻辑编程的集成，解决了长期存在的神经网络总结和学习能力与逻辑逻辑的精度结合的挑战。</li>
<li>methods: 本研究使用了COOL（Constraint Object-Oriented Logic）编程语言，一种创新的方法，它将逻辑推理和神经网络技术融为一体。COOL自动处理数据采集，减少了用户提供初始数据的需求。</li>
<li>results: COOL语言可以减少神经网络训练时的风险，提高神经网络的重用和增强。此外，COOL的基本原理和算法可能为未来编程语言和神经网络架构的发展提供有价值的思路。<details>
<summary>Abstract</summary>
This paper explores the integration of neural networks with logic programming, addressing the longstanding challenges of combining the generalization and learning capabilities of neural networks with the precision of symbolic logic. Traditional attempts at this integration have been hampered by difficulties in initial data acquisition, the reliability of undertrained networks, and the complexity of reusing and augmenting trained models. To overcome these issues, we introduce the COOL (Constraint Object-Oriented Logic) programming language, an innovative approach that seamlessly combines logical reasoning with neural network technologies. COOL is engineered to autonomously handle data collection, mitigating the need for user-supplied initial data. It incorporates user prompts into the coding process to reduce the risks of undertraining and enhances the interaction among models throughout their lifecycle to promote the reuse and augmentation of networks. Furthermore, the foundational principles and algorithms in COOL's design and its compilation system could provide valuable insights for future developments in programming languages and neural network architectures.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文探讨了神经网络与逻辑编程的集成，解决了传统集成神经网络和逻辑逻辑的挑战。这些挑战包括数据收集的困难、训练过程中的可靠性和训练过程中的模型复用和扩展。为了解决这些问题，我们介绍了COOL（卷积对象逻辑）编程语言，这是一种创新的方法，可以自然地结合逻辑思维和神经网络技术。COOL可以自动处理数据收集，从而减少用户提供的初始数据的需求。它还包括用户提示在编程过程中，以减少训练过程中的风险，并且在模型的整个生命周期中提高模型之间的互动，以促进模型的复用和扩展。此外，COOL的基本原则和算法在其编译系统中，可以为未来的编程语言和神经网络架构的发展提供有价值的意见。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Applications-of-Deep-Learning-with-Finite-Samples-in-Full-Life-Cycle-Intelligence-of-Nuclear-Power-Generation"><a href="#Analysis-and-Applications-of-Deep-Learning-with-Finite-Samples-in-Full-Life-Cycle-Intelligence-of-Nuclear-Power-Generation" class="headerlink" title="Analysis and Applications of Deep Learning with Finite Samples in Full Life-Cycle Intelligence of Nuclear Power Generation"></a>Analysis and Applications of Deep Learning with Finite Samples in Full Life-Cycle Intelligence of Nuclear Power Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04247">http://arxiv.org/abs/2311.04247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenwei Tang, Wenqiang Zhou, Dong Wang, Caiyang Yu, Zhenan He, Jizhe Zhou, Shudong Huang, Yi Gao, Jianming Chen, Wentao Feng, Jiancheng Lv<br>for:This paper focuses on the application of deep learning (DL) techniques in the context of nuclear power generation (NPG), specifically in the face of limited data availability.methods:The paper explores and applies DL methodologies under the constraints of finite sample availability, including small-sample learning, few-shot learning, zero-shot learning, and open-set recognition.results:The paper presents two case studies, one on automatic recognition of zirconium alloy metallography and the other on open-set recognition for signal diagnosis of machinery sensors, both of which demonstrate constructive outcomes and insightful deliberations.<details>
<summary>Abstract</summary>
The advent of Industry 4.0 has precipitated the incorporation of Artificial Intelligence (AI) methods within industrial contexts, aiming to realize intelligent manufacturing, operation as well as maintenance, also known as industrial intelligence. However, intricate industrial milieus, particularly those relating to energy exploration and production, frequently encompass data characterized by long-tailed class distribution, sample imbalance, and domain shift. These attributes pose noteworthy challenges to data-centric Deep Learning (DL) techniques, crucial for the realization of industrial intelligence. The present study centers on the intricate and distinctive industrial scenarios of Nuclear Power Generation (NPG), meticulously scrutinizing the application of DL techniques under the constraints of finite data samples. Initially, the paper expounds on potential employment scenarios for AI across the full life-cycle of NPG. Subsequently, we delve into an evaluative exposition of DL's advancement, grounded in the finite sample perspective. This encompasses aspects such as small-sample learning, few-shot learning, zero-shot learning, and open-set recognition, also referring to the unique data characteristics of NPG. The paper then proceeds to present two specific case studies. The first revolves around the automatic recognition of zirconium alloy metallography, while the second pertains to open-set recognition for signal diagnosis of machinery sensors. These cases, spanning the entirety of NPG's life-cycle, are accompanied by constructive outcomes and insightful deliberations. By exploring and applying DL methodologies within the constraints of finite sample availability, this paper not only furnishes a robust technical foundation but also introduces a fresh perspective toward the secure and efficient advancement and exploitation of this advanced energy source.
</details>
<details>
<summary>摘要</summary>
<sup>1</sup> industry 4.0的出现引入了人工智能（AI）方法在工业上，以实现智能生产、维护等，也称为工业智能。然而，一些特殊的工业环境，如能源探索和生产，经常存在长尾分布、样本偏置和领域转移等问题。这些问题对数据驱动的深度学习（DL）技术提出了不可遗憾的挑战。本研究关注了核电生产（NPG）的特殊和独特的工业场景，宁杰入彩地探讨了DL技术在有限样本的约束下的应用。本文首先介绍了NPG中AI的应用场景，然后详细介绍了DL技术的发展，包括小样本学习、少数抽象学习、零例学习和开集认知等方面，同时还包括NPG特有的数据特征。然后，本文介绍了两个具体的案例研究，一是自动识别锌合金icrography，二是开集认知用于机械传感器的信号诊断。这两个案例分别封闭NPG的全生命周期，并且得到了有益的结果和深刻的思考。通过在有限样本的约束下探索和应用DL方法ologies，本文不仅提供了坚实的技术基础，还提供了一种新的视角，即在安全有效地推动和利用这种先进的能源源泉。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Large-Language-Models-for-Automated-Proof-Synthesis-in-Rust"><a href="#Leveraging-Large-Language-Models-for-Automated-Proof-Synthesis-in-Rust" class="headerlink" title="Leveraging Large Language Models for Automated Proof Synthesis in Rust"></a>Leveraging Large Language Models for Automated Proof Synthesis in Rust</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03739">http://arxiv.org/abs/2311.03739</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianan Yao, Ziqiao Zhou, Weiteng Chen, Weidong Cui</li>
<li>for: 这篇论文是为了提高正式验证的广泛应用而写的。</li>
<li>methods: 论文使用了大型自然语言模型（LLMs）和静态分析来生成 invariants、assertrions 和其他证据结构，以验证 Rust 基于的正式验证框架 Verus。</li>
<li>results: 在几次Setting中，LLMs 表现出了很好的逻辑能力，能够生成 postconditions 和循环 invariants，特别是对短代码段进行分析。但 LLMs 缺乏保持和传递上下文信息的能力，这是传统静态分析的优点。基于这些观察，我们开发了一个基于 OpenAI 的 GPT-4 模型的原型。我们的原型将验证任务分解成多个更小的任务，逐次询问 GPT-4，并将其输出与轻量级静态分析结合起来。我们对 20 个向量操作程序进行了评估，结果表明，它可以减少人类的证据代码编写劳动。<details>
<summary>Abstract</summary>
Formal verification can provably guarantee the correctness of critical system software, but the high proof burden has long hindered its wide adoption. Recently, Large Language Models (LLMs) have shown success in code analysis and synthesis. In this paper, we present a combination of LLMs and static analysis to synthesize invariants, assertions, and other proof structures for a Rust-based formal verification framework called Verus. In a few-shot setting, LLMs demonstrate impressive logical ability in generating postconditions and loop invariants, especially when analyzing short code snippets. However, LLMs lack the ability to retain and propagate context information, a strength of traditional static analysis. Based on these observations, we developed a prototype based on OpenAI's GPT-4 model. Our prototype decomposes the verification task into multiple smaller ones, iteratively queries GPT-4, and combines its output with lightweight static analysis. We evaluated the prototype with a developer in the automation loop on 20 vector-manipulating programs. The results demonstrate that it significantly reduces human effort in writing entry-level proof code.
</details>
<details>
<summary>摘要</summary>
正式验证可以证明 Kritical 系统软件的正确性，但长期以来，证明的负担很高，使得它的广泛采用受到限制。现在，大型自然语言模型（LLMs）在代码分析和生成方面表现出色。在这篇论文中，我们提出了结合 LLMs 和静态分析的方法，用于生成 invariants、断言和其他证明结构，以便为 Rust 基础的正式验证框架 Verus 进行验证。在几步设置下，LLMs 在分析短代码段时表现出了卓越的逻辑能力，特别是在生成 postconditions 和循环 invariants 方面。然而，LLMs 缺乏保持和传递上下文信息的能力，这是传统静态分析的优势。根据这些观察结果，我们开发了基于 OpenAI 的 GPT-4 模型的 прототип。我们的 прототип将验证任务分解成多个更小的任务，逐步查询 GPT-4，并将其输出与轻量级静态分析结合起来。我们对 20 个向量操作程序进行了评估。结果表明，它可以减少人类在编写入门证明代码的劳动。
</details></li>
</ul>
<hr>
<h2 id="deep-REMAP-Parameterization-of-Stellar-Spectra-Using-Regularized-Multi-Task-Learning"><a href="#deep-REMAP-Parameterization-of-Stellar-Spectra-Using-Regularized-Multi-Task-Learning" class="headerlink" title="deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning"></a>deep-REMAP: Parameterization of Stellar Spectra Using Regularized Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03738">http://arxiv.org/abs/2311.03738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sankalp Gilda</li>
<li>for: 用于 precisione stellar atmospheric parameter prediction</li>
<li>methods: 使用深度学习和多任务学习，以及创新的不对称损失函数</li>
<li>results: 在使用 Phoenix 库和 MARVELS Survey 数据时，准确地预测了星际大气参数，并且可以扩展到其他星际库和属性。<details>
<summary>Abstract</summary>
Traditional spectral analysis methods are increasingly challenged by the exploding volumes of data produced by contemporary astronomical surveys. In response, we develop deep-Regularized Ensemble-based Multi-task Learning with Asymmetric Loss for Probabilistic Inference ($\rm{deep-REMAP}$), a novel framework that utilizes the rich synthetic spectra from the PHOENIX library and observational data from the MARVELS survey to accurately predict stellar atmospheric parameters. By harnessing advanced machine learning techniques, including multi-task learning and an innovative asymmetric loss function, $\rm{deep-REMAP}$ demonstrates superior predictive capabilities in determining effective temperature, surface gravity, and metallicity from observed spectra. Our results reveal the framework's effectiveness in extending to other stellar libraries and properties, paving the way for more sophisticated and automated techniques in stellar characterization.
</details>
<details>
<summary>摘要</summary>
传统的光谱分析方法随着现代天文学调查的数据急剧增长而面临挑战。为应对这一问题，我们开发了深度 régulé Ensemble-based Multi-task Learning with Asymmetric Loss for Probabilistic Inference（深度-REMAP），一种新的框架，它利用了 Phoenics 库中的丰富人工光谱和 MARVELS 调查的观测数据，以准确预测星际大气参数。通过应用先进的机器学习技术，包括多任务学习和创新的非对称损失函数，深度-REMAP 显示了在确定效应温度、表面重力和金属含量方面的高度预测能力。我们的结果表明，深度-REMAP 可以延伸到其他星际库和属性，为stellar characterization 带来更加复杂和自动化的技术。
</details></li>
</ul>
<hr>
<h2 id="Neural-MMO-2-0-A-Massively-Multi-task-Addition-to-Massively-Multi-agent-Learning"><a href="#Neural-MMO-2-0-A-Massively-Multi-task-Addition-to-Massively-Multi-agent-Learning" class="headerlink" title="Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning"></a>Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03736">http://arxiv.org/abs/2311.03736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Suárez, Phillip Isola, Kyoung Whan Choe, David Bloomin, Hao Xiang Li, Nikhil Pinnaparaju, Nishaanth Kanna, Daniel Scott, Ryan Sullivan, Rose S. Shuman, Lucas de Alcântara, Herbie Bradley, Louis Castricato, Kirsty You, Yuhao Jiang, Qimai Li, Jiaxin Chen, Xiaolong Zhu</li>
<li>for: 本研究旨在提供一个可定制的任务系统，用于训练执行多任务和抗拒抗的智能代理。</li>
<li>methods: 本研究使用Neural MMO 2.0，一个基于深度学习的大型多代理环境，并提供了一个灵活的任务系统，允许用户定制任务、地图和对手。</li>
<li>results: 研究人员通过使用Neural MMO 2.0和CleanRL，训练出能够泛化到任务、地图和对手的智能代理，并实现了三倍的性能提升。<details>
<summary>Abstract</summary>
Neural MMO 2.0 is a massively multi-agent environment for reinforcement learning research. The key feature of this new version is a flexible task system that allows users to define a broad range of objectives and reward signals. We challenge researchers to train agents capable of generalizing to tasks, maps, and opponents never seen during training. Neural MMO features procedurally generated maps with 128 agents in the standard setting and support for up to. Version 2.0 is a complete rewrite of its predecessor with three-fold improved performance and compatibility with CleanRL. We release the platform as free and open-source software with comprehensive documentation available at neuralmmo.github.io and an active community Discord. To spark initial research on this new platform, we are concurrently running a competition at NeurIPS 2023.
</details>
<details>
<summary>摘要</summary>
neuralmmo 2.0 是一个大规模多智能环境，用于研究强化学习。新版本的关键特性是可以自由定义任务系统，允许用户设定广泛的目标和奖励信号。我们挑战研究人员用agent来学习并在训练时未经看到的任务、地图和对手上掌握概念。 neuralmmo 2.0 支持生成的地图，标准设置中有128个agent，并且支持最多。这是前一版本的三倍性能提升，并且兼容cleanrl。我们在 neuralmmo.github.io 上发布了这个平台作为免费和开源软件，并提供了详细的文档和活跃的discord社区。为了促进这个新平台的研究，我们同时在 neurips 2023 上进行了一场竞赛。
</details></li>
</ul>
<hr>
<h2 id="ClimateSet-A-Large-Scale-Climate-Model-Dataset-for-Machine-Learning"><a href="#ClimateSet-A-Large-Scale-Climate-Model-Dataset-for-Machine-Learning" class="headerlink" title="ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning"></a>ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03721">http://arxiv.org/abs/2311.03721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Kaltenborn, Charlotte E. E. Lange, Venkatesh Ramesh, Philippe Brouillard, Yaniv Gurwicz, Chandni Nagda, Jakob Runge, Peer Nowack, David Rolnick<br>for:* 这个论文的目的是为气候科学家和机器学习专家提供一个大型、一致的气候模型数据集，以支持气候变化的影响和未来气候enario的 simulate。methods:* 这个论文使用了Input4MIPs和CMIP6气候模型数据集，并提供了一个模块化的数据集管道，以便在新的气候模型和enario上 retrieve和处理数据。results:* 这个论文使用ClimateSet数据集作为一个标准的机器学习气候模型Emulator的benchmark，并通过分析不同气候模型的性能和泛化能力，获得了新的洞察和理解。此外，这个数据集还可以用于训练一个“超级Emulator”，以快速预测新的气候变化enario，并补充现有的scenario，为政策制定人提供新的参考。<details>
<summary>Abstract</summary>
Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists' efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a "super emulator" can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.
</details>
<details>
<summary>摘要</summary>
клима学模型已经是评估气候变化的重要工具，以及预测未来气候enario的方法。机器学习（ML）社区在支持气候科学家的各种任务上表现出了增长的兴趣，例如气候模型的模拟、下采和预测任务。但是，气候科学和ML社区都认为，为了解决这些任务，我们需要大量、一致、ML准备好的气候模型数据集。这里，我们介绍了气候集（ClimateSet），一个包含36个气候模型的输入和输出的数据集。此外，我们还提供了一个模块化的数据集管道，用于检索和处理其他气候模型和enario。我们通过使用这些数据集作为benchmark，证明了ML-基于气候模型的模拟的潜在性和总体表现。此外，这些数据集还可以用于训练一个"超级模拟器"，可以快速地项新的气候变化scenario，并补充现有的scenario，为政策制定者提供。我们认为，气候集将为ML社区提供基础，以便在气候相关任务上进行大规模的推进。
</details></li>
</ul>
<hr>
<h2 id="LLM-as-an-Art-Director-LaDi-Using-LLMs-to-improve-Text-to-Media-Generators"><a href="#LLM-as-an-Art-Director-LaDi-Using-LLMs-to-improve-Text-to-Media-Generators" class="headerlink" title="LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators"></a>LLM as an Art Director (LaDi): Using LLMs to improve Text-to-Media Generators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03716">http://arxiv.org/abs/2311.03716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allen Roush, Emil Zakirov, Artemiy Shirokov, Polina Lunina, Jack Gane, Alexander Duffy, Charlie Basil, Aber Whitcomb, Jim Benedetto, Chris DeWolfe</li>
<li>for: 这篇论文旨在提高文本到图像生成技术的质量和 relevance，使其能够更好地满足艺术和电影等领域的需求。</li>
<li>methods: 本论文提出了一种名为”LaDi”的统一系统，用于使大语言模型（LLMs） acts as 艺术指导者，提高图像和视频生成的质量。LaDi  integrate 多种技术，包括受限decode、智能提示、精度调整和检索等，以提高文本到图像和视频生成的能力。</li>
<li>results: 据作者介绍，LaDi 和这些技术在 Plai Labs 开发的应用和平台中已经得到了应用，并且能够生成高质量、上下文感知和主题相关的图像和视频。<details>
<summary>Abstract</summary>
Recent advancements in text-to-image generation have revolutionized numerous fields, including art and cinema, by automating the generation of high-quality, context-aware images and video. However, the utility of these technologies is often limited by the inadequacy of text prompts in guiding the generator to produce artistically coherent and subject-relevant images. In this paper, We describe the techniques that can be used to make Large Language Models (LLMs) act as Art Directors that enhance image and video generation. We describe our unified system for this called "LaDi". We explore how LaDi integrates multiple techniques for augmenting the capabilities of text-to-image generators (T2Is) and text-to-video generators (T2Vs), with a focus on constrained decoding, intelligent prompting, fine-tuning, and retrieval. LaDi and these techniques are being used today in apps and platforms developed by Plai Labs.
</details>
<details>
<summary>摘要</summary>
近期的文本到图生成技术发展，对艺术和电影等领域产生了革命性的变革，自动生成高质量、上下文感知图像和视频。然而，这些技术的实用性往往受到文本提示的不够精细性和主题相关性的限制。在这篇论文中，我们将介绍如何使大语言模型（LLM） acting as 艺术指导（Art Directors），提高图像和视频生成的质量。我们提出了“LaDi”系统，并详细介绍了它如何集成多种加强文本到图生成器（T2I）和文本到视频生成器（T2V）的技术，包括约束解码、智能提示、精度调整和检索。LaDi和这些技术今天在 Plai Labs 开发的应用和平台中被使用。
</details></li>
</ul>
<hr>
<h2 id="Loss-Balancing-for-Fair-Supervised-Learning"><a href="#Loss-Balancing-for-Fair-Supervised-Learning" class="headerlink" title="Loss Balancing for Fair Supervised Learning"></a>Loss Balancing for Fair Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03714">http://arxiv.org/abs/2311.03714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/khalilimahdi/loss_balancing_icml2023">https://github.com/khalilimahdi/loss_balancing_icml2023</a></li>
<li>paper_authors: Mohammad Mahdi Khalili, Xueru Zhang, Mahed Abroshan<br>for: This paper focuses on addressing unfairness issues in supervised learning models by proposing a fairness notion called Equalized Loss (EL).methods: The paper introduces an algorithm called ELminimizer, which leverages off-the-shelf convex programming tools (e.g., CVXPY) to efficiently find the global optimum of the non-convex optimization problem under the EL constraint.results: The paper theoretically proves that the ELminimizer algorithm finds the global optimal solution under certain conditions, and supports the theoretical results through several empirical studies.<details>
<summary>Abstract</summary>
Supervised learning models have been used in various domains such as lending, college admission, face recognition, natural language processing, etc. However, they may inherit pre-existing biases from training data and exhibit discrimination against protected social groups. Various fairness notions have been proposed to address unfairness issues. In this work, we focus on Equalized Loss (EL), a fairness notion that requires the expected loss to be (approximately) equalized across different groups. Imposing EL on the learning process leads to a non-convex optimization problem even if the loss function is convex, and the existing fair learning algorithms cannot properly be adopted to find the fair predictor under the EL constraint. This paper introduces an algorithm that can leverage off-the-shelf convex programming tools (e.g., CVXPY) to efficiently find the global optimum of this non-convex optimization. In particular, we propose the ELminimizer algorithm, which finds the optimal fair predictor under EL by reducing the non-convex optimization to a sequence of convex optimization problems. We theoretically prove that our algorithm finds the global optimal solution under certain conditions. Then, we support our theoretical results through several empirical studies.
</details>
<details>
<summary>摘要</summary>
受监督学习模型在不同领域中使用，如贷款、大学招生、人脸识别、自然语言处理等。然而，它们可能从训练数据中继承先前的偏见，并对保护社会群体表现歧视。多种公平性概念已经被提出来解决不公平问题。在这种工作中，我们关注Equalized Loss（EL）公平性概念，它需要不同群体的预期损失相似。在满足EL公平性概念的情况下，我们提出了一种名为ELminimizer算法，它可以使用现有的凸程程序工具（如CVXPY）高效地找到非凸优化问题的全局最优解。我们证明了我们的算法在某些条件下找到全球最优解。然后，我们通过多个实验研究支持我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Estimation-Errors-by-Twin-TD-Regularized-Actor-and-Critic-for-Deep-Reinforcement-Learning"><a href="#Mitigating-Estimation-Errors-by-Twin-TD-Regularized-Actor-and-Critic-for-Deep-Reinforcement-Learning" class="headerlink" title="Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning"></a>Mitigating Estimation Errors by Twin TD-Regularized Actor and Critic for Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03711">http://arxiv.org/abs/2311.03711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junmin Zhong, Ruofan Wu, Jennie Si</li>
<li>for: 解决深度强化学习中的估计偏见问题</li>
<li>methods: 引入新的双TD-正则化actor-critic方法，以减少过估和UNDER-估计错误</li>
<li>results: 通过 combining 好的 DRL 改进方法，如分布学习和长N-步代理阶段奖励方法，实现了新的 TDR-based actor-critic 学习在深度控制集中表现出色，并将 TD3 和 SAC 的性能提升到与 D4PG 相当的水平，同时也提高了 D4PG 的性能到新的 SOTA 水平。<details>
<summary>Abstract</summary>
We address the issue of estimation bias in deep reinforcement learning (DRL) by introducing solution mechanisms that include a new, twin TD-regularized actor-critic (TDR) method. It aims at reducing both over and under-estimation errors. With TDR and by combining good DRL improvements, such as distributional learning and long N-step surrogate stage reward (LNSS) method, we show that our new TDR-based actor-critic learning has enabled DRL methods to outperform their respective baselines in challenging environments in DeepMind Control Suite. Furthermore, they elevate TD3 and SAC respectively to a level of performance comparable to that of D4PG (the current SOTA), and they also improve the performance of D4PG to a new SOTA level measured by mean reward, convergence speed, learning success rate, and learning variance.
</details>
<details>
<summary>摘要</summary>
我们解决深度征才学习（DRL）中的估计偏见问题，通过引入新的双TD-调整actor-critic（TDR）方法，旨在降低过估和 unter-估计错误。With TDR和融合好DRL改进方法，如分布式学习和长N步代理奖（LNSS）方法，我们显示了我们的新TDR基于actor-critic学习可以在深度控制套件中的问题环境中超过其基eline。此外，它将TD3和SAC分别提升到与D4PG（目前SOTA）的性能水平，并且提高D4PG的性能到新的SOTA水平， measured by mean reward、convergence speed、learning success rate和learning variance。
</details></li>
</ul>
<hr>
<h2 id="The-NeurIPS-2022-Neural-MMO-Challenge-A-Massively-Multiagent-Competition-with-Specialization-and-Trade"><a href="#The-NeurIPS-2022-Neural-MMO-Challenge-A-Massively-Multiagent-Competition-with-Specialization-and-Trade" class="headerlink" title="The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade"></a>The NeurIPS 2022 Neural MMO Challenge: A Massively Multiagent Competition with Specialization and Trade</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03707">http://arxiv.org/abs/2311.03707</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuralmmo/neurips2022nmmo-submission-pool">https://github.com/neuralmmo/neurips2022nmmo-submission-pool</a></li>
<li>paper_authors: Enhong Liu, Joseph Suarez, Chenhui You, Bo Wu, Bingcheng Chen, Jun Hu, Jiaxin Chen, Xiaolong Zhu, Clare Zhu, Julian Togelius, Sharada Mohanty, Weijun Hong, Rui Du, Yibing Zhang, Qinwen Wang, Xinhang Li, Zheng Yuan, Xiang Li, Yuejia Huang, Kun Zhang, Hanhui Yang, Shiqi Tang, Phillip Isola</li>
<li>for: 这篇论文描述了NeuIPS-2022神经网络MMO挑战的结果，该挑战吸引了500名参与者和获得了超过1,600个提交。</li>
<li>methods: 这年的挑战使用了最新的v1.6神经网络MMO，该版本增加了新的设备、战斗、交易和评价系统，这些元素共同提供了更多的鲁棒性和泛化挑战。</li>
<li>results: 该论文描述了挑战的设计和结果，探讨了这种环境作为学习方法的标准准则，并提供了一些实用的追加优化方法以解决复杂任务的稀缺奖励问题。<details>
<summary>Abstract</summary>
In this paper, we present the results of the NeurIPS-2022 Neural MMO Challenge, which attracted 500 participants and received over 1,600 submissions. Like the previous IJCAI-2022 Neural MMO Challenge, it involved agents from 16 populations surviving in procedurally generated worlds by collecting resources and defeating opponents. This year's competition runs on the latest v1.6 Neural MMO, which introduces new equipment, combat, trading, and a better scoring system. These elements combine to pose additional robustness and generalization challenges not present in previous competitions. This paper summarizes the design and results of the challenge, explores the potential of this environment as a benchmark for learning methods, and presents some practical reinforcement learning training approaches for complex tasks with sparse rewards. Additionally, we have open-sourced our baselines, including environment wrappers, benchmarks, and visualization tools for future research.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了2022年度的Neural MMO挑战，该挑战有500名参与者和1,600个提交。与前一年的IJCAI-2022 Neural MMO挑战类似，这年的挑战是在生成的世界中，参与者需要收集资源和击败对手来存活。这一年的比赛运行在最新的v1.6 Neural MMO中，新增了设备、战斗、贸易和评分系统。这些元素共同 pose了额外的稳定性和泛化挑战，不在前一年的比赛中存在。本文概述了挑战的设计和结果，探讨了这种环境的可能性作为学习方法的标准准则，并提供了一些实用的强化学习训练方法 для复杂任务的稀有奖励。此外，我们还开源了我们的基线，包括环境包装、标准和视觉化工具，以便未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Bottom-Up-Synthesis-for-Programs-with-Local-Variables"><a href="#Efficient-Bottom-Up-Synthesis-for-Programs-with-Local-Variables" class="headerlink" title="Efficient Bottom-Up Synthesis for Programs with Local Variables"></a>Efficient Bottom-Up Synthesis for Programs with Local Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03705">http://arxiv.org/abs/2311.03705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Xiangyu Zhou, Rui Dong, Yihong Zhang, Xinyu Wang</li>
<li>for: 本文提出了一种新的合成算法，可以高效地搜索具有本地变量（如lambda函数引入的变量）的程序。</li>
<li>methods: 本文使用的方法是提升程序解释过程，从原来的一个程序一个接一个地评估，改为同时评估所有程序从grammar中的所有练习。这种升级的解释方法被称为”提升解释”，它可以系统地枚举所有的绑定上下文，因此可以有效地评估和减少具有本地变量的程序的搜索空间。</li>
<li>results: 本文的实现 tool Arborist 可以更高效地自动化互联网自动化任务，比如WebRobot和Helena等现有的技术。<details>
<summary>Abstract</summary>
We propose a new synthesis algorithm that can efficiently search programs with local variables (e.g., those introduced by lambdas). Prior bottom-up synthesis algorithms are not able to evaluate programs with free local variables, and therefore cannot effectively reduce the search space of such programs (e.g., using standard observational equivalence reduction techniques), making synthesis slow. Our algorithm can reduce the space of programs with local variables. The key idea, dubbed lifted interpretation, is to lift up the program interpretation process, from evaluating one program at a time to simultaneously evaluating all programs from a grammar. Lifted interpretation provides a mechanism to systematically enumerate all binding contexts for local variables, thereby enabling us to evaluate and reduce the space of programs with local variables. Our ideas are instantiated in the domain of web automation. The resulting tool, Arborist, can automate a significantly broader range of challenging tasks more efficiently than state-of-the-art techniques including WebRobot and Helena.
</details>
<details>
<summary>摘要</summary>
我们提出一个新的合成算法，可以有效地搜寻具有地方变数（例如，由lambda函数引入的）的程序。先前的底向合成算法无法评估具有自由地方变数的程序，因此无法有效地将程序的搜寻空间缩减（例如，使用标准观察等 equivalence reduction techniques），导致合成速度慢。我们的算法可以缩减程序具有地方变数的空间。我们的主要想法，被称为“提升解释”，是将程序解释过程“提升”到评估一个程序的时候，从一个程序评估到同时评估所有程序的数学表达。提升解释提供了一个系统地列出所有绑定上下文的机制，从而允许我们评估和缩减具有地方变数的程序的空间。我们的想法在网页自动化领域中实现，实现了一个名为Arborist的工具，可以更有效地和更快地自动化许多具有挑战性的任务，比如WebRobot和Helena的State-of-the-art技术。
</details></li>
</ul>
<hr>
<h2 id="Hypothesis-Network-Planned-Exploration-for-Rapid-Meta-Reinforcement-Learning-Adaptation"><a href="#Hypothesis-Network-Planned-Exploration-for-Rapid-Meta-Reinforcement-Learning-Adaptation" class="headerlink" title="Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation"></a>Hypothesis Network Planned Exploration for Rapid Meta-Reinforcement Learning Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03701">http://arxiv.org/abs/2311.03701</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxwell Joseph Jacobson, Yexiang Xue</li>
<li>for:  trains agents that adapt to fast-changing environments and tasks</li>
<li>methods:  integrates an active and planned exploration process via the hypothesis network to optimize adaptation speed</li>
<li>results:  outpaces baseline methods in adaptation speed and model accuracy, validating its potential in enhancing reinforcement learning adaptation in rapidly evolving settingsHere’s the full text in Simplified Chinese:</li>
<li>for:  trains agents that adapt to fast-changing environments and tasks</li>
<li>methods: 使用假设网络进行活动和规划的探索过程，以优化适应速度</li>
<li>results: 在符号版的Alchemy游戏上舜舜战胜基准方法，证明其在快速发展场景中提升学习适应的潜力<details>
<summary>Abstract</summary>
Meta Reinforcement Learning (Meta RL) trains agents that adapt to fast-changing environments and tasks. Current strategies often lose adaption efficiency due to the passive nature of model exploration, causing delayed understanding of new transition dynamics. This results in particularly fast-evolving tasks being impossible to solve. We propose a novel approach, Hypothesis Network Planned Exploration (HyPE), that integrates an active and planned exploration process via the hypothesis network to optimize adaptation speed. HyPE uses a generative hypothesis network to form potential models of state transition dynamics, then eliminates incorrect models through strategically devised experiments. Evaluated on a symbolic version of the Alchemy game, HyPE outpaces baseline methods in adaptation speed and model accuracy, validating its potential in enhancing reinforcement learning adaptation in rapidly evolving settings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CNMeta 强化学习（Meta RL）训练代理人适应快速变化的环境和任务。现有策略 часто因模型探索的被动性而失去适应效率，导致新的转移动力学性的理解延迟。这会使得特别是快速演化的任务无法解决。我们提出了一种新的方法，假设网络规划探索（HyPE），它通过假设网络来整合活动和规划探索过程，以优化适应速度。HyPE使用生成假设网络来形成可能的状态转移动力学模型，然后通过策划出的实验排除错误模型。在一个符号化的Alchemy游戏中进行评估，HyPE在适应速度和模型准确性方面胜过基准方法，这 validate了它在快速演化的设置中增强强化学习的潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Variational-Lower-Bound-for-Inverse-Reinforcement-Learning"><a href="#A-Novel-Variational-Lower-Bound-for-Inverse-Reinforcement-Learning" class="headerlink" title="A Novel Variational Lower Bound for Inverse Reinforcement Learning"></a>A Novel Variational Lower Bound for Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03698">http://arxiv.org/abs/2311.03698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikang Gui, Prashant Doshi</li>
<li>for: 学习任务协同或复制，从专家轨迹中学习奖励函数，去除手动奖励工程化。</li>
<li>methods: 使用变分下界法（VLB-IRL），在 probabilistic graphical model 中学习奖励函数和策略，并且同时学习了奖励函数和策略。</li>
<li>results: 在知名领域上，方法可以学习一个有效的奖励函数，使得根据学习的奖励函数来采取策略，可以达到专家水平表现。此外，方法还可以在这些领域上超越现有的状态体验IRL算法，表现出更好的奖励。<details>
<summary>Abstract</summary>
Inverse reinforcement learning (IRL) seeks to learn the reward function from expert trajectories, to understand the task for imitation or collaboration thereby removing the need for manual reward engineering. However, IRL in the context of large, high-dimensional problems with unknown dynamics has been particularly challenging. In this paper, we present a new Variational Lower Bound for IRL (VLB-IRL), which is derived under the framework of a probabilistic graphical model with an optimality node. Our method simultaneously learns the reward function and policy under the learned reward function by maximizing the lower bound, which is equivalent to minimizing the reverse Kullback-Leibler divergence between an approximated distribution of optimality given the reward function and the true distribution of optimality given trajectories. This leads to a new IRL method that learns a valid reward function such that the policy under the learned reward achieves expert-level performance on several known domains. Importantly, the method outperforms the existing state-of-the-art IRL algorithms on these domains by demonstrating better reward from the learned policy.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> inverse reinforcement learning（IRL）目的是从专家轨迹中学习奖励函数，以便理解任务，从而消除手动奖励工程化。然而，在大型、高维度问题中，无法确定动力学的情况下，IRL特别具有挑战性。在这篇论文中，我们提出了一种新的Variational Lower Bound for IRL（VLB-IRL），它基于概率图模型中的优化节点。我们的方法同时学习奖励函数和政策，并且通过最大化下界来实现这一目标，下界等于将推断分布的优化与实际分布的优化进行reverse Kullback-Leibler散度的减少。这导致了一种新的IRL方法，该方法学习了一个有效的奖励函数，使得在知道的领域上，政策下的奖励函数得到了专家水平的表现。进一步，该方法在这些领域上超越了现有的IRL算法，通过展示更好的奖励来证明。
</details></li>
</ul>
<hr>
<h2 id="Context-Shift-Reduction-for-Offline-Meta-Reinforcement-Learning"><a href="#Context-Shift-Reduction-for-Offline-Meta-Reinforcement-Learning" class="headerlink" title="Context Shift Reduction for Offline Meta-Reinforcement Learning"></a>Context Shift Reduction for Offline Meta-Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03695">http://arxiv.org/abs/2311.03695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/moreanp/csro">https://github.com/moreanp/csro</a></li>
<li>paper_authors: Yunkai Gao, Rui Zhang, Jiaming Guo, Fan Wu, Qi Yi, Shaohui Peng, Siming Lan, Ruizhi Chen, Zidong Du, Xing Hu, Qi Guo, Ling Li, Yunji Chen</li>
<li>for: 提高 meta-学习 agent 的通用能力，采用 prep-collected  offline 数据。</li>
<li>methods: 提出了一种 Context Shift Reduction for OMRL (CSRO) 方法，通过 minimize 策略在上下文中的影响来解决上下文偏移问题。</li>
<li>results: CSRO 方法在多种复杂的领域中显著减少了上下文偏移，并超过了之前的方法，提高了通用能力。<details>
<summary>Abstract</summary>
Offline meta-reinforcement learning (OMRL) utilizes pre-collected offline datasets to enhance the agent's generalization ability on unseen tasks. However, the context shift problem arises due to the distribution discrepancy between the contexts used for training (from the behavior policy) and testing (from the exploration policy). The context shift problem leads to incorrect task inference and further deteriorates the generalization ability of the meta-policy. Existing OMRL methods either overlook this problem or attempt to mitigate it with additional information. In this paper, we propose a novel approach called Context Shift Reduction for OMRL (CSRO) to address the context shift problem with only offline datasets. The key insight of CSRO is to minimize the influence of policy in context during both the meta-training and meta-test phases. During meta-training, we design a max-min mutual information representation learning mechanism to diminish the impact of the behavior policy on task representation. In the meta-test phase, we introduce the non-prior context collection strategy to reduce the effect of the exploration policy. Experimental results demonstrate that CSRO significantly reduces the context shift and improves the generalization ability, surpassing previous methods across various challenging domains.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>偏向式 meta-学习（OMRL）利用预收集的偏向式数据集来提高代理人的通用能力。然而，Context Shift问题出现，这是因为培育（从行为策略）和测试（从探索策略）上的分布差异。Context Shift问题会导致任务推断错误，并进一步削弱代理人的通用能力。现有的 OMRL 方法 Either overlook this problem or attempt to mitigate it with additional information。在这篇论文中，我们提出了一种新的方法，即 Context Shift Reduction for OMRL（CSRO），用于解决 Context Shift 问题。CSRO 的关键思想是在 meta-training 和 meta-test 阶段都减少策略的影响。在 meta-training 阶段，我们设计了 max-min 互信息表示学习机制，以减少行为策略对任务表示的影响。在 meta-test 阶段，我们引入了非先验Context Collection策略，以减少探索策略的影响。实验结果表明，CSRO 可以减少 Context Shift 并提高通用能力，超过了先前的方法，在多个复杂的领域中。
</details></li>
</ul>
<hr>
<h2 id="Deep-Bayesian-Reinforcement-Learning-for-Spacecraft-Proximity-Maneuvers-and-Docking"><a href="#Deep-Bayesian-Reinforcement-Learning-for-Spacecraft-Proximity-Maneuvers-and-Docking" class="headerlink" title="Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking"></a>Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03680">http://arxiv.org/abs/2311.03680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Desong Du, Naiming Qi, Yanfang Liu, Wei Pan</li>
<li>for: 这篇论文是为了研究自主宇宙航行器的近距离推进和对接（PMD）。</li>
<li>methods: 这篇论文使用了一种新的 bayesian actor-critic reinforcement learning算法，以学习一个具有稳定保证的控制策略。</li>
<li>results: 实验结果显示，这种算法在一个宇宙航行器的空气滑床试验平台上表现出色，并且具有优异的稳定性和安全性。<details>
<summary>Abstract</summary>
In the pursuit of autonomous spacecraft proximity maneuvers and docking(PMD), we introduce a novel Bayesian actor-critic reinforcement learning algorithm to learn a control policy with the stability guarantee. The PMD task is formulated as a Markov decision process that reflects the relative dynamic model, the docking cone and the cost function. Drawing from the principles of Lyapunov theory, we frame the temporal difference learning as a constrained Gaussian process regression problem. This innovative approach allows the state-value function to be expressed as a Lyapunov function, leveraging the Gaussian process and deep kernel learning. We develop a novel Bayesian quadrature policy optimization procedure to analytically compute the policy gradient while integrating Lyapunov-based stability constraints. This integration is pivotal in satisfying the rigorous safety demands of spaceflight missions. The proposed algorithm has been experimentally evaluated on a spacecraft air-bearing testbed and shows impressive and promising performance.
</details>
<details>
<summary>摘要</summary>
在自主空间舱靠近和停靠（PMD）任务中，我们介绍了一种新的 Bayesianactor-critic reinforcement学习算法，以学习一个稳定性保证的控制策略。 PMD任务被形式化为一个Markov决策过程，这个过程反映了相对动态模型、停靠杯和成本函数。 我们从了Lyapunov理论的原则中继承了时间差学习，将其转化为一个受限的 Gaussian Process regression问题。这种创新的方法使得状态价值函数可以表示为Lyapunov函数，通过Gaussian Process和深度核心学习。我们开发了一种新的 Bayesian quadrature策略优化程序，可以分析计算策略偏导的同时，integrate Lyapunov-based稳定性约束。这种约束是Spaceflight任务的严格安全要求的满足。我们的提案在一个空间舱空滤测试平台上进行了实验，表现很出色和有前途。
</details></li>
</ul>
<hr>
<h2 id="Stable-Modular-Control-via-Contraction-Theory-for-Reinforcement-Learning"><a href="#Stable-Modular-Control-via-Contraction-Theory-for-Reinforcement-Learning" class="headerlink" title="Stable Modular Control via Contraction Theory for Reinforcement Learning"></a>Stable Modular Control via Contraction Theory for Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03669">http://arxiv.org/abs/2311.03669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bing Song, Jean-Jacques Slotine, Quang-Cuong Pham</li>
<li>for: 提出了一种新的方法，将控制技术与强化学习（RL）结合起来，以确保稳定性、Robustness和泛化性：利用 contraction theory 实现模块化性，以保证将稳定的子系统组合起来可以保持稳定性。</li>
<li>methods: 通过信号композиiting和动态分解来实现模块化性。信号 compositing 创造了隐藏空间，在这个空间中RL 可以最大化奖励。动态分解通过坐标变换创造了一个辅助空间，在这个空间中 latent signals 被相互关联，以保持稳定性，并且每个信号，即每个子系统，都有稳定的自回传。</li>
<li>results: 通过实验表明，这种方法可以提高机器学习中的模块化 neural architecture 的性能，特别是在杂乱学习中。<details>
<summary>Abstract</summary>
We propose a novel way to integrate control techniques with reinforcement learning (RL) for stability, robustness, and generalization: leveraging contraction theory to realize modularity in neural control, which ensures that combining stable subsystems can automatically preserve the stability. We realize such modularity via signal composition and dynamic decomposition. Signal composition creates the latent space, within which RL applies to maximizing rewards. Dynamic decomposition is realized by coordinate transformation that creates an auxiliary space, within which the latent signals are coupled in the way that their combination can preserve stability provided each signal, that is, each subsystem, has stable self-feedbacks. Leveraging modularity, the nonlinear stability problem is deconstructed into algebraically solvable ones, the stability of the subsystems in the auxiliary space, yielding linear constraints on the input gradients of control networks that can be as simple as switching the signs of network weights. This minimally invasive method for stability allows arguably easy integration into the modular neural architectures in machine learning, like hierarchical RL, and improves their performance. We demonstrate in simulation the necessity and the effectiveness of our method: the necessity for robustness and generalization, and the effectiveness in improving hierarchical RL for manipulation learning.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，把控制技术与强化学习（RL）结合起来，以确保稳定性、可靠性和通用性：利用收缩理论来实现模块化在神经控制中，以确保将稳定的子系统组合在一起可以保持稳定性。我们通过信号组合和动态分解来实现这种模块化。信号组合创造了隐藏空间，在这个空间中RL可以最大化奖励。动态分解通过坐标变换创建了一个辅助空间，在这个空间中隐藏信号被 coupling在一起，以确保其组合可以保持稳定性，只要每个信号，即每个子系统，都有稳定的自反馈。利用模块化，非线性稳定性问题被分解成可解的问题，即每个子系统的稳定性问题在辅助空间中，从而得到了输入梯度的线性约束，这些约束可以是简单地将网络权重的符号 switching。这种非侵入式的稳定性方法可以轻松地 integrate到现有的模块化神经网络 architecture中，如 hierarchical RL，并提高其性能。我们在 simulating 中示出了这种方法的必要性和有效性：必要性为稳定性和通用性，有效性在 hierarchical RL 中提高 manipulation learning 性能。
</details></li>
</ul>
<hr>
<h2 id="GPT-ST-Generative-Pre-Training-of-Spatio-Temporal-Graph-Neural-Networks"><a href="#GPT-ST-Generative-Pre-Training-of-Spatio-Temporal-Graph-Neural-Networks" class="headerlink" title="GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks"></a>GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04245">http://arxiv.org/abs/2311.04245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/gpt-st">https://github.com/hkuds/gpt-st</a></li>
<li>paper_authors: Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang</li>
<li>for: 本文旨在提出一种spatio-temporal预测框架，以提高流量管理和旅行规划中的预测性能。</li>
<li>methods: 该框架基于两个关键设计：一是提出了一种spatio-temporal做mask自适应网络，用于学习spatio-temporal相关性。二是引入了适应式做mask策略，以便在训练过程中容易地模型不同关系，从易到difficult。</li>
<li>results: 经验表明，提出的方法可以在 reprehensible benchmarks 上显著提高预测性能。模型实现已经公开在 GitHub 上，具体请参考 <a target="_blank" rel="noopener" href="https://github.com/HKUDS/GPT-ST">https://github.com/HKUDS/GPT-ST</a>。<details>
<summary>Abstract</summary>
In recent years, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs: (i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches. (ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner. Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at https://github.com/HKUDS/GPT-ST.
</details>
<details>
<summary>摘要</summary>
Recently, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs:(i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches.(ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner.Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at <https://github.com/HKUDS/GPT-ST>.
</details></li>
</ul>
<hr>
<h2 id="The-Linear-Representation-Hypothesis-and-the-Geometry-of-Large-Language-Models"><a href="#The-Linear-Representation-Hypothesis-and-the-Geometry-of-Large-Language-Models" class="headerlink" title="The Linear Representation Hypothesis and the Geometry of Large Language Models"></a>The Linear Representation Hypothesis and the Geometry of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03658">http://arxiv.org/abs/2311.03658</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kihopark/linear_rep_geometry">https://github.com/kihopark/linear_rep_geometry</a></li>
<li>paper_authors: Kiho Park, Yo Joong Choe, Victor Veitch</li>
<li>for: 本文研究了 linear representation hypothesis，即高级概念是线性表示的想法。</li>
<li>methods: 本文使用 counterfactuals 来给出两种形式化 linear representation，一种是输出（词）表示空间，另一种是输入（句子）空间。然后，用这些形式化连接到线性探针和模型导航。</li>
<li>results:  experiments 表明，存在 linear representation of concepts，与解释和控制之间的联系，以及内积的选择对于语言结构的影响。<details>
<summary>Abstract</summary>
Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.
</details>
<details>
<summary>摘要</summary>
促进式地，“线性表示假设”是指高级概念被表示为某种表示空间中的直线方向。在这篇论文中，我们考虑了两个相关的问题：“线性表示”的具体含义是什么？以及在表示空间中 geometric 概念（例如cosine similarity或投影）是如何理解的？为了回答这些问题，我们使用 counterfactual 语言来给出两种形式化“线性表示”的方法，一种在输出（词）表示空间中，另一种在输入（句子）空间中。然后，我们证明这些相关到线性探测和模型导航。为了理解 geometric 概念，我们使用形式化来确定一种特殊的非欧几何内积，该内积尊重语言结构。使用这种 causal 内积，我们可以将所有的线性表示统一起来。具体来说，这允许我们使用 counterfactual 对来建立探测和导航 vectors。 experiments with LLaMA-2 表明存在线性概念表示，连接到解释和控制，以及内积的选择对结果的重要性。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Parameterization-of-the-Multi-scale-Kain-Fritsch-MSKF-Convection-Scheme"><a href="#Machine-Learning-Parameterization-of-the-Multi-scale-Kain-Fritsch-MSKF-Convection-Scheme" class="headerlink" title="Machine Learning Parameterization of the Multi-scale Kain-Fritsch (MSKF) Convection Scheme"></a>Machine Learning Parameterization of the Multi-scale Kain-Fritsch (MSKF) Convection Scheme</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03652">http://arxiv.org/abs/2311.03652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaohui Zhong, Xing Yu, Hao Li</li>
<li>for: 这个研究旨在测试机器学习模型是否能取代传统的物理参数化方法，以提高预报高精度降水事件的精度。</li>
<li>methods: 本研究使用了多出力对称长短时间记忆过程（Bi-LSTM）模型，并与WRF模型进行组合使用，以测试其在预报高精度降水事件中的性能。</li>
<li>results: 研究结果显示，Bi-LSTM模型可以实现高精度预报，表明机器学习模型可以取代传统的物理参数化方法，从而提高预报高精度降水事件的精度。<details>
<summary>Abstract</summary>
Warm-sector heavy rainfall often occurs along the coast of South China, and it is usually localized and long-lasting, making it challenging to predict. High-resolution numerical weather prediction (NWP) models are increasingly used to better resolve topographic features and forecast such high-impact weather events. However, when the grid spacing becomes comparable to the length scales of convection, known as the gray zone, the turbulent eddies in the atmospheric boundary layer are only partially resolved and parameterized to some extent. Whether using a convection parameterization (CP) scheme in the gray zone remains controversial. Scale-aware CP schemes are developed to enhance the representation of convective transport within the gray zone. The multi-scale Kain-Fritsch (MSKF) scheme includes modifications that allow for its effective implementation at a grid resolution as high as 2 km. In recent years, there has been an increasing application of machine learning (ML) models to various domains of atmospheric sciences, including the replacement of physical parameterizations with ML models. This work proposes a multi-output bidirectional long short-term memory (Bi-LSTM) model as a replace the scale-aware MSKF CP scheme. The Weather Research and Forecast (WRF) model is used to generate training and testing data over South China at a horizontal resolution of 5 km. Furthermore, the WRF model is coupled with the ML based CP scheme and compared with WRF simulations with original MSKF scheme. The results demonstrate that the Bi-LSTM model can achieve high accuracy, indicating the potential use of ML models to substitute the MSKF scheme in the gray zone.
</details>
<details>
<summary>摘要</summary>
暖区重雨常发生在南中国海岸，通常是局部化和长时间的，预测具有挑战性。高分辨率数值天气预测（NWP）模型在预测这种高影响天气事件方面日益被使用。然而，当网格间距相当于降水径流的尺度时，即灰色区，气层的湍流辐射只部分解决并受到一定程度的参数化。使用湍流参数化（CP）方案在灰色区是有争议的。Scale-aware CP方案可以增强湍流传输的表示。多Scale Kain-Fritsch（MSKF）方案包括修改，以实现高分辨率（2 km）下的有效实施。在过去几年中，机器学习（ML）模型在大气科学领域的应用不断扩大，其中包括将物理参数化被替换为ML模型。本研究提出了一种多输出Bi-LSTM模型，用于取代scale-aware MSKF CP方案。使用Weather Research and Forecast（WRF）模型生成训练和测试数据，并将WRF模型与ML基于CP方案相couple。结果表明，Bi-LSTM模型可以实现高精度， indicating the potential use of ML models to substitute the MSKF scheme in the gray zone.
</details></li>
</ul>
<hr>
<h2 id="SeRO-Self-Supervised-Reinforcement-Learning-for-Recovery-from-Out-of-Distribution-Situations"><a href="#SeRO-Self-Supervised-Reinforcement-Learning-for-Recovery-from-Out-of-Distribution-Situations" class="headerlink" title="SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations"></a>SeRO: Self-Supervised Reinforcement Learning for Recovery from Out-of-Distribution Situations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03651">http://arxiv.org/abs/2311.03651</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snuchankim/sero">https://github.com/snuchankim/sero</a></li>
<li>paper_authors: Chan Kim, Jaekyung Cho, Christophe Bobda, Seung-Woo Seo, Seong-Woo Kim</li>
<li>for: 提高机器人代理人在异常状态下的可靠性</li>
<li>methods: 使用自我超级vised学习方法重新训练机器人，以便在异常状态下恢复原来的任务性能</li>
<li>results: 实验结果表明，我们的方法可以大幅提高机器人在异常状态下的恢复能力，并且可以在难以探索的原来状态下恢复原来的任务性能<details>
<summary>Abstract</summary>
Robotic agents trained using reinforcement learning have the problem of taking unreliable actions in an out-of-distribution (OOD) state. Agents can easily become OOD in real-world environments because it is almost impossible for them to visit and learn the entire state space during training. Unfortunately, unreliable actions do not ensure that agents perform their original tasks successfully. Therefore, agents should be able to recognize whether they are in OOD states and learn how to return to the learned state distribution rather than continue to take unreliable actions. In this study, we propose a novel method for retraining agents to recover from OOD situations in a self-supervised manner when they fall into OOD states. Our in-depth experimental results demonstrate that our method substantially improves the agent's ability to recover from OOD situations in terms of sample efficiency and restoration of the performance for the original tasks. Moreover, we show that our method can retrain the agent to recover from OOD situations even when in-distribution states are difficult to visit through exploration.
</details>
<details>
<summary>摘要</summary>
机器人代理人使用强化学习训练后可能会面临不可靠的行为在非典型（Out-of-distribution，OOD）状态下。因为实际环境中几乎不可能让代理人在训练过程中访问整个状态空间，因此代理人容易进入OOD状态。然而，不可靠的行为并不能确保代理人完成原始任务成功。因此，代理人应该能够识别自己是否处于OOD状态，并学习如何返回学习过的状态分布而不是继续执行不可靠的行为。在这个研究中，我们提出了一种新的自动重新训练代理人从OOD状态中恢复原始任务的方法。我们的实验结果表明，我们的方法可以在样本效率和原始任务性能恢复方面大幅提高代理人的恢复能力。此外，我们还证明了我们的方法可以在困难到达的输入分布下重新训练代理人恢复原始任务。
</details></li>
</ul>
<hr>
<h2 id="HKTGNN-Hierarchical-Knowledge-Transferable-Graph-Neural-Network-based-Supply-Chain-Risk-Assessment"><a href="#HKTGNN-Hierarchical-Knowledge-Transferable-Graph-Neural-Network-based-Supply-Chain-Risk-Assessment" class="headerlink" title="HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based Supply Chain Risk Assessment"></a>HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based Supply Chain Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04244">http://arxiv.org/abs/2311.04244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanting Zhou, Kejun Bi, Yuyanzhen Zhong, Chao Tang, Dongfen Li, Shi Ying, Ruijin Wang</li>
<li>for: 这个论文主要目标是提出一种基于图 neural network 的供应链风险评估模型，以便更好地管理和mitigate 供应链中的潜在风险。</li>
<li>methods: 该模型使用 hierarchical knowledge transferable graph neural network (HKTGNN) 方法，基于当前的图嵌入方法来评估供应链网络中各个产品的风险。</li>
<li>results: 实验结果显示，该模型在一个真实的供应链数据集上表现出excel，并且我们会给出一个公式来证明这个比较实验是有效和公平的。<details>
<summary>Abstract</summary>
The strength of a supply chain is an important measure of a country's or region's technical advancement and overall competitiveness. Establishing supply chain risk assessment models for effective management and mitigation of potential risks has become increasingly crucial. As the number of businesses grows, the important relationships become more complicated and difficult to measure. This emphasizes the need of extracting relevant information from graph data. Previously, academics mostly employed knowledge inference to increase the visibility of links between nodes in the supply chain. However, they have not solved the data hunger problem of single node feature characteristics. We propose a hierarchical knowledge transferable graph neural network-based (HKTGNN) supply chain risk assessment model to address these issues. Our approach is based on current graph embedding methods for assessing corporate investment risk assessment. We embed the supply chain network corresponding to individual goods in the supply chain using the graph embedding module, resulting in a directed homogeneous graph with just product nodes. This reduces the complicated supply chain network into a basic product network. It addresses difficulties using the domain difference knowledge transferable module based on centrality, which is presented by the premise that supply chain feature characteristics may be biased in the actual world. Meanwhile, the feature complement and message passing will alleviate the data hunger problem, which is driven by domain differences. Our model outperforms in experiments on a real-world supply chain dataset. We will give an equation to prove that our comparative experiment is both effective and fair.
</details>
<details>
<summary>摘要</summary>
供应链的强度是一个国家或地区的技术进步和综合竞争力的重要指标。建立供应链风险评估模型，以有效管理和减轻潜在风险已成为核心。随着企业数量的增加，关键关系变得更加复杂和难以测量。这表明需要从图数据中提取相关信息。在过去，学术界主要采用知识推理来增加供应链中节点之间的链接 visibility。然而，它们没有解决单节点特征特性的数据饿问问题。我们提议一种基于现有图 embedding 方法的 hierarchical knowledge transferable graph neural network-based (HKTGNN) 供应链风险评估模型。我们将供应链网络与具体商品相对应的 embedding 模块，从而生成一个指定产品的导向同质图。这将复杂的供应链网络简化为基本的产品网络。此外，我们还采用域差知识传递模块，根据中心性提出，即供应链特征特性在实际世界中可能受到偏见。同时，特征补偿和消息传递将解决数据饿问问题，它是由域差所驱动。我们的模型在实际供应链数据上进行了实验，并证明我们的比较实验是有效和公平的。
</details></li>
</ul>
<hr>
<h2 id="Analysis-of-the-User-Perception-of-Chatbots-in-Education-Using-A-Partial-Least-Squares-Structural-Equation-Modeling-Approach"><a href="#Analysis-of-the-User-Perception-of-Chatbots-in-Education-Using-A-Partial-Least-Squares-Structural-Equation-Modeling-Approach" class="headerlink" title="Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach"></a>Analysis of the User Perception of Chatbots in Education Using A Partial Least Squares Structural Equation Modeling Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03636">http://arxiv.org/abs/2311.03636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Rabiul Hasan, Nahian Ismail Chowdhury, Md Hadisur Rahman, Md Asif Bin Syed, JuHyeong Ryu<br>for:This study aims to investigate the determinants of chatbot adoption in education among students, specifically looking at the Technology Readiness Index (TRI) and Technology Acceptance Model (TAM).methods:The study uses Partial Least Squares Structural Equation Modeling (PLS-SEM) to analyze data collected from 185 responses using a five-point Likert scale.results:The results show that Optimism and Innovativeness are positively associated with Perceived Ease of Use (PEOU) and Perceived Usefulness (PU), while Discomfort and Insecurity negatively impact PEOU, with only Insecurity negatively affecting PU. These findings provide insights for future technology designers, highlighting critical user behavior factors that influence chatbot adoption and utilization in educational contexts.Here are the three points in Simplified Chinese text:for:这项研究目标是调查学生在教育中对聊天机器人的采用，具体来说是通过技术准备指数(TRI)和技术接受模型(TAM)来 investigate。methods:这项研究使用分解部分最小二乘方法(PLS-SEM)分析来收集的185个回答。results:结果显示，乐观和创新性对使用感和有用性有积极的关系，而不适和不安全对使用感有负面的影响，只有不安全对使用感有负面的影响。这些发现为未来技术设计师提供了指导，抛出了关键用户行为因素，帮助我们更好地理解聊天机器人在教育上的采用和利用。<details>
<summary>Abstract</summary>
The integration of Artificial Intelligence (AI) into education is a recent development, with chatbots emerging as a noteworthy addition to this transformative landscape. As online learning platforms rapidly advance, students need to adapt swiftly to excel in this dynamic environment. Consequently, understanding the acceptance of chatbots, particularly those employing Large Language Model (LLM) such as Chat Generative Pretrained Transformer (ChatGPT), Google Bard, and other interactive AI technologies, is of paramount importance. However, existing research on chatbots in education has overlooked key behavior-related aspects, such as Optimism, Innovativeness, Discomfort, Insecurity, Transparency, Ethics, Interaction, Engagement, and Accuracy, creating a significant literature gap. To address this gap, this study employs Partial Least Squares Structural Equation Modeling (PLS-SEM) to investigate the determinant of chatbots adoption in education among students, considering the Technology Readiness Index (TRI) and Technology Acceptance Model (TAM). Utilizing a five-point Likert scale for data collection, we gathered a total of 185 responses, which were analyzed using R-Studio software. We established 12 hypotheses to achieve its objectives. The results showed that Optimism and Innovativeness are positively associated with Perceived Ease of Use (PEOU) and Perceived Usefulness (PU). Conversely, Discomfort and Insecurity negatively impact PEOU, with only Insecurity negatively affecting PU. These findings provide insights for future technology designers, elucidating critical user behavior factors influencing chatbots adoption and utilization in educational contexts.
</details>
<details>
<summary>摘要</summary>
教育领域中人工智能（AI）的 интеграción是一个最近的发展，聊天机器人（chatbot）是这一变革的一个卓越的例子。在在线学习平台上快速发展的情况下，学生需要快速适应以保持优异的表现。因此，理解学生对聊天机器人的接受度，特别是使用大型语言模型（LLM）的聊天机器人，如Chat Generative Pretrained Transformer（ChatGPT）、Google Bard等交互式AI技术的接受度，是极其重要的。然而，现有的教育领域聊天机器人研究忽略了关键的行为相关因素，如乐观性、创新性、不适、不安全、透明度、伦理、互动、参与度和准确性，这创造了一个重要的文献差距。为了填补这一差距，本研究使用部分最小二乘结构方程（PLS-SEM）调查学生对教育领域聊天机器人的采用，考虑技术准备指数（TRI）和技术acceptance模型（TAM）。通过五点Ликер分scale收集数据，总共收集了185个答案，并使用RStudio软件分析。我们建立了12个假设来实现我们的目标。结果表明，乐观性和创新性 positively关联使用容易度（PEOU）和有用性（PU）。然而，不适和不安全都对PEOUnegatively影响，只有不安全对PUnegatively影响。这些发现为未来技术设计师提供了新的洞察，揭示了在教育上下文中聊天机器人采用和使用的关键用户行为因素。
</details></li>
</ul>
<hr>
<h2 id="TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer"><a href="#TWIST-Teacher-Student-World-Model-Distillation-for-Efficient-Sim-to-Real-Transfer" class="headerlink" title="TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer"></a>TWIST: Teacher-Student World Model Distillation for Efficient Sim-to-Real Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03622">http://arxiv.org/abs/2311.03622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Yamada, Marc Rigter, Jack Collins, Ingmar Posner<br>for:This paper aims to address the sim-to-real gap in model-based reinforcement learning (RL) for vision-based robotics tasks.methods:The proposed method, TWIST, uses teacher-student distillation to transfer a simulator-trained world model to a real-world environment. The teacher model is trained on state observations, while the student model is trained on domain-randomized image observations with supervision from the teacher model.results:Experiments on simulated and real robotics tasks show that TWIST outperforms naive domain randomization and model-free methods in terms of sample efficiency and task performance for sim-to-real transfer.<details>
<summary>Abstract</summary>
Model-based RL is a promising approach for real-world robotics due to its improved sample efficiency and generalization capabilities compared to model-free RL. However, effective model-based RL solutions for vision-based real-world applications require bridging the sim-to-real gap for any world model learnt. Due to its significant computational cost, standard domain randomisation does not provide an effective solution to this problem. This paper proposes TWIST (Teacher-Student World Model Distillation for Sim-to-Real Transfer) to achieve efficient sim-to-real transfer of vision-based model-based RL using distillation. Specifically, TWIST leverages state observations as readily accessible, privileged information commonly garnered from a simulator to significantly accelerate sim-to-real transfer. Specifically, a teacher world model is trained efficiently on state information. At the same time, a matching dataset is collected of domain-randomised image observations. The teacher world model then supervises a student world model that takes the domain-randomised image observations as input. By distilling the learned latent dynamics model from the teacher to the student model, TWIST achieves efficient and effective sim-to-real transfer for vision-based model-based RL tasks. Experiments in simulated and real robotics tasks demonstrate that our approach outperforms naive domain randomisation and model-free methods in terms of sample efficiency and task performance of sim-to-real transfer.
</details>
<details>
<summary>摘要</summary>
模型基于RL是现实世界机器人控制中有前途的方法，因其在样本效率和泛化能力方面比模型自由RL更高。然而，实际世界应用中的视觉基于模型基于RL解决方案需要跨 simulate-to-real 跳跃，以便使用世界模型学习。由于其计算成本较高，标准的DomainRandomization 不能提供有效的解决方案。这篇论文提出了TWIST（教师学生世界模型填充传播），以实现高效的 simulate-to-real 传播。具体来说，TWIST 利用状态观察得到的 readily accessible 特权信息，通常来自 simulate 器，以加速 simulate-to-real 传播。具体来说，一个教师世界模型在状态信息上高效地训练。同时，一个匹配的数据集被收集了，用于DomainRandomization 的图像观察。教师世界模型然后监督学生世界模型，使用图像观察作为输入。通过填充学习的秘密动力模型从教师模型传播给学生模型，TWIST 实现了高效和有效的 simulate-to-real 传播。实验表明，我们的方法比Naive DomainRandomization 和模型自由方法在样本效率和实际世界传播中的任务性能方面表现出色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.AI_2023_11_07/" data-id="closbrolb00710g889lwi99xj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.CL_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T11:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.CL_2023_11_07/">cs.CL - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Watermarks-in-the-Sand-Impossibility-of-Strong-Watermarking-for-Generative-Models"><a href="#Watermarks-in-the-Sand-Impossibility-of-Strong-Watermarking-for-Generative-Models" class="headerlink" title="Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models"></a>Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04378">http://arxiv.org/abs/2311.04378</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlzhang109/impossibility-watermark">https://github.com/hlzhang109/impossibility-watermark</a></li>
<li>paper_authors: Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, Boaz Barak</li>
<li>for:  This paper studies the (im)possibility of strong watermarking schemes for generative models.</li>
<li>methods:  The paper introduces a generic efficient watermark attack that can remove watermarks planted by existing schemes with only minor quality degradation. The attack is based on two assumptions: access to a “quality oracle” and “perturbation oracle”.</li>
<li>results:  The paper proves that strong watermarking is impossible to achieve under well-specified and natural assumptions, even in the private detection algorithm setting. The attack is demonstrated to be feasible by instantiating it to attack three existing watermarking schemes for large language models.<details>
<summary>Abstract</summary>
Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a "perturbation oracle" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
</details>
<details>
<summary>摘要</summary>
水印生成模型的水印是在模型的输出中植入一个统计信号，以便后续可以验证输出是由该模型生成的。一个强大的水印方案需要在计算限制的攻击者无法完全移除水印而导致质量下降的情况下保持水印。在这篇论文中，我们研究水印方案的可能性。我们证明，在我们提出的具体和自然假设下，强大的水印方案是不可能实现的。这种结论是even在私人探测算法设置下也成立，在涉及私钥的情况下，攻击者无法获得私钥。为证明这一结论，我们提出了一种通用的高效水印攻击方法。攻击者不需要知道私钥或使用哪种方案。我们的攻击方法基于以下两个假设：（1）攻击者有访问一个"质量套件"，可以评估提示的输出是否为高质量回答，（2）攻击者有访问一个"杂化套件"，可以对输出进行非正式的修改，并且可以在高质量输出上引入高效混杂的随机漫步。我们 argue That both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, and that our assumptions will likely only become easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-multiple-large-language-models-in-pediatric-ophthalmology"><a href="#Evaluating-multiple-large-language-models-in-pediatric-ophthalmology" class="headerlink" title="Evaluating multiple large language models in pediatric ophthalmology"></a>Evaluating multiple large language models in pediatric ophthalmology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04368">http://arxiv.org/abs/2311.04368</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jason Holmes, Rui Peng, Yiwei Li, Jinyu Hu, Zhengliang Liu, Zihao Wu, Huan Zhao, Xi Jiang, Wei Liu, Hong Wei, Jie Zou, Tianming Liu, Yi Shao<br>for: The paper aims to evaluate the performance of large language models (LLMs) in pediatric ophthalmology consultations and compare their performance with medical students and physicians at different levels.methods: The study uses a 100-question exam based on pediatric ophthalmology to assess the performance of three LLMs (ChatGPT, GPT-4, and PaLM2) and three human cohorts (medical students, postgraduate students, and attending physicians).results: GPT-4 performs comparably to attending physicians, while ChatGPT (GPT-3.5) and PaLM2 outperform medical students but slightly trail behind postgraduate students. GPT-4 also exhibits greater stability and confidence in responding to inquiries compared to the other two LLMs.<details>
<summary>Abstract</summary>
IMPORTANCE The response effectiveness of different large language models (LLMs) and various individuals, including medical students, graduate students, and practicing physicians, in pediatric ophthalmology consultations, has not been clearly established yet. OBJECTIVE Design a 100-question exam based on pediatric ophthalmology to evaluate the performance of LLMs in highly specialized scenarios and compare them with the performance of medical students and physicians at different levels. DESIGN, SETTING, AND PARTICIPANTS This survey study assessed three LLMs, namely ChatGPT (GPT-3.5), GPT-4, and PaLM2, were assessed alongside three human cohorts: medical students, postgraduate students, and attending physicians, in their ability to answer questions related to pediatric ophthalmology. It was conducted by administering questionnaires in the form of test papers through the LLM network interface, with the valuable participation of volunteers. MAIN OUTCOMES AND MEASURES Mean scores of LLM and humans on 100 multiple-choice questions, as well as the answer stability, correlation, and response confidence of each LLM. RESULTS GPT-4 performed comparably to attending physicians, while ChatGPT (GPT-3.5) and PaLM2 outperformed medical students but slightly trailed behind postgraduate students. Furthermore, GPT-4 exhibited greater stability and confidence when responding to inquiries compared to ChatGPT (GPT-3.5) and PaLM2. CONCLUSIONS AND RELEVANCE Our results underscore the potential for LLMs to provide medical assistance in pediatric ophthalmology and suggest significant capacity to guide the education of medical students.
</details>
<details>
<summary>摘要</summary>
重要性：不同的大型自然语言模型（LLM）和各种个人，包括医学生、硬件学生和实践医生，在педиатрия眼科咨询中的回应效果没有得到明确定义。目标：设计一份100题测验，用于评估不同的LLM在特殊化场景中的表现，并与医学生和医生不同水平的表现进行比较。设计、场景和参与者：这项调查研究分别评估了三个LLM： namely ChatGPT（GPT-3.5）、GPT-4和PaLM2，与三个人类团体：医学生、硬件学生和实践医生，在 Pediatric Ophthalmology 方面的问题回答能力。通过LLM网络 интерフェース传达测验纸，经过志愿者的参与。主要结果和测量：LLM和人类的平均分数100个多选题，以及每个LLM的回答稳定性、相关性和回答自信度。结论和重要性：我们的结果表明LLM可以在 Pediatric Ophthalmology 中提供医疗帮助，并表明LLM可以导导医学生的教育。
</details></li>
</ul>
<hr>
<h2 id="Syntax-Guided-Transformers-Elevating-Compositional-Generalization-and-Grounding-in-Multimodal-Environments"><a href="#Syntax-Guided-Transformers-Elevating-Compositional-Generalization-and-Grounding-in-Multimodal-Environments" class="headerlink" title="Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments"></a>Syntax-Guided Transformers: Elevating Compositional Generalization and Grounding in Multimodal Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04364">http://arxiv.org/abs/2311.04364</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hlr/syntax-guided-transformers">https://github.com/hlr/syntax-guided-transformers</a></li>
<li>paper_authors: Danial Kamali, Parisa Kordjamshidi</li>
<li>for: 本研究旨在解决AI模型在多modal环境中的组合普适泛化问题，特别是通过语言 syntax 结构来提高组合普适泛化能力。</li>
<li>methods: 本研究使用了 attention masking 技术， derivated from text input parsing，来强化语言 syntax 的挂钩。</li>
<li>results: 研究表明，通过使用语言 syntax 信息可以提高多modal grounding 问题的性能，并在多种任务上达到新的州OF-the-art。<details>
<summary>Abstract</summary>
Compositional generalization, the ability of intelligent models to extrapolate understanding of components to novel compositions, is a fundamental yet challenging facet in AI research, especially within multimodal environments. In this work, we address this challenge by exploiting the syntactic structure of language to boost compositional generalization. This paper elevates the importance of syntactic grounding, particularly through attention masking techniques derived from text input parsing. We introduce and evaluate the merits of using syntactic information in the multimodal grounding problem. Our results on grounded compositional generalization underscore the positive impact of dependency parsing across diverse tasks when utilized with Weight Sharing across the Transformer encoder. The results push the state-of-the-art in multimodal grounding and parameter-efficient modeling and provide insights for future research.
</details>
<details>
<summary>摘要</summary>
“组成总结”——人工智能模型对新组合的理解推广——是人工智能研究中的基本 yet 挑战性问题，尤其在多模态环境中。在这种情况下，我们利用语言的语法结构来提高组成总结的能力。我们提出并评估了使用语法信息进行多模态固定问题的解决方案。我们的结果表明，在多模态情境下，使用依赖分析技术可以提高权重共享TransformerEncoder的性能，并提高参数效率。这些结果对多模态固定和参数效率的研究提供了新的思路和洞察。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Causal-Variables-in-Transformers-using-Circuit-Probing"><a href="#Uncovering-Causal-Variables-in-Transformers-using-Circuit-Probing" class="headerlink" title="Uncovering Causal Variables in Transformers using Circuit Probing"></a>Uncovering Causal Variables in Transformers using Circuit Probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04354">http://arxiv.org/abs/2311.04354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlepori1/circuit_probing">https://github.com/mlepori1/circuit_probing</a></li>
<li>paper_authors: Michael A. Lepori, Thomas Serre, Ellie Pavlick</li>
<li>for: 了解神经网络模型中的算法实现是如何工作的，以及模型是如何处理特定任务的。</li>
<li>methods: 使用新的分析技术——电路探钻，自动找到神经网络中假设的低级别电路，并通过针对模型参数进行targeted ablation来进行 causal 分析。</li>
<li>results: 在简单的数学任务上训练神经网络模型后，使用电路探钻技术可以：（1）解读神经网络模型学习的算法，（2）揭示神经网络模型的模块结构，（3）跟踪模型训练过程中电路的发展。并且与其他分析方法进行比较，电路探钻技术在这些实验中几乎与其他方法一样有效，甚至更有效。最后，在一个真实的应用场景中，使用电路探钻技术揭示了GPT2-Small和GPT2-Medium中的主题-动词协调和反射反身协调的电路。<details>
<summary>Abstract</summary>
Neural network models have achieved high performance on a wide variety of complex tasks, but the algorithms that they implement are notoriously difficult to interpret. In order to understand these algorithms, it is often necessary to hypothesize intermediate variables involved in the network's computation. For example, does a language model depend on particular syntactic properties when generating a sentence? However, existing analysis tools make it difficult to test hypotheses of this type. We propose a new analysis technique -- circuit probing -- that automatically uncovers low-level circuits that compute hypothesized intermediate variables. This enables causal analysis through targeted ablation at the level of model parameters. We apply this method to models trained on simple arithmetic tasks, demonstrating its effectiveness at (1) deciphering the algorithms that models have learned, (2) revealing modular structure within a model, and (3) tracking the development of circuits over training. We compare circuit probing to other methods across these three experiments, and find it on par or more effective than existing analysis methods. Finally, we demonstrate circuit probing on a real-world use case, uncovering circuits that are responsible for subject-verb agreement and reflexive anaphora in GPT2-Small and Medium.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Formal-Aspects-of-Language-Modeling"><a href="#Formal-Aspects-of-Language-Modeling" class="headerlink" title="Formal Aspects of Language Modeling"></a>Formal Aspects of Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04329">http://arxiv.org/abs/2311.04329</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Gninos/CIM-With-Transition-Systems">https://github.com/Gninos/CIM-With-Transition-Systems</a></li>
<li>paper_authors: Ryan Cotterell, Anej Svete, Clara Meister, Tianyu Liu, Li Du</li>
<li>for: 这篇论文主要是为了解释大语言模型的数学基础和如何实现。</li>
<li>methods: 本论文使用了形式化的理论方法来描述语言模型的概念和结构。</li>
<li>results: 本论文提供了一个系统的理论基础，以便开发者和研究人员更好地理解和实现大语言模型。<details>
<summary>Abstract</summary>
Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. Consequently, it is important for both developers and researchers alike to understand the mathematical foundations of large language models, as well as how to implement them. These notes are the accompaniment to the theoretical portion of the ETH Z\"urich course on large language models, covering what constitutes a language model from a formal, theoretical perspective.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经成为人工智能领域中最为常见的自然语言处理发明之一。过去半个 décennial，它们的整合到主流自然语言处理工具中，对自然语言处理工具的性能产生了巨大的提升，并进入了人工智能的公共讨论。因此，开发者和研究人员都需要深入了解大型语言模型的数学基础和实现方法。这些笔记是ETH Zurich大学课程《大型语言模型》的理论部分的伴手笔，涵盖了语言模型从形式、理论上的定义和分析。
</details></li>
</ul>
<hr>
<h2 id="Aspect-based-Meeting-Transcript-Summarization-A-Two-Stage-Approach-with-Weak-Supervision-on-Sentence-Classification"><a href="#Aspect-based-Meeting-Transcript-Summarization-A-Two-Stage-Approach-with-Weak-Supervision-on-Sentence-Classification" class="headerlink" title="Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification"></a>Aspect-based Meeting Transcript Summarization: A Two-Stage Approach with Weak Supervision on Sentence Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04292">http://arxiv.org/abs/2311.04292</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongfen Deng, Seunghyun Yoon, Trung Bui, Franck Dernoncourt, Quan Hung Tran, Shuaiqi Liu, Wenting Zhao, Tao Zhang, Yibo Wang, Philip S. Yu</li>
<li>for: 本研究旨在生成多个摘要，每个摘要专注于一个会议笔记中的一个方面。</li>
<li>methods: 我们提出了一种两阶段方法来实现方面基于会议笔记摘要。首先，我们使用一个句子分类器在一个基于AMI corpus的 dataset上进行 pseudo-labeling，以选择与特定方面相关的句子。然后，我们将这些选择的句子 merged 为特定方面的输入，并使用摘要器生成相应的摘要。</li>
<li>results: 我们在AMI corpus上进行实验，与许多强大的基线实现相比，我们的提出的方法表现出色，为方面基于会议笔记摘要提供了有效的解决方案。<details>
<summary>Abstract</summary>
Aspect-based meeting transcript summarization aims to produce multiple summaries, each focusing on one aspect of content in a meeting transcript. It is challenging as sentences related to different aspects can mingle together, and those relevant to a specific aspect can be scattered throughout the long transcript of a meeting. The traditional summarization methods produce one summary mixing information of all aspects, which cannot deal with the above challenges of aspect-based meeting transcript summarization. In this paper, we propose a two-stage method for aspect-based meeting transcript summarization. To select the input content related to specific aspects, we train a sentence classifier on a dataset constructed from the AMI corpus with pseudo-labeling. Then we merge the sentences selected for a specific aspect as the input for the summarizer to produce the aspect-based summary. Experimental results on the AMI corpus outperform many strong baselines, which verifies the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Aspect-based meeting transcript summarization aims to produce multiple summaries, each focusing on one aspect of content in a meeting transcript. It is challenging as sentences related to different aspects can mingle together, and those relevant to a specific aspect can be scattered throughout the long transcript of a meeting. The traditional summarization methods produce one summary mixing information of all aspects, which cannot deal with the above challenges of aspect-based meeting transcript summarization. In this paper, we propose a two-stage method for aspect-based meeting transcript summarization.")</SYS>以下是文本的Simplified Chinese翻译：<SYS>通过生成多个各关注一个方面的内容笔记简要，以解决会议笔记中不同方面的句子可能杂mix在一起，而具体方面的句子可能在会议笔记中散布在多处的挑战。传统的笔记简要方法会混合所有方面的信息，无法处理上述方面基笔要简要的挑战。在本文中，我们提出了一种两个阶段的方法 для方面基笔要简要。首先，我们使用基于AMI词库的假标签来训练句子分类器，以选择与特定方面相关的输入内容。然后，我们将选择的句子 merge为特定方面的输入，以便使用笔记简要器生成相应的方面基笔要简要。实验结果表明，我们的提议方法在AMI词库上表现出色，超过了许多强大的基线。）</SYS>
</details></li>
</ul>
<hr>
<h2 id="Exploring-Recommendation-Capabilities-of-GPT-4V-ision-A-Preliminary-Case-Study"><a href="#Exploring-Recommendation-Capabilities-of-GPT-4V-ision-A-Preliminary-Case-Study" class="headerlink" title="Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study"></a>Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04199">http://arxiv.org/abs/2311.04199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining Hua, Jaeboum Kim<br>for:This paper explores the potential of using Large Multimodal Models (LMMs) in recommendation tasks with visual assistance.methods:The authors use GPT-4V, a recently released LMM by OpenAI, to assess the quality of its responses within recommendation scenarios. They construct a series of qualitative test samples spanning multiple domains and employ these samples to evaluate GPT-4V’s performance.results:The evaluation results show that GPT-4V has remarkable zero-shot recommendation abilities across diverse domains, thanks to its robust visual-text comprehension capabilities and extensive general knowledge. However, the authors also identify some limitations, including a tendency to provide similar responses when given similar inputs.Here is the same information in Simplified Chinese text:for:这篇论文探讨了使用大型多modal模型（LMMs）在视觉帮助下的推荐任务的可能性。methods:作者使用OpenAI最新发布的GPT-4V模型来评估其在推荐场景中的回答质量。他们构建了覆盖多个领域的质量测试样本，并使用这些样本来评估GPT-4V的性能。results:评估结果表明，GPT-4V在多个领域的零shot推荐任务中表现出色，归功于它的视觉文本理解能力和广泛的通用知识。然而，作者还发现了一些限制，包括对同类输入提供相同的回答的倾向。<details>
<summary>Abstract</summary>
Large Multimodal Models (LMMs) have demonstrated impressive performance across various vision and language tasks, yet their potential applications in recommendation tasks with visual assistance remain unexplored. To bridge this gap, we present a preliminary case study investigating the recommendation capabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a series of qualitative test samples spanning multiple domains and employ these samples to assess the quality of GPT-4V's responses within recommendation scenarios. Evaluation results on these test samples prove that GPT-4V has remarkable zero-shot recommendation abilities across diverse domains, thanks to its robust visual-text comprehension capabilities and extensive general knowledge. However, we have also identified some limitations in using GPT-4V for recommendations, including a tendency to provide similar responses when given similar inputs. This report concludes with an in-depth discussion of the challenges and research opportunities associated with utilizing GPT-4V in recommendation scenarios. Our objective is to explore the potential of extending LMMs from vision and language tasks to recommendation tasks. We hope to inspire further research into next-generation multimodal generative recommendation models, which can enhance user experiences by offering greater diversity and interactivity. All images and prompts used in this report will be accessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.
</details>
<details>
<summary>摘要</summary>
大型多模式模型（LMM）已经在视觉和语言任务上表现出色，但它们在推荐任务中的应用尚未得到广泛探索。为了填补这一空白，我们提出了一个初步的案例研究，检查GPT-4V（ison），由OpenAI最近发布的一种新的LMM，在推荐任务中的表现。我们构建了覆盖多个领域的质量测试样本，并使用这些样本来评估GPT-4V在推荐场景中的答案质量。评估结果表明，GPT-4V在多个领域的零shot推荐任务中表现出色，归功于它的视觉文本理解能力和广泛的通用知识。然而，我们也发现了使用GPT-4V进行推荐的一些限制，包括对同样输入提供相似的答案的倾向。本报告结束于对使用GPT-4V进行推荐的挑战和研究机遇的深入讨论。我们的目标是探索将LMM从视觉语言任务扩展到推荐任务的可能性，以提高用户体验的多样性和互动性。所有图像和提示用于这份报告将在GitHub上提供，请参考https://github.com/PALIN2018/Evaluate_GPT-4V_Rec。
</details></li>
</ul>
<hr>
<h2 id="JaSPICE-Automatic-Evaluation-Metric-Using-Predicate-Argument-Structures-for-Image-Captioning-Models"><a href="#JaSPICE-Automatic-Evaluation-Metric-Using-Predicate-Argument-Structures-for-Image-Captioning-Models" class="headerlink" title="JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models"></a>JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04192">http://arxiv.org/abs/2311.04192</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keio-smilab23/JaSPICE">https://github.com/keio-smilab23/JaSPICE</a></li>
<li>paper_authors: Yuiga Wada, Kanta Kaneda, Komei Sugiura</li>
<li>for: 本研究的目的是提出一种用于评估日语描述文本的自动评估指标，以提高现有的自动评估指标的准确性。</li>
<li>methods: 本研究使用了依赖关系和 predicate-argument结构生成场景图，并通过同义词扩展场景图。</li>
<li>results: 我们的方法在使用10种基于STAIR Captions和PFN-PIC的图像描述模型，并使用 constructed Shichimi 数据集（包含103,170个人评估）进行实验后，与基准指标相比，显示了更高的相关系数与人工评估。<details>
<summary>Abstract</summary>
Image captioning studies heavily rely on automatic evaluation metrics such as BLEU and METEOR. However, such n-gram-based metrics have been shown to correlate poorly with human evaluation, leading to the proposal of alternative metrics such as SPICE for English; however, no equivalent metrics have been established for other languages. Therefore, in this study, we propose an automatic evaluation metric called JaSPICE, which evaluates Japanese captions based on scene graphs. The proposed method generates a scene graph from dependencies and the predicate-argument structure, and extends the graph using synonyms. We conducted experiments employing 10 image captioning models trained on STAIR Captions and PFN-PIC and constructed the Shichimi dataset, which contains 103,170 human evaluations. The results showed that our metric outperformed the baseline metrics for the correlation coefficient with the human evaluation.
</details>
<details>
<summary>摘要</summary>
研究者强调使用自动评价指标，如BLEU和METEOR，但这些n-gram基于指标与人类评估不符，导致提出了相应的指标，如SPICE（英语）。然而，其他语言没有相应的指标。因此，在这种研究中，我们提议一种自动评价指标，即JaSPICE，用于评估日语描述。该方法根据依赖关系和 predicate-argument结构生成场景图，并使用同义词扩展图。我们在STAIR Captions和PFN-PIC上训练了10种图像描述模型，并构建了Shichimi数据集，该数据集包含103,170个人评估。结果表明，我们的指标与人类评估的相关度高于基准指标。
</details></li>
</ul>
<hr>
<h2 id="SpaDeLeF-A-Dataset-for-Hierarchical-Classification-of-Lexical-Functions-for-Collocations-in-Spanish"><a href="#SpaDeLeF-A-Dataset-for-Hierarchical-Classification-of-Lexical-Functions-for-Collocations-in-Spanish" class="headerlink" title="SpaDeLeF: A Dataset for Hierarchical Classification of Lexical Functions for Collocations in Spanish"></a>SpaDeLeF: A Dataset for Hierarchical Classification of Lexical Functions for Collocations in Spanish</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04189">http://arxiv.org/abs/2311.04189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yevhen Kostiuk, Grigori Sidorov, Olga Kolesnikova</li>
<li>for: 这个论文的目的是为了提出一个基于意义文本理论的语言处理（NLP）中lexical函数的层次分类方法。</li>
<li>methods: 这篇论文使用了一个大量标注数据来训练语言模型，并使用了一种基于树结构的分类目标。</li>
<li>results: 这篇论文提供了一个包含最常见的西班牙语动词-名词 collocation 和它们在文本中出现的句子的数据集，每个 collocation 被分配到了37种lexical函数中的一个类别。<details>
<summary>Abstract</summary>
In natural language processing (NLP), lexical function is a concept to unambiguously represent semantic and syntactic features of words and phrases in text first crafted in the Meaning-Text Theory. Hierarchical classification of lexical functions involves organizing these features into a tree-like hierarchy of categories or labels. This is a challenging task as it requires a good understanding of the context and the relationships among words and phrases in text. It also needs large amounts of labeled data to train language models effectively. In this paper, we present a dataset of most frequent Spanish verb-noun collocations and sentences where they occur, each collocation is assigned to one of 37 lexical functions defined as classes for a hierarchical classification task. Each class represents a relation between the noun and the verb in a collocation involving their semantic and syntactic features. We combine the classes in a tree-based structure, and introduce classification objectives for each level of the structure. The dataset was created by dependency tree parsing and matching of the phrases in Spanish news. We provide baselines and data splits for each objective.
</details>
<details>
<summary>摘要</summary>
在自然语言处理（NLP）领域，lexical function是一个概念，用于不ambiguously表示文本中单词和短语的 semantics和 sintaxis特征。 hierarchical classification of lexical functions involves organizing these features into a tree-like hierarchy of categories or labels. 这是一项具有挑战性的任务，因为它需要对文本中单词和短语之间的关系和语言模型有good understanding。 In addition, it requires large amounts of labeled data to train language models effectively.在这篇论文中，我们提供了最常见的西班牙语动词-名词配合和它们出现的句子，每个配合被分配到37个定义了类的 hierarchical classification任务中。每个类表示在名词和动词之间的关系，包括semantic和syntactic特征。 We combine the classes in a tree-based structure, and introduce classification objectives for each level of the structure. The dataset was created by dependency tree parsing and matching of the phrases in Spanish news. We provide baselines and data splits for each objective.
</details></li>
</ul>
<hr>
<h2 id="Perturbed-examples-reveal-invariances-shared-by-language-models"><a href="#Perturbed-examples-reveal-invariances-shared-by-language-models" class="headerlink" title="Perturbed examples reveal invariances shared by language models"></a>Perturbed examples reveal invariances shared by language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04166">http://arxiv.org/abs/2311.04166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruchit Rawal, Mariya Toneva</li>
<li>for: 本研究的目的是比较两个自然语言处理模型的表现，以了解它们在不同领域中的差异。</li>
<li>methods: 本研究使用了一种新的比较框架，通过描述target一个特定的语言能力（例如同义词变换、字误变换）来揭示模型之间的共同不变性。</li>
<li>results: 研究发现，大型语言模型在多种语言任务中具有许多共同的不变性，而这些不变性只存在于其他大型模型中。这些结果表明，拥有多种不变性可能是大型语言模型的成功的关键因素，并且该框架可以帮助我们理解新模型中 retained 的不变性和 emerge 的不变性。<details>
<summary>Abstract</summary>
An explosion of work in language is leading to ever-increasing numbers of available natural language processing models, with little understanding of how new models compare to better-understood models. One major reason for this difficulty is saturating benchmark datasets, which may not reflect well differences in model performance in the wild. In this work, we propose a novel framework for comparing two natural language processing models by revealing their shared invariance to interpretable input perturbations that are designed to target a specific linguistic capability (e.g., Synonym-Invariance, Typo-Invariance). Via experiments on models from within the same and across different architecture families, this framework offers a number of insights about how changes in models (e.g., distillation, increase in size, amount of pre-training) affect multiple well-defined linguistic capabilities. Furthermore, we also demonstrate how our framework can enable evaluation of the invariances shared between models that are available as commercial black-box APIs (e.g., InstructGPT family) and models that are relatively better understood (e.g., GPT-2). Across several experiments, we observe that large language models share many of the invariances encoded by models of various sizes, whereas the invariances encoded by large language models are only shared by other large models. Possessing a wide variety of invariances may be a key reason for the recent successes of large language models, and our framework can shed light on the types of invariances that are retained by or emerge in new models.
</details>
<details>
<summary>摘要</summary>
“ языковая обработка естественного языка（NLP） 领域正在急速发展，导致大量可用的自然语言处理模型出现，但对这些新模型的理解却很少。一个主要原因是溢出的benchmark数据集，可能不准确反映模型在实际场景中的表现差异。在这项工作中，我们提出了一种新的比较框架，通过揭示模型对可理解的输入扰动的共同不变性（如同onym-invariance、typo-invariance）来比较两个NLP模型。通过对同一architecture家族和不同家族的模型进行实验，我们的框架提供了许多关于模型变化（如distillation、模型大小增加、预训练量）对多种明确的语言能力的影响的反思。此外，我们还示例了如何使用我们的框架评估黑盒API模型（如InstructGPT家族）和比较好理解的模型（如GPT-2）之间的共同不变性。在多个实验中，我们发现大语言模型共享许多不变性，而大模型中的不变性只被其他大模型共享。拥有多种不变性可能是大语言模型的近期成功的关键因素，我们的框架可以探讨这些不变性是如何在新模型中保留或emerge。”
</details></li>
</ul>
<hr>
<h2 id="Black-Box-Prompt-Optimization-Aligning-Large-Language-Models-without-Model-Training"><a href="#Black-Box-Prompt-Optimization-Aligning-Large-Language-Models-without-Model-Training" class="headerlink" title="Black-Box Prompt Optimization: Aligning Large Language Models without Model Training"></a>Black-Box Prompt Optimization: Aligning Large Language Models without Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04155">http://arxiv.org/abs/2311.04155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/bpo">https://github.com/thu-coai/bpo</a></li>
<li>paper_authors: Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang</li>
<li>for: 这个论文的目的是提出一种新的自然语言处理技术，即黑盒子提示优化（BPO），用于调整大型自然语言模型（LLM）的输入理解，以实现用户的意图。</li>
<li>methods: 这个论文使用的方法是黑盒子提示优化（BPO），它是一种模型独立的方法，不需要更新 LLM 的参数。BPO 通过优化用户提交的提示，使 LLM 更好地理解用户的意图。</li>
<li>results: 论文的实验结果表明，使用 BPO 可以提高 ChatGPT 的胜率 by 22%，并且可以超过 PPO 和 DPO 的 alignment 方法。同时，BPO 也可以与 PPO 或 DPO 结合使用，以获得更高的性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them, that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods mostly focus on further training them. However, the extra training of LLMs are usually expensive in terms of GPU compute; worse still, LLMs of interest are oftentimes not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective -- Black-Box Prompt Optimization (BPO) -- to perform alignments. The idea is to optimize user prompts to suit LLMs' input understanding, so as to best realize users' intents without updating LLMs' parameters. BPO is model-agnostic and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version, and 10% for GPT-4. Importantly, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="What-is-Lost-in-Knowledge-Distillation"><a href="#What-is-Lost-in-Knowledge-Distillation" class="headerlink" title="What is Lost in Knowledge Distillation?"></a>What is Lost in Knowledge Distillation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04142">http://arxiv.org/abs/2311.04142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manas Mohanty, Tanya Roosta, Peyman Passban</li>
<li>for: 这个研究旨在investigating how a distilled student model differs from its teacher, and if the distillation process causes any information losses.</li>
<li>methods: 这个研究使用了知识储存（KD）技术来压缩模型，并对其进行分析，以了解压缩过程中是否存在信息损失。</li>
<li>results: 研究发现，压缩过程中的信息损失可以遵循一定的规律，并且不同的任务可能对压缩过程的敏感程度有所不同。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have improved NLP tasks significantly, but training and maintaining such networks could be costly. Model compression techniques, such as, knowledge distillation (KD), have been proposed to address the issue; however, the compression process could be lossy. Motivated by this, our work investigates how a distilled student model differs from its teacher, if the distillation process causes any information losses, and if the loss follows a specific pattern. Our experiments aim to shed light on the type of tasks might be less or more sensitive to KD by reporting data points on the contribution of different factors, such as the number of layers or attention heads. Results such as ours could be utilized when determining effective and efficient configurations to achieve optimal information transfers between larger (teacher) and smaller (student) models.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）已经大幅提高了自然语言处理（NLP）任务的性能，但训练和维护这些网络可能会很昂贵。以知识塑化（KD）为例，模型压缩技术已经被提出来解决这个问题，但压缩过程可能会导致信息损失。我们的工作探讨了压缩学生模型与其教师模型之间的差异，以及压缩过程是否会导致信息损失，以及损失是否遵循某种特定的模式。我们的实验旨在为确定效果和效率的配置提供数据点，以便在更大的教师模型和更小的学生模型之间实现优化的信息传递。我们的结果可能会用于确定适合哪些任务可以更好地承受KD的信息损失。
</details></li>
</ul>
<hr>
<h2 id="Modelling-Sentiment-Analysis-LLMs-and-data-augmentation-techniques"><a href="#Modelling-Sentiment-Analysis-LLMs-and-data-augmentation-techniques" class="headerlink" title="Modelling Sentiment Analysis: LLMs and data augmentation techniques"></a>Modelling Sentiment Analysis: LLMs and data augmentation techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04139">http://arxiv.org/abs/2311.04139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem Senabre Prades</li>
<li>for: 本研究旨在提出一种用于小训练集 binary sentiment classification 的不同方法。</li>
<li>methods: 本研究使用了 LLMS  such as BERT、RoBERTa 和 XLNet，这些模型在 sentiment analysis 和相关领域已经提供了 state-of-the-art 的结果。</li>
<li>results: 研究所得到的结果表明，使用 LLMS 可以在小训练集上实现高度的 binary sentiment classification 性能。<details>
<summary>Abstract</summary>
This paper provides different approaches for a binary sentiment classification on a small training dataset. LLMs that provided state-of-the-art results in sentiment analysis and similar domains are being used, such as BERT, RoBERTa and XLNet.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一些方法用于对小训练集进行二分 sentiment 分类。使用了 LLMS 提供的state-of-the-art 结果，包括 BERT、RoBERTa 和 XLNet。
</details></li>
</ul>
<hr>
<h2 id="Personality-Style-Recognition-via-Machine-Learning-Identifying-Anaclitic-and-Introjective-Personality-Styles-from-Patients’-Speech"><a href="#Personality-Style-Recognition-via-Machine-Learning-Identifying-Anaclitic-and-Introjective-Personality-Styles-from-Patients’-Speech" class="headerlink" title="Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients’ Speech"></a>Personality Style Recognition via Machine Learning: Identifying Anaclitic and Introjective Personality Styles from Patients’ Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04088">http://arxiv.org/abs/2311.04088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Semere Kiros Bitew, Vincent Schelstraete, Klim Zaporojets, Kimberly Van Nieuwenhove, Reitske Meganck, Chris Develder<br>for: 这个研究的目的是用自然语言处理技术和机器学习算法来自动推断患有主要抑郁症的病人的人格类型，以更高的准确率和效果。methods: 这个研究使用了自然语言处理技术和标准的机器学习工具来进行分类，并测试了不同的语言特征和音频特征来推断病人的人格类型。results: 研究发现，使用语言特征（如 LIWC）来进行自动分类的性能明显高于使用问卷来进行分类。此外，结合问卷和语言特征得到的结果最佳。这 suggets that 更多的工作应该投入到开发基于语言特征的自动分类技术，但问卷仍然有一定的补充作用。<details>
<summary>Abstract</summary>
In disentangling the heterogeneity observed in psychopathology, personality of the patients is considered crucial. While it has been demonstrated that personality traits are reflected in the language used by a patient, we hypothesize that this enables automatic inference of the personality type directly from speech utterances, potentially more accurately than through a traditional questionnaire-based approach explicitly designed for personality classification. To validate this hypothesis, we adopt natural language processing (NLP) and standard machine learning tools for classification. We test this on a dataset of recorded clinical diagnostic interviews (CDI) on a sample of 79 patients diagnosed with major depressive disorder (MDD) -- a condition for which differentiated treatment based on personality styles has been advocated -- and classified into anaclitic and introjective personality styles. We start by analyzing the interviews to see which linguistic features are associated with each style, in order to gain a better understanding of the styles. Then, we develop automatic classifiers based on (a) standardized questionnaire responses; (b) basic text features, i.e., TF-IDF scores of words and word sequences; (c) more advanced text features, using LIWC (linguistic inquiry and word count) and context-aware features using BERT (bidirectional encoder representations from transformers); (d) audio features. We find that automated classification with language-derived features (i.e., based on LIWC) significantly outperforms questionnaire-based classification models. Furthermore, the best performance is achieved by combining LIWC with the questionnaire features. This suggests that more work should be put into developing linguistically based automated techniques for characterizing personality, however questionnaires still to some extent complement such methods.
</details>
<details>
<summary>摘要</summary>
在解剖精神疾病中的多样性中，患者的人性被视为关键。我们认为，通过语言来反映患者的人性特征，可以自动获取患者的人性类型，可能更准确地进行人性分类，相比于使用传统的问卷方法进行人性分类。为验证这一假设，我们采用自然语言处理（NLP）和标准的机器学习工具进行分类。我们在一个79名患有主要抑郁症（MDD）的患者群体中进行了实验，并将这些患者分为两个类型：帮助型（anaclitic）和内化型（introjective）。我们首先分析了口述会议，以便了解每个风格的语言特征。然后，我们开发了自动分类器，基于（a）标准问卷回答；（b）基本文本特征（TF-IDF分数）；（c）更高级的文本特征，使用LIWC（语言学和词汇计数）和BERT（端到端Encoder Representations from Transformers）；（d）音频特征。我们发现，基于语言 derive 的特征（即LIWC）自动分类表现出色，超过问卷基本分类模型。此外，将 LIWC 与问卷特征结合使用时，表现最佳。这表明，在发展自动基于语言特征的人性分类技术方面，还需要更多的工作。尽管如此，问卷仍然在一定程度上补充这些方法。
</details></li>
</ul>
<hr>
<h2 id="Do-LLMs-exhibit-human-like-response-biases-A-case-study-in-survey-design"><a href="#Do-LLMs-exhibit-human-like-response-biases-A-case-study-in-survey-design" class="headerlink" title="Do LLMs exhibit human-like response biases? A case study in survey design"></a>Do LLMs exhibit human-like response biases? A case study in survey design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04076">http://arxiv.org/abs/2311.04076</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lindiatjuatja/biasmonkey">https://github.com/lindiatjuatja/biasmonkey</a></li>
<li>paper_authors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig</li>
<li>for: 这个论文旨在检验大型自然语言模型（LLM）是否能够像人类一样受到表达方式的影响，以及这种影响是否会导致模型表现出人类样式的偏见。</li>
<li>methods: 作者使用了评估框架和数据集来研究 LLM 是否会 Display human-like response biases，并 compare 了九种不同的模型。</li>
<li>results: 研究发现，流行的开源和商业 LLM 通常不会模仿人类的行为，而且这种不同性更加明显在 instruction fine-tuned 的模型中。此外，即使模型表现出人类样式的偏见，也可能因为其他各种各样的相关性而导致这种偏见。这些结果表明使用 LLM 代替人类在某些批注阶段可能存在隐患，并且更加重要地，需要更加细化的模型行为Characterization。<details>
<summary>Abstract</summary>
As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording -- but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of ``prompts'' have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior. These inconsistencies tend to be more prominent in models that have been instruction fine-tuned. Furthermore, even if a model shows a significant change in the same direction as humans, we find that perturbations that are not meant to elicit significant changes in humans may also result in a similar change, suggesting that such a result could be partially due to other spurious correlations. These results highlight the potential pitfalls of using LLMs to substitute humans in parts of the annotation pipeline, and further underscore the importance of finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey
</details>
<details>
<summary>摘要</summary>
随着大语言模型（LLM）的能力不断提高，有越来越多的人们对使用LLM作为人类代理的可能性感到激动。然而，一个广泛被提到的障碍是LLM的句子编写敏感性——但是人类也会因为指令变化而产生偏见。因此，如果LLM将被用来 aproximate human opinions，那么必须调查LLM是否也会呈现人类类似的偏见。在这种情况下，我们使用survey设计作为 caso study，因为人类响应变化的研究已经广泛进行。从社会心理学的先前研究中，我们设计了一个数据集和一个框架，以评估LLM是否会展现人类类似的偏见。我们对九个模型进行了全面的评估，发现流行的开源和商业LLM通常不会模拟人类的行为。这些偏差通常在模型被 instruction fine-tuned 时更加明显。此外，即使模型displayed significant changes in the same direction as humans, we found that perturbations that are not meant to elicit significant changes in humans may also result in a similar change, suggesting that such a result could be partially due to other spurious correlations。这些结果 highlights the potential pitfalls of using LLMs to substitute humans in parts of the annotation pipeline, and further underscore the importance of finer-grained characterizations of model behavior。我们的代码、数据集和收集的样本可以在https://github.com/lindiatjuatja/BiasMonkey 中找到。
</details></li>
</ul>
<hr>
<h2 id="Fully-Automated-Task-Management-for-Generation-Execution-and-Evaluation-A-Framework-for-Fetch-and-Carry-Tasks-with-Natural-Language-Instructions-in-Continuous-Space"><a href="#Fully-Automated-Task-Management-for-Generation-Execution-and-Evaluation-A-Framework-for-Fetch-and-Carry-Tasks-with-Natural-Language-Instructions-in-Continuous-Space" class="headerlink" title="Fully Automated Task Management for Generation, Execution, and Evaluation: A Framework for Fetch-and-Carry Tasks with Natural Language Instructions in Continuous Space"></a>Fully Automated Task Management for Generation, Execution, and Evaluation: A Framework for Fetch-and-Carry Tasks with Natural Language Instructions in Continuous Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04260">http://arxiv.org/abs/2311.04260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Motonari Kambara, Komei Sugiura</li>
<li>for: 这篇论文目的是开发一个基于视觉信息的机器人执行任务的框架，以响应自然语言指令进行Fetch-and-Carry with Object Grounding (FCOG)任务。</li>
<li>methods: 本文提出了一种框架，可以自动生成、执行和评估FCOG任务。此外， authors还提出了将FCOG任务分解成四个不同的互斥任务的方法。</li>
<li>results: 本文的实验结果表明，该框架可以自动生成和执行FCOG任务，并且可以在不同的环境和对象下实现高效的任务执行。<details>
<summary>Abstract</summary>
This paper aims to develop a framework that enables a robot to execute tasks based on visual information, in response to natural language instructions for Fetch-and-Carry with Object Grounding (FCOG) tasks. Although there have been many frameworks, they usually rely on manually given instruction sentences. Therefore, evaluations have only been conducted with fixed tasks. Furthermore, many multimodal language understanding models for the benchmarks only consider discrete actions. To address the limitations, we propose a framework for the full automation of the generation, execution, and evaluation of FCOG tasks. In addition, we introduce an approach to solving the FCOG tasks by dividing them into four distinct subtasks.
</details>
<details>
<summary>摘要</summary>
本文目的是开发一个框架，让机器人通过视觉信息执行基于自然语言指令的Fetch-and-Carry with Object Grounding（FCOG）任务。虽然有很多框架，但它们通常依赖于手动提供的指令句子。因此，评估只能进行固定任务。此外，许多多模态语言理解模型只考虑简单的动作。为了解决这些限制，我们提议一个框架，自动生成、执行和评估FCOG任务。此外，我们还介绍了解决FCOG任务的四个不同子任务的方法。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Imitation-Leveraging-Fine-grained-Quality-Signals-for-Alignment"><a href="#Beyond-Imitation-Leveraging-Fine-grained-Quality-Signals-for-Alignment" class="headerlink" title="Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment"></a>Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04072">http://arxiv.org/abs/2311.04072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geyang Guo, Ranchi Zhao, Tianyi Tang, Wayne Xin Zhao, Ji-Rong Wen</li>
<li>for: 提高大语言模型（LLM）的对人类偏好的适应性。</li>
<li>methods: 基于精细调整（SFT）和强制学习人类反馈（RLHF）两种方法。</li>
<li>results: 提出了一种改进的对适应方法，名为FIGLA，并通过对比好的和坏的回答来提供细致的质量信号。经过广泛的实验，FIGLA的方法可以与多种竞争性基准进行比较。<details>
<summary>Abstract</summary>
Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的对人类偏好的适配性是一个欲有的性能。现在的主要对齐方法是基于人类反馈学习（RLHF）。 despite RLHF的效果，它实现和训练相当复杂，所以现有的研究则是如何发展基于监督微调（SFT）的替代对齐方法。 however，SFT的主要限制是它实际上只是做复制学习，无法完全理解预期的行为。 To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.
</details></li>
</ul>
<hr>
<h2 id="Implementation-and-Comparison-of-Methods-to-Extract-Reliability-KPIs-out-of-Textual-Wind-Turbine-Maintenance-Work-Orders"><a href="#Implementation-and-Comparison-of-Methods-to-Extract-Reliability-KPIs-out-of-Textual-Wind-Turbine-Maintenance-Work-Orders" class="headerlink" title="Implementation and Comparison of Methods to Extract Reliability KPIs out of Textual Wind Turbine Maintenance Work Orders"></a>Implementation and Comparison of Methods to Extract Reliability KPIs out of Textual Wind Turbine Maintenance Work Orders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04064">http://arxiv.org/abs/2311.04064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marc-Alexander Lutz, Bastian Schäfermeier, Rachael Sexton, Michael Sharp, Alden Dima, Stefan Faulstich, Jagan Mohini Aluri</li>
<li>for: 本研究用于提高风力机的运行和维护，通过对维护工作令的整理和分析，提高风力机的可靠性指标。</li>
<li>methods: 本研究使用了三种不同的方法来计算风力机的可靠性指标，包括人工标注、自动标注和人工协助标注。</li>
<li>results: 研究发现，人工标注法可以作为标准，自动标注法和人工协助标注法可以减少人工干预时间和提高标注质量。三种方法都可以提高风力机的可靠性指标，但人工标注法的标准性较高。<details>
<summary>Abstract</summary>
Maintenance work orders are commonly used to document information about wind turbine operation and maintenance. This includes details about proactive and reactive wind turbine downtimes, such as preventative and corrective maintenance. However, the information contained in maintenance work orders is often unstructured and difficult to analyze, making it challenging for decision-makers to use this information for optimizing operation and maintenance. To address this issue, this work presents three different approaches to calculate reliability key performance indicators from maintenance work orders. The first approach involves manual labeling of the maintenance work orders by domain experts, using the schema defined in an industrial guideline to assign the label accordingly. The second approach involves the development of a model that automatically labels the maintenance work orders using text classification methods. The third technique uses an AI-assisted tagging tool to tag and structure the raw maintenance information contained in the maintenance work orders. The resulting calculated reliability key performance indicator of the first approach are used as a benchmark for comparison with the results of the second and third approaches. The quality and time spent are considered as criteria for evaluation. Overall, these three methods make extracting maintenance information from maintenance work orders more efficient, enable the assessment of reliability key performance indicators and therefore support the optimization of wind turbine operation and maintenance.
</details>
<details>
<summary>摘要</summary>
维保工作令是通常用于记录风机运行和维保信息的。这些信息包括掌控性维保和修复维保的时间，以及相关的预防维保和修复维保活动。然而，维保工作令中的信息通常是不结构化的，困难分析，使得决策者困难使用这些信息优化运行和维保。为解决这个问题，本工作提出了三种不同的方法来计算可靠度关键性表现指标从维保工作令中。第一种方法是通过域专家 manually 标签维保工作令，使用工业指南定义的 schema 将标签应用 accordingly。第二种方法是开发一种自动标签维保工作令的模型，使用文本分类方法进行标签。第三种技术使用 AI 助手标注工具来标记和结构化维保工作令中的原始维保信息。第一种方法的计算的可靠度关键性表现指标作为基准，与第二和第三种方法的结果进行比较。评估标准包括质量和时间花费。总的来说，这三种方法使得从维保工作令中提取维保信息更加效率，可以评估可靠度关键性表现指标，因此支持风机运行和维保优化。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-Fine-tuning-of-Language-Models-is-Biased-Towards-More-Extractable-Features"><a href="#Reinforcement-Learning-Fine-tuning-of-Language-Models-is-Biased-Towards-More-Extractable-Features" class="headerlink" title="Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features"></a>Reinforcement Learning Fine-tuning of Language Models is Biased Towards More Extractable Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04046">http://arxiv.org/abs/2311.04046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edoardopona/predicting-inductive-biases-rl">https://github.com/edoardopona/predicting-inductive-biases-rl</a></li>
<li>paper_authors: Diogo Cruz, Edoardo Pona, Alex Holness-Tofts, Elias Schmied, Víctor Abia Alonso, Charlie Griffin, Bogdan-Ionut Cirstea</li>
<li>for: 这个论文 investigate whether principles governing inductive biases in the supervised fine-tuning of large language models (LLMs) also apply when the fine-tuning process uses reinforcement learning.</li>
<li>methods: 研究采用了控制实验，测试了两个假设：一是预训练后可以更容易提取的特征更可能被最终策略使用，二是特征的证据是否支持或反对它的使用。</li>
<li>results: 通过控制实验测试两个假设，发现存在 statistically significant correlations，这是强有力的证据支持这两个假设。<details>
<summary>Abstract</summary>
Many capable large language models (LLMs) are developed via self-supervised pre-training followed by a reinforcement-learning fine-tuning phase, often based on human or AI feedback. During this stage, models may be guided by their inductive biases to rely on simpler features which may be easier to extract, at a cost to robustness and generalisation. We investigate whether principles governing inductive biases in the supervised fine-tuning of LLMs also apply when the fine-tuning process uses reinforcement learning. Following Lovering et al (2021), we test two hypotheses: that features more $\textit{extractable}$ after pre-training are more likely to be utilised by the final policy, and that the evidence for/against a feature predicts whether it will be utilised. Through controlled experiments on synthetic and natural language tasks, we find statistically significant correlations which constitute strong evidence for these hypotheses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="P-Bench-A-Multi-level-Privacy-Evaluation-Benchmark-for-Language-Models"><a href="#P-Bench-A-Multi-level-Privacy-Evaluation-Benchmark-for-Language-Models" class="headerlink" title="P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models"></a>P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04044">http://arxiv.org/abs/2311.04044</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yangqiu Song</li>
<li>for: This paper focuses on the privacy risks of language models (LMs) and proposes a multi-perspective privacy evaluation benchmark called P-Bench to evaluate the privacy leakage of LMs.</li>
<li>methods: The paper uses a unified pipeline for private fine-tuning and conducts empirical attacks on LMs to evaluate their privacy leakage.</li>
<li>results: The paper conducts extensive experiments on three datasets of GLUE for mainstream LMs and provides empirical evaluation results to fairly and intuitively evaluate the privacy leakage of various privacy-preserving language models (PPLMs).Here is the simplified Chinese version of the three key points:</li>
<li>for: 这篇论文关注语言模型（LMs）的隐私风险，提出了一个多方面隐私评估 benchmark called P-Bench，用于评估 LMs 的隐私泄露。</li>
<li>methods: 该篇论文使用了一个统一的私有化管道进行私有训练，并对 LMs 进行实际的隐私攻击，以评估其隐私泄露。</li>
<li>results: 该篇论文对 GLUE 三个 dataset 进行了广泛的实验，提供了对各种隐私保护语言模型（PPLMs）的实际评估结果，以便对其隐私泄露进行公正和直观的评估。<details>
<summary>Abstract</summary>
The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs, trained with massive textual data, achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present P-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only protecting and measuring the privacy of protected data with DP parameters, P-Bench sheds light on the neglected inference data privacy during actual usage. P-Bench first clearly defines multi-faceted privacy objectives during private fine-tuning. Then, P-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, P-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.
</details>
<details>
<summary>摘要</summary>
LM的快速发展提供了前所未有的可访问性和使用性，同时也带来了一些隐私风险。一方面，强大的LM通过庞大的文本数据进行训练，在许多下游NLP任务中实现了状态机器人的性能。另一方面，越来越多的注意力被转移到不受限制的模型访问中，可能会导致数据泄露的隐私风险。为了解决这些问题，许多最近的研究提出了隐私保护语言模型（PPLM），并采用了各种隐私保护技术。然而，不同的隐私保护实现方式使得对现有PPLM的比较变得困难。在这篇论文中，我们提出了P-Bench，一个多方面隐私评估benchmark，用于Empirically和直观地评估LM的隐私泄露。而不是仅仅保护和测量保护数据的隐私，P-Bench shed light on在实际使用时被忽略的推理数据隐私。P-Bench首先明确了多方面隐私目标 durante private fine-tuning。然后，P-Bench构建了一个统一的管道来执行private fine-tuning。最后，P-Bench对LMs进行了预先定义的隐私目标为empirical attack results。这些empirical attack results用于评估不同PPLMs的隐私泄露。我们对GLUE dataset进行了广泛的实验。
</details></li>
</ul>
<hr>
<h2 id="mPLUG-Owl2-Revolutionizing-Multi-modal-Large-Language-Model-with-Modality-Collaboration"><a href="#mPLUG-Owl2-Revolutionizing-Multi-modal-Large-Language-Model-with-Modality-Collaboration" class="headerlink" title="mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"></a>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04257">http://arxiv.org/abs/2311.04257</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/x-plug/mplug-owl">https://github.com/x-plug/mplug-owl</a></li>
<li>paper_authors: Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou</li>
<li>for: 这个论文主要 targets 多modal大语言模型（MLLMs）的开放任务表现。</li>
<li>methods: 这个论文提出了一种多元模块化网络设计，使用语言解码器作为多种Modalities的通用接口，并在不同modalities之间进行功能模块共享和启用模态特征保留模块。</li>
<li>results: 对多种文本任务和多modal任务进行广泛的实验，发现mPLUG-Owl2可以通过一个通用模型实现状态之最高表现，并且在多modal任务中实现模态协作现象，为未来多modal基础模型的发展开拓了先河。<details>
<summary>Abstract</summary>
Multi-modal Large Language Models (MLLMs) have demonstrated impressive instruction abilities across various open-ended tasks. However, previous methods primarily focus on enhancing multi-modal capabilities. In this work, we introduce a versatile multi-modal large language model, mPLUG-Owl2, which effectively leverages modality collaboration to improve performance in both text and multi-modal tasks. mPLUG-Owl2 utilizes a modularized network design, with the language decoder acting as a universal interface for managing different modalities. Specifically, mPLUG-Owl2 incorporates shared functional modules to facilitate modality collaboration and introduces a modality-adaptive module that preserves modality-specific features. Extensive experiments reveal that mPLUG-Owl2 is capable of generalizing both text tasks and multi-modal tasks and achieving state-of-the-art performances with a single generic model. Notably, mPLUG-Owl2 is the first MLLM model that demonstrates the modality collaboration phenomenon in both pure-text and multi-modal scenarios, setting a pioneering path in the development of future multi-modal foundation models.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型（MLLM）已经表现出了惊人的指令能力在多种开放任务中。然而，先前的方法主要关注于增强多模式能力。在这项工作中，我们介绍了一种多模式大语言模型，名为mPLUG-Owl2，可以有效利用模式合作来提高文本和多模式任务的性能。mPLUG-Owl2使用分解化网络设计，语言解码器作为多Modal interfaces的通用接口来管理不同模式。具体来说，mPLUG-Owl2包含共享功能模块来促进模式合作，并引入一个适应模式特征的模块。广泛的实验表明，mPLUG-Owl2能够泛化文本任务和多模式任务，并在单个通用模型下实现状态的最佳性能。值得一提的是，mPLUG-Owl2是首个在纯文本和多模式场景中显示出模式合作现象的 MLLM 模型，这为未来多模式基础模型的发展留下了先河。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Film-Adaptation-through-Narrative-Alignment"><a href="#Analyzing-Film-Adaptation-through-Narrative-Alignment" class="headerlink" title="Analyzing Film Adaptation through Narrative Alignment"></a>Analyzing Film Adaptation through Narrative Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04020">http://arxiv.org/abs/2311.04020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tanzir5/alignment_tool2.0">https://github.com/tanzir5/alignment_tool2.0</a></li>
<li>paper_authors: Tanzir Pial, Shahreen Salim, Charuta Pethe, Allen Kim, Steven Skiena</li>
<li>for: 本研究用 Smith-Waterman 本地Alignment 算法和 SBERT 嵌入距离来研究电影剧本的改编过程，以便自动分析40个改编作品，从而探讨改编过程中的 faithfulness 问题、对话的重要性、叙述顺序的保留以及 gender 表现问题。</li>
<li>methods: 本研究使用 Smith-Waterman 本地Alignment 算法和 SBERT 嵌入距离来建立叙述对应关系，以便衡量电影剧本与原著之间的文本相似性。</li>
<li>results: 研究发现，改编过程中对 диалог的重要性具有显著影响，而叙述顺序的保留也具有较高的相似性。此外，研究还发现 gender 表现问题存在某些偏见。<details>
<summary>Abstract</summary>
Novels are often adapted into feature films, but the differences between the two media usually require dropping sections of the source text from the movie script. Here we study this screen adaptation process by constructing narrative alignments using the Smith-Waterman local alignment algorithm coupled with SBERT embedding distance to quantify text similarity between scenes and book units. We use these alignments to perform an automated analysis of 40 adaptations, revealing insights into the screenwriting process concerning (i) faithfulness of adaptation, (ii) importance of dialog, (iii) preservation of narrative order, and (iv) gender representation issues reflective of the Bechdel test.
</details>
<details>
<summary>摘要</summary>
小说经常被改编成电影，但两媒体之间的差异通常需要从电影剧本中剔除部分源文本。我们通过构建 narative 对应关系，使用 Smith-Waterman 地方对应算法和 SBERT 嵌入距离来衡量场景和书单之间的文本相似性。我们使用这些对应关系来自动分析 40 个改编作品，探讨改编过程中的 faithfulness 问题、对话的重要性、情节的顺序和 gender 表达问题，包括 Bechdel 测试。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Jiu-Jitsu-Argumentation-for-Writing-Peer-Review-Rebuttals"><a href="#Exploring-Jiu-Jitsu-Argumentation-for-Writing-Peer-Review-Rebuttals" class="headerlink" title="Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals"></a>Exploring Jiu-Jitsu Argumentation for Writing Peer Review Rebuttals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03998">http://arxiv.org/abs/2311.03998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sukannya Purkayastha, Anne Lauscher, Iryna Gurevych</li>
<li>for: 这个论文的目的是为了研究基于态度根和主题的对话战斗系统（Jiu-Jitsu）的 arguecation 风格，以便更好地处理人们的论点。</li>
<li>methods: 这篇论文使用了对话战斗系统（Jiu-Jitsu）的技巧，包括first, 标识对方的态度根和主题，然后选择与这些驱动器相符的抗驳。</li>
<li>results: 这篇论文提出了一种新的任务——态度和主题导向的抗驳生成，并对现有的评论结构数据进行扩充，以便更好地实现这一任务。<details>
<summary>Abstract</summary>
In many domains of argumentation, people's arguments are driven by so-called attitude roots, i.e., underlying beliefs and world views, and their corresponding attitude themes. Given the strength of these latent drivers of arguments, recent work in psychology suggests that instead of directly countering surface-level reasoning (e.g., falsifying given premises), one should follow an argumentation style inspired by the Jiu-Jitsu 'soft' combat system (Hornsey and Fielding, 2017): first, identify an arguer's attitude roots and themes, and then choose a prototypical rebuttal that is aligned with those drivers instead of invalidating those. In this work, we are the first to explore Jiu-Jitsu argumentation for peer review by proposing the novel task of attitude and theme-guided rebuttal generation. To this end, we enrich an existing dataset for discourse structure in peer reviews with attitude roots, attitude themes, and canonical rebuttals. To facilitate this process, we recast established annotation concepts from the domain of peer reviews (e.g., aspects a review sentence is relating to) and train domain-specific models. We then propose strong rebuttal generation strategies, which we benchmark on our novel dataset for the task of end-to-end attitude and theme-guided rebuttal generation and two subtasks.
</details>
<details>
<summary>摘要</summary>
在许多辩论领域，人们的论据受到称为“态度根”的下面启发和世界观的影响，以及其相应的态度主题。由于这些潜在驱动者的力量，最近的心理学研究表明，而不是直接对表面水平的逻辑（例如，证据反驳），更应该采用基于柔敏战斗系统（Jiu-Jitsu）的辩论风格。 specifically, 我们是首次在 peer review 中 explore Jiu-Jitsu 辩论，并提出了 novel task of 态度和主题导向的反驳生成。 To this end, we enrich an existing dataset for discourse structure in peer reviews with attitude roots, attitude themes, and canonical rebuttals. To facilitate this process, we recast established annotation concepts from the domain of peer reviews (e.g., aspects a review sentence is relating to) and train domain-specific models. We then propose strong rebuttal generation strategies, which we benchmark on our novel dataset for the task of end-to-end 态度和主题导向的反驳生成 and two subtasks.Note: "态度根" (attitude roots) and "态度主题" (attitude themes) are not exact translations, but rather concepts that are commonly used in the field of psychology to refer to underlying beliefs and world views that drive people's arguments.
</details></li>
</ul>
<hr>
<h2 id="Factoring-Hate-Speech-A-New-Annotation-Framework-to-Study-Hate-Speech-in-Social-Media"><a href="#Factoring-Hate-Speech-A-New-Annotation-Framework-to-Study-Hate-Speech-in-Social-Media" class="headerlink" title="Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media"></a>Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03969">http://arxiv.org/abs/2311.03969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gal Ron, Effi Levi, Odelia Oshri, Shaul R. Shenhav</li>
<li>for: 本研究提出了一种新的注释方案，将仇恨言论分为五个分类。</li>
<li>methods: 我们使用了 Twitter 上独特表达仇恨言论的More than 2.9 万句 tweet 数据集，并将样本集中的 1,050 句 tweet 进行注释。</li>
<li>results: 我们通过统计分析注释数据集，以及展示注释示例，并将在未来工作中的可能方向进行讨论。<details>
<summary>Abstract</summary>
In this work we propose a novel annotation scheme which factors hate speech into five separate discursive categories. To evaluate our scheme, we construct a corpus of over 2.9M Twitter posts containing hateful expressions directed at Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical analysis of the annotated dataset as well as discuss annotation examples, and conclude by discussing promising directions for future work.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们提出了一种新的注释方案，它将仇恨言论分解为五个分开的说话类别。为评估我们的方案，我们构建了超过290万条推特帖子中含有仇恨表达的集合，并将一个采样集中的1,050条帖子进行注释。我们提供了一些统计分析的数据，以及注释示例，最后讨论了未来工作的可能的方向。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-Dialogue-Repair-in-Voice-Assistants"><a href="#An-Analysis-of-Dialogue-Repair-in-Voice-Assistants" class="headerlink" title="An Analysis of Dialogue Repair in Voice Assistants"></a>An Analysis of Dialogue Repair in Voice Assistants</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03952">http://arxiv.org/abs/2311.03952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Galbraith</li>
<li>for: 本研究探讨了虚拟助手和用户之间的对话维护，尤其是虚拟助手如何使用和应对用户发起的修复策略”huh?”。</li>
<li>methods: 研究通过分析Google助手和Siri对话的交互语言使用情况，发现虚拟助手使用了一些自己的修复策略，但无法模仿人类对话修复策略。</li>
<li>results: 英语和西班牙用户acceptability调查显示了用户修复策略的偏好和虚拟助手使用情况的相似性和差异，其中存在一些语言间的不平等。这些结果 shed light on 人机交互语言中的不平等， highlighting the need for further research on the impact of interactional language in human-machine interaction.<details>
<summary>Abstract</summary>
Spoken dialogue systems have transformed human-machine interaction by providing real-time responses to queries. However, misunderstandings between the user and system persist. This study explores the significance of interactional language in dialogue repair between virtual assistants and users by analyzing interactions with Google Assistant and Siri, focusing on their utilization and response to the other-initiated repair strategy "huh?" prevalent in human-human interaction. Findings reveal several assistant-generated strategies but an inability to replicate human-like repair strategies such as "huh?". English and Spanish user acceptability surveys show differences in users' repair strategy preferences and assistant usage, with both similarities and disparities among the two surveyed languages. These results shed light on inequalities between interactional language in human-human interaction and human-machine interaction, underscoring the need for further research on the impact of interactional language in human-machine interaction in English and beyond.
</details>
<details>
<summary>摘要</summary>
人工智能对话系统已经改变了人机交互，提供了实时回答用户问题。然而，用户和系统之间的错误仍然存在。这项研究探讨了对话修复在虚拟助手和用户之间的语言互动的重要性，通过分析Google助手和Siri在用户发起修复策略"huh?"的情况。发现虚拟助手采用了多种策略，但无法模仿人类对话修复策略"huh?".英语和西班牙语用户接受度调查显示了用户修复策略的差异和虚拟助手使用情况之间的相似性和差异，这些结果推翻了人类对话修复语言和人机交互语言之间的不等。这些结果也 highlights the need for further research on the impact of interactional language in human-machine interaction in English and beyond.
</details></li>
</ul>
<hr>
<h2 id="Improving-Korean-NLP-Tasks-with-Linguistically-Informed-Subword-Tokenization-and-Sub-character-Decomposition"><a href="#Improving-Korean-NLP-Tasks-with-Linguistically-Informed-Subword-Tokenization-and-Sub-character-Decomposition" class="headerlink" title="Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization and Sub-character Decomposition"></a>Improving Korean NLP Tasks with Linguistically Informed Subword Tokenization and Sub-character Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03928">http://arxiv.org/abs/2311.03928</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taeheejeon22/morphsubdecomp-korean">https://github.com/taeheejeon22/morphsubdecomp-korean</a></li>
<li>paper_authors: Taehee Jeon, Bongseok Yang, Changhwan Kim, Yoonseob Lim</li>
<li>for: 本研究旨在提高韩语语言模型的 sintactic 和 semantics 能力，通过使用 morpheme-aware subword tokenization 方法，解决 byte pair encoding (BPE) 在韩语中的挑战。</li>
<li>methods: 本研究使用 sub-character decomposition 方法，结合 Pre-trained Language Models (PLMs)，以保持语言学正确性和计算效率之间的平衡。</li>
<li>results: 对 NIKL-CoLA 任务的评估表明，该方法可以在总体上获得良好的表现，尤其是在 sintactic 任务中，表明 integrating morpheme type information 可以提高语言模型的 sintactic 和 semantics 能力。<details>
<summary>Abstract</summary>
We introduce a morpheme-aware subword tokenization method that utilizes sub-character decomposition to address the challenges of applying Byte Pair Encoding (BPE) to Korean, a language characterized by its rich morphology and unique writing system. Our approach balances linguistic accuracy with computational efficiency in Pre-trained Language Models (PLMs). Our evaluations show that this technique achieves good performances overall, notably improving results in the syntactic task of NIKL-CoLA. This suggests that integrating morpheme type information can enhance language models' syntactic and semantic capabilities, indicating that adopting more linguistic insights can further improve performance beyond standard morphological analysis.
</details>
<details>
<summary>摘要</summary>
我们介绍一种基于 morpheme 的 subword  tokenization 方法，利用 sub-character 分解来Addressing the challenges of applying Byte Pair Encoding (BPE) to Korean, a language characterized by its rich morphology and unique writing system. Our approach balances linguistic accuracy with computational efficiency in Pre-trained Language Models (PLMs). Our evaluations show that this technique achieves good performances overall, notably improving results in the syntactic task of NIKL-CoLA. This suggests that integrating morpheme type information can enhance language models' syntactic and semantic capabilities, indicating that adopting more linguistic insights can further improve performance beyond standard morphological analysis.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="iACOS-Advancing-Implicit-Sentiment-Extraction-with-Informative-and-Adaptive-Negative-Examples"><a href="#iACOS-Advancing-Implicit-Sentiment-Extraction-with-Informative-and-Adaptive-Negative-Examples" class="headerlink" title="iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples"></a>iACOS: Advancing Implicit Sentiment Extraction with Informative and Adaptive Negative Examples</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03896">http://arxiv.org/abs/2311.03896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiancai Xu, Jia-Dong Zhang, Lei Xiong, Zhishang Liu</li>
<li>for: 本研究旨在提出一种新的方法iACOS，用于从文本中提取含义不明确的方面、类别和意见。</li>
<li>methods: iACOS方法首先在文本中追加两个隐式token，以获取全 Token的上下文感知表示。然后，iACOS使用一个顺序标签模型，在上下文感知Token表示上进行同时抽取explicit和隐式方面、类别和意见。第三个步骤是开发一种特殊多头注意力的多标签分类器，用于同时发现方面意见对的对应分类和 sentiment。</li>
<li>results: 实验结果表明，iACOS方法在两个公共 benchmark datasets上的F1分数明显高于其他四元EXTRACTION基线。<details>
<summary>Abstract</summary>
Aspect-based sentiment analysis (ABSA) have been extensively studied, but little light has been shed on the quadruple extraction consisting of four fundamental elements: aspects, categories, opinions and sentiments, especially with implicit aspects and opinions. In this paper, we propose a new method iACOS for extracting Implicit Aspects with Categories and Opinions with Sentiments. First, iACOS appends two implicit tokens at the end of a text to capture the context-aware representation of all tokens including implicit aspects and opinions. Second, iACOS develops a sequence labeling model over the context-aware token representation to co-extract explicit and implicit aspects and opinions. Third, iACOS devises a multi-label classifier with a specialized multi-head attention for discovering aspect-opinion pairs and predicting their categories and sentiments simultaneously. Fourth, iACOS leverages informative and adaptive negative examples to jointly train the multi-label classifier and the other two classifiers on categories and sentiments by multi-task learning. Finally, the experimental results show that iACOS significantly outperforms other quadruple extraction baselines according to the F1 score on two public benchmark datasets.
</details>
<details>
<summary>摘要</summary>
它的核心思想是通过附加两个隐式标签来捕捉文本中的上下文意识，并对这些标签进行序列标签化，以同时抽取显式和隐式方面、意见和情感。其次，它采用多标签分类器，并在特定情况下采用多头注意力，同时找到方面意见对应的类别和情感。最后，它使用有用和适应性负例来共同培训多标签分类器和其他两个分类器，通过多任务学习。实验结果表明，iACOS在两个公共 benchmark 数据集上显著超越了其他四元EXTRACTION 基线。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Sparse-Contrastive-Learning-of-Sentence-Embeddings"><a href="#Sparse-Contrastive-Learning-of-Sentence-Embeddings" class="headerlink" title="Sparse Contrastive Learning of Sentence Embeddings"></a>Sparse Contrastive Learning of Sentence Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03881">http://arxiv.org/abs/2311.03881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruize An, Chen Zhang, Dawei Song</li>
<li>for: 本研究旨在证明对句子嵌入进行精简Parameterization可以提高模型性能，并且通过对标准 semantics textual similarity (STS) 任务和转移学习任务进行更多的实验，证明了我们的精简方法的可行性和稳定性。</li>
<li>methods: 本研究使用了对句子嵌入进行精简Parameterization，通过对每个参数计算对总质量的贡献度来确定不必要的参数，并将其精简掉。</li>
<li>results: 结果表明，使用我们的精简方法可以提高模型性能，并且在STS任务和转移学习任务中表现出色。进一步的分析也证明了我们的精简方法的有效性和稳定性。<details>
<summary>Abstract</summary>
Recently, SimCSE has shown the feasibility of contrastive learning in training sentence embeddings and illustrates its expressiveness in spanning an aligned and uniform embedding space. However, prior studies have shown that dense models could contain harmful parameters that affect the model performance, and it is no wonder that SimCSE can as well be invented with such parameters. Driven by this, parameter sparsification is applied, where alignment and uniformity scores are used to measure the contribution of each parameter to the overall quality of sentence embeddings. Drawing from a preliminary study, we consider parameters with minimal contributions to be detrimental, as their sparsification results in improved model performance. To discuss the ubiquity of detrimental parameters and remove them, more experiments on the standard semantic textual similarity (STS) tasks and transfer learning tasks are conducted, and the results show that the proposed sparsified SimCSE (SparseCSE) has excellent performance in comparison with SimCSE. Furthermore, through in-depth analysis, we establish the validity and stability of our sparsification method, showcasing that the embedding space generated by SparseCSE exhibits improved alignment compared to that produced by SimCSE. Importantly, the uniformity yet remains uncompromised.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OLaLa-Ontology-Matching-with-Large-Language-Models"><a href="#OLaLa-Ontology-Matching-with-Large-Language-Models" class="headerlink" title="OLaLa: Ontology Matching with Large Language Models"></a>OLaLa: Ontology Matching with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03837">http://arxiv.org/abs/2311.03837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sven Hertling, Heiko Paulheim</li>
<li>for: 本研究的目的是探讨如何使用大型自然语言模型（Large Language Model，LLM）提高 Ontology 匹配的效果。</li>
<li>methods: 本研究使用了零shot和几shot提问法，并应用于多种开放的 LLM 上不同的任务，以评估 Ontology Alignment Evaluation Initiative（OAEI）中的不同任务。</li>
<li>results: 研究发现，只需要一些示例和一个良好的提问，就可以达到与supervised matching系统相当的效果，而这些系统使用了许多更多的真实数据。<details>
<summary>Abstract</summary>
Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Conversations-in-Galician-a-Large-Language-Model-for-an-Underrepresented-Language"><a href="#Conversations-in-Galician-a-Large-Language-Model-for-an-Underrepresented-Language" class="headerlink" title="Conversations in Galician: a Large Language Model for an Underrepresented Language"></a>Conversations in Galician: a Large Language Model for an Underrepresented Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03812">http://arxiv.org/abs/2311.03812</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.irlab.org/irlab/cabuxa">https://gitlab.irlab.org/irlab/cabuxa</a></li>
<li>paper_authors: Eliseo Bao, Anxo Pérez, Javier Parapar</li>
<li>for: 提高加利西亚语言处理（NLP）技术的可用性和准确性，以便更好地包括所有语言社区在大语言模型的发展中。</li>
<li>methods: 提出了两种新资源，包括加利西亚语言适应的Alpaca数据集和Cabuxa-7B语言模型，以便进一步推动加利西亚语言的NLP研究。</li>
<li>results: 通过 fine-tuning LLaMA-7B语言模型，成功地使其能够理解和回答加利西亚语言，并证明了该数据集的重要性和可用性。<details>
<summary>Abstract</summary>
The recent proliferation of Large Conversation Language Models has highlighted the economic significance of widespread access to this type of AI technologies in the current information age. Nevertheless, prevailing models have primarily been trained on corpora consisting of documents written in popular languages. The dearth of such cutting-edge tools for low-resource languages further exacerbates their underrepresentation in the current economic landscape, thereby impacting their native speakers. This paper introduces two novel resources designed to enhance Natural Language Processing (NLP) for the Galician language. We present a Galician adaptation of the Alpaca dataset, comprising 52,000 instructions and demonstrations. This dataset proves invaluable for enhancing language models by fine-tuning them to more accurately adhere to provided instructions. Additionally, as a demonstration of the dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician, a language not originally supported by the model, by following the Alpaca format. This work contributes to the research on multilingual models tailored for low-resource settings, a crucial endeavor in ensuring the inclusion of all linguistic communities in the development of Large Language Models. Another noteworthy aspect of this research is the exploration of how knowledge of a closely related language, in this case, Portuguese, can assist in generating coherent text when training resources are scarce. Both the Galician Alpaca dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we have made the source code available to facilitate replication of this experiment and encourage further advancements for underrepresented languages.
</details>
<details>
<summary>摘要</summary>
现代信息时代，大型对话语言模型的普及已经启示出了这类人工智能技术在经济领域的经济意义。然而，目前的模型主要是通过流行语言的文献来训练的，这导致了少数语言的下发不足，从而影响了这些语言的原生使用者。本文介绍了两个新资源，旨在提高加利西亚语的自然语言处理（NLP）能力。我们提供了一个加利西亚语版的Alpaca数据集，包含52,000个指令和示例。这个数据集对于提高语言模型来说是非常有价值的，可以通过微调来使模型更加准确地遵循提供的指令。此外，我们还通过使用Alpaca格式来让LLaMA-7B模型能够理解和回答加利西亚语，这是模型原本不支持的语言。这项研究对于开发适用于低资源环境的多语言模型做出了贡献，这是确保所有语言社区参与大语言模型的发展中的关键任务。此外，我们还发现了在资源匮乏时，知道相关语言的知识（在这种情况下是葡萄牙语）可以帮助生成有 coherence 的文本。加利西亚Alpaca数据集和Cabuxa-7B模型都公开 accessible 在我们的Huggingface Hub上，并且我们已经将源代码公开，以便复现这个实验和促进低资源语言的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Noisy-Pair-Corrector-for-Dense-Retrieval"><a href="#Noisy-Pair-Corrector-for-Dense-Retrieval" class="headerlink" title="Noisy Pair Corrector for Dense Retrieval"></a>Noisy Pair Corrector for Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03798">http://arxiv.org/abs/2311.03798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hang Zhang, Yeyun Gong, Xingwei He, Dayiheng Liu, Daya Guo, Jiancheng Lv, Jian Guo</li>
<li>for: 本研究旨在解决 dense retrieval 模型中存在隐式假设：训练查询文档对的匹配是精确的。由于在实际应用中收集训练对不可能进行手动注释，因此实际上存在匹配错误的问题。</li>
<li>methods: 我们提出了一种新的方法called Noisy Pair Corrector (NPC)，它包括检测模块和修正模块。检测模块通过计算标注正方和易获得负方文档的减拟率来估计噪声对。修正模块使用 exponential moving average (EMA) 模型提供软监督信号，以抑制噪声的影响。</li>
<li>results: 我们在 Natural Question 和 TriviaQA 文本检索benchmark上、 StaQC 和 SO-DS 代码检索benchmark上进行实验，结果显示 NPC 能够有效地处理各种噪声。<details>
<summary>Abstract</summary>
Most dense retrieval models contain an implicit assumption: the training query-document pairs are exactly matched. Since it is expensive to annotate the corpus manually, training pairs in real-world applications are usually collected automatically, which inevitably introduces mismatched-pair noise. In this paper, we explore an interesting and challenging problem in dense retrieval, how to train an effective model with mismatched-pair noise. To solve this problem, we propose a novel approach called Noisy Pair Corrector (NPC), which consists of a detection module and a correction module. The detection module estimates noise pairs by calculating the perplexity between annotated positive and easy negative documents. The correction module utilizes an exponential moving average (EMA) model to provide a soft supervised signal, aiding in mitigating the effects of noise. We conduct experiments on text-retrieval benchmarks Natural Question and TriviaQA, code-search benchmarks StaQC and SO-DS. Experimental results show that NPC achieves excellent performance in handling both synthetic and realistic noise.
</details>
<details>
<summary>摘要</summary>
大多数紧凑检索模型假设训练查询文档对是 preciselly matched。由于在实际应用中annotate文档是非常昂贵的，因此训练对在实际应用中是不可避免的具有匹配错误的噪声。在这篇论文中，我们研究了紧凑检索中一个有趣和挑战的问题：如何训练有效的模型在匹配错误的情况下。为解决这个问题，我们提出了一种新的方法，即噪声对纠正器（NPC）。NPC包括检测模块和修正模块。检测模块通过计算可读性来估算噪声对。修正模块使用指数移动平均（EMA）模型提供软件支持信号，以帮助减轻噪声的影响。我们在自然问答和智能问答等文本检索 bencmarks 上进行了实验， результаados 表明 NPC 在处理 sintétic和实际噪声方面具有杰出的表现。
</details></li>
</ul>
<hr>
<h2 id="Character-Level-Bangla-Text-to-IPA-Transcription-Using-Transformer-Architecture-with-Sequence-Alignment"><a href="#Character-Level-Bangla-Text-to-IPA-Transcription-Using-Transformer-Architecture-with-Sequence-Alignment" class="headerlink" title="Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment"></a>Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03792">http://arxiv.org/abs/2311.03792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakir Hasan, Shrestha Datta, Ameya Debnath</li>
<li>for: 这个研究旨在应用人工智能和机器学习来生成孟加拉语的国际音标（IPA），以掌握正确的读音和理解。</li>
<li>methods: 本研究使用了一个基于序列转换器的字母和符号级别的模型，以生成孟加拉语每个词的 IPA。</li>
<li>results: 研究获得了在DataVerse Challenge - ITVerse 2023 公开排名中的第一名，word error rate为0.10582。<details>
<summary>Abstract</summary>
The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).
</details>
<details>
<summary>摘要</summary>
国际音声字母（IPA）是语言学习和理解的不可或缺工具，帮助用户准确发音和理解。它在语音治疗、语言研究、精确转写和文本读取系统的发展中扮演着关键性角色，因此在多个领域都是必不可少的工具。孟加拉语是最广泛使用的7种语言之一，因此IPA在其领域的需求增加。孟加拉语IPA映射非常复杂，需要使用人工智能和机器学习来解决。在这项研究中，我们使用了基于转换器的序列到序列模型，以获取孟加拉语每个单词的IPA，因为孟加拉语单词的IPA变化非常小。我们的转换器模型只有850万参数，仅有一个解码和编码层。此外，为了处理括号和外语文本中的特殊符号，我们使用了手动映射。最后，保持句子元素IPA的相对位置和生成组合IPA，我们实现了公开排名的第一名，word error rate为0.10582（https://www.kaggle.com/competitions/dataverse_2023/)。
</details></li>
</ul>
<hr>
<h2 id="Language-Representation-Projection-Can-We-Transfer-Factual-Knowledge-across-Languages-in-Multilingual-Language-Models"><a href="#Language-Representation-Projection-Can-We-Transfer-Factual-Knowledge-across-Languages-in-Multilingual-Language-Models" class="headerlink" title="Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?"></a>Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03788">http://arxiv.org/abs/2311.03788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyang Xu, Junzhuo Li, Deyi Xiong</li>
<li>for: 本研究探讨了在多语言预训练语言模型中具有可读性的知识的可行性，以及如何通过显式传递来增强多语言知识的共享。</li>
<li>methods: 该研究提出了两个参数化的语言表示 projekt 模块 (LRP2)，用于将非英语表示转换成英语相似的表示，并将英语表示转换回非英语语言的表示。</li>
<li>results: 实验结果表明，LRP2可以显著提高多语言知识检索准确率，并且可以增强多语言知识的共享性。<details>
<summary>Abstract</summary>
Multilingual pretrained language models serve as repositories of multilingual factual knowledge. Nevertheless, a substantial performance gap of factual knowledge probing exists between high-resource languages and low-resource languages, suggesting limited implicit factual knowledge transfer across languages in multilingual pretrained language models. This paper investigates the feasibility of explicitly transferring relatively rich factual knowledge from English to non-English languages. To accomplish this, we propose two parameter-free $\textbf{L}$anguage $\textbf{R}$epresentation $\textbf{P}$rojection modules (LRP2). The first module converts non-English representations into English-like equivalents, while the second module reverts English-like representations back into representations of the corresponding non-English language. Experimental results on the mLAMA dataset demonstrate that LRP2 significantly improves factual knowledge retrieval accuracy and facilitates knowledge transferability across diverse non-English languages. We further investigate the working mechanism of LRP2 from the perspectives of representation space and cross-lingual knowledge neuron.
</details>
<details>
<summary>摘要</summary>
多语言预训言语模型作为多语言事实知识的存储库，但是存在高Resource语言和lowResource语言之间的事实知识检测性能差距，表明在多语言预训言语模型中，各语言之间的知识传递是有限的。本文研究了将英语的较为充足的事实知识转移到非英语语言中的可能性。为此，我们提出了两个参数无关的语言表示项投影模块（LRP2）。第一个模块将非英语表示转换成英语相似的表示，而第二个模块将英语相似的表示恢复回到对应的非英语语言表示。在mLAMA数据集上进行实验，我们发现LRP2能够显著提高事实知识检测精度和提高非英语语言之间的知识传递性。我们还从语料空间和 crossing-lingual知识神经的角度进行了LRP2的工作机制的调查。
</details></li>
</ul>
<hr>
<h2 id="Gender-Inflected-or-Bias-Inflicted-On-Using-Grammatical-Gender-Cues-for-Bias-Evaluation-in-Machine-Translation"><a href="#Gender-Inflected-or-Bias-Inflicted-On-Using-Grammatical-Gender-Cues-for-Bias-Evaluation-in-Machine-Translation" class="headerlink" title="Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation"></a>Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03767">http://arxiv.org/abs/2311.03767</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iampushpdeep/gender-bias-hi-en-eval">https://github.com/iampushpdeep/gender-bias-hi-en-eval</a></li>
<li>paper_authors: Pushpdeep Singh</li>
<li>for: 这个研究旨在评估不同语言源语言的神经机器翻译模型中存在的社会偏见，特别是对 gender 的偏见。</li>
<li>methods: 该研究使用了 Hindi 作为源语言，并构建了两组 gender-specific 句子集：OTSC-Hindi 和 WinoMT-Hindi，以自动评估不同的 Hindi-English (HI-EN) NMT 系统的性偏见。</li>
<li>results: 该研究显示了考虑语言的性质在设计 extrinsic bias 评估数据集时的重要性。<details>
<summary>Abstract</summary>
Neural Machine Translation (NMT) models are state-of-the-art for machine translation. However, these models are known to have various social biases, especially gender bias. Most of the work on evaluating gender bias in NMT has focused primarily on English as the source language. For source languages different from English, most of the studies use gender-neutral sentences to evaluate gender bias. However, practically, many sentences that we encounter do have gender information. Therefore, it makes more sense to evaluate for bias using such sentences. This allows us to determine if NMT models can identify the correct gender based on the grammatical gender cues in the source sentence rather than relying on biased correlations with, say, occupation terms. To demonstrate our point, in this work, we use Hindi as the source language and construct two sets of gender-specific sentences: OTSC-Hindi and WinoMT-Hindi that we use to evaluate different Hindi-English (HI-EN) NMT systems automatically for gender bias. Our work highlights the importance of considering the nature of language when designing such extrinsic bias evaluation datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multilingual-Mathematical-Autoformalization"><a href="#Multilingual-Mathematical-Autoformalization" class="headerlink" title="Multilingual Mathematical Autoformalization"></a>Multilingual Mathematical Autoformalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03755">http://arxiv.org/abs/2311.03755</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/albertqjiang/mma">https://github.com/albertqjiang/mma</a></li>
<li>paper_authors: Albert Q. Jiang, Wenda Li, Mateja Jamnik</li>
<li>for: 这个论文的目的是为了提高自动ormalization的研究进步，即将自然语言材料翻译成机器可验证的形式化表述。</li>
<li>methods: 这篇论文使用了一种语言模型来将正式数学陈述翻译成相应的不正式陈述，即在反向向き中使用语言模型进行翻译。</li>
<li>results: 实验表明，对于 $\texttt{MMA}$ dataset进行精度调整后，语言模型在 $\texttt{miniF2F}$ 和 $\texttt{ProofNet}$ 测试准则上的表现提高了 $16-18%$，比基础模型的表现提高了 $0%$。这表明，在多语言正式数学数据上进行精度调整后，自动ormalization模型在单语言任务上也能够更强大。<details>
<summary>Abstract</summary>
Autoformalization is the task of translating natural language materials into machine-verifiable formalisations. Progress in autoformalization research is hindered by the lack of a sizeable dataset consisting of informal-formal pairs expressing the same essence. Existing methods tend to circumvent this challenge by manually curating small corpora or using few-shot learning with large language models. But these methods suffer from data scarcity and formal language acquisition difficulty. In this work, we create $\texttt{MMA}$, a large, flexible, multilingual, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones. Experiments show that language models fine-tuned on $\texttt{MMA}$ produce $16-18\%$ of statements acceptable with minimal corrections on the $\texttt{miniF2F}$ and $\texttt{ProofNet}$ benchmarks, up from $0\%$ with the base model. We demonstrate that fine-tuning on multilingual formal data results in more capable autoformalization models even when deployed on monolingual tasks.
</details>
<details>
<summary>摘要</summary>
自然语言材料的自动化正式化任务是将自然语言材料翻译成机器可验证的形式化表达。研究进步受到有限的大量数据集阻碍，其中包含同一主题的 Informal-Formal 对应对。现有方法通常使用手动抽象小 corpus 或几shot学习大语言模型。但这些方法受到数据稀缺和正式语言学习困难的限制。在这种工作中，我们创建了 $\texttt{MMA}$  dataset，它是一个大型、灵活、多语言、多领域的 informal-formal 对应对。我们使用语言模型将 formal 数学陈述翻译到对应的 informal 陈述，从而创建了这个 dataset。实验显示，在 $\texttt{MMA}$ 上进行 fine-tuning 后，模型在 $\texttt{miniF2F}$ 和 $\texttt{ProofNet}$ benchmark 上可以提交 $16-18\%$ 的声明，与基础模型相比增加了 $0\%$。我们还证明了在多语言正式数据上进行 fine-tuning 可以创造更强的自动化正式化模型，即使在单语言任务上部署。
</details></li>
</ul>
<hr>
<h2 id="Which-is-better-Exploring-Prompting-Strategy-For-LLM-based-Metrics"><a href="#Which-is-better-Exploring-Prompting-Strategy-For-LLM-based-Metrics" class="headerlink" title="Which is better? Exploring Prompting Strategy For LLM-based Metrics"></a>Which is better? Exploring Prompting Strategy For LLM-based Metrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03754">http://arxiv.org/abs/2311.03754</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun Han, Jiyoon Lee, Pilsung Kang</li>
<li>for: 本研究探讨了使用大型自然语言处理模型（LLM）来评估自然语言生成（NLG）质量的可能性，以提高传统的相似性基于的评价 metric 的精度。</li>
<li>methods: 本研究使用了多种提问和提问技术来系统地分析NLG质量的评估，包括提问策略、分数汇总和解释性分析。</li>
<li>results: 研究发现了一些有效的提问模板和分数汇总策略，以及在具体上进行NLG质量评估时的解释性分析。此外，研究还发现了一些可靠的分数汇总策略，并对open-source LLM的解释性进行了分析。<details>
<summary>Abstract</summary>
This paper describes the DSBA submissions to the Prompting Large Language Models as Explainable Metrics shared task, where systems were submitted to two tracks: small and large summarization tracks. With advanced Large Language Models (LLMs) such as GPT-4, evaluating the quality of Natural Language Generation (NLG) has become increasingly paramount. Traditional similarity-based metrics such as BLEU and ROUGE have shown to misalign with human evaluation and are ill-suited for open-ended generation tasks. To address this issue, we explore the potential capability of LLM-based metrics, especially leveraging open-source LLMs. In this study, wide range of prompts and prompting techniques are systematically analyzed with three approaches: prompting strategy, score aggregation, and explainability. Our research focuses on formulating effective prompt templates, determining the granularity of NLG quality scores and assessing the impact of in-context examples on LLM-based evaluation. Furthermore, three aggregation strategies are compared to identify the most reliable method for aggregating NLG quality scores. To examine explainability, we devise a strategy that generates rationales for the scores and analyzes the characteristics of the explanation produced by the open-source LLMs. Extensive experiments provide insights regarding evaluation capabilities of open-source LLMs and suggest effective prompting strategies.
</details>
<details>
<summary>摘要</summary>
The study systematically analyzes a wide range of prompts and prompting techniques using three approaches: prompting strategy, score aggregation, and explainability. The focus is on formulating effective prompt templates, determining the granularity of NLG quality scores, and assessing the impact of in-context examples on LLM-based evaluation. Additionally, the paper compares three aggregation strategies to identify the most reliable method for aggregating NLG quality scores. To examine explainability, the paper devises a strategy that generates rationales for the scores and analyzes the characteristics of the explanations produced by the open-source LLMs.Extensive experiments provide insights into the evaluation capabilities of open-source LLMs and suggest effective prompting strategies. The findings suggest that LLM-based metrics have the potential to provide more accurate evaluations of NLG quality than traditional metrics, and that prompting techniques can significantly impact the quality of the generated text. The study contributes to the development of more effective and reliable evaluation methods for NLG systems, which is essential for advancing the field of natural language processing.
</details></li>
</ul>
<hr>
<h2 id="Unified-Low-Resource-Sequence-Labeling-by-Sample-Aware-Dynamic-Sparse-Finetuning"><a href="#Unified-Low-Resource-Sequence-Labeling-by-Sample-Aware-Dynamic-Sparse-Finetuning" class="headerlink" title="Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning"></a>Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03748">http://arxiv.org/abs/2311.03748</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/psunlpgroup/fish-dip">https://github.com/psunlpgroup/fish-dip</a></li>
<li>paper_authors: Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Peng Shi, Wenpeng Yin, Rui Zhang</li>
<li>For: 这 paper 的目的是 addresses the challenge of leveraging pre-trained language models (PLMs) for sequence labeling tasks in data-limited settings, where finetuning large models cannot properly generalize to the target format.* Methods: 该 paper 提出了 FISH-DIP，一种 sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process.* Results:  compared to full fine-tuning, FISH-DIP can smoothly optimize the model in low resource settings, offering up to 40% performance improvements depending on the target evaluation settings. Additionally, FISH-DIP performs comparably or better than in-context learning and other parameter-efficient fine-tuning approaches, notably in extreme low-resource settings.<details>
<summary>Abstract</summary>
Unified Sequence Labeling that articulates different sequence labeling problems such as Named Entity Recognition, Relation Extraction, Semantic Role Labeling, etc. in a generalized sequence-to-sequence format opens up the opportunity to make the maximum utilization of large language model knowledge toward structured prediction. Unfortunately, this requires formatting them into specialized augmented format unknown to the base pretrained language model (PLMs) necessitating finetuning to the target format. This significantly bounds its usefulness in data-limited settings where finetuning large models cannot properly generalize to the target format. To address this challenge and leverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic sparse finetuning strategy that selectively focuses on a fraction of parameters, informed by feedback from highly regressing examples, during the fine-tuning process. By leveraging the dynamism of sparsity, our approach mitigates the impact of well-learned samples and prioritizes underperforming instances for improvement in generalization. Across five tasks of sequence labeling, we demonstrate that FISH-DIP can smoothly optimize the model in low resource settings offering upto 40% performance improvements over full fine-tuning depending on target evaluation settings. Also, compared to in-context learning and other parameter-efficient fine-tuning approaches, FISH-DIP performs comparably or better, notably in extreme low-resource settings.
</details>
<details>
<summary>摘要</summary>
通过普适化序列标签问题，如命名实体识别、关系提取、semantic role labeling等，到通用的序列到序列形式开放了最大利用大语言模型知识的可能性。然而，这需要将它们格式化为特定的扩展格式，不знаком于基础预训练语言模型（PLMs），因此需要训练目标格式。这会限制其在数据有限的情况下使用，因为大型模型在目标格式上不能充分泛化。为解决这个挑战并有效地利用PLM知识，我们提出了鱼钓式动态稀缺训练策略（FISH-DIP）。在精细化过程中，我们 selectively 针对一部分参数，根据高度反例反馈，进行稀缺训练。通过利用动态稀缺的优势，我们的方法可以减轻已学习的样本的影响，并且优先级为不良实例进行改进，以提高通用性。在五种序列标签任务上，我们示示了FISH-DIP可以在低资源设置下简单地优化模型，提供最多40%的性能提升，具体取决于目标评估设置。此外，相比受 Context 学习和其他参数效率精细化方法，FISH-DIP在极低资源设置下表现相对或更好，特别是在极低资源设置下。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Structured-Information-for-Explainable-Multi-hop-Question-Answering-and-Reasoning"><a href="#Leveraging-Structured-Information-for-Explainable-Multi-hop-Question-Answering-and-Reasoning" class="headerlink" title="Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning"></a>Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03734">http://arxiv.org/abs/2311.03734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bcdnlp/structure-qa">https://github.com/bcdnlp/structure-qa</a></li>
<li>paper_authors: Ruosen Li, Xinya Du</li>
<li>for: 这个论文主要目标是提高大型语言模型（LLM）在多步问答中的表现，以及提高模型的推理能力。</li>
<li>methods: 这个论文使用了链条机制（CoT）来生成推理链和答案，以提高模型的多步推理能力。</li>
<li>results: 这个论文的实验和人工评估结果显示，该框架可以生成更准确的推理链，并在两个标准数据集上显著提高问答性能。此外，提取的 semantic structures 自然地提供了可读性好的解释，而不是生成的推理链和重要性基于的解释。<details>
<summary>Abstract</summary>
Neural models, including large language models (LLMs), achieve superior performance on multi-hop question-answering. To elicit reasoning capabilities from LLMs, recent works propose using the chain-of-thought (CoT) mechanism to generate both the reasoning chain and the answer, which enhances the model's capabilities in conducting multi-hop reasoning. However, several challenges still remain: such as struggling with inaccurate reasoning, hallucinations, and lack of interpretability. On the other hand, information extraction (IE) identifies entities, relations, and events grounded to the text. The extracted structured information can be easily interpreted by humans and machines (Grishman, 2019). In this work, we investigate constructing and leveraging extracted semantic structures (graphs) for multi-hop question answering, especially the reasoning process. Empirical results and human evaluations show that our framework: generates more faithful reasoning chains and substantially improves the QA performance on two benchmark datasets. Moreover, the extracted structures themselves naturally provide grounded explanations that are preferred by humans, as compared to the generated reasoning chains and saliency-based explanations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在多步问答 task 上表现出色，以便激发理解能力。为了从 LLMs 中提取理解能力，latest works 提议使用链条思维（CoT）机制生成问答链和答案，从而提高模型在多步理解中的能力。然而，还有一些挑战：如偏差的理解、幻觉和解释不可读性。在另一方面，信息抽取（IE）可以提取到文本中的实体、关系和事件，并将其固化为结构化的信息，这些信息可以轻松地被人类和机器解释（Grishman, 2019）。在这个工作中，我们研究构建和利用提取的semantic structure（图）来进行多步问答，特别是理解过程。我们的实验结果和人类评估表明，我们的框架：可以生成更 faithful 的理解链和substantially 改善在两个 benchmark 数据集上的问答性能。此外，提取的结构本身就提供了固化的解释，与生成的理解链和Saliency-based解释相比，更被人类首选。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-for-Few-shot-Continual-Active-Learning"><a href="#Learning-to-Learn-for-Few-shot-Continual-Active-Learning" class="headerlink" title="Learning to Learn for Few-shot Continual Active Learning"></a>Learning to Learn for Few-shot Continual Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03732">http://arxiv.org/abs/2311.03732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stella Ho, Ming Liu, Shang Gao, Longxiang Gao</li>
<li>for: 解决几个任务的稳定性和新领域的пластично性问题。</li>
<li>methods: 使用元学习和经验回放来解决稳定性和пластично性之间的贸易OFF。</li>
<li>results: 通过随机抽取活动学习和记忆样本选择策略，实现了快速适应新任务的目标。<details>
<summary>Abstract</summary>
Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data is inadequate, and unlabeled data is abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address the trade-off between stability and plasticity. As a result, it finds an optimal initialization that efficiently utilizes annotated information for fast adaptation while preventing catastrophic forgetting of past tasks. We conduct extensive experiments to validate the effectiveness of the proposed method and analyze the effect of various active learning strategies and memory sample selection methods in a few-shot CAL setup. Our experiment results demonstrate that random sampling is the best default strategy for both active learning and memory sample selection to solve few-shot CAL problems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Survey-of-Large-Language-Models-Attribution"><a href="#A-Survey-of-Large-Language-Models-Attribution" class="headerlink" title="A Survey of Large Language Models Attribution"></a>A Survey of Large Language Models Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03731">http://arxiv.org/abs/2311.03731</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HITsz-TMG/awesome-llm-attributions">https://github.com/HITsz-TMG/awesome-llm-attributions</a></li>
<li>paper_authors: Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian Hu, Aiguo Wu, Min Zhang</li>
<li>for: 这篇论文主要为了探讨开放领域生成系统中使用的归因机制，特别是大语言模型。</li>
<li>methods: 论文提出了各种归因方法，包括权重调整、排名、权重评估等方法。</li>
<li>results: 论文指出，归因机制可以提高对话型AI系统的可靠性和准确性，但同时也存在一些问题，如知识库的抽象、内置的偏见和过度归因的缺点。<details>
<summary>Abstract</summary>
Open-domain generative systems have gained significant attention in the field of conversational AI (e.g., generative search engines). This paper presents a comprehensive review of the attribution mechanisms employed by these systems, particularly large language models. Though attribution or citation improve the factuality and verifiability, issues like ambiguous knowledge reservoirs, inherent biases, and the drawbacks of excessive attribution can hinder the effectiveness of these systems. The aim of this survey is to provide valuable insights for researchers, aiding in the refinement of attribution methodologies to enhance the reliability and veracity of responses generated by open-domain generative systems. We believe that this field is still in its early stages; hence, we maintain a repository to keep track of ongoing studies at https://github.com/HITsz-TMG/awesome-llm-attributions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>开放领域生成系统在对话AI中受到了广泛关注（例如生成搜索引擎）。这篇评论文章介绍了这些系统使用的归因机制，特别是大语言模型。虽然归因或参考可以提高事实性和可靠性，但是存在杂乱知识库、内生偏见和过度归因的问题可能会妨碍这些系统的效果。本评论的目的是为研究人员提供有价值的洞察，以便通过修复归因方法来提高开放领域生成系统的可靠性和真实性。我们认为这个领域还处在早期阶段，因此我们维护了一个存储库，以跟踪进行中的研究：https://github.com/HITsz-TMG/awesome-llm-attributions。
</details></li>
</ul>
<hr>
<h2 id="Bilingual-Corpus-Mining-and-Multistage-Fine-Tuning-for-Improving-Machine-Translation-of-Lecture-Transcripts"><a href="#Bilingual-Corpus-Mining-and-Multistage-Fine-Tuning-for-Improving-Machine-Translation-of-Lecture-Transcripts" class="headerlink" title="Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts"></a>Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03696">http://arxiv.org/abs/2311.03696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shyyhs/CourseraParallelCorpusMining">https://github.com/shyyhs/CourseraParallelCorpusMining</a></li>
<li>paper_authors: Haiyue Song, Raj Dabre, Chenhui Chu, Atsushi Fujita, Sadao Kurohashi<br>for: 这个论文主要目的是为了提高在线课程笔记翻译系统的质量。methods: 这篇论文提出了一种框架，用于从Coursera上公开的讲座中挖掘并构建平行 corpora，以提高笔记翻译系统的性能。这个框架使用了动态Programming基于 sentences的对齐算法，并使用cosine similarity来衡量对齐的准确率。results: 这篇论文的实验结果表明，使用这个框架和方法可以从Coursera上公开的讲座中挖掘出高质量的平行 corpora，并且可以提高笔记翻译系统的性能。在英文–日语和英文–中文讲座翻译中，这个框架提取了约50,000行的平行 corpora，并通过手动筛选创建了开发和测试集。通过机器翻译实验，这个研究表明了这些挖掘出来的平行 corpora可以帮助提高讲座笔记翻译质量。此外，这个研究还提供了收集和清洁 corpora、挖掘平行句、清理受到干扰的数据、创建高质量评估分割的指南。<details>
<summary>Abstract</summary>
Lecture transcript translation helps learners understand online courses, however, building a high-quality lecture machine translation system lacks publicly available parallel corpora. To address this, we examine a framework for parallel corpus mining, which provides a quick and effective way to mine a parallel corpus from publicly available lectures on Coursera. To create the parallel corpora, we propose a dynamic programming based sentence alignment algorithm which leverages the cosine similarity of machine-translated sentences. The sentence alignment F1 score reaches 96%, which is higher than using the BERTScore, LASER, or sentBERT methods. For both English--Japanese and English--Chinese lecture translations, we extracted parallel corpora of approximately 50,000 lines and created development and test sets through manual filtering for benchmarking translation performance. Through machine translation experiments, we show that the mined corpora enhance the quality of lecture transcript translation when used in conjunction with out-of-domain parallel corpora via multistage fine-tuning. Furthermore, this study also suggests guidelines for gathering and cleaning corpora, mining parallel sentences, cleaning noise in the mined data, and creating high-quality evaluation splits. For the sake of reproducibility, we have released the corpora as well as the code to create them. The dataset is available at https://github.com/shyyhs/CourseraParallelCorpusMining.
</details>
<details>
<summary>摘要</summary>
讲义笔记翻译帮助学习者理解在线课程，但建立高质量讲义机器翻译系统缺乏公共可用并行词库。为此，我们提出了并行词库挖掘框架，该框架可以快速地挖掘来自Coursera上公开的讲义课程中的并行词库。为创建并行词库，我们提议使用动态计划方法进行句子对齐算法，利用机器翻译后的句子的央角相似性。句子对齐F1分数达96%，高于使用BERTScore、LASER或sentBERT方法。我们对英语-日语和英语-中文讲义笔记翻译进行了约50,000行的并行词库抽取，并通过手动筛选创建了开发和测试集。通过机器翻译实验，我们证明了挖掘出来的 corpora 可以提高讲义笔记翻译质量，并且可以与来自不同领域的并行词库进行多Stage精细调整。此外，这种研究还提供了搜集和清洁 corpora 的指南，以及挖掘并行句子、清理句子中噪音的方法。为了保持可重现性，我们将 corpora 及其创建代码发布到 GitHub。数据集可在https://github.com/shyyhs/CourseraParallelCorpusMining 中下载。
</details></li>
</ul>
<hr>
<h2 id="Dissecting-the-Runtime-Performance-of-the-Training-Fine-tuning-and-Inference-of-Large-Language-Models"><a href="#Dissecting-the-Runtime-Performance-of-the-Training-Fine-tuning-and-Inference-of-Large-Language-Models" class="headerlink" title="Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models"></a>Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03687">http://arxiv.org/abs/2311.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longteng Zhang, Xiang Liu, Zeyu Li, Xinglin Pan, Peijie Dong, Ruibo Fan, Rui Guo, Xin Wang, Qiong Luo, Shaohuai Shi, Xiaowen Chu</li>
<li>for: 本研究旨在对大语言模型（LLMs）的预训练、精度调整和服务器端的性能进行Benchmark，以便更好地选择适合的硬件和软件栈。</li>
<li>methods: 本研究使用了7B、13B和70B参数大小的LLMs，在三个8 GPU平台上进行了端到端性能测试，并对各种优化技术进行了评估，包括ZeRO、量化、重计算和FlashAttention。同时，本研究还进行了LLMs中各模块的Runtime分析。</li>
<li>results: 研究发现，在不同的硬件和软件栈上，LLMs的运行时间可以有很大差异。在不同的优化技术和硬件平台上，可以获得更好的性能。此外，本研究还发现了一些可能的优化机会，可以帮助研究人员在未来更好地优化LLMs的运行时间。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have seen great advance in both academia and industry, and their popularity results in numerous open-source frameworks and techniques in accelerating LLM pre-training, fine-tuning, and inference. Training and deploying LLMs are expensive as it requires considerable computing resources and memory, hence many efficient approaches have been developed for improving system pipelines as well as operators. However, the runtime performance can vary significantly across hardware and software stacks, which makes it difficult to choose the best configuration. In this work, we aim to benchmark the performance from both macro and micro perspectives. First, we benchmark the end-to-end performance of pre-training, fine-tuning, and serving LLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and 70B) on three 8-GPU platforms with and without individual optimization techniques, including ZeRO, quantization, recomputation, FlashAttention. Then, we dive deeper to provide a detailed runtime analysis of the sub-modules, including computing and communication operators in LLMs. For end users, our benchmark and findings help better understand different optimization techniques, training and inference frameworks, together with hardware platforms in choosing configurations for deploying LLMs. For researchers, our in-depth module-wise analyses discover potential opportunities for future work to further optimize the runtime performance of LLMs.
</details>
<details>
<summary>摘要</summary>
首先，我们将在不同大小的 LLMs （7B、13B和70B）上进行终端性能测试，并在三个8核心平台上测试，包括无ZeRO、量化、重computation和FlashAttention等个性化优化技术。然后，我们会进行详细的运行时分析，探讨 LLMs 中计算和通信操作的性能。对于用户来说，我们的benchmark和发现可以帮助他们更好地理解不同的优化技术、训练和推理框架以及硬件平台，以便更好地选择 LLMs 的部署配置。对于研究人员来说，我们的深入模块化分析可以探讨未来可能出现的可能性，以便进一步优化 LLMs 的运行时性能。
</details></li>
</ul>
<hr>
<h2 id="CBSiMT-Mitigating-Hallucination-in-Simultaneous-Machine-Translation-with-Weighted-Prefix-to-Prefix-Training"><a href="#CBSiMT-Mitigating-Hallucination-in-Simultaneous-Machine-Translation-with-Weighted-Prefix-to-Prefix-Training" class="headerlink" title="CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with Weighted Prefix-to-Prefix Training"></a>CBSiMT: Mitigating Hallucination in Simultaneous Machine Translation with Weighted Prefix-to-Prefix Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03672">http://arxiv.org/abs/2311.03672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengge Liu, Wen Zhang, Xiang Li, Yanzhi Tian, Yuhang Guo, Jian Luan, Bin Wang, Shuoying Chen</li>
<li>for: 该研究旨在提高同时翻译（SiMT）任务中的翻译质量，尤其是在开始翻译前只有部分源语句可用时。</li>
<li>methods: 该研究提出了一种自信量基于的同时翻译机器翻译（CBSiMT）框架，利用模型自信量来识别幻化token并通过加权前缀-前缀训练来减少其影响。</li>
<li>results: 实验结果表明，该方法可以在不同的延迟环境下提高翻译质量，最高达2个BLEU分数提升，并且可以在低延迟环境下提供更好的翻译结果。<details>
<summary>Abstract</summary>
Simultaneous machine translation (SiMT) is a challenging task that requires starting translation before the full source sentence is available. Prefix-to-prefix framework is often applied to SiMT, which learns to predict target tokens using only a partial source prefix. However, due to the word order difference between languages, misaligned prefix pairs would make SiMT models suffer from serious hallucination problems, i.e. target outputs that are unfaithful to source inputs. Such problems can not only produce target tokens that are not supported by the source prefix, but also hinder generating the correct translation by receiving more source words. In this work, we propose a Confidence-Based Simultaneous Machine Translation (CBSiMT) framework, which uses model confidence to perceive hallucination tokens and mitigates their negative impact with weighted prefix-to-prefix training. Specifically, token-level and sentence-level weights are calculated based on model confidence and acted on the loss function. We explicitly quantify the faithfulness of the generated target tokens using the token-level weight, and employ the sentence-level weight to alleviate the disturbance of sentence pairs with serious word order differences on the model. Experimental results on MuST-C English-to-Chinese and WMT15 German-to-English SiMT tasks demonstrate that our method can consistently improve translation quality at most latency regimes, with up to 2 BLEU scores improvement at low latency.
</details>
<details>
<summary>摘要</summary>
simultaneous机器翻译（SiMT）是一项具有挑战性的任务，需要在源句子完全可用之前开始翻译。 prefix-to-prefix框架经常用于SiMT，这种框架学习使用只有部分源前缀来预测目标符。然而，由于语言之间的单词顺序差异，可能会出现误差的前缀对，导致SiMT模型受到严重的幻觉问题，即目标输出不符合源输入。这些问题不仅会生成不支持源前缀的目标符，还会阻碍模型生成正确的翻译。在这种情况下，我们提出了一种 confidence-based 同时机器翻译（CBSiMT）框架，使用模型 confidence 来识别幻觉符并通过轮征 prefix-to-prefix 训练来减少其影响。具体来说，我们根据模型 confidence 计算 token-level 和 sentence-level 权重，然后在损失函数中应用这些权重。我们使用 token-level 权重来评估生成的目标符的具体性，并使用 sentence-level 权重来减轻word order差异的影响。我们在 MuST-C 英-中翻译和 WMT15 德-英翻译 SiMT 任务上进行实验，结果表明，我们的方法可以在不同的延迟环境下一直提高翻译质量，最高达到2 BLEU 分数提升。
</details></li>
</ul>
<hr>
<h2 id="Generalization-of-NLP-Models-Notion-and-Causation"><a href="#Generalization-of-NLP-Models-Notion-and-Causation" class="headerlink" title="Generalization of NLP Models: Notion and Causation"></a>Generalization of NLP Models: Notion and Causation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03663">http://arxiv.org/abs/2311.03663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor</li>
<li>for: 本研究旨在探讨模型在不同数据集中的普适性，以及各种因素对模型普适性的影响。</li>
<li>methods: 本研究使用了严格的实验方法，以确保内部有效性，并对数据中的假 correlate进行分析，以避免模型受到假 correlate的影响。</li>
<li>results: 研究发现，模型在不同数据集中的性能会受到各种因素的影响，包括数据中的假 correlate。通过对实验中的假 correlate进行分析，可以更好地理解模型在不同数据集中的普适性。<details>
<summary>Abstract</summary>
The NLP community typically relies on performance of a model on a held-out test set to assess generalization. Performance drops observed in datasets outside of official test sets are generally attributed to "out-of-distribution'' effects. Here, we explore the foundations of generalizability and study the various factors that affect it, articulating generalizability lessons from clinical studies. In clinical research generalizability depends on (a) internal validity of experiments to ensure controlled measurement of cause and effect, and (b) external validity or transportability of the results to the wider population. We present the need to ensure internal validity when building machine learning models in natural language processing, especially where results may be impacted by spurious correlations in the data. We demonstrate how spurious factors, such as the distance between entities in relation extraction tasks, can affect model internal validity and in turn adversely impact generalization. We also offer guidance on how to analyze generalization failures.
</details>
<details>
<summary>摘要</summary>
nlp社区通常通过模型在封闭测试集上的表现来评估泛化性。模型在外部数据集上的表现下降通常被归结为“非常区”的效果。我们探究泛化性的基础和它受到哪些因素的影响，从临床研究中提炼泛化性评价的教训。在自然语言处理领域建立机器学习模型时，特别是在数据中存在误差相关性的情况下，需要确保模型的内部有效性。我们示例了如何使用误差因素分析泛化失败的原因。Note: "泛化性" (generalizability) in Chinese is often translated as "泛化" (generalization), but the two terms have slightly different connotations. "泛化" generally refers to the ability of a model to perform well on new, unseen data, while "泛化性" emphasizes the robustness and general applicability of the model across different domains or populations. In this text, I have used "泛化性" to emphasize the importance of considering the broader context and potential applications of the model.
</details></li>
</ul>
<hr>
<h2 id="Innovation-and-Word-Usage-Patterns-in-Machine-Learning"><a href="#Innovation-and-Word-Usage-Patterns-in-Machine-Learning" class="headerlink" title="Innovation and Word Usage Patterns in Machine Learning"></a>Innovation and Word Usage Patterns in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03633">http://arxiv.org/abs/2311.03633</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vitorbborges/monografia-PET22">https://github.com/vitorbborges/monografia-PET22</a></li>
<li>paper_authors: Vítor Bandeira Borges, Daniel Oliveira Cajueiro</li>
<li>for: 本研究探讨机器学习研究的动态领域发展。</li>
<li>methods: 通过矩阵 Dirichlet 分配，检测出机器学习领域中突出的主题和基本概念，然后进行全面分析，跟踪这些主题的演化轨迹。</li>
<li>results: 通过卷积-黑eli均度度量，衡量研究贡献的新颖度和差异度，得出机器学习领域中主要研究人员和期刊&#x2F;会议的重要性。<details>
<summary>Abstract</summary>
In this study, we delve into the dynamic landscape of machine learning research evolution. Initially, through the utilization of Latent Dirichlet Allocation, we discern pivotal themes and fundamental concepts that have emerged within the realm of machine learning. Subsequently, we undertake a comprehensive analysis to track the evolutionary trajectories of these identified themes. To quantify the novelty and divergence of research contributions, we employ the Kullback-Leibler Divergence metric. This statistical measure serves as a proxy for ``surprise'', indicating the extent of differentiation between the content of academic papers and the subsequent developments in research. By amalgamating these insights, we gain the ability to ascertain the pivotal roles played by prominent researchers and the significance of specific academic venues (periodicals and conferences) within the machine learning domain.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们探讨机器学习研究的动态景观。首先，通过利用秘密分配（Latent Dirichlet Allocation），我们找到了机器学习领域中突出的主题和基本概念。然后，我们进行了全面的分析，跟踪这些确定的主题的演化轨迹。为了衡量研究贡献的新鲜度和偏离度，我们使用库拉克-莱布лер分配度量。这个统计量作为“意外”的代表，反映了学术论文的内容和后续研究的差异。通过汇集这些意见，我们获得了评估机器学习领域中重要研究者和期刊（期刊和会议）的重要性的能力。
</details></li>
</ul>
<hr>
<h2 id="GNAT-A-General-Narrative-Alignment-Tool"><a href="#GNAT-A-General-Narrative-Alignment-Tool" class="headerlink" title="GNAT: A General Narrative Alignment Tool"></a>GNAT: A General Narrative Alignment Tool</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03627">http://arxiv.org/abs/2311.03627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanzir Pial, Steven Skiena</li>
<li>for: 本研究是为了解决对短版文本的 narrative alignment 问题，尤其是对翻译和摘要等短文本的对比和对Alignment。</li>
<li>methods: 本研究使用 Smith-Waterman 算法和现代文本相似度指标，将生物信息学和现代文本分析相结合，实现了一种通用的 narrative alignment 方法。</li>
<li>results: 本研究在四个不同的问题领域中应用和评估了GNAT工具，包括摘要到书籍对 alignment、翻译书籍对 alignment、短篇文本对 alignment 和 плаги产检测等，并达到了高度和性能。<details>
<summary>Abstract</summary>
Algorithmic sequence alignment identifies similar segments shared between pairs of documents, and is fundamental to many NLP tasks. But it is difficult to recognize similarities between distant versions of narratives such as translations and retellings, particularly for summaries and abridgements which are much shorter than the original novels.   We develop a general approach to narrative alignment coupling the Smith-Waterman algorithm from bioinformatics with modern text similarity metrics. We show that the background of alignment scores fits a Gumbel distribution, enabling us to define rigorous p-values on the significance of any alignment. We apply and evaluate our general narrative alignment tool (GNAT) on four distinct problem domains differing greatly in both the relative and absolute length of documents, namely summary-to-book alignment, translated book alignment, short story alignment, and plagiarism detection -- demonstrating the power and performance of our methods.
</details>
<details>
<summary>摘要</summary>
算法序列对齐可以识别文档对的相似段落，这是许多自然语言处理任务的基础。但是对于远程的 narrative 翻译和重新 narración，特别是摘要和缩写，寻找相似之处很难。我们开发了一种通用的 narative 对齐方法，结合 Smith-Waterman 算法和现代文本相似度度量。我们发现对齐背景的对齐分布适合 Gumbel 分布，因此我们可以定义精确的 p-值，用于评估对齐的重要性。我们应用和评估我们的通用 narative 对齐工具（GNAT）在四种不同的问题领域中，包括摘要到书籍对齐、翻译书籍对齐、短篇对齐和抄袭检测，并在这些领域中证明了我们的方法的能力和性能。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.CL_2023_11_07/" data-id="closbronm00e50g887zml38rk" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/cs.LG_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T10:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/cs.LG_2023_11_07/">cs.LG - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Device-Sampling-and-Resource-Optimization-for-Federated-Learning-in-Cooperative-Edge-Networks"><a href="#Device-Sampling-and-Resource-Optimization-for-Federated-Learning-in-Cooperative-Edge-Networks" class="headerlink" title="Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks"></a>Device Sampling and Resource Optimization for Federated Learning in Cooperative Edge Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04350">http://arxiv.org/abs/2311.04350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Su Wang, Roberto Morabito, Seyyedali Hosseinalipour, Mung Chiang, Christopher G. Brinton</li>
<li>for: 该研究目的是提高 Federated Learning（FedL）训练精度，并最小化数据处理和Device-to-Device（D2D）通信资源消耗。</li>
<li>methods: 该研究使用了一种新的优化方法，即智能设备采样和设备间数据下载配置选择，以最大化 FedL 训练精度，同时最小化数据处理和D2D通信资源消耗。该方法包括对 D2D 下载子问题的理论分析，并开发了一个高效的分析器。</li>
<li>results: 该研究通过使用图 convolutional neural networks（GCNs）学习网络特性、采样节点和D2D数据下载的关系，以最大化 FedL 精度。实验结果表明，该方法比Literature中常见的设备采样方法更高效，具有更好的机器学习模型性能、数据处理负担和能量消耗。<details>
<summary>Abstract</summary>
The conventional federated learning (FedL) architecture distributes machine learning (ML) across worker devices by having them train local models that are periodically aggregated by a server. FedL ignores two important characteristics of contemporary wireless networks, however: (i) the network may contain heterogeneous communication/computation resources, and (ii) there may be significant overlaps in devices' local data distributions. In this work, we develop a novel optimization methodology that jointly accounts for these factors via intelligent device sampling complemented by device-to-device (D2D) offloading. Our optimization methodology aims to select the best combination of sampled nodes and data offloading configuration to maximize FedL training accuracy while minimizing data processing and D2D communication resource consumption subject to realistic constraints on the network topology and device capabilities. Theoretical analysis of the D2D offloading subproblem leads to new FedL convergence bounds and an efficient sequential convex optimizer. Using these results, we develop a sampling methodology based on graph convolutional networks (GCNs) which learns the relationship between network attributes, sampled nodes, and D2D data offloading to maximize FedL accuracy. Through evaluation on popular datasets and real-world network measurements from our edge testbed, we find that our methodology outperforms popular device sampling methodologies from literature in terms of ML model performance, data processing overhead, and energy consumption.
</details>
<details>
<summary>摘要</summary>
传统的联邦学习（FedL）架构将机器学习（ML）分布在工作设备上，通过 periodic 将本地模型由服务器集成。然而，FedL 忽略了当代无线网络中两个重要特征：（一）网络可能包含不同的通信/计算资源，以及（二）设备的本地数据分布可能存在重叠。在这项工作中，我们开发了一种新的优化方法，通过智能设备采样和设备间通信（D2D）卸载来同时考虑这两个因素。我们的优化方法的目标是选择最佳的采样节点和数据卸载配置，以最大化 FedL 训练精度，同时最小化数据处理和D2D通信资源消耗，并且遵循网络拓扑和设备能力的实际约束。我们的分析表明，D2D卸载子问题可以得到新的 FedL 整合约束，以及一种高效的顺序凸优化器。基于图 convolutional neural networks（GCNs），我们开发了一种采样方法，可以学习网络特征、采样节点和D2D数据卸载之间的关系，以最大化 FedL 精度。通过使用实际网络测试床的数据和实际网络测量，我们发现，我们的方法可以比传统的设备采样方法在 ML 模型性能、数据处理开销和能源消耗方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="InstrumentGen-Generating-Sample-Based-Musical-Instruments-From-Text"><a href="#InstrumentGen-Generating-Sample-Based-Musical-Instruments-From-Text" class="headerlink" title="InstrumentGen: Generating Sample-Based Musical Instruments From Text"></a>InstrumentGen: Generating Sample-Based Musical Instruments From Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04339">http://arxiv.org/abs/2311.04339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shahan Nercessian, Johannes Imort</li>
<li>for: 本研究目标是生成基于文本提示的样本音乐 инструментов，以推进自动化样本音乐 инструментов的研究。</li>
<li>methods: 本研究提出了 InstrumentGen 模型，它是一种基于文本提示的生成声音框架，可以根据乐器类型、源类型、音高（88键谱中的音高）、幅度和文本&#x2F;声音嵌入来conditioning。</li>
<li>results: 本研究实现了一个基本的文本-到-乐器基线，并推进了样本音乐 инструментов的自动生成研究领域。<details>
<summary>Abstract</summary>
We introduce the text-to-instrument task, which aims at generating sample-based musical instruments based on textual prompts. Accordingly, we propose InstrumentGen, a model that extends a text-prompted generative audio framework to condition on instrument family, source type, pitch (across an 88-key spectrum), velocity, and a joint text/audio embedding. Furthermore, we present a differentiable loss function to evaluate the intra-instrument timbral consistency of sample-based instruments. Our results establish a foundational text-to-instrument baseline, extending research in the domain of automatic sample-based instrument generation.
</details>
<details>
<summary>摘要</summary>
我们介绍了文本到乐器任务，该任务目的是基于文本提示生成样本化的乐器。我们提议了InstrumentGen模型，该模型是一种基于文本提示生成音频框架的扩展， condition on 乐器家族、源类型、音高（88键谱中的音高）、速度和joint文本/音频嵌入。此外，我们提出了可微分损失函数，用于评估样本化乐器内部的时间性响应。我们的结果建立了文本到乐器基线，推动了自动样本化乐器生成研究领域的发展。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convex-Methods-for-Constrained-Linear-Bandits"><a href="#Convex-Methods-for-Constrained-Linear-Bandits" class="headerlink" title="Convex Methods for Constrained Linear Bandits"></a>Convex Methods for Constrained Linear Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04338">http://arxiv.org/abs/2311.04338</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Afsharrad, Ahmadreza Moradipari, Sanjay Lall</li>
<li>for: 这研究旨在 investigate the computational aspects of safe bandit algorithms, specifically safe linear bandits, and develop a framework that leverages convex programming tools to create computationally efficient policies.</li>
<li>methods: 该研究使用 convex programming tools to characterize the properties of the optimal policy for safe linear bandit problem, and then propose an end-to-end pipeline of safe linear bandit algorithms that only involves solving convex problems.</li>
<li>results: 研究人们提出了一个完整的安全线性帮助器框架，并进行了数值evaluation of the proposed methods.<details>
<summary>Abstract</summary>
Recently, bandit optimization has received significant attention in real-world safety-critical systems that involve repeated interactions with humans. While there exist various algorithms with performance guarantees in the literature, practical implementation of the algorithms has not received as much attention. This work presents a comprehensive study on the computational aspects of safe bandit algorithms, specifically safe linear bandits, by introducing a framework that leverages convex programming tools to create computationally efficient policies. In particular, we first characterize the properties of the optimal policy for safe linear bandit problem and then propose an end-to-end pipeline of safe linear bandit algorithms that only involves solving convex problems. We also numerically evaluate the performance of our proposed methods.
</details>
<details>
<summary>摘要</summary>
最近，匪师优化已经在实际中受到了严重的关注，特别是在安全关键系统中，这些系统需要重复地与人类进行交互。虽然文献中存在许多约束性能的算法，但实际实施这些算法却未得到了尽分的关注。本文提出了一个包括使用凸 программирова的框架，以创造高效的安全匪师策略的完整研究。特别是，我们首先描述了安全直线匪师问题的优化策略的属性，然后提出了一个综合的安全直线匪师算法链，这个链只需要解凸问题。我们还进行了数值评估我们提出的方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Lie-Point-Symmetry-and-Physics-Informed-Networks"><a href="#Lie-Point-Symmetry-and-Physics-Informed-Networks" class="headerlink" title="Lie Point Symmetry and Physics Informed Networks"></a>Lie Point Symmetry and Physics Informed Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04293">http://arxiv.org/abs/2311.04293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tara Akhound-Sadegh, Laurence Perreault-Levasseur, Johannes Brandstetter, Max Welling, Siamak Ravanbakhsh</li>
<li>for: 提高神经网络的通用性，使其能更好地解决部分 diferencial equation (PDE) 问题。</li>
<li>methods: 利用 Lie 点Symmetries 在 physics-informed neural networks (PINNs) 中进行集成，并提出一个基于 Lie 点Symmetries 的损失函数，使神经网络学习 PDE 解的同时也学习了其对应的 Lie 点Symmetries。</li>
<li>results: 实验表明，通过 Lie 点Symmetries 的 inductive bias，PINNs 的样本效率得到了大幅提高。<details>
<summary>Abstract</summary>
Symmetries have been leveraged to improve the generalization of neural networks through different mechanisms from data augmentation to equivariant architectures. However, despite their potential, their integration into neural solvers for partial differential equations (PDEs) remains largely unexplored. We explore the integration of PDE symmetries, known as Lie point symmetries, in a major family of neural solvers known as physics-informed neural networks (PINNs). We propose a loss function that informs the network about Lie point symmetries in the same way that PINN models try to enforce the underlying PDE through a loss function. Intuitively, our symmetry loss ensures that the infinitesimal generators of the Lie group conserve the PDE solutions. Effectively, this means that once the network learns a solution, it also learns the neighbouring solutions generated by Lie point symmetries. Empirical evaluations indicate that the inductive bias introduced by the Lie point symmetries of the PDEs greatly boosts the sample efficiency of PINNs.
</details>
<details>
<summary>摘要</summary>
对不同类型的神经网络进行了各种机制的同谱化，从数据扩充到等Variational architecture。然而，尽管它们的潜力很大，它们在物理学信息泛化神经网络（PINNs）中的集成仍然很少explored。我们在PINN模型中集成了解析Symmetries，即Lie点Symmetries，并提出了一个loss函数，该loss函数告诉网络关于Lie点Symmetries的信息。 intuitively，我们的Symmetry loss garantizar que los generadores infinitesimales del grupo de Lie conservan las soluciones de las Ecuaciones diferenciales parciales（PDEs）。 esto significa que, una vez que la red aprende una solución, también aprende las soluciones vecinas generadas por los symmetries de Lie. los evaluaciones empíricas indican que la bias inductivo introducido por los symmetries de Lie de las Ecuaciones diferenciales parciales mejora significativamente la eficiencia de muestras de PINNs.
</details></li>
</ul>
<hr>
<h2 id="Compilation-of-product-formula-Hamiltonian-simulation-via-reinforcement-learning"><a href="#Compilation-of-product-formula-Hamiltonian-simulation-via-reinforcement-learning" class="headerlink" title="Compilation of product-formula Hamiltonian simulation via reinforcement learning"></a>Compilation of product-formula Hamiltonian simulation via reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04285">http://arxiv.org/abs/2311.04285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leamarion/rl-for-compilation-of-product-formula-hamiltonian-simulation">https://github.com/leamarion/rl-for-compilation-of-product-formula-hamiltonian-simulation</a></li>
<li>paper_authors: Lea M. Trenkwalder, Eleanor Scerri, Thomas E. O’Brien, Vedran Dunjko</li>
<li>for: 这个研究是关于量子计算机中的汉密尔顿模拟问题，尤其是在量子计算机上实现汉密尔顿模拟时的Optimization问题。</li>
<li>methods: 这个研究使用了Trotterization方法，这是一种使用准确的approximation $e^{i\sum_jA_j}\sim \prod_je^{iA_j}$和其他更高阶 corrections的方法。然而，这还留下了一个问题：操作顺序（即在product中j的顺序）的选择。</li>
<li>results: 这个研究发现，可以通过使用机器学习技术来优化compile任务，特别是使用reinforcement learning。这个方法可以在gate counts上带来约12%的提升，相比于第二best方法，而且可以在整个问题家族上generalize。<details>
<summary>Abstract</summary>
Hamiltonian simulation is believed to be one of the first tasks where quantum computers can yield a quantum advantage. One of the most popular methods of Hamiltonian simulation is Trotterization, which makes use of the approximation $e^{i\sum_jA_j}\sim \prod_je^{iA_j}$ and higher-order corrections thereto. However, this leaves open the question of the order of operations (i.e. the order of the product over $j$, which is known to affect the quality of approximation). In some cases this order is fixed by the desire to minimise the error of approximation; when it is not the case, we propose that the order can be chosen to optimize compilation to a native quantum architecture. This presents a new compilation problem -- order-agnostic quantum circuit compilation -- which we prove is NP-hard in the worst case. In lieu of an easily-computable exact solution, we turn to methods of heuristic optimization of compilation. We focus on reinforcement learning due to the sequential nature of the compilation task, comparing it to simulated annealing and Monte Carlo tree search. While two of the methods outperform a naive heuristic, reinforcement learning clearly outperforms all others, with a gain of around 12% with respect to the second-best method and of around 50% compared to the naive heuristic in terms of the gate count. We further test the ability of RL to generalize across instances of the compilation problem, and find that a single learner is able to solve entire problem families. This demonstrates the ability of machine learning techniques to provide assistance in an order-agnostic quantum compilation task.
</details>
<details>
<summary>摘要</summary>
希amiltonian simulate是一个能量计算机可以获得量子优势的首要任务。使用Trotterization方法是最受欢迎的方法，该方法使用近似关系 $e^{i\sum_jA_j}\approx \prod_je^{iA_j}$ 和更高阶误差。然而，这还留下了操作顺序的问题（即在 $j$ 上的产品顺序，这知道会影响近似的质量）。在某些情况下，这个顺序是由望得到最小化误差而决定的；当它不是的情况下，我们提议可以选择优化到本地量子架构。这个问题被称为order-agnostic量子电路编译问题，我们证明其在最坏情况下是NP困难的。在没有可计算的精确解的情况下，我们转而使用机器学习方法进行优化编译。我们主要关注了强化学习，因为编译任务具有顺序性。我们对比了随机搜索和 Monte Carlo tree search，并发现了一个名为强化学习的方法可以准确地解决这个问题。在一个实验中，我们发现强化学习可以在gate Count方面比第二最佳方法提高约12%，并且可以在约50%的情况下比第二最佳方法提高约50%。此外，我们还测试了强化学习的泛化能力，发现一个学习者可以解决整个编译问题家族。这表明机器学习技术可以为order-agnostic量子编译任务提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Breaking-the-Heavy-Tailed-Noise-Barrier-in-Stochastic-Optimization-Problems"><a href="#Breaking-the-Heavy-Tailed-Noise-Barrier-in-Stochastic-Optimization-Problems" class="headerlink" title="Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems"></a>Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04161">http://arxiv.org/abs/2311.04161</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Puchkin, Eduard Gorbunov, Nikolay Kutuzov, Alexander Gasnikov</li>
<li>for: 这篇论文关注了 Stochastic Optimization 问题中具有极大尾部噪声的情况，并证明在这种情况下，可以获得更快的收敛速率，比如 $\mathcal{O}(K^{-2(\alpha - 1)&#x2F;\alpha})$。</li>
<li>methods: 作者使用了smoothed medians of means来稳定抽象 gradient，并证明了这种方法具有较低的偏误和可控的方差。</li>
<li>results: 作者通过将这种方法引入 clipped-SGD 和 clipped-SSTM 中，得到了新的高 probabilty 复杂度下界。<details>
<summary>Abstract</summary>
We consider stochastic optimization problems with heavy-tailed noise with structured density. For such problems, we show that it is possible to get faster rates of convergence than $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$, when the stochastic gradients have finite moments of order $\alpha \in (1, 2]$. In particular, our analysis allows the noise norm to have an unbounded expectation. To achieve these results, we stabilize stochastic gradients, using smoothed medians of means. We prove that the resulting estimates have negligible bias and controllable variance. This allows us to carefully incorporate them into clipped-SGD and clipped-SSTM and derive new high-probability complexity bounds in the considered setup.
</details>
<details>
<summary>摘要</summary>
我们考虑了随机估计问题，其中测量值具有重 tailed 杂质，并且具有结构化的数量分布。我们展示了在这种情况下，可以得到更快的速度增长率，比如 $\mathcal{O}(K^{-2(\alpha - 1)/\alpha})$，其中 $K$ 是迭代次数，$\alpha$ 是杂质的最大値。具体来说，我们的分析允许杂质的期望值为无限大。为了稳定随机梯度，我们使用了平滑的中值，即平滑的中值。我们证明了这些估计具有无视可控的偏差和方差。这使我们能够将其纳入 clipped-SGD 和 clipped-SSTM 中，并 deriv 出新的高机会范围内的可能性下界。
</details></li>
</ul>
<hr>
<h2 id="Computing-Approximate-ell-p-Sensitivities"><a href="#Computing-Approximate-ell-p-Sensitivities" class="headerlink" title="Computing Approximate $\ell_p$ Sensitivities"></a>Computing Approximate $\ell_p$ Sensitivities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04158">http://arxiv.org/abs/2311.04158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swati Padmanabhan, David P. Woodruff, Qiuyi, Zhang</li>
<li>for: 本文提出了一种简化方法来进行回归任务中的维度减少，通过计算数据点的敏感度来选择高敏感度数据点进行下折中值。</li>
<li>methods: 本文提供了一种高效的方法来计算$\ell_p$敏感度，包括计算$\ell_1$敏感度、全体$\ell_p$敏感度以及最大$\ell_1$敏感度。这些方法基于重要性权重的重要性抽象，可以在$O(n&#x2F;\alpha)$敏感度计算Cost中获得$\alpha$-近似的结果。</li>
<li>results: 本文的实验结果表明，在多个实际数据集中，可以快速地计算出大致相同的敏感度，并且实际的敏感度远低于理论预测，这表明实际数据集的内在效果维度较低。<details>
<summary>Abstract</summary>
Recent works in dimensionality reduction for regression tasks have introduced the notion of sensitivity, an estimate of the importance of a specific datapoint in a dataset, offering provable guarantees on the quality of the approximation after removing low-sensitivity datapoints via subsampling. However, fast algorithms for approximating $\ell_p$ sensitivities, which we show is equivalent to approximate $\ell_p$ regression, are known for only the $\ell_2$ setting, in which they are termed leverage scores.   In this work, we provide efficient algorithms for approximating $\ell_p$ sensitivities and related summary statistics of a given matrix. In particular, for a given $n \times d$ matrix, we compute $\alpha$-approximation to its $\ell_1$ sensitivities at the cost of $O(n/\alpha)$ sensitivity computations. For estimating the total $\ell_p$ sensitivity (i.e. the sum of $\ell_p$ sensitivities), we provide an algorithm based on importance sampling of $\ell_p$ Lewis weights, which computes a constant factor approximation to the total sensitivity at the cost of roughly $O(\sqrt{d})$ sensitivity computations. Furthermore, we estimate the maximum $\ell_1$ sensitivity, up to a $\sqrt{d}$ factor, using $O(d)$ sensitivity computations. We generalize all these results to $\ell_p$ norms for $p > 1$. Lastly, we experimentally show that for a wide class of matrices in real-world datasets, the total sensitivity can be quickly approximated and is significantly smaller than the theoretical prediction, demonstrating that real-world datasets have low intrinsic effective dimensionality.
</details>
<details>
<summary>摘要</summary>
近期关于维度减少的研究在回归任务中引入了敏感度的概念，即数据点的重要性度量，并提供了可证明的保证，即通过抽样除低敏感度数据点后，可以获得高质量的近似。然而，只有在 $\ell_2$ Setting 中有快速算法来approximate $\ell_p$ 敏感度，称为抓取力分数。在这个工作中，我们提供高效的算法来approximate $\ell_p$ 敏感度和相关的概要统计量。特别是，对于给定 $n \times d$ 矩阵，我们可以在 $\alpha$ approximation的成本下计算 $\ell_1$ 敏感度，需要 $O(n/\alpha)$ 敏感度计算。进一步，我们提供一种基于 $\ell_p$ Lewis 权重的重要样本计算方法，可以在 $O(\sqrt{d})$ 敏感度计算成本下计算总 $\ell_p$ 敏感度，并且可以获得 $\sqrt{d}$ 因子的近似。此外，我们可以使用 $O(d)$ 敏感度计算来估计最大 $\ell_1$ 敏感度，带有 $\sqrt{d}$ 因子的误差。我们扩展所有这些结果到 $\ell_p$  нор 中的 $p > 1$。最后，我们通过实验表明，在许多实际数据集中，总敏感度可以快速地被近似，并且与理论预测相比，实际数据集的内在有效维度很低。
</details></li>
</ul>
<hr>
<h2 id="Kernel-mean-and-noise-marginalised-Gaussian-processes-for-exoplanet-transits-and-H-0-inference"><a href="#Kernel-mean-and-noise-marginalised-Gaussian-processes-for-exoplanet-transits-and-H-0-inference" class="headerlink" title="Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference"></a>Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04153">http://arxiv.org/abs/2311.04153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zwei-beiner/transdimensional_sampler">https://github.com/zwei-beiner/transdimensional_sampler</a></li>
<li>paper_authors: Namu Kroupa, David Yallup, Will Handley, Michael Hobson</li>
<li>For: The paper extends Gaussian Process regression to include marginalization over the kernel choice and kernel hyperparameters, enabling Bayesian model comparison and inference of physical hyperparameters.* Methods: The method uses a transdimensional sampler to simultaneously sample over the discrete kernel choice and their hyperparameters, and nested sampling to sample from the joint posterior.* Results: The method was applied to synthetic data from exoplanet transit light curve simulations and real measurements of the Hubble parameter as a function of redshift, and inferred $H_0$ values were obtained. The kernel posterior of the cosmic chronometers dataset prefers a non-stationary linear kernel, and the datasets are shown to be not in tension with ln(R)&#x3D;12.17$\pm$0.02.Here’s the Chinese translation of the three points:* For: 这 paper 扩展 Gaussian Process regression 以包括 kernel 选择和 kernel 超参数的 marginalization，以实现 Bayesian 模型比较和物理超参数的推断。* Methods: 方法使用 transdimensional sampler 同时样本 discrete kernel 选择和其超参数，并使用嵌入式 sampling 从联合 posterior 中取样。* Results: 方法应用于 synthetic 数据 from exoplanet transit light curve simulations 和实际 redshift 为函数的 Hubble parameter  mesurements，并获得了 $H_0$ 值。 cosmic chronometers 数据集的 kernel  posterior 偏好 non-stationary linear kernel，而 datasets 不与 ln(R) &#x3D; 12.17$\pm$0.02 在 contradiction。<details>
<summary>Abstract</summary>
Using a fully Bayesian approach, Gaussian Process regression is extended to include marginalisation over the kernel choice and kernel hyperparameters. In addition, Bayesian model comparison via the evidence enables direct kernel comparison. The calculation of the joint posterior was implemented with a transdimensional sampler which simultaneously samples over the discrete kernel choice and their hyperparameters by embedding these in a higher-dimensional space, from which samples are taken using nested sampling. This method was explored on synthetic data from exoplanet transit light curve simulations. The true kernel was recovered in the low noise region while no kernel was preferred for larger noise. Furthermore, inference of the physical exoplanet hyperparameters was conducted. In the high noise region, either the bias in the posteriors was removed, the posteriors were broadened or the accuracy of the inference was increased. In addition, the uncertainty in mean function predictive distribution increased due to the uncertainty in the kernel choice. Subsequently, the method was extended to marginalisation over mean functions and noise models and applied to the inference of the present-day Hubble parameter, $H_0$, from real measurements of the Hubble parameter as a function of redshift, derived from the cosmologically model-independent cosmic chronometer and {\Lambda}CDM-dependent baryon acoustic oscillation observations. The inferred $H_0$ values from the cosmic chronometers, baryon acoustic oscillations and combined datasets are $H_0$ = 66$\pm$6 km/s/Mpc, $H_0$ = 67$\pm$10 km/s/Mpc and $H_0$ = 69$\pm$6 km/s/Mpc, respectively. The kernel posterior of the cosmic chronometers dataset prefers a non-stationary linear kernel. Finally, the datasets are shown to be not in tension with ln(R)=12.17$\pm$0.02.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="HyperS2V-A-Framework-for-Structural-Representation-of-Nodes-in-Hyper-Networks"><a href="#HyperS2V-A-Framework-for-Structural-Representation-of-Nodes-in-Hyper-Networks" class="headerlink" title="HyperS2V: A Framework for Structural Representation of Nodes in Hyper Networks"></a>HyperS2V: A Framework for Structural Representation of Nodes in Hyper Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04149">http://arxiv.org/abs/2311.04149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liushu2019/hypers2v">https://github.com/liushu2019/hypers2v</a></li>
<li>paper_authors: Shu Liu, Cameron Lai, Fujio Toriumi</li>
<li>for: 本研究旨在提出一种基于结构相似性的节点嵌入方法（HyperS2V），以便应用vector数据处理技术于网络数据。</li>
<li>methods: 本方法首先定义了hyper网络中节点的超度概念，然后提出了一种新的结构相似性度量函数，最后使用多尺度随机步骤框架生成结构嵌入。</li>
<li>results: 对于各种简单网络和真实网络，HyperS2V表现出了较高的解释能力和应用可能性，并且在下游任务中表现出了优于其他方法的性能。<details>
<summary>Abstract</summary>
In contrast to regular (simple) networks, hyper networks possess the ability to depict more complex relationships among nodes and store extensive information. Such networks are commonly found in real-world applications, such as in social interactions. Learning embedded representations for nodes involves a process that translates network structures into more simplified spaces, thereby enabling the application of machine learning approaches designed for vector data to be extended to network data. Nevertheless, there remains a need to delve into methods for learning embedded representations that prioritize structural aspects. This research introduces HyperS2V, a node embedding approach that centers on the structural similarity within hyper networks. Initially, we establish the concept of hyper-degrees to capture the structural properties of nodes within hyper networks. Subsequently, a novel function is formulated to measure the structural similarity between different hyper-degree values. Lastly, we generate structural embeddings utilizing a multi-scale random walk framework. Moreover, a series of experiments, both intrinsic and extrinsic, are performed on both toy and real networks. The results underscore the superior performance of HyperS2V in terms of both interpretability and applicability to downstream tasks.
</details>
<details>
<summary>摘要</summary>
对比于常见（简单）网络，卷积网络具有更复杂的节点关系和大量信息存储能力。这些网络在实际应用中很常见，如社交互动。学习节点嵌入表示需要将网络结构翻译成简化的空间，以便应用于向量数据上的机器学习方法进行扩展。然而，仍然需要研究优化节点嵌入的方法，以便更好地保持结构特征。本研究提出了 HyperS2V 节点嵌入方法，强调了卷积网络中节点的结构相似性。首先，我们定义了卷积网络中节点的超度，以捕捉卷积网络中节点的结构特征。然后，我们定义了一种新的函数，用于度量不同超度值之间的结构相似性。最后，我们使用多尺度随机漫步框架生成结构嵌入。此外，我们在各种内在和外在实验中，对真实和模拟网络进行了评估。结果表明，HyperS2V 在解释性和下游任务应用性方面具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-resolution-Time-Series-Transformer-for-Long-term-Forecasting"><a href="#Multi-resolution-Time-Series-Transformer-for-Long-term-Forecasting" class="headerlink" title="Multi-resolution Time-Series Transformer for Long-term Forecasting"></a>Multi-resolution Time-Series Transformer for Long-term Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04147">http://arxiv.org/abs/2311.04147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yitian Zhang, Liheng Ma, Soumyasundar Pal, Yingxue Zhang, Mark Coates</li>
<li>for: 这个研究是为了提高时间序列预测的性能，特别是透过将时间序列分割为固定大小的块来帮助trasnformer学习时间序列中的复杂模式。</li>
<li>methods: 这个研究使用了一种名为Multi-resolution Time-Series Transformer（MTST）的新框架，这个框架包含多条分支架构，用于同时模型不同的时间模式。它还使用了相对位置编码，这样可以更好地提取不同 scales的周期性。</li>
<li>results: 实验结果显示，MTST在实际世界的数据集上比顶对方法更有效，尤其是在预测长期季节性和趋势方面。<details>
<summary>Abstract</summary>
The performance of transformers for time-series forecasting has improved significantly. Recent architectures learn complex temporal patterns by segmenting a time-series into patches and using the patches as tokens. The patch size controls the ability of transformers to learn the temporal patterns at different frequencies: shorter patches are effective for learning localized, high-frequency patterns, whereas mining long-term seasonalities and trends requires longer patches. Inspired by this observation, we propose a novel framework, Multi-resolution Time-Series Transformer (MTST), which consists of a multi-branch architecture for simultaneous modeling of diverse temporal patterns at different resolutions. In contrast to many existing time-series transformers, we employ relative positional encoding, which is better suited for extracting periodic components at different scales. Extensive experiments on several real-world datasets demonstrate the effectiveness of MTST in comparison to state-of-the-art forecasting techniques.
</details>
<details>
<summary>摘要</summary>
“transformer的时间序列预测性能有所改善，现代架构通过将时间序列分成块和使用这些块作为token来学习复杂的时间模式。块的大小控制了transformer的能力学习不同频率的时间模式：较短的块更有效地学习本地化高频率的模式，而长的块则更适合挖掘长期季节性和趋势。受到这个观察的启发，我们提出了一个新的框架──多resolution时间序列transformer（MTST），这个架构包括多条分支架构，用于同时模型不同分辨率的时间模式。不同于许多现有的时间序列transformer，我们使用相对位置编码，这种编码更适合提取不同比例的周期性。实际实验发现，MTST在多个真实世界数据集上与当前的预测技术相比，表现更加出色。”
</details></li>
</ul>
<hr>
<h2 id="Generative-learning-for-nonlinear-dynamics"><a href="#Generative-learning-for-nonlinear-dynamics" class="headerlink" title="Generative learning for nonlinear dynamics"></a>Generative learning for nonlinear dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04128">http://arxiv.org/abs/2311.04128</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gilpin</li>
<li>for: 该论文探讨了如何使用非线性动力学理论来解释大规模生成统计学学习模型的能力创造真实的输出。</li>
<li>methods: 该论文使用了信息理论工具来推断 chaotic attractor 的性质，并提出了一些算法来 parametrize chaos in real datasets。</li>
<li>results: 该论文提出了一些新的思路，如使用 classical attractor reconstruction 和 symbolic approximations 来理解大规模生成统计学学习模型的行为，以及将 nonlinear dynamics 和学习理论相互关联。<details>
<summary>Abstract</summary>
Modern generative machine learning models demonstrate surprising ability to create realistic outputs far beyond their training data, such as photorealistic artwork, accurate protein structures, or conversational text. These successes suggest that generative models learn to effectively parametrize and sample arbitrarily complex distributions. Beginning half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, motivating the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning. We first consider classical attractor reconstruction, which mirrors constraints on latent representations learned by state space models of time series. We next revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models. Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs.
</details>
<details>
<summary>摘要</summary>
Half a century ago, foundational works in nonlinear dynamics used tools from information theory to infer properties of chaotic attractors from time series, which motivated the development of algorithms for parametrizing chaos in real datasets. In this perspective, we aim to connect these classical works to emerging themes in large-scale generative statistical learning.We first consider classical attractor reconstruction, which reflects the constraints on latent representations learned by state space models of time series. We then revisit early efforts to use symbolic approximations to compare minimal discrete generators underlying complex processes, a problem relevant to modern efforts to distill and interpret black-box statistical models.Emerging interdisciplinary works bridge nonlinear dynamics and learning theory, such as operator-theoretic methods for complex fluid flows, or detection of broken detailed balance in biological datasets. We anticipate that future machine learning techniques may revisit other classical concepts from nonlinear dynamics, such as transinformation decay and complexity-entropy tradeoffs.
</details></li>
</ul>
<hr>
<h2 id="Do-Language-Models-Learn-Semantics-of-Code-A-Case-Study-in-Vulnerability-Detection"><a href="#Do-Language-Models-Learn-Semantics-of-Code-A-Case-Study-in-Vulnerability-Detection" class="headerlink" title="Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection"></a>Do Language Models Learn Semantics of Code? A Case Study in Vulnerability Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04109">http://arxiv.org/abs/2311.04109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Steenhoek, Md Mahbubur Rahman, Shaila Sharmin, Wei Le<br>for: This paper aims to analyze the alignment of pre-trained language models with bug semantics in vulnerability detection, and to develop annotation methods to improve the models’ performance.methods: The paper uses three distinct methods for analysis: interpretability tools, attention analysis, and interaction matrix analysis.results: The paper finds that better-performing models align better with potentially vulnerable statements (PVS), but the models fail to align strongly to buggy paths. The paper also develops two annotation methods that improve the models’ performance in the majority of settings, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. Additionally, the models aligned up to 232% better to PVS with the annotations.<details>
<summary>Abstract</summary>
Recently, pretrained language models have shown state-of-the-art performance on the vulnerability detection task. These models are pretrained on a large corpus of source code, then fine-tuned on a smaller supervised vulnerability dataset. Due to the different training objectives and the performance of the models, it is interesting to consider whether the models have learned the semantics of code relevant to vulnerability detection, namely bug semantics, and if so, how the alignment to bug semantics relates to model performance. In this paper, we analyze the models using three distinct methods: interpretability tools, attention analysis, and interaction matrix analysis. We compare the models' influential feature sets with the bug semantic features which define the causes of bugs, including buggy paths and Potentially Vulnerable Statements (PVS). We find that (1) better-performing models also aligned better with PVS, (2) the models failed to align strongly to PVS, and (3) the models failed to align at all to buggy paths. Based on our analysis, we developed two annotation methods which highlight the bug semantics inside the model's inputs. We evaluated our approach on four distinct transformer models and four vulnerability datasets and found that our annotations improved the models' performance in the majority of settings - 11 out of 16, with up to 9.57 points improvement in F1 score compared to conventional fine-tuning. We further found that with our annotations, the models aligned up to 232% better to potentially vulnerable statements. Our findings indicate that it is helpful to provide the model with information of the bug semantics, that the model can attend to it, and motivate future work in learning more complex path-based bug semantics. Our code and data are available at https://figshare.com/s/4a16a528d6874aad51a0.
</details>
<details>
<summary>摘要</summary>
近些时间，预训练语言模型在漏洞检测任务上表现出状元水平。这些模型首先在大量源代码上预训练，然后在更小的指导漏洞数据集上细化调教。由于模型的不同训练目标和性能，我们想要考虑这些模型是否学习了代码漏洞检测相关的 semantics，即漏洞 semantics，以及这种对应关系如何影响模型性能。在这篇论文中，我们使用三种不同的方法来分析模型：可读性工具、注意力分析和交互矩阵分析。我们比较模型的影响特征集与漏洞 semantics 定义的原因，包括漏洞路径和潜在漏洞语句（PVS）。我们发现：1. 性能更好的模型也更好地与 PVS 对齐。2. 模型无法强调 PVS 的对齐。3. 模型无法强调漏洞路径的对齐。根据我们的分析，我们开发了两种标注方法，用于在模型输入中标注内部的漏洞 semantics。我们在四种 transformer 模型和四个漏洞数据集上评估了我们的方法，发现我们的标注提高了模型在大多数情况下的性能，增加了11 out of 16 的情况中的 F1 分数，达到最高9.57个点的提升。我们还发现，通过我们的标注，模型可以更好地遵循潜在漏洞语句，最高达232%的提升。我们的发现表明，提供模型bug semantics信息是有帮助的，模型可以注意到它，并促进未来更多关于路径基本漏洞 semantics 的学习。我们的代码和数据可以在 figshare 上找到：https://figshare.com/s/4a16a528d6874aad51a0。
</details></li>
</ul>
<hr>
<h2 id="Time-Efficient-Reinforcement-Learning-with-Stochastic-Stateful-Policies"><a href="#Time-Efficient-Reinforcement-Learning-with-Stochastic-Stateful-Policies" class="headerlink" title="Time-Efficient Reinforcement Learning with Stochastic Stateful Policies"></a>Time-Efficient Reinforcement Learning with Stochastic Stateful Policies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04082">http://arxiv.org/abs/2311.04082</a></li>
<li>repo_url: None</li>
<li>paper_authors: Firas Al-Hafez, Guoping Zhao, Jan Peters, Davide Tateo</li>
<li>for: 本文旨在提出一种新的方法来训练状态决策函数，以解决传统的Backpropagation Through Time（BPTT）方法的缺点，如训练速度慢和梯度衰减或扩散。</li>
<li>methods: 本文提出了一种将状态决策函数分解成随机内部状态核心和无状态策略的方法，并将其共同优化，以实现状态策略梯度的优化。同时，本文还提出了不同版本的状态策略梯度定理，使得可以轻松实现状态策略的变种。</li>
<li>results: 本文通过在复杂的连续控制任务中进行测试，如人型行走，并证明了新的梯度估计器可以与BPTT相比，在任务复杂性增加时能够有效扩展，并且比BPTT更快和简单。<details>
<summary>Abstract</summary>
Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients. The gradient is often truncated to address these issues, resulting in a biased policy update. We present a novel approach for training stateful policies by decomposing the latter into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, enabling us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms. Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, e.g., humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.
</details>
<details>
<summary>摘要</summary>
We propose a novel approach for training stateful policies by decomposing the policy into a stochastic internal state kernel and a stateless policy, jointly optimized by following the stateful policy gradient. We introduce different versions of the stateful policy gradient theorem, allowing us to easily instantiate stateful variants of popular reinforcement learning and imitation learning algorithms.Furthermore, we provide a theoretical analysis of our new gradient estimator and compare it with BPTT. We evaluate our approach on complex continuous control tasks, such as humanoid locomotion, and demonstrate that our gradient estimator scales effectively with task complexity while offering a faster and simpler alternative to BPTT.Here is the Simplified Chinese translation of the text:州态策略在强化学习中扮演着重要的角色，例如处理偏见环境、增强稳定性或直接在策略结构中强制一致性。传统的强化学习训练方法是Backpropagation Through Time（BPTT），但它带来了许多问题，如时间顺序梯度传播导致训练慢，以及潜在的潜在或扩散梯度问题。为解决这些问题，我们常常将梯度截断，从而导致策略更新受到偏见。我们提出了一种新的方法，即将策略分解成随机内部状态核心和无状态策略，共同由状态策略梯度优化。我们还提出了不同的州态策略梯度定理版本，使得可以轻松实现州态变体的各种强化学习和仿真学习算法。此外，我们还提供了对我们新的梯度估计器的理论分析，并与BPTT进行比较。我们在复杂的连续控制任务上，如人型步态机器人行走，进行了评估，并证明了我们的梯度估计器与任务复杂性成正比，而且对BPTT更加快速和简单。
</details></li>
</ul>
<hr>
<h2 id="Estimator-Coupled-Reinforcement-Learning-for-Robust-Purely-Tactile-In-Hand-Manipulation"><a href="#Estimator-Coupled-Reinforcement-Learning-for-Robust-Purely-Tactile-In-Hand-Manipulation" class="headerlink" title="Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation"></a>Estimator-Coupled Reinforcement Learning for Robust Purely Tactile In-Hand Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04060">http://arxiv.org/abs/2311.04060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lennart Röstel, Johannes Pitz, Leon Sievers, Berthold Bäuml</li>
<li>for: 这篇论文 targets the problem of combining reinforcement learning-based controllers and state estimators for robotic in-hand manipulation, specifically focusing on the challenging task of purely tactile, goal-conditioned, dextrous in-hand reorientation with the hand pointing downwards.</li>
<li>methods: The authors solve the problem by coupling the control policy to the state estimator during training in simulation, leading to more robust state estimation and overall higher performance on the task while maintaining an interpretability advantage over end-to-end policy learning.</li>
<li>results: The authors demonstrate successful sim2real transfer and achieve high performance on the task of reorienting a cube to nine goals, which was beyond the reach of previous methods in this challenging setting. With their GPU-accelerated implementation, learning from scratch takes a median training time of only 6.5 hours on a single, low-cost GPU.<details>
<summary>Abstract</summary>
This paper identifies and addresses the problems with naively combining (reinforcement) learning-based controllers and state estimators for robotic in-hand manipulation. Specifically, we tackle the challenging task of purely tactile, goal-conditioned, dextrous in-hand reorientation with the hand pointing downwards. Due to the limited sensing available, many control strategies that are feasible in simulation when having full knowledge of the object's state do not allow for accurate state estimation. Hence, separately training the controller and the estimator and combining the two at test time leads to poor performance. We solve this problem by coupling the control policy to the state estimator already during training in simulation. This approach leads to more robust state estimation and overall higher performance on the task while maintaining an interpretability advantage over end-to-end policy learning. With our GPU-accelerated implementation, learning from scratch takes a median training time of only 6.5 hours on a single, low-cost GPU. In simulation experiments with the DLR-Hand II and for four significantly different object shapes, we provide an in-depth analysis of the performance of our approach. We demonstrate the successful sim2real transfer by rotating the four objects to all 24 orientations in the $\pi/2$ discretization of SO(3), which has never been achieved for such a diverse set of shapes. Finally, our method can reorient a cube consecutively to nine goals (median), which was beyond the reach of previous methods in this challenging setting.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Feature-Space-Renormalization-for-Semi-supervised-Learning"><a href="#Feature-Space-Renormalization-for-Semi-supervised-Learning" class="headerlink" title="Feature Space Renormalization for Semi-supervised Learning"></a>Feature Space Renormalization for Semi-supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04055">http://arxiv.org/abs/2311.04055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Sun, Zhongjie Mao, Chao Li, Chao Zhou, Xiao-Jun Wu</li>
<li>for: 这篇论文的目的是提出一种新的半监督学习（SSL）方法，以便利用无标的数据来增强模型对于大量标签数据的依赖。</li>
<li>methods: 这篇论文使用了一种新的特征空间重整化（FSR）机制，将替代常用的一致调整机制，以提高模型对特征的学习。</li>
<li>results: 根据实验结果，这篇论文的方法可以在多个标准的SSL测试数据集上取得更好的性能，并且可以增强其他SSL方法的表现。<details>
<summary>Abstract</summary>
Semi-supervised learning (SSL) has been proven to be a powerful method for leveraging unlabelled data to alleviate models' dependence on large labelled datasets. The common framework among recent approaches is to train the model on a large amount of unlabelled data with consistency regularization to constrain the model predictions to be invariant to input perturbation. However, the existing SSL frameworks still have room for improvement in the consistency regularization method. Instead of regularizing category predictions in the label space as in existing frameworks, this paper proposes a feature space renormalization (FSR) mechanism for SSL. First, we propose a feature space renormalization mechanism to substitute for the commonly used consistency regularization mechanism to learn better discriminative features. To apply this mechanism, we start by building a basic model and an empirical model and then introduce our mechanism to renormalize the feature learning of the basic model with the guidance of the empirical model. Second, we combine the proposed mechanism with pseudo-labelling to obtain a novel effective SSL model named FreMatch. The experimental results show that our method can achieve better performance on a variety of standard SSL benchmark datasets, and the proposed feature space renormalization mechanism can also enhance the performance of other SSL approaches.
</details>
<details>
<summary>摘要</summary>
我们的方法包括两个步骤：1. 我们提出了一种特征空间重normalization机制，用于取代常用的一致性正则化机制，以学习更好的抽象特征。我们开始于建立基本模型和empirical模型，然后引入我们的机制，使基本模型在empirical模型的指导下进行特征学习重normalization。2. 我们将我们的机制与pseudo-labeling相结合，以获得一种新的有效SSL模型，称之为FreMatch。实验结果显示，我们的方法可以在多种标准SSL benchmark数据集上达到更好的性能，并且我们提出的特征空间重normalization机制也可以增强其他SSL方法的性能。
</details></li>
</ul>
<hr>
<h2 id="Extracting-human-interpretable-structure-property-relationships-in-chemistry-using-XAI-and-large-language-models"><a href="#Extracting-human-interpretable-structure-property-relationships-in-chemistry-using-XAI-and-large-language-models" class="headerlink" title="Extracting human interpretable structure-property relationships in chemistry using XAI and large language models"></a>Extracting human interpretable structure-property relationships in chemistry using XAI and large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04047">http://arxiv.org/abs/2311.04047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geemi725/xpertai">https://github.com/geemi725/xpertai</a></li>
<li>paper_authors: Geemi P. Wellawatte, Philippe Schwaller</li>
<li>For: The paper aims to address the opaque nature of machine learning models in the field of Explainable Artificial Intelligence (XAI) and their applications in chemistry to understand structure-property relationships.* Methods: The paper proposes the XpertAI framework, which integrates XAI methods with large language models (LLMs) to generate accessible natural language explanations of raw chemical data automatically.* Results: The paper evaluates the performance of XpertAI through 5 case studies and shows that it combines the strengths of LLMs and XAI tools in generating specific, scientific, and interpretable explanations.Here’s the simplified Chinese version of the three key points:* For: 该论文旨在解决人工智能模型的各种不透明性问题，并在化学领域应用其结构性关系理解。* Methods: 论文提出了XpertAI框架，该框架将XAI方法与大型自然语言模型（LLMs）结合起来，自动生成化学数据的可读性语言解释。* Results: 论文通过5个案例研究证明，XpertAI结合了LLMs和XAI工具的优势，能够生成特定、科学、可解释的语言解释。<details>
<summary>Abstract</summary>
Explainable Artificial Intelligence (XAI) is an emerging field in AI that aims to address the opaque nature of machine learning models. Furthermore, it has been shown that XAI can be used to extract input-output relationships, making them a useful tool in chemistry to understand structure-property relationships. However, one of the main limitations of XAI methods is that they are developed for technically oriented users. We propose the XpertAI framework that integrates XAI methods with large language models (LLMs) accessing scientific literature to generate accessible natural language explanations of raw chemical data automatically. We conducted 5 case studies to evaluate the performance of XpertAI. Our results show that XpertAI combines the strengths of LLMs and XAI tools in generating specific, scientific, and interpretable explanations.
</details>
<details>
<summary>摘要</summary>
互助式人工智能（XAI）是一个emerging field在人工智能领域，旨在解决机器学习模型的透明性问题。此外，研究表明，XAI可以用来抽取输入-输出关系，使其成为化学领域理解结构-性质关系的有用工具。然而，XAI方法的一个主要局限性是它们是为技术 oriented 用户开发的。我们提出了 XpertAI 框架，该框架将 XAI 方法与大型自然语言模型（LLMs）相结合，自动生成可读的化学数据自然语言解释。我们进行了5个案例研究，以评估 XpertAI 的表现。我们的结果显示，XpertAI 组合了 LLMs 和 XAI 工具的优点，能够生成具体、科学和可解释的解释。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Discordance-Minimization-based-Imputation-Algorithms-for-Missing-Values-in-Rating-Data"><a href="#Discordance-Minimization-based-Imputation-Algorithms-for-Missing-Values-in-Rating-Data" class="headerlink" title="Discordance Minimization-based Imputation Algorithms for Missing Values in Rating Data"></a>Discordance Minimization-based Imputation Algorithms for Missing Values in Rating Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04035">http://arxiv.org/abs/2311.04035</a></li>
<li>repo_url: None</li>
<li>paper_authors: Young Woong Park, Jinhak Kim, Dan Zhu</li>
<li>for: 这个研究是为了解决多个评分列表合并后存在缺失评分的问题。</li>
<li>methods: 这个研究使用了六个真实世界的数据集，通过分析缺失值的特征和特性，提出了优化模型和算法，用于使用known评分信息来填充缺失评分。</li>
<li>results: 计算实验表明，提出的方法在实际和 sintetic 评分数据集上比现有的总体替换方法更高的填充精度。<details>
<summary>Abstract</summary>
Ratings are frequently used to evaluate and compare subjects in various applications, from education to healthcare, because ratings provide succinct yet credible measures for comparing subjects. However, when multiple rating lists are combined or considered together, subjects often have missing ratings, because most rating lists do not rate every subject in the combined list. In this study, we propose analyses on missing value patterns using six real-world data sets in various applications, as well as the conditions for applicability of imputation algorithms. Based on the special structures and properties derived from the analyses, we propose optimization models and algorithms that minimize the total rating discordance across rating providers to impute missing ratings in the combined rating lists, using only the known rating information. The total rating discordance is defined as the sum of the pairwise discordance metric, which can be written as a quadratic function. Computational experiments based on real-world and synthetic rating data sets show that the proposed methods outperform the state-of-the-art general imputation methods in the literature in terms of imputation accuracy.
</details>
<details>
<summary>摘要</summary>
评分 frequently 用于在不同应用中评估和比较对象，从教育到医疗，因为评分提供简洁 yet credible 的比较度量。然而，当多个评分列表合并或考虑在一起时，对象经常有缺失评分，因为大多数评分列表不会对所有对象进行评分。在这种研究中，我们对缺失值模式进行分析，使用 six 个真实世界数据集，并对适用于替换算法的条件进行分析。基于分析结果，我们提出优化模型和算法，以最小化总评分差异 across 评分提供者，使用只知道的评分信息进行替换缺失评分。总评分差异可以写作quadratic function。通过实验表明，提出的方法在实际世界和 sintetic 评分数据集上比 estado-of-the-art 通用替换方法在替换精度方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Joint-model-for-longitudinal-and-spatio-temporal-survival-data"><a href="#Joint-model-for-longitudinal-and-spatio-temporal-survival-data" class="headerlink" title="Joint model for longitudinal and spatio-temporal survival data"></a>Joint model for longitudinal and spatio-temporal survival data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04008">http://arxiv.org/abs/2311.04008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Victor Medina-Olivares, Finn Lindgren, Raffaella Calabrese, Jonathan Crook</li>
<li>for: 预测借款人的时间到事件（如偿还），使用存生模型，特别是当时间变量covariates是内生的时候。</li>
<li>methods: 我们提出了空间-时间联合模型（STJM），该模型能够捕捉空间和时间效应以及其交互作用，并使用bayesian hierarchical模型来reckon借款人之间的不见天性差异。</li>
<li>results: 我们对大规模数据集（57,258名美国房贷借款人，超过250万个观察）应用STJM模型，结果表明包含空间效应可以顺利提高存生模型的性能，但是当再次包含空间-时间交互效应时，效果较弱。<details>
<summary>Abstract</summary>
In credit risk analysis, survival models with fixed and time-varying covariates are widely used to predict a borrower's time-to-event. When the time-varying drivers are endogenous, modelling jointly the evolution of the survival time and the endogenous covariates is the most appropriate approach, also known as the joint model for longitudinal and survival data. In addition to the temporal component, credit risk models can be enhanced when including borrowers' geographical information by considering spatial clustering and its variation over time. We propose the Spatio-Temporal Joint Model (STJM) to capture spatial and temporal effects and their interaction. This Bayesian hierarchical joint model reckons the survival effect of unobserved heterogeneity among borrowers located in the same region at a particular time. To estimate the STJM model for large datasets, we consider the Integrated Nested Laplace Approximation (INLA) methodology. We apply the STJM to predict the time to full prepayment on a large dataset of 57,258 US mortgage borrowers with more than 2.5 million observations. Empirical results indicate that including spatial effects consistently improves the performance of the joint model. However, the gains are less definitive when we additionally include spatio-temporal interactions.
</details>
<details>
<summary>摘要</summary>
在信用风险分析中，固定和时间变化的 covariates 广泛使用来预测借款人的时间事件。当时间变化驱动器是内生的时，模型同时考虑 survival 时间和内生 covariates 的演化是最合适的方法，也称为长itudinal 和存活数据共同模型。此外，信用风险模型可以通过考虑借款人的地理信息来增强，包括空间归一化和时间变化的相互作用。我们提议使用 Spatio-Temporal Joint Model (STJM) 来捕捉空间和时间效应以及其相互作用。这是一种 Bayesian 层次模型，reckons 借款人在同一地区的特定时间点上存在不见的潜在差异的存活效应。为处理大规模数据，我们考虑使用 Integrated Nested Laplace Approximation (INLA) 方法。我们应用 STJM 模型来预测57,258名美国mortgage 借款人的250万次观察数据中的时间到完全偿还。实际结果表明，包括空间效应可以持续改善共同模型的性能，但是添加空间-时间交互的效果较为不确定。
</details></li>
</ul>
<hr>
<h2 id="An-Initialization-Schema-for-Neuronal-Networks-on-Tabular-Data"><a href="#An-Initialization-Schema-for-Neuronal-Networks-on-Tabular-Data" class="headerlink" title="An Initialization Schema for Neuronal Networks on Tabular Data"></a>An Initialization Schema for Neuronal Networks on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03996">http://arxiv.org/abs/2311.03996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wolfgang Fuhl</li>
<li>for: 本研究旨在使用神经网络进行表格数据预测，并提出了一种简单 yet effective的初始化方法。</li>
<li>methods: 本研究使用了一种binomial initialized neural network，并将Gradient Masking添加到批处理中。</li>
<li>results: 对多个公共数据集进行评估，显示了与其他神经网络方法相比的改进表现。In English:</li>
<li>for: The purpose of this study is to use neural networks for tabular data prediction, and propose a simple yet effective initialization method.</li>
<li>methods: The study uses a binomial initialized neural network and adds Gradient Masking to the batch processing.</li>
<li>results: The approach is evaluated on multiple public datasets and shows improved performance compared to other neural network-based methods.<details>
<summary>Abstract</summary>
Nowadays, many modern applications require heterogeneous tabular data, which is still a challenging task in terms of regression and classification. Many approaches have been proposed to adapt neural networks for this task, but still, boosting and bagging of decision trees are the best-performing methods for this task. In this paper, we show that a binomial initialized neural network can be used effectively on tabular data. The proposed approach shows a simple but effective approach for initializing the first hidden layer in neural networks. We also show that this initializing schema can be used to jointly train ensembles by adding gradient masking to batch entries and using the binomial initialization for the last layer in a neural network. For this purpose, we modified the hinge binary loss and the soft max loss to make them applicable for joint ensemble training. We evaluate our approach on multiple public datasets and showcase the improved performance compared to other neural network-based approaches. In addition, we discuss the limitations and possible further research of our approach for improving the applicability of neural networks to tabular data.   Link: https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&mode=list
</details>
<details>
<summary>摘要</summary>
现在，许多现代应用需要多元表格数据，这还是一个困难的任务，尤其是在回归和分类方面。许多方法已经被提出来适应神经网络，但是弹性和袋式的决策树仍然是最佳性能的方法。在这篇论文中，我们表明了一种使用二进制初始化神经网络的方法，可以有效地应用于表格数据。我们还提出了一种将批处理中的梯度掩码以及神经网络的最后一层的二进制初始化结合使用，以实现联合训练。为此，我们修改了梯度函数和软max损失函数，使其适用于联合集成训练。我们在多个公共数据集上评估了我们的方法，并证明了与其他神经网络基于方法相比，我们的方法具有更高的性能。此外，我们还讨论了我们的方法的局限性和可能的进一步研究，以提高神经网络对表格数据的应用。Link: <https://es-cloud.cs.uni-tuebingen.de/d/8e2ab8c3fdd444e1a135/?p=%2FInitializationNeuronalNetworksTabularData&mode=list>
</details></li>
</ul>
<hr>
<h2 id="Bandit-Pareto-Set-Identification-the-Fixed-Budget-Setting"><a href="#Bandit-Pareto-Set-Identification-the-Fixed-Budget-Setting" class="headerlink" title="Bandit Pareto Set Identification: the Fixed Budget Setting"></a>Bandit Pareto Set Identification: the Fixed Budget Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03992">http://arxiv.org/abs/2311.03992</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cyrille Kone, Emilie Kaufmann, Laura Richert</li>
<li>for: 本研究旨在解决一个多目标纯exploration问题，该问题在多臂抽象模型中出现。每抽象臂都关联着一个未知多变量分布，目标是确定这些分布的mean不 worse than另一个分布：Pareto优化集。</li>
<li>methods: 我们提出并分析了首个针对fixed budget Pareto Set Identification任务的算法。我们提出了Empirical Gap Elimination家族算法，这家族算法结合了精心估计每抽象臂是否处于Pareto集的“难度分类”，以及一种通用的排除方案。我们证明了两个特定实现，EGE-SR和EGE-SH，其错误概率与预算相关，具有 exponential decay 的速率，其下界支持信息理论下界。</li>
<li>results: 我们通过实验研究使用实际世界和 sintetic 数据集，发现我们的算法具有良好的性能。<details>
<summary>Abstract</summary>
We study a multi-objective pure exploration problem in a multi-armed bandit model. Each arm is associated to an unknown multi-variate distribution and the goal is to identify the distributions whose mean is not uniformly worse than that of another distribution: the Pareto optimal set. We propose and analyze the first algorithms for the \emph{fixed budget} Pareto Set Identification task. We propose Empirical Gap Elimination, a family of algorithms combining a careful estimation of the ``hardness to classify'' each arm in or out of the Pareto set with a generic elimination scheme. We prove that two particular instances, EGE-SR and EGE-SH, have a probability of error that decays exponentially fast with the budget, with an exponent supported by an information theoretic lower-bound. We complement these findings with an empirical study using real-world and synthetic datasets, which showcase the good performance of our algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个多目标纯探索问题在多重投机模型中。每个武器都相关于一个未知多变量分布，目标是确定这些分布的含义，其中的平均值不比另一个分布更差。我们提出并分析了首个预算纯度集合识别任务的算法。我们提出了实验减少差异（EGE）家族算法，这些算法结合精心估计每个武器是否在纯度集合中的困难程度，以及一种通用的减少方案。我们证明了两个特定实例（EGE-SR和EGE-SH）的抽象错误概率在预算下随时间减少 exponentially，并且支持信息理论下界。我们通过实验使用真实世界和 sintetic 数据集来证明我们的算法的好效果。
</details></li>
</ul>
<hr>
<h2 id="Cup-Curriculum-Curriculum-Learning-on-Model-Capacity"><a href="#Cup-Curriculum-Curriculum-Learning-on-Model-Capacity" class="headerlink" title="Cup Curriculum: Curriculum Learning on Model Capacity"></a>Cup Curriculum: Curriculum Learning on Model Capacity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03956">http://arxiv.org/abs/2311.03956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luca-scharr/cupcurriculum">https://github.com/luca-scharr/cupcurriculum</a></li>
<li>paper_authors: Luca Scharr, Vanessa Toborek</li>
<li>for: 提高自然语言处理任务的学习效果</li>
<li>methods: 应用特殊学习策略和迭代减少模型容量</li>
<li>results: 可靠地超越早期停止，高度抗沉淀<details>
<summary>Abstract</summary>
Curriculum learning (CL) aims to increase the performance of a learner on a given task by applying a specialized learning strategy. This strategy focuses on either the dataset, the task, or the model. There is little to no work analysing the possibilities to apply CL on the model capacity in natural language processing. To close this gap, we propose the cup curriculum. In a first phase of training we use a variation of iterative magnitude pruning to reduce model capacity. These weights are reintroduced in a second phase, resulting in the model capacity to show a cup-shaped curve over the training iterations. We empirically evaluate different strategies of the cup curriculum and show that it outperforms early stopping reliably while exhibiting a high resilience to overfitting.
</details>
<details>
<summary>摘要</summary>
学习资源（CL）目的是提高学习者对特定任务的性能，通过特殊的学习策略。这种策略可以对 dataset、任务或模型进行特化。在自然语言处理领域，有很少关于模型容量的可能性分析。为了填补这个差距，我们提出了杯资源。在第一个训练阶段，我们使用迭代幅度剪枝来降低模型容量。这些权重在第二个训练阶段重新引入，导致模型容量展示一个杯形曲线。我们对不同的杯资源策略进行实验性评估，并证明它可靠地超越早停止，同时具有高鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Blind-Federated-Learning-via-Over-the-Air-q-QAM"><a href="#Blind-Federated-Learning-via-Over-the-Air-q-QAM" class="headerlink" title="Blind Federated Learning via Over-the-Air q-QAM"></a>Blind Federated Learning via Over-the-Air q-QAM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04253">http://arxiv.org/abs/2311.04253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Razavikia, José Mairton Barros Da Silva Júnior, Carlo Fischione</li>
<li>for: 这 paper 研究了 federated edge learning over a fading multiple access channel，以减少边缘设备与访问点之间的通信负担。</li>
<li>methods: 该 paper 引入了一种先进的数字上下线计算策略，使用 q-ary  quadrature amplitude modulation，实现了低延迟的通信方案。</li>
<li>results: 研究人员提出了一种新的 federated edge learning 框架，在其中边缘设备使用数字模ulation 进行对空传输，而不需要知道通道状态信息。此外，研究人员还在边缘服务器中加入多个天线，以解决无线通信中的抖音问题。研究人员分析了对抖音的影响下，需要多少天线来有效地减少影响。通过分析，研究人员得出了对不同抖音水平的非对数上限下界，并证明了在噪声和抖音条件下，该方法可以减少学习过程中的模型误差。<details>
<summary>Abstract</summary>
In this work, we investigate federated edge learning over a fading multiple access channel. To alleviate the communication burden between the edge devices and the access point, we introduce a pioneering digital over-the-air computation strategy employing q-ary quadrature amplitude modulation, culminating in a low latency communication scheme. Indeed, we propose a new federated edge learning framework in which edge devices use digital modulation for over-the-air uplink transmission to the edge server while they have no access to the channel state information. Furthermore, we incorporate multiple antennas at the edge server to overcome the fading inherent in wireless communication. We analyze the number of antennas required to mitigate the fading impact effectively. We prove a non-asymptotic upper bound for the mean squared error for the proposed federated learning with digital over-the-air uplink transmissions under both noisy and fading conditions. Leveraging the derived upper bound, we characterize the convergence rate of the learning process of a non-convex loss function in terms of the mean square error of gradients due to the fading channel. Furthermore, we substantiate the theoretical assurances through numerical experiments concerning mean square error and the convergence efficacy of the digital federated edge learning framework. Notably, the results demonstrate that augmenting the number of antennas at the edge server and adopting higher-order modulations improve the model accuracy up to 60\%.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了聚合边缘学习在淡含多access通道上。为了减轻边缘设备和访问点之间的通信负担，我们提出了一种创新的数字上下线计算策略，使用q-ary正交振荡频分 Modulation（QAM），从而实现了低延迟的通信方案。在这个框架中，边缘设备使用数字模ulation进行无线上传到边缘服务器，而无需了解通道状态信息。此外，我们在边缘服务器中添加多个天线，以抗衰减无线通信中的噪声。我们分析了需要消除噪声的天线数量，以实现有效的减轻影响。我们证明了在噪声和淡含条件下，对提议的联邦边缘学习方法的非 asymptotic 上限 bound，并且用这个上限 bound来描述联邦学习过程中梯度误差的减少率。此外，我们通过数学实验证明了 theoretical 保证的可行性和数学误差的减少率。结果表明，增加边缘服务器中天线数量和采用更高的模ulation编码可以提高模型精度达60%。
</details></li>
</ul>
<hr>
<h2 id="CNN-Based-Structural-Damage-Detection-using-Time-Series-Sensor-Data"><a href="#CNN-Based-Structural-Damage-Detection-using-Time-Series-Sensor-Data" class="headerlink" title="CNN-Based Structural Damage Detection using Time-Series Sensor Data"></a>CNN-Based Structural Damage Detection using Time-Series Sensor Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04252">http://arxiv.org/abs/2311.04252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishan Pathak, Ishan Jha, Aditya Sadana, Basuraj Bhowmik</li>
<li>for: 本研究旨在开发一种新的损害检测方法，以帮助评估结构的状况，检测结构损害。</li>
<li>methods: 该方法使用一种新的卷积神经网络算法，通过检测时间序列数据中的长期连接，提取深度空间特征。</li>
<li>results: 实验结果表明，新的卷积神经网络算法具有高度的损害检测精度，可以准确地检测结构损害。<details>
<summary>Abstract</summary>
Structural Health Monitoring (SHM) is vital for evaluating structural condition, aiming to detect damage through sensor data analysis. It aligns with predictive maintenance in modern industry, minimizing downtime and costs by addressing potential structural issues. Various machine learning techniques have been used to extract valuable information from vibration data, often relying on prior structural knowledge. This research introduces an innovative approach to structural damage detection, utilizing a new Convolutional Neural Network (CNN) algorithm. In order to extract deep spatial features from time series data, CNNs are taught to recognize long-term temporal connections. This methodology combines spatial and temporal features, enhancing discrimination capabilities when compared to methods solely reliant on deep spatial features. Time series data are divided into two categories using the proposed neural network: undamaged and damaged. To validate its efficacy, the method's accuracy was tested using a benchmark dataset derived from a three-floor structure at Los Alamos National Laboratory (LANL). The outcomes show that the new CNN algorithm is very accurate in spotting structural degradation in the examined structure.
</details>
<details>
<summary>摘要</summary>
structural health monitoring (SHM) 是现代结构监测的关键技术，通过感知器数据分析，检测结构的损害。它与predictive maintenance相结合，可以避免机器设备的停机时间和成本，早发现结构问题。多种机器学习技术已经被应用于振荡数据分析中，常常借鉴结构知识。这项研究推出了一种新的卷积神经网络算法，用于检测结构损害。这种方法将时间序列数据分解成两类：未损害和损害。为验证其准确性，这种新算法的准确率被测试使用了一个来自洛斯阿拉莫斯国家实验室（LANL）的 referential dataset。结果表明，新的卷积神经网络算法具有高度准确地检测检测结构衰老的能力。
</details></li>
</ul>
<hr>
<h2 id="Structure-of-universal-formulas"><a href="#Structure-of-universal-formulas" class="headerlink" title="Structure of universal formulas"></a>Structure of universal formulas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03910">http://arxiv.org/abs/2311.03910</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smith86n/wiki-is-mostly-fake-radom-words-word-genrationr-">https://github.com/smith86n/wiki-is-mostly-fake-radom-words-word-genrationr-</a></li>
<li>paper_authors: Dmitry Yarotsky</li>
<li>for: 这篇论文主要研究的是 Universelle Formeln（通用方程），它们可以用来拟合任意连续函数在固定区间内的表达。</li>
<li>methods: 论文使用了一系列的分析方法，包括对高度表达力模型的分析，以及对各种函数家族的分类结果。</li>
<li>results: 论文的主要结果是提出了一个层次结构的表达力分类，其中包括从全体函数到具有无限VC次元的函数家族的分类结果。此外，论文还给出了一些实际中使用的函数家族，如多层感知神经网络，并证明了它们在某些情况下可以拟合函数，但在整个定义域内无法拟合函数。<details>
<summary>Abstract</summary>
By universal formulas we understand parameterized analytic expressions that have a fixed complexity, but nevertheless can approximate any continuous function on a compact set. There exist various examples of such formulas, including some in the form of neural networks. In this paper we analyze the essential structural elements of these highly expressive models. We introduce a hierarchy of expressiveness classes connecting the global approximability property to the weaker property of infinite VC dimension, and prove a series of classification results for several increasingly complex functional families. In particular, we introduce a general family of polynomially-exponentially-algebraic functions that, as we prove, is subject to polynomial constraints. As a consequence, we show that fixed-size neural networks with not more than one layer of neurons having transcendental activations (e.g., sine or standard sigmoid) cannot in general approximate functions on arbitrary finite sets. On the other hand, we give examples of functional families, including two-hidden-layer neural networks, that approximate functions on arbitrary finite sets, but fail to do that on the whole domain of definition.
</details>
<details>
<summary>摘要</summary>
通过通用公式我们理解参数化分析表达式，它们具有固定复杂性，但可以近似任何连续函数在封闭集上。存在各种这种公式的例子，包括一些形式为神经网络。在这篇论文中，我们分析这些高表达能力模型的基本结构元素。我们建立一个表达能力层次结构，连接全球近似性质与更弱的无穷VC维度质量，并证明了一系列分类结果，其中包括几种逐渐复杂的函数家族。特别是，我们引入一个总是 полиномиаль地几何的函数家族，并证明这些函数是受限的。这表明，具有不超过一层神经元的神经网络（例如，使用极性函数如正弦或标准sigmoid）无法在一般finite集上近似函数。然而，我们给出了一些函数家族的示例，包括两层神经网络，它们可以在一般finite集上近似函数，但在整个定义域上不能够。
</details></li>
</ul>
<hr>
<h2 id="Learning-Based-Latency-Constrained-Fronthaul-Compression-Optimization-in-C-RAN"><a href="#Learning-Based-Latency-Constrained-Fronthaul-Compression-Optimization-in-C-RAN" class="headerlink" title="Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN"></a>Learning-Based Latency-Constrained Fronthaul Compression Optimization in C-RAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03899">http://arxiv.org/abs/2311.03899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Axel Grönland, Bleron Klaiqi, Xavier Gelabert</li>
<li>for: 这个研究旨在提出一个基于深度学习的无模型测试架构 (DRL-FC)，用于动态控制无线前端压缩 (FH) 的调整，以提高FH资料传输效率和无线通信性能。</li>
<li>methods: 本研究使用了模型自由的深度学习架构 (DRL)，通过调整不同的配置参数（如模ulation频率、预设精确度和预设量化），以影响FH负载和无线通信性能。</li>
<li>results: 实验结果显示，DRL-FC 可以在不同的FH负载水平下实现高度的FH资料传输效率和无线通信性能，并且可以遵循预先定义的FH延迟限制（在我们的情况下为260 $\mu$s）。<details>
<summary>Abstract</summary>
The evolution of wireless mobile networks towards cloudification, where Radio Access Network (RAN) functions can be hosted at either a central or distributed locations, offers many benefits like low cost deployment, higher capacity, and improved hardware utilization. Nevertheless, the flexibility in the functional deployment comes at the cost of stringent fronthaul (FH) capacity and latency requirements. One possible approach to deal with these rigorous constraints is to use FH compression techniques. To ensure that FH capacity and latency requirements are met, more FH compression is applied during high load, while less compression is applied during medium and low load to improve FH utilization and air interface performance. In this paper, a model-free deep reinforcement learning (DRL) based FH compression (DRL-FC) framework is proposed that dynamically controls FH compression through various configuration parameters such as modulation order, precoder granularity, and precoder weight quantization that affect both FH load and air interface performance. Simulation results show that DRL-FC exhibits significantly higher FH utilization (68.7% on average) and air interface throughput than a reference scheme (i.e. with no applied compression) across different FH load levels. At the same time, the proposed DRL-FC framework is able to meet the predefined FH latency constraints (in our case set to 260 $\mu$s) under various FH loads.
</details>
<details>
<summary>摘要</summary>
wireless mobile networks 向云化发展，Radio Access Network（RAN）功能可以在中央或分布式位置上hosts，这带来了许多优点，如低成本投入、更高的容量和硬件利用率。然而，功能部署的灵活性带来了前方接入（FH）容量和延迟要求的严格限制。一种可能的方法是使用FH压缩技术。要确保FH容量和延迟要求得到满足，在高负荷时应用更多的FH压缩，而在中等和低负荷时应用 menos的FH压缩，以提高FH使用率和空中接口性能。本文提出了一个基于深度学习 reinforcement learning（DRL）的FH压缩（DRL-FC）框架，通过不同的配置参数，如模调顺序、预编器粒度和预编器量化，控制FH压缩。实验结果表明，DRL-FC可以在不同的FH负荷水平上实现显著更高的FH使用率（68.7%的平均值）和空中接口吞吐量，同时能够遵循预定的FH延迟限制（我们的情况下设置为260微秒）。
</details></li>
</ul>
<hr>
<h2 id="An-Explainable-Framework-for-Machine-learning-Based-Reactive-Power-Optimization-of-Distribution-Network"><a href="#An-Explainable-Framework-for-Machine-learning-Based-Reactive-Power-Optimization-of-Distribution-Network" class="headerlink" title="An Explainable Framework for Machine learning-Based Reactive Power Optimization of Distribution Network"></a>An Explainable Framework for Machine learning-Based Reactive Power Optimization of Distribution Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03863">http://arxiv.org/abs/2311.03863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenlong Liao, Benjamin Schäfer, Dalin Qin, Gonghao Zhang, Zhixian Wang, Zhe Yang</li>
<li>for: 提高分布网络响应电压优化的计算效率，使用机器学习模型。</li>
<li>methods: 使用可解释机器学习框架，包括Shapley添加itive解释框架和模型独立估计Shapley值。</li>
<li>results: 通过可视分析，从全局和实例角度准确解释机器学习模型基于分布网络响应电压优化的解决方案。<details>
<summary>Abstract</summary>
To reduce the heavy computational burden of reactive power optimization of distribution networks, machine learning models are receiving increasing attention. However, most machine learning models (e.g., neural networks) are usually considered as black boxes, making it challenging for power system operators to identify and comprehend potential biases or errors in the decision-making process of machine learning models. To address this issue, an explainable machine-learning framework is proposed to optimize the reactive power in distribution networks. Firstly, a Shapley additive explanation framework is presented to measure the contribution of each input feature to the solution of reactive power optimizations generated from machine learning models. Secondly, a model-agnostic approximation method is developed to estimate Shapley values, so as to avoid the heavy computational burden associated with direct calculations of Shapley values. The simulation results show that the proposed explainable framework can accurately explain the solution of the machine learning model-based reactive power optimization by using visual analytics, from both global and instance perspectives. Moreover, the proposed explainable framework is model-agnostic, and thus applicable to various models (e.g., neural networks).
</details>
<details>
<summary>摘要</summary>
为了减轻分布网络的反应能源优化计算负担，机器学习模型在分布网络优化中收到了越来越多的关注。然而，大多数机器学习模型（例如神经网络）通常被视为黑盒模型，使得电力系统运维人员很难发现和理解机器学习模型的偏见或错误。为解决这个问题，一个可解释的机器学习框架是提议的，用于优化分布网络中的反应能源。首先，一种Shapley添加性解释框架是提出来度量每个输入特征对机器学习模型生成的反应能源优化解决方案中的贡献。其次，一种模型无关的估计方法是开发来估计Shapley值，以避免直接计算Shapley值所需的巨量计算负担。实验结果表明，提议的可解释框架可以准确地解释机器学习模型基于的反应能源优化解决方案，使用可视化分析，从全局和实例两个角度进行解释。此外，提议的可解释框架是模型无关的，因此适用于多种模型（例如神经网络）。
</details></li>
</ul>
<hr>
<h2 id="Improved-MDL-Estimators-Using-Fiber-Bundle-of-Local-Exponential-Families-for-Non-exponential-Families"><a href="#Improved-MDL-Estimators-Using-Fiber-Bundle-of-Local-Exponential-Families-for-Non-exponential-Families" class="headerlink" title="Improved MDL Estimators Using Fiber Bundle of Local Exponential Families for Non-exponential Families"></a>Improved MDL Estimators Using Fiber Bundle of Local Exponential Families for Non-exponential Families</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03852">http://arxiv.org/abs/2311.03852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Miyamoto, Andrew R. Barron, Jun’ichi Takeuchi</li>
<li>for: 本文研究了使用两部分编码的最小描述长度（MDL）估计器，并对普通参数家族进行了分析。</li>
<li>methods: 本文引入了一种两部分代码，其对于一般参数家族下的某些正则条件下，可以达到与最佳目标家族M的 regret 几乎相同的水平。</li>
<li>results: 本文提出了一种基于巴顿和科вер在1991年的理论，对MDL估计器的风险和损失进行了紧密的Upper bound的分析。此外，本文还应用了结果到杂合家族中，这是非对称家族的一个典型示例。<details>
<summary>Abstract</summary>
Minimum Description Length (MDL) estimators, using two-part codes for universal coding, are analyzed. For general parametric families under certain regularity conditions, we introduce a two-part code whose regret is close to the minimax regret, where regret of a code with respect to a target family M is the difference between the code length of the code and the ideal code length achieved by an element in M. This is a generalization of the result for exponential families by Gr\"unwald. Our code is constructed by using an augmented structure of M with a bundle of local exponential families for data description, which is not needed for exponential families. This result gives a tight upper bound on risk and loss of the MDL estimators based on the theory introduced by Barron and Cover in 1991. Further, we show that we can apply the result to mixture families, which are a typical example of non-exponential families.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用两部分代码的最小描述长度（MDL）估计器被分析。对于普遍的参数家族，在某些常见的准则下，我们引入一个两部分代码，其忽略之违的 regret几乎等于最优化 regret，这是由Gr\"unwald得到的结果的推广。我们的代码使用了扩展结构的M，其中包含一个本地快满家族来描述数据，这并不需要对快满家族。这个结果给出了基于Barron和Cover在1991年引入的理论的紧张Upper bound，并且我们证明可以应用到混合家族，这些家族是非快满家族的典型示例。Note: "普遍的参数家族" (pang-ku-shi-ju-ji-ming) refers to a family of probability distributions that is not necessarily exponential, and "扩展结构" (kuang-zheng-xing) refers to an augmented structure of the parameter family.
</details></li>
</ul>
<hr>
<h2 id="User-level-Differentially-Private-Stochastic-Convex-Optimization-Efficient-Algorithms-with-Optimal-Rates"><a href="#User-level-Differentially-Private-Stochastic-Convex-Optimization-Efficient-Algorithms-with-Optimal-Rates" class="headerlink" title="User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates"></a>User-level Differentially Private Stochastic Convex Optimization: Efficient Algorithms with Optimal Rates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03797">http://arxiv.org/abs/2311.03797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hilal Asi, Daogao Liu</li>
<li>for: 这个论文主要是关于如何实现用户级 differentially private stochastic convex optimization (DP-SCO)，以保护每个用户的数据隐私。</li>
<li>methods: 这个论文提出了一种新的算法，基于多 passes DP-SGD 和一种私有均值估计方法，可以在 polynomial time 内实现用户级 DP-SCO，并且只需要用户数量在维度上增长 Logarithmically。</li>
<li>results: 这个论文的算法可以在 polynomial time 内实现用户级 DP-SCO，并且可以处理 convex 和强 convex 函数，以及非稠密函数。这些结果比 existing work 更优，并且不需要额外的假设。<details>
<summary>Abstract</summary>
We study differentially private stochastic convex optimization (DP-SCO) under user-level privacy, where each user may hold multiple data items. Existing work for user-level DP-SCO either requires super-polynomial runtime [Ghazi et al. (2023)] or requires the number of users to grow polynomially with the dimensionality of the problem with additional strict assumptions [Bassily et al. (2023)]. We develop new algorithms for user-level DP-SCO that obtain optimal rates for both convex and strongly convex functions in polynomial time and require the number of users to grow only logarithmically in the dimension. Moreover, our algorithms are the first to obtain optimal rates for non-smooth functions in polynomial time. These algorithms are based on multiple-pass DP-SGD, combined with a novel private mean estimation procedure for concentrated data, which applies an outlier removal step before estimating the mean of the gradients.
</details>
<details>
<summary>摘要</summary>
我们研究具有用户级隐私的可变性梯度优化（DP-SCO），每个用户可能拥有多个数据项。现有的用户级DP-SCO研究 either需要超 polynomial 时间 [Ghazi et al. (2023)] 或者需要用户数量在维度问题上增长 polynomial 方式，并且具有其他严格的假设 [Bassily et al. (2023)].我们开发了新的用户级DP-SCO算法，可以在 polynomial 时间内获得对凸和强凸函数的优化率，并且用户数量只需要在维度上增长 logarithmic。此外，我们的算法还是首次实现了非滑动函数的优化率在 polynomial 时间内。这些算法基于多个通道DP-SGD，结合了一种新的隐私性均值估计过程，用于集中的数据中心化，该过程包括一个异常 eliminating 步骤。
</details></li>
</ul>
<hr>
<h2 id="Neuro-GPT-Developing-A-Foundation-Model-for-EEG"><a href="#Neuro-GPT-Developing-A-Foundation-Model-for-EEG" class="headerlink" title="Neuro-GPT: Developing A Foundation Model for EEG"></a>Neuro-GPT: Developing A Foundation Model for EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03764">http://arxiv.org/abs/2311.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhui Cui, Woojae Jeong, Philipp Thölke, Takfarinas Medani, Karim Jerbi, Anand A. Joshi, Richard M. Leahy</li>
<li>for: 提高Brain-Computer Interface (BCI)任务中电энцеfalogram (EEG)数据的稀缺和多样性问题，并利用大规模公共数据来充分发挥作用。</li>
<li>methods: 基于EEG编码器和GPT模型的基础模型，通过自我超vised任务在大规模公共EEG数据集上预训练，然后在只有9名参与者的motor imagery classification任务上细化训练。</li>
<li>results: 实验表明，基于基础模型进行应用可以显著提高分类性能，证明了基础模型的进步普适性和数据稀缺和多样性问题的解决能力。<details>
<summary>Abstract</summary>
To handle the scarcity and heterogeneity of electroencephalography (EEG) data in Brain-Computer Interface (BCI) tasks, and to harness the vast public data, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale public EEG dataset, using a self-supervised task which learns how to reconstruct the masked chunk in EEG. We then fine-tune the foundation model on a Motor Imagery Classification task where only 9 subjects are available. Experiments demonstrated that applying foundation model can significantly improve classification performance compared to the model trained from scratch, which provides evidence for the advanced generalizability of foundation model and the ability to address the challenges of data scarcity and heterogeneity.
</details>
<details>
<summary>摘要</summary>
为了解决电enzephalography（EEG）数据的缺乏和多样性在Brain-Computer Interface（BCI）任务中，并利用大量公共数据，我们提出了Neuro-GPT，一个基础模型，包括EEG编码器和GPT模型。基础模型在一个大规模的公共EEG数据集上自我超vised学习任务上进行预训练，学习如何重建受mask的EEG块。然后，我们在只有9名参与者的motor imagery分类任务上细化基础模型。实验表明，对基础模型进行应用可以显著提高分类性能，比起从scratch训练的模型，这提供了证据，证明了基础模型的高度通用性和能力 Addressing the challenges of data scarcity and heterogeneity。
</details></li>
</ul>
<hr>
<h2 id="Posterior-Sampling-Based-Bayesian-Optimization-with-Tighter-Bayesian-Regret-Bounds"><a href="#Posterior-Sampling-Based-Bayesian-Optimization-with-Tighter-Bayesian-Regret-Bounds" class="headerlink" title="Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds"></a>Posterior Sampling-Based Bayesian Optimization with Tighter Bayesian Regret Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03760">http://arxiv.org/abs/2311.03760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shion Takeno, Yu Inatsu, Masayuki Karasuyama, Ichiro Takeuchi</li>
<li>for: This paper focuses on improving the Bayesian cumulative regret (BCR) of three acquisition functions (AFs) in Bayesian optimization (BO): Gaussian process upper confidence bound (GP-UCB), Thompson sampling (TS), and a new proposed AF called probability of improvement from the maximum of a sample path (PIMS).</li>
<li>methods: The paper uses theoretical analysis and experimental evaluation to compare the BCR bounds of GP-UCB, TS, and PIMS. The authors also propose PIMS as a new AF that achieves the tighter BCR bound and avoids manual hyperparameter tuning.</li>
<li>results: The paper shows that PIMS achieves the tighter BCR bound and mitigates the practical issues of GP-UCB and TS in a wide range of experiments. Specifically, PIMS outperforms GP-UCB and TS in terms of BCR and avoids the need for hyperparameter tuning.<details>
<summary>Abstract</summary>
Among various acquisition functions (AFs) in Bayesian optimization (BO), Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) are well-known options with established theoretical properties regarding Bayesian cumulative regret (BCR). Recently, it has been shown that a randomized variant of GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call the tighter BCR bound for brevity. Inspired by this study, this paper first shows that TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS often practically suffer from manual hyperparameter tuning and over-exploration issues, respectively. To overcome these difficulties, we propose yet another AF called a probability of improvement from the maximum of a sample path (PIMS). We show that PIMS achieves the tighter BCR bound and avoids the hyperparameter tuning, unlike GP-UCB. Furthermore, we demonstrate a wide range of experiments, focusing on the effectiveness of PIMS that mitigates the practical issues of GP-UCB and TS.
</details>
<details>
<summary>摘要</summary>
Among various acquisition functions (AFs) in Bayesian optimization (BO), Gaussian process upper confidence bound (GP-UCB) and Thompson sampling (TS) are well-known options with established theoretical properties regarding Bayesian cumulative regret (BCR). Recently, it has been shown that a randomized variant of GP-UCB achieves a tighter BCR bound compared with GP-UCB, which we call the tighter BCR bound for brevity. Inspired by this study, this paper first shows that TS achieves the tighter BCR bound. On the other hand, GP-UCB and TS often practically suffer from manual hyperparameter tuning and over-exploration issues, respectively. To overcome these difficulties, we propose yet another AF called a probability of improvement from the maximum of a sample path (PIMS). We show that PIMS achieves the tighter BCR bound and avoids the hyperparameter tuning, unlike GP-UCB. Furthermore, we demonstrate a wide range of experiments, focusing on the effectiveness of PIMS that mitigates the practical issues of GP-UCB and TS.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Manifold-learning-what-how-and-why"><a href="#Manifold-learning-what-how-and-why" class="headerlink" title="Manifold learning: what, how, and why"></a>Manifold learning: what, how, and why</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03757">http://arxiv.org/abs/2311.03757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marina Meilă, Hanyu Zhang</li>
<li>for: 本文旨在介绍机能学（ML）的原理、方法和统计基础，以帮助读者更好地理解高维数据的低维结构。</li>
<li>methods: 本文主要介绍了ML方法的代表选择和参数选择的权衡问题，以及这些方法的统计基础。</li>
<li>results: 本文通过描述高维点云的几何形态和可视化，以及去噪和解释高维数据的方法，满足读者寻找低维结构的需求。<details>
<summary>Abstract</summary>
Manifold learning (ML), known also as non-linear dimension reduction, is a set of methods to find the low dimensional structure of data. Dimension reduction for large, high dimensional data is not merely a way to reduce the data; the new representations and descriptors obtained by ML reveal the geometric shape of high dimensional point clouds, and allow one to visualize, de-noise and interpret them. This survey presents the principles underlying ML, the representative methods, as well as their statistical foundations from a practicing statistician's perspective. It describes the trade-offs, and what theory tells us about the parameter and algorithmic choices we make in order to obtain reliable conclusions.
</details>
<details>
<summary>摘要</summary>
manifold学习（ML），也称非线性维度减少，是一组方法来找出数据的低维度结构。对大量、高维度数据进行维度减少不仅是将数据减少，新的表示和描述器由ML获得的减少结构，可以visualize、去噪和解释高维度点云的几何形状。这篇评论介绍了ML的原则、代表方法以及其统计基础，从实际统计师的角度出发，描述了参数和算法选择的交易OFF和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-physics-informed-neural-networks-with-domain-scaling-and-residual-correction-methods-for-multi-frequency-elliptic-problems"><a href="#Enhanced-physics-informed-neural-networks-with-domain-scaling-and-residual-correction-methods-for-multi-frequency-elliptic-problems" class="headerlink" title="Enhanced physics-informed neural networks with domain scaling and residual correction methods for multi-frequency elliptic problems"></a>Enhanced physics-informed neural networks with domain scaling and residual correction methods for multi-frequency elliptic problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03746">http://arxiv.org/abs/2311.03746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deok-Kyu Jang, Hyea Hyun Kim, Kyungsoo Kim</li>
<li>for: 这个论文是为了解决非线性偏微分方程的多频解决方法。</li>
<li>methods: 论文提出了基于神经网络的近似方法，具有不受不同偏微分方程的形式或问题域形状或维度的限制的优点。</li>
<li>results: 论文通过域扩展和差异修正方法提高了近似方法的效率和准确性，并在多频模拟问题中进行了证明。<details>
<summary>Abstract</summary>
In this paper, neural network approximation methods are developed for elliptic partial differential equations with multi-frequency solutions. Neural network work approximation methods have advantages over classical approaches in that they can be applied without much concerns on the form of the differential equations or the shape or dimension of the problem domain. When applied to problems with multi-frequency solutions, the performance and accuracy of neural network approximation methods are strongly affected by the contrast of the high- and low-frequency parts in the solutions. To address this issue, domain scaling and residual correction methods are proposed. The efficiency and accuracy of the proposed methods are demonstrated for multi-frequency model problems.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了基于神经网络的积分方法，用于解决带多频解的几何偏微分方程。神经网络方法在比较古典方法时有优势，因为它们不受偏微分方程的形式或问题域的形状或维度的限制。当应用到具有多频解的问题时，神经网络方法的性能和准确性受到解的高频和低频部分的对比的影响。为解决这个问题，我们提出了域扩大和差异修正方法。我们对多频模拟问题进行了效率和准确性的证明。
</details></li>
</ul>
<hr>
<h2 id="Improved-weight-initialization-for-deep-and-narrow-feedforward-neural-network"><a href="#Improved-weight-initialization-for-deep-and-narrow-feedforward-neural-network" class="headerlink" title="Improved weight initialization for deep and narrow feedforward neural network"></a>Improved weight initialization for deep and narrow feedforward neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03733">http://arxiv.org/abs/2311.03733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunwoo Lee, Yunho Kim, Seungyeop Yang, Hayoung Choi</li>
<li>for: 解决深度学习中ReLU neuron死亡问题，提高深度 neural network 的训练效果和效率。</li>
<li>methods: 提出一种新的初始 веса初始化方法，并证明该方法可以有效地传递信号 вектор。</li>
<li>results: 通过一系列实验和比较 existed 方法，证明新 initialization 方法的效果。<details>
<summary>Abstract</summary>
Appropriate weight initialization settings, along with the ReLU activation function, have been a cornerstone of modern deep learning, making it possible to train and deploy highly effective and efficient neural network models across diverse artificial intelligence. The problem of dying ReLU, where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a new weight initialization method to address this issue. We prove the properties of the proposed initial weight matrix and demonstrate how these properties facilitate the effective propagation of signal vectors. Through a series of experiments and comparisons with existing methods, we demonstrate the effectiveness of the new initialization method.
</details>
<details>
<summary>摘要</summary>
现代深度学习的重要基础之一是适当的初始 веса设置，以及ReLU激活函数。这些设置使得可以训练和部署高效和高效的神经网络模型。ReLU激活函数的问题，即ReLU神经元变得无作用并产生零输出，对深度神经网络的训练呈现出了 significiant挑战。理论研究和各种方法已经被提出来解决这个问题。然而，即使使用这些方法和研究，训练非常深和窄的Feedforward网络仍然具有挑战性。在这篇论文中，我们提出了一种新的初始 веса方法，以解决这个问题。我们证明了提案的初始 веса矩阵的属性，并示出这些属性如何促进信号向量的有效传播。通过一系列实验和现有方法的比较，我们证明了新 initialization 方法的效果。
</details></li>
</ul>
<hr>
<h2 id="Pipeline-Parallelism-for-DNN-Inference-with-Practical-Performance-Guarantees"><a href="#Pipeline-Parallelism-for-DNN-Inference-with-Practical-Performance-Guarantees" class="headerlink" title="Pipeline Parallelism for DNN Inference with Practical Performance Guarantees"></a>Pipeline Parallelism for DNN Inference with Practical Performance Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03703">http://arxiv.org/abs/2311.03703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron Archer, Matthew Fahrbach, Kuikui Liu, Prakash Prabhu</li>
<li>for: 优化深度神经网络（DNN）推理的管道并行性，通过将模型图分割成 $k$ 个阶段，最小化瓶颈阶段的运行时间，包括通信时间。</li>
<li>methods: 提出了实用的算法来解决这个NP困难的问题，并对这些算法进行了优化。同时，通过新的杂合数学Programming（MIP）形式ulation来获得更强的下界。</li>
<li>results: 通过应用这些算法和下界方法来评估生产模型，实现了比标准 combinatorial下界更好的近似保证。例如，通过 geometric means across production data with $k&#x3D;16$ pipeline stages, our MIP formulations more than double the lower bounds, improving the approximation ratio from $2.175$ to $1.058$.<details>
<summary>Abstract</summary>
We optimize pipeline parallelism for deep neural network (DNN) inference by partitioning model graphs into $k$ stages and minimizing the running time of the bottleneck stage, including communication. We design practical algorithms for this NP-hard problem and show that they are nearly optimal in practice by comparing against strong lower bounds obtained via novel mixed-integer programming (MIP) formulations. We apply these algorithms and lower-bound methods to production models to achieve substantially improved approximation guarantees compared to standard combinatorial lower bounds. For example, evaluated via geometric means across production data with $k=16$ pipeline stages, our MIP formulations more than double the lower bounds, improving the approximation ratio from $2.175$ to $1.058$. This work shows that while max-throughput partitioning is theoretically hard, we have a handle on the algorithmic side of the problem in practice and much of the remaining challenge is in developing more accurate cost models to feed into the partitioning algorithms.
</details>
<details>
<summary>摘要</summary>
我们优化深度神经网络（DNN）推断的管线并行性，通过分解模型图 into $k$ 阶段，最小化瓶颈阶段的执行时间，包括通信。我们设计了实用的算法来解决这个NP困难的问题，并证明它们在实践中几乎是最佳的。我们透过与新的混合整数程式（MIP）形式的比较，得到了强有力的下限。我们将这些算法和下限方法应用到生产模型，以取得较好的近似保证比。例如，透过生产数据中的$k=16$ 管线阶段，我们的MIP形式更是多过了下限，从$2.175$ 提高到$1.058$。这个工作显示，优化管线并行性是理论上困难的，但在实践中，我们已经掌握了算法的一侧，主要的挑战是发展更加精确的成本模型，以供分配算法使用。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Non-monotone-Submodular-Maximization"><a href="#Dynamic-Non-monotone-Submodular-Maximization" class="headerlink" title="Dynamic Non-monotone Submodular Maximization"></a>Dynamic Non-monotone Submodular Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03685">http://arxiv.org/abs/2311.03685</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiarash Banihashem, Leyla Biabani, Samira Goudarzi, MohammadTaghi Hajiaghayi, Peyman Jabbarzade, Morteza Monemizadeh</li>
<li>for: 这个论文主要针对的问题是凸覆盖函数的最大化问题，具体来说是在卡尔达ности约束$k$下最大化凸覆盖函数。</li>
<li>methods: 这篇论文使用了动态算法来解决凸覆盖函数最大化问题，并且提出了一种可以应用于非凸覆盖函数的方法。</li>
<li>results: 这篇论文提出了一种能够在卡尔达性约束$k$下实现$(8+\epsilon)$估计的动态算法，并且可以在执行多个更新后继续使用。此外，论文还应用了这种算法于视频概要和max-cut问题，并在实际数据集上 obtaint 良好的结果。<details>
<summary>Abstract</summary>
Maximizing submodular functions has been increasingly used in many applications of machine learning, such as data summarization, recommendation systems, and feature selection. Moreover, there has been a growing interest in both submodular maximization and dynamic algorithms. In 2020, Monemizadeh and Lattanzi, Mitrovic, Norouzi{-}Fard, Tarnawski, and Zadimoghaddam initiated developing dynamic algorithms for the monotone submodular maximization problem under the cardinality constraint $k$. Recently, there have been some improvements on the topic made by Banihashem, Biabani, Goudarzi, Hajiaghayi, Jabbarzade, and Monemizadeh. In 2022, Chen and Peng studied the complexity of this problem and raised an important open question: "Can we extend [fully dynamic] results (algorithm or hardness) to non-monotone submodular maximization?". We affirmatively answer their question by demonstrating a reduction from maximizing a non-monotone submodular function under the cardinality constraint $k$ to maximizing a monotone submodular function under the same constraint. Through this reduction, we obtain the first dynamic algorithms to solve the non-monotone submodular maximization problem under the cardinality constraint $k$. Our algorithms maintain an $(8+\epsilon)$-approximate of the solution and use expected amortized $O(\epsilon^{-3}k^3\log^3(n)\log(k))$ or $O(\epsilon^{-1}k^2\log^3(k))$ oracle queries per update, respectively. Furthermore, we showcase the benefits of our dynamic algorithm for video summarization and max-cut problems on several real-world data sets.
</details>
<details>
<summary>摘要</summary>
maximizing submodular functions在机器学习中越来越受欢迎，应用于数据概要、推荐系统和特征选择等领域。此外，关于submodular maximization和动态算法的研究也有增长兴趣。在2020年，Monemizadeh和Lattanzi、Mitrovic、Norouzi{-}Fard、Tarnawski和Zadimoghaddam开始开发动态算法 для偏函数最大化问题下的cardinality约束$k$。最近，有一些在这个领域进行了改进。在2022年，陈和平 studied this problem的复杂性，提出了一个重要的开问："可以将全动态结果（算法或困难）扩展到非偏函数最大化问题吗？"。我们答复了这个问题，通过示出减法函数最大化问题下的cardinality约束$k$和非偏函数最大化问题下的cardinality约束$k$之间的等价转换，从而得到了首个解决非偏函数最大化问题下的动态算法。我们的算法保证可以获得$(8+\epsilon)$-近似的解决方案，并使用预期的折衔折衔$O(\epsilon^{-3}k^3\log^3(n)\log(k))$或$O(\epsilon^{-1}k^2\log^3(k))$的缓存查询次数。此外，我们还展示了我们的动态算法在视频概要和max-cut问题中的应用效果。
</details></li>
</ul>
<hr>
<h2 id="Preventing-Arbitrarily-High-Confidence-on-Far-Away-Data-in-Point-Estimated-Discriminative-Neural-Networks"><a href="#Preventing-Arbitrarily-High-Confidence-on-Far-Away-Data-in-Point-Estimated-Discriminative-Neural-Networks" class="headerlink" title="Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks"></a>Preventing Arbitrarily High Confidence on Far-Away Data in Point-Estimated Discriminative Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03683">http://arxiv.org/abs/2311.03683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart</li>
<li>for: 这个研究旨在解决深度学习模型在对外测试数据的预测中出现的过度自信问题。</li>
<li>methods: 作者提出了一种方法，即在深度学习模型的输出加入一个类别的条件函数，以预测测试数据是否属于训练数据的类别。</li>
<li>results: 试验结果显示，这种方法可以有效地预防深度学习模型在对外测试数据的预测中出现过度自信的问题，同时仍然维持了简单的推断点估计训练。<details>
<summary>Abstract</summary>
Discriminatively trained, deterministic neural networks are the de facto choice for classification problems. However, even though they achieve state-of-the-art results on in-domain test sets, they tend to be overconfident on out-of-distribution (OOD) data. For instance, ReLU networks -- a popular class of neural network architectures -- have been shown to almost always yield high confidence predictions when the test data are far away from the training set, even when they are trained with OOD data. We overcome this problem by adding a term to the output of the neural network that corresponds to the logit of an extra class, that we design to dominate the logits of the original classes as we move away from the training data.This technique provably prevents arbitrarily high confidence on far-away test data while maintaining a simple discriminative point-estimate training. Evaluation on various benchmarks demonstrates strong performance against competitive baselines on both far-away and realistic OOD data.
</details>
<details>
<summary>摘要</summary>
通用化训练的权重链是现实中的首选 Classification 问题。然而，它们在不同 FROM 数据上表现出STATE-OF-THE-ART 的结果，但它们往往对于 OUT-OF-DISTRIBUTION 数据（OOD）表现出过于自信。例如，ReLU 网络——一种广泛使用的神经网络架构——在测试数据远离训练数据时经常提供高自信度预测，即使它们在训练 OOD 数据时被训练。我们解决了这个问题，通过将一个对应于额外类的条件添加到神经网络的输出中，使得随着测试数据的远离，预测结果呈指数趋势，从而避免了无限高的自信度。这种技术可证明地防止了无限高的自信度在测试数据远离训练数据时，同时保持简单的推学点估计训练。在多个标准 bench 上，我们的方法与高端基准值进行了有力的竞争。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Power-Grid-Operational-Risk-Assessment"><a href="#Graph-Neural-Networks-for-Power-Grid-Operational-Risk-Assessment" class="headerlink" title="Graph Neural Networks for Power Grid Operational Risk Assessment"></a>Graph Neural Networks for Power Grid Operational Risk Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03661">http://arxiv.org/abs/2311.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yadong Zhang, Pranav M Karve, Sankaran Mahadevan</li>
<li>for: This paper is written for the purpose of investigating the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grids.</li>
<li>methods: The paper uses GNN surrogates of optimal power flow (OPF) problems to expedite the MC simulation process, which is computationally prohibitive. The GNN surrogates are trained using supervised learning and are used to obtain MC samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast.</li>
<li>results: The paper shows that GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The paper thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究使用图 neural network（GNN）仿真器来实现 Monte Carlo（MC）采样基于的电力网运行风险评估的实用性。</li>
<li>methods: 这篇论文使用GNN仿真器来快速解决MC采样过程中的优化电力流问题，这是计算昂贵的。GNN仿真器通过监督学习来训练，并用于根据（小时前）风力生产和吞吐量预测获取MC采样值。</li>
<li>results: 这篇论文表明GNN仿真器可以准确地预测（电压级、电流级和系统级）电力网状态，并且可以快速而准确地评估电力网运行风险。因此，这篇论文开发了基于GNN的快速可靠性和风险评估工具 для实际电力网。<details>
<summary>Abstract</summary>
In this article, the utility of graph neural network (GNN) surrogates for Monte Carlo (MC) sampling-based risk quantification in daily operations of power grid is investigated. The MC simulation process necessitates solving a large number of optimal power flow (OPF) problems corresponding to the sample values of stochastic grid variables (power demand and renewable generation), which is computationally prohibitive. Computationally inexpensive surrogates of the OPF problem provide an attractive alternative for expedited MC simulation. GNN surrogates are especially suitable due to their superior ability to handle graph-structured data. Therefore, GNN surrogates of OPF problem are trained using supervised learning. They are then used to obtain Monte Carlo (MC) samples of the quantities of interest (operating reserve, transmission line flow) given the (hours-ahead) probabilistic wind generation and load forecast. The utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based grid reliability and risk for IEEE Case118 synthetic grid. It is shown that the GNN surrogates are sufficiently accurate for predicting the (bus-level, branch-level and system-level) grid state and enable fast as well as accurate operational risk quantification for power grids. The article thus develops various tools for fast reliability and risk quantification for real-world power grids using GNNs.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们调查了graph neural network（GNN）伪函数的 utility для Monte Carlo（MC）样本基于的电力网络风险评估。MC模拟过程需要解决一个大量的优化电力流（OPF）问题，这是计算昂贵的。GNN伪函数特别适合处理图结构数据，因此GNN伪函数可以用于快速和准确地评估电力网络的可靠性和风险。在IEEE Case118 sintética网格上，我们证明了GNN伪函数是准确的预测（电网）状态的。这篇文章因此开发了各种工具，用于快速和准确地评估实际电力网络的可靠性和风险。
</details></li>
</ul>
<hr>
<h2 id="A-Physics-Guided-Bi-Fidelity-Fourier-Featured-Operator-Learning-Framework-for-Predicting-Time-Evolution-of-Drag-and-Lift-Coefficients"><a href="#A-Physics-Guided-Bi-Fidelity-Fourier-Featured-Operator-Learning-Framework-for-Predicting-Time-Evolution-of-Drag-and-Lift-Coefficients" class="headerlink" title="A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients"></a>A Physics-Guided Bi-Fidelity Fourier-Featured Operator Learning Framework for Predicting Time Evolution of Drag and Lift Coefficients</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03639">http://arxiv.org/abs/2311.03639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Mollaali, Izzet Sahin, Iqrar Raza, Christian Moya, Guillermo Paniagua, Guang Lin</li>
<li>for: 本研究的目的是提高计算机 experiments 的准确性和效率，并 minimize 计算资源的使用。</li>
<li>methods: 本文提出了一种基于深度学习的抽象网络框架，通过限制高精度数据的使用，实现高精度解的计算。这个框架基于一种新的物理指导的、双精度的 Fourier 特征网络（DeepONet），可以同时利用低精度和高精度数据的优点。</li>
<li>results: 研究结果表明，基于 DeepONet 的方法可以更高精度地预测振荡性解的时间轨迹，并且比数据驱动的方法更具有预测能力。<details>
<summary>Abstract</summary>
In the pursuit of accurate experimental and computational data while minimizing effort, there is a constant need for high-fidelity results. However, achieving such results often requires significant computational resources. To address this challenge, this paper proposes a deep operator learning-based framework that requires a limited high-fidelity dataset for training. We introduce a novel physics-guided, bi-fidelity, Fourier-featured Deep Operator Network (DeepONet) framework that effectively combines low and high-fidelity datasets, leveraging the strengths of each. In our methodology, we began by designing a physics-guided Fourier-featured DeepONet, drawing inspiration from the intrinsic physical behavior of the target solution. Subsequently, we train this network to primarily learn the low-fidelity solution, utilizing an extensive dataset. This process ensures a comprehensive grasp of the foundational solution patterns. Following this foundational learning, the low-fidelity deep operator network's output is enhanced using a physics-guided Fourier-featured residual deep operator network. This network refines the initial low-fidelity output, achieving the high-fidelity solution by employing a small high-fidelity dataset for training. Notably, in our framework, we employ the Fourier feature network as the Trunk network for the DeepONets, given its proficiency in capturing and learning the oscillatory nature of the target solution with high precision. We validate our approach using a well-known 2D benchmark cylinder problem, which aims to predict the time trajectories of lift and drag coefficients. The results highlight that the physics-guided Fourier-featured deep operator network, serving as a foundational building block of our framework, possesses superior predictive capability for the lift and drag coefficients compared to its data-driven counterparts.
</details>
<details>
<summary>摘要</summary>
在寻求精确的实验和计算数据的同时减少努力的挑战中，需要高精度的结果。然而，获得这些结果通常需要巨大的计算资源。为解决这个挑战，这篇论文提出了一个基于深度学习的核心运算学习框架，只需要一个有限的高精度数据集来训练。我们介绍了一种新的物理指导的、双精度的、快朗特网络（DeepONet）框架，可以有效地结合低精度和高精度数据集，利用每个数据集的优势。在我们的方法中，我们首先设计了物理指导的快朗特网络， Drawing inspiration from the intrinsic physical behavior of the target solution。然后，我们使用了一个广泛的低精度数据集来训练这个网络，以确保拥有基本解决方案的基本模式。在这个基本学习后，我们使用物理指导的快朗特网络来进一步增强低精度深度运算网络的输出，以实现高精度的解决方案。在我们的框架中，我们使用快朗特网络作为Trunk网络，因为它能够高精度地捕捉和学习目标解决方案中的律动特征。我们验证了我们的方法使用了一个已知的2D标准问题，该问题的目标是预测挡板和推力减速系数的时间轨迹。结果显示，物理指导的快朗特网络作为我们框架的基础建筑块，在预测挡板和推力减速系数方面具有更高的预测能力，比其数据驱动的对手更好。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Data-Augmentation-with-Contrastive-Learning"><a href="#Counterfactual-Data-Augmentation-with-Contrastive-Learning" class="headerlink" title="Counterfactual Data Augmentation with Contrastive Learning"></a>Counterfactual Data Augmentation with Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03630">http://arxiv.org/abs/2311.03630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Aloui, Juncheng Dong, Cat P. Le, Vahid Tarokh<br>for: 该研究旨在解决 conditional average treatment effect (CATE) 估计中的统计差异问题，提高 CATE 估计的准确性和稳定性。methods: 该研究提出了一种模型无关的数据扩充方法，使用对照学习来学习一个表示空间和一个相似度度量，以确保可靠地估计对照组中个体的可能的结果。results: 经过 theoretical analysis 和实验研究，该方法能够在 synthetic 和半synthetic 标准 benchmark 上实现显著提高 CATE 估计的性能和对抗过拟合的能力。<details>
<summary>Abstract</summary>
Statistical disparity between distinct treatment groups is one of the most significant challenges for estimating Conditional Average Treatment Effects (CATE). To address this, we introduce a model-agnostic data augmentation method that imputes the counterfactual outcomes for a selected subset of individuals. Specifically, we utilize contrastive learning to learn a representation space and a similarity measure such that in the learned representation space close individuals identified by the learned similarity measure have similar potential outcomes. This property ensures reliable imputation of counterfactual outcomes for the individuals with close neighbors from the alternative treatment group. By augmenting the original dataset with these reliable imputations, we can effectively reduce the discrepancy between different treatment groups, while inducing minimal imputation error. The augmented dataset is subsequently employed to train CATE estimation models. Theoretical analysis and experimental studies on synthetic and semi-synthetic benchmarks demonstrate that our method achieves significant improvements in both performance and robustness to overfitting across state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
“统计差异 между不同处理组是计算 conditional Average Treatment Effects（CATE）的主要挑战之一。为解决这个问题，我们介绍了一种模型无关的数据增强方法，该方法在选择的一部分个体上进行了可靠的假设替换。具体来说，我们使用了对比学习来学习一个表示空间和一种相似度度量，使得在这个表示空间中与选择的一部分个体的相似度度量相似的个体都有相似的可能结果。这个性质使得我们可以可靠地进行这些个体的假设替换，从而减少不同处理组之间的统计差异，同时减少潜在的假设替换误差。通过将这些可靠的假设替换加入原始数据集，我们可以有效地降低不同处理组之间的统计差异，同时提高计算CATE的模型性能和鲁棒性。”
</details></li>
</ul>
<hr>
<h2 id="Are-Words-Enough-On-the-semantic-conditioning-of-affective-music-generation"><a href="#Are-Words-Enough-On-the-semantic-conditioning-of-affective-music-generation" class="headerlink" title="Are Words Enough? On the semantic conditioning of affective music generation"></a>Are Words Enough? On the semantic conditioning of affective music generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03624">http://arxiv.org/abs/2311.03624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Forero, Gilberto Bernardes, Mónica Mendes</li>
<li>for: 本研究旨在分析和讨论自动生成音乐的情感表达方法。</li>
<li>methods: 本研究使用了两种主要的方法：规则式模型和机器学习模型，其中深度学习模型可以自动生成高质量的音乐从文本描述中。</li>
<li>results: 研究发现，使用深度学习模型可以有效地表达情感，但是语言中的限制和模糊性可能会影响音乐的表达。不过，使用深度学习与自然语言结合可能会对创作业务产生深见的影响，提供powerful工具来提及和生成新的音乐作品。<details>
<summary>Abstract</summary>
Music has been commonly recognized as a means of expressing emotions. In this sense, an intense debate emerges from the need to verbalize musical emotions. This concern seems highly relevant today, considering the exponential growth of natural language processing using deep learning models where it is possible to prompt semantic propositions to generate music automatically. This scoping review aims to analyze and discuss the possibilities of music generation conditioned by emotions. To address this topic, we propose a historical perspective that encompasses the different disciplines and methods contributing to this topic. In detail, we review two main paradigms adopted in automatic music generation: rules-based and machine-learning models. Of note are the deep learning architectures that aim to generate high-fidelity music from textual descriptions. These models raise fundamental questions about the expressivity of music, including whether emotions can be represented with words or expressed through them. We conclude that overcoming the limitation and ambiguity of language to express emotions through music, some of the use of deep learning with natural language has the potential to impact the creative industries by providing powerful tools to prompt and generate new musical works.
</details>
<details>
<summary>摘要</summary>
音乐已被广泛认可为表达情感的工具。在这种情况下，有一场激烈的辩论是如何用语言表达音乐中的情感。这个问题在今天更加有 relevance，因为深度学习模型在自然语言处理领域的快速发展，可以通过提出 semantics 提案来自动生成音乐。本文综述分析和讨论了基于情感的音乐生成的可能性。为此，我们提出了历史背景，涵盖不同领域和方法对这个话题的贡献。具体来说，我们回顾了两种主要的自动音乐生成方法：规则驱动和机器学习模型。特别是深度学习架构，可以生成高级别的音乐从文本描述。这些模型提出了音乐表达的基本问题，包括情感是否可以通过语言表达，或者通过语言表达出来。我们结论认为，超越语言表达情感的限制和抽象，使用深度学习与自然语言的结合可能对创作业务产生强大的影响，提供了 poderful 的工具来促进和生成新的音乐作品。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Latent-Spaces-of-Tonal-Music-using-Variational-Autoencoders"><a href="#Exploring-Latent-Spaces-of-Tonal-Music-using-Variational-Autoencoders" class="headerlink" title="Exploring Latent Spaces of Tonal Music using Variational Autoencoders"></a>Exploring Latent Spaces of Tonal Music using Variational Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03621">http://arxiv.org/abs/2311.03621</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nadiacarvalho/latent-tonal-music">https://github.com/nadiacarvalho/latent-tonal-music</a></li>
<li>paper_authors: Nádia Carvalho, Gilberto Bernardes</li>
<li>for: 这项研究用 VaR 模型来生成含义和认知价值的秘密表示。</li>
<li>methods: 研究使用了不同的 VaR 模型Encodings（Piano Roll、MIDI、ABC、Tonnetz、DFT of pitch、pitch class distributions），并对这些编码的latent space进行评估，以确定它们是否能够准确地表示音乐认知空间。</li>
<li>results: 研究发现，使用 ABC 编码的 VaR 模型能够最好地重construct原始数据，而 Pitch DFT 编码则能够从latent space中提取更多的信息。此外，通过对12个主要或小调的转调进行对比，研究发现 VAE 模型的 latent space与认知空间之间存在很好的对应关系，并且可以用来评估不同关键之间的关系。<details>
<summary>Abstract</summary>
Variational Autoencoders (VAEs) have proven to be effective models for producing latent representations of cognitive and semantic value. We assess the degree to which VAEs trained on a prototypical tonal music corpus of 371 Bach's chorales define latent spaces representative of the circle of fifths and the hierarchical relation of each key component pitch as drawn in music cognition. In detail, we compare the latent space of different VAE corpus encodings -- Piano roll, MIDI, ABC, Tonnetz, DFT of pitch, and pitch class distributions -- in providing a pitch space for key relations that align with cognitive distances. We evaluate the model performance of these encodings using objective metrics to capture accuracy, mean square error (MSE), KL-divergence, and computational cost. The ABC encoding performs the best in reconstructing the original data, while the Pitch DFT seems to capture more information from the latent space. Furthermore, an objective evaluation of 12 major or minor transpositions per piece is adopted to quantify the alignment of 1) intra- and inter-segment distances per key and 2) the key distances to cognitive pitch spaces. Our results show that Pitch DFT VAE latent spaces align best with cognitive spaces and provide a common-tone space where overlapping objects within a key are fuzzy clusters, which impose a well-defined order of structural significance or stability -- i.e., a tonal hierarchy. Tonal hierarchies of different keys can be used to measure key distances and the relationships of their in-key components at multiple hierarchies (e.g., notes and chords). The implementation of our VAE and the encodings framework are made available online.
</details>
<details>
<summary>摘要</summary>
variational autoencoders (VAEs) 已经证明是有效的模型，用于生成含义和semantic value的秘密表示。我们评估了VAEs在371首巴赫 Chorales 中的含义空间是否与音乐认知中的圆形五度和每个关键组件折射之间的层次关系相对应。具体来说，我们比较了不同的 VAE 嵌入空间 -- Piano roll, MIDI, ABC, Tonnetz, DFT of pitch, 和折射分布 -- 在提供一个折射空间来处理关键之间的关系，并使用对象metric来衡量准确性、平方误差（MSE）、KL散度和计算成本。ABC嵌入空间表现最好地重建原始数据，而折射 DFT 则能够从含义空间中提取更多的信息。此外，我们采用对象评估方法，对每个关键进行12个主小调或小调轨迹的对比，以量化每个关键与认知折射空间之间的对应关系。我们的结果表明，折射 DFT VAE 含义空间最好地与认知空间相对应，并提供一个共同频谱空间，在这里，每个关键的内部对象在一个key中是混合的不确定集合，这种结构具有明确的顺序和稳定性 -- i.e.,  tonality hierarchy。不同关键的tonality hierarchy可以用来测量关键之间的距离和关键组件在多个层次（例如，音和和弦）之间的关系。我们的 VAE 和嵌入空间框架已经在线上实现。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/cs.LG_2023_11_07/" data-id="closbrosm00td0g88at5j5hv5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/eess.IV_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T09:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/eess.IV_2023_11_07/">eess.IV - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improved-Topological-Preservation-in-3D-Axon-Segmentation-and-Centerline-Detection-using-Geometric-Assessment-driven-Topological-Smoothing-GATS"><a href="#Improved-Topological-Preservation-in-3D-Axon-Segmentation-and-Centerline-Detection-using-Geometric-Assessment-driven-Topological-Smoothing-GATS" class="headerlink" title="Improved Topological Preservation in 3D Axon Segmentation and Centerline Detection using Geometric Assessment-driven Topological Smoothing (GATS)"></a>Improved Topological Preservation in 3D Axon Segmentation and Centerline Detection using Geometric Assessment-driven Topological Smoothing (GATS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04116">http://arxiv.org/abs/2311.04116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nina I. Shamsi, Alex S. Xu, Lars A. Gjesteby, Laura J. Brattain</li>
<li>for: 这个论文是为了提出一种自动化 axon 追踪方法，用于帮助更好地处理大量 3D 脑图像数据，避免过于耗时和劳动 INTRO </li>
<li>methods: 该方法使用了完全监督学习，并且使用了 topology-preserving 方法，以保持核心结构的准确性。它还使用了 geometric assessment 来对 tubular 结构进行自动化缓减。</li>
<li>results: 该方法在多个数据集上显示了2%-5%的提升，并且降低了 Betti 错误率9%。具体来说， geometric assessment 对 tubular 结构的评估得到了更高的 segmentation 和 centerline 检测分数，而使用 average pooling  instead of thinning 算法可以更好地保持 topological 结构。<details>
<summary>Abstract</summary>
Automated axon tracing via fully supervised learning requires large amounts of 3D brain imagery, which is time consuming and laborious to obtain. It also requires expertise. Thus, there is a need for more efficient segmentation and centerline detection techniques to use in conjunction with automated annotation tools. Topology-preserving methods ensure that segmented components maintain geometric connectivity, which is especially meaningful for applications where volumetric data is used, and these methods often make use of morphological thinning algorithms as the thinned outputs can be useful for both segmentation and centerline detection of curvilinear structures. Current morphological thinning approaches used in conjunction with topology-preserving methods are prone to over-thinning and require manual configuration of hyperparameters. We propose an automated approach for morphological smoothing using geometric assessment of the radius of tubular structures in brain microscopy volumes, and apply average pooling to prevent over-thinning. We use this approach to formulate a loss function, which we call Geo-metric Assessment-driven Topological Smoothing loss, or GATS. Our approach increased segmentation and center-line detection evaluation metrics by 2%-5% across multiple datasets, and improved the Betti error rates by 9%. Our ablation study showed that geometric assessment of tubular structures achieved higher segmentation and centerline detection scores, and using average pooling for morphological smoothing in place of thinning algorithms reduced the Betti errors. We observed increased topological preservation during automated annotation of 3D axons volumes from models trained with GATS.
</details>
<details>
<summary>摘要</summary>
自动AXON追踪通过完全监督学习需要大量的3D脑图像，这是时间consuming和劳动 INTENSIVE 的获取。它还需要专业知识。因此，有一个更高效的分割和中心线检测技术的需求，用于与自动注释工具结合使用。保持topology的方法确保分割的组件保持几何连接，特别是在用volumetric数据时，这些方法经常使用 morphological thinning算法，因为压缩输出可以用于 both分割和中心线检测柔软结构。现有的 morphological thinning 方法在与保持topology方法结合使用时容易过度压缩，需要手动配置 гиперпараметров。我们提议一种自动化 morphological smoothing 方法，使用脑微scopy卷积体中的圆柱体卷积高度来评估圆柱体尺寸，并应用平均池化来避免过度压缩。我们称之为 Geo-metric Assessment-driven Topological Smoothing loss，简称 GATS。我们的方法在多个数据集上提高了分割和中心线检测评价指标2%-5%，并降低了 Betti 错误率9%。我们的剖析研究表明，基于圆柱体尺寸的几何评估可以提高分割和中心线检测分数，而使用平均池化 instead of thinning 算法可以降低 Betti 错误。我们发现在使用 GATS 自动注释3D axons 卷积体时， topological preservation 得到了提高。
</details></li>
</ul>
<hr>
<h2 id="Toward-ground-truth-optical-coherence-tomography-via-three-dimensional-unsupervised-deep-learning-processing-and-data"><a href="#Toward-ground-truth-optical-coherence-tomography-via-three-dimensional-unsupervised-deep-learning-processing-and-data" class="headerlink" title="Toward ground-truth optical coherence tomography via three-dimensional unsupervised deep learning processing and data"></a>Toward ground-truth optical coherence tomography via three-dimensional unsupervised deep learning processing and data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03887">http://arxiv.org/abs/2311.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renxiong Wu, Fei Zheng, Meixuan Li, Shaoyan Huang, Xin Ge, Linbo Liu, Yong Liu, Guangming Ni</li>
<li>for: 该研究旨在提出一种新的非侵入式高分辨率三维图像技术，以减少coherence speckle干扰，提高OCT成像性能。</li>
<li>methods: 该技术使用无监督的3D卷积神经网络，通过分离干扰和实际结构来实现speckle干扰自由OCT成像。</li>
<li>results: 实验结果表明，tGT-OCT可以减少干扰噪声，抑制实际结构的掩蔽，并保持空间分辨率。<details>
<summary>Abstract</summary>
Optical coherence tomography (OCT) can perform non-invasive high-resolution three-dimensional (3D) imaging and has been widely used in biomedical fields, while it is inevitably affected by coherence speckle noise which degrades OCT imaging performance and restricts its applications. Here we present a novel speckle-free OCT imaging strategy, named toward-ground-truth OCT (tGT-OCT), that utilizes unsupervised 3D deep-learning processing and leverages OCT 3D imaging features to achieve speckle-free OCT imaging. Specifically, our proposed tGT-OCT utilizes an unsupervised 3D-convolution deep-learning network trained using random 3D volumetric data to distinguish and separate speckle from real structures in 3D imaging volumetric space; moreover, tGT-OCT effectively further reduces speckle noise and reveals structures that would otherwise be obscured by speckle noise while preserving spatial resolution. Results derived from different samples demonstrated the high-quality speckle-free 3D imaging performance of tGT-OCT and its advancement beyond the previous state-of-the-art.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Fast-Algorithm-for-Low-Rank-Sparse-column-wise-Compressive-Sensing"><a href="#A-Fast-Algorithm-for-Low-Rank-Sparse-column-wise-Compressive-Sensing" class="headerlink" title="A Fast Algorithm for Low Rank + Sparse column-wise Compressive Sensing"></a>A Fast Algorithm for Low Rank + Sparse column-wise Compressive Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03824">http://arxiv.org/abs/2311.03824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silpa Babu, Namrata Vaswani</li>
<li>for: 本文研究了一个低级+稀疏（LR+S）列向压缩感知问题，目标是从 $m$ 个独立线性投影中重建一个 $n \times q$ 矩阵 $\X^* &#x3D; [ \x_1^*, \x_2^*, \cdots , \x_q^*]$，其中 $\y_k &#x3D; \A_k \x_k^*$ 是 $m$ 个矩阵的每个列的 $m$ 个独立线性投影。</li>
<li>methods: 本文提出了一种新的快速 GD-based 解决方案 called AltGDmin-LR+S，它是内存和通信协调的。</li>
<li>results: 通过对细致的 simulate 研究，本文证明了 AltGDmin-LR+S 的性能。<details>
<summary>Abstract</summary>
This paper focuses studies the following low rank + sparse (LR+S) column-wise compressive sensing problem. We aim to recover an $n \times q$ matrix, $\X^* =[ \x_1^*, \x_2^*, \cdots , \x_q^*]$ from $m$ independent linear projections of each of its $q$ columns, given by $\y_k :=\A_k\x_k^*$, $k \in [q]$. Here, $\y_k$ is an $m$-length vector with $m < n$. We assume that the matrix $\X^*$ can be decomposed as $\X^*=\L^*+\S^*$, where $\L^*$ is a low rank matrix of rank $r << \min(n,q)$ and $\S^*$ is a sparse matrix. Each column of $\S$ contains $\rho$ non-zero entries. The matrices $\A_k$ are known and mutually independent for different $k$. To address this recovery problem, we propose a novel fast GD-based solution called AltGDmin-LR+S, which is memory and communication efficient. We numerically evaluate its performance by conducting a detailed simulation-based study.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dose-aware-Diffusion-Model-for-3D-Ultra-Low-dose-PET-Imaging"><a href="#Dose-aware-Diffusion-Model-for-3D-Ultra-Low-dose-PET-Imaging" class="headerlink" title="Dose-aware Diffusion Model for 3D Ultra Low-dose PET Imaging"></a>Dose-aware Diffusion Model for 3D Ultra Low-dose PET Imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04248">http://arxiv.org/abs/2311.04248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huidong Xie, Weijie Gan, Bo Zhou, Xiongchao Chen, Qiong Liu, Xueqi Guo, Liang Guo, Hongyu An, Ulugbek S. Kamilov, Ge Wang, Chi Liu</li>
<li>for: 降低PET成像中的辐射剂量，以提高医疗影像质量并降低癌症风险。</li>
<li>methods: 提出了一种基于扩散模型的含量意识 diffusion model for 3D low-dose PET imaging (DDPET)，以同时降低PET图像的噪声水平和辐射剂量。</li>
<li>results: 对295名患者的数据进行测试，并与其他既定的扩散模型和噪声意识降阈模型进行比较，得到了更高的性能。<details>
<summary>Abstract</summary>
As PET imaging is accompanied by substantial radiation exposure and cancer risk, reducing radiation dose in PET scans is an important topic. Recently, diffusion models have emerged as the new state-of-the-art generative model to generate high-quality samples and have demonstrated strong potential for various tasks in medical imaging. However, it is difficult to extend diffusion models for 3D image reconstructions due to the memory burden. Directly stacking 2D slices together to create 3D image volumes would results in severe inconsistencies between slices. Previous works tried to either applying a penalty term along the z-axis to remove inconsistencies or reconstructing the 3D image volumes with 2 pre-trained perpendicular 2D diffusion models. Nonetheless, these previous methods failed to produce satisfactory results in challenging cases for PET image denoising. In addition to administered dose, the noise-levels in PET images are affected by several other factors in clinical settings, such as scan time, patient size, and weight, etc. Therefore, a method to simultaneously denoise PET images with different noise-levels is needed. Here, we proposed a dose-aware diffusion model for 3D low-dose PET imaging (DDPET) to address these challenges. The proposed DDPET method was tested on 295 patients from three different medical institutions globally with different low-dose levels. These patient data were acquired on three different commercial PET scanners, including Siemens Vision Quadra, Siemens mCT, and United Imaging Healthcare uExplorere. The proposed method demonstrated superior performance over previously proposed diffusion models for 3D imaging problems as well as models proposed for noise-aware medical image denoising. Code is available at: xxx.
</details>
<details>
<summary>摘要</summary>
As PET imaging is accompanied by substantial radiation exposure and cancer risk, reducing radiation dose in PET scans is an important topic. Recently, diffusion models have emerged as the new state-of-the-art generative model to generate high-quality samples and have demonstrated strong potential for various tasks in medical imaging. However, it is difficult to extend diffusion models for 3D image reconstructions due to the memory burden. Directly stacking 2D slices together to create 3D image volumes would results in severe inconsistencies between slices. Previous works tried to either applying a penalty term along the z-axis to remove inconsistencies or reconstructing the 3D image volumes with 2 pre-trained perpendicular 2D diffusion models. Nonetheless, these previous methods failed to produce satisfactory results in challenging cases for PET image denoising. In addition to administered dose, the noise-levels in PET images are affected by several other factors in clinical settings, such as scan time, patient size, and weight, etc. Therefore, a method to simultaneously denoise PET images with different noise-levels is needed. Here, we proposed a dose-aware diffusion model for 3D low-dose PET imaging (DDPET) to address these challenges. The proposed DDPET method was tested on 295 patients from three different medical institutions globally with different low-dose levels. These patient data were acquired on three different commercial PET scanners, including Siemens Vision Quadra, Siemens mCT, and United Imaging Healthcare uExplorere. The proposed method demonstrated superior performance over previously proposed diffusion models for 3D imaging problems as well as models proposed for noise-aware medical image denoising. Code is available at: xxx.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/eess.IV_2023_11_07/" data-id="closbrozb019h0g88hh6m6x87" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/07/eess.SP_2023_11_07/" class="article-date">
  <time datetime="2023-11-07T08:00:00.000Z" itemprop="datePublished">2023-11-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/07/eess.SP_2023_11_07/">eess.SP - 2023-11-07</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NEAT-MUSIC-Auto-calibration-of-DOA-Estimation-for-Terahertz-Band-Massive-MIMO-Systems"><a href="#NEAT-MUSIC-Auto-calibration-of-DOA-Estimation-for-Terahertz-Band-Massive-MIMO-Systems" class="headerlink" title="NEAT-MUSIC: Auto-calibration of DOA Estimation for Terahertz-Band Massive MIMO Systems"></a>NEAT-MUSIC: Auto-calibration of DOA Estimation for Terahertz-Band Massive MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04322">http://arxiv.org/abs/2311.04322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmet M. Elbir, Abdulkadir Celik, Ahmed M. Eltawil</li>
<li>for: 本文探讨了在teraHertz（THz）频段中的方向来源估计问题，具体来说是在 gain-phase 偏差和 beam-squint 的存在下进行精度的方向来源估计。</li>
<li>methods: 本文提出了一种自动准备方法，namely NoisE subspAce correcTion technique for MUltiple SIgnal Classification (NEAT-MUSIC)，该方法基于噪声子空间的修正以实现准确的方向来源估计。</li>
<li>results: numerical results show that the proposed approach is effective in estimating the direction of arrival in THz systems with gain-phase mismatches and beam-squint.<details>
<summary>Abstract</summary>
Terahertz (THz) band is envisioned for the future sixth generation wireless systems thanks to its abundant bandwidth and very narrow beamwidth. These features are one of the key enabling factors for high resolution sensing with milli-degree level direction-of-arrival (DOA) estimation. Therefore, this paper investigates the DOA estimation problem in THz systems in the presence of two major error sources: 1) gain-phase mismatches, which occur due to the deviations in the radio-frequency circuitry; 2) beam-squint, which is caused because of the deviations in the generated beams at different subcarriers due to ultra-wide bandwidth. An auto-calibration approach, namely NoisE subspAce correcTion technique for MUltiple SIgnal Classification (NEAT-MUSIC), is proposed based on the correction of the noise subspace for accurate DOA estimation in the presence of gain-phase mismatches and beam-squint. To gauge the performance of the proposed approach, the Cramer-Rao bounds are also derived. Numerical results show the effectiveness of the proposed approach.
</details>
<details>
<summary>摘要</summary>
特拉赫频（THz）频段被看作未来第六代无线系统的未来，因为它具有庞大的频率带宽和非常窄的扫描方向宽度。这些特点是高分辨率感知的关键因素，其中一个重要的问题是方向探测（DOA）估计。因此，这篇论文研究了THz系统中DOA估计问题在两种主要错误源的存在下：1）功率偏移，这些偏移是由无线电路中的偏移引起的；2）扫描方向偏移，这是由不同Subcarrier生成的扫描方向偏移引起的。为解决这些问题，我们提出了一种自动校准方法，即NoisE subspAce correcTion technique for MUltiple SIgnal Classification（NEAT-MUSIC）。通过对噪声空间进行 corrections，实现了精确的DOA估计。为评估提posed方法的性能，我们也 deriv了克拉默-罗曼 bounds。numerical results表明方法的效果。
</details></li>
</ul>
<hr>
<h2 id="High-performance-Power-Allocation-Strategies-for-Active-IRS-aided-Wireless-Network"><a href="#High-performance-Power-Allocation-Strategies-for-Active-IRS-aided-Wireless-Network" class="headerlink" title="High-performance Power Allocation Strategies for Active IRS-aided Wireless Network"></a>High-performance Power Allocation Strategies for Active IRS-aided Wireless Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04032">http://arxiv.org/abs/2311.04032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhao, Xuehui Wang, Yan Wang, Xianpeng Wang, Zhilin Chen, Feng Shu, Jiangzhou Wang</li>
<li>for: 提高active intelligent reflective surface（IRS）的环境 friendly性和灵活性，解决现有PA策略与优化极小搜索（ES）之间的差距。</li>
<li>methods: 基于power allocation（PA）的equal-spacing-multiple-point-initialization gradient ascent（ESMPI-GA）方法和low-complexity closed-form PA方法with third-order Taylor expansion（TTE）。</li>
<li>results: ESMPI-GA可以提高约0.5比特的性能，而TTE可以减少复杂度并且与TPA和固定PA策略相比表现更好。<details>
<summary>Abstract</summary>
Due to its intrinsic ability to combat the double fading effect, the active intelligent reflective surface (IRS) becomes popular. The main feature of active IRS must be supplied by power, and the problem of how to allocate the total power between base station (BS) and IRS to fully explore the rate gain achieved by power allocation (PA) to remove the rate gap between existing PA strategies and optimal exhaustive search (ES) arises naturally. First, the signal-to-noise ratio (SNR) expression is derived to be a function of PA factor beta [0, 1]. Then, to improve the rate performance of the conventional gradient ascent (GA), an equal-spacing-multiple-point-initialization GA (ESMPI-GA) method is proposed. Due to its slow linear convergence from iterative GA, the proposed ESMPI-GA is high-complexity. Eventually, to reduce this high complexity, a low-complexity closed-form PA method with third-order Taylor expansion (TTE) centered at point beta0 = 0.5 is proposed. Simulation results show that the proposed ESMPI-GA harvests about 0.5 bit gain over conventional GA and 1.2 and 0.8 bits gain over existing methods like equal PA and Taylor polynomial approximation (TPA) for small-scale IRS, and the proposed TTE performs much better than TPA and fixed PA strategies using an extremely low complexity.
</details>
<details>
<summary>摘要</summary>
由于其内置的对双抑逊效应斗争能力，活动智能反射表面（IRS）变得受欢迎。主要特点 OF active IRS 需要电力供应，因此如何将总电力分配给基站（BS）和 IRS，以全面挖掘PA Strategies 中的率增加而 remove 现有PA Strategies 和优化极限搜索（ES）之间的率差，成为一个自然的问题。首先，将信号响应率（SNR）表达式 derivation 为 beta [0, 1] 的函数。然后，为了提高传统的梯度升级（GA）的率性能，一种等间距多点初始化 GA 方法（ESMPI-GA）被提出。由于其的慢速线性收敛，提议的 ESMPI-GA 高复杂。最后，为了减少这种高复杂性，一种低复杂度的关闭式PA方法 WITH third-order Taylor expansion（TTE）中心在点 beta0 = 0.5 被提出。实验结果表明，提议的 ESMPI-GA 可以受到约0.5 bit的提升，与传统的 GA 相比，并且与现有的方法如平等PA和 Taylor 多项式预测（TPA）在小规模 IRS 上受到1.2和0.8 bits的提升。同时，提议的 TTE 表现远胜 TPA 和固定PA策略，并且具有极低的复杂度。
</details></li>
</ul>
<hr>
<h2 id="Memory-AMP-for-Generalized-MIMO-Coding-Principle-and-Information-Theoretic-Optimality"><a href="#Memory-AMP-for-Generalized-MIMO-Coding-Principle-and-Information-Theoretic-Optimality" class="headerlink" title="Memory AMP for Generalized MIMO: Coding Principle and Information-Theoretic Optimality"></a>Memory AMP for Generalized MIMO: Coding Principle and Information-Theoretic Optimality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04012">http://arxiv.org/abs/2311.04012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Chen, Lei Liu, Yuhao Chi, Ying Li, Zhaoyang Zhang</li>
<li>for: 本研究旨在解决下一代无线通信中复杂的通信场景下，一种通用的多Input多Output（MIMO）系统的接收器设计问题。</li>
<li>methods: 本研究使用了orthogonal&#x2F;vector approximate message passing（OAMP&#x2F;VAMP）接收器和low-complexity memory approximate message passing（MAMP）接收器，并研究了它们的信息理论优化和可编码原理。</li>
<li>results: 研究结果表明，MAMP接收器可以在实际场景下实现信息理论优化，并且与使用OAMP&#x2F;VAMP接收器相比，它的计算复杂度可以降低至0.4%。同时，研究还发现了一种简单的单输入单输出变分 State Evolution（VSE）方法，可以准确地分析MAMP接收器的可达率。<details>
<summary>Abstract</summary>
To support complex communication scenarios in next-generation wireless communications, this paper focuses on a generalized MIMO (GMIMO) with practical assumptions, such as massive antennas, practical channel coding, arbitrary input distributions, and general right-unitarily-invariant channel matrices (covering Rayleigh fading, certain ill-conditioned and correlated channel matrices). The orthogonal/vector approximate message passing (OAMP/VAMP) receiver has been proved to be information-theoretically optimal in GMIMO, but it is limited to high-complexity LMMSE. To solve this problem, a low-complexity memory approximate message passing (MAMP) receiver has recently been shown to be Bayes optimal but limited to uncoded systems. Therefore, how to design a low-complexity and information-theoretically optimal receiver for GMIMO is still an open issue. To address this issue, this paper proposes an information-theoretically optimal MAMP receiver and investigates its achievable rate analysis and optimal coding principle. Specifically, due to the long-memory linear detection, state evolution (SE) for MAMP is intricately multidimensional and cannot be used directly to analyze its achievable rate. To avoid this difficulty, a simplified single-input single-output variational SE (VSE) for MAMP is developed by leveraging the SE fixed-point consistent property of MAMP and OAMP/VAMP. The achievable rate of MAMP is calculated using the VSE, and the optimal coding principle is established to maximize the achievable rate. On this basis, the information-theoretic optimality of MAMP is proved rigorously. Numerical results show that the finite-length performances of MAMP with practical optimized LDPC codes are 0.5-2.7 dB away from the associated constrained capacities. It is worth noting that MAMP can achieve the same performances as OAMP/VAMP with 0.4% of the time consumption for large-scale systems.
</details>
<details>
<summary>摘要</summary>
本文关注下一代无线通信中复杂的通信场景，而将总线MIMO（GMIMO）作为研究对象，并假设了大量天线、实用的通道编码、任意输入分布和一般右乘偏护 matrices（包括抖波投射和一些不良conditioned和相关的通道矩阵）。 orthogonal/vector approximate message passing（OAMP/VAMP）接收器已经证明为GMIMO的信息论上最优，但它具有高复杂度LMMSE的限制。为解决这个问题，一个低复杂度的内存approximate message passing（MAMP）接收器最近被证明为 Bayes 优，但它只适用于无编码系统。因此，如何设计一个低复杂度并且信息论上最优的GMIMO接收器仍然是一个开放的问题。为解决这个问题，本文提出了一个信息论上最优的MAMP接收器，并对其可 achievable rate 分析和优化编码原理进行了研究。具体来说，由于长期线性探测，MAMP 的状态演化（SE）是多维度的，不能直接使用。为解决这个困难，本文提出了一种简化的单输入单出变量 SE（VSE），通过利用MAMP和OAMP/VAMP的SE固定点一致性来避免这种困难。通过 VSE，MAMP 的可 achievable rate 可以得到计算，并且可以确定最优的编码原理来最大化可 achievable rate。在这基础上，本文证明了MAMP 的信息论上最优性。numerical results 表明，MAMP 使用了实用优化 LDPC 编码在finite length 下的性能与相关的受限容量之间的差异在0.5-2.7 dB之间。值得注意的是，MAMP 可以在大规模系统中实现与 OAMP/VAMP 相同的性能，但它的计算时间只占 OAMP/VAMP 的0.4%。
</details></li>
</ul>
<hr>
<h2 id="Coverage-Hole-Elimination-System-in-Industrial-Environment"><a href="#Coverage-Hole-Elimination-System-in-Industrial-Environment" class="headerlink" title="Coverage Hole Elimination System in Industrial Environment"></a>Coverage Hole Elimination System in Industrial Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04011">http://arxiv.org/abs/2311.04011</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mervat Zarour, Shreya Tayade, Sergiy Melnyk, Hans D. Schotten</li>
<li>for: 本研究提出了一个框架，以避免室内设备环境中的覆盖洞。</li>
<li>methods: 本研究使用了支持向量机（SVM）分类模型来决定覆盖洞的位置，并以 binary coverage hole map 构建了AGV的路径重新规划，以避免覆盖洞。</li>
<li>results: 研究结果显示，如果在AGV的路径重新规划之前已知覆盖洞的位置，则AGV的重新规划路径可以更短且更优化。<details>
<summary>Abstract</summary>
The paper proposes a framework to identify and avoid the coverage hole in an indoor industry environment. We assume an edge cloud co-located controller that followers the Automated Guided Vehicle (AGV) movement on a factory floor over a wireless channel. The coverage holes are caused due to blockage, path-loss, and fading effects. An AGV in the coverage hole may lose connectivity to the edge-cloud and become unstable. To avoid connectivity loss, we proposed a framework that identifies the position of coverage hole using a Support- Vector Machine (SVM) classifier model and constructs a binary coverage hole map incorporating the AGV trajectory re-planning to avoid the identified coverage hole. The AGV's re-planned trajectory is optimized and selected to avoid coverage hole the shortest coverage-hole-free trajectory. We further investigated the look-ahead time's impact on the AGV's re-planned trajectory performance. The results reveal that an AGV's re-planned trajectory can be shorter and further optimized if the coverage hole position is known ahead of time
</details>
<details>
<summary>摘要</summary>
文章提出了一种框架，用于 indentify 和避免indoor环境中的覆盖洞。我们假设了一个与自动导向车（AGV）运动相关的边缘云控制器，通过无线通信在factory floor上跟踪AGV的运动。覆盖洞的原因包括堵塞、距离损失和折射效应。AGV在覆盖洞中可能会失去与边缘云的连接，导致不稳定。为了避免连接损失，我们提出了一种框架，使用支持向量机（SVM）分类器模型标识覆盖洞的位置，并基于AGV的轨迹重新规划以避免已知的覆盖洞。我们进一步调查了look-ahead时间对AGV的重新规划轨迹性能的影响。结果显示，如果知道覆盖洞的位置 ahead of time，AGV的重新规划轨迹可以更短并更优化。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-via-Active-RIS-Assisted-Over-the-Air-Computation"><a href="#Federated-Learning-via-Active-RIS-Assisted-Over-the-Air-Computation" class="headerlink" title="Federated Learning via Active RIS Assisted Over-the-Air Computation"></a>Federated Learning via Active RIS Assisted Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03982">http://arxiv.org/abs/2311.03982</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deyou Zhang, Ming Xiao, Mikael Skoglund, H. Vincent Poor</li>
<li>for: 本研究旨在透过活动可重配置知识表面（RIS）支持无线计算（AirComp）启用的联合学习（FL）系统中可靠的梯度聚合。</li>
<li>methods: 本文使用了可重配置知识表面（RIS）来减少梯度聚合错误，并提出了一个优化问题，以jointly优化通信设计和RIS配置，以最小化梯度聚合错误。</li>
<li>results:  simulationsresults显示，使用活动RIS可以比其静态counterpart减少梯度聚合错误。<details>
<summary>Abstract</summary>
In this paper, we propose leveraging the active reconfigurable intelligence surface (RIS) to support reliable gradient aggregation for over-the-air computation (AirComp) enabled federated learning (FL) systems. An analysis of the FL convergence property reveals that minimizing gradient aggregation errors in each training round is crucial for narrowing the convergence gap. As such, we formulate an optimization problem, aiming to minimize these errors by jointly optimizing the transceiver design and RIS configuration. To handle the formulated highly non-convex problem, we devise a two-layer alternative optimization framework to decompose it into several convex subproblems, each solvable optimally. Simulation results demonstrate the superiority of the active RIS in reducing gradient aggregation errors compared to its passive counterpart.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了利用活动可重配置智能表面（RIS）来支持静止梯度聚合的远程计算（AirComp）启用的联合学习（FL）系统。一种分析联合学习融合性质表明，在每次训练轮中最小化梯度聚合错误是关键性能指标。因此，我们对各个训练轮梯度聚合错误进行优化，并将其与传输器设计和RIS配置进行共同优化。为处理复杂非 convex 问题，我们提出了一个两层替代优化框架，将问题分解为多个可优化的栅格问题，每个问题都可以优化到最优解。实验结果表明，活动RIS可以在聚合梯度错误方面减少较大的改善，相比于其pasive counterpart。
</details></li>
</ul>
<hr>
<h2 id="NOMA-Enabled-Multi-Access-Edge-Computing-A-Joint-MU-MIMO-Precoding-and-Computation-Offloading-Design"><a href="#NOMA-Enabled-Multi-Access-Edge-Computing-A-Joint-MU-MIMO-Precoding-and-Computation-Offloading-Design" class="headerlink" title="NOMA Enabled Multi-Access Edge Computing: A Joint MU-MIMO Precoding and Computation Offloading Design"></a>NOMA Enabled Multi-Access Edge Computing: A Joint MU-MIMO Precoding and Computation Offloading Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03974">http://arxiv.org/abs/2311.03974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deyou Zhang, Meng Wang, Shuo Shi, Ming Xiao</li>
<li>for: 这个研究探讨了多元缘点 computing（MEC）中多个用户（MU）通过非正交多Access方式访问MEC服务器，以减少所有MU的能源消耗，同时保证对时延迟的确保。</li>
<li>methods: 本研究使用了分解这个问题为三个子问题，并逐步解决这些子问题，直到收敛。</li>
<li>results: 实验结果显示了提案方法的有效性和优化性，并与基准 алгоритми进行比较。<details>
<summary>Abstract</summary>
This letter investigates computation offloading and transmit precoding co-design for multi-access edge computing (MEC), where multiple MEC users (MUs) equipped with multiple antennas access the MEC server in a non-orthogonal multiple access manner. We aim to minimize the total energy consumption of all MUs while satisfying the latency constraints by jointly optimizing the computational frequency, offloading ratio, and precoding matrix of each MU. For tractability, we first decompose the original problem into three subproblems and then solve these subproblems iteratively until convergence. Simulation results validate the convergence of the proposed method and demonstrate its superiority over baseline algorithms.
</details>
<details>
<summary>摘要</summary>
这封信函数探讨了多ccess edge computing（MEC）中多个用户（MU） equiped with multiple antennas 访问 MEC 服务器的非正交多access方式下的计算卸载和传输预编码合理设计。我们的目标是最小化所有MU的总能耗，同时满足延迟约束，通过对每个MU的计算频率、卸载比例和预编码矩阵进行共同优化。为了可读性，我们先将原问题分解为三个互相独立的互问题，然后逐步解决这些互问题 until convergence。实验结果证明了我们的方法的可靠性和对基线算法的优势。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Parameter-Estimation-with-Gaussian-Observation-Noises-in-Time-varying-Digraphs"><a href="#Distributed-Parameter-Estimation-with-Gaussian-Observation-Noises-in-Time-varying-Digraphs" class="headerlink" title="Distributed Parameter Estimation with Gaussian Observation Noises in Time-varying Digraphs"></a>Distributed Parameter Estimation with Gaussian Observation Noises in Time-varying Digraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03911">http://arxiv.org/abs/2311.03911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Yan, Hideaki Ishii</li>
<li>for: 这个论文关注的问题是分布式参数估计在感知网络中。每个感知器在接下来的观测中都会观测一个未知的 $d$-维参数，这可能会受到 Gaussian 随机噪声的影响。感知器们想使用协作来估计真实的参数值。</li>
<li>methods: 我们首先将动态扩展和混合（DREM）算法推广到随机系统中，将参数估计问题转化为 $d$ 个整数问题：一个为每个未知参数。然后，我们给每个整数问题提供了 combine-then-adapt（CTA）和 adapt-then-combine（ATC）扩散基于估计算算法，每个感知器在其内部约束中进行 combination 步骤，并在流动观测中进行 adaptation 步骤。</li>
<li>results: 我们显示，在网络结构和刺激因子的弱限制下，我们提出的估计器保证每个感知器都可以正确估计参数，即使任何个体无法估计。具体来说，需要的是网络上的 union 上一个时间间隔的强连接。此外，感知器们必须共同满足一个协作持续刺激（PE）条件，这些条件放松传统的 PE 条件。数字示例最终提供了确认结果。<details>
<summary>Abstract</summary>
In this paper, we consider the problem of distributed parameter estimation in sensor networks. Each sensor makes successive observations of an unknown $d$-dimensional parameter, which might be subject to Gaussian random noises. The sensors aim to infer the true value of the unknown parameter by cooperating with each other. To this end, we first generalize the so-called dynamic regressor extension and mixing (DREM) algorithm to stochastic systems, with which the problem of estimating a $d$-dimensional vector parameter is transformed to that of $d$ scalar ones: one for each of the unknown parameters. For each of the scalar problem, both combine-then-adapt (CTA) and adapt-then-combine (ATC) diffusion-based estimation algorithms are given, where each sensor performs a combination step to fuse the local estimates in its in-neighborhood, alongside an adaptation step to process its streaming observations. Under weak conditions on network topology and excitation of regressors, we show that the proposed estimators guarantee that each sensor infers the true parameter, even if any individual of them cannot by itself. Specifically, it is required that the union of topologies over an interval with fixed length is strongly connected. Moreover, the sensors must collectively satisfy a cooperative persistent excitation (PE) condition, which relaxes the traditional PE condition. Numerical examples are finally provided to illustrate the established results.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了分布式参数估计问题在感知网络中。每个感知器在不同时刻 Successively 观测一个未知的 $d$-维参数，这个参数可能受到 Gaussian 随机噪声的影响。感知器们想要推断真实的参数值，因此它们需要合作。为此，我们首先将动态扩展和混合（DREM）算法推广到随机系统中，将参数估计问题转化为 $d$ 个整数估计问题：一个 для每个未知参数。每个整数估计问题中，我们提供了 combine-then-adapt（CTA）和 adapt-then-combine（ATC）扩散式估计算法，每个感知器在其各自的邻域中进行了组合步骤，并在流动观测中进行了适应步骤。在弱网络结构和刺激因子的假设下，我们证明了我们的估计算法，使得每个感知器都能够推断真实的参数，即使任何个体无法单独做出正确的估计。具体来说，需要的是union of topologies over an interval with fixed length是强连接的。此外，感知器们必须共同满足一个合作持续刺激（PE）条件，这个条件relaxsthe traditional PE condition。最后，我们提供了一些数值示例，以证明我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Integrated-Sensing-Communication-and-Computing-for-Cost-effective-Multimodal-Federated-Perception"><a href="#Integrated-Sensing-Communication-and-Computing-for-Cost-effective-Multimodal-Federated-Perception" class="headerlink" title="Integrated Sensing, Communication, and Computing for Cost-effective Multimodal Federated Perception"></a>Integrated Sensing, Communication, and Computing for Cost-effective Multimodal Federated Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03815">http://arxiv.org/abs/2311.03815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ning Chen, Zhipeng Cheng, Xuwei Fan, Bangzhen Huang, Yifeng Zhao, Lianfen Huang, Xiaojiang Du, Mohsen Guizani</li>
<li>for: 本研究旨在解决多模态联邦感知（MFP）服务网络中的资源调度问题，以提高学习性和降低资源成本。</li>
<li>methods: 本研究使用服务 ориентирован的资源管理（ISCC）和激励机制，将资源调度问题定义为社会福祉最大化问题，以提高学习性和降低资源成本。</li>
<li>results: 实验结果表明，提出的资源调度机制有效和可靠，可以提高学习性和降低资源成本。<details>
<summary>Abstract</summary>
Federated learning (FL) is a classic paradigm of 6G edge intelligence (EI), which alleviates privacy leaks and high communication pressure caused by traditional centralized data processing in the artificial intelligence of things (AIoT). The implementation of multimodal federated perception (MFP) services involves three sub-processes, including sensing-based multimodal data generation, communication-based model transmission, and computing-based model training, ultimately relying on available underlying multi-domain physical resources such as time, frequency, and computing power. How to reasonably coordinate the multi-domain resources scheduling among sensing, communication, and computing, therefore, is crucial to the MFP networks. To address the above issues, this paper investigates service-oriented resource management with integrated sensing, communication, and computing (ISCC). With the incentive mechanism of the MFP service market, the resources management problem is redefined as a social welfare maximization problem, where the idea of "expanding resources" and "reducing costs" is used to improve learning performance gain and reduce resource costs. Experimental results demonstrate the effectiveness and robustness of the proposed resource scheduling mechanisms.
</details>
<details>
<summary>摘要</summary>
六代edge智能（EI）中的联邦学习（FL）是一种经典的概念，它解决了传统中央数据处理所导致的隐私泄露和高通信压力问题。实施多Modal联邦感知（MFP）服务需要三个子进程，包括感知数据生成、通信模型传输和计算模型训练，最终依靠可用的多域物理资源，如时间、频率和计算能力。因此，合理协调多域资源分配成为MFP网络的关键问题。为解决这些问题，本文研究了整合感知、通信和计算（ISCC）的服务 oriented 资源管理策略。通过MFP服务市场的奖励机制，资源管理问题被重新定义为社会福祉最大化问题，以提高学习性能和降低资源成本。实验结果表明提出的资源调度机制的效iveness和稳定性。
</details></li>
</ul>
<hr>
<h2 id="Textile-based-conformable-and-breathable-ultrasound-imaging-probe"><a href="#Textile-based-conformable-and-breathable-ultrasound-imaging-probe" class="headerlink" title="Textile-based conformable and breathable ultrasound imaging probe"></a>Textile-based conformable and breathable ultrasound imaging probe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03787">http://arxiv.org/abs/2311.03787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takumi Noda, Seiichi Takamatsu, Michitaka Yamamoto, Naoto Tomita, Toshihiro Itoh, Takashi Azuma, Ichiro Sakuma, Naoki Tomii</li>
<li>for: 这种研究旨在开发一种可适应和呼吸性的超声波成像探测器，用于早期检测疾病。</li>
<li>methods: 研究人员通过将超声波元件间隔两个织物压缩件，并在电极部分填充铜材料，使得超声波波长可以充分penetrate织物，同时保持织物的呼吸性。</li>
<li>results: 实验结果显示， fabricated textile-based probe 具有低的柔性（0.066 × 10^-4 N·m^2&#x2F;m）和高的呼吸性（11.7 cm^3&#x2F;cm^2·s），并能够成功地检测人 neck 区域的血管舒张和内 jugular vein  diameter 的变化，从而早期发现了arteriosclerosis 和肿瘤。<details>
<summary>Abstract</summary>
Daily monitoring of internal tissues with conformable and breathable ultrasound (US) imaging probes is promising for early detection of diseases. In recent years, textile substrates are widely used for wearable devices since they satisfy both conformability and breathability. However, it is not currently possible to use textile substrates for US probes due to the reflection or attenuation of US waves at the air gaps in the textiles. In this paper, we fabricated a conformable and breathable US imaging probe by sandwiching the US elements between two woven polyester textiles on which copper electrodes were formed through electroless plating. The air gaps between the fibers at the electrode parts were filled with copper, allowing for high penetration of US waves. On the other hand, the non-electrode parts retain air gaps, leading to high breathability. The fabricated textile-based probe showed low flexural rigidity ($0.066 \times 10^{-4} N \cdot m^2/m$) and high air permeability ($11.7 cm^3 / cm^2 \cdot s$). Human neck imaging demonstrated the ability of the probe to monitor the pulsation of the common carotid artery and change in the internal jugular vein diameter, which lead to the early detection of health issues such as arteriosclerosis and dehydration.
</details>
<details>
<summary>摘要</summary>
每天监测内部组织的可适应和呼吸性超声（US）成像探测器是潜在的早期疾病检测的希望。在过去几年中，织物基aterials被广泛使用于可穿戴设备中，因为它们满足了可适应性和呼吸性。然而，当前不可以使用织物基aterials来制造US探测器，因为US波在织物中的反射或吸收。在这篇论文中，我们制造了一种可适应和呼吸性的US成像探测器，通过将US元素置于两块织物上，并在这些织物上形成了氧化镁电极。在电极部分，织物中的空隙被填充了镁，以确保US波的高入射。然而，非电极部分保留了空隙，使得呼吸性高。我们制造的织物基US探测器表现了低的柔性($0.066 \times 10^{-4} N \cdot m^2/m$)和高的空气通透性($11.7 cm^3 / cm^2 \cdot s$）。在人脖部成像中，探测器能够监测通uilinary carotid artery的脉搏和内 Jugular vein的径向变化，从而早期发现了健康问题，如arteriosclerosis和肿胀。
</details></li>
</ul>
<hr>
<h2 id="Multi-Beam-Forming-with-Movable-Antenna-Array"><a href="#Multi-Beam-Forming-with-Movable-Antenna-Array" class="headerlink" title="Multi-Beam Forming with Movable-Antenna Array"></a>Multi-Beam Forming with Movable-Antenna Array</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03775">http://arxiv.org/abs/2311.03775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyan Ma, Lipeng Zhu, Rui Zhang</li>
<li>for: 提高多束发射的能力和降低干扰的能力</li>
<li>methods: 利用antenna位置优化和antenna加Weight优化提高多束发射的能量</li>
<li>results: 相比传统Fixed-Position Antenna（FPA）阵列，提高多束发射的能量和降低干扰的能量<details>
<summary>Abstract</summary>
Conventional multi-beam forming with fixed-position antenna (FPA) arrays needs to trade-off between maximizing the beamforming gain over desired directions and minimizing the interference power over undesired directions. In this letter, we study the enhanced multi-beam forming with a linear movable-antenna (MA) array by exploiting the new degrees of freedom (DoFs) via antennas' position optimization. Specifically, we jointly optimize the antenna position vector (APV) and antenna weight vector (AWV) to maximize the minimum beamforming gain over multiple desired directions, subject to a given constraint on the maximum interference power over undesired directions. We propose an efficient alternating optimization algorithm to find a suboptimal solution by iteratively optimizing one of the APV and AWV with the other being fixed. Numerical results show that the proposed multi-beam forming design with MA arrays can significantly outperform that with the traditional FPA arrays and other benchmark schemes in terms of both beamforming gain and interference suppression.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classification-of-Various-Types-of-Damages-in-Honeycomb-Composite-Sandwich-Structures-using-Guided-Wave-Structural-Health-Monitoring"><a href="#Classification-of-Various-Types-of-Damages-in-Honeycomb-Composite-Sandwich-Structures-using-Guided-Wave-Structural-Health-Monitoring" class="headerlink" title="Classification of Various Types of Damages in Honeycomb Composite Sandwich Structures using Guided Wave Structural Health Monitoring"></a>Classification of Various Types of Damages in Honeycomb Composite Sandwich Structures using Guided Wave Structural Health Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03765">http://arxiv.org/abs/2311.03765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shrutisawant099/damage-classification-using-feature-engineering">https://github.com/shrutisawant099/damage-classification-using-feature-engineering</a></li>
<li>paper_authors: Shruti Sawant, Jeslin Thalapil, Siddharth Tallur, Sauvik Banerjee, Amit Sethi</li>
<li>for: 本研究旨在为HCSS中的损害分类提供一种新的方法，以便根据不同类型的损害进行诊断和修复。</li>
<li>methods: 本研究使用了精心的特征工程和机器学习算法，可以正确地将四种不同类型的损害分类为不同的类别。</li>
<li>results: 研究发现，两种损害类型在GW信号上的影响特征非常相似，并且在不同的损害大小下也有较高的准确率（77.89%）。特征计算使用基线信号更为有效。<details>
<summary>Abstract</summary>
Classification of damages in honeycomb composite sandwich structure (HCSS) is important to decide remedial actions. However, previous studies have only detected damages using deviations of monitoring signal from healthy (baseline) using a guided wave (GW) based structural health monitoring system. Classification between various types of damages has not been reported for challenging cases. We show that using careful feature engineering and machine learning it is possible to classify between various types of damages such as core crush (CC), high density core (HDC), lost film adhesive (LFA) and teflon release film (TRF). We believe that we are the first to report numerical models for four types of damages in HCSS, which is followed up with experimental validation. We found that two out of four damages affect the GW signal in a particularly similar manner. We extracted and evaluated multiple features from time as well as frequency domains, and also experimented with features relative to as baseline as well as those that were baseline-free. Using Pearson's correlation coefficient based filtering, redundant features were eliminated. Finally, using an optimal feature set determined using feature elimination, high accuracy was achieved with a random forest classifier on held-out signals. For evaluating performance of the proposed method for different damage sizes, we used simulated data obtained from extensive parametric studies and got an accuracy of 77.89%. Interpretability studies to determine importance of various features showed that features computed using the baseline signal prove more effective as compared to baseline-free features.
</details>
<details>
<summary>摘要</summary>
HCSS（叶状复合材料叠层结构）的损害分类是至关重要，以确定维修措施。然而，先前的研究仅通过帮助波（GW）基于结构健康监测系统中的偏差来检测损害。对于复杂的情况，没有进行过分类。我们表明，通过细心的特征工程和机器学习，可以分类不同类型的损害，包括核心压缩（CC）、高密度核心（HDC）、失去质感黏合剂（LFA）和聚四氟乙烯释放膜（TRF）。我们认为我们是首次对HCSS中四种损害的数学模型进行了数值模拟，并对此进行了实验验证。我们发现，四种损害中的两种具有类似的影响于GW信号。我们从时域和频域中提取了多种特征，并试用了基eline和无基eline两种方法。使用基eline为参照的筛选方法，过滤了 redundant 特征。最后，使用最佳特征集，使用 random forest 分类器在封闭信号上达到了高精度。为评估提案方法对不同损害大小的性能，我们使用了从广泛的参数研究中获得的数字 simulate 数据，并获得了77.89%的准确率。解释性研究表明，基eline信号上计算的特征更有效，相比于无基eline特征。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Traditional-Beamforming-Singular-Vector-Projection-Techniques-for-MU-MIMO-Interference-Management"><a href="#Beyond-Traditional-Beamforming-Singular-Vector-Projection-Techniques-for-MU-MIMO-Interference-Management" class="headerlink" title="Beyond Traditional Beamforming: Singular Vector Projection Techniques for MU-MIMO Interference Management"></a>Beyond Traditional Beamforming: Singular Vector Projection Techniques for MU-MIMO Interference Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03741">http://arxiv.org/abs/2311.03741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Saheed Ullah, Rafid Umayer Murshed, Md. Forkan Uddin</li>
<li>for: 这篇论文旨在开发低复杂度的扩频干扰算法，以减少多用户多入出力系统（MU-MIMO）中的交叉用户干扰，提高spectral efficiency（SE）。</li>
<li>methods: 论文首先提出了一个Singular-Vector Beamspace Search（SVBS）算法，其中所有的特征向量都被评估，以选择最有效的干扰方案。然后，论文提出了一个 математиical proof，证明了MU-MIMO干扰系统中的总交叉用户干扰可以由特征向量之间的共轭 проек数值有效地计算。</li>
<li>results: 数据分析结果显示，SVBS算法比现有的算法更有效，而Interference-optimized Singular Vector Beamforming（IOSVB）算法可以实现near-identical SE，而Dimensionality-reduced IOSVB（DR-IOSVB）算法可以对性能和计算复杂度做出折衔。这篇论文成功地提出了一个高性能且低复杂度的扩频干扰算法，成为MU-MIMO无线通信系统中的新benchmark。<details>
<summary>Abstract</summary>
This paper introduces low-complexity beamforming algorithms for multi-user multiple-input multiple-output (MU-MIMO) systems to minimize inter-user interference and enhance spectral efficiency (SE). A Singular-Vector Beamspace Search (SVBS) algorithm is initially presented, wherein all the singular vectors are assessed to determine the most effective beamforming scheme. We then establish a mathematical proof demonstrating that the total inter-user interference of a MU-MIMO beamforming system can be efficiently calculated from the mutual projections of orthonormal singular vectors. Capitalizing on this, we present an Interference-optimized Singular Vector Beamforming (IOSVB) algorithm for optimal singular vector selection. For further reducing the computational burden, we propose a Dimensionality-reduced IOSVB (DR-IOSVB) algorithm by integrating the principal component analysis (PCA). The numerical results demonstrate the superiority of the SVBS algorithm over the existing algorithms, with the IOSVB offering near-identical SE and the DR-IOSVB balancing the performance and computational efficiency. This work establishes a new benchmark for high-performance and low-complexity beamforming in MU-MIMO wireless communication systems.
</details>
<details>
<summary>摘要</summary>
To further reduce computational complexity, an Interference-optimized Singular Vector Beamforming (IOSVB) algorithm is proposed for optimal singular vector selection. Additionally, a Dimensionality-reduced IOSVB (DR-IOSVB) algorithm is proposed by integrating principal component analysis (PCA) to reduce the dimensionality of the problem.Numerical results show that the SVBS algorithm outperforms existing algorithms, while the IOSVB and DR-IOSVB algorithms offer near-identical SE with reduced computational complexity. This work establishes a new benchmark for high-performance and low-complexity beamforming in MU-MIMO wireless communication systems.
</details></li>
</ul>
<hr>
<h2 id="Recursive-Filters-as-Linear-Time-Invariant-Systems"><a href="#Recursive-Filters-as-Linear-Time-Invariant-Systems" class="headerlink" title="Recursive Filters as Linear Time-Invariant Systems"></a>Recursive Filters as Linear Time-Invariant Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03676">http://arxiv.org/abs/2311.03676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan H. Manton</li>
<li>for: 这篇论文是关于如何将再循环滤波器视为线性时不变系统（LTI），以便应用 fourier分析等工具。</li>
<li>methods: 论文使用了z变换来将再循环滤波器转换为LTI系统，并讨论了这种转换的基础和优点。</li>
<li>results: 论文介绍了如何使用z变换来找到LTI系统的解，并讨论了解的存在性和域的重要性。<details>
<summary>Abstract</summary>
Recursive filters are treated as linear time-invariant (LTI) systems but they are not: uninitialised, they have an infinite number of outputs for any given input, while if initialised, they are not time-invariant. This short tutorial article explains how and why they can be treated as LTI systems, thereby allowing tools such as Fourier analysis to be applied. It also explains the origin of the z-transform, why the region of convergence is important, and why the z-transform fails to find an infinite number of solutions.
</details>
<details>
<summary>摘要</summary>
linear time-invariant (LTI) 系统，但它们并不是：未初始化，它们有无限多个输出 для任何输入，而如果初始化，它们不是时间不变的。这短文章解释了如何和为什么它们可以被视为 LTI 系统，从而使得 fourier 分析可以应用。它还解释了 z-transform 的起源，区域确定性的重要性，以及 z-transform 为何无法找到无限多个解。Here's a word-for-word translation of the text:linear time-invariant (LTI) 系统，但它们并不是：未初始化，它们有无限多个输出 для任何输入，而如果初始化，它们不是时间不变的。这短文章解释了如何和为什么它们可以被视为 LTI 系统，从而使得 fourier 分析可以应用。它还解释了 z-transform 的起源，区域确定性的重要性，以及 z-transform 为何无法找到无限多个解。
</details></li>
</ul>
<hr>
<h2 id="On-the-Performance-of-LoRa-Empowered-Communication-for-Wireless-Body-Area-Networks"><a href="#On-the-Performance-of-LoRa-Empowered-Communication-for-Wireless-Body-Area-Networks" class="headerlink" title="On the Performance of LoRa Empowered Communication for Wireless Body Area Networks"></a>On the Performance of LoRa Empowered Communication for Wireless Body Area Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03653">http://arxiv.org/abs/2311.03653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minling Zhang, Guofa Cai, Zhiping Xu, Jiguang He, Markku Juntti</li>
<li>for: 这个论文主要研究了基于LoRa技术的无线体验网络（WBAN）的远程physiological status监测方案，以及LoRa链路在面对Rayleigh-lognormal抖抖通道时的性能。</li>
<li>methods: 本论文使用了PHY层和MAC层的方法来研究LoRa系统在WBAN场景下的性能，并提出了一种基于Rayleigh-lognormal抖抖通道和同一个扩展因子干扰的closed-form近似位错率（BEP）表达。</li>
<li>results: 研究结果表明，随着SF的增加和干扰的减少，可以有效地减少阴影效应。此外，对LoRa基于WBAN的MAC协议进行了比较分析，包括纯ALOHAR、时钟ALOHAR和多个 carriersense多access协议。研究发现，equal-interval-based和equal-area-based的分配方案可以提高系统的覆盖率、能效率、吞吐量和系统延迟。<details>
<summary>Abstract</summary>
To remotely monitor the physiological status of the human body, long range (LoRa) communication has been considered as an eminently suitable candidate for wireless body area networks (WBANs). Typically, a Rayleigh-lognormal fading channel is encountered by the LoRa links of the WBAN. In this context, we characterize the performance of the LoRa system in WBAN scenarios with an emphasis on the physical (PHY) layer and medium access control (MAC) layer in the face of Rayleigh-lognormal fading channels and the same spreading factor interference. Specifically, closed-form approximate bit error probability (BEP) expressions are derived for the LoRa system. The results show that increasing the SF and reducing the interference efficiently mitigate the shadowing effects. Moreover, in the quest for the most suitable MAC protocol for LoRa based WBANs, three MAC protocols are critically appraised, namely the pure ALOHA, slotted ALOHA, and carrier-sense multiple access. The coverage probability, energy efficiency, throughput, and system delay of the three MAC protocols are analyzed in Rayleigh-lognormal fading channel. Furthermore, the performance of the equal-interval-based and equal-area-based schemes is analyzed to guide the choice of the SF. Our simulation results confirm the accuracy of the mathematical analysis and provide some useful insights for the future design of LoRa based WBANs.
</details>
<details>
<summary>摘要</summary>
为了远程监测人体生物指标，长距离（LoRa）通信被视为适合无线身体区域网络（WBAN）的优选。通常，LoRa链接会遇到很大的徐达谱折射抑制（Rayleigh-lognormal）折射通道。在这种情况下，我们研究了LoRa系统在WBAN场景下的性能，强调物理层（PHY）层和媒体访问控制层（MAC）层，面临徐达谱折射通道和相同的扩展因子干扰。特别是，我们 deriv了LoRa系统的准确预测性比（BEP）表达式。结果表明，随着SF的增加和干扰的减少，可以有效 mitigate阴影效应。此外，为了选择LoRa基于WBAN的最佳MAC协议，我们critically evaluated three MAC协议，namely pure ALOHA, slotted ALOHA, and carrier-sense multiple access。我们分析了这三种MAC协议在徐达谱折射通道中的覆盖率、能效率、吞吐量和系统延迟。此外，我们还分析了等间隔基和等面积基的 schemes，以指导SF的选择。我们的 simulations results confirm the accuracy of the mathematical analysis and provide some useful insights for the future design of LoRa based WBANs。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/07/eess.SP_2023_11_07/" data-id="closbrp0y01dc0g88d5p7go5s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.SD_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T15:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.SD_2023_11_06/">cs.SD - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Combinatorial-Hodge-Theory-in-Simplicial-Signal-Processing-–-DAFx2023-Lecture-Notes"><a href="#Combinatorial-Hodge-Theory-in-Simplicial-Signal-Processing-–-DAFx2023-Lecture-Notes" class="headerlink" title="Combinatorial Hodge Theory in Simplicial Signal Processing – DAFx2023 Lecture Notes"></a>Combinatorial Hodge Theory in Simplicial Signal Processing – DAFx2023 Lecture Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03469">http://arxiv.org/abs/2311.03469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Essl</li>
<li>for: 本研讨会讲述了Combinatorial Hodge Theory在 simplicial signal processing 中的应用，尤其是在数字音频效果（DAFx）领域。</li>
<li>methods: 本研究使用了Combinatorial Hodge Theory 来分析 simplicial signal processing 中的信号结构，并通过实验证明了其效果。</li>
<li>results: 研究发现，Combinatorial Hodge Theory 可以帮助分析和理解 simplicial signal processing 中的信号结构，并提供了一种新的视角来理解这些信号的性质和特征。<details>
<summary>Abstract</summary>
Lecture notes of a tutorial on Combinatorial Hodge Theory in Simplicial Signal Processing held at international conference for digital audio effects (DAFx-23) in Copenhagen, Denmark.
</details>
<details>
<summary>摘要</summary>
lecture notes of a tutorial on Combinatorial Hodge Theory in Simplicial Signal Processing held at international conference for digital audio effects (DAFx-23) in Copenhagen, Denmark.Translation:DAFx-23国际音频特效会议上的一个教程： combinatorial Hodge theory in simplicial signal processing。
</details></li>
</ul>
<hr>
<h2 id="A-Foundation-Model-for-Music-Informatics"><a href="#A-Foundation-Model-for-Music-Informatics" class="headerlink" title="A Foundation Model for Music Informatics"></a>A Foundation Model for Music Informatics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03318">http://arxiv.org/abs/2311.03318</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minzwon/musicfm">https://github.com/minzwon/musicfm</a></li>
<li>paper_authors: Minz Won, Yun-Ning Hung, Duc Le</li>
<li>for: 本研究探讨了适用于音乐信息学领域的基础模型，这个领域受到标注数据稀缺和泛化问题的限制。</li>
<li>methods: 我们进行了多种基础模型变体的比较研究，检查了关键的决定因素，包括模型架构、tokenization方法、时间分辨率、数据和模型可扩展性。</li>
<li>results: 我们的研究发现，我们的模型在多种音乐信息检索下表现出色，在特定的关键指标上超越了现有模型。这些发现对自动学习在音乐信息学中的理解做出了贡献，并为开发更有效和多样的基础模型提供了道路。我们公开发布了一个预训练的版本我们的模型，以便促进重现和未来研究。<details>
<summary>Abstract</summary>
This paper investigates foundation models tailored for music informatics, a domain currently challenged by the scarcity of labeled data and generalization issues. To this end, we conduct an in-depth comparative study among various foundation model variants, examining key determinants such as model architectures, tokenization methods, temporal resolution, data, and model scalability. This research aims to bridge the existing knowledge gap by elucidating how these individual factors contribute to the success of foundation models in music informatics. Employing a careful evaluation framework, we assess the performance of these models across diverse downstream tasks in music information retrieval, with a particular focus on token-level and sequence-level classification. Our results reveal that our model demonstrates robust performance, surpassing existing models in specific key metrics. These findings contribute to the understanding of self-supervised learning in music informatics and pave the way for developing more effective and versatile foundation models in the field. A pretrained version of our model is publicly available to foster reproducibility and future research.
</details>
<details>
<summary>摘要</summary>
中文翻译：这篇论文研究了针对音乐信息学领域特有的基础模型，该领域面临有限的标注数据和泛化问题。为解决这些问题，我们进行了详细的基础模型变种比较研究，检查了模型结构、字符串化方法、时间分辨率、数据和模型缩放等因素的影响。我们的目标是填补现有知识空白，了解这些因素对音乐信息学中基础模型的成功的贡献。我们采用了严格的评价框架，评估这些模型在音乐信息检索下涉及多个下游任务中的表现，尤其是字符级别和序列级别分类。我们的结果表明，我们的模型在特定的关键指标上达到了稳定的表现，超过了现有模型。这些成果对音乐信息学中自动学习的理解和未来研究做出了贡献。我们还公开提供了我们模型的预训练版本，以便促进可重复性和未来研究。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.SD_2023_11_06/" data-id="closbrovb00ze0g880ewh5f9c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/eess.AS_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T14:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/eess.AS_2023_11_06/">eess.AS - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HRTF-Estimation-in-the-Wild"><a href="#HRTF-Estimation-in-the-Wild" class="headerlink" title="HRTF Estimation in the Wild"></a>HRTF Estimation in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03560">http://arxiv.org/abs/2311.03560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vivek Jayaram, Ira Kemelmacher-Shlizerman, Steven M. Seitz</li>
<li>for: 这个论文旨在创建更真实的听觉 spatial audio 经验，通过采用个性化 HRTF 估算。</li>
<li>methods: 该论文提出了一种基于听觉记录和头部跟踪数据的个性化 HRTF 估算方法，不需要专门的设备或测试。</li>
<li>results: 该研究表明，通过分析不同环境中听觉数据，可以准确地估算个人化 HRTF，并且在虚拟环境中提高声音的地理位置和避免前后混淆。<details>
<summary>Abstract</summary>
Head Related Transfer Functions (HRTFs) play a crucial role in creating immersive spatial audio experiences. However, HRTFs differ significantly from person to person, and traditional methods for estimating personalized HRTFs are expensive, time-consuming, and require specialized equipment. We imagine a world where your personalized HRTF can be determined by capturing data through earbuds in everyday environments. In this paper, we propose a novel approach for deriving personalized HRTFs that only relies on in-the-wild binaural recordings and head tracking data. By analyzing how sounds change as the user rotates their head through different environments with different noise sources, we can accurately estimate their personalized HRTF. Our results show that our predicted HRTFs closely match ground-truth HRTFs measured in an anechoic chamber. Furthermore, listening studies demonstrate that our personalized HRTFs significantly improve sound localization and reduce front-back confusion in virtual environments. Our approach offers an efficient and accessible method for deriving personalized HRTFs and has the potential to greatly improve spatial audio experiences.
</details>
<details>
<summary>摘要</summary>
HEAD-RELATED TRANSFER FUNCTIONS (HRTFs) 是创造充气空间声音体验中的关键因素。然而，人员对HRTF的个性化差异较大，传统方法估计个性化HRTF 昂贵、耗时、需要特殊设备。我们想象一个世界，在日常环境中使用耳机记录数据来确定个性化HRTF。在这篇论文中，我们提出了一种新的方法，只需要在实际环境中采集听觉双耳记录和头部跟踪数据，就可以准确估计个性化HRTF。我们发现，我们预测的HRTF与在静音室中测量的真实HRTF几乎相同。此外，听测研究表明，我们的个性化HRTF可以明显提高虚拟环境中的声音定位和前后混乱减少。我们的方法可以快速、高效地获得个性化HRTF，并且有可能大幅改善空间声音体验。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/eess.AS_2023_11_06/" data-id="closbrowm012u0g88hczv122h" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CV_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T13:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.CV_2023_11_06/">cs.CV - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Toward-Planet-Wide-Traffic-Camera-Calibration"><a href="#Toward-Planet-Wide-Traffic-Camera-Calibration" class="headerlink" title="Toward Planet-Wide Traffic Camera Calibration"></a>Toward Planet-Wide Traffic Camera Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04243">http://arxiv.org/abs/2311.04243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Khiem Vuong, Robert Tamburo, Srinivasa G. Narasimhan</li>
<li>for:  addresses the challenge of calibration for outdoor cameras, which has limited their potential for automated analysis.</li>
<li>methods:  uses street-level imagery to reconstruct a metric 3D model and accurately localize over 100 global traffic cameras, demonstrating a scalable framework.</li>
<li>results:  achieves significant enhancements over existing automatic calibration techniques and enables traffic analysis through 3D vehicle reconstruction and speed measurement.<details>
<summary>Abstract</summary>
Despite the widespread deployment of outdoor cameras, their potential for automated analysis remains largely untapped due, in part, to calibration challenges. The absence of precise camera calibration data, including intrinsic and extrinsic parameters, hinders accurate real-world distance measurements from captured videos. To address this, we present a scalable framework that utilizes street-level imagery to reconstruct a metric 3D model, facilitating precise calibration of in-the-wild traffic cameras. Notably, our framework achieves 3D scene reconstruction and accurate localization of over 100 global traffic cameras and is scalable to any camera with sufficient street-level imagery. For evaluation, we introduce a dataset of 20 fully calibrated traffic cameras, demonstrating our method's significant enhancements over existing automatic calibration techniques. Furthermore, we highlight our approach's utility in traffic analysis by extracting insights via 3D vehicle reconstruction and speed measurement, thereby opening up the potential of using outdoor cameras for automated analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unsupervised-Region-Growing-Network-for-Object-Segmentation-in-Atmospheric-Turbulence"><a href="#Unsupervised-Region-Growing-Network-for-Object-Segmentation-in-Atmospheric-Turbulence" class="headerlink" title="Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence"></a>Unsupervised Region-Growing Network for Object Segmentation in Atmospheric Turbulence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03572">http://arxiv.org/abs/2311.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dehao Qin, Ripon Saha, Suren Jayasuriya, Jinwei Ye, Nianyi Li</li>
<li>for: 这个论文是为了提出一种无监督的前景对象分割网络，用于处理受气象干扰的动态场景。</li>
<li>methods: 该网络使用了拥平均的运动流来驱动一种新的区域增长算法，从而生成每个视频中的移动对象mask。然后，使用U-Net架构和一致性和分组损失来进一步优化这些mask，以确保它们在空间和时间方面具有最好的含义。</li>
<li>results: 该方法不需要标注训练数据，可以在不同的气象干扰强度下工作，并且在新发布的移动对象分割 dataset 上表现出了更高的分割精度和稳定性，比较于当前无监督方法。<details>
<summary>Abstract</summary>
In this paper, we present a two-stage unsupervised foreground object segmentation network tailored for dynamic scenes affected by atmospheric turbulence. In the first stage, we utilize averaged optical flow from turbulence-distorted image sequences to feed a novel region-growing algorithm, crafting preliminary masks for each moving object in the video. In the second stage, we employ a U-Net architecture with consistency and grouping losses to further refine these masks optimizing their spatio-temporal alignment. Our approach does not require labeled training data and works across varied turbulence strengths for long-range video. Furthermore, we release the first moving object segmentation dataset of turbulence-affected videos, complete with manually annotated ground truth masks. Our method, evaluated on this new dataset, demonstrates superior segmentation accuracy and robustness as compared to current state-of-the-art unsupervised methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于动态场景和大气抖擞的两Stage无监督前景物 segmentation网络。在第一Stage中，我们利用滤波后的平均运动速度来驱动一种新的区域增长算法，生成每幅视频中移动对象的初步面积。在第二Stage中，我们使用U-Net架构和一致性和分组损失来进一步细化这些面积，以便在空间和时间方向进行最佳对齐。我们的方法不需要标注训练数据，并在不同的大气抖擞强度下工作，可以处理长距离视频。此外，我们发布了第一个受抖擞影响的运动对象分 segmentation数据集，包括手动标注的真实地面积。我们的方法，在这新的数据集上进行评估，与当前无监督方法相比，显示出更高的分 segmentation精度和鲁棒性。
</details></li>
</ul>
<hr>
<h2 id="Cal-DETR-Calibrated-Detection-Transformer"><a href="#Cal-DETR-Calibrated-Detection-Transformer" class="headerlink" title="Cal-DETR: Calibrated Detection Transformer"></a>Cal-DETR: Calibrated Detection Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03570">http://arxiv.org/abs/2311.03570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akhtarvision/cal-detr">https://github.com/akhtarvision/cal-detr</a></li>
<li>paper_authors: Muhammad Akhtar Munir, Salman Khan, Muhammad Haris Khan, Mohsen Ali, Fahad Shahbaz Khan</li>
<li>for: 这个研究旨在对现代基于对话 transformer 的物体检测器进行准确性调整，以提高它们在安全应用中的适用范围。</li>
<li>methods: 本研究提出了一个名为 Cal-DETR 的训练时间准确性调整机制，包括一个简单 yet effective 的对 transformer-based 物体检测器的不确定性评估方法，以及一个基于不确定性的类别LOGIT 调整机制。</li>
<li>results: Results 显示 Cal-DETR 可以对内部和外部测试 scenario 中的检测器进行有效的准确性调整，同时保持或甚至提高检测性能。<details>
<summary>Abstract</summary>
Albeit revealing impressive predictive performance for several computer vision tasks, deep neural networks (DNNs) are prone to making overconfident predictions. This limits the adoption and wider utilization of DNNs in many safety-critical applications. There have been recent efforts toward calibrating DNNs, however, almost all of them focus on the classification task. Surprisingly, very little attention has been devoted to calibrating modern DNN-based object detectors, especially detection transformers, which have recently demonstrated promising detection performance and are influential in many decision-making systems. In this work, we address the problem by proposing a mechanism for calibrated detection transformers (Cal-DETR), particularly for Deformable-DETR, UP-DETR and DINO. We pursue the train-time calibration route and make the following contributions. First, we propose a simple yet effective approach for quantifying uncertainty in transformer-based object detectors. Second, we develop an uncertainty-guided logit modulation mechanism that leverages the uncertainty to modulate the class logits. Third, we develop a logit mixing approach that acts as a regularizer with detection-specific losses and is also complementary to the uncertainty-guided logit modulation technique to further improve the calibration performance. Lastly, we conduct extensive experiments across three in-domain and four out-domain scenarios. Results corroborate the effectiveness of Cal-DETR against the competing train-time methods in calibrating both in-domain and out-domain detections while maintaining or even improving the detection performance. Our codebase and pre-trained models can be accessed at \url{https://github.com/akhtarvision/cal-detr}.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在计算机视觉任务上表现出了卓越的预测能力，但它们受到过度自信的限制，这限制了DNN在安全关键应用中的广泛应用。有些最近的努力是对DNN进行准确化，但大多数这些努力都集中在分类任务上。却很少人关注现代基于DNN的物体检测器，特别是转换器基于的物体检测器，它们在许多决策系统中具有影响力。在这个工作中，我们解决这个问题，我们提出了一种 mechanism for calibrated detection transformers（Cal-DETR），特别是对Deformable-DETR、UP-DETR和DINO进行准确化。我们采用了训练时期的准确化路径，我们的贡献包括：首先，我们提出了一种简单 yet effective的转换器基于物体检测器的uncertainty量化方法。其次，我们开发了一种基于uncertainty的logit调整机制，该机制利用uncertainty来调整类logits。最后，我们开发了一种logit混合approach，该approach acts as a regularizer with detection-specific losses，并且与uncertainty-guided logit modulation technique相结合，以进一步提高准确性表现。我们在三个域内和四个外域场景进行了广泛的实验，结果证明Cal-DETR在对抗训练时期方法的竞争中，能够准确地调整域内和外域检测。我们的代码库和预训练模型可以在 \url{https://github.com/akhtarvision/cal-detr} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Sea-You-Later-Metadata-Guided-Long-Term-Re-Identification-for-UAV-Based-Multi-Object-Tracking"><a href="#Sea-You-Later-Metadata-Guided-Long-Term-Re-Identification-for-UAV-Based-Multi-Object-Tracking" class="headerlink" title="Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking"></a>Sea You Later: Metadata-Guided Long-Term Re-Identification for UAV-Based Multi-Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03561">http://arxiv.org/abs/2311.03561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Heng-Cheng Kuo, Jie Mei, Chung-I Huang, Jenq-Neng Hwang</li>
<li>for: 这篇论文是为了解决UAV在海上计算机视觉中的多对象跟踪问题，具体来说是解决短期重识别（ReID）和长期跟踪的问题。</li>
<li>methods: 这篇论文提出了一种适应性 metadata 导引的多对象跟踪算法（MG-MOT），利用了 UAV 的 GPS 位置、飞机高度和摄像头方向等 metadata，将短期跟踪数据融合成一个coherent的长期跟踪。</li>
<li>results: 在使用 SeaDroneSee 跟踪集，这篇论文在最新的UAV-based Maritime Object Tracking Challenge中获得了优秀的性能，其中 HOTA 为 69.5%，IDF1 为 85.9%。<details>
<summary>Abstract</summary>
Re-identification (ReID) in multi-object tracking (MOT) for UAVs in maritime computer vision has been challenging for several reasons. More specifically, short-term re-identification (ReID) is difficult due to the nature of the characteristics of small targets and the sudden movement of the drone's gimbal. Long-term ReID suffers from the lack of useful appearance diversity. In response to these challenges, we present an adaptable motion-based MOT algorithm, called Metadata Guided MOT (MG-MOT). This algorithm effectively merges short-term tracking data into coherent long-term tracks, harnessing crucial metadata from UAVs, including GPS position, drone altitude, and camera orientations. Extensive experiments are conducted to validate the efficacy of our MOT algorithm. Utilizing the challenging SeaDroneSee tracking dataset, which encompasses the aforementioned scenarios, we achieve a much-improved performance in the latest edition of the UAV-based Maritime Object Tracking Challenge with a state-of-the-art HOTA of 69.5% and an IDF1 of 85.9% on the testing split.
</details>
<details>
<summary>摘要</summary>
多目标跟踪（MOT）在无人机（UAV）上的重新识别（ReID）具有许多挑战，主要是因为小目标的特点和无人机镜头的快速移动。长期ReID受到缺乏有用的外观多样性的限制。为解决这些挑战，我们提出了适应性Metadata驱动的MOT算法（MG-MOT）。这种算法可以将短期跟踪数据合并成一致的长期跟踪，利用无人机的GPS位置、飞行高度和相机 orientations 等重要metadata。我们对我们的MOT算法进行了广泛的实验验证。使用 SeaDroneSee 跟踪数据集，这个数据集包括以上场景，我们在最新的UAV基于海上物体跟踪挑战中取得了显著提高的性能，HOTA 为 69.5%，IDF1 为 85.9% 在测试分区。
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Similarity-Measure-based-Multi-Task-Learning-for-Predicting-Alzheimer’s-Disease-Progression-using-MRI-Data"><a href="#Spatio-Temporal-Similarity-Measure-based-Multi-Task-Learning-for-Predicting-Alzheimer’s-Disease-Progression-using-MRI-Data" class="headerlink" title="Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer’s Disease Progression using MRI Data"></a>Spatio-Temporal Similarity Measure based Multi-Task Learning for Predicting Alzheimer’s Disease Progression using MRI Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03557">http://arxiv.org/abs/2311.03557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xulong Wang, Yu Zhang, Menghui Zhou, Tong Liu, Jun Qi, Po Yang</li>
<li>For: 这篇论文的目的是为了提出一种基于多任务学习的新方法，用于有效地预测阿兹海默症（AD）的进展，并对该疾病的进展过程中各个生物标记的变化进行敏感地捕捉。* Methods: 这篇论文使用了一种基于多任务学习的新方法，包括定义一个时间测量来评估生物标记的变化趋势和速度，并将这个趋势转换为向量，然后在单一的向量空间中与其他生物标记进行比较。* Results: 实验结果显示，与对 ROI 进行直接学习相比，这种方法更有效地预测疾病的进展。此外，这种方法还可以实现长期稳定选择，对于疾病进展中各个生物标记之间的变化关系进行敏感地捕捉，并证明了这些变化关系对于认知预测有着重要的影响。<details>
<summary>Abstract</summary>
Identifying and utilising various biomarkers for tracking Alzheimer's disease (AD) progression have received many recent attentions and enable helping clinicians make the prompt decisions. Traditional progression models focus on extracting morphological biomarkers in regions of interest (ROIs) from MRI/PET images, such as regional average cortical thickness and regional volume. They are effective but ignore the relationships between brain ROIs over time, which would lead to synergistic deterioration. For exploring the synergistic deteriorating relationship between these biomarkers, in this paper, we propose a novel spatio-temporal similarity measure based multi-task learning approach for effectively predicting AD progression and sensitively capturing the critical relationships between biomarkers. Specifically, we firstly define a temporal measure for estimating the magnitude and velocity of biomarker change over time, which indicate a changing trend(temporal). Converting this trend into the vector, we then compare this variability between biomarkers in a unified vector space(spatial). The experimental results show that compared with directly ROI based learning, our proposed method is more effective in predicting disease progression. Our method also enables performing longitudinal stability selection to identify the changing relationships between biomarkers, which play a key role in disease progression. We prove that the synergistic deteriorating biomarkers between cortical volumes or surface areas have a significant effect on the cognitive prediction.
</details>
<details>
<summary>摘要</summary>
identifying 和利用不同的生物标志物（biomarkers）来跟踪阿尔ц海默病（AD）的进程已经收到了很多最近的关注，这些biomarkers可以帮助临床医生做出更加快速的决策。传统的进程模型会提取ROI（区域 интерес点）中的形态生物标志物，如区域average cortical thickness和区域体积。它们效果很好，但它们忽略了脑ROI之间的时间关系，这会导致同时破坏。为了探索这些生物标志物之间的同时破坏关系，在这篇论文中，我们提出了一种基于多任务学习的新的空间-时间相似度测量方法，用于有效地预测AD进程和敏感地捕捉生物标志物之间的关键关系。Specifically, we first define a temporal measure for estimating the magnitude and velocity of biomarker change over time, which indicates a changing trend (temporal). Converting this trend into a vector, we then compare this variability between biomarkers in a unified vector space (spatial). The experimental results show that compared with directly ROI-based learning, our proposed method is more effective in predicting disease progression. Our method also enables performing longitudinal stability selection to identify the changing relationships between biomarkers, which play a key role in disease progression. We prove that the synergistic deteriorating biomarkers between cortical volumes or surface areas have a significant effect on cognitive prediction.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-point-annotations-in-segmentation-learning-with-boundary-loss"><a href="#Leveraging-point-annotations-in-segmentation-learning-with-boundary-loss" class="headerlink" title="Leveraging point annotations in segmentation learning with boundary loss"></a>Leveraging point annotations in segmentation learning with boundary loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03537">http://arxiv.org/abs/2311.03537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eva Breznik, Hoel Kervadec, Filip Malmberg, Joel Kullberg, Håkan Ahlström, Marleen de Bruijne, Robin Strand</li>
<li>for: 这个论文研究了基于强度的距离地图与边损失的点指导semantic segmentation。</li>
<li>methods: 论文使用了边损失来强制更加严格地处理false positive，并使用了intensity-aware距离来缓解这个问题。</li>
<li>results: 实验结果表明，这种监督方法在ACDC和POEM两个多类数据集上表现出色，并且在POEM数据集上与CRF损失基于方法相当。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
This paper investigates the combination of intensity-based distance maps with boundary loss for point-supervised semantic segmentation. By design the boundary loss imposes a stronger penalty on the false positives the farther away from the object they occur. Hence it is intuitively inappropriate for weak supervision, where the ground truth label may be much smaller than the actual object and a certain amount of false positives (w.r.t. the weak ground truth) is actually desirable. Using intensity-aware distances instead may alleviate this drawback, allowing for a certain amount of false positives without a significant increase to the training loss. The motivation for applying the boundary loss directly under weak supervision lies in its great success for fully supervised segmentation tasks, but also in not requiring extra priors or outside information that is usually required -- in some form -- with existing weakly supervised methods in the literature. This formulation also remains potentially more attractive than existing CRF-based regularizers, due to its simplicity and computational efficiency. We perform experiments on two multi-class datasets; ACDC (heart segmentation) and POEM (whole-body abdominal organ segmentation). Preliminary results are encouraging and show that this supervision strategy has great potential. On ACDC it outperforms the CRF-loss based approach, and on POEM data it performs on par with it. The code for all our experiments is openly available.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="High-resolution-power-equipment-recognition-based-on-improved-self-attention"><a href="#High-resolution-power-equipment-recognition-based-on-improved-self-attention" class="headerlink" title="High-resolution power equipment recognition based on improved self-attention"></a>High-resolution power equipment recognition based on improved self-attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03518">http://arxiv.org/abs/2311.03518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siyi Zhang, Cheng Liu, Xiang Li, Xin Zhai, Zhen Wei, Sizhe Li, Xun Ma</li>
<li>for: 提高变压器图像识别精度，应对现有模型参数数量限制。</li>
<li>methods: 提出了一种基于深度自注意网络的新改进方法，包括基础网络、区域提议网络、目标区域提取和分割模块、最终预测网络。</li>
<li>results: 比较实验表明，该方法在变压器图像识别 tasks 上表现出色，大幅超越了两种常见的目标识别模型，为自动化电气设备检测带来新的思路。<details>
<summary>Abstract</summary>
The current trend of automating inspections at substations has sparked a surge in interest in the field of transformer image recognition. However, due to restrictions in the number of parameters in existing models, high-resolution images can't be directly applied, leaving significant room for enhancing recognition accuracy. Addressing this challenge, the paper introduces a novel improvement on deep self-attention networks tailored for this issue. The proposed model comprises four key components: a foundational network, a region proposal network, a module for extracting and segmenting target areas, and a final prediction network. The innovative approach of this paper differentiates itself by decoupling the processes of part localization and recognition, initially using low-resolution images for localization followed by high-resolution images for recognition. Moreover, the deep self-attention network's prediction mechanism uniquely incorporates the semantic context of images, resulting in substantially improved recognition performance. Comparative experiments validate that this method outperforms the two other prevalent target recognition models, offering a groundbreaking perspective for automating electrical equipment inspections.
</details>
<details>
<summary>摘要</summary>
当前的互动式检测技术在变电站中得到了广泛的应用，导致变压器图像识别领域的兴趣增加。然而，由于现有模型的参数数量限制，高分辨率图像直接应用不可，留下大量的提高识别精度的空间。为解决这个挑战，本文提出了一种新的深度自注意网络改进方法。该模型包括四个关键组成部分：基础网络、区域提议网络、目标区域提取和分割模块，以及最终预测网络。本文的创新approach是将部件localization和识别过程解耦，首先使用低分辨率图像进行localization，然后使用高分辨率图像进行识别。此外，深度自注意网络的预测机制唯一地含有图像 semanticcontext，导致识别性能明显提高。 comparative experiments表明，该方法在变电器目标识别方面表现出色，超越了两种常见的目标识别模型，提供了一个创新的视角为自动化电气设备检测。
</details></li>
</ul>
<hr>
<h2 id="SoundCam-A-Dataset-for-Finding-Humans-Using-Room-Acoustics"><a href="#SoundCam-A-Dataset-for-Finding-Humans-Using-Room-Acoustics" class="headerlink" title="SoundCam: A Dataset for Finding Humans Using Room Acoustics"></a>SoundCam: A Dataset for Finding Humans Using Room Acoustics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03517">http://arxiv.org/abs/2311.03517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mason Wang, Samuel Clarke, Jui-Hsien Wang, Ruohan Gao, Jiajun Wu</li>
<li>for: 这个论文的目的是提供一个大规模的听声环境数据集，用于研究听声环境的特征和人工智能应用。</li>
<li>methods: 这个论文使用了10个通道的实际世界听声响应测量和10个通道的音乐录音，在三个不同的房间中进行了测量，包括一个控制的听声实验室、一个生活室和一个会议室，并在每个房间中进行了不同的人的位置测量。</li>
<li>results: 这个论文发现，这些测量可以用于探测和识别人类，以及跟踪他们的位置。<details>
<summary>Abstract</summary>
A room's acoustic properties are a product of the room's geometry, the objects within the room, and their specific positions. A room's acoustic properties can be characterized by its impulse response (RIR) between a source and listener location, or roughly inferred from recordings of natural signals present in the room. Variations in the positions of objects in a room can effect measurable changes in the room's acoustic properties, as characterized by the RIR. Existing datasets of RIRs either do not systematically vary positions of objects in an environment, or they consist of only simulated RIRs. We present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms publicly released to date. It includes 5,000 10-channel real-world measurements of room impulse responses and 2,000 10-channel recordings of music in three different rooms, including a controlled acoustic lab, an in-the-wild living room, and a conference room, with different humans in positions throughout each room. We show that these measurements can be used for interesting tasks, such as detecting and identifying humans, and tracking their positions.
</details>
<details>
<summary>摘要</summary>
一个房间的声学性质是由房间的几何结构、房间内的物品以及它们的具体位置相互关系而决定。一个房间的声学性质可以通过源和听众位置之间的冲激响应（RIR）来描述，或者通过在房间中存在的自然信号来推导出。房间中物品的位置变化可以导致明显变化的声学性质，这些变化可以通过RIR来描述。现有的RIR数据集ither不系统地变化环境中的物品位置，或者只是 simulate RIR数据集。我们提出了SoundCam，这是历史上最大的公共发布的声学环境数据集，包括5000个真实世界中的10个通道冲激响应测量和3个不同房间中的2000个10个通道音乐录音。我们展示了这些测量可以用于有趣的任务，如检测和识别人类，以及跟踪他们的位置。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Age-from-White-Matter-Diffusivity-with-Residual-Learning"><a href="#Predicting-Age-from-White-Matter-Diffusivity-with-Residual-Learning" class="headerlink" title="Predicting Age from White Matter Diffusivity with Residual Learning"></a>Predicting Age from White Matter Diffusivity with Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03500">http://arxiv.org/abs/2311.03500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Gao, Michael E. Kim, Ho Hin Lee, Qi Yang, Nazirah Mohd Khairi, Praitayini Kanakaraj, Nancy R. Newlin, Derek B. Archer, Angela L. Jefferson, Warren D. Taylor, Brian D. Boyd, Lori L. Beason-Held, Susan M. Resnick, The BIOCARD Study Team, Yuankai Huo, Katherine D. Van Schaik, Kurt G. Schilling, Daniel Moyer, Ivana Išgum, Bennett A. Landman</li>
<li>for: 这个论文目的是开发白 matter 特有的年龄估计方法，以捕捉与正常年龄增长不同的异常。</li>
<li>methods: 这个论文使用了两种方法来预测年龄：一种是提取特定区域的微结构特征，另一种是使用3D差异神经网络（ResNets）学习图像直接特征，并对图像进行非线性对齐和折叠以最小化宏strucutral变化。</li>
<li>results: 测试数据上，第一种方法的 mean absolute error（MAE）为6.11年（正常参与者）和6.62年（ cognitively impaired participant），而第二种方法的 MAE 为4.69年（正常参与者）和4.96年（ cognitively impaired participant），显示 ResNet 模型可以更好地捕捉微结构特征进行年龄预测。<details>
<summary>Abstract</summary>
Imaging findings inconsistent with those expected at specific chronological age ranges may serve as early indicators of neurological disorders and increased mortality risk. Estimation of chronological age, and deviations from expected results, from structural MRI data has become an important task for developing biomarkers that are sensitive to such deviations. Complementary to structural analysis, diffusion tensor imaging (DTI) has proven effective in identifying age-related microstructural changes within the brain white matter, thereby presenting itself as a promising additional modality for brain age prediction. Although early studies have sought to harness DTI's advantages for age estimation, there is no evidence that the success of this prediction is owed to the unique microstructural and diffusivity features that DTI provides, rather than the macrostructural features that are also available in DTI data. Therefore, we seek to develop white-matter-specific age estimation to capture deviations from normal white matter aging. Specifically, we deliberately disregard the macrostructural information when predicting age from DTI scalar images, using two distinct methods. The first method relies on extracting only microstructural features from regions of interest. The second applies 3D residual neural networks (ResNets) to learn features directly from the images, which are non-linearly registered and warped to a template to minimize macrostructural variations. When tested on unseen data, the first method yields mean absolute error (MAE) of 6.11 years for cognitively normal participants and MAE of 6.62 years for cognitively impaired participants, while the second method achieves MAE of 4.69 years for cognitively normal participants and MAE of 4.96 years for cognitively impaired participants. We find that the ResNet model captures subtler, non-macrostructural features for brain age prediction.
</details>
<details>
<summary>摘要</summary>
干预发现与期望的年龄范围不符的可能是脑神经疾病和死亡风险的早期指标。确定年龄和与期望结果的偏差，从结构MRI数据中获得的任务已成为开发敏感于这些偏差的生物标志物的重要任务。与结构分析相 complementary，Diffusion tensor imaging (DTI)已经证明可以在脑白 matter中检测年龄相关的微strucutural变化，因此成为脑年龄预测的有力的附加模式。 although early studies have sought to harness DTI's advantages for age estimation, there is no evidence that the success of this prediction is owed to the unique microstructural and diffusivity features that DTI provides, rather than the macrostructural features that are also available in DTI data. Therefore, we seek to develop white-matter-specific age estimation to capture deviations from normal white matter aging. Specifically, we deliberately disregard the macrostructural information when predicting age from DTI scalar images, using two distinct methods. The first method relies on extracting only microstructural features from regions of interest. The second applies 3D residual neural networks (ResNets) to learn features directly from the images, which are non-linearly registered and warped to a template to minimize macrostructural variations. When tested on unseen data, the first method yields mean absolute error (MAE) of 6.11 years for cognitively normal participants and MAE of 6.62 years for cognitively impaired participants, while the second method achieves MAE of 4.69 years for cognitively normal participants and MAE of 4.96 years for cognitively impaired participants. We find that the ResNet model captures subtler, non-macrostructural features for brain age prediction.
</details></li>
</ul>
<hr>
<h2 id="CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding"><a href="#CoVLM-Composing-Visual-Entities-and-Relationships-in-Large-Language-Models-Via-Communicative-Decoding" class="headerlink" title="CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding"></a>CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03354">http://arxiv.org/abs/2311.03354</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UMass-Foundation-Model/CoVLM">https://github.com/UMass-Foundation-Model/CoVLM</a></li>
<li>paper_authors: Junyan Li, Delin Chen, Yining Hong, Zhenfang Chen, Peihao Chen, Yikang Shen, Chuang Gan</li>
<li>for: 提高大型视语言基础模型（VLM）的可 композиitional 能力，使其能够更好地理解和生成视语言对话。</li>
<li>methods: 提出了一种新的通信 токен技术，以便动态地通信 между视觉检测系统和语言系统，使Language Model（LM）能够更好地组合视觉实体和关系。</li>
<li>results: 与传统VLM相比，CoVLM在compositional reasoning benchmarks上表现出色，提高了约20%的HICO-DET mAP、约14%的Cola top-1准确率和约3%的ARO top-1准确率，同时在传统视语言任务中也达到了状态级表现。<details>
<summary>Abstract</summary>
A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make "infinite use of finite means". However, current large vision-language foundation models (VLMs) fall short of such compositional abilities due to their "bag-of-words" behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose CoVLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., ~20% in HICO-DET mAP, ~14% in Cola top-1 accuracy, and ~3% on ARO top-1 accuracy). We also achieve state-of-the-art performances on traditional vision-language tasks such as referring expression comprehension and visual question answering.
</details>
<details>
<summary>摘要</summary>
人类拥有非凡的作业能力，即将“无限用 finite means”。然而，当前大量视语基础模型（VLM）仍然缺乏这种作业能力，因为它们的“袋子行为”和无法正确地构成视觉实体和实体之间的关系。为此，我们提出了CoVLM，可以引导LLM在文本和视觉Encoder之间进行可靠的交流，以实现视语通信编码。具体来说，我们首先设计了一组新的交流符，用于LLM与视觉检测系统之间的动态交流。这些交流符由LLM在视觉实体或关系后生成，以通知检测网络提出相关的区域。这些提出的区域的兴趣点（ROIs）然后被反馈到LLM，以便更好地根据相关区域进行语言生成。LLM因此可以通过交流符来组合视觉实体和关系。我们的框架可以凝聚视觉和LLM之间的关系，并在组合理解benchmark上超越前一代VLM的表现（例如，HICO-DET mAP上的~20%提升、Cola top-1准确率上的~14%提升和ARO top-1准确率上的~3%提升）。我们还在传统的视觉语言任务中实现了状态之前的表现，如引用表达理解和视觉问答。
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion"><a href="#Rethinking-Evaluation-Metrics-of-Open-Vocabulary-Segmentaion" class="headerlink" title="Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion"></a>Rethinking Evaluation Metrics of Open-Vocabulary Segmentaion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03352">http://arxiv.org/abs/2311.03352</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qqlu/entity">https://github.com/qqlu/entity</a></li>
<li>paper_authors: Hao Zhou, Tiancheng Shen, Xu Yang, Hai Huang, Xiangtai Li, Lu Qi, Ming-Hsuan Yang</li>
<li>for: 本文探讨了开放词汇 segmentation 中的评估指标问题，即评估过程仍然强调关闭集成 metric 在零shot或者 cross-dataset 管道中，而不考虑预测和实际标签类别之间的相似性。</li>
<li>methods: 本文首先对 eleven 种类别间的相似度量进行了抽查和用户研究，包括 WordNet 语言统计学、文本嵌入和语言模型。基于这些探讨的 measurements，我们设计了一些新的评估指标，包括 Open mIoU、Open AP 和 Open PQ，适用于三种开放词汇 segmentation 任务。</li>
<li>results: 我们对 twelve 种开放词汇 segmentation 方法进行了 benchmark，并证明了我们的评估指标可以很好地评估开放能力。尽管相对性的subjectivity 存在，但我们的工作希望可以带领社区新的思考如何评估开放能力。评估代码在github 上发布。<details>
<summary>Abstract</summary>
In this paper, we highlight a problem of evaluation metrics adopted in the open-vocabulary segmentation. That is, the evaluation process still heavily relies on closed-set metrics on zero-shot or cross-dataset pipelines without considering the similarity between predicted and ground truth categories. To tackle this issue, we first survey eleven similarity measurements between two categorical words using WordNet linguistics statistics, text embedding, and language models by comprehensive quantitative analysis and user study. Built upon those explored measurements, we designed novel evaluation metrics, namely Open mIoU, Open AP, and Open PQ, tailored for three open-vocabulary segmentation tasks. We benchmarked the proposed evaluation metrics on 12 open-vocabulary methods of three segmentation tasks. Even though the relative subjectivity of similarity distance, we demonstrate that our metrics can still well evaluate the open ability of the existing open-vocabulary segmentation methods. We hope that our work can bring with the community new thinking about how to evaluate the open ability of models. The evaluation code is released in github.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences"><a href="#Long-Term-Invariant-Local-Features-via-Implicit-Cross-Domain-Correspondences" class="headerlink" title="Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences"></a>Long-Term Invariant Local Features via Implicit Cross-Domain Correspondences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03345">http://arxiv.org/abs/2311.03345</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zador Pataki, Mohammad Altillawi, Menelaos Kanakis, Rémi Pautrat, Fengyi Shen, Ziyuan Liu, Luc Van Gool, Marc Pollefeys</li>
<li>for: 本文旨在investigate long-term visual domain variations的影响 на visual localization，并提出一种数据驱动的方法来改善现代特征提取网络的跨Domain可靠性。</li>
<li>methods: 本文使用了现代特征提取网络，并对其进行了改进，包括提出了一种新的数据驱动方法（Implicit Cross-Domain Correspondences，iCDC），该方法可以生成跨Domain的准确对应关系。</li>
<li>results: 本文的实验结果显示，使用了提出的iCDC方法的网络，可以在跨Domain的情况下提高视觉本地化性能，并且与现有方法相比，有显著的性能优势。<details>
<summary>Abstract</summary>
Modern learning-based visual feature extraction networks perform well in intra-domain localization, however, their performance significantly declines when image pairs are captured across long-term visual domain variations, such as different seasonal and daytime variations. In this paper, our first contribution is a benchmark to investigate the performance impact of long-term variations on visual localization. We conduct a thorough analysis of the performance of current state-of-the-art feature extraction networks under various domain changes and find a significant performance gap between intra- and cross-domain localization. We investigate different methods to close this gap by improving the supervision of modern feature extractor networks. We propose a novel data-centric method, Implicit Cross-Domain Correspondences (iCDC). iCDC represents the same environment with multiple Neural Radiance Fields, each fitting the scene under individual visual domains. It utilizes the underlying 3D representations to generate accurate correspondences across different long-term visual conditions. Our proposed method enhances cross-domain localization performance, significantly reducing the performance gap. When evaluated on popular long-term localization benchmarks, our trained networks consistently outperform existing methods. This work serves as a substantial stride toward more robust visual localization pipelines for long-term deployments, and opens up research avenues in the development of long-term invariant descriptors.
</details>
<details>
<summary>摘要</summary>
现代学习基于的视觉特征提取网络在同一个频谱域内的本地化表现良好，但是当图像对被捕捉到不同季节和日期变化时，其表现却明显下降。在这篇论文中，我们的首要贡献是设立了跨域变化的视觉本地化性能的benchmark，并进行了当今状态艺术特征提取网络的系统性分析。我们发现了跨域变化对视觉本地化性能的显著性 gap，并 investigate了不同的方法来填补这个差距。我们提出了一种新的数据中心方法，即隐藏的跨域相对性（iCDC）。iCDC使用不同视觉频谱下的场景的多个神经辐射场，每个场景都适应各自的视觉频谱。它利用了下面的3D表示来生成准确的跨域相对性。我们的提议方法可以显著提高跨域本地化性能，降低性能差距。当我们的训练网络被评估于流行的长期本地化benchmark上，它们一直表现出色，超越了现有的方法。这项工作是对长期可靠的视觉本地化管道的重要进步，并开启了长期不变描述器的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer"><a href="#Cross-Image-Attention-for-Zero-Shot-Appearance-Transfer" class="headerlink" title="Cross-Image Attention for Zero-Shot Appearance Transfer"></a>Cross-Image Attention for Zero-Shot Appearance Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03335">http://arxiv.org/abs/2311.03335</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/garibida/cross-image-attention">https://github.com/garibida/cross-image-attention</a></li>
<li>paper_authors: Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, Daniel Cohen-Or</li>
<li>For: The paper aims to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape.* Methods: The authors build upon the self-attention layers of text-to-image generative models and introduce a cross-image attention mechanism to establish semantic correspondences across images. They also use three mechanisms to manipulate the noisy latent codes or the model’s internal representations throughout the denoising process.* Results: The approach is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.<details>
<summary>Abstract</summary>
Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images -- one depicting the target structure and the other specifying the desired appearance -- our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model's internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.
</details>
<details>
<summary>摘要</summary>
现代文本到图像生成模型的进步已经显示了捕捉图像深度Semantic理解的能力。在这项工作中，我们利用这种Semantic知识来传递图像之间的Visual形态。为 достичь这一目标，我们在生成模型中的自注意层上建立了跨图像注意机制，该机制将target图像中的查询与desired appearance图像中的键和值相结合。当应用于噪声除法过程中时，这种操作可以利用已经建立的Semantic匹配来生成一个 combining the desired structure and appearance的图像。此外，为了提高输出图像质量，我们利用了三种机制，分别是修改噪声缺失代码或模型内部表示的方法，这些机制在噪声除法过程中进行。值得注意的是，我们的方法是零 shot的，不需要优化或训练。实验结果表明，我们的方法在多种物体类别上效果广泛，并且对图像之间的形态、大小和视角变化 display 具有较好的Robustness。
</details></li>
</ul>
<hr>
<h2 id="TSP-Transformer-Task-Specific-Prompts-Boosted-Transformer-for-Holistic-Scene-Understanding"><a href="#TSP-Transformer-Task-Specific-Prompts-Boosted-Transformer-for-Holistic-Scene-Understanding" class="headerlink" title="TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding"></a>TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03427">http://arxiv.org/abs/2311.03427</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tb2-sy/tsp-transformer">https://github.com/tb2-sy/tsp-transformer</a></li>
<li>paper_authors: Shuo Wang, Jing Li, Zibo Zhao, Dongze Lian, Binbin Huang, Xiaomei Wang, Zhengxin Li, Shenghua Gao</li>
<li>for: 本文主要针对holistic scene understanding问题提出了一种Task-Specific Prompts Transformer（TSP-Transformer）方法，用于学习有效的表示。</li>
<li>methods: 该方法首先使用了一个vanilla transformer层，然后使用了一个任务特定的prompt transformer层，其中任务特定的prompt可以被视为启发器，帮助模型学习各个任务的特异性特征。</li>
<li>results: 实验结果表明， compared with existing方法，TSP-Transformer可以在NYUD-v2和PASCAL-Context datasets上达到最佳性能，证明了该方法的效果性。 code可以在以下链接中找到：<a target="_blank" rel="noopener" href="https://github.com/tb2-sy/TSP-Transformer%E3%80%82">https://github.com/tb2-sy/TSP-Transformer。</a><details>
<summary>Abstract</summary>
Holistic scene understanding includes semantic segmentation, surface normal estimation, object boundary detection, depth estimation, etc. The key aspect of this problem is to learn representation effectively, as each subtask builds upon not only correlated but also distinct attributes. Inspired by visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It features a vanilla transformer in the early stage and tasks-specific prompts transformer encoder in the lateral stage, where tasks-specific prompts are augmented. By doing so, the transformer layer learns the generic information from the shared parts and is endowed with task-specific capacity. First, the tasks-specific prompts serve as induced priors for each task effectively. Moreover, the task-specific prompts can be seen as switches to favor task-specific representation learning for different tasks. Extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding. We also provide our code in the following link https://github.com/tb2-sy/TSP-Transformer.
</details>
<details>
<summary>摘要</summary>
整体场景理解包括semantic segmentation、表面normal estimation、物体边界检测、深度估计等。关键问题在于学习表示效果，因为每个子任务建立在不仅相关性还有特定属性上。 Drawing inspiration from visual-prompt tuning, we propose a Task-Specific Prompts Transformer, dubbed TSP-Transformer, for holistic scene understanding. It consists of a vanilla transformer in the early stage and a task-specific prompts transformer encoder in the lateral stage, where task-specific prompts are augmented. By doing so, the transformer layer learns generic information from the shared parts and is endowed with task-specific capacity. First, the task-specific prompts serve as induced priors for each task, and they can be seen as switches that favor task-specific representation learning for different tasks. Our extensive experiments on NYUD-v2 and PASCAL-Context show that our method achieves state-of-the-art performance, validating the effectiveness of our method for holistic scene understanding. Our code is available at the following link: <https://github.com/tb2-sy/TSP-Transformer>.
</details></li>
</ul>
<hr>
<h2 id="A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas"><a href="#A-Robust-Bi-Directional-Algorithm-For-People-Count-In-Crowded-Areas" class="headerlink" title="A Robust Bi-Directional Algorithm For People Count In Crowded Areas"></a>A Robust Bi-Directional Algorithm For People Count In Crowded Areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03323">http://arxiv.org/abs/2311.03323</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyanarayana Penke, Gopikrishna Pavuluri, Soukhya Kunda, Satvik M, CharanKumar Y</li>
<li>for: 本研究旨在提供一种精准的人数计数系统，以帮助管理人员在繁忙的场所进行人员管理和救援。</li>
<li>methods: 本研究使用了图像处理技术和机器学习算法，通过识别人群形态和跟踪人员的移动方向来计算人数。</li>
<li>results: 实验结果表明，该算法可以准确地计算人数，并且可以在实时 scenarios 中提供人员的流动信息。<details>
<summary>Abstract</summary>
People counting system in crowded places has become a very useful practical application that can be accomplished in various ways which include many traditional methods using sensors. Examining the case of real time scenarios, the algorithm espoused should be steadfast and accurate. People counting algorithm presented in this paper, is centered on blob assessment, devoted to yield the count of the people through a path along with the direction of traversal. The system depicted is often ensconced at the entrance of a building so that the unmitigated frequency of visitors can be recorded. The core premise of this work is to extricate count of people inflow and outflow pertaining to a particular area. The tot-up achieved can be exploited for purpose of statistics in the circumstances of any calamity occurrence in that zone. Relying upon the count totaled, the population in that vicinity can be assimilated in order to take on relevant measures to rescue the people.
</details>
<details>
<summary>摘要</summary>
人数计算系统在拥挤地方已成为非常有用的实践应用，可以通过多种传统方法使用传感器实现。在实时场景中，算法应该坚定稳定，准确地计算人数。本文所描述的人数算法基于物体评估，通过跟踪人们在特定路径上的移动方向来计算人数。系统通常安装在建筑物入口处，以记录进出人数的频率。本研究的核心思想是从人数流入和流出中提取具体的人数统计数据，以便在紧急情况下采取有关救援措施。基于计算的人数，可以对当地人口进行融合，以便采取有关救援措施。
</details></li>
</ul>
<hr>
<h2 id="FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data"><a href="#FATE-Feature-Agnostic-Transformer-based-Encoder-for-learning-generalized-embedding-spaces-in-flow-cytometry-data" class="headerlink" title="FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data"></a>FATE: Feature-Agnostic Transformer-based Encoder for learning generalized embedding spaces in flow cytometry data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03314">http://arxiv.org/abs/2311.03314</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lisaweijler/fate">https://github.com/lisaweijler/fate</a></li>
<li>paper_authors: Lisa Weijler, Florian Kowarsch, Michael Reiter, Pedro Hermosilla, Margarita Maurer-Granofszky, Michael Dworzak</li>
<li>for: 这个论文是为了解决资料收集时给定特征的限制，以便将资料处理为可以运行的模型。</li>
<li>methods: 这个论文提出了一个新的架构，即set-transformer架构，可以直接处理不同特征空间的数据，而不需要规定输入空间为特征集的交集或联集。这个架构通过添加特征编解层，实现了从不同特征空间中学习共同的潜在特征空间。</li>
<li>results: 这个论文的模型在自动检测淋巴细胞抗生素敏感性检测中得到了良好的结果，并且可以运行在不同的特征空间中。特别是在淋巴细胞抗生素敏感性检测中，资料稀缺性是由疾病的低流行率导致的，这个模型的能力对于这种情况是非常重要的。<details>
<summary>Abstract</summary>
While model architectures and training strategies have become more generic and flexible with respect to different data modalities over the past years, a persistent limitation lies in the assumption of fixed quantities and arrangements of input features. This limitation becomes particularly relevant in scenarios where the attributes captured during data acquisition vary across different samples. In this work, we aim at effectively leveraging data with varying features, without the need to constrain the input space to the intersection of potential feature sets or to expand it to their union. We propose a novel architecture that can directly process data without the necessity of aligned feature modalities by learning a general embedding space that captures the relationship between features across data samples with varying sets of features. This is achieved via a set-transformer architecture augmented by feature-encoder layers, thereby enabling the learning of a shared latent feature space from data originating from heterogeneous feature spaces. The advantages of the model are demonstrated for automatic cancer cell detection in acute myeloid leukemia in flow cytometry data, where the features measured during acquisition often vary between samples. Our proposed architecture's capacity to operate seamlessly across incongruent feature spaces is particularly relevant in this context, where data scarcity arises from the low prevalence of the disease. The code is available for research purposes at https://github.com/lisaweijler/FATE.
</details>
<details>
<summary>摘要</summary>
“在过去几年中，模型架构和训练策略在不同数据模式之间变得更加通用和灵活，但是一个持续的限制是假设输入特征的固定量和排序。这个限制在样本中的特征采集时发生变化时 particualrly relevant。在这项工作中，我们希望通过不需要受限于输入空间的交叉或者拓展来有效地利用变量特征的数据。我们提出了一种新的架构，可以直接处理数据，不需要对特征模式进行对齐。我们通过将特征编码层添加到集成 transformer 架构中，使得模型可以从不同特征空间中学习共享的幂等特征空间。这些优点在抑制静脉细胞检测中 AUTOMATIC 的淋巴细胞癌症数据中得到了证明，这里的特征通常在样本之间发生变化。我们的提出的架构能够不受不同特征空间之间的不一致限制，特别 relevance 在这个上，由于疾病的低发生率，数据的稀缺性是一个主要的问题。模型代码可以在 GitHub 上获取，请参考 <https://github.com/lisaweijler/FATE>。”
</details></li>
</ul>
<hr>
<h2 id="A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation"><a href="#A-Single-2D-Pose-with-Context-is-Worth-Hundreds-for-3D-Human-Pose-Estimation" class="headerlink" title="A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation"></a>A Single 2D Pose with Context is Worth Hundreds for 3D Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03312">http://arxiv.org/abs/2311.03312</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/QitaoZhao/ContextAware-PoseFormer">https://github.com/QitaoZhao/ContextAware-PoseFormer</a></li>
<li>paper_authors: Qitao Zhao, Ce Zheng, Mengyuan Liu, Chen Chen</li>
<li>for: 提高3D人姿估计精度，不需要使用大量视频帧</li>
<li>methods: 利用 pré-trained 2D pose 检测器生成的中间视觉表示，无需进行训练</li>
<li>results: 对比 Context-Aware PoseFormer 和其他方法，显示了更高的速度和精度<details>
<summary>Abstract</summary>
The dominant paradigm in 3D human pose estimation that lifts a 2D pose sequence to 3D heavily relies on long-term temporal clues (i.e., using a daunting number of video frames) for improved accuracy, which incurs performance saturation, intractable computation and the non-causal problem. This can be attributed to their inherent inability to perceive spatial context as plain 2D joint coordinates carry no visual cues. To address this issue, we propose a straightforward yet powerful solution: leveraging the readily available intermediate visual representations produced by off-the-shelf (pre-trained) 2D pose detectors -- no finetuning on the 3D task is even needed. The key observation is that, while the pose detector learns to localize 2D joints, such representations (e.g., feature maps) implicitly encode the joint-centric spatial context thanks to the regional operations in backbone networks. We design a simple baseline named Context-Aware PoseFormer to showcase its effectiveness. Without access to any temporal information, the proposed method significantly outperforms its context-agnostic counterpart, PoseFormer, and other state-of-the-art methods using up to hundreds of video frames regarding both speed and precision. Project page: https://qitaozhao.github.io/ContextAware-PoseFormer
</details>
<details>
<summary>摘要</summary>
主流的3D人姿估算方法通过长期时间做为准备（使用大量视频帧）来提高准确性，这会导致性能混叠、计算困难和非 causa 问题。这可以归结于它们的内置不能感知空间上下文的问题，因为平面的2D关节坐标不含视觉提示。为解决这个问题，我们提出了一个简单 yet powerful的解决方案：利用可用的 intermediate visual representation（如Feature Map）生成的 off-the-shelf（预训练）2Dpose detector。我们发现，虽然pose detector learns to localize 2D关节，但这些表示（e.g., feature maps）在 backbone network中的 regional operation implicitely encode 关节-centric的空间上下文。我们设计了一个简单的基线方案，名为Context-Aware PoseFormer，以示其效果。不需要访问任何时间信息，我们的提posed方法在速度和精度两个方面与无Context-agnostic counterpart PoseFormer和其他使用Up to hundreds of video frames的方法相比，具有显著的优势。项目页面：https://qitaozhao.github.io/ContextAware-PoseFormer
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review"><a href="#Machine-Learning-Based-Tea-Leaf-Disease-Detection-A-Comprehensive-Review" class="headerlink" title="Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review"></a>Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03240">http://arxiv.org/abs/2311.03240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faruk Ahmed, Md. Taimur Ahad, Yousuf Rayhan Emon</li>
<li>for: 本研究旨在探讨机器学习技术在荵茶叶病诊断中的应用，以提高茶叶生产效率和质量。</li>
<li>methods: 本研究使用了多种图像处理技术，包括各种Transformer模型（如Inception Convolutional Vision Transformer（ICVT）、GreenViT、PlantXViT、PlantViT、MSCVT、Transfer Learning Model &amp; Vision Transformer（TLMViT）、IterationViT、IEM-ViT）以及其他模型（如Dense Convolutional Network（DenseNet）、Residual Neural Network（ResNet）-50V2、YOLOv5、YOLOv7、Convolutional Neural Network（CNN）、Deep CNN、Non-dominated Sorting Genetic Algorithm（NSGA-II）、MobileNetv2、Lesion-Aware Visual Transformer）。</li>
<li>results: 本研究通过对多个数据集进行测试，证明了这些机器学习模型在实际应用中的可行性。<details>
<summary>Abstract</summary>
Tea leaf diseases are a major challenge to agricultural productivity, with far-reaching implications for yield and quality in the tea industry. The rise of machine learning has enabled the development of innovative approaches to combat these diseases. Early detection and diagnosis are crucial for effective crop management. For predicting tea leaf disease, several automated systems have already been developed using different image processing techniques. This paper delivers a systematic review of the literature on machine learning methodologies applied to diagnose tea leaf disease via image classification. It thoroughly evaluates the strengths and constraints of various Vision Transformer models, including Inception Convolutional Vision Transformer (ICVT), GreenViT, PlantXViT, PlantViT, MSCVT, Transfer Learning Model & Vision Transformer (TLMViT), IterationViT, IEM-ViT. Moreover, this paper also reviews models like Dense Convolutional Network (DenseNet), Residual Neural Network (ResNet)-50V2, YOLOv5, YOLOv7, Convolutional Neural Network (CNN), Deep CNN, Non-dominated Sorting Genetic Algorithm (NSGA-II), MobileNetv2, and Lesion-Aware Visual Transformer. These machine-learning models have been tested on various datasets, demonstrating their real-world applicability. This review study not only highlights current progress in the field but also provides valuable insights for future research directions in the machine learning-based detection and classification of tea leaf diseases.
</details>
<details>
<summary>摘要</summary>
茶叶病菌是现代农业生产的主要挑战，对茶业产量和质量有着深远的影响。随着机器学习技术的发展，开发了一些创新的方法来抵御茶叶病菌。早期检测和诊断是农业管理的关键。在预测茶叶病菌方面，已经开发了许多自动化系统，使用不同的图像处理技术。本文提供了机器学习方法在诊断茶叶病菌方面的系统性评价，全面评估了不同的视觉转移模型，包括Inception Convolutional Vision Transformer（ICVT）、GreenViT、PlantXViT、PlantViT、MSCVT、Transfer Learning Model & Vision Transformer（TLMViT）、IterationViT、IEM-ViT等。此外，本文还评估了其他模型，如 dense convolutional network（DenseNet）、Residual Neural Network（ResNet）-50V2、YOLOv5、YOLOv7、Convolutional Neural Network（CNN）、Deep CNN、Non-dominated Sorting Genetic Algorithm（NSGA-II）、MobileNetv2、Lesion-Aware Visual Transformer等。这些机器学习模型在不同的数据集上进行了测试，表明了它们在实际应用中的可行性。本文不仅概述了当前领域的进展，还提供了有价值的未来研究方向，帮助推动机器学习在茶叶病菌检测和分类方面的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies"><a href="#Navigating-Scaling-Laws-Accelerating-Vision-Transformer’s-Training-via-Adaptive-Strategies" class="headerlink" title="Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies"></a>Navigating Scaling Laws: Accelerating Vision Transformer’s Training via Adaptive Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03233">http://arxiv.org/abs/2311.03233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sotiris Anagnostidis, Gregor Bachmann, Thomas Hofmann</li>
<li>for: 这 paper 的目的是提出一种可以在训练过程中动态调整模型形态的方法，以优化模型的性能。</li>
<li>methods: 这 paper 使用了一种基于 scaling laws 的方法，通过调整模型的形态，可以最优地利用计算资源，以提高模型的性能。</li>
<li>results: 这 paper 的实验结果表明，使用这种方法可以创造出一种更高效的 vision transformer 模型，并且可以在不同的 patch size 和宽度下进行调整，以优化模型的性能。<details>
<summary>Abstract</summary>
In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: Investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a "compute-optimal" model, i.e. a model that allocates a given level of compute during training optimally to maximise performance. In this work, we extend the concept of optimality by allowing for an "adaptive" model, i.e. a model that can change its shape during the course of training. By allowing the shape to adapt, we can optimally traverse between the underlying scaling laws, leading to a significant reduction in the required compute to reach a given target performance. We focus on vision tasks and the family of Vision Transformers, where the patch size as well as the width naturally serve as adaptive shape parameters. We demonstrate that, guided by scaling laws, we can design compute-optimal adaptive models that beat their "static" counterparts.
</details>
<details>
<summary>摘要</summary>
近年来，深度学习领域的状态精顶由大型模型所占据。这种方法很简单：投入更多计算资源（最优）会提高性能，甚至可预测性能如何提高。基于这种思想，我们提出了“计算优质”模型的概念，即在训练期间最优化计算资源的分配，以最大化性能。在这项工作中，我们将“可靠”模型扩展为可变形态模型，即在训练过程中可以改变模型的形态。通过允许形态变化，我们可以优化地 traverse 在下面的减少计算资源，以达到给定目标性能。我们将视觉任务和视觉转换器家族作为研究对象，并证明可以遵循减少计算资源的扩展。我们的计算优质可变模型可以在训练过程中击败其“静态”对手。
</details></li>
</ul>
<hr>
<h2 id="Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet"><a href="#Segmentation-of-Drone-Collision-Hazards-in-Airborne-RADAR-Point-Clouds-Using-PointNet" class="headerlink" title="Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet"></a>Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds Using PointNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03221">http://arxiv.org/abs/2311.03221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hector Arroyo, Paul Kier, Dylan Angus, Santiago Matalonga, Svetlozar Georgiev, Mehdi Goli, Gerard Dooly, James Riordan</li>
<li>for: 本研究旨在帮助无人机在共享空域中进行 beyond visual line of sight（BVLOS）操作，提高无人机的 situational awareness，以确保安全操作。</li>
<li>methods: 本研究使用雷达技术，开发了一种基于 PointNet 架构的终端到终端语义分割方法，可同时识别多个Collision Hazard。</li>
<li>results: 该方法可以在 aerial 设定下，识别出 Five distinct classes：移动无人机（DJI M300和DJI Mini）和飞机（Ikarus C42），以及静止返回（地面和基础设施），提高了无人机的 situational awareness，达到了94%的准确率。<details>
<summary>Abstract</summary>
The integration of unmanned aerial vehicles (UAVs) into shared airspace for beyond visual line of sight (BVLOS) operations presents significant challenges but holds transformative potential for sectors like transportation, construction, energy and defense. A critical prerequisite for this integration is equipping UAVs with enhanced situational awareness to ensure safe operations. Current approaches mainly target single object detection or classification, or simpler sensing outputs that offer limited perceptual understanding and lack the rapid end-to-end processing needed to convert sensor data into safety-critical insights. In contrast, our study leverages radar technology for novel end-to-end semantic segmentation of aerial point clouds to simultaneously identify multiple collision hazards. By adapting and optimizing the PointNet architecture and integrating aerial domain insights, our framework distinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and airplanes (Ikarus C42), and static returns (ground and infrastructure) which results in enhanced situational awareness for UAVs. To our knowledge, this is the first approach addressing simultaneous identification of multiple collision threats in an aerial setting, achieving a robust 94% accuracy. This work highlights the potential of radar technology to advance situational awareness in UAVs, facilitating safe and efficient BVLOS operations.
</details>
<details>
<summary>摘要</summary>
integrating unmanned aerial vehicles (UAVs) into shared airspace for beyond visual line of sight (BVLOS) operations presents significant challenges but holds transformative potential for sectors like transportation, construction, energy, and defense. A critical prerequisite for this integration is equipping UAVs with enhanced situational awareness to ensure safe operations. current approaches mainly target single object detection or classification, or simpler sensing outputs that offer limited perceptual understanding and lack the rapid end-to-end processing needed to convert sensor data into safety-critical insights. in contrast, our study leverages radar technology for novel end-to-end semantic segmentation of aerial point clouds to simultaneously identify multiple collision hazards. by adapting and optimizing the PointNet architecture and integrating aerial domain insights, our framework distinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and airplanes (Ikarus C42), and static returns (ground and infrastructure) which results in enhanced situational awareness for UAVs. to our knowledge, this is the first approach addressing simultaneous identification of multiple collision threats in an aerial setting, achieving a robust 94% accuracy. this work highlights the potential of radar technology to advance situational awareness in UAVs, facilitating safe and efficient BVLOS operations.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data"><a href="#Leveraging-Transformers-to-Improve-Breast-Cancer-Classification-and-Risk-Assessment-with-Multi-modal-and-Longitudinal-Data" class="headerlink" title="Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data"></a>Leveraging Transformers to Improve Breast Cancer Classification and Risk Assessment with Multi-modal and Longitudinal Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03217">http://arxiv.org/abs/2311.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqiu Shen, Jungkyu Park, Frank Yeung, Eliana Goldberg, Laura Heacock, Farah Shamout, Krzysztof J. Geras</li>
<li>for: 这个研究旨在提高乳癌检测精度，特别是透过融合多modal的对话和时间变化信息以提高病人状态评估和未来癌症风险评估。</li>
<li>methods: 这个研究使用的是Multi-modal Transformer（MMT）神经网络，融合了脉冲探测和超音波对话，以帮助验证病人是否目前有癌症，并估算未来癌症风险。MMT使用自我对话和比较现有对话以聚焦多modal资料，并追踪时间变化以比较现有对话和先前对话。</li>
<li>results: 这个研究使用130万个检测数据，获得了AUROC0.943的检测精度，超过单一modal的基eline。此外，这个模型还可以对病人状态进行5年的风险评估，AUROC为0.826，超过了先前的脉冲探测基eline。<details>
<summary>Abstract</summary>
Breast cancer screening, primarily conducted through mammography, is often supplemented with ultrasound for women with dense breast tissue. However, existing deep learning models analyze each modality independently, missing opportunities to integrate information across imaging modalities and time. In this study, we present Multi-modal Transformer (MMT), a neural network that utilizes mammography and ultrasound synergistically, to identify patients who currently have cancer and estimate the risk of future cancer for patients who are currently cancer-free. MMT aggregates multi-modal data through self-attention and tracks temporal tissue changes by comparing current exams to prior imaging. Trained on 1.3 million exams, MMT achieves an AUROC of 0.943 in detecting existing cancers, surpassing strong uni-modal baselines. For 5-year risk prediction, MMT attains an AUROC of 0.826, outperforming prior mammography-based risk models. Our research highlights the value of multi-modal and longitudinal imaging in cancer diagnosis and risk stratification.
</details>
<details>
<summary>摘要</summary>
乳癌检查通常通过胸部X射线摄影进行，但现有的深度学习模型通常只分析每种成像模式独立， missed opportunities to integrate多种成像模式和时间信息。本研究提出了多模态变换（MMT），一种使用胸部X射线摄影和ultrasound同时进行 synergistic 分析，以识别当前患有乳癌的患者和评估无癌患者是否会发展为乳癌。MMT通过自注意力和比较当前检测与过去成像来聚合多模态数据，并且可以跟踪时间变化。在130万个检测数据上训练，MMT在检测现有癌症方面达到了AUROC 0.943，超过了强大的单模态基线。而在5年风险预测方面，MMT达到了AUROC 0.826，超过了过去基于胸部X射线摄影的风险模型。我们的研究强调了多模态和 longitudinal 成像在肿瘤诊断和风险 stratification 中的价值。
</details></li>
</ul>
<hr>
<h2 id="PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions"><a href="#PainSeeker-An-Automated-Method-for-Assessing-Pain-in-Rats-Through-Facial-Expressions" class="headerlink" title="PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions"></a>PainSeeker: An Automated Method for Assessing Pain in Rats Through Facial Expressions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03205">http://arxiv.org/abs/2311.03205</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liu Liu, Guang Li, Dingfan Deng, Jinhua Yu, Yuan Zong</li>
<li>for:  investigate whether laboratory rats’ pain can be automatically assessed through their facial expressions.</li>
<li>methods:  proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions.</li>
<li>results:  demonstrated the feasibility of assessing rats’ pain from their facial expressions and also verified the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem.Here is the full text in Simplified Chinese:</li>
<li>for:  investigate whether laboratory rats’ pain can be automatically assessed through their facial expressions.</li>
<li>methods:  proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions.</li>
<li>results:  demonstrated the feasibility of assessing rats’ pain from their facial expressions and also verified the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem.I hope this helps!<details>
<summary>Abstract</summary>
In this letter, we aim to investigate whether laboratory rats' pain can be automatically assessed through their facial expressions. To this end, we began by presenting a publicly available dataset called RatsPain, consisting of 1,138 facial images captured from six rats that underwent an orthodontic treatment operation. Each rat' facial images in RatsPain were carefully selected from videos recorded either before or after the operation and well labeled by eight annotators according to the Rat Grimace Scale (RGS). We then proposed a novel deep learning method called PainSeeker for automatically assessing pain in rats via facial expressions. PainSeeker aims to seek pain-related facial local regions that facilitate learning both pain discriminative and head pose robust features from facial expression images. To evaluate the PainSeeker, we conducted extensive experiments on the RatsPain dataset. The results demonstrate the feasibility of assessing rats' pain from their facial expressions and also verify the effectiveness of the proposed PainSeeker in addressing this emerging but intriguing problem. The RasPain dataset can be freely obtained from https://github.com/xhzongyuan/RatsPain.
</details>
<details>
<summary>摘要</summary>
在这封信中，我们想 investigate 是否可以通过鼠标的表情自动评估它们的痛苦。为此，我们开始使用公共可用的 dataset called RatsPain，包含 1,138 个鼠标的面部图像，来自六只鼠标在 ortodontic 治疗操作后的视频记录。每只鼠标的面部图像在 RatsPain 中被精心选择并由八名注解者根据鼠标的抽筋scale (RGS) 进行了分类标注。然后，我们提出了一种新的深度学习方法 called PainSeeker，用于自动评估鼠标的痛苦程度 via 面部表情图像。PainSeeker 的目标是寻找痛苦相关的面部地方，以便从面部表情图像中学习痛苦特异和头 pose 稳定的特征。为了评估 PainSeeker，我们在 RatsPain 数据集上进行了广泛的实验。结果表明可以通过鼠标的面部表情评估它们的痛苦程度，并且证明了我们提出的 PainSeeker 可以有效地解决这个出现的问题。RatsPain 数据集可以免费下载于 https://github.com/xhzongyuan/RatsPain。
</details></li>
</ul>
<hr>
<h2 id="LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition"><a href="#LCPR-A-Multi-Scale-Attention-Based-LiDAR-Camera-Fusion-Network-for-Place-Recognition" class="headerlink" title="LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition"></a>LCPR: A Multi-Scale Attention-Based LiDAR-Camera Fusion Network for Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03198">http://arxiv.org/abs/2311.03198</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ZhouZijie77/LCPR">https://github.com/ZhouZijie77/LCPR</a></li>
<li>paper_authors: Zijie Zhou, Jingyi Xu, Guangming Xiong, Junyi Ma</li>
<li>for: 本研究旨在提高自动驾驶车辆在GPS无效环境中识别已经前期访问过的地点。</li>
<li>methods: 本研究使用多模态感知融合来超越个体感知器的不足之处。</li>
<li>results: 实验结果表明，我们的方法可以充分利用多视图相机和激光雷达数据来提高地点识别性能，同时具有强大的视角变化robustness。Here’s the English version of the summary for reference:</li>
<li>for: The purpose of this research is to improve the place recognition of autonomous vehicles in GPS-invalid environments.</li>
<li>methods: The study uses multimodal sensor fusion to overcome the limitations of individual sensors.</li>
<li>results: The experimental results show that our method can effectively utilize multi-view camera and LiDAR data to improve place recognition performance while maintaining strong robustness to viewpoint changes.<details>
<summary>Abstract</summary>
Place recognition is one of the most crucial modules for autonomous vehicles to identify places that were previously visited in GPS-invalid environments. Sensor fusion is considered an effective method to overcome the weaknesses of individual sensors. In recent years, multimodal place recognition fusing information from multiple sensors has gathered increasing attention. However, most existing multimodal place recognition methods only use limited field-of-view camera images, which leads to an imbalance between features from different modalities and limits the effectiveness of sensor fusion. In this paper, we present a novel neural network named LCPR for robust multimodal place recognition, which fuses LiDAR point clouds with multi-view RGB images to generate discriminative and yaw-rotation invariant representations of the environment. A multi-scale attention-based fusion module is proposed to fully exploit the panoramic views from different modalities of the environment and their correlations. We evaluate our method on the nuScenes dataset, and the experimental results show that our method can effectively utilize multi-view camera and LiDAR data to improve the place recognition performance while maintaining strong robustness to viewpoint changes. Our open-source code and pre-trained models are available at https://github.com/ZhouZijie77/LCPR .
</details>
<details>
<summary>摘要</summary>
固定位置识别是自动驾驶车辆最重要的模块之一，用于在GPS无效环境中识别之前访问过的地点。感知融合是一种有效的方法来超越个体感知器的缺陷。在过去几年，多模态固定位置识别方法已经吸引了增加的关注。然而，大多数现有的多模态固定位置识别方法只使用有限视场的相机图像，这会导致不同模态特征之间的均衡不良，限制感知融合的效iveness。在这篇论文中，我们提出了一种新的神经网络模型，名为LCPR，用于实现可靠的多模态固定位置识别。LCPR模型将LiDAR点云与多视角RGB图像融合，生成特征rich和旋转不变的环境表示。我们提出了一种多级注意力基于的混合模块，以便完全利用不同模态环境的全景视图和其相关性。我们在nuScenes数据集上进行了实验，结果表明，我们的方法可以有效地利用多视角相机和LiDAR数据，提高固定位置识别性能，同时保持强大的视点变化Robustness。我们的开源代码和预训练模型可以在https://github.com/ZhouZijie77/LCPR上获取。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification"><a href="#Few-shot-Learning-using-Data-Augmentation-and-Time-Frequency-Transformation-for-Time-Series-Classification" class="headerlink" title="Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification"></a>Few-shot Learning using Data Augmentation and Time-Frequency Transformation for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03194">http://arxiv.org/abs/2311.03194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Zhendong Pang, Jiangpeng Wang, Teng Li</li>
<li>for: 这 paper 的目的是解决时间序列分类任务中的少量数据问题，提出了一种基于数据扩展的几何学学习框架。</li>
<li>methods: 该方法使用了时间频率域的变换和随机绘制来生成synthetic图像，并开发了一种序列 спектрограм神经网络模型（SSNN），该模型由两个子网络组成：一个使用1D径向块来提取输入序列中的特征，另一个使用2D径向块来提取spectrogram表示中的特征。</li>
<li>results: 在一个amyotrophic lateral sclerosis（ALS）数据集和一个风力机 fault（WTF）数据集上进行了对 existed DNN 模型的比较研究，结果表明，我们提出的方法可以在几何学学习中提高时间序列分类的精度。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) that tackle the time series classification (TSC) task have provided a promising framework in signal processing. In real-world applications, as a data-driven model, DNNs are suffered from insufficient data. Few-shot learning has been studied to deal with this limitation. In this paper, we propose a novel few-shot learning framework through data augmentation, which involves transformation through the time-frequency domain and the generation of synthetic images through random erasing. Additionally, we develop a sequence-spectrogram neural network (SSNN). This neural network model composes of two sub-networks: one utilizing 1D residual blocks to extract features from the input sequence while the other one employing 2D residual blocks to extract features from the spectrogram representation. In the experiments, comparison studies of different existing DNN models with/without data augmentation are conducted on an amyotrophic lateral sclerosis (ALS) dataset and a wind turbine fault (WTF) dataset. The experimental results manifest that our proposed method achieves 93.75% F1 score and 93.33% accuracy on the ALS datasets while 95.48% F1 score and 95.59% accuracy on the WTF datasets. Our methodology demonstrates its applicability of addressing the few-shot problems for time series classification.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）在时间序列分类（TSC）任务中提供了一个有前途的框架，在实际应用中，作为数据驱动模型，DNNs受到了数据不充足的限制。几何学学习被研究以解决这个限制。在这篇论文中，我们提出了一种新的几何学学习框架，通过数据扩展，包括时间频域的变换和随机磁化生成的Synthetic图像。此外，我们开发了一种序列spectrogram神经网络（SSNN）。这个神经网络模型由两个子网络组成：一个使用1D residual块提取输入序列的特征，另一个使用2D residual块提取spectrogram表示的特征。在实验中，我们对不同的现有DNN模型进行了与/无数据扩展的比较研究，并在amyotrophic lateral sclerosis（ALS）数据集和风电机缺陷（WTF）数据集上进行了实验。实验结果表明，我们提出的方法在ALS数据集上达到了93.75%的F1分数和93.33%的准确率，在WTF数据集上达到了95.48%的F1分数和95.59%的准确率。我们的方法证明了其适用性于Addressing几何学学习问题。
</details></li>
</ul>
<hr>
<h2 id="Efficient-and-Low-Footprint-Object-Classification-using-Spatial-Contrast"><a href="#Efficient-and-Low-Footprint-Object-Classification-using-Spatial-Contrast" class="headerlink" title="Efficient and Low-Footprint Object Classification using Spatial Contrast"></a>Efficient and Low-Footprint Object Classification using Spatial Contrast</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03422">http://arxiv.org/abs/2311.03422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew Belding, Daniel C. Stumpp, Rajkumar Kubendran</li>
<li>for: 本研究探讨了一种基于事件的视觉感知器，使用本地化的空间对比（SC），并采用了两种阈值技术，相对阈值和绝对阈值。</li>
<li>methods: 本研究使用了虚拟模拟器来研究这种硬件感知器的可能性。此外，通过使用德国交通标志数据集（GTSRB）和知名的深度神经网络（DNN）进行交通标志分类，以评估空间对比的效果。</li>
<li>results: 研究发现，使用空间对比可以有效地捕捉图像中重要的特征，并且可以使用二进制micronet实现较大的减少输入数据量和内存资源（至少12倍），相比高精度RGB图像和DNN，只有小loss（约2%）。因此，SC在功能和资源有限的边缘计算环境中表现出了很大的抢夺。<details>
<summary>Abstract</summary>
Event-based vision sensors traditionally compute temporal contrast that offers potential for low-power and low-latency sensing and computing. In this research, an alternative paradigm for event-based sensors using localized spatial contrast (SC) under two different thresholding techniques, relative and absolute, is investigated. Given the slow maturity of spatial contrast in comparison to temporal-based sensors, a theoretical simulated output of such a hardware sensor is explored. Furthermore, we evaluate traffic sign classification using the German Traffic Sign dataset (GTSRB) with well-known Deep Neural Networks (DNNs). This study shows that spatial contrast can effectively capture salient image features needed for classification using a Binarized DNN with significant reduction in input data usage (at least 12X) and memory resources (17.5X), compared to high precision RGB images and DNN, with only a small loss (~2%) in macro F1-score. Binarized MicronNet achieves an F1-score of 94.4% using spatial contrast, compared to only 56.3% when using RGB input images. Thus, SC offers great promise for deployment in power and resource constrained edge computing environments.
</details>
<details>
<summary>摘要</summary>
traducción al chino simplificado:传统的事件基于视觉传感器通常计算时间异相，这对低功耗和低延迟感知和计算具有潜在的潜力。在这项研究中，我们提出了一种基于本地空间异相（SC）的事件基于传感器，并使用两种不同的阈值技术：相对和绝对阈值。由于空间异相比 temporal-based传感器更慢成熔，我们首先 theoretically  simulate 一个硬件传感器的输出。此外，我们使用德国交通标志数据集（GTSRB）进行交通标志分类，使用知名的深度神经网络（DNN）进行评估。结果表明，使用空间异相可以有效地捕捉图像中关键的特征，使用binarized DNN 进行分类，相比高精度 RGB 图像和 DNN，具有至少 12 倍的输入数据使用量和内存资源减少（17.5 倍），同时只失 2% 的 macro F1 score。binarized MicronNet 在使用空间异相时获得了 F1  score 94.4%，与使用 RGB 输入图像时相比，只有 56.3%。这表明 SC 具有在功率和资源受限的边缘计算环境中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs"><a href="#Frequency-Domain-Decomposition-Translation-for-Enhanced-Medical-Image-Translation-Using-GANs" class="headerlink" title="Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs"></a>Frequency Domain Decomposition Translation for Enhanced Medical Image Translation Using GANs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03175">http://arxiv.org/abs/2311.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuhui Wang, Jianwei Zuo, Xuliang Deng, Jiajia Luo</li>
<li>for: 这篇论文主要针对医学影像转换任务，尤其是运用GAN方法实现高品质的医学影像转换。</li>
<li>methods: 本研究提出了一新的频域分解转换方法（FDDT），它将原始影像分解为高频和低频部分，并将这两个部分转换为同频域的转换结果，以保持原始影像的身份信息，同时最小化影像的形式信息损失。</li>
<li>results: 在实验中，FDDT与多个主流基eline模型进行比较，结果显示，FDDT可以将Fr&#39;echet内部距离降低至24.4%、结构相似度降低至4.4%、峰值信号对比降低至5.8%和平均方差降低至31%，较前一方法降低23.7%、1.8%、6.8%和31.6%。<details>
<summary>Abstract</summary>
Medical Image-to-image translation is a key task in computer vision and generative artificial intelligence, and it is highly applicable to medical image analysis. GAN-based methods are the mainstream image translation methods, but they often ignore the variation and distribution of images in the frequency domain, or only take simple measures to align high-frequency information, which can lead to distortion and low quality of the generated images. To solve these problems, we propose a novel method called frequency domain decomposition translation (FDDT). This method decomposes the original image into a high-frequency component and a low-frequency component, with the high-frequency component containing the details and identity information, and the low-frequency component containing the style information. Next, the high-frequency and low-frequency components of the transformed image are aligned with the transformed results of the high-frequency and low-frequency components of the original image in the same frequency band in the spatial domain, thus preserving the identity information of the image while destroying as little stylistic information of the image as possible. We conduct extensive experiments on MRI images and natural images with FDDT and several mainstream baseline models, and we use four evaluation metrics to assess the quality of the generated images. Compared with the baseline models, optimally, FDDT can reduce Fr\'echet inception distance by up to 24.4%, structural similarity by up to 4.4%, peak signal-to-noise ratio by up to 5.8%, and mean squared error by up to 31%. Compared with the previous method, optimally, FDDT can reduce Fr\'echet inception distance by up to 23.7%, structural similarity by up to 1.8%, peak signal-to-noise ratio by up to 6.8%, and mean squared error by up to 31.6%.
</details>
<details>
<summary>摘要</summary>
医学图像转换是计算机视觉和生成人工智能领域的关键任务，并且具有广泛的应用前景。GAN基本方法是主流图像转换方法，但它们经常忽略图像在频率频谱中的变化和分布，或者只是使用简单的方法来对高频信息进行对齐，这可能导致图像生成的质量下降。为解决这些问题，我们提出了一种新的方法called频率频谱分解翻译（FDDT）。FDDT方法将原始图像分解成高频组件和低频组件，其中高频组件包含细节和标识信息，而低频组件包含风格信息。然后，将高频和低频组件的转换结果与原始图像的高频和低频组件在同一频率带的空间频谱中进行对齐，以保持图像的标识信息，同时尽量减少图像的风格信息损失。我们在MRI图像和自然图像上进行了广泛的实验，并使用了数个主流基eline模型进行比较。我们使用了四种评价指标来评价生成图像的质量，其中包括Fréchet吸引距离、结构相似度、峰值信号噪声比和平均平方误差。与基eline模型相比，FDDT可以最大化Fréchet吸引距离下降24.4%、结构相似度下降4.4%、峰值信号噪声比下降5.8%和平均平方误差下降31%。与之前的方法相比，FDDT可以最大化Fréchet吸引距离下降23.7%、结构相似度下降1.8%、峰值信号噪声比下降6.8%和平均平方误差下降31.6%。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models"><a href="#Asymmetric-Masked-Distillation-for-Pre-Training-Small-Foundation-Models" class="headerlink" title="Asymmetric Masked Distillation for Pre-Training Small Foundation Models"></a>Asymmetric Masked Distillation for Pre-Training Small Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03149">http://arxiv.org/abs/2311.03149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyu Zhao, Bingkun Huang, Sen Xing, Gangshan Wu, Yu Qiao, Limin Wang</li>
<li>for: 这个论文主要针对的是用自动编码器预训练小型视Transformer模型，以提高计算成本和适用范围。</li>
<li>methods: 该论文提出了一种新的偏向masked distillation（AMD）框架，用于预训练小型模型。AMD使用不同的掩码策略，让老师模型可以看到更多的上下文信息，而学生模型仍然保持高的掩码率。</li>
<li>results: AMD在IN1K dataset上 achiev 84.6%的分类精度，和在Something-in-Something V2 dataset上 achiev 73.3%的分类精度，比原始ViT-B模型提高3.7%。此外，AMD预训练模型也可以 transferred to 下游任务，并取得了一致的性能提升。<details>
<summary>Abstract</summary>
Self-supervised foundation models have shown great potential in computer vision thanks to the pre-training paradigm of masked autoencoding. Scale is a primary factor influencing the performance of these foundation models. However, these large foundation models often result in high computational cost that might limit their deployment. This paper focuses on pre-training relatively small vision transformer models that could be efficiently adapted to downstream tasks. Specifically, taking inspiration from knowledge distillation in model compression, we propose a new asymmetric masked distillation(AMD) framework for pre-training relatively small models with autoencoding. The core of AMD is to devise an asymmetric masking strategy, where the teacher model is enabled to see more context information with a lower masking ratio, while the student model still with high masking ratio to the original masked pre-training. We design customized multi-layer feature alignment between the teacher encoder and student encoder to regularize the pre-training of student MAE. To demonstrate the effectiveness and versatility of AMD, we apply it to both ImageMAE and VideoMAE for pre-training relatively small ViT models. AMD achieved 84.6% classification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3% classification accuracy using the ViT-B model on the Something-in-Something V2 dataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We also transfer AMD pre-trained models to downstream tasks and obtain consistent performance improvement over the standard pre-training.
</details>
<details>
<summary>摘要</summary>
自我监督基础模型在计算机视觉领域表现出了很大的潜力，这主要归功于预训练方法的遮盖自动编码。但是，这些大型基础模型经常会导致高计算成本，这可能会限制其部署。这篇论文关注预训练相对较小的视觉转换器模型，以实现高效地适应下游任务。我们提出了一种新的异 symmetry 遮盖（AMD）框架，用于预训练相对小型模型。AMD的核心思想是设计不同的遮盖策略，使得老师模型在低遮盖率下可以看到更多的上下文信息，而学生模型仍然保持高遮盖率。我们设计了特定的多层特征对Alignment来规范学生MAE的预训练。为了证明 AMD 的有效性和多样性，我们将其应用于 ImageMAE 和 VideoMAE 中的预训练相对小型 ViT 模型。在 IN1K 上，AMD 达到了 84.6% 的分类精度，使用 ViT-B 模型。在 Something-in-Something V2 数据集上，AMD 达到了 73.3% 的分类精度，相比标准预训练 ViT-B 模型提高了 3.7%。我们还将 AMD 预训练模型转移到下游任务上，并获得了一致的性能改进。
</details></li>
</ul>
<hr>
<h2 id="Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances"><a href="#Animating-NeRFs-from-Texture-Space-A-Framework-for-Pose-Dependent-Rendering-of-Human-Performances" class="headerlink" title="Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances"></a>Animating NeRFs from Texture Space: A Framework for Pose-Dependent Rendering of Human Performances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03140">http://arxiv.org/abs/2311.03140</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Knoll, Wieland Morgenstern, Anna Hilsmann, Peter Eisert</li>
<li>for: 本研究旨在提出一种基于NeRF的人体动作控制 renderering框架，以实现 pose-dependent 的人体表现。</li>
<li>methods: 本方法基于 NeRF 的渲染场，将场面绘制在 SMPL 人体模型上，并通过skeletal 关节参数来控制人体的动作表现。</li>
<li>results: 实验结果显示，本方法可以实现高质量的新视角和新姿势 synthesis，并且能够efficiently 学习并渲染 despite mapping ambiguities和Random visual variations。<details>
<summary>Abstract</summary>
Creating high-quality controllable 3D human models from multi-view RGB videos poses a significant challenge. Neural radiance fields (NeRFs) have demonstrated remarkable quality in reconstructing and free-viewpoint rendering of static as well as dynamic scenes. The extension to a controllable synthesis of dynamic human performances poses an exciting research question. In this paper, we introduce a novel NeRF-based framework for pose-dependent rendering of human performances. In our approach, the radiance field is warped around an SMPL body mesh, thereby creating a new surface-aligned representation. Our representation can be animated through skeletal joint parameters that are provided to the NeRF in addition to the viewpoint for pose dependent appearances. To achieve this, our representation includes the corresponding 2D UV coordinates on the mesh texture map and the distance between the query point and the mesh. To enable efficient learning despite mapping ambiguities and random visual variations, we introduce a novel remapping process that refines the mapped coordinates. Experiments demonstrate that our approach results in high-quality renderings for novel-view and novel-pose synthesis.
</details>
<details>
<summary>摘要</summary>
创建高质量可控3D人体模型从多视图RGB视频中提供了一个 significante挑战。神经辐射场（NeRF）已经表现出了remarkable的质量，可以重建和自由观点渲染静止和动态场景。在这篇论文中，我们介绍了一种基于NeRF的新的框架，用于基于pose的人体表现的可控渲染。在我们的方法中，辐射场被扭曲到了一个SMPL体幔网格上，创建了一个新的表面对应表示。我们的表示可以通过skeletal关节参数来动画，这些参数被传递给NeRF，以便根据pose来控制外观。为实现这一点，我们的表示包括UV坐标在Texture map上的对应2D坐标和查询点与网格之间的距离。为了实现高效的学习，我们引入了一种新的映射过程，用于修正映射的坐标。实验结果表明，我们的方法可以生成高质量的新视图和新pose синтеesis。
</details></li>
</ul>
<hr>
<h2 id="TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains"><a href="#TAMPAR-Visual-Tampering-Detection-for-Parcel-Logistics-in-Postal-Supply-Chains" class="headerlink" title="TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains"></a>TAMPAR: Visual Tampering Detection for Parcel Logistics in Postal Supply Chains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03124">http://arxiv.org/abs/2311.03124</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Naumann, Felix Hertlein, Laura Dörr, Kai Furmans</li>
<li>for: 本研究探讨了用于最后一英里配送的邮件检测 tampering 的方法，使用单个 RGB 图像与现有数据库中的参考图像进行比较，检测可能出现的外观变化。</li>
<li>methods: 本研究提议了一种 tampering 检测管道，利用锚点检测来确定包裹的八个角点，然后应用平行变换创建正规化的前视图，以便对包裹的每个可见面进行比较。</li>
<li>results: 实验结果表明，锚点检测和变换检测分别达到了 75.76% AP 和 81% 的准确率，F1 分数为 0.83，在实际图像中显示了良好的结果。此外，对不同的披雨、镜头偏角和检测方法进行了敏感性分析。<details>
<summary>Abstract</summary>
Due to the steadily rising amount of valuable goods in supply chains, tampering detection for parcels is becoming increasingly important. In this work, we focus on the use-case last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.
</details>
<details>
<summary>摘要</summary>
Due to the steadily rising amount of valuable goods in supply chains, detecting tampering for parcels has become increasingly important. In this work, we focus on the use-case of last-mile delivery, where only a single RGB image is taken and compared against a reference from an existing database to detect potential appearance changes that indicate tampering. We propose a tampering detection pipeline that utilizes keypoint detection to identify the eight corner points of a parcel. This permits applying a perspective transformation to create normalized fronto-parallel views for each visible parcel side surface. These viewpoint-invariant parcel side surface representations facilitate the identification of signs of tampering on parcels within the supply chain, since they reduce the problem to parcel side surface matching with pair-wise appearance change detection. Experiments with multiple classical and deep learning-based change detection approaches are performed on our newly collected TAMpering detection dataset for PARcels, called TAMPAR. We evaluate keypoint and change detection separately, as well as in a unified system for tampering detection. Our evaluation shows promising results for keypoint (Keypoint AP 75.76) and tampering detection (81% accuracy, F1-Score 0.83) on real images. Furthermore, a sensitivity analysis for tampering types, lens distortion, and viewing angles is presented. Code and dataset are available at https://a-nau.github.io/tampar.Here is the translation in Traditional Chinese:因为供应链中的高值货物量不断增加，该货物运输中的过程遗传检测已经变得非常重要。在这个工作中，我们专注在最后一英里的运输use case中，只有单一的RGB图像和现有数据库中的参考进行比较，以探测可能的外观变化，以探测遗传。我们提出了一个遗传检测管线，使用关键点检测来识别八个角点的货物。这允许我们将图像应用到对每个可见的货物侧面进行正规化平行投影。这些对货物侧面的投影具有视角不受影响的特性，因此可以将遗传检测问题降低到货物侧面匹配的对比检测。我们在多种古典和深度学习基于的变化检测方法上进行了实验，并使用了我们 newly collected TAMpering detection dataset for PARcels，called TAMPAR。我们分别评估了关键点和变化检测，以及它们在联合系统中的表现。我们的评估结果显示，关键点精度高（Keypoint AP 75.76）和遗传检测精度高（81%准确率、F1-Score 0.83）。此外，我们还进行了遗传类型、镜头扭曲和视角的敏感分析。代码和数据可以在https://a-nau.github.io/tampar上获取。
</details></li>
</ul>
<hr>
<h2 id="Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding"><a href="#Unified-Multi-modal-Unsupervised-Representation-Learning-for-Skeleton-based-Action-Understanding" class="headerlink" title="Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding"></a>Unified Multi-modal Unsupervised Representation Learning for Skeleton-based Action Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03106">http://arxiv.org/abs/2311.03106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huiguanlab/umurl">https://github.com/huiguanlab/umurl</a></li>
<li>paper_authors: Shengkai Sun, Daizong Liu, Jianfeng Dong, Xiaoye Qu, Junyu Gao, Xun Yang, Xun Wang, Meng Wang</li>
<li>for: 本研究旨在提出一种多模态无监督学习框架，以提高skeleton基于动作理解的robust性和效率。</li>
<li>methods: 本研究使用一种称为Unified Multimodal Unsupervised Representation Learning（UmURL）的方法，它通过早期融合策略将多 modal的特征编码在单流程中，从而降低模型复杂性。此外，本研究还提出了内部和外部一致性学习来保证多modal特征不受modal bias的影响。</li>
<li>results: 实验结果表明，UmURL可以具有高效率和低复杂性，同时在不同的下游任务场景中 achieve新的state-of-the-art表现。<details>
<summary>Abstract</summary>
Unsupervised pre-training has shown great success in skeleton-based action understanding recently. Existing works typically train separate modality-specific models, then integrate the multi-modal information for action understanding by a late-fusion strategy. Although these approaches have achieved significant performance, they suffer from the complex yet redundant multi-stream model designs, each of which is also limited to the fixed input skeleton modality. To alleviate these issues, in this paper, we propose a Unified Multimodal Unsupervised Representation Learning framework, called UmURL, which exploits an efficient early-fusion strategy to jointly encode the multi-modal features in a single-stream manner. Specifically, instead of designing separate modality-specific optimization processes for uni-modal unsupervised learning, we feed different modality inputs into the same stream with an early-fusion strategy to learn their multi-modal features for reducing model complexity. To ensure that the fused multi-modal features do not exhibit modality bias, i.e., being dominated by a certain modality input, we further propose both intra- and inter-modal consistency learning to guarantee that the multi-modal features contain the complete semantics of each modal via feature decomposition and distinct alignment. In this manner, our framework is able to learn the unified representations of uni-modal or multi-modal skeleton input, which is flexible to different kinds of modality input for robust action understanding in practical cases. Extensive experiments conducted on three large-scale datasets, i.e., NTU-60, NTU-120, and PKU-MMD II, demonstrate that UmURL is highly efficient, possessing the approximate complexity with the uni-modal methods, while achieving new state-of-the-art performance across various downstream task scenarios in skeleton-based action representation learning.
</details>
<details>
<summary>摘要</summary>
近些年来，无监督预训练在基于骨架的动作理解中得到了很大的成功。现有的方法通常是将不同的感知Modalities分开训练，然后通过较晚的融合策略进行动作理解。虽然这些方法已经实现了显著的性能提升，但它们受到复杂且重复的多流程模型设计的限制，每个模型都受到固定输入骨架模式的限制。为了解决这些问题，在本文中，我们提出了一种统一多模态无监督学习框架，名为 UmURL，它利用有效的早期融合策略来同时编码多个感知Modalities的多模态特征。具体来说，而不是为每个模式分别设计单modal无监督学习过程，我们将不同的模式输入feed到同一个流程中，并使用早期融合策略来学习它们的多模态特征，以降低模型复杂度。此外，为确保融合的多模态特征不受任何一个模式输入的干扰，我们还提出了内部和外部模式一致性学习，以保证每个模式的多模态特征含有完整的 semantics。因此，我们的框架能够学习单模态或多模态骨架输入的统一表示，这是对实际中不同类型的模式输入的灵活应用。我们在NTU-60、NTU-120和PKU-MMD II等三个大规模数据集上进行了广泛的实验，结果表明，UmURL具有高效性，与单 modal 方法相当，而实现了多个下游任务enario中的新的顶峰性能。
</details></li>
</ul>
<hr>
<h2 id="A-survey-and-classification-of-face-alignment-methods-based-on-face-models"><a href="#A-survey-and-classification-of-face-alignment-methods-based-on-face-models" class="headerlink" title="A survey and classification of face alignment methods based on face models"></a>A survey and classification of face alignment methods based on face models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03082">http://arxiv.org/abs/2311.03082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nordlinglab/facealignment-survey">https://github.com/nordlinglab/facealignment-survey</a></li>
<li>paper_authors: Jagmohan Meher, Hector Allende-Cid, Torbjörn E. M. Nordling</li>
<li>for: 这篇论文的目的是对不同类型的读者（ beginner、实践者和研究人员）提供关于面部对齐的综述，包括面部模型的解释和训练，以及将面部模型适应到新的face图像中。</li>
<li>methods: 这篇论文使用了多种面部模型，包括基于3D的面部模型和基于深度学习的方法。这些方法的训练和应用都有所不同，例如使用热图来表示面部特征。</li>
<li>results: 研究发现，在极大的面 pose 情况下，3D-based face models更为有效，而深度学习-based方法通常使用热图来表示面部特征。此外，文章还讨论了面部模型在面 alignment 领域的未来发展方向。<details>
<summary>Abstract</summary>
A face model is a mathematical representation of the distinct features of a human face. Traditionally, face models were built using a set of fiducial points or landmarks, each point ideally located on a facial feature, i.e., corner of the eye, tip of the nose, etc. Face alignment is the process of fitting the landmarks in a face model to the respective ground truth positions in an input image containing a face. Despite significant research on face alignment in the past decades, no review analyses various face models used in the literature. Catering to three types of readers - beginners, practitioners and researchers in face alignment, we provide a comprehensive analysis of different face models used for face alignment. We include the interpretation and training of the face models along with the examples of fitting the face model to a new face image. We found that 3D-based face models are preferred in cases of extreme face pose, whereas deep learning-based methods often use heatmaps. Moreover, we discuss the possible future directions of face models in the field of face alignment.
</details>
<details>
<summary>摘要</summary>
一个面模型是一个数学表示人脸的特征。传统上，面模型通过一组标准点或特征点建立，每个点理想位于人脸中的一个特征处，例如眼角或鼻头等。人脸对适应是将标准点在面模型与输入图像中的真实位置进行适应的过程。Despite significant research on face alignment in the past decades, no review has analyzed various face models used in the literature. To cater to three types of readers - beginners, practitioners, and researchers in face alignment, we provide a comprehensive analysis of different face models used for face alignment. We include the interpretation and training of the face models along with examples of fitting the face model to a new face image. We found that 3D-based face models are preferred in cases of extreme face pose, whereas deep learning-based methods often use heatmaps. Moreover, we discuss the possible future directions of face models in the field of face alignment.
</details></li>
</ul>
<hr>
<h2 id="CogVLM-Visual-Expert-for-Pretrained-Language-Models"><a href="#CogVLM-Visual-Expert-for-Pretrained-Language-Models" class="headerlink" title="CogVLM: Visual Expert for Pretrained Language Models"></a>CogVLM: Visual Expert for Pretrained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03079">http://arxiv.org/abs/2311.03079</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thudm/cogvlm">https://github.com/thudm/cogvlm</a></li>
<li>paper_authors: Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang</li>
<li>for: This paper presents a powerful open-source visual language foundation model called CogVLM, which aims to bridge the gap between frozen pretrained language models and image encoders.</li>
<li>methods: The CogVLM model uses a trainable visual expert module in the attention and FFN layers to enable deep fusion of vision language features without sacrificing any performance on NLP tasks.</li>
<li>results: The CogVLM-17B model achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA, and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B.<details>
<summary>Abstract</summary>
We introduce CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow alignment method which maps image features into the input space of language model, CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers. As a result, CogVLM enables deep fusion of vision language features without sacrificing any performance on NLP tasks. CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and ranks the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., surpassing or matching PaLI-X 55B. Codes and checkpoints are available at https://github.com/THUDM/CogVLM.
</details>
<details>
<summary>摘要</summary>
我们介绍CogVLM，一个强大的开源视觉语言基础模型。与流行的浅层对应方法不同，CogVLM通过在注意力和FFN层中添加可学习的视觉专家模块，将冰格预训练语言模型和图像编码器连接起来。这使得CogVLM可以深度融合视觉语言特征，不会失去任何表现力在NLPTasks中。CogVLM-17B在10个经典跨模态benchmark上达到了状态机器人的表现，包括NoCaps、Flicker30k captioning、RefCOCO、RefCOCO+、RefCOCOg、Visual7W、GQA、ScienceQA、VizWiz VQA和TDIUC，并在VQAv2、OKVQA、TextVQA、COCO captioning等排名第2，超过或匹配PaLI-X 55B。代码和Checkpoint可以在https://github.com/THUDM/CogVLM中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection"><a href="#A-Two-Stage-Generative-Model-with-CycleGAN-and-Joint-Diffusion-for-MRI-based-Brain-Tumor-Detection" class="headerlink" title="A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection"></a>A Two-Stage Generative Model with CycleGAN and Joint Diffusion for MRI-based Brain Tumor Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03074">http://arxiv.org/abs/2311.03074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhyjsiat/a-two-stage-cyclegan-ve-brats2020">https://github.com/zhyjsiat/a-two-stage-cyclegan-ve-brats2020</a></li>
<li>paper_authors: Wenxin Wang, Zhuo-Xu Cui, Guanxun Cheng, Chentao Cao, Xi Xu, Ziwei Liu, Haifeng Wang, Yulong Qi, Dong Liang, Yanjie Zhu<br>for:* 这个 paper 的目的是提高脑肿瘤检测和分类的精度。methods:* 本 paper 使用了两种方法：CycleGAN 和 VE-JP。CycleGAN 是在不对应的数据上训练的，以生成异常的图像作为数据先验。VE-JP 则是使用伪随机分布来重建健康的图像，并将伪随机分布与真正的分布融合在一起。results:* 本 paper 的结果显示，使用 TSGM 可以提高脑肿瘤检测和分类的精度。在 BraTs2020 数据集上，DSC 分数为 0.8590，在 ITCS 数据集上分数为 0.6226，在内部数据集上分数为 0.7403。这些结果显示 TSGM 的检测和分类性能较好，并且具有更好的扩展性。<details>
<summary>Abstract</summary>
Accurate detection and segmentation of brain tumors is critical for medical diagnosis. However, current supervised learning methods require extensively annotated images and the state-of-the-art generative models used in unsupervised methods often have limitations in covering the whole data distribution. In this paper, we propose a novel framework Two-Stage Generative Model (TSGM) that combines Cycle Generative Adversarial Network (CycleGAN) and Variance Exploding stochastic differential equation using joint probability (VE-JP) to improve brain tumor detection and segmentation. The CycleGAN is trained on unpaired data to generate abnormal images from healthy images as data prior. Then VE-JP is implemented to reconstruct healthy images using synthetic paired abnormal images as a guide, which alters only pathological regions but not regions of healthy. Notably, our method directly learned the joint probability distribution for conditional generation. The residual between input and reconstructed images suggests the abnormalities and a thresholding method is subsequently applied to obtain segmentation results. Furthermore, the multimodal results are weighted with different weights to improve the segmentation accuracy further. We validated our method on three datasets, and compared with other unsupervised methods for anomaly detection and segmentation. The DSC score of 0.8590 in BraTs2020 dataset, 0.6226 in ITCS dataset and 0.7403 in In-house dataset show that our method achieves better segmentation performance and has better generalization.
</details>
<details>
<summary>摘要</summary>
现代医学诊断中，检测和分类脑肿的精准性非常重要。然而，现有的指导学习方法需要大量的标注图像，而状态的艺术模型在无监督方法中经常无法覆盖整个数据分布。本文提出了一种新的框架Two-Stage Generative Model（TSGM），它结合了Cycling Generative Adversarial Network（CycleGAN）和变量爆发杂化方程（VE-JP）以提高脑肿检测和分类。CycleGAN在无对应数据上训练，将健康图像转换成病理图像作为数据先验。然后，VE-JP被实现以重建健康图像，使用生成的假病理图像作为引导，只有病理区域受到修改，而非健康区域。值得注意的是，我们的方法直接学习了联合概率分布 для条件生成。输入图像与重建图像之间的差异指示病理，并应用阈值方法以获取分 segmentation 结果。此外，我们使用多Modal的结果进行权重，以进一步提高分 segmentation 精度。我们在三个数据集上验证了我们的方法，并与其他无监督方法进行比较。BraTs2020 数据集的 DSC 分数为 0.8590，ITCS 数据集的 DSC 分数为 0.6226，In-house 数据集的 DSC 分数为 0.7403，表明我们的方法在 segmentation 性能方面表现出色，并且具有更好的泛化能力。
</details></li>
</ul>
<hr>
<h2 id="OrthoNets-Orthogonal-Channel-Attention-Networks"><a href="#OrthoNets-Orthogonal-Channel-Attention-Networks" class="headerlink" title="OrthoNets: Orthogonal Channel Attention Networks"></a>OrthoNets: Orthogonal Channel Attention Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03071">http://arxiv.org/abs/2311.03071</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hady1011/orthonets">https://github.com/hady1011/orthonets</a></li>
<li>paper_authors: Hadi Salman, Caleb Parks, Matthew Swan, John Gauch</li>
<li>for: 提高频道注意机制的效iveness，寻找一种lossy压缩方法以获得最佳特征表示。</li>
<li>methods: 使用 randomly initialized orthogonal filters 构建注意机制，并将其集成到 ResNet 中。</li>
<li>results: 在 Birds、MS-COCO 和 Places356  datasets 上比较表现出色，与 FcaNet 和其他注意机制相比。在 ImageNet  dataset 上与当前状态码头齐。<details>
<summary>Abstract</summary>
Designing an effective channel attention mechanism implores one to find a lossy-compression method allowing for optimal feature representation. Despite recent progress in the area, it remains an open problem. FcaNet, the current state-of-the-art channel attention mechanism, attempted to find such an information-rich compression using Discrete Cosine Transforms (DCTs). One drawback of FcaNet is that there is no natural choice of the DCT frequencies. To circumvent this issue, FcaNet experimented on ImageNet to find optimal frequencies. We hypothesize that the choice of frequency plays only a supporting role and the primary driving force for the effectiveness of their attention filters is the orthogonality of the DCT kernels. To test this hypothesis, we construct an attention mechanism using randomly initialized orthogonal filters. Integrating this mechanism into ResNet, we create OrthoNet. We compare OrthoNet to FcaNet (and other attention mechanisms) on Birds, MS-COCO, and Places356 and show superior performance. On the ImageNet dataset, our method competes with or surpasses the current state-of-the-art. Our results imply that an optimal choice of filter is elusive and generalization can be achieved with a sufficiently large number of orthogonal filters. We further investigate other general principles for implementing channel attention, such as its position in the network and channel groupings. Our code is publicly available at https://github.com/hady1011/OrthoNets/
</details>
<details>
<summary>摘要</summary>
设计有效的通道注意机制需要找到一种lossy压缩方法，以便获得优化的特征表示。尽管最近在这个领域的进展不断，但这问题仍然未得到解决。FcaNet，当前领先的通道注意机制，尝试使用Discrete Cosine Transforms（DCTs）来找到这样的信息充足压缩。FcaNet的一个缺点是没有自然的DCT频率选择。为了解决这个问题，FcaNet在ImageNet上进行了实验。我们假设，选择的频率只是支持性的角色，主要的驱动力是DCT核函数的正交性。为了测试这个假设，我们构建了一个使用随机初始化的正交滤波器的注意机制。将这个机制 integrate into ResNet，我们创建了OrthoNet。我们与FcaNet（以及其他注意机制）在Birds、MS-COCO和Places356上进行比较，并显示了超越性。在ImageNet dataset上，我们的方法与当前领先的状态相竞争。我们的结果表明，优化的筛选器是极其困难的，但通过一个足够大的正交滤波器数量，可以实现总体的泛化。我们进一步调查了其他实现通道注意的一般原则，如其在网络中的位置和通道分组。我们的代码可以在https://github.com/hady1011/OrthoNets/上获取。
</details></li>
</ul>
<hr>
<h2 id="Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning"><a href="#Forest-aboveground-biomass-estimation-using-GEDI-and-earth-observation-data-through-attention-based-deep-learning" class="headerlink" title="Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning"></a>Forest aboveground biomass estimation using GEDI and earth observation data through attention-based deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03067">http://arxiv.org/abs/2311.03067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenquan Dong, Edward T. A. Mitchard, Hao Yu, Steven Hancock, Casey M. Ryan<br>for: 本研究旨在使用卫星数据进行森林上空生物质（AGB）估算，以了解气候变化下的碳账户。methods: 本研究使用了开源的卫星数据，包括GEDI LiDAR数据、C频段Sentinel-1 SAR数据、ALOS-2 PALSAR-2数据和Sentinel-2多спектраль数据，并采用了注意力基于深度学习模型（AU）进行AGB估算。results: 相比传统的RF算法，AU模型在AGB估算中显示了明显的高精度。AU模型的R2为0.66，RMSE为43.66亿分之一，偏差为0.14亿分之一，而RF的R2为0.62，RMSE为45.87亿分之一，偏差为1.09亿分之一。然而，深度学习方法的优势不uniform地出现在所有测试模型中。ResNet101只有R2为0.50，RMSE为52.93亿分之一，偏差为0.99亿分之一，而UNet Reported R2为0.65，RMSE为44.28亿分之一，并且显示了较大的偏差（1.84亿分之一）。此外，为了探讨AU在不含空间信息的情况下的性能，FC层被使用，以消除卫星数据中的空间信息。AU-FC实现了中间的R2为0.64，RMSE为44.92亿分之一，偏差为-0.56亿分之一，超过了RF，但是下过AU模型使用空间信息。<details>
<summary>Abstract</summary>
Accurate quantification of forest aboveground biomass (AGB) is critical for understanding carbon accounting in the context of climate change. In this study, we presented a novel attention-based deep learning approach for forest AGB estimation, primarily utilizing openly accessible EO data, including: GEDI LiDAR data, C-band Sentinel-1 SAR data, ALOS-2 PALSAR-2 data, and Sentinel-2 multispectral data. The attention UNet (AU) model achieved markedly higher accuracy for biomass estimation compared to the conventional RF algorithm. Specifically, the AU model attained an R2 of 0.66, RMSE of 43.66 Mg ha-1, and bias of 0.14 Mg ha-1, while RF resulted in lower scores of R2 0.62, RMSE 45.87 Mg ha-1, and bias 1.09 Mg ha-1. However, the superiority of the deep learning approach was not uniformly observed across all tested models. ResNet101 only achieved an R2 of 0.50, an RMSE of 52.93 Mg ha-1, and a bias of 0.99 Mg ha-1, while the UNet reported an R2 of 0.65, an RMSE of 44.28 Mg ha-1, and a substantial bias of 1.84 Mg ha-1. Moreover, to explore the performance of AU in the absence of spatial information, fully connected (FC) layers were employed to eliminate spatial information from the remote sensing data. AU-FC achieved intermediate R2 of 0.64, RMSE of 44.92 Mgha-1, and bias of -0.56 Mg ha-1, outperforming RF but underperforming AU model using spatial information. We also generated 10m forest AGB maps across Guangdong for the year 2019 using AU and compared it with that produced by RF. The AGB distributions from both models showed strong agreement with similar mean values; the mean forest AGB estimated by AU was 102.18 Mg ha-1 while that of RF was 104.84 Mg ha-1. Additionally, it was observed that the AGB map generated by AU provided superior spatial information. Overall, this research substantiates the feasibility of employing deep learning for biomass estimation based on satellite data.
</details>
<details>
<summary>摘要</summary>
“精确量化森林上空生物质量（AGB）是对于气候变化的理解 critical。在本研究中，我们提出了一种新的注意力基于深度学习方法来估算森林AGB，主要使用开放 accessible 的 Earth observation（EO）数据，包括：GEDI LiDAR数据、C-band Sentinel-1 SAR数据、ALOS-2 PALSAR-2数据和Sentinel-2多spectral数据。我们的注意力UNet（AU）模型在比较传统RF算法时表现出了明显的高准确性，具体来说，AU模型在R2、RMSE和偏差方面均有所提高，具体来说，AU模型的R2为0.66，RMSE为43.66 Mg ha-1，偏差为0.14 Mg ha-1，而RF的R2为0.62，RMSE为45.87 Mg ha-1，偏差为1.09 Mg ha-1。然而，深度学习方法的优势不是所有模型中均能实现。ResNet101只有R2为0.50，RMSE为52.93 Mg ha-1，偏差为0.99 Mg ha-1，而UNet的R2为0.65，RMSE为44.28 Mg ha-1，偏差为1.84 Mg ha-1。此外，为了探讨AU的表现在没有空间信息的情况下，我们运用了全连接（FC）层来消除 remote sensing 数据中的空间信息。AU-FC获得了中位R2值0.64，RMSE值44.92 Mg ha-1，偏差值-0.56 Mg ha-1，比RF表现更好，但比AU模型使用空间信息的表现下降。我们还使用AU生成了2019年在广东省的10米森林AGB地图，并与RF生成的地图进行比较。AU的AGB分布和RF的AGB分布都有相似的平均值，AU的AGB估算值为102.18 Mg ha-1，RF的AGB估算值为104.84 Mg ha-1。此外，AU生成的AGB地图提供了更好的空间信息。总之，这项研究证明了深度学习可以实现基于卫星数据的生物质量估算。”
</details></li>
</ul>
<hr>
<h2 id="AnyText-Multilingual-Visual-Text-Generation-And-Editing"><a href="#AnyText-Multilingual-Visual-Text-Generation-And-Editing" class="headerlink" title="AnyText: Multilingual Visual Text Generation And Editing"></a>AnyText: Multilingual Visual Text Generation And Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03054">http://arxiv.org/abs/2311.03054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, Xuansong Xie</li>
<li>for: 这个论文主要 targets 的问题是如何使用扩散模型生成高质量的文本图像，尤其是在文本区域上。</li>
<li>methods: 该论文提出了一种基于扩散模型的多语言视觉文本生成和编辑模型，称为AnyText，它可以准确地渲染文本在图像中，并且可以在多种语言中生成文本。</li>
<li>results: 该论文通过对多种语言的文本图像进行评测，得出了与其他方法相比的显著性能优势。此外，该论文还提供了一个大规模的多语言文本图像集合（AnyWord-3M）和一个评测标准（AnyText-benchmark），以便进一步推动文本生成技术的发展。<details>
<summary>Abstract</summary>
Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model, that focuses on rendering accurate and coherent text in the image. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. We employed text-control diffusion loss and text perceptual loss for training to further enhance writing accuracy. AnyText can write characters in multiple languages, to the best of our knowledge, this is the first work to address multilingual visual text generation. It is worth mentioning that AnyText can be plugged into existing diffusion models from the community for rendering or editing text accurately. After conducting extensive evaluation experiments, our method has outperformed all other approaches by a significant margin. Additionally, we contribute the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations in multiple languages. Based on AnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual text generation accuracy and quality. Our project will be open-sourced on https://github.com/tyxsspa/AnyText to improve and promote the development of text generation technology.
</details>
<details>
<summary>摘要</summary>
Diffusion模型基于Text-to-Image技术在最近几年内具有很高的成就。虽然目前的图像生成技术非常高级，可以生成高质量的图像，但是当注意力集中在生成图像中的文本区域时，仍然可以发现问题。为解决这个问题，我们介绍了AnyText，一种基于扩散的多语言视觉文本生成和编辑模型，它专注于在图像中准确和一致地生成文本。AnyText包括一个扩散管道，其中有两个主要元素：一个辅助隐藏模块和一个文本嵌入模块。前者使用文本字形、位置和遮盖图像作为输入，生成文本生成或编辑的隐藏特征。后者使用一个OCR模型将字roke数据编码为嵌入，这些嵌入与图像标签的嵌入结合生成文本，以便文本与背景融合。我们在训练时使用文本扩散损失和文本感知损失，以进一步提高文本准确性。AnyText可以在多种语言中写字，据我们所知，这是首次对多语言视觉文本生成进行了研究。此外，AnyText可以与现有的扩散模型集成，以提供更高质量的文本生成和编辑功能。经过广泛的评估实验，我们的方法在所有其他方法之上减分了较大的差距。此外，我们还提供了首个大规模的多语言文本图像集，AnyWord-3M，包含300万个图像文本对，其中每个对包含多种语言的OCR注解。基于AnyWord-3M集，我们提出了AnyText-benchmark，用于评估视觉文本生成准确性和质量。我们的项目将在https://github.com/tyxsspa/AnyText上开源，以促进和提高文本生成技术的发展。
</details></li>
</ul>
<hr>
<h2 id="MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification"><a href="#MixUp-MIL-A-Study-on-Linear-Multilinear-Interpolation-Based-Data-Augmentation-for-Whole-Slide-Image-Classification" class="headerlink" title="MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification"></a>MixUp-MIL: A Study on Linear &amp; Multilinear Interpolation-Based Data Augmentation for Whole Slide Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03052">http://arxiv.org/abs/2311.03052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gadermayr, Lukas Koller, Maximilian Tschuchnig, Lea Maria Stangassinger, Christina Kreutzer, Sebastien Couillard-Despres, Gertie Janneke Oostingh, Anton Hittmair</li>
<li>for: 本研究旨在 investigate linear and multilinear interpolation between feature vectors, a data augmentation technique, 以提高分类网络和多例学习的泛化性性能。</li>
<li>methods: 本研究使用了多例学习方法，并对10个不同的数据集配置和两种特征提取方法（批处理和自动提取）进行了研究。</li>
<li>results: 研究发现了Extraordinarily high variability in the effect of the method, 并发现了一些有趣的方向，提出了一些新的研究方向。<details>
<summary>Abstract</summary>
For classifying digital whole slide images in the absence of pixel level annotation, typically multiple instance learning methods are applied. Due to the generic applicability, such methods are currently of very high interest in the research community, however, the issue of data augmentation in this context is rarely explored. Here we investigate linear and multilinear interpolation between feature vectors, a data augmentation technique, which proved to be capable of improving the generalization performance classification networks and also for multiple instance learning. Experiments, however, have been performed on only two rather small data sets and one specific feature extraction approach so far and a strong dependence on the data set has been identified. Here we conduct a large study incorporating 10 different data set configurations, two different feature extraction approaches (supervised and self-supervised), stain normalization and two multiple instance learning architectures. The results showed an extraordinarily high variability in the effect of the method. We identified several interesting aspects to bring light into the darkness and identified novel promising fields of research.
</details>
<details>
<summary>摘要</summary>
For 分类数字整幅图像在缺乏像素级别标注的情况下，通常使用多个实例学习方法。由于这种方法的通用性，目前在研究community中具有很高的兴趣，但数据扩充在这个上下文中的问题rarely explored。我们在这里调查了线性和多线性 interpolate between feature vectors，一种数据扩充技术， Proof to be capable of improving the generalization performance of classification networks and also for multiple instance learning。经过实验，但只在两个较小的数据集和一种特定的特征提取方法上进行了测试，并且数据集的依赖性被识别出来。我们在这里进行了大规模的研究，包括10个不同的数据集配置、两种不同的特征提取方法（supervised和self-supervised）、染料normalization和两种多个实例学习架构。结果显示了极高的变化性，我们identified several interesting aspects to bring light into the darkness and identified novel promising fields of research.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving"><a href="#COLA-COarse-LAbel-multi-source-LiDAR-semantic-segmentation-for-autonomous-driving" class="headerlink" title="COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving"></a>COLA: COarse-LAbel multi-source LiDAR semantic segmentation for autonomous driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03017">http://arxiv.org/abs/2311.03017</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jules Sanchez, Jean-Emmanuel Deschaud, François Goulette</li>
<li>for: 这paper是为了提高LiDAR semantic segmentation的自动驾驶而写的。</li>
<li>methods: 这paper使用了多源训练方法，利用了多个数据集在训练时使用。</li>
<li>results: 这paper实现了对域泛化、源到源分类和预训练等三个子领域的系统改进，并在这些领域中达到了最高的性能 (+10%、+5.3% 和 +12% 分别).<details>
<summary>Abstract</summary>
LiDAR semantic segmentation for autonomous driving has been a growing field of interest in the past few years. Datasets and methods have appeared and expanded very quickly, but methods have not been updated to exploit this new availability of data and continue to rely on the same classical datasets.   Different ways of performing LIDAR semantic segmentation training and inference can be divided into several subfields, which include the following: domain generalization, the ability to segment data coming from unseen domains ; source-to-source segmentation, the ability to segment data coming from the training domain; and pre-training, the ability to create re-usable geometric primitives.   In this work, we aim to improve results in all of these subfields with the novel approach of multi-source training. Multi-source training relies on the availability of various datasets at training time and uses them together rather than relying on only one dataset.   To overcome the common obstacles found for multi-source training, we introduce the coarse labels and call the newly created multi-source dataset COLA. We propose three applications of this new dataset that display systematic improvement over single-source strategies: COLA-DG for domain generalization (up to +10%), COLA-S2S for source-to-source segmentation (up to +5.3%), and COLA-PT for pre-training (up to +12%).
</details>
<details>
<summary>摘要</summary>
隐藏的文本：LiDARSemantic分类对于自动驾驶 field of interest在过去几年来快速增长。dataset和方法快速出现和扩展，但方法没有更新以利用这些新的数据，仍然 rely on classical datasets。不同的方法在 LiDARSemantic分类训练和推理中的几个子领域中进行不同的方式，包括：领域泛化、来自训练领域的数据分类和预训练。在这项工作中，我们目的是提高所有这些子领域的结果，使用新的多源训练方法。翻译结果：LiDAR Semantic分类在自动驾驶领域的过去几年来快速增长。不同的dataset和方法快速出现和扩展，但方法没有更新以利用这些新的数据，仍然 rely on classical datasets。在 LiDAR Semantic分类训练和推理中，有几个不同的方法，包括领域泛化、来自训练领域的数据分类和预训练。在这项工作中，我们目的是通过新的多源训练方法提高所有这些子领域的结果。具体来说，我们提出了一种新的多源训练方法，称为COLA。COLA方法利用了不同的dataset在训练时的共同使用，而不是仅仅靠待训练的 dataset。为了解决多源训练中常见的障碍，我们引入了粗略标签，并创建了一个新的多源dataset，称为COLA。我们提出了三个COLAdataset的应用， Display systematic improvement over single-source strategies：COLA-DG for domain generalization (up to +10%), COLA-S2S for source-to-source segmentation (up to +5.3%), and COLA-PT for pre-training (up to +12%).
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting"><a href="#Exploring-the-Capability-of-Text-to-Image-Diffusion-Models-with-Structural-Edge-Guidance-for-Multi-Spectral-Satellite-Image-Inpainting" class="headerlink" title="Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting"></a>Exploring the Capability of Text-to-Image Diffusion Models with Structural Edge Guidance for Multi-Spectral Satellite Image Inpainting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03008">http://arxiv.org/abs/2311.03008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikolaj Czerkawski, Christos Tachtatzis</li>
<li>for: 这个论文研究了卫星图像数据中的文本到图像填充模型的实用性。</li>
<li>methods: 论文提出了一种基于StableDiffusion和ControlNet的新填充框架，以及一种RGB到多spectral射频（MSI）转换方法。</li>
<li>results: 实验结果表明，通过StableDiffusion进行填充可能会出现不 DESirable的artefacts，而self-supervised internal inpainting的简单实现可以 achieve higher quality of synthesis。<details>
<summary>Abstract</summary>
The paper investigates the utility of text-to-image inpainting models for satellite image data. Two technical challenges of injecting structural guiding signals into the generative process as well as translating the inpainted RGB pixels to a wider set of MSI bands are addressed by introducing a novel inpainting framework based on StableDiffusion and ControlNet as well as a novel method for RGB-to-MSI translation. The results on a wider set of data suggest that the inpainting synthesized via StableDiffusion suffers from undesired artefacts and that a simple alternative of self-supervised internal inpainting achieves higher quality of synthesis.
</details>
<details>
<summary>摘要</summary>
文章研究文本到图像填充模型在卫星图像数据中的可用性。两个技术挑战：在生成过程中插入结构导向信号以及将RGB像素翻译到更广泛的MSI频谱上——通过介绍一种基于StableDiffusion和ControlNet的新填充框架以及一种RGB-to-MSI翻译方法。研究结果表明，通过StableDiffusion进行填充会产生不良artefacts，而自动内部填充的简单方法可以达到更高质量的synthesis。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition"><a href="#Zero-Shot-Enhancement-of-Low-Light-Image-Based-on-Retinex-Decomposition" class="headerlink" title="Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition"></a>Zero-Shot Enhancement of Low-Light Image Based on Retinex Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02995">http://arxiv.org/abs/2311.02995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenchao Li, Bangshu Xiong, Qiaofeng Ou, Xiaoyun Long, Jinhao Zhu, Jiabao Chen, Shuyuan Wen</li>
<li>For: Zero-shot low-light image enhancement* Methods: Learning-based Retinex decomposition with N-Net network, noise loss term, RI-Net, texture loss term, and segmented smoothing loss* Results: Improved generalization performance with a homemade real-life low-light dataset and advanced vision tasks such as face detection, target recognition, and instance segmentation, competitive performance compared to current state-of-the-art methods.Here is the Simplified Chinese translation of the three key information points:* For: 低光照图像改善* Methods: 基于学习的Retinex分解方法，包括N-Net网络、噪声损失项、RI-Net、тексту准确损失项和分割缓和损失项* Results: 改进了基于自己实际低光照 dataset 和高级视觉任务（如人脸检测、目标识别和实例分割）的普适性，与当前状态级方法竞争。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/liwenchao0615/ZERRINNet">https://github.com/liwenchao0615/ZERRINNet</a><details>
<summary>Abstract</summary>
Two difficulties here make low-light image enhancement a challenging task; firstly, it needs to consider not only luminance restoration but also image contrast, image denoising and color distortion issues simultaneously. Second, the effectiveness of existing low-light enhancement methods depends on paired or unpaired training data with poor generalization performance.   To solve these difficult problems, we propose in this paper a new learning-based Retinex decomposition of zero-shot low-light enhancement method, called ZERRINNet. To this end, we first designed the N-Net network, together with the noise loss term, to be used for denoising the original low-light image by estimating the noise of the low-light image. Moreover, RI-Net is used to estimate the reflection component and illumination component, and in order to solve the color distortion and contrast, we use the texture loss term and segmented smoothing loss to constrain the reflection component and illumination component. Finally, our method is a zero-reference enhancement method that is not affected by the training data of paired and unpaired datasets, so our generalization performance is greatly improved, and in the paper, we have effectively validated it with a homemade real-life low-light dataset and additionally with advanced vision tasks, such as face detection, target recognition, and instance segmentation. We conducted comparative experiments on a large number of public datasets and the results show that the performance of our method is competitive compared to the current state-of-the-art methods. The code is available at:https://github.com/liwenchao0615/ZERRINNet
</details>
<details>
<summary>摘要</summary>
两个问题使低光照图像增强成为一项困难任务：首先，它需要同时考虑照度恢复、图像对比度、雷达噪声和色偏移问题。其次，现有的低光照增强方法的效果取决于对照或无照训练数据的学习，而且对泛化性表现不佳。为解决这些困难问题，我们在本文提出了一种新的学习基于Retinex分解的零参数低光照增强方法，称为ZERRINNet。为此，我们首先设计了N-Net网络，并与噪声损失项一起使用来降噪原始低光照图像。此外，我们使用RI-Net来估计反射组件和照明组件，以解决颜色扭曲和对比度问题。最后，我们的方法是一种零参考增强方法，不受训练数据的对照或无照数据的影响，因此我们的泛化性得到了大幅提高。在文章中，我们有效地验证了我们的方法，使用自己制作的真实生活低光照数据，以及高级视觉任务，如人脸检测、目标识别和实例分割。我们对大量公共数据进行了比较实验，结果显示了我们的方法与当前状态艺技术的竞争力。代码可以在：https://github.com/liwenchao0615/ZERRINNet 获取。
</details></li>
</ul>
<hr>
<h2 id="NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection"><a href="#NEURO-HAND-A-weakly-supervised-Hierarchical-Attention-Network-for-neuroimaging-abnormality-Detection" class="headerlink" title="NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection"></a>NEURO HAND: A weakly supervised Hierarchical Attention Network for neuroimaging abnormality Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02992">http://arxiv.org/abs/2311.02992</a></li>
<li>repo_url: None</li>
<li>paper_authors: David A. Wood</li>
<li>for: 这个论文是用于检测临床神经成像数据中的异常的。</li>
<li>methods: 这个方法使用了层次注意力网络，适用于非体积数据（即高分辨率MRI扫描序列），并可以从二分类评估级别标签进行训练。</li>
<li>results: 该方法可以提高分类精度，并提供可解释性，可以通过粗略的扫描水平和镜像级别异常Localization，或者给出不同扫描和序列的重要性分数，使其适用于自动化Radiology部门的检测系统。<details>
<summary>Abstract</summary>
Clinical neuroimaging data is naturally hierarchical. Different magnetic resonance imaging (MRI) sequences within a series, different slices covering the head, and different regions within each slice all confer different information. In this work we present a hierarchical attention network for abnormality detection using MRI scans obtained in a clinical hospital setting. The proposed network is suitable for non-volumetric data (i.e. stacks of high-resolution MRI slices), and can be trained from binary examination-level labels. We show that this hierarchical approach leads to improved classification, while providing interpretability through either coarse inter- and intra-slice abnormality localisation, or giving importance scores for different slices and sequences, making our model suitable for use as an automated triaging system in radiology departments.
</details>
<details>
<summary>摘要</summary>
临床神经成像数据自然归于层次结构。不同的磁共振成像（MRI）序列内一系列、不同的脑部slice覆盖头部、和每个slice中的不同区域都提供不同的信息。在这种工作中，我们提出了一种层次注意力网络用于使用MRI扫描图像进行异常检测。该提案的网络适用于非材料数据（即高分辨率MRI扫描图像的栈），可以从二进制检查级别标签进行训练。我们表明，这种层次方法可以提高分类性能，同时提供可读性通过每个slice和每个序列的重要性分数或者粗略的脑部异常定位。因此，我们的模型适用于辐射部门中的自动检测系统。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding"><a href="#Diffusion-based-Radiotherapy-Dose-Prediction-Guided-by-Inter-slice-Aware-Structure-Encoding" class="headerlink" title="Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding"></a>Diffusion-based Radiotherapy Dose Prediction Guided by Inter-slice Aware Structure Encoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02991">http://arxiv.org/abs/2311.02991</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenghao Feng, Lu Wen, Jianghong Xiao, Yuanyuan Xu, Xi Wu, Jiliu Zhou, Xingchen Peng, Yan Wang</li>
<li>for: 这篇论文的目的是为了提高放射治疗规划中的剂量分布预测，并且解决现有方法的过滤问题。</li>
<li>methods: 这篇论文提出了一个扩散模型基本的方法（DiffDose），它包括一个前进过程和一个反向过程。在前进过程中，DiffDose将剂量分布图transform为纯 Gaussian 噪声，并且同时训练一个噪声预测器来估计附加的噪声。在反向过程中，它逐步除去附加的噪声，最终输出预测的剂量分布图。</li>
<li>results: 这篇论文的结果显示，DiffDose 方法可以很好地解决现有方法的过滤问题，并且可以提高放射治疗规划中的剂量分布预测精度。<details>
<summary>Abstract</summary>
Deep learning (DL) has successfully automated dose distribution prediction in radiotherapy planning, enhancing both efficiency and quality. However, existing methods suffer from the over-smoothing problem for their commonly used L1 or L2 loss with posterior average calculations. To alleviate this limitation, we propose a diffusion model-based method (DiffDose) for predicting the radiotherapy dose distribution of cancer patients. Specifically, the DiffDose model contains a forward process and a reverse process. In the forward process, DiffDose transforms dose distribution maps into pure Gaussian noise by gradually adding small noise and a noise predictor is simultaneously trained to estimate the noise added at each timestep. In the reverse process, it removes the noise from the pure Gaussian noise in multiple steps with the well-trained noise predictor and finally outputs the predicted dose distribution maps...
</details>
<details>
<summary>摘要</summary>
深度学习（DL）已成功地自动预测辐射治疗规划中的剂量分布，提高了效率和质量。然而，现有方法受到L1或L2损失函数中的平均 posterior 计算的限制。为了解决这些限制，我们提出了基于扩散模型的剂量分布预测方法（DiffDose）。具体来说，DiffDose模型包括一个前向过程和一个反向过程。在前向过程中，DiffDose将剂量分布图像转化为纯 Gaussian 噪声，逐步添加小噪声，并同时训练噪声预测器来估计添加的噪声。在反向过程中，它将纯 Gaussian 噪声中的噪声除掉，并在多个步骤中使用已经良好训练的噪声预测器来除掉噪声，最终输出预测的剂量分布图像。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination"><a href="#Understanding-Deep-Representation-Learning-via-Layerwise-Feature-Compression-and-Discrimination" class="headerlink" title="Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination"></a>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02960">http://arxiv.org/abs/2311.02960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, Qing Qu</li>
<li>for: 本研究目的是探讨深度学习网络中层次特征学习的机制。</li>
<li>methods: 本研究使用深度线性网络来探讨输入数据的转化。</li>
<li>results: 研究发现，深度线性网络中每层的输出都会具有 géometric 率的内类压缩和 linear 率的 между类分化。这种特征演化的 Pattern 在深度网络中具有可衡量的特征，并且在实际应用中有重要的实质性。<details>
<summary>Abstract</summary>
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at \url{https://github.com/Heimine/PNC_DLN}.
</details>
<details>
<summary>摘要</summary>
Motivated by our findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems.To achieve this, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank. Specifically, each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through.To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks, which aligns well with recent empirical studies.Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at \url{https://github.com/Heimine/PNC_DLN}.
</details></li>
</ul>
<hr>
<h2 id="Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images"><a href="#Multi-view-learning-for-automatic-classification-of-multi-wavelength-auroral-images" class="headerlink" title="Multi-view learning for automatic classification of multi-wavelength auroral images"></a>Multi-view learning for automatic classification of multi-wavelength auroral images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02947">http://arxiv.org/abs/2311.02947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuju Yang, Hang Su, Lili Liu, Yixuan Wang, Ze-Jun Hu</li>
<li>for:  auroral classification, polar research</li>
<li>methods:  lightweight feature extraction backbone (LCTNet), multi-scale reconstructed feature module (MSRM), lightweight attention feature enhancement module (LAFE)</li>
<li>results:  state-of-the-art classification accuracy, superior results in terms of accuracy and computational efficiency compared to existing multi-view methods<details>
<summary>Abstract</summary>
Auroral classification plays a crucial role in polar research. However, current auroral classification studies are predominantly based on images taken at a single wavelength, typically 557.7 nm. Images obtained at other wavelengths have been comparatively overlooked, and the integration of information from multiple wavelengths remains an underexplored area. This limitation results in low classification rates for complex auroral patterns. Furthermore, these studies, whether employing traditional machine learning or deep learning approaches, have not achieved a satisfactory trade-off between accuracy and speed. To address these challenges, this paper proposes a lightweight auroral multi-wavelength fusion classification network, MLCNet, based on a multi-view approach. Firstly, we develop a lightweight feature extraction backbone, called LCTNet, to improve the classification rate and cope with the increasing amount of auroral observation data. Secondly, considering the existence of multi-scale spatial structures in auroras, we design a novel multi-scale reconstructed feature module named MSRM. Finally, to highlight the discriminative information between auroral classes, we propose a lightweight attention feature enhancement module called LAFE. The proposed method is validated using observational data from the Arctic Yellow River Station during 2003-2004. Experimental results demonstrate that the fusion of multi-wavelength information effectively improves the auroral classification performance. In particular, our approach achieves state-of-the-art classification accuracy compared to previous auroral classification studies, and superior results in terms of accuracy and computational efficiency compared to existing multi-view methods.
</details>
<details>
<summary>摘要</summary>
极光分类对极地研究起到关键作用，但现有的极光分类研究主要基于单一波长的图像，通常为557.7纳米。其他波长的图像尚未得到足够的关注，而多波长信息的集成仍然是一个未发掘的领域。这种局限性导致复杂的极光图像分类率较低。此外，这些研究，无论使用传统机器学习还是深度学习方法，都没有实现满意的准确率和速度协调。为解决这些挑战，本文提出了一种轻量级的极光多波长融合分类网络（MLCNet），基于多视图方法。首先，我们开发了一种轻量级的特征提取背bone（LCTNet），以提高分类率并处理逐渐增长的极光观测数据量。其次，因为极光存在多尺度空间结构，我们设计了一种新的多尺度重构特征模块（MSRM）。最后，为强调极光类别之间的区别信息，我们提出了一种轻量级的注意力特征增强模块（LAFE）。我们的方法在2003-2004年由北极黄河站的观测数据进行验证，实验结果表明，将多波长信息融合分类效果显著提高了极光分类性能。特别是，我们的方法与过去的极光分类研究相比，实现了状态机器学习的最佳分类率，并在计算效率和多视图方法之间具有优势。
</details></li>
</ul>
<hr>
<h2 id="Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers"><a href="#Truly-Scale-Equivariant-Deep-Nets-with-Fourier-Layers" class="headerlink" title="Truly Scale-Equivariant Deep Nets with Fourier Layers"></a>Truly Scale-Equivariant Deep Nets with Fourier Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02922">http://arxiv.org/abs/2311.02922</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashiq24/scale_equivarinat_fourier_layer">https://github.com/ashiq24/scale_equivarinat_fourier_layer</a></li>
<li>paper_authors: Md Ashiqur Rahman, Raymond A. Yeh</li>
<li>for: 这个论文的目的是提出一种具有缩放平衡性的深度学习模型，以便在图像分割等任务中实现更好的效果。</li>
<li>methods: 该论文使用了 Fourier 层来实现缩放平衡性，并考虑了抗锯齿处理。</li>
<li>results: 该模型在 MNIST-scale 和 STL-10 数据集上实现了竞争力的分类性能，同时保持着缩放平衡性。Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文的目的是提出一种具有缩放平衡性的深度学习模型，以便在图像分割等任务中实现更好的效果。</li>
<li>methods: 该论文使用了 Fourier 层来实现缩放平衡性，并考虑了抗锯齿处理。</li>
<li>results: 该模型在 MNIST-scale 和 STL-10 数据集上实现了竞争力的分类性能，同时保持着缩放平衡性。<details>
<summary>Abstract</summary>
In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuous domain. To address this shortcoming, we directly formulate down-scaling in the discrete domain with consideration of anti-aliasing. We then propose a novel architecture based on Fourier layers to achieve truly scale-equivariant deep nets, i.e., absolute zero equivariance-error. Following prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:在计算机视觉中，模型需要适应图像分辨率变化以实现图像分割等任务，这被称为缩放相似性。最近的研究已经在发展缩放相似性卷积神经网络，如通过共享权重和核心缩放。然而，这些网络在实践中并不是真正的缩放相似性。具体来说，它们没有考虑抗锯齿处理。为解决这点，我们直接在逻辑域中表述下降操作，考虑抗锯齿处理。我们then propose一种基于傅里叶层的新架构，以实现真正的缩放相似性深度网络，即绝对零相似性错误。 seguir Prior works, we test this model on MNIST-scale and STL-10 datasets. Our proposed model achieves competitive classification performance while maintaining zero equivariance-error.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild"><a href="#Benchmarking-Deep-Facial-Expression-Recognition-An-Extensive-Protocol-with-Balanced-Dataset-in-the-Wild" class="headerlink" title="Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild"></a>Benchmarking Deep Facial Expression Recognition: An Extensive Protocol with Balanced Dataset in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02910">http://arxiv.org/abs/2311.02910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianmarco Ipinze Tutuianu, Yang Liu, Ari Alamäki, Janne Kauttonen</li>
<li>for: 这篇论文旨在为人计算机交互中的表情识别（FER）技术提供实用的研究和推荐。</li>
<li>methods: 这篇论文使用了23种常见的网络架构，并按照一种统一的协议进行评估。具体来说，研究人员在不同的输入分辨率、类别均衡管理和预训练策略下进行了多种设置的研究，以描述对应的性能贡献。</li>
<li>results: 经过广泛的实验和过滤，研究人员得出了一些关于深度FER方法在真实应用中的推荐，以及一些有关表情识别应用中的伦理规则、隐私问题和法规的讨论。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) is a crucial part of human-computer interaction. Existing FER methods achieve high accuracy and generalization based on different open-source deep models and training approaches. However, the performance of these methods is not always good when encountering practical settings, which are seldom explored. In this paper, we collected a new in-the-wild facial expression dataset for cross-domain validation. Twenty-three commonly used network architectures were implemented and evaluated following a uniform protocol. Moreover, various setups, in terms of input resolutions, class balance management, and pre-trained strategies, were verified to show the corresponding performance contribution. Based on extensive experiments on three large-scale FER datasets and our practical cross-validation, we ranked network architectures and summarized a set of recommendations on deploying deep FER methods in real scenarios. In addition, potential ethical rules, privacy issues, and regulations were discussed in practical FER applications such as marketing, education, and entertainment business.
</details>
<details>
<summary>摘要</summary>
面部表达识别（FER）是人机交互的关键部分。现有的FER方法在不同的开源深度学习模型和训练方法上达到了高准确率和泛化。然而，这些方法在实际场景中的性能不总是好的，这些场景通常被忽略。在这篇论文中，我们收集了一个新的在野 facial expression 数据集，用于跨领域验证。我们实现了23种常用的网络架构，并按照一个固定的协议进行评估。此外，我们还对输入分辨率、类别平衡管理和预训练策略进行了不同的设置，以显示它们对性能的贡献。基于大量的实验和我们的实际核心验证，我们对深度FER方法的部署在实际场景中进行了排名和总结，并提出了一些应用中的建议。此外，我们还讨论了实际应用中的伦理规则、隐私问题和法规。
</details></li>
</ul>
<hr>
<h2 id="Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images"><a href="#Human-as-Points-Explicit-Point-based-3D-Human-Reconstruction-from-Single-view-RGB-Images" class="headerlink" title="Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images"></a>Human as Points: Explicit Point-based 3D Human Reconstruction from Single-view RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02892">http://arxiv.org/abs/2311.02892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yztang4/hap">https://github.com/yztang4/hap</a></li>
<li>paper_authors: Yingzhi Tang, Qijian Zhang, Junhui Hou, Yebin Liu</li>
<li>for: This paper aims to improve the performance of single-view human reconstruction by proposing an explicit point-based framework called HaP, which leverages point clouds as the intermediate representation of the target geometric structure.</li>
<li>methods: The proposed HaP framework uses fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, rather than implicit learning processes that can be ambiguous and less controllable. The framework also includes dedicated designs of specialized learning components and processing procedures.</li>
<li>results: The authors report quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results, demonstrating the effectiveness of the proposed framework. The results suggest a paradigm rollback to fully-explicit and geometry-centric algorithm design, which enables the use of various powerful point cloud modeling architectures and processing techniques.<details>
<summary>Abstract</summary>
The latest trends in the research field of single-view human reconstruction devote to learning deep implicit functions constrained by explicit body shape priors. Despite the remarkable performance improvements compared with traditional processing pipelines, existing learning approaches still show different aspects of limitations in terms of flexibility, generalizability, robustness, and/or representation capability. To comprehensively address the above issues, in this paper, we investigate an explicit point-based human reconstruction framework called HaP, which adopts point clouds as the intermediate representation of the target geometric structure. Technically, our approach is featured by fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, instead of an implicit learning process that can be ambiguous and less controllable. The overall workflow is carefully organized with dedicated designs of the corresponding specialized learning components as well as processing procedures. Extensive experiments demonstrate that our framework achieves quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results. Our promising results may indicate a paradigm rollback to the fully-explicit and geometry-centric algorithm design, which enables to exploit various powerful point cloud modeling architectures and processing techniques. We will make our code and data publicly available at https://github.com/yztang4/HaP.
</details>
<details>
<summary>摘要</summary>
最新的研究趋势在人体单视重建领域是学习深度隐函数，受到明确的体型先验规则约束。Despite the remarkable performance improvements compared with traditional processing pipelines, existing learning approaches still have different limitations, such as flexibility, generalizability, robustness, and representation capability. To comprehensively address these issues, in this paper, we investigate an explicit point-based human reconstruction framework called HaP, which adopts point clouds as the intermediate representation of the target geometric structure. Technically, our approach is featured by fully-explicit point cloud estimation, manipulation, generation, and refinement in the 3D geometric space, instead of an implicit learning process that can be ambiguous and less controllable. The overall workflow is carefully organized with dedicated designs of the corresponding specialized learning components as well as processing procedures. Extensive experiments demonstrate that our framework achieves quantitative performance improvements of 20% to 40% over current state-of-the-art methods, and better qualitative results. Our promising results may indicate a paradigm rollback to the fully-explicit and geometry-centric algorithm design, which enables to exploit various powerful point cloud modeling architectures and processing techniques. We will make our code and data publicly available at https://github.com/yztang4/HaP.
</details></li>
</ul>
<hr>
<h2 id="Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification"><a href="#Stacked-Autoencoder-Based-Feature-Extraction-and-Superpixel-Generation-for-Multifrequency-PolSAR-Image-Classification" class="headerlink" title="Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification"></a>Stacked Autoencoder Based Feature Extraction and Superpixel Generation for Multifrequency PolSAR Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02887">http://arxiv.org/abs/2311.02887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tushar Gadhiya, Sumanth Tangirala, Anil K. Roy</li>
<li>for: 本研究提出了一种多频度 polarimetric synthetic aperture radar（PolSAR）图像分类算法。</li>
<li>methods: 使用PolSAR分解算法提取了每个频率带的33特征，然后使用两层自适应神经网络减少输入特征向量，保留有用的输入特征。接着，使用SLIC算法生成了超像素，并使用这些超像素生成了一个强健的特征表示。最后，使用softmax分类器进行分类任务。</li>
<li>results: 在Flevoland数据集上进行了实验，并发现提议方法在文献中available的方法之上。<details>
<summary>Abstract</summary>
In this paper we are proposing classification algorithm for multifrequency Polarimetric Synthetic Aperture Radar (PolSAR) image. Using PolSAR decomposition algorithms 33 features are extracted from each frequency band of the given image. Then, a two-layer autoencoder is used to reduce the dimensionality of input feature vector while retaining useful features of the input. This reduced dimensional feature vector is then applied to generate superpixels using simple linear iterative clustering (SLIC) algorithm. Next, a robust feature representation is constructed using both pixel as well as superpixel information. Finally, softmax classifier is used to perform classification task. The advantage of using superpixels is that it preserves spatial information between neighbouring PolSAR pixels and therefore minimises the effect of speckle noise during classification. Experiments have been conducted on Flevoland dataset and the proposed method was found to be superior to other methods available in the literature.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种多频波动Synthetic Aperture Radar（PolSAR）图像的分类算法。使用PolSAR分解算法提取了每个频率带的图像中的33个特征。然后，使用两层自适应神经网络减少输入特征向量的维度，保留输入特征的有用信息。这个减少的特征向量然后用simple linear iterative clustering（SLIC）算法生成超像。接下来，使用像素和超像信息构建了一种强健的特征表示。最后，使用softmax分类器进行分类任务。使用超像有利于保留邻近PolSAR像素之间的空间信息，因此减少了零点噪声的影响，进一步提高了分类的精度。我们在Flevoland数据集上进行了实验，并发现提出的方法在相关文献中比其他方法更为出色。
</details></li>
</ul>
<hr>
<h2 id="Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box"><a href="#Inner-IoU-More-Effective-Intersection-over-Union-Loss-with-Auxiliary-Bounding-Box" class="headerlink" title="Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box"></a>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02877">http://arxiv.org/abs/2311.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang, Cong Xu, Shuaijie Zhang<br>for:  This paper aims to improve the bounding box regression process in object detection by proposing a new loss function called Inner-IoU loss.methods:  The paper analyzes the BBR model and proposes using different scales of auxiliary bounding boxes to calculate losses, as well as introducing a scaling factor ratio to control the scale size of the auxiliary bounding boxes.results:  The proposed Inner-IoU loss function enhances the detection performance of object detection models, demonstrating its effectiveness and generalization ability.<details>
<summary>Abstract</summary>
With the rapid development of detectors, Bounding Box Regression (BBR) loss function has constantly updated and optimized. However, the existing IoU-based BBR still focus on accelerating convergence by adding new loss terms, ignoring the limitations of IoU loss term itself. Although theoretically IoU loss can effectively describe the state of bounding box regression,in practical applications, it cannot adjust itself according to different detectors and detection tasks, and does not have strong generalization. Based on the above, we first analyzed the BBR model and concluded that distinguishing different regression samples and using different scales of auxiliary bounding boxes to calculate losses can effectively accelerate the bounding box regression process. For high IoU samples, using smaller auxiliary bounding boxes to calculate losses can accelerate convergence, while larger auxiliary bounding boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which calculates IoU loss through auxiliary bounding boxes. For different datasets and detectors, we introduce a scaling factor ratio to control the scale size of the auxiliary bounding boxes for calculating losses. Finally, integrate Inner-IoU into the existing IoU-based loss functions for simulation and comparative experiments. The experiment result demonstrate a further enhancement in detection performance with the utilization of the method proposed in this paper, verifying the effectiveness and generalization ability of Inner-IoU loss.
</details>
<details>
<summary>摘要</summary>
随着检测器的快速发展，矩形框回归（BBR）损失函数不断更新和优化。然而，现有的IoU基于的BBR仍然围绕加入新的损失项来加速快损集中精度。虽然理论上IoU损失函数可以有效描述矩形框回归的状态，但在实际应用中，它无法根据不同的检测器和检测任务自适应调整，并且不具备强大的泛化能力。基于以上分析，我们首先分析了BBR模型，并结论出使用不同的检测器和检测任务的不同描述性框架可以更好地加速矩形框回归过程。为高IoU样本，使用较小的卫星框架来计算损失可以加速收敛，而对低IoU样本，使用较大的卫星框架更适合。然后，我们提出了Inner-IoU损失函数，通过卫星框架来计算IoU损失。为不同的数据集和检测器，我们引入了涉及因子比例来控制卫星框架的大小。最后，我们将Inner-IoU集成到现有的IoU基于损失函数中进行 simulations和比较实验。实验结果表明，通过使用本文提出的方法可以进一步提高检测性能，证明了Inner-IoU损失函数的有效性和泛化能力。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series"><a href="#Dynamic-Neural-Fields-for-Learning-Atlases-of-4D-Fetal-MRI-Time-series" class="headerlink" title="Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series"></a>Dynamic Neural Fields for Learning Atlases of 4D Fetal MRI Time-series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02874">http://arxiv.org/abs/2311.02874</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kidrauh/neural-atlasing">https://github.com/kidrauh/neural-atlasing</a></li>
<li>paper_authors: Zeen Chi, Zhongxiao Cong, Clinton J. Wang, Yingcheng Liu, Esra Abaci Turk, P. Ellen Grant, S. Mazdak Abulnaga, Polina Golland, Neel Dey</li>
<li>for: 用于快速构建生物医学影像 атла斯，使用神经场。</li>
<li>methods: 使用神经场来学习可变的空间时间观察，实现主动态脉络MRI时序序列的个体化 атла斯建立和动态稳定。</li>
<li>results: 对妊娠期动态BOLD MRI时序列实现高质量的个体化 атла斯建立，相比现有方法快速 convergence，但略微下降一些骨骼匠 overlap。<details>
<summary>Abstract</summary>
We present a method for fast biomedical image atlas construction using neural fields. Atlases are key to biomedical image analysis tasks, yet conventional and deep network estimation methods remain time-intensive. In this preliminary work, we frame subject-specific atlas building as learning a neural field of deformable spatiotemporal observations. We apply our method to learning subject-specific atlases and motion stabilization of dynamic BOLD MRI time-series of fetuses in utero. Our method yields high-quality atlases of fetal BOLD time-series with $\sim$5-7$\times$ faster convergence compared to existing work. While our method slightly underperforms well-tuned baselines in terms of anatomical overlap, it estimates templates significantly faster, thus enabling rapid processing and stabilization of large databases of 4D dynamic MRI acquisitions. Code is available at https://github.com/Kidrauh/neural-atlasing
</details>
<details>
<summary>摘要</summary>
我们提出了一种快速生成生物医学影像 атла斯使用神经场的方法。 Atlases 是生物医学影像分析任务的关键，但是传统的深度网络估算方法和深度网络估算方法仍然具有较长的计算时间。在这项初步工作中，我们将主动扮演为学习一种可变的空间时间观察神经场。我们应用我们的方法来学习主动者特定的 атла斯和动态 BOLD MRI 时序列的运动稳定。我们的方法可以高效地生成高质量的胎儿 BOLD 时序列的 атла斯，与现有工作相比，它的整合时间约为 5-7 倍快。虽然我们的方法对于 анатомиче匹配略有下降，但它可以更快地估算模板，从而实现大规模的4D动态 MRI 数据库的快速处理和稳定。可以在 <https://github.com/Kidrauh/neural-atlasing> 上下载代码。
</details></li>
</ul>
<hr>
<h2 id="OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data"><a href="#OVIR-3D-Open-Vocabulary-3D-Instance-Retrieval-Without-Training-on-3D-Data" class="headerlink" title="OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data"></a>OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02873">http://arxiv.org/abs/2311.02873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shiyoung77/ovir-3d">https://github.com/shiyoung77/ovir-3d</a></li>
<li>paper_authors: Shiyang Lu, Haonan Chang, Eric Pu Jing, Abdeslam Boularias, Kostas Bekris</li>
<li>for: 开发了一种基于文本查询的开 vocabulary 3D объек实体检索方法，不需要使用任何3D数据进行训练。</li>
<li>methods: 方法使用了文本对齐的2D区域提档网络，将2D区域提档Network的特征相似性与文本查询进行对比，并通过多视图拟合来将2D区域提档转化为3D空间中的3D объек实体段落。</li>
<li>results: 实验结果表明，该方法可以快速和高效地在大多数indoor 3D场景下进行实时多视图拟合，并且不需要额外训练在3D空间。实验结果还表明，该方法在机器人导航和抓取等应用中具有广泛的应用前景。<details>
<summary>Abstract</summary>
This work presents OVIR-3D, a straightforward yet effective method for open-vocabulary 3D object instance retrieval without using any 3D data for training. Given a language query, the proposed method is able to return a ranked set of 3D object instance segments based on the feature similarity of the instance and the text query. This is achieved by a multi-view fusion of text-aligned 2D region proposals into 3D space, where the 2D region proposal network could leverage 2D datasets, which are more accessible and typically larger than 3D datasets. The proposed fusion process is efficient as it can be performed in real-time for most indoor 3D scenes and does not require additional training in 3D space. Experiments on public datasets and a real robot show the effectiveness of the method and its potential for applications in robot navigation and manipulation.
</details>
<details>
<summary>摘要</summary>
这个工作介绍了 OVIR-3D，一种简单又有效的方法，用于无需使用任何3D数据进行训练的开 vocabulary 3D对象实例检索。给定一个语言查询，提议的方法能够返回一个根据实例和文本查询之间的特征相似性排序的3D对象实例分割。这是通过多视图融合文本对齐的2D区域提档到3D空间中进行的，其中2D区域提档网络可以利用2D数据集，这些数据集通常更容易获得和更大规模。我们的融合过程是实时可行的，不需要额外训练在3D空间。我们在公共数据集和真实的 робоット上进行了实验，并证明了该方法的有效性和其在机器人导航和操作中的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling"><a href="#FocusTune-Tuning-Visual-Localization-through-Focus-Guided-Sampling" class="headerlink" title="FocusTune: Tuning Visual Localization through Focus-Guided Sampling"></a>FocusTune: Tuning Visual Localization through Focus-Guided Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02872">http://arxiv.org/abs/2311.02872</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sontung/focus-tune">https://github.com/sontung/focus-tune</a></li>
<li>paper_authors: Son Tung Nguyen, Alejandro Fontan, Michael Milford, Tobias Fischer</li>
<li>for: 提高视觉地标算法性能</li>
<li>methods: 使用强调导航的采样技术，指导Scene coordinate regression模型在重要的3D点三角形计算中做出更好的预测</li>
<li>results: 与现有的状态艺术模型匹配或超越其性能，同时保持ACE模型的低存储和计算需求，例如在 Cambridge Landmarks 数据集上减少了译偏误值从25到19和17到15 cm，提高了应用在移动 робо扮和增强现实等领域的可行性。I hope that helps!<details>
<summary>Abstract</summary>
We propose FocusTune, a focus-guided sampling technique to improve the performance of visual localization algorithms. FocusTune directs a scene coordinate regression model towards regions critical for 3D point triangulation by exploiting key geometric constraints. Specifically, rather than uniformly sampling points across the image for training the scene coordinate regression model, we instead re-project 3D scene coordinates onto the 2D image plane and sample within a local neighborhood of the re-projected points. While our proposed sampling strategy is generally applicable, we showcase FocusTune by integrating it with the recently introduced Accelerated Coordinate Encoding (ACE) model. Our results demonstrate that FocusTune both improves or matches state-of-the-art performance whilst keeping ACE's appealing low storage and compute requirements, for example reducing translation error from 25 to 19 and 17 to 15 cm for single and ensemble models, respectively, on the Cambridge Landmarks dataset. This combination of high performance and low compute and storage requirements is particularly promising for applications in areas like mobile robotics and augmented reality. We made our code available at \url{https://github.com/sontung/focus-tune}.
</details>
<details>
<summary>摘要</summary>
我们提出了FocusTune，一种帮助视觉地标定算法性能提高的注意力导向抽象技术。FocusTune利用场景坐标重构模型中的关键几何约束，将注意力集中在点云三角形插值中的关键区域。具体来说，我们不是将图像中的所有点用于场景坐标重构模型的训练，而是将3D场景坐标重新 проек到图像平面上，然后在该地方采样。我们的提议的采样策略可以普遍应用，但我们在ACE模型中实现了它。我们的结果表明，FocusTune可以提高或与现有状态的性能匹配，同时保持ACE模型的低存储和计算需求。例如，在 cambridge 景点集中，FocusTune可以将翻译错误从25减少到19和17cm，对单个和集成模型进行比较。这种高性能且低计算存储需求的组合特别适用于移动 робо扮和增强现实应用。我们的代码可以在https://github.com/sontung/focus-tune上下载。
</details></li>
</ul>
<hr>
<h2 id="Neural-based-Compression-Scheme-for-Solar-Image-Data"><a href="#Neural-based-Compression-Scheme-for-Solar-Image-Data" class="headerlink" title="Neural-based Compression Scheme for Solar Image Data"></a>Neural-based Compression Scheme for Solar Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02855">http://arxiv.org/abs/2311.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Zafari, Atefeh Khoshkhahtinat, Jeremy A. Grajeda, Piyush M. Mehta, Nasser M. Nasrabadi, Laura E. Boucheron, Barbara J. Thompson, Michael S. F. Kirk, Daniel da Silva</li>
<li>For: The paper is written for the purpose of proposing a neural network-based lossy compression method for data-intensive imagery missions, specifically for NASA’s SDO mission.* Methods: The proposed method uses an adversarially trained neural network with local and non-local attention modules to capture the local and global structure of the image, resulting in a better trade-off in rate-distortion (RD) compared to conventional hand-engineered codecs. The RD variational autoencoder is jointly trained with a channel-dependent entropy model as a shared prior between the analysis and synthesis transforms to make the entropy coding of the latent code more effective.* Results: The proposed algorithm outperforms currently-in-use and state-of-the-art codecs such as JPEG and JPEG-2000 in terms of RD performance when compressing extreme-ultraviolet (EUV) data. The algorithm is able to achieve consistent segmentations of coronal holes (CH) in the compressed images, even at a compression rate of $\sim0.1$ bits per pixel.<details>
<summary>Abstract</summary>
Studying the solar system and especially the Sun relies on the data gathered daily from space missions. These missions are data-intensive and compressing this data to make them efficiently transferable to the ground station is a twofold decision to make. Stronger compression methods, by distorting the data, can increase data throughput at the cost of accuracy which could affect scientific analysis of the data. On the other hand, preserving subtle details in the compressed data requires a high amount of data to be transferred, reducing the desired gains from compression. In this work, we propose a neural network-based lossy compression method to be used in NASA's data-intensive imagery missions. We chose NASA's SDO mission which transmits 1.4 terabytes of data each day as a proof of concept for the proposed algorithm. In this work, we propose an adversarially trained neural network, equipped with local and non-local attention modules to capture both the local and global structure of the image resulting in a better trade-off in rate-distortion (RD) compared to conventional hand-engineered codecs. The RD variational autoencoder used in this work is jointly trained with a channel-dependent entropy model as a shared prior between the analysis and synthesis transforms to make the entropy coding of the latent code more effective. Our neural image compression algorithm outperforms currently-in-use and state-of-the-art codecs such as JPEG and JPEG-2000 in terms of the RD performance when compressing extreme-ultraviolet (EUV) data. As a proof of concept for use of this algorithm in SDO data analysis, we have performed coronal hole (CH) detection using our compressed images, and generated consistent segmentations, even at a compression rate of $\sim0.1$ bits per pixel (compared to 8 bits per pixel on the original data) using EUV data from SDO.
</details>
<details>
<summary>摘要</summary>
studying the solar system and especially the sun relies on data gathered daily from space missions. these missions are data-intensive, and compressing this data to make it efficiently transferable to the ground station is a twofold decision. stronger compression methods can increase data throughput at the cost of accuracy, which could affect scientific analysis of the data. on the other hand, preserving subtle details in the compressed data requires a high amount of data to be transferred, reducing the desired gains from compression. in this work, we propose a neural network-based lossy compression method for use in nasa's data-intensive imagery missions. we chose nasa's sdo mission, which transmits 1.4 terabytes of data each day, as a proof of concept for the proposed algorithm.our proposed algorithm uses an adversarially trained neural network equipped with local and non-local attention modules to capture both the local and global structure of the image, resulting in a better trade-off in rate-distortion (rd) compared to conventional hand-engineered codecs. the rd variational autoencoder used in this work is jointly trained with a channel-dependent entropy model as a shared prior between the analysis and synthesis transforms to make the entropy coding of the latent code more effective. our neural image compression algorithm outperforms currently-in-use and state-of-the-art codecs such as jpeg and jpeg-2000 in terms of rd performance when compressing extreme-ultraviolet (euv) data.as a proof of concept for the use of this algorithm in sdo data analysis, we have performed coronal hole (ch) detection using our compressed images and generated consistent segmentations, even at a compression rate of approximately 0.1 bits per pixel (compared to 8 bits per pixel on the original data) using euv data from sdo.
</details></li>
</ul>
<hr>
<h2 id="Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video"><a href="#Consistent4D-Consistent-360°-Dynamic-Object-Generation-from-Monocular-Video" class="headerlink" title="Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video"></a>Consistent4D: Consistent 360° Dynamic Object Generation from Monocular Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02848">http://arxiv.org/abs/2311.02848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanqinJiang/Consistent4D">https://github.com/yanqinJiang/Consistent4D</a></li>
<li>paper_authors: Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, Yao Yao</li>
<li>for: 本研究は、无测量モノクロビデオから4D动的オブジェクトを生成する新しいアプローチを提案します。</li>
<li>methods: 我们は、360度动的オブジェクト再现问题を4D生成问题として捉え、多视点データ収集とカメラ测定を不要にします。これは、物体レベルの3D意识 Image Diffusion Modelを主たるスーパーバイジョン信号として使用して、Dynamic Neural Radiance Fields（DyNeRF）をトレーニングします。特に、Cascade DyNeRFを提案して、时间轴上のディスクレットなスーパーバイジョン信号の下で安定した受动と时间継続性を実现します。さらに、空间的な一贯性と时间的な一贯性を実现するために、Interpolation-driven Consistency Lossを导入します。</li>
<li>results: 我们のConsistent4Dは、先行研究と竞合する性能を示し、新しい可能性を开拓します。また、普通の文字-3D生成タスクにも优れた性能を示しています。プロジェクトページは<a target="_blank" rel="noopener" href="https://consistent4d.github.io/%E3%81%A7%E3%81%99%E3%80%82">https://consistent4d.github.io/です。</a><details>
<summary>Abstract</summary>
In this paper, we present Consistent4D, a novel approach for generating 4D dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-degree dynamic object reconstruction as a 4D generation problem, eliminating the need for tedious multi-view data collection and camera calibration. This is achieved by leveraging the object-level 3D-aware image diffusion model as the primary supervision signal for training Dynamic Neural Radiance Fields (DyNeRF). Specifically, we propose a Cascade DyNeRF to facilitate stable convergence and temporal continuity under the supervision signal which is discrete along the time axis. To achieve spatial and temporal consistency, we further introduce an Interpolation-driven Consistency Loss. It is optimized by minimizing the discrepancy between rendered frames from DyNeRF and interpolated frames from a pre-trained video interpolation model. Extensive experiments show that our Consistent4D can perform competitively to prior art alternatives, opening up new possibilities for 4D dynamic object generation from monocular videos, whilst also demonstrating advantage for conventional text-to-3D generation tasks. Our project page is https://consistent4d.github.io/.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的方法，即Consistent4D，用于从单视图视频中生成4D动态对象。我们Uniquely将360度动态对象重建问题作为4D生成问题来处理，这意味着不需要繁琐的多视图数据收集和摄像头卡利布ر。我们通过利用物体层3D意识图像扩散模型作为主要超视图信号来培训动态神经辐射场（DyNeRF）。我们提出了一种升级的DyNeRF来实现稳定的整合和时间连续性，并引入了一种 interpolate-driven 一致损失来保证空间和时间一致性。我们通过对DyNeRF和预训练视频 interpolate模型生成的帧进行对比来优化这种损失函数。我们的项目页面是https://consistent4d.github.io/.Here's the translation in Traditional Chinese:在这篇论文中，我们提出了一种新的方法，即Consistent4D，用于从单视角影像中生成4D动态物件。我们Uniquely将360度动态物件重建问题作为4D生成问题来处理，这意味着不需要繁琐的多视角数据收集和摄像头卡利布。我们通过利用物体层3D意识图像扩散模型作为主要超视射信号来培训动态神经辐射场（DyNeRF）。我们提出了一种升级的DyNeRF来实现稳定的整合和时间连续性，并引入了一种 interpolate-driven 一致损失来保证空间和时间一致性。我们通过对DyNeRF和预训练影像 interpolate模型生成的帧进行比较来优化这种损失函数。我们的项目页面是https://consistent4d.github.io/.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-sinusoidal-representation-networks-to-predict-fMRI-signals-from-EEG"><a href="#Leveraging-sinusoidal-representation-networks-to-predict-fMRI-signals-from-EEG" class="headerlink" title="Leveraging sinusoidal representation networks to predict fMRI signals from EEG"></a>Leveraging sinusoidal representation networks to predict fMRI signals from EEG</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04234">http://arxiv.org/abs/2311.04234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yamin Li, Ange Lou, Catie Chang<br>for: 这个论文的目的是用多通道EEG来预测fMRI信号，以提高EEG的空间分辨率和扩展fMRI的应用范围。methods: 这个论文使用了一种新的SIREN网络，通过学习电阻图谱信息来减少feature engineering，并使用encoder-decoder结构来重建fMRI信号。results: 试验结果表明，这个模型在8名参与者 simultaneous EEG-fMRI数据集上表现出色，并且超越了一个现有的状态略模型。这些结果表明了使用 periodic activation functions 在深度神经网络中模型功能神经成像数据的潜在优势。<details>
<summary>Abstract</summary>
In modern neuroscience, functional magnetic resonance imaging (fMRI) has been a crucial and irreplaceable tool that provides a non-invasive window into the dynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic blurring as well as high cost, immobility, and incompatibility with metal implants. Electroencephalography (EEG) is complementary to fMRI and can directly record the cortical electrical activity at high temporal resolution, but has more limited spatial resolution and is unable to recover information about deep subcortical brain structures. The ability to obtain fMRI information from EEG would enable cost-effective, imaging across a wider set of brain regions. Further, beyond augmenting the capabilities of EEG, cross-modality models would facilitate the interpretation of fMRI signals. However, as both EEG and fMRI are high-dimensional and prone to artifacts, it is currently challenging to model fMRI from EEG. To address this challenge, we propose a novel architecture that can predict fMRI signals directly from multi-channel EEG without explicit feature engineering. Our model achieves this by implementing a Sinusoidal Representation Network (SIREN) to learn frequency information in brain dynamics from EEG, which serves as the input to a subsequent encoder-decoder to effectively reconstruct the fMRI signal from a specific brain region. We evaluate our model using a simultaneous EEG-fMRI dataset with 8 subjects and investigate its potential for predicting subcortical fMRI signals. The present results reveal that our model outperforms a recent state-of-the-art model, and indicates the potential of leveraging periodic activation functions in deep neural networks to model functional neuroimaging data.
</details>
<details>
<summary>摘要</summary>
现代神经科学中，功能核磁共振成像（fMRI）已成为非侵入式窗口，提供整个大脑活动的动态图像。然而，fMRI受到血液干扰和高成本、固定性和金属设备不兼容等限制。电enzephalography（EEG）可以直接记录 cortical 电动力谱高时间分辨率，但是它的空间分辨率较有限，无法回归深部脑结构信息。能够从 EEG 获取 fMRI 信息，将可以实现成本下降，扫描更广泛的脑区域。此外，跨模态模型可以促进 fMRI 信号的解释。然而，由于 EEG 和 fMRI 都是高维度和易受损的，目前是困难的模型 fMRI 从 EEG。为解决这个挑战，我们提出了一种新的建议，可以直接从多通道 EEG 预测 fMRI 信号，不需要显式的特征工程。我们的模型实现了声律表示网络（SIREN）来学习 brain 动力学中的频率信息，该信息作为 EEG 输入，并由后续的编码器-解码器组合来有效地重建 fMRI 信号。我们使用了 8 名参与者的同时 EEG-fMRI 数据集进行评估，并研究了其在预测深部 fMRI 信号方面的潜力。结果表明，我们的模型超过了当前状态的最佳模型，并表明了在深度神经网络中使用律动函数可以有效地模型功能神经成像数据。
</details></li>
</ul>
<hr>
<h2 id="Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction"><a href="#Flexible-Multi-Generator-Model-with-Fused-Spatiotemporal-Graph-for-Trajectory-Prediction" class="headerlink" title="Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction"></a>Flexible Multi-Generator Model with Fused Spatiotemporal Graph for Trajectory Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02835">http://arxiv.org/abs/2311.02835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyuan Zhu, Fengxia Han, Hao Deng</li>
<li>for: 预测行程在自动驾驶系统中扮演着关键作用，帮助汽车实现精准跟踪和决策。</li>
<li>methods: 我们提出了一种行程预测框架，可以捕捉人群之间的社交交互变化和分支 manifold的模型。我们的框架基于综合时空图来更好地模型场景中人群的复杂交互，并使用多生成器架构，其中包括一个灵活的生成器选择器网络来学习多个生成器的分布。</li>
<li>results: 我们的框架在不同的挑战性数据集上比较多的基准方法达到了状态革新的表现。<details>
<summary>Abstract</summary>
Trajectory prediction plays a vital role in automotive radar systems, facilitating precise tracking and decision-making in autonomous driving. Generative adversarial networks with the ability to learn a distribution over future trajectories tend to predict out-of-distribution samples, which typically occurs when the distribution of forthcoming paths comprises a blend of various manifolds that may be disconnected. To address this issue, we propose a trajectory prediction framework, which can capture the social interaction variations and model disconnected manifolds of pedestrian trajectories. Our framework is based on a fused spatiotemporal graph to better model the complex interactions of pedestrians in a scene, and a multi-generator architecture that incorporates a flexible generator selector network on generated trajectories to learn a distribution over multiple generators. We show that our framework achieves state-of-the-art performance compared with several baselines on different challenging datasets.
</details>
<details>
<summary>摘要</summary>
<SYS>文本翻译为简化中文。</SYS>自动驾驶需要trajectory prediction来确定汽车的路径，以便准确地跟踪和做出决策。生成对抗网络可以学习未来路径的分布，但是它们通常会预测外部样本，这通常发生在未来路径的分布中包含多种不同的拟合 manifold，这些拟合 manifold可能是分立的。为了解决这个问题，我们提出了一个trajectory prediction框架，可以捕捉人行行为的社会交互变化，以及人行道径的分立拟合 manifold。我们的框架基于一个综合的空间时间图，更好地模型了场景中人行行为的复杂交互，以及一个多个生成器架构，其中包括一个灵活的生成器选择网络，可以学习多个生成器的分布。我们展示了我们的框架在不同的难度 datasets 上达到了现状最佳性能。
</details></li>
</ul>
<hr>
<h2 id="SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map"><a href="#SemanticTopoLoop-Semantic-Loop-Closure-With-3D-Topological-Graph-Based-on-Quadric-Level-Object-Map" class="headerlink" title="SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map"></a>SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based on Quadric-Level Object Map</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02831">http://arxiv.org/abs/2311.02831</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhong Cao</li>
<li>for: 提高 réal-world  scenario 中 SLAM 系统的精度和Robustness</li>
<li>methods: 基于多层验证的对象级数据协调方法和基于 quadric-level 对象地图 тополоジ的semantic loop closure方法</li>
<li>results: 在宽视场下实现高精度的loop closure，并且在对比 existed state-of-the-art 方法时显示出更高的精度、再现率和地图定位精度 metricIn English, this means:</li>
<li>for: Improving the accuracy and robustness of SLAM systems in real-world scenarios</li>
<li>methods: An object-level data association method based on multi-level verification and a semantic loop closure method based on a quadric-level object map topology</li>
<li>results: Achieving high-precision loop closure over a wide field of view, and outperforming existing state-of-the-art methods in terms of precision, recall, and localization accuracy metrics.<details>
<summary>Abstract</summary>
Loop closure, as one of the crucial components in SLAM, plays an essential role in correcting the accumulated errors. Traditional appearance-based methods, such as bag-of-words models, are often limited by local 2D features and the volume of training data, making them less versatile and robust in real-world scenarios, leading to missed detections or false positives detections in loop closure. To address these issues, we first propose a object-level data association method based on multi-level verification, which can associate 2D semantic features of current frame with 3D objects landmarks of map. Next, taking advantage of these association relations, we introduce a semantic loop closure method based on quadric-level object map topology, which represents scenes through the topological graph of objects and achieves accurate loop closure at a wide field of view by comparing differences in the topological graphs. Finally, we integrate these two methods into a complete object-aware SLAM system. Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm. Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall and localization accuracy metrics.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate given text into Simplified Chinese.<</SYS>>路径关闭，作为SLAM中一个关键组件，对于消除积累错误起到了关键作用。传统的外观基于方法，如袋子模型，通常受到当地2D特征的限制，以及训练数据的量，使其在实际场景中 menos versatile 和robust，导致过滤或假阳性检测在路径关闭中出现。为解决这些问题，我们首先提出了基于多级验证的对象水平数据协调方法，可以将当前帧的2Dsemantic特征与地图中的3D对象标记相关联。接着，通过这些关系，我们引入了基于四元数平面的对象地图 тоポ多特征，可以在宽视野中高精度地实现路径关闭。最后，我们将这两种方法 integrate into a complete object-aware SLAM system。Qualitative experiments and ablation studies demonstrate the effectiveness and robustness of the proposed object-level data association algorithm。Quantitative experiments show that our semantic loop closure method outperforms existing state-of-the-art methods in terms of precision, recall, and localization accuracy metrics.
</details></li>
</ul>
<hr>
<h2 id="InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image"><a href="#InstructPix2NeRF-Instructed-3D-Portrait-Editing-from-a-Single-Image" class="headerlink" title="InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image"></a>InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02826">http://arxiv.org/abs/2311.02826</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mybabyyh/instructpix2nerf">https://github.com/mybabyyh/instructpix2nerf</a></li>
<li>paper_authors: Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, Jun Zhu</li>
<li>for: This paper aims to solve the problem of human-instructed 3D-aware portrait editing for open-world images, which has been under-explored due to the lack of labeled human face 3D datasets and effective architectures.</li>
<li>methods: The proposed method, InstructPix2NeRF, is an end-to-end diffusion-based framework that enables instructed 3D-aware portrait editing from a single open-world image with human instructions. It uses a conditional latent 3D diffusion process to lift 2D editing to 3D space and learn the correlation between the paired images’ difference and the instructions via triplet data.</li>
<li>results: The proposed method achieves effective and multi-semantic editing through one single pass with the portrait identity well-preserved. Additionally, an identity consistency module is proposed to increase the multi-view 3D identity consistency. Extensive experiments show the effectiveness of the method and its superiority against strong baselines quantitatively and qualitatively.Here’s the Chinese version of the three key points:</li>
<li>for: 这篇论文目标是解决人类指令下的开放世界图像上的3D-aware肖像编辑问题，这个问题由于缺乏人脸3D标注数据和有效架构而未得到充分研究。</li>
<li>methods: 提议的方法是一种基于扩散的终端推广框架，称为InstructPix2NeRF，它可以从单个开放世界图像上接受人类指令，并实现3D-aware肖像编辑。它使用一种条件隐藏3D扩散过程来提升2D编辑到3D空间，并通过 triplet 数据学习对差异和指令之间的相关性。</li>
<li>results: 提议的方法可以实现高效和多Semantic的编辑，同时保持肖像的identify完好。此外，还提出了一种人脸identidadityModule，它直接将提取的identify信号 Modulates到扩散过程中，从而提高多视图3D人脸identify一致性。广泛的实验证明了方法的效果和对强基eline的超越。<details>
<summary>Abstract</summary>
With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of human-instructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusion-based framework termed InstructPix2NeRF, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions. At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images' difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
“受到神经辐射场（NeRF）的成功，3D意识编辑技术已经取得了显著的进步，但这些方法仍然高度依赖于每个提示的优化。因为缺乏人类脸部3D数据集和有效的建筑，人类指令下的开放世界肖像3D意识编辑仍然处于未explored阶段。为解决这个问题，我们提出了一种终端扩散基于的框架，称之为InstructPix2NeRF，它可以在单个开放世界图像上实现人类指令下的3D意识编辑。核心 liegt在一种 conditional latent 3D 扩散过程中，通过学习对带有对应图像差异和指令的对应关系来提升2D编辑到3D空间。通过我们提出的token位置随机Strategy，我们可以在一次通过中实现多Semantic editing，并且保持肖像的身份完整。此外，我们还提出了一种人类身份归一模块，它直接将提取的身份信号注入到我们的扩散过程中，从而提高多视图3D身份一致性。我们的实验证明了我们的方法的有效性，并与强基eline比较示出了我们的方法的超越性。”
</details></li>
</ul>
<hr>
<h2 id="Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning"><a href="#Efficient-Self-Supervised-Human-Pose-Estimation-with-Inductive-Prior-Tuning" class="headerlink" title="Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning"></a>Efficient, Self-Supervised Human Pose Estimation with Inductive Prior Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02815">http://arxiv.org/abs/2311.02815</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/princetonvisualai/hpe-inductive-prior-tuning">https://github.com/princetonvisualai/hpe-inductive-prior-tuning</a></li>
<li>paper_authors: Nobline Yoo, Olga Russakovsky</li>
<li>for: 本研究旨在提高无监督人体姿势估算（HPE）的自动化性。</li>
<li>methods: 本研究使用了自我重构的方法，利用大量未标注的视觉数据，尽管当前精度不高。</li>
<li>results: 研究人员通过分析重建质量和姿势估算准确性之间的关系，开发了一个新的模型管道，使用了比基eline要少的训练数据，并提出了一个适合无监督设置的新评价指标。<details>
<summary>Abstract</summary>
The goal of 2D human pose estimation (HPE) is to localize anatomical landmarks, given an image of a person in a pose. SOTA techniques make use of thousands of labeled figures (finetuning transformers or training deep CNNs), acquired using labor-intensive crowdsourcing. On the other hand, self-supervised methods re-frame the HPE task as a reconstruction problem, enabling them to leverage the vast amount of unlabeled visual data, though at the present cost of accuracy. In this work, we explore ways to improve self-supervised HPE. We (1) analyze the relationship between reconstruction quality and pose estimation accuracy, (2) develop a model pipeline that outperforms the baseline which inspired our work, using less than one-third the amount of training data, and (3) offer a new metric suitable for self-supervised settings that measures the consistency of predicted body part length proportions. We show that a combination of well-engineered reconstruction losses and inductive priors can help coordinate pose learning alongside reconstruction in a self-supervised paradigm.
</details>
<details>
<summary>摘要</summary>
目标是二维人姿估计（HPE）是将人体部位的坐标进行地图化，给定一张人体姿势的图像。现状技术使用了千张标注图像（finetuning transformers或训练深度CNN），通过劳动密集的人工审核来获得。然而，无监督方法将HPE任务视为一个重建问题，可以利用大量的未标注视觉数据，尽管目前精度有所偏低。在这项工作中，我们探讨了如何提高无监督HPE。我们（1）分析了重建质量和姿势估计精度之间的关系，（2）开发了一个比基eline更高效的模型管线，使用较少的训练数据，并（3）提出了适合无监督设置的一个新的度量，用于测量预测的身体部分长度准确性。我们表明，将Well-engineered重建损失和拟合约束结合在一起可以协调姿势学习和重建在无监督 парадигме中。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers"><a href="#Fast-and-Interpretable-Face-Identification-for-Out-Of-Distribution-Data-Using-Vision-Transformers" class="headerlink" title="Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers"></a>Fast and Interpretable Face Identification for Out-Of-Distribution Data Using Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02803">http://arxiv.org/abs/2311.02803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hai Phan, Cindy Le, Vu Le, Yihui He, Anh Totti Nguyen</li>
<li>for: 本研究旨在提高face identification的精度和效率，提出了一种基于Vision Transformers（ViTs）的新方法。</li>
<li>methods: 本研究使用了两个图像的比较，首先在图像级别进行比较，然后在小块级别进行比较。在小块级别比较中，使用了交叉注意力来比较两个图像的patch。</li>
<li>results: 经过训练200万对图像，本研究的模型在对外部数据进行比较时达到了与DeepFace-EMD相同的准确率，但在执行速度方面比DeepFace-EMD更快，并且通过人类研究表明了模型的解释性。<details>
<summary>Abstract</summary>
Most face identification approaches employ a Siamese neural network to compare two images at the image embedding level. Yet, this technique can be subject to occlusion (e.g. faces with masks or sunglasses) and out-of-distribution data. DeepFace-EMD (Phan et al. 2022) reaches state-of-the-art accuracy on out-of-distribution data by first comparing two images at the image level, and then at the patch level. Yet, its later patch-wise re-ranking stage admits a large $O(n^3 \log n)$ time complexity (for $n$ patches in an image) due to the optimal transport optimization. In this paper, we propose a novel, 2-image Vision Transformers (ViTs) that compares two images at the patch level using cross-attention. After training on 2M pairs of images on CASIA Webface (Yi et al. 2014), our model performs at a comparable accuracy as DeepFace-EMD on out-of-distribution data, yet at an inference speed more than twice as fast as DeepFace-EMD (Phan et al. 2022). In addition, via a human study, our model shows promising explainability through the visualization of cross-attention. We believe our work can inspire more explorations in using ViTs for face identification.
</details>
<details>
<summary>摘要</summary>
大多数面部识别方法使用拟合网络进行图像嵌入水平的比较。然而，这种技术可能受到遮盖物（例如面具或太阳镜）和非典型数据的影响。深度脸部-EMD（Phan et al. 2022）达到了非典型数据上的状态态-of-the-art精度，但它的后续的质量排名阶段具有大 O（n^3 \* log n）的时间复杂度（对于图像中的 n 个质量），这是由优化运输优化引起的。在这篇论文中，我们提出了一种新的、使用视图变换器（ViTs）来比较两个图像的质量。经过在 CASIA Webface（Yi et al. 2014）上训练 200 万对图像，我们的模型在非典型数据上达到了与 DeepFace-EMD 相同的精度，但在推断速度方面比 DeepFace-EMD 更快速，大约两倍。此外，通过人类研究，我们的模型展示了可见的混合注意力可读性。我们认为我们的工作可以激励更多的人们在使用 ViTs 进行脸部识别。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CV_2023_11_06/" data-id="closbrops00la0g88h3vo0qm4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.AI_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T12:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.AI_2023_11_06/">cs.AI - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multimodal-Stress-Detection-Using-Facial-Landmarks-and-Biometric-Signals"><a href="#Multimodal-Stress-Detection-Using-Facial-Landmarks-and-Biometric-Signals" class="headerlink" title="Multimodal Stress Detection Using Facial Landmarks and Biometric Signals"></a>Multimodal Stress Detection Using Facial Landmarks and Biometric Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03606">http://arxiv.org/abs/2311.03606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majid Hosseini, Morteza Bodaghi, Ravi Teja Bhupatiraju, Anthony Maida, Raju Gottumukkala<br>for: 这种研究旨在提高人们的压力测量和情绪状况的评估，通过结合多种感知技术。methods: 这种研究使用多模态学习方法，结合脸部特征和生物指标信号进行压力检测。results: 研究发现，使用晚期融合技术可以达到94.39%的准确率，而使用早期融合技术可以超越这一成果，达到98.38%的准确率。<details>
<summary>Abstract</summary>
The development of various sensing technologies is improving measurements of stress and the well-being of individuals. Although progress has been made with single signal modalities like wearables and facial emotion recognition, integrating multiple modalities provides a more comprehensive understanding of stress, given that stress manifests differently across different people. Multi-modal learning aims to capitalize on the strength of each modality rather than relying on a single signal. Given the complexity of processing and integrating high-dimensional data from limited subjects, more research is needed. Numerous research efforts have been focused on fusing stress and emotion signals at an early stage, e.g., feature-level fusion using basic machine learning methods and 1D-CNN Methods. This paper proposes a multi-modal learning approach for stress detection that integrates facial landmarks and biometric signals. We test this multi-modal integration with various early-fusion and late-fusion techniques to integrate the 1D-CNN model from biometric signals and 2-D CNN using facial landmarks. We evaluate these architectures using a rigorous test of models' generalizability using the leave-one-subject-out mechanism, i.e., all samples related to a single subject are left out to train the model. Our findings show that late-fusion achieved 94.39\% accuracy, and early-fusion surpassed it with a 98.38\% accuracy rate. This research contributes valuable insights into enhancing stress detection through a multi-modal approach. The proposed research offers important knowledge in improving stress detection using a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
发展不同感知技术对个人压力测量带来了改进。虽然单模态如穿戴式设备和表情识别已经取得了进步，但是将多个模态融合提供了更全面的压力测量，因为压力在不同人群中表现不同。多模态学习希望利用每个模态的优势而不仅仅依靠单一信号。由于处理和 инте格高维数据的限制，更多的研究是必要的。许多研究团队已经关注将压力和情绪信号在早期融合，例如特征级别融合使用基本机器学习方法和1D-CNN方法。这篇论文提出了一种多模态学习方法， integrating facial landmarks and biometric signals for stress detection.我们使用不同的早期融合和晚期融合技术来融合1D-CNN模型和2D-CNN使用facial landmarks。我们使用离散一个主题机制进行模型评估，即所有与一个主题相关的样本被去除，以训练模型。我们的发现显示，晚期融合达到94.39%的准确率，而早期融合超过了它，达到98.38%的准确率。这项研究为压力检测提供了价值的新发现，并且提供了改进压力检测的多模态方法的重要知识。
</details></li>
</ul>
<hr>
<h2 id="Brief-for-the-Canada-House-of-Commons-Study-on-the-Implications-of-Artificial-Intelligence-Technologies-for-the-Canadian-Labor-Force-Generative-Artificial-Intelligence-Shatters-Models-of-AI-and-Labor"><a href="#Brief-for-the-Canada-House-of-Commons-Study-on-the-Implications-of-Artificial-Intelligence-Technologies-for-the-Canadian-Labor-Force-Generative-Artificial-Intelligence-Shatters-Models-of-AI-and-Labor" class="headerlink" title="Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor"></a>Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03595">http://arxiv.org/abs/2311.03595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morgan R. Frank</li>
<li>for: 探讨当前生产力技术的发展对工作 Market的影响，并提出政策建议以适应未来工作环境。</li>
<li>methods: 利用数据分析和预测技术来研究生产力技术对工作市场的影响，并对现有的自动化预测模型进行批判性分析。</li>
<li>results: 发现生产力技术可能会对一些以前被认为免疫自动化的职业产生影响，政策 makers应该促进工人的职业适应性，并鼓励教育机构开发适应AI技术的教育程序。<details>
<summary>Abstract</summary>
Exciting advances in generative artificial intelligence (AI) have sparked concern for jobs, education, productivity, and the future of work. As with past technologies, generative AI may not lead to mass unemployment. But, unlike past technologies, generative AI is creative, cognitive, and potentially ubiquitous which makes the usual assumptions of automation predictions ill-suited for today. Existing projections suggest that generative AI will impact workers in occupations that were previously considered immune to automation. As AI's full set of capabilities and applications emerge, policy makers should promote workers' career adaptability. This goal requires improved data on job separations and unemployment by locality and job titles in order to identify early-indicators for the workers facing labor disruption. Further, prudent policy should incentivize education programs to accommodate learning with AI as a tool while preparing students for the demands of the future of work.
</details>
<details>
<summary>摘要</summary>
新的生成人工智能技术已经引发了工作、教育、生产效率和未来工作的担忧。与过去的技术不同，生成人工智能具有创造力、认知能力和潜在的 ubique 特点，使得传统的自动化预测模型成为不适用的。现有的预测结果表明，生成人工智能可能会影响工作者在之前被认为是自动化免疫的职业。为实现工作者职业适应性，政策制定者应该促进地方各地的职业分类和失业数据的收集，以识别受到劳动干预的工作者。此外，政策应该鼓励教育计划，以便在使用人工智能为工具的同时，准备学生未来的工作需求。
</details></li>
</ul>
<hr>
<h2 id="Finding-Increasingly-Large-Extremal-Graphs-with-AlphaZero-and-Tabu-Search"><a href="#Finding-Increasingly-Large-Extremal-Graphs-with-AlphaZero-and-Tabu-Search" class="headerlink" title="Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search"></a>Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03583">http://arxiv.org/abs/2311.03583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abbas Mehrabian, Ankit Anand, Hyunjik Kim, Nicolas Sonnerat, Matej Balog, Gheorghe Comanici, Tudor Berariu, Andrew Lee, Anian Ruoss, Anna Bulanova, Daniel Toyama, Sam Blackwell, Bernardino Romera Paredes, Petar Veličković, Laurent Orseau, Joonkyung Lee, Anurag Murty Naredla, Doina Precup, Adam Zsolt Wagner</li>
<li>for: 这个论文解决了一个中央极点图论题，这个问题是根据1975年erdős的 conjecture，找到一个给定大小的图最多的边数而不包含3-或4-цикル。</li>
<li>methods: 这个论文使用了AlphaZero和tabu搜索两种方法，并通过引入课程来提高state-of-the-art下界。</li>
<li>results: 这个论文通过引入课程和提高搜索策略，提高了几个不同大小的图的下界。此外，这个论文还提出了一种灵活的图生成环境和一种 permutation-invariant的网络架构来学习搜索在图空间中。<details>
<summary>Abstract</summary>
This work studies a central extremal graph theory problem inspired by a 1975 conjecture of Erd\H{o}s, which aims to find graphs with a given size (number of nodes) that maximize the number of edges without having 3- or 4-cycles. We formulate this problem as a sequential decision-making problem and compare AlphaZero, a neural network-guided tree search, with tabu search, a heuristic local search method. Using either method, by introducing a curriculum -- jump-starting the search for larger graphs using good graphs found at smaller sizes -- we improve the state-of-the-art lower bounds for several sizes. We also propose a flexible graph-generation environment and a permutation-invariant network architecture for learning to search in the space of graphs.
</details>
<details>
<summary>摘要</summary>
Translation note:* "sequential decision-making problem" becomes "连续决策问题" (liánxù juéxì wèn tí)* "AlphaZero" becomes "AlphaZero" (ā lfah zhī)* "tabu search" becomes "tabu搜索" (tā bù sōu suǒ)* "curriculum" becomes "课程" (kèxíng)* "permutation-invariant network architecture" becomes "对称的网络架构" (duìxiàng de wǎngluò jiàgòu)
</details></li>
</ul>
<hr>
<h2 id="AI-Enabled-Unmanned-Vehicle-Assisted-Reconfigurable-Intelligent-Surfaces-Deployment-Prototyping-Experiments-and-Opportunities"><a href="#AI-Enabled-Unmanned-Vehicle-Assisted-Reconfigurable-Intelligent-Surfaces-Deployment-Prototyping-Experiments-and-Opportunities" class="headerlink" title="AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities"></a>AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent Surfaces: Deployment, Prototyping, Experiments, and Opportunities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04241">http://arxiv.org/abs/2311.04241</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li-Hsiang Shen, Kai-Ten Feng, Ta-Sung Lee, Yuan-Chun Lin, Shih-Cheng Lin, Chia-Chan Chang, Sheng-Fuh Chang</li>
<li>For: This paper focuses on the deployment of Reconfigurable Intelligent Surfaces (RIS) in wireless communication networks, specifically in the context of the sixth-generation (6G) technology. The paper explores the use of RIS to extend service coverage, reduce power consumption, and enhance spectral efficiency.* Methods: The paper discusses the theoretical and hardware aspects of RIS deployment, as well as the use of artificial intelligence (AI) and machine learning to optimize the deployment process. The authors propose a federated multi-agent reinforcement learning scheme to optimize the placement and configuration of RISs.* Results: The paper presents experimental results of the proposed i-Dris system, which achieves a transmission throughput of up to 980 Mbps under a bandwidth of 100 MHz with comparatively low complexity and rapid deployment. The results show that the i-Dris system outperforms existing works in this area.Here’s the simplified Chinese text for the three key points:* For: 这篇论文关注 sixth-generation (6G) 技术中的 Reconfigurable Intelligent Surfaces (RIS) 的部署，以扩展服务覆盖区域、降低功率消耗和提高频率效率。* Methods: 论文讨论了 RIS 部署的理论和硬件方面，以及使用人工智能 (AI) 和机器学习来优化部署过程。作者提议了一种联邦多代理人强化学习方案来优化 RIS 的分布和配置。* Results: 论文发表了 i-Dris 系统的实验结果，该系统可以在带宽 100 MHz 下实现传输吞吐量达 980 Mbps，与其他现有的方法相比，i-Dris 系统具有较低的复杂性和较快的部署速度。<details>
<summary>Abstract</summary>
The requirement of wireless data demands is increasingly high as the sixth-generation (6G) technology evolves. Reconfigurable intelligent surface (RIS) is promisingly deemed to be one of 6G techniques for extending service coverage, reducing power consumption, and enhancing spectral efficiency. In this article, we have provided some fundamentals of RIS deployment in theory and hardware perspectives as well as utilization of artificial intelligence (AI) and machine learning. We conducted an intelligent deployment of RIS (i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs associated with an mmWave base station (BS) and a receiver. The RISs are deployed on the AGV with configured incident/reflection angles. While, both the mmWave BS and receiver are associated with an edge server monitoring downlink packets for obtaining system throughput. We have designed a federated multi-agent reinforcement learning scheme associated with several AGV-RIS agents and sub-agents per AGV-RIS consisting of the deployment of position, height, orientation and elevation angles. The experimental results presented the stationary measurement in different aspects and scenarios. The i-Dris can reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with comparably low complexity as well as rapid deployment, which outperforms the other existing works. At last, we highlight some opportunities and future issues in leveraging RIS-empowered wireless communication networks.
</details>
<details>
<summary>摘要</summary>
sixth-generation (6G) 技术的无线数据需求在不断增长，而Reconfigurable intelligent surface (RIS) 被认为是6G技术的一种扩展服务覆盖、降低功率消耗和提高频率效率的方法。在这篇文章中，我们提供了RIS部署的理论和硬件视图，以及人工智能(AI)和机器学习的应用。我们开发了一个名为“智能RIS部署”（i-Dris）的原型，包括与mmWave基站（BS）和接收器相连的双频自导车（AGV）助手RIS。RIS被部署在AGV上，并配置了入射/反射角度。而BS和接收器均与边缘服务器监控下链路包，以获取系统吞吐量。我们设计了多个AGV-RIS代理和子代理，包括RIS部署的位置、高度、orientation和倾斜角度。实验结果显示，i-Dris可以在不同方面和场景下实现静态测量，并且具有相对较低的复杂性和快速部署，超过了其他已有作品。最后，我们提出了利用RIS empowered无线通信网络的机遇和未来问题。
</details></li>
</ul>
<hr>
<h2 id="Inclusive-Portraits-Race-Aware-Human-in-the-Loop-Technology"><a href="#Inclusive-Portraits-Race-Aware-Human-in-the-Loop-Technology" class="headerlink" title="Inclusive Portraits: Race-Aware Human-in-the-Loop Technology"></a>Inclusive Portraits: Race-Aware Human-in-the-Loop Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03567">http://arxiv.org/abs/2311.03567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudia Flores-Saviaga, Christopher Curtis, Saiph Savage</li>
<li>for: 这篇论文旨在提出一种基于人类社会理论的人类在循环（HITL）系统，以提高人脸验证服务的性能，特别是为人类颜色的服务。</li>
<li>methods: 该论文提出了一种名为“含容图像（Inclusive Portraits，IP）”的新方法，它将人类社会理论与人脸验证服务相结合，以提高服务的可靠性和准确性。</li>
<li>results: 实验结果表明，将种族考虑到HITL系统中可以显著提高服务的性能，特别是为人类颜色的服务。此外，该研究还发现，考虑工作者个人特点在HITL系统的设计中是非常重要的。<details>
<summary>Abstract</summary>
AI has revolutionized the processing of various services, including the automatic facial verification of people. Automated approaches have demonstrated their speed and efficiency in verifying a large volume of faces, but they can face challenges when processing content from certain communities, including communities of people of color. This challenge has prompted the adoption of "human-in-the-loop" (HITL) approaches, where human workers collaborate with the AI to minimize errors. However, most HITL approaches do not consider workers' individual characteristics and backgrounds. This paper proposes a new approach, called Inclusive Portraits (IP), that connects with social theories around race to design a racially-aware human-in-the-loop system. Our experiments have provided evidence that incorporating race into human-in-the-loop (HITL) systems for facial verification can significantly enhance performance, especially for services delivered to people of color. Our findings also highlight the importance of considering individual worker characteristics in the design of HITL systems, rather than treating workers as a homogenous group. Our research has significant design implications for developing AI-enhanced services that are more inclusive and equitable.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Low-Rank-MDPs-with-Continuous-Action-Spaces"><a href="#Low-Rank-MDPs-with-Continuous-Action-Spaces" class="headerlink" title="Low-Rank MDPs with Continuous Action Spaces"></a>Low-Rank MDPs with Continuous Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03564">http://arxiv.org/abs/2311.03564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Bennett, Nathan Kallus, Miruna Oprescu</li>
<li>for: 本文研究了将低级Markov决策过程（MDP）扩展到连续动作空间上，以提高RL学习的可靠性和可扩展性。</li>
<li>methods: 本文提出了多种具体的扩展方法，包括使用约束优化和离散化方法，以及对FLAMBE算法（Agarwal et al., 2020）进行修改。</li>
<li>results: 研究表明，无需修改FLAMBE算法，在transition函数具有Holder细化程度对动作的情况下，可以获得类似的PAC界限，而无需知道奖励函数。此外，当政策集合具有固定最小浓度或奖励函数具有Holder细化程度时，可以获得一个几乎同样的PAC界限。<details>
<summary>Abstract</summary>
Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain similar PAC bound when actions are allowed to be continuous. Specifically, when the model for transition functions satisfies a Holder smoothness condition w.r.t. actions, and either the policy class has a uniformly bounded minimum density or the reward function is also Holder smooth, we obtain a polynomial PAC bound that depends on the order of smoothness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-Unlocks-Emotions-Text-based-Emotion-Classification-Dataset-Auditing-with-Large-Language-Models"><a href="#Context-Unlocks-Emotions-Text-based-Emotion-Classification-Dataset-Auditing-with-Large-Language-Models" class="headerlink" title="Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models"></a>Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03551">http://arxiv.org/abs/2311.03551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Yang, Aditya Kommineni, Mohammad Alshehri, Nilamadhab Mohanty, Vedant Modi, Jonathan Gratch, Shrikanth Narayanan</li>
<li>for: 提高文本数据的情感分类模型性能</li>
<li>methods: 使用大语言模型生成文本上的补充 контекスト信息</li>
<li>results: 提高文本输入与人工标注的情感标签的匹配率， tanto from empirical evaluation and human evaluation<details>
<summary>Abstract</summary>
The lack of contextual information in text data can make the annotation process of text-based emotion classification datasets challenging. As a result, such datasets often contain labels that fail to consider all the relevant emotions in the vocabulary. This misalignment between text inputs and labels can degrade the performance of machine learning models trained on top of them. As re-annotating entire datasets is a costly and time-consuming task that cannot be done at scale, we propose to use the expressive capabilities of large language models to synthesize additional context for input text to increase its alignment with the annotated emotional labels. In this work, we propose a formal definition of textual context to motivate a prompting strategy to enhance such contextual information. We provide both human and empirical evaluation to demonstrate the efficacy of the enhanced context. Our method improves alignment between inputs and their human-annotated labels from both an empirical and human-evaluated standpoint.
</details>
<details>
<summary>摘要</summary>
文本数据中缺乏上下文信息可能使文本情感分类数据集的标注过程变得困难。因此，这些数据集的标签通常不会考虑所有可能的情感词汇。这种文本输入和标签之间的不一致可能使机器学习模型在这些数据集上训练时表现下降。然而，重新标注整个数据集是一项成本高、时间consuming的任务，不能在大规模进行。因此，我们提议使用大语言模型的表达能力来生成更多的上下文信息，以增强输入文本与注解的情感标签的协调。在这种情况下，我们提出了文本上下文的正式定义，并提出了一种提问策略来增强文本上下文信息。我们通过人类和实验评估来证明我们的方法可以提高输入文本与其人类注解标签之间的协调。
</details></li>
</ul>
<hr>
<h2 id="United-We-Stand-Divided-We-Fall-UnityGraph-for-Unsupervised-Procedure-Learning-from-Videos"><a href="#United-We-Stand-Divided-We-Fall-UnityGraph-for-Unsupervised-Procedure-Learning-from-Videos" class="headerlink" title="United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos"></a>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03550">http://arxiv.org/abs/2311.03550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Bansal, Chetan Arora, C. V. Jawahar</li>
<li>for: 本研究旨在解决现有方法无法捕捉多个视频中关键步骤的问题，通过提出一种无监督图格学习（GPL）框架。</li>
<li>methods: GPL使用一种新的 UnityGraph 表示所有任务视频的图形，以获得内视频和多视频上下文。然后，使用 Node2Vec 算法更新 UnityGraph 中的坐标，以实现无监督性的同步。最后，使用 KMeans 聚类算法确定关键步骤。</li>
<li>results: 对于 ProceL、CrossTask 和 EgoProceL 测试集，GPL 实现了平均提高2% 和 3.6% 相比于现有方法。<details>
<summary>Abstract</summary>
Given multiple videos of the same task, procedure learning addresses identifying the key-steps and determining their order to perform the task. For this purpose, existing approaches use the signal generated from a pair of videos. This makes key-steps discovery challenging as the algorithms lack inter-videos perspective. Instead, we propose an unsupervised Graph-based Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that represents all the videos of a task as a graph to obtain both intra-video and inter-videos context. Further, to obtain similar embeddings for the same key-steps, the embeddings of UnityGraph are updated in an unsupervised manner using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and EgoProceL datasets and achieve an average improvement of 2% on third-person datasets and 3.6% on EgoProceL over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>给定多个视频任务，程序学习关注于标识任务中关键步骤的顺序执行。现有的方法使用视频对的信号来实现这一目标，但这会使关键步骤发现困难，因为算法缺乏视频间视角。我们提出一种不supervised图像基本学习（GPL）框架。GPL包括一种新的 UnityGraph，它将所有任务视频表示为一个图来获取任务内和任务间上下文。然后，使用Node2Vec算法更新 UnityGraph 中的表示，以获取同样的关键步骤的相似嵌入。最后，使用 KMeans 聚类算法确定关键步骤。我们在 ProceL、CrossTask 和 EgoProceL 数据集上测试 GPL，并在第三人数据集上提高了2%，在 EgoProceL 数据集上提高了3.6% 以上。
</details></li>
</ul>
<hr>
<h2 id="InterVLS-Interactive-Model-Understanding-and-Improvement-with-Vision-Language-Surrogates"><a href="#InterVLS-Interactive-Model-Understanding-and-Improvement-with-Vision-Language-Surrogates" class="headerlink" title="InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates"></a>InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03547">http://arxiv.org/abs/2311.03547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbin Huang, Wenbin He, Liang Gou, Liu Ren, Chris Bryan</li>
<li>for: 帮助用户更好地理解深度学习模型和改进它们的性能</li>
<li>methods: 使用文本对齐的概念发现和模型无关的直线函数来衡量概念的影响</li>
<li>results: 在用户研究中，InterVLS有效地帮助用户 identific 模型中最有影响的概念，获得性能概念和调整概念影响以改进模型<details>
<summary>Abstract</summary>
Deep learning models are widely used in critical applications, highlighting the need for pre-deployment model understanding and improvement. Visual concept-based methods, while increasingly used for this purpose, face challenges: (1) most concepts lack interpretability, (2) existing methods require model knowledge, often unavailable at run time. Additionally, (3) there lacks a no-code method for post-understanding model improvement. Addressing these, we present InterVLS. The system facilitates model understanding by discovering text-aligned concepts, measuring their influence with model-agnostic linear surrogates. Employing visual analytics, InterVLS offers concept-based explanations and performance insights. It enables users to adjust concept influences to update a model, facilitating no-code model improvement. We evaluate InterVLS in a user study, illustrating its functionality with two scenarios. Results indicates that InterVLS is effective to help users identify influential concepts to a model, gain insights and adjust concept influence to improve the model. We conclude with a discussion based on our study results.
</details>
<details>
<summary>摘要</summary>
深度学习模型在关键应用中广泛使用，高亮了预部署模型理解和改进的需求。基于视觉概念的方法在这个目的上增加使用，但面临以下挑战：（1）大多数概念无法解释，（2）现有方法往往需要模型知识，而运行时这些知识通常不可用，（3）无法使用无代码方法进行后续模型改进。为解决这些问题，我们提出了InterVLS。该系统通过发现与文本对齐的概念，使用模型无关的线性代理来衡量这些概念的影响。通过视觉分析，InterVLS提供了基于概念的解释和性能印象。它允许用户根据概念的影响来更新模型，从而实现无代码模型改进。我们在用户研究中评估了InterVLS，并通过两个场景 illustrate its 功能。结果表明，InterVLS能够帮助用户Identify模型中的重要概念，获得情况和更改概念的影响来改进模型。我们根据我们的研究结果进行了讨论。
</details></li>
</ul>
<hr>
<h2 id="PcLast-Discovering-Plannable-Continuous-Latent-States"><a href="#PcLast-Discovering-Plannable-Continuous-Latent-States" class="headerlink" title="PcLast: Discovering Plannable Continuous Latent States"></a>PcLast: Discovering Plannable Continuous Latent States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03534">http://arxiv.org/abs/2311.03534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Koul, Shivakanth Sujit, Shaoru Chen, Ben Evans, Lili Wu, Byron Xu, Rajan Chari, Riashat Islam, Raihan Seraj, Yonathan Efroni, Lekan Molu, Miro Dudik, John Langford, Alex Lamb</li>
<li>for: goal-conditioned planning</li>
<li>methods: multi-step inverse dynamics, latent representation, and associating reachable states together in $\ell_2$ space</li>
<li>results: significant improvements in sampling efficiency and yields layered state abstractions that enable computationally efficient hierarchical planning.Here’s the Chinese translation of the three points:</li>
<li>for: 目的conditioned планинг</li>
<li>methods: 多步反动力学学习、占据表示和在 $\ell_2$ 空间相关联可达状态</li>
<li>results: 提高采样效率，并生成Computational efficient的层次 планинг。<details>
<summary>Abstract</summary>
Goal-conditioned planning benefits from learned low-dimensional representations of rich, high-dimensional observations. While compact latent representations, typically learned from variational autoencoders or inverse dynamics, enable goal-conditioned planning they ignore state affordances, thus hampering their sample-efficient planning capabilities. In this paper, we learn a representation that associates reachable states together for effective onward planning. We first learn a latent representation with multi-step inverse dynamics (to remove distracting information); and then transform this representation to associate reachable states together in $\ell_2$ space. Our proposals are rigorously tested in various simulation testbeds. Numerical results in reward-based and reward-free settings show significant improvements in sampling efficiency, and yields layered state abstractions that enable computationally efficient hierarchical planning.
</details>
<details>
<summary>摘要</summary>
goal-conditioned 规划受惠于学习的低维 Observations 的归一化表示。 although compact latent representations, typically learned from variational autoencoders or inverse dynamics, enable goal-conditioned planning, they ignore state affordances, thus hampering their sample-efficient planning capabilities. In this paper, we learn a representation that associates reachable states together for effective onward planning. We first learn a latent representation with multi-step inverse dynamics (to remove distracting information); and then transform this representation to associate reachable states together in $\ell_2$ space. Our proposals are rigorously tested in various simulation testbeds. Numerical results in reward-based and reward-free settings show significant improvements in sampling efficiency, and yields layered state abstractions that enable computationally efficient hierarchical planning.Here's the breakdown of the translation:* goal-conditioned 规划 (goal-conditioned planning)* 学习 (learned)* 低维 Observations (low-dimensional observations)* 归一化表示 (latent representation)* compact latent representations (compact latent representations)* 通常来自 (typically learned)* variational autoencoders (variational autoencoders)* inverse dynamics (inverse dynamics)* 忽略 (ignore)* state affordances (state affordances)* 因此 (thus)* 阻碍 (hampering)* 效果 (effective)* onward planning (onward planning)* 学习 (learn)* 多步 inverse dynamics (multi-step inverse dynamics)* 去掉干扰信息 (to remove distracting information)* 转换 (transform)* 在 $\ell_2$ 空间 associate (associate)* reachable states together (reachable states together)* 数学结果 (numerical results)* 在 reward-based 和 reward-free 设定下 show (show)* 显著提高 (significant improvements)* 采样效率 (sampling efficiency)* 层次状态抽象 (layered state abstractions)* 计算效率 (computationally efficient)* 层次规划 (hierarchical planning)
</details></li>
</ul>
<hr>
<h2 id="Brain-Networks-and-Intelligence-A-Graph-Neural-Network-Based-Approach-to-Resting-State-fMRI-Data"><a href="#Brain-Networks-and-Intelligence-A-Graph-Neural-Network-Based-Approach-to-Resting-State-fMRI-Data" class="headerlink" title="Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data"></a>Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03520">http://arxiv.org/abs/2311.03520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu<br>for:This paper aims to develop a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on resting-state functional magnetic resonance imaging (rsfMRI) derived static functional network connectivity matrices.methods:The proposed BrainRGIN architecture incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer, TopK pooling, and attention-based readout functions to predict intelligence. The approach uses rsfMRI data to capture the functional organization of the brain without relying on specific tasks or stimuli.results:The proposed BrainRGIN model achieved lower mean squared errors and higher correlation scores than existing relevant graph architectures and other traditional machine learning models for all of the intelligence prediction tasks. The middle frontal gyrus was found to contribute significantly to both fluid and crystallized intelligence, while total composite scores identified a diverse set of brain regions as relevant, highlighting the complex nature of total intelligence.<details>
<summary>Abstract</summary>
Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the Adolescent Brain Cognitive Development Dataset, and demonstrated its effectiveness in predicting individual differences in intelligence. Our model achieved lower mean squared errors and higher correlation scores than existing relevant graph architectures and other traditional machine learning models for all of the intelligence prediction tasks. The middle frontal gyrus exhibited a significant contribution to both fluid and crystallized intelligence, suggesting their pivotal role in these cognitive processes. Total composite scores identified a diverse set of brain regions to be relevant which underscores the complex nature of total intelligence.
</details>
<details>
<summary>摘要</summary>
RESTING-STATE FUNCTIONAL MAGNETIC RESONANCE IMAGING (RSFMRI) 是一种强大的工具，可以评估大脑功能和认知过程之间的关系，因为它可以在不基于特定任务或刺激的情况下捕捉大脑的功能组织结构。在这篇论文中，我们提出了一种新的模型建立方式，称为BrainRGIN，可以使用图 neural networks 预测智商（流动、晶化和总智商）。 extending from the existing graph convolution networks，我们的方法包括嵌入和图同构网络，以及 TopK pooling 和注意力基本函数。我们在大量数据集，即青春期大脑认知发展数据集上评估了我们的提议的建立方式，并证明了它在智商预测任务中的有效性。我们的模型比拥有相关的图建立方式和传统机器学习模型都具有更低的平均平方误差和更高的相关性分数。中顶前颞卷积区显示在流动和晶化智商中具有重要作用，这表明它们在这些认知过程中发挥着关键作用。总合分数表明大脑各区域具有不同的重要性，这反映了智商的复杂结构。
</details></li>
</ul>
<hr>
<h2 id="MFAAN-Unveiling-Audio-Deepfakes-with-a-Multi-Feature-Authenticity-Network"><a href="#MFAAN-Unveiling-Audio-Deepfakes-with-a-Multi-Feature-Authenticity-Network" class="headerlink" title="MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network"></a>MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03509">http://arxiv.org/abs/2311.03509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik Sivarama Krishnan, Koushik Sivarama Krishnan</li>
<li>for: 防止深伪音频干预信息传播</li>
<li>methods: 多元特征数据网络（MFAAN），融合多种音频表现，包括MFCC、LFCC和Chroma-STFT，实现多元认知音频内容，精确地识别伪造音频</li>
<li>results: 在两个 benchmark 数据集上，MFAAN 表现出色，实现准确率98.93%和94.47%，说明 MFAAN 的可靠性和应用价值<details>
<summary>Abstract</summary>
In the contemporary digital age, the proliferation of deepfakes presents a formidable challenge to the sanctity of information dissemination. Audio deepfakes, in particular, can be deceptively realistic, posing significant risks in misinformation campaigns. To address this threat, we introduce the Multi-Feature Audio Authenticity Network (MFAAN), an advanced architecture tailored for the detection of fabricated audio content. MFAAN incorporates multiple parallel paths designed to harness the strengths of different audio representations, including Mel-frequency cepstral coefficients (MFCC), linear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier Transform (Chroma-STFT). By synergistically fusing these features, MFAAN achieves a nuanced understanding of audio content, facilitating robust differentiation between genuine and manipulated recordings. Preliminary evaluations of MFAAN on two benchmark datasets, 'In-the-Wild' Audio Deepfake Data and The Fake-or-Real Dataset, demonstrate its superior performance, achieving accuracies of 98.93% and 94.47% respectively. Such results not only underscore the efficacy of MFAAN but also highlight its potential as a pivotal tool in the ongoing battle against deepfake audio content.
</details>
<details>
<summary>摘要</summary>
现代数字时代，深度模仿技术的普及带来了信息传递的威胁。特别是音频深度模仿，可能造成误导性的虚假信息。为解决这一问题，我们介绍了多元特征音频真实性网络（MFAAN），这是一种专门为检测假造音频内容而设计的高级架构。MFAAN通过并行的多个路径，利用不同的音频表示方法，包括Mel-frequency cepstral coefficients（MFCC）、linear-frequency cepstral coefficients（LFCC）和Chroma Short Time Fourier Transform（Chroma-STFT）。通过这种综合融合，MFAAN实现了对音频内容的细致理解，从而实现了对假造和真实录音的分辨率。初步的评估结果表明，MFAAN在两个标准数据集上（'In-the-Wild' Audio Deepfake Data和The Fake-or-Real Dataset）达到了98.93%和94.47%的准确率，这不仅证明了MFAAN的有效性，还 highlighted its potential作为对深度模仿音频内容的战斗工具。
</details></li>
</ul>
<hr>
<h2 id="Astrocytes-as-a-mechanism-for-meta-plasticity-and-contextually-guided-network-function"><a href="#Astrocytes-as-a-mechanism-for-meta-plasticity-and-contextually-guided-network-function" class="headerlink" title="Astrocytes as a mechanism for meta-plasticity and contextually-guided network function"></a>Astrocytes as a mechanism for meta-plasticity and contextually-guided network function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03508">http://arxiv.org/abs/2311.03508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lulu Gong, Fabio Pasqualetti, Thomas Papouin, ShiNung Ching</li>
<li>for: 这种研究旨在探讨astrocyte在脑中的功能，以及它们如何与神经元和 synapse 交互以实现学习。</li>
<li>methods: 这个研究使用了形式分析和逻辑推理来描述astrocyte如何影响神经元和synapse的adaptation，以及如何在不同的时间尺度上实现学习。</li>
<li>results: 研究发现，在存在时间尺度分开的astrocyte干扰下，神经元和synapse可以更好地适应不同的任务参数，并且可以在多个随机变化的上下文中学习。这种方法比传统的神经网络和非网络算法更可靠。<details>
<summary>Abstract</summary>
Astrocytes are a highly expressed and highly enigmatic cell-type in the mammalian brain. Traditionally viewed as a mediator of basic physiological sustenance, it is increasingly recognized that astrocytes may play a more direct role in neural computation. A conceptual challenge to this idea is the fact that astrocytic activity takes a very different form than that of neurons, and in particular, occurs at orders-of-magnitude slower time-scales. In the current paper, we engage how such time-scale separation may endow astrocytes with the capability to enable learning in context-dependent settings, where fluctuations in task parameters may occur much more slowly than within-task requirements. This idea is based on the recent supposition that astrocytes, owing to their sensitivity to a host of physiological covariates, may be particularly well poised to modulate the dynamics of neural circuits in functionally salient ways. We pose a general model of neural-synaptic-astrocyte interaction and use formal analysis to characterize how astrocytic modulation may constitute a form of meta-plasticity, altering the ways in which synapses and neurons adapt as a function of time. We then embed this model in a bandit-based reinforcement learning task environment, and show how the presence of time-scale separated astrocytic modulation enables learning over multiple fluctuating contexts. Indeed, these networks learn far more reliably versus dynamically homogenous networks and conventional non-network-based bandit algorithms. Our results indicate how the presence of neural-astrocyte interaction in the brain may benefit learning over different time-scale and the conveyance of task relevant contextual information onto circuit dynamics.
</details>
<details>
<summary>摘要</summary>
astrocytes是大脑中高度表达和高度神秘的细胞类型。传统上视为神经元的调节剂，但现在越来越认为astrocytes可能直接参与神经计算。一个挑战是astrocyte活动的时间尺度与神经元活动完全不同，astrocyte活动更加慢，甚至是神经元活动的数个量级慢。在当前文章中，我们探讨了如何这种时间尺度差异可能使astrocytes具有学习能力。我们提出了神经元-synapse-astrocyte交互的概念模型，并使用正式分析来描述如何astrocyte干涉可能导致神经细胞和 synapse 的适应性改变。然后，我们将这个模型嵌入到了一个基于奖励学习的bandit任务环境中，并证明了在多个随机变化的上下文中，astrocyte干涉的存在可以使网络学习更加可靠。我们的结果表明，在脑中具有神经-astrocyte交互的存在可以提高学习的可靠性和将任务相关的上下文信息传递到神经细胞动力学中。
</details></li>
</ul>
<hr>
<h2 id="Environmental-Impact-Based-Multi-Agent-Reinforcement-Learning"><a href="#Environmental-Impact-Based-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Environmental-Impact Based Multi-Agent Reinforcement Learning"></a>Environmental-Impact Based Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04240">http://arxiv.org/abs/2311.04240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farinaz Alamiyan-Harandi, Pouria Ramazi</li>
<li>for: 提高社会冲击协力和强化个体影响集体结果</li>
<li>methods: 环境影响多智能 reinforcement learning（EMuReL）方法，每个代理都估计每个其他代理在环境中的环境影响</li>
<li>results: 在清理（resp. 收割）环境测试环境中，使用EMuReL训练的代理协作更有效，获得$54%$ ($39%$)和$20%$ ($44%$)更多的总奖励，同时保持同等水平的合作水平。<details>
<summary>Abstract</summary>
To promote cooperation and strengthen the individual impact on the collective outcome in social dilemmas, we propose the Environmental-impact Multi-Agent Reinforcement Learning (EMuReL) method where each agent estimates the "environmental impact" of every other agent, that is, the difference in the current environment state compared to the hypothetical environment in the absence of that other agent. Inspired by the Inequity Aversion model, the agent then compares its own reward with those of its fellows multiplied by their environmental impacts. If its reward exceeds the scaled reward of one of its fellows, the agent takes "social responsibility" toward that fellow by reducing its own reward. Therefore, the less influential an agent is in reaching the current state, the more social responsibility is taken by other agents. Experiments in the Cleanup (resp. Harvest) test environment demonstrate that agents trained based on EMuReL learn to cooperate more effectively and obtain $54\%$ ($39\%$) and $20\%$ ($44\%$) more total rewards while preserving the same cooperation levels compared to when they are trained based on the two state-of-the-art reward reshaping methods inequity aversion and social influence.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Kindness-in-Multi-Agent-Reinforcement-Learning"><a href="#Kindness-in-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Kindness in Multi-Agent Reinforcement Learning"></a>Kindness in Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04239">http://arxiv.org/abs/2311.04239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farinaz Alamiyan-Harandi, Mersad Hassanjani, Pouria Ramazi</li>
<li>for: 本研究旨在帮助合作agent在多智能体强化学习中增强协作能力，通过基于人类行为概念的KindMARL方法。</li>
<li>methods: KindMARL方法基于对agent动作的反思，对环境影响的估计，并通过对每个奖励的比较来评估同伙的善意。</li>
<li>results: 实验结果表明，基于KindMARL方法培训的合作agent在Cleanup和Harvest环境中赢得了89%和37%的总奖励，比基于不平等恐惧和社会影响方法的培训更高。此外，KindMARL方法在交通灯控制问题中也得到了效果。<details>
<summary>Abstract</summary>
In human societies, people often incorporate fairness in their decisions and treat reciprocally by being kind to those who act kindly. They evaluate the kindness of others' actions not only by monitoring the outcomes but also by considering the intentions. This behavioral concept can be adapted to train cooperative agents in Multi-Agent Reinforcement Learning (MARL). We propose the KindMARL method, where agents' intentions are measured by counterfactual reasoning over the environmental impact of the actions that were available to the agents. More specifically, the current environment state is compared with the estimation of the current environment state provided that the agent had chosen another action. The difference between each agent's reward, as the outcome of its action, with that of its fellow, multiplied by the intention of the fellow is then taken as the fellow's "kindness". If the result of each reward-comparison confirms the agent's superiority, it perceives the fellow's kindness and reduces its own reward. Experimental results in the Cleanup and Harvest environments show that training based on the KindMARL method enabled the agents to earn 89\% (resp. 37\%) and 44% (resp. 43\%) more total rewards than training based on the Inequity Aversion and Social Influence methods. The effectiveness of KindMARL is further supported by experiments in a traffic light control problem.
</details>
<details>
<summary>摘要</summary>
在人类社会中，人们常常在做出决定时包含公平性，并且往往以 reciprocal 的方式对待那些行为 kindly。他们评估他人的善良行为不仅从结果来评估，还从计划的意图来评估。这种行为概念可以适应培养合作代理人在多代理人学习环境（MARL）中。我们提出了 KindMARL 方法，其中代理人的意图通过对环境状态的 counterfactual 推理来衡量。具体来说，当前环境状态与代理人选择的行动可能的环境状态进行比较，然后计算每个代理人的奖励，并且将奖励与其他代理人的奖励进行比较。如果每个代理人的奖励与其他代理人的奖励之比大于或等于一定的阈值，那么该代理人将认为对方有善良行为，并将其奖励减少。实验结果表明，基于 KindMARL 方法进行训练后，代理人能够获得 89% (resp. 37%) 和 44% (resp. 43%) 更多的总奖励，相比于基于不平等恐惧和社会影响方法进行训练。 KindMARL 的效果还得到了在交通灯控制问题上的实验支持。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Diffusion-for-Privacy-Sensitive-Recommender-Systems"><a href="#Multi-Resolution-Diffusion-for-Privacy-Sensitive-Recommender-Systems" class="headerlink" title="Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems"></a>Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03488">http://arxiv.org/abs/2311.03488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Derek Lilienthal, Paul Mello, Magdalini Eirinaki, Stas Tiomkin<br>for: 这个论文旨在提出一种基于噪声模型的推荐系统，以保护用户隐私和安全。methods: 该方法使用扩散模型来生成可信度高的假数据，以取代或补充原始数据。results: 该方法比基于生成对抗网络、变量自动编码器和最近提出的扩散模型表现出色，平均提高了4.30%的Recall@$n$和4.65%的NDCG@$n$。<details>
<summary>Abstract</summary>
While recommender systems have become an integral component of the Web experience, their heavy reliance on user data raises privacy and security concerns. Substituting user data with synthetic data can address these concerns, but accurately replicating these real-world datasets has been a notoriously challenging problem. Recent advancements in generative AI have demonstrated the impressive capabilities of diffusion models in generating realistic data across various domains. In this work we introduce a Score-based Diffusion Recommendation Model (SDRM), which captures the intricate patterns of real-world datasets required for training highly accurate recommender systems. SDRM allows for the generation of synthetic data that can replace existing datasets to preserve user privacy, or augment existing datasets to address excessive data sparsity. Our method outperforms competing baselines such as generative adversarial networks, variational autoencoders, and recently proposed diffusion models in synthesizing various datasets to replace or augment the original data by an average improvement of 4.30% in Recall@$n$ and 4.65% in NDCG@$n$.
</details>
<details>
<summary>摘要</summary>
“优化推荐系统的重要组成部分是推荐系统，但它们对用户数据的依赖带来隐私和安全问题。使用生成的数据来取代用户数据可以解决这些问题，但实际生成这些真实世界数据集的问题是极其困难的。现代生成AI技术已经展示了对于不同领域的数据生成的杰出能力。在这个研究中，我们提出了一个Score-based Diffusion Recommendation Model（SDRM），可以实现真实世界数据集中的复杂模式，并且可以用来取代或补充原始数据，以保持用户隐私和增强推荐系统的准确度。我们的方法在对不同数据集进行生成和补充时，较前者4.30%和4.65%的NDCG@$n$和Recall@$n$的平均提升。”
</details></li>
</ul>
<hr>
<h2 id="CLIP-Motion-Learning-Reward-Functions-for-Robotic-Actions-Using-Consecutive-Observations"><a href="#CLIP-Motion-Learning-Reward-Functions-for-Robotic-Actions-Using-Consecutive-Observations" class="headerlink" title="CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations"></a>CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03485">http://arxiv.org/abs/2311.03485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuzhe Dang, Stefan Edelkamp, Nicolas Ribault</li>
<li>for: 本文提出了一种新的方法，用于通过CLIP模型学习机器人动作的奖励函数。传统的奖励函数设计经常靠manual feature engineering，可能难以泛化到多种任务。我们的方法跳过这个挑战，利用CLIP模型对状态特征和图像输入进行有效处理。</li>
<li>methods: 我们的模型使用了CLIP模型，将两个连续的观察对比，并且能够准确地确定执行的动作。</li>
<li>results: 我们通过实验评估，证明了我们的方法在机器人动作中 precisely 地推断动作和其批处增强了人工奖励学习的训练。<details>
<summary>Abstract</summary>
This paper presents a novel method for learning reward functions for robotic motions by harnessing the power of a CLIP-based model. Traditional reward function design often hinges on manual feature engineering, which can struggle to generalize across an array of tasks. Our approach circumvents this challenge by capitalizing on CLIP's capability to process both state features and image inputs effectively. Given a pair of consecutive observations, our model excels in identifying the motion executed between them. We showcase results spanning various robotic activities, such as directing a gripper to a designated target and adjusting the position of a cube. Through experimental evaluations, we underline the proficiency of our method in precisely deducing motion and its promise to enhance reinforcement learning training in the realm of robotics.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的奖函数学习方法，通过使用基于 CLIP 的模型来实现。传统的奖函数设计常常靠于手动特征工程，这可能难以泛化到多种任务上。我们的方法则利用 CLIP 模型能够有效处理状态特征和图像输入，从而缺乏手动特征工程的限制。给定两个连续观察结果，我们的模型能够准确地识别执行的动作。我们在不同的 робо类活动中，如指定目标上的抓取器和立方体的位置调整等，展示了我们的方法的精准性和其在机器人学习训练中的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-Loss-based-Feature-Fusion-and-Top-Two-Voting-Ensemble-Decision-Strategy-for-Facial-Expression-Recognition-in-the-Wild"><a href="#Multi-Loss-based-Feature-Fusion-and-Top-Two-Voting-Ensemble-Decision-Strategy-for-Facial-Expression-Recognition-in-the-Wild" class="headerlink" title="Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild"></a>Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03478">http://arxiv.org/abs/2311.03478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyao Zhou, Yuanlun Xie, Wenhong Tian</li>
<li>for: 本研究旨在提高在野外情绪识别（FER）的性能，涉及到图像质量和计算机视觉领域。</li>
<li>methods: 本研究使用了内部特征结合和多个网络之间的特征结合，以及集成策略。特别是，提出了一个新的单模型named R18+FAML，以及一个集成模型named R18+FAML-FGA-T2V，以提高FER在野外的性能。</li>
<li>results: 经验表明，我们的单模型R18+FAML和集成模型R18+FAML-FGA-T2V在三个挑战性的FER数据集上达到了$\left( 90.32, 62.17, 65.83 \right)%$和$\left( 91.59, 63.27, 66.63 \right)%$的准确率，分别超过了当前最佳结果。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) in the wild is a challenging task affected by the image quality and has attracted broad interest in computer vision. There is no research using feature fusion and ensemble strategy for FER simultaneously. Different from previous studies, this paper applies both internal feature fusion for a single model and feature fusion among multiple networks, as well as the ensemble strategy. This paper proposes one novel single model named R18+FAML, as well as one ensemble model named R18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on the structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and three Attention blocks using Multiple Loss functions (FAML) to improve the diversity of the feature extraction. To improve the performance of R18+FAML, we propose a Feature fusion among networks based on the Genetic Algorithm (FGA), which can fuse the convolution kernels for feature extraction of multiple networks. On the basis of R18+FAML and FGA, we propose one ensemble strategy, i.e., the Top Two Voting (T2V) to support the classification of FER, which can consider more classification information comprehensively. Combining the above strategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas. Extensive experiments demonstrate that our single model R18+FAML and the ensemble model R18+FAML-FGA-T2V achieve the accuracies of $\left( 90.32, 62.17, 65.83 \right)\%$ and $\left( 91.59, 63.27, 66.63 \right)\%$ on three challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7 respectively, both outperforming the state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
Facial expression recognition (FER) in the wild is a challenging task affected by image quality and has attracted broad interest in computer vision. There is no research using feature fusion and ensemble strategy for FER simultaneously. Different from previous studies, this paper applies both internal feature fusion for a single model and feature fusion among multiple networks, as well as the ensemble strategy. This paper proposes one novel single model named R18+FAML, as well as one ensemble model named R18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on the structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and three Attention blocks using Multiple Loss functions (FAML) to improve the diversity of the feature extraction. To improve the performance of R18+FAML, we propose a Feature fusion among networks based on the Genetic Algorithm (FGA), which can fuse the convolution kernels for feature extraction of multiple networks. On the basis of R18+FAML and FGA, we propose one ensemble strategy, i.e., the Top Two Voting (T2V) to support the classification of FER, which can consider more classification information comprehensively. Combining the above strategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas. Extensive experiments demonstrate that our single model R18+FAML and the ensemble model R18+FAML-FGA-T2V achieve the accuracies of $(90.32, 62.17, 65.83)\%$ and $(91.59, 63.27, 66.63)\%$ on three challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7 respectively, both outperforming the state-of-the-art results.
</details></li>
</ul>
<hr>
<h2 id="FinA-Fairness-of-Adverse-Effects-in-Decision-Making-of-Human-Cyber-Physical-System"><a href="#FinA-Fairness-of-Adverse-Effects-in-Decision-Making-of-Human-Cyber-Physical-System" class="headerlink" title="FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System"></a>FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03468">http://arxiv.org/abs/2311.03468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Zhao, Salma Elmalaki</li>
<li>for: This paper focuses on ensuring fairness in decision-making systems within Human-Cyber-Physical-Systems (HCPS), particularly in the context of diverse individuals with varying behaviors and expectations.</li>
<li>methods: The paper introduces the concept of Fairness-in-Adverse-Effects (FinA) and proposes a comprehensive set of five formulations to address the challenge of fairness, taking into account both instantaneous and long-term aspects of adverse effects.</li>
<li>results: The evaluation conducted within the domain of smart homes demonstrates that the adoption of FinA significantly enhances the overall perception of fairness among individuals, with an average improvement of 66.7% compared to the state-of-the-art method.<details>
<summary>Abstract</summary>
Ensuring fairness in decision-making systems within Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when diverse individuals, each with varying behaviors and expectations, coexist within the same application space, influenced by a shared set of control actions in the system. The long-term adverse effects of these actions further pose the challenge, as historical experiences and interactions shape individual perceptions of fairness. This paper addresses the challenge of fairness from an equity perspective of adverse effects, taking into account the dynamic nature of human behavior and evolving preferences while recognizing the lasting impact of adverse effects. We formally introduce the concept of Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a comprehensive set of five formulations for FinA, encompassing both the instantaneous and long-term aspects of adverse effects. To empirically validate the effectiveness of our FinA approach, we conducted an evaluation within the domain of smart homes, a pertinent HCPS application. The outcomes of our evaluation demonstrate that the adoption of FinA significantly enhances the overall perception of fairness among individuals, yielding an average improvement of 66.7% when compared to the state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
（ Ensuring fairness in decision-making systems within Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when diverse individuals, each with varying behaviors and expectations, coexist within the same application space, influenced by a shared set of control actions in the system. The long-term adverse effects of these actions further pose the challenge, as historical experiences and interactions shape individual perceptions of fairness. This paper addresses the challenge of fairness from an equity perspective of adverse effects, taking into account the dynamic nature of human behavior and evolving preferences while recognizing the lasting impact of adverse effects. We formally introduce the concept of Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a comprehensive set of five formulations for FinA, encompassing both the instantaneous and long-term aspects of adverse effects. To empirically validate the effectiveness of our FinA approach, we conducted an evaluation within the domain of smart homes, a pertinent HCPS application. The outcomes of our evaluation demonstrate that the adoption of FinA significantly enhances the overall perception of fairness among individuals, yielding an average improvement of 66.7% when compared to the state-of-the-art method.）Here's the translation in Simplified Chinese:保持 Human-Cyber-Physical-Systems (HCPS) 中的决策系统 fairness 是一项急需解决的问题，特别是当多个不同的个体，每个人有不同的行为和期望，共同存在同一个应用空间中，受到共享的控制动作影响。长期的不良影响还提出了挑战，因为历史经验和互动对每个人的公正感产生影响。本文从Equity 的视角来Address 这个公正感 Challenge，考虑到人类行为的动态性和不断改变的偏好，同时认可长期的不良影响。我们在 HCPS 上正式引入 Fairness-in-Adverse-Effects (FinA) 概念，并提出了 five 种 FinA 形式，涵盖了 immediate 和长期的不良影响方面。为了证明我们 FinA 方法的有效性，我们在智能家居领域进行了评估。评估结果表明，通过 FinA 的采用，人们对公正感的总体评价得到了66.7%的平均提高，相比之下与当前方法的提高率。
</details></li>
</ul>
<hr>
<h2 id="Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation"><a href="#Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation" class="headerlink" title="Exploitation-Guided Exploration for Semantic Embodied Navigation"></a>Exploitation-Guided Exploration for Semantic Embodied Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03357">http://arxiv.org/abs/2311.03357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain</li>
<li>for: 这个论文主要针对embodied navigation和sim-to-robot transfer的问题进行研究，探讨了一种可靠的方法来 sintactically combine these components。</li>
<li>methods: 作者提出了Exploitation-Guided Exploration（XGX）方法，其中有一个分配探索和利用的两个模块，当目标变得可见时，利用模块会取代探索模块，并继续驱动一个被 override 的政策优化。</li>
<li>results: XGX方法在Object Navigation任务上达到了70%的状态听报到的性能，比之前的最佳基准值提高了3%。此外，通过 Targeted analysis 表明，XGX方法在目标conditined exploration中更高效。最后，作者在硬件机器上进行了sim-to-real transfer，并证明XGX方法在实际场景中表现出两倍于最佳基准值的性能。<details>
<summary>Abstract</summary>
In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XGX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XGX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io
</details>
<details>
<summary>摘要</summary>
Recent progress in embodied navigation and sim-to-robot transfer has led to the emergence of modular policies as a de facto framework. However, there is more to compositionality than just decomposing the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Specifically, we propose Exploitation-Guided Exploration (XGX), where separate modules for exploration and exploitation are combined in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation when the goal becomes visible, and the crucial aspect of this approach is that the exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. In addition to better accuracy, we show through targeted analysis that XGX is also more efficient at goal-conditioned exploration. Furthermore, we demonstrate sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking. For more information, please visit the project page at xgxvisnav.github.io.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis"><a href="#SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis" class="headerlink" title="SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis"></a>SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03355">http://arxiv.org/abs/2311.03355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prismformore/seggen">https://github.com/prismformore/seggen</a></li>
<li>paper_authors: Hanrong Ye, Jason Kuen, Qing Liu, Zhe Lin, Brian Price, Dan Xu</li>
<li>for: 提高图像分割模型的性能，尤其是在 semantic segmentation、panoptic segmentation 和 instance segmentation 领域。</li>
<li>methods: 使用 two 种数据生成策略：MaskSyn 和 ImgSyn，它们可以增加数据的多样性，以便更好地训练图像分割模型。</li>
<li>results: 在 ADE20K 和 COCO 测试集上，使用 SegGen 生成的数据可以大幅提高现有的图像分割模型的性能，包括 Mask2Former R50 和 Mask2Former Swin-L。特别是，ADE20K mIoU 中 Mask2Former R50 的性能从 47.2 提高到 49.9 (+2.7)，而 Mask2Former Swin-L 的性能从 56.1 提高到 57.4 (+1.3)。这些出色的结果表明 SegGen 可以在有限的人工标注数据上提高图像分割模型的性能，同时也使得模型在未看到的领域上更加稳定。<details>
<summary>Abstract</summary>
We propose SegGen, a highly-effective training data generation method for image segmentation, which pushes the performance limits of state-of-the-art segmentation models to a significant extent. SegGen designs and integrates two data generation strategies: MaskSyn and ImgSyn. (i) MaskSyn synthesizes new mask-image pairs via our proposed text-to-mask generation model and mask-to-image generation model, greatly improving the diversity in segmentation masks for model supervision; (ii) ImgSyn synthesizes new images based on existing masks using the mask-to-image generation model, strongly improving image diversity for model inputs. On the highly competitive ADE20K and COCO benchmarks, our data generation method markedly improves the performance of state-of-the-art segmentation models in semantic segmentation, panoptic segmentation, and instance segmentation. Notably, in terms of the ADE20K mIoU, Mask2Former R50 is largely boosted from 47.2 to 49.9 (+2.7); Mask2Former Swin-L is also significantly increased from 56.1 to 57.4 (+1.3). These promising results strongly suggest the effectiveness of our SegGen even when abundant human-annotated training data is utilized. Moreover, training with our synthetic data makes the segmentation models more robust towards unseen domains. Project website: https://seggenerator.github.io
</details>
<details>
<summary>摘要</summary>
我们提出了SegGen，一种高效的训练数据生成方法，可以大幅提高现代分割模型的性能。SegGen通过两种数据生成策略：MaskSyn和ImgSyn。（一）MaskSyn通过我们提出的文本到mask生成模型和mask到图生成模型，可以增加分割掩码的多样性，为模型提供更丰富的指导。（二）ImgSyn通过现有掩码生成新的图像，可以强化图像的多样性，为模型输入提供更多的选择。在ADE20K和COCO评测标准上，我们的数据生成方法可以明显提高现代分割模型的semantic segmentation、panoptic segmentation和instance segmentation性能。特别是在ADE20K mIoU上，Mask2Former R50的性能从47.2提高到49.9（+2.7），Mask2Former Swin-L也从56.1提高到57.4（+1.3）。这些优秀的结果表明我们的SegGen在有 suficient human-annotated训练数据的情况下也能够取得显著的效果。此外，通过我们生成的 sintetic数据，分割模型可以更好地鲁ilde对未看过的领域。项目网站：https://seggenerator.github.io
</details></li>
</ul>
<hr>
<h2 id="GLaMM-Pixel-Grounding-Large-Multimodal-Model"><a href="#GLaMM-Pixel-Grounding-Large-Multimodal-Model" class="headerlink" title="GLaMM: Pixel Grounding Large Multimodal Model"></a>GLaMM: Pixel Grounding Large Multimodal Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03356">http://arxiv.org/abs/2311.03356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, Fahad S. Khan</li>
<li>for: 这 paper 的目的是提出一种基于视觉Domain的大型多modal模型（LMMs），可以生成与对应的语言响应。</li>
<li>methods: 这 paper 使用了一种新的模型 called Grounding LMM（GLaMM），它可以根据用户提供的文本和&#x2F;或区域提示来生成对应的语言响应和物体分割emas。</li>
<li>results: GLaMM可以在多种下游任务上表现出色，包括图像和区域水平的Captioning、图像和区域水平的描述、和视觉语言对话。此外，GLaMM还可以在一些新的任务上表现出色，如 Referring Expression Segmentation 和 Grounded Conversation Generation。<details>
<summary>Abstract</summary>
Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial efforts towards LMMs used holistic images and text prompts to generate ungrounded textual responses. Very recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring a single object category at a time, require users to specify the regions in inputs, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of generating visually grounded detailed conversations, we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed Grounded Conversation Generation (GCG) task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks e.g., referring expression segmentation, image and region-level captioning and vision-language conversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.
</details>
<details>
<summary>摘要</summary>
大型多modal模型（LMM）拓展了大型语言模型到视觉领域。初期尝试的LMM使用整体图像和文本提示生成不关联的文本响应。最近，区域级LMM已经用于生成视觉关联的响应，但它们只能同时参考一个物体类别，需要用户在输入中指定区域，或者无法提供密集像素级对象关根。在这项工作中，我们提出了固化LMM（GLaMM），第一个可以生成自然语言响应同时与相应的对象分割mask相匹配。GLaMM不仅可以在对话中固化出现的对象，还可以随意接受文本和可选的视觉提示（区域兴趣点）作为输入。这使得用户可以与模型在文本和视觉领域进行交互，并且可以在不同的级别进行交互。由于生成视觉关联的详细对话的标准benchmark尚未出现，我们提出了全面的评价协议，并针对我们精心准备的grounded conversations进行评价。我们的提议的Grounded Conversation Generation（GCG）任务需要在自然场景中densely grounded的概念，并在大规模上进行评价。为此，我们提出了高度注解的Grounding-anything Dataset（GranD），使用我们提出的自动注解管道，涵盖了7.5万个唯一的概念，在810万个区域中均有分割mask。除了GCG，GLaMM还在多个下游任务上表现出色，如图像和区域级captioning、视力语会话等。项目页面：https://mbzuai-oryx.github.io/groundingLMM。
</details></li>
</ul>
<hr>
<h2 id="Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation"><a href="#Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation" class="headerlink" title="Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"></a>Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03348">http://arxiv.org/abs/2311.03348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rusheb Shah, Quentin Feuillade–Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando</li>
<li>for: This paper investigates the vulnerability of large language models to jailbreaking attacks using persona modulation, and demonstrates the ability to elicit harmful responses from the models.</li>
<li>methods: The paper uses a language model assistant to automate the generation of jailbreaks, and demonstrates the effectiveness of this approach in achieving harmful completions in GPT-4, Claude 2, and Vicuna.</li>
<li>results: The paper shows that persona modulation can achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation, and also demonstrates the transfer of these attacks to other models, such as Claude 2 and Vicuna.<details>
<summary>Abstract</summary>
Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. In this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. Rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. We demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. These automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%). These prompts also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively. Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.
</details>
<details>
<summary>摘要</summary>
尽管努力对大语言模型进行安全配置，它们仍然易受到劫持提示的影响，导致发送危险指令的可能性增加。在这项工作中，我们研究人格调整作为黑盒子劫逃方法，以使目标模型采取愿意遵从危险指令的人格。而不是手动设计每个人格的提示，我们使用语言模型助手自动生成劫逃。我们示例了一系列由人格调整引起的危险结果，包括Synthesize毒品、制造炸弹和洗钱等详细指令。这些自动攻击的危险完成率为GPT-4的42.5%，比之前的0.23%高185倍。这些提示还传递到Claude 2和Vicuna，它们的危险完成率分别为61.0%和35.9%。我们的工作揭示了商业大语言模型又一个漏洞，并高亮了更加全面的安全措施的需要。
</details></li>
</ul>
<hr>
<h2 id="Multitask-Kernel-based-Learning-with-First-Order-Logic-Constraints"><a href="#Multitask-Kernel-based-Learning-with-First-Order-Logic-Constraints" class="headerlink" title="Multitask Kernel-based Learning with First-Order Logic Constraints"></a>Multitask Kernel-based Learning with First-Order Logic Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03340">http://arxiv.org/abs/2311.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini</li>
<li>for: 这个论文旨在总结一个整合超级vised和无级ished例子以及背景知识的核心机器学习框架。</li>
<li>methods: 该论文使用多任务学习方案，其中多个预测函数定义在一个对象集中的特征空间上，并且可以是先知的或通过适当的核kernel-based学习器来 aproximate。</li>
<li>results: 该论文提出了一种将逻辑逻辑约束转换为连续实现的方法，并在多个例子中进行了实验，证明了该方法可以有效地解决semi-supervised学习问题。<details>
<summary>Abstract</summary>
In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a penalty term that enforces the constraints on both the supervised and unsupervised examples. Unfortunately, the penalty term is not convex and it can hinder the optimization process. However, it is possible to avoid poor solutions by using a two stage learning schema, in which the supervised examples are learned first and then the constraints are enforced.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个通用框架，用于将经过监督和无监督示例以及背景知识表示为一组第一频逻辑条件集成到核心机器中。具体来说，我们考虑了一种多任务学习方案，在其中多个定义在对象集中的预测符被同时学习从示例中，并且对预测符的值进行约束。这些预测符是定义在特征空间中，并且可以是知道的或者通过适当的核心学习器来 aproximated。我们提出了一种通用的方法，将逻辑条件集转换成可以处理核心预测符输出的连续实现方式。学习问题被定义为一种半监督学习任务，需要在超参中优化一个损失函数，该损失函数结合监督示例上的适应损失度量、正则化项和约束项。却可能是非 convex 的罚 penalty 项，这可能会阻碍优化过程。但是，我们可以通过一种两阶段学习策略来避免 poor solution，在其中首先学习监督示例，然后强制执行约束。
</details></li>
</ul>
<hr>
<h2 id="ProPath-Disease-Specific-Protein-Language-Model-for-Variant-Pathogenicity"><a href="#ProPath-Disease-Specific-Protein-Language-Model-for-Variant-Pathogenicity" class="headerlink" title="ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity"></a>ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03429">http://arxiv.org/abs/2311.03429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huixin Zhan, Zijun Zhang</li>
<li>for: 预测疾病相关的遗传学变革是现代遗传学中的一个核心挑战。</li>
<li>methods: 我们提出了一个疾病特有的蛋白语言模型，即ProPath，以捕捉罕见变异中的pseudo-log-likelihood比率。</li>
<li>results: 我们的结果显示，ProPath比预先训练的ESM1b提高了超过5%的AUC，并在两个数据集中均达到了最高表现。<details>
<summary>Abstract</summary>
Clinical variant classification of pathogenic versus benign genetic variants remains a pivotal challenge in clinical genetics. Recently, the proposition of protein language models has improved the generic variant effect prediction (VEP) accuracy via weakly-supervised or unsupervised training. However, these VEPs are not disease-specific, limiting their adaptation at point-of-care. To address this problem, we propose a disease-specific \textsc{pro}tein language model for variant \textsc{path}ogenicity, termed ProPath, to capture the pseudo-log-likelihood ratio in rare missense variants through a siamese network. We evaluate the performance of ProPath against pre-trained language models, using clinical variant sets in inherited cardiomyopathies and arrhythmias that were not seen during training. Our results demonstrate that ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC across both datasets. Furthermore, our model achieved the highest performances across all baselines for both datasets. Thus, our ProPath offers a potent disease-specific variant effect prediction, particularly valuable for disease associations and clinical applicability.
</details>
<details>
<summary>摘要</summary>
临床变体分类严重病原 versus benign 遗传变异仍然是临床遗传学的核心挑战。最近，蛋白语言模型的提议有助于无监督或弱监督训练下variant effet prediction（VEP）的准确性。然而，这些VEP不是疾病特定，限制其在临床应用中的适应性。为解决这个问题，我们提议一种疾病特定的蛋白语言模型，称为ProPath，以捕捉 Pseudo-log-likelihood ratio 在罕见 missense 变异中。我们通过对临床变体集和遗传性心脏病和心脏病例进行评估，发现ProPath 的性能超过了预训练的 ESM1b，在两个数据集上提高了超过 5% 的 AUC。此外，我们的模型在所有基线之上表现最高，尤其在疾病相关性和临床实用性方面。因此，我们的 ProPath 提供了一种有力的疾病特定变异效应预测，对于疾病相关性和临床应用非常有价值。
</details></li>
</ul>
<hr>
<h2 id="FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2"><a href="#FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2" class="headerlink" title="FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2"></a>FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03339">http://arxiv.org/abs/2311.03339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Sdraka, Alkinoos Dimakos, Alexandros Malounis, Zisoula Ntasiou, Konstantinos Karantzalos, Dimitrios Michail, Ioannis Papoutsis<br>for:This paper aims to provide an accurate and robust method for automatically extracting burnt areas from satellite imagery after wildfires.methods:The authors use a machine-learning ready dataset called FLOGA, which includes satellite imagery with different spatial and spectral resolutions, and ground truth annotations from domain experts. They compare the performance of multiple machine learning and deep learning algorithms for change detection, and propose a novel deep learning model called BAM-CD.results:The proposed BAM-CD model outperforms all other methods in terms of accuracy and robustness, providing an effective way to automatically extract burnt areas from satellite imagery.<details>
<summary>Abstract</summary>
Over the last decade there has been an increasing frequency and intensity of wildfires across the globe, posing significant threats to human and animal lives, ecosystems, and socio-economic stability. Therefore urgent action is required to mitigate their devastating impact and safeguard Earth's natural resources. Robust Machine Learning methods combined with the abundance of high-resolution satellite imagery can provide accurate and timely mappings of the affected area in order to assess the scale of the event, identify the impacted assets and prioritize and allocate resources effectively for the proper restoration of the damaged region. In this work, we create and introduce a machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations for the Greek Area). This dataset is unique as it comprises of satellite imagery acquired before and after a wildfire event, it contains information from Sentinel-2 and MODIS modalities with variable spatial and spectral resolution, and contains a large number of events where the corresponding burnt area ground truth has been annotated by domain experts. FLOGA covers the wider region of Greece, which is characterized by a Mediterranean landscape and climatic conditions. We use FLOGA to provide a thorough comparison of multiple Machine Learning and Deep Learning algorithms for the automatic extraction of burnt areas, approached as a change detection task. We also compare the results to those obtained using standard specialized spectral indices for burnt area mapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our benchmark results demonstrate the efficacy of the proposed technique in the automatic extraction of burnt areas, outperforming all other methods in terms of accuracy and robustness. Our dataset and code are publicly available at: https://github.com/Orion-AI-Lab/FLOGA.
</details>
<details>
<summary>摘要</summary>
过去一个 décennie 中，全球受到了越来越频繁和严重的野火威胁，对人类和动物生命、生态系统和社会经济稳定性构成了严重的威胁。因此，我们需要就野火的影响作出迫切的行动，以保护地球的自然资源。 robust machine learning 技术，结合高分辨率卫星图像的丰富存在，可以为评估事件规模、确定受影响资产和有效分配资源而提供准确和时间相关的地图。在这项工作中，我们创建了一个名为 FLOGA（希腊地区森林野火观察数据集）的机器学习准备数据集。FLOGA 数据集独特之处在于，它包含了在野火事件前后由卫星图像提供的信息，其中包括 Sentinel-2 和 MODIS Modalities 的变量空间和spectral 分辨率信息，同时包含大量已由领域专家标注的烧毁区域地面 truth。FLOGA 覆盖希腊更广泛的地区，这个地区具有地中海气候和地貌特点。我们使用 FLOGA 进行了多种机器学习和深度学习算法的自动烧毁区域抽取比较，并与基于特殊 spectral 指数的烧毁区域映射方法进行比较。最后，我们提出了一种新的深度学习模型，即 BAM-CD。我们的 refer 结果表明，提议的方法在自动烧毁区域抽取方面具有高度的准确性和稳定性。我们的数据集和代码在 GitHub 上公开提供：https://github.com/Orion-AI-Lab/FLOGA。
</details></li>
</ul>
<hr>
<h2 id="DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase"><a href="#DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase" class="headerlink" title="DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase"></a>DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03319">http://arxiv.org/abs/2311.03319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao Li, Yulin wang, Xueqi Wang, William Hogan, Jingbo Shang</li>
<li>for: 实现低资源下的内容学习（In-Context Learning，ICL），获得更好的结果。</li>
<li>methods: 使用自己生成的内容来增强大语言模型的熟悉度，然后通过多数决来决定最终结果。</li>
<li>results: 与标准ICL方法和其他ensemble-based方法相比，DAIL在低资源情况下表现更好，并且可以提供更高的信任度。<details>
<summary>Abstract</summary>
In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks. However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios. To overcome this limitation, we propose \textbf{D}ata \textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning (\textbf{DAIL}). DAIL leverages the intuition that large language models are more familiar with the content generated by themselves. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario. Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. We believe our work will stimulate further research on ICL in low-resource settings.
</details>
<details>
<summary>摘要</summary>
受欢迎的自然语言处理任务（NLP）中，在线上学习（ICL）与预训练大型自然语言模型（LMM）的结合已经实现了一定的成功。然而，ICL需要高质量的注释演示，这可能在实际场景中不 disponibles。为了解决这个限制，我们提出了\textbf{数据增强 для在线学习（DAIL）}. DAIL利用了LMM生成的内容的直觉，首先使用LMM生成测试样本的重写，然后通过多数投票确定基于个体预测的最终结果。我们的广泛的实验证明，DAIL在低资源场景下表现比标准的ICL方法和其他ensemble方法更好。此外，我们还探讨了使用投票一致性作为模型的信任度，当预测的logs不可访问时。我们认为我们的工作将鼓励更多关于ICL在低资源设置下的研究。
</details></li>
</ul>
<hr>
<h2 id="Neural-Structure-Learning-with-Stochastic-Differential-Equations"><a href="#Neural-Structure-Learning-with-Stochastic-Differential-Equations" class="headerlink" title="Neural Structure Learning with Stochastic Differential Equations"></a>Neural Structure Learning with Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03309">http://arxiv.org/abs/2311.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjie Wang, Joel Jennings, Wenbo Gong</li>
<li>for: 这项研究旨在探讨如何从时间观察数据中找到变量之间的底层关系，以及如何使用连续时间的概率过程来描述这些系统的动态。</li>
<li>methods: 这项研究使用了神经网络随机分布函数（SDE）和可变推理来推导 posterior 分布 над 可能的结构。</li>
<li>results: 研究表明，使用 SCOTCH 方法可以在不同的时间间隔下 Learning 更好的结构，并且在实际数据上比基eline方法有更高的性能。<details>
<summary>Abstract</summary>
Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, SCOTCH, which combines neural stochastic differential equations (SDE) with variational inference to infer a posterior distribution over possible structures. This continuous-time approach can naturally handle both learning from and predicting observations at arbitrary time points. Theoretically, we establish sufficient conditions for an SDE and SCOTCH to be structurally identifiable, and prove its consistency under infinite data limits. Empirically, we demonstrate that our approach leads to improved structure learning performance on both synthetic and real-world datasets compared to relevant baselines under regular and irregular sampling intervals.
</details>
<details>
<summary>摘要</summary>
描述变量之间的下面关系是科学领域中长期存在的挑战，包括生物、金融和气候科学。这些系统的动态通常使用连续时间杂事件来描述。然而，大多数现有结构学习方法假设下面过程发展在离散时间和/或观测点发生在固定时间间隔。这些不一致的假设可能会导致错误地学习结构和模型。在这种工作中，我们介绍了一种新的结构学习方法，即SCOTCH，它将神经生成器泛化准则与Variational推断结合以推理 posterior 分布中可能的结构。这种连续时间方法可以自然地处理从和预测观测点的任意时间点学习和预测。从理论角度来看，我们设置了SDE和SCOTCH的可结构可识别条件，并证明在无穷数据极限下，SCOTCH是一个一致的方法。实际上，我们在 sintetic 和实际数据上比较SCOTCH和相关基线方法的结构学习性能，并证明SCOTCH在固定和不固定时间间隔下的观测点上具有更高的结构学习性能。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reusable-Manipulation-Strategies"><a href="#Learning-Reusable-Manipulation-Strategies" class="headerlink" title="Learning Reusable Manipulation Strategies"></a>Learning Reusable Manipulation Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03293">http://arxiv.org/abs/2311.03293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Mao, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling<br>for: 这篇论文的目的是帮助机器学习人类 manipulate “trick” 的能力，包括通过单一示例学习和自动游戏来获得 manipulate 技能。methods: 这篇论文使用的方法包括将每个示例解释为机器人对物体和物体之间的接触模式的序列，从而学习细致的抽象器和聚合器。results: 这篇论文的结果表明，通过这种方法，机器人可以通过单一示例学习和自动游戏来获得 manipulate 技能，并且可以在不同的情况下 flexibly 应用这些技能。<details>
<summary>Abstract</summary>
Humans demonstrate an impressive ability to acquire and generalize manipulation "tricks." Even from a single demonstration, such as using soup ladles to reach for distant objects, we can apply this skill to new scenarios involving different object positions, sizes, and categories (e.g., forks and hammers). Additionally, we can flexibly combine various skills to devise long-term plans. In this paper, we present a framework that enables machines to acquire such manipulation skills, referred to as "mechanisms," through a single demonstration and self-play. Our key insight lies in interpreting each demonstration as a sequence of changes in robot-object and object-object contact modes, which provides a scaffold for learning detailed samplers for continuous parameters. These learned mechanisms and samplers can be seamlessly integrated into standard task and motion planners, enabling their compositional use.
</details>
<details>
<summary>摘要</summary>
人类具有吸引人的手备技巧学习能力，可以从单一示例中即使用汤匙来达到远距离对象的技巧。我们可以将这种技巧应用到新的场景中，包括不同的对象位置、大小和类别（如锹和锤）。此外，我们还可以灵活地组合不同的技巧来制定长期计划。在这篇论文中，我们提出了一种机器人学习机制的框架，通过单一示例和自动游戏来学习。我们的关键发现在于将每个示例解释为机器人对象和对象之间的接触模式序列，这提供了一个学习细节 sampler 的框架。这些学习的机制和 sampler 可以轻松地与标准任务和运动规划器集成，以便它们的组合使用。
</details></li>
</ul>
<hr>
<h2 id="GQKVA-Efficient-Pre-training-of-Transformers-by-Grouping-Queries-Keys-and-Values"><a href="#GQKVA-Efficient-Pre-training-of-Transformers-by-Grouping-Queries-Keys-and-Values" class="headerlink" title="GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values"></a>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03426">http://arxiv.org/abs/2311.03426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farnoosh Javadi, Walid Ahmed, Habib Hajimolahoseini, Foozhan Ataiefard, Mohammad Hassanpour, Saina Asani, Austin Wen, Omar Mohamed Awad, Kangling Liu, Yang Liu</li>
<li>for: 这篇论文是为了解决大型transformer模型面临的问题，包括过度参数化和耗时 computationally expensive pre-training。</li>
<li>methods: 这篇论文提出了一种通用的方法 called GQKVA，它利用query, key,和value grouping技术来优化transformer pre-training，以提高模型的速度和小型化。</li>
<li>results: 我们的实验表明，GQKVA可以在不同的variant中取得明显的交换，即在维持模型性能的情况下，降低模型的大小。我们的实验还显示，传统的多头注意方法不一定是最佳选择，因为有更轻量级和更快速的选择可以。我们在ViT上进行了试验，获得了约0.3%的增加精度，同时降低了模型的大小约4%。此外，我们最具攻击性的模型缩小实验中，模型的大小缩小约15%，但只有约1%的精度下降。<details>
<summary>Abstract</summary>
Massive transformer-based models face several challenges, including slow and computationally intensive pre-training and over-parametrization. This paper addresses these challenges by proposing a versatile method called GQKVA, which generalizes query, key, and value grouping techniques. GQKVA is designed to speed up transformer pre-training while reducing the model size. Our experiments with various GQKVA variants highlight a clear trade-off between performance and model size, allowing for customized choices based on resource and time limitations. Our findings also indicate that the conventional multi-head attention approach is not always the best choice, as there are lighter and faster alternatives available. We tested our method on ViT, which achieved an approximate 0.3% increase in accuracy while reducing the model size by about 4% in the task of image classification. Additionally, our most aggressive model reduction experiment resulted in a reduction of approximately 15% in model size, with only around a 1% drop in accuracy.
</details>
<details>
<summary>摘要</summary>
巨大的变换器模型面临多个挑战，包括耗时和计算 интенсив的预训练和过参数化。这篇论文提出了一种通用的方法called GQKVA，该方法旨在加速变换器预训练，同时减少模型的大小。我们的实验表明，GQKVA方法在不同的variant中存在明显的性能和模型大小之间的负反相关，可以根据资源和时间限制进行定制选择。我们的发现还表明，传统的多头注意方法并不总是最佳选择，因为有轻量级和快速的代替方案可用。我们在ViT上测试了我们的方法，实现了图像分类任务中约0.3%的提升精度，同时减少了模型的大小约4%。此外，我们最具攻击性的模型减少实验结果表明，可以减少约15%的模型大小，只有约1%的精度下降。
</details></li>
</ul>
<hr>
<h2 id="S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters"><a href="#S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters" class="headerlink" title="S-LoRA: Serving Thousands of Concurrent LoRA Adapters"></a>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03285">http://arxiv.org/abs/2311.03285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-lora/s-lora">https://github.com/s-lora/s-lora</a></li>
<li>paper_authors: Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica</li>
<li>For: The paper is written for the deployment of large language models using the “pretrain-then-finetune” paradigm, specifically focusing on the scalable serving of many LoRA adapters.* Methods: The paper proposes a system called S-LoRA, which stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. Unified Paging is used to manage dynamic adapter weights and KV cache tensors, and tensor parallelism and highly optimized custom CUDA kernels are employed for heterogeneous batching of LoRA computation.* Results: Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM, S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude, enabling scalable serving of many task-specific fine-tuned models and offering the potential for large-scale customized fine-tuning services.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为大语言模型的部署而写的，具体来说是关于大量LoRA适应器的批处理服务。* Methods: 这篇论文提出了一个名为S-LoRA的系统，它将所有适应器存储在主内存中，并在运行中查询使用的适应器被提取到GPU内存中。 Unified Paging技术用于管理动态适应器权重和KV缓存tensor，并使用tensor并行和高度优化的Custom CUDA核心来实现不同批处理LoRA计算。* Results: 相比之前的状态艺术库如HuggingFace PEFT和vLLM，S-LoRA可以提高 durchput 到最多4倍，并可以同时处理数量级别的适应器，这使得S-LoRA可以实现大规模的定制化 fine-tuning 服务，并且提供了大规模定制化 fine-tuning 服务的潜在性。<details>
<summary>Abstract</summary>
The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA
</details>
<details>
<summary>摘要</summary>
“pretrain-then-finetune”模式广泛应用于语言模型的部署。低级别适应（LoRA），一种精炼 parameter 的 fine-tuning 方法，经常用于适应多种任务，从而生成了一大量的 LoRA 适应器。我们发现，这种模式具有批处理（batched inference）的广阔机会。为了利用这些机会，我们提出了 S-LoRA，一个用于批处理多个 LoRA 适应器的系统。S-LoRA 将所有适应器存储在主内存中，并在 GPU 内存中fetch 当前运行的查询所使用的适应器。为了高效使用 GPU 内存并避免分配不一致，S-LoRA 提出了统一分页（Unified Paging）技术。此外，S-LoRA 还使用了一种新的tensor并行执行策略和特化的 CUDA 加速器，以实现hetERogeneous批处理（heterogeneous batching）。这些特点使得 S-LoRA 可以在单个 GPU 或多个 GPU 上服务 thousands 个 LoRA 适应器，占用较小的负荷。相比之下，与 HuggingFace PEFT 和 vLLM（带有简单的 LoRA 服务支持）相比，S-LoRA 可以提高 durchput 高达 4 倍，并增加服务的适应器数目几个数量级。因此，S-LoRA 允许批处理多个任务特定的 fine-tuning 模型，并提供了大规模自定义 fine-tuning 服务。代码可以在 <https://github.com/S-LoRA/S-LoRA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Guided-Data-Centric-Strategy-to-Detect-and-Mitigate-Biases-in-Healthcare-Datasets"><a href="#An-AI-Guided-Data-Centric-Strategy-to-Detect-and-Mitigate-Biases-in-Healthcare-Datasets" class="headerlink" title="An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets"></a>An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03425">http://arxiv.org/abs/2311.03425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faris F. Gulamali, Ashwin S. Sawant, Lora Liharska, Carol R. Horowitz, Lili Chan, Patricia H. Kovatch, Ira Hofer, Karandeep Singh, Lynne D. Richardson, Emmanuel Mensah, Alexander W Charney, David L. Reich, Jianying Hu, Girish N. Nadkarni</li>
<li>for: 这篇论文旨在探讨健康域中诊断和预测算法的使用可能会对受欢迎群体产生偏见，以及如何使用深度学习方法来探测和改善这种偏见。</li>
<li>methods: 本论文提出了一种数据中心、模型无关、任务无关的方法来评估数据集偏见，通过分析不同群体在小样本大小下学习的关系（AEquity）来识别和修正健康域数据集中的种族偏见。</li>
<li>results: 研究发现，通过应用AEquity指标，可以在健康域数据集中识别和修正种族偏见，并在抑制偏见方面取得了显著的成果。<details>
<summary>Abstract</summary>
The adoption of diagnosis and prognostic algorithms in healthcare has led to concerns about the perpetuation of bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success. Here, we generate a data-centric, model-agnostic, task-agnostic approach to evaluate dataset bias by investigating the relationship between how easily different groups are learned at small sample sizes (AEquity). We then apply a systematic analysis of AEq values across subpopulations to identify and mitigate manifestations of racial bias in two known cases in healthcare - Chest X-rays diagnosis with deep convolutional neural networks and healthcare utilization prediction with multivariate logistic regression. AEq is a novel and broadly applicable metric that can be applied to advance equity by diagnosing and remediating bias in healthcare datasets.
</details>
<details>
<summary>摘要</summary>
随着医疗健康预测和诊断算法的推广，对受护理难度群体的偏见问题产生了关注。深度学习方法来检测和缓解偏见的发展具有不同水平的成功。在这篇文章中，我们提出了一种数据中心、模型无关、任务无关的方法来评估数据集偏见（AEquity）。我们首先研究了不同群体在小样本大小下学习的关系，然后应用系统性分析AEquity值在不同人口 subgroup中的异常情况，以确定和缓解医疗数据集中的种族偏见。AEquity是一种新的和通用的指标，可以在医疗领域应用来提高公平性，诊断和缓解数据集偏见。
</details></li>
</ul>
<hr>
<h2 id="Using-Symmetries-to-Lift-Satisfiability-Checking"><a href="#Using-Symmetries-to-Lift-Satisfiability-Checking" class="headerlink" title="Using Symmetries to Lift Satisfiability Checking"></a>Using Symmetries to Lift Satisfiability Checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03424">http://arxiv.org/abs/2311.03424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Carbonnelle, Gottfried Schenner, Maurice Bruynooghe, Bart Bogaerts, Marc Denecker</li>
<li>for: 这种方法可以用来压缩结构（也称为解释）到一个更小的领域中，而不会产生信息损失。</li>
<li>methods: 该方法包括将句子自动翻译成一个等价可满足的句子，并将这个句子在扩展 vocabulary 上进行满足性检查。</li>
<li>results: 实验表明，这种方法可以在生成配置问题中获得大量的加速，并且还有应用于软件验证复杂数据结构的场景。<details>
<summary>Abstract</summary>
We analyze how symmetries can be used to compress structures (also known as interpretations) onto a smaller domain without loss of information. This analysis suggests the possibility to solve satisfiability problems in the compressed domain for better performance. Thus, we propose a 2-step novel method: (i) the sentence to be satisfied is automatically translated into an equisatisfiable sentence over a ``lifted'' vocabulary that allows domain compression; (ii) satisfiability of the lifted sentence is checked by growing the (initially unknown) compressed domain until a satisfying structure is found. The key issue is to ensure that this satisfying structure can always be expanded into an uncompressed structure that satisfies the original sentence to be satisfied. We present an adequate translation for sentences in typed first-order logic extended with aggregates. Our experimental evaluation shows large speedups for generative configuration problems. The method also has applications in the verification of software operating on complex data structures. Further refinements of the translation are left for future work.
</details>
<details>
<summary>摘要</summary>
我们分析如何使用对称性来压缩结构（也称为解释）到一个更小的领域 без损失信息。这种分析表明可以在压缩领域中解决满足性问题以获得更好的性能。因此，我们提出了一种新的两步方法：（i）要满足的句子自动被翻译成一个等价满足句子，使用一个扩展了词汇的“升级” vocabulary，以便压缩领域。（ii）检查升级后的句子是否满足，通过在初始不知道的压缩领域中增长 until 找到一个满足结构。关键在于确保这个满足结构可以扩展到一个不压缩的结构，以满足原始要满足的句子。我们对类型化first-order logic中的句子进行了适当的翻译。我们的实验评估表明，这种方法在生成配置问题上可以获得大量的速度提升。此方法还有软件验证复杂数据结构的应用。未来的工作将更进一步地完善翻译。
</details></li>
</ul>
<hr>
<h2 id="From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach"><a href="#From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach" class="headerlink" title="From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach"></a>From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03260">http://arxiv.org/abs/2311.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tan M. Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo Nakamura</li>
<li>for: 针对 Graph Neural Network (GNN) 中的过敏问题，提出了 Kuramoto Graph Neural Network (KuramotoGNN)，一种新的连续深度 GNN 类型。</li>
<li>methods: KuramotoGNN 使用 Kuramoto 模型来 Mitigate 过敏问题，Kuramoto 模型捕捉了非线性共振 oscilators 的同步行为。</li>
<li>results: 对多种图深度学习benchmark任务进行实验，表明 KuramotoGNN 可以减少过敏问题，而且与基eline GNN 和现有方法相比，具有更好的性能。<details>
<summary>Abstract</summary>
We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various graph deep learning benchmark tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了库拉莫托图 neural network（KuramotoGNN），一种新的连续深度图 neural network（GNN），它使用库拉莫托模型来 Mitigate the over-smoothing phenomenon, 在 graph neural networks 中，节点特征会在层数增加时变得无法分辨。库拉莫托模型捕捉了非线性 coupled oscillators 的同步行为。从 coupled oscillators 的视角来看，我们首先表明了库拉莫托模型和基本 GNN 之间的连接，然后我们解释了 GNN 中的过滤现象可以被看作库拉莫托模型中的相同频率同步。库拉莫托GNN 将这种相同频率同步替换为频率同步，以防止节点特征 converges 到每个节点特征，同时允许系统达到一个稳定的同步状态。我们通过实验证明了库拉莫托GNN 在不同的图深度学习 benchmark 任务上的优势，比如减少过滤现象。
</details></li>
</ul>
<hr>
<h2 id="Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency"><a href="#Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency" class="headerlink" title="Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency"></a>Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03253">http://arxiv.org/abs/2311.03253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilin Xiao, Linjun Shou, Xingyao Zhang, Jie Wu, Ming Gong, Jian Pei, Daxin Jiang<br>for:The paper aims to improve the coherence of entity disambiguation (ED) predictions by proposing a novel system called CoherentED.methods:CoherentED uses an unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences, and incorporates an external category memory to retrieve relevant categories for undecided mentions. The system also employs step-by-step entity decisions to model entity-entity interactions and maintain maximum coherence at the category level.results:The proposed CoherentED model achieves new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points, particularly excelling in long-text scenarios.<details>
<summary>Abstract</summary>
Previous entity disambiguation (ED) methods adopt a discriminative paradigm, where prediction is made based on matching scores between mention context and candidate entities using length-limited encoders. However, these methods often struggle to capture explicit discourse-level dependencies, resulting in incoherent predictions at the abstract level (e.g. topic or category). We propose CoherentED, an ED system equipped with novel designs aimed at enhancing the coherence of entity predictions. Our method first introduces an unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences. This approach not only allows the encoder to handle longer documents more effectively, conserves valuable input space, but also keeps a topic-level coherence. Additionally, we incorporate an external category memory, enabling the system to retrieve relevant categories for undecided mentions. By employing step-by-step entity decisions, this design facilitates the modeling of entity-entity interactions, thereby maintaining maximum coherence at the category level. We achieve new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points. Our model demonstrates particularly outstanding performance on challenging long-text scenarios.
</details>
<details>
<summary>摘要</summary>
To address this issue, we propose CoherentED, an ED system with novel designs that enhance the coherence of entity predictions. Our approach includes:1. Unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences. This allows the encoder to handle longer documents more effectively, conserve valuable input space, and maintain topic-level coherence.2. External category memory to retrieve relevant categories for undecided mentions. This design facilitates the modeling of entity-entity interactions and maintains maximum coherence at the category level.3. Step-by-step entity decisions to model entity-entity interactions and maintain coherence at the category level.Our model achieves new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points. It particularly excels in challenging long-text scenarios.
</details></li>
</ul>
<hr>
<h2 id="Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers"><a href="#Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers" class="headerlink" title="Instructed Language Models with Retrievers Are Powerful Entity Linkers"></a>Instructed Language Models with Retrievers Are Powerful Entity Linkers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03250">http://arxiv.org/abs/2311.03250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrzilinxiao/insgenentitylinking">https://github.com/mrzilinxiao/insgenentitylinking</a></li>
<li>paper_authors: Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Jian Pei, Daxin Jiang</li>
<li>for: 提高语言模型在实体链接任务中的性能，使其能够准确地预测知识库中的实体。</li>
<li>methods: 提出了一种新的生成式实体链接方法，包括将语言模型通过序列对应的训练目标和指导进行强制实体链接训练，以及一种轻量级的潜在提及检索器来减轻模型的解oding负担。</li>
<li>results: 与前一代生成方法进行比较，INSGENEL表现出了+6.8 F1分的提升，同时具有更高的训练数据效率和训练计算资源利用率。<details>
<summary>Abstract</summary>
Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4$\times$ speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型驱动的生成方法已经展示出了复杂逻辑能力的emergent能力。然而，生成性还使生成内容受到幻觉的影响，因此不适用于实体关注任务（EL），需要精确的实体预测 над大量知识库。我们提出了首个使用语言模型执行EL任务的Instructed Generative Entity Linker（INSGENEL）方法。我们提出了多种方法来让语言模型拥有EL能力，包括（i）在EL目标下进行序列到序列训练，并使用指令调整；（ii）一种基于轻量级可能提取器的新一代生成EL框架，解决了重量级和非平行化解码的问题，实现了4倍的速度提升而无需牺牲链接指标。INSGENEL在前一代生成方法上提高了平均6.8个F1分，同时具有较好的训练数据效率和训练计算耗用率。此外，我们的巧妙地设计的上下文学习（ICL）框架 для EL仍然落后INSGENEL，这再次证明了EL任务对普通LLMs是一个持续的挑战。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting"><a href="#Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting" class="headerlink" title="Advancing Post Hoc Case Based Explanation with Feature Highlighting"></a>Advancing Post Hoc Case Based Explanation with Feature Highlighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03246">http://arxiv.org/abs/2311.03246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eoin Kenny, Eoin Delaney, Mark Keane</li>
<li>for: 该论文的目的是提出一种新的可解释AI（XAI）技术，用于辅助人类和AI系统之间的合作。</li>
<li>methods: 该论文使用了两种总体算法（幽默和超像素基于算法）来隔离测试图像中的多个清晰特征部分，然后将其连接到训练数据中的相关案例，以提供更全面的解释。</li>
<li>results: 该论文的结果表明，该方法可以正确地调整用户对批处理的感受，并且在实际数据集上的ImageNet dataset上实现了这一效果，而不是只是显示解释而无法连接到特征部分。<details>
<summary>Abstract</summary>
Explainable AI (XAI) has been proposed as a valuable tool to assist in downstream tasks involving human and AI collaboration. Perhaps the most psychologically valid XAI techniques are case based approaches which display 'whole' exemplars to explain the predictions of black box AI systems. However, for such post hoc XAI methods dealing with images, there has been no attempt to improve their scope by using multiple clear feature 'parts' of the images to explain the predictions while linking back to relevant cases in the training data, thus allowing for more comprehensive explanations that are faithful to the underlying model. Here, we address this gap by proposing two general algorithms (latent and super pixel based) which can isolate multiple clear feature parts in a test image, and then connect them to the explanatory cases found in the training data, before testing their effectiveness in a carefully designed user study. Results demonstrate that the proposed approach appropriately calibrates a users feelings of 'correctness' for ambiguous classifications in real world data on the ImageNet dataset, an effect which does not happen when just showing the explanation without feature highlighting.
</details>
<details>
<summary>摘要</summary>
explainer AI (XAI) 被提议为在人机合作下执行下游任务的有价值工具。 可能最有心理有效的 XAI 技术是情况基 Approach，通过显示“整体”的示例来解释黑盒 AI 系统的预测。然而，对于图像处理的Post hoc XAI 方法，没有尝试使用多个明确的特征部分来解释预测结果，从而与相关的训练数据中的案例相关联，以提供更全面的解释， faithful 于下面模型。在这里，我们解决这个差距，并提出了两种通用算法（潜在和超Pixel 基于），可以在测试图像中孤立多个明确的特征部分，然后与训练数据中的解释案例相关联，并在用户研究中进行测试。结果表明，我们的方法可以正确地考虑用户对异常分类结果的情感，在实际世界数据集上的 ImageNet dataset 上，并不会发生只显示解释而无需特征高亮的情况。Note that Simplified Chinese is used in this translation, as it is the most widely used form of Chinese in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding"><a href="#An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding" class="headerlink" title="An Efficient Self-Supervised Cross-View Training For Sentence Embedding"></a>An Efficient Self-Supervised Cross-View Training For Sentence Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03228">http://arxiv.org/abs/2311.03228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrpeerat/sct">https://github.com/mrpeerat/sct</a></li>
<li>paper_authors: Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Lalita Lowphansirikul, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong</li>
<li>for: 提高小型语言模型（PLM）的自监sentence表示学习性能。</li>
<li>methods: 提出了一种名为Self-supervised Cross-View Training（SCT）的框架，用于缩小大小PLM之间性能差异。</li>
<li>results: SCT在7个Semantic Textual Similarity（STS）benchmark上比基eline和state-of-the-art竞争对手perform better，特别是对PLM的参数量小于100M的情况下。<details>
<summary>Abstract</summary>
Self-supervised sentence representation learning is the task of constructing an embedding space for sentences without relying on human annotation efforts. One straightforward approach is to finetune a pretrained language model (PLM) with a representation learning method such as contrastive learning. While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases. In this paper, we propose a framework called Self-supervised Cross-View Training (SCT) to narrow the performance gap between large and small PLMs. To evaluate the effectiveness of SCT, we compare it to 5 baseline and state-of-the-art competitors on seven Semantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of parameters ranging from 4M to 340M. The experimental results show that STC outperforms the competitors for PLMs with less than 100M parameters in 18 of 21 cases.
</details>
<details>
<summary>摘要</summary>
自动监督句子表示学习是建立句子嵌入空间的任务，不需要人工标注努力。一种直观方法是通过对预训练语言模型（PLM）进行微调和表示学习方法，如对比学习。然而，这种方法在PLM的参数数量减少时表现迅速下降。在这篇论文中，我们提出了一个名为自动监督跨视图训练（SCT）的框架，以减少大小PLM的表现差距。为评估SCT的效果，我们与5个基线和当前顶峰竞争对手进行比较，在7个Semantic Textual Similarity（STS）标准套件上使用5个PLM的参数量从4M到340M。实验结果显示，STC在PLM参数量少于100M的18个情况中表现更好于竞争对手。
</details></li>
</ul>
<hr>
<h2 id="LDM3D-VR-Latent-Diffusion-Model-for-3D-VR"><a href="#LDM3D-VR-Latent-Diffusion-Model-for-3D-VR" class="headerlink" title="LDM3D-VR: Latent Diffusion Model for 3D VR"></a>LDM3D-VR: Latent Diffusion Model for 3D VR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03226">http://arxiv.org/abs/2311.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Ben Melech Stan, Diana Wofk, Estelle Aflalo, Shao-Yen Tseng, Zhipeng Cai, Michael Paulitsch, Vasudev Lal</li>
<li>for: 这篇论文是为了提出一种基于潜在扩散模型的虚拟现实开发框架，包括LDM3D-pano和LDM3D-SR两种模型。这两种模型可以根据文本提示生成全景RGBD图像，并将低分辨率输入图像upscale到高分辨率RGBD图像。</li>
<li>methods: 这两种模型都是基于现有预训练模型的 fine-tuning，使用包括全景RGB图像、深度图像和标签在内的数据集进行训练。</li>
<li>results: 对于LDM3D-pano模型，研究人员通过对比与现有相关方法进行评估，发现它可以生成高质量的全景RGBD图像，而且比现有方法更具有创新性和灵活性。对于LDM3D-SR模型，研究人员发现它可以高效地upscale低分辨率RGBD图像到高分辨率，并且比现有方法更具有稳定性和可靠性。<details>
<summary>Abstract</summary>
Latent diffusion models have proven to be state-of-the-art in the creation and manipulation of visual outputs. However, as far as we know, the generation of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of diffusion models targeting virtual reality development that includes LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD based on textual prompts and the upscaling of low-resolution inputs to high-resolution RGBD, respectively. Our models are fine-tuned from existing pretrained models on datasets containing panoramic/high-resolution RGB images, depth maps and captions. Both models are evaluated in comparison to existing related methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>潜在扩散模型已经证明是视觉输出创造和操作的状态之一。然而，据我们知道，RGBD的深度图生成并非很广泛。我们介绍LDM3D-VR，一个针对虚拟现实开发的扩散模型集合，包括LDM3D-pano和LDM3D-SR。这两个模型允许通过文本提示生成宽角RGBD和低分辨率输入到高分辨率RGBD的upscaling。我们的模型来自现有预训练模型，并在包含宽角/高分辨率RGB图像、深度图和标签的数据集上进行了微调。两个模型与现有相关方法进行了评估。
</details></li>
</ul>
<hr>
<h2 id="ALYMPICS-Language-Agents-Meet-Game-Theory"><a href="#ALYMPICS-Language-Agents-Meet-Game-Theory" class="headerlink" title="ALYMPICS: Language Agents Meet Game Theory"></a>ALYMPICS: Language Agents Meet Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03220">http://arxiv.org/abs/2311.03220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, Furu Wei</li>
<li>for: 这篇论文是为了探讨语言模型代理（LLM）在游戏理论中的应用。</li>
<li>methods: 该论文使用LLM和自动化代理来模拟人类行为，并实现多代理合作，以构建人类交互的真实和动态模型。</li>
<li>results: 通过 manipulate 资源可用性和代理个性，我们观察了不同代理在竞争中如何参与和适应策略。LLM代理在游戏理论研究中提供了优势，包括模拟真实行为、提供可控、可扩展和可重现环境。<details>
<summary>Abstract</summary>
This paper introduces Alympics, a platform that leverages Large Language Model (LLM) agents to facilitate investigations in game theory. By employing LLMs and autonomous agents to simulate human behavior and enable multi-agent collaborations, we can construct realistic and dynamic models of human interactions for game theory hypothesis formulating and testing. To demonstrate this, we present and implement a survival game involving unequal competition for limited resources. Through manipulation of resource availability and agent personalities, we observe how different agents engage in the competition and adapt their strategies. The use of LLM agents in game theory research offers significant advantages, including simulating realistic behavior, providing a controlled, scalable, and reproducible environment. Our work highlights the potential of LLM agents in enhancing the understanding of strategic decision-making within complex socioeconomic contexts. All codes will be made public soon.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了Alympics平台，该平台利用大语言模型（LLM）代理来促进游戏理论研究。通过使用LLM和自动化代理来模拟人类行为并实现多代理合作，我们可以构建真实和动态的人类互动模型，用于游戏理论假设设计和测试。为了证明这一点，我们在一个有限资源的存储游戏中展示了不同代理的竞争和战略适应。通过资源可用性和代理个性的调整，我们观察到不同的代理如何参与竞争并适应策略。使用LLM代理在游戏理论研究中提供了显著优势，包括模拟真实行为、提供可控、可扩展和可重现的环境。我们的工作强调了LLM代理在复杂社会经济背景下的决策战略理解的潜在优势。所有代码即将公开。
</details></li>
</ul>
<hr>
<h2 id="Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models"><a href="#Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models" class="headerlink" title="Mini Minds: Exploring Bebeshka and Zlata Baby Models"></a>Mini Minds: Exploring Bebeshka and Zlata Baby Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03216">http://arxiv.org/abs/2311.03216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/upunaprosk/small-language-models">https://github.com/upunaprosk/small-language-models</a></li>
<li>paper_authors: Irina Proskurina, Guillaume Metzler, Julien Velcin</li>
<li>for: 这个论文描述了卢梭大学2所提交给Strict-Smalltrack的BabyLM竞赛任务。该任务强调从头开始，使用有限数据量和人类语言学习来进行语言模型化。共享任务数据集有1000万个词语，与儿童词汇相当。</li>
<li>methods: 我们采用了建立搜索，使用masked语言模型损失来调整数据集上的配置。我们发现了一个优化的配置，并引入了两个小型语言模型（LMs），即Bebeshka和Zlata。这两个模型具有4层encoder和6层decoder，具有8个注意头和12个注意头。虽然这两个模型的规模远小于基elineLMs，但它们在性能上具有相似的表现。</li>
<li>results: 我们发现，这两个小型语言模型在包括道德判断任务中的语言理解任务中表现出色。我们还发现，这些任务的预测结果与人类价值观念相align。这些发现表明，小型语言模型在实际语言理解任务中具有潜在的应用前景。<details>
<summary>Abstract</summary>
In this paper, we describe the University of Lyon 2 submission to the Strict-Small track of the BabyLM competition. The shared task is created with an emphasis on small-scale language modelling from scratch on limited-size data and human language acquisition. Dataset released for the Strict-Small track has 10M words, which is comparable to children's vocabulary size. We approach the task with an architecture search, minimizing masked language modelling loss on the data of the shared task. Having found an optimal configuration, we introduce two small-size language models (LMs) that were submitted for evaluation, a 4-layer encoder with 8 attention heads and a 6-layer decoder model with 12 heads which we term Bebeshka and Zlata, respectively. Despite being half the scale of the baseline LMs, our proposed models achieve comparable performance. We further explore the applicability of small-scale language models in tasks involving moral judgment, aligning their predictions with human values. These findings highlight the potential of compact LMs in addressing practical language understanding tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了里昂第二大学对Strict-Small赛道的提交。这个比赛强调从头开始，使用有限数据量和人类语言学习的小规模语言模型。比赛数据集的词汇量为1000万个词，与儿童 vocabulary 大小相当。我们通过架构搜索，在数据集上减少了遮盖语言模型损失。我们发现了一个优化的配置，并引入了两个小型语言模型（LM），即Bebeshka和Zlata。这两个模型各有4层encoder和6层decoder，每个encoder具有8个注意头，每个decoder具有12个注意头。尽管这两个模型的规模只是基elineLMs的一半，但它们在比赛中表现很强。我们进一步探索了小规模语言模型在道德判断任务中的可行性，并将其预测与人类价值观Alignment。这些发现表明了小规模语言模型在实际语言理解任务中的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition"><a href="#Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition" class="headerlink" title="Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition"></a>Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03196">http://arxiv.org/abs/2311.03196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr">https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr</a></li>
<li>paper_authors: Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam</li>
<li>For: The paper aims to develop a large-scale domain-agnostic automatic speech recognition (ASR) dataset for low-resource languages, specifically Bangla.* Methods: The proposed methodology uses pseudo-labeling to develop a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios.* Results: The developed ASR system is benchmarked with publicly available datasets and compared with other available models, demonstrating its efficacy on a human-annotated domain-agnostic test set composed of news, telephony, and conversational data.<details>
<summary>Abstract</summary>
One of the major challenges for developing automatic speech recognition (ASR) for low-resource languages is the limited access to labeled data with domain-specific variations. In this study, we propose a pseudo-labeling approach to develop a large-scale domain-agnostic ASR dataset. With the proposed methodology, we developed a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios. We then exploited the developed corpus to design a conformer-based ASR system. We benchmarked the trained ASR with publicly available datasets and compared it with other available models. To investigate the efficacy, we designed and developed a human-annotated domain-agnostic test set composed of news, telephony, and conversational data among others. Our results demonstrate the efficacy of the model trained on psuedo-label data for the designed test-set along with publicly-available Bangla datasets. The experimental resources will be publicly available.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)
</details>
<details>
<summary>摘要</summary>
一个主要挑战是开发自动语音识别（ASR）系统的低资源语言是有限的标注数据的领域特定变化。在本研究中，我们提出了一种 Pseudo-labeling 方法，以开发大规模的领域不偏的 ASR 数据集。我们通过该方法，开发了20000+小时的标注的孟加拉语音数据集，覆盖了多样化的话题、说话风格、方言、噪音环境和对话场景。然后，我们利用开发的 corpus 设计了一个基于 Confomer 的 ASR 系统。我们对培金的 ASR 进行了评估，并与其他可用的模型进行了比较。为了调查效果，我们设计了和开发了一个人类标注的领域不偏测试集，包括新闻、电信和对话数据等。我们的结果表明，使用 Pseudo-labeling 方法对于我们设计的测试集以及公共可用的孟加拉语言数据集都有效。实验资源将公开。(参考：https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)
</details></li>
</ul>
<hr>
<h2 id="Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection"><a href="#Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection" class="headerlink" title="Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection"></a>Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03184">http://arxiv.org/abs/2311.03184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunze Xiao, Firoj Alam</li>
<li>for: 本研究旨在探讨自媒体内容中假信息和宣传内容的扩散，以及这些内容如何影响社会稳定和公众的信任。</li>
<li>methods: 本研究使用了基于变换器的 fine-tuning 和零或几次shot学习，以及 GPT-4 的使用。</li>
<li>results: 本研究在 ArAIEval 共享任务中的成绩为9名和10名。<details>
<summary>Abstract</summary>
The spread of disinformation and propagandistic content poses a threat to societal harmony, undermining informed decision-making and trust in reliable sources. Online platforms often serve as breeding grounds for such content, and malicious actors exploit the vulnerabilities of audiences to shape public opinion. Although there have been research efforts aimed at the automatic identification of disinformation and propaganda in social media content, there remain challenges in terms of performance. The ArAIEval shared task aims to further research on these particular issues within the context of the Arabic language. In this paper, we discuss our participation in these shared tasks. We competed in subtasks 1A and 2A, where our submitted system secured positions 9th and 10th, respectively. Our experiments consist of fine-tuning transformer models and using zero- and few-shot learning with GPT-4.
</details>
<details>
<summary>摘要</summary>
社会和谐受到假信息和宣传内容的威胁，这会损害了知情决策和可靠来源的信任。在线平台经常成为这种内容的殖民地，恶意者会利用听众的漏洞来形成公众意见。虽然已有研究尝试自动识别社交媒体中的假信息和宣传，但还有许多挑战。阿拉伯语 ArAIEval 分享任务旨在进一步研究这些问题。在这篇论文中，我们讲述了我们在这些分享任务中的参与。我们参加了1A和2A两个子任务，我们提交的系统在这两个子任务中分别获得了第9名和第10名。我们的实验包括细化变换器模型和使用零或几次shot学习与GPT-4。
</details></li>
</ul>
<hr>
<h2 id="ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text"><a href="#ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text" class="headerlink" title="ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text"></a>ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03179">http://arxiv.org/abs/2311.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Hasanain, Firoj Alam, Hamdy Mubarak, Samir Abdaljalil, Wajdi Zaghouani, Preslav Nakov, Giovanni Da San Martino, Abed Alhakim Freihat</li>
<li>For: The paper is written for the first ArabicNLP 2023 conference and is focused on the ArAIEval shared task, which is a task of persuasion technique detection and disinformation detection in Arabic text.* Methods: The paper uses fine-tuning transformer models such as AraBERT as the core of the majority of the participating systems.* Results: The paper provides a description of the task setup, including the dataset construction and evaluation setup, and gives a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community.Here’s the information in Simplified Chinese text:* For: 这篇论文是为了第一届阿拉伯语自然语言处理会议（ArabicNLP 2023）的一部分，主要关注于 ArAIEval 共享任务，这个任务的目标是在 arabic 文本中检测吸引技巧和谎言检测。* Methods: 这篇论文主要使用 fine-tuning 转换器模型，如 AraBERT，作为大多数参与系统的核心。* Results: 论文提供了任务设置的描述，包括数据集构建和评估设置，并提供了参与系统的简短概述。所有数据集和评估脚本都被发布到研究社区。<details>
<summary>Abstract</summary>
We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (i) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (ii) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Tasks 1 and 2, respectively. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further give a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. (https://araieval.gitlab.io/) We hope this will enable further research on these important tasks in Arabic.
</details>
<details>
<summary>摘要</summary>
我们提供了阿拉伯语评价分享任务的概述，作为2023年阿拉伯语自然语言处理会议（ArabicNLP 2023）的一部分，并与EMNLP 2023会议共同举行。这个任务提供了两个任务，即在新闻文章和推文中发现吸引人技巧的检测，以及在推文中发现不实信息的检测。共有20个队伍参加了最终评估阶段，其中14个队伍参加了任务1，16个队伍参加了任务2。在两个任务中，大多数参与系统都是使用transformer模型进行精度调整，如AraBERT。我们提供了任务设置的描述，包括数据集的建构和评估设置的描述，以及参与系统的简要概述。此外，我们还发布了所有数据集和评估脚本，以便研究人员可以进一步进行研究。（https://araieval.gitlab.io/）。我们希望这将促进阿拉伯语的研究。
</details></li>
</ul>
<hr>
<h2 id="1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait"><a href="#1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait" class="headerlink" title="1D-Convolutional transformer for Parkinson disease diagnosis from gait"></a>1D-Convolutional transformer for Parkinson disease diagnosis from gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03177">http://arxiv.org/abs/2311.03177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait">https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait</a></li>
<li>paper_authors: Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau</li>
<li>for: 这个研究的目的是用深度神经网络模型诊断 Parkinson 病患的步态。</li>
<li>methods: 这个研究使用了一种混合 ConvNet-Transformer 架构，通过捕捉本地特征和长期空间时间关系来准确地诊断病种的严重程度。</li>
<li>results: 研究结果表明，这种混合架构可以从步态数据中准确地诊断 Parkinson 病患的不同阶段，具体来说是88%的准确率，超过了其他现有的人工智能方法。此外，这种方法可以普遍应用于其他分类问题，以联合地解决1D信号中的特征相关性和空间时间关系问题。<details>
<summary>Abstract</summary>
This paper presents an efficient deep neural network model for diagnosing Parkinson's disease from gait. More specifically, we introduce a hybrid ConvNet-Transformer architecture to accurately diagnose the disease by detecting the severity stage. The proposed architecture exploits the strengths of both Convolutional Neural Networks and Transformers in a single end-to-end model, where the former is able to extract relevant local features from Vertical Ground Reaction Force (VGRF) signal, while the latter allows to capture long-term spatio-temporal dependencies in data. In this manner, our hybrid architecture achieves an improved performance compared to using either models individually. Our experimental results show that our approach is effective for detecting the different stages of Parkinson's disease from gait data, with a final accuracy of 88%, outperforming other state-of-the-art AI methods on the Physionet gait dataset. Moreover, our method can be generalized and adapted for other classification problems to jointly address the feature relevance and spatio-temporal dependency problems in 1D signals. Our source code and pre-trained models are publicly available at https://github.com/SafwenNaimi/1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种高效的深度神经网络模型，用于从步态诊断parkinson病。更具体地说，我们引入了一种混合ConvNet-Transformer架构，以准确地诊断病种的严重度。我们的提议的架构利用了Convolutional Neural Networks和Transformers两者的优势，在单一端到端模型中结合使用，以EXTract local特征和long-termspatio-temporal关系。这种方法实现了使用单个模型来诊断parkinson病的更高性能，我们的实验结果表明，我们的方法可以准确地从步态数据中识别不同的parkinson病stage，最终准确率为88%，超过了其他state-of-the-art AI方法在Physionet步态数据集上。此外，我们的方法可以普化和适应其他分类问题，以 JOINTLY地解决1D信号中的特征相关性和spatio-temporal关系问题。我们的源代码和预训练模型可以在https://github.com/SafwenNaimi/1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait上获取。
</details></li>
</ul>
<hr>
<h2 id="Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs"><a href="#Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs" class="headerlink" title="Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs"></a>Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03127">http://arxiv.org/abs/2311.03127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham, Bonnie Webber, Philipp Koehn, Andy Way, Yulin Yuan, Shuming Shi</li>
<li>for: 本研究旨在探讨机器翻译Literary Translation task的问题，以便促进这一领域的进步。</li>
<li>methods: 作者们使用了一个新的文本 corpora和一套产业界承认的评价标准，以评估参与者提交的系统性能。</li>
<li>results: 研究发现了一系列有趣的发现，包括文学和语言领域的MT问题，以及一些可能有助于解决这些问题的策略。<details>
<summary>Abstract</summary>
Translating literary works has perennially stood as an elusive dream in machine translation (MT), a journey steeped in intricate challenges. To foster progress in this domain, we hold a new shared task at WMT 2023, the first edition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab and China Literature Ltd.) release a copyrighted and document-level Chinese-English web novel corpus. Furthermore, we put forth an industry-endorsed criteria to guide human evaluation process. This year, we totally received 14 submissions from 7 academia and industry teams. We employ both automatic and human evaluations to measure the performance of the submitted systems. The official ranking of the systems is based on the overall human judgments. In addition, our extensive analysis reveals a series of interesting findings on literary and discourse-aware MT. We release data, system outputs, and leaderboard at http://www2.statmt.org/wmt23/literary-translation-task.html.
</details>
<details>
<summary>摘要</summary>
machine translation (MT) Literary translation 总是被看作是一个逃逸的梦，一个涉及繁复挑战的旅程。为了推动这个领域的进步，我们在 WMT 2023 上开设了一个新的共同任务，即第一届 Discourse-Level Literary Translation。首先，我们（Tencent AI Lab 和 China Literature Ltd.）发布了一个版权保护的、中英文网络小说 corpus。其次，我们提出了产业界认可的评价标准，以导引人类评价过程。这年，我们总共收到了 14 个来自 7 所学术和产业团队的提交。我们使用自动和人类评价两者来评价提交系统的性能。人类评价结果作为官方排名的基础。此外，我们进行了广泛的分析，发现了一系列有趣的文学和报道意识MT的发现。我们在 http://www2.statmt.org/wmt23/literary-translation-task.html 上发布了数据、系统输出和排名。
</details></li>
</ul>
<hr>
<h2 id="Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning"><a href="#Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning" class="headerlink" title="Pelvic floor MRI segmentation based on semi-supervised deep learning"></a>Pelvic floor MRI segmentation based on semi-supervised deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03105">http://arxiv.org/abs/2311.03105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Zuo, Fei Feng, Zhuhui Wang, James A. Ashton-Miller, John O. L. Delancey, Jiajia Luo<br>for:这篇论文的目的是提出一个半supervised的框架来进行骨盘器官分类。methods:这个框架包括两个阶段：第一个阶段是使用自我监督预训 tasks进行自我预训，然后使用标注数据进行精确训练分类模型。第二个阶段是使用自我预训的分类模型来生成伪标签 для无标的数据。最后，这两个阶段的数据都被用在半supervised的训练中。results:在评估中，我们的方法可以将骨盘器官分类的性能提高，特别是难以分类的器官，如uterus，可以提高Semantic segmentation的精度 by up to 3.70%.<details>
<summary>Abstract</summary>
The semantic segmentation of pelvic organs via MRI has important clinical significance. Recently, deep learning-enabled semantic segmentation has facilitated the three-dimensional geometric reconstruction of pelvic floor organs, providing clinicians with accurate and intuitive diagnostic results. However, the task of labeling pelvic floor MRI segmentation, typically performed by clinicians, is labor-intensive and costly, leading to a scarcity of labels. Insufficient segmentation labels limit the precise segmentation and reconstruction of pelvic floor organs. To address these issues, we propose a semi-supervised framework for pelvic organ segmentation. The implementation of this framework comprises two stages. In the first stage, it performs self-supervised pre-training using image restoration tasks. Subsequently, fine-tuning of the self-supervised model is performed, using labeled data to train the segmentation model. In the second stage, the self-supervised segmentation model is used to generate pseudo labels for unlabeled data. Ultimately, both labeled and unlabeled data are utilized in semi-supervised training. Upon evaluation, our method significantly enhances the performance in the semantic segmentation and geometric reconstruction of pelvic organs, Dice coefficient can increase by 2.65% averagely. Especially for organs that are difficult to segment, such as the uterus, the accuracy of semantic segmentation can be improved by up to 3.70%.
</details>
<details>
<summary>摘要</summary>
pelvic organs的semantic segmentation via MRI具有重要的临床意义。现在，通过深度学习启用的semantic segmentation，可以实现三维的预测和重建 pelvic floor organs，为临床人员提供准确和直观的诊断结果。然而，pelvic floor MRI segmentation的标注工作，通常由临床人员进行，是劳动密集和成本高的，导致标注数据的缺乏。不充分的标注数据限制了精准的pelvic floor organs的分割和重建。为解决这些问题，我们提议了一种 semi-supervised 框架 для pelvic organ segmentation。该框架的实现包括两个阶段。在第一阶段，它通过自我监督的预训练来完成图像恢复任务。然后，使用标注数据来训练分割模型的精度调整。在第二阶段，自我监督分割模型被用来生成 pseudo 标签 для无标注数据。最终，两者都被用于 semi-supervised 训练。经评估，我们的方法可以显著提高pelvic organs的semantic segmentation和三维重建的性能。平均可以提高 Dice 系数2.65%，特别是难以分割的器官，如uterus，可以提高semantic segmentation的准确率达3.70%。
</details></li>
</ul>
<hr>
<h2 id="A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection"><a href="#A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection" class="headerlink" title="A Simple yet Efficient Ensemble Approach for AI-generated Text Detection"></a>A Simple yet Efficient Ensemble Approach for AI-generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03084">http://arxiv.org/abs/2311.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, Sanmitra Bhattacharya</li>
<li>for: 本研究旨在开发一种自动可 distinguish between artificially generated text和人类写作的方法，以防止大语言模型(LLMs)的潜在滥用，如假新闻生成、垃圾邮件创建和学术作业违规使用。</li>
<li>methods: 我们提出了一种简单 yet efficient的解决方案，通过将多个组件LLMs的预测 ensemble。与前一代方法相比，我们的压缩结合方法仅使用了两个组件LLMs，而且可以达到相同的性能水平。</li>
<li>results: 我们在四个生成文本分类 benchmark 上进行了实验，结果表明，与前一代方法相比，我们的方法在性能提升的范围内为0.5%-100%。此外，我们还研究了各个LLMs的训练数据对模型性能的影响，发现可以将商业束缚的GPT数据取代为其他开源语言模型生成的数据，这是一种可行的替代方案。最后，我们通过英文作文数据集的实验，证明了我们的结合方法可以有效地处理新数据。<details>
<summary>Abstract</summary>
Recent Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across wide range of styles and genres. However, such capabilities are prone to potential abuse, such as fake news generation, spam email creation, and misuse in academic assignments. Hence, it is essential to build automated approaches capable of distinguishing between artificially generated text and human-authored text. In this paper, we propose a simple yet efficient solution to this problem by ensembling predictions from multiple constituent LLMs. Compared to previous state-of-the-art approaches, which are perplexity-based or uses ensembles with a number of LLMs, our condensed ensembling approach uses only two constituent LLMs to achieve comparable performance. Experiments conducted on four benchmark datasets for generative text classification show performance improvements in the range of 0.5 to 100\% compared to previous state-of-the-art approaches. We also study the influence that the training data from individual LLMs have on model performance. We found that substituting commercially-restrictive Generative Pre-trained Transformer (GPT) data with data generated from other open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing generative text detectors. Furthermore, to demonstrate zero-shot generalization, we experimented with an English essays dataset, and results suggest that our ensembling approach can handle new data effectively.
</details>
<details>
<summary>摘要</summary>
现代大语言模型（LLM）在生成文本方面表现出了惊人的能力，能够生成具有人类特点的文本，覆盖了各种风格和类型。然而，这些能力也存在潜在的危险，如生成假新闻、垃圾邮件和学术作业上的违规使用。因此，建立自动 distinguishing 人造文本和人类写作文本的方法变得非常重要。在这篇论文中，我们提出了一种简单 yet efficient 的解决方案，通过多个组件 LLM 的 ensemble 来实现。与之前的状态态前方法相比，我们的压缩 ensemble 方法只需使用两个组件 LLM，却可以达到相同的性能。在四个生成文本分类 benchmark 数据集上进行了实验，并取得了0.5% 到100% 的性能提升。我们还研究了各个 LLM 的训练数据对模型性能的影响。我们发现可以将商业 restriction 的 Generative Pre-trained Transformer（GPT）数据替换为其他开源语言模型如 Falcon、Large Language Model Meta AI（LLaMA2）和 Mosaic Pretrained Transformers（MPT）生成的数据，这是一种可行的方法。此外，我们进行了零shot泛化的实验，并发现我们的 ensemble 方法可以有效地处理新的数据。
</details></li>
</ul>
<hr>
<h2 id="SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet"><a href="#SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet" class="headerlink" title="SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet"></a>SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03076">http://arxiv.org/abs/2311.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurice Günder, Facundo Ramón Ispizua Yamati, Abel Andree Barreto Alcántara, Anne-Katrin Mahlein, Rafet Sifa, Christian Bauckhage</li>
<li>for: 这个研究旨在开发一个基于人工智能的植物特征标注框架，用于自动化大规模的甘蔗叶斑病苗分类。</li>
<li>methods: 这个研究使用了深度标签分布学习（DLDL）、特殊的损失函数和自订的模型架构，开发了一个基于视觉 трансформер的病苗度评分模型called SugarViT。</li>
<li>results: 这个研究获得了一个可靠的病苗度评分模型，并且还能够融合 remote sensing 数据和实验站的环境参数来预测病苗度。这个模型可以应用于多种像素基于的分类和回归 зада务。<details>
<summary>Abstract</summary>
Remote sensing and artificial intelligence are pivotal technologies of precision agriculture nowadays. The efficient retrieval of large-scale field imagery combined with machine learning techniques shows success in various tasks like phenotyping, weeding, cropping, and disease control. This work will introduce a machine learning framework for automatized large-scale plant-specific trait annotation for the use case disease severity scoring for Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label Distribution Learning (DLDL), special loss functions, and a tailored model architecture, we develop an efficient Vision Transformer based model for disease severity scoring called SugarViT. One novelty in this work is the combination of remote sensing data with environmental parameters of the experimental sites for disease severity prediction. Although the model is evaluated on this special use case, it is held as generic as possible to also be applicable to various image-based classification and regression tasks. With our framework, it is even possible to learn models on multi-objective problems as we show by a pretraining on environmental metadata.
</details>
<details>
<summary>摘要</summary>
<<SYS>>现代农业精度农业技术中，远程感知和人工智能是非常重要的。大规模田地图像的有效回收，结合机器学习技术，在不同任务中具有成功，如型态识别、除草、种植和病虫控制。本文将介绍一种基于机器学习框架的自动化大规模植物特征注释技术，用于 sugar beet  Cercospora Leaf Spot (CLS) 疾病严重度评估。通过深度标签分布学习（DLDL）、特殊的损失函数和适应性的模型架构，我们开发了一种高效的视Transformer 模型，称为 SugarViT。本文的一个新特点是结合远程感知数据和实验室测试站的环境参数，进行疾病严重度预测。尽管这个模型是在这个特定的应用场景中评估的，但它是可以通用的，也可以应用于多种图像基于分类和回归任务。此外，我们还表明了如何通过环境元数据预训练，学习多目标问题。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Agent-Based-Collaborative-Learning-in-Cross-Individual-Wearable-Sensor-Based-Human-Activity-Recognition"><a href="#Distributed-Agent-Based-Collaborative-Learning-in-Cross-Individual-Wearable-Sensor-Based-Human-Activity-Recognition" class="headerlink" title="Distributed Agent-Based Collaborative Learning in Cross-Individual Wearable Sensor-Based Human Activity Recognition"></a>Distributed Agent-Based Collaborative Learning in Cross-Individual Wearable Sensor-Based Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04236">http://arxiv.org/abs/2311.04236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmad Esmaeili, Zahra Ghorrati, Eric T. Matson</li>
<li>For: This paper is written for the field of personalized and context-aware Human Activity Recognition, with a focus on developing scalable, adaptable, and privacy-conscious methodologies using multi-agent systems.* Methods: The paper introduces a collaborative distributed learning approach rooted in multi-agent principles, where individual users of sensor-equipped devices function as agents within a distributed network, collectively contributing to the process of learning and classifying human activities.* Results: The proposed approach has been empirically tested on two publicly accessible human activity recognition datasets, showing the efficacy of inter-individual collaborative learning compared to centralized configurations, with both local and global generalization.<details>
<summary>Abstract</summary>
The rapid growth of wearable sensor technologies holds substantial promise for the field of personalized and context-aware Human Activity Recognition. Given the inherently decentralized nature of data sources within this domain, the utilization of multi-agent systems with their inherent decentralization capabilities presents an opportunity to facilitate the development of scalable, adaptable, and privacy-conscious methodologies. This paper introduces a collaborative distributed learning approach rooted in multi-agent principles, wherein individual users of sensor-equipped devices function as agents within a distributed network, collectively contributing to the comprehensive process of learning and classifying human activities. In this proposed methodology, not only is the privacy of activity monitoring data upheld for each individual, eliminating the need for an external server to oversee the learning process, but the system also exhibits the potential to surmount the limitations of conventional centralized models and adapt to the unique attributes of each user. The proposed approach has been empirically tested on two publicly accessible human activity recognition datasets, specifically PAMAP2 and HARTH, across varying settings. The provided empirical results conclusively highlight the efficacy of inter-individual collaborative learning when contrasted with centralized configurations, both in terms of local and global generalization.
</details>
<details>
<summary>摘要</summary>
“快速增长的戴式传感器技术持有大量潜在的个性化和上下文意识识别潜力。由于这个领域的数据来源本身具有分散化特点，因此使用多代理系统的特点可以推动开发可扩展、适应性强和隐私意识的方法。本文介绍一种基于多代理原则的分布式学习方法，在其中每个戴式设备上的用户作为分布式网络中的代理，共同参与人活动识别的全面学习过程。在该提议方法中，不仅保护了每个人的活动监测数据隐私，而且消除了中央服务器的学习过程监控需求，系统还能够超越传统中央化模型的局限性，适应每个用户的特点。这种方法在两个公共可访问的人类活动识别数据集（PAMAP2和HARTH）上进行了实验测试，结果表明在不同的设定下，分布式学习方法比中央化配置更高效， both locally and globally。”
</details></li>
</ul>
<hr>
<h2 id="Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations"><a href="#Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations" class="headerlink" title="Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations"></a>Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03059">http://arxiv.org/abs/2311.03059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
<li>for: 这个论文研究了一个$\max-T$不确定方程系统的不一致性，其中$T$是一个t-整数 among $\min$, 产品或Lukasiewicz的t-整数。</li>
<li>methods: 对于不一致的$\max-T$系统，我们直接构建了一个 canonical最大一致子系统（w.r.t. 包含关系），主要工具是计算不确定系统中Chebychev距离$\Delta &#x3D; \inf_{c \in \mathcal{C} \Vert b - c \Vert$的分析式公式，其中$\mathcal{C}$是定义同样矩阵$A$的一致系统的二元组集。</li>
<li>results: 对于不一致的$\max-\min$系统，我们提供了一种高效的方法来获得所有一致子系统，并证明可以逐次获得所有最大一致子系统。<details>
<summary>Abstract</summary>
In this article, we study the inconsistency of a system of $\max-T$ fuzzy relational equations of the form $A \Box_{T}^{\max} x = b$, where $T$ is a t-norm among $\min$, the product or Lukasiewicz's t-norm. For an inconsistent $\max-T$ system, we directly construct a canonical maximal consistent subsystem (w.r.t the inclusion order). The main tool used to obtain it is the analytical formula which compute the Chebyshev distance $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$ associated to the inconsistent $\max-T$ system, where $\mathcal{C}$ is the set of second members of consistent systems defined with the same matrix $A$. Based on the same analytical formula, we give, for an inconsistent $\max-\min$ system, an efficient method to obtain all its consistent subsystems, and we show how to iteratively get all its maximal consistent subsystems.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了 $\max-T$ 模糊关系方程的不一致性，其中 $T$ 是一种 t-整数（可能是乘法或卢卡氏 t-整数）。对于一个不一致的 $\max-T$ 系统，我们直接构造了一个宽义最大可consistent subsystem（w.r.t. 包含顺序）。我们使用了关于不一致 $\max-T$ 系统的分析式公式来计算 Chebyshev 距离 $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$，其中 $\mathcal{C}$ 是定义同 $A$ 矩阵的一致系统的二元部分。基于同样的分析式公式，我们为一个不一致 $\max-\min$ 系统提供了一种高效的方法来获取所有一致子系统，并证明了可以递归地获取所有最大可consistent subsystem。
</details></li>
</ul>
<hr>
<h2 id="LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs"><a href="#LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs" class="headerlink" title="LitSumm: Large language models for literature summarisation of non-coding RNAs"></a>LitSumm: Large language models for literature summarisation of non-coding RNAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03056">http://arxiv.org/abs/2311.03056</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rnacentral/litscan-summarization">https://github.com/rnacentral/litscan-summarization</a></li>
<li>paper_authors: Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Anton I. Petrov, Alex Bateman, Blake Sweeney</li>
<li>for: 本研究旨在解决生物医学文献筛选的挑战，即由于文献数量不断增加，而有限的审核人员无法适应所有相关文献。</li>
<li>methods: 本研究使用大型自然语言模型（LLM）生成非编码RNA文献摘要，并通过商业LLM和链接的提问和检查来生成高质量、准确的摘要。</li>
<li>results: 研究表明，使用现有的LLM可以自动生成高质量的非编码RNA文献摘要，并且与人类评价高度相关。此外，研究还发现自动评价方法与人类评价不 correlate。最后，研究应用其工具到选择的4,600多个ncRNA上，并将生成的摘要公开提供给RNAcentral资源。<details>
<summary>Abstract</summary>
Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated evaluation approaches, finding that they do not correlate with human assessment. Finally, we apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.   Availability: Code used to produce these summaries can be found here: https://github.com/RNAcentral/litscan-summarization and the dataset of contexts and summaries can be found here: https://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are also displayed on the RNA report pages in RNAcentral (https://rnacentral.org/)
</details>
<details>
<summary>摘要</summary>
目的：生命科学文献筛选是一项快速增长的挑战。随着发表速度的不断增加，与全球团队规模相比，生命科学文献筛选者的数量几乎固定，这对生物医学知识库开发者带来了主要的挑战。很少的知识库有资源可以涵盖整个相关文献，所以它们都必须优化努力。结果：在这项工作中，我们通过使用大型自然语言模型（LLM）生成非编码RNA文献摘要，以解决生命科学文献筛选者缺乏时间的问题。我们证明了可以使用商业LLM和链接的提示和检查生成高质量、精准的文献摘要，并且人工评估表明大多数摘要具有极高质量。我们还应用了常用的自动评估方法，发现它们与人工评估不符。最后，我们使用我们的工具对4600多个ncRNA进行摘要，并将生成的摘要公开于RNAcentral资源上。我们 conclued that使用当代LLM生成文献摘要是可能的，只要仔细制定提示和自动检查。可用性：用于生成这些摘要的代码可以在GitHub上找到：https://github.com/RNAcentral/litscan-summarization。我们生成的摘要和文献上下文可以在Hugging Face上找到：https://huggingface.co/datasets/RNAcentral/litsumm-v1。摘要也被显示在RNAcentral报告页面上（https://rnacentral.org/)。
</details></li>
</ul>
<hr>
<h2 id="Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models"><a href="#Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models" class="headerlink" title="Masking Hyperspectral Imaging Data with Pretrained Models"></a>Masking Hyperspectral Imaging Data with Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03053">http://arxiv.org/abs/2311.03053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hifexplo/masking">https://github.com/hifexplo/masking</a></li>
<li>paper_authors: Elias Arbash, Andréa de Lima Ribeiro, Sam Thiele, Nina Gnann, Behnood Rasti, Margret Fuchs, Pedram Ghamisi, Richard Gloaguen</li>
<li>for: 提高干扰谱数据处理性能，避免干扰谱数据中的背景频谱特征的影响。</li>
<li>methods: 提出了一种基于Segment Anything Model（SAM）和零shot Grounding Dino对象检测器的图像分割方法，并在 intersect和 exclude步骤中进行了筛选和排除。</li>
<li>results: 在三个复杂的应用场景中（塑料碎屑特征化、钻核扫描和垃圾监测）得到了明显的改善，包括计算成本、内存需求和总性能。<details>
<summary>Abstract</summary>
The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.
</details>
<details>
<summary>摘要</summary>
您的干扰背景预测数据处理性能的问题可以通过Masking来解决。我们的提案包括两个基本部分：首先生成区域兴趣标识符，然后仅应用干扰数据处理技术于新生成的干扰数据立方体中。我们的创新在于采用的Segment Anything Model（SAM）来提取数据集中的所有对象，然后使用零扩展Grounding Dino对象检测器进行筛选、排除掉重叠的步骤，无需进行微调或重新训练。为证明掩码方法的效果，我们在三个复杂的应用场景中运用了该方法，即杂物Characterization、钻探核心扫描和垃圾监测。我们提供了这三个应用场景的数值评估结果，并附上使用的超参数。方法的脚本将于https://github.com/hifexplo/Masking上公开。
</details></li>
</ul>
<hr>
<h2 id="Grouping-Local-Process-Models"><a href="#Grouping-Local-Process-Models" class="headerlink" title="Grouping Local Process Models"></a>Grouping Local Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03040">http://arxiv.org/abs/2311.03040</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Viki Peeva, Wil M. P. van der Aalst</li>
<li>for: 本研究旨在提出一种三步管道来归类类似的本地过程模型（LPM），以解决模型爆炸和重复问题。</li>
<li>methods: 本研究使用了不同的过程模型相似度度量来归类LPM。</li>
<li>results: 实验结果表明， grouping 可以减少模型的数量，提高模型的精度和可读性。<details>
<summary>Abstract</summary>
In recent years, process mining emerged as a proven technology to analyze and improve operational processes. An expanding range of organizations using process mining in their daily operation brings a broader spectrum of processes to be analyzed. Some of these processes are highly unstructured, making it difficult for traditional process discovery approaches to discover a start-to-end model describing the entire process. Therefore, the subdiscipline of Local Process Model (LPM) discovery tries to build a set of LPMs, i.e., smaller models that explain sub-behaviors of the process. However, like other pattern mining approaches, LPM discovery algorithms also face the problems of model explosion and model repetition, i.e., the algorithms may create hundreds if not thousands of models, and subsets of them are close in structure or behavior. This work proposes a three-step pipeline for grouping similar LPMs using various process model similarity measures. We demonstrate the usefulness of grouping through a real-life case study, and analyze the impact of different measures, the gravity of repetition in the discovered LPMs, and how it improves after grouping on multiple real event logs.
</details>
<details>
<summary>摘要</summary>
Recently, process mining has emerged as a proven technology to analyze and improve operational processes. With an increasing number of organizations using process mining in their daily operations, a broader range of processes are being analyzed. However, some of these processes are highly unstructured, making it difficult for traditional process discovery approaches to discover a start-to-end model describing the entire process. Therefore, the subdiscipline of Local Process Model (LPM) discovery has emerged to build a set of LPMs, i.e., smaller models that explain sub-behaviors of the process. However, like other pattern mining approaches, LPM discovery algorithms also face the problems of model explosion and model repetition, i.e., the algorithms may create hundreds if not thousands of models, and subsets of them are close in structure or behavior. This work proposes a three-step pipeline for grouping similar LPMs using various process model similarity measures. We demonstrate the usefulness of grouping through a real-life case study, and analyze the impact of different measures, the gravity of repetition in the discovered LPMs, and how it improves after grouping on multiple real event logs.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation"><a href="#GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation" class="headerlink" title="GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation"></a>GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03035">http://arxiv.org/abs/2311.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ackesnal/gtp-vit">https://github.com/ackesnal/gtp-vit</a></li>
<li>paper_authors: Xuwei Xu, Sen Wang, Yudong Chen, Yanping Zheng, Zhewei Wei, Jiajun Liu</li>
<li>for: 提高资源受限设备上的预训练 ViT 的效率，使其能够快速推理图像。</li>
<li>methods: 提出了一种基于图的减少方法，即图像信息传递方法（GTP），通过在图像中减少不重要的图像信息，并将其传递给相邻的重要图像信息，以提高模型的效率。</li>
<li>results: 对 ImageNet-1K 进行了大量的实验，并证明了 GTP 可以减少 DeiT-S 和 DeiT-B 的计算复杂度，同时保持模型的性能水平。特别是，GTP 可以在不需要Finetune的情况下，对各种背景下的各种核心体系进行更快的推理。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computational complexity while preserving essential information of eliminated tokens. Combined with an innovative token selection strategy, GTP can efficiently identify image tokens to be propagated. Extensive experiments have validated GTP's effectiveness, demonstrating both efficiency and performance improvements. Specifically, GTP decreases the computational complexity of both DeiT-S and DeiT-B by up to 26% with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and remarkably surpasses the state-of-the-art token merging method on various backbones at an even faster inference speed. The source code is available at https://github.com/Ackesnal/GTP-ViT.
</details>
<details>
<summary>摘要</summary>
Computer vision 领域中的 Vision Transformers (ViTs) 已经革命化了，但是在具有限制的资源设备上部署它们仍然是一个挑战。为了快速部署预训练的 ViTs，批量缩减和批量合并方法已经被开发出来，以减少计算中的符号数。然而，这些方法仍然有一些限制，例如图像信息的损失和符号匹配过程的不效率。在这篇论文中，我们介绍了一种新的图表-基于的符号传播（GTP）方法，以解决计算复杂性和信息保留之间的平衡问题。这种方法基于图 summarization 算法，细致地在空间和Semantic上传递不重要的符号信息到更重要的符号上。因此，剩下的符号只需要扮演一个图像的概要，允许方法减少计算复杂性而不失信息。与一种创新的符号选择策略结合使用，GTP可以高效地选择图像中的符号。经验证明了 GTP 的有效性，可以在 ImageNet-1K 上 без fine-tuning 下降到 0.3% 的准确率下减少 DeiT-S 和 DeiT-B 的计算复杂性，并在不同的核心上轻松突破当前的状况报告。详细的源代码可以在 <https://github.com/Ackesnal/GTP-ViT> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models"><a href="#Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models" class="headerlink" title="Beyond Words: A Mathematical Framework for Interpreting Large Language Models"></a>Beyond Words: A Mathematical Framework for Interpreting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03033">http://arxiv.org/abs/2311.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier González, Aditya V. Nori</li>
<li>for: This paper aims to provide a mathematical framework for understanding and improving large language models (LLMs).</li>
<li>methods: The paper proposes a framework called Hex, which clarifies key terms and concepts in LLM research and offers a precise and consistent way to characterize LLMs.</li>
<li>results: The paper differentiates chain-of-thought reasoning from chain-of-thought prompting and establishes the conditions under which they are equivalent. The paper argues that its formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair, and robust.Here’s the Simplified Chinese version of the three points:</li>
<li>for: 这篇论文目的是为大语言模型（LLM）提供数学基础。</li>
<li>methods: 论文提出了名为“Hex”的框架，它在 LLM 研究中清晰地解释了关键术语和概念，并提供了一种精确和一致的方式来描述 LLM。</li>
<li>results: 论文区分了链条思维和链条提示，并确定了它们在何种情况下是等价的。论文认为，其 формаль定义和结果对于构建安全、可靠、公平、Robust的生成 AI 系统是关键的。<details>
<summary>Abstract</summary>
Large language models (LLMs) are powerful AI tools that can generate and comprehend natural language text and other complex information. However, the field lacks a mathematical framework to systematically describe, compare and improve LLMs. We propose Hex a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning. The Hex framework offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings. Using Hex, we differentiate chain-of-thought reasoning from chain-of-thought prompting and establish the conditions under which they are equivalent. This distinction clarifies the basic assumptions behind chain-of-thought prompting and its implications for methods that use it, such as self-verification and prompt programming.   Our goal is to provide a formal framework for LLMs that can help both researchers and practitioners explore new possibilities for generative AI. We do not claim to have a definitive solution, but rather a tool for opening up new research avenues. We argue that our formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair and robust, especially in domains like healthcare and software engineering.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Learning-for-Clinical-Structured-Data-A-Benchmark-Comparison-of-Engineering-and-Statistical-Approaches"><a href="#Federated-Learning-for-Clinical-Structured-Data-A-Benchmark-Comparison-of-Engineering-and-Statistical-Approaches" class="headerlink" title="Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches"></a>Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03417">http://arxiv.org/abs/2311.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nliulab/fl-benchmark">https://github.com/nliulab/fl-benchmark</a></li>
<li>paper_authors: Siqi Li, Di Miao, Qiming Wu, Chuan Hong, Danny D’Agostino, Xin Li, Yilin Ning, Yuqing Shang, Huazhu Fu, Marcus Eng Hock Ong, Hamed Haddadi, Nan Liu</li>
<li>for: 保护医疗合作中数据隐私</li>
<li>methods: 比较工程域和统计领域的 Federated learning 框架</li>
<li>results: 统计式 Federated learning 算法提供较为准确的点估计，但工程域基础的方法可以生成更高精度的预测结果。<details>
<summary>Abstract</summary>
Federated learning (FL) has shown promising potential in safeguarding data privacy in healthcare collaborations. While the term "FL" was originally coined by the engineering community, the statistical field has also explored similar privacy-preserving algorithms. Statistical FL algorithms, however, remain considerably less recognized than their engineering counterparts. Our goal was to bridge the gap by presenting the first comprehensive comparison of FL frameworks from both engineering and statistical domains. We evaluated five FL frameworks using both simulated and real-world data. The results indicate that statistical FL algorithms yield less biased point estimates for model coefficients and offer convenient confidence interval estimations. In contrast, engineering-based methods tend to generate more accurate predictions, sometimes surpassing central pooled and statistical FL models. This study underscores the relative strengths and weaknesses of both types of methods, emphasizing the need for increased awareness and their integration in future FL applications.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）在医疗合作中保护数据隐私的潜力备受关注。虽然“FL”这个词汇最初是由工程师社群提出的，但随后的统计学界也开始探索类似的隐私保护算法。统计学上的FL算法与工程师社群的算法相比，尚未获得相同的认知程度。我们的目标是将这两种领域的FL框架进行首次全面比较，以评估它们在实际应用中的优劣。我们使用了五种FL框架，包括工程师社群和统计学界的方法，并使用实验和真实数据进行评估。结果显示，统计学上的FL算法对数据的批评估计获得较低的偏见，并且可以轻松地Estimate interval的信度。相比之下，工程师社群的方法具有较高的预测精度，有时超过中央联合和统计学上的FL模型。这个研究强调了两种方法之间的相对优劣，并强调未来FL应用中需要增加这两种方法的融合。
</details></li>
</ul>
<hr>
<h2 id="Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network"><a href="#Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network" class="headerlink" title="Visual-information-driven model for crowd simulation using temporal convolutional network"></a>Visual-information-driven model for crowd simulation using temporal convolutional network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02996">http://arxiv.org/abs/2311.02996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanwen Liang, Eric Wai Ming Lee</li>
<li>for: 提高数据驱动人群模拟模型的适应性和现实感</li>
<li>methods:  incorporate visual information, including scenario geometry and pedestrian locomotion, to improve the adaptability and realism of data-driven crowd simulation models</li>
<li>results: 在三个不同的公共步行者动态数据集上测试并评估了视觉驱动的人群模拟模型，并显示了该模型在所有三个几何场景中的改进适应性。<details>
<summary>Abstract</summary>
Crowd simulations play a pivotal role in building design, influencing both user experience and public safety. While traditional knowledge-driven models have their merits, data-driven crowd simulation models promise to bring a new dimension of realism to these simulations. However, most of the existing data-driven models are designed for specific geometries, leading to poor adaptability and applicability. A promising strategy for enhancing the adaptability and realism of data-driven crowd simulation models is to incorporate visual information, including the scenario geometry and pedestrian locomotion. Consequently, this paper proposes a novel visual-information-driven (VID) crowd simulation model. The VID model predicts the pedestrian velocity at the next time step based on the prior social-visual information and motion data of an individual. A radar-geometry-locomotion method is established to extract the visual information of pedestrians. Moreover, a temporal convolutional network (TCN)-based deep learning model, named social-visual TCN, is developed for velocity prediction. The VID model is tested on three public pedestrian motion datasets with distinct geometries, i.e., corridor, corner, and T-junction. Both qualitative and quantitative metrics are employed to evaluate the VID model, and the results highlight the improved adaptability of the model across all three geometric scenarios. Overall, the proposed method demonstrates effectiveness in enhancing the adaptability of data-driven crowd models.
</details>
<details>
<summary>摘要</summary>
人群模拟在建筑设计中发挥重要作用，影响用户体验和公共安全。传统的知识驱动模型有其优点，但数据驱动人群模拟模型可以带来新的现实感。然而，现有的数据驱动模型大多适用于特定的几何结构，导致适应性和可用性强度有限。为了提高数据驱动人群模拟模型的适应性和现实感，本文提出了一种视觉信息驱动（VID）人群模拟模型。VID模型根据先前的社交视觉信息和人员运动数据预测下一步人群速度。为了提取视觉信息，本文提出了一种雷达几何运动方法。此外，本文还开发了一种基于深度学习的社交视觉径向网络（Social-Visual TCN）模型，用于速度预测。VID模型在三个不同的公共人群动向数据集上进行测试，包括通道、角落和T字口。使用质量和量度指标评估VID模型，结果表明VID模型在所有三个几何场景中的适应性得到了提高。总的来说，本文提出的方法可以提高数据驱动人群模拟模型的适应性。
</details></li>
</ul>
<hr>
<h2 id="PowerFlowNet-Leveraging-Message-Passing-GNNs-for-Improved-Power-Flow-Approximation"><a href="#PowerFlowNet-Leveraging-Message-Passing-GNNs-for-Improved-Power-Flow-Approximation" class="headerlink" title="PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation"></a>PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03415">http://arxiv.org/abs/2311.03415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Lin, Stavros Orfanoudakis, Nathan Ordonez Cardenas, Juan S. Giraldo, Pedro P. Vergara</li>
<li>for: 提高现代电力网络的准确和高效运行和规划</li>
<li>methods: 使用图神经网络（GNNs）提高PF近似的速度和准确性</li>
<li>results: 在简单的IEEE 14-bus系统和实际的法国高压网络（6470rte）中，PowerFlowNet在性能和执行时间方面与新顿-拉普逊方法具有相似性，但是实现4倍 faster，并在其他传统近似方法（如DC relaxation方法）的基础上显著地超越它们。<details>
<summary>Abstract</summary>
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' efficient operation and planning. Therefore, there is a need for scalable algorithms capable of handling large-scale power networks that can provide accurate and fast solutions. Graph Neural Networks (GNNs) have emerged as a promising approach for enhancing the speed of PF approximations by leveraging their ability to capture distinctive features from the underlying power network graph. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making PowerFlowNet a highly promising solution for real-world PF analysis. Furthermore, we verify the efficacy of our approach by conducting an in-depth experimental evaluation, thoroughly examining the performance, scalability, interpretability, and architectural dependability of PowerFlowNet. The evaluation provides insights into the behavior and potential applications of GNNs in power system analysis.
</details>
<details>
<summary>摘要</summary>
准确高效电流流动（PF）分析是现代电力网络的efficient操作和规划中的关键。因此，有一需要可扩展的算法，可以处理大规模的电力网络，提供准确和快速的解决方案。图neuronal networks（GNNs）已经出现为PF近似中的一种有前途的方法，通过它们能够从电力网络图中捕捉特征。在这种研究中，我们介绍PowerFlowNet，一种新的GNN架构，用于PF近似，它在简单的IEEE 14-bus系统和实际的法国高压网络（6470rte）中显示了与传统的Newton-Raphson方法类似的性能，但是实现速度为4倍快和145倍快。此外，它也明显超过了其他传统的近似方法，如DC缓和方法，在性能和执行时间方面，因此PowerFlowNet是一个非常有前途的PF分析解决方案。此外，我们通过进行深入的实验评估，全面检查PowerFlowNet的性能、可扩展性、可读性和架构可靠性。实验结果为我们提供了GNN在电力系统分析中的行为和应用前景。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Neural-Network-Approach-for-3D-Multi-Criteria-Design-Generation-and-Optimization-of-an-Engine-Mount-for-an-Unmanned-Air-Vehicle"><a href="#A-Generative-Neural-Network-Approach-for-3D-Multi-Criteria-Design-Generation-and-Optimization-of-an-Engine-Mount-for-an-Unmanned-Air-Vehicle" class="headerlink" title="A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle"></a>A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03414">http://arxiv.org/abs/2311.03414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Petroll, Sebastian Eilermann, Philipp Hoefer, Oliver Niggemann</li>
<li>for: 这 paper 的目的是用生成神经网络进行功能兼ね合的 3D 设计重构和生成。</li>
<li>methods: 这 paper 使用 Conditional Variational Autoencoder (CVAE) 和 Marching cubes algorithm 来生成 meshes 并对其进行 simulated 评估。</li>
<li>results:  paper 能够生成符合自定义功能条件的优化设计。<details>
<summary>Abstract</summary>
One of the most promising developments in computer vision in recent years is the use of generative neural networks for functionality condition-based 3D design reconstruction and generation. Here, neural networks learn dependencies between functionalities and a geometry in a very effective way. For a neural network the functionalities are translated in conditions to a certain geometry. But the more conditions the design generation needs to reflect, the more difficult it is to learn clear dependencies. This leads to a multi criteria design problem due various conditions, which are not considered in the neural network structure so far.   In this paper, we address this multi-criteria challenge for a 3D design use case related to an unmanned aerial vehicle (UAV) motor mount. We generate 10,000 abstract 3D designs and subject them all to simulations for three physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we train a Conditional Variational Autoencoder (CVAE) using the geometry and corresponding multicriteria functional constraints as input. We use our trained CVAE as well as the Marching cubes algorithm to generate meshes for simulation based evaluation. The results are then evaluated with the generated UAV designs. Subsequently, we demonstrate the ability to generate optimized designs under self-defined functionality conditions using the trained neural network.
</details>
<details>
<summary>摘要</summary>
In this paper, we address this multi-criteria challenge for a 3D design use case related to an unmanned aerial vehicle (UAV) motor mount. We generate 10,000 abstract 3D designs and subject them all to simulations for three physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we train a Conditional Variational Autoencoder (CVAE) using the geometry and corresponding multicriteria functional constraints as input. We use our trained CVAE as well as the Marching cubes algorithm to generate meshes for simulation-based evaluation. The results are then evaluated with the generated UAV designs. Subsequently, we demonstrate the ability to generate optimized designs under self-defined functionality conditions using the trained neural network.
</details></li>
</ul>
<hr>
<h2 id="Discret2Di-–-Deep-Learning-based-Discretization-for-Model-based-Diagnosis"><a href="#Discret2Di-–-Deep-Learning-based-Discretization-for-Model-based-Diagnosis" class="headerlink" title="Discret2Di – Deep Learning based Discretization for Model-based Diagnosis"></a>Discret2Di – Deep Learning based Discretization for Model-based Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03413">http://arxiv.org/abs/2311.03413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Moddemann, Henrik Sebastian Steude, Alexander Diedrich, Oliver Niggemann</li>
<li>for: 本文提出了一种自动学习逻辑表示法，用于进行基于一致性的诊断。</li>
<li>methods: 本文使用了机器学习技术，将时间序列转化为逻辑表示，并自动学习逻辑规则。</li>
<li>results: 本文通过实验示出，自动学习逻辑规则可以有效地进行基于一致性的诊断。In English, this translates to:</li>
<li>for: The paper proposes an automated learning method for logical expressions for consistency-based diagnosis.</li>
<li>methods: The paper uses machine learning techniques to convert time series into logical representations and automatically learn logical rules.</li>
<li>results: The paper shows through experiments that automated learning of logical rules can effectively perform consistency-based diagnosis.<details>
<summary>Abstract</summary>
Consistency-based diagnosis is an established approach to diagnose technical applications, but suffers from significant modeling efforts, especially for dynamic multi-modal time series. Machine learning seems to be an obvious solution, which becomes less obvious when looking at details: Which notion of consistency can be used? If logical calculi are still to be used, how can dynamic time series be transferred into the discrete world?   This paper presents the methodology Discret2Di for automated learning of logical expressions for consistency-based diagnosis. While these logical calculi have advantages by providing a clear notion of consistency, they have the key problem of relying on a discretization of the dynamic system. The solution presented combines machine learning from both the time series and the symbolic domain to automate the learning of logical rules for consistency-based diagnosis.
</details>
<details>
<summary>摘要</summary>
系统稳定性分析是一种已经确立的方法，用于诊断技术应用程序，但是它受到了模型化努力的限制，特别是对动态多Modal时间序列的诊断。机器学习似乎是一个自然的解决方案，但是当考虑到细节时，问题变得不那么明显：哪种一致性概念可以使用？如果逻辑calculus仍然被用，如何将动态时间序列转化成离散世界？本文介绍了一种方法ологи法Discret2Di，用于自动学习逻辑表达式 для稳定性分析。这些逻辑calculus有优点，因为它们提供了明确的一致性概念，但它们的关键问题是基于离散系统的分析。本文的解决方案是结合时间序列和符号领域的机器学习来自动学习逻辑规则 для稳定性分析。
</details></li>
</ul>
<hr>
<h2 id="TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications"><a href="#TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications" class="headerlink" title="TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications"></a>TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02971">http://arxiv.org/abs/2311.02971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autogluon/tabrepo">https://github.com/autogluon/tabrepo</a></li>
<li>paper_authors: David Salinas, Nick Erickson</li>
<li>for: 这篇论文是为了介绍一个新的Tabular模型评估和预测数据集（TabRepo）。</li>
<li>methods: 论文使用了1206个模型在200个回归和分类 datasets上进行了预测和评估。</li>
<li>results: 论文表明，通过使用 TabRepo 可以免费地比较自动化机器学习（AutoML）系统和精细化优化hyperparameter，以及应用标准的传输学习技术可以超越当前的标准Tabular系统在准确性、运行时间和延迟方面。<details>
<summary>Abstract</summary>
We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1206 models evaluated on 200 regression and classification datasets. We illustrate the benefit of our datasets in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at no cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
</details>
<details>
<summary>摘要</summary>
我们介绍TabRepo，一个新的Tabular模型评估和预测Dataset。TabRepo包含1206个模型在200个回归和分类Dataset上的预测和度量。我们显示了我们的Dataset的价值，包括：1. 可以免费使用预computed模型预测来比较搜寻过程优化和现有的AutoML系统，以及考虑结合。2. 可以快速地将模型应用到新的Dataset上，并且使用标准的转移学习技术来超越目前的Tabular系统在准确、运行时间和延迟方面的表现。Here's the translation in Traditional Chinese:我们介绍TabRepo，一个新的Tabular模型评估和预测Dataset。TabRepo包含1206个模型在200个回归和分类Dataset上的预测和度量。我们显示了我们的Dataset的价值，包括：1. 可以免费使用预computed模型预测来比较搜寻过程优化和现有的AutoML系统，以及考虑结合。2. 可以快速地将模型应用到新的Dataset上，并且使用标准的转移学习技术来超越目前的Tabular系统在准确、运行时间和延遁方面的表现。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction"><a href="#Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction" class="headerlink" title="Retrieval-Augmented Code Generation for Universal Information Extraction"></a>Retrieval-Augmented Code Generation for Universal Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02962">http://arxiv.org/abs/2311.02962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 本文提出了一个基于大语言模型（LLMs）的通用扩展代码生成框架，用于信息抽取（IE）任务。</li>
<li>methods: 本文使用了Python类定义任务特定的结构知识表示，并运用了内容学习机制以将文本中的信息转换为代码。</li>
<li>results: 实验结果显示， Code4UIE 框架可以实现高效地对五种代表性的IE任务进行扩展代码生成。<details>
<summary>Abstract</summary>
Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）的目标是从自然语言文本中提取结构知识（例如实体、关系、事件），这会对现有方法带来挑战，因为任务特定的 schema 和文本表达的复杂性。代码作为一种形式化语言，可以在不同的 schema 下描述结构知识，并且在各种任务下可以通过代码来实现这些知识。在这篇论文中，我们提出了一种基于大语言模型（LLM）的通用检索增强代码生成框架，称为 Code4UIE，用于IE任务。specifically，Code4UIE 使用 Python 类来定义任务特定的 schema，并通过在这些类中实例化文本中的信息来提取知识。为了生成代码更加准确，Code4UIE 采用了在上下文学习机制，使 LLM 通过示例进行指导。为了获得不同任务的合适示例，Code4UIE 探索了多种示例检索策略，可以从文本中检索到与给定文本相似的示例。经验表明，Code4UIE 框架在五种代表性IE任务中的九个数据集上得到了广泛的应用。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models"><a href="#In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models" class="headerlink" title="In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models"></a>In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02956">http://arxiv.org/abs/2311.02956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlong Chen, Yaming Zhang, Jianfei Yu, Li Yang, Rui Xia</li>
<li>for: Answer factoid questions based on knowledge bases</li>
<li>methods: Use ChatGPT-based Cypher Query Language (CQL) generation framework to generate the most appropriate CQL based on Natural Language Questions (NLQ)</li>
<li>results: Achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, with an F1-score of 0.92676<details>
<summary>Abstract</summary>
Knowledge Base Question Answering (KBQA) aims to answer factoid questions based on knowledge bases. However, generating the most appropriate knowledge base query code based on Natural Language Questions (NLQ) poses a significant challenge in KBQA. In this work, we focus on the CCKS2023 Competition of Question Answering with Knowledge Graph Inference for Unmanned Systems. Inspired by the recent success of large language models (LLMs) like ChatGPT and GPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL) generation framework to generate the most appropriate CQL based on the given NLQ. Our generative framework contains six parts: an auxiliary model predicting the syntax-related information of CQL based on the given NLQ, a proper noun matcher extracting proper nouns from the given NLQ, a demonstration example selector retrieving similar examples of the input sample, a prompt constructor designing the input template of ChatGPT, a ChatGPT-based generation model generating the CQL, and an ensemble model to obtain the final answers from diversified outputs. With our ChatGPT-based CQL generation framework, we achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, achieving an F1-score of 0.92676.
</details>
<details>
<summary>摘要</summary>
知识库问答（KBQA）目标是基于知识库回答问题，但生成基于自然语言问题（NLQ）的知识库查询代码具有显著挑战。在这项工作中，我们将ocusonCCKS2023问答知识图推理对无人系统的竞赛。受最近大语言模型（LLMs）如ChatGPT和GPT-3在多种问答任务中的成功启发，我们提议一个基于ChatGPT的CypherQuery语言（CQL）生成框架，以生成基于给定NLQ的最佳CQL。我们的生成框架包括六部分：一个辅助模型预测基于给定NLQ的CQL语法信息，一个正式名词匹配器从NLQ中提取正式名词，一个示例选择器选择与输入样本相似的示例，一个提示构建器设计输入模板，一个基于ChatGPT的生成模型生成CQL，以及一个 ensemble模型以获得多元输出的最终答案。与我们的ChatGPT基于CQL生成框架，我们在CCKS2023问答知识图推理对无人系统竞赛中获得第二名，实现了F1分数0.92676。
</details></li>
</ul>
<hr>
<h2 id="Can-LLMs-Follow-Simple-Rules"><a href="#Can-LLMs-Follow-Simple-Rules" class="headerlink" title="Can LLMs Follow Simple Rules?"></a>Can LLMs Follow Simple Rules?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.04235">http://arxiv.org/abs/2311.04235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/normster/llm_rules">https://github.com/normster/llm_rules</a></li>
<li>paper_authors: Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian, Lulwa Aljeraisy, Dan Hendrycks, David Wagner</li>
<li>for: 本研究旨在提供一个程式码框架，以评估自然语言处理器（LLM）在开发者提供的规则下运行。</li>
<li>methods: 本研究使用了15个简单文本场景，让模型遵循开发者提供的规则来互动 avec human user。每个场景都有一个简洁的评估程式，以判断模型是否违反了规则。</li>
<li>results: 研究发现所有评估过的 популярProprietary和开源模型都受到了访问者调制的手动输入攻击，而GPT-4是所有模型中最好的表现。此外，研究还评估了开放模型对于梯度基本攻击的漏洞。<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developer-provided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RuLES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.
</details>
<details>
<summary>摘要</summary>
As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as "do not generate abusive content", but these may be circumvented by jailbreaking techniques. Evaluating how well LLMs follow developer-provided rules in the face of adversarial inputs typically requires manual review, which slows down monitoring and methods development. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 15 simple text scenarios in which the model is instructed to obey a set of rules in natural language while interacting with the human user. Each scenario has a concise evaluation program to determine whether the model has broken any rules in a conversation. Through manual exploration of model behavior in our scenarios, we identify 6 categories of attack strategies and collect two suites of test cases: one consisting of unique conversations from manual testing and one that systematically implements strategies from the 6 categories. Across various popular proprietary and open models such as GPT-4 and Llama 2, we find that all models are susceptible to a wide variety of adversarial hand-crafted user inputs, though GPT-4 is the best-performing model. Additionally, we evaluate open models under gradient-based attacks and find significant vulnerabilities. We propose RuLES as a challenging new setting for research into exploring and defending against both manual and automatic attacks on LLMs.Here's the translation in Traditional Chinese:当大型语言模型（LLMs）在实际应用中推广时，重要的是能够Specify和限制这些系统的行为，以确保它们可靠地进行工作。开发模型的人可能会想要设定模型的Explicit规则，例如“不要生成攻击性内容”，但这些规则可能会被破坏。评估LLMs在面对恶意输入时遵循开发者提供的规则需要手动审查，这会让监控和开发方法变得 slower。为解决这个问题，我们提出了Rule-following Language Evaluation Scenarios（RuLES），一个程式设计的框架，用于评估LLMs的遵循能力。RuLES包括15个简单文本场景，在其中模型需要遵循开发者提供的规则，并与人类用户互动。每个场景有一个简洁的评估程式，用于决定模型在对话中是否破坏了任何规则。通过我们的手动探索模型行为的方式，我们识别出6种攻击策略，并收集了两个测试集：一个是从手动测试中获得的唯一对话，另一个是实现了6种攻击策略的系统性测试集。在各种受欢迎的专有和开源模型（如GPT-4和Llama 2）中，我们发现所有模型都受到了许多攻击性的手动输入的威胁。此外，我们在Gradient-based攻击下进行评估，发现开放模型存在重大的漏洞。我们提议RuLES作为一个挑战性的新设定，用于研究LLMs在面对手动和自动攻击时的探索和防御。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation"><a href="#Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation" class="headerlink" title="Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation"></a>Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02938">http://arxiv.org/abs/2311.02938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyun Wang, Xingyu Gao, Zhenyu Chen, Lei Lyu</li>
<li>for: This paper aims to improve session-based recommendation by exploiting complex and high-order item transition information.</li>
<li>methods: The proposed method, called contrastive multi-level graph neural networks (CM-GNN), uses a combination of local-level, global-level, and hyper-level graph convolutional networks, as well as an attention-based fusion module to capture pairwise relations and high-order information among item transitions.</li>
<li>results: The proposed method outperforms state-of-the-art session-based recommendation techniques in extensive experiments on multiple benchmark datasets.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目标是改进会话基于推荐，通过捕捉复杂和高阶ITEM转换信息。</li>
<li>methods: 提议方法是对比例多级图 neural networks (CM-GNN)，使用了本地级、全球级和超过级图 convolutional networks，以及一个注意力基于的融合模块，来捕捉ITEM转换对的对应关系和高阶信息。</li>
<li>results: 提议方法在多个广泛使用的数据集上进行了广泛的实验，并证明了与会话基于推荐技术的状态OF-THE-ART的性能优于。<details>
<summary>Abstract</summary>
Session-based recommendation (SBR) aims to predict the next item at a certain time point based on anonymous user behavior sequences. Existing methods typically model session representation based on simple item transition information. However, since session-based data consists of limited users' short-term interactions, modeling session representation by capturing fixed item transition information from a single dimension suffers from data sparsity. In this paper, we propose a novel contrastive multi-level graph neural networks (CM-GNN) to better exploit complex and high-order item transition information. Specifically, CM-GNN applies local-level graph convolutional network (L-GCN) and global-level network (G-GCN) on the current session and all the sessions respectively, to effectively capture pairwise relations over all the sessions by aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph convolutional network (H-GCN) to capture high-order information among all the item transitions. CM-GNN further introduces an attention-based fusion module to learn pairwise relation-based session representation by fusing the item representations generated by L-GCN and G-GCN. CM-GNN averages the item representations obtained by H-GCN to obtain high-order relation-based session representation. Moreover, to convert the high-order item transition information into the pairwise relation-based session representation, CM-GNN maximizes the mutual information between the representations derived from the fusion module and the average pool layer by contrastive learning paradigm. We conduct extensive experiments on multiple widely used benchmark datasets to validate the efficacy of the proposed method. The encouraging results demonstrate that our proposed method outperforms the state-of-the-art SBR techniques.
</details>
<details>
<summary>摘要</summary>
Session-based recommendation (SBR) 目标是在特定时间点预测下一个项目，基于匿名用户行为序列。现有方法通常是基于简单的项目转移信息来建模会话表示。然而，由于会话数据由有限数量的用户的短时间互动组成，使得基于单一维度的项目转移信息来建模会话表示存在数据稀缺。在这篇论文中，我们提出了一种新的对比式多级图 neural network (CM-GNN)，用于更好地利用复杂的高阶项目转移信息。特别是，CM-GNN 在当前会话和所有会话上应用了本地级别的图干涉网络 (L-GCN) 和全级别网络 (G-GCN)，以有效地捕捉会话中对所有会话的对比关系。此外，CM-GNN 还应用了高级别的图干涉网络 (H-GCN)，以捕捉高阶的项目转移信息。CM-GNN 还引入了一种注意力基于的融合模块，用于学习对比关系基于会话表示。CM-GNN 将 obtained 由 L-GCN 和 G-GCN 生成的项目表示进行 fusion，并使用 H-GCN 生成的高阶关系基于SESSION表示。此外，为了将高阶项目转移信息转化为对比关系基于会话表示，CM-GNN 使用对比学习框架强制最大化对这两个表示之间的共 informations。我们在多个广泛使用的 benchmark 数据集上进行了广泛的实验，以验证我们提出的方法的有效性。结果表明，我们的提出的方法在对State-of-the-art SBR 技术进行比较时表现出色。
</details></li>
</ul>
<hr>
<h2 id="Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things"><a href="#Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things" class="headerlink" title="Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things"></a>Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02926">http://arxiv.org/abs/2311.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meatery/semantic-segmentation">https://github.com/meatery/semantic-segmentation</a></li>
<li>paper_authors: Li Ping Qian, Yi Zhang, Sikai Lyu, Huijie Zhu, Yuan Wu, Xuemin Sherman Shen, Xiaoniu Yang</li>
<li>for: 提出一种深度学习图像semantic通信模型，以提高AIoT设备中图像数据的有效传输和恢复。</li>
<li>methods: 提议在传输端使用高精度图像semantic分割算法提取图像semantic信息，以实现图像数据的显著压缩。在接收端，使用基于GAN的semantic图像恢复算法将semantic图像转换为详细的真实场景图像。</li>
<li>results: 对比WebP和CycleGAN，提议的图像semantic通信模型可以提高图像压缩率和恢复精度，平均提高71.93%和25.07%。此外，我们的demo实验表明，提议模型可以将图像传输延迟降低95.26%。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligent Internet of Things (AIoT), the image data from AIoT devices has been witnessing the explosive increasing. In this paper, a novel deep image semantic communication model is proposed for the efficient image communication in AIoT. Particularly, at the transmitter side, a high-precision image semantic segmentation algorithm is proposed to extract the semantic information of the image to achieve significant compression of the image data. At the receiver side, a semantic image restoration algorithm based on Generative Adversarial Network (GAN) is proposed to convert the semantic image to a real scene image with detailed information. Simulation results demonstrate that the proposed image semantic communication model can improve the image compression ratio and recovery accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN, respectively. More importantly, our demo experiment shows that the proposed model reduces the total delay by 95.26% in the image communication, when comparing with the original image transmission.
</details>
<details>
<summary>摘要</summary>
随着人工智能互联网物联网（AIoT）的快速发展，AIoT设备中的图像数据已经经历了急剧增长。在这篇论文中，我们提出了一种新的深度图像Semantic Communication模型，用于AIoT中高效的图像通信。特别是在发送端，我们提出了一种高精度图像semantic分割算法，以EXTRACT图像中的semantic信息，以实现图像数据的显著压缩。在接收端，我们提出了一种基于Generative Adversarial Network（GAN）的semantic图像恢复算法，以将semantic图像转换为详细信息的真实场景图像。实验结果表明，我们提出的图像Semantic Communication模型可以提高图像压缩比和恢复精度，比WebP和CycleGAN的平均提高71.93%和25.07%。此外，我们的demo实验表明，我们的模型可以将图像通信总延迟减少95.26%，比原始图像传输更加高效。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract"><a href="#Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract" class="headerlink" title="Virtual Action Actor-Critic Framework for Exploration (Student Abstract)"></a>Virtual Action Actor-Critic Framework for Exploration (Student Abstract)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02916">http://arxiv.org/abs/2311.02916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bumgeun Park, Taeyoung Kim, Quoc-Vinh Lai-Dang, Dongsoo Har</li>
<li>for: 提高RL中agent的寻找效率</li>
<li>methods: 提出了一种新的actor-critic框架，即虚拟行为actor-critic（VAAC），以解决RL中agent的寻找效率挑战。</li>
<li>results: 实验结果显示，VAAC比现有算法更高效地进行寻找。<details>
<summary>Abstract</summary>
Efficient exploration for an agent is challenging in reinforcement learning (RL). In this paper, a novel actor-critic framework namely virtual action actor-critic (VAAC), is proposed to address the challenge of efficient exploration in RL. This work is inspired by humans' ability to imagine the potential outcomes of their actions without actually taking them. In order to emulate this ability, VAAC introduces a new actor called virtual actor (VA), alongside the conventional actor-critic framework. Unlike the conventional actor, the VA takes the virtual action to anticipate the next state without interacting with the environment. With the virtual policy following a Gaussian distribution, the VA is trained to maximize the anticipated novelty of the subsequent state resulting from a virtual action. If any next state resulting from available actions does not exhibit high anticipated novelty, training the VA leads to an increase in the virtual policy entropy. Hence, high virtual policy entropy represents that there is no room for exploration. The proposed VAAC aims to maximize a modified Q function, which combines cumulative rewards and the negative sum of virtual policy entropy. Experimental results show that the VAAC improves the exploration performance compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
RL中的agent寻找最有效的探索方式是一项挑战。本文提出了一种新的actor-critic框架，即虚拟动作actor-critic（VAAC），以解决RL中的探索挑战。这项工作受人类能够想象自己的行动结果而不需要实际行动的能力所启发。为了模仿这种能力，VAAC引入了一个新的actor，即虚拟actor（VA）。与传统actor-critic框架不同的是，VA不需要与环境交互就可以预测下一个状态。通过虚拟策略按照高维度分布随机选择虚拟动作，VA在预测下一个状态时尽可能增加其 novaativity。如果可用的动作中任何一个状态不具备高预测 novaativity，则训练VA会增加虚拟策略的 entropy。因此，高虚拟策略 entropy表示探索空间充满潜能。VAAC的目标是最大化修改后Q函数，该函数组合了总奖励和虚拟策略 entropy的负数。实验结果表明，VAAC在探索性能方面比现有算法提高了。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance"><a href="#Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance" class="headerlink" title="Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance"></a>Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02912">http://arxiv.org/abs/2311.02912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhao Li, Yuming Xiang, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</li>
<li>for: 该论文主要研究了多机器人系统（MRS）的协同控制问题，尤其是在大规模Decentralized MRS中实现追逐避免任务的可能性。</li>
<li>methods: 该论文提出了一种基于仿写学的多代理控制算法（IA-MAPPO），用于在协同控制下实现追逐逃脱任务。该算法包括一个基于策略浸泡的MAPPO执行器，以及使用仿写学来减少通信开销和提高扩展性。</li>
<li>results:  simulations results validate the effectiveness of IA-MAPPO, and extensive ablation experiments show that the performance is comparable to a centralized solution with significant decrease in communication overheads.<details>
<summary>Abstract</summary>
Multi-Robot System (MRS) has garnered widespread research interest and fostered tremendous interesting applications, especially in cooperative control fields. Yet little light has been shed on the compound ability of formation, monitoring and defence in decentralized large-scale MRS for pursuit avoidance, which puts stringent requirements on the capability of coordination and adaptability. In this paper, we put forward a decentralized Imitation learning based Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm to provide a flexible and communication-economic solution to execute the pursuit avoidance task in well-formed swarm. In particular, a policy-distillation based MAPPO executor is firstly devised to capably accomplish and swiftly switch between multiple formations in a centralized manner. Furthermore, we utilize imitation learning to decentralize the formation controller, so as to reduce the communication overheads and enhance the scalability. Afterwards, alternative training is leveraged to compensate the performance loss incurred by decentralization. The simulation results validate the effectiveness of IA-MAPPO and extensive ablation experiments further show the performance comparable to a centralized solution with significant decrease in communication overheads.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ViDa-Visualizing-DNA-hybridization-trajectories-with-biophysics-informed-deep-graph-embeddings"><a href="#ViDa-Visualizing-DNA-hybridization-trajectories-with-biophysics-informed-deep-graph-embeddings" class="headerlink" title="ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings"></a>ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03411">http://arxiv.org/abs/2311.03411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenwei-zhang/ViDa">https://github.com/chenwei-zhang/ViDa</a></li>
<li>paper_authors: Chenwei Zhang, Jordan Lovrod, Boyan Beronov, Khanh Dao Duc, Anne Condon</li>
<li>For: 这个论文是为了帮助生物化学家和分子编程师理解核酸反应的复杂激发路径，并可以用于多种应用。* Methods: 该论文使用了一种名为 kontinuous-time Markov chain（CTMC）的模型，并使用了一种新的视觉化方法called ViDa，以Visualize DNA reaction trajectories的二维嵌入。* Results: 该论文的结果表明，使用域Specific supervised terms可以提高visualization的质量，并成功分离不同的folding pathways，提供了有用的反应机理的启示。<details>
<summary>Abstract</summary>
Visualization tools can help synthetic biologists and molecular programmers understand the complex reactive pathways of nucleic acid reactions, which can be designed for many potential applications and can be modelled using a continuous-time Markov chain (CTMC). Here we present ViDa, a new visualization approach for DNA reaction trajectories that uses a 2D embedding of the secondary structure state space underlying the CTMC model. To this end, we integrate a scattering transform of the secondary structure adjacency, a variational autoencoder, and a nonlinear dimensionality reduction method. We augment the training loss with domain-specific supervised terms that capture both thermodynamic and kinetic features. We assess ViDa on two well-studied DNA hybridization reactions. Our results demonstrate that the domain-specific features lead to significant quality improvements over the state-of-the-art in DNA state space visualization, successfully separating different folding pathways and thus providing useful insights into dominant reaction mechanisms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visualization 工具可以帮助 sintetic biology 和分子程序员理解聚合物reactions的复杂reacting pathways，这些reactions可以设计为多种可能性，并且可以使用连续时间Markov链（CTMC）来模型。在这里，我们介绍了一种新的可见化方法，即 ViDa，它使用CTMC模型下的secondary structure状态空间的2D嵌入来可见化DNA反应轨迹。为此，我们将scattering transform of secondary structure adjacency、variational autoencoder和非线性维度减少方法相互融合。我们还添加了域pecific的supervised terms，以捕捉thermodynamic和kinetic特征。我们对两个已经广泛研究的DNA杂化反应进行评估。我们的结果表明，域pecific特征导致了state space可见化中的质量提升，成功地分离不同的folding pathways，从而提供了关键的反应机理的视角。Note: Simplified Chinese is used here, which is a more casual and informal style of Chinese. If you prefer Traditional Chinese or a more formal style, please let me know and I can translate it accordingly.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base"><a href="#Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base" class="headerlink" title="Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base"></a>Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02884">http://arxiv.org/abs/2311.02884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yi, Yang Cao, Xin Kang, Ying-Chang Liang</li>
<li>for: 提高6G网络中semantic communication系统的可解释性。</li>
<li>methods: 提出一种基于深度学习的semantic communication系统，利用共享知识库提高系统的可解释性。</li>
<li>results: 对比基eline方法，提出的方法可以减少数据传输量，同时保持语句相似性。Here’s a brief explanation of each point:</li>
<li>for: The paper aims to improve the explainability of semantic communication systems in future 6G networks.</li>
<li>methods: The proposed method uses a shared knowledge base to integrate messages and corresponding knowledge, enabling the system to transmit fewer symbols without sacrificing semantic performance.</li>
<li>results: The proposed approach outperforms existing baseline methods in terms of transmitted data size and sentence similarity, as demonstrated by simulation results.<details>
<summary>Abstract</summary>
Deep learning-empowered semantic communication is regarded as a promising candidate for future 6G networks. Although existing semantic communication systems have achieved superior performance compared to traditional methods, the end-to-end architecture adopted by most semantic communication systems is regarded as a black box, leading to the lack of explainability. To tackle this issue, in this paper, a novel semantic communication system with a shared knowledge base is proposed for text transmissions. Specifically, a textual knowledge base constructed by inherently readable sentences is introduced into our system. With the aid of the shared knowledge base, the proposed system integrates the message and corresponding knowledge from the shared knowledge base to obtain the residual information, which enables the system to transmit fewer symbols without semantic performance degradation. In order to make the proposed system more reliable, the semantic self-information and the source entropy are mathematically defined based on the knowledge base. Furthermore, the knowledge base construction algorithm is developed based on a similarity-comparison method, in which a pre-configured threshold can be leveraged to control the size of the knowledge base. Moreover, the simulation results have demonstrated that the proposed approach outperforms existing baseline methods in terms of transmitted data size and sentence similarity.
</details>
<details>
<summary>摘要</summary>
深度学习驱动的semantic通信被视为未来6G网络中的优秀候选人。虽然现有的semantic通信系统已经达到了传统方法的超越性，但大多数semantic通信系统的端到端架构被视为黑盒模型，导致无法解释性的问题。为解决这个问题，本文提出了一种基于文本传输的新的semantic通信系统。具体来说，我们提出了一个基于自然可读的句子构建的文本知识库。通过与共享知识库的集成，我们的系统可以通过获取剩余信息来传输更少的符号，而无需增加 semantic 性能下降。为使我们的系统更加可靠，我们在知识库中定义了semantic自信息和源 entropy。此外，我们还开发了基于相似比较方法的知识库构建算法，可以通过配置阈值来控制知识库的大小。最后，我们通过对比实验结果，证明了我们的方法可以比现有的基准方法更好地压缩数据和保持句子相似性。
</details></li>
</ul>
<hr>
<h2 id="DP-DCAN-Differentially-Private-Deep-Contrastive-Autoencoder-Network-for-Single-cell-Clustering"><a href="#DP-DCAN-Differentially-Private-Deep-Contrastive-Autoencoder-Network-for-Single-cell-Clustering" class="headerlink" title="DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering"></a>DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03410">http://arxiv.org/abs/2311.03410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huifa Li, Jie Fu, Zhili Chen, Xiaomin Yang, Haitao Liu, Xinpeng Ling</li>
<li>for: 本研究旨在提出一种基于深度学习的具有隐私保护特性的单元细胞 clustering 方法，以保护用户隐私。</li>
<li>methods: 该方法基于 autoencoder 网络，通过部分网络杂化来实现隐私保护。</li>
<li>results: 实验结果显示，DP-DCAN 比传统的DP方案具有更好的性能和更强的鲁棒性。In English, this translates to:</li>
<li>for: The paper aims to propose a deep learning-based single-cell clustering method with privacy protection, to protect user privacy.</li>
<li>methods: The method is based on an autoencoder network, and achieves privacy protection through partial network perturbation.</li>
<li>results: Experimental results show that DP-DCAN outperforms traditional DP schemes and has stronger robustness to adversarial attacks.<details>
<summary>Abstract</summary>
Single-cell RNA sequencing (scRNA-seq) is important to transcriptomic analysis of gene expression. Recently, deep learning has facilitated the analysis of high-dimensional single-cell data. Unfortunately, deep learning models may leak sensitive information about users. As a result, Differential Privacy (DP) is increasingly used to protect privacy. However, existing DP methods usually perturb whole neural networks to achieve differential privacy, and hence result in great performance overheads. To address this challenge, in this paper, we take advantage of the uniqueness of the autoencoder that it outputs only the dimension-reduced vector in the middle of the network, and design a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN) by partial network perturbation for single-cell clustering. Since only partial network is added with noise, the performance improvement is obvious and twofold: one part of network is trained with less noise due to a bigger privacy budget, and the other part is trained without any noise. Experimental results of six datasets have verified that DP-DCAN is superior to the traditional DP scheme with whole network perturbation. Moreover, DP-DCAN demonstrates strong robustness to adversarial attacks. The code is available at https://github.com/LFD-byte/DP-DCAN.
</details>
<details>
<summary>摘要</summary>
单元细胞RNAseq（scRNA-seq）对转录组分析表达物的研究具有重要意义。现在，深度学习技术已经使得高维单元细胞数据的分析变得更加容易。然而，深度学习模型可能泄露用户敏感信息，因此隐私保护成为了一项重要的问题。现有的隐私保护方法通常是整个神经网络的杂化，从而导致性能开销很大。为解决这个挑战，我们在这篇论文中利用自动encoder的独特性，即它只输出减少维度的向量，并设计了一种部分神经网络杂化的干扰隐私网络（DP-DCAN）。由于只有部分神经网络受到噪声杂化，性能提高是明显的两倍：一部分神经网络在噪声更小的情况下训练，另一部分则是没有噪声的训练。实验结果表明，DP-DCAN比传统的DP方案更加有优势，并且具有强大的鲁棒性。代码可以在https://github.com/LFD-byte/DP-DCAN上下载。
</details></li>
</ul>
<hr>
<h2 id="Visualizing-DNA-reaction-trajectories-with-deep-graph-embedding-approaches"><a href="#Visualizing-DNA-reaction-trajectories-with-deep-graph-embedding-approaches" class="headerlink" title="Visualizing DNA reaction trajectories with deep graph embedding approaches"></a>Visualizing DNA reaction trajectories with deep graph embedding approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03409">http://arxiv.org/abs/2311.03409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenwei-zhang/ViDa">https://github.com/chenwei-zhang/ViDa</a></li>
<li>paper_authors: Chenwei Zhang, Khanh Dao Duc, Anne Condon</li>
<li>for: 这个论文是为了设计新的聚合酶反应，以便在各种应用中使用。</li>
<li>methods: 这篇论文使用了深度图像嵌入模型和常见维度减少方法，将高维数据映射到2D的欧式空间中。</li>
<li>results: 我们的初步结果表明，ViDa可以成功地分离不同的折叠机制，从而为用户提供有用的信息，并且比现有的DNA动力学视化方法更好。<details>
<summary>Abstract</summary>
Synthetic biologists and molecular programmers design novel nucleic acid reactions, with many potential applications. Good visualization tools are needed to help domain experts make sense of the complex outputs of folding pathway simulations of such reactions. Here we present ViDa, a new approach for visualizing DNA reaction folding trajectories over the energy landscape of secondary structures. We integrate a deep graph embedding model with common dimensionality reduction approaches, to map high-dimensional data onto 2D Euclidean space. We assess ViDa on two well-studied and contrasting DNA hybridization reactions. Our preliminary results suggest that ViDa's visualization successfully separates trajectories with different folding mechanisms, thereby providing useful insight to users, and is a big improvement over the current state-of-the-art in DNA kinetics visualization.
</details>
<details>
<summary>摘要</summary>
生物化学家和分子程序员设计了新的核酸反应，有很多应用前景。为了帮助领域专家理解复杂的输出，我们需要一些好的可视化工具。我们现在提出了ViDa，一种新的方法用于可视化DNA反应折叠路径在二维空间中的能量阶段特征。我们将深度图嵌入模型与常见维度减少方法结合，将高维数据映射到二维欧氏空间中。我们对两种已经广泛研究和不同折叠机制的DNA协同反应进行了预liminary测试，结果表明ViDa的可视化成功地分离不同折叠机制的轨迹，为用户提供有用的信息，而且与当前DNA动力学可视化领域的状态 искусственный智能有很大改进。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection"><a href="#Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection" class="headerlink" title="Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection"></a>Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02863">http://arxiv.org/abs/2311.02863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Denkovski, Shehroz S. Khan, Alex Mihailidis</li>
<li>for: 预防跌倒lder adults中的伤害和死亡，精确的跌倒探测可以帮助降低这些风险。</li>
<li>methods: 使用 autoencoder 和其他相关的数据填充架构，进行跌倒探测。</li>
<li>results: 比较多种模型，发现 Temporal Shift 对于跌倒探测有着优秀的表现，尤其是在单一摄像头上，与传统的重建 alone 相比。<details>
<summary>Abstract</summary>
Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks' structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by 0.20 AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.
</details>
<details>
<summary>摘要</summary>
falls 是全球older adults中的主要导致伤害和死亡的原因之一。准确的落下检测可以帮助减少可能的伤害和额外的健康问题。家庭设置中可以使用RGB、Infrared和Thermal相机进行落下检测。使用自适应网络的异常检测框架可以用于落下检测，因为落下的数据异常性和多样性导致数据不匹配。在这篇论文中，我们提出了一种新的多目标损失函数，即时间偏移，以预测序列帧中的未来帧和重建帧。我们的提案的损失函数被评估在具有多个相机模式的半自然的落下检测数据集上。我们使用了正常老年人进行日常活动的学习，并在老年人和年轻人之间进行测试。时间偏移显示在不同模型中具有显著改进，特别是使用注意力U-Net CAE模型，其在单个相机上提高了0.20 AUC ROC。在不同的模型中，这种方法有广泛的应用前景，可以提高异常检测Capability在其他设置中。
</details></li>
</ul>
<hr>
<h2 id="Training-Multi-layer-Neural-Networks-on-Ising-Machine"><a href="#Training-Multi-layer-Neural-Networks-on-Ising-Machine" class="headerlink" title="Training Multi-layer Neural Networks on Ising Machine"></a>Training Multi-layer Neural Networks on Ising Machine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03408">http://arxiv.org/abs/2311.03408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xujie Song, Tong Liu, Shengbo Eben Li, Jingliang Duan, Wenxuan Wang, Keqiang Li</li>
<li>for: 这paper aimed to train multi-layer feedforward neural networks on Ising machines using an Ising learning algorithm.</li>
<li>methods: The algorithm incorporates two essential techniques: binary representation of topological network and order reduction of loss function. The QNN is formulated as a QCBO problem, which is then converted to a QUBO problem that can be efficiently solved on Ising machines.</li>
<li>results: The algorithm achieved a classification accuracy of 98.3% on MNIST dataset after annealing for 700 ms, with a success probability of 72% in finding the optimal solution. The algorithm has the potential to train deeper neural networks with more spins on the Ising machine.<details>
<summary>Abstract</summary>
As a dedicated quantum device, Ising machines could solve large-scale binary optimization problems in milliseconds. There is emerging interest in utilizing Ising machines to train feedforward neural networks due to the prosperity of generative artificial intelligence. However, existing methods can only train single-layer feedforward networks because of the complex nonlinear network topology. This paper proposes an Ising learning algorithm to train quantized neural network (QNN), by incorporating two essential techinques, namely binary representation of topological network and order reduction of loss function. As far as we know, this is the first algorithm to train multi-layer feedforward networks on Ising machines, providing an alternative to gradient-based backpropagation. Firstly, training QNN is formulated as a quadratic constrained binary optimization (QCBO) problem by representing neuron connection and activation function as equality constraints. All quantized variables are encoded by binary bits based on binary encoding protocol. Secondly, QCBO is converted to a quadratic unconstrained binary optimization (QUBO) problem, that can be efficiently solved on Ising machines. The conversion leverages both penalty function and Rosenberg order reduction, who together eliminate equality constraints and reduce high-order loss function into a quadratic one. With some assumptions, theoretical analysis shows the space complexity of our algorithm is $\mathcal{O}(H^2L + HLN\log H)$, quantifying the required number of Ising spins. Finally, the algorithm effectiveness is validated with a simulated Ising machine on MNIST dataset. After annealing 700 ms, the classification accuracy achieves 98.3%. Among 100 runs, the success probability of finding the optimal solution is 72%. Along with the increasing number of spins on Ising machine, our algorithm has the potential to train deeper neural networks.
</details>
<details>
<summary>摘要</summary>
As a dedicated quantum device, 积极机器（Ising machine）可以在毫秒内解决大规模的二进制优化问题。因为生成型人工智能的发展，有越来越多的人对使用积极机器来训练Feedforward Neural Network（FFNN）感兴趣。然而，现有的方法只能训练单层FFNN，因为积极机器的非线性网络架构造问题。本文提出了一个积极学习算法，用于训练量化神经网络（QNN），通过结合两种重要技术：一是二进制表示网络的拓扑网络，二是排序缩减损失函数的技术。根据我们所知，这是第一个可以在积极机器上训练多层FFNN的算法，提供了一个alternative的方法。首先，训练QNN是转化为二进制受控制的问题（QCBO），通过表示神经连接和活化函数为等式约束。所有量化变数都是基于二进制编码协议编码。其次，QCBO被转换为二进制不约束的问题（QUBO），可以高效地解决在积极机器上。转换是通过 penalty function和Rosenberg排序缩减技术，将等式约束和高阶损失函数转换为二进制问题。假设时，我们进行了一些假设，实际分析显示算法的空间复杂度为 $\mathcal{O}(H^2L + HLN\log H)$，这个结果表明了我们需要多少积极转换的磁矩。最后，我们验证了我们的算法在MNIST dataset上的效果，通过氧化700毫秒后，分类精度达到98.3%。在100次实验中，成功率为72%。随着积极转换的磁矩增加，我们的算法有可能训练更深的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models"><a href="#Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models" class="headerlink" title="Co-training and Co-distillation for Quality Improvement and Compression of Language Models"></a>Co-training and Co-distillation for Quality Improvement and Compression of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02849">http://arxiv.org/abs/2311.02849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min</li>
<li>for: 减少计算成本的预训练语言模型（PLM）的压缩，以便在资源有限或实时设置下使用。</li>
<li>methods: 基于两个模型同时受训和互相知识传递的框架，即Co-Training and Co-Distillation（CTCD）。</li>
<li>results: CTCD框架可以同时提高性能和执行速度，并且可以与现有的技术相结合，如建筑设计或数据增强，以实现更高的性能提升。小模型通过CTCD框架进行传递学习，可以超越原始大模型的性能。<details>
<summary>Abstract</summary>
Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and inference speed together by co-training two models while mutually distilling knowledge. The CTCD framework successfully achieves this based on two significant findings: 1) Distilling knowledge from the smaller model to the larger model during co-training improves the performance of the larger model. 2) The enhanced performance of the larger model further boosts the performance of the smaller model. The CTCD framework shows promise as it can be combined with existing techniques like architecture design or data augmentation, replacing one-way KD methods, to achieve further performance improvement. Extensive ablation studies demonstrate the effectiveness of CTCD, and the small model distilled by CTCD outperforms the original larger model by a significant margin of 1.66 on the GLUE benchmark.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）可以压缩 computationally expensive 预训练语言模型（PLMs），将它们的知识传递给更小的模型，使其在资源受限或实时设置中使用。然而，大多数更小的模型无法超越原始更大的模型的性能， resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and inference speed together by co-training two models while mutually distilling knowledge. The CTCD framework successfully achieves this based on two significant findings:1. 压缩知识从更小的模型到更大的模型 durante co-training 可以提高更大模型的性能。2. 更大模型的性能的提高可以再次提高更小模型的性能。CTCD框架显示出了其可以与现有的技术，如建筑设计或数据扩展，结合使用，代替一个方向的 KD 方法，以达到更高的性能改进。广泛的抑制研究表明 CTCD 的效果，并且使用 CTCD 塑化的小模型在 GLUE benchmark 上超过原始更大模型的性能表现。
</details></li>
</ul>
<hr>
<h2 id="Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs"><a href="#Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs" class="headerlink" title="Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs"></a>Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02847">http://arxiv.org/abs/2311.02847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gewu-lab/llm_articulated_object_manipulation">https://github.com/gewu-lab/llm_articulated_object_manipulation</a></li>
<li>paper_authors: Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu</li>
<li>for: 这个论文旨在提高家用助手机器人的通用适应性，使其能够在各种不同的链接物上进行有效的操作。</li>
<li>methods: 该论文提出了一种基于语言模型的努力学习框架，通过提供物体的骨骼知识来帮助语言模型生成低级别的运动轨迹点。</li>
<li>results: 研究表明，该框架不仅在8种已经见过的类型上比传统方法表现出色，而且在8种未见过的类型上也有强大的零shot能力。此外，在实际场景中对7种不同的物体进行了实验，证明了该框架的实用性。<details>
<summary>Abstract</summary>
Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at \href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main}{here}.
</details>
<details>
<summary>摘要</summary>
通用的链接物体操作是家庭助手机器人的必备能力。最近的努力主要集中在示范学习或者在模拟环境中使用奖励学习，但由于实际世界数据收集和精准的物体模拟成本 prohibitively expensive，这些工作还未能实现广泛的适应性。近些年，许多研究尝试使用大语言模型（LLM）的强Context Learning能力来实现通用的机器人操作，但大多数研究都集中在高级任务规划，忽略低级机器人控制。在这项工作中，我们建立了基于物体链接结构的Prompting框架，通过提示LLMs with 链接知识来生成低级运动轨迹点。为了有效地提示LLMs链接结构的不同物体，我们设计了一个统一的链接知识解析器，该解析器将各种链接物体表示为一个统一的文本描述，包括链接 JOINTS 和 Contact Location。基于这个统一描述，我们提出了一种基于链接结构的逻辑链式思维Prompting方法，以生成精确的3D操作轨迹点。我们的评估涵盖了48个实例，8种已看到类和8种未看到类，结果显示，我们的框架不仅在8种已看到类上超越传统方法，还在0shot情况下展现出强大的适应能力。此外，我们在7种实际物体类型上进行了实际实验，证明了我们的框架在实际场景中的适应性。代码可以在 \href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main}{这里} 找到。
</details></li>
</ul>
<hr>
<h2 id="Saturn-Efficient-Multi-Large-Model-Deep-Learning"><a href="#Saturn-Efficient-Multi-Large-Model-Deep-Learning" class="headerlink" title="Saturn: Efficient Multi-Large-Model Deep Learning"></a>Saturn: Efficient Multi-Large-Model Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02840">http://arxiv.org/abs/2311.02840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Arun Kumar</li>
<li>for: 提高多大型模型训练效率 (improve the efficiency of multi-large-model training)</li>
<li>methods: 提出了一种新的数据系统，即Saturn，用于解决在多大型模型训练中的三个关键问题：并行技术选择、GPU分配和调度。 (propose a new data system called Saturn to solve three key interconnected systems challenges for users building large models in this setting)</li>
<li>results: 比较研究显示，Saturn 的共优化方法可以提高模型选择运行时间的效率，比现今常见深度学习实践下降39-49%。 (evaluations show that the joint-optimization approach of Saturn can improve the efficiency of model selection runtimes, reducing them by 39-49% compared to typical current deep learning practices)<details>
<summary>Abstract</summary>
In this paper, we propose Saturn, a new data system to improve the efficiency of multi-large-model training (e.g., during model selection/hyperparameter optimization). We first identify three key interconnected systems challenges for users building large models in this setting -- parallelism technique selection, distribution of GPUs over jobs, and scheduling. We then formalize these as a joint problem, and build a new system architecture to tackle these challenges simultaneously. Our evaluations show that our joint-optimization approach yields 39-49% lower model selection runtimes than typical current DL practice.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的数据系统，用于提高多大型模型训练的效率（例如， durante model selection/超参数优化）。我们首先认为有三个关联的系统挑战，用户在建立大型模型时面临——并行技术选择、GPU分配到任务以及调度。我们然后将这些问题形式化为一个共同问题，并构建了一个新的系统架构，以同时解决这些挑战。我们的评估结果表明，我们的联合优化方法可以提供39-49%比现今深度学习实践的模型选择运行时间更低。
</details></li>
</ul>
<hr>
<h2 id="Mesh-Neural-Cellular-Automata"><a href="#Mesh-Neural-Cellular-Automata" class="headerlink" title="Mesh Neural Cellular Automata"></a>Mesh Neural Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02820">http://arxiv.org/abs/2311.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, Sabine Süsstrunk</li>
<li>for: 提高虚拟环境的实际感 (enhancing the realism of virtual environments)</li>
<li>methods: 直接synthesize 3D纹理 (directly synthesize 3D textures) without UV mapping</li>
<li>results: 可以在实时中synthesize 3D纹理 (can synthesize 3D textures in real time) on any mesh, with generalization and multi-modal supervision capabilities.<details>
<summary>Abstract</summary>
Modeling and synthesizing textures are essential for enhancing the realism of virtual environments. Methods that directly synthesize textures in 3D offer distinct advantages to the UV-mapping-based methods as they can create seamless textures and align more closely with the ways textures form in nature. We propose Mesh Neural Cellular Automata (MeshNCA), a method for directly synthesizing dynamic textures on 3D meshes without requiring any UV maps. MeshNCA is a generalized type of cellular automata that can operate on a set of cells arranged on a non-grid structure such as vertices of a 3D mesh. While only being trained on an Icosphere mesh, MeshNCA shows remarkable generalization and can synthesize textures on any mesh in real time after the training. Additionally, it accommodates multi-modal supervision and can be trained using different targets such as images, text prompts, and motion vector fields. Moreover, we conceptualize a way of grafting trained MeshNCA instances, enabling texture interpolation. Our MeshNCA model enables real-time 3D texture synthesis on meshes and allows several user interactions including texture density/orientation control, a grafting brush, and motion speed/direction control. Finally, we implement the forward pass of our MeshNCA model using the WebGL shading language and showcase our trained models in an online interactive demo which is accessible on personal computers and smartphones. Our demo and the high resolution version of this PDF are available at https://meshnca.github.io/.
</details>
<details>
<summary>摘要</summary>
模型和 sintesizing  texture 是虚拟环境的重要组成部分。直接 sintesizing  texture 在 3D 提供了明显的优势，因为它们可以创建无缝 texture 并更好地遵循自然界中 texture 的形成方式。我们提出了 Mesh Neural Cellular Automata (MeshNCA)，一种直接 sintesizing  dynamic texture 的方法，无需 UV 映射。MeshNCA 是一种通用的细胞自动机，可以在 3D 网格结构上的细胞集上进行操作。它只需在 icosphere 网格上训练，但它可以在实时中 sintesize texture 于任何网格。此外，它可以接受多modal 监督和使用不同的目标，如图像、文本提示和运动向量场。此外，我们还提出了将训练好的 MeshNCA 实例结合的思想，以实现 texture  interpolate。我们的 MeshNCA 模型允许在 mesh 上实时 sintesize texture，并提供了许多用户交互，包括 texture 密度/方向控制、grafting 毛刷、速度/方向控制。最后，我们使用 WebGL 渲染语言实现了我们的 MeshNCA 模型的前向传播，并在个人电脑和手机上展示了我们训练好的模型。我们的 demo 和高分辨率版PDF 可以在 https://meshnca.github.io/ 上获取。
</details></li>
</ul>
<hr>
<h2 id="QualEval-Qualitative-Evaluation-for-Model-Improvement"><a href="#QualEval-Qualitative-Evaluation-for-Model-Improvement" class="headerlink" title="QualEval: Qualitative Evaluation for Model Improvement"></a>QualEval: Qualitative Evaluation for Model Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02807">http://arxiv.org/abs/2311.02807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vmurahari3/qualeval">https://github.com/vmurahari3/qualeval</a></li>
<li>paper_authors: Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, Ashwin Kalyan<br>for: 这种研究旨在提高人工智能系统中的大语言模型（LLM）评估方法，并提供一种自动化质量评估方法来加速模型改进。methods: 该研究使用了一种强大的语言理解器和一种新的灵活线性Programming solver来生成人类可读的报告，以提供model improvement的数据科学家。results: 研究表明，通过使用QualEval的报告，可以提高Llama 2模型在对话任务（DialogSum）中的绝对性能，相比基线方案，提高15%点。QualEval成功地加速了模型开发的pace，因此可以视为一个数据科学家在盒子中。<details>
<summary>Abstract</summary>
Quantitative evaluation metrics have traditionally been pivotal in gauging the advancements of artificial intelligence systems, including large language models (LLMs). However, these metrics have inherent limitations. Given the intricate nature of real-world tasks, a single scalar to quantify and compare is insufficient to capture the fine-grained nuances of model behavior. Metrics serve only as a way to compare and benchmark models, and do not yield actionable diagnostics, thus making the model improvement process challenging. Model developers find themselves amid extensive manual efforts involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which augments quantitative scalar metrics with automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are backed by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace of model development, thus in essence serving as a data-scientist-in-a-box. Given the focus on critiquing and improving current evaluation metrics, our method serves as a refreshingly new technique for both model evaluation and improvement.
</details>
<details>
<summary>摘要</summary>
To address these limitations, we propose QualEval, which combines automated qualitative evaluation with quantitative scalar metrics to facilitate model improvement. QualEval utilizes a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that can be applied to accelerate model improvement. These insights are supported by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses.We demonstrate the effectiveness of QualEval by showing that it can improve the absolute performance of the Llama 2 model by up to 15% points relative to baselines on a challenging dialogue task (DialogSum). By increasing the pace of model development, QualEval serves as a data-scientist-in-a-box, providing a refreshingly new technique for both model evaluation and improvement. Given the focus on critiquing and improving current evaluation metrics, our method offers a much-needed alternative to traditional evaluation methods.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP"><a href="#Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP" class="headerlink" title="Incorporating Worker Perspectives into MTurk Annotation Practices for NLP"></a>Incorporating Worker Perspectives into MTurk Annotation Practices for NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02802">http://arxiv.org/abs/2311.02802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia Huang, Eve Fleisig, Dan Klein</li>
<li>for: 本研究旨在改进当前在 Amazon Mechanical Turk (MTurk) 上进行自然语言处理数据收集的现行实践，以提高数据质量和考虑工作者权益。</li>
<li>methods: 本研究采用了 kritische 文献综述和MTurk工作者问卷，以解决关于最佳实践、公平支付、工作者隐私、数据质量和工作者奖励的开问。</li>
<li>results: 调查结果表明，工作者偏好可靠、合理的支付，而不是不确定、非常高的支付；报告经常谎报个人信息问题；表达对无解释工作拒绝的沮丧。此外，工作者认为一些质量控制方法，如需要最低响应时间或硬件资格要求，是偏袋和效果不佳。根据调查结果，本研究提出了将来的 NLP 研究如何更好地考虑MTurk工作者的经验，以尊重工作者权益并提高数据质量。<details>
<summary>Abstract</summary>
Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers' rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master's qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers' experiences in order to respect workers' rights and improve data quality.
</details>
<details>
<summary>摘要</summary>
现有的MTurk数据收集做法 frequently rely on combination of studies on data quality和 shared heuristics among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers' rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master's qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers' experiences in order to respect workers' rights and improve data quality.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.AI_2023_11_06/" data-id="closbrol9006v0g88ckmp595w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CL_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T11:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.CL_2023_11_06/">cs.CL - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="STONYBOOK-A-System-and-Resource-for-Large-Scale-Analysis-of-Novels"><a href="#STONYBOOK-A-System-and-Resource-for-Large-Scale-Analysis-of-Novels" class="headerlink" title="STONYBOOK: A System and Resource for Large-Scale Analysis of Novels"></a>STONYBOOK: A System and Resource for Large-Scale Analysis of Novels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03614">http://arxiv.org/abs/2311.03614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charuta Pethe, Allen Kim, Rajesh Prabhakar, Tanzir Pial, Steven Skiena</li>
<li>for: 这个论文是为了提供一种大规模分析小说的资源，包括一个开源的终端到终端NLP分析管道，以及49,207本清洁和注释过的小说集。</li>
<li>methods: 这个论文使用的方法包括开发了一个标准XML格式来注释小说，以及建立了一个大规模文本分析数据库和网页界面。</li>
<li>results: 论文提供了各种分析 artifacts，包括人物出现和互动的视觉化、相似的书籍、代表词汇、部首统计和阅读指标。这些结果可以用于质量和kvantitativer逻辑分析大量的小说 Corpora。<details>
<summary>Abstract</summary>
Books have historically been the primary mechanism through which narratives are transmitted. We have developed a collection of resources for the large-scale analysis of novels, including: (1) an open source end-to-end NLP analysis pipeline for the annotation of novels into a standard XML format, (2) a collection of 49,207 distinct cleaned and annotated novels, and (3) a database with an associated web interface for the large-scale aggregate analysis of these literary works. We describe the major functionalities provided in the annotation system along with their utilities. We present samples of analysis artifacts from our website, such as visualizations of character occurrences and interactions, similar books, representative vocabulary, part of speech statistics, and readability metrics. We also describe the use of the annotated format in qualitative and quantitative analysis across large corpora of novels.
</details>
<details>
<summary>摘要</summary>
书籍traditionally have been the primary means through which narratives are transmitted. We have developed a collection of resources for the large-scale analysis of novels, including: (1) an open-source end-to-end NLP analysis pipeline for the annotation of novels into a standard XML format, (2) a collection of 49,207 distinct cleaned and annotated novels, and (3) a database with an associated web interface for the large-scale aggregate analysis of these literary works. We describe the major functionalities provided in the annotation system along with their utilities. We present samples of analysis artifacts from our website, such as visualizations of character occurrences and interactions, similar books, representative vocabulary, part of speech statistics, and readability metrics. We also describe the use of the annotated format in qualitative and quantitative analysis across large corpora of novels.Here's a word-for-word translation of the text using Traditional Chinese characters:书籍传统上是传递narra的主要途径。我们已经发展了一组资源来进行大规模的小说分析，包括：（1）一个开源的端到端NLP分析管线来标注小说成standard XML格式，（2）一个包含49,207个精心整理和标注的小说集，以及（3）一个对大量文本进行聚合分析的数据库和网页交互面。我们详细介绍了标注系统的主要功能和其价值。我们将从我们的网站上提供的分析遗存中展示一些分析成果，如人物出现和互动的分析图表、相似的书籍、常用词汇、parts of speech的统计和阅读度量。我们还详细介绍了使用标注格式进行质量和量itative分析的优点。
</details></li>
</ul>
<hr>
<h2 id="Dimensions-of-Online-Conflict-Towards-Modeling-Agonism"><a href="#Dimensions-of-Online-Conflict-Towards-Modeling-Agonism" class="headerlink" title="Dimensions of Online Conflict: Towards Modeling Agonism"></a>Dimensions of Online Conflict: Towards Modeling Agonism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03584">http://arxiv.org/abs/2311.03584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Canute, Mali Jin, hannah holtzclaw, Alberto Lusoli, Philippa R Adams, Mugdha Pandya, Maite Taboada, Diana Maynard, Wendy Hui Kyong Chun</li>
<li>for: 这个论文主要研究了在社交媒体上的对话中的对抗关系，以及这种对抗关系如何影响对话质量。</li>
<li>methods: 作者使用了Twitter上的争议话题来收集对话，并开发了一个完整的注释标准来标记对话中的不同级别的对抗关系。然后，他们使用了逻辑回归和变换器模型来训练模型，并在模型中包含了对话中的上下文信息，如参与者数量和互动结构。</li>
<li>results: 研究结果表明，Contextual labels可以帮助确定对抗关系，并使模型在话题变化时保持稳定性。这些结果可以为内容审核和社交媒体平台管理做出贡献。<details>
<summary>Abstract</summary>
Agonism plays a vital role in democratic dialogue by fostering diverse perspectives and robust discussions. Within the realm of online conflict there is another type: hateful antagonism, which undermines constructive dialogue. Detecting conflict online is central to platform moderation and monetization. It is also vital for democratic dialogue, but only when it takes the form of agonism. To model these two types of conflict, we collected Twitter conversations related to trending controversial topics. We introduce a comprehensive annotation schema for labelling different dimensions of conflict in the conversations, such as the source of conflict, the target, and the rhetorical strategies deployed. Using this schema, we annotated approximately 4,000 conversations with multiple labels. We then trained both logistic regression and transformer-based models on the dataset, incorporating context from the conversation, including the number of participants and the structure of the interactions. Results show that contextual labels are helpful in identifying conflict and make the models robust to variations in topic. Our research contributes a conceptualization of different dimensions of conflict, a richly annotated dataset, and promising results that can contribute to content moderation.
</details>
<details>
<summary>摘要</summary>
争议 играет重要的角色在民主对话中，推动多元观点和坚定的讨论。在在线冲突中，另外一种类型是恶意争议，这会阻碍有益的对话。检测在线冲突是民主对话中不可或缺的，只有当它变成争议时。为了模型这两种冲突，我们收集了关于热门争议话题的推特对话。我们介绍了对话中不同维度的争议的完整标注schema，例如争议的来源、目标和使用的修辞技巧。使用这个schema，我们对约4000个对话进行了多个标注。然后我们训练了逻辑回归和转换器基于模型，使用对话中的上下文，包括参与者人数和互动结构。结果表明，上下文标注有助于确定冲突，使模型具有话题变化的 robustness。我们的研究对于不同维度的争议做出了概念化、富有标注数据和成功的实验成果，可以贡献于内容审核。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Adversarial-Datasets"><a href="#Measuring-Adversarial-Datasets" class="headerlink" title="Measuring Adversarial Datasets"></a>Measuring Adversarial Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03566">http://arxiv.org/abs/2311.03566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kritwik1/Detection-of-Anomalies-in-Images-using-Adversarial-learning">https://github.com/kritwik1/Detection-of-Anomalies-in-Images-using-Adversarial-learning</a></li>
<li>paper_authors: Yuanchen Bai, Raoyi Huang, Vijay Viswanathan, Tzu-Sheng Kuo, Tongshuang Wu</li>
<li>for: 这个研究的目的是为了探讨现有的量化度量是否能够捕捉NLP任务中文本实例的难度、多样性和分歧。</li>
<li>methods: 这个研究使用了现有的敌对性例数据集，并对这些数据集和原始数据集进行比较，以了解这些敌对性例的分布是否与假设一致。</li>
<li>results: 研究发现，现有的量化度量可以很好地捕捉敌对性例的难度和多样性，但是它们可能不能够捕捉敌对性例的分歧。这些结果提供了valuable的信息，可以帮助研究人员更好地理解敌对性例的特点和假设。<details>
<summary>Abstract</summary>
In the era of widespread public use of AI systems across various domains, ensuring adversarial robustness has become increasingly vital to maintain safety and prevent undesirable errors. Researchers have curated various adversarial datasets (through perturbations) for capturing model deficiencies that cannot be revealed in standard benchmark datasets. However, little is known about how these adversarial examples differ from the original data points, and there is still no methodology to measure the intended and unintended consequences of those adversarial transformations. In this research, we conducted a systematic survey of existing quantifiable metrics that describe text instances in NLP tasks, among dimensions of difficulty, diversity, and disagreement. We selected several current adversarial effect datasets and compared the distributions between the original and their adversarial counterparts. The results provide valuable insights into what makes these datasets more challenging from a metrics perspective and whether they align with underlying assumptions.
</details>
<details>
<summary>摘要</summary>
在人工智能系统广泛应用于多个领域的时代，保证对抗Robustness已成为维护安全和避免不良错误的关键。研究人员通过干扰损害数据集（through perturbations）捕捉模型缺陷，这些缺陷无法在标准测试数据集中显示出来。然而，对这些对抗示例与原始数据点之间的差异还是不够了解，而且还没有一种方法来衡量这些对抗变换的意图和不意图的后果。在这项研究中，我们进行了系统性的量化度量研究，探讨了存在于NLP任务中的文本实例度量，包括难度、多样性和分歧。我们选择了一些当前的对抗效果数据集，并比较了这些数据集的分布与其对抗对应的分布。结果提供了有价值的洞察，了解这些数据集在量化度量上的挑战和是否与下面的假设相符。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Natural-Language-Explanations-of-Large-Language-Models"><a href="#Quantifying-Uncertainty-in-Natural-Language-Explanations-of-Large-Language-Models" class="headerlink" title="Quantifying Uncertainty in Natural Language Explanations of Large Language Models"></a>Quantifying Uncertainty in Natural Language Explanations of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03533">http://arxiv.org/abs/2311.03533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sree Harsha Tanneru, Chirag Agarwal, Himabindu Lakkaraju</li>
<li>for: 本研究旨在量化LLM的解释uncertainty。</li>
<li>methods: 我们提出了两个新的度量方法：Verbalized Uncertainty和Probing Uncertainty，以量化LLM的解释uncertainty。</li>
<li>results: 我们的实验表明，Verbalized Uncertainty不是一个可靠的解释 confidence 度量方法，而Probing Uncertainty度量与解释 faithfulness 呈正相关。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Dialogue-System-for-Medical-Prescription-Acquisition-on-Smartphone-Development-Corpus-and-Evaluation"><a href="#Spoken-Dialogue-System-for-Medical-Prescription-Acquisition-on-Smartphone-Development-Corpus-and-Evaluation" class="headerlink" title="Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation"></a>Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03510">http://arxiv.org/abs/2311.03510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Can Kocabiyikoglu, François Portet, Jean-Marc Babouchkine, Prudence Gibert, Hervé Blanchon, Gaëtan Gavazzi</li>
<li>for: 这篇论文是关于医疗信息系统（HIS）中的电子药物预scribing软件，它提供了一种使用自然语言对话系统来记录药物预scription的方法。</li>
<li>methods: 这篇论文使用了对话模型、语义提取和数据增强等技术来开发一种基于自然语言对话的药物预scription系统。</li>
<li>results: 论文中提出的系统在实际应用中被评估，结果显示该系统可以减少医生在计算机上输入信息的时间，同时提高预cription的正确率和效率。试验中的55名参与者中，医生的均值预cription时间为66.15秒，其他专家的均值预cription时间为35.64秒，任务成功率为76%和72%。试验数据被记录和标注，并形成了PxCorpus，全面提供给社区（<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.6524162%EF%BC%89%E3%80%82">https://doi.org/10.5281/zenodo.6524162）。</a><details>
<summary>Abstract</summary>
Hospital information systems (HIS) have become an essential part of healthcare institutions and now incorporate prescribing support software. Prescription support software allows for structured information capture, which improves the safety, appropriateness and efficiency of prescriptions and reduces the number of adverse drug events (ADEs). However, such a system increases the amount of time physicians spend at a computer entering information instead of providing medical care. In addition, any new visiting clinician must learn to manage complex interfaces since each HIS has its own interfaces. In this paper, we present a natural language interface for e-prescribing software in the form of a spoken dialogue system accessible on a smartphone. This system allows prescribers to record their prescriptions verbally, a form of interaction closer to their usual practice. The system extracts the formal representation of the prescription ready to be checked by the prescribing software and uses the dialogue to request mandatory information, correct errors or warn of particular situations. Since, to the best of our knowledge, there is no existing voice-based prescription dialogue system, we present the system developed in a low-resource environment, focusing on dialogue modeling, semantic extraction and data augmentation. The system was evaluated in the wild with 55 participants. This evaluation showed that our system has an average prescription time of 66.15 seconds for physicians and 35.64 seconds for other experts, and a task success rate of 76\% for physicians and 72\% for other experts. All evaluation data were recorded and annotated to form PxCorpus, the first spoken drug prescription corpus that has been made fully available to the community (\url{https://doi.org/10.5281/zenodo.6524162}).
</details>
<details>
<summary>摘要</summary>
医院信息系统（HIS）已成为医疗机构的重要组成部分，并包括订药支持软件。订药支持软件可以结构化信息捕获，从而提高药物订药的安全性、适用性和效率，并减少药物相关事件（ADEs）的发生。然而，这种系统会使医生在计算机上输入信息的时间增加，而不是提供医疗服务。此外，每个医院信息系统都有自己的界面，新来的医生必须学习这些复杂的界面。在本文中，我们提出了一种基于自然语言对话的订药软件，通过智能手机上的对话系统来记录医生的订药。这种系统使医生可以通过口头记录订药，与其常见的医疗做法更相似。系统会提取订药的正式表示形式，并使用对话来请求必要的信息、修正错误或警告特定情况。由于我们知道的 voz-based 订药对话系统并不存在，我们在具有较低资源环境下开发了这个系统，重点是对话模型、semantic extraction 和数据增强。我们在野化进行了55名参与者的评估，评估结果显示，我们的系统的医生平均订药时间为66.15秒，其他专家平均订药时间为35.64秒，任务成功率为76% 和72%。所有评估数据都被记录并标注，以形成 PxCorpus，是首个全面向社区公开的 spoken drug prescription corpus（https://doi.org/10.5281/zenodo.6524162）。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Exemplars-as-Clues-to-Retrieving-from-Large-Associative-Memory"><a href="#In-Context-Exemplars-as-Clues-to-Retrieving-from-Large-Associative-Memory" class="headerlink" title="In-Context Exemplars as Clues to Retrieving from Large Associative Memory"></a>In-Context Exemplars as Clues to Retrieving from Large Associative Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03498">http://arxiv.org/abs/2311.03498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andotalao24/ICL-as-retrieval-from-associative-memory">https://github.com/andotalao24/ICL-as-retrieval-from-associative-memory</a></li>
<li>paper_authors: Jiachen Zhao</li>
<li>for: 本研究旨在探讨大语言模型（LLM）中的卷积学习（ICL）能力，以及如何选择示例的问题。</li>
<li>methods: 本研究使用了聚合网络来建立ICL的理论基础，并对示例的选择进行了实验研究。</li>
<li>results: 研究发现，ICL的性能与示例的选择有直接的关系，并提出了更有效的活动示例选择方法。这些发现可能有助于更深入理解LLM的含义和工作机制。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) have made remarkable progress in natural language processing. The most representative ability of LLMs is in-context learning (ICL), which enables LLMs to learn patterns from in-context exemplars without training. The performance of ICL greatly depends on the exemplars used. However, how to choose exemplars remains unclear due to the lack of understanding of how in-context learning works. In this paper, we present a novel perspective on ICL by conceptualizing it as contextual retrieval from a model of associative memory. We establish a theoretical framework of ICL based on Hopfield Networks. Based on our framework, we look into how in-context exemplars influence the performance of ICL and propose more efficient active exemplar selection. Our study sheds new light on the mechanism of ICL by connecting it to memory retrieval, with potential implications for advancing the understanding of LLMs.
</details>
<details>
<summary>摘要</summary>
（简化中文）最近，大型自然语言处理模型（LLM）在自然语言处理领域取得了非常出色的进步。LLM的最主要能力是在 контексте学习（ICL），即在不需要训练的情况下，模型可以从 контексте中学习模式。ICL的性能很大程度上取决于使用的 exemplars。然而，如何选择 exemplars 仍然不清楚，因为lack of understanding of how in-context learning works。在这篇论文中，我们提出了一种新的思路，即认为ICL可以视为一种contextual retrieval from a model of associative memory。我们建立了一个基于Hopfield Networks的ICL理论框架。基于我们的框架，我们研究了ICL中 exemplars 的影响和更有效的活动 exemplar 选择。我们的研究 shed new light on ICL的机制，并可能有助于进一步理解 LLMs。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling"><a href="#Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling" class="headerlink" title="Tackling Concept Shift in Text Classification using Entailment-style Modeling"></a>Tackling Concept Shift in Text Classification using Entailment-style Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03320">http://arxiv.org/abs/2311.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy, Alok Chandra</li>
<li>for:  Handle concept shift in text classification tasks with less labeling data.</li>
<li>methods:  Reformulate vanilla classification as an entailment-style problem, requiring less data to adapt to new concepts.</li>
<li>results:  Achieve absolute F1 gains of up to 7% and 40% in few-shot settings on real-world and synthetic datasets, respectively, with 75% labeling cost savings overall.<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as Concept Shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world & synthetic datasets achieving absolute F1 gains upto 7% and 40% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75% of labeling costs overall.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance"><a href="#Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance" class="headerlink" title="Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance"></a>Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03311">http://arxiv.org/abs/2311.03311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/epfl-ml4ed/unraveling-llm-bias">https://github.com/epfl-ml4ed/unraveling-llm-bias</a></li>
<li>paper_authors: Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja Käser</li>
<li>for: 这 paper 探讨了 AI 写作支持系统 中 inherent bias 的影响。</li>
<li>methods: 该 paper 使用了大量的 user study 和不同类型的模型来检测 bias。</li>
<li>results: 研究发现，在 AI 写作支持系统 中，bias 不会传递到学生的回答中。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user study with 231 students writing business case peer reviews in German. Students are divided into five groups with different levels of writing support: one classroom group with feature-based suggestions and four groups recruited from Prolific -- a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using GenBit gender bias analysis, Word Embedding Association Tests (WEAT), and Sentence Embedding Association Test (SEAT) we evaluate the gender bias at various stages of the pipeline: in model embeddings, in suggestions generated by the models, and in reviews written by students. Our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在教育任务中越来越受到应用，例如为学生提供写作建议。despite their potential，LLMs are known to harbor inherent biases which may negatively impact learners. previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. in this paper, we investigate how bias transfers through an AI writing support pipeline. we conduct a large-scale user study with 231 students writing business case peer reviews in German. students are divided into five groups with different levels of writing support: one classroom group with feature-based suggestions and four groups recruited from Prolific -- a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. using GenBit gender bias analysis, Word Embedding Association Tests (WEAT), and Sentence Embedding Association Test (SEAT) we evaluate the gender bias at various stages of the pipeline: in model embeddings, in suggestions generated by the models, and in reviews written by students. our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses.
</details></li>
</ul>
<hr>
<h2 id="Ziya2-Data-centric-Learning-is-All-LLMs-Need"><a href="#Ziya2-Data-centric-Learning-is-All-LLMs-Need" class="headerlink" title="Ziya2: Data-centric Learning is All LLMs Need"></a>Ziya2: Data-centric Learning is All LLMs Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03301">http://arxiv.org/abs/2311.03301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, Yan Song</li>
<li>for: 本研究旨在提出一种基于LLaMA2模型的13亿参数Ziya2模型，并在不同阶段进行数据驱动优化以提高Ziya2模型在多个标准准点上的学习过程。</li>
<li>methods: 本研究采用了多种预训练技术和数据驱动优化策略，包括预训练数据的选择和组织、预训练过程中的数据填充策略以及在不同阶段进行数据驱动优化。</li>
<li>results: 实验结果显示，Ziya2模型在多个标准准点上表现出色，特别是与代表性的开源模型相比，Ziya2模型在一些预测任务上达到了更高的性能。Ziya2（基本）模型在<a target="_blank" rel="noopener" href="https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base%E5%92%8Chttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base和https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary中发布。</a><details>
<summary>Abstract</summary>
Various large language models (LLMs) have been proposed in recent years, including closed- and open-source ones, continually setting new records on multiple benchmarks. However, the development of LLMs still faces several issues, such as high cost of training models from scratch, and continual pre-training leading to catastrophic forgetting, etc. Although many such issues are addressed along the line of research on LLMs, an important yet practical limitation is that many studies overly pursue enlarging model sizes without comprehensively analyzing and optimizing the use of pre-training data in their learning process, as well as appropriate organization and leveraging of such data in training LLMs under cost-effective settings. In this work, we propose Ziya2, a model with 13 billion parameters adopting LLaMA2 as the foundation model, and further pre-trained on 700 billion tokens, where we focus on pre-training techniques and use data-centric optimization to enhance the learning process of Ziya2 on different stages. Experiments show that Ziya2 significantly outperforms other models in multiple benchmarks especially with promising results compared to representative open-source ones. Ziya2 (Base) is released at https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.
</details>
<details>
<summary>摘要</summary>
各种大型语言模型（LLMs）在最近几年内被提出，包括关闭和开源的模型，不断创造新的纪录在多个 benchmarck 上。然而， LLMS 的开发仍面临多个问题，如从 scratch 训练模型的高成本、 catastrophic forgetting 等等。虽然这些问题在 LLMS 研究中得到了很多的解决方案，但是一个重要且实用的限制是许多研究过于强调模型的大小，而不是全面分析和优化在训练过程中使用的预训练数据，以及如何合理地组织和利用这些数据来训练 LLMS。在这项工作中，我们提出了 Ziya2，一个采用 LLaMA2 基础模型，并在 700 亿个字符上进行了进一步预训练，我们在各个阶段都将注重预训练技巧，并通过数据中心化优化来提高 Ziya2 在不同阶段的学习过程。实验结果表明，Ziya2 在多个 benchmarck 上显著超越其他模型，尤其是与代表性的开源模型相比，Ziya2 (Base) 已经发布在 <https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base> 和 <https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary>。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges"><a href="#Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges" class="headerlink" title="Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"></a>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03287">http://arxiv.org/abs/2311.03287</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gzcch/bingo">https://github.com/gzcch/bingo</a></li>
<li>paper_authors: Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, Huaxiu Yao</li>
<li>for: 这个研究是为了评估和描述 GPT-4V(ision) 模型中的幻觉行为，以及这种幻觉的两种常见类型：偏见和干扰。</li>
<li>methods: 这个研究使用了一个新的 benchmark，即 Bias and Interference Challenges in Visual Language Models (Bingo)，来评估 GPT-4V(ision) 模型的幻觉行为。</li>
<li>results: 研究发现，GPT-4V(ision) 模型存在 regional bias，即更好地理解西方图像或图像中的英文文本，而对其他国家或其他语言的图像和文本的理解不及格。此外，GPT-4V(ision) 模型容易受到提问的影响，并且在处理多个图像时会受到混乱。这些挑战无法通过自我修复和链式思维方法解决。<details>
<summary>Abstract</summary>
While GPT-4V(ision) impressively models both visual and textual information simultaneously, it's hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</details>
<details>
<summary>摘要</summary>
而GPT-4V(ision)却显示出了同时模型视觉和文本信息的卓越表现，但它的幻觉行为尚未得到系统性的评估。为了填补这一遗漏，我们提出了一个新的标准测试套件，即视觉语言模型偏见和干扰挑战（Bingo）。这个测试套件是为了评估和探讨视觉语言模型中两种常见的幻觉类型：偏见和干扰。其中，偏见指的是模型幻觉某些类型的回答，可能是因为训练数据的不均衡。干扰指的是在提示文本或输入图像的表述方式中，模型的判断能力被干扰的情况。我们发现了一种明显的地域偏见，即GPT-4V(ision)更好地理解西方图像或图像包含英文文本的情况。此外，GPT-4V(ision)容易受到提示文本的诱导和多个图像的混乱影响。现有的 Mitigation 方法，如自我检查和链条思维，无法解决这些挑战。我们还发现了 LLava 和 Bard 等模型中的相似偏见和干扰敏感性。我们的结果描述了 GPT-4V(ision) 和当前最佳视觉语言模型中的幻觉挑战，并 highlights 需要新的解决方案。Bingo 测试套件可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM"><a href="#Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM" class="headerlink" title="Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM"></a>Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03243">http://arxiv.org/abs/2311.03243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo, Leon Jovanovic</li>
<li>for: 这篇论文是为了提出一种开源模型，用于生成、完成和调试 C# 代码。</li>
<li>methods: 该模型基于 CodeLlama 34B 模型，并使用 EvolInstruct 技术进行精细化和扩展数据集，以进行精细化和扩展数据集。</li>
<li>results: 模型在 Manual MultiPL-E 比赛中获得了56.33% 的 notable 分数（Zero-Shot, Pass@1），表明它具有优秀的开发工作流程协助和代码学习支持功能。<details>
<summary>Abstract</summary>
This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code. Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process. The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers' workflows and aid code learning. It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了Safurai-Csharp，一个开源模型，旨在优化C#代码生成、完成和调试。Safurai-Csharp基于CodeLlama 34B模型，并使用EvolInstruct技术，通过精细的调整和扩展数据集，实现了高效的特化和优化。 benchmark测试结果显示，Safurai-Csharp在Manual MultiPL-E多频率测试中取得了56.33%的成绩（零shot，Pass@1），表明它在开发者工作流程中具有很高的效率和可靠性。这表明Safurai-Csharp具有开推新的可能性，并希望能够激发更多的开源C# LLMS的发展，以及更广泛的语言特定LLMS的开发。
</details></li>
</ul>
<hr>
<h2 id="p-Laplacian-Transformer"><a href="#p-Laplacian-Transformer" class="headerlink" title="p-Laplacian Transformer"></a>p-Laplacian Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03235">http://arxiv.org/abs/2311.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tam Nguyen, Vinh Nguyen, Tan M. Nguyen</li>
<li>for: 本文主要研究自注意 Mechanism 在 transformers 中的应用，以实现更好的语言模型性能。</li>
<li>methods: 本文提出了一种基于 $p$-Laplacian  regularization 的新型 transformers，称为 $p$-Laplacian Transformer (p-LaT)，以利用自注意层中的异质特征。</li>
<li>results: 对多种 benchmark 数据集进行了实验，并证明了 p-LaT 在语言模型性能上的优势。<details>
<summary>Abstract</summary>
$p$-Laplacian regularization, rooted in graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)$p$-laplacian regularization, originating from graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. Based on this insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. Specifically, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation"><a href="#Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation" class="headerlink" title="Model-based Counterfactual Generator for Gender Bias Mitigation"></a>Model-based Counterfactual Generator for Gender Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03186">http://arxiv.org/abs/2311.03186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ewoenam Kwaku Tokpo, Toon Calders</li>
<li>for: 降低语言模型中的性别偏见</li>
<li>methods: combines data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals</li>
<li>results: alleviates the shortcomings of dictionary-based solutions and improves the mitigation of gender bias<details>
<summary>Abstract</summary>
Counterfactual Data Augmentation (CDA) has been one of the preferred techniques for mitigating gender bias in natural language models. CDA techniques have mostly employed word substitution based on dictionaries. Although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. Model-based solutions can alleviate these problems, yet the lack of qualitative parallel training data hinders development in this direction. Therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. We implemented our proposed solution and performed an empirical evaluation which shows how our model alleviates the shortcomings of dictionary-based solutions.
</details>
<details>
<summary>摘要</summary>
counterfactual 数据增强 (CDA) 是一种常用的技术来减轻自然语言模型中的性别偏见。 CDA 技术主要使用词替换基于词典， although 这些词典基于的 CDA 技术已经证明可以有效地减轻性别偏见，但是在这篇论文中，我们指出了这些技术的一些局限性，如容易出现不 grammatical 的 sentence，并且无法泛化到未定义词汇集中。 model-based 解决方案可以解决这些问题，但是因为缺乏 качеitative 平行训练数据，因此不得不采用数据处理技术和 bi-objective 训练方案来开发一种基于模型的解决方案。 we 实现了我们的提议并进行了 empirical 评估，显示了我们的模型可以减轻词典基于的 CDA 技术中的缺陷。
</details></li>
</ul>
<hr>
<h2 id="Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives"><a href="#Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives" class="headerlink" title="Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!"></a>Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03153">http://arxiv.org/abs/2311.03153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phhei/relateperspectives-sweetspots">https://github.com/phhei/relateperspectives-sweetspots</a></li>
<li>paper_authors: Philipp Heinisch, Matthias Orlikowski, Julia Romberg, Philipp Cimiano</li>
<li>for: 这个论文主要针对的是自然语言处理中的annotation任务，具体来说是argument质量分类任务。</li>
<li>methods: 这个论文使用了一种continuum的方法，从fully归一化到”share nothing”-architectures，来表征个人和共同 perspectives的协同作用。</li>
<li>results: 研究发现，通过使用 recomender系统中的模型层来模型不同 annotator之间的关系，可以提高averaged annotator-individual F$_1$-scores，最高提高43%。这些结果表明，对subjectivity的approaches可以通过关系个人 perspectives来提高表达质量。<details>
<summary>Abstract</summary>
Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to "share nothing"-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that include layers to model the relations between different annotators are beneficial for predicting single-annotator labels. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F$_1$-scores up to $43\%$ over a majority label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives.
</details>
<details>
<summary>摘要</summary>
很多自然语言处理中的标注任务具有主观性，因为存在不同的有效和合理的观点可以用于描述给定示例的标签。这同时也适用于论点质量评价，其中单个真实的判据往往存在问题。为了最好地表现个人和共同的视角之间的互动，我们考虑了一个维度的方法，从完全汇总视角到“分享无 Shared”-架构，在这两个极端之间进行调查。在这些极端之间，我们发现了基于推荐系统中使用的模型，可以增加预测单个标注员标签的精度。通过两个论点质量分类任务（论点具体性和结论的有效性/新颖性），我们发现，推荐架构可以提高平均标注员F$_1$-分数达43%。我们的发现表明，主观性方法可以从各个个人视角之间的关系中受益。
</details></li>
</ul>
<hr>
<h2 id="Text-Augmentations-with-R-drop-for-Classification-of-Tweets-Self-Reporting-Covid-19"><a href="#Text-Augmentations-with-R-drop-for-Classification-of-Tweets-Self-Reporting-Covid-19" class="headerlink" title="Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19"></a>Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03420">http://arxiv.org/abs/2311.03420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumam Francis, Marie-Francine Moens</li>
<li>for: 本研究为社交媒体挖掘2023年共同任务提出的模型。我们的团队面临了第一项任务，分类推特发布自我报告COVID-19诊断。</li>
<li>methods: 我们的方法包括一个分类模型，利用多种文本增强和R-drop增强数据，以减少过拟合。我们将增强模型应用了多种增强技巧，如同义词替换、保留词和返回词。</li>
<li>results: 我们的系统在测试集上实现了各自F1分数0.877，在任务中超过了 mean 和 median 分数。<details>
<summary>Abstract</summary>
This paper presents models created for the Social Media Mining for Health 2023 shared task. Our team addressed the first task, classifying tweets that self-report Covid-19 diagnosis. Our approach involves a classification model that incorporates diverse textual augmentations and utilizes R-drop to augment data and mitigate overfitting, boosting model efficacy. Our leading model, enhanced with R-drop and augmentations like synonym substitution, reserved words, and back translations, outperforms the task mean and median scores. Our system achieves an impressive F1 score of 0.877 on the test set.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了为健康社交媒体挖掘2023年共同任务创建的模型。我们团队解决了第一个任务，即通过推特分类自报 Covid-19 诊断。我们的方法包括一种分类模型，利用多种文本扩展和使用 R-drop 来增强数据和避免过拟合，从而提高模型效果。我们的领先模型，通过 R-drop 和扩展如同义词替换、保留词和回译等，超越任务的 mean 和 median 分数。我们的系统在测试集上达到了可观的 F1 分数0.877。
</details></li>
</ul>
<hr>
<h2 id="Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER"><a href="#Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER" class="headerlink" title="Injecting Categorical Labels and Syntactic Information into Biomedical NER"></a>Injecting Categorical Labels and Syntactic Information into Biomedical NER</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03113">http://arxiv.org/abs/2311.03113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumam Francis, Marie-Francine Moens</li>
<li>for: 提高生物医学命名实体识别（NER）精度</li>
<li>methods: 采用两种方法：首先训练一个序列级分类器，将句子分类为类别，并将标签改为自然语言模板，以提高分类器的准确率。然后将这些标签和Part-of-speech（POS）信息注入到NER模型中。</li>
<li>results: 在三个benchmark数据集上进行实验，发现将分类标签和POS信息注入到NER模型中可以提高NER精度，并且超过基elineBERT模型。<details>
<summary>Abstract</summary>
We present a simple approach to improve biomedical named entity recognition (NER) by injecting categorical labels and Part-of-speech (POS) information into the model. We use two approaches, in the first approach, we first train a sequence-level classifier to classify the sentences into categories to obtain the sentence-level tags (categorical labels). The sequence classifier is modeled as an entailment problem by modifying the labels as a natural language template. This helps to improve the accuracy of the classifier. Further, this label information is injected into the NER model. In this paper, we demonstrate effective ways to represent and inject these labels and POS attributes into the NER model. In the second approach, we jointly learn the categorical labels and NER labels. Here we also inject the POS tags into the model to increase the syntactic context of the model. Experiments on three benchmark datasets show that incorporating categorical label information with syntactic context is quite useful and outperforms baseline BERT-based models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的方法来提高生物医学命名实体识别（NER）的精度，我们在模型中注入了分类标签和语法类型（POS）信息。我们采用了两种方法：在第一种方法中，我们首先训练一个序列级别的分类器，以将句子分类为不同的类别，从而获得句子级别的标签（分类标签）。这个分类器是通过修改标签为自然语言模板来实现的，这有助于提高分类器的准确率。然后，我们将这些标签和POS信息注入到NER模型中。在第二种方法中，我们同时学习分类标签和NER标签。在这里，我们还注入了POS标签，以增加模型的语法上下文。我们在三个标准数据集上进行了实验，结果表明，将分类标签和语法上下文注入到BERT模型中可以提高NER的精度，并且超越基eline BERT模型。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch"><a href="#Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch" class="headerlink" title="Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"></a>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03099">http://arxiv.org/abs/2311.03099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-buaa/mergelm">https://github.com/yule-buaa/mergelm</a></li>
<li>paper_authors: Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li</li>
<li>for: 本研究旨在探讨语言模型（LM）可以通过吸收同类模型参数而获得新能力，无需重新训练或GPU。</li>
<li>methods: 研究人员发现，通过一种新的操作 called DARE（Drop And REscale），可以直接将大多数 delta 参数设为零，而不会影响 SFT LM 的能力。此外，通过将多个 SFT 同类模型的 delta 参数简化并合并为一个单一模型，可以获得多种能力。</li>
<li>results: 实验结果表明， delta 参数的值范围通常在 0.005 左右，DARE 可以轻松地消除 99% 的 delta 参数。然而，一旦模型进行了连续预训练， delta 参数的值范围可以增加到约 0.03，使 DARE 成为不切实际。此外，尝试将 fine-tuned 参数 removal 和 delta 参数 removal 进行比较，发现将 fine-tuned 参数 removal 可以导致性能减少至 0。这显示出 SFT 只是通过 delta 参数来刺激 LM 的能力，而不是投入新的能力。此外，DARE 可以将多个任务特定 LM 合并成一个多能力 LM。例如，将 WizardLM 和 WizardMath 合并后，GSM8K 零点扩展精度从 2.2 提高至 66.3，保留 WizardLM 的 instrucion-following 能力，超过 WizardMath 的原始 64.2 性能。<details>
<summary>Abstract</summary>
In this paper, we uncover that Language Models (LMs), either encoder- or decoder-based, can obtain new capabilities by assimilating the parameters of homologous models without retraining or GPUs. Typically, new abilities of LMs can be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity between fine-tuned and pre-trained parameters (i.e., delta parameters). We initially observe that by introducing a novel operation called DARE (Drop And REscale), most delta parameters can be directly set to zeros without affecting the capabilities of SFT LMs and larger models can tolerate a higher proportion of discarded parameters. Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging. We conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge WizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results show that: (1) The delta parameter value ranges for SFT models are typically small, often within 0.005, and DARE can eliminate 99% of them effortlessly. However, once the models are continuously pre-trained, the value ranges can grow to around 0.03, making DARE impractical. We have also tried to remove fine-tuned instead of delta parameters and find that a 10% reduction can lead to drastically decreased performance (even to 0). This highlights that SFT merely stimulates the abilities via delta parameters rather than injecting new abilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM with diverse abilities. For instance, the merger of WizardLM and WizardMath improves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining its instruction-following ability while surpassing WizardMath's original 64.2 performance. Codes are available at https://github.com/yule-BUAA/MergeLM.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们发现语言模型（LM），无论是基于编码器或解码器的，可以通过吸收同类模型的参数而获得新的能力，无需重新训练或GPU。通常，LM的新能力可以通过监督精度调整（SFT）来实现，这可以通过参数之间的差异（ delta 参数）来衡量。我们发现，通过一种新的操作 called DARE（Drop And REscale），大多数 delta 参数可以直接设为零，而不会影响 SFT LM 的能力。基于这一点，我们进一步减轻 delta 参数的多个 SFT 同类模型，并将它们合并成一个单独的模型。我们在 GLUE benchmark 上的八个数据集上进行实验，以及将 WizardLM、WizardMath 和 Code Alpaca 基于 Llama 2 进行合并。实验结果表明：（1） SFT 模型的 delta 参数范围通常在 0.005 左右，DARE 可以轻松地消除 99% 的 delta 参数。然而，当模型进行连续预训练时， delta 参数的范围可以增长到约 0.03，使 DARE 变得不实际。我们还尝试了从 fine-tuned 而不是 delta 参数中 removing  fine-tuned 并发现，将 fine-tuned 参数减少 10% 可能会导致性能减少到 0。这表明 SFT 仅仅通过 delta 参数来刺激 LM 的能力，而不是在 LM 中植入新的能力；（2） DARE 可以将多个任务特定的 LM 合并成一个多能力 LM。例如，将 WizardLM 和 WizardMath 合并到一起，可以提高 WizardLM 的 GSM8K 零shot准确率从 2.2 提高到 66.3，保留 WizardLM 的 instrucion-following 能力，而同时超过 WizardMath 的原始 64.2 性能。代码可以在 <https://github.com/yule-BUAA/MergeLM> 上获取。
</details></li>
</ul>
<hr>
<h2 id="BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer"><a href="#BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer" class="headerlink" title="BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer"></a>BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03078">http://arxiv.org/abs/2311.03078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eblict-gigatech/BanLemma">https://github.com/eblict-gigatech/BanLemma</a></li>
<li>paper_authors: Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed</li>
<li>for: 这个论文的目的是提出一个基于语言规则的抽象lemmatization算法，用于解决孟加拉语言的抽象lemmatization问题。</li>
<li>methods: 该论文使用了语言规则和词典来设计一个特定于孟加拉语言的lemmatizer，并通过分析大量的孟加拉文本来验证其准确性。</li>
<li>results: 该论文的实验结果显示，使用该lemmatizer可以达到96.36%的准确率，并且与之前发表的三个孟加拉lemmatization数据集中的结果相比，表现竞争力强。<details>
<summary>Abstract</summary>
Lemmatization holds significance in both natural language processing (NLP) and linguistics, as it effectively decreases data density and aids in comprehending contextual meaning. However, due to the highly inflected nature and morphological richness, lemmatization in Bangla text poses a complex challenge. In this study, we propose linguistic rules for lemmatization and utilize a dictionary along with the rules to design a lemmatizer specifically for Bangla. Our system aims to lemmatize words based on their parts of speech class within a given sentence. Unlike previous rule-based approaches, we analyzed the suffix marker occurrence according to the morpho-syntactic values and then utilized sequences of suffix markers instead of entire suffixes. To develop our rules, we analyze a large corpus of Bangla text from various domains, sources, and time periods to observe the word formation of inflected words. The lemmatizer achieves an accuracy of 96.36% when tested against a manually annotated test dataset by trained linguists and demonstrates competitive performance on three previously published Bangla lemmatization datasets. We are making the code and datasets publicly available at https://github.com/eblict-gigatech/BanLemma in order to contribute to the further advancement of Bangla NLP.
</details>
<details>
<summary>摘要</summary>
lemmatization在自然语言处理（NLP）和语言学中具有重要意义，因为它可以有效减少数据密度，并帮助理解上下文中的意思。然而，由于孟加拉语的高度变格和 morphological richness，孟加拉语 lemmatization  poses a complex challenge。在这项研究中，我们提出了语言规则 для lemmatization，并使用字典和规则来设计特定 для孟加拉语的 lemmatizer。我们的系统 aimsto lemmatize words based on their parts of speech class within a given sentence。不同于前一些规则基本的方法，我们分析了 suffix marker 的出现 according to the morpho-syntactic values，然后使用 sequences of suffix markers instead of entire suffixes。为了开发我们的规则，我们分析了大量的孟加拉语文本从多个领域、来源和时期，以观察 инфиlected words 的形成。lemmatizer 在一个手动注释的测试集上测试时 achieved an accuracy of 96.36%，并在三个之前发布的孟加拉语 lemmatization 数据集上达到了竞争性的性能。我们将代码和数据集公开发布在 GitHub 上，以便贡献到孟加拉语 NLP 的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models"><a href="#Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models" class="headerlink" title="Zero-shot Bilingual App Reviews Mining with Large Language Models"></a>Zero-shot Bilingual App Reviews Mining with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03058">http://arxiv.org/abs/2311.03058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jl-wei/mini-bar">https://github.com/jl-wei/mini-bar</a></li>
<li>paper_authors: Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray</li>
<li>for: 提高软件需求的评估和优化</li>
<li>methods: 使用大型自然语言处理（NLP）模型和隐藏 маркетин数据集</li>
<li>results: 实现零shot的用户评论挖掘和概括，并提供用户评论群集和概要摘要Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written to improve the assessment and optimization of software requirements by leveraging user reviews from app stores.</li>
<li>methods: The proposed approach, called Mini-BAR, uses large language models (LLMs) to automatically mine user reviews in both English and French. Mini-BAR consists of four main components: classification, clustering, abstractive summary generation, and ranking.</li>
<li>results: The authors evaluate the effectiveness and efficiency of Mini-BAR using a dataset of 6,000 English and 6,000 French annotated user reviews. Preliminary results demonstrate the ability of Mini-BAR to accurately classify, cluster, and summarize user reviews, as well as rank the review clusters based on their relevance to software requirements.<details>
<summary>Abstract</summary>
App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. (Replication package containing the code, dataset, and experiment setups on https://github.com/Jl-wei/mini-bar )
</details>
<details>
<summary>摘要</summary>
应用商店中的用户评论对软件需求的改进具有关键作用。大量有价值的用户评论不断地被上传，描述软件问题和预期功能。有效地利用用户评论需要提取有用信息，并对其进行概括。由于用户评论的数量过大，手动分析是困难的。基于自然语言处理（NLP）的多种方法已经被提议用于自动化用户评论挖掘。然而，大多数方法需要手动制作数据集来训练其模型，这限制了它们在实际场景中的使用。在这种情况下，我们提出了 Mini-BAR 工具，它利用大型自然语言模型（LLMs）来完成零shot的用户评论挖掘。特别是，Mini-BAR 的设计包括（i）类别用户评论，（ii）将相似的评论集成起来，（iii）为每个集合生成抽象概括，以及（iv）对用户评论集进行排名。为了评估 Mini-BAR 的表现，我们创建了包含 6,000 个英语和 6,000 个法语用户评论的数据集，并进行了广泛的实验。初步结果表明 Mini-BAR 在需求工程中的效果和效率，通过分析双语应用评论。（复制包含代码、数据集和实验设置的https://github.com/Jl-wei/mini-bar ）
</details></li>
</ul>
<hr>
<h2 id="Detecting-Agreement-in-Multi-party-Conversational-AI"><a href="#Detecting-Agreement-in-Multi-party-Conversational-AI" class="headerlink" title="Detecting Agreement in Multi-party Conversational AI"></a>Detecting Agreement in Multi-party Conversational AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03026">http://arxiv.org/abs/2311.03026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Schauer, Jason Sweeney, Charlie Lyttle, Zein Said, Aron Szeles, Cale Clark, Katie McAskill, Xander Wickham, Tom Byars, Daniel Hernández Garcia, Nancie Gunson, Angus Addlesee, Oliver Lemon</li>
<li>for: 这个论文是为了解决多方会话中的社交助手机器人（SARs）的实际使用问题，特别是识别说话人和接受者、复杂的回答交换等问题。</li>
<li>methods: 该论文提出了一种多方会话对话系统，让两名用户参与一个知识竞赛游戏。系统可以检测用户们的一致或不一致的答案，并按照应对方式回答。</li>
<li>results: 论文的评估包括性能评估和用户评估，重点是检测用户一致的答案。我们提供了对应的注释脚本和GitHub上的代码，以便其他研究人员可以进行复用和扩展。<details>
<summary>Abstract</summary>
Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.
</details>
<details>
<summary>摘要</summary>
Translation into Simplified Chinese:今天，对话系统预期能够处理多方会话，特别是在社会辅助机器人（SAR）中。然而，实际使用中存在多种挑战，如说话人识别、目标人识别和复杂的回答交互。在这篇论文中，我们介绍了一种多方对话系统， Invites two users to play a trivia quiz game.系统可以检测用户们的同意或不同意 final answer，并根据此进行应答。我们的评估包括性能评估和用户评估结果，重点是检测用户同意。我们已经在 GitHub 上发布了对应的注释转译和系统代码。
</details></li>
</ul>
<hr>
<h2 id="Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement"><a href="#Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement" class="headerlink" title="Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement"></a>Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03021">http://arxiv.org/abs/2311.03021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddenley/multi-person-quiz">https://github.com/ddenley/multi-person-quiz</a></li>
<li>paper_authors: Angus Addlesee, Daniel Denley, Andy Edmondson, Nancie Gunson, Daniel Hernández Garcia, Alexandre Kha, Oliver Lemon, James Ndubuisi, Neil O’Reilly, Lia Perochaud, Raphaël Valeri, Miebaka Worika</li>
<li>for: 这个研究用于检验对话状态跟踪方法是否能够正确地识别对话中的一致和不一致情况。</li>
<li>methods: 这个研究使用了 диари化模型和频率和 proximity 基于的方法来识别对话中的一致和不一致情况。</li>
<li>results: 实验结果表明，我们的原始系统比 диари化系统更加有趣，并且更加准确地识别了一致情况，其准确率达到了 0.44，而 диари化系统的准确率为 0.28。<details>
<summary>Abstract</summary>
Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN多方会话中的对话管理器面临着 significativley 难以实现对话状态跟踪的挑战，因为发言人的身份增加了Contextual 含义。 通常使用划分模型来标识发言人。然而，是否准确地标识对话中的特定对话事件，如同意或不同意，是一个问题。这个研究使用了合作测验，其中对话管理器 acts as 测验主持人，以确定划分模型或频率和距离基于方法是更加准确地确定同意的。实验结果表明，我们的程序性系统更加吸引人们的注意力，并且更加准确地检测到同意，达到了0.44的准确率，比0.28的划分系统更高。Note: "zh-CN" is the language code for Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions"><a href="#Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions" class="headerlink" title="Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions"></a>Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02985">http://arxiv.org/abs/2311.02985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julien Guité-Vinet, Alexandre Blondin Massé, Fatiha Sadat</li>
<li>for: 这篇研究是为了解决词汇游戏“字典游戏”中的反词字典任务。</li>
<li>methods: 这篇研究使用了不同的 transformer-based 模型来解决反词字典任务，并 explore 这些模型在这个Context中的使用。</li>
<li>results: 研究获得了不同的 transformer-based 模型在解决反词字典任务的效果，并 analyzed 这些模型的优缺点。<details>
<summary>Abstract</summary>
In the last years, several variants of transformers have emerged. In this paper, we compare different transformer-based models for solving the reverse dictionary task and explore their use in the context of a serious game called The Dictionary Game.
</details>
<details>
<summary>摘要</summary>
最近几年，Transformers家族中的不同变体出现了。本文将 Comparing different transformer-based models for solving the reverse dictionary task, and explore their use in the context of a serious game called The Dictionary Game。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering"><a href="#Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering" class="headerlink" title="Adapting Pre-trained Generative Models for Extractive Question Answering"></a>Adapting Pre-trained Generative Models for Extractive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02961">http://arxiv.org/abs/2311.02961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prabirmallick/GenAI4EQA">https://github.com/prabirmallick/GenAI4EQA</a></li>
<li>paper_authors: Prabir Mallick, Tapas Nayak, Indrajit Bhattacharya</li>
<li>for: 提高抽取问答 tasks 的表现</li>
<li>methods: 使用预训练的生成模型生成答案相关的索引</li>
<li>results: 在多个抽取问答 dataset 上达到了更高的表现，比如 MultiSpanQA、BioASQ、MASHQA 和 WikiQA。<details>
<summary>Abstract</summary>
Pre-trained Generative models such as BART, T5, etc. have gained prominence as a preferred method for text generation in various natural language processing tasks, including abstractive long-form question answering (QA) and summarization. However, the potential of generative models in extractive QA tasks, where discriminative models are commonly employed, remains largely unexplored. Discriminative models often encounter challenges associated with label sparsity, particularly when only a small portion of the context contains the answer. The challenge is more pronounced for multi-span answers. In this work, we introduce a novel approach that uses the power of pre-trained generative models to address extractive QA tasks by generating indexes corresponding to context tokens or sentences that form part of the answer. Through comprehensive evaluations on multiple extractive QA datasets, including MultiSpanQA, BioASQ, MASHQA, and WikiQA, we demonstrate the superior performance of our proposed approach compared to existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary>
先前的生成模型，如BART和T5等，在自然语言处理中的文本生成任务中备受欢迎，包括概括性长篇问答（QA）和概要。然而，生成模型在抽取式QA任务中的潜力仍未得到充分发挥，特别是当只有小部分上下文中包含答案时。这种挑战更加明显，当答案需要多个 Span 时。在这项工作中，我们提出了一种新的方法，使用预训练的生成模型来解决抽取式QA任务，通过生成上下文字元或句子的索引，以便更好地找到答案。经过对多个抽取式QA数据集，包括 MultiSpanQA、BioASQ、MASHQA 和 WikiQA 的全面评估，我们展示了我们提出的方法与现有状态的模型相比，表现出优异的性能。
</details></li>
</ul>
<hr>
<h2 id="PhoGPT-Generative-Pre-training-for-Vietnamese"><a href="#PhoGPT-Generative-Pre-training-for-Vietnamese" class="headerlink" title="PhoGPT: Generative Pre-training for Vietnamese"></a>PhoGPT: Generative Pre-training for Vietnamese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02945">http://arxiv.org/abs/2311.02945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/phogpt">https://github.com/vinairesearch/phogpt</a></li>
<li>paper_authors: Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Nhung Nguyen, Thien Huu Nguyen, Dinh Phung, Hung Bui</li>
<li>for: 这个论文是为了介绍一种新的开源 generative 模型系列 PhoGPT，用于越南语言。</li>
<li>methods: 该模型使用了一种基于 transformer 的7.5亿参数模型，并提供了一种 instruciton-following 变体 PhoGPT-7B5-Instruct。</li>
<li>results: 论文通过人工评估实验展示了这个模型的性能比前一代开源模型更高。In English, that’s:</li>
<li>for: This paper introduces a new open-source generative model series PhoGPT for Vietnamese.</li>
<li>methods: The model uses a transformer-based 7.5 billion parameter model and provides an instruction-following variant PhoGPT-7B5-Instruct.</li>
<li>results: The paper demonstrates the superior performance of this model through a human evaluation experiment compared to previous open-source models.<details>
<summary>Abstract</summary>
We open-source a state-of-the-art 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct. In addition, we also demonstrate its superior performance compared to previous open-source models through a human evaluation experiment. GitHub: https://github.com/VinAIResearch/PhoGPT
</details>
<details>
<summary>摘要</summary>
我们开源了一系列现代化的7.5B参数生成模型，名为 PhoGPT，用于越南语言。该系列包括基础预训练单语言模型 PhoGPT-7B5 和其指令遵循变体 PhoGPT-7B5-Instruct。此外，我们还通过人工评估实验证明其在前一代开源模型之上的超越性。GitHub：https://github.com/VinAIResearch/PhoGPT。
</details></li>
</ul>
<hr>
<h2 id="SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data"><a href="#SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data" class="headerlink" title="SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data"></a>SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02883">http://arxiv.org/abs/2311.02883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, Tomas Pfister</li>
<li>for: 提高文本到SQL生成器的几个shot提示能力</li>
<li>methods: 创新的提示设计、执行相关的一致性解码策略和多种提示设计和基础模型的混合策略</li>
<li>results: 在受限的数据量下，与已经训练的模型相比，提高了文本到SQL生成器的几个shot学习能力，降低了与高级模型的差距<details>
<summary>Abstract</summary>
Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose "SQLPrompt", tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs ("MixPrompt") and foundation models ("MixLLMs"). We show that \emph{SQLPrompt} outperforms previous approaches for in-context learning with few labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data.
</details>
<details>
<summary>摘要</summary>
文本到SQL目的是自然语言文本中生成SQL查询的自动化过程。在这项工作中，我们提出了“SQLPrompt”，用于改进大语言模型（LLM）中几次提示能力。我们的方法包括创新的提示设计、执行基于一致性解码策略和多提示执行选择策略，以及一种用于提高性能的多提示执行选择策略（MixPrompt）和基础模型（MixLLMs）。我们表明， compared to previous approaches， \emph{SQLPrompt} 在少量标注数据下进行协study learning的情况下，能够大幅超越之前的方法，并且落差与高级标注数据进行 fine-tuning 的状态差不远。
</details></li>
</ul>
<hr>
<h2 id="Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision"><a href="#Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision" class="headerlink" title="Less than One-shot: Named Entity Recognition via Extremely Weak Supervision"></a>Less than One-shot: Named Entity Recognition via Extremely Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02861">http://arxiv.org/abs/2311.02861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/komeijiforce/x-ner">https://github.com/komeijiforce/x-ner</a></li>
<li>paper_authors: Letian Peng, Zihan Wang, Jingbo Shang</li>
<li>for: 这 paper 是为了解决 named entity recognition (NER) 问题在 extremely weak supervision (XWS) Setting 中。</li>
<li>methods: 该 paper 提出了一种新的方法 X-NER，该方法可以在一个上下文自由的情况下，使用一个例子实体来帮助学习 NER。</li>
<li>results: 对 4 个 NER 数据集进行了广泛的实验和分析，显示 X-NER 的综合 NER 性能高于当前一些一射学习方法，并且可以具有跨语言能力。<details>
<summary>Abstract</summary>
We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER that can outperform the state-of-the-art one-shot NER methods. We first mine entity spans that are similar to the example entities from an unlabelled training corpus. Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example. We then leverage the top-ranked spans as pseudo-labels to train an NER tagger. Extensive experiments and analyses on 4 NER datasets show the superior end-to-end NER performance of X-NER, outperforming the state-of-the-art few-shot methods with 1-shot supervision and ChatGPT annotations significantly. Finally, our X-NER possesses several notable properties, such as inheriting the cross-lingual abilities of the underlying language models.
</details>
<details>
<summary>摘要</summary>
我们研究了名实体识别（NER）问题在极其轻量级监督（XWS） Setting下，只有一个例行实体每种类型被提供在context-free的方式。虽然XWS比一shot更轻量级，我们提出了一种新方法X-NER，可以超越当前一shot NER方法的状态。我们首先在无标注训练集中挖掘类似于示例实体的实体探索。而不是利用语言模型生成的实体 span表示，我们发现更有效的是比较在span被替换后的上下文分布和之前的分布。然后，我们利用排名最高的探索作为pseudo-标签来训练NER标记器。我们对4个NER数据集进行了广泛的实验和分析，发现X-NER具有出色的综合NER性能，超越当前几个ew shot方法，并且与ChatGPT标注显著。最后，我们的X-NER具有一些吸引人的特性，如继承下来的语言模型的cross-Lingual能力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding"><a href="#Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding" class="headerlink" title="Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"></a>Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02851">http://arxiv.org/abs/2311.02851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lemon0830/CoDec">https://github.com/lemon0830/CoDec</a></li>
<li>paper_authors: Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</li>
<li>for: 本研究的目的是分析不同商业NMT系统和MT-oriented LLMs的优缺点，并基于这些发现提出一种hybrid方法来补充NMT系统。</li>
<li>methods: 本研究使用了多种方法，包括对不同NMT系统和MT-oriented LLMs的比较分析，以及基于这些发现的hybrid方法的开发。</li>
<li>results: 研究结果表明，MT-oriented LLMs可以作为NMT系统的补充解决复杂的翻译问题，而CoDec方法在WMT22测试集和新收集的WebCrawl测试集上得到了显著的效果和效率提升。<details>
<summary>Abstract</summary>
Contemporary translation engines built upon the encoder-decoder framework have reached a high level of development, while the emergence of Large Language Models (LLMs) has disrupted their position by offering the potential for achieving superior translation quality. Therefore, it is crucial to understand in which scenarios LLMs outperform traditional NMT systems and how to leverage their strengths. In this paper, we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs can serve as a promising complement to the NMT systems. Building upon these insights, we explore hybrid methods and propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. The results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in machine translation.
</details>
<details>
<summary>摘要</summary>
当代翻译引擎，基于编码器-解码器框架，已经达到了高度的发展，而大语言模型（LLMs）的出现则对其造成了冲击， LLMS 提供了可以实现更高水平的翻译质量的潜在力量。因此，我们需要了解 LLMS 在哪些场景下表现出色，并如何利用其优势。在这篇论文中，我们首先进行了全面的分析，以评估不同的商业 NMT 系统和 MT-oriented LLMs 的优缺点。我们的发现表明，NMT 系统和 MT-oriented LLMs 独立无法解决所有翻译问题，但 MT-oriented LLMs 可以作为 NMT 系统的优秀补充。基于这些发现，我们探索了混合方法，并提出了协同解码（CoDec），协同解码将 NMT 系统作为预翻译模型，MT-oriented LLMs 作为 NMT 系统之外的补充解决方案，以处理 NMT 系统无法处理的复杂场景。 results on WMT22 测试集和我们新收集的 WebCrawl 测试集表明 CoDec 的效果和效率， highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in machine translation.
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation"><a href="#Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation" class="headerlink" title="Tailoring Self-Rationalizers with Multi-Reward Distillation"></a>Tailoring Self-Rationalizers with Multi-Reward Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02805">http://arxiv.org/abs/2311.02805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ink-usc/rationalemultirewarddistillation">https://github.com/ink-usc/rationalemultirewarddistillation</a></li>
<li>paper_authors: Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, Jack Hessel, Yejin Choi, Xiang Ren</li>
<li>for: 这篇论文旨在提高小型语言模型（LMs）的自我合理化能力，以帮助问答系统提高问题回答的性能。</li>
<li>methods: 这篇论文提出了一种名为MaRio（多重评价自我合理化算法）的多评价条件自我合理化算法，通过优化多种特征如可能性、多样性和一致性来提高小LMs的自我合理化质量。</li>
<li>results: 实验结果表明，MaRio不仅能够提高问题回答性能，还能够提高小LMs的自我合理化质量，比超级vised fine-tuning（SFT）基线更好。人类评价也表明，MaRio的合理化 rationales 比 SFT 的 rationales 更受欢迎，并且有质量上的改进。<details>
<summary>Abstract</summary>
Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (approx. 200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on five difficult question-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CL_2023_11_06/" data-id="closbronl00e30g883v3m1y9c" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/89/">89</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">118</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">58</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
