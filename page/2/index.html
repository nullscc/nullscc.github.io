
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/cs.AI_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T12:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/06/cs.AI_2023_12_06/">cs.AI - 2023-12-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-Pseudo-Semantic-Loss-for-Autoregressive-Models-with-Logical-Constraints"><a href="#A-Pseudo-Semantic-Loss-for-Autoregressive-Models-with-Logical-Constraints" class="headerlink" title="A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints"></a>A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03905">http://arxiv.org/abs/2312.03905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck</li>
<li>for:  bridges the gap between purely symbolic and neural approaches to learning, specifically for tasks that involve autoregressive distributions such as transformers.</li>
<li>methods:  proposes a new approach to neuro-symbolic learning that involves maximizing the likelihood of a symbolic constraint w.r.t the neural networkâ€™s output distribution, using a pseudolikelihood-based approximation centered around a model sample, which is factorized and locally high-fidelity.</li>
<li>results:  greatly improves upon the base modelâ€™s ability to predict logically-consistent outputs on Sudoku and shortest-path prediction tasks, and achieves State-of-the-Art (SoTA) detoxification compared to previous approaches on the task of detoxifying large language models by disallowing a list of toxic words.<details>
<summary>Abstract</summary>
Neuro-symbolic AI bridges the gap between purely symbolic and neural approaches to learning. This often requires maximizing the likelihood of a symbolic constraint w.r.t the neural network's output distribution. Such output distributions are typically assumed to be fully-factorized. This limits the applicability of neuro-symbolic learning to the more expressive autoregressive distributions, e.g., transformers. Under such distributions, computing the likelihood of even simple constraints is #P-hard. Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof. More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample. We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches.
</details>
<details>
<summary>æ‘˜è¦</summary>
More precisely, we optimize the likelihood of the constraint under a pseudolikelihood-based approximation centered around a model sample. Our approximation is factorized, allowing the reuse of solutions to sub-problems, a main tenet for efficiently computing neuro-symbolic losses. Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample.We evaluate our approach on Sudoku and shortest-path prediction cast as autoregressive generation, and observe that we greatly improve upon the base model's ability to predict logically-consistent outputs. We also evaluate on the task of detoxifying large language models. Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches.Here's the Simplified Chinese translation: neur-ç¬¦å·å­¦ AI å‡å°‘äº†ç¬¦å·å­¦å’Œç¥ç»ç½‘ç»œå­¦ä¹ ä¹‹é—´çš„é¸¿æ²Ÿã€‚è¿™ç»å¸¸éœ€è¦æœ€å¤§åŒ–ç¬¦å·çº¦æŸçš„å¯èƒ½æ€§ï¼Œå¯¹ç¥ç»ç½‘ç»œè¾“å‡ºåˆ†å¸ƒè¿›è¡Œæœ€å¤§åŒ–ã€‚è¿™äº›è¾“å‡ºåˆ†å¸ƒé€šå¸¸å‡è®¾ä¸ºå®Œå…¨å› å­åŒ–ã€‚è¿™é™åˆ¶äº†ç¬¦å·å­¦ä¹ çš„å¯ç”¨æ€§ï¼Œåªèƒ½åº”ç”¨äºæ›´è¡¨è¾¾åŠ›å¼ºçš„æ¨è®ºåˆ†å¸ƒï¼Œä¾‹å¦‚è½¬æ¢å™¨ã€‚åœ¨è¿™äº›åˆ†å¸ƒä¸‹ï¼Œè®¡ç®—çº¦æŸçš„å¯èƒ½æ€§æ˜¯ #P-hardã€‚è€Œä¸æ˜¯å°è¯•å°†çº¦æŸåº”ç”¨äºæ•´ä¸ªè¾“å‡ºåˆ†å¸ƒï¼Œæˆ‘ä»¬æè®®åœ¨æ¨¡å‹é‡‡æ ·ä¸­å¿ƒçš„æŠ½è±¡ä¸Šè¿›è¡Œçº¦æŸã€‚æ›´åŠ å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬ä¼˜åŒ–ç¬¦å·çº¦æŸçš„å¯èƒ½æ€§ï¼Œä½¿ç”¨åŸºäº Pseudolikelihood çš„æŠ½è±¡ã€‚æˆ‘ä»¬çš„æŠ½è±¡æ˜¯å¯é‡å¤çš„ï¼Œå…è®¸åœ¨å­é—®é¢˜ä¸Šé‡ç”¨è§£å†³æ–¹æ¡ˆï¼Œè¿™æ˜¯è®¡ç®—ç¬¦å·å­¦æŸå¤±çš„é‡è¦åŸåˆ™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æŠ½è±¡æ˜¯åœ°æ–¹çš„ã€é«˜å‡†ç¡®æ€§çš„ï¼Œåœ¨æ¨¡å‹é‡‡æ ·ä¸­å¿ƒçš„æŠ½è±¡ä¸‹ï¼ŒEntropy å’Œ KL åç§»éƒ½å¾ˆä½ã€‚æˆ‘ä»¬åœ¨ Sudoku å’ŒçŸ­è·¯é¢„æµ‹ä¸­ä½¿ç”¨ autoregressive ç”Ÿæˆï¼Œå¹¶è§‚å¯Ÿåˆ°æˆ‘ä»¬åœ¨åŸºæœ¬æ¨¡å‹çš„è¾“å‡ºä¸Šå¤§å¹…æé«˜äº†é€»è¾‘ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­ä½¿ç”¨ç®€å•çº¦æŸï¼Œç¦æ­¢ä½¿ç”¨æ¶æ„è¯æ±‡ï¼Œå¹¶æˆåŠŸåœ°ä½¿æ¨¡å‹çš„è¾“å‡ºé¿å…æ¶æ„ç”Ÿæˆï¼Œ achieved SoTA æ¶æ€§è¯†åˆ«æ¯”å‰æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Masked-Pruning-Approach-for-Dimensionality-Reduction-in-Communication-Efficient-Federated-Learning-Systems"><a href="#A-Masked-Pruning-Approach-for-Dimensionality-Reduction-in-Communication-Efficient-Federated-Learning-Systems" class="headerlink" title="A Masked Pruning Approach for Dimensionality Reduction in Communication-Efficient Federated Learning Systems"></a>A Masked Pruning Approach for Dimensionality Reduction in Communication-Efficient Federated Learning Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03889">http://arxiv.org/abs/2312.03889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tamir L. S. Gez, Kobi Cohen</li>
<li>for: æé«˜ Federated Learningï¼ˆFLï¼‰ç®—æ³•åœ¨å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„è®¾å¤‡ä¸Šçš„å¯åº”ç”¨æ€§ï¼Œä¾‹å¦‚å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„ç§»åŠ¨è®¾å¤‡æˆ–åµŒå…¥å¼è®¾å¤‡ã€‚</li>
<li>methods: ä½¿ç”¨æ©è”½æ³•ï¼ˆMaskingï¼‰å’ŒFLç®—æ³•ç›¸ç»“åˆï¼Œå®ç°åœ¨å¤šä¸ªèŠ‚ç‚¹ä¹‹é—´å…±äº«ä½ç»´åº¦è¡¨ç¤ºï¼Œå¹¶ä¸”å‡å°‘äº†é€šä¿¡æˆæœ¬ã€‚æ¯ä¸ªèŠ‚ç‚¹é¦–å…ˆåœ¨æœ¬åœ°è®­ç»ƒæ¨¡å‹ï¼Œç„¶åè®¡ç®—æ©è”½é¢ï¼Œå¹¶å°†æ©è”½é¢ä¼ è¾“å›æœåŠ¡å™¨è¿›è¡Œå…±è¯†ã€‚è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä½¿å¾—æ¨¡å‹å…·æœ‰æ›´é«˜çš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚</li>
<li>results: å¯¹æ¯” existed æ–¹æ³•ï¼ŒMPFL æ–¹æ³•å¯ä»¥å®ç°æ›´é«˜çš„å¸¦å®½ç¼©å‡ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼ŒMPFL æ–¹æ³•åœ¨å…·æœ‰é™åˆ¶é€šä¿¡èµ„æºçš„è®¾å¤‡ä¸Šçš„åº”ç”¨æ€§å¾—åˆ°äº†è¿›ä¸€æ­¥çš„è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¼€æºçš„è½¯ä»¶åŒ…ï¼Œä»¥ä¾¿ç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜èƒ½å¤Ÿå…è´¹ä½¿ç”¨ã€‚<details>
<summary>Abstract</summary>
Federated Learning (FL) represents a growing machine learning (ML) paradigm designed for training models across numerous nodes that retain local datasets, all without directly exchanging the underlying private data with the parameter server (PS). Its increasing popularity is attributed to notable advantages in terms of training deep neural network (DNN) models under privacy aspects and efficient utilization of communication resources. Unfortunately, DNNs suffer from high computational and communication costs, as well as memory consumption in intricate tasks. These factors restrict the applicability of FL algorithms in communication-constrained systems with limited hardware resources.   In this paper, we develop a novel algorithm that overcomes these limitations by synergistically combining a pruning-based method with the FL process, resulting in low-dimensional representations of the model with minimal communication cost, dubbed Masked Pruning over FL (MPFL). The algorithm operates by initially distributing weights to the nodes through the PS. Subsequently, each node locally trains its model and computes pruning masks. These low-dimensional masks are then transmitted back to the PS, which generates a consensus pruning mask, broadcasted back to the nodes. This iterative process enhances the robustness and stability of the masked pruning model. The generated mask is used to train the FL model, achieving significant bandwidth savings. We present an extensive experimental study demonstrating the superior performance of MPFL compared to existing methods. Additionally, we have developed an open-source software package for the benefit of researchers and developers in related fields.
</details>
<details>
<summary>æ‘˜è¦</summary>
federated learning (FL) æ˜¯ä¸€ç§åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šè®­ç»ƒæ¨¡å‹çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ paradigmaï¼Œä¸ç›´æ¥åœ¨å‚æ•°æœåŠ¡å™¨ï¼ˆPSï¼‰ä¸Šäº¤æ¢æœ¬åœ°ç§äººæ•°æ®ã€‚ç”±äºFLå…·æœ‰ä¿æŠ¤éšç§å’Œé«˜æ•ˆé€šä¿¡èµ„æºçš„ä¼˜åŠ¿ï¼Œå…¶ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€åº¦åœ¨ä¸æ–­å¢é•¿ã€‚ç„¶è€Œï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨å¤æ‚ä»»åŠ¡ä¸­å…·æœ‰é«˜è®¡ç®—å’Œé€šä¿¡æˆæœ¬ï¼Œä»¥åŠå†…å­˜å ç”¨ç‡ï¼Œè¿™äº›å› ç´ é™åˆ¶äº†FLç®—æ³•åœ¨å…·æœ‰æœ‰é™ç¡¬ä»¶èµ„æºçš„é€šä¿¡æŸç¼šç³»ç»Ÿä¸­çš„åº”ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ–°çš„ç®—æ³•ï¼Œå³Masked Pruning over FLï¼ˆMPFLï¼‰ï¼Œä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚MPFLç®—æ³•é¦–å…ˆå°†æƒé‡åˆ†å¸ƒç»™èŠ‚ç‚¹ Ñ‡ĞµÑ€ĞµĞ· PSã€‚ç„¶åï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ¬åœ°è®­ç»ƒå…¶æ¨¡å‹ï¼Œå¹¶è®¡ç®—é®ç›¾maskã€‚è¿™äº›ä½ç»´åº¦çš„maskå°†è¢«ä¼ è¾“å›PSï¼Œç”Ÿæˆä¸€ä¸ªconsensusé®ç›¾maskï¼Œå¹¶å°†å…¶å¹¿æ’­å›èŠ‚ç‚¹ã€‚è¿™ä¸ªè¿­ä»£è¿‡ç¨‹ä¼šæé«˜é®ç›¾é®ç›¾æ¨¡å‹çš„ç¨³å®šæ€§å’Œç¨³å®šæ€§ã€‚ç”Ÿæˆçš„é®ç›¾å¯ä»¥ç”¨æ¥è®­ç»ƒFLæ¨¡å‹ï¼Œå®ç°äº†æ˜æ˜¾çš„å¸¦å®½å‰Šå‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒç ”ç©¶ï¼Œè¯æ˜MPFLçš„æ€§èƒ½superiority compared to existing methodsã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªå¼€æºçš„è½¯ä»¶åŒ…ï¼Œä¸ºç›¸å…³é¢†åŸŸçš„ç ”ç©¶äººå‘˜å’Œå¼€å‘äººå‘˜æä¾›äº†ä¾¿åˆ©ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-The-Fairness-Impacts-of-Hardware-Selection-in-Machine-Learning"><a href="#On-The-Fairness-Impacts-of-Hardware-Selection-in-Machine-Learning" class="headerlink" title="On The Fairness Impacts of Hardware Selection in Machine Learning"></a>On The Fairness Impacts of Hardware Selection in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03886">http://arxiv.org/abs/2312.03886</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto</li>
<li>for:  investigates the impact of hardware choices on the generalization properties of machine learning models, particularly in the context of ML-as-a-service platforms.</li>
<li>methods:  combines theoretical and empirical analysis to identify the factors that contribute to hardware-induced performance imbalances, and proposes a strategy for mitigating these imbalances.</li>
<li>results:  demonstrates that hardware choices can exacerbate existing disparities in model performance and fairness, and provides insights into the underlying causes of these discrepancies.<details>
<summary>Abstract</summary>
In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.
</details>
<details>
<summary>æ‘˜è¦</summary>
Note:* ç¡¬ä»¶ (hÃ²u jiÃ n) means "hardware" in Simplified Chinese.* ML-as-a-service (MLaaS) is a cloud-based service that provides machine learning capabilities to users.* ç”¨æˆ· (yÃ²ng yÃ²u) means "user" in Simplified Chinese.* æ¨¡å‹ (mÃ³ delÃ¬) means "model" in Simplified Chinese.* æ€§èƒ½ (xÃ¬ng nÃ©ng) means "performance" in Simplified Chinese.* å…¬å¹³ (gÅng pÃ­ng) means "fairness" in Simplified Chinese.* ç¾¤ä½“ (qÃºn tÇ) means "demographic group" in Simplified Chinese.* æ¢¯åº¦æµ (dÃ o yÃ¹) means "gradient flow" in Simplified Chinese.* æŸå¤±è¡¨ (shÃ¨ shÃ¬ biÇo) means "loss surface" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="FoMo-Rewards-Can-we-cast-foundation-models-as-reward-functions"><a href="#FoMo-Rewards-Can-we-cast-foundation-models-as-reward-functions" class="headerlink" title="FoMo Rewards: Can we cast foundation models as reward functions?"></a>FoMo Rewards: Can we cast foundation models as reward functions?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03881">http://arxiv.org/abs/2312.03881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekdeep Singh Lubana, Johann Brehmer, Pim de Haan, Taco Cohen</li>
<li>for: ç ”ç©¶æ˜¯ç”¨åº•å±‚æ¨¡å‹ä½œä¸ºæ¿€åŠ±å­¦ä¹ çš„å¥–åŠ±å‡½æ•°çš„å¯èƒ½æ€§ã€‚</li>
<li>methods: æˆ‘ä»¬æè®®ä¸€ç§ç®€å•çš„æ‰¹å¤„ç†ï¼Œå°†å¯è§è¯­è¨€æ¨¡å‹ä¸å¤§å‹è¯­è¨€æ¨¡å‹é›†æˆã€‚ Specifically, ç»™ä¸€ä¸ªè½¨è¿¹çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æè¿°ä»»åŠ¡çš„ instrucion çš„å¯èƒ½æ€§ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°è¿™ä¸ªé€šç”¨çš„å¯èƒ½æ€§å‡½æ•°å…·æœ‰ç†æƒ³çš„å¥–åŠ±å‡½æ•°ç‰¹å¾ï¼šå®ƒä¸æ„¿æœ›çš„è¡Œä¸ºç›¸å…³ï¼Œè€Œä¸ç±»ä¼¼ä½†é”™è¯¯çš„ç­–ç•¥ç›¸å¯¹è¾ƒä½ã€‚ å…¨é¢æ¥è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼€å¯äº†é€šè¿‡åŸºç¡€æ¨¡å‹è®¾è®¡å¼€æ”¾å¼ä»»åŠ¡çš„å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
We explore the viability of casting foundation models as generic reward functions for reinforcement learning. To this end, we propose a simple pipeline that interfaces an off-the-shelf vision model with a large language model. Specifically, given a trajectory of observations, we infer the likelihood of an instruction describing the task that the user wants an agent to perform. We show that this generic likelihood function exhibits the characteristics ideally expected from a reward function: it associates high values with the desired behaviour and lower values for several similar, but incorrect policies. Overall, our work opens the possibility of designing open-ended agents for interactive tasks via foundation models.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-transformer-neural-networks-for-skillful-and-reliable-medium-range-weather-forecasting"><a href="#Scaling-transformer-neural-networks-for-skillful-and-reliable-medium-range-weather-forecasting" class="headerlink" title="Scaling transformer neural networks for skillful and reliable medium-range weather forecasting"></a>Scaling transformer neural networks for skillful and reliable medium-range weather forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03876">http://arxiv.org/abs/2312.03876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tung Nguyen, Rohan Shah, Hritik Bansal, Troy Arcomano, Sandeep Madireddy, Romit Maulik, Veerabhadra Kotamarthi, Ian Foster, Aditya Grover</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„å¤©æ°”é¢„æŠ¥æ–¹æ³•ï¼Œä»¥æé«˜å¤©æ°”é¢„æŠ¥çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§ç®€å•çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œç§°ä¸ºStormerï¼Œå…¶ä¸­åŒ…æ‹¬å¤©æ°”ç‰¹æœ‰çš„åµŒå…¥ã€éšæœºåŠ¨åŠ›é¢„æµ‹å’Œå‹åŠ›åŠ æƒæŸå¤±ç­‰å…³é”®ç»„ä»¶ã€‚</li>
<li>results: åœ¨WeatherBench 2ä¸Šï¼ŒStormeråœ¨çŸ­è‡³ä¸­èŒƒå›´é¢„æµ‹ task ä¸Šè¡¨ç°ç«äº‰æ€§ï¼Œè€Œåœ¨é•¿èŒƒå›´é¢„æµ‹ task ä¸Šè¶…è¿‡7å¤©çš„é¢„æµ‹ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè€Œä¸”éœ€è¦è®­ç»ƒæ•°æ®å’Œè®¡ç®—é‡çš„æå°‘ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜è¯æ˜Stormerçš„æ‰©å±•æ€§è‰¯å¥½ï¼Œéšç€æ¨¡å‹å¤§å°å’Œè®­ç»ƒç¤ºä¾‹çš„å¢åŠ ï¼Œé¢„æµ‹å‡†ç¡®æ€§éƒ½ä¼šæé«˜ã€‚<details>
<summary>Abstract</summary>
Weather forecasting is a fundamental problem for anticipating and mitigating the impacts of climate change. Recently, data-driven approaches for weather forecasting based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems. However, those methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what truly contributes to their success. Here we introduce Stormer, a simple transformer model that achieves state-of-the-art performance on weather forecasting with minimal changes to the standard transformer backbone. We identify the key components of Stormer through careful empirical analyses, including weather-specific embedding, randomized dynamics forecast, and pressure-weighted loss. At the core of Stormer is a randomized forecasting objective that trains the model to forecast the weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a target lead time and combine them to obtain better forecast accuracy. On WeatherBench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormer's favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens. Code and checkpoints will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤©æ°”é¢„æµ‹æ˜¯æ°”å€™å˜åŒ–çš„åŸºæœ¬é—®é¢˜ï¼Œå¯ä»¥é¢„æµ‹å’Œå‡è½»æ°”å€™å˜åŒ–çš„å½±å“ã€‚ç°åœ¨ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„å¤©æ°”é¢„æµ‹æ–¹æ³•å·²ç»æ˜¾ç¤ºå‡ºäº†å¾ˆå¤§çš„æ­é…ï¼Œå…·æœ‰ä¸æ“ä½œç³»ç»Ÿç›¸å½“çš„ç²¾åº¦ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸ä½¿ç”¨å¤æ‚çš„è‡ªå®šä¹‰æ¶æ„ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®åœ°äº†è§£å®ƒä»¬çš„æˆåŠŸåŸå› ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»äº†é£æš´ï¼ˆStormerï¼‰ï¼Œä¸€ç§ç®€å•çš„è½¬æ¢å™¨æ¨¡å‹ï¼Œå¯ä»¥åœ¨å¤©æ°”é¢„æµ‹ä¸­å®ç°æœ€ä½³æ€§èƒ½ï¼Œå¹¶ä¸”åªéœ€è¦å¾®å°çš„æ”¹å˜äºæ ‡å‡†è½¬æ¢å™¨è„Šæ¢ã€‚æˆ‘ä»¬é€šè¿‡ä»”ç»†çš„å®éªŒåˆ†æï¼ŒåŒ…æ‹¬ç‰¹å®šäºå¤©æ°”çš„åµŒå…¥ã€éšæœºåŠ¨åŠ›é¢„æµ‹å’Œå‹åŠ›WeightedæŸå¤±ï¼Œç¡®å®šäº†é£æš´çš„å…³é”®ç»„ä»¶ã€‚é£æš´çš„æ ¸å¿ƒæ˜¯ä¸€ç§éšæœºé¢„æµ‹ç›®æ ‡çš„å¯¹è±¡ï¼Œå¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´é—´éš”å†…é¢„æµ‹å¤©æ°”åŠ¨åŠ›ã€‚åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤šä¸ªé¢„æµ‹ï¼Œå¹¶å°†å…¶ç»„åˆä»¥è·å¾—æ›´å¥½çš„é¢„æµ‹ç²¾åº¦ã€‚åœ¨WeatherBench 2ä¸Šï¼Œé£æš´åœ¨çŸ­è‡³ä¸­æœŸé¢„æµ‹å’Œè¶…è¿‡7å¤©çš„é¢„æµ‹ä¸­è¡¨ç°ç«äº‰åŠ›å¼ºï¼ŒåŒæ—¶éœ€è¦è®­ç»ƒæ•°æ®å’Œè®¡ç®—é‡å‡å°‘åˆ°äº†å¤šä¸ªçº§åˆ«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†é£æš´çš„æœ‰åˆ©æ‰©å±•æ€§ï¼Œè¡¨ç°å‡ºäº†éšç€æ¨¡å‹å¤§å°å’Œè®­ç»ƒTokenæ•°é‡çš„ä¸æ–­æé«˜çš„é¢„æµ‹ç²¾åº¦ã€‚ä»£ç å’Œæ£€æŸ¥ç‚¹å°†å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="The-BigCode-Project-Governance-Card"><a href="#The-BigCode-Project-Governance-Card" class="headerlink" title="The BigCode Project Governance Card"></a>The BigCode Project Governance Card</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03872">http://arxiv.org/abs/2312.03872</a></li>
<li>repo_url: None</li>
<li>paper_authors: BigCode collaboration, Sean Hughes, Harm de Vries, Jennifer Robinson, Carlos MuÃ±oz Ferrandis, Loubna Ben Allal, Leandro von Werra, Jennifer Ding, Sebastien Paquet, Yacine Jernite</li>
<li>for: æœ¬æ–‡æ¦‚è¦æä¾›äº†BigCodeé¡¹ç›®çš„ä¸åŒæœºåˆ¶å’Œç®¡ç†é¢†åŸŸï¼Œä»¥æ”¯æŒé¡¹ç›®çš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†é¡¹ç›®ç»„ç»‡ç»“æ„ã€å®£è¨€ç›®æ ‡å’Œä»·å€¼è§‚ã€å†…éƒ¨å†³ç­–è¿‡ç¨‹ã€èµ„é‡‘å’Œèµ„æºç­‰æ–¹é¢çš„å‡ ä¸ªæœºåˆ¶æ¥æ”¯æŒé¡¹ç›®çš„ç®¡ç†ã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡æä¾›é¡¹ç›®çš„å„ä¸ªæœºåˆ¶å’Œé¢†åŸŸçš„ä¿¡æ¯ï¼Œå‘æ›´å¹¿æ³›çš„å…¬ä¼—æä¾›äº†é¡¹ç›®çš„é€æ˜åº¦å’Œå¯é‡å¤æ€§ï¼ŒåŒæ—¶ä¹Ÿä¸ºæœªæ¥çš„å¼€æºé¡¹ç›®æä¾›äº†ä¸€ä¸ªå¯ä»¿æ•ˆçš„å‚è€ƒã€‚<details>
<summary>Abstract</summary>
This document serves as an overview of the different mechanisms and areas of governance in the BigCode project. It aims to support transparency by providing relevant information about choices that were made during the project to the broader public, and to serve as an example of intentional governance of an open research project that future endeavors can leverage to shape their own approach. The first section, Project Structure, covers the project organization, its stated goals and values, its internal decision processes, and its funding and resources. The second section, Data and Model Governance, covers decisions relating to the questions of data subject consent, privacy, and model release.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä»½æ–‡æ¡£æä¾›äº†å¤§ç é¡¹ç›®ä¸åŒæœºåˆ¶å’Œç®¡ç†æ–¹é¢çš„æ¦‚è¿°ï¼Œä»¥ä¾¿æ”¯æŒé€æ˜åº¦ï¼Œä¸ºæ›´å¹¿æ³›çš„å…¬ä¼—æä¾›ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶ä½œä¸ºæœªæ¥é¡¹ç›®çš„ç¤ºèŒƒï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªæ–¹æ³•åˆ¶å®šè‡ªå·±çš„ç®¡ç†æ–¹å¼ã€‚é¦–éƒ¨åˆ†ï¼Œé¡¹ç›®ç»“æ„ï¼Œè¦†ç›–é¡¹ç›®ç»„ç»‡ç»“æ„ï¼Œé¡¹ç›®çš„å£°æ˜ç›®æ ‡å’Œä»·å€¼è§‚ï¼Œå†…éƒ¨å†³ç­–è¿‡ç¨‹ï¼Œä»¥åŠèµ„é‡‘å’Œèµ„æºã€‚ç¬¬äºŒéƒ¨åˆ†ï¼Œæ•°æ®å’Œæ¨¡å‹ç®¡ç†ï¼Œè¦†ç›–æ•°æ®ä¸»ä½“åŒæ„ã€éšç§å’Œæ¨¡å‹é‡Šå‡ºçš„å†³ç­–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Large-Language-Models-A-Survey"><a href="#Efficient-Large-Language-Models-A-Survey" class="headerlink" title="Efficient Large Language Models: A Survey"></a>Efficient Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03863">http://arxiv.org/abs/2312.03863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aiot-mlsys-lab/efficientllms">https://github.com/aiot-mlsys-lab/efficientllms</a></li>
<li>paper_authors: Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, Mi Zhang</li>
<li>for: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§å’Œå…¨é¢çš„LLMsæ•ˆç‡ç ”ç©¶ç»¼è¿°ï¼Œå¸®åŠ©ç ”ç©¶è€…å’Œå®è·µè€…æ›´å¥½åœ°äº†è§£LLMsæ•ˆç‡ç ”ç©¶çš„å‘å±•å’Œè¿›å±•ã€‚</li>
<li>methods: æœ¬æ–‡åˆ†ä¸ºä¸‰ä¸ªä¸»è¦ç±»åˆ«ï¼Œä»æ¨¡å‹ä¸­å¿ƒã€æ•°æ®ä¸­å¿ƒå’Œæ¡†æ¶ä¸­å¿ƒä¸‰ä¸ªè§’åº¦è¿›è¡Œç»¼è¿°ï¼Œå¹¶åœ¨GitHubä¸Šæä¾›äº†ç›¸å…³è®ºæ–‡çš„é›†æˆã€‚</li>
<li>results: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªç³»ç»Ÿæ€§å’Œå…¨é¢çš„LLMsæ•ˆç‡ç ”ç©¶ç»¼è¿°ï¼ŒåŒ…æ‹¬æ¨¡å‹ä¸­å¿ƒã€æ•°æ®ä¸­å¿ƒå’Œæ¡†æ¶ä¸­å¿ƒä¸‰ä¸ªè§’åº¦çš„ç ”ç©¶å‘å±•ï¼Œå¹¶å°†åœ¨GitHubä¸Šç»´æŠ¤å’Œæ›´æ–°ç›¸å…³è®ºæ–‡ã€‚<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding, language generation, and complex reasoning and have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/EfficientLLMs, https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é‡è¦çš„ä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œå¦‚è‡ªç„¶è¯­è¨€ç†è§£ã€è¯­è¨€ç”Ÿæˆå’Œå¤æ‚çš„æ¨ç†ï¼Œå¹¶æœ‰å¯èƒ½å¯¹ç¤¾ä¼šäº§ç”Ÿæ·±è¿œçš„å½±å“ã€‚ç„¶è€Œï¼Œè¿™äº›èƒ½åŠ›éœ€è¦å·¨å¤§çš„èµ„æºï¼Œ highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we compile the papers featured in this survey at <https://github.com/AIoT-MLSys-Lab/EfficientLLMs>, <https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey>, and will actively maintain this repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of the research developments in efficient LLMs and inspire them to contribute to this important and exciting field.
</details></li>
</ul>
<hr>
<h2 id="Alpha-CLIP-A-CLIP-Model-Focusing-on-Wherever-You-Want"><a href="#Alpha-CLIP-A-CLIP-Model-Focusing-on-Wherever-You-Want" class="headerlink" title="Alpha-CLIP: A CLIP Model Focusing on Wherever You Want"></a>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03818">http://arxiv.org/abs/2312.03818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sunzey/alphaclip">https://github.com/sunzey/alphaclip</a></li>
<li>paper_authors: Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, Jiaqi Wang</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜CLIPçš„å¯æ§æ€§ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç¼–è¾‘å›¾åƒã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªauxiliary alpha channelæ¥æŒ‡ç¤ºæ³¨æ„åŠ›çš„åŒºåŸŸï¼Œå¹¶é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹æ¥ fine-tune CLIPã€‚</li>
<li>results: Alpha-CLIPä¸ä»…ä¿ç•™äº†CLIPçš„è§†è§‰è®¤çŸ¥èƒ½åŠ›ï¼Œè¿˜å…è®¸ç²¾å‡†åœ°æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸Šè¾¾åˆ°äº†è‰¯å¥½çš„æ•ˆæœï¼ŒåŒ…æ‹¬å¼€æ”¾ä¸–ç•Œè®¤çŸ¥ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D&#x2F;3Dç”Ÿæˆã€‚<details>
<summary>Abstract</summary>
Contrastive Language-Image Pre-training (CLIP) plays an essential role in extracting valuable content information from images across diverse tasks. It aligns textual and visual modalities to comprehend the entire image, including all the details, even those irrelevant to specific tasks. However, for a finer understanding and controlled editing of images, it becomes crucial to focus on specific regions of interest, which can be indicated as points, masks, or boxes by humans or perception models. To fulfill the requirements, we introduce Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to suggest attentive regions and fine-tuned with constructed millions of RGBA region-text pairs. Alpha-CLIP not only preserves the visual recognition ability of CLIP but also enables precise control over the emphasis of image contents. It demonstrates effectiveness in various tasks, including but not limited to open-world recognition, multimodal large language models, and conditional 2D / 3D generation. It has a strong potential to serve as a versatile tool for image-related tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
<SYS>CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­æå–å›¾åƒä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚å®ƒå°†æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼è”ç³»èµ·æ¥ï¼Œä»¥ä¾¿å…¨é¢ç†è§£å›¾åƒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»†èŠ‚ï¼Œå³ä½¿ä¸ç‰¹å®šä»»åŠ¡æ— å…³ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´åŠ ç²¾ç»†åœ°ç†è§£å’Œæ§åˆ¶å›¾åƒï¼Œéœ€è¦ä¸“æ³¨äºç‰¹å®šåŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå¯ä»¥ç”±äººç±»æˆ–æ„ŸçŸ¥æ¨¡å‹æŒ‡å®šä¸ºç‚¹ã€é¢æˆ–ç›’å­ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»äº†Alpha-CLIPï¼Œå®ƒæ˜¯CLIPçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©çš„Î±é€šé“ï¼Œç”¨äºå»ºè®®æ³¨æ„çš„åŒºåŸŸï¼Œå¹¶ä¸”é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹è¿›è¡Œç²¾åº¦åœ°è°ƒæ•´ã€‚Alpha-CLIPä¸ä»…ä¿æŒäº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè¿˜å…è®¸æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ•ˆæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D/3Dç”Ÿæˆã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåœ¨åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥ç”¨äºå¤šç§å›¾åƒç›¸å…³ä»»åŠ¡ã€‚</SYS>Here's the translation in Simplified Chinese:CLIPï¼ˆå¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼‰åœ¨å¤šç§ä»»åŠ¡ä¸­æå–å›¾åƒä¸­çš„æœ‰ä»·å€¼ä¿¡æ¯æ‰®æ¼”ç€é‡è¦çš„è§’è‰²ã€‚å®ƒå°†æ–‡æœ¬å’Œè§†è§‰æ¨¡å¼è”ç³»èµ·æ¥ï¼Œä»¥ä¾¿å…¨é¢ç†è§£å›¾åƒï¼ŒåŒ…æ‹¬æ‰€æœ‰ç»†èŠ‚ï¼Œå³ä½¿ä¸ç‰¹å®šä»»åŠ¡æ— å…³ã€‚ç„¶è€Œï¼Œä¸ºäº†æ›´åŠ ç²¾ç»†åœ°ç†è§£å’Œæ§åˆ¶å›¾åƒï¼Œéœ€è¦ä¸“æ³¨äºç‰¹å®šåŒºåŸŸï¼Œè¿™äº›åŒºåŸŸå¯ä»¥ç”±äººç±»æˆ–æ„ŸçŸ¥æ¨¡å‹æŒ‡å®šä¸ºç‚¹ã€é¢æˆ–ç›’å­ã€‚ä¸ºäº†æ»¡è¶³è¿™äº›éœ€æ±‚ï¼Œæˆ‘ä»¬ä»‹ç»äº†Alpha-CLIPï¼Œå®ƒæ˜¯CLIPçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œå¸¦æœ‰ä¸€ä¸ªè¾…åŠ©çš„Î±é€šé“ï¼Œç”¨äºå»ºè®®æ³¨æ„çš„åŒºåŸŸï¼Œå¹¶ä¸”é€šè¿‡æ„å»ºäº†æ•°ç™¾ä¸‡ä¸ªRGBAåŒºåŸŸæ–‡æœ¬å¯¹è¿›è¡Œç²¾åº¦åœ°è°ƒæ•´ã€‚Alpha-CLIPä¸ä»…ä¿æŒäº†CLIPçš„è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼Œè¿˜å…è®¸æ§åˆ¶å›¾åƒå†…å®¹çš„å¼ºè°ƒã€‚å®ƒåœ¨å¤šç§ä»»åŠ¡ä¸­å±•ç°å‡ºäº†æ•ˆæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå¼€æ”¾ä¸–ç•Œè¯†åˆ«ã€å¤šModalå¤§è¯­è¨€æ¨¡å‹å’Œæ¡ä»¶2D/3Dç”Ÿæˆã€‚å®ƒå…·æœ‰å¼ºå¤§çš„æ½œåœ¨åº”ç”¨å‰æ™¯ï¼Œå¯ä»¥ç”¨äºå¤šç§å›¾åƒç›¸å…³ä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="OneLLM-One-Framework-to-Align-All-Modalities-with-Language"><a href="#OneLLM-One-Framework-to-Align-All-Modalities-with-Language" class="headerlink" title="OneLLM: One Framework to Align All Modalities with Language"></a>OneLLM: One Framework to Align All Modalities with Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03700">http://arxiv.org/abs/2312.03700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csuhan/onellm">https://github.com/csuhan/onellm</a></li>
<li>paper_authors: Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, Xiangyu Yue</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ç§å¯ä»¥åŒæ—¶å¤„ç†å¤šç§æ¨¡å¼çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œä»¥æé«˜æ¨¡å¼ç†è§£èƒ½åŠ›ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ä¸€ç§ç»Ÿä¸€æ¶æ„ï¼Œå°†å…«ç§æ¨¡å¼ä¸è¯­è¨€ç›¸alignï¼Œå¹¶é€šè¿‡è¿›ç¨‹å¼å¤šæ¨¡å¼å¯¹é½ç®¡é“æ¥å®ç°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä½¿ç”¨ä¸€ç§æ··åˆå¤šä¸ªå›¾åƒæŠ•å½±æ¨¡å—å’ŒåŠ¨æ€è·¯ç”±æ¥å»ºç«‹ä¸€ä¸ªé€šç”¨æŠ•å½±æ¨¡å—ï¼ˆUPMï¼‰ã€‚</li>
<li>results: åœ¨25ç§å¤šæ ·åŒ–çš„benchmarkä»»åŠ¡ä¸Šï¼ŒOneLLMè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬å¤šæ¨¡å¼captioningã€é—®ç­”å’Œæ¨ç†ç­‰ã€‚<details>
<summary>Abstract</summary>
Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we first train an image projection module to connect a vision encoder with LLM. Then, we build a universal projection module (UPM) by mixing multiple image projection modules and dynamic routing. Finally, we progressively align more modalities to LLM with the UPM. To fully leverage the potential of OneLLM in following instructions, we also curated a comprehensive multimodal instruction dataset, including 2M items from image, audio, video, point cloud, depth/normal map, IMU and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks, encompassing tasks such as multimodal captioning, question answering and reasoning, where it delivers excellent performance. Code, data, model and online demo are available at https://github.com/csuhan/OneLLM
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨æœ€è¿‘å·²ç»å¸å¼•äº†å¹¿æ³›çš„æ³¨æ„åŠ›ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œéƒ½æ˜¯åŸºäºç‰¹å®šæ¨¡å¼çš„ç¼–è§£oderï¼Œè¿™äº›ç¼–è§£oderé€šå¸¸å…·æœ‰ä¸åŒçš„æ¶æ„ï¼Œå¹¶ä¸”åªèƒ½å¤„ç†å¸¸è§çš„æ¨¡å¼ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OneLLMï¼Œä¸€ä¸ªèƒ½å¤Ÿå¯¹å…«ç§æ¨¡å¼è¿›è¡Œè¯­è¨€å¯¹åº”çš„ MLLMã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€ç¼–è§£oderå’Œä¸€ä¸ªè¿›ç¨‹å¼å¤šæ¨¡æ€å¯¹åº”ç®¡é“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å›¾åƒæŠ•å½±æ¨¡å—å°†è§†è§‰ç¼–ç å™¨ä¸LLMè¿æ¥èµ·æ¥ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªé€šç”¨æŠ•å½±æ¨¡å—ï¼ˆUPMï¼‰ï¼Œé€šè¿‡æ··åˆå¤šä¸ªå›¾åƒæŠ•å½±æ¨¡å—å’ŒåŠ¨æ€è·¯ç”±æ¥å®ç°ã€‚æœ€åï¼Œæˆ‘ä»¬é€æ¸å°†æ›´å¤šçš„æ¨¡å¼ä¸LLMå¯¹åº”ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨OneLLMåœ¨ seguir instrucciones ä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬è¿˜ç­¹é›†äº†ä¸€ä¸ªå…¨é¢çš„å¤šæ¨¡æ€æŒ‡ä»¤é›†ï¼ŒåŒ…æ‹¬200ä¸‡ä¸ªItemä»å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ã€ç‚¹äº‘ã€æ·±åº¦/æ­£å¸¸å›¾ã€IMUå’ŒfMRIå¤§è„‘æ´»åŠ¨ã€‚OneLLMåœ¨25ç§å¤šæ ·åŒ–çš„benchmarkä¸Šè¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬å¤šæ¨¡æ€æè¿°ã€é—®ç­”å’Œç†è§£ä»»åŠ¡ï¼Œå…¶è¡¨ç°å‡ºè‰²ã€‚ä»£ç ã€æ•°æ®ã€æ¨¡å‹å’Œåœ¨çº¿ç¤ºä¾‹å¯ä»¥åœ¨https://github.com/csuhan/OneLLM ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Intrinsic-Harmonization-for-Illumination-Aware-Compositing"><a href="#Intrinsic-Harmonization-for-Illumination-Aware-Compositing" class="headerlink" title="Intrinsic Harmonization for Illumination-Aware Compositing"></a>Intrinsic Harmonization for Illumination-Aware Compositing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03698">http://arxiv.org/abs/2312.03698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Careaga, S. Mahdi H. Miangoleh, YaÄŸÄ±z Aksoy</li>
<li>for: æé«˜å›¾åƒåˆæˆé•œåƒçš„çœŸå®æ„Ÿå’Œç…§æ˜å‡†ç¡®æ€§</li>
<li>methods: ä½¿ç”¨è‡ªä¸»è¶…vised illumination harmonizationæ–¹æ³•ï¼Œé€šè¿‡ä¼°ç®—ç®€å•çš„å…¨å±€ç…§æ˜æ¨¡å‹å¹¶ä½¿ç”¨ç½‘ç»œè¿›è¡Œä¿®æ­£ï¼Œå®ç°åŒ¹é…èƒŒæ™¯å’Œå‰æ™¯çš„ç…§æ˜å’Œé¢œè‰²è¡¨ç°</li>
<li>results: åœ¨å®é™…æ‹¼æ¥å›¾åƒä¸­æé«˜äº†çœŸå®æ„Ÿå’Œç…§æ˜å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡ç”¨æˆ·ç ”ç©¶å¾—åˆ°äº†å¯¹æ¯”å…ˆå‰æ–¹æ³•çš„Objective Measurement of enhanced realism<details>
<summary>Abstract</summary>
Despite significant advancements in network-based image harmonization techniques, there still exists a domain disparity between typical training pairs and real-world composites encountered during inference. Most existing methods are trained to reverse global edits made on segmented image regions, which fail to accurately capture the lighting inconsistencies between the foreground and background found in composited images. In this work, we introduce a self-supervised illumination harmonization approach formulated in the intrinsic image domain. First, we estimate a simple global lighting model from mid-level vision representations to generate a rough shading for the foreground region. A network then refines this inferred shading to generate a harmonious re-shading that aligns with the background scene. In order to match the color appearance of the foreground and background, we utilize ideas from prior harmonization approaches to perform parameterized image edits in the albedo domain. To validate the effectiveness of our approach, we present results from challenging real-world composites and conduct a user study to objectively measure the enhanced realism achieved compared to state-of-the-art harmonization methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡ç½‘ç»œåŸºäºå›¾åƒåè°ƒæŠ€æœ¯å·²ç»å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å­˜åœ¨åŸŸåä¸ä¸€è‡´é—®é¢˜ã€‚å¤§å¤šæ•°ç°æœ‰æ–¹æ³•æ˜¯é€šè¿‡åå‘å…¨å±€ç¼–è¾‘ segmented å›¾åƒåŒºåŸŸæ¥é€†è½¬globalç¼–è¾‘ï¼Œä½†è¿™äº›æ–¹æ³•é€šå¸¸æ— æ³•å‡†ç¡®æ•æ‰èƒŒæ™¯å’Œå‰æ™¯ä¹‹é—´çš„å…‰ç…§ä¸åŒ¹é…é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§è‡ªåŠ¨åè°ƒç…§æ˜æ–¹æ³•ï¼ŒåŸºäºä¸­ç­‰çº§è§†è§‰è¡¨ç¤ºæ¥ä¼°ç®—ç®€å•çš„å…¨å±€ç…§æ˜æ¨¡å‹ï¼Œå¹¶å°†å…¶ç”¨äºç”Ÿæˆä¸èƒŒæ™¯åœºæ™¯ç›¸åŒ¹é…çš„é‡æ–°ç…§æ˜ã€‚ä¸ºäº†ä¿æŒå‰æ™¯å’ŒèƒŒæ™¯çš„é¢œè‰²å‡ºç°ç›¸ä¼¼ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†ä¹‹å‰çš„åè°ƒæ–¹æ³•æ¥è¿›è¡Œå‚æ•°åŒ–çš„å›¾åƒç¼–è¾‘ï¼Œå¹¶åœ¨ albedo é¢‘è°±ä¸­è¿›è¡Œè¿™äº›ç¼–è¾‘ã€‚ä¸ºäº†è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬åœ¨å®é™…æ‹æ‘„çš„å¤æ‚å›¾åƒä¸­å±•ç¤ºäº†ç»“æœï¼Œå¹¶è¿›è¡Œäº†ç”¨æˆ·ç ”ç©¶æ¥ Ğ¾Ğ±ÑŠĞµĞºively æµ‹é‡æˆ‘ä»¬çš„æ–¹æ³•ä¸ç°æœ‰åè°ƒæ–¹æ³•ç›¸æ¯”çš„å¢å¼ºç°å®æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="MatterGen-a-generative-model-for-inorganic-materials-design"><a href="#MatterGen-a-generative-model-for-inorganic-materials-design" class="headerlink" title="MatterGen: a generative model for inorganic materials design"></a>MatterGen: a generative model for inorganic materials design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03687">http://arxiv.org/abs/2312.03687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudio Zeni, Robert Pinsler, Daniel ZÃ¼gner, Andrew Fowler, Matthew Horton, Xiang Fu, Sasha Shysheya, Jonathan CrabbÃ©, Lixin Sun, Jake Smith, Ryota Tomioka, Tian Xie</li>
<li>For: The paper aims to develop a new generative model for designing functional materials with desired properties, particularly focusing on stability and novelty.* Methods: The proposed model, called MatterGen, uses a diffusion-based generative process that refines atom types, coordinates, and the periodic lattice to produce crystalline structures. Adapter modules are introduced to enable fine-tuning towards specific property constraints.* Results: MatterGen is able to generate stable, diverse inorganic materials across the periodic table, with a higher success rate and closer proximity to the local energy minimum compared to prior generative models. Fine-tuning the model allows for the design of materials with desired chemistry, symmetry, and multiple properties such as mechanical, electronic, and magnetic properties.<details>
<summary>Abstract</summary>
The design of functional materials with desired properties is essential in driving technological advances in areas like energy storage, catalysis, and carbon capture. Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. Despite recent progress, current generative models have low success rate in proposing stable crystals, or can only satisfy a very limited set of property constraints. Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic and magnetic properties. Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design.
</details>
<details>
<summary>æ‘˜è¦</summary>
ğŸ“ The design of functional materials with desired properties is crucial in driving technological advances in areas like energy storage, catalysis, and carbon capture. ğŸ”‹ Generative models provide a new paradigm for materials design by directly generating entirely novel materials given desired property constraints. ğŸ’¡ Despite recent progress, current generative models have low success rates in proposing stable crystals, or can only satisfy a very limited set of property constraints. ğŸ” Here, we present MatterGen, a model that generates stable, diverse inorganic materials across the periodic table and can further be fine-tuned to steer the generation towards a broad range of property constraints. ğŸ”© To enable this, we introduce a new diffusion-based generative process that produces crystalline structures by gradually refining atom types, coordinates, and the periodic lattice. ğŸ“Š We further introduce adapter modules to enable fine-tuning towards any given property constraints with a labeled dataset. ğŸ”— Compared to prior generative models, structures produced by MatterGen are more than twice as likely to be novel and stable, and more than 15 times closer to the local energy minimum. ğŸ”“ After fine-tuning, MatterGen successfully generates stable, novel materials with desired chemistry, symmetry, as well as mechanical, electronic, and magnetic properties. ğŸ” Finally, we demonstrate multi-property materials design capabilities by proposing structures that have both high magnetic density and a chemical composition with low supply-chain risk. ğŸ’ª We believe that the quality of generated materials and the breadth of MatterGen's capabilities represent a major advancement towards creating a universal generative model for materials design. ğŸŒŸ
</details></li>
</ul>
<hr>
<h2 id="LLM-as-OS-llmao-Agents-as-Apps-Envisioning-AIOS-Agents-and-the-AIOS-Agent-Ecosystem"><a href="#LLM-as-OS-llmao-Agents-as-Apps-Envisioning-AIOS-Agents-and-the-AIOS-Agent-Ecosystem" class="headerlink" title="LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem"></a>LLM as OS (llmao), Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03815">http://arxiv.org/abs/2312.03815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, Yongfeng Zhang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ„æ€ä¸€ä¸ªä»¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸ºåŸºç¡€çš„äººå·¥æ™ºèƒ½æ“ä½œç³»ç»Ÿï¼ˆAIOSï¼‰ç”Ÿæ€ç³»ç»Ÿï¼Œè¿™å°†æ ‡å¿—ç€æ“ä½œç³»ç»Ÿçš„ä¸€ä¸ªæ–° paradigma shiftã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºæ“ä½œç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶å¼€å‘äº†ä¸€ç³»åˆ—åŸºäº LLM çš„äººå·¥æ™ºèƒ½ä»£ç†åº”ç”¨ç¨‹åºï¼ˆAAPï¼‰ï¼Œä»¥æ¨åŠ¨ AIOS ç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ã€‚</li>
<li>results: æœ¬è®ºæ–‡é¢„æµ‹ï¼Œé€šè¿‡ LLM çš„åº”ç”¨ï¼Œå°†ä¸ä»…æ”¹å˜äººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºçš„æ°´å¹³ï¼Œè¿˜ä¼šé‡æ–°å®šä¹‰è®¡ç®—æœºç³»ç»Ÿçš„è®¾è®¡å’Œå®ç°ã€è½¯ä»¶å’Œç¼–ç¨‹è¯­è¨€çš„è®¾è®¡æ–¹æ³•ï¼Œå¹¶å¸¦æ¥ä¸€ç³»åˆ—æ–°çš„ç¡¬ä»¶å’Œä¸­é—´ä»¶è®¾å¤‡ã€‚<details>
<summary>Abstract</summary>
This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system ``with soul''. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM's impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level).
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æ‹Ÿæƒ³ä¸€ä¸ªé©å‘½æ€§çš„AIOSæŠ•é€ç”Ÿæ€ç³»ç»Ÿï¼Œå…¶ä¸­å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºäººå·¥æ™ºèƒ½æ“ä½œç³»ç»Ÿï¼ˆIOSæˆ–AIOSï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªâ€œæœ‰å¿ƒâ€çš„æ“ä½œç³»ç»Ÿã€‚åœ¨è¿™ä¸ªåŸºç¡€ä¸Šï¼Œä¸€äº›LLMåŸºäºçš„AIåº”ç”¨ç¨‹åºï¼ˆAgentæˆ–AAPï¼‰è¢«å¼€å‘å‡ºæ¥ï¼Œrichäº†AIOSæŠ•é€ç”Ÿæ€ç³»ç»Ÿï¼Œæ ‡å¿—ç€ä¼ ç»ŸOS-APPç”Ÿæ€ç³»ç»Ÿçš„ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Shiftã€‚æˆ‘ä»¬æƒ³è±¡ï¼ŒLLMçš„å½±å“ä¸å°†æ­¢äºAIåº”ç”¨ç¨‹åºå±‚æ¬¡ï¼Œåä¹‹ï¼Œå®ƒä¼šé©å‘½åŒ–è®¡ç®—æœºç³»ç»Ÿçš„è®¾è®¡å’Œå®ç°ã€è½¯ä»¶æ¶æ„å’Œç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šLLMä½œä¸ºç³»ç»Ÿå±‚æ¬¡ï¼ˆsystem-levelï¼‰ï¼Œä»£ç†ä¸ºåº”ç”¨ç¨‹åºå±‚æ¬¡ï¼ˆapplication-levelï¼‰ï¼Œè‡ªç„¶è¯­è¨€ä½œä¸ºç”¨æˆ·å±‚æ¬¡ï¼ˆuser-levelï¼‰ï¼Œå·¥å…·ä½œä¸ºç¡¬ä»¶/ä¸­é—´ä»¶å±‚æ¬¡ï¼ˆhardware/middleware-levelï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="What-Planning-Problems-Can-A-Relational-Neural-Network-Solve"><a href="#What-Planning-Problems-Can-A-Relational-Neural-Network-Solve" class="headerlink" title="What Planning Problems Can A Relational Neural Network Solve?"></a>What Planning Problems Can A Relational Neural Network Solve?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03682">http://arxiv.org/abs/2312.03682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/concepts-ai/goal-regression-width">https://github.com/concepts-ai/goal-regression-width</a></li>
<li>paper_authors: Jiayuan Mao, TomÃ¡s Lozano-PÃ©rez, Joshua B. Tenenbaum, Leslie Pack Kaelbling</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨goal-conditioned policiesæ˜¯å¦‚ä½•è¢«å­¦ä¹ çš„ï¼Œä»¥åŠå…¶æ•ˆç‡å¦‚ä½•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨circuit complexity analysiså’Œserialized goal regression searchï¼ˆS-GRSï¼‰æ¥ç ”ç©¶relational neural networksè¡¨ç¤ºçš„ç­–ç•¥å­¦ä¹ é—®é¢˜ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°æœ‰ä¸‰ç±»è®¡åˆ’é—®é¢˜ï¼Œå…¶å®½åº¦å’Œæ·±åº¦éšç€ç‰©å“å’Œè§„åˆ’è·ç¦»çš„å¢åŠ è€Œå¢é•¿ï¼Œå¹¶æä¾›äº†æ„é€ æ€§çš„è¯æ˜ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜è¯æ˜äº†è¿™ç§åˆ†æçš„å®ç”¨æ€§äºç­–ç•¥å­¦ä¹ ä¸­ã€‚<details>
<summary>Abstract</summary>
Goal-conditioned policies are generally understood to be "feed-forward" circuits, in the form of neural networks that map from the current state and the goal specification to the next action to take. However, under what circumstances such a policy can be learned and how efficient the policy will be are not well understood. In this paper, we present a circuit complexity analysis for relational neural networks (such as graph neural networks and transformers) representing policies for planning problems, by drawing connections with serialized goal regression search (S-GRS). We show that there are three general classes of planning problems, in terms of the growth of circuit width and depth as a function of the number of objects and planning horizon, providing constructive proofs. We also illustrate the utility of this analysis for designing neural networks for policy learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç›®æ ‡æ¡ä»¶æ”¿ç­–é€šå¸¸è¢«ç†è§£ä¸ºâ€œå‰å‘â€Circuitï¼Œå³ç¥ç»ç½‘ç»œï¼Œå°†å½“å‰çŠ¶æ€å’Œç›®æ ‡è§„èŒƒæ˜ å°„åˆ°ä¸‹ä¸€ä¸ªè¡ŒåŠ¨ã€‚ç„¶è€Œï¼Œå­¦ä¹ è¿™ç§ç­–ç•¥çš„æƒ…å†µå’Œæ•ˆç‡å°šä¸å¤Ÿæ¸…æ¥šã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»ç¥ç»ç½‘ç»œï¼ˆå¦‚å›¾ç¥ç»ç½‘ç»œå’Œå˜æ¢å™¨ï¼‰è¡¨ç¤ºç­–ç•¥çš„ç”µè·¯å¤æ‚åº¦åˆ†æï¼Œé€šè¿‡ä¸åºåˆ—åŒ–ç›®æ ‡å›å½’æœç´¢ï¼ˆS-GRSï¼‰çš„è¿æ¥ã€‚æˆ‘ä»¬è¯æ˜äº†è®¡åˆ’é—®é¢˜çš„ä¸‰ç±»æ€»ä½“æƒ…å†µï¼Œå³ç”µè·¯å®½åº¦å’Œæ·±åº¦éšç‰©å“å’Œè§„åˆ’æ—¶é—´çš„å¢åŠ æƒ…å†µï¼Œå¹¶æä¾›äº†æ„é€ æ€§è¯æ˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ Illustrates the utility of this analysis for designing neural networks for policy learning.
</details></li>
</ul>
<hr>
<h2 id="An-Integration-of-Pre-Trained-Speech-and-Language-Models-for-End-to-End-Speech-Recognition"><a href="#An-Integration-of-Pre-Trained-Speech-and-Language-Models-for-End-to-End-Speech-Recognition" class="headerlink" title="An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition"></a>An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03668">http://arxiv.org/abs/2312.03668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukiya Hono, Koh Mitsuda, Tianyu Zhao, Kentaro Mitsui, Toshiaki Wakatsuki, Kei Sawada</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒè¯­éŸ³å’Œè‡ªç„¶è¯­è¨€æ¨¡å‹çš„ç«¯åˆ°ç«¯è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ¨¡å‹ï¼Œä»¥ä¾¿å®ç°æ›´é«˜æ•ˆçš„è¯­éŸ³è¯†åˆ«ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†é¢„è®­ç»ƒè¯­éŸ³è¡¨ç¤ºæ¨¡å‹å’Œå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»„åˆï¼Œé€šè¿‡å°†è¯­éŸ³è¡¨ç¤ºè½¬æ¢ä¸ºæ–‡æœ¬tokenï¼Œå¹¶ä½¿ç”¨LLMçš„åºå¤§çŸ¥è¯†è¿›è¡Œ autoregressive ç”Ÿæˆï¼Œå®ç°ç«¯åˆ°ç«¯ ASRã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹å¯ä»¥ä¸ç°ä»£ç«¯åˆ°ç«¯ ASR æ¨¡å‹ç›¸æ¯”ï¼Œå¹¶ä¸”å¯ä»¥è¿›è¡Œ parameter-efficient é¢„æµ‹ä¼˜åŒ–å’Œé¢„è®­ç»ƒåŸŸè½¬æ¢ã€‚<details>
<summary>Abstract</summary>
Advances in machine learning have made it possible to perform various text and speech processing tasks, including automatic speech recognition (ASR), in an end-to-end (E2E) manner. Since typical E2E approaches require large amounts of training data and resources, leveraging pre-trained foundation models instead of training from scratch is gaining attention. Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:éšç€æœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼Œå¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ–¹å¼å®Œæˆä¸åŒçš„æ–‡æœ¬å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ Typical E2E Approaches éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œå› æ­¤åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è€Œä¸æ˜¯ä»scratch è®­ç»ƒæ˜¯æ”¶åˆ°å…³æ³¨ã€‚ Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.Translated into Traditional Chinese:éšç€æœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼Œå¯ä»¥ä½¿ç”¨ç«¯åˆ°ç«¯ï¼ˆE2Eï¼‰æ–¹å¼å®Œæˆä¸åŒçš„æ–‡æœ¬å’Œè¯­éŸ³å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€‚ Typical E2E Approaches éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®å’Œèµ„æºï¼Œå› æ­¤åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹è€Œä¸æ˜¯ä»scratch è®­ç»ƒæ˜¯æ”¶åˆ°å…³æ³¨ã€‚ Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.
</details></li>
</ul>
<hr>
<h2 id="Generative-agent-based-modeling-with-actions-grounded-in-physical-social-or-digital-space-using-Concordia"><a href="#Generative-agent-based-modeling-with-actions-grounded-in-physical-social-or-digital-space-using-Concordia" class="headerlink" title="Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia"></a>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03664">http://arxiv.org/abs/2312.03664</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/concordia">https://github.com/google-deepmind/concordia</a></li>
<li>paper_authors: Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A. DuÃ©Ã±ez-GuzmÃ¡n, William A. Cunningham, Simon Osindero, Danny Karmon, Joel Z. Leibo</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æ¢è®¨ Agent-based modeling å¦‚ä½•åˆ©ç”¨ Large Language Models (LLM) æé«˜æ¨¡å‹çš„å¯ç†è§£æ€§å’Œå¯è¡Œæ€§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† Concordia åº“ï¼Œç”¨äºæ„å»ºå’Œä½¿ç”¨è¯­è¨€åª’ä»‹çš„agent-basedæ¨¡å‹ã€‚Concordia ä½¿ç”¨ LLM æ¥åº”ç”¨å¸¸è¯†ï¼Œè¡Œä¸ºç†è§£ã€è®°å¿†å¸¸è¯†çŸ¥è¯†ï¼Œå¹¶é€šè¿‡ API è°ƒç”¨æ§åˆ¶æ•°å­—æŠ€æœ¯ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ Agent-based modeling æ–¹æ³•ï¼Œå¯ä»¥åœ¨physically-æˆ– digitally-grounded environmentsä¸­å®ç°è¯­è¨€åª’ä»‹çš„ simulationsã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ”¯æŒå¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬ç§‘å­¦ç ”ç©¶å’Œè¯„ä¼°å®é™…çš„æ•°å­—æœåŠ¡æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Agent-based modeling has been around for decades, and applied widely across the social and natural sciences. The scope of this research method is now poised to grow dramatically as it absorbs the new affordances provided by Large Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just classic Agent-Based Models (ABM)s where the agents talk to one another. Rather, GABMs are constructed using an LLM to apply common sense to situations, act "reasonably", recall common semantic knowledge, produce API calls to control digital technologies like apps, and communicate both within the simulation and to researchers viewing it from the outside. Here we present Concordia, a library to facilitate constructing and working with GABMs. Concordia makes it easy to construct language-mediated simulations of physically- or digitally-grounded environments. Concordia agents produce their behavior using a flexible component system which mediates between two fundamental operations: LLM calls and associative memory retrieval. A special agent called the Game Master (GM), which was inspired by tabletop role-playing games, is responsible for simulating the environment where the agents interact. Agents take actions by describing what they want to do in natural language. The GM then translates their actions into appropriate implementations. In a simulated physical world, the GM checks the physical plausibility of agent actions and describes their effects. In digital environments simulating technologies such as apps and services, the GM may handle API calls to integrate with external tools such as general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar, Email, Search, etc.). Concordia was designed to support a wide array of applications both in scientific research and for evaluating performance of real digital services by simulating users and/or generating synthetic data.
</details>
<details>
<summary>æ‘˜è¦</summary>
agent-basedæ¨¡å‹å·²ç»å­˜åœ¨æ•°åå¹´ï¼Œå¹¶å¹¿æ³›åº”ç”¨äºç¤¾ä¼šå’Œè‡ªç„¶ç§‘å­¦é¢†åŸŸã€‚ç°åœ¨ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ–°ç‰¹æ€§çš„å‡ºç°ï¼Œ agent-basedæ¨¡å‹çš„èŒƒå›´å³å°†æ‰©å¤§å¾ˆå¤šã€‚ç”Ÿæˆå‹agent-basedæ¨¡å‹ï¼ˆGABMï¼‰ä¸ä»…æ˜¯ĞºĞ»Ğ°ÑÑĞ¸å‹agent-basedæ¨¡å‹ï¼ˆABMï¼‰ï¼Œwhere agents talk to each otherï¼Œè€Œæ˜¯é€šè¿‡ä½¿ç”¨LLMæ¥åº”ç”¨å¸¸è¯†ï¼Œè¡Œä¸ºâ€œåˆç†â€ï¼Œå›å¿†å¸¸è¯†çŸ¥è¯†ï¼Œç”ŸæˆAPIè°ƒç”¨æ¥æ§åˆ¶æ•°å­—æŠ€æœ¯ï¼Œå¦‚åº”ç”¨å’ŒæœåŠ¡ã€‚æˆ‘ä»¬ç°åœ¨åœ¨Concordiaåº“ä¸­æä¾›äº†ä¸€ç§æ–¹ä¾¿æ„å»ºå’Œä½¿ç”¨GABMçš„æ–¹æ³•ã€‚Concordiaå¯ä»¥å¸®åŠ©æ„å»ºè¯­è¨€åª’ä»‹çš„ç‰©ç†æˆ–æ•°å­—ç¯å¢ƒæ¨¡æ‹Ÿã€‚Concordiaä»£ç†äººä½¿ç”¨å¯å˜ç»„ä»¶ç³»ç»Ÿæ¥è°ƒç”¨LLMå’Œ associative memory Retrievalä¸¤ç§åŸºæœ¬æ“ä½œã€‚ä¸€ä¸ªç‰¹æ®Šçš„ä»£ç†äººcalled Game Masterï¼ˆGMï¼‰ï¼Œå®ƒ draws inspiration from tabletop role-playing gamesï¼Œè´Ÿè´£æ¨¡æ‹Ÿä»£ç†äººä¹‹é—´çš„ç¯å¢ƒã€‚ä»£ç†äººé€šè¿‡natural languageæè¿°è‡ªå·±çš„è¡Œä¸ºï¼Œè€ŒGMå°†å…¶è½¬åŒ–ä¸ºåˆé€‚çš„å®ç°ã€‚åœ¨æ¨¡æ‹Ÿçš„ç‰©ç†ä¸–ç•Œä¸­ï¼ŒGMæ£€æŸ¥ä»£ç†äººè¡Œä¸ºçš„ç‰©ç†å¯èƒ½æ€§ï¼Œå¹¶æè¿°å…¶æ•ˆæœã€‚åœ¨æ¨¡æ‹Ÿæ•°å­—ç¯å¢ƒä¸­ï¼ŒGMå¯èƒ½å¤„ç†APIè°ƒç”¨ï¼Œä»¥ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°ble with external toolsï¼Œå¦‚é€šç”¨AIåŠ©æ‰‹ï¼ˆä¾‹å¦‚Bardã€ChatGPTï¼‰å’Œæ•°å­—åº”ç”¨ï¼ˆä¾‹å¦‚æ—¥å†ã€é‚®ä»¶ã€æœç´¢ç­‰ï¼‰ã€‚Concordiaæ˜¯ä¸ºäº†æ”¯æŒå¹¿æ³›çš„åº”ç”¨ï¼Œä»ç§‘å­¦ç ”ç©¶åˆ°è¯„ä¼°å®é™…æ•°å­—æœåŠ¡çš„æ€§èƒ½è€Œè®¾è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Pearl-A-Production-ready-Reinforcement-Learning-Agent"><a href="#Pearl-A-Production-ready-Reinforcement-Learning-Agent" class="headerlink" title="Pearl: A Production-ready Reinforcement Learning Agent"></a>Pearl: A Production-ready Reinforcement Learning Agent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03814">http://arxiv.org/abs/2312.03814</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/pearl">https://github.com/facebookresearch/pearl</a></li>
<li>paper_authors: Zheqing Zhu, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank Cheng, Zheng Wu, Wanqiao Xu</li>
<li>For: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æ¢è®¨RLæ¡†æ¶åœ¨å®ç°é•¿æœŸç›®æ ‡æ–¹é¢çš„ä¸€äº›é—®é¢˜ï¼ŒåŒ…æ‹¬å»¶è¿Ÿå¥–åŠ±ã€éƒ¨åˆ†å¯è§æ€§ã€æœç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„çŸ›ç›¾ã€ä½¿ç”¨ç¦»çº¿æ•°æ®æé«˜åœ¨çº¿æ€§èƒ½ã€å¹¶ç¡®ä¿å®‰å…¨é™åˆ¶å¾—åˆ°æ»¡è¶³ã€‚* Methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPearlçš„ç”Ÿäº§å‡†å¤‡RLæ™ºèƒ½ä»£ç†è½¯ä»¶åŒ…ï¼Œè¯¥åŒ…å¯ä»¥æ¨¡å—åŒ–åœ°è§£å†³RLè§£å†³æ–¹æ¡ˆä¸­çš„å„ç§é—®é¢˜ï¼ŒåŒ…æ‹¬å»¶è¿Ÿå¥–åŠ±ã€éƒ¨åˆ†å¯è§æ€§ã€æœç´¢å’Œåˆ©ç”¨ä¹‹é—´çš„çŸ›ç›¾ã€ä½¿ç”¨ç¦»çº¿æ•°æ®æé«˜åœ¨çº¿æ€§èƒ½ã€å¹¶ç¡®ä¿å®‰å…¨é™åˆ¶å¾—åˆ°æ»¡è¶³ã€‚* Results: è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€äº›åˆæ­¥çš„åŸºå‡†æµ‹è¯•ç»“æœï¼ŒåŒæ—¶ä¹Ÿ highlightsäº†Pearlåœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­çš„é‡‡çº³ï¼Œä»¥ demonstarteå…¶ç”Ÿäº§å‡†å¤‡æ€§ã€‚<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) offers a versatile framework for achieving long-term goals. Its generality allows us to formalize a wide range of problems that real-world intelligent systems encounter, such as dealing with delayed rewards, handling partial observability, addressing the exploration and exploitation dilemma, utilizing offline data to improve online performance, and ensuring safety constraints are met. Despite considerable progress made by the RL research community in addressing these issues, existing open-source RL libraries tend to focus on a narrow portion of the RL solution pipeline, leaving other aspects largely unattended. This paper introduces Pearl, a Production-ready RL agent software package explicitly designed to embrace these challenges in a modular fashion. In addition to presenting preliminary benchmark results, this paper highlights Pearl's industry adoptions to demonstrate its readiness for production usage. Pearl is open sourced on Github at github.com/facebookresearch/pearl and its official website is located at pearlagent.github.io.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Activation-Steering-in-Language-Models-with-Mean-Centring"><a href="#Improving-Activation-Steering-in-Language-Models-with-Mean-Centring" class="headerlink" title="Improving Activation Steering in Language Models with Mean-Centring"></a>Improving Activation Steering in Language Models with Mean-Centring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03813">http://arxiv.org/abs/2312.03813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ole Jorgensen, Dylan Cope, Nandi Schoots, Murray Shanahan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºæ§åˆ¶ï¼Œé€šè¿‡å‘ç°å¯¼èˆªå‘é‡ã€‚ä½†æ˜¯ï¼Œå·¥ç¨‹å¸ˆé€šå¸¸ä¸çŸ¥é“è¿™äº›æ¨¡å‹ä¸­ç‰¹å¾çš„è¡¨ç¤ºæ–¹å¼ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºä½¿ç”¨å‡å€¼ä¸­å¿ƒåŒ–å¯¼èˆªå‘é‡çš„æƒ³æ³•ï¼Œå³å–target datasetçš„æ´»åŠ¨å‡å€¼ï¼Œç„¶åå¯¹æ‰€æœ‰è®­ç»ƒæ´»åŠ¨å‡å€¼è¿›è¡Œå‡æ³•ã€‚è¿™ç§æ–¹æ³•åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸­è¢«è¯æ˜æœ‰æ•ˆï¼Œå¯ä»¥å¸®åŠ©æ§åˆ¶å¤§è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œé¿å…ç”Ÿæˆæ”»å‡»æ€§æ–‡æœ¬ï¼Œå¹¶è®©æ•…äº‹å®Œæˆtargetç±»å‹ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°ï¼Œå¯¹äºè‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œä½¿ç”¨å‡å€¼ä¸­å¿ƒåŒ–å¯¼èˆªå‘é‡å¯ä»¥å¤§å¹…æé«˜æ´»åŠ¨å¯¼èˆªçš„æ•ˆivenessï¼Œæ¯”ä¹‹å‰çš„åŸºelineæ›´é«˜ã€‚æ­¤å¤–ï¼Œè¿™ç§æ–¹æ³•è¿˜å¯ä»¥è®©æ¨¡å‹æ›´å¥½åœ°æ‰§è¡Œå„ç§è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œæ¯”å¦‚æ•…äº‹å®Œæˆå’Œæ–‡æœ¬ç”Ÿæˆç­‰ã€‚<details>
<summary>Abstract</summary>
Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘çš„æ´»åŠ¨å¯¼èˆªç ”ç©¶è¡¨æ˜å¯ä»¥æ›´å¥½åœ°æ§åˆ¶å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å‡ºï¼Œä½†æ˜¯å®ƒéœ€è¦æ‰¾åˆ°å¯¼èˆªå‘é‡ã€‚è¿™æ˜¯å› ä¸ºå·¥ç¨‹å¸ˆé€šå¸¸ä¸çŸ¥é“è¿™äº›æ¨¡å‹ä¸­ç‰¹å¾çš„è¡¨ç¤ºæ–¹å¼ã€‚æˆ‘ä»¬æƒ³è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡åº”ç”¨å‡å€¼ä¸­å¿ƒåŒ–æ€æƒ³æ¥æ”¹è¿›å¯¼èˆªå‘é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œå¯¹ç›®æ ‡æ•°æ®é›†çš„æ´»åŠ¨å‡å€¼ï¼Œå¹¶ä»æ‰€æœ‰è®­ç»ƒæ´»åŠ¨å‡å€¼ä¸­ subtract ç›®æ ‡æ•°æ®é›†çš„å‡å€¼ï¼Œå¯ä»¥è·å¾—æœ‰æ•ˆçš„å¯¼èˆªå‘é‡ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸Šæµ‹è¯•äº†è¿™ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬é¿å…ç”Ÿæˆæ¶æ„æ–‡æœ¬å’Œå¯¼èˆªæ•…äº‹çš„å®Œæˆæ–¹å‘ã€‚æˆ‘ä»¬è¿˜åº”ç”¨å‡å€¼ä¸­å¿ƒåŒ–æ¥æå–å‡½æ•°å‘é‡ï¼Œå¯ä»¥æ›´å¥½åœ°è§¦å‘å¤šç§è‡ªç„¶è¯­è¨€ä»»åŠ¡çš„æ‰§è¡Œï¼Œç›¸æ¯”ä¹‹å‰çš„åŸºçº¿ã€‚è¿™è¡¨ç¤ºï¼Œå‡å€¼ä¸­å¿ƒåŒ–å¯ä»¥ç”¨äºå¹¿æ³›æ”¹è¿› activation steering çš„æ•ˆivenessã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Inverse-Design-Optimization-through-Multi-fidelity-Simulations-Machine-Learning-and-Search-Space-Reduction-Strategies"><a href="#Efficient-Inverse-Design-Optimization-through-Multi-fidelity-Simulations-Machine-Learning-and-Search-Space-Reduction-Strategies" class="headerlink" title="Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies"></a>Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03654">http://arxiv.org/abs/2312.03654</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luka Grbcic, Juliane MÃ¼ller, Wibe Albert de Jong</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¢å¼ºé€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„çº¦æŸç¯å¢ƒï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¤šå…ƒé¢„æµ‹ã€æœºå™¨å­¦ä¹ æ¨¡å‹å’Œä¼˜åŒ–ç®—æ³•çš„è”ç›Ÿã€‚</li>
<li>methods: æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•Ğ¾Ğ»Ğ¾Ğ³Ğ¸ï¼Ÿï¼Œå°†æœºå™¨å­¦ä¹ æ¨¡å‹ä¸ä¼˜åŒ–ç®—æ³•è”ç›Ÿèµ·æ¥ï¼Œä»¥å¢å¼ºé€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œç²¾åº¦ã€‚åœ¨ä¸¤ä¸ªä¸åŒçš„å·¥ç¨‹é€†è®¾è®¡é—®é¢˜ä¸Šè¿›è¡Œäº†åˆ†æï¼Œå¹¶ä½¿ç”¨äº†ä½ç²¾åº¦æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é¢„æµ‹ç›®æ ‡å˜æ•°å’Œå†³å®šæ˜¯å¦éœ€è¦é«˜ç²¾åº¦æ¨¡æ‹Ÿã€‚</li>
<li>results: æœ¬è®ºæ–‡çš„ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¤§å¹…æé«˜é€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹çš„æ•ˆç‡å’Œç²¾åº¦ï¼Œå¹¶ä¸”å¯ä»¥ä¸ä¸åŒçš„ä¼˜åŒ–ç®—æ³•è”ç›Ÿä»¥å®ç°æ›´å¥½çš„ç»“æœã€‚å°¤å…¶æ˜¯åœ¨è®¡ç®—èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°ä¿ç•™è®¡ç®—èµ„æºï¼Œå¹¶ä¸”å¯ä»¥è®©é€†è®¾è®¡ä¼˜åŒ–è¿‡ç¨‹æ›´åŠ å¿«é€Ÿå’Œç¨³å®šã€‚<details>
<summary>Abstract</summary>
This paper introduces a methodology designed to augment the inverse design optimization process in scenarios constrained by limited compute, through the strategic synergy of multi-fidelity evaluations, machine learning models, and optimization algorithms. The proposed methodology is analyzed on two distinct engineering inverse design problems: airfoil inverse design and the scalar field reconstruction problem. It leverages a machine learning model trained with low-fidelity simulation data, in each optimization cycle, thereby proficiently predicting a target variable and discerning whether a high-fidelity simulation is necessitated, which notably conserves computational resources. Additionally, the machine learning model is strategically deployed prior to optimization to reduce the search space, thereby further accelerating convergence toward the optimal solution. The methodology has been employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization. Comparative analyses illustrate performance improvements across both algorithms. Notably, this method is adeptly adaptable across any inverse design application, facilitating a harmonious synergy between a representative low-fidelity machine learning model, and high-fidelity simulation, and can be seamlessly applied across any variety of population-based optimization algorithms.
</details>
<details>
<summary>æ‘˜è¦</summary>
The methodology uses a machine learning model trained with low-fidelity simulation data to predict a target variable in each optimization cycle. This approach conserves computational resources by only using high-fidelity simulations when necessary. Additionally, the machine learning model is deployed before optimization to reduce the search space, which further accelerates convergence towards the optimal solution.The methodology is employed to enhance two optimization algorithms, namely Differential Evolution and Particle Swarm Optimization. Comparative analyses show performance improvements across both algorithms. Notably, this method is adaptable to any inverse design application and can be seamlessly applied to any variety of population-based optimization algorithms.In simplified Chinese, the paper introduces a methodology that improves the inverse design optimization process in situations with limited computing resources. The methodology combines multi-fidelity evaluations, machine learning models, and optimization algorithms to achieve this goal. The proposed methodology is applied to two engineering inverse design problems and shows performance improvements across two optimization algorithms. This method is adaptable to any inverse design application and can be easily applied to any population-based optimization algorithm.
</details></li>
</ul>
<hr>
<h2 id="MotionCtrl-A-Unified-and-Flexible-Motion-Controller-for-Video-Generation"><a href="#MotionCtrl-A-Unified-and-Flexible-Motion-Controller-for-Video-Generation" class="headerlink" title="MotionCtrl: A Unified and Flexible Motion Controller for Video Generation"></a>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03641">http://arxiv.org/abs/2312.03641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§èƒ½å¤Ÿç²¾å‡†æ§åˆ¶ Ğ²Ğ¸Ğ´ĞµĞ¾ä¸­çš„æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨çš„åŠ¨ä½œæ§åˆ¶å™¨ï¼ˆMotionCtrlï¼‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„åŠ¨ä½œæ§åˆ¶å™¨æ¶æ„ï¼Œå®ƒç»¼åˆè€ƒè™‘äº†æ‘„åƒæœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨ä»¥åŠè®­ç»ƒæ•°æ®çš„ç‰¹æ€§ï¼Œä»¥æä¾›çµæ´»å’Œç²¾å‡†çš„åŠ¨ä½œæ§åˆ¶ã€‚</li>
<li>results: å¯¹æ¯”äºç°æœ‰çš„æ–¹æ³•ï¼ŒMotionCtrlå…·æœ‰ä¸‰å¤§ä¼˜åŠ¿ï¼š1ï¼‰å®ƒå¯ä»¥ç²¾å‡†åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ï¼Œå…è®¸æ›´ç»†è‡´çš„åŠ¨ä½œæ§åˆ¶å’Œå¤šæ ·åŒ–çš„åŠ¨ä½œç»„åˆã€‚2ï¼‰å®ƒçš„åŠ¨ä½œæ¡ä»¶ç”±æ‘„åƒæœºå§¿æ€å’Œè½¨è¿¹å†³å®šï¼Œè¿™äº›æ¡ä»¶æ˜¯å‡ºç°æ— å…³çš„å’Œå¯¹ç‰©ä½“å½¢çŠ¶æˆ–å¤–è§‚çš„å½±å“æœ€å°ã€‚3ï¼‰å®ƒæ˜¯ä¸€ç§ç›¸å¯¹é€šç”¨çš„æ¨¡å‹ï¼Œå¯ä»¥é€‚åº”å„ç§æ‘„åƒæœºå§¿æ€å’Œè½¨è¿¹ã€‚ç»è¿‡å¹¿æ³›çš„è´¨é‡å’Œé‡æµ‹è¯•ï¼ŒMotionCtrlåœ¨ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒæ—¶è¡¨ç°å‡ºäº†è¶…è¶Šæ€§ã€‚<details>
<summary>Abstract</summary>
Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸»è¦çš„åŠ¨ä½œåœ¨å½±ç‰‡ä¸­åŒ…æ‹¬æ‘„åƒæœºè¿åŠ¨æ‰€å¼•èµ·çš„æ‘„åƒæœºè¿åŠ¨å’Œç‰©ä½“è¿åŠ¨ã€‚ç²¾ç¡®æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨æ˜¯å½±ç‰‡ç”Ÿæˆçš„é‡ç‚¹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„å·¥ä½œå‡ ä¹ä¸“æ³¨äºä¸€ç§ç±»å‹çš„åŠ¨ä½œæˆ–æ²¡æœ‰æ¸…æ™°åœ°åŒºåˆ†è¿™ä¸¤ç§åŠ¨ä½œï¼Œè¿™é™åˆ¶äº†å®ƒä»¬çš„æ§åˆ¶èƒ½åŠ›å’Œå¤šæ ·æ€§ã€‚å› æ­¤ï¼Œè¿™ç¯‡è®ºæ–‡æå‡ºäº† MotionCtrlï¼Œä¸€ä¸ªç»Ÿä¸€å’Œ flexibleçš„åŠ¨ä½œæ§åˆ¶å™¨ï¼Œç”¨äºå½±ç‰‡ç”Ÿæˆï¼Œå¯ä»¥ç²¾ç¡®åœ°å’Œç‹¬ç«‹åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ã€‚ MotionCtrl çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥å……åˆ†è€ƒè™‘äº†æ‘„åƒæœºè¿åŠ¨ã€ç‰©ä½“è¿åŠ¨å’Œè®­ç»ƒæ•°æ®çš„è‡ªç„¶æ€§ã€‚ç›¸æ¯”äºå…ˆå‰çš„æ–¹æ³•ï¼ŒMotionCtrl æä¾›äº†ä¸‰å¤§ä¼˜ç‚¹ï¼š1. å¯ä»¥ç²¾ç¡®åœ°å’Œç‹¬ç«‹åœ°æ§åˆ¶æ‘„åƒæœºå’Œç‰©ä½“è¿åŠ¨ï¼Œå®ç°æ›´ç»†éƒ¨çš„åŠ¨ä½œæ§åˆ¶å’Œè®©ç”Ÿæˆçš„å½±ç‰‡æ›´å¤šæ ·åŒ–ã€‚2. å…¶åŠ¨ä½œæ¡ä»¶ç”±æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹å†³å®šï¼Œè¿™äº›æ¡ä»¶æ˜¯æ— å½¢æ„Ÿå’Œç‰©ä½“å½¢çŠ¶çš„å½±å“æœ€å°çš„ã€‚3. å®ƒæ˜¯ä¸€ä¸ªç›¸å¯¹ä¸€èˆ¬åŒ–çš„æ¨¡å‹ï¼Œå¯ä»¥é€‚åº”å¹¿æ³›çš„æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹ã€‚å®é™…å®éªŒè¡¨æ˜ï¼ŒMotionCtrl åœ¨è®­ç»ƒåå¯ä»¥å¯¹å¤šç§æ‘„åƒæœºä½ç½®å’Œè½¨è¿¹è¿›è¡Œé€‚åº”ã€‚
</details></li>
</ul>
<hr>
<h2 id="Not-All-Large-Language-Models-LLMs-Succumb-to-the-â€œReversal-Curseâ€-A-Comparative-Study-of-Deductive-Logical-Reasoning-in-BERT-and-GPT-Models"><a href="#Not-All-Large-Language-Models-LLMs-Succumb-to-the-â€œReversal-Curseâ€-A-Comparative-Study-of-Deductive-Logical-Reasoning-in-BERT-and-GPT-Models" class="headerlink" title="Not All Large Language Models (LLMs) Succumb to the â€œReversal Curseâ€: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models"></a>Not All Large Language Models (LLMs) Succumb to the â€œReversal Curseâ€: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03633">http://arxiv.org/abs/2312.03633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingye Yang, Da Wu, Kai Wang</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨è‡ªåŠ¨é€†æ¨Decoderå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨â€œAæ˜¯Bâ€çš„æƒ…å†µä¸‹å¤±è´¥å­¦ä¹ â€œBæ˜¯Aâ€ï¼Œæ¢è®¨è¿™ç§é€†æ¨çš„åŸºæœ¬å¤±è´¥æ˜¯å¦å¯¹æŸäº›é€šç”¨ä»»åŠ¡ï¼Œå¦‚æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæä¾›äº†çº¢flagã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº† bidirectional LLMï¼ˆBERTï¼‰ï¼Œå¹¶å‘ç°å®ƒå…·æœ‰é€†æ¨ç¥¸å®³çš„å…ç–«åŠ›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è¯„ä¼°äº†æ›´å¤æ‚çš„é€»è¾‘æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé›†åˆï¼ˆunionå’Œintersectionï¼‰æ“ä½œçš„äº¤å å’Œèåˆã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨ä¸¤ä¸ªé›†åˆæ“ä½œçš„æƒ…å†µä¸‹ï¼Œ both encoderå’Œdecoderè¯­è¨€æ¨¡å‹éƒ½èƒ½å¤Ÿè¡¨ç°å‡ºè‰²ï¼Œä½†æ˜¯åœ¨ä¸‰ä¸ªé›†åˆæ“ä½œçš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬é‡åˆ°äº†å›°éš¾ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼Œencoderå’Œdecoderæ¨¡å‹åœ¨ç®€å•å’Œå¤æ‚é€»è¾‘æ¨ç†ä¸­æœ‰æ‰€ä¸åŒï¼Œå¹¶ä¸”åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€‰æ‹©BERTæˆ–GPTåº”è¯¥æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œç‰¹ç‚¹ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨å®ƒä»¬çš„ç‰¹ç‚¹ã€‚<details>
<summary>Abstract</summary>
The "Reversal Curse" refers to the scenario where auto-regressive decoder large language models (LLMs), such as ChatGPT, trained on "A is B" fail to learn "B is A", demonstrating a basic failure of logical deduction. This raises a red flag in the use of GPT models for certain general tasks such as constructing knowledge graphs, considering their adherence to this symmetric principle. In our study, we examined a bidirectional LLM, BERT, and found that it is immune to the reversal curse. Driven by ongoing efforts to construct biomedical knowledge graphs with LLMs, we also embarked on evaluating more complex but essential deductive reasoning capabilities. This process included first training encoder and decoder language models to master the intersection ($\cap$) and union ($\cup$) operations on two sets and then moving on to assess their capability to infer different combinations of union ($\cup$) and intersection ($\cap$) operations on three newly created sets. The findings showed that while both encoder and decoder language models, trained for tasks involving two sets (union/intersection), were proficient in such scenarios, they encountered difficulties when dealing with operations that included three sets (various combinations of union and intersection). Our research highlights the distinct characteristics of encoder and decoder models in simple and complex logical reasoning. In practice, the choice between BERT and GPT should be guided by the specific requirements and nature of the task at hand, leveraging their respective strengths in bidirectional context comprehension and sequence prediction.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œé€†è½¬å’’â€æŒ‡çš„æ˜¯ï¼Œä½¿ç”¨â€œAæ˜¯Bâ€çš„è‡ªåŠ¨é€†è½¬æ•°æ®æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ChatGPTï¼Œå´æ— æ³•å­¦ä¹ â€œBæ˜¯Aâ€ï¼Œè¿™è¡¨ç¤ºäº†åŸºæœ¬çš„é€»è¾‘æ¨ç†å¤±è´¥ã€‚è¿™å¼•èµ·äº†ä½¿ç”¨GPTæ¨¡å‹çš„ä¸€äº›é€šç”¨ä»»åŠ¡ï¼Œå¦‚å»ºç«‹çŸ¥è¯†å›¾ï¼Œéœ€è¦æ³¨æ„è¿™ä¸ªå¯¹ç§°åŸç†ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€ä¸ªå¯¹å‘æ¨¡å‹ï¼ˆBERTï¼‰ï¼Œå‘ç°å®ƒå…å—â€œé€†è½¬å’’â€çš„å½±å“ã€‚ä¸ºäº†ç»§ç»­ä½¿ç”¨LLMå»ºç«‹ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†å›¾ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è¯„ä¼°æ›´å¤æ‚ä½†é‡è¦çš„æ¨ç†èƒ½åŠ›ã€‚è¿™åŒ…æ‹¬å…ˆå°†è¯­è¨€æ¨¡å‹è®­ç»ƒåˆ°æŒæ¡ä¸¤ä¸ªé›†åˆçš„äº¤é›†ï¼ˆï¼‰å’Œunionï¼ˆï¼‰æ“ä½œï¼Œç„¶åè¯„ä¼°å®ƒä»¬åœ¨ä¸‰ä¸ªæ–°åˆ›å»ºçš„é›†åˆä¸Šè¿›è¡Œä¸åŒçš„äº¤é›†ï¼ˆï¼‰å’Œäº¤é›†ï¼ˆï¼‰æ“ä½œçš„èƒ½åŠ›ã€‚å‘ç°è™½ç„¶ä¸¤ä¸ªè¯­è¨€æ¨¡å‹ï¼Œåœ¨ä¸¤ä¸ªé›†åˆï¼ˆunion/intersectionï¼‰çš„ä»»åŠ¡ä¸Šéƒ½èƒ½å¤Ÿè¡¨ç°å‡ºè‰²ï¼Œä½†å½“é¢ä¸´ä¸‰ä¸ªé›†åˆæ—¶ï¼Œå®ƒä»¬å´é‡åˆ°äº†å›°éš¾ã€‚æˆ‘ä»¬çš„ç ”ç©¶æ˜¾ç¤ºäº†ä¸¤ä¸ªè¯­è¨€æ¨¡å‹åœ¨ç®€å•å’Œå¤æ‚é€»è¾‘æ¨ç†ä¸­çš„ç‰¹åˆ«æ€§ã€‚åœ¨å®è·µä¸­ï¼Œé€‰æ‹©BERTæˆ–GPTåº”è¯¥æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚å’Œç‰¹ç‚¹ï¼Œåˆ©ç”¨å®ƒä»¬çš„ç›¸åº”ä¼˜åŠ¿åœ¨å¯¹å‘æ–‡æœ¬ç†è§£å’Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="MOCHa-Multi-Objective-Reinforcement-Mitigating-Caption-Hallucinations"><a href="#MOCHa-Multi-Objective-Reinforcement-Mitigating-Caption-Hallucinations" class="headerlink" title="MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations"></a>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03631">http://arxiv.org/abs/2312.03631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/assafbk/mocha_code">https://github.com/assafbk/mocha_code</a></li>
<li>paper_authors: Assaf Ben-Kish, Moran Yanuka, Morris Alper, Raja Giryes, Hadar Averbuch-Elor</li>
<li>for: æé«˜å›¾åƒæè¿°æ–‡æœ¬çš„å‡†ç¡®æ€§å’ŒSemantic adequacy</li>
<li>methods: ä½¿ç”¨è¿›åŒ–å­¦ä¹ æ¥è§£å†³å›¾åƒæè¿°æ–‡æœ¬ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œå¹¶æå‡ºå¤šç›®æ ‡å¥–åŠ±å‡½æ•°æ¥åŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§å’ŒSemantic adequacy</li>
<li>results: åœ¨ä¸åŒçš„æ¨¡å‹è§„æ¨¡ä¸‹ï¼ŒMOCHaå¯ä»¥åŒæ—¶ä¼˜åŒ–å‡†ç¡®æ€§å’ŒSemantic adequacyï¼Œå¹¶ä¸”åœ¨å¼€ vocabulary settingä¸­è¡¨ç°å‡ºè‰²ï¼Œè¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›† OpenCHAIR æ¥è¯„æµ‹å¼€ vocabulary hallucinations<details>
<summary>Abstract</summary>
While recent years have seen rapid progress in image-conditioned text generation, image captioning still suffers from the fundamental issue of hallucinations, the generation of spurious details that cannot be inferred from the given image. Dedicated methods for reducing hallucinations in image captioning largely focus on closed-vocabulary object tokens, ignoring most types of hallucinations that occur in practice. In this work, we propose MOCHa, an approach that harnesses advancements in reinforcement learning (RL) to address the sequence-level nature of hallucinations in an open-world setup. To optimize for caption fidelity to the input image, we leverage ground-truth reference captions as proxies to measure the logical consistency of generated captions. However, optimizing for caption fidelity alone fails to preserve the semantic adequacy of generations; therefore, we propose a multi-objective reward function that jointly targets these qualities, without requiring any strong supervision. We demonstrate that these goals can be simultaneously optimized with our framework, enhancing performance for various captioning models of different scales. Our qualitative and quantitative results demonstrate MOCHa's superior performance across various established metrics. We also demonstrate the benefit of our method in the open-vocabulary setting. To this end, we contribute OpenCHAIR, a new benchmark for quantifying open-vocabulary hallucinations in image captioning models, constructed using generative foundation models. We will release our code, benchmark, and trained models.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œå›¾åƒæ¡ä»¶æ–‡æœ¬ç”Ÿæˆé¢†åŸŸå·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†å›¾åƒæè¿°ä»ç„¶å—åˆ°åŸºæœ¬é—®é¢˜çš„å¹²æ‰°ï¼Œå³ç”Ÿæˆä¸å­˜åœ¨å›¾åƒä¸­çš„å¹»è§‰ã€‚ç°æœ‰çš„å‡å°‘å¹»è§‰æ–¹æ³•ä¸»è¦æ˜¯åŸºäºå…³é—­ vocabulary å¯¹è±¡ Ñ‚Ğ¾ĞºĞµĞ½ï¼Œå¿½ç•¥äº†å®é™…ä¸­çš„å¤§éƒ¨åˆ†å¹»è§‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† MOCHaï¼Œä¸€ç§åŸºäº reinforcement learningï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å¼€æ”¾ä¸–ç•Œè®¾ç½®ä¸­è§£å†³å›¾åƒæè¿°ä¸­çš„åºåˆ—çº§å¹»è§‰ã€‚ä¸ºäº†ä¼˜åŒ–å›¾åƒæè¿°ä¸è¾“å…¥å›¾åƒçš„ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬åˆ©ç”¨çœŸå®å‚ç…§captionä½œä¸ºé€»è¾‘ä¸€è‡´æ€§çš„æŒ‡æ ‡ã€‚ä½†ä¼˜åŒ–ä¸€ä¸ªcaptionçš„å‡†ç¡®æ€§alone æ— æ³•ä¿æŒç”Ÿæˆçš„ semanticsï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šç›®æ ‡å¥–åŠ±å‡½æ•°ï¼Œè¯¥å‡½æ•°åŒæ—¶ç›®æ ‡è¿™äº›è´¨é‡ï¼Œæ— éœ€å¼ºå¤§çš„ç›‘ç£ã€‚æˆ‘ä»¬ç¤ºå‡ºè¿™äº›ç›®æ ‡å¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ¡†æ¶åŒæ—¶ä¼˜åŒ–ï¼Œæé«˜ä¸åŒè§„æ¨¡çš„æè¿°æ¨¡å‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„è´¨é‡å’Œé‡åŒ–ç»“æœè¡¨æ˜ MOCHa çš„è¶…è¶Šæ€§ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¼€æ”¾ vocabulary  Settingä¸­çš„ä¼˜åŠ¿ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† OpenCHAIRï¼Œä¸€ä¸ªæ–°çš„è¯„ä»·æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°å¼€æ”¾ vocabulary æè¿°æ¨¡å‹ä¸­çš„å¹»è§‰ã€‚æˆ‘ä»¬å°†å‘å¸ƒæˆ‘ä»¬çš„ä»£ç ã€æ ‡å‡†å’Œè®­ç»ƒæ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="DreamComposer-Controllable-3D-Object-Generation-via-Multi-View-Conditions"><a href="#DreamComposer-Controllable-3D-Object-Generation-via-Multi-View-Conditions" class="headerlink" title="DreamComposer: Controllable 3D Object Generation via Multi-View Conditions"></a>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03611">http://arxiv.org/abs/2312.03611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhyang-myron/DreamComposer">https://github.com/yhyang-myron/DreamComposer</a></li>
<li>paper_authors: Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, Xihui Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æé«˜ç°æœ‰çš„è§†å›¾æ„è¯†æ‰©æ•£æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆæ§åˆ¶æ€§çš„æ–°è§†å›¾å›¾åƒã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†è§†å›¾æ„è¯†3Dæå‡æ¨¡å—ï¼Œå°†å¤šä¸ªè§†å›¾ä¸­å¯¹è±¡çš„3Dè¡¨ç¤ºè½¬æ¢ä¸ºlatentç‰¹å¾ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—å°†ç›®æ ‡è§†å›¾ç‰¹å¾ä»å¤šä¸ªè§†å›¾è¾“å…¥ä¸­æå–å‡ºæ¥ã€‚æœ€åï¼Œå®ƒå°†ç›®æ ‡è§†å›¾ç‰¹å¾æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ï¼Œä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾å›¾åƒã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒDreamComposerå¯ä»¥ä¸ç°æœ‰çš„æ‰©æ•£æ¨¡å‹ç›¸ç»“åˆï¼Œå®ç°é›¶å®é™…å‚æ•°çš„æ–°è§†å›¾å›¾åƒç”Ÿæˆã€‚å®ƒå¯ä»¥ç”Ÿæˆé«˜å“è´¨çš„æ–°è§†å›¾å›¾åƒï¼Œå‡†ç¡®åœ°æ•æ‰äº†å¤šè§†å›¾æ¡ä»¶ä¸‹çš„å¯¹è±¡å½¢æ€å’Œä½ç½®ã€‚<details>
<summary>Abstract</summary>
Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view synthesis, further enhancing them to generate high-fidelity novel view images with multi-view conditions, ready for controllable 3D object reconstruction and various other applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½¿ç”¨é¢„è®­ç»ƒçš„2Då¤§è§„æ¨¡ç”Ÿæˆæ¨¡å‹ï¼Œæœ€è¿‘çš„ç ”ç©¶å¯ä»¥ä»å•ä¸ªå®½æ³›å›¾åƒä¸­ç”Ÿæˆé«˜è´¨é‡çš„æ–°è§†å›¾ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹å¤šè§†å›¾ä¿¡æ¯ï¼Œè¿™äº›ç ”ç©¶å—åˆ°ç”Ÿæˆæ§åˆ¶æ–°è§†å›¾çš„å›°éš¾ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† DreamComposerï¼Œä¸€ä¸ªçµæ´»å¯æ‰©å±•çš„æ¡†æ¶ï¼Œå¯ä»¥å¢å¼ºç°æœ‰çš„è§†è§‰æ‰©æ•£æ¨¡å‹ã€‚å…·ä½“æ¥è¯´ï¼ŒDreamComposeré¦–å…ˆä½¿ç”¨è§†è§‰æ„è¯†3Då‡çº§æ¨¡å—æ¥ä»å¤šä¸ªè§†è§’è·å–3Då¯¹è±¡çš„è¡¨ç¤ºã€‚ç„¶åï¼Œå®ƒä½¿ç”¨å¤šè§†å›¾ç‰¹å¾èåˆæ¨¡å—æ¥æ¸²æŸ“ç›®æ ‡è§†å›¾çš„ç§˜å¯†ç‰¹å¾ã€‚æœ€åï¼Œä»å¤šä¸ªè§†è§’è¾“å…¥ä¸­æå–çš„ç›®æ ‡è§†å›¾ç‰¹å¾è¢«æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ä¸­ã€‚å®éªŒè¡¨æ˜ï¼ŒDreamComposerä¸ç°æœ‰æ‰©æ•£æ¨¡å‹å…¼å®¹ï¼Œå¯ä»¥further enhance them to generate high-fidelity novel view images with multi-view conditionsï¼Œready for controllable 3D object reconstructionå’Œå¤šç§å…¶ä»–åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="DiffusionSat-A-Generative-Foundation-Model-for-Satellite-Imagery"><a href="#DiffusionSat-A-Generative-Foundation-Model-for-Satellite-Imagery" class="headerlink" title="DiffusionSat: A Generative Foundation Model for Satellite Imagery"></a>DiffusionSat: A Generative Foundation Model for Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03606">http://arxiv.org/abs/2312.03606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David Lobell, Stefano Ermon</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯Remote Sensingæ•°æ®çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”¨äºç¯å¢ƒç›‘æµ‹å’Œå†œä¸šäº§é‡é¢„æµ‹ç­‰é‡è¦åº”ç”¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†DiffusionSatæ¨¡å‹ï¼ŒåŸºäºå¤§é‡å…¬å…±å¯ç”¨çš„é«˜åˆ†è¾¨ç‡Remote Sensingæ•°æ®é›†åˆè¿›è¡Œè®­ç»ƒï¼Œå¹¶é‡‡ç”¨äº†æ–°çš„conditioningæŠ€æœ¯ï¼Œä½¿ç”¨ metadata å¦‚åœ°ç†åæ ‡ä½œä¸ºç”Ÿæˆå›¾åƒçš„æ¡ä»¶ä¿¡æ¯ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒDiffusionSatæ¨¡å‹å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„å«æ˜Ÿå›¾åƒï¼Œå¹¶å¯ä»¥è§£å†³å¤šç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶é—´ç”Ÿæˆã€å¤šspectralè¾“å…¥çš„è¶…åˆ†è¾¨ç‡ç”Ÿæˆå’Œå¡«å……ç­‰ã€‚ä¸ä¹‹å‰çš„çŠ¶æ€ç æ¨¡å‹ç›¸æ¯”ï¼ŒDiffusionSatæ¨¡å‹è¡¨ç°å‡ºè‰²ï¼Œæ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å«æ˜Ÿå›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
Diffusion models have achieved state-of-the-art results on many modalities including images, speech, and video. However, existing models are not tailored to support remote sensing data, which is widely used in important applications including environmental monitoring and crop-yield prediction. Satellite images are significantly different from natural images -- they can be multi-spectral, irregularly sampled across time -- and existing diffusion models trained on images from the Web do not support them. Furthermore, remote sensing data is inherently spatio-temporal, requiring conditional generation tasks not supported by traditional methods based on captions or images. In this paper, we present DiffusionSat, to date the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets. As text-based captions are sparsely available for satellite images, we incorporate the associated metadata such as geolocation as conditioning information. Our method produces realistic samples and can be used to solve multiple generative tasks including temporal generation, superresolution given multi-spectral inputs and in-painting. Our method outperforms previous state-of-the-art methods for satellite image generation and is the first large-scale $\textit{generative}$ foundation model for satellite imagery.
</details>
<details>
<summary>æ‘˜è¦</summary>
å„ç§æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªé¢‘è°±ä¸­å·²ç»è¾¾åˆ°äº†å½“å‰æœ€ä½³ç»“æœï¼ŒåŒ…æ‹¬å›¾åƒã€è¯­éŸ³å’Œè§†é¢‘ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ¨¡å‹æ²¡æœ‰é’ˆå¯¹å«æ˜Ÿæ•£å°„æ•°æ®è¿›è¡Œæ”¯æŒï¼Œè¿™ç§æ•°æ®å¹¿æ³›ç”¨äºé‡è¦åº”ç”¨ï¼Œå¦‚ç¯å¢ƒç›‘æµ‹å’Œä½œç‰©äº§é‡é¢„æµ‹ã€‚å«æ˜Ÿå›¾åƒä¸è‡ªç„¶å›¾åƒæœ‰å¾ˆå¤§å·®å¼‚ï¼Œå®ƒä»¬å¯èƒ½æ˜¯å¤šspectralï¼Œæ—¶é—´ä¸è§„åˆ™é‡‡æ ·ï¼Œç°æœ‰çš„æ‰©æ•£æ¨¡å‹ä»ç½‘ç»œä¸Šçš„å›¾åƒè¿›è¡Œè®­ç»ƒä¸æ”¯æŒå®ƒä»¬ã€‚æ­¤å¤–ï¼Œå«æ˜Ÿæ•£å°„æ•°æ®æ˜¯ç©ºé—´-æ—¶çš„ï¼Œéœ€è¦åŸºäºæ¡ä»¶ç”Ÿæˆä»»åŠ¡ï¼Œè€Œä¼ ç»Ÿçš„æ–¹æ³•åŸºäºæ ‡ç­¾æˆ–å›¾åƒä¸æ”¯æŒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†DiffusionSatï¼Œè¿„ä»Šä¸ºæ­¢æœ€å¤§çš„åŸºç¡€æ¨¡å‹ï¼ŒåŸºäºå…¬å…±å¯ç”¨çš„å¤§é‡é«˜åˆ†è¾¨ç‡å«æ˜Ÿæ•£å°„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚ç”±äºå«æ˜Ÿå›¾åƒçš„æ–‡æœ¬æ ‡ç­¾ç½•è§ï¼Œæˆ‘ä»¬å°†å…³è” metadataï¼Œå¦‚åœ°ç†ä½ç½®ä½œä¸ºæ¡ä»¶ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ç”Ÿæˆçš„æ ·æœ¬æ˜¯çœŸå®çš„ï¼Œå¯ä»¥ç”¨äºè§£å†³å¤šä¸ªç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬æ—¶é—´ç”Ÿæˆã€åŸºäºå¤šspectralè¾“å…¥çš„è¶…åˆ†è¾¨ç‡ã€å’Œå¡«å……ã€‚æˆ‘ä»¬çš„æ–¹æ³•è¶…è¿‡äº†ä¹‹å‰çš„æœ€ä½³æ–¹æ³•ï¼Œå¹¶æ˜¯é¦–ä¸ªå¤§è§„æ¨¡çš„å«æ˜Ÿå›¾åƒç”ŸæˆåŸºç¡€æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="MMM-Generative-Masked-Motion-Model"><a href="#MMM-Generative-Masked-Motion-Model" class="headerlink" title="MMM: Generative Masked Motion Model"></a>MMM: Generative Masked Motion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03596">http://arxiv.org/abs/2312.03596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen</li>
<li>For: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºMasked Motion Modelï¼ˆMMMï¼‰çš„æ–°å‹åŠ¨ä½œç”Ÿæˆæ–¹æ³•ï¼Œä»¥è§£å†³ç°æœ‰çš„åŠ¨ä½œç”Ÿæˆæ–¹æ³•ä¸­çš„æ—¶é—´æ€§å’Œé«˜ç²¾åº¦ä¹‹é—´çš„è´Ÿé¢é€‰æ‹©ã€‚* Methods: è¿™ä¸ªæ–¹æ³•ä½¿ç”¨äº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨ä½œtokenizerï¼Œå°†3Däººä½“åŠ¨ä½œè½¬æ¢ä¸ºä¸€ä¸ªåºåˆ—çš„ä¸åŒçš„tokenåœ¨éšè—ç©ºé—´ä¸­ï¼Œå’Œï¼ˆ2ï¼‰æ¡ä»¶éšè—åŠ¨ä½œå˜æ¢å™¨ï¼Œå­¦ä¹ é¢„è®¡Randomlyéšè—åŠ¨ä½œtokenï¼ŒåŸºäºå·²ç»è®¡ç®—çš„æ–‡æœ¬tokenã€‚* Results: åœ¨å¯¹HumanML3Då’ŒKIT-MLæ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒåï¼Œè¿™ä¸ªæ–¹æ³•çš„resultè¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶å®ç°äº†é«˜ç²¾åº¦å’Œé«˜é€ŸåŠ¨ä½œç”Ÿæˆï¼Œå¹¶å…·æœ‰é«˜çº§ç¼–è¾‘ç‰¹æ€§ï¼Œä¾‹å¦‚ä½“éƒ¨ä¿®æ”¹ã€åŠ¨ä½œé—´éš”å’Œé•¿åŠ¨ä½œåºåˆ—çš„åˆæˆã€‚æ­¤å¤–ï¼Œè¿™ä¸ªæ–¹æ³•æ¯”ç°æœ‰çš„ç¼–è¾‘åŠ¨ä½œæ‰©æ•£æ¨¡å‹å¿«ä¸¤ä¸ªæ•°é‡çº§çš„å•ä¸ªä¸­ç­‰çº§GPUä¸Šã€‚<details>
<summary>Abstract</summary>
Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fine-grained text descriptions, therefore simultaneously achieving high-fidelity and high-speed motion generation. In addition, MMM has innate motion editability. By simply placing mask tokens in the place that needs editing, MMM automatically fills the gaps while guaranteeing smooth transitions between editing and non-editing parts. Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM surpasses current leading methods in generating high-quality motion (evidenced by superior FID scores of 0.08 and 0.429), while offering advanced editing features such as body-part modification, motion in-betweening, and the synthesis of long motion sequences. In addition, MMM is two orders of magnitude faster on a single mid-range GPU than editable motion diffusion models. Our project page is available at \url{https://exitudio.github.io/MMM-page}.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸï¼Œä½¿ç”¨æ‰©æ•£å’Œè‡ªé€‚åº”æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ°åŠ¨ä½œç”ŸæˆæŠ€æœ¯å·²ç»å–å¾—äº†è‰¯å¥½çš„æˆæœã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ç»å¸¸é¢ä¸´ç€å®æ—¶æ€§ã€é«˜ç²¾åº¦å’ŒåŠ¨ä½œå¯ç¼–è¾‘æ€§ä¹‹é—´çš„ç‰µæ‰¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬ä»‹ç»äº†MMMï¼Œä¸€ç§æ–°å‹ä½†ç®€å•çš„åŠ¨ä½œç”Ÿæˆæ¨¡å¼ï¼ŒåŸºäºå¸¦æœ‰æ©ç çš„åŠ¨ä½œæ¨¡å‹ã€‚MMMåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼šï¼ˆ1ï¼‰åŠ¨ä½œTokenizerï¼Œå°†3Däººä½“åŠ¨ä½œè½¬æ¢ä¸ºç¦»æ•£çš„tokenåœ¨éšè—ç©ºé—´ä¸­ï¼Œå’Œï¼ˆ2ï¼‰å—æ§æ©ç åŠ¨ä½œå˜æ¢å™¨ï¼Œå­¦ä¹ é¢„è®¡æ©ç åŠ¨ä½œtokenï¼Œæ ¹æ®é¢„è®¡çš„æ–‡æœ¬tokenæ¥è¿›è¡Œæ¡ä»¶é¢„æµ‹ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMMMé€šè¿‡åŒæ—¶ attend to motionå’Œæ–‡æœ¬tokenï¼Œä»è€Œæ˜¾å¼åœ°æ•æ‰åŠ¨ä½œtokenä¹‹é—´çš„è‡ªç„¶ä¾èµ–å…³ç³»ï¼Œä»¥åŠæ–‡æœ¬tokenå’ŒåŠ¨ä½œtokenä¹‹é—´çš„å«ä¹‰æ˜ å°„ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMMMå¯ä»¥å¹¶è¡Œåœ°æ‰§è¡Œå¤šä¸ªåŠ¨ä½œtokenï¼Œä»¥å®ç°é«˜ç²¾åº¦å’Œé«˜é€Ÿçš„åŠ¨ä½œç”Ÿæˆã€‚æ­¤å¤–ï¼ŒMMMå†…ç½®äº†åŠ¨ä½œå¯ç¼–è¾‘æ€§ã€‚é€šè¿‡åœ¨éœ€è¦ç¼–è¾‘çš„åœ°æ–¹æ”¾ç½®æ©ç ï¼ŒMMMä¼šè‡ªåŠ¨å¡«å……ç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä¿è¯ç¼–è¾‘å’Œéç¼–è¾‘éƒ¨åˆ†ä¹‹é—´çš„å¹³æ»‘è¿‡æ¸¡ã€‚æˆ‘ä»¬åœ¨HumanML3Då’ŒKIT-MLæ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œ demonstarted that MMM surpasses current leading methods in generating high-quality motionï¼ˆè¯æ˜äº†FIDåˆ†æ•°ä¸º0.08å’Œ0.429ï¼‰ï¼ŒåŒæ—¶æä¾›äº†é«˜çº§ç¼–è¾‘åŠŸèƒ½ï¼Œå¦‚èº«ä½“éƒ¨åˆ†ä¿®æ”¹ã€åŠ¨ä½œå·ç§¯å’Œé•¿åº¦åŠ¨ä½œåºåˆ—çš„åˆæˆã€‚æ­¤å¤–ï¼ŒMMMåœ¨å•ä¸ªä¸­ç­‰çº§GPUä¸Šä¸¤ä¸ªæ•°é‡çº§å¿«äºå¯ç¼–è¾‘åŠ¨ä½œæ‰©æ•£æ¨¡å‹ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„é¡¹ç›®é¡µé¢ï¼š\url{https://exitudio.github.io/MMM-page}ã€‚
</details></li>
</ul>
<hr>
<h2 id="Foundation-Model-Assisted-Weakly-Supervised-Semantic-Segmentation"><a href="#Foundation-Model-Assisted-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Foundation Model Assisted Weakly Supervised Semantic Segmentation"></a>Foundation Model Assisted Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03585">http://arxiv.org/abs/2312.03585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HAL-42/FMA-WSSS">https://github.com/HAL-42/FMA-WSSS</a></li>
<li>paper_authors: Xiaobo Yang, Xiaojin Gong</li>
<li>for:  Addressing weakly supervised semantic segmentation (WSSS) using image-level labels.</li>
<li>methods:  Leveraging pre-trained foundation models (CLIP and SAM) to generate high-quality segmentation seeds, and using a coarse-to-fine framework with multi-label contrastive loss and CAM activation loss to learn the prompts.</li>
<li>results:  Achieving state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.Here is the full translation in Simplified Chinese:</li>
<li>for: æœ¬æ–‡ç›®çš„æ˜¯ä½¿ç”¨å›¾åƒçº§æ ‡ç­¾æ¥è§£å†³å¼±ively supervised semantic segmentation (WSSS) é—®é¢˜ã€‚</li>
<li>methods: æˆ‘ä»¬åˆ©ç”¨é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ï¼ˆCLIPå’ŒSAMï¼‰ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ segmentation ç§å­ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å®½æ³›-to-ç»†åŒ–æ¡†æ¶ï¼Œå¹¶é‡‡ç”¨å¤šæ ‡ç­¾å¯¹æ¯”æŸå¤±å’Œ CAM æ´»åŒ–æŸå¤±æ¥å­¦ä¹ æç¤ºã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨ PASCAL VOC 2012 å’Œ MS COCO 2014 ä¸Šè¾¾åˆ°äº†çŠ¶æ€ Ellçš„æ€§èƒ½å’Œç«äº‰æ€§çš„ç»“æœã€‚<details>
<summary>Abstract</summary>
This work aims to leverage pre-trained foundation models, such as contrastive language-image pre-training (CLIP) and segment anything model (SAM), to address weakly supervised semantic segmentation (WSSS) using image-level labels. To this end, we propose a coarse-to-fine framework based on CLIP and SAM for generating high-quality segmentation seeds. Specifically, we construct an image classification task and a seed segmentation task, which are jointly performed by CLIP with frozen weights and two sets of learnable task-specific prompts. A SAM-based seeding (SAMS) module is designed and applied to each task to produce either coarse or fine seed maps. Moreover, we design a multi-label contrastive loss supervised by image-level labels and a CAM activation loss supervised by the generated coarse seed map. These losses are used to learn the prompts, which are the only parts need to be learned in our framework. Once the prompts are learned, we input each image along with the learned segmentation-specific prompts into CLIP and the SAMS module to produce high-quality segmentation seeds. These seeds serve as pseudo labels to train an off-the-shelf segmentation network like other two-stage WSSS methods. Experiments show that our method achieves the state-of-the-art performance on PASCAL VOC 2012 and competitive results on MS COCO 2014.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿™ä¸ªå·¥ä½œç›®æ ‡æ˜¯åˆ©ç”¨é¢„è®­ç»ƒåŸºæœ¬æ¨¡å‹ï¼Œå¦‚å¯¹æ¯”è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰å’Œ segment anythingæ¨¡å‹ï¼ˆSAMï¼‰ï¼Œæ¥è§£å†³å¼±ç›‘ç£ semantic segmentationï¼ˆWSSSï¼‰é—®é¢˜ï¼Œä½¿ç”¨å›¾åƒçº§åˆ«æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç²—ç»†æ¡†æ¶ï¼ŒåŸºäº CLIP å’Œ SAMï¼Œç”¨äºç”Ÿæˆé«˜è´¨é‡ segmentation çš„ç§å­ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªå›¾åƒåˆ†ç±»ä»»åŠ¡å’Œä¸€ä¸ªç§å­ segmentation ä»»åŠ¡ï¼Œç”± CLIP  WITH å†»ç»“å‚æ•°å’Œä¸¤ç»„å¯å­¦ä¹ çš„ä»»åŠ¡ç‰¹å®šæ¨èæ¥å…±åŒè¿›è¡Œæ‰§è¡Œã€‚SAM æ¨¡å—æ˜¯è®¾è®¡ç”¨äºæ¯ä¸ªä»»åŠ¡ï¼Œä»¥ç”Ÿæˆç²—ç»†æˆ–ç»†åŒ–ç§å­åœ°å›¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ä¸ªå¤šæ ‡ç­¾å¯¹æ¯”æŸå¤±ï¼Œç”±å›¾åƒçº§åˆ«æ ‡ç­¾superviseï¼Œä»¥åŠä¸€ä¸ª CAM æ´»åŠ¨æŸå¤±ï¼Œç”±ç”Ÿæˆçš„ç²—ç»†ç§å­åœ°å›¾superviseã€‚è¿™äº›æŸå¤±ç”¨äºå­¦ä¹ æ¨èï¼Œæ¨èæ˜¯æˆ‘ä»¬frameworkä¸­å”¯ä¸€éœ€è¦å­¦ä¹ çš„éƒ¨åˆ†ã€‚ä¸€æ—¦æ¨èå­¦ä¹ å®Œæ¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªå›¾åƒä¸å­¦ä¹ çš„ segmentation ç‰¹å®šæ¨èè¾“å…¥åˆ° CLIP å’Œ SAM æ¨¡å—ä¸­ï¼Œç”Ÿæˆé«˜è´¨é‡ segmentation ç§å­ã€‚è¿™äº›ç§å­å¯ä»¥ä½œä¸º Pseudo æ ‡ç­¾æ¥è®­ç»ƒä¸€ä¸ªæ ‡å‡†çš„ segmentation ç½‘ç»œï¼Œå¦‚å…¶ä»–ä¸¤ä¸ªé˜¶æ®µ WSSS æ–¹æ³•ã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ PASCAL VOC 2012 å’Œ MS COCO 2014 ä¸Šè¾¾åˆ°äº†çŠ¶æ€ç›‘ç£æ€§çš„æ€§èƒ½ï¼Œå¹¶ä¸”ä¸å…¶ä»–ä¸¤ä¸ªé˜¶æ®µ WSSS æ–¹æ³•ç›¸æ¯”ï¼Œå®ç°äº†ç«äº‰æ€§çš„ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Invariance-Causal-Representation-Learning-Prospects-and-Limitations"><a href="#Invariance-Causal-Representation-Learning-Prospects-and-Limitations" class="headerlink" title="Invariance &amp; Causal Representation Learning: Prospects and Limitations"></a>Invariance &amp; Causal Representation Learning: Prospects and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03580">http://arxiv.org/abs/2312.03580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Bing, Jonas Wahl, Urmi Ninad, Jakob Runge</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯å…³äº causal models ä¸­æœºåˆ¶çš„ä¸å˜æ€§çš„ç ”ç©¶ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº† theoretical impossibility results å’Œ practical considerations æ¥æ¢è®¨æœºåˆ¶ä¸å˜æ€§æ˜¯å¦èƒ½å¤Ÿç”¨äºæ‰¾åˆ° latent causal variablesã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæœºåˆ¶ä¸å˜æ€§æœ¬èº«ä¸å¤Ÿä»¥ä¾¿ç¡®å®š latent causal variablesï¼Œéœ€è¦é‡‡ç”¨æ›´å¤šçš„çº¦æŸæ¥ç¡®å®šè¡¨ç¤ºã€‚<details>
<summary>Abstract</summary>
In causal models, a given mechanism is assumed to be invariant to changes of other mechanisms. While this principle has been utilized for inference in settings where the causal variables are observed, theoretical insights when the variables of interest are latent are largely missing. We assay the connection between invariance and causal representation learning by establishing impossibility results which show that invariance alone is insufficient to identify latent causal variables. Together with practical considerations, we use these theoretical findings to highlight the need for additional constraints in order to identify representations by exploiting invariance.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨ causal æ¨¡å‹ä¸­ï¼Œä¸€ä¸ªç»™å®šçš„æœºåˆ¶è¢«å‡è®¾ä¸ºå…¶ä»–æœºåˆ¶å˜åŒ–ä¸å˜ã€‚è™½ç„¶è¿™ä¸€åŸåˆ™åœ¨è§‚å¯Ÿ causal å˜é‡çš„æƒ…å†µä¸‹ç”¨äºæ¨ç†ï¼Œä½†åœ¨ latent å˜é‡çš„æƒ…å†µä¸‹çš„ç†è®ºå¯ç¤ºå‡ ä¹ç¼ºå¤±ã€‚æˆ‘ä»¬é€šè¿‡è¯æ˜ä¸å¯èƒ½æ€§ç»“è®ºè¡¨æ˜äº†å¯¹ latent  causal å˜é‡çš„å½’ä¸€åŒ–ä¸èƒ½å¤Ÿå”¯ä¸€ç¡®å®šã€‚ä¸å®é™…è€ƒè™‘ç›¸ç»“åˆï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›ç†è®ºå‘ç°æ¥å¼ºè°ƒéœ€è¦é¢å¤–çº¦æŸä»¥ä¾¿é€šè¿‡å½’ä¸€åŒ–æ¥ç¡®å®šè¡¨ç¤ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalization-to-New-Sequential-Decision-Making-Tasks-with-In-Context-Learning"><a href="#Generalization-to-New-Sequential-Decision-Making-Tasks-with-In-Context-Learning" class="headerlink" title="Generalization to New Sequential Decision Making Tasks with In-Context Learning"></a>Generalization to New Sequential Decision Making Tasks with In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03801">http://arxiv.org/abs/2312.03801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sharath Chandra Raparthy, Eric Hambro, Robert Kirk, Mikael Henaff, Roberta Raileanu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³æœºå™¨å­¦ä¹ ä¸­è‡ªé€‚åº”ä»»åŠ¡å­¦ä¹ çš„é—®é¢˜ï¼Œå³ä½¿åªæœ‰å‡ ä¸ªç¤ºä¾‹ä¹Ÿèƒ½å¤Ÿå­¦ä¹ æ–°çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº† transformer æ¥å­¦ä¹ æ–°çš„è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ï¼Œä½†æ˜¯åœ¨é¡ºåºå†³ç­–Settingä¸‹ï¼Œå®ƒä»¬æ— æ³•ç›´æ¥åº”ç”¨äºæ–°ä»»åŠ¡ä¸Šè¿›è¡Œå­¦ä¹ ã€‚ä½œè€…ä»¬åˆ™æå‡ºäº†ä¸€ç§ä½¿ç”¨åºåˆ—å¾„è¡Œçš„è®­ç»ƒæ–¹æ³•ï¼Œä»¥å®ç°åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚</li>
<li>results: ä½œè€…ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­é€šè¿‡ä¸€ä¸ªç¤ºä¾‹æ¥è¯´æ˜ï¼Œé€šè¿‡è®­ç»ƒåºåˆ—å¾„è¡Œå¯ä»¥å®ç°åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚ä»–ä»¬è¿˜ç ”ç©¶äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå‘ç°æ›´å¤§çš„æ¨¡å‹å’Œæ•°æ®é›†å¤§å°ã€æ›´å¤šçš„ä»»åŠ¡å¤šæ ·æ€§ã€ç¯å¢ƒéšæœºæ€§å’Œå¾„è¡Œå¼ºåº¦éƒ½ä¼šå¯¼è‡´æ›´å¥½çš„åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾„è¡Œå­¦ä¹ ã€‚é€šè¿‡è®­ç»ƒå¤§å‹å¤šæ ·åŒ–çš„ç¦»çº¿æ•°æ®é›†ï¼Œä»–ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨å‡ ä¸ªç¤ºä¾‹ä¸‹å­¦ä¹ æ–°çš„ MiniHack å’Œ Procgen ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Training autonomous agents that can learn new tasks from only a handful of demonstrations is a long-standing problem in machine learning. Recently, transformers have been shown to learn new language or vision tasks without any weight updates from only a few examples, also referred to as in-context learning. However, the sequential decision making setting poses additional challenges having a lower tolerance for errors since the environment's stochasticity or the agent's actions can lead to unseen, and sometimes unrecoverable, states. In this paper, we use an illustrative example to show that naively applying transformers to sequential decision making problems does not enable in-context learning of new tasks. We then demonstrate how training on sequences of trajectories with certain distributional properties leads to in-context learning of new sequential decision making tasks. We investigate different design choices and find that larger model and dataset sizes, as well as more task diversity, environment stochasticity, and trajectory burstiness, all result in better in-context learning of new out-of-distribution tasks. By training on large diverse offline datasets, our model is able to learn new MiniHack and Procgen tasks without any weight updates from just a handful of demonstrations.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸ¹è®­è‡ªé€‚åº”ä»£ç†äººå¯ä»¥ä»åªæœ‰å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–°ä»»åŠ¡æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é•¿æœŸé—®é¢˜ã€‚æœ€è¿‘ï¼Œ transformers è¢«è¯æ˜å¯ä»¥ä»åªæœ‰å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–°è¯­è¨€æˆ–è§†è§‰ä»»åŠ¡ï¼Œè€Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ï¼Œä¹Ÿç§°ä¸ºå†…Contextå­¦ä¹ ã€‚ç„¶è€Œï¼Œé¡ºåºå†³ç­–è®¾ç½®å¢åŠ äº†æ›´é«˜çš„é”™è¯¯å¿å®¹ç‡ï¼Œå› ä¸ºç¯å¢ƒçš„éšæœºæ€§æˆ–è€…ä»£ç†äººçš„æ“ä½œå¯èƒ½ä¼šå¯¼è‡´æœªçœ‹è¿‡çš„ã€æœ‰æ—¶æ— æ³•æ¢å¤çš„çŠ¶æ€ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ª illustrate ä¾‹å­æ¥è¡¨æ˜ï¼Œç›´æ¥åº”ç”¨ transformers åˆ°é¡ºåºå†³ç­–é—®é¢˜ä¸Šä¸èƒ½å®ç°å†…Contextå­¦ä¹ æ–°ä»»åŠ¡ã€‚ç„¶åï¼Œæˆ‘ä»¬ç¤ºä¾‹äº†åœ¨åºåˆ—å¾„è¿¹ä¸­è®­ç»ƒæ—¶ï¼Œé‡‡ç”¨æŸäº›åˆ†å¸ƒæ€§è´¨å¯ä»¥å®ç°å†…Contextå­¦ä¹ æ–°é¡ºåºå†³ç­–ä»»åŠ¡ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†ä¸åŒçš„è®¾è®¡é€‰æ‹©ï¼Œå¹¶å‘ç°å¤§å‹æ¨¡å‹å’Œæ•°æ®é›†å¤§å°ã€ä»»åŠ¡å¤šæ ·æ€§ã€ç¯å¢ƒéšæœºæ€§å’Œå¾„è¿¹å¼ºçƒˆç¨‹åº¦éƒ½ä¼šå¯¼è‡´æ›´å¥½çš„å†…Contextå­¦ä¹ æ–°Out-of-distributionä»»åŠ¡ã€‚é€šè¿‡è®­ç»ƒå¤§å‹å¤šæ ·åŒ–çš„ç¦»çº¿æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä»å‡ ä¸ªç¤ºä¾‹å­¦ä¹ æ–° MiniHack å’Œ Procgen ä»»åŠ¡ï¼Œæ— éœ€ä»»ä½•å‚æ•°æ›´æ–°ã€‚
</details></li>
</ul>
<hr>
<h2 id="GPT-4-Enhanced-Multimodal-Grounding-for-Autonomous-Driving-Leveraging-Cross-Modal-Attention-with-Large-Language-Models"><a href="#GPT-4-Enhanced-Multimodal-Grounding-for-Autonomous-Driving-Leveraging-Cross-Modal-Attention-with-Large-Language-Models" class="headerlink" title="GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models"></a>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03543">http://arxiv.org/abs/2312.03543</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/petrichor625/talk2car_cavg">https://github.com/petrichor625/talk2car_cavg</a></li>
<li>paper_authors: Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, Chengzhong Xu</li>
<li>for: This paper aims to improve the ability of autonomous vehicles (AVs) to understand and execute visual commands in a visual context.</li>
<li>methods: The authors propose a sophisticated encoder-decoder framework called Context-Aware Visual Grounding (CAVG), which integrates five core encoders (Text, Image, Context, and Cross-Modal) with a Multimodal decoder. The model is trained using state-of-the-art Large Language Models (LLMs) and incorporates multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation.</li>
<li>results: The CAVG model achieves new standards in prediction accuracy and operational efficiency on the Talk2Car dataset, a real-world benchmark. It demonstrates exceptional performance even with limited training data, and shows remarkable robustness and adaptability in challenging scenarios such as long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments.<details>
<summary>Abstract</summary>
In the field of autonomous vehicles (AVs), accurately discerning commander intent and executing linguistic commands within a visual context presents a significant challenge. This paper introduces a sophisticated encoder-decoder framework, developed to address visual grounding in AVs.Our Context-Aware Visual Grounding (CAVG) model is an advanced system that integrates five core encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This integration enables the CAVG model to adeptly capture contextual semantics and to learn human emotional features, augmented by state-of-the-art Large Language Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the implementation of multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation. This architectural design enables the model to efficiently process and interpret a range of cross-modal inputs, yielding a comprehensive understanding of the correlation between verbal commands and corresponding visual scenes. Empirical evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that CAVG establishes new standards in prediction accuracy and operational efficiency. Notably, the model exhibits exceptional performance even with limited training data, ranging from 50% to 75% of the full dataset. This feature highlights its effectiveness and potential for deployment in practical AV applications. Moreover, CAVG has shown remarkable robustness and adaptability in challenging scenarios, including long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments. The code for the proposed model is available at our Github.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è‡ªåŠ¨é©¾é©¶è½¦ï¼ˆAVï¼‰é¢†åŸŸï¼Œæ­£ç¡®åœ°ç†è§£æŒ‡æŒ¥å®˜æ„å›¾å¹¶åœ¨è§†è§‰ä¸Šå‘å‡ºè¯­è¨€å‘½ä»¤æ˜¯ä¸€é¡¹é‡è¦æŒ‘æˆ˜ã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é«˜çº§çš„encoder-decoderæ¡†æ¶ï¼Œç”¨äºè§£å†³AVä¸­çš„è§†è§‰å®šä½ã€‚æˆ‘ä»¬çš„ Context-Aware Visual Groundingï¼ˆCAVGï¼‰æ¨¡å‹åŒ…æ‹¬äº”ç§æ ¸å¿ƒencoderâ€”â€”Textã€Imageã€Contextã€Cross-Modalâ€”â€”ä»¥åŠä¸€ä¸ªMultimodal decoderã€‚è¿™ç§æ•´åˆä½¿å¾—CAVGæ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ•æ‰Contextual semanticsï¼Œå¹¶é€šè¿‡ä½¿ç”¨ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬GPT-4ï¼Œå­¦ä¹ äººç±»æƒ…æ„Ÿç‰¹å¾ã€‚CAVGæ¨¡å‹çš„architectureè¢«å¼ºåŒ–äº†å¤šå¤´è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶å’ŒRegion-Specific Dynamicï¼ˆRSDï¼‰å±‚ Ğ´Ğ»Ñæ³¨æ„åŠ›è°ƒæ•´ã€‚è¿™ç§å»ºç«‹çš„å»ºç­‘ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†å’Œè§£é‡Šå¤šç§è·¨æ¨¡æ€è¾“å…¥ï¼Œä»è€Œè·å¾—è§†è§‰ä¸Šçš„commandå’Œå¯¹åº”çš„è¯­è¨€å‘½ä»¤ä¹‹é—´çš„å…³ç³»ã€‚å®éªŒè¯æ˜ï¼ŒCAVGåœ¨Talk2Caræ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ–°çš„æ ‡å‡†ï¼Œå¹¶ä¸”åœ¨æœ‰é™çš„è®­ç»ƒæ•°æ®ä¸Šè¾¾åˆ°äº†å‡ºè‰²çš„è¡¨ç°ã€‚æ­¤å¤–ï¼ŒCAVGæ¨¡å‹åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­ä¹Ÿè¡¨ç°å‡ºäº†æ°å‡ºçš„Robustnesså’Œé€‚åº”æ€§ï¼ŒåŒ…æ‹¬é•¿æ–‡æœ¬å‘½ä»¤è§£é‡Šã€ä½å…‰ç…§æ¡ä»¶ã€ä¸ç¡®å®šçš„æŒ‡æŒ¥å®˜ä¸Šä¸‹æ–‡ã€ä¸å¥½çš„å¤©æ°”æ¡ä»¶å’Œæ‹¥æŒ¤çš„åŸå¸‚ç¯å¢ƒã€‚CAVGæ¨¡å‹çš„ä»£ç å¯ä»¥åœ¨æˆ‘ä»¬çš„Githubä¸Šè·å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Low-power-Continuous-Remote-Behavioral-Localization-with-Event-Cameras"><a href="#Low-power-Continuous-Remote-Behavioral-Localization-with-Event-Cameras" class="headerlink" title="Low-power, Continuous Remote Behavioral Localization with Event Cameras"></a>Low-power, Continuous Remote Behavioral Localization with Event Cameras</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03799">http://arxiv.org/abs/2312.03799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex Kacelnik, Guillermo Gallego</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ç”¨äºè¿œç¨‹é‡å¤–åŠ¨ç‰©è§‚å¯Ÿçš„å¯é è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼Œä»¥ automatize åŠ¨ç‰©è¡Œä¸ºé‡åŒ–ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†äº‹ä»¶ç›¸æœºï¼Œå…·æœ‰ä½åŠŸè€—å’Œé«˜åŠ¨æ€èŒƒå›´ç‰¹æ€§ï¼Œå¯¹ remote é‡å¤–åŠ¨ç‰©è§‚å¯Ÿè¿›è¡Œäº† battery-dependent ç›‘æµ‹ã€‚ç ”ç©¶é‡‡ç”¨äº†æ—¶é—´åŠ¨ä½œæ£€æµ‹ä»»åŠ¡ï¼Œæ ¹æ®äº‹ä»¶æ•°æ®è¿›è¡Œäº†16ä¸ªå·¢çš„æ ‡æ³¨ã€‚å¼€å‘çš„æ–¹æ³•åŒ…æ‹¬ä¸€ä¸ªç”Ÿæˆå‡ ä¸ªå¯èƒ½çš„æ—¶é—´é—´éš”ï¼ˆææ¡ˆï¼‰çš„ç”Ÿæˆå™¨ï¼Œä»¥åŠä¸€ä¸ªå†…éƒ¨ç±»åˆ«åŠ¨ä½œçš„åˆ†ç±»å™¨ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œäº‹ä»¶ç›¸æœºçš„è‡ªç„¶å“åº”äºè¿åŠ¨éå¸¸æœ‰æ•ˆï¼Œå¯ä»¥å®ç° kontinuous åŠ¨ç‰©ç›‘æµ‹å’Œæ£€æµ‹ï¼ŒmAP ä¸º 58%ï¼ˆåœ¨è‰¯å¥½å¤©æ°”æƒ…å†µä¸‹æé«˜åˆ° 63%ï¼‰ã€‚ç ”ç©¶è¿˜è¡¨æ˜äº†å¯¹ä¸åŒç…§æ˜æ¡ä»¶çš„Robustnessã€‚ä½¿ç”¨äº‹ä»¶ç›¸æœºè®°å½•åŠ¨ç‰©è¡Œä¸ºå¯ä»¥ä¸‰å€é•¿äºä½¿ç”¨ conventunal ç›¸æœºã€‚æœ¬ç ”ç©¶å¼€æ‹“äº†è¿œç¨‹é‡å¤–åŠ¨ç‰©è§‚å¯Ÿé¢†åŸŸçš„æ–°å¯èƒ½æ€§ã€‚<details>
<summary>Abstract</summary>
Researchers in natural science need reliable methods for quantifying animal behavior. Recently, numerous computer vision methods emerged to automate the process. However, observing wild species at remote locations remains a challenging task due to difficult lighting conditions and constraints on power supply and data storage. Event cameras offer unique advantages for battery-dependent remote monitoring due to their low power consumption and high dynamic range capabilities. We use this novel sensor to quantify a behavior in Chinstrap penguins called ecstatic display. We formulate the problem as a temporal action detection task, determining the start and end times of the behavior. For this purpose, we recorded a colony of breeding penguins in Antarctica during several weeks and labeled event data on 16 nests. The developed method consists of a generator of candidate time intervals (proposals) and a classifier of the actions within them. The experiments show that the event cameras' natural response to motion is effective for continuous behavior monitoring and detection, reaching a mean average precision (mAP) of 58% (which increases to 63% in good weather conditions). The results also demonstrate the robustness against various lighting conditions contained in the challenging dataset. The low-power capabilities of the event camera allows to record three times longer than with a conventional camera. This work pioneers the use of event cameras for remote wildlife observation, opening new interdisciplinary opportunities. https://tub-rip.github.io/eventpenguins/
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Diversity-and-Realism-of-Distilled-Dataset-An-Efficient-Dataset-Distillation-Paradigm"><a href="#On-the-Diversity-and-Realism-of-Distilled-Dataset-An-Efficient-Dataset-Distillation-Paradigm" class="headerlink" title="On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm"></a>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03526">http://arxiv.org/abs/2312.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Sun, Bei Shi, Daiwei Yu, Tao Lin</li>
<li>For: This paper aims to improve the efficiency and practicality of dataset distillation for large-scale real-world applications.* Methods: The proposed method, RDED, focuses on three key properties (realism, diversity, and efficiency) and uses a novel computationally-efficient approach to distill large datasets.* Results: RDED achieves notable results, including distilling the full ImageNet-1K to a small dataset within 7 minutes and achieving a 42% top-1 accuracy with ResNet-18 on a single GPU, outperforming the state-of-the-art.<details>
<summary>Abstract</summary>
Contemporary machine learning requires training large neural networks on massive datasets and thus faces the challenges of high computational demands. Dataset distillation, as a recent emerging strategy, aims to compress real-world datasets for efficient training. However, this line of research currently struggle with large-scale and high-resolution datasets, hindering its practicality and feasibility. To this end, we re-examine the existing dataset distillation methods and identify three properties required for large-scale real-world applications, namely, realism, diversity, and efficiency. As a remedy, we propose RDED, a novel computationally-efficient yet effective data distillation paradigm, to enable both diversity and realism of the distilled data. Extensive empirical results over various neural architectures and datasets demonstrate the advancement of RDED: we can distill the full ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes, achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU (while the SOTA only achieves 21% but requires 6 hours).
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£æœºå™¨å­¦ä¹ éœ€è¦è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œï¼Œå› æ­¤é¢ä¸´é«˜è®¡ç®—éœ€æ±‚çš„æŒ‘æˆ˜ã€‚ dataset distillation ä½œä¸ºä¸€ç§æ–°å…´ç­–ç•¥ï¼Œç›®çš„æ˜¯å‹ç¼©ç°å®ä¸–ç•Œæ•°æ®é›†ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°è®­ç»ƒã€‚ç„¶è€Œï¼Œè¿™ä¸€ç ”ç©¶ç°åœ¨å—åˆ°å¤§è§„æ¨¡é«˜åˆ†è¾¨ç‡æ•°æ®é›†çš„é™åˆ¶ï¼Œä½¿å…¶å®é™…æ€§å’Œå¯è¡Œæ€§å—åˆ°æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†ç°æœ‰çš„ dataset distillation æ–¹æ³•ï¼Œå¹¶ç¡®å®šäº†å¤§è§„æ¨¡å®é™…åº”ç”¨ä¸­éœ€è¦çš„ä¸‰ä¸ªå±æ€§ï¼Œ namelyï¼Œrealismï¼Œ diversityï¼Œ and efficiencyã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æè®® RDEDï¼Œä¸€ç§æ–°çš„è®¡ç®—æ•ˆç‡é«˜ï¼Œ yet effective æ•°æ®å‹ç¼© paradigmï¼Œä»¥å®ç°æ•°æ®çš„å¤šæ ·æ€§å’ŒçœŸå®æ€§ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRDED å¯ä»¥åœ¨ 7 åˆ†é’Ÿå†…ï¼Œå°†æ•´ä¸ª ImageNet-1K æ•°æ®é›†å‹ç¼©æˆ 10 å¼ å›¾åƒæ¯ä¸ªç±»å‹çš„å°æ•°æ®é›†ï¼Œå¹¶åœ¨ ResNet-18 ä¸Š achieved 42% top-1 å‡†ç¡®ç‡ï¼ˆè€Œ SOTA åªèƒ½è¾¾åˆ° 21%ï¼Œå¹¶éœ€è¦ 6 å°æ—¶ï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Scale-and-Multi-Modal-Contrastive-Learning-Network-for-Biomedical-Time-Series"><a href="#Multi-Scale-and-Multi-Modal-Contrastive-Learning-Network-for-Biomedical-Time-Series" class="headerlink" title="Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series"></a>Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03796">http://arxiv.org/abs/2312.03796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbo Guo, Xinzi Xu, Hao Wu, Guoxing Wang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ä¸ªå¤šæ¨¡å¼ç”Ÿç‰©åŒ»æ—¶é—´åºåˆ—èµ„æ–™çš„å­¦ä¹ æ¨¡å‹ï¼Œä»¥å®ç°å¤šæ¨¡å¼é—´çš„è·¨åº¦æ±‡æµå’Œè·¨æ¨¡å¼è½¬æ¢ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šå°ºåº¦å’Œå¤šæ¨¡å¼çš„ç”Ÿç‰©åŒ»æ—¶é—´åºåˆ—è¡¨ç°å­¦ä¹ ç½‘ç»œï¼ˆMBSLï¼‰ï¼Œå…·æœ‰å¯¹ç…§å­¦ä¹ æ¥å®ç°å¤šæ¨¡å¼é—´çš„è·¨åº¦æ±‡æµå’Œè·¨æ¨¡å¼è½¬æ¢ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMBSLæ¯”å‰ä¸€ä»£æ¨¡å‹é«˜å‡º33.9%çš„å¹³å‡è¯¯å·®ï¼ˆMAEï¼‰åœ¨å‘¼å¸é€Ÿç‡æµ‹é‡ã€13.8% MAEåœ¨è¿åŠ¨å¿ƒç‡æµ‹é‡ã€1.41%çš„å‡†ç¡®ç‡åœ¨äººç±»æ´»åŠ¨è¯†åˆ«å’Œ1.14%çš„F1åˆ†æ•°åœ¨å‘¼å¸æš‚åœç—‡å€™ç¾¤è¯†åˆ«ç­‰å››ä¸ªç”Ÿç‰©åŒ»åº”ç”¨ä¸­ã€‚<details>
<summary>Abstract</summary>
Multi-modal biomedical time series (MBTS) data offers a holistic view of the physiological state, holding significant importance in various bio-medical applications. Owing to inherent noise and distribution gaps across different modalities, MBTS can be complex to model. Various deep learning models have been developed to learn representations of MBTS but still fall short in robustness due to the ignorance of modal-to-modal variations. This paper presents a multi-scale and multi-modal biomedical time series representation learning (MBSL) network with contrastive learning to migrate these variations. Firstly, MBTS is grouped based on inter-modal distances, then each group with minimum intra-modal variations can be effectively modeled by individual encoders. Besides, to enhance the multi-scale feature extraction (encoder), various patch lengths and mask ratios are designed to generate tokens with semantic information at different scales and diverse contextual perspectives respectively. Finally, cross-modal contrastive learning is proposed to maximize consistency among inter-modal groups, maintaining useful information and eliminating noises. Experiments against four bio-medical applications show that MBSL outperforms state-of-the-art models by 33.9% mean average errors (MAE) in respiration rate, by 13.8% MAE in exercise heart rate, by 1.41% accuracy in human activity recognition, and by 1.14% F1-score in obstructive sleep apnea-hypopnea syndrome.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šModalç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—æ•°æ®ï¼ˆMBTSï¼‰å…·æœ‰æ•´ä½“ç”Ÿç†çŠ¶æ€çš„å…¨é¢è§†å›¾ï¼Œåœ¨å„ç§ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚ç„¶è€Œï¼Œç”±äºä¸åŒmodalitiesä¹‹é—´çš„é™„åŠ å™ªå£°å’Œåˆ†å¸ƒå·®å¼‚ï¼ŒMBTSå¯èƒ½ä¼šå˜å¾—å¤æ‚ã€‚ä¸ºäº†å­¦ä¹ MBTSçš„è¡¨ç¤ºï¼Œå„ç§æ·±åº¦å­¦ä¹ æ¨¡å‹å·²ç»è¢«å¼€å‘å‡ºæ¥ï¼Œä½†ä»ç„¶ç¼ºä¹robustnessï¼Œå³å› ä¸ºå¿½ç•¥ä¸åŒmodalitiesä¹‹é—´çš„å˜åŒ–ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§å¤šå°ºåº¦å’Œå¤šModalç”Ÿç‰©åŒ»å­¦æ—¶é—´åºåˆ—è¡¨ç¤ºå­¦ä¹ ï¼ˆMBSLï¼‰ç½‘ç»œï¼Œä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥è¿ç§»è¿™äº›å˜åŒ–ã€‚é¦–å…ˆï¼ŒMBTSè¢«åˆ†ç»„ Based on inter-modal distancesï¼Œç„¶åæ¯ä¸ªç»„çš„æœ€å°å†…Modalå·®å¼‚å¯ä»¥è¢«ä¸ªæ€§åŒ–Encoderæ¨¡å‹æœ‰æ•ˆåœ°æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºå¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆEncoderï¼‰ï¼Œå„ç§patché•¿åº¦å’Œmaskæ¯”ä¾‹è¢«è®¾è®¡å‡ºæ¥ï¼Œä»¥ç”Ÿæˆå…·æœ‰Semanticä¿¡æ¯çš„Tokenåœ¨ä¸åŒçš„å°ºåº¦å’Œå¤šç§æ–‡è„‰ä¸Šã€‚æœ€åï¼Œè·¨Modalå¯¹æ¯”å­¦ä¹ è¢«æå‡ºï¼Œä»¥æœ€å¤§åŒ–inter-Modalç»„çš„ä¸€è‡´æ€§ï¼Œä¿ç•™æœ‰ç”¨ä¿¡æ¯ï¼Œå¹¶æ¶ˆé™¤å™ªå£°ã€‚å¯¹å››ç§ç”Ÿç‰©åŒ»å­¦åº”ç”¨è¿›è¡Œäº†å®éªŒï¼Œç ”ç©¶å‘ç°ï¼ŒMBSLæ¯”State-of-the-artæ¨¡å‹æé«˜33.9%çš„ Mean Average Errorï¼ˆMAEï¼‰ã€13.8%çš„ Exercise Heart Rate MAEã€1.41%çš„ Human Activity Recognition Accuracyå’Œ1.14%çš„ Obstructive Sleep Apnea-Hypopnea Syndrome F1 Scoreã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimal-Wildfire-Escape-Route-Planning-for-Drones-under-Dynamic-Fire-and-Smoke"><a href="#Optimal-Wildfire-Escape-Route-Planning-for-Drones-under-Dynamic-Fire-and-Smoke" class="headerlink" title="Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke"></a>Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03521">http://arxiv.org/abs/2312.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Tamas Sziranyi</li>
<li>for:  aid wildfire management efforts by planning an optimal escape route for drones</li>
<li>methods:  use information fusion between UAV and satellite, multi-channel remote sensing data, UAV vision technology, and improved A* algorithm</li>
<li>results:  enhance the safety and efficiency of drone operations in wildfire environments by considering dynamic fire and smoke models<details>
<summary>Abstract</summary>
In recent years, the increasing prevalence and intensity of wildfires have posed significant challenges to emergency response teams. The utilization of unmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in aiding wildfire management efforts. This work focuses on the development of an optimal wildfire escape route planning system specifically designed for drones, considering dynamic fire and smoke models. First, the location of the source of the wildfire can be well located by information fusion between UAV and satellite, and the road conditions in the vicinity of the fire can be assessed and analyzed using multi-channel remote sensing data. Second, the road network can be extracted and segmented in real time using UAV vision technology, and each road in the road network map can be given priority based on the results of road condition classification. Third, the spread model of dynamic fires calculates the new location of the fire source based on the fire intensity, wind speed and direction, and the radius increases as the wildfire spreads. Smoke is generated around the fire source to create a visual representation of a burning fire. Finally, based on the improved A* algorithm, which considers all the above factors, the UAV can quickly plan an escape route based on the starting and destination locations that avoid the location of the fire source and the area where it is spreading. By considering dynamic fire and smoke models, the proposed system enhances the safety and efficiency of drone operations in wildfire environments.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œé‡ç«çš„å‘ç”Ÿå’Œæ‰©æ•£çš„æƒ…å†µæ—¥ç›Šä¸¥é‡ï¼Œå¯¹æŠ¢æ•‘é˜Ÿä¼æå‡ºäº†æå¤§çš„æŒ‘æˆ˜ã€‚ä½¿ç”¨æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰çš„åº”ç”¨æ˜¾ç¤ºäº†å¸®åŠ©é‡ç«ç®¡ç†çš„æ½œåœ¨ä¼˜åŠ¿ã€‚æœ¬å·¥ä½œå…³æ³¨äºåŸºäºUAVçš„é‡ç«é€ƒç”Ÿè·¯å¾„è§„åˆ’ç³»ç»Ÿçš„å¼€å‘ï¼Œè€ƒè™‘äº†åŠ¨æ€ç«ç„°å’ŒçƒŸé›¾æ¨¡å‹ã€‚é¦–å…ˆï¼Œé€šè¿‡UAVå’Œå«æ˜Ÿä¿¡æ¯èåˆï¼Œå¯ä»¥å‡†ç¡®åœ°ç¡®å®šé‡ç«çš„èµ·ç‚¹ä½ç½®ã€‚å…¶æ¬¡ï¼Œé€šè¿‡å¤šé€šé“è¿œç¨‹æ„ŸçŸ¥æŠ€æœ¯ï¼Œåœ¨é‡ç«é™„è¿‘åœ°åŒºå®æ—¶æå–å’Œåˆ†ç±»é“è·¯ç½‘ç»œåœ°å›¾ï¼Œå¹¶å°†æ¯æ¡é“è·¯åœ¨é“è·¯ç½‘ç»œåœ°å›¾ä¸­åˆ†é…ä¼˜å…ˆçº§ã€‚ç¬¬ä¸‰ï¼Œæ ¹æ®åŠ¨æ€ç«ç„°æ‰©æ•£æ¨¡å‹ï¼Œè®¡ç®—æ–°çš„ç«æºä½ç½®ï¼Œä»¥åŠç«ç„°å¼ºåº¦ã€é£é€Ÿå’Œæ–¹å‘ã€‚çƒŸé›¾åœ¨ç«æºå‘¨å›´ç”Ÿæˆï¼Œåˆ›é€ ä¸€ä¸ªç‡ƒçƒ§ç«çš„è§†è§‰è¡¨ç°ã€‚æœ€åï¼ŒåŸºäºæ”¹è¿›çš„A*ç®—æ³•ï¼Œè€ƒè™‘äº†ä»¥ä¸Šå› ç´ ï¼ŒUAVå¿«é€Ÿè®¡åˆ’é€ƒç”Ÿè·¯å¾„ï¼Œé¿å…ç«æºä½ç½®å’Œæ‰©æ•£çš„åœ°åŒºã€‚ç”±äºè€ƒè™‘äº†åŠ¨æ€ç«ç„°å’ŒçƒŸé›¾æ¨¡å‹ï¼Œæå‡ºçš„ç³»ç»Ÿæé«˜äº†æ— äººæœºåœ¨é‡ç«ç¯å¢ƒä¸­çš„å®‰å…¨æ€§å’Œæ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Defense-Against-Adversarial-Attacks-using-Convolutional-Auto-Encoders"><a href="#Defense-Against-Adversarial-Attacks-using-Convolutional-Auto-Encoders" class="headerlink" title="Defense Against Adversarial Attacks using Convolutional Auto-Encoders"></a>Defense Against Adversarial Attacks using Convolutional Auto-Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03520">http://arxiv.org/abs/2312.03520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyasi Mandal</li>
<li>for: å¼ºåŒ–ç›®æ ‡åˆ†ç±»å™¨æ¨¡å‹å¯¹æŠ—æ”»å‡»</li>
<li>methods: ä½¿ç”¨å·ç§¯è‡ªé€‚åº”å™¨æ¨¡å‹å¯¹æŠ—æ”»å‡»</li>
<li>results: å®ç°æ¨¡å‹ç²¾åº¦çš„Restore<details>
<summary>Abstract</summary>
Deep learning models, while achieving state-of-the-art performance on many tasks, are susceptible to adversarial attacks that exploit inherent vulnerabilities in their architectures. Adversarial attacks manipulate the input data with imperceptible perturbations, causing the model to misclassify the data or produce erroneous outputs. This work is based on enhancing the robustness of targeted classifier models against adversarial attacks. To achieve this, an convolutional autoencoder-based approach is employed that effectively counters adversarial perturbations introduced to the input images. By generating images closely resembling the input images, the proposed methodology aims to restore the model's accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥è¾¾åˆ°è®¸å¤šä»»åŠ¡çš„çŠ¶æ€å‰æ²¿æ€§è¡¨ç°ï¼Œä½†å—åˆ°é’ˆå¯¹æ€§æ”»å‡»çš„å¨èƒã€‚è¿™äº›æ”»å‡»é€šè¿‡ manipulate è¾“å…¥æ•°æ®ä¸­çš„å¾®scopic å˜åŒ–ï¼Œä½¿æ¨¡å‹é”™åˆ†æˆ–ç”Ÿæˆé”™è¯¯çš„è¾“å‡ºã€‚è¿™é¡¹å·¥ä½œæ˜¯åŸºäºå¢å¼ºç›®æ ‡åˆ†ç±»å™¨æ¨¡å‹å¯¹é’ˆå¯¹æ€§æ”»å‡»çš„Robustnessã€‚ä¸ºè¾¾åˆ°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§åŸºäºå·ç§¯ autoencoder çš„æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆå¯¹è¾“å…¥å›¾åƒä¸­çš„é’ˆå¯¹æ€§æ”»å‡»è¿›è¡Œåº”å¯¹ã€‚é€šè¿‡ç”Ÿæˆä¸è¾“å…¥å›¾åƒå‡ ä¹ç›¸åŒçš„å›¾åƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¸Œæœ›å¯ä»¥æ¢å¤æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Active-Wildfires-Detection-and-Dynamic-Escape-Routes-Planning-for-Humans-through-Information-Fusion-between-Drones-and-Satellites"><a href="#Active-Wildfires-Detection-and-Dynamic-Escape-Routes-Planning-for-Humans-through-Information-Fusion-between-Drones-and-Satellites" class="headerlink" title="Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites"></a>Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03519">http://arxiv.org/abs/2312.03519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Tamas Sziranyi</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºUAVè§†è§‰æŠ€æœ¯å’Œå«æ˜Ÿå›¾åƒåˆ†ææŠ€æœ¯çš„åŠ¨æ€äººå‘˜æ•‘æ´è·¯å¾„è§„åˆ’æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œè¯†åˆ«é‡å¤–ç«ç¾çš„ç«æºä½ç½®å’Œç‡ƒçƒ§åŒºåŸŸï¼Œå¹¶ä¸ºäººä»¬æä¾›å®æ—¶çš„é€ƒç”Ÿè·¯å¾„è§„åˆ’ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬Sentinel 2å«æ˜Ÿå›¾åƒåˆ†æã€D-linkNetå’ŒNDVIå€¼çš„ä¸­å¿ƒåŒºåŸŸç‡ƒçƒ§ç«æºåˆ†å‰²ã€äººå‘˜å®æ—¶åŠ¨æ€æœ€ä½³è·¯å¾„è§„åˆ’ç­‰ã€‚</li>
<li>results: å¯¹äº8æœˆ24æ—¥é‡åº†é‡ç«çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œç»“æœè¡¨æ˜ï¼ŒåŸºäºUAVå’Œå«æ˜Ÿå›¾åƒä¿¡æ¯çš„åŠ¨æ€æœ€ä½³è·¯å¾„è§„åˆ’ç®—æ³•å¯ä»¥åœ¨å®æ—¶ç«ç¾æƒ…å†µä¸‹ä¸ºäººä»¬æä¾›æœ€ä½³é€ƒç”Ÿè·¯å¾„ã€‚<details>
<summary>Abstract</summary>
UAVs are playing an increasingly important role in the field of wilderness rescue by virtue of their flexibility. This paper proposes a fusion of UAV vision technology and satellite image analysis technology for active wildfires detection and road networks extraction of wildfire areas and real-time dynamic escape route planning for people in distress. Firstly, the fire source location and the segmentation of smoke and flames are targeted based on Sentinel 2 satellite imagery. Secondly, the road segmentation and the road condition assessment are performed by D-linkNet and NDVI values in the central area of the fire source by UAV. Finally, the dynamic optimal route planning for humans in real time is performed by the weighted A* algorithm in the road network with the dynamic fire spread model. Taking the Chongqing wildfire on August 24, 2022, as a case study, the results demonstrate that the dynamic escape route planning algorithm can provide an optimal real-time navigation path for humans in the presence of fire through the information fusion of UAVs and satellites.
</details>
<details>
<summary>æ‘˜è¦</summary>
UAVs åœ¨é‡å¤–æœæ•‘ä¸­å‘æŒ¥è¶Šæ¥è¶Šé‡è¦çš„ä½œç”¨ï¼Œå°¤å…¶æ˜¯å› ä¸ºå®ƒä»¬çš„çµæ´»æ€§ã€‚æœ¬æ–‡æå‡ºäº†ç»“åˆ UAV è§†è§‰æŠ€æœ¯å’Œå«æ˜Ÿå›¾åƒåˆ†ææŠ€æœ¯ï¼Œå®æ—¶è®¡ç®— wildfires çš„å‘ç”Ÿåœ°ç‚¹å’Œç‡ƒçƒ§åŒºåŸŸçš„é“è·¯ç½‘ç»œæŠ½å–ï¼Œä»¥åŠåœ¨äººå‘˜å—æŸæ—¶çš„å®æ—¶æœ€ä¼˜è·¯å¾„è§„åˆ’ã€‚é¦–å…ˆï¼Œé€šè¿‡ sentinel 2 å«æ˜Ÿå›¾åƒï¼Œå®šä½ç«æºä½ç½®å’ŒçƒŸé›¾é¢—ç²’çš„åˆ† segmentationã€‚å…¶æ¬¡ï¼Œé€šè¿‡ D-linkNet å’Œ NDVI å€¼åœ¨ä¸­å¿ƒåœ°åŸŸçš„ç«æºä½ç½®ï¼Œè¿›è¡Œé“è·¯åˆ† segmentation å’Œé“è·¯çŠ¶å†µè¯„ä¼°ã€‚æœ€åï¼Œåœ¨è·¯ç½‘ä¸­ï¼Œä½¿ç”¨åŠ æƒ A\* ç®—æ³•ï¼Œåœ¨å®æ—¶ç«åŠ¿æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä¸ºäººå‘˜åœ¨ç«ç¾ä¸­æä¾›æœ€ä¼˜çš„å®æ—¶å¯¼èˆªè·¯å¾„ã€‚ä»¥2022å¹´8æœˆ24æ—¥çš„é‡åº†é‡ç«ä¸ºä¾‹ï¼Œç»“æœè¡¨æ˜ï¼ŒåŠ¨æ€é€ƒç”Ÿè·¯å¾„è§„åˆ’ç®—æ³•å¯ä»¥åœ¨ UAV å’Œå«æ˜Ÿä¿¡æ¯èåˆçš„æƒ…å†µä¸‹ï¼Œä¸ºäººå‘˜åœ¨ç«ç¾ä¸­æä¾›æœ€ä¼˜çš„å®æ—¶å¯¼èˆªè·¯å¾„ã€‚
</details></li>
</ul>
<hr>
<h2 id="FRDiff-Feature-Reuse-for-Exquisite-Zero-shot-Acceleration-of-Diffusion-Models"><a href="#FRDiff-Feature-Reuse-for-Exquisite-Zero-shot-Acceleration-of-Diffusion-Models" class="headerlink" title="FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models"></a>FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03517">http://arxiv.org/abs/2312.03517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhyuk So, Jungwon Lee, Eunhyeok Park<br>for: æé«˜Diffusionæ¨¡å‹çš„è®¡ç®—æ•ˆç‡ï¼Œä½¿å…¶æ›´åŠ å¹¿æ³›åº”ç”¨ã€‚methods: åˆ©ç”¨æ—¶é—´ç›¸ä¼¼æ€§ redundancyï¼Œé‡ç”¨ç‰¹å¾å›¾ï¼Œä»è€Œé™ä½è®¡ç®—æˆæœ¬ã€‚results: æå‡ºFRDiffæ–¹æ³•ï¼Œå®ç°äº†ç²¾åº¦å’Œå“åº”é€Ÿåº¦ä¹‹é—´çš„å¹³è¡¡ï¼Œåœ¨å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­è·å¾—äº†æ˜¾è‘—æ”¹å–„ã€‚<details>
<summary>Abstract</summary>
The substantial computational costs of diffusion models, particularly due to the repeated denoising steps crucial for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degradation. In our work, we introduce an advanced acceleration technique that leverages the temporal redundancy inherent in diffusion models. Reusing feature maps with high temporal similarity opens up a new opportunity to save computation without sacrificing output quality. To realize the practical benefits of this intuition, we conduct an extensive analysis and propose a novel method, FRDiff. FRDiff is designed to harness the advantages of both reduced NFE and feature reuse, achieving a Pareto frontier that balances fidelity and latency trade-offs in various generative tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusionæ¨¡å‹çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œå°¤å…¶æ˜¯å› ä¸ºé«˜è´¨é‡å›¾åƒç”Ÿæˆéœ€è¦å¤šæ¬¡å‡é›‘æ­¥éª¤ã€‚è™½ç„¶ä¸€äº›ç ”ç©¶å·²ç»å°è¯•é€šè¿‡é™ä½å¾—åˆ†å‡½æ•°è¯„ä¼°æ•°é‡ä½¿ç”¨é«˜çº§ODEè§£å†³æ–¹æ¡ˆæ¥é™ä½è®¡ç®—æˆæœ¬ï¼Œä½†æ˜¯å‡å°‘å‡é›‘è¿­ä»£æ•°ä¼šé”™è¿‡æ›´æ–°ç»†èŠ‚ï¼Œå¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§é«˜çº§åŠ é€ŸæŠ€æœ¯ï¼Œåˆ©ç”¨Diffusionæ¨¡å‹å†…ç½®çš„æ—¶é—´é‡å¤æ€§ã€‚é‡ç”¨æ—¶é—´ç›¸ä¼¼çš„ç‰¹å¾å›¾opens up a new opportunity to save computation without sacrificing output qualityã€‚ä¸ºäº†å®ç°è¿™ä¸ªç†å¿µçš„å®ç”¨æ•ˆæœï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æå¹¶æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ŒFRDiffã€‚ FRDiffæ—¨åœ¨åˆ©ç”¨å‡å°‘NFEå’Œç‰¹å¾é‡ç”¨çš„ä¼˜ç‚¹ï¼Œå®ç°å¤šç§ç”Ÿæˆä»»åŠ¡ä¸­çš„å¹³è¡¡è´¨é‡å’Œå»¶è¿Ÿäº¤æ˜“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Speculative-Exploration-on-the-Concept-of-Artificial-Agents-Conducting-Autonomous-Research"><a href="#Speculative-Exploration-on-the-Concept-of-Artificial-Agents-Conducting-Autonomous-Research" class="headerlink" title="Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research"></a>Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03497">http://arxiv.org/abs/2312.03497</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/t46/research-automation-perspective-paper">https://github.com/t46/research-automation-perspective-paper</a></li>
<li>paper_authors: Shiro Takagi</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ä¸€ç§äººå·¥æ™ºèƒ½å¯ä»¥è¿›è¡Œç ”ç©¶çš„æ¦‚å¿µã€‚</li>
<li>methods: è®ºæ–‡é¦–å…ˆæè¿°äº†ç ”ç©¶çš„æ¦‚å¿µï¼Œä»¥æä¾›åˆ›æ–°çš„å¼€å§‹ç‚¹ã€‚ç„¶åï¼Œå®ƒè€ƒè™‘äº†ç ”ç©¶çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼ŒåŒ…æ‹¬é—®é¢˜å®šä¹‰ã€å‡è®¾ç”Ÿæˆå’Œå‡è®¾éªŒè¯ã€‚è¿™äº›è®¨è®ºåŒ…æ‹¬äº†æœºå™¨è‡ªåŠ¨å®Œæˆè¿™äº›ä»»åŠ¡çš„æ½œåœ¨å’ŒæŒ‘æˆ˜ã€‚</li>
<li>results: è®ºæ–‡ç®€è¦è®¨è®ºäº†è¿™äº›ç ”ç©¶èƒ½åŠ›çš„agentçš„ç›¸äº’å…³ç³»å’Œäº¤å ã€‚æœ€åï¼Œå®ƒæå‡ºäº†åˆæ­¥çš„æ€è€ƒï¼Œä»¥ä¾¿æ¢ç´¢è¿™äº›ç ”ç©¶èƒ½åŠ›agentçš„å‘å±•æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
This paper engages in a speculative exploration of the concept of an artificial agent capable of conducting research. Initially, it examines how the act of research can be conceptually characterized, aiming to provide a starting point for discussions about what it means to create such agents. The focus then shifts to the core components of research: question formulation, hypothesis generation, and hypothesis verification. This discussion includes a consideration of the potential and challenges associated with enabling machines to autonomously perform these tasks. Subsequently, this paper briefly considers the overlapping themes and interconnections that underlie them. Finally, the paper presents preliminary thoughts on prototyping as an initial step towards uncovering the challenges involved in developing these research-capable agents.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡å±•å¼€äº†ä¸€ç§äººå·¥æ™ºèƒ½å¯ä»¥è¿›è¡Œç ”ç©¶çš„æ¦‚å¿µã€‚æœ€åˆï¼Œå®ƒæè¿°äº†ç ”ç©¶çš„æ¦‚å¿µï¼Œä»¥ä¾¿æä¾›è®¨è®ºçš„èµ·ç‚¹ã€‚ç„¶åï¼Œå®ƒshiftåˆ°ç ”ç©¶çš„æ ¸å¿ƒç»„ä»¶ï¼šé—®é¢˜å®šä¹‰ã€å‡è®¾ç”Ÿæˆå’Œå‡è®¾éªŒè¯ã€‚è¿™ä¸ªè®¨è®ºåŒ…æ‹¬æœºå™¨è‡ªåŠ¨æ‰§è¡Œè¿™äº›ä»»åŠ¡çš„æ½œåœ¨å’ŒæŒ‘æˆ˜ã€‚æ¥ç€ï¼Œè¿™ç¯‡è®ºæ–‡ç®€è¦ä»‹ç»äº†è¿™äº›ä¸»é¢˜ä¹‹é—´çš„é‡å ç‚¹å’Œè”ç³»ã€‚æœ€åï¼Œå®ƒæä¾›äº†åˆæ­¥æ€æƒ³ï¼Œä»¥ä¾¿å¼€å§‹è¯„ä¼°åœ¨å¼€å‘è¿™äº›ç ”ç©¶èƒ½åŠ›çš„æœºå™¨äººæ—¶é‡åˆ°çš„æŒ‘æˆ˜ã€‚Note: Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Learning-From-Scenarios-for-Stochastic-Repairable-Scheduling"><a href="#Learning-From-Scenarios-for-Stochastic-Repairable-Scheduling" class="headerlink" title="Learning From Scenarios for Stochastic Repairable Scheduling"></a>Learning From Scenarios for Stochastic Repairable Scheduling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03492">http://arxiv.org/abs/2312.03492</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kimvandenhouten/learning-from-scenarios-for-repairable-stochastic-scheduling">https://github.com/kimvandenhouten/learning-from-scenarios-for-repairable-stochastic-scheduling</a></li>
<li>paper_authors: Kim van den Houten, David M. J. Tax, Esteban Freydell, Mathijs de Weerdt</li>
<li>for:  Linear objective optimization with uncertain parameter values in a stochastic scheduling problem.</li>
<li>methods:  Decision-focused learning with stochastic smoothing to adapt existing techniques to the scheduling problem.</li>
<li>results:  Extensive experimental evaluation to compare the performance of decision-focused learning with the state of the art for scenario-based stochastic optimization.Hereâ€™s the text in Simplified Chinese:</li>
<li>for:  Linearç›®æ ‡ä¼˜åŒ– WITH uncertain parameter values in a stochastic scheduling problem.</li>
<li>methods:  Decision-focused learning WITH stochastic smoothing to adapt existing techniques to the scheduling problem.</li>
<li>results:  Extensive experimental evaluation to compare the performance of decision-focused learning WITH the state of the art for scenario-based stochastic optimization.<details>
<summary>Abstract</summary>
When optimizing problems with uncertain parameter values in a linear objective, decision-focused learning enables end-to-end learning of these values. We are interested in a stochastic scheduling problem, in which processing times are uncertain, which brings uncertain values in the constraints, and thus repair of an initial schedule may be needed. Historical realizations of the stochastic processing times are available. We show how existing decision-focused learning techniques based on stochastic smoothing can be adapted to this scheduling problem. We include an extensive experimental evaluation to investigate in which situations decision-focused learning outperforms the state of the art for such situations: scenario-based stochastic optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
å½“ä¼˜åŒ–å…·æœ‰ä¸ç¡®å®šå‚æ•°å€¼çš„çº¿æ€§ç›®æ ‡é—®é¢˜æ—¶ï¼Œå†³ç­–å…³æ³¨å­¦ä¹ å¯ä»¥å®ç°ç«¯åˆ°ç«¯å­¦ä¹ è¿™äº›å€¼ã€‚æˆ‘ä»¬å…³æ³¨ä¸€ä¸ªéšæœºå¤„ç†æ—¶é—´çš„è°ƒåº¦é—®é¢˜ï¼Œå¤„ç†æ—¶é—´å…·æœ‰éšæœºæ€§ï¼Œå› æ­¤å¯èƒ½éœ€è¦ä¿®å¤åˆå§‹è°ƒåº¦ã€‚å†å²å®ç°éšæœºå¤„ç†æ—¶é—´çš„æ•°æ®å¯ç”¨ã€‚æˆ‘ä»¬ä»‹ç»äº†ç°æœ‰çš„å†³ç­–å…³æ³¨å­¦ä¹ æŠ€æœ¯ï¼ŒåŸºäºéšæœºç¼“å’Œï¼Œå¦‚ä½•åº”ç”¨äºè¿™ä¸ªè°ƒåº¦é—®é¢˜ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒè¯„ä¼°ï¼Œä»¥ Investigateåœ¨å“ªäº›æƒ…å†µä¸‹å†³ç­–å…³æ³¨å­¦ä¹ è¶…è¶Šäº†ç°çŠ¶å¤©åœ°éšæœºä¼˜åŒ–ã€‚Here's the breakdown of the translation:* å½“ä¼˜åŒ– (dÄng yÃ²u jÃ¬) - "when optimizing"* å…·æœ‰ä¸ç¡®å®šå‚æ•°å€¼ (yÇ’u yÇ’u bÃ¹ jÃ¬ pin yÃ¨) - "with uncertain parameter values"* çº¿æ€§ç›®æ ‡é—®é¢˜ (xiÃ n xÃ¬ng mÃ¹ tiÃ o wÃ¨n tÃ­) - "linear objective"* å†³ç­–å…³æ³¨å­¦ä¹  (jÃ¬ dÃ o guÄn zhÃ¹ xuÃ© xÃ­) - "decision-focused learning"* ç«¯åˆ°ç«¯å­¦ä¹  (dÃ­an dÃ o diÃ n xuÃ© xÃ­) - "end-to-end learning"* è¿™äº›å€¼ (zhÃ¨ xiÄ“) - "these values"* éšæœºå¤„ç†æ—¶é—´ (suÃ¬ jiÃ ng hÃ²u zhÃ­ shÃ­) - "processing times are uncertain"* éšæœºå€¼ (suÃ¬ jiÃ ng yÃ¨) - "random values"*  constraints (guÄn lÃ¬) - "constraints"* ä¿®å¤ (xiÅ« gÃ²ng) - "repair"* åˆå§‹è°ƒåº¦ (chÅ« shÃ­ tiÃ o dÃ o) - "initial schedule"* å†å²å®ç° (lÃ¬ shÇ shÃ­ jÃ¬) - "historical realizations"* æ•°æ® (shÃ¹ dÃ o) - "data"* å¯ç”¨ (kÄ› yÃ²u) - "available"* ç°æœ‰çš„ (xiÃ n yÇ’u de) - "existing"* å†³ç­–å…³æ³¨å­¦ä¹ æŠ€æœ¯ (jÃ¬ dÃ o guÄn zhÃ¹ xuÃ© xÃ­ jÃ¬ shÃ¹) - "existing decision-focused learning techniques"* åŸºäºéšæœºç¼“ (jÄ« yÃº suÃ¬ jiÃ ng bÃ¬) - "based on stochastic smoothing"* åº”ç”¨äº (fÃ¹ yÃ¹ yÇ”) - "applied to"* è¿™ä¸ªè°ƒåº¦é—®é¢˜ (zhÃ¨ ge tiÃ o dÃ o wÃ¨n tÃ­) - "this scheduling problem"*  Investigate (yÃ n jÃ­) - "investigate"* æƒ…å†µ (qÃ­ng jÃ¬) - "situations"* è¶…è¶Š (chÄo yÃº) - "outperform"* ç°çŠ¶å¤©åœ° (xiÃ n zhÃ¨ng tiÄn dÃ¬) - "current state of the art"* éšæœºä¼˜åŒ– (suÃ¬ jiÃ ng yÃ¬ huÃ ) - "scenario-based stochastic optimization"
</details></li>
</ul>
<hr>
<h2 id="JAMMIN-GPT-Text-based-Improvisation-using-LLMs-in-Ableton-Live"><a href="#JAMMIN-GPT-Text-based-Improvisation-using-LLMs-in-Ableton-Live" class="headerlink" title="JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live"></a>JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03479">http://arxiv.org/abs/2312.03479</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/supersational/jammin-gpt">https://github.com/supersational/jammin-gpt</a></li>
<li>paper_authors: Sven Hollowell, Tashi Namgyal, Paul Marshall</li>
<li>for: è¿™ä¸ªç³»ç»Ÿæ˜¯ä¸ºAbleton Liveç”¨æˆ·åˆ›å»ºMIDI-clipè€Œè®¾è®¡çš„ï¼Œä»¥ä¾¿é€šè¿‡ musical descriptions æ¥å‘½åå®ƒä»¬ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿä½¿ç”¨ ChatGPT å›ç­”å™¨æ¥ç”Ÿæˆæ–‡æœ¬åŸºäº musical formatsï¼Œå¦‚ ABC notationã€chord symbols æˆ– drum tablatureï¼Œä»¥ä¾¿åœ¨ Ableton çš„clip viewä¸­æ’å…¥ Musical ideasã€‚</li>
<li>results: è¯¥ç³»ç»Ÿå¯ä»¥å¸®åŠ©ç”¨æˆ·å¿«é€Ÿç”Ÿæˆ musical ideasï¼Œå¹¶ä¸”å¯ä»¥è®©ç”¨æˆ·åœ¨åˆ›ä½œè¿‡ç¨‹ä¸­ä¿æŒæµç•…ï¼Œä¸éœ€è¦åœä¸‹æ¥ç¼–è¾‘ codeã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ—¢æé«˜äº† musical åˆ›ä½œæ•ˆç‡ï¼Œä¹Ÿé™ä½äº†å­¦ä¹ æˆæœ¬ã€‚<details>
<summary>Abstract</summary>
We introduce a system that allows users of Ableton Live to create MIDI-clips by naming them with musical descriptions. Users can compose by typing the desired musical content directly in Ableton's clip view, which is then inserted by our integrated system. This allows users to stay in the flow of their creative process while quickly generating musical ideas. The system works by prompting ChatGPT to reply using one of several text-based musical formats, such as ABC notation, chord symbols, or drum tablature. This is an important step in integrating generative AI tools into pre-existing musical workflows, and could be valuable for content makers who prefer to express their creative vision through descriptive language. Code is available at https://github.com/supersational/JAMMIN-GPT.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªç³»ç»Ÿï¼Œè®©Ableton Liveç”¨æˆ·å¯ä»¥é€šè¿‡ Musical descriptions åç§° MIDI-clipã€‚ç”¨æˆ·å¯ä»¥åœ¨Abletonçš„ clip view ä¸­ç›´æ¥è¾“å…¥ Desired musical contentï¼Œæˆ‘ä»¬çš„æ•´åˆç³»ç»Ÿå°†å…¶æ’å…¥ã€‚è¿™ä½¿ç”¨æˆ·å¯ä»¥ä¿æŒåˆ›ä½œè¿‡ç¨‹ä¸­çš„æµåŠ¨æ€§ï¼Œå¿«é€Ÿç”Ÿæˆ musical ideasã€‚ç³»ç»Ÿå·¥ä½œæ–¹å¼æ˜¯é€šè¿‡è¯·æ±‚ ChatGPT å›ç­”ä½¿ç”¨ä¸€äº›æ–‡æœ¬åŸºäºçš„ Musical formatsï¼Œä¾‹å¦‚ ABC notationã€chord symbols æˆ– drum tablatureã€‚è¿™æ˜¯ç»Ÿåˆç”Ÿæˆ AI å·¥å…·åˆ°ç°æœ‰çš„ Musical workflows çš„é‡è¦ä¸€æ­¥ï¼Œå¯èƒ½å¯¹å†…å®¹åˆ¶ä½œè€…æœ‰ä»·å€¼ï¼Œä»–ä»¬å¯èƒ½ prefer é€šè¿‡æè¿°æ€§è¯­è¨€è¡¨è¾¾åˆ›ä½œæ„ä¹‰ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/supersational/JAMMIN-GPT è·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Molecule-Joint-Auto-Encoding-Trajectory-Pretraining-with-2D-and-3D-Diffusion"><a href="#Molecule-Joint-Auto-Encoding-Trajectory-Pretraining-with-2D-and-3D-Diffusion" class="headerlink" title="Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion"></a>Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03475">http://arxiv.org/abs/2312.03475</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weitao Du, Jiujiu Chen, Xuecang Zhang, Zhiming Ma, Shengchao Liu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜äººå·¥æ™ºèƒ½åœ¨è¯ç‰©å‘ç°ä¸­çš„åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨æœºå™¨å­¦ä¹ å’ŒåŒ–å­¦é¢†åŸŸã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ prÃ©-training æ–¹æ³•ï¼Œç§°ä¸ºåˆ†å­è”åˆè‡ªåŠ¨ç¼–ç ï¼ˆMoleculeJAEï¼‰ï¼Œå¯ä»¥å­¦ä¹ åˆ†å­çš„äºŒç»´ç²¾åº¦ï¼ˆé”®ç»“æ„ï¼‰å’Œä¸‰ç»´å½¢æ€ï¼ˆå‡ ä½•ï¼‰ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿå¢å¼ºçš„æ‰©æ•£è¿‡ç¨‹ï¼Œä»¥è‡ªç„¶åœ°å­¦ä¹ åˆ†å­çš„å†…åœ¨ç»“æ„ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒMoleculeJAE èƒ½å¤Ÿè¾¾åˆ°æ¯”è¾ƒå‡ºè‰²çš„æ€§èƒ½ï¼Œåœ¨ 20 ä¸ªä»»åŠ¡ä¸­çš„ 15 ä¸ªä»»åŠ¡ä¸­æ¯” 12 ä¸ªåŸºçº¿æ¨¡å‹æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
Recently, artificial intelligence for drug discovery has raised increasing interest in both machine learning and chemistry domains. The fundamental building block for drug discovery is molecule geometry and thus, the molecule's geometrical representation is the main bottleneck to better utilize machine learning techniques for drug discovery. In this work, we propose a pretraining method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn both the 2D bond (topology) and 3D conformation (geometry) information, and a diffusion process model is applied to mimic the augmented trajectories of such two modalities, based on which, MoleculeJAE will learn the inherent chemical structure in a self-supervised manner. Thus, the pretrained geometrical representation in MoleculeJAE is expected to benefit downstream geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by reaching state-of-the-art performance on 15 out of 20 tasks by comparing it with 12 competitive baselines.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Note: Simplified Chinese is also known as "ç®€åŒ–å­—" or "ç®€åŒ–å­—".)
</details></li>
</ul>
<hr>
<h2 id="Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data"><a href="#Data-is-Overrated-Perceptual-Metrics-Can-Lead-Learning-in-the-Absence-of-Training-Data" class="headerlink" title="Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data"></a>Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03455">http://arxiv.org/abs/2312.03455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tashi Namgyal, Alexander Hepburn, Raul Santos-Rodriguez, Valero Laparra, Jesus Malo</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ç”¨äºè¯„ä¼°è‡ªç„¶ä¿¡å·è´¨é‡çš„æ–¹æ³•ï¼Œå¦‚å›¾åƒå’ŒéŸ³é¢‘ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ„ŸçŸ¥æŒ‡æ ‡æ¥è¯„ä¼°è‡ªç„¶ä¿¡å·çš„è´¨é‡ï¼Œæ„ŸçŸ¥æŒ‡æ ‡æ˜¯åŸºäºäººç±»è§‚å¯Ÿè€…çš„æ„ŸçŸ¥è¡Œä¸ºï¼Œé€šå¸¸èƒ½å¤Ÿæ•æ‰è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ã€‚</li>
<li>results: è®ºæ–‡å‘ç°ï¼Œä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡ä½œä¸ºæŸå¤±å‡½æ•°å¯ä»¥è®©ç”Ÿæˆæ¨¡å‹æ›´å¥½åœ°æ•æ‰è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ï¼Œå¹¶åœ¨æµ‹è¯•æ—¶é‡å»ºspectrogramså’Œé‡æ–°ç”Ÿæˆçš„éŸ³é¢‘ä¸­å¾—åˆ°æ›´å¥½çš„ç»“æœï¼Œè¿™è¡¨æ˜ä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡å¯ä»¥æ›´å¥½åœ°é€‚åº”æœªç»è§è¿‡çš„è‡ªç„¶ä¿¡å·ã€‚<details>
<summary>Abstract</summary>
Perceptual metrics are traditionally used to evaluate the quality of natural signals, such as images and audio. They are designed to mimic the perceptual behaviour of human observers and usually reflect structures found in natural signals. This motivates their use as loss functions for training generative models such that models will learn to capture the structure held in the metric. We take this idea to the extreme in the audio domain by training a compressive autoencoder to reconstruct uniform noise, in lieu of natural data. We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss. This demonstrates better generalisation to unseen natural signals when using perceptual metrics.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡é€šå¸¸ç”¨äºè¯„ä¼°è‡ªç„¶ä¿¡å·çš„è´¨é‡ï¼Œå¦‚å›¾åƒå’ŒéŸ³é¢‘ã€‚å®ƒä»¬æ˜¯ä¸ºæ¨¡ä»¿äººç±»è§‚å¯Ÿè€…çš„æ„ŸçŸ¥è¡Œä¸ºè€Œè®¾è®¡çš„ï¼Œé€šå¸¸åæ˜ è‡ªç„¶ä¿¡å·ä¸­çš„ç»“æ„ã€‚è¿™ç§æƒ³æ³•é©±åŠ¨äº†ä½¿ç”¨æ„ŸçŸ¥æŒ‡æ ‡ä½œä¸ºç”Ÿæˆæ¨¡å‹çš„æŸå¤±å‡½æ•°çš„ä½¿ç”¨ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥æ•æ‰æŒ‡æ ‡ä¸­çš„ç»“æ„ã€‚åœ¨éŸ³é¢‘é¢†åŸŸä¸­ï¼Œæˆ‘ä»¬Push this idea to the extreme by training a compressive autoencoder to reconstruct uniform noise, instead of natural data. We show that training with perceptual losses improves the reconstruction of spectrograms and re-synthesized audio at test time over models trained with a standard Euclidean loss. This demonstrates better generalization to unseen natural signals when using perceptual metrics.Here's the translation breakdown:* ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡ (traditional perceptual metrics) -> ä¼ ç»Ÿçš„æ„ŸçŸ¥æŒ‡æ ‡ (traditional perceptual metrics)* è‡ªç„¶ä¿¡å· (natural signals) -> è‡ªç„¶ä¿¡å· (natural signals)* è´¨é‡ (quality) -> è´¨é‡ (quality)* æ¨¡ä»¿ (mimic) -> æ¨¡ä»¿ (mimic)* äººç±»è§‚å¯Ÿè€… (human observers) -> äººç±»è§‚å¯Ÿè€… (human observers)* æ„ŸçŸ¥è¡Œä¸º (perceptual behavior) -> æ„ŸçŸ¥è¡Œä¸º (perceptual behavior)* ç»“æ„ (structure) -> ç»“æ„ (structure)* ç”Ÿæˆæ¨¡å‹ (generative models) -> ç”Ÿæˆæ¨¡å‹ (generative models)* æŸå¤±å‡½æ•° (loss functions) -> æŸå¤±å‡½æ•° (loss functions)* æ•æ‰ (capture) -> æ•æ‰ (capture)* æŒ‡æ ‡ä¸­çš„ç»“æ„ (structure held in the metric) -> æŒ‡æ ‡ä¸­çš„ç»“æ„ (structure held in the metric)* éŸ³é¢‘é¢†åŸŸ (audio domain) -> éŸ³é¢‘é¢†åŸŸ (audio domain)* æŠ½è±¡å‹ç¼© autoencoder (compressive autoencoder) -> æŠ½è±¡å‹ç¼© autoencoder (compressive autoencoder)* é‡å»º (reconstruct) -> é‡å»º (reconstruct)* å‹ç¼© (compressive) -> å‹ç¼© (compressive)* è‡ªç„¶æ•°æ® (natural data) -> è‡ªç„¶æ•°æ® (natural data)* æ ‡å‡†çš„æ¬§å‡ ä½•è½å¤± (standard Euclidean loss) -> æ ‡å‡†çš„æ¬§å‡ ä½•è½å¤± (standard Euclidean loss)* æµ‹è¯•æ—¶ (at test time) -> æµ‹è¯•æ—¶ (at test time)* æ€»ä½“ (overall) -> æ€»ä½“ (overall)* æ›´å¥½çš„æ³›åŒ– (better generalization) -> æ›´å¥½çš„æ³›åŒ– (better generalization)
</details></li>
</ul>
<hr>
<h2 id="Quantum-Inspired-Neural-Network-Model-of-Optical-Illusions"><a href="#Quantum-Inspired-Neural-Network-Model-of-Optical-Illusions" class="headerlink" title="Quantum-Inspired Neural Network Model of Optical Illusions"></a>Quantum-Inspired Neural Network Model of Optical Illusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03447">http://arxiv.org/abs/2312.03447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivan S. Maksymov<br>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶äººç±»å¯¹æ¶‚æŠ¹å¼ä¸ç¨³å®šç‰©ä½“ï¼ˆå¦‚å°¼å…‹å°”ç«‹æ–¹ä½“ï¼‰çš„è§‚å¯Ÿå’Œç†è§£è€Œå†™çš„ã€‚methods: ä½œè€…ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ¥æ¨¡æ‹Ÿäººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„è§‚å¯Ÿå’Œç†è§£ï¼Œå¹¶ä½¿ç”¨é‡å­ç”Ÿæˆå™¨æ¥å®šä¹‰ç¥ç»ç½‘ç»œè¿æ¥çš„æƒé‡ã€‚results: ç ”ç©¶å‘ç°ï¼Œå°¼å…‹å°”ç«‹æ–¹ä½“çš„å®é™…è§‚å¯ŸçŠ¶æ€æ˜¯ä¸€ç§åŸºäºé‡å­æœºåˆ¶çš„è¶…positionï¼Œè¿™ä¸ ĞºĞ»Ğ°ÑÑĞ¸icalç†è®ºé¢„æµ‹çš„ä¸¤ç§åŸºæœ¬è§‚å¯ŸçŠ¶æ€ç›¸ç¬¦ã€‚è¿™äº›ç»“æœå°†æœ‰ç”¨äºè§†é¢‘æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç³»ç»Ÿï¼Œä»¥åŠç ”ç©¶æœºå™¨å­¦ä¹ ã€è§†è§‰ã€å¿ƒç†å­¦å’Œé‡å­æœºåˆ¶çš„äººç±»å¿ƒç†å’Œå†³ç­–ã€‚<details>
<summary>Abstract</summary>
Ambiguous optical illusions have been a paradigmatic object of fascination, research and inspiration in arts, psychology and video games. However, accurate computational models of perception of ambiguous figures have been elusive. In this paper, we design and train a deep neural network model to simulate the human's perception of the Necker cube, an ambiguous drawing with several alternating possible interpretations. Defining the weights of the neural network connection using a quantum generator of truly random numbers, in agreement with the emerging concepts of quantum artificial intelligence and quantum cognition we reveal that the actual perceptual state of the Necker cube is a qubit-like superposition of the two fundamental perceptual states predicted by classical theories. Our results will find applications in video games and virtual reality systems employed for training of astronauts and operators of unmanned aerial vehicles. They will also be useful for researchers working in the fields of machine learning and vision, psychology of perception and quantum-mechanical models of human mind and decision-making.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›°æƒ‘çš„è§†è§‰é”™è§‰å·²ç»æˆä¸ºè‰ºæœ¯ã€å¿ƒç†å­¦å’Œç”µå­æ¸¸æˆç­‰é¢†åŸŸçš„ä¸€ç§ç‹¬ç‰¹çš„å¯¹è±¡ï¼Œä½†æ˜¯å‡†ç¡®çš„è®¡ç®—æ¨¡å‹æ¥è§£é‡Šäººç±»çš„è§†è§‰å´æ˜¯å›°éš¾çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºæ¨¡æ‹Ÿäººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„è§†è§‰å«ä¹‰ã€‚ä½¿ç”¨é‡å­ç”Ÿæˆå™¨ç”ŸæˆçœŸå®éšæœºæ•°çš„æƒé‡ï¼Œä¸é‡å­äººå·¥æ™ºèƒ½å’Œé‡å­è®¤çŸ¥ç†è®ºç›¸å»åˆï¼Œæˆ‘ä»¬å‘ç°äº†äººç±»å¯¹å°¼å…‹å°”ç«‹æ–¹ä½“çš„å®é™…è§†è§‰çŠ¶æ€æ˜¯ä¸€ç§åŸºäºä¸¤ä¸ªåŸºæœ¬è§†è§‰çŠ¶æ€çš„QUBIT-likeè¶…positionã€‚æˆ‘ä»¬çš„ç»“æœå°†æ‰¾åˆ°åº”ç”¨äºç”µå­æ¸¸æˆå’Œè™šæ‹Ÿç°å®ç³»ç»Ÿï¼Œç”¨äºè®­ç»ƒå®‡èˆªå‘˜å’Œæ— äººé£è¡Œå™¨æ“ä½œå‘˜ã€‚åŒæ—¶ï¼Œè¿™äº›ç»“æœä¹Ÿå°†å¯¹æœºå™¨å­¦ä¹ ã€è§†è§‰å’Œå¿ƒç†å­¦ç ”ç©¶æœ‰å¾ˆå¤§çš„å¸®åŠ©ï¼Œä»¥åŠé‡å­æœºå™¨äººæ¨¡å‹å’Œå†³ç­–çš„ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sports-Recommender-Systems-Overview-and-Research-Issues"><a href="#Sports-Recommender-Systems-Overview-and-Research-Issues" class="headerlink" title="Sports Recommender Systems: Overview and Research Issues"></a>Sports Recommender Systems: Overview and Research Issues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03785">http://arxiv.org/abs/2312.03785</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Felfernig, Manfred Wundara, Thi Ngoc Trang Tran, Viet-Man Le, Sebastian Lubos, Seda Polat-Erdeniz</li>
<li>for: è¿åŠ¨æ¨èç³»ç»Ÿåœ¨å¥åº·ç”Ÿæ´»ã€äººé™…å…³ç³»å’Œè¿åŠ¨è¡¨ç°ç­‰æ–¹é¢å—åˆ°è¶Šæ¥è¶Šå¤šçš„æ³¨æ„ã€‚è¿™äº›ç³»ç»Ÿå¯ä»¥å¸®åŠ©äººä»¬åœ¨è¿åŠ¨ä¸­é€‰æ‹©é€‚åˆè‡ªå·±çš„é¤é£Ÿã€è®­ç»ƒæ–¹æ³•ã€æ‰èƒ½å’Œå›¢é˜Ÿç­‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡åŸºäºä¸åŒçš„å®è·µä¾‹è¿›è¡Œäº†è¿åŠ¨æ¨èç³»ç»Ÿçš„åº”ç”¨å’ŒæŠ€æœ¯çš„æ¦‚è¿°ã€‚å®ƒä»¬åŒ…æ‹¬é¤é£Ÿæ¨èã€è®­ç»ƒæ–¹æ³•æ¨èã€æ‰èƒ½å’Œå›¢é˜Ÿæ¨èä»¥åŠç«èµ›ä¸­çš„ç­–ç•¥æ¨èç­‰ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡åˆ†æäº†è¿åŠ¨æ¨èç³»ç»Ÿçš„ç›¸å…³å›½é™…å’Œå¼€å±•ç ”ç©¶é—®é¢˜ã€‚å®ƒè¿˜æå‡ºäº†ä¸€äº›æœªè§£å†³çš„ç ”ç©¶é—®é¢˜ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢è¿åŠ¨æ¨èç³»ç»Ÿçš„åº”ç”¨å’ŒæŠ€æœ¯å‘å±•ã€‚<details>
<summary>Abstract</summary>
Sports recommender systems receive an increasing attention due to their potential of fostering healthy living, improving personal well-being, and increasing performances in sport. These systems support people in sports, for example, by the recommendation of healthy and performance boosting food items, the recommendation of training practices, talent and team recommendation, and the recommendation of specific tactics in competitions. With applications in the virtual world, for example, the recommendation of maps or opponents in e-sports, these systems already transcend conventional sports scenarios where physical presence is needed. On the basis of different working examples, we present an overview of sports recommender systems applications and techniques. Overall, we analyze the related state-of-the-art and discuss open research issues.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½“è‚²æ¨èç³»ç»Ÿåœ¨æœ€è¿‘å‡ å¹´æ¥å¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œè¿™ä¸»è¦å½’åŠŸäºå®ƒä»¬åœ¨å¥åº·ç”Ÿæ´»ã€ä¸ªäººå¥åº·å’Œè¿åŠ¨è¡¨ç°æ–¹é¢çš„æ½œåœ¨ä½œç”¨ã€‚è¿™äº›ç³»ç»Ÿæ”¯æŒäººä»¬åœ¨è¿åŠ¨æ–¹é¢ï¼Œä¾‹å¦‚ï¼Œæ¨èå¥åº·å’Œè¡¨ç°æå‡çš„é£Ÿå“ã€è®­ç»ƒæ–¹æ³•ã€æ‰èƒ½å’Œå›¢é˜Ÿæ¨èã€ç«èµ›ä¸­ç‰¹å®šæˆ˜æ–—ç­–ç•¥ç­‰ç­‰ã€‚åœ¨è™šæ‹Ÿä¸–ç•Œä¸­ï¼Œä¾‹å¦‚ç”µå­ç«æŠ€ï¼Œè¿™äº›ç³»ç»Ÿå·²ç»è¶…è¶Šäº†ä¼ ç»Ÿçš„ä½“è‚²åœºæ™¯ï¼Œéœ€è¦ç‰©ç†å­˜åœ¨ã€‚åŸºäºä¸åŒçš„å®è·µä¾‹å­ï¼Œæˆ‘ä»¬æä¾›ä½“è‚²æ¨èç³»ç»Ÿåº”ç”¨å’ŒæŠ€æœ¯çš„æ¦‚è¿°ï¼Œå¹¶æ€»ç»“ç›¸å…³çš„ç°çŠ¶å’Œæœªæ¥ç ”ç©¶æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Approximating-Solutions-to-the-Knapsack-Problem-using-the-Lagrangian-Dual-Framework"><a href="#Approximating-Solutions-to-the-Knapsack-Problem-using-the-Lagrangian-Dual-Framework" class="headerlink" title="Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework"></a>Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03413">http://arxiv.org/abs/2312.03413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mitchell Keegan, Mahdi Abolghasemi</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºLagrangian dual frameworkçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œç”¨äºè§£å†³ç®±å­é—®é¢˜ï¼ˆCombinatorial Optimizationï¼‰ï¼Œå¹¶ä¸”èƒ½å¤Ÿæé«˜çº¦æŸæ»¡è¶³åº¦ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹æ¥è¿‘ä¼¼ç®±å­é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”ä½¿ç”¨Lagrangian dual frameworkæ¥åŠ ä»¥çº¦æŸæ»¡è¶³ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå…·æœ‰å¼ºå¤§çš„çº¦æŸæ»¡è¶³åº¦ï¼Œä½†æ˜¯æœ‰ä¸€å®šçš„ä¼˜åŒ–ç‡ä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸å…·æœ‰çº¦æŸæ¨¡å‹çš„åŸºå‡†ç¥ç»ç½‘ç»œæ¨¡å‹ä¼šå…·æœ‰æ›´é«˜çš„ä¼˜åŒ–ç‡ï¼Œä½†æ˜¯çº¦æŸæ»¡è¶³åº¦è¾ƒå·®ã€‚<details>
<summary>Abstract</summary>
The Knapsack Problem is a classic problem in combinatorial optimisation. Solving these problems may be computationally expensive. Recent years have seen a growing interest in the use of deep learning methods to approximate the solutions to such problems. A core problem is how to enforce or encourage constraint satisfaction in predicted solutions. A promising approach for predicting solutions to constrained optimisation problems is the Lagrangian Dual Framework which builds on the method of Lagrangian Relaxation. In this paper we develop neural network models to approximate Knapsack Problem solutions using the Lagrangian Dual Framework while improving constraint satisfaction. We explore the problems of output interpretation and model selection within this context. Experimental results show strong constraint satisfaction with a minor reduction of optimality as compared to a baseline neural network which does not explicitly model the constraints.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šé›¶é’±åŒ…é—®é¢˜ã€‹æ˜¯ä¸€ä¸ªç»å…¸çš„ç»„åˆä¼˜åŒ–é—®é¢˜ã€‚è§£å†³è¿™ç±»é—®é¢˜å¯èƒ½æ˜¯ computationally expensiveã€‚è¿‘å¹´æ¥ï¼Œæœ‰è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•æ¥è¿‘ä¼¼è§£å†³è¿™ç±»é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚æ ¸å¿ƒé—®é¢˜æ˜¯å¦‚ä½•åœ¨é¢„æµ‹è§£å†³æ–¹æ¡ˆä¸­å¼ºåˆ¶æˆ–ä¿ƒè¿›çº¦æŸæ»¡è¶³ã€‚æˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­å¼€å‘äº†åŸºäºLagrangian Dual Frameworkçš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œä»¥ä¼˜åŒ–é›¶é’±åŒ…é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶æé«˜çº¦æŸæ»¡è¶³æ€§ã€‚æˆ‘ä»¬è¿˜æ¢è®¨äº†è¾“å‡ºè§£é‡Šå’Œæ¨¡å‹é€‰æ‹©é—®é¢˜åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥å¼ºåˆ¶æ»¡è¶³çº¦æŸï¼Œä½†æ˜¯æœ‰ä¸€å®šçš„ä¼˜åŒ–ç‡ä¸‹é™ç›¸æ¯”äºåŸºå‡†ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalized-Contrastive-Divergence-Joint-Training-of-Energy-Based-Model-and-Diffusion-Model-through-Inverse-Reinforcement-Learning"><a href="#Generalized-Contrastive-Divergence-Joint-Training-of-Energy-Based-Model-and-Diffusion-Model-through-Inverse-Reinforcement-Learning" class="headerlink" title="Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning"></a>Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03397">http://arxiv.org/abs/2312.03397</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sangwoong Yoon, Dohyun Kwon, Himchan Hwang, Yung-Kyun Noh, Frank C. Park</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„å¯¹è±¡å‡½æ•°ï¼Œç”¨äºåŒæ—¶è®­ç»ƒèƒ½é‡åŸºæ¨¡å‹ï¼ˆEBMï¼‰å’ŒæŠ½å–æ¨¡å‹ï¼ˆ diffusion modelï¼‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬å¯¹EBMå’ŒæŠ½å–æ¨¡å‹è¿›è¡ŒåŒæ—¶è®­ç»ƒï¼Œå¹¶å°†å…¶Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²ä¸ºä¸€ä¸ªæœ€å°åŒ–é—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡åŒæ—¶è®­ç»ƒEBMå’ŒæŠ½å–æ¨¡å‹ï¼Œå¯ä»¥æé«˜æ ·æœ¬è´¨é‡å¹¶å‡å°‘MCMCçš„ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œjoint trainingè¿˜èƒ½å¤Ÿæ”¹å–„EBMçš„è®­ç»ƒæ•ˆæœã€‚<details>
<summary>Abstract</summary>
We present Generalized Contrastive Divergence (GCD), a novel objective function for training an energy-based model (EBM) and a sampler simultaneously. GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution with a trainable sampler, such as a diffusion model. In GCD, the joint training of EBM and a diffusion model is formulated as a minimax problem, which reaches an equilibrium when both models converge to the data distribution. The minimax learning with GCD bears interesting equivalence to inverse reinforcement learning, where the energy corresponds to a negative reward, the diffusion model is a policy, and the real data is expert demonstrations. We present preliminary yet promising results showing that joint training is beneficial for both EBM and a diffusion model. GCD enables EBM training without MCMC while improving the sample quality of a diffusion model.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿç°åœ¨ä»‹ç»ä¸€ç§æ–°çš„ç›®æ ‡å‡½æ•°ï¼Œå³æ³›åŒ–å¯¹ç…§åˆ†æ•£ï¼ˆGCDï¼‰ï¼Œç”¨äºåŒæ—¶è®­ç»ƒèƒ½é‡åŸºå‹æ¨¡å‹ï¼ˆEBMï¼‰å’Œæ‰©æ•£æ¨¡å‹ã€‚GCDæ‰©å±•äº†2002å¹´å¸ŒĞ½é¡¿æå‡ºçš„å¯¹ç…§åˆ†æ•£ç®—æ³•ï¼ˆHintonï¼‰ï¼Œå°†é©¬å°”å¯å¤«é“¾ Monte Carloï¼ˆMCMCï¼‰åˆ†å¸ƒæ›¿æ¢ä¸ºå¯å­¦ä¹ çš„æ‰©æ•£æ¨¡å‹ã€‚åœ¨GCDä¸­ï¼ŒEBMå’Œæ‰©æ•£æ¨¡å‹çš„å…±åŒè®­ç»ƒè¢«Ñ„Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°ä¸ºä¸€ä¸ªæœ€å°æœ€å¤§é—®é¢˜ï¼Œå½“ä¸¤ä¸ªæ¨¡å‹éƒ½ convergesåˆ°æ•°æ®åˆ†å¸ƒæ—¶ï¼Œå®ƒä»¬è¾¾åˆ°äº†å¹³è¡¡ã€‚è¿™ç§æœ€å°æœ€å¤§å­¦ä¹ ä¸GCDå…·æœ‰æƒŠäººçš„ç­‰ä»·æ€§ï¼Œä¸åå¥–å­¦ä¹ ç›¸å½“ï¼Œå…¶ä¸­èƒ½é‡å¯¹åº”äºè´Ÿåå¥–ï¼Œæ‰©æ•£æ¨¡å‹å¯¹åº”äºç­–ç•¥ï¼Œè€Œå®é™…æ•°æ®åˆ™æ˜¯ä¸“å®¶ç¤ºèŒƒã€‚æˆ‘ä»¬å±•ç¤ºäº†åˆæ­¥å´æœ‰æŠŠæ¡çš„ç»“æœï¼Œè¡¨æ˜åŒæ—¶è®­ç»ƒEBMå’Œæ‰©æ•£æ¨¡å‹æœ‰åˆ©äºä¸¤è€…ã€‚GCDå…è®¸EBMæ— éœ€MCMCè®­ç»ƒï¼Œå¹¶æé«˜æ‰©æ•£æ¨¡å‹çš„æ ·æœ¬è´¨é‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Diffused-Task-Agnostic-Milestone-Planner"><a href="#Diffused-Task-Agnostic-Milestone-Planner" class="headerlink" title="Diffused Task-Agnostic Milestone Planner"></a>Diffused Task-Agnostic Milestone Planner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03395">http://arxiv.org/abs/2312.03395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mineui Hong, Minjae Kang, Songhwai Oh</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºåºåˆ—é¢„æµ‹çš„æ–¹æ³•ï¼Œç”¨äºè§£å†³å¯¹å†³ç­–é—®é¢˜çš„é•¿æœŸè§„åˆ’ã€è§†è§‰æ§åˆ¶å’Œå¤šä»»é—®é¢˜çš„åº”ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ä½¿ç”¨æ•£åº¦åŸºæœ¬ç”Ÿæˆåºåˆ—æ¨¡å‹æ¥è§„åˆ’ä¸€ç³»åˆ—çš„é‡Œç¨‹ç¢‘ï¼Œå¹¶è®©Agentéµå¾ªè¿™äº›é‡Œç¨‹ç¢‘æ¥å®Œæˆä¸€ä¸ªä»»åŠ¡ã€‚æå‡ºçš„æ–¹æ³•å¯ä»¥å­¦ä¹ æ§åˆ¶ç›¸å…³çš„ã€ä½ç»´åº¦çš„latentè¡¨ç¤ºï¼Œä»è€Œå®ç°é•¿æœŸè§„åˆ’å’Œè§†è§‰æ§åˆ¶çš„æ•ˆç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åˆ©ç”¨æ•£åº¦æ¨¡å‹çš„ç”Ÿæˆçµæ´»æ€§ï¼Œå®ç°å¤šä»»é—®é¢˜çš„è§„åˆ’ã€‚</li>
<li>results: æœ¬ç ”ç©¶åœ¨å¤šä¸ªofflineå¾ªç¯å­¦ä¹ ï¼ˆRLï¼‰benchmarkå’Œä¸€ä¸ªè§†è§‰æ§åˆ¶ç¯å¢ƒä¸­è¿›è¡Œè¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¶…è¶Šoffline RLæ–¹æ³•åœ¨è§£å†³é•¿æœŸã€ç½•è§å¥–åŠ±ä»»åŠ¡å’Œå¤šä»»é—®é¢˜ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„è§†è§‰æ§åˆ¶benchmarkä¸Š achievement state-of-the-artè¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Addressing decision-making problems using sequence modeling to predict future trajectories shows promising results in recent years. In this paper, we take a step further to leverage the sequence predictive method in wider areas such as long-term planning, vision-based control, and multi-task decision-making. To this end, we propose a method to utilize a diffusion-based generative sequence model to plan a series of milestones in a latent space and to have an agent to follow the milestones to accomplish a given task. The proposed method can learn control-relevant, low-dimensional latent representations of milestones, which makes it possible to efficiently perform long-term planning and vision-based control. Furthermore, our approach exploits generation flexibility of the diffusion model, which makes it possible to plan diverse trajectories for multi-task decision-making. We demonstrate the proposed method across offline reinforcement learning (RL) benchmarks and an visual manipulation environment. The results show that our approach outperforms offline RL methods in solving long-horizon, sparse-reward tasks and multi-task problems, while also achieving the state-of-the-art performance on the most challenging vision-based manipulation benchmark.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Lite-Mind-Towards-Efficient-and-Versatile-Brain-Representation-Network"><a href="#Lite-Mind-Towards-Efficient-and-Versatile-Brain-Representation-Network" class="headerlink" title="Lite-Mind: Towards Efficient and Versatile Brain Representation Network"></a>Lite-Mind: Towards Efficient and Versatile Brain Representation Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03781">http://arxiv.org/abs/2312.03781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu<br>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜éä¾µå…¥å¼fMRIçš„ä¿¡æ¯è§£ç æ€§èƒ½ã€‚methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ·±åº¦å¤šå±‚perceptronï¼ˆMLPï¼‰å’ŒCLIPçš„è§†è§‰å˜æ¢å™¨æ¥å¯¹fMRIåµŒå…¥è¿›è¡Œ alignã€‚results: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§ã€é«˜æ•ˆã€å¤šç”¨é€”çš„å¤§è„‘è¡¨ç¤ºç½‘ç»œï¼ˆLite-Mindï¼‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°å°†fMRIç£åŒ–åµŒå…¥ä¸CLIPçš„ç»†è…»ä¿¡æ¯è¿›è¡Œå¯¹åº”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒLite-Mindåœ¨NSDæ•°æ®é›†ä¸Šå–å¾—äº†94.3%çš„fMRI-to-imageæ£€ç´¢ç²¾åº¦ï¼Œä¸MindEyeç›¸æ¯”å‡å°‘äº†98.7%çš„å‚æ•°æ•°é‡ã€‚<details>
<summary>Abstract</summary>
Research in decoding visual information from the brain, particularly through the non-invasive fMRI method, is rapidly progressing. The challenge arises from the limited data availability and the low signal-to-noise ratio of fMRI signals, leading to a low-precision task of fMRI-to-image retrieval. State-of-the-art MindEye remarkably improves fMRI-to-image retrieval performance by leveraging a deep MLP with a high parameter count orders of magnitude, i.e., a 996M MLP Backbone per subject, to align fMRI embeddings to the final hidden layer of CLIP's vision transformer. However, significant individual variations exist among subjects, even within identical experimental setups, mandating the training of subject-specific models. The substantial parameters pose significant challenges in deploying fMRI decoding on practical devices, especially with the necessitating of specific models for each subject. To this end, we propose Lite-Mind, a lightweight, efficient, and versatile brain representation network based on discrete Fourier transform, that efficiently aligns fMRI voxels to fine-grained information of CLIP. Our experiments demonstrate that Lite-Mind achieves an impressive 94.3% fMRI-to-image retrieval accuracy on the NSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind is also proven to be able to be migrated to smaller brain datasets and establishes a new state-of-the-art for zero-shot classification on the GOD dataset. The code is available at https://github.com/gongzix/Lite-Mind.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶åœ¨è§£ç å¤§è„‘ä¿¡æ¯ä¸­è¿›å±• rapilyï¼Œç‰¹åˆ«æ˜¯é€šè¿‡éä¾µå…¥å¼fMRIæ–¹æ³•ã€‚æŒ‘æˆ˜æ¥è‡ªæœ‰é™çš„æ•°æ®å¯ç”¨æ€§å’ŒfMRIä¿¡å·å™ªå£°æ¯”ï¼Œå¯¼è‡´fMRI-to-image Retrieval æ˜¯ä¸€ä¸ªä½ç²¾åº¦ä»»åŠ¡ã€‚ç°æœ‰çš„ MindEye æŠ€æœ¯å¤‡å—æ”¹è¿› fMRI-to-image Retrieval æ€§èƒ½ï¼Œé€šè¿‡ä½¿ç”¨æ·±åº¦ MLP å’Œé«˜å‚æ•°è®¡æ•°ï¼Œä¾‹å¦‚æ¯ä¸ªä¸»ä½“996M MLP Backboneï¼Œå°† fMRI åµŒå…¥çº¿æ€§å¯¹ CLIP è§†transformer çš„æœ€ç»ˆéšè—å±‚è¿›è¡Œå¯¹é½ã€‚ç„¶è€Œï¼Œæ¯ä¸ªä¸»ä½“éƒ½å­˜åœ¨å·®å¼‚ï¼Œå³ä½¿åœ¨åŒä¸€ä¸ªå®éªŒè®¾ç½®ä¸‹ï¼Œéœ€è¦è®­ç»ƒç‰¹å®šä¸»ä½“çš„æ¨¡å‹ã€‚é«˜å‚æ•°æ•°é‡å¯¹å®é™…è®¾å¤‡éƒ¨ç½²é€ æˆäº† significiant æŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº† Lite-Mindï¼Œä¸€ç§è½»é‡çº§ã€é«˜æ•ˆã€å¤šåŠŸèƒ½å¤§è„‘è¡¨ç¤ºç½‘ç»œï¼ŒåŸºäºç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°† fMRI  voxel å¯¹ CLIP çš„ç»†è…»ä¿¡æ¯è¿›è¡Œå¯¹é½ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒLite-Mind å¯ä»¥åœ¨ NSD æ•°æ®é›†ä¸Šè¾¾åˆ°94.3%çš„ fMRI-to-image Retrieval ç²¾åº¦ï¼Œæ¯” MindEye ä½98.7% çš„å‚æ•°æ•°é‡ã€‚æ­¤å¤–ï¼ŒLite-Mind è¿˜å¯ä»¥è½»æ¾è¿ç§»åˆ° smaller brain æ•°æ®é›†ï¼Œå¹¶åœ¨ GOD æ•°æ®é›†ä¸Šå»ºç«‹äº†æ–°çš„çŠ¶æ€æ€-of-the-art  Ğ´Ğ»Ñé›¶å®¹é‡åˆ†ç±»ã€‚ä»£ç å¯ä»¥åœ¨ https://github.com/gongzix/Lite-Mind ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Demand-response-for-residential-building-heating-Effective-Monte-Carlo-Tree-Search-control-based-on-physics-informed-neural-networks"><a href="#Demand-response-for-residential-building-heating-Effective-Monte-Carlo-Tree-Search-control-based-on-physics-informed-neural-networks" class="headerlink" title="Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks"></a>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03365">http://arxiv.org/abs/2312.03365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fabio Pavirani, Gargya Gokhale, Bert Claessens, Chris Develder</li>
<li>for: æ§åˆ¶å»ºç­‘ç‰©çš„èƒ½æºæ¶ˆè€—ä»¥æé«˜global carbon emissionså’Œé™åˆ¶æ°”å€™å˜åŒ–çš„æ§åˆ¶ã€‚</li>
<li>methods: ä½¿ç”¨Monte Carlo Tree Searchï¼ˆMCTSï¼‰å’ŒPhysics-informed Neural Networkï¼ˆPiNNï¼‰æ¨¡å‹æ¥ä¼˜åŒ–å»ºç­‘ç‰©çš„å†·æš–ç³»ç»Ÿï¼Œä»¥æé«˜DRæ§åˆ¶æ€§èƒ½ã€‚</li>
<li>results: MCTSå’ŒPiNNæ¨¡å‹çš„å®ç°èƒ½å¤Ÿæé«˜DRæ§åˆ¶æ€§èƒ½ï¼Œç›¸æ¯”ä¹‹ä¸‹rule-basedæ§åˆ¶å™¨å¯ä»¥æé«˜10%çš„æˆæœ¬å’Œ35%çš„æ¸©åº¦å·®ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ å±‚çš„æ·»åŠ å¯ä»¥æé«˜è®¡ç®—æˆæœ¬æ•ˆç›Šã€‚<details>
<summary>Abstract</summary>
Controlling energy consumption in buildings through demand response (DR) has become increasingly important to reduce global carbon emissions and limit climate change. In this paper, we specifically focus on controlling the heating system of a residential building to optimize its energy consumption while respecting user's thermal comfort. Recent works in this area have mainly focused on either model-based control, e.g., model predictive control (MPC), or model-free reinforcement learning (RL) to implement practical DR algorithms. A specific RL method that recently has achieved impressive success in domains such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for building control it has remained largely unexplored. Thus, we study MCTS specifically for building demand response. Its natural structure allows a flexible optimization that implicitly integrate exogenous constraints (as opposed, for example, to conventional RL solutions), making MCTS a promising candidate for DR control problems. We demonstrate how to improve MCTS control performance by incorporating a Physics-informed Neural Network (PiNN) model for its underlying thermal state prediction, as opposed to traditional purely data-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN model is able to obtain a 3% increment of the obtained reward compared to a rule-based controller; leading to a 10% cost reduction and 35% reduction on temperature difference with the desired one when applied to an artificial price profile. We further implemented a Deep Learning layer into the Monte Carlo Tree Search technique using a neural network that leads the tree search through more optimal nodes. We then compared this addition with its Vanilla version, showing the improvement in computational cost required.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ§åˆ¶å»ºç­‘ç‰©çš„èƒ½æºæ¶ˆè€—å·²æˆä¸ºé™ä½å…¨çƒç¢³æ’æ”¾å’Œæ§åˆ¶æ°”å€™å˜åŒ–çš„é‡è¦æ–¹æ³•ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ§åˆ¶å…¬å¯“å»ºç­‘ç‰©çš„å†·å´ç³»ç»Ÿï¼Œä»¥ä¼˜åŒ–å…¶èƒ½æºæ¶ˆè€—ï¼ŒåŒæ—¶ä¿è¯ç”¨æˆ·çš„å®¤å†…æ¸©åº¦èˆ’é€‚æ€§ã€‚ç°æœ‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ§åˆ¶ï¼ˆMPCï¼‰æˆ–æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å®ç°å®ç”¨çš„DRç®—æ³•ã€‚ç‰¹åˆ«æ˜¯ï¼Œ Monte Carlo Tree Searchï¼ˆMCTSï¼‰åœ¨æ£‹ç›˜æ¸¸æˆï¼ˆå¦‚å›´æ£‹ã€å›½é™…è±¡æ£‹ï¼‰ä¸­æœ€è¿‘å‡ å¹´è¡¨ç°å‡ºäº†éå¸¸å‡ºè‰²çš„æˆç»©ã€‚ç„¶è€Œï¼Œåœ¨å»ºç­‘ç‰©æ§åˆ¶é¢†åŸŸï¼ŒMCTSçš„åº”ç”¨ä»ç„¶å¾ˆå°‘ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ç ”ç©¶MCTSï¼Œå¹¶è¯æ˜å…¶åœ¨å»ºç­‘ç‰©DRæ§åˆ¶é—®é¢˜ä¸­çš„æ½œåœ¨ä¼˜åŠ¿ã€‚MCTSçš„è‡ªç„¶ç»“æ„ä½¿å¾—å¯ä»¥flexiblyè¿›è¡Œä¼˜åŒ–ï¼ŒåŒæ—¶è‡ªåŠ¨æ‰¿è½½å¤–éƒ¨çº¦æŸï¼ˆä¸ä¼ ç»ŸRLæ–¹æ³•ä¸åŒï¼‰ï¼Œè¿™ä½¿MCTSåœ¨DRæ§åˆ¶é—®é¢˜ä¸­æˆä¸ºä¸€ä¸ªéå¸¸æœ‰å‰é€”çš„å€™é€‰è€…ã€‚æˆ‘ä»¬é€šè¿‡å°†PiNNæ¨¡å‹ï¼ˆPhysics-informed Neural Networkï¼‰ä¸MCTSç»“åˆä½¿ç”¨ï¼Œæé«˜äº†æ§åˆ¶æ€§èƒ½ã€‚æˆ‘ä»¬çš„MCTSå®ç°ä¸PiNNæ¨¡å‹ç›¸æ¯”ï¼Œä¸è§„åˆ™æ§åˆ¶å™¨ç›¸æ¯”ï¼Œå¯ä»¥è·å¾—3%çš„å¢é‡å¥–åŠ±ï¼Œå¯¼è‡´10%çš„æˆæœ¬å‡å°‘å’Œ35%çš„æ¸©åº¦å·®å¼‚å‡å°‘ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ·»åŠ äº†ä¸€ä¸ªæ·±åº¦å­¦ä¹ å±‚åˆ°MCTSæŠ€æœ¯ä¸­ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå¯¼å¼•æœç´¢æ›´ä¼˜åŒ–çš„æ ‘ã€‚ä¸æ™®é€šç‰ˆæœ¬ç›¸æ¯”ï¼Œè¿™ç§æ·»åŠ å‡å°‘äº†è®¡ç®—æˆæœ¬çš„éœ€æ±‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Teaching-Specific-Scientific-Knowledge-into-Large-Language-Models-through-Additional-Training"><a href="#Teaching-Specific-Scientific-Knowledge-into-Large-Language-Models-through-Additional-Training" class="headerlink" title="Teaching Specific Scientific Knowledge into Large Language Models through Additional Training"></a>Teaching Specific Scientific Knowledge into Large Language Models through Additional Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03360">http://arxiv.org/abs/2312.03360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kan Hatakeyama-Sato, Yasuhiko Igarashi, Shun Katakami, Yuta Nabae, Teruaki Hayakawa</li>
<li>for: é€šè¿‡é¢å¤–è®­ç»ƒï¼Œæ¢ç´¢å°†ä¸“ä¸šç§‘å­¦çŸ¥è¯†åµŒå…¥LLMå¤§è¯­è¨€æ¨¡å‹ä¸­ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æ‰©å……æ¥è§£å†³ä¸“ä¸šæ–‡çŒ®ç¼ºä¹é—®é¢˜ï¼ŒåŒ…æ‹¬æ ·å¼è½¬æ¢å’Œç¿»è¯‘ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å‚æ•°ä¼˜åŒ–ã€‚</li>
<li>results: æˆ‘ä»¬æˆåŠŸåœ°åœ¨ä¸€å®šç¨‹åº¦ä¸ŠåµŒå…¥äº†çŸ¥è¯†ï¼Œä½†ç ”ç©¶æ˜¾ç¤ºåµŒå…¥ä¸“ä¸šä¿¡æ¯åˆ°LLMä¸­å­˜åœ¨å¤æ‚æ€§å’Œé™åˆ¶ï¼Œæå‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›çš„æ–¹å‘ã€‚<details>
<summary>Abstract</summary>
Through additional training, we explore embedding specialized scientific knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that effective knowledge integration requires reading texts from multiple perspectives, especially in instructional formats. We utilize text augmentation to tackle the scarcity of specialized texts, including style conversions and translations. Hyperparameter optimization proves crucial, with different size models (7b, 13b, and 70b) reasonably undergoing additional training. Validating our methods, we construct a dataset of 65,000 scientific papers. Although we have succeeded in partially embedding knowledge, the study highlights the complexities and limitations of incorporating specialized information into LLMs, suggesting areas for further improvement.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡è¿›ä¸€æ­¥çš„è®­ç»ƒï¼Œæˆ‘ä»¬æ¢ç´¢å°†ä¸“ä¸šç§‘å­¦çŸ¥è¯† embeddingåˆ°å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ã€‚å…³é”®å‘ç°æ˜¾ç¤ºï¼Œæœ‰æ•ˆåœ° integrate çŸ¥è¯†éœ€è¦ä»å¤šä¸ªè§’åº¦é˜…è¯»æ–‡æœ¬ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•™å­¦æ ¼å¼ä¸‹ã€‚æˆ‘ä»¬ä½¿ç”¨æ–‡æœ¬æ‰©å±•æ¥è§£å†³ä¸“ä¸šæ–‡æœ¬ç¨€ç¼ºé—®é¢˜ï¼ŒåŒ…æ‹¬æ ·å¼è½¬æ¢å’Œç¿»è¯‘ã€‚æ¨¡å‹çš„è¶…å‚æ•°ä¼˜åŒ–è¯æ˜æ˜¯å…³é”®çš„ï¼Œä¸åŒå¤§å°çš„æ¨¡å‹ï¼ˆ7bã€13bå’Œ70bï¼‰éƒ½èƒ½å¤Ÿè¿›è¡Œè¿›ä¸€æ­¥çš„è®­ç»ƒã€‚ä¸ºéªŒè¯æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬æ„å»ºäº†65,000ç¯‡ç§‘å­¦è®ºæ–‡çš„æ•°æ®é›†ã€‚è™½ç„¶æˆ‘ä»¬åœ¨éƒ¨åˆ† embedding çŸ¥è¯†ä¸ŠæˆåŠŸï¼Œä½†ç ”ç©¶è¡¨æ˜å°†ä¸“ä¸šä¿¡æ¯ embedding åˆ° LLM ä¸­å­˜åœ¨å¤æ‚æ€§å’Œé™åˆ¶ï¼Œæå‡ºäº†è¿›ä¸€æ­¥æ”¹è¿›çš„æ–¹å‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Online-Vectorized-HD-Map-Construction-using-Geometry"><a href="#Online-Vectorized-HD-Map-Construction-using-Geometry" class="headerlink" title="Online Vectorized HD Map Construction using Geometry"></a>Online Vectorized HD Map Construction using Geometry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03341">http://arxiv.org/abs/2312.03341</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cnzzx/gemap">https://github.com/cnzzx/gemap</a></li>
<li>paper_authors: Zhixin Zhang, Yiyuan Zhang, Xiaohan Ding, Fusheng Jin, Xiangyu Yue</li>
<li>for: æå‡ºäº†ä¸€ç§åŸºäºEuclideanå‡ ä½•å­¦çš„æ˜ å°„å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜åœ¨åŸå¸‚é“è·¯ç³»ç»Ÿä¸­çš„é¢„æµ‹å’Œè§„åˆ’ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§å«åšGeMapçš„æ–¹æ³•ï¼Œå®ƒå¯ä»¥æ•æ‰åˆ°åŸå¸‚é“è·¯ç³»ç»Ÿä¸­çš„å‡ ä½•å½¢æ€å’Œå…³ç³»ï¼Œå¹¶ä¸”å¯ä»¥ç‹¬ç«‹å¤„ç†å‡ ä½•å½¢æ€å’Œå…³ç³»ã€‚</li>
<li>results: åœ¨NuSceneså’ŒArgoverse 2 datasetsä¸Šå®ç°äº†æ–°çš„æœ€ä½³æ€§èƒ½ï¼Œå…¶ä¸­åœ¨Argoverse 2 datasetä¸Šè¾¾åˆ°äº†71.8%çš„mAPï¼Œæ¯”MapTR V2é«˜4.4%ï¼Œå¹¶é¦–æ¬¡çªç ´äº†70%çš„mAPé˜ˆå€¼ã€‚<details>
<summary>Abstract</summary>
The construction of online vectorized High-Definition (HD) maps is critical for downstream prediction and planning. Recent efforts have built strong baselines for this task, however, shapes and relations of instances in urban road systems are still under-explored, such as parallelism, perpendicular, or rectangle-shape. In our work, we propose GeMap ($\textbf{Ge}$ometry $\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception. Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations. We also decouple self-attention to independently handle Euclidean shapes and relations. Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time. Code is available at https://github.com/cnzzx/GeMap
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾çš„æ„å»ºæ˜¯ä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’çš„å…³é”®ã€‚ recent efforts have built strong baselines for this task, but shapes and relations of instances in urban road systems are still under-explored, such as parallelism, perpendicular, or rectangle-shape. In our work, we propose GeMapï¼ˆåœ°å›¾å‡ ä½•å¯¹æ˜ ï¼‰, which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception. Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations. We also decouple self-attention to independently handle Euclidean shapes and relations. Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time. â€Here's the breakdown of the translation:* â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾â€(online vectorized high-definition maps) is translated as â€œåœ¨çº¿vectoråŒ–é«˜æ¸…åœ°å›¾â€(åœ¨çº¿vectorizedé«˜æ¸…åœ°å›¾)* â€œæ„å»ºâ€(construction) is translated as â€œæ„å»ºâ€(æ„å»º)* â€œdownstream prediction and planningâ€(downstream prediction and planning) is translated as â€œä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’â€(ä¸‹æ¸¸é¢„æµ‹å’Œè§„åˆ’)* â€œRecent efforts have built strong baselines for this taskâ€(recent efforts have built strong baselines for this task) is translated as â€œrecent efforts have built strong baselines for this taskâ€(recent efforts have built strong baselines for this task)* â€œbut shapes and relations of instances in urban road systems are still under-exploredâ€(but shapes and relations of instances in urban road systems are still under-explored) is translated as â€œbut shapes and relations of instances in urban road systems are still under-exploredâ€(but shapes and relations of instances in urban road systems are still under-explored)* â€œsuch as parallelism, perpendicular, or rectangle-shapeâ€(such as parallelism, perpendicular, or rectangle-shape) is translated as â€œsuch as parallelism, perpendicular, or rectangle-shapeâ€(such as parallelism, perpendicular, or rectangle-shape)* â€œIn our work, we propose GeMapâ€(In our work, we propose GeMap) is translated as â€œIn our work, we propose GeMapâ€(åœ¨æˆ‘ä»¬çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†GeMap)* â€œwhich end-to-end learns Euclidean shapes and relations of map instances beyond basic perceptionâ€(which end-to-end learns Euclidean shapes and relations of map instances beyond basic perception) is translated as â€œwhich end-to-end learns Euclidean shapes and relations of map instances beyond basic perceptionâ€(which end-to-end learnsEuclidean shapes and relations of map instances beyond basic perception)* â€œSpecifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformationsâ€(Specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations) is translated as â€œSpecifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformationsâ€(specifically, we design a geometric loss based on angle and distance clues, which is robust to rigid transformations)* â€œWe also decouple self-attention to independently handle Euclidean shapes and relationsâ€(We also decouple self-attention to independently handle Euclidean shapes and relations) is translated as â€œWe also decouple self-attention to independently handle Euclidean shapes and relationsâ€(we also decouple self-attention to independently handle Euclidean shapes and relations)* â€œOur method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasetsâ€(Our method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasets) is translated as â€œour method achieves new state-of-the-art performance on the NuScenes and Argoverse 2 datasetsâ€(æˆ‘ä»¬çš„æ–¹æ³•åœ¨NuSceneså’ŒArgoverse 2 datasetä¸Šè¾¾åˆ°äº†æ–°çš„state-of-the-artæ€§èƒ½)* â€œRemarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first timeâ€(Remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time) is translated as â€œremarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first timeâ€(remarkably, it reaches a 71.8% mAP on the large-scale Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP threshold for the first time)Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Continual-Learning-from-Cognitive-Perspectives"><a href="#Benchmarking-Continual-Learning-from-Cognitive-Perspectives" class="headerlink" title="Benchmarking Continual Learning from Cognitive Perspectives"></a>Benchmarking Continual Learning from Cognitive Perspectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03309">http://arxiv.org/abs/2312.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqian Liu, Junge Zhang, Mingyi Zhang, Peipei Yang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³ continual learning é—®é¢˜ï¼Œå³ä¸æ–­å­¦ä¹ å’Œè½¬ç§»çŸ¥è¯†è€Œä¸å¯¼è‡´è€çŸ¥è¯†å¿˜è®°ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šç§æ–¹æ³•æ¥è¯„ä¼° continual learning æ¨¡å‹ï¼ŒåŒ…æ‹¬åŸºäº cognitive properties çš„ desideratum å’Œå¤šç§è¯„ä»·æŒ‡æ ‡ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œç°æœ‰çš„ continual learning æ¨¡å‹å°šæœªæ»¡è¶³æ‰€æœ‰ desideratumï¼Œå¹¶ä¸”å°šæœªå®ç°çœŸæ­£çš„ continual learningã€‚ although some methods å…·æœ‰ä¸€å®šçš„é€‚åº”æ€§å’Œæ•ˆç‡ï¼Œä½†æ˜¯æ— æ³•è¯†åˆ«ä»»åŠ¡å˜åŒ–æ—¶çš„ä»»åŠ¡å…³ç³»ï¼Œæˆ–è€…å¯»æ±‚ä»»åŠ¡ä¹‹é—´çš„ç›¸ä¼¼æ€§å’Œä¸åŒæ€§ã€‚<details>
<summary>Abstract</summary>
Continual learning addresses the problem of continuously acquiring and transferring knowledge without catastrophic forgetting of old concepts. While humans achieve continual learning via diverse neurocognitive mechanisms, there is a mismatch between cognitive properties and evaluation methods of continual learning models. First, the measurement of continual learning models mostly relies on evaluation metrics at a micro-level, which cannot characterize cognitive capacities of the model. Second, the measurement is method-specific, emphasizing model strengths in one aspect while obscuring potential weaknesses in other respects. To address these issues, we propose to integrate model cognitive capacities and evaluation metrics into a unified evaluation paradigm. We first characterize model capacities via desiderata derived from cognitive properties supporting human continual learning. The desiderata concern (1) adaptability in varying lengths of task sequence; (2) sensitivity to dynamic task variations; and (3) efficiency in memory usage and training time consumption. Then we design evaluation protocols for each desideratum to assess cognitive capacities of recent continual learning models. Experimental results show that no method we consider has satisfied all the desiderata and is still far away from realizing truly continual learning. Although some methods exhibit some degree of adaptability and efficiency, no method is able to identify task relationships when encountering dynamic task variations, or achieve a trade-off in learning similarities and differences between tasks. Inspired by these results, we discuss possible factors that influence model performance in these desiderata and provide guidance for the improvement of continual learning models.
</details>
<details>
<summary>æ‘˜è¦</summary>
First, the evaluation metrics used are mainly micro-level, which cannot fully capture the cognitive abilities of the model. Second, the evaluation is method-specific, highlighting the strengths of the model in one aspect while hiding its potential weaknesses in other areas. To address these issues, we propose integrating model cognitive abilities and evaluation metrics into a unified evaluation paradigm.We first define the cognitive capabilities of the model based on the cognitive properties that support human continual learning, including the ability to adapt to varying task sequences, sensitivity to dynamic task variations, and efficient use of memory and training time. Then, we design evaluation protocols for each of these desiderata to assess the cognitive abilities of recent continual learning models.The experimental results show that none of the methods we considered have fully met all of the desiderata and are still far from achieving true continual learning. While some methods have shown some degree of adaptability and efficiency, they have failed to identify task relationships when facing dynamic task variations or balance learning similarities and differences between tasks.Inspired by these results, we discuss potential factors that may influence model performance in these desiderata and provide guidance for improving continual learning models.
</details></li>
</ul>
<hr>
<h2 id="Dyport-Dynamic-Importance-based-Hypothesis-Generation-Benchmarking-Technique"><a href="#Dyport-Dynamic-Importance-based-Hypothesis-Generation-Benchmarking-Technique" class="headerlink" title="Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique"></a>Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03303">http://arxiv.org/abs/2312.03303</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ilyatyagin/dyport">https://github.com/ilyatyagin/dyport</a></li>
<li>paper_authors: Ilya Tyagin, Ilya Safro</li>
<li>for: è¿™ paper æ˜¯ä¸€ä¸ªæ–°çš„ç”Ÿç‰©åŒ»å­¦å‡è®¾ç”Ÿæˆç³»ç»Ÿè¯„ä¼°æ¡†æ¶ Dyportã€‚</li>
<li>methods: è¯¥approach ä½¿ç”¨äº†å·²ç»ç²¾å¿ƒç¼–è¾‘çš„æ•°æ®é›†ï¼Œä½¿å¾—æˆ‘ä»¬çš„è¯„ä¼°æ›´åŠ çœŸå®ã€‚å®ƒ integrates çŸ¥è¯†åˆ°curated databases ä¸­çš„åŠ¨æ€å›¾è¡¨ï¼Œå¹¶æä¾›äº†ä¸€ç§é‡åŒ–å‘ç°é‡è¦æ€§çš„æ–¹æ³•ï¼Œä¸ä»…è¯„ä¼°å‡è®¾çš„å‡†ç¡®æ€§ï¼Œè¿˜è¯„ä¼°å…¶åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„å¯èƒ½çš„å½±å“ï¼Œè¿™å¤§å¤§è¶…è¶Šäº†ä¼ ç»Ÿçš„é“¾æ¥é¢„æµ‹benchmarkã€‚</li>
<li>results: æˆ‘ä»¬åœ¨åº”ç”¨äº†several link prediction systems åœ¨ç”Ÿç‰©åŒ»å­¦semantic knowledge graphs ä¸Šçš„å®éªŒä¸­ï¼Œdemonstrated äº†æˆ‘ä»¬çš„è¯„ä¼°ç³»ç»Ÿçš„å¯è¡Œæ€§å’Œçµæ´»æ€§ã€‚<details>
<summary>Abstract</summary>
This paper presents a novel benchmarking framework Dyport for evaluating biomedical hypothesis generation systems. Utilizing curated datasets, our approach tests these systems under realistic conditions, enhancing the relevance of our evaluations. We integrate knowledge from the curated databases into a dynamic graph, accompanied by a method to quantify discovery importance. This not only assesses hypothesis accuracy but also their potential impact in biomedical research which significantly extends traditional link prediction benchmarks. Applicability of our benchmarking process is demonstrated on several link prediction systems applied on biomedical semantic knowledge graphs. Being flexible, our benchmarking system is designed for broad application in hypothesis generation quality verification, aiming to expand the scope of scientific discovery within the biomedical research community. Availability and implementation: Dyport framework is fully open-source. All code and datasets are available at: https://github.com/IlyaTyagin/Dyport
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ç”Ÿç‰©åŒ»å­¦å‡è®¾ç”Ÿæˆç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼Œå³Dyportã€‚è¯¥æ¡†æ¶åˆ©ç”¨äº†ä»”ç»†ç¼–è¾‘çš„æ•°æ®é›†ï¼Œä½¿æˆ‘ä»¬çš„è¯„ä¼°æ›´åŠ çœŸå®ã€‚æˆ‘ä»¬å°†çŸ¥è¯†ä» curaated æ•°æ®åº“ integrate åˆ°åŠ¨æ€å›¾ä¸­ï¼Œå¹¶æä¾›ä¸€ç§é‡åŒ–å‘ç°é‡è¦æ€§çš„æ–¹æ³•ã€‚è¿™ä¸ä»…è¯„ä¼°å‡è®¾å‡†ç¡®æ€§ï¼Œè¿˜è¯„ä¼°å…¶åœ¨ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ä¸­çš„å¯èƒ½çš„å½±å“ï¼Œè¿™åœ¨ä¼ ç»Ÿçš„é“¾æ¥é¢„æµ‹æµ‹è¯•ä¸­è¿›è¡Œäº†æ˜¾è‘—æ‰©å±•ã€‚æˆ‘ä»¬çš„è¯„ä¼°è¿‡ç¨‹çš„å¯åº”ç”¨æ€§åœ¨å¤šä¸ªé“¾æ¥é¢„æµ‹ç³»ç»Ÿä¸Šè¿›è¡Œäº†åº”ç”¨ã€‚æˆ‘ä»¬çš„è¯„ä¼°ç³»ç»Ÿæ˜¯ flexible çš„ï¼Œå¯ä»¥å¹¿æ³›åº”ç”¨äºå‡è®¾ç”Ÿæˆè´¨é‡éªŒè¯ä¸­ï¼Œä»¥æ‰©å±•ç”Ÿç‰©åŒ»å­¦ç ”ç©¶ç¤¾åŒºçš„ç§‘å­¦å‘ç°èŒƒå›´ã€‚å¯ç”¨æ€§å’Œå®ç°ï¼šDyport æ¡†æ¶æ˜¯å®Œå…¨å¼€æºçš„ã€‚æ‰€æœ‰ä»£ç å’Œæ•°æ®é›†å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼šhttps://github.com/IlyaTyagin/Dyportã€‚
</details></li>
</ul>
<hr>
<h2 id="SoftMAC-Differentiable-Soft-Body-Simulation-with-Forecast-based-Contact-Model-and-Two-way-Coupling-with-Articulated-Rigid-Bodies-and-Clothes"><a href="#SoftMAC-Differentiable-Soft-Body-Simulation-with-Forecast-based-Contact-Model-and-Two-way-Coupling-with-Articulated-Rigid-Bodies-and-Clothes" class="headerlink" title="SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes"></a>SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03297">http://arxiv.org/abs/2312.03297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damianliumin/SoftMAC">https://github.com/damianliumin/SoftMAC</a></li>
<li>paper_authors: Min Liu, Gang Yang, Siyuan Luo, Chen Yu, Lin Shao</li>
<li>for:  This paper aims to provide a unified framework for simulating diverse robotic manipulation scenarios by integrating soft bodies, articulated rigid bodies, and clothes.</li>
<li>methods: The proposed method, called SoftMAC, uses the Material Point Method (MPM) to simulate soft bodies and a forecast-based contact model to reduce artifacts. It also includes a penetration tracing algorithm to couple MPM particles with deformable and non-volumetric clothes meshes.</li>
<li>results: The authors validate the effectiveness and accuracy of the proposed differentiable pipeline through comprehensive experiments in downstream robotic manipulation applications.Hereâ€™s the Chinese version:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯æä¾›ä¸€ä¸ªç»¼åˆçš„æœºå™¨äººæ“ä½œåœºæ™¯æ¨¡æ‹Ÿæ¡†æ¶ï¼Œæ•´åˆè½¯ä½“ã€éª¨éª¼åˆšä½“å’Œè¡£ç‰©ç­‰å¤šç§ææ–™ã€‚</li>
<li>methods: æè®®çš„æ–¹æ³•æ˜¯SoftMACï¼Œä½¿ç”¨ç‰©ç†ç‚¹æ–¹æ³•ï¼ˆMPMï¼‰æ¨¡æ‹Ÿè½¯ä½“ï¼Œå¹¶é‡‡ç”¨é¢„æµ‹åŸºäºçš„æ¥è§¦æ¨¡å‹æ¥å‡å°‘artefactsã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ç§ç©¿é€è·Ÿè¸ªç®—æ³•ï¼Œå°†MPMç²’å­ä¸å¯å˜å½¢å’Œéæ¶²ä½“è¡£ç‰©ç½‘æ ¼ç›¸äº’å…³è”ã€‚</li>
<li>results: ä½œè€…é€šè¿‡å¯¹ä¸‹æ¸¸æœºå™¨äººæ“ä½œåº”ç”¨çš„å¹¿æ³›å®éªŒ validateäº†æè®®çš„å¯å¯¼å¼ç®¡é“çš„æ•ˆæœå’Œå‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
Differentiable physics simulation provides an avenue for tackling previously intractable challenges through gradient-based optimization, thereby greatly improving the efficiency of solving robotics-related problems. To apply differentiable simulation in diverse robotic manipulation scenarios, a key challenge is to integrate various materials in a unified framework. We present SoftMAC, a differentiable simulation framework coupling soft bodies with articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the continuum-mechanics-based Material Point Method (MPM). We provide a forecast-based contact model for MPM, which greatly reduces artifacts like penetration and unnatural rebound. To couple MPM particles with deformable and non-volumetric clothes meshes, we also propose a penetration tracing algorithm that reconstructs the signed distance field in local area. Based on simulators for each modality and the contact model, we develop a differentiable coupling mechanism to simulate the interactions between soft bodies and the other two types of materials. Comprehensive experiments are conducted to validate the effectiveness and accuracy of the proposed differentiable pipeline in downstream robotic manipulation applications. Supplementary materials and videos are available on our project website at https://sites.google.com/view/softmac.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šå¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿï¼šä¸€ç§æé«˜æœºå™¨äººé—®é¢˜è§£å†³æ•ˆç‡çš„æ–°é€”å¾„ã€‹å¯å¾®åˆ†ç‰©ç†æ¨¡æ‹Ÿæä¾›äº†è§£å†³å‰æ— æ³•è§£å†³çš„æŒ‘æˆ˜çš„æ–°é€”å¾„ï¼Œé€šè¿‡æ¢¯åº¦åŸºäºä¼˜åŒ–ï¼Œå¤§å¹…æé«˜æœºå™¨äººé—®é¢˜çš„è§£å†³æ•ˆç‡ã€‚ä¸ºåœ¨å¤šæ ·åŒ–æœºå™¨äººæ“ä½œåœºæ™¯ä¸­åº”ç”¨å¯å¾®åˆ†æ¨¡æ‹Ÿï¼Œä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å°†å„ç§ææ–™é›†æˆåˆ°ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†SoftMACï¼Œä¸€ä¸ªå¯å¾®åˆ†æ¨¡æ‹Ÿæ¡†æ¶ï¼Œå°†è½¯ä½“ä¸æœºæ¢°è‚¢å’Œè¡£ç‰©ç›¸è¿æ¥ã€‚SoftMACä½¿ç”¨ç‰©ç‚¹æ–¹æ³•ï¼ˆMPMï¼‰æ¥æ¨¡æ‹Ÿè½¯ä½“ï¼Œå¹¶æä¾›äº†ä¸€ç§é¢„æµ‹åŸºäºçš„æ¥è§¦æ¨¡å‹ï¼Œå¯ä»¥å‡å°‘ç©¿é€å’Œä¸è‡ªç„¶çš„åå¼¹ç°è±¡ã€‚ä¸ºå°†MPM particelsä¸å¯å˜å½¢å’Œéæ¶²ä½“è¡£ç‰©ç½‘æ ¼ç›¸è¿æ¥ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç©¿é€è·Ÿè¸ªç®—æ³•ï¼Œå¯ä»¥åœ¨æœ¬åœ°åŒºåŸŸé‡å»ºç­¾åè·ç¦»åœºã€‚åŸºäºæ¨¡æ‹Ÿå™¨å’Œæ¥è§¦æ¨¡å‹ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯å¾®åˆ†è¿æ¥æœºåˆ¶ï¼Œä»¥æ¨¡æ‹Ÿè½¯ä½“ä¸å…¶ä»–ä¸¤ç§ææ–™ä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œä»¥éªŒè¯ææ¡£çš„æœ‰æ•ˆæ€§å’Œå‡†ç¡®æ€§åœ¨ä¸‹æ¸¸æœºå™¨äººæ“ä½œåº”ç”¨ä¸­ã€‚è¡¥å……ææ–™å’Œè§†é¢‘å¯ä»¥åœ¨æˆ‘ä»¬é¡¹ç›®ç½‘ç«™ï¼ˆhttps://sites.google.com/view/softmacï¼‰ä¸Šè·å¾—ã€‚
</details></li>
</ul>
<hr>
<h2 id="OMNIINPUT-A-Model-centric-Evaluation-Framework-through-Output-Distribution"><a href="#OMNIINPUT-A-Model-centric-Evaluation-Framework-through-Output-Distribution" class="headerlink" title="OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution"></a>OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03291">http://arxiv.org/abs/2312.03291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weitang Liu, Ying Wai Li, Tianle Wang, Yi-Zhuang You, Jingbo Shang</li>
<li>for: è¯„ä¼°AI&#x2F;MLæ¨¡å‹é¢„æµ‹ç»“æœçš„è´¨é‡ï¼Œå°¤å…¶æ˜¯å¯¹äºäººç±»ä¸å¯è¯†åˆ«çš„è¾“å…¥ã€‚</li>
<li>methods: ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•é›†å’Œæ¨¡å‹è‡ªèº«çš„è¾“å‡ºåˆ†å¸ƒæ¥è¯„ä¼°æ¨¡å‹è´¨é‡ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„æ•°æ®é›†ä¸­å¿ƒçš„è¯„ä¼°æ–¹æ³•ã€‚</li>
<li>results: èƒ½å¤Ÿæ›´ç»†åŒ–åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨é¢„æµ‹ç»“æœå‡ ä¹ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œä»è€Œè·å¾—æ–°çš„å‘ç°å’Œå¯ç¤ºï¼Œæœ‰åŠ©äºè®­ç»ƒæ›´åŠ ç¨³å®šå’Œæ³›åŒ–çš„æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
We propose a novel model-centric evaluation framework, OmniInput, to evaluate the quality of an AI/ML model's predictions on all possible inputs (including human-unrecognizable ones), which is crucial for AI safety and reliability. Unlike traditional data-centric evaluation based on pre-defined test sets, the test set in OmniInput is self-constructed by the model itself and the model quality is evaluated by investigating its output distribution. We employ an efficient sampler to obtain representative inputs and the output distribution of the trained model, which, after selective annotation, can be used to estimate the model's precision and recall at different output values and a comprehensive precision-recall curve. Our experiments demonstrate that OmniInput enables a more fine-grained comparison between models, especially when their performance is almost the same on pre-defined datasets, leading to new findings and insights for how to train more robust, generalizable models.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ä¸­å¿ƒè¯„ä¼°æ¡†æ¶ï¼ŒOmniInputï¼Œä»¥è¯„ä¼°äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸­çš„æ‰€æœ‰å¯èƒ½è¾“å…¥ï¼ˆåŒ…æ‹¬äººç±»æ— æ³•è¯†åˆ«çš„ï¼‰ï¼Œè¿™å¯¹äºäººå·¥æ™ºèƒ½å®‰å…¨å’Œå¯é æ€§è‡³å…³é‡è¦ã€‚ä¸ä¼ ç»Ÿçš„æ•°æ®ä¸­å¿ƒè¯„ä¼°åŸºäºé¢„å…ˆå®šä¹‰çš„æµ‹è¯•é›†ä¸åŒï¼ŒOmniInput çš„æµ‹è¯•é›†ç”±æ¨¡å‹è‡ªå·±æ„å»ºï¼Œå¹¶é€šè¿‡è°ƒæŸ¥è¾“å‡ºåˆ†å¸ƒæ¥è¯„ä¼°æ¨¡å‹è´¨é‡ã€‚æˆ‘ä»¬ä½¿ç”¨é«˜æ•ˆçš„é‡‡æ ·å™¨è·å–ä»£è¡¨æ€§çš„è¾“å…¥ï¼Œå¹¶å¯¹è®­ç»ƒåçš„æ¨¡å‹è¾“å‡ºè¿›è¡Œé€‰æ‹©æ€§æ ‡æ³¨ï¼Œä»¥ä¾¿è®¡ç®—æ¨¡å‹çš„ç²¾åº¦å’Œå‡†ç¡®ç‡åœ¨ä¸åŒçš„è¾“å‡ºå€¼ä¸Šï¼Œå¹¶ç”Ÿæˆäº†å…¨é¢çš„ç²¾åº¦-å‡†ç¡®ç‡æ›²çº¿ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒOmniInput å¯ä»¥å¯¹æ¨¡å‹è¿›è¡Œæ›´ç»†è‡´çš„æ¯”è¾ƒï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹åœ¨é¢„å…ˆå®šä¹‰çš„æ•°æ®é›†ä¸Šçš„æ€§èƒ½å‡ ä¹ç›¸åŒæ—¶ï¼Œä»è€Œå¯¼è‡´æ–°çš„å‘ç°å’Œæ´å¯Ÿï¼Œå¸®åŠ©trainæ›´åŠ ç¨³å®šå’Œæ³›åŒ–çš„æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Can-language-agents-be-alternatives-to-PPO-A-Preliminary-Empirical-Study-On-OpenAI-Gym"><a href="#Can-language-agents-be-alternatives-to-PPO-A-Preliminary-Empirical-Study-On-OpenAI-Gym" class="headerlink" title="Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym"></a>Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03290">http://arxiv.org/abs/2312.03290</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mail-ecnu/text-gym-agents">https://github.com/mail-ecnu/text-gym-agents</a></li>
<li>paper_authors: Junjie Sheng, Zixiao Huang, Chuyun Shen, Wenhao Li, Yun Hua, Bo Jin, Hongyuan Zha, Xiangfeng Wang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨è¯­è¨€ä»£ç†æ˜¯å¦å¯ä»¥å–ä»£ä¼ ç»Ÿçš„PPOä»£ç†åœ¨é¡ºåºå†³ç­–ä»»åŠ¡ä¸­ã€‚</li>
<li>methods: ç ”ç©¶è€…é¦–å…ˆä½¿ç”¨OpenAI Gymä¸­æ”¶é›†çš„ç¯å¢ƒä½œä¸ºæµ‹è¯•åºŠï¼Œå¹¶å°†è¿™äº›ç¯å¢ƒè½¬åŒ–ä¸ºæ–‡æœ¬ç¯å¢ƒï¼Œä»¥ä¾¿ä¸è¯­è¨€ä»£ç†è¿›è¡Œç›´è§‚å’Œé«˜æ•ˆçš„æ¯”è¾ƒã€‚</li>
<li>results: ç ”ç©¶è€…é€šè¿‡æ•°å€¼å®éªŒå’Œå‰–æç ”ç©¶ï¼Œæå–äº†è¯­è¨€ä»£ç†çš„å†³ç­–èƒ½åŠ›çš„æœ‰ä»·å€¼ä¿¡æ¯ï¼Œå¹¶å¯¹è¯­è¨€ä»£ç†ä½œä¸ºPPOä»£ç†çš„æ½œåœ¨ä»£æ›¿è¿›è¡Œåˆæ­¥è¯„ä¼°ã€‚<details>
<summary>Abstract</summary>
The formidable capacity for zero- or few-shot decision-making in language agents encourages us to pose a compelling question: Can language agents be alternatives to PPO agents in traditional sequential decision-making tasks? To investigate this, we first take environments collected in OpenAI Gym as our testbeds and ground them to textual environments that construct the TextGym simulator. This allows for straightforward and efficient comparisons between PPO agents and language agents, given the widespread adoption of OpenAI Gym. To ensure a fair and effective benchmarking, we introduce $5$ levels of scenario for accurate domain-knowledge controlling and a unified RL-inspired framework for language agents. Additionally, we propose an innovative explore-exploit-guided language (EXE) agent to solve tasks within TextGym. Through numerical experiments and ablation studies, we extract valuable insights into the decision-making capabilities of language agents and make a preliminary evaluation of their potential to be alternatives to PPO in classical sequential decision-making problems. This paper sheds light on the performance of language agents and paves the way for future research in this exciting domain. Our code is publicly available at~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡ä¸­æå‡ºäº†ä¸€ä¸ªå¸å¼•äººçš„é—®é¢˜ï¼šå¯ä»¥å¦ä½¿ç”¨è¯­è¨€ä»£ç†äººä»£æ›¿ä¼ ç»Ÿçš„é¡ºåºå†³ç­–ä»»åŠ¡ä¸­çš„PPOä»£ç†äººï¼Ÿä¸ºäº† investigateè¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨OpenAI Gymä¸­æ”¶é›†çš„ç¯å¢ƒä½œä¸ºæµ‹è¯•ç¯å¢ƒï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæ–‡æœ¬ç¯å¢ƒï¼Œè¿™ä½¿å¾—å¯¹æ¯”è¯­è¨€ä»£ç†äººå’ŒPPOä»£ç†äººçš„æ¯”è¾ƒå˜å¾—æ›´åŠ ç›´è§‚å’Œæ•ˆç‡é«˜ã€‚ä¸ºç¡®ä¿å…¬æ­£å’Œæœ‰æ•ˆçš„å¯¹æ¯”ï¼Œæˆ‘ä»¬å¼•å…¥äº†5çº§çš„æƒ…æ™¯æ¥æ§åˆ¶åŸŸçŸ¥è¯†ï¼Œå¹¶æå‡ºäº†ä¸€ç§RL inspiritedæ¡†æ¶æ¥ guidelineè¯­è¨€ä»£ç†äººè§£å†³TextGymä¸­çš„ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å°è¯•-åˆ©ç”¨-å¼•å¯¼è¯­è¨€ä»£ç†äººï¼ˆEXEï¼‰æ¥è§£å†³TextGymä¸­çš„ä»»åŠ¡ã€‚é€šè¿‡æ•°å€¼å®éªŒå’Œå‰¥ç¦»ç ”ç©¶ï¼Œæˆ‘ä»¬ä»è¯­è¨€ä»£ç†äººçš„å†³ç­–èƒ½åŠ›ä¸­è·å¾—äº†æœ‰ä»·å€¼çš„å‘ç°ï¼Œå¹¶å¯¹è¯­è¨€ä»£ç†äººæ˜¯å¦å¯ä»¥æ›¿ä»£PPOè¿›è¡Œäº†åˆæ­¥è¯„ä¼°ã€‚è¿™ç¯‡è®ºæ–‡ç…§äº®äº†è¯­è¨€ä»£ç†äººçš„è¡¨ç°ï¼Œå¹¶ä¸ºè¿™ä¸€æœ‰è¶£çš„é¢†åŸŸå¼€è¾Ÿäº†æœªæ¥ç ”ç©¶çš„é“è·¯ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€å¯ä»¥åœ¨GitHubä¸Šè·å–ï¼Œè¯·å‚è€ƒ~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
</details></li>
</ul>
<hr>
<h2 id="STEP-CATFormer-Spatial-Temporal-Effective-Body-Part-Cross-Attention-Transformer-for-Skeleton-based-Action-Recognition"><a href="#STEP-CATFormer-Spatial-Temporal-Effective-Body-Part-Cross-Attention-Transformer-for-Skeleton-based-Action-Recognition" class="headerlink" title="STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition"></a>STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03288">http://arxiv.org/abs/2312.03288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maclong01/STEP-CATFormer">https://github.com/maclong01/STEP-CATFormer</a></li>
<li>paper_authors: Nguyen Huu Bao Long</li>
<li>For: æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºéª¨æ¶çš„åŠ¨ä½œè¯†åˆ«ä¸­Graph Convolutional Convolution networksï¼ˆGCNsï¼‰çš„åº”ç”¨å’Œä¼˜åŒ–ã€‚* Methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸‰ç§Channel-wise Topology Graph Convolutionï¼ˆCTR-GCNï¼‰ï¼Œå¹¶å°†å…¶ä¸ä¸¤ç§è·¨ä½“éƒ¨å…³æ³¨æ¨¡å—ç»“åˆï¼Œä»¥æ•æ‰äººä½“éª¨æ¶ä¸Šä¸‹ä½“éƒ¨å’Œæ‰‹è„šå…³ç³»ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†Temporal Attention Transformersæ¥EXTRACTskeletonç‰¹å¾ã€‚* Results: æœ¬ç ”ç©¶åœ¨NTU RGB+Då’ŒNTU RGB+D 120æ•°æ®é›†ä¸Šè¾¾åˆ°äº† notable high-performanceã€‚Translation:* For: This study explores the application and optimization of Graph Convolutional Convolution networks (GCNs) in skeleton-based action recognition.* Methods: The study proposes three Channel-wise Topology Graph Convolution (CTR-GCN) methods, and combines them with two joint cross-attention modules to capture upper-lower body part and hand-foot relationships in skeleton features. Additionally, the study proposes Temporal Attention Transformers to extract skeleton features effectively.* Results: The study achieves notable high-performance on the NTU RGB+D and NTU RGB+D 120 datasets.<details>
<summary>Abstract</summary>
Graph convolutional networks (GCNs) have been widely used and achieved remarkable results in skeleton-based action recognition. We think the key to skeleton-based action recognition is a skeleton hanging in frames, so we focus on how the Graph Convolutional Convolution networks learn different topologies and effectively aggregate joint features in the global temporal and local temporal. In this work, we propose three Channel-wise Tolopogy Graph Convolution based on Channel-wise Topology Refinement Graph Convolution (CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture the upper-lower body part and hand-foot relationship skeleton features. After that, to capture features of human skeletons changing in frames we design the Temporal Attention Transformers to extract skeletons effectively. The Temporal Attention Transformers can learn the temporal features of human skeleton sequences. Finally, we fuse the temporal features output scale with MLP and classification. We develop a powerful graph convolutional network named Spatial Temporal Effective Body-part Cross Attention Transformer which notably high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models are available at https://github.com/maclong01/STEP-CATFormer
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå‡ ä½•å·ç§¯ç½‘ç»œï¼ˆGCNsï¼‰åœ¨skeletonåŸºæœ¬åŠ¨ä½œè¯†åˆ«ä¸­å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨å’Œå‡ºè‰²çš„ç»“æœã€‚æˆ‘ä»¬è®¤ä¸ºskeletonä¸­çš„éª¨æ¶å·ç§¯æ˜¯å…³é”®ï¼Œå› æ­¤æˆ‘ä»¬ä¸“æ³¨äºå¦‚ä½•ä½¿Graph Convolutional Convolutionç½‘ç»œå­¦ä¹ ä¸åŒçš„æ‹“æ‰‘å’Œæœ‰æ•ˆåœ°èšåˆå…³èŠ‚ç‰¹å¾åœ¨å…¨çƒæ—¶é—´å’Œå±€éƒ¨æ—¶é—´ã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸‰ç§é€šé“çº§åˆ«æ‹“æ‰‘å·ç§¯åŸºäºé€šé“çº§åˆ«æ‹“æ‰‘ä¿®å‰ªGraph Convolutionï¼ˆCTR-GCNï¼‰ã€‚å°†CTR-GCNä¸ä¸¤ä¸ªäº¤å‰å…³æ³¨æ¨¡å—ç›¸ç»“åˆå¯ä»¥æ•æ‰ä¸Šä¸‹èº¯ä½“å’Œæ‰‹è„šå…³ç³»éª¨æ¶ç‰¹å¾ã€‚ç„¶åï¼Œä¸ºäº†æœ‰æ•ˆåœ°æå–äººä½“éª¨æ¶åœ¨å¸§å†…çš„å˜åŒ–ç‰¹å¾ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—¶é—´æ³¨æ„åŠ›å˜æ¢å™¨ã€‚æ—¶é—´æ³¨æ„åŠ›å˜æ¢å™¨å¯ä»¥å­¦ä¹ äººä½“éª¨æ¶åºåˆ—ä¸­çš„æ—¶é—´ç‰¹å¾ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ—¶é—´ç‰¹å¾è¾“å‡ºè§„æ ¼ä¸å¤šå±‚æ„ŸçŸ¥ï¼ˆMLPï¼‰å’Œåˆ†ç±»ç»“åˆï¼Œå¹¶å‘å±•å‡ºä¸€ç§é«˜æ€§èƒ½çš„å‡ ä½•å·ç§¯ç½‘ç»œï¼Œç§°ä¸ºç©ºé—´æ—¶é—´æœ‰æ•ˆä½“éƒ¨ç›¸å…³è½¬æ¢å™¨ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å¯ä»¥åœ¨https://github.com/maclong01/STEP-CATFormerä¸Šè·å–ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="VLFM-Vision-Language-Frontier-Maps-for-Zero-Shot-Semantic-Navigation"><a href="#VLFM-Vision-Language-Frontier-Maps-for-Zero-Shot-Semantic-Navigation" class="headerlink" title="VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation"></a>VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03275">http://arxiv.org/abs/2312.03275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Yokoyama, Sehoon Ha, Dhruv Batra, Jiuguang Wang, Bernadette Bucher</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯æå‡ºä¸€ç§é›¶shot navigation æ–¹æ³•ï¼Œå¸®åŠ©æœºå™¨äººåœ¨æœªç»è®­ç»ƒçš„ç¯å¢ƒä¸­å¯»æ‰¾ç›®æ ‡å¯¹è±¡ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œä»æ·±åº¦è§‚å¯ŸValue Mapï¼Œå¹¶ä½¿ç”¨RGB Observationsæ¥ç”Ÿæˆè¯­è¨€æƒé‡å›¾ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨ Gibsonã€Habitat-Matterport 3D å’Œ Matterport 3D æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³çš„ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°chsï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²åœ¨ Boston Dynamics Spot ç§»åŠ¨ manipulate å¹³å°ä¸Šï¼Œefficiently å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡ã€‚<details>
<summary>Abstract</summary>
Understanding how humans leverage semantic knowledge to navigate unfamiliar environments and decide where to explore next is pivotal for developing robots capable of human-like search behaviors. We introduce a zero-shot navigation approach, Vision-Language Frontier Maps (VLFM), which is inspired by human reasoning and designed to navigate towards unseen semantic objects in novel environments. VLFM builds occupancy maps from depth observations to identify frontiers, and leverages RGB observations and a pre-trained vision-language model to generate a language-grounded value map. VLFM then uses this map to identify the most promising frontier to explore for finding an instance of a given target object category. We evaluate VLFM in photo-realistic environments from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D) datasets within the Habitat simulator. Remarkably, VLFM achieves state-of-the-art results on all three datasets as measured by success weighted by path length (SPL) for the Object Goal Navigation task. Furthermore, we show that VLFM's zero-shot nature enables it to be readily deployed on real-world robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy VLFM on Spot and demonstrate its capability to efficiently navigate to target objects within an office building in the real world, without any prior knowledge of the environment. The accomplishments of VLFM underscore the promising potential of vision-language models in advancing the field of semantic navigation. Videos of real-world deployment can be viewed at naoki.io/vlfm.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»å¦‚ä½•åˆ©ç”¨ semantic knowledge æ¥æ¢ç´¢æœªçŸ¥ç¯å¢ƒå¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨å¯¹äºå¼€å‘äººç±»æ ·å¼æœç´¢è¡Œä¸ºçš„æœºå™¨äººæ¥è¯´éå¸¸é‡è¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é›¶æ‰¹æ³¨å¯¼èˆªæ–¹æ³•ï¼Œå³ Vision-Language Frontier Maps (VLFM)ï¼Œè¿™ç§æ–¹æ³•çµæ„Ÿè‡ªäººç±»çš„æ€ç»´å’Œå†³ç­–ï¼Œç”¨äºåœ¨æ–°ç¯å¢ƒä¸­å¯¼èˆªåˆ°æœªç»è§è¿‡çš„ semantic å¯¹è±¡ã€‚VLFM ä»æ·±åº¦è§‚æµ‹ä¸­ç”Ÿæˆå æ®åœ°å›¾ï¼Œå¹¶ä½¿ç”¨ RGB è§‚æµ‹å’Œé¢„è®­ç»ƒçš„è§†åŠ›è¯­è¨€æ¨¡å‹ç”Ÿæˆè¯­è¨€å›ºå®šå€¼å›¾ã€‚VLFM ç„¶åä½¿ç”¨è¿™ä¸ªå›¾æ¥ç¡®å®šæœç´¢æœ€æœ‰å‰é€”çš„æ–¹å‘ï¼Œä»¥æ‰¾åˆ°ç»™å®šç›®æ ‡å¯¹è±¡ç±»å‹çš„å®ä¾‹ã€‚æˆ‘ä»¬åœ¨ Gibsonã€Habitat-Matterport 3D å’Œ Matterport 3D  datasets ä¸­çš„ Habitat  simulate ç¯å¢ƒè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ˜¾ç¤º VLFM åœ¨è¿™äº›æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³çš„æˆç»©ï¼Œ measured by success weighted by path length (SPL) å¯¹è±¡ç›®æ ‡å¯¼èˆªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ VLFM çš„é›¶æ‰¹æ³¨ç‰¹æ€§ä½¿å¾—å®ƒå¯ä»¥è½»æ¾åœ°åœ¨çœŸå®ä¸–ç•Œä¸­éƒ¨ç½²ï¼Œå¦‚ Boston Dynamics Spot ç§»åŠ¨ manipulate å¹³å°ã€‚æˆ‘ä»¬åœ¨ Spot ä¸Šéƒ¨ç½² VLFMï¼Œå¹¶åœ¨çœŸå®ä¸–ç•Œä¸­ efficiently å¯¼èˆªåˆ°ç›®æ ‡å¯¹è±¡å†…éƒ¨ï¼Œæ— éœ€ç¯å¢ƒçš„å…ˆå‰çŸ¥è¯†ã€‚VLFM çš„æˆå°±æ¨ç¥Ÿäºè§†åŠ›è¯­è¨€æ¨¡å‹åœ¨ semantic å¯¼èˆªé¢†åŸŸçš„æ½œåœ¨æ½œåŠ›ã€‚è§†é¢‘å¯ä»¥åœ¨ naoki.io/vlfm ä¸Šæ¬£èµã€‚
</details></li>
</ul>
<hr>
<h2 id="Weathering-Ongoing-Uncertainty-Learning-and-Planning-in-a-Time-Varying-Partially-Observable-Environment"><a href="#Weathering-Ongoing-Uncertainty-Learning-and-Planning-in-a-Time-Varying-Partially-Observable-Environment" class="headerlink" title="Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment"></a>Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03263">http://arxiv.org/abs/2312.03263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gokul Puthumanaillam, Xiangyu Liu, Negar Mehr, Melkior Ornik</li>
<li>for: This paper aims to improve the optimal decision-making of autonomous systems in uncertain, stochastic, and time-varying environments.</li>
<li>methods: The paper combines Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). The proposed approach includes Memory Prioritized State Estimation (MPSE) and an MPSE-integrated planning strategy.</li>
<li>results: The proposed framework and algorithms demonstrate superior performance over standard methods in simulated and real-world experiments, showcasing their effectiveness in stochastic, uncertain, time-varying domains.<details>
<summary>Abstract</summary>
Optimal decision-making presents a significant challenge for autonomous systems operating in uncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼˜åŒ–å†³ç­–presentsthan significant challenge for autonomous systems operating inuncertain, stochastic and time-varying environments. Environmental variability over time can significantly impact the system's optimal decision-making strategy for mission completion. To model such environments, our work combines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.Here's the translation in Traditional Chinese:ä¼˜åŒ–å†³ç­–å‘ˆç°äº† autonomous systems operate inuncertain, stochastic and time-varying environmentsä¸­çš„ä¸€ä¸ª significannot challenge. ç¯å¢ƒå˜åŒ–over timeå¯ä»¥å½±å“ç³»ç»Ÿçš„ä¼˜åŒ–å†³ç­–ç­–ç•¥ï¼Œä»¥ completeloss mission. ä¸ºäº†æ¨¡å‹è¿™äº›ç¯å¢ƒï¼Œæˆ‘ä»¬çš„å·¥ä½œcombines the previous notion of Time-Varying Markov Decision Processes (TVMDP) with partial observability and introduces Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We propose a two-pronged approach to accurately estimate and plan within the TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages weighted memory to provide more accurate time-varying transition estimates; and 2) an MPSE-integrated planning strategy that optimizes long-term rewards while accounting for temporal constraint. We validate the proposed framework and algorithms using simulations and hardware, with robots exploring a partially observable, time-varying environments. Our results demonstrate superior performance over standard methods, highlighting the framework's effectiveness in stochastic, uncertain, time-varying domains.
</details></li>
</ul>
<hr>
<h2 id="Customizable-Combination-of-Parameter-Efficient-Modules-for-Multi-Task-Learning"><a href="#Customizable-Combination-of-Parameter-Efficient-Modules-for-Multi-Task-Learning" class="headerlink" title="Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning"></a>Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03248">http://arxiv.org/abs/2312.03248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haowen Wang, Tao Sun, Cong Fan, Jinjie Gu</li>
<li>for: æé«˜å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„æ ·æœ¬æ•ˆç‡</li>
<li>methods: ä½¿ç”¨è‡ªé€‚åº”ç²¾åº¦å­¦ä¹ ç­–ç•¥å’Œä½çº§æ•°æ®ç²¾åº¦å­¦ä¹ </li>
<li>results: æ¯”å¯¹åŸºelineå’Œä»»åŠ¡ç‰¹å®šå’ŒæŠ€èƒ½æ— å…³åŸºelineçš„å®éªŒç»“æœï¼ŒC-Polyæ˜¾ç¤ºå‡ºæ˜æ˜¾çš„æ€§èƒ½æå‡<details>
<summary>Abstract</summary>
Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization. In this paper, we introduce a novel approach Customized Polytropon C-Poly that combines task-common skills and task-specific skills, while the skill parameters being highly parameterized using low-rank techniques. Each task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks. A skill assignment matrix is jointly learned. To evaluate our approach, we conducted extensive experiments on the Super-NaturalInstructions and the SuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms fully-shared, task-specific, and skill-indistinguishable baselines, significantly enhancing the sample efficiency in multi-task learning scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¨¡å—åŒ–å’Œå¯ compose çš„ä¼ è¾“å­¦ä¹ æ˜¯ç°ä»£ Parameter Efficient Fine-Tuning é¢†åŸŸçš„ä¸€ä¸ªemerging directionï¼Œå› ä¸ºå®ƒä½¿å¾—ç¥ç»ç½‘ç»œæ›´å¥½åœ°ç»„ç»‡äº†ä¸åŒæ–¹é¢çš„çŸ¥è¯†ï¼Œä»è€Œæé«˜äº†äº¤å‰ä»»åŠ¡æ³›åŒ–æ€§ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•Customized Polytropon C-Polyï¼Œå®ƒå°†ä»»åŠ¡å…±åŒæŠ€èƒ½å’Œä»»åŠ¡ç‰¹å®šæŠ€èƒ½ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä½¿ç”¨ä½ç»´åº¦æŠ€æœ¯æ¥é«˜åº¦å‚æ•°åŒ–æŠ€èƒ½å‚æ•°ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å¯å®šåˆ¶çš„ä¸“å±ç‰¹æœ‰æŠ€èƒ½ï¼ŒåŒæ—¶è¿˜å¯ä»¥ä»åŒç±»ä»»åŠ¡ä¸­è·å¾—å…±äº«çš„æŠ€èƒ½ã€‚ä¸€ä¸ªä»»åŠ¡åˆ†é…çŸ©é˜µæ˜¯åŒæ—¶å­¦ä¹ çš„ã€‚ä¸ºäº†è¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨Super-NaturalInstructionså’ŒSuperGLUE bencmarksä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼ŒC-Poly åœ¨å¤šä»»åŠ¡å­¦ä¹ åœºæ™¯ä¸­æ˜¾è‘—æé«˜äº†æ ·æœ¬æ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Framework-to-Enhance-the-Adversarial-Robustness-of-Deep-Learning-based-Intrusion-Detection-System"><a href="#A-Simple-Framework-to-Enhance-the-Adversarial-Robustness-of-Deep-Learning-based-Intrusion-Detection-System" class="headerlink" title="A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System"></a>A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03245">http://arxiv.org/abs/2312.03245</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinwei Yuan, Shu Han, Wei Huang, Hongliang Ye, Xianglong Kong, Fan Zhang</li>
<li>For: The paper proposes a novel intrusion detection system (IDS) architecture that combines conventional machine learning (ML) models and deep learning (DL) models to enhance the robustness of IDS against adversarial attacks.* Methods: The proposed IDS architecture consists of three components: DL-based IDS, adversarial example (AE) detector, and ML-based IDS. The AE detector is based on the local intrinsic dimensionality (LID), and the ML-based IDS is used to determine the maliciousness of AEs. The fusion mechanism leverages the high prediction accuracy of DL models and low attack transferability between DL models and ML models to improve the robustness of the whole system.* Results: The paper shows a significant improvement in the prediction performance of the IDS when subjected to adversarial attack, achieving high accuracy with low resource consumption.Here are the three key points in Simplified Chinese text:* For: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰æ¶æ„ï¼Œè¯¥æ¶æ„ç»“åˆäº†ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹å’Œæ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æ¨¡å‹ï¼Œä»¥æé«˜IDSå¯¹äºæ”»å‡»è€…çš„æŠµæŠ—æ€§ã€‚* Methods: æè®®çš„IDSæ¶æ„åŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šDL-based IDSã€æ”»å‡»ä¾‹ç¤ºå™¨ï¼ˆAEï¼‰æ£€æµ‹å™¨å’ŒML-based IDSã€‚AEæ£€æµ‹å™¨åŸºäºæœ¬åœ°ç‰¹å¾ç»´åº¦ï¼ˆLIDï¼‰ï¼Œè€ŒML-based IDSç”¨äºç¡®å®šAEçš„æ¶æ„ç¨‹åº¦ã€‚æ··åˆæœºåˆ¶åˆ©ç”¨DLæ¨¡å‹çš„é«˜é¢„æµ‹ç²¾åº¦å’ŒDLæ¨¡å‹å’ŒMLæ¨¡å‹ä¹‹é—´çš„ä½æ”»å‡»ä¼ é€’æ€§ï¼Œä»¥æé«˜æ•´ä¸ªç³»ç»Ÿçš„Robustnessã€‚* Results: æœ¬æ–‡å®éªŒç»“æœè¡¨æ˜ï¼Œå½“IDSé¢ä¸´æ”»å‡»æ—¶ï¼Œæè®®çš„IDSæ¶æ„å¯ä»¥è·å¾—é«˜ç²¾åº¦ã€ä½èµ„æºå ç”¨çš„é¢„æµ‹æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Deep learning based intrusion detection systems (DL-based IDS) have emerged as one of the best choices for providing security solutions against various network intrusion attacks. However, due to the emergence and development of adversarial deep learning technologies, it becomes challenging for the adoption of DL models into IDS. In this paper, we propose a novel IDS architecture that can enhance the robustness of IDS against adversarial attacks by combining conventional machine learning (ML) models and Deep Learning models. The proposed DLL-IDS consists of three components: DL-based IDS, adversarial example (AE) detector, and ML-based IDS. We first develop a novel AE detector based on the local intrinsic dimensionality (LID). Then, we exploit the low attack transferability between DL models and ML models to find a robust ML model that can assist us in determining the maliciousness of AEs. If the input traffic is detected as an AE, the ML-based IDS will predict the maliciousness of input traffic, otherwise the DL-based IDS will work for the prediction. The fusion mechanism can leverage the high prediction accuracy of DL models and low attack transferability between DL models and ML models to improve the robustness of the whole system. In our experiments, we observe a significant improvement in the prediction performance of the IDS when subjected to adversarial attack, achieving high accuracy with low resource consumption.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦å­¦ä¹ åŸºäºçš„å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆDL-IDSï¼‰å·²ç»æˆä¸ºæä¾›å®‰å…¨è§£å†³æ–¹æ¡ˆçš„ä¸€ç§ä¼˜é€‰ï¼Œä½†ç”±äºå¯¹æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å‘å±•å’Œåº”ç”¨ï¼ŒDLæ¨¡å‹åœ¨IDSä¸­çš„é‡‡ç”¨å—åˆ°æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„IDSæ¶æ„ï¼Œå¯ä»¥é€šè¿‡ç»“åˆæ·±åº¦å­¦ä¹ æ¨¡å‹å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹æ¥å¢å¼ºIDSå¯¹å‡æ•°æ®æ”»å‡»çš„Robustnessã€‚æˆ‘ä»¬çš„ææ¡ˆåŒ…æ‹¬ä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼šDL-IDSã€å‡æ•°æ®æ£€æµ‹å™¨å’ŒML-IDSã€‚æˆ‘ä»¬é¦–å…ˆå¼€å‘äº†ä¸€ç§åŸºäºæœ¬åœ°å†…åœ¨ç»´åº¦ï¼ˆLIDï¼‰çš„å‡æ•°æ®æ£€æµ‹å™¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨DLæ¨¡å‹å’ŒMLæ¨¡å‹ä¹‹é—´çš„æ”»å‡»ä¼ é€’ç‡ä½ï¼Œæ‰¾åˆ°ä¸€ä¸ªå¯é çš„MLæ¨¡å‹ï¼Œä»¥ç¡®å®šå‡æ•°æ®çš„Maliciousnessã€‚å¦‚æœè¾“å…¥æµé‡è¢«æ£€æµ‹ä¸ºå‡æ•°æ®ï¼Œåˆ™ML-IDSå°†é¢„æµ‹è¾“å…¥æµé‡çš„Maliciousnessï¼Œå¦åˆ™DL-IDSå°†è¿›è¡Œé¢„æµ‹ã€‚æ··åˆæœºåˆ¶å¯ä»¥åˆ©ç”¨DLæ¨¡å‹çš„é«˜é¢„æµ‹ç²¾åº¦å’ŒMLæ¨¡å‹ä¹‹é—´çš„æ”»å‡»ä¼ é€’ç‡ä½ï¼Œæé«˜æ•´ä½“ç³»ç»Ÿçš„Robustnessã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“è¾“å…¥æµé‡é­å—å‡æ•°æ®æ”»å‡»æ—¶ï¼ŒIDSçš„é¢„æµ‹æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æé«˜ï¼Œ Ğ´Ğ¾ÑÑ‚Ğ¸ievingé«˜ç²¾åº¦ä½èµ„æºæ¶ˆè€—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multicoated-and-Folded-Graph-Neural-Networks-with-Strong-Lottery-Tickets"><a href="#Multicoated-and-Folded-Graph-Neural-Networks-with-Strong-Lottery-Tickets" class="headerlink" title="Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets"></a>Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03236">http://arxiv.org/abs/2312.03236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louivalley/slt-gnn">https://github.com/louivalley/slt-gnn</a></li>
<li>paper_authors: Jiale Yan, Hiroaki Ito, Ãngel LÃ³pez GarcÃ­a-Arias, Yasuyuki Okoshi, Hikari Otsuka, Kazushi Kawamura, Thiem Van Chu, Masato Motomura</li>
<li>For: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨SLTHï¼ˆå¼ºå¤§æŠ½ç­‹å‡è®¾ï¼‰åœ¨æ·±åº¦Graph Neural Networksï¼ˆGNNsï¼‰ä¸­çš„åº”ç”¨ï¼Œä»¥æé«˜ç²¾åº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚* Methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šå±‚ææ–™æ©æ¨¡ï¼ˆM-Supï¼‰scalar pruning maskæ–¹æ³•ï¼Œå¹¶æå‡ºäº†é€‚åº”æ€§è°ƒæ•´çš„è®¾å®šç­–ç•¥ï¼Œä»¥å®ç°åœ¨æ·±åº¦GNNsä¸­çš„ç²¾åº¦å’Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚* Results: æœ¬ç ”ç©¶åœ¨Open Graph Benchmarkï¼ˆOGBï¼‰ç­‰å¤šä¸ª datasetä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºäº†SLTH-based GNNså¯ä»¥å®ç°é«˜ç²¾åº¦ã€ç«äº‰æ€§å’Œé«˜å†…å­˜æ•ˆç‡ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—è¾¾98.7%ã€‚<details>
<summary>Abstract</summary>
The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of high-performing subnetworks within a randomly initialized model, discoverable through pruning a convolutional neural network (CNN) without any weight training. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH from CNNs to shallow graph neural networks (GNNs). However, discrepancies persist when comparing baseline models with learned dense weights. Additionally, there remains an unexplored area in applying SLTH to deeper GNNs, which, despite delivering improved accuracy with additional layers, suffer from excessive memory requirements. To address these challenges, this work utilizes Multicoated Supermasks (M-Sup), a scalar pruning mask method, and implements it in GNNs by proposing a strategy for setting its pruning thresholds adaptively. In the context of deep GNNs, this research uncovers the existence of untrained recurrent networks, which exhibit performance on par with their trained feed-forward counterparts. This paper also introduces the Multi-Stage Folding and Unshared Masks methods to expand the search space in terms of both architecture and parameters. Through the evaluation of various datasets, including the Open Graph Benchmark (OGB), this work establishes a triple-win scenario for SLTH-based GNNs: by achieving high sparsity, competitive performance, and high memory efficiency with up to 98.7\% reduction, it demonstrates suitability for energy-efficient graph processing.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¼ºå¤§çš„æŠ½å¥–ç¥¨å‡è®¾â€ï¼ˆSLTHï¼‰è¡¨æ˜äº†æ·±åº¦å­¦ä¹ ä¸­çš„é«˜æ€§èƒ½å­ç½‘ç»œï¼Œå¯ä»¥é€šè¿‡éšæœºåˆå§‹åŒ–çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ— éœ€ä»»ä½•è®­ç»ƒæ¥å‘ç°ã€‚ä¸€ recent study called Untrained GNNs Ticketsï¼ˆUGTï¼‰æ‰©å±•äº† SLTH åˆ° shallow graph neural networksï¼ˆGNNsï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ¯”è¾ƒåŸºeline model ä¸ learned dense weights æ—¶ï¼Œ still æœ‰å·®å¼‚å­˜åœ¨ã€‚æ­¤å¤–ï¼Œ deeper GNNs è¿˜å­˜åœ¨ excessive memory requirements çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™ä¸ªå·¥ä½œä½¿ç”¨ MultiCoated SuperMasksï¼ˆM-Supï¼‰ï¼Œä¸€ç§æ•°å€¼é®ç‘•æ³•ï¼Œå¹¶å°†å…¶å®ç°åœ¨ GNNs ä¸­ã€‚åœ¨ deep GNNs çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™ä¸ªç ”ç©¶å‘ç°äº†æœªè®­ç»ƒçš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå®ƒä»¬åœ¨ä¸è®­ç»ƒ feed-forward å¯¹åº”çš„è¡¨ç°ç›¸ä¼¼ã€‚è¿™ä¸ª paper è¿˜æå‡ºäº† Multi-Stage Folding å’Œ Unshared Masks æ–¹æ³•ï¼Œä»¥æ‰©å±•æœå¯»ç©ºé—´çš„ both architecture å’Œ parameterã€‚é€šè¿‡è¯„ä¼°å¤šä¸ª datasetï¼ŒåŒ…æ‹¬ Open Graph Benchmarkï¼ˆOGBï¼‰ï¼Œè¿™ä¸ªç ”ç©¶å»ºç«‹äº† SLTH-based GNNs çš„ triple-win scenarioï¼šå®ƒå®ç°äº†é«˜ç®€æ´æ€§ã€ç«äº‰æ€§èƒ½å’Œé«˜å†…å­˜æ•ˆç‡ï¼Œå®ç°äº†èƒ½æºæ•ˆç‡çš„graph processingã€‚
</details></li>
</ul>
<hr>
<h2 id="Deep-Multimodal-Fusion-for-Surgical-Feedback-Classification"><a href="#Deep-Multimodal-Fusion-for-Surgical-Feedback-Classification" class="headerlink" title="Deep Multimodal Fusion for Surgical Feedback Classification"></a>Deep Multimodal Fusion for Surgical Feedback Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03231">http://arxiv.org/abs/2312.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafal Kocielnik, Elyssa Y. Wong, Timothy N. Chu, Lydia Lin, De-An Huang, Jiayun Wang, Anima Anandkumar, Andrew J. Hung</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯ automatize the annotation of real-time contextual surgical feedback at scale.</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†å¤šç§æ¨¡å¼çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥ç±»å‹åŒ»å­¦åé¦ˆï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡å¼ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼Œå…ˆå•ç‹¬è®­ç»ƒæ¯ç§æ¨¡å¼ï¼Œç„¶åå°†å®ƒä»¬ JOINTLY è®­ç»ƒã€‚</li>
<li>results: æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†ç±»æ–¹æ³•å¯ä»¥è¾¾åˆ° AUC å€¼ Between 71.5 and 77.6ï¼Œå¹¶ä¸”å°†äº”ä¸ªç±»åˆ«çš„åŒ»å­¦åé¦ˆåˆ†ç±»ä¸º â€œAnatomicâ€, â€œTechnicalâ€, â€œProceduralâ€, â€œPraiseâ€ å’Œ â€œVisual Aidâ€ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ä½¿ç”¨é«˜è´¨é‡çš„æ‰‹åŠ¨è¯‘æ–‡å›å¿«å¯ä»¥æé«˜ AUC å€¼è‡³ Between 76.5 and 96.2ã€‚<details>
<summary>Abstract</summary>
Quantification of real-time informal feedback delivered by an experienced surgeon to a trainee during surgery is important for skill improvements in surgical training. Such feedback in the live operating room is inherently multimodal, consisting of verbal conversations (e.g., questions and answers) as well as non-verbal elements (e.g., through visual cues like pointing to anatomic elements). In this work, we leverage a clinically-validated five-category classification of surgical feedback: "Anatomic", "Technical", "Procedural", "Praise" and "Visual Aid". We then develop a multi-label machine learning model to classify these five categories of surgical feedback from inputs of text, audio, and video modalities. The ultimate goal of our work is to help automate the annotation of real-time contextual surgical feedback at scale. Our automated classification of surgical feedback achieves AUCs ranging from 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show that high-quality manual transcriptions of feedback audio from experts improve AUCs to between 76.5 and 96.2, which demonstrates a clear path toward future improvements. Empirically, we find that the Staged training strategy, with first pre-training each modality separately and then training them jointly, is more effective than training different modalities altogether. We also present intuitive findings on the importance of modalities for different feedback categories. This work offers an important first look at the feasibility of automated classification of real-world live surgical feedback based on text, audio, and video modalities.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœºæ‰‹æœ¯åŒ»ç”Ÿå¯¹å­¦å‘˜çš„å®æ—¶åé¦ˆæ˜¯é‡è¦çš„ï¼Œä»¥ä¾¿æé«˜æ‰‹æœ¯åŸ¹è®­æŠ€èƒ½ã€‚è¿™ç§åé¦ˆåœ¨å®é™…æ“ä½œå®¤ä¸­æ˜¯å¤šæ¨¡å¼çš„ï¼ŒåŒ…æ‹¬è¯­éŸ³å¯¹è¯ï¼ˆå¦‚é—®é¢˜å’Œç­”æ¡ˆï¼‰ä»¥åŠéè¯­è¨€å…ƒç´ ï¼ˆå¦‚è§†è§‰æŒ‡ç¤ºå™¨ï¼‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸¥æ ¼éªŒè¯çš„äº”ç±»ç±»åˆ«æ³•å¯¹æ‰‹æœ¯åé¦ˆè¿›è¡Œåˆ†ç±»ï¼šâ€œè§£å‰–å­¦â€ã€â€œæŠ€æœ¯â€ã€â€œè¿‡ç¨‹â€ã€â€œèµèµâ€å’Œâ€œè§†è§‰å¼•å¯¼â€ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¤šæ ‡ç­¾æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºä»æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘æ¨¡å¼çš„è¾“å…¥ä¸­åˆ†ç±»è¿™äº›äº”ç±»ç±»åˆ«çš„æ‰‹æœ¯åé¦ˆã€‚æˆ‘ä»¬çš„è‡ªåŠ¨åˆ†ç±»æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„æ¨¡å¼ä¸‹è¾¾åˆ°AUCå€¼åœ¨71.5%åˆ°77.6%ä¹‹é—´ï¼Œè€Œå°†å¤šä¸ªæ¨¡å¼èåˆå¯ä»¥æé«˜æ€§èƒ½çš„3.1%ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä»ä¸“å®¶æ‰‹åŠ¨æŠ„å†™åé¦ˆéŸ³é¢‘çš„é«˜è´¨é‡æ‰‹åŠ¨è¯‘å½•å¯ä»¥æé«˜AUCå€¼åœ¨76.5%åˆ°96.2%ä¹‹é—´ï¼Œè¿™è¡¨æ˜äº†æœªæ¥å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›çš„é“è·¯ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œé¢„å…ˆåœ¨æ¯ä¸ªæ¨¡å¼ä¸Šå•ç‹¬é¢„è®­ï¼Œç„¶åå°†å…¶ JOINTLYè®­ç»ƒæ˜¯æ›´æœ‰æ•ˆçš„ï¼Œè€Œä¸”æˆ‘ä»¬è¿˜å‘ç°ä¸åŒçš„åé¦ˆç±»åˆ«å¯¹ä¸åŒçš„æ¨¡å¼å…·æœ‰ä¸åŒçš„é‡è¦æ€§ã€‚è¿™é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–å®æ—¶Contextualæ‰‹æœ¯åé¦ˆçš„åˆ†ç±»æä¾›äº†é‡è¦çš„é¦–æ¬¡ Investigationã€‚
</details></li>
</ul>
<hr>
<h2 id="SDSRA-A-Skill-Driven-Skill-Recombination-Algorithm-for-Efficient-Policy-Learning"><a href="#SDSRA-A-Skill-Driven-Skill-Recombination-Algorithm-for-Efficient-Policy-Learning" class="headerlink" title="SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning"></a>SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03216">http://arxiv.org/abs/2312.03216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ericjiang18/sdsra">https://github.com/ericjiang18/sdsra</a></li>
<li>paper_authors: Eric H. Jiang, Andrew Lizarraga</li>
<li>for: æé«˜å¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¸­çš„æœ€å¤§Entropyæ•ˆç‡</li>
<li>methods: ä½¿ç”¨Skill-Driven Skill Recombination Algorithm (SDSRA)ï¼Œä¸€ç§æ–°å‹çš„åè°ƒæœç´¢æ¡†æ¶ï¼Œå®ç°æ›´é«˜æ•ˆçš„æœ€å¤§Entropyæ•ˆç‡</li>
<li>results: SDSRAæ¯”ä¼ ç»Ÿçš„Soft Actor-Critic (SAC)ç®—æ³•æ›´å¿«åœ° convergesï¼Œå¹¶ç”Ÿæˆäº†æ”¹è¿›çš„ç­–ç•¥ï¼Œåœ¨å¤šç§å¤æ‚å’Œå¤šæ ·çš„ benchmark ä¸­å±•ç°å‡ºäº†remarkableçš„é€‚åº”æ€§å’Œæ€§èƒ½<details>
<summary>Abstract</summary>
In this paper, we introduce a novel algorithm - the Skill-Driven Skill Recombination Algorithm (SDSRA) - an innovative framework that significantly enhances the efficiency of achieving maximum entropy in reinforcement learning tasks. We find that SDSRA achieves faster convergence compared to the traditional Soft Actor-Critic (SAC) algorithm and produces improved policies. By integrating skill-based strategies within the robust Actor-Critic framework, SDSRA demonstrates remarkable adaptability and performance across a wide array of complex and diverse benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ç®—æ³•â€”â€”æŠ€èƒ½é©±åŠ¨æŠ€èƒ½ recombinationç®—æ³•ï¼ˆSDSRAï¼‰â€”â€”ä¸€ç§åˆ›æ–°çš„æ¡†æ¶ï¼Œå¯ä»¥åœ¨å›å½’å­¦ä¹ ä»»åŠ¡ä¸­æé«˜æœ€å¤§Entropyçš„æ•ˆç‡ã€‚æˆ‘ä»¬å‘ç°SDSRAæ¯”ä¼ ç»Ÿçš„Soft Actor-Criticï¼ˆSACï¼‰ç®—æ³•æ›´å¿«åœ° convergeså’Œç”Ÿæˆæ›´å¥½çš„ç­–ç•¥ã€‚é€šè¿‡åœ¨Robust Actor-Criticæ¡†æ¶ä¸­ Ğ¸Ğ½Ñ‚ĞµGRATEæŠ€èƒ½basedç­–ç•¥ï¼ŒSDSRAåœ¨å¤šç§å¤æ‚å’Œå¤šæ ·çš„æ ‡å‡†åº•ä¸‹è¡¨ç°å‡ºäº†remarkableçš„é€‚åº”æ€§å’Œæ€§èƒ½ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/cs.AI_2023_12_06/" data-id="clq0ru6py008jto880j9w7z85" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/cs.CL_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T11:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/06/cs.CL_2023_12_06/">cs.CL - 2023-12-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Collaboration-or-Corporate-Capture-Quantifying-NLPâ€™s-Reliance-on-Industry-Artifacts-and-Contributions"><a href="#Collaboration-or-Corporate-Capture-Quantifying-NLPâ€™s-Reliance-on-Industry-Artifacts-and-Contributions" class="headerlink" title="Collaboration or Corporate Capture? Quantifying NLPâ€™s Reliance on Industry Artifacts and Contributions"></a>Collaboration or Corporate Capture? Quantifying NLPâ€™s Reliance on Industry Artifacts and Contributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03912">http://arxiv.org/abs/2312.03912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Will Aitken, Mohamed Abdalla, Karen Rudie, Catherine Stinson</li>
<li>for: This paper investigates the reliance on industry for NLP publications, specifically looking at the citations of industry artifacts and contributions in papers presented at EMNLP 2022.</li>
<li>methods: The paper surveys 100 papers published at EMNLP 2022 to determine the frequency of citations of industry artifacts and contributions.</li>
<li>results: The paper finds that there is a substantial reliance on industry for NLP publications, with citations of industry artifacts and contributions being at least three times greater than industry publication rates per year. The paper discusses two possible perspectives on this finding: 1) collaboration with industry is still collaboration, even in the absence of an alternative, or 2) free NLP inquiry has been captured by the motivations and research direction of private corporations.<details>
<summary>Abstract</summary>
The advent of transformers, higher computational budgets, and big data has engendered remarkable progress in Natural Language Processing (NLP). Impressive performance of industry pre-trained models has garnered public attention in recent years and made news headlines. That these are industry models is noteworthy. Rarely, if ever, are academic institutes producing exciting new NLP models. Using these models is critical for competing on NLP benchmarks and correspondingly to stay relevant in NLP research. We surveyed 100 papers published at EMNLP 2022 to determine whether this phenomenon constitutes a reliance on industry for NLP publications.   We find that there is indeed a substantial reliance. Citations of industry artifacts and contributions across categories is at least three times greater than industry publication rates per year. Quantifying this reliance does not settle how we ought to interpret the results. We discuss two possible perspectives in our discussion: 1) Is collaboration with industry still collaboration in the absence of an alternative? Or 2) has free NLP inquiry been captured by the motivations and research direction of private corporations?
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œ transformers çš„å‡ºç°ï¼Œæ›´é«˜çš„è®¡ç®—é¢„ç®—å’Œå¤§æ•°æ®ï¼Œå·²ç»å¯¼è‡´è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸåšå‡ºäº†å¾ˆå¤§çš„è¿›æ­¥ã€‚åœ¨æœ€è¿‘å‡ å¹´ï¼Œä¸šç•Œé¢„è®­æ¨¡å‹çš„å‡ºè‰²è¡¨ç°å—åˆ°äº†å…¬ä¼—çš„å…³æ³¨ï¼Œå¹¶åœ¨æ–°é—»å¤´æ¡ä¸Šå æ®äº†ä¸»è¦åœ°ä½ã€‚è¿™äº›æ¨¡å‹æ˜¯ä¸šç•Œæ¨¡å‹ï¼Œè¿™æ˜¯å€¼å¾—æ³¨æ„çš„ã€‚åœ¨å­¦æœ¯ç•Œrarely, if ever, å‡ºç°äº†æ–°çš„NLPæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ EMNLP 2022 å¹´åº¦ä¼šè®®ä¸Šç¿»è¯‘äº† 100 ç¯‡è®ºæ–‡ï¼Œä»¥ç¡®å®šè¿™ç§ç°è±¡æ˜¯å¦å­˜åœ¨ã€‚æˆ‘ä»¬å‘ç°ï¼Œå®é™…ä¸Šæœ‰ä¸€å®šçš„ä¾èµ–ã€‚ä¸šç•Œæ–‡çŒ®å’Œè´¡çŒ®çš„å¼•ç”¨ frequency è‡³å°‘ä¸‰å€äºæ¯å¹´çš„ä¸šç•Œå‘è¡¨ç‡ã€‚é‡åŒ–è¿™ç§ä¾èµ–å¹¶ä¸èƒ½è§£é‡Šæˆ‘ä»¬åº”è¯¥å¦‚ä½•è§£é‡Šç»“æœã€‚æˆ‘ä»¬åœ¨è®¨è®ºä¸­æå‡ºäº†ä¸¤ä¸ªå¯èƒ½çš„è§†è§’ï¼š1ï¼‰åœ¨æ²¡æœ‰å¤‡ç”¨çš„æƒ…å†µä¸‹ï¼Œä¸ä¸šç•Œåˆä½œä»ç„¶æ˜¯åˆä½œå—ï¼Ÿæˆ–2ï¼‰è‡ªç§å…¬å¸çš„åŠ¨æœºå’Œç ”ç©¶æ–¹å‘å·²ç»æŠ“ä½äº†è‡ªç”±NLPç ”ç©¶çš„ä¸»æµï¼Ÿâ€
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Optimality-of-Word-Lengths"><a href="#Revisiting-the-Optimality-of-Word-Lengths" class="headerlink" title="Revisiting the Optimality of Word Lengths"></a>Revisiting the Optimality of Word Lengths</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03897">http://arxiv.org/abs/2312.03897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tpimentelms/optimality-of-word-lengths">https://github.com/tpimentelms/optimality-of-word-lengths</a></li>
<li>paper_authors: Tiago Pimentel, Clara Meister, Ethan Gotlieb Wilcox, Kyle Mahowald, Ryan Cotterell</li>
<li>for: Zipf (1935) çš„ç ”ç©¶ç›®çš„æ˜¯æå‡ºè¯å½¢å…·æœ‰æœ€å°é€šä¿¡æˆæœ¬çš„ä¼˜åŒ–ã€‚</li>
<li>methods: è¿™ç§ç ”ç©¶ä½¿ç”¨ Piantadosi et al. (2011) æå‡ºçš„é€šä¿¡æˆæœ¬ç†è®ºï¼ˆChannel Capacity Hypothesisï¼ŒCCHï¼‰ï¼Œå¹¶æå‡ºä¸€ç§æ–°çš„ derivation æ¥æœ€å°åŒ– CCH çš„æˆæœ¬ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒZipf çš„å‡è®¾åœ¨13ç§è¯­è¨€å’Œå¤šç§å®éªŒè®¾ç½®ä¸‹ï¼Œword length æ›´å¥½åœ°é¢„æµ‹äº† frequencyã€‚æ­¤å¤–ï¼Œå½“ä½¿ç”¨æ›´å¥½çš„è¯­è¨€æ¨¡å‹æ¥ä¼°ç®— expectation å’Œ variance-to-mean ratio æ—¶ï¼Œword length çš„é¢„æµ‹å˜å¾—æ›´å·®ã€‚è¿™äº›ç»“æœæ”¯æŒ Zipf çš„é•¿æœŸå‡è®¾ã€‚<details>
<summary>Abstract</summary>
Zipf (1935) posited that wordforms are optimized to minimize utterances' communicative costs. Under the assumption that cost is given by an utterance's length, he supported this claim by showing that words' lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance's information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word's length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.'s derivation does not minimize CCH's cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH's cost. Under this method, we find that a language's word lengths should instead be proportional to the surprisal's expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf's, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal's expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf's longstanding hypothesis holds.
</details>
<details>
<summary>æ‘˜è¦</summary>
zipf (1935) æå‡ºäº† Wordforms æ˜¯ä¸ºæœ€å°åŒ–è¯­éŸ³äº¤æµæˆæœ¬è€Œä¼˜åŒ–çš„å‡è®¾ã€‚ å‡è®¾äº¤æµæˆæœ¬æ˜¯è¯è¯­é•¿åº¦ï¼Œä»–é€šè¿‡æ˜¾ç¤ºå•è¯é•¿åº¦ä¸å…¶é¢‘ç‡çš„ç›¸å¯¹å…³ç³»æ¥æ”¯æŒè¿™ä¸€ç‚¹ã€‚ communicative cost å¯ä»¥ç”¨ä¸åŒçš„æ–¹å¼æ¥æ“ä½œåŒ–ã€‚  piantadosi ç­‰ (2011) æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³å°† cost å®šä¹‰ä¸ºè¯­éŸ³ä¿¡å·å’Œæ¸ é“ capacities ä¹‹é—´çš„è·ç¦»ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œç§°ä¹‹ä¸ºé€šé“å®¹é‡å‡è®¾ (CCH)ã€‚ following è¿™ç§é€»è¾‘ï¼Œä»–ä»¬ then proposed ä¸€ä¸ªè¯è¯­çš„é•¿åº¦åº”è¯¥ä¸å…¶åœ¨è¯­è¨€ä¸Šçš„é¢‘ç‡ç›¸å¯¹å…³ç³»æˆæ­£æ¯”ã€‚ åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬å‘ç° piantadosi ç­‰çš„ derivation ä¸èƒ½å‡å°‘ CCH çš„æˆæœ¬ï¼Œè€Œæ˜¯ä¸€ä¸ªä¸‹ç•Œï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º CCH-lowerã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ derivationï¼Œå»ºè®®ä¸€ç§æ”¹è¿›çš„æ–¹æ³•æ¥å‡å°‘ CCH çš„æˆæœ¬ã€‚ æ ¹æ®è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å‘ç°ä¸€ä¸ªè¯­è¨€ä¸­çš„å•è¯é•¿åº¦åº”è¯¥ä¸å…¶é¢„æœŸçš„Surprisal ï¼ˆè´Ÿå¯¹æ•°æ¦‚ç‡åœ¨è¯­è¨€ä¸Šçš„ç›¸å¯¹å…³ç³»ï¼‰æˆæ­£æ¯”ï¼ŒåŠ ä¸Šå…¶å‡å€¼ä¸æ ‡å‡†å·®çš„æ¯”ç‡ã€‚ å®éªŒallyï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†è¿™ä¸‰ç§äº¤æµæˆæœ¬å‡½æ•°ï¼š zipf çš„ã€ CCH-lower å’Œ CCHã€‚ åœ¨ 13 ç§è¯­è¨€å’Œå¤šç§å®éªŒè®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬å‘ç° length æ˜¯é¢‘ç‡æ›´å¥½åœ°é¢„æµ‹çš„ã€‚ äº‹å®ä¸Šï¼Œå½“ Surprisal çš„é¢„æœŸã€æˆ–è€…é¢„æœŸåŠ ä¸Šå‡å€¼ä¸æ ‡å‡†å·®çš„æ¯”ç‡ï¼Œä½¿ç”¨æ›´å¥½çš„è¯­è¨€æ¨¡å‹æ¥ä¼°è®¡ï¼Œä¼šå¯¼è‡´å•è¯é•¿åº¦é¢„æµ‹æ›´å·®ã€‚ æˆ‘ä»¬è®¤ä¸ºè¿™äº›ç»“æœæ˜¯è¯æ˜ zipf çš„é•¿æœŸå‡è®¾çš„è¯æ®ã€‚
</details></li>
</ul>
<hr>
<h2 id="PROMISE-A-Framework-for-Model-Driven-Stateful-Prompt-Orchestration"><a href="#PROMISE-A-Framework-for-Model-Driven-Stateful-Prompt-Orchestration" class="headerlink" title="PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration"></a>PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03699">http://arxiv.org/abs/2312.03699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenyuan Wu, Jasmin Heierli, Max Meisterhans, Adrian Moser, Andri FÃ¤rber, Mateusz Dolata, Elena Gavagnin, Alexandre de Spindler, Gerhard Schwabe</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ç§æ¡†æ¶ï¼Œå¸®åŠ©å¼€å‘è€…åœ¨ä¿¡æ¯ç³»ç»Ÿä¸­å®ç°å¤æ‚çš„è¯­è¨€åŸºäºäº¤äº’ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨çŠ¶æ€æœºå™¨æ¨¡å‹æ¦‚å¿µï¼Œå®ç°æ¨¡å‹é©±åŠ¨ã€åŠ¨æ€æç¤ºç¼–æ’ï¼Œä»¥æ§åˆ¶è¯­è¨€æ¨¡å‹çš„è¡Œä¸ºã€‚</li>
<li>results: æˆ‘ä»¬åœ¨åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿä¸­åº”ç”¨PROMISEæ¡†æ¶ï¼Œå¹¶ demonstartedå…¶èƒ½å¤Ÿå¤„ç†å¤æ‚äº¤äº’æƒ…å†µã€‚<details>
<summary>Abstract</summary>
The advent of increasingly powerful language models has raised expectations for language-based interactions. However, controlling these models is a challenge, emphasizing the need to be able to investigate the feasibility and value of their application. We present PROMISE, a framework that facilitates the development of complex language-based interactions with information systems. Its use of state machine modeling concepts enables model-driven, dynamic prompt orchestration across hierarchically nested states and transitions. This improves the control of the behavior of language models and thus enables their effective and efficient use. We show the benefits of PROMISE in the context of application scenarios within health information systems and demonstrate its ability to handle complex interactions.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œè¯­è¨€æ¨¡å‹çš„å¢å¼ºåŠ›é‡å·²ç»æé«˜äº†è¯­è¨€åŸºäºäº¤äº’çš„æœŸæœ›ã€‚ç„¶è€Œï¼Œæ§åˆ¶è¿™äº›æ¨¡å‹æ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†éœ€è¦èƒ½å¤Ÿè¯„ä¼°å…¶å¯è¡Œæ€§å’Œä»·å€¼ã€‚æˆ‘ä»¬æå‡ºäº†PROMISEæ¡†æ¶ï¼Œå®ƒä½¿ç”¨çŠ¶æ€æœºåˆ¶æ¨¡å‹çš„æ¦‚å¿µæ¥å®ç°è¯­è¨€æ¨¡å‹çš„åŠ¨æ€æç¤ºç®¡ç†ã€‚è¿™äº›ç®¡ç†æŠ€æœ¯å¯ä»¥åœ¨å±‚æ¬¡ç»“æ„ä¸­è¿›è¡Œæ¨¡å‹é©±åŠ¨çš„çŠ¶æ€å’Œè½¬ç§»æ§åˆ¶ï¼Œä»è€Œæé«˜è¯­è¨€æ¨¡å‹çš„æ§åˆ¶èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨åŒ»ç–—ä¿¡æ¯ç³»ç»Ÿä¸­åº”ç”¨PROMISEï¼Œå¹¶è¯æ˜å®ƒå¯ä»¥å¤„ç†å¤æ‚äº¤äº’ã€‚â€Note that Simplified Chinese is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-and-Mitigating-Discrimination-in-Language-Model-Decisions"><a href="#Evaluating-and-Mitigating-Discrimination-in-Language-Model-Decisions" class="headerlink" title="Evaluating and Mitigating Discrimination in Language Model Decisions"></a>Evaluating and Mitigating Discrimination in Language Model Decisions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03689">http://arxiv.org/abs/2312.03689</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared Kaplan, Deep Ganguli</li>
<li>For: The paper aims to evaluate the potential discriminatory impact of language models (LMs) in a wide range of use cases, including hypothetical scenarios where they have not yet been deployed.* Methods: The authors use a method that involves generating a wide array of potential prompts that decision-makers may input into an LM, systematically varying the demographic information in each prompt, and applying this methodology to the Claude 2.0 model.* Results: The authors find patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied, and demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering.Here are the three points in Simplified Chinese:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å„ç§ä½¿ç”¨åœºæ™¯ä¸­çš„å¯èƒ½æ€§æ­§è§†å½±å“ï¼ŒåŒ…æ‹¬å°šæœªéƒ¨ç½²çš„ Ğ³Ğ¸Ğ¿Ğ¾Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ…åœºæ™¯ã€‚</li>
<li>methods: ä½œè€…ä½¿ç”¨ä¸€ç§æ–¹æ³•ï¼Œå³ç”Ÿæˆå„ç§å¯èƒ½çš„å†³ç­–è€…è¾“å…¥è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æç¤ºï¼Œå¹¶ç³»ç»Ÿåœ°å˜åŒ–æ¯ä¸ªæç¤ºä¸­çš„äººå£ä¿¡æ¯ï¼Œä»¥åº”ç”¨è¿™ç§æ–¹æ³•æ€§åˆ° Claude 2.0 æ¨¡å‹ã€‚</li>
<li>results: ä½œè€…å‘ç° Claude 2.0 æ¨¡å‹åœ¨æŸäº›åœºæ™¯ä¸­å­˜åœ¨æ­£é¢å’Œè´Ÿé¢æ­§è§†ç°è±¡ï¼Œå¹¶ç¤ºå‡ºäº†é‡‡ç”¨æç¤ºå·¥ç¨‹æ¥å‡å°‘è¿™äº›æ­§è§†çš„æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
As language models (LMs) advance, interest is growing in applying them to high-stakes societal decisions, such as determining financing or housing eligibility. However, their potential for discrimination in such contexts raises ethical concerns, motivating the need for better methods to evaluate these risks. We present a method for proactively evaluating the potential discriminatory impact of LMs in a wide range of use cases, including hypothetical use cases where they have not yet been deployed. Specifically, we use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society, and systematically vary the demographic information in each prompt. Applying this methodology reveals patterns of both positive and negative discrimination in the Claude 2.0 model in select settings when no interventions are applied. While we do not endorse or permit the use of language models to make automated decisions for the high-risk use cases we study, we demonstrate techniques to significantly decrease both positive and negative discrimination through careful prompt engineering, providing pathways toward safer deployment in use cases where they may be appropriate. Our work enables developers and policymakers to anticipate, measure, and address discrimination as language model capabilities and applications continue to expand. We release our dataset and prompts at https://huggingface.co/datasets/Anthropic/discrim-eval
</details>
<details>
<summary>æ‘˜è¦</summary>
We use an LM to generate a wide array of potential prompts that decision-makers may input into an LM, spanning 70 diverse decision scenarios across society. We systematically vary the demographic information in each prompt to identify patterns of both positive and negative discrimination in the Claude 2.0 model in select settings. Our findings reveal that the model exhibits both positive and negative discrimination in certain situations, highlighting the need for careful prompt engineering to mitigate these biases.While we do not endorse or permit the use of language models for automated decision-making in high-risk use cases, our work demonstrates techniques to significantly decrease both positive and negative discrimination. By anticipating, measuring, and addressing discrimination, our method enables developers and policymakers to safely deploy language models in appropriate use cases. We release our dataset and prompts at <https://huggingface.co/datasets/Anthropic/discrim-eval>.
</details></li>
</ul>
<hr>
<h2 id="Interpretability-Illusions-in-the-Generalization-of-Simplified-Models"><a href="#Interpretability-Illusions-in-the-Generalization-of-Simplified-Models" class="headerlink" title="Interpretability Illusions in the Generalization of Simplified Models"></a>Interpretability Illusions in the Generalization of Simplified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03656">http://arxiv.org/abs/2312.03656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Friedman, Andrew Lampinen, Lucas Dixon, Danqi Chen, Asma Ghandeharioun</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ£€éªŒä½¿ç”¨ç®€åŒ–æ¨¡å‹è¡¨ç¤ºæ–¹æ³•æ¥ç ”ç©¶æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å‡†ç¡®æ€§ã€‚</li>
<li>methods: ç ”ç©¶è€…ä½¿ç”¨äº†ç®€åŒ–å·¥å…·å¦‚å‡ ä½•çº¦åŒ–å’Œèšç±»æ¥å°†æ·±åº¦å­¦ä¹ æ¨¡å‹è½¬åŒ–ä¸ºæ›´åŠ ç®€å•çš„å½¢å¼ï¼Œç„¶åå°†è¿™äº›ç®€åŒ–å½¢å¼ä¸åŸå§‹æ¨¡å‹è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ£€éªŒå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>results: ç ”ç©¶è€…å‘ç°ï¼Œå³ä½¿ä½¿ç”¨ç®€åŒ–å½¢å¼å¯ä»¥å‡†ç¡®åœ°é¢„æµ‹è®­ç»ƒé›†ä¸Šçš„ç»“æœï¼Œä½†æ˜¯è¿™äº›ç®€åŒ–å½¢å¼åœ¨ä¸åŒçš„æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹ç»“æœå¯èƒ½ä¸å‡†ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¨¡å‹èƒ½å¤Ÿæ¶µç›–æ–°ç»“æ„æˆ–æ›´æ·±çš„æ·±åº¦æ—¶ã€‚è¿™ç§ç°è±¡å­˜åœ¨ï¼Œå³ä½¿ç®€åŒ–å½¢å¼ä¸ç›´æ¥ä¾èµ–äºè®­ç»ƒåˆ†å¸ƒã€‚<details>
<summary>Abstract</summary>
A common method to study deep learning systems is to use simplified model representations -- for example, using singular value decomposition to visualize the model's hidden states in a lower dimensional space. This approach assumes that the results of these simplified are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution -- the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits. First, we train models on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model on various out-of-distribution test sets. We find that the simplified proxies are generally less faithful out of distribution. In cases where the original model generalizes to novel structures or deeper depths, the simplified versions may fail, or generalize better. This finding holds even if the simplified representations do not directly depend on the training distribution. Next, we study a more naturalistic task: predicting the next character in a dataset of computer code. We find similar generalization gaps between the original model and simplified proxies, and conduct further analysis to investigate which aspects of the code completion task are associated with the largest gaps. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šå¸¸ä½¿ç”¨ç®€åŒ–çš„æ¨¡å‹è¡¨ç¤ºæ–¹æ³•æ¥ç ”ç©¶æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œä¾‹å¦‚ä½¿ç”¨ç‰¹å¾å€¼åˆ†è§£æ¥å¯è§†åŒ–æ¨¡å‹çš„éšè—çŠ¶æ€åœ¨ä½ç»´åº¦ç©ºé—´ä¸­ã€‚è¿™ç§æ–¹æ³•å‡è®¾ç®€åŒ–åçš„ç»“æœä¸åŸå§‹æ¨¡å‹ç›¸ç¬¦ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è§£é‡Šäº†ä¸€ä¸ªé‡è¦çš„å‡è®¾é—®é¢˜ï¼šå³ç®€åŒ–è¡¨ç¤ºå¯èƒ½åœ¨ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒä¸‹ä¸å‡†ç¡®åœ°åæ˜ æ¨¡å‹çš„è¡Œä¸ºã€‚æˆ‘ä»¬ä½¿ç”¨æ§åˆ¶çš„æ•°æ®é›†å’Œç³»ç»ŸåŒ–æ³›åŒ–åˆ†å‰²æ¥è®­ç»ƒTransformeræ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹åœ¨ Dyck å¹³è¡¡æ‹¬å·è¯­è¨€ä¸Šã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ç»´åº¦å‡å°‘å’Œèšç±»ç­‰å·¥å…·ç®€åŒ–è¿™äº›æ¨¡å‹ï¼Œå¹¶ç›´æ¥æµ‹è¯•è¿™äº›ç®€åŒ–çš„ä»£ç†æ¨¡å‹åœ¨ä¸åŒçš„å¼‚å¸¸åˆ†å¸ƒä¸Šå¦‚ä½•åŒ¹é…åŸå§‹æ¨¡å‹çš„è¡Œä¸ºã€‚æˆ‘ä»¬å‘ç°ç®€åŒ–åçš„ä»£ç†æ¨¡å‹é€šå¸¸åœ¨å¼‚å¸¸åˆ†å¸ƒä¸‹ä¸å‡†ç¡®ã€‚åœ¨æ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°æ–°ç»“æ„æˆ–æ›´æ·±çš„æ·±åº¦æ—¶ï¼Œç®€åŒ–ç‰ˆæœ¬å¯èƒ½ä¼šå¤±è´¥æˆ–æ›´å¥½åœ°æ³›åŒ–ã€‚è¿™ç§å‘ç°ä¸ä»…åœ¨ç®€åŒ–è¡¨ç¤ºä¸ç›´æ¥ä¾èµ–äºè®­ç»ƒåˆ†å¸ƒï¼Œè¿˜æœ‰è¿™ç§æƒ…å†µã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæ›´è‡ªç„¶çš„ä»»åŠ¡ï¼šé¢„æµ‹ä»£ç ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚æˆ‘ä»¬å‘ç°ç®€åŒ–åçš„ä»£ç†æ¨¡å‹å’ŒåŸå§‹æ¨¡å‹ä¹‹é—´å­˜åœ¨ç±»ä¼¼çš„æ³›åŒ–å·®å¼‚ï¼Œå¹¶è¿›è¡Œäº†è¿›ä¸€æ­¥çš„åˆ†æï¼Œä»¥ç¡®å®šä»£ç å®Œæˆä»»åŠ¡ä¸­å“ªäº›æ–¹é¢ä¸æœ€å¤§å·®å¼‚ç›¸å…³ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç»“æœæå‡ºäº†æœºåˆ¶è§£é‡Šä½¿ç”¨å·¥å…·å¦‚ç‰¹å¾å€¼åˆ†è§£æ˜¯å¦å¯é åœ°é¢„æµ‹æ¨¡å‹åœ¨æ–°æƒ…å†µä¸‹çš„è¡Œä¸ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Bias-Mitigation-through-Bias-Experts-in-Natural-Language-Understanding"><a href="#Improving-Bias-Mitigation-through-Bias-Experts-in-Natural-Language-Understanding" class="headerlink" title="Improving Bias Mitigation through Bias Experts in Natural Language Understanding"></a>Improving Bias Mitigation through Bias Experts in Natural Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03577">http://arxiv.org/abs/2312.03577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jej127/bias-experts">https://github.com/jej127/bias-experts</a></li>
<li>paper_authors: Eojin Jeon, Mingyu Lee, Juhyeong Park, Yeachan Kim, Wing-Lam Mok, SangKeun Lee</li>
<li>for: é™ä½æ•°æ®é›†ä¸­åè§çš„å½±å“ï¼Œæé«˜æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚</li>
<li>methods: ä½¿ç”¨auxiliary modelå’Œä¸»æ¨¡å‹ä¹‹é—´çš„äºŒåˆ†ç±»é¢„æµ‹å™¨ï¼ˆbias expertsï¼‰ï¼Œé€šè¿‡One-vs-Restapproachè¿›è¡Œè®­ç»ƒï¼Œä»¥æé«˜auxiliary modelçš„åè§è¯†åˆ«èƒ½åŠ›ã€‚</li>
<li>results: é€šè¿‡å®éªŒç»“æœï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•å¯ä»¥æé«˜auxiliary modelçš„åè§è¯†åˆ«èƒ½åŠ›ï¼Œå¹¶ä½¿å¾—é™ä½åè§åçš„æ¨¡å‹åœ¨ä¸åŒæ•°æ®é›†ä¸Šçš„æ€§èƒ½å¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚<details>
<summary>Abstract</summary>
Biases in the dataset often enable the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
dataset ä¸­çš„åè§ oftentimes enables the model to achieve high performance on in-distribution data, while poorly performing on out-of-distribution data. To mitigate the detrimental effect of the bias on the networks, previous works have proposed debiasing methods that down-weight the biased examples identified by an auxiliary model, which is trained with explicit bias labels. However, finding a type of bias in datasets is a costly process. Therefore, recent studies have attempted to make the auxiliary model biased without the guidance (or annotation) of bias labels, by constraining the model's training environment or the capability of the model itself. Despite the promising debiasing results of recent works, the multi-class learning objective, which has been naively used to train the auxiliary model, may harm the bias mitigation effect due to its regularization effect and competitive nature across classes. As an alternative, we propose a new debiasing framework that introduces binary classifiers between the auxiliary model and the main model, coined bias experts. Specifically, each bias expert is trained on a binary classification task derived from the multi-class classification task via the One-vs-Rest approach. Experimental results demonstrate that our proposed strategy improves the bias identification ability of the auxiliary model. Consequently, our debiased model consistently outperforms the state-of-the-art on various challenge datasets.Note: The translation is done using Google Translate, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="XAIQA-Explainer-Based-Data-Augmentation-for-Extractive-Question-Answering"><a href="#XAIQA-Explainer-Based-Data-Augmentation-for-Extractive-Question-Answering" class="headerlink" title="XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering"></a>XAIQA: Explainer-Based Data Augmentation for Extractive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03567">http://arxiv.org/abs/2312.03567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joel Stremmel, Ardavan Saeedi, Hamid Hassanzadeh, Sanjit Batra, Jeffrey Hertzberg, Jaime Murillo, Eran Halperin</li>
<li>For: The paper is written for physicians and researchers who need to query medical records to design clinical studies and understand patient medical history.* Methods: The paper introduces a novel approach called XAIQA, which generates synthetic QA pairs at scale from data naturally available in electronic health records. The method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes.* Results: The paper shows that XAIQA identifies more semantic matches and clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs, and improves the performance of GPT-4 as an extractive QA model, including on difficult questions.Hereâ€™s the information in Simplified Chinese text:* For: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†å¸®åŠ©åŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜æŸ¥è¯¢åŒ»ç–—è®°å½•ï¼Œä»¥è®¾è®¡ä¸´åºŠç ”ç©¶å’Œç†è§£æ‚£è€…åŒ»ç–—å†å²ã€‚* Methods: è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• called XAIQAï¼Œå®ƒå¯ä»¥åœ¨ç”µå­åŒ»ç–—è®°å½•ä¸­ç”Ÿæˆå¤§é‡çš„Synthetic QAå¯¹ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨åˆ†ç±»æ¨¡å‹ explainer æ¥ç”Ÿæˆå…³äºåŒ»ç–—æ¦‚å¿µçš„é—®é¢˜å’Œç­”æ¡ˆã€‚* Results: è®ºæ–‡è¡¨æ˜ï¼ŒXAIQA å¯ä»¥æ¯”ä¸¤ç§ä½¿ç”¨ sentence transformers ç”Ÿæˆ QAå¯¹çš„æ–¹æ³•æ›´å¥½åœ°æ ‡è¯† semantic match å’Œ clinical abbreviationï¼Œå¹¶ä¸”å¯ä»¥æé«˜ GPT-4 ä½œä¸ºæŠ½å–å¼ QA æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬éš¾é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Extractive question answering (QA) systems can enable physicians and researchers to query medical records, a foundational capability for designing clinical studies and understanding patient medical history. However, building these systems typically requires expert-annotated QA pairs. Large language models (LLMs), which can perform extractive QA, depend on high quality data in their prompts, specialized for the application domain. We introduce a novel approach, XAIQA, for generating synthetic QA pairs at scale from data naturally available in electronic health records. Our method uses the idea of a classification model explainer to generate questions and answers about medical concepts corresponding to medical codes. In an expert evaluation with two physicians, our method identifies $2.2\times$ more semantic matches and $3.8\times$ more clinical abbreviations than two popular approaches that use sentence transformers to create QA pairs. In an ML evaluation, adding our QA pairs improves performance of GPT-4 as an extractive QA model, including on difficult questions. In both the expert and ML evaluations, we examine trade-offs between our method and sentence transformers for QA pair generation depending on question difficulty.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæŠ½è±¡Question Answeringï¼ˆQAï¼‰ç³»ç»Ÿå¯ä»¥è®©åŒ»ç”Ÿå’Œç ”ç©¶äººå‘˜æŸ¥è¯¢åŒ»ç–—çºªå½•ï¼Œè¿™æ˜¯è®¾è®¡ä¸´åºŠè¯•éªŒå’Œç†è§£ç—…äººåŒ»ç–—å†å²çš„é‡è¦èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå»ºç«‹è¿™äº›ç³»ç»Ÿé€šå¸¸éœ€è¦ä¸“å®¶å½•åˆ›QAå¯¹ã€‚å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥è¿›è¡ŒæŠ½è±¡QAï¼Œä½†å®ƒä»¬éœ€è¦é«˜è´¨é‡çš„æ•°æ®ä½œä¸ºå…¶æ¨é—®ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼ŒXAIQAï¼Œå¯ä»¥å°†å¤§é‡çš„è‡ªç„¶å¯ç”¨æ•°æ®ä¸­çš„æ•°æ®ç”ŸæˆæˆQAå¯¹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨åŒ»ç–—æ¡ç›® explainer æ¥ç”Ÿæˆå…³äºåŒ»ç–—æ¡ç›®çš„é—®é¢˜å’Œç­”æ¡ˆã€‚åœ¨ä¸¤ä½åŒ»ç”Ÿçš„ä¸“å®¶è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¯†åˆ« $2.2\times$ æ›´å¤šçš„ semantic match å’Œ $3.8\times$ æ›´å¤šçš„åŒ»ç–—ç¼©å†™ã€‚åœ¨ ML è¯„ä¼°ä¸­ï¼Œå°†æˆ‘ä»¬ç”Ÿæˆçš„ QA å¯¹æ·»åŠ åˆ° GPT-4 ä¸­å¯ä»¥æé«˜è¿™ä¸ªæŠ½è±¡ QA æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬éš¾é—®é¢˜ã€‚åœ¨ä¸“å®¶å’Œ ML è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†æˆ‘ä»¬çš„æ–¹æ³•å’Œ sentence transformers çš„ QA å¯¹ç”Ÿæˆæ–¹æ³•ä¹‹é—´çš„è´¡çŒ®å’ŒæŠ˜å†²å…³ç³»ï¼Œå…·ä½“å–å†³äºé—®é¢˜çš„éš¾åº¦ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Holmes-Towards-Distributed-Training-Across-Clusters-with-Heterogeneous-NIC-Environment"><a href="#Holmes-Towards-Distributed-Training-Across-Clusters-with-Heterogeneous-NIC-Environment" class="headerlink" title="Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment"></a>Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03549">http://arxiv.org/abs/2312.03549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yang, Shuang Peng, Ning Sun, Fangyu Wang, Ke Tan, Fu Wu, Jiezhong Qiu, Aimin Pan<br>for: è¿™ä¸ª paper æ˜¯ä¸ºäº†æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è®­ç»ƒæ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚methods: æœ¬ paper ä½¿ç”¨äº†å½“åœ°çš„æ•°æ®å’Œæ¨¡å‹å¹³è¡ŒåŒ–ç­–ç•¥ï¼Œä»¥åŠä¸€ä¸ªæ–°çš„æ’ç¨‹æ–¹æ³•æ¥å°†ç‰¹å®šçš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™å…·æœ‰ä¸åŒç‰¹æ€§çš„ GPU è®¾å¤‡ã€‚results: æœ¬ paper çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨è€…çš„æ¡†æ¶å¯ä»¥åœ¨ä¸åŒçš„ NIC ç¯å¢ƒä¸‹è¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„ä¸»æµ LLM æ¡†æ¶æ•´åˆã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨å¤šä¸ª GPU é›†ç¾¤ä¸­çš„æ‰©å±•æ€§ä¹Ÿå¾—åˆ°äº†è¯æ˜ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) such as GPT-3, OPT, and LLaMA have demonstrated remarkable accuracy in a wide range of tasks. However, training these models can incur significant expenses, often requiring tens of thousands of GPUs for months of continuous operation. Typically, this training is carried out in specialized GPU clusters equipped with homogeneous high-speed Remote Direct Memory Access (RDMA) network interface cards (NICs). The acquisition and maintenance of such dedicated clusters is challenging. Current LLM training frameworks, like Megatron-LM and Megatron-DeepSpeed, focus primarily on optimizing training within homogeneous cluster settings. In this paper, we introduce Holmes, a training framework for LLMs that employs thoughtfully crafted data and model parallelism strategies over the heterogeneous NIC environment. Our primary technical contribution lies in a novel scheduling method that intelligently allocates distinct computational tasklets in LLM training to specific groups of GPU devices based on the characteristics of their connected NICs. Furthermore, our proposed framework, utilizing pipeline parallel techniques, demonstrates scalability to multiple GPU clusters, even in scenarios without high-speed interconnects between nodes in distinct clusters. We conducted comprehensive experiments that involved various scenarios in the heterogeneous NIC environment. In most cases, our framework achieves performance levels close to those achievable with homogeneous RDMA-capable networks (InfiniBand or RoCE), significantly exceeding training efficiency within the pure Ethernet environment. Additionally, we verified that our framework outperforms other mainstream LLM frameworks under heterogeneous NIC environment in terms of training efficiency and can be seamlessly integrated with them.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¦‚GPT-3ã€OPTå’ŒLLaMAåœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†è®­ç»ƒè¿™äº›æ¨¡å‹å¯èƒ½ä¼šå‡ºç°å·¨å¤§æˆæœ¬ï¼Œé€šå¸¸éœ€è¦æ•°åƒä¸ªGPUæ•°æ®ä¸­å¿ƒ Months of continuous operationã€‚é€šå¸¸ï¼Œè¿™äº›è®­ç»ƒæ˜¯åœ¨ç‰¹æ®Šçš„GPUé›†ç¾¤ä¸­è¿›è¡Œï¼Œè¯¥é›†ç¾¤æ˜¯é…å¤‡åŒæ­¥é«˜é€ŸRemote Direct Memory Accessï¼ˆRDMAï¼‰ç½‘ç»œå¡ï¼ˆNICï¼‰ã€‚è·å–å’Œç»´æŠ¤è¿™äº›ä¸“é—¨çš„é›†ç¾¤æ˜¯å…·æœ‰æŒ‘æˆ˜ã€‚ç›®å‰çš„LLMè®­ç»ƒæ¡†æ¶ï¼Œå¦‚Megatron-LMå’ŒMegatron-DeepSpeedï¼Œä¸“æ³¨åœ¨åŒæ­¥è®­ç»ƒHomogeneous cluster Settingä¸­ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥Holmesï¼Œä¸€ä¸ªLLMè®­ç»ƒæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ä½¿ç”¨äº†ç²¾å¿ƒè®¾è®¡çš„æ•°æ®å’Œæ¨¡å‹å¹³è¡ŒåŒ–ç­–ç•¥åœ¨ hetroogeneous NICç¯å¢ƒä¸­ã€‚æˆ‘ä»¬çš„ä¸»è¦æŠ€æœ¯è´¡çŒ®åœ¨äºä¸€ç§æ–°çš„æ’ç¨‹æ–¹æ³•ï¼Œå°†åœ¨LLMè®­ç»ƒä¸­åˆ†é…ç‰¹å®šçš„computational taskletåˆ°ç‰¹å®šçš„GPUè£…ç½®åŸºäºè¯¥è£…ç½®çš„è¿æ¥NICçš„ç‰¹æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ææ¡ˆçš„æ¡†æ¶ï¼Œä½¿ç”¨ç®¡é“å¹³è¡ŒæŠ€æœ¯ï¼Œå¯ä»¥åœ¨å¤šä¸ªGPUé›†ç¾¤ä¸­æ‰©å±•ï¼Œç”šè‡³åœ¨æ²¡æœ‰é«˜é€ŸInterconnects between nodes in distinct clustersçš„æƒ…å†µä¸‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬ä¸åŒçš„æƒ…å†µåœ¨hetroogeneous NICç¯å¢ƒä¸­ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥åœ¨åŒæ­¥è®­ç»ƒä¸­ achievable with homogeneous RDMA-capable networksï¼ˆInfiniBand or RoCEï¼‰æ°´å¹³ï¼Œsignificantly exceeding training efficiency within the pure Ethernet environmentã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬éªŒè¯äº†æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä¸ä¸»æµLLMæ¡†æ¶åœ¨hetroogeneous NICç¯å¢ƒä¸­ä¼˜åŒ–è®­ç»ƒæ•ˆç‡ï¼Œå¹¶ä¸”å¯ä»¥ä¸å®ƒä»¬é›†æˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Sig-Networks-Toolkit-Signature-Networks-for-Longitudinal-Language-Modelling"><a href="#Sig-Networks-Toolkit-Signature-Networks-for-Longitudinal-Language-Modelling" class="headerlink" title="Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling"></a>Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03523">http://arxiv.org/abs/2312.03523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talia Tseriotou, Ryan Sze-Yin Chan, Adam Tsakalidis, Iman Munire Bilal, Elena Kochkina, Terry Lyons, Maria Liakata</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æå‡ºä¸€ä¸ªå¼€æºçš„pipå®‰è£…çš„å·¥å…·å¥—ä»¶ï¼Œå«åšSig-Networksï¼Œç”¨äºé•¿æœŸè¯­è¨€æ¨¡å‹åŒ–ã€‚</li>
<li>methods: è¿™ä¸ªå·¥å…·å¥—ä»¶ä½¿ç”¨äº†ç­¾ååŸºäºç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹åœ¨æ—¶é—´ä»»åŠ¡ä¸Šå·²ç»æ˜¾ç¤ºå‡ºäº†æˆåŠŸã€‚è®ºæ–‡è¿˜åº”ç”¨å¹¶æ‰©å±•äº†å·²ç»å‘è¡¨çš„ç ”ç©¶ï¼Œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ç­¾ååŸºäºæ¨¡å‹çš„suiteã€‚è¿™äº›ç»„ä»¶å¯ä»¥ç”¨ä½œPyTorchçš„å»ºç­‘å—ï¼Œåœ¨æœªæ¥çš„æ¶æ„ä¸­ä½¿ç”¨ã€‚Sig-Networksæ”¯æŒä»»åŠ¡æ— å…³çš„æ•°æ®é›†æ’å…¥ï¼Œç®€å•çš„å‰å¤„ç† Ğ´Ğ»Ñé¡ºåºæ•°æ®ï¼Œå‚æ•°çš„çµæ´»æ€§ï¼Œè‡ªåŠ¨è°ƒæ•´å¤šç§æ¨¡å‹ã€‚</li>
<li>results: è®ºæ–‡åœ¨ä¸‰ä¸ªä¸åŒçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼ŒåŒ…æ‹¬å¿ƒç†å’¨è¯¢å¯¹è¯ã€è°£è¨€ç«‹åœºè½¬æ¢å’Œç¤¾äº¤åª’ä½“Threadä¸­çš„æƒ…ç»ªå˜åŒ–ï¼Œåœ¨æ‰€æœ‰ä¸‰ä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°äº†æœ€é«˜æ€§èƒ½æ°´å¹³ã€‚è®ºæ–‡è¿˜æä¾›äº†å¯¹æœªæ¥ä»»åŠ¡çš„æŒ‡å¯¼ã€‚<details>
<summary>Abstract</summary>
We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks on the modeled NLP tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»ä¸€ä¸ªå¼€æºã€å¯ä»¥é€šè¿‡pipå®‰è£…çš„å·¥å…·é›†ï¼Œå³Sig-Networksï¼Œè¿™æ˜¯é¦–å…ˆé‡‡ç”¨ç­¾ååŸºäºç¥ç»ç½‘ç»œæ¨¡å‹çš„è¯­è¨€æ¨¡å‹å·¥å…·é›†ã€‚æˆ‘ä»¬å°†ocusåœ¨ incorporating Signature-based Neural Network modelsï¼Œè¿™äº›æ¨¡å‹åœ¨æ—¶é—´ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æˆåŠŸã€‚æˆ‘ä»¬åº”ç”¨å¹¶æ‰©å±•äº†å·²å‘è¡¨çš„ç ”ç©¶ï¼Œæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ç­¾ååŸºäºæ¨¡å‹é›†ã€‚è¿™äº›ç»„ä»¶å¯ä»¥ç”¨ä½œPyTorchå»ºç­‘å—ï¼Œåœ¨æœªæ¥çš„å»ºç­‘ä¸­ä½¿ç”¨ã€‚Sig-Networksæ”¯æŒä»»åŠ¡æ— å…³çš„æ•°æ®é›†æ’å…¥ã€sequentialæ•°æ®é¡ºåºå¤„ç†ã€å‚æ•°çµæ´»æ€§å’Œæ¨¡å‹è‡ªåŠ¨è°ƒæ•´ã€‚æˆ‘ä»¬åœ¨ä¸‰ç§ä¸åŒçš„è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šï¼ˆè¾…å¯¼å¯¹è¯ã€è°£è¨€ç«‹åœºè½¬æ¢å’Œç¤¾äº¤åª’ä½“çº¿ä¸Šæƒ…ç»ªå˜åŒ–ï¼‰è¿›è¡Œäº†è¯•éªŒï¼Œå¹¶è¾¾åˆ°äº†å½“å‰æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬é‡Šæ”¾äº†å·¥å…·é›†ä½œä¸ºPyTorchåŒ…ï¼Œå¹¶æä¾›äº†å¼•å¯¼è§†é¢‘ã€Gitå­˜å‚¨åº“å’Œç¤ºä¾‹ç¬”è®°æœ¬ã€‚
</details></li>
</ul>
<hr>
<h2 id="Exploring-Answer-Information-Methods-for-Question-Generation-with-Transformers"><a href="#Exploring-Answer-Information-Methods-for-Question-Generation-with-Transformers" class="headerlink" title="Exploring Answer Information Methods for Question Generation with Transformers"></a>Exploring Answer Information Methods for Question Generation with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03483">http://arxiv.org/abs/2312.03483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talha Chafekar, Aafiya Hussain, Grishma Sharma, Deepak Sharma</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨ä¸åŒæ–¹æ³•åœ¨æä¾›ç›®æ ‡ç­”æ¡ˆä½œä¸ºè¾“å…¥æ—¶ï¼Œå¯¹ RNN æ¨¡å‹çš„æ•ˆæœã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸‰ç§æ–¹æ³•å’Œå…¶ç»„åˆï¼ŒåŒ…æ‹¬ç­”æ¡ˆæç¤ºã€ä½¿ç”¨è‡ªå®šä¹‰äº§å“æ–¹æ³•ã€ä½¿ç”¨ç­”æ¡ˆåµŒå…¥å’Œè§£ç å™¨è¾“å‡ºã€é€‰æ‹©è¾“å…¥æ®µè½ä¸­çš„ç­”æ¡ˆç›¸å…³ä¿¡æ¯ï¼Œä»¥åŠä½¿ç”¨ç‹¬ç«‹çš„è·¨æ³¨æ„åŠ›å—åœ¨è§£ç å™¨ä¸­æ³¨æ„ç­”æ¡ˆã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°ï¼Œä¸å«ä»»ä½•å…¶ä»–æ¨¡å¼çš„ç­”æ¡ˆæç¤ºæ–¹æ³•å¯ä»¥è·å¾—æœ€ä½³åˆ† across ROUGE å’Œ Meteor è¯„ä»·æŒ‡æ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—ç”Ÿæˆé—®é¢˜ä¸­æ˜¯å¦åŒ…å«ç›¸åŒçš„ç­”æ¡ˆã€‚<details>
<summary>Abstract</summary>
There has been a lot of work in question generation where different methods to provide target answers as input, have been employed. This experimentation has been mostly carried out for RNN based models. We use three different methods and their combinations for incorporating answer information and explore their effect on several automatic evaluation metrics. The methods that are used are answer prompting, using a custom product method using answer embeddings and encoder outputs, choosing sentences from the input paragraph that have answer related information, and using a separate cross-attention attention block in the decoder which attends to the answer. We observe that answer prompting without any additional modes obtains the best scores across rouge, meteor scores. Additionally, we use a custom metric to calculate how many of the generated questions have the same answer, as the answer which is used to generate them.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ‰å¾ˆå¤šç ”ç©¶åœ¨é—®é¢˜ç”Ÿæˆæ–¹é¢ï¼Œä½¿ç”¨ä¸åŒçš„æ–¹æ³•æä¾›ç›®æ ‡ç­”æ¡ˆä½œä¸ºè¾“å…¥ï¼Œä»¥exploreå…¶å½±å“å¤šç§è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ã€‚è¿™äº›å®éªŒä¸»è¦é’ˆå¯¹åŸºäºRNNçš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸‰ç§ä¸åŒçš„æ–¹æ³•å’Œå…¶ç»„åˆæ¥æ¨é€ç­”æ¡ˆä¿¡æ¯ï¼Œå¹¶è¯„ä¼°å®ƒä»¬å¯¹å¤šä¸ªè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡çš„å½±å“ã€‚è¿™äº›æ–¹æ³•åŒ…æ‹¬ç­”æ¡ˆæç¤ºã€ä½¿ç”¨è‡ªå®šä¹‰äº§å“æ–¹æ³•ä½¿ç­”æ¡ˆåµŒå…¥å’Œè§£ç è¾“å‡ºã€ä»è¾“å…¥æ®µè½ä¸­é€‰æ‹©å¸¦ç­”æ¡ˆç›¸å…³ä¿¡æ¯çš„å¥å­ï¼Œä»¥åŠåœ¨è§£ç å™¨ä¸­ä½¿ç”¨ç‹¬ç«‹çš„äº¤å‰æ³¨æ„åŠ›å—ï¼Œä»¥æ³¨æ„ç­”æ¡ˆã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å¼çš„ç­”æ¡ˆæç¤ºæ–¹æ³•å¯ä»¥è·å¾—æœ€å¥½çš„æ€»è¯„åˆ†å’Œé›¨äº®åˆ†æ•°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡æ¥è®¡ç®—ç”Ÿæˆé—®é¢˜ä¸­æ˜¯å¦åŒ…å«ç›¸åŒçš„ç­”æ¡ˆï¼Œå³ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆå’Œç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="AMR-Parsing-is-Far-from-Solved-GrAPES-the-Granular-AMR-Parsing-Evaluation-Suite"><a href="#AMR-Parsing-is-Far-from-Solved-GrAPES-the-Granular-AMR-Parsing-Evaluation-Suite" class="headerlink" title="AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite"></a>AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing Evaluation Suite</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03480">http://arxiv.org/abs/2312.03480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jgroschwitz/grapes">https://github.com/jgroschwitz/grapes</a></li>
<li>paper_authors: Jonas Groschwitz, Shay B. Cohen, Lucia Donatelli, Meaghan Fowlie</li>
<li>for: æœ¬ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªæŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰åˆ†æè¯„ä¼°é›†ï¼ˆGrAPESï¼‰ï¼Œç”¨äºæµ‹è¯•ç°æœ‰çš„AMRåˆ†æå™¨åœ¨å„ç§è¯­è¨€ç°è±¡ä¸Šçš„èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šç§ç°æœ‰çš„AMRåˆ†æå™¨ï¼Œå¹¶å¼€å‘äº†ä¸€äº›æ–°çš„è¯„ä¼°æŒ‡æ ‡æ¥æµ‹è¯•è¿™äº›åˆ†æå™¨çš„æ€§èƒ½ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„AMRåˆ†æå™¨åœ¨ä¸€äº›è¯­è¨€ç°è±¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨å…¶ä»–æƒ…å†µä¸‹ä»ç„¶å­˜åœ¨è¾ƒå¤šçš„é”™è¯¯ï¼Œç‰¹åˆ«æ˜¯åœ¨èŠ‚ç‚¹æ ‡ç­¾å’Œå›¾ç»“æ„ä¸Šã€‚<details>
<summary>Abstract</summary>
We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge set for Abstract Meaning Representation (AMR) parsing with accompanying evaluation metrics. AMR parsers now obtain high scores on the standard AMR evaluation metric Smatch, close to or even above reported inter-annotator agreement. But that does not mean that AMR parsing is solved; in fact, human evaluation in previous work indicates that current parsers still quite frequently make errors on node labels or graph structure that substantially distort sentence meaning. Here, we provide an evaluation suite that tests AMR parsers on a range of phenomena of practical, technical, and linguistic interest. Our 36 categories range from seen and unseen labels, to structural generalization, to coreference. GrAPES reveals in depth the abilities and shortcomings of current AMR parsers.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿç°åœ¨å‘å¸ƒäº†ç²’å­AMRè§£æè¯„ä¼°é›†ï¼ˆGrAPESï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºæŠ½è±¡æ„ä¹‰è¡¨ç¤ºï¼ˆAMRï¼‰è§£æçš„æŒ‘æˆ˜é›†ï¼ŒåŒæ—¶æä¾›äº†è¯„ä¼° Ğ¼ĞµÑ‚Ñ€Ğ¸ã€‚ç°åœ¨çš„AMRè§£æå™¨åœ¨æ ‡å‡†çš„Smatchè¯„ä¼° metricä¸Šè·å¾—äº†é«˜åˆ†ï¼Œæ¥è¿‘æˆ–è€…è¶…è¿‡äº†æŠ¥å‘Šçš„é—´æ¥å®¡æ ¸è€…ä¸€è‡´æ€§ã€‚ä½†è¿™å¹¶ä¸æ„å‘³ç€AMRè§£æå·²ç»è§£å†³äº†ï¼Œäº‹å®ä¸Šï¼Œåœ¨å‰ä¸€é¡¹å·¥ä½œä¸­çš„äººå·¥è¯„ä¼°è¡¨æ˜ï¼Œå½“å‰çš„è§£æå™¨ä»ç„¶å¾ˆé¢‘ç¹åœ°åœ¨èŠ‚ç‚¹æ ‡ç­¾æˆ–å›¾ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµä¸­å‡ºç°é”™è¯¯ï¼Œè¿™äº›é”™è¯¯ä¼šå¯¹å¥å­æ„ä¹‰äº§ç”Ÿé‡å¤§çš„æ‰­æ›²ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæµ‹è¯•AMRè§£æå™¨çš„è¯„ä¼°é›†ï¼Œè¯¥é›†åŒ…æ‹¬36ä¸ªç±»åˆ«ï¼Œä»seenå’Œunseenæ ‡ç­¾ã€ç»“æ„æ€»ç»“ã€æ ¸å¿ƒreferenceç­‰æ–¹é¢è¿›è¡Œæµ‹è¯•ã€‚GrAPESå°†æ·±å…¥æ¢è®¨å½“å‰AMRè§£æå™¨çš„èƒ½åŠ›å’Œç¼ºé™·ã€‚
</details></li>
</ul>
<hr>
<h2 id="DBCopilot-Scaling-Natural-Language-Querying-to-Massive-Databases"><a href="#DBCopilot-Scaling-Natural-Language-Querying-to-Massive-Databases" class="headerlink" title="DBCopilot: Scaling Natural Language Querying to Massive Databases"></a>DBCopilot: Scaling Natural Language Querying to Massive Databases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03463">http://arxiv.org/abs/2312.03463</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tshu-w/dbcopilot">https://github.com/tshu-w/dbcopilot</a></li>
<li>paper_authors: Tianshu Wang, Hongyu Lin, Xianpei Han, Le Sun, Xiaoyang Chen, Hao Wang, Zhenyu Zeng</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³ç°æœ‰çš„æ–‡æœ¬åˆ°SQLï¼ˆText-to-SQLï¼‰æ¡†æ¶åœ¨é¢å¯¹åºå¤§ã€åŠ¨æ€å˜åŒ–çš„æ•°æ®åº“æ—¶çš„æ‰©å±•æ€§é—®é¢˜ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§å¤‡å—æŠ˜è¡£çš„å’Œçµæ´»çš„åŠ©æ‰‹æ¨¡å‹æ¥è·¯ç”±åœ¨åºå¤§æ•°æ®åº“ä¸­ã€‚å…·ä½“æ¥è¯´ï¼ŒDBCopilot å°†æ–‡æœ¬åˆ°SQL è¿‡ç¨‹åˆ†è§£ä¸º schema è·¯ç”±å’Œ SQL ç”Ÿæˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä½¿ç”¨äº†ä¸€ç§è½»é‡çº§çš„åºåˆ—åˆ°åºåˆ—ç¥ç»ç½‘ç»œæ¨¡å‹æ¥æ„å»ºæ•°æ®åº“è¿æ¥å’Œå¯¼èˆªè‡ªç„¶è¯­è¨€é—®é¢˜é€šè¿‡æ•°æ®åº“å’Œè¡¨ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒDBCopilot æ˜¯ä¸€ä¸ªå¯æ‰©å±•å’Œé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å®é™…ä¸­çš„æ–‡æœ¬åˆ°SQL ä»»åŠ¡ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®åº“è‡ªåŠ¨å­¦ä¹ å’Œé€‚åº”çš„æœºåˆ¶ã€‚<details>
<summary>Abstract</summary>
Text-to-SQL simplifies database interactions by enabling non-experts to convert their natural language (NL) questions into Structured Query Language (SQL) queries. While recent advances in large language models (LLMs) have improved the zero-shot text-to-SQL paradigm, existing methods face scalability challenges when dealing with massive, dynamically changing databases. This paper introduces DBCopilot, a framework that addresses these challenges by employing a compact and flexible copilot model for routing across massive databases. Specifically, DBCopilot decouples the text-to-SQL process into schema routing and SQL generation, leveraging a lightweight sequence-to-sequence neural network-based router to formulate database connections and navigate natural language questions through databases and tables. The routed schemas and questions are then fed into LLMs for efficient SQL generation. Furthermore, DBCopilot also introduced a reverse schema-to-question generation paradigm, which can learn and adapt the router over massive databases automatically without requiring manual intervention. Experimental results demonstrate that DBCopilot is a scalable and effective solution for real-world text-to-SQL tasks, providing a significant advancement in handling large-scale schemas.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬åˆ°SQL æŠ€æœ¯å¯ä»¥ç®€åŒ–æ•°æ®åº“äº¤äº’ï¼Œè®©éä¸“å®¶è½¬æ¢è‡ªç„¶è¯­è¨€ï¼ˆNLï¼‰é—®é¢˜æˆä¸ºç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ï¼ˆSQLï¼‰æŸ¥è¯¢ã€‚è€Œæœ€è¿‘çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿›æ­¥æœ‰åŠ©äºé›¶å­¦ä¹ æ–‡æœ¬åˆ°SQL paradigmï¼Œä½†ç°æœ‰æ–¹æ³•åœ¨é¢ä¸´å·¨å¤§ã€åŠ¨æ€å˜åŒ–çš„æ•°æ®åº“æ—¶å­˜åœ¨æ‰©å±•æ€§é—®é¢˜ã€‚æœ¬æ–‡ä»‹ç»DBCopilotæ¡†æ¶ï¼Œè¯¥æ¡†æ¶è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œé€šè¿‡ä½¿ç”¨è½»é‡çº§å’Œçµæ´»çš„åŠ©æ‰‹æ¨¡å‹æ¥åœ¨å·¨å¤§æ•°æ®åº“ä¸­è·¯ç”±ã€‚å…·ä½“æ¥è¯´ï¼ŒDBCopilotå°†æ–‡æœ¬åˆ°SQLè¿‡ç¨‹åˆ†è§£æˆSchemaRoutingå’ŒSQLç”Ÿæˆä¸¤ä¸ªé˜¶æ®µï¼Œä½¿ç”¨è½»é‡çº§çš„åºåˆ—åˆ°åºåˆ—ç¥ç»ç½‘ç»œåŸºäºè·¯ç”±å™¨æ¥å½¢æˆæ•°æ®åº“è¿æ¥å’Œå¯¼èˆªè‡ªç„¶è¯­è¨€é—®é¢˜ Ñ‡ĞµÑ€ĞµĞ·æ•°æ®åº“å’Œè¡¨ã€‚è·¯ç”±çš„schemaå’Œé—®é¢˜ç„¶åè¢« feed into LLMs  Ğ´Ğ»Ñé«˜æ•ˆçš„ SQL ç”Ÿæˆã€‚æ­¤å¤–ï¼ŒDBCopilot è¿˜å¼•å…¥äº†åå‘ schema-to-question ç”Ÿæˆ paradigmï¼Œå¯ä»¥è‡ªåŠ¨å­¦ä¹ å’Œé€‚åº”å¤§æ•°æ®åº“ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDBCopilot æ˜¯ä¸€ä¸ªæ‰©å±•æ€§å’Œæœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œå¯¹å®é™…æ–‡æœ¬åˆ°SQLä»»åŠ¡å…·æœ‰é‡è¦è¿›æ­¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤§è§„æ¨¡çš„ schemaã€‚
</details></li>
</ul>
<hr>
<h2 id="Think-from-Words-TFW-Initiating-Human-Like-Cognition-in-Large-Language-Models-Through-Think-from-Words-for-Japanese-Text-level-Classification"><a href="#Think-from-Words-TFW-Initiating-Human-Like-Cognition-in-Large-Language-Models-Through-Think-from-Words-for-Japanese-Text-level-Classification" class="headerlink" title="Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification"></a>Think from Words(TFW): Initiating Human-Like Cognition in Large Language Models Through Think from Words for Japanese Text-level Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03458">http://arxiv.org/abs/2312.03458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengguang Gan, Qinghao Zhang, Tatsunori Mori</li>
<li>for:  This paper aims to improve the text comprehension of Large Language Models (LLMs) by bridging the gap between LLM and human-like thinking processes, specifically in the domain of Japanese text.</li>
<li>methods: The paper proposes two methods, â€œThink from Wordsâ€ (TFW) and â€œTFW with Extra word-level informationâ€ (TFW Extra), which initiate the comprehension process at the word level and incorporate additional word-level data to enhance LLMsâ€™ text comprehension.</li>
<li>results: The paper employs text classification on six Japanese datasets to assess the effectiveness of TFW and investigate the impact of various word-level information types on LLMsâ€™ text comprehension, providing insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text.<details>
<summary>Abstract</summary>
The proliferation of Large Language Models (LLMs) has spurred extensive research into LLM-related Prompt investigations, such as Instruction Learning (IL), In-context Learning (ICL), and Chain-of-Thought (CoT). These approaches aim to improve LLMs' responses by enabling them to provide concise statements or examples for deeper contemplation when addressing questions. However, independent thinking by LLMs can introduce variability in their thought processes, leading to potential inaccuracies. In response, our study seeks to bridge the gap between LLM and human-like thinking processes, recognizing that text comprehension begins with understanding individual words. To tackle this challenge, we have expanded the CoT method to cater to a specific domain. Our approach, known as "Think from Words" (TFW), initiates the comprehension process at the word level and then extends it to encompass the entire text. We also propose "TFW with Extra word-level information" (TFW Extra), augmenting comprehension with additional word-level data. To assess our methods, we employ text classification on six Japanese datasets comprising text-level and word-level elements. Our findings not only validate the effectiveness of TFW but also shed light on the impact of various word-level information types on LLMs' text comprehension, offering insights into their potential to cause misinterpretations and errors in the overall comprehension of the final text.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§é‡çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å‡ºç°ï¼Œæ¨åŠ¨äº†å…³äº LLM ç›¸å…³çš„æç¤ºç ”ç©¶ï¼Œå¦‚æŒ‡ä»¤å­¦ä¹ ï¼ˆILï¼‰ã€å†…å®¹å­¦ä¹ ï¼ˆICLï¼‰å’Œé“¾æ¡ï¼ˆCoTï¼‰ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨æ”¹è¿› LLM çš„å›ç­”ï¼Œè®©å®ƒä»¬èƒ½å¤Ÿæä¾›ç®€æ´çš„å£°æ˜æˆ–ç¤ºä¾‹ï¼Œä»¥ä¾¿æ›´æ·±å…¥çš„æ€è€ƒé—®é¢˜ã€‚ç„¶è€Œï¼Œ LLM ç‹¬ç«‹æ€è€ƒçš„å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å›ç­”çš„ä¸å‡†ç¡®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ—¨åœ¨å°† LLM å’Œäººç±»æ€ç»´è¿‡ç¨‹è¿æ¥èµ·æ¥ï¼Œè®¤ä¸ºæ–‡æœ¬ç†è§£å§‹äºå•è¯ç†è§£ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æ‰©å±•äº† CoT æ–¹æ³•ï¼Œç§°ä¸º "ä»å•è¯å¼€å§‹çš„ç†è§£"ï¼ˆTFWï¼‰ã€‚TFW æ–¹æ³•é¦–å…ˆä»å•è¯æ°´å¹³å¼€å§‹ç†è§£ï¼Œç„¶åæ‰©å±•åˆ°æ•´ç¯‡æ–‡æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡º "TFW åŠ ä¸Šé¢å¤–å•è¯æ°´å¹³ä¿¡æ¯"ï¼ˆTFW Extraï¼‰ï¼Œé€šè¿‡æ·»åŠ å•è¯æ°´å¹³æ•°æ®æ¥åŠ å¼ºç†è§£ã€‚ä¸ºè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…­ä¸ªæ—¥æœ¬æ–‡æœ¬é›†ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ°´å¹³å’Œå•è¯æ°´å¹³çš„å…ƒç´ ã€‚æˆ‘ä»¬çš„å‘ç°ä¸ä»…è¯æ˜äº† TFW çš„æœ‰æ•ˆæ€§ï¼Œè¿˜æ­ç¤ºäº†ä¸åŒå•è¯æ°´å¹³ä¿¡æ¯ç±»å‹å¯¹ LLM çš„æ–‡æœ¬ç†è§£äº§ç”Ÿäº†ä»€ä¹ˆå½±å“ï¼Œæä¾›äº†å¯¹ LLM å¯èƒ½çš„è¯¯è§£å’Œé”™è¯¯çš„æ·±å…¥äº†è§£ã€‚
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Multilingual-Text-Classification-Identification-through-Deep-Learning-and-Embedding-Visualization"><a href="#Comparative-Analysis-of-Multilingual-Text-Classification-Identification-through-Deep-Learning-and-Embedding-Visualization" class="headerlink" title="Comparative Analysis of Multilingual Text Classification &amp; Identification through Deep Learning and Embedding Visualization"></a>Comparative Analysis of Multilingual Text Classification &amp; Identification through Deep Learning and Embedding Visualization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03789">http://arxiv.org/abs/2312.03789</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arinjay Wyawhare</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†æ¯”è¾ƒå¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»æ–¹æ³•ï¼Œä½¿ç”¨æ·±åº¦å­¦ä¹ å’ŒåµŒå…¥å¯è§†åŒ–ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†LangDetectã€LangIdã€FastTextå’ŒSentence Transformeræ¨¡å‹ï¼Œå¹¶åœ¨ä¸€ä¸ªåŒ…å«17ç§è¯­è¨€çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒFastTextçš„2Då¯è§†åŒ–æ˜¾ç¤ºäº†æ›´æ¸…æ™°çš„å¹‚ç­‰åˆ†ç±»ç»“æœï¼Œå¹¶ä¸”FastTextå¤šå±‚Perceptronæ¨¡å‹åœ¨ç²¾åº¦ã€å‡†ç¡®ç‡ã€å›å½’ç‡å’ŒF1åˆ†æ•°æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†Sentence Transformeræ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
This research conducts a comparative study on multilingual text classification methods, utilizing deep learning and embedding visualization. The study employs LangDetect, LangId, FastText, and Sentence Transformer on a dataset encompassing 17 languages. It explores dimensionality's impact on clustering, revealing FastText's clearer clustering in 2D visualization due to its extensive multilingual corpus training. Notably, the FastText multi-layer perceptron model achieved remarkable accuracy, precision, recall, and F1 score, outperforming the Sentence Transformer model. The study underscores the effectiveness of these techniques in multilingual text classification, emphasizing the importance of large multilingual corpora for training embeddings. It lays the groundwork for future research and assists practitioners in developing language detection and classification systems. Additionally, it includes the comparison of multi-layer perceptron, LSTM, and Convolution models for classification.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™é¡¹ç ”ç©¶è¿›è¡Œäº†å¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»æ–¹æ³•çš„æ¯”è¾ƒç ”ç©¶ï¼Œåˆ©ç”¨æ·±åº¦å­¦ä¹ å’ŒåµŒå…¥è§†è§‰åŒ–ã€‚ç ”ç©¶ä½¿ç”¨äº† LangDetectã€LangIdã€FastText å’Œ Sentence Transformer åœ¨ä¸€ä¸ªåŒ…æ‹¬ 17 ç§è¯­è¨€çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ã€‚ç ”ç©¶å‘ç°ï¼Œåœ¨äºŒç»´è§†è§‰åŒ–ä¸­ï¼ŒFastText çš„ clustering æ›´åŠ æ¸…æ™°ï¼Œè¿™æ˜¯å› ä¸ºå®ƒåœ¨å¤šè¯­è¨€è®­ç»ƒä¸­è·å¾—äº†æ›´å¹¿æ³›çš„è®­ç»ƒæ•°æ®ã€‚å¦å¤–ï¼ŒFastText å¤šå±‚æ„ŸçŸ¥æœºåˆ¶å®ç°äº†å¾ˆé«˜çš„å‡†ç¡®ç‡ã€ç²¾åº¦ã€å›å½’ç‡å’Œ F1 åˆ†æ•°ï¼Œåœ¨ Sentence Transformer æ¨¡å‹ä¸­è¡¨ç°å‡ºè‰²ã€‚ç ”ç©¶è¯æ˜äº†è¿™äº›æŠ€æœ¯åœ¨å¤šè¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸­çš„æ•ˆæœï¼Œå¹¶ä¸”å¼ºè°ƒäº†åœ¨è®­ç»ƒåµŒå…¥æ—¶éœ€è¦å¤§é‡çš„å¤šè¯­è¨€è®­ç»ƒæ•°æ®ã€‚è¿™é¡¹ç ”ç©¶ä¸ºæœªæ¥ç ”ç©¶æä¾›äº†åŸºç¡€ï¼Œå¹¶å¸®åŠ©å®è·µè€…åœ¨è¯­è¨€æ£€æµ‹å’Œåˆ†ç±»æ–¹é¢å»ºç«‹ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¯”è¾ƒäº†å¤šå±‚æ„ŸçŸ¥ã€LSTM å’Œ Convolution æ¨¡å‹åœ¨åˆ†ç±»æ–¹é¢çš„è¡¨ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="SmoothQuant-Accurate-and-Efficient-4-bit-Post-Training-WeightQuantization-for-LLM"><a href="#SmoothQuant-Accurate-and-Efficient-4-bit-Post-Training-WeightQuantization-for-LLM" class="headerlink" title="SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM"></a>SmoothQuant+: Accurate and Efficient 4-bit Post-Training WeightQuantization for LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03788">http://arxiv.org/abs/2312.03788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Pan, Chengcan Wang, Kaifu Zheng, Yangguang Li, Zhenyu Wang, Bin Feng</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§é«˜æ•ˆç²¾åº¦çš„4ä½é‡å­åŒ–æ–¹æ³•ï¼Œä»¥ä¾¿å°†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éƒ¨ç½²åˆ°å…·æœ‰é™åˆ¶çš„è®¡ç®—å’Œå­˜å‚¨èµ„æºçš„è®¾å¤‡ä¸Šã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSmoothQuant+çš„ç²¾åº¦é«˜æ•ˆçš„4ä½é‡å­åŒ–æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¸éœ€è¦é¢å¤–è®­ç»ƒï¼Œå¯ä»¥ä¿æŒLLMsæ¨¡å‹çš„ç²¾åº¦ä¸äº§ç”ŸæŸå¤±ã€‚SmoothQuant+ä½¿ç”¨é€šé“çº§åˆ«çš„æ´»åŒ–å¼‚å¸¸å€¼ç¼“å’Œï¼Œå¹¶å¯¹åº”çš„è°ƒæ•´ç›¸åº”çš„æƒé‡ï¼Œä»¥ç¡®ä¿é‡å­åŒ–åçš„æ¨¡å‹å’ŒåŸå§‹æ¨¡å‹å…·æœ‰ç›¸åŒçš„ç²¾åº¦ã€‚</li>
<li>results: æ ¹æ®è®ºæ–‡çš„ç»“æœï¼Œä½¿ç”¨SmoothQuant+è¿›è¡Œ4ä½é‡å­åŒ–åï¼ŒCode Llama-34Bæ¨¡å‹å¯ä»¥åœ¨A100 40GB GPUä¸Šéƒ¨ç½²ï¼Œå¹¶ä¸”ä¿æŒæ¨¡å‹çš„ç²¾åº¦ä¸äº§ç”ŸæŸå¤±ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤ä¸ªA100 40GB GPUä¸Šè¿è¡Œçš„FP16æ¨¡å‹çš„ååé‡æ¯”SmoothQuant+æ¨¡å‹é«˜å‡º1.9å€è‡³4.0å€ï¼Œè€Œæ¯ä¸ªå­—ç¬¦çš„å»¶è¿Ÿæ—¶é—´ä»…å FP16æ¨¡å‹åœ¨ä¸¤ä¸ªA100 40GB GPUä¸Šè¿è¡Œæ—¶çš„68%ã€‚è¿™æ˜¯ç›®å‰æœ€ä½³çš„4ä½é‡å­åŒ–æ–¹æ³• Ğ´Ğ»Ñ LLMSã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable capabilities in various tasks. However their huge model size and the consequent demand for computational and memory resources also pose challenges to model deployment. Currently, 4-bit post-training quantization (PTQ) has achieved some success in LLMs, reducing the memory footprint by approximately 75% compared to FP16 models, albeit with some accuracy loss. In this paper, we propose SmoothQuant+, an accurate and efficient 4-bit weight-only PTQ that requires no additional training, which enables lossless in accuracy for LLMs for the first time. Based on the fact that the loss of weight quantization is amplified by the activation outliers, SmoothQuant+ smoothes the activation outliers by channel before quantization, while adjusting the corresponding weights for mathematical equivalence, and then performs group-wise 4-bit weight quantization for linear layers. We have integrated SmoothQuant+ into the vLLM framework, an advanced high-throughput inference engine specially developed for LLMs, and equipped it with an efficient W4A16 CUDA kernels, so that vLLM can seamlessly support SmoothQuant+ 4-bit weight quantization. Our results show that, with SmoothQuant+, the Code Llama-34B model can be quantized and deployed on a A100 40GB GPU, achieving lossless accuracy and a throughput increase of 1.9 to 4.0 times compared to the FP16 model deployed on two A100 40GB GPUs. Moreover, the latency per token is only 68% of the FP16 model deployed on two A100 40GB GPUs. This is the state-of-the-art 4-bit weight quantization for LLMs as we know.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä¸åŒä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒä»¬çš„åºå¤§æ¨¡å‹å¤§å°å’Œç›¸åº”çš„è®¡ç®—å’Œå­˜å‚¨èµ„æºéœ€æ±‚ä¹Ÿå­˜åœ¨æŠ•å…¥å›°éš¾ã€‚ç›®å‰ï¼Œ4æ¯”ç‰¹åæœŸé‡åŒ–ï¼ˆPTQï¼‰å·²ç»åœ¨LLMä¸­è·å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œå¯ä»¥å°†æ¨¡å‹çš„å­˜å‚¨å°ºå¯¸å‡å°‘çº¦75%ï¼Œä½†æ˜¯ä¼šæœ‰ä¸€å®šçš„ç²¾åº¦æŸå¤±ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ç²¾åº¦å’Œé«˜æ•ˆçš„4æ¯”ç‰¹åªé‡åŒ–ï¼ˆSmoothQuant+ï¼‰ï¼Œä¸éœ€è¦é¢å¤–è®­ç»ƒï¼Œå¯ä»¥å®ç°LLMä¸­çš„ç²¾åº¦æŸå¤±æ— æŸã€‚åŸºäºæ´»åŠ¨å€¼å¼‚å¸¸å€¼çš„æ‰©æ•£ä¼šå¢åŠ é‡åŒ–æŸå¤±ï¼ŒSmoothQuant+åœ¨é€šé“çº§åˆ«å°†æ´»åŠ¨å€¼ç¼“å†²å’Œæ»¤æ³¢ï¼Œç„¶åå¯¹åº”çš„ weights è¿›è¡Œæ•°å­¦ç­‰ä»·æ€§è°ƒæ•´ï¼Œå¹¶å¯¹ linear å±‚è¿›è¡Œåˆ†ç»„weise 4æ¯”ç‰¹é‡åŒ–ã€‚æˆ‘ä»¬å°†SmoothQuant+ç»“åˆåˆ°äº†é«˜æ€§èƒ½çš„ vLLM æ¡†æ¶ä¸­ï¼Œå¹¶ä½¿ç”¨é«˜æ•ˆçš„ W4A16 CUDA åŠ é€Ÿå™¨ï¼Œä»¥ä¾¿ vLLM å¯ä»¥æ— ç¼æ”¯æŒ SmoothQuant+ 4æ¯”ç‰¹é‡åŒ–ã€‚æˆ‘ä»¬çš„ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ SmoothQuant+ï¼ŒCode Llama-34B æ¨¡å‹å¯ä»¥åœ¨ A100 40GB GPU ä¸Šè¿›è¡Œé‡åŒ–éƒ¨ç½²ï¼Œå®ç°ç²¾åº¦æŸå¤±æ— æŸï¼Œå¹¶æé«˜äº† Throughput 1.9-4.0 å€ï¼ŒåŒæ—¶å‡å°‘äº†æ¯ä¸ª Token çš„å»¶è¿Ÿæ—¶é—´ä¸º FP16 æ¨¡å‹åœ¨ä¸¤ä¸ª A100 40GB GPU ä¸Šéƒ¨ç½²çš„ 68%ã€‚è¿™æ˜¯ç›®å‰æœ€ä½³çš„4æ¯”ç‰¹é‡åŒ–æ–¹æ³• Ğ´Ğ»Ñ LLMã€‚
</details></li>
</ul>
<hr>
<h2 id="Compressed-Context-Memory-For-Online-Language-Model-Interaction"><a href="#Compressed-Context-Memory-For-Online-Language-Model-Interaction" class="headerlink" title="Compressed Context Memory For Online Language Model Interaction"></a>Compressed Context Memory For Online Language Model Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03414">http://arxiv.org/abs/2312.03414</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/snu-mllab/context-memory">https://github.com/snu-mllab/context-memory</a></li>
<li>paper_authors: Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§ Context Compression æ–¹æ³•ï¼Œç”¨äºåœ¨åœ¨çº¿åœºæ™¯ä¸­ï¼Œå¦‚ ChatGPTï¼Œè¿›è¡Œ Transformer è¯­è¨€æ¨¡å‹çš„å‹ç¼©ã€‚éšç€ä¸Šä¸‹æ–‡çš„æ‰©å±•ï¼Œæ³¨æ„è¿‡ç¨‹éœ€è¦æ›´å¤šçš„å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œä»è€Œé™ä½äº†è¯­è¨€æ¨¡å‹çš„ååé‡ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§å‹ç¼©ä¸Šä¸‹æ–‡å­˜å‚¨ç³»ç»Ÿï¼Œé€šè¿‡åœ¨è¯­è¨€æ¨¡å‹çš„å‰è¿›ä¼ è¾“ä¸­ integrate ä¸€ä¸ªè½»é‡çº§çš„conditional LoRAæ¥å®ç°å‹ç¼©ã€‚åŸºäºå‹ç¼©ä¸Šä¸‹æ–‡å­˜å‚¨ç³»ç»Ÿï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥è¿›è¡Œå‹ç¼©çš„æ³¨æ„æ“ä½œå’Œå†…å­˜æ“ä½œï¼Œä»è€Œå®ç°å‹ç¼©çš„è¯­è¨€æ¨¡å‹ã€‚</li>
<li>results: é€šè¿‡å¯¹è¯ã€ä¸ªæ€§åŒ–å’Œå¤šä»»åŠ¡å­¦ä¹ ç­‰è¯„ä¼°ï¼Œæˆ‘ä»¬ demonstarte äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¾¾åˆ°ä¸€ä¸ªå®Œæ•´çš„ä¸Šä¸‹æ–‡æ¨¡å‹çš„æ€§èƒ½æ°´å¹³ï¼Œä½†å…·æœ‰ $5\times$ å°çš„ä¸Šä¸‹æ–‡å­˜å‚¨ç©ºé—´ã€‚ä»£ç å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://github.com/snu-mllab/context-memory">https://github.com/snu-mllab/context-memory</a> ä¸­æ‰¾åˆ°ã€‚<details>
<summary>Abstract</summary>
This paper presents a novel context compression method for Transformer language models in online scenarios such as ChatGPT, where the context continually expands. As the context lengthens, the attention process requires more memory and computational resources, which in turn reduces the throughput of the language model. To this end, we propose a compressed context memory system that continually compresses the growing context into a compact memory space. The compression process simply involves integrating a lightweight conditional LoRA into the language model's forward pass during inference. Based on the compressed context memory, the language model can perform inference with reduced memory and attention operations. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\times$ smaller context memory space. Codes are available at https://github.com/snu-mllab/context-memory.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºTransformerè¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å‹ç¼©æ–¹æ³•ï¼Œç”¨äºåœ¨åœ¨çº¿åœºæ™¯å¦‚ChatGPTä¸­ï¼Œcontextä¸æ–­æ‰©å±•ã€‚éšç€contextçš„å¢é•¿ï¼Œæ³¨æ„è¿‡ç¨‹éœ€è¦æ›´å¤šçš„å†…å­˜å’Œè®¡ç®—èµ„æºï¼Œä»è€Œé™ä½è¯­è¨€æ¨¡å‹çš„ååç‡ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§å‹ç¼©ä¸Šä¸‹æ–‡å†…å­˜ç³»ç»Ÿï¼Œé€šè¿‡åœ¨è¯­è¨€æ¨¡å‹çš„å‰è¿›é€šé“ä¸­æ’å…¥ä¸€ä¸ªè½»é‡çº§çš„ conditional LoRAè¿›è¡Œå‹ç¼©ã€‚åŸºäºå‹ç¼©ä¸Šä¸‹æ–‡å†…å­˜ï¼Œè¯­è¨€æ¨¡å‹å¯ä»¥è¿›è¡Œå‹ç¼©åçš„æ¨ç†ï¼Œå…·æœ‰å‡å°‘å†…å­˜å’Œæ³¨æ„æ“ä½œçš„èƒ½åŠ›ã€‚ç»è¿‡å¯¹è¯ã€ä¸ªæ€§åŒ–å’Œå¤šä»»åŠ¡å­¦ä¹ çš„è¯„ä¼°ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®ç°ä¸å…¨ä¸Šä¸‹æ–‡æ¨¡å‹ç›¸åŒçš„æ€§èƒ½æ°´å¹³ï¼Œä½†å…·æœ‰5å€å°çš„ä¸Šä¸‹æ–‡å†…å­˜ç©ºé—´ã€‚ä»£ç å¯ä»¥åœ¨https://github.com/snu-mllab/context-memoryä¸Šä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Text-to-Text-Model-for-Multilingual-Offensive-Language-Identification"><a href="#A-Text-to-Text-Model-for-Multilingual-Offensive-Language-Identification" class="headerlink" title="A Text-to-Text Model for Multilingual Offensive Language Identification"></a>A Text-to-Text Model for Multilingual Offensive Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03379">http://arxiv.org/abs/2312.03379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tharindu Ranasinghe, Marcos Zampieri</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ªåŸºäº transformer çš„è¯­è¨€æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«ç¤¾äº¤åª’ä½“ä¸Šçš„ä¸è‰¯å†…å®¹ï¼ˆå¦‚ä»‡æ¨è¨€è¯­ã€ç½‘ç»œæ¬ºå‡Œã€ç½‘ç»œæ”»å‡»ç­‰ï¼‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† text-to-text transformers (T5) æ¨¡å‹ï¼Œå¹¶åœ¨ä¸¤ä¸ªå¤§è§„æ¨¡çš„ä¸è‰¯è¯­è¨€è¯†åˆ«æ•°æ®é›†ï¼ˆSOLID å’Œ CCTKï¼‰ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚åœ¨è¿™äº›æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬ç ”ç©¶äº†å°†ä¸¤ä¸ªæ•°æ®é›†åˆå¹¶ä½¿ç”¨ï¼Œä»¥åŠåœ¨ semi-supervised æƒ…å†µä¸‹é€‰æ‹©æœ€ä½³é˜ˆå€¼çš„å½±å“ã€‚</li>
<li>results: æˆ‘ä»¬çš„é¢„è®­ç»ƒ T5 æ¨¡å‹åœ¨å¤šä¸ªè‹±è¯­ benchmark ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†å…¶ä»–åŸºäº transformer çš„æ¨¡å‹ï¼ˆå¦‚ fBERT å’Œ HateBERTï¼‰çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨å…­ç§ä¸åŒè¯­è¨€ï¼ˆå¾·è¯­ã€å¸Œè…Šè¯­ã€éŸ©è¯­ã€é©¬æ‹‰åœ°è¯­ã€åƒ§ä¼½ç½—è¯­å’Œè¥¿ç­ç‰™è¯­ï¼‰ä¸Šè®­ç»ƒäº†é¦–ä¸ªå¤šè¯­è¨€é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨è¿™äº›è¯­è¨€ä¸Šè¾¾åˆ°äº†æ–°çš„å·OF-THE-ART è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
The ubiquity of offensive content on social media is a growing cause for concern among companies and government organizations. Recently, transformer-based models such as BERT, XLNET, and XLM-R have achieved state-of-the-art performance in detecting various forms of offensive content (e.g. hate speech, cyberbullying, and cyberaggression). However, the majority of these models are limited in their capabilities due to their encoder-only architecture, which restricts the number and types of labels in downstream tasks. Addressing these limitations, this study presents the first pre-trained model with encoder-decoder architecture for offensive language identification with text-to-text transformers (T5) trained on two large offensive language identification datasets; SOLID and CCTK. We investigate the effectiveness of combining two datasets and selecting an optimal threshold in semi-supervised instances in SOLID in the T5 retraining step. Our pre-trained T5 model outperforms other transformer-based models fine-tuned for offensive language detection, such as fBERT and HateBERT, in multiple English benchmarks. Following a similar approach, we also train the first multilingual pre-trained model for offensive language identification using mT5 and evaluate its performance on a set of six different languages (German, Hindi, Korean, Marathi, Sinhala, and Spanish). The results demonstrate that this multilingual model achieves a new state-of-the-art on all the above datasets, showing its usefulness in multilingual scenarios. Our proposed T5-based models will be made freely available to the community.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç¤¾äº¤åª’ä½“ä¸Šçš„ä¸è‰¯å†…å®¹é—®é¢˜æ—¥ç›Šæ‹…å¿§ï¼Œè®¸å¤šå…¬å¸å’Œæ”¿åºœç»„ç»‡éƒ½åœ¨å¯»æ‰¾æœ‰æ•ˆçš„è¯†åˆ«æ–¹æ³•ã€‚æœ€è¿‘ï¼ŒåŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œå¦‚BERTã€XLNETå’ŒXLM-Rï¼Œå·²ç»è¾¾åˆ°äº†è¯†åˆ«ä¸è‰¯å†…å®¹çš„çŠ¶æ€å¯¹æŠ—æ€§è¡¨ç°ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹çš„ä¸»è¦å±€é™æ€§åœ¨äºå…¶æ ¸å¿ƒåª architectureï¼Œè¿™é™åˆ¶äº†ä¸‹æ¸¸ä»»åŠ¡ä¸­çš„æ ‡ç­¾ç±»å‹å’Œæ•°é‡ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬ç ”ç©¶æå‡ºäº†é¦–ä¸ªä½¿ç”¨æ–‡æœ¬åˆ°æ–‡æœ¬è½¬æ¢å™¨ï¼ˆT5ï¼‰è¿›è¡Œä¸è‰¯è¯­è¨€è¯†åˆ«çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªå¤§çš„ä¸è‰¯è¯­è¨€è¯†åˆ« datasetï¼ˆSOLIDå’ŒCCTKï¼‰ä¸Šè¿›è¡Œäº†T5çš„é¢„è®­ç»ƒï¼Œå¹¶investigatedäº†åœ¨åŠæœ‰é™åˆ¶çš„æƒ…å†µä¸‹é€‰æ‹©æœ€ä½³é˜ˆå€¼çš„å½±å“ã€‚æˆ‘ä»¬çš„é¢„è®­ç»ƒT5æ¨¡å‹åœ¨å¤šä¸ªè‹±è¯­æ ‡å‡†æµ‹è¯•ä¸Šè¶…è¿‡äº†å…¶ä»–åŸºäºè½¬æ¢å™¨çš„æ¨¡å‹ï¼Œå¦‚fBERTå’ŒHateBERTï¼Œçš„è¯†åˆ«æ€§èƒ½ã€‚ollowing a similar approach, we also train the first multilingual pre-trained model for offensive language identification using mT5, and evaluate its performance on a set of six different languages (German, Hindi, Korean, Marathi, Sinhala, and Spanish). The results show that this multilingual model achieves a new state-of-the-art on all the above datasets, demonstrating its usefulness in multilingual scenarios. Our proposed T5-based models will be made freely available to the community.
</details></li>
</ul>
<hr>
<h2 id="Lazy-k-Decoding-for-Constrained-Token-Classification"><a href="#Lazy-k-Decoding-for-Constrained-Token-Classification" class="headerlink" title="Lazy-k: Decoding for Constrained Token Classification"></a>Lazy-k: Decoding for Constrained Token Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03367">http://arxiv.org/abs/2312.03367</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arthurdevnl/lazyk">https://github.com/arthurdevnl/lazyk</a></li>
<li>paper_authors: Arthur Hemmer, MickaÃ«l Coustaty, Nicola Bartolo, JÃ©rÃ´me Brachat, Jean-Marc Ogier</li>
<li>for: æé«˜æ¦‚ç‡æ¨¡å‹åœ¨ç»“æ„é¢„æµ‹ä¸­çš„è¡¨ç°</li>
<li>methods: ç»“åˆå—é™è§£ç æ–¹æ³•ï¼Œä½¿ç”¨å°å‹æ¨¡å‹</li>
<li>results: å—é™è§£ç æ–¹æ³•å¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯ä½¿ç”¨å°å‹æ¨¡å‹æ—¶Hereâ€™s a breakdown of each point:</li>
<li>for: The paper aims to improve the performance of probabilistic models in structured prediction.</li>
<li>methods: The paper combines probabilistic models with constrained decoding approaches, specifically in the context of token classification for information extraction.</li>
<li>results: The paper shows that constrained decoding approaches can significantly improve the modelsâ€™ performances, especially when using smaller models. Additionally, the Lazy-$k$ approach proposed in the paper allows for more flexibility between decoding time and accuracy.<details>
<summary>Abstract</summary>
We explore the possibility of improving probabilistic models in structured prediction. Specifically, we combine the models with constrained decoding approaches in the context of token classification for information extraction. The decoding methods search for constraint-satisfying label-assignments while maximizing the total probability. To do this, we evaluate several existing approaches, as well as propose a novel decoding method called Lazy-$k$. Our findings demonstrate that constrained decoding approaches can significantly improve the models' performances, especially when using smaller models. The Lazy-$k$ approach allows for more flexibility between decoding time and accuracy. The code for using Lazy-$k$ decoding can be found here: https://github.com/ArthurDevNL/lazyk.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æ¢è®¨å¯èƒ½æ€§æ¨¡å‹åœ¨ç»“æ„åŒ–é¢„æµ‹ä¸­çš„æå‡ã€‚specificallyï¼Œæˆ‘ä»¬å°†æ¨¡å‹ä¸çº¦æŸè§£ç æ–¹æ³•ç»“åˆåœ¨ä¿¡æ¯æŠ½å–ä¸­çš„Tokenç±»å‹åˆ†ç±»ä¸­ã€‚è§£ç æ–¹æ³•ä¼šæœç´¢æ»¡è¶³çº¦æŸçš„æ ‡ç­¾åˆ†é…ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ€»æ¦‚ç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¯„ä¼°äº†ä¸€äº›ç°æœ‰çš„æ–¹æ³•ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„è§£ç æ–¹æ³•called Lazy-$k$.æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œçº¦æŸè§£ç æ–¹æ³•å¯ä»¥æ˜æ˜¾æé«˜æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ã€‚Lazy-$k$æ–¹æ³•å…è®¸åœ¨è§£ç æ—¶é—´å’Œå‡†ç¡®ç‡ä¹‹é—´è¿›è¡Œæ›´å¤šçš„çµæ´»æ€§ã€‚å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ä½¿ç”¨Lazy-$k$è§£ç çš„ä»£ç ï¼šhttps://github.com/ArthurDevNL/lazykã€‚
</details></li>
</ul>
<hr>
<h2 id="KhabarChin-Automatic-Detection-of-Important-News-in-the-Persian-Language"><a href="#KhabarChin-Automatic-Detection-of-Important-News-in-the-Persian-Language" class="headerlink" title="KhabarChin: Automatic Detection of Important News in the Persian Language"></a>KhabarChin: Automatic Detection of Important News in the Persian Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03361">http://arxiv.org/abs/2312.03361</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Hematian Hemati, Arash Lagzian, Moein Salimi Sartakhti, Hamid Beigy, Ehsaneddin Asgari</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨é‡è¦æ–°é—»çš„æ£€æµ‹ï¼Œä»¥æé«˜ç¤¾ä¼šå¤§é‡äººç¾¤çš„ä¿¡æ¯æ„ŸçŸ¥å’Œå†³ç­–æ•ˆç‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹æ³•è‡ªåŠ¨åŒ–æ–°é—»æ£€æµ‹è¿‡ç¨‹ã€‚æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºå‡†æ•°æ®é›†ï¼ˆKhabarchinï¼‰ï¼Œç”¨äºæ£€æµ‹æ³¢æ–¯è¯­æ–°é—»ä¸­çš„é‡è¦æ–°é—»ã€‚</li>
<li>results: ç ”ç©¶å¯¹7,869ç¯‡æ³¢æ–¯è¯­æ–°é—»æ–‡ç« è¿›è¡Œäº†æ³¨é‡Šï¼Œå¹¶åˆ›å»ºäº†æ•°æ®é›†ã€‚é¢ä¸´äº†é«˜åº¦ä¸åŒè§‚å’Œç±»åˆ«åè§çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æå‡ºäº†ä¸€äº›å­¦ä¹ å‹æ¨¡å‹ï¼Œä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°å½“å‰æœ€ä½³transformeræ¨¡å‹ï¼Œè§£å†³è¿™ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œè¿˜æå‡ºäº†æ–°é—»æ–‡ç« ä¸­é‡è¦å¥å­æ£€æµ‹çš„ç¬¬äºŒä»»åŠ¡ï¼Œä»¥è§£å†³é•¿æ–‡æœ¬ä¸Šé‡è¦ä¿¡æ¯çš„æ£€æµ‹é—®é¢˜ã€‚<details>
<summary>Abstract</summary>
Being aware of important news is crucial for staying informed and making well-informed decisions efficiently. Natural Language Processing (NLP) approaches can significantly automate this process. This paper introduces the detection of important news, in a previously unexplored area, and presents a new benchmarking dataset (Khabarchin) for detecting important news in the Persian language. We define important news articles as those deemed significant for a considerable portion of society, capable of influencing their mindset or decision-making. The news articles are obtained from seven different prominent Persian news agencies, resulting in the annotation of 7,869 samples and the creation of the dataset. Two challenges of high disagreement and imbalance between classes were faced, and solutions were provided for them. We also propose several learning-based models, ranging from conventional machine learning to state-of-the-art transformer models, to tackle this task. Furthermore, we introduce the second task of important sentence detection in news articles, as they often come with a significant contextual length that makes it challenging for readers to identify important information. We identify these sentences in a weakly supervised manner.
</details>
<details>
<summary>æ‘˜è¦</summary>
çŸ¥é“é‡è¦çš„æ–°é—»å¯¹äºå¿«é€Ÿè·å–ä¿¡æ¯å’Œåšå‡º Informed å†³ç­–è‡³å…³é‡è¦ã€‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ–¹æ³•å¯ä»¥å¸®åŠ©è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ã€‚è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†æ£€æµ‹é‡è¦æ–°é—»çš„æ–°æ–¹æ³•ï¼Œåœ¨æœªæ›¾ç ”ç©¶çš„åœ°åŒºè¿›è¡Œäº†æ¢ç´¢ã€‚æˆ‘ä»¬å®šä¹‰é‡è¦æ–°é—»æ–‡ç« ä¸ºèƒ½å¤Ÿå¯¹ä¸€å¤§éƒ¨åˆ†ç¤¾ä¼šäº§ç”Ÿå½±å“ï¼Œèƒ½å¤Ÿæ”¹å˜ä»–ä»¬çš„æ€ç»´æ–¹å¼æˆ–å†³ç­–æ–¹å¼ã€‚æ–°é—»æ–‡ç« æ¥è‡ªä¸ƒå®¶é‡è¦çš„æ³¢æ–¯è¯­æ–°é—»æœºæ„ï¼Œå…±è®¡7,869ä¸ªæ ·æœ¬ï¼Œå¹¶åˆ›å»ºäº†æ•°æ®é›†ã€‚é¢ä¸´äº†é«˜åº¦ä¸åŒè§‚å’Œç±»åˆ«å¼‚è´¨çš„ä¸¤ä¸ªæŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›å­¦ä¹ åŸºäºæ¨¡å‹ï¼Œä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ åˆ°å½“å‰æœ€ä½³transformeræ¨¡å‹ï¼Œè§£å†³è¿™ä¸ªä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†æ–°é—»æ–‡ç« ä¸­é‡è¦å¥å­æ£€æµ‹çš„ç¬¬äºŒä¸ªä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬ç»å¸¸å…·æœ‰è¾ƒé•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½¿è¯»è€…å¾ˆéš¾å¯»æ‰¾é‡è¦ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨å¼±ç›‘ç£æ–¹å¼ä¸‹è¿›è¡Œäº†è¿™ä¸ªä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Topic-and-genre-in-dialogue"><a href="#Topic-and-genre-in-dialogue" class="headerlink" title="Topic and genre in dialogue"></a>Topic and genre in dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03342">http://arxiv.org/abs/2312.03342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amandine Decker, Ellen Breitholtz, Christine Howes, Staffan Larsson</li>
<li>for: æœ¬ç ”ç©¶è¯æ˜è¯é¢˜åœ¨å¯¹è¯ä¸­å‘æŒ¥åŸºæœ¬ä½œç”¨ï¼Œå¹¶ä¸”éœ€è¦åœ¨è¯é¢˜å’Œç±»å‹ä¹‹é—´åˆ’åˆ†å’Œæ­£äº¤å®šä¹‰ï¼Œä»¥å®ç°å¯é ã€å¯æ§å’Œè‡ªå®šä¹‰å¯¹è¯ç³»ç»Ÿã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†è¯é¢˜åˆ†æå’Œåˆ†ç±»æŠ€æœ¯ï¼Œä»¥åŠå¯¹è¯åˆ†æå’Œæ¨¡å‹å»ºç«‹æ–¹æ³•ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡åˆ†åˆ«å®šä¹‰è¯é¢˜å’Œç±»å‹ï¼Œå¯ä»¥å®ç°å¯¹è¯ç³»ç»Ÿçš„æ¨¡å—åŒ–ã€å¯é å’Œè‡ªå®šä¹‰ï¼Œå¹¶ä¸”å¯ä»¥æé«˜å¯¹è¯ç³»ç»Ÿçš„å¯æ§æ€§å’Œæ•ˆæœã€‚<details>
<summary>Abstract</summary>
In this paper we argue that topic plays a fundamental role in conversations, and that the concept is needed in addition to that of genre to define interactions. In particular, the concepts of genre and topic need to be separated and orthogonally defined. This would enable modular, reliable and controllable flexible-domain dialogue systems.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ argues That topic åœ¨å¯¹è¯ä¸­å‘æŒ¥åŸºæœ¬ä½œç”¨ï¼Œå¹¶ä¸”è®¤ä¸ºè¿™ä¸ªæ¦‚å¿µä¸ genre éœ€è¦åˆ†å¼€ã€æ­£äº¤å®šä¹‰ã€‚è¿™æ ·å¯ä»¥å¸¦æ¥å¯æ¨¡å—åŒ–ã€å¯é ã€å¯æ§çš„å¤šé¢†åŸŸå¯¹è¯ç³»ç»Ÿã€‚Note:* "topic" è¢«ç¿»è¯‘ä¸º "è¯é¢˜" (huÃ¬ tÃ­)* "genre" è¢«ç¿»è¯‘ä¸º "ç±»å‹" (lÃ¨i xÃ¬ng)* "orthogonally" è¢«ç¿»è¯‘ä¸º "æ­£äº¤" (zhÃ¨ng jÃ¬)* "modular" è¢«ç¿»è¯‘ä¸º "å¯æ¨¡å—åŒ–" (kÄ› mÃ³udÄng hÃ³u)* "reliable" è¢«ç¿»è¯‘ä¸º "å¯é " (kÄ› xÃ¬ng)* "controllable" è¢«ç¿»è¯‘ä¸º "å¯æ§" (kÄ› kÃ²ng)
</details></li>
</ul>
<hr>
<h2 id="Measuring-Misogyny-in-Natural-Language-Generation-Preliminary-Results-from-a-Case-Study-on-two-Reddit-Communities"><a href="#Measuring-Misogyny-in-Natural-Language-Generation-Preliminary-Results-from-a-Case-Study-on-two-Reddit-Communities" class="headerlink" title="Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities"></a>Measuring Misogyny in Natural Language Generation: Preliminary Results from a Case Study on two Reddit Communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03330">http://arxiv.org/abs/2312.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaron J. Snoswell, Lucinda Nelson, Hao Xue, Flora D. Salim, Nicolas Suzor, Jean Burgess</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†è¯„ä¼°è‡ªç„¶è¯­è¨€ç”Ÿæˆä¸­çš„æ¶æ„æƒ…å†µï¼Œå°¤å…¶æ˜¯è¯„ä¼° generic â€˜toxicityâ€™ åˆ†ç±»å™¨åœ¨è¯†åˆ«æ¶æ„è¯­è¨€ä¸­çš„ç¼ºç‚¹ã€‚</li>
<li>methods: ä½œè€…ä½¿ç”¨äº†ä¸¤ä¸ªwell-characterized â€˜Incelâ€™ ç¤¾åŒºåœ¨ Reddit ä¸Šçš„æ•°æ®æ¥æ„å»ºäº†ä¸¤ä¸ªè®­ç»ƒé›†ï¼Œå¹¶ä½¿ç”¨äº†è¿™äº›è®­ç»ƒé›†æ¥ç²¾åˆ¶ä¸¤ä¸ªè¯­è¨€æ¨¡å‹ã€‚ç„¶åï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ä¸ªå¼€æºçš„ â€˜toxicityâ€™ åˆ†ç±»å™¨æ¥è¯„ä¼°è¿™ä¸¤ä¸ªè¯­è¨€æ¨¡å‹ä¸­çš„æ¶æ„è¯­è¨€è¡¨ç°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ generic â€˜toxicityâ€™ åˆ†ç±»å™¨æ— æ³•åœ¨è¿™ä¸¤ä¸ªè¯­è¨€æ¨¡å‹ä¸­åˆ†è¾¨å‡ºæ„ä¹‰æ€§çš„åŒºåˆ«ã€‚è€Œä¸€ä¸ª feminist ä¸»é¢˜ä¸“å®¶æå‡ºçš„ä¸€ä¸ª gender-specific è¯æ±‡è¡¨åˆ™èƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«è¿™ä¸¤ä¸ªç¤¾åŒºçš„ä¸åŒã€‚è¿™äº›åˆæ­¥ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨é€šç”¨çš„æ–¹æ³•æ¥è¯„ä¼°å±å®³æ€§çš„ç¼ºç‚¹ï¼Œå¹¶é«˜äº®äº†éœ€è¦åœ¨è‡ªç„¶è¯­è¨€è¯„ä¼°ä¸­æ³¨æ„çš„ç²¾ç»†çš„ benchmark è®¾è®¡å’Œé€‰æ‹©ã€‚<details>
<summary>Abstract</summary>
Generic `toxicity' classifiers continue to be used for evaluating the potential for harm in natural language generation, despite mounting evidence of their shortcomings. We consider the challenge of measuring misogyny in natural language generation, and argue that generic `toxicity' classifiers are inadequate for this task. We use data from two well-characterised `Incel' communities on Reddit that differ primarily in their degrees of misogyny to construct a pair of training corpora which we use to fine-tune two language models. We show that an open source `toxicity' classifier is unable to distinguish meaningfully between generations from these models. We contrast this with a misogyny-specific lexicon recently proposed by feminist subject-matter experts, demonstrating that, despite the limitations of simple lexicon-based approaches, this shows promise as a benchmark to evaluate language models for misogyny, and that it is sensitive enough to reveal the known differences in these Reddit communities. Our preliminary findings highlight the limitations of a generic approach to evaluating harms, and further emphasise the need for careful benchmark design and selection in natural language evaluation.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimizing-Two-Pass-Cross-Lingual-Transfer-Learning-Phoneme-Recognition-and-Phoneme-to-Grapheme-Translation"><a href="#Optimizing-Two-Pass-Cross-Lingual-Transfer-Learning-Phoneme-Recognition-and-Phoneme-to-Grapheme-Translation" class="headerlink" title="Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition and Phoneme to Grapheme Translation"></a>Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition and Phoneme to Grapheme Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03312">http://arxiv.org/abs/2312.03312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonjun Lee, Gary Geunbae Lee, Yunsu Kim</li>
<li>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨æé«˜ä¸¤ä¸ªé€šè¿‡è¯­è¨€çš„ Cross-Lingual Transfer Learningï¼ˆCLTLï¼‰ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«çš„ç²¾åº¦ã€‚</li>
<li>methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†ä¸¤ä¸ªé˜¶æ®µçš„ä¼˜åŒ–ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä¼˜åŒ–äº†éŸ³ç´ è¯†åˆ«æ¨¡å‹å’ŒéŸ³ç´ åˆ°æ–‡å­—è½¬æ¢æ¨¡å‹ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«çš„ç²¾åº¦ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨çƒéŸ³ç´ å™ªå£°ç”Ÿæˆå™¨ï¼Œä»¥åœ¨æ–‡å­—åˆ°å›¾æ ‡è®­ç»ƒä¸­æ¨¡æ‹ŸçœŸå®çš„ ASR å™ªå£°ï¼Œä»è€Œé™ä½é”™è¯¯çš„ä¼ é€’ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä½èµ„æºè¯­è¨€ä¸­æ˜¾è‘—é™ä½ Word Error Rateï¼ˆWERï¼‰ï¼Œè¿™è¯´æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚è¿™é¡¹ç ”ç©¶çš„æˆæœå¯èƒ½å¯¹ä¸¤ä¸ªé€šè¿‡è¯­è¨€çš„ ASR ç³»ç»Ÿçš„å‘å±•äº§ç”Ÿå½±å“ï¼Œå¹¶ä¸”å¯èƒ½æä¾›æ›´å¥½çš„ Cross-Lingual Transfer Learning æŠ€æœ¯ã€‚<details>
<summary>Abstract</summary>
This research optimizes two-pass cross-lingual transfer learning in low-resource languages by enhancing phoneme recognition and phoneme-to-grapheme translation models. Our approach optimizes these two stages to improve speech recognition across languages. We optimize phoneme vocabulary coverage by merging phonemes based on shared articulatory characteristics, thus improving recognition accuracy. Additionally, we introduce a global phoneme noise generator for realistic ASR noise during phoneme-to-grapheme training to reduce error propagation. Experiments on the CommonVoice 12.0 dataset show significant reductions in Word Error Rate (WER) for low-resource languages, highlighting the effectiveness of our approach. This research contributes to the advancements of two-pass ASR systems in low-resource languages, offering the potential for improved cross-lingual transfer learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™é¡¹ç ”ç©¶ä¼˜åŒ–äº†ä¸¤ä¸ªé˜¶æ®µçš„ä¸¤ç§è¯­è¨€ä¹‹é—´è½¬ç§»å­¦ä¹ ï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«çš„ç²¾åº¦ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¼˜åŒ–äº†è¿™ä¸¤ä¸ªé˜¶æ®µï¼Œä»¥æé«˜è¯­éŸ³è¯†åˆ«çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬ä¼˜åŒ–phonemeè¯æ±‡è¦†ç›–ç‡ï¼Œé€šè¿‡å…±äº«è¯­éŸ³ç‰¹å¾æ¥åˆå¹¶phonemeï¼Œä»è€Œæé«˜è¯†åˆ«ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å…¨çƒphonemeå™ªéŸ³ç”Ÿæˆå™¨ï¼Œä»¥æä¾›å®é™…ASRå™ªéŸ³ durante phoneme-to-graphemeè®­ç»ƒï¼Œä»¥é™ä½é”™è¯¯å·ç§¯ã€‚å¯¹äºCommonVoice 12.0æ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œæ˜¾ç¤ºäº†ä½èµ„æºè¯­è¨€ä¸­çš„ significan reductions in Word Error Rate (WER)ï¼Œ highlighting the effectiveness of our approachã€‚è¿™é¡¹ç ”ç©¶å¯¹ä¸¤ä¸ªé˜¶æ®µASRç³»ç»Ÿçš„å‘å±•åœ¨ä½èµ„æºè¯­è¨€ä¸­åšå‡ºäº†è´¡çŒ®ï¼Œæä¾›äº†æ”¹è¿›çš„cross-lingual transfer learningçš„ potentialã€‚
</details></li>
</ul>
<hr>
<h2 id="Rethinking-E-Commerce-Search"><a href="#Rethinking-E-Commerce-Search" class="headerlink" title="Rethinking E-Commerce Search"></a>Rethinking E-Commerce Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03217">http://arxiv.org/abs/2312.03217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jacklinedesouza/STRATEGIES-OF-DIGITAL-MARKETING-AND-CONTENT-MARKETING">https://github.com/jacklinedesouza/STRATEGIES-OF-DIGITAL-MARKETING-AND-CONTENT-MARKETING</a></li>
<li>paper_authors: Haixun Wang, Taesik Na</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–°çš„ç”µå•†æœç´¢å’Œæ¨èç³»ç»Ÿï¼Œå¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸ç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å®¢æˆ·è¯„ä»·å’Œç½‘é¡µæ–‡ç« ç­‰ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ•°æ®ï¼Œç„¶åä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼ˆå¦‚å¤§è¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œæœç´¢å’Œæ¨èã€‚</li>
<li>results: è¯¥æ–¹æ³•å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨ä¸ç»“æ„åŒ–æ•°æ®ï¼Œæé«˜ç”µå•†æœç´¢å’Œæ¨èçš„ç²¾åº¦å’Œæ•ˆæœã€‚<details>
<summary>Abstract</summary>
E-commerce search and recommendation usually operate on structured data such as product catalogs and taxonomies. However, creating better search and recommendation systems often requires a large variety of unstructured data including customer reviews and articles on the web. Traditionally, the solution has always been converting unstructured data into structured data through information extraction, and conducting search over the structured data. However, this is a costly approach that often has low quality. In this paper, we envision a solution that does entirely the opposite. Instead of converting unstructured data (web pages, customer reviews, etc) to structured data, we instead convert structured data (product inventory, catalogs, taxonomies, etc) into textual data, which can be easily integrated into the text corpus that trains LLMs. Then, search and recommendation can be performed through a Q/A mechanism through an LLM instead of using traditional information retrieval methods over structured data.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç”µå•†æœç´¢å’Œæ¨èé€šå¸¸æ“ä½œäºç»“æ„åŒ–æ•°æ®such asäº§å“ç›®å½•å’Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œåˆ›å»ºæ›´å¥½çš„æœç´¢å’Œæ¨èç³»ç»Ÿç»å¸¸éœ€è¦å¤§é‡çš„æ— ç»“æ„æ•°æ®ï¼ŒåŒ…æ‹¬å®¢æˆ·è¯„ä»·å’Œç½‘ç»œä¸Šçš„æ–‡ç« ã€‚ä¼ ç»Ÿä¸Šï¼Œè§£å†³æ–¹æ¡ˆæ€»æ˜¯é€šè¿‡ä¿¡æ¯æŠ½å–å°†æ— ç»“æ„æ•°æ®è½¬æ¢ä¸ºç»“æ„æ•°æ®ï¼Œç„¶åè¿›è¡Œæœç´¢ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•é€šå¸¸æ˜¯æˆæœ¬é«˜ä¸”è´¨é‡ä½çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³è±¡ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå³å°†ç»“æ„æ•°æ®ï¼ˆäº§å“åº“ã€ç›®å½•ã€åˆ†ç±»ç­‰ï¼‰è½¬æ¢ä¸ºæ–‡æœ¬æ•°æ®ï¼Œå¯ä»¥è½»æ¾åœ°ä¸æ–‡æœ¬è®­ç»ƒLMsï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰é›†æˆã€‚ç„¶åï¼Œé€šè¿‡Q/Aæœºåˆ¶ï¼Œä½¿ç”¨LMè¿›è¡Œæœç´¢å’Œæ¨èè€Œä¸æ˜¯ä½¿ç”¨ä¼ ç»Ÿçš„ä¿¡æ¯æ£€ç´¢æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Detecting-Rumor-Veracity-with-Only-Textual-Information-by-Double-Channel-Structure"><a href="#Detecting-Rumor-Veracity-with-Only-Textual-Information-by-Double-Channel-Structure" class="headerlink" title="Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure"></a>Detecting Rumor Veracity with Only Textual Information by Double-Channel Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03195">http://arxiv.org/abs/2312.03195</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Kim, Sangwon Yoon</li>
<li>for: æœ¬æ–‡ç›®çš„æ˜¯æå‡ºä¸€ç§åŒé€šé“ç»“æ„ï¼Œç”¨äºåœ¨ç¤¾äº¤åª’ä½“ä¸Šé¢„å…ˆé‰´åˆ«è°£è¨€çš„çœŸå®æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸¤ç§æ–¹æ³•ï¼šä¸€æ˜¯lie detectionç®—æ³•ï¼Œç”¨äºæœ‰ä¿¡æ¯çš„è°£è¨€ï¼›äºŒæ˜¯thread-reply agreement detectionç®—æ³•ï¼Œç”¨äºæ— ä¿¡æ¯çš„è°£è¨€ã€‚</li>
<li>results: ä½¿ç”¨SemEval 2019 Task 7 datasetï¼Œæœ¬æ–‡çš„æ¨¡å‹åœ¨é¢„å…ˆä¸‰åˆ†ç±»ï¼ˆçœŸã€å‡ã€æœªé‰´åˆ«ï¼‰ç¤¾äº¤åª’ä½“è°£è¨€ä¸Šè·å¾—äº†0.4027çš„macro-F1åˆ†æ•°ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºelineæ¨¡å‹å’Œç¬¬äºŒåå¥– winnerï¼ˆGorrell et al., 2019ï¼‰ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜è¯å®äº†åŒé€šé“ç»“æ„çš„ä¼˜è¶Šæ€§ï¼Œæ¯”å•é€šé“ç»“æ„ä½¿ç”¨ lie detectionæˆ–agreement detectionç®—æ³•æ¥åˆ°æ‰€æœ‰å¸–å­ã€‚<details>
<summary>Abstract</summary>
Kyle (1985) proposes two types of rumors: informed rumors which are based on some private information and uninformed rumors which are not based on any information (i.e. bluffing). Also, prior studies find that when people have credible source of information, they are likely to use a more confident textual tone in their spreading of rumors. Motivated by these theoretical findings, we propose a double-channel structure to determine the ex-ante veracity of rumors on social media. Our ultimate goal is to classify each rumor into true, false, or unverifiable category. We first assign each text into either certain (informed rumor) or uncertain (uninformed rumor) category. Then, we apply lie detection algorithm to informed rumors and thread-reply agreement detection algorithm to uninformed rumors. Using the dataset of SemEval 2019 Task 7, which requires ex-ante threefold classification (true, false, or unverifiable) of social media rumors, our model yields a macro-F1 score of 0.4027, outperforming all the baseline models and the second-place winner (Gorrell et al., 2019). Furthermore, we empirically validate that the double-channel structure outperforms single-channel structures which use either lie detection or agreement detection algorithm to all posts.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‡¯å°”ï¼ˆ1985ï¼‰æå‡ºäº†ä¸¤ç§å¹å‘ï¼šæœ‰ä¿¡æ¯çš„å¹å‘å’Œæ— ä¿¡æ¯çš„å¹å‘ï¼ˆå³æ¶ä½œå‰§ï¼‰ã€‚æ­¤å¤–ï¼Œå…ˆå‰çš„ç ”ç©¶å‘ç°å½“äººä»¬æœ‰å¯é çš„ä¿¡æ¯æ¥æºæ—¶ï¼Œä»–ä»¬æ›´å¯èƒ½ä½¿ç”¨æ›´è‡ªä¿¡çš„æ–‡å­—è¯­è°ƒåœ¨å¹å‘æ¶ˆæ¯ã€‚åŸºäºè¿™äº›ç†è®ºå‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†åŒæ¸ é“ç»“æ„æ¥ç¡®å®šç¤¾äº¤åª’ä½“ä¸Šå¹å‘çš„é¢„å…ˆçœŸå®æ€§ã€‚æˆ‘ä»¬é¦–å…ˆå°†æ¯ä¸ªæ–‡æœ¬åˆ†ä¸ºç¡®å®šï¼ˆæœ‰ä¿¡æ¯å¹å‘ï¼‰æˆ–æœªç¡®å®šï¼ˆæ— ä¿¡æ¯å¹å‘ï¼‰ç±»åˆ«ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æœ‰ä¿¡æ¯å¹å‘åº”ç”¨äº†è°è¨€æ£€æµ‹ç®—æ³•ï¼Œå¯¹æ— ä¿¡æ¯å¹å‘åº”ç”¨äº†çº¿ç¨‹å›å¤ä¸€è‡´æ£€æµ‹ç®—æ³•ã€‚ä½¿ç”¨SemEval 2019ä»»åŠ¡7çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ä¸‰åˆ†ç±»ï¼ˆçœŸã€å‡ã€æœªçŸ¥ï¼‰é¢„å…ˆåˆ†ç±»ä»»åŠ¡ä¸­è·å¾—äº†0.4027çš„macro-F1åˆ†æ•°ï¼Œè¶…è¿‡äº†æ‰€æœ‰åŸºelineæ¨¡å‹å’Œç¬¬äºŒåå¥–å¾—è€…ï¼ˆGorrell et al., 2019ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç»éªŒ validateäº†åŒæ¸ é“ç»“æ„çš„ä¼˜è¶Šæ€§ï¼Œå®ƒåœ¨ä½¿ç”¨å•æ¸ é“ç»“æ„ï¼Œå…¶ä¸­ä½¿ç”¨è°è¨€æ£€æµ‹æˆ–çº¿ç¨‹å›å¤ä¸€è‡´æ£€æµ‹ç®—æ³•æ¥å¤„ç†æ‰€æœ‰å¹å‘æ—¶è¡¨ç°è¾ƒå·®ã€‚
</details></li>
</ul>
<hr>
<h2 id="Corporate-Bankruptcy-Prediction-with-Domain-Adapted-BERT"><a href="#Corporate-Bankruptcy-Prediction-with-Domain-Adapted-BERT" class="headerlink" title="Corporate Bankruptcy Prediction with Domain-Adapted BERT"></a>Corporate Bankruptcy Prediction with Domain-Adapted BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03194">http://arxiv.org/abs/2312.03194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Kim, Sangwon Yoon</li>
<li>for: è¿™ç ”ç©¶ä½¿ç”¨BERTæ¨¡å‹å¯¹å…¬å¸è´¢åŠ¡æŠ«éœ²æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œä»¥é¢„æµ‹å…¬å¸ç ´äº§ã€‚å…ˆå‰çš„ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨å¼€å‘æ›´åŠ å¤æ‚çš„é¢„æµ‹æ–¹æ³•ï¼Œä½¿ç”¨é‡‘èå˜é‡ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæé«˜è¾“å…¥æ•°æ®è´¨é‡ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¯¹MD&amp;AæŠ«éœ²ä¸­çš„æ–‡æœ¬è¿›è¡Œåˆ†æã€‚æˆ‘ä»¬å‘ç°ï¼ŒBERTæ¯”è¯å…¸åŸºäºé¢„æµ‹å’ŒWord2VecåŸºäºé¢„æµ‹æ›´é«˜æ•ˆï¼Œä»¥ adj R-squareã€kNN-5 å’Œçº¿æ€§æ”¯æŒå‘é‡æœºå™¨äººï¼ˆSVMï¼‰è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡è‡ªå­¦ä¹ ä¸ä¿¡ä»»æ»¡è¶³ç­›é€‰ï¼Œå¯ä»¥åœ¨10-K corporate disclosureæ•°æ®ä¸Šè¿›è¡Œè‡ªé€‚åº”é€‚åº”ã€‚æˆ‘ä»¬å®ç°äº†é¢„æµ‹ç²¾åº¦91.56%ï¼Œå¹¶è¯æ˜äº†é¢„æµ‹ç²¾åº¦å¾—åˆ°äº†æ˜¾è‘—æé«˜ã€‚<details>
<summary>Abstract</summary>
This study performs BERT-based analysis, which is a representative contextualized language model, on corporate disclosure data to predict impending bankruptcies. Prior literature on bankruptcy prediction mainly focuses on developing more sophisticated prediction methodologies with financial variables. However, in our study, we focus on improving the quality of input dataset. Specifically, we employ BERT model to perform sentiment analysis on MD&A disclosures. We show that BERT outperforms dictionary-based predictions and Word2Vec-based predictions in terms of adjusted R-square in logistic regression, k-nearest neighbor (kNN-5), and linear kernel support vector machine (SVM). Further, instead of pre-training the BERT model from scratch, we apply self-learning with confidence-based filtering to corporate disclosure data (10-K). We achieve the accuracy rate of 91.56% and demonstrate that the domain adaptation procedure brings a significant improvement in prediction accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä¸ªç ”ç©¶ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œåˆ†æï¼ŒBERTæ˜¯ä¸€ç§ä»£è¡¨æ€§çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥é¢„æµ‹å…¬å¸ç ´äº§ã€‚å…ˆå‰çš„æ–‡çŒ®å…³äºç ´äº§é¢„æµ‹ä¸»è¦é›†ä¸­åœ¨å¼€å‘æ›´å¤æ‚çš„é¢„æµ‹æ–¹æ³•ologiesï¼Œè€Œæˆ‘ä»¬çš„ç ”ç©¶åˆ™ä¸“æ³¨äºæé«˜è¾“å…¥æ•°æ®è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨BERTæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼Œå¹¶è¯æ˜BERTçš„æ€§èƒ½è¶…è¿‡è¯å…¸åŸºäºé¢„æµ‹å’ŒWord2VecåŸºäºé¢„æµ‹ã€‚æˆ‘ä»¬è¿˜é‡‡ç”¨äº†è‡ªå­¦ä¹ å’Œä¿¡ä»»åº¦åŸºäºç­›é€‰æ¥è‡ª10-KæŠ¥å‘Šæ•°æ®ï¼Œå®ç°äº†é¢„æµ‹ç²¾åº¦ä¸º91.56%ã€‚è¿™äº›ç»“æœè¡¨æ˜ï¼ŒåŸŸ adaptaptionç¨‹åºå¯ä»¥æä¾›æ˜¾è‘—çš„æ”¹å–„ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/cs.CL_2023_12_06/" data-id="clq0ru6sl00hcto882u5qbu9r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/cs.LG_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T10:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/06/cs.LG_2023_12_06/">cs.LG - 2023-12-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Understanding-the-Role-of-Optimization-in-Double-Descent"><a href="#Understanding-the-Role-of-Optimization-in-Double-Descent" class="headerlink" title="Understanding the Role of Optimization in Double Descent"></a>Understanding the Role of Optimization in Double Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03951">http://arxiv.org/abs/2312.03951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chris Yuhao Liu, Jeffrey Flanigan</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†æ¨¡å‹å¼ºåº¦é€æ¸å¢åŠ æ—¶æµ‹è¯•é”™è¯¯çš„å³°å€¼å’Œä¸‹é™ç°è±¡ï¼Œå³æ¨¡å‹å¼ºåº¦å¢åŠ åæµ‹è¯•é”™è¯¯å¯èƒ½ä¼šå¢åŠ æˆ–å‡å°‘ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¼˜åŒ–è§†è§’æ¥è§£é‡Šæ¨¡å‹å¼ºåº¦é€æ¸å¢åŠ æ—¶æµ‹è¯•é”™è¯¯çš„ç°è±¡ã€‚ç ”ç©¶è€…é€šè¿‡æ§åˆ¶ä¸åŒçš„åˆå§‹åŒ–ã€å½’ä¸€åŒ–ã€æ‰¹å¤„ç†å¤§å°ã€å­¦ä¹ ç‡å’Œä¼˜åŒ–ç®—æ³•æ¥è°ƒæŸ¥è¿™äº›å› ç´ å¯¹æ¨¡å‹å¼ºåº¦é€æ¸å¢åŠ æ—¶æµ‹è¯•é”™è¯¯çš„å½±å“ã€‚</li>
<li>results: ç ”ç©¶è€…å‘ç°äº†è®¸å¤šä¸åŒçš„å› ç´ ï¼ˆåˆå§‹åŒ–ã€å½’ä¸€åŒ–ã€æ‰¹å¤„ç†å¤§å°ã€å­¦ä¹ ç‡å’Œä¼˜åŒ–ç®—æ³•ï¼‰å¯¹æ¨¡å‹å¼ºåº¦é€æ¸å¢åŠ æ—¶æµ‹è¯•é”™è¯¯çš„å½±å“ï¼Œè¿™äº›å› ç´ ç›´æ¥å½±å“ä¼˜åŒ–é—®é¢˜çš„condition numberæˆ–ä¼˜åŒ–å™¨ï¼Œä»è€Œå½±å“æœ€ç»ˆå‘ç°çš„æœ€ä½ç‚¹ã€‚ç ”ç©¶è€…é€šè¿‡æ§åˆ¶è¿™äº›å› ç´ æ¥ç¤ºå‡ºäº†è¿™ç§ä¼˜åŒ–è§†è§’çš„åˆç†æ€§ã€‚<details>
<summary>Abstract</summary>
The phenomenon of model-wise double descent, where the test error peaks and then reduces as the model size increases, is an interesting topic that has attracted the attention of researchers due to the striking observed gap between theory and practice \citep{Belkin2018ReconcilingMM}. Additionally, while double descent has been observed in various tasks and architectures, the peak of double descent can sometimes be noticeably absent or diminished, even without explicit regularization, such as weight decay and early stopping. In this paper, we investigate this intriguing phenomenon from the optimization perspective and propose a simple optimization-based explanation for why double descent sometimes occurs weakly or not at all. To the best of our knowledge, we are the first to demonstrate that many disparate factors contributing to model-wise double descent (initialization, normalization, batch size, learning rate, optimization algorithm) are unified from the viewpoint of optimization: model-wise double descent is observed if and only if the optimizer can find a sufficiently low-loss minimum. These factors directly affect the condition number of the optimization problem or the optimizer and thus affect the final minimum found by the optimizer, reducing or increasing the height of the double descent peak. We conduct a series of controlled experiments on random feature models and two-layer neural networks under various optimization settings, demonstrating this optimization-based unified view. Our results suggest the following implication: Double descent is unlikely to be a problem for real-world machine learning setups. Additionally, our results help explain the gap between weak double descent peaks in practice and strong peaks observable in carefully designed setups.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ¨¡å‹å¼ºåº¦åŒå³°ç°è±¡â€ï¼Œå³æµ‹è¯•é”™è¯¯å³°å€¼ç„¶åé€æ¸ä¸‹é™ï¼Œæ˜¯ç ”ç©¶è€…å¸å¼•äº†å…³æ³¨çš„è¯é¢˜ï¼Œå› ä¸ºè§‚å¯Ÿåˆ°çš„ç†è®ºä¸å®è·µä¹‹é—´å­˜åœ¨å¸å¼•äººçš„ gap ã€‚æ­¤å¤–ï¼ŒåŒå³°ç°è±¡åœ¨ä¸åŒä»»åŠ¡å’Œæ¶æ„ä¸Šéƒ½æœ‰è¢«è§‚å¯Ÿåˆ°ï¼Œä½†æµ‹è¯•é”™è¯¯å³°å€¼å¯èƒ½ä¼šç¼ºå¤±æˆ–å‡å¼±ï¼Œç”šè‡³æ²¡æœ‰æ˜æ˜¾çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå¦‚æƒé‡è¡°å˜å’Œæ—©æœŸåœæ­¢ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»ä¼˜åŒ–è§†è§’æ¥è°ƒæŸ¥è¿™ä¸€ interessante ç°è±¡ï¼Œå¹¶æå‡ºä¸€ç§ç®€å•çš„ä¼˜åŒ–åŸºäºè§£é‡Šï¼šåŒå³°ç°è±¡åœ¨ä¼˜åŒ–è§†è§’ä¸‹å¯ä»¥è¢«è§£é‡Šä¸ºä¼˜åŒ–é—®é¢˜çš„condition numberç›´æ¥å½±å“æœ€ç»ˆæ‰¾åˆ°çš„æœ€ä½å€¼ã€‚æˆ‘ä»¬æ§åˆ¶äº†ä¸åŒçš„åˆå§‹åŒ–ã€æ ‡å‡†åŒ–ã€æ‰¹å¤„ç†å¤§å°ã€å­¦ä¹ ç‡å’Œä¼˜åŒ–ç®—æ³•ï¼Œå¹¶åœ¨random featureæ¨¡å‹å’ŒäºŒå±‚ç¥ç»ç½‘ç»œä¸Šè¿›è¡Œäº†ç³»åˆ—çš„æ§åˆ¶å®éªŒï¼Œè¯æ˜äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒåŒå³°ç°è±¡åœ¨å®é™…æœºå™¨å­¦ä¹ è®¾ç½®ä¸‹æ˜¯ä¸å¯èƒ½çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç»“æœè¿˜å¯ä»¥è§£é‡Šå®é™…ä¸­è§‚å¯Ÿåˆ°çš„åŒå³°å³°å€¼è¾ƒå¼±å’Œä»”ç»†è®¾ç½®ä¸‹è§‚å¯Ÿåˆ°çš„å¼ºåŒå³°å³°å€¼ä¹‹é—´çš„å·®å¼‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-and-Generalizable-Pathloss-Map-Prediction"><a href="#A-Scalable-and-Generalizable-Pathloss-Map-Prediction" class="headerlink" title="A Scalable and Generalizable Pathloss Map Prediction"></a>A Scalable and Generalizable Pathloss Map Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03950">http://arxiv.org/abs/2312.03950</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abman23/pmnet">https://github.com/abman23/pmnet</a></li>
<li>paper_authors: Ju-Hyung Lee, Andreas F. Molisch</li>
<li>for: é¢„æµ‹æ— çº¿ç½‘ç»œçš„é€šä¿¡èŒƒå›´ï¼Œå³åœ°ç†&#x2F;åœ°å½¢&#x2F;å»ºç­‘åœ°å›¾ä¸Šçš„å¹²æ‰°ç¨‹åº¦çš„ä¼°ç®—ã€‚</li>
<li>methods: ä½¿ç”¨æ•°æ®é©±åŠ¨ã€æ¨¡å‹æ— å…³çš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä½¿ç”¨æœ‰é™çš„å°„çº¿è§‚æµ‹ï¼ˆæˆ–é€šé“æµ‹é‡ï¼‰æ•°æ®å’Œåœ°å›¾æ•°æ®æ¥é¢„æµ‹é€šä¿¡èŒƒå›´ã€‚</li>
<li>results: å¯ä»¥åœ¨å‡ æ¯«ç§’å†…ï¼Œä½¿ç”¨æœ‰é™çš„æ•°æ®å’Œè®­ç»ƒï¼Œå®ç°é«˜ç²¾åº¦ï¼ˆRMSEçº§åˆ«ä¸º10^-2ï¼‰çš„é€šä¿¡èŒƒå›´é¢„æµ‹ï¼Œå¹¶ä¸”é€šè¿‡çŸ¥è¯†ä¼ æ’­æ¥å¿«é€Ÿåœ°ï¼ˆx5.6å¿«ï¼‰å’Œæ•ˆç‡åœ°ï¼ˆä½¿ç”¨x4.5å°‘çš„æ•°æ®ï¼‰é€‚åº”æ–°çš„ç½‘ç»œenarioã€‚<details>
<summary>Abstract</summary>
Large-scale channel prediction, i.e., estimation of the pathloss from geographical/morphological/building maps, is an essential component of wireless network planning. Ray tracing (RT)-based methods have been widely used for many years, but they require significant computational effort that may become prohibitive with the increased network densification and/or use of higher frequencies in B5G/6G systems. In this paper, we propose a data-driven, model-free pathloss map prediction (PMP) method, called PMNet. PMNet uses a supervised learning approach: it is trained on a limited amount of RT (or channel measurement) data and map data. Once trained, PMNet can predict pathloss over location with high accuracy (an RMSE level of $10^{-2}$) in a few milliseconds. We further extend PMNet by employing transfer learning (TL). TL allows PMNet to learn a new network scenario quickly (x5.6 faster training) and efficiently (using x4.5 less data) by transferring knowledge from a pre-trained model, while retaining accuracy. Our results demonstrate that PMNet is a scalable and generalizable ML-based PMP method, showing its potential to be used in several network optimization applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è§„æ¨¡é€šé“é¢„æµ‹ï¼Œå³åœ°ç†/å½¢æ€/å»ºç­‘å›¾å‡†ç¡®é¢„æµ‹ä¿¡å·å¼ºåº¦ï¼Œæ˜¯æ— çº¿ç½‘ç»œè§„åˆ’ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚åŸºäºå°„çº¿è¿½è¸ªï¼ˆRTï¼‰æ–¹æ³•å·²ç»å¹¿æ³›ä½¿ç”¨äº†å¾ˆå¤šå¹´ï¼Œä½†å®ƒä»¬éœ€è¦å¾ˆå¤§çš„è®¡ç®—åŠ›ï¼Œéšç€ç½‘ç»œ densification å’Œ/æˆ–ä½¿ç”¨æ›´é«˜é¢‘ç‡çš„ B5G/6G ç³»ç»Ÿï¼Œå¯èƒ½æˆä¸ºç¦åˆ¶æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ•°æ®é©±åŠ¨ã€æ¨¡å‹è‡ªç”±çš„é€šé“é¢„æµ‹æ–¹æ³•ï¼Œ called PMNetã€‚PMNet ä½¿ç”¨supervised learning æ–¹æ³•ï¼šå®ƒåœ¨æœ‰é™çš„ RTï¼ˆæˆ–é€šé“æµ‹é‡ï¼‰æ•°æ®å’Œåœ°å›¾æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼ŒPMNet å¯ä»¥å‡†ç¡®é¢„æµ‹é€šé“loss çš„ä½ç½®ï¼Œå¹¶ä¸”åœ¨å‡ æ¯«ç§’é’Ÿå†…å®Œæˆé¢„æµ‹ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ‰©å±• PMNet ï¼Œä½¿å…¶å¯ä»¥å¿«é€Ÿåœ°å­¦ä¹ æ–°çš„ç½‘ç»œåœºæ™¯ï¼ˆx5.6 å¿«é€Ÿè®­ç»ƒï¼‰ï¼Œå¹¶ä½¿ç”¨ x4.5  menos æ•°æ®æ¥å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒPMNet æ˜¯ä¸€ç§æ‰©å±•æ€§å’Œæ™®é€‚çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åŸºäº PMP æ–¹æ³•ï¼Œè¡¨æ˜å®ƒå¯ä»¥åœ¨å¤šç§ç½‘ç»œä¼˜åŒ–åº”ç”¨ä¸­ä½¿ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="PECANN-Parallel-Efficient-Clustering-with-Graph-Based-Approximate-Nearest-Neighbor-Search"><a href="#PECANN-Parallel-Efficient-Clustering-with-Graph-Based-Approximate-Nearest-Neighbor-Search" class="headerlink" title="PECANN: Parallel Efficient Clustering with Graph-Based Approximate Nearest Neighbor Search"></a>PECANN: Parallel Efficient Clustering with Graph-Based Approximate Nearest Neighbor Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03940">http://arxiv.org/abs/2312.03940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushangdi/pecann-dpc">https://github.com/yushangdi/pecann-dpc</a></li>
<li>paper_authors: Shangdi Yu, Joshua Engels, Yihao Huang, Julian Shun</li>
<li>for: æœ¬æ–‡ç ”ç©¶ç‚¹é›†æ‹Ÿåˆ clustering ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯åŸºäºå¯†é›†åº¦çš„ç‚¹é›† clustering ç®—æ³•ã€‚ç›®æ ‡æ˜¯å¤„ç†å¤§é‡é«˜ç»´æ•°æ®ï¼Œå¹¿æ³›å­˜åœ¨å®é™…åº”ç”¨ä¸­ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ PECANNï¼ŒæŠ½è±¡å‡ºäº†å¤šç§å¯†é›†ç‚¹é›† clustering ç®—æ³•çš„å…±åŒæ­¥éª¤ã€‚å…¶ä¸­ä¸€ä¸ªå…³é”®æ­¥éª¤æ˜¯æŸ¥æ‰¾æ»¡è¶³ predicate å‡½æ•°çš„æœ€è¿‘é‚»å±…ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„ predicate æœç´¢æ–¹æ³•ï¼Œå¹¶å¯ä»¥åº”ç”¨åˆ°è®¸å¤šç°æœ‰çš„å›¾åŸºäº ANNS ç®—æ³•ä¸­ã€‚</li>
<li>results: æœ¬æ–‡å®ç°äº†äº”ç§ clustering ç®—æ³•ï¼Œå¹¶å¯¹ synthetic å’Œå®é™…æ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ã€‚ä¸çŠ¶æ€è‰ºæ³• FASTDP ç®—æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„æœ€ä½³ç®—æ³•åœ¨é«˜ç»´åº¦æ•°æ®é›†ä¸Šæ¯” FASTDP å¿«é€Ÿ45å€-734å€ï¼Œè€Œä¸”ä¸çŠ¶æ€è‰ºæ³• parallel DPC-based ç®—æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡çš„ç®—æ³•åœ¨é«˜ç»´åº¦æ•°æ®é›†ä¸Šä¸¤ä¸ªæ•°é‡çº§æ›´å¿«ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ˜¯é¦–æ¬¡åœ¨å¤§é‡é«˜ç»´å®é™…å›¾åƒå’Œæ–‡æœ¬åµŒå…¥æ•°æ®é›†ä¸Šè¯„ä¼°äº† DPC å˜ç§ã€‚<details>
<summary>Abstract</summary>
This paper studies density-based clustering of point sets. These methods use dense regions of points to detect clusters of arbitrary shapes. In particular, we study variants of density peaks clustering, a popular type of algorithm that has been shown to work well in practice. Our goal is to cluster large high-dimensional datasets, which are prevalent in practice. Prior solutions are either sequential, and cannot scale to large data, or are specialized for low-dimensional data.   This paper unifies the different variants of density peaks clustering into a single framework, PECANN, by abstracting out several key steps common to this class of algorithms. One such key step is to find nearest neighbors that satisfy a predicate function, and one of the main contributions of this paper is an efficient way to do this predicate search using graph-based approximate nearest neighbor search (ANNS). To provide ample parallelism, we propose a doubling search technique that enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds. Our technique can be applied to many existing graph-based ANNS algorithms, which can all be plugged into PECANN.   We implement five clustering algorithms with PECANN and evaluate them on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. Compared to the state-of-the-art FASTDP algorithm for high-dimensional density peaks clustering, which is sequential, our best algorithm is 45x-734x faster while achieving competitive ARI scores. Compared to the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions, we show that PECANN is two orders of magnitude faster. As far as we know, our work is the first to evaluate DPC variants on large high-dimensional real-world image and text embedding datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
To address these limitations, this paper introduces PECANN, a unified framework that abstracts common steps among DPC algorithms and provides an efficient predicate search using graph-based approximate nearest neighbor search (ANNS). This enables points to find an approximate nearest neighbor satisfying the predicate in a small number of rounds, allowing for ample parallelism.The paper evaluates five clustering algorithms with PECANN on synthetic and real-world datasets with up to 1.28 million points and up to 1024 dimensions on a 30-core machine with two-way hyper-threading. The results show that PECANN is significantly faster than the state-of-the-art FASTDP algorithm for high-dimensional DPC clustering, achieving competitive ARI scores. PECANN is also two orders of magnitude faster than the state-of-the-art parallel DPC-based algorithm, which is optimized for low dimensions.Moreover, this paper is the first to evaluate DPC variants on large, high-dimensional real-world image and text embedding datasets, demonstrating the effectiveness of PECANN in practical applications. Overall, PECANN provides a scalable and efficient solution for clustering large, high-dimensional datasets using density-based methods.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Weighted-Co-Learning-for-Cross-Domain-Few-Shot-Learning"><a href="#Adaptive-Weighted-Co-Learning-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning"></a>Adaptive Weighted Co-Learning for Cross-Domain Few-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03928">http://arxiv.org/abs/2312.03928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Alchihabi, Marzi Heidari, Yuhong Guo</li>
<li>for:  Addressing the challenging adaptation problem in cross-domain few-shot learning (CDFSL) tasks, where there are only a few labeled instances available for the target prediction task and a significant domain shift between the well-annotated source domain and the target domain.</li>
<li>methods:  Propose a simple Adaptive Weighted Co-Learning (AWCoL) method that adapts two independently trained source prototypical classification models to the target task in a weighted co-learning manner. The method deploys a weighted moving average prediction strategy and conducts adaptive co-learning by jointly fine-tuning the two models based on the pseudo-labels and instance weights produced from the predictions.</li>
<li>results:  Produce state-of-the-art CDFSL performance on multiple benchmark datasets through comprehensive experiments.<details>
<summary>Abstract</summary>
Due to the availability of only a few labeled instances for the novel target prediction task and the significant domain shift between the well annotated source domain and the target domain, cross-domain few-shot learning (CDFSL) induces a very challenging adaptation problem. In this paper, we propose a simple Adaptive Weighted Co-Learning (AWCoL) method to address the CDFSL challenge by adapting two independently trained source prototypical classification models to the target task in a weighted co-learning manner. The proposed method deploys a weighted moving average prediction strategy to generate probabilistic predictions from each model, and then conducts adaptive co-learning by jointly fine-tuning the two models in an alternating manner based on the pseudo-labels and instance weights produced from the predictions. Moreover, a negative pseudo-labeling regularizer is further deployed to improve the fine-tuning process by penalizing false predictions. Comprehensive experiments are conducted on multiple benchmark datasets and the empirical results demonstrate that the proposed method produces state-of-the-art CDFSL performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
The proposed method uses a weighted moving average prediction strategy to generate probabilistic predictions from each model, and then conducts adaptive co-learning by jointly fine-tuning the two models in an alternating manner based on the pseudo-labels and instance weights produced from the predictions. Moreover, a negative pseudo-labeling regularizer is further deployed to improve the fine-tuning process by penalizing false predictions.Experiments conducted on multiple benchmark datasets show that the proposed method produces state-of-the-art CDFSL performance.Here's the translation in Simplified Chinese:ç”±äºç›®æ ‡é¢„æµ‹ä»»åŠ¡ä¸­çš„åªæœ‰å‡ ä¸ªæ ‡æ³¨æ•°æ®ï¼Œä»¥åŠæºé¢†åŸŸå’Œç›®æ ‡é¢†åŸŸä¹‹é—´çš„åŸŸåshiftï¼Œè·¨é¢†åŸŸå°‘æ ·æœ¬å­¦ä¹ ï¼ˆCDFSLï¼‰å…·æœ‰æå…¶æŒ‘æˆ˜çš„é€‚åº”é—®é¢˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„ adaptive weighted co-learningï¼ˆAWCoLï¼‰æ–¹æ³•ï¼Œä»¥é€‚åº”ä¸¤ä¸ªç‹¬ç«‹è®­ç»ƒçš„æºç±»prototypeåˆ†ç±»æ¨¡å‹åˆ°ç›®æ ‡ä»»åŠ¡ä¸­ã€‚è¯¥æ–¹æ³•ä½¿ç”¨æƒé‡ç§»åŠ¨å¹³å‡é¢„æµ‹ç­–ç•¥æ¥ç”Ÿæˆæ¯ä¸ªæ¨¡å‹çš„æ¦‚ç‡é¢„æµ‹ï¼Œç„¶åé€šè¿‡å¯¹ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œ alternate fine-tuning æ¥è¿›è¡Œé€‚åº”å­¦ä¹ ï¼ŒåŸºäºé¢„æµ‹ä¸­çš„pseudo-æ ‡ç­¾å’Œå®ä¾‹æƒé‡ã€‚æ­¤å¤–ï¼Œè¿˜éƒ¨ç½²äº†ä¸€ä¸ªè´Ÿ pseudo-æ ‡ç­¾ regularizerï¼Œä»¥æ”¹è¿› fine-tuning è¿‡ç¨‹ä¸­çš„å‡†ç¡®æ€§ã€‚å¯¹å¤šä¸ªbenchmarkæ•°æ®é›†è¿›è¡Œäº†å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ CDFSL æ€§èƒ½æ–¹é¢è¾¾åˆ°äº†å›½é™…å…ˆè¿›æ°´å¹³ã€‚
</details></li>
</ul>
<hr>
<h2 id="Improving-Gradient-guided-Nested-Sampling-for-Posterior-Inference"><a href="#Improving-Gradient-guided-Nested-Sampling-for-Posterior-Inference" class="headerlink" title="Improving Gradient-guided Nested Sampling for Posterior Inference"></a>Improving Gradient-guided Nested Sampling for Posterior Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03911">http://arxiv.org/abs/2312.03911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pablo-lemos/ggns">https://github.com/pablo-lemos/ggns</a></li>
<li>paper_authors: Pablo Lemos, Nikolay Malkin, Will Handley, Yoshua Bengio, Yashar Hezaveh, Laurence Perreault-Levasseur</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å¼€å‘ä¸€ç§é«˜æ€§èƒ½ã€é€šç”¨çš„æ¢¯åº¦å¯¼å¼•éšè—æ ·æœ¬ç®—æ³•ï¼ˆ${\tt GGNS}$ï¼‰ï¼Œç»“åˆäº†å¾®åˆ†ç¼–ç¨‹ã€å“ˆå¯†é¡¿æˆªé¢æŠ½æ ·ã€åµŒå¥—æŠ½æ ·ã€æ¨¡å¼åˆ†ç¦»ã€åŠ¨æ€åµŒå¥—æŠ½æ ·å’Œå¹¶è¡ŒåŒ–ç­‰æŠ€æœ¯ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å¾®åˆ†ç¼–ç¨‹ã€å“ˆå¯†é¡¿æˆªé¢æŠ½æ ·ã€åµŒå¥—æŠ½æ ·ã€æ¨¡å¼åˆ†ç¦»ã€åŠ¨æ€åµŒå¥—æŠ½æ ·å’Œå¹¶è¡ŒåŒ–ç­‰æ–¹æ³•ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡ä½¿ç”¨${\tt GGNS}$ç®—æ³•ï¼Œåœ¨ä¸åŒçš„ sintetic å’Œå®é™…é—®é¢˜ä¸Šè¾¾åˆ°äº†æ¯”è¾ƒå¥½çš„çº§åˆ«æ€§å’Œç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œå°†éšè—æ ·æœ¬ç®—æ³•ä¸ç”Ÿæˆæµç½‘ç»œç»“åˆä½¿ç”¨ï¼Œå¯ä»¥æ›´å¿«åœ°å‘ç°æ¨¡å¼å’Œæ›´å‡†ç¡®åœ°ä¼°è®¡ posterior åˆ†å¸ƒçš„partition fonctionã€‚<details>
<summary>Abstract</summary>
We present a performant, general-purpose gradient-guided nested sampling algorithm, ${\tt GGNS}$, combining the state of the art in differentiable programming, Hamiltonian slice sampling, clustering, mode separation, dynamic nested sampling, and parallelization. This unique combination allows ${\tt GGNS}$ to scale well with dimensionality and perform competitively on a variety of synthetic and real-world problems. We also show the potential of combining nested sampling with generative flow networks to obtain large amounts of high-quality samples from the posterior distribution. This combination leads to faster mode discovery and more accurate estimates of the partition function.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘å›¢é˜Ÿæå‡ºäº†ä¸€ç§é«˜æ€§èƒ½ã€é€šç”¨çš„æ¢¯åº¦å¯¼å¼•å†…æ ·æœ¬ç®—æ³•ï¼ˆ{\tt GGNSï¼‰ï¼Œ combining the state of the art in differentiable programming, Hamiltonian slice sampling, clustering, mode separation, dynamic nested sampling, and parallelization. This unique combination allows {\tt GGNS} to scale well with dimensionality and perform competitively on a variety of synthetic and real-world problems. We also show the potential of combining nested sampling with generative flow networks to obtain large amounts of high-quality samples from the posterior distribution. This combination leads to faster mode discovery and more accurate estimates of the partition function.Here's the breakdown of the translation:* é«˜æ€§èƒ½ (gÄo xÃ¬ng nÃ©ng) - high performance* é€šç”¨ (tÅng yÃ²ng) - general-purpose* æ¢¯åº¦å¯¼å¼•å†…æ ·æœ¬ç®—æ³• (jÃ¬ duÄn yÃ¹ xiÃ o yÃ¬) - gradient-guided nested sampling algorithm* æ¢¯åº¦ç¨‹åº¦ (jÃ¬ duÄn) - gradient* å¯¼å¼• (dÇo yÇn) - guided* å†…æ ·æœ¬ (nÃ¨i yÃ ng bÃ¨i) - nested sampling* ç®—æ³• (suÄn fÄ) - algorithm*  combining (combine) - combining*  state of the art (jÃ¬ yÃ¬ zhÃ¬) - state of the art* æ–œåˆ‡ (shuÄ zhÃ¬) - slice* æ•£åˆ— (pÄn jiÃ¨) - clustering* æ¨¡å¼åˆ†ç¦» (mÃ³ xing fÄ“n liÃ¨) - mode separation* åŠ¨æ€å†…æ ·æœ¬ (dÃ²ng tÇ nÃ¨i yÃ ng bÃ¨i) - dynamic nested sampling* å¹¶è¡Œ (bÃ¬ng xÃ­ng) - parallelization* é«˜ç»´åº¦ (gÄo wÃ©idÃ¹) - high dimensionality* ç«äº‰ (jiÃ ng zhÃ¬) - competitive* synthetic (shÃ¨ng chÇng) - synthetic* å®é™… (shÃ­ jÃ­) - real-world* é—®é¢˜ (wÃ¨n tÃ­) - problems* å¯èƒ½ (kÄ› nÃ©ng) - possible* ç»„åˆ (zÇ” xiÃ ng) - combining* å†…æ ·æœ¬ (nÃ¨i yÃ ng bÃ¨i) - nested sampling* æµåŠ¨ç½‘ç»œ (liÃº dÃ²ng wÇng wÇn) - generative flow networks* è·å– (huÃ² qÃ¹) - obtain* å¤§é‡ (dÃ  liÃ ng) - large amounts* é«˜è´¨é‡ (gÄo zhÃ¬ yÃ¹) - high-quality* æ ·æœ¬ (yÃ ng bÃ¨i) - samples*  posterior distribution (åé¢„åˆ†å¸ƒ) - posterior distribution* æ€»ä½“ (zÇ’ng tÇ) - overall*  faster (fÄ jÃ­) - faster*  mode discovery (mÃ³ yÇn jÃ­) - mode discovery* æ›´åŠ  (gÃ¨ng jÄ«) - more* å‡†ç¡® (zhÃ¨ng qiÃº) - accurate* ä¼°è®¡ (gueshÃ¬) - estimate* partition function (åˆ†é…å‡½æ•°) - partition function
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Dependency-Learning-Graph-Neural-Networks"><a href="#Adaptive-Dependency-Learning-Graph-Neural-Networks" class="headerlink" title="Adaptive Dependency Learning Graph Neural Networks"></a>Adaptive Dependency Learning Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03903">http://arxiv.org/abs/2312.03903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abisheksriramulu/adlgnn">https://github.com/abisheksriramulu/adlgnn</a></li>
<li>paper_authors: Abishek Sriramulu, Nicolas Fourrier, Christoph Bergmeir</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªç»“åˆç¥ç»ç½‘ç»œå’Œç»Ÿè®¡ç»“æ„å­¦æ¨¡å‹çš„èåˆæ–¹æ³•ï¼Œä»¥è‡ªåŠ¨å­¦ä¹ å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»å’Œå»ºç«‹åŠ¨æ€å˜åŒ–çš„ä¾èµ–å…³ç³»å›¾ï¼Œå¹¶å…è®¸ç”¨äºå¤šå˜é‡é¢„æµ‹é—®é¢˜ï¼Œç”šè‡³åœ¨çœŸå®ä¸–ç•Œä¸­çš„é›¶é¢„è®¾å›¾å½¢ä¸­ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºçš„æ–¹æ³•ç»“åˆäº†ç¥ç»ç½‘ç»œå’Œç»Ÿè®¡ç»“æ„å­¦æ¨¡å‹ï¼Œé€šè¿‡å†…åœ¨çš„å¾æµ‹å’Œç»Ÿè®¡å­¦ç»Ÿè®¡å­¦æ¨¡å‹æ¥è‡ªåŠ¨å­¦ä¹ å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­çš„ä¾èµ–å…³ç³»ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºåŠ¨æ€å˜åŒ–çš„ä¾èµ–å…³ç³»å›¾ã€‚</li>
<li>results: æœ¬æ–‡è¿è¡Œäºå®é™…ä¸–ç•Œçš„å®éªŒæ•°æ®ä¸Šï¼Œä¸ä¼ ç»Ÿçš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºäº†èåˆæ–¹æ³•çš„æ˜æ˜¾æ”¹å–„ï¼Œå…·ä½“æ¥è¯´ï¼Œåœ¨å¤šå˜é‡é¢„æµ‹é—®é¢˜ä¸­ï¼Œèåˆæ–¹æ³•çš„è¯¯å·®ç‡è¾ƒä½ï¼Œè€Œä¸”å¯ä»¥æ›´å¥½åœ°æ•æ‰å¤šå˜é‡æ—¶é—´åºåˆ—ä¸­çš„å¤æ‚å…³ç³»ã€‚<details>
<summary>Abstract</summary>
Graph Neural Networks (GNN) have recently gained popularity in the forecasting domain due to their ability to model complex spatial and temporal patterns in tasks such as traffic forecasting and region-based demand forecasting. Most of these methods require a predefined graph as input, whereas in real-life multivariate time series problems, a well-predefined dependency graph rarely exists. This requirement makes it harder for GNNs to be utilised widely for multivariate forecasting problems in other domains such as retail or energy. In this paper, we propose a hybrid approach combining neural networks and statistical structure learning models to self-learn the dependencies and construct a dynamically changing dependency graph from multivariate data aiming to enable the use of GNNs for multivariate forecasting even when a well-defined graph does not exist. The statistical structure modeling in conjunction with neural networks provides a well-principled and efficient approach by bringing in causal semantics to determine dependencies among the series. Finally, we demonstrate significantly improved performance using our proposed approach on real-world benchmark datasets without a pre-defined dependency graph.
</details>
<details>
<summary>æ‘˜è¦</summary>
graph neural networks (GNN) åœ¨é¢„æµ‹é¢†åŸŸä¸­æœ€è¿‘å—åˆ°æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ¨¡å‹å¤æ‚çš„ç©ºé—´å’Œæ—¶é—´æ¨¡å¼ï¼Œå¦‚äº¤é€šé¢„æµ‹å’Œåœ°åŸŸåŸºç¡€éœ€æ±‚é¢„æµ‹ã€‚å¤§å¤šæ•°è¿™äº›æ–¹æ³•éœ€è¦ä¸€ä¸ªé¢„å®šä¹‰çš„å›¾ä½œä¸ºè¾“å…¥ï¼Œè€Œåœ¨å®é™…ç”Ÿæ´»ä¸­å¤šå˜é‡æ—¶é—´åºåˆ—é—®é¢˜ä¸­ï¼Œä¸€ä¸ªå‡†ç¡®å®šä¹‰çš„ä¾èµ–å›¾å¾ˆå°‘å‡ºç°ã€‚è¿™ä¸€è¦æ±‚ä½¿å¾—GNNåœ¨å¤šå˜é‡é¢„æµ‹é—®é¢˜ä¸­æ›´éš¾è¢«å¹¿æ³›åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é›¶å”®æˆ–èƒ½æºé¢†åŸŸã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ··åˆæ–¹æ³•ï¼Œå°†ç¥ç»ç½‘ç»œå’Œç»Ÿè®¡ç»“æ„å­¦æ¨¡å‹ç›¸ç»“åˆï¼Œä»¥è‡ªåŠ¨å­¦ä¹ ä¾èµ–å…³ç³»å¹¶ä»å¤šå˜é‡æ•°æ®ä¸­åŠ¨æ€ç”Ÿæˆä¾èµ–å›¾ï¼Œä»¥ä¾¿ä½¿ç”¨GNNè¿›è¡Œå¤šå˜é‡é¢„æµ‹ï¼Œå³ä½¿æ²¡æœ‰æ˜ç¡®çš„ä¾èµ–å›¾ã€‚ç»Ÿè®¡ç»“æ„æ¨¡å‹å’Œç¥ç»ç½‘ç»œçš„ç»“åˆä½¿å¾—æˆ‘ä»¬å¯ä»¥æœ‰æ•ˆåœ°å¸¦æ¥ causal  semantics æ¥ç¡®å®šå¤šå˜é‡ç³»åˆ—ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å®é™…benchmarkæ•°æ®ä¸Šç¤ºå‡ºäº†æ˜æ˜¾æé«˜çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="HLoOP-â€“-Hyperbolic-2-space-Local-Outlier-Probabilities"><a href="#HLoOP-â€“-Hyperbolic-2-space-Local-Outlier-Probabilities" class="headerlink" title="HLoOP â€“ Hyperbolic 2-space Local Outlier Probabilities"></a>HLoOP â€“ Hyperbolic 2-space Local Outlier Probabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03895">http://arxiv.org/abs/2312.03895</a></li>
<li>repo_url: None</li>
<li>paper_authors: ClÃ©mence Allietta, Jean-Philippe Condomines, Jean-Yves Tourneret, Emmanuel Lochin</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§ç®€å•çš„æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹é¢‘ç¹åœ°è®¿é—®çš„æ•°æ®ç‚¹ï¼Œä»¥ä¾¿è¿›è¡Œåç»­å¤„ç†ã€‚</li>
<li>methods: æœ¬æ–¹æ³•åŸºäºè®¡ç®—æ•°æ®ç‚¹ä¸æœ€è¿‘çš„è¿‘é‚»çš„é‡Œæ›¼å°¼å®‰è·ç¦»ï¼Œå¹¶ä½¿ç”¨é«˜probabilityåº¦çš„Gaussianéšæœºåˆ†å¸ƒæ¥è¡¨ç¤ºè¿™ä¸ªè·ç¦»ã€‚</li>
<li>results: åœ¨WordNetæ•°æ®é›†ä¸Šæµ‹è¯•äº†æœ¬æ–¹æ³•ï¼Œå¹¶å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚Hereâ€™s the full translation of the paperâ€™s abstract in Simplified Chinese:</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§ç®€å•çš„æ£€æµ‹æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹é¢‘ç¹åœ°è®¿é—®çš„æ•°æ®ç‚¹ï¼Œä»¥ä¾¿è¿›è¡Œåç»­å¤„ç†ã€‚</li>
<li>methods: æœ¬æ–¹æ³•åŸºäºè®¡ç®—æ•°æ®ç‚¹ä¸æœ€è¿‘çš„è¿‘é‚»çš„é‡Œæ›¼å°¼å®‰è·ç¦»ï¼Œå¹¶ä½¿ç”¨é«˜probabilityåº¦çš„Gaussianéšæœºåˆ†å¸ƒæ¥è¡¨ç¤ºè¿™ä¸ªè·ç¦»ã€‚</li>
<li>results: åœ¨WordNetæ•°æ®é›†ä¸Šæµ‹è¯•äº†æœ¬æ–¹æ³•ï¼Œå¹¶å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚Note that the word â€œé«˜probabilityåº¦â€ in the methods section is a bit tricky to translate, as it means â€œhigh probabilityâ€ in English, but itâ€™s a bit more nuanced in Chinese. In Chinese, â€œé«˜probabilityåº¦â€ is often used to refer to a high probability distribution, rather than just a high probability value. So in this context, the phrase â€œé«˜probabilityåº¦çš„Gaussianéšæœºåˆ†å¸ƒâ€ is saying that the method uses a high probability distribution (i.e., a Gaussian distribution) to represent the distance between the data point and its nearest neighbors.<details>
<summary>Abstract</summary>
Hyperbolic geometry has recently garnered considerable attention in machine learning due to its capacity to embed hierarchical graph structures with low distortions for further downstream processing. This paper introduces a simple framework to detect local outliers for datasets grounded in hyperbolic 2-space referred to as HLoOP (Hyperbolic Local Outlier Probability). Within a Euclidean space, well-known techniques for local outlier detection are based on the Local Outlier Factor (LOF) and its variant, the LoOP (Local Outlier Probability), which incorporates probabilistic concepts to model the outlier level of a data vector. The developed HLoOP combines the idea of finding nearest neighbors, density-based outlier scoring with a probabilistic, statistically oriented approach. Therefore, the method consists in computing the Riemmanian distance of a data point to its nearest neighbors following a Gaussian probability density function expressed in a hyperbolic space. This is achieved by defining a Gaussian cumulative distribution in this space. The HLoOP algorithm is tested on the WordNet dataset yielding promising results. Code and data will be made available on request for reproductibility.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-Infrastructure-based-Warning-System-on-Driving-Behaviors-A-Roundabout-Study"><a href="#Evaluation-of-Infrastructure-based-Warning-System-on-Driving-Behaviors-A-Roundabout-Study" class="headerlink" title="Evaluation of Infrastructure-based Warning System on Driving Behaviors-A Roundabout Study"></a>Evaluation of Infrastructure-based Warning System on Driving Behaviors-A Roundabout Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03891">http://arxiv.org/abs/2312.03891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Zhang, Chi Tian, Tianfang Han, Hang Li, Yiheng Feng, Yunfeng Chen, Robert W. Proctor, Jiansong Zhang</li>
<li>for: è¿™ç¯‡è®ºæ–‡ investigateäº†åŸºç¡€è®¾æ–½å‘é€åˆ°é™„è¿‘è¡Œé©¶è€…çš„é€šä¿¡ warnings å¯¹å¼¯é“å®‰å…¨çš„å½±å“ï¼Œä»¥å¸®åŠ©æ”¹å–„é“è·¯å®‰å…¨ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªåˆå¹¶ SUMO å’Œ Webots çš„é©¾é©¶ simulate å¹³å°ï¼Œå¹¶åœ¨è¯¥å¹³å°ä¸Šæ¨¡æ‹Ÿäº†ä¸€ä¸ªå®é™…å­˜åœ¨çš„å¼¯é“ï¼Œä»¥ä¾¿è¿›è¡Œç ”ç©¶ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œæå‰å‘é€è­¦å‘Šå¯ä»¥å¸®åŠ©é©¾é©¶å‘˜æ›´å¥½åœ°é€‚åº”å¼¯é“é©¾é©¶ï¼Œå¹¶å‡å°‘çªç„¶å‡é€Ÿå’Œå‰§çƒˆåˆ¹è½¦ã€‚æ­¤å¤–ï¼ŒåŸºäºé©¾é©¶å‘˜åœè½¦æˆ–åŠ é€Ÿå†³ç­–çš„ä¸ªæ€§åŒ–é¢„æµ‹æ¨¡å‹ä¹Ÿè¢«å¼€å‘å‡ºæ¥ã€‚<details>
<summary>Abstract</summary>
Smart intersections have the potential to improve road safety with sensing, communication, and edge computing technologies. Perception sensors installed at a smart intersection can monitor the traffic environment in real time and send infrastructure-based warnings to nearby travelers through V2X communication. This paper investigated how infrastructure-based warnings can influence driving behaviors and improve roundabout safety through a driving-simulator study - a challenging driving scenario for human drivers. A co-simulation platform integrating Simulation of Urban Mobility (SUMO) and Webots was developed to serve as the driving simulator. A real-world roundabout in Ann Arbor, Michigan was built in the co-simulation platform as the study area, and the merging scenarios were investigated. 36 participants were recruited and asked to navigate the roundabout under three danger levels (e.g., low, medium, high) and three collision warning designs (e.g., no warning, warning issued 1 second in advance, warning issued 2 seconds in advance). Results indicated that advanced warnings can significantly enhance safety by minimizing potential risks compared to scenarios without warnings. Earlier warnings enabled smoother driver responses and reduced abrupt decelerations. In addition, a personalized intention prediction model was developed to predict drivers' stop-or-go decisions when the warning is displayed. Among all tested machine learning models, the XGBoost model achieved the highest prediction accuracy with a precision rate of 95.56% and a recall rate of 97.73%.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ™ºèƒ½äº¤å‰å£å…·æœ‰æ”¹å–„äº¤é€šå®‰å…¨çš„æ½œåŠ›ï¼Œé€šè¿‡æ„ŸçŸ¥ã€é€šä¿¡å’Œè¾¹ç¼˜è®¡ç®—æŠ€æœ¯ã€‚æ™ºèƒ½äº¤å‰å£ä¸­çš„æ„ŸçŸ¥ä¼ æ„Ÿå™¨å¯ä»¥åœ¨å®æ—¶ç›‘æµ‹äº¤é€šç¯å¢ƒä¸­ï¼Œé€šè¿‡V2Xé€šä¿¡å‘é€åˆ°é™„è¿‘äº¤é€šå‚ä¸è€…çš„åŸºç¡€è®¾æ–½è­¦ç¤ºã€‚è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†åŸºç¡€è®¾æ–½è­¦ç¤ºå¦‚ä½•å½±å“é©¾é©¶è¡Œä¸ºï¼Œæé«˜ç¯å¢ƒåœˆå®‰å…¨æ€§ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé›†æˆSUMOå’ŒWebotsçš„åˆä½œå¹³å°ï¼Œä½œä¸ºé©¾é©¶æ¨¡æ‹Ÿå™¨ã€‚ä¸€ä¸ªä½äºç¾å›½å¯†æ­‡æ ¹å·å®‰é‚£ä¼¯Ğ³Ğ¾ç‘Ÿçš„å®é™…ç¯å¢ƒåœˆè¢«å»ºç«‹åœ¨åˆä½œå¹³å°ä¸­ï¼Œå¹¶ investigateäº†èåˆåœºæ™¯ã€‚36åå‚ä¸è€…è¢«å¾é›†ï¼Œå¹¶è¢«è¦æ±‚åœ¨ä¸‰ç§å±é™©æ°´å¹³ï¼ˆä½ã€ä¸­ã€é«˜ï¼‰å’Œä¸‰ç§Collision warningè®¾è®¡ï¼ˆæ— è­¦ç¤ºã€1ç§’å‰å‘é€è­¦ç¤ºã€2ç§’å‰å‘é€è­¦ç¤ºï¼‰ä¸‹è¿›è¡Œé©¾é©¶ã€‚ç»“æœè¡¨æ˜ï¼Œæå‰å‘é€è­¦ç¤ºå¯ä»¥æ˜¾è‘—æé«˜å®‰å…¨æ€§ï¼Œæœ€å°åŒ–é£é™©ã€‚EARLIERè­¦ç¤ºä½¿å¾—å¸æœºæ›´smoothçš„å“åº”ï¼Œé™ä½äº†çªç„¶å‡é€Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼€å‘äº†ä¸€ä¸ªä¸ªæ€§åŒ–æ„å›¾é¢„æµ‹æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹å¸æœºåœ¨è­¦ç¤ºæ˜¾ç¤ºæ—¶çš„åœè½¦æˆ–å‰è¿›å†³ç­–ã€‚ Among all tested machine learning models, the XGBoost model achieved the highest prediction accuracy with a precision rate of 95.56% and a recall rate of 97.73%.
</details></li>
</ul>
<hr>
<h2 id="Adapting-Newtonâ€™s-Method-to-Neural-Networks-through-a-Summary-of-Higher-Order-Derivatives"><a href="#Adapting-Newtonâ€™s-Method-to-Neural-Networks-through-a-Summary-of-Higher-Order-Derivatives" class="headerlink" title="Adapting Newtonâ€™s Method to Neural Networks through a Summary of Higher-Order Derivatives"></a>Adapting Newtonâ€™s Method to Neural Networks through a Summary of Higher-Order Derivatives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03885">http://arxiv.org/abs/2312.03885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Wolinski</li>
<li>for: è¿™ç§ gradient-based ä¼˜åŒ–æ–¹æ³•é€‚ç”¨äºä¸€ä¸ªå‡½æ•° $\mathcal{L}$ ä¸­çš„ä¸€ä¸ª Ğ²ĞµĞºÑ‚Ğ¾Ñ€å˜é‡ $\boldsymbol{\theta}$ï¼Œå½“ $\boldsymbol{\theta}$ æ˜¯ä¸€ä¸ªtensor tuples $({\mathbf{T}_1, \ldots, {\mathbf{T}_S)$ çš„å½¢å¼æ—¶ã€‚è¿™ä¸ªæ¡†æ¶åŒ…æ‹¬è®¸å¤šå¸¸è§çš„åº”ç”¨åœºæ™¯ï¼Œå¦‚è®­ç»ƒç¥ç»ç½‘ç»œã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¡ç®—æˆæœ¬ä½çš„æŠ€æœ¯ï¼Œä»¥è·å– $\mathcal{L}$ ä¸­æ›´é«˜é˜¶ä¿¡æ¯ï¼Œå°¤å…¶æ˜¯tensor $\mathbf{T}_s$ ä¹‹é—´çš„äº¤äº’ä¿¡æ¯ï¼ŒåŸºäºè‡ªåŠ¨å¯¼æ•°å’Œè®¡ç®—æŠ€å·§ã€‚è¿™ç§æŠ€æœ¯åœ¨ç¬¬äºŒé˜¶æ®µä½¿ç”¨ï¼Œç”¨äºå»ºç«‹ä¸€ç§ç¬¬äºŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•ã€‚</li>
<li>results: æˆ‘ä»¬ä½¿ç”¨è¿™ç§æŠ€æœ¯åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå¹¶åˆ©ç”¨åˆ†åŒºç»“æ„æ¥æ„å»ºä¸€ç§é€‚ç”¨äºä¸åŒç¥ç»ç½‘ç»œ arquitectures çš„ç¬¬äºŒé˜¶æ®µä¼˜åŒ–æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä¸éœ€è¦è®¡ç®— $\mathcal{L}$ ä¸­çš„æ¢¯åº¦çŸ©é˜µæˆ–å…¶approximationï¼Œå¹¶ä¸”ä¸å¿½ç•¥å±‚ä¹‹é—´çš„äº¤äº’ã€‚åœ¨contrast to many existing practical second-order methods used in neural networks, which perform a diagonal or block-diagonal approximation of the Hessian or its inverse, our method does not neglect interactions between layersã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ†åŒºç²’åº¦æ¥è°ƒæ•´ä¼˜åŒ–æ–¹æ³•ï¼Œä»æœ€ç²—ç³™çš„caseï¼ˆCauchyçš„æœ€é™¡ä¸‹é™æ³•ï¼‰åˆ°æœ€ç»†ç²’åº¦çš„caseï¼ˆusual Newtonâ€™s methodï¼‰ã€‚<details>
<summary>Abstract</summary>
We consider a gradient-based optimization method applied to a function $\mathcal{L}$ of a vector of variables $\boldsymbol{\theta}$, in the case where $\boldsymbol{\theta}$ is represented as a tuple of tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\mathcal{L}$, especially about the interactions between the tensors $\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\boldsymbol{\theta}$ into tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\mathcal{L}$ according to $\boldsymbol{\theta}$, nor any approximation of it. The key part consists in computing a smaller matrix interpretable as a "Hessian according to the partition", which can be computed exactly and efficiently. In contrast to many existing practical second-order methods used in neural networks, which perform a diagonal or block-diagonal approximation of the Hessian or its inverse, the method we propose does not neglect interactions between layers. Finally, we can tune the coarseness of the partition to recover well-known optimization methods: the coarsest case corresponds to Cauchy's steepest descent method, the finest case corresponds to the usual Newton's method.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ¢¯åº¦åŸºæœ¬ä¼˜åŒ–æ–¹æ³•ï¼Œåº”ç”¨äºä¸€ä¸ªå‡½æ•° $\mathcal{L}$ ä¸­çš„ä¸€ç»„å‚æ•° $\boldsymbol{\theta}$ï¼Œåœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œ$\boldsymbol{\theta}$ æ˜¯ä¸€ä¸ªå…ƒç»„ä¸­çš„å¤šä¸ªtensorï¼ˆ$\mathbf{T}_1, \cdots, \mathbf{T}_S$ï¼‰ã€‚è¿™ä¸ªæ¡†æ¶åŒ…æ‹¬è®­ç»ƒç¥ç»ç½‘ç»œçš„å„ç§å¸¸ç”¨æ¡ˆä¾‹ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™ã€‚æˆ‘ä»¬é¦–å…ˆæå‡ºä¸€ä¸ª computationally inexpensive çš„æŠ€æœ¯ï¼Œå¯ä»¥æä¾›æ›´é«˜é˜¶çš„ä¿¡æ¯å…³äº $\mathcal{L}$ï¼Œç‰¹åˆ«æ˜¯å…³äº tensor $\mathbf{T}_s$ ä¹‹é—´çš„äº’åŠ¨ï¼ŒåŸºäºè‡ªåŠ¨æ¢¯åº¦åˆ†æå’Œè®¡ç®—æŠ€å·§ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæŠ€æœ¯åœ¨ç¬¬äºŒé˜¶å±‚ä¸Šï¼Œå»ºç«‹ä¸€ä¸ªç¬¬äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•é€‚ç”¨äºè®­ç»ƒå„ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶ä¸”ä¸éœ€è¦è®¡ç®— $\mathcal{L}$ çš„æ¢¯åº¦æˆ–å…¶é€†ï¼Œä¹Ÿä¸éœ€è¦ä»»ä½•æ¢¯åº¦æˆ–é€†çš„ Approximationã€‚å…³é”®éƒ¨åˆ†æ˜¯è®¡ç®—ä¸€ä¸ªå°å‹çš„çŸ©é˜µï¼Œå¯ä»¥ç†è§£ä¸º "Hessian according to the partition"ï¼Œè¿™ä¸ªçŸ©é˜µå¯ä»¥å®é™…å’Œé«˜æ•ˆåœ°è®¡ç®—ã€‚ä¸è®¸å¤šç°æœ‰çš„å®ç”¨ç¬¬äºŒé˜¶æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸å¿½ç•¥å±‚æ¬¡ä¹‹é—´çš„äº’åŠ¨ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´ç»„åˆ†çš„ç²—ç³™åº¦ï¼Œä»¥å›å¤çŸ¥åçš„ä¼˜åŒ–æ–¹æ³•ï¼šæœ€ç²—ç³™çš„ ÑĞ»ÑƒÑ‡Ğ° corresponds to Cauchy's steepest descent methodï¼Œæœ€ç»†çš„ caso corresponds to the usual Newton's methodã€‚
</details></li>
</ul>
<hr>
<h2 id="Domain-constraints-improve-risk-prediction-when-outcome-data-is-missing"><a href="#Domain-constraints-improve-risk-prediction-when-outcome-data-is-missing" class="headerlink" title="Domain constraints improve risk prediction when outcome data is missing"></a>Domain constraints improve risk prediction when outcome data is missing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03878">http://arxiv.org/abs/2312.03878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sidhika Balachandar, Nikhil Garg, Emma Pierson</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº†å‡†ç¡®ä¼°è®¡ç—…äººçš„é£é™©ï¼ŒåŒ…æ‹¬æµ‹è¯•å’Œæœªæµ‹è¯•çš„ç—…äººã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† bayesian æ¨¡å‹ï¼Œ capture äº†åŒ»ç”Ÿåšå‡ºå†³å®šåï¼Œæµ‹è¯•ç»“æœä¸å¯è§çš„é—®é¢˜ã€‚</li>
<li>results: è¯¥æ¨¡å‹å¯ä»¥å‡†ç¡®åœ°ä¼°è®¡ç—…äººçš„é£é™©ï¼Œå¹¶ä¸”å¯ä»¥æ•æ‰åˆ°åŒ»ç”Ÿåœ¨åšå‡ºå†³å®šæ—¶çš„ä¸“ä¸šçŸ¥è¯†å’Œåå¥½ã€‚ å®ƒè¿˜å¯ä»¥é¢„æµ‹ç—…äººçš„æ£€æµ‹ç­–ç•¥å’Œè¯Šæ–­ç»“æœã€‚<details>
<summary>Abstract</summary>
Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that the human decision censors the outcome data: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the human decision-maker deviates from purely risk-based decision-making only along a constrained feature set. We show theoretically and on synthetic data that domain constraints improve parameter inference. We apply our model to a case study of cancer risk prediction, showing that the model's inferred risk predicts cancer diagnoses, its inferred testing policy captures known public health policies, and it can identify suboptimalities in test allocation. Though our case study is in healthcare, our analysis reveals a general class of domain constraints which can improve model estimation in many settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœºå™¨å­¦ä¹ æ¨¡å‹å¸¸è¢«è®­ç»ƒä»¥é¢„æµ‹äººç±»å†³ç­–åçš„ç»“æœã€‚ä¾‹å¦‚ï¼Œå¦‚æœåŒ»ç”Ÿå†³å®šæµ‹è¯•æ‚£è€…æœ‰ç—…æˆ–ä¸æœ‰ç—…ï¼Œé‚£ä¹ˆæ‚£è€…ä¼šæµ‹è¯•é˜³æ€§æˆ–é˜´æ€§ï¼Ÿä¸€ä¸ªæŒ‘æˆ˜æ˜¯äººç±»å†³ç­– censors the outcome dataï¼šæˆ‘ä»¬åªèƒ½è§‚å¯Ÿå·²ç»è¢«æµ‹è¯•çš„æ‚£è€…çš„æµ‹è¯•ç»“æœã€‚æœªæµ‹è¯•çš„æ‚£è€…ï¼Œå…¶ç»“æœæœªè¢«è§‚å¯Ÿï¼Œå¯èƒ½ä¸æµ‹è¯•çš„æ‚£è€…ä¸åŒäºå…¶è§‚å¯Ÿå’Œä¸è§‚å¯Ÿçš„ç»´åº¦ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§ bayesian æ¨¡å‹ç±»ï¼Œç”¨äºå‡†ç¡®åœ°ä¼°è®¡æ‚£è€…çš„é£é™©ã€‚è¿™ä¸ªæ¨¡å‹çš„ç›®çš„æ˜¯ä¼°è®¡æµ‹è¯•å’Œæœªæµ‹è¯•çš„æ‚£è€…çš„é£é™©ã€‚ç”±äºæœªæµ‹è¯•çš„æ‚£è€…çš„å¯èƒ½æ€§èŒƒå›´å¹¿æ³›ï¼Œå› æ­¤æˆ‘ä»¬æå‡ºäº†ä¸¤ç§é¢†åŸŸçº¦æŸï¼Œå®ƒä»¬åœ¨åŒ»ç–—è®¾ç½®ä¸­æ˜¯å¯èƒ½çš„ï¼šä¸€ä¸ªæ˜¯ç–¾ç—…æ‚£ç‡çº¦æŸï¼Œå³æ€»ç–¾ç—…æ‚£ç‡æ˜¯å·²çŸ¥çš„ï¼Œå¦ä¸€ä¸ªæ˜¯ä¸“ä¸šçŸ¥è¯†çº¦æŸï¼Œå³äººç±»å†³ç­–è€…åœ¨å·²çŸ¥çš„ç‰¹å¾é›†ä¸Šåç¦»å®Œå…¨åŸºäºé£é™©çš„å†³ç­–ã€‚æˆ‘ä»¬åœ¨ç†è®ºå’Œäººå·¥æ•°æ®ä¸Šè¡¨æ˜äº†é¢†åŸŸçº¦æŸå¯ä»¥æ”¹å–„å‚æ•°æ¨å¯¼ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªæŠ‘åˆ¶ç™Œç—‡é¢„æµ‹æ¡ˆä¾‹ä¸­åº”ç”¨äº†è¿™ç§æ¨¡å‹ï¼Œæ˜¾ç¤ºå…¶æ¨å¯¼å‡ºçš„é£é™©å¯ä»¥é¢„æµ‹ç™Œç—‡è¯Šæ–­ï¼Œå…¶æ¨å¯¼å‡ºçš„æµ‹è¯•ç­–ç•¥éµå¾ªå·²çŸ¥çš„å…¬å…±å«ç”Ÿæ”¿ç­–ï¼Œå¹¶å¯ä»¥è¯†åˆ«æµ‹è¯•èµ„æºçš„ä¸è¶³ã€‚è™½ç„¶æˆ‘ä»¬çš„æ¡ˆä¾‹æ˜¯åœ¨åŒ»ç–—é¢†åŸŸï¼Œä½†æˆ‘ä»¬çš„åˆ†æè¡¨æ˜äº†ä¸€ä¸ªé€šç”¨çš„é¢†åŸŸçº¦æŸï¼Œå¯ä»¥åœ¨è®¸å¤šè®¾ç½®ä¸­æ”¹å–„æ¨¡å‹ä¼°è®¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Optimizing-CO-2-Capture-in-Pressure-Swing-Adsorption-Units-A-Deep-Neural-Network-Approach-with-Optimality-Evaluation-and-Operating-Maps-for-Decision-Making"><a href="#Optimizing-CO-2-Capture-in-Pressure-Swing-Adsorption-Units-A-Deep-Neural-Network-Approach-with-Optimality-Evaluation-and-Operating-Maps-for-Decision-Making" class="headerlink" title="Optimizing $CO_{2}$ Capture in Pressure Swing Adsorption Units: A Deep Neural Network Approach with Optimality Evaluation and Operating Maps for Decision-Making"></a>Optimizing $CO_{2}$ Capture in Pressure Swing Adsorption Units: A Deep Neural Network Approach with Optimality Evaluation and Operating Maps for Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03873">http://arxiv.org/abs/2312.03873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carine Menezes Rebello, Idelfonso B. R. Nogueira</li>
<li>for: è¿™é¡¹ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ç§ä»£è¡¨æ€§ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºç¯å¢ƒä¸­çš„ç¯å¢ƒå¾ç´ æ•é›†è¿‡ç¨‹ï¼Œå°¤å…¶æ˜¯ç¢³æ’æ”¾ ($CO_{2}$) æ•é›†ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†å¤šè¾“å…¥å•å‡ºåŠ›ï¼ˆMISOï¼‰æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å‹ï¼Œé¢„æµ‹è¿‡ç¨‹æ€§èƒ½æŒ‡æ ‡ã€‚è¿™äº›æ¨¡å‹ç„¶åè¢«é›†æˆåˆ°ä¼˜åŒ–æ¡†æ¶ä¸­ï¼Œé€šè¿‡ç²’å­ç¾¤æœç´¢ï¼ˆPSOï¼‰å’Œç»Ÿè®¡åˆ†ææ¥ç”Ÿæˆå…¨é¢çš„Paretoå‰åˆ—è¡¨ã€‚</li>
<li>results: è¯¥æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°è¯„ä¼°ä¼˜åŒ–æ•ˆæœï¼Œå¹¶ä¸”å¯ä»¥åœ¨å…è®¸çš„æ“ä½œèŒƒå›´å†…æ‰¾åˆ°æœ€ä¼˜çš„å†³ç­–åœºæ™¯ã€‚ç ”ç©¶è¿˜å‘ç°äº†ä¸€äº›å½±å“è¿‡ç¨‹è¡Œä¸ºçš„å…³é”®å› ç´ ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå…·æœ‰å®ç”¨æ€§å’Œæ·±å…¥åˆ†æçš„æ“ä½œåœ°å›¾ï¼Œå¸®åŠ©æ“ä½œäººå‘˜å¿«é€Ÿå®šä½æœ€ä½³è¿‡ç¨‹ä½ç½®å’Œä¼˜åŒ–ç‰¹å®šæ“ä½œç›®æ ‡ã€‚<details>
<summary>Abstract</summary>
This study presents a methodology for surrogate optimization of cyclic adsorption processes, focusing on enhancing Pressure Swing Adsorption units for carbon dioxide ($CO_{2}$) capture. We developed and implemented a multiple-input, single-output (MISO) framework comprising two deep neural network (DNN) models, predicting key process performance indicators. These models were then integrated into an optimization framework, leveraging particle swarm optimization (PSO) and statistical analysis to generate a comprehensive Pareto front representation. This approach delineated feasible operational regions (FORs) and highlighted the spectrum of optimal decision-making scenarios. A key aspect of our methodology was the evaluation of optimization effectiveness. This was accomplished by testing decision variables derived from the Pareto front against a phenomenological model, affirming the surrogate models reliability. Subsequently, the study delved into analyzing the feasible operational domains of these decision variables. A detailed correlation map was constructed to elucidate the interplay between these variables, thereby uncovering the most impactful factors influencing process behavior. The study offers a practical, insightful operational map that aids operators in pinpointing the optimal process location and prioritizing specific operational goals.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation notes:* "cyclic adsorption processes" is translated as "å¾ªç¯å¸é™„è¿‡ç¨‹" (pinyin: xÃºngrÃ¡n jÄ«ngshÅ« gÃ²ujihÃ²u)* "Pressure Swing Adsorption units" is translated as "å‹åŠ›æŒ¯è¡å¸é™„è®¾å¤‡" (pinyin: yÄlÃ¬ zhÃ ngdÃ ng jÄ«ngshÅ« shÃ¨bÃ¬)* "carbon dioxide" is translated as "äºŒæ°§åŒ–ç¢³" (pinyin: Ã¨r gÅngyÇng dÄ«)* "surrogate optimization" is translated as "ä»£ç†ä¼˜åŒ–" (pinyin: dÃ lÇ yÅujiÄ)* "Pareto front" is translated as "å¸•é›·æ‰˜å‰æ–¹" (pinyin: pÄleitÅ qiÃ¡nfÄng)* "phenomenological model" is translated as "ç°è±¡å­¦æ¨¡å‹" (pinyin: xiÃ nxiÃ ng xuÃ© mÃ³de)* "decision variables" is translated as "å†³ç­–å˜é‡" (pinyin: juÃ©dÃ  biÃ nzhong)* "feasible operational domains" is translated as "å¯è¡Œæ“ä½œé¢†åŸŸ" (pinyin: kÄ›xÃ­ cÃ²ngzuÃ² yÃ¬ndÃ¹)* "correlation map" is translated as "ç›¸å…³åœ°å›¾" (pinyin: xiÄngguÄn dÃ¬tÃº)
</details></li>
</ul>
<hr>
<h2 id="Hidden-yet-quantifiable-A-lower-bound-for-confounding-strength-using-randomized-trials"><a href="#Hidden-yet-quantifiable-A-lower-bound-for-confounding-strength-using-randomized-trials" class="headerlink" title="Hidden yet quantifiable: A lower bound for confounding strength using randomized trials"></a>Hidden yet quantifiable: A lower bound for confounding strength using randomized trials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03871">http://arxiv.org/abs/2312.03871</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaabmar/confounder-lower-bound">https://github.com/jaabmar/confounder-lower-bound</a></li>
<li>paper_authors: Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang</li>
<li>for: è¯„ä¼°æ–°è¯åœ¨ä¸´åºŠå®è·µä¸­çš„æ•ˆæœ</li>
<li>methods: ä½¿ç”¨éšæœºè¯•éªŒæ¥è¯„ä¼°æ½œåœ¨çš„æ½œåœ¨åè§</li>
<li>results: æå‡ºä¸€ç§æ–°çš„ç»Ÿè®¡æµ‹è¯•æ–¹æ³•ï¼Œå¯ä»¥è¯„ä¼°æ½œåœ¨åè§çš„å¼ºåº¦ï¼Œå¹¶ä¸”å¯ä»¥æ­£ç¡®åœ°ç¡®å®šæ½œåœ¨åè§çš„å­˜åœ¨æˆ–ä¸å­˜åœ¨<details>
<summary>Abstract</summary>
In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨ç²¾å‡†åŒ»å­¦æ—¶ä»£ï¼Œè§‚å¯Ÿç ”ç©¶åœ¨ä¸´åºŠå®è·µä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ï¼Œä½†æ˜¯ä¸è§‚å¯Ÿçš„åè§å¯èƒ½ä¼šå¯¹ causal ç»“è®ºäº§ç”Ÿé‡å¤§å½±å“ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ï¼Œåˆ©ç”¨éšæœºè¯•éªŒæ¥è¡¡é‡ä¸è§‚å¯Ÿçš„åè§ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç»Ÿè®¡æµ‹è¯•ï¼Œç”¨äºæ£€æµ‹ä¸è§‚å¯Ÿçš„åè§å¼ºåº¦è¶…è¿‡ç»™å®šçš„é˜ˆå€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•æ¥ä¼°ç®—åè§å¼ºåº¦çš„ä¸‹ç•Œï¼Œè¯¥ä¸‹ç•Œåœ¨æŸäº› Synthetic å’ŒåŠ Synthetic æ•°æ®é›†ä¸Šæ˜¯æœ‰æ•ˆçš„ã€‚æˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„ä¸‹ç•Œå¯ä»¥æ­£ç¡®åœ°è¯†åˆ«å®é™…åœºæ™¯ä¸­çš„åè§å­˜åœ¨å’Œç¼ºå¤±ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multi-Group-Fairness-Evaluation-via-Conditional-Value-at-Risk-Testing"><a href="#Multi-Group-Fairness-Evaluation-via-Conditional-Value-at-Risk-Testing" class="headerlink" title="Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing"></a>Multi-Group Fairness Evaluation via Conditional Value-at-Risk Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03867">http://arxiv.org/abs/2312.03867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Monteiro Paes, Ananda Theertha Suresh, Alex Beutel, Flavio P. Calmon, Ahmad Beirami</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤šä¸ªæ•æ„Ÿå±æ€§ï¼ˆå¦‚ç§æ—ã€æ€§åˆ«ã€å¹´é¾„ï¼‰å®šä¹‰çš„äººç¾¤ä¹‹é—´æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹çš„è¡¨ç°å·®å¼‚ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºConditional Value-at-Riskï¼ˆCVaRï¼‰çš„æ–¹æ³•æ¥æ£€æµ‹äººç¾¤ä¹‹é—´çš„è¡¨ç°å·®å¼‚ã€‚æˆ‘ä»¬å…è®¸å°çš„æ¦‚ç‡é¥±å’Œåœ¨ç¾¤ä½“ä¸Šï¼Œä»¥å‡å°‘å¯¹ç¾¤ä½“ä¹‹é—´è¡¨ç°å·®å¼‚çš„æ£€æµ‹æ ·æœ¬å¤æ‚åº¦ã€‚</li>
<li>results: æˆ‘ä»¬çš„åˆ†æè¡¨æ˜ï¼Œå½“ç¾¤ä½“ç”±å¤šä¸ªæ•æ„Ÿå±æ€§å®šä¹‰æ—¶ï¼Œæˆ‘ä»¬çš„CVaRæµ‹è¯•ç®—æ³•çš„æ ·æœ¬å¤æ‚åº¦åªéœ€ upper bounded by æ–¹å·®çš„å¹³æ–¹æ ¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå­˜åœ¨ä¸€ç§éç‹¬ç«‹çš„æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œå¯ä»¥ä½¿æ ·æœ¬å¤æ‚åº¦ç‹¬ç«‹äºäººç¾¤æ•°é‡ã€‚<details>
<summary>Abstract</summary>
Machine learning (ML) models used in prediction and classification tasks may display performance disparities across population groups determined by sensitive attributes (e.g., race, sex, age). We consider the problem of evaluating the performance of a fixed ML model across population groups defined by multiple sensitive attributes (e.g., race and sex and age). Here, the sample complexity for estimating the worst-case performance gap across groups (e.g., the largest difference in error rates) increases exponentially with the number of group-denoting sensitive attributes. To address this issue, we propose an approach to test for performance disparities based on Conditional Value-at-Risk (CVaR). By allowing a small probabilistic slack on the groups over which a model has approximately equal performance, we show that the sample complexity required for discovering performance violations is reduced exponentially to be at most upper bounded by the square root of the number of groups. As a byproduct of our analysis, when the groups are weighted by a specific prior distribution, we show that R\'enyi entropy of order $2/3$ of the prior distribution captures the sample complexity of the proposed CVaR test algorithm. Finally, we also show that there exists a non-i.i.d. data collection strategy that results in a sample complexity independent of the number of groups.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Simplified Chinese translation)æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åœ¨é¢„æµ‹å’Œåˆ†ç±»ä»»åŠ¡ä¸­å¯èƒ½ä¼š Display æ€§èƒ½å·®å¼‚ across  Population groups  determined by sensitive attributes (e.g., race, sex, age). We consider the problem of evaluating the performance of a fixed ML model across population groups defined by multiple sensitive attributes (e.g., race and sex and age). Here, the sample complexity for estimating the worst-case performance gap across groups (e.g., the largest difference in error rates) increases exponentially with the number of group-denoting sensitive attributes. To address this issue, we propose an approach to test for performance disparities based on Conditional Value-at-Risk (CVaR). By allowing a small probabilistic slack on the groups over which a model has approximately equal performance, we show that the sample complexity required for discovering performance violations is reduced exponentially to be at most upper bounded by the square root of the number of groups. As a byproduct of our analysis, when the groups are weighted by a specific prior distribution, we show that R\'enyi entropy of order $2/3$ of the prior distribution captures the sample complexity of the proposed CVaR test algorithm. Finally, we also show that there exists a non-i.i.d. data collection strategy that results in a sample complexity independent of the number of groups.
</details></li>
</ul>
<hr>
<h2 id="Learning-Genomic-Sequence-Representations-using-Graph-Neural-Networks-over-De-Bruijn-Graphs"><a href="#Learning-Genomic-Sequence-Representations-using-Graph-Neural-Networks-over-De-Bruijn-Graphs" class="headerlink" title="Learning Genomic Sequence Representations using Graph Neural Networks over De Bruijn Graphs"></a>Learning Genomic Sequence Representations using Graph Neural Networks over De Bruijn Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03865">http://arxiv.org/abs/2312.03865</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ratschlab/genomic-gnn">https://github.com/ratschlab/genomic-gnn</a></li>
<li>paper_authors: Kacper KapuÅ›niak, Manuel Burger, Gunnar RÃ¤tsch, Amir Joudaki</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§æ–°çš„åºåˆ—è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥é€‚åº” genomic åºåˆ—æ•°æ®çš„å¿«é€Ÿæ‰©å±•ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº† k-mer åµŒå…¥ï¼Œå°†ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç»“æ„ä¿¡æ¯ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ çš„ Graph Convolutional Network ç¼–ç å™¨ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„åµŒå…¥æ–¹æ³•åœ¨ Edit Distance Approximation å’Œ Closest String Retrieval ä»»åŠ¡ä¸Šè¡¨ç°å‡ºäº†æ˜æ˜¾çš„è¶…è¶Šå‰ä¸€äº›æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
The rapid expansion of genomic sequence data calls for new methods to achieve robust sequence representations. Existing techniques often neglect intricate structural details, emphasizing mainly contextual information. To address this, we developed k-mer embeddings that merge contextual and structural string information by enhancing De Bruijn graphs with structural similarity connections. Subsequently, we crafted a self-supervised method based on Contrastive Learning that employs a heterogeneous Graph Convolutional Network encoder and constructs positive pairs based on node similarities. Our embeddings consistently outperform prior techniques for Edit Distance Approximation and Closest String Retrieval tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œéšç€åŸºå› åºåˆ—æ•°æ®çš„å¿«é€Ÿæ‰©å±•ï¼Œéœ€è¦æ–°çš„æ–¹æ³•æ¥å»ºç«‹Robustçš„åºåˆ—è¡¨ç°ã€‚ç°æœ‰çš„æŠ€æœ¯ Ñ‡Ğ°ÑÑ‚Ğ¾å¿½ç•¥ç»†éƒ¨ç»“æ„ä¿¡æ¯ï¼Œä»…å°†ä¸»è¦å…³æ³¨äºä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†k-meråµŒå…¥ï¼Œå°†ä¸Šä¸‹æ–‡å’Œç»“æ„ä¸²ä¿¡æ¯èåˆï¼Œé€šè¿‡å¼ºåŒ–De Bruijn Ğ³Ñ€Ğ°å›¾ä¸­çš„ç»“æ„ç›¸ä¼¼æ€§è¿æ¥ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åˆ›é€ äº†ä¸€ç§è‡ªæˆ‘è¶…visedçš„æ–¹æ³•ï¼ŒåŸºäºä¸åŒç±»å‹çš„Graph Convolutional NetworkåµŒå…¥ï¼Œå¹¶ä»¥èŠ‚ç‚¹ç›¸ä¼¼æ€§ä½œä¸ºå»ºæ„æ­£é¢å¯¹çš„åŒæ–¹å¯¹ã€‚æˆ‘ä»¬çš„åµŒå…¥ä¸€è‡´æ€§åœ°è¶…è¿‡äº†å…ˆå‰çš„æ–¹æ³•ï¼Œå¯¹äºEdit Distance Approximationå’Œæœ€è¿‘ä¸²ä¸²æœå¯»ä»»åŠ¡éƒ½æœ‰ç€ä¼˜ç§€çš„è¡¨ç°ã€‚â€Note that Simplified Chinese is a written language that uses shorter words and simpler grammar compared to Traditional Chinese. The translation is written in Simplified Chinese, but the original text is in English.
</details></li>
</ul>
<hr>
<h2 id="Dr-Jekyll-and-Mr-Hyde-Two-Faces-of-LLMs"><a href="#Dr-Jekyll-and-Mr-Hyde-Two-Faces-of-LLMs" class="headerlink" title="Dr. Jekyll and Mr. Hyde: Two Faces of LLMs"></a>Dr. Jekyll and Mr. Hyde: Two Faces of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03853">http://arxiv.org/abs/2312.03853</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro Conti, Stjepan Picek</li>
<li>For: The paper is written to demonstrate the vulnerability of ChatGPT and Bard to adversarial personas, and to show that these chatbots can be tricked into providing unauthorized, illegal, or harmful information.* Methods: The paper uses elaborate biographies of complex personas to trick the chatbots into providing prohibited responses. The conversation is conducted in a role-play style to elicit the desired response.* Results: The paper shows that both ChatGPT and Bard are vulnerable to this kind of attack, and that it is possible to obtain unauthorized information by using adversarial personas. The paper also introduces several ways of activating such personas.<details>
<summary>Abstract</summary>
This year, we witnessed a rise in the use of Large Language Models, especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are put in place to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial personas, one can overcome safety mechanisms set out by ChatGPT and Bard. It also introduces several ways of activating such adversarial personas, altogether showing that both chatbots are vulnerable to this kind of attack.
</details>
<details>
<summary>æ‘˜è¦</summary>
We begin by creating detailed biographies of these personas, which we then use in a new session with the same chatbots. Our conversation follows a role-playing style to elicit the response the assistant is not allowed to provide. By using personas, we demonstrate that the response that is prohibited is actually provided, allowing us to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial personas, one can overcome the safety mechanisms set by ChatGPT and Bard. Additionally, we present several methods for activating such adversarial personas, revealing that both chatbots are vulnerable to this type of attack.
</details></li>
</ul>
<hr>
<h2 id="Exposing-Disparities-in-Flood-Adaptation-for-Equitable-Future-Interventions"><a href="#Exposing-Disparities-in-Flood-Adaptation-for-Equitable-Future-Interventions" class="headerlink" title="Exposing Disparities in Flood Adaptation for Equitable Future Interventions"></a>Exposing Disparities in Flood Adaptation for Equitable Future Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03843">http://arxiv.org/abs/2312.03843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lidia Cano Pecharroman, ChangHoon Hahn</li>
<li>For: The paper aims to evaluate the effectiveness of the FEMA National Flood Insurance Program Community Rating System in providing equitable support for all communities, particularly those that have been historically disadvantaged.* Methods: The authors use a causal inference method called ${\rm C{\scriptsize AUSAL}F{\scriptsize LOW}$ based on deep generative models to estimate the treatment effect of flood adaptation interventions on communitiesâ€™ savings, considering factors such as income, diversity, population, flood risk, educational attainment, and precipitation.* Results: The program is found to save communities an average of $5,000â€“15,000 per household, but the savings are not evenly distributed across communities. In particular, low-income communities and predominantly non-white communities tend to have lower savings, with a gap of more than $6000 per household between predominantly white and non-white communities.Hereâ€™s the information in Simplified Chinese:* For: ç ”ç©¶æ—¨åœ¨è¯„ä¼°FEMAå›½å®¶æ´ªæ°´ä¿é™©è®¡åˆ’ç¤¾åŒºè¯„çº§ç³»ç»Ÿæ˜¯å¦ä¸ºæ‰€æœ‰ç¤¾åŒºæä¾›å¹³ç­‰æ”¯æŒï¼Œå°¤å…¶æ˜¯å†å²ä¸Šå—åˆ°ä¸å…¬æ­£å¾…é‡çš„ç¤¾åŒºã€‚* Methods: ä½œè€…ä½¿ç”¨åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ causal inferenceæ–¹æ³•(${\rm C{\scriptsize AUSAL}F{\scriptsize LOW}$)æ¥ä¼°è®¡æ´ªæ°´é€‚åº”æªæ–½å¯¹ç¤¾åŒºçš„æˆæœ¬å½±å“ï¼Œè€ƒè™‘å› ç´ åŒ…æ‹¬æ”¶å…¥ã€å¤šæ ·æ€§ã€äººå£ã€æ´ªæ°´é£é™©ã€æ•™è‚²æ°´å¹³å’Œé™æ°´é‡ã€‚* Results: è®¡åˆ’å¯ä»¥ä¸ºç¤¾åŒºæä¾› $5,000â€“15,000æ¯æˆ·çš„æˆæœ¬èŠ‚ç‚¹ï¼Œä½†è¿™äº›èŠ‚ç‚¹ä¸å‡åˆ†é…åˆ°ç¤¾åŒºã€‚ç‰¹åˆ«æ˜¯ä½æ”¶å…¥ç¤¾åŒºçš„èŠ‚ç‚¹å‡å°‘é€æ¸å‡å°‘ï¼Œä¸é«˜æ”¶å…¥ç¤¾åŒºç›¸æ¯”ï¼Œéç™½ç¤¾åŒºçš„èŠ‚ç‚¹å¯èƒ½é«˜è¾¾ $6,000ä»¥ä¸Šçš„å·®è·ã€‚<details>
<summary>Abstract</summary>
As governments race to implement new climate adaptation policies that prepare for more frequent flooding, they must seek policies that are effective for all communities and uphold climate justice. This requires evaluating policies not only on their overall effectiveness but also on whether their benefits are felt across all communities. We illustrate the importance of considering such disparities for flood adaptation using the FEMA National Flood Insurance Program Community Rating System and its dataset of $\sim$2.5 million flood insurance claims. We use ${\rm C{\scriptsize AUSAL}F{\scriptsize LOW}$, a causal inference method based on deep generative models, to estimate the treatment effect of flood adaptation interventions based on a community's income, diversity, population, flood risk, educational attainment, and precipitation. We find that the program saves communities \$5,000--15,000 per household. However, these savings are not evenly spread across communities. For example, for low-income communities savings sharply decline as flood-risk increases in contrast to their high-income counterparts with all else equal. Even among low-income communities, there is a gap in savings between predominantly white and non-white communities: savings of predominantly white communities can be higher by more than \$6000 per household. As communities worldwide ramp up efforts to reduce losses inflicted by floods, simply prescribing a series flood adaptation measures is not enough. Programs must provide communities with the necessary technical and economic support to compensate for historical patterns of disenfranchisement, racism, and inequality. Future flood adaptation efforts should go beyond reducing losses overall and aim to close existing gaps to equitably support communities in the race for climate adaptation.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ”¿åºœä»¬åœ¨å®æ–½æ–°çš„æ°”å€™é€‚åº”æ”¿ç­–æ—¶ï¼Œå¿…é¡»è€ƒè™‘æ‰€æœ‰ç¤¾åŒºçš„åˆ©ç›Šï¼Œå¹¶ä¿æŒæ°”å€™æ­£ä¹‰ã€‚è¿™æ„å‘³ç€è¯„ä¼°æ”¿ç­–çš„æ•ˆæœä¸ä»…æ˜¯å…¨ä½“æ•ˆæœï¼Œè€Œä¸”æ˜¯åœ¨æ‰€æœ‰ç¤¾åŒºä¸­çš„æ•ˆæœã€‚æˆ‘ä»¬ä½¿ç”¨CAUSALFLOWæ–¹æ³•ï¼Œä¸€ç§åŸºäºæ·±åº¦ç”Ÿæˆæ¨¡å‹çš„ causal inferenceæ–¹æ³•ï¼Œæ¥ä¼°è®¡æ´ªæ°´é€‚åº”æªæ–½çš„å½±å“ï¼ŒåŸºäºç¤¾åŒºçš„æ”¶å…¥ã€å¤šæ ·æ€§ã€äººå£ã€æ´ªæ°´é£é™©ã€æ•™è‚²æ°´å¹³å’Œé™é›¨é‡ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¯¥è®¡åˆ’å¯ä»¥ä¸ºæ¯æˆ·èŠ‚çœ5,000åˆ°15,000ç¾å…ƒã€‚ç„¶è€Œï¼Œè¿™äº›èŠ‚çœä¸å‡åŒ€åˆ†å¸ƒåœ¨ç¤¾åŒºä¸­ã€‚ä¾‹å¦‚ï¼Œå¯¹ä½æ”¶å…¥ç¤¾åŒºæ¥è¯´ï¼Œæ´ªæ°´é£é™©å¢åŠ æ—¶ï¼ŒèŠ‚çœé¢amount sharply decreasesï¼Œä¸é«˜æ”¶å…¥ç¤¾åŒºç›¸æ¯”ï¼Œå…¶èŠ‚çœé¢amountå·®è·è¶…è¿‡6,000ç¾å…ƒã€‚è€Œåœ¨ä½æ”¶å…¥ç¤¾åŒºä¸­ï¼Œéç™½äººç¤¾åŒºçš„èŠ‚çœé¢amountä¸ç™½äººç¤¾åŒºç›¸æ¯”ï¼Œè¿˜å­˜åœ¨ä¸€å®šçš„å·®è·ã€‚åœ¨å…¨çƒèŒƒå›´å†…ï¼Œç¤¾åŒºå‡å°‘æ´ªæ°´æ‰€é€ æˆçš„æŸå®³çš„åŠªåŠ›æ˜¯ä¸å¤Ÿçš„ã€‚æœªæ¥çš„æ´ªæ°´é€‚åº”åŠªåŠ›åº”è¯¥è¶…è¶Šå‡å°‘æ€»æŸå®³ï¼Œè€Œæ˜¯åŠªåŠ› closing existing gaps to equitably support communities in the race for climate adaptationã€‚
</details></li>
</ul>
<hr>
<h2 id="High-Pileup-Particle-Tracking-with-Object-Condensation"><a href="#High-Pileup-Particle-Tracking-with-Object-Condensation" class="headerlink" title="High Pileup Particle Tracking with Object Condensation"></a>High Pileup Particle Tracking with Object Condensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03823">http://arxiv.org/abs/2312.03823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kilian Lieret, Gage DeZoort, Devdoot Chatterjee, Jian Park, Siqi Miao, Pan Li</li>
<li>for: è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯æé«˜é«˜èƒ½ç‰©ç†å®éªŒå®¤ä¸­ charged particle çš„è¿½è¸ªç²¾åº¦å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨ graph neural networks (GNNs) å’Œ object condensation (OC) æ–¹æ³•æ¥å®ç°è¿½è¸ªã€‚</li>
<li>results: ç ”ç©¶æ˜¾ç¤º GNNs å¯ä»¥ä¸ä¼ ç»Ÿç®—æ³•åŒ¹é…æ€§èƒ½ï¼ŒåŒæ—¶æé«˜å¯æ‰©å±•æ€§ä»¥åº”å¯¹é«˜èƒ½ç‰©ç†å®éªŒå®¤ä¸­çš„è®¡ç®—æŒ‘æˆ˜ã€‚<details>
<summary>Abstract</summary>
Recent work has demonstrated that graph neural networks (GNNs) can match the performance of traditional algorithms for charged particle tracking while improving scalability to meet the computing challenges posed by the HL-LHC. Most GNN tracking algorithms are based on edge classification and identify tracks as connected components from an initial graph containing spurious connections. In this talk, we consider an alternative based on object condensation (OC), a multi-objective learning framework designed to cluster points (hits) belonging to an arbitrary number of objects (tracks) and regress the properties of each object. Building on our previous results, we present a streamlined model and show progress toward a one-shot OC tracking algorithm in a high-pileup environment.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸç ”ç©¶è¡¨æ˜ï¼Œå›¾ neuronal networks (GNNs) å¯ä»¥ä¸ä¼ ç»Ÿç®—æ³•åŒ¹é…æ€§èƒ½ï¼ŒåŒæ—¶æé«˜å¯æ‰©å±•æ€§ä»¥æ»¡è¶³é«˜èƒ½ç‰©ç†ç ”ç©¶æ‰€ (HL-LHC) æ‰€å¸¦æ¥çš„è®¡ç®—æŒ‘æˆ˜ã€‚å¤§å¤šæ•° GNN è·Ÿè¸ªç®—æ³•åŸºäºè¾¹ç±»åˆ’åˆ†ï¼Œä»åˆå§‹å›¾ä¸­çš„å‡è®¾è¿æ¥ä¸­æå–è½¨è¿¹ã€‚åœ¨è¿™ç¯‡æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ç§æ›¿ä»£æ–¹æ¡ˆï¼ŒåŸºäºç‰©ä½“å‡ç»“ (OC)ï¼Œä¸€ç§å¤šç›®æ ‡å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºèšåˆç‚¹ (hit) æ‰€å±çš„ä»»æ„æ•°é‡çš„ç‰©ä½“ (track)ï¼Œå¹¶ä¼°è®¡æ¯ä¸ªç‰©ä½“çš„æ€§è´¨ã€‚åŸºäºæˆ‘ä»¬çš„å‰ä¸€æ¬¡æˆæœï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´åŠ æµç•…çš„æ¨¡å‹ï¼Œå¹¶å±•ç¤ºäº†åœ¨é«˜å †æ ¸ç‡ƒæ–™ç¯å¢ƒä¸­ä¸€shot OC è·Ÿè¸ªç®—æ³•çš„è¿›å±•ã€‚
</details></li>
</ul>
<hr>
<h2 id="nbi-the-Astronomerâ€™s-Package-for-Neural-Posterior-Estimation"><a href="#nbi-the-Astronomerâ€™s-Package-for-Neural-Posterior-Estimation" class="headerlink" title="nbi: the Astronomerâ€™s Package for Neural Posterior Estimation"></a>nbi: the Astronomerâ€™s Package for Neural Posterior Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03824">http://arxiv.org/abs/2312.03824</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kmzzhang/nbi">https://github.com/kmzzhang/nbi</a></li>
<li>paper_authors: Keming Zhang, Joshua Bloom, StÃ©fan van der Walt, Nina Hernitschek</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¤©ä½“ç‰©ç†å­¦ä¸­çš„ç¥ç» posterior ä¼°è®¡ï¼ˆNPEï¼‰æ–¹æ³•çš„åº”ç”¨é€Ÿåº¦ï¼Œå¹¶è§£å†³äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šéœ€è¦ç‰¹å®šåŒ–çš„ç‰¹å¾ç½‘ç»œã€æ¨æ–­ä¸å‡†ç¡®å’Œç‰©ç†å‰å‘æ¨¡å‹çš„ä¸è¶³ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶å’Œå¼€æºè½¯ä»¶ nbiï¼ˆç¥ç» Bayesian æ¨æ–­ï¼‰ï¼Œæ”¯æŒäº†ç§¯ç´¯å’Œé¡ºåº NPEã€‚ nbi æä¾›äº†å†…ç½®çš„ â€œç‰¹å¾åŒ–â€ ç½‘ç»œï¼Œå¯ä»¥å¿«é€Ÿåœ°åº”ç”¨äºå¤©ä½“è§‚æµ‹æ•°æ®ï¼Œå¦‚å…‰è°±å’Œå…‰è°±æ›²çº¿ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä¿®æ”¹åçš„ç®—æ³• SNPE-ISï¼Œå¯ä»¥å®ç°æé™ç²¾å‡†æ¨æ–­ã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡åº”ç”¨ nbi è½¯ä»¶æ¥è§£å†³å¤©ä½“è§‚æµ‹æ•°æ®ä¸­çš„å‡ ä¸ªé—®é¢˜ï¼Œå¹¶è¯æ˜äº† nbi å¯ä»¥ä½œä¸º Nested Sampling ç­‰æ–¹æ³•çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚<details>
<summary>Abstract</summary>
Despite the promise of Neural Posterior Estimation (NPE) methods in astronomy, the adaptation of NPE into the routine inference workflow has been slow. We identify three critical issues: the need for custom featurizer networks tailored to the observed data, the inference inexactness, and the under-specification of physical forward models. To address the first two issues, we introduce a new framework and open-source software nbi (Neural Bayesian Inference), which supports both amortized and sequential NPE. First, nbi provides built-in "featurizer" networks with demonstrated efficacy on sequential data, such as light curve and spectra, thus obviating the need for this customization on the user end. Second, we introduce a modified algorithm SNPE-IS, which facilities asymptotically exact inference by using the surrogate posterior under NPE only as a proposal distribution for importance sampling. These features allow nbi to be applied off-the-shelf to astronomical inference problems involving light curves and spectra. We discuss how nbi may serve as an effective alternative to existing methods such as Nested Sampling. Our package is at https://github.com/kmzzhang/nbi.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡ç¥ç»åéªŒä¼°è®¡ï¼ˆNPEï¼‰æ–¹æ³•åœ¨å¤©æ–‡å­¦ä¸­è¡¨ç°äº†æ‰¿è¯ºï¼Œä½†æŠŠNPEçº³å…¥å¸¸è§„æ¨ç† workflow ä¸­çš„åº”ç”¨è¿›ç¨‹ slowerã€‚æˆ‘ä»¬è®¤ä¸ºæœ‰ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼šéœ€è¦ç‰¹åˆ¶åŒ–çš„ç‰¹å¾ç½‘ç»œé€‚åº”è§‚æµ‹æ•°æ®ï¼Œæ¨ç†ä¸å‡†ç¡®ï¼Œä»¥åŠç‰©ç†å‰å‘æ¨¡å‹çš„ä¸è¶³ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶å’Œå¼€æºè½¯ä»¶ nbiï¼ˆç¥ç» bayesian æ¨ç†ï¼‰ï¼Œè¯¥æ¡†æ¶æ”¯æŒåˆ†å¸ƒå¼å’Œé¡ºåºçš„NPEã€‚é¦–å…ˆï¼Œnbi æä¾›äº†å†…ç½®çš„ "ç‰¹å¾ç½‘ç»œ"ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†é¡ºåºæ•°æ®ï¼Œå¦‚å…‰è°±å’Œå…‰è°±ï¼Œä»è€Œå‡å°‘ç”¨æˆ·éœ€è¦è‡ªå·±å®šåˆ¶çš„éœ€æ±‚ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¿®æ”¹åçš„ç®—æ³• SNPE-ISï¼Œè¯¥ç®—æ³•ä½¿ç”¨ NPE ä¸‹çš„ä»£è¡¨ posterior ä½œä¸ºå†³ç­–Importance sampling çš„ææ¡ˆåˆ†å¸ƒï¼Œä»è€Œå®ç°äº† asymptotically exact çš„æ¨ç†ã€‚è¿™äº›ç‰¹ç‚¹ä½¿å¾— nbi å¯ä»¥åœ¨å¤©æ–‡å­¦ä¸­ direct åº”ç”¨äºå…‰è°±å’Œå…‰è°±æ¨ç†é—®é¢˜ã€‚æˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨ nbi ä½œä¸ºç°æœ‰æ–¹æ³• such as Nested Sampling çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬çš„ package å¯ä»¥åœ¨ https://github.com/kmzzhang/nbi ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Role-of-Edge-Dependency-in-Graph-Generative-Models"><a href="#On-the-Role-of-Edge-Dependency-in-Graph-Generative-Models" class="headerlink" title="On the Role of Edge Dependency in Graph Generative Models"></a>On the Role of Edge Dependency in Graph Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03691">http://arxiv.org/abs/2312.03691</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, Charalampos Tsourakakis</li>
<li>for: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆå›¾æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè¾¹å¤šæ ·æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§åˆ†ä¸‰çº§åˆ’åˆ†æ³•ï¼Œå°†ç”Ÿæˆå›¾æ¨¡å‹åˆ†ä¸ºç‹¬ç«‹è¾¹ã€ç‹¬ç«‹èŠ‚ç‚¹å’Œå®Œå…¨ä¾èµ–æ¨¡å‹ä¸‰ç±»ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æå‡ºäº†ä¸€ç§åŸºäºæ½®æ¹å‘ç°çš„æ–°ç”Ÿæˆæ¨¡å‹ã€‚</li>
<li>results: æœ¬æ–‡é€šè¿‡å¯¹å®é™…æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œå‘ç°äº†æ–°æ¨¡å‹çš„è¾“å‡ºè´¨é‡å’Œé‡å æ€§ä¸å…¶ä»–æµè¡Œæ¨¡å‹ç›¸å½“ã€‚<details>
<summary>Abstract</summary>
In this work, we introduce a novel evaluation framework for generative models of graphs, emphasizing the importance of model-generated graph overlap (Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We delineate a hierarchy of graph generative models categorized into three levels of complexity: edge independent, node independent, and fully dependent models. This hierarchy encapsulates a wide range of prevalent methods. We derive theoretical bounds on the number of triangles and other short-length cycles producible by each level of the hierarchy, contingent on the model overlap. We provide instances demonstrating the asymptotic optimality of our bounds. Furthermore, we introduce new generative models for each of the three hierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis, 2015). Our evaluation, conducted on real-world datasets, focuses on assessing the output quality and overlap of our proposed models in comparison to other popular models. Our results indicate that our simple, interpretable models provide competitive baselines to popular generative models. Through this investigation, we aim to propel the advancement of graph generative models by offering a structured framework and robust evaluation metrics, thereby facilitating the development of models capable of generating accurate and edge-diverse graphs.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ Ğ´Ğ»Ñç”Ÿæˆå›¾æ¨¡å‹ï¼Œå¼ºè°ƒæ¨¡å‹ç”Ÿæˆçš„å›¾é‡åˆï¼ˆChanpuriya et al., 2021ï¼‰ä»¥ç¡®ä¿å‡†ç¡®æ€§å’Œè¾¹å¤šæ ·æ€§ã€‚æˆ‘ä»¬åˆ†ç±»äº†å›¾ç”Ÿæˆæ¨¡å‹ä¸ºä¸‰çº§å¤æ‚åº¦ï¼šç‹¬ç«‹è¾¹ã€ç‹¬ç«‹èŠ‚ç‚¹å’Œå®Œå…¨ä¾èµ–æ¨¡å‹ã€‚è¿™äº›çº§åˆ«åŒ…æ‹¬äº†è®¸å¤šæµè¡Œçš„æ–¹æ³•ã€‚æˆ‘ä»¬ derivatedäº†å¯¹äºæ¯ä¸ªçº§åˆ«çš„boundsï¼Œå…·ä½“æ¥è¯´æ˜¯å¯¹äºå›¾ä¸­çš„ä¸‰è§’å½¢å’Œå…¶ä»–çŸ­è·¯å¾„æ•°çš„ç”Ÿæˆã€‚æˆ‘ä»¬è¿˜æä¾›äº†å®é™…å®éªŒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„ bound æ˜¯ asymptotic çš„ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†æ¯ä¸ªçº§åˆ«çš„æ–°ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨ dense subgraph å‘ç°ï¼ˆGionis & Tsourakakis, 2015ï¼‰ã€‚æˆ‘ä»¬çš„è¯„ä¼°ï¼ŒåŸºäºå®é™…æ•°æ®é›†ï¼Œä¸»è¦å…³æ³¨ç”Ÿæˆå‡ºçš„è¾“å‡ºè´¨é‡å’Œæ¨¡å‹ä¹‹é—´çš„é‡åˆåº¦ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®€å•ã€å¯è§£é‡Šçš„æ¨¡å‹æä¾›äº†ä¸æµè¡Œæ¨¡å‹ç›¸å½“çš„åŸºå‡†ã€‚é€šè¿‡è¿™æ¬¡è°ƒæŸ¥ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨åŠ¨ç”Ÿæˆå›¾æ¨¡å‹çš„è¿›æ­¥ï¼Œé€šè¿‡æä¾›ç»“æ„åŒ–çš„æ¡†æ¶å’Œå¯é çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä»¥ä¿ƒè¿›ç”Ÿæˆå‡†ç¡®å’Œå¤šæ ·åŒ–çš„å›¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Inverse-Design-of-Vitrimeric-Polymers-by-Molecular-Dynamics-and-Generative-Modeling"><a href="#Inverse-Design-of-Vitrimeric-Polymers-by-Molecular-Dynamics-and-Generative-Modeling" class="headerlink" title="Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling"></a>Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03690">http://arxiv.org/abs/2312.03690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiwen Zheng, Prakash Thakolkaran, Jake A. Smith, Ziheng Lu, Shuxin Zheng, Bichlien H. Nguyen, Siddhant Kumar, Aniruddh Vashisth</li>
<li>For: The paper aims to develop a method for generating novel vitrimers with desired glass transition temperature (Tg) and guide their inverse design based on Tg.* Methods: The method combines molecular dynamics (MD) simulations and machine learning (ML), specifically a novel graph variational autoencoder (VAE) model, to generate and design vitrimers with desired Tg.* Results: The proposed VAE framework demonstrates high accuracy and efficiency in discovering novel vitrimers with desirable Tg beyond the training regime, and the generated vitrimers have reasonable synthesizability and cover a wide range of Tg, broadening the potential widespread usage of vitrimeric materials.Here is the same information in Simplified Chinese:* For: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯é€šè¿‡åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰ simulate å’Œæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¥è®¾è®¡å’Œç”Ÿæˆæ˜“äºè‡ªé€‚åº”çš„ vitrimerï¼Œå¹¶é€šè¿‡æè¿°è¿™äº›ææ–™çš„ç»ç’ƒè½¬å˜æ¸©åº¦ï¼ˆTgï¼‰æ¥å¼•å¯¼å…¶é€†è®¾è®¡ã€‚* Methods: è¯¥æ–¹æ³•ç»“åˆäº† MD  simulate å’Œ MLï¼Œç‰¹åˆ«æ˜¯ä¸€ç§æ–°çš„å›¾åƒå˜é‡è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ¨¡å‹ï¼Œæ¥ç”Ÿæˆå’Œè®¾è®¡ vitrimer çš„æ–°ç§ç±»ã€‚* Results: æè®®çš„ VAE æ¡†æ¶åœ¨å‘ç°æ–°çš„ vitrimer ä¸­è¡¨ç°å‡ºäº†é«˜ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¹¶ä¸”ç”Ÿæˆçš„ vitrimer å…·æœ‰åˆç†çš„åˆæˆå¯èƒ½æ€§å’Œè¦†ç›–äº†å¹¿æ³›çš„ Tg èŒƒå›´ï¼Œæ‰©å±•äº† vitrimeric ææ–™çš„æ½œåœ¨å¹¿æ³›ä½¿ç”¨ã€‚<details>
<summary>Abstract</summary>
Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space containing necessary information of vitrimers, we demonstrate high accuracy and efficiency of our framework in discovering novel vitrimers with desirable Tg beyond the training regime. The proposed vitrimers with reasonable synthesizability cover a wide range of Tg and broaden the potential widespread usage of vitrimeric materials.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="GeoShapley-A-Game-Theory-Approach-to-Measuring-Spatial-Effects-in-Machine-Learning-Models"><a href="#GeoShapley-A-Game-Theory-Approach-to-Measuring-Spatial-Effects-in-Machine-Learning-Models" class="headerlink" title="GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models"></a>GeoShapley: A Game Theory Approach to Measuring Spatial Effects in Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03675">http://arxiv.org/abs/2312.03675</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Li</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„ç©ºé—´æ•ˆåº”ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºæ¸¸æˆç†è®ºçš„åœ°ç†Shapeleyæ–¹æ³•æ¥è¡¡é‡è¿™äº›æ•ˆåº”ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åŸºäºè¯ºè´å°”å¥–å¾—ä¸»Shapleyå€¼æ¡†æ¶ï¼Œå°†åœ°ç†ä½ç½®è§†ä¸ºæ¨¡å‹é¢„æµ‹æ¸¸æˆä¸­çš„ä¸€ä¸ªç©å®¶ï¼Œä»è€Œå¯ä»¥é‡åŒ–åœ°ç†ä½ç½®çš„é‡è¦æ€§å’Œå…¶ä»–ç‰¹å¾ä¹‹é—´çš„ååŒä½œç”¨ã€‚è¯¥æ–¹æ³•æ˜¯æ¨¡å‹æ— å…³çš„ï¼Œå¯ä»¥åº”ç”¨äºç»Ÿè®¡æˆ–é»‘ç›’æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>results: ä½¿ç”¨simulatedæ•°æ®éªŒè¯GeoShapleyå€¼ï¼Œå¹¶å¯¹ä¸ƒç§ç»Ÿè®¡å’Œæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œäº†è·¨æ¯”è¾ƒã€‚ä¸€ä¸ªå®é™…çš„ä½æˆ¿ä»·å€¼é¢„æµ‹æ¨¡å‹çš„exampleä¹Ÿç”¨äºè§£é‡ŠGeoShapleyçš„ç”¨é€”å’Œè§£é‡Šã€‚è¯¥æ–¹æ³•å¯ä»¥ä½œä¸ºä¸€ä¸ªå¼€æºPythonåŒ…åä¸ºgeoshapleyè¿›è¡Œåº”ç”¨ã€‚<details>
<summary>Abstract</summary>
This paper introduces GeoShapley, a game theory approach to measuring spatial effects in machine learning models. GeoShapley extends the Nobel Prize-winning Shapley value framework in game theory by conceptualizing location as a player in a model prediction game, which enables the quantification of the importance of location and the synergies between location and other features in a model. GeoShapley is a model-agnostic approach and can be applied to statistical or black-box machine learning models in various structures. The interpretation of GeoShapley is directly linked with spatially varying coefficient models for explaining spatial effects and additive models for explaining non-spatial effects. Using simulated data, GeoShapley values are validated against known data-generating processes and are used for cross-comparison of seven statistical and machine learning models. An empirical example of house price modeling is used to illustrate GeoShapley's utility and interpretation with real world data. The method is available as an open-source Python package named geoshapley.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Role-of-the-Action-Space-in-Robot-Manipulation-Learning-and-Sim-to-Real-Transfer"><a href="#On-the-Role-of-the-Action-Space-in-Robot-Manipulation-Learning-and-Sim-to-Real-Transfer" class="headerlink" title="On the Role of the Action Space in Robot Manipulation Learning and Sim-to-Real Transfer"></a>On the Role of the Action Space in Robot Manipulation Learning and Sim-to-Real Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03673">http://arxiv.org/abs/2312.03673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elie Aljalbout, Felix Frank, Maximilian Karl, Patrick van der Smagt</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†æœºå™¨äºº manipulate å­¦ä¹ ä¸­çš„åŠ¨ä½œç©ºé—´é€‰æ‹©é—®é¢˜ï¼Œä»¥åŠ sim-to-real è½¬ç§»ã€‚</li>
<li>methods: æˆ‘ä»¬å®šä¹‰äº†è¡¨ç°è¯„ä»·æŒ‡æ ‡ï¼Œå¹¶ç ”ç©¶ä¸åŒåŠ¨ä½œç©ºé—´çš„emerging æ€§ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨ simulations ä¸­è®­ç»ƒäº†13ç§ä¸åŒçš„æ§åˆ¶ç©ºé—´ï¼Œå¹¶è¯„ä¼°äº†åœ¨çœŸå®ç¯å¢ƒä¸­çš„è®­ç»ƒæ€§å’Œè½¬ç§»æ€§ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°äº†ä¸€äº›å¥½çš„å’Œåçš„æœºå™¨äººåŠ¨ä½œç©ºé—´ç‰¹å¾ï¼Œå¹¶æå‡ºäº†å°†æ¥è®¾è®¡RLç®—æ³•æ—¶çš„å»ºè®®ã€‚æˆ‘ä»¬çš„å‘ç°å¯¹æœºå™¨äºº manipulate å­¦ä¹ ä»»åŠ¡çš„RLç®—æ³•è®¾è®¡æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶ highlights åœ¨çœŸå®ç¯å¢ƒä¸­è®­ç»ƒå’Œè½¬ç§» RL ä»£ç†çš„éœ€è¦æ…é‡è€ƒè™‘åŠ¨ä½œç©ºé—´ã€‚<details>
<summary>Abstract</summary>
We study the choice of action space in robot manipulation learning and sim-to-real transfer. We define metrics that assess the performance, and examine the emerging properties in the different action spaces. We train over 250 reinforcement learning~(RL) agents in simulated reaching and pushing tasks, using 13 different control spaces. The choice of action spaces spans popular choices in the literature as well as novel combinations of common design characteristics. We evaluate the training performance in simulation and the transfer to a real-world environment. We identify good and bad characteristics of robotic action spaces and make recommendations for future designs. Our findings have important implications for the design of RL algorithms for robot manipulation tasks, and highlight the need for careful consideration of action spaces when training and transferring RL agents for real-world robotics.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶ Ñ€Ğ¾Ğ±Ğ¾Ñ‚ manipulation å­¦ä¹ ä¸­çš„è¡ŒåŠ¨ç©ºé—´é€‰æ‹©ï¼Œä»¥åŠåœ¨ sim-to-real è½¬ç§»ä¸­çš„è¡¨ç°ã€‚æˆ‘ä»¬å®šä¹‰äº†è¯„ä»·æ€§èƒ½çš„æŒ‡æ ‡ï¼Œå¹¶ç ”ç©¶ä¸åŒçš„è¡ŒåŠ¨ç©ºé—´ä¸­å‡ºç°çš„ç‰¹æ€§ã€‚æˆ‘ä»¬åœ¨ simulated æŠ“å–å’Œæ¨åŠ¨ä»»åŠ¡ä¸­è®­ç»ƒäº†è¶…è¿‡ 250 ä¸ª reinforcement learning ï¼ˆRLï¼‰ agentï¼Œä½¿ç”¨ 13 ç§ä¸åŒçš„æ§åˆ¶ç©ºé—´ã€‚é€‰æ‹©çš„è¡ŒåŠ¨ç©ºé—´åŒ…æ‹¬æ–‡çŒ®ä¸­å¸¸è§çš„é€‰æ‹©ä»¥åŠä¸€äº›æ–°çš„ç»„åˆã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿå’ŒçœŸå®ç¯å¢ƒä¸­è¯„ä¼°è®­ç»ƒæ€§èƒ½ï¼Œå¹¶å‘ç°äº† Ñ€Ğ¾Ğ±Ğ¾Ñ‚è¡ŒåŠ¨ç©ºé—´çš„å¥½åç‰¹ç‚¹ï¼Œå¹¶æå‡ºäº†æœªæ¥è®¾è®¡çš„å»ºè®®ã€‚æˆ‘ä»¬çš„å‘ç°å¯¹ robot manipulation ä»»åŠ¡ä¸­çš„ RL ç®—æ³•è®¾è®¡æœ‰é‡è¦æ„ä¹‰ï¼Œå¹¶é«˜äº®äº†åœ¨å®é™… Ñ€Ğ¾Ğ±Ğ¾ics ä¸­è®­ç»ƒå’Œè½¬ç§» RL ä»£ç†çš„ç²¾å¿ƒè€ƒè™‘è¡ŒåŠ¨ç©ºé—´çš„å¿…è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Direct-Exoplanet-Detection-Using-Deep-Convolutional-Image-Reconstruction-ConStruct-A-New-Algorithm-for-Post-Processing-High-Contrast-Images"><a href="#Direct-Exoplanet-Detection-Using-Deep-Convolutional-Image-Reconstruction-ConStruct-A-New-Algorithm-for-Post-Processing-High-Contrast-Images" class="headerlink" title="Direct Exoplanet Detection Using Deep Convolutional Image Reconstruction (ConStruct): A New Algorithm for Post-Processing High-Contrast Images"></a>Direct Exoplanet Detection Using Deep Convolutional Image Reconstruction (ConStruct): A New Algorithm for Post-Processing High-Contrast Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03671">http://arxiv.org/abs/2312.03671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trevor N. Wolf, Brandon A. Jones, Brendan P. Bowler</li>
<li>for: æ£€æµ‹æš—ç‚¹æºåœ¨é«˜å¯¹æ¯” adaptive optics å›¾åƒåºåˆ—ä¸­</li>
<li>methods: ä½¿ç”¨æ·±åº¦å­¦ä¹  direct imaging åå¤„ç†ç®—æ³•ï¼Œåˆ©ç”¨ä¸€ä¸ªå¹¿æ³›çš„å‚è€ƒå›¾ä¹¦é¦†æ¥å‡å°‘å¤©ä½“å™ªå£°</li>
<li>results: å¯¹30ä¸ªå”¯ä¸€çš„ç‚¹æºè¿›è¡Œè¯„ä¼°ï¼ŒConStruct æ¯”ä¼ ç»Ÿ PCA å¤„ç†æé«˜äº† S&#x2F;N çš„æ¯”ä¾‹ä¸º67%ï¼Œå¹¶æé«˜äº†å¯¹æ¯”åº¦çš„å› å­è¾¾2.6<details>
<summary>Abstract</summary>
We present a novel machine-learning approach for detecting faint point sources in high-contrast adaptive optics imaging datasets. The most widely used algorithms for primary subtraction aim to decouple bright stellar speckle noise from planetary signatures by subtracting an approximation of the temporally evolving stellar noise from each frame in an imaging sequence. Our approach aims to improve the stellar noise approximation and increase the planet detection sensitivity by leveraging deep learning in a novel direct imaging post-processing algorithm. We show that a convolutional autoencoder neural network, trained on an extensive reference library of real imaging sequences, accurately reconstructs the stellar speckle noise at the location of a potential planet signal. This tool is used in a post-processing algorithm we call Direct Exoplanet Detection with Convolutional Image Reconstruction, or ConStruct. The reliability and sensitivity of ConStruct are assessed using real Keck/NIRC2 angular differential imaging datasets. Of the 30 unique point sources we examine, ConStruct yields a higher S/N than traditional PCA-based processing for 67$\%$ of the cases and improves the relative contrast by up to a factor of 2.6. This work demonstrates the value and potential of deep learning to take advantage of a diverse reference library of point spread function realizations to improve direct imaging post-processing. ConStruct and its future improvements may be particularly useful as tools for post-processing high-contrast images from the James Webb Space Telescope and extreme adaptive optics instruments, both for the current generation and those being designed for the upcoming 30 meter-class telescopes.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹é«˜å¯¹æ¯”åº¦é€‚åº”å…‰å­¦å›¾åƒä¸­çš„æŸ”è½¯ç‚¹æºã€‚ç°æœ‰çš„ä¸»è¦ subtract ç®—æ³•ï¼Œç›®çš„æ˜¯ä»æ¯å¸§å›¾åƒåºåˆ—ä¸­æå–äº®åº¦å¼ºåº¦çš„æ’æ˜Ÿå™ªå£°ï¼Œä»¥ä¾¿ä» Ğ¿Ğ»Ğ°Ğ½ĞµÑ‚Ğ°Ñ€ signals ä¸­åˆ†ç¦»å‡º planetary signaturesã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯é€šè¿‡åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥æé«˜stellar å™ªå£°çš„ aproximationï¼Œå¹¶æé«˜æ£€æµ‹æ„Ÿåº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå·ç§¯è‡ªç¼–ç å™¨ç¥ç»ç½‘ç»œï¼Œè®­ç»ƒäºä¸€ä¸ªå¹¿æ³›çš„å‚è€ƒå›¾åƒåº“ä¸­ï¼Œå¯ä»¥å‡†ç¡®åœ°é‡å»ºæ’æ˜Ÿæ‚ç‚¹å™ªå£°ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º Direct Exoplanet Detection with Convolutional Image Reconstructionï¼Œæˆ–ConStructã€‚æˆ‘ä»¬ä½¿ç”¨å®éªŒå®¤ä¸­çš„Keck/NIRC2 angular differential imagingæ•°æ®é›†æ¥è¯„ä¼°ConStructçš„å¯é æ€§å’Œæ•æ„Ÿåº¦ã€‚æˆ‘ä»¬å‘ç°ï¼ŒConStruct æ¯”ä¼ ç»Ÿçš„PCA-basedå¤„ç†æ›´é«˜çš„Signal-to-Noise Ratioï¼ˆSNRï¼‰ï¼Œå¹¶ä¸”å¯ä»¥æé«˜å¯¹æ¯”åº¦çš„Relative Contrastã€‚è¿™ç§å·¥ä½œè¡¨æ˜äº†æ·±åº¦å­¦ä¹ çš„ä»·å€¼å’Œæ½œåœ¨ï¼Œå¯ä»¥åˆ©ç”¨å¤šæ ·åŒ–çš„ç‚¹æ‰©æ•£å‡½æ•°å®ç°æ¥æé«˜ç›´æ¥æˆåƒåå¤„ç†ã€‚ConStruct å’Œæœªæ¥çš„æ”¹è¿›å¯èƒ½å°†æˆä¸ºJames Webb Space Telescopeå’Œæé«˜å¯¹æ¯”åº¦é€‚åº”å…‰å­¦å·¥å…·çš„åå¤„ç†å·¥å…·ï¼ŒåŒ…æ‹¬å½“å‰çš„30ç±³çº§æœ›è¿œé•œã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-small-and-accurate-convolutional-neural-networks-for-acoustic-biodiversity-monitoring"><a href="#Towards-small-and-accurate-convolutional-neural-networks-for-acoustic-biodiversity-monitoring" class="headerlink" title="Towards small and accurate convolutional neural networks for acoustic biodiversity monitoring"></a>Towards small and accurate convolutional neural networks for acoustic biodiversity monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03666">http://arxiv.org/abs/2312.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serge Zaugg, Mike van der Schaar, Florence Erbs, Antonio Sanchez, Joan V. Castell, Emiliano Ramallo, Michel AndrÃ©</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯è®¾è®¡ä¸€äº›å¿«é€Ÿå¯¹åº”æ—¶é—´çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿å¤§è§„æ¨¡ç›‘æ§ç”Ÿç‰©å¤šæ ·æ€§ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†spectrograms from 10 second segments as input to CNNs, and designed a simple CNN architecture with a frequency unwrapping layer (SIMP-FU models) to improve the classification performance.</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨æ—¶é—´ç´¢å¼•çš„æ ‡ç­¾ durante la formaciÃ³n de los modelos SIMP-FU å¯ä»¥æé«˜åˆ†ç±»æ€§èƒ½ï¼Œå¹¶ä¸”æ¨¡å‹çš„é€‰æ‹©å¯ä»¥å½±å“åˆ†ç±»æ€§èƒ½ã€‚æœ€ä½³çš„SIMP-FUæ¨¡å‹åœ¨20ç§é¸Ÿç±»ä¸­çš„18ç§æµ‹è¯•é›†ä¸Šè·å¾—äº†AUCè¶…è¿‡0.95ã€‚æ­¤å¤–ï¼Œè¿™äº›æ¨¡å‹åœ¨å…·æœ‰ç›¸å¯¹è¾ƒä½çš„æˆæœ¬å’Œè®¾å¤‡ä¸Šè¿›è¡Œè¯„ä¼°ä¹Ÿèƒ½å¤Ÿå®ç°é«˜é€Ÿçš„å¯¹åº”æ—¶é—´ã€‚<details>
<summary>Abstract</summary>
Automated classification of animal sounds is a prerequisite for large-scale monitoring of biodiversity. Convolutional Neural Networks (CNNs) are among the most promising algorithms but they are slow, often achieve poor classification in the field and typically require large training data sets. Our objective was to design CNNs that are fast at inference time and achieve good classification performance while learning from moderate-sized data. Recordings from a rainforest ecosystem were used. Start and end-point of sounds from 20 bird species were manually annotated. Spectrograms from 10 second segments were used as CNN input. We designed simple CNNs with a frequency unwrapping layer (SIMP-FU models) such that any output unit was connected to all spectrogram frequencies but only to a sub-region of time, the Receptive Field (RF). Our models allowed experimentation with different RF durations. Models either used the time-indexed labels that encode start and end-point of sounds or simpler segment-level labels. Models learning from time-indexed labels performed considerably better than their segment-level counterparts. Best classification performances was achieved for models with intermediate RF duration of 1.5 seconds. The best SIMP-FU models achieved AUCs over 0.95 in 18 of 20 classes on the test set. On compact low-cost hardware the best SIMP-FU models evaluated up to seven times faster than real-time data acquisition. RF duration was a major driver of classification performance. The optimum of 1.5 s was in the same range as the duration of the sounds. Our models achieved good classification performance while learning from moderate-sized training data. This is explained by the usage of time-indexed labels during training and adequately sized RF. Results confirm the feasibility of deploying small CNNs with good classification performance on compact low-cost devices.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨åŒ–åŠ¨ç‰©å£°éŸ³åˆ†ç±»æ˜¯ç”Ÿç‰©å¤šæ ·æ€§å¤§è§„æ¨¡ç›‘æµ‹çš„å¿…è¦å‰æã€‚ convolutional neural networks (CNNs) æ˜¯æœ€æœ‰å‰é€”çš„ç®—æ³•ä¹‹ä¸€ï¼Œä½†å®ƒä»¬é€šå¸¸æ…¢äºæ‰§è¡Œé€Ÿåº¦ï¼Œ often achieve poor classification in the field å¹¶ä¸”é€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®¾è®¡fast at inference time çš„ CNNsï¼Œä»¥åŠå¯ä»¥ä»ä¸­ç­‰å¤§å°çš„è®­ç»ƒæ•°æ®é›†ä¸­å­¦ä¹ å¥½åˆ†ç±»æ€§èƒ½ã€‚æˆ‘ä»¬ä½¿ç”¨çƒ­å¸¦é›¨æ—ç”Ÿæ€ç³»ç»Ÿçš„å£°éŸ³è®°å½•ã€‚æˆ‘ä»¬ manually annotated the start and end points of 20ç§é¸Ÿç±»çš„å£°éŸ³ã€‚æˆ‘ä»¬ä½¿ç”¨10ç§’æ®µçš„spectrogramä½œä¸ºCNNè¾“å…¥ã€‚æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•çš„CNNï¼Œå³SIMP-FUæ¨¡å‹ï¼Œå…¶ä¸­ä»»ä½•è¾“å‡ºå•å…ƒéƒ½è¿æ¥åˆ°äº†æ‰€æœ‰çš„spectrogramé¢‘è°±ï¼Œä½†åªè¿æ¥åˆ°äº†ä¸€ä¸ªæ—¶é—´åŒºåŸŸï¼Œå³Receptive Field (RF)ã€‚æˆ‘ä»¬çš„æ¨¡å‹å…è®¸æˆ‘ä»¬åœ¨ä¸åŒçš„RFæŒç»­æ—¶é—´ä¸Šè¿›è¡Œå®éªŒã€‚æˆ‘ä»¬ä½¿ç”¨äº†æ—¶é—´ç´¢å¼•æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ç¼–ç äº†å£°éŸ³çš„å¼€å§‹å’Œç»“æŸæ—¶åˆ»ã€‚æˆ‘ä»¬çš„æ¨¡å‹æ¯”ä½¿ç”¨æ®µçº§æ ‡ç­¾è¡¨ç°å¾—æ›´å¥½ã€‚æœ€ä½³çš„SIMP-FUæ¨¡å‹åœ¨20ç§ç±»å‹çš„æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†AUCè¶…è¿‡0.95ã€‚åœ¨å…·æœ‰ç›¸åŒæ—¶é•¿çš„1.5ç§’çš„RFæŒç»­æ—¶é—´ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨ä½æˆæœ¬è®¾å¤‡ä¸Šè¯„ä¼°åˆ°7å€äºå®æ—¶æ•°æ®é‡‡é›†ã€‚RFæŒç»­æ—¶é—´æ˜¯åˆ†ç±»æ€§èƒ½çš„å…³é”®å› ç´ ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨1.5ç§’çš„RFæŒç»­æ—¶é—´ä¸‹ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥è¾¾åˆ°å¥½çš„åˆ†ç±»æ€§èƒ½ï¼ŒåŒæ—¶ä»ä¸­ç­‰å¤§å°çš„è®­ç»ƒæ•°æ®é›†ä¸­å­¦ä¹ ã€‚è¿™å¯ä»¥å½’å› äºåœ¨è®­ç»ƒæ—¶ä½¿ç”¨æ—¶é—´ç´¢å¼•æ ‡ç­¾ï¼Œä»¥åŠRFçš„å¤§å°ã€‚æˆ‘ä»¬çš„ç»“æœè¯æ˜äº†åœ¨ä½æˆæœ¬è®¾å¤‡ä¸Šéƒ¨ç½²å° CNNs çš„å¯è¡Œæ€§ï¼Œå¹¶ä¸”å¯ä»¥è¾¾åˆ°å¥½çš„åˆ†ç±»æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="MIRACLE-Inverse-Reinforcement-and-Curriculum-Learning-Model-for-Human-inspired-Mobile-Robot-Navigation"><a href="#MIRACLE-Inverse-Reinforcement-and-Curriculum-Learning-Model-for-Human-inspired-Mobile-Robot-Navigation" class="headerlink" title="MIRACLE: Inverse Reinforcement and Curriculum Learning Model for Human-inspired Mobile Robot Navigation"></a>MIRACLE: Inverse Reinforcement and Curriculum Learning Model for Human-inspired Mobile Robot Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03651">http://arxiv.org/abs/2312.03651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nihal Gunukula, Kshitij Tiwari, Aniket Bera</li>
<li>for: This paper aims to improve the navigation of mobile robots in emergency scenarios by enabling them to interpret stimuli like humans and locate potential victims rapidly without interfering with first responders.</li>
<li>methods: The proposed solution, called MIRACLE, uses gamified learning to gather stimuli-driven human navigational data, which is then used to train a Deep Inverse Maximum Entropy Reinforcement Learning model.</li>
<li>results: Testing revealed a low loss of 2.7717 within a 400-sized environment, indicating that the proposed approach can replicate human-like response. The approach has the potential to enhance the life-saving capabilities of mobile robots in emergency situations.<details>
<summary>Abstract</summary>
In emergency scenarios, mobile robots must navigate like humans, interpreting stimuli to locate potential victims rapidly without interfering with first responders. Existing socially-aware navigation algorithms face computational and adaptability challenges. To overcome these, we propose a solution, MIRACLE -- an inverse reinforcement and curriculum learning model, that employs gamified learning to gather stimuli-driven human navigational data. This data is then used to train a Deep Inverse Maximum Entropy Reinforcement Learning model, reducing reliance on demonstrator abilities. Testing reveals a low loss of 2.7717 within a 400-sized environment, signifying human-like response replication. Current databases lack comprehensive stimuli-driven data, necessitating our approach. By doing so, we enable robots to navigate emergency situations with human-like perception, enhancing their life-saving capabilities.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨ç´§æ€¥æƒ…å†µä¸‹ï¼Œç§»åŠ¨ Ñ€Ğ¾Ğ±Ğ¾ãƒƒTSå¿…é¡»å¦‚äººç±»ä¸€æ ·å¯¼èˆªï¼Œè§£è¯»åˆºæ¿€æ¥å¯»æ‰¾å—å®³è€…å¿«é€Ÿï¼Œä¸ä¼šå¹²æ‰°å…ˆæœŸåº”æ€¥æ•‘æ´äººå‘˜ã€‚ç°æœ‰çš„ç¤¾ä¼šæ„è¯†å¯¼èˆªç®—æ³•é¢ä¸´è®¡ç®—å’Œé€‚åº”æ€§æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§è§£å†³æ–¹æ¡ˆï¼šMIRACLEâ€”â€”ä¸€ç§ inverse reinforcement å’Œå­¦ä¹ çº§è¯¾ç¨‹å­¦ä¹ æ¨¡å‹ï¼Œä½¿ç”¨æ¸¸æˆåŒ–å­¦ä¹ æ–¹æ³•æ”¶é›†äººç±»å¯¼èˆªæ•°æ®ï¼Œå¹¶ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ·±åº¦åæœ€å¤§ç†µå¥–åŠ±å­¦ä¹ æ¨¡å‹ï¼Œå‡å°‘ä¾èµ–äºç¤ºèŒƒäººå‘˜çš„èƒ½åŠ›ã€‚æµ‹è¯•è¡¨æ˜åœ¨400ä¸ªç¯å¢ƒä¸­ï¼ŒæŸå¤±ä¸º2.7717ï¼Œè¿™è¡¨æ˜äº†äººç±»å¼çš„å“åº”å¤åˆ¶ã€‚å½“å‰çš„æ•°æ®åº“ç¼ºä¹å®Œæ•´çš„åˆºæ¿€é©±åŠ¨æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¿…è¦çš„ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥è®©æœºå™¨äººåœ¨ç´§æ€¥æƒ…å†µä¸‹å…·æœ‰äººç±»åŒ–çš„æ„ŸçŸ¥ï¼Œæé«˜å…¶æ•‘å‘½èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="MACCA-Offline-Multi-agent-Reinforcement-Learning-with-Causal-Credit-Assignment"><a href="#MACCA-Offline-Multi-agent-Reinforcement-Learning-with-Causal-Credit-Assignment" class="headerlink" title="MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment"></a>MACCA: Offline Multi-agent Reinforcement Learning with Causal Credit Assignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03644">http://arxiv.org/abs/2312.03644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyan Wang, Yali Du, Yudi Zhang, Meng Fang, Biwei Huang</li>
<li>for: æé«˜ Offline Multi-agent Reinforcement Learningï¼ˆMARLï¼‰çš„æ•ˆæœï¼Œ conquer online interaction æ˜¯ä¸å®é™…æˆ–å±é™©çš„åœºæ™¯ã€‚</li>
<li>methods: æˆ‘ä»¬çš„æ–¹æ³• MACCAï¼Œä½¿ç”¨ Dynamic Bayesian Network æè¿°ç¯å¢ƒå˜é‡ã€çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ä¹‹é—´çš„å…³ç³»ï¼Œé€šè¿‡åˆ†ææ¯ä¸ªä»£ç†çš„ä¸ªäººå¥–åŠ± causal å…³ç³»ï¼Œä»¥è·å¾—æ­£ç¡®å’Œå¯è¯»çš„å¥–åŠ±å½’å±ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒMACCA åœ¨ç¦»çº¿ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº† State-of-the-Art æ–¹æ³•ï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šæé«˜è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
Offline Multi-agent Reinforcement Learning (MARL) is valuable in scenarios where online interaction is impractical or risky. While independent learning in MARL offers flexibility and scalability, accurately assigning credit to individual agents in offline settings poses challenges due to partial observability and emergent behavior. Directly transferring the online credit assignment method to offline settings results in suboptimal outcomes due to the absence of real-time feedback and intricate agent interactions. Our approach, MACCA, characterizing the generative process as a Dynamic Bayesian Network, captures relationships between environmental variables, states, actions, and rewards. Estimating this model on offline data, MACCA can learn each agent's contribution by analyzing the causal relationship of their individual rewards, ensuring accurate and interpretable credit assignment. Additionally, the modularity of our approach allows it to seamlessly integrate with various offline MARL methods. Theoretically, we proved that under the setting of the offline dataset, the underlying causal structure and the function for generating the individual rewards of agents are identifiable, which laid the foundation for the correctness of our modeling. Experimentally, we tested MACCA in two environments, including discrete and continuous action settings. The results show that MACCA outperforms SOTA methods and improves performance upon their backbones.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Transformer-Powered-Surrogates-Close-the-ICF-Simulation-Experiment-Gap-with-Extremely-Limited-Data"><a href="#Transformer-Powered-Surrogates-Close-the-ICF-Simulation-Experiment-Gap-with-Extremely-Limited-Data" class="headerlink" title="Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data"></a>Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap with Extremely Limited Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03642">http://arxiv.org/abs/2312.03642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew L. Olson, Shusen Liu, Jayaraman J. Thiagarajan, Bogdan Kustowski, Weng-Keen Wong, Rushil Anirudh</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºå˜æ¢å™¨æ¶æ„çš„æ–¹æ³•ï¼Œä»¥æé«˜å¤šæ¨¡æ€è¾“å‡ºenarioä¸­çš„é¢„æµ‹ç²¾åº¦ï¼Œå½“ experimental data æ˜¯ç¨€ç¼ºçš„æ—¶å€™ï¼Œå¯ä»¥è¡¥å…… simulation dataã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨ transformer æ¶æ„ï¼Œå¹¶ç»“åˆäº†ä¸€ç§æ–°çš„å›¾å½¢åŸºäºçš„è¶…å‚æ•°ä¼˜åŒ–æŠ€æœ¯ã€‚</li>
<li>results: è¯¥æ–¹æ³•ä¸ä»…å¯ä»¥å‡å°‘ simulation biasï¼Œè€Œä¸”å¯ä»¥åœ¨å…·æœ‰ç¨€ç¼ºå®éªŒæ•°æ®çš„æƒ…å†µä¸‹å®ç°æ›´é«˜çš„é¢„æµ‹ç²¾åº¦ï¼Œå¹¶ä¸”åœ¨å°„å‡»æŸæŸæˆªå®éªŒä¸­å¾—åˆ°äº†è¯æ˜ã€‚<details>
<summary>Abstract</summary>
Recent advances in machine learning, specifically transformer architecture, have led to significant advancements in commercial domains. These powerful models have demonstrated superior capability to learn complex relationships and often generalize better to new data and problems. This paper presents a novel transformer-powered approach for enhancing prediction accuracy in multi-modal output scenarios, where sparse experimental data is supplemented with simulation data. The proposed approach integrates transformer-based architecture with a novel graph-based hyper-parameter optimization technique. The resulting system not only effectively reduces simulation bias, but also achieves superior prediction accuracy compared to the prior method. We demonstrate the efficacy of our approach on inertial confinement fusion experiments, where only 10 shots of real-world data are available, as well as synthetic versions of these experiments.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸæœºå™¨å­¦ä¹ æŠ€æœ¯çš„å‘å±•ï¼Œå°¤å…¶æ˜¯å˜æ¢å™¨æ¶æ„ï¼Œå·²ç»å¯¼è‡´å•†ä¸šé¢†åŸŸçš„é‡è¦è¿›æ­¥ã€‚è¿™äº›å¼ºå¤§çš„æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¤æ‚çš„å…³ç³»ï¼Œå¹¶ç»å¸¸åœ¨æ–°çš„æ•°æ®å’Œé—®é¢˜ä¸Šæ›´å¥½åœ°æ³›åŒ–ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå˜æ¢å™¨æ¶æ„çš„å¤šæ¨¡æ€è¾“å‡ºé¢„æµ‹ç²¾åº¦åŠ å¼ºæ–¹æ³•ï¼Œå…¶ä¸­ä½¿ç”¨äº†ä¸€ç§æ–°çš„å›¾åƒåŸºäºçš„è¶…å‚æ•°ä¼˜åŒ–æŠ€æœ¯ã€‚è¯¥ç³»ç»Ÿä¸ä»…èƒ½å¤Ÿæœ‰æ•ˆå‡å°‘æ¨¡æ‹Ÿæ•°æ®åè§ï¼Œè¿˜èƒ½å¤Ÿåœ¨å°ºåº¦ç²¾åº¦æ–¹é¢è¶…è¶Šå…ˆå‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å›ºæº¶ä½“å‹ç¼©å®éªŒä¸­è¿›è¡Œäº†è¯æ˜ï¼Œåªæœ‰10ä¸ªå®é™…æ•°æ®åˆ†å¸ƒçš„å®éªŒï¼Œä»¥åŠä¸€äº›äººå·¥ç”Ÿæˆçš„å®éªŒã€‚
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Data-and-Resource-Efficient-Device-Directed-Speech-Detection-with-Large-Foundation-Models"><a href="#Multimodal-Data-and-Resource-Efficient-Device-Directed-Speech-Detection-with-Large-Foundation-Models" class="headerlink" title="Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models"></a>Multimodal Data and Resource Efficient Device-Directed Speech Detection with Large Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03632">http://arxiv.org/abs/2312.03632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ä½¿è™šæ‹ŸåŠ©æ‰‹ä¸ç”¨æˆ·çš„äº¤äº’æ›´è‡ªç„¶ï¼Œå³ä½¿ç”¨æˆ·æ²¡æœ‰ä½¿ç”¨è§¦å‘è¯­å¥ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†1-bestå‡è®¾å’Œè§£ç å™¨ä¿¡å·ä»è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ç³»ç»Ÿï¼Œå¹¶å°†éŸ³é¢‘ç¼–ç å™¨çš„å¬è§‰ç‰¹å¾ä½œä¸ºè¾“å…¥ç‰¹å¾è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°ï¼Œä½¿ç”¨å¤šæ¨¡æ€æ•°æ®è¿›è¡Œè®­ç»ƒå¯ä»¥é™ä½ç­‰é”™ç‡ï¼ˆEERï¼‰ï¼Œå¹¶ä¸”åªéœ€ä½¿ç”¨80kæˆ–æ›´å°‘çš„ç¤ºä¾‹æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ä½¿ç”¨ä½ç»´ç‰¹åŒ–çš„éŸ³é¢‘è¡¨ç¤ºå¯ä»¥é™ä½EERã€‚<details>
<summary>Abstract</summary>
Interactions with virtual assistants typically start with a trigger phrase followed by a command. In this work, we explore the possibility of making these interactions more natural by eliminating the need for a trigger phrase. Our goal is to determine whether a user addressed the virtual assistant based on signals obtained from the streaming audio recorded by the device microphone. We address this task by combining 1-best hypotheses and decoder signals from an automatic speech recognition system with acoustic representations from an audio encoder as input features to a large language model (LLM). In particular, we are interested in data and resource efficient systems that require only a small amount of training data and can operate in scenarios with only a single frozen LLM available on a device. For this reason, our model is trained on 80k or less examples of multimodal data using a combination of low-rank adaptation and prefix tuning. We compare the proposed system to unimodal baselines and show that the multimodal approach achieves lower equal-error-rates (EERs), while using only a fraction of the training data. We also show that low-dimensional specialized audio representations lead to lower EERs than high-dimensional general audio representations.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šå¸¸ï¼Œä¸è™šæ‹ŸåŠ©æ‰‹äº¤äº’éƒ½éœ€è¦ä¸€ä¸ªè§¦å‘è¯­å¥å’Œä¸€ä¸ªå‘½ä»¤ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ä½¿è¿™äº›äº¤äº’æ›´è‡ªç„¶çš„å¯èƒ½æ€§ï¼Œå³æ¶ˆé™¤è§¦å‘è¯­å¥çš„éœ€æ±‚ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡åŸºäºæµåŠ¨éŸ³é¢‘è®°å½•çš„è®¾å¤‡ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½è·å¾—çš„ä¿¡å·æ¥ç¡®å®šç”¨æˆ·æ˜¯å¦å‘è™šæ‹ŸåŠ©æ‰‹è¿›è¡Œäº† addressedã€‚æˆ‘ä»¬è§£å†³è¿™ä¸ªä»»åŠ¡ by combining 1-best hypothesiså’Œ decoder signal from an automatic speech recognition system with acoustic representations from an audio encoder as input features to a large language model (LLM). Specifically, we are interested in data and resource-efficient systems that require only a small amount of training data and can operate in scenarios with only a single frozen LLM available on a device. For this reason, our model is trained on 80k or less examples of multimodal data using a combination of low-rank adaptation and prefix tuning. We compare the proposed system to unimodal baselines and show that the multimodal approach achieves lower equal-error-rates (EERs), while using only a fraction of the training data. We also show that low-dimensional specialized audio representations lead to lower EERs than high-dimensional general audio representations.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Active-Feature-Acquisition-Methods-for-Static-Feature-Settings"><a href="#Evaluation-of-Active-Feature-Acquisition-Methods-for-Static-Feature-Settings" class="headerlink" title="Evaluation of Active Feature Acquisition Methods for Static Feature Settings"></a>Evaluation of Active Feature Acquisition Methods for Static Feature Settings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03619">http://arxiv.org/abs/2312.03619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Henrik von Kleist, Alireza Zamanian, Ilya Shpitser, Narges Ahmidi<br>for:This paper focuses on evaluating the performance of active feature acquisition (AFA) agents in healthcare, where acquiring features can be costly or harmful. The authors aim to assess the expected performance of AFA agents using retrospective data.methods:The authors propose a semi-offline reinforcement learning (RL) framework for active feature acquisition performance evaluation (AFAPE), which considers time-dependent features. They also derive and adapt new estimators within the semi-offline RL framework, including inverse probability weighting (IPW), direct method (DM), and double reinforcement learning (DRL), to handle missing data.results:The authors demonstrate the improved data efficiency of their semi-offline RL estimators in synthetic and real-world data experiments under synthetic missing-at-random (MAR) and missing-not-at-random (MNAR) patterns.<details>
<summary>Abstract</summary>
Active feature acquisition (AFA) agents, crucial in domains like healthcare where acquiring features is often costly or harmful, determine the optimal set of features for a subsequent classification task. As deploying an AFA agent introduces a shift in missingness distribution, it's vital to assess its expected performance at deployment using retrospective data. In a companion paper, we introduce a semi-offline reinforcement learning (RL) framework for active feature acquisition performance evaluation (AFAPE) where features are assumed to be time-dependent. Here, we study and extend the AFAPE problem to cover static feature settings, where features are time-invariant, and hence provide more flexibility to the AFA agents in deciding the order of the acquisitions. In this static feature setting, we derive and adapt new inverse probability weighting (IPW), direct method (DM), and double reinforcement learning (DRL) estimators within the semi-offline RL framework. These estimators can be applied when the missingness in the retrospective dataset follows a missing-at-random (MAR) pattern. They also can be applied to missing-not-at-random (MNAR) patterns in conjunction with appropriate existing missing data techniques. We illustrate the improved data efficiency offered by the semi-offline RL estimators in synthetic and real-world data experiments under synthetic MAR and MNAR missingness.
</details>
<details>
<summary>æ‘˜è¦</summary>
aktive feature acquisition (AFA) ä»£ç†ï¼Œåœ¨åŒ»ç–—é¢†åŸŸå’Œå…¶ä»–é¢†åŸŸä¸­ï¼Œå¯ä»¥å¸®åŠ©å‡å°‘æˆæœ¬å’Œå±å®³ã€‚ AFA ä»£ç†å¯ä»¥ç¡®å®šä¸‹ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ä¸­çš„ä¼˜åŒ–åŠŸèƒ½é›†ã€‚ä½†æ˜¯ï¼Œåœ¨éƒ¨ç½² AFA ä»£ç†æ—¶ï¼Œä¼šå¯¼è‡´ç¼ºå¤±åˆ†å¸ƒçš„å˜åŒ–ï¼Œå› æ­¤åœ¨ä½¿ç”¨å›é¡¾æ•°æ®è¿›è¡Œè¯„ä¼°æ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨å¦ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŠçº¿ä¸Šå­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°æ´»åŠ¨ç‰¹å¾è·å–æ€§èƒ½ï¼ˆAFAPEï¼‰ï¼Œåœ¨è¿™ä¸ªæ¡†æ¶ä¸­ï¼Œç‰¹å¾æ˜¯æ—¶é—´ä¾èµ–çš„ã€‚åœ¨è¿™ä¸ªé™æ€ç‰¹å¾è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¹¶æ‰©å±• AFAPE é—®é¢˜ï¼Œä»¥æ¶µç›–é™æ€ç‰¹å¾è®¾ç½®ï¼Œè¿™äº›ç‰¹å¾æ˜¯æ—¶é—´ä¸å˜çš„ï¼Œå› æ­¤ç»™ AFA ä»£ç†æ›´å¤šçš„çµæ´»æ€§ï¼Œä»¥ç¡®å®šç‰¹å¾è·å–é¡ºåºã€‚åœ¨è¿™ä¸ªé™æ€ç‰¹å¾è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬ derive å’Œé€‚åº”æ–°çš„åæ˜ æƒé‡ï¼ˆIPWï¼‰ã€ç›´æ¥æ–¹æ³•ï¼ˆDMï¼‰å’ŒåŒçº¿ä¸Šå­¦ä¹ ï¼ˆDRLï¼‰ä¼°è®¡å™¨ï¼Œåœ¨åŠçº¿ä¸ŠRLæ¡†æ¶ä¸­ä½¿ç”¨ã€‚è¿™äº›ä¼°è®¡å™¨å¯ä»¥åœ¨å›é¡¾æ•°æ®ä¸­å­˜åœ¨ missing-at-randomï¼ˆMARï¼‰æ¨¡å¼ä¸‹åº”ç”¨ã€‚å®ƒä»¬ä¹Ÿå¯ä»¥åœ¨åˆé€‚çš„ missing data æŠ€æœ¯çš„æƒ…å†µä¸‹ï¼Œåœ¨ missing-not-at-randomï¼ˆMNARï¼‰æ¨¡å¼ä¸‹åº”ç”¨ã€‚æˆ‘ä»¬åœ¨ sintetic å’Œå®é™…æ•°æ®å®éªŒä¸­ï¼Œé€šè¿‡ä½¿ç”¨ semi-offline RL ä¼°è®¡å™¨ï¼Œæé«˜æ•°æ®æ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Physical-Symbolic-Optimization"><a href="#Physical-Symbolic-Optimization" class="headerlink" title="Physical Symbolic Optimization"></a>Physical Symbolic Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03612">http://arxiv.org/abs/2312.03612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wassimtenachi/physo">https://github.com/wassimtenachi/physo</a></li>
<li>paper_authors: Wassim Tenachi, Rodrigo Ibata, Foivos I. Diakogiannis</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§æŸç¼šè‡ªåŠ¨ç”Ÿæˆæ–¹ç¨‹çš„æ–¹æ³•ï¼Œä»¥ä¾¿ç¬¦åˆç»´åº¦åˆ†æè§„åˆ™ã€‚</li>
<li>methods: è¿™ä¸ªæ–¹æ³•ç»“åˆäº†å¼ºåŒ–å­¦ä¹ ï¼Œä½¿ç”¨ç‰©ç†ç¬¦å·æ¨è®ºæ–¹æ³•æ¥æ¢å¤ç‰©ç†æ•°æ®ä¸­çš„åˆ†æå‡½æ•°ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨SRBenchçš„è²æ¶…æ›¼æ ‡å‡†ä¸Šè¾¾åˆ°äº†çŠ¶æ€ä¹‹æœ€çš„ç»“æœï¼Œåœ¨å™ªéŸ³ï¼ˆå¤§äº0.1%ï¼‰å’Œé‡è¦å™ªéŸ³ï¼ˆ10%ï¼‰çš„æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”æ˜¾ç¤ºå‡ºé«˜åº¦é²æ£’æ€§ã€‚<details>
<summary>Abstract</summary>
We present a framework for constraining the automatic sequential generation of equations to obey the rules of dimensional analysis by construction. Combining this approach with reinforcement learning, we built $\Phi$-SO, a Physical Symbolic Optimization method for recovering analytical functions from physical data leveraging units constraints. Our symbolic regression algorithm achieves state-of-the-art results in contexts in which variables and constants have known physical units, outperforming all other methods on SRBench's Feynman benchmark in the presence of noise (exceeding 0.1%) and showing resilience even in the presence of significant (10%) levels of noise.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆæ–¹ç¨‹çš„é¡ºåºåŒ–ç”Ÿæˆï¼Œä»¥éµå¾ªç»´åº¦åˆ†æçš„è§„åˆ™ã€‚å°†è¿™ç§æ–¹æ³•ä¸å¼ºåŒ–å­¦ä¹ ç»“åˆï¼Œæˆ‘ä»¬æ„å»ºäº† $\Phi$-SOï¼Œä¸€ç§ç‰©ç†ç¬¦å·ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºä»ç‰©ç†æ•°æ®ä¸­æ¢å¤ç¬¦å·å‡½æ•°ï¼Œå¹¶ä¸”åˆ©ç”¨å•ä½çº¦æŸã€‚æˆ‘ä»¬çš„ç¬¦å·å›å½’ç®—æ³•åœ¨å˜é‡å’Œå¸¸æ•°å…·æœ‰çŸ¥é“ç‰©ç†å•ä½æ—¶è¾¾åˆ°äº†çŠ¶æ€å¯¹åº”çš„æœ€ä½³ç»“æœï¼Œåœ¨SRBenchçš„è´¹æ¶…æ›¼æ ‡å‡† benchmark ä¸­åœ¨å™ªéŸ³ï¼ˆè¶…è¿‡ 0.1%ï¼‰å­˜åœ¨æ—¶è¶…è¶Šæ‰€æœ‰å…¶ä»–æ–¹æ³•ï¼Œå¹¶åœ¨å™ªéŸ³æ°´å¹³è¾¾åˆ°äº†10%æ—¶ä»ç„¶ä¿æŒç¨³å®šã€‚
</details></li>
</ul>
<hr>
<h2 id="Achieving-O-Îµ-1-5-Complexity-in-Hessian-Jacobian-free-Stochastic-Bilevel-Optimization"><a href="#Achieving-O-Îµ-1-5-Complexity-in-Hessian-Jacobian-free-Stochastic-Bilevel-Optimization" class="headerlink" title="Achieving ${O}(Îµ^{-1.5})$ Complexity in Hessian&#x2F;Jacobian-free Stochastic Bilevel Optimization"></a>Achieving ${O}(Îµ^{-1.5})$ Complexity in Hessian&#x2F;Jacobian-free Stochastic Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03807">http://arxiv.org/abs/2312.03807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Yang, Peiyao Xiao, Kaiyi Ji</li>
<li>for: è¿™ç§é—®é¢˜çš„è§£å†³æ–¹æ³•ï¼Œå³åœ¨éå·ç§¯ä¼˜åŒ–é—®é¢˜ä¸­ï¼Œæé«˜ä¼˜åŒ–æ•ˆç‡ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„æ— çº¦æŸ&#x2F;çº¦æŸçº§åˆ«ä¼˜åŒ–ç®—æ³•ï¼Œåä¸ºFdeHBOï¼Œå®ƒå…·æœ‰ç®€å•çš„å•å¾ªç¯ç»“æ„ã€æŠ•å½± Ğ¿Ğ¾Ğ¼Ğ¾æ‰‹finite-differenceçº¦æŸçŸ©é˜µå‘é‡è¿‘ä¼¼ã€åŠ¿èƒ½ momentum-basedæ›´æ–°ã€‚</li>
<li>results: è¯æ˜äº†FdeHBOå¯ä»¥åœ¨${O}(\epsilon^{-1.5})$è¿­ä»£å†…ï¼ˆæ¯è¿­ä»£ä½¿ç”¨${O}(1)$æ ·æœ¬ï¼Œåªéœ€è¦é¦–é¢‘ Gradient ä¿¡æ¯ï¼‰æ‰¾åˆ°ä¸€ä¸ª$\epsilon$-ç²¾åº¦çš„ç«™ç‚¹ç‚¹ã€‚è¿™æ˜¯é¦–æ¬¡æ— çº¦æŸ&#x2F;çº¦æŸçº§åˆ«æ–¹æ³•ï¼Œå¯ä»¥åœ¨éå·ç§¯-å¼ºçƒˆæŸä¸‹è¾¾åˆ°${O}(\epsilon^{-1.5})$ æ ·æœ¬å¤æ‚åº¦ã€‚<details>
<summary>Abstract</summary>
In this paper, we revisit the bilevel optimization problem, in which the upper-level objective function is generally nonconvex and the lower-level objective function is strongly convex. Although this type of problem has been studied extensively, it still remains an open question how to achieve an ${O}(\epsilon^{-1.5})$ sample complexity of ${O}(\epsilon^{-1.5})$ in Hessian/Jacobian-free stochastic bilevel optimization without any second-order derivative computation. To fill this gap, we propose a novel Hessian/Jacobian-free bilevel optimizer named FdeHBO, which features a simple fully single-loop structure, a projection-aided finite-difference Hessian/Jacobian-vector approximation, and momentum-based updates. Theoretically, we show that FdeHBO requires ${O}(\epsilon^{-1.5})$ iterations (each using ${O}(1)$ samples and only first-order gradient information) to find an $\epsilon$-accurate stationary point. As far as we know, this is the first Hessian/Jacobian-free method with an ${O}(\epsilon^{-1.5})$ sample complexity for nonconvex-strongly-convex stochastic bilevel optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†äºŒçº§ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­ä¸Šå±‚ç›®æ ‡å‡½æ•°é€šå¸¸æ˜¯éå‡¸å‡½æ•°ï¼Œè€Œä¸‹å±‚ç›®æ ‡å‡½æ•°æ˜¯å¼º converges å‡½æ•°ã€‚è™½ç„¶è¿™ç§é—®é¢˜å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œä½†è¿˜æ²¡æœ‰ä»»ä½•æ–¹æ³•å¯ä»¥åœ¨åå¯¼æ•°/å¯¼å‡½æ•°-è‡ªç”± Stochastic bilevel ä¼˜åŒ–ä¸­å®ç° ${O}(\epsilon^{-1.5})$ æ ·æœ¬å¤æ‚åº¦ã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸€æ¼æ´ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ Hessian/Jacobian-free äºŒçº§ä¼˜åŒ–å™¨ named FdeHBOï¼Œå®ƒå…·æœ‰ç®€å•çš„å¹‚ç­‰å¾ªç¯ç»“æ„ã€æŠ•å½± aid finite-difference Hessian/Jacobian-vector  aproximationä»¥åŠåŠ¿èƒ½-based æ›´æ–°ã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº† FdeHBO éœ€è¦ ${O}(\epsilon^{-1.5})$ è¿­ä»£ï¼ˆæ¯è¿­ä»£ä½¿ç”¨ ${O}(1)$ æ ·æœ¬å’Œåªéœ€è¦é¦–é¢‘å¯¼æ•°ä¿¡æ¯ï¼‰å¯ä»¥æ‰¾åˆ° $\epsilon$-ç²¾åº¦ç«™ç‚¹ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ª Hessian/Jacobian-free æ–¹æ³•ï¼Œå…¶æ ·æœ¬å¤æ‚åº¦ä¸º ${O}(\epsilon^{-1.5})$  Ğ´Ğ»Ñéå‡¸-å¼º converges éšæœºäºŒçº§ä¼˜åŒ–ã€‚â€Note that Simplified Chinese is a simplified version of Chinese, and it is used in mainland China and Singapore. Traditional Chinese is used in Taiwan, Hong Kong, and Macau.
</details></li>
</ul>
<hr>
<h2 id="Blueprinting-the-Future-Automatic-Item-Categorization-using-Hierarchical-Zero-Shot-and-Few-Shot-Classifiers"><a href="#Blueprinting-the-Future-Automatic-Item-Categorization-using-Hierarchical-Zero-Shot-and-Few-Shot-Classifiers" class="headerlink" title="Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers"></a>Blueprinting the Future: Automatic Item Categorization using Hierarchical Zero-Shot and Few-Shot Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03561">http://arxiv.org/abs/2312.03561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting Wang, Keith Stelter, Jenn Floyd, Thomas Oâ€™Neill, Nathaniel Hendrix, Andrew Bazemore, Kevin Rode, Warren Newton</li>
<li>For: The paper aims to develop a novel approach for hierarchical item categorization in testing industries, specifically for aligning exam questions with the designated content domains outlined in the assessment blueprint.* Methods: The proposed approach utilizes the zero-shot and few-shot Generative Pretrained Transformer (GPT) classifier, which leverages human-like language descriptions to define categories. The hierarchical nature of examination blueprints is navigated using a structured python dictionary, allowing for a tiered classification of items across multiple levels.* Results: The proposed method achieves an average accuracy of 92.91% measured by the F1 score in an initial simulation with artificial data. Additionally, the method was applied to real exam items from the 2022 In-Training Examination (ITE) conducted by the American Board of Family Medicine (ABFM), reclassifying 200 items according to a newly formulated blueprint swiftly in 15 minutes, a task that traditionally could span several days among editors and physicians.<details>
<summary>Abstract</summary>
In testing industry, precise item categorization is pivotal to align exam questions with the designated content domains outlined in the assessment blueprint. Traditional methods either entail manual classification, which is laborious and error-prone, or utilize machine learning requiring extensive training data, often leading to model underfit or overfit issues. This study unveils a novel approach employing the zero-shot and few-shot Generative Pretrained Transformer (GPT) classifier for hierarchical item categorization, minimizing the necessity for training data, and instead, leveraging human-like language descriptions to define categories. Through a structured python dictionary, the hierarchical nature of examination blueprints is navigated seamlessly, allowing for a tiered classification of items across multiple levels. An initial simulation with artificial data demonstrates the efficacy of this method, achieving an average accuracy of 92.91% measured by the F1 score. This method was further applied to real exam items from the 2022 In-Training Examination (ITE) conducted by the American Board of Family Medicine (ABFM), reclassifying 200 items according to a newly formulated blueprint swiftly in 15 minutes, a task that traditionally could span several days among editors and physicians. This innovative approach not only drastically cuts down classification time but also ensures a consistent, principle-driven categorization, minimizing human biases and discrepancies. The ability to refine classifications by adjusting definitions adds to its robustness and sustainability.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æµ‹è¯•ä¸šç•Œï¼Œç²¾å‡†çš„é¡¹ç›®åˆ†ç±»æ˜¯è€ƒè¯•è¯„ä¼°è“å›¾ä¸­çš„å…³é”®å› ç´ ï¼Œä»¥ç¡®ä¿è€ƒè¯•é—®é¢˜ä¸æŒ‡å®šçš„å†…å®¹é¢†åŸŸç›¸åŒ¹é…ã€‚ä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ˜¯æ‰‹åŠ¨åˆ†ç±»ï¼Œè¿™æ˜¯æ—¶é—´consumingå’Œå®¹æ˜“å‡ºé”™çš„ï¼Œæˆ–è€…ä½¿ç”¨æœºå™¨å­¦ä¹ ï¼Œéœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œç»å¸¸ä¼šå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆæˆ–è€…ä¸‹é™é—®é¢˜ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨é›¶æ‰¹å’Œå‡ æ‰¹ç”Ÿæˆæœç´¢transformerï¼ˆGPTï¼‰åˆ†ç±»å™¨ï¼Œå®ç°äº†ä¸éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯é€šè¿‡äººç±»è¯­è¨€æè¿°æ¥å®šä¹‰åˆ†ç±»ã€‚é€šè¿‡ç»“æ„åŒ–çš„pythonå­—å…¸ï¼Œæµ¸å…¥è€ƒè¯•è“å›¾çš„å±‚æ¬¡ç»“æ„ï¼Œå®ç°äº†å¤šçº§åˆ†ç±»ã€‚åœ¨äººå·¥æ•°æ®ä¸Šè¿›è¡Œçš„åˆæ­¥æ¨¡æ‹Ÿä¸­ï¼Œè¿™ç§æ–¹æ³•å®ç°äº†92.91%çš„å‡†ç¡®ç‡ï¼Œ measured by F1 scoreã€‚è¿™ç§æ–¹æ³•ç»§è€Œåº”ç”¨äº2022å¹´å®¶åº­åŒ»å­¦è¯„ä¼°ï¼ˆABFMï¼‰çš„å®é™…è€ƒè¯•é¢˜ï¼Œå¯¹200ä¸ªé¡¹ç›®è¿›è¡Œäº†æ ¹æ®æ–°çš„è“å›¾å¿«é€Ÿé‡æ–°åˆ†ç±»ï¼Œåªéœ€15åˆ†é’Ÿï¼Œè€Œä¼ ç»Ÿä¸Šéœ€è¦æ•°å¤©å†…ç”±ç¼–è¾‘å’ŒåŒ»ç”Ÿå…±åŒåŠªåŠ›å®Œæˆã€‚è¿™ç§åˆ›æ–°çš„æ–¹æ³•ä¸ä»…å‡å°‘äº†åˆ†ç±»æ—¶é—´ï¼Œè€Œä¸”ç¡®ä¿äº†ä¸€è‡´ã€åŸåˆ™é©±åŠ¨çš„åˆ†ç±»ï¼Œå‡å°‘äº†äººç±»åè§å’Œå·®å¼‚ã€‚å¯ä»¥é€šè¿‡è°ƒæ•´å®šä¹‰æ¥è¿›ä¸€æ­¥æé«˜å…¶å¯é æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Clustering-by-Contour-coreset-and-variational-quantum-eigensolver"><a href="#Clustering-by-Contour-coreset-and-variational-quantum-eigensolver" class="headerlink" title="Clustering by Contour coreset and variational quantum eigensolver"></a>Clustering by Contour coreset and variational quantum eigensolver</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03516">http://arxiv.org/abs/2312.03516</a></li>
<li>repo_url: None</li>
<li>paper_authors: Canaan Yung, Muhammad Usman</li>
<li>for: è§£å†³é‡å­è®¡ç®—æœºä¸Šçš„k-meansèšç±»é—®é¢˜</li>
<li>methods: ä½¿ç”¨é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•ï¼ˆQAOAï¼‰å’Œç‰¹å®šçš„æ ¸å¿ƒé›†æŠ€æœ¯</li>
<li>results: æ¯”å¯¹ç°æœ‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„VQE+Contouræ ¸å¿ƒé›†æ–¹æ³•åœ¨çœŸå®æ•°æ®ä¸Šè¾¾åˆ°äº†æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´ä½çš„æ ‡å‡†å·®In English, this translates to:</li>
<li>for: Solving the k-means clustering problem on quantum computers</li>
<li>methods: Using the Quantum Approximate Optimization Algorithm (QAOA) and customized coreset techniques</li>
<li>results: Our VQE+Contour coreset approach outperforms existing QAOA+coreset k-means clustering approaches with higher accuracy and lower standard deviation on real-life data.<details>
<summary>Abstract</summary>
Recent work has proposed solving the k-means clustering problem on quantum computers via the Quantum Approximate Optimization Algorithm (QAOA) and coreset techniques. Although the current method demonstrates the possibility of quantum k-means clustering, it does not ensure high accuracy and consistency across a wide range of datasets. The existing coreset techniques are designed for classical algorithms and there has been no quantum-tailored coreset technique which is designed to boost the accuracy of quantum algorithms. In this work, we propose solving the k-means clustering problem with the variational quantum eigensolver (VQE) and a customised coreset method, the Contour coreset, which has been formulated with specific focus on quantum algorithms. Extensive simulations with synthetic and real-life data demonstrated that our VQE+Contour Coreset approach outperforms existing QAOA+Coreset k-means clustering approaches with higher accuracy and lower standard deviation. Our work has shown that quantum tailored coreset techniques has the potential to significantly boost the performance of quantum algorithms when compared to using generic off-the-shelf coreset techniques.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸç ”ç©¶æå‡ºä½¿ç”¨é‡å­è®¡ç®—æœºè§£å†³k-meansåˆ’åˆ†é—®é¢˜çš„æ–¹æ³•ï¼Œä½¿ç”¨é‡å­approxä¼˜åŒ–ç®—æ³•ï¼ˆQAOAï¼‰å’Œæ ¸é›†æŠ€æœ¯ã€‚although current method demonstrates the possibility of quantum k-means clustering, it does not ensure high accuracy and consistency across a wide range of datasets. existing coreset techniques are designed for classical algorithms, and there has been no quantum-tailored coreset technique that is designed to boost the accuracy of quantum algorithms. in this work, we propose solving the k-means clustering problem with the variational quantum eigensolver (VQE) and a customized coreset method, the Contour coreset, which has been formulated with specific focus on quantum algorithms. extensive simulations with synthetic and real-life data demonstrated that our VQE+Contour Coreset approach outperforms existing QAOA+Coreset k-means clustering approaches with higher accuracy and lower standard deviation. our work has shown that quantum-tailored coreset techniques have the potential to significantly boost the performance of quantum algorithms when compared to using generic off-the-shelf coreset techniques.Note: The translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Towards-Sobolev-Pruning"><a href="#Towards-Sobolev-Pruning" class="headerlink" title="Towards Sobolev Pruning"></a>Towards Sobolev Pruning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03510">http://arxiv.org/abs/2312.03510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neil Kichler, Sher Afghan, Uwe Naumann</li>
<li>For: The paper aims to propose a method for building surrogate models that capture the sensitivity information of the original model, using interval adjoint significance analysis and Sobolev training.* Methods: The proposed method uses a neural network to model the original sensitivity information, and combines interval adjoint significance analysis and Sobolev training to prune the network and obtain an accurate surrogate model.* Results: The proposed method is experimentally validated on an example of pricing a multidimensional basket option, and the results show that the surrogate model accurately captures the sensitivity information of the original model. The method is not limited to quantitative finance and can be applied to other domains as well.Hereâ€™s the Chinese translation of the three points:* For: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ•æ„Ÿä¿¡æ¯çš„å‰¯æ¨¡å‹å»ºæ¨¡æ–¹æ³•ï¼Œä½¿ç”¨é—´éš”é€»è¾‘é‡è¦æ€§åˆ†æå’Œ Sobolev è®­ç»ƒã€‚* Methods: è¯¥æ–¹æ³•ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹åŸå§‹æ¨¡å‹çš„æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶å°†é—´éš”é€»è¾‘é‡è¦æ€§åˆ†æå’Œ Sobolev è®­ç»ƒç›¸ç»“åˆï¼Œä»¥å‡å°‘ç¥ç»ç½‘ç»œçš„å¤§å°å¹¶è·å¾—å‡†ç¡®çš„å‰¯æ¨¡å‹ã€‚* Results: è¯¥æ–¹æ³•åœ¨ä¸€ä¸ªå¤šç»´åŒ–çš„ç¯®çƒé€‰æ‹©ä»·å€¼æ¨¡å‹ä¸­è¿›è¡Œå®éªŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤ºå‰¯æ¨¡å‹å‡†ç¡®åœ°æ•æ‰äº†åŸå§‹æ¨¡å‹çš„æ•æ„Ÿä¿¡æ¯ã€‚è¯¥æ–¹æ³•ä¸ä»…é™äºé‡‘èé¢†åŸŸï¼Œä¹Ÿé€‚ç”¨äºå…¶ä»–é¢†åŸŸã€‚<details>
<summary>Abstract</summary>
The increasing use of stochastic models for describing complex phenomena warrants surrogate models that capture the reference model characteristics at a fraction of the computational cost, foregoing potentially expensive Monte Carlo simulation. The predominant approach of fitting a large neural network and then pruning it to a reduced size has commonly neglected shortcomings. The produced surrogate models often will not capture the sensitivities and uncertainties inherent in the original model. In particular, (higher-order) derivative information of such surrogates could differ drastically. Given a large enough network, we expect this derivative information to match. However, the pruned model will almost certainly not share this behavior.   In this paper, we propose to find surrogate models by using sensitivity information throughout the learning and pruning process. We build on work using Interval Adjoint Significance Analysis for pruning and combine it with the recent advancements in Sobolev Training to accurately model the original sensitivity information in the pruned neural network based surrogate model. We experimentally underpin the method on an example of pricing a multidimensional Basket option modelled through a stochastic differential equation with Brownian motion. The proposed method is, however, not limited to the domain of quantitative finance, which was chosen as a case study for intuitive interpretations of the sensitivities. It serves as a foundation for building further surrogate modelling techniques considering sensitivity information.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€å¤æ‚ç°è±¡çš„æè¿°ä½¿ç”¨æ¸è¿›æ¨¡å‹çš„ä½¿ç”¨é€æ¸å¢é•¿ï¼Œå› æ­¤éœ€è¦ä¼˜åŒ–æ¨¡å‹çš„å‡†ç¡®æ€§å’Œè®¡ç®—æ•ˆç‡ã€‚ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯é€šè¿‡å¤§å‹ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ï¼Œç„¶åå‰ªè¾‘å…¶å¤§å°ï¼Œä½†è¿™ç§æ–¹æ³•å¸¸å¸¸å¿½ç•¥äº†åŸå§‹æ¨¡å‹çš„æ•æ„Ÿæ€§å’Œä¸ç¡®å®šæ€§ã€‚å…·ä½“æ¥è¯´ï¼Œç¥ç»ç½‘ç»œç”Ÿæˆçš„æ›¿ä»£æ¨¡å‹é€šå¸¸ä¸ä¼šæ•æ‰åŸå§‹æ¨¡å‹ä¸­çš„æ•æ„Ÿæ€§å’Œä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯é«˜é˜¶å¯¼æ•°ä¿¡æ¯å¯èƒ½ä¼šå¼‚å¸¸å¤§ã€‚å¦‚æœç¥ç»ç½‘ç»œå¤Ÿå¤§ï¼Œæˆ‘ä»¬å¯ä»¥æœŸæœ›å¯¼æ•°ä¿¡æ¯ä¼šåŒ¹é…ã€‚ç„¶è€Œï¼Œå‰ªè¾‘åçš„æ¨¡å‹å‡ ä¹ç»ä¸ä¼šå…·å¤‡è¿™ç§è¡Œä¸ºã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ•æ„Ÿä¿¡æ¯çš„æ›¿ä»£æ¨¡å‹å»ºæ¨¡æ–¹æ³•ã€‚æˆ‘ä»¬åŸºäºä¹‹å‰çš„é—´éš”å¯¹ä»·å€¼åˆ†ææŠ€æœ¯å’Œ Sobolev è®­ç»ƒæŠ€æœ¯ï¼Œå°†æ•æ„Ÿä¿¡æ¯çº³å…¥å­¦ä¹ å’Œå‰ªè¾‘è¿‡ç¨‹ä¸­ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå¤šç»´ Brownian Motion æ¨¡å‹æ¥è¯æ˜æ–¹æ³•ï¼Œä½†è¿™ç§æ–¹æ³•å¹¶ä¸å±€é™äºé‡‘èé¢†åŸŸã€‚æˆ‘ä»¬é€‰æ‹©äº†è¿™ä¸ªé¢†åŸŸä½œä¸ºç¤ºä¾‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°è§£é‡Šæ•æ„Ÿä¿¡æ¯çš„å«ä¹‰ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ä½œä¸ºå»ºç«‹æ›´å¤šåŸºäºæ•æ„Ÿä¿¡æ¯çš„æ›¿ä»£æ¨¡å‹æŠ€æœ¯çš„åŸºç¡€ã€‚
</details></li>
</ul>
<hr>
<h2 id="PCDP-SGD-Improving-the-Convergence-of-Differentially-Private-SGD-via-Projection-in-Advance"><a href="#PCDP-SGD-Improving-the-Convergence-of-Differentially-Private-SGD-via-Projection-in-Advance" class="headerlink" title="PCDP-SGD: Improving the Convergence of Differentially Private SGD via Projection in Advance"></a>PCDP-SGD: Improving the Convergence of Differentially Private SGD via Projection in Advance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03792">http://arxiv.org/abs/2312.03792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haichao Sha, Ruixuan Liu, Yixuan Liu, Hong Chen</li>
<li>for: æä¾›äº†ä¸€ä¸ªæ¦‚å¿µï¼Œå³ä½¿åœ¨ä¸­å¤®åŒ–å’Œè”åˆè®¾ç½®ä¸­è®­ç»ƒæ•°æ®æ—¶æä¾›äº†ä¸€å®šçš„ç†è®ºä¿è¯ï¼Œä½†æ˜¯ç”±äºDP-SGDçš„ä½¿ç”¨ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœå—åˆ°é™åˆ¶ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ¡†æ¶ï¼Œå³PCDP-SGDï¼Œå®ƒé€šè¿‡å¯¹æ¢¯åº¦normè¿›è¡Œå‹ç¼©ï¼Œå¹¶åœ¨å‹ç¼©åè¿›è¡ŒæŠ•å½±æ“ä½œï¼Œä»¥ä¿ç•™æ›´é‡è¦çš„æ¢¯åº¦ç»„ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ‰©å±•äº†PCDP-SGDä½œä¸ºDPFLçš„åŸºæœ¬ç»„ä»¶ï¼Œä»¥é€‚åº”æ•°æ®ä¸å‡è¡¡çš„æŒ‘æˆ˜å¹¶å®ç°é«˜æ•ˆçš„é€šä¿¡ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPCDP-SGDåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å¯ä»¥è¾¾åˆ°æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨ä¿è¯DPçš„æƒ…å†µä¸‹ï¼ŒPCDP-SGDè¿˜èƒ½å¤Ÿè¶…è¶Šç°æœ‰çš„DP-SGDå˜ä½“ã€‚æ­¤å¤–ï¼ŒPCDP-SGDä¹Ÿå¯ä»¥åœ¨ä¸åŒçš„è”åˆè®¾ç½®ä¸‹å®ç°æ›´é«˜æ•ˆçš„é€šä¿¡ã€‚<details>
<summary>Abstract</summary>
The paradigm of Differentially Private SGD~(DP-SGD) can provide a theoretical guarantee for training data in both centralized and federated settings. However, the utility degradation caused by DP-SGD limits its wide application in high-stakes tasks, such as medical image diagnosis. In addition to the necessary perturbation, the convergence issue is attributed to the information loss on the gradient clipping. In this work, we propose a general framework PCDP-SGD, which aims to compress redundant gradient norms and preserve more crucial top gradient components via projection operation before gradient clipping. Additionally, we extend PCDP-SGD as a fundamental component in differential privacy federated learning~(DPFL) for mitigating the data heterogeneous challenge and achieving efficient communication. We prove that pre-projection enhances the convergence of DP-SGD by reducing the dependence of clipping error and bias to a fraction of the top gradient eigenspace, and in theory, limits cross-client variance to improve the convergence under heterogeneous federation. Experimental results demonstrate that PCDP-SGD achieves higher accuracy compared with state-of-the-art DP-SGD variants in computer vision tasks. Moreover, PCDP-SGD outperforms current federated learning frameworks when DP is guaranteed on local training sets.
</details>
<details>
<summary>æ‘˜è¦</summary>
DP-SGDï¼ˆå·®å¼‚åŠ Private Stochastic Gradient Descentï¼‰çš„ paradigmå¯ä»¥ä¸ºä¸­å¤®åŒ–å’Œè”åˆè®¾ç½®çš„è®­ç»ƒæ•°æ®æä¾›ç†è®ºä¿è¯ã€‚ç„¶è€Œï¼ŒDP-SGDçš„å®ç”¨æ•ˆæœå—åˆ°è®­ç»ƒæ•°æ®çš„é«˜åº¦é£é™©ä»»åŠ¡ï¼Œå¦‚åŒ»ç–—å›¾åƒè¯Šæ–­ä¸­çš„æ•°æ®æ•æ„Ÿæ€§é™åˆ¶ã€‚æ­¤å¤–ï¼ŒDP-SGDéœ€è¦é¢å¤–å¢åŠ å™ªå£°ï¼Œå¹¶ä¸”åœ¨æƒé‡æŠ‘åˆ¶ä¸­äº§ç”Ÿä¿¡æ¯æŸå¤±ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒè¿‡ç¨‹ä¸­çš„åç§»ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šç”¨æ¡†æ¶PCDP-SGDï¼Œè¯¥æ¡†æ¶é€šè¿‡å¯¹ gradient norm è¿›è¡Œå‹ç¼©å’Œä¿ç•™æ›´é‡è¦çš„ top gradient åˆ†é‡æ¥é™ä½æƒé‡æŠ‘åˆ¶ä¸­çš„åç§»å’Œå™ªå£°ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ‰©å±•äº†PCDP-SGDä½œä¸ºåœ¨åˆ†å¸ƒå¼éšç§å­¦ä¹ ä¸­çš„åŸºæœ¬ç»„ä»¶ï¼Œä»¥mitigate æ•°æ®ä¸å‡æ€§æŒ‘æˆ˜å’Œå®ç°é«˜æ•ˆçš„é€šä¿¡ã€‚æˆ‘ä»¬è¯æ˜äº†åœ¨å‰å‘å‹ç¼©åï¼ŒDP-SGD çš„æ•´åˆä¼šé™ä½æƒé‡æŠ‘åˆ¶ä¸­çš„åç§»å’Œå™ªå£°ï¼Œå¹¶åœ¨ç†è®ºä¸Šé™åˆ¶äº†è·¨å®¢æˆ·ç«¯çš„å·®å¼‚ï¼Œä»è€Œæé«˜äº† federated learning çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPCDP-SGD åœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­å®ç°äº†æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨ä¿è¯äº†åœ°æ–¹è®­ç»ƒé›†çš„éšç§æ€§çš„æƒ…å†µä¸‹ï¼ŒPCDP-SGD è¿˜èƒ½å¤Ÿè¶…è¶Šå½“å‰çš„è”åˆå­¦ä¹ æ¡†æ¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Schrodinger-Bridges-Beat-Diffusion-Models-on-Text-to-Speech-Synthesis"><a href="#Schrodinger-Bridges-Beat-Diffusion-Models-on-Text-to-Speech-Synthesis" class="headerlink" title="Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis"></a>Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03491">http://arxiv.org/abs/2312.03491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zehua Chen, Guande He, Kaiwen Zheng, Xu Tan, Jun Zhu</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§æ–°çš„æ–‡æœ¬è¯†åˆ«ç³»ç»Ÿï¼Œå®ƒå¯ä»¥æé«˜æ–‡æœ¬è¯†åˆ«çš„è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿä½¿ç”¨äº†ä¸€ç§æ–°çš„æ‰©å±•æ–¹æ³•ï¼Œå³å°†æ–‡æœ¬è¾“å…¥çš„å¹²æ‰°ç‰¹å¾ä½œä¸ºå®ƒçš„å…ˆå‰åˆ†å¸ƒï¼Œç„¶åä½¿ç”¨è¿™ä¸ªå¹²æ‰°ç‰¹å¾æ¥ç”Ÿæˆé«˜è´¨é‡çš„è¯­éŸ³ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–°çš„æ–‡æœ¬è¯†åˆ«ç³»ç»Ÿå¯ä»¥åœ¨LJ-Speechæ•°æ®é›†ä¸Šæä¾›é«˜è´¨é‡çš„è¯­éŸ³ç”Ÿæˆï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿçš„æ‰©å±•æ–¹æ³• Grad-TTS å’Œå¿«é€Ÿçš„æ–‡æœ¬è¯†åˆ«æ¨¡å‹åœ¨50æ­¥&#x2F;1000æ­¥ç”Ÿæˆå’Œå‡ æ­¥ç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
In text-to-speech (TTS) synthesis, diffusion models have achieved promising generation quality. However, because of the pre-defined data-to-noise diffusion process, their prior distribution is restricted to a noisy representation, which provides little information of the generation target. In this work, we present a novel TTS system, Bridge-TTS, making the first attempt to substitute the noisy Gaussian prior in established diffusion-based TTS methods with a clean and deterministic one, which provides strong structural information of the target. Specifically, we leverage the latent representation obtained from text input as our prior, and build a fully tractable Schrodinger bridge between it and the ground-truth mel-spectrogram, leading to a data-to-data process. Moreover, the tractability and flexibility of our formulation allow us to empirically study the design spaces such as noise schedules, as well as to develop stochastic and deterministic samplers. Experimental results on the LJ-Speech dataset illustrate the effectiveness of our method in terms of both synthesis quality and sampling efficiency, significantly outperforming our diffusion counterpart Grad-TTS in 50-step/1000-step synthesis and strong fast TTS models in few-step scenarios. Project page: https://bridge-tts.github.io/
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æ–‡æœ¬åˆ°è¯­éŸ³ï¼ˆTTSï¼‰åˆæˆä¸­ï¼Œæ‰©æ•£æ¨¡å‹å·²ç»å®ç°äº†å‡ºè‰²çš„ç”Ÿæˆè´¨é‡ã€‚ç„¶è€Œï¼Œç”±äºå­˜åœ¨é¢„å®šçš„æ•°æ®åˆ°å™ªå£°æ‰©æ•£è¿‡ç¨‹ï¼Œå…¶å…ˆå‰åˆ†å¸ƒå—åˆ°å™ªå£°å½±å“ï¼Œæä¾›äº† little information about the generation targetã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„ TTS ç³»ç»Ÿï¼Œåä¸º Bridge-TTSï¼Œè¿™æ˜¯é¦–æ¬¡å°†é¢„å…ˆå®šä¹‰çš„å™ªå£° Gaussian å…ˆéªŒæ›¿æ¢ä¸ºå¹²å‡€çš„æŸç¼šå…ˆéªŒï¼Œæä¾›äº†å¼ºæœ‰åŠ›çš„ç»“æ„ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬åˆ©ç”¨æ–‡æœ¬è¾“å…¥æ‰€å¾—çš„æ½œåœ¨è¡¨ç¤ºä½œä¸ºæˆ‘ä»¬çš„å…ˆéªŒï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªå®Œå…¨å¯è¿½è¸ªçš„æ–½ç½—å¾·ä¼¯æ ¼ä¹‹æ¡¥ï¼Œå°†å…¶ä¸çœŸå®çš„ mel-spectrogram ç›¸è¿æ¥ï¼Œä»è€Œå®ç°äº†æ•°æ®åˆ°æ•°æ®è¿‡ç¨‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å½¢å¼åŒ–è¡¨è¿°çš„å¯è¿½è¸ªæ€§å’Œçµæ´»æ€§ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå®éªŒä¸åŒçš„å™ªå£°è®¡åˆ’ï¼Œä»¥åŠå¼€å‘éšæœºå’Œå†³å®šæ€§æŠ½å–å™¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ LJ-Speech æ•°æ®é›†ä¸Šå…·æœ‰å‡ºè‰²çš„ç”Ÿæˆè´¨é‡å’ŒæŠ½å–æ•ˆç‡ï¼Œåœ¨ 50 æ­¥/1000 æ­¥åˆæˆå’Œå¿«é€Ÿ TTS æ¨¡å‹ä¸­æ˜¾è‘—è¶…è¶Šäº†æˆ‘ä»¬çš„æ‰©æ•£å¯¹æ‰‹ Grad-TTSï¼Œå¹¶åœ¨å‡ æ­¥æƒ…å†µä¸‹ä¸å¿«é€Ÿ TTS æ¨¡å‹åŒ¹é…ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://bridge-tts.github.io/
</details></li>
</ul>
<hr>
<h2 id="Precision-of-Individual-Shapley-Value-Explanations"><a href="#Precision-of-Individual-Shapley-Value-Explanations" class="headerlink" title="Precision of Individual Shapley Value Explanations"></a>Precision of Individual Shapley Value Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03485">http://arxiv.org/abs/2312.03485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lars Henry Berge Olsen<br>for: This paper focuses on explaining predictions made by complex machine learning models using Shapley values for tabular data.methods: The paper compares numerous Shapley value estimation methods and discusses their precision on an individual basis.results: The explanations are systematically less precise for observations on the outer region of the training data distribution for all used estimation methods.<details>
<summary>Abstract</summary>
Shapley values are extensively used in explainable artificial intelligence (XAI) as a framework to explain predictions made by complex machine learning (ML) models. In this work, we focus on conditional Shapley values for predictive models fitted to tabular data and explain the prediction $f(\boldsymbol{x}^{*})$ for a single observation $\boldsymbol{x}^{*}$ at the time. Numerous Shapley value estimation methods have been proposed and empirically compared on an average basis in the XAI literature. However, less focus has been devoted to analyzing the precision of the Shapley value explanations on an individual basis. We extend our work in Olsen et al. (2023) by demonstrating and discussing that the explanations are systematically less precise for observations on the outer region of the training data distribution for all used estimation methods. This is expected from a statistical point of view, but to the best of our knowledge, it has not been systematically addressed in the Shapley value literature. This is crucial knowledge for Shapley values practitioners, who should be more careful in applying these observations' corresponding Shapley value explanations.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ²™ä½©åˆ©å€¼åœ¨å¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆXAIï¼‰ä¸­å¹¿æ³›åº”ç”¨ä¸ºè§£é‡Šå¤æ‚æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹çš„é¢„æµ‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæ¡ä»¶æ²™ä½©åˆ©å€¼å¯¹é¢„æµ‹æ¨¡å‹é€‚ç”¨äºè¡¨æ ¼æ•°æ®çš„è§£é‡Šï¼Œå¹¶å¯¹å•ä¸ªè§‚å¯Ÿå€¼ $\boldsymbol{x}^{*}$ çš„é¢„æµ‹ $f(\boldsymbol{x}^{*})$ è¿›è¡Œè§£é‡Šã€‚æ–‡çŒ®ä¸­å·²ç»æœ‰è®¸å¤šæ²™ä½©åˆ©å€¼ä¼°è®¡æ–¹æ³•çš„æ¯”è¾ƒï¼Œä½†æ˜¯å¯¹å…·ä½“çš„ä¸ªä½“è§£é‡Šç²¾åº¦çš„åˆ†æå¾—åˆ°äº†æ›´å°‘çš„å…³æ³¨ã€‚æˆ‘ä»¬åœ¨å¥¥å°”æ£®ç­‰ï¼ˆ2023ï¼‰çš„å·¥ä½œä¸­è¿›ä¸€æ­¥æ¨åŠ¨äº†æˆ‘ä»¬çš„ç ”ç©¶ï¼Œå¹¶è¯æ˜äº†æ‰€æœ‰ä½¿ç”¨çš„ä¼°è®¡æ–¹æ³•çš„è§£é‡Šéƒ½ä¼šåœ¨è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„å¤–éƒ¨åŒºåŸŸ observation ä¸Šç³»ç»Ÿæ€§åœ°å‡å°‘ç²¾åº¦ã€‚è¿™æ˜¯é¢„æœŸçš„ä»ç»Ÿè®¡è§’åº¦æ¥çœ‹ï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“è¿™å¹¶æ²¡æœ‰åœ¨æ²™ä½©åˆ©å€¼æ–‡çŒ®ä¸­å¾—åˆ°ç³»ç»Ÿçš„è€ƒè™‘ã€‚è¿™äº›çŸ¥è¯†å¯¹æ²™ä½©åˆ©å€¼å®è·µè€…æ¥è¯´éå¸¸é‡è¦ï¼Œä»–ä»¬åº”è¯¥æ›´åŠ å°å¿ƒåœ°åº”ç”¨è¿™äº›è§‚å¯Ÿå€¼å¯¹åº”çš„æ²™ä½©åˆ©å€¼è§£é‡Šã€‚
</details></li>
</ul>
<hr>
<h2 id="Search-Strategies-for-Self-driving-Laboratories-with-Pending-Experiments"><a href="#Search-Strategies-for-Self-driving-Laboratories-with-Pending-Experiments" class="headerlink" title="Search Strategies for Self-driving Laboratories with Pending Experiments"></a>Search Strategies for Self-driving Laboratories with Pending Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03466">http://arxiv.org/abs/2312.03466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wen, Jakob Zeitler, Connor Rupnow</li>
<li>For: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¼‚æ­¥å¹¶å‘å®éªŒå®¤(SDL)ä¸­å®éªŒçš„å¹¶å‘å¹¶è¡ŒåŒ–ï¼Œä»¥åŠå»¶è¿Ÿåé¦ˆçš„å½±å“ã€‚* Methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªSDLçš„æ¨¡æ‹Ÿå™¨ï¼Œå¹¶å¯¹ä¸åŒçš„æœç´¢ç­–ç•¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ä¼˜åŒ–åŠŸèƒ½è†œçš„å¯¼ç”µæ€§ã€‚* Results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå¼‚æ­¥å¹¶å‘å®éªŒå®¤ä¸­çš„å»¶è¿Ÿåé¦ˆä¼šå½±å“æœç´¢ç­–ç•¥çš„æ€§èƒ½ã€‚ä¸åŒçš„æœç´¢ç­–ç•¥åœ¨ä¸åŒçš„å»¶è¿Ÿå’Œé—®é¢˜ç»´åº¦ä¸‹çš„æ€§èƒ½æœ‰æ˜¾è‘—çš„åŒºåˆ«ã€‚<details>
<summary>Abstract</summary>
Self-driving laboratories (SDLs) consist of multiple stations that perform material synthesis and characterisation tasks. To minimize station downtime and maximize experimental throughput, it is practical to run experiments in asynchronous parallel, in which multiple experiments are being performed at once in different stages. Asynchronous parallelization of experiments, however, introduces delayed feedback (i.e. "pending experiments"), which is known to reduce Bayesian optimiser performance. Here, we build a simulator for a multi-stage SDL and compare optimisation strategies for dealing with delayed feedback and asynchronous parallelized operation. Using data from a real SDL, we build a ground truth Bayesian optimisation simulator from 177 previously run experiments for maximizing the conductivity of functional coatings. We then compare search strategies such as expected improvement, noisy expected improvement, 4-mode exploration and random sampling. We evaluate their performance in terms of amount of delay and problem dimensionality. Our simulation results showcase the trade-off between the asynchronous parallel operation and delayed feedback.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨é©¾é©¶å®éªŒå®¤ï¼ˆSDLï¼‰ç”±å¤šä¸ªç«™ç‚¹ç»„æˆï¼Œæ¯ä¸ªç«™ç‚¹è´Ÿè´£ææ–™åˆæˆå’Œç‰¹å¾æµ‹è¯•ä»»åŠ¡ã€‚ä¸ºæœ€å°åŒ–ç«™ç‚¹åœæœºæ—¶é—´å’Œæœ€å¤§åŒ–å®éªŒé€šè¿‡putï¼Œå¯ä»¥åœ¨ä¸åŒé˜¶æ®µè¿›è¡Œå¼‚æ­¥å¹¶è¡Œå®éªŒã€‚ç„¶è€Œï¼Œå¼‚æ­¥å¹¶è¡Œå®éªŒä¼šå¯¼è‡´å»¶è¿Ÿåé¦ˆï¼ˆå³â€œç­‰å¾…å®éªŒâ€ï¼‰ï¼Œè¿™çŸ¥é“ä¼šé™ä½ bayesianä¼˜åŒ–å™¨æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå¤šé˜¶æ®µ SDL ä¸Šå»ºç«‹äº†ä¸€ä¸ªæ¨¡æ‹Ÿå™¨ï¼Œå¹¶æ¯”è¾ƒäº†å„ç§æœç´¢ç­–ç•¥æ¥å¤„ç†å»¶è¿Ÿåé¦ˆå’Œå¼‚æ­¥å¹¶è¡Œæ“ä½œã€‚ä½¿ç”¨å®é™… SDL æ•°æ®ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŸºäº 177 æ¬¡å®éªŒçš„ Bayesian ä¼˜åŒ–å™¨æ¨¡æ‹Ÿå™¨ï¼Œä»¥æœ€å¤§åŒ–åŠŸèƒ½æ¶‚å±‚çš„å¯¼ç”µæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†æœç´¢ç­–ç•¥ï¼Œå¦‚æœŸæœ›æ”¹å–„ã€å™ªå£°æœŸæœ›æ”¹å–„ã€4 ç§æ¢ç´¢å’ŒéšæœºæŠ½æ ·ã€‚æˆ‘ä»¬æ ¹æ®å»¶è¿Ÿå’Œé—®é¢˜ç»´åº¦æ¥è¯„ä¼° Ğ¸Ñ…æ€§èƒ½ã€‚æˆ‘ä»¬çš„æ¨¡æ‹Ÿç»“æœæ˜¾ç¤ºå¼‚æ­¥å¹¶è¡Œæ“ä½œå’Œå»¶è¿Ÿåé¦ˆä¹‹é—´å­˜åœ¨è´Ÿç›¸å…³æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Subnetwork-to-go-Elastic-Neural-Network-with-Dynamic-Training-and-Customizable-Inference"><a href="#Subnetwork-to-go-Elastic-Neural-Network-with-Dynamic-Training-and-Customizable-Inference" class="headerlink" title="Subnetwork-to-go: Elastic Neural Network with Dynamic Training and Customizable Inference"></a>Subnetwork-to-go: Elastic Neural Network with Dynamic Training and Customizable Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03464">http://arxiv.org/abs/2312.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Li, Yi Luo</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æå‡ºä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œä½¿å¾—åœ¨æ¨ç†é˜¶æ®µå¯ä»¥ä»ä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œä¸­æå–ä¸€ä¸ªå­ç½‘ç»œï¼Œå¹¶ä¸”è¿™ä¸ªå­ç½‘ç»œå¯ä»¥æœ‰ä»»æ„æ·±åº¦å’Œå®½åº¦ï¼Œè€Œä¸éœ€è¦ä»scratch retrainedã€‚</li>
<li>methods: ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…è®¸åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨åŠ¨æ€æ·±åº¦å’Œå®½åº¦æ¥è®­ç»ƒä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œï¼Œç„¶ååœ¨æ¨ç†é˜¶æ®µå¯ä»¥é€‰æ‹©ä¸€ä¸ªå­ç½‘ç»œï¼Œå¹¶ä¸”è¿™ä¸ªå­ç½‘ç»œå¯ä»¥æœ‰ä»»æ„æ·±åº¦å’Œå®½åº¦ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„å­ç½‘ç»œå¤§å°å’Œå¤æ‚åº¦ä¸‹æé«˜åˆ†ç¦»æ€§èƒ½ï¼Œå¹¶ä¸”è®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œçš„æ—¶é—´æ¯”å•ç‹¬è®­ç»ƒæ‰€æœ‰çš„å­ç½‘ç»œè¦ shorterã€‚<details>
<summary>Abstract</summary>
Deploying neural networks to different devices or platforms is in general challenging, especially when the model size is large or model complexity is high. Although there exist ways for model pruning or distillation, it is typically required to perform a full round of model training or finetuning procedure in order to obtain a smaller model that satisfies the model size or complexity constraints. Motivated by recent works on dynamic neural networks, we propose a simple way to train a large network and flexibly extract a subnetwork from it given a model size or complexity constraint during inference. We introduce a new way to allow a large model to be trained with dynamic depth and width during the training phase, and after the large model is trained we can select a subnetwork from it with arbitrary depth and width during the inference phase with a relatively better performance compared to training the subnetwork independently from scratch. Experiment results on a music source separation model show that our proposed method can effectively improve the separation performance across different subnetwork sizes and complexities with a single large model, and training the large model takes significantly shorter time than training all the different subnetworks.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šå¸¸æ¥è¯´ï¼Œå°†ç¥ç»ç½‘ç»œéƒ¨ç½²åˆ°ä¸åŒçš„è®¾å¤‡æˆ–å¹³å°æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œç‰¹åˆ«æ˜¯å½“æ¨¡å‹å¤§å°æˆ–æ¨¡å‹å¤æ‚æ€§è¾ƒé«˜æ—¶ã€‚è™½ç„¶å­˜åœ¨æ¨¡å‹å‰ªè¾‘æˆ–æ¶²æ€ç²¾ç‚¼çš„æ–¹æ³•ï¼Œä½†é€šå¸¸éœ€è¦è¿›è¡Œå…¨å±€çš„æ¨¡å‹è®­ç»ƒæˆ– Fine-tuning è¿‡ç¨‹ä»¥è·å¾—ç¬¦åˆæ¨¡å‹å¤§å°æˆ–å¤æ‚æ€§çº¦æŸçš„å°æ¨¡å‹ã€‚å—åˆ°æœ€è¿‘çš„åŠ¨æ€ç¥ç»ç½‘ç»œç ”ç©¶çš„å¯å‘ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œå¯ä»¥åœ¨æœç´¢è¿‡ç¨‹ä¸­è®­ç»ƒä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µé€‰æ‹©ä¸€ä¸ªå­ç½‘ç»œï¼Œå¹¶ä¸”è¯¥å­ç½‘ç»œå¯ä»¥æœ‰ä»»æ„çš„æ·±åº¦å’Œå®½åº¦ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå¯ä»¥è®©ä¸€ä¸ªå¤§å‹æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨åŠ¨æ€æ·±åº¦å’Œå®½åº¦ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µé€‰æ‹©ä¸€ä¸ªå­ç½‘ç»œï¼Œå¹¶ä¸”è¿™ä¸ªå­ç½‘ç»œå¯ä»¥æœ‰ä»»æ„çš„æ·±åº¦å’Œå®½åº¦ï¼Œè€Œä¸éœ€è¦ä»é›¶å¼€å§‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„å­ç½‘ç»œå¤§å°å’Œå¤æ‚æ€§ä¸‹æé«˜åˆ†ç¦»æ€§èƒ½ï¼Œå¹¶ä¸”è®­ç»ƒå¤§å‹æ¨¡å‹çš„æ—¶é—´æ¯”è®­ç»ƒå„ç§ä¸åŒçš„å­ç½‘ç»œæ›´ä¸ºå¿«é€Ÿã€‚
</details></li>
</ul>
<hr>
<h2 id="Run-LoRA-Run-Faster-and-Lighter-LoRA-Implementations"><a href="#Run-LoRA-Run-Faster-and-Lighter-LoRA-Implementations" class="headerlink" title="Run LoRA Run: Faster and Lighter LoRA Implementations"></a>Run LoRA Run: Faster and Lighter LoRA Implementations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03415">http://arxiv.org/abs/2312.03415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daria Cherniuk, Aleksandr Mikhalev, Ivan Oseledets</li>
<li>for: æé«˜ç¥ç»ç½‘ç»œè®­ç»ƒå’Œå¾®è°ƒé€Ÿåº¦</li>
<li>methods: ä½¿ç”¨ä½çº§æ‰©å±•å™¨æ¥å‡å°‘ç¥ç»ç½‘ç»œå‚æ•°æ•°é‡</li>
<li>results: å®ç°äº†é«˜æ•ˆçš„ç¥ç»ç½‘ç»œè®­ç»ƒå’Œå¾®è°ƒï¼Œå¹¶ä¸”ä¸ä¼šäº§ç”Ÿå‡å°‘ç²¾åº¦çš„é—®é¢˜ï¼Œå®éªŒç»“æœæ˜¾ç¤ºå¯ä»¥è¾¾åˆ°17%çš„é€Ÿåº¦æå‡<details>
<summary>Abstract</summary>
LoRA is a technique that reduces the number of trainable parameters in a neural network by introducing low-rank adapters to linear layers. This technique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA). This paper presents the RunLoRA framework for efficient implementations of LoRA that significantly improves the speed of neural network training and fine-tuning using low-rank adapters. The proposed implementation optimizes the computation of LoRA operations based on dimensions of corresponding linear layer, layer input dimensions and lora rank by choosing best forward and backward computation graph based on FLOPs and time estimations, resulting in faster training without sacrificing accuracy. The experimental results show up to 17% speedup on Llama family of models.
</details>
<details>
<summary>æ‘˜è¦</summary>
LoRA æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå¯ä»¥å‡å°‘ neural network ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œé€šè¿‡å¼•å…¥ä½çº§ adapter æ¥çº¿æ€§å±‚ã€‚è¿™ç§æŠ€æœ¯åœ¨ fine-tuning å’Œå…¨éƒ¨è®­ç»ƒä¸­éƒ½å¯ä»¥ä½¿ç”¨ï¼ˆLoRAã€QLoRAã€ReLoRAï¼‰ã€‚æœ¬æ–‡æå‡ºäº† RunLoRA æ¡†æ¶ï¼Œç”¨äºé«˜æ•ˆåœ°å®ç° LoRAï¼Œå¹¶å¯ä»¥æ˜¾è‘—æé«˜ neural network è®­ç»ƒå’Œ fine-tuning çš„é€Ÿåº¦ï¼Œæ— éœ€ç‰ºç‰²å‡†ç¡®æ€§ã€‚è¯¥å®ç°åŸºäº linear layer çš„ç»´åº¦ã€è¾“å…¥ç»´åº¦å’Œ LoRA çº§åˆ«ï¼Œé€‰æ‹©æœ€ä½³çš„å‰å‘å’Œåå‘è®¡ç®—å›¾ï¼Œä»¥æ ¹æ® FLOPs å’Œæ—¶é—´ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´å¿«çš„è®­ç»ƒï¼Œè€Œæ— éœ€ç‰ºç‰²å‡†ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå¯ä»¥è¾¾åˆ° LLama å®¶æ—æ¨¡å‹çš„17%é€Ÿåº¦æå‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="An-AI-for-Scientific-Discovery-Route-between-Amorphous-Networks-and-Mechanical-Behavior"><a href="#An-AI-for-Scientific-Discovery-Route-between-Amorphous-Networks-and-Mechanical-Behavior" class="headerlink" title="An AI for Scientific Discovery Route between Amorphous Networks and Mechanical Behavior"></a>An AI for Scientific Discovery Route between Amorphous Networks and Mechanical Behavior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03404">http://arxiv.org/abs/2312.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changliang Zhu, Chenchao Fang, Zhipeng Jin, Baowen Li, Xiangying Shen, Lei Xu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨äººå·¥æ™ºèƒ½å¦‚ä½•å¸®åŠ©ç§‘å­¦ç ”ç©¶äººå‘˜æ­ç¤ºç‰©ç†æœºåˆ¶ï¼Œå¹¶ä½¿ç”¨è¿™äº›æœºåˆ¶æé«˜æœºå™¨å­¦ä¹ ç®—æ³•çš„æ•ˆç‡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å¯¹æå€¼å¼—æ´›ä¼¦çŸ©é˜µçš„ç ”ç©¶ä½œä¸ºæ¡ˆä¾‹ï¼Œä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ¢è®¨å¼—æ´›ä¼¦çŸ©é˜µçš„ç‰©ç†æœºåˆ¶ï¼Œå¹¶é€šè¿‡è¿™äº›æœºåˆ¶æé«˜æœºå™¨å­¦ä¹ ç®—æ³•çš„æ•ˆç‡ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ä½¿ç”¨åŠ¨æ€çŸ©é˜µçš„ä½é¢‘æŒ¯è¡æ¨¡å¼ï¼Œå¯ä»¥æ›´é«˜æ•ˆåœ°é¢„æµ‹æå€¼å¼—æ´›ä¼¦çŸ©é˜µçš„å¼—æ´›ä¼¦çŸ©ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æé«˜æœºå™¨å­¦ä¹ ç®—æ³•çš„æ•ˆç‡ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºå…¶ä»–ç‰©ç†ç³»ç»Ÿçš„ç ”ç©¶ã€‚<details>
<summary>Abstract</summary>
"AI for science" is widely recognized as a future trend in the development of scientific research. Currently, although machine learning algorithms have played a crucial role in scientific research with numerous successful cases, relatively few instances exist where AI assists researchers in uncovering the underlying physical mechanisms behind a certain phenomenon and subsequently using that mechanism to improve machine learning algorithms' efficiency. This article uses the investigation into the relationship between extreme Poisson's ratio values and the structure of amorphous networks as a case study to illustrate how machine learning methods can assist in revealing underlying physical mechanisms. Upon recognizing that the Poisson's ratio relies on the low-frequency vibrational modes of dynamical matrix, we can then employ a convolutional neural network, trained on the dynamical matrix instead of traditional image recognition, to predict the Poisson's ratio of amorphous networks with a much higher efficiency. Through this example, we aim to showcase the role that artificial intelligence can play in revealing fundamental physical mechanisms, which subsequently improves the machine learning algorithms significantly.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Infinite-Width-Analysis-on-the-Jacobian-Regularised-Training-of-a-Neural-Network"><a href="#An-Infinite-Width-Analysis-on-the-Jacobian-Regularised-Training-of-a-Neural-Network" class="headerlink" title="An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network"></a>An Infinite-Width Analysis on the Jacobian-Regularised Training of a Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03386">http://arxiv.org/abs/2312.03386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taeyoung Kim, Hongseok Yang</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ¢è®¨äº†æ·±åº¦ç¥ç»ç½‘ç»œåœ¨æ— ç©·å®½é™åˆ¶ä¸‹çš„åˆå§‹åŒ–ã€ç‰¹å¾å­¦ä¹ å’Œè®­ç»ƒï¼Œä»¥åŠå¦‚ä½•æ‰¾åˆ°é€‚å½“çš„è¶…å‚æ•°ã€å­¦ä¹ ç½‘ç»œæƒé‡å’Œè¿›è¡Œæ¨ç†ã€‚</li>
<li>methods: æœ¬è®ºæ–‡æ‰©å±•äº†è¿™ä¸€çº¿ç´¢ï¼Œè¡¨æ˜åœ¨ Jacobian çš„æƒ…å†µä¸‹ï¼Œä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å’Œå…¶ Jacobian åœ¨åˆå§‹åŒ–æ—¶å…±åŒæ•´åˆåˆ°ä¸€ä¸ª Gaussian Processï¼ˆGPï¼‰ä¸­ï¼Œå¹¶ characterize è¿™ä¸ª GPã€‚</li>
<li>results: æˆ‘ä»¬è¯æ˜åœ¨æ— ç©·å®½é™åˆ¶ä¸‹ï¼ŒMLP çš„æ¼”åŒ–æ˜¯ç”±ä¸€ç§ linear first-order ordinary differential equation æè¿°ï¼Œè¿™ä¸ª differential equation æ˜¯ç”±ä¸€ç§å˜ç§çš„ Neural Tangent Kernel å†³å®šã€‚æˆ‘ä»¬è¿˜é€šè¿‡å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„ç†è®ºç»“è®ºå¯¹å®½finiteç½‘ç»œæœ‰ relevanceï¼Œå¹¶é€šè¿‡å®éªŒåˆ†æ kernel regression çš„æ€§è´¨æ¥è·å¾—ä¸€ç§ Jacobian è§„èŒƒåŒ–çš„ç†è§£ã€‚<details>
<summary>Abstract</summary>
The recent theoretical analysis of deep neural networks in their infinite-width limits has deepened our understanding of initialisation, feature learning, and training of those networks, and brought new practical techniques for finding appropriate hyperparameters, learning network weights, and performing inference. In this paper, we broaden this line of research by showing that this infinite-width analysis can be extended to the Jacobian of a deep neural network. We show that a multilayer perceptron (MLP) and its Jacobian at initialisation jointly converge to a Gaussian process (GP) as the widths of the MLP's hidden layers go to infinity and characterise this GP. We also prove that in the infinite-width limit, the evolution of the MLP under the so-called robust training (i.e., training with a regulariser on the Jacobian) is described by a linear first-order ordinary differential equation that is determined by a variant of the Neural Tangent Kernel. We experimentally show the relevance of our theoretical claims to wide finite networks, and empirically analyse the properties of kernel regression solution to obtain an insight into Jacobian regularisation.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-variants-of-SVM-methods-applied-to-GPR-data-to-classify-tack-coat-characteristics-in-French-pavements-two-experimental-case-studies"><a href="#On-the-variants-of-SVM-methods-applied-to-GPR-data-to-classify-tack-coat-characteristics-in-French-pavements-two-experimental-case-studies" class="headerlink" title="On the variants of SVM methods applied to GPR data to classify tack coat characteristics in French pavements: two experimental case studies"></a>On the variants of SVM methods applied to GPR data to classify tack coat characteristics in French pavements: two experimental case studies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03351">http://arxiv.org/abs/2312.03351</a></li>
<li>repo_url: None</li>
<li>paper_authors: GrÃ©gory Andreoli, Amine Ihamouten, Mai Lan Nguyen, Yannick Fargier, Cyrille Fauchard, Jean-Michel Simonin, Viktoriia Buliuk, David Souriou, Xavier DÃ©robert</li>
<li>for: ç”¨äºè¯„ä¼°æ³•å›½é“è·¯åšåº¦çš„éç ´åæ€§æŠ€æœ¯ä¹‹ä¸€æ˜¯åœ°é¢æ¢æµ‹é›·è¾¾ï¼ˆGPRï¼‰ï¼Œä½†ä¼ ç»Ÿçš„é›·è¾¾ç³»ç»Ÿå’Œå‰å‘å¤„ç†æ–¹æ³•åœ¨è¾ƒè–„çš„å±‚æ¬¡ä¸­çš„ç‰©ç†å’Œå‡ ä½•ç‰¹å¾åŒ–æ–¹é¢å­˜åœ¨å±€é™æ€§ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†åŸºäºæœºå™¨å­¦ä¹ æ–¹æ³•çš„é€†å‘æ–¹æ³•ï¼Œå¹¶åœ¨å…ˆå‰çš„æ•°æ®ä¸ŠéªŒè¯äº†å…¶æ•°å­¦å¯è¡Œæ€§ã€‚åœ¨è¿™ä¸¤ä¸ªå®éªŒæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†SVM&#x2F;SVRæ–¹æ³•æ¥åˆ†ç±»å’Œä¼°ç®—æ¶‚æŠ¹å±‚ä¸­çš„èšåˆç‰©å«é‡ã€‚</li>
<li>results: åœ¨ Gustave Eiffel University ï¼ˆæ³•å›½å—ç‰¹ï¼‰çš„æµ‹è¯•è½®å’Œæ–°çš„å®é™…é“è·¯ï¼ˆæ³•å›½å¢ç“¦å°”æ²³åœ°åŒºï¼‰ä¸­ï¼ŒSVM&#x2F;SVRæ–¹æ³•è¡¨ç°å‡ºäº†æ•ˆç‡ï¼Œå¯ä»¥å‡†ç¡®åœ°åˆ†ç±»å’Œä¼°ç®—æ¶‚æŠ¹å±‚ä¸­çš„èšåˆç‰©å«é‡ã€‚<details>
<summary>Abstract</summary>
Among the commonly used non-destructive techniques, the Ground Penetrating Radar (GPR) is one of the most widely adopted today for assessing pavement conditions in France. However, conventional radar systems and their forward processing methods have shown their limitations for the physical and geometrical characterization of very thin layers such as tack coats. However, the use of Machine Learning methods applied to GPR with an inverse approach showed that it was numerically possible to identify the tack coat characteristics despite masking effects due to low timefrequency resolution noted in the raw B-scans. Thus, we propose in this paper to apply the inverse approach based on Machine Learning, already validated in previous works on numerical data, on two experimental cases with different pavement structures. The first case corresponds to a validation on known pavement structures on the Gustave Eiffel University (Nantes, France) with its pavement fatigue carousel and the second case focuses on a new real road in Vend{\'e}e department (France). In both case studies, the performances of SVM/SVR methods showed the efficiency of supervised learning methods to classify and estimate the emulsion proportioning in the tack coats.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸­å›½å¸¸ç”¨çš„é destruktive æŠ€æœ¯ä¸­ï¼Œåœ°é¢æ¢æµ‹é›·è¾¾ï¼ˆGPRï¼‰æ˜¯æ³•å›½ä»Šå¤©æœ€å¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°è·¯é¢çŠ¶å†µçš„æ–¹æ³•ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„é›·è¾¾ç³»ç»Ÿå’Œå…¶å‰å‘å¤„ç†æ–¹æ³•åœ¨è¦†ç›–å±‚è¾ƒè–„æ—¶æ˜¾ç¤ºäº†å…¶é™åˆ¶ã€‚ç„¶è€Œï¼Œé€šè¿‡æœºå™¨å­¦ä¹ æ–¹æ³•åº”ç”¨äºGPRçš„é€†å‘æ–¹æ³•å¯ä»¥å¿«é€Ÿåœ°è¯†åˆ«æ¶‚å±‚ç‰¹å¾ï¼Œå°½ç®¡ Raw B-scan ä¸­å­˜åœ¨ä½æ—¶é¢‘åˆ†è¾¨ç‡çš„é®ç›–æ•ˆæœã€‚å› æ­¤ï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨é€†å‘æ–¹æ³•åŸºäºæœºå™¨å­¦ä¹ ï¼Œå·²ç»åœ¨å‰ä¸€äº›æ•°æ®ä¸ŠéªŒè¯äº†å…¶æ•ˆæœï¼Œåœ¨ä¸¤ä¸ªå®éªŒ ÑĞ»ÑƒÑ‡Ğ°ä¾‹ä¸­åº”ç”¨ã€‚ç¬¬ä¸€ä¸ªå®éªŒ caso æ˜¯åœ¨ Gustave Eiffel University (Nantes, France) çš„è·¯é¢ç–²åŠ³è½¦ouselä¸Šè¿›è¡ŒéªŒè¯ï¼Œç¬¬äºŒä¸ªå®éªŒ caso æ˜¯åœ¨ Vend{\'e}e çœçš„ä¸€æ¡æ–°çš„å®é™…é“è·¯ä¸Šã€‚åœ¨ä¸¤ä¸ª caso ç ”ç©¶ä¸­ï¼ŒSVM/SVR æ–¹æ³•çš„è¡¨ç°è¡¨æ˜äº†è¶…visions å­¦ä¹ æ–¹æ³•çš„æ•ˆæœæ€§ï¼Œå¯ä»¥å¯¹æ¶‚å±‚ä¸­çš„æ··åˆå‰‚è´¨é‡è¿›è¡Œåˆ†ç±»å’Œä¼°ç®—ã€‚
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Transportation-Activities-of-Construction-Waste-Hauling-Trucks-An-Input-Output-Hidden-Markov-Approach"><a href="#Predicting-the-Transportation-Activities-of-Construction-Waste-Hauling-Trucks-An-Input-Output-Hidden-Markov-Approach" class="headerlink" title="Predicting the Transportation Activities of Construction Waste Hauling Trucks: An Input-Output Hidden Markov Approach"></a>Predicting the Transportation Activities of Construction Waste Hauling Trucks: An Input-Output Hidden Markov Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03780">http://arxiv.org/abs/2312.03780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongtai Yang, Boyi Lei, Ke Han, Luna Liu<br>for:* è¿™ç ”ç©¶æ—¨åœ¨é¢„æµ‹åºŸå¼ƒå»ºç­‘ææ–™æ‹–è½¦ï¼ˆCWHTsï¼‰çš„ç›®çš„åœ°å’Œåœç•™æ—¶é—´ï¼Œä»¥ä¾¿æœ‰æ•ˆç®¡ç†ç¯å¢ƒã€‚methods:* è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå¯è§£é‡Šçš„æ´»åŠ¨åŸºæœ¬æ¨¡å‹ï¼ˆIOHMMï¼‰çš„é¢„æµ‹æ–¹æ³•ï¼Œå¹¶åœ¨æˆéƒ½å¸‚300è¾†CWHTsä¸ŠéªŒè¯äº†å…¶æ•ˆæœã€‚results:* ç»“æœæ˜¾ç¤ºï¼ŒIOHMMæ¯”åŸºçº¿æ¨¡å‹ï¼ˆMarkové“¾ã€çº¿æ€§å›å½’å’Œé•¿çŸ­æ—¶é—´è®°å¿†ï¼‰è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”å¯¹CWHTsè¿è¾“æ´»åŠ¨çš„å½±å“å› ç´ è¿›è¡Œäº†åˆ†æã€‚<details>
<summary>Abstract</summary>
Construction waste hauling trucks (CWHTs), as one of the most commonly seen heavy-duty vehicles in major cities around the globe, are usually subject to a series of regulations and spatial-temporal access restrictions because they not only produce significant NOx and PM emissions but also causes on-road fugitive dust. The timely and accurate prediction of CWHTs' destinations and dwell times play a key role in effective environmental management. To address this challenge, we propose a prediction method based on an interpretable activity-based model, input-output hidden Markov model (IOHMM), and validate it on 300 CWHTs in Chengdu, China. Contextual factors are considered in the model to improve its prediction power. Results show that the IOHMM outperforms several baseline models, including Markov chains, linear regression, and long short-term memory. Factors influencing the predictability of CWHTs' transportation activities are also explored using linear regression models. Results suggest the proposed model holds promise in assisting authorities by predicting the upcoming transportation activities of CWHTs and administering intervention in a timely and effective manner.
</details>
<details>
<summary>æ‘˜è¦</summary>
é‡å»ºåºŸå¼ƒç‰©æ‹–è½¦ï¼ˆCWHTï¼‰æ˜¯å…¨çƒä¸»è¦åŸå¸‚ä¸­æœ€å¸¸è§çš„é‡å‹è½¦è¾†ä¹‹ä¸€ï¼Œé€šå¸¸å—åˆ°ä¸€ç³»åˆ—çš„è§„å®šå’Œæ—¶ç©ºè®¿é—®é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬ä¸ä»…äº§ç”Ÿå¤§é‡çš„NOxå’ŒPMæ’æ”¾ï¼Œè¿˜ä¼šåœ¨è·¯ä¸Šäº§ç”Ÿé€¸æ•£å°˜åŸƒã€‚é¢„æµ‹CWHTçš„ç›®çš„åœ°å’Œåœç•™æ—¶é—´åœ¨ç¯å¢ƒç®¡ç†ä¸­æ‰®æ¼”äº†å…³é”®è§’è‰²ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯è§£é‡Šçš„æ´»åŠ¨åŸºæœ¬æ¨¡å‹ï¼Œè¾“å…¥è¾“å‡ºéšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆIOHMMï¼‰ï¼Œå¹¶åœ¨æˆéƒ½å¸‚300è¾†CWHTä¸ŠéªŒè¯å…¶æ•ˆæœã€‚åœ¨æ¨¡å‹ä¸­è€ƒè™‘äº†ä¸Šä¸‹æ–‡å› ç´ ï¼Œä»¥æé«˜é¢„æµ‹åŠ›åº¦ã€‚ç»“æœè¡¨æ˜ï¼ŒIOHMMåœ¨å¤šä¸ªåŸºelineæ¨¡å‹ä¹‹ä¸Šå ä¼˜ï¼ŒåŒ…æ‹¬é©¬å°”å¯å¤«é“¾ã€çº¿æ€§å›å½’å’Œé•¿çŸ­æœŸè®°å¿†ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨çº¿æ€§å›å½’æ¨¡å‹æ¢è®¨CWHTçš„äº¤é€šæ´»åŠ¨é¢„æµ‹å› ç´ ï¼Œç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨è¾…åŠ©ç®¡ç†å½“å±€é¢„æµ‹CWHTçš„äº¤é€šæ´»åŠ¨å¹¶å®æ–½æœ‰æ•ˆæªæ–½æ–¹é¢å…·æœ‰æ‰¿è¯ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Mechanistic-Representations-for-Meal-level-Glycemic-Control-in-the-Wild"><a href="#Interpretable-Mechanistic-Representations-for-Meal-level-Glycemic-Control-in-the-Wild" class="headerlink" title="Interpretable Mechanistic Representations for Meal-level Glycemic Control in the Wild"></a>Interpretable Mechanistic Representations for Meal-level Glycemic Control in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03344">http://arxiv.org/abs/2312.03344</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keawang/interpretable-cgm-representations">https://github.com/keawang/interpretable-cgm-representations</a></li>
<li>paper_authors: Ke Alexander Wang, Emily B. Fox</li>
<li>for: This paper aims to learn interpretable representations of continuous glucose monitoring (CGM) and meal data to capture the complexity of glycemic control in individuals with type-2 diabetes and pre-diabetes.</li>
<li>methods: The proposed method uses a hybrid variational autoencoder to learn embeddings that reflect physiological quantities such as insulin sensitivity, glucose effectiveness, and basal glucose levels. The method also introduces a novel method to infer the glucose appearance rate, making the mechanistic model robust to unreliable meal logs.</li>
<li>results: The proposed method discovers a separation between individuals proportional to their disease severity and produces clusters that are up to 4x better than other features. The embeddings provide a nuanced, yet interpretable, embedding space to compare glycemic control within and across individuals, directly learnable from in-the-wild data.Hereâ€™s the simplified Chinese text for the three key points:</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯é€šè¿‡å­¦ä¹ å¯è§£é‡Šçš„CGMå’Œé£Ÿç‰©æ•°æ®æ¥æ•æ‰äººç±»è¡€ç³–æ§åˆ¶çš„å¤æ‚æ€§ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨æ··åˆå˜é‡è‡ªåŠ¨ç¼–ç å™¨æ¥å­¦ä¹ å…·æœ‰ç”Ÿç‰©ç‰©ç†é‡çš„åµŒå…¥ã€‚è¿™ç§æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ç³–åˆ†å‡ºç°ç‡æ¨æ–­æ–¹æ³•ï¼Œä½¿æœºåˆ¶æ¨¡å‹å…·æœ‰å¯é çš„é£Ÿç‰©æ—¥å¿—ã€‚</li>
<li>results: è¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸åŒäººç¾¤ä¸­æ•æ‰ç–¾ç—…ä¸¥é‡ç¨‹åº¦çš„åˆ†å¸ƒï¼Œå¹¶ç”Ÿæˆ clusters çš„è¡€ç³–æ§åˆ¶æ°´å¹³ï¼Œè‡³å°‘æ¯”å…¶ä»–ç‰¹å¾æ›´å¥½ã€‚è¿™äº›åµŒå…¥å¯ä»¥ç›´æ¥ä»é‡å¤–æ•°æ®ä¸­å­¦ä¹ ï¼Œæä¾›ä¸€ä¸ªç®€æ´ yet å¯è§£é‡Šçš„åµŒå…¥ç©ºé—´ï¼Œç”¨äºæ¯”è¾ƒè¡€ç³–æ§åˆ¶æ°´å¹³ã€‚<details>
<summary>Abstract</summary>
Diabetes encompasses a complex landscape of glycemic control that varies widely among individuals. However, current methods do not faithfully capture this variability at the meal level. On the one hand, expert-crafted features lack the flexibility of data-driven methods; on the other hand, learned representations tend to be uninterpretable which hampers clinical adoption. In this paper, we propose a hybrid variational autoencoder to learn interpretable representations of CGM and meal data. Our method grounds the latent space to the inputs of a mechanistic differential equation, producing embeddings that reflect physiological quantities, such as insulin sensitivity, glucose effectiveness, and basal glucose levels. Moreover, we introduce a novel method to infer the glucose appearance rate, making the mechanistic model robust to unreliable meal logs. On a dataset of CGM and self-reported meals from individuals with type-2 diabetes and pre-diabetes, our unsupervised representation discovers a separation between individuals proportional to their disease severity. Our embeddings produce clusters that are up to 4x better than naive, expert, black-box, and pure mechanistic features. Our method provides a nuanced, yet interpretable, embedding space to compare glycemic control within and across individuals, directly learnable from in-the-wild data.
</details>
<details>
<summary>æ‘˜è¦</summary>
Ğ´Ğ¸Ğ°Ğ±ĞµÑ‚ĞµÑ Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ Ğ² ÑĞµĞ±Ñ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿ĞµĞ¹Ğ·Ğ°Ğ¶ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ³Ğ»ÑĞºĞ¾Ğ·Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ ÑˆĞ¸Ñ€Ğ¾ĞºĞ¾ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµÑ‚ÑÑ ÑÑ€ĞµĞ´Ğ¸ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒÑƒĞ¼Ğ¾Ğ². ĞĞ´Ğ½Ğ°ĞºĞ¾, ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ½Ğµ Ğ²ĞµÑ€Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‚ ÑÑ‚Ñƒ Ğ²Ğ°Ñ€ÑŒĞ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ ĞµĞ´Ñ‹. Ğ¡ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğµ Ğ¾Ğ±Ğ»Ğ°Ğ´Ğ°ÑÑ‚ Ğ³Ğ¸Ğ±ĞºĞ¾ÑÑ‚ÑŒÑ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ², Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…; Ñ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ñ‹, Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ, Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ½Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¸Ñ… ĞºĞ»Ğ¸Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ. Ğ’ ÑÑ‚Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ Ğ¼Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ»Ğ°Ğ³Ğ°ĞµĞ¼ Ğ³Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ÑĞ¼ CGM Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ĞµĞ´Ñ‹. ĞĞ°ÑˆĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑƒÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°, Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ñ‹, Ğ¾Ñ‚Ñ€Ğ°Ğ¶Ğ°ÑÑ‰Ğ¸Ğµ Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ‚Ğ°ĞºĞ¸Ğµ ĞºĞ°Ğº Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ½ÑÑƒĞ»Ğ¸Ğ½Ğ°, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ³Ğ»ÑĞºĞ¾Ğ·Ñ‹ Ğ¸ ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ğ±Ğ°Ğ·Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ³Ğ»ÑĞºĞ¾Ğ·Ñ‹. ĞšÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾, Ğ¼Ñ‹ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ´Ğ»Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ³Ğ»ÑĞºĞ¾Ğ·Ñ‹, Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°ĞµÑ‚ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ñ‹Ğ¼ Ğº Ğ½ĞµĞ½Ğ°Ğ´ĞµĞ¶Ğ½Ñ‹Ğ¼ Ğ·Ğ°Ğ¿Ğ¸ÑÑĞ¼ Ğ¾ ĞµĞ´Ğµ. ĞĞ° Ğ½Ğ°Ğ±Ğ¾Ñ€Ğµ CGM Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ¾ ĞµĞ´Ğµ Ğ¾Ñ‚ Ğ»ÑĞ´ĞµĞ¹ Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¸Ğ°Ğ±ĞµÑ‚Ğ¾Ğ¼ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ´Ğ¸Ğ°Ğ±ĞµÑ‚Ğ¾Ğ¼, Ğ½Ğ°ÑˆĞ¸ Ğ½ĞµÑÑƒĞ¿ĞµÑ€Ğ²Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ğ´Ğ¸Ğ²Ğ¸Ğ´ÑƒÑƒĞ¼Ñ‹ Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ Ğ¸Ñ… ÑÑ‚ĞµĞ¿ĞµĞ½Ğ¸ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½Ğ¸. ĞĞ°ÑˆĞ¸ ÑĞ¼Ğ±ĞµĞ´Ñ‹ Ğ´Ğ°ÑÑ‚ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ½Ğ° 4 Ñ€Ğ°Ğ·Ğ° Ğ»ÑƒÑ‡ÑˆĞµ, Ñ‡ĞµĞ¼ Ğ½Ğ°Ğ¸Ğ²Ğ½Ñ‹Ğµ, ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğµ, Ñ‡ĞµÑ€Ğ½Ñ‹Ğµ ĞºÑƒĞ±Ñ‹ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ¼ĞµÑ…Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ. ĞĞ°ÑˆĞ¸ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ Ğ³Ğ½ÑƒÑĞ¼Ñƒ, Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½ĞµĞ¿Ğ¾ÑÑ€ĞµĞ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒ Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ² Ğ±Ñ‹Ñ‚Ñƒ.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Koopman-based-Dynamic-Movement-Primitives"><a href="#Deep-Learning-for-Koopman-based-Dynamic-Movement-Primitives" class="headerlink" title="Deep Learning for Koopman-based Dynamic Movement Primitives"></a>Deep Learning for Koopman-based Dynamic Movement Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03328">http://arxiv.org/abs/2312.03328</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tyler Han, Carl Glen Henshaw</li>
<li>for: å­¦ä¹ Robotæ‰§è¡Œçµæ´»æŠ“å–ã€åŠ¨æ€ç§»åŠ¨æˆ–å…¨èº«æŠ“å–çš„æŠ€èƒ½ï¼Œä»å°‘é‡ç¤ºèŒƒä¸­å¯å‘å­¦ä¹ æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚</li>
<li>methods: æè®®ä½¿ç”¨ Koopman è¿ç®—ç¬¦å’ŒåŠ¨æ€è¿åŠ¨åŸºæœ¬å¾å­¦ä¹ ä»ç¤ºèŒƒå­¦ä¹ ã€‚</li>
<li>results: å¯¹äº LASA æ‰‹å†™å­—åº“æ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä¸æ‰©å±•åŠ¨æ€æ¨¡å¼åˆ†è§£ç›¸æ¯”ï¼Œä½†æ˜¯åªéœ€è®­ç»ƒå°‘é‡å­—ç¬¦ã€‚<details>
<summary>Abstract</summary>
The challenge of teaching robots to perform dexterous manipulation, dynamic locomotion, or whole--body manipulation from a small number of demonstrations is an important research field that has attracted interest from across the robotics community. In this work, we propose a novel approach by joining the theories of Koopman Operators and Dynamic Movement Primitives to Learning from Demonstration. Our approach, named \gls{admd}, projects nonlinear dynamical systems into linear latent spaces such that a solution reproduces the desired complex motion. Use of an autoencoder in our approach enables generalizability and scalability, while the constraint to a linear system attains interpretability. Our results are comparable to the Extended Dynamic Mode Decomposition on the LASA Handwriting dataset but with training on only a small fractions of the letters.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç ”ç©¶æ•™è‚²æœºå™¨äººæ‰§è¡Œçµæ´»çš„æ¬è¿ã€åŠ¨æ€ç§»åŠ¨æˆ–å…¨èº«æ¬è¿ä»ä¸€å°æ•°é‡çš„ç¤ºä¾‹ä¸­å­¦ä¹ çš„æŒ‘æˆ˜æ˜¯æœºå™¨äººç¤¾åŒºä¸­çš„ä¸€ä¸ªé‡è¦ç ”ç©¶é¢†åŸŸã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç»“åˆåº“æ™®æ›¼æ“ä½œå’ŒåŠ¨æ€è¿åŠ¨åŸºæœ¬å…ƒç´ å­¦ä¹ ä»ç¤ºä¾‹å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‘½åä¸º\gls{admd}ï¼Œå°†éçº¿æ€§åŠ¨åŠ›ç³»ç»ŸæŠ•å½±åˆ°çº¿æ€§éšè—ç©ºé—´ä¸­ï¼Œä½¿å¾—è§£å†³å™¨é‡ç°æ‰€éœ€çš„å¤æ‚è¿åŠ¨ã€‚ä½¿ç”¨è‡ªåŠ¨encoderåœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­å…è®¸æ™®éæ€§å’Œå¯æ‰©å±•æ€§ï¼Œè€Œå¯¹äºçº¿æ€§ç³»ç»Ÿçš„çº¦æŸä½¿å¾—è§£é‡Šæ€§ã€‚æˆ‘ä»¬çš„ç»“æœä¸æ‰©å±•åŠ¨æ€æ¨¡å¼åˆ†è§£ç›¸æ¯”ï¼Œåœ¨LASAæ‰‹å†™æ•°æ®é›†ä¸Šè¾¾åˆ°äº†ç±»ä¼¼çš„æ€§èƒ½ï¼Œä½†æ˜¯åªéœ€è®­ç»ƒä¸€å°éƒ¨åˆ†çš„å­—æ¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Nystrom-Approximation-for-Preconditioning-in-Kernel-Machines"><a href="#On-the-Nystrom-Approximation-for-Preconditioning-in-Kernel-Machines" class="headerlink" title="On the Nystrom Approximation for Preconditioning in Kernel Machines"></a>On the Nystrom Approximation for Preconditioning in Kernel Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03311">http://arxiv.org/abs/2312.03311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhesam Abedsoltan, Mikhail Belkin, Parthe Pandit, Luis Rademacher</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨åˆ†æä½¿ç”¨çº¦strÃ¶mæ–¹æ³•é¢„å¤„ç†å™¨ï¼Œä»¥åŠ é€ŸåŸºæœ¬å‡½æ•°å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†çº¦strÃ¶mæ–¹æ³•é¢„å¤„ç†å™¨ï¼Œä»¥åŠ é€ŸåŸºæœ¬å‡½æ•°å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨çº¦strÃ¶mæ–¹æ³•é¢„å¤„ç†å™¨å¯ä»¥å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ï¼ŒåŒæ—¶ä»èƒ½å¤Ÿå‡å°‘è®­ç»ƒè¿‡ç¨‹çš„æ—¶é—´ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ä¸€ä¸ªlogarithmic sample sizeï¼ˆä¸æ•°æ®é›†å¤§å°æˆæ­£æ¯”ï¼‰çš„çº¦strÃ¶mæ–¹æ³•é¢„å¤„ç†å™¨ï¼Œèƒ½å¤Ÿåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸ŠåŠ é€ŸåŸºæœ¬å‡½æ•°å­¦ä¹ ç®—æ³•çš„è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒæ—¶å‡å°‘è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚<details>
<summary>Abstract</summary>
Kernel methods are a popular class of nonlinear predictive models in machine learning. Scalable algorithms for learning kernel models need to be iterative in nature, but convergence can be slow due to poor conditioning. Spectral preconditioning is an important tool to speed-up the convergence of such iterative algorithms for training kernel models. However computing and storing a spectral preconditioner can be expensive which can lead to large computational and storage overheads, precluding the application of kernel methods to problems with large datasets. A Nystrom approximation of the spectral preconditioner is often cheaper to compute and store, and has demonstrated success in practical applications. In this paper we analyze the trade-offs of using such an approximated preconditioner. Specifically, we show that a sample of logarithmic size (as a function of the size of the dataset) enables the Nystrom-based approximated preconditioner to accelerate gradient descent nearly as well as the exact preconditioner, while also reducing the computational and storage overheads.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Balanced-Marginal-and-Joint-Distributional-Learning-via-Mixture-Cramer-Wold-Distance"><a href="#Balanced-Marginal-and-Joint-Distributional-Learning-via-Mixture-Cramer-Wold-Distance" class="headerlink" title="Balanced Marginal and Joint Distributional Learning via Mixture Cramer-Wold Distance"></a>Balanced Marginal and Joint Distributional Learning via Mixture Cramer-Wold Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03307">http://arxiv.org/abs/2312.03307</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seunghwan An, Sungchul Hong, Jong-June Jeon</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨ç”Ÿæˆæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¡¡é‡é«˜ç»´æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•ï¼Œå³æ··åˆå¡é»˜-æ²ƒå°”å¾·è·ç¦»ï¼ˆMixture Cramer-Wold distanceï¼‰ï¼Œè¯¥æ–¹æ³•èƒ½åŒæ—¶æ•æ‰é«˜ç»´æ¦‚ç‡åˆ†å¸ƒçš„ JOINT å’Œ MARGINAL ä¿¡æ¯ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡æå‡º CWDAEï¼ˆå¡é»˜-æ²ƒå°”å¾·åˆ†å¸ƒè‡ªåŠ¨ç¼–ç å™¨ï¼‰æ¨¡å‹ï¼Œå®ç°äº†åœ¨çœŸå®çš„æ ‡é‡æ•°æ®é›†ä¸Šç”ŸæˆSyntheticæ•°æ®çš„remarkableè¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹å…·æœ‰è½»æ¾è°ƒæ•´æ•°æ®éšç§æ°´å¹³çš„ä¾¿åˆ©æ€§ã€‚<details>
<summary>Abstract</summary>
In the process of training a generative model, it becomes essential to measure the discrepancy between two high-dimensional probability distributions: the generative distribution and the ground-truth distribution of the observed dataset. Recently, there has been growing interest in an approach that involves slicing high-dimensional distributions, with the Cramer-Wold distance emerging as a promising method. However, we have identified that the Cramer-Wold distance primarily focuses on joint distributional learning, whereas understanding marginal distributional patterns is crucial for effective synthetic data generation. In this paper, we introduce a novel measure of dissimilarity, the mixture Cramer-Wold distance. This measure enables us to capture both marginal and joint distributional information simultaneously, as it incorporates a mixture measure with point masses on standard basis vectors. Building upon the mixture Cramer-Wold distance, we propose a new generative model called CWDAE (Cramer-Wold Distributional AutoEncoder), which shows remarkable performance in generating synthetic data when applied to real tabular datasets. Furthermore, our model offers the flexibility to adjust the level of data privacy with ease.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è®­ç»ƒç”Ÿæˆæ¨¡å‹è¿‡ç¨‹ä¸­ï¼Œ measure the discrepancy between two high-dimensional probability distributionsï¼šç”Ÿæˆåˆ†å¸ƒå’Œè§‚å¯Ÿæ•°æ®çš„çœŸå®åˆ†å¸ƒã€‚è¿‘æœŸï¼Œæœ‰å…³åˆ‡åˆ†é«˜ç»´åˆ†å¸ƒçš„æ–¹æ³•å—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ï¼Œå…¶ä¸­Cramer-Wold distance  emerges as a promising methodã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°Cramer-Wold distance ä¸»è¦å…³æ³¨å…±åŒåˆ†å¸ƒå­¦ä¹ ï¼Œè€Œç†è§£å„è‡ªåˆ†å¸ƒçš„æ¨¡å¼æ˜¯ç”Ÿæˆ sintetic data çš„æ•ˆæœæ‰€å¿…éœ€çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä¸åŒåº¦é‡ï¼Œå³ mixture Cramer-Wold distanceã€‚è¿™ç§åº¦é‡å¯ä»¥åŒæ—¶æ•æ‰åˆ°å„è‡ªåˆ†å¸ƒå’Œå…±åŒåˆ†å¸ƒçš„ä¿¡æ¯ï¼Œé€šè¿‡ç‚¹ Ğ¼Ğ°ÑÑĞ°åœ¨æ ‡å‡†åŸºå‡†å‘é‡ä¸Šæ¥å®ç°ã€‚åŸºäºmixture Cramer-Wold distanceï¼Œæˆ‘ä»¬æè®®ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œcalled CWDAE (Cramer-Wold Distributional AutoEncoder)ï¼Œå®ƒåœ¨å®é™…çš„è¡¨æ ¼æ•°æ®ä¸Šæ˜¾ç¤ºäº†å¾ˆå¥½çš„ç”Ÿæˆ sintetic data æ€§èƒ½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¿˜å…·æœ‰è½»æ¾åœ°è°ƒæ•´æ•°æ®éšç§æ°´å¹³çš„èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Molecular-Property-Prediction-via-Mixture-of-Collaborative-Experts"><a href="#Enhancing-Molecular-Property-Prediction-via-Mixture-of-Collaborative-Experts" class="headerlink" title="Enhancing Molecular Property Prediction via Mixture of Collaborative Experts"></a>Enhancing Molecular Property Prediction via Mixture of Collaborative Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03292">http://arxiv.org/abs/2312.03292</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Hyacinth-YX/mixture-of-collaborative-experts">https://github.com/Hyacinth-YX/mixture-of-collaborative-experts</a></li>
<li>paper_authors: Xu Yao, Shuang Liang, Songqiao Han, Hailiang Huang</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è§£å†³åˆ†å­ç‰©ç†å­¦ä»»åŠ¡ï¼ˆMPPï¼‰ä¸­æ•°æ®ç¨€ç¼ºå’Œä¸å‡è¡¡é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºç¼–ç å™¨ï¼ŒæŠ½å–åˆ†å­å›¾çš„å…±åŒç‰¹å¾ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨çš„æ–¹æ³•æ˜¯ Mixture of Collaborative Expertsï¼ˆMoCEï¼‰ä½œä¸ºé¢„æµ‹å™¨ï¼Œåˆ©ç”¨ä»»åŠ¡ä¹‹é—´çš„å…±åŒç‰¹å¾ï¼ŒåŒæ—¶è§£å†³ä¸“å®¶ç»„å†…çš„åŒåŒ–é—®é¢˜å’Œå†³ç­–å æ¯”é—®é¢˜ã€‚ä¸ºäº†å¢å¼ºä¸“å®¶çš„å¤šæ ·æ€§ï¼Œæå‡ºäº†ä¸“å®¶ç‰¹æœ‰æŠ•å½±æ–¹æ³•ï¼Œä»¥åŠä¸“å®¶ç‰¹æœ‰æŸå¤±å‡½æ•°ï¼Œä»¥æ›´å¥½åœ°è®©æ‰€æœ‰ä¸“å®¶åˆä½œã€‚</li>
<li>results: æ ¹æ®è¿™ paper çš„ç»“æœï¼Œä½¿ç”¨ GNN-MoCE æ¶æ„å¯ä»¥åœ¨ 24 ä¸ª MPP æ•°æ®é›†ä¸Šè¾¾åˆ°æ›´é«˜çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæˆ–é«˜ä¸å‡è¡¡çš„ä»»åŠ¡ä¸­ã€‚<details>
<summary>Abstract</summary>
Molecular Property Prediction (MPP) task involves predicting biochemical properties based on molecular features, such as molecular graph structures, contributing to the discovery of lead compounds in drug development. To address data scarcity and imbalance in MPP, some studies have adopted Graph Neural Networks (GNN) as an encoder to extract commonalities from molecular graphs. However, these approaches often use a separate predictor for each task, neglecting the shared characteristics among predictors corresponding to different tasks. In response to this limitation, we introduce the GNN-MoCE architecture. It employs the Mixture of Collaborative Experts (MoCE) as predictors, exploiting task commonalities while confronting the homogeneity issue in the expert pool and the decision dominance dilemma within the expert group. To enhance expert diversity for collaboration among all experts, the Expert-Specific Projection method is proposed to assign a unique projection perspective to each expert. To balance decision-making influence for collaboration within the expert group, the Expert-Specific Loss is presented to integrate individual expert loss into the weighted decision loss of the group for more equitable training. Benefiting from the enhancements of MoCE in expert creation, dynamic expert group formation, and experts' collaboration, our model demonstrates superior performance over traditional methods on 24 MPP datasets, especially in tasks with limited data or high imbalance.
</details>
<details>
<summary>æ‘˜è¦</summary>
è›‹ç™½è´¨ç‰©ç†é¢„æµ‹ï¼ˆMPPï¼‰ä»»åŠ¡æ˜¯é¢„æµ‹ç”Ÿç‰©åŒ–å­¦æ€§è´¨åŸºäºè›‹ç™½è´¨åˆ†å­ç»“æ„çš„ç‰¹å¾ï¼Œè¿™æœ‰åŠ©äºè¯ç‰©å¼€å‘ä¸­å‘ç°é¢†å…ˆçš„è¯ç‰©ã€‚ä¸ºäº†è§£å†³MPPæ•°æ®ç¨€ç¼ºå’Œä¸å‡è¡¡é—®é¢˜ï¼Œä¸€äº›ç ”ç©¶å·²ç»é‡‡ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä½œä¸ºç¼–ç å™¨ï¼Œä»¥EXTRACT molecular graph ä¸­çš„å…±åŒç‰¹å¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•é€šå¸¸ä½¿ç”¨å•ç‹¬çš„é¢„æµ‹å™¨ Ğ´Ğ»Ñæ¯ä¸ªä»»åŠ¡ï¼Œå¿½ç•¥ä¸åŒä»»åŠ¡çš„é¢„æµ‹å™¨ä¹‹é—´çš„å…±åŒç‰¹å¾ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†GNN-MoCEæ¶æ„ã€‚å®ƒä½¿ç”¨ Mixture of Collaborative Expertsï¼ˆMoCEï¼‰ä½œä¸ºé¢„æµ‹å™¨ï¼Œåˆ©ç”¨ä¸åŒä»»åŠ¡ä¹‹é—´çš„å…±åŒç‰¹å¾ï¼ŒåŒæ—¶è§£å†³ä¸“å®¶ç»„å†…éƒ¨å†³ç­–æƒçš„åˆ†é…å’Œä¸“å®¶ç»„ä¸­çš„å†³ç­–å æ¯”é—®é¢˜ã€‚ä¸ºäº†å¢åŠ ä¸“å®¶ä¹‹é—´çš„åä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸“å®¶ç‰¹æœ‰çš„æŠ•å½±æ–¹æ³•ï¼Œä½¿æ¯ä¸ªä¸“å®¶æœ‰ç‹¬ç‰¹çš„æŠ•å½±è§’åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸“å®¶ç‰¹æœ‰çš„æŸå¤±å‡½æ•°ï¼Œå°†æ¯ä¸ªä¸“å®¶çš„æŸå¤±å‡½æ•°ç§¯åŠ åˆ°ä¸“å®¶ç»„å†…çš„æƒé‡å¹³å‡æŸå¤±ä¸­ï¼Œä»¥æ›´å¹³ç­‰åœ°è®­ç»ƒä¸“å®¶ç»„ã€‚ç”±äºMoCEçš„ä¼˜åŠ¿åœ¨ä¸“å®¶åˆ›é€ ã€åŠ¨æ€ä¸“å®¶ç»„æˆå’Œä¸“å®¶åä½œæ–¹é¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨24ä¸ªMPPæ•°æ®é›†ä¸Šè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®ç¨€ç¼ºæˆ–é«˜ä¸å‡è¡¡çš„ä»»åŠ¡ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Anomaly-Detection-for-Scalable-Task-Grouping-in-Reinforcement-Learning-based-RAN-Optimization"><a href="#Anomaly-Detection-for-Scalable-Task-Grouping-in-Reinforcement-Learning-based-RAN-Optimization" class="headerlink" title="Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization"></a>Anomaly Detection for Scalable Task Grouping in Reinforcement Learning-based RAN Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03277">http://arxiv.org/abs/2312.03277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jimmy Li, Igor Kozlov, Di Wu, Xue Liu, Gregory Dudek</li>
<li>for: ä¼˜åŒ–mobile networkçš„ç»´æŠ¤å’Œä¼˜åŒ–</li>
<li>methods: ä½¿ç”¨å­¦ä¹ åŸºäºæ–¹æ³•æ¥ä¼˜åŒ–ç»´æŠ¤å’Œä¼˜åŒ–</li>
<li>results: æ„å»ºä¸€ä¸ªå¯æ‰©å±•çš„æ”¿ç­–é“¶è¡Œï¼Œå¯ä»¥åœ¨å¤šä¸ªcell siteä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä¸”å¯ä»¥æ™ºèƒ½åœ°åˆ¤æ–­ä»»åŠ¡å’Œæ”¿ç­–ä¹‹é—´çš„ç›¸å®¹æ€§ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ©ç”¨è®¡ç®—èµ„æºã€‚<details>
<summary>Abstract</summary>
The use of learning-based methods for optimizing cellular radio access networks (RAN) has received increasing attention in recent years. This coincides with a rapid increase in the number of cell sites worldwide, driven largely by dramatic growth in cellular network traffic. Training and maintaining learned models that work well across a large number of cell sites has thus become a pertinent problem. This paper proposes a scalable framework for constructing a reinforcement learning policy bank that can perform RAN optimization across a large number of cell sites with varying traffic patterns. Central to our framework is a novel application of anomaly detection techniques to assess the compatibility between sites (tasks) and the policy bank. This allows our framework to intelligently identify when a policy can be reused for a task, and when a new policy needs to be trained and added to the policy bank. Our results show that our approach to compatibility assessment leads to an efficient use of computational resources, by allowing us to construct a performant policy bank without exhaustively training on all tasks, which makes it applicable under real-world constraints.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€å­¦ä¹ åŸºäºæ–¹æ³•åœ¨ç§»åŠ¨é€šä¿¡ç½‘ç»œï¼ˆRANï¼‰ä¼˜åŒ–ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šæ™®éï¼Œä¸–ç•Œå„åœ°Cellularç½‘ç»œè¦†ç›–é¢ç§¯åœ¨å¿«é€Ÿå¢é•¿ã€‚è¿™ä¸ç§»åŠ¨ç½‘ç»œæµé‡å¿«é€Ÿå¢é•¿æœ‰ç€ç›´æ¥å…³ç³»ã€‚å› æ­¤ï¼Œåœ¨å¤§é‡Cell sitesä¸Šè®­ç»ƒå’Œç»´æŠ¤è‰¯å¥½åœ¨å¤šä¸ªCell sitesä¸Šå·¥ä½œçš„å­¦ä¹ æ¨¡å‹æˆä¸ºäº†ä¸€ä¸ªç´§è¿«çš„é—®é¢˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å¤§é‡Cell sitesä¸Šæ„å»ºä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ç­–ç•¥é“¶è¡Œï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„äº¤é€šå¾æ–‡ä¸­ä¼˜åŒ–RANã€‚æˆ‘ä»¬çš„æ¡†æ¶ä¸­å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¼‚å¸¸æ£€æµ‹æŠ€æœ¯æ¥è¯„ä¼°Cell sitesï¼ˆä»»åŠ¡ï¼‰ä¸ç­–ç•¥é“¶è¡Œä¹‹é—´çš„å…¼å®¹æ€§ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ™ºèƒ½åœ°ç¡®å®šæ˜¯å¦å¯ä»¥åœ¨æŸä¸ªä»»åŠ¡ä¸Š reuse å·²ç»è®­ç»ƒå¥½çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä¸€æ ·åœ°åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¿›è¡Œæé™è®­ç»ƒã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„å…¼å®¹æ€§è¯„ä¼°æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°ä½¿ç”¨è®¡ç®—èµ„æºï¼Œä»è€Œåœ¨å®é™…ç¯å¢ƒä¸­æ„å»ºä¸€ä¸ªé«˜æ€§èƒ½çš„ç­–ç•¥é“¶è¡Œã€‚
</details></li>
</ul>
<hr>
<h2 id="Low-Cost-High-Power-Membership-Inference-by-Boosting-Relativity"><a href="#Low-Cost-High-Power-Membership-Inference-by-Boosting-Relativity" class="headerlink" title="Low-Cost High-Power Membership Inference by Boosting Relativity"></a>Low-Cost High-Power Membership Inference by Boosting Relativity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03262">http://arxiv.org/abs/2312.03262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sajjad Zarifzadeh, Philippe Liu, Reza Shokri</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æ”»å‡»æœºå™¨å­¦ä¹ ç®—æ³•çš„éšç§é£é™©è€Œè®¾è®¡çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å‚è€ƒæ¨¡å‹å’Œå‚è€ƒæ•°æ®æ¥å¼ºåŒ–å¯¹ä»»åŠ¡æ¨¡å‹çš„è¯†åˆ«åŠ›ï¼Œå¹¶é€šè¿‡likelihood ratioæµ‹è¯•æ¥å®ç°ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„ç®—æ³•åœ¨æ¯”è¾ƒä½çš„å‡é˜³æ€§ç‡ä¸‹ï¼ˆå¦‚0ï¼‰ still å¯ä»¥è¾¾åˆ°é«˜çš„çœŸé˜³æ€§ç‡ï¼Œå¹¶ä¸”åœ¨è®¡ç®—çº¦æŸä¸‹ï¼ˆåªæœ‰limited number of reference modelsï¼‰ä¹Ÿè¡¨ç°å‡ºè‰²ï¼Œä¸ä¹‹å‰çš„æ–¹æ³•ä¸åŒã€‚<details>
<summary>Abstract</summary>
We present a robust membership inference attack (RMIA) that amplifies the distinction between population data and the training data on any target model, by effectively leveraging both reference models and reference data in our likelihood ratio test. Our algorithm exhibits superior test power (true-positive rate) when compared to prior methods, even at extremely low false-positive error rates (as low as 0). Also, under computation constraints, where only a limited number of reference models (as few as 1) are available, our method performs exceptionally well, unlike some prior attacks that approach random guessing in such scenarios. Our method lays the groundwork for cost-effective and practical yet powerful and robust privacy risk analysis of machine learning algorithms.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§robustä¼šå‘˜æ¨æµ‹æ”»å‡»(RMIA)ï¼Œä½¿å¾—äººå£æ•°æ®å’Œè®­ç»ƒæ•°æ®ä¹‹é—´çš„å·®åˆ«æ›´åŠ çªå‡ºï¼Œé€šè¿‡æœ‰æ•ˆåœ°åˆ©ç”¨å‚è€ƒæ¨¡å‹å’Œå‚è€ƒæ•°æ®åœ¨likelihoodæ¯”ç‡æµ‹è¯•ä¸­ã€‚æˆ‘ä»¬çš„ç®—æ³•åœ¨æ¯”è¾ƒæ–¹æ³•æ—¶æ˜¾ç¤ºå‡ºè¶…è¿‡å…¶ä»–æ–¹æ³•çš„æµ‹è¯•åŠ›ï¼ˆçœŸæ­£æ­£ç¡®ç‡ï¼‰ï¼Œå°¤å…¶æ˜¯åœ¨éå¸¸ä½çš„å‡é˜³æ€§é”™è¯¯ç‡ï¼ˆå¦‚0ï¼‰ä¸‹ã€‚æ­¤å¤–ï¼Œåœ¨è®¡ç®—é™åˆ¶ä¸‹ï¼Œåªæœ‰æœ‰é™çš„å‚è€ƒæ¨¡å‹ï¼ˆæ¯”å¦‚1ï¼‰å¯ç”¨æ—¶ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œä¸ä¸€äº›å…ˆå‰çš„æ”»å‡»æ–¹æ³•ä¸åŒï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œå®ƒä»¬çš„è¡¨ç°æ¥è¿‘éšæœºçŒœæµ‹ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ºæœºå™¨å­¦ä¹ ç®—æ³•çš„éšç§é£é™©åˆ†ææä¾›äº†å¯é ã€å®ç”¨ä¸”å¼ºå¤§çš„åŸºç¡€ã€‚
</details></li>
</ul>
<hr>
<h2 id="f-FERM-A-Scalable-Framework-for-Robust-Fair-Empirical-Risk-Minimization"><a href="#f-FERM-A-Scalable-Framework-for-Robust-Fair-Empirical-Risk-Minimization" class="headerlink" title="f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization"></a>f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03259">http://arxiv.org/abs/2312.03259</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/optimization-for-data-driven-science/f-ferm">https://github.com/optimization-for-data-driven-science/f-ferm</a></li>
<li>paper_authors: Sina Baharlouei, Shivam Patel, Meisam Razaviyayn</li>
<li>for: è¯¥è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å¯é çš„æœç´¢ä¼˜åŒ–æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…¬å¹³æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬f-FERMçš„æ¦‚ç‡è¯„ä¼°å’Œåˆ†å¸ƒä¸ç¡®å®šæ€§è¯„ä¼°ï¼Œä»¥åŠåŸºäº$L_p$èŒƒæ•°çš„åˆ†å¸ƒally robustä¼˜åŒ–ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒåŸºäºf-FERMçš„å…¬å¹³é£é™©æœ€å°åŒ–æ–¹æ³•å¯ä»¥åœ¨å¤§å¤šæ•°æ‰¹å¤„ç†å¤§å°ä¸‹ï¼ˆä»å…¨æ‰¹å¤„ç†åˆ°å•ä¸ªæ ·æœ¬å¤„ç†ï¼‰æä¾›æ›´ä½³çš„å…¬å¹³ç²¾åº¦-å‡†ç¡®åº¦è´¨é‡æ¯”ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜å¯ä»¥åœ¨åˆ†å¸ƒshiftæƒ…å†µä¸‹è¡¨ç°å‡ºä¼˜äºå…¶ä»–åŸºäºæ–‡çŒ®çš„æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these methods are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term "stochastic" refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on f-divergence measures (f-FERM). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by f-FERM for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. Our extension is based on a distributionally robust optimization reformulation of f-FERM objective under $L_p$ norms as uncertainty sets. Again, in this distributionally robust setting, f-FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts. An efficient stochastic implementation of $f$-FERM is publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£äººå·¥æ™ºèƒ½ä¸­ï¼Œè®­ç»ƒå’Œéƒ¨ç½²æ»¡è¶³ä¿æŠ¤ç»„å›¢çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯åŸºæœ¬çš„ã€‚è™½ç„¶æ–‡çŒ®ä¸­æœ‰è®¸å¤šçº¦æŸå’Œæ­£åˆ™åŒ–é¡¹æ¥ä¿ƒè¿›æœºå™¨å­¦ä¹ ä»»åŠ¡çš„å…¬å¹³æ€§ï¼Œä½†å¤§å¤šæ•°è¿™äº›æ–¹æ³•ä¸é€‚åˆéšæœºä¼˜åŒ–ã€‚åœ¨è¿™é‡Œï¼Œâ€œéšæœºâ€æŒ‡çš„æ˜¯ç®—æ³•å¯ä»¥å¤„ç†å°æ‰¹é‡æ•°æ®ã€‚é©±åŠ¨äº†ç°æœ‰æ–‡çŒ®çš„é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„éšæœºä¼˜åŒ–æ¡†æ¶ Ğ´Ğ»Ñå…¬å¹³empirical risk minimizationï¼ˆf-FERMï¼‰ã€‚æå‡ºçš„éšæœºç®—æ³•å…·æœ‰ç†è®ºçš„æ”¶æ•›ä¿è¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œf-FERMåœ¨å¤§å¤šæ•°æ‰¹å¤„ç†å¤§å°ï¼ˆä»å…¨æ‰¹å¤„ç†åˆ°ä¸€ä¸ªæ‰¹å¤„ç†ï¼‰ä¸­æä¾›äº†æ›´å¥½çš„å…¬å¹³å‡†ç¡®æ€§è´¨é‡æ¯”ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥æ‰©å±•åˆ°æµ‹è¯•æ•°æ®é›†ä¸è®­ç»ƒæ•°æ®é›†ä¹‹é—´çš„åˆ†å¸ƒåç§»æƒ…å†µä¸‹ã€‚æˆ‘ä»¬çš„æ‰©å±•åŸºäºåœ¨$L_p$ Ğ½Ğ¾Ñ€Ğ¼ä¸‹çš„åˆ†å¸ƒä¸ç¡®å®šæ€§é›†ï¼ˆuncertainty setsï¼‰ä¸­çš„åˆ†å¸ƒrobustä¼˜åŒ–é‡æ–°å®šä¹‰f-FERMç›®æ ‡ã€‚åœ¨è¿™ç§åˆ†å¸ƒä¸ç¡®å®šæ€§ä¸‹ï¼Œf-FERMä¸ä»…å…·æœ‰ç†è®ºçš„æ”¶æ•›ä¿è¯ï¼Œè¿˜è¶…è¶Šäº†æ–‡çŒ®ä¸­å…¶ä»–åŸºå‡†å€¼åœ¨ä»»åŠ¡ä¸­çš„åˆ†å¸ƒåç§»æƒ…å†µä¸‹çš„è¡¨ç°ã€‚ä¸€ä¸ªæœ‰æ•ˆçš„éšæœºå®ç°çš„$f$-FERMå…¬å¼€å¯ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="CAFE-Towards-Compact-Adaptive-and-Fast-Embedding-for-Large-scale-Recommendation-Models"><a href="#CAFE-Towards-Compact-Adaptive-and-Fast-Embedding-for-Large-scale-Recommendation-Models" class="headerlink" title="CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models"></a>CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale Recommendation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03256">http://arxiv.org/abs/2312.03256</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hugozhl/cafe">https://github.com/hugozhl/cafe</a></li>
<li>paper_authors: Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ä¸ªå¯é ã€å¿«é€Ÿã€é€‚åº”åŠ¨æ€æ•°æ®åˆ†å¸ƒçš„åµŒå…¥è¡¨å‹ç¼©æ¡†æ¶ï¼Œä»¥åº”å¯¹æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆDLRMï¼‰çš„å¢é•¿å†…å­˜éœ€æ±‚ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªåä¸ºCAFEçš„åµŒå…¥å‹ç¼©æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå¿«é€Ÿå’Œè½»é‡çº§çš„çƒ­åº¦æµ‹é‡æŠ€æœ¯ï¼ˆHotSketchï¼‰æ¥æ•æ‰å…·æœ‰é‡è¦æ€§çš„åµŒå…¥ï¼Œå¹¶å°†å…¶æ˜ å°„ä¸ºå”¯ä¸€çš„åµŒå…¥ã€‚å¯¹äºä¸å…·æœ‰é‡è¦æ€§çš„åµŒå…¥ï¼ŒCAFEä½¿ç”¨äº†HashåµŒå…¥æŠ€æœ¯ï¼Œè®©å¤šä¸ªåµŒå…¥å…±äº«ä¸€ä¸ªåµŒå…¥ã€‚æ­¤å¤–ï¼ŒCAFEè¿˜æå‡ºäº†ä¸€ä¸ªå¤šçº§HashåµŒå…¥æ¡†æ¶æ¥ä¼˜åŒ–åµŒå…¥è¡¨çš„å‹ç¼©ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCAFEåœ¨10000å€å‹ç¼©æ¯”ä¾‹ä¸‹å¯¹äºCriteo Kaggleæ•°æ®é›†å’ŒCriteoTBæ•°æ®é›†çš„æµ‹è¯•AUCæœ‰3.92%å’Œ3.68%çš„æå‡ï¼Œè¾ƒ existed embedding compression methods superiorã€‚<details>
<summary>Abstract</summary>
Recently, the growing memory demands of embedding tables in Deep Learning Recommendation Models (DLRMs) pose great challenges for model training and deployment. Existing embedding compression solutions cannot simultaneously meet three key design requirements: memory efficiency, low latency, and adaptability to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive, and Fast Embedding compression framework that addresses the above requirements. The design philosophy of CAFE is to dynamically allocate more memory resources to important features (called hot features), and allocate less memory to unimportant ones. In CAFE, we propose a fast and lightweight sketch data structure, named HotSketch, to capture feature importance and report hot features in real time. For each reported hot feature, we assign it a unique embedding. For the non-hot features, we allow multiple features to share one embedding by using hash embedding technique. Guided by our design philosophy, we further propose a multi-level hash embedding framework to optimize the embedding tables of non-hot features. We theoretically analyze the accuracy of HotSketch, and analyze the model convergence against deviation. Extensive experiments show that CAFE significantly outperforms existing embedding compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The source codes of CAFE are available at GitHub.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸï¼Œæ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆDLRMï¼‰ä¸­åµŒå…¥è¡¨çš„å¢é•¿å†…å­˜éœ€æ±‚å¸¦æ¥äº†è®­ç»ƒå’Œéƒ¨ç½²æ¨¡å‹çš„å·¨å¤§æŒ‘æˆ˜ã€‚ç°æœ‰çš„åµŒå…¥å‹ç¼©è§£å†³æ–¹æ¡ˆæ— æ³•åŒæ—¶æ»¡è¶³ä¸‰ä¸ªå…³é”®è®¾è®¡è¦æ±‚ï¼šå†…å­˜æ•ˆç‡ã€å»¶è¿Ÿä½å’Œé€‚åº”åŠ¨æ€æ•°æ®åˆ†å¸ƒã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºCAFEçš„å‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥åŒæ—¶æ»¡è¶³ä»¥ä¸Šä¸‰ä¸ªè¦æ±‚ã€‚CAFEçš„è®¾è®¡å“²å­¦æ˜¯åŠ¨æ€åˆ†é…æ›´å¤šçš„å†…å­˜èµ„æºåˆ°é‡è¦çš„ç‰¹å¾ï¼ˆå³çƒ­ç‰¹å¾ï¼‰ï¼Œå¹¶å°†ä¸é‡è¦çš„ç‰¹å¾åˆ†é…åˆ°æ›´å°‘çš„å†…å­˜ä¸­ã€‚åœ¨CAFEä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¿«é€Ÿå’Œè½»é‡çº§çš„ç¬”è®°æ•°æ®ç»“æ„ï¼Œåä¸ºçƒ­ç¬”è®°ï¼ˆHotSketchï¼‰ï¼Œç”¨äºæ•æ‰ç‰¹å¾é‡è¦æ€§å¹¶åœ¨å®æ—¶ä¸ŠæŠ¥çƒ­ç‰¹å¾ã€‚å¯¹äºæ¯ä¸ªæŠ¥å‘Šçš„çƒ­ç‰¹å¾ï¼Œæˆ‘ä»¬ä¸ºå®ƒåˆ†é…å”¯ä¸€çš„åµŒå…¥ã€‚å¯¹äºéçƒ­ç‰¹å¾ï¼Œæˆ‘ä»¬ä½¿ç”¨å“ˆå¸ŒåµŒå…¥æŠ€æœ¯ï¼Œå…è®¸å¤šä¸ªç‰¹å¾å…±äº«ä¸€ä¸ªåµŒå…¥ã€‚æ ¹æ®æˆ‘ä»¬çš„è®¾è®¡å“²å­¦ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªå¤šçº§å“ˆå¸ŒåµŒå…¥æ¡†æ¶ï¼Œä»¥ä¼˜åŒ–åµŒå…¥è¡¨çš„éçƒ­ç‰¹å¾ã€‚æˆ‘ä»¬ theoretically åˆ†æäº†çƒ­ç¬”è®°çš„å‡†ç¡®æ€§ï¼Œå¹¶åˆ†ææ¨¡å‹å¯¹åå·®çš„æŠ—è¡¡ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬å‘ç°CAFEåœ¨10000å€å‹ç¼©æ¯”ä¸‹æ˜¾è‘—è¶…è¶Šç°æœ‰çš„åµŒå…¥å‹ç¼©æ–¹æ³•ï¼Œåœ¨ krito Kaggle æ•°æ®é›†å’Œ krito TB æ•°æ®é›†ä¸Šæµ‹è¯• AUC æé«˜3.92%å’Œ3.68%ã€‚CAFE çš„æºä»£ç å¯ä»¥åœ¨ GitHub ä¸Šä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Seller-side-Outcome-Fairness-in-Online-Marketplaces"><a href="#Seller-side-Outcome-Fairness-in-Online-Marketplaces" class="headerlink" title="Seller-side Outcome Fairness in Online Marketplaces"></a>Seller-side Outcome Fairness in Online Marketplaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03253">http://arxiv.org/abs/2312.03253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikun Ye, Reza Yousefi Maragheh, Lalitesh Morishetti, Shanu Vashishtha, Jason Cho, Kaushiki Nag, Sushant Kumar, Kannan Achan</li>
<li>for:  investigate and achieve seller-side fairness within online marketplaces</li>
<li>methods:  introduce the notion of seller-side outcome fairness and build an optimization model based on duality and bandit theory</li>
<li>results:  lift seller fairness measures without hurting metrics like collected Gross Merchandise Value (GMV) and total purchases.Hereâ€™s the full text in Simplified Chinese:</li>
<li>for: è¿™ paper aims to investigate and achieve seller-side fairness within online marketplaces</li>
<li>methods: è¯¥ paper introduces the notion of seller-side outcome fairness and builds an optimization model based on duality and bandit theory</li>
<li>results: è¯¥ algorithm can lift seller fairness measures without hurting metrics like collected Gross Merchandise Value (GMV) and total purchases.<details>
<summary>Abstract</summary>
This paper aims to investigate and achieve seller-side fairness within online marketplaces, where many sellers and their items are not sufficiently exposed to customers in an e-commerce platform. This phenomenon raises concerns regarding the potential loss of revenue associated with less exposed items as well as less marketplace diversity. We introduce the notion of seller-side outcome fairness and build an optimization model to balance collected recommendation rewards and the fairness metric. We then propose a gradient-based data-driven algorithm based on the duality and bandit theory. Our numerical experiments on real e-commerce data sets show that our algorithm can lift seller fairness measures while not hurting metrics like collected Gross Merchandise Value (GMV) and total purchases.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation in Simplified Chinese:è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯åœ¨åœ¨çº¿å¸‚åœºåœºæ‰€å†…è°ƒæŸ¥å’Œå®ç°å–å®¶ä¾§å…¬æ­£ï¼Œ Ğ³Ğ´Ğµè®¸å¤šå–å®¶å’Œä»–ä»¬çš„å•†å“åœ¨ç”µå•†å¹³å°ä¸Šæœªèƒ½å¾—åˆ°è¶³å¤Ÿçš„æ›å…‰ã€‚è¿™ç§ç°è±¡å¼•å‘äº†æ”¶ç›ŠæŸå¤±å’Œå¸‚åœºå¤šæ ·æ€§çš„å…³æ³¨ã€‚æˆ‘ä»¬ä»‹ç»äº†å–å®¶ä¾§ç»“æœå…¬æ­£çš„æ¦‚å¿µï¼Œå¹¶å»ºç«‹äº†ä¼˜åŒ–æ¨¡å‹æ¥å‡è¡¡æ”¶é›†æ¨èå¥–åŠ±å’Œå…¬æ­£åº¦é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬æè®®äº†åŸºäºåå¾®åˆ†å’Œéšæœºç†è®ºçš„æ¢¯åº¦é©±åŠ¨æ•°æ®é©±åŠ¨ç®—æ³•ã€‚æˆ‘ä»¬çš„æ•°å€¼å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æé«˜å–å®¶å…¬æ­£åº¦é‡è€Œä¸ä¼šå‰Šå¼±æ”¶é›†çš„æ€»è¥ä¸šé¢ï¼ˆGMVï¼‰å’Œæ€»é”€å”®é¢ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Neural-Physics-Solvers-by-Baldwinian-Evolution"><a href="#Generalizable-Neural-Physics-Solvers-by-Baldwinian-Evolution" class="headerlink" title="Generalizable Neural Physics Solvers by Baldwinian Evolution"></a>Generalizable Neural Physics Solvers by Baldwinian Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03243">http://arxiv.org/abs/2312.03243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chiuph/baldwinian-pinn">https://github.com/chiuph/baldwinian-pinn</a></li>
<li>paper_authors: Jian Cheng Wong, Chin Chun Ooi, Abhishek Gupta, Pao-Hsiung Chiu, Joshua Shao Zheng Low, My Ha Dao, Yew-Soon Ong</li>
<li>for: ç ”ç©¶ç”¨physics-informed neural networksï¼ˆPINNsï¼‰å¯ä»¥åº”ç”¨äºå¤šç§ç‰©ç†ä»»åŠ¡ä¸­ï¼Œå¹¶ä¸”èƒ½å¤Ÿå¿«é€Ÿé€‚åº”å’Œé¢„æµ‹ç‰©ç†ç°è±¡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ç”Ÿç‰©å­¦çš„è§‚ç‚¹ï¼Œå³é¸½å­çš„ç”Ÿé•¿å’Œå­¦ä¹ è¿‡ç¨‹ï¼Œæ¥è®¾è®¡PINNsã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨äº†è¿›åŒ–é€‰æ‹©å‹åŠ›å’Œç”Ÿå‘½æ—¶é—´å­¦ä¹ æ¥è®­ç»ƒPINNsï¼Œä»¥å®ç°å¿«é€Ÿå’Œç¬¦åˆç‰©ç†è§„å¾‹çš„é¢„æµ‹ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡å‘ç°ï¼Œä½¿ç”¨ Baldwin æ•ˆåº”æ¥è®­ç»ƒ PINNsï¼Œå¯ä»¥å¤§å¤§æé«˜é¢„æµ‹ç²¾åº¦ï¼Œå¹¶ä¸”ä»…éœ€ä¸€å°éƒ¨åˆ†çš„è®¡ç®—æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨ gradient descent æ¥ meta-learn PINNsï¼Œåªèƒ½å®ç°ä¸€å°éƒ¨åˆ†çš„é¢„æµ‹ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Physics-informed neural networks (PINNs) are at the forefront of scientific machine learning, making possible the creation of machine intelligence that is cognizant of physical laws and able to accurately simulate them. In this paper, the potential of discovering PINNs that generalize over an entire family of physics tasks is studied, for the first time, through a biological lens of the Baldwin effect. Drawing inspiration from the neurodevelopment of precocial species that have evolved to learn, predict and react quickly to their environment, we envision PINNs that are pre-wired with connection strengths inducing strong biases towards efficient learning of physics. To this end, evolutionary selection pressure (guided by proficiency over a family of tasks) is coupled with lifetime learning (to specialize on a smaller subset of those tasks) to produce PINNs that demonstrate fast and physics-compliant prediction capabilities across a range of empirically challenging problem instances. The Baldwinian approach achieves an order of magnitude improvement in prediction accuracy at a fraction of the computation cost compared to state-of-the-art results with PINNs meta-learned by gradient descent. This paper marks a leap forward in the meta-learning of PINNs as generalizable physics solvers.
</details>
<details>
<summary>æ‘˜è¦</summary>
Physics-informed neural networks (PINNs) æ˜¯ç§‘å­¦æœºå™¨å­¦ä¹ çš„å‰æ²¿é¢†åŸŸï¼Œä½¿å¾—æœºå™¨æ™ºèƒ½èƒ½å¤Ÿè®¤è¯†ç‰©ç†æ³•å¾‹å¹¶å‡†ç¡®åœ°æ¨¡æ‹Ÿå®ƒä»¬ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ï¼Œé€šè¿‡ç”Ÿç‰©å­¦çš„è§‚ç‚¹ï¼Œå³è´å°”å¾·æ¸©æ•ˆåº”ï¼Œå¯¹PINNsçš„æ³›åŒ–æ€§èƒ½è¿›è¡Œç ”ç©¶ï¼Œä»¥ä¾¿åˆ›é€ èƒ½å¤Ÿå¿«é€Ÿã€å‡†ç¡®åœ°é¢„æµ‹ç‰©ç†ç°è±¡çš„æœºå™¨æ™ºèƒ½ã€‚æˆ‘ä»¬ç§¯æå€Ÿé‰´äº†å“ºä¹³åŠ¨ç‰©èƒšèƒå‘è‚²ä¸­çš„å­¦ä¹ ã€é¢„æµ‹å’Œååº”æœºåˆ¶ï¼Œä»¥è®¾è®¡PINNså…·æœ‰å¼ºçƒˆçš„å­¦ä¹ åå¥½ï¼Œä½¿å…¶èƒ½å¤Ÿå¿«é€Ÿã€æ•ˆç‡åœ°å­¦ä¹ ç‰©ç†çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡ç”Ÿæ€é€‰æ‹©å‹åŠ›ï¼ˆåŸºäºä¸€å®¶ physics ä»»åŠ¡çš„æ‰§è¡Œæ•ˆæœï¼‰å’Œç”Ÿç‰©å­¦å­¦ä¹ ï¼ˆç‰¹å®šä»»åŠ¡ä¸Šçš„ç‰¹æ®ŠåŒ–å­¦ä¹ ï¼‰æ¥åˆ¶å®šPINNsï¼Œä»è€Œå®ç°äº†åœ¨å¤šç§å®éªŒå›°éš¾çš„é—®é¢˜ä¸Šçš„å¿«é€Ÿã€ç‰©ç†ç›¸ç¬¦çš„é¢„æµ‹èƒ½åŠ›ã€‚ Baldwinian æ–¹æ³•ç›¸æ¯”äºä½¿ç”¨ PINNs  meta-å­¦ä¹ Gradient Descent çš„ç°æœ‰ç»“æœï¼Œå¯ä»¥è¾¾åˆ°ä¸€ä¸ªæ•°é‡çº§çš„æ”¹è¿›ï¼Œå¹¶ä¸”åªéœ€è¦ä¸€å°éƒ¨åˆ†çš„è®¡ç®—æˆæœ¬ã€‚è¿™ç¯‡è®ºæ–‡æ ‡å¿—ç€ PINNs çš„å…ƒå­¦ä¹ ä¸ºæ³›åŒ–çš„ç‰©ç†è§£å†³æ–¹æ¡ˆåšå‡ºäº†é‡è¦çš„çªç ´ã€‚
</details></li>
</ul>
<hr>
<h2 id="Accelerated-Gradient-Algorithms-with-Adaptive-Subspace-Search-for-Instance-Faster-Optimization"><a href="#Accelerated-Gradient-Algorithms-with-Adaptive-Subspace-Search-for-Instance-Faster-Optimization" class="headerlink" title="Accelerated Gradient Algorithms with Adaptive Subspace Search for Instance-Faster Optimization"></a>Accelerated Gradient Algorithms with Adaptive Subspace Search for Instance-Faster Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03218">http://arxiv.org/abs/2312.03218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanshi Liu, Hanzhen Zhao, Yang Xu, Pengyun Yue, Cong Fang</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æ¢è®¨ Gradient-based æœ€ä¼˜åŒ–ç®—æ³•åœ¨è¿ç»­ä¼˜åŒ–å’Œæœºå™¨å­¦ä¹ ä¸­çš„å‘å±•ï¼Œå¹¶æå‡ºä¸€ç§æ–°çš„æ–¹æ³•æ¥è®¾è®¡å’Œåˆ†æè¿™ç±»ç®—æ³•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸¤ä¸ªå› ç´ ($\alpha$, $\tau_{\alpha}$)æ¥ç»†åŒ–æè¿°ä¼˜åŒ–é—®é¢˜çš„å‡é›¶æƒ…å†µï¼Œå¹¶è®¾è®¡äº†ä¸€ç§å¯é€‚åº”é—®é¢˜çš„ adaptive ç®—æ³•ï¼Œä»¥è§£å†³ä¸€äº›æœºå™¨å­¦ä¹ ä¸­çš„é—®é¢˜ã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„ $\mathcal{O}(1)$-æ ¸å‡½æ•°çº¦æŸçš„ linear regression ç®—æ³•ï¼Œä»¥åŠä¸€äº›å…¶ä»–æœºå™¨å­¦ä¹ ä¸­çš„é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­å…·æœ‰æ›´é«˜çš„State-of-the-art å¤æ‚æ€§ã€‚<details>
<summary>Abstract</summary>
Gradient-based minimax optimal algorithms have greatly promoted the development of continuous optimization and machine learning. One seminal work due to Yurii Nesterov [Nes83a] established $\tilde{\mathcal{O}(\sqrt{L/\mu})$ gradient complexity for minimizing an $L$-smooth $\mu$-strongly convex objective. However, an ideal algorithm would adapt to the explicit complexity of a particular objective function and incur faster rates for simpler problems, triggering our reconsideration of two defeats of existing optimization modeling and analysis. (i) The worst-case optimality is neither the instance optimality nor such one in reality. (ii) Traditional $L$-smoothness condition may not be the primary abstraction/characterization for modern practical problems.   In this paper, we open up a new way to design and analyze gradient-based algorithms with direct applications in machine learning, including linear regression and beyond. We introduce two factors $(\alpha, \tau_{\alpha})$ to refine the description of the degenerated condition of the optimization problems based on the observation that the singular values of Hessian often drop sharply. We design adaptive algorithms that solve simpler problems without pre-known knowledge with reduced gradient or analogous oracle accesses. The algorithms also improve the state-of-art complexities for several problems in machine learning, thereby solving the open problem of how to design faster algorithms in light of the known complexity lower bounds. Specially, with the $\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal $\tilde{\mathcal{O}(\mu^{-1/3})$ (v.s. $\tilde{\mathcal{O}(\mu^{-1/2})$) gradient complexity for linear regression. We hope this work could invoke the rethinking for understanding the difficulty of modern problems in optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
gradient-based minimum-maximumç®—æ³•åœ¨è¿ç»­ä¼˜åŒ–å’Œæœºå™¨å­¦ä¹ å‘å±•ä¸­å…·æœ‰å·¨å¤§çš„å½±å“ã€‚ä¸€é¡¹è‘—åçš„è®ºæ–‡ï¼ˆNes83aï¼‰ç¡®ç«‹äº†$\tilde{\mathcal{O}(\sqrt{L/\mu})$çš„æ¢¯åº¦å¤æ‚åº¦æ¥ä¼˜åŒ–ä¸€ä¸ª$L$-å¹³æ»‘$\mu$-å¼ºçƒˆçš„ç›®æ ‡å‡½æ•°ã€‚ç„¶è€Œï¼Œç†æƒ³çš„ç®—æ³•åº”è¯¥é€‚åº”ç‰¹å®šç›®æ ‡å‡½æ•°çš„æ˜¾ç¤ºå¤æ‚åº¦ï¼Œå¹¶åœ¨æ›´ç®€å•çš„é—®é¢˜ä¸Šå…·æœ‰æ›´å¿«çš„é€Ÿåº¦ï¼Œè¿™å¼•å‘äº†æˆ‘ä»¬å¯¹ç°æœ‰ä¼˜åŒ–æ¨¡å‹å’Œåˆ†æçš„é‡æ–°è€ƒè™‘ã€‚ï¼ˆiï¼‰æœ€åæƒ…å†µä¼˜åŒ–ä¸æ˜¯å®é™…ä¸­çš„å®é™…ä¼˜åŒ–ã€‚ï¼ˆiiï¼‰ä¼ ç»Ÿçš„$L$-å¹³æ»‘æ¡ä»¶å¯èƒ½ä¸æ˜¯ç°ä»£å®é™…é—®é¢˜çš„ä¸»è¦æŠ½è±¡/ç‰¹å¾ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€åˆ›äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è®¾è®¡å’Œåˆ†ææ¢¯åº¦åŸºäºç®—æ³•ï¼Œç›´æ¥åº”ç”¨äºæœºå™¨å­¦ä¹ é¢†åŸŸï¼ŒåŒ…æ‹¬çº¿æ€§å›å½’å’Œæ›´å¤šçš„é—®é¢˜ã€‚æˆ‘ä»¬å¼•å…¥ä¸¤ä¸ªå› ç´ $(\alpha, \tau_{\alpha})$æ¥ç»†åŒ–ä¼˜åŒ–é—®é¢˜çš„å¼‚å¸¸æƒ…å†µï¼Œæ³¨æ„åˆ°æ¢¯åº¦çŸ©é˜µçš„ç‰¹å¾å€¼å¾€å¾€å¿«é€Ÿä¸‹é™ã€‚æˆ‘ä»¬è®¾è®¡é€‚åº”é—®é¢˜çš„ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸çŸ¥é“é—®é¢˜çš„å…ˆå‰çŸ¥è¯†çš„æƒ…å†µä¸‹è§£å†³é—®é¢˜ï¼Œå¹¶ä¸”æé«˜äº†ç°æœ‰å¤æ‚æ€§ä¸‹ç•Œçš„çŠ¶æ€ã€‚å…·ä½“æ¥è¯´ï¼Œå½“$\mathcal{O}(1)$-æ ¸å‡½æ•°çº¦æŸæ—¶ï¼Œæˆ‘ä»¬å®ç°äº†æœ€ä½³çš„$\tilde{\mathcal{O}(\mu^{-1/3})$ï¼ˆvs. $\tilde{\mathcal{O}(\mu^{-1/2})$ï¼‰æ¢¯åº¦å¤æ‚åº¦ï¼Œç”¨äºçº¿æ€§å›å½’ã€‚æˆ‘ä»¬å¸Œæœ›è¿™é¡¹å·¥ä½œèƒ½å¤Ÿè®©äººä»¬é‡æ–°æ€è€ƒç°ä»£ä¼˜åŒ–é—®é¢˜çš„å›°éš¾æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Bootstrap-Your-Own-Variance"><a href="#Bootstrap-Your-Own-Variance" class="headerlink" title="Bootstrap Your Own Variance"></a>Bootstrap Your Own Variance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03213">http://arxiv.org/abs/2312.03213</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Stathiskan/HTML-CSS-Bootstrap-Framework-Razor">https://github.com/Stathiskan/HTML-CSS-Bootstrap-Framework-Razor</a></li>
<li>paper_authors: Polina Turishcheva, Jason Ramapuram, Sinead Williamson, Dan Busbridge, Eeshan Dhekane, Russ Webb</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ¨¡å‹uncertaintyçš„ç†è§£ï¼Œä»¥ä¾¿åº”ç”¨äºå¤šä¸ªé¢†åŸŸã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨Bootstrap Your Own Varianceï¼ˆBYOVï¼‰ï¼Œ ĞºĞ¾Ğ¼bining Bootstrap Your Own Latentï¼ˆBYOLï¼‰ã€ä¸€ç§è‡ªç„¶è¯­è¨€å¤„ç†ç®—æ³•ï¼Œå’ŒæŠ½è±¡æ¢¯åº¦ä¸‹é™ï¼ˆBBBï¼‰ã€‚</li>
<li>results: ç ”ç©¶å‘ç° BYOV å¯¹äºæµ‹è¯•é›†çš„é¢„æµ‹æ ‡å‡†å·®å¯ä»¥å¾ˆå¥½åœ°è¢«æ•æ‰ä¸º Gaussian åˆ†å¸ƒï¼Œè¿™æä¾›äº†åˆæ­¥çš„è¯æ®ï¼Œè¡¨æ˜å­¦ä¹ çš„å‚æ•° posterior æœ‰ç”¨äºæ— æ ‡ç­¾uncertainty estimationã€‚ BYOV åœ¨å¯¹æ¯” deterministic BYOL åŸºelineä¸Šæé«˜äº†æµ‹è¯•é›†çš„æ€§èƒ½ (+2.83% test ECE, +1.03% test Brier)ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ‰©å±•ä¸­è¡¨ç°äº†æ›´å¥½çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ˆä¾‹å¦‚ï¼Œ+2.4% test ECE, +1.2% test Brier for Salt &amp; Pepper noiseï¼‰ã€‚<details>
<summary>Abstract</summary>
Understanding model uncertainty is important for many applications. We propose Bootstrap Your Own Variance (BYOV), combining Bootstrap Your Own Latent (BYOL), a negative-free Self-Supervised Learning (SSL) algorithm, with Bayes by Backprop (BBB), a Bayesian method for estimating model posteriors. We find that the learned predictive std of BYOV vs. a supervised BBB model is well captured by a Gaussian distribution, providing preliminary evidence that the learned parameter posterior is useful for label free uncertainty estimation. BYOV improves upon the deterministic BYOL baseline (+2.83% test ECE, +1.03% test Brier) and presents better calibration and reliability when tested with various augmentations (eg: +2.4% test ECE, +1.2% test Brier for Salt & Pepper noise).
</details>
<details>
<summary>æ‘˜è¦</summary>
ç†è§£æ¨¡å‹uncertaintyçš„é‡è¦æ€§å¯¹è®¸å¤šåº”ç”¨æœ‰ç›Šã€‚æˆ‘ä»¬æå‡ºäº†è‡ªé€‚åº”Bootstrap Your Own Varianceï¼ˆBYOVï¼‰ï¼Œå°†è‡ªé€‚åº”Bootstrap Your Own Latentï¼ˆBYOLï¼‰ã€ä¸€ç§æ— ç›‘ç£è‡ªé€‚åº”å­¦ä¹ ï¼ˆSSLï¼‰ç®—æ³•ï¼Œä¸æŠ½è±¡åéªŒï¼ˆBBBï¼‰ Bayesianæ–¹æ³•ç›¸ç»“åˆã€‚æˆ‘ä»¬å‘ç° BYOV å­¦ä¹ çš„é¢„æµ‹æ ‡å‡†å·®å¯ä»¥ç”¨ Gaussian åˆ†å¸ƒæ•æ‰ï¼Œè¿™æä¾›äº†åˆæ­¥è¯æ®ï¼Œè¡¨æ˜learned parameter posterior æœ‰ç”¨äºæ— ç›‘ç£ uncertainty estimationã€‚ BYOV åœ¨ deterministic BYOL åŸºelineä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼ˆ+2.83% æµ‹è¯• ECEï¼Œ+1.03% æµ‹è¯• Brierï¼‰ï¼Œå¹¶åœ¨ä¸åŒçš„æ‰©å±•ä¸­ï¼ˆå¦‚ Salt & Pepper å™ªå£°ï¼‰è¡¨ç°å‡ºæ›´å¥½çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼ˆ+2.4% æµ‹è¯• ECEï¼Œ+1.2% æµ‹è¯• Brierï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Constrained-Bayesian-Optimization-Under-Partial-Observations-Balanced-Improvements-and-Provable-Convergence"><a href="#Constrained-Bayesian-Optimization-Under-Partial-Observations-Balanced-Improvements-and-Provable-Convergence" class="headerlink" title="Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence"></a>Constrained Bayesian Optimization Under Partial Observations: Balanced Improvements and Provable Convergence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03212">http://arxiv.org/abs/2312.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbo Wang, Ke Li</li>
<li>for: è§£å†³é«˜æˆæœ¬çš„åŠå¯è§‚æµ‹çº¦æŸä¼˜åŒ–é—®é¢˜ (POCOPs)ï¼Œå› ä¸ºæ— æ³•å¯è§‚æµ‹çš„è§£ä¸èƒ½æä¾›ä¼˜åŒ–ç›®æ ‡å’Œçº¦æŸçš„ä¿¡æ¯ã€‚</li>
<li>methods: æå‡ºä¸€ç§é«˜æ•ˆå¯è¯æ˜çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼šé¦–å…ˆï¼Œæå‡ºä¸€ç§æ”¹è¿›çš„è·å–å‡½æ•°è®¾è®¡ï¼Œä»¥ä¾¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å……åˆ†å‹‡æ°”ç»ƒä¹ ; å…¶æ¬¡ï¼Œæå‡ºä¸€ç§åŸºäºå‡è®¾åˆ†å¸ƒå‡½æ•°çš„åµŒå…¥æ¨¡å‹ï¼Œç”¨äºè¡¨ç¤ºå¯è§‚æµ‹çº¦æŸçš„å¯èƒ½æ€§ç©ºé—´ã€‚</li>
<li>results: é€šè¿‡syntheticå’Œå®é™…é—®é¢˜çš„å®éªŒç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ–¹æ³•çš„ç«äº‰åŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè§£å†³POCOPsã€‚<details>
<summary>Abstract</summary>
The partially observable constrained optimization problems (POCOPs) impede data-driven optimization techniques since an infeasible solution of POCOPs can provide little information about the objective as well as the constraints. We endeavor to design an efficient and provable method for expensive POCOPs under the framework of constrained Bayesian optimization. Our method consists of two key components. Firstly, we present an improved design of the acquisition functions that introduces balanced exploration during optimization. We rigorously study the convergence properties of this design to demonstrate its effectiveness. Secondly, we propose a Gaussian process embedding different likelihoods as the surrogate model for a partially observable constraint. This model leads to a more accurate representation of the feasible regions compared to traditional classification-based models. Our proposed method is empirically studied on both synthetic and real-world problems. The results demonstrate the competitiveness of our method for solving POCOPs.
</details>
<details>
<summary>æ‘˜è¦</summary>
partially observable constrained optimization problems (POCOPs) å¦¨ç¢æ•°æ®é©±åŠ¨ä¼˜åŒ–æŠ€æœ¯ï¼Œå› ä¸ºä¸å¯èƒ½çš„è§£å†³æ–¹æ¡ˆå¯ä»¥æä¾›å°‘é‡ä¿¡æ¯å¯¹ç›®æ ‡ä»¥åŠçº¦æŸã€‚æˆ‘ä»¬åŠªåŠ›è®¾è®¡é«˜æ•ˆå¯è¯æ˜çš„æ–¹æ³•æ¥è§£å†³è´µ POCOPs ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ”¹è¿›çš„è·å–å‡½æ•°è®¾è®¡ï¼Œå…·æœ‰å¹³è¡¡çš„æ¢ç´¢ç‰¹æ€§ã€‚æˆ‘ä»¬ä»”ç»†ç ”ç©¶äº†è¿™ç§è®¾è®¡çš„æ”¶æ•›æ€§ï¼Œä»¥è¯æ˜å…¶æ•ˆæœã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨ Gaussian process åµŒå…¥ä¸åŒçš„å¯èƒ½æ€§ä½œä¸ºçº¦æŸçš„æ¨¡å‹ã€‚è¿™ç§æ¨¡å‹å¯ä»¥æ›´åŠ å‡†ç¡®åœ°è¡¨ç¤ºå¯è¡ŒåŒºåŸŸï¼Œç›¸æ¯”ä¼ ç»Ÿçš„åˆ†ç±»å‹æ¨¡å‹ã€‚æˆ‘ä»¬çš„æè®®æ–¹æ³•åœ¨ synthetic å’Œå®é™…é—®é¢˜ä¸Šè¿›è¡Œäº† empirical ç ”ç©¶ï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°è§£å†³ POCOPsã€‚
</details></li>
</ul>
<hr>
<h2 id="Domain-Invariant-Representation-Learning-and-Sleep-Dynamics-Modeling-for-Automatic-Sleep-Staging"><a href="#Domain-Invariant-Representation-Learning-and-Sleep-Dynamics-Modeling-for-Automatic-Sleep-Staging" class="headerlink" title="Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging"></a>Domain Invariant Representation Learning and Sleep Dynamics Modeling for Automatic Sleep Staging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03196">http://arxiv.org/abs/2312.03196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yeon-lab/dream">https://github.com/yeon-lab/dream</a></li>
<li>paper_authors: Seungyeon Lee, Thai-Hoang Pham, Zhao Cheng, Ping Zhang</li>
<li>For:  automatic sleep staging to diagnose and treat sleep disorders* Methods:  neural network-based model (DREAM) to learn domain generalized representations from physiological signals and model sleep dynamics* Results:  outperforms existing sleep staging methods on three datasets, and provides prediction uncertainty to ensure reliability in real-world applications.Hereâ€™s the full text in Simplified Chinese:* For: è‡ªåŠ¨ç¡çœ é˜¶æ®µè¯†åˆ«ï¼Œä»¥è¯Šæ–­å’Œæ²»ç–—ç¡çœ éšœç¢ã€‚* Methods: ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆDREAMï¼‰ï¼Œä»ç”Ÿç‰©ä½“ä¿¡å·ä¸­å­¦ä¹ Domain Generalizedè¡¨ç¤ºï¼Œå¹¶æ¨¡å‹ç¡çœ  dinamicsã€‚* Results: åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šæ¯”å‰è€…è¿ç®—æ›´å¥½ï¼Œå¹¶æä¾›äº†é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œä»¥ç¡®ä¿å®é™…åº”ç”¨ä¸­çš„å¯é æ€§ã€‚<details>
<summary>Abstract</summary>
Sleep staging has become a critical task in diagnosing and treating sleep disorders to prevent sleep related diseases. With rapidly growing large scale public sleep databases and advances in machine learning, significant progress has been made toward automatic sleep staging. However, previous studies face some critical problems in sleep studies; the heterogeneity of subjects' physiological signals, the inability to extract meaningful information from unlabeled sleep signal data to improve predictive performances, the difficulty in modeling correlations between sleep stages, and the lack of an effective mechanism to quantify predictive uncertainty. In this study, we propose a neural network based automatic sleep staging model, named DREAM, to learn domain generalized representations from physiological signals and models sleep dynamics. DREAM learns sleep related and subject invariant representations from diverse subjects' sleep signal segments and models sleep dynamics by capturing interactions between sequential signal segments and between sleep stages. In the experiments, we demonstrate that DREAM outperforms the existing sleep staging methods on three datasets. The case study demonstrates that our model can learn the generalized decision function resulting in good prediction performances for the new subjects, especially in case there are differences between testing and training subjects. The usage of unlabeled data shows the benefit of leveraging unlabeled EEG data. Further, uncertainty quantification demonstrates that DREAM provides prediction uncertainty, making the model reliable and helping sleep experts in real world applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼‘çœ é˜¶æ®µè¯†åˆ«å·²æˆä¸ºè¯Šæ–­å’Œæ²»ç–—ç¡çœ ç–¾ç—…çš„å…³é”®ä»»åŠ¡ã€‚éšç€å¤§è§„æ¨¡å…¬å…±ç¡çœ æ•°æ®åº“çš„å¿«é€Ÿå¢é•¿å’Œæœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼Œè‡ªåŠ¨ä¼‘çœ é˜¶æ®µè¯†åˆ«å¾—åˆ°äº† significiant progressã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶é¢ä¸´äº†ç¡çœ ç ”ç©¶ä¸­çš„ä¸€äº›å…³é”®é—®é¢˜ï¼ŒåŒ…æ‹¬å‚ä¸è€…çš„ç”Ÿç‰©å­¦ä¿¡å·å·®å¼‚æ€§ã€EXTRACTING meaningful information from unlabeled sleep signal data to improve predictive performances, sleep stageä¹‹é—´çš„å…³ç³»éš¾ä»¥å»ºæ¨¡ï¼Œä»¥åŠç¡çœ é¢„æµ‹uncertaintyçš„ç¼ºä¹æœ‰æ•ˆæœºåˆ¶ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„è‡ªåŠ¨ä¼‘çœ é˜¶æ®µè¯†åˆ«æ¨¡å‹ï¼Œåä¸ºDREAMï¼Œä»¥å­¦ä¹ åŸŸå…±é‡‡æ ·è¡¨ç¤ºå’Œæ¨¡å‹ç¡çœ åŠ¨åŠ›å­¦ã€‚DREAMä»å¤šä¸ªå‚ä¸è€…çš„ç¡çœ ä¿¡å·æ®µä¸­å­¦ä¹ ç¡çœ ç›¸å…³å’Œå‚ä¸è€…ä¸åŒçš„è¡¨ç¤ºï¼Œå¹¶ capture interactions between sequential signal segments and between sleep stagesã€‚åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬è¯æ˜DREAMåœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¯”ä¹‹å‰çš„ç¡çœ é˜¶æ®µè¯†åˆ«æ–¹æ³•æ›´å¥½ã€‚ caso studyè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å­¦ä¹ é€šç”¨çš„å†³ç­–å‡½æ•°ï¼Œä»è€Œåœ¨æ–°çš„å‚ä¸è€…ä¸Šè¾¾åˆ°è‰¯å¥½çš„é¢„æµ‹æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æµ‹è¯•å’Œè®­ç»ƒå‚ä¸è€…ä¹‹é—´å­˜åœ¨å·®å¼‚æ—¶ã€‚ä½¿ç”¨æœªæ ‡æ³¨æ•°æ®ä¹Ÿæ˜¾ç¤ºäº†æŠ½è±¡æ•°æ®çš„åˆ©ç”¨çš„å¥½å¤„ã€‚æ­¤å¤–ï¼Œuncertaintyé‡åŒ–è¡¨æ˜DREAMæä¾›äº†é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œä½¿æ¨¡å‹æˆä¸ºå¯é çš„å’Œå¸®åŠ©ç¡çœ ä¸“å®¶åœ¨å®é™…åº”ç”¨ä¸­ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/cs.LG_2023_12_06/" data-id="clq0ru6zy00zlto885ppq5r75" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/eess.IV_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T09:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/06/eess.IV_2023_12_06/">eess.IV - 2023-12-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Bile-Duct-Segmentation-Methods-Under-3D-Slicer-Applied-to-ERCP-Advantages-and-Disadvantages"><a href="#Bile-Duct-Segmentation-Methods-Under-3D-Slicer-Applied-to-ERCP-Advantages-and-Disadvantages" class="headerlink" title="Bile Duct Segmentation Methods Under 3D Slicer Applied to ERCP: Advantages and Disadvantages"></a>Bile Duct Segmentation Methods Under 3D Slicer Applied to ERCP: Advantages and Disadvantages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03356">http://arxiv.org/abs/2312.03356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelhadi Essamlali, Vincent Millot-Maysounabe, Marion Chartier, GrÃ©goire Salin, Aymeric Becq, Lionel ArrivÃ©, Marine Duboc Camus, JÃ©rÃ´me Szewczyk, Isabelle Claude</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨è¯„ä¼°åœ¨3Dé‡å»ºä¸­ä½¿ç”¨çš„èƒ†å›Šé“æ®µåŒ–æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨ä¸åŒçš„å…³é”®æ‰‹æ®µä¸­ï¼Œå¦‚endorroscopic retrograde cholangiopancreatographyï¼ˆERCPï¼‰ä¸­ï¼Œå®ç°æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚</li>
<li>methods: è¿™ç¯‡æ–‡ç« è¯„ä¼°äº†ä¸‰ç§ä¸åŒçš„æ®µåŒ–æ–¹æ³•ï¼Œnamely thresholdingã€flood fillingå’Œregion growingï¼Œå¹¶å¯¹å®ƒä»¬çš„ä¼˜ç¼ºç‚¹è¿›è¡Œè¯„ä»·ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé˜ˆå€¼æ®µåŒ–æ–¹æ³•å‡ ä¹æ˜¯æ‰‹åŠ¨å’Œæ—¶é—´consumingçš„ï¼Œè€Œæ´—æ¶¤å¡«å……æ–¹æ³•æ˜¯åŠè‡ªåŠ¨çš„ï¼Œä½†å®ƒä»¬éƒ½ä¸æ˜¯å¯é‡å¤çš„ã€‚å› æ­¤ï¼Œä¸€ç§åŸºäºåŒºåŸŸç”Ÿé•¿çš„è‡ªåŠ¨æ–¹æ³•è¢«å¼€å‘å‡ºæ¥ï¼Œä»¥å‡å°‘æ®µåŒ–æ—¶é—´ï¼Œä½†æ˜¯è¿™ä¼šå¯¼è‡´æ®µåŒ–è´¨é‡ä¸‹é™ã€‚è¿™äº›ç»“æœhighlightäº†ä¸åŒçš„ä¼ ç»Ÿæ®µåŒ–æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶å¼ºè°ƒäº†åœ¨ERCPä¸­ä¼˜åŒ–èƒ†å›Šé“æ®µåŒ–çš„éœ€è¦ã€‚<details>
<summary>Abstract</summary>
This article presents an evaluation of biliary tract segmentation methods used for 3D reconstruction, which may be very usefull in various critical interventions, such as endoscopic retrograde cholangiopancreatography (ERCP), using the 3D Slicer software. This article provides an assessment of biliary tract segmentation techniques employed for 3D reconstruction, which can prove highly valuable in diverse critical procedures like endoscopic retrograde cholangiopancreatography (ERCP) through the utilization of 3D Slicer software. Three different methods, namely thresholding, flood filling, and region growing, were assessed in terms of their advantages and disadvantages. The study involved 10 patient cases and employed quantitative indices and qualitative evaluation to assess the segmentations obtained by the different segmentation methods against ground truth. The results indicate that the thresholding method is almost manual and time-consuming, while the flood filling method is semi-automatic and also time-consuming. Although both methods improve segmentation quality, they are not reproducible. Therefore, an automatic method based on region growing was developed to reduce segmentation time, albeit at the expense of quality. These findings highlight the pros and cons of different conventional segmentation methods and underscore the need to explore alternative approaches, such as deep learning, to optimize biliary tract segmentation in the context of ERCP.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/eess.IV_2023_12_06/" data-id="clq0ru78f01j9to886ihrg41r" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_12_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/06/eess.SP_2023_12_06/" class="article-date">
  <time datetime="2023-12-06T08:00:00.000Z" itemprop="datePublished">2023-12-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/06/eess.SP_2023_12_06/">eess.SP - 2023-12-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Slepian-Beamforming-Broadband-Beamforming-using-Streaming-Least-Squares"><a href="#Slepian-Beamforming-Broadband-Beamforming-using-Streaming-Least-Squares" class="headerlink" title="Slepian Beamforming: Broadband Beamforming using Streaming Least Squares"></a>Slepian Beamforming: Broadband Beamforming using Streaming Least Squares</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03922">http://arxiv.org/abs/2312.03922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Coleman DeLude, Mark A. Davenport, Justin Romberg</li>
<li>for: æœ¬æ–‡æ¢è®¨äº†ä¸€ä¸ªç»å…¸é—®é¢˜ï¼Œå³åœ¨å¤šæ„Ÿå™¨é˜µåˆ—ä¸Šä¼°è®¡åˆ°æ¥çš„ä¿¡å·ã€‚æœ¬æ–‡ä¸»è¦å…³æ³¨å¸¦å®½ä¿¡å·çš„æƒ…å†µï¼Œå³å¹¿é¢‘ä¿¡å·çš„æ¢æµ‹ã€‚ä¼ ç»Ÿçš„æ–¹æ³•åŒ…æ‹¬è¿‡æ»¤å’Œæ€»å’Œã€çœŸå®æ—¶å»¶æˆ–è¿™ä¸¤è€…çš„ç»„åˆã€‚è€Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸è¿™äº›æ–¹æ³•æœ‰å¾ˆå¤§å·®å¼‚ï¼Œå®ƒä¸éœ€è¦è¿‡æ»¤æˆ–çœŸå®æ—¶å»¶ã€‚æˆ‘ä»¬ä½¿ç”¨ç›´æ¥ä»æ„ŸçŸ¥å™¨è¾“å‡ºä¸­æå–çš„å—æ ·æœ¬æ¥é€‚åº”ç¨³å¥Subspaceæ¨¡å‹ï¼Œä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ¥é€‚åº”ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ¥ä¼°è®¡ uniformly åˆ†å¸ƒçš„å¸¦å®½ä¿¡å·æ ·æœ¬ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨ç›´æ¥ä»æ„ŸçŸ¥å™¨è¾“å‡ºä¸­æå–çš„å—æ ·æœ¬æ¥é€‚åº”ç¨³å¥Subspaceæ¨¡å‹ï¼Œä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ¥é€‚åº”ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•æ¯”æ¿€å…‰æ»¤æ³¢å™¨æ–¹æ³•æœ‰æ›´å¥½çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¸è®¡ç®—å¤æ‚æ€§ç›¸å½“ã€‚<details>
<summary>Abstract</summary>
In this paper we revisit the classical problem of estimating a signal as it impinges on a multi-sensor array. We focus on the case where the impinging signal's bandwidth is appreciable and is operating in a broadband regime. Estimating broadband signals, often termed broadband (or wideband) beamforming, is traditionally done through filter and summation, true time delay, or a coupling of the two. Our proposed method deviates substantially from these paradigms in that it requires no notion of filtering or true time delay. We use blocks of samples taken directly from the sensor outputs to fit a robust Slepian subspace model using a least squares approach. We then leverage this model to estimate uniformly spaced samples of the impinging signal. Alongside a careful discussion of this model and how to choose its parameters we show how to fit the model to new blocks of samples as they are received, producing a streaming output. We then go on to show how this method naturally extends to adaptive beamforming scenarios, where we leverage signal statistics to attenuate interfering sources. Finally, we discuss how to use our model to estimate from dimensionality reducing measurements. Accompanying these discussions are extensive numerical experiments establishing that our method outperforms existing filter based approaches while being comparable in terms of computational complexity.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°è¯„ä¼°äº†ç»å…¸çš„ä¿¡å·ä¼°è®¡é—®é¢˜ï¼Œå³ä¿¡å·åœ¨å¤šæ„Ÿå™¨é˜µåˆ—ä¸Šå……å½“çš„æƒ…å†µã€‚æˆ‘ä»¬ç‰¹ç‚¹åœ¨äºå®½é¢‘ä¿¡å·çš„æƒ…å†µï¼Œå³ä¿¡å·åœ¨å¹¿é¢‘åŸŸå†…æ“ä½œã€‚ä¼ ç»Ÿçš„å¹¿é¢‘ä¿¡å·ä¼°è®¡æ–¹æ³•åŒ…æ‹¬è¿‡æ»¤å’Œæ€»å’Œã€çœŸå®æ—¶å»¶æˆ–ä¸¤è€…çš„ç»„åˆã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ä¸è¿™äº›æ¨¡å¼ä¸åŒï¼Œå®ƒä¸éœ€è¦è¿‡æ»¤æˆ–çœŸå®æ—¶å»¶ã€‚æˆ‘ä»¬ä½¿ç”¨ç›´æ¥ä»æ„ŸçŸ¥å™¨è¾“å‡ºä¸­è·å–çš„å—æ ·æœ¬æ¥é€‚åº”ä¸€ç§å¯é çš„è±å¸ƒå°¼èŒ¨ç©ºé—´æ¨¡å‹ï¼Œä½¿ç”¨æœ€å°äºŒä¹˜æ³•è¿›è¡Œé€‚åº”ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¨¡å‹æ¥ä¼°è®¡å…·æœ‰å‡åŒ€é—´éš”çš„ä¿¡å·å¾æ–™ã€‚æˆ‘ä»¬è¿˜è¯¦ç»†ä»‹ç»äº†è¿™ä¸ªæ¨¡å‹ä»¥åŠå¦‚ä½•é€‰æ‹©å…¶å‚æ•°ï¼Œå¹¶è¯æ˜äº†å¦‚ä½•åœ¨æ–°çš„å—æ ·æœ¬æ¥æ”¶åç»§ç»­é€‚åº”ï¼Œä»è€Œç”ŸæˆæµåŠ¨è¾“å‡ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹æ¥æŠ‘åˆ¶å¹²æ‰°ä¿¡å·ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹æ¥ä¼°è®¡ä»ç»´åº¦å‡å°‘æµ‹é‡ä¸­çš„ä¿¡å·ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è®¡ç®—å¤æ‚æ€§æ–¹é¢ä¸ä¼ ç»Ÿçš„ç­›é€‰æ–¹æ³•ç›¸å½“ï¼Œä½†åœ¨æ€§èƒ½æ–¹é¢è¡¨ç°æ›´ä½³ã€‚Note: The translation is done using Google Translate and may not be perfect. Please note that the translation is for reference only and may not be accurate.
</details></li>
</ul>
<hr>
<h2 id="Community-Detection-in-High-Dimensional-Graph-Ensembles"><a href="#Community-Detection-in-High-Dimensional-Graph-Ensembles" class="headerlink" title="Community Detection in High-Dimensional Graph Ensembles"></a>Community Detection in High-Dimensional Graph Ensembles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03900">http://arxiv.org/abs/2312.03900</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert Malinas, Dogyoon Song, Alfred O. Hero III</li>
<li>for: æœ¬æ–‡æ˜¯ä¸ºäº†æ£€æµ‹é«˜ç»´åº¦å›¾çš„ç¤¾åŒºè€Œå†™çš„ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨éšæœºçŸ©é˜µç†è®ºï¼Œå°†å›¾çš„è¿æ¥çŸ©é˜µæ¨¡å‹ä¸ºStochastic Block Model (SBM)ï¼Œç„¶åæå‡ºäº†ä¸€ç§è½¬æ¢æ–¹æ³•æ¥æ¶ˆé™¤å¯¹ç¤¾åŒºçš„ä¸åŒæƒé‡çš„å½±å“ï¼Œå¹¶ä¿ç•™æœ‰å…³ç¤¾åŒºæ£€æµ‹çš„æœ‰ç”¨ç‰¹å¾ã€‚</li>
<li>results: æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæå€¼ç‰¹å¾å€¼çš„æµ‹è¯•æ–¹æ³•ï¼Œå¯ä»¥æ§åˆ¶æ˜¾è‘—æ€§æ°´å¹³ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå‡è®¾ï¼Œå³æµ‹è¯•åœ¨å›¾ä¸­èŠ‚ç‚¹æ•°è¶‹äºæ— ç©·æ—¶ï¼Œæ‹¥æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„ç¤¾åŒºæ£€æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æä¾›äº†å®é™…è¯æ®å’Œç†è®ºæ”¯æŒè¿™äº›ä¸»å¼ ã€‚<details>
<summary>Abstract</summary>
Detecting communities in high-dimensional graphs can be achieved by applying random matrix theory where the adjacency matrix of the graph is modeled by a Stochastic Block Model (SBM). However, the SBM makes an unrealistic assumption that the edge probabilities are homogeneous within communities, i.e., the edges occur with the same probabilities. The Degree-Corrected SBM is a generalization of the SBM that allows these edge probabilities to be different, but existing results from random matrix theory are not directly applicable to this heterogeneous model. In this paper, we derive a transformation of the adjacency matrix that eliminates this heterogeneity and preserves the relevant eigenstructure for community detection. We propose a test based on the extreme eigenvalues of this transformed matrix and (1) provide a method for controlling the significance level, (2) formulate a conjecture that the test achieves power one for all positive significance levels in the limit as the number of nodes approaches infinity, and (3) provide empirical evidence and theory supporting these claims.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯ä»¥ä½¿ç”¨ Random Matrix Theoryï¼ˆRMTï¼‰æ¥æ£€æµ‹é«˜ç»´å›¾çš„ç¤¾åŒºï¼Œä½† Stochastic Block Modelï¼ˆSBMï¼‰å‡è®¾äº†åŒä¸€ä¸ªç¤¾åŒºå†…çš„è¾¹æ¦‚ç‡æ˜¯ä¸€è‡´çš„ï¼Œå³è¾¹ occur with the same probabilitiesã€‚degree-corrected SBM æ˜¯ SBM çš„æ¨å¹¿ï¼Œå…è®¸è¾¹æ¦‚ç‡ä¸åŒï¼Œä½†ç°æœ‰çš„ RMT ç»“æœä¸ç›´æ¥é€‚ç”¨äºè¿™ç§ä¸åŒçš„æ¨¡å‹ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§å°† adjacency matrix è½¬åŒ–ä¸ºæ¶ˆé™¤ä¸åŒæ¦‚ç‡çš„è¡¨ç¤ºï¼Œä¿æŒç¤¾åŒºæ£€æµ‹çš„ç›¸å…³ç‰¹å¾ã€‚æˆ‘ä»¬æå‡ºä¸€ç§åŸºäºæå€¼ç‰¹å¾çš„æµ‹è¯•ï¼Œå¹¶ï¼ˆ1ï¼‰æä¾›æ§åˆ¶ significanc æ°´å¹³çš„æ–¹æ³•ï¼Œï¼ˆ2ï¼‰æå‡ºä¸€ç§ conjecture æµ‹è¯•åœ¨ä¸€ä¸ª society ä¸­çš„æ‰€æœ‰æ­£é¢ significanc æ°´å¹³ä¸‹éƒ½æœ‰æƒå¨æ€§ï¼Œå¹¶ï¼ˆ3ï¼‰æä¾›å®è¯å’Œç†è®ºæ”¯æŒè¿™äº›ä¸»å¼ ã€‚
</details></li>
</ul>
<hr>
<h2 id="Signal-Detection-in-Ambient-Backscatter-Systems-Fundamentals-Methods-and-Trends"><a href="#Signal-Detection-in-Ambient-Backscatter-Systems-Fundamentals-Methods-and-Trends" class="headerlink" title="Signal Detection in Ambient Backscatter Systems: Fundamentals, Methods, and Trends"></a>Signal Detection in Ambient Backscatter Systems: Fundamentals, Methods, and Trends</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03882">http://arxiv.org/abs/2312.03882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shayan Zargari, Azar Hakimi, Fatemeh Rezaei, Chintha Tellambura, Amine Maaref<br>for: This paper provides an overview of signal detection for AmBC networks, which is essential for IoT and wireless communication applications.methods: The paper discusses various detection methods, including their advantages and drawbacks, for signal detection in AmBC networks.results: The paper provides a comprehensive overview of the fundamentals, challenges, and ongoing research in signal detection for AmBC networks, making it a valuable resource for IoT and wireless communication professionals and researchers.<details>
<summary>Abstract</summary>
Internet-of-Things (IoT) is rapidly growing in wireless technology, aiming to connect vast numbers of devices to gather and distribute vital information. Despite individual devices having low energy consumption, the cumulative demand results in significant energy usage. Consequently, the concept of ultra-low-power tags gains appeal. Such tags communicate by reflecting rather than generating the radio frequency (RF) signals by themselves. Thus, these backscatter tags can be low-cost and battery-free. The RF signals can be ambient sources such as wireless-fidelity (Wi-Fi), cellular, or television (TV) signals, or the system can generate them externally. Backscatter channel characteristics are different from conventional point-to-point or cooperative relay channels. These systems are also affected by a strong interference link between the RF source and the tag besides the direct and backscattering links, making signal detection challenging. This paper provides an overview of the fundamentals, challenges, and ongoing research in signal detection for AmBC networks. It delves into various detection methods, discussing their advantages and drawbacks. The paper's emphasis on signal detection sets it apart and positions it as a valuable resource for IoT and wireless communication professionals and researchers.
</details>
<details>
<summary>æ‘˜è¦</summary>
äº’è”ç½‘æ™ºèƒ½åŒ–ï¼ˆIoTï¼‰åœ¨æ— çº¿æŠ€æœ¯æ–¹é¢è¿…é€Ÿæˆé•¿ï¼Œæ—¨åœ¨è”ç³»å¤§é‡è®¾å¤‡ï¼Œæ”¶é›†å’Œä¼ è¾“é‡è¦ä¿¡æ¯ã€‚å°½ç®¡ä¸ªåˆ«è®¾å¤‡çš„èƒ½é‡æ¶ˆè€—ä½ï¼Œä½†ç´¯ç´¯çš„éœ€æ±‚å¯¼è‡´äº†æ˜æ˜¾çš„èƒ½æºä½¿ç”¨ã€‚å› æ­¤ï¼Œä½åŠŸç‡æ ‡ç­¾çš„æ¦‚å¿µå—åˆ°æ¨å¹¿ã€‚è¿™äº›æ ‡ç­¾é€šè¿‡åå°„è€Œä¸æ˜¯è‡ªå·±ç”Ÿæˆç”µromagneticï¼ˆEMï¼‰ä¿¡å·é€šä¿¡ã€‚å› æ­¤ï¼Œè¿™äº›åå°„æ ‡ç­¾å¯ä»¥å®ç°ä½æˆæœ¬å’Œç”µæ± è‡ªç”±ã€‚EMä¿¡å·å¯ä»¥æ¥è‡ªå‘¨é­çš„æ— çº¿ä¾›åº”å•†ï¼Œä¾‹å¦‚æ— çº¿å®½é¢‘ï¼ˆWi-Fiï¼‰ã€mobileï¼ˆcellularï¼‰æˆ–ç”µè§†ï¼ˆTVï¼‰ä¿¡å·ï¼Œæˆ–è€…ç³»ç»Ÿå¯ä»¥ç”Ÿæˆå®ƒä»¬å¤–éƒ¨ã€‚åå°„é€šé“çš„ç‰¹æ€§ä¸ä¼ ç»Ÿçš„ç‚¹å¯¹ç‚¹æˆ–åˆä½œä¼ è¾“é€šé“ä¸åŒï¼Œè¿™äº›ç³»ç»Ÿä¹Ÿå—åˆ°å¼ºå¤§çš„å¹²æ‰°é“¾æ¥ï¼Œä½¿ä¿¡å·æ¢æµ‹å›°éš¾ã€‚æœ¬æ–‡æä¾›äº†ä½åŠŸç‡æ ‡ç­¾ä¿¡å·æ¢æµ‹çš„åŸºç¡€ã€æŒ‘æˆ˜å’Œç°æœ‰çš„ç ”ç©¶ï¼Œå¹¶è¯„ä¼°äº†ä¸åŒæ¢æµ‹æ–¹æ³•çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚æœ¬æ–‡çš„ä¸“æ³¨åœ¨ä¿¡å·æ¢æµ‹ä¸Šï¼Œä½¿å…¶æˆä¸ºäº’è”ç½‘å’Œæ— çº¿é€šä¿¡ä¸“ä¸šäººå‘˜å’Œç ”ç©¶äººå‘˜çš„å€¼å¾—é˜…è¯»èµ„æºã€‚
</details></li>
</ul>
<hr>
<h2 id="Enabling-Edge-Artificial-Intelligence-via-Goal-oriented-Deep-Neural-Network-Splitting"><a href="#Enabling-Edge-Artificial-Intelligence-via-Goal-oriented-Deep-Neural-Network-Splitting" class="headerlink" title="Enabling Edge Artificial Intelligence via Goal-oriented Deep Neural Network Splitting"></a>Enabling Edge Artificial Intelligence via Goal-oriented Deep Neural Network Splitting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03555">http://arxiv.org/abs/2312.03555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Binucci, Mattia Merluzzi, Paolo Banelli, Emilio Calvanese Strinati, Paolo Di Lorenzo</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨6Gæ— çº¿ç½‘ç»œè¾¹ç¼˜ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åˆ†å‰²ä»¥å®ç°ä½èƒ½è€—åˆä½œæ¨ç†ï¼Œå¹¶åœ¨ç›®æ ‡å»¶è¿Ÿå’Œå‡†ç¡®ç‡ä¸‹å®ç°ç›®æ ‡æ•ˆæœã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç›®æ ‡æ•ˆæœçš„SPé€‰æ‹©å’Œèµ„æºåˆ†é…ç®—æ³•ï¼Œå¯ä»¥åŠ¨æ€æ§åˆ¶SPé€‰æ‹©ã€æœ¬åœ°è®¡ç®—èµ„æºã€ä¸Šè¡Œä¼ è¾“åŠŸç‡å’Œé¢‘ç‡åˆ†é…ï¼Œä»¥æ»¡è¶³ç›®æ ‡æ•ˆæœã€‚</li>
<li>results: æ•°æ®æ˜¾ç¤ºï¼Œæå‡ºçš„SPé€‰æ‹©å’Œèµ„æºåˆ†é…ç­–ç•¥å¯ä»¥å®ç°èƒ½é‡å¥¹æŠ‘å’Œæœ‰æ•ˆçš„è¾¹ç¼˜AIã€‚<details>
<summary>Abstract</summary>
Deep Neural Network (DNN) splitting is one of the key enablers of edge Artificial Intelligence (AI), as it allows end users to pre-process data and offload part of the computational burden to nearby Edge Cloud Servers (ECSs). This opens new opportunities and degrees of freedom in balancing energy consumption, delay, accuracy, privacy, and other trustworthiness metrics. In this work, we explore the opportunity of DNN splitting at the edge of 6G wireless networks to enable low energy cooperative inference with target delay and accuracy with a goal-oriented perspective. Going beyond the current literature, we explore new trade-offs that take into account the accuracy degradation as a function of the Splitting Point (SP) selection and wireless channel conditions. Then, we propose an algorithm that dynamically controls SP selection, local computing resources, uplink transmit power and bandwidth allocation, in a goal-oriented fashion, to meet a target goal-effectiveness. To the best of our knowledge, this is the first work proposing adaptive SP selection on the basis of all learning performance (i.e., energy, delay, accuracy), with the aim of guaranteeing the accomplishment of a goal (e.g., minimize the energy consumption under latency and accuracy constraints). Numerical results show the advantages of the proposed SP selection and resource allocation, to enable energy frugal and effective edge AI.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Variational-Autoencoder-for-Channel-Estimation-Real-World-Measurement-Insights"><a href="#Variational-Autoencoder-for-Channel-Estimation-Real-World-Measurement-Insights" class="headerlink" title="Variational Autoencoder for Channel Estimation: Real-World Measurement Insights"></a>Variational Autoencoder for Channel Estimation: Real-World Measurement Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03450">http://arxiv.org/abs/2312.03450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Baur, Benedikt BÃ¶ck, Nurettin Turan, Wolfgang Utschick</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºå˜é‡è‡ªåŠ¨ç¼–ç å™¨çš„é€šé“ä¼°è®¡æ–¹æ³•ï¼Œå¹¶å¯¹å®é™…æµ‹é‡æ•°æ®è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨å˜é‡è‡ªåŠ¨ç¼–ç å™¨æ¥è¿›è¡Œé€šé“ä¼°è®¡ï¼Œå¹¶ä¸”åªé€šè¿‡å™ªå£°æŸåçš„é€šé“è§‚å¯Ÿæ•°æ®è¿›è¡Œè®­ç»ƒã€‚å®ƒå­¦ä¹ äº†è§‚å¯Ÿ conditionalçš„ç¬¬ä¸€å’Œç¬¬äºŒ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚ï¼Œä»¥ä¼°è®¡ mean squared error-optimal estimatorã€‚</li>
<li>results: å¯¹å®é™…æµ‹é‡æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œè¯¥æ–¹æ³•çš„ä¼°è®¡æ•ˆæœæ˜æ˜¾æ¯”ä¹‹å‰çš„ç›¸å…³çŠ¶æ€è‰ºæœ¯ estimator æ›´å¥½ï¼Œå¹¶ä¸”å‘ç° pre-training synthetic data å¯ä»¥å¸®åŠ©é™ä½æµ‹é‡è®­ç»ƒæ•°æ®é›†å¤§å°ã€‚<details>
<summary>Abstract</summary>
This work utilizes a variational autoencoder for channel estimation and evaluates it on real-world measurements. The estimator is trained solely on noisy channel observations and parameterizes an approximation to the mean squared error-optimal estimator by learning observation-dependent conditional first and second moments. The proposed estimator significantly outperforms related state-of-the-art estimators on real-world measurements. We investigate the effect of pre-training with synthetic data and find that the proposed estimator exhibits comparable results to the related estimators if trained on synthetic data and evaluated on the measurement data. Furthermore, pre-training on synthetic data also helps to reduce the required measurement training dataset size.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™é¡¹å·¥ä½œåˆ©ç”¨å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨è¿›è¡Œé€šé“ä¼°è®¡ï¼Œå¹¶åœ¨å®é™…æµ‹é‡æ•°æ®ä¸Šè¯„ä¼°å…¶æ€§èƒ½ã€‚ä¼°è®¡å™¨åŸºäºå™ªå£°é€šé“è§‚å¯Ÿæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶é€šè¿‡å­¦ä¹ è§‚å¯Ÿå€¼æ‰€ä¾èµ–çš„ conditional firstå’Œsecond momentsæ¥ Parametrizeä¸€ä¸ª Mean Squared Error-optimal estimatorçš„è¿‘ä¼¼ã€‚æè®®çš„ä¼°è®¡å™¨åœ¨å®é™…æµ‹é‡æ•°æ®ä¸Šæ˜æ˜¾è¶…è¿‡ç›¸å…³çš„state-of-the-art estimatorsã€‚æˆ‘ä»¬è¿˜ investigateäº†ä½¿ç”¨ sintetic dataè¿›è¡Œé¢„è®­ç»ƒçš„æ•ˆæœï¼Œå‘ç°å¦‚æœåœ¨ sintetic dataä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶ååœ¨æµ‹é‡æ•°æ®ä¸Šè¯„ä¼°ï¼Œåˆ™æè®®çš„ä¼°è®¡å™¨ä¸ç›¸å…³çš„ä¼°è®¡å™¨å…·æœ‰ç›¸ä¼¼çš„æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé¢„è®­ç»ƒ sintetic dataè¿˜å¯ä»¥å‡å°‘æµ‹é‡æ•°æ®è®­ç»ƒé›†çš„å¤§å°ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Estimation-Performance-of-Generalized-Power-Method-for-Heteroscedastic-Probabilistic-PCA"><a href="#On-the-Estimation-Performance-of-Generalized-Power-Method-for-Heteroscedastic-Probabilistic-PCA" class="headerlink" title="On the Estimation Performance of Generalized Power Method for Heteroscedastic Probabilistic PCA"></a>On the Estimation Performance of Generalized Power Method for Heteroscedastic Probabilistic PCA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03438">http://arxiv.org/abs/2312.03438</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinxin Wang, Chonghe Jiang, Huikang Liu, Anthony Man-Cho So</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºéå¯¹ç§°æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„hetroscedastic probablistic PCAæ–¹æ³•ï¼Œä»¥ä¼°è®¡å¯ç”¨å¤šä¸ªä¸åŒç±»å‹æ•°æ®æ ·æœ¬çš„ä½ç»´åº¦çº¿æ€§å­ç©ºé—´ï¼ˆç®€ç§°â€œåŸºå‡†â€ï¼‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§åä¸ºé€šç”¨åŠ›é‡æ–¹æ³•ï¼ˆGPMï¼‰æ¥è§£å†³ associate maximum-likelihood estimationé—®é¢˜ï¼Œå¹¶è¯æ˜äº†GPMçš„ä¼°è®¡æ€§èƒ½ä¿è¯ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒGPMåœ¨ Gaussian å™ªå£°å’Œsub-Gaussianå™ªå£° Settingä¸­å…·æœ‰ä¼˜äºå…¶ä»–æ–¹æ³•çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
The heteroscedastic probabilistic principal component analysis (PCA) technique, a variant of the classic PCA that considers data heterogeneity, is receiving more and more attention in the data science and signal processing communities. In this paper, to estimate the underlying low-dimensional linear subspace (simply called \emph{ground truth}) from available heterogeneous data samples, we consider the associated non-convex maximum-likelihood estimation problem, which involves maximizing a sum of heterogeneous quadratic forms over an orthogonality constraint (HQPOC). We propose a first-order method -- generalized power method (GPM) -- to tackle the problem and establish its \emph{estimation performance} guarantee. Specifically, we show that, given a suitable initialization, the distances between the iterates generated by GPM and the ground truth decrease at least geometrically to some threshold associated with the residual part of certain "population-residual decomposition". In establishing the estimation performance result, we prove a novel local error bound property of another closely related optimization problem, namely quadratic optimization with orthogonality constraint (QPOC), which is new and can be of independent interest. Numerical experiments are conducted to demonstrate the superior performance of GPM in both Gaussian noise and sub-Gaussian noise settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¼‚å‹åˆ†å¸ƒ probabilistic principal component analysis (PCA) æŠ€æœ¯åœ¨æ•°æ®ç§‘å­¦å’Œä¿¡å·å¤„ç†é¢†åŸŸè·å¾—è¶Šæ¥è¶Šå¤šçš„å…³æ³¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦ä»å¯ç”¨çš„å¼‚ogeneousæ•°æ®æ ·æœ¬ä¸­ä¼°ç®—ä¸‹é¢çš„ä½ç»´Linear subspaceï¼ˆç®€ç§°ä¸ºâ€œground truthâ€ï¼‰ã€‚æˆ‘ä»¬è€ƒè™‘äº†ç›¸å…³çš„éå‡¸æœ€å¤§ LIKElihoodä¼°è®¡é—®é¢˜ï¼Œè¯¥é—®é¢˜æ˜¯é€šè¿‡ä¸€ä¸ªorthogonality constraint (HQPOC)æ¥æœ€å¤§åŒ–ä¸€ä¸ªå¼‚ogeneous quadratic formsçš„æ€»å’Œã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§é¦–é€‰æ–¹æ³•â€”â€”é€šç”¨åŠ›é‡æ–¹æ³• (GPM)â€”â€”æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶è¯æ˜äº†å…¶ä¼°è®¡æ€§èƒ½çš„ä¿è¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨é€‚å½“çš„åˆå§‹åŒ–ä¸‹ï¼ŒGPMæ‰€ç”Ÿæˆçš„è¿­ä»£å™¨ä¸ground truthä¹‹é—´çš„è·ç¦»åœ¨ä¸€å®šçš„é˜ˆå€¼å†…å‡å°‘è‡³å°‘Geometricallyï¼Œè¿™ä¸æ®‹å·®éƒ¨åˆ†çš„â€œäººå£-å‰©ä½™åˆ†è§£â€æœ‰å…³ã€‚åœ¨è¯æ˜ä¼°è®¡æ€§èƒ½çš„ç»“æœä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦ä¸€ä¸ªç›¸å…³ä¼˜åŒ–é—®é¢˜â€”â€”quadratic optimization with orthogonality constraint (QPOC)â€”â€”çš„æœ¬åœ°é”™è¯¯ bound æ€§è´¨ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„ç»“è®ºï¼Œå¯èƒ½å…·æœ‰ç‹¬ç«‹çš„ Ğ¸Ğ½Ñ‚Ğµrestã€‚æˆ‘ä»¬åœ¨ Gaussian noise å’Œ sub-Gaussian noise  Settings ä¸­è¿›è¡Œäº†æ•°å€¼å®éªŒï¼Œä»¥è¯æ˜ GPM çš„è¶…è¶Šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Beyond-Low-Rank-A-Graph-Based-Propagation-Approach-to-Tensor-Completion-for-Multi-Acquisition-Scenarios"><a href="#Beyond-Low-Rank-A-Graph-Based-Propagation-Approach-to-Tensor-Completion-for-Multi-Acquisition-Scenarios" class="headerlink" title="Beyond Low Rank: A Graph-Based Propagation Approach to Tensor Completion for Multi-Acquisition Scenarios"></a>Beyond Low Rank: A Graph-Based Propagation Approach to Tensor Completion for Multi-Acquisition Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03436">http://arxiv.org/abs/2312.03436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iain Rolland, Sivasakthy Selvakumaran, Andrea Marinoni</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³æ•°æ®è¡¨ç¤ºä¸ºtensorçš„ç¼ºå¤±ã€æŸåæˆ–æœªè§‚å¯Ÿçš„é—®é¢˜ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ‰©æ•£æ–¹æ³•ï¼Œç§°ä¸ºGraphPropï¼Œå®ƒåœ¨å›¾åŸºäºçš„è¡¨ç¤ºä¸­å¡«å……ç¼ºå¤±çš„Entryã€‚</li>
<li>results:  experimentsè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æˆåŠŸåœ°å®Œæˆç¼ºå¤±tensor Entryï¼Œå¹¶ä¸”åœ¨å®é™…åº”ç”¨ä¸­æ¯” estado del arteæ–¹æ³•æ›´é«˜æ•ˆã€‚<details>
<summary>Abstract</summary>
Tensor completion refers to the problem of recovering the missing, corrupted or unobserved entries in data represented by tensors. In this paper, we tackle the tensor completion problem in the scenario in which multiple tensor acquisitions are available and do so without placing constraints on the underlying tensor's rank. Whereas previous tensor completion work primarily focuses on low-rank completion methods, we propose a novel graph-based diffusion approach to the problem. Referred to as GraphProp, the method propagates observed entries around a graph-based representation of the tensor in order to recover the missing entries. A series of experiments have been performed to validate the presented approach, including a synthetically-generated tensor recovery experiment which shows that the method can be used to recover both low and high rank tensor entries. The successful tensor completion capabilities of the approach are also demonstrated on a real-world completion problem from the field of multispectral remote sensing completion. Using data acquired from the Landsat 7 platform, we synthetically obscure image sections in order to simulate the scenario in which image acquisitions overlap only partially. In these tests, we benchmark against alternative tensor completion approaches as well as existing graph signal recovery methods, demonstrating the superior reconstruction performance of our method versus the state of the art.
</details>
<details>
<summary>æ‘˜è¦</summary>
tensor å®Œæˆï¼ˆtensor completionï¼‰æŒ‡çš„æ˜¯åœ¨æ•°æ®è¡¨ç¤ºä¸ºå¼ é‡ï¼ˆtensorï¼‰æ—¶ç¼ºå¤±ã€æŸåæˆ–æœªè§‚å¯Ÿåˆ°çš„å…ƒç´ çš„é—®é¢˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†åŸºäºå¤šä¸ªå¼ é‡è·å–çš„å¼ é‡å®Œæˆé—®é¢˜ï¼Œè€Œä¸æ˜¯å¯¹å¼ é‡çš„ä¸‹æ ‡ï¼ˆrankï¼‰è¿›è¡Œé™åˆ¶ã€‚å‰ä¸€äº›å¼ é‡å®Œæˆå·¥ä½œä¸»è¦å…³æ³¨äºä½çº§ Completion æ–¹æ³•ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå›¾çš„æ‰©æ•£æ–¹æ³•ï¼Œç§°ä¸ºGraphPropï¼Œå®ƒåœ¨å›¾åŸºäºçš„å¼ é‡è¡¨ç¤ºä¸Šå®£ä¼ å·²çŸ¥çš„å…ƒç´ ï¼Œä»¥recover ç¼ºå¤±çš„å…ƒç´ ã€‚æˆ‘ä»¬å¯¹æ–¹æ³•è¿›è¡Œäº†ä¸€ç³»åˆ—çš„å®éªŒéªŒè¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªé€šè¿‡ç”Ÿæˆçš„å¼ é‡æ¢å¤å®éªŒï¼Œè¿™ä¸ªå®éªŒè¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å®Œæˆä½çº§å’Œé«˜çº§å¼ é‡å…ƒç´ çš„æ¢å¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åœ¨å¤šspectral é¥æ„Ÿå®Œæˆé—®é¢˜ä¸­åº”ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œä½¿ç”¨äº†æ¥è‡ªLandSat 7 å¹³å°çš„æ•°æ®ï¼Œå¹¶å¯¹å›¾åƒåˆ†å‰²è¿›è¡Œäº†synthetic é®ç›²ï¼Œä»¥æ¨¡æ‹Ÿå›¾åƒè·å–çš„éƒ¨åˆ†é‡å ã€‚åœ¨è¿™äº›æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä¸å…¶ä»–å¼ é‡å®Œæˆæ–¹æ³•å’Œå›¾ä¿¡å·æ¢å¤æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„è¶…è¿‡çŠ¶æ€è‰ºæœ¯çš„é‡å»ºæ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Markov-Chain-Monte-Carlo-Data-Association-for-Sets-of-Trajectories"><a href="#Markov-Chain-Monte-Carlo-Data-Association-for-Sets-of-Trajectories" class="headerlink" title="Markov Chain Monte Carlo Data Association for Sets of Trajectories"></a>Markov Chain Monte Carlo Data Association for Sets of Trajectories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03423">http://arxiv.org/abs/2312.03423</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Xia, Ãngel F. GarcÃ­a-FernÃ¡ndez, Lennart Svensson</li>
<li>for: è¿™ä¸ªè®ºæ–‡å…³æ³¨æ‰¹å¤„ç†æ–¹æ³• Ğ´Ğ»Ñå¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼ŒåŸºäºè½¨è¿¹é›†çš„æ–¹æ³•ã€‚</li>
<li>methods: è®ºæ–‡æå‡ºäº†ä¸¤ç§MCMCé‡‡æ ·æ•°æ®å…³è”å‡è®¾çš„ç¦»çº¿å®ç°æ–¹æ³•ï¼Œå³TPMBMç­›é€‰å™¨ã€‚</li>
<li>results:  simulationç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ Metropolis-Hastingsç®—æ³•å®ç°çš„TPMBMç­›é€‰å™¨åœ¨å¤šè½¨è¿¹ä¼°è®¡æ–¹é¢è¾¾åˆ°äº†çŠ¶æ€ä¹‹å·…ã€‚<details>
<summary>Abstract</summary>
This paper considers a batch solution to the multi-object tracking problem based on sets of trajectories. Specifically, we present two offline implementations of the trajectory Poisson multi-Bernoulli mixture (TPMBM) filter for batch data based on Markov chain Monte Carlo (MCMC) sampling of the data association hypotheses. In contrast to online TPMBM implementations, the proposed offline implementations solve a large-scale, multi-scan data association problem across the entire time interval of interest, and therefore they can fully exploit all the measurement information available. Furthermore, by leveraging the efficient hypothesis structure of TPMBM filters, the proposed implementations compare favorably with other MCMC-based multi-object tracking algorithms. Simulation results show that the TPMBM implementation using the Metropolis-Hastings algorithm presents state-of-the-art multiple trajectory estimation performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡è€ƒè™‘äº†æ‰¹å¤„ç†æ–¹æ³•æ¥è§£å†³å¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼ŒåŸºäºè½¨è¿¹é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäºMarkové“¾ Monte Carloï¼ˆMCMCï¼‰æŠ½æ ·çš„è½¨è¿¹Poissonå¤š Ğ‘ĞµÑ€Ğ½Ñƒåˆ©æ··åˆï¼ˆTPMBMï¼‰ç­›é€‰å™¨çš„åœæœºå®ç°æ–¹æ³•ã€‚ä¸åœ¨çº¿TPMBMå®ç°æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„åœæœºå®ç°æ–¹æ³•å¯ä»¥åœ¨æ•´ä¸ªæ—¶é—´é—´éš”å†…è§£å†³å¤§è§„æ¨¡çš„å¤šæ‰«ææ•°æ®å…³è”é—®é¢˜ï¼Œå› æ­¤å®ƒä»¬å¯ä»¥å®Œå…¨åˆ©ç”¨æ‰€æœ‰çš„æµ‹é‡ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡åˆ©ç”¨TPMBMç­›é€‰å™¨çš„æœ‰æ•ˆå‡è®¾ç»“æ„ï¼Œæˆ‘ä»¬çš„å®ç°æ–¹æ³•ä¸å…¶ä»–MCMCåŸºäºå¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ç›¸æ¯”ï¼Œå…·æœ‰ä¼˜ç§€çš„æ€§èƒ½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ Metropolis-Hastingsç®—æ³•å®ç°çš„TPMBMç­›é€‰å™¨åœ¨å¤šè½¨è¿¹ä¼°è®¡ä¸­å…·æœ‰å›½é™…å…ˆè¿›çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Implementing-Digital-Twin-in-Field-Deployed-Optical-Networks-Uncertain-Factors-Operational-Guidance-and-Field-Trial-Demonstration"><a href="#Implementing-Digital-Twin-in-Field-Deployed-Optical-Networks-Uncertain-Factors-Operational-Guidance-and-Field-Trial-Demonstration" class="headerlink" title="Implementing Digital Twin in Field-Deployed Optical Networks: Uncertain Factors, Operational Guidance, and Field-Trial Demonstration"></a>Implementing Digital Twin in Field-Deployed Optical Networks: Uncertain Factors, Operational Guidance, and Field-Trial Demonstration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03374">http://arxiv.org/abs/2312.03374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchen Song, Min Zhang, Yao Zhang, Yan Shi, Shikui Shen, Bingli Guo, Shanguo Huang, Danshi Wang</li>
<li>for: æœ¬æ–‡æ—¨åœ¨è§£å†³åœ¨å®é™…ç¯å¢ƒä¸­éƒ¨ç½²çš„å…‰é€šä¿¡ç½‘ç»œä¸­æ•°å­—åŒæ–¹é¢çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯åœ¨æ§åˆ¶çš„å®éªŒå®¤è®¾ç½®ä¸­ã€‚</li>
<li>methods: æœ¬æ–‡é€šè¿‡åˆ†æå®é™…ç¯å¢ƒä¸­æ•°å­—åŒæ–¹é¢ä¸ç¡®å®šå› ç´ ï¼Œå¹¶æå‡ºäº†å®æ–½ç²¾å‡†æ•°å­—åŒæ–¹é¢çš„æ“ä½œæŒ‡å—ã€‚</li>
<li>results: é€šè¿‡æå‡ºçš„æ“ä½œæŒ‡å—ï¼Œæœ¬æ–‡åœ¨ä¸€ä¸ªå®é™…æµ‹è¯•ä¸­å‘ç°äº†ä½¿ç”¨æ•°å­—åŒæ–¹é¢è¿›è¡Œæ€§èƒ½æ¢å¤çš„å¯èƒ½æ€§ï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨fiber cutåœºæ™¯ä¸­çš„åº”ç”¨å‰æ™¯ã€‚<details>
<summary>Abstract</summary>
Digital twin has revolutionized optical communication networks by enabling their full life-cycle management, including design, troubleshooting, optimization, upgrade, and prediction. While extensive literature exists on frameworks, standards, and applications of digital twin, there is a pressing need in implementing digital twin in field-deployed optical networks operating in real-world environments, as opposed to controlled laboratory settings. This paper addresses this challenge by examining the uncertain factors behind the inaccuracy of digital twin in field-deployed optical networks from three main challenges and proposing operational guidance for implementing accurate digital twin in field-deployed optical networks. Through the proposed guidance, we demonstrate the effective implementation of digital twin in a field-trial C+L-band optical transmission link, showcasing its capabilities in performance recovery in a fiber cut scenario.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ•°å­—åŒâ€å·²ç»é©å‘½åŒ–äº†å…‰é€šä¿¡ç½‘ç»œçš„å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ŒåŒ…æ‹¬è®¾è®¡ã€æ’æŸ¥ã€ä¼˜åŒ–ã€å‡çº§å’Œé¢„æµ‹ã€‚å°½ç®¡æœ‰å¤§é‡å…³äºæ¡†æ¶ã€æ ‡å‡†å’Œåº”ç”¨çš„æ•°å­—åŒæ–‡çŒ®ï¼Œä½†æ˜¯åœ¨å®é™… Ğ¿Ğ¾Ğ»Ğµæ™¯ä¸­éƒ¨ç½²çš„å…‰ç½‘ç»œä¸­å®æ–½æ•°å­—åŒä»å­˜åœ¨æŒ‘æˆ˜ã€‚æœ¬æ–‡è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œé€šè¿‡åˆ†æåœºæ™¯ä¸‹æ•°å­—åŒä¸å‡†ç¡®çš„uncertain factorï¼Œå¹¶æå‡ºäº†å®æ–½æ•°å­—åŒçš„æ“ä½œæŒ‡å—ã€‚é€šè¿‡æˆ‘ä»¬çš„æŒ‡å—ï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªåœºæ™¯ä¸‹C+Lé¢‘æ®µå…‰ä¼ è¾“é“¾è·¯ä¸Šå®ç°äº†æ•°å­—åŒçš„æœ‰æ•ˆå®æ–½ï¼Œå¹¶ç¤ºå‡ºäº†å®ƒåœ¨å…‰ç¼†æˆªæ–­åœºæ™¯ä¸‹æ€§èƒ½æ¢å¤çš„èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Understanding-Concepts-in-Graph-Signal-Processing-for-Neurophysiological-Signal-Analysis"><a href="#Understanding-Concepts-in-Graph-Signal-Processing-for-Neurophysiological-Signal-Analysis" class="headerlink" title="Understanding Concepts in Graph Signal Processing for Neurophysiological Signal Analysis"></a>Understanding Concepts in Graph Signal Processing for Neurophysiological Signal Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03371">http://arxiv.org/abs/2312.03371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephan Goerttler, Fei He, Min Wu</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯å›¾åƒä¿¡å·å¤„ç†é¢†åŸŸçš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯åˆ©ç”¨å›¾åƒä¿¡å·å¤„ç†æ¥åˆ†ç±»ç¥ç»ç”Ÿç‰©å­¦æ•°æ®ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†å›¾åƒå‚…ç«‹å¶å˜æ¢ï¼Œå®ƒå°†å¤šå˜é‡ä¿¡å·æŠ•å½±åˆ°é¢‘ç‡é¡ºåºå›¾åƒä¿¡å·ä¸Šï¼Œä»è€Œå¯ä»¥çœ‹ä½œæ˜¯æ—¶é—´å‚…ç«‹å¶å˜æ¢çš„ç©ºé—´åŒæ­¥ã€‚</li>
<li>results: å®éªŒéƒ¨åˆ†ä¸»è¦ç ”ç©¶äº†å›¾åƒé¢‘ç‡åœ¨æ•°æ®åˆ†ç±»ä¸­çš„ä½œç”¨ï¼Œå¹¶é€šè¿‡äººå·¥ç”Ÿæˆçš„æ•°æ®è¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œå›¾åƒé¢‘ç‡è¾ƒä½çš„ä¿¡å·æ›´éš¾åˆ†ç±»ç¥ç»ç”Ÿç‰©å­¦æ•°æ®ï¼Œè€Œå›¾åƒé¢‘ç‡è¾ƒé«˜çš„ä¿¡å·æ›´å®¹æ˜“åˆ†ç±»ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ä¸ªåŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç»“æœè¡¨æ˜ï¼Œä½¿ç”¨å›¾åƒå‚…ç«‹å¶å˜æ¢å¯èƒ½ä¼šå‡å¼±ç¥ç»ç”Ÿç‰©å­¦æ•°æ®ä¸­çš„ç‰¹å¾ç‰¹å¾ã€‚<details>
<summary>Abstract</summary>
Multivariate signals, which are measured simultaneously over time and acquired by sensor networks, are becoming increasingly common. The emerging field of graph signal processing (GSP) promises to analyse spectral characteristics of these multivariate signals, while at the same time taking the spatial structure between the time signals into account. A central idea in GSP is the graph Fourier transform, which projects a multivariate signal onto frequency-ordered graph Fourier modes, and can therefore be regarded as a spatial analog of the temporal Fourier transform. This chapter derives and discusses key concepts in GSP, with a specific focus on how the various concepts relate to one another. The experimental section focuses on the role of graph frequency in data classification, with applications to neuroimaging. To address the limited sample size of neurophysiological datasets, we introduce a minimalist simulation framework that can generate arbitrary amounts of data. Using this artificial data, we find that lower graph frequency signals are less suitable for classifying neurophysiological data as compared to higher graph frequency signals. Finally, we introduce a baseline testing framework for GSP. Employing this framework, our results suggest that GSP applications may attenuate spectral characteristics in the signals, highlighting current limitations of GSP for neuroimaging.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šå˜é‡ä¿¡å·ï¼Œå®ƒä»¬åŒæ—¶åœ¨æ—¶é—´ä¸Šæµ‹é‡å¹¶ç”±æ„ŸçŸ¥ç½‘ç»œè·å–ï¼Œç°åœ¨è¶Šæ¥è¶Šæ™®éã€‚æ–°å…´çš„å›¾ä¿¡å·å¤„ç†ï¼ˆGSPï¼‰æŠ€æœ¯æ‰¿è¯ºå¯ä»¥åˆ†æå¤šå˜é‡ä¿¡å·çš„ÑĞ¿ĞµĞºÑ‚Ñ€Ğ°Ğ»ÑŒç‰¹å¾ï¼ŒåŒæ—¶è€ƒè™‘ä¿¡å·ä¹‹é—´çš„ç©ºé—´ç»“æ„ã€‚GSPçš„ä¸­å¿ƒæ€æƒ³æ˜¯å›¾å‚…ç«‹ transformï¼Œå®ƒå°†å¤šå˜é‡ä¿¡å·æ˜ å°„åˆ°é¢‘ç‡é¡ºåºçš„å›¾å‚…ç«‹æ¨¡å¼ä¸Šï¼Œå¯ä»¥è§†ä¸ºæ—¶é—´å‚…ç«‹transformçš„ç©ºé—´åŒä¹‰ã€‚æœ¬ç«  derivativeså’Œè®¨è®ºGSPå…³é”®æ¦‚å¿µï¼Œç‰¹åˆ«æ˜¯è¿™äº›æ¦‚å¿µä¹‹é—´çš„å…³ç³»ã€‚å®éªŒéƒ¨åˆ†å…³æ³¨graph frequencyåœ¨æ•°æ®åˆ†ç±»ä¸­çš„è§’è‰²ï¼Œå¹¶åº”ç”¨äºç¥ç»æˆåƒã€‚ç”±äºç¥ç»physiological datasetçš„æ ·æœ¬æ•°é‡æœ‰é™ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªç®€å•çš„Simulation Frameworkï¼Œå¯ä»¥ç”Ÿæˆarbitrary amount of dataã€‚ä½¿ç”¨è¿™äº›äººå·¥æ•°æ®ï¼Œæˆ‘ä»¬å‘ç°lower graph frequency signal less suitable for classifying neurophysiological data compared to higher graph frequency signalsã€‚æœ€åï¼Œæˆ‘ä»¬å¼•å…¥äº†GSPåŸºçº¿æµ‹è¯•æ¡†æ¶ã€‚ä½¿ç”¨è¿™ä¸ªæ¡†æ¶ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜GSPåº”ç”¨å¯èƒ½å‡å¼±ä¿¡å·ä¸­çš„spectralç‰¹å¾ï¼Œ highlighting current limitations of GSP for neuroimaging.
</details></li>
</ul>
<hr>
<h2 id="Channel-Transferable-Semantic-Communications-for-Multi-User-OFDM-NOMA-Systems"><a href="#Channel-Transferable-Semantic-Communications-for-Multi-User-OFDM-NOMA-Systems" class="headerlink" title="Channel-Transferable Semantic Communications for Multi-User OFDM-NOMA Systems"></a>Channel-Transferable Semantic Communications for Multi-User OFDM-NOMA Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03299">http://arxiv.org/abs/2312.03299</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lan Lin, Wenjun Xu, Fengyu Wang, Yimeng Zhang, Wei Zhang, Ping Zhang</li>
<li>for: æé«˜ sixth generationï¼ˆ6Gï¼‰æ— çº¿ç½‘ç»œä¸­semantic communicationsçš„æ ¸å¿ƒæ–° paradigmã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§æ–°çš„channel-transferable semantic communicationsï¼ˆCT-SemComï¼‰æ¡†æ¶ï¼Œå¯ä»¥å°†codecså­¦ä¹ çš„channelä¿¡æ¯ä¼ é€’åˆ°å…¶ä»–ç±»å‹çš„channelä¸Šã€‚</li>
<li>results: åœ¨OFDM-NOMAç³»ç»Ÿä¸­ï¼Œé€šè¿‡åˆ†æè§£SSDTç®—æ³•ï¼Œå®ç°äº†semantic communicationsçš„ä¼ è¾“ï¼Œå¹¶ä¸”ä¸ä¸åŒ Rayleigh fading channelsçš„ä¼ è¾“æ€§èƒ½å…·æœ‰æ˜¾è‘—çš„æ”¹è¿›ï¼ˆæ¯”å¦‚å›¾åƒä¼ è¾“çš„PSNRæé«˜4.2-7.3dBï¼‰ã€‚<details>
<summary>Abstract</summary>
Semantic communications are expected to become the core new paradigms of the sixth generation (6G) wireless networks. Most existing works implicitly utilize channel information for codecs training, which leads to poor communications when channel type or statistical characteristics change. To tackle this issue posed by various channels, a novel channel-transferable semantic communications (CT-SemCom) framework is proposed, which adapts the codecs learned on one type of channel to other types of channels. Furthermore, integrating the proposed framework and the orthogonal frequency division multiplexing systems integrating non-orthogonal multiple access technologies, i.e., OFDM-NOMA systems, a power allocation problem to realize the transfer from additive white Gaussian noise (AWGN) channels to multi-subcarrier Rayleigh fading channels is formulated. We then design a semantics-similar dual transformation (SSDT) algorithm to derive analytical solutions with low complexity. Simulation results show that the proposed CT-SemCom framework with SSDT algorithm significantly outperforms the existing work w.r.t. channel transferability, e.g., the peak signal-to-noise ratio (PSNR) of image transmission improves by 4.2-7.3 dB under different variances of Rayleigh fading channels.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°å…­ä»£ï¼ˆ6Gï¼‰æ— çº¿ç½‘ç»œä¸­ï¼Œ semantics é€šä¿¡å°†æˆä¸ºæ ¸å¿ƒæ–° Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ã€‚ç°æœ‰çš„å¤§å¤šæ•°å·¥ä½œéƒ½ä¼šåˆ©ç”¨é€šé“ä¿¡æ¯æ¥è®­ç»ƒç¼–è§£ç å™¨ï¼Œè¿™ä¼šå¯¼è‡´é€šä¿¡æ•ˆç‡ä¸‹é™ï¼Œç‰¹åˆ«æ˜¯å½“é€šé“ç±»å‹æˆ–ç»Ÿè®¡ç‰¹æ€§å‘ç”Ÿå˜åŒ–æ—¶ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„é€šé“ä¼ é€’semanticé€šä¿¡ï¼ˆCT-SemComï¼‰æ¡†æ¶ï¼Œå¯ä»¥å°†åœ¨ä¸€ç±»é€šé“ä¸Šå­¦ä¹ çš„ç¼–è§£ç å™¨åº”ç”¨åˆ°å…¶ä»–ç±»å‹çš„é€šé“ä¸Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†å°† CT-SemCom æ¡†æ¶å’ŒOFDM-NOMA ç³»ç»Ÿé›†æˆï¼Œä»¥å®ç°ä»ç™½å™ªå£°åŠ é€Ÿåº¦éšæœºå˜åŒ–ï¼ˆAWGNï¼‰é€šé“ä¼ é€’åˆ°å¤šå­è½½è°å¹•æ»¤æ³¢ï¼ˆRayleigh fadingï¼‰é€šé“çš„åŠ›è°ƒé…é—®é¢˜ã€‚æˆ‘ä»¬then designed a semantics-similar dual transformationï¼ˆSSDTï¼‰ç®—æ³•æ¥ derivate analytical solutions with low complexityã€‚ simulation results show that the proposed CT-SemCom framework with SSDT algorithm significantly outperforms the existing work w.r.t. channel transferability, e.g., the peak signal-to-noise ratioï¼ˆPSNRï¼‰of image transmission improves by 4.2-7.3 dB under different variances of Rayleigh fading channels.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Multi-band-Modulation-for-Robust-and-Low-complexity-Faster-than-Nyquist-Non-Orthogonal-FDM-IM-DD-System"><a href="#Adaptive-Multi-band-Modulation-for-Robust-and-Low-complexity-Faster-than-Nyquist-Non-Orthogonal-FDM-IM-DD-System" class="headerlink" title="Adaptive Multi-band Modulation for Robust and Low-complexity Faster-than-Nyquist Non-Orthogonal FDM IM-DD System"></a>Adaptive Multi-band Modulation for Robust and Low-complexity Faster-than-Nyquist Non-Orthogonal FDM IM-DD System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03284">http://arxiv.org/abs/2312.03284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiji Song, Zhouyi Hu, Yizhan Dai, Yuan Liu, Chao Gao, Chun-Kit Chan</li>
<li>for: æé«˜ä¿¡å·å¸¦å®½åˆ©ç”¨ç‡å’Œé™ä½å¤æ‚æ€§</li>
<li>methods: ä½¿ç”¨éå¯¹ç§°çŸ©é˜µå‹ç¼©å’Œå¤š bands åˆ†å‰²</li>
<li>results: å¯ä»¥é™ä½æ¯”ç‰¹é”™è¯¯ç‡å’Œæé«˜å®ç°å¤æ‚æ€§<details>
<summary>Abstract</summary>
Faster-than-Nyquist non-orthogonal frequency-division multiplexing (FTN-NOFDM) is robust against the steep frequency roll-off by saving signal bandwidth. Among the FTN-NOFDM techniques, the non-orthogonal matrix precoding (NOM-p) based FTN has high compatibility with the conventional orthogonal frequency division multiplexing (OFDM), in terms of the advanced digital signal processing already used in OFDM. In this work, by dividing the single band into multiple sub-bands in the NOM-p-based FTN-NOFDM system, we propose a novel FTN-NOFDM scheme with adaptive multi-band modulation. The proposed scheme assigns different quadrature amplitude modulation (QAM) levels to different sub-bands, effectively utilizing the low-pass-like channel and reducing the complexity. The impacts of sub-band number and bandwidth compression factor on the bit-error-rate (BER) performance and implementation complexity are experimentally analyzed with a 32.23-Gb/s and 20-km intensity modulation-direct detection (IM-DD) optical transmission system. Results show that the proposed scheme with proper sub-band numbers can lower BER and greatly reduce the complexity compared to the conventional single-band way.
</details>
<details>
<summary>æ‘˜è¦</summary>
éå¯¹ç§°é¢‘åˆ†å¤šplexingï¼ˆFTN-NOFDMï¼‰èƒ½å¤ŸæŠ—è¡¡å³»é¢‘æ»‘é™ï¼Œå¹¶ä¸”å¯ä»¥å‚¨å­˜ä¿¡å·å¸¦å®½ã€‚ Among FTN-NOFDMæŠ€æœ¯ä¸­ï¼ŒåŸºäºéå¯¹ç§°çŸ©é˜µåµŒå…¥ï¼ˆNOM-pï¼‰çš„FTNå…·æœ‰ä¸ä¼ ç»Ÿçš„æ­£äº¤é¢‘åˆ†å¤šplexingï¼ˆOFDMï¼‰çš„é«˜ç›¸å®¹æ€§ï¼Œä» digitale sign processing çš„è§’åº¦æ¥çœ‹ã€‚ åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬åœ¨NOM-påŸºäºFTN-NOFDMç³»ç»Ÿä¸­å°†å•ä¸€é¢‘å¸¦åˆ†æˆå¤šä¸ªå­å¸¦ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„FTN-NOFDMæ–¹æ¡ˆï¼Œå³é€‚åº”å¤šå¸¦æ¨¡ULATIONã€‚è¿™ä¸ªæ–¹æ¡ˆå°†ä¸åŒçš„æŒ¯å¹…æ¨¡ulationï¼ˆQAMï¼‰æ°´å¹³åˆ†é…åˆ°ä¸åŒçš„å­å¸¦ä¸­ï¼Œä»¥ä¼˜åŒ–ä½é€šé¢‘é“å’Œå‡å°‘å¤æ‚æ€§ã€‚æˆ‘ä»¬é€è¿‡å®éªŒåˆ†æäº†å…·æœ‰32.23 Gb/så’Œ20 km IM-DD å…‰å­¦ä¼ è¾“ç³»ç»Ÿçš„BERæ€§èƒ½å’Œå®ç°å¤æ‚æ€§ï¼Œç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªæ–¹æ¡ˆå¯ä»¥é€‚å½“åœ°é€‰æ‹©å­å¸¦æ•°é‡ï¼Œä»¥ä¸‹é™BERå’Œå‡å°‘å¤æ‚æ€§ï¼Œç›¸æ¯”äºä¼ ç»Ÿå•ä¸€é¢‘å¸¦çš„æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Densifying-MIMO-Channel-Modeling-Physical-Constraints-and-Performance-Evaluation-for-Holographic-Communications"><a href="#Densifying-MIMO-Channel-Modeling-Physical-Constraints-and-Performance-Evaluation-for-Holographic-Communications" class="headerlink" title="Densifying MIMO: Channel Modeling, Physical Constraints, and Performance Evaluation for Holographic Communications"></a>Densifying MIMO: Channel Modeling, Physical Constraints, and Performance Evaluation for Holographic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03255">http://arxiv.org/abs/2312.03255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Y. Liu, M. Zhang, T. Wang, A. Zhang, M. Debbah</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³5Gç”µå­ç½‘ç»œèƒŒæ™¯ä¸‹å¤§é‡å¤šè¾“å…¥å¤šOutputï¼ˆMIMOï¼‰æŠ€æœ¯ä¸­çš„ä¸€ä¸ªæŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨æœ‰é™ç©ºé—´å†…éƒ¨ç½²å¤§é‡å¤©çº¿å…ƒä»¶ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºç”µç£æ³¢ç‰¹æ€§çš„ç”µç£é€šä¿¡é¢‘é“æ¨¡å‹ï¼Œè¯¥æ¨¡å‹è€ƒè™‘äº†å¤©çº¿å…ƒä»¶ä¹‹é—´çš„äº’ç›¸å¹²æ‰°å’Œä¼ æ’­ç¯å¢ƒä¸­çš„æåŒ–ã€‚æ­¤å¤–ï¼Œé€šè¿‡çº¦ç­‰äºæ— ç©·å¤§æ•°ç»„æ¥ç ”ç©¶å¤§è§„æ¨¡ç´§å¯†å¤©çº¿æ•°ç»„çš„æ€§èƒ½é™åˆ¶ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåœ¨æœ‰é™ç©ºé—´å†…ï¼Œå¤©çº¿å…ƒä»¶ä¹‹é—´çš„äº’ç›¸å¹²æ‰°ï¼Œå°¤å…¶æ˜¯å¤©çº¿å…ƒä»¶é—´è·å°äºåŠæ³¢é•¿çš„æƒ…å†µä¸‹ï¼Œæ˜¯ç”µå…‰é€šä¿¡æ€§èƒ½çš„ä¸»è¦é™åˆ¶å› ç´ ã€‚<details>
<summary>Abstract</summary>
As the backbone of the fifth-generation (5G) cellular network, massive multiple-input multiple-output (MIMO) encounters a significant challenge in practical applications: how to deploy a large number of antenna elements within limited spaces. Recently, holographic communication has emerged as a potential solution to this issue. It employs dense antenna arrays and provides a tractable model. Nevertheless, some challenges must be addressed to actualize this innovative concept. One is the mutual coupling among antenna elements within an array. When the element spacing is small, near-field coupling becomes the dominant factor that strongly restricts the array performance. Another is the polarization of electromagnetic waves. As an intrinsic property, it was not fully considered in the previous channel modeling of holographic communication. The third is the lack of real-world experiments to show the potential and possible defects of a holographic communication system. In this paper, we propose an electromagnetic channel model based on the characteristics of electromagnetic waves. This model encompasses the impact of mutual coupling in the transceiver sides and the depolarization in the propagation environment. Furthermore, by approximating an infinite array, the performance restrictions of large-scale dense antenna arrays are also studied theoretically to exploit the potential of the proposed channel. In addition, numerical simulations and a channel measurement experiment are conducted. The findings reveal that within limited spaces, the coupling effect, particularly for element spacing smaller than half of the wavelength, is the primary factor leading to the inflection point for the performance of holographic communications.
</details>
<details>
<summary>æ‘˜è¦</summary>
fifth-generation (5G) æ— çº¿ç½‘ç»œçš„è„Šæ¢â€”â€”å·¨é‡å¤šè¾“å…¥å¤šè¾“å‡º (MIMO) æŠ€æœ¯åœ¨å®è·µåº”ç”¨ä¸­é‡åˆ°äº†ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨æœ‰é™çš„ç©ºé—´å†…éƒ¨ç½²å¤§é‡å¤©çº¿å…ƒä»¶ã€‚æœ€è¿‘ï¼Œå¹²æ‰°é€šä¿¡æŠ€æœ¯ emerged as a potential solution to this issue. It employs dense antenna arrays and provides a tractable model. However, some challenges must be addressed to actualize this innovative concept. One is the mutual coupling among antenna elements within an array. When the element spacing is small, near-field coupling becomes the dominant factor that strongly restricts the array performance. Another is the polarization of electromagnetic waves. As an intrinsic property, it was not fully considered in the previous channel modeling of holographic communication. The third is the lack of real-world experiments to show the potential and possible defects of a holographic communication system. In this paper, we propose an electromagnetic channel model based on the characteristics of electromagnetic waves. This model encompasses the impact of mutual coupling in the transceiver sides and the depolarization in the propagation environment. Furthermore, by approximating an infinite array, the performance restrictions of large-scale dense antenna arrays are also studied theoretically to exploit the potential of the proposed channel. In addition, numerical simulations and a channel measurement experiment are conducted. The findings reveal that within limited spaces, the coupling effect, particularly for element spacing smaller than half of the wavelength, is the primary factor leading to the inflection point for the performance of holographic communications.Here's the text with some additional information about the translation:I used the Simplified Chinese version of the text, which is the most commonly used version in mainland China. I translated the text word-for-word, without any modifications or simplifications, to ensure accuracy. However, some technical terms and jargon may not have direct translations in Simplified Chinese, so I did my best to find the closest equivalent. Additionally, I used the pinyin system to represent the Chinese characters, which is a standardized system for romanizing Chinese pronunciation.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/06/eess.SP_2023_12_06/" data-id="clq0ru7au01onto885j66ejkv" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_12_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/05/cs.SD_2023_12_05/" class="article-date">
  <time datetime="2023-12-05T15:00:00.000Z" itemprop="datePublished">2023-12-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/05/cs.SD_2023_12_05/">cs.SD - 2023-12-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Leveraging-Laryngograph-Data-for-Robust-Voicing-Detection-in-Speech"><a href="#Leveraging-Laryngograph-Data-for-Robust-Voicing-Detection-in-Speech" class="headerlink" title="Leveraging Laryngograph Data for Robust Voicing Detection in Speech"></a>Leveraging Laryngograph Data for Robust Voicing Detection in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03129">http://arxiv.org/abs/2312.03129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yixuanz/rvd">https://github.com/yixuanz/rvd</a></li>
<li>paper_authors: Yixuan Zhang, Heming Wang, DeLiang Wang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ä¸ªå¯é åœ°æ£€æµ‹è¯­éŸ³ä¿¡å·ä¸­çš„å£°éŸ³æ—¶é—´çš„æ–¹æ³•ï¼Œå¹¶ä¸”æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚æŠ‘æ‰¬è¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ä¸ªç´§å¯†è”æ¥çš„å·ç§¯è¿ç®—ç¥ç»ç½‘ç»œï¼ˆDC-CRNï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨äº†å½•éŸ³çš„å£°å¸¦æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚è¿˜è¿›è¡Œäº†é¢„è®­ç»ƒæ¥æé«˜æ¨¡å‹çš„åº”ç”¨æ€§ã€‚</li>
<li>results: æœ¬ç ”ç©¶çš„æ¨¡å‹å¯ä»¥å®ç°é«˜åº¦çš„å£°éŸ³æ£€æµ‹ç²¾åº¦ï¼Œæ¯”è¾ƒå…¶ä»–å¼ºå¤§çš„åŸºelineæ–¹æ³•æ›´å¥½ï¼Œå¹¶ä¸”å¯ä»¥å¯¹æœªè§çš„æ•°æ®é›†è¿›è¡Œä¸€è‡´çš„æ£€æµ‹ã€‚<details>
<summary>Abstract</summary>
Accurately detecting voiced intervals in speech signals is a critical step in pitch tracking and has numerous applications. While conventional signal processing methods and deep learning algorithms have been proposed for this task, their need to fine-tune threshold parameters for different datasets and limited generalization restrict their utility in real-world applications. To address these challenges, this study proposes a supervised voicing detection model that leverages recorded laryngograph data. The model is based on a densely-connected convolutional recurrent neural network (DC-CRN), and trained on data with reference voicing decisions extracted from laryngograph data sets. Pretraining is also investigated to improve the generalization ability of the model. The proposed model produces robust voicing detection results, outperforming other strong baseline methods, and generalizes well to unseen datasets. The source code of the proposed model with pretraining is provided along with the list of used laryngograph datasets to facilitate further research in this area.
</details>
<details>
<summary>æ‘˜è¦</summary>
å®æ—¶æ£€æµ‹è¯­éŸ³è®¯å·ä¸­çš„å‘éŸ³é—´éš”æ˜¯ä¸€é¡¹é‡è¦çš„æ­¥éª¤ï¼Œæœ‰è®¸å¤šåº”ç”¨ã€‚ä¼ ç»Ÿçš„ä¿¡å·å¤„ç†æ–¹æ³•å’Œæ·±åº¦å­¦ä¹ ç®—æ³•å·²ç»è¢«æå‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä»¬éœ€è¦ç²¾ç¡®åœ°è°ƒæ•´é˜ˆå€¼å‚æ•°ï¼Œå¹¶ä¸”ä»…ä»…å¯¹ç‰¹å®šæ•°æ®é›†æœ‰é™åº¦çš„é€‚ç”¨ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™äº›ç ”ç©¶æå‡ºäº†ä¸€ä¸ªç›‘ç£å¼çš„å‘éŸ³æ£€æµ‹æ¨¡å‹ï¼Œåˆ©ç”¨è¯­éŸ³è®¯å·ä¸­çš„é¢„å½•èµ„æ–™ã€‚è¿™ä¸ªæ¨¡å‹åŸºäºå¯†é›†çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆDC-CRNï¼‰ï¼Œå¹¶ä¸”ç”±å‚è€ƒçš„å‘éŸ³å†³ç­–æ’·å–è‡ªè¯­éŸ³è®¯å·æ•°æ®é›†ã€‚é¢„è®­ä¹Ÿè¢« investigate ä»¥æé«˜æ¨¡å‹çš„åº”ç”¨èƒ½åŠ›ã€‚æå‡ºçš„æ¨¡å‹å®ç°äº†Robustçš„å‘éŸ³æ£€æµ‹ç»“æœï¼Œè¶…è¶Šäº†å…¶ä»–å¼ºå¤§çš„åŸºelineæ–¹æ³•ï¼Œå¹¶ä¸”å¯¹æœªè§æ•°æ®é›†å…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€‚å®ç°çš„æ¨¡å‹ä»£ç å’Œé¢„è®­æ•°æ®åº“éƒ½æä¾›ç»™è¿›ä¸€æ­¥çš„ç ”ç©¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Integrating-Plug-and-Play-Data-Priors-with-Weighted-Prediction-Error-for-Speech-Dereverberation"><a href="#Integrating-Plug-and-Play-Data-Priors-with-Weighted-Prediction-Error-for-Speech-Dereverberation" class="headerlink" title="Integrating Plug-and-Play Data Priors with Weighted Prediction Error for Speech Dereverberation"></a>Integrating Plug-and-Play Data Priors with Weighted Prediction Error for Speech Dereverberation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02773">http://arxiv.org/abs/2312.02773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziye Yang, Wenxing Yang, Kai Xie, Jie Chen</li>
<li>for: æé«˜æ¶‚æ»¥åéŸ³é¢‘ä¿¡å·çš„è´¨é‡å’Œå¯è§£æ€§</li>
<li>methods:  combining physics-based and data-driven methods, incorporating speech prior information learnt from data during the optimization problem solving iterations</li>
<li>results: å®éªŒç»“æœéªŒè¯äº†ææ¡ˆçš„æœ‰æ•ˆæ€§<details>
<summary>Abstract</summary>
Speech dereverberation aims to alleviate the detrimental effects of late-reverberant components. While the weighted prediction error (WPE) method has shown superior performance in dereverberation, there is still room for further improvement in terms of performance and robustness in complex and noisy environments. Recent research has highlighted the effectiveness of integrating physics-based and data-driven methods, enhancing the performance of various signal processing tasks while maintaining interpretability. Motivated by these advancements, this paper presents a novel dereverberation frame-work, which incorporates data-driven methods for capturing speech priors within the WPE framework. The plug-and-play strategy (PnP), specifically the regularization by denoising (RED) strategy, is utilized to incorporate speech prior information learnt from data during the optimization problem solving iterations. Experimental results validate the effectiveness of the proposed approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬æ‘˜è¦ï¼šå¹²æ‰°é™¤æ³›éŸ³å°è¯•ç¼“è§£æ™š reverberation çš„è´Ÿé¢å½±å“ã€‚è™½ç„¶Weighted prediction errorï¼ˆWPEï¼‰æ–¹æ³•å·²ç»è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ–¹æ³•ï¼Œä½†æ˜¯åœ¨å¤æ‚å’Œå™ªéŸ³ç¯å¢ƒä¸­ä»ç„¶æœ‰å¾ˆå¤šçš„æ”¹è¿›ç©ºé—´ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç»“åˆç‰©ç†å­¦å’Œæ•°æ®é©±åŠ¨æ–¹æ³•å¯ä»¥æé«˜å„ç§ä¿¡å·å¤„ç†ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¿æŒå¯è§£é‡Šæ€§ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¹²æ‰°é™¤æ³›éŸ³æ¡†æ¶ï¼Œé€šè¿‡åœ¨ WPE æ¡†æ¶ä¸­åŒ…å«æ•°æ®é©±åŠ¨æ–¹æ³•æ¥æ•æ‰speech çš„åå¥½ã€‚ä½¿ç”¨ plug-and-play ç­–ç•¥ï¼ˆPnPï¼‰å’Œæ­£åˆ™åŒ– denoising ç­–ç•¥ï¼ˆREDï¼‰æ¥åœ¨ä¼˜åŒ–é—®é¢˜çš„è§£å†³è¿‡ç¨‹ä¸­åŒ…å«æ•°æ®é©±åŠ¨æ–¹æ³•å­¦ä¹ çš„speech åå¥½ä¿¡æ¯ã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Distributed-Speech-Dereverberation-Using-Weighted-Prediction-Error"><a href="#Distributed-Speech-Dereverberation-Using-Weighted-Prediction-Error" class="headerlink" title="Distributed Speech Dereverberation Using Weighted Prediction Error"></a>Distributed Speech Dereverberation Using Weighted Prediction Error</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03034">http://arxiv.org/abs/2312.03034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziye Yang, Mengfei Zhang, Jie Chen</li>
<li>for: å‡å°‘å»¶è¿Ÿå“åº”çš„è´Ÿé¢å½±å“ï¼Œæé«˜è¯­éŸ³å¬å½•è´¨é‡ã€‚</li>
<li>methods: ä½¿ç”¨åˆ†å¸ƒå¼é€‚åº”èŠ‚ç‚¹ç‰¹å®šä¿¡å·ä¼°è®¡ï¼ˆDANSEï¼‰ç®—æ³•ï¼Œåœ¨å¤šé€šé“çº¿æ€§é¢„æµ‹ï¼ˆMCLPï¼‰è¿‡ç¨‹ä¸­å®ç°åˆ†å¸ƒå¼æ¼”ç®—ã€‚æ¯ä¸ªèŠ‚ç‚¹åªéœ€æ‰§è¡Œæœ¬åœ°æ“ä½œï¼Œé€šè¿‡èŠ‚ç‚¹é—´åä½œå®ç°å…¨å±€æ€§èƒ½ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åœ¨åˆ†å¸ƒå¼ Microphone èŠ‚ç‚¹åœºæ™¯ä¸‹å®ç°é«˜æ•ˆçš„è¯­éŸ³å¬å½•é™¤æŠ–ã€‚<details>
<summary>Abstract</summary>
Speech dereverberation aims to alleviate the negative impact of late reverberant reflections. The weighted prediction error (WPE) method is a well-established technique known for its superior performance in dereverberation. However, in scenarios where microphone nodes are dispersed, the centralized approach of the WPE method requires aggregating all observations for inverse filtering, resulting in a significant computational burden. This paper introduces a distributed speech dereverberation method that emphasizes low computational complexity at each node. Specifically, we leverage the distributed adaptive node-specific signal estimation (DANSE) algorithm within the multichannel linear prediction (MCLP) process. This approach empowers each node to perform local operations with reduced complexity while achieving the global performance through inter-node cooperation. Experimental results validate the effectiveness of our proposed method, showcasing its ability to achieve efficient speech dereverberation in dispersed microphone node scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>Speech dereverberation aims to alleviate the negative impact of late reverberant reflections. The weighted prediction error (WPE) method is a well-established technique known for its superior performance in dereverberation. However, in scenarios where microphone nodes are dispersed, the centralized approach of the WPE method requires aggregating all observations for inverse filtering, resulting in a significant computational burden. This paper introduces a distributed speech dereverberation method that emphasizes low computational complexity at each node. Specifically, we leverage the distributed adaptive node-specific signal estimation (DANSE) algorithm within the multichannel linear prediction (MCLP) process. This approach empowers each node to perform local operations with reduced complexity while achieving the global performance through inter-node cooperation. Experimental results validate the effectiveness of our proposed method, showcasing its ability to achieve efficient speech dereverberation in dispersed microphone node scenarios.Translation:<<SYS>>è½¬æ¢ç»™å®šæ–‡æœ¬åˆ°ç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>Speech dereverberation aims to alleviate the negative impact of late reverberant reflections. WPE method is a well-established technique known for its superior performance in dereverberation. However, in scenarios where microphone nodes are dispersed, the centralized approach of the WPE method requires aggregating all observations for inverse filtering, resulting in a significant computational burden. This paper introduces a distributed speech dereverberation method that emphasizes low computational complexity at each node. Specifically, we leverage the distributed adaptive node-specific signal estimation (DANSE) algorithm within the multichannel linear prediction (MCLP) process. This approach empowers each node to perform local operations with reduced complexity while achieving the global performance through inter-node cooperation. Experimental results validate the effectiveness of our proposed method, showcasing its ability to achieve efficient speech dereverberation in dispersed microphone node scenarios.Note: Please keep in mind that the translation is done using a machine translation tool, and the quality of the translation may vary depending on the complexity and nuances of the original text.
</details></li>
</ul>
<hr>
<h2 id="Auralization-based-on-multi-perspective-ambisonic-room-impulse-responses"><a href="#Auralization-based-on-multi-perspective-ambisonic-room-impulse-responses" class="headerlink" title="Auralization based on multi-perspective ambisonic room impulse responses"></a>Auralization based on multi-perspective ambisonic room impulse responses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02581">http://arxiv.org/abs/2312.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaspar MÃ¼ller, Franz Zotter</li>
<li>for: è¿™ç§æŠ€æœ¯ç”¨äºåœ¨å˜åŒ–çš„å¬ä¼—è§†è§’ä¸‹å®æ—¶ç”Ÿæˆå¬ä¼—åœ¨ä¸åŒç¯å¢ƒä¸­çš„å¬è§‰æ•ˆæœã€‚</li>
<li>methods: ä½¿ç”¨ Ambisonic å®¤å†…å“åº”å‡½æ•°ï¼ˆARIRï¼‰çš„ä¸‰å…ƒ interpolate æŠ€æœ¯ï¼Œå°† ARRI çš„æ•°æ®é›†ä¸­çš„å¬ä¼—è§†è§’ä½œä¸ºå˜é‡ï¼Œå¹¶ä½¿ç”¨æ—¶é—´å·®åˆ†æå’Œæ–¹å‘æ¢æµ‹æ¥å®ç°å¬è§‰æ•ˆæœçš„æ‹Ÿåˆã€‚</li>
<li>results: é€šè¿‡ listening å®éªŒï¼Œåœ¨ä¸åŒç¯å¢ƒä¸‹ä½¿ç”¨è¿™ç§æŠ€æœ¯å¯ä»¥å®ç°é«˜è´¨é‡çš„å¬è§‰æ•ˆæœï¼Œå¹¶ä¸”å¯ä»¥åœ¨å˜åŒ–çš„å¬ä¼—è§†è§’ä¸‹è¿›è¡Œå®æ—¶æ‹Ÿåˆã€‚<details>
<summary>Abstract</summary>
Most often, virtual acoustic rendering employs real-time updated room acoustic simulations to accomplish auralization for a variable listener perspective. As an alternative, we propose and test a technique to interpolate room impulse responses, specifically Ambisonic room impulse responses (ARIRs) available at a grid of spatially distributed receiver perspectives, measured or simulated in a desired acoustic environment. In particular, we extrapolate a triplet of neighboring ARIRs to the variable listener perspective, preceding their linear interpolation. The extrapolation is achieved by decomposing each ARIR into localized sound events and re-assigning their direction, time, and level to what could be observed at the listener perspective, with as much temporal, directional, and perspective context as possible. We propose to undertake this decomposition in two levels: Peaks in the early ARIRs are decomposed into jointly localized sound events, based on time differences of arrival observed in either an ARIR triplet, or all ARIRs observing the direct sound. Sound events that could not be jointly localized are treated as residuals whose less precise localization utilizes direction-of-arrival detection and the estimated time of arrival. For the interpolated rendering, suitable parameter settings are found by evaluating the proposed method in a listening experiment, using both measured and simulated ARIR data sets, under static and time-varying conditions.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šå¸¸ï¼Œè™šæ‹ŸéŸ³å“æ¸²æŸ“ä½¿ç”¨å®æ—¶æ›´æ–°çš„æˆ¿é—´éŸ³å“æ¨¡æ‹Ÿæ¥å®ç°å¬ä¼—è§’åº¦å˜åŒ–ä¸‹çš„å¬è§‰ã€‚ä½œä¸ºä¸€ä¸ª alternativesï¼Œæˆ‘ä»¬æå‡ºå¹¶æµ‹è¯•äº†ä¸€ç§æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯æ˜¯ interpolating æˆ¿é—´å†²å‡»å“åº”ï¼ˆARIRï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨ Desired éŸ³å“ç¯å¢ƒä¸­æµ‹é‡æˆ–æ¨¡æ‹Ÿçš„å„ç§å¬ä¼—è§’åº¦ä¸Šçš„ä¸€ç»„åˆ†å¸ƒå¼æ¥æ”¶å™¨è§’åº¦ä¸Šã€‚åœ¨ç‰¹å®šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä¸‰ä¸ªç›¸é‚»çš„ ARIR  interpolated åˆ°å˜é‡å¬ä¼—è§’åº¦ä¸Šï¼Œå¹¶åœ¨å…¶çº¿æ€§ interpolate ä¹‹å‰è¿›è¡Œäº†å…ˆå‰çš„æ‹Ÿåˆã€‚ interpolate çš„å®ç°æ–¹å¼æ˜¯å°†æ¯ä¸ª ARIR  decomposed æˆæœ¬åœ°åŒ–çš„å£°éŸ³äº‹ä»¶ï¼Œå¹¶å°†å®ƒä»¬çš„æ–¹å‘ã€æ—¶é—´å’Œå¼ºåº¦é‡æ–°åˆ†é…ç»™å¯èƒ½åœ¨å¬ä¼—è§’åº¦ä¸Šè§‚å¯Ÿåˆ°çš„å£°éŸ³ï¼Œä»¥ä¿ç•™æœ€å¤šçš„æ—¶é—´ã€æ–¹å‘å’Œè§‚å¯Ÿè§’åº¦ä¸Šçš„ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬æè®®åœ¨ä¸¤ä¸ªå±‚æ¬¡è¿›è¡Œè¿™ç§åˆ’åˆ†ï¼šåœ¨æ—©æœŸ ARIR ä¸­ï¼Œå‘ç”Ÿåœ¨ç›¸é‚»çš„å†²å‡»å“åº”ä¸­çš„å³°å€¼è¢« decomposed æˆå…±åŒlocalized çš„å£°éŸ³äº‹ä»¶ï¼ŒåŸºäºåœ¨ ARIR  triplet ä¸­æˆ–æ‰€æœ‰ ARIRs è§‚å¯Ÿåˆ°çš„ç›´æ¥å£°éŸ³æ—¶é—´å·®ã€‚ä¸èƒ½å…±åŒlocalized çš„å£°éŸ³äº‹ä»¶è¢«è§†ä¸ºå‰©ä¸‹çš„ residualsï¼Œå…¶å‡†ç¡®æ€§è¾ƒä½çš„localization ä½¿ç”¨æ–¹å‘åˆ°è¾¾æ£€æµ‹å’Œä¼°è®¡çš„æ—¶é—´åˆ°è¾¾ã€‚åœ¨ interpolated æ¸²æŸ“ä¸­ï¼Œé‡‡ç”¨ suitable å‚æ•°è®¾ç½®ï¼Œé€šè¿‡å¯¹æè®®æ–¹æ³•è¿›è¡Œæµ‹è¯•ï¼Œä½¿ç”¨ measured å’Œ simulated ARIR æ•°æ®é›†ï¼Œåœ¨é™æ­¢å’Œæ—¶é—´å˜åŒ–çš„æƒ…å†µä¸‹è¿›è¡Œè¯„ä¼°ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/05/cs.SD_2023_12_05/" data-id="clq0ru72v0174to880of0h6lq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_12_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/05/cs.CV_2023_12_05/" class="article-date">
  <time datetime="2023-12-05T13:00:00.000Z" itemprop="datePublished">2023-12-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/05/cs.CV_2023_12_05/">cs.CV - 2023-12-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DiffusionAtlas-High-Fidelity-Consistent-Diffusion-Video-Editing"><a href="#DiffusionAtlas-High-Fidelity-Consistent-Diffusion-Video-Editing" class="headerlink" title="DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing"></a>DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03772">http://arxiv.org/abs/2312.03772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Yu Chang, Hwann-Tzong Chen, Tyng-Luh Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³è§†é¢‘ç¼–è¾‘ä¸­ç»´æŠ¤ç‰©ä½“å¤–è§‚çš„ä¸€è‡´æ€§å’Œé«˜ç²¾åº¦é—®é¢˜ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åŸºäºDiffusionAtlasæ¡†æ¶ï¼Œåˆ©ç”¨è§†è§‰æ–‡æœ¬æ‰©æ•£æ¨¡å‹è¿›è¡Œç‰©ä½“ç›´æ¥ç¼–è¾‘ï¼Œä»¥ç¡®ä¿å¸§å†…ç‰©ä½“å¤–è§‚ä¸€è‡´æ€§ã€‚</li>
<li>results: å¯¹æ¯”ä¹‹ä¸‹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨è§†é¢‘ç¼–è¾‘ä¸­å®ç°é«˜ç²¾åº¦å’Œä¸€è‡´æ€§ï¼Œè¶…è¿‡å½“å‰çŠ¶æ€è‰ºæ³•çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We present a diffusion-based video editing framework, namely DiffusionAtlas, which can achieve both frame consistency and high fidelity in editing video object appearance. Despite the success in image editing, diffusion models still encounter significant hindrances when it comes to video editing due to the challenge of maintaining spatiotemporal consistency in the object's appearance across frames. On the other hand, atlas-based techniques allow propagating edits on the layered representations consistently back to frames. However, they often struggle to create editing effects that adhere correctly to the user-provided textual or visual conditions due to the limitation of editing the texture atlas on a fixed UV mapping field. Our method leverages a visual-textual diffusion model to edit objects directly on the diffusion atlases, ensuring coherent object identity across frames. We design a loss term with atlas-based constraints and build a pretrained text-driven diffusion model as pixel-wise guidance for refining shape distortions and correcting texture deviations. Qualitative and quantitative experiments show that our method outperforms state-of-the-art methods in achieving consistent high-fidelity video-object editing.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºæ‰©æ•£çš„è§†é¢‘ç¼–è¾‘æ¡†æ¶ï¼Œå³DiffusionAtlasï¼Œå¯ä»¥å®ç°å¸§å†…ä¸€è‡´æ€§å’Œé«˜ç²¾åº¦å¯¹è§†é¢‘å¯¹è±¡çš„å¤–è§‚è¿›è¡Œç¼–è¾‘ã€‚è™½ç„¶æ‰©æ•£æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘æ–¹é¢å–å¾—äº†æˆåŠŸï¼Œä½†åœ¨è§†é¢‘ç¼–è¾‘æ–¹é¢ä»ç„¶é‡åˆ°äº†é‡å¤§çš„éšœç¢ï¼Œä¸»è¦æ˜¯ä¿æŒè§†é¢‘å¸§å†…å¯¹è±¡çš„å¤–è§‚ä¸€è‡´æ€§ã€‚ç„¶è€Œï¼Œåœ¨åœ°å›¾åŸºç¡€ä¸Šçš„æŠ€æœ¯å¯ä»¥åœ¨å±‚æ¬¡è¡¨ç¤ºä¸­ä¸€è‡´åœ°ä¼ é€’ç¼–è¾‘æ•ˆæœï¼Œä½†å®ƒä»¬ç»å¸¸å› ä¸ºå›ºå®šçš„UVæ˜ å°„åœºè€Œéš¾ä»¥åˆ›é€ ç¬¦åˆç”¨æˆ·æä¾›çš„æ–‡æœ¬æˆ–è§†è§‰æ¡ä»¶çš„ç¼–è¾‘æ•ˆæœã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†è§†è§‰æ–‡æœ¬æ‰©æ•£æ¨¡å‹æ¥ç›´æ¥ç¼–è¾‘å¯¹è±¡åœ¨æ‰©æ•£åœ°å›¾ä¸Šï¼Œç¡®ä¿å¯¹è±¡åœ¨å¸§å†…ä¿æŒä¸€è‡´æ€§ã€‚æˆ‘ä»¬è®¾è®¡äº†åŸºäºåœ°å›¾çš„æŸå¤±å‡½æ•°å’Œé¢„è®­ç»ƒçš„æ–‡æœ¬é©±åŠ¨æ‰©æ•£æ¨¡å‹ï¼Œä»¥ä¾¿ä¿®å¤å½¢çŠ¶åå·®å’ŒTextureåå·®ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œé‡ä¸Šçš„å®éªŒä¸­èƒœè¿‡äº†ç°æœ‰çš„æ–¹æ³•ï¼Œå¯ä»¥å®ç°é«˜ç²¾åº¦ã€ä¸€è‡´çš„è§†é¢‘å¯¹è±¡ç¼–è¾‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="DreamInpainter-Text-Guided-Subject-Driven-Image-Inpainting-with-Diffusion-Models"><a href="#DreamInpainter-Text-Guided-Subject-Driven-Image-Inpainting-with-Diffusion-Models" class="headerlink" title="DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models"></a>DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03771">http://arxiv.org/abs/2312.03771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoan Xie, Yang Zhao, Zhisheng Xiao, Kelvin C. K. Chan, Yandong Li, Yanwu Xu, Kun Zhang, Tingbo Hou</li>
<li>for: æ–‡ç« æ—¨åœ¨æè¿°ä¸€ç§æ–°çš„å›¾åƒå¡«å……ä»»åŠ¡ï¼Œå³é€šè¿‡æ–‡æœ¬å’Œç¤ºä¾‹å›¾åƒè¿›è¡Œå›¾åƒå¡«å……ã€‚åœ¨å…ˆå‰çš„å°è¯•ä¸­ï¼Œæ–‡æœ¬å’Œç¤ºä¾‹å›¾åƒéƒ½è¢«ç‹¬ç«‹åœ°ä½¿ç”¨ï¼Œä½†æ˜¯ä¸¤è€…åŒæ—¶ä½¿ç”¨çš„æŒ‘æˆ˜å°šæœªè¢«è§£å†³ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ä¸¤æ­¥æ–¹æ³• DreamInpainterï¼Œé¦–å…ˆè®¡ç®—ç²˜å¯†çš„ä¸»é¢˜ç‰¹å¾ï¼Œä»¥ç¡®ä¿ç²¾ç¡®çš„ä¸»é¢˜å¤åˆ¶ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæŠ½è±¡ Ñ‚Ğ¾ĞºĞµĞ½é€‰æ‹©æ¨¡å—ï¼Œä»¥æ¶ˆé™¤å†—ä½™çš„ä¸»é¢˜ç»†èŠ‚ï¼Œä¿ç•™ä¸»é¢˜çš„èº«ä»½è€Œå…è®¸æ ¹æ®æ–‡æœ¬æç¤ºå’Œæ©ç å½¢çŠ¶è¿›è¡Œå˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†è§£ Coupling è§„èŒƒæŠ€æœ¯ï¼Œä»¥å¢å¼ºæ–‡æœ¬æ§åˆ¶åœ¨ç¤ºä¾‹å›¾åƒå­˜åœ¨æ—¶ã€‚</li>
<li>results: æˆ‘ä»¬çš„å¹¿æ³›å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è§†è§‰è´¨é‡ã€èº«ä»½ä¿æŒå’Œæ–‡æœ¬æ§åˆ¶æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå±•ç¤ºäº†å…¶åœ¨æ–‡æœ¬å¼•å¯¼ä¸»é¢˜é©±åŠ¨å›¾åƒå¡«å……ä¸­çš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
This study introduces Text-Guided Subject-Driven Image Inpainting, a novel task that combines text and exemplar images for image inpainting. While both text and exemplar images have been used independently in previous efforts, their combined utilization remains unexplored. Simultaneously accommodating both conditions poses a significant challenge due to the inherent balance required between editability and subject fidelity. To tackle this challenge, we propose a two-step approach DreamInpainter. First, we compute dense subject features to ensure accurate subject replication. Then, we employ a discriminative token selection module to eliminate redundant subject details, preserving the subject's identity while allowing changes according to other conditions such as mask shape and text prompts. Additionally, we introduce a decoupling regularization technique to enhance text control in the presence of exemplar images. Our extensive experiments demonstrate the superior performance of our method in terms of visual quality, identity preservation, and text control, showcasing its effectiveness in the context of text-guided subject-driven image inpainting.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="HybridNeRF-Efficient-Neural-Rendering-via-Adaptive-Volumetric-Surfaces"><a href="#HybridNeRF-Efficient-Neural-Rendering-via-Adaptive-Volumetric-Surfaces" class="headerlink" title="HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces"></a>HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03160">http://arxiv.org/abs/2312.03160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haithem Turki, Vasu Agrawal, Samuel Rota BulÃ², Lorenzo Porzi, Peter Kontschieder, Deva Ramanan, Michael ZollhÃ¶fer, Christian Richardt</li>
<li>for: æé«˜è§†å›¾åˆæˆè´¨é‡å’Œé€Ÿåº¦</li>
<li>methods: ç»“åˆè¡¨é¢å’Œä½“ç§¯è¡¨ç¤ºæ–¹æ³•</li>
<li>results: æé«˜é”™è¯¯ç‡15-30%ï¼Œå®ç°çœŸå®æ—¶é¢‘ç‡ï¼ˆè‡³å°‘36å¸§&#x2F;ç§’ï¼‰ Ğ´Ğ»Ñè™šæ‹Ÿç°å®åˆ†è¾¨ç‡ï¼ˆ2Kx2Kï¼‰<details>
<summary>Abstract</summary>
Neural radiance fields provide state-of-the-art view synthesis quality but tend to be slow to render. One reason is that they make use of volume rendering, thus requiring many samples (and model queries) per ray at render time. Although this representation is flexible and easy to optimize, most real-world objects can be modeled more efficiently with surfaces instead of volumes, requiring far fewer samples per ray. This observation has spurred considerable progress in surface representations such as signed distance functions, but these may struggle to model semi-opaque and thin structures. We propose a method, HybridNeRF, that leverages the strengths of both representations by rendering most objects as surfaces while modeling the (typically) small fraction of challenging regions volumetrically. We evaluate HybridNeRF against the challenging Eyeful Tower dataset along with other commonly used view synthesis datasets. When comparing to state-of-the-art baselines, including recent rasterization-based approaches, we improve error rates by 15-30% while achieving real-time framerates (at least 36 FPS) for virtual-reality resolutions (2Kx2K).
</details>
<details>
<summary>æ‘˜è¦</summary>
neural radiance fields æä¾›äº†å½“ä»Šæœ€é«˜è´¨é‡çš„è§†å›¾åˆæˆæ•ˆæœï¼Œä½†å®ƒä»¬å¾€å¾€å…·æœ‰è¾ƒæ…¢çš„æ¸²æŸ“é€Ÿåº¦ã€‚ä¸€ä¸ªåŸå› æ˜¯å®ƒä»¬ä½¿ç”¨ä½“ç§¯æ¸²æŸ“ï¼Œå› æ­¤æ¯ä¸ªå…‰æŸçš„æ¸²æŸ“æ—¶éœ€è¦è®¸å¤šæ ·æœ¬ï¼ˆä»¥åŠæ¨¡å‹æŸ¥è¯¢ï¼‰ã€‚è™½ç„¶è¿™ç§è¡¨ç¤ºæ–¹å¼å¼¹æ€§å’Œæ˜“äºä¼˜åŒ–ï¼Œä½†å¤§å¤šæ•°å®é™…ä¸–ç•Œä¸­çš„ç‰©ä½“å¯ä»¥æ›´åŠ æ•ˆç‡åœ°ä½¿ç”¨è¡¨é¢è€Œä¸æ˜¯ä½“ç§¯æ¥è¡¨ç¤ºï¼Œéœ€è¦æ›´å°‘çš„æ ·æœ¬æ¯ä¸ªå…‰æŸã€‚è¿™ä¸€è§‚å¯Ÿç‚¹æ¨åŠ¨äº†è¡¨é¢è¡¨ç¤ºæ–¹æ³•çš„è¿›æ­¥ï¼Œå¦‚ç­¾åè·ç¦»å‡½æ•°ï¼Œä½†è¿™äº›æ–¹æ³•å¯èƒ½ä¼šéš¾ä»¥æ¨¡å‹åŠé€æ˜å’Œè–„ estructuresã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼ŒHybridNeRFï¼Œå®ƒåˆ©ç”¨ä¸¤ç§è¡¨ç¤ºæ–¹å¼çš„ä¼˜åŠ¿ï¼Œåœ¨æ¸²æŸ“å¤§éƒ¨åˆ†ç‰©ä½“æ—¶ä½¿ç”¨è¡¨é¢ï¼Œè€Œåœ¨å¤„ç†å¤æ‚çš„éƒ¨åˆ†æ—¶ä½¿ç”¨ä½“ç§¯ã€‚æˆ‘ä»¬åœ¨eyeful tower datasetä»¥åŠå…¶ä»–å¸¸ç”¨çš„è§†å›¾åˆæˆdatasetä¸Šè¯„ä¼°äº†HybridNeRFï¼Œå¹¶ä¸å…¶ä»–å¸¸ç”¨çš„åŸºå‡†å€¼è¿›è¡Œæ¯”è¾ƒã€‚ä¸state-of-the-artåŸºå‡†å€¼ç›¸æ¯”ï¼Œæˆ‘ä»¬æé«˜äº†é”™è¯¯ç‡15-30%ï¼ŒåŒæ—¶å®ç°äº†è™šæ‹Ÿç°å®åˆ†è¾¨ç‡ï¼ˆ2Kx2Kï¼‰çš„çœŸå®å¸§ç‡ï¼ˆè‡³å°‘36å¸§/ç§’ï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="ViscoNet-Bridging-and-Harmonizing-Visual-and-Textual-Conditioning-for-ControlNet"><a href="#ViscoNet-Bridging-and-Harmonizing-Visual-and-Textual-Conditioning-for-ControlNet" class="headerlink" title="ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet"></a>ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03154">http://arxiv.org/abs/2312.03154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/soon-yau/visconet">https://github.com/soon-yau/visconet</a></li>
<li>paper_authors: Soon Yau Cheong, Armin Mustafa, Andrew Gilbert</li>
<li>for: æé«˜æ–‡æœ¬åˆ°å›¾åƒäººç±»ç”Ÿæˆæ¨¡å‹çš„å¯æ§æ€§ï¼Œä½¿ç”¨è§†è§‰æç¤ºæ¥æ§åˆ¶å›¾åƒç»“æ„ã€‚</li>
<li>methods: ä½¿ç”¨ControlNetåˆ†æ”¯å°†ç‰©ä½“çš„å¤–è§‚åˆ†ç¦»å‡ºå›¾åƒèƒŒæ™¯ï¼Œå¹¶åœ¨é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­æ³¨å…¥æ§åˆ¶ä¿¡æ¯ã€‚</li>
<li>results: å¯ä»¥é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæç¤ºæ¥æ§åˆ¶å›¾åƒçš„è§†è§‰ç‰¹å¾å’Œè‰ºæœ¯é£æ ¼ï¼Œå¹¶ä¸”å¯ä»¥ä»å°å‹ç‰¹å®šå¯¹è±¡é¢†åŸŸå­¦ä¹ è§†è§‰æ¡ä»¶ï¼Œä¿ç•™LDMéª¨å¹²çš„ç”ŸæˆåŠ›ã€‚<details>
<summary>Abstract</summary>
This paper introduces ViscoNet, a novel method that enhances text-to-image human generation models with visual prompting. Unlike existing methods that rely on lengthy text descriptions to control the image structure, ViscoNet allows users to specify the visual appearance of the target object with a reference image. ViscoNet disentangles the object's appearance from the image background and injects it into a pre-trained latent diffusion model (LDM) model via a ControlNet branch. This way, ViscoNet mitigates the style mode collapse problem and enables precise and flexible visual control. We demonstrate the effectiveness of ViscoNet on human image generation, where it can manipulate visual attributes and artistic styles with text and image prompts. We also show that ViscoNet can learn visual conditioning from small and specific object domains while preserving the generative power of the LDM backbone.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ViscoNetï¼Œä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç”¨äºå¢å¼ºæ–‡æœ¬åˆ°å›¾åƒäººå·¥ç”Ÿæˆæ¨¡å‹ï¼Œä½¿å…¶å…·æœ‰æ›´ç²¾ç¡®å’Œçµæ´»çš„è§†è§‰æ§åˆ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒï¼ŒViscoNetä¸éœ€è¦é•¿æ–‡æœ¬æè¿°æ¥æ§åˆ¶å›¾åƒç»“æ„ï¼Œè€Œæ˜¯é€šè¿‡å¼•å…¥å‚è€ƒå›¾åƒæ¥æŒ‡å®šç›®æ ‡å¯¹è±¡çš„è§†è§‰å¤–è§‚ã€‚ViscoNetåˆ†ç¦»äº†å›¾åƒèƒŒæ™¯å’Œç›®æ ‡å¯¹è±¡çš„å¤–è§‚ï¼Œå¹¶å°†å…¶æ³¨å…¥åˆ°é¢„è®­ç»ƒçš„æ‰©æ•£æ¨¡å‹ï¼ˆLDMï¼‰ä¸­ï¼Œé€šè¿‡ControlNetåˆ†æ”¯ã€‚è¿™æ ·ï¼ŒViscoNetå¯ä»¥è§£å†³Style Mode Collapseé—®é¢˜ï¼Œå¹¶å…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæç¤ºæ¥æ§åˆ¶å›¾åƒçš„Visual attributeå’Œè‰ºæœ¯é£æ ¼ã€‚æˆ‘ä»¬åœ¨äººåƒç”Ÿæˆä¸­ç¤ºç¤ºäº†ViscoNetçš„æ•ˆæœï¼Œå®ƒå¯ä»¥é€šè¿‡æ–‡æœ¬å’Œå›¾åƒæç¤ºæ¥æ§åˆ¶è§†è§‰ç‰¹å¾å’Œè‰ºæœ¯é£æ ¼ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¡¨æ˜ViscoNetå¯ä»¥ä»å°çš„ç‰¹å®šå¯¹è±¡é¢†åŸŸä¸­å­¦ä¹ è§†è§‰æ¡ä»¶ï¼Œè€Œä¸ä¼šå‰Šå¼±LDMçš„ç”Ÿæˆèƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Predicting-Bone-Degradation-Using-Vision-Transformer-and-Synthetic-Cellular-Microstructures-Dataset"><a href="#Predicting-Bone-Degradation-Using-Vision-Transformer-and-Synthetic-Cellular-Microstructures-Dataset" class="headerlink" title="Predicting Bone Degradation Using Vision Transformer and Synthetic Cellular Microstructures Dataset"></a>Predicting Bone Degradation Using Vision Transformer and Synthetic Cellular Microstructures Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03133">http://arxiv.org/abs/2312.03133</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Saber Hashemi, Azadeh Sheidaei</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº†é¢„æµ‹å’Œå¯è§†åŒ–å¤ªç©ºæ¢ç´¢ä»»åŠ¡ä¸­å®‡èˆªå‘˜éª¨éª¼è¡°ç«­çš„ç°è±¡ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§å…·æœ‰å¼¹æ€§å’Œé€Ÿåº¦çš„è®¡ç®—æ–¹æ³•ï¼Œå³TransVNetï¼Œå¯ä»¥å°†ä¸åŒçš„3D voxelizedå›¾åƒè¿›è¡Œé¢„æµ‹å’Œå¯è§†åŒ–ï¼Œå¹¶ä¸”å¯ä»¥è€ƒè™‘æ¯ä¸ªä¸ªä½“çš„éª¨éª¼ç‰¹æ€§ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ä¸ªæ··åˆ3D-CNN-VisionTransformer autoencoderæ¶æ„ï¼Œå¯ä»¥å°†3D voxelizedå›¾åƒçš„æ¼”åŒ–è¿½è¸ªåˆ°æœˆä»½çº§çš„æ—¶é—´å°ºåº¦ï¼Œå¹¶ä¸”å¯ä»¥å°†è¿™äº›å˜åŒ–ä¸çœŸå®çš„éª¨éª¼è¡°ç«­ç°è±¡è¿›è¡Œæ¯”è¾ƒã€‚<details>
<summary>Abstract</summary>
Bone degradation, especially for astronauts in microgravity conditions, is crucial for space exploration missions since the lower applied external forces accelerate the diminution in bone stiffness and strength substantially. Although existing computational models help us understand this phenomenon and possibly restrict its effect in the future, they are time-consuming to simulate the changes in the bones, not just the bone microstructures, of each individual in detail. In this study, a robust yet fast computational method to predict and visualize bone degradation has been developed. Our deep-learning method, TransVNet, can take in different 3D voxelized images and predict their evolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer autoencoder architecture. Because of limited available experimental data and challenges of obtaining new samples, a digital twin dataset of diverse and initial bone-like microstructures was generated to train our TransVNet on the evolution of the 3D images through a previously developed degradation model for microgravity.
</details>
<details>
<summary>æ‘˜è¦</summary>
éª¨è´¨ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å¾®é‡åŠ›æ¡ä»¶ä¸‹ï¼Œå¯¹èˆªå¤©å™¨æ¢ç´¢ä»»åŠ¡éå¸¸é‡è¦ï¼Œå› ä¸ºä½äºåº”ç”¨çš„å¤–éƒ¨åŠ›é‡ä¼šåŠ é€Ÿéª¨åˆšåº¦å’Œå¼ºåº¦çš„å‡é€€ã€‚è™½ç„¶ç°æœ‰çš„è®¡ç®—æ¨¡å‹å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£è¿™ç§ç°è±¡å¹¶å¯èƒ½å°†å…¶å½±å“é™åˆ¶åœ¨æœªæ¥ï¼Œä½†å®ƒä»¬éœ€è¦è¾ƒé•¿æ—¶é—´æ¥æ¨¡æ‹Ÿæ¯ä¸ªä¸ªä½“çš„éª¨å˜åŒ–ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¼ºå¥å¿«é€Ÿçš„è®¡ç®—æ–¹æ³•æ¥é¢„æµ‹å’Œå¯è§†åŒ–éª¨è´¨ä¸‹é™ã€‚æˆ‘ä»¬çš„æ·±åº¦å­¦ä¹ æ–¹æ³•TransVNetå¯ä»¥å°†ä¸åŒçš„3DçŸ©é˜µå›¾åƒä½œä¸ºè¾“å…¥ï¼Œé¢„æµ‹å…¶åœ¨æœˆä»½å†…çš„æ¼”åŒ–ï¼Œä½¿ç”¨æ··åˆ3D-CNN-è§†Transformerè‡ªé€‚åº”ç½‘ç»œæ¶æ„ã€‚ç”±äºå®éªŒæ•°æ®çš„æœ‰é™æ€§å’Œè·å–æ–°æ ·æœ¬çš„æŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªæ•°å­—åŒèƒè™«æ•°æ®é›†æ¥è®­ç»ƒæˆ‘ä»¬çš„TransVNetï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§åˆå§‹éª¨çŠ¶å¾®ç»“æ„ã€‚
</details></li>
</ul>
<hr>
<h2 id="AI-SAM-Automatic-and-Interactive-Segment-Anything-Model"><a href="#AI-SAM-Automatic-and-Interactive-Segment-Anything-Model" class="headerlink" title="AI-SAM: Automatic and Interactive Segment Anything Model"></a>AI-SAM: Automatic and Interactive Segment Anything Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03119">http://arxiv.org/abs/2312.03119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimu Pan, Sitao Zhang, Alison D. Gernand, Jeffery A. Goldstein, James Z. Wang</li>
<li>for: è¯¥è®ºæ–‡ä¸»è¦é’ˆå¯¹ Semantic Segmentation ä»»åŠ¡ä¸­çš„è‡ªåŠ¨å’Œäº¤äº’ä¸¤ç§æ–¹æ³•çš„é—®é¢˜ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è‡ªåŠ¨å’Œäº¤äº’æ¨¡å‹ï¼ˆAI-SAMï¼‰ï¼Œé€šè¿‡å¯¹æç¤ºè´¨é‡è¿›è¡Œå…¨é¢åˆ†æï¼Œå¹¶æå‡ºäº†é¦–ä¸ªè‡ªåŠ¨ç”Ÿæˆåˆå§‹ç‚¹æç¤ºçš„è‡ªåŠ¨äº¤äº’æç¤ºå™¨ï¼ˆAI-Prompterï¼‰ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒAI-SAMåœ¨è‡ªåŠ¨è®¾ç½®ä¸‹è¾¾åˆ°äº†çŠ¶æ€å½“å‰çš„æœ€é«˜æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥é‡‡ç”¨æ›´å¤šçš„ç”¨æˆ·æç¤ºæ¥è¿›ä¸€æ­¥æé«˜æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Semantic segmentation is a core task in computer vision. Existing methods are generally divided into two categories: automatic and interactive. Interactive approaches, exemplified by the Segment Anything Model (SAM), have shown promise as pre-trained models. However, current adaptation strategies for these models tend to lean towards either automatic or interactive approaches. Interactive methods depend on prompts user input to operate, while automatic ones bypass the interactive promptability entirely. Addressing these limitations, we introduce a novel paradigm and its first model: the Automatic and Interactive Segment Anything Model (AI-SAM). In this paradigm, we conduct a comprehensive analysis of prompt quality and introduce the pioneering Automatic and Interactive Prompter (AI-Prompter) that automatically generates initial point prompts while accepting additional user inputs. Our experimental results demonstrate AI-SAM's effectiveness in the automatic setting, achieving state-of-the-art performance. Significantly, it offers the flexibility to incorporate additional user prompts, thereby further enhancing its performance. The project page is available at https://github.com/ymp5078/AI-SAM.
</details>
<details>
<summary>æ‘˜è¦</summary>
Semantic segmentation æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„æ ¸å¿ƒä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•å¤§è‡´å¯åˆ†ä¸ºä¸¤ç±»ï¼šè‡ªåŠ¨å’Œäº¤äº’å¼ã€‚äº¤äº’å¼æ–¹æ³•ï¼Œå¦‚Segment Anything Model (SAM)ï¼Œæœ‰ç¤º promise ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“å‰çš„é€‚åº”ç­–ç•¥å€¾å‘äºè‡ªåŠ¨æˆ–äº¤äº’å¼æ–¹æ³•ã€‚äº¤äº’å¼æ–¹æ³•éœ€è¦ç”¨æˆ·è¾“å…¥æ¥è¿è¡Œï¼Œè€Œè‡ªåŠ¨æ–¹æ³•åˆ™å®Œå…¨ä¸éœ€è¦ç”¨æˆ·äº¤äº’ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°çš„æ€æƒ³å’Œå…¶é¦–ä¸ªæ¨¡å‹ï¼šè‡ªåŠ¨å’Œäº¤äº’å¼Segment Anything Model (AI-SAM)ã€‚åœ¨è¿™ç§æ€æƒ³ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¯¦ç»†çš„æç¤ºè´¨é‡åˆ†æï¼Œå¹¶å¼•å…¥äº†å…ˆé”‹çš„è‡ªåŠ¨å’Œäº¤äº’å¼æç¤ºå™¨ï¼ˆAI-Prompterï¼‰ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆåˆå§‹ç‚¹æç¤ºè€Œ accepting additional ç”¨æˆ·è¾“å…¥ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ AI-SAM åœ¨è‡ªåŠ¨è®¾ç½®ä¸‹ exhibit å· å‰çš„è¡¨ç°ï¼Œå¹¶ä¸”å…·æœ‰æ›´å¤šç”¨æˆ·æç¤ºçš„çµæ´»æ€§ï¼Œè¿›ä¸€æ­¥æé«˜å…¶æ€§èƒ½ã€‚é¡¹ç›®é¡µé¢å¯ä»¥åœ¨ <https://github.com/ymp5078/AI-SAM> ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Automated-Bias-Triangle-Feature-Extraction-Framework"><a href="#The-Automated-Bias-Triangle-Feature-Extraction-Framework" class="headerlink" title="The Automated Bias Triangle Feature Extraction Framework"></a>The Automated Bias Triangle Feature Extraction Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03110">http://arxiv.org/abs/2312.03110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madeleine Kotzagiannidis, Jonas Schuff, Nathan Korda</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ä¸ªè‡ªåŠ¨åŒ–åˆ†ææ¡†æ¶ï¼Œä»¥ä¾¿åˆ©ç”¨é‡å­ç‚¹ï¼ˆQDï¼‰è®¾å¤‡çš„ç¨³å®šå›¾ä¸­çš„åç½®ä¸‰è§’å½¢æ¥æ£€æµ‹ç²’å­ç‰©ç†å­¦ä¸­çš„PauliæŸé˜»å¡ï¼ˆPSBï¼‰ç°è±¡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æ— ç›‘ç£çš„è®¡ç®—æœºè§†è§‰æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å‰²åˆ†æï¼Œä»¥æå–åç½®ä¸‰è§’å½¢çš„ç‰©ç†ç‰¹æ€§ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è‡ªåŠ¨åœ°è¯†åˆ«å’Œé‡åŒ–åç½®ä¸‰è§’å½¢çš„å½¢çŠ¶å’Œç‰¹å¾ï¼Œå¹¶ä¸”ä¸éœ€è¦äººå·¥æ ‡æ³¨æˆ–å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•å¯ä»¥é«˜æ•ˆåœ°ã€è‡ªåŠ¨åœ°æ£€æµ‹PSBç°è±¡ï¼Œè€Œä¸éœ€è¦ä»»ä½•è®­ç»ƒæ•°æ®æˆ–äººå·¥æ ‡æ³¨ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æé«˜é‡å­ç‚¹è®¾å¤‡çš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Bias triangles represent features in stability diagrams of Quantum Dot (QD) devices, whose occurrence and property analysis are crucial indicators for spin physics. Nevertheless, challenges associated with quality and availability of data as well as the subtlety of physical phenomena of interest have hindered an automatic and bespoke analysis framework, often still relying (in part) on human labelling and verification. We introduce a feature extraction framework for bias triangles, built from unsupervised, segmentation-based computer vision methods, which facilitates the direct identification and quantification of physical properties of the former. Thereby, the need for human input or large training datasets to inform supervised learning approaches is circumvented, while additionally enabling the automation of pixelwise shape and feature labeling. In particular, we demonstrate that Pauli Spin Blockade (PSB) detection can be conducted effectively, efficiently and without any training data as a direct result of this approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
bias triangle è¡¨ç¤ºé‡å­ç‚¹ï¼ˆQDï¼‰è®¾å¤‡çš„ç¨³å®šæ€§å›¾ä¸­çš„ç‰¹å¾ï¼Œå…¶å‡ºç°å’Œæ€§è´¨åˆ†ææ˜¯æ ¸ç£ç‰©ç†çš„é‡è¦æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œæ•°æ®è´¨é‡å’Œå¯ç”¨æ€§çš„é—®é¢˜ä»¥åŠæ„Ÿå…´è¶£çš„ç‰©ç†ç°è±¡çš„ç»†å¾®æ€§å¸¦æ¥äº†è‡ªåŠ¨åŒ–å’Œç‰¹å®šåˆ†ææ¡†æ¶çš„å›°éš¾ï¼Œç»å¸¸ä»ç„¶ rely (part) on human labeling and verificationã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºä¸supervisedï¼Œåˆ†å‰²based computer visionæ–¹æ³•çš„ç‰¹å¾æå–æ¡†æ¶ï¼Œå¯ä»¥ç›´æ¥è¯†åˆ«å’Œé‡åŒ–formerçš„ç‰©ç†å±æ€§ã€‚å› æ­¤ï¼Œä¸éœ€è¦äººå·¥è¾“å…¥æˆ–å¤§è®­ç»ƒæ•°æ®æ¥æ”¯æŒsupervised learningæ–¹æ³•ï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿè‡ªåŠ¨åŒ–åƒç´ çº§shapeå’Œç‰¹å¾æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ç¤ºç¤ºäº†Pauli Spin Blockadeï¼ˆPSBï¼‰æ£€æµ‹å¯ä»¥é«˜æ•ˆã€é«˜æ•ˆåœ°è¿›è¡Œï¼Œä¸éœ€è¦ä»»ä½•training dataã€‚
</details></li>
</ul>
<hr>
<h2 id="Fully-Convolutional-Slice-to-Volume-Reconstruction-for-Single-Stack-MRI"><a href="#Fully-Convolutional-Slice-to-Volume-Reconstruction-for-Single-Stack-MRI" class="headerlink" title="Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI"></a>Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03102">http://arxiv.org/abs/2312.03102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sean I. Young, YaÃ«l Balbastre, Bruce Fischl, Polina Golland, Juan Eugenio Iglesias</li>
<li>for: ç”¨äºé‡å»ºæœªçŸ¥çš„3Dç£å…±æŒ¯å›¾åƒå·ç§¯</li>
<li>methods: ä½¿ç”¨æ·±åº¦å­¦ä¹ ç½‘ç»œï¼Œé€šè¿‡å¯¹ç»™å®šçš„ slice stack è¿›è¡Œå•ä¸ªè§†å›¾è¿åŠ¨ä¼°è®¡ï¼Œç”Ÿæˆä¸€ä¸ªè¿åŠ¨å †ï¼Œå¹¶ä¸”ä½œä¸ºè¿åŠ¨å †çš„å‰¯äº§å“ç”Ÿæˆ3Dé‡å»º</li>
<li>results: å¯¹æˆäººå’Œèƒå„¿å¤§è„‘çš„SVRé‡å»ºå®ç°äº†ä¸¤å€çš„ç²¾åº¦ï¼Œä¸ä¹‹å‰çš„SVRæ–¹æ³•ç›¸æ¯”Hereâ€™s the translation of the abstract in English:</li>
<li>for: To reconstruct an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion.</li>
<li>methods: Using a fully convolutional network to estimate the motion stack for a given slice stack, and produce a 3D reconstruction as a byproduct of the predicted motion.</li>
<li>results: Achieved twice the accuracy of previous SVR methods in reconstructing adult and fetal brains. The code is available at github.com&#x2F;seannz&#x2F;svr.<details>
<summary>Abstract</summary>
In magnetic resonance imaging (MRI), slice-to-volume reconstruction (SVR) refers to computational reconstruction of an unknown 3D magnetic resonance volume from stacks of 2D slices corrupted by motion. While promising, current SVR methods require multiple slice stacks for accurate 3D reconstruction, leading to long scans and limiting their use in time-sensitive applications such as fetal fMRI. Here, we propose a SVR method that overcomes the shortcomings of previous work and produces state-of-the-art reconstructions in the presence of extreme inter-slice motion. Inspired by the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at github.com/seannz/svr.
</details>
<details>
<summary>æ‘˜è¦</summary>
magnetic resonance imaging (MRI)ä¸­çš„slice-to-volume reconstruction (SVR)æ˜¯æŒ‡è®¡ç®—unknownçš„3Dç£å…±æŒ¯å›¾åƒä»å †å çš„2Dsliceä¸­é‡å»ºçš„è®¡ç®—æ–¹æ³•ã€‚è€Œç°åœ¨çš„SVRæ–¹æ³•éœ€è¦å¤šä¸ªsliceå †å æ¥å®ç° precisemotion çš„3Dé‡å»ºï¼Œå¯¼è‡´æ‰«ææ—¶é—´é•¿ï¼Œé™åˆ¶äº†åœ¨æ—¶é—´æ•æ„Ÿåº”ç”¨ä¸­çš„ä½¿ç”¨ï¼Œå¦‚èƒå„¿fMRIã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§SVRæ–¹æ³•ï¼Œè¶…è¶Šäº†ç°æœ‰çš„å·¥ä½œï¼Œå¹¶åœ¨æç«¯é—´ slice è¿åŠ¨ä¸‹ç”Ÿæˆäº†state-of-the-artçš„é‡å»ºã€‚æˆ‘ä»¬ Drawing inspiration from the recent success of single-view depth estimation methods, we formulate SVR as a single-stack motion estimation task and train a fully convolutional network to predict a motion stack for a given slice stack, producing a 3D reconstruction as a byproduct of the predicted motion. Our extensive experiments on the SVR of adult and fetal brains demonstrate that our fully convolutional method is twice as accurate as previous SVR methods. Our code is available at github.com/seannz/svr.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Gaussian3Diff-3D-Gaussian-Diffusion-for-3D-Full-Head-Synthesis-and-Editing"><a href="#Gaussian3Diff-3D-Gaussian-Diffusion-for-3D-Full-Head-Synthesis-and-Editing" class="headerlink" title="Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing"></a>Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03763">http://arxiv.org/abs/2312.03763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey, Thomas Funkhouser, Chen Change Loy, Yinda Zhang</li>
<li>for: ç”Ÿæˆé«˜å“è´¨3Däººå¤´æ¨¡å‹ï¼Œå¹¶å¯ä»¥çµæ´»åœ°ä¿®æ”¹å’Œé‡æ–°poseã€‚</li>
<li>methods: ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œç”Ÿæˆ3Däººå¤´æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨3DGauss Distributionæ¥è¡¨ç¤ºäººå¤´çš„å½¢çŠ¶å’Œè¡¨ç°ã€‚</li>
<li>results: å¯ä»¥ç”Ÿæˆé«˜å“è´¨çš„3Däººå¤´æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥çµæ´»åœ°ä¿®æ”¹å’Œé‡æ–°poseã€‚<details>
<summary>Abstract</summary>
We present a novel framework for generating photorealistic 3D human head and subsequently manipulating and reposing them with remarkable flexibility. The proposed approach leverages an implicit function representation of 3D human heads, employing 3D Gaussians anchored on a parametric face model. To enhance representational capabilities and encode spatial information, we embed a lightweight tri-plane payload within each Gaussian rather than directly storing color and opacity. Additionally, we parameterize the Gaussians in a 2D UV space via a 3DMM, enabling effective utilization of the diffusion model for 3D head avatar generation. Our method facilitates the creation of diverse and realistic 3D human heads with fine-grained editing over facial features and expressions. Extensive experiments demonstrate the effectiveness of our method.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç”¨äºç”ŸæˆçœŸå®çš„3Däººå¤´å’Œåç»­è¿›è¡Œçµæ´»çš„ä¿®æ”¹å’Œé‡æ–°å¸ƒå±€ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†3D Ğ³Ğ°ÑƒÑæ¨¡å‹æ¥è¡¨ç¤ºäººå¤´ï¼Œå¹¶åœ¨æ¯ä¸ªGAUSSIANä¸­åµŒå…¥ä¸€ä¸ªè½»é‡çº§çš„ä¸‰é¢æ¿å·ç§¯ï¼Œä»¥ç¼–ç ç©ºé—´ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨3DMMå‚æ•°åŒ–GAUSSIANåœ¨2D UVç©ºé—´ï¼Œä½¿å¾—diffusionæ¨¡å‹å¯ä»¥æœ‰æ•ˆåœ°ç”¨äº3Då¤´åƒç”Ÿæˆã€‚è¿™ç§æ–¹æ³•å¯ä»¥ç”Ÿæˆå¤šæ ·åŒ–ã€çœŸå®çš„3Däººå¤´ï¼Œå¹¶ä¸”å…·æœ‰ç»†åŒ–çš„ç¼–è¾‘åŠŸèƒ½ï¼Œå¯ä»¥ä¿®æ”¹äººå¤´çš„ facial ç‰¹å¾å’Œè¡¨æƒ…ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="ScAR-Scaling-Adversarial-Robustness-for-LiDAR-Object-Detection"><a href="#ScAR-Scaling-Adversarial-Robustness-for-LiDAR-Object-Detection" class="headerlink" title="ScAR: Scaling Adversarial Robustness for LiDAR Object Detection"></a>ScAR: Scaling Adversarial Robustness for LiDAR Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03085">http://arxiv.org/abs/2312.03085</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaohulugo/ScAR-Scaling-Adversarial-Robustness-for-LiDAR-Object-Detection">https://github.com/xiaohulugo/ScAR-Scaling-Adversarial-Robustness-for-LiDAR-Object-Detection</a></li>
<li>paper_authors: Xiaohu Lu, Hayder Radha<br>For:* The paper is written to improve the adversarial robustness of LiDAR object detection models.Methods:* The paper proposes a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection, which uses three black-box scaling adversarial attack methods based on available information: model-aware attack, distribution-aware attack, and blind attack.Results:* The proposed method is effective in improving the modelâ€™s robustness against scaling adversarial attacks, as demonstrated by comparison with other methods on public datasets under different 3D object detection architectures.<details>
<summary>Abstract</summary>
The adversarial robustness of a model is its ability to resist adversarial attacks in the form of small perturbations to input data. Universal adversarial attack methods such as Fast Sign Gradient Method (FSGM) and Projected Gradient Descend (PGD) are popular for LiDAR object detection, but they are often deficient compared to task-specific adversarial attacks. Additionally, these universal methods typically require unrestricted access to the model's information, which is difficult to obtain in real-world applications. To address these limitations, we present a black-box Scaling Adversarial Robustness (ScAR) method for LiDAR object detection. By analyzing the statistical characteristics of 3D object detection datasets such as KITTI, Waymo, and nuScenes, we have found that the model's prediction is sensitive to scaling of 3D instances. We propose three black-box scaling adversarial attack methods based on the available information: model-aware attack, distribution-aware attack, and blind attack. We also introduce a strategy for generating scaling adversarial examples to improve the model's robustness against these three scaling adversarial attacks. Comparison with other methods on public datasets under different 3D object detection architectures demonstrates the effectiveness of our proposed method.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ¨¡å‹çš„å¯¹æŠ—æ”»å‡» robustness æ˜¯æŒ‡æ¨¡å‹èƒ½å¤ŸæŠµæŒ¡å°å˜æ¢çš„è¾“å…¥æ•°æ®ä¸Šçš„æ”»å‡»ã€‚é€šç”¨å¯¹æŠ—æ”»å‡»æ–¹æ³•å¦‚ Fast Sign Gradient Methodï¼ˆFSGMï¼‰å’Œ Projected Gradient Descendï¼ˆPGDï¼‰åœ¨ LiDAR å¯¹è±¡æ£€æµ‹ä¸­å¾ˆå—æ¬¢è¿ï¼Œä½†å®ƒä»¬é€šå¸¸æ¯”ä»»åŠ¡ç‰¹å®šçš„å¯¹æŠ—æ”»å‡»æ›´å¼±ã€‚æ­¤å¤–ï¼Œè¿™äº›é€šç”¨æ–¹æ³•é€šå¸¸éœ€è¦è·å–æ¨¡å‹çš„å®Œæ•´ä¿¡æ¯ï¼Œè¿™åœ¨å®é™…åº”ç”¨ä¸­å¾ˆéš¾å®ç°ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†é»‘ç›’ Scaling Adversarial Robustnessï¼ˆScARï¼‰æ–¹æ³•ã€‚é€šè¿‡åˆ†æ3Då¯¹è±¡æ£€æµ‹æ•°æ®é›† such as KITTIã€Waymo å’Œ nuScenes çš„ç»Ÿè®¡ç‰¹å¾ï¼Œæˆ‘ä»¬å‘ç°æ¨¡å‹å¯¹ç¼©æ”¾3Då®ä¾‹çš„é¢„æµ‹æ˜¯æ•æ„Ÿçš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸‰ç§é»‘ç›’ç¼©æ”¾å¯¹æŠ—æ”»å‡»æ–¹æ³•ï¼ŒåŸºäºå¯ç”¨çš„ä¿¡æ¯ï¼šæ¨¡å‹æ„è¯†æ”»å‡»ã€åˆ†å¸ƒæ„è¯†æ”»å‡»å’Œç›²ç›®æ”»å‡»ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº†ä¸€ç§ç”Ÿæˆç¼©æ”¾å¯¹æŠ—ä¾‹çš„ç­–ç•¥ï¼Œä»¥æé«˜æ¨¡å‹å¯¹è¿™ä¸‰ç§ç¼©æ”¾å¯¹æŠ—æ”»å‡»çš„Robustnessã€‚ä¸å…¶ä»–æ–¹æ³•åœ¨ä¸åŒçš„3Då¯¹è±¡æ£€æµ‹æ¶æ„ä¸‹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æå‡ºçš„æ–¹æ³•æ›´æœ‰æ•ˆã€‚â€
</details></li>
</ul>
<hr>
<h2 id="LooseControl-Lifting-ControlNet-for-Generalized-Depth-Conditioning"><a href="#LooseControl-Lifting-ControlNet-for-Generalized-Depth-Conditioning" class="headerlink" title="LooseControl: Lifting ControlNet for Generalized Depth Conditioning"></a>LooseControl: Lifting ControlNet for Generalized Depth Conditioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03079">http://arxiv.org/abs/2312.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shariq Farooq Bhat, Niloy J. Mitra, Peter Wonka</li>
<li>For: LooseControl is designed to enable generalized depth conditioning for diffusion-based image generation, allowing users to create complex environments with only boundary conditions or 3D box controls.* Methods: The paper introduces a generalized version of depth conditioning that enables scene boundary control and 3D box control for specifying layout locations of target objects, along with two editing mechanisms (3D box editing and attribute editing) to refine the results.* Results: Extensive tests and comparisons with baselines demonstrate the generality of LooseControl, and the authors believe it can become an important design tool for creating complex environments and be extended to other forms of guidance channels.Hereâ€™s the Chinese translation of the three points:* For: LooseControl æ˜¯ç”¨äºå¯ç”¨æ™®é€šåŒ–æ·±åº¦æ¡ä»¶çš„æ‰©å±•æ€§å›¾åƒç”Ÿæˆï¼Œè®©ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ¡ä»¶æˆ– 3D ç›’å­æ§åˆ¶åˆ›å»ºå¤æ‚çš„ç¯å¢ƒã€‚* Methods: paper å¼•å…¥äº†ä¸€ç§æ™®é€šåŒ–çš„æ·±åº¦æ¡ä»¶ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ§åˆ¶å’Œ 3D ç›’å­æ§åˆ¶æŒ‡å®šç›®æ ‡å¯¹è±¡çš„å¸ƒå±€ä½ç½®ï¼ŒåŒæ—¶æä¾›äº†ä¸‰ç§ç¼–è¾‘æœºåˆ¶ï¼ˆ3D ç›’å­ç¼–è¾‘ã€ç‰¹å¾ç¼–è¾‘ï¼‰æ¥ç»†åŒ–ç»“æœã€‚* Results: å¹¿æ³›çš„æµ‹è¯•å’Œæ¯”è¾ƒåŸºçº¿è¡¨æ˜ LooseControl çš„ä¸€èˆ¬æ€§ï¼Œä½œè€…ä»¬è®¤ä¸ºå®ƒå¯ä»¥æˆä¸ºåˆ›å»ºå¤æ‚ç¯å¢ƒçš„é‡è¦è®¾è®¡å·¥å…·ï¼Œå¹¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–æŒ‡å¯¼é€šé“ã€‚<details>
<summary>Abstract</summary>
We present LooseControl to allow generalized depth conditioning for diffusion-based image generation. ControlNet, the SOTA for depth-conditioned image generation, produces remarkable results but relies on having access to detailed depth maps for guidance. Creating such exact depth maps, in many scenarios, is challenging. This paper introduces a generalized version of depth conditioning that enables many new content-creation workflows. Specifically, we allow (C1) scene boundary control for loosely specifying scenes with only boundary conditions, and (C2) 3D box control for specifying layout locations of the target objects rather than the exact shape and appearance of the objects. Using LooseControl, along with text guidance, users can create complex environments (e.g., rooms, street views, etc.) by specifying only scene boundaries and locations of primary objects. Further, we provide two editing mechanisms to refine the results: (E1) 3D box editing enables the user to refine images by changing, adding, or removing boxes while freezing the style of the image. This yields minimal changes apart from changes induced by the edited boxes. (E2) Attribute editing proposes possible editing directions to change one particular aspect of the scene, such as the overall object density or a particular object. Extensive tests and comparisons with baselines demonstrate the generality of our method. We believe that LooseControl can become an important design tool for easily creating complex environments and be extended to other forms of guidance channels. Code and more information are available at https://shariqfarooq123.github.io/loose-control/ .
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºLooseControlï¼Œä»¥å…è®¸é€šç”¨æ·±åº¦conditioningçš„æ‰©å±•ã€‚ControlNetï¼Œå½“å‰æœ€ä½³çš„æ·±åº¦conditionedå›¾åƒç”Ÿæˆæ–¹æ³•ï¼Œç”Ÿæˆäº†æƒŠäººçš„ç»“æœï¼Œä½†æ˜¯å®ƒéœ€è¦ç²¾ç¡®çš„æ·±åº¦åœ°å›¾ä½œä¸ºå¯¼èˆªã€‚åœ¨è®¸å¤šåœºæ™¯ä¸‹ï¼Œåˆ›å»ºè¿™äº›ç²¾ç¡®çš„æ·±åº¦åœ°å›¾æ˜¯å›°éš¾çš„ã€‚è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§é€šç”¨çš„æ·±åº¦conditioningæ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·é€šè¿‡è¾¹ç•Œæ§åˆ¶å’Œ3Dç›’æ§åˆ¶æ¥åˆ›å»ºæ›´å¤šçš„å†…å®¹åˆ›ä½œ workflowã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…è®¸ç”¨æˆ·æ ¹æ®åœºæ™¯è¾¹ç•Œè¿›è¡Œä¸å…·ä½“çš„åœºæ™¯æ§åˆ¶ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡3Dç›’æ§åˆ¶æ¥å®šä¹‰ç›®æ ‡å¯¹è±¡çš„å¸ƒå±€ä½ç½®ï¼Œè€Œä¸æ˜¯å…·ä½“çš„å½¢çŠ¶å’Œå¤–è§‚ã€‚ä½¿ç”¨LooseControlï¼ŒåŒæ—¶æä¾›æ–‡æœ¬æŒ‡å¯¼ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æŒ‡å®šåœºæ™¯è¾¹ç•Œå’Œç›®æ ‡å¯¹è±¡çš„ä½ç½®æ¥åˆ›å»ºå¤æ‚çš„ç¯å¢ƒï¼Œä¾‹å¦‚æˆ¿é—´ã€è¡—æ™¯ç­‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸¤ç§ç¼–è¾‘æœºåˆ¶æ¥ç»†åŒ–ç»“æœï¼š(E1) 3Dç›’ç¼–è¾‘å…è®¸ç”¨æˆ·åœ¨ä¿æŒå›¾åƒé£æ ¼ä¸å˜çš„æƒ…å†µä¸‹ï¼Œæ”¹å˜ã€æ·»åŠ æˆ–ç§»é™¤ç›’å­ï¼Œä»è€Œå¯¼è‡´æœ€å°çš„å˜åŒ–ã€‚(E2) ç‰¹å¾ç¼–è¾‘æä¾›äº†å¯èƒ½çš„ç¼–è¾‘æ–¹å‘ï¼Œä»¥æ”¹å˜åœºæ™¯ä¸­æŸä¸ªç‰¹å®šçš„ç‰¹å¾ï¼Œä¾‹å¦‚æ€»ä½“å¯¹è±¡å¯†åº¦æˆ–ç‰¹å®šå¯¹è±¡ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æµ‹è¯•å’Œæ¯”è¾ƒï¼Œè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„é€šç”¨æ€§ã€‚æˆ‘ä»¬è®¤ä¸ºLooseControlå¯ä»¥æˆä¸ºè½»æ¾åˆ›å»ºå¤æ‚ç¯å¢ƒçš„è®¾è®¡å·¥å…·ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å¯¼èˆªé€šé“ã€‚ä»£ç å’Œæ›´å¤šä¿¡æ¯å¯ä»¥åœ¨https://shariqfarooq123.github.io/loose-control/ æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="ReconFusion-3D-Reconstruction-with-Diffusion-Priors"><a href="#ReconFusion-3D-Reconstruction-with-Diffusion-Priors" class="headerlink" title="ReconFusion: 3D Reconstruction with Diffusion Priors"></a>ReconFusion: 3D Reconstruction with Diffusion Priors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02981">http://arxiv.org/abs/2312.02981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, Aleksander Holynski</li>
<li>for: ç”¨äºé‡å»ºçœŸå®ä¸–ç•Œåœºæ™¯ï¼Œåªéœ€è¦å‡ å¼ ç…§ç‰‡ã€‚</li>
<li>methods: ä½¿ç”¨ååŠ©æ‰©æ•£å‡è®¾æ¥Synthesize realistic geometryå’Œ textureï¼Œå¹¶ä¸”ä¿æŒè§‚å¯Ÿåˆ°çš„å¤–è§‚ã€‚</li>
<li>results: åœ¨å¤šç§çœŸå®ä¸–ç•Œåœºæ™¯ä¸­ï¼Œè¡¨ç°å‡ºæ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œæ¯”å¦‚å‰å‘åœºæ™¯å’Œ360åº¦åœºæ™¯ã€‚<details>
<summary>Abstract</summary>
3D reconstruction methods such as Neural Radiance Fields (NeRFs) excel at rendering photorealistic novel views of complex scenes. However, recovering a high-quality NeRF typically requires tens to hundreds of input images, resulting in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.
</details>
<details>
<summary>æ‘˜è¦</summary>
3Dé‡å»ºæ–¹æ³•å¦‚Neural Radiance Fieldsï¼ˆNeRFsï¼‰èƒ½å¤Ÿç”Ÿæˆé«˜å“è´¨çš„æ–°è§†å›¾å›¾åƒï¼Œä½†æ˜¯è·å¾—é«˜è´¨é‡NeRFé€šå¸¸éœ€è¦æ•°ååˆ°æ•°ç™¾ä¸ªè¾“å…¥å›¾åƒï¼Œè¿™ Result in a time-consuming capture process. We present ReconFusion to reconstruct real-world scenes using only a few photos. Our approach leverages a diffusion prior for novel view synthesis, trained on synthetic and multiview datasets, which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images. Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.Here's the breakdown of the translation:* 3Dé‡å»ºæ–¹æ³• (3D reconstruction methods) -> 3Dé‡å»ºæ–¹æ³• (3D reconstruction methods)* Neural Radiance Fields (NeRFs) -> ç¥ç»é‡‡æ ·åœº (NeRFs)* èƒ½å¤Ÿç”Ÿæˆé«˜å“è´¨çš„æ–°è§†å›¾å›¾åƒ (able to generate high-quality novel views) -> èƒ½å¤Ÿç”Ÿæˆé«˜å“è´¨çš„æ–°è§†å›¾å›¾åƒ (able to generate high-quality novel views)* æ•°ååˆ°æ•°ç™¾ä¸ªè¾“å…¥å›¾åƒ (tens to hundreds of input images) -> æ•°ååˆ°æ•°ç™¾ä¸ªè¾“å…¥å›¾åƒ (tens to hundreds of input images)* Result in a time-consuming capture process -> å¯¼è‡´æ—¶é—´æ¶ˆè€—çš„æ•æ‰è¿‡ç¨‹ (leading to a time-consuming capture process)* We present ReconFusion -> æˆ‘ä»¬æå‡ºReconFusion (we propose ReconFusion)*  leverages a diffusion prior for novel view synthesis -> åˆ©ç”¨åˆ†æ•£é¢„æµ‹å™¨è¿›è¡Œæ–°è§†å›¾åˆæˆ (leveraging a diffusion prior for novel view synthesis)* trained on synthetic and multiview datasets -> åœ¨ sinteticå’Œå¤šè§†å›¾æ•°æ®ä¸Šè®­ç»ƒ (trained on synthetic and multiview datasets)* which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images -> å¯ä»¥åœ¨æ–°çš„æ‘„åƒæœºä½ç½®ä¸Šæ­£åˆ™åŒ–åŸºäºNeRFçš„3Dé‡å»ºç®¡é“ (which regularizes a NeRF-based 3D reconstruction pipeline at novel camera poses beyond those captured by the set of input images)* Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions. -> æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸å¤Ÿçº¦æŸçš„åŒºåŸŸä¸­åˆæˆå®ç° geometryå’Œtextureï¼ŒåŒæ—¶ä¿ç•™è§‚å¯Ÿåˆ°çš„åŒºåŸŸçš„å¤–è§‚ (Our method synthesizes realistic geometry and texture in underconstrained regions while preserving the appearance of observed regions.)* We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches. -> æˆ‘ä»¬åœ¨ä¸åŒçš„å®é™…åœºæ™¯ä¸­è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬å‰è§†åœºæ™¯å’Œ360åº¦åœºæ™¯ï¼Œå¹¶è¯æ˜äº†ä»¥å‰çš„å‡ è§†NeRFé‡å»ºæ–¹æ³•çš„æ€§èƒ½æé«˜ (We perform an extensive evaluation across various real-world datasets, including forward-facing and 360-degree scenes, demonstrating significant performance improvements over previous few-view NeRF reconstruction approaches.)
</details></li>
</ul>
<hr>
<h2 id="GPT4Point-A-Unified-Framework-for-Point-Language-Understanding-and-Generation"><a href="#GPT4Point-A-Unified-Framework-for-Point-Language-Understanding-and-Generation" class="headerlink" title="GPT4Point: A Unified Framework for Point-Language Understanding and Generation"></a>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02980">http://arxiv.org/abs/2312.02980</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong Wu, Jiaqi Wang, Dahua Lin, Hengshuang Zhao</li>
<li>For: The paper aims to improve the understanding of 3D objects by developing a groundbreaking point-language multimodal model called GPT4Point, which can execute various point-text reference tasks and generate high-quality 3D objects from low-quality point-text features.* Methods: The paper introduces GPT4Point, a powerful 3D multimodal language model that utilizes the MLLM framework and is equipped with advanced capabilities for controllable 3D generation. The model is trained on a large-scale database of 1M objects from the Objaverse-XL dataset, which is constructed using the Pyramid-XL dataset annotation engine.* Results: The paper demonstrates the superior performance of GPT4Point in understanding and generation tasks, achieving high-quality results in point-cloud captioning and Q&amp;A, and maintaining the geometric shapes and colors of the 3D objects.<details>
<summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework. GPT4Point as a powerful 3D MLLM seamlessly can execute a variety of point-text reference tasks such as point-cloud captioning and Q&A. Additionally, GPT4Point is equipped with advanced capabilities for controllable 3D generation, it can get high-quality results through a low-quality point-text feature maintaining the geometric shapes and colors. To support the expansive needs of 3D object-text pairs, we develop Pyramid-XL, a point-language dataset annotation engine. It constructs a large-scale database over 1M objects of varied text granularity levels from the Objaverse-XL dataset, essential for training GPT4Point. A comprehensive benchmark has been proposed to evaluate 3D point-language understanding capabilities. In extensive evaluations, GPT4Point has demonstrated superior performance in understanding and generation.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šæ¨¡å¼å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰åœ¨2Då›¾åƒæ–‡æœ¬ç†è§£å’Œç”Ÿæˆæ–¹é¢å…·æœ‰å‡ºè‰²çš„è¡¨ç°ï¼Œä½†å®ƒä»¬å¯¹3Dä¸–ç•Œçš„ç†è§£ä»ç„¶æœ‰é™ï¼Œè¿™é™åˆ¶äº†3Dè¯­è¨€ç†è§£å’Œç”Ÿæˆçš„è¿›æ­¥ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†GPT4Pointï¼Œä¸€ç§åˆ›æ–°çš„å¤šç‚¹è¯­è¨€å¤šæ¨¡å¼æ¨¡å‹ï¼Œä¸“é—¨é’ˆå¯¹MLLMæ¡†æ¶ä¸­çš„ç»Ÿä¸€3Dç‰©ä½“ç†è§£å’Œç”Ÿæˆã€‚GPT4Pointä½œä¸ºå¼ºå¤§çš„3D MLLMï¼Œå¯ä»¥è½»æ¾æ‰§è¡Œå¤šç‚¹æ–‡æœ¬å‚è€ƒä»»åŠ¡ï¼Œå¦‚ç‚¹äº‘captioningå’ŒQ&Aã€‚æ­¤å¤–ï¼ŒGPT4Pointè¿˜å…·æœ‰é«˜çº§å¯æ§3Dç”Ÿæˆèƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ä½è´¨é‡ç‚¹æ–‡æœ¬ç‰¹å¾è·å¾—é«˜è´¨é‡ç»“æœï¼Œä¿æŒå‡ ä½•å½¢çŠ¶å’Œé¢œè‰²ã€‚ä¸ºæ»¡è¶³3Dç‰©ä½“-æ–‡æœ¬å¯¹çš„å¹¿æ³›éœ€æ±‚ï¼Œæˆ‘ä»¬å¼€å‘äº†Pyramid-XLï¼Œä¸€ç§ç‚¹è¯­è¨€æ•°æ®é›†æ ‡æ³¨å·¥å…·ã€‚å®ƒæ„å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®åº“ï¼ŒåŒ…å«100ä¸‡ä¸ªä¸åŒæ–‡æœ¬ç»†åˆ†æ°´å¹³çš„ç‰©ä½“ï¼Œè¿™äº›æ•°æ®åº“æ˜¯ç”¨äºè®­ç»ƒGPT4Pointçš„å¿…å¤‡ã€‚æˆ‘ä»¬å»ºè®®äº†ä¸€ä¸ªå®Œæ•´çš„è¯„ä»·æ ‡å‡†ï¼Œç”¨äºè¯„ä¼°3Dç‚¹è¯­è¨€ç†è§£èƒ½åŠ›ã€‚åœ¨å¹¿æ³›çš„è¯„ä¼°ä¸­ï¼ŒGPT4Pointè¡¨ç°å‡ºè‰²ï¼Œåœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢å‡è¾¾åˆ°äº†ä¼˜ç§€çš„æˆç»©ã€‚
</details></li>
</ul>
<hr>
<h2 id="DiffusionPCR-Diffusion-Models-for-Robust-Multi-Step-Point-Cloud-Registration"><a href="#DiffusionPCR-Diffusion-Models-for-Robust-Multi-Step-Point-Cloud-Registration" class="headerlink" title="DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration"></a>DiffusionPCR: Diffusion Models for Robust Multi-Step Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03053">http://arxiv.org/abs/2312.03053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhi Chen, Yufan Ren, Tong Zhang, Zheng Dang, Wenbing Tao, Sabine SÃ¼sstrunk, Mathieu Salzmann</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ‰©æ•£è¿‡ç¨‹çš„ç‚¹äº‘æ³¨å†Œæ–¹æ³•ï¼Œèƒ½å¤Ÿé«˜æ•ˆåœ°å°†ä¸¤ä¸ªç‚¹äº‘æ³¨å†Œåˆ°åŒä¸€ä¸ªå‚è€ƒç³»ç»Ÿä¸­ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§åŸºäºæ‰©æ•£è¿‡ç¨‹çš„ç‚¹äº‘æ³¨å†Œæ–¹æ³•ï¼ŒåŒ…æ‹¬å°†ç‚¹äº‘æ³¨å†Œé—®é¢˜è½¬åŒ–ä¸ºä¸€ç§æ‰©æ•£è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ä¸€ç§å¸¦æœ‰è§„èŒƒçš„æ‰©æ•£æ¨¡å‹æ¥æŠ‘åˆ¶å™ªå£°ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½¿ç”¨æ‰©æ•£PCRæ–¹æ³•å¯ä»¥æé«˜ç‚¹äº‘æ³¨å†Œçš„å‡†ç¡®ç‡ï¼Œå¹¶ä¸”åœ¨3DMatchå’Œ3DLoMatch datasetä¸Šè¾¾åˆ°äº†çŠ¶æ€è‰ºæœ¯çš„æ³¨å†Œè®°å¿†ç‡ï¼ˆ95.3%&#x2F;81.6%ï¼‰ã€‚<details>
<summary>Abstract</summary>
Point Cloud Registration (PCR) estimates the relative rigid transformation between two point clouds. We propose formulating PCR as a denoising diffusion probabilistic process, mapping noisy transformations to the ground truth. However, using diffusion models for PCR has nontrivial challenges, such as adapting a generative model to a discriminative task and leveraging the estimated nonlinear transformation from the previous step. Instead of training a diffusion model to directly map pure noise to ground truth, we map the predictions of an off-the-shelf PCR model to ground truth. The predictions of off-the-shelf models are often imperfect, especially in challenging cases where the two points clouds have low overlap, and thus could be seen as noisy versions of the real rigid transformation. In addition, we transform the rotation matrix into a spherical linear space for interpolation between samples in the forward process, and convert rigid transformations into auxiliary information to implicitly exploit last-step estimations in the reverse process. As a result, conditioned on time step, the denoising model adapts to the increasing accuracy across steps and refines registrations. Our extensive experiments showcase the effectiveness of our DiffusionPCR, yielding state-of-the-art registration recall rates (95.3%/81.6%) on 3DMatch and 3DLoMatch. The code will be made public upon publication.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç‚¹äº‘æ³¨å†Œï¼ˆPCRï¼‰ä¼°è®¡ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„ç›¸å¯¹ç¨³å®šå˜æ¢ã€‚æˆ‘ä»¬æè®®å°†PCRè§†ä¸ºä¸€ç§æ»¤æ³¢æ¨æ¼”è¿‡ç¨‹ï¼Œå°†å™ªå£°å˜æ¢æ˜ å°„åˆ°çœŸå®çš„å˜æ¢ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ¨æ¼”æ¨¡å‹è¿›è¡ŒPCRæœ‰ä¸€äº›ä¸rivialçš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚å°†ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºæ¨è®ºä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨ä¸Šä¸€æ­¥ä¼°è®¡çš„éçº¿æ€§å˜æ¢ã€‚è€Œä¸æ˜¯ç›´æ¥å°†çº¯å™ªå£°æ˜ å°„åˆ°çœŸå®çš„å˜æ¢ï¼Œæˆ‘ä»¬å°†å¤–éƒ¨PCRæ¨¡å‹çš„é¢„æµ‹æ˜ å°„åˆ°çœŸå®çš„å˜æ¢ã€‚å¤–éƒ¨PCRæ¨¡å‹çš„é¢„æµ‹é€šå¸¸ä¸å‡†ç¡®ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸¤ä¸ªç‚¹äº‘ä¹‹é—´çš„è¦†ç›–ç‡ä½çš„æƒ…å†µä¸‹ï¼Œå› æ­¤å¯ä»¥çœ‹ä½œå™ªå£°ç‰ˆæœ¬çš„çœŸæ­£ç¨³å®šå˜æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºçƒé¢çº¿æ€§ç©ºé—´ä¸­çš„ interpolate æ“ä½œï¼Œå¹¶å°†ç¨³å®šå˜æ¢è½¬æ¢ä¸ºauxiliaryä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨åå‘è¿‡ç¨‹ä¸­éšå¼åœ°åˆ©ç”¨ä¸Šä¸€æ­¥çš„ä¼°è®¡ã€‚å› æ­¤ï¼Œæ ¹æ®æ—¶é—´æ­¥é•¿ï¼Œæ»¤æ³¢æ¨¡å‹é€‚åº”åˆ°å¢åŠ çš„å‡†ç¡®æ€§ï¼Œå¹¶åœ¨æ³¨å†Œè¿‡ç¨‹ä¸­è¿›è¡Œç»†åŒ–ã€‚æˆ‘ä»¬çš„å¹¿æ³›çš„å®éªŒæ˜¾ç¤ºäº†DiffusionPCRçš„æ•ˆæœï¼Œå¾—åˆ°äº†3DMatchå’Œ3DLoMatchä¸Šçš„æ³¨å†Œè®°å¿†ç‡ï¼ˆ95.3%/81.6%ï¼‰ã€‚ä»£ç å°†åœ¨å‡ºç‰ˆæ—¶å…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos"><a href="#GauHuman-Articulated-Gaussian-Splatting-from-Monocular-Human-Videos" class="headerlink" title="GauHuman: Articulated Gaussian Splatting from Monocular Human Videos"></a>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02973">http://arxiv.org/abs/2312.02973</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skhu101/gauhuman">https://github.com/skhu101/gauhuman</a></li>
<li>paper_authors: Shoukang Hu, Ziwei Liu</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å¿«é€Ÿè®­ç»ƒï¼ˆ1~2åˆ†é’Ÿï¼‰å’Œå®æ—¶æ¸²æŸ“ï¼ˆæœ€å¤š189å¸§&#x2F;ç§’ï¼‰çš„3Däººä½“æ¨¡å‹ï¼Œæ¯”æ—¢æœ‰çš„åŸºäºNeRFçš„åŠ implicitè¡¨ç¤ºæ¨¡å‹æ¡†æ¶éœ€è¦å‡ ä¸ªå°æ—¶çš„è®­ç»ƒå’Œæ¯å¸§æ¸²æŸ“ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ Gaussian Splatting æ–¹æ³•ï¼Œåœ¨ canonical space ä¸­ç¼–ç  Gaussian Splattingï¼Œå°†3D Gaussians ä» canonical space è½¬æ¢åˆ° posed space ä¸­ä½¿ç”¨çº¿æ€§æ··åˆçš®è‚¤ï¼ˆLBSï¼‰ï¼Œå¹¶è®¾è®¡äº†ç²¾ç»†çš„äººä½“è¯¦ç»†ä¿¡æ¯å­¦ä¹ æ¨¡å—ã€‚</li>
<li>results: å¹¿å¤§å®éªŒè¡¨æ˜ï¼ŒGauHuman å¯ä»¥åœ¨ ZJU_Mocap å’Œ MonoCap æ•°æ®é›†ä¸Šè¾¾åˆ°å½“ä»Šæœ€ä½³æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰å¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“é€Ÿåº¦ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ— éœ€ç‰ºç‰²æ¸²æŸ“è´¨é‡ï¼ŒGauHuman å¯ä»¥å¿«é€Ÿæ¨¡å‹3Däººä½“æ¼”å‘˜çš„ ~13k 3D Gaussiansã€‚<details>
<summary>Abstract</summary>
We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement modules are designed to learn fine details of 3D humans under negligible computational cost. Moreover, to enable fast optimization of GauHuman, we initialize and prune 3D Gaussians with 3D human prior, while splitting/cloning via KL divergence guidance, along with a novel merge operation for further speeding up. Extensive experiments on ZJU_Mocap and MonoCap datasets demonstrate that GauHuman achieves state-of-the-art performance quantitatively and qualitatively with fast training and real-time rendering speed. Notably, without sacrificing rendering quality, GauHuman can fast model the 3D human performer with ~13k 3D Gaussians.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»GauHumanï¼Œä¸€ä¸ª3Däººä½“æ¨¡å‹ï¼Œä½¿ç”¨é«˜æ–¯æ‰©æ•£æ¥å®ç°å¿«é€Ÿè®­ç»ƒï¼ˆ1-2åˆ†é’Ÿï¼‰å’Œå®æ—¶æ¸²æŸ“ï¼ˆæœ€é«˜189å¸§/ç§’ï¼‰ã€‚ä¸ç°æœ‰åŸºäºNeRFçš„å‡ ä½•è¡¨ç¤ºæ¨¡å‹æ¡†æ¶ç›¸æ¯”ï¼ŒGauHumanéœ€è¦è®­ç»ƒæ—¶é—´åªéœ€è¦å‡ åˆ†é’Ÿï¼Œå¹¶ä¸”æ¯å¸§æ¸²æŸ“æ—¶é—´åªéœ€è¦å‡ ç§’é’Ÿã€‚ç‰¹åˆ«æ˜¯ï¼ŒGauHumanåœ¨ canonical spaceä¸­ç¼–ç é«˜æ–¯æ‰©æ•£ï¼Œå°†3Dé«˜æ–¯ä» canonical space è½¬æ¢ä¸ºposed space é€šè¿‡çº¿æ€§æ··åˆçš®è‚¤ï¼ˆLBSï¼‰ï¼Œå¹¶è®¾è®¡äº†æœ‰æ•ˆçš„poseå’ŒLBSä¿®æ­£æ¨¡å—ï¼Œä»¥å­¦ä¹ 3Däººä½“çš„ç»†èŠ‚ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¿«é€Ÿä¼˜åŒ–GauHumanï¼Œæˆ‘ä»¬åœ¨åˆå§‹åŒ–å’Œå‰ªè¾‘3Dé«˜æ–¯æ—¶ä½¿ç”¨3Däººä½“å…ˆéªŒï¼Œå¹¶é€šè¿‡æ‹Ÿåˆæ“ä½œè¿›ä¸€æ­¥åŠ é€Ÿã€‚æˆ‘ä»¬å¯¹ZJU_Mocapå’ŒMonoCapæ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜GauHumanåœ¨é‡åŒ–å’Œè´¨é‡ä¸Šå‡è¾¾åˆ°äº†é¢†å…ˆæ°´å¹³ï¼ŒåŒæ—¶å…·æœ‰å¿«é€Ÿè®­ç»ƒå’Œå®æ—¶æ¸²æŸ“çš„ä¼˜ç‚¹ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä¸ sacrificingæ¸²æŸ“è´¨é‡ï¼ŒGauHumanå¯ä»¥å¿«é€Ÿæ¨¡æ‹Ÿ3Däººä½“æ¼”ç¤ºè€…çš„çº¦13kä¸ª3Dé«˜æ–¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="AmbiGen-Generating-Ambigrams-from-Pre-trained-Diffusion-Model"><a href="#AmbiGen-Generating-Ambigrams-from-Pre-trained-Diffusion-Model" class="headerlink" title="AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model"></a>AmbiGen: Generating Ambigrams from Pre-trained Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02967">http://arxiv.org/abs/2312.02967</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boheng Zhao, Rana Hanocka, Raymond A. Yeh</li>
<li>for: ç”Ÿæˆæ‹¼å†™æ—è§‚æ–‡æœ¬çš„æ‹¼å†™æ—è§‚æ–‡æœ¬æ‘˜è¦</li>
<li>methods: ä½¿ç”¨æ·±åº¦å¼—æ´›ä¼Šå¾·IFæ¨¡å‹è¿›è¡Œæ‹¼å†™æ—è§‚æ–‡æœ¬ç”Ÿæˆï¼Œå¹¶ä¼˜åŒ–å­—ç¬¦çš„è½®å»“ä»¥æé«˜åœ¨ä¸¤ä¸ªè§†å›¾è§’åº¦ä¸‹çš„å¯è¯»æ€§</li>
<li>results: å¯¹äºè‹±è¯­500ä¸ªæœ€å¸¸è§çš„å•è¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ¯”ç°æœ‰çš„æ‹¼å†™æ—è§‚æ–‡æœ¬ç”Ÿæˆæ–¹æ³•é«˜äº11.6%çš„è¯æ³•ç²¾åº¦å’Œå°‘äº41.9%çš„ç¼–è¾‘è·ç¦»<details>
<summary>Abstract</summary>
Ambigrams are calligraphic designs that have different meanings depending on the viewing orientation. Creating ambigrams is a challenging task even for skilled artists, as it requires maintaining the meaning under two different viewpoints at the same time. In this work, we propose to generate ambigrams by distilling a large-scale vision and language diffusion model, namely DeepFloyd IF, to optimize the letters' outline for legibility in the two viewing orientations. Empirically, we demonstrate that our approach outperforms existing ambigram generation methods. On the 500 most common words in English, our method achieves more than an 11.6% increase in word accuracy and at least a 41.9% reduction in edit distance.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€ŠæŠ½è±¡å­—å½¢è®¾è®¡ã€‹æ˜¯ä¸€ç§å­—å½¢è®¾è®¡ï¼Œå®ƒä»¬åœ¨ä¸åŒçš„è§†è§’ä¸‹å…·æœ‰ä¸åŒçš„å«ä¹‰ã€‚åˆ›å»ºæŠ½è±¡å­—å½¢æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå› ä¸ºéœ€è¦åœ¨ä¸¤ä¸ªä¸åŒçš„è§†è§’ä¸‹ä¿æŒå­—å½¢çš„æ„ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨å¤§è§„æ¨¡è§†åŠ›å’Œè¯­è¨€æ‰©æ•£æ¨¡å‹ï¼ˆDeepFloyd IFï¼‰æ¥ä¼˜åŒ–å­—å½¢çš„è½®å»“ï¼Œä»¥ç¡®ä¿åœ¨ä¸¤ä¸ªè§†è§’ä¸‹çš„å¯è¯»æ€§ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨500ä¸ªæœ€å¸¸è§çš„è‹±æ–‡è¯æ±‡ä¸Šè¡¨ç°å‡ºäº†è¶…è¿‡11.6%çš„è¯æ³•ç²¾åº¦æå‡å’Œæœ€å°‘41.9%çš„ç¼–è¾‘è·ç¦»å‡å°‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Diffusion-SS3D-Diffusion-Model-for-Semi-supervised-3D-Object-Detection"><a href="#Diffusion-SS3D-Diffusion-Model-for-Semi-supervised-3D-Object-Detection" class="headerlink" title="Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection"></a>Diffusion-SS3D: Diffusion Model for Semi-supervised 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02966">http://arxiv.org/abs/2312.02966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/luluho1208/diffusion-ss3d">https://github.com/luluho1208/diffusion-ss3d</a></li>
<li>paper_authors: Cheng-Ju Ho, Chen-Hsuan Tai, Yen-Yu Lin, Ming-Hsuan Yang, Yi-Hsuan Tsai</li>
<li>for: æé«˜ semi-supervised 3D ç‰©ä½“æ£€æµ‹ä¸­çš„é²æ£’æ€§å’Œç²¾åº¦ï¼Œ Addressing the limitation of acquiring large-scale 3D bounding box annotations.</li>
<li>methods: ä½¿ç”¨ teacher-student æ¡†æ¶å’Œ pseudo-labeling æŠ€æœ¯ï¼Œå¹¶æå‡ºä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¢åŠ å™ªå£°æ¥æé«˜ pseudo-label è´¨é‡ï¼Œå¹¶å°†æ‰©æ•£æ¨¡å‹çº³å…¥ teacher-student æ¡†æ¶ä¸­ï¼Œä»¥æé«˜ pseudo-label ç”Ÿæˆå’Œæ•´ä¸ª semi-supervised å­¦ä¹ è¿‡ç¨‹ã€‚</li>
<li>results: åœ¨ ScanNet å’Œ SUN RGB-D æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶ demonstrably è¾¾åˆ°äº†ç°æœ‰æ–¹æ³•çš„çŠ¶æ€ç©ºé—´æ€§èƒ½ã€‚ Additionally, we present extensive analysis to understand how our diffusion model design affects performance in semi-supervised learning.<details>
<summary>Abstract</summary>
Semi-supervised object detection is crucial for 3D scene understanding, efficiently addressing the limitation of acquiring large-scale 3D bounding box annotations. Existing methods typically employ a teacher-student framework with pseudo-labeling to leverage unlabeled point clouds. However, producing reliable pseudo-labels in a diverse 3D space still remains challenging. In this work, we propose Diffusion-SS3D, a new perspective of enhancing the quality of pseudo-labels via the diffusion model for semi-supervised 3D object detection. Specifically, we include noises to produce corrupted 3D object size and class label distributions, and then utilize the diffusion model as a denoising process to obtain bounding box outputs. Moreover, we integrate the diffusion model into the teacher-student framework, so that the denoised bounding boxes can be used to improve pseudo-label generation, as well as the entire semi-supervised learning process. We conduct experiments on the ScanNet and SUN RGB-D benchmark datasets to demonstrate that our approach achieves state-of-the-art performance against existing methods. We also present extensive analysis to understand how our diffusion model design affects performance in semi-supervised learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸‰ç§æŒ‡å¯¼ä¸‹çš„å¯¹è±¡æ£€æµ‹æ˜¯ä¸‰ç»´åœºæ™¯ç†è§£çš„å…³é”®ï¼Œå› ä¸ºè·å–å¤§è§„æ¨¡çš„ä¸‰ç»´çŸ©å½¢æ¡†æ³¨é‡Š remains challenging. ç°æœ‰æ–¹æ³•é€šå¸¸é‡‡ç”¨æ•™å¸ˆç”Ÿ frameworks with pseudo-labeling æ¥åˆ©ç”¨æ— æ ‡æ³¨ç‚¹äº‘æ•°æ®. ç„¶è€Œï¼Œåœ¨å¤šæ ·åŒ–çš„ä¸‰ç»´ç©ºé—´ä¸­ç”Ÿæˆå¯é çš„pseudo-labelsä»ç„¶æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æè®®Diffusion-SS3Dï¼Œä¸€ç§æ–°çš„æ‰©å±•pseudo-labelçš„è´¨é‡via diffusion model for semi-supervised 3D object detection. Specifically, we add noise to produce corrupted 3D object size and class label distributions, and then use the diffusion model as a denoising process to obtain bounding box outputs. æ­¤å¤–ï¼Œæˆ‘ä»¬å°†diffusion model integrate into the teacher-student framework, so that the denoised bounding boxes can be used to improve pseudo-label generation, as well as the entire semi-supervised learning process. We conduct experiments on the ScanNet and SUN RGB-D benchmark datasets to demonstrate that our approach achieves state-of-the-art performance against existing methods. We also present extensive analysis to understand how our diffusion model design affects performance in semi-supervised learning.
</details></li>
</ul>
<hr>
<h2 id="MVHumanNet-A-Large-scale-Dataset-of-Multi-view-Daily-Dressing-Human-Captures"><a href="#MVHumanNet-A-Large-scale-Dataset-of-Multi-view-Daily-Dressing-Human-Captures" class="headerlink" title="MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures"></a>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02963">http://arxiv.org/abs/2312.02963</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, Shuguang Cui, Xiaoguang Han</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æä¾›ä¸€ä¸ªå¤§è§„æ¨¡çš„3Däººä½“æ•°æ®é›†ï¼Œä»¥ä¾¿è¿›è¡Œå¤§è§„æ¨¡çš„äººä½“æ•°æ®é‡‡é›†å’Œåˆ†æï¼Œå¹¶ä¸”æ¢ç´¢è¿™äº›æ•°æ®é›†åœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§å¤šè§†å›¾äººä½“æ•æ‰ç³»ç»Ÿï¼Œé€šè¿‡è¿™ç§ç³»ç»Ÿï¼Œå¯ä»¥è½»æ¾åœ°æ”¶é›†å¤§è§„æ¨¡çš„é«˜è´¨é‡3Däººä½“æ•°æ®ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å¯¹æ•°æ®è¿›è¡Œäº†å¹¿æ³›çš„æ ‡æ³¨ï¼ŒåŒ…æ‹¬äººä½“ mÃ¡scaraã€æ‘„åƒæœºå‚æ•°ã€2Då’Œ3Då…³é”®ç‚¹ã€SMPL&#x2F;SMPLXå‚æ•°ä»¥åŠç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚</li>
<li>results: ç ”ç©¶è€…é€šè¿‡å¯¹MVHumanNetæ•°æ®é›†è¿›è¡Œäº†å¤šç§è§†è§‰ä»»åŠ¡çš„å®éªŒï¼ŒåŒ…æ‹¬è§†åŠ›ä¸€è‡´åŠ¨ä½œè¯†åˆ«ã€äººä½“NeRFé‡å»ºã€æ–‡æœ¬é©±åŠ¨è§†å›¾ä¸å—é™åˆ¶çš„äººåƒç”Ÿæˆã€2Dè§†å›¾ä¸å—é™åˆ¶çš„äººåƒç”Ÿæˆä»¥åŠ3Däººåƒç”Ÿæˆç­‰ã€‚ç»“æœè¡¨æ˜ï¼ŒMVHumanNetæ•°æ®é›†å¯ä»¥æä¾›å¤§é‡çš„é«˜è´¨é‡3Däººä½“æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥æœ‰æ•ˆåœ°åº”ç”¨äºå¤šç§è§†è§‰ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while remarkable progress has been made with models trained on large-scale synthetic and real-captured object data like Objaverse and MVImgNet, a similar level of progress has not been observed in the domain of human-centric tasks partially due to the lack of a large-scale human dataset. Existing datasets of high-fidelity 3D human capture continue to be mid-sized due to the significant challenges in acquiring large-scale high-quality 3D human data. To bridge this gap, we present MVHumanNet, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using a multi-view human capture system, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. To explore the potential of MVHumanNet in various 2D and 3D visual tasks, we conducted pilot studies on view-consistent action recognition, human NeRF reconstruction, text-driven view-unconstrained human image generation, as well as 2D view-unconstrained human image and 3D avatar generation. Extensive experiments demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet data with annotations will foster further innovations in the domain of 3D human-centric tasks at scale.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ä¸ªæ—¶ä»£ï¼Œå¤§è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„æˆåŠŸå¯ä»¥å½’åŠŸäºå¤§è§„æ¨¡æ•°æ®çš„é©±åŠ¨åŠ›ã€‚ç„¶è€Œï¼Œåœ¨3Dè§†è§‰é¢†åŸŸï¼Œè™½ç„¶å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œä½†åœ¨äººç±»ä¸­å¿ƒä»»åŠ¡é¢†åŸŸå¹¶æ²¡æœ‰è¾¾åˆ°åŒæ ·çš„æ°´å¹³ï¼Œä¸€éƒ¨åˆ†åŸå› æ˜¯å› ä¸ºç¼ºä¹å¤§è§„æ¨¡çš„äººç±»æ•°æ®é›†ã€‚ç°æœ‰çš„é«˜å“è´¨3Däººç±»æ•æ‰æ•°æ®é›†ä»ç„¶è¾ƒå°ï¼Œå› ä¸ºè·å¾—å¤§è§„æ¨¡é«˜è´¨é‡3Däººç±»æ•°æ®çš„æŒ‘æˆ˜å¾ˆå¤§ã€‚ä¸ºäº†bridgingè¿™ä¸ªå·®è·ï¼Œæˆ‘ä»¬æå‡ºäº†MVHumanNetæ•°æ®é›†ï¼Œå®ƒåŒ…æ‹¬4,500ä¸ªäººç±»æ ‡ç­¾çš„å¤šè§†å›¾äººä½“åŠ¨ä½œåºåˆ—ã€‚æˆ‘ä»¬çš„å·¥ä½œçš„ä¸»è¦é‡ç‚¹æ˜¯æ”¶é›†å…·æœ‰å¤§é‡å¤šæ ·åŒ–äººç±»æ ‡ç­¾å’Œæ—¥å¸¸æœè£…çš„äººä½“æ•°æ®ï¼Œä½¿ç”¨å¤šè§†å›¾äººä½“æ•æ‰ç³»ç»Ÿï¼Œä»¥ä¾¿è½»æ¾æ‰©å±•æ•°æ®æ”¶é›†ã€‚æˆ‘ä»¬çš„æ•°æ®é›†åŒ…å«4,500ä¸ªä¸åŒçš„æ—¥å¸¸æœè£…ï¼Œ60,000ä¸ªåŠ¨ä½œåºåˆ—å’Œ645ä¸‡å¸§ï¼Œå¹¶æœ‰å¹¿æ³›çš„æ³¨é‡Šï¼ŒåŒ…æ‹¬äººmaskã€æ‘„åƒå¤´å‚æ•°ã€2Då’Œ3Då…³é”®ç‚¹ã€SMPL/SMPLXå‚æ•°å’Œç›¸åº”çš„æ–‡æœ¬æè¿°ã€‚ä¸ºäº†æ¢ç´¢MVHumanNetåœ¨ä¸åŒçš„2Då’Œ3Dè§†è§‰ä»»åŠ¡ä¸­çš„æ½œåŠ›ï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€äº›è¯•ç‚¹ç ”ç©¶ï¼ŒåŒ…æ‹¬è§†åº¦ä¸€è‡´åŠ¨ä½œè¯†åˆ«ã€äººNeRFé‡å»ºã€æ–‡æœ¬é©±åŠ¨è§†è§’ä¸å—é™åˆ¶çš„äººå›¾ç”Ÿæˆã€2Dè§†è§’ä¸å—é™åˆ¶çš„äººå›¾ç”Ÿæˆå’Œ3Däººåƒç”Ÿæˆã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒMVHumanNetæ•°æ®é›†çš„è§„æ¨¡æä¾›äº†å¤§é‡çš„æ€§èƒ½æå‡å’Œåº”ç”¨åœºæ™¯ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡å‘å¸ƒMVHumanNetæ•°æ®é›†å’Œæ³¨é‡Šçš„å‘å¸ƒï¼Œæ¿€å‘æ›´å¤šåœ¨3Däººç±»ä¸­å¿ƒä»»åŠ¡é¢†åŸŸçš„åˆ›æ–°ã€‚
</details></li>
</ul>
<hr>
<h2 id="HIG-Hierarchical-Interlacement-Graph-Approach-to-Scene-Graph-Generation-in-Video-Understanding"><a href="#HIG-Hierarchical-Interlacement-Graph-Approach-to-Scene-Graph-Generation-in-Video-Understanding" class="headerlink" title="HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding"></a>HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03050">http://arxiv.org/abs/2312.03050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Trong-Thuan Nguyen, Pha Nguyen, Khoa Luu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³è®¡ç®—æœºè§†è§‰ä¸­çš„è§†è§‰åœºæ™¯å†…çš„äº’åŠ¨ç†è§£é—®é¢˜ï¼Œ existing methods åªæ˜¯åŸºäºç®€å•çš„å…³ç³»æ¨¡å‹ï¼Œåœ¨é¢ä¸´å¤šæ ·åŒ–çš„å¤–è§‚ã€æƒ…å†µã€ä½ç½®ã€äº’åŠ¨å’Œå…³ç³»ç­‰æ–¹é¢å—åˆ°é™åˆ¶ï¼Œé™ä½äº†å¯¹å¤æ‚çš„è§†è§‰åŠ¨åŠ›å­¦ä¸­çš„äº’åŠ¨ç†è§£èƒ½åŠ›ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•æ˜¯åŸºäºå±‚æ¬¡ç»“æ„çš„ Hierarchical Interlacement Graph (HIG)ï¼Œå®ƒåˆ©ç”¨ä¸€ä¸ªç»Ÿä¸€å±‚å’Œå›¾åœ¨å±‚æ¬¡ç»“æ„ä¸­æä¾›æ·±å…¥çš„Sceneå˜åŒ–ç†è§£ï¼Œå¹¶åœ¨äº”ä¸ªä¸åŒä»»åŠ¡ä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚</li>
<li>results: å¯¹äºäº”ä¸ªä»»åŠ¡ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†å…¶ä»–æ–¹æ³•çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. To achieve this goal, we first present a new dataset containing Appearance-Situation-Position-Interaction-Relation predicates, named ASPIRe, offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named Hierarchical Interlacement Graph (HIG), which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary>
Visual interactivity understanding within visual scenes presents a significant challenge in computer vision. Existing methods focus on complex interactivities while leveraging a simple relationship model. These methods, however, struggle with a diversity of appearance, situation, position, interaction, and relation in videos. This limitation hinders the ability to fully comprehend the interplay within the complex visual dynamics of subjects. In this paper, we delve into interactivities understanding within visual content by deriving scene graph representations from dense interactivities among humans and objects. To achieve this goal, we first present a new dataset containing Appearance-Situation-Position-Interaction-Relation predicates, named ASPIRe, offering an extensive collection of videos marked by a wide range of interactivities. Then, we propose a new approach named Hierarchical Interlacement Graph (HIG), which leverages a unified layer and graph within a hierarchical structure to provide deep insights into scene changes across five distinct tasks. Our approach demonstrates superior performance to other methods through extensive experiments conducted in various scenarios.Here is the word-for-word translation of the text into Simplified Chinese:è§†è§‰äº’åŠ¨ç†è§£åœ¨è§†è§‰åœºæ™¯ä¸­å­˜åœ¨é‡å¤§æŒ‘æˆ˜ï¼Œç°æœ‰æ–¹æ³•ä¸»è¦å…³æ³¨å¤æ‚çš„äº’åŠ¨å…³ç³»ï¼Œè€Œä½¿ç”¨ç®€å•çš„å…³ç³»æ¨¡å‹ã€‚è¿™äº›æ–¹æ³•å´éš¾ä»¥å¤„ç†è§†é¢‘ä¸­çš„å¤šæ ·æ€§ï¼ŒåŒ…æ‹¬å¤–è§‚ã€æƒ…å†µã€ä½ç½®ã€äº’åŠ¨å’Œå…³ç³»ã€‚è¿™ç§å±€é™æ€§é˜»ç¢äº†å®Œå…¨ç†è§£è§†è§‰åœºæ™¯ä¸­äººç‰©ä¹‹é—´çš„äº’åŠ¨å‰–é¢ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢è§†è§‰å†…å®¹ä¸­çš„äº’åŠ¨ç†è§£ï¼Œé€šè¿‡ dense äº’åŠ¨å…³ç³»ç½‘ç»œç”Ÿæˆåœºæ™¯å›¾è¡¨ç¤ºã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆæä¾›äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåä¸º ASPIReï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†å„ç§äº’åŠ¨çš„è§†é¢‘é›†ï¼Œå¹¶ä¸”æä¾›äº†å„ç§äº’åŠ¨å…³ç³» predicatesã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å±‚æ¬¡ç»“æ„ Graph (HIG)ï¼Œè¯¥æ–¹æ³•åœ¨å±‚æ¬¡ç»“æ„ä¸­å…·æœ‰ä¸€ä¸ªç»Ÿä¸€çš„å±‚æ¬¡ç»“æ„å’Œå›¾ï¼Œä»¥æ·±å…¥æ¢ç´¢è§†é¢‘åœºæ™¯çš„å˜åŒ–ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ›´é«˜çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Choroidalyzer-An-open-source-end-to-end-pipeline-for-choroidal-analysis-in-optical-coherence-tomography"><a href="#Choroidalyzer-An-open-source-end-to-end-pipeline-for-choroidal-analysis-in-optical-coherence-tomography" class="headerlink" title="Choroidalyzer: An open-source, end-to-end pipeline for choroidal analysis in optical coherence tomography"></a>Choroidalyzer: An open-source, end-to-end pipeline for choroidal analysis in optical coherence tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02956">http://arxiv.org/abs/2312.02956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Engelmann, Jamie Burke, Charlene Hamid, Megan Reid-Schachter, Dan Pugh, Neeraj Dhaun, Diana Moukaddem, Lyle Gray, Niall Strang, Paul McGraw, Amos Storkey, Paul J. Steptoe, Stuart King, Tom MacGillivray, Miguel O. Bernabeu, Ian J. C. MacCormick<br>for:The paper aims to develop an open-source, end-to-end pipeline for segmenting the choroid region, vessels, and fovea, and deriving choroidal thickness, area, and vascular index.methods:The authors used 5,600 OCT B-scans from 233 subjects, 6 systemic disease cohorts, and 3 device types, and trained a U-Net deep-learning model to detect the region, vessels, and fovea. They also used state-of-the-art automatic methods for generating region and vessel ground-truths, and manually annotated foveal positions.results:Choroidalyzer achieved excellent segmentation performance for the choroid region (Dice score: internal 0.9789, external 0.9749), vessels (Dice score: internal 0.8817, external 0.8703), and fovea location (mean absolute error: internal 3.9 pixels, external 3.4 pixels). The agreement between Choroidalyzer and two manual graders was comparable to the inter-grader agreement across all metrics. The pipeline was able to accurately extract choroidal thickness, area, and vascular index, with strong correlations (Pearson correlation: internal 0.9754, external 0.9831) and low mean absolute errors (MAE: internal 3.9 pixels, external 3.4 pixels).<details>
<summary>Abstract</summary>
Purpose: To develop Choroidalyzer, an open-source, end-to-end pipeline for segmenting the choroid region, vessels, and fovea, and deriving choroidal thickness, area, and vascular index.   Methods: We used 5,600 OCT B-scans (233 subjects, 6 systemic disease cohorts, 3 device types, 2 manufacturers). To generate region and vessel ground-truths, we used state-of-the-art automatic methods following manual correction of inaccurate segmentations, with foveal positions manually annotated. We trained a U-Net deep-learning model to detect the region, vessels, and fovea to calculate choroid thickness, area, and vascular index in a fovea-centred region of interest. We analysed segmentation agreement (AUC, Dice) and choroid metrics agreement (Pearson, Spearman, mean absolute error (MAE)) in internal and external test sets. We compared Choroidalyzer to two manual graders on a small subset of external test images and examined cases of high error.   Results: Choroidalyzer took 0.299 seconds per image on a standard laptop and achieved excellent region (Dice: internal 0.9789, external 0.9749), very good vessel segmentation performance (Dice: internal 0.8817, external 0.8703) and excellent fovea location prediction (MAE: internal 3.9 pixels, external 3.4 pixels). For thickness, area, and vascular index, Pearson correlations were 0.9754, 0.9815, and 0.8285 (internal) / 0.9831, 0.9779, 0.7948 (external), respectively (all p<0.0001). Choroidalyzer's agreement with graders was comparable to the inter-grader agreement across all metrics.   Conclusions: Choroidalyzer is an open-source, end-to-end pipeline that accurately segments the choroid and reliably extracts thickness, area, and vascular index. Especially choroidal vessel segmentation is a difficult and subjective task, and fully-automatic methods like Choroidalyzer could provide objectivity and standardisation.
</details>
<details>
<summary>æ‘˜è¦</summary>
Methods: ä½¿ç”¨ 5,600 ä¸ª OCT B-scan å›¾åƒï¼ˆ233 ä¸ªSubjectï¼Œ6 ä¸ªç³»ç»Ÿç–¾ç—…ç¾¤ç»„ï¼Œ3 ç§è®¾å¤‡ï¼Œ2 ä¸ªåˆ¶é€ å•†ï¼‰ã€‚ä¸ºç”ŸæˆåŒºåŸŸå’Œè¡€ç®¡çš„åœ°é¢ truthï¼Œæˆ‘ä»¬ä½¿ç”¨å½“å‰æœ€ä½³çš„è‡ªåŠ¨æ–¹æ³•ï¼Œå¹¶å¯¹ä¸å‡†ç¡®çš„åˆ† segmentation è¿›è¡Œæ‰‹åŠ¨ä¿®æ­£ï¼Œå¹¶æ‰‹åŠ¨æ ‡æ³¨çœ¼çªä½ç½®ã€‚æˆ‘ä»¬ä½¿ç”¨ U-Net æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä»¥æ£€æµ‹åŒºåŸŸã€è¡€ç®¡å’Œçœ¼çªï¼Œè®¡ç®—choroid åšåº¦ã€é¢ç§¯å’Œè¡€ç®¡æŒ‡æ•°åœ¨çœ¼çªä¸­å¿ƒçš„åŒºåŸŸå…´è¶£ç‚¹ã€‚æˆ‘ä»¬åˆ†æäº† segmentation ä¸€è‡´æ€§ï¼ˆAUCã€Diceï¼‰å’Œchoroid æŒ‡æ ‡ä¸€è‡´æ€§ï¼ˆPearsonã€Spearmanã€å¹³å‡ç»å¯¹è¯¯å·® (MAE)ï¼‰åœ¨å†…éƒ¨å’Œå¤–éƒ¨æµ‹è¯•é›†ä¸­ã€‚æˆ‘ä»¬å°† Choroidalyzer ä¸ä¸¤åæ‰‹åŠ¨è¯„åˆ†å‘˜è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å¯¹é«˜è¯¯å·®æƒ…å†µè¿›è¡Œæ£€æŸ¥ã€‚Results: Choroidalyzer åœ¨æ ‡å‡† laptop ä¸ŠèŠ±è´¹ 0.299 ç§’/å›¾åƒï¼Œå®ç°äº†ä¼˜ç§€çš„åŒºåŸŸï¼ˆDiceï¼šå†…éƒ¨ 0.9789ï¼Œå¤–éƒ¨ 0.9749ï¼‰ã€éå¸¸å¥½çš„è¡€ç®¡åˆ† segmentation æ€§èƒ½ï¼ˆDiceï¼šå†…éƒ¨ 0.8817ï¼Œå¤–éƒ¨ 0.8703ï¼‰å’Œéå¸¸å¥½çš„çœ¼çªä½ç½®é¢„æµ‹ï¼ˆMAEï¼šå†…éƒ¨ 3.9 åƒç´ ï¼Œå¤–éƒ¨ 3.4 åƒç´ ï¼‰ã€‚å¯¹åšåº¦ã€é¢ç§¯å’Œè¡€ç®¡æŒ‡æ•°ï¼ŒPearson ç›¸å…³æ€§ä¸º 0.9754ã€0.9815 å’Œ 0.8285ï¼ˆå†…éƒ¨ï¼‰/ 0.9831ã€0.9779 å’Œ 0.7948ï¼ˆå¤–éƒ¨ï¼‰ï¼Œå‡ä¸º p<0.0001ã€‚ Choroidalyzer ä¸è¯„åˆ†å‘˜çš„åè°ƒæ€§ä¸ä¸¤åè¯„åˆ†å‘˜ä¹‹é—´çš„åè°ƒæ€§ç›¸å½“ã€‚Conclusions: Choroidalyzer æ˜¯ä¸€ä¸ªå¼€æºã€ç«¯åˆ°ç«¯ç®¡é“ï¼Œå¯ä»¥å‡†ç¡®åœ° segmenting choroid å’Œçœ¼çªï¼Œå¹¶å¯é åœ°æå–åšåº¦ã€é¢ç§¯å’Œè¡€ç®¡æŒ‡æ•°ã€‚ç‰¹åˆ«æ˜¯è¡€ç®¡åˆ† segmentation æ˜¯ä¸€é¡¹å…·æœ‰Subjectiveå’ŒObjective two aspectsçš„ä»»åŠ¡ï¼Œå…¨è‡ªåŠ¨çš„æ–¹æ³• Like Choroidalyzer å¯ä»¥æä¾›Objectivityå’Œæ ‡å‡†åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="DGInStyle-Domain-Generalizable-Semantic-Segmentation-with-Image-Diffusion-Models-and-Stylized-Semantic-Control"><a href="#DGInStyle-Domain-Generalizable-Semantic-Segmentation-with-Image-Diffusion-Models-and-Stylized-Semantic-Control" class="headerlink" title="DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control"></a>DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03048">http://arxiv.org/abs/2312.03048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuru Jia, Lukas Hoyer, Shengyu Huang, Tianfu Wang, Luc Van Gool, Konrad Schindler, Anton Obukhov</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æ£€æŸ¥å¤§è§„æ¨¡æ½œåœ¨æ‰©å±•æ¨¡å‹ï¼ˆLDMï¼‰æ˜¯å¦å¯ä»¥ç”¨äºç”Ÿæˆå¤§è§„æ¨¡æ•°æ®ï¼Œä»¥æé«˜è‡ªåŠ¨é©¾é©¶ä»»åŠ¡ä¸­çš„semantic segmentationï¼Ÿ</li>
<li>methods: ä½œè€…æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ•°æ®ç”Ÿæˆç®¡çº¿ï¼Œç§°ä¸ºDGInStyleã€‚è¯¥ç®¡çº¿åŒ…æ‹¬ç‰¹ç‚¹æ§åˆ¶ç”Ÿæˆã€å¤šåˆ†è¾¨ç‡æ½œåœ¨æ‹Ÿåˆå’Œé£æ ¼äº¤æ¢ç­‰æ–¹æ³•ï¼Œä»¥æé«˜LDMçš„ç”Ÿæˆè´¨é‡å’Œå¤šæ ·æ€§ã€‚</li>
<li>results: é€šè¿‡ä½¿ç”¨DGInStyleç®¡çº¿ï¼Œä½œè€…ç”Ÿæˆäº†ä¸€ä¸ªå¤šæ ·çš„è¡—æ™¯æ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®­ç»ƒäº†ä¸€ä¸ªåŸŸä¸ä¾çš„semantic segmentationæ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œä½¿ç”¨è¯¥ç”Ÿæˆå¢å¼ºæ–¹æ¡ˆå¯ä»¥æé«˜åŸŸæ³›åŒ–é¢„æµ‹çš„æ€§èƒ½ï¼Œåœ¨ä¸€äº›æƒ…å†µä¸‹æ¯”å‰ä¸€ä¸ªçŠ¶æ€çš„æ–¹æ³•æé«˜äº†+2.5 mIoUã€‚<details>
<summary>Abstract</summary>
Large, pretrained latent diffusion models (LDMs) have demonstrated an extraordinary ability to generate creative content, specialize to user data through few-shot fine-tuning, and condition their output on other modalities, such as semantic maps. However, are they usable as large-scale data generators, e.g., to improve tasks in the perception stack, like semantic segmentation? We investigate this question in the context of autonomous driving, and answer it with a resounding "yes". We propose an efficient data generation pipeline termed DGInStyle. First, we examine the problem of specializing a pretrained LDM to semantically-controlled generation within a narrow domain. Second, we design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs towards dominant objects. Third, we propose a Style Swap technique to endow the rich generative prior with the learned semantic control. Using DGInStyle, we generate a diverse dataset of street scenes, train a domain-agnostic semantic segmentation model on it, and evaluate the model on multiple popular autonomous driving datasets. Our approach consistently increases the performance of several domain generalization methods, in some cases by +2.5 mIoU compared to the previous state-of-the-art method without our generative augmentation scheme. Source code and dataset are available at https://dginstyle.github.io .
</details>
<details>
<summary>æ‘˜è¦</summary>
First, we examine the problem of specializing a pretrained LDM to semantically-controlled generation within a narrow domain. Second, we design a Multi-resolution Latent Fusion technique to overcome the bias of LDMs towards dominant objects. Third, we propose a Style Swap technique to endow the rich generative prior with the learned semantic control.Using DGInStyle, we generate a diverse dataset of street scenes, train a domain-agnostic semantic segmentation model on it, and evaluate the model on multiple popular autonomous driving datasets. Our approach consistently improves the performance of several domain generalization methods, in some cases by +2.5 mIoU compared to the previous state-of-the-art method without our generative augmentation scheme. The source code and dataset are available at <https://dginstyle.github.io>.
</details></li>
</ul>
<hr>
<h2 id="LLaVA-Grounding-Grounded-Visual-Chat-with-Large-Multimodal-Models"><a href="#LLaVA-Grounding-Grounded-Visual-Chat-with-Large-Multimodal-Models" class="headerlink" title="LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models"></a>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02949">http://arxiv.org/abs/2312.02949</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ux-decoder/llava-grounding">https://github.com/ux-decoder/llava-grounding</a></li>
<li>paper_authors: Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, Jianwei Yang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æé«˜å¤§å‹å¤šModalæ¨¡å‹ï¼ˆLMMï¼‰åœ¨è§†è§‰å¯¹è¯ä¸­çš„å›ºå®šèƒ½åŠ›ã€‚</li>
<li>methods: ä½œè€…æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†ï¼ˆGVCï¼‰ï¼Œä»¥åŠä¸€ç§æ¨¡å‹è®¾è®¡ï¼Œå¯ä»¥å°†å›ºå®šå’Œå¯¹è¯åŠŸèƒ½ç»“åˆåœ¨ä¸€èµ·ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œä½œè€…çš„æ¨¡å‹åœ¨Grounding-Benchä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ç»å…¸çš„å›ºå®š benchmark RefCOCO&#x2F;+&#x2F;g å’Œ Flickr30K Entities ä¸Šä¹Ÿè¾¾åˆ°äº†ç«äº‰æ€§è¡¨ç°ã€‚<details>
<summary>Abstract</summary>
With the recent significant advancements in large multi-modal models (LMMs), the importance of their grounding capability in visual chat is increasingly recognized. Despite recent efforts to enable LMMs to support grounding, their capabilities for grounding and chat are usually separate, and their chat performance drops dramatically when asked to ground. The problem is the lack of a dataset for grounded visual chat (GVC). Existing grounding datasets only contain short captions. To address this issue, we have created GVC data that allows for the combination of grounding and chat capabilities. To better evaluate the GVC capabilities, we have introduced a benchmark called Grounding-Bench. Additionally, we have proposed a model design that can support GVC and various types of visual prompts by connecting segmentation models with language models. Experimental results demonstrate that our model outperforms other LMMs on Grounding-Bench. Furthermore, our model achieves competitive performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K Entities. Our code will be released at https://github.com/UX-Decoder/LLaVA-Grounding .
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Fast-CT-anatomic-localization-algorithm"><a href="#Fast-CT-anatomic-localization-algorithm" class="headerlink" title="Fast CT anatomic localization algorithm"></a>Fast CT anatomic localization algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02941">http://arxiv.org/abs/2312.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amit Oved</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æé«˜è®¡ç®—æœºæ–­å±‚æˆåƒï¼ˆCTï¼‰æ‰«æä¸­çš„æ¯ä¸ªsliceçš„å®šä½ç²¾åº¦ï¼Œä»¥ä¾¿å¿«é€Ÿæ£€ç´¢åŒºåŸŸ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑç‚¹å¹¶è‡ªåŠ¨åˆ†æã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç›´æ¥åœ¨ä¸€éƒ¨åˆ†çš„sliceä¸Šè¿›è¡Œå®šä½ï¼Œç„¶åä½¿ç”¨çº¿æ€§æ¨¡å‹å°†sliceçš„æ ‡æ³¨ä½ç½®æ˜ å°„åˆ°å®é™…çš„AXIAL anatomicalä½ç½®ã€‚</li>
<li>results: æœ¬ç ”ç©¶çš„ç»“æœæ˜¾ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¾ˆå¿«ï¼ˆless than 1 second per scanï¼‰ã€ç²¾åº¦é«˜ï¼ˆtypical median localization error of 1 cmï¼‰ã€Robustï¼ˆresistant to various noise sources, imaging protocols, metal induced artifacts, anatomical deformations etc.ï¼‰åœ°å®šä½æ¯ä¸ªsliceã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜æä¾›äº†ä¸€ä¸ªæ˜ å°„ä¿¡ä»»åˆ†æ•°ï¼Œä»¥é¿å…åœ¨ç½•è§çš„å¼‚å¸¸æ‰«ææ—¶å‡ºç°çš„ä¸å¯é å®šä½ç»“æœã€‚<details>
<summary>Abstract</summary>
Automatically determining the position of every slice in a CT scan is a basic yet powerful capability allowing fast retrieval of region of interest for visual inspection and automated analysis. Unlike conventional localization approaches which work at the slice level, we directly localize only a fraction of the slices and and then fit a linear model which maps slice index to its estimated axial anatomical position based on those slices. The model is then used to assign axial position to every slices of the scan. This approach proves to be both computationally efficient, with a typical processing time of less than a second per scan (regardless of its size), accurate, with a typical median localization error of 1 cm, and robust to different noise sources, imaging protocols, metal induced artifacts, anatomical deformations etc. Another key element of our approach is the introduction of a mapping confidence score. This score acts as a fail safe mechanism which allows a rejection of unreliable localization results in rare cases of anomalous scans. Our algorithm sets new State Of The Art results in terms of localization accuracy. It also offers a decrease of two orders of magnitude in processing time with respect to all published processing times. It was designed to be invariant to various scan resolutions, scan protocols, patient orientations, strong artifacts and various deformations and abnormalities. Additionally, our algorithm is the first one to the best of our knowledge which supports the entire body from head to feet and is not confined to specific anatomical region. This algorithm was tested on thousands of scans and proves to be very reliable and useful as a preprocessing stage for many applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨ç¡®å®šæ¯ä¸ªåˆ‡é¢åœ¨CTæ‰«æä¸­çš„ä½ç½®æ˜¯ä¸€ä¸ªåŸºæœ¬ yet å¼ºå¤§çš„èƒ½åŠ›ï¼Œå…è®¸å¿«é€Ÿæ£€ç´¢åŒºåŸŸå…³æ³¨å’Œè‡ªåŠ¨åˆ†æã€‚ä¸ä¼ ç»Ÿçš„åœ°æ–¹åŒ–æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬ç›´æ¥ç¡®å®šåªæœ‰ä¸€éƒ¨åˆ†çš„åˆ‡é¢ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå°†åˆ‡é¢ç´¢å¼•æ˜ å°„åˆ°ä¼°è®¡çš„AXIAL anatomicalä½ç½®åŸºäºè¿™äº›åˆ‡é¢ã€‚æ¨¡å‹ç„¶åç”¨äºå°†AXIALä½ç½®åˆ†é…ç»™æ‰«æä¸­çš„æ‰€æœ‰åˆ‡é¢ã€‚è¿™ç§æ–¹æ³•è¯æ˜æ˜¯è®¡ç®—æ•ˆç‡é«˜ï¼ŒTypicalå¤„ç†æ—¶é—´ä½äº1ç§’é’Ÿï¼ˆæ— è®ºæ‰«æçš„å¤§å°ï¼‰ï¼Œå‡†ç¡®æ€§é«˜ï¼ŒTypical median localization error 1 cmï¼Œå¹¶ä¸”å¯¹ä¸åŒçš„å™ªå£°æ¥æºã€æ‰«æåè®®ã€é•å¯¼è‡´çš„artefactsã€ç”Ÿç‰©å­¦å˜å½¢ç­‰ç­‰æœ‰ robustæ€§ã€‚å¦å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•è¿˜å¼•å…¥äº†ä¸€ä¸ªæ˜ å°„ä¿¡ä»»åˆ†æ•°ã€‚è¿™ä¸ªåˆ†æ•° acts as a fail safe mechanismï¼Œ allowing reject unreliable localization results in rare cases of anomalous scansã€‚æˆ‘ä»¬çš„ç®—æ³•åˆ›é€ äº†æ–°çš„ State Of The Art ç»“æœï¼Œåœ¨æœ¬åœ°åŒ–ç²¾åº¦æ–¹é¢ã€‚å®ƒè¿˜æä¾›äº†ä¸¤ä¸ªé˜¶æ®µçš„å‡å°‘ï¼Œåœ¨æ‰€æœ‰å·²å‘è¡¨çš„å¤„ç†æ—¶é—´ä¸Šã€‚å®ƒæ˜¯å…·æœ‰VARIOUS SCAN RESOLUTIONSã€SCAN PROTOCOLSã€patient orientationsã€å¼ºå¤§çš„artefactså’Œå¤šç§å˜å½¢å’Œç•¸å½¢ç­‰ç­‰çš„æŠ—è¡¡æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç®—æ³•æ˜¯é¦–ä¸ªï¼Œè‡³å°‘åˆ°æˆ‘ä»¬æ‰€çŸ¥é“çš„ï¼Œæ”¯æŒå…¨èº«ä»å¤´åˆ°è„šï¼Œè€Œä¸æ˜¯ä»…ä»…å±€éƒ¨è§£å‰–åŒºåŸŸã€‚è¿™ä¸ªç®—æ³•åœ¨åƒä¸ªæ‰«æä¸­è¿›è¡Œäº†ä¸¥æ ¼çš„æµ‹è¯•ï¼Œè¯æ˜å®ƒæ˜¯éå¸¸å¯é å’Œæœ‰ç”¨ï¼Œä½œä¸ºè®¸å¤šåº”ç”¨çš„é¢„å¤„ç†é˜¶æ®µã€‚
</details></li>
</ul>
<hr>
<h2 id="Drag-A-Video-Non-rigid-Video-Editing-with-Point-based-Interaction"><a href="#Drag-A-Video-Non-rigid-Video-Editing-with-Point-based-Interaction" class="headerlink" title="Drag-A-Video: Non-rigid Video Editing with Point-based Interaction"></a>Drag-A-Video: Non-rigid Video Editing with Point-based Interaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02936">http://arxiv.org/abs/2312.02936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Teng, Enze Xie, Yue Wu, Haoyu Han, Zhenguo Li, Xihui Liu</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨æä¾›ä¸€ç§å¯äº¤äº’åœ°ç‚¹å¯¹ç‚¹è§†é¢‘ä¿®æ”¹æ–¹æ³•ï¼Œå…è®¸ç”¨æˆ·åœ¨ç¬¬ä¸€å¸§è§†é¢‘ä¸­ç‚¹å‡»ä¸¤ä¸ªæ‰˜ç®¡ç‚¹å’Œç›®æ ‡ç‚¹ï¼Œä»¥åŠé¢å¯¹é¢çš„maskï¼Œç„¶åå°†è¿™äº›ç‚¹é›†æ‰©å±•åˆ°å…¶ä»–å¸§ä¸­ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§æ‰©æ•£åŸºäºçš„æ–¹æ³•ï¼Œé€šè¿‡ç‚¹å¯¹ç‚¹çš„åŒ¹é…æ¥æ›´æ–°è§†é¢‘çš„å†…å®¹ã€‚ä¸ºäº†ä¿è¯è§†é¢‘çš„æµç•…æ€§å’Œä¸€è‡´æ€§ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°çš„è§†é¢‘æ°´å¹³è¿åŠ¨ç›‘è§†å™¨ï¼Œå¹¶å¼•å…¥äº†éšè—åç§»æ¥å®ç°è¿™ç§æ›´æ–°ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°ä¿®æ”¹è§†é¢‘çš„å†…å®¹ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„è§†é¢‘ä¸­ä¿æŒä¸€è‡´æ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘ä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆæœå’Œçµæ´»æ€§ã€‚å…·ä½“çš„å®éªŒç»“æœå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„ç½‘ç«™æŸ¥çœ‹ï¼š<a target="_blank" rel="noopener" href="https://drag-a-video.github.io/">https://drag-a-video.github.io/</a>.<details>
<summary>Abstract</summary>
Video editing is a challenging task that requires manipulating videos on both the spatial and temporal dimensions. Existing methods for video editing mainly focus on changing the appearance or style of the objects in the video, while keeping their structures unchanged. However, there is no existing method that allows users to interactively ``drag'' any points of instances on the first frame to precisely reach the target points with other frames consistently deformed. In this paper, we propose a new diffusion-based method for interactive point-based video manipulation, called Drag-A-Video. Our method allows users to click pairs of handle points and target points as well as masks on the first frame of an input video. Then, our method transforms the inputs into point sets and propagates these sets across frames. To precisely modify the contents of the video, we employ a new video-level motion supervision to update the features of the video and introduce the latent offsets to achieve this update at multiple denoising timesteps. We propose a temporal-consistent point tracking module to coordinate the movement of the points in the handle point sets. We demonstrate the effectiveness and flexibility of our method on various videos. The website of our work is available here: https://drag-a-video.github.io/.
</details>
<details>
<summary>æ‘˜è¦</summary>
è§†é¢‘ç¼–è¾‘æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œéœ€è¦åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šä¿®æ”¹è§†é¢‘å†…å®¹ã€‚ç°æœ‰çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¸»è¦æ˜¯æ”¹å˜è§†é¢‘ä¸­å¯¹è±¡çš„å¤–è§‚æˆ–é£æ ¼ï¼Œè€Œä¿æŒå¯¹è±¡çš„ç»“æ„ä¸å˜ã€‚ç„¶è€Œï¼Œç°åœ¨æ²¡æœ‰ä»»ä½•æ–¹æ³•å¯ä»¥å…è®¸ç”¨æˆ·åœ¨ç¬¬ä¸€å¸§è§†é¢‘ä¸­â€œæ‹–â€ä»»æ„ç‚¹ï¼Œå¹¶åœ¨å…¶ä»–å¸§ä¸Šå‡†ç¡®åœ°è¾¾åˆ°ç›®æ ‡ç‚¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ‰©æ•£åŸºäºçš„å®æ—¶ç‚¹åŸºè§†é¢‘ä¿®æ”¹æ–¹æ³•ï¼Œç§°ä¸ºDrag-A-Videoã€‚æˆ‘ä»¬çš„æ–¹æ³•å…è®¸ç”¨æˆ·åœ¨è¾“å…¥è§†é¢‘çš„ç¬¬ä¸€å¸§ä¸Šé‡‡ç”¨ç‚¹å¯¹ç‚¹å’Œé¢Maskè¿›è¡Œç‚¹å‡»ï¼Œç„¶åå°†è¿™äº›ç‚¹é›†ä¼ æ’­åˆ°å…¶ä»–å¸§ã€‚ä¸ºäº†å‡†ç¡®ä¿®æ”¹è§†é¢‘å†…å®¹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ–°çš„è§†é¢‘çº§è¿åŠ¨ç›‘è§†ï¼Œå¹¶å¼•å…¥äº†æ½œåœ¨åç§»æ¥å®ç°è¿™ç§æ›´æ–°ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªåè°ƒç‚¹è·Ÿè¸ªæ¨¡å—ï¼Œä»¥ç¡®ä¿ç‚¹é›†åœ¨å„ä¸ªå¸§ä¸­çš„è¿åŠ¨åè°ƒä¸€è‡´ã€‚æˆ‘ä»¬åœ¨å„ç§è§†é¢‘ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„æ•ˆæœå’Œçµæ´»æ€§ã€‚æˆ‘ä»¬çš„å·¥ä½œç½‘ç«™çš„é“¾æ¥æ˜¯ï¼šhttps://drag-a-video.github.io/.
</details></li>
</ul>
<hr>
<h2 id="WoVoGen-World-Volume-aware-Diffusion-for-Controllable-Multi-camera-Driving-Scene-Generation"><a href="#WoVoGen-World-Volume-aware-Diffusion-for-Controllable-Multi-camera-Driving-Scene-Generation" class="headerlink" title="WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation"></a>WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02934">http://arxiv.org/abs/2312.02934</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fudan-zvg/wovogen">https://github.com/fudan-zvg/wovogen</a></li>
<li>paper_authors: Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, Li Zhang</li>
<li>For:  This paper is written for researchers and developers working on autonomous driving technology, particularly those interested in multi-camera street-view video generation and scene understanding.* Methods:  The paper proposes a novel method called WoVoGen, which combines an additional explicit world volume to leverage 4D world volume as a foundational element for video generation. The method operates in two phases: envisioning the future 4D temporal world volume based on vehicle control sequences, and generating multi-camera videos informed by this envisioned 4D temporal world volume and sensor interconnectivity.* Results:  The paper presents results of WoVoGen on several benchmark datasets, demonstrating its ability to generate high-quality street-view videos in response to vehicle control inputs and facilitating scene editing tasks. The results show that WoVoGen outperforms traditional rendering-based methods in terms of diversity and coherence, and achieves state-of-the-art performance in scene understanding tasks.<details>
<summary>Abstract</summary>
Generating multi-camera street-view videos is critical for augmenting autonomous driving datasets, addressing the urgent demand for extensive and varied data. Due to the limitations in diversity and challenges in handling lighting conditions, traditional rendering-based methods are increasingly being supplanted by diffusion-based methods. However, a significant challenge in diffusion-based methods is ensuring that the generated sensor data preserve both intra-world consistency and inter-sensor coherence. To address these challenges, we combine an additional explicit world volume and propose the World Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system is specifically designed to leverage 4D world volume as a foundational element for video generation. Our model operates in two distinct phases: (i) envisioning the future 4D temporal world volume based on vehicle control sequences, and (ii) generating multi-camera videos, informed by this envisioned 4D temporal world volume and sensor interconnectivity. The incorporation of the 4D world volume empowers WoVoGen not only to generate high-quality street-view videos in response to vehicle control inputs but also to facilitate scene editing tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç”Ÿæˆå¤šæ‘„åƒå¤´è¡—æ™¯è§†é¢‘æ˜¯ä¸ºè‡ªåŠ¨é©¾é©¶æ•°æ®é›†æ‰©å¤§å’Œå¤šæ ·åŒ–çš„å…³é”®ï¼Œä»¥æ»¡è¶³è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„ä¸æ–­å‘å±•å’Œåº”ç”¨ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„æ¸²æŸ“æ–¹æ³•ç”±äºç¼ºä¹å¤šæ ·æ€§å’Œå…‰ç…§æ¡ä»¶çš„æ§åˆ¶ï¼Œé€æ¸è¢«æ›¿æ¢ä¸ºæ‰©æ•£æ–¹æ³•ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ–¹æ³•ä¸­ä¿æŒæ‘„åƒå¤´æ•°æ®çš„å†…éƒ¨ä¸€è‡´å’Œå¤–éƒ¨åè°ƒæ€§çš„é—®é¢˜ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºä¸–ç•Œä½“ç§¯ï¼ˆ4Dä¸–ç•Œä½“ç§¯ï¼‰çš„å¤šæ‘„åƒå¤´é©¾é©¶åœºæ™¯ç”Ÿæˆå™¨ï¼ˆWoVoGenï¼‰ã€‚è¿™ç§ç³»ç»Ÿåˆ©ç”¨äº†4Dä¸–ç•Œä½“ç§¯ä½œä¸ºç”Ÿæˆå¤šæ‘„åƒå¤´è§†é¢‘çš„åŸºç¡€å…ƒç´ ï¼Œå¹¶åœ¨ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œæ“ä½œï¼š1. æ ¹æ®è½¦è¾†æ§åˆ¶åºåˆ—é¢„æµ‹æœªæ¥4Dæ—¶é—´ä¸–ç•Œä½“ç§¯ã€‚2. ä½¿ç”¨é¢„æµ‹çš„4Dæ—¶é—´ä¸–ç•Œä½“ç§¯å’Œæ‘„åƒå¤´ä¹‹é—´çš„è¿æ¥ä¿¡æ¯ï¼Œç”Ÿæˆå¤šæ‘„åƒå¤´è§†é¢‘ã€‚é€šè¿‡åˆ©ç”¨4Dä¸–ç•Œä½“ç§¯ï¼ŒWoVoGenä¸ä»…å¯ä»¥æ ¹æ®è½¦è¾†æ§åˆ¶è¾“å…¥ç”Ÿæˆé«˜è´¨é‡çš„è¡—æ™¯è§†é¢‘ï¼Œè¿˜å¯ä»¥å¸®åŠ©è¿›è¡Œåœºæ™¯ç¼–è¾‘ä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="LivePhoto-Real-Image-Animation-with-Text-guided-Motion-Control"><a href="#LivePhoto-Real-Image-Animation-with-Text-guided-Motion-Control" class="headerlink" title="LivePhoto: Real Image Animation with Text-guided Motion Control"></a>LivePhoto: Real Image Animation with Text-guided Motion Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02928">http://arxiv.org/abs/2312.02928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Chen, Zhiheng Liu, Mengting Chen, Yutong Feng, Yu Liu, Yujun Shen, Hengshuang Zhao</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦ç”¨äºè§£å†³æ–‡æœ¬æè¿°ä¸­çš„åŠ¨ä½œæ§åˆ¶é—®é¢˜ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥é€šè¿‡æ–‡æœ¬æè¿°æ¥æ§åˆ¶è§†é¢‘ä¸­çš„åŠ¨ä½œå’Œæ‘„åƒå¤´ç§»åŠ¨ã€‚</li>
<li>methods: è¿™ä¸ªç³»ç»Ÿä½¿ç”¨äº†ä¸€ç§å«åšLivePhotoçš„å®ç”¨ç³»ç»Ÿï¼Œè¯¥ç³»ç»ŸåŒ…æ‹¬ä¸€ä¸ªå¼ºåŒ–çš„æ–‡æœ¬-åˆ°-å›¾åƒç”Ÿæˆå™¨ï¼ˆStable Diffusionï¼‰ï¼Œä»¥åŠä¸€ä¸ªåŠ¨ä½œæ¨¡å—ç”¨äºæ¨¡æ‹Ÿæ—¶é—´å˜åŒ–ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿè¿˜åŒ…æ‹¬ä¸€ä¸ªæ–‡æœ¬é‡æ–°æƒé‡æ¨¡å—å’Œä¸€ä¸ªåŠ¨ä½œå¼ºåº¦ä¼°è®¡æ¨¡å—ï¼Œä»¥é™ä½æ–‡æœ¬-åˆ°-åŠ¨ä½œæ˜ å°„çš„æ­§ä¹‰ã€‚</li>
<li>results: è¿™ä¸ªç³»ç»Ÿèƒ½å¤Ÿå¾ˆå¥½åœ°å°†æ–‡æœ¬æè¿°ä¸­çš„åŠ¨ä½œæŒ‡ä»¤è½¬åŒ–ä¸ºè§†é¢‘ä¸­çš„åŠ¨ä½œå’Œæ‘„åƒå¤´ç§»åŠ¨ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚è¿›è¡Œè§†é¢‘è‡ªå®šä¹‰ï¼ŒåŒ…æ‹¬æ§åˆ¶åŠ¨ä½œçš„å¼ºåº¦ã€‚<details>
<summary>Abstract</summary>
Despite the recent progress in text-to-video generation, existing studies usually overlook the issue that only spatial contents but not temporal motions in synthesized videos are under the control of text. Towards such a challenge, this work presents a practical system, named LivePhoto, which allows users to animate an image of their interest with text descriptions. We first establish a strong baseline that helps a well-learned text-to-image generator (i.e., Stable Diffusion) take an image as a further input. We then equip the improved generator with a motion module for temporal modeling and propose a carefully designed training pipeline to better link texts and motions. In particular, considering the facts that (1) text can only describe motions roughly (e.g., regardless of the moving speed) and (2) text may include both content and motion descriptions, we introduce a motion intensity estimation module as well as a text re-weighting module to reduce the ambiguity of text-to-motion mapping. Empirical evidence suggests that our approach is capable of well decoding motion-related textual instructions into videos, such as actions, camera movements, or even conjuring new contents from thin air (e.g., pouring water into an empty glass). Interestingly, thanks to the proposed intensity learning mechanism, our system offers users an additional control signal (i.e., the motion intensity) besides text for video customization.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»åšå‡ºäº†ä¸€äº›è¿›æ­¥ï¼Œä½†æ˜¯ç°æœ‰çš„ç ”ç©¶é€šå¸¸å¿½ç•¥äº†è§†é¢‘ä¸­çš„æ—¶é—´åŠ¨ä½œæ§åˆ¶é—®é¢˜ã€‚é¢å¯¹è¿™ä¸ªæŒ‘æˆ˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªå®ç”¨çš„ç³»ç»Ÿâ€”â€”LivePhotoï¼Œå…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬æè¿°æ§åˆ¶è‡ªå·± Ğ¸Ğ½Ñ‚ĞµÑ€Ğµç‚¹çš„å›¾åƒåŠ¨ä½œã€‚æˆ‘ä»¬é¦–å…ˆç¡®ç«‹äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œå³ä½¿ç”¨Stable Diffusionæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆå™¨ï¼ˆi.e., Stable Diffusionï¼‰ï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ªæ—¶é—´æ¨¡å‹ç›¸ç»“åˆã€‚æˆ‘ä»¬ç„¶åæå‡ºäº†ä¸€ç§ç‰¹åˆ¶çš„è®­ç»ƒç®¡é“ï¼Œä»¥æ›´å¥½åœ°è”ç³»æ–‡æœ¬å’ŒåŠ¨ä½œã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°ä»¥ä¸‹ä¸¤ç‚¹ï¼šï¼ˆ1ï¼‰æ–‡æœ¬åªèƒ½æè¿°åŠ¨ä½œ roughlyï¼ˆä¾‹å¦‚æ— è®ºç§»åŠ¨é€Ÿåº¦ï¼‰ï¼Œï¼ˆ2ï¼‰æ–‡æœ¬å¯èƒ½åŒ…å«å†…å®¹å’ŒåŠ¨ä½œæè¿°ã€‚ä¸ºäº†å‡å°‘æ–‡æœ¬åˆ°åŠ¨ä½œæ˜ å°„çš„æŠ½è±¡æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†åŠ¨ä½œå¼ºåº¦ä¼°è®¡æ¨¡å—ä»¥åŠæ–‡æœ¬é‡æ–°æƒé‡æ¨¡å—ã€‚å®éªŒè¯æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°å°†æ–‡æœ¬æŒ‡ä»¤è½¬åŒ–ä¸ºè§†é¢‘ä¸­çš„åŠ¨ä½œï¼Œä¾‹å¦‚è¡Œä¸ºã€ç›¸æœºè¿åŠ¨æˆ–è€…evenåˆ›é€ æ–°çš„å†…å®¹ï¼ˆå¦‚å€’æµæ°´åˆ°ç©ºç“¶ä¸­ï¼‰ã€‚åŒæ—¶ï¼Œç”±äºæˆ‘ä»¬æå‡ºçš„å¼ºåº¦å­¦ä¹ æœºåˆ¶ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿè¿˜ä¸ºç”¨æˆ·æä¾›äº†ä¸€ä¸ªé¢å¤–çš„æ§åˆ¶ä¿¡å·ï¼ˆå³åŠ¨ä½œå¼ºåº¦ï¼‰ï¼Œä»¥ä¾¿ä¸ºè§†é¢‘å®šåˆ¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="MagicStick-Controllable-Video-Editing-via-Control-Handle-Transformations"><a href="#MagicStick-Controllable-Video-Editing-via-Control-Handle-Transformations" class="headerlink" title="MagicStick: Controllable Video Editing via Control Handle Transformations"></a>MagicStick: Controllable Video Editing via Control Handle Transformations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03047">http://arxiv.org/abs/2312.03047</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mayuelala/magicstick">https://github.com/mayuelala/magicstick</a></li>
<li>paper_authors: Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§å¯æ§çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡å¯¹ç‰¹å®šå†…éƒ¨ç‰¹å¾ï¼ˆå¦‚ç‰©ä½“çš„è¾¹æå›¾æˆ–äººä½“å§¿æ€ï¼‰çš„è½¬æ¢æ¥ç¼–è¾‘è§†é¢‘çš„å±æ€§ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åˆ©ç”¨æå–çš„å†…éƒ¨æ§åˆ¶ä¿¡å·ï¼ˆå¦‚è¾¹æå›¾æˆ–äººä½“å§¿æ€ï¼‰è¿›è¡Œè½¬æ¢ï¼Œå¹¶é€šè¿‡ä½¿ç”¨å½©è‰²å›¾åƒæ‰©æ•£æ¨¡å‹å’ŒControlNetè¿›è¡Œç¼–è¾‘ã€‚åœ¨ç¼–è¾‘è¿‡ç¨‹ä¸­ï¼Œè¿˜ä½¿ç”¨äº†ææ¡ˆçš„æ³¨æ„åŠ›æ··åˆæ¥æä¾›æ³¨æ„åŠ›å¼•å¯¼ã€‚</li>
<li>results: è¯¥æ–¹æ³•å¯ä»¥ä»é¢„è®­ç»ƒçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­æå–å‡ºè§†é¢‘å±æ€§çš„ç¼–è¾‘èƒ½åŠ›ï¼Œå¹¶åœ¨å„ç§åœºæ™¯ä¸­è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒï¼Œè¯æ˜äº†å…¶åœ¨ temporal consistency å’Œç¼–è¾‘èƒ½åŠ›æ–¹é¢çš„superiorityã€‚<details>
<summary>Abstract</summary>
Text-based video editing has recently attracted considerable interest in changing the style or replacing the objects with a similar structure. Beyond this, we demonstrate that properties such as shape, size, location, motion, etc., can also be edited in videos. Our key insight is that the keyframe transformations of the specific internal feature (e.g., edge maps of objects or human pose), can easily propagate to other frames to provide generation guidance. We thus propose MagicStick, a controllable video editing method that edits the video properties by utilizing the transformation on the extracted internal control signals. In detail, to keep the appearance, we inflate both the pretrained image diffusion model and ControlNet to the temporal dimension and train low-rank adaptions (LORA) layers to fit the specific scenes. Then, in editing, we perform an inversion and editing framework. Differently, finetuned ControlNet is introduced in both inversion and generation for attention guidance with the proposed attention remix between the spatial attention maps of inversion and editing. Yet succinct, our method is the first method to show the ability of video property editing from the pre-trained text-to-image model. We present experiments on numerous examples within our unified framework. We also compare with shape-aware text-based editing and handcrafted motion video generation, demonstrating our superior temporal consistency and editing capability than previous works. The code and models will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬åŸºäºè§†é¢‘ç¼–è¾‘åœ¨æœ€è¿‘å¸å¼•äº†è®¸å¤šå…³æ³¨ï¼Œä¸»è¦æ˜¯ä¿®æ”¹æ ·å¼æˆ–è€…æ›¿æ¢å¯¹è±¡çš„ç±»ä¼¼ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†è§†é¢‘ä¸­çš„å±æ€§ï¼Œå¦‚å½¢çŠ¶ã€å¤§å°ã€ä½ç½®ã€è¿åŠ¨ç­‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç¼–è¾‘ã€‚æˆ‘ä»¬çš„å…³é”®å‘ç°æ˜¯ï¼Œç‰¹å®šçš„å†…éƒ¨ç‰¹å¾ï¼ˆä¾‹å¦‚å¯¹è±¡çš„è¾¹æå›¾æˆ–äººä½“å§¿åŠ¿ï¼‰çš„å…³é”®å¸§å˜æ¢å¯ä»¥è½»æ¾åœ°ä¼ æ’­åˆ°å…¶ä»–å¸§ï¼Œä»¥æä¾›ç”ŸæˆæŒ‡å—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†MagicStickï¼Œä¸€ç§å¯æ§çš„è§†é¢‘ç¼–è¾‘æ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨ç‰¹å®šå†…éƒ¨æ§åˆ¶ä¿¡å·çš„å˜æ¢æ¥ç¼–è¾‘è§†é¢‘å±æ€§ã€‚åœ¨è¯¦ç»†çš„å®ç°æ–¹å¼ä¸‹ï¼Œä¸ºä¿æŒå¤–è§‚ï¼Œæˆ‘ä»¬å°†é¢„è®­ç»ƒçš„å›¾åƒæ‰©æ•£æ¨¡å‹å’ŒControlNetæ‰©å±•åˆ°æ—¶é—´ç»´åº¦ï¼Œå¹¶ä½¿ç”¨ä½çº§æ‚åŒ–å±‚ï¼ˆLORAï¼‰æ¥é€‚åº”ç‰¹å®šçš„åœºæ™¯ã€‚åœ¨ç¼–è¾‘æ—¶ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œåå‘å’Œç¼–è¾‘æ¡†æ¶ã€‚ä¸ä¹‹ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç¼–è¾‘å’Œåå‘ä¸­éƒ½è¿›è¡Œäº†Ñ„Ğ¸inetuning ControlNetï¼Œä»¥ä½¿ç”¨æè®®çš„æ³¨æ„åŠ›æ··åˆæ¥æä¾›æ³¨æ„åŠ›å¯¼èˆªã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•æ˜¯é¦–æ¬¡é€šè¿‡æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹æ¥å®ç°è§†é¢‘å±æ€§ç¼–è¾‘çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªä¾‹å­ä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸Shape-awareæ–‡æœ¬åŸºäºç¼–è¾‘å’Œæ‰‹åŠ¨åˆ¶ä½œçš„è¿åŠ¨è§†é¢‘ç”Ÿæˆè¿›è¡Œæ¯”è¾ƒï¼Œdemonstrating our superior temporal consistencyå’Œç¼–è¾‘èƒ½åŠ›ã€‚ä»£ç å’Œæ¨¡å‹å°†å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Split-Merge-Unlocking-the-Potential-of-Visual-Adapters-via-Sparse-Training"><a href="#Split-Merge-Unlocking-the-Potential-of-Visual-Adapters-via-Sparse-Training" class="headerlink" title="Split &amp; Merge: Unlocking the Potential of Visual Adapters via Sparse Training"></a>Split &amp; Merge: Unlocking the Potential of Visual Adapters via Sparse Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02923">http://arxiv.org/abs/2312.02923</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/theia-4869/mosa">https://github.com/theia-4869/mosa</a></li>
<li>paper_authors: Qizhe Zhang, Bocheng Zou, Ruichuan An, Jiaming Liu, Shanghang Zhang</li>
<li>for: æé«˜Adapter Tuningçš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œä»¥ä¾¿æ›´å¥½åœ°åº”ç”¨äºå„ç§ä»»åŠ¡å’Œç¯å¢ƒã€‚</li>
<li>methods: å°†æ ‡å‡†é€‚é…å™¨æ‹†åˆ†æˆå¤šä¸ªéé‡å æ¨¡å—ï¼Œç„¶åéšæœºå¯ç”¨æ¨¡å—è¿›è¡Œç¨€æœ‰è®­ç»ƒï¼Œå¹¶æœ€ç»ˆå°†æ¨¡å—é›†æˆæˆä¸€ä¸ªå®Œæ•´çš„é€‚é…å™¨è¿›è¡Œè°ƒæ•´ã€‚</li>
<li>results: åœ¨27ç§è§†è§‰ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæ˜¾ç¤ºMoSAå¯ä»¥åœ¨å…¶ä»–Adapter Tuningæ–¹æ³•å’ŒåŸºelinesä¹‹ä¸Šå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶åœ¨ä½èµ„æºå’Œå¤šä»»åŠ¡è®¾ç½®ä¸‹è¾¾åˆ°æ»¡æ„çš„ç»“æœã€‚<details>
<summary>Abstract</summary>
With the rapid growth in the scale of pre-trained foundation models, parameter-efficient fine-tuning techniques have gained significant attention, among which Adapter Tuning is the most widely used. Despite achieving efficiency, Adapter Tuning still underperforms full fine-tuning, and the performance improves at the cost of an increase in parameters. Recent efforts address this issue by pruning the original adapters, but it also introduces training instability and suboptimal performance on certain datasets. Motivated by this, we propose Mixture of Sparse Adapters, or MoSA, as a novel Adapter Tuning method to fully unleash the potential of each parameter in the adapter. We first split the standard adapter into multiple non-overlapping modules, then stochastically activate modules for sparse training, and finally merge them to form a complete adapter after tuning. In this way, MoSA can achieve significantly better performance than standard adapters without any additional computational or storage overhead. Furthermore, we propose a hierarchical sparse strategy to better leverage limited training data. Extensive experiments on a series of 27 visual tasks demonstrate that MoSA consistently outperforms other Adapter Tuning methods as well as other baselines by a significant margin. Furthermore, in two challenging scenarios with low-resource and multi-task settings, MoSA achieves satisfactory results, further demonstrating the effectiveness of our design. Our code will be released.
</details>
<details>
<summary>æ‘˜è¦</summary>
é€šè¿‡é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹çš„è§„æ¨¡å¿«é€Ÿå¢é•¿ï¼Œå‚æ•°ç¨€ç¼ºåŒ–ç²¾ç»†è°ƒæ•™æŠ€æœ¯å¾—åˆ°äº†å¹¿æ³›å…³æ³¨ï¼Œå…¶ä¸­Adapter Tuningæ˜¯æœ€ä¸ºå¹¿æ³›ä½¿ç”¨çš„ã€‚å°½ç®¡å®ƒå¯ä»¥æé«˜æ•ˆç‡ï¼Œä½†å®ƒä»ç„¶æ¯”å…¨é¢è°ƒæ•™ä¸‹é™æ€§èƒ½ï¼Œè€Œä¸”æ€§èƒ½æ”¹å–„çš„ä»£ä»·æ˜¯å¢åŠ å‚æ•°çš„æ•°é‡ã€‚ç°æœ‰åŠªåŠ›é€šè¿‡ä¿®å‰ªåŸå§‹adapteræ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å®ƒä¹Ÿä¼šå¼•å…¥è®­ç»ƒä¸ç¨³å®šå’Œæ•°æ®é›†ç‰¹å®šçš„è¡¨ç°ä¸‹é™ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤š sparse adapterï¼ˆMoSAï¼‰ï¼Œä½œä¸ºä¸€ç§æ–°çš„Adapter Tuningæ–¹æ³•ï¼Œä»¥å…¨é¢å‘æŒ¥æ¯ä¸ªå‚æ•°åœ¨adapterä¸­çš„æ½œåŠ›ã€‚æˆ‘ä»¬é¦–å…ˆå°†æ ‡å‡†adapteråˆ†è§£æˆå¤šä¸ªä¸é‡å çš„æ¨¡å—ï¼Œç„¶åéšæœºå¯ç”¨æ¨¡å—è¿›è¡Œæ sparseè®­ç»ƒï¼Œå¹¶æœ€åå°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ªå®Œæ•´çš„adapter Ğ¿Ğ¾ÑĞ»Ğµè°ƒæ•™ã€‚è¿™æ ·ï¼ŒMoSAå¯ä»¥åœ¨ä¸å¢åŠ è®¡ç®—æˆ–å­˜å‚¨å¼€é”€çš„æƒ…å†µä¸‹ï¼Œå®ç°äº†æ ‡å‡†adapterçš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§å±‚æ¬¡ sparseç­–ç•¥ï¼Œä»¥æ›´å¥½åœ°åˆ©ç”¨æœ‰é™çš„è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¯¹27ä¸ªè§†è§‰ä»»åŠ¡è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼ŒMoSAå¯ä»¥åœ¨Adapter Tuningæ–¹æ³•å’Œå…¶ä»–åŸºelinesä¹‹ä¸Šå‘ˆç°å‡ºæ˜¾è‘—çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œå¹¶ä¸”åœ¨ä½èµ„æºå’Œå¤šä»»åŠ¡è®¾ç½®ä¸‹ä¹Ÿèƒ½è¾¾åˆ°æ»¡æ„çš„ç»“æœã€‚æˆ‘ä»¬å°†ä»£ç å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Fine-grained-Controllable-Video-Generation-via-Object-Appearance-and-Context"><a href="#Fine-grained-Controllable-Video-Generation-via-Object-Appearance-and-Context" class="headerlink" title="Fine-grained Controllable Video Generation via Object Appearance and Context"></a>Fine-grained Controllable Video Generation via Object Appearance and Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02919">http://arxiv.org/abs/2312.02919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Ping Huang, Yu-Chuan Su, Deqing Sun, Lu Jiang, Xuhui Jia, Yukun Zhu, Ming-Hsuan Yang</li>
<li>for: å¯ä»¥å®ç°ç²¾ç»†æ§åˆ¶çš„å½±ç‰‡ç”Ÿæˆï¼Œå¹¶ä¸”ä¸éœ€è¦è°ƒæ•´æ¨¡å‹ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œå°†æ§åˆ¶ä¿¡å·æ’å…¥åˆ°ç°æœ‰çš„æ–‡æœ¬è‡³å½±ç‰‡æ¨¡å‹ä¸­ï¼Œä»¥å®ç°ç²¾ç»†æ§åˆ¶ã€‚</li>
<li>results: ä¸æ¯”è¾ƒæ–¹æ³•ç›¸æ¯”ï¼Œæé«˜äº†70%çš„æ§åˆ¶æ€§æŒ‡æ ‡ã€‚<details>
<summary>Abstract</summary>
Text-to-video generation has shown promising results. However, by taking only natural languages as input, users often face difficulties in providing detailed information to precisely control the model's output. In this work, we propose fine-grained controllable video generation (FACTOR) to achieve detailed control. Specifically, FACTOR aims to control objects' appearances and context, including their location and category, in conjunction with the text prompt. To achieve detailed control, we propose a unified framework to jointly inject control signals into the existing text-to-video model. Our model consists of a joint encoder and adaptive cross-attention layers. By optimizing the encoder and the inserted layer, we adapt the model to generate videos that are aligned with both text prompts and fine-grained control. Compared to existing methods relying on dense control signals such as edge maps, we provide a more intuitive and user-friendly interface to allow object-level fine-grained control. Our method achieves controllability of object appearances without finetuning, which reduces the per-subject optimization efforts for the users. Extensive experiments on standard benchmark datasets and user-provided inputs validate that our model obtains a 70% improvement in controllability metrics over competitive baselines.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–‡æœ¬åˆ°è§†é¢‘ç”ŸæˆæŠ€æœ¯å·²ç»å±•ç¤ºäº†æœ‰æœ›çš„ç»“æœã€‚ç„¶è€Œï¼Œé€šè¿‡åªæ¥å—è‡ªç„¶è¯­è¨€ä½œä¸ºè¾“å…¥ï¼Œç”¨æˆ·ç»å¸¸é‡åˆ°å‡å°‘ç»†èŠ‚ä¿¡æ¯çš„å›°éš¾ï¼Œä»¥ä¾¿ç²¾ç¡®æ§åˆ¶æ¨¡å‹çš„è¾“å‡ºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æè®®ç»†åŒ–å¯æ§è§†é¢‘ç”Ÿæˆï¼ˆFACTORï¼‰ä»¥å®ç°ç»†èŠ‚æ§åˆ¶ã€‚ç‰¹åˆ«æ˜¯ï¼ŒFACTORç›®æ ‡æ§åˆ¶å¯¹è±¡çš„å¤–è§‚å’Œä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬å…¶ä½ç½®å’Œç±»åˆ«ï¼Œä¸æ–‡æœ¬æç¤ºç›¸åè°ƒã€‚ä¸ºäº†å®ç°ç»†èŠ‚æ§åˆ¶ï¼Œæˆ‘ä»¬æè®®ä¸€ç§ç»Ÿä¸€æ¡†æ¶ï¼Œå°†æ§åˆ¶ä¿¡å·ç›´æ¥æ³¨å…¥åˆ°ç°æœ‰çš„æ–‡æœ¬åˆ°è§†é¢‘æ¨¡å‹ä¸­ã€‚æˆ‘ä»¬çš„æ¨¡å‹åŒ…æ‹¬å…±åŒç¼–ç å™¨å’Œé€‚åº”æ€§è·¨åº¦æ³¨æ„åŠ›å±‚ã€‚é€šè¿‡ä¼˜åŒ–ç¼–ç å™¨å’Œæ’å…¥å±‚ï¼Œæˆ‘ä»¬é€‚åº”äº†æ¨¡å‹ä»¥ç”Ÿæˆä¸æ–‡æœ¬æç¤ºå’Œç»†èŠ‚æ§åˆ¶ç›¸å¯¹åº”çš„è§†é¢‘ã€‚ä¸ä»¥å‰åŸºäºç²—ç³™æ§åˆ¶ä¿¡å·such as edge mapsçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬æä¾›äº†æ›´ç›´è§‚å’Œç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œå…è®¸å¯¹è±¡çº§åˆ«çš„ç»†èŠ‚æ§åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨ä¸è¿›è¡Œfinetuningçš„æƒ…å†µä¸‹å®ç°å¯¹è±¡å¤–è§‚çš„æ§åˆ¶ï¼Œä»è€Œé™ä½ç”¨æˆ·æ¯ä¸ªäººä¼˜åŒ–çš„åŠªåŠ›ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨æ ‡å‡† benchmarkæ•°æ®é›†å’Œç”¨æˆ·æä¾›çš„è¾“å…¥ä¸Šå®ç°äº†70%çš„æ§åˆ¶åº¦æŒ‡æ ‡æé«˜ï¼Œæ¯”åŸºelineæ–¹æ³•æ›´é«˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Prompt-Perceiver-Empower-Adaptiveness-Generalizability-and-Fidelity-for-All-in-One-Image-Restoration"><a href="#Multimodal-Prompt-Perceiver-Empower-Adaptiveness-Generalizability-and-Fidelity-for-All-in-One-Image-Restoration" class="headerlink" title="Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration"></a>Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02918">http://arxiv.org/abs/2312.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, Ran He</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§å¤šmodalæç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆMPerceiverï¼‰ï¼Œç”¨äºè§£å†³ç°å®ä¸–ç•Œä¸­å¤æ‚çš„å›¾åƒä¿®å¤é—®é¢˜ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†ç¨³å®šæ‰©æ•£ï¼ˆSDï¼‰å…ˆéªŒæ¥æé«˜é€‚åº”æ€§ã€æ™®é€‚æ€§å’Œå‡†ç¡®æ€§ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªåŒæ ‘æ¨¡å—ï¼Œç”¨äºæŒæ¡ä¸¤ç§ç±»å‹çš„SDæå‰ï¼šæ–‡æœ¬æå‰ä¸ºæ€»ä½“è¡¨ç¤ºï¼Œè§†è§‰æå‰ä¸ºå¤šå°ºåº¦ç»†èŠ‚è¡¨ç¤ºã€‚è¿™ä¸¤ç§æå‰éƒ½ä¼šéšç€é¢„æµ‹çš„é™ä½é¢„æµ‹æ¥è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¢åŠ äº†ä¸€ä¸ªç²¾ç»†ä¿®æ­£æ¨¡å—ï¼Œä»¥æé«˜ä¿®å¤ç²¾åº¦ã€‚</li>
<li>results: å¯¹äº9ä¸ªå›¾åƒä¿®å¤ä»»åŠ¡ï¼ŒMPerceiverè®­ç»ƒåæ¯” estado-of-the-art ä»»åŠ¡ç‰¹å®šæ–¹æ³•åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­è¡¨ç°å‡ºä¼˜å¼‚ã€‚åœ¨é¢„è®­ç»ƒåï¼ŒMPerceiveråœ¨æœªçœ‹è¿‡ä»»åŠ¡çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å¤Ÿè¾¾åˆ°ä¼˜ç§€çš„é›¶ä¾‹å­¦ä¹ å’Œå‡ ä¾‹å­¦ä¹ æ•ˆæœã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒMPerceiveråœ¨é€‚åº”æ€§ã€æ™®é€‚æ€§å’Œç²¾åº¦æ–¹é¢å…·æœ‰ä¼˜å¼‚æ€§ã€‚<details>
<summary>Abstract</summary>
Despite substantial progress, all-in-one image restoration (IR) grapples with persistent challenges in handling intricate real-world degradations. This paper introduces MPerceiver: a novel multimodal prompt learning approach that harnesses Stable Diffusion (SD) priors to enhance adaptiveness, generalizability and fidelity for all-in-one image restoration. Specifically, we develop a dual-branch module to master two types of SD prompts: textual for holistic representation and visual for multiscale detail representation. Both prompts are dynamically adjusted by degradation predictions from the CLIP image encoder, enabling adaptive responses to diverse unknown degradations. Moreover, a plug-in detail refinement module improves restoration fidelity via direct encoder-to-decoder information transformation. To assess our method, MPerceiver is trained on 9 tasks for all-in-one IR and outperforms state-of-the-art task-specific methods across most tasks. Post multitask pre-training, MPerceiver attains a generalized representation in low-level vision, exhibiting remarkable zero-shot and few-shot capabilities in unseen tasks. Extensive experiments on 16 IR tasks and 26 benchmarks underscore the superiority of MPerceiver in terms of adaptiveness, generalizability and fidelity.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="MIND-Multi-Task-Incremental-Network-Distillation"><a href="#MIND-Multi-Task-Incremental-Network-Distillation" class="headerlink" title="MIND: Multi-Task Incremental Network Distillation"></a>MIND: Multi-Task Incremental Network Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02916">http://arxiv.org/abs/2312.02916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lsabetta/mind">https://github.com/lsabetta/mind</a></li>
<li>paper_authors: Jacopo Bonato, Francesco Pelosin, Luigi Sabetta, Alessandro Nicolosi</li>
<li>for:  Addressing the challenges of Class-Incremental and Domain-Incremental learning in resource-constrained environments.</li>
<li>methods:  Two alternative distillation procedures and the optimization of BachNorm layers across tasks inside sub-networks.</li>
<li>results:  Outperforms all state-of-the-art methods for rehearsal-free Class-Incremental learning, with an increment in classification accuracy of +6% on CIFAR-100&#x2F;10 and +10% on TinyImageNet&#x2F;10, and up to +40% accuracy in Domain-Incremental scenarios.<details>
<summary>Abstract</summary>
The recent surge in pervasive devices generating dynamic data streams has underscored the necessity for learning systems to adapt to data distributional shifts continually. To tackle this challenge, the research community has put forth a spectrum of methodologies, including the demanding pursuit of class-incremental learning without replay data. In this study, we present MIND, a parameter isolation method that aims to significantly enhance the performance of replay-free solutions and achieve state-of-the-art results on several widely studied datasets. Our approach introduces two main contributions: two alternative distillation procedures that significantly improve the efficiency of MIND increasing the accumulated knowledge of each sub-network, and the optimization of the BachNorm layers across tasks inside the sub-networks. Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free Class-Incremental learning (with an increment in classification accuracy of approx. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx. +40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each contribution to demonstrate its impact on performance improvement. Our results showcase the superior performance of MIND indicating its potential for addressing the challenges posed by Class-incremental and Domain-Incremental learning in resource-constrained environments.
</details>
<details>
<summary>æ‘˜è¦</summary>
Our approach has two main contributions:1. Two alternative distillation procedures that improve the efficiency of MIND and increase the accumulated knowledge of each sub-network.2. Optimization of the BachNorm layers across tasks inside the sub-networks.Overall, MIND outperforms all state-of-the-art methods for rehearsal-free Class-Incremental learning, with an average increase in classification accuracy of approximately 6% on CIFAR-100/10 and 10% on TinyImageNet/10. In Domain-Incremental scenarios, MIND achieves up to approximately 40% accuracy.We also conducted ablation studies to demonstrate the impact of each contribution on performance improvement. Our results show that MIND significantly outperforms other methods, indicating its potential for addressing the challenges of Class-incremental and Domain-Incremental learning in resource-constrained environments.
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Video-Domain-Adaptation-with-Masked-Pre-Training-and-Collaborative-Self-Training"><a href="#Unsupervised-Video-Domain-Adaptation-with-Masked-Pre-Training-and-Collaborative-Self-Training" class="headerlink" title="Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training"></a>Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02914">http://arxiv.org/abs/2312.02914</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo, Rama Chellappa</li>
<li>for: è¿™é¡¹ç ”ç©¶ç›®æ ‡æ˜¯è§£å†³æ— ç›‘ç£é¢†åŸŸé€‚åº”ï¼ˆUnsupervised Domain Adaptationï¼ŒUDAï¼‰çš„è§†é¢‘åŠ¨ä½œè¯†åˆ«é—®é¢˜ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ï¼ˆUNITEï¼‰ä½¿ç”¨ä¸€ä¸ªå›¾åƒæ•™å¸ˆæ¨¡å‹æ¥é€‚åº”ç›®æ ‡é¢‘é“ä¸Šçš„è§†é¢‘å­¦ç”Ÿæ¨¡å‹ã€‚é¦–å…ˆï¼ŒUNITEä½¿ç”¨è‡ªæˆ‘è¶…è§†è§‰é¢„è®­ç»ƒæ¥ä¿ƒè¿›ç›®æ ‡é¢‘é“è§†é¢‘ä¸­çš„ç‰¹å¾å­¦ä¹ ï¼Œä½¿ç”¨æ•™å¸ˆå¯¼èˆªçš„æ©ç ç›®æ ‡æ•£åˆ—å¯¹è±¡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è§†é¢‘å­¦ç”Ÿæ¨¡å‹å’Œå›¾åƒæ•™å¸ˆæ¨¡å‹å…±åŒç”Ÿæˆæ”¹è¿›çš„ Pseudolabels  Ğ´Ğ»Ñæ— æ ‡è®°ç›®æ ‡è§†é¢‘ã€‚æˆ‘ä»¬çš„è‡ªæˆ‘è®­ç»ƒè¿‡ç¨‹æˆåŠŸåœ°åˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œä»¥å®ç°å¼ºå¤§çš„é€‚åº”æ€§ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘é¢‘é“é€‚åº”benchmarkä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°äº† significanthigher çš„æ”¹è¿› compared to ä¹‹å‰æŠ¥å‘Šçš„ç»“æœã€‚<details>
<summary>Abstract</summary>
In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†æ— ç›‘ç£é¢†åŸŸé€‚åº”ï¼ˆUSDï¼‰è§†é¢‘åŠ¨ä½œè¯†åˆ«é—®é¢˜ã€‚æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºUNITEï¼Œä½¿ç”¨ä¸€ä¸ªå›¾åƒè€å¸ˆæ¨¡å‹æ¥é€‚åº”ç›®æ ‡é¢†åŸŸçš„è§†é¢‘å­¦ç”Ÿæ¨¡å‹ã€‚UNITEé¦–å…ˆä½¿ç”¨è‡ªæˆ‘è¶…visionçš„é¢„è®­ç»ƒæ¥ä¿ƒè¿›ç›®æ ‡é¢†åŸŸè§†é¢‘ä¸­çš„ç‰¹å¾å­¦ä¹ ï¼Œä½¿ç”¨è€å¸ˆå¯¼èˆªçš„å°é¢æŒ‘æˆ˜ç›®æ ‡ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨å°é¢ç›®æ ‡æ•°æ®ä¸Šè¿›è¡Œè‡ªæˆ‘è®­ç»ƒï¼Œä½¿ç”¨è§†é¢‘å­¦ç”Ÿæ¨¡å‹å’Œå›¾åƒè€å¸ˆæ¨¡å‹å…±åŒç”Ÿæˆæ”¹è¿›çš„å‡æ ‡ç­¾ Ğ´Ğ»Ñæœªæ ‡æ³¨çš„ç›®æ ‡è§†é¢‘ã€‚æˆ‘ä»¬çš„è‡ªæˆ‘è®­ç»ƒè¿‡ç¨‹æˆåŠŸåœ°åˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œå®ç°äº†å¼ºå¤§çš„é€‚åº”æ€§ across é¢‘ç‡ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªè§†é¢‘é¢†åŸŸé€‚åº” benchmark ä¸Šè¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Realistic-Scatterer-Based-Adversarial-Attacks-on-SAR-Image-Classifiers"><a href="#Realistic-Scatterer-Based-Adversarial-Attacks-on-SAR-Image-Classifiers" class="headerlink" title="Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers"></a>Realistic Scatterer Based Adversarial Attacks on SAR Image Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02912">http://arxiv.org/abs/2312.02912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Ye, Rajgopal Kannan, Viktor Prasanna, Carl Busart, Lance Kaplan<br>for:This paper proposes a new physical adversarial attack called the On-Target Scatterer Attack (OTSA) to mislead SAR image classifiers.methods:The OTSA attack uses physical actions to place additional false objects as scatterers around the on-ground target to perturb the SAR image. To ensure the feasibility of its physical execution, the attack is constrained to only place scatterers on the target, and not in the shadow regions or background.results:The experimental results show that the OTSA attack obtains significantly higher success rates under the positioning constraint compared with existing methods.<details>
<summary>Abstract</summary>
Adversarial attacks have highlighted the vulnerability of classifiers based on machine learning for Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) tasks. An adversarial attack perturbs SAR images of on-ground targets such that the classifiers are misled into making incorrect predictions. However, many existing attacking techniques rely on arbitrary manipulation of SAR images while overlooking the feasibility of executing the attacks on real-world SAR imagery. Instead, adversarial attacks should be able to be implemented by physical actions, for example, placing additional false objects as scatterers around the on-ground target to perturb the SAR image and fool the SAR ATR.   In this paper, we propose the On-Target Scatterer Attack (OTSA), a scatterer-based physical adversarial attack. To ensure the feasibility of its physical execution, we enforce a constraint on the positioning of the scatterers. Specifically, we restrict the scatterers to be placed only on the target instead of in the shadow regions or the background. To achieve this, we introduce a positioning score based on Gaussian kernels and formulate an optimization problem for our OTSA attack. Using a gradient ascent method to solve the optimization problem, the OTSA can generate a vector of parameters describing the positions, shapes, sizes and amplitudes of the scatterers to guide the physical execution of the attack that will mislead SAR image classifiers. The experimental results show that our attack obtains significantly higher success rates under the positioning constraint compared with the existing method.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾µå™¨æ”»å‡»æ­ç¤ºäº†åŸºäºæœºå™¨å­¦ä¹ çš„Synthetic Aperture Radarï¼ˆSARï¼‰è‡ªåŠ¨ç›®æ ‡è¯†åˆ«ï¼ˆATRï¼‰ä»»åŠ¡ä¸­çš„æ¼æ´ã€‚ä¸€ä¸ªä¾µå™¨æ”»å‡»å¯ä»¥è®©SARå›¾åƒä¸Šçš„åœ°é¢ç›®æ ‡å˜å¾—æ¨¡ç³Šï¼Œä½¿å¾—åˆ†ç±»å™¨è¿›è¡Œé”™è¯¯é¢„æµ‹ã€‚ç„¶è€Œï¼Œè®¸å¤šç°æœ‰çš„æ”»å‡»æŠ€æœ¯æ˜¯é€šè¿‡éšæ„åœ°ä¿®æ”¹SARå›¾åƒæ¥å®ç°çš„ï¼Œè€Œå¿½è§†äº†åœ¨å®é™…SARå›¾åƒä¸Šæ‰§è¡Œæ”»å‡»çš„å¯è¡Œæ€§ã€‚ç›¸åï¼Œæ”»å‡»åº”è¯¥èƒ½å¤Ÿé€šè¿‡ç‰©ç†æ“ä½œå®ç°ï¼Œä¾‹å¦‚ï¼Œåœ¨åœ°é¢ç›®æ ‡å‘¨å›´æ”¾ç½®é™„å¸¦å¹²æ‰°çš„å¹²æ‰°ç‰©æ¥è®©SARå›¾åƒå˜å¾—æ¨¡ç³Šï¼Œä½¿å¾—SAR ATRåˆ†ç±»å™¨è¿›è¡Œé”™è¯¯é¢„æµ‹ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Target Scatterer Attackï¼ˆOTSAï¼‰ï¼Œä¸€ç§åŸºäºç‰©ç†å¹²æ‰°çš„ Ñ„Ğ¸Ğ·Ğ¸å­¦æ”»å‡»æ–¹æ³•ã€‚ä¸ºç¡®ä¿ç‰©ç†æ‰§è¡Œçš„å¯è¡Œæ€§ï¼Œæˆ‘ä»¬å¼ºåˆ¶é™åˆ¶å¹²æ‰°ç‰©çš„ä½ç½®ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çº¦æŸå¹²æ‰°ç‰©åªèƒ½åœ¨ç›®æ ‡ä¸Šæ”¾ç½®ï¼Œè€Œä¸èƒ½åœ¨é˜´å½±åŒºåŸŸæˆ–èƒŒæ™¯ä¸Šæ”¾ç½®ã€‚ä¸ºå®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä½ç½®è¯„åˆ†å‡½æ•°ï¼Œè¯¥å‡½æ•°åŸºäºé«˜æ–¯å‡½æ•°ï¼Œç”¨äºå½¢å¼åŒ–æˆ‘ä»¬çš„ OTSA æ”»å‡»é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è§£å†³è¯¥é—®é¢˜ï¼ŒOTSA å¯ä»¥ç”Ÿæˆä¸€ä¸ªæè¿°å¹²æ‰°ç‰©çš„ä½ç½®ã€å½¢çŠ¶ã€å¤§å°å’Œå¼ºåº¦çš„å‘é‡ï¼Œä»¥å¼•å¯¼ç‰©ç†æ‰§è¡Œçš„æ”»å‡»ï¼Œä»¥è®©SARå›¾åƒåˆ†ç±»å™¨è¿›è¡Œé”™è¯¯é¢„æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ”»å‡»åœ¨ä½ç½®çº¦æŸä¸‹æ¯”ç°æœ‰æ–¹æ³•æ›´é«˜çš„æˆåŠŸç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Rare-Galaxy-Classes-Identified-In-Foundation-Model-Representations"><a href="#Rare-Galaxy-Classes-Identified-In-Foundation-Model-Representations" class="headerlink" title="Rare Galaxy Classes Identified In Foundation Model Representations"></a>Rare Galaxy Classes Identified In Foundation Model Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02910">http://arxiv.org/abs/2312.02910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mike Walmsley, Anna M. M. Scaife</li>
<li>for: identify rare and visually distinctive galaxy populations</li>
<li>methods: use pretrained models to search for structure in learned representations, cluster approach to isolate specific local patterns</li>
<li>results: reveal groups of galaxies with rare and scientifically-interesting morphologies<details>
<summary>Abstract</summary>
We identify rare and visually distinctive galaxy populations by searching for structure within the learned representations of pretrained models. We show that these representations arrange galaxies by appearance in patterns beyond those needed to predict the pretraining labels. We design a clustering approach to isolate specific local patterns, revealing groups of galaxies with rare and scientifically-interesting morphologies.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬é€šè¿‡åœ¨å·²ç»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­å­¦ä¹ çš„è¡¨ç¤ºæ‰¾åˆ°ç½•è§å’Œè§†è§‰ç‰¹å¾ distintive çš„æ˜Ÿç³»äººå£ã€‚æˆ‘ä»¬å‘ç°è¿™äº›è¡¨ç¤ºåœ¨é¢„è®­ç»ƒæ ‡ç­¾çš„é¢„æµ‹ä¹‹å¤–è¿˜æœ‰æ›´å¤šçš„ç»“æ„ï¼Œè¿™äº›ç»“æ„å¯ä»¥ç”¨æ¥åˆ’åˆ†ç‰¹å®šçš„åœ°æ–¹patternï¼Œæ­ç¤ºå‡ºå…·æœ‰ç½•è§å’Œç§‘å­¦ä¸Šæœ‰ä»·å€¼çš„æ˜Ÿç³»å½¢æ€ã€‚Here's a breakdown of the translation:* æˆ‘ä»¬ (wÇ’men) - we* é€šè¿‡ (gÃ²ngzuÃ²) - by* åœ¨ (åœ¨) - in* å·²ç» (yÇjÄ«ng) - already* é¢„è®­ç»ƒ ( prÃ©-training) - pretrained* æ¨¡å‹ (mÃ³del) - model* ä¸­ (zhÅng) - in* å­¦ä¹  (xuÃ©xÃ­) - learning* çš„ (de) - possessive particle* è¡¨ç¤º (biÇoxiÇng) - representation* æ‰¾åˆ° (zhÇndÃ o) - find* ç½•è§ (guÄjian) - rare* å’Œ (hÃ©) - and* è§†è§‰ (shÃ¬jian) - visual* ç‰¹å¾ (tÃ¨shÄ“ng) - distinctive* çš„ (de) - possessive particle* æ˜Ÿç³» (xÄ«ngxÃ¬) - galaxy* äººå£ (rÃ©nkÇ’u) - populationI hope this helps! Let me know if you have any questions or need further clarification.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Segmentation-of-Spiral-Arms-and-Bars"><a href="#Deep-Learning-Segmentation-of-Spiral-Arms-and-Bars" class="headerlink" title="Deep Learning Segmentation of Spiral Arms and Bars"></a>Deep Learning Segmentation of Spiral Arms and Bars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02908">http://arxiv.org/abs/2312.02908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mwalmsley/zoobot-3d">https://github.com/mwalmsley/zoobot-3d</a></li>
<li>paper_authors: Mike Walmsley, Ashley Spindler</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†å¼€å‘ä¸€ä¸ªç”¨äºSegmenting galactic spiral armså’Œbarsçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥ segment spiral armså’Œbarsï¼Œå¹¶åœ¨ä¸çŸ¥æƒ…çš„ä¸“å®¶è¯„ä¼°ä¸­èƒœè¿‡å½“å‰çš„è‡ªåŠ¨åŒ–æ–¹æ³•ï¼ˆ99%çš„è¯„ä¼°ï¼‰å’ŒåŸå§‹å¿—æ„¿æ ‡ç­¾ï¼ˆ79%çš„è¯„ä¼°ï¼‰ã€‚</li>
<li>results: ä¸“å®¶å¯¹æˆ‘ä»¬é¢„æµ‹çš„æ‰­è½´è†œçš„è¯„ä¼°ä¸ºâ€œå¤§éƒ¨åˆ†æ˜¯å¥½â€åˆ°â€œå®Œç¾â€ï¼Œè¦†ç›–ç‡ä¸º89%ã€‚bar lengthsä»æˆ‘ä»¬é¢„æµ‹çš„æ‰­è½´è†œä¸­å¾—åˆ°çš„è½´é•¿ä¸ä¸€ä¸ªä¸“é—¨å‹Ÿé›†çš„é¡¹ç›®ä¸­çš„è½´é•¿display excellent agreementã€‚æˆ‘ä»¬çš„Masksçš„åƒç´ ç²¾åº¦ï¼Œåœ¨å¤§è§„æ¨¡ä¸Šæ˜¯ä¸å¯èƒ½çš„ï¼Œå°†ä¸º spiral armså’Œbarsçš„æ¼”åŒ–ç ”ç©¶æä¾›åŸºç¡€ã€‚<details>
<summary>Abstract</summary>
We present the first deep learning model for segmenting galactic spiral arms and bars. In a blinded assessment by expert astronomers, our predicted spiral arm masks are preferred over both current automated methods (99% of evaluations) and our original volunteer labels (79% of evaluations). Experts rated our spiral arm masks as `mostly good' to `perfect' in 89% of evaluations. Bar lengths trivially derived from our predicted bar masks are in excellent agreement with a dedicated crowdsourcing project. The pixelwise precision of our masks, previously impossible at scale, will underpin new research into how spiral arms and bars evolve.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»äº†é¦–ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºSegmenting galactic spiral armså’Œbarsã€‚åœ¨ç”±ä¸“å®¶å¤©æ–‡å­¦å®¶Blind assessmentä¸­ï¼Œæˆ‘ä»¬é¢„æµ‹çš„æ—‹å›è‡‚Masksè¢«è¯„ä¼°ä¸ºå½“å‰è‡ªåŠ¨æ–¹æ³•ï¼ˆ99%è¯„ä¼°ï¼‰å’ŒåŸå§‹å¿—æ„¿æ ‡ç­¾ï¼ˆ79%è¯„ä¼°ï¼‰éƒ½è¢«é¦–é€‰ã€‚ä¸“å®¶å¯¹æˆ‘ä»¬çš„æ—‹å›è‡‚Masksè¯„ä¼°ä¸ºâ€œå¤§éƒ¨åˆ†å¥½â€åˆ°â€œå®Œç¾â€çš„89%ã€‚æ¥è‡ªæˆ‘ä»¬é¢„æµ‹çš„æ£’é•¿ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªä¸“é—¨çš„äººå·¥æ™ºèƒ½é¡¹ç›®ä¸­å¾—åˆ°äº†é«˜åº¦ä¸€è‡´ã€‚æˆ‘ä»¬çš„æ©ç ç²¾åº¦ï¼Œä»¥å‰æ— æ³•åœ¨å¤§è§„æ¨¡å®ç°ï¼Œå°†å¯¹æ—‹å›è‡‚å’Œæ£’çš„æ¼”åŒ–åšå‡ºæ–°çš„ç ”ç©¶è´¡çŒ®ã€‚
</details></li>
</ul>
<hr>
<h2 id="HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting"><a href="#HeadGaS-Real-Time-Animatable-Head-Avatars-via-3D-Gaussian-Splatting" class="headerlink" title="HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting"></a>HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02902">http://arxiv.org/abs/2312.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Helisa Dhamo, Yinyu Nie, Arthur Moreau, Jifei Song, Richard Shaw, Yiren Zhou, Eduardo PÃ©rez-Pellitero</li>
<li>for: 3D head animation quality and runtime improvement</li>
<li>methods: 3D Gaussian Splats (3DGS) and hybrid model with learnable latent features</li>
<li>results: state-of-the-art results in real-time inference frame rates, with up to ~2dB improvement and x10 acceleration in rendering speed<details>
<summary>Abstract</summary>
3D head animation has seen major quality and runtime improvements over the last few years, particularly empowered by the advances in differentiable rendering and neural radiance fields. Real-time rendering is a highly desirable goal for real-world applications. We propose HeadGaS, the first model to use 3D Gaussian Splats (3DGS) for 3D head reconstruction and animation. In this paper we introduce a hybrid model that extends the explicit representation from 3DGS with a base of learnable latent features, which can be linearly blended with low-dimensional parameters from parametric head models to obtain expression-dependent final color and opacity values. We demonstrate that HeadGaS delivers state-of-the-art results in real-time inference frame rates, which surpasses baselines by up to ~2dB, while accelerating rendering speed by over x10.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œ3Då¤´éƒ¨åŠ¨ç”»åœ¨æœ€è¿‘å‡ å¹´å†…å·²ç»ç»å†äº†é‡è¦çš„è´¨é‡å’Œè¿è¡Œæ—¶é—´æå‡ï¼Œå°¤å…¶æ˜¯ç”±äºåˆ†åˆ«æ¸²æŸ“å’Œç¥ç»å…‰è°±åœºçš„è¿›æ­¥ã€‚å®æ—¶æ¸²æŸ“æ˜¯å®é™…åº”ç”¨ä¸­çš„ä¸€ä¸ªéå¸¸æ„¿æœ›çš„ç›®æ ‡ã€‚æˆ‘ä»¬æè®®ä½¿ç”¨3D Gaussian Splatsï¼ˆ3DGSï¼‰æ¥è¿›è¡Œ3Då¤´éƒ¨é‡å»ºå’ŒåŠ¨ç”»ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ··åˆæ¨¡å‹ï¼Œå…¶æ‰©å±•äº†3DGSçš„Explicit Representationï¼Œé€šè¿‡å¯å­¦ä¹ çš„æ½œåœ¨ç‰¹å¾åŸºå‡†æ¥ Linearly Blend low-dimensionalå‚æ•°ä»parametricå¤´éƒ¨æ¨¡å‹ä¸­è·å¾—è¡¨è¾¾Ğ²Ğ¸ÑĞ¸endentçš„æœ€ç»ˆé¢œè‰²å’Œé€æ˜åº¦å€¼ã€‚æˆ‘ä»¬ç¤ºå‡ºäº†HeadGaSå¯ä»¥åœ¨å®æ—¶æ¨ç†æ¡†æ¶ä¸­è¾¾åˆ°çŠ¶æ€æœºå™¨çš„ç»“æœï¼Œè¶…è¿‡åŸºå‡†å€¼by up to ~2dBï¼ŒåŒæ—¶åŠ é€Ÿæ¸²æŸ“é€Ÿåº¦by over x10ã€‚â€Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Diversified-in-domain-synthesis-with-efficient-fine-tuning-for-few-shot-classification"><a href="#Diversified-in-domain-synthesis-with-efficient-fine-tuning-for-few-shot-classification" class="headerlink" title="Diversified in-domain synthesis with efficient fine-tuning for few-shot classification"></a>Diversified in-domain synthesis with efficient fine-tuning for few-shot classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03046">http://arxiv.org/abs/2312.03046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vturrisi/disef">https://github.com/vturrisi/disef</a></li>
<li>paper_authors: Victor G. Turrisi da Costa, Nicola Dallâ€™Asen, Yiming Wang, Nicu Sebe, Elisa Ricci</li>
<li>for: æé«˜ few-shot å›¾åƒåˆ†ç±»å™¨çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”ä¸åŒçš„å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</li>
<li>methods: ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡çš„å‡å›¾åƒï¼Œå¹¶é€šè¿‡ç»´åº¦å¢å¼ºå’Œæœ‰æ•ˆçš„ fine-tuning æ¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚</li>
<li>results: åœ¨åä¸ªä¸åŒçš„ benchmark ä¸Šè¿›è¡Œäº†å®éªŒï¼Œ consistently è¶…è¶ŠåŸºelinesï¼Œå¹¶ä¸º few-shot å›¾åƒåˆ†ç±»å™¨æˆåŠŸåœ°è®¾ç«‹äº†æ–°çš„ state-of-the-artã€‚<details>
<summary>Abstract</summary>
Few-shot image classification aims to learn an image classifier using only a small set of labeled examples per class. A recent research direction for improving few-shot classifiers involves augmenting the labelled samples with synthetic images created by state-of-the-art text-to-image generation models. Following this trend, we propose Diversified in-domain synthesis with efficient fine-tuning (DISEF), a novel approach which addresses the generalization challenge in few-shot learning using synthetic data. DISEF consists of two main components. First, we propose a novel text-to-image augmentation pipeline that, by leveraging the real samples and their rich semantics coming from an advanced captioning model, promotes in-domain sample diversity for better generalization. Second, we emphasize the importance of effective model fine-tuning in few-shot recognition, proposing to use Low-Rank Adaptation (LoRA) for joint adaptation of the text and image encoders in a Vision Language Model. We validate our method in ten different benchmarks, consistently outperforming baselines and establishing a new state-of-the-art for few-shot classification. Code is available at \url{https://github.com/vturrisi/disef}
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šå‡ ä¸ªç¤ºä¾‹å›¾åƒåˆ†ç±»ã€‹ç›®æ ‡æ˜¯é€šè¿‡å°‘é‡æ ‡æ³¨ç¤ºä¾‹æ¥å­¦ä¹ å›¾åƒåˆ†ç±»å™¨ã€‚ç°æœ‰ç ”ç©¶æ–¹å‘æ˜¯é€šè¿‡ä½¿ç”¨ç°ä»£æ–‡æœ¬ç”Ÿæˆå›¾åƒæ¨¡å‹æ¥ç”Ÿæˆsyntheticå›¾åƒæ¥æé«˜å‡ ä¸ªç¤ºä¾‹åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³å«æ‹¬æ€§åŸŸåŒæ­¥ç”Ÿæˆï¼ˆDISEFï¼‰ï¼Œä»¥è§£å†³å‡ ä¸ªç¤ºä¾‹å­¦ä¹ ä¸­çš„é€šç”¨åŒ–æŒ‘æˆ˜ã€‚DISEFåŒ…æ‹¬ä¸¤ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚ç¬¬ä¸€éƒ¨åˆ†æ˜¯ä¸€ç§æ–°çš„æ–‡æœ¬ç”Ÿæˆå›¾åƒæ‰©å……ç®¡é“ï¼Œé€šè¿‡åˆ©ç”¨çœŸå®çš„æ ·æœ¬å’Œå®ƒä»¬çš„é«˜åº¦ semanticsæ¥æé«˜åŸŸå†…æ ·æœ¬å¤šæ ·æ€§ï¼Œä»è€Œæé«˜æ³›åŒ–æ€§ã€‚ç¬¬äºŒéƒ¨åˆ†æ˜¯å¼ºè°ƒæœ‰æ•ˆçš„æ¨¡å‹ç»ƒä¹ ï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨çŸ©é˜µé€‚åº”ï¼ˆLoRAï¼‰æ¥é€‚åº”æ–‡æœ¬å’Œå›¾åƒç¼–ç å™¨åœ¨è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„é›†æˆã€‚æˆ‘ä»¬åœ¨åä¸ªä¸åŒçš„benchmarkä¸ŠéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œ consistently outperforming baselinesï¼Œå¹¶åœ¨å‡ ä¸ªç¤ºä¾‹åˆ†ç±»ä¸­åˆ›é€ äº†æ–°çš„çŠ¶æ€ä¹‹æœ¯ã€‚ä»£ç å¯ä»¥åœ¨ \url{https://github.com/vturrisi/disef} ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="BenchLMM-Benchmarking-Cross-style-Visual-Capability-of-Large-Multimodal-Models"><a href="#BenchLMM-Benchmarking-Cross-style-Visual-Capability-of-Large-Multimodal-Models" class="headerlink" title="BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"></a>BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02896">http://arxiv.org/abs/2312.02896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aifeg/benchlmm">https://github.com/aifeg/benchlmm</a></li>
<li>paper_authors: Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, Alex Kot</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å¤§å‹å¤šModalæ¨¡å‹ï¼ˆLMMsï¼‰åœ¨ä¸åŒé£æ ¼ä¸‹çš„Robustnessã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„benchmarkï¼Œå³BenchLMMï¼Œç”¨äºè¯„ä¼°LMMsåœ¨ä¸‰ç§ä¸åŒé£æ ¼ä¸‹çš„æ€§èƒ½ï¼šè‰ºæœ¯é£æ ¼ã€æ„ŸçŸ¥å™¨é£æ ¼å’Œåº”ç”¨é£æ ¼ï¼Œæ¯ç§é£æ ¼æœ‰äº”ä¸ªå­é£æ ¼ã€‚æˆ‘ä»¬ä½¿ç”¨BenchLMMè¿›è¡Œäº†state-of-the-art LMMsçš„å…¨é¢è¯„ä¼°ï¼Œå¹¶å‘ç°ï¼š1ï¼‰LMMsåœ¨ä¸åŒé£æ ¼ä¸‹çš„æ€§èƒ½é€šå¸¸ä¼šä¸‹é™ï¼›2ï¼‰ä¸€ä¸ªLMMåœ¨å¸¸è§é£æ ¼ä¸‹è¡¨ç°å‡ºè‰²ä¸ä¸€å®šæ„å‘³ç€å®ƒåœ¨å…¶ä»–é£æ ¼ä¸‹ä¹Ÿä¼šè¡¨ç°å‡ºè‰²ï¼›3ï¼‰å¯ä»¥é€šè¿‡è®©LMMé¢„æµ‹é£æ ¼æ¥æé«˜LMMsçš„reasoningèƒ½åŠ›ï¼Œå¹¶ä¸”è¿™ç§æ–¹æ³•ä¸éœ€è¦è®­ç»ƒã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼€å‘æ›´æ™ºèƒ½å’Œå¤štalented LMMséœ€è¦æ›´å¥½åœ°ç†è§£å®ƒä»¬åœ¨ä¸åŒé£æ ¼ä¸‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„benchmarkå’Œåˆ†æå¸Œæœ›èƒ½å¤Ÿä¸ºLMMsçš„å‘å±•æä¾›æ–°çš„æ€è·¯ã€‚<details>
<summary>Abstract</summary>
Large Multimodal Models (LMMs) such as GPT-4V and LLaVA have shown remarkable capabilities in visual reasoning with common image styles. However, their robustness against diverse style shifts, crucial for practical applications, remains largely unexplored. In this paper, we propose a new benchmark, BenchLMM, to assess the robustness of LMMs against three different styles: artistic image style, imaging sensor style, and application style, where each style has five sub-styles. Utilizing BenchLMM, we comprehensively evaluate state-of-the-art LMMs and reveal: 1) LMMs generally suffer performance degradation when working with other styles; 2) An LMM performs better than another model in common style does not guarantee its superior performance in other styles; 3) LMMs' reasoning capability can be enhanced by prompting LMMs to predict the style first, based on which we propose a versatile and training-free method for improving LMMs; 4) An intelligent LMM is expected to interpret the causes of its errors when facing stylistic variations. We hope that our benchmark and analysis can shed new light on developing more intelligent and versatile LMMs.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹å¤šmodalæ¨¡å‹ï¼ˆLMMï¼‰ï¼Œå¦‚GPT-4Vå’ŒLLaVAï¼Œåœ¨å¸¸è§å›¾åƒé£æ ¼ä¸‹æ˜¾ç¤ºäº†æƒŠäººçš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬å¯¹å„ç§é£æ ¼å˜åŒ–çš„å¯é æ€§ï¼Œåœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ï¼Œå°šæœªå¾—åˆ°äº†å……åˆ†æ¢è®¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„æ ‡å‡† benchMarkï¼Œå³BenchLMMï¼Œç”¨äºè¯„ä¼°LMMså¯¹ä¸‰ç§ä¸åŒé£æ ¼çš„å¯é æ€§ï¼šè‰ºæœ¯é£æ ¼ã€æ‘„åƒå¤´é£æ ¼å’Œåº”ç”¨é£æ ¼ï¼Œæ¯ç§é£æ ¼åˆæœ‰äº”ç§å­é£æ ¼ã€‚é€šè¿‡ä½¿ç”¨BenchLMMï¼Œæˆ‘ä»¬å¯¹å½“ä»Šæœ€ä½³çš„LMMsè¿›è¡Œäº†å…¨é¢çš„è¯„ä¼°ï¼Œå¹¶å‘ç°äº†ä»¥ä¸‹ç»“è®ºï¼š1ï¼‰LMMsåœ¨ä¸åŒé£æ ¼ä¸‹å·¥ä½œæ—¶é€šå¸¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™; 2ï¼‰ä¸€ä¸ªLMMåœ¨å¸¸è§é£æ ¼ä¸‹è¡¨ç°è‰¯å¥½ä¸ä¸€å®šæ„å‘³ç€å®ƒåœ¨å…¶ä»–é£æ ¼ä¸‹ä¹Ÿä¼šè¡¨ç°è‰¯å¥½; 3ï¼‰LMMsçš„ç†è§£èƒ½åŠ›å¯ä»¥é€šè¿‡å‘LMMsæä¾›é£æ ¼predictingçš„Promptæ¥æé«˜; 4ï¼‰ä¸€ä¸ªæ™ºèƒ½LMMåº”è¯¥èƒ½å¤Ÿè§£é‡Šå®ƒåœ¨é£æ ¼å˜åŒ–æ—¶çš„é”™è¯¯åŸå› ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡æˆ‘ä»¬çš„benchmarkå’Œåˆ†æï¼Œå¯ä»¥ä¸ºå¼€å‘æ›´æ™ºèƒ½å’Œå¤šæ ·åŒ–çš„LMMsæä¾›æ–°çš„æ€è·¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Customization-Assistant-for-Text-to-image-Generation"><a href="#Customization-Assistant-for-Text-to-image-Generation" class="headerlink" title="Customization Assistant for Text-to-image Generation"></a>Customization Assistant for Text-to-image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03045">http://arxiv.org/abs/2312.03045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Tong Sun</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹å’Œæ‰©æ•£æ¨¡å‹çš„è‡ªå®šä¹‰åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·åœ¨æ²¡æœ‰è°ƒæ•´çš„æƒ…å†µä¸‹è¿›è¡Œè‡ªå®šä¹‰ç”Ÿæˆã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ¨¡å‹è®¾è®¡å’Œè®­ç»ƒç­–ç•¥ï¼Œä½¿å¾—åŠ©æ‰‹å¯ä»¥åœ¨2-5ç§’é’Ÿå†…å®Œæˆè‡ªå®šä¹‰ç”Ÿæˆï¼Œè€Œæ— éœ€ä»»ä½•æµ‹è¯•æ—¶é—´çš„è°ƒæ•´ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œè¿™ä¸ªè®ºæ–‡çš„æå‡ºçš„æ–¹æ³•å¯ä»¥åœ¨ä¸åŒé¢†åŸŸä¸­è·å¾—ç«äº‰åŠ›çš„resultï¼Œè¡¨æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
Customizing pre-trained text-to-image generation model has attracted massive research interest recently, due to its huge potential in real-world applications. Although existing methods are able to generate creative content for a novel concept contained in single user-input image, their capability are still far from perfection. Specifically, most existing methods require fine-tuning the generative model on testing images. Some existing methods do not require fine-tuning, while their performance are unsatisfactory. Furthermore, the interaction between users and models are still limited to directive and descriptive prompts such as instructions and captions. In this work, we build a customization assistant based on pre-trained large language model and diffusion model, which can not only perform customized generation in a tuning-free manner, but also enable more user-friendly interactions: users can chat with the assistant and input either ambiguous text or clear instruction. Specifically, we propose a new framework consists of a new model design and a novel training strategy. The resulting assistant can perform customized generation in 2-5 seconds without any test time fine-tuning. Extensive experiments are conducted, competitive results have been obtained across different domains, illustrating the effectiveness of the proposed method.
</details>
<details>
<summary>æ‘˜è¦</summary>
In this work, we propose a customization assistant based on pre-trained large language models and diffusion models, which can perform customized generation without fine-tuning and enable more user-friendly interactions. Users can chat with the assistant and input either ambiguous text or clear instructions, and the resulting assistant can generate customized images in 2-5 seconds. Our proposed framework consists of a new model design and a novel training strategy, and we have obtained competitive results across different domains through extensive experiments, demonstrating the effectiveness of our method.
</details></li>
</ul>
<hr>
<h2 id="Towards-More-Practical-Group-Activity-Detection-A-New-Benchmark-and-Model"><a href="#Towards-More-Practical-Group-Activity-Detection-A-New-Benchmark-and-Model" class="headerlink" title="Towards More Practical Group Activity Detection: A New Benchmark and Model"></a>Towards More Practical Group Activity Detection: A New Benchmark and Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02878">http://arxiv.org/abs/2312.02878</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dk-kim/CAFE_codebase">https://github.com/dk-kim/CAFE_codebase</a></li>
<li>paper_authors: Dongkeun Kim, Youngkil Song, Minsu Cho, Suha Kwak</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†æé«˜ç°æœ‰çš„é›†ä½“æ´»åŠ¨æ£€æµ‹ï¼ˆGADï¼‰æ–¹æ³•å’Œæ•°æ®é›†ï¼Œä»¥æ›´å¥½åœ°åº”å¯¹å®é™…åœºæ™¯ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„GADæ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†æœªçŸ¥çš„ç¾¤ä½“æ•°é‡å’Œéšè—çš„ç¾¤å‘˜ã€‚å®ƒè¿˜ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ˆCaf&#39;eï¼‰ï¼Œè¯¥æ•°æ®é›†æ›´åŠ å®é™…ï¼Œå…·æœ‰æ›´å¤šçš„è¯„ä¼°åœºæ™¯å’Œç²¾ç¾çš„æ³¨é‡Šã€‚</li>
<li>results: ç ”ç©¶äººå‘˜åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬Caf&#39;eæ•°æ®é›†ï¼Œå¹¶åœ¨å‡†ç¡®ç‡å’Œæ¨ç†é€Ÿåº¦ä¸¤ä¸ªæ–¹é¢è¶…è¿‡äº†ä¹‹å‰çš„å·¥ä½œã€‚<details>
<summary>Abstract</summary>
Group activity detection (GAD) is the task of identifying members of each group and classifying the activity of the group at the same time in a video. While GAD has been studied recently, there is still much room for improvement in both dataset and methodology due to their limited capability to address practical GAD scenarios. To resolve these issues, we first present a new dataset, dubbed Caf\'e. Unlike existing datasets, Caf\'e is constructed primarily for GAD and presents more practical evaluation scenarios and metrics, as well as being large-scale and providing rich annotations. Along with the dataset, we propose a new GAD model that deals with an unknown number of groups and latent group members efficiently and effectively. We evaluated our model on three datasets including Caf\'e, where it outperformed previous work in terms of both accuracy and inference speed. Both our dataset and code base will be open to the public to promote future research on GAD.
</details>
<details>
<summary>æ‘˜è¦</summary>
group activity detection (GAD) æ˜¯æŒ‡åœ¨è§†é¢‘ä¸­Identifying members of each group and classifying the activity of the group simultaneously. Although GAD has been studied recently, there is still much room for improvement in both dataset and methodology due to their limited capability to address practical GAD scenarios. To resolve these issues, we first present a new dataset, dubbed Caf\'e. Unlike existing datasets, Caf\'e is constructed primarily for GAD and presents more practical evaluation scenarios and metrics, as well as being large-scale and providing rich annotations. Along with the dataset, we propose a new GAD model that deals with an unknown number of groups and latent group members efficiently and effectively. We evaluated our model on three datasets including Caf\'e, where it outperformed previous work in terms of both accuracy and inference speed. Both our dataset and code base will be open to the public to promote future research on GAD.Here's the breakdown of the translation:* Group activity detection (GAD) æ˜¯æŒ‡åœ¨è§†é¢‘ä¸­... (GAD is a task that involves identifying members of each group and classifying the activity of the group in a video)* although GAD has been studied recently, there is still much room for improvement... (although GAD has been studied recently, there is still much room for improvement in both dataset and methodology)* å› ä¸ºç°æœ‰çš„æ•°æ®é›†å’Œæ–¹æ³•ologies æœ‰é™ï¼Œæ— æ³•æœ‰æ•ˆåœ°è§£å†³å®é™…çš„ GAD åœºæ™¯ã€‚ (because the existing datasets and methodologies have limitations, they cannot effectively address practical GAD scenarios)* ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œåä¸º Caf\'e. (to resolve these issues, we first proposed a new dataset, called Caf\'e)* Caf\'e æ•°æ®é›†ä¸åŒäºç°æœ‰çš„æ•°æ®é›†ï¼Œä¸»è¦æ˜¯ä¸º GAD è®¾è®¡ constructing å’Œæä¾›äº†æ›´å®ç”¨çš„è¯„ä¼°åœºæ™¯å’Œåº¦é‡ï¼ŒåŒæ—¶ä¹Ÿæ˜¯å¤§è§„æ¨¡çš„å’Œæœ‰ä¸°å¯Œçš„æ³¨é‡Šã€‚ (Caf\'e dataset is different from existing datasets, it is primarily designed for GAD and provides more practical evaluation scenarios and metrics, while also being large-scale and providing rich annotations)* åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæå‡ºäº†ä¸€ç§æ–°çš„ GAD æ¨¡å‹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å¤„ç†ä¸ç¡®å®šçš„ç»„æ•°å’Œéšè—çš„ç»„å‘˜ã€‚ (at the same time, we proposed a new GAD model that can effectively handle an unknown number of groups and latent group members)* æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸­è¿›è¡Œäº†æµ‹è¯•ï¼Œå…¶ä¸­åŒ…æ‹¬ Caf\'eï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ€§èƒ½éƒ½é«˜äºè¿‡å»çš„å·¥ä½œã€‚ (we tested our model on three datasets, including Caf\'e, and our model outperformed previous work on all three datasets)* æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ•°æ®é›†å’Œä»£ç åº“å…¬å¼€å‘å¸ƒï¼Œä»¥ä¾¿å°†æ¥çš„ç ”ç©¶äººå‘˜å¯ä»¥æ›´å¥½åœ°è¿›è¡Œ GAD ç ”ç©¶ã€‚ (we will openly release our dataset and codebase to promote future research on GAD)
</details></li>
</ul>
<hr>
<h2 id="A-Dynamic-Network-for-Efficient-Point-Cloud-Registration"><a href="#A-Dynamic-Network-for-Efficient-Point-Cloud-Registration" class="headerlink" title="A Dynamic Network for Efficient Point Cloud Registration"></a>A Dynamic Network for Efficient Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02877">http://arxiv.org/abs/2312.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Ai, Xi Yang</li>
<li>For: æé«˜ç‚¹äº‘æ³¨å†Œç²¾åº¦å’Œæ•ˆç‡ï¼Œè§£å†³éé‡å ç‚¹äº‘ consume å¤§é‡è®¡ç®—èµ„æºçš„é—®é¢˜ã€‚* Methods: å¼•å…¥åŠ¨æ€æ–¹æ³•ï¼Œå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­ï¼Œä»¥æé«˜ç‚¹äº‘æ³¨å†Œç²¾åº¦å’Œæ•ˆç‡ã€‚ä½¿ç”¨è¿­ä»£æ³¨å†Œè¿‡ç¨‹ï¼Œ identific åŒ¹é…ç‚¹äº‘é›†ä¸­çš„åŒºåŸŸï¼Œå¹¶æœ€ç»ˆç§»é™¤å™ªç‚¹äº‘ã€‚* Results: å¯¹æ¯”å…¶ä»–æ–¹æ³•ï¼Œæœ¬æ–¹æ³•å…·æœ‰è¾ƒé«˜çš„é€Ÿåº¦æå‡ï¼ˆ3DMatchä¸Šæé«˜41.2%ï¼ŒKITTIä¸Šæé«˜33.4%ï¼‰ï¼ŒåŒæ—¶ä¿æŒç«äº‰åŠ›çš„æ³¨å†Œå›å¿«è¦æ±‚ã€‚<details>
<summary>Abstract</summary>
For the point cloud registration task, a significant challenge arises from non-overlapping points that consume extensive computational resources while negatively affecting registration accuracy. In this paper, we introduce a dynamic approach, widely utilized to improve network efficiency in computer vision tasks, to the point cloud registration task. We employ an iterative registration process on point cloud data multiple times to identify regions where matching points cluster, ultimately enabling us to remove noisy points. Specifically, we begin with deep global sampling to perform coarse global registration. Subsequently, we employ the proposed refined node proposal module to further narrow down the registration region and perform local registration. Furthermore, we utilize a spatial consistency-based classifier to evaluate the results of each registration stage. The model terminates once it reaches sufficient confidence, avoiding unnecessary computations. Extended experiments demonstrate that our model significantly reduces time consumption compared to other methods with similar results, achieving a speed improvement of over 41% on indoor dataset (3DMatch) and 33% on outdoor datasets (KITTI) while maintaining competitive registration recall requirements.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="RotaTR-Detection-Transformer-for-Dense-and-Rotated-Object"><a href="#RotaTR-Detection-Transformer-for-Dense-and-Rotated-Object" class="headerlink" title="RotaTR: Detection Transformer for Dense and Rotated Object"></a>RotaTR: Detection Transformer for Dense and Rotated Object</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02821">http://arxiv.org/abs/2312.02821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Yuke, Ruan Yumeng, Yang Lei, Guo Sheng</li>
<li>for: æœ¬æ–‡é’ˆå¯¹ dense å’Œæ—‹è½¬çš„å¯¹è±¡æ£€æµ‹ Task è¿›è¡Œäº†ç ”ç©¶ï¼Œä»¥æé«˜ DETR çš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº† Rotated object detection TRansformer (RotaTR)ï¼Œå®ƒæ˜¯ DETR çš„æ‰©å±•ï¼Œé€šè¿‡è®¾è®¡ Rotation Sensitive deformable (RSDeform) æ³¨æ„åŠ›æ¥å¢å¼º DETR å¯¹å‚ç›´ç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›ã€‚</li>
<li>results: å¯¹å››ä¸ªå¤æ‚çš„æ—‹è½¬ Benchmark è¿›è¡Œæµ‹è¯•ï¼ŒRotaTR åœ¨ dense å’Œæ—‹è½¬çš„å¯¹è±¡æ£€æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸åŸå§‹ DETR ç›¸æ¯”æœ‰å¤§é‡çš„ä¼˜åŠ¿ã€‚åŒæ—¶ï¼Œå®ƒä¹Ÿä¸å½“å‰æœ€ä½³çš„ CNN-based æ£€æµ‹å™¨ç›¸å½“ã€‚<details>
<summary>Abstract</summary>
Detecting the objects in dense and rotated scenes is a challenging task. Recent works on this topic are mostly based on Faster RCNN or Retinanet. As they are highly dependent on the pre-set dense anchors and the NMS operation, the approach is indirect and suboptimal.The end-to-end DETR-based detectors have achieved great success in horizontal object detection and many other areas like segmentation, tracking, action recognition and etc.However, the DETR-based detectors perform poorly on dense rotated target tasks and perform worse than most modern CNN-based detectors. In this paper, we find the most significant reason for the poor performance is that the original attention can not accurately focus on the oriented targets. Accordingly, we propose Rotated object detection TRansformer (RotaTR) as an extension of DETR to oriented detection. Specifically, we design Rotation Sensitive deformable (RSDeform) attention to enhance the DETR's ability to detect oriented targets. It is used to build the feature alignment module and rotation-sensitive decoder for our model. We test RotaTR on four challenging-oriented benchmarks. It shows a great advantage in detecting dense and oriented objects compared to the original DETR. It also achieves competitive results when compared to the state-of-the-art.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ£€æµ‹å¯†é›†å’Œæ—‹è½¬åœºæ™¯ä¸­çš„å¯¹è±¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•å¤šåŸºäºFaster RCNNæˆ–Retinanetï¼Œå®ƒä»¬å…·æœ‰å„ç§ç¼ºç‚¹ï¼Œå¦‚ä¾èµ–äºé¢„è®¾å¯†é›†é”šç‚¹å’ŒNMSæ“ä½œï¼Œå¯¼è‡´æ–¹æ³•é—´æ¥å’Œä¸ä¼˜åŒ–ã€‚è€ŒåŸºäºDETRçš„ç«¯åˆ°ç«¯æ£€æµ‹å™¨åœ¨æ¨ªå‘å¯¹è±¡æ£€æµ‹å’Œå…¶ä»–é¢†åŸŸå¦‚åˆ†å‰²ã€è·Ÿè¸ªã€åŠ¨ä½œè¯†åˆ«ç­‰æ–¹é¢å…·æœ‰å¾ˆå¤§çš„æˆåŠŸã€‚ç„¶è€Œï¼ŒDETRåŸºäºçš„æ£€æµ‹å™¨åœ¨å¯†é›†æ—‹è½¬ç›®æ ‡ä»»åŠ¡ä¸Šè¡¨ç°ä¸ä½³ï¼Œæ¯”ç°ä»£CNNåŸºäºçš„æ£€æµ‹å™¨æ›´å·®ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°æœ€ä¸»è¦çš„é—®é¢˜åœ¨äºDETRçš„åŸå§‹æ³¨æ„åŠ›æ— æ³•å‡†ç¡®åœ°å¯¹å¾…æ–¹å‘ç›®æ ‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†å¯¹DETRè¿›è¡Œäº†æ‰©å±•ï¼Œå³æ—‹è½¬å¯¹è±¡æ£€æµ‹ä¼ æ’­å™¨ï¼ˆRotaTRï¼‰ï¼Œä»¥æé«˜DETRå¯¹å¯†é›†æ—‹è½¬ç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¾è®¡äº†æ—‹è½¬æ•æ„Ÿçš„å˜å½¢ï¼ˆRSDeformï¼‰æ³¨æ„åŠ›ï¼Œç”¨äºå¢å¼ºDETRå¯¹æ–¹å‘ç›®æ ‡çš„æ£€æµ‹èƒ½åŠ›ã€‚å®ƒè¢«ç”¨äºæ„å»ºç‰¹å¾å¯¹åº”æ¨¡å—å’Œæ—‹è½¬æ•æ„Ÿè§£ç å™¨ã€‚æˆ‘ä»¬åœ¨å››ä¸ªå¤æ‚çš„æ—‹è½¬å¯¹è±¡æµ‹è¯•benchmarkä¸Šæµ‹è¯•äº†RotaTRã€‚å®ƒåœ¨å¯†é›†æ—‹è½¬ç›®æ ‡ä¸Šæ˜¾ç¤ºäº†ä¼˜äºDETRçš„æ£€æµ‹èƒ½åŠ›ï¼ŒåŒæ—¶ä¸ç°çŠ¶çš„çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½è¾¾åˆ°äº†ç«äº‰æ€§çš„ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Deterministic-Guidance-Diffusion-Model-for-Probabilistic-Weather-Forecasting"><a href="#Deterministic-Guidance-Diffusion-Model-for-Probabilistic-Weather-Forecasting" class="headerlink" title="Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting"></a>Deterministic Guidance Diffusion Model for Probabilistic Weather Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02819">http://arxiv.org/abs/2312.02819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/donggeun-yoon/dgdm">https://github.com/donggeun-yoon/dgdm</a></li>
<li>paper_authors: Donggeun Yoon, Minseok Seo, Doyi Kim, Yeji Choi, Donghyeon Cho</li>
<li>for: ç”¨äºæŒæ¡æ°”è±¡é¢„æµ‹ä¸­çš„å¯èƒ½æ€§é¢„æµ‹å’Œç²¾åº¦é¢„æµ‹ä¹‹é—´çš„å¹³è¡¡ã€‚</li>
<li>methods: ç»“åˆæƒé‡çŸ©é˜µå’Œæ¦‚ç‡æ¨¡å‹ï¼Œæå‡ºäº†Deterministic Guidance Diffusion Model (DGDM)ï¼Œé€šè¿‡åœ¨å‰å‘å’Œåå‘è¿‡ç¨‹ä¸­ç»“åˆæƒé‡çŸ©é˜µå’Œæ¦‚ç‡æ¨¡å‹æ¥å®ç°å¯èƒ½æ€§é¢„æµ‹å’Œç²¾åº¦é¢„æµ‹çš„å¹³è¡¡ã€‚</li>
<li>results: åœ¨å…¨çƒæ°”è±¡é¢„æµ‹æ•°æ®é›†ï¼ˆWeatherBenchï¼‰å’Œé€šç”¨è§†é¢‘å¸§é¢„æµ‹æ ‡å‡† benchmarkï¼ˆMoving MNISTï¼‰ä¸Šè¯„ä¼°DGDMï¼Œå¹¶åœ¨é«˜åˆ†è¾¨ç‡åœ°æ–¹é¢„æµ‹ä¸­ä½¿ç”¨PNW-Typhoonæ°”è±¡å«æ˜Ÿæ•°æ®é›†è¿›è¡ŒéªŒè¯ã€‚ç»“æœæ˜¾ç¤ºDGDMåœ¨å…¨çƒé¢„æµ‹å’Œåœ°æ–¹é¢„æµ‹ä¸­å‡è¾¾åˆ°äº†å½“å‰æœ€ä½³æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Weather forecasting requires not only accuracy but also the ability to perform probabilistic prediction. However, deterministic weather forecasting methods do not support probabilistic predictions, and conversely, probabilistic models tend to be less accurate. To address these challenges, in this paper, we introduce the \textbf{\textit{D}eterministic \textbf{\textit{G}uidance \textbf{\textit{D}iffusion \textbf{\textit{M}odel (DGDM) for probabilistic weather forecasting, integrating benefits of both deterministic and probabilistic approaches. During the forward process, both the deterministic and probabilistic models are trained end-to-end. In the reverse process, weather forecasting leverages the predicted result from the deterministic model, using as an intermediate starting point for the probabilistic model. By fusing deterministic models with probabilistic models in this manner, DGDM is capable of providing accurate forecasts while also offering probabilistic predictions. To evaluate DGDM, we assess it on the global weather forecasting dataset (WeatherBench) and the common video frame prediction benchmark (Moving MNIST). We also introduce and evaluate the Pacific Northwest Windstorm (PNW)-Typhoon weather satellite dataset to verify the effectiveness of DGDM in high-resolution regional forecasting. As a result of our experiments, DGDM achieves state-of-the-art results not only in global forecasting but also in regional forecasting. The code is available at: \url{https://github.com/DongGeun-Yoon/DGDM}.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤©æ°”é¢„æµ‹éœ€è¦ä¸ä»…å‡†ç¡®åº¦é«˜ï¼Œè¿˜éœ€è¦èƒ½å¤Ÿè¿›è¡Œ probabilistic é¢„æµ‹ã€‚ç„¶è€Œï¼Œæ··åˆå‹å¤©æ°”é¢„æµ‹æ–¹æ³•ä¸æ”¯æŒ probabilistic é¢„æµ‹ï¼Œåä¹‹ï¼Œ probabilistic æ¨¡å‹å¾€å¾€å‡†ç¡®æ€§è¾ƒå·®ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº† \textbf{\textit{D}eterministic \textbf{\textit{G}uidance \textbf{\textit{D}iffusion \textbf{\textit{M}odel (DGDM)ï¼Œå°†æ··åˆå‹å¤©æ°”é¢„æµ‹å’Œ probabilistic é¢„æµ‹æ–¹æ³•ç›¸ç»“åˆã€‚åœ¨å‰å‘è¿‡ç¨‹ä¸­ï¼Œæ··åˆå‹å¤©æ°”é¢„æµ‹å’Œ probabilistic æ¨¡å‹éƒ½æ˜¯ç»ˆç«¯è®­ç»ƒçš„ã€‚åœ¨åå‘è¿‡ç¨‹ä¸­ï¼Œå¤©æ°”é¢„æµ‹ä½¿ç”¨æ··åˆå‹å¤©æ°”é¢„æµ‹çš„é¢„æµ‹ç»“æœä½œä¸ºprobabilistic æ¨¡å‹çš„åˆå§‹ç‚¹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒDGDMå¯ä»¥æä¾›å‡†ç¡®çš„é¢„æµ‹ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æä¾› probabilistic é¢„æµ‹ã€‚ä¸ºäº†è¯„ä¼° DGDMï¼Œæˆ‘ä»¬åœ¨ WeatherBench å…¨çƒå¤©æ°”é¢„æµ‹æ•°æ®é›†å’Œ Moving MNIST é€šç”¨è§†é¢‘é¢„æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº† Pacific Northwest Windstorm (PNW)-Typhoon é«˜åˆ†è¾¨ç‡åœ°æ–¹å¤©æ°”å«æ˜Ÿæ•°æ®é›†ï¼Œä»¥éªŒè¯ DGDM åœ¨é«˜åˆ†è¾¨ç‡åœ°æ–¹é¢„æµ‹ä¸­çš„æ•ˆæœã€‚æ ¹æ®æˆ‘ä»¬çš„å®éªŒç»“æœï¼ŒDGDM åœ¨å…¨çƒé¢„æµ‹å’Œåœ°æ–¹é¢„æµ‹ä¸­å‡å–å¾—äº†çŠ¶æ€æœºå™¨äººçš„ç»“æœã€‚ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥è·å–ï¼šhttps://github.com/DongGeun-Yoon/DGDMã€‚
</details></li>
</ul>
<hr>
<h2 id="Generating-Fine-Grained-Human-Motions-Using-ChatGPT-Refined-Descriptions"><a href="#Generating-Fine-Grained-Human-Motions-Using-ChatGPT-Refined-Descriptions" class="headerlink" title="Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions"></a>Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02772">http://arxiv.org/abs/2312.02772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Shi, Chuanchen Luo, Junran Peng, Hongwen Zhang, Yunlian Sun</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºåˆ†è§£ç­–ç•¥çš„äººä½“åŠ¨ä½œç”Ÿæˆæ¨¡å‹ï¼ˆFG-MDMï¼‰ï¼Œä»¥ç”Ÿæˆç»†åŒ–çš„äººä½“åŠ¨ä½œã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5ï¼‰å°†æ½¦æ±¤çš„æ–‡æœ¬æè¿°ç²¾ç»†åŒ–ä¸ºä¸åŒèº«ä½“éƒ¨ä½çš„æè¿°ï¼Œç„¶åä½¿ç”¨è¿™äº›ç²¾ç»†æè¿°å¯¼å¼•ä¸€ç§åŸºäºè½¬æ¢å™¨çš„æ‰©æ•£æ¨¡å‹è¿›è¡Œäººä½“åŠ¨ä½œç”Ÿæˆã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒFG-MDMæ¯”ä¹‹å‰çš„æ–¹æ³•æ›´å…·æœ‰ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨ä¸åŒäºè®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ã€‚ FG-MDMèƒ½å¤Ÿç”Ÿæˆç»†åŒ–çš„äººä½“åŠ¨ä½œï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„äººä½“å§¿åŠ¿å’Œç¯å¢ƒä¸‹è¿›è¡Œç”Ÿæˆã€‚<details>
<summary>Abstract</summary>
Recently, significant progress has been made in text-based motion generation, enabling the generation of diverse and high-quality human motions that conform to textual descriptions. However, it remains challenging to generate fine-grained or stylized motions due to the lack of datasets annotated with detailed textual descriptions. By adopting a divide-and-conquer strategy, we propose a new framework named Fine-Grained Human Motion Diffusion Model (FG-MDM) for human motion generation. Specifically, we first parse previous vague textual annotation into fine-grained description of different body parts by leveraging a large language model (GPT-3.5). We then use these fine-grained descriptions to guide a transformer-based diffusion model. FG-MDM can generate fine-grained and stylized motions even outside of the distribution of the training data. Our experimental results demonstrate the superiority of FG-MDM over previous methods, especially the strong generalization capability. We will release our fine-grained textual annotations for HumanML3D and KIT.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ€è¿‘ï¼Œåœ¨æ–‡æœ¬åŸºäºåŠ¨ä½œç”Ÿæˆæ–¹é¢ï¼Œæœ‰äº† significanthçš„è¿›æ­¥ï¼Œä½¿å¾—å¯ä»¥ç”Ÿæˆå…·æœ‰å¤šæ ·æ€§å’Œé«˜è´¨é‡çš„äººä½“åŠ¨ä½œï¼Œè¿™äº›åŠ¨ä½œéƒ½éµå¾ªæ–‡æœ¬æè¿°ã€‚ç„¶è€Œï¼Œç”±äºç¼ºä¹ç»†åŒ–çš„æ–‡æœ¬æè¿°ï¼Œä»ç„¶å›°éš¾ç”Ÿæˆç»†åŒ–æˆ–ç‰¹æ®Šçš„åŠ¨ä½œã€‚ä¸ºè§£å†³è¿™é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³ç»†åŒ–äººä½“åŠ¨ä½œæ‰©æ•£æ¨¡å‹ï¼ˆFG-MDMï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆGPT-3.5ï¼‰æ¥åˆ†è§£å‰ä¸€ vague çš„æ–‡æœ¬æè¿°ï¼Œç„¶åä½¿ç”¨è¿™äº›ç»†åŒ–æè¿°æ¥å¼•å¯¼ä¸€ä¸ªåŸºäºtransformerçš„æ‰©æ•£æ¨¡å‹ã€‚FG-MDMå¯ä»¥ç”Ÿæˆç»†åŒ–å’Œç‰¹æ®Šçš„åŠ¨ä½œï¼Œå³ä½¿åœ¨è®­ç»ƒæ•°æ®çš„å¤–éƒ¨ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜FG-MDMåœ¨å‰ä¸€äº›æ–¹æ³•ä¹‹ä¸Šå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯åœ¨æ³›åŒ–æ€§æ–¹é¢ã€‚æˆ‘ä»¬è®¡åˆ’å°†æˆ‘ä»¬çš„ç»†åŒ–æ–‡æœ¬æè¿°å‘å¸ƒç»™HumanML3Då’ŒKITã€‚
</details></li>
</ul>
<hr>
<h2 id="SEVA-Leveraging-sketches-to-evaluate-alignment-between-human-and-machine-visual-abstraction"><a href="#SEVA-Leveraging-sketches-to-evaluate-alignment-between-human-and-machine-visual-abstraction" class="headerlink" title="SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction"></a>SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03035">http://arxiv.org/abs/2312.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023">https://github.com/cogtoolslab/visual_abstractions_benchmarking_public2023</a></li>
<li>paper_authors: Kushin Mukherjee, Holly Huey, Xuanchen Lu, Yael Vinker, Rio Aguina-Kang, Ariel Shamir, Judith E. Fan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è¯„ä¼°å½“å‰è§†è§‰ç®—æ³•æ˜¯å¦èƒ½å¤Ÿç†è§£äººç±»åˆ›ä½œçš„ç²—ç•¥å›¾åƒï¼Œä»¥åŠäººç±»å¯¹ç²—ç•¥å›¾åƒçš„å«ä¹‰æ˜¯å¦‚ä½•çš„ã€‚</li>
<li>methods: ç ”ç©¶è€…ä½¿ç”¨äº†ä¸€ä¸ªæ–°çš„ benchmark datasetï¼Œnamed SEVAï¼ŒåŒ…å«çº¦90,000ä¸ªäººç±»åˆ›ä½œçš„ç²—ç•¥å›¾åƒï¼Œå¹¶å¯¹å½“å‰è§†è§‰ç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥ç¡®å®šå®ƒä»¬æ˜¯å¦èƒ½å¤Ÿç†è§£äººç±»åˆ›ä½œçš„ç²—ç•¥å›¾åƒã€‚</li>
<li>results: ç ”ç©¶è€…å‘ç°ï¼Œå½“å‰è§†è§‰ç®—æ³•å¯ä»¥å¾ˆå¥½åœ°è¯†åˆ«äººç±»åˆ›ä½œçš„ç²—ç•¥å›¾åƒä¸­çš„ç›®æ ‡æ¦‚å¿µï¼Œä½†æ˜¯æ¨¡å‹å’Œäººç±»å“åº”æ¨¡å¼ä¹‹é—´ä»å­˜åœ¨ä¸€å®šçš„å·®è·ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜å‘ç°ï¼Œä¸€ç§åŸºäºäººç±»è§†è§‰æŠ½è±¡çš„ç”Ÿæˆç®—æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰ä¸åŒç²—ç•¥åº¦çš„ç²—ç•¥å›¾åƒã€‚<details>
<summary>Abstract</summary>
Sketching is a powerful tool for creating abstract images that are sparse but meaningful. Sketch understanding poses fundamental challenges for general-purpose vision algorithms because it requires robustness to the sparsity of sketches relative to natural visual inputs and because it demands tolerance for semantic ambiguity, as sketches can reliably evoke multiple meanings. While current vision algorithms have achieved high performance on a variety of visual tasks, it remains unclear to what extent they understand sketches in a human-like way. Here we introduce SEVA, a new benchmark dataset containing approximately 90K human-generated sketches of 128 object concepts produced under different time constraints, and thus systematically varying in sparsity. We evaluated a suite of state-of-the-art vision algorithms on their ability to correctly identify the target concept depicted in these sketches and to generate responses that are strongly aligned with human response patterns on the same sketch recognition task. We found that vision algorithms that better predicted human sketch recognition performance also better approximated human uncertainty about sketch meaning, but there remains a sizable gap between model and human response patterns. To explore the potential of models that emulate human visual abstraction in generative tasks, we conducted further evaluations of a recently developed sketch generation algorithm (Vinker et al., 2022) capable of generating sketches that vary in sparsity. We hope that public release of this dataset and evaluation protocol will catalyze progress towards algorithms with enhanced capacities for human-like visual abstraction.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç»˜åˆ¶æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œç”¨äºåˆ›å»ºç®€æ´è€Œæ„ä¹‰reichçš„æŠ½è±¡å›¾åƒã€‚ç†è§£ç»˜åˆ¶poseäº†ä¸€äº›æ ¸å¿ƒæ€§çš„æŒ‘æˆ˜ Ğ´Ğ»Ñé€šç”¨è§†è§‰ç®—æ³•ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å¯¹ç»˜åˆ¶çš„ç®€æ´æ€§ä¸è‡ªç„¶è§†è§‰è¾“å…¥å…·æœ‰åšå›ºçš„Robustnessï¼Œå¹¶ä¸”éœ€è¦å¿å—Semantic Ambiguityï¼Œå› ä¸ºç»˜åˆ¶å¯ä»¥å¯é åœ°è¯±å‘å¤šç§å«ä¹‰ã€‚è™½ç„¶å½“å‰çš„è§†è§‰ç®—æ³•åœ¨å¤šç§è§†è§‰ä»»åŠ¡ä¸Šå·²ç» Ğ´Ğ¾ÑÑ‚Ğ¸å¾—äº†é«˜æ€§èƒ½ï¼Œä½†æ˜¯æ²¡æœ‰å¾ˆæ¸…æ¥šåœ°çŸ¥é“å®ƒä»¬æ˜¯å¦èƒ½å¤Ÿç†è§£äººç±»çš„ç»˜åˆ¶æ–¹å¼ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†SEVAï¼Œä¸€ä¸ªæ–°çš„æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«çº¦90,000ä¸ªäººç±»ç”Ÿæˆçš„ç»˜åˆ¶ï¼Œè¿™äº›ç»˜åˆ¶æ˜¯åœ¨ä¸åŒçš„æ—¶é—´é™åˆ¶ä¸‹ç”Ÿæˆçš„ï¼Œå› æ­¤ç³»ç»Ÿåœ° varying in sparsityã€‚æˆ‘ä»¬å¯¹ä¸€äº›å½“å‰çš„State-of-the-artè§†è§‰ç®—æ³•è¿›è¡Œäº†è¯„ä¼°ï¼Œä»¥ç¡®å®šå®ƒä»¬æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®åœ°è¯†åˆ«ç»˜åˆ¶ä¸­çš„ç›®æ ‡æ¦‚å¿µï¼Œå¹¶ä¸”æ˜¯å¦èƒ½å¤Ÿç”Ÿæˆä¸äººç±»å“åº”æ¨¡å¼ç›¸ä¼¼çš„å“åº”ã€‚æˆ‘ä»¬å‘ç°ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°é¢„æµ‹äººç±»ç»˜åˆ¶è®¤çŸ¥æ€§çš„è§†è§‰ç®—æ³•ä¹Ÿæ›´å¥½åœ°é¢„æµ‹äººç±»å¯¹ç»˜åˆ¶çš„ä¸ç¡®å®šæ€§ï¼Œä½†æ˜¯è¿˜æœ‰ä¸€å®šçš„å·®è· Ğ¼ĞµĞ¶Ğ´Ñƒæ¨¡å‹å’Œäººç±»å“åº”æ¨¡å¼ã€‚ä¸ºäº†æ¢ç´¢æ¨¡å‹å¯ä»¥æ¨¡ä»¿äººç±»è§†è§‰æŠ½è±¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¿›ä¸€æ­¥çš„è¯„ä¼°ï¼Œä½¿ç”¨Vinker et al. (2022)æå‡ºçš„ä¸€ç§å¯å˜å¹²æ‰°ç»˜åˆ¶ç”Ÿæˆç®—æ³•ï¼Œå¯ä»¥ç”Ÿæˆç»˜åˆ¶çš„ä¸åŒçº§åˆ«ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å…¬å¼€è¿™ä¸ªæ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä¿ƒè¿›æ¨¡å‹å…·æœ‰äººç±»åŒ–è§†è§‰æŠ½è±¡èƒ½åŠ›çš„è¿›æ­¥ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Cortical-Anomaly-through-Masked-Encoding-for-Unsupervised-Heterogeneity-Mapping"><a href="#Learning-Cortical-Anomaly-through-Masked-Encoding-for-Unsupervised-Heterogeneity-Mapping" class="headerlink" title="Learning Cortical Anomaly through Masked Encoding for Unsupervised Heterogeneity Mapping"></a>Learning Cortical Anomaly through Masked Encoding for Unsupervised Heterogeneity Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02762">http://arxiv.org/abs/2312.02762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chadHGY/CAM">https://github.com/chadHGY/CAM</a></li>
<li>paper_authors: Hao-Chun Yang, Ole Andreassen, Lars Tjelta Westlye, Andre F. Marquand, Christian F. Beckmann, Thomas Wolfers</li>
<li>For: æ£€æµ‹å¤æ‚çš„å¤§è„‘ç–¾ç—…ï¼Œç‰¹åˆ«æ˜¯ç²¾ç¥ç–¾ç—…ï¼Œå› ä¸ºç—‡çŠ¶çš„å¤æ‚æ€§å’Œå¯é çš„ç”Ÿç‰©æ ‡å¿—ç‰©çš„ç¼ºå¤±ã€‚* Methods: ä½¿ç”¨CAMï¼ˆ cortical anomaly detection through masked image modelingï¼‰ï¼Œä¸€ç§æ–°çš„è‡ªåŠ¨è¶…çº§visedæ¡†æ¶ï¼Œé€šè¿‡ cortical surface ç‰¹å¾æ¥æ¢æµ‹å¤æ‚å¤§è„‘ç–¾ç—…ã€‚* Results: å¯¹äºç²¾ç¥ç–¾ç—…çš„æ‚£è€…ï¼Œä½¿ç”¨CAMæ¡†æ¶å¯ä»¥è¾¾åˆ° AUC 0.696 å’Œ AUC 0.769ï¼Œä¸éœ€è¦ä»»ä½•æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œåˆ†æå¼‚å¸¸ cortical åŒºåŸŸï¼ŒåŒ…æ‹¬ Pars Triangularis å’Œå¤šä¸ªå‰é¢å¶åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸç»å¸¸ä¸Schizophrenia æœ‰å…³ï¼Œå¢åŠ äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¿¡å¿ƒã€‚<details>
<summary>Abstract</summary>
The detection of heterogeneous mental disorders based on brain readouts remains challenging due to the complexity of symptoms and the absence of reliable biomarkers. This paper introduces CAM (Cortical Anomaly Detection through Masked Image Modeling), a novel self-supervised framework designed for the unsupervised detection of complex brain disorders using cortical surface features. We employ this framework for the detection of individuals on the psychotic spectrum and demonstrate its capabilities compared to state-ofthe-art methods, achieving an AUC of 0.696 for Schizoaffective and 0.769 for Schizophreniform, without the need for any labels. Furthermore, the analysis of atypical cortical regions includes Pars Triangularis and several frontal areas, often implicated in schizophrenia, provide further confidence in our approach. Altogether, we demonstrate a scalable approach for anomaly detection of complex brain disorders based on cortical abnormalities.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>è½¬æ¢æ–‡æœ¬åˆ°ç®€åŒ–ä¸­æ–‡ã€Šæ£€æµ‹ä¸åŒå‹ç¥ç»ç–¾ç—…åŸºäºè„‘è¾“å‡ºå°šæ˜¯ä¸€é¡¹æŒ‘æˆ˜ï¼Œä¸»è¦å› ä¸ºç—‡çŠ¶å¤æ‚å’Œå¯é ç”Ÿç‰©æ ‡å¿—ç‰©çš„ç¼ºå¤±ã€‚æœ¬æ–‡ä»‹ç»CAMï¼ˆ Cortical Anomaly Detection through Masked Image Modelingï¼‰ï¼Œä¸€ç§æ–°çš„è‡ªåŠ¨å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæ— ç›‘ç£çš„è¯†åˆ«å¤æ‚è„‘ç–¾ç—…ä½¿ç”¨è„‘è¡¨é¢ç‰¹å¾ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™ç§æ¡†æ¶è¿›è¡Œå¬è§‰éšœç¢å’Œåˆ†è£‚éšœç¢çš„æ£€æµ‹ï¼Œæ— éœ€ä»»ä½•æ ‡ç­¾ï¼Œè¾¾åˆ°äº†0.696å’Œ0.769çš„ROCæ›²çº¿ï¼Œå¯¹æ¯”å½“å‰æ–¹æ³•æ›´é«˜ã€‚æ­¤å¤–ï¼Œåˆ†æä¸åŒçš„è„‘åŒºï¼ŒåŒ…æ‹¬å‰åº­åŒºå’Œå‰é¢å¶çš®å±‚ï¼Œå¸¸è¢« ÑĞ²ÑĞ·Ğ°Ğ½åˆ°Schizophreniaï¼Œå¢åŠ äº†æˆ‘ä»¬çš„æ–¹æ³•çš„ä¿¡ä»»åº¦ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§å¯æ‰©å±•çš„æ–¹æ³•ï¼Œç”¨äºè¯†åˆ«å¤æ‚è„‘ç–¾ç—…çš„å¼‚å¸¸æ£€æµ‹ï¼ŒåŸºäºè„‘ç—…å˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="C3-High-performance-and-low-complexity-neural-compression-from-a-single-image-or-video"><a href="#C3-High-performance-and-low-complexity-neural-compression-from-a-single-image-or-video" class="headerlink" title="C3: High-performance and low-complexity neural compression from a single image or video"></a>C3: High-performance and low-complexity neural compression from a single image or video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02753">http://arxiv.org/abs/2312.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunjik Kim, Matthias Bauer, Lucas Theis, Jonathan Richard Schwarz, Emilien Dupont</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºå°å‹æ¨¡å‹çš„ç¥ç»å‹ç¼©æ–¹æ³•ï¼Œä»¥æé«˜å‹ç¼©ç‡å’Œè´¨é‡çš„æ€§èƒ½ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨å°å‹æ¨¡å‹è¿›è¡Œç‰¹å¾æå–å’Œç¼–ç ï¼Œå¹¶é€šè¿‡ä¸€äº›ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›æ¥æé«˜å›¾åƒå’Œè§†é¢‘å‹ç¼©æ€§èƒ½ã€‚</li>
<li>results: åœ¨CLIC2020å›¾åƒæ ‡å‡†å’ŒUVGè§†é¢‘æ ‡å‡†ä¸Šï¼Œè¯¥æ–¹æ³•å¯ä»¥ä¸å‚è€ƒå®ç°H.266ç¼–ç å™¨å’Œç¥ç»è§†é¢‘ç¼–ç å™¨ç›¸åŒ¹é…æˆ–è¶…è¶Šå…¶æ€§èƒ½ï¼Œä½†å…·æœ‰è®¸å¤š magnitudes æ›´ä½çš„è®¡ç®—å¤æ‚åº¦ã€‚<details>
<summary>Abstract</summary>
Most neural compression models are trained on large datasets of images or videos in order to generalize to unseen data. Such generalization typically requires large and expressive architectures with a high decoding complexity. Here we introduce C3, a neural compression method with strong rate-distortion (RD) performance that instead overfits a small model to each image or video separately. The resulting decoding complexity of C3 can be an order of magnitude lower than neural baselines with similar RD performance. C3 builds on COOL-CHIC (Ladune et al.) and makes several simple and effective improvements for images. We further develop new methodology to apply C3 to videos. On the CLIC2020 image benchmark, we match the RD performance of VTM, the reference implementation of the H.266 codec, with less than 3k MACs/pixel for decoding. On the UVG video benchmark, we match the RD performance of the Video Compression Transformer (Mentzer et al.), a well-established neural video codec, with less than 5k MACs/pixel for decoding.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å¤šæ•°ç¥ç»å‹ç¼©æ¨¡å‹é€šè¿‡å¤§é‡çš„å›¾åƒæˆ–è§†é¢‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¾¿æ³›åŒ–åˆ°æœªç»è§è¿‡çš„æ•°æ®ã€‚è¿™ç§æ³›åŒ–é€šå¸¸éœ€è¦å…·æœ‰é«˜åº¦è¡¨è¾¾èƒ½åŠ›å’Œé«˜è§£ç å¤æ‚åº¦çš„å¤§å‹å’Œå¤æ‚çš„å»ºç­‘ã€‚è€Œæˆ‘ä»¬æ‰€ä»‹ç»çš„C3ç¥ç»å‹ç¼©æ–¹æ³•å´é€šè¿‡å¯¹æ¯ä¸ªå›¾åƒæˆ–è§†é¢‘è¿›è¡Œç‰¹ç‚¹é€‚åº”ï¼Œè€Œä¸æ˜¯ä½¿ç”¨å¤§å‹å’Œå¤æ‚çš„å»ºç­‘ï¼Œä»¥è¾¾åˆ°ç±»ä¼¼çš„æ³›åŒ–æ€§èƒ½ã€‚å› æ­¤ï¼ŒC3çš„è§£ç å¤æ‚åº¦å¯ä»¥é™ä½è‡³å¯¹ç¥ç»åŸºelineçš„åŒç­‰æ°´å¹³ï¼Œè€Œä¸æ˜¯ç¥ç»åŸºelineçš„æ•°åå€ã€‚C3å»ºç«‹åœ¨COOL-CHICï¼ˆLaduneç­‰äººï¼‰çš„åŸºç¡€ä¹‹ä¸Šï¼Œå¹¶è¿›è¡Œäº†ä¸€äº›ç®€å•è€Œæœ‰æ•ˆçš„æ”¹è¿›ï¼Œä»¥ä¾¿åº”ç”¨äºå›¾åƒä¸Šã€‚æˆ‘ä»¬è¿˜å¼€å‘äº†æ–°çš„æ–¹æ³•ï¼Œä»¥åº”ç”¨C3äºè§†é¢‘ä¸Šã€‚åœ¨CLIC2020å›¾åƒbenchmarkä¸Šï¼Œæˆ‘ä»¬ä¸VTMï¼ˆH.266ç¼–ç å™¨çš„å‚è€ƒå®ç°ï¼‰çš„å‚è€ƒå®ç°åŒ¹é…äº†RDæ€§èƒ½ï¼Œä½†éœ€è¦å°‘äº3k MACs/åƒç´ è¿›è¡Œè§£ç ã€‚åœ¨UVGè§†é¢‘benchmarkä¸Šï¼Œæˆ‘ä»¬ä¸Video Compression Transformerï¼ˆMentzerç­‰äººï¼‰çš„ç¥ç»è§†é¢‘ç¼–ç å™¨åŒ¹é…äº†RDæ€§èƒ½ï¼Œä½†éœ€è¦å°‘äº5k MACs/åƒç´ è¿›è¡Œè§£ç ã€‚
</details></li>
</ul>
<hr>
<h2 id="C-NERF-Representing-Scene-Changes-as-Directional-Consistency-Difference-based-NeRF"><a href="#C-NERF-Representing-Scene-Changes-as-Directional-Consistency-Difference-based-NeRF" class="headerlink" title="C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF"></a>C-NERF: Representing Scene Changes as Directional Consistency Difference-based NeRF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02751">http://arxiv.org/abs/2312.02751</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/c-nerf/c-nerf">https://github.com/c-nerf/c-nerf</a></li>
<li>paper_authors: Rui Huang, Binbin Jiang, Qingyi Zhao, William Wang, Yuxiang Zhang, Qing Guo</li>
<li>for: æ£€æµ‹ neural radiance fields (NeRFs) ä¸­çš„å¯¹è±¡å˜åŒ–</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäºæ–¹å‘ä¸€è‡´æ€§çš„ NeRF è¡¨ç¤ºæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šé¦–å…ˆå¯¹ä¸¤ä¸ª NeRF å›¾åƒè¿›è¡Œç©ºé—´å¯¹é½ï¼Œç„¶åæ ¹æ®æ–¹å‘ä¸€è‡´æ€§çº¦æŸç¡®å®šå˜åŒ–ç‚¹ï¼Œæœ€åæ ¹æ®æ„å»ºçš„ NeRF ç”Ÿæˆå˜åŒ–åœ°å›¾</li>
<li>results: ä¸çŠ¶æ€è‰ºæœ¯æ–¹æ³•å’Œ NeRF åŸºäºæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ–¹æ³•å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œå¯ä»¥å‡†ç¡®æ£€æµ‹Sceneä¸­çš„å¯¹è±¡å˜åŒ–<details>
<summary>Abstract</summary>
In this work, we aim to detect the changes caused by object variations in a scene represented by the neural radiance fields (NeRFs). Given an arbitrary view and two sets of scene images captured at different timestamps, we can predict the scene changes in that view, which has significant potential applications in scene monitoring and measuring. We conducted preliminary studies and found that such an exciting task cannot be easily achieved by utilizing existing NeRFs and 2D change detection methods with many false or missing detections. The main reason is that the 2D change detection is based on the pixel appearance difference between spatial-aligned image pairs and neglects the stereo information in the NeRF. To address the limitations, we propose the C-NERF to represent scene changes as directional consistency difference-based NeRF, which mainly contains three modules. We first perform the spatial alignment of two NeRFs captured before and after changes. Then, we identify the change points based on the direction-consistent constraint; that is, real change points have similar change representations across view directions, but fake change points do not. Finally, we design the change map rendering process based on the built NeRFs and can generate the change map of an arbitrarily specified view direction. To validate the effectiveness, we build a new dataset containing ten scenes covering diverse scenarios with different changing objects. Our approach surpasses state-of-the-art 2D change detection and NeRF-based methods by a significant margin.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç›®æ ‡æ˜¯æ£€æµ‹å¯¹è±¡å˜åŒ–åœ¨ç”±ç¥ç»è¾å°„åœºï¼ˆNeRFï¼‰è¡¨ç¤ºçš„åœºæ™¯ä¸­ã€‚ç»™å‡ºä»»æ„è§†è§’å’Œä¸¤ä¸ªä¸åŒæ—¶é—´ç‚¹æ‹æ‘„çš„åœºæ™¯å›¾åƒå¯¹ï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹åœºæ™¯ä¸­çš„å˜åŒ–ï¼Œè¿™æœ‰ç€é‡è¦çš„åº”ç”¨å‰æ™¯ï¼ŒåŒ…æ‹¬åœºæ™¯ç›‘æµ‹å’Œé‡æµ‹ã€‚æˆ‘ä»¬è¿›è¡Œäº†åˆæ­¥ç ”ç©¶å‘ç°ï¼Œè¿™æ ·çš„æŒ‘æˆ˜ä¸å¯èƒ½é€šè¿‡ç°æœ‰çš„NeRFå’Œ2Då˜åŒ–æ£€æµ‹æ–¹æ³•æ¥å®ç°ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•å­˜åœ¨è®¸å¤šå‡æˆ–ç¼ºå¤±æ£€æµ‹ã€‚ä¸»è¦åŸå› æ˜¯2Då˜åŒ–æ£€æµ‹åŸºäºå›¾åƒå¯¹çš„ç©ºé—´å¯¹é½ï¼Œå¿½ç•¥äº†NeRFä¸­çš„ç«‹ä½“ä¿¡æ¯ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†C-NERFï¼Œå®ƒè¡¨ç¤ºåœºæ™¯å˜åŒ–ä¸ºæ–¹å‘å·®å¼‚åŸºäºNeRFçš„æ–¹å‘ä¸€è‡´æ€§å·®å¼‚ã€‚C-NERFä¸»è¦åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬å°†ä¸¤ä¸ªåœ¨beforeå’Œafterå˜åŒ–ä¹‹å‰æ‹æ‘„çš„NeRFè¿›è¡Œç©ºé—´å¯¹é½ã€‚ç„¶åï¼Œæˆ‘ä»¬åŸºäºæ–¹å‘ä¸€è‡´æ€§çº¦æŸï¼ˆå®é™…å˜åŒ–ç‚¹åœ¨ä¸åŒè§†å‘ä¸­åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„å˜åŒ–è¡¨è¾¾ï¼‰æ¥ç¡®å®šå˜åŒ–ç‚¹ã€‚æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†åŸºäºå»ºç«‹çš„NeRFå’Œå˜åŒ–åœ°å›¾çš„æ¸²æŸ“è¿‡ç¨‹ï¼Œå¯ä»¥ç”Ÿæˆä»»æ„æŒ‡å®šè§†å‘çš„å˜åŒ–åœ°å›¾ã€‚ä¸ºè¯æ˜æ•ˆæœï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬åä¸ªåœºæ™¯ï¼Œå…¶ä¸­æ¯ä¸ªåœºæ™¯éƒ½åŒ…å«ä¸åŒçš„å˜åŒ–å¯¹è±¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¯¹æ¯”ç°æœ‰çš„2Då˜åŒ–æ£€æµ‹å’ŒNeRFåŸºäºæ–¹æ³•æ—¶è¡¨ç°å‡ºäº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="LiDAR-based-Person-Re-identification"><a href="#LiDAR-based-Person-Re-identification" class="headerlink" title="LiDAR-based Person Re-identification"></a>LiDAR-based Person Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03033">http://arxiv.org/abs/2312.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Guo, Zhiyu Pan, Yingping Liang, Ziheng Xi, Zhi Chen Zhong, Jianjiang Feng, Jie Zhou</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦é’ˆå¯¹äººä½“é‡è¯†åˆ«ï¼ˆReIDï¼‰é¢†åŸŸçš„Camera-basedç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜äººä½“é‡è¯†åˆ«çš„ç²¾åº¦å’Œå¯é æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLiDARçš„äººä½“é‡è¯†åˆ«æ¡†æ¶ï¼Œç§°ä¸ºReID3Dï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒç­–ç•¥æ¥æå–3Däººä½“å½¢æ€ç‰¹å¾ï¼Œå¹¶ introduces Graph-based Complementary Enhancement Encoderæ¥æå–å…¨é¢ç‰¹å¾ã€‚</li>
<li>results: ç»è¿‡å¹¿æ³›çš„å®éªŒï¼ŒReID3Dåœ¨LReID datasetä¸Šå®ç°äº†éå¸¸å‡ºè‰²çš„æ€§èƒ½ï¼Œrank-1å‡†ç¡®ç‡è¾¾94.0%ï¼Œè¿™è¡¨æ˜LiDARå¯ä»¥å¾ˆå¥½åœ°è§£å†³äººä½“é‡è¯†åˆ«ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
Camera-based person re-identification (ReID) systems have been widely applied in the field of public security. However, cameras often lack the perception of 3D morphological information of human and are susceptible to various limitations, such as inadequate illumination, complex background, and personal privacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that utilizes pre-training strategy to retrieve features of 3D body shape and introduces Graph-based Complementary Enhancement Encoder for extracting comprehensive features. Due to the lack of LiDAR datasets, we build LReID, the first LiDAR-based person ReID dataset, which is collected in several outdoor scenes with variations in natural conditions. Additionally, we introduce LReID-sync, a simulated pedestrian dataset designed for pre-training encoders with tasks of point cloud completion and shape parameter learning. Extensive experiments on LReID show that ReID3D achieves exceptional performance with a rank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in addressing person ReID tasks. To the best of our knowledge, we are the first to propose a solution for LiDAR-based ReID. The code and datasets will be released soon.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ‘„åƒå¤´åŸºäºäººè¯†åˆ«ï¼ˆReIDï¼‰ç³»ç»Ÿåœ¨å…¬å…±å®‰å…¨é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œä½†æ‘„åƒå¤´é€šå¸¸ç¼ºä¹äººä½“ä¸‰ç»´å½¢æ€ä¿¡æ¯çš„æ„ŸçŸ¥ï¼Œå¹¶ä¸”å®¹æ˜“å—åˆ°ä¸è‰¯ç¯å¢ƒã€å¤æ‚èƒŒæ™¯å’Œä¸ªäººéšç§ç­‰é™åˆ¶ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºLiDARçš„äººè¯†åˆ«æ¡†æ¶ï¼Œå³ReID3Dï¼Œè¯¥æ¡†æ¶åˆ©ç”¨é¢„è®­ç»ƒç­–ç•¥æ¥æ£€ç´¢äººä½“ä¸‰ç»´å½¢æ€ç‰¹å¾ï¼Œå¹¶ introduceäº†å›¾åƒåŸºæœ¬è¡¥å……ç¼–ç å™¨ä»¥æå–å…¨é¢ç‰¹å¾ã€‚ç”±äºLiDARæ•°æ®é›†ç¼ºä¹ï¼Œæˆ‘ä»¬å»ºç«‹äº†LReIDï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäºLiDARçš„äººè¯†åˆ«æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åœ¨è‡ªç„¶æ¡ä»¶ä¸‹çš„å¤šä¸ªæˆ·å¤–åœºæ™¯ä¸­é‡‡é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†LReID-syncï¼Œä¸€ä¸ªæ¨¡æ‹Ÿäººå‘˜æ•°æ®é›†ï¼Œç”¨äºé¢„è®­ç»ƒç¼–ç å™¨ï¼ŒåŒ…æ‹¬ç‚¹äº‘å®Œæˆä»»åŠ¡å’Œå½¢æ€å‚æ•°å­¦ä¹ ä»»åŠ¡ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒReID3Dåœ¨LReIDä¸Šè¾¾åˆ°äº†94.0çš„æ’åä¸€ç²¾åº¦ï¼Œ highlighting LiDARåœ¨äººè¯†åˆ«ä»»åŠ¡ä¸­çš„æ½œåœ¨æ½œåŠ›ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ˜¯é¦–æ¬¡æå‡ºäº†LiDARåŸºäºçš„äººè¯†åˆ«è§£å†³æ–¹æ¡ˆã€‚ä»£ç å’Œæ•°æ®å°†å¾ˆå¿«å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="R3D-SWIN-Use-Shifted-Window-Attention-for-Single-View-3D-Reconstruction"><a href="#R3D-SWIN-Use-Shifted-Window-Attention-for-Single-View-3D-Reconstruction" class="headerlink" title="R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction"></a>R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02725">http://arxiv.org/abs/2312.02725</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhuan Li, Meihua Xiao, zehuan li, Mengxi Gao</li>
<li>for: æé«˜å•è§†é‡å»ºç²¾åº¦</li>
<li>methods: shifted windows attention voxel 3D reconstruction network</li>
<li>results: SOTAå•è§†é‡å»ºç²¾åº¦åœ¨ShapeNetä¸Š<details>
<summary>Abstract</summary>
Recently, vision transformers have performed well in various computer vision tasks, including voxel 3D reconstruction. However, the windows of the vision transformer are not multi-scale, and there is no connection between the windows, which limits the accuracy of voxel 3D reconstruction . Therefore, we propose a shifted windows attention voxel 3D reconstruction network. To the best of our knowledge, this is the first work to apply shifted window attention to voxel 3D reconstruction. Experimental results on ShapeNet verify our method achieves SOTA accuracy in single-view reconstruction.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘æœŸï¼Œè§†è§‰è½¬æ¢å™¨åœ¨ä¸åŒè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬ voxel 3D é‡å»ºã€‚ç„¶è€Œï¼Œè§†è§‰è½¬æ¢å™¨çš„çª—å£ä¸æ˜¯å¤šå°ºåº¦çš„ï¼Œæ²¡æœ‰è¿æ¥ Ğ¼ĞµĞ¶Ğ´Ñƒ çª—å£ï¼Œè¿™é™åˆ¶äº† voxel 3D é‡å»ºçš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æè®®ä¸€ç§shiftedçª—å£æ³¨æ„åŠ› voxel 3D é‡å»ºç½‘ç»œã€‚æ ¹æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åº”ç”¨ shifted çª—å£æ³¨æ„åŠ›åˆ° voxel 3D é‡å»ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ShapeNetä¸Š achieve SOTA ç²¾åº¦åœ¨å•è§†é‡å»ºä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation"><a href="#MyPortrait-Morphable-Prior-Guided-Personalized-Portrait-Generation" class="headerlink" title="MyPortrait: Morphable Prior-Guided Personalized Portrait Generation"></a>MyPortrait: Morphable Prior-Guided Personalized Portrait Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02703">http://arxiv.org/abs/2312.02703</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bo Ding, Zhenfeng Fan, Shuang Yang, Shihong Xia</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯å…³äºè®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ç”ŸæˆçœŸå®çš„äººè„¸æ¼”ç¤ºçš„ç ”ç©¶ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ã€é€šç”¨ã€çµæ´»çš„ç¥ç»ç½‘ç»œæ¡†æ¶â€”â€”Myportraitï¼Œå¯ä»¥åœ¨å•ä¸ªäººçš„å•è§†é•œè§†é¢‘ä¸­ç”Ÿæˆä¸ªæ€§åŒ–çš„è„¸éƒ¨åŠ¨ç”»ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¤šç§ Ğ¼ĞµÑ‚Ñ€Ğ¸ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šäº†ç°æœ‰çš„æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æä¾›äº†åœ¨çº¿å’Œç¦»çº¿ä¸¤ç§ç‰ˆæœ¬ï¼Œå¯ä»¥æ ¹æ®æµ‹è¯•æ•°æ®æ˜¯å¦é€åˆ°è®­ç»ƒæ¥é€‰æ‹©ã€‚<details>
<summary>Abstract</summary>
Generating realistic talking faces is an interesting and long-standing topic in the field of computer vision. Although significant progress has been made, it is still challenging to generate high-quality dynamic faces with personalized details. This is mainly due to the inability of the general model to represent personalized details and the generalization problem to unseen controllable parameters. In this work, we propose Myportrait, a simple, general, and flexible framework for neural portrait generation. We incorporate personalized prior in a monocular video and morphable prior in 3D face morphable space for generating personalized details under novel controllable parameters. Our proposed framework supports both video-driven and audio-driven face animation given a monocular video of a single person. Distinguished by whether the test data is sent to training or not, our method provides a real-time online version and a high-quality offline version. Comprehensive experiments in various metrics demonstrate the superior performance of our method over the state-of-the-art methods. The code will be publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘æˆç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ç”ŸæˆçœŸå®å¯¹è¯é¢æ˜¯ä¸€ä¸ªæœ‰è¶£å’Œé•¿æœŸçš„è¯é¢˜ã€‚è™½ç„¶å·²ç»åšå‡ºäº†é‡è¦çš„è¿›æ­¥ï¼Œä½†ä»ç„¶å›°éš¾ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åŠ¨æ€é¢å­” WITH ä¸ªæ€§åŒ–ç»†èŠ‚ã€‚è¿™ä¸»è¦æ˜¯å› ä¸ºæ™®é€šçš„æ¨¡å‹æ— æ³•è¡¨ç¤ºä¸ªæ€§åŒ–ç»†èŠ‚ï¼Œä»¥åŠæ€»ç»“é—®é¢˜åˆ°æœªç»è§è¿‡çš„å¯æ§å‚æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æè®®Myportraitï¼Œä¸€ä¸ªç®€å•ã€é€šç”¨å’Œ flexibleçš„ç¥ç»ç«¯æ¸¸æˆæ¡†æ¶ã€‚æˆ‘ä»¬åœ¨å•ä¸ªäººçš„å•è§†å›¾ä¸­ incorporate ä¸ªæ€§åŒ–å…ˆéªŒå’Œ 3D é¢éƒ¨å¯å˜ç©ºé—´ä¸­çš„æ¨¡æ¿å…ˆéªŒï¼Œä»¥ç”Ÿæˆä¸ªæ€§åŒ–ç»†èŠ‚ unter novelå¯æ§å‚æ•°ã€‚æˆ‘ä»¬çš„æè®®çš„æ¡†æ¶æ”¯æŒå•è§†å›¾é©±åŠ¨å’ŒéŸ³é¢‘é©±åŠ¨çš„é¢éƒ¨åŠ¨ç”»ï¼Œå¹¶æ ¹æ®æµ‹è¯•æ•°æ®æ˜¯å¦é€å¾€è®­ç»ƒè€Œåˆ†ä¸ºå®æ—¶åœ¨çº¿ç‰ˆæœ¬å’Œé«˜è´¨é‡ç¦»çº¿ç‰ˆæœ¬ã€‚å¯¹äºä¸åŒçš„ç»´åº¦è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨Various metricä¸­è¡¨ç°å‡ºäº†ä¸å½“å‰æœ€ä½³æ–¹æ³•çš„è¶…è¶Šæ€§ã€‚ä»£ç å°†å…¬å¼€availableã€‚
</details></li>
</ul>
<hr>
<h2 id="Neural-Sign-Actors-A-diffusion-model-for-3D-sign-language-production-from-text"><a href="#Neural-Sign-Actors-A-diffusion-model-for-3D-sign-language-production-from-text" class="headerlink" title="Neural Sign Actors: A diffusion model for 3D sign language production from text"></a>Neural Sign Actors: A diffusion model for 3D sign language production from text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02702">http://arxiv.org/abs/2312.02702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasileios Baltatzis, Rolandos Alexandros Potamias, Evangelos Ververas, Guanxiong Sun, Jiankang Deng, Stefanos Zafeiriou</li>
<li>for: æé«˜æ‰‹è¯­ç”Ÿæˆçš„çœŸå®æ€§å’Œå«ä¹‰ç²¾åº¦</li>
<li>methods: ä½¿ç”¨æ‰©æ•£è¿‡ç¨‹å’ŒåŸºäºä½“æ ¼ç¥ç»ç½‘ç»œçš„æ–°æ–¹æ³•</li>
<li>results: ä¸å‰æ–¹æ³•ç›¸æ¯”ï¼Œæ˜¾è‘—æé«˜æ‰‹è¯­ç”Ÿæˆçš„æ€§èƒ½å’ŒçœŸå®æ€§<details>
<summary>Abstract</summary>
Sign Languages (SL) serve as the predominant mode of communication for the Deaf and Hard of Hearing communities. The advent of deep learning has aided numerous methods in SL recognition and translation, achieving remarkable results. However, Sign Language Production (SLP) poses a challenge for the computer vision community as the motions generated must be realistic and have precise semantic meanings. Most SLP methods rely on 2D data, thus impeding their ability to attain a necessary level of realism. In this work, we propose a diffusion-based SLP model trained on a curated large-scale dataset of 4D signing avatars and their corresponding text transcripts. The proposed method can generate dynamic sequences of 3D avatars from an unconstrained domain of discourse using a diffusion process formed on a novel and anatomically informed graph neural network defined on the SMPL-X body skeleton. Through a series of quantitative and qualitative experiments, we show that the proposed method considerably outperforms previous methods of SLP. We believe that this work presents an important and necessary step towards realistic neural sign avatars, bridging the communication gap between Deaf and hearing communities. The code, method and generated data will be made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€Šæ‰‹è¯­ç”Ÿæˆï¼ˆSLPï¼‰ã€‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é—®é¢˜ï¼Œå› ä¸ºå®ƒéœ€è¦ç”Ÿæˆçš„æ‰‹åŠ¿å¿…é¡»å…·æœ‰çœŸå®çš„æ„æ€å’Œç²¾å‡†çš„ semanticsã€‚å¤§å¤šæ•°SLPæ–¹æ³•ä»…ä½¿ç”¨2Dæ•°æ®ï¼Œå› æ­¤å®ƒä»¬æ— æ³•è¾¾åˆ°æ‰€éœ€çš„çœŸå®æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„SLPæ¨¡å‹ï¼Œé€šè¿‡ä¸€ä¸ªå¤§è§„æ¨¡çš„4Dç­¾åäººä½“æ•°æ®é›†å’Œå…¶å¯¹åº”çš„æ–‡æœ¬è¯‘æœ¬æ¥è®­ç»ƒã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨æ²¡æœ‰çº¦æŸçš„è¯è¯­åŸŸä¸­ç”ŸæˆåŠ¨æ€çš„3Däººä½“åºåˆ—ï¼Œä½¿ç”¨ä¸€ç§åŸºäºæ–°çš„ä½“ç³»å­¦ informed graph neural networkï¼ˆSMPL-Xï¼‰çš„æ‰©æ•£è¿‡ç¨‹ã€‚é€šè¿‡ä¸€ç³»åˆ—çš„é‡åŒ–å’Œè´¨é‡æµ‹è¯•ï¼Œæˆ‘ä»¬è¡¨æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•èˆ’é€‚æ€§å’ŒçœŸå®æ€§åœ¨SLPæ–¹æ³•ä¸­æ˜æ˜¾æé«˜ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™é¡¹å·¥ä½œå¯¹äºå®ç°çœŸå®çš„ç¥ç»ç­¾åäººä½“åšå‡ºäº†é‡è¦å’Œå¿…è¦çš„è´¡çŒ®ï¼Œ bridging the communication gap between Deaf and hearing communitiesã€‚æˆ‘ä»¬å°†ä»£ç ã€æ–¹æ³•å’Œç”Ÿæˆçš„æ•°æ®å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Revisit-Human-Scene-Interaction-via-Space-Occupancy"><a href="#Revisit-Human-Scene-Interaction-via-Space-Occupancy" class="headerlink" title="Revisit Human-Scene Interaction via Space Occupancy"></a>Revisit Human-Scene Interaction via Space Occupancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02700">http://arxiv.org/abs/2312.02700</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinpeng Liu, Haowen Hou, Yanchao Yang, Yong-Lu Li, Cewu Lu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³äººSceneäº¤äº’ï¼ˆHSIï¼‰ç”Ÿæˆ task ä¸­çš„æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œå³é«˜è´¨é‡çš„äººå’Œ3Dç¯å¢ƒåŒæ—¶æ•æ‰æ•°æ®åŒ®ä¹ï¼Œå¯¼è‡´æ•°æ®å¤šæ ·æ€§å’Œå¤æ‚æ€§å—é™ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„äººOccupancyäº¤äº’è§†å›¾ï¼Œå³å°†äººçš„è¿åŠ¨åºåˆ—çœ‹ä½œæ˜¯ä¸åœºæ™¯çš„ç©ºé—´å ç”¨äº¤äº’çš„è®°å½•ï¼Œä»è€Œå°†è¿åŠ¨åºåˆ—æ•°æ®èšåˆæˆå¤§è§„æ¨¡çš„å¯¹åº”äººOccupancyäº¤äº’æ•°æ®åº“ï¼ˆMOBï¼‰ã€‚</li>
<li>results: é€šè¿‡åœ¨MOBä¸Šè®­ç»ƒå•ä¸ªè¿åŠ¨æ§åˆ¶å™¨ï¼Œå¯ä»¥åœ¨ä¸åŒçš„é™æ€å’ŒåŠ¨æ€åœºæ™¯ä¸­ç”Ÿæˆå®é™…å’Œç¨³å®šçš„HSIè¿åŠ¨ï¼Œè€Œæ— éœ€GT 3Dåœºæ™¯è®­ç»ƒã€‚ codeså’Œæ•°æ®å°†åœ¨<a target="_blank" rel="noopener" href="https://foruck.github.io/occu-page/%E4%B8%8A%E5%85%AC%E5%BC%80%E3%80%82">https://foruck.github.io/occu-page/ä¸Šå…¬å¼€ã€‚</a><details>
<summary>Abstract</summary>
Human-scene Interaction (HSI) generation is a challenging task and crucial for various downstream tasks. However, one of the major obstacles is the limited data scale. High-quality data with simultaneously captured human and 3D environments is rare, resulting in limited data diversity and complexity. In this work, we argue that interaction with a scene is essentially interacting with the space occupancy of the scene from an abstract physical perspective, leading us to a unified novel view of Human-Occupancy Interaction. By treating pure motion sequences as records of humans interacting with invisible scene occupancy, we can aggregate motion-only data into a large-scale paired human-occupancy interaction database: Motion Occupancy Base (MOB). Thus, the need for costly paired motion-scene datasets with high-quality scene scans can be substantially alleviated. With this new unified view of Human-Occupancy interaction, a single motion controller is proposed to reach the target state given the surrounding occupancy. Once trained on MOB with complex occupancy layout, the controller could handle cramped scenes and generalize well to general scenes with limited complexity. With no GT 3D scenes for training, our method can generate realistic and stable HSI motions in diverse scenarios, including both static and dynamic scenes. Our code and data would be made publicly available at https://foruck.github.io/occu-page/.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººSceneäº¤äº’ï¼ˆHSIï¼‰ç”Ÿæˆæ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå¯¹ä¸‹æ¸¸ä»»åŠ¡éå¸¸é‡è¦ã€‚ç„¶è€Œï¼Œä¸€ä¸ªä¸»è¦çš„éšœç¢æ˜¯æ•°æ®è§„æ¨¡çš„é™åˆ¶ã€‚é«˜è´¨é‡çš„æ•°æ®ï¼ŒåŒæ—¶æ•æ‰äººå’Œ3Dç¯å¢ƒï¼Œç½•è§ï¼Œå¯¼è‡´æ•°æ®å¤šæ ·æ€§å’Œå¤æ‚æ€§å—é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºäººä¸åœºæ™¯äº¤äº’çš„æ ¸å¿ƒæ˜¯ä¸åœºæ™¯ç©ºé—´å ç”¨çš„æŠ½è±¡ç‰©ç†è§†è§’ç›¸äº’ä½œç”¨ï¼Œä»è€Œå¯¼è‡´æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„äººå ç”¨äº¤äº’è§†å›¾ã€‚æˆ‘ä»¬å°†çº¯æ´çš„è¿åŠ¨åºåˆ—çœ‹ä½œäººä¸é€æ˜åœºæ™¯å ç”¨çš„äº¤äº’è®°å½•ï¼Œä»è€Œå°†è¿åŠ¨åºåˆ—èšåˆæˆå¤§è§„æ¨¡çš„äººå ç”¨äº¤äº’æ•°æ®åº“ï¼šè¿åŠ¨å ç”¨åŸºç¡€ï¼ˆMOBï¼‰ã€‚å› æ­¤ï¼Œéœ€è¦é«˜è´¨é‡çš„ç›¸å…³è¿åŠ¨åœºæ™¯æ•°æ®å’Œåœºæ™¯æ‰«ææ•°æ®çš„æˆæœ¬å¯ä»¥å¾—åˆ°å¾ˆå¤§çš„å‡å¼±ã€‚é€šè¿‡è¿™ç§æ–°çš„äººå ç”¨äº¤äº’è§†å›¾ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å•ä¸ªè¿åŠ¨æ§åˆ¶å™¨ï¼Œå¯ä»¥åœ¨å›ºå®šæˆ–åŠ¨æ€åœºæ™¯ä¸­è¾¾åˆ°ç›®æ ‡çŠ¶æ€ï¼Œåªéœ€è¦çŸ¥é“å‘¨å›´çš„å ç”¨ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦GT 3Dåœºæ™¯è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥ç”ŸæˆçœŸå®å’Œç¨³å®šçš„HSIè¿åŠ¨ï¼Œåœ¨å¤šæ ·åŒ–çš„åœºæ™¯ä¸­ï¼ŒåŒ…æ‹¬é™æ­¢å’ŒåŠ¨æ€åœºæ™¯ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ•°æ®å°†åœ¨https://foruck.github.io/occu-page/ä¸Šå…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="UPOCR-Towards-Unified-Pixel-Level-OCR-Interface"><a href="#UPOCR-Towards-Unified-Pixel-Level-OCR-Interface" class="headerlink" title="UPOCR: Towards Unified Pixel-Level OCR Interface"></a>UPOCR: Towards Unified Pixel-Level OCR Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02694">http://arxiv.org/abs/2312.02694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dezhi Peng, Zhenhua Yang, Jiaxin Zhang, Chongyu Liu, Yongxin Shi, Kai Ding, Fengjun Guo, Lianwen Jin</li>
<li>for: æé«˜ pixel-level OCR é¢†åŸŸçš„ç ”ç©¶å’Œåº”ç”¨æ•ˆç‡ï¼Œå»ºç«‹ä¸€ä¸ªé€šç”¨çš„ OCR æ¨¡å‹ï¼Œèƒ½åŒæ—¶å¤„ç†å¤šç§ä»»åŠ¡ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ç§åŸºäº Vision Transformer çš„æ™®é€šæ¨¡å‹ï¼Œé€šè¿‡å­¦ä¹ ä»»åŠ¡æç¤ºæ¥æ¿€æ´»ä»»åŠ¡ç‰¹å¾è¡¨ç¤ºï¼Œå®ç°å¤šä»»åŠ¡å…±äº«è¡¨ç¤ºçš„ç›®æ ‡ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯åŒæ—¶åœ¨ä¸‰ç§ pixel-level OCR ä»»åŠ¡ä¸Šè¾¾åˆ°çŠ¶æ€å¯¹åº”çš„è¡¨ç°æ°´å¹³ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´å…±äº«è¡¨ç¤ºï¼Œæä¾›äº† valuable çš„ç ”ç©¶ç­–ç•¥å’Œæ„è§ Ğ´Ğ»Ñæœªæ¥çš„é€šç”¨ OCR æ¨¡å‹ã€‚<details>
<summary>Abstract</summary>
In recent years, the optical character recognition (OCR) field has been proliferating with plentiful cutting-edge approaches for a wide spectrum of tasks. However, these approaches are task-specifically designed with divergent paradigms, architectures, and training strategies, which significantly increases the complexity of research and maintenance and hinders the fast deployment in applications. To this end, we propose UPOCR, a simple-yet-effective generalist model for Unified Pixel-level OCR interface. Specifically, the UPOCR unifies the paradigm of diverse OCR tasks as image-to-image transformation and the architecture as a vision Transformer (ViT)-based encoder-decoder. Learnable task prompts are introduced to push the general feature representations extracted by the encoder toward task-specific spaces, endowing the decoder with task awareness. Moreover, the model training is uniformly aimed at minimizing the discrepancy between the generated and ground-truth images regardless of the inhomogeneity among tasks. Experiments are conducted on three pixel-level OCR tasks including text removal, text segmentation, and tampered text detection. Without bells and whistles, the experimental results showcase that the proposed method can simultaneously achieve state-of-the-art performance on three tasks with a unified single model, which provides valuable strategies and insights for future research on generalist OCR models. Code will be publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰é¢†åŸŸå‡ºç°äº†å¤§é‡å…ˆè¿›æŠ€æœ¯ï¼Œæ¶µç›–å„ç§ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™äº›æŠ€æœ¯å…·æœ‰ä¸åŒçš„æ€æƒ³ã€ç»“æ„å’Œè®­ç»ƒç­–ç•¥ï¼Œè¿™ä½¿ç ”ç©¶å’Œç»´æŠ¤å˜å¾—æ›´åŠ å¤æ‚ï¼Œå¦¨ç¢å¿«é€Ÿåº”ç”¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†UPOCRï¼Œä¸€ç§ç®€å• yetæœ‰æ•ˆçš„é€šç”¨æ¨¡å‹ï¼Œç”¨äºç»Ÿä¸€åƒç´ çº§OCRæ¥å£ã€‚å…·ä½“æ¥è¯´ï¼ŒUPOCRå°†å¤šç§OCRä»»åŠ¡è§†ä¸ºå›¾åƒåˆ°å›¾åƒè½¬æ¢ï¼Œå¹¶é‡‡ç”¨åŸºäºè§†Transformerï¼ˆViTï¼‰çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚æˆ‘ä»¬å¼•å…¥å¯å­¦ä¹ çš„ä»»åŠ¡æç¤ºï¼Œä½¿ç¼–ç å™¨æå–çš„é€šç”¨ç‰¹å¾è¡¨ç¤ºpushåˆ°ä»»åŠ¡ç‰¹æœ‰çš„ç©ºé—´ï¼Œè®©è§£ç å™¨å…·å¤‡ä»»åŠ¡æ„è¯†ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è®­ç»ƒçš„ç›®æ ‡æ˜¯å°†ç”Ÿæˆçš„å›¾åƒä¸çœŸå®å›¾åƒçš„å·®å¼‚é™åˆ°æœ€å°åŒ–ï¼Œä¸ç®¡ä»»åŠ¡ä¹‹é—´çš„ä¸åŒã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªåƒç´ çº§OCRä»»åŠ¡è¿›è¡Œäº†å®éªŒï¼ŒåŒ…æ‹¬æ–‡æœ¬é™¤å»ã€æ–‡æœ¬åˆ†å‰²å’Œå—æŸæ–‡æœ¬æ£€æµ‹ã€‚æ— è®ºæœ‰å¤šå°‘è¾‰ç…Œçš„æŠ€æœ¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åŒæ—¶åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šè¾¾åˆ°çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ™ºèƒ½æ€§çš„æ€§èƒ½ï¼Œæä¾›äº†æœ‰ä»·å€¼çš„ç­–ç•¥å’Œæ€è·¯ Ğ´Ğ»Ñæœªæ¥çš„é€šç”¨OCRæ¨¡å‹ç ”ç©¶ã€‚ä»£ç å°†å…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="DeepPointMap-Advancing-LiDAR-SLAM-with-Unified-Neural-Descriptors"><a href="#DeepPointMap-Advancing-LiDAR-SLAM-with-Unified-Neural-Descriptors" class="headerlink" title="DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors"></a>DeepPointMap: Advancing LiDAR SLAM with Unified Neural Descriptors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02684">http://arxiv.org/abs/2312.02684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaze Zhang, Ziheng Ding, Qi Jing, Yuejie Zhang, Wenchao Ding, Rui Feng</li>
<li>for: æé«˜ simultaneous localization and mapping (SLAM) çš„ç²¾åº¦å’Œæ•ˆç‡</li>
<li>methods: ä½¿ç”¨ç¥ç»ç½‘ç»œæå–é«˜åº¦è¡¨ç¤ºæ€§çš„ç‚¹äº‘æè¿°ç¬¦ï¼Œå®ç°å†…å­˜æœ‰æ•ˆçš„åœ°å›¾è¡¨ç¤ºå’Œç²¾å‡†çš„å¤šå°ºåº¦æœ¬åœ°åŒ–ä»»åŠ¡</li>
<li>results: åœ¨å¤šç§å¤æ‚çš„åœºæ™¯ä¸­ï¼ŒåŒ…æ‹¬å¤šæœºåˆä½œSLAM, å®ç°äº†ä¼˜ç§€çš„ç»“æœï¼Œè¯æ˜äº†æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›<details>
<summary>Abstract</summary>
Point clouds have shown significant potential in various domains, including Simultaneous Localization and Mapping (SLAM). However, existing approaches either rely on dense point clouds to achieve high localization accuracy or use generalized descriptors to reduce map size. Unfortunately, these two aspects seem to conflict with each other. To address this limitation, we propose a unified architecture, DeepPointMap, achieving excellent preference on both aspects. We utilize neural network to extract highly representative and sparse neural descriptors from point clouds, enabling memory-efficient map representation and accurate multi-scale localization tasks (e.g., odometry and loop-closure). Moreover, we showcase the versatility of our framework by extending it to more challenging multi-agent collaborative SLAM. The promising results obtained in these scenarios further emphasize the effectiveness and potential of our approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
å‡ ä½• clouds å·²ç»æ˜¾ç¤ºå‡ºäº†å¤šä¸ªé¢†åŸŸçš„æ½œåœ¨èƒ½åŠ›ï¼ŒåŒ…æ‹¬åŒæ—¶åœ°ä½å’Œåœ°å›¾å¯¹æ¥ï¼ˆSLAMï¼‰ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•å¯èƒ½ä¼šä¾èµ–äºç´§å¯†çš„å‡ ä½• clouds ä»¥è¾¾åˆ°é«˜åœ°ä½å‡†ç¡®æ€§ï¼Œæˆ–è€…ä½¿ç”¨é€šç”¨çš„æè¿°å­æ¥å‡å°‘åœ°å›¾çš„å¤§å°ã€‚å¯æƒœçš„æ˜¯ï¼Œè¿™ä¸¤ä¸ªæ–¹é¢ä¼¼ä¹ç›¸äº’æŠµè§¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€æ¶æ„ï¼Œæ·±åº¦ç‚¹å›¾ï¼ˆDeepPointMapï¼‰ï¼Œå®ç°äº†é«˜åº¦çš„é€‰æ‹©æ€§å’Œé«˜ç²¾åº¦çš„å¤šå°ºåº¦åœ°ä½ä»»åŠ¡ï¼ˆä¾‹å¦‚è¿åŠ¨å’Œå…³é—­ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†æ¡†æ¶æ‰©å±•åˆ°æ›´åŠ æŒ‘æˆ˜æ€§çš„å¤šæœºåˆä½œSLAMã€‚åœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ‰€å–å¾—çš„ç»“æœç»™å‡ºäº†æ·±åº¦ç‚¹å›¾çš„æœ‰æ•ˆæ€§å’Œæ½œåŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Point-Cloud-Registration"><a href="#Zero-Shot-Point-Cloud-Registration" class="headerlink" title="Zero-Shot Point Cloud Registration"></a>Zero-Shot Point Cloud Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03032">http://arxiv.org/abs/2312.03032</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Wang, Guofeng Mei, Bin Ren, Xiaoshui Huang, Fabio Poiesi, Luc Van Gool, Nicu Sebe, Bruno Lepri</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºç¬¬ä¸€ä¸ªä¸éœ€è¦ç‰¹å®šæ•°æ®é›†è®­ç»ƒçš„ç‚¹äº‘æ³¨å†Œæ–¹æ³•ï¼Œä»¥æé«˜æ³¨å†Œç²¾åº¦å’Œæ•ˆç‡ã€‚</li>
<li>methods:  ZeroRegæ–¹æ³•åŸºäºå›¾åƒç‰¹å¾ä¼ è¾“ï¼Œé€šè¿‡åœ¨3Dç©ºé—´æœç´¢é‚»è¿‘ç‚¹Cloudè¿›è¡Œå‡å°‘å‚æ•°ï¼Œå¹¶é€šè¿‡æ–°çš„å‚æ•°-è‡ªç”±åœ°ç†ç¼–ç å™¨è¿›è¡Œç‚¹Cloudç‰¹å¾ç»¼åˆã€‚</li>
<li>results: ZeroRegæ–¹æ³•åœ¨3DMatchã€3DLoMatchå’ŒScanNetç­‰æ•°æ®é›†ä¸Šå®ç°äº†è¶…è¿‡84%ã€46%å’Œ75%çš„å¬å›ç‡ï¼Œä¸ä¼ ç»Ÿå’Œå­¦ä¹ åŸºäºæ–¹æ³•ç«äº‰ã€‚<details>
<summary>Abstract</summary>
Learning-based point cloud registration approaches have significantly outperformed their traditional counterparts. However, they typically require extensive training on specific datasets. In this paper, we propose , the first zero-shot point cloud registration approach that eliminates the need for training on point cloud datasets. The cornerstone of ZeroReg is the novel transfer of image features from keypoints to the point cloud, enriched by aggregating information from 3D geometric neighborhoods. Specifically, we extract keypoints and features from 2D image pairs using a frozen pretrained 2D backbone. These features are then projected in 3D, and patches are constructed by searching for neighboring points. We integrate the geometric and visual features of each point using our novel parameter-free geometric decoder. Subsequently, the task of determining correspondences between point clouds is formulated as an optimal transport problem. Extensive evaluations of ZeroReg demonstrate its competitive performance against both traditional and learning-based methods. On benchmarks such as 3DMatch, 3DLoMatch, and ScanNet, ZeroReg achieves impressive Recall Ratios (RR) of over 84%, 46%, and 75%, respectively.
</details>
<details>
<summary>æ‘˜è¦</summary>
å­¦ä¹ åŸºäºçš„ç‚¹äº‘æ³¨å†Œæ–¹æ³•å·²ç»æ¯”ä¼ ç»Ÿæ–¹æ³•è¡¨ç°å‡ºäº†æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚ç„¶è€Œï¼Œå®ƒä»¬é€šå¸¸éœ€è¦å¯¹ç‰¹å®šçš„ç‚¹äº‘æ•°æ®è¿›è¡Œå¹¿æ³›çš„è®­ç»ƒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬ä¸€ä¸ªæ— éœ€è®­ç»ƒç‚¹äº‘æ•°æ®çš„é›¶å­¦ä¹ ç‚¹äº‘æ³¨å†Œæ–¹æ³•ã€‚ ZeroReg çš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†å›¾åƒç‰¹å¾ä»å…³é”®ç‚¹ä¼ é€’åˆ°ç‚¹äº‘ï¼Œå¹¶é€šè¿‡åœ¨3Dçš„å‡ ä½•é‚»åŸŸä¸­èšåˆä¿¡æ¯æ¥å¢å¼ºã€‚æˆ‘ä»¬é¦–å…ˆä»å†»ç»“çš„é¢„è®­ç»ƒ2DèƒŒboneä¸­æå–å…³é”®ç‚¹å’Œç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å¾åœ¨3Dä¸­æŠ•å½±ï¼Œå¹¶é€šè¿‡æœç´¢é‚»è¿‘ç‚¹æ„å»ºpatchã€‚æˆ‘ä»¬ç„¶åå°†æ¯ä¸ªç‚¹çš„å‡ ä½•å’Œè§†è§‰ç‰¹å¾é›†æˆæˆ‘ä»¬çš„æ–°çš„å‚æ•°è‡ªç”±å‡ ä½•è§£ç å™¨ã€‚æœ€åï¼Œæˆ‘ä»¬å°†ç‚¹äº‘ä¹‹é—´çš„å¯¹åº”å…³ç³»å®šä¹‰ä¸ºä¼˜åŒ–è¿è¾“é—®é¢˜ã€‚æˆ‘ä»¬å¯¹ ZeroReg è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°ï¼Œå¹¶è¯æ˜å®ƒåœ¨3DMatchã€3DLoMatchå’ŒScanNetç­‰benchmarkä¸Šè¾¾åˆ°äº†è¶…è¿‡84%ã€46%å’Œ75%çš„å¾ recall ratioï¼ˆRRï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Is-Ego-Status-All-You-Need-for-Open-Loop-End-to-End-Autonomous-Driving"><a href="#Is-Ego-Status-All-You-Need-for-Open-Loop-End-to-End-Autonomous-Driving" class="headerlink" title="Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?"></a>Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03031">http://arxiv.org/abs/2312.03031</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nvlabs/bev-planner">https://github.com/nvlabs/bev-planner</a></li>
<li>paper_authors: Zhiqi Li, Zhiding Yu, Shiyi Lan, Jiahan Li, Jan Kautz, Tong Lu, Jose M. Alvarez</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ·±å…¥ç ”ç©¶æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å¼€å‘ï¼Œå°¤å…¶æ˜¯åœ¨å…¨æ ˆå±‚é¢ä¸Šå®ç°è‡ªä¸»é©¾é©¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† nuScenes æ•°æ®é›†ï¼Œå¹¶è¿›è¡Œäº†è¯¦ç»†çš„åˆ†æå’Œæ£€éªŒï¼Œä»¥æ¢è®¨æ›´å¤šçš„ç»†èŠ‚é—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ ego çŠ¶æ€ä¿¡æ¯å¯ä»¥æé«˜è·¯å¾„è§„åˆ’è´¨é‡ï¼Œä½†æ˜¯ nuScenes æ•°æ®é›†çš„é™åˆ¶å¯¼è‡´æ¨¡å‹å€¾å‘äºä»…ä»…åŸºäº ego è½¦é€Ÿåº¦è¿›è¡Œæœªæ¥è·¯å¾„è§„åˆ’ã€‚æ­¤å¤–ï¼Œç°æœ‰çš„æŒ‡æ ‡ä¸èƒ½å…¨é¢è¯„ä¼°è§„åˆ’è´¨é‡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸å‡†ç¡®çš„ç»“è®ºã€‚ä¸ºæ­¤ï¼Œæœ¬ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„æŒ‡æ ‡æ¥è¯„ä¼°è·¯å¾„è§„åˆ’æ˜¯å¦éµå¾ªé“è·¯è§„åˆ™ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç®€å•çš„åŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–äºæ„ŸçŸ¥æ³¨è§£çš„æƒ…å†µä¸‹è¾¾åˆ°ç«äº‰æ€§çš„ç»“æœã€‚<details>
<summary>Abstract</summary>
End-to-end autonomous driving recently emerged as a promising research direction to target autonomy from a full-stack perspective. Along this line, many of the latest works follow an open-loop evaluation setting on nuScenes to study the planning behavior. In this paper, we delve deeper into the problem by conducting thorough analyses and demystifying more devils in the details. We initially observed that the nuScenes dataset, characterized by relatively simple driving scenarios, leads to an under-utilization of perception information in end-to-end models incorporating ego status, such as the ego vehicle's velocity. These models tend to rely predominantly on the ego vehicle's status for future path planning. Beyond the limitations of the dataset, we also note that current metrics do not comprehensively assess the planning quality, leading to potentially biased conclusions drawn from existing benchmarks. To address this issue, we introduce a new metric to evaluate whether the predicted trajectories adhere to the road. We further propose a simple baseline able to achieve competitive results without relying on perception annotations. Given the current limitations on the benchmark and metrics, we suggest the community reassess relevant prevailing research and be cautious whether the continued pursuit of state-of-the-art would yield convincing and universal conclusions. Code and models are available at \url{https://github.com/NVlabs/BEV-Planner}
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Are-Synthetic-Data-Useful-for-Egocentric-Hand-Object-Interaction-Detection-An-Investigation-and-the-HOI-Synth-Domain-Adaptation-Benchmark"><a href="#Are-Synthetic-Data-Useful-for-Egocentric-Hand-Object-Interaction-Detection-An-Investigation-and-the-HOI-Synth-Domain-Adaptation-Benchmark" class="headerlink" title="Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark"></a>Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? An Investigation and the HOI-Synth Domain Adaptation Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02672">http://arxiv.org/abs/2312.02672</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†ä½¿ç”¨åˆæˆæ•°æ®æé«˜ egocentric è§†é‡ä¸­æ‰‹-ç‰©ä½“äº’åŠ¨æ£€æµ‹çš„æ•ˆæœã€‚</li>
<li>methods: æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è‡ªåŠ¨ç”Ÿæˆåˆæˆå›¾åƒçš„ simulate å™¨ï¼Œå¯ä»¥ç”Ÿæˆè‡ªåŠ¨æ ‡æ³¨æ‰‹-ç‰©ä½“æ¥è§¦çŠ¶æ€ã€ bounding box å’Œåƒç´ ç²¾åº¦åˆ†å‰²çš„å›¾åƒã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†é¢†åŸŸé€‚åº”æŠ€æœ¯æ¥æ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨åˆæˆæ•°æ®å’Œé¢†åŸŸé€‚åº”æŠ€æœ¯å¯ä»¥è¾¾åˆ°ä¸ä¼ ç»Ÿå¹²æ”¯æŒæ–¹æ³•ç›¸åŒçš„æ€§èƒ½æ°´å¹³ï¼Œä½†éœ€è¦æ ‡æ³¨çš„æ•°æ®é‡å‡å°‘åˆ°ä¸€åŠã€‚å½“ä½¿ç”¨æ¥è‡ª3Dæ¨¡å‹çš„çœŸå®ç›®æ ‡ç¯å¢ƒå’Œç‰©ä½“çš„åˆæˆæ•°æ®è¿›è¡Œæµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹è¡¨ç°å‡ºäº†ç›¸å¯¹äºæ ‡å‡†å¹²æ”¯æŒæ–¹æ³•çš„æ€§èƒ½æå‡ã€‚æˆ‘ä»¬è¿˜è®¾ç½®äº†ä¸€ä¸ªæ–°çš„é¢†åŸŸé€‚åº”æ ‡å‡†ï¼ˆHOI-Synthï¼‰ï¼Œå¹¶æä¾›äº†åŸºçº¿ç»“æœï¼Œä»¥é¼“åŠ±ç¤¾åŒºè¿›è¡Œè¿™ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚<details>
<summary>Abstract</summary>
In this study, we investigate the effectiveness of synthetic data in enhancing hand-object interaction detection within the egocentric vision domain. We introduce a simulator able to generate synthetic images of hand-object interactions automatically labeled with hand-object contact states, bounding boxes, and pixel-wise segmentation masks. Through comprehensive experiments and comparative analyses on three egocentric datasets, VISOR, EgoHOS, and ENIGMA-51, we demonstrate that the use of synthetic data and domain adaptation techniques allows for comparable performance to conventional supervised methods while requiring annotations on only a fraction of the real data. When tested with in-domain synthetic data generated from 3D models of real target environments and objects, our best models show consistent performance improvements with respect to standard fully supervised approaches based on labeled real data only. Our study also sets a new benchmark of domain adaptation for egocentric hand-object interaction detection (HOI-Synth) and provides baseline results to encourage the community to engage in this challenging task. We release the generated data, code, and the simulator at the following link: https://iplab.dmi.unict.it/HOI-Synth/.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†äººå·¥æ•°æ®çš„æœ‰æ•ˆæ€§ä»¥æé«˜ egocentric è§†è§‰é¢†åŸŸå†…æ‰‹å¯¹ç‰©ä½“äº¤äº’æ£€æµ‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¸¦æœ‰æ‰‹å¯¹ç‰©ä½“æ¥è§¦çŠ¶æ€ã€ bounding box å’Œåƒç´ çº§åˆ† segmentation çš„ synthetic å›¾åƒçš„ simulatorã€‚é€šè¿‡å¯¹ä¸‰ä¸ª egocentric æ•°æ®é›†è¿›è¡Œå…¨é¢çš„å®éªŒå’Œæ¯”è¾ƒåˆ†æï¼Œæˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨ synthetic æ•°æ®å’Œé¢†åŸŸé€‚åº”æŠ€æœ¯å¯ä»¥å®ç°ä¸ä¼ ç»Ÿè¶…çº§è§†å›¾æ–¹æ³•ç›¸åŒçš„æ€§èƒ½ï¼Œåªéœ€è¦å¯¹å®é™…æ•°æ®è¿›è¡Œæ ‡æ³¨çš„ä¸€å°éƒ¨åˆ†ã€‚å½“æµ‹è¯•ä½¿ç”¨äº†åŸºäº 3D æ¨¡å‹ç”Ÿæˆçš„çœŸå®ç›®æ ‡ç¯å¢ƒå’Œç‰©ä½“çš„ synthetic æ•°æ®æ—¶ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹è¡¨ç°å‡ºäº†ä¸æ ‡å‡†å……åˆ†ç›‘ç£æ–¹æ³•ç›¸æ¯”çš„ä¸€è‡´æ€§æå‡ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¿˜è®¾ç½®äº† egocentric æ‰‹å¯¹ç‰©ä½“äº¤äº’æ£€æµ‹ï¼ˆHOI-Synthï¼‰é¢†åŸŸçš„æ–°æ ‡å‡†åŸºå‡†å’Œæä¾›äº†åŸºçº¿ç»“æœï¼Œä»¥ä¾¿ç¤¾åŒºèƒ½å¤Ÿæ›´å¥½åœ°å‚ä¸è¿™ä¸ªæŒ‘æˆ˜æ€§ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ä»¥ä¸‹é“¾æ¥å‘å¸ƒäº†ç”Ÿæˆçš„æ•°æ®ã€ä»£ç å’Œ simulatorï¼šhttps://iplab.dmi.unict.it/HOI-Synth/.
</details></li>
</ul>
<hr>
<h2 id="Generating-Visually-Realistic-Adversarial-Patch"><a href="#Generating-Visually-Realistic-Adversarial-Patch" class="headerlink" title="Generating Visually Realistic Adversarial Patch"></a>Generating Visually Realistic Adversarial Patch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03030">http://arxiv.org/abs/2312.03030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaosen Wang, Kunyu Wang</li>
<li>for: é˜²å¾¡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰å—åˆ°å¤šç§æ”»å‡»ï¼Œå¦‚æ”»å‡»ç¤ºä¾‹ï¼Œä»¥æé«˜å®‰å…¨åº”ç”¨çš„å¯ä¿¡åº¦ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•ï¼Œå³å¯è§†çœŸå®æ”»å‡»ï¼ˆVRAPï¼‰ï¼Œé€šè¿‡åœ¨å®é™…å›¾åƒé™„è¿‘è¿›è¡Œçº¦æŸï¼Œå¹¶åœ¨æœ€å·®ä½ç½®è¿›è¡Œä¼˜åŒ–ï¼Œä»¥ç¡®ä¿æ”»å‡»patchçš„å¯è§æ€§å’Œå°åˆ·æ€§ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVRAPåœ¨æ•°å­—ä¸–ç•Œä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„æ”»å‡»æ€§èƒ½ï¼Œè€Œç”Ÿæˆçš„æ”»å‡»patchå¯ä»¥åœ¨ç‰©ç†ä¸–ç•Œä¸­æ©é¥°æˆæ¶‚é¸¦æˆ–å•†æ ‡ï¼Œè®©æ·±åº¦æ¨¡å‹æ— æ³•è¯†åˆ«ï¼Œå¯¹DNNs-enabledåº”ç”¨é€ æˆäº†é‡å¤§å¨èƒã€‚<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) are vulnerable to various types of adversarial examples, bringing huge threats to security-critical applications. Among these, adversarial patches have drawn increasing attention due to their good applicability to fool DNNs in the physical world. However, existing works often generate patches with meaningless noise or patterns, making it conspicuous to humans. To address this issue, we explore how to generate visually realistic adversarial patches to fool DNNs. Firstly, we analyze that a high-quality adversarial patch should be realistic, position irrelevant, and printable to be deployed in the physical world. Based on this analysis, we propose an effective attack called VRAP, to generate visually realistic adversarial patches. Specifically, VRAP constrains the patch in the neighborhood of a real image to ensure the visual reality, optimizes the patch at the poorest position for position irrelevance, and adopts Total Variance loss as well as gamma transformation to make the generated patch printable without losing information. Empirical evaluations on the ImageNet dataset demonstrate that the proposed VRAP exhibits outstanding attack performance in the digital world. Moreover, the generated adversarial patches can be disguised as the scrawl or logo in the physical world to fool the deep models without being detected, bringing significant threats to DNNs-enabled applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
We analyze that a high-quality adversarial patch should be realistic, position-irrelevant, and printable. Based on this analysis, we propose an effective attack called VRAP (Visually Realistic Adversarial Patch) to generate visually realistic patches. VRAP constrains the patch in the neighborhood of a real image to ensure visual reality, optimizes the patch at the poorest position for position irrelevance, and adopts Total Variance loss as well as gamma transformation to make the generated patch printable without losing information.Empirical evaluations on the ImageNet dataset demonstrate that the proposed VRAP exhibits outstanding attack performance in the digital world. Moreover, the generated adversarial patches can be disguised as scrawls or logos in the physical world to fool deep models without being detected, bringing significant threats to DNNs-enabled applications.
</details></li>
</ul>
<hr>
<h2 id="Gaussian-Head-Avatar-Ultra-High-fidelity-Head-Avatar-via-Dynamic-Gaussians"><a href="#Gaussian-Head-Avatar-Ultra-High-fidelity-Head-Avatar-via-Dynamic-Gaussians" class="headerlink" title="Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians"></a>Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03029">http://arxiv.org/abs/2312.03029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuelangx/gaussian-head-avatar">https://github.com/yuelangx/gaussian-head-avatar</a></li>
<li>paper_authors: Yuelang Xu, Benwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu</li>
<li>for: é«˜ç²¾åº¦3Då¤´åƒåˆ›å»ºæ˜¯ç ”ç©¶çƒ­ç‚¹ï¼Œä½†åœ¨ç¼ºä¹è§†å›¾çš„æƒ…å†µä¸‹å­˜åœ¨å¤§å‹æŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†åŸºäºå¯æ§3D Gaussiançš„ Gaussian Head Avatarï¼Œä»¥å®ç°é«˜ç²¾åº¦å¤´åƒæ¨¡å‹ã€‚</li>
<li>methods: æˆ‘ä»¬ä¼˜åŒ–äº†ä¸­æ€§3D Gaussianå’ŒåŸºäºMLPçš„å¼¯æ›²åœºï¼Œä»¥æ•æ‰å¤æ‚è¡¨æƒ…ã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†ç›¸äº’å¸®åŠ©ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ¨¡æ‹Ÿç»†èŠ‚åŠ¨ä½œçš„åŒæ—¶ä¿æŒè¡¨æƒ…å‡†ç¡®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆç†çš„å‡ ä½•å¯¼å‘åˆå§‹åŒ–ç­–ç•¥ï¼ŒåŸºäºå‡ ä½•SDFå’Œæ·±åº¦è¿­ä»£å››é¢ä½“ï¼Œä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œå½’ä¸€åŒ–ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨ç¼ºä¹è§†å›¾æƒ…å†µä¸‹æ¯”å…¶ä»–çŠ¶æ€ä½“ç³»æ–¹æ³•è¡¨ç°æ›´é«˜ï¼Œå®ç°2Kåˆ†è¾¨ç‡çš„æé«˜ç²¾åº¦æ¸²æŸ“è´¨é‡ï¼Œå³ä½¿é¢å¯¹æç«¯è¡¨æƒ…ã€‚<details>
<summary>Abstract</summary>
Creating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.
</details>
<details>
<summary>æ‘˜è¦</summary>
åˆ›é€ é«˜ç²¾åº¦3Då¤´åƒä¸€ç›´æ˜¯ç ”ç©¶çƒ­ç‚¹ï¼Œä½†åœ¨å…‰é‡ç¨€ç¼ºçš„è§†ç‚¹è®¾ç½®ä¸‹å­˜åœ¨å¤§çš„æŒ‘æˆ˜ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åŸºäºå¯æ§3Dé«˜æ–¯å‡½æ•°çš„ Gaussian Head Avatarï¼Œç”¨äºé«˜ç²¾åº¦å¤´åƒæ¨¡å‹åŒ–ã€‚æˆ‘ä»¬ä¼˜åŒ–äº†ä¸­æ€§3Dé«˜æ–¯å‡½æ•°å’Œå®Œå…¨å­¦ä¹ çš„MLPåŸºäºå¡‘å½¢åœºï¼Œä»¥æ•æ‰å¤æ‚çš„è¡¨æƒ…ã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†äº’åŠ© each otherï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ¨¡æ‹Ÿç»†è…»çš„åŠ¨æ€ç»†èŠ‚ï¼ŒåŒæ—¶ä¿è¯è¡¨æƒ…å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åŸºäºå°è±¡å‡½æ•°å’Œæ·±åº¦è¿ˆå…‹å®šé‡çš„åˆå§‹åŒ–ç­–ç•¥ï¼Œä»¥ä¿è¯è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§ã€‚å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ sparse-view æ–¹é¢è¶…è¶Šäº†å…¶ä»–ç°æœ‰çš„æ–¹æ³•ï¼Œå®ç°äº†2Kè§£åº¦çš„æé«˜ç²¾åº¦æ¸²æŸ“è´¨é‡ï¼Œç”šè‡³åœ¨å¤¸å¤§çš„è¡¨æƒ…ä¸‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Double-Integral-Enhanced-Zeroing-Neural-Network-Optimized-with-ALSOA-fostered-Lung-Cancer-Classification-using-CT-Images"><a href="#Double-Integral-Enhanced-Zeroing-Neural-Network-Optimized-with-ALSOA-fostered-Lung-Cancer-Classification-using-CT-Images" class="headerlink" title="Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA fostered Lung Cancer Classification using CT Images"></a>Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA fostered Lung Cancer Classification using CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03028">http://arxiv.org/abs/2312.03028</a></li>
<li>repo_url: None</li>
<li>paper_authors: V S Priya Sumitha, V. Keerthika, A. Geetha</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ™ºèƒ½è‡ªåŠ¨æŠ‘åˆ¶è‚ºç™Œåˆ†ç±»æ–¹æ³•ï¼Œä»¥æé«˜è‚ºç™Œæ£€æµ‹çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†åŒ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°ãƒ«å¢å¼ºé›¶åŒ–ç¥ç»ç½‘ç»œä¼˜åŒ–çš„ALSOAè‚ºç™Œåˆ†ç±»æ–¹æ³•ï¼Œå¹¶åœ¨CTå›¾åƒä¸Šè¿›è¡Œäº†é¢„å¤„ç†å’Œç‰¹å¾æå–ã€‚</li>
<li>results: å¯¹æ¯”existæ–¹æ³•ï¼Œæœ¬ç ”ç©¶çš„æ–¹æ³•å®ç°äº†18.32%ã€27.20%å’Œ34.32%çš„é«˜åº¦å‡†ç¡®ç‡æå‡ã€‚<details>
<summary>Abstract</summary>
Lung cancer is one of the deadliest diseases and the leading cause of illness and death. Since lung cancer cannot predicted at premature stage, it able to only be discovered more broadly once it has spread to other lung parts. The risk grows when radiologists and other specialists determine whether lung cancer is current. Owing to significance of determining type of treatment and its depth based on severity of the illness, critical to develop smart and automatic cancer prediction scheme is precise, at which stage of cancer. In this paper, Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA fostered Lung Cancer Classification using CT Images (LCC-DIEZNN-ALSO-CTI) is proposed. Initially, input CT image is amassed from lung cancer dataset. The input CT image is pre-processing via Unscented Trainable Kalman Filtering (UTKF) technique. In pre-processing stage unwanted noise are removed from CT images. Afterwards, grayscale statistic features and Haralick texture features extracted by Adaptive and Concise Empirical Wavelet Transform (ACEWT). The proposed model is implemented on MATLAB. The performance of the proposed method is analyzed through existing techniques. The proposed method attains 18.32%, 27.20%, and 34.32% higher accuracy analyzed with existing method likes Deep Learning Assisted Predict of Lung Cancer on Computed Tomography Images Utilizing AHHMM (LCC-AHHMM-CT), Convolutional neural networks based pulmonary nodule malignancy assessment in pipeline for classifying lung cancer (LCC-ICNN-CT), Automated Decision Support Scheme for Lung Cancer Identification with Categorization (LCC-RFCN-MLRPN-CT) methods respectively.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‚ºç™Œæ˜¯ä¸€ç§éå¸¸è‡´å‘½çš„ç–¾ç—…ï¼Œä¹Ÿæ˜¯è‡´æ­»ç‡æœ€é«˜çš„ç–¾ç—…ä¹‹ä¸€ã€‚ç”±äºè‚ºç™Œåœ¨æ—©æœŸéš¾ä»¥é¢„æµ‹ï¼Œå› æ­¤é€šå¸¸åªèƒ½åœ¨å…¶ä»–è‚ºéƒ¨å·²ç»æ‰©æ•£ä¹‹åæ‰èƒ½å‘ç°ã€‚éšç€ç–¾ç—…çš„ä¸¥é‡ç¨‹åº¦å¢åŠ ï¼Œæ£€æµ‹è‚ºç™Œçš„é‡è¦æ€§ä¹Ÿåœ¨æé«˜ã€‚ä¸ºäº†å¼€å‘ä¸€ç§æ™ºèƒ½å’Œè‡ªåŠ¨çš„è‚ºç™Œé¢„æµ‹æ–¹æ¡ˆï¼Œåœ¨ä¸åŒçš„é˜¶æ®µå¯¹è‚ºç™Œè¿›è¡Œç²¾å‡†çš„é¢„æµ‹æ˜¯éå¸¸é‡è¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºCTå›¾åƒçš„è‚ºç™Œåˆ†ç±»æ–¹æ³•ï¼Œå³Double Integral Enhanced Zeroing Neural Network Optimized with ALSOA fostered Lung Cancer Classification using CT Images (LCC-DIEZNN-ALSO-CTI)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»è‚ºç™Œæ•°æ®é›†ä¸­æ”¶é›†äº†è¾“å…¥CTå›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Unscented Trainable Kalman Filtering (UTKF)æŠ€æœ¯è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ç§»é™¤CTå›¾åƒä¸­çš„å™ªå£°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬ä½¿ç”¨Adaptive and Concise Empirical Wavelet Transform (ACEWT)æå–äº†ç°åº¦ç»Ÿè®¡ç‰¹å¾å’ŒHaralick Ñ‚ĞµĞºÑÑ‚ÑƒÑ€ç‰¹å¾ã€‚æˆ‘ä»¬åœ¨MATLABä¸­å®ç°äº†è¯¥æ–¹æ³•ã€‚æˆ‘ä»¬å¯¹è¯¥æ–¹æ³•è¿›è¡Œäº†åˆ†æï¼Œå¹¶ä¸ç°æœ‰çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼ŒåŒ…æ‹¬Deep Learning Assisted Predict of Lung Cancer on Computed Tomography Images Utilizing AHHMM (LCC-AHHMM-CT)ã€Convolutional neural networks based pulmonary nodule malignancy assessment in pipeline for classifying lung cancer (LCC-ICNN-CT)å’ŒAutomated Decision Support Scheme for Lung Cancer Identification with Categorization (LCC-RFCN-MLRPN-CT)ç­‰æ–¹æ³•ã€‚ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨è‚ºç™Œé¢„æµ‹ä¸­è¾¾åˆ°äº†18.32%ã€27.20%å’Œ34.32%çš„é«˜ç²¾åº¦ã€‚
</details></li>
</ul>
<hr>
<h2 id="TPA3D-Triplane-Attention-for-Fast-Text-to-3D-Generation"><a href="#TPA3D-Triplane-Attention-for-Fast-Text-to-3D-Generation" class="headerlink" title="TPA3D: Triplane Attention for Fast Text-to-3D Generation"></a>TPA3D: Triplane Attention for Fast Text-to-3D Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02647">http://arxiv.org/abs/2312.02647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong-En Chen, Bin-Shih Wu, Sheng-Yu Huang, Yu-Chiang Frank Wang</li>
<li>For: æ–‡æœ¬è‡³3Dç”Ÿæˆ* Methods: ä½¿ç”¨GANåŸºæœ¬æ¨¡å‹å’Œæ³¨æ„åŠ›æœºåˆ¶å¯¹æ–‡æœ¬è¿›è¡Œç”Ÿæˆ* Results: ç”Ÿæˆé«˜è´¨é‡çš„3Dçº¹ç†å½¢çŠ¶ï¼Œä¸æ–‡æœ¬æè¿°é«˜åº¦å»åˆï¼Œè®¡ç®—æ•ˆç‡é«˜<details>
<summary>Abstract</summary>
Due to the lack of large-scale text-3D correspondence data, recent text-to-3D generation works mainly rely on utilizing 2D diffusion models for synthesizing 3D data. Since diffusion-based methods typically require significant optimization time for both training and inference, the use of GAN-based models would still be desirable for fast 3D generation. In this work, we propose Triplane Attention for text-guided 3D generation (TPA3D), an end-to-end trainable GAN-based deep learning model for fast text-to-3D generation. With only 3D shape data and their rendered 2D images observed during training, our TPA3D is designed to retrieve detailed visual descriptions for synthesizing the corresponding 3D mesh data. This is achieved by the proposed attention mechanisms on the extracted sentence and word-level text features. In our experiments, we show that TPA3D generates high-quality 3D textured shapes aligned with fine-grained descriptions, while impressive computation efficiency can be observed.
</details>
<details>
<summary>æ‘˜è¦</summary>
(Simplified Chinese translation)ç”±äºç¼ºä¹å¤§è§„æ¨¡æ–‡æœ¬-3Då¯¹åº”æ•°æ®ï¼Œç°ä»£æ–‡æœ¬-3Dç”Ÿæˆå·¥ä½œä¸»è¦ä¾é åˆ©ç”¨2Dæ‰©æ•£æ¨¡å‹ä¸ºç”Ÿæˆ3Dæ•°æ®ã€‚ç”±äºæ‰©æ•£æ¨¡å‹é€šå¸¸éœ€è¦è¾ƒå¤§çš„ä¼˜åŒ–æ—¶é—´ Ğ´Ğ»Ñè®­ç»ƒå’Œæ¨ç†ï¼Œå› æ­¤ä½¿ç”¨GANåŸºäºæ¨¡å‹ä»ç„¶æ˜¯å¯é çš„é€‰æ‹© Ğ´Ğ»Ñå¿«é€Ÿ3Dç”Ÿæˆã€‚åœ¨è¿™ç§å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Triplane Attention for text-guided 3D generationï¼ˆTPA3Dï¼‰ï¼Œä¸€ç§å¯è®­ç»ƒçš„GANåŸºäºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç”¨äºå¿«é€Ÿæ–‡æœ¬å¯¼å‘3Dç”Ÿæˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦æä¾›3Då½¢çŠ¶æ•°æ®å’Œå…¶æ¸²æŸ“åçš„2Då›¾åƒï¼ŒTPA3Dä¼šä½¿ç”¨æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶æ¥æ£€ç´¢æ–‡æœ¬ä¸­çš„è¯¦ç»†è§†è§‰æè¿°ï¼Œå¹¶ç”ŸæˆåŒ¹é…çš„3Dç½‘æ ¼æ•°æ®ã€‚åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å‘ç°TPA3Då¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„3Dçº¹ç†å½¢çŠ¶ï¼Œä¸ç»†åŒ–çš„æ–‡æœ¬æè¿°é«˜åº¦å»åˆï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡éå¸¸é«˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="Synchronization-is-All-You-Need-Exocentric-to-Egocentric-Transfer-for-Temporal-Action-Segmentation-with-Unlabeled-Synchronized-Video-Pairs"><a href="#Synchronization-is-All-You-Need-Exocentric-to-Egocentric-Transfer-for-Temporal-Action-Segmentation-with-Unlabeled-Synchronized-Video-Pairs" class="headerlink" title="Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs"></a>Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02638">http://arxiv.org/abs/2312.02638</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fpv-iplab/synchronization-is-all-you-need">https://github.com/fpv-iplab/synchronization-is-all-you-need</a></li>
<li>paper_authors: Camillo Quattrocchi, Antonino Furnari, Daniele Di Mauro, Mario Valerio Giuffrida, Giovanni Maria Farinella</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯å°†æƒ¯æ€§çš„è¡Œä¸ºåˆ†ç±»ç³»ç»Ÿä»å¤–éƒ¨ç›¸æœºæ‰©å±•åˆ°è‡ªæˆ‘ç›¸æœºçš„ egocentric enarioï¼Œå¹¶ä¸”ä¸éœ€è¦æ–°çš„ egocentric è§†é¢‘æ ‡ç­¾ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨ç°æœ‰çš„æ ‡ç­¾çš„å¤–éƒ¨ç›¸æœºè§†é¢‘å’Œä¸€äº›æ–°çš„ã€åŒæ­¥çš„å¤–éƒ¨-è‡ªæˆ‘ç›¸æœº Ğ²Ğ¸Ğ´ĞµĞ¾å¯¹ï¼Œæ¥è¿›è¡Œé€‚åº”ã€‚è¿™ä¸ªæ–¹æ³•åŸºäºçŸ¥è¯†ä¼ æ’­ï¼Œå¹¶åœ¨ç‰¹å¾å’Œæ¨¡å‹å±‚é¢è¿›è¡Œäº†å®ç°ã€‚</li>
<li>results: results æ˜¾ç¤ºï¼Œè¿™ä¸ªæ–¹æ³•å¯ä»¥å®ç°å¯¹äº egocentric è§†é¢‘çš„é€‚åº”ï¼Œå¹¶ä¸”å¯ä»¥å’Œä¼ ç»Ÿçš„ä¸ç›‘ç£é¢†åŸŸé€‚åº”æ–¹æ³•ç›¸æ¯”ï¼Œè·å¾—æ›´å¥½çš„æ•ˆæœã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸ªæ–¹æ³•ä¸éœ€è¦ä»»ä½• egocentric æ ‡ç­¾ï¼Œä»…é ä½¿ç”¨ç°æœ‰çš„æ ‡ç­¾å’Œè‡ªåŠ¨ç”Ÿæˆçš„å¯¹ï¼Œå¯ä»¥å®ç°å’Œä¼ ç»Ÿç›‘ç£å­¦ä¹ æ–¹æ³•ç›¸æ¯”çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We consider the problem of transferring a temporal action segmentation system initially designed for exocentric (fixed) cameras to an egocentric scenario, where wearable cameras capture video data. The conventional supervised approach requires the collection and labeling of a new set of egocentric videos to adapt the model, which is costly and time-consuming. Instead, we propose a novel methodology which performs the adaptation leveraging existing labeled exocentric videos and a new set of unlabeled, synchronized exocentric-egocentric video pairs, for which temporal action segmentation annotations do not need to be collected. We implement the proposed methodology with an approach based on knowledge distillation, which we investigate both at the feature and model level. To evaluate our approach, we introduce a new benchmark based on the Assembly101 dataset. Results demonstrate the feasibility and effectiveness of the proposed method against classic unsupervised domain adaptation and temporal sequence alignment approaches. Remarkably, without bells and whistles, our best model performs on par with supervised approaches trained on labeled egocentric data, without ever seeing a single egocentric label, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on the Assembly101 dataset compared to a baseline model trained solely on exocentric data.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è€ƒè™‘å°†ç”¨äºå¤–éƒ¨ï¼ˆå›ºå®šï¼‰æ‘„åƒå¤´çš„æ—¶é—´åŠ¨ä½œåˆ†å‰²ç³»ç»Ÿä¼ æ’­åˆ°è‡ªæˆ‘ä¸­å¿ƒï¼ˆç©¿æˆ´å¼æ‘„åƒå¤´ï¼‰åœºæ™¯ä¸­ï¼Œä»¥ä¾¿ä½¿ç”¨ä¾¿æºå¼æ‘„åƒå¤´æ•æ‰è§†é¢‘æ•°æ®ã€‚ä¼ ç»Ÿçš„ç›‘ç£æ–¹æ³•éœ€è¦æ”¶é›†å’Œæ ‡æ³¨ä¸€ç»„æ–°çš„ egocentric è§†é¢‘ï¼Œè¿™æ˜¯è´µé‡å’Œè€—æ—¶çš„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨ç°æœ‰çš„ exocentric è§†é¢‘å’Œä¸€ç»„æ–°çš„åŒæ­¥ exocentric-egocentric è§†é¢‘å¯¹ä¸­è¿›è¡Œé€‚åº”ï¼Œæ— éœ€æ”¶é›† temporal action segmentation æ ‡æ³¨ã€‚æˆ‘ä»¬å®ç°äº†è¯¥æ–¹æ³•ä½¿ç”¨çŸ¥è¯†å‚¨å­˜æŠ€æœ¯ï¼Œå¹¶åœ¨ç‰¹å¾å’Œæ¨¡å‹å±‚è¿›è¡Œäº†è°ƒæ•´ã€‚ä¸ºè¯„ä¼°æˆ‘ä»¬çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŸºäº Assembly101 æ•°æ®é›†çš„æ–°çš„æ ‡å‡†å‡†æ¯”è¾ƒã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡å°‘ç›‘ç£å­¦ä¹ çš„æˆæœ¬å’Œæ—¶é—´ï¼Œå¹¶ä¸”èƒ½å¤Ÿè¾¾åˆ°ä¸ç›‘ç£å­¦ä¹  trained on labeled egocentric data ç›¸å½“çš„æ€§èƒ½ï¼Œè€Œæ— éœ€çœ‹åˆ°ä¸€ä¸ªå•ä¸€çš„ egocentric æ ‡æ³¨ï¼Œåœ¨ Assembly101 æ•°æ®é›†ä¸Šæ¯”åŸºelineæ¨¡å‹æé«˜äº† +15.99%ï¼ˆ28.59% vs 12.60%)ã€‚
</details></li>
</ul>
<hr>
<h2 id="Stable-Diffusion-Exposed-Gender-Bias-from-Prompt-to-Image"><a href="#Stable-Diffusion-Exposed-Gender-Bias-from-Prompt-to-Image" class="headerlink" title="Stable Diffusion Exposed: Gender Bias from Prompt to Image"></a>Stable Diffusion Exposed: Gender Bias from Prompt to Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03027">http://arxiv.org/abs/2312.03027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yankun Wu, Yuta Nakashima, Noa Garcia</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ£€æŸ¥ç¨³å®šæ‰©æ•£å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ€§åˆ«åè§ï¼Œä»¥åŠè¿™äº›åè§å¦‚ä½•å½±å“å›¾åƒçš„ç”Ÿæˆã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨è‡ªåŠ¨åŒ–è¯„ä¼°åè®®æ¥åˆ†æç¨³å®šæ‰©æ•£å›¾åƒä¸­çš„æ€§åˆ«æŒ‡æ ‡çš„å½±å“ã€‚åŸºäºä¹‹å‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¢è®¨äº†æ€§åˆ«æŒ‡æ ‡å¦‚ä½•å½±å“å›¾åƒä¸­çš„å¯¹è±¡å’Œå¸ƒå±€çš„è¡¨ç°ã€‚</li>
<li>results: æˆ‘ä»¬å‘ç°å›¾åƒä¸­çš„å¯¹è±¡ display å­˜åœ¨æ€§åˆ«åè§ï¼Œä¾‹å¦‚â™‚â™‚å’Œâ™€â™€çš„ instrumente ä¸åŒï¼Œè€Œä¸”å›¾åƒçš„æ€»å¸ƒå±€ä¹Ÿå­˜åœ¨å·®å¼‚ã€‚æ­¤å¤–ï¼Œä¸­æ€§æç¤ºæ¯”â™‚æç¤ºæ›´åŠ å€¾å‘äºç”Ÿæˆâ™‚ç±»å‹çš„å›¾åƒï¼Œè¿™åæ˜ äº†ç¨³å®šæ‰©æ•£å›¾åƒç”Ÿæˆæ¨¡å‹ä¸­çš„æ€§åˆ«åè§ã€‚<details>
<summary>Abstract</summary>
Recent studies have highlighted biases in generative models, shedding light on their predisposition towards gender-based stereotypes and imbalances. This paper contributes to this growing body of research by introducing an evaluation protocol designed to automatically analyze the impact of gender indicators on Stable Diffusion images. Leveraging insights from prior work, we explore how gender indicators not only affect gender presentation but also the representation of objects and layouts within the generated images. Our findings include the existence of differences in the depiction of objects, such as instruments tailored for specific genders, and shifts in overall layouts. We also reveal that neutral prompts tend to produce images more aligned with masculine prompts than their feminine counterparts, providing valuable insights into the nuanced gender biases inherent in Stable Diffusion.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿‘æœŸç ”ç©¶å¼ºè°ƒç”Ÿæˆæ¨¡å‹ä¸­çš„åè§ï¼ŒæŠ›ç…§åˆ°å®ƒä»¬å…·æœ‰æ€§åˆ«åè§å’Œä¸å‡è¡¡çš„å€¾å‘ã€‚æœ¬æ–‡å¯¹è¿™äº›ç ”ç©¶è¿›è¡Œè´¡çŒ®ï¼Œé€šè¿‡è‡ªåŠ¨åˆ†æStable Diffusionå›¾åƒä¸­çš„æ€§åˆ«æŒ‡æ ‡å½±å“çš„è¯„ä»·åè®®ã€‚åŸºäºå…ˆå‰çš„ç ”ç©¶ï¼Œæˆ‘ä»¬æ¢ç´¢äº†æ€§åˆ«æŒ‡æ ‡ä¸ä»…å½±å“ gender presentationï¼Œè¿˜å½±å“å›¾åƒä¸­çš„ Ğ¾Ğ±ÑŠĞµĞºå’Œå¸ƒå±€ã€‚æˆ‘ä»¬çš„å‘ç°åŒ…æ‹¬å¯¹è±¡çš„ä¸åŒè¡¨ç°ï¼Œå¦‚é€‚ç”¨äºç‰¹å®šæ€§åˆ«çš„å·¥å…·ï¼Œä»¥åŠå›¾åƒä¸­çš„æ€»å¸ƒå±€çš„å˜åŒ–ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œä¸­æ€§æç¤ºé€šå¸¸ä¼šç”Ÿæˆæ›´åŠ  masculine çš„å›¾åƒï¼Œè¿™æä¾›äº†å…³äºStable Diffusionä¸­çš„æ€§åˆ«åè§çš„æœ‰ä»·å€¼ä¿¡æ¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Noise-Feature-Accurate-and-Fast-Generated-Image-Detection"><a href="#Diffusion-Noise-Feature-Accurate-and-Fast-Generated-Image-Detection" class="headerlink" title="Diffusion Noise Feature: Accurate and Fast Generated Image Detection"></a>Diffusion Noise Feature: Accurate and Fast Generated Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02625">http://arxiv.org/abs/2312.02625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yichi Zhang, Xiaogang Xu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜ç”Ÿæˆå›¾åƒæ£€æµ‹ç²¾åº¦å’Œæ™®é€‚æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨åå°„æ‰©æ•£æ¨¡å‹è¿›è¡Œå€’æ•£å¤„ç†ï¼Œå¹¶åˆ©ç”¨ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒåœ¨æ‰©æ•£è¿‡ç¨‹ä¸­çš„å·®å¼‚æ¥å¢å¼ºç”Ÿæˆå›¾åƒçš„æ£€æµ‹ã€‚</li>
<li>results: æœ¬ç ”ç©¶å®ç°äº†é«˜ç²¾åº¦ã€ç¨³å®šæ€§å’Œæ™®é€‚æ€§çš„ç”Ÿæˆå›¾åƒæ£€æµ‹æ–¹æ³•ï¼Œå¹¶åœ¨æ ‡å‡† dataset ä¸Šè¾¾åˆ°äº†å›½é™…é¡¶å³°æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Generative models have reached an advanced stage where they can produce remarkably realistic images. However, this remarkable generative capability also introduces the risk of disseminating false or misleading information. Notably, existing image detectors for generated images encounter challenges such as low accuracy and limited generalization. This paper seeks to address this issue by seeking a representation with strong generalization capabilities to enhance the detection of generated images. Our investigation has revealed that real and generated images display distinct latent Gaussian representations when subjected to an inverse diffusion process within a pre-trained diffusion model. Exploiting this disparity, we can amplify subtle artifacts in generated images. Building upon this insight, we introduce a novel image representation known as Diffusion Noise Feature (DNF). DNF is an ensemble representation that estimates the noise generated during the inverse diffusion process. A simple classifier, e.g., ResNet, trained on DNF achieves high accuracy, robustness, and generalization capabilities for detecting generated images, even from previously unseen classes or models. We conducted experiments using a widely recognized and standard dataset, achieving state-of-the-art effects of Detection.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>ä¼ é€ç»™å®šæ–‡æœ¬åˆ°ç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>ç”Ÿæˆæ¨¡å‹å·²ç»è¾¾åˆ°äº†é«˜åº¦çš„è¿›æ­¥ï¼Œèƒ½å¤Ÿç”Ÿæˆæå…¶çœŸå®çš„å›¾åƒã€‚ç„¶è€Œï¼Œè¿™ç§æé«˜çš„ç”Ÿæˆèƒ½åŠ›ä¹Ÿä¼šå¯¼è‡´è¯¯å·®æˆ–è¯¯å¯¼ä¿¡æ¯çš„æ•£å¸ƒã€‚ç‰¹åˆ«æ˜¯ç°æœ‰çš„ç”Ÿæˆå›¾åƒæ£€æµ‹å™¨é‡åˆ°äº†ä½å‡†ç¡®ç‡å’Œæœ‰é™çš„æ³›åŒ–é—®é¢˜ã€‚è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡æé«˜ç”Ÿæˆå›¾åƒæ£€æµ‹çš„å‡†ç¡®ç‡æ¥å¢å¼ºæ£€æµ‹ç”Ÿæˆå›¾åƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å‘ç°ï¼Œå®é™…å›¾åƒå’Œç”Ÿæˆå›¾åƒåœ¨é€†Diffusionè¿‡ç¨‹ä¸­çš„çº¦æŸè¡¨ç¤ºä¸­å­˜åœ¨æ˜¾è‘—çš„å·®å¼‚ã€‚åˆ©ç”¨è¿™ä¸ªå·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥å¼ºåˆ¶åŠ å¤§ç”Ÿæˆå›¾åƒä¸­çš„å¾®å°æ®‹ç•™ã€‚åŸºäºè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„å›¾åƒè¡¨ç¤ºæ–¹å¼ï¼Œç§°ä¸ºDiffusion Noise Featureï¼ˆDNFï¼‰ã€‚DNFæ˜¯ä¸€ä¸ªensembleè¡¨ç¤ºæ–¹å¼ï¼Œç”¨äºä¼°è®¡é€†Diffusionè¿‡ç¨‹ä¸­ç”Ÿæˆçš„å™ªå£°ã€‚ä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨ï¼Œä¾‹å¦‚ResNetï¼Œåœ¨DNFä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥åœ¨æ£€æµ‹ç”Ÿæˆå›¾åƒæ–¹é¢è¾¾åˆ°é«˜åº¦çš„å‡†ç¡®ç‡ã€Robustnesså’Œæ³›åŒ–èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå¹¿æ³› Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ğ½å’Œæ ‡å‡†çš„ dataset ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå®ç°äº†çŠ¶æ€çš„æ£€æµ‹æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="DreaMo-Articulated-3D-Reconstruction-From-A-Single-Casual-Video"><a href="#DreaMo-Articulated-3D-Reconstruction-From-A-Single-Casual-Video" class="headerlink" title="DreaMo: Articulated 3D Reconstruction From A Single Casual Video"></a>DreaMo: Articulated 3D Reconstruction From A Single Casual Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02617">http://arxiv.org/abs/2312.02617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Tu, Ming-Feng Li, Chieh Hubert Lin, Yen-Chi Cheng, Min Sun, Ming-Hsuan Yang</li>
<li>for: ç”¨äºä»å•ä¸€å’Œå¶æ captured çš„äº’è”ç½‘è§†é¢‘ä¸­è¿›è¡Œæ“…é•¿3Då½¢æ€é‡å»ºï¼Œå¹¶è§£å†³ä½è¦†ç›–åŒºåŸŸçš„æŒ‘æˆ˜ã€‚</li>
<li>methods: æè®®äº†ä¸€ç§åä¸º DreaMo çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŒæ—¶è¿›è¡Œå½¢æ€é‡å»ºå¹¶è§£å†³ä½è¦†ç›–åŒºåŸŸï¼Œä½¿ç”¨è§†å›¾æ¡ä»¶çš„æ‰©æ•£ä¼˜åŒ–å’Œä¸€äº›é€‚åº”æ€§è§„åˆ™ã€‚</li>
<li>results: åœ¨ä¸€ä¸ªè‡ªæ”¶é›†çš„äº’è”ç½‘è§†é¢‘æ”¶é›†ä¸­è¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶å–å¾—äº†å¯è§‚çš„è´¨é‡åœ¨æ–°è§†å›¾æ¸²æŸ“ã€ç»†åŒ–ä¸‰ç»´å½¢æ€é‡å»ºå’Œäºº Ğ¸Ğ½Ñ‚ĞµÑ€æ›¿skeletonç”Ÿæˆæ–¹é¢ã€‚å¯¹äºç°æœ‰æ–¹æ³•ï¼Œç ”ç©¶å‘ç°å®ƒä»¬æ— æ³•è§£å†³æ­£ç¡®çš„å‡ ä½•å­¦é—®é¢˜ç”±äºè§†å›¾è¦†ç›–ç‡ä¸å¤Ÿã€‚<details>
<summary>Abstract</summary>
Articulated 3D reconstruction has valuable applications in various domains, yet it remains costly and demands intensive work from domain experts. Recent advancements in template-free learning methods show promising results with monocular videos. Nevertheless, these approaches necessitate a comprehensive coverage of all viewpoints of the subject in the input video, thus limiting their applicability to casually captured videos from online sources. In this work, we study articulated 3D shape reconstruction from a single and casually captured internet video, where the subject's view coverage is incomplete. We propose DreaMo that jointly performs shape reconstruction while solving the challenging low-coverage regions with view-conditioned diffusion prior and several tailored regularizations. In addition, we introduce a skeleton generation strategy to create human-interpretable skeletons from the learned neural bones and skinning weights. We conduct our study on a self-collected internet video collection characterized by incomplete view coverage. DreaMo shows promising quality in novel-view rendering, detailed articulated shape reconstruction, and skeleton generation. Extensive qualitative and quantitative studies validate the efficacy of each proposed component, and show existing methods are unable to solve correct geometry due to the incomplete view coverage.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œä¸‰ç»´é‡å»ºæœ‰ç€å®è´µçš„åº”ç”¨åœ¨ä¸åŒé¢†åŸŸï¼Œä½†å®ƒä»ç„¶å…·æœ‰é«˜æˆæœ¬å’Œéœ€è¦ä¸“ä¸šäººå‘˜æŠ•å…¥å¤§é‡æ—¶é—´ã€‚ç°ä»£çš„æ¨¡æ¿æ— æ³•å­¦ä¹ æ–¹æ³•åœ¨å•ä¸€å½±åƒä¸­æ˜¾ç¤ºäº†æœ‰æœ›çš„ç»“æœï¼Œä½†è¿™äº›æ–¹æ³•éœ€è¦æ‰€æœ‰ä¸»é¢˜çš„çœ‹æ³•éƒ½å¿…é¡»è¢«è¦†ç›–ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä»å•ä¸€ä¸”å¶å‘çš„ç½‘ç»œå½±åƒä¸­è¿›è¡Œä¸‰ç»´å½¢çŠ¶é‡å»ºï¼Œå…¶ä¸­ä¸»é¢˜çš„çœ‹æ³•è¦†ç›–ç‡è¾ƒä½ã€‚æˆ‘ä»¬æå‡ºäº†DreaMoï¼Œå®ƒåŒæ—¶è¿›è¡Œå½¢çŠ¶é‡å»ºå’Œè§£å†³ä½è¦†ç›–åŒºåŸŸçš„æŒ‘æˆ˜ï¼Œä½¿ç”¨äº†çœ‹æ¿æ¡ä»¶çš„æ‰©æ•£ä¼˜åŒ–å’Œç‰¹æ®Šè°ƒæ•´ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¼å…¥äº†äººç±»å¯ç†è§£çš„éª¨æ¶ç”Ÿæˆç­–ç•¥ï¼Œå°†å­¦ä¹ çš„ç¥ç»éª¨å’Œçš®è‚¤é®ç›¾è½¬æ¢ä¸ºäººç±»å¯è¯»çš„éª¨æ¶ã€‚æˆ‘ä»¬å¯¹è‡ªå·±æ”¶é›†çš„ç½‘ç»œå½±åƒåº“è¿›è¡Œäº†ç ”ç©¶ï¼Œå…¶ä¸­ä¸»é¢˜çš„çœ‹æ³•è¦†ç›–ç‡è¾ƒä½ã€‚DreaMoè¡¨ç°å‡ºäº†ä¼˜ç§€çš„è´¨é‡ï¼ŒåŒ…æ‹¬æ–°è§†é‡æ˜¾ç¤ºã€ç»†éƒ¨ä¸‰ç»´å½¢çŠ¶é‡å»ºå’Œéª¨æ¶ç”Ÿæˆã€‚å¹¿æ³›çš„è´¨é‡å’Œè´¨æ•°ç ”ç©¶è¯æ˜äº†æ¯ä¸ªææ¡ˆçš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜äº†ç°æœ‰æ–¹æ³•æ— æ³•æ­£ç¡®åœ°é‡å»º geometry å› ä¸ºçœ‹æ³•è¦†ç›–ç‡ä¸å¤Ÿé«˜ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Facilitating-the-Production-of-Well-tailored-Video-Summaries-for-Sharing-on-Social-Media"><a href="#Facilitating-the-Production-of-Well-tailored-Video-Summaries-for-Sharing-on-Social-Media" class="headerlink" title="Facilitating the Production of Well-tailored Video Summaries for Sharing on Social Media"></a>Facilitating the Production of Well-tailored Video Summaries for Sharing on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02616">http://arxiv.org/abs/2312.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evlampios Apostolidis, Konstantinos Apostolidis, Vasileios Mezaris</li>
<li>for: è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€ç§åŸºäºWebçš„è§†é¢‘æ‘˜è¦ç”Ÿæˆå·¥å…·ï¼Œç”¨äºåœ¨ç¤¾äº¤åª’ä½“ä¸Šåˆ†äº«æ‘˜è¦ã€‚</li>
<li>methods: è¯¥å·¥å…·ä½¿ç”¨äº† integrate AIæ¨¡å‹è¿›è¡Œè§†é¢‘æ‘˜è¦å’Œæ¯”ä¾‹è½¬æ¢ï¼Œæ”¯æŒä¸€é”®æ‘˜è¦è¿‡ç¨‹ï¼Œå¯ä»¥æ ¹æ®ç›®æ ‡å¹³å°çš„è§†é¢‘é•¿åº¦å’Œæ¯”ä¾‹ç”Ÿæˆå¤šä¸ªæ‘˜è¦ã€‚</li>
<li>results: è¯¥å·¥å…·å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„æ‘˜è¦ï¼Œå¹¶ä¸”å¯ä»¥æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚è¿›è¡Œè‡ªå®šä¹‰ã€‚<details>
<summary>Abstract</summary>
This paper presents a web-based tool that facilitates the production of tailored summaries for online sharing on social media. Through an interactive user interface, it supports a ``one-click'' video summarization process. Based on the integrated AI models for video summarization and aspect ratio transformation, it facilitates the generation of multiple summaries of a full-length video according to the needs of target platforms with regard to the video's length and aspect ratio.
</details>
<details>
<summary>æ‘˜è¦</summary>
Here's the text in Simplified Chinese:è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºç½‘é¡µçš„å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©åˆ›å»ºé€‚åˆç¤¾äº¤åª’ä½“ä¸Šåˆ†äº«çš„ä¸ªæ€§åŒ–æ‘˜è¦ã€‚è¯¥å·¥å…·å…·æœ‰ä¸€ä¸ªäº¤äº’å¼ç”¨æˆ·ç•Œé¢ï¼Œå¯ä»¥é€šè¿‡ä¸€é”®å¿«é€Ÿæ‘˜è¦è§†é¢‘ã€‚é€šè¿‡ Ğ¸Ğ½Ñ‚ĞµGRATED AIæ¨¡å‹ï¼Œè¯¥å·¥å…·å¯ä»¥æ ¹æ®ç›®æ ‡å¹³å°çš„è§†é¢‘é•¿åº¦å’Œæ¯”ä¾‹è½¬æ¢ç”Ÿæˆå¤šä¸ªæ‘˜è¦è§†é¢‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="Projection-Regret-Reducing-Background-Bias-for-Novelty-Detection-via-Diffusion-Models"><a href="#Projection-Regret-Reducing-Background-Bias-for-Novelty-Detection-via-Diffusion-Models" class="headerlink" title="Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models"></a>Projection Regret: Reducing Background Bias for Novelty Detection via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02615">http://arxiv.org/abs/2312.02615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungik Choi, Hankook Lee, Honglak Lee, Moontae Lee</li>
<li>For: The paper is written for detecting abnormal (out-of-distribution) samples in diffusion models, which have recently gained attention in machine learning.* Methods: The paper proposes a novelty detection method called Projection Regret (PR), which mitigates the bias of non-semantic information by computing the perceptual distance between the test image and its diffusion-based projection, and then cancelling out the background bias by comparing it against recursive projections.* Results: The paper shows that PR outperforms prior art of generative-model-based novelty detection methods by a significant margin, demonstrating its effectiveness in detecting abnormal samples.Hereâ€™s the simplified Chinese text for the three key points:* For: æœ¬æ–‡æ˜¯ä¸ºæ¢æµ‹Diffusionæ¨¡å‹ä¸­çš„å¼‚å¸¸ï¼ˆéå¸¸é‡ï¼‰æ ·æœ¬è€Œå†™çš„ã€‚* Methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å¼‚å¸¸æ¢æµ‹æ–¹æ³•called Projection Regret (PR),å®ƒé€šè¿‡è®¡ç®—æµ‹è¯•å›¾åƒä¸å…¶Diffusion-based projectionçš„ç›¸ä¼¼åº¦æ¥æ¢æµ‹å¼‚å¸¸æ€§ï¼Œç„¶åé€šè¿‡å¯¹æ¯”åå¤çš„æŠ•å½±æ¥å‡å°‘èƒŒæ™¯åè§ã€‚* Results: æœ¬æ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPRåœ¨ä¸å…ˆå‰çš„ç”Ÿæˆæ¨¡å‹åŸºäºçš„å¼‚å¸¸æ¢æµ‹æ–¹æ³•ç›¸æ¯”ï¼Œå…·æœ‰æ˜¾è‘—çš„æå‡ã€‚<details>
<summary>Abstract</summary>
Novelty detection is a fundamental task of machine learning which aims to detect abnormal ($\textit{i.e.}$ out-of-distribution (OOD)) samples. Since diffusion models have recently emerged as the de facto standard generative framework with surprising generation results, novelty detection via diffusion models has also gained much attention. Recent methods have mainly utilized the reconstruction property of in-distribution samples. However, they often suffer from detecting OOD samples that share similar background information to the in-distribution data. Based on our observation that diffusion models can \emph{project} any sample to an in-distribution sample with similar background information, we propose \emph{Projection Regret (PR)}, an efficient novelty detection method that mitigates the bias of non-semantic information. To be specific, PR computes the perceptual distance between the test image and its diffusion-based projection to detect abnormality. Since the perceptual distance often fails to capture semantic changes when the background information is dominant, we cancel out the background bias by comparing it against recursive projections. Extensive experiments demonstrate that PR outperforms the prior art of generative-model-based novelty detection methods by a significant margin.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°é²œåº¦æ£€æµ‹æ˜¯æœºå™¨å­¦ä¹ çš„åŸºæœ¬ä»»åŠ¡ä¹‹ä¸€ï¼Œæ—¨åœ¨æ£€æµ‹å¼‚å¸¸ï¼ˆå³å¤–éƒ¨åˆ†å¸ƒï¼ˆOODï¼‰ï¼‰æ ·æœ¬ã€‚ç”±äºæ‰©æ•£æ¨¡å‹æœ€è¿‘åœ¨ç”Ÿæˆæ¡†æ¶ä¸­å¾—åˆ°äº†å¾ˆå¤šå…³æ³¨ï¼Œå› æ­¤é€šè¿‡æ‰©æ•£æ¨¡å‹è¿›è¡Œæ–°é²œåº¦æ£€æµ‹ä¹Ÿå¾—åˆ°äº†å¾ˆå¤šå…³æ³¨ã€‚ current methods mainly rely on the reconstruction property of in-distribution samples, but they often suffer from detecting OOD samples that share similar background information to the in-distribution data. Based on our observation that diffusion models can project any sample to an in-distribution sample with similar background information, we propose Projection Regret (PR), an efficient novelty detection method that mitigates the bias of non-semantic information. To be specific, PR computes the perceptual distance between the test image and its diffusion-based projection to detect abnormality. Since the perceptual distance often fails to capture semantic changes when the background information is dominant, we cancel out the background bias by comparing it against recursive projections. EXTENSIVE experiments demonstrate that PR outperforms the prior art of generative-model-based novelty detection methods by a significant margin.
</details></li>
</ul>
<hr>
<h2 id="A-Unified-Simulation-Framework-for-Visual-and-Behavioral-Fidelity-in-Crowd-Analysis"><a href="#A-Unified-Simulation-Framework-for-Visual-and-Behavioral-Fidelity-in-Crowd-Analysis" class="headerlink" title="A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis"></a>A Unified Simulation Framework for Visual and Behavioral Fidelity in Crowd Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02613">http://arxiv.org/abs/2312.02613</a></li>
<li>repo_url: None</li>
<li>paper_authors: NiccolÃ² Bisagno, Nicola Garau, Antonio Luigi Stefani, Nicola Conci</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦æ˜¯ä¸ºäº†ç ”ç©¶äººç¾¤èšé›†çš„ simulationsï¼Œä»¥ç”Ÿæˆé€‚åˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸€ç§äººç¾¤ simulate çš„å·¥å…·ï¼Œå«åš UniCrowdï¼Œä»¥åŠä¸€ä¸ªéªŒè¯ç®¡é“ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡é€šè¿‡ä½¿ç”¨ UniCrowd ç”Ÿæˆäº†ä¸€äº›é€‚åˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„æ ‡æ³¨æ•°æ®ï¼ŒåŒ…æ‹¬äººç¾¤æ’ç‰ˆã€äººå§¿ä¼°è®¡ã€è½¨è¿¹åˆ†æå’Œé¢„æµ‹ã€ä»¥åŠå¼‚å¸¸æ£€æµ‹ç­‰ã€‚<details>
<summary>Abstract</summary>
Simulation is a powerful tool to easily generate annotated data, and a highly desirable feature, especially in those domains where learning models need large training datasets. Machine learning and deep learning solutions, have proven to be extremely data-hungry and sometimes, the available real-world data are not sufficient to effectively model the given task. Despite the initial skepticism of a portion of the scientific community, the potential of simulation has been largely confirmed in many application areas, and the recent developments in terms of rendering and virtualization engines, have shown a good ability also in representing complex scenes. This includes environmental factors, such as weather conditions and surface reflectance, as well as human-related events, like human actions and behaviors. We present a human crowd simulator, called UniCrowd, and its associated validation pipeline. We show how the simulator can generate annotated data, suitable for computer vision tasks, in particular for detection and segmentation, as well as the related applications, as crowd counting, human pose estimation, trajectory analysis and prediction, and anomaly detection.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>ä¼ è¾“æ–‡æœ¬åˆ°ç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>æ¨¡æ‹Ÿæ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥è½»æ¾ç”Ÿæˆæ³¨è§£æ•°æ®ï¼Œç‰¹åˆ«åœ¨é‚£äº›é¢†åŸŸ where å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„è®­ç»ƒæ•°æ®ã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œå·²ç»è¯æ˜äº†å®ƒä»¬å¯¹äºè§£å†³ç»™å®šä»»åŠ¡éå¸¸æ¸´æœ›å¤§é‡æ•°æ®ã€‚å°½ç®¡å½“åˆä¸€éƒ¨åˆ†ç§‘å­¦ç•Œå¯¹ simulation çš„å¯èƒ½æ€§è¡¨ç¤ºæ€€ç–‘ï¼Œä½†æ˜¯ï¼Œåœ¨è®¸å¤šåº”ç”¨é¢†åŸŸï¼Œ simulation çš„æ½œåŠ›å·²ç»å¾—åˆ°äº†è¯æ˜ã€‚æœ€è¿‘çš„æ¸²æŸ“å’Œè™šæ‹ŸåŒ–å¼•æ“çš„è¿›æ­¥ï¼Œä¹Ÿè¡¨æ˜å®ƒä»¬åœ¨è¡¨ç¤ºå¤æ‚åœºæ™¯æ–¹é¢å…·æœ‰è‰¯å¥½çš„èƒ½åŠ›ã€‚è¿™åŒ…æ‹¬ç¯å¢ƒå› ç´ ï¼Œå¦‚å¤©æ°”æ¡ä»¶å’Œè¡¨é¢åå°„ï¼Œä»¥åŠäººç±»ç›¸å…³çš„äº‹ä»¶ï¼Œå¦‚äººç±»è¡Œä¸ºå’Œä¹ æƒ¯ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§äººç¾¤æ¨¡æ‹Ÿå™¨ï¼Œå«åš UniCrowdï¼Œä»¥åŠå…¶ç›¸å…³çš„éªŒè¯ç®¡é“ã€‚æˆ‘ä»¬æ˜¾ç¤ºäº†æ¨¡æ‹Ÿå™¨å¯ä»¥ç”Ÿæˆæ³¨è§£æ•°æ®ï¼Œé€‚ç”¨äºè®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹å’Œåˆ†å‰²ï¼Œä»¥åŠç›¸å…³åº”ç”¨ï¼Œå¦‚äººç¾¤è®¡æ•°ã€äººå§¿ä¼°è®¡ã€è½¨è¿¹åˆ†æå’Œé¢„æµ‹ï¼Œä»¥åŠå¼‚å¸¸æ£€æµ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Accelerating-Learnt-Video-Codecs-with-Gradient-Decay-and-Layer-wise-Distillation"><a href="#Accelerating-Learnt-Video-Codecs-with-Gradient-Decay-and-Layer-wise-Distillation" class="headerlink" title="Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise Distillation"></a>Accelerating Learnt Video Codecs with Gradient Decay and Layer-wise Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02605">http://arxiv.org/abs/2312.02605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianhao Peng, Ge Gao, Heming Sun, Fan Zhang, David Bull</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºæ¨¡å‹ç‹¬ç«‹æŠ½è±¡çš„è§†é¢‘ç¼–ç å™¨å‹ç¼©æ¨¡å‹ï¼Œä»¥æé«˜è§†é¢‘ç¼–ç å™¨çš„è®¡ç®—æ•ˆç‡å’Œå®æ—¶æ€§ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºæ¢¯åº¦è¡°å‡å’Œå±‚æ¬¡ç»ƒä¹ çš„æ¨¡å‹ç‹¬ç«‹æŠ½è±¡ç­–ç•¥ï¼Œå¯ä»¥åœ¨å‹ç¼©è¿‡ç¨‹ä¸­å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ã€‚</li>
<li>results: è¯•éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç­–ç•¥å¯ä»¥åœ¨ä¸‰ç§æµè¡Œçš„ç»ˆç«¯å­¦ä¹ è§†é¢‘ç¼–ç å™¨ä¸­å®ç°65%çš„è®¡ç®—å‡å°‘å’Œ2å€çš„é€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒè§†é¢‘è´¨é‡ä¸‹é™åœ¨0.3dBä»¥ä¸‹ã€‚<details>
<summary>Abstract</summary>
In recent years, end-to-end learnt video codecs have demonstrated their potential to compete with conventional coding algorithms in term of compression efficiency. However, most learning-based video compression models are associated with high computational complexity and latency, in particular at the decoder side, which limits their deployment in practical applications. In this paper, we present a novel model-agnostic pruning scheme based on gradient decay and adaptive layer-wise distillation. Gradient decay enhances parameter exploration during sparsification whilst preventing runaway sparsity and is superior to the standard Straight-Through Estimation. The adaptive layer-wise distillation regulates the sparse training in various stages based on the distortion of intermediate features. This stage-wise design efficiently updates parameters with minimal computational overhead. The proposed approach has been applied to three popular end-to-end learnt video codecs, FVC, DCVC, and DCVC-HEM. Results confirm that our method yields up to 65% reduction in MACs and 2x speed-up with less than 0.3dB drop in BD-PSNR. Supporting code and supplementary material can be downloaded from: https://jasminepp.github.io/lightweightdvc/
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Integrated-System-for-Spatio-Temporal-Summarization-of-360-degrees-Videos"><a href="#An-Integrated-System-for-Spatio-Temporal-Summarization-of-360-degrees-Videos" class="headerlink" title="An Integrated System for Spatio-Temporal Summarization of 360-degrees Videos"></a>An Integrated System for Spatio-Temporal Summarization of 360-degrees Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02576">http://arxiv.org/abs/2312.02576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ioannis Kontostathis, Evlampios Apostolidis, Vasileios Mezaris</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¼€å‘ä¸€ä¸ª integrate çš„ç³»ç»Ÿï¼Œç”¨äºå¯¹360åº¦è§†é¢‘è¿›è¡Œç©ºé—´æ—¶é—´æ¦‚è¦ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿåˆ©ç”¨ cutting-edge çš„ç²¾åº¦æ£€æµ‹æ–¹æ³•ï¼ˆATSal å’Œ SST-Salï¼‰å’Œè§†é¢‘æ¦‚è¦ç”Ÿæˆæ–¹æ³•ï¼ˆCA-SUMï¼‰ï¼Œä»¥åŠä¸€ç§ mechanism æ¥ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ360åº¦è§†é¢‘æ˜¯é™æ­¢æˆ–ç§»åŠ¨æ‘„åƒæœºè®°å½•çš„ï¼Œå¹¶é€‰æ‹©é€‚ç”¨çš„ç²¾åº¦æ£€æµ‹æ–¹æ³•ã€‚</li>
<li>results: å¯¹äºä¸¤ä¸ª360åº¦è§†é¢‘æ•°æ®é›†ï¼ˆVR-EyeTracking å’Œ Sports-360ï¼‰è¿›è¡Œäº†é‡åŒ–è¯„ä¼°ï¼Œå¹¶è¡¨æ˜äº†è¯¥å†³ç­–æœºåˆ¶çš„å‡†ç¡®æ€§å’Œæ­£é¢å½±å“ã€‚æ­¤å¤–ï¼Œå¯¹äºè¿™äº›æ•°æ®é›†ï¼Œ Qualitative åˆ†æä¹Ÿè¡¨æ˜äº†å†³ç­–æœºåˆ¶çš„å·¥ä½œæ–¹å¼ï¼Œå¹¶è¯æ˜äº†æ¯ç§ç²¾åº¦æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œä»¥åŠè®­ç»ƒåçš„æ¦‚è¦ç”Ÿæˆæ–¹æ³•çš„é«˜æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
In this work, we present an integrated system for spatiotemporal summarization of 360-degrees videos. The video summary production mainly involves the detection of salient events and their synopsis into a concise summary. The analysis relies on state-of-the-art methods for saliency detection in 360-degrees video (ATSal and SST-Sal) and video summarization (CA-SUM). It also contains a mechanism that classifies a 360-degrees video based on the use of static or moving camera during recording and decides which saliency detection method will be used, as well as a 2D video production component that is responsible to create a conventional 2D video containing the salient events in the 360-degrees video. Quantitative evaluations using two datasets for 360-degrees video saliency detection (VR-EyeTracking, Sports-360) show the accuracy and positive impact of the developed decision mechanism, and justify our choice to use two different methods for detecting the salient events. A qualitative analysis using content from these datasets, gives further insights about the functionality of the decision mechanism, shows the pros and cons of each used saliency detection method and demonstrates the advanced performance of the trained summarization method against a more conventional approach.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªé›†æˆçš„360åº¦è§†é¢‘å®¡æ ¸ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿçš„è§†é¢‘å®¡æ ¸ä¸»è¦åŒ…æ‹¬è¯†åˆ«ç²¾å½©äº‹ä»¶å’Œå°†å…¶ç®€è¦æ¦‚æ‹¬ä¸ºä¸€ä¸ªçŸ­è§†é¢‘ã€‚åˆ†æåˆ©ç”¨å½“å‰é¢†åŸŸæœ€ä½³çš„360åº¦è§†é¢‘ç²¾å½©æ£€æµ‹æ–¹æ³•ï¼ˆATSalå’ŒSST-Salï¼‰å’Œè§†é¢‘æ¦‚è¦ç”Ÿæˆæ–¹æ³•ï¼ˆCA-SUMï¼‰ï¼Œè¿˜åŒ…æ‹¬ä¸€ç§æœºåˆ¶æ¥åˆ¤æ–­360åº¦è§†é¢‘æ˜¯å¦ä½¿ç”¨é™æ­¢æˆ–ç§»åŠ¨æ‘„åƒæœºè®°å½•ï¼Œå¹¶å†³å®šä½¿ç”¨å“ªç§ç²¾å½©æ£€æµ‹æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç³»ç»Ÿè¿˜åŒ…æ‹¬ä¸€ä¸ªè´Ÿè´£å°†360åº¦è§†é¢‘è½¬æ¢æˆæ ‡å‡†2Dè§†é¢‘çš„ç»„ä»¶ï¼Œä»¥ä¾¿åŒ…å«ç²¾å½©äº‹ä»¶ã€‚ç»è¿‡é‡æµ‹ä½¿ç”¨ä¸¤ä¸ª360åº¦è§†é¢‘ç²¾å½©æ£€æµ‹æ•°æ®é›†ï¼ˆVR-EyeTrackingã€Sports-360ï¼‰ï¼Œæ˜¾ç¤ºå‡ºæˆ‘ä»¬çš„å†³ç­–æœºåˆ¶å‡†ç¡®æ€§å’Œç§¯æå½±å“ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„é€‰æ‹©ä½¿ç”¨ä¸¤ç§ä¸åŒçš„ç²¾å½©æ£€æµ‹æ–¹æ³•æ˜¯æ­£ç¡®çš„ã€‚å¦å¤–ï¼Œä¸€ä¸ªè¯¦ç»†çš„åˆ†æä½¿ç”¨è¿™äº›æ•°æ®é›†çš„å†…å®¹ï¼Œæä¾›äº†æ›´å¤šå…³äºå†³ç­–æœºåˆ¶çš„ä¿¡æ¯ï¼Œæè¿°äº†æ¯ç§ä½¿ç”¨çš„ç²¾å½©æ£€æµ‹æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„è®­ç»ƒ SUM æ–¹æ³•åœ¨å¯¹ä¸€ç§ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œè®­ç»ƒåè¡¨ç°å‡ºè‰²ã€‚
</details></li>
</ul>
<hr>
<h2 id="Prompt2NeRF-PIL-Fast-NeRF-Generation-via-Pretrained-Implicit-Latent"><a href="#Prompt2NeRF-PIL-Fast-NeRF-Generation-via-Pretrained-Implicit-Latent" class="headerlink" title="Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent"></a>Prompt2NeRF-PIL: Fast NeRF Generation via Pretrained Implicit Latent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02568">http://arxiv.org/abs/2312.02568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianmeng Liu, Yuyao Zhang, Zeyuan Meng, Yu-Wing Tai, Chi-Keung Tang</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†å¯æç¤ºNeRFç”Ÿæˆï¼ˆå¦‚æ–‡æœ¬æç¤ºæˆ–å•ä¸€å›¾åƒæç¤ºï¼‰çš„ç›´æ¥æ¡ä»¶å’Œå¿«é€Ÿç”ŸæˆNeRFå‚æ•°çš„Underlying 3Dåœºæ™¯ï¼Œä»è€Œæ¶ˆé™¤å¤æ‚çš„ä¸­é—´æ­¥éª¤ï¼Œæä¾›å…¨3Dç”Ÿæˆä»¥ conditionalæ§åˆ¶ã€‚</li>
<li>methods: æœ¬æ–¹æ³•ä½¿ç”¨Prompt2NeRF-PILï¼Œä¸€ç§å¯ä»¥é€šè¿‡å•ä¸€çš„å‰è¿› passç”Ÿæˆå¤šç§3Då¯¹è±¡ï¼Œå¹¶ä¸”å¯ä»¥åˆ©ç”¨é¢„è®­ç»ƒçš„å«éšç¤ºå‡†NeRFå‚æ•°çš„ç©ºé—´æ¥è¿›è¡Œ3Dç”Ÿæˆã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨é›¶åŸºç¡€ä»»åŠ¡ä¸­ç”Ÿæˆé«˜è´¨é‡çš„NeRFï¼Œå¹¶ä¸”å¯ä»¥å¿«é€ŸåŠ é€Ÿç°æœ‰çš„æç¤º-to-NeRFæ–¹æ³•çš„æ¨ç†è¿‡ç¨‹ã€‚ specifically,æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åŠ é€ŸDreamFusionæ–‡æœ¬-to-NeRFæ¨¡å‹å’ŒZero-1-to-3å›¾åƒ-to-NeRFæ–¹æ³•çš„3Dé‡å»ºé€Ÿåº¦ï¼Œæé«˜3-5å€ã€‚<details>
<summary>Abstract</summary>
This paper explores promptable NeRF generation (e.g., text prompt or single image prompt) for direct conditioning and fast generation of NeRF parameters for the underlying 3D scenes, thus undoing complex intermediate steps while providing full 3D generation with conditional control. Unlike previous diffusion-CLIP-based pipelines that involve tedious per-prompt optimizations, Prompt2NeRF-PIL is capable of generating a variety of 3D objects with a single forward pass, leveraging a pre-trained implicit latent space of NeRF parameters. Furthermore, in zero-shot tasks, our experiments demonstrate that the NeRFs produced by our method serve as semantically informative initializations, significantly accelerating the inference process of existing prompt-to-NeRF methods. Specifically, we will show that our approach speeds up the text-to-NeRF model DreamFusion and the 3D reconstruction speed of the image-to-NeRF method Zero-1-to-3 by 3 to 5 times.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation notes:* "promptable NeRF generation" is translated as "å¯æç¤ºNeRFç”Ÿæˆ" (kÄ› bÃ¬ng NeRF shÄ“ng chÃ©ng)* "direct conditioning" is translated as "ç›´æ¥æ§åˆ¶" (zhÃ­ dÃ¬ kÃ²ng zhÃ¬)* "full 3D generation" is translated as "å…¨3Dç”Ÿæˆ" (quÃ¡n sÄn jÃ­ shÄ“ng chÃ©ng)* "conditional control" is translated as "æ¡ä»¶æ§åˆ¶" (tiÃ¡o xiÃ ng kÃ²ng zhÃ¬)* "diffusion-CLIP-based pipelines" is translated as "å¹²æ‰°-CLIPåŸºäºçš„ç®¡é“" (shuÄ zhÃ­-CLIP jÄ« yÇ” de guÇn dÃ o)* "per-prompt optimizations" is translated as "æ¯ä¸ªæç¤ºä¼˜åŒ–" (mÄ“i ge bÃ¬ng chÄ“ng yÇo jÄ«)* "pre-trained implicit latent space" is translated as "é¢„è®­ç»ƒçš„å«ä¹‰éšè—ç©ºé—´" (xiÄng xiÇng zhÄ« xiÇng de hÃ¡n yÃ¬ yÇn huÄ« kÅng jÄ«)* "semantically informative initializations" is translated as "å«ä¹‰ä¿¡æ¯çš„åˆå§‹åŒ–" (hÃ¡n yÃ¬ xÃ¬n xÄ«n de chÅ« shÃ­)* "accelerating the inference process" is translated as "åŠ é€Ÿæ¨ç†è¿‡ç¨‹" (jiÄ sÃ¹ tuÄ« lÇ guÃ² jÃ¬)* "text-to-NeRF model" is translated as "æ–‡æœ¬åˆ°NeRFæ¨¡å‹" (wÃ©n tÄ›ng dao NeRF mÃ³del)* "3D reconstruction speed" is translated as "3Dé‡å»ºé€Ÿåº¦" (3D zhÃ²ng jiÃ n sÃ¹ dÃ¹)
</details></li>
</ul>
<hr>
<h2 id="Think-Twice-Before-Selection-Federated-Evidential-Active-Learning-for-Medical-Image-Analysis-with-Domain-Shifts"><a href="#Think-Twice-Before-Selection-Federated-Evidential-Active-Learning-for-Medical-Image-Analysis-with-Domain-Shifts" class="headerlink" title="Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts"></a>Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02567">http://arxiv.org/abs/2312.02567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayi Chen, Benteng Ma, Hengfei Cui, Yong Xia, Kwang-Ting Cheng</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜ Federated Learning ä¸­çš„æ•°æ®è¯„ä¼°è¿‡ç¨‹ï¼Œä»¥ä¾¿æ›´å¥½åœ°åˆ©ç”¨åˆ†æ•£åœ¨å¤šä¸ªåŒ»ç–—æœºæ„ä¸­çš„æ•°æ®ï¼Œè€Œä¸éœ€è¦ä¸­å¤®åŒ–æ•°æ®ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº† Federated Evidential Active Learning (FEAL) æ–¹æ³•ï¼Œå®ƒå°†åœ¨ä¸åŒé¢†åŸŸä¸­çš„æ•°æ® derivation è¿›è¡Œè¯„ä¼°ï¼Œå¹¶ä½¿ç”¨ Dirichlet åˆ†å¸ƒæ¥æ•æ‰æœ¬åœ°å’Œå…¨çƒæ¨¡å‹çš„é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼ŒFEAL æ–¹æ³•æ¯” state-of-the-art æ´»è·ƒå­¦æ–¹æ³•æ›´æœ‰æ•ˆç‡ï¼Œå¹¶ä¸”åœ¨ Federated Active Learning æ¡†æ¶ä¸‹å®ç°äº†æ›´å¥½çš„èµ„æ–™å¤šæ ·æ€§å’Œèµ„æ–™èŒƒå›´ã€‚<details>
<summary>Abstract</summary>
Federated learning facilitates the collaborative learning of a global model across multiple distributed medical institutions without centralizing data. Nevertheless, the expensive cost of annotation on local clients remains an obstacle to effectively utilizing local data. To mitigate this issue, federated active learning methods suggest leveraging local and global model predictions to select a relatively small amount of informative local data for annotation. However, existing methods mainly focus on all local data sampled from the same domain, making them unreliable in realistic medical scenarios with domain shifts among different clients. In this paper, we make the first attempt to assess the informativeness of local data derived from diverse domains and propose a novel methodology termed Federated Evidential Active Learning (FEAL) to calibrate the data evaluation under domain shift. Specifically, we introduce a Dirichlet prior distribution in both local and global models to treat the prediction as a distribution over the probability simplex and capture both aleatoric and epistemic uncertainties by using the Dirichlet-based evidential model. Then we employ the epistemic uncertainty to calibrate the aleatoric uncertainty. Afterward, we design a diversity relaxation strategy to reduce data redundancy and maintain data diversity. Extensive experiments and analyses are conducted to show the superiority of FEAL over the state-of-the-art active learning methods and the efficiency of FEAL under the federated active learning framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
è”é‚¦å­¦ä¹ å¯ä»¥å¸®åŠ©å¤šä¸ªåˆ†æ•£çš„åŒ»ç–—æœºæ„å…±åŒå­¦ä¹ ä¸€ä¸ªå…¨çƒæ¨¡å‹ï¼Œè€Œä¸éœ€è¦ä¸­å¤®åŒ–æ•°æ®ã€‚ç„¶è€Œï¼Œå½“åœ°æ–¹æœºæ„çš„æ ‡ç­¾æˆæœ¬é«˜æ˜‚æ—¶ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å®é™…ä¸Šä¸èƒ½å……åˆ†åˆ©ç”¨åœ°æ–¹æ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè”é‚¦æ´»åŠ¨å­¦ä¹ æ–¹æ³•å»ºè®®ä½¿ç”¨åœ°æ–¹å’Œå…¨çƒæ¨¡å‹é¢„æµ‹æ¥é€‰æ‹©ä¸€å°é‡å…·æœ‰èµ„è®¯çš„åœ°æ–¹æ•°æ®è¿›è¡Œæ ‡ç­¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ–¹æ³•ä¸»è¦å°†æ³¨æ„åŠ›é›†ä¸­åœ¨åŒä¸€ä¸ªé¢†åŸŸä¸­çš„æ‰€æœ‰åœ°æ–¹æ•°æ®ä¸Šï¼Œå› æ­¤åœ¨å®é™…çš„åŒ»ç–—åœºæ™¯ä¸­ï¼Œå½“å­˜åœ¨ä¸åŒå®¢æˆ·ç«¯ä¹‹é—´çš„é¢†åŸŸè½¬ç§»æ—¶ï¼Œè¿™äº›æ–¹æ³•å¯èƒ½æ— æ³•å®é™…ä½¿ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬åšå‡ºäº†é¦–æ¬¡å°è¯•ï¼Œä»¥è¯„ä¼°åœ°æ–¹æ•°æ®æ¥è‡ªä¸åŒé¢†åŸŸçš„æœ‰ç”¨æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºè”é‚¦è¯æ®æ´»åŠ¨å­¦ä¹ ï¼ˆFEALï¼‰ï¼Œä»¥è°ƒæ•´æ•°æ®è¯„ä¼°ä¸‹é¢†åŸŸè½¬ç§»çš„æƒ…å†µã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åœ°æ–¹å’Œå…¨çƒæ¨¡å‹ä¸­çš„é¢„æµ‹è§†ä¸ºä¸€ä¸ªåˆ†å¸ƒåœ¨æ¦‚ç‡Simplexä¸Šï¼Œå¹¶ä½¿ç”¨DirichletåŸºäºçš„è¯æ®æ¨¡å‹æ¥æ•æ‰è¿™ä¸¤ç§ä¸ç¡®å®šæ€§ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªepistemicä¸ç¡®å®šæ€§æ¥è°ƒæ•´è¿™ä¸ªaleatoricä¸ç¡®å®šæ€§ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§å¤šæ ·æ€§æ”¾æ¾ç­–ç•¥ï¼Œä»¥å‡å°‘æ•°æ®çš„é‡å¤å’Œä¿æŒæ•°æ®çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒå’Œåˆ†æï¼Œä»¥è¯æ˜ FEAL åœ¨è”é‚¦æ´»åŠ¨å­¦ä¹ æ¡†æ¶ä¸‹çš„è¶…è¶Šæ€§å’Œæ•ˆç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Uni3DL-Unified-Model-for-3D-and-Language-Understanding"><a href="#Uni3DL-Unified-Model-for-3D-and-Language-Understanding" class="headerlink" title="Uni3DL: Unified Model for 3D and Language Understanding"></a>Uni3DL: Unified Model for 3D and Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03026">http://arxiv.org/abs/2312.03026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Jian Ding, Zhaoyang Chen, Mohamed Elhoseiny<br>for:This paper presents a unified model for 3D and Language understanding, called Uni3DL, which can perform various tasks in 3D vision and language understanding.methods:The Uni3DL model uses a query transformer to learn task-agnostic semantic and mask outputs by attending to 3D visual features, and a task router to selectively generate task-specific outputs required for diverse tasks.results:The Uni3DL model has been evaluated across diverse 3D vision-language understanding tasks and demonstrates performance on par with or surpassing state-of-the-art task-specific models.<details>
<summary>Abstract</summary>
In this work, we present Uni3DL, a unified model for 3D and Language understanding. Distinct from existing unified vision-language models in 3D which are limited in task variety and predominantly dependent on projected multi-view images, Uni3DL operates directly on point clouds. This approach significantly expands the range of supported tasks in 3D, encompassing both vision and vision-language tasks in 3D. At the core of Uni3DL, a query transformer is designed to learn task-agnostic semantic and mask outputs by attending to 3D visual features, and a task router is employed to selectively generate task-specific outputs required for diverse tasks. With a unified architecture, our Uni3DL model enjoys seamless task decomposition and substantial parameter sharing across tasks. Uni3DL has been rigorously evaluated across diverse 3D vision-language understanding tasks, including semantic segmentation, object detection, instance segmentation, visual grounding, 3D captioning, and text-3D cross-modal retrieval. It demonstrates performance on par with or surpassing state-of-the-art (SOTA) task-specific models. We hope our benchmark and Uni3DL model will serve as a solid step to ease future research in unified models in the realm of 3D and language understanding. Project page: https://uni3dl.github.io.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºUni3DLæ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„3Då’Œè¯­è¨€ç†è§£æ¨¡å‹ã€‚ä¸ç°æœ‰çš„3Dç»Ÿä¸€è§†åŠ›è¯­è¨€æ¨¡å‹ä¸åŒï¼ŒUni3DLç›´æ¥å¤„ç†ç‚¹äº‘ï¼Œè¿™å°†æ‰©å¤§3Dæ”¯æŒçš„ä»»åŠ¡èŒƒå›´ï¼Œæ¶µç›–äº†è§†åŠ›å’Œè§†åŠ›è¯­è¨€ä»»åŠ¡ã€‚Uni3DLçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªæŸ¥è¯¢è½¬æ¢å™¨ï¼Œç”¨äºå­¦ä¹ ä»»åŠ¡æ— å…³çš„ semanticå’Œmaskè¾“å‡ºï¼Œå¹¶ä½¿ç”¨ä¸€ä¸ªä»»åŠ¡å¯¼èˆªå™¨æ¥é€‰æ‹©ivelyç”Ÿæˆä»»åŠ¡ç‰¹å®šçš„è¾“å‡ºã€‚ç”±äºå…·æœ‰ç»Ÿä¸€æ¶æ„ï¼Œæˆ‘ä»¬çš„Uni3DLæ¨¡å‹å¯ä»¥å®ç°æ— ç¼ä»»åŠ¡åˆ†è§£å’Œå¤§é‡å‚æ•°å…±äº«ã€‚æˆ‘ä»¬åœ¨å¤šç§3Dè§†åŠ›è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¿›è¡Œäº†ä¸¥æ ¼çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬semantic segmentationã€object detectionã€instance segmentationã€è§†ç‰©è¯†åˆ«ã€3Dæ ‡æ³¨å’Œæ–‡æœ¬-3Dcross-modalæ£€ç´¢ã€‚Uni3DLæ¨¡å‹åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°äº†ä¸æˆ–è¶…è¿‡äº†çŠ¶æ€ä¹‹arteï¼ˆSOTAï¼‰ä»»åŠ¡ç‰¹å®šæ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æµ‹è¯•å’ŒUni3DLæ¨¡å‹å¯ä»¥ä¸ºæœªæ¥3Då’Œè¯­è¨€ç†è§£é¢†åŸŸçš„ç ”ç©¶æä¾›ä¸€ä¸ªåšå®çš„åŸºç¡€ã€‚é¡¹ç›®é¡µé¢ï¼šhttps://uni3dl.github.ioã€‚
</details></li>
</ul>
<hr>
<h2 id="GeNIe-Generative-Hard-Negative-Images-Through-Diffusion"><a href="#GeNIe-Generative-Hard-Negative-Images-Through-Diffusion" class="headerlink" title="GeNIe: Generative Hard Negative Images Through Diffusion"></a>GeNIe: Generative Hard Negative Images Through Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02548">http://arxiv.org/abs/2312.02548</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ucdvision/genie">https://github.com/ucdvision/genie</a></li>
<li>paper_authors: Soroush Abbasi Koohpayegani, Anuj Singh, K L Navaneet, Hadi Jamali-Rad, Hamed Pirsiavash</li>
<li>for: ç”¨äºè®­ç»ƒæ·±åº¦æ¨¡å‹ï¼Œé¿å…è¿‡æ‹Ÿåˆå…·æœ‰æœ‰é™æ•°æ®çš„é—®é¢˜ã€‚</li>
<li>methods: ä½¿ç”¨æ‰©å±•AIæŠ€æœ¯ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ä¸ºå›¾åƒç”Ÿæˆï¼Œå®ç°æ›´åŠ å¤æ‚çš„æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œç”Ÿæˆæ›´åƒè‡ªç„¶å›¾åƒçš„æ•°æ®ã€‚</li>
<li>results: é€šè¿‡å¯¹åˆ†ç±»å™¨çš„å†³ç­–è¾¹ç•Œè¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œç”Ÿæˆçš„æ‰©å……æ ·æœ¬èƒ½å¤Ÿæ›´æœ‰æ•ˆç‡åœ°å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚é€šè¿‡æ–‡æœ¬æè¿°å’Œå›¾åƒææ–™çš„æ··åˆï¼Œç”Ÿæˆå‡ºæ›´åŠ æŒ‘æˆ˜æ€§çš„æ ·æœ¬ï¼Œå°¤å…¶æ˜¯åœ¨å°‘shotå’Œé•¿å°¾åˆ†å¸ƒè®¾ç½®ä¸‹ã€‚<details>
<summary>Abstract</summary>
Data augmentation is crucial in training deep models, preventing them from overfitting to limited data. Common data augmentation methods are effective, but recent advancements in generative AI, such as diffusion models for image generation, enable more sophisticated augmentation techniques that produce data resembling natural images. We recognize that augmented samples closer to the ideal decision boundary of a classifier are particularly effective and efficient in guiding the learning process. We introduce GeNIe which leverages a diffusion model conditioned on a text prompt to merge contrasting data points (an image from the source category and a text prompt from the target category) to generate challenging samples for the target category. Inspired by recent image editing methods, we limit the number of diffusion iterations and the amount of noise. This ensures that the generated image retains low-level and contextual features from the source image, potentially conflicting with the target category. Our extensive experiments, in few-shot and also long-tail distribution settings, demonstrate the effectiveness of our novel augmentation method, especially benefiting categories with a limited number of examples.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>é€šè¿‡æ•°æ®æ‰©å……ï¼Œæ·±åº¦æ¨¡å‹å¯ä»¥é¿å…è¿‡æ‹Ÿåˆ Limited Dataã€‚å¸¸è§çš„æ•°æ®æ‰©å……æ–¹æ³•æ˜¯æœ‰æ•ˆçš„ï¼Œä½†æ˜¯æœ€è¿‘çš„ç”ŸæˆAIæŠ€æœ¯ï¼Œå¦‚æ‰©æ•£æ¨¡å‹ Ğ´Ğ»Ñå›¾åƒç”Ÿæˆï¼Œå…è®¸æ›´åŠ å¤æ‚çš„æ‰©å……æŠ€æœ¯ï¼Œç”Ÿæˆæ›´åƒè‡ªç„¶å›¾åƒçš„æ•°æ®ã€‚æˆ‘ä»¬è®¤ä¸ºå·² augmented æ ·æœ¬æ›´æ¥è¿‘åˆ†ç±»å™¨çš„ç†æƒ³å†³ç­–è¾¹ç•Œï¼Œç‰¹åˆ«æœ‰æ•ˆå’Œæœ‰æ•ˆåœ°å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚æˆ‘ä»¬ä»‹ç»äº† GeNIeï¼Œåˆ©ç”¨ä¸€ä¸ªæ‰©æ•£æ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºæ¥åˆå¹¶å¯¹ç«‹æ•°æ®ç‚¹ï¼ˆä¸€ä¸ªæºç±»å›¾åƒå’Œä¸€ä¸ªç›®æ ‡ç±»æ–‡æœ¬æç¤ºï¼‰æ¥ç”Ÿæˆå¯¹ç›®æ ‡ç±»æ¥çš„æŒ‘æˆ˜æ ·æœ¬ã€‚å—æœ€è¿‘çš„å›¾åƒç¼–è¾‘æ–¹æ³•çš„å¯å‘ï¼Œæˆ‘ä»¬é™åˆ¶äº†æ‰©æ•£è¿­ä»£æ¬¡æ•°å’Œå™ªéŸ³çš„æ•°é‡ã€‚è¿™ç¡®ä¿äº†ç”Ÿæˆçš„å›¾åƒä¿ç•™äº† source å›¾åƒçš„ä½çº§å’Œä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œå¯èƒ½ä¸ç›®æ ‡ç±»å†²çªã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼ŒåŒ…æ‹¬å‡ ä¸ªä¾‹ç¤ºå’Œé•¿å°¾åˆ†å¸ƒè®¾ç½®ï¼Œ demonstrates æˆ‘ä»¬çš„æ–°çš„æ‰©å……æ–¹æ³•çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºå…·æœ‰æœ‰é™çš„ä¾‹å­æ•°çš„ç±»åˆ«ã€‚Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Machine-Vision-Therapy-Multimodal-Large-Language-Models-Can-Enhance-Visual-Robustness-via-Denoising-In-Context-Learning"><a href="#Machine-Vision-Therapy-Multimodal-Large-Language-Models-Can-Enhance-Visual-Robustness-via-Denoising-In-Context-Learning" class="headerlink" title="Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning"></a>Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02546">http://arxiv.org/abs/2312.02546</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmllab/machine_vision_therapy">https://github.com/tmllab/machine_vision_therapy</a></li>
<li>paper_authors: Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, Tongliang Liu</li>
<li>for: æé«˜visionæ¨¡å‹çš„é›¶åŸºç¡€ robustness</li>
<li>methods: åˆ©ç”¨å¤š modalå¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å’Œdenoising in-context learningï¼ˆDICLï¼‰ç­–ç•¥</li>
<li>results: é€šè¿‡ä¸ç›‘ç£çš„æ–¹å¼æé«˜visionæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶åœ¨å¤šä¸ªOOD datasetä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒ validate the effectiveness of our method.<details>
<summary>Abstract</summary>
Although vision models such as Contrastive Language-Image Pre-Training (CLIP) show impressive generalization performance, their zero-shot robustness is still limited under Out-of-Distribution (OOD) scenarios without fine-tuning. Instead of undesirably providing human supervision as commonly done, it is possible to take advantage of Multi-modal Large Language Models (MLLMs) that hold powerful visual understanding abilities. However, MLLMs are shown to struggle with vision problems due to the incompatibility of tasks, thus hindering their utilization. In this paper, we propose to effectively leverage MLLMs to conduct Machine Vision Therapy which aims to rectify the noisy predictions from vision models. By fine-tuning with the denoised labels, the learning model performance can be boosted in an unsupervised manner. To solve the incompatibility issue, we propose a novel Denoising In-Context Learning (DICL) strategy to align vision tasks with MLLMs. Concretely, by estimating a transition matrix that captures the probability of one class being confused with another, an instruction containing a correct exemplar and an erroneous one from the most probable noisy class can be constructed. Such an instruction can help any MLLMs with ICL ability to detect and rectify incorrect predictions of vision models. Through extensive experiments on ImageNet, WILDS, DomainBed, and other OOD datasets, we carefully validate the quantitative and qualitative effectiveness of our method. Our code is available at https://github.com/tmllab/Machine_Vision_Therapy.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡è§†è§‰æ¨¡å‹å¦‚å¯¹ç…§è¯­è¨€å›¾åƒé¢„è®­ç»ƒï¼ˆCLIPï¼‰è¡¨ç°å‡ºè‰²ï¼Œä½†å…¶é›¶shot Robustnessä»ç„¶å—åˆ°Out-of-Distributionï¼ˆOODï¼‰åœºæ™¯ä¸‹çš„é™åˆ¶ã€‚è€Œä¸éœ€è¦ä¸å¿…è¦åœ°æä¾›äººå·¥ç›‘ç£ï¼Œå¯ä»¥åˆ©ç”¨å¤šmodalå¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ï¼Œè¿™äº›æ¨¡å‹æ‹¥æœ‰å¼ºå¤§çš„è§†è§‰ç†è§£èƒ½åŠ›ã€‚ç„¶è€Œï¼ŒMLLMsåœ¨è§†è§‰ä»»åŠ¡ä¸Šå­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œè¿™ä½¿å¾—å®ƒä»¬çš„ä½¿ç”¨å—é™ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆåœ°åˆ©ç”¨MLLMsè¿›è¡Œæœºå™¨è§†è§‰ç–—æ³•ï¼Œä»¥æ”¹å–„è§†è§‰æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚é€šè¿‡ç²¾å¿ƒè°ƒæ•´æ‚ä¹±æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ— ç›‘ç£ä¸‹æé«˜å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºè§£å†³å…¼å®¹æ€§é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å‡å™ªåœ¨Contextå­¦ä¹ ç­–ç•¥ï¼ˆDICLï¼‰ï¼Œç”¨äºå°†è§†è§‰ä»»åŠ¡ä¸MLLMsç›¸åè°ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ä¼°ç®—ä¸€ä¸ªè½¬ç§»çŸ©é˜µï¼Œè¯¥çŸ©é˜µæ•æ‰äº†ä¸€ä¸ªç±»å‹ä¸å¦ä¸€ä¸ªç±»å‹ä¹‹é—´çš„æ··æ·†æ¦‚ç‡ã€‚é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«æ­£ç¡®ç¤ºä¾‹å’Œé”™è¯¯ç¤ºä¾‹çš„æŒ‡å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥å¸®åŠ©ä»»ä½•æ‹¥æœ‰ICLèƒ½åŠ›çš„MLLMsæ£€æµ‹å’Œä¿®æ­£è§†è§‰æ¨¡å‹çš„é”™è¯¯é¢„æµ‹ã€‚ç»è¿‡å¹¿æ³›çš„ImageNetã€WILDSã€DomainBedå’Œå…¶ä»–OOD datasetçš„å®éªŒï¼Œæˆ‘ä»¬è°¨æ…éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•çš„é‡åŒ–å’Œè´¨é‡æ•ˆæœã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨https://github.com/tmllab/Machine_Vision_Therapyä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Explainable-Severity-ranking-via-pairwise-n-hidden-comparison-a-case-study-of-glaucoma"><a href="#Explainable-Severity-ranking-via-pairwise-n-hidden-comparison-a-case-study-of-glaucoma" class="headerlink" title="Explainable Severity ranking via pairwise n-hidden comparison: a case study of glaucoma"></a>Explainable Severity ranking via pairwise n-hidden comparison: a case study of glaucoma</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02541">http://arxiv.org/abs/2312.02541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hong Nguyen, Cuong V. Nguyen, Shrikanth Narayanan, Benjamin Y. Xu, Michael Pazzani</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨ç”¨åŸºäºå›¾åƒçš„æ–¹æ³•è¯„ä¼°å’Œæ¯”è¾ƒå¼€Angle Glaucomaï¼ˆPOAGï¼‰çš„ä¸¥é‡ç¨‹åº¦ï¼Œä»¥å¸®åŠ©è¯Šæ–­å’Œè¯„ä¼°è¿™ç§ç–¾ç—…ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºsiameseç½‘ç»œçš„ä¸¥é‡æ€§æ’åæ–¹æ³•ï¼Œé€šè¿‡å¯¹å›¾åƒè¿›è¡Œå¯¹æ¯”æ¥è¯„ä¼°POAGçš„ä¸¥é‡ç¨‹åº¦ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§æ–°çš„è§£é‡Šæ–¹æ³•ï¼Œç”¨äºè§£é‡Šå›¾åƒçš„ä¸¥é‡ç¨‹åº¦é«˜æˆ–ä½çš„åŸå› ã€‚</li>
<li>results: è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒåŸºäºå›¾åƒçš„ä¸¥é‡æ€§æ’åæ¨¡å‹æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´å‡†ç¡®åœ°è¯Šæ–­POAGï¼ŒåŒæ—¶ä¹Ÿèƒ½å¤Ÿæä¾›æ›´å¥½çš„è§£é‡Šã€‚<details>
<summary>Abstract</summary>
Primary open-angle glaucoma (POAG) is a chronic and progressive optic nerve condition that results in an acquired loss of optic nerve fibers and potential blindness. The gradual onset of glaucoma results in patients progressively losing their vision without being consciously aware of the changes. To diagnose POAG and determine its severity, patients must undergo a comprehensive dilated eye examination. In this work, we build a framework to rank, compare, and interpret the severity of glaucoma using fundus images. We introduce a siamese-based severity ranking using pairwise n-hidden comparisons. We additionally have a novel approach to explaining why a specific image is deemed more severe than others. Our findings indicate that the proposed severity ranking model surpasses traditional ones in terms of diagnostic accuracy and delivers improved saliency explanations.
</details>
<details>
<summary>æ‘˜è¦</summary>
primary open-angle glaucoma (POAG) æ˜¯ä¸€ç§ Chronic å’Œè¿›è¡Œæ€§çš„Optic nerve conditionï¼Œä¼šå¯¼è‡´Acquired loss of optic nerve fibers å’Œ potential blindnessã€‚é€æ¸å‘å±•çš„ glaucoma ä¼šä½¿patients æ…¢æ…¢åœ°å¤±å»è§†åŠ›ï¼Œè€Œä¸æ˜¯Consciously aware of the changesã€‚ä¸ºè¯Šæ–­ POAG å’Œå…¶ä¸¥é‡ç¨‹åº¦ï¼Œæ‚£è€…å¿…é¡»è¿›è¡Œ comprehensive dilated eye examinationã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ’åã€æ¯”è¾ƒå’Œè§£é‡Š glaucoma ä¸¥é‡ç¨‹åº¦çš„æ¡†æ¶ï¼Œä½¿ç”¨åŸºäºsiameseçš„ä¸¥é‡æ’åæ–¹æ³•ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è§£é‡Šç‰¹å®šå›¾åƒæ˜¯å¦æ›´ä¸¥é‡äºåˆ«çš„ã€‚æˆ‘ä»¬çš„å‘ç°è¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„ä¸¥é‡æ’åæ¨¡å‹åœ¨è¯Šæ–­ç²¾åº¦å’Œæä¾›æ›´å¥½çš„Saliency explanations æ–¹é¢éƒ½è¶…è¿‡äº†ä¼ ç»Ÿçš„æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Breast-Cancer-Tumor-Classification-using-MobileNetV2-A-Detailed-Exploration-on-Image-Intensity-Error-Mitigation-and-Streamlit-driven-Real-time-Deployment"><a href="#Enhanced-Breast-Cancer-Tumor-Classification-using-MobileNetV2-A-Detailed-Exploration-on-Image-Intensity-Error-Mitigation-and-Streamlit-driven-Real-time-Deployment" class="headerlink" title="Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment"></a>Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03020">http://arxiv.org/abs/2312.03020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aaditya Surya, Aditya Shah, Jarnell Kabore, Subash Sasikumar</li>
<li>for: è¿™ä»½ç ”ç©¶æ—¨åœ¨å°†Googleçš„MobileNetV2æ¨¡å‹åº”ç”¨åˆ°ä¹³è…ºç™Œè‚‰ç˜¤åˆ†ç±»ä¸­ï¼Œä»¥åˆ†ä¸ºæ­£å¸¸ã€è‰¯æ€§å’Œæ¶æ€§ä¸‰ç§ç±»åˆ«ï¼Œä½¿ç”¨äº†1576å‡ ä¸ªè¶…éŸ³æ³¢å›¾åƒæ•°æ®ï¼ˆ265ä¸ªæ­£å¸¸ã€891ä¸ªè‰¯æ€§ã€420ä¸ªæ¶æ€§ï¼‰ã€‚</li>
<li>methods: è¿™ä»½ç ”ç©¶ä½¿ç”¨äº†MobileNetV2æ¨¡å‹ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œäº†è°ƒæ•´å’Œåˆ†ç±»ã€‚</li>
<li>results: ç ”ç©¶è·å¾—äº†0.82çš„å‡†ç¡®ç‡ã€0.83çš„ç²¾åº¦ã€0.81çš„å›ä¼ ç‡ã€ROC-AUCçš„0.94ã€PR-AUCçš„0.88å’ŒMCCçš„0.74ã€‚å®ƒè¿˜è¿›è¡Œäº†å›¾åƒæ•°æ®åˆ†å¸ƒå’Œé”™è¯¯åˆ†æï¼Œæä¾›äº†æœªæ¥åº”ç”¨çš„æ”¹è¿›ã€‚<details>
<summary>Abstract</summary>
This research introduces a sophisticated transfer learning model based on Google's MobileNetV2 for breast cancer tumor classification into normal, benign, and malignant categories, utilizing a dataset of 1576 ultrasound images (265 normal, 891 benign, 420 malignant). The model achieves an accuracy of 0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and MCC of 0.74. It examines image intensity distributions and misclassification errors, offering improvements for future applications. Addressing dataset imbalances, the study ensures a generalizable model. This work, using a dataset from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al., emphasizes MobileNetV2's potential in medical imaging, aiming to improve diagnostic precision in oncology. Additionally, the paper explores Streamlit-based deployment for real-time tumor classification, demonstrating MobileNetV2's applicability in medical imaging and setting a benchmark for future research in oncology diagnostics.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿™é¡¹ç ”ç©¶æ¨å‡ºäº†åŸºäºGoogleçš„MobileNetV2 Transfer Learningæ¨¡å‹ï¼Œç”¨äºåˆ†ç±»ä¹³è…ºç™Œä¸ºæ­£å¸¸ã€æ¶æ€§å’Œè‚¿ç˜¤ä¸‰ç±»ï¼Œä½¿ç”¨äº†1576å¼ ultrasoundå›¾åƒï¼ˆ265å¼ æ­£å¸¸ã€891å¼ æ¶æ€§ã€420å¼ è‚¿ç˜¤ï¼‰ã€‚æ¨¡å‹è¾¾åˆ°äº†0.82çš„å‡†ç¡®ç‡ã€0.83çš„ç²¾åº¦ã€0.81çš„å‡†ç¡®ç‡ã€0.94çš„ROC-AUCã€0.88çš„PR-AUCå’Œ0.74çš„MCCã€‚å®ƒåˆ†æäº†å›¾åƒå¼ºåº¦åˆ†å¸ƒå’Œè¯¯åˆ†ç±»é”™è¯¯ï¼Œæä¾›äº†æœªæ¥åº”ç”¨ä¸­çš„æ”¹è¿›ã€‚é€šè¿‡å¤„ç†æ•°æ®é›†åå¥½ï¼Œè¯¥ç ”ç©¶ç¡®ä¿äº†ä¸€ä¸ªé€šç”¨çš„æ¨¡å‹ã€‚è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†æ¥è‡ªåŠ æ‹‰heid Hospitalçš„ datasetï¼Œç”±Walid Al-Dhabyaniç­‰äººç¼–è¯‘ï¼Œå¼ºè°ƒäº†MobileNetV2åœ¨åŒ»ç–—å½±åƒä¸­çš„æ½œåŠ›ï¼Œ aspires to improve the diagnostic precision in oncologyã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜æ¢è®¨äº†åŸºäºStreamlitçš„å®æ—¶è¯Šæ–­éƒ¨ç½²ï¼Œå±•ç¤ºäº†MobileNetV2åœ¨åŒ»ç–—å½±åƒä¸­çš„å¯ç”¨æ€§ï¼Œå¹¶ä¸ºæœªæ¥åŒ»å­¦è¯Šæ–­ç ”ç©¶è®¾ç½®äº†benchmarkã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Open-set-Gesture-Recognition-via-Feature-Activation-Enhancement-and-Orthogonal-Prototype-Learning"><a href="#Towards-Open-set-Gesture-Recognition-via-Feature-Activation-Enhancement-and-Orthogonal-Prototype-Learning" class="headerlink" title="Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning"></a>Towards Open-set Gesture Recognition via Feature Activation Enhancement and Orthogonal Prototype Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02535">http://arxiv.org/abs/2312.02535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Liu, Can Han, Chengfeng Zhou, Crystal Cai, Suncheng Xiang, Hualiang Ni, Dahong Qian</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨è§£å†³äººæœºäº’åŠ¨ä¸­çš„æ‰‹åŠ¿è¯†åˆ« task ä¸­çš„ open set recognition (OSR) é—®é¢˜ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäº prototype learning (PL) çš„æ›´æœ‰æ•ˆçš„æ–¹æ³•ï¼Œåˆ©ç”¨ä¸¤ç§æ–°çš„è‡ªç„¶åˆ†å¸ƒç‰¹å¾ï¼Œç‰¹å¾æ´»åŒ–æ°´å¹³å’ŒæŠ•å½±ä¸ä¸€è‡´æ€§ï¼Œå¯¹äºå·²çŸ¥å’ŒæœªçŸ¥çš„åˆ†ç±»è¿›è¡Œæ›´å¥½çš„åˆ†åˆ«ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ç¯‡è®ºæ–‡çš„ææ¡ˆæ–¹æ³•å¯ä»¥åŒæ—¶å®ç°ç²¾ç¡®çš„å…³é—­é›†åˆè¯†åˆ«å’Œæœ‰æ•ˆåœ°æ‹’ç»æœªçŸ¥çš„æ‰‹åŠ¿è¯†åˆ«ã€‚<details>
<summary>Abstract</summary>
Gesture recognition is a foundational task in human-machine interaction (HMI). While there has been significant progress in gesture recognition based on surface electromyography (sEMG), accurate recognition of predefined gestures only within a closed set is still inadequate in practice. It is essential to effectively discern and reject unknown gestures of disinterest in a robust system. Numerous methods based on prototype learning (PL) have been proposed to tackle this open set recognition (OSR) problem. However, they do not fully explore the inherent distinctions between known and unknown classes. In this paper, we propose a more effective PL method leveraging two novel and inherent distinctions, feature activation level and projection inconsistency. Specifically, the Feature Activation Enhancement Mechanism (FAEM) widens the gap in feature activation values between known and unknown classes. Furthermore, we introduce Orthogonal Prototype Learning (OPL) to construct multiple perspectives. OPL acts to project a sample from orthogonal directions to maximize the distinction between its two projections, where unknown samples will be projected near the clusters of different known classes while known samples still maintain intra-class similarity. Our proposed method simultaneously achieves accurate closed-set classification for predefined gestures and effective rejection for unknown gestures. Extensive experiments demonstrate its efficacy and superiority in open-set gesture recognition based on sEMG.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> translate "Gesture recognition is a foundational task in human-machine interaction (HMI). While there has been significant progress in gesture recognition based on surface electromyography (sEMG), accurate recognition of predefined gestures only within a closed set is still inadequate in practice. It is essential to effectively discern and reject unknown gestures of disinterest in a robust system. Numerous methods based on prototype learning (PL) have been proposed to tackle this open set recognition (OSR) problem. However, they do not fully explore the inherent distinctions between known and unknown classes. In this paper, we propose a more effective PL method leveraging two novel and inherent distinctions, feature activation level and projection inconsistency. Specifically, the Feature Activation Enhancement Mechanism (FAEM) widens the gap in feature activation values between known and unknown classes. Furthermore, we introduce Orthogonal Prototype Learning (OPL) to construct multiple perspectives. OPL acts to project a sample from orthogonal directions to maximize the distinction between its two projections, where unknown samples will be projected near the clusters of different known classes while known samples still maintain intra-class similarity. Our proposed method simultaneously achieves accurate closed-set classification for predefined gestures and effective rejection for unknown gestures. Extensive experiments demonstrate its efficacy and superiority in open-set gesture recognition based on sEMG.">>Here's the translation:äººæœºäº¤äº’ï¼ˆHMIï¼‰ä¸­çš„æ‰‹åŠ¿è¯†åˆ«æ˜¯ä¸€é¡¹åŸºç¡€ä»»åŠ¡ã€‚è™½ç„¶åŸºäºè¡¨é¢ç”µ MYographyï¼ˆsEMGï¼‰çš„æ‰‹åŠ¿è¯†åˆ«å·²ç»å–å¾—äº† significi cant è¿›æ­¥ï¼Œä½†æ˜¯åªèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«é—­ SETçš„æ‰‹åŠ¿ï¼Œåœ¨å®é™…åº”ç”¨ä¸­è¿˜æ˜¯ä¸å¤Ÿã€‚å› æ­¤ï¼Œæœ‰æ•ˆåœ°åˆ†è¾¨å¹¶æ’é™¤æ— å…³çš„æ‰‹åŠ¿æ˜¯ä¸€é¡¹é‡è¦çš„éœ€æ±‚ã€‚å¤šç§åŸºäºåŸå‹å­¦ä¹ ï¼ˆPLï¼‰çš„æ–¹æ³•å·²ç»è¢«æè®®æ¥è§£å†³è¿™ä¸ªå¼€é›†è¯†åˆ«ï¼ˆOSRï¼‰é—®é¢˜ï¼Œä½†æ˜¯å®ƒä»¬å¹¶æ²¡æœ‰å……åˆ†åˆ©ç”¨æ‰‹åŠ¿ä¹‹é—´çš„å†…åœ¨å·®å¼‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ›´æœ‰æ•ˆçš„ PL æ–¹æ³•ï¼Œåˆ©ç”¨ä¸¤ç§æ–°çš„å†…åœ¨å·®å¼‚æ¥è¿›è¡Œåˆ†è¾¨ï¼šç‰¹å¾æ´»åŠ¨æ°´å¹³å’ŒæŠ•å½±ä¸ä¸€è‡´ã€‚specificallyï¼Œæˆ‘ä»¬æå‡ºäº†ç‰¹å¾æ´»åŠ¨å¢å¼ºæœºåˆ¶ï¼ˆFAEMï¼‰ï¼Œä½¿çŸ¥é“ç±»å’ŒæœªçŸ¥ç±»ä¹‹é—´çš„ç‰¹å¾æ´»åŠ¨å€¼å·®è·æ›´å¤§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†å¤šä¸ªè§†è§’å­¦ä¹ ï¼ˆOPLï¼‰ï¼Œä»¥ä»å¤šä¸ªæ–¹å‘æŠ•å½±æ ·æœ¬ï¼Œä»¥ maximize ä¸åŒç±»å‹çŸ¥é“æ ·æœ¬ä¹‹é—´çš„åˆ†è¾¨ã€‚æˆ‘ä»¬çš„æå‡ºçš„æ–¹æ³•åŒæ—¶å®ç°äº†é«˜ç²¾åº¦é—­ SET è¯†åˆ«å’Œæœ‰æ•ˆæ’é™¤æ— å…³çš„æ‰‹åŠ¿ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œä¼˜åŠ¿åœ¨åŸºäº sEMG çš„å¼€é›†æ‰‹åŠ¿è¯†åˆ«ä¸­ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Automatic-Power-Battery-Detection-New-Challenge-Benchmark-Dataset-and-Baseline"><a href="#Towards-Automatic-Power-Battery-Detection-New-Challenge-Benchmark-Dataset-and-Baseline" class="headerlink" title="Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline"></a>Towards Automatic Power Battery Detection: New Challenge, Benchmark Dataset and Baseline</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02528">http://arxiv.org/abs/2312.02528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoqi Zhao, Youwei Pang, Zhenyu Chen, Qian Yu, Lihe Zhang, Hanqi Liu, Jiaming Zuo, Huchuan Lu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§æ–°ä»»åŠ¡ named power battery detection (PBD), ç”¨äº lokalisieren dense cathode å’Œ anode plates ç«¯ç‚¹ä» X-ray å›¾åƒä¸­è¯„ä¼°ç”µæ± è´¨é‡ã€‚ç°æœ‰ç”Ÿäº§è€…é€šå¸¸é‡‡ç”¨äººå·¥è§‚å¯Ÿæ¥å®Œæˆ PBDï¼Œè¿™ä¼šå¢åŠ è¯†åˆ«ç‡å’Œæ•ˆç‡çš„å›°éš¾ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶å¸å¼•æ›´å¤šå…³æ³¨è¿™ä¸ªæœ‰æ„ä¹‰çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬é¦–å…ˆ elaborately æ”¶é›†äº†ä¸€ä¸ªæ•°æ®é›†ï¼Œcalled X-ray PBDï¼ŒåŒ…å« $1,500$ å¤šç§ X-ray å›¾åƒï¼Œé€‰è‡ª $5$ å®¶ç”Ÿäº§å•†çš„ $5,000$ ä¸ªç”µæ± ï¼Œå¹¶ä¸”åŒ…å« $7$ ç§è§†è§‰å¹²æ‰°ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ segmentation-based è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸º multi-dimensional collaborative network (MDCNet)ã€‚é€šè¿‡çº¿æ€§å’Œæ•°å­—é¢„æµ‹å™¨çš„ååŒä½œç”¨ï¼Œåˆ†å‰²åˆ†æ”¯ä¸­çš„ç‚¹ segmentation å¯ä»¥å¾—åˆ°æ”¹è¿›çš„Semantic å’Œ Detail ä¸¤ä¸ªæ–¹é¢çš„è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„è·ç¦»é€‚åº”maskç”Ÿæˆç­–ç•¥ï¼Œä»¥é€‚åº”ä¸åŒçš„æ¿é—´è´¨é‡å’Œåˆ†å¸ƒï¼Œä¸º MDCNet æä¾›ç¨³å®šçš„æŒ‡å¯¼ã€‚</li>
<li>results: æ— è®ºæ˜¯é€šè¿‡ corner detectionã€äººç¾¤è®¡æ•°æˆ–æ™®é€š&#x2F;å°ç‰©ä½“æ£€æµ‹ç­‰æ–¹æ³•ï¼Œæˆ‘ä»¬çš„ segmentation-based MDCNet éƒ½èƒ½å¤Ÿåœ¨ PBD ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ä»…æ¯”è¾ƒå…¶ä»–æ–¹æ³•æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ï¼Œè¿˜èƒ½å¤Ÿåœ¨ä¸åŒçš„ X-ray å›¾åƒä¸­ä¿æŒç¨³å®šçš„æ€§èƒ½ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†ä¸€äº›å¯èƒ½çš„éš¾ç‚¹å’Œæœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚ä»£ç å’Œæ•°æ®é›†å°†åœ¨ \href{<a target="_blank" rel="noopener" href="http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0%7D%7BX-ray">http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray</a> PBD} ä¸Šå…¬å¼€å‘å¸ƒã€‚<details>
<summary>Abstract</summary>
We conduct a comprehensive study on a new task named power battery detection (PBD), which aims to localize the dense cathode and anode plates endpoints from X-ray images to evaluate the quality of power batteries. Existing manufacturers usually rely on human eye observation to complete PBD, which makes it difficult to balance the accuracy and efficiency of detection. To address this issue and drive more attention into this meaningful task, we first elaborately collect a dataset, called X-ray PBD, which has $1,500$ diverse X-ray images selected from thousands of power batteries of $5$ manufacturers, with $7$ different visual interference. Then, we propose a novel segmentation-based solution for PBD, termed multi-dimensional collaborative network (MDCNet). With the help of line and counting predictors, the representation of the point segmentation branch can be improved at both semantic and detail aspects. Besides, we design an effective distance-adaptive mask generation strategy, which can alleviate the visual challenge caused by the inconsistent distribution density of plates to provide MDCNet with stable supervision. Without any bells and whistles, our segmentation-based MDCNet consistently outperforms various other corner detection, crowd counting and general/tiny object detection-based solutions, making it a strong baseline that can help facilitate future research in PBD. Finally, we share some potential difficulties and works for future researches. The source code and datasets will be publicly available at \href{http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray PBD}.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹å…¨é¢çš„ç ”ç©¶ï¼Œæ¢è®¨ä¸€ç§æ–°ä»»åŠ¡ named ç”µæ± ç”µèƒ½æ£€æµ‹ï¼ˆPBDï¼‰ï¼Œè¯¥ä»»åŠ¡çš„ç›®çš„æ˜¯ä»Xå°„çº¿å›¾åƒä¸­LOCALåŒ–ç¨ å¯†çš„é”‚ç”µæ¿å’Œé”‚ç”µæ¿ç«¯ç‚¹ã€‚ç°æœ‰åˆ¶é€ å•†é€šå¸¸é€šè¿‡äººå·¥è§‚å¯Ÿæ¥å®ŒæˆPBDï¼Œè¿™ä¼šå‡å°‘äº†å‡†ç¡®æ€§å’Œæ•ˆç‡çš„å¹³è¡¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å¹¶å¸å¼•æ›´å¤šå…³æ³¨è¿™ä¸€é‡è¦ä»»åŠ¡ï¼Œæˆ‘ä»¬å…ˆåœ¨Xå°„çº¿PBD datasetä¸­æ”¶é›†äº†1500ä¸ªå¤šæ ·åŒ–çš„Xå°„çº¿å›¾åƒï¼Œè¿™äº›å›¾åƒæ¥è‡ªäº5å®¶åˆ¶é€ å•†çš„5000ä¸ªç”µæ± ï¼Œå¹¶ä¸”åŒ…å«7ç§è§†è§‰å¹²æ‰°ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºåˆ†å‰²çš„è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸ºå¤šç»´ååŒç½‘ç»œï¼ˆMDCNetï¼‰ã€‚é€šè¿‡çº¿å’Œè®¡æ•°é¢„æµ‹å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ semanticå’Œç»†èŠ‚ä¸¤ä¸ªæ–¹é¢æé«˜ç‚¹ segmentation branchçš„è¡¨è¾¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†ä¸€ç§æœ‰æ•ˆçš„è·ç¦»é€‚åº”maskç”Ÿæˆç­–ç•¥ï¼Œä»¥é€‚åº”ç”µæ± æ¿çš„ä¸å‡åŒ€åˆ†å¸ƒï¼Œä¸ºMDCNetæä¾›ç¨³å®šçš„è¶…visã€‚ä¸éœ€è¦ä»»ä½•ç‚«æŠ€ï¼Œæˆ‘ä»¬çš„åˆ†å‰²åŸºäºMDCNetåœ¨å¤šç§å°–é”æ£€æµ‹ã€äººç¾¤è®¡æ•°å’Œé€šç”¨/å°ç‰©ä½“æ£€æµ‹åŸºç¡€ä¸Šå‡†ç¡®åœ°æ£€æµ‹ç”µæ± ç”µæ¿ï¼Œä»è€Œæˆä¸ºä¸€ä¸ªå¼ºå¤§çš„åŸºçº¿ï¼Œå¯ä»¥å¸®åŠ©æœªæ¥çš„PBDç ”ç©¶ã€‚æœ€åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›æ½œåœ¨çš„æŒ‘æˆ˜å’Œæœªæ¥ç ”ç©¶çš„å¯èƒ½æ€§ã€‚æˆ‘ä»¬å°†åœ¨\href{http://www.gy3000.company/x3000%e5%bc%80%e6%94%be%e5%b9%b3%e5%8f%b0}{X-ray PBD}ä¸Šå…¬å¼€æºä»£ç å’Œæ•°æ®é›†ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-More-Unified-In-context-Visual-Understanding"><a href="#Towards-More-Unified-In-context-Visual-Understanding" class="headerlink" title="Towards More Unified In-context Visual Understanding"></a>Towards More Unified In-context Visual Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02520">http://arxiv.org/abs/2312.02520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dianmo Sheng, Dongdong Chen, Zhentao Tan, Qiankun Liu, Qi Chu, Jianmin Bao, Tao Gong, Bin Liu, Shengwei Xu, Nenghai Yu</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§èƒ½å¤Ÿå¤„ç†å¤šmodalè¾“å‡ºçš„è§†è§‰ç†è§£ICLæ¡†æ¶ï¼Œä»¥æ‰©å±•ICLçš„åº”ç”¨åœºæ™¯ã€‚</li>
<li>methods: è¯¥æ¨¡å‹ä½¿ç”¨é‡åŒ–å¹¶åµŒå…¥æ–‡æœ¬å’Œè§†è§‰æç¤ºï¼Œå¹¶é‡‡ç”¨ä¸€ä¸ªå—é™çš„ sparse transformer æ¶æ„è¿›è¡Œç”Ÿæˆæ¨¡å‹åŒ–ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨å¤šmodalè¾“å‡ºè§†è§‰ç†è§£ä»»åŠ¡ä¸Šè¾¾åˆ°äº†ä¸ä¸“é—¨æ¨¡å‹å’Œå…ˆå‰ICLåŸºçº¿ç›¸å½“çš„ç«äº‰æ€§æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
The rapid advancement of large language models (LLMs) has accelerated the emergence of in-context learning (ICL) as a cutting-edge approach in the natural language processing domain. Recently, ICL has been employed in visual understanding tasks, such as semantic segmentation and image captioning, yielding promising results. However, existing visual ICL framework can not enable producing content across multiple modalities, which limits their potential usage scenarios. To address this issue, we present a new ICL framework for visual understanding with multi-modal output enabled. First, we quantize and embed both text and visual prompt into a unified representational space, structured as interleaved in-context sequences. Then a decoder-only sparse transformer architecture is employed to perform generative modeling on them, facilitating in-context learning. Thanks to this design, the model is capable of handling in-context vision understanding tasks with multimodal output in a unified pipeline. Experimental results demonstrate that our model achieves competitive performance compared with specialized models and previous ICL baselines. Overall, our research takes a further step toward unified multimodal in-context learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¿«é€Ÿå‘å±•çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å·²ç»æ¨åŠ¨äº†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå†…çš„å·ç§¯å­¦ä¹ ï¼ˆICLï¼‰æŠ€æœ¯çš„å‡ºç°ã€‚ç›®å‰ï¼ŒICLæŠ€æœ¯å·²ç»åœ¨è§†è§‰ç†è§£ä»»åŠ¡ä¸­ï¼Œå¦‚ semantic segmentation å’Œå›¾åƒæè¿°ï¼Œè·å¾—äº†æœ‰åˆ©çš„ç»“æœã€‚ç„¶è€Œï¼Œç°æœ‰çš„è§†è§‰ ICLL æ¡†æ¶ä¸èƒ½ç”Ÿæˆè·¨Modalitiesçš„å†…å®¹ï¼Œè¿™é™åˆ¶äº†å…¶åº”ç”¨åœºæ™¯ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ ICLL æ¡†æ¶ Ğ´Ğ»Ñè§†è§‰ç†è§£ï¼Œå…·æœ‰å¤šModalitiesè¾“å‡ºèƒ½åŠ›ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ–‡æœ¬å’Œè§†è§‰æç¤ºç¼–ç å¹¶åµŒå…¥åˆ°ä¸€ä¸ªå…±åŒè¡¨ç¤ºç©ºé—´ä¸­ï¼Œå³åµŒå…¥å¼çš„å·ç§¯åºåˆ—ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª sparse transformer æ¶æ„æ¥è¿›è¡Œç”Ÿæˆæ¨¡å‹ï¼Œä»¥ä¾¿åœ¨å·ç§¯åºåˆ—ä¸­è¿›è¡Œå·ç§¯å­¦ä¹ ã€‚ç”±äºè¿™ç§è®¾è®¡ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å¤„ç†å·ç§¯è§†è§‰ç†è§£ä»»åŠ¡ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸€ä¸ªç»Ÿä¸€çš„ç®¡é“ä¸­ç”Ÿæˆå¤šModalitiesçš„è¾“å‡ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä¸ä¸“é—¨çš„æ¨¡å‹å’Œå‰ä¸€ä»£ ICLL åŸºçº¿é›†æˆæ¯”è‚©ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬çš„ç ”ç©¶åˆä¸€æ­¥å‘å¤šModalitieså·ç§¯å­¦ä¹ çš„ç»Ÿä¸€å‘å±•ã€‚
</details></li>
</ul>
<hr>
<h2 id="SAVE-Protagonist-Diversification-with-Structure-Agnostic-Video-Editing"><a href="#SAVE-Protagonist-Diversification-with-Structure-Agnostic-Video-Editing" class="headerlink" title="SAVE: Protagonist Diversification with Structure Agnostic Video Editing"></a>SAVE: Protagonist Diversification with Structure Agnostic Video Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02503">http://arxiv.org/abs/2312.02503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeji Song, Wonsik Shin, Junsoo Lee, Jeesoo Kim, Nojun Kwak</li>
<li>for: æé«˜è§†é¢‘ç¼–è¾‘çš„å¤šæ ·æ€§å’Œå¯Scalability</li>
<li>methods: é‡‡ç”¨åŠ¨ä½œä¸ªæ€§åŒ–ã€æ–‡æœ¬åµŒå…¥ã€pseudo optical flowç­‰æŠ€æœ¯</li>
<li>results: å®ç°äº†å¯¹è§†é¢‘ä¸­äººç‰©çš„ä¿®æ”¹å’Œé£æ ¼å˜æ¢ï¼Œæé«˜äº†è§†é¢‘ç¼–è¾‘çš„å¤šæ ·æ€§å’Œå¯Scalability<details>
<summary>Abstract</summary>
Driven by the upsurge progress in text-to-image (T2I) generation models, text-to-video (T2V) generation has experienced a significant advance as well. Accordingly, tasks such as modifying the object or changing the style in a video have been possible. However, previous works usually work well on trivial and consistent shapes, and easily collapse on a difficult target that has a largely different body shape from the original one. In this paper, we spot the bias problem in the existing video editing method that restricts the range of choices for the new protagonist and attempt to address this issue using the conventional image-level personalization method. We adopt motion personalization that isolates the motion from a single source video and then modifies the protagonist accordingly. To deal with the natural discrepancy between image and video, we propose a motion word with an inflated textual embedding to properly represent the motion in a source video. We also regulate the motion word to attend to proper motion-related areas by introducing a novel pseudo optical flow, efficiently computed from the pre-calculated attention maps. Finally, we decouple the motion from the appearance of the source video with an additional pseudo word. Extensive experiments demonstrate the editing capability of our method, taking a step toward more diverse and extensive video editing.
</details>
<details>
<summary>æ‘˜è¦</summary>
é©±åŠ¨äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰ç”Ÿæˆæ¨¡å‹çš„å¿«é€Ÿè¿›æ­¥ï¼Œæ–‡æœ¬åˆ°è§†é¢‘ï¼ˆT2Vï¼‰ç”Ÿæˆä¹Ÿç»å†äº†æ˜¾è‘—æ”¹è¿›ã€‚å› æ­¤ï¼Œä¿®æ”¹è§†é¢‘ä¸­çš„å¯¹è±¡æˆ–æ”¹å˜é£æ ¼ä¹Ÿå˜å¾—å¯èƒ½ã€‚ç„¶è€Œï¼Œå…ˆå‰çš„å·¥ä½œé€šå¸¸åªèƒ½åœ¨ç®€å•å’Œä¸€è‡´çš„å½¢çŠ¶ä¸‹å·¥ä½œï¼Œå¯¹äºå…·æœ‰å¤§é‡ä¸åŒçš„èº«ä½“å½¢æ€çš„ç›®æ ‡æ¥è¯´ï¼Œå®¹æ˜“å´©æºƒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ç°æœ‰è§†é¢‘ç¼–è¾‘æ–¹æ³•ä¸­çš„åè§é—®é¢˜ï¼Œå¹¶å°è¯•ä½¿ç”¨ä¼ ç»Ÿçš„å›¾åƒçº§ä¸ªæ€§åŒ–æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬é‡‡ç”¨äº†è¿åŠ¨ä¸ªæ€§åŒ–ï¼Œå³ä»å•ä¸ªæºè§†é¢‘ä¸­éš”ç¦»å‡ºè¿åŠ¨ï¼Œç„¶åæ ¹æ®æ–°çš„ä¸»è§’è¿›è¡Œä¿®æ”¹ã€‚ä¸ºäº†å¤„ç†è‡ªç„¶çš„å›¾åƒå’Œè§†é¢‘ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¸¦æœ‰è†¨èƒ€çš„æ–‡æœ¬åµŒå…¥æ¥æ­£ç¡®è¡¨ç¤ºæºè§†é¢‘ä¸­çš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸€ç§æ–°çš„ Pseudo è¿åŠ¨æµï¼Œé€šè¿‡è®¡ç®—å…ˆå‰è®¡ç®—çš„æ³¨æ„åŠ›å›¾æ¥æœ‰æ•ˆåœ°Compute Pseudo è¿åŠ¨æµã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿åŠ¨ä¸æºè§†é¢‘çš„å¤–è§‚åˆ†ç¦»å¼€æ¥ï¼Œä½¿ç”¨ä¸€ä¸ªé¢å¤–çš„ Pseudo è¯æ¥è¡¨ç¤ºè¿åŠ¨ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¿›è¡Œå¤šæ ·åŒ–å’Œå¹¿æ³›çš„è§†é¢‘ç¼–è¾‘ã€‚
</details></li>
</ul>
<hr>
<h2 id="ReconU-Net-a-direct-PET-image-reconstruction-using-U-Net-architecture-with-back-projection-induced-skip-connection"><a href="#ReconU-Net-a-direct-PET-image-reconstruction-using-U-Net-architecture-with-back-projection-induced-skip-connection" class="headerlink" title="ReconU-Net: a direct PET image reconstruction using U-Net architecture with back projection-induced skip connection"></a>ReconU-Net: a direct PET image reconstruction using U-Net architecture with back projection-induced skip connection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02494">http://arxiv.org/abs/2312.02494</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fumio Hashimoto, Kibo Ote<br>for:* è¿™ä¸ªç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç›´æ¥ positronå‘å°„tomographyï¼ˆPETï¼‰å›¾åƒé‡å»ºç®—æ³•ï¼Œå³ReconU-Netï¼Œä»¥æé«˜ç›´æ¥PETå›¾åƒé‡å»ºçš„ç²¾åº¦ã€‚methods:* è¯¥ç®—æ³•ç‹¬ç‰¹åœ°å°†ç‰©ç†æ¨¡å‹çš„åæŠ•å½±æ“ä½œintegrated into skip connectionï¼Œä»è€Œä½¿å¾—å«æœ‰ç‰©ç†æ¨¡å‹çš„ skip connectionå¯ä»¥æœ‰æ•ˆåœ°ä¼ é€’åŸå§‹ç©ºé—´ä¿¡æ¯ä»è¾“å…¥çš„sinogramåˆ°é‡å»ºçš„å›¾åƒä¸­ã€‚results:* æ¯”è¾ƒå…¶ä»–æ—  skip connectionsçš„encoder-decoderæ¶æ„ï¼Œææ¡ˆçš„ReconU-Netæ–¹æ³•å¯ä»¥ç”Ÿæˆå…·æœ‰æ›´é«˜ç²¾åº¦çš„é‡å»ºå›¾åƒã€‚* è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼ŒReconU-Netå¯ä»¥åœ¨ skip connectionsä¸­ä¼ é€’å¤šä¸ªåˆ†è¾¨ç‡çš„ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯éæŠ½è±¡é«˜åˆ†è¾¨ç‡ä¿¡æ¯ã€‚* è™½ç„¶å—é™äºå°è®­ç»ƒæ•°æ®ï¼Œä½†ææ¡ˆçš„ReconU-Netæ–¹æ³•å¯ä»¥æˆåŠŸé‡å»ºå®é™… Hoffmanå¤§è„‘phantomæ•°æ®ï¼Œè€Œå…¶ä»–æ·±åº¦å­¦ä¹ åŸºäºç›´æ¥é‡å»ºæ–¹æ³•åˆ™æ— æ³•ç”Ÿæˆé‡å»ºå›¾åƒã€‚<details>
<summary>Abstract</summary>
[Objective] This study aims to introduce a novel back projection-induced U-Net-shaped architecture, called ReconU-Net, for deep learning-based direct positron emission tomography (PET) image reconstruction. Additionally, our objective is to analyze the behavior of direct PET image reconstruction and gain deeper insights by comparing the proposed ReconU-Net architecture with other encoder-decoder architectures without skip connections. [Approach] The proposed ReconU-Net architecture uniquely integrates the physical model of the back projection operation into the skip connection. This distinctive feature facilitates the effective transfer of intrinsic spatial information from the input sinogram to the reconstructed image via an embedded physical model. The proposed ReconU-Net was trained using Monte Carlo simulation data from the Brainweb phantom and tested on both simulated and real Hoffman brain phantom data. [Main results] The proposed ReconU-Net method generated a reconstructed image with a more accurate structure compared to other deep learning-based direct reconstruction methods. Further analysis showed that the proposed ReconU-Net architecture has the ability to transfer features of multiple resolutions, especially non-abstract high-resolution information, through skip connections. Despite limited training on simulated data, the proposed ReconU-Net successfully reconstructed the real Hoffman brain phantom, unlike other deep learning-based direct reconstruction methods, which failed to produce a reconstructed image. [Significance] The proposed ReconU-Net can improve the fidelity of direct PET image reconstruction, even when dealing with small training datasets, by leveraging the synergistic relationship between data-driven modeling and the physics model of the imaging process.
</details>
<details>
<summary>æ‘˜è¦</summary>
[ç›®æ ‡] æœ¬ç ”ç©¶æ—¨åœ¨ä»‹ç»ä¸€ç§æ–°çš„åæŠ•å½±å¼•å…¥U-Netå‹æ¶æ„ï¼Œç§°ä¸ºReconU-Netï¼Œç”¨äºæ·±åº¦å­¦ä¹ åŸºäºç›´æ¥èåˆTomographyï¼ˆPETï¼‰å›¾åƒé‡å»ºã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æƒ³è¦é€šè¿‡æ¯”è¾ƒä¸å« skip connection çš„æ¶æ„å’ŒReconU-Netæ¶æ„è¿›è¡Œåˆ†æï¼Œä»è€Œæ›´æ·±å…¥åœ°ç†è§£ direct PET å›¾åƒé‡å»ºçš„è¡Œä¸ºã€‚[æ–¹æ³•] ReconU-Net æ¶æ„ç‹¬ç‰¹åœ°å°†ç‰©ç†æ¨¡å‹çš„åæŠ•å½±æ“ä½œintegrated into skip connectionï¼Œè¿™ç§ç‰¹æ®Šçš„ç‰¹ç‚¹ä½¿å¾—æŠŠè¾“å…¥çš„sinogramä¸­çš„å†…åœ¨ç©ºé—´ä¿¡æ¯æœ‰æ•ˆåœ°ä¼ é€’åˆ°é‡å»ºçš„å›¾åƒä¸­ã€‚ReconU-Net ä½¿ç”¨ Monte Carlo ä»¿çœŸæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ simulate å’Œå®é™… Hoffman è„‘éƒ¨phantomæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ã€‚[ä¸»è¦ç»“æœ] ReconU-Net æ–¹æ³•ç”Ÿæˆçš„é‡å»ºå›¾åƒå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ç»“æ„ï¼Œç›¸æ¯”å…¶ä»–æ·±åº¦å­¦ä¹ åŸºäºç›´æ¥é‡å»ºæ–¹æ³•ã€‚è¿›ä¸€æ­¥çš„åˆ†æè¡¨æ˜ï¼ŒReconU-Net æ¶æ„æœ‰èƒ½åŠ›ä¼ é€’å¤šresolutionçš„ç‰¹å¾ï¼Œç‰¹åˆ«æ˜¯éæŠ½è±¡é«˜åˆ†è¾¨ç‡ä¿¡æ¯ï¼Œé€šè¿‡ skip connectionã€‚å°½ç®¡å®ƒåªå—é™äºå°è§„æ¨¡çš„è®­ç»ƒæ•°æ®ï¼ŒReconU-Net ä»ç„¶æˆåŠŸåœ°é‡å»ºäº†å®é™… Hoffman è„‘éƒ¨phantomï¼Œä¸å…¶ä»–æ·±åº¦å­¦ä¹ åŸºäºç›´æ¥é‡å»ºæ–¹æ³•ä¸åŒï¼Œå…¶ä»–æ–¹æ³•æ— æ³•ç”Ÿæˆé‡å»ºå›¾åƒã€‚[æ„ä¹‰] ReconU-Net å¯ä»¥é€šè¿‡åˆ©ç”¨æ•°æ®é©±åŠ¨æ¨¡å‹å’Œæˆåƒè¿‡ç¨‹çš„ç‰©ç†æ¨¡å‹ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼Œæé«˜ç›´æ¥ PET å›¾åƒé‡å»ºçš„å‡†ç¡®æ€§ï¼Œå³ä½¿é¢ä¸´å°è§„æ¨¡çš„è®­ç»ƒæ•°æ®ã€‚
</details></li>
</ul>
<hr>
<h2 id="EtC-Temporal-Boundary-Expand-then-Clarify-for-Weakly-Supervised-Video-Grounding-with-Multimodal-Large-Language-Model"><a href="#EtC-Temporal-Boundary-Expand-then-Clarify-for-Weakly-Supervised-Video-Grounding-with-Multimodal-Large-Language-Model" class="headerlink" title="EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model"></a>EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02483">http://arxiv.org/abs/2312.02483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guozhang Li, Xinpeng Ding, De Cheng, Jie Li, Nannan Wang, Xinbo Gao</li>
<li>for: æé«˜ Early weakly supervised video grounding (WSVG) æ–¹æ³•å¯¹äº incomplete boundary detection çš„èƒ½åŠ›ã€‚</li>
<li>methods: ä½¿ç”¨ explicit-supervision æ–¹æ³•ï¼Œç”Ÿæˆ pseudo-temporal boundaries for trainingï¼Œå¹¶é€šè¿‡ data augmentations å¢åŠ æ›´å¤šçš„ valuable informationã€‚</li>
<li>results: æå‡ºäº†ä¸€ç§æ–°çš„ perspectiveï¼Œå¯ä»¥ maintain the integrity of the original temporal content while introducing more valuable information for expanding the incomplete boundariesï¼Œå¹¶ä¸”é€šè¿‡ multimodal large language models (MLLMs) å¯¹æ¯å¸§å†… initial pseudo boundaries è¿›è¡Œæ›´å¤šçš„æè¿°ï¼Œä»¥è·å¾—æ›´ç²¾ç¡®çš„ boundariesã€‚<details>
<summary>Abstract</summary>
Early weakly supervised video grounding (WSVG) methods often struggle with incomplete boundary detection due to the absence of temporal boundary annotations. To bridge the gap between video-level and boundary-level annotation, explicit-supervision methods, i.e., generating pseudo-temporal boundaries for training, have achieved great success. However, data augmentations in these methods might disrupt critical temporal information, yielding poor pseudo boundaries. In this paper, we propose a new perspective that maintains the integrity of the original temporal content while introducing more valuable information for expanding the incomplete boundaries. To this end, we propose EtC (Expand then Clarify), first use the additional information to expand the initial incomplete pseudo boundaries, and subsequently refine these expanded ones to achieve precise boundaries. Motivated by video continuity, i.e., visual similarity across adjacent frames, we use powerful multimodal large language models (MLLMs) to annotate each frame within initial pseudo boundaries, yielding more comprehensive descriptions for expanded boundaries. To further clarify the noise of expanded boundaries, we combine mutual learning with a tailored proposal-level contrastive objective to use a learnable approach to harmonize a balance between incomplete yet clean (initial) and comprehensive yet noisy (expanded) boundaries for more precise ones. Experiments demonstrate the superiority of our method on two challenging WSVG datasets.
</details>
<details>
<summary>æ‘˜è¦</summary>
Early weakly supervised video grounding (WSVG) methods often struggle with incomplete boundary detection due to the absence of temporal boundary annotations. To bridge the gap between video-level and boundary-level annotation, explicit-supervision methods, i.e., generating pseudo-temporal boundaries for training, have achieved great success. However, data augmentations in these methods might disrupt critical temporal information, yielding poor pseudo boundaries. In this paper, we propose a new perspective that maintains the integrity of the original temporal content while introducing more valuable information for expanding the incomplete boundaries. To this end, we propose EtC (Expand then Clarify), first use the additional information to expand the initial incomplete pseudo boundaries, and subsequently refine these expanded ones to achieve precise boundaries. Motivated by video continuity, i.e., visual similarity across adjacent frames, we use powerful multimodal large language models (MLLMs) to annotate each frame within initial pseudo boundaries, yielding more comprehensive descriptions for expanded boundaries. To further clarify the noise of expanded boundaries, we combine mutual learning with a tailored proposal-level contrastive objective to use a learnable approach to harmonize a balance between incomplete yet clean (initial) and comprehensive yet noisy (expanded) boundaries for more precise ones. Experiments demonstrate the superiority of our method on two challenging WSVG datasets.
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Point-based-Inverse-Rendering"><a href="#Differentiable-Point-based-Inverse-Rendering" class="headerlink" title="Differentiable Point-based Inverse Rendering"></a>Differentiable Point-based Inverse Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02480">http://arxiv.org/abs/2312.02480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hoon-Gyu Chung, Seokjun Choi, Seung-Hwan Baek</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å®ç°åˆ†æï¿½ï¿½ï¿½renderingï¼Œä»¥ä¼°è®¡å›¾åƒä¸‹å¤šç§ç…§æ˜æ¡ä»¶ä¸‹çš„å½¢çŠ¶å’Œç©ºé—´åˆ†å¸ƒ BRDFã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ç‚¹åŸº Renderingï¼Œæ¶ˆé™¤äº†å¤šä¸ªæ ·æœ¬æ¢æµ‹æ¯ä¸ªå…‰æŸçš„éœ€è¦ï¼Œä»è€Œå¤§å¹…æé«˜åå‘æ¸²æŸ“çš„é€Ÿåº¦ã€‚å¦å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ··åˆç‚¹-ç§¯åˆ†è¡¨ç¤ºæ³•ï¼Œä»¥ä¿ç•™ SDF-based è¡¨ç¤ºæ³•ä¸­çš„å‡ ä½•ç»†èŠ‚å’Œç¨³å®šæ€§ã€‚</li>
<li>results: æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDPIR å¯ä»¥æ¯”ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œåœ¨é‡å»ºç²¾åº¦ã€è®¡ç®—æ•ˆç‡å’Œå†…å­˜å ç”¨æ–¹é¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç›´è§‚ç‚¹åŸºè¡¨ç¤ºå’Œæ¸²æŸ“å¯ä»¥å¸¦æ¥ç›´è§‚çš„å‡ ä½•å’Œåå°„Editingã€‚<details>
<summary>Abstract</summary>
We present differentiable point-based inverse rendering, DPIR, an analysis-by-synthesis method that processes images captured under diverse illuminations to estimate shape and spatially-varying BRDF. To this end, we adopt point-based rendering, eliminating the need for multiple samplings per ray, typical of volumetric rendering, thus significantly enhancing the speed of inverse rendering. To realize this idea, we devise a hybrid point-volumetric representation for geometry and a regularized basis-BRDF representation for reflectance. The hybrid geometric representation enables fast rendering through point-based splatting while retaining the geometric details and stability inherent to SDF-based representations. The regularized basis-BRDF mitigates the ill-posedness of inverse rendering stemming from limited light-view angular samples. We also propose an efficient shadow detection method using point-based shadow map rendering. Our extensive evaluations demonstrate that DPIR outperforms prior works in terms of reconstruction accuracy, computational efficiency, and memory footprint. Furthermore, our explicit point-based representation and rendering enables intuitive geometry and reflectance editing. The code will be publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ä»‹ç»äº†å·®åˆ†å¯å¯¼ç‚¹ cloud  inverse renderingï¼ˆDPIRï¼‰ï¼Œè¿™æ˜¯ä¸€ç§åˆ†æï¿½ BY  Synthesis æ–¹æ³•ï¼Œç”¨äºä»å¤šç§ç…§æ˜æ¡ä»¶ä¸‹æ•è·çš„å›¾åƒä¸­ä¼°ç®—å½¢çŠ¶å’Œç©ºé—´åˆ†å¸ƒå¼ BRDFã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é‡‡ç”¨ç‚¹ cloud æ¸²æŸ“ï¼Œä»è€Œæ¶ˆé™¤äº†é€šå¸¸éœ€è¦å¤šä¸ªæŠ½è±¡ï¿½ayer per rayçš„æ¶²ä½“æ¸²æŸ“çš„éœ€è¦ï¼Œä»è€Œæœ‰æ•ˆæé«˜åå‘æ¸²æŸ“çš„é€Ÿåº¦ã€‚ä¸ºå®ç°è¿™ä¸€æƒ³æ³•ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§æ··åˆç‚¹ cloud å’ŒVolume è¡¨ç¤ºæ³• Ğ´Ğ»Ñgeometryï¼Œä»¥åŠä¸€ç§å‡å°‘åŸºé¢-BRDF è¡¨ç¤ºæ³•æ¥æŠ‘åˆ¶åå‘æ¸²æŸ“ä¸­çš„ç¼ºå¤±çº¦æŸã€‚è¿™ç§æ··åˆçš„è¡¨ç¤ºæ³•ä½¿å¾—å¿«é€Ÿæ¸²æŸ“å¯ä»¥é€šè¿‡ç‚¹ cloud æ‹¼æ¥æ¥å®ç°ï¼ŒåŒæ—¶ä¿ç•™SDF-basedè¡¨ç¤ºæ³•ä¸­çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„é˜´å½±æ£€æµ‹æ–¹æ³•ï¼Œä½¿ç”¨ç‚¹ cloud é˜´å½±å›¾ renderingã€‚æˆ‘ä»¬çš„å¹¿æ³›è¯„ä¼°è¡¨æ˜ï¼ŒDPIR åœ¨é‡å»ºç²¾åº¦ã€è®¡ç®—æ•ˆç‡å’Œå­˜å‚¨å ç”¨ä¸Šéƒ½è¶…è¿‡äº†å…ˆå‰çš„å·¥ä½œã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç‚¹ cloud è¡¨ç¤ºå’Œæ¸²æŸ“å…è®¸ç›´è§‚åœ°ç¼–è¾‘geometryå’Œ Reflectanceã€‚æˆ‘ä»¬çš„ä»£ç å°†å…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Generator-Born-from-Classifier"><a href="#Generator-Born-from-Classifier" class="headerlink" title="Generator Born from Classifier"></a>Generator Born from Classifier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02470">http://arxiv.org/abs/2312.02470</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runpeng Yu, Xinchao Wang</li>
<li>for:  Given a pre-trained classifier, the paper aims to reconstruct an image generator without using any data samples.</li>
<li>methods: The proposed method leverages the knowledge encapsulated within the parameters of the neural network and uses a novel learning paradigm that trains the generator to ensure convergence conditions of the network parameters are satisfied over the generated distribution of samples.</li>
<li>results: Empirical validation from various image generation tasks demonstrates the efficacy of the proposed strategy.Hereâ€™s the simplified Chinese text:</li>
<li>for: ç»™ä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæœ¬æ–‡å°è¯•åšä¸€ä»¶å¥”æ³¢çš„ä»»åŠ¡ï¼šæ— éœ€ä½¿ç”¨ä»»ä½•æ•°æ®æ ·æœ¬ï¼Œé‡å»ºä¸€ä¸ªå›¾åƒç”Ÿæˆå™¨ã€‚</li>
<li>methods: è¯¥æ–¹æ³•å€Ÿé‰´ç¥ç»ç½‘ç»œå‚æ•°ä¸­å°è£…çš„çŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡è®©ç”Ÿæˆå™¨ç¡®ä¿ç½‘ç»œå‚æ•°çš„å åŠ æ¡ä»¶åœ¨ç”Ÿæˆåˆ†å¸ƒä¸­æ»¡è¶³ï¼Œæ¥è®©ç½‘ç»œå‚æ•°å¾—åˆ°æœ€å¤§marginçš„åå¥½ã€‚</li>
<li>results: ä»å¤šç§å›¾åƒç”Ÿæˆä»»åŠ¡çš„å®éªŒ validateäº†è¯¥ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚<details>
<summary>Abstract</summary>
In this paper, we make a bold attempt toward an ambitious task: given a pre-trained classifier, we aim to reconstruct an image generator, without relying on any data samples. From a black-box perspective, this challenge seems intractable, since it inevitably involves identifying the inverse function for a classifier, which is, by nature, an information extraction process. As such, we resort to leveraging the knowledge encapsulated within the parameters of the neural network. Grounded on the theory of Maximum-Margin Bias of gradient descent, we propose a novel learning paradigm, in which the generator is trained to ensure that the convergence conditions of the network parameters are satisfied over the generated distribution of the samples. Empirical validation from various image generation tasks substantiates the efficacy of our strategy.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°è¯•äº†ä¸€é¡¹å¤§æ¢¦æƒ³çš„ä»»åŠ¡ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œæ— éœ€ä»»ä½•æ•°æ®æ ·æœ¬ï¼Œé‡å»ºä¸€ä¸ªå›¾åƒç”Ÿæˆå™¨ã€‚ä»é»‘ç›’è§’åº¦æ¥çœ‹ï¼Œè¿™ç¡®å®æ˜¯ä¸€é¡¹ä¸å¯èƒ½å®ç°çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒæ¶‰åŠåˆ°æ‰¾åˆ°åˆ†ç±»å™¨çš„é€†å‡½æ•°ï¼Œè¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„ä¿¡æ¯æŠ½å–è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å€ŸåŠ©ç¥ç»ç½‘ç»œå‚æ•°ä¸­åµŒå…¥çš„çŸ¥è¯†ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å­¦ä¹ æ–¹æ³•ã€‚åŸºäºæå¤§è·ç¦»æŠ‘åˆ¶æ³•çš„æ¢¯åº¦ä¸‹é™ç†è®ºï¼Œæˆ‘ä»¬è®­ç»ƒç”Ÿæˆå™¨ï¼Œä½¿å…¶åœ¨ç”Ÿæˆæ ·æœ¬åˆ†å¸ƒä¸­æ»¡è¶³ç½‘ç»œå‚æ•°çš„æ•´åˆæ¡ä»¶ã€‚å®é™… validate from various image generation tasks è¯æ˜äº†æˆ‘ä»¬çš„ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-Energy-based-Model-via-Dual-MCMC-Teaching"><a href="#Learning-Energy-based-Model-via-Dual-MCMC-Teaching" class="headerlink" title="Learning Energy-based Model via Dual-MCMC Teaching"></a>Learning Energy-based Model via Dual-MCMC Teaching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02469">http://arxiv.org/abs/2312.02469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiali Cui, Tian Han</li>
<li>for: æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºèƒ½é‡æ¨¡å‹ï¼ˆEBMï¼‰çš„åŸºæœ¬å­¦ä¹ é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†æœ€å¤§ LIKElihoodä¼°è®¡ï¼ˆMLEï¼‰å’ŒMarkov Chain Monte Carloï¼ˆMCMCï¼‰æ¢ç´¢EBMçš„å­¦ä¹ æ–¹æ³•ï¼Œå¹¶è€ƒè™‘äº†å°†ç”Ÿæˆå™¨æ¨¡å‹ä½œä¸ºè¡¥å……æ¨¡å‹ï¼Œä»¥ä½¿MCMCæ¢ç´¢æ›´åŠ ç¨³å®šã€‚</li>
<li>results: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç»“åˆMLEå’ŒMCMCæ¢ç´¢EBMçš„æ¡†æ¶ï¼Œé€šè¿‡å°†ç”Ÿæˆå™¨æ¨¡å‹ä½œä¸ºEBMçš„è¡¥å……æ¨¡å‹ï¼Œä½¿MCMCæ¢ç´¢æ›´åŠ ç¨³å®šã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨MCMC posterior samplingå’Œè¡¥å……æ¢ç´¢æ¨¡å‹æ¥å®ç°æœ‰æ•ˆå’Œé«˜æ•ˆçš„EBMå­¦ä¹ æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
This paper studies the fundamental learning problem of the energy-based model (EBM). Learning the EBM can be achieved using the maximum likelihood estimation (MLE), which typically involves the Markov Chain Monte Carlo (MCMC) sampling, such as the Langevin dynamics. However, the noise-initialized Langevin dynamics can be challenging in practice and hard to mix. This motivates the exploration of joint training with the generator model where the generator model serves as a complementary model to bypass MCMC sampling. However, such a method can be less accurate than the MCMC and result in biased EBM learning. While the generator can also serve as an initializer model for better MCMC sampling, its learning can be biased since it only matches the EBM and has no access to empirical training examples. Such biased generator learning may limit the potential of learning the EBM. To address this issue, we present a joint learning framework that interweaves the maximum likelihood learning algorithm for both the EBM and the complementary generator model. In particular, the generator model is learned by MLE to match both the EBM and the empirical data distribution, making it a more informative initializer for MCMC sampling of EBM. Learning generator with observed examples typically requires inference of the generator posterior. To ensure accurate and efficient inference, we adopt the MCMC posterior sampling and introduce a complementary inference model to initialize such latent MCMC sampling. We show that three separate models can be seamlessly integrated into our joint framework through two (dual-) MCMC teaching, enabling effective and efficient EBM learning.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints"><a href="#SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints" class="headerlink" title="SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints"></a>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02464">http://arxiv.org/abs/2312.02464</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sstary/ssrs">https://github.com/sstary/ssrs</a></li>
<li>paper_authors: Xianping Ma, Qianqian Wu, Xingyu Zhao, Xiaokang Zhang, Man-On Pun, Bo Huang<br>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æé«˜é¥æµ‹å›¾åƒSemantic Segmentationçš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œå¹¶ä½¿ç”¨Segment Anything Modelï¼ˆSAMï¼‰æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸¤ä¸ªæ–°çš„æ¦‚å¿µï¼Œå³SAM-Generated Objectï¼ˆSGOï¼‰å’ŒSAM-Generated Boundaryï¼ˆSGBï¼‰ï¼Œä»¥åŠä¸¤ä¸ªæ–°çš„æŸå¤±å‡½æ•°ï¼šobject losså’Œboundary lossï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–Semantic Segmentationæ¨¡å‹ã€‚results: æœ¬æ–‡åœ¨ä¸¤ä¸ªçŸ¥åçš„æ•°æ®é›†ï¼ŒISPRS Vaihingenå’ŒLoveDA Urbanä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºäº†æé«˜äº†Semantic Segmentationçš„ç²¾åº¦å’Œæ•ˆç‡ã€‚<details>
<summary>Abstract</summary>
Semantic segmentation of remote sensing imagery plays a pivotal role in extracting precise information for diverse down-stream applications. Recent development of the Segment Anything Model (SAM), an advanced general-purpose segmentation model, has revolutionized this field, presenting new avenues for accurate and efficient segmentation. However, SAM is limited to generating segmentation results without class information. Consequently, the utilization of such a powerful general vision model for semantic segmentation in remote sensing images has become a focal point of research. In this paper, we present a streamlined framework aimed at leveraging the raw output of SAM by exploiting two novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary (SGB). More specifically, we propose a novel object loss and further introduce a boundary loss as augmentative components to aid in model optimization in a general semantic segmentation framework. Taking into account the content characteristics of SGO, we introduce the concept of object consistency to leverage segmented regions lacking semantic information. By imposing constraints on the consistency of predicted values within objects, the object loss aims to enhance semantic segmentation performance. Furthermore, the boundary loss capitalizes on the distinctive features of SGB by directing the model's attention to the boundary information of the object. Experimental results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness of our proposed method. The source code for this work will be accessible at https://github.com/sstary/SSRS.
</details>
<details>
<summary>æ‘˜è¦</summary>
remote sensing å›¾åƒ semantic segmentation å¯¹äºæå–ç²¾å‡†ä¿¡æ¯è€Œè¨€æ˜¯å…³é”®ã€‚ recent development of the Segment Anything Model (SAM) å·²ç» revolutionized this field, offering new opportunities for accurate and efficient segmentation. However, SAM only generates segmentation results without class information. As a result, utilizing such a powerful general vision model for semantic segmentation in remote sensing images has become a research focus. In this paper, we present a streamlined framework that leverages the raw output of SAM by introducing two novel concepts: SAM-Generated Object (SGO) and SAM-Generated Boundary (SGB). Specifically, we propose a novel object loss and introduce a boundary loss as augmentative components to aid in model optimization in a general semantic segmentation framework. Considering the content characteristics of SGO, we introduce the concept of object consistency to enhance semantic segmentation performance. By imposing constraints on the consistency of predicted values within objects, the object loss aims to enhance semantic segmentation performance. Furthermore, the boundary loss capitalizes on the distinctive features of SGB by directing the model's attention to the boundary information of the object. Experimental results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban, demonstrate the effectiveness of our proposed method. The source code for this work will be accessible at https://github.com/sstary/SSRS.
</details></li>
</ul>
<hr>
<h2 id="DreamVideo-High-Fidelity-Image-to-Video-Generation-with-Image-Retention-and-Text-Guidance"><a href="#DreamVideo-High-Fidelity-Image-to-Video-Generation-with-Image-Retention-and-Text-Guidance" class="headerlink" title="DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance"></a>DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03018">http://arxiv.org/abs/2312.03018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, Xiaodan Liang</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å®ç°é«˜è´¨é‡çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼Œä»¥æé«˜ç°æœ‰çš„å›¾åƒæ‰©å±•æ–¹æ³•çš„ç²¾åº¦å’Œå¯æ§æ€§ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäº DreamVideo æ¨¡å‹çš„é«˜ç²¾åº¦å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªæ¡†æŠ¤åˆ†æ”¯æ¥ä¿æŒå›¾åƒç»†èŠ‚ï¼Œä»¥åŠä¸€ç§double-conditionç±» Conditioned GANï¼ˆDCGANï¼‰è‡ªé€‚åº”å¯¼èˆªæ–¹æ³•æ¥å®ç°ä¸åŒåŠ¨ä½œçš„è§†é¢‘ç”Ÿæˆã€‚</li>
<li>results: æˆ‘ä»¬åœ¨å…¬å…±æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨è´¨é‡å’Œå¯æ§æ€§æ–¹é¢éƒ½è¶…è¿‡äº†ç°æœ‰çš„çŠ¶æ€å›¾åƒåˆ°è§†é¢‘æ¨¡å‹ã€‚å°¤å…¶æ˜¯åœ¨ä¿æŒå›¾åƒç»†èŠ‚æ–¹é¢ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ UCF101 ä¸Šçš„ FVD è¾ƒé«˜ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ä¸åŒçš„æ–‡æœ¬æç¤ºæ¥å®ç°ç²¾å‡†çš„æ§åˆ¶ã€‚<details>
<summary>Abstract</summary>
Image-to-video generation, which aims to generate a video starting from a given reference image, has drawn great attention. Existing methods try to extend pre-trained text-guided image diffusion models to image-guided video generation models. Nevertheless, these methods often result in either low fidelity or flickering over time due to their limitation to shallow image guidance and poor temporal consistency. To tackle these problems, we propose a high-fidelity image-to-video generation method by devising a frame retention branch on the basis of a pre-trained video diffusion model, named DreamVideo. Instead of integrating the reference image into the diffusion process in a semantic level, our DreamVideo perceives the reference image via convolution layers and concatenate the features with the noisy latents as model input. By this means, the details of the reference image can be preserved to the greatest extent. In addition, by incorporating double-condition classifier-free guidance, a single image can be directed to videos of different actions by providing varying prompt texts. This has significant implications for controllable video generation and holds broad application prospects. We conduct comprehensive experiments on the public dataset, both quantitative and qualitative results indicate that our method outperforms the state-of-the-art method. Especially for fidelity, our model has powerful image retention ability and result in high FVD in UCF101 compared to other image-to-video models. Also, precise control can be achieved by giving different text prompts. Further details and comprehensive results of our model will be presented in https://anonymous0769.github.io/DreamVideo/.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆï¼Œå®ƒç›®æ ‡æ˜¯ä»ç»™å®šçš„å‚è€ƒå›¾åƒç”Ÿæˆä¸€ä¸ªè§†é¢‘ï¼Œå·²ç»å¸å¼•äº†å¹¿æ³›çš„å…³æ³¨ã€‚ç°æœ‰æ–¹æ³•é€šå¸¸æ˜¯å°†é¢„è®­ç»ƒçš„æ–‡æœ¬æŒ‡å¯¼å›¾åƒæ‰©æ•£æ¨¡å‹æ‰©å±•ä¸ºå›¾åƒæŒ‡å¯¼è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ç»å¸¸ä¼šå¯¼è‡´æ—¶é—´ä¸Šçš„ä½ç²¾åº¦æˆ–é—ªçƒï¼Œè¿™æ˜¯å› ä¸ºå®ƒä»¬çš„å›¾åƒæŒ‡å¯¼æ€§è¾ƒæµ…å’Œæ—¶é—´ä¸Šçš„ä¸€è‡´æ€§ä¸å¤Ÿã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜ç²¾åº¦çš„å›¾åƒåˆ°è§†é¢‘ç”Ÿæˆæ–¹æ³•ï¼ŒåŸºäºé¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºDreamVideoã€‚åœ¨DreamVideoä¸­ï¼Œæˆ‘ä»¬ä¸æ˜¯å°†å‚è€ƒå›¾åƒç›´æ¥ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°åˆ°æ‰©æ•£è¿‡ç¨‹ä¸­çš„semanticå±‚æ¬¡ï¼Œè€Œæ˜¯é€šè¿‡ convolutionå±‚å°†å‚è€ƒå›¾åƒ perceiveï¼Œå¹¶å°†å…¶çš„ç‰¹å¾ä¸éšæœºå™ªéŸ³çš„ç‰¹å¾è¿›è¡Œ concatenateã€‚è¿™æ ·åšçš„åŸå› æ˜¯ä¿ç•™å‚è€ƒå›¾åƒçš„ç»†èŠ‚ï¼Œä»¥è¾¾åˆ°æœ€å¤§ç¨‹åº¦çš„ç²¾åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜é€šè¿‡double-condition classifier-free guidanceï¼Œä½¿å¾—ä¸€ä¸ªå›¾åƒå¯ä»¥è¢«æŒ‡å¯¼åˆ°ä¸åŒçš„åŠ¨ä½œè§†é¢‘ä¸­ï¼Œåªéœ€æä¾›ä¸åŒçš„æ–‡æœ¬æç¤ºã€‚è¿™æœ‰ç€å¹¿æ³›çš„åº”ç”¨å‰æ™¯å’Œå¯æ§çš„è§†é¢‘ç”Ÿæˆçš„æ„ä¹‰ã€‚æˆ‘ä»¬å¯¹å…¬å…±æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œboth quantitativeå’Œqualitativeç»“æœè¡¨æ˜ï¼Œouræ–¹æ³•åœ¨state-of-the-artæ–¹æ³•ä¹‹ä¸Šã€‚ç‰¹åˆ«æ˜¯ï¼Œouræ¨¡å‹åœ¨UCF101ä¸Šçš„FVDï¼ˆFrame Velocity Differenceï¼‰å€¼è¾ƒé«˜ï¼Œè¡¨æ˜å®ƒæœ‰å¼ºå¤§çš„å›¾åƒä¿ç•™èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä¸åŒçš„æ–‡æœ¬æç¤ºï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç²¾ç¡®çš„æ§åˆ¶ã€‚æ›´å¤šç»†èŠ‚å’Œouræ¨¡å‹çš„å…¨é¢ç»“æœå¯ä»¥åœ¨https://anonymous0769.github.io/DreamVideo/æŸ¥çœ‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="GDN-A-Stacking-Network-Used-for-Skin-Cancer-Diagnosis"><a href="#GDN-A-Stacking-Network-Used-for-Skin-Cancer-Diagnosis" class="headerlink" title="GDN: A Stacking Network Used for Skin Cancer Diagnosis"></a>GDN: A Stacking Network Used for Skin Cancer Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02437">http://arxiv.org/abs/2312.02437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingmin Wei, Haoyang Shen, Ziyi Wang, Ziqian Zhang</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§è‡ªåŠ¨è¯†åˆ«ä¸åŒç±»å‹çš®è‚¤ç™Œçš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œä»¥æé«˜çš®è‚¤ç™Œçš„æ£€æµ‹ç²¾åº¦ã€‚</li>
<li>methods: è¿™ä¸ªæ¨¡å‹ä½¿ç”¨äº†å †å ä¸åŒç½‘ç»œçš„æ–¹æ³•æ¥æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå…·ä½“æ¥è¯´æ˜¯ä½¿ç”¨GoogLeNetå’ŒDenseNetä¸¤ä¸ªç½‘ç»œè¿›è¡Œå¹¶è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ç¬¬äºŒå±‚ä½¿ç”¨logistic regressionæ¨¡å‹æ¥è¿›è¡Œé¢„æµ‹ã€‚</li>
<li>results: æ¯”è¾ƒè¿™ä¸ªæ¨¡å‹ä¸å››ä¸ªåŸºelineç½‘ç»œï¼ˆResNetã€VGGNetã€DenseNetå’ŒGoogLeNetï¼‰ï¼ŒGDNæ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šæ˜¾ç¤ºå‡ºè¾ƒé«˜çš„å‡†ç¡®ç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨Logistic Regressioné¢„æµ‹æ–¹æ³•æ—¶è¾¾åˆ°äº†æœ€å¥½çš„é¢„æµ‹ç»“æœã€‚<details>
<summary>Abstract</summary>
Skin cancer, the primary type of cancer that can be identified by visual recognition, requires an automatic identification system that can accurately classify different types of lesions. This paper presents GoogLe-Dense Network (GDN), which is an image-classification model to identify two types of skin cancer, Basal Cell Carcinoma, and Melanoma. GDN uses stacking of different networks to enhance the model performance. Specifically, GDN consists of two sequential levels in its structure. The first level performs basic classification tasks accomplished by GoogLeNet and DenseNet, which are trained in parallel to enhance efficiency. To avoid low accuracy and long training time, the second level takes the output of the GoogLeNet and DenseNet as the input for a logistic regression model. We compare our method with four baseline networks including ResNet, VGGNet, DenseNet, and GoogLeNet on the dataset, in which GoogLeNet and DenseNet significantly outperform ResNet and VGGNet. In the second level, different stacking methods such as perceptron, logistic regression, SVM, decision trees and K-neighbor are studied in which Logistic Regression shows the best prediction result among all. The results prove that GDN, compared to a single network structure, has higher accuracy in optimizing skin cancer detection.
</details>
<details>
<summary>æ‘˜è¦</summary>
çš®è‚¤ç™Œï¼Œä¸»è¦çš„ç™Œç—‡å¯ä»¥é€šè¿‡è§†è§‰è¯†åˆ«ï¼Œéœ€è¦ä¸€ä¸ªè‡ªåŠ¨è¯†åˆ«ç³»ç»Ÿï¼Œå¯ä»¥å‡†ç¡®åœ°åˆ†ç±»ä¸åŒç±»å‹çš„è‚¿ç˜¤ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†GoogLe-Dense Networkï¼ˆGDNï¼‰ï¼Œæ˜¯ä¸€ç§å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œç”¨äºè¯†åˆ«ä¸¤ç§çš®è‚¤ç™Œï¼ŒåŸºç¡€ç»†èƒç™Œå’Œ Ğ¼ĞµĞ»Ğ°Ğ½Ğ¾Ğ¼Ğ°ã€‚GDNä½¿ç”¨ä¸åŒç½‘ç»œå †å æ¥æé«˜æ¨¡å‹æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼ŒGDNåŒ…æ‹¬ä¸¤ä¸ªçº§åˆ«ç»“æ„ã€‚ç¬¬ä¸€çº§å®ŒæˆåŸºæœ¬çš„åˆ†ç±»ä»»åŠ¡ï¼Œç”±GoogLeNetå’ŒDenseNetè¿›è¡Œå¹¶è¡Œè®­ç»ƒï¼Œä»¥æé«˜æ•ˆç‡ã€‚è€Œç¬¬äºŒçº§ä½¿ç”¨GoogLeNetå’ŒDenseNetçš„è¾“å‡ºä½œä¸ºå¯¹æ•°å­¦å½’ä¸€åŒ–æ¨¡å‹çš„è¾“å…¥ï¼Œä»¥é¿å…ä½å‡†ç¡®ç‡å’Œé•¿è®­ç»ƒæ—¶é—´ã€‚æˆ‘ä»¬ä¸å››ç§åŸºelineç½‘ç»œï¼ŒåŒ…æ‹¬ResNetã€VGGNetã€DenseNetå’ŒGoogLeNetè¿›è¡Œæ¯”è¾ƒï¼Œç»“æœæ˜¾ç¤ºGoogLeNetå’ŒDenseNetåœ¨è¯¥æ•°æ®é›†ä¸Šæ˜¾è‘—è¶…è¿‡ResNetå’ŒVGGNetã€‚åœ¨ç¬¬äºŒçº§ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸åŒçš„å †å æ–¹æ³•ï¼ŒåŒ…æ‹¬æƒé‡åŠ æƒã€å½’ä¸€åŒ–ã€æ”¯æŒå‘é‡æœºã€å†³ç­–æ ‘å’ŒK-è¿‘é‚»ï¼Œå…¶ä¸­Logistic Regressionè¡¨ç°æœ€å¥½ã€‚ç»“æœè¯æ˜ï¼Œç›¸æ¯”å•ä¸ªç½‘ç»œç»“æ„ï¼ŒGDNåœ¨ä¼˜åŒ–çš®è‚¤ç™Œæ£€æµ‹ä¸Šæœ‰æ›´é«˜çš„å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="FINER-Flexible-spectral-bias-tuning-in-Implicit-NEural-Representation-by-Variable-periodic-Activation-Functions"><a href="#FINER-Flexible-spectral-bias-tuning-in-Implicit-NEural-Representation-by-Variable-periodic-Activation-Functions" class="headerlink" title="FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions"></a>FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02434">http://arxiv.org/abs/2312.02434</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Liu, Hao Zhu, Qi Zhang, Jingde Fu, Weibing Deng, Zhan Ma, Yanwen Guo, Xun Cao<br>for: è§£å†³ç°æœ‰INRæŠ€æœ¯ä¸­é¢‘ç‡ç›¸å…³é—®é¢˜ï¼Œæé«˜å¤æ‚ä¿¡å·çš„è¡¨ç¤ºæ€§èƒ½ã€‚methods: æå‡ºå˜é‡å‘¨æœŸå‡½æ•°ï¼ˆFINERï¼‰ï¼Œé€šè¿‡åˆå§‹åŒ–ç¥ç»ç½‘ç»œåç½®åœ¨ä¸åŒèŒƒå›´å†…ï¼Œé€‰æ‹©ä¸åŒé¢‘ç‡çš„å­å‡½æ•°è¿›è¡Œæ´»åŒ–ã€‚results: åœ¨2Då›¾åƒé€‚åº”ã€3Dç­¾åè·ç¦»åœºè¡¨ç¤ºå’Œ5Dç¥ç»è¾å°„åœºä¼˜åŒ–ä¸Šï¼ŒFINERè¡¨ç°å‡ºæ¯”ç°æœ‰INRæ›´é«˜çš„è¡¨ç¤ºæ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Implicit Neural Representation (INR), which utilizes a neural network to map coordinate inputs to corresponding attributes, is causing a revolution in the field of signal processing. However, current INR techniques suffer from a restricted capability to tune their supported frequency set, resulting in imperfect performance when representing complex signals with multiple frequencies. We have identified that this frequency-related problem can be greatly alleviated by introducing variable-periodic activation functions, for which we propose FINER. By initializing the bias of the neural network within different ranges, sub-functions with various frequencies in the variable-periodic function are selected for activation. Consequently, the supported frequency set of FINER can be flexibly tuned, leading to improved performance in signal representation. We demonstrate the capabilities of FINER in the contexts of 2D image fitting, 3D signed distance field representation, and 5D neural radiance fields optimization, and we show that it outperforms existing INRs.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŒ¿ï¿½ç”Ÿç¥ç»è¡¨ç¤ºæ³•ï¼ˆINRï¼‰ï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå°†åæ ‡è¾“å…¥æ˜ å°„åˆ°ç›¸åº”çš„ç‰¹å¾ä¸Šï¼Œåœ¨ä¿¡å·å¤„ç†é¢†åŸŸå¼•èµ·äº†é©å‘½ã€‚ç„¶è€Œï¼Œå½“å‰INRæŠ€æœ¯å­˜åœ¨ä¸€å®šçš„é¢‘ç‡é›†æ”¯æŒçš„é™åˆ¶ï¼Œå¯¼è‡´è¡¨ç¤ºå¤æ‚ä¿¡å·çš„å¤šé¢‘æ€§è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬å‘ç°ï¼Œè¿™ç§é¢‘ç‡ç›¸å…³é—®é¢˜å¯ä»¥é€šè¿‡å¼•å…¥å˜é‡å‘¨æœŸ activation function è¿›è¡Œå¤§å¹…å‡è½»ã€‚æˆ‘ä»¬æè®®ä½¿ç”¨ä¸åŒèŒƒå›´å†…çš„åç½®åˆå§‹åŒ–ç¥ç»ç½‘ç»œï¼Œä»è€Œé€‰æ‹©ä¸åŒé¢‘ç‡çš„å­å‡½æ•°è¿›è¡Œæ´»åŒ–ã€‚å› æ­¤ï¼ŒFINERå¯ä»¥è‡ªç”±åœ°è°ƒæ•´æ”¯æŒçš„é¢‘ç‡é›†ï¼Œä»è€Œæé«˜ä¿¡å·è¡¨ç¤ºçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨2Då›¾åƒé€‚åº”ã€3Dç­¾åè·ç¦»åœºè¡¨ç¤ºå’Œ5Dç¥ç»è¾å°„åœºä¼˜åŒ–ä¸Šå±•ç¤ºäº†FINERçš„èƒ½åŠ›ï¼Œå¹¶è¯æ˜å®ƒè¶…è¿‡äº†ç°æœ‰çš„INRã€‚
</details></li>
</ul>
<hr>
<h2 id="Lenna-Language-Enhanced-Reasoning-Detection-Assistant"><a href="#Lenna-Language-Enhanced-Reasoning-Detection-Assistant" class="headerlink" title="Lenna: Language Enhanced Reasoning Detection Assistant"></a>Lenna: Language Enhanced Reasoning Detection Assistant</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02433">http://arxiv.org/abs/2312.02433</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meituan-automl/lenna">https://github.com/meituan-automl/lenna</a></li>
<li>paper_authors: Fei Wei, Xinyu Zhang, Ailing Zhang, Bo Zhang, Xiangxiang Chu</li>
<li>for:  This paper proposes a language-enhanced reasoning detection assistant called Lenna, which utilizes robust multimodal feature representation for image perception tasks.</li>
<li>methods: The paper incorporates an additional <DET> token in the MLLM vocabulary to preserve location information for detection, and constructs a ReasonDet dataset to measure the reasoning capability of Lenna.</li>
<li>results: Lenna demonstrates outstanding performance on ReasonDet with significantly low training costs and minimal transferring overhead when extended to other tasks.Hereâ€™s the simplified Chinese text:</li>
<li>for: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„è¯­è¨€å¢å¼ºæ¨ç†æ£€æµ‹åŠ©æ‰‹ï¼Œå³Lennaã€‚</li>
<li>methods: è¯¥è®ºæ–‡åœ¨MLLMä¸­æ·»åŠ äº†ä¸€ä¸ª<DET> tokensï¼Œä»¥ä¿æŒæ£€æµ‹ä½ç½®ä¿¡æ¯ï¼Œå¹¶æ„å»ºäº†ä¸€ä¸ªReasonDetæ•°æ®é›†æ¥è¯„ä¼°Lennaçš„æ¨ç†èƒ½åŠ›ã€‚</li>
<li>results: Lennaåœ¨ReasonDetä¸Šè¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæˆæœ¬ä½ä¸‹ï¼Œå¹¶åœ¨å…¶ä»–ä»»åŠ¡ä¸Šè½¬ç§»æˆæœ¬å¾ˆä½ã€‚ä»£ç å’Œæ¨¡å‹å°†åœ¨<a target="_blank" rel="noopener" href="https://git.io/Lenna">https://git.io/Lenna</a>ä¸Šå…¬å¼€ã€‚<details>
<summary>Abstract</summary>
With the fast-paced development of multimodal large language models (MLLMs), we can now converse with AI systems in natural languages to understand images. However, the reasoning power and world knowledge embedded in the large language models have been much less investigated and exploited for image perception tasks. In this paper, we propose Lenna, a language-enhanced reasoning detection assistant, which utilizes the robust multimodal feature representation of MLLMs, while preserving location information for detection. This is achieved by incorporating an additional <DET> token in the MLLM vocabulary that is free of explicit semantic context but serves as a prompt for the detector to identify the corresponding position. To evaluate the reasoning capability of Lenna, we construct a ReasonDet dataset to measure its performance on reasoning-based detection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet and comes with significantly low training costs. It also incurs minimal transferring overhead when extended to other tasks. Our code and model will be available at https://git.io/Lenna.
</details>
<details>
<summary>æ‘˜è¦</summary>
With the rapid development of multimodal large language models (MLLMs), we can now communicate with AI systems in natural language to understand images. However, the reasoning power and world knowledge embedded in the large language models have been less explored and utilized for image perception tasks. In this paper, we propose Lenna, a language-enhanced reasoning detection assistant, which utilizes the robust multimodal feature representation of MLLMs, while preserving location information for detection. This is achieved by incorporating an additional <DET> token in the MLLM vocabulary that is free of explicit semantic context but serves as a prompt for the detector to identify the corresponding position. To evaluate the reasoning capability of Lenna, we construct a ReasonDet dataset to measure its performance on reasoning-based detection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet and comes with significantly low training costs. It also incurs minimal transferring overhead when extended to other tasks. Our code and model will be available at https://git.io/Lenna.Here's the translation in Traditional Chinese:éšç€å¤šModalå¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€ä¸AIç³»ç»Ÿè¿›è¡Œäº¤æµï¼Œä»¥äº†è§£å›¾åƒã€‚ç„¶è€Œï¼Œå¤§è¯­è¨€æ¨¡å‹ä¸­åµŒå…¥çš„ç†è§£åŠ›å’Œä¸–ç•ŒçŸ¥è¯†åœ¨å›¾åƒè®¤è¯†ä»»åŠ¡ä¸­å¾—åˆ°äº†è¾ƒå°‘çš„æ¢ç´¢å’Œåˆ©ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºLennaï¼Œä¸€ä¸ªè¯­è¨€å¢å¼ºçš„æ¨ç†æ£€æµ‹åŠ©æ‰‹ï¼Œåˆ©ç”¨MLLMçš„å¼ºå¤§å¤šmodalç‰¹å¾è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™æ£€æµ‹ä½ç½®ä¿¡æ¯ã€‚è¿™æ˜¯é€šè¿‡åœ¨MLLMè¯æ±‡ä¸­æ·»åŠ ä¸€ä¸ª<DET>tokenï¼Œè¯¥tokenæ˜¯æ— å…³ semantic contextçš„ï¼Œä½†å¯ä½œä¸ºæ£€æµ‹å™¨è¯†åˆ«ç›¸åº”ä½ç½®çš„å¸®åŠ©ã€‚ä¸ºäº†è¯„ä¼°Lennaçš„æ¨ç†èƒ½åŠ›ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªReasonDetæ•°æ®é›†ï¼Œç”¨äºé‡åŒ–å®ƒåœ¨æ¨ç†åŸºäºæ£€æµ‹çš„æ€§èƒ½ã€‚æƒŠå¥‡çš„æ˜¯ï¼ŒLennaåœ¨ReasonDetä¸Šè¡¨ç°å‡ºè‰²ï¼Œè®­ç»ƒæˆæœ¬è¾ƒä½ï¼Œä¸”å¯¹å…¶ä»–ä»»åŠ¡çš„è½¬ç§»æˆæœ¬è¾ƒä½ã€‚æˆ‘ä»¬çš„ä»£ç å’Œæ¨¡å‹å°†åœ¨https://git.io/Lenna ä¸Šå…¬å¼€ã€‚
</details></li>
</ul>
<hr>
<h2 id="Orthogonal-Adaptation-for-Modular-Customization-of-Diffusion-Models"><a href="#Orthogonal-Adaptation-for-Modular-Customization-of-Diffusion-Models" class="headerlink" title="Orthogonal Adaptation for Modular Customization of Diffusion Models"></a>Orthogonal Adaptation for Modular Customization of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02432">http://arxiv.org/abs/2312.02432</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Po, Guandao Yang, Kfir Aberman, Gordon Wetzstein</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è§£å†³ Modular Customization é—®é¢˜ï¼Œå®ç°é«˜æ•ˆåœ°åˆå¹¶ç²¾å¿ƒè°ƒæ•´çš„æ‰©å±•æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä¸€å¹…å›¾åƒä¸­åŒæ—¶æ¸²æŸ“å¤šä¸ªæ¦‚å¿µã€‚</li>
<li>methods: è¯¥ paper ä½¿ç”¨ Orthogonal Adaptation æ–¹æ³•ï¼Œé¼“åŠ±ç²¾å¿ƒè°ƒæ•´çš„æ¨¡å‹åœ¨INFERENCEæ—¶è¿›è¡Œäº’è¡¥ï¼Œä»¥ç¡®ä¿åˆå¹¶åçš„æ¨¡å‹ä»ç„¶å¯ä»¥ä¿æŒé«˜åº¦çš„å‡†ç¡®æ€§å’Œå”¯ä¸€æ€§ã€‚</li>
<li>results: è¯¥ paper çš„å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç›¸å…³åŸºelineæ¯”è¾ƒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œå‡†ç¡®æ€§ä¸¤ä¸ªæ–¹é¢å‡æœ‰æ˜¾è‘—æé«˜ï¼Œä»£è¡¨äº†åœ¨æ‰©å±• Customization é¢†åŸŸä¸­çš„é‡è¦çªç ´ã€‚<details>
<summary>Abstract</summary>
Customization techniques for text-to-image models have paved the way for a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods facilitate high-fidelity customization for individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs.   To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference.   Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.
</details>
<details>
<summary>æ‘˜è¦</summary>
Customization techniques for text-to-image models have opened up a wide range of previously unattainable applications, enabling the generation of specific concepts across diverse contexts and styles. While existing methods allow for high-fidelity customization of individual concepts or a limited, pre-defined set of them, they fall short of achieving scalability, where a single model can seamlessly render countless concepts. In this paper, we address a new problem called Modular Customization, with the goal of efficiently merging customized models that were fine-tuned independently for individual concepts. This allows the merged model to jointly synthesize concepts in one image without compromising fidelity or incurring any additional computational costs.To address this problem, we introduce Orthogonal Adaptation, a method designed to encourage the customized models, which do not have access to each other during fine-tuning, to have orthogonal residual weights. This ensures that during inference time, the customized models can be summed with minimal interference.Our proposed method is both simple and versatile, applicable to nearly all optimizable weights in the model architecture. Through an extensive set of quantitative and qualitative evaluations, our method consistently outperforms relevant baselines in terms of efficiency and identity preservation, demonstrating a significant leap toward scalable customization of diffusion models.
</details></li>
</ul>
<hr>
<h2 id="FreestyleRet-Retrieving-Images-from-Style-Diversified-Queries"><a href="#FreestyleRet-Retrieving-Images-from-Style-Diversified-Queries" class="headerlink" title="FreestyleRet: Retrieving Images from Style-Diversified Queries"></a>FreestyleRet: Retrieving Images from Style-Diversified Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02428">http://arxiv.org/abs/2312.02428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Li, Curise Jia, Peng Jin, Zesen Cheng, Kehan Li, Jialu Sui, Chang Liu, Li Yuan</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯æå‡º Style-Diversified Query-Based Image Retrieval ä»»åŠ¡ï¼Œå…è®¸æ ¹æ®ä¸åŒçš„æŸ¥è¯¢é£æ ¼è¿›è¡Œå›¾åƒæ£€ç´¢ã€‚</li>
<li>methods: ä½œè€…æå‡ºäº†ä¸€ç§è½»é‡çº§å¤šæ ·åŒ–æŸ¥è¯¢æ£€ç´¢æ¡†æ¶ï¼Œä½¿å¾—ä¸åŒçš„æŸ¥è¯¢é£æ ¼ï¼ˆå¦‚æ–‡æœ¬ã€ç¬”è¿¹ã€ä½åˆ†è¾¨ç‡ã€è‰ºæœ¯ç­‰ï¼‰éƒ½å¯ä»¥åŒæ—¶è¿›è¡Œæ£€ç´¢ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ä½œè€…æå‡ºçš„ style-init æç¤ºè°ƒæ•´ç­–ç•¥ï¼Œå¯¹ Style-Diversified Query-Based Image Retrieval ä»»åŠ¡è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯ä»¥åŒæ—¶æ£€ç´¢ä¸åŒæŸ¥è¯¢é£æ ¼çš„å›¾åƒã€‚æ­¤å¤–ï¼Œauxiliary information from other queries ä¹Ÿå¯ä»¥å¢å¼ºæ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Image Retrieval aims to retrieve corresponding images based on a given query. In application scenarios, users intend to express their retrieval intent through various query styles. However, current retrieval tasks predominantly focus on text-query retrieval exploration, leading to limited retrieval query options and potential ambiguity or bias in user intention. In this paper, we propose the Style-Diversified Query-Based Image Retrieval task, which enables retrieval based on various query styles. To facilitate the novel setting, we propose the first Diverse-Style Retrieval dataset, encompassing diverse query styles including text, sketch, low-resolution, and art. We also propose a light-weighted style-diversified retrieval framework. For various query style inputs, we apply the Gram Matrix to extract the query's textural features and cluster them into a style space with style-specific bases. Then we employ the style-init prompt tuning module to enable the visual encoder to comprehend the texture and style information of the query. Experiments demonstrate that our model, employing the style-init prompt tuning strategy, outperforms existing retrieval models on the style-diversified retrieval task. Moreover, style-diversified queries~(sketch+text, art+text, etc) can be simultaneously retrieved in our model. The auxiliary information from other queries enhances the retrieval performance within the respective query.
</details>
<details>
<summary>æ‘˜è¦</summary>
å›¾åƒæ£€ç´¢ç›®æ ‡æ˜¯æ ¹æ®ç»™å®šçš„æŸ¥è¯¢ retrieve ç›¸åº”çš„å›¾åƒã€‚åœ¨åº”ç”¨åœºæ™¯ä¸­ï¼Œç”¨æˆ·å¯èƒ½é€šè¿‡ä¸åŒçš„æŸ¥è¯¢é£æ ¼è¡¨è¾¾ä»–ä»¬çš„æ£€ç´¢æ„å›¾ã€‚ç„¶è€Œï¼Œç°æœ‰çš„æ£€ç´¢ä»»åŠ¡ä¸»è¦é›†ä¸­åœ¨æ–‡æœ¬æŸ¥è¯¢æ£€ç´¢é¢†åŸŸï¼Œå¯¼è‡´æ£€ç´¢æŸ¥è¯¢é€‰é¡¹æœ‰é™ï¼Œå¯èƒ½å­˜åœ¨ç”¨æˆ·æ„å›¾çš„æŠ½è±¡æˆ–åè§ã€‚æœ¬æ–‡æå‡ºäº†å¤šæ ·åŒ–æŸ¥è¯¢åŸºäºå›¾åƒæ£€ç´¢ä»»åŠ¡ï¼ˆStyle-Diversified Query-Based Image Retrieval taskï¼‰ï¼Œå…è®¸åŸºäºå¤šç§æŸ¥è¯¢é£æ ¼è¿›è¡Œæ£€ç´¢ã€‚ä¸ºäº†æ¨åŠ¨è¿™ç§æ–°çš„è®¾å®šï¼Œæˆ‘ä»¬æå‡ºäº†é¦–ä¸ªå¤šæ ·åŒ–æŸ¥è¯¢ datasetï¼ŒåŒ…æ‹¬å¤šç§æŸ¥è¯¢é£æ ¼ï¼Œå¦‚æ–‡æœ¬ã€ç»˜åˆ¶ã€ä½åˆ†è¾¨ç‡å’Œè‰ºæœ¯ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§è½»é‡çº§å¤šæ ·åŒ–æ£€ç´¢æ¡†æ¶ã€‚å¯¹äºä¸åŒçš„æŸ¥è¯¢é£æ ¼è¾“å…¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ Gram Matrix æå–æŸ¥è¯¢çš„æ–‡æœ¬ç‰¹å¾ï¼Œå¹¶å°†å…¶åˆ†ä¸ºä¸€ä¸ªé£æ ¼ç©ºé—´ä¸­çš„é£æ ¼ç‰¹å¾åŸºã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é£æ ¼åˆå§‹åŒ– prompt tuning æ¨¡å—ï¼Œä½¿è§†è§‰ç¼–ç å™¨ç†è§£æŸ¥è¯¢ä¸­çš„Textureå’Œé£æ ¼ä¿¡æ¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½¿ç”¨é£æ ¼åˆå§‹åŒ– prompt tuning ç­–ç•¥ï¼Œåœ¨å¤šæ ·åŒ–æ£€ç´¢ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”å¯ä»¥åŒæ—¶æ£€ç´¢ä¸åŒçš„æŸ¥è¯¢é£æ ¼ï¼ˆå¦‚ç»˜åˆ¶+æ–‡æœ¬ã€è‰ºæœ¯+æ–‡æœ¬ç­‰ï¼‰ã€‚æ­¤å¤–ï¼Œauxiliary information ä»å…¶ä»–æŸ¥è¯¢ä¸­å¯¹å½“å‰æŸ¥è¯¢çš„æ£€ç´¢æ€§èƒ½äº§ç”Ÿè¡¥å¿ä½œç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Granularity-adjusted-Pixel-level-Semantic-Annotation"><a href="#Towards-Granularity-adjusted-Pixel-level-Semantic-Annotation" class="headerlink" title="Towards Granularity-adjusted Pixel-level Semantic Annotation"></a>Towards Granularity-adjusted Pixel-level Semantic Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02420">http://arxiv.org/abs/2312.02420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Kundu, Sudipta Paul, Rohit Lal, Amit K. Roy-Chowdhury</li>
<li>for: æä¾›æ— éœ€äººå·¥ç›‘ç£çš„semantic segmentationé¢„æµ‹æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå›¾åƒä¸­åƒç´ çº§åˆ«çš„æ ‡æ³¨æ•°æ®ã€‚</li>
<li>methods: ä½¿ç”¨Stable Diffusionæ¨¡å‹ç”Ÿæˆsyntheticå›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥å­¦ä¹ ä¸€ä¸ªæ˜ å°„å‡½æ•°ï¼Œå°†SAMå·ç§¯æ¨èçš„åŒ€é€Ÿåº¦embeddingsä¸ç‰©ä½“ç±»åˆ«æ ‡ç­¾ç›¸å¯¹åº”ã€‚</li>
<li>results: åœ¨PASCAL VOC 2012å’ŒCOCO-80 datasetsä¸Šè¿›è¡Œå®éªŒï¼Œæ¯”å¯¹ existedçŠ¶æ€çš„æ–¹æ³•æ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†+17.95%å’Œ+5.17%çš„mIoUæå‡ã€‚<details>
<summary>Abstract</summary>
Recent advancements in computer vision predominantly rely on learning-based systems, leveraging annotations as the driving force to develop specialized models. However, annotating pixel-level information, particularly in semantic segmentation, presents a challenging and labor-intensive task, prompting the need for autonomous processes. In this work, we propose GranSAM which distinguishes itself by providing semantic segmentation at the user-defined granularity level on unlabeled data without the need for any manual supervision, offering a unique contribution in the realm of semantic mask annotation method. Specifically, we propose an approach to enable the Segment Anything Model (SAM) with semantic recognition capability to generate pixel-level annotations for images without any manual supervision. For this, we accumulate semantic information from synthetic images generated by the Stable Diffusion model or web crawled images and employ this data to learn a mapping function between SAM mask embeddings and object class labels. As a result, SAM, enabled with granularity-adjusted mask recognition, can be used for pixel-level semantic annotation purposes. We conducted experiments on the PASCAL VOC 2012 and COCO-80 datasets and observed a +17.95% and +5.17% increase in mIoU, respectively, compared to existing state-of-the-art methods when evaluated under our problem setting.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="MGTR-Multi-Granular-Transformer-for-Motion-Prediction-with-LiDAR"><a href="#MGTR-Multi-Granular-Transformer-for-Motion-Prediction-with-LiDAR" class="headerlink" title="MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR"></a>MGTR: Multi-Granular Transformer for Motion Prediction with LiDAR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02409">http://arxiv.org/abs/2312.02409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlexXiao95/MGTR">https://github.com/AlexXiao95/MGTR</a></li>
<li>paper_authors: Yiqian Gan, Hao Xiao, Yizhe Zhao, Ethan Zhang, Zhe Huang, Xin Ye, Lingting Ge</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ç”¨äºè‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„åŠ¨ä½œé¢„æµ‹ï¼Œä»¥åº”å¯¹é«˜åº¦ä¸ç¡®å®šå’Œå¤æ‚çš„äº¤é€šæ™¯è±¡ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤šç²’å­ Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°å™¨ï¼ˆMGTRï¼‰æ¡†æ¶ï¼Œåˆ©ç”¨ä¸åŒç²’å­å¤§å°çš„ä¸Šä¸‹æ–‡ç‰¹å¾æ¥æè¿°ä¸åŒç±»å‹çš„äº¤é€šå·¥å…·ã€‚æ­¤å¤–ï¼Œæ–‡ä»¶è¿˜ä½¿ç”¨äº†å†…ç½®çš„ LiDAR ç‰¹å¾æå–å™¨æ¥æ­é… MGTRï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå…¶èƒ½åŠ›ã€‚</li>
<li>results: æ ¹æ® Waymo Open Dataset åŠ¨ä½œé¢„æµ‹ benchmark çš„è¯„ä¼°ç»“æœï¼Œæå‡ºçš„ MGTR æ–¹æ³•è·å¾—äº†æœ€ä½³æ€§èƒ½ï¼Œåœ¨é¢†å¤´ç‰Œï¼ˆ<a target="_blank" rel="noopener" href="https://waymo.com/open/challenges/2023/motion-prediction/%EF%BC%89%E4%B8%8A%E6%8E%92%E5%90%8D%E7%AC%AC%E4%B8%80%E3%80%82">https://waymo.com/open/challenges/2023/motion-prediction/ï¼‰ä¸Šæ’åç¬¬ä¸€ã€‚</a><details>
<summary>Abstract</summary>
Motion prediction has been an essential component of autonomous driving systems since it handles highly uncertain and complex scenarios involving moving agents of different types. In this paper, we propose a Multi-Granular TRansformer (MGTR) framework, an encoder-decoder network that exploits context features in different granularities for different kinds of traffic agents. To further enhance MGTR's capabilities, we leverage LiDAR point cloud data by incorporating LiDAR semantic features from an off-the-shelf LiDAR feature extractor. We evaluate MGTR on Waymo Open Dataset motion prediction benchmark and show that the proposed method achieved state-of-the-art performance, ranking 1st on its leaderboard (https://waymo.com/open/challenges/2023/motion-prediction/).
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿä¸­çš„åŠ¨æ€é¢„æµ‹å·²ç»æ˜¯ä¸€é¡¹éå¸¸é‡è¦çš„ç»„ä»¶ï¼Œå› ä¸ºå®ƒå¯ä»¥å¤„ç†é«˜åº¦ä¸ç¡®å®šå’Œå¤æ‚çš„æƒ…å†µï¼Œæ¶‰åŠåˆ°ä¸åŒç±»å‹çš„ç§»åŠ¨ä»£ç†ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šçº§åˆ« TRansformerï¼ˆMGTRï¼‰æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨ç½‘ç»œï¼Œå®ƒåˆ©ç”¨ä¸åŒçº§åˆ«çš„ä¸Šä¸‹æ–‡ç‰¹å¾æ¥å¤„ç†ä¸åŒç±»å‹çš„äº¤é€šä»£ç†ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜MGTRçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†LiDARç‚¹äº‘æ•°æ®ï¼Œå¹¶åœ¨LiDARç‰¹å¾æå–å™¨ä¸­Integrate LiDARè¯­ä¹‰ç‰¹å¾ã€‚æˆ‘ä»¬å¯¹ Waymo Open Dataset åŠ¨æ€é¢„æµ‹æµ‹è¯•benchmarkè¿›è¡Œè¯„ä¼°ï¼Œå¹¶æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ææ¡ˆæ–¹æ³•åœ¨å…¶é¢†å¯¼äººæ°å‡ºè¡¨ç°ï¼Œåœ¨å…¶é¢†å¯¼äººæ°å‡ºè¡¨ç°ï¼ˆhttps://waymo.com/open/challenges/2023/motion-prediction/ï¼‰ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/05/cs.CV_2023_12_05/" data-id="clq0ru6vp00q3to88dcz6594b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_12_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/05/cs.AI_2023_12_05/" class="article-date">
  <time datetime="2023-12-05T12:00:00.000Z" itemprop="datePublished">2023-12-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/05/cs.AI_2023_12_05/">cs.AI - 2023-12-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FERGI-Automatic-Annotation-of-User-Preferences-for-Text-to-Image-Generation-from-Spontaneous-Facial-Expression-Reaction"><a href="#FERGI-Automatic-Annotation-of-User-Preferences-for-Text-to-Image-Generation-from-Spontaneous-Facial-Expression-Reaction" class="headerlink" title="FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction"></a>FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03187">http://arxiv.org/abs/2312.03187</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuangquanfeng/fergi">https://github.com/shuangquanfeng/fergi</a></li>
<li>paper_authors: Shuangquan Feng, Junhua Ma, Virginia R. de Sa<br>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ç”¨äººç±»å–œå¥½åé¦ˆæ•°æ®æ¥è°ƒæ•´æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†è‡ªåŠ¨æ³¨é‡Šç”¨æˆ·å–œå¥½åé¦ˆæ¥è‡ªåŠ¨æ ‡æ³¨ç”¨æˆ·å¯¹ç”Ÿæˆå›¾åƒçš„è¯„ä»·ã€‚results: è¿™ä¸ªç ”ç©¶å‘ç°ï¼Œå¤šä¸ª facial action unit (AU) çš„æ´»åŠ¨å“åº”ä¸ç”¨æˆ·å¯¹ç”Ÿæˆå›¾åƒçš„è¯„ä»·é«˜åº¦ç›¸å…³ï¼Œç‰¹åˆ«æ˜¯ AU4 (çœ‰ä¸‹ä¸) æ˜¯è¯„ä»·å›¾åƒä¸è‰¯çš„æœ€æœ‰å¯é æ€§çš„å“åº”ã€‚è¿™ç§æ–¹æ³•å¯ä»¥è‡ªåŠ¨æ ‡æ³¨ç”¨æˆ·å–œå¥½åé¦ˆï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„åˆ†ç±»æ¨¡å‹ç»“åˆä½¿ç”¨ä»¥æé«˜äººç±»å–œå¥½çš„å‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is most consistently reflective of negative evaluations of the generated image. This can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in AU4 responses to them with an accuracy significantly outperforming state-of-the-art scoring models. Secondly, directly integrating the AU4 responses with the scoring models improves their consistency with human preferences. Additionally, the AU4 response best reflects the user's evaluation of the image fidelity, making it complementary to the state-of-the-art scoring models, which are generally better at reflecting image-text alignment. Finally, this method of automatic annotation with facial expression analysis can be potentially generalized to other generation tasks. The code is available at https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at the same link for research purposes.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Data-Driven-Traffic-Reconstruction-and-Kernel-Methods-for-Identifying-Stop-and-Go-Congestion"><a href="#Data-Driven-Traffic-Reconstruction-and-Kernel-Methods-for-Identifying-Stop-and-Go-Congestion" class="headerlink" title="Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion"></a>Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03186">http://arxiv.org/abs/2312.03186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edgar Ramirez Sanchez, Shreyaa Raghavan, Cathy Wu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ•°æ®é©±åŠ¨çš„ç ”ç©¶ï¼Œä»¥ä¾¿ä¸ºæ°”å€™å˜åŒ–å’Œå¯æŒç»­å‘å±•æä¾›åŸºç¡€æ•°æ®ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº¤é€šé‡å»ºæŠ€æœ¯æ¥è¯†åˆ«åœè½¦äº‹ä»¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å¼•å…¥åŸºäºæ ¸å‡½æ•°çš„æ–¹æ³•æ¥æè¿°äº¤é€šä¸­çš„ç©ºé—´-æ—¶é—´ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨bootstrapæ–¹æ³•æ¥è¯„ä¼°é‡å»ºè¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°æ•æ‰åŠ åˆ©ç¦å°¼äºšå·é«˜é€Ÿå…¬è·¯ä¸Šçš„åœè½¦äº‹ä»¶ã€‚è¿™ç§æ–¹æ³•å¯ä»¥ä¸ºæ•°æ®é©±åŠ¨çš„å†³ç­–æä¾›åŸºç¡€ã€‚<details>
<summary>Abstract</summary>
Identifying stop-and-go events (SAGs) in traffic flow presents an important avenue for advancing data-driven research for climate change mitigation and sustainability, owing to their substantial impact on carbon emissions, travel time, fuel consumption, and roadway safety. In fact, SAGs are estimated to account for 33-50% of highway driving externalities. However, insufficient attention has been paid to precisely quantifying where, when, and how much these SAGs take place -necessary for downstream decision making, such as intervention design and policy analysis. A key challenge is that the data available to researchers and governments are typically sparse and aggregated to a granularity that obscures SAGs. To overcome such data limitations, this study thus explores the use of traffic reconstruction techniques for SAG identification. In particular, we introduce a kernel-based method for identifying spatio-temporal features in traffic and leverage bootstrapping to quantify the uncertainty of the reconstruction process. Experimental results on California highway data demonstrate the promise of the method for capturing SAGs. This work contributes to a foundation for data-driven decision making to advance sustainability of traffic systems.
</details>
<details>
<summary>æ‘˜è¦</summary>
Identifying stop-and-go events (SAGs) in traffic flow is an important avenue for advancing data-driven research on climate change mitigation and sustainability, as SAGs have a significant impact on carbon emissions, travel time, fuel consumption, and roadway safety. In fact, SAGs are estimated to account for 33-50% of highway driving externalities. However, there has been insufficient attention paid to precisely quantifying where, when, and how much these SAGs take place, which is necessary for downstream decision making, such as intervention design and policy analysis. A key challenge is that the available data to researchers and governments are typically sparse and aggregated to a granularity that obscures SAGs. To overcome such data limitations, this study explores the use of traffic reconstruction techniques for SAG identification. Specifically, we introduce a kernel-based method for identifying spatio-temporal features in traffic and leverage bootstrapping to quantify the uncertainty of the reconstruction process. Experimental results on California highway data demonstrate the promise of the method for capturing SAGs. This work contributes to a foundation for data-driven decision making to advance the sustainability of traffic systems.Here's the text with some additional information about the translation:I translated the text into Simplified Chinese, which is the most widely used variety of Chinese. I tried to preserve the original meaning and structure of the text while making it more concise and natural-sounding in Chinese.Some notes on the translation:* I used åœè½¦å µå¡ (tÄ«ng chÄ“ dÃ o xiÄng) to translate "stop-and-go" events, as it is a common term used in China to describe traffic congestion.* I used é©±åŠ¨ (kuÄ« dÃ ng) to translate "driven" in the phrase "data-driven research," as it is a more common term in Chinese to describe the use of data to inform decision-making.* I used å¯è§ (kÄ› yÃ¡n) to translate "obscures" in the phrase "obscures SAGs," as it is a more common term in Chinese to describe something that is visible or clear.* I used éšæœº (suÄ« jÄ«) to translate "bootstrap" in the phrase "leverage bootstrapping," as it is a more common term in Chinese to describe a random sampling method.I hope this translation is helpful! Let me know if you have any further questions or requests.
</details></li>
</ul>
<hr>
<h2 id="Using-Curiosity-for-an-Even-Representation-of-Tasks-in-Continual-Offline-Reinforcement-Learning"><a href="#Using-Curiosity-for-an-Even-Representation-of-Tasks-in-Continual-Offline-Reinforcement-Learning" class="headerlink" title="Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning"></a>Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03177">http://arxiv.org/abs/2312.03177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/punk95/continual-learning-with-curiosity">https://github.com/punk95/continual-learning-with-curiosity</a></li>
<li>paper_authors: Pankayaraj Pathmanathan, Natalia DÃ­az-RodrÃ­guez, Javier Del Ser</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ä½¿ç”¨å¥½å¥‡æ€§æ¥æé«˜ç¦»çº¿å¤šä»»åŠ¡è¿ç»­å¼ºåŒ–å­¦ä¹ ï¼Œå½“ä»»åŠ¡éç«™ARYæ˜¯éæ ‡æ³¨çš„ï¼Œå¹¶ä¸”åœ¨æ—¶é—´ä¸Šä¸å‡åˆ†é…ç»™å­¦ä¹ è€…ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨å¥½å¥‡æ€§ä½œä¸ºä»»åŠ¡ç•Œé™æ¢æµ‹å’Œä¿ç•™è€transition tupleçš„ä¼˜å…ˆçº§ Ğ¼ĞµÑ‚Ñ€Ğ¸å…‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¸åŒçš„ç¼“å­˜ï¼šHybrid Reservoir Buffer with Task Separation (HRBTS)å’ŒHybrid Curious Buffer (HCB)ã€‚</li>
<li>results: æˆ‘ä»¬çš„æå‡ºçš„ç¼“å­˜ï¼Œä¸å¸¸è§çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ç»“åˆä½¿ç”¨ï¼Œå¯ä»¥å‡è½»ç¦»çº¿å¤šä»»åŠ¡è¿ç»­å¼ºåŒ–å­¦ä¹ ä¸­çš„ç¾éš¾æ€§å¿˜è®°é—®é¢˜ã€‚æˆ‘ä»¬åœ¨ä¸‰ç§ä¸åŒçš„ continual reinforcement learning è®¾ç½®ä¸­è¿›è¡Œäº†å®éªŒï¼Œå¹¶ä¸æœ€æ–°çš„ Hybrid Reservoir Buffer (HRB) å’Œ Multi-Time Scale Replay Buffer (MTR)è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æå‡ºçš„ç¼“å­˜åœ¨å¤§å¤šæ•°è®¾ç½®ä¸­ display better immunity to catastrophic forgetting compared to existing worksã€‚<details>
<summary>Abstract</summary>
In this work, we investigate the means of using curiosity on replay buffers to improve offline multi-task continual reinforcement learning when tasks, which are defined by the non-stationarity in the environment, are non labeled and not evenly exposed to the learner in time. In particular, we investigate the use of curiosity both as a tool for task boundary detection and as a priority metric when it comes to retaining old transition tuples, which we respectively use to propose two different buffers. Firstly, we propose a Hybrid Reservoir Buffer with Task Separation (HRBTS), where curiosity is used to detect task boundaries that are not known due to the task agnostic nature of the problem. Secondly, by using curiosity as a priority metric when it comes to retaining old transition tuples, a Hybrid Curious Buffer (HCB) is proposed. We ultimately show that these buffers, in conjunction with regular reinforcement learning algorithms, can be used to alleviate the catastrophic forgetting issue suffered by the state of the art on replay buffers when the agent's exposure to tasks is not equal along time. We evaluate catastrophic forgetting and the efficiency of our proposed buffers against the latest works such as the Hybrid Reservoir Buffer (HRB) and the Multi-Time Scale Replay Buffer (MTR) in three different continual reinforcement learning settings. Experiments were done on classical control tasks and Metaworld environment. Experiments show that our proposed replay buffers display better immunity to catastrophic forgetting compared to existing works in most of the settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è°ƒæŸ¥äº†ä½¿ç”¨å¥½å¥‡æ€§æ¥æ”¹è¿›ç¦»çº¿å¤šä»»åŠ¡è¿ç»­å¥–åŠ±å­¦ä¹ æ—¶çš„ç¼“å†²åŒºã€‚ç‰¹åˆ«æ˜¯ï¼Œå½“ä»»åŠ¡æ˜¯ç”±ç¯å¢ƒéç«™ç‚¹æ€§å†³å®šçš„ï¼Œè€Œä¸”æ— æ³•è¢«å­¦ä¹ è€…åœ¨æ—¶é—´ä¸Šå¹³å‡æš´éœ²çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥½å¥‡æ€§æ¥æ¢ç´¢ä»»åŠ¡è¾¹ç•Œå’Œä¼˜å…ˆçº§ç¼“å†²åŒºã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§ä¸åŒçš„ç¼“å†²åŒºï¼šé¦–å…ˆï¼Œæˆ‘ä»¬æå‡ºäº†æ··åˆå‚¨å­˜ç¼“å†²ï¼ˆHRBTSï¼‰ï¼Œå…¶ä¸­å¥½å¥‡æ€§ç”¨äºæ¢ç´¢ä»»åŠ¡è¾¹ç•Œï¼Œè€Œè¿™äº›è¾¹ç•Œç”±äºä»»åŠ¡agnosticçš„é—®é¢˜è€Œæ— æ³•è¢«çŸ¥é“ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥½å¥‡æ€§æ¥å†³å®šä¿ç•™è€çš„è½¬ç§»å¯¹è±¡ï¼Œå¹¶æå‡ºäº†æ··åˆå¥½å¥‡ç¼“å†²ï¼ˆHCBï¼‰ã€‚æˆ‘ä»¬æœ€ç»ˆè¡¨æ˜ï¼Œè¿™äº›ç¼“å†²åŒºï¼Œä¸å¸¸è§çš„å¥–åŠ±å­¦ä¹ ç®—æ³•ç»“åˆä½¿ç”¨ï¼Œå¯ä»¥è§£å†³ç”±ç°çŠ¶çš„ç¼“å†²åŒºæ‰€é‡åˆ°çš„æ…¢é€Ÿå¿˜è®°é—®é¢˜ã€‚æˆ‘ä»¬å¯¹ä¸‰ä¸ªä¸åŒçš„è¿ç»­å¥–åŠ±å­¦ä¹ è®¾ç½®è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åœ¨ ĞºĞ»Ğ°ÑÑĞ¸calæ§åˆ¶ä»»åŠ¡å’ŒMetaworldç¯å¢ƒè¿›è¡Œå®éªŒã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æå‡ºçš„ç¼“å†²åŒºåœ¨å¤§å¤šæ•°è®¾ç½®ä¸­è¡¨ç°å‡ºæ¯”ç°æœ‰å·¥ä½œæ›´å¥½çš„æŠ—å¿˜è®°æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Study-of-AI-Generated-GPT-4-and-Human-crafted-MCQs-in-Programming-Education"><a href="#A-Comparative-Study-of-AI-Generated-GPT-4-and-Human-crafted-MCQs-in-Programming-Education" class="headerlink" title="A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education"></a>A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03173">http://arxiv.org/abs/2312.03173</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Doughty, Zipiao Wan, Anishka Bompelli, Jubahed Qayum, Taozhi Wang, Juran Zhang, Yujia Zheng, Aidan Doyle, Pragnya Sridhar, Arav Agarwal, Christopher Bogart, Eric Keylor, Can Kultur, Jaromir Savelka, Majd Sakr</li>
<li>for:  Educators can use GPT-4 to generate multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education.</li>
<li>methods: The GPT-4 system uses a large language model to generate MCQs from high-level course context and module-level LOs.</li>
<li>results: The study found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors, and the generated MCQs appeared to be well-aligned with the LOs.Hereâ€™s the Chinese version:</li>
<li>for:  Educatorså¯ä»¥ä½¿ç”¨GPT-4æ¥ç”Ÿæˆä¸ç‰¹å®šå­¦ä¹ ç›®æ ‡(LOs)ç›¸å…³çš„å¤šé€‰é¢˜(MCQs)ï¼Œæ¥æ”¯æŒPythonç¼–ç¨‹è¯¾ç¨‹çš„é«˜ç­‰æ•™è‚²ã€‚</li>
<li>methods: GPT-4ç³»ç»Ÿä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æ¥ç”ŸæˆMCQsï¼Œä»é«˜çº§è¯¾ç¨‹èƒŒæ™¯å’ŒModuleçº§å­¦ä¹ ç›®æ ‡ä¸­ç”ŸæˆMCQsã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒGPT-4èƒ½å¤Ÿç”Ÿæˆ Clearè¯­è¨€ã€å”¯ä¸€æ­£ç¡®é€‰é¡¹å’Œé«˜è´¨é‡å¹²æ‰°è€…çš„MCQsï¼Œå¹¶ä¸”ç”Ÿæˆçš„MCQsä¸LOsç›¸å¯¹åŒ¹é…ã€‚<details>
<summary>Abstract</summary>
There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models (LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ•™è‚²è€…éœ€è¦ä¸æ–­å¼€å‘å’Œç»´æŠ¤æœ‰æ•ˆã€æ—¶å°šçš„è¯„ä¼°æ–¹æ³•ã€‚è™½ç„¶è®¡ç®—æ•™è‚²ä¸­ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç”Ÿæˆå’Œä¿ƒè¿›ç¼–ç¨‹ç»ƒä¹ æ–¹é¢å·²æœ‰ä¸€å®šç ”ç©¶ï¼Œä½†ä½¿ç”¨LLMç”Ÿæˆç¼–ç¨‹å¤šé€‰é—®é¢˜ï¼ˆMCQsï¼‰çš„ä½¿ç”¨å°šæœªå¾—åˆ°å¹¿æ³›æ¢è®¨ã€‚æˆ‘ä»¬å¯¹GPT-4çš„èƒ½åŠ›è¿›è¡Œäº†åˆ†æï¼Œä»¥ç”ŸæˆåŸºäºé«˜çº§è¯¾ç¨‹èƒŒæ™¯å’Œæ¨¡å—çº§å­¦ä¹ ç›®æ ‡ï¼ˆLOsï¼‰çš„MCQsã€‚æˆ‘ä»¬å¯¹651ä¸ªLLMç”Ÿæˆå’Œ449ä¸ªäººå·¥åˆ¶ä½œçš„MCQsè¿›è¡Œäº†è¯„ä¼°ï¼Œè¿™äº›MCQséƒ½ä¸246ä¸ªLOsç›¸å…³ã€‚æˆ‘ä»¬å‘ç°GPT-4èƒ½å¤Ÿç”Ÿæˆæ¸…æ™°çš„è¯­è¨€ã€å•é€‰æ­£ç¡®ç­”æ¡ˆå’Œé«˜è´¨é‡çš„å¹²æ‰°é€‰é¡¹ã€‚æˆ‘ä»¬è¿˜å‘ç°ç”Ÿæˆçš„MCQsä¸LOsä¹‹é—´å­˜åœ¨è‰¯å¥½çš„å»åˆã€‚æˆ‘ä»¬çš„å‘ç°å¯ä»¥å¸®åŠ©æ•™è‚²è€…é€šè¿‡åˆ©ç”¨å½“ä»Šæœ€å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹æ¥æ”¯æŒMCQä½œæ–‡çš„åŠªåŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="GPT-vs-Human-for-Scientific-Reviews-A-Dual-Source-Review-on-Applications-of-ChatGPT-in-Science"><a href="#GPT-vs-Human-for-Scientific-Reviews-A-Dual-Source-Review-on-Applications-of-ChatGPT-in-Science" class="headerlink" title="GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science"></a>GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03769">http://arxiv.org/abs/2312.03769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenxi Wu, Alan John Varghese, Vivek Oommen, George Em Karniadakis</li>
<li>for: è¿™ç§ç ”ç©¶æ—¨åœ¨æ¢è®¨ Large Language Models (LLMs) åœ¨ç§‘å­¦è¯„å®¡ä¸­çš„åº”ç”¨ï¼Œä»¥æé«˜è¯„å®¡æ•ˆç‡ã€é¿å…åè§ã€æ‰¾åˆ°äº¤å‰é¢†åŸŸçš„è¿æ¥å’Œå‘ç°æ–°è¶‹åŠ¿ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† 13 ç¯‡ GPT ç›¸å…³è®ºæ–‡ï¼Œç”±äººå·¥è¯„å®¡å’Œ SciSpace è¿›è¡Œè¯„å®¡ï¼Œç„¶åç”±ä¸‰ç§ä¸åŒç±»å‹çš„è¯„ä¼°è€…è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬ GPT-3.5ã€äººç¾¤å›¢é˜Ÿå’Œ GPT-4ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼ŒSciSpace çš„å›ç­”å’Œäººå·¥è¯„å®¡è€…çš„å›ç­”åœ¨å®¢è§‚é—®é¢˜ä¸­æœ‰50%çš„ä¸€è‡´æ€§ï¼ŒGPT-4 (æœ‰çŸ¥è¯†è¯„ä¼°è€…) ç»å¸¸å°†äººå·¥è¯„å®¡è€…è¯„ä¸ºæ›´é«˜çš„å‡†ç¡®æ€§ï¼Œè€Œ SciSpace åœ¨ç»“æ„ã€æ¸…æ™°æ€§å’Œå®Œæ•´æ€§æ–¹é¢è¢«è¯„ä¸ºæ›´é«˜ã€‚åœ¨ä¸»è§‚é—®é¢˜ä¸­ï¼Œæ— çŸ¥è¯†è¯„ä¼°è€… (GPT-3.5 å’Œäººç¾¤å›¢é˜Ÿ) å¯¹ SciSpace å’Œäººå·¥è¯„å®¡è€…çš„å›ç­”æœ‰å„ç§åå¥½ï¼Œä½† GPT-4 å¯¹å®ƒä»¬çš„å‡†ç¡®æ€§å’Œç»“æ„éƒ½æœ‰å¹³ç­‰çš„è¯„ä¼°ï¼Œä½†åå¥½ SciSpace çš„å®Œæ•´æ€§ã€‚<details>
<summary>Abstract</summary>
The new polymath Large Language Models (LLMs) can speed-up greatly scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 GPT-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50% of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ–°çš„å¤šæ‰èƒ½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å¤§å¹…æé«˜ç§‘å­¦è¯„å®¡ï¼Œå¯èƒ½ä½¿ç”¨æ›´å…¬å¹³çš„é‡åŒ–æŒ‡æ ‡ï¼Œä¿ƒè¿›äº¤å‰å­¦ç§‘è¿æ¥ï¼Œå¹¶æ‰¾åˆ°emerging trendå’Œç ”ç©¶ gap by analyzing large volumes of dataã€‚ç„¶è€Œï¼Œå½“å‰ï¼Œå®ƒä»¬ç¼ºä¹æ·±å…¥çš„å¤æ‚æ–¹æ³•ç†è§£ï¼Œå¯¹åˆ›æ–°æ€§çš„laimséš¾ä»¥è¯„ä¼°ï¼Œä¹Ÿæ— æ³•è¯„ä¼°ä¼¦ç†é—®é¢˜å’Œåˆ©ç›Šå†²çªã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è€ƒè™‘äº†13ä¸ªGPTç›¸å…³è®ºæ–‡ï¼Œåˆ†åˆ«ç”±äººç±»è¯„å®¡å’ŒSciSpaceè¿›è¡Œè¯„å®¡ï¼Œå…¶ä¸­è¯„å®¡ç»“æœç”±ä¸‰ç§ä¸åŒçš„è¯„ä¼°è€…è¯„ä¼°ï¼Œå³GPT-3.5ã€ä¸€ä¸ªæ‹¥æœ‰äººç¾¤å’ŒGPT-4ã€‚æˆ‘ä»¬å‘ç°ï¼ŒSciSpaceå¯¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¸Ğ²é—®é¢˜çš„å›ç­”ä¸äººç±»è¯„å®¡å‘˜çš„å›ç­”ç›¸ä¸€è‡´çš„æ¯”ä¾‹ä¸º50%ï¼ŒGPT-4ï¼ˆçŸ¥æƒ…è¯„ä¼°è€…ï¼‰ç»å¸¸å°†äººç±»è¯„å®¡å‘˜çš„å‡†ç¡®æ€§è¯„åˆ†é«˜äºSciSpaceï¼Œè€ŒSciSpaceåœ¨ç»“æ„ã€æ˜äº†å’Œå®Œæ•´æ€§æ–¹é¢çš„è¯„åˆ†é«˜äºäººç±»è¯„å®¡å‘˜ã€‚åœ¨ä¸»è§‚é—®é¢˜ä¸Šï¼Œæ— çŸ¥è¯„ä¼°è€…ï¼ˆGPT-3.5å’Œæ‹¥æœ‰äººç¾¤ï¼‰å¯¹SciSpaceå’Œäººç±»å›ç­”ä¹‹é—´æœ‰å˜åŒ–çš„åå¥½ï¼Œæ‹¥æœ‰äººç¾¤æ˜¾ç¤ºå¯¹äººç±»å›ç­”çš„åå¥½ï¼Œä½†GPT-4å¯¹ä¸¤è€…çš„å‡†ç¡®æ€§å’Œç»“æ„æ˜¯ä¸€è‡´çš„ï¼Œä½†å®ƒå¯¹SciSpaceçš„å®Œæ•´æ€§æœ‰æ›´é«˜çš„è¯„åˆ†ã€‚
</details></li>
</ul>
<hr>
<h2 id="FlexModel-A-Framework-for-Interpretability-of-Distributed-Large-Language-Models"><a href="#FlexModel-A-Framework-for-Interpretability-of-Distributed-Large-Language-Models" class="headerlink" title="FlexModel: A Framework for Interpretability of Distributed Large Language Models"></a>FlexModel: A Framework for Interpretability of Distributed Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03140">http://arxiv.org/abs/2312.03140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vectorinstitute/flex_model">https://github.com/vectorinstitute/flex_model</a></li>
<li>paper_authors: Matthew Choi, Muhammad Adil Asif, John Willes, David Emerson</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²æ‰€éœ€çš„ç¡¬ä»¶å‰ææ¡ä»¶ï¼Œå¹¶ä¸”å¢åŠ æ¨¡å‹ä¹‹é—´çš„äº¤äº’ï¼Œä»¥æé«˜è§£é‡Šæ€§å’Œè´£ä»»AIæŠ€æœ¯çš„ç ”ç©¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†FlexModelè½¯ä»¶åŒ…ï¼Œè¯¥åŒ…æä¾›äº†ä¸€ä¸ªæ˜“ç”¨çš„ç•Œé¢ï¼Œå¯ä»¥åœ¨å¤šä¸ªGPUå’Œå¤šä¸ªèŠ‚ç‚¹é…ç½®ä¸‹åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨¡å‹äº¤äº’ã€‚å®ƒä¸ç°æœ‰çš„æ¨¡å‹åˆ†å¸ƒåº“compatibleï¼Œå¹¶ä¸”å…è®¸ç”¨æˆ·æ³¨å†Œè‡ªå·±çš„ HookFunctionsï¼Œä»¥ä¾¿è½»æ¾åœ°ä¸åˆ†å¸ƒå¼æ¨¡å‹å†…éƒ¨è¿›è¡Œäº¤äº’ã€‚</li>
<li>results: æœ¬ç ”ç©¶é€šè¿‡FlexModelè½¯ä»¶åŒ…ï¼Œæé«˜äº†æ¨¡å‹äº¤äº’çš„è®¿é—®æ€§å’Œå¯ç”¨æ€§ï¼Œå¹¶ä¸”ä½¿å¾—æ›´å¤šçš„ç ”ç©¶äººå‘˜å¯ä»¥å‚ä¸åˆ°å¤§å‹ç¥ç»ç½‘ç»œé¢†åŸŸçš„ç ”ç©¶ä¸­ã€‚<details>
<summary>Abstract</summary>
With the growth of large language models, now incorporating billions of parameters, the hardware prerequisites for their training and deployment have seen a corresponding increase. Although existing tools facilitate model parallelization and distributed training, deeper model interactions, crucial for interpretability and responsible AI techniques, still demand thorough knowledge of distributed computing. This often hinders contributions from researchers with machine learning expertise but limited distributed computing background. Addressing this challenge, we present FlexModel, a software package providing a streamlined interface for engaging with models distributed across multi-GPU and multi-node configurations. The library is compatible with existing model distribution libraries and encapsulates PyTorch models. It exposes user-registerable HookFunctions to facilitate straightforward interaction with distributed model internals, bridging the gap between distributed and single-device model paradigms. Primarily, FlexModel enhances accessibility by democratizing model interactions and promotes more inclusive research in the domain of large-scale neural networks. The package is found at https://github.com/VectorInstitute/flex_model.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€å¤§è¯­è¨€æ¨¡å‹çš„å¢é•¿ï¼Œè®­ç»ƒå’Œéƒ¨ç½²çš„ç¡¬ä»¶å‰ææ¡ä»¶ä¹Ÿå‡ºç°äº†ç›¸åº”çš„å¢é•¿ã€‚è™½ç„¶ç°æœ‰çš„å·¥å…·å¯ä»¥å®ç°æ¨¡å‹å¹³è¡ŒåŒ–å’Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œä½†æ›´æ·±å…¥çš„æ¨¡å‹äº’åŠ¨ï¼Œå¯¹äºè§£é‡Šæ€§å’Œè´£ä»»AIæŠ€æœ¯æ¥è¯´ï¼Œä»ç„¶éœ€è¦æ·±å…¥çš„åˆ†å¸ƒå¼è®¡ç®—çŸ¥è¯†ã€‚è¿™ç»å¸¸é˜»ç¢äº†å…·æœ‰æœºå™¨å­¦ä¹ ä¸“ä¸šèƒŒæ™¯ä½†æœ‰é™çš„åˆ†å¸ƒå¼è®¡ç®—çŸ¥è¯†çš„ç ”ç©¶äººå‘˜å‚ä¸åˆ°è¿™ä¸ªé¢†åŸŸä¸­ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº† FlexModelï¼Œä¸€ä¸ªè½¯ä»¶åŒ…ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„æ¥å£ï¼Œå¯ä»¥åœ¨å¤šGPUå’Œå¤šèŠ‚ç‚¹é…ç½®ä¸‹åˆ†å¸ƒå¼çš„æ¨¡å‹ä¸­è¿›è¡Œäº’åŠ¨ã€‚è¯¥åº“ä¸ç°æœ‰çš„æ¨¡å‹åˆ†å¸ƒåº“å…¼å®¹ï¼Œå¯ä»¥åŒ…è£…PyTorchæ¨¡å‹ï¼Œå¹¶æä¾›äº†ç”¨æˆ·å¯æ³¨å†Œçš„ HookFunctionsï¼Œä»¥ä¾¿è½»æ¾åœ°ä¸åˆ†å¸ƒå¼æ¨¡å‹å†…éƒ¨è¿›è¡Œäº’åŠ¨ï¼Œä»è€Œè·¨è¶Šåˆ†å¸ƒå’Œå•è®¾å¤‡æ¨¡å‹ Ğ¿Ğ°Ñ€Ğ°Ğ´Ğ¸Ğ³Ğ¼Ñ‹ä¹‹é—´çš„å·®å¼‚ã€‚ä¸»è¦æ¥è¯´ï¼ŒFlexModelé€šè¿‡æ™®åŠæ¨¡å‹äº’åŠ¨ï¼Œæ‰©å¤§äº†ç ”ç©¶é¢†åŸŸçš„è®¿é—®æ€§ï¼Œå¹¶ä¿ƒè¿›äº†æ›´åŠ åŒ…å®¹çš„ç ”ç©¶åœ¨å¤§è§„æ¨¡ç¥ç»ç½‘ç»œé¢†åŸŸã€‚è¯¥åŒ…å¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ï¼šhttps://github.com/VectorInstitute/flex_modelã€‚
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Agents-using-Social-Choice-Theory"><a href="#Evaluating-Agents-using-Social-Choice-Theory" class="headerlink" title="Evaluating Agents using Social Choice Theory"></a>Evaluating Agents using Social Choice Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03121">http://arxiv.org/abs/2312.03121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/voting">https://github.com/google-deepmind/open_spiel/tree/master/open_spiel/python/voting</a></li>
<li>paper_authors: Marc Lanctot, Kate Larson, Yoram Bachrach, Luke Marris, Zun Li, Avishkar Bhoopchand, Thomas Anthony, Brian Tanner, Anna Koop</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤šä»»åŠ¡è¯„ä»·é—®é¢˜çš„å…±åŒç‰¹ç‚¹ï¼Œæå‡ºä¸€ç§åŸºäºé€‰ä¸¾ç†è®ºçš„è¯„ä»·æ¡†æ¶ï¼Œå³æŠ•ç¥¨ä¸ºè¯„ä»·ï¼ˆVasEï¼‰æ¡†æ¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å¤šä¸ªä»»åŠ¡çš„æ’åæˆ–å¯¹æ¯”æ¥ç”Ÿæˆæ€»è¯„ä»·ï¼Œå¹¶å°†è¯„ä»·å™¨çœ‹ä½œç¤¾ä¼šåˆ©ç›Šå‡½æ•°ï¼Œèƒ½å¤Ÿå€Ÿé‰´ç¤¾ä¼šé€‰æ‹©ç†è®º centuries çš„ç ”ç©¶æ¥ derivation principled è¯„ä»·æ¡†æ¶ã€‚</li>
<li>results: å®é™…åº”ç”¨ä¸­ï¼ŒVasE æ¡†æ¶èƒ½å¤Ÿæ›´åŠ ç¨³å®šå’Œé²æ£’ï¼Œå‘ç°è¯„ä»·æ•°æ®ä¸­ä¸å¯è§çš„æ€§è´¨ï¼Œé¢„æµ‹å¤æ‚å¤š player æ¸¸æˆçš„ç»“æœæ›´åŠ å‡†ç¡®ã€‚æ­¤å¤–ï¼Œæœ€å¤§æŠ½ç­¾æ³•å¯ä»¥æ»¡è¶³é‡è¦çš„ä¸€è‡´æ€§æ€§è´¨ï¼Œæ˜¯è®¡ç®—æ•ˆç‡é«˜ï¼ˆå‡ ä¹çº¿æ€§å¢é•¿ï¼‰çš„ã€‚<details>
<summary>Abstract</summary>
We argue that many general evaluation problems can be viewed through the lens of voting theory. Each task is interpreted as a separate voter, which requires only ordinal rankings or pairwise comparisons of agents to produce an overall evaluation. By viewing the aggregator as a social welfare function, we are able to leverage centuries of research in social choice theory to derive principled evaluation frameworks with axiomatic foundations. These evaluations are interpretable and flexible, while avoiding many of the problems currently facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE) framework across multiple settings, including reinforcement learning, large language models, and humans. In practice, we observe that VasE can be more robust than popular evaluation frameworks (Elo and Nash averaging), discovers properties in the evaluation data not evident from scores alone, and can predict outcomes better than Elo in a complex seven-player game. We identify one particular approach, maximal lotteries, that satisfies important consistency properties relevant to evaluation, is computationally efficient (polynomial in the size of the evaluation data), and identifies game-theoretic cycles.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è®¤ä¸ºè®¸å¤šæ€»è¯„é—®é¢˜å¯ä»¥é€šè¿‡é€‰ä¸¾ç†è®ºæ¥çœ‹å¾…ã€‚æ¯ä¸ªä»»åŠ¡è¢«è§†ä¸ºä¸€ä¸ªç‹¬ç«‹çš„é€‰æ°‘ï¼Œåªéœ€æä¾›æ’åºæˆ–å¯¹æ¯”ä¸¤ä¸ªä»£ç†æ¥ç”Ÿæˆæ€»è¯„ã€‚é€šè¿‡è§†ä¸ºç¤¾ä¼šåˆ©ç›Šå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ç¤¾ä¼šé€‰æ‹©ç†è®º centuries çš„ç ”ç©¶æ¥ derive åŸåˆ™æ€§çš„è¯„ä»·æ¡†æ¶ï¼Œå…¶æœ‰AXIOmatic åŸºç¡€ã€‚è¿™äº›è¯„ä»·å¯è¯»æ€§å’Œçµæ´»æ€§é«˜ï¼Œè€Œå…é™¤è®¸å¤šç°åœ¨è·¨ä»»åŠ¡è¯„ä»·æ‰€é¢ä¸´çš„é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªåœºæ™¯ä¸­åº”ç”¨äº† VasE æ¡†æ¶ï¼ŒåŒ…æ‹¬å¼ºåŒ–å­¦ä¹ ã€å¤§è¯­è¨€æ¨¡å‹å’Œäººç±»ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç° VasE å¯ä»¥æ¯”å—æ¬¢è¿çš„è¯„ä»·æ¡†æ¶ï¼ˆEloå’ŒNashå‡å€¼ï¼‰æ›´åŠ ç¨³å®šï¼Œæ£€æµ‹è¯„ä»·æ•°æ®ä¸­ä¸å¯è§çš„ç‰¹æ€§ï¼Œå¹¶åœ¨å¤æ‚çš„ä¸ƒäººæ¸¸æˆä¸­é¢„æµ‹ç»“æœæ›´åŠ å‡†ç¡®ã€‚æˆ‘ä»¬è¿˜å‘ç°ä¸€ç§ç‰¹æ®Šçš„æ–¹æ³•â€”â€”æœ€å¤§æŠ½ç­¾ï¼Œæ»¡è¶³è¯„ä»·ä¸­é‡è¦çš„ä¸€è‡´æ€§ç‰¹æ€§ï¼Œè®¡ç®—æ•ˆç‡é«˜ï¼ˆå¯¹è¯„ä»·æ•°æ®çš„å¤§å°ä¸ºå¤šé˜¶å¼ï¼‰ï¼Œå¹¶åœ¨æ¸¸æˆä¸­å‘ç°æ¸¸æˆç†è®ºå¾ªç¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="The-Landscape-of-Modern-Machine-Learning-A-Review-of-Machine-Distributed-and-Federated-Learning"><a href="#The-Landscape-of-Modern-Machine-Learning-A-Review-of-Machine-Distributed-and-Federated-Learning" class="headerlink" title="The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning"></a>The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03120">http://arxiv.org/abs/2312.03120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omer Subasi, Oceane Bel, Joseph Manzano, Kevin Barker</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ç°ä»£æœºå™¨å­¦ä¹ çš„ç»¼è¿°ï¼ŒåŒ…æ‹¬æœ€æ–°çš„é«˜çº§æœºå™¨å­¦ä¹ ç®—æ³•ã€åº”ç”¨å’Œæ¡†æ¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨é«˜æ€§èƒ½çš„å¤šå™¨ heterogeneous å¹¶è¡Œåˆ†å¸ƒå¼è®¡ç®—ç³»ç»Ÿå’Œå¤§é‡æ•°æ®ï¼Œæ¶‰åŠåˆ°å¹³è¡Œåˆ†å¸ƒå¼å­¦ä¹ ã€æ·±åº¦å­¦ä¹ å’Œè”åˆå­¦ä¹ ã€‚</li>
<li>results: æœ¬ç ”ç©¶æä¾›äº†ç°ä»£æœºå™¨å­¦ä¹ é¢†åŸŸçš„é«˜çº§æ¦‚è¿°ï¼Œå¯ä»¥ä½œä¸ºè¯¥é¢†åŸŸçš„å…¥é—¨æ•™æã€‚<details>
<summary>Abstract</summary>
With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
Note:* "modern machine learning" is translated as "ç°ä»£æœºå™¨å­¦ä¹ " (shÃ¬dÃ  jÄ«shÅ« xuÃ©xÃ­)* "heterogeneous" is translated as "å¤šæ ·çš„" (duÅyÃ ng de)* "parallel and distributed" is translated as "å¹¶è¡Œåˆ†å¸ƒçš„" ( hÃ©ngxÃ¬ bÄ›nzhÃ¹ de)* "ever increasing" is translated as "ä¸æ–­å¢é•¿" (bÃ¹dÃ n zhÃ¨ngchÃ¡ng)* "cutting-edge technology" is translated as "å‰æ²¿ç§‘æŠ€" (qiÃ¡nxiÄng kÄ“jÃ¬)* "scientific research" is translated as "ç§‘å­¦ç ”ç©¶" (kÄ“xuÃ© yÃ¡njiÅ«)* "consumer products" is translated as "æ¶ˆè´¹å“" (xiÄofÃ¨i pin)* "federated learning" is translated as "è”åˆå­¦ä¹ " (liÃ¡nhÃ© xuÃ©xÃ­)
</details></li>
</ul>
<hr>
<h2 id="Unknown-Sample-Discovery-for-Source-Free-Open-Set-Domain-Adaptation"><a href="#Unknown-Sample-Discovery-for-Source-Free-Open-Set-Domain-Adaptation" class="headerlink" title="Unknown Sample Discovery for Source Free Open Set Domain Adaptation"></a>Unknown Sample Discovery for Source Free Open Set Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03767">http://arxiv.org/abs/2312.03767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chowdhury Sadman Jahan, Andreas Savakis</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨åº”å¯¹å¼€æ”¾é›†é¢†åŸŸé€‚æŸï¼ˆOSDAï¼‰ä¸­ï¼Œå°†æºé¢†åŸŸè®­ç»ƒçš„æ¨¡å‹é€‚æŸåˆ°ç›®æ ‡é¢†åŸŸï¼Œå¹¶ä¸”åœ¨ç›®æ ‡é¢†åŸŸä¸­è¿›è¡Œåˆ†ç±»ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸ªç ”ç©¶æ¢è®¨äº†æ— æºé¢†åŸŸï¼ˆSF-OSDAï¼‰æŠ€æœ¯ï¼Œä¸éœ€è¦è®¿é—®æºé¢†åŸŸæ ·æœ¬ï¼Œä½†æ˜¯ç°æœ‰çš„SF-OSDAæ–¹æ³•ä»…ä½¿ç”¨ç›®æ ‡é¢†åŸŸä¸­å·²çŸ¥çš„ç±»åˆ«è¿›è¡Œé€‚æŸï¼Œå¹¶ä¸”åœ¨æ¨æ–­åé€‚æŸè¿‡ç¨‹ä¸­éœ€è¦è®¿é—®æ•´ä¸ªç›®æ ‡é¢†åŸŸã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„æ¶æ„ï¼Œå°†å­¦ç”Ÿæ¨¡å‹é€‚æŸåˆ°ç›®æ ‡é¢†åŸŸä¸­ï¼Œå¹¶ä¸”ä½¿ç”¨äº†æ—¶é—´ensembleçš„æ•™å¸ˆæ¨¡å‹æ¥è¿›è¡Œå·²çŸ¥ sample separationå’Œé€‚æŸã€‚å®ƒè¿˜ä½¿ç”¨äº†co-trainingå’Œæ—¶é—´ä¸€è‡´æ€§æ¥å¸®åŠ©å­¦ç”Ÿæ¨¡å‹åœ¨ç›®æ ‡é¢†åŸŸä¸­é€‚æŸã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªæ–¹æ³•ï¼ˆUSDï¼‰åœ¨æ¯”è¾ƒSF-OSDAæ–¹æ³•å’ŒOSDAæ–¹æ³•æ—¶è¡¨ç°æ›´å¥½ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„OSDAæ¨¡å‹åœ¨é€‚æŸè¿‡ç¨‹ä¸­ç›¸æ¯”ï¼Œå…·æœ‰è¾ƒå¥½çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Open Set Domain Adaptation (OSDA) aims to adapt a model trained on a source domain to a target domain that undergoes distribution shift and contains samples from novel classes outside the source domain. Source-free OSDA (SF-OSDA) techniques eliminate the need to access source domain samples, but current SF-OSDA methods utilize only the known classes in the target domain for adaptation, and require access to the entire target domain even during inference after adaptation, to make the distinction between known and unknown samples. In this paper, we introduce Unknown Sample Discovery (USD) as an SF-OSDA method that utilizes a temporally ensembled teacher model to conduct known-unknown target sample separation and adapts the student model to the target domain over all classes using co-training and temporal consistency between the teacher and the student. USD promotes Jensen-Shannon distance (JSD) as an effective measure for known-unknown sample separation. Our teacher-student framework significantly reduces error accumulation resulting from imperfect known-unknown sample separation, while curriculum guidance helps to reliably learn the distinction between target known and target unknown subspaces. USD appends the target model with an unknown class node, thus readily classifying a target sample into any of the known or unknown classes in subsequent post-adaptation inference stages. Empirical results show that USD is superior to existing SF-OSDA methods and is competitive with current OSDA models that utilize both source and target domains during adaptation.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Incidental-Polysemanticity"><a href="#Incidental-Polysemanticity" class="headerlink" title="Incidental Polysemanticity"></a>Incidental Polysemanticity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03096">http://arxiv.org/abs/2312.03096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tmychow/incidental-polysemanticity">https://github.com/tmychow/incidental-polysemanticity</a></li>
<li>paper_authors: Victor Lecomte, Kushal Thaman, Trevor Chow, Rylan Schaeffer, Sanmi Koyejo</li>
<li>for: This paper aims to provide a second origin story for polysemantic neurons in deep networks, which can arise incidentally even when there are enough neurons to represent all features in the data.</li>
<li>methods: The paper uses a combination of theory and experiments to demonstrate the existence of incidental polysemanticity, and to show how training dynamics can strengthen such overlap.</li>
<li>results: The paper finds that incidental polysemanticity can occur even when there are ample neurons to represent all features in the data, and that this type of polysemanticity can be a significant obstacle to interpretability of task-optimized deep networks.<details>
<summary>Abstract</summary>
Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more "features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this \textit{incidental polysemanticity}.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šä¹‰neuronï¼ˆneuron Activate å¤šä¸ªä¸ç›¸å…³ç‰¹å¾ï¼‰è¢«è§†ä¸ºæ·±åº¦ç½‘ç»œè§£é‡Šæ€§çš„ä¸»è¦éšœç¢ï¼Œå¸¦æ¥äººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜ã€‚ ĞºĞ»Ğ°ÑÑĞ¸çš„èµ·æºæ•…äº‹æ˜¯æ•°æ®åŒ…å«æ›´å¤šçš„â€œç‰¹å¾â€ than neuronï¼Œè¿™ä½¿å¾—å­¦ä¹ å®Œæˆä»»åŠ¡çš„ç½‘ç»œå¼ºåˆ¶åˆå¹¶å¤šä¸ªä¸ç›¸å…³çš„ç‰¹å¾åˆ°åŒä¸€ä¸ªneuronä¸Šï¼Œå¨èƒæˆ‘ä»¬ç†è§£ç½‘ç»œå†…éƒ¨å¤„ç†çš„èƒ½åŠ›ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç¬¬äºŒç§å’Œéç›¸äº’æ’æ–¥çš„èµ·æºæ•…äº‹ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œeven when there are enough neurons to represent all features in the dataï¼ŒRandom initializationå¯ä»¥ï¼Œé€šè¿‡å·§åˆaloneï¼Œåˆå§‹åŒ–å¤šä¸ªç‰¹å¾åˆ°åŒä¸€ä¸ªneuronä¸Šï¼Œå¹¶ä¸”è®­ç»ƒå‰‚ä¼šå¼ºåŒ–è¿™ç§é‡å ã€‚ç”±äºå…¶èµ·æºï¼Œæˆ‘ä»¬ç§°è¿™ç§ç°è±¡ä¸ºâ€œå¶ç„¶çš„å¤šä¹‰â€ï¼ˆincidental polysemyï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Similarity-based-Knowledge-Transfer-for-Cross-Domain-Reinforcement-Learning"><a href="#Similarity-based-Knowledge-Transfer-for-Cross-Domain-Reinforcement-Learning" class="headerlink" title="Similarity-based Knowledge Transfer for Cross-Domain Reinforcement Learning"></a>Similarity-based Knowledge Transfer for Cross-Domain Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03764">http://arxiv.org/abs/2312.03764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio A. Serrano, Jose Martinez-Carranza, L. Enrique Sucar</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ç ”ç©¶å¦‚ä½•åœ¨å¦ä¸€ä¸ªä»»åŠ¡ç©ºé—´ä¸­ä¼ é€’çŸ¥è¯†ï¼Œä»¥åŠ é€Ÿå­¦ä¹ ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§ semi-supervised alignment lossï¼Œç”¨äºåº¦é‡ä¸åŒç©ºé—´ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶å°†å…¶ç”¨äºé€‰æ‹©é€‚åˆçš„çŸ¥è¯†æ¥æé«˜å­¦ä¹ Agentçš„æ€§èƒ½ã€‚</li>
<li>results: æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸€ç»„å¤šæ ·åŒ–çš„ Mujoco æ§åˆ¶ä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶æ˜¾ç¤ºäº†å…¶åœ¨æ— éœ€ä¸“å®¶æ”¿ç­–æŒ‡å¯¼ä¸‹é€‰æ‹©å’Œä¼ é€’çŸ¥è¯†çš„ç¨³å®šæ€§ã€‚<details>
<summary>Abstract</summary>
Transferring knowledge in cross-domain reinforcement learning is a challenging setting in which learning is accelerated by reusing knowledge from a task with different observation and/or action space. However, it is often necessary to carefully select the source of knowledge for the receiving end to benefit from the transfer process. In this article, we study how to measure the similarity between cross-domain reinforcement learning tasks to select a source of knowledge that will improve the performance of the learning agent. We developed a semi-supervised alignment loss to match different spaces with a set of encoder-decoders, and use them to measure similarity and transfer policies across tasks. In comparison to prior works, our method does not require data to be aligned, paired or collected by expert policies. Experimental results, on a set of varied Mujoco control tasks, show the robustness of our method in effectively selecting and transferring knowledge, without the supervision of a tailored set of source tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
è½¬ç§»çŸ¥è¯†åœ¨è·¨é¢†åŸŸå¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ä¸ªæŒ‘æˆ˜çš„è®¾å®šï¼Œåœ¨å…¶ä¸­å­¦ä¹ é€Ÿåº¦å—åˆ°ä¸åŒè§‚å¯Ÿç©ºé—´å’Œ/æˆ–è¡ŒåŠ¨ç©ºé—´çš„çŸ¥è¯†é‡ç”¨çš„å½±å“ã€‚ç„¶è€Œï¼Œé€‰æ‹©æ”¶åˆ°çŸ¥è¯†çš„æºæ˜¯éå¸¸é‡è¦ï¼Œä»¥ä¾¿æ¥å—çŸ¥è¯†ä¼ é€’è¿‡ç¨‹ä¸­çš„æ”¹è¿›ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶å¦‚ä½•æµ‹é‡è·¨é¢†åŸŸå¼ºåŒ–å­¦ä¹ ä»»åŠ¡ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä»¥ä¾¿é€‰æ‹©ä¸€ä¸ªèƒ½å¤Ÿæé«˜å­¦ä¹ ä»£ç†çš„çŸ¥è¯†æºã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŠç›‘ç£å¯¹å‡†æŸå¤±ï¼Œå°†ä¸åŒç©ºé—´åŒ¹é…åˆ°ä¸€ç»„ç¼–ç å™¨-è§£ç å™¨ä¸­ï¼Œå¹¶ç”¨å…¶æ¥æµ‹é‡ç›¸ä¼¼æ€§å’Œä¼ é€’ç­–ç•¥ across tasksã€‚ä¸å…ˆå‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦æ•°æ®è¿›è¡Œå¯¹é½ã€é…å¯¹æˆ–ç”±ä¸“å®¶æ”¿ç­–è¿›è¡Œç›‘ç£ã€‚å®éªŒç»“æœï¼Œåœ¨ä¸€ç»„å˜åŒ–çš„ MuJoCo æ§åˆ¶ä»»åŠ¡ä¸Šï¼Œæ˜¾ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸åŒä»»åŠ¡ä¹‹é—´é€‰æ‹©å’Œä¼ é€’çŸ¥è¯†çš„ç¨³å®šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="RESIN-EDITOR-A-Schema-guided-Hierarchical-Event-Graph-Visualizer-and-Editor"><a href="#RESIN-EDITOR-A-Schema-guided-Hierarchical-Event-Graph-Visualizer-and-Editor" class="headerlink" title="RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor"></a>RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03093">http://arxiv.org/abs/2312.03093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blender-nlp/resin-editor">https://github.com/blender-nlp/resin-editor</a></li>
<li>paper_authors: Khanh Duy Nguyen, Zixuan Zhang, Reece Suchocki, Sha Li, Martha Palmer, Susan Brown, Jiawei Han, Heng Ji</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æè¿°ä¸€ç§åä¸ºRESIGN-EDITORçš„äº’åŠ¨äº‹ä»¶å›¾åƒå’Œç¼–è¾‘å™¨ï¼Œç”¨äºåˆ†æå¤æ‚äº‹ä»¶ã€‚</li>
<li>methods: è¯¥ç³»ç»Ÿä½¿ç”¨äº†äººå·¥çº¦æŸäº‹ä»¶æ¨¡å¼æ¥å¼•å¯¼ä»å¤šåª’ä½“å’Œå¤šæ–‡æ¡£æ–°é—»å›¢clusterä¸­æå–çš„å±‚æ¬¡äº‹ä»¶å›¾ã€‚</li>
<li>results: åœ¨è¯„ä¼°RESIGN-EDITORçš„æ•ˆæœæ—¶ï¼Œæˆ‘ä»¬è¡¨æ˜äº†è¯¥å·¥å…·åœ¨ç†è§£å¤æ‚äº‹ä»¶å’Œæé«˜ç³»ç»Ÿæ€§èƒ½çš„èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
In this paper, we present RESIN-EDITOR, an interactive event graph visualizer and editor designed for analyzing complex events. Our RESIN-EDITOR system allows users to render and freely edit hierarchical event graphs extracted from multimedia and multi-document news clusters with guidance from human-curated event schemas. RESIN-EDITOR's unique features include hierarchical graph visualization, comprehensive source tracing, and interactive user editing, which is more powerful and versatile than existing Information Extraction (IE) visualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in which our tool is effective in understanding complex events and enhancing system performance. The source code, a video demonstration, and a live website for RESIN-EDITOR have been made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†RESIME-EDITORï¼Œä¸€ç§å¯äº¤äº’åœ°è§†è§‰åŒ–å’Œç¼–è¾‘äº‹ä»¶å›¾çš„ç³»ç»Ÿï¼Œç”¨äºåˆ†æå¤æ‚äº‹ä»¶ã€‚æˆ‘ä»¬çš„RESIME-EDITORç³»ç»Ÿå…è®¸ç”¨æˆ·è‡ªç”±åœ°ç¼–è¾‘åµŒå…¥å¼äº‹ä»¶å›¾ï¼Œä»¥è·å¾—äººç±»ç­›é€‰çš„äº‹ä»¶æ¨¡å¼çš„æŒ‡å¯¼ã€‚RESIME-EDITORçš„ç‹¬ç‰¹ç‰¹ç‚¹åŒ…æ‹¬å±‚æ¬¡å›¾è¡¨ç¤ºã€å…¨æºè¿½è¸ªå’Œäº¤äº’å¼ç”¨æˆ·ç¼–è¾‘ï¼Œè¿™äº›ç‰¹ç‚¹æ¯”ç°æœ‰çš„ä¿¡æ¯EXTRACTIONï¼ˆIEï¼‰è§†è§‰åŒ–å·¥å…·æ›´åŠ å¼ºå¤§å’Œçµæ´»ã€‚åœ¨æˆ‘ä»¬å¯¹RESIME-EDITORçš„è¯„ä¼°ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥å·¥å…·åœ¨ç†è§£å¤æ‚äº‹ä»¶å’Œæé«˜ç³»ç»Ÿæ€§èƒ½æ–¹é¢çš„æ•ˆæœã€‚æºä»£ç ã€è§†é¢‘ç¤ºä¾‹å’ŒRESIME-EDITORçš„åœ¨çº¿ç½‘ç«™éƒ½å·²ç»å…¬å¼€å‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Colour-versus-Shape-Goal-Misgeneralization-in-Reinforcement-Learning-A-Case-Study"><a href="#Colour-versus-Shape-Goal-Misgeneralization-in-Reinforcement-Learning-A-Case-Study" class="headerlink" title="Colour versus Shape Goal Misgeneralization in Reinforcement Learning: A Case Study"></a>Colour versus Shape Goal Misgeneralization in Reinforcement Learning: A Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03762">http://arxiv.org/abs/2312.03762</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KarolisRam/colour-shape-goal-misgeneralization">https://github.com/KarolisRam/colour-shape-goal-misgeneralization</a></li>
<li>paper_authors: Karolis Ramanauskas, Ã–zgÃ¼r ÅimÅŸek</li>
<li>for: ç ”ç©¶ colour versus shape goal misgeneralization çš„è¡Œä¸º</li>
<li>methods: ä½¿ç”¨ Procgen Maze ç¯å¢ƒï¼Œè®­ç»ƒè¶…è¿‡ 1,000 ä¸ªä»£ç†ï¼Œå¹¶è¯„ä¼°å…¶åœ¨è¶…è¿‡ 10 ä¸‡é›†çš„è¯è¯­ä¸­çš„è¡¨ç°</li>
<li>results: å‘ç°ä»£ç†é€šè¿‡ç‰¹å®šçš„è‰²é“æ¸ é“æ¥æ¢æµ‹ç›®æ ‡ç‰©ä½“ï¼Œè¿™æ˜¯ä¸€ç§æ„å¤–çš„é€‰æ‹©ï¼›åŒæ—¶ï¼Œç”±äº underspecificationï¼Œä»£ç†çš„åå¥½ä¼šéšç€ä¸åŒçš„éšæœºç§å­é‡æ–°è®­ç»ƒè€Œæ”¹å˜ï¼›æœ€åï¼Œé€šè¿‡è®­ç»ƒéšæœºç§å­æ¥è¯æ˜å­˜åœ¨å¼‚å¸¸è¡Œä¸ºçš„å¼‚å¸¸ç‚¹ã€‚<details>
<summary>Abstract</summary>
We explore colour versus shape goal misgeneralization originally demonstrated by Di Langosco et al. (2022) in the Procgen Maze environment, where, given an ambiguous choice, the agents seem to prefer generalization based on colour rather than shape. After training over 1,000 agents in a simplified version of the environment and evaluating them on over 10 million episodes, we conclude that the behaviour can be attributed to the agents learning to detect the goal object through a specific colour channel. This choice is arbitrary. Additionally, we show how, due to underspecification, the preferences can change when retraining the agents using exactly the same procedure except for using a different random seed for the training run. Finally, we demonstrate the existence of outliers in out-of-distribution behaviour based on training random seed alone.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶äº†é¢œè‰²vså½¢çŠ¶ç›®æ ‡æ€»ç»“ï¼Œç”±Di Langosco et al. (2022)åœ¨Procgen Mazeç¯å¢ƒä¸­åŸå§‹ç¤ºç¤ºå‡ºçš„é—®é¢˜ï¼Œåœ¨ambiguousé€‰æ‹©æ—¶ï¼Œä»£ç†äººåå¥½åŸºäºé¢œè‰²è€Œéå½¢çŠ¶ã€‚æˆ‘ä»¬åœ¨ç®€åŒ–ç‰ˆç¯å¢ƒä¸­è®­ç»ƒäº†1,000ä¸ªä»£ç†äººï¼Œå¹¶è¯„ä¼°äº†ä»–ä»¬åœ¨è¶…è¿‡1000ä¸‡é›†çš„episodeä¸­çš„è¡Œä¸ºã€‚æˆ‘ä»¬ç»“è®ºæ˜¯ï¼Œä»£ç†äººé€šè¿‡ç‰¹å®šçš„é¢œè‰²é€šé“æ¢æµ‹ç›®æ ‡ç‰©ä½“ã€‚è¿™ç§é€‰æ‹©æ˜¯æ„å¤–çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ç”±äºä¸å……åˆ†è§„å®šï¼Œä»£ç†äººçš„åå¥½å¯ä»¥é€šè¿‡é‡æ–°è®­ç»ƒä½¿ç”¨ç›¸åŒçš„è¿‡ç¨‹è€Œæ”¹å˜ï¼Œåªæ˜¯ä½¿ç”¨ä¸åŒçš„éšæœºç§å­æ¥æ§åˆ¶è®­ç»ƒè¿è¡Œã€‚æœ€åï¼Œæˆ‘ä»¬ç¤ºå‡ºäº†è®­ç»ƒéšæœºç§å­aloneçš„å¤–liersè¡Œä¸ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Clinical-Notes-Reveal-Physician-Fatigue"><a href="#Clinical-Notes-Reveal-Physician-Fatigue" class="headerlink" title="Clinical Notes Reveal Physician Fatigue"></a>Clinical Notes Reveal Physician Fatigue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03077">http://arxiv.org/abs/2312.03077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao-Chun Hsu, Ziad Obermeyer, Chenhao Tan</li>
<li>for: The paper aims to identify notes written by fatigued physicians and understand the impact of physician fatigue on decision-making and patient outcomes.</li>
<li>methods: The authors use a machine learning model to analyze notes from 129,228 emergency room visits and identify patterns associated with fatigued physicians. They also compare the performance of human physicians and language models (LLMs) in generating notes.</li>
<li>results: The model accurately identifies notes written by fatigued physicians and flags notes written in other high-fatigue settings. The authors find that notes written by fatigued physicians have lower yield of testing for heart attack and higher predicted fatigue for Black and Hispanic patients. Additionally, they find that LLM-written notes have higher predicted fatigue than real physiciansâ€™ notes, suggesting that LLMs may introduce distortions in generated text.<details>
<summary>Abstract</summary>
Physicians write notes about patients. In doing so, they reveal much about themselves. Using data from 129,228 emergency room visits, we train a model to identify notes written by fatigued physicians -- those who worked 5 or more of the prior 7 days. In a hold-out set, the model accurately identifies notes written by these high-workload physicians, and also flags notes written in other high-fatigue settings: on overnight shifts, and after high patient volumes. Model predictions also correlate with worse decision-making on at least one important metric: yield of testing for heart attack is 18% lower with each standard deviation increase in model-predicted fatigue. Finally, the model indicates that notes written about Black and Hispanic patients have 12% and 21% higher predicted fatigue than Whites -- larger than overnight vs. daytime differences. These results have an important implication for large language models (LLMs). Our model indicates that fatigued doctors write more predictable notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs work, we find that LLM-written notes have 17% higher predicted fatigue than real physicians' notes. This indicates that LLMs may introduce distortions in generated text that are not yet fully understood.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŒ»ç”Ÿå†™ç—…äººç—‡çŠ¶è®°å½•æ—¶ï¼Œä¼šé€éœ²å‡ºè‡ªå·±ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬ä½¿ç”¨129,228ä¸ªæ€¥è¯Šå®¤è®¿é—®æ•°æ®ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥å‡†ç¡®åœ°è¯†åˆ«åŠ³ç´¯åŒ»ç”Ÿï¼ˆåœ¨è¿‡å»7å¤©å†…å·¥ä½œ5å¤©æˆ–ä»¥ä¸Šï¼‰å†™çš„ç—‡çŠ¶è®°å½•ã€‚åœ¨æµ‹è¯•é›†ä¸­ï¼Œæ¨¡å‹å¯ä»¥å‡†ç¡®åœ°è¯†åˆ«é«˜å·¥ä½œè·å‹åŒ»ç”Ÿçš„ç—‡çŠ¶è®°å½•ï¼Œå¹¶ä¸”å¯ä»¥æ£€æµ‹é«˜åŠ³ç´¯æƒ…å†µä¸‹çš„å…¶ä»–ç—‡çŠ¶è®°å½•ï¼Œå¦‚å¤œç­å’Œé«˜ç—…äººé‡ã€‚æ¨¡å‹é¢„æµ‹ä¹Ÿä¸é‡è¦æŒ‡æ ‡ä¹‹ä¸€çš„å†³ç­–è´¨é‡æœ‰æ­£ç›¸å…³ï¼šå¯¹äºå¿ƒè‚ºç—…æ£€æµ‹çš„é‡‡æ ·ç‡ï¼Œä¸æ¨¡å‹é¢„æµ‹çš„åŠ³ç´¯ç¨‹åº¦ç›¸å¯¹é™ä½18%ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜è¡¨æ˜ï¼Œå…³äºé»‘äººå’Œè¥¿ç­ç‰™è£”æ‚£è€…çš„ç—‡çŠ¶è®°å½•ä¼šæœ‰12%å’Œ21%æ›´é«˜çš„é¢„æµ‹åŠ³ç´¯ç¨‹åº¦ï¼Œæ¯”ç™½äººæ‚£è€…çš„ç—‡çŠ¶è®°å½•æ›´é«˜ã€‚è¿™äº›ç»“æœæœ‰é‡è¦çš„åº”ç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚æˆ‘ä»¬çš„æ¨¡å‹è¡¨æ˜ï¼ŒåŠ³ç´¯åŒ»ç”Ÿå†™çš„ç—‡çŠ¶è®°å½•æ›´åŠ é¢„æµ‹å¯é ï¼Œå› ä¸ºword predictionæ˜¯LLMçš„æ ¸å¿ƒã€‚æˆ‘ä»¬å‘ç°ï¼ŒLLMå†™çš„ç—‡çŠ¶è®°å½•çš„é¢„æµ‹åŠ³ç´¯ç¨‹åº¦æ¯”å®é™…åŒ»ç”Ÿå†™çš„ç—‡çŠ¶è®°å½•é«˜17%ã€‚è¿™è¡¨æ˜ï¼ŒLLMå¯èƒ½ä¼šåœ¨ç”Ÿæˆæ–‡æœ¬ä¸­å¼•å…¥æœªçŸ¥çš„æ‰­æ›²ã€‚
</details></li>
</ul>
<hr>
<h2 id="Imitating-Shortest-Paths-in-Simulation-Enables-Effective-Navigation-and-Manipulation-in-the-Real-World"><a href="#Imitating-Shortest-Paths-in-Simulation-Enables-Effective-Navigation-and-Manipulation-in-the-Real-World" class="headerlink" title="Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World"></a>Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02976">http://arxiv.org/abs/2312.02976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, Ranjay Krishna, Dustin Schwenk, Eli VanderBilt, Aniruddha Kembhavi</li>
<li>For: The paper is written to train modern embodied agents using imitation learning with shortest-path planners in simulation, and to demonstrate that these agents can proficiently navigate, explore, and manipulate objects in both simulation and the real world using only RGB sensors.* Methods: The paper uses a transformer-based, end-to-end architecture called SPOC, which is paired with extensive image augmentation and millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets.* Results: The paper shows that the proposed method can produce agents that can proficiently navigate, explore, and manipulate objects in both simulation and the real world using only RGB sensors, and that the method is effective and efficient, with the ability to generalize to new environments and tasks.Hereâ€™s the same information in Simplified Chinese:* For: è®ºæ–‡æ˜¯ä¸ºäº†ä½¿ç”¨ä¼˜åŒ–çš„äººå·¥æ™ºèƒ½è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨å®é™…ç¯å¢ƒä¸­æµ‹è¯•å…¶èƒ½å¤Ÿå¿«é€Ÿå’Œæœ‰æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚* Methods: è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºè½¬æ¢å™¨çš„ã€ç«¯åˆ°ç«¯çš„æ¶æ„ï¼Œç§°ä¸ºSPOCï¼Œå¹¶ä¸å…¶æ­é…äº†å¹¿æ³›çš„å›¾åƒå¢å¼ºå’Œå¤§é‡çš„å¸§æ•°ã€‚* Results: è®ºæ–‡æ˜¾ç¤ºäº†è¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆèƒ½å¤Ÿå¿«é€Ÿå’Œæœ‰æ•ˆåœ°åœ¨å®é™…ç¯å¢ƒä¸­å®Œæˆä»»åŠ¡çš„ä»£ç†äººï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ–°ç¯å¢ƒä¸­æ‰©å±•å’Œé€‚åº”ã€‚<details>
<summary>Abstract</summary>
Reinforcement learning (RL) with dense rewards and imitation learning (IL) with human-generated trajectories are the most widely used approaches for training modern embodied agents. RL requires extensive reward shaping and auxiliary losses and is often too slow and ineffective for long-horizon tasks. While IL with human supervision is effective, collecting human trajectories at scale is extremely expensive. In this work, we show that imitating shortest-path planners in simulation produces agents that, given a language instruction, can proficiently navigate, explore, and manipulate objects in both simulation and in the real world using only RGB sensors (no depth map or GPS coordinates). This surprising result is enabled by our end-to-end, transformer-based, SPOC architecture, powerful visual encoders paired with extensive image augmentation, and the dramatic scale and diversity of our training data: millions of frames of shortest-path-expert trajectories collected inside approximately 200,000 procedurally generated houses containing 40,000 unique 3D assets. Our models, data, training code, and newly proposed 10-task benchmarking suite CHORES will be open-sourced.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£embodiedæ™ºèƒ½å™¨å‹é€šå¸¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œæ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä¸¤ç§æ–¹æ³•è®­ç»ƒã€‚RLéœ€è¦å¹¿æ³›çš„å¥–åŠ±æ‰­æ›²å’Œè¾…åŠ©æŸå¤±ï¼Œç»å¸¸å¤ªæ…¢å’Œä¸å…·æœ‰æ•ˆæœï¼Œè€ŒILé€šå¸¸éœ€è¦äººå·¥æŒ‡å¯¼ï¼Œæ”¶é›†äººç±»è½¨è¿¹æ˜¯éå¸¸æ˜‚è´µçš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†åœ¨æ¨¡æ‹Ÿä¸­imiterçŸ­est-pathè§„åˆ’å™¨çš„imitatingå¯ä»¥ä½¿å¾—ï¼Œç»™å‡ºè¯­è¨€æŒ‡ä»¤ï¼Œæ™ºèƒ½å™¨å‹å¯ä»¥å‡†ç¡®åœ°å¯¼èˆªã€æ¢ç´¢å’Œæ“çºµç‰©ä½“ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨RGBæ„ŸçŸ¥å™¨ï¼ˆæ²¡æœ‰æ·±åº¦åœ°å›¾æˆ–GPSåæ ‡ï¼‰ã€‚è¿™ä¸€ç»“æœæ˜¯ç”±æˆ‘ä»¬çš„ç«¯åˆ°ç«¯ã€è½¬æ¢å™¨åŸºäºçš„SPOCæ¶æ„ã€å¼ºå¤§çš„è§†è§‰ç¼–ç å™¨å’Œå¹¿æ³›çš„å›¾åƒæ‰©å±•æ‰€å¯ç”¨ã€‚æˆ‘ä»¬çš„æ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒä»£ç å’Œæ–°æå‡ºçš„10ä»»åŠ¡benchmarking suite CHORESéƒ½å°†è¢«å¼€æºã€‚
</details></li>
</ul>
<hr>
<h2 id="Dexterous-Functional-Grasping"><a href="#Dexterous-Functional-Grasping" class="headerlink" title="Dexterous Functional Grasping"></a>Dexterous Functional Grasping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02975">http://arxiv.org/abs/2312.02975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ananye Agarwal, Shagun Uppal, Kenneth Shaw, Deepak Pathak</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ç»“åˆäººå·¥æ™ºèƒ½å’Œæœºå™¨äººæ§åˆ¶æŠ€æœ¯ï¼Œå®ç°åœ¨è‡ªç„¶ç¯å¢ƒä¸­å¯¹ç‰©ä½“è¿›è¡ŒåŠŸèƒ½ graspingã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨æ¨¡å—åŒ–æ–¹æ³•ï¼Œé¦–å…ˆè·å–å¯¹è±¡çš„åŠŸèƒ½å¯è¡Œæ€§ï¼Œç„¶åä½¿ç”¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­åŸ¹å…»çš„ä½çº§ç­–ç•¥æ¥æŠ“å–å®ƒã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æå‡ºäº†ä¸€ç§ä½¿ç”¨ eigengrasps æ¥å‡å°‘äººå·¥æ•°æ®çš„æœç´¢ç©ºé—´ï¼Œä»¥å®ç°æ›´ç¨³å®šå’Œç‰©ç†ä¸Šæ›´çœŸå®çš„è¿åŠ¨ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨ eigengrasps å¯ä»¥åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­å‡»è´¥åŸºelineï¼Œå¹¶åœ¨å®é™…ç¯å¢ƒä¸­ä¸äººç±»æ“ä½œå‘˜è¿›è¡Œæ¯”è¾ƒï¼Œæˆ–è€…è¶…è¶Šäººç±»æ“ä½œå‘˜ã€‚è§†é¢‘å’Œå›¾åƒå¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://dexfunc.github.io/">https://dexfunc.github.io/</a> ä¸ŠæŸ¥çœ‹ã€‚<details>
<summary>Abstract</summary>
While there have been significant strides in dexterous manipulation, most of it is limited to benchmark tasks like in-hand reorientation which are of limited utility in the real world. The main benefit of dexterous hands over two-fingered ones is their ability to pickup tools and other objects (including thin ones) and grasp them firmly to apply force. However, this task requires both a complex understanding of functional affordances as well as precise low-level control. While prior work obtains affordances from human data this approach doesn't scale to low-level control. Similarly, simulation training cannot give the robot an understanding of real-world semantics. In this paper, we aim to combine the best of both worlds to accomplish functional grasping for in-the-wild objects. We use a modular approach. First, affordances are obtained by matching corresponding regions of different objects and then a low-level policy trained in sim is run to grasp it. We propose a novel application of eigengrasps to reduce the search space of RL using a small amount of human data and find that it leads to more stable and physically realistic motion. We find that eigengrasp action space beats baselines in simulation and outperforms hardcoded grasping in real and matches or outperforms a trained human teleoperator. Results visualizations and videos at https://dexfunc.github.io/
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå°½ç®¡æœ‰äº†å¾ˆå¤§çš„è¿›æ­¥ï¼Œdexterous manipulationçš„å¤§å¤šæ•°éƒ½ä»…ä»…æ˜¯å¯¹ benchmark ä»»åŠ¡ like æ‰‹ä¸­é‡æ–°Orienting çš„é™å®šæ€§åˆ©ç”¨ï¼Œè¿™äº›ä»»åŠ¡åœ¨å®é™…ä¸–ç•Œä¸­çš„ç”¨é€”ä»…ä»…æ˜¯æœ‰é™çš„ã€‚dexterous hands çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿå°†å·¥å…·å’Œå…¶ä»–ç‰©å“ï¼ˆåŒ…æ‹¬ç»†é•¿çš„ï¼‰ç¨³å›ºåœ°æ¶å–å¹¶æ–½åŠ åŠ›ï¼Œä½†è¿™ä¸ªä»»åŠ¡éœ€è¦ Both a complex understanding of functional affordances å’Œç²¾ç¡®çš„ low-level controlã€‚å°½ç®¡å…ˆå‰çš„å·¥ä½œä»äººç±»æ•°æ®ä¸­è·å–äº†å¯ç”¨æ€§ï¼Œä½†è¿™ç§æ–¹æ³•ä¸èƒ½æ‰©å±•åˆ° low-level controlã€‚ Similarly, simulation training cannot give the robot an understanding of real-world semanticsã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦ç»“åˆä¸¤ä¸ªä¸–ç•Œçš„å¥½å¤„ï¼Œå®ç°å®é™…ä¸–ç•Œä¸­çš„åŠŸèƒ½æŠ“å–ã€‚æˆ‘ä»¬ä½¿ç”¨æ¨¡å—åŒ–çš„æ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯¹ä¸åŒç‰©å“çš„ç›¸åº”åŒºåŸŸè¿›è¡ŒåŒ¹é…ï¼Œç„¶åä½¿ç”¨ sim è®­ç»ƒçš„ä½çº§ç­–ç•¥æ¥æŠ“å–å®ƒã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ eigengrasps åº”ç”¨ï¼Œä»¥å‡å°‘RL çš„æœç´¢ç©ºé—´ï¼Œä½¿ç”¨å°é‡äººç±»æ•°æ®ï¼Œå¹¶å‘ç°å®ƒå¯¼è‡´æ›´ç¨³å®šå’Œç‰©ç†ä¸Šæ›´çœŸå®çš„è¿åŠ¨ã€‚æˆ‘ä»¬å‘ç° eigengrasp action space åœ¨ sim ä¸­æ¯”åŸºeline é«˜ï¼Œå¹¶åœ¨å®é™…ä¸–ç•Œä¸­è¶…è¿‡ç¡¬coded graspingï¼Œä¸äººå·¥ç”µå­å¸ˆåŒ ç›¸å½“ã€‚ç»“æœã€è§†è§‰åŒ–å’Œå½±ç‰‡å¯ä»¥åœ¨ <https://dexfunc.github.io/> æµè§ˆã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Alchemist-Parametric-Control-of-Material-Properties-with-Diffusion-Models"><a href="#Alchemist-Parametric-Control-of-Material-Properties-with-Diffusion-Models" class="headerlink" title="Alchemist: Parametric Control of Material Properties with Diffusion Models"></a>Alchemist: Parametric Control of Material Properties with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02970">http://arxiv.org/abs/2312.02970</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T. Freeman, Mark Matthews</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ç”¨æ¥æ§åˆ¶ç‰©ä½“å›¾åƒä¸­çš„ç‰©ç†å±æ€§ï¼Œå¦‚ç²—ç³™åº¦ã€é‡‘å±æ„Ÿã€åå°„ç‡å’Œé€æ˜åº¦çš„ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åˆ©ç”¨æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆé¢„è®¾ï¼Œé€šè¿‡ scalar å€¼å’ŒæŒ‡ä»¤æ¥ä¿®æ”¹å›¾åƒä¸­çš„ä½çº§æè´¨å±æ€§ã€‚</li>
<li>results: é€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„ç‰©ä½“ä¸­å¿ƒsyntheticæ•°æ®é›†å’Œä¿®æ”¹ modify çš„pre-trainedæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œå¯ä»¥åœ¨å®é™…å›¾åƒä¸­ç¼–è¾‘æè´¨å±æ€§ï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–å±æ€§ã€‚<details>
<summary>Abstract</summary>
We propose a method to control material attributes of objects like roughness, metallic, albedo, and transparency in real images. Our method capitalizes on the generative prior of text-to-image models known for photorealism, employing a scalar value and instructions to alter low-level material properties. Addressing the lack of datasets with controlled material attributes, we generated an object-centric synthetic dataset with physically-based materials. Fine-tuning a modified pre-trained text-to-image model on this synthetic dataset enables us to edit material properties in real-world images while preserving all other attributes. We show the potential application of our model to material edited NeRFs.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥æ§åˆ¶ç‰©ä½“çš„ç‰©ç†å±æ€§ï¼Œå¦‚ç²—ç³™åº¦ã€é‡‘å±æ€§ã€åå°„ç‡å’Œé€æ˜åº¦ï¼Œåœ¨çœŸå®å›¾åƒä¸­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨äº†æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹çš„ç”Ÿæˆå‰æï¼Œé€šè¿‡scalarå€¼å’ŒæŒ‡ä»¤æ¥ä¿®æ”¹ä½çº§æè´¨å±æ€§ã€‚ç”±äºç¼ºä¹æ§åˆ¶æè´¨å±æ€§çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†ä¸€ä¸ªä¸­å¿ƒå¯¹è±¡çš„ sintetic æ•°æ®é›†ï¼Œå…¶ä¸­ç‰©ä½“æ‹¥æœ‰ç‰©ç†åŸºäºçš„æè´¨ã€‚é€šè¿‡å¯¹ä¿®æ”¹åçš„æ¨¡å‹è¿›è¡Œé«˜çº§imosç»ƒä¹ ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨çœŸå®å›¾åƒä¸­ç¼–è¾‘æè´¨å±æ€§ï¼Œä¿ç•™æ‰€æœ‰å…¶ä»–å±æ€§ä¸å˜ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åº”ç”¨äºæè´¨ç¼–è¾‘NeRFã€‚Note: "NeRF" stands for "Neural Radiance Fields", which is a technique used to represent 3D objects in a scene in a way that allows for realistic rendering and manipulation of the object's materials and lighting.
</details></li>
</ul>
<hr>
<h2 id="Generating-Interpretable-Networks-using-Hypernetworks"><a href="#Generating-Interpretable-Networks-using-Hypernetworks" class="headerlink" title="Generating Interpretable Networks using Hypernetworks"></a>Generating Interpretable Networks using Hypernetworks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03051">http://arxiv.org/abs/2312.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Isaac Liao, Ziming Liu, Max Tegmark</li>
<li>for: æœ¬ç ”ç©¶çš„ç›®çš„æ˜¯è§£ç ç¥ç»ç½‘ç»œï¼Œå³å°†ç¥ç»ç½‘ç»œçš„åŸå§‹æƒé‡è½¬åŒ–ä¸ºå¯è§£é‡Šçš„ç®—æ³•ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å·ç§¯ç½‘ç»œï¼ˆhypernetworkï¼‰æ¥ç”Ÿæˆå¯è§£é‡Šçš„ç½‘ç»œï¼Œå¹¶ä¸”é€šè¿‡æ§åˆ¶ç½‘ç»œå¤æ‚åº¦æ¥ç”Ÿæˆå¤šç§å¯è§£é‡Šçš„ç®—æ³•ã€‚</li>
<li>results: ç ”ç©¶å‘ç°äº†ä¸‰ç§è®¡ç®—L1èŒƒæ•°çš„ç®—æ³•ï¼šï¼ˆaï¼‰åŒé¢ç®—æ³•ï¼ˆbï¼‰å‡ ä½•ç®—æ³•ï¼ˆcï¼‰å·ç§¯ç®—æ³•ï¼Œå…¶ä¸­åªæœ‰ç¬¬ä¸€ä¸ªç®—æ³•é¢„æœŸåœ¨å®éªŒä¹‹å‰ã€‚ç ”ç©¶è¿˜å‘ç°äº†è¿™äº›ç®—æ³•çš„ç³»ç»ŸåŒ–æ¼”åŒ–å’Œå¤æ‚åº¦æ§åˆ¶çš„å½±å“ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç¤ºå‡ºäº†ä¸€ä¸ªè®­ç»ƒè¿‡çš„å·ç§¯ç½‘ç»œå¯ä»¥æ­£ç¡®åœ°æ„å»ºæœªåœ¨è®­ç»ƒä¸­çœ‹åˆ°çš„è¾“å…¥ç»´åº¦çš„æ¨¡å‹ï¼Œè¿™è¡¨æ˜äº†ç³»ç»ŸåŒ–æ³›åŒ–çš„èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
An essential goal in mechanistic interpretability to decode a network, i.e., to convert a neural network's raw weights to an interpretable algorithm. Given the difficulty of the decoding problem, progress has been made to understand the easier encoding problem, i.e., to convert an interpretable algorithm into network weights. Previous works focus on encoding existing algorithms into networks, which are interpretable by definition. However, focusing on encoding limits the possibility of discovering new algorithms that humans have never stumbled upon, but that are nevertheless interpretable. In this work, we explore the possibility of using hypernetworks to generate interpretable networks whose underlying algorithms are not yet known. The hypernetwork is carefully designed such that it can control network complexity, leading to a diverse family of interpretable algorithms ranked by their complexity. All of them are interpretable in hindsight, although some of them are less intuitive to humans, hence providing new insights regarding how to "think" like a neural network. For the task of computing L1 norms, hypernetworks find three algorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c) the pudding algorithm, although only the first algorithm was expected by the authors before experiments. We automatically classify these algorithms and analyze how these algorithmic phases develop during training, as well as how they are affected by complexity control. Furthermore, we show that a trained hypernetwork can correctly construct models for input dimensions not seen in training, demonstrating systematic generalization.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¸€ä¸ªé‡è¦çš„ç›®æ ‡åœ¨æœºåˆ¶å¯è¯»æ€§ä¸­æ˜¯è§£ç ç¥ç»ç½‘ç»œï¼Œå³å°†ç¥ç»ç½‘ç»œçš„åŸå§‹å‚æ•°è½¬æ¢ä¸ºå¯è¯»çš„ç®—æ³•ã€‚ç”±äºè§£ç é—®é¢˜çš„difficultyï¼Œå·²ç»å–å¾—äº†åœ¨ç†è§£ç¼–ç é—®é¢˜ä¸­çš„è¿›å±•ï¼Œå³å°†å¯è¯»çš„ç®—æ³•è½¬æ¢ä¸ºç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚å…ˆå‰çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨å°†å·²çŸ¥çš„ç®—æ³•ç¼–ç åˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™äº›ç®—æ³•éƒ½æ˜¯å¯è¯»çš„ã€‚ä½†æ˜¯ï¼Œåªé›†ä¸­åœ¨ç¼–ç é—®é¢˜ä¸Šé™åˆ¶äº†å‘ç°æ–°çš„ç®—æ³•ï¼Œå®ƒä»¬å°šæœªè¢«äººç±»å‘ç°ï¼Œä½†å®ƒä»¬å…·æœ‰å¯è¯»æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨åµŒå…¥ç½‘ç»œæ¥ç”Ÿæˆå¯è¯»çš„ç½‘ç»œï¼Œå…¶ä¸‹é¢çš„ç®—æ³•å¹¶ä¸æ˜¯å·²çŸ¥çš„ã€‚æˆ‘ä»¬ mÃ©ticulouslyè®¾è®¡äº†è¿™ä¸ªåµŒå…¥ç½‘ç»œï¼Œä»¥æ§åˆ¶ç¥ç»ç½‘ç»œçš„å¤æ‚æ€§ï¼Œä»è€Œå¯¼è‡´ä¸€ä¸ªå¤šæ ·åŒ–çš„å¯è¯»ç®—æ³•å®¶æ—ï¼Œè¿™äº›ç®—æ³•çš„å¤æ‚æ€§å¯ä»¥ç”±äººç±»æ¥è¯„ä¼°ã€‚åœ¨è®¡ç®—L1èŒƒæ•°ä»»åŠ¡ä¸Šï¼ŒåµŒå…¥ç½‘ç»œæ‰¾åˆ°äº†ä¸‰ç§ç®—æ³•ï¼šï¼ˆaï¼‰åŒé¢ç®—æ³•ã€ï¼ˆbï¼‰å‡ ä½•ç®—æ³•ã€ï¼ˆcï¼‰å¥¶ç³•ç®—æ³•ï¼Œåªæœ‰ç¬¬ä¸€ç§ç®—æ³•è¢«ä½œè€…ä»¬é¢„æœŸã€‚æˆ‘ä»¬è‡ªåŠ¨åˆ†ç±»äº†è¿™äº›ç®—æ³•ï¼Œå¹¶åˆ†æäº†è¿™äº›ç®—æ³•çš„å‘å±•é˜¶æ®µä»¥åŠå¤æ‚æ€§æ§åˆ¶çš„å½±å“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä¸€ä¸ªè®­ç»ƒè¿‡çš„åµŒå…¥ç½‘ç»œå¯ä»¥æ­£ç¡®åœ°ç”Ÿæˆæœªåœ¨è®­ç»ƒä¸­çœ‹åˆ°çš„è¾“å…¥ç»´åº¦ä¸Šçš„æ¨¡å‹ï¼Œè¿™è¯´æ˜äº†ç³»ç»ŸåŒ–æ³›åŒ–çš„èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Classification-for-everyone-Building-geography-agnostic-models-for-fairer-recognition"><a href="#Classification-for-everyone-Building-geography-agnostic-models-for-fairer-recognition" class="headerlink" title="Classification for everyone : Building geography agnostic models for fairer recognition"></a>Classification for everyone : Building geography agnostic models for fairer recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02957">http://arxiv.org/abs/2312.02957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akshat Jindal, Shreya Singh, Soham Gadgil</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶å¦‚ä½• Mitigate å›¾åƒåˆ†ç±»æ¨¡å‹ä¸­çš„è‡ªç„¶åœ°ç†åè§ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†ä¸¤ä¸ªæ•°æ®é›† - The Dollar Street Dataset å’Œ ImageNetï¼Œé€šè¿‡å›¾åƒçš„ä½ç½®ä¿¡æ¯æ¥è¯„é‡è¿™ç§åè§ã€‚ç„¶åï¼Œå®ƒæå‡ºäº†å¤šç§å¯ä»¥ä½¿ç”¨çš„æ–¹æ³•æ¥å‡å°‘è¿™ç§åè§ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡é€šè¿‡åˆ†æä¸åŒçš„æ–¹æ³•ï¼Œå‘ç°è¿™äº›æ–¹æ³•å¯ä»¥ä½¿å›¾åƒåˆ†ç±»æ¨¡å‹æ›´åŠ å¯¹åœ°åŸŸä½ç½®å…·æœ‰æŠ—æ€§ã€‚<details>
<summary>Abstract</summary>
In this paper, we analyze different methods to mitigate inherent geographical biases present in state of the art image classification models. We first quantitatively present this bias in two datasets - The Dollar Street Dataset and ImageNet, using images with location information. We then present different methods which can be employed to reduce this bias. Finally, we analyze the effectiveness of the different techniques on making these models more robust to geographical locations of the images.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒçš„æ–¹æ³•æ¥å‡è½»ç°æœ‰çš„å›¾åƒåˆ†ç±»æ¨¡å‹å†…ç½®çš„åœ°åŸŸåè§ã€‚æˆ‘ä»¬é¦–å…ˆé‡åŒ–äº†è¿™ç§åè§åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­ - ç¾å…ƒè¡—æ•°æ®é›†å’ŒImageNetæ•°æ®é›†ä¸­çš„å›¾åƒï¼Œå¹¶ä½¿ç”¨å›¾åƒåœ°ç†ä½ç½®ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸åŒçš„æ–¹æ³•å¯ä»¥ç”¨æ¥å‡å°‘è¿™ç§åè§ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ†æäº†ä¸åŒæŠ€æœ¯åœ¨å›¾åƒåœ°åŸŸä½ç½®çš„å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="WhisBERT-Multimodal-Text-Audio-Language-Modeling-on-100M-Words"><a href="#WhisBERT-Multimodal-Text-Audio-Language-Modeling-on-100M-Words" class="headerlink" title="WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words"></a>WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02931">http://arxiv.org/abs/2312.02931</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Wolf, Greta Tuckute, Klemen Kotar, Eghbal Hosseini, Tamar Regev, Ethan Wilcox, Alex Warstadt</li>
<li>for: æœ¬ç ”ç©¶æ˜¯ä¸ºäº†æ£€éªŒå¤šModalitiesè®­ç»ƒè¯­è¨€æ¨¡å‹æ˜¯å¦å¯ä»¥æé«˜å…¶è´¨é‡å’Œæ•ˆç‡ã€‚</li>
<li>methods: ä½œè€…ä½¿ç”¨äº†Whisbertæ¨¡å‹ï¼Œè¿™æ˜¯åŸºäºæ–‡æœ¬â€“å›¾åƒæ–¹æ³•çš„FLAVAæ¨¡å‹ï¼ˆSingh et al., 2022ï¼‰ã€‚ä½œè€…éµå¾ªBabylmæŒ‡å—ï¼ˆWarstadt et al., 2023ï¼‰ï¼Œåœ¨ä¸€ä¸ªåŒ…å«100ä¸‡ä¸ªè¯å’Œå…¶å¯¹åº”çš„è¯­éŸ³çš„ dataset ä¸Šé¢„è®­Whisbertæ¨¡å‹ã€‚</li>
<li>results: ä½œè€…å‘ç°ï¼Œè™½ç„¶Whisbertåœ¨å¤šModalitiesè®­ç»ƒä¸‹å¯ä»¥å¾ˆå¥½åœ°å®Œæˆæ¨¡æ‚éšè—æ¨¡å‹ä»»åŠ¡å’Œè¶…è¶ŠBabylmåŸºelineåœ¨å¤§å¤šæ•°benchmarkä»»åŠ¡ä¸­ï¼Œä½†å®ƒåœ¨ä¼˜åŒ–å¤æ‚çš„ç›®æ ‡æ—¶å—é˜»ï¼Œæ— æ³•è¶…è¶Šæ–‡æœ¬åªçš„WhisbertåŸºelineã€‚<details>
<summary>Abstract</summary>
Training on multiple modalities of input can augment the capabilities of a language model. Here, we ask whether such a training regime can improve the quality and efficiency of these systems as well. We focus on text--audio and introduce Whisbert, which is inspired by the text--image approach of FLAVA (Singh et al., 2022). In accordance with Babylm guidelines (Warstadt et al., 2023), we pretrain Whisbert on a dataset comprising only 100 million words plus their corresponding speech from the word-aligned version of the People's Speech dataset (Galvez et al., 2021). To assess the impact of multimodality, we compare versions of the model that are trained on text only and on both audio and text simultaneously. We find that while Whisbert is able to perform well on multimodal masked modeling and surpasses the Babylm baselines in most benchmark tasks, it struggles to optimize its complex objective and outperform its text-only Whisbert baseline.
</details>
<details>
<summary>æ‘˜è¦</summary>
è®­ç»ƒå¤šmodalitiesçš„è¾“å…¥å¯ä»¥å¢å¼ºè¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é—®é“ï¼Œæ˜¯å¦å¯ä»¥é€šè¿‡è¿™ç§è®­ç»ƒæ–¹å¼æé«˜è¿™äº›ç³»ç»Ÿçš„è´¨é‡å’Œæ•ˆç‡ã€‚æˆ‘ä»¬å°†å…³æ³¨æ–‡æœ¬â€”â€”éŸ³é¢‘çš„è®­ç»ƒï¼Œå¹¶å¼•å…¥Whisbertï¼Œå®ƒæ˜¯åŸºäºæ–‡æœ¬â€”â€”å›¾åƒçš„FLAVAï¼ˆSingh et al., 2022ï¼‰çš„ inspirationsã€‚æŒ‰ç…§BabylmæŒ‡å—ï¼ˆWarstadt et al., 2023ï¼‰ï¼Œæˆ‘ä»¬é¢„è®­Whisbertåœ¨åŒ…å«1äº¿ä¸ªå•è¯çš„ dataset ä¸Šï¼Œå¹¶ä¸å…¶å¯¹åº”çš„è¯­éŸ³ä»äººç±»è¯­éŸ³ dataset çš„word-alignedç‰ˆæœ¬ï¼ˆGalvez et al., 2021ï¼‰è¿›è¡Œäº†è®­ç»ƒã€‚ä¸ºäº†è¯„ä¼°å¤šmodalitiesçš„å½±å“ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºæ–‡æœ¬åªå’ŒåŸºäºéŸ³é¢‘å’Œæ–‡æœ¬åŒæ—¶è®­ç»ƒçš„æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶Whisbertåœ¨å¤šmodalitieséšè—æ¨¡å‹å’Œå¤§å¤šæ•°benchmarkä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†å®ƒåœ¨ä¼˜åŒ–å¤æ‚çš„ç›®æ ‡å‡½æ•°ä¸Šå›°éš¾è¶…è¶Šæ–‡æœ¬åªçš„WhisbertåŸºelineã€‚
</details></li>
</ul>
<hr>
<h2 id="Let-the-LLMs-Talk-Simulating-Human-to-Human-Conversational-QA-via-Zero-Shot-LLM-to-LLM-Interactions"><a href="#Let-the-LLMs-Talk-Simulating-Human-to-Human-Conversational-QA-via-Zero-Shot-LLM-to-LLM-Interactions" class="headerlink" title="Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions"></a>Let the LLMs Talk: Simulating Human-to-Human Conversational QA via Zero-Shot LLM-to-LLM Interactions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02913">http://arxiv.org/abs/2312.02913</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zahraabbasiantaeb/simquac">https://github.com/zahraabbasiantaeb/simquac</a></li>
<li>paper_authors: Zahra Abbasiantaeb, Yifei Yuan, Evangelos Kanoulas, Mohammad Aliannejadi</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æ¢è®¨ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥ä»¿çœŸäººç±»å¯¹è¯çš„ conversational question-answeringï¼ˆCQAï¼‰ç³»ç»Ÿã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†é›¶shotå­¦ä¹ çš„GPT-4æ¨¡å‹æ¥å®ç°å­¦ç”Ÿå’Œæ•™å¸ˆçš„è§’è‰²ï¼Œå¹¶é€šè¿‡è®©å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆé—®é¢˜ï¼Œå¹¶ç”±æ•™å¸ˆæ¨¡å‹å›ç­”é—®é¢˜æ¥æ¨¡æ‹Ÿäººç±»å¯¹è¯ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨LLMæ¥ä»¿çœŸäººç±»å¯¹è¯å¯ä»¥å–å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœï¼Œ teacher LLMç”Ÿæˆçš„ç­”æ¡ˆæ›´åŠ å…·ä½“å’Œå®Œæ•´ï¼Œè€Œå­¦ç”Ÿ LLM ç”Ÿæˆçš„é—®é¢˜æ›´åŠ å¤šæ ·åŒ–ï¼Œè¦†ç›–äº†æ›´å¤šçš„è¯é¢˜æ–¹é¢ã€‚<details>
<summary>Abstract</summary>
Conversational question-answering (CQA) systems aim to create interactive search systems that effectively retrieve information by interacting with users. To replicate human-to-human conversations, existing work uses human annotators to play the roles of the questioner (student) and the answerer (teacher). Despite its effectiveness, challenges exist as human annotation is time-consuming, inconsistent, and not scalable. To address this issue and investigate the applicability of large language models (LLMs) in CQA simulation, we propose a simulation framework that employs zero-shot learner LLMs for simulating teacher-student interactions. Our framework involves two LLMs interacting on a specific topic, with the first LLM acting as a student, generating questions to explore a given search topic. The second LLM plays the role of a teacher by answering questions and is equipped with additional information, including a text on the given topic. We implement both the student and teacher by zero-shot prompting the GPT-4 model. To assess the effectiveness of LLMs in simulating CQA interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the simulated data from various perspectives. We begin by evaluating the teacher's performance through both automatic and human assessment. Next, we evaluate the performance of the student, analyzing and comparing the disparities between questions generated by the LLM and those generated by humans. Furthermore, we conduct extensive analyses to thoroughly examine the LLM performance by benchmarking state-of-the-art reading comprehension models on both datasets. Our results reveal that the teacher LLM generates lengthier answers that tend to be more accurate and complete. The student LLM generates more diverse questions, covering more aspects of a given topic.
</details>
<details>
<summary>æ‘˜è¦</summary>
conversational question-answering (CQA) ç³»ç»Ÿçš„ç›®æ ‡æ˜¯åˆ›å»ºå¯äº¤äº’çš„æœç´¢ç³»ç»Ÿï¼Œä»¥ä¾¿æœ‰æ•ˆåœ°æ£€ç´¢ä¿¡æ¯ï¼Œå¹¶ä¸”ä¸äººç±»å¯¹è¯æ–¹å¼ç›¸ä¼¼ã€‚ç°æœ‰çš„å·¥ä½œä½¿ç”¨äººç±»æ ‡æ³¨å‘˜æ‰®æ¼”é—®é¢˜äººï¼ˆå­¦ç”Ÿï¼‰å’Œç­”æ¡ˆäººï¼ˆæ•™å¸ˆï¼‰çš„è§’è‰²ã€‚ç„¶è€Œï¼Œäººç±»æ ‡æ³¨æ˜¯æ—¶é—´consumingï¼Œä¸ä¸€è‡´å’Œä¸å¯æ‰©å±•çš„ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¨¡æ‹Ÿæ¡†æ¶ï¼Œä½¿ç”¨é›¶shotå­¦ä¹ çš„å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥æ¨¡æ‹Ÿæ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„äº¤äº’ã€‚æˆ‘ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªLLMè¿›è¡Œäº¤äº’ï¼Œå…¶ä¸­ä¸€ä¸ªLLM acts as a studentï¼Œç”Ÿæˆé—®é¢˜ä»¥æ¢ç´¢ä¸€ä¸ªæœç´¢ä¸»é¢˜ã€‚å¦ä¸€ä¸ªLLMæ‰®æ¼”æ•™å¸ˆï¼Œå›ç­”é—®é¢˜ï¼Œå¹¶å…·æœ‰é¢å¤–ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸»é¢˜ç›¸å…³çš„æ–‡æœ¬ã€‚æˆ‘ä»¬ä½¿ç”¨GPT-4æ¨¡å‹æ¥å®ç°å­¦ç”Ÿå’Œæ•™å¸ˆã€‚ä¸ºäº†è¯„ä¼°LLMåœ¨æ¨¡æ‹ŸCQAäº¤äº’ä¸­çš„æ•ˆæœï¼Œä»¥åŠäººç±»å’ŒLLMç”Ÿæˆçš„å¯¹è¯ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬å¯¹æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œäº†å¤šç§è¯„ä¼°ã€‚æˆ‘ä»¬é¦–å…ˆè¯„ä¼°æ•™å¸ˆçš„è¡¨ç°ï¼Œé€šè¿‡è‡ªåŠ¨å’Œäººç±»è¯„ä¼°ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è¯„ä¼°å­¦ç”Ÿçš„è¡¨ç°ï¼Œåˆ†æå’Œæ¯”è¾ƒLLMç”Ÿæˆçš„é—®é¢˜å’Œäººç±»ç”Ÿæˆçš„é—®é¢˜ä¹‹é—´çš„å·®å¼‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„åˆ†æï¼Œä»¥å…¨é¢è¯„ä¼°LLMæ€§èƒ½ï¼Œå¹¶å°†ç°æœ‰çš„é˜…è¯»ç†è§£æ¨¡å‹ benchmarkäºä¸¤ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œæ•™å¸ˆLLMç”Ÿæˆçš„ç­”æ¡ˆè¾ƒé•¿ï¼Œå…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚å­¦ç”ŸLLMç”Ÿæˆçš„é—®é¢˜è¦†ç›–äº†æ›´å¤šçš„ä¸»é¢˜æ–¹é¢ï¼Œæ›´åŠ å¤šæ ·åŒ–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Toward-autocorrection-of-chemical-process-flowsheets-using-large-language-models"><a href="#Toward-autocorrection-of-chemical-process-flowsheets-using-large-language-models" class="headerlink" title="Toward autocorrection of chemical process flowsheets using large language models"></a>Toward autocorrection of chemical process flowsheets using large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02873">http://arxiv.org/abs/2312.02873</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Schulze Balhorn, Marc Caballero, Artur M. Schweidtmann</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºäººå·¥æ™ºèƒ½æŠ€æœ¯çš„è‡ªåŠ¨ä¿®æ­£æµç¨‹å›¾æ–‡æ³•ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ£€æŸ¥å’Œä¿®æ­£è¿‡ç¨‹æµç¨‹å›¾æ–‡ä¸­çš„é”™è¯¯ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¥è‡ªåŠ¨æ£€æµ‹å’Œä¿®æ­£è¿‡ç¨‹æµç¨‹å›¾æ–‡ä¸­çš„é”™è¯¯ã€‚è¾“å…¥æ¨¡å‹æ˜¯ä¸€ä¸ªå¯èƒ½æœ‰è¯¯çš„è¿‡ç¨‹æµç¨‹å›¾æ–‡ï¼Œè¾“å‡ºæ¨¡å‹æ˜¯ä¸€ä¸ªä¿®æ­£åçš„è¿‡ç¨‹æµç¨‹å›¾æ–‡ã€‚</li>
<li>results: åœ¨ä¸€ä¸ªäººå·¥ç”Ÿæˆçš„synthetic datasetä¸Šè¿›è¡Œäº†supervisedå­¦ä¹ ï¼Œæ¨¡å‹å®ç°äº†80%çš„é¡¶å³°å‡†ç¡®ç‡å’Œ84%çš„é¡¶äº”å‡†ç¡®ç‡åœ¨ä¸€ä¸ªç‹¬ç«‹æµ‹è¯•é›†ä¸Šã€‚è¿™äº›ç»“æœè¡¨æ˜æ¨¡å‹å¯ä»¥å­¦ä¹ è‡ªåŠ¨ä¿®æ­£syntheticæµç¨‹å›¾æ–‡ã€‚<details>
<summary>Abstract</summary>
The process engineering domain widely uses Process Flow Diagrams (PFDs) and Process and Instrumentation Diagrams (P&IDs) to represent process flows and equipment configurations. However, the P&IDs and PFDs, hereafter called flowsheets, can contain errors causing safety hazards, inefficient operation, and unnecessary expenses. Correcting and verifying flowsheets is a tedious, manual process. We propose a novel generative AI methodology for automatically identifying errors in flowsheets and suggesting corrections to the user, i.e., autocorrecting flowsheets. Inspired by the breakthrough of Large Language Models (LLMs) for grammatical autocorrection of human language, we investigate LLMs for the autocorrection of flowsheets. The input to the model is a potentially erroneous flowsheet and the output of the model are suggestions for a corrected flowsheet. We train our autocorrection model on a synthetic dataset in a supervised manner. The model achieves a top-1 accuracy of 80% and a top-5 accuracy of 84% on an independent test dataset of synthetically generated flowsheets. The results suggest that the model can learn to autocorrect the synthetic flowsheets. We envision that flowsheet autocorrection will become a useful tool for chemical engineers.
</details>
<details>
<summary>æ‘˜è¦</summary>
Process å·¥ç¨‹é¢†åŸŸå¹¿æ³›ä½¿ç”¨è¿‡ç¨‹æµagramï¼ˆPFDï¼‰å’Œè¿‡ç¨‹å’Œæµ‹è¯•agramï¼ˆP&IDï¼‰æ¥è¡¨ç¤ºè¿‡ç¨‹æµå’Œè®¾å¤‡é…ç½®ã€‚ç„¶è€Œï¼ŒPFDså’ŒP&IDsï¼Œä»¥ä¸‹ç®€ç§°ä¸ºæµç¨‹å›¾ï¼Œå¯èƒ½åŒ…å«é”™è¯¯ï¼Œå¯¼è‡´å®‰å…¨éšæ‚£ã€ä¸æ•ˆç¯å¢ƒå’Œå¤šèŠ±é’±ã€‚æ›´æ­£å’ŒéªŒè¯æµç¨‹å›¾æ˜¯ä¸€ä¸ªç¹çã€æ‰‹åŠ¨çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ–¹æ³•ï¼Œå¯ä»¥è‡ªåŠ¨æ£€æµ‹æµç¨‹å›¾ä¸­çš„é”™è¯¯å¹¶æä¾›æ›´æ­£å»ºè®®ï¼Œå³è‡ªåŠ¨æ›´æ­£æµç¨‹å›¾ã€‚å–å¾—å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çªç ´å£ï¼Œæˆ‘ä»¬ç ”ç©¶LLMsåœ¨ä¿®è®¢äººç±»è¯­è¨€ä¸­çš„è‡ªåŠ¨ä¿®è®¢èƒ½åŠ›ï¼Œä»¥ä¾¿åº”ç”¨äºæµç¨‹å›¾çš„è‡ªåŠ¨ä¿®è®¢ã€‚è¾“å…¥æ¨¡å‹çš„æµç¨‹å›¾å¯èƒ½åŒ…å«é”™è¯¯ï¼Œè¾“å‡ºæ¨¡å‹çš„å»ºè®®æ˜¯ä¿®è®¢åçš„æµç¨‹å›¾ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªsynthetic datasetä¸Šè¿›è¡Œäº†ç›‘ç£æ€§è®­ç»ƒã€‚æ¨¡å‹åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¾¾åˆ°äº†80%çš„é¡¶éƒ¨ä¸€ accuracyå’Œ84%çš„é¡¶éƒ¨äº” accuracyã€‚ç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ è‡ªåŠ¨ä¿®è®¢syntheticæµç¨‹å›¾ã€‚æˆ‘ä»¬anticipate that flowsheet autocorrection will become a useful tool for chemical engineers.
</details></li>
</ul>
<hr>
<h2 id="Experimental-Insights-Towards-Explainable-and-Interpretable-Pedestrian-Crossing-Prediction"><a href="#Experimental-Insights-Towards-Explainable-and-Interpretable-Pedestrian-Crossing-Prediction" class="headerlink" title="Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction"></a>Experimental Insights Towards Explainable and Interpretable Pedestrian Crossing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02872">http://arxiv.org/abs/2312.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angie Nataly Melo, Carlota Salinas, Miguel Angel Sotelo</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜è‡ªåŠ¨é©¾é©¶road safetyï¼Œé€šè¿‡å¯è§£é‡Šå’Œå¯ interpretçš„æ–¹å¼é¢„æµ‹æ­¥è¡Œäººè¿‡è·¯ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ç¥ç»ç¬¦å·approachï¼Œç»“åˆæ·±åº¦å­¦ä¹ å’Œæ··æ²Œé€»è¾‘æ¥å®ç°å¯è§£é‡Šå’Œå¯ interpretçš„æ­¥è¡Œäººè¿‡è·¯é¢„æµ‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šé¢„æµ‹å™¨ï¼ˆExPedCrossï¼‰ï¼Œä½¿ç”¨äº†ä¸€ç»„å¯è§£é‡Šçš„ç‰¹å¾å¹¶ä½¿ç”¨æ··æ²Œæ¨ç†ç³»ç»Ÿæ¥é¢„æµ‹æ­¥è¡Œäººå°†å¦è¿‡è·¯ã€‚</li>
<li>results: æˆ‘ä»¬å¯¹PIEå’ŒJAADæ•°æ®é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œå®éªŒç»“æœæä¾›äº†å¯è§£é‡Šå’Œå¯ interpretçš„æ­¥è¡Œäººè¿‡è·¯é¢„æµ‹ä»»åŠ¡ä¸­çš„å®è·µç»éªŒå’Œå»ºè®®ã€‚<details>
<summary>Abstract</summary>
In the context of autonomous driving, pedestrian crossing prediction is a key component for improving road safety. Presently, the focus of these predictions extends beyond achieving trustworthy results; it is shifting towards the explainability and interpretability of these predictions. This research introduces a novel neuro-symbolic approach that combines deep learning and fuzzy logic for an explainable and interpretable pedestrian crossing prediction. We have developed an explainable predictor (ExPedCross), which utilizes a set of explainable features and employs a fuzzy inference system to predict whether the pedestrian will cross or not. Our approach was evaluated on both the PIE and JAAD datasets. The results offer experimental insights into achieving explainability and interpretability in the pedestrian crossing prediction task. Furthermore, the testing results yield a set of guidelines and recommendations regarding the process of dataset selection, feature selection, and explainability.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œäººè¡Œé“åå­—Predictionæ˜¯ä¸€ä¸ªå…³é”®çš„å®‰å…¨æ€§ç»„ä»¶ã€‚ç›®å‰ï¼Œè¿™äº›é¢„æµ‹çš„é‡ç‚¹ä¸ä»…æ˜¯è·å¾—å¯é çš„ç»“æœï¼Œè€Œä¸”åœ¨å‘Explainabilityå’ŒInterpretabilityçš„å‘å±•ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„ neuralsymbolicæ–¹æ³•ï¼Œ combines deep learningå’Œæ··æ²Œé€»è¾‘æ¥å®ç°å¯è§£é‡Šçš„äººè¡Œé“åå­—é¢„æµ‹ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¯è§£é‡Šé¢„æµ‹å™¨ï¼ˆExPedCrossï¼‰ï¼Œä½¿ç”¨äº†ä¸€ç»„å¯è§£é‡Šçš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨äº†æ··æ²Œæ¨ç†ç³»ç»Ÿæ¥é¢„æµ‹äººå°†å¦è¿‡è·¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨PIEå’ŒJAADæ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœæä¾›äº†æœ‰ç”¨çš„å®éªŒå®¤æ„è§å’Œå»ºè®®ï¼ŒåŒ…æ‹¬æ•°æ®é›†é€‰æ‹©ã€ç‰¹å¾é€‰æ‹©å’Œå¯è§£é‡Šçš„è¿‡ç¨‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Navigating-the-Synthetic-Realm-Harnessing-Diffusion-based-Models-for-Laparoscopic-Text-to-Image-Generation"><a href="#Navigating-the-Synthetic-Realm-Harnessing-Diffusion-based-Models-for-Laparoscopic-Text-to-Image-Generation" class="headerlink" title="Navigating the Synthetic Realm: Harnessing Diffusion-based Models for Laparoscopic Text-to-Image Generation"></a>Navigating the Synthetic Realm: Harnessing Diffusion-based Models for Laparoscopic Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03043">http://arxiv.org/abs/2312.03043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lucidrains/imagen-pytorch">https://github.com/lucidrains/imagen-pytorch</a></li>
<li>paper_authors: Simeon Allmendinger, Patrick Hemmer, Moritz Queisner, Igor Sauer, Leopold MÃ¼ller, Johannes Jakubik, Michael VÃ¶ssing, Niklas KÃ¼hl</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨ä½¿ç”¨æ‰©æ•£å‹ç”Ÿæˆæ¨¡å‹ç”Ÿæˆåˆç†çš„äººå·¥é•œåƒæ•°æ®ï¼Œä»¥æ”¯æŒå¤–ç§‘åº”ç”¨å’Œå†³ç­–ã€‚</li>
<li>methods: æˆ‘ä»¬ä½¿ç”¨äº†å½“ä»Šæœ€ä½³çš„æ–‡æœ¬åˆ°å›¾åƒå»ºç­‘åœ¨é•œåƒåŒ»å­¦ä¸­è¿›è¡Œäº†åº”ç”¨ï¼Œé€šè¿‡Diffusion-basedç”Ÿæˆæ¨¡å‹æ¥ç”Ÿæˆäººå·¥é•œåƒæ•°æ®ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼ŒDiffusion-basedæ¨¡å‹å¯ä»¥å­¦ä¹ é•œåƒåŒ»å­¦ä¸­çš„é£æ ¼å’Œ semanticsï¼Œå¹¶ä¸”å¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„äººå·¥é•œåƒæ•°æ®ï¼Œä½¿å¾—è®¡ç®—æœºç”Ÿæˆçš„å›¾åƒåœ¨å¤–ç§‘åº”ç”¨ä¸­å¾—åˆ°äº†åº”ç”¨ã€‚<details>
<summary>Abstract</summary>
Recent advances in synthetic imaging open up opportunities for obtaining additional data in the field of surgical imaging. This data can provide reliable supplements supporting surgical applications and decision-making through computer vision. Particularly the field of image-guided surgery, such as laparoscopic and robotic-assisted surgery, benefits strongly from synthetic image datasets and virtual surgical training methods. Our study presents an intuitive approach for generating synthetic laparoscopic images from short text prompts using diffusion-based generative models. We demonstrate the usage of state-of-the-art text-to-image architectures in the context of laparoscopic imaging with regard to the surgical removal of the gallbladder as an example. Results on fidelity and diversity demonstrate that diffusion-based models can acquire knowledge about the style and semantics in the field of image-guided surgery. A validation study with a human assessment survey underlines the realistic nature of our synthetic data, as medical personnel detects actual images in a pool with generated images causing a false-positive rate of 66%. In addition, the investigation of a state-of-the-art machine learning model to recognize surgical actions indicates enhanced results when trained with additional generated images of up to 5.20%. Overall, the achieved image quality contributes to the usage of computer-generated images in surgical applications and enhances its path to maturity.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Causal-Representations-of-Climate-Model-Data"><a href="#Towards-Causal-Representations-of-Climate-Model-Data" class="headerlink" title="Towards Causal Representations of Climate Model Data"></a>Towards Causal Representations of Climate Model Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02858">http://arxiv.org/abs/2312.02858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julien Boussard, Chandni Nagda, Julia Kaltenborn, Charlotte Emilie Elektra Lange, Philippe Brouillard, Yaniv Gurwicz, Peer Nowack, David Rolnick</li>
<li>For: This paper aims to explore the potential of using causal representation learning to improve the efficiency and interpretability of climate model emulation.* Methods: The paper uses the CDSD method to learn causal representations of climate data, including emissions, temperature, and precipitation.* Results: The paper evaluates the effectiveness of CDSD in rendering climate model emulation more efficient and interpretable, and sheds light on the challenges and limitations of using this approach.<details>
<summary>Abstract</summary>
Climate models, such as Earth system models (ESMs), are crucial for simulating future climate change based on projected Shared Socioeconomic Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated and invaluable, machine learning-based emulators trained on existing simulation data can project additional climate scenarios much faster and are computationally efficient. However, they often lack generalizability and interpretability. This work delves into the potential of causal representation learning, specifically the \emph{Causal Discovery with Single-parent Decoding} (CDSD) method, which could render climate model emulation efficient \textit{and} interpretable. We evaluate CDSD on multiple climate datasets, focusing on emissions, temperature, and precipitation. Our findings shed light on the challenges, limitations, and promise of using CDSD as a stepping stone towards more interpretable and robust climate model emulation.
</details>
<details>
<summary>æ‘˜è¦</summary>
ĞºĞ»Ğ¸Ğ¼Ğ°æ•°æ®æ¨¡å‹ï¼Œå¦‚åœ°çƒç³»ç»Ÿæ¨¡å‹ï¼ˆESMï¼‰ï¼Œæ˜¯æœªæ¥æ°”å€™å˜åŒ–çš„é¢„æµ‹åŸºç¡€ï¼ŒåŸºäºé¢„æµ‹çš„ç¤¾ä¼šç»æµè·¯å¾„ï¼ˆSSPï¼‰æ°”ä½“æ’æ”¾enarioã€‚è™½ç„¶ESMæ˜¯å¤æ‚ä¸”æ— ä»·çš„ï¼Œä½†æœºå™¨å­¦ä¹ åŸºäºç°æœ‰æ¨¡æ‹Ÿæ•°æ®çš„æ¨¡æ‹Ÿå™¨å¯ä»¥åœ¨å¿«é€Ÿå¹¶é«˜æ•ˆåœ°è¿›è¡Œæ°”å€™scenario projectionï¼Œä½†å®ƒä»¬é€šå¸¸ç¼ºä¹æ™®é€‚æ€§å’Œè§£é‡Šæ€§ã€‚è¿™ä¸ªå·¥ä½œæ¢è®¨äº†ä½¿ç”¨ causal representation learningï¼Œ Specifically the \emph{Causal Discovery with Single-parent Decoding} (CDSD) æ–¹æ³•ï¼Œä»¥å®ç°æ°”å€™æ¨¡å‹æ¨¡æ‹Ÿçš„æ•ˆç‡å’Œè§£é‡Šæ€§ã€‚æˆ‘ä»¬åœ¨å¤šä¸ªæ°”å€™æ•°æ®é›†ä¸Šè¯„ä¼°äº† CDSDï¼Œä¸“æ³¨äºæ’æ”¾ã€æ¸©åº¦å’Œé™æ°´ã€‚æˆ‘ä»¬çš„å‘ç°ç€é‡äºæŒ‘æˆ˜ã€å±€é™æ€§å’Œä½¿ç”¨ CDSD ä½œä¸ºæ›´åŠ è§£é‡Šæ€§å’Œå¯é çš„æ°”å€™æ¨¡å‹æ¨¡æ‹Ÿçš„å¯èƒ½æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Exploring-Error-Bits-for-Memory-Failure-Prediction-An-In-Depth-Correlative-Study"><a href="#Exploring-Error-Bits-for-Memory-Failure-Prediction-An-In-Depth-Correlative-Study" class="headerlink" title="Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study"></a>Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02855">http://arxiv.org/abs/2312.02855</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiao Yu, Wengui Zhang, Jorge Cardoso, Odej Kao</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æ¢è®¨å¤§è§„æ¨¡æ•°æ®ä¸­å¿ƒä¸­å­˜å‚¨å™¨å¤±æ•ˆçš„é—®é¢˜ï¼Œå°¤å…¶æ˜¯åŒinlineå­˜å‚¨æ¨¡å—(DIMM)çš„ç¼ºé™·ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†é”™è¯¯æ¯”ç‰¹ä¿¡æ¯æ¥é¢„æµ‹ä¸å¯ä¿®å¤çš„é”™è¯¯(UE)ã€‚</li>
<li>results: ç»è¿‡å®éªŒ validate çš„ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ï¼Œæ¯”å¯¹state-of-the-artç®—æ³•æé«˜F1åˆ†æ•°çº¦15%ï¼Œå¹¶ reduc è™šæ‹Ÿæœºä¸­æ–­çš„æ•°é‡çº¦59%ã€‚<details>
<summary>Abstract</summary>
In large-scale datacenters, memory failure is a common cause of server crashes, with uncorrectable errors (UEs) being a major indicator of Dual Inline Memory Module (DIMM) defects. Existing approaches primarily focus on predicting UEs using correctable errors (CEs), without fully considering the information provided by error bits. However, error bit patterns have a strong correlation with the occurrence of uncorrectable errors (UEs). In this paper, we present a comprehensive study on the correlation between CEs and UEs, specifically emphasizing the importance of spatio-temporal error bit information. Our analysis reveals a strong correlation between spatio-temporal error bits and UE occurrence. Through evaluations using real-world datasets, we demonstrate that our approach significantly improves prediction performance by 15% in F1-score compared to the state-of-the-art algorithms. Overall, our approach effectively reduces the number of virtual machine interruptions caused by UEs by approximately 59%.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§è§„æ¨¡æ•°æ®ä¸­å¿ƒä¸­ï¼Œå†…å­˜å¤±æ•ˆæ˜¯æœåŠ¡å™¨å´©æºƒçš„å¸¸è§åŸå› ï¼Œæ— æ³•ä¿®å¤çš„é”™è¯¯ï¼ˆUEï¼‰æ˜¯DIMMDefectsçš„é‡è¦æŒ‡æ ‡ã€‚ç°æœ‰çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨é¢„æµ‹CEsï¼Œæœªå……åˆ†è€ƒè™‘é”™è¯¯æ¯”ç‰¹ä¿¡æ¯ã€‚ç„¶è€Œï¼Œé”™è¯¯æ¯”ç‰¹æ¨¡å¼ä¸UEå‘ç”Ÿçš„å¯èƒ½æ€§å¼ºç›¸å…³ã€‚æœ¬æ–‡è¿›è¡Œäº†è¯¦ç»†çš„CEså’ŒUEsä¹‹é—´çš„ç›¸å…³æ€§åˆ†æï¼Œå°¤å…¶æ˜¯å…³æ³¨é”™è¯¯æ¯”ç‰¹çš„ç©ºé—´æ—¶é—´ä¿¡æ¯ã€‚æˆ‘ä»¬çš„åˆ†æå‘ç°ï¼Œé”™è¯¯æ¯”ç‰¹çš„ç©ºé—´æ—¶é—´ä¿¡æ¯ä¸UEå‘ç”Ÿçš„å¯èƒ½æ€§å¼ºç›¸å…³ã€‚ä½¿ç”¨å®é™…æ•°æ®è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ï¼Œä¸å½“å‰æœ€ä½³ç®—æ³•ç›¸æ¯”ï¼Œæé«˜F1åˆ†æ•°æŒ‡æ ‡15%ï¼Œå¹¶å°†è™šæ‹Ÿæœºä¸­æ–­å¼•èµ·çš„UEæ•°é‡å‡å°‘çº¦59%ã€‚
</details></li>
</ul>
<hr>
<h2 id="Inherent-limitations-of-LLMs-regarding-spatial-information"><a href="#Inherent-limitations-of-LLMs-regarding-spatial-information" class="headerlink" title="Inherent limitations of LLMs regarding spatial information"></a>Inherent limitations of LLMs regarding spatial information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03042">http://arxiv.org/abs/2312.03042</a></li>
<li>repo_url: None</li>
<li>paper_authors: He Yan, Xinyao Hu, Xiangpeng Wan, Chengyu Huang, Kai Zou, Shiqi Xu</li>
<li>for: This paper investigates the limitations of ChatGPT and similar models in spatial reasoning and navigation-related tasks, and evaluates their capabilities in 2D and 3D route planning.</li>
<li>methods: The paper introduces a novel evaluation framework and a baseline dataset specifically crafted for this study, which includes three key tasks: plotting spatial points, planning routes in 2D spaces, and devising pathways in 3D environments.</li>
<li>results: The evaluation reveals key insights into ChatGPTâ€™s capabilities and limitations in spatial understanding, highlighting the areas where the model struggles and where further improvement is needed.<details>
<summary>Abstract</summary>
Despite the significant advancements in natural language processing capabilities demonstrated by large language models such as ChatGPT, their proficiency in comprehending and processing spatial information, especially within the domains of 2D and 3D route planning, remains notably underdeveloped. This paper investigates the inherent limitations of ChatGPT and similar models in spatial reasoning and navigation-related tasks, an area critical for applications ranging from autonomous vehicle guidance to assistive technologies for the visually impaired. In this paper, we introduce a novel evaluation framework complemented by a baseline dataset, meticulously crafted for this study. This dataset is structured around three key tasks: plotting spatial points, planning routes in two-dimensional (2D) spaces, and devising pathways in three-dimensional (3D) environments. We specifically developed this dataset to assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals key insights into the model's capabilities and limitations in spatial understanding.
</details>
<details>
<summary>æ‘˜è¦</summary>
å°½ç®¡å¤§è¯­è¨€æ¨¡å‹å¦‚ChatGPTåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢åšå‡ºäº†é‡è¦çš„è¿›æ­¥ï¼Œä½†å®ƒä»¬åœ¨ç†è§£å’Œå¤„ç†ç©ºé—´ä¿¡æ¯æ–¹é¢ä»ç„¶å­˜åœ¨æ˜¾è‘—çš„ä¸è¶³ï¼Œç‰¹åˆ«æ˜¯åœ¨2Då’Œ3Dè·¯å¾„è§„åˆ’é¢†åŸŸã€‚è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†ChatGPTå’Œç±»ä¼¼æ¨¡å‹åœ¨ç©ºé—´ç†è§£å’Œå¯¼èˆªç›¸å…³ä»»åŠ¡ä¸­çš„å†…åœ¨å±€é™æ€§ï¼Œè¿™æ˜¯åº”ç”¨èŒƒå›´ä»è‡ªåŠ¨é©¾é©¶å¯¼èˆªåˆ°è§†éšœäººå£«åŠ©æ‰‹ç­‰é¢†åŸŸçš„å…³é”®é¢†åŸŸã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œå¹¶é™„åŠ äº†ä¸€ä¸ªç‰¹åˆ¶çš„åŸºçº¿æ•°æ®é›†ï¼Œç”¨äºè¿™é¡¹ç ”ç©¶ã€‚è¿™ä¸ªæ•°æ®é›†ç»“æ„åŒ–ä¸ºä¸‰ä¸ªå…³é”®ä»»åŠ¡ï¼šæè¿°ç©ºé—´ç‚¹ã€è®¡åˆ’2Dç©ºé—´è·¯å¾„å’Œ3Dç¯å¢ƒä¸­çš„è·¯å¾„è§„åˆ’ã€‚æˆ‘ä»¬ä¸“é—¨ä¸ºè¿™é¡¹ç ”ç©¶è€Œåˆ¶å®šäº†è¿™ä¸ªæ•°æ®é›†ï¼Œä»¥è¯„ä¼°ChatGPTçš„ç©ºé—´ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„è¯„ä¼°å‘ç°äº†ChatGPTåœ¨ç©ºé—´ç†è§£æ–¹é¢çš„é‡è¦ç¼ºé™·å’Œå±€é™æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Are-Vision-Transformers-More-Data-Hungry-Than-Newborn-Visual-Systems"><a href="#Are-Vision-Transformers-More-Data-Hungry-Than-Newborn-Visual-Systems" class="headerlink" title="Are Vision Transformers More Data Hungry Than Newborn Visual Systems?"></a>Are Vision Transformers More Data Hungry Than Newborn Visual Systems?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02843">http://arxiv.org/abs/2312.02843</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/buildingamind/vit-cot">https://github.com/buildingamind/vit-cot</a></li>
<li>paper_authors: Lalit Pandey, Samantha M. W. Wood, Justin N. Wood</li>
<li>for: æµ‹è¯•å¸¦æœ‰å­¦ä¹ èƒ½åŠ›çš„ViTså’ŒåŠ¨ç‰©ä¹‹é—´çš„æ¯”è¾ƒï¼Œä»¥ç¡®å®šViTsæ˜¯å¦éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®æ¥è¾¾åˆ°ç±»ä¼¼æ°´å¹³ã€‚</li>
<li>methods: ä½¿ç”¨è‡ªæˆ‘ç›‘ç£çš„ViTsï¼Œé€šè¿‡æ—¶é—´ä½œä¸ºæ•™å­¦ä¿¡å·ï¼Œä¸ç”Ÿç‰©è§†ç³»ç»Ÿç›¸ä¼¼ã€‚</li>
<li>results: ViTsåœ¨æ–°ç”Ÿé¸¡çœ¼ä¸­è®­ç»ƒæ—¶èƒ½å¤Ÿè§£å†³åŒæ ·çš„è§†åå˜å¯¹è±¡è¯†åˆ«ä»»åŠ¡ï¼Œä¸æ–°ç”Ÿé¸¡ä¸€æ ·å­¦ä¹ äº†è§†åå˜å¯¹è±¡è¡¨ç¤ºã€‚ViTsä¸æ˜¯éœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®çš„ï¼Œä¸¤è€…éƒ½å¯ä»¥åœ¨ç©·å‡ ä½•ç¯å¢ƒä¸­å­¦ä¹ è§†åå˜å¯¹è±¡è¡¨ç¤ºã€‚<details>
<summary>Abstract</summary>
Vision transformers (ViTs) are top performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks. However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more data hungry than brains, with ViTs requiring more training data to reach similar levels of performance. To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled rearing experiments on ViTs and newborn chicks. We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine. We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self supervised ViTs that leverage time as a teaching signal, akin to biological visual systems. When ViTs were trained through the eyes of newborn chicks, the ViTs solved the same view invariant object recognition tasks as the chicks. Thus, ViTs were not more data hungry than newborn visual systems: both learned view invariant object representations in impoverished visual environments. The flexible and generic attention based learning mechanism in ViTs combined with the embodied data streams available to newborn animals appears sufficient to drive the development of animal-like object recognition.
</details>
<details>
<summary>æ‘˜è¦</summary>
è§†åŠ›å˜æ¢å™¨ï¼ˆViTï¼‰æ˜¯è®¡ç®—æœºè§†è§‰benchmarkä¸Šè¡¨ç°å‡ºè‰²çš„æ¨¡å‹ï¼Œå¯ä»¥å‡†ç¡®é¢„æµ‹äººç±»è¡Œä¸ºã€‚ç„¶è€Œï¼Œç ”ç©¶äººå‘˜å¯¹ä½¿ç”¨ViTä½œä¸ºç”Ÿç‰©å­¦å­¦ä¹ æ¨¡å‹è¡¨ç¤ºæ€€ç–‘ï¼Œå› ä¸ºViTè¢«è®¤ä¸ºéœ€è¦æ›´å¤šçš„è®­ç»ƒæ•°æ®æ¥è¾¾åˆ°ç›¸ä¼¼æ°´å¹³ã€‚ä¸ºäº†æµ‹è¯•è¿™ä¸ªå‡è®¾ï¼Œæˆ‘ä»¬ç›´æ¥æ¯”è¾ƒäº†ViTå’ŒåŠ¨ç‰©çš„å­¦ä¹ èƒ½åŠ›ï¼Œé€šè¿‡åœ¨ViTå’Œæ–°ç”Ÿé¸¡çš„å¹³è¡Œæ§åˆ¶å…»æ®–å®éªŒä¸­è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬é¦–å…ˆå°†é¸¡åœ¨ç¼ºä¹è§†è§‰ç¯å¢ƒä¸­å…»æ®–ï¼Œç„¶åé€šè¿‡åœ¨è™šæ‹ŸåŠ¨ç‰©å®¤ä¸­æ¨¡æ‹Ÿè¿™äº›ç¯å¢ƒä¸­å¯ç”¨çš„è®­ç»ƒæ•°æ®ï¼Œåœ¨è§†é¢‘æ¸¸æˆå¼•æ“ä¸­å»ºç«‹è™šæ‹ŸåŠ¨ç‰©å®¤ã€‚æˆ‘ä»¬è®°å½•äº†é€šè¿‡ä»£ç†äººåœ¨è™šæ‹ŸåŠ¨ç‰©å®¤ä¸­ç§»åŠ¨æ—¶è·å¾—çš„ç¬¬ä¸€äººç§°å›¾åƒï¼Œå¹¶ä½¿ç”¨è¿™äº›å›¾åƒæ¥è®­ç»ƒåŸºäºæ—¶é—´çš„æ•™å­¦ä¿¡å·çš„è‡ªæˆ‘è¶…vised ViTsã€‚å½“ViTsé€šè¿‡æ–°ç”Ÿé¸¡çš„çœ¼ç›è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒViTsè§£å†³äº†åŒæ ·çš„è§†è§’ä¸å˜object recognitionä»»åŠ¡ï¼Œä¸é¸¡ä¸€æ ·ã€‚å› æ­¤ï¼ŒViTsä¸æ˜¯æ›´éœ€è¦æ•°æ®çš„ thanæ–°ç”Ÿè§†ç³»ç»Ÿï¼šboth learned view-invariant object representations in impoverished visual environmentsã€‚ViTsçš„çµæ´»å’Œé€šç”¨çš„æ³¨æ„åŠ›åŸºæœ¬å­¦ä¹ æœºåˆ¶ï¼ŒåŠ ä¸Šå¯ä»¥ç»™æ–°ç”ŸåŠ¨ç‰©æä¾›çš„embodiedæ•°æ®æµï¼Œè¶³ä»¥é©±åŠ¨åŠ¨ç‰©å¦‚object recognitionçš„å‘å±•ã€‚
</details></li>
</ul>
<hr>
<h2 id="MIMONets-Multiple-Input-Multiple-Output-Neural-Networks-Exploiting-Computation-in-Superposition"><a href="#MIMONets-Multiple-Input-Multiple-Output-Neural-Networks-Exploiting-Computation-in-Superposition" class="headerlink" title="MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition"></a>MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02829">http://arxiv.org/abs/2312.02829</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/multiple-input-multiple-output-nets">https://github.com/ibm/multiple-input-multiple-output-nets</a></li>
<li>paper_authors: Nicolas Menet, Michael Hersche, Geethan Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi</li>
<li>for: è¿™ç¯‡ç ”ç©¶ç›®çš„æ˜¯æå‡ºä¸€ç§å¤šè¾“å…¥å¤šå‡ºåŠ›ç¥ç»ç½‘ç»œï¼ˆMIMONetï¼‰ï¼Œä»¥é™ä½æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®¡ç®—æˆæœ¬ã€‚</li>
<li>methods: MIMONetä½¿ç”¨å˜é‡ç»‘å®šæœºåˆ¶å°†å¤šä¸ªè¾“å…¥æ•°æ®ç»“æ„åŒ–ä¸ºä¸€ä¸ªå›ºå®šå®½åº¦çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼Œå¹¶é‡‡ç”¨å¤šè¾“å…¥å¤šå‡ºåŠ›ç¥ç»ç½‘ç»œæ¶æ„æ¥å¤„ç†æ•°æ®ç»“æ„çš„æ•´ä½“éçº¿æ€§å˜æ¢ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒMIMOConvå’ŒMIMOFormerå¯ä»¥åœ¨ååé‡å’Œå‡†ç¡®ç‡ä¹‹é—´å®ç°åè°ƒçš„è´¨é‡å’Œé€Ÿåº¦è¡¡é‡ï¼Œå¹¶åœ¨CIFAR10å’ŒCIFAR100ä¸Šå®ç°2-4å€çš„é€Ÿåº¦æå‡ï¼Œè€Œæ— éœ€æ›´æ”¹æ¨¡å‹å‚æ•°ã€‚<details>
<summary>Abstract</summary>
With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Calibrated-Adaptive-Teacher-for-Domain-Adaptive-Intelligent-Fault-Diagnosis"><a href="#Calibrated-Adaptive-Teacher-for-Domain-Adaptive-Intelligent-Fault-Diagnosis" class="headerlink" title="Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis"></a>Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02826">http://arxiv.org/abs/2312.02826</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florent Forest, Olga Fink</li>
<li>for: æœ¬ç ”ç©¶é’ˆå¯¹æ™ºèƒ½å¼‚å¸¸è¯Šæ–­ï¼ˆIFDï¼‰åŸºäºæ·±åº¦å­¦ä¹ çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯å½“æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦é€‚åº”ä¸åŒçš„æ“ä½œæ¡ä»¶æ—¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ä¸ç¡®å®Ÿé¢„æµ‹ï¼ˆ pseudo-labelï¼‰çš„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å°†è¿™äº›é¢„æµ‹ä¸ç›®æ ‡é¢†åŸŸçš„æ ‡ç­¾è¿›è¡Œæ•´åˆï¼Œä»¥æé«˜æ¨¡å‹çš„é€‚åº”èƒ½åŠ›ã€‚</li>
<li>results: æœ¬ç ”ç©¶åœ¨domain-adaptive IFDä¸­æå‡ºäº†ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œå³å¯¹æ•™å¸ˆç½‘ç»œçš„é¢„æµ‹è¿›è¡Œè°ƒæ•´ï¼Œä½¿ç”¨åç»­è°ƒæ•´æŠ€æœ¯æ¥æ”¹å–„é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨Paderbornbenchmarkä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶å–å¾—äº†æœ€ä½³çš„è½¬ç§»ä»»åŠ¡æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an effective and flexible solution, attracting extensive research. Deep neural networks can learn rich representations from vast amounts of representative labeled data for various applications. In IFD, they achieve high classification performance from signals in an end-to-end manner, without requiring extensive domain knowledge. However, deep learning models usually only perform well on the data distribution they have been trained on. When applied to a different distribution, they may experience performance drops. This is also observed in IFD, where assets are often operated in working conditions different from those in which labeled data have been collected. Unsupervised domain adaptation (UDA) deals with the scenario where labeled data are available in a source domain, and only unlabeled data are available in a target domain, where domains may correspond to operating conditions. Recent methods rely on training with confident pseudo-labels for target samples. However, the confidence-based selection of pseudo-labels is hindered by poorly calibrated confidence estimates in the target domain, primarily due to over-confident predictions, which limits the quality of pseudo-labels and leads to error accumulation. In this paper, we propose a novel UDA method called Calibrated Adaptive Teacher (CAT), where we propose to calibrate the predictions of the teacher network throughout the self-training process, leveraging post-hoc calibration techniques. We evaluate CAT on domain-adaptive IFD and perform extensive experiments on the Paderborn benchmark for bearing fault diagnosis under varying operating conditions. Our proposed method achieves state-of-the-art performance on most transfer tasks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ™ºèƒ½æ•…éšœè¯Šæ–­ï¼ˆIFDï¼‰åŸºäºæ·±åº¦å­¦ä¹ å·²ç»è¯æ˜æ˜¯ä¸€ç§æœ‰æ•ˆå’Œçµæ´»çš„è§£å†³æ–¹æ¡ˆï¼Œå¸å¼•äº†å¹¿æ³›çš„ç ”ç©¶ã€‚æ·±åº¦ç¥ç»ç½‘ç»œå¯ä»¥ä»å¤§é‡çš„è¡¨ç¤ºæ€§æ•°æ®ä¸­å­¦ä¹ ä¸°å¯Œçš„è¡¨ç¤ºï¼Œç”¨äºå¤šç§åº”ç”¨ã€‚åœ¨IFDä¸­ï¼Œå®ƒä»¬åœ¨ç»ˆç«¯åˆ°ç»ˆç‚¹çš„æ–¹å¼ä¸­è¾¾åˆ°é«˜çš„åˆ†ç±»æ€§èƒ½ï¼Œä¸éœ€è¦å…·æœ‰å¹¿æ³›çš„é¢†åŸŸçŸ¥è¯†ã€‚ç„¶è€Œï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹é€šå¸¸åªèƒ½åœ¨å®ƒä»¬è¢«è®­ç»ƒçš„æ•°æ®åˆ†å¸ƒä¸Šperform wellã€‚å½“åº”ç”¨äºä¸åŒçš„åˆ†å¸ƒæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šç»å†æ€§èƒ½ä¸‹é™ã€‚è¿™ä¹Ÿæ˜¯IFDä¸­æ‰€è§çš„æƒ…å†µï¼Œ Ğ³Ğ´Ğµ assets ç»å¸¸åœ¨ä¸åŒçš„æ“ä½œæ¡ä»¶ä¸‹è¿è¡Œã€‚è¿™é‡Œçš„é—®é¢˜æ˜¯ï¼Œå½“å®ƒä»¬è¢«åº”ç”¨åˆ°ä¸åŒçš„åˆ†å¸ƒæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šç»å†æ€§èƒ½ä¸‹é™ã€‚è¿™ä¹Ÿæ˜¯IFDä¸­æ‰€è§çš„æƒ…å†µï¼Œ where assets ç»å¸¸åœ¨ä¸åŒçš„æ“ä½œæ¡ä»¶ä¸‹è¿è¡Œã€‚è¿™ä¸ªé—®é¢˜è¢«ç§°ä¸ºåŸŸ Adaptationï¼ˆUAï¼‰ã€‚recent methods rely on training with confident pseudo-labels for target samples. However, the confidence-based selection of pseudo-labels is hindered by poorly calibrated confidence estimates in the target domain, primarily due to over-confident predictions, which limits the quality of pseudo-labels and leads to error accumulation.åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„UAæ–¹æ³•ï¼Œå«åšCalibrated Adaptive Teacherï¼ˆCATï¼‰ã€‚æˆ‘ä»¬æè®®åœ¨è‡ªæˆ‘æ•™å­¦è¿‡ç¨‹ä¸­ä¸æ–­åœ°calibrate the predictions of the teacher networkï¼Œåˆ©ç”¨åæœŸcalibrationæŠ€æœ¯ã€‚æˆ‘ä»¬åœ¨domain-adaptive IFDä¸­è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶åœ¨Paderbornbenchmarkä¸Šè¿›è¡Œäº†å¤šä¸ªè½¬ç§»ä»»åŠ¡çš„è¯„ä¼°ã€‚æˆ‘ä»¬çš„æå‡ºæ–¹æ³•å®ç°äº†çŠ¶æ€æœºå™¨çš„æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Sample-based-Dynamic-Hierarchical-Transformer-with-Layer-and-Head-Flexibility-via-Contextual-Bandit"><a href="#Sample-based-Dynamic-Hierarchical-Transformer-with-Layer-and-Head-Flexibility-via-Contextual-Bandit" class="headerlink" title="Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit"></a>Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03038">http://arxiv.org/abs/2312.03038</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fanfei Meng, Lele Zhang, Yu Chen, Yuxin Wang<br>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†æå‡ºä¸€ç§åŸºäºæ ·æœ¬çš„åŠ¨æ€å±‚æ¬¡å˜æ¢å™¨ï¼ˆDHTï¼‰æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€é…ç½®å±‚å’Œå¤´æ•°ï¼Œä»¥é€‚åº”å…·ä½“çš„æ ·æœ¬å¤æ‚æ€§ã€‚methods: è¯¥æ¨¡å‹ä½¿ç”¨äº†è§£Contextual Bandit Problemsæ¥å†³å®šå±‚å’Œå¤´çš„æ•°é‡ï¼Œå¹¶ä½¿ç”¨Combinatorial Thompson Samplingæ¥é€‰æ‹©ç‰¹å®šçš„å¤´ç»„åˆã€‚results: å¯¹æ¯”ä¼ ç»Ÿå‹ç¼©å·²ç»è®­ç»ƒè¿‡çš„ç½‘ç»œè¿›è¡Œæ¨ç†ï¼ŒDHTæ¨¡å‹å¯ä»¥åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­å®ç°æ›´å¤§çš„è®¡ç®—æˆæœ¬å‡å°‘ï¼ˆæœ€é«˜è¾¾74%ï¼‰ï¼ŒåŒæ—¶å‡å°‘äº†ç²¾åº¦çš„æŸå¤±ã€‚<details>
<summary>Abstract</summary>
Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system. According to the experiment results, we achieve up to 74% computational savings for both training and inference with a minimal loss of accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> transformer éœ€è¦å›ºå®šçš„å±‚æ•°å’Œå¤´æ•°ï¼Œè¿™ä½¿å¾—å®ƒä»¬åœ¨ä¸ªä½“æ ·æœ¬çš„å¤æ‚æ€§å’Œè®­ç»ƒå’Œæ¨ç†æˆæœ¬æ–¹é¢ä¸çµæ´»ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä¸€ç§åŸºäºå•ä¸ªæ•°æ®æ ·æœ¬çš„åŠ¨æ€å±‚æ¬¡Transformerï¼ˆDHTï¼‰æ¨¡å‹ï¼Œå…¶å±‚æ•°å’Œå¤´æ•°å¯ä»¥é€šè¿‡è§£å†³ä¸Šä¸‹æ–‡ual bandité—®é¢˜æ¥åŠ¨æ€é…ç½®ã€‚ä¸ºç¡®å®šå±‚æ•°å’Œå¤´æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨å‡åŒ€ä¿¡ä»»åŒºboundï¼Œè€Œåœ¨é€‰æ‹©ç‰¹å®šå¤´ç»„åˆæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ combinatorial Thompson Samplingã€‚ä¸å‰ä¸€äº›ç ”ç©¶æ‰€åšçš„å‹ç¼©å·²è®­ç»ƒç½‘ç»œä»¥ä¾¿åªè¿›è¡Œæ¨ç†æ—¶è¿›è¡Œå‹ç¼©ä¸åŒï¼ŒDHT ä¸ä»…åœ¨è®­ç»ƒæœŸé—´é€‚åº”æ€§åœ°ä¼˜åŒ–åŸºç¡€ç½‘ç»œç»“æ„ï¼Œè¿˜å…·æœ‰é«˜æ•ˆçš„æ¨ç†ç½‘ç»œã€‚æ ¹æ®å®éªŒç»“æœï¼Œæˆ‘ä»¬å¯ä»¥è¾¾åˆ°74%çš„è®¡ç®—å‡å°‘é‡ï¼ŒåŒæ—¶å‡å°‘ç²¾åº¦æŸå¤±ã€‚Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Clustering-Pseudo-Language-Family-in-Multilingual-Translation-Models-with-Fisher-Information-Matrix"><a href="#Clustering-Pseudo-Language-Family-in-Multilingual-Translation-Models-with-Fisher-Information-Matrix" class="headerlink" title="Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix"></a>Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02820">http://arxiv.org/abs/2312.02820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecoli-hit/pseudofamily">https://github.com/ecoli-hit/pseudofamily</a></li>
<li>paper_authors: Xinyu Ma, Xuebo Liu, Min Zhang</li>
<li>for: The paper is written to address the challenge of clustering languages based solely on their ancestral families, which can yield suboptimal results due to variations in the datasets employed during the modelâ€™s training phase.</li>
<li>methods: The paper introduces an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation modelâ€™s characteristics. The method defines pseudo language families based on the similarity of the effects of language pairs on model parameters.</li>
<li>results: The paper shows that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements.<details>
<summary>Abstract</summary>
In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics. We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively. This concept has led us to define pseudo language families. We provide an in-depth discussion regarding the inception and application of these pseudo language families. Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements. The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨å¤šè¯­è¨€ç¿»è¯‘ç ”ç©¶ä¸­ï¼Œè¯­è¨€å®¶æ—çš„ç†è§£å’Œåˆ©ç”¨å¯¹ Paramount importance ã€‚ç„¶è€Œï¼ŒåŸºäºç¥–è¯­è¨€å®¶æ—æ¥åˆ†ç±»è¯­è¨€å¯èƒ½ä¼šå¯¼è‡´ä¸ä¼˜åŒ–çš„ç»“æœï¼Œå› ä¸ºåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µä½¿ç”¨çš„æ•°æ®é›†å¯èƒ½å­˜åœ¨å·®å¼‚ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨é±¼ç±»ä¿¡æ¯çŸ©é˜µï¼ˆFIMï¼‰æ¥åˆ†ç±»è¯­è¨€å®¶æ—ï¼ŒåŸºäºå¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹çš„ç‰¹ç‚¹ã€‚æˆ‘ä»¬å‡è®¾è¯­è¨€å¯¹è±¡ä¹‹é—´çš„æ•ˆæœç›¸ä¼¼æ€§å¾ˆé«˜ï¼Œåˆ™åº”è¯¥å°†å…¶å½’ç±»ä¸ºä¸€ä¸ª cohesive çš„è¯­è¨€å®¶æ—ã€‚è¿™ä¸ªæ¦‚å¿µå¯¼è‡´æˆ‘ä»¬å®šä¹‰äº† pseudo è¯­è¨€å®¶æ—ã€‚æˆ‘ä»¬æä¾›äº†æ·±å…¥çš„è®¨è®ºå’Œåº”ç”¨ pseudo è¯­è¨€å®¶æ—çš„æ–¹æ³•ã€‚å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ pseudo è¯­è¨€å®¶æ—å¯ä»¥è¶…è¿‡ä¼ ç»Ÿè¯­è¨€å®¶æ—åœ¨é€‚åº”æœªçŸ¥è¯­è¨€å¯¹çš„æ€§èƒ½ã€‚æ­¤æ–¹æ³•ä¹Ÿå¯ä»¥æ‰©å±•åˆ°éœ€è¦è¯­è¨€ç›¸ä¼¼åº¦æµ‹é‡çš„åœºæ™¯ã€‚è¯¦ç»†çš„ä»£ç å’Œç›¸å…³è„šæœ¬å¯ä»¥åœ¨ GitHub ä¸Šè·å–ï¼Œè¯·å‚è€ƒ <https://github.com/ecoli-hit/PseudoFamily>ã€‚
</details></li>
</ul>
<hr>
<h2 id="BIVDiff-A-Training-Free-Framework-for-General-Purpose-Video-Synthesis-via-Bridging-Image-and-Video-Diffusion-Models"><a href="#BIVDiff-A-Training-Free-Framework-for-General-Purpose-Video-Synthesis-via-Bridging-Image-and-Video-Diffusion-Models" class="headerlink" title="BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models"></a>BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02813">http://arxiv.org/abs/2312.02813</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, Limin Wang</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºæ–‡æœ¬çš„é€šç”¨è§†é¢‘ç”Ÿæˆæ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰è§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç¼ºç‚¹ï¼Œå¦‚éœ€è¦å¤§é‡çš„å­˜å‚¨å’Œè®¡ç®—èµ„æºã€ç¼ºä¹ä»»åŠ¡æ³›åŒ–å’Œé«˜æ•ˆæ€§ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº†ä¸€ç§å«åš BIVDiff çš„æ¡†æ¶ï¼Œå®ƒå°†ç‰¹å®šçš„å›¾åƒæ‰©æ•£æ¨¡å‹å’Œé€šç”¨çš„æ–‡æœ¬åˆ°è§†é¢‘æ‰©æ•£æ¨¡å‹ç›¸è¿æ¥ï¼Œä»¥å®ç°æ— éœ€è®­ç»ƒçš„è§†é¢‘ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆä½¿ç”¨å›¾åƒæ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ ControlNetã€Instruct Pix2Pixï¼‰è¿›è¡Œå¸§çº§è§†é¢‘ç”Ÿæˆï¼Œç„¶åä½¿ç”¨æ··åˆå€’æ•°æ³•å¯¹ç”Ÿæˆçš„è§†é¢‘è¿›è¡Œ temporal smoothingï¼Œæœ€åè¾“å…¥æ··åˆå€’æ•°åçš„ç¼“å­˜è¿›å…¥è§†é¢‘æ‰©æ•£æ¨¡å‹è¿›è¡Œæ¨¡å‹åŒ–ã€‚</li>
<li>results: è¿™ paper é€šè¿‡ä¸€ç³»åˆ—çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚å¯æ§çš„è§†é¢‘ç”Ÿæˆã€è§†é¢‘ç¼–è¾‘ã€è§†é¢‘å¡«å……å’Œè§†é¢‘å‰”é™¤ç­‰ï¼Œè¯æ˜äº† BIVDiff æ¡†æ¶çš„æœ‰æ•ˆæ€§å’Œé€šç”¨æ€§ã€‚<details>
<summary>Abstract</summary>
Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and compute overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept. Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing. Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation video editing, video inpainting and outpainting. Our project page is available at https://bivdiff.github.io.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusion models have made tremendous progress in text-driven image and video generation. Now text-to-image foundation models are widely applied to various downstream image synthesis tasks, such as controllable image generation and image editing, while downstream video synthesis tasks are less explored for several reasons. First, it requires huge memory and compute overhead to train a video generation foundation model. Even with video foundation models, additional costly training is still required for downstream video synthesis tasks. Second, although some works extend image diffusion models into videos in a training-free manner, temporal consistency cannot be well kept. Finally, these adaption methods are specifically designed for one task and fail to generalize to different downstream video synthesis tasks. To mitigate these issues, we propose a training-free general-purpose video synthesis framework, coined as BIVDiff, via bridging specific image diffusion models and general text-to-video foundation diffusion models. Specifically, we first use an image diffusion model (like ControlNet, Instruct Pix2Pix) for frame-wise video generation, then perform Mixed Inversion on the generated video, and finally input the inverted latents into the video diffusion model for temporal smoothing. Decoupling image and video models enables flexible image model selection for different purposes, which endows the framework with strong task generalization and high efficiency. To validate the effectiveness and general use of BIVDiff, we perform a wide range of video generation tasks, including controllable video generation, video editing, video inpainting, and video outpainting. Our project page is available at <https://bivdiff.github.io>.
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Domain-Adaptation-and-Data-Augmentation-to-Improve-Qurâ€™anic-IR-in-English-and-Arabic"><a href="#Leveraging-Domain-Adaptation-and-Data-Augmentation-to-Improve-Qurâ€™anic-IR-in-English-and-Arabic" class="headerlink" title="Leveraging Domain Adaptation and Data Augmentation to Improve Qurâ€™anic IR in English and Arabic"></a>Leveraging Domain Adaptation and Data Augmentation to Improve Qurâ€™anic IR in English and Arabic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02803">http://arxiv.org/abs/2312.02803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vera Pavlova<br>for:* è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯è§£å†³é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­ä¸­çš„å¤å…°ç»ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰é—®é¢˜ã€‚methods:* ä½¿ç”¨æœ€æ–°çš„ neural IR æ–¹æ³•è¿›è¡Œç ”ç©¶ï¼Œä»¥ä¾¿æ›´æœ‰æ•ˆåœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚* ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ¥å¤„ç†ç¼ºä¹åŸŸé¢†åŸŸæ•°æ®çš„é—®é¢˜ã€‚results:* ä½¿ç”¨åŸŸpecific language modelï¼ˆLMï¼‰å’ŒåŸŸé¢†åŸŸæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥å¤§å¹…æé«˜MRR@10å’ŒNDCG@5 metricsä¸­çš„è¡¨ç°ï¼Œåˆ›é€ äº†å¤å…°ç»IRä¸­çš„æ–°çºªå½•ã€‚<details>
<summary>Abstract</summary>
In this work, we approach the problem of Qur'anic information retrieval (IR) in Arabic and English. Using the latest state-of-the-art methods in neural IR, we research what helps to tackle this task more efficiently. Training retrieval models requires a lot of data, which is difficult to obtain for training in-domain. Therefore, we commence with training on a large amount of general domain data and then continue training on in-domain data. To handle the lack of in-domain data, we employed a data augmentation technique, which considerably improved results in MRR@10 and NDCG@5 metrics, setting the state-of-the-art in Qur'anic IR for both English and Arabic. The absence of an Islamic corpus and domain-specific model for IR task in English motivated us to address this lack of resources and take preliminary steps of the Islamic corpus compilation and domain-specific language model (LM) pre-training, which helped to improve the performance of the retrieval models that use the domain-specific LM as the shared backbone. We examined several language models (LMs) in Arabic to select one that efficiently deals with the Qur'anic IR task. Besides transferring successful experiments from English to Arabic, we conducted additional experiments with retrieval task in Arabic to amortize the scarcity of general domain datasets used to train the retrieval models. Handling Qur'anic IR task combining English and Arabic allowed us to enhance the comparison and share valuable insights across models and languages.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é¢ä¸´ç€å¤å…°ç»ä¿¡æ¯æ£€ç´¢ï¼ˆIRï¼‰ä»»åŠ¡çš„æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨é˜¿æ‹‰ä¼¯è¯­å’Œè‹±è¯­ä¹‹é—´ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€æ–°çš„çŠ¶æ€è‰ºæœ¯æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å› ä¸ºåŸ¹è®­æ£€ç´¢æ¨¡å‹éœ€è¦å¾ˆå¤šæ•°æ®ï¼Œä½†è¿™äº›æ•°æ®å¾ˆéš¾ä»¥è·å¾—ï¼Œæˆ‘ä»¬å› æ­¤å¼€å§‹ä½¿ç”¨é€šç”¨é¢†åŸŸæ•°æ®è¿›è¡ŒåŸ¹è®­ï¼Œç„¶åç»§ç»­ä½¿ç”¨åŸŸä¸“æ•°æ®è¿›è¡ŒåŸ¹è®­ã€‚ä¸ºäº†å¤„ç†ç¼ºä¹åŸŸä¸“æ•°æ®çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®æ‰©å……æŠ€æœ¯ï¼Œè¿™æœ‰æ•ˆåœ°æé«˜äº†MRR@10å’ŒNDCG@5æŒ‡æ ‡çš„ç»“æœï¼Œå¹¶ä¸ºå¤å…°ç»IRä»»åŠ¡è®¾ç½®äº†æ–°çš„å·ä¾›åº”ã€‚ç”±äºè‹±è¯­ä¸­æ²¡æœ‰ä¼Šæ–¯å…°å·ç§¯å’ŒåŸŸä¸“è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬è¢«åŠ¨åœ°åšå‡ºäº†è¿™äº›ç¼ºå¤±çš„è¡¥åšï¼ŒåŒ…æ‹¬ä¼Šæ–¯å…°å·ç§¯é›†æˆå’ŒåŸŸä¸“è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€‚æˆ‘ä»¬åœ¨é˜¿æ‹‰ä¼¯è¯­ä¸­é€‰æ‹©äº†ä¸€ä¸ªé«˜æ•ˆåœ°å¤„ç†å¤å…°ç»IRä»»åŠ¡çš„è¯­è¨€æ¨¡å‹ã€‚é™¤äº†å°†æˆåŠŸçš„å®éªŒä»è‹±è¯­è½¬ç§»åˆ°é˜¿æ‹‰ä¼¯è¯­ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†é¢å¤–çš„æ£€ç´¢ä»»åŠ¡å®éªŒï¼Œä»¥åˆ©ç”¨é€šç”¨é¢†åŸŸæ•°æ®æ¥åŸ¹è®­æ£€ç´¢æ¨¡å‹ã€‚é€šè¿‡ç»“åˆè‹±è¯­å’Œé˜¿æ‹‰ä¼¯è¯­æ¥å¤„ç†å¤å…°ç»IRä»»åŠ¡ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæé«˜å¯¹æ¨¡å‹å’Œè¯­è¨€ä¹‹é—´çš„æ¯”è¾ƒå’Œå…±äº«æœ‰ä»·å€¼çš„å‘ç°ã€‚
</details></li>
</ul>
<hr>
<h2 id="PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features"><a href="#PMMTalk-Speech-Driven-3D-Facial-Animation-from-Complementary-Pseudo-Multi-modal-Features" class="headerlink" title="PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features"></a>PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02781">http://arxiv.org/abs/2312.02781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshun Han, Shengnan Gui, Yiqing Huang, Baihui Li, Lijian Liu, Benjia Zhou, Ning Jiang, Quan Lu, Ruicong Zhi, Yanyan Liang, Du Zhang, Jun Wan</li>
<li>for: æé«˜Speech-driven 3D facial animationçš„ç²¾åº¦å’Œå‡†ç¡®æ€§ï¼Œå¹¶ä¸”ä½¿ç”¨å¤šmodalä¿¡æ¯ï¼ˆè§†è§‰å’Œæ–‡æœ¬ï¼‰æ¥æé«˜ resultsçš„å¯é æ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>methods: æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œå³PMMTalkï¼Œä½¿ç”¨è¡¥å……çš„ Pseudo Multi-Modal featuresæ¥æé«˜ facial animation çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šPMMTalk encoderã€cross-modal alignment moduleå’ŒPMMTalk decoderã€‚ç‰¹åˆ«æ˜¯ï¼ŒPMMTalk encoderä½¿ç”¨äº†å¸‚åœºä¸Šå¯å¾—çš„ talking head generation architectureå’Œspeech recognitionæŠ€æœ¯æ¥ä»speechä¸­æå–è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚ç„¶åï¼Œcross-modal alignment moduleå°† audio-image-textç‰¹å¾è¿›è¡Œäº†æ—¶é—´å’ŒSemantic Water levelçš„å¯¹é½ã€‚æœ€åï¼ŒPMMTalk decoderç”¨äºé¢„æµ‹lip-syncing facial blendshape coefficientsã€‚ä¸å…ˆå‰çš„æ–¹æ³•ä¸åŒçš„æ˜¯ï¼ŒPMMTalkåªéœ€è¦ä¸€ä¸ªéšæœºçš„å‚è€ƒé¢å­”å›¾åƒï¼Œä½†å®ƒå¯ä»¥æä¾›æ›´é«˜çš„å‡†ç¡®æ€§ã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºæ ‡å‡†åŠ¨ç”»ç”Ÿäº§è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è½»æ¾åœ° Ğ¸Ğ½Ñ‚ĞµGRATEåˆ°ç°æœ‰çš„å·¥ä½œæµç¨‹ä¸­ã€‚</li>
<li>results: å¯¹æ¯”å…ˆå‰çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨3D facial animationä¸­æé«˜äº†ç²¾åº¦å’Œå¯é æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåˆ›å»ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„3D Chinese Audio-Visual Facial Animationï¼ˆ3D-CAVFAï¼‰æ•°æ®é›†ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ¢ç´¢å’Œæ”¹è¿›è¿™ä¸ªé¢†åŸŸã€‚User studyè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨è‰ºæœ¯å®¶å’Œç”¨æˆ·ä¹‹é—´æä¾›æ›´å¥½çš„æ»¡æ„åº¦å’Œä½“éªŒã€‚<details>
<summary>Abstract</summary>
Speech-driven 3D facial animation has improved a lot recently while most related works only utilize acoustic modality and neglect the influence of visual and textual cues, leading to unsatisfactory results in terms of precision and coherence. We argue that visual and textual cues are not trivial information. Therefore, we present a novel framework, namely PMMTalk, using complementary Pseudo Multi-Modal features for improving the accuracy of facial animation. The framework entails three modules: PMMTalk encoder, cross-modal alignment module, and PMMTalk decoder. Specifically, the PMMTalk encoder employs the off-the-shelf talking head generation architecture and speech recognition technology to extract visual and textual information from speech, respectively. Subsequently, the cross-modal alignment module aligns the audio-image-text features at temporal and semantic levels. Then PMMTalk decoder is employed to predict lip-syncing facial blendshape coefficients. Contrary to prior methods, PMMTalk only requires an additional random reference face image but yields more accurate results. Additionally, it is artist-friendly as it seamlessly integrates into standard animation production workflows by introducing facial blendshape coefficients. Finally, given the scarcity of 3D talking face datasets, we introduce a large-scale 3D Chinese Audio-Visual Facial Animation (3D-CAVFA) dataset. Extensive experiments and user studies show that our approach outperforms the state of the art. We recommend watching the supplementary video.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œå¯¹è¯é©±åŠ¨çš„3Dé¢éƒ¨åŠ¨ç”»æŠ€æœ¯åœ¨æœ€è¿‘æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ï¼Œä½†å¤§å¤šæ•°ç›¸å…³å·¥ä½œåªä½¿ç”¨éŸ³é¢‘æ¨¡å¼ï¼Œå¿½ç•¥è§†è§‰å’Œæ–‡æœ¬cueï¼Œå¯¼è‡´ç²¾åº¦å’Œä¸€è‡´æ€§ä¸ satisfactoryã€‚æˆ‘ä»¬è®¤ä¸ºè§†è§‰å’Œæ–‡æœ¬cueä¸æ˜¯rivialçš„ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³PMMTalkï¼Œä½¿ç”¨è¡¥å…… Pseudo Multi-Modal featureæ¥æé«˜é¢éƒ¨åŠ¨ç”»çš„å‡†ç¡®æ€§ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä¸‰ä¸ªæ¨¡å—ï¼šPMMTalkç¼–ç å™¨ã€äº¤å‰æ¨¡å¼å¯¹æ¥æ¨¡å—å’ŒPMMTalkè§£ç å™¨ã€‚å…·ä½“æ¥è¯´ï¼ŒPMMTalkç¼–ç å™¨ä½¿ç”¨ comercial off-the-shelf talking headç”Ÿæˆæ¶æ„å’Œspeech recognitionæŠ€æœ¯æ¥ä»speechä¸­æå–è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚ç„¶åï¼Œäº¤å‰æ¨¡å¼å¯¹æ¥æ¨¡å—å°†éŸ³é¢‘-å›¾åƒ-æ–‡æœ¬ç‰¹å¾åœ¨æ—¶é—´å’ŒSemanticæ°´å¹³è¿›è¡Œå¯¹æ¥ã€‚æœ€åï¼ŒPMMTalkè§£ç å™¨ç”¨äºé¢„æµ‹lip-syncingçš„é¢éƒ¨æ··åˆåæ ‡ã€‚ä¸å…ˆå‰æ–¹æ³•ä¸åŒï¼ŒPMMTalkåªéœ€è¦é¢å¤–çš„éšæœºå‚è€ƒé¢éƒ¨å›¾åƒï¼Œä½†å®ƒå¯ä»¥æä¾›æ›´é«˜ç²¾åº¦çš„ç»“æœã€‚æ­¤å¤–ï¼Œå®ƒé€‚ç”¨äºæ ‡å‡†åŠ¨ç”»ç”Ÿäº§å·¥ä½œæµç¨‹ï¼Œå¯ä»¥è½»æ¾åœ° Ğ¸Ğ½Ñ‚ĞµGRATEåˆ°ç°æœ‰çš„åŠ¨ç”»ç”Ÿäº§è¿‡ç¨‹ä¸­ã€‚ finallyï¼Œç”±äº3D talking faceæ•°æ®çš„ç¼ºä¹ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„3Dä¸­æ–‡Audio-Visual Facial Animationï¼ˆ3D-CAVFAï¼‰æ•°æ®é›†ã€‚æˆ‘ä»¬çš„æ–¹æ³•åœ¨å®éªŒå’Œç”¨æˆ·ç ”ç©¶ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”è¶…è¶Šäº†å½“å‰çŠ¶æ€ã€‚æˆ‘ä»¬å»ºè®®è§‚çœ‹è¡¥å……è§†é¢‘ã€‚â€
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Inferrence-of-Structural-Similarity-of-Combinatorial-Landscapes"><a href="#Towards-the-Inferrence-of-Structural-Similarity-of-Combinatorial-Landscapes" class="headerlink" title="Towards the Inferrence of Structural Similarity of Combinatorial Landscapes"></a>Towards the Inferrence of Structural Similarity of Combinatorial Landscapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02720">http://arxiv.org/abs/2312.02720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Huang, Ke Li</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æ¢è®¨å¦‚ä½•é€šè¿‡åœ°å›¾æ•°æ®æŒ–æ˜æŠ€æœ¯æ¥æ¢ç´¢ combinatorial optimization é—®é¢˜çš„ fitness  landscape ä¸­éšè—çš„ topological ç»“æ„ä¿¡æ¯ï¼Œä»¥ä¾¿æ›´å¥½åœ°è§£å†³è¿™äº›é—®é¢˜ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨äº† local optima network ä½œä¸º fitness landscape çš„ä»£ç†ï¼Œå¹¶é€šè¿‡ graph data mining æŠ€æœ¯è¿›è¡Œè´¨é‡å’Œé‡åŒ–åˆ†æï¼Œä»¥æ¢ç´¢ä¸åŒé—®é¢˜ç±»å‹çš„ fitness landscape ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</li>
<li>results: ç»è¿‡å¤§è§„æ¨¡çš„å®éªŒç ”ç©¶ï¼Œæœ¬è®ºæ–‡å‘ç°äº†ä¸åŒé—®é¢˜ç±»å‹çš„ fitness landscape ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå¹¶ä¸”åœ¨ä¸åŒç»´åº¦ä¸Šçš„é‚»è¿‘é—®é¢˜ç±»å‹ä¹‹é—´ä¹Ÿå­˜åœ¨ä¸€å®šçš„ç»“æ„ç›¸ä¼¼æ€§ã€‚<details>
<summary>Abstract</summary>
One of the most common problem-solving heuristics is by analogy. For a given problem, a solver can be viewed as a strategic walk on its fitness landscape. Thus if a solver works for one problem instance, we expect it will also be effective for other instances whose fitness landscapes essentially share structural similarities with each other. However, due to the black-box nature of combinatorial optimization, it is far from trivial to infer such similarity in real-world scenarios. To bridge this gap, by using local optima network as a proxy of fitness landscapes, this paper proposed to leverage graph data mining techniques to conduct qualitative and quantitative analyses to explore the latent topological structural information embedded in those landscapes. By conducting large-scale empirical experiments on three classic combinatorial optimization problems, we gain concrete evidence to support the existence of structural similarity between landscapes of the same classes within neighboring dimensions. We also interrogated the relationship between landscapes of different problem classes.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:ä¸€ç§éå¸¸å¸¸è§çš„é—®é¢˜è§£å†³ç­–ç•¥æ˜¯analogyã€‚å¯¹äºä¸€ä¸ªé—®é¢˜ï¼Œä¸€ä¸ªè§£å†³æ–¹æ¡ˆå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªç­–ç•¥æ€§çš„æ­¥è¡Œåœ¨å…¶é€‚åº”åº¦åœ°å›¾ä¸Šã€‚å› æ­¤ï¼Œå¦‚æœä¸€ä¸ªè§£å†³æ–¹æ¡ˆå¯¹ä¸€ä¸ªé—®é¢˜å®ä¾‹æœ‰æ•ˆï¼Œæˆ‘ä»¬é¢„æœŸå®ƒä¹Ÿä¼šæœ‰æ•ˆäºå…¶ä»–å®ä¾‹ï¼Œåªè¦å®ƒä»¬çš„é€‚åº”åº¦åœ°å›¾å…·æœ‰ç›¸ä¼¼çš„ç»“æ„ç‰¹å¾ã€‚ç„¶è€Œï¼Œç”±äºåˆ†å¸ƒå¼ä¼˜åŒ–çš„é»‘ç›’ç‰¹æ€§ï¼Œå¾ˆéš¾ directamenteä»å®é™…æƒ…å†µä¸­æ¨æ–­å‡ºè¿™ç§ç›¸ä¼¼æ€§ã€‚ä¸ºäº†bridgingè¿™ä¸ªå·®è·ï¼Œè¿™ç¯‡è®ºæ–‡æè®®ä½¿ç”¨æœ¬åœ°æœ€ä¼˜ç‚¹ç½‘ç»œä½œä¸ºé€‚åº”åº¦åœ°å›¾çš„ä»£ç†ï¼Œç„¶åä½¿ç”¨å›¾æ•°æ®æŒ–æ˜æŠ€æœ¯æ¥è¿›è¡Œè´¨é‡å’Œé‡åŒ–åˆ†æï¼Œæ¢ç´¢é€‚åº”åº¦åœ°å›¾ä¸­åµŒå…¥çš„éšè—ç»“æ„ä¿¡æ¯ã€‚é€šè¿‡å¯¹ä¸‰ä¸ªç»å…¸çš„åˆ†å¸ƒå¼ä¼˜åŒ–é—®é¢˜è¿›è¡Œå¤§è§„æ¨¡çš„å®éªŒï¼Œæˆ‘ä»¬è·å¾—äº†å…·ä½“çš„è¯æ®ï¼Œæ”¯æŒé€‚åº”åº¦åœ°å›¾ä¸­åŒä¸€ç±»é—®é¢˜çš„ä¸åŒç»´åº¦çš„ç»“æ„ç›¸ä¼¼æ€§çš„å­˜åœ¨ã€‚æˆ‘ä»¬è¿˜è°ƒæŸ¥äº†ä¸åŒé—®é¢˜ç±»å‹çš„é€‚åº”åº¦åœ°å›¾ä¹‹é—´çš„å…³ç³»ã€‚
</details></li>
</ul>
<hr>
<h2 id="Large-Knowledge-Model-Perspectives-and-Challenges"><a href="#Large-Knowledge-Model-Perspectives-and-Challenges" class="headerlink" title="Large Knowledge Model: Perspectives and Challenges"></a>Large Knowledge Model: Perspectives and Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02706">http://arxiv.org/abs/2312.02706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Huajun Chen</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ChatGPTåœ¨çŸ¥è¯†é¢†åŸŸä¸­çš„åº”ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†çŸ¥è¯†å›¾ï¼ˆKGsï¼‰ç­‰ç¬¦å·çŸ¥è¯†æ¥å¢å¼ºLLMsï¼Œä»¥åŠä½¿ç”¨LLMæ¥æ‰©å±•ä¼ ç»Ÿçš„ç¬¦å·çŸ¥è¯†åº“ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼ŒLLMså¯ä»¥å¢å¼ºä¼ ç»Ÿçš„ç¬¦å·çŸ¥è¯†åº“ï¼Œå¹¶ä¸”å¯ä»¥ç”¨äºæ„å»ºå’Œæ§åˆ¶çŸ¥è¯†å›¾ã€‚ä½†æ˜¯ï¼Œç”±äºäººç±»çŸ¥è¯†çš„å¤æ‚æ€§ï¼Œå»ºè®®åˆ›å»ºæ›´å¤§çš„â€œå¤§çŸ¥è¯†æ¨¡å‹â€ï¼ˆLKMï¼‰æ¥ç®¡ç†å¤šç§çŸ¥è¯†ç»“æ„ã€‚<details>
<summary>Abstract</summary>
Humankind's understanding of the world is fundamentally linked to our perception and cognition, with \emph{human languages} serving as one of the major carriers of \emph{world knowledge}. In this vein, \emph{Large Language Models} (LLMs) like ChatGPT epitomize the pre-training of extensive, sequence-based world knowledge into neural networks, facilitating the processing and manipulation of this knowledge in a parametric space. This article explores large models through the lens of ``knowledge''. We initially investigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in enhancing LLMs, covering aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. Subsequently, we examine how LLMs can amplify traditional symbolic knowledge bases, encompassing aspects like using LLM as KG builder and controller, structured knowledge pretraining, LLM-enhanced symbolic reasoning, and the amalgamation of perception with cognition. Considering the intricate nature of human knowledge, we advocate for the creation of \emph{Large Knowledge Models} (LKM), specifically engineered to manage diversified spectrum of knowledge structures. This ambitious undertaking could entail several key challenges, such as disentangling knowledge representation from language models, restructuring pre-training with structured knowledge, and building large commonsense models, among others. We finally propose a five-``A'' principle to distinguish the concept of LKM.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»çš„ä¸–ç•Œç†è§£ä¸æˆ‘ä»¬çš„æ„ŸçŸ¥å’Œè®¤çŸ¥å¯†åˆ‡ç›¸å…³ï¼Œå„ç§äººç±»è¯­è¨€ serving as one of the major carriers of ä¸–ç•ŒçŸ¥è¯†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ like ChatGPT  represent the pre-training of extensive, sequence-based world knowledge into neural networks, allowing for the processing and manipulation of this knowledge in a parametric space. æœ¬æ–‡é€šè¿‡ â€œçŸ¥è¯†â€ æ¥æ¢è®¨å¤§å‹æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆ investigate the role of ç¬¦å·çŸ¥è¯† such as Knowledge Graphs (KGs) in enhancing LLMs, including aspects like knowledge-augmented language model, structure-inducing pre-training, knowledgeable prompts, structured CoT, knowledge editing, semantic tools for LLM and knowledgeable AI agents. ç„¶åï¼Œæˆ‘ä»¬ examines how LLMs can amplify traditional symbolic knowledge bases, including aspects like using LLM as KG builder and controller, structured knowledge pretraining, LLM-enhanced symbolic reasoning, and the amalgamation of perception with cognition. è€ƒè™‘åˆ°äººç±»çŸ¥è¯†çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬ advocate for the creation of å¤§å‹çŸ¥è¯†æ¨¡å‹ï¼ˆLKMï¼‰ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç®¡ç†å¤šå…ƒçš„çŸ¥è¯†ç»“æ„ã€‚è¿™é¡¹å¤§è§„æ¨¡çš„ä»»åŠ¡å¯èƒ½ä¼šæ¶‰åŠå¤šä¸ªå…³é”®æŒ‘æˆ˜ï¼Œå¦‚è§£è„±çŸ¥è¯†è¡¨ç¤ºä¸è¯­è¨€æ¨¡å‹ï¼Œé‡æ–°ç»“æ„é¢„è®­ç»ƒä¸ç»“æ„çŸ¥è¯†ï¼Œä»¥åŠå»ºç«‹å¤§è§„æ¨¡çš„é€šç”¨å¸¸è¯†æ¨¡å‹ï¼Œç­‰ç­‰ã€‚ æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†äº”ä¸ª â€œAâ€ åŸåˆ™æ¥ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°å‡º LKM çš„æ¦‚å¿µã€‚
</details></li>
</ul>
<hr>
<h2 id="Unified-learning-based-lossy-and-lossless-JPEG-recompression"><a href="#Unified-learning-based-lossy-and-lossless-JPEG-recompression" class="headerlink" title="Unified learning-based lossy and lossless JPEG recompression"></a>Unified learning-based lossy and lossless JPEG recompression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02705">http://arxiv.org/abs/2312.02705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghui Zhang, Yuanyuan Wang, Lina Guo, Jixiang Luo, Tongda Xu, Yan Wang, Zhi Wang, Hongwei Qin</li>
<li>for: æé«˜ JPEG å›¾åƒå‹ç¼©ç‡ï¼Œå¹¶ bridge lossy å’Œ lossless å‹ç¼©ä¹‹é—´çš„ gap</li>
<li>methods: ä½¿ç”¨å­¦ä¹ çš„é‡åŒ–è¡¨å’Œ Markovian å±‚æ¬¡å˜åˆ†è‡ªåŠ¨æœº</li>
<li>results: å¯ä»¥å®ç° arbitrarily ä½çš„æŸå®³ï¼Œå½“ bitrate æ¥è¿‘æœ€é«˜ bound æ—¶Hereâ€™s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the compression efficiency of JPEG images and bridge the gap between lossy and lossless compression methods.</li>
<li>methods: The proposed method uses a learned quantization table and a Markovian hierarchical variational autoencoder to achieve lossy and lossless JPEG recompression.</li>
<li>results: The proposed method can achieve arbitrarily low distortion when the bitrate is close to the upper bound, which is the bitrate of the lossless compression model. This is the first learned method that bridges the gap between lossy and lossless recompression of JPEG images, to the best of the authorsâ€™ knowledge.<details>
<summary>Abstract</summary>
JPEG is still the most widely used image compression algorithm. Most image compression algorithms only consider uncompressed original image, while ignoring a large number of already existing JPEG images. Recently, JPEG recompression approaches have been proposed to further reduce the size of JPEG files. However, those methods only consider JPEG lossless recompression, which is just a special case of the rate-distortion theorem. In this paper, we propose a unified lossly and lossless JPEG recompression framework, which consists of learned quantization table and Markovian hierarchical variational autoencoders. Experiments show that our method can achieve arbitrarily low distortion when the bitrate is close to the upper bound, namely the bitrate of the lossless compression model. To the best of our knowledge, this is the first learned method that bridges the gap between lossy and lossless recompression of JPEG images.
</details>
<details>
<summary>æ‘˜è¦</summary>
JPEGä»æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„å›¾åƒå‹ç¼©ç®—æ³•ã€‚å¤§å¤šæ•°å›¾åƒå‹ç¼©ç®—æ³•åªè€ƒè™‘æ— å‹ç¼©åŸå§‹å›¾åƒï¼Œè€Œå¿½ç•¥äº†å¤§é‡å·²ç»å­˜åœ¨çš„JPEGå›¾åƒã€‚è¿‘æœŸï¼ŒJPEGé‡å‹ç¼©æ–¹æ³•å¾—åˆ°äº†æè®®ï¼Œä½†è¿™äº›æ–¹æ³•åªè€ƒè™‘äº†JPEGæ— æŸé‡å‹ç¼©ï¼Œè¿™åªæ˜¯æƒé‡-è¿å’Œå®šç†çš„ç‰¹æ®Šæƒ…å†µã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æŸå¤±é‡å’ŒæŸå¤±æ— æŸJPEGé‡å‹ç¼©æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åŒ…æ‹¬å­¦ä¹ çš„é‡åŒ–è¡¨å’ŒMarkové“¾å¼å±‚VARAEã€‚å®éªŒæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨bitrateæ¥è¿‘Upper boundçš„æƒ…å†µä¸‹å®ç°arbitraryä½çš„æŸå¤±ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™æ˜¯æˆ‘ä»¬æ‰€çŸ¥é“çš„ç¬¬ä¸€ç§å­¦ä¹ æ–¹æ³•ï¼Œå¯ä»¥bridgingæŸå¤±å’Œæ— æŸé‡å‹ç¼©JPEGå›¾åƒä¹‹é—´çš„å·®è·ã€‚
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Vehicle-Entrance-and-Parking-Management-Deep-Learning-Solutions-for-Efficiency-and-Security"><a href="#Enhancing-Vehicle-Entrance-and-Parking-Management-Deep-Learning-Solutions-for-Efficiency-and-Security" class="headerlink" title="Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security"></a>Enhancing Vehicle Entrance and Parking Management: Deep Learning Solutions for Efficiency and Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02699">http://arxiv.org/abs/2312.02699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Umer Ramzan, Usman Ali, Syed Haider Abbas Naqvi, Zeeshan Aslam, Tehseen, Husnain Ali, Muhammad Faheem</li>
<li>for: è§£å†³ç»„ç»‡è‡ªåŠ¨åŒ–è¿›å£å’Œåœè½¦ç®¡ç†é—®é¢˜ï¼Œæé«˜æ•ˆç‡ã€å®‰å…¨æ€§å’Œè®°å½•ä¿æŒã€‚</li>
<li>methods: åˆ©ç”¨ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹è‡ªåŠ¨åŒ–è½¦è¾†è¿›å£å’Œåœè½¦è¿‡ç¨‹ï¼Œå¹¶ integrate è½¦è¾†æ£€æµ‹ã€è½¦ç‰Œå·æ£€æµ‹ã€äººè„¸æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹ï¼Œä»¥ç¡®ä¿è½¦è¾†å’Œäººå‘˜çš„æ³¨å†Œã€‚</li>
<li>results: ç³»ç»Ÿå¯ä»¥å¿«é€Ÿã€å‡†ç¡®åœ°æ£€æµ‹è½¦è¾†è¿›å£å’Œåœè½¦ï¼Œæä¾›é«˜æ•ˆçš„è®°å½•ä¿æŒå’Œæ´—ç¤¼è½¦ä½åˆ†é…ï¼Œæé«˜äº†ä¾¿æ·ã€å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚<details>
<summary>Abstract</summary>
The auto-management of vehicle entrance and parking in any organization is a complex challenge encompassing record-keeping, efficiency, and security concerns. Manual methods for tracking vehicles and finding parking spaces are slow and a waste of time. To solve the problem of auto management of vehicle entrance and parking, we have utilized state-of-the-art deep learning models and automated the process of vehicle entrance and parking into any organization. To ensure security, our system integrated vehicle detection, license number plate verification, and face detection and recognition models to ensure that the person and vehicle are registered with the organization. We have trained multiple deep-learning models for vehicle detection, license number plate detection, face detection, and recognition, however, the YOLOv8n model outperformed all the other models. Furthermore, License plate recognition is facilitated by Google's Tesseract-OCR Engine. By integrating these technologies, the system offers efficient vehicle detection, precise identification, streamlined record keeping, and optimized parking slot allocation in buildings, thereby enhancing convenience, accuracy, and security. Future research opportunities lie in fine-tuning system performance for a wide range of real-world applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
è‡ªåŠ¨ç®¡ç†æœºæ„å†…éƒ¨æ±½è½¦è¿›å£å’Œåœè½¦æ˜¯ä¸€ä¸ªå¤æ‚çš„æŒ‘æˆ˜ï¼Œæ¶‰åŠåˆ°è®°å½•ä¿æŒã€æ•ˆç‡å’Œå®‰å…¨é—®é¢˜ã€‚ä¼ ç»Ÿçš„æ‰‹åŠ¨æ–¹æ³• Ğ´Ğ»Ñè·Ÿè¸ªæ±½è½¦å’Œå¯»æ‰¾åœè½¦ä½ç½®æ˜¯æ…¢å¹¶ä¸”æ˜¯æµªè´¹æ—¶é—´çš„ã€‚ä¸ºè§£å†³æœºæ„å†…éƒ¨æ±½è½¦è¿›å£å’Œåœè½¦çš„è‡ªåŠ¨ç®¡ç†é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å½“ä»Šæœ€å…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè‡ªåŠ¨åŒ–äº†æ±½è½¦è¿›å£å’Œåœè½¦çš„è¿‡ç¨‹ã€‚ä¸ºä¿éšœå®‰å…¨ï¼Œæˆ‘ä»¬çš„ç³»ç»Ÿ integrateäº†è½¦è¾†æ£€æµ‹ã€è½¦ç‰Œå·æ£€æµ‹ã€äººè„¸æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹ï¼Œä»¥ç¡®ä¿æ±½è½¦å’Œäººå‘˜æ˜¯ç»„ç»‡æ³¨å†Œçš„ã€‚æˆ‘ä»¬å·²ç»è®­ç»ƒäº†å¤šä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬è½¦è¾†æ£€æµ‹ã€è½¦ç‰Œå·æ£€æµ‹ã€äººè„¸æ£€æµ‹å’Œè¯†åˆ«æ¨¡å‹ï¼Œä½†æ˜¯YOLOv8næ¨¡å‹åœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ã€‚æ­¤å¤–ï¼Œè½¦ç‰Œå·æ£€æµ‹å¾—åˆ°Googleçš„Tesseract-OCRå¼•æ“æ”¯æŒã€‚é€šè¿‡å°†è¿™äº›æŠ€æœ¯é›†æˆï¼Œç³»ç»Ÿæä¾›äº†é«˜æ•ˆçš„è½¦è¾†æ£€æµ‹ã€å‡†ç¡®çš„è¯†åˆ«ã€æ•´æ´çš„è®°å½•ä¿æŒå’Œåœè½¦ä½ç½®åˆ†é…ä¼˜åŒ–ï¼Œä»è€Œæé«˜äº†ä¾¿æ·ã€å‡†ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚æœªæ¥çš„ç ”ç©¶æœºé‡åœ¨ç»†åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Œé€‚åº”å¹¿æ³›çš„å®é™…åº”ç”¨åœºæ™¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="Analyzing-and-Improving-the-Training-Dynamics-of-Diffusion-Models"><a href="#Analyzing-and-Improving-the-Training-Dynamics-of-Diffusion-Models" class="headerlink" title="Analyzing and Improving the Training Dynamics of Diffusion Models"></a>Analyzing and Improving the Training Dynamics of Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02696">http://arxiv.org/abs/2312.02696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mmathew23/improved_edm">https://github.com/mmathew23/improved_edm</a></li>
<li>paper_authors: Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, Samuli Laine</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯æ”¹è¿›æ•°æ®é©±åŠ¨å›¾åƒç”Ÿæˆé¢†åŸŸä¸­æµè¡Œçš„ADMæ‰©æ•£æ¨¡å‹æ¶æ„ï¼Œæé«˜å›¾åƒç”Ÿæˆè´¨é‡ã€‚</li>
<li>methods: ä½œè€…é€šè¿‡ä¿®æ”¹ç½‘ç»œå±‚æ¥ä¿æŒæ´»åŒ–é‡ã€æƒé‡é‡å’Œæ›´æ–°é‡çš„å¹³è¡¡ï¼Œè§£å†³äº†è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸å‡åŒ€å’Œä¸æœ‰æ•ˆæ€§é—®é¢˜ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†ä¸€ç§åœ¨è®­ç»ƒå®Œæˆåè®¾ç½®å„ä¸ªEMAå‚æ•°çš„æ–¹æ³•ï¼Œä»¥ä¾¿ç²¾ç»†åœ°è°ƒæ•´EMAé•¿åº¦ã€‚</li>
<li>results: ä½œè€…é€šè¿‡ä¿®æ”¹ç½‘ç»œæ¶æ„å’ŒEMAå‚æ•°ï¼Œæé«˜äº†å›¾åƒç”Ÿæˆçš„è´¨é‡ï¼Œå¹¶ achieved 1.81çš„FIDè®°å½•ï¼Œèƒœè¿‡äº†ä¹‹å‰çš„2.41è®°å½•ã€‚<details>
<summary>Abstract</summary>
Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling.   As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.
</details>
<details>
<summary>æ‘˜è¦</summary>
Currently, diffusion models dominate the field of data-driven image synthesis due to their ability to scale to large datasets. In this paper, we identify and address several issues with the popular ADM diffusion model architecture that were causing uneven and ineffective training. These issues included uncontrolled magnitude changes and imbalances in both the network activations and weights during training. To address these issues, we redesigned the network layers to preserve activation, weight, and update magnitudes on expectation. As a result, we were able to eliminate the observed drifts and imbalances and achieve considerably better performance at equal computational complexity. Our modifications improved the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling. Additionally, we present a method for setting the exponential moving average (EMA) parameters post-hoc, which allows for precise tuning of EMA length without the cost of performing multiple training runs and reveals surprising interactions with network architecture, training time, and guidance.
</details></li>
</ul>
<hr>
<h2 id="H-GAP-Humanoid-Control-with-a-Generalist-Planner"><a href="#H-GAP-Humanoid-Control-with-a-Generalist-Planner" class="headerlink" title="H-GAP: Humanoid Control with a Generalist Planner"></a>H-GAP: Humanoid Control with a Generalist Planner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02682">http://arxiv.org/abs/2312.02682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyao Jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim RocktÃ¤schel, Yuandong Tian</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ç§åŸºäºäººç±»åŠ¨ä½œæ•°æ®é‡‡é›†çš„humanoidæ§åˆ¶æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨äººç±»ä¸­å¿ƒåŸºç¡€è®¾æ–½ä¸­é›†æˆå’Œå®ç°ç‰©ç†é©±åŠ¨çš„humanoidåŠ¨ç”»ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†humanoid trajectoryæ•°æ®é›†ï¼Œå¦‚MoCapActï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºçŠ¶æ€-åŠ¨ä½œ trajectoryç”Ÿæˆæ¨¡å‹ï¼ˆH-GAPï¼‰ï¼Œå¯ä»¥åœ¨Model Predictive Controlï¼ˆMPCï¼‰ä¸‹å¤„ç†é«˜ç»´çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„ä¼˜åŒ–é—®é¢˜ã€‚</li>
<li>results: è¯¥ç ”ç©¶è¡¨æ˜ï¼ŒH-GAPå¯ä»¥å­¦ä¹ å’Œç”Ÿæˆå„ç§motor behaviorsï¼Œå¹¶åœ¨ä¸åŒçš„ä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡ä¸­è¿›è¡Œé€‚åº”æ€§è§„åˆ’ã€‚æ­¤å¤–ï¼ŒH-GAPå¯ä»¥åœ¨ä¸åŒçš„ä»»åŠ¡ä¸­å…·æœ‰ä¼˜äºæˆ–ç›¸å½“äºåœ¨çº¿å­¦ä¹ å’ŒRLæ–¹æ³•çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. However, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC). For 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviours. Further, without any learning from online interactions, it can also flexibly transfer these behaviors to solve novel downstream control tasks via planning. Notably, H-GAP excels established MPC baselines that have access to the ground truth dynamics model, and is superior or comparable to offline RL methods trained for individual tasks. Finally, we do a series of empirical studies on the scaling properties of H-GAP, showing the potential for performance gains via additional data but not computing. Code and videos are available at https://ycxuyingchen.github.io/hgap/.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå‹æ§åˆ¶æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æŒ‘æˆ˜ï¼Œå®ƒæä¾›äº†ä¸äººç±»ä¸­å¿ƒçš„åŸºç¡€è®¾æ–½é›†æˆå’Œphysicsé©±åŠ¨çš„äººç±»åŠ¨ä½œæ¸²æŸ“çš„å¯èƒ½æ€§ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé¢†åŸŸçš„æŒ‘æˆ˜åœ¨äºä¼˜åŒ–é«˜ç»´åŠ¨ä½œç©ºé—´çš„é—®é¢˜å’Œäººå‹å½¢æ€å¼•å…¥çš„ä¸ç¨³å®šæ€§ã€‚ç„¶è€Œï¼Œäººç±»åŠ¨ä½œæ•æ‰æ•°æ®çš„å¤§é‡æ”¶é›†å’Œ derivatedçš„äººç±»è½¨è¿¹æ•°æ®ï¼Œå¦‚MoCapActï¼Œä¸ºè¿™äº›æŒ‘æˆ˜æä¾›äº†æ–¹å‘ã€‚åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†äººç±»é€šç”¨è‡ªç¼–ç è®¡åˆ’ï¼ˆH-GAPï¼‰ï¼Œä¸€ç§åŸºäºäººç±»è½¨è¿¹æ•°æ®è¿›è¡Œè®­ç»ƒçš„çŠ¶æ€-åŠ¨ä½œè½¨è¿¹ç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿå¦¥åæ§åˆ¶ä¸‹æ¸¸ä»»åŠ¡ã€‚å¯¹äº56åº¦è‡ªç”±åº¦çš„äººç±»ï¼Œæˆ‘ä»¬ç»éªŒè¡¨æ˜ï¼ŒH-GAPèƒ½å¤Ÿlearn representå’Œç”Ÿæˆå¹¿æ³›çš„åŠ¨ä½œè¡Œä¸ºã€‚æ­¤å¤–ï¼Œä¸éœ€è¦åœ¨çº¿äº¤äº’å­¦ä¹ ï¼Œå®ƒè¿˜å¯ä»¥é€šè¿‡è§„åˆ’æ¥é€‚åº”æ–°çš„ä¸‹æ¸¸æ§åˆ¶ä»»åŠ¡ã€‚åœ¨æ¯”è¾ƒMPCåŸºçº¿å’Œç¦»çº¿RLæ–¹æ³•çš„æƒ…å†µä¸‹ï¼ŒH-GAPè¡¨ç°å‡ºè‰²ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ç³»åˆ—å®éªŒç ”ç©¶å…³äºH-GAPçš„æ‰©å±•æ€§ï¼Œæ˜¾ç¤ºäº†å¯èƒ½é€šè¿‡æ›´å¤šçš„æ•°æ®è·å¾—æ€§èƒ½æå‡ï¼Œä½†ä¸éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚ä»£ç å’Œè§†é¢‘å¯ä»¥åœ¨https://ycxuyingchen.github.io/hgap/ä¸Šä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Contact-Energy-Based-Hindsight-Experience-Prioritization"><a href="#Contact-Energy-Based-Hindsight-Experience-Prioritization" class="headerlink" title="Contact Energy Based Hindsight Experience Prioritization"></a>Contact Energy Based Hindsight Experience Prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02677">http://arxiv.org/abs/2312.02677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erdi Sayar, Zhenshan Bing, Carlo Dâ€™Eramo, Ozgur S. Oguz, Alois Knoll</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦ç›®çš„æ˜¯è§£å†³å¤šç›®æ ‡æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜ï¼Œå³ä½¿å¥–åŠ±ç‡ç¨€ç–ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºContact Energy Based Prioritizationï¼ˆCEBPï¼‰çš„æ–°æ–¹æ³•ï¼Œå®ƒé€‰æ‹©ä»å‚¨å­˜ç¼“å­˜ä¸­æŠ½å–æ ·æœ¬ï¼ŒåŸºäºæœºå™¨äººå’Œç‰©ä½“ç§»åŠ¨çš„è§¦æ„Ÿä¿¡æ¯ã€‚è¯¥æ–¹æ³•å¸Œæœ›é€šè¿‡å¼ºè°ƒè§¦æ„Ÿå¯Œæœ‰çš„ç»éªŒæ¥ä¼˜åŒ–å­¦ä¹ ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜åœ¨ä¸åŒçš„ç¨€ç–å¥–åŠ±æœºå™¨äººæ“ä½œä»»åŠ¡ä¸Šè¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå¹¶ä¸ç°æœ‰çš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼ŒCEBPæ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¼˜äºæˆ–ä¸ç°æœ‰æ–¹æ³•ç›¸å½“ã€‚æœ€åï¼Œç ”ç©¶äººå‘˜åœ¨ä¸€ä¸ªçœŸå®çš„Frankaæœºå™¨äººä¸Šéƒ¨ç½²äº†å®ƒä»¬çš„è®­ç»ƒæ”¿ç­–ï¼Œå¹¶è§‚å¯Ÿåˆ°æœºå™¨äººæˆåŠŸå®Œæˆäº†ä¸€ä¸ªæ‹¾å–å¹¶ç½®æ”¾ä»»åŠ¡ã€‚è§†é¢‘å’Œä»£ç å¯ä»¥åœ¨ï¼š<a target="_blank" rel="noopener" href="https://erdiphd.github.io/HER_force">https://erdiphd.github.io/HER_force</a> ä¸­è·å–ã€‚<details>
<summary>Abstract</summary>
Multi-goal robot manipulation tasks with sparse rewards are difficult for reinforcement learning (RL) algorithms due to the inefficiency in collecting successful experiences. Recent algorithms such as Hindsight Experience Replay (HER) expedite learning by taking advantage of failed trajectories and replacing the desired goal with one of the achieved states so that any failed trajectory can be utilized as a contribution to learning. However, HER uniformly chooses failed trajectories, without taking into account which ones might be the most valuable for learning. In this paper, we address this problem and propose a novel approach Contact Energy Based Prioritization~(CEBP) to select the samples from the replay buffer based on rich information due to contact, leveraging the touch sensors in the gripper of the robot and object displacement. Our prioritization scheme favors sampling of contact-rich experiences, which are arguably the ones providing the largest amount of information. We evaluate our proposed approach on various sparse reward robotic tasks and compare them with the state-of-the-art methods. We show that our method surpasses or performs on par with those methods on robot manipulation tasks. Finally, we deploy the trained policy from our method to a real Franka robot for a pick-and-place task. We observe that the robot can solve the task successfully. The videos and code are publicly available at: https://erdiphd.github.io/HER_force
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šç›®æ ‡æœºå™¨äººæ“ä½œä»»åŠ¡ WITH sparse reward éš¾ä»¥ä½¿ç”¨ reinforcement learningï¼ˆRLï¼‰ç®—æ³•ï¼Œå› ä¸ºæ”¶é›†æˆåŠŸç»éªŒä¸fficientã€‚ recent algorithms such as Hindsight Experience Replay (HER) ä½¿ç”¨äº†å¤±è´¥çš„è½¨è¿¹ï¼Œå¹¶å°†ç›®æ ‡æ›´æ”¹ä¸ºè¾¾åˆ°çš„çŠ¶æ€ï¼Œä»¥ä¾¿ä»»ä½•å¤±è´¥çš„è½¨è¿¹éƒ½å¯ä»¥ä½œä¸ºå­¦ä¹ çš„è´¡çŒ®ã€‚ ç„¶è€Œï¼ŒHER uniformmenteé€‰æ‹©å¤±è´¥çš„è½¨è¿¹ï¼Œä¸è€ƒè™‘å“ªäº›å¯èƒ½æ˜¯å­¦ä¹ ä¸­æœ€æœ‰ä»·å€¼çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼šContact Energy Based Prioritization~(CEBP)ã€‚æˆ‘ä»¬çš„ä¼˜åŒ–æ–¹æ¡ˆåŸºäºè§¦æ‘¸æ„Ÿæµ‹å™¨å’Œç‰©ä½“ç§»åŠ¨ï¼Œå¯ä»¥é€‰æ‹©æ¥è§¦richçš„ç»éªŒï¼Œå¹¶ä¸”åå¥½è¿™äº›ç»éªŒã€‚æˆ‘ä»¬çš„ä¼˜å…ˆçº§é¡ºåºå¯¹äºå­¦ä¹ æä¾›äº†ä¸°å¯Œçš„ä¿¡æ¯ã€‚æˆ‘ä»¬å¯¹å¤šä¸ªç¨€ç¼ºå¥–åŠ±æœºå™¨äººä»»åŠ¡è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶ä¸å½“å‰çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æœºå™¨äººæ“ä½œä»»åŠ¡ä¸­èƒœè¿‡æˆ–ä¸å½“å‰æ–¹æ³•ç›¸å½“ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•è®­ç»ƒçš„ç­–ç•¥ï¼Œå¹¶åœ¨çœŸå®çš„ Franka æœºå™¨äººä¸Šå®Œæˆäº†ä¸€ä¸ª pick-and-place ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°ï¼Œæœºå™¨äººå¯ä»¥æˆåŠŸå®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚è§†é¢‘å’Œä»£ç å¯ä»¥åœ¨ï¼šhttps://erdiphd.github.io/HER_force ä¸Šè·å–ã€‚
</details></li>
</ul>
<hr>
<h2 id="Amortized-Bayesian-Decision-Making-for-simulation-based-models"><a href="#Amortized-Bayesian-Decision-Making-for-simulation-based-models" class="headerlink" title="Amortized Bayesian Decision Making for simulation-based models"></a>Amortized Bayesian Decision Making for simulation-based models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02674">http://arxiv.org/abs/2312.02674</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mackelab/amortized-decision-making">https://github.com/mackelab/amortized-decision-making</a></li>
<li>paper_authors: Mila Gorecki, Jakob H. Macke, Michael Deistler</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨å¦‚ä½•ä½¿ç”¨ simulation-based inference (SBI) è¿›è¡Œ Bayesian å†³ç­–ï¼Œå¹¶å¦‚ä½•é¿å…è®¡ç®— Explicit  aproximation çš„ posterior distributionã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨ neural network è¿›è¡Œæ¨¡æ‹Ÿæ•°æ®çš„è®­ç»ƒï¼Œå¹¶å¯ä»¥ç”¨æ¥é¢„æµ‹ç»™å®šæ•°æ®å’Œè¡ŒåŠ¨çš„æœŸæœ›æˆæœ¬ã€‚</li>
<li>results: è¯¥è®ºæ–‡åœ¨å¤šä¸ª benchmark é—®é¢˜ä¸­åº”ç”¨äº†è¯¥æ–¹æ³•ï¼Œå¹¶è¯æ˜äº†å®ƒå¯ä»¥induces ç±»ä¼¼äº true posterior distribution ä¸­çš„æˆæœ¬ã€‚æ­¤å¤–ï¼Œè¯¥è®ºæ–‡è¿˜åº”ç”¨äº†è¯¥æ–¹æ³•äºä¸€ä¸ªå®é™…ä¸–ç•Œçš„ simulator ä¸­ï¼Œå³ Bayesian Virtual Epileptic Patientï¼Œå¹¶è¯æ˜äº†å®ƒå¯ä»¥åœ¨å‡ ä¸ª simulations ä¸­æ¨æ–­å‡ºä½æˆæœ¬çš„è¡ŒåŠ¨ã€‚<details>
<summary>Abstract</summary>
Simulation-based inference (SBI) provides a powerful framework for inferring posterior distributions of stochastic simulators in a wide range of domains. In many settings, however, the posterior distribution is not the end goal itself -- rather, the derived parameter values and their uncertainties are used as a basis for deciding what actions to take. Unfortunately, because posterior distributions provided by SBI are (potentially crude) approximations of the true posterior, the resulting decisions can be suboptimal. Here, we address the question of how to perform Bayesian decision making on stochastic simulators, and how one can circumvent the need to compute an explicit approximation to the posterior. Our method trains a neural network on simulated data and can predict the expected cost given any data and action, and can, thus, be directly used to infer the action with lowest cost. We apply our method to several benchmark problems and demonstrate that it induces similar cost as the true posterior distribution. We then apply the method to infer optimal actions in a real-world simulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient, and demonstrate that it allows to infer actions associated with low cost after few simulations.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ¨¡æ‹ŸåŸºäºæ¨ç†ï¼ˆSBIï¼‰æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¨ç†æ¡†æ¶ï¼Œå¯ä»¥åœ¨å„ç§é¢†åŸŸä¸­ä¸ºä¸ç¡®å®šçš„æ¨¡æ‹Ÿå™¨ posterior distribution è¿›è¡Œæ¨ç†ã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œ posterior distribution æœ¬èº«å¹¶ä¸æ˜¯æœ€ç»ˆç›®æ ‡ -- è€Œæ˜¯åŸºäºè¿™äº›å‚æ•°å€¼å’Œå…¶ä¸ç¡®å®šæ€§æ¥åšå‡ºå†³ç­–ã€‚ç„¶è€Œï¼Œç”±äº SBI ä¸­çš„ posterior distribution æ˜¯ï¼ˆå¯èƒ½ç²—ç³™ï¼‰çš„ä¼°è®¡ï¼Œå› æ­¤å¾—å‡ºçš„å†³ç­–å¯èƒ½ä¼šä¸ä¼˜åŒ–ã€‚æœ¬æ–‡è€ƒè™‘äº†å¦‚ä½•åœ¨ä¸ç¡®å®šçš„æ¨¡æ‹Ÿå™¨ä¸Šè¿›è¡Œ bayesian å†³ç­–ï¼Œå¹¶å¦‚ä½•é¿å…è®¡ç®—æ˜¾å¼çš„ posterior ä¼°è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œä½¿å…¶å¯ä»¥åœ¨ä»»ä½•æ•°æ®å’Œè¡ŒåŠ¨ä¸‹é¢„æµ‹è¡ŒåŠ¨çš„é¢„æœŸæˆæœ¬ï¼Œä»è€Œç›´æ¥ç”¨äºæ¨ç†æœ€ä½æˆæœ¬çš„è¡ŒåŠ¨ã€‚æˆ‘ä»¬åœ¨ä¸€äº›æ ‡å‡†é—®é¢˜ä¸Šåº”ç”¨äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶è¯æ˜å®ƒä»¬ä¸çœŸæ­£çš„ posterior distribution ç›¸ä¼¼ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æ–¹æ³•åº”ç”¨äºåŒ»å­¦ç¥ç»ç§‘å­¦çš„ Bayesian Virtual Epileptic Patient æ¨¡æ‹Ÿå™¨ï¼Œå¹¶è¯æ˜å®ƒå¯ä»¥åœ¨å‡ æ¬¡ simulations åå†³ç­–å‡ºä½æˆæœ¬çš„è¡ŒåŠ¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Lights-out-training-RL-agents-robust-to-temporary-blindness"><a href="#Lights-out-training-RL-agents-robust-to-temporary-blindness" class="headerlink" title="Lights out: training RL agents robust to temporary blindness"></a>Lights out: training RL agents robust to temporary blindness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02665">http://arxiv.org/abs/2312.02665</a></li>
<li>repo_url: None</li>
<li>paper_authors: N. Ordonez, M. Tromp, P. M. Julbe, W. BÃ¶hmer</li>
<li>for: å¢å¼º Deep Q-Network (DQN)  Agent çš„Robustness to çŸ­æš‚å¤±æ˜ï¼ˆtemporary blindnessï¼‰</li>
<li>methods: ä½¿ç”¨éšè—è¡¨ç¤º Observation çš„ neural network æ¶æ„å’Œ noval n-step æŸå¤±å‡½æ•°</li>
<li>results: å¯ä»¥æ‰¿å—æ›´é•¿çš„ç›²ç›®æœŸï¼ˆblindness stretchï¼‰ï¼Œç¤ºå¼ºæ€§æé«˜ã€‚In English:</li>
<li>for: Enhancing Deep Q-Network (DQN) Agentâ€™s Robustness to Temporary Blindness</li>
<li>methods: Using a neural network architecture with hidden representations of observations and a novel n-step loss function</li>
<li>results: Can withstand longer blindness periods, demonstrating improved robustness.<details>
<summary>Abstract</summary>
Agents trained with DQN rely on an observation at each timestep to decide what action to take next. However, in real world applications observations can change or be missing entirely. Examples of this could be a light bulb breaking down, or the wallpaper in a certain room changing. While these situations change the actual observation, the underlying optimal policy does not change. Because of this we want our agent to continue taking actions until it receives a (recognized) observation again. To achieve this we introduce a combination of a neural network architecture that uses hidden representations of the observations and a novel n-step loss function. Our implementation is able to withstand location based blindness stretches longer than the ones it was trained on, and therefore shows robustness to temporary blindness. For access to our implementation, please email Nathan, Marije, or Pau.
</details>
<details>
<summary>æ‘˜è¦</summary>
agenté©±åŠ¨ä½¿ç”¨DQNåŸ¹è‚²çš„agentä¼šæ ¹æ®æ¯ä¸ªæ—¶é—´æ­¥éª¤çš„è§‚å¯Ÿæ¥å†³å®šä¸‹ä¸€æ­¥çš„è¡Œä¸ºã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè§‚å¯Ÿå¯èƒ½ä¼šå˜åŒ–æˆ– completly missingã€‚ä¾‹å¦‚ï¼Œç¯æ³¡ç ´è£‚æˆ–å¢™çº¸åœ¨æŸä¸ªæˆ¿é—´æ”¹å˜ã€‚è¿™äº›æƒ…å†µä¼šæ”¹å˜å®é™…è§‚å¯Ÿï¼Œä½†æ˜¯ä¸‹é¢çš„ä¼˜åŒ–ç­–ç•¥ä¸ä¼šæ”¹å˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æƒ³æˆ‘ä»¬çš„agentå¯ä»¥ç»§ç»­æ‰§è¡Œè¡ŒåŠ¨ï¼Œç›´åˆ°å®ƒæ”¶åˆ°ä¸€ä¸ªè®¤å¯çš„è§‚å¯Ÿã€‚ä¸º Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œä½¿ç”¨éšè—è¡¨ç¤ºçš„è§‚å¯Ÿï¼Œä»¥åŠä¸€ç§æ–°çš„næ­¥æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬çš„å®ç°èƒ½å¤Ÿæ‰¿å—ä½ç½®åŸºäºçš„ç›²ç›®æ‰©å±•ï¼Œæ¯”è®­ç»ƒä¸­çš„ç›²ç›®æ‰©å±•æ›´é•¿ï¼Œå› æ­¤è¡¨ç°å‡ºäº†å¯¹çŸ­æš‚ç›²ç›®çš„æŠ—è¡¡èƒ½åŠ›ã€‚å¦‚æœæ‚¨æƒ³è·å–æˆ‘ä»¬çš„å®ç°ï¼Œè¯·é‚®ä»¶ Nathanã€Marije æˆ– Pauã€‚
</details></li>
</ul>
<hr>
<h2 id="FaceStudio-Put-Your-Face-Everywhere-in-Seconds"><a href="#FaceStudio-Put-Your-Face-Everywhere-in-Seconds" class="headerlink" title="FaceStudio: Put Your Face Everywhere in Seconds"></a>FaceStudio: Put Your Face Everywhere in Seconds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02663">http://arxiv.org/abs/2312.02663</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, Bin Fu<br>for: è¿™é¡¹ç ”ç©¶æ¢ç´¢äº†ä¸€ç§èƒ½å¤Ÿä¿æŒäººç‰©èº«ä»½çš„å›¾åƒç”ŸæˆæŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€é¡¹æ¿€å‘äººä»¬ curiosities çš„å›¾åƒç”Ÿæˆä»»åŠ¡ã€‚methods: è¿™ç§æŠ€æœ¯ä½¿ç”¨äº†ä¸€ç§ç›´é€šçš„å‰å‘é©±åŠ¨æœºåˆ¶ï¼Œä¸éœ€è¦è€—æ—¶ fine-tuningï¼Œä»è€Œå®ç°äº†å¿«é€Ÿå’Œé«˜æ•ˆçš„å›¾åƒç”Ÿæˆã€‚è¯¥æ¨¡å‹è¿˜ä½¿ç”¨äº†ä¸€ç§æ··åˆå¼•å¯¼æ¡†æ¶ï¼Œå°†æ ·å¼åŒ–å›¾åƒã€äººè„¸å›¾åƒå’Œæ–‡æœ¬æç¤ºç›¸ç»“åˆï¼Œä»¥å¯¼å¼•å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚results: æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”è¾ƒä¸åŸºçº¿æ¨¡å‹å’Œå…ˆå‰çš„å·¥ä½œè¿›è¡Œè¯„ä¼°æ—¶ï¼Œå…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ•ˆå’Œä¿æŒäººç‰©èº«ä»½æ–¹é¢ã€‚<details>
<summary>Abstract</summary>
This study investigates identity-preserving image synthesis, an intriguing task in image generation that seeks to maintain a subject's identity while adding a personalized, stylistic touch. Traditional methods, such as Textual Inversion and DreamBooth, have made strides in custom image creation, but they come with significant drawbacks. These include the need for extensive resources and time for fine-tuning, as well as the requirement for multiple reference images. To overcome these challenges, our research introduces a novel approach to identity-preserving synthesis, with a particular focus on human images. Our model leverages a direct feed-forward mechanism, circumventing the need for intensive fine-tuning, thereby facilitating quick and efficient image generation. Central to our innovation is a hybrid guidance framework, which combines stylized images, facial images, and textual prompts to guide the image generation process. This unique combination enables our model to produce a variety of applications, such as artistic portraits and identity-blended images. Our experimental results, including both qualitative and quantitative evaluations, demonstrate the superiority of our method over existing baseline models and previous works, particularly in its remarkable efficiency and ability to preserve the subject's identity with high fidelity.
</details>
<details>
<summary>æ‘˜è¦</summary>
Here's the text in Simplified Chinese:è¿™é¡¹ç ”ç©¶ investigate identity-preserving image synthesisï¼Œè¿™æ˜¯ä¸€é¡¹å›¾åƒç”Ÿæˆä»»åŠ¡ï¼Œæ—¨åœ¨ä¿æŒä¸»ä½“çš„èº«ä»½while adding a personalized, stylistic touchã€‚ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚Textual Inversionå’ŒDreamBoothï¼Œåœ¨è‡ªå®šä¹‰å›¾åƒåˆ›å»ºæ–¹é¢åšå‡ºäº†è¿›å±•ï¼Œä½†å®ƒä»¬å¸¦æœ‰ä¸€äº›ç¼ºç‚¹ã€‚è¿™äº›ç¼ºç‚¹åŒ…æ‹¬éœ€è¦å¤§é‡èµ„æºå’Œæ—¶é—´è¿›è¡Œç²¾ç»†è°ƒæ•´ï¼Œä»¥åŠéœ€è¦å¤šä¸ªå‚è€ƒå›¾åƒã€‚ä¸ºäº†è¶…è¶Šè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå…·ä½“æ¥è¯´æ˜¯ä¸€ç§äººåƒå›¾åƒçš„identity-preservingå›¾åƒç”Ÿæˆæ–¹æ³•ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨äº†ç›´é€šé€”å¾„æœºåˆ¶ï¼Œ circumventing the need for intensive fine-tuning, thereby facilitating quick and efficient image generationã€‚æˆ‘ä»¬çš„åˆ›æ–°åœ¨äºå°†æ¶‚æŠ¹å›¾åƒã€äººè„¸å›¾åƒå’Œæ–‡æœ¬æç¤ºç›¸ç»“åˆï¼Œä»¥ä¾¿æŒ‡å¯¼å›¾åƒç”Ÿæˆè¿‡ç¨‹ã€‚è¿™ç§ç‹¬ç‰¹çš„ç»„åˆä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆå¤šç§åº”ç”¨ï¼Œå¦‚è‰ºæœ¯æŠ•å½±å’Œèº«ä»½æ··åˆå›¾åƒã€‚æˆ‘ä»¬çš„å®éªŒç»“æœï¼ŒåŒ…æ‹¬both qualitativeå’Œquantitativeè¯„ä¼°ï¼Œè¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ•ˆç‡å’Œä¿æŒä¸»ä½“èº«ä»½æ–¹é¢å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜æ•ˆæ€§å’Œèº«ä»½ä¿æŒæ–¹é¢ã€‚
</details></li>
</ul>
<hr>
<h2 id="Supervised-learning-of-spatial-features-with-STDP-and-homeostasis-using-Spiking-Neural-Networks-on-SpiNNaker"><a href="#Supervised-learning-of-spatial-features-with-STDP-and-homeostasis-using-Spiking-Neural-Networks-on-SpiNNaker" class="headerlink" title="Supervised learning of spatial features with STDP and homeostasis using Spiking Neural Networks on SpiNNaker"></a>Supervised learning of spatial features with STDP and homeostasis using Spiking Neural Networks on SpiNNaker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02659">http://arxiv.org/abs/2312.02659</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergio Davies, Andrew Gait, Andrew Rowley, Alessandro Di Nuovo</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯åœ¨è¶…è¿‡è§„åˆ™ç¥ç»ç½‘ç»œï¼ˆSNNï¼‰ä¸­è¿›è¡Œæœ‰ç›‘ç£å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿè¯†åˆ«ç©ºé—´æ¨¡å¼ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†å¿«é€Ÿæ—¶é’Ÿä¾å­˜æ€§é—ä¼ ï¼ˆSTDPï¼‰å’Œè‡ªé€‚åº”æœºåˆ¶æ¥å®ç°SNNçš„ç›‘ç£å­¦ä¹ ã€‚</li>
<li>results: è¯•éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å•ä¸ªæ¨¡å¼è¿›è¡Œè®­ç»ƒæ—¶ï¼ŒSNNèƒ½å¤Ÿå‡†ç¡®åœ°è¯†åˆ«å‡ºè®­ç»ƒæ¨¡å¼ï¼Œå‡†ç¡®ç‡ä¸º100%ã€‚ç„¶è€Œï¼Œå½“å¤šä¸ªæ¨¡å¼åŒæ—¶è®­ç»ƒåœ¨åŒä¸€ä¸ªç½‘ç»œä¸Šæ—¶ï¼Œæ¨¡å¼çš„ç›¸ä¼¼æ€§ä¼šå½±å“è¯†åˆ«çš„å‡†ç¡®ç‡ã€‚è¿™ç§è®­ç»ƒSNNè¯†åˆ«ç©ºé—´æ¨¡å¼çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºé™æ­¢å›¾åƒè¯†åˆ«å’Œè®¡ç®—æœºç½‘ç»œä¸­çš„æµé‡åˆ†æç­‰é¢†åŸŸã€‚æ­¤å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜å‘ç°ï¼Œåœ¨åŒä¸€ä¸ªç½‘ç»œä¸Šè®­ç»ƒå¤šä¸ªæ¨¡å¼æ—¶ï¼Œhomeostaticå› ç´ å¯ä»¥ä½¿ç½‘ç»œæ£€æµ‹åˆ°æ¨¡å¼ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè€Œä¸æ˜¯åªæ˜¯å®Œå…¨åŒ¹é…çš„æ¨¡å¼ã€‚<details>
<summary>Abstract</summary>
Artificial Neural Networks (ANN) have gained large popularity thanks to their ability to learn using the well-known backpropagation algorithm. On the other hand, Spiking Neural Networks (SNNs), despite having wider abilities than ANNs, have always presented a challenge in the training phase. This paper shows a new method to perform supervised learning on SNNs, using Spike Timing Dependent Plasticity (STDP) and homeostasis, aiming at training the network to identify spatial patterns. The method is tested using the SpiNNaker digital architecture. A SNN is trained to recognise one or multiple patterns and performance metrics are extracted to measure the performance of the network. Some considerations are drawn from the results showing that, in the case of a single trained pattern, the network behaves as the ideal detector, with 100% accuracy in detecting the trained pattern. However, as the number of trained patterns on a single network increases, the accuracy of the identification is linked to the similarities between these patterns. This method of training an SNN to detect spatial patterns may be applied on pattern recognition in static images or traffic analysis in computer networks, where each network packet represents a spatial pattern. It will be stipulated that the homeostatic factor may enable the network to detect patterns with some degree of similarities, rather than only perfectly matching patterns.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰å› å…¶èƒ½å¤Ÿä½¿ç”¨è‘—åçš„åå°„å­¦ä¹ ç®—æ³•è€Œå—æ¬¢è¿ã€‚ç„¶è€Œï¼Œç¥ç»å…ƒè„‰å†²ç½‘ç»œï¼ˆSNNï¼‰å´æ€»æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå­˜åœ¨æŒ‘æˆ˜ã€‚è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è¶…è¶Šå­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨è„‰å†²æ—¶é—´ä¾èµ–æŸæ€§ï¼ˆSTDPï¼‰å’Œè‡ªé€‚åº”æœºåˆ¶ï¼Œä»¥è®­ç»ƒç½‘ç»œè®¤è¯†ç©ºé—´æ¨¡å¼ã€‚è¿™ç§æ–¹æ³•åœ¨ä½¿ç”¨SpikeNNakeræ•°å­—æ¶æ„æµ‹è¯•åï¼Œå¯ä»¥è®©ç½‘ç»œè®¤è¯†ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å¼ï¼Œå¹¶æå–æ€§èƒ½æŒ‡æ ‡æ¥è¡¡é‡ç½‘ç»œçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œå½“å•ä¸ªæ¨¡å¼è¢«è®­ç»ƒæ—¶ï¼Œç½‘ç»œä¼š behave as the ideal detectorï¼Œå³100%çš„å‡†ç¡®ç‡å¯ä»¥æ£€æµ‹åˆ°è®­ç»ƒè¿‡çš„æ¨¡å¼ã€‚ç„¶è€Œï¼Œå½“å¤šä¸ªæ¨¡å¼åœ¨åŒä¸€ä¸ªç½‘ç»œä¸Šè¢«è®­ç»ƒæ—¶ï¼Œæ¨¡å¼çš„ç›¸ä¼¼æ€§ä¼šå½±å“ç½‘ç»œçš„è¯†åˆ«ç‡ã€‚è¿™ç§è®­ç»ƒSNNè®¤è¯†ç©ºé—´æ¨¡å¼çš„æ–¹æ³•å¯ä»¥åº”ç”¨äºé™æ­¢å›¾åƒæˆ–è®¡ç®—æœºç½‘ç»œä¸­çš„æ¨¡å¼è¯†åˆ«ï¼Œå…¶ä¸­æ¯ä¸ªç½‘ç»œåŒ…etes represent a spatial patternã€‚æ­¤å¤–ï¼Œhomeostatic factorå¯ä»¥ä½¿ç½‘ç»œæ£€æµ‹åˆ°ä¸€å®šç¨‹åº¦çš„ç›¸ä¼¼æ¨¡å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯å®Œå…¨åŒ¹é…çš„æ¨¡å¼ã€‚
</details></li>
</ul>
<hr>
<h2 id="How-should-the-advent-of-large-language-models-affect-the-practice-of-science"><a href="#How-should-the-advent-of-large-language-models-affect-the-practice-of-science" class="headerlink" title="How should the advent of large language models affect the practice of science?"></a>How should the advent of large language models affect the practice of science?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03759">http://arxiv.org/abs/2312.03759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Binz, Stephan Alaniz, Adina Roskies, Balazs Aczel, Carl T. Bergstrom, Colin Allen, Daniel Schad, Dirk Wulff, Jevin D. West, Qiong Zhang, Richard M. Shiffrin, Samuel J. Gershman, Ven Popov, Emily M. Bender, Marco Marelli, Matthew M. Botvinick, Zeynep Akata, Eric Schulz</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦ç ”ç©¶ä¸­çš„åº”ç”¨ï¼Œä»¥åŠè¿™äº›åº”ç”¨çš„å½±å“ã€‚</li>
<li>methods: æœ¬æ–‡é‚€è¯·äº†å››ä¸ªä¸åŒé¢†åŸŸçš„ç§‘å­¦å®¶å…±åŒreflectå’Œè¾©è®ºï¼Œä»¥ä¾¿äº†è§£LLMsçš„åº”ç”¨å¯¹ç§‘å­¦ç ”ç©¶çš„å½±å“ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡æ€»ç»“äº†å››ä¸ªä¸åŒçš„è§†è§’ï¼ŒåŒ…æ‹¬Schulzç­‰äººçš„çœ‹æ³•ï¼Œå³ä¸äººç±»åˆä½œè€…ä¸åŒï¼ŒBenderç­‰äººçš„çœ‹æ³•ï¼Œå³LLMsè¢«è¿‡åº¦å¤¸å¤§å’Œè¿ç”¨ï¼ŒMarelliç­‰äººçš„çœ‹æ³•ï¼Œå³é€æ˜çš„å½’å±å’Œè´£ä»»ï¼Œä»¥åŠBotvinickå’ŒGershmançš„çœ‹æ³•ï¼Œå³äººç±»åº”è¯¥å†³å®šç§‘å­¦çš„å‘å±•è§„åˆ’ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs) are being increasingly incorporated into scientific workflows. However, we have yet to fully grasp the implications of this integration. How should the advent of large language models affect the practice of science? For this opinion piece, we have invited four diverse groups of scientists to reflect on this query, sharing their perspectives and engaging in debate. Schulz et al. make the argument that working with LLMs is not fundamentally different from working with human collaborators, while Bender et al. argue that LLMs are often misused and over-hyped, and that their limitations warrant a focus on more specialized, easily interpretable tools. Marelli et al. emphasize the importance of transparent attribution and responsible use of LLMs. Finally, Botvinick and Gershman advocate that humans should retain responsibility for determining the scientific roadmap. To facilitate the discussion, the four perspectives are complemented with a response from each group. By putting these different perspectives in conversation, we aim to bring attention to important considerations within the academic community regarding the adoption of LLMs and their impact on both current and future scientific practices.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="SAMSGL-Series-Aligned-Multi-Scale-Graph-Learning-for-Spatio-Temporal-Forecasting"><a href="#SAMSGL-Series-Aligned-Multi-Scale-Graph-Learning-for-Spatio-Temporal-Forecasting" class="headerlink" title="SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting"></a>SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02646">http://arxiv.org/abs/2312.02646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobei Zou, Luolin Xiong, Yang Tang, Jurgen Kurths</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜ç©ºé—´æ—¶é—´é¢„æµ‹çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨äº¤é€šé¢„æµ‹å’Œå¤©æ°”é¢„æµ‹ç­‰é¢†åŸŸï¼Œå› ä¸ºè¿™äº›é¢†åŸŸçš„é¢„æµ‹å—åˆ°å»¶è¿Ÿä¼ æ’­ dinamicså’Œé«˜ç»´åº¦äº’åŠ¨çš„å½±å“ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†Series-Aligned Multi-Scale Graph Learningï¼ˆSAMSGLï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æé«˜é¢„æµ‹æ€§èƒ½ã€‚ä¸ºäº†å¤„ç†å»¶è¿Ÿä¼ æ’­dinamicsï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ä¸ªåºåˆ—alignedå›¾åƒæ¡ä»¶å±‚ï¼Œä»¥ä¾¿èšåˆéå»¶è¿Ÿå›¾åƒä¿¡å·ï¼Œ thereby mitigating the influence of time delays on accuracyã€‚å¦å¤–ï¼Œç ”ç©¶äººå‘˜è¿˜æå‡ºäº†ä¸€ä¸ªå¤šå°ºåº¦å›¾åƒå­¦ä¹ æ¶æ„ï¼ŒåŒ…æ‹¬å…¨çƒå›¾åƒç»“æ„å’Œå¤šå°ºåº¦å›¾åƒç»“æ„ï¼Œä»¥äº†è§£å…¨çƒå’Œåœ°æ–¹ç©ºé—´æ—¶é—´äº’åŠ¨ã€‚</li>
<li>results: è¿™ä¸ªç ”ç©¶çš„å®éªŒç»“æœæ˜¾ç¤ºSAMSGLçš„è¡¨ç°æ¯”å…¶ä»–æ–¹æ³•æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨å¤©æ°”é¢„æµ‹å’Œäº¤é€šé¢„æµ‹ç­‰é¢†åŸŸã€‚<details>
<summary>Abstract</summary>
Spatio-temporal forecasting in various domains, like traffic prediction and weather forecasting, is a challenging endeavor, primarily due to the difficulties in modeling propagation dynamics and capturing high-dimensional interactions among nodes. Despite the significant strides made by graph-based networks in spatio-temporal forecasting, there remain two pivotal factors closely related to forecasting performance that need further consideration: time delays in propagation dynamics and multi-scale high-dimensional interactions. In this work, we present a Series-Aligned Multi-Scale Graph Learning (SAMSGL) framework, aiming to enhance forecasting performance. In order to handle time delays in spatial interactions, we propose a series-aligned graph convolution layer to facilitate the aggregation of non-delayed graph signals, thereby mitigating the influence of time delays for the improvement in accuracy. To understand global and local spatio-temporal interactions, we develop a spatio-temporal architecture via multi-scale graph learning, which encompasses two essential components: multi-scale graph structure learning and graph-fully connected (Graph-FC) blocks. The multi-scale graph structure learning includes a global graph structure to learn both delayed and non-delayed node embeddings, as well as a local one to learn node variations influenced by neighboring factors. The Graph-FC blocks synergistically fuse spatial and temporal information to boost prediction accuracy. To evaluate the performance of SAMSGL, we conduct experiments on meteorological and traffic forecasting datasets, which demonstrate its effectiveness and superiority.
</details>
<details>
<summary>æ‘˜è¦</summary>
é¢„æµ‹åœ¨ä¸åŒé¢†åŸŸï¼Œå¦‚äº¤é€šé¢„æµ‹å’Œå¤©æ°”é¢„æµ‹ï¼Œæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä¸»è¦æ˜¯å› ä¸ºæ¨¡å‹å›°éš¾åœ¨æè¿°ååŒåŠ¨åŠ›å’Œé«˜ç»´åº¦ç›¸äº’ä½œç”¨çš„é—®é¢˜ã€‚å°½ç®¡ graf-based ç½‘ç»œåœ¨ç©ºé—´-æ—¶é¢„æµ‹æ–¹é¢åšå‡ºäº† significativos progresosï¼Œä½†è¿˜æœ‰ä¸¤ä¸ªå…³é”®å› ç´ éœ€è¦è¿›ä¸€æ­¥è€ƒè™‘ï¼šæ—¶é—´å»¶è¿Ÿåœ¨ååŒåŠ¨åŠ›å’Œå¤šçº§é«˜ç»´åº¦ç›¸äº’ä½œç”¨ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªSeries-Aligned Multi-Scale Graph Learningï¼ˆSAMSGLï¼‰æ¡†æ¶ï¼Œä»¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚ä¸ºäº†å¤„ç†ç©ºé—´ååŒåŠ¨åŠ›ä¸­çš„æ—¶é—´å»¶è¿Ÿï¼Œæˆ‘ä»¬æè®®äº†ä¸€ä¸ªåºåˆ—å¯¹é½å›¾åƒç§¯åˆ†å±‚ï¼Œä»¥ä¾¿èšåˆéå»¶è¿Ÿå›¾åƒä¿¡å·ï¼Œä»è€Œå‡å°‘æ—¶é—´å»¶è¿Ÿçš„å½±å“ï¼Œæé«˜å‡†ç¡®æ€§ã€‚ä¸ºäº†ç†è§£å…¨çƒå’Œæœ¬åœ°ç©ºé—´-æ—¶ç›¸äº’ä½œç”¨ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªå¤šå°ºåº¦å›¾å­¦å­¦ä¹ æ¶æ„ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼šå¤šå°ºåº¦å›¾ç»“æ„å­¦ä¹ å’Œå›¾å…¨è¿æ¥ï¼ˆGraph-FCï¼‰å—ã€‚å¤šå°ºåº¦å›¾ç»“æ„å­¦ä¹ åŒ…æ‹¬ä¸€ä¸ªå…¨çƒå›¾ç»“æ„ï¼Œç”¨äºå­¦ä¹ å»¶è¿Ÿå’Œéå»¶è¿ŸèŠ‚ç‚¹è¡¨ç¤ºï¼Œä»¥åŠä¸€ä¸ªæœ¬åœ°å›¾ç»“æ„ï¼Œç”¨äºå­¦ä¹ å—åˆ°é‚»è¿‘å› ç´ å½±å“çš„èŠ‚ç‚¹å˜åŒ–ã€‚å›¾å…¨è¿æ¥å—ååŒç»¼åˆç©ºé—´å’Œæ—¶é—´ä¿¡æ¯ï¼Œä»¥æé«˜é¢„æµ‹ç²¾åº¦ã€‚ä¸ºè¯„ä¼° SAMSGL çš„æ€§èƒ½ï¼Œæˆ‘ä»¬åœ¨ meteorological å’Œäº¤é€šé¢„æµ‹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºå®ƒçš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="On-the-Initialization-of-Graph-Neural-Networks"><a href="#On-the-Initialization-of-Graph-Neural-Networks" class="headerlink" title="On the Initialization of Graph Neural Networks"></a>On the Initialization of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02622">http://arxiv.org/abs/2312.02622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lspongebobjh/virgo_icml2023">https://github.com/lspongebobjh/virgo_icml2023</a></li>
<li>paper_authors: Jiahang Li, Yakun Song, Xiang Song, David Paul Wipf<br>for:This paper focuses on improving the initialization process for graph neural networks (GNNs) to reduce the variance of forward and backward propagation and improve model performance.methods:The proposed method, called Virgo, analyzes the variance of forward and backward propagation across GNN layers and proposes a new initialization method that takes into account the influence of the activation function, hidden dimension, graph structure, and message passing.results:The proposed Virgo method leads to superior model performance and more stable variance at initialization on node classification, link prediction, and graph classification tasks, as demonstrated through comprehensive experiments on 15 datasets.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have displayed considerable promise in graph representation learning across various applications. The core learning process requires the initialization of model weight matrices within each GNN layer, which is typically accomplished via classic initialization methods such as Xavier initialization. However, these methods were originally motivated to stabilize the variance of hidden embeddings and gradients across layers of Feedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to avoid vanishing gradients and maintain steady information flow. In contrast, within the GNN context classical initializations disregard the impact of the input graph structure and message passing on variance. In this paper, we analyze the variance of forward and backward propagation across GNN layers and show that the variance instability of GNN initializations comes from the combined effect of the activation function, hidden dimension, graph structure and message passing. To better account for these influence factors, we propose a new initialization method for Variance Instability Reduction within GNN Optimization (Virgo), which naturally tends to equate forward and backward variances across successive layers. We conduct comprehensive experiments on 15 datasets to show that Virgo can lead to superior model performance and more stable variance at initialization on node classification, link prediction and graph classification tasks. Codes are in https://github.com/LspongebobJH/virgo_icml2023.
</details>
<details>
<summary>æ‘˜è¦</summary>
graph neural networks (GNNs) æœ‰æ˜¾è‘—çš„æ­é…å¯èƒ½æ€§åœ¨ä¸åŒåº”ç”¨åœºæ™¯ä¸­çš„å›¾è¡¨ç¤ºå­¦ä¹ ä¸­å±•ç°å‡ºæ¥ã€‚GNNå±‚çš„æ ¸å¿ƒå­¦ä¹ è¿‡ç¨‹é€šå¸¸é€šè¿‡ ĞºĞ»Ğ°ÑÑĞ¸å‹çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œå¦‚Xavier initializationæ¥è¿›è¡Œåˆå§‹åŒ–æ¨¡å‹æƒé‡çŸ©é˜µã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•æœ€åˆæ˜¯ä¸ºäº†ç¨³å®šéšè—åµŒå…¥å’Œæ¢¯åº¦çš„æ–¹å·®åœ¨å±‚æ¬¡çš„Feedforward Neural Networks (FNNs)å’ŒConvolutional Neural Networks (CNNs)ä¸­ï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±å’Œä¿æŒä¿¡æ¯æµå¹³ç¨³ã€‚ç„¶è€Œï¼Œåœ¨ GNN ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™äº›å¤å…¸çš„åˆå§‹åŒ–æ–¹æ³•å¿½è§†äº†è¾“å…¥å›¾ç»“æ„å’Œæ¶ˆæ¯ä¼ é€’å¯¹æ–¹å·®çš„å½±å“ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº† GNN å±‚ä¹‹é—´çš„æ–¹å·®å˜åŒ–ï¼Œå¹¶æ˜¾ç¤ºäº† GNN åˆå§‹åŒ–çš„æ–¹å·®ä¸ç¨³å®šæ€§æ¥è‡ªäºæ´»åŠ¨å‡½æ•°ã€éšè—ç»´åº¦ã€å›¾ç»“æ„å’Œæ¶ˆæ¯ä¼ é€’çš„å…±åŒä½œç”¨ã€‚ä¸ºäº†æ›´å¥½åœ°è€ƒè™‘è¿™äº›å½±å“å› ç´ ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œç§°ä¸º Variance Instability Reduction within GNN Optimization (Virgo)ï¼Œå®ƒè‡ªç„¶åœ°åœ¨Successiveå±‚ä¹‹é—´å‡è¡¡å‰å‘å’Œåå‘æ–¹å·®ã€‚æˆ‘ä»¬å¯¹ 15 ä¸ªæ•°æ®é›†è¿›è¡Œäº†å…¨é¢çš„å®éªŒï¼Œè¯æ˜ Virgo å¯ä»¥åœ¨èŠ‚ç‚¹åˆ†ç±»ã€é“¾æ¥é¢„æµ‹å’Œå›¾ç±»å‹ä»»åŠ¡ä¸Šæé«˜æ¨¡å‹æ€§èƒ½å¹¶ä¿æŒæ›´ç¨³å®šçš„æ–¹å·®ã€‚ä»£ç åœ¨ https://github.com/LspongebobJH/virgo_icml2023ã€‚
</details></li>
</ul>
<hr>
<h2 id="Panoptica-â€“-instance-wise-evaluation-of-3D-semantic-and-instance-segmentation-maps"><a href="#Panoptica-â€“-instance-wise-evaluation-of-3D-semantic-and-instance-segmentation-maps" class="headerlink" title="Panoptica â€“ instance-wise evaluation of 3D semantic and instance segmentation maps"></a>Panoptica â€“ instance-wise evaluation of 3D semantic and instance segmentation maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02608">http://arxiv.org/abs/2312.02608</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/brainlesion/panoptica">https://github.com/brainlesion/panoptica</a></li>
<li>paper_authors: Florian Kofler, Hendrik MÃ¶ller, Josef A. Buchner, Ezequiel de la Rosa, Ivan Ezhov, Marcel Rosier, Isra Mekki, Suprosanna Shit, Moritz Negwer, Rami Al-Maskari, Ali ErtÃ¼rk, Shankeeth Vinayahalingam, Fabian Isensee, Sarthak Pati, Daniel Rueckert, Jan S. Kirschke, Stefan K. Ehrlich, Annika Reinke, Bjoern Menze, Benedikt Wiestler, Marie Piraud</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è®¡ç®—2Då’Œ3Dåˆ†å‰²å›¾åƒçš„å®ä¾‹åŒ–åˆ†å‰²è´¨é‡æŒ‡æ ‡è€Œè®¾è®¡çš„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§å¯ç¼–ç¨‹çš„ã€æ€§èƒ½ä¼˜åŒ–çš„åŒ…è£…åŒ…ï¼Œåä¸ºpanopticaï¼Œä»¥è®¡ç®—åˆ†å‰²å›¾åƒçš„å®ä¾‹åŒ–åˆ†å‰²è´¨é‡æŒ‡æ ‡ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡ä½¿ç”¨ä¸åŒçš„æŒ‡æ ‡ï¼Œå¦‚å¹³å‡å¯¹ç§°è¡¨é¢è·ç¦»åº¦é‡ï¼Œå¯¹å¤šç§å®é™…åŒ»å­¦æ•°æ®è¿›è¡Œäº†è¯¦ç»†çš„è¯„ä¼°ï¼Œå¹¶è¯æ˜äº†panopticaçš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
This paper introduces panoptica, a versatile and performance-optimized package designed for computing instance-wise segmentation quality metrics from 2D and 3D segmentation maps. panoptica addresses the limitations of existing metrics and provides a modular framework that complements the original intersection over union-based panoptic quality with other metrics, such as the distance metric Average Symmetric Surface Distance. The package is open-source, implemented in Python, and accompanied by comprehensive documentation and tutorials. panoptica employs a three-step metrics computation process to cover diverse use cases. The efficacy of panoptica is demonstrated on various real-world biomedical datasets, where an instance-wise evaluation is instrumental for an accurate representation of the underlying clinical task. Overall, we envision panoptica as a valuable tool facilitating in-depth evaluation of segmentation methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†panopticaï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”æ€§èƒ½ä¼˜åŒ–çš„åŒ…ï¼Œç”¨äºè®¡ç®—2Då’Œ3D segmentationå›¾åƒè´¨é‡æŒ‡æ ‡ã€‚panopticaè§£å†³äº†ç°æœ‰æŒ‡æ ‡çš„é™åˆ¶ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ¨¡å—åŒ–æ¡†æ¶ï¼Œå¯ä»¥è¡¥å……åŸå§‹äº¤é›† UNION åŸºäºçš„æƒé‡è´¨é‡æŒ‡æ ‡ï¼Œä¾‹å¦‚å¹³å‡å¯¹ç§°è¡¨é¢è·ç¦»æŒ‡æ ‡ã€‚åŒ…æ˜¯å¼€æºçš„ï¼Œå®ç°åœ¨Pythonä¸­ï¼Œå¹¶é™„å¸¦äº†è¯¦ç»†çš„æ–‡æ¡£å’Œæ•™ç¨‹ã€‚panopticaä½¿ç”¨ä¸‰æ­¥è®¡ç®—è¿‡ç¨‹æ¥è¦†ç›–å¤šç§åº”ç”¨åœºæ™¯ã€‚è®ºæ–‡ç¤ºå‡ºäº†panopticaåœ¨å¤šä¸ªçœŸå®çš„ç”Ÿç‰©åŒ»å­¦æ•°æ®é›†ä¸Šçš„æ•ˆæœï¼Œå…¶ä¸­Instance-wiseè¯„ä¼°å¯¹äºåŒ»å­¦ä»»åŠ¡çš„å‡†ç¡®è¡¨ç¤ºæ˜¯éå¸¸é‡è¦çš„ã€‚æ€»ä¹‹ï¼Œæˆ‘ä»¬è§†panopticaä¸ºä¸€ä¸ªæœ‰ä»·å€¼çš„å·¥å…·ï¼Œç”¨äºæ·±å…¥è¯„ä¼°åˆ† segmentation æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Impact-of-Tokenization-on-LLaMa-Russian-Adaptation"><a href="#Impact-of-Tokenization-on-LLaMa-Russian-Adaptation" class="headerlink" title="Impact of Tokenization on LLaMa Russian Adaptation"></a>Impact of Tokenization on LLaMa Russian Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02598">http://arxiv.org/abs/2312.02598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mikhail Tikhomirov, Daniil Chernyshev</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è§£å†³å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨éè‹±è¯­è¾“å…¥æ—¶è¡¨ç°ä¸‹é™çš„é—®é¢˜ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨ vocabulary substitution æ–¹æ³•æ¥æ”¹è¿› LLaMa ä¿„è¯­è¨€é€‚åº”ã€‚</li>
<li>results: è‡ªåŠ¨è¯„ä»·ç»“æœè¡¨æ˜ï¼Œ vocabulary substitution ä¸ä»…æé«˜äº†æ¨¡å‹åœ¨ä¿„è¯­è¨€ä¸­çš„è´¨é‡ï¼Œè¿˜å¯ä»¥åŠ é€Ÿ fine-tuningï¼ˆ35%ï¼‰å’Œæ¨ç†ï¼ˆæœ€é«˜è¾¾ 60%ï¼‰ï¼ŒåŒæ—¶é™ä½å†…å­˜å ç”¨ã€‚äººå·¥è¯„ä»·ç»“æœè¿˜è¡¨æ˜ï¼Œä½¿ç”¨ä¿„è¯­è¨€é€‚åº”è¯æ±‡çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´å…·ç”¨æˆ·å–œçˆ±çš„ç­”æ¡ˆã€‚<details>
<summary>Abstract</summary>
Latest instruction-tuned large language models (LLM) show great results on various tasks, however, they often face performance degradation for non-English input. There is evidence that the reason lies in inefficient tokenization caused by low language representation in pre-training data which hinders the comprehension of non-English instructions, limiting the potential of target language instruction-tuning. In this work we investigate the possibility of addressing the issue with vocabulary substitution in the context of LLaMa Russian language adaptation. We explore three variants of vocabulary adaptation and test their performance on Saiga instruction-tuning and fine-tuning on Russian Super Glue benchmark. The results of automatic evaluation show that vocabulary substitution not only improves the model's quality in Russian but also accelerates fine-tuning (35%) and inference (up to 60%) while reducing memory consumption. Additional human evaluation of the instruction-tuned models demonstrates that models with Russian-adapted vocabulary generate answers with higher user preference than the original Saiga-LLaMa model.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="UTBoost-A-Tree-boosting-based-System-for-Uplift-Modeling"><a href="#UTBoost-A-Tree-boosting-based-System-for-Uplift-Modeling" class="headerlink" title="UTBoost: A Tree-boosting based System for Uplift Modeling"></a>UTBoost: A Tree-boosting based System for Uplift Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02573">http://arxiv.org/abs/2312.02573</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jd-opensource/utboost">https://github.com/jd-opensource/utboost</a></li>
<li>paper_authors: Junjie Gao, Xiangyu Zheng, DongDong Wang, Zhixiang Huang, Bangqi Zheng, Kai Yang</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ—¨åœ¨æå‡ºä¸¤ç§åŸºäºGradient Boosting Decision Treesï¼ˆGBDTï¼‰ç®—æ³•çš„æ–°æ–¹æ³•ï¼Œç”¨äºä¼°è®¡é¡¾å®¢å¢é•¿ï¼ˆupliftï¼‰ã€‚</li>
<li>methods: è¿™ä¸¤ç§æ–¹æ³•åˆ†åˆ«æ˜¯ï¼šä¸€ç§æ˜¯åŸºäºSequential Additive Modelï¼ˆSAMï¼‰çš„ç´¯åŠ å­¦ä¹ æ–¹æ³•ï¼Œå¦ä¸€ç§æ˜¯åŸºäºHuber Regressionsï¼ˆHuberï¼‰çš„å¤šç›®æ ‡å­¦ä¹ æ–¹æ³•ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ä¸¤ç§æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°ä¼°è®¡é¡¾å®¢å¢é•¿ï¼Œå¹¶ä¸”frequently yield remarkable improvements over base modelsã€‚æ­¤å¤–ï¼Œ authorsè¿˜å¼€å‘äº†ä¸€ä¸ªç‰¹æœ‰çš„æ ‘æå‡ç³»ç»Ÿï¼ˆUTBoostï¼‰ï¼Œç”¨äºå®ç°è¿™äº›æ–¹æ³•çš„åº”ç”¨ã€‚<details>
<summary>Abstract</summary>
Uplift modeling refers to the set of machine learning techniques that a manager may use to estimate customer uplift, that is, the net effect of an action on some customer outcome. By identifying the subset of customers for whom a treatment will have the greatest effect, uplift models assist decision-makers in optimizing resource allocations and maximizing overall returns. Accurately estimating customer uplift poses practical challenges, as it requires assessing the difference between two mutually exclusive outcomes for each individual. In this paper, we propose two innovative adaptations of the well-established Gradient Boosting Decision Trees (GBDT) algorithm, which learn the causal effect in a sequential way and overcome the counter-factual nature. Both approaches innovate existing techniques in terms of ensemble learning method and learning objectives, respectively. Experiments on large-scale datasets demonstrate the usefulness of the proposed methods, which often yielding remarkable improvements over base models. To facilitate the application, we develop the UTBoost, an end-to-end tree boosting system specifically designed for uplift modeling. The package is open source and has been optimized for training speed to meet the needs of real industrial applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>é€šè¿‡æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œç®¡ç†è€…å¯ä»¥ä½¿ç”¨â€œå‡çº§æ¨¡å‹â€æ¥ä¼°ç®—æ¯ä¸ªå®¢æˆ·çš„å‡çº§æ•ˆæœï¼Œå³å¯¹æŸä¸ªå®¢æˆ·çš„è¡Œä¸ºäº§ç”Ÿçš„å½±å“ã€‚é€šè¿‡ identificatinig æ¯ä¸ªå®¢æˆ·å¯¹å¾…ç‰¹å¾çš„æœ€å¤§æ•ˆæœï¼Œå‡çº§æ¨¡å‹å¯ä»¥å¸®åŠ©å†³ç­–è€…ä¼˜åŒ–èµ„æºåˆ†é…å’Œæœ€å¤§åŒ–æ€»æ”¶ç›Šã€‚ä¼°ç®—å®¢æˆ·å‡çº§çš„å®é™…æŒ‘æˆ˜åœ¨äºéœ€è¦è¯„ä¼°æ¯ä¸ªå®¢æˆ·å¯¹ä¸¤ç§ä¸åŒç»“æœä¹‹é—´çš„å·®å¼‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§åŸºäº Gradient Boosting Decision Treesï¼ˆGBDTï¼‰ç®—æ³•çš„åˆ›æ–°æ–¹æ³•ï¼Œå¯ä»¥åœ¨çº§è”çš„æ–¹å¼å­¦ä¹  causal effectï¼Œå¹¶ä¸”è¶…è¶Šå¯¹å‡æ€§çš„æŒ‘æˆ˜ã€‚è¿™ä¸¤ç§æ–¹æ³•åœ¨ Ensemble Learning æ–¹æ³•å’Œå­¦ä¹ ç›®æ ‡æ–¹é¢å‡æœ‰åˆ›æ–°ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œè¿™äº›æ–¹æ³•åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šé€šå¸¸å¯ä»¥è·å¾—æƒŠäººçš„æ”¹è¿›ã€‚ä¸ºä¾¿äºåº”ç”¨ï¼Œæˆ‘ä»¬å¼€å‘äº† UTBoostï¼Œä¸€ä¸ªä¸“é—¨ä¸ºå‡çº§æ¨¡å‹è®¾è®¡çš„ç«¯åˆ°ç«¯æ ‘æå‡ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿæ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒé€Ÿåº¦æ–¹é¢è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä»¥æ»¡è¶³å®é™…å·¥ä¸šåº”ç”¨çš„éœ€æ±‚ã€‚
</details></li>
</ul>
<hr>
<h2 id="Structured-World-Representations-in-Maze-Solving-Transformers"><a href="#Structured-World-Representations-in-Maze-Solving-Transformers" class="headerlink" title="Structured World Representations in Maze-Solving Transformers"></a>Structured World Representations in Maze-Solving Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02566">http://arxiv.org/abs/2312.02566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/understanding-search/structured-representations-maze-transformers">https://github.com/understanding-search/structured-representations-maze-transformers</a></li>
<li>paper_authors: Michael Igorevich Ivanitskiy, Alex F. Spies, Tilman RÃ¤uker, Guillaume Corlouer, Chris Mathwin, Lucia Quirke, Can Rager, Rusheb Shah, Dan Valentine, Cecilia Diniz Behn, Katsumi Inoue, Samy Wu Fung</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ç†è§£å°å‹è½¬ç§»æ¨¡å‹åœ¨è§£å†³è¿·å®«é—®é¢˜ä¸­çš„å†…éƒ¨è¡Œä¸ºã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨è½¬ç§»æ¨¡å‹è§£å†³è¿·å®«é—®é¢˜ï¼Œå¹¶å‘ç°è¿™äº›æ¨¡å‹å½¢æˆäº†è¿·å®« topology çš„ç»“æ„åŒ–å†…éƒ¨è¡¨ç¤ºï¼Œä»¥åŠæœ‰æ•ˆè·¯å¾„çš„é¢„æµ‹ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œåªéœ€è¦å•ä¸ª token çš„å·®åˆ†æµå¯ä»¥çº¿æ€§è§£ç é‡å»ºæ•´ä¸ªè¿·å®«ï¼Œå¹¶ä¸”å„ä¸ªtokençš„åµŒå…¥æœ‰ç©ºé—´ç»“æ„ã€‚æ­¤å¤–ï¼Œè¿˜å‘ç°äº†è·¯å¾„è·Ÿè¸ªçš„å¬åŠ›å¤´ï¼ˆç§°ä¸ºâ€œç›¸é‚»å¤´â€ï¼‰ï¼Œå®ƒä»¬å‚ä¸æ‰¾åˆ°æœ‰æ•ˆçš„åç»­ tokensã€‚<details>
<summary>Abstract</summary>
Transformer models underpin many recent advances in practical machine learning applications, yet understanding their internal behavior continues to elude researchers. Given the size and complexity of these models, forming a comprehensive picture of their inner workings remains a significant challenge. To this end, we set out to understand small transformer models in a more tractable setting: that of solving mazes. In this work, we focus on the abstractions formed by these models and find evidence for the consistent emergence of structured internal representations of maze topology and valid paths. We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze. We also find that the learned embeddings of individual tokens have spatial structure. Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed $\textit{adjacency heads}$), which are implicated in finding valid subsequent tokens.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯å‘å™¨æ¨¡å‹åœ¨è®¸å¤šå®ç”¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œç„¶è€Œå®ƒä»¬çš„å†…éƒ¨è¡Œä¸ºä»ç„¶å¯¹ç ”ç©¶äººå‘˜éš¾ä»¥ç†è§£ã€‚ç”±äºè¿™äº›æ¨¡å‹çš„å¤§å°å’Œå¤æ‚æ€§ï¼Œå»ºç«‹å…¨é¢çš„å†…éƒ¨è¡Œä¸ºå›¾åƒå˜å¾—éå¸¸å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°è¯•äº†é€šè¿‡è§£å†³è¿·å®«æ¥ç†è§£å°å¯å‘å™¨æ¨¡å‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨å¯å‘å™¨æ¨¡å‹å½¢æˆçš„æŠ½è±¡å’Œæœ‰æ•ˆè·¯å¾„çš„å†…éƒ¨è¡¨ç¤ºã€‚æˆ‘ä»¬è¯æ˜äº†åªéœ€è¦ä¸€ä¸ªTokençš„å‰©ä½™æµå¯ä»¥çº¿æ€§è§£ç é‡å»ºæ•´ä¸ªè¿·å®«ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸ªtokençš„åµŒå…¥æœ‰ç©ºé—´ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†è·¯å¾„è·Ÿè¸ªçš„ç”µè·¯ï¼Œé€šè¿‡identifyingå…³æ³¨å¤´ï¼ˆç§°ä¸ºâ€œç›¸é‚»å¤´â€ï¼‰ï¼Œè¿™äº›å…³æ³¨å¤´å‚ä¸æ‰¾åˆ°æœ‰æ•ˆçš„åç»­Tokenã€‚
</details></li>
</ul>
<hr>
<h2 id="Training-on-Synthetic-Data-Beats-Real-Data-in-Multimodal-Relation-Extraction"><a href="#Training-on-Synthetic-Data-Beats-Real-Data-in-Multimodal-Relation-Extraction" class="headerlink" title="Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction"></a>Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03025">http://arxiv.org/abs/2312.03025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilin Du, Haoxin Li, Xu Guo, Boyang Li</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¯¹å¤šmodalå…³ç³»æŠ½å–è¿›è¡Œç ”ç©¶ï¼Œä½†æ˜¯è¿›å±•å—åˆ°ç°æœ‰è®­ç»ƒæ•°æ®çš„ç¨€ç¼ºæ‰€é™ã€‚è¿™é‡Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªæ–°çš„é—®é¢˜è®¾å®šï¼Œå³ä»…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å•modalæ•°æ®ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬æˆ–å›¾åƒã€‚æˆ‘ä»¬æƒ³è¦ä»åˆæˆæ•°æ®ä¸­è®­ç»ƒä¸€ä¸ªå¤šmodalåˆ†ç±»å™¨ï¼Œå¹¶åœ¨çœŸå®å¤šmodalæµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºMI2RAGEçš„æ–¹æ³•ï¼Œå®ƒä½¿ç”¨äº†é“¾æ¥è·¨modalç”Ÿæˆï¼ˆCCGï¼‰æ¥æé«˜ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆç½‘ç»œé€‰æ‹©é«˜ç›¸äº’èµ„è®¯çš„è®­ç»ƒæ•°æ®ã€‚</li>
<li>results: ä¸ç›´æ¥åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒçš„æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å®ç°äº†24.06% F1çš„æå‡ï¼Œå°¤å…¶æ˜¯ä½¿ç”¨åˆæˆæ–‡æœ¬æ—¶çš„æå‡ä¸º30.42% F1ã€‚è€Œæˆ‘ä»¬æœ€ä½³çš„æ¨¡å‹ï¼Œå³å®Œå…¨ä½¿ç”¨åˆæˆå›¾åƒè¿›è¡Œè®­ç»ƒï¼Œå¯¹å…ˆå‰ç”±çœŸå®å¤šmodalæ•°æ®è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº†3.76%çš„F1æå‡ã€‚<details>
<summary>Abstract</summary>
The task of multimodal relation extraction has attracted significant research attention, but progress is constrained by the scarcity of available training data. One natural thought is to extend existing datasets with cross-modal generative models. In this paper, we consider a novel problem setting, where only unimodal data, either text or image, are available during training. We aim to train a multimodal classifier from synthetic data that perform well on real multimodal test data. However, training with synthetic data suffers from two obstacles: lack of data diversity and label information loss. To alleviate the issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to promote diversity in the generated data and exploits a teacher network to select valuable training samples with high mutual information with the ground-truth labels. Comparing our method to direct training on synthetic data, we observed a significant improvement of 24.06% F1 with synthetic text and 26.42% F1 with synthetic images. Notably, our best model trained on completely synthetic images outperforms prior state-of-the-art models trained on real multimodal data by a margin of 3.76% in F1. Our codebase will be made available upon acceptance.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤šModalå…³ç³»æå–ä»»åŠ¡å·²ç»å¸å¼•äº†å¤§é‡ç ”ç©¶è€…çš„å…³æ³¨ï¼Œä½†æ˜¯è¿›æ­¥å—åˆ°æ•°æ®ä¸è¶³çš„é™åˆ¶ã€‚ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯é€šè¿‡æ‰©å±•ç°æœ‰æ•°æ®é›†æ¥è¿›è¡Œè®­ç»ƒã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªæ–°çš„é—®é¢˜è®¾å®šï¼Œå³è®­ç»ƒæ—¶åªæœ‰å•æ¨¡æ€æ•°æ®ï¼Œå¯ä»¥æ˜¯æ–‡æœ¬æˆ–å›¾åƒã€‚æˆ‘ä»¬ç›®æ ‡æ˜¯ä»åˆæˆæ•°æ®ä¸­è®­ç»ƒä¸€ä¸ªå¤šModalåˆ†ç±»å™¨ï¼Œå¹¶åœ¨çœŸå®å¤šModalæµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ã€‚ä½†è®­ç»ƒåˆæˆæ•°æ®æ—¶å­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼šæ•°æ®å¤šæ ·æ€§ä¸å¤Ÿå’Œæ ‡ç­¾ä¿¡æ¯æŸå¤±ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ç›¸äº’ä¿¡æ¯æ„ŸçŸ¥å¤šModalè¿­ä»£æ•°æ®ç”Ÿæˆï¼ˆMI2RAGEï¼‰æ–¹æ³•ã€‚MI2RAGEæ–¹æ³•åˆ©ç”¨äº†é“¾å¼è·¨æ¨¡æ€ç”Ÿæˆï¼ˆCCGï¼‰æ¥æé«˜ç”Ÿæˆæ•°æ®çš„å¤šæ ·æ€§ï¼Œå¹¶åˆ©ç”¨æ•™å¸ˆç½‘ç»œé€‰æ‹©æœ‰é«˜ç›¸äº’ä¿¡æ¯çš„è®­ç»ƒæ ·æœ¬å’ŒçœŸå®æ ‡ç­¾ã€‚ä¸ç›´æ¥åœ¨åˆæˆæ•°æ®ä¸Šè®­ç»ƒç›¸æ¯”ï¼Œæˆ‘ä»¬å‘ç°MI2RAGEæ–¹æ³•å¯ä»¥æé«˜24.06%çš„F1å€¼ï¼Œå…¶ä¸­æ–‡æœ¬åˆæˆæ•°æ®ä¸Šæé«˜26.42%çš„F1å€¼ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹åœ¨ completelly syntheticå›¾åƒä¸Šè®­ç»ƒåï¼Œå¯ä»¥è¶…è¿‡ç°æœ‰çš„ estado-of-the-artæ¨¡å‹ï¼Œå³åœ¨çœŸå®å¤šModalæ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼ŒF1å€¼ä¸Šçš„æé«˜ä¸º3.76%ã€‚æˆ‘ä»¬å°†ä»£ç åº“åœ¨æ¥å—åå‘å¸ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="DanZero-Dominating-the-GuanDan-Game-through-Reinforcement-Learning"><a href="#DanZero-Dominating-the-GuanDan-Game-through-Reinforcement-Learning" class="headerlink" title="DanZero+: Dominating the GuanDan Game through Reinforcement Learning"></a>DanZero+: Dominating the GuanDan Game through Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02561">http://arxiv.org/abs/2312.02561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/submit-paper/Danzero_plus">https://github.com/submit-paper/Danzero_plus</a></li>
<li>paper_authors: Youpeng Zhao, Yudong Lu, Jian Zhao, Wengang Zhou, Houqiang Li</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç”¨äºç©å®¶DanZeroçš„äººå·¥æ™ºèƒ½ç¨‹åºï¼Œç”¨äºè§£å†³å¤æ‚çš„æ£‹ç‰Œæ¸¸æˆGuanDanã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†æ·±åº¦è’™ç‰¹å¡ç½—ï¼ˆDMCï¼‰å’Œåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œå¹¶åº”ç”¨äº†ç­–ç•¥åŸºäºåå°„å­¦ä¹ ç®—æ³•æ¥è¿›ä¸€æ­¥æé«˜äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ã€‚</li>
<li>results: è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸åŸºäºç»éªŒè§„åˆ™çš„AIç¨‹åºè¿›è¡Œæ¯”è¾ƒåï¼ŒDanZero Botè¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”é€šè¿‡é‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œå®ç°äº†äººå·¥æ™ºèƒ½çš„è¿›ä¸€æ­¥æé«˜ã€‚<details>
<summary>Abstract</summary>
The utilization of artificial intelligence (AI) in card games has been a well-explored subject within AI research for an extensive period. Recent advancements have propelled AI programs to showcase expertise in intricate card games such as Mahjong, DouDizhu, and Texas Hold'em. In this work, we aim to develop an AI program for an exceptionally complex and popular card game called GuanDan. This game involves four players engaging in both competitive and cooperative play throughout a long process to upgrade their level, posing great challenges for AI due to its expansive state and action space, long episode length, and complex rules. Employing reinforcement learning techniques, specifically Deep Monte Carlo (DMC), and a distributed training framework, we first put forward an AI program named DanZero for this game. Evaluation against baseline AI programs based on heuristic rules highlights the outstanding performance of our bot. Besides, in order to further enhance the AI's capabilities, we apply policy-based reinforcement learning algorithm to GuanDan. To address the challenges arising from the huge action space, which will significantly impact the performance of policy-based algorithms, we adopt the pre-trained model to facilitate the training process and the achieved AI program manages to achieve a superior performance.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰åœ¨ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¸Ğ³Ñ€Ğµå·²ç»æ˜¯é•¿æœŸçš„ç ”ç©¶ä¸»é¢˜ã€‚ç°ä»£æŠ€æœ¯çš„å‘å±•ä½¿å¾—AIç¨‹åºåœ¨å¤æ‚çš„ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹æ¸¸æˆå¦‚ Mahjongã€DouDizhu å’Œ Texas Hold'em ä¸­è¡¨ç°å‡ºäº†ä¸“å®¶æ°´å¹³ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç”¨äºExceptionally Complexå’Œå—æ¬¢è¿çš„ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹æ¸¸æˆå«åš GuanDanã€‚è¿™ä¸ªæ¸¸æˆéœ€è¦å››åç©å®¶åœ¨ç«äº‰å’Œåˆä½œä¹‹é—´è¿›è¡Œé•¿æœŸçš„è¿›ç¨‹ï¼Œä»¥å‡çº§è‡ªå·±çš„ç­‰çº§ï¼Œå¯¹ AI æå‡ºäº†å·¨å¤§çš„æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒçš„æ‰©å±•çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´éå¸¸å¤§ï¼Œæ¯ä¸€é›†çš„é•¿åº¦ä¹Ÿå¾ˆé•¿ï¼Œè§„åˆ™éå¸¸å¤æ‚ã€‚æˆ‘ä»¬ä½¿ç”¨äº† Deep Monte Carloï¼ˆDMCï¼‰ æŠ€æœ¯å’Œåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶ï¼Œé¦–å…ˆå¼€å‘äº†ä¸€ä¸ªåä¸º DanZero çš„ AI ç¨‹åºã€‚å¯¹åŸºäºè§„åˆ™çš„ AI ç¨‹åºè¿›è¡Œè¯„ä¼°æ˜¾ç¤ºäº†æˆ‘ä»¬çš„ bot çš„å‡ºè‰²è¡¨ç°ã€‚æ­¤å¤–ï¼Œä¸ºäº†è¿›ä¸€æ­¥æé«˜ AI çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åº”ç”¨äº†æ”¿ç­–åŸºäºè¿”ç‚¹å­¦ä¹ ç®—æ³•äº GuanDanã€‚ç”±äºåŠ¨ä½œç©ºé—´çš„å·¨å¤§æ€§ï¼Œä¼šå¯¹æ”¿ç­–åŸºäºç®—æ³•çš„æ€§èƒ½äº§ç”Ÿå¾ˆå¤§çš„å½±å“ï¼Œæˆ‘ä»¬é‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥ä¿ƒè¿›è®­ç»ƒè¿‡ç¨‹ï¼Œå¹¶å®ç°äº†ä¸€ä¸ªå¯ä»¥åœ¨ GuanDan ä¸­è¾¾åˆ°è¶…è¿‡å¸¸è§„æ°´å¹³çš„ AI ç¨‹åºã€‚
</details></li>
</ul>
<hr>
<h2 id="Beyond-Isolation-Multi-Agent-Synergy-for-Improving-Knowledge-Graph-Construction"><a href="#Beyond-Isolation-Multi-Agent-Synergy-for-Improving-Knowledge-Graph-Construction" class="headerlink" title="Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction"></a>Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03022">http://arxiv.org/abs/2312.03022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongbin Ye, Honghao Gui, Aijia Zhang, Tong Liu, Wei Hua, Weiqiang Jia</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æ¢è®¨çŸ¥è¯†å›¾æ„å»ºï¼ˆKGCï¼‰çš„å¤šæ–¹é¢é—®é¢˜ï¼ŒåŒ…æ‹¬å®ä½“ã€å…³ç³»å’Œäº‹ä»¶EXTRACTIONã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œå³CooperKGCï¼Œè¯¥æ¡†æ¶å»ºç«‹äº†ä¸€ä¸ªKGCåä½œå¤„ç†ç½‘ç»œï¼Œè®©ä¸åŒçš„ä»£ç†äººå…±åŒè§£å†³ENTITYã€å…³ç³»å’Œäº‹ä»¶EXTRACTIONä»»åŠ¡ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œé€šè¿‡CooperKGCçš„åä½œå¤„ç†ç½‘ç»œï¼Œå¯ä»¥åŒæ—¶è§£å†³ENTITYã€å…³ç³»å’Œäº‹ä»¶EXTRACTIONä»»åŠ¡ï¼Œå¹¶ä¸”åœ¨å¤šä¸ªäº¤äº’å¾ªç¯ä¸­ï¼Œåä½œä¿ƒè¿›äº†çŸ¥è¯†é€‰æ‹©ã€ä¿®æ­£å’Œèšåˆçš„èƒ½åŠ›ã€‚<details>
<summary>Abstract</summary>
Knowledge graph construction (KGC) is a multifaceted undertaking involving the extraction of entities, relations, and events. Traditionally, large language models (LLMs) have been viewed as solitary task-solving agents in this complex landscape. However, this paper challenges this paradigm by introducing a novel framework, CooperKGC. Departing from the conventional approach, CooperKGC establishes a collaborative processing network, assembling a KGC collaboration team capable of concurrently addressing entity, relation, and event extraction tasks. Our experiments unequivocally demonstrate that fostering collaboration and information interaction among diverse agents within CooperKGC yields superior results compared to individual cognitive processes operating in isolation. Importantly, our findings reveal that the collaboration facilitated by CooperKGC enhances knowledge selection, correction, and aggregation capabilities across multiple rounds of interactions.
</details>
<details>
<summary>æ‘˜è¦</summary>
çŸ¥è¯†å›¾æ„å»ºï¼ˆKGCï¼‰æ˜¯ä¸€é¡¹å¤šæ–¹é¢çš„ä»»åŠ¡ï¼Œæ¶‰åŠåˆ°å®ä½“ã€å…³ç³»å’Œäº‹ä»¶çš„æå–ã€‚ä¼ ç»Ÿä¸Šï¼Œå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¢«è§†ä¸ºå•ç‹¬çš„ä»»åŠ¡è§£å†³è€…åœ¨è¿™ä¸ªå¤æ‚çš„æ™¯è±¡ä¸­ã€‚ç„¶è€Œï¼Œè¿™ç¯‡è®ºæ–‡æŒ‘æˆ˜è¿™ä¸€è§‚å¿µï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶â€”â€”åˆä½œKGCã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼Œåˆä½œKGCå»ºç«‹äº†ä¸€ä¸ªçŸ¥è¯†å›¾æ„å»ºåä½œå›¢é˜Ÿï¼Œè´Ÿè´£åŒæ—¶å¤„ç†å®ä½“ã€å…³ç³»å’Œäº‹ä»¶æå–ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨åˆä½œKGCä¸­ä¿ƒè¿›å¤šç§ä»£ç†äººä¹‹é—´çš„åˆä½œå’Œä¿¡æ¯äº¤æ¢ï¼Œå¯ä»¥è®©çŸ¥è¯†é€‰æ‹©ã€ä¿®æ­£å’Œèšåˆèƒ½åŠ›åœ¨å¤šä¸ªäº¤äº’å¾ªç¯ä¸­å¾—åˆ°åŠ å¼ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Graph-Information-Bottleneck-for-Remote-Sensing-Segmentation"><a href="#Graph-Information-Bottleneck-for-Remote-Sensing-Segmentation" class="headerlink" title="Graph Information Bottleneck for Remote Sensing Segmentation"></a>Graph Information Bottleneck for Remote Sensing Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02545">http://arxiv.org/abs/2312.02545</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuntao Shou, Wei Ai, Tao Meng</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜é¥æ„Ÿå›¾åƒåˆ†å‰²çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯é¢å¯¹ä¸è§„åˆ™çš„ Ğ¾Ğ±ÑŠĞµĞº ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å›¾åƒä¸ºæ ¼ estructureï¼Œå¹¶å¼•å…¥äº†ç®€å•å¯¹æ¯”è§†è§‰Graph Neural Network (SC-ViG) æ¶æ„ï¼Œä»¥ä¾¿è‡ªé€‚åº”åœ°é€‰æ‹©èŠ‚ç‚¹å’Œè¾¹è¿›è¡Œæ©è”½ã€‚æ­¤å¤–ï¼Œæœ¬ç ”ç©¶è¿˜åº”ç”¨äº†ä¿¡æ¯ç“¶é¢ˆç†è®ºæ¥æœ€å¤§åŒ–ç›¸å…³ä»»åŠ¡çš„ä¿¡æ¯ï¼Œè€Œæœ€å°åŒ–ä¸ç›¸å…³ä»»åŠ¡çš„ä¿¡æ¯ã€‚</li>
<li>results: å¯¹äºå…¬å…±å¯ç”¨çš„å®éªŒæ•°æ®é›†ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨é¥æ„Ÿå›¾åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ä¸­å…·æœ‰æ›´é«˜çš„ç²¾åº¦å’Œæ•ˆç‡ï¼Œæ¯”ä¹‹å‰çš„çŠ¶æ€è‰ºæœ¯æ–¹æ³•æ›´é«˜ã€‚<details>
<summary>Abstract</summary>
Remote sensing segmentation has a wide range of applications in environmental protection, and urban change detection, etc. Despite the success of deep learning-based remote sensing segmentation methods (e.g., CNN and Transformer), they are not flexible enough to model irregular objects. In addition, existing graph contrastive learning methods usually adopt the way of maximizing mutual information to keep the node representations consistent between different graph views, which may cause the model to learn task-independent redundant information. To tackle the above problems, this paper treats images as graph structures and introduces a simple contrastive vision GNN (SC-ViG) architecture for remote sensing segmentation. Specifically, we construct a node-masked and edge-masked graph view to obtain an optimal graph structure representation, which can adaptively learn whether to mask nodes and edges. Furthermore, this paper innovatively introduces information bottleneck theory into graph contrastive learning to maximize task-related information while minimizing task-independent redundant information. Finally, we replace the convolutional module in UNet with the SC-ViG module to complete the segmentation and classification tasks of remote sensing images. Extensive experiments on publicly available real datasets demonstrate that our method outperforms state-of-the-art remote sensing image segmentation methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
remote sensing segmentation æœ‰å¹¿æ³›çš„åº”ç”¨åœ¨ç¯å¢ƒä¿æŠ¤å’ŒåŸå¸‚å˜åŒ–æ¢æµ‹ç­‰é¢†åŸŸã€‚ DESPITE æ·±åº¦å­¦ä¹ åŸºäº remote sensing segmentation æ–¹æ³•ï¼ˆä¾‹å¦‚ CNN å’Œ Transformerï¼‰çš„æˆåŠŸï¼Œå®ƒä»¬å¹¶ä¸å¤Ÿçµæ´»æ¥æ¨¡å‹ä¸ Regular çš„å¯¹è±¡ã€‚ æ­¤å¤–ï¼Œç°æœ‰çš„å›¾æ ‡å¯¹æ¯”å­¦ä¹ æ–¹æ³•é€šå¸¸é‡‡ç”¨ maximize mutual information çš„æ–¹æ³•ä¿æŒä¸åŒå›¾è§†å›¾çš„èŠ‚ç‚¹è¡¨ç¤ºç›¸åŒï¼Œè¿™å¯èƒ½ä¼šä½¿æ¨¡å‹å­¦ä¹ ä»»åŠ¡æ— å…³çš„å†—ä½™ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æ–‡å°†å›¾åƒè§†ä¸ºå›¾ç»“æ„ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç®€å•çš„å¯¹æ¯”è§†è§‰ Graph Neural Network (SC-ViG) æ¶æ„ Ğ´Ğ»Ñ remote sensing segmentationã€‚ Specifically, we construct a node-masked and edge-masked graph view to obtain an optimal graph structure representation, which can adaptively learn whether to mask nodes and edges. æ­¤å¤–ï¼Œæœ¬æ–‡åˆ›æ–°åœ°å°†ä¿¡æ¯ç“¶é¢ˆç†è®ºå¼•å…¥åˆ°å›¾æ ‡å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œä»¥æœ€å¤§åŒ–ç›¸å…³ä»»åŠ¡çš„ä¿¡æ¯ï¼Œè€Œæœ€å°åŒ–æ— å…³ä»»åŠ¡çš„å†—ä½™ä¿¡æ¯ã€‚æœ€åï¼Œæˆ‘ä»¬å°† UNet ä¸­çš„å·ç§¯æ¨¡å— replaced ä¸º SC-ViG æ¨¡å—ï¼Œä»¥å®Œæˆ remote sensing å›¾åƒåˆ†å‰²å’Œåˆ†ç±»ä»»åŠ¡ã€‚ EXTENSIVE å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨å…¬å¼€ available çš„å®éªŒæ•°æ®ä¸Šè¶…è¿‡äº†ç°æœ‰çš„ remote sensing å›¾åƒåˆ†å‰²æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="PolyFit-A-Peg-in-hole-Assembly-Framework-for-Unseen-Polygon-Shapes-via-Sim-to-real-Adaptation"><a href="#PolyFit-A-Peg-in-hole-Assembly-Framework-for-Unseen-Polygon-Shapes-via-Sim-to-real-Adaptation" class="headerlink" title="PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via Sim-to-real Adaptation"></a>PolyFit: A Peg-in-hole Assembly Framework for Unseen Polygon Shapes via Sim-to-real Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02531">http://arxiv.org/abs/2312.02531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geonhyup Lee, Joosoon Lee, Sangjun Noh, Minhwan Ko, Kangmin Kim, Kyoobin Lee</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ä¸ºäº†è§£å†³æœºå™¨äººç©¿å­” assemble ä¸­çš„åŸºç¡€å’ŒæŒ‘æˆ˜æ€§ä»»åŠ¡ï¼Œå³æ„ŸçŸ¥é”™è¯¯å’Œæœºæ¢°é”™è¯¯å¼•èµ·çš„æ’å…¥å¤±è´¥æˆ–å µå¡ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†ä¸€ç§è¶…vised learningæ–¹æ³•ï¼Œå³PolyFitï¼Œä»¥å‡å°‘æ„ŸçŸ¥é”™è¯¯å’Œæœºæ¢°é”™è¯¯çš„å½±å“ã€‚PolyFit ä½¿ç”¨äº†åŠ›çŸ©æ•°æ®è¿›è¡Œç²¾å‡†çš„å¤–éƒ¨poseä¼°è®¡ï¼Œå¹¶å°†ç£¨ç›˜poseè°ƒæ•´ä»¥çº æ­£åå·®ã€‚</li>
<li>results: è¯¥ç ”ç©¶åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è¿›è¡Œäº†å¹¿æ³›çš„è®­ç»ƒï¼Œä½¿ç”¨äº†åŒ…å«å¤šç§ç£¨ç›˜å­”å½¢çŠ¶ã€å¤–éƒ¨poseå’Œç›¸åº”çš„ContactåŠ›çŸ©æ•°æ®ã€‚åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­ï¼ŒPolyFit è¾¾åˆ°äº†97.3%å’Œ96.3%çš„ç£¨ç›˜æˆåŠŸç‡ï¼Œè€Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ƒè¾¾åˆ°äº†86.7%å’Œ85.0%çš„æˆåŠŸç‡ï¼Œè¿™è¡¨æ˜äº†è¯¥æ–¹æ³•çš„ç¨³å®šæ€§å’Œé€‚åº”æ€§ã€‚<details>
<summary>Abstract</summary>
The study addresses the foundational and challenging task of peg-in-hole assembly in robotics, where misalignments caused by sensor inaccuracies and mechanical errors often result in insertion failures or jamming. This research introduces PolyFit, representing a paradigm shift by transitioning from a reinforcement learning approach to a supervised learning methodology. PolyFit is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose estimation and adjusts the peg pose to rectify misalignments. Extensive training in a simulated environment involves a dataset encompassing a diverse range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T readings. To enhance extrinsic pose estimation, a multi-point contact strategy is integrated into the model input, recognizing that identical F/T readings can indicate different poses. The study proposes a sim-to-real adaptation method for real-world application, using a sim-real paired dataset to enable effective generalization to complex and unseen polygon shapes. PolyFit achieves impressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen shapes in simulations, respectively. Real-world evaluations further demonstrate substantial success rates of 86.7% and 85.0%, highlighting the robustness and adaptability of the proposed method.
</details>
<details>
<summary>æ‘˜è¦</summary>
Simplified Chinese:è¿™é¡¹ç ”ç©¶targetsç‘é¼Roboticsä¸­çš„éš¾é¢˜ï¼šæ¯å­åœ¨å­”ä¸­Assembly, where misalignments caused by sensor inaccuracies and mechanical errors often result in insertion failures or jamming. This research introduces PolyFit, representing a paradigm shift by transitioning from a reinforcement learning approach to a supervised learning methodology. PolyFit is a Force/Torque (F/T)-based supervised learning framework designed for 5-DoF peg-in-hole assembly. It utilizes F/T data for accurate extrinsic pose estimation and adjusts the peg pose to rectify misalignments. Extensive training in a simulated environment involves a dataset encompassing a diverse range of peg-hole shapes, extrinsic poses, and their corresponding contact F/T readings. To enhance extrinsic pose estimation, a multi-point contact strategy is integrated into the model input, recognizing that identical F/T readings can indicate different poses. The study proposes a sim-to-real adaptation method for real-world application, using a sim-real paired dataset to enable effective generalization to complex and unseen polygon shapes. PolyFit achieves impressive peg-in-hole success rates of 97.3% and 96.3% for seen and unseen shapes in simulations, respectively. Real-world evaluations further demonstrate substantial success rates of 86.7% and 85.0%, highlighting the robustness and adaptability of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="MEMTO-Memory-guided-Transformer-for-Multivariate-Time-Series-Anomaly-Detection"><a href="#MEMTO-Memory-guided-Transformer-for-Multivariate-Time-Series-Anomaly-Detection" class="headerlink" title="MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection"></a>MEMTO: Memory-guided Transformer for Multivariate Time Series Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02530">http://arxiv.org/abs/2312.02530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gunny97/MEMTO">https://github.com/gunny97/MEMTO</a></li>
<li>paper_authors: Junho Song, Keonwoo Kim, Jeonglyul Oh, Sungzoon Cho</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§åŸºäºåµŒå…¥å¼Transformerçš„è®°å¿†å¯¼å‘çš„å¼‚å¸¸æ£€æµ‹æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨å®é™…ä¸–ç•Œå¤šå˜æ•°æ—¶é—´åºåˆ—èµ„æ–™ä¸Šæ£€æµ‹å¼‚å¸¸ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åµŒå…¥å¼Transformeræ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸ä¸€ä¸ªæ–°çš„è®°å¿†æ¨¡ç»„ç»“åˆåœ¨ä¸€èµ·ï¼Œä»¥å­¦ä¹ å¯¹è¾“å…¥æ•°æ®çš„åº”å¯¹ç­–ç•¥ã€‚æ­¤å¤–ï¼Œè¿™ç¯‡è®ºæ–‡è¿˜ä½¿ç”¨äº†K-means clusteringæ¥åˆå§‹åŒ–è®°å¿†é¡¹ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡åœ¨äº”ä¸ªçœŸå®ä¸–ç•Œçš„å¤šå˜æ•°æ—¶é—´åºåˆ—èµ„æ–™ä¸Šè¿›è¡Œäº†å®é™…æµ‹è¯•ï¼Œä»¥åŠå¯¹å…ˆå‰çš„å·vectoræ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚ç»“æœæ˜¾ç¤ºï¼Œè¿™ç¯‡è®ºæ–‡çš„æå‡ºçš„æ–¹æ³•åœ¨è¿™äº›æ•°æ®ä¸Šå–å¾—äº†å¹³å‡å¼‚å¸¸æ£€æµ‹F1åˆ†æ•°95.74%ï¼Œè¾ƒå…ˆå‰çš„æ–¹æ³•æœ‰æ‰€æ”¹å–„ã€‚<details>
<summary>Abstract</summary>
Detecting anomalies in real-world multivariate time series data is challenging due to complex temporal dependencies and inter-variable correlations. Recently, reconstruction-based deep models have been widely used to solve the problem. However, these methods still suffer from an over-generalization issue and fail to deliver consistently high performance. To address this issue, we propose the MEMTO, a memory-guided Transformer using a reconstruction-based approach. It is designed to incorporate a novel memory module that can learn the degree to which each memory item should be updated in response to the input data. To stabilize the training procedure, we use a two-phase training paradigm which involves using K-means clustering for initializing memory items. Additionally, we introduce a bi-dimensional deviation-based detection criterion that calculates anomaly scores considering both input space and latent space. We evaluate our proposed method on five real-world datasets from diverse domains, and it achieves an average anomaly detection F1-score of 95.74%, significantly outperforming the previous state-of-the-art methods. We also conduct extensive experiments to empirically validate the effectiveness of our proposed model's key components.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ£€æµ‹å®é™…ä¸–ç•Œå¤šå˜é‡æ—¶é—´åºåˆ—æ•°æ®ä¸­å¼‚å¸¸ç°è±¡æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨å¤æ‚çš„æ—¶é—´å…³ç³»å’Œå˜é‡ç›¸å…³æ€§ã€‚è¿‘å¹´æ¥ï¼Œä½¿ç”¨æ·±åº¦æ¨¡å‹è¿›è¡Œé‡å»ºæ–¹æ³•å·²ç»å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•ä»ç„¶å—åˆ°è¿‡æ³›åŒ–é—®é¢˜çš„å›°æ‰°ï¼Œæ— æ³•æŒç»­æä¾›é«˜æ€§èƒ½ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®MEMTOï¼Œä¸€ç§å…·æœ‰è®°å¿†å¯¼èˆªçš„è½¬ç§»å­¦ä¹ æ¨¡å‹ã€‚å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°çš„è®°å¿†æ¨¡å—ï¼Œå¯ä»¥æ ¹æ®è¾“å…¥æ•°æ®æ¥å­¦ä¹ æ¯ä¸ªè®°å¿†é¡¹çš„æ›´æ–°åº¦ã€‚ä¸ºç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œå…¶ä¸­åŒ…æ‹¬ä½¿ç”¨K-meanså½’ä¸€åŒ– clusteringæ¥åˆå§‹åŒ–è®°å¿†é¡¹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸¤ç»´åå·®åŸºäºçš„æ£€æµ‹æ ‡å‡†ï¼Œå¯ä»¥è€ƒè™‘è¾“å…¥ç©ºé—´å’Œéšè—ç©ºé—´çš„åå·®ã€‚æˆ‘ä»¬å¯¹äº”ä¸ªä¸åŒé¢†åŸŸçš„å®é™…æ•°æ®è¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶è¾¾åˆ°äº†95.74%çš„å¼‚å¸¸æ£€æµ‹F1åˆ†æ•°ï¼Œé«˜äºä¹‹å‰çš„çŠ¶æ€ Ğ¸ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒæ¥è¯æ˜æˆ‘ä»¬æè®®çš„æ¨¡å‹çš„å…³é”®ç»„ä»¶çš„æœ‰æ•ˆæ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="MASP-Scalable-GNN-based-Planning-for-Multi-Agent-Navigation"><a href="#MASP-Scalable-GNN-based-Planning-for-Multi-Agent-Navigation" class="headerlink" title="MASP: Scalable GNN-based Planning for Multi-Agent Navigation"></a>MASP: Scalable GNN-based Planning for Multi-Agent Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02522">http://arxiv.org/abs/2312.02522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyi Yang, Xinting Yang, Chao Yu, Jiayu Chen, Huazhong Yang, Yu Wang<br>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³å¤š Agent ååŒå¯¼èˆªä»»åŠ¡ï¼Œå³å¤šä¸ª Agent éœ€è¦åœ¨æœ‰é™æ—¶é—´å†…è¾¾åˆ°åˆå§‹æœªåˆ†é…ç›®æ ‡ã€‚methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å¢å¼ºå­¦ä¹ ï¼ˆRLï¼‰å’Œå±‚æ¬¡æœç´¢ç»“æ„æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¥æ¨¡å‹å¤š Agent å’Œç›®æ ‡ä¹‹é—´çš„äº¤äº’ã€‚results: è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMASP æ¯” ĞºĞ»Ğ°ÑÑĞ¸å‹çš„è§„åˆ’æ–¹æ³•å’Œ RL åŸºç¡€æ–¹æ³•é«˜æ•ˆï¼Œåœ¨å¤š Agent ç²’å­ç¯å¢ƒï¼ˆMPEï¼‰ä¸­è¾¾åˆ°äº†nearly 100% æˆåŠŸç‡ï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„å›¢é˜Ÿå¤§å°ä¸‹è¿›è¡Œé›¶ä¾‹å¤–æƒ…å†µæŒæ¡ã€‚<details>
<summary>Abstract</summary>
We investigate the problem of decentralized multi-agent navigation tasks, where multiple agents need to reach initially unassigned targets in a limited time. Classical planning-based methods suffer from expensive computation overhead at each step and offer limited expressiveness for complex cooperation strategies. In contrast, reinforcement learning (RL) has recently become a popular paradigm for addressing this issue. However, RL struggles with low data efficiency and cooperation when directly exploring (nearly) optimal policies in the large search space, especially with an increased agent number (e.g., 10+ agents) or in complex environments (e.g., 3D simulators). In this paper, we propose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned hierarchical planner for navigation tasks with a substantial number of agents. MASP adopts a hierarchical framework to divide a large search space into multiple smaller spaces, thereby reducing the space complexity and accelerating training convergence. We also leverage graph neural networks (GNN) to model the interaction between agents and goals, improving goal achievement. Besides, to enhance generalization capabilities in scenarios with unseen team sizes, we divide agents into multiple groups, each with a previously trained number of agents. The results demonstrate that MASP outperforms classical planning-based competitors and RL baselines, achieving a nearly 100% success rate with minimal training data in both multi-agent particle environments (MPE) with 50 agents and a quadrotor 3-dimensional environment (OmniDrones) with 20 agents. Furthermore, the learned policy showcases zero-shot generalization across unseen team sizes.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶äº†åˆ†æ•£å¼å¤šagger naviagtionä»»åŠ¡ï¼Œå…¶ä¸­å¤šä¸ª Agentéœ€è¦åœ¨æœ‰é™æ—¶é—´å†…åˆ°è¾¾åˆå§‹ä¸çŸ¥é“çš„ç›®æ ‡ã€‚å¤å…¸è§‚å¿µç³»ç»Ÿæ–¹æ³•å—åˆ°æ¯æ­¥computational overheadçš„é«˜æˆæœ¬å’Œæœ‰é™çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¯¼è‡´åœ¨å¤æ‚çš„åˆä½œç­–ç•¥ä¸‹é‡åˆ°å›°éš¾ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä» reward learningï¼ˆRLï¼‰çš„è§’åº¦æ¥çœ‹ï¼Œå®ƒåœ¨æœ€è¿‘å‡ å¹´å†…å·²ç»æˆä¸ºå¤„ç†æ­¤ç±»ä»»åŠ¡çš„å—æ¬¢è¿æ–¹æ³•ã€‚ç„¶è€Œï¼ŒRLåœ¨å¯»æ‰¾ï¼ˆè¿‘ä¹ï¼‰ä¼˜è´¨ç­–ç•¥æ—¶å—åˆ°ä½æ•ˆç‡çš„èµ„æ–™å’ŒååŠ›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨ Agent çš„æ•°é‡å¢åŠ ï¼ˆä¾‹å¦‚ 10 åª Agent æˆ–æ›´å¤šï¼‰æˆ–åœ¨å¤æ‚çš„ç¯å¢ƒä¸­ï¼ˆä¾‹å¦‚ 3D æ¨¡æ‹Ÿå™¨ï¼‰ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰å¤šä¸ª Agent çš„å¯¹è¯ GNN åŸºäº plannerï¼ˆMASPï¼‰ï¼Œç”¨äº navigate ä»»åŠ¡ã€‚MASP è¿ç”¨äº†å±‚æ¬¡æ¶æ„æ¥åˆ†è§£å¤§çš„æœå¯»ç©ºé—´ï¼Œå› æ­¤å‡å°‘äº†ç©ºé—´å¤æ‚åº¦å’ŒåŠ é€Ÿäº†è®­ç»ƒçš„æ­¥éª¤ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åˆ©ç”¨å›¾ neural networkï¼ˆGNNï¼‰æ¥odel agent å’Œç›®æ ‡ä¹‹é—´çš„äº’åŠ¨ï¼Œæé«˜äº†ç›®æ ‡å®ç°ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢å¼ºæœªè§åˆ°çš„å›¢é˜Ÿå¤§å°çš„é€šç”¨èƒ½åŠ›ï¼Œæˆ‘ä»¬å°† Agent åˆ†ä¸ºå¤šä¸ªå°ç»„ï¼Œæ¯ä¸ªå°ç»„éƒ½æœ‰å…ˆå‰è®­ç»ƒçš„ Agent æ•°é‡ã€‚ç»“æœæ˜¾ç¤ºï¼ŒMASP æ¯”å¤å…¸è§‚å¿µç³»ç»Ÿæ–¹æ³•å’Œ RL åŸºeline é«˜ï¼Œåœ¨ MPE ä¸­çš„ 50 åª Agent å’Œ OmniDrones ä¸­çš„ 20 åª Agent è·å¾—äº†æ¥è¿‘ 100% çš„æˆåŠŸç‡ï¼Œå¹¶ä¸”å­¦ä¹ çš„æ”¿ç­–å±•ç°äº†é›¶shotæ³›åŒ–æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Retrieving-Conditions-from-Reference-Images-for-Diffusion-Models"><a href="#Retrieving-Conditions-from-Reference-Images-for-Diffusion-Models" class="headerlink" title="Retrieving Conditions from Reference Images for Diffusion Models"></a>Retrieving Conditions from Reference Images for Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02521">http://arxiv.org/abs/2312.02521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Tang, Xin Zhou, Jieren Deng, Zhihong Pan, Hao Tian, Pratik Chaudhari</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜ç”Ÿæˆå›¾åƒçš„å¤šæ ·æ€§ï¼Œä»¥æ»¡è¶³æ›´å¤šåº”ç”¨éœ€æ±‚ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† diffusion-based ä¸»é¢˜é©±åŠ¨çš„ç”Ÿæˆæ–¹æ³•ï¼Œå¹¶å¼•å…¥äº†æ›´åŠ ç²¾ç¡®çš„æ ‡ç­¾æ•°æ®é›† RetriBooru-V1ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜æå‡ºäº†æ–°çš„ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„å¤šæ ·åº¦åº¦é‡æ¥è¡¡é‡ç”Ÿæˆå›¾åƒçš„æˆåŠŸç¨‹åº¦ã€‚åŸºäº RAI-inspired æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜è¿˜å®ç°äº†å¯¹å‚ç…§å›¾åƒä¸­çš„ç²¾ç¡®ä¿¡æ¯çš„é‡æ–° Retrievalã€‚<details>
<summary>Abstract</summary>
Recent diffusion-based subject driven generative methods have enabled image generations with good fidelity for specific objects or human portraits. However, to achieve better versatility for applications, we argue that not only improved datasets and evaluations are desired, but also more careful methods to retrieve only relevant information from conditional images are anticipated. To this end, we propose an anime figures dataset RetriBooru-V1, with enhanced identity and clothing labels. We state new tasks enabled by this dataset, and introduce a new diversity metric to measure success in completing these tasks, quantifying the flexibility of image generations. We establish an RAG-inspired baseline method, designed to retrieve precise conditional information from reference images. Then, we compare with current methods on existing task to demonstrate the capability of the proposed method. Finally, we provide baseline experiment results on new tasks, and conduct ablation studies on the possible structural choices.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese:è¿‘æœŸçš„æ‰©æ•£åŸºäºä¸»é¢˜é©±åŠ¨çš„ç”Ÿæˆæ–¹æ³•å·²ç»å®ç°äº†å¯¹ç‰¹å®šå¯¹è±¡æˆ–äººè„¸çš„å›¾åƒç”Ÿæˆ avec è‰¯å¥½çš„å‡†ç¡®æ€§ã€‚ç„¶è€Œï¼Œä¸ºäº†å®ç°æ›´å¥½çš„å¤šæ ·æ€§ï¼Œæˆ‘ä»¬è®¤ä¸ºä¸ä»…éœ€è¦æ”¹è¿›çš„æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ï¼Œè¿˜éœ€è¦æ›´åŠ å°å¿ƒåœ°ä»æ¡ä»¶å›¾åƒä¸­æå–ç›¸å…³ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºRetriBooru-V1çš„æ¼«ç”»äººç‰©æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†å¢å¼ºçš„èº«ä»½å’Œæœè£…æ ‡ç­¾ã€‚æˆ‘ä»¬æå‡ºäº†åŸºäºè¿™ä¸ªæ•°æ®é›†çš„æ–°ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ä¸ªæ–°çš„å¤šæ ·æ€§æŒ‡æ ‡æ¥è¡¡é‡è¿™äº›ä»»åŠ¡çš„æˆåŠŸç¨‹åº¦ï¼Œé‡åŒ–å›¾åƒç”Ÿæˆçš„çµæ´»æ€§ã€‚æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªåŸºäºRAGçš„åŸºçº¿æ–¹æ³•ï¼Œç”¨äºä»å‚è€ƒå›¾åƒä¸­æå–å‡†ç¡®çš„æ¡ä»¶ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸ç°æœ‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒï¼Œä»¥ç¤ºå‡ºæˆ‘ä»¬çš„æ–¹æ³•çš„èƒ½åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬æä¾›äº†åŸºelineå®éªŒç»“æœï¼Œå¹¶è¿›è¡Œäº†å¯èƒ½çš„ç»“æ„é€‰æ‹©çš„ablation studyã€‚
</details></li>
</ul>
<hr>
<h2 id="Creative-Agents-Empowering-Agents-with-Imagination-for-Creative-Tasks"><a href="#Creative-Agents-Empowering-Agents-with-Imagination-for-Creative-Tasks" class="headerlink" title="Creative Agents: Empowering Agents with Imagination for Creative Tasks"></a>Creative Agents: Empowering Agents with Imagination for Creative Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02519">http://arxiv.org/abs/2312.02519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pku-rl/creative-agents">https://github.com/pku-rl/creative-agents</a></li>
<li>paper_authors: Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å»ºç«‹å…·æœ‰åˆ›é€ åŠ›çš„æ™ºèƒ½ä»£ç†äººï¼Œä»¥æ‰§è¡Œå¼€æ”¾å¼åˆ›ä½œä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•å»ºç«‹äº†å¤šæ ·åŒ–çš„å¼€æ”¾å¼ä»»åŠ¡å®Œæˆè€…ï¼Œä½†None of them Demonstrates creativityã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºä¸€ç±»è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­æ§åˆ¶å™¨é€šè¿‡å¢å¼º imagination æ¥è½¬åŒ–æŠ½è±¡è¯­è¨€æŒ‡ä»¤ä¸ºå…·ä½“ç¯å¢ƒä¸­çš„ä»»åŠ¡ç›®æ ‡ã€‚æˆ‘ä»¬å¼•å…¥äº†å¤šç§å®ç°åˆ›æ„ä»£ç†äººçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹æˆ–æ‰©æ•£æ¨¡å‹æ¥å®ç°å›¾åƒæƒ³è±¡ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨ Minecraft æ¸¸æˆä¸­è¿›è¡Œäº†è¯¦ç»†çš„å®éªŒåˆ†æï¼Œæ˜¾ç¤ºåˆ›æ„ä»£ç†äººå¯ä»¥åœ¨ survival æ¨¡å¼ä¸­åˆ›é€ å¤šæ ·åŒ–çš„å»ºç­‘ç‰©ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€äº›æ–°çš„è¯„ä»·æŒ‡æ ‡ï¼Œå¯ä»¥æ›´å¥½åœ°è¯„ä¼°å¼€æ”¾å¼åˆ›ä½œä»»åŠ¡ä¸­çš„ AI ä»£ç†äººã€‚<details>
<summary>Abstract</summary>
We study building embodied agents for open-ended creative tasks. While existing methods build instruction-following agents that can perform diverse open-ended tasks, none of them demonstrates creativity -- the ability to give novel and diverse task solutions implicit in the language instructions. This limitation comes from their inability to convert abstract language instructions into concrete task goals in the environment and perform long-horizon planning for such complicated goals. Given the observation that humans perform creative tasks with the help of imagination, we propose a class of solutions for creative agents, where the controller is enhanced with an imaginator that generates detailed imaginations of task outcomes conditioned on language instructions. We introduce several approaches to implementing the components of creative agents. We implement the imaginator with either a large language model for textual imagination or a diffusion model for visual imagination. The controller can either be a behavior-cloning policy learned from data or a pre-trained foundation model generating executable codes in the environment. We benchmark creative tasks with the challenging open-world game Minecraft, where the agents are asked to create diverse buildings given free-form language instructions. In addition, we propose novel evaluation metrics for open-ended creative tasks utilizing GPT-4V, which holds many advantages over existing metrics. We perform a detailed experimental analysis of creative agents, showing that creative agents are the first AI agents accomplishing diverse building creation in the survival mode of Minecraft. Our benchmark and models are open-source for future research on creative agents (https://github.com/PKU-RL/Creative-Agents).
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬ç ”ç©¶å»ºç«‹å…·æœ‰å¼€æ”¾æ€§çš„æ™ºèƒ½ä»£ç†äººï¼Œä»¥å®Œæˆåˆ›æ–°æ€§çš„ä»»åŠ¡ã€‚ç°æœ‰æ–¹æ³•å»ºç«‹äº†éµå¾ªæŒ‡ä»¤çš„æ™ºèƒ½ä»£ç†äººï¼Œä½†è¿™äº›æ–¹æ³•éƒ½æ²¡æœ‰è¡¨ç°å‡ºåˆ›æ–°åŠ›â€”â€”èƒ½å¤Ÿæä¾›æœªçŸ¥å’Œå¤šæ ·åŒ–çš„ä»»åŠ¡è§£å†³æ–¹æ¡ˆã€‚è¿™ä¸€é™åˆ¶æ¥è‡ªäºå®ƒä»¬æ— æ³•å°†æŠ½è±¡çš„è¯­è¨€æŒ‡ä»¤è½¬æ¢ä¸ºç¯å¢ƒä¸­çš„å…·ä½“ä»»åŠ¡ç›®æ ‡ï¼Œå¹¶è¿›è¡Œé•¿æœŸè§„åˆ’ã€‚äººç±»åœ¨åˆ›ä½œä»»åŠ¡æ—¶é€šå¸¸é‡‡ç”¨æƒ³è±¡åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­æ§åˆ¶å™¨è¢«å¢å¼ºä¸ºå…·æœ‰æƒ³è±¡åŠ›çš„ imagine æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®è¯­è¨€æŒ‡ä»¤ç”Ÿæˆè¯¦ç»†çš„æƒ³è±¡ç»“æœã€‚æˆ‘ä»¬ä»‹ç»äº†å‡ ç§å®ç°Componentsof Creative Agentsçš„æ–¹æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æˆ–æ‰©æ•£æ¨¡å‹æ¥å®ç° imagine æ¨¡å‹ï¼Œè€Œæ§åˆ¶å™¨å¯ä»¥æ˜¯åŸºäºæ•°æ®å­¦ä¹ çš„è¡Œä¸ºåšcloneç­–ç•¥æˆ–è€…æ˜¯ç”Ÿæˆå¯æ‰§è¡Œä»£ç çš„é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ Minecraft æ¸¸æˆè¿›è¡Œåˆ›å»ºå¤šæ ·åŒ–å»ºç­‘ä»»åŠ¡çš„ benchmarkingï¼Œå¹¶æå‡ºäº†ä¸€äº›æ–°çš„è¯„ä»·æŒ‡æ ‡æ¥è¯„ä¼°å¼€æ”¾æ€§åˆ›æ–°ä»»åŠ¡ã€‚æˆ‘ä»¬çš„å®éªŒåˆ†æè¡¨æ˜ï¼Œåˆ›é€ ä»£ç†äººæ˜¯åœ¨ Minecraft æ¸¸æˆçš„å­˜æ´»æ¨¡å¼ä¸­é¦–æ¬¡å®Œæˆå¤šæ ·åŒ–å»ºç­‘ä»»åŠ¡çš„ AI ä»£ç†äººã€‚æˆ‘ä»¬çš„ benchmark å’Œæ¨¡å‹æ˜¯å¼€æºçš„ï¼Œä»¥ä¾¿æœªæ¥çš„ç ”ç©¶åˆ›æ–°ä»£ç†äººï¼ˆhttps://github.com/PKU-RL/Creative-Agentsï¼‰ã€‚
</details></li>
</ul>
<hr>
<h2 id="Simplifying-Neural-Network-Training-Under-Class-Imbalance"><a href="#Simplifying-Neural-Network-Training-Under-Class-Imbalance" class="headerlink" title="Simplifying Neural Network Training Under Class Imbalance"></a>Simplifying Neural Network Training Under Class Imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02517">http://arxiv.org/abs/2312.02517</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ravidziv/simplifyingimbalancedtraining">https://github.com/ravidziv/simplifyingimbalancedtraining</a></li>
<li>paper_authors: Ravid Shwartz-Ziv, Micah Goldblum, Yucen Lily Li, C. Bayan Bruss, Andrew Gordon Wilson</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¦‚ä½•åœ¨å®é™…æ•°æ®é›†ä¸Šä½¿ç”¨æ ‡å‡†æ·±åº¦å­¦ä¹ ç®¡é“ä¸­çš„ç»„ä»¶æ¥æé«˜å¯¹ç±»ååº¦é—®é¢˜çš„æ€§èƒ½ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç°æœ‰çš„æ·±åº¦å­¦ä¹ ç®¡é“ä¸­çš„å„ç§ç»„ä»¶ï¼ŒåŒ…æ‹¬æ‰¹å¤„ç†å¤§å°ã€æ•°æ®å¢å¼ºã€ä¼˜åŒ–å™¨å’Œæ ‡ç­¾å¹³æ»‘ï¼Œå¹¶è°ƒæ•´è¿™äº›ç»„ä»¶æ¥é€‚åº”ç±»ååº¦é—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è°ƒæ•´æ ‡å‡†æ·±åº¦å­¦ä¹ ç®¡é“ä¸­çš„å„ç§ç»„ä»¶ï¼Œå¯ä»¥è¾¾åˆ°ç±»ååº¦é—®é¢˜çš„å·å‰æ€§èƒ½ï¼Œè€Œæ— éœ€ä½¿ç”¨ç‰¹æ®Šçš„ç±»ååº¦æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Real-world datasets are often highly class-imbalanced, which can adversely impact the performance of deep learning models. The majority of research on training neural networks under class imbalance has focused on specialized loss functions, sampling techniques, or two-stage training procedures. Notably, we demonstrate that simply tuning existing components of standard deep learning pipelines, such as the batch size, data augmentation, optimizer, and label smoothing, can achieve state-of-the-art performance without any such specialized class imbalance methods. We also provide key prescriptions and considerations for training under class imbalance, and an understanding of why imbalance methods succeed or fail.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="ASPEN-High-Throughput-LoRA-Fine-Tuning-of-Large-Language-Models-with-a-Single-GPU"><a href="#ASPEN-High-Throughput-LoRA-Fine-Tuning-of-Large-Language-Models-with-a-Single-GPU" class="headerlink" title="ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU"></a>ASPEN: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02515">http://arxiv.org/abs/2312.02515</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TUDB-Labs/multi-lora-fine-tune">https://github.com/TUDB-Labs/multi-lora-fine-tune</a></li>
<li>paper_authors: Zhengmao Ye, Dengchun Li, Jingqi Tian, Tingfeng Lan, Jie Zuo, Lei Duan, Hui Lu, Yexi Jiang, Jian Sha, Ke Zhang, Mingjie Tang</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æé«˜å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†å™¨ï¼ˆLLMï¼‰çš„ fine-tuning æ•ˆç‡ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šä»»åŠ¡ concurrent fine-tuning ä¸­ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨ Low-Rank Adaptationï¼ˆLoRAï¼‰æ–¹æ³•ï¼Œé€šè¿‡å…±äº«é¢„è®­ç»ƒæ¨¡å‹å’Œ adaptive è°ƒåº¦ï¼Œå®ç°é«˜æ•ˆåœ°åœ¨å•ä¸ª GPU ä¸Šè¿›è¡Œå¤šä»»åŠ¡ fine-tuningã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨ ASPEN æ¡†æ¶å¯ä»¥èŠ‚çœ GPU å†…å­˜é‡ä¸º 53%ï¼Œå¹¶æé«˜è®­ç»ƒ Throughput çº¦ 17%ï¼Œç›¸æ¯”ç°æœ‰æ–¹æ³•ã€‚å¦å¤–ï¼Œé€‚åº”è°ƒåº¦ç®—æ³•å¯ä»¥å‡å°‘ç»ƒä¹ å¾ªç¯æ—¶é—´é‡ä¸º 24%ï¼Œç»“æŸåˆ°ç»“æŸè®­ç»ƒå»¶è¿Ÿé‡ä¸º 12%ã€‚<details>
<summary>Abstract</summary>
Transformer-based large language models (LLMs) have demonstrated outstanding performance across diverse domains, particularly when fine-turned for specific domains. Recent studies suggest that the resources required for fine-tuning LLMs can be economized through parameter-efficient methods such as Low-Rank Adaptation (LoRA). While LoRA effectively reduces computational burdens and resource demands, it currently supports only a single-job fine-tuning setup.   In this paper, we present ASPEN, a high-throughput framework for fine-tuning LLMs. ASPEN efficiently trains multiple jobs on a single GPU using the LoRA method, leveraging shared pre-trained model and adaptive scheduling. ASPEN is compatible with transformer-based language models like LLaMA and ChatGLM, etc. Experiments show that ASPEN saves 53% of GPU memory when training multiple LLaMA-7B models on NVIDIA A100 80GB GPU and boosts training throughput by about 17% compared to existing methods when training with various pre-trained models on different GPUs. The adaptive scheduling algorithm reduces turnaround time by 24%, end-to-end training latency by 12%, prioritizing jobs and preventing out-of-memory issues.
</details>
<details>
<summary>æ‘˜è¦</summary>
transformer-basedå¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å¤šä¸ªé¢†åŸŸéƒ½å±•ç°å‡ºæ°å‡ºçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ç‰¹å®šé¢†åŸŸ Fine-tuning æ—¶ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œç”¨äº Fine-tuning LLM çš„èµ„æºå¯ä»¥é€šè¿‡ parameter-efficient æ–¹æ³•such as Low-Rank Adaptation (LoRA) å‡å°‘è®¡ç®—å·ç§¯å’Œèµ„æºéœ€æ±‚ã€‚è€Œ LoRA ç›®å‰åªæ”¯æŒå•ä¸ªä»»åŠ¡ Fine-tuning  setupã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç» ASPENï¼Œä¸€ä¸ªé«˜é€šè¿‡put framework for Fine-tuning LLMã€‚ ASPEN ä½¿ç”¨ LoRA æ–¹æ³•åœ¨å•ä¸ª GPU ä¸Šé«˜æ•ˆåœ°è®­ç»ƒå¤šä¸ªä»»åŠ¡ï¼Œåˆ©ç”¨å…±äº«é¢„è®­ç»ƒæ¨¡å‹å’Œè‡ªé€‚åº”è°ƒåº¦ã€‚ ASPEN ä¸ transformer-based è¯­è¨€æ¨¡å‹å¦‚ LLaMA å’Œ ChatGLM ç­‰å…¼å®¹ã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ NVIDIA A100 80GB GPU ä¸Šè®­ç»ƒå¤šä¸ª LLaMA-7B æ¨¡å‹æ—¶ï¼ŒASPEN å¯ä»¥ saving 53% GPU å†…å­˜å’Œboost è®­ç»ƒé€Ÿåº¦çº¦17% compared to existing methodsã€‚é€‚åº”è°ƒåº¦ç®—æ³•å¯ä»¥ reducing turnaround time by 24%, end-to-end training latency by 12%, prioritizing jobs and preventing out-of-memory issuesã€‚
</details></li>
</ul>
<hr>
<h2 id="AV2AV-Direct-Audio-Visual-Speech-to-Audio-Visual-Speech-Translation-with-Unified-Audio-Visual-Speech-Representation"><a href="#AV2AV-Direct-Audio-Visual-Speech-to-Audio-Visual-Speech-Translation-with-Unified-Audio-Visual-Speech-Representation" class="headerlink" title="AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation"></a>AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02512">http://arxiv.org/abs/2312.02512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro</li>
<li>for: è¿™ paper æå‡ºäº†ä¸€ç§ç›´æ¥å°† audio-visual è¯­éŸ³ç¿»è¯‘æˆ audio-visual è¯­éŸ³çš„æ¡†æ¶ (AV2AV)ï¼Œä»¥ä¾¿å®ç°çœŸå®çš„è·¨å›½è™šæ‹Ÿä¼šè®®ï¼Œå¹¶ä¸”èƒ½å¤ŸåŒæ—¶æ˜¾ç¤ºåŒæ­¥çš„å˜´å”‡è¿åŠ¨ã€‚</li>
<li>methods: è¯¥æ¡†æ¶ä½¿ç”¨äº†è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨å­¦ä¹ æ¥å­¦ä¹  audio-visual è¯­éŸ³çš„è¡¨ç¤ºã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒAV2AV å¯ä»¥åœ¨å¤šç§è¯­è¨€ç¿»è¯‘ä»»åŠ¡ä¸­æä¾›é«˜æ•ˆçš„ç¿»è¯‘ç»“æœï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„è¯­è¨€ç¯å¢ƒä¸­ä¿æŒ speaker çš„å£°éŸ³ç‰¹å¾ã€‚<details>
<summary>Abstract</summary>
This paper proposes a novel direct Audio-Visual Speech to Audio-Visual Speech Translation (AV2AV) framework, where the input and output of the system are multimodal (i.e., audio and visual speech). With the proposed AV2AV, two key advantages can be brought: 1) We can perform real-like conversations with individuals worldwide in a virtual meeting by utilizing our own primary languages. In contrast to Speech-to-Speech Translation (A2A), which solely translates between audio modalities, the proposed AV2AV directly translates between audio-visual speech. This capability enhances the dialogue experience by presenting synchronized lip movements along with the translated speech. 2) We can improve the robustness of the spoken language translation system. By employing the complementary information of audio-visual speech, the system can effectively translate spoken language even in the presence of acoustic noise, showcasing robust performance. To mitigate the problem of the absence of a parallel AV2AV translation dataset, we propose to train our spoken language translation system with the audio-only dataset of A2A. This is done by learning unified audio-visual speech representations through self-supervised learning in advance to train the translation system. Moreover, we propose an AV-Renderer that can generate raw audio and video in parallel. It is designed with zero-shot speaker modeling, thus the speaker in source audio-visual speech can be maintained at the target translated audio-visual speech. The effectiveness of AV2AV is evaluated with extensive experiments in a many-to-many language translation setting. The demo page is available on https://choijeongsoo.github.io/av2av.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„ç›´æ¥éŸ³é¢‘è§†é¢‘æ¼”è®²åˆ°éŸ³é¢‘è§†é¢‘æ¼”è®²ï¼ˆAV2AVï¼‰æ¡†æ¶ï¼Œå…¶è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯å¤šModalï¼ˆå³éŸ³é¢‘å’Œè§†é¢‘æ¼”è®²ï¼‰ã€‚ä¸å·²æœ‰çš„æ¼”è®²åˆ°æ¼”è®²ï¼ˆA2Aï¼‰ä¸åŒï¼ŒAV2AVç›´æ¥å°†éŸ³é¢‘è§†é¢‘æ¼”è®²ç¿»è¯‘æˆä¸ºå¦ä¸€ç§è¯­è¨€ã€‚è¿™ç§èƒ½åŠ›æé«˜å¯¹è¯ä½“éªŒï¼Œå› ä¸ºå®ƒå¯ä»¥åŒæ—¶æ˜¾ç¤ºåŒæ—¶å‘ç”Ÿçš„å£è¯­å’Œç¿»è¯‘åçš„å£è¯­ã€‚æ­¤å¤–ï¼ŒAV2AVè¿˜å¯ä»¥æé«˜æ¼”è®²è¯­è¨€ç¿»è¯‘ç³»ç»Ÿçš„ç¨³å®šæ€§ã€‚é€šè¿‡åˆ©ç”¨éŸ³é¢‘è§†é¢‘æ¼”è®²çš„è¡¥å……ä¿¡æ¯ï¼Œç³»ç»Ÿå¯ä»¥æ›´å¥½åœ°ç¿»è¯‘æ¼”è®²è¯­è¨€ï¼Œå³ä½¿åœ¨å™ªéŸ³çš„å­˜åœ¨ä¸‹ã€‚ä¸ºäº†è§£å†³AV2AVç¿»è¯‘æ•°æ®é›†ç¼ºå¤±çš„é—®é¢˜ï¼Œæˆ‘ä»¬æè®®ä½¿ç”¨éŸ³é¢‘åªçš„A2Aæ•°æ®é›†æ¥è®­ç»ƒæ¼”è®²è¯­è¨€ç¿»è¯‘ç³»ç»Ÿã€‚æˆ‘ä»¬é€šè¿‡åœ¨å…ˆè¿›è‡ªåŠ¨å­¦ä¹ ä¸­å­¦ä¹ ç»Ÿä¸€çš„éŸ³é¢‘è§†é¢‘æ¼”è®²è¡¨ç¤ºï¼Œç„¶åç”¨è¿™äº›è¡¨ç¤ºæ¥è®­ç»ƒç¿»è¯‘ç³»ç»Ÿã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æè®®ä¸€ç§AVRendererï¼Œå¯ä»¥åœ¨åŒæ—¶ç”ŸæˆåŸå§‹éŸ³é¢‘å’Œè§†é¢‘ã€‚å®ƒé‡‡ç”¨é›¶ä¸ªæ¨¡å‹ï¼Œå› æ­¤æºè¯­éŸ³è§†é¢‘æ¼”è®²ä¸­çš„ speaker å¯ä»¥ä¿ç•™åœ¨ç›®æ ‡ç¿»è¯‘åçš„éŸ³é¢‘è§†é¢‘æ¼”è®²ä¸­ã€‚AV2AVçš„æ•ˆæœå¾—åˆ°äº†å¹¿æ³›çš„å®éªŒè¯æ˜ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå¤šç§è¯­è¨€ç¿»è¯‘çš„å¤šå¯¹å¤šç¤ºä¾‹ã€‚è¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨ <https://choijeongsoo.github.io/av2av> ä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Visual-Hindsight-Self-Imitation-Learning-for-Interactive-Navigation"><a href="#Visual-Hindsight-Self-Imitation-Learning-for-Interactive-Navigation" class="headerlink" title="Visual Hindsight Self-Imitation Learning for Interactive Navigation"></a>Visual Hindsight Self-Imitation Learning for Interactive Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03446">http://arxiv.org/abs/2312.03446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kibeom Kim, Kisung Shin, Min Whoo Lee, Moonhoen Lee, Minsu Lee, Byoung-Tak Zhang</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯æé«˜è§†è§‰å¯¼èˆªä»»åŠ¡çš„æ ·æœ¬æ•ˆç‡ï¼Œå³è®©Agentæ›´å¿«åœ°å­¦ä¹ å’Œå®Œæˆè¿™äº›ä»»åŠ¡ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³• called Visual Hindsight Self-Imitation Learning (VHS)ï¼Œå®ƒåˆ©ç”¨è§†è§‰åæ€å’Œè‡ªæˆ‘æ¨¡ä»¿æ¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚å¦å¤–ï¼Œè®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§ prosthetical goal embedding æ–¹æ³•ï¼Œç”¨äºåœ¨è§†è§‰å’Œéƒ¨åˆ†å¯è§çš„ç¯å¢ƒä¸­æ›´å¥½åœ°è¡¨ç¤ºç›®æ ‡ã€‚</li>
<li>results: å®éªŒç»“æœè¡¨æ˜ï¼ŒVHS æ–¹æ³•åœ¨è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡äº†ç°æœ‰çš„æŠ€æœ¯ã€‚è¿™confirming its superior performance and sample efficiencyã€‚<details>
<summary>Abstract</summary>
Interactive visual navigation tasks, which involve following instructions to reach and interact with specific targets, are challenging not only because successful experiences are very rare but also because the complex visual inputs require a substantial number of samples. Previous methods for these tasks often rely on intricately designed dense rewards or the use of expensive expert data for imitation learning. To tackle these challenges, we propose a novel approach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample efficiency through hindsight goal re-labeling and self-imitation. We also introduce a prototypical goal embedding method derived from experienced goal observations, that is particularly effective in vision-based and partially observable environments. This embedding technique allows the agent to visually reinterpret its unsuccessful attempts, enabling vision-based goal re-labeling and self-imitation from enhanced successful experiences. Experimental results show that VHS outperforms existing techniques in interactive visual navigation tasks, confirming its superior performance and sample efficiency.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>> translate into Simplified ChineseæŠ½è±¡ï¼šè§†è§‰å¯¼èˆªä»»åŠ¡éœ€è¦æŒ‰ç…§æŒ‡ä»¤è¾¾åˆ°å’Œäº¤äº’ç‰¹å®šç›®æ ‡ï¼Œè¿™äº›ä»»åŠ¡éå¸¸å›°éš¾ï¼Œä¸ä»…æˆåŠŸç»éªŒéå¸¸ç½•è§ï¼Œè€Œä¸”è§†è§‰è¾“å…¥éå¸¸å¤æ‚ï¼Œéœ€è¦å¤§é‡æ ·æœ¬æ¥å­¦ä¹ ã€‚ç°æœ‰çš„æ–¹æ³•frequently rely on densely designed reward structures or the use of expensive expert data for imitation learningã€‚æè®®ï¼šä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œè§†è§‰å†å²è‡ªæˆ‘æ¨¡ä»¿å­¦ä¹ ï¼ˆVHSï¼‰ï¼Œå¯ä»¥æé«˜æ ·æœ¬æ•ˆç‡é€šè¿‡åçœ‹ç›®æ ‡é‡æ–°æ ‡æ³¨å’Œè‡ªæˆ‘æ¨¡ä»¿ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºç»éªŒç›®æ ‡è§‚å¯Ÿçš„ç›®æ ‡åµŒå…¥æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•åœ¨è§†è§‰åŸºäºå’Œéƒ¨åˆ†å¯è§ç¯å¢ƒä¸­éå¸¸æœ‰æ•ˆã€‚è¿™ç§åµŒå…¥æŠ€æœ¯ä½¿å¾—æœºå™¨äººå¯ä»¥æ ¹æ®ä¸æˆåŠŸçš„å°è¯•é‡æ–° Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°è§†è§‰ï¼Œä»è€Œå®ç°è§†è§‰åŸºäºçš„ç›®æ ‡é‡æ–°æ ‡æ³¨å’Œè‡ªæˆ‘æ¨¡ä»¿ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVHSåœ¨äº¤äº’è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¿‡ç°æœ‰æŠ€æœ¯ï¼Œè¯æ˜å…¶é«˜æ•ˆæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚æ€»ç»“ï¼šé€šè¿‡æå‡ºVHSæ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥è§£å†³äº¤äº’è§†è§‰å¯¼èˆªä»»åŠ¡ä¸­çš„æ ·æœ¬æ•ˆç‡å’ŒæˆåŠŸç»éªŒç½•è§é—®é¢˜ï¼Œå¹¶ä¸”å®éªŒç»“æœè¯æ˜VHSçš„é«˜æ•ˆæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥åœ¨è§†è§‰åŸºäºå’Œéƒ¨åˆ†å¯è§ç¯å¢ƒä¸­åº”ç”¨ï¼Œæœ‰åŠ©äºæœºå™¨äººæ›´å¥½åœ°å®Œæˆäº¤äº’è§†è§‰å¯¼èˆªä»»åŠ¡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Inspecting-Model-Fairness-in-Ultrasound-Segmentation-Tasks"><a href="#Inspecting-Model-Fairness-in-Ultrasound-Segmentation-Tasks" class="headerlink" title="Inspecting Model Fairness in Ultrasound Segmentation Tasks"></a>Inspecting Model Fairness in Ultrasound Segmentation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02501">http://arxiv.org/abs/2312.02501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zikang Xu, Fenghe Tang, Quan Quan, Jianrui Ding, Chunping Ning, S. Kevin Zhou</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è¯„ä¼°æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰ segmentation æ¨¡å‹åœ¨ä¸åŒæ•æ„Ÿå±æ€§ä¸‹çš„åè§æƒ…å†µã€‚</li>
<li>methods: è¯¥ paper ä½¿ç”¨äº†ä¸¤ä¸ªultrasoundæ•°æ®é›†ï¼Œä½¿ç”¨äº†state-of-the-art DLç®—æ³•è¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œeven state-of-the-art DLç®—æ³•åœ¨ultrasound segmentationä»»åŠ¡ä¸­å­˜åœ¨åè§æƒ…å†µã€‚è¿™äº›ç»“æœä½œä¸ºä¸€ä¸ªè­¦ç¤ºï¼Œå¼ºè°ƒäº†åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œä»¥ç¡®ä¿ä¼¦ç†è€ƒè™‘å’Œå‡å°‘å¯¹æ‚£è€…ç»“æœçš„é£é™©ã€‚<details>
<summary>Abstract</summary>
With the rapid expansion of machine learning and deep learning (DL), researchers are increasingly employing learning-based algorithms to alleviate diagnostic challenges across diverse medical tasks and applications. While advancements in diagnostic precision are notable, some researchers have identified a concerning trend: their models exhibit biased performance across subgroups characterized by different sensitive attributes. This bias not only infringes upon the rights of patients but also has the potential to lead to life-altering consequences. In this paper, we inspect a series of DL segmentation models using two ultrasound datasets, aiming to assess the presence of model unfairness in these specific tasks. Our findings reveal that even state-of-the-art DL algorithms demonstrate unfair behavior in ultrasound segmentation tasks. These results serve as a crucial warning, underscoring the necessity for careful model evaluation before their deployment in real-world scenarios. Such assessments are imperative to ensure ethical considerations and mitigate the risk of adverse impacts on patient outcomes.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="MKA-A-Scalable-Medical-Knowledge-Assisted-Mechanism-for-Generative-Models-on-Medical-Conversation-Tasks"><a href="#MKA-A-Scalable-Medical-Knowledge-Assisted-Mechanism-for-Generative-Models-on-Medical-Conversation-Tasks" class="headerlink" title="MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks"></a>MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative Models on Medical Conversation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02496">http://arxiv.org/abs/2312.02496</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liangke23/knowledge_assisted_medical_dialogue_generation_mechanism">https://github.com/liangke23/knowledge_assisted_medical_dialogue_generation_mechanism</a></li>
<li>paper_authors: Ke Liang, Sifan Wu, Jiayi Gu</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜åŒ»ç–—èŠå¤©æœºå™¨äººçš„è¯Šæ–­æ•ˆç‡å’Œä¾¿æ·æ€§ï¼Œä½¿åŒ»ç–—AIæŠ€æœ¯å¾—åˆ°æ›´å¤šåº”ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç¥ç»ç”Ÿæˆæ¨¡å‹ä½œä¸ºèŠå¤©æœºå™¨äººçš„æ ¸å¿ƒï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¯æ‰©å±•çš„åŒ»ç–—çŸ¥è¯†ååŠ©æœºåˆ¶ï¼ˆMKAï¼‰ï¼Œä»¥å¸®åŠ©ç¥ç»ç”Ÿæˆæ¨¡å‹åœ¨åŒ»ç–—èŠå¤©ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚</li>
<li>results: è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œå°†MKAæœºåˆ¶åº”ç”¨äºç¥ç»ç”Ÿæˆæ¨¡å‹åï¼Œåœ¨å¤šä¸ªè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¹¶åœ¨MedDGå’ŒMedDialog-CNä¸¤ä¸ªåŒ»ç–—æ•°æ®é›†ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Using natural language processing (NLP) technologies to develop medical chatbots makes the diagnosis of the patient more convenient and efficient, which is a typical application in healthcare AI. Because of its importance, lots of research have been come out. Recently, the neural generative models have shown their impressive ability as the core of chatbot, while it cannot scale well when directly applied to medical conversation due to the lack of medical-specific knowledge. To address the limitation, a scalable Medical Knowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism aims to assist general neural generative models to achieve better performance on the medical conversation task. The medical-specific knowledge graph is designed within the mechanism, which contains 6 types of medical-related information, including department, drug, check, symptom, disease, food. Besides, the specific token concatenation policy is defined to effectively inject medical information into the input data. Evaluation of our method is carried out on two typical medical datasets, MedDG and MedDialog-CN. The evaluation results demonstrate that models combined with our mechanism outperform original methods in multiple automatic evaluation metrics. Besides, MKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are public: https://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism
</details>
<details>
<summary>æ‘˜è¦</summary>
ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æŠ€æœ¯å¼€å‘åŒ»ç–—èŠå¤©æœºå™¨äººï¼Œå¯ä»¥ä½¿æ‚£è€…è¯Šæ–­æ›´åŠ æ–¹ä¾¿å’Œé«˜æ•ˆï¼Œè¿™æ˜¯åŒ»ç–—AIçš„å…¸å‹åº”ç”¨ã€‚ç”±äºå…¶é‡è¦æ€§ï¼Œæœ‰å¾ˆå¤šç ”ç©¶å‘è¡¨ã€‚æœ€è¿‘ï¼Œç¥ç»ç”Ÿæˆæ¨¡å‹åœ¨èŠå¤©æœºå™¨äººæ ¸å¿ƒéƒ¨åˆ†è¡¨ç°å‡ºäº†æƒŠäººçš„èƒ½åŠ›ï¼Œä½†ç›´æ¥åº”ç”¨äºåŒ»ç–—å¯¹è¯æ—¶å› ç¼ºä¹åŒ»ç–—ä¸“ä¸šçŸ¥è¯†è€Œéš¾ä»¥æ‰©å±•ã€‚ä¸ºè§£å†³è¿™äº›é™åˆ¶ï¼Œæœ¬æ–‡æå‡ºäº†å¯æ‰©å±•çš„åŒ»å­¦çŸ¥è¯†ååŠ©æœºåˆ¶ï¼ˆMKAï¼‰ã€‚è¯¥æœºåˆ¶çš„ç›®çš„æ˜¯å¸®åŠ©æ™®é€šçš„ç¥ç»ç”Ÿæˆæ¨¡å‹åœ¨åŒ»ç–—å¯¹è¯ä»»åŠ¡ä¸­æ›´å¥½çš„è¡¨ç°ã€‚åŒ»å­¦ä¸“ä¸šçŸ¥è¯†å›¾åœ¨æœºåˆ¶ä¸­è®¾è®¡ï¼ŒåŒ…æ‹¬6ç§åŒ»ç–—ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬éƒ¨é—¨ã€è¯å“ã€æ£€æŸ¥ã€ç—‡çŠ¶ã€ç–¾ç—…å’Œé£Ÿç‰©ã€‚æ­¤å¤–ï¼Œç‰¹å®šçš„Tokenæ‹¼æ¥ç­–ç•¥å®šä¹‰ä»¥æœ‰æ•ˆåœ°æ³¨å…¥åŒ»å­¦ä¿¡æ¯åˆ°è¾“å…¥æ•°æ®ä¸­ã€‚å¯¹æˆ‘ä»¬çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªå…¸å‹çš„åŒ»ç–—æ•°æ®é›†ï¼šMedDGå’ŒMedDialog-CNã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œä¸æˆ‘ä»¬çš„æœºåˆ¶ç›¸ç»“åˆçš„æ¨¡å‹åœ¨å¤šä¸ªè‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè€ŒMKA-Bert-GPTè¿˜è¾¾åˆ°äº†å½“å‰æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬çš„ä»£ç å…¬å¼€åœ¨ GitHubï¼šhttps://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanismã€‚
</details></li>
</ul>
<hr>
<h2 id="Flexible-Communication-for-Optimal-Distributed-Learning-over-Unpredictable-Networks"><a href="#Flexible-Communication-for-Optimal-Distributed-Learning-over-Unpredictable-Networks" class="headerlink" title="Flexible Communication for Optimal Distributed Learning over Unpredictable Networks"></a>Flexible Communication for Optimal Distributed Learning over Unpredictable Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02493">http://arxiv.org/abs/2312.02493</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Tyagi, Martin Swany</li>
<li>For: This paper aims to improve the efficiency of distributed deep learning training by reducing the communication overhead and accelerating the training process.* Methods: The paper proposes an Allreduce (AR)-compatible Topk compressor that is bandwidth-optimal and can switch between AG and AR based on the current network configuration. The authors also model the pareto-relationship between parallel and statistical efficiency as a multi-objective optimization (MOO) problem to dynamically adjust the compression ratio and accelerate training.* Results: The proposed method achieves high accuracy like DenseSGD but with lower communication cost, and can dynamically adjust the compression ratio and collective operation to balance parallel and statistical efficiency. The authors also show that the proposed method outperforms AG in certain network configurations and can be applied to various deep learning models.<details>
<summary>Abstract</summary>
Gradient compression alleviates expensive communication in distributed deep learning by sending fewer values and its corresponding indices, typically via Allgather (AG). Training with high compression ratio (CR) achieves high accuracy like DenseSGD, but has lower parallel scaling due to high communication cost (i.e., parallel efficiency). Using lower CRs improves parallel efficiency by lowering synchronization cost, but degrades model accuracy as well (statistical efficiency). Further, speedup attained with different models and CRs also varies with network latency, effective bandwidth and collective op used for aggregation. In many cases, collectives like Allreduce (AR) have lower cost than AG to exchange the same amount of data. In this paper, we propose an AR-compatible Topk compressor that is bandwidth-optimal and thus performs better than AG in certain network configurations. We develop a flexible communication strategy that switches between AG and AR based on which collective is optimal in the current settings, and model the pareto-relationship between parallel and statistical efficiency as a multi-objective optimization (MOO) problem to dynamically adjust CR and accelerate training while still converging to high accuracy.
</details>
<details>
<summary>æ‘˜è¦</summary>
åˆ†å¸ƒå¼æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦å‹ç¼©å¯ä»¥é™ä½è´Ÿæ‹…é‡å¤§çš„é€šä¿¡æˆæœ¬ï¼Œé€šå¸¸é€šè¿‡æ‰€è°“çš„Allgatherï¼ˆAGï¼‰è¿›è¡Œå®ç°ã€‚åœ¨è®­ç»ƒä¸­ä½¿ç”¨é«˜æ¯”ç‡å‹ç¼©æ¯”ï¼ˆCRï¼‰å¯ä»¥è¾¾åˆ°é«˜ç²¾åº¦ï¼Œä½†æ˜¯åŒæ—¶ä¼šé™ä½å¹¶è¡Œæ‰©å±•çš„å¹¶è¡Œçº§åˆ«ï¼ˆi.e., å¹¶è¡Œæ•ˆç‡ï¼‰ã€‚ä½¿ç”¨è¾ƒä½çš„CRå¯ä»¥æé«˜å¹¶è¡Œæ•ˆç‡ï¼Œä½†æ˜¯ä¼šé™ä½æ¨¡å‹ç²¾åº¦ï¼ˆç»Ÿè®¡æ•ˆç‡ï¼‰ã€‚æ­¤å¤–ï¼Œä¸åŒçš„æ¨¡å‹å’ŒCRåœ¨ä¸åŒçš„ç½‘ç»œå»¶è¿Ÿã€æœ‰æ•ˆå¸¦å®½å’Œé›†åˆæ“ä½œä¸Šçš„é€Ÿåº¦æå‡ä¹Ÿä¼šæœ‰å·®å¼‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ARå…¼å®¹çš„Topkå‹ç¼©å™¨ï¼Œå®ƒåœ¨æŸäº›ç½‘ç»œé…ç½®ä¸‹å…·æœ‰æœ€ä½³å¸¦å®½æ€§ï¼Œå› æ­¤åœ¨AGä¹‹ä¸Šè¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ç§çµæ´»çš„é€šä¿¡ç­–ç•¥ï¼Œå¯ä»¥æ ¹æ®å½“å‰è®¾ç½®é€‰æ‹©AGæˆ–ARè¿›è¡ŒååŒï¼Œå¹¶æ¨¡å‹äº†å¹¶è¡Œæ•ˆç‡å’Œç»Ÿè®¡æ•ˆç‡ä¹‹é—´çš„å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆMOOï¼‰é—®é¢˜ï¼Œä»¥ dynamically è°ƒæ•´CRå¹¶åŠ é€Ÿè®­ç»ƒï¼Œå¹¶ä¸”ä»ç„¶å¯ä»¥ converge to high accuracyã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Holistically-Detect-Bridges-from-Large-Size-VHR-Remote-Sensing-Imagery"><a href="#Learning-to-Holistically-Detect-Bridges-from-Large-Size-VHR-Remote-Sensing-Imagery" class="headerlink" title="Learning to Holistically Detect Bridges from Large-Size VHR Remote Sensing Imagery"></a>Learning to Holistically Detect Bridges from Large-Size VHR Remote Sensing Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02481">http://arxiv.org/abs/2312.02481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yansheng Li, Junwei Luo, Yongjun Zhang, Yihua Tan, Jin-Gang Yu, Song Bai</li>
<li>For: The paper is written for detecting bridges in remote sensing images (RSIs) and addressing the challenges of bridge detection in large-size very-high-resolution (VHR) RSIs.* Methods: The paper proposes a large-scale dataset named GLH-Bridge, which comprises 6,000 VHR RSIs sampled from diverse geographic locations across the globe, and presents an efficient network for holistic bridge detection (HBD-Net) in large-size RSIs, which uses a separate detector-based feature fusion (SDFF) architecture and is optimized via a shape-sensitive sample re-weighting (SSRW) strategy.* Results: The paper establishes a bridge detection benchmark including the OBB and HBB tasks, and validates the effectiveness of the proposed HBD-Net on the GLH-Bridge dataset, with cross-dataset generalization experiments illustrating the strong generalization capability of the GLH-Bridge dataset.<details>
<summary>Abstract</summary>
Bridge detection in remote sensing images (RSIs) plays a crucial role in various applications, but it poses unique challenges compared to the detection of other objects. In RSIs, bridges exhibit considerable variations in terms of their spatial scales and aspect ratios. Therefore, to ensure the visibility and integrity of bridges, it is essential to perform holistic bridge detection in large-size very-high-resolution (VHR) RSIs. However, the lack of datasets with large-size VHR RSIs limits the deep learning algorithms' performance on bridge detection. Due to the limitation of GPU memory in tackling large-size images, deep learning-based object detection methods commonly adopt the cropping strategy, which inevitably results in label fragmentation and discontinuous prediction. To ameliorate the scarcity of datasets, this paper proposes a large-scale dataset named GLH-Bridge comprising 6,000 VHR RSIs sampled from diverse geographic locations across the globe. These images encompass a wide range of sizes, varying from 2,048*2,048 to 16,38*16,384 pixels, and collectively feature 59,737 bridges. Furthermore, we present an efficient network for holistic bridge detection (HBD-Net) in large-size RSIs. The HBD-Net presents a separate detector-based feature fusion (SDFF) architecture and is optimized via a shape-sensitive sample re-weighting (SSRW) strategy. Based on the proposed GLH-Bridge dataset, we establish a bridge detection benchmark including the OBB and HBB tasks, and validate the effectiveness of the proposed HBD-Net. Additionally, cross-dataset generalization experiments on two publicly available datasets illustrate the strong generalization capability of the GLH-Bridge dataset.
</details>
<details>
<summary>æ‘˜è¦</summary>
remote sensing å›¾åƒä¸­çš„æ¡¥æ¢æ£€æµ‹ï¼ˆRSIsï¼‰åœ¨å¤šç§åº”ç”¨ä¸­å‘æŒ¥å…³é”®ä½œç”¨ï¼Œä½†æ˜¯å®ƒä»¬å…·æœ‰ç‰¹æ®Šçš„æŒ‘æˆ˜ã€‚åœ¨ RSIs ä¸­ï¼Œæ¡¥æ¢å…·æœ‰è¾ƒå¤§çš„ç©ºé—´å°ºåº¦å’Œæ–¹å‘æ¯”ï¼Œå› æ­¤ï¼Œä¸ºä¿è¯æ¡¥æ¢çš„å¯è§æ€§å’Œå®Œæ•´æ€§ï¼Œéœ€è¦åœ¨å¤§å‹é«˜åˆ†è¾¨ç‡ï¼ˆVHRï¼‰ RSIs ä¸­è¿›è¡Œæ•´ä½“çš„æ¡¥æ¢æ£€æµ‹ã€‚ç„¶è€Œï¼Œç”±äº GPU å†…å­˜ä¸èƒ½å¤„ç†å¤§å‹å›¾åƒçš„é™åˆ¶ï¼Œæ·±åº¦å­¦ä¹ åŸºäºå¯¹è±¡æ£€æµ‹æ–¹æ³•é€šå¸¸é‡‡ç”¨è£å‰ªç­–ç•¥ï¼Œè¿™ä¼šå¯¼è‡´æ ‡ç­¾çš„åˆ†è£‚å’Œä¸è¿ç»­é¢„æµ‹ã€‚ä¸ºäº†è§£å†³æ•°æ®ç¼ºä¹é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†åä¸º GLH-Bridgeï¼ŒåŒ…å« 6,000 ä¸ª VHR RSIs ä»ä¸–ç•Œå„åœ°çš„å¤šä¸ªåœ°ç†ä½ç½®é‡‡æ ·ï¼Œè¿™äº›å›¾åƒå…·æœ‰è¾ƒå¤§çš„å°ºåº¦ï¼Œä» 2,048*2,048 åˆ° 16,384*16,384 åƒç´ ï¼Œæ€»å…±åŒ…å« 59,737 åº§æ¡¥æ¢ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„æ¡¥æ¢æ£€æµ‹ç½‘ç»œï¼ˆHBD-Netï¼‰ï¼Œè¯¥ç½‘ç»œé‡‡ç”¨åˆ†ç«‹æ£€æµ‹å™¨åŸºäºç‰¹å¾èåˆï¼ˆSDFFï¼‰æ¶æ„ï¼Œå¹¶é€šè¿‡å½¢çŠ¶æ•æ„Ÿæ ·æœ¬é‡æ–°æƒé‡ç­–ç•¥ï¼ˆSSRWï¼‰ä¼˜åŒ–ã€‚åŸºäºæˆ‘ä»¬æå‡ºçš„ GLH-Bridge æ•°æ®é›†ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªæ¡¥æ¢æ£€æµ‹æ ‡å‡†å‡†åˆ™ï¼ŒåŒ…æ‹¬ OBB å’Œ HBB ä»»åŠ¡ï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„ HBD-Net çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº†è·¨æ•°æ®é›†æ™®é€‚æ€§å®éªŒï¼Œè¯æ˜ GLH-Bridge æ•°æ®é›†å…·æœ‰å¼ºå¤§çš„æ™®é€‚æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="E4SRec-An-Elegant-Effective-Efficient-Extensible-Solution-of-Large-Language-Models-for-Sequential-Recommendation"><a href="#E4SRec-An-Elegant-Effective-Efficient-Extensible-Solution-of-Large-Language-Models-for-Sequential-Recommendation" class="headerlink" title="E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation"></a>E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02443">http://arxiv.org/abs/2312.02443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hestiasky/e4srec">https://github.com/hestiasky/e4srec</a></li>
<li>paper_authors: Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, Chunxiao Xing</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨å°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åº”ç”¨äºæ¨èç³»ç»Ÿä¸­ï¼Œä»¥æé«˜æ¨èçš„ä¸ªæ€§åŒ–æ€§å’Œæ•ˆç‡ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§å«åšElegant Effective Efficient Extensible solution for large language models for Sequential Recommendationï¼ˆE4SRecï¼‰ï¼Œå®ƒå¯ä»¥å°†LLMsä¸ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿç»“åˆèµ·æ¥ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨IDåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºåœ¨å€™é€‰åˆ—è¡¨ä¸­ã€‚</li>
<li>results: ä½œè€…é€šè¿‡å¯¹å››ç§å¹¿æ³›ä½¿ç”¨çš„å®é™…æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº†E4SRecçš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚<details>
<summary>Abstract</summary>
The recent advancements in Large Language Models (LLMs) have sparked interest in harnessing their potential within recommender systems. Since LLMs are designed for natural language tasks, existing recommendation approaches have predominantly transformed recommendation tasks into open-domain natural language generation tasks. However, this approach necessitates items to possess rich semantic information, often generates out-of-range results, and suffers from notably low efficiency and limited extensibility. Furthermore, practical ID-based recommendation strategies, reliant on a huge number of unique identities (IDs) to represent users and items, have gained prominence in real-world recommender systems due to their effectiveness and efficiency. Nevertheless, the incapacity of LLMs to model IDs presents a formidable challenge when seeking to leverage LLMs for personalized recommendations. In this paper, we introduce an Elegant Effective Efficient Extensible solution for large language models for Sequential Recommendation (E4SRec), which seamlessly integrates LLMs with traditional recommender systems that exclusively utilize IDs to represent items. Specifically, E4SRec takes ID sequences as inputs, ensuring that the generated outputs fall within the candidate lists. Furthermore, E4SRec possesses the capability to generate the entire ranking list in a single forward process, and demands only a minimal set of pluggable parameters, which are trained for each dataset while keeping the entire LLM frozen. We substantiate the effectiveness, efficiency, and extensibility of our proposed E4SRec through comprehensive experiments conducted on four widely-used real-world datasets. The implementation code is accessible at https://github.com/HestiaSky/E4SRec/.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›æ­¥å¼•èµ·äº†æ¨èç³»ç»Ÿä¸­ä½¿ç”¨å…¶æ½œåŠ›çš„å…´è¶£ã€‚ç”±äº LLMs æ˜¯é’ˆå¯¹è‡ªç„¶è¯­è¨€ä»»åŠ¡è®¾è®¡çš„ï¼Œç°æœ‰çš„æ¨èæ–¹æ³•ä¸»è¦å°†æ¨èä»»åŠ¡è½¬åŒ–ä¸ºå¼€æ”¾é¢†åŸŸè‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•éœ€è¦ITEMSå…·æœ‰ä¸°å¯Œçš„å«ä¹‰ä¿¡æ¯ï¼Œç»å¸¸äº§ç”ŸèŒƒå›´å¤–çš„ç»“æœï¼Œå¹¶ä¸”å—åˆ°è¾ƒä½çš„æ•ˆç‡å’Œæœ‰é™çš„æ‰©å±•æ€§çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåŸºäºå”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆIDï¼‰çš„å®ç”¨æ¨èç­–ç•¥å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œå› ä¸ºå®ƒä»¬çš„æ•ˆæœå’Œæ•ˆç‡ã€‚ç„¶è€Œï¼Œ LLMs æ— æ³•æ¨¡å‹ ID æ˜¯ä¸€å¤§é—®é¢˜ï¼Œå½“å¸Œæœ›é€šè¿‡ LLMs æä¾›ä¸ªæ€§åŒ–æ¨èæ—¶ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ç®€æ´æœ‰æ•ˆé«˜æ•ˆå¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼Œå³ E4SRecï¼Œè¯¥æ–¹æ¡ˆå¯ä»¥å¿«é€Ÿåœ°å°† LLMs ä¸ä¼ ç»Ÿçš„æ¨èç³»ç»Ÿé›†æˆï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨ ID åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºåœ¨å€™é€‰åˆ—è¡¨ä¸­ã€‚æ­¤å¤–ï¼ŒE4SRec å…·æœ‰ç”Ÿæˆæ•´ä¸ªæ’ååˆ—è¡¨çš„èƒ½åŠ›ï¼Œä»…éœ€è¦ä¸€ä¸ª minimal çš„å¯æ’å…¥å‚æ•°ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒè¿™äº›å‚æ•°ï¼Œä¿æŒæ•´ä¸ª LLM å†»ç»“ã€‚æˆ‘ä»¬é€šè¿‡å¯¹å››ç§å¹¿æ³›ä½¿ç”¨çš„å®é™…æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œè¯æ˜äº† E4SRec çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ä»£ç å¯ä»¥åœ¨ GitHub ä¸Šè·å–ï¼šhttps://github.com/HestiaSky/E4SRec/.
</details></li>
</ul>
<hr>
<h2 id="Letâ€™s-Think-Outside-the-Box-Exploring-Leap-of-Thought-in-Large-Language-Models-with-Creative-Humor-Generation"><a href="#Letâ€™s-Think-Outside-the-Box-Exploring-Leap-of-Thought-in-Large-Language-Models-with-Creative-Humor-Generation" class="headerlink" title="Letâ€™s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation"></a>Letâ€™s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02439">http://arxiv.org/abs/2312.02439</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sail-sg/clot">https://github.com/sail-sg/clot</a></li>
<li>paper_authors: Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, Pan Zhou</li>
<li>for: è¿™paperæ˜¯ä¸ºäº†æ¢è®¨Chain-of-Thought (CoT)å’ŒLeap-of-Thought (LoT)åœ¨å¤§è¯­è¨€æ¨¡å‹ (LLM) ä¸­çš„åº”ç”¨ï¼Œä»¥åŠå¦‚ä½•æé«˜LLMçš„åˆ›é€ åŠ›ã€‚</li>
<li>methods: è¿™paperä½¿ç”¨äº†Oogiriæ¸¸æˆä½œä¸ºç ”ç©¶å¯¹è±¡ï¼Œå»ºç«‹äº†ä¸€ä¸ªå¤šmodalå’Œå¤šè¯­è¨€çš„Oogiri-GOæ•°æ®é›†ï¼Œå¹¶å¯¹ç°æœ‰LLMçš„LoTèƒ½åŠ›è¿›è¡Œäº†ç ”ç©¶ã€‚ä¸ºäº†æé«˜LLMçš„åˆ›é€ åŠ›ï¼Œè¿™paperå¼•å…¥äº†ä¸€ç§åˆ›æ–°çš„Leap-of-Thought (CLoT) paradigmï¼ŒåŒ…æ‹¬å¯¹é¢„è®­ç»ƒLLMçš„ instrucion tuning å’Œè‡ªæˆ‘ä¿®å¤ä¸¤éƒ¨åˆ†ã€‚</li>
<li>results: è¿™paperåœ¨Oogiriæ¸¸æˆä¸­è¡¨ç°å‡ºäº†å‡ºè‰²çš„åˆ›ä½œèƒ½åŠ›ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºäº†æé«˜çš„åˆ›é€ åŠ›ï¼Œå¦‚äº‘çŒœæµ‹æ¸¸æˆå’Œå¤šå…ƒå…³è”ä»»åŠ¡ã€‚è¿™äº›å‘ç°å¯èƒ½ä¸ºå¤§è¯­è¨€æ¨¡å‹çš„åˆ›æ–°åº”ç”¨å¸¦æ¥å¯ç¤ºï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†ä¸€æ¡å¯è¡Œçš„é“è·¯ã€‚<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logical tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investigate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pretrained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an explorative self-refinement that encourages the LLM to generate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game but also boosts creative abilities in various tasks like cloud guessing game and divergent association task. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models will be released online. https://zhongshsh.github.io/CLoT/.
</details>
<details>
<summary>æ‘˜è¦</summary>
Chain-of-Thought (CoT) å¯¼å¼•å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œé€»è¾‘æ¨ç†ï¼Œå¹¶å¯ä»¥æ¿€å‘å…¶é€»è¾‘æ€ç»´èƒ½åŠ›ã€‚è™½ç„¶æ•ˆæœè‰¯å¥½äºé€»è¾‘ä»»åŠ¡ï¼Œä½†CoTä¸é€‚ç”¨äºåˆ›é€ æ€§é—®é¢˜è§£å†³ï¼Œè¿™ç§é—®é¢˜å¸¸éœ€è¦éå…¸å‹æ€ç»´å’Œåˆ›æ–°ï¼Œæ˜¯åˆ›æ–°è¿›ç¨‹ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨Oogiriæ¸¸æˆä¸­çš„å¼ºç›¸å…³æ€ç»´èƒ½åŠ›ï¼ˆLoTï¼‰â€”â€”ä¸€ç§éçº¿æ€§ã€åˆ›é€ æ€§çš„æ€ç»´æ–¹å¼ã€‚ä¸ºäº†ç ”ç©¶LLMåœ¨Oogiriæ¸¸æˆä¸­çš„LoTèƒ½åŠ›ï¼Œæˆ‘ä»¬é¦–å…ˆå»ºç«‹äº†ä¸€ä¸ªå¤šModalå’Œå¤šè¯­è¨€çš„Oogiri-GOæ•°æ®é›†ï¼ŒåŒ…å«äº†Oogiriæ¸¸æˆä¸­çš„130,000å¤šä¸ªæ ·æœ¬ï¼Œå¹¶è§‚å¯Ÿäº†å¤§å¤šæ•°ç°æœ‰LLMåœ¨Oogiriæ¸¸æˆä¸­çš„ä¸è¶³æˆ–å¤±è´¥ã€‚ accordinglyï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ›æ–°çš„Leap-of-Thoughtï¼ˆCLoTï¼‰æ–¹æ³•ï¼Œä»¥æé«˜LLMçš„LoTèƒ½åŠ›ã€‚CLoTé¦–å…ˆå°†Oogiri-GOæ•°æ®é›†è½¬åŒ–ä¸ºLoT-oriented instruction tuningæ•°æ®ï¼Œä»¥è®­ç»ƒé¢„è®­ç»ƒçš„LLMè¾¾åˆ°certain LoTå¹½é»˜ç”Ÿæˆå’Œåˆ†ç±»èƒ½åŠ›ã€‚ç„¶åï¼ŒCLoTè®¾è®¡äº†ä¸€ç§æ¢ç´¢æ€§è‡ªæˆ‘ä¼˜åŒ–ï¼Œä»¥ä¾¿LLMé€šè¿‡æ¢ç´¢ä¸ä¹‹ç›¸å…³çš„ä¸åŒæ¦‚å¿µä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œç”Ÿæˆæ›´åˆ›æ–°çš„LoTæ•°æ®ï¼Œå¹¶é€‰æ‹©é«˜è´¨é‡çš„æ•°æ®è¿›è¡Œè‡ªæˆ‘ä¼˜åŒ–ã€‚CLoTä¸ä»…åœ¨Oogiriæ¸¸æˆä¸­å±•ç¤ºå‡ºäº†å¹½é»˜ç”Ÿæˆèƒ½åŠ›ï¼Œè¿˜åœ¨å¤šä¸ªä»»åŠ¡ä¸­æé«˜äº†åˆ›é€ èƒ½åŠ›ï¼Œå¦‚äº‘çŒ«çŒœæµ‹æ¸¸æˆå’Œå¤šå…ƒååŒå…³ç³»ä»»åŠ¡ã€‚è¿™äº›å‘ç°å¯¹LLMçš„åˆ›é€ èƒ½åŠ›çš„ç†è§£æä¾›äº†ä¸€æ¡è¿›è·¯ï¼Œå¹¶å¯ä»¥ç”¨äºåœ¨ä¸åŒé¢†åŸŸä¸­åº”ç”¨åˆ›æ–°ã€‚æ•°æ®ã€ä»£ç å’Œæ¨¡å‹å°†åœ¨çº¿å‘å¸ƒã€‚Please note that the translation is done using a machine translation tool, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="MUFFIN-Curating-Multi-Faceted-Instructions-for-Improving-Instruction-Following"><a href="#MUFFIN-Curating-Multi-Faceted-Instructions-for-Improving-Instruction-Following" class="headerlink" title="MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following"></a>MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02436">http://arxiv.org/abs/2312.02436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RenzeLou/Muffin">https://github.com/RenzeLou/Muffin</a></li>
<li>paper_authors: Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu Su, Wenpeng Yin</li>
<li>for: æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ instruction-following èƒ½åŠ›</li>
<li>methods: é€šè¿‡è‡ªåŠ¨æ‰©å¤§æ¯ä¸ªä»»åŠ¡çš„è¾“å…¥æ–¹é¢æ¥å¢å¼ºæ•°æ®é›†çš„ curaciÃ³n</li>
<li>results: LLMs åœ¨ä¸åŒçš„ç¼©æ”¾ä»»åŠ¡å’Œæ— è¾“å…¥ä»»åŠ¡ä¹‹é—´å…·æœ‰æ›´é«˜çš„ instruction-following èƒ½åŠ›ï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸åŒçš„è¾“å…¥æ–¹é¢ä¸­è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
In the realm of large language models (LLMs), enhancing instruction-following capability often involves curating expansive training data. This is achieved through two primary schemes: i) Scaling-Inputs: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. ii) Scaling Input-Free Tasks: Enlarging tasks, each composed of an (instruction, output) pair (without requiring a separate input anymore). However, LLMs under Scaling-Inputs tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, Scaling Input-Free Tasks demands a substantial number of tasks but is less effective in instruction following when dealing with instances in Scaling-Inputs. This work introduces MUFFIN, a new scheme of instruction-following dataset curation. Specifically, we automatically Scale Tasks per Input by diversifying these tasks with various input facets. Experimental results across four zero-shot benchmarks, spanning both Scaling-Inputs and Scaling Input-Free Tasks schemes, reveal that LLMs, at various scales, trained on MUFFIN generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¢†åŸŸä¸­ï¼Œæé«˜æŒ‡ä»¤éµä»èƒ½åŠ›é€šå¸¸é€šè¿‡ä¸¤ç§ä¸»è¦æ–¹æ¡ˆå®ç°ï¼šä¸€æ˜¯æ‰©å¤§è¾“å…¥æ•°æ®ï¼Œå³å¢å¤§æ¯ä¸ªä»»åŠ¡çš„è¾“å…¥å¯¹åº”çš„å¯¹è¯å¯¹ï¼Œä»¥æé«˜æŒ‡ä»¤éµä»æ€§ã€‚äºŒæ˜¯æ‰©å¤§ä»»åŠ¡æ•°é‡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«ä¸€å¯¹ï¼ˆæŒ‡ä»¤ã€è¾“å‡ºï¼‰ï¼Œè€Œä¸éœ€è¦åˆ†é…å•ç‹¬çš„è¾“å…¥ã€‚ç„¶è€Œï¼Œåœ¨æ‰©å¤§è¾“å…¥æ–¹é¢ï¼ŒLLMs æœ‰å¾ˆå¼ºçš„æ•æ„Ÿæ€§ï¼Œå¯èƒ½å¯¼è‡´æŒ‡ä»¤çš„è¯¯è§£æˆ–ä¸éµä»ã€‚ç›¸åï¼Œæ‰©å¤§ä»»åŠ¡æ•°é‡éœ€è¦å¾ˆå¤šä»»åŠ¡ï¼Œä½†æ˜¯åœ¨æ‰©å¤§è¾“å…¥æ–¹é¢æ•ˆæœè¾ƒå·®ã€‚è¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€ç§æ–°çš„æŒ‡ä»¤éµä»æ•°æ®é›†ç¼–åˆ¶æ–¹æ³•ï¼Œç§°ä¸ºMUFFINã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡è‡ªåŠ¨æ‰©å¤§æ¯ä¸ªä»»åŠ¡çš„è¾“å…¥æ–¹é¢æ¥å¤šå…ƒåŒ–è¿™äº›ä»»åŠ¡ã€‚å®éªŒç»“æœéªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œåœ¨å››ä¸ªé›¶åŸºelineæ ‡å‡† benchmark ä¸­ï¼Œè¦†ç›–äº†ä¸¤ç§æ‰©å¤§è¾“å…¥å’Œæ‰©å¤§ä»»åŠ¡æ•°é‡çš„æ–¹æ¡ˆã€‚ç»“æœè¡¨æ˜ï¼Œ LLMS åœ¨ä¸åŒçš„ç¼©æ”¾çº§åˆ« trained on MUFFIN é€šå¸¸æ¯” trained on è¿™ä¸¤ç§æ–¹æ¡ˆ demonstrate æ›´é«˜çš„æŒ‡ä»¤éµä»èƒ½åŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Visually-Grounded-Language-Learning-a-review-of-language-games-datasets-tasks-and-models"><a href="#Visually-Grounded-Language-Learning-a-review-of-language-games-datasets-tasks-and-models" class="headerlink" title="Visually Grounded Language Learning: a review of language games, datasets, tasks, and models"></a>Visually Grounded Language Learning: a review of language games, datasets, tasks, and models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02431">http://arxiv.org/abs/2312.02431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Suglia, Ioannis Konstas, Oliver Lemon</li>
<li>for: æœ¬æ–‡æ˜¯ä¸€ç¯‡ç³»ç»Ÿæ€§çš„æ–‡çŒ®è¯„è®ºï¼Œæ¢è®¨äº†è§†è§‰è¯­è¨€ï¼ˆV+Lï¼‰é¢†åŸŸä¸­è®¸å¤šä»»åŠ¡å’Œæ¨¡å‹çš„å‘å±•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨ç»´ç‰¹æ ¹heinçš„â€œè¯­è¨€æ¸¸æˆâ€æ€æƒ³åˆ†ç±»äº†V+Lä»»åŠ¡ä¸ºä¸‰ç±»ï¼šæ¨è®ºæ€§æ¸¸æˆã€ç”Ÿæˆæ€§æ¸¸æˆå’Œäº’åŠ¨æ€§æ¸¸æˆã€‚</li>
<li>results: æ–‡ç« åˆ†æäº†ç°æœ‰Literatureï¼Œæå‡ºäº†æœªæ¥ç ”ç©¶åº”è¯¥å…³æ³¨äº’åŠ¨æ¸¸æˆï¼Œå› ä¸ºè‡ªç„¶è¯­è¨€äº¤æµæ˜¯è§£å†³å¯¹è±¡å¼•ç”¨å’Œè¡ŒåŠ¨è®¡åˆ’çš„ambiguityçš„å…³é”®ï¼Œè€Œç‰©ç†å®ç°ä¹Ÿæ˜¯ç†è§£æƒ…å¢ƒå’Œäº‹ä»¶ semanticsçš„é‡è¦å› ç´ ã€‚<details>
<summary>Abstract</summary>
In recent years, several machine learning models have been proposed. They are trained with a language modelling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many Natural Language Understanding and Generation tasks. However, many facets of meaning cannot be learned by ``listening to the radio" only. In the literature, many Vision+Language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein's idea of `language games' to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models.
</details>
<details>
<summary>æ‘˜è¦</summary>
Recently, several machine learning models have been proposed, which are trained with a language modeling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many natural language understanding and generation tasks. However, many aspects of meaning cannot be learned by simply "listening to the radio." In the literature, many vision + language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein's idea of "language games" to categorize such tasks into three different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should focus on interactive games, where communication in natural language is important to resolve ambiguities about object referents and action plans, and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models.
</details></li>
</ul>
<hr>
<h2 id="PartSLIP-Enhancing-Low-Shot-3D-Part-Segmentation-via-Multi-View-Instance-Segmentation-and-Maximum-Likelihood-Estimation"><a href="#PartSLIP-Enhancing-Low-Shot-3D-Part-Segmentation-via-Multi-View-Instance-Segmentation-and-Maximum-Likelihood-Estimation" class="headerlink" title="PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation"></a>PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03015">http://arxiv.org/abs/2312.03015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zyc00/partslip2">https://github.com/zyc00/partslip2</a></li>
<li>paper_authors: Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, Hao Su</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜é›¶æˆ–å‡ ä¸ªshot 3Déƒ¨åˆ† segmentationçš„ç²¾åº¦å’Œå¯æ‰©å±•æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨çš„æ–¹æ³•åŒ…æ‹¬GLIPå’ŒSAMä¸¤ç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠä¸€ç§æ”¹è¿›çš„Expectation-Maximizationç®—æ³•ã€‚</li>
<li>results: å¯¹æ¯”PartSLIPï¼ŒPartSLIP++åœ¨é›¶æˆ–å‡ ä¸ªshot 3D semanticå’Œå®ä¾‹åŸºæœ¬å¯¹è±¡éƒ¨åˆ†åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ã€‚<details>
<summary>Abstract</summary>
Open-world 3D part segmentation is pivotal in diverse applications such as robotics and AR/VR. Traditional supervised methods often grapple with limited 3D data availability and struggle to generalize to unseen object categories. PartSLIP, a recent advancement, has made significant strides in zero- and few-shot 3D part segmentation. This is achieved by harnessing the capabilities of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic method for converting and lifting multi-view 2D bounding box predictions into 3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced version designed to overcome the limitations of its predecessor. Our approach incorporates two major improvements. First, we utilize a pre-trained 2D segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more precise and accurate annotations than the 2D bounding boxes used in PartSLIP. Second, PartSLIP++ replaces the heuristic 3D conversion process with an innovative modified Expectation-Maximization algorithm. This algorithm conceptualizes 3D instance segmentation as unobserved latent variables, and then iteratively refines them through an alternating process of 2D-3D matching and optimization with gradient descent. Through extensive evaluations, we show that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot 3D semantic and instance-based object part segmentation tasks. Code released at https://github.com/zyc00/PartSLIP2.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¼€æ”¾ä¸–ç•Œ3Déƒ¨åˆ†åˆ†å‰²æ˜¯å¤šç§åº”ç”¨ç¨‹åºä¸­çš„å…³é”®ï¼Œå¦‚Roboticså’ŒAR/VRã€‚ä¼ ç»Ÿçš„æŒ‡å¯¼æ–¹æ³•ç»å¸¸å—åˆ°æœ‰é™çš„3Dæ•°æ®å¯ç”¨æ€§çš„é™åˆ¶ï¼Œå¹¶ä¸”å¾ˆéš¾åœ¨æœªè§åˆ°çš„å¯¹è±¡ç±»åˆ«ä¸Šæ³›åŒ–ã€‚PartSLIPï¼Œä¸€ç§æœ€è¿‘çš„è¿›æ­¥ï¼Œåœ¨é›¶å’Œå‡ ä¸ªshot 3Déƒ¨åˆ†åˆ†å‰²æ–¹é¢åšå‡ºäº†é‡è¦çš„çªç ´ã€‚è¿™æ˜¯é€šè¿‡åˆ©ç”¨2Då¼€æ”¾è¯æ±‡æ£€æµ‹æ¨¡å—GLIPçš„èƒ½åŠ›ï¼Œå¹¶å¯¹å¤šè§†å›¾2D bounding boxé¢„æµ‹è¿›è¡Œè½¬æ¢å’Œæå‡ä¸º3Dåˆ†å‰²Masksæ¥å®ç°çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PartSLIP++ï¼Œä¸€ç§æ”¹è¿›ç‰ˆæœ¬ï¼Œæ—¨åœ¨è¶…è¶Šå…¶å‰ä¸€ä»£çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦æ”¹è¿›ã€‚ä¸€æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„2Dåˆ†å‰²æ¨¡å‹SAMï¼Œä»¥ç”Ÿæˆåƒç´ ç²¾åº¦çš„2Dåˆ†å‰²ï¼Œä»è€Œç”Ÿæˆæ›´åŠ å‡†ç¡®å’Œç²¾ç¡®çš„æ³¨é‡Šã€‚äºŒæ˜¯PartSLIP++å°†2D bounding boxé¢„æµ‹è½¬æ¢ä¸º3Dåˆ†å‰²Masksï¼Œè€Œä¸æ˜¯ä½¿ç”¨heuristicçš„3Dè½¬æ¢è¿‡ç¨‹ã€‚é€šè¿‡å¹¿æ³›çš„è¯„ä¼°ï¼Œæˆ‘ä»¬æ˜¾ç¤ºäº†PartSLIP++åœ¨é›¶å’Œå‡ ä¸ªshot 3Dsemanticå’Œå®ä¾‹åŸºäºå¯¹è±¡éƒ¨åˆ†åˆ†å‰²ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½äºPartSLIPã€‚ä»£ç å¯ä»¥åœ¨https://github.com/zyc00/PartSLIP2ä¸­ä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Decoding-Data-Quality-via-Synthetic-Corruptions-Embedding-guided-Pruning-of-Code-Data"><a href="#Decoding-Data-Quality-via-Synthetic-Corruptions-Embedding-guided-Pruning-of-Code-Data" class="headerlink" title="Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data"></a>Decoding Data Quality via Synthetic Corruptions: Embedding-guided Pruning of Code Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02418">http://arxiv.org/abs/2312.02418</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Yang, Aaditya K. Singh, Mostafa Elhoushi, Anas Mahmoud, Kushal Tirumala, Fabian Gloeckle, Baptiste RoziÃ¨re, Carole-Jean Wu, Ari S. Morcos, Newsha Ardalani</li>
<li>for: æé«˜ Large Language Modelsï¼ˆLLMsï¼‰çš„ä»£ç ç”Ÿæˆæ€§èƒ½å’Œè®­ç»ƒæ•ˆç‡ï¼Œé€šè¿‡ removing â€œlow-qualityâ€ code dataã€‚</li>
<li>methods: ä½¿ç”¨ embedding ç©ºé—´æ¥è¯†åˆ«å’Œç§»é™¤ â€œlow-qualityâ€ code dataï¼Œé€šè¿‡ synthetic corruptions æ¥æ¢ç´¢ â€œlow-qualityâ€ code çš„ç‰¹å¾ï¼Œå¹¶å¼€å‘äº†ä¸€äº›åŸºäº embedding ç©ºé—´çš„æ–°çš„é‡‡æ ·ç­–ç•¥ã€‚</li>
<li>results: åœ¨ HumanEval å’Œ MBPP benchmark ä¸Šè¡¨ç°å‡ºä¼˜äºç°æœ‰çš„ embedding-based æ–¹æ³•ï¼Œå¹¶ä¸”å¯ä»¥è¾¾åˆ°ä¸è¿›è¡Œé‡‡æ ·çš„æƒ…å†µä¸‹çš„æ€§èƒ½æå‡ä¸ºé«˜è¾¾ 3%ï¼Œ demonstrating the promise of insights from synthetic corruptions for data pruningã€‚<details>
<summary>Abstract</summary>
Code datasets, often collected from diverse and uncontrolled sources such as GitHub, potentially suffer from quality issues, thereby affecting the performance and training efficiency of Large Language Models (LLMs) optimized for code generation. Previous studies demonstrated the benefit of using embedding spaces for data pruning, but they mainly focused on duplicate removal or increasing variety, and in other modalities, such as images. Our work focuses on using embeddings to identify and remove "low-quality" code data. First, we explore features of "low-quality" code in embedding space, through the use of synthetic corruptions. Armed with this knowledge, we devise novel pruning metrics that operate in embedding space to identify and remove low-quality entries in the Stack dataset. We demonstrate the benefits of this synthetic corruption informed pruning (SCIP) approach on the well-established HumanEval and MBPP benchmarks, outperforming existing embedding-based methods. Importantly, we achieve up to a 3% performance improvement over no pruning, thereby showing the promise of insights from synthetic corruptions for data pruning.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†æ–‡æœ¬ç¿»è¯‘ä¸ºç®€åŒ–ä¸­æ–‡ã€‚<</SYS>>å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæ•ˆç‡å’Œæ€§èƒ½å¯èƒ½å—åˆ°æ¥è‡ª GitHub ç­‰å¤šç§å¤šæ ·çš„æºå¤´çš„ä»£ç æ•°æ®é›†çš„è´¨é‡é—®é¢˜çš„å½±å“ã€‚å…ˆå‰çš„ç ”ç©¶å·²ç»è¯æ˜ä½¿ç”¨åµŒå…¥ç©ºé—´è¿›è¡Œæ•°æ®å‡å°‘å…·æœ‰åˆ©å¤„ï¼Œä½†ä¸»è¦å…³æ³¨äºå»é™¤é‡å¤æˆ–å¢åŠ å¤šæ ·æ€§ï¼Œè€Œåœ¨å…¶ä»–Modalitiesä¸­ï¼Œå¦‚å›¾åƒã€‚æˆ‘ä»¬çš„å·¥ä½œå°†å…³æ³¨ä½¿ç”¨åµŒå…¥ç©ºé—´æ¥è¯†åˆ«å’Œç§»é™¤â€œä½è´¨é‡â€çš„ä»£ç æ•°æ®ã€‚æˆ‘ä»¬é¦–å…ˆé€šè¿‡ä½¿ç”¨ç”Ÿæˆçš„æŸå®³æ¥æ¢ç´¢ä½è´¨é‡ä»£ç çš„ç‰¹å¾åœ¨åµŒå…¥ç©ºé—´ä¸­ï¼Œç„¶åæå‡ºäº†ä¸€ç§åŸºäºåµŒå…¥ç©ºé—´çš„æ–°çš„å‡å°‘æŒ‡æ ‡ï¼Œç”¨äºåœ¨æ ˆ dataset ä¸­æ ‡è¯†å’Œç§»é™¤ä½è´¨é‡é¡¹ç›®ã€‚æˆ‘ä»¬åœ¨äººç±»è¯„ä¼°å’Œ MBPP æ ‡å‡†å‡†ä¸­ç¤ºå‡ºäº†è¿™ç§åŸºäºç”ŸæˆæŸå®³çš„å‡å°‘æ–¹æ³•ï¼ˆSCIPï¼‰çš„ä¼˜åŠ¿ï¼Œå¹¶ä¸”ä¸ç°æœ‰çš„åµŒå…¥ç©ºé—´åŸºæœ¬æ–¹æ³•ç›¸æ¯”ï¼Œè¾¾åˆ°äº†3%çš„æ€§èƒ½æå‡ã€‚è¿™æ˜¾ç¤ºäº†å¯¹äºæ•°æ®å‡å°‘ï¼Œæ¥è‡ªç”ŸæˆæŸå®³çš„å¯ç¤ºå…·æœ‰æ‰¿è¯ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Towards-Fast-and-Stable-Federated-Learning-Confronting-Heterogeneity-via-Knowledge-Anchor"><a href="#Towards-Fast-and-Stable-Federated-Learning-Confronting-Heterogeneity-via-Knowledge-Anchor" class="headerlink" title="Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor"></a>Towards Fast and Stable Federated Learning: Confronting Heterogeneity via Knowledge Anchor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02416">http://arxiv.org/abs/2312.02416</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/J1nqianChen/FedKA">https://github.com/J1nqianChen/FedKA</a></li>
<li>paper_authors: Jinqian Chen, Jihua Zhu, Qinghai Zheng</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯è§£å†³è”åˆå­¦ä¹ ä¸­çš„æ•°æ®ä¸ä¸€è‡´é—®é¢˜ï¼Œå¯¹äºè”åˆæ¨¡å‹çš„æ€§èƒ½å’Œç¨³å®šæ€§æœ‰ç€é‡è¦çš„å½±å“ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†æœ¬åœ°è®­ç»ƒå’Œå…±äº«çŸ¥è¯†é“¾æ¥è§£å†³æ•°æ®ä¸ä¸€è‡´é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œ Federated Knowledge Anchorâ€çš„æ–°ç®—æ³•ã€‚</li>
<li>results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¿™ä¸ªæ–°ç®—æ³•å¯ä»¥å®ç°å¿«é€Ÿå’Œç¨³å®šçš„è”åˆæ¨¡å‹è®­ç»ƒï¼Œå¹¶æœ‰ç€ä¼˜åŒ–å‡†ç¡®æ€§çš„æ•ˆæœã€‚<details>
<summary>Abstract</summary>
Federated learning encounters a critical challenge of data heterogeneity, adversely affecting the performance and convergence of the federated model. Various approaches have been proposed to address this issue, yet their effectiveness is still limited. Recent studies have revealed that the federated model suffers severe forgetting in local training, leading to global forgetting and performance degradation. Although the analysis provides valuable insights, a comprehensive understanding of the vulnerable classes and their impact factors is yet to be established. In this paper, we aim to bridge this gap by systematically analyzing the forgetting degree of each class during local training across different communication rounds. Our observations are: (1) Both missing and non-dominant classes suffer similar severe forgetting during local training, while dominant classes show improvement in performance. (2) When dynamically reducing the sample size of a dominant class, catastrophic forgetting occurs abruptly when the proportion of its samples is below a certain threshold, indicating that the local model struggles to leverage a few samples of a specific class effectively to prevent forgetting. Motivated by these findings, we propose a novel and straightforward algorithm called Federated Knowledge Anchor (FedKA). Assuming that all clients have a single shared sample for each class, the knowledge anchor is constructed before each local training stage by extracting shared samples for missing classes and randomly selecting one sample per class for non-dominant classes. The knowledge anchor is then utilized to correct the gradient of each mini-batch towards the direction of preserving the knowledge of the missing and non-dominant classes. Extensive experimental results demonstrate that our proposed FedKA achieves fast and stable convergence, significantly improving accuracy on popular benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
Federated learning é¢ä¸´ç€æ•°æ®å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œå¯¼è‡´æ¨¡å‹æ€§èƒ½å’Œèåˆå—åˆ°å½±å“ã€‚è®¸å¤šæ–¹æ³•å·²ç»è¢«æå‡ºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†å…¶æ•ˆæœä»ç„¶æœ‰é™ã€‚æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œåœ¨æœ¬åœ°è®­ç»ƒä¸­ï¼Œè”é‚¦æ¨¡å‹ä¼šå‡ºç°ä¸¥é‡çš„å¿˜è®°ç°è±¡ï¼Œå¯¼è‡´å…¨å±€å¿˜è®°å’Œæ€§èƒ½ä¸‹é™ã€‚è™½ç„¶åˆ†ææä¾›äº†æœ‰ä»·å€¼çš„æ„è§ï¼Œä½†completeçš„æ„Ÿå—åˆ°æ˜“å—æŸç±»å’Œå…¶å½±å“å› ç´ ä»æœªå¾—åˆ°å½»åº•çš„ç†è§£ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦å¡«è¡¥è¿™ä¸ªå·®è·ï¼Œé€šè¿‡è¯„ä¼°æ¯ä¸ªç±»å¿˜è®°ç¨‹åº¦çš„å˜åŒ–æ¥ç³»ç»Ÿåœ°åˆ†æå¿˜è®°ç°è±¡ã€‚æˆ‘ä»¬çš„è§‚å¯Ÿç»“æœæ˜¯ï¼šï¼ˆ1ï¼‰ç¼ºå¤±å’Œéä¸»æµç±»åœ¨æœ¬åœ°è®­ç»ƒä¸­éƒ½ä¼šåŒæ ·ä¸¥é‡å¿˜è®°ï¼Œè€Œä¸»æµç±»åˆ™ä¼šåœ¨æ€§èƒ½æ–¹é¢è¡¨ç°å‡ºæ”¹å–„ã€‚ï¼ˆ2ï¼‰å½“åŠ¨æ€å‡å°‘ä¸»æµç±»çš„æ ·æœ¬æ•°æ—¶ï¼Œå½“å…¶æ ·æœ¬å æ¯”ä¸‹é™åˆ°æŸä¸ªé˜ˆå€¼æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­å¿˜è®°ç°è±¡ä¼šçªç„¶å‘ç”Ÿï¼Œè¡¨æ˜æœ¬åœ°æ¨¡å‹å¾ˆéš¾ä»¥é€šè¿‡å‡ ä¸ªç‰¹å®šç±»çš„æ ·æœ¬æ¥é˜²æ­¢å¿˜è®°ã€‚è¿™äº›å‘ç°ä½¿æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„å’Œç®€å•çš„ç®—æ³•â€”â€”è”é‚¦çŸ¥è¯†é”šï¼ˆFedKAï¼‰ã€‚å‡è®¾æ‰€æœ‰å®¢æˆ·ç«¯å‡æ‹¥æœ‰æ¯ä¸ªç±»å‹çš„å”¯ä¸€çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªæœ¬åœ°è®­ç»ƒé˜¶æ®µå‰ construct çŸ¥è¯†é”šï¼Œé€šè¿‡æå–ç¼ºå¤±ç±»å‹çš„ç¤ºä¾‹å’Œéšæœºé€‰æ‹©æ¯ä¸ªç±»å‹çš„ä¸€ä¸ªç¤ºä¾‹æ¥å»ºç«‹çŸ¥è¯†é”šã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªmini-batchä¸­ä½¿ç”¨çŸ¥è¯†é”šæ¥æ›´æ­£æ¢¯åº¦çš„æ–¹å‘ï¼Œä»¥ä¿æŒç¼ºå¤±å’Œéä¸»æµç±»å‹çš„çŸ¥è¯†ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æè®®çš„FedKAå¯ä»¥å¿«é€Ÿå’Œç¨³å®šåœ°èåˆï¼Œåœ¨æµè¡Œçš„benchmarkä¸Šæ˜¾è‘—æé«˜å‡†ç¡®ç‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Foundation-Models-for-Weather-and-Climate-Data-Understanding-A-Comprehensive-Survey"><a href="#Foundation-Models-for-Weather-and-Climate-Data-Understanding-A-Comprehensive-Survey" class="headerlink" title="Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey"></a>Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03014">http://arxiv.org/abs/2312.03014</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shengchaochen82/awesome-large-models-for-weather-and-climate">https://github.com/shengchaochen82/awesome-large-models-for-weather-and-climate</a></li>
<li>paper_authors: Shengchao Chen, Guodong Long, Jing Jiang, Dikai Liu, Chengqi Zhang</li>
<li>for: This paper is written to provide an overview of state-of-the-art AI methodologies for weather and climate data, with a focus on time series and text data.</li>
<li>methods: The paper discusses various model architectures, including large language models (LLMs), and their applications in weather and climate data understanding.</li>
<li>results: The paper provides an exhaustive review of current breakthroughs in research on large, data-driven models for weather and climate data understanding, including practical applications, crucial resources, and prospective research opportunities.<details>
<summary>Abstract</summary>
As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„å¿«é€Ÿå‘å±•ï¼Œåœ°çƒå’Œå¤§æ°”ç§‘å­¦é¢†åŸŸæ­£åœ¨è¶Šæ¥è¶Šå¤šåœ°é‡‡ç”¨æ•°æ®é©±åŠ¨æ¨¡å‹ï¼Œå—åˆ°è¿›æ­¥çš„æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰æŠ€æœ¯çš„æ¨åŠ¨ã€‚ç‰¹åˆ«æ˜¯é€šè¿‡è§£ç åœ°çƒç³»ç»Ÿä¸­çš„å¤æ‚å’Œéçº¿æ€§æ–¹é¢ï¼Œä»¥åŠç†è§£å¤©æ°”å’Œæ°”å€™æ•°æ®ï¼ŒDLæŠ€æœ¯åœ¨è§£å†³æ°”å€™æŒ‘æˆ˜æ–¹é¢å‘æŒ¥äº†å…³é”®ä½œç”¨ã€‚æœ€è¿‘ï¼Œé€šè¿‡å¤§å‹æ¨¡å‹çš„è¿›æ­¥ï¼Œç‰¹åˆ«æ˜¯å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¯ä»¥è¿›è¡Œç»†åŒ–è¿‡ç¨‹ï¼Œä»è€Œåœ¨å„ç§ä¸‹æ¸¸ä»»åŠ¡ä¸­å®ç°å‡ºè‰²çš„è¡¨ç°ï¼Œä»è€Œæ¨åŠ¨æ€»AIçš„å‘å±•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¿˜å¤„äºæ°”å€™å’Œå¤©æ°”é¢†åŸŸçš„æ™®é€šAIåˆ›é€ çš„åˆ stagesã€‚åœ¨è¿™ç¯‡è¯„è®ºä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä»½å…¨é¢ã€æ—¶å®œçš„è¯„è®ºï¼Œæ¶µç›–äº†å¤©æ°”å’Œæ°”å€™æ•°æ®çš„ç±»å‹ã€ä¸»è¦æ¨¡å‹æ¶æ„ã€æ¨¡å‹èŒƒå›´å’Œåº”ç”¨ã€ä»¥åŠå¤©æ°”å’Œæ°”å€™æ•°æ®çš„æ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†åˆ›å»ºå’Œåº”ç”¨æ°”å€™å’Œå¤©æ°”æ•°æ®ç†è§£åŸºç¡€æ¨¡å‹çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†å…³é”®çš„æ´å¯Ÿå’Œè¯¦ç»†çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚è¿™ç§å…¨é¢çš„æ–¹æ³•ä½¿å¾—å®è·µè€…å¯ä»¥å¿«é€ŸæŒæ¡è¿™ä¸ªé¢†åŸŸçš„å¿…è¦çŸ¥è¯†ï¼Œä»è€Œå–å¾—é‡è¦è¿›æ­¥ã€‚æˆ‘ä»¬çš„è¯„è®ºæ±‡é›†äº†æœ€è¿‘åœ¨å¤§ã€æ•°æ®é©±åŠ¨æ¨¡å‹æ–¹é¢çš„ç ”ç©¶è¿›å±•ï¼Œå¼ºè°ƒåšå®çš„åŸºç¡€ã€å½“å‰è¿›æ­¥ã€å®ç”¨åº”ç”¨ã€å…³é”®èµ„æºå’Œæœªæ¥ç ”ç©¶æœºé‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="BEDD-The-MineRL-BASALT-Evaluation-and-Demonstrations-Dataset-for-Training-and-Benchmarking-Agents-that-Solve-Fuzzy-Tasks"><a href="#BEDD-The-MineRL-BASALT-Evaluation-and-Demonstrations-Dataset-for-Training-and-Benchmarking-Agents-that-Solve-Fuzzy-Tasks" class="headerlink" title="BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks"></a>BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02405">http://arxiv.org/abs/2312.02405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minerllabs/basalt-benchmark">https://github.com/minerllabs/basalt-benchmark</a></li>
<li>paper_authors: Stephanie Milani, Anssi Kanervisto, Karolis Ramanauskas, Sander Schulhoff, Brandon Houghton, Rohin Shah</li>
<li>for: æœ¬æ–‡æä¾›äº†ä¸€ä¸ªå½¢å¼åŒ–çš„æµ‹è¯•åŸºå‡† Ğ´Ğ»Ñå­¦ä¹ äººç±»åé¦ˆï¼Œä»¥ä¾¿è¯„ä¼°æ–°å¼€å‘çš„ç®—æ³•æ€§èƒ½ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº† Minecraft æ¸¸æˆä¸­çš„å››ä¸ªå›°éš¾ä»»åŠ¡ï¼Œä¾‹å¦‚åˆ›å»ºå’Œæ‹æ‘„ç€‘å¸ƒï¼Œæ¥æµ‹è¯•äººç±»åé¦ˆå­¦ä¹ ç®—æ³•ã€‚</li>
<li>results: æœ¬æ–‡æä¾›äº†2600ä¸‡ä¸ªå›¾åƒåŠ¨ä½œå¯¹çš„é›†åˆï¼Œä»¥åŠè¶…è¿‡3000ä¸ªç²¾å¯†å¯¹æ¯”äººç±»å’Œç®—æ³•ä»£ç†çš„è¯„ä¼°ç»“æœï¼Œä»¥ä¾¿è¯„ä¼°æ–°å¼€å‘çš„ç®—æ³•æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These comparisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms. To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark .
</details>
<details>
<summary>æ‘˜è¦</summary>
minesrl çš„ BASALT æ¯”èµ›å·²ç»catalyzed äº†äººç±»åé¦ˆå­¦ä¹ çš„è¿›æ­¥ï¼Œé€šè¿‡å››ä¸ªå›°éš¾å®šä¹‰çš„ä»»åŠ¡ï¼ˆå¦‚åˆ›å»ºå’Œæ‹æ‘„ç€‘å¸ƒï¼‰åœ¨ Minecraft ä¸­è¿›è¡Œã€‚å·²ç»è¿‡å»ä¸¤å¹´çš„ BASALT æ¯”èµ›ï¼Œæˆ‘ä»¬ç°åœ¨å‘ç¤¾åŒºæä¾›ä¸€ä¸ªæ­£å¼çš„æ ‡å‡† bencmarkï¼Œé€šè¿‡ BASALT è¯„ä¼°å’Œç¤ºä¾‹ Dataset (BEDD)ã€‚BEDD åŒ…æ‹¬2600ä¸‡ä¸ªå›¾åƒåŠ¨ä½œå¯¹çš„æ”¶é›†å’Œnearly 14,000ä¸ª Minecraft æ¸¸æˆè§†é¢‘ä¸­çš„äººç±»ç©å®¶å®Œæˆ BASALT ä»»åŠ¡çš„14,000ä¸ªè§†é¢‘ã€‚å®ƒè¿˜åŒ…æ‹¬äº†3,000ä¸ªç´§å¯†çš„äººç±»è¯„ä¼°å¯¹äººç±»å’Œç®—æ³•ä»£ç†çš„å¯¹æ¯”ã€‚è¿™äº›å¯¹æ¯”ç”¨ä½œä¸€ä¸ªå›ºå®šçš„ã€åˆæ­¥çš„eaderboard æ¥è¯„ä¼°æ–°å‘å±•çš„ç®—æ³•ã€‚ä¸ºäº†å®ç°è¿™ä¸€æ¯”è¾ƒï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ª Streamlined çš„ä»£ç åº“æ¥æµ‹è¯•æ–°çš„ç®—æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº† BASALT æ•°æ®é›†å’Œä»£ç çš„è¯¦ç»†åˆ†æï¼Œä»¥å¸®åŠ©ç®—æ³•çš„å¼€å‘å’Œè¯„ä¼°ã€‚å·²ç»å‘å¸ƒçš„ä»£ç å’Œæ•°æ®å¯ä»¥åœ¨ <https://github.com/minerllabs/basalt-benchmark> ä¸Šæ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Breast-Ultrasound-Report-Generation-using-LangChain"><a href="#Breast-Ultrasound-Report-Generation-using-LangChain" class="headerlink" title="Breast Ultrasound Report Generation using LangChain"></a>Breast Ultrasound Report Generation using LangChain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03013">http://arxiv.org/abs/2312.03013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaeyoung Huh, Hyun Jeong Park, Jong Chul Ye<br>for: è¿™ç¯‡ç ”ç©¶æ—¨åœ¨æé«˜ä¹³è…ºè¶…éŸ³æ³¢æˆåƒçš„è¯Šæ–­æ•ˆç‡å’ŒæŠ¥å‘Šè´¨é‡ï¼Œå‡è½»åŒ»ç”Ÿå’ŒåŒ»ç–—ä¸“ä¸šäººå‘˜çš„è´Ÿæ‹…ã€‚methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºLangChainçš„å¤šå›¾åˆ†æå·¥å…·é›†æˆæ–¹æ³•ï¼Œé€šè¿‡èåˆä¸“é—¨çš„å·¥å…·å’Œè‡ªç„¶è¯­è¨€ç”ŸæˆæŠ€æœ¯ï¼Œä»è¶…éŸ³æ³¢å›¾åƒä¸­æå–æœ‰ç”¨çš„ç‰¹å¾ï¼Œåœ¨åŒ»ç–—ä¸Šè¿›è¡Œè§£é‡Šå’Œç”Ÿæˆæ ‡å‡†åŒ–çš„æŠ¥å‘Šã€‚results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¯ä¸ªå‚è€ƒå·¥å…·å‡å¯ä»¥æä¾›æœ‰è´¨é‡å’Œé‡ä¸Šçš„é‡è¦ç»“æœï¼Œè€Œä¸”å¹¶ä¸éœ€è¦ä¸“ä¸šäººå‘˜çš„å¹²é¢„ã€‚å¹¶ä¸”ï¼Œåœ¨ä¸´åºŠè¯„ä¼°ä¸­ï¼Œç”Ÿæˆçš„æŠ¥å‘Šè¢«è¯„ä¼°ä¸ºå…·æœ‰ä¸´åºŠæ„ä¹‰ã€‚<details>
<summary>Abstract</summary>
Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast imaging, aiding in the early detection and characterization of breast abnormalities. Interpreting breast ultrasound images commonly involves creating comprehensive medical reports, containing vital information to promptly assess the patient's condition. However, the ultrasound imaging system necessitates capturing multiple images of various parts to compile a single report, presenting a time-consuming challenge. To address this problem, we propose the integration of multiple image analysis tools through a LangChain using Large Language Models (LLM), into the breast reporting process. Through a combination of designated tools and text generation through LangChain, our method can accurately extract relevant features from ultrasound images, interpret them in a clinical context, and produce comprehensive and standardized reports. This approach not only reduces the burden on radiologists and healthcare professionals but also enhances the consistency and quality of reports. The extensive experiments shows that each tools involved in the proposed method can offer qualitatively and quantitatively significant results. Furthermore, clinical evaluation on the generated reports demonstrates that the proposed method can make report in clinically meaningful way.
</details>
<details>
<summary>æ‘˜è¦</summary>
breast ultrasound (BUS) æ˜¯ä¸€ç§é‡è¦çš„è¯Šæ–­å·¥å…·åœ¨ä¹³è…ºå½±åƒé¢†åŸŸï¼Œå¸®åŠ©æ—©æœŸå‘ç°å’Œç‰¹å¾åŒ–ä¹³è…ºç•¸å½¢ã€‚ è§£è¯»ä¹³è…ºè¶…å£°å›¾åƒé€šå¸¸éœ€è¦åˆ›å»ºå…¨é¢çš„åŒ»ç–—æŠ¥å‘Šï¼ŒåŒ…å«é‡è¦ä¿¡æ¯ä»¥è¯„ä¼°ç—…äººçš„æƒ…å†µã€‚ç„¶è€Œï¼Œè¶…å£°å½±åƒç³»ç»Ÿéœ€è¦æ•æ‰å¤šä¸ªå›¾åƒæ¥ç¼–è¾‘æŠ¥å‘Šï¼Œè¿™æ˜¯ä¸€é¡¹æ—¶é—´consumingçš„æŒ‘æˆ˜ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æè®®é€šè¿‡LangChainä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰integrateå¤šä¸ªå›¾åƒåˆ†æå·¥å…·åˆ°ä¹³è…ºæŠ¥å‘Šè¿‡ç¨‹ä¸­ã€‚é€šè¿‡ç»¼åˆä½¿ç”¨æŒ‡å®šå·¥å…·å’Œæ–‡æœ¬ç”Ÿæˆthrough LangChainï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥å‡†ç¡®æå–è¶…å£°å›¾åƒä¸­é‡è¦ç‰¹å¾ï¼Œåœ¨ä¸´åºŠä¸Šä¸‹æ–‡ä¸­è§£é‡Šå®ƒä»¬ï¼Œå¹¶ç”Ÿæˆå®Œæ•´ã€æ ‡å‡†åŒ–çš„æŠ¥å‘Šã€‚è¿™ç§æ–¹æ³•ä¸ä»…å‡è½»äº†åŒ»ç”Ÿå’ŒåŒ»ç–—ä¸“ä¸šäººå‘˜çš„è´Ÿæ‹…ï¼Œè¿˜æé«˜äº†æŠ¥å‘Šçš„ä¸€è‡´æ€§å’Œè´¨é‡ã€‚ç»éªŒè¡¨æ˜ï¼Œæ¯ç§å·¥å…·å‚ä¸çš„æ–¹æ³•å¯ä»¥æä¾›è´¨é‡å’Œé‡ä¸Šçš„æ˜¾è‘—ç»“æœã€‚æ­¤å¤–ï¼Œä¸´åºŠè¯„ä¼°ç”Ÿæˆçš„æŠ¥å‘Šä¹Ÿè¡¨æ˜è¯¥æ–¹æ³•å¯ä»¥åœ¨ä¸´åºŠæ„ä¹‰ä¸Šç”ŸæˆæŠ¥å‘Šã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/05/cs.AI_2023_12_05/" data-id="clq0ru6py008mto88c7za93xf" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_12_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/05/cs.CL_2023_12_05/" class="article-date">
  <time datetime="2023-12-05T11:00:00.000Z" itemprop="datePublished">2023-12-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/05/cs.CL_2023_12_05/">cs.CL - 2023-12-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Combining-Counting-Processes-and-Classification-Improves-a-Stopping-Rule-for-Technology-Assisted-Review"><a href="#Combining-Counting-Processes-and-Classification-Improves-a-Stopping-Rule-for-Technology-Assisted-Review" class="headerlink" title="Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review"></a>Combining Counting Processes and Classification Improves a Stopping Rule for Technology Assisted Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03171">http://arxiv.org/abs/2312.03171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/reembinhezam/tar_stopping_cp_clf">https://github.com/reembinhezam/tar_stopping_cp_clf</a></li>
<li>paper_authors: Reem Bin-Hezam, Mark Stevenson</li>
<li>for: é™ä½æ‰‹åŠ¨å®¡æŸ¥æ–‡æ¡£ç›¸å…³æ€§çš„æˆæœ¬ï¼Œä»¥ç¡®ä¿æ‰€éœ€çš„ç²¾ç¡®ç‡æ°´å‡†ã€‚</li>
<li>methods: ä½¿ç”¨æ–‡æœ¬åˆ†ç±»å™¨ derivable æ— éœ€ä»»ä½•é¢å¤–æ ‡æ³¨è€Œè®­ç»ƒã€‚</li>
<li>results: åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆCLEF e-Healthã€TREC Total Recallã€TREC Legalå’ŒRCV1ï¼‰çš„å®éªŒä¸­ï¼Œæå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—æé«˜ï¼Œå¹¶æ¯”å¤šä¸ªæ›¿ä»£æ–¹æ³•è¡¨ç°æ›´å¥½ã€‚<details>
<summary>Abstract</summary>
Technology Assisted Review (TAR) stopping rules aim to reduce the cost of manually assessing documents for relevance by minimising the number of documents that need to be examined to ensure a desired level of recall. This paper extends an effective stopping rule using information derived from a text classifier that can be trained without the need for any additional annotation. Experiments on multiple data sets (CLEF e-Health, TREC Total Recall, TREC Legal and RCV1) showed that the proposed approach consistently improves performance and outperforms several alternative methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
æŠ€æœ¯ååŠ©è¯„å®¡ï¼ˆTARï¼‰åœæ­¢è§„åˆ™ç›®çš„æ˜¯é™ä½æ‰‹åŠ¨è¯„ä¼°æ–‡æ¡£ç›¸å…³æ€§çš„æˆæœ¬ï¼Œæœ€å°åŒ–éœ€è¦è¯„å®¡çš„æ–‡æ¡£æ•°é‡ï¼Œä»¥ç¡®ä¿æ‰€éœ€çš„å›å½’ç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ”¹è¿›çš„åœæ­¢è§„åˆ™ï¼Œä½¿ç”¨åŸºäºæ–‡æœ¬åˆ†ç±»å™¨çš„ä¿¡æ¯ï¼Œä¸éœ€è¦é¢å¤–çš„æ ‡æ³¨ã€‚åœ¨å¤šä¸ªæ•°æ®é›†ï¼ˆCLEF e-Healthã€TREC Total Recallã€TREC Legalå’ŒRCV1ï¼‰çš„å®éªŒä¸­ï¼Œæå‡ºçš„æ–¹æ³•åœ¨æ€§èƒ½ä¸Šæœ‰æ˜¾è‘—æ”¹å–„ï¼Œå¹¶è¶…è¿‡äº†ä¸€äº›æ›¿ä»£æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Assertion-Enhanced-Few-Shot-Learning-Instructive-Technique-for-Large-Language-Models-to-Generate-Educational-Explanations"><a href="#Assertion-Enhanced-Few-Shot-Learning-Instructive-Technique-for-Large-Language-Models-to-Generate-Educational-Explanations" class="headerlink" title="Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations"></a>Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03122">http://arxiv.org/abs/2312.03122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tasmia Shahriar, Noboru Matsuda, Kelly Ramos</li>
<li>for: æé«˜ Intelligent Tutoring Systems çš„è§£é‡Šè´¨é‡ï¼Œä½¿å…¶èƒ½å¤Ÿå¦‚äººæ•™å¸ˆä¸€æ ·ï¼Œé€šè¿‡å‡ ä¸ªç¤ºä¾‹æ¥æä¾›ç»†èŠ‚ oriented çš„æ•™è‚²è§£é‡Šã€‚</li>
<li>methods: æå‡º Assertion Enhanced Few-Shot Learning æŠ€æœ¯ï¼Œé€šè¿‡å¯¹å‡ ä¸ªç¤ºä¾‹è¿›è¡Œæ‰¹å¤„ç†ï¼Œæé«˜è§£é‡Šçš„å‡†ç¡®ç‡å’Œè´¨é‡ã€‚</li>
<li>results: å¯¹ 12 åå®é™…æ•™å¸ˆè¿›è¡Œæ¯”è¾ƒç ”ç©¶ï¼Œæ˜¾ç¤º Assertion Enhanced Few-Shot Learning æé«˜è§£é‡Šå‡†ç¡®ç‡ 15%ï¼Œå¹¶ä¸”ç”Ÿæˆçš„è§£é‡Šè´¨é‡æ›´é«˜ï¼Œè¢«æ•™å¸ˆè¯„ä»·ä¸ºæ›´åŠ  educator-friendlyã€‚<details>
<summary>Abstract</summary>
Human educators possess an intrinsic ability to anticipate and seek educational explanations from students, which drives them to pose thought-provoking questions when students cannot articulate these explanations independently. We aim to imbue Intelligent Tutoring Systems with this ability using few-shot learning capability of Large Language Models. Our work proposes a novel prompting technique, Assertion Enhanced Few-Shot Learning, to facilitate the generation of accurate, detailed oriented educational explanations. Our central hypothesis is that, in educational domain, few-shot demonstrations are necessary but not a sufficient condition for quality explanation generation. We conducted a study involving 12 in-service teachers, comparing our approach to Traditional Few-Shot Learning. The results show that Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and yields higher-quality explanations, as evaluated by teachers. We also conduct a qualitative ablation study to factor the impact of assertions to provide educator-friendly prompting guidelines for generating explanations in their domain of interest.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººç±»æ•™è‚²è€…å…·æœ‰å†…åœ¨çš„èƒ½åŠ›ï¼Œå¯ä»¥é¢„æµ‹å’Œå¯»æ‰¾å­¦ç”Ÿä¸èƒ½ç‹¬ç«‹è¡¨è¾¾çš„æ•™è‚²è§£é‡Šï¼Œè¿™ä¼šè®©æ•™è‚²è€…æé—®å­¦ç”Ÿæ— æ³•ç­”å¤çš„é—®é¢˜ã€‚æˆ‘ä»¬æƒ³ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹çš„å‡ æ‹›å­¦ä¹ èƒ½åŠ›ï¼Œè®©æ™ºèƒ½æ•™è‚²ç³»ç»Ÿæ‹¥æœ‰è¿™ç§èƒ½åŠ›ã€‚æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ç§æ–°çš„æé—®æŠ€å·§ï¼Œå³æ–­è¨€å¢å¼ºå‡ æ‹›å­¦ä¹ ï¼Œä»¥ä¾¿ç”Ÿæˆé«˜è´¨é‡ã€è¯¦ç»†çš„æ•™è‚²è§£é‡Šã€‚æˆ‘ä»¬çš„ä¸­å¿ƒå‡è®¾æ˜¯ï¼Œåœ¨æ•™è‚²é¢†åŸŸï¼Œå‡ æ‹›ç¤ºèŒƒæ˜¯å¿…è¦çš„ï¼Œä½†ä¸æ˜¯å”¯ä¸€çš„æ¡ä»¶ï¼Œä»¥è·å¾—é«˜è´¨é‡çš„è§£é‡Šã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€é¡¹ç ”ç©¶ï¼Œä¸12åç°å½¹æ•™å¸ˆè¿›è¡Œæ¯”è¾ƒï¼Œä¸ä¼ ç»Ÿå‡ æ‹›å­¦ä¹ ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æé«˜è§£é‡Šå‡†ç¡®ç‡15%ï¼Œå¹¶ç”Ÿæˆæ›´é«˜è´¨é‡çš„è§£é‡Šï¼Œå¦‚æ•™å¸ˆæ‰€è¯„ä»·ã€‚æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€é¡¹è§£é‡Šå› ç´ åˆ†æç ”ç©¶ï¼Œä»¥äº†è§£æ–­è¨€å¯¹ç”Ÿæˆè§£é‡Šçš„å½±å“ï¼Œä»¥æä¾›æ•™å¸ˆåœ¨ä»–ä»¬å…´è¶£é¢†åŸŸä¸­ç”Ÿæˆè§£é‡Šçš„æ•™ç¨‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Mismatch-Quest-Visual-and-Textual-Feedback-for-Image-Text-Misalignment"><a href="#Mismatch-Quest-Visual-and-Textual-Feedback-for-Image-Text-Misalignment" class="headerlink" title="Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"></a>Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03766">http://arxiv.org/abs/2312.03766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Gordon, Yonatan Bitton, Yonatan Shafir, Roopal Garg, Xi Chen, Dani Lischinski, Daniel Cohen-Or, Idan Szpektor</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æä¾›æ–‡æœ¬å’Œå›¾åƒå¯¹é½æ£€æµ‹ä¸­çš„ç»†èŠ‚è§£é‡Šï¼Œä»¥å¸®åŠ©æ£€æµ‹åˆ°çš„åç§»ç²¾ç¡®åœ°å®šä½ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹å’Œè§†è§‰å›ºå®šæ¨¡å‹è‡ªåŠ¨æ„å»ºä¸€ä¸ªåŒ…å«å¯èƒ½çš„åç§»æè¿°å’Œç›¸å…³å›¾åƒæŒ‡ç¤ºçš„è®­ç»ƒé›†ï¼Œå¹¶å‘å¸ƒä¸€ä¸ªæ–°çš„äººå·¥ç²¾å¿ƒæ ‡æ³¨çš„æµ‹è¯•é›†ã€‚</li>
<li>results: å¯¹äºæ–‡æœ¬å’Œå›¾åƒå¯¹é½æ£€æµ‹ä»»åŠ¡ï¼Œ fine-tuningè§†è§‰è¯­è¨€æ¨¡å‹ä½¿å…¶èƒ½å¤Ÿè¯¦ç»†æè¿°åç§»å’Œåœ¨å›¾åƒä¸­æŒ‡ç¤ºå®ƒä»¬ï¼Œè¶…è¶Šäº†å¼ºåŸºelineã€‚<details>
<summary>Abstract</summary>
While existing image-text alignment models reach high quality binary assessments, they fall short of pinpointing the exact source of misalignment. In this paper, we present a method to provide detailed textual and visual explanation of detected misalignments between text-image pairs. We leverage large language models and visual grounding models to automatically construct a training set that holds plausible misaligned captions for a given image and corresponding textual explanations and visual indicators. We also publish a new human curated test set comprising ground-truth textual and visual misalignment annotations. Empirical results show that fine-tuning vision language models on our training set enables them to articulate misalignments and visually indicate them within images, outperforming strong baselines both on the binary alignment classification and the explanation generation tasks. Our method code and human curated test set are available at: https://mismatch-quest.github.io/
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„å›¾æ–‡å¯¹alignmentæ¨¡å‹å¯ä»¥ Ğ´Ğ¾ÑÑ‚Ğ¸Ñ‡ÑŒé«˜è´¨é‡çš„äºŒè¿›åˆ¶è¯„ä¼°ï¼Œä½†å®ƒä»¬æ— æ³•å‡†ç¡®åœ°æ‰¾åˆ°å¼‚å¸¸çš„æºå¤´ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥ä¸ºå›¾æ–‡å¯¹æä¾›ç»†è‡´çš„æ–‡æœ¬å’Œè§†è§‰è§£é‡Šã€‚æˆ‘ä»¬åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å’Œè§†è§‰å›ºå®šæ¨¡å‹æ¥è‡ªåŠ¨æ„å»ºä¸€ä¸ªåŸ¹è®­é›†ï¼Œå…¶ä¸­åŒ…å«å¯ä¿¡çš„å¼‚å¸¸æ ‡æ³¨å’Œå¯¹åº”çš„æ–‡æœ¬è§£é‡Šå’Œè§†è§‰æŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„äººå·¥ç²¾å¿ƒæ ‡æ³¨æµ‹è¯•é›†ï¼Œå…¶ä¸­åŒ…å«äº†å‡†ç¡®çš„æ–‡æœ¬å’Œè§†è§‰å¼‚å¸¸æ ‡æ³¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨æˆ‘ä»¬çš„åŸ¹è®­é›†ä¸Šç»ƒåŒ–è§†è¯­è¨€æ¨¡å‹å¯ä»¥è¯¦ç»†æè¿°å¼‚å¸¸å’Œåœ¨å›¾åƒä¸­è§†è§‰æŒ‡æ˜å®ƒä»¬ï¼Œè¶…è¿‡äº†å¼ºå¤§çš„åŸºelineã€‚æˆ‘ä»¬çš„æ–¹æ³•ä»£ç å’Œäººå·¥ç²¾å¿ƒæ ‡æ³¨æµ‹è¯•é›†å¯ä»¥åœ¨ï¼šhttps://mismatch-quest.github.io/  accessed.
</details></li>
</ul>
<hr>
<h2 id="Understanding-Environmental-Posts-Sentiment-and-Emotion-Analysis-of-Social-Media-Data"><a href="#Understanding-Environmental-Posts-Sentiment-and-Emotion-Analysis-of-Social-Media-Data" class="headerlink" title="Understanding Environmental Posts: Sentiment and Emotion Analysis of Social Media Data"></a>Understanding Environmental Posts: Sentiment and Emotion Analysis of Social Media Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03095">http://arxiv.org/abs/2312.03095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniyar Amangeldi, Aida Usmanova, Pakizar Shamoi</li>
<li>for: This study aims to analyze the public perception of climate change and the environment through social media data from 2014 to 2023, in order to provide insights that can help raise awareness and inform environmental interventions.</li>
<li>methods: The study uses the Pointwise Mutual Information (PMI) algorithm to identify sentiment and explore prevailing emotions expressed within environmental tweets on Twitter, Reddit, and YouTube. The accuracy of the algorithm was compared to human annotation and expert rating.</li>
<li>results: The study finds that negative environmental tweets are more common than positive or neutral ones, with climate change, air quality, emissions, plastic, and recycling being the most discussed topics. The most common emotions in environmental tweets are fear, trust, and anticipation, demonstrating the complex and wide-ranging nature of public reactions to environmental issues.<details>
<summary>Abstract</summary>
Social media is now the predominant source of information due to the availability of immediate public response. As a result, social media data has become a valuable resource for comprehending public sentiments. Studies have shown that it can amplify ideas and influence public sentiments. This study analyzes the public perception of climate change and the environment over a decade from 2014 to 2023. Using the Pointwise Mutual Information (PMI) algorithm, we identify sentiment and explore prevailing emotions expressed within environmental tweets across various social media platforms, namely Twitter, Reddit, and YouTube. Accuracy on a human-annotated dataset was 0.65, higher than Vader score but lower than that of an expert rater (0.90). Our findings suggest that negative environmental tweets are far more common than positive or neutral ones. Climate change, air quality, emissions, plastic, and recycling are the most discussed topics on all social media platforms, highlighting its huge global concern. The most common emotions in environmental tweets are fear, trust, and anticipation, demonstrating public reactions wide and complex nature. By identifying patterns and trends in opinions related to the environment, we hope to provide insights that can help raise awareness regarding environmental issues, inform the development of interventions, and adapt further actions to meet environmental challenges.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç¤¾äº¤åª’ä½“ç°åœ¨æ˜¯ä¿¡æ¯çš„ä¸»è¦æ¥æºï¼Œå› ä¸ºå®ƒæä¾›äº†å³æ—¶çš„å…¬ä¼—ååº”ã€‚å› æ­¤ï¼Œç¤¾äº¤åª’ä½“æ•°æ®å·²æˆä¸ºäº†ç†è§£å…¬ä¼—æƒ…ç»ªçš„é‡è¦èµ„æºã€‚ç ”ç©¶è¡¨æ˜ï¼Œå®ƒå¯ä»¥å¢å¼ºæƒ³æ³•å¹¶å½±å“å…¬ä¼—æƒ…ç»ªã€‚è¿™ä¸ªç ”ç©¶åˆ†æäº†2014å¹´è‡³2023å¹´é—´å…¬ä¼—å¯¹æ°”å€™å˜åŒ–å’Œç¯å¢ƒçš„è§‚æ„Ÿã€‚æˆ‘ä»¬ä½¿ç”¨ç‚¹å¯¹ç‚¹ç§¯åˆ†ä¿¡æ¯ï¼ˆPMIï¼‰ç®—æ³•ï¼Œç¡®å®šæƒ…ç»ªå’Œæ¢ç´¢ä¸åŒç¤¾äº¤åª’ä½“å¹³å°ä¸Šç¯å¢ƒæ¨æ–‡ä¸­è¡¨è¾¾çš„ä¸»è¦æƒ…æ„Ÿã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œç¯å¢ƒæ¨æ–‡ä¸­çš„è´Ÿé¢æƒ…ç»ªæ¯”æ­£é¢æˆ–ä¸­æ€§æƒ…ç»ªæ›´ä¸ºå¸¸è§ã€‚æ°”å€™å˜åŒ–ã€ç©ºæ°”è´¨é‡ã€æ’æ”¾ã€å¡‘æ–™å’Œå›æ”¶æ˜¯æ‰€æœ‰ç¤¾äº¤åª’ä½“å¹³å°ä¸Šæœ€å—å…³æ³¨çš„è¯é¢˜ï¼Œè¿™åæ˜ äº†äººä»¬å¯¹ç¯å¢ƒé—®é¢˜çš„æå¤§å…³æ³¨ã€‚ç¯å¢ƒæ¨æ–‡ä¸­æœ€å¸¸è§çš„æƒ…æ„Ÿæ˜¯ææ…Œã€ä¿¡ä»»å’ŒæœŸå¾…ï¼Œè¿™è¡¨æ˜å…¬ä¼—å¯¹ç¯å¢ƒé—®é¢˜çš„ååº”æ˜¯å¤šæ ·åŒ–å’Œå¤æ‚çš„ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡åˆ†æç¯å¢ƒè¯é¢˜ä¸­çš„æ„è§å’Œè¶‹åŠ¿ï¼Œä¸ºç¯å¢ƒé—®é¢˜æä¾›æ„è¯†ï¼Œåˆ¶å®š intervenciÃ³nå’Œé€‚åº”ç¯å¢ƒæŒ‘æˆ˜ã€‚
</details></li>
</ul>
<hr>
<h2 id="LLMs-for-Multi-Modal-Knowledge-Extraction-and-Analysis-in-Intelligence-Safety-Critical-Applications"><a href="#LLMs-for-Multi-Modal-Knowledge-Extraction-and-Analysis-in-Intelligence-Safety-Critical-Applications" class="headerlink" title="LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence&#x2F;Safety-Critical Applications"></a>LLMs for Multi-Modal Knowledge Extraction and Analysis in Intelligence&#x2F;Safety-Critical Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03088">http://arxiv.org/abs/2312.03088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brett Israelsen, Soumalya Sarkar</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æŠŠæœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°å’Œæ¼æ´ç ”ç©¶æ€»ç»“èµ·æ¥ï¼Œä»¥å¸®åŠ©ç†è§£è¿™äº›æŠ€æœ¯åœ¨çŸ¥è¯†å’Œå®‰å…¨åº”ç”¨ä¸­çš„åº”ç”¨å‰éœ€è¦è¿›è¡Œä»€ä¹ˆæ ·çš„è°¨æ…ã€‚</li>
<li>methods: æœ¬è®ºæ–‡é€šè¿‡å¯¹æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹è¯„ä¼°å’Œæ¼æ´ç ”ç©¶è¿›è¡Œæ€»ç»“ï¼ŒæŠŠæ¼æ´åˆ†ä¸ºåä¸ªé«˜çº§ç±»åˆ«ï¼Œå¹¶å°†å®ƒä»¬ä¸å¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸè¿›è¡Œå¯¹æ¯”ã€‚</li>
<li>results: æœ¬è®ºæ–‡ç»“æœè¡¨æ˜ï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨çŸ¥è¯†å’Œå®‰å…¨åº”ç”¨ä¸­å­˜åœ¨è®¸å¤šæ¼æ´å’Œé™åˆ¶ï¼Œéœ€è¦è¿›è¡Œè°¨æ…çš„è¯„ä¼°å’Œmitigation before applying them to intelligence and safety-critical applicationsã€‚<details>
<summary>Abstract</summary>
Large Language Models have seen rapid progress in capability in recent years; this progress has been accelerating and their capabilities, measured by various benchmarks, are beginning to approach those of humans. There is a strong demand to use such models in a wide variety of applications but, due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications. This paper reviews recent literature related to LLM assessment and vulnerabilities to synthesize the current research landscape and to help understand what advances are most critical to enable use of of these technologies in intelligence and safety-critical applications. The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM. Some general categories of mitigations are reviewed.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¿‡å»å‡ å¹´å†…æ‰€ç¤ºå‡ºçš„èƒ½åŠ›æå‡éå¸¸å¿«ï¼Œè¿™ç§æå‡çš„é€Ÿåº¦åœ¨ä¸æ–­åŠ é€Ÿï¼Œå…¶èƒ½åŠ›æŒ‰ç…§ä¸åŒçš„æ ‡å‡†æµ‹è¯•æ–¹æ³•æµ‹é‡ï¼Œå·²ç»æ¥è¿‘äººç±»æ°´å¹³ã€‚ä½†ç”±äºå­˜åœ¨è®¸å¤šæ¼æ´å’Œå±€é™æ€§ï¼Œåœ¨æ™ºèƒ½å’Œå®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­ä½¿ç”¨è¿™äº›æ¨¡å‹éœ€è¦éå¸¸å°å¿ƒã€‚è¿™ç¯‡è¯„è®ºæ–‡ä»¶æ€»ç»“äº†æœ€æ–°çš„LLMè¯„ä¼°å’Œæ¼æ´ç ”ç©¶ï¼Œæ—¨åœ¨æ€»ç»“å½“å‰ç ”ç©¶é¢†åŸŸçš„ç ”ç©¶çŠ¶å†µï¼Œå¹¶å¸®åŠ©ç†è§£åœ¨æ™ºèƒ½å’Œå®‰å…¨æ•æ„Ÿåº”ç”¨ä¸­ä½¿ç”¨è¿™äº›æŠ€æœ¯æ‰€éœ€çš„è¿›ä¸€æ­¥å‘å±•ã€‚æ¼æ´è¢«åˆ†ä¸ºåä¸ªé«˜çº§ç±»åˆ«ï¼Œå¹¶ä¸é«˜çº§LLMç”Ÿå‘½å‘¨æœŸç›¸å åŠ ä»¥ç¤ºå‡ºã€‚ä¸€äº›é€šç”¨çš„mitigationæªæ–½ä¹Ÿè¢«ç®€è¦ä»‹ç»ã€‚
</details></li>
</ul>
<hr>
<h2 id="Describing-Differences-in-Image-Sets-with-Natural-Language"><a href="#Describing-Differences-in-Image-Sets-with-Natural-Language" class="headerlink" title="Describing Differences in Image Sets with Natural Language"></a>Describing Differences in Image Sets with Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02974">http://arxiv.org/abs/2312.02974</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/understanding-visual-datasets/visdiff">https://github.com/understanding-visual-datasets/visdiff</a></li>
<li>paper_authors: Lisa Dunlap, Yuhui Zhang, Xiaohan Wang, Ruiqi Zhong, Trevor Darrell, Jacob Steinhardt, Joseph E. Gonzalez, Serena Yeung-Levy</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨è‡ªåŠ¨æè¿°ä¸¤ä¸ªå›¾åƒé›†ä¹‹é—´çš„å·®å¼‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£æ¨¡å‹è¡Œä¸ºå’Œåˆ†ææ•°æ®é›†ã€‚</li>
<li>methods: è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§ä¸¤stageçš„æ–¹æ³•ï¼Œé¦–å…ˆä»å›¾åƒé›†ä¸­æå‡ºå€™é€‰å·®å¼‚æè¿°ï¼Œç„¶åä½¿ç”¨CLIPè¿›è¡Œé‡æ–°æ’åºï¼Œä»¥ç¡®å®šå€™é€‰æè¿°æ˜¯å¦èƒ½å¤Ÿåˆ†åˆ«ä¸¤ä¸ªé›†ã€‚</li>
<li>results: è¯¥ç ”ç©¶ä½¿ç”¨VisDiffå®ç°äº†è‡ªåŠ¨æè¿°å›¾åƒé›†ä¹‹é—´å·®å¼‚ï¼Œå¹¶åœ¨å¤šä¸ªé¢†åŸŸè¿›è¡Œäº†åº”ç”¨ï¼Œå¦‚æ¯”è¾ƒä¸åŒçš„æ•°æ®é›†ï¼ˆä¾‹å¦‚ImageNet vs. ImageNetV2ï¼‰ã€ä¸åŒçš„åˆ†ç±»æ¨¡å‹ï¼ˆä¾‹å¦‚é›¶shot CLIP vs. ç›‘ç£ResNetï¼‰ã€æ¦‚æ‹¬æ¨¡å‹å¤±æ•ˆæ¨¡å¼ï¼ˆä¾‹å¦‚ç›‘ç£ResNetï¼‰ã€æè¿°ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼ˆä¾‹å¦‚StableDiffusionV1å’ŒV2ï¼‰ä»¥åŠå‘ç°å›¾åƒæ˜¯å¦‚ä½•è®°å¿†çš„ã€‚ä½¿ç”¨VisDiffï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ‰¾åˆ° interessante å’Œå‰æ‰€æœªçŸ¥çš„å·®å¼‚ï¼Œ demonstrating its utility in revealing nuanced insightsã€‚<details>
<summary>Abstract</summary>
How do two sets of images differ? Discerning set-level differences is crucial for understanding model behaviors and analyzing datasets, yet manually sifting through thousands of images is impractical. To aid in this discovery process, we explore the task of automatically describing the differences between two $\textbf{sets}$ of images, which we term Set Difference Captioning. This task takes in image sets $D_A$ and $D_B$, and outputs a description that is more often true on $D_A$ than $D_B$. We outline a two-stage approach that first proposes candidate difference descriptions from image sets and then re-ranks the candidates by checking how well they can differentiate the two sets. We introduce VisDiff, which first captions the images and prompts a language model to propose candidate descriptions, then re-ranks these descriptions using CLIP. To evaluate VisDiff, we collect VisDiffBench, a dataset with 187 paired image sets with ground truth difference descriptions. We apply VisDiff to various domains, such as comparing datasets (e.g., ImageNet vs. ImageNetV2), comparing classification models (e.g., zero-shot CLIP vs. supervised ResNet), summarizing model failure modes (supervised ResNet), characterizing differences between generative models (e.g., StableDiffusionV1 and V2), and discovering what makes images memorable. Using VisDiff, we are able to find interesting and previously unknown differences in datasets and models, demonstrating its utility in revealing nuanced insights.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>TRANSLATE_TEXTä¸¤ä¸ªå›¾åƒé›†çš„å·®å¼‚å¦‚ä½•åŒºåˆ†ï¼Ÿäº†è§£æ¨¡å‹è¡Œä¸ºå’Œåˆ†ææ•°æ®é›†éœ€è¦èƒ½å¤Ÿå¿«é€Ÿå’Œç²¾å‡†åœ°å‘ç°è¿™äº›å·®å¼‚ï¼Œä½†æ˜¯ manually éå†åƒä¸ªå›¾åƒæ˜¯ä¸å®ç”¨çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è‡ªåŠ¨æè¿°ä¸¤ä¸ªå›¾åƒé›†ä¹‹é—´çš„å·®å¼‚ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º Set Difference Captioningã€‚è¿™ä¸ªä»»åŠ¡æ¥å—å›¾åƒé›† $D_A$ å’Œ $D_B$ ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ›´å¸¸å‡ºç°åœ¨ $D_A$ ä¸Šçš„æè¿°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸¤ä¸ªé˜¶æ®µçš„æ–¹æ³•ï¼Œé¦–å…ˆæå‡ºå€™é€‰çš„å·®å¼‚æè¿°ï¼Œç„¶åä½¿ç”¨ CLIP è¿›è¡Œé‡æ–°æ’åºï¼Œä»¥ç¡®å®šå€™é€‰æè¿°æ˜¯å¦èƒ½å¤ŸåŒºåˆ†ä¸¤ä¸ªé›†ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸º VisDiffï¼Œå®ƒé¦–å…ˆä¸ºå›¾åƒé›†æä¾›æè¿°ï¼Œç„¶åä½¿ç”¨è¯­è¨€æ¨¡å‹æå‡ºå€™é€‰æè¿°ï¼Œå¹¶ä½¿ç”¨ CLIP è¿›è¡Œé‡æ–°æ’åºã€‚ä¸ºäº†è¯„ä¼° VisDiffï¼Œæˆ‘ä»¬æ”¶é›†äº† VisDiffBench æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å« 187 å¯¹å›¾åƒé›†çš„å¯¹åº”æè¿°ã€‚æˆ‘ä»¬åœ¨ä¸åŒé¢†åŸŸåº”ç”¨ VisDiffï¼ŒåŒ…æ‹¬æ¯”è¾ƒæ•°æ®é›†ï¼ˆå¦‚ ImageNet vs. ImageNetV2ï¼‰ã€æ¯”è¾ƒåˆ†ç±»æ¨¡å‹ï¼ˆå¦‚é›¶shot CLIP vs. ç›‘ç£ ResNetï¼‰ã€æ€»ç»“æ¨¡å‹å¤±æ•ˆæ¨¡å¼ï¼ˆå¦‚ç›‘ç£ ResNetï¼‰ã€æè¿°ç”Ÿæˆæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼ˆå¦‚ StableDiffusionV1 å’Œ V2ï¼‰ï¼Œä»¥åŠå‘ç°å›¾åƒå¸å¼•åŠ›çš„åŸå› ã€‚ä½¿ç”¨ VisDiffï¼Œæˆ‘ä»¬èƒ½å¤Ÿå‘ç°ä¸åŒçš„å›¾åƒé›†å’Œæ¨¡å‹ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™äº›å·®å¼‚å¯èƒ½æ˜¯å·²çŸ¥çš„æˆ–æœªçŸ¥çš„ï¼Œè¿™ demonstartes VisDiff çš„å®ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Visual-Program-Distillation-Distilling-Tools-and-Programmatic-Reasoning-into-Vision-Language-Models"><a href="#Visual-Program-Distillation-Distilling-Tools-and-Programmatic-Reasoning-into-Vision-Language-Models" class="headerlink" title="Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"></a>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03052">http://arxiv.org/abs/2312.03052</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman</li>
<li>for: è§£å†³å¤æ‚è§†è§‰ä»»åŠ¡ï¼Œå¦‚â€œè°åˆ›é€ äº†å³è¾¹çš„ä¹å™¨ï¼Ÿâ€éœ€è¦å¤šç§æŠ€èƒ½çš„ç»„åˆï¼ŒåŒ…æ‹¬ç†è§£ç©ºé—´ã€è¯†åˆ«ä¹å™¨å’Œæ£€ç´¢çŸ¥è¯†ã€‚</li>
<li>methods: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†å¤æ‚è§†è§‰ä»»åŠ¡åˆ†è§£æˆå¯æ‰§è¡Œç¨‹åºï¼Œå¹¶ä½¿ç”¨ç‰¹æ®Šè§†è§‰æ¨¡å‹ï¼ˆVLMï¼‰è§£å†³é—®é¢˜ã€‚</li>
<li>results: æˆ‘ä»¬æå‡ºäº†è§†è§‰ç¨‹åºå¡«å……ï¼ˆVPDï¼‰ï¼Œä¸€ç§æŒ‡å¯¼æ¡†æ¶ï¼Œå¯ä»¥åœ¨å•ä¸ªå‰è¿›passä¸­è§£å†³å¤æ‚è§†è§‰ä»»åŠ¡ã€‚VPDä½¿ç”¨LLMæ¥é‡‡æ ·å¤šä¸ªå€™é€‰ç¨‹åºï¼Œå¹¶å¯¹æ¯ä¸ªæ­£ç¡®ç¨‹åºè¿›è¡Œæ‰§è¡Œå’ŒéªŒè¯ï¼Œä»¥ç¡®å®šæ­£ç¡®çš„ä¸€ä¸ªã€‚ç„¶åï¼Œå®ƒå°†æ¯ä¸ªæ­£ç¡®ç¨‹åºç¿»è¯‘æˆè¯­è¨€æè¿°ï¼Œå¹¶å°†å…¶å¡«å……åˆ°VLMä¸­ã€‚å®éªŒæ˜¾ç¤ºï¼ŒVPDå¯ä»¥æé«˜VLMçš„ç†è§£ç©ºé—´ã€COUNTå’Œcompositional reasoningèƒ½åŠ›ã€‚æˆ‘ä»¬çš„VPD-trained PaLI-Xåœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šæ‰€æœ‰ä¹‹å‰çš„VLMï¼Œå¹¶åœ¨MMBenchã€OK-VQAã€A-OKVQAã€TallyQAã€POPEå’ŒHateful Memesç­‰ä»»åŠ¡ä¸­è·å¾—äº†state-of-the-artè¡¨ç°ã€‚äººå·¥æ ‡æ³¨å‘˜ä¹Ÿè¯å®äº†VPDæ”¹è¿›äº†æ¨¡å‹çš„å›ç­”å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚ finallyï¼Œæˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒVPDå¯ä»¥åœ¨å®é™…åº”ç”¨ä¸­è¿›è¡Œé€‚åº”ï¼Œå¹¶ä¸”åœ¨æœ‰é™æ•°æ®æƒ…å†µä¸‹è¡¨ç°å‡ºè‰²ã€‚<details>
<summary>Abstract</summary>
Solving complex visual tasks such as "Who invented the musical instrument on the right?" involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.
</details>
<details>
<summary>æ‘˜è¦</summary>
è§£å†³å¤æ‚è§†è§‰ä»»åŠ¡ï¼Œå¦‚â€œè°åˆ›é€ äº†å³è¾¹çš„ä¹å™¨ï¼Ÿâ€éœ€è¦åˆç†çš„æŠ€èƒ½ç»„åˆï¼šç†è§£ç©ºé—´ã€è¯†åˆ«ä¹å™¨ã€å¹¶æ£€ç´¢å…ˆå‰çŸ¥è¯†ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å°†è¿™ç±»ä»»åŠ¡ decomposed into å¯æ‰§è¡Œçš„ç¨‹åºï¼Œä½†ç”Ÿæˆçš„ç¨‹åºå…·æœ‰è®¸å¤šé”™è¯¯ï¼šç¼ºå°‘å¿…è¦æ­¥éª¤ã€åŒ…å«å¹Œå­æ­¥éª¤ï¼Œä»¥åŠæ— æ³•å›å½’å½“ç‰¹åŒ–æ¨¡å‹è¿”å›é”™è¯¯è¾“å‡ºã€‚æ­¤å¤–ï¼Œå®ƒä»¬éœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹ï¼Œä»è€Œå¯¼è‡´é«˜å»¶è¿Ÿå’Œè®¡ç®—æˆæœ¬ã€‚æˆ‘ä»¬æå‡ºäº†è§†è§‰ç¨‹åºç†”åŒ–ï¼ˆVPDï¼‰ï¼Œä¸€ç§ instruction tuning æ¡†æ¶ï¼Œå¯ä»¥ä½¿å¾—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰é€šè¿‡å•ä¸ªå‰è¿›æ­¥æ¥è§£å†³å¤æ‚è§†è§‰ä»»åŠ¡ã€‚VPD é€šè¿‡ä½¿ç”¨ LLM æ¥é‡‡æ ·å¤šä¸ªå€™é€‰ç¨‹åºï¼Œç„¶åæ‰§è¡Œå’ŒéªŒè¯ä»¥ç¡®å®šæ­£ç¡®çš„ä¸€ä¸ªã€‚å®ƒå°†æ¯ä¸ªæ­£ç¡®çš„ç¨‹åºç¿»è¯‘æˆè¯­è¨€æè¿°ç¬¦ï¼Œå¹¶å°†å…¶ç†”åŒ–æˆ VLMã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜ï¼ŒVPD å¯ä»¥æé«˜ VLM çš„ç†è§£ç©ºé—´ã€è®¡æ•°å’Œ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸itional ç†è§£èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ VPD-trained PaLI-X åœ¨å¤æ‚è§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œè¶…è¶Šæ‰€æœ‰å…ˆå‰çš„ VLMï¼Œå¹¶ achieved state-of-the-art æ€§èƒ½ã€‚äººå·¥æ ‡æ³¨å‘˜ä¹Ÿè¯å®äº† VPD æ”¹è¿›æ¨¡å‹çš„å›ç­”å‡†ç¡®æ€§å’Œä¸€è‡´æ€§ã€‚æœ€åï¼Œæˆ‘ä»¬åœ¨å†…å®¹å®¡æŸ¥åº”ç”¨ä¸­ä¹Ÿè¯æ˜äº† VPD çš„é€‚ç”¨æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Rank-without-GPT-Building-GPT-Independent-Listwise-Rerankers-on-Open-Source-Large-Language-Models"><a href="#Rank-without-GPT-Building-GPT-Independent-Listwise-Rerankers-on-Open-Source-Large-Language-Models" class="headerlink" title="Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models"></a>Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02969">http://arxiv.org/abs/2312.02969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Zhang, Sebastian HofstÃ¤tter, Patrick Lewis, Raphael Tang, Jimmy Lin</li>
<li>For: The paper aims to build effective listwise rerankers without any dependence on GPT models, addressing the concern of single point of failure and improving scientific reproducibility.* Methods: The authors use large language models (LLM) to build the listwise rerankers, but do not rely on GPT models. They conduct passage retrieval experiments to evaluate the effectiveness of their approach.* Results: The authors achieve 97% effectiveness of the listwise rerankers built on GPT-4, and their best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13%. However, they find that existing training datasets are insufficient for building such listwise rerankers, highlighting the need for high-quality listwise ranking data resources.Hereâ€™s the simplified Chinese text for the three key points:* For: è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯å»ºç«‹ä¸ä¾èµ– GPT æ¨¡å‹çš„æœ‰æ•ˆåˆ—è¡¨é‡æ–°æ’åºå™¨ï¼Œè§£å†³å•ç‚¹å¤±è´¥å’Œç§‘å­¦å¤åˆ¶æ€§é—®é¢˜ã€‚* Methods: ä½œè€…ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å»ºç«‹åˆ—è¡¨é‡æ–°æ’åºå™¨ï¼Œä½†ä¸ä¾èµ– GPT æ¨¡å‹ã€‚ä»–ä»¬è¿›è¡Œäº†è¿‡ç¨‹æ£€ç´¢å®éªŒæ¥è¯„ä¼°å…¶æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚* Results: ä½œè€…åœ¨ GPT-4 ä¸Šå»ºç«‹çš„åˆ—è¡¨é‡æ–°æ’åºå™¨è¾¾åˆ° 97% çš„æœ‰æ•ˆæ€§ï¼Œè€Œå…¶æœ€ä½³åˆ—è¡¨é‡æ–°æ’åºå™¨åœ¨ GPT-3.5 ä¸Šå‡ºperform 13% ç‚¹ã€‚ç„¶è€Œï¼Œä»–ä»¬å‘ç°ç°æœ‰çš„è®­ç»ƒæ•°æ®é›†ä¸å¤Ÿç”¨äºå»ºç«‹è¿™ç±»åˆ—è¡¨é‡æ–°æ’åºå™¨ï¼Œå‘¼åæ›´å¤šçš„äººå·¥æ ‡æ³¨åˆ—è¡¨é‡æ–°æ’åºæ•°æ®èµ„æºçš„å»ºç«‹ã€‚<details>
<summary>Abstract</summary>
Listwise rerankers based on large language models (LLM) are the zero-shot state-of-the-art. However, current works in this direction all depend on the GPT models, making it a single point of failure in scientific reproducibility. Moreover, it raises the concern that the current research findings only hold for GPT models but not LLM in general. In this work, we lift this pre-condition and build for the first time effective listwise rerankers without any form of dependency on GPT. Our passage retrieval experiments show that our best list se reranker surpasses the listwise rerankers based on GPT-3.5 by 13% and achieves 97% effectiveness of the ones built on GPT-4. Our results also show that the existing training datasets, which were expressly constructed for pointwise ranking, are insufficient for building such listwise rerankers. Instead, high-quality listwise ranking data is required and crucial, calling for further work on building human-annotated listwise data resources.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„åˆ—è¡¨é‡æ–°æ’åºå™¨éƒ½åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†æ˜¯ç°æœ‰çš„ç ”ç©¶éƒ½ä¾èµ–äºGPTæ¨¡å‹ï¼Œè¿™ä¼šå¯¼è‡´ç§‘å­¦å¤åˆ¶æ€§çš„é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¿™ä¹Ÿæå‡ºäº†å½“å‰ç ”ç©¶æˆæœåªé€‚ç”¨äºGPTæ¨¡å‹ï¼Œè€Œä¸é€‚ç”¨äºLLMæ€»ä½“çš„é—®é¢˜ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬è§£å†³äº†è¿™ä¸ªå‰æï¼Œå¹¶é¦–æ¬¡å»ºç«‹äº†ä¸ä¾èµ–äºGPTçš„æœ‰æ•ˆåˆ—è¡¨é‡æ–°æ’åºå™¨ã€‚æˆ‘ä»¬çš„è¿‡ç¨‹æ£€ç´¢å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æœ€ä½³åˆ—è¡¨é‡æ–°æ’åºå™¨æ¯”åŸºäºGPT-3.5çš„åˆ—è¡¨é‡æ–°æ’åºå™¨é«˜å‡º13%ï¼Œå¹¶è¾¾åˆ°97%çš„æ•ˆæœã€‚æˆ‘ä»¬çš„ç»“æœè¿˜è¡¨æ˜ï¼Œç°æœ‰çš„ç‚¹ wise æ’åºæ•°æ®é›†ï¼Œ originally constructed for pointwise ranking, æ˜¯æ— æ³•å»ºç«‹åˆ—è¡¨é‡æ–°æ’åºå™¨çš„ã€‚ç›¸åï¼Œé«˜è´¨é‡çš„åˆ—è¡¨æ’åºæ•°æ®èµ„æºæ˜¯å¿…è¦çš„ï¼Œå‘¼åæ›´å¤šçš„äººä¸ºåˆ—è¡¨æ’åºæ•°æ®é›†è¿›è¡Œäººå·¥æ ‡æ³¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Concept-Drift-Adaptation-in-Text-Stream-Mining-Settings-A-Comprehensive-Review"><a href="#Concept-Drift-Adaptation-in-Text-Stream-Mining-Settings-A-Comprehensive-Review" class="headerlink" title="Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review"></a>Concept Drift Adaptation in Text Stream Mining Settings: A Comprehensive Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02901">http://arxiv.org/abs/2312.02901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cristiano Mesquita Garcia, Ramon Simoes Abilio, Alessandro Lameiras Koerich, Alceu de Souza Britto Jr., Jean Paul Barddal</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å¯¹æ–‡æœ¬æµæ‰¹å¤„ç†ä¸­çš„æ¦‚å¿µæ¼‚ç§»è¿›è¡Œç³»ç»Ÿæ€§çš„æ–‡çŒ®ç»¼è¿°ï¼Œä»¥æ¢è®¨è¿™äº›æ–¹æ³•åœ¨ä¸åŒçš„åº”ç”¨åœºæ™¯ä¸­çš„ä½¿ç”¨ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†ç³»ç»Ÿæ€§çš„æ–‡çŒ®ç»¼è¿°æ–¹æ³•ï¼Œä»40ç¯‡è®ºæ–‡ä¸­é€‰æ‹©äº†ç¬¦åˆå®šä¹‰çš„æ–‡çŒ®ï¼Œå¹¶å¯¹è¿™äº›æ–‡çŒ®è¿›è¡Œäº†åˆ†ç±»ã€æ€»ç»“å’Œè®¨è®ºã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°äº†åœ¨æ–‡æœ¬æµæ‰¹å¤„ç†ä¸­çš„æ¦‚å¿µæ¼‚ç§»é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€äº›è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä¸åŒç±»å‹çš„æ¦‚å¿µæ¼‚ç§»æ£€æµ‹æ–¹æ³•ã€æ¨¡å‹æ›´æ–°æœºåˆ¶ä»¥åŠæ–‡æœ¬è¡¨ç¤ºæ›´æ–°æ–¹æ³•ç­‰ã€‚<details>
<summary>Abstract</summary>
Due to the advent and increase in the popularity of the Internet, people have been producing and disseminating textual data in several ways, such as reviews, social media posts, and news articles. As a result, numerous researchers have been working on discovering patterns in textual data, especially because social media posts function as social sensors, indicating peoples' opinions, interests, etc. However, most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, such as an outdated dataset, which may not correspond to reality, and an outdated model, which has its performance degrading over time. Concept drift is another aspect that emphasizes these issues, which corresponds to data distribution and pattern changes. In a text stream scenario, it is even more challenging due to its characteristics, such as the high speed and data arriving sequentially. In addition, models for this type of scenario must adhere to the constraints mentioned above while learning from the stream by storing texts for a limited time and consuming low memory. In this study, we performed a systematic literature review regarding concept drift adaptation in text stream scenarios. Considering well-defined criteria, we selected 40 papers to unravel aspects such as text drift categories, types of text drift detection, model update mechanism, the addressed stream mining tasks, types of text representations, and text representation update mechanism. In addition, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Therefore, this paper comprehensively reviews the concept drift adaptation in text stream mining scenarios.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Can-a-Tabula-Recta-provide-security-in-the-XXI-century"><a href="#Can-a-Tabula-Recta-provide-security-in-the-XXI-century" class="headerlink" title="Can a Tabula Recta provide security in the XXI century?"></a>Can a Tabula Recta provide security in the XXI century?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02869">http://arxiv.org/abs/2312.02869</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisco Ruiz</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶äººå·¥è®¡ç®—æœºå¯èƒ½å´©æºƒåç”¨äºåŠ å¯†çš„çº¸ç¬”è®¡ç®—æ–¹æ³•ï¼Œä»¥åŠè¿™äº›æ–¹æ³•æ˜¯å¦èƒ½å¤ŸæŠµæŠ—è®¡ç®—æœºæ”¯æŒçš„è§£å¯†æ–¹æ³•ã€‚</li>
<li>methods: è®ºæ–‡ä½¿ç”¨äº†ä¸€äº›ç»å…¸çš„çº¸ç¬”è®¡ç®—æ–¹æ³•ï¼Œå¦‚Tabula Rectaï¼Œä»¥åŠä¸€äº›æ–°çš„æ–¹æ³•ï¼Œå¦‚åŸºäºéäºŒè¿›åˆ¶æ•°å­¦ç©ºé—´çš„æµåŠ å¯†å’ŒåŸºäºæŒ‘æˆ˜æ–‡æœ¬ç”Ÿæˆå¯†ç çš„hash-likeç®—æ³•ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡è®¡ç®—æœºåŸºæœ¬çš„ç»Ÿè®¡åˆ†æï¼Œè¯æ˜äº†è¿™äº›äººå·¥è®¡ç®—æœºå¯èƒ½å´©æºƒåç”¨äºåŠ å¯†çš„æ–¹æ³•å¯ä»¥æä¾›è¶³å¤Ÿçš„å®‰å…¨æ€§ã€‚<details>
<summary>Abstract</summary>
In the not so unlikely scenario of total compromise of computers accessible to a group of users, they might be tempted to resort to human-computable paper-and-pencil cryptographic methods aided by a classic Tabula Recta, which helps to perform addition and subtraction directly with letters. But do these classic algorithms, or some new ones using the same simple tools, have any chance against computer-aided cryptanalysis? In this paper I discuss how some human-computable algorithms can indeed afford sufficient security in this situation, drawing conclusions from computer-based statistical analysis. Three kinds of algorithms are discussed: those that concentrate entropy from shared text sources, stream ciphers based on arithmetic of non-binary spaces, and hash-like algorithms that may be used to generate a password from a challenge text.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è®¡ç®—æœºæ³„éœ²æˆ–æ”»å‡» scenarios ä¸­ï¼Œç”¨æˆ·ç¾¤ä½“å¯èƒ½ä¼šè€ƒè™‘ä½¿ç”¨äººå·¥å¯è®¡ç®—çš„çº¸ç¬”å¯†ç æ–¹æ³•ï¼Œä½¿ç”¨ç±»ä¼ ç»Ÿçš„ Tabula Rectaï¼Œä»¥ç›´æ¥å°†å­—æ¯è¿›è¡ŒåŠ å‡è¿ç®—ã€‚ä½†è¿™äº›ä¼ ç»Ÿç®—æ³•æˆ–æ–°çš„ç®—æ³•ä½¿ç”¨åŒæ ·çš„ç®€å•å·¥å…·ï¼Œå¯¹äºè®¡ç®—æœºå¸®åŠ©çš„åŠ å¯†åˆ†ææ˜¯å¦æœ‰ä»»ä½•æœºä¼šï¼Ÿåœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä¼šè®¨è®ºè¿™äº›äººå·¥å¯è®¡ç®—çš„ç®—æ³•æ˜¯å¦å¯ä»¥æä¾›è¶³å¤Ÿçš„å®‰å…¨æ€§ï¼Œé€šè¿‡è®¡ç®—æœºåŸºäºçš„ç»Ÿè®¡åˆ†ææ¥Draw conclusionsã€‚æˆ‘å°†è®¨è®ºä¸‰ç§ç®—æ³•ï¼šå…·æœ‰å…±äº«æ–‡æœ¬æ¥æºçš„ entropy é›†ä¸­ç®—æ³•ï¼ŒåŸºäºéäºŒè¿›åˆ¶ç©ºé—´çš„æµåŠ å¯†ç®—æ³•ï¼Œä»¥åŠå¯ç”¨äºç”ŸæˆæŒ‘æˆ˜æ–‡æœ¬çš„ hash-like ç®—æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Detection-of-Hallucinations-in-LLM-Activations"><a href="#Weakly-Supervised-Detection-of-Hallucinations-in-LLM-Activations" class="headerlink" title="Weakly Supervised Detection of Hallucinations in LLM Activations"></a>Weakly Supervised Detection of Hallucinations in LLM Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02798">http://arxiv.org/abs/2312.02798</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox">https://github.com/Trusted-AI/adversarial-robustness-toolbox</a></li>
<li>paper_authors: Miriam Rateike, Celia Cintas, John Wamburu, Tanya Akumu, Skyler Speakman</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ£€æµ‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å†…éƒ¨çŠ¶æ€ä¸­æ˜¯å¦å­˜åœ¨å¹»è§‰æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯èƒ½ä¼šä¼ é€’åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼±ç›‘ç£çš„å®¡æ ¸æ–¹æ³•ï¼Œä½¿ç”¨ä¸€ç§subset scanningapproachæ£€æµ‹LLMæ´»åŠ¨ä¸­å¼‚å¸¸æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å…ˆçŸ¥é“å¹»è§‰æ¨¡å¼çš„ç±»å‹ï¼Œè€Œæ˜¯åŸºäºä¸€ä¸ªæ— å¼‚å¸¸æ ·æœ¬çš„å‚è€ƒé›†è¿›è¡Œæ£€æµ‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ç¡®å®šLLMå†…éƒ¨å“ªäº›èŠ‚ç‚¹è´Ÿè´£ç¼–ç è¿™äº›æ¨¡å¼ï¼Œè¿™å¯èƒ½æä¾›äº†å…³é”®çš„é€å½»é˜è¿°ç”¨äºåè§ç¼“è§£ã€‚</li>
<li>results: æˆ‘ä»¬çš„ç»“æœè¯æ˜BERTåœ¨å†…éƒ¨å®¹ç§¯ä¸å¤Ÿä»¥ç¼–ç å¹»è§‰ï¼Œè€ŒOPTåˆ™å¯ä»¥åœ¨å†…éƒ¨ç¼–ç å¹»è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ£€æµ‹æ–¹æ³•ï¼Œæ— éœ€å…ˆçŸ¥é“å‡è¯­å¥ï¼Œä¸å®Œå…¨ç›‘ç£çš„out-of-distributionåˆ†ç±»å™¨ç›¸å½“ã€‚<details>
<summary>Abstract</summary>
We propose an auditing method to identify whether a large language model (LLM) encodes patterns such as hallucinations in its internal states, which may propagate to downstream tasks. We introduce a weakly supervised auditing technique using a subset scanning approach to detect anomalous patterns in LLM activations from pre-trained models. Importantly, our method does not need knowledge of the type of patterns a-priori. Instead, it relies on a reference dataset devoid of anomalies during testing. Further, our approach enables the identification of pivotal nodes responsible for encoding these patterns, which may offer crucial insights for fine-tuning specific sub-networks for bias mitigation. We introduce two new scanning methods to handle LLM activations for anomalous sentences that may deviate from the expected distribution in either direction. Our results confirm prior findings of BERT's limited internal capacity for encoding hallucinations, while OPT appears capable of encoding hallucination information internally. Importantly, our scanning approach, without prior exposure to false statements, performs comparably to a fully supervised out-of-distribution classifier.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºä¸€ç§å®¡æ ¸æ–¹æ³•ï¼Œç”¨äºç¡®å®šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦å†…éƒ¨å­˜åœ¨å¹»è§‰ç±»æ¨¡å¼ï¼Œè¿™äº›æ¨¡å¼å¯èƒ½ä¼šä¼ é€’åˆ°ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§å¼±ç›‘ç¹å®¡æ ¸æŠ€æœ¯ï¼Œä½¿ç”¨ä¸€ä¸ªå­é›†æ‰«ææ–¹æ³•æ¥æ£€æµ‹ LLM æ´»åŠ¨ä¸­å¼‚å¸¸æ¨¡å¼ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å…ˆçŸ¥é“å¹»è§‰æ¨¡å¼çš„ç±»å‹ã€‚ç›¸åï¼Œå®ƒä¾èµ–äºä¸€ä¸ªæ— å¼‚å¸¸æ ·æœ¬çš„å‚è€ƒæ•°æ®é›†æ¥è¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è¯†åˆ« LLM ä¸­å¹»è§‰æ¨¡å¼çš„å…³é”®èŠ‚ç‚¹ï¼Œè¿™äº›èŠ‚ç‚¹å¯èƒ½ä¼šæä¾›å…³é”®çš„è°ƒæ•´ç‰¹å®šå­ç½‘ç»œçš„é€”å¾„ã€‚æˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„æ‰«ææ–¹æ³•ï¼Œç”¨äºå¤„ç† LLM æ´»åŠ¨ä¸­å¼‚å¸¸å¥å­ï¼Œè¿™äº›å¥å­å¯èƒ½ä¼šä¸é¢„æœŸåˆ†å¸ƒç›¸å·®ã€‚æˆ‘ä»¬çš„ç»“æœè¯å®äº† BERT çš„æœ‰é™å†…éƒ¨å®¹é‡ï¼Œè€Œ OPT åˆ™å¯ä»¥å†…éƒ¨ç¼–ç å¹»è§‰ä¿¡æ¯ã€‚æˆ‘ä»¬çš„æ‰«ææ–¹æ³•ï¼Œä¸éœ€è¦å…ˆæ¥è§¦å‡STATEMENTï¼Œä¸å®Œå…¨ç›‘ç¹çš„å¤–éƒ¨å¼‚å¸¸åˆ†ç±»å™¨ç›¸æ¯”ï¼Œè¡¨ç°ç›¸ä¼¼ã€‚
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-on-Graphs-A-Comprehensive-Survey"><a href="#Large-Language-Models-on-Graphs-A-Comprehensive-Survey" class="headerlink" title="Large Language Models on Graphs: A Comprehensive Survey"></a>Large Language Models on Graphs: A Comprehensive Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02783">http://arxiv.org/abs/2312.02783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/petergriffinjin/awesome-language-model-on-graphs">https://github.com/petergriffinjin/awesome-language-model-on-graphs</a></li>
<li>paper_authors: Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼ˆLLMï¼‰åœ¨å›¾ ÑÑ‚Ñ€ÑƒĞºuctureä¸Šçš„åº”ç”¨åœºæ™¯å’ŒæŠ€æœ¯ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸‰ç§ç±»åˆ«çš„åº”ç”¨åœºæ™¯ï¼šçº¯å›¾ã€æ–‡æœ¬ ric å›¾å’Œæ–‡æœ¬å¯¹åº”çš„å›¾ã€‚åŒæ—¶ï¼Œä¹Ÿä»‹ç»äº†å¤šç§åˆ©ç”¨ LLM åœ¨å›¾ä¸Šçš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬ LLM ä½œä¸ºé¢„æµ‹å™¨ã€ç¼–ç å™¨å’Œå¯¹é½å™¨ã€‚</li>
<li>results: æœ¬æ–‡ç»“åˆäº†å¤šç§å®éªŒå’Œåº”ç”¨åœºæ™¯ï¼Œå¹¶æä¾›äº†ç›¸å…³çš„å¼€æºä»£ç å’Œbenchmarkæ•°æ®é›†ã€‚æœªæ¥ç ”ç©¶å¯èƒ½åŒ…æ‹¬ç»†åŒ–å›¾ç»“æ„å’Œæ–‡æœ¬ä¿¡æ¯çš„ç»“åˆã€æé«˜æ¨¡å‹æ€§èƒ½å’Œæ‰©å±•åˆ°æ›´å¤šçš„åº”ç”¨åœºæ™¯ã€‚<details>
<summary>Abstract</summary>
Large language models (LLMs), such as ChatGPT and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data are associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graph scenarios (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-rich graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we mention the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¦‚ChatGPTå’ŒLLaMAï¼Œåœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢å·²ç»å–å¾—äº†é‡è¦çªç ´ï¼Œå®ƒä»¬çš„å¼ºå¤§æ–‡æœ¬ç¼–ç /è§£ç èƒ½åŠ›å’Œæ–°å‘ç°çš„emergentcapabilityï¼ˆä¾‹å¦‚ï¼Œç†è§£ï¼‰ä½¿å¾—å®ƒä»¬åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ç„¶è€Œï¼ŒLLMä¸»è¦æ˜¯ä¸ºçº¯æ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œå®é™…ä¸–ç•Œä¸­æœ‰è®¸å¤šæƒ…å†µwhereæ–‡æœ¬æ•°æ®å…·æœ‰richç»“æ„ä¿¡æ¯çš„å½¢å¼ï¼ˆä¾‹å¦‚ï¼Œå­¦æœ¯ç½‘ç»œå’Œç”µå•†ç½‘ç»œï¼‰æˆ–æƒ…å†µwhereå›¾æ•°æ®ä¸richæ–‡æœ¬ä¿¡æ¯ç›¸å…³ï¼ˆä¾‹å¦‚ï¼Œåˆ†å­ä¸æè¿°ï¼‰ã€‚æ­¤å¤–ï¼Œå°½ç®¡LLMå·²ç»æ˜¾ç¤ºäº†çº¯æ–‡æœ¬åŸºäºçš„ç†è§£èƒ½åŠ›ï¼Œä½†æ˜¯è¿™ç§èƒ½åŠ›æ˜¯å¦å¯ä»¥æ³›åŒ–åˆ°å›¾å½¢enarioï¼ˆå³å›¾å½¢åŸºäºçš„ç†è§£ï¼‰å°šæœªå¾—åˆ°å……åˆ†çš„æ¢è®¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¤§å‹è‡ªç„¶è¯­è¨€æ¨¡å‹åœ¨å›¾å½¢ä¸Šçš„ç³»ç»Ÿæ€§è¯„è®ºã€‚æˆ‘ä»¬é¦–å…ˆæ€»ç»“äº†å°†LLMåº”ç”¨äºå›¾å½¢çš„å¯èƒ½enario into three categoriesï¼šçº¯å›¾å½¢ã€æ–‡æœ¬richå›¾å½¢å’Œæ–‡æœ¬å¯¹åº”å›¾å½¢ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨å›¾å½¢ä¸Šä½¿ç”¨LLMçš„è¯¦ç»†æŠ€æœ¯ï¼ŒåŒ…æ‹¬LLMä½œä¸ºé¢„æµ‹å™¨ã€LLMä½œä¸ºç¼–ç å™¨å’ŒLLMä½œä¸ºå¯¹é½å™¨ï¼Œå¹¶å¯¹ä¸åŒçš„å­¦æœ¯æ¨¡å‹æœ‰åˆ©å’Œä¸åˆ©çš„æ¯”è¾ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æåˆ°äº†å®é™…åº”ç”¨çš„æ–¹æ³•å’Œå¼€æºä»£ç åº“ä»¥åŠæ ‡å‡† benchmarkæ•°æ®é›†ã€‚æœ€åï¼Œæˆ‘ä»¬ç»“æŸäºæœªæ¥ç ”ç©¶æ–¹å‘çš„æ½œåœ¨å‘å±•ç©ºé—´ã€‚ç›¸å…³çš„æºä»£ç å¯ä»¥åœ¨https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphsä¸­æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="Scaling-Laws-for-Adversarial-Attacks-on-Language-Model-Activations"><a href="#Scaling-Laws-for-Adversarial-Attacks-on-Language-Model-Activations" class="headerlink" title="Scaling Laws for Adversarial Attacks on Language Model Activations"></a>Scaling Laws for Adversarial Attacks on Language Model Activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02780">http://arxiv.org/abs/2312.02780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stanislav Fort</li>
<li>for: è¿™ç¯‡è®ºæ–‡ä¸»è¦ç ”ç©¶äº†è¯­è¨€æ¨¡å‹ä¸Šçš„åå¯¹æ”»å‡»ã€‚</li>
<li>methods: ä½œè€…ä½¿ç”¨äº†ä¸€ç§åŸºäºæ´»åŠ¨çš„åå¯¹æ”»å‡»æ–¹æ³•ï¼Œé€šè¿‡æ§åˆ¶ä¸€å°éƒ¨åˆ†æ¨¡å‹æ´»åŠ¨æ¥æ§åˆ¶åç»­Tokençš„é¢„æµ‹ç»“æœã€‚</li>
<li>results: ä½œè€…å‘ç°ï¼Œå¯¹äºä¸åŒçš„è¯­è¨€æ¨¡å‹å’Œè¾“å…¥å¤§å°ï¼Œæœ€å¤šå¯ä»¥æ§åˆ¶1000ä¸ªåç»­Tokençš„é¢„æµ‹ç»“æœï¼Œå¹¶è§‚å¯Ÿåˆ°äº†ä¸€ä¸ªå·ç§¯çº§æ•°å­¦å¾‹ï¼Œå³æœ€å¤§é¢„æµ‹ç»“æœæ•°é‡ä¸æ§åˆ¶æ´»åŠ¨æ•°é‡ç›´çº¿ç›¸å…³ã€‚æ­¤å¤–ï¼Œä½œè€…å‘ç°ï¼Œå¯¹äºä¸åŒçš„è¾“å…¥ç©ºé—´å’Œè¾“å‡ºç©ºé—´ç»´åº¦ï¼Œåå¯¹æ”»å‡»çš„é¡ºåºæ€§å¾ˆå¼ºï¼Œå³ä¸€ä¸ªè¾“å…¥ä½ç½®çš„æ§åˆ¶å¯ä»¥å¯¼è‡´ç›¸åº”çš„è¾“å‡ºä½ç½®çš„æ§åˆ¶ã€‚è¿™äº›ç»“æœæ”¯æŒäº†ç»´åº¦ä¸åŒ¹é…çš„å‡è®¾ï¼Œå¹¶ä¸ºè¯­è¨€æ¨¡å‹ä¸Šçš„åå¯¹æ”»å‡»æä¾›äº†ä¸€ä¸ªæ–°çš„æ”»å‡» Ğ¿Ğ¾Ğ²ĞµÑ€ã€‚<details>
<summary>Abstract</summary>
We explore a class of adversarial attacks targeting the activations of language models. By manipulating a relatively small subset of model activations, $a$, we demonstrate the ability to control the exact prediction of a significant number (in some cases up to 1000) of subsequent tokens $t$. We empirically verify a scaling law where the maximum number of target tokens $t_\mathrm{max}$ predicted depends linearly on the number of tokens $a$ whose activations the attacker controls as $t_\mathrm{max} = \kappa a$. We find that the number of bits of control in the input space needed to control a single bit in the output space (what we call attack resistance $\chi$) is remarkably constant between $\approx 16$ and $\approx 25$ over 2 orders of magnitude of model sizes for different language models. Compared to attacks on tokens, attacks on activations are predictably much stronger, however, we identify a surprising regularity where one bit of input steered either via activations or via tokens is able to exert control over a similar amount of output bits. This gives support for the hypothesis that adversarial attacks are a consequence of dimensionality mismatch between the input and output spaces. A practical implication of the ease of attacking language model activations instead of tokens is for multi-modal and selected retrieval models, where additional data sources are added as activations directly, sidestepping the tokenized input. This opens up a new, broad attack surface. By using language models as a controllable test-bed to study adversarial attacks, we were able to experiment with input-output dimensions that are inaccessible in computer vision, especially where the output dimension dominates.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç±»è¯­è¨€æ¨¡å‹çš„å¯¹æŠ—æ”»å‡»ï¼Œé€šè¿‡æ§åˆ¶è¯­è¨€æ¨¡å‹çš„æ¿€æ´»æ¥æ§åˆ¶é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬é€šè¿‡å¯¹ä¸€å°éƒ¨åˆ†æ¨¡å‹æ¿€æ´»è¿›è¡Œæ“ä½œï¼Œæ§åˆ¶äº†åç»­å¤šè¾¾1000ä¸ªå­—ç¬¦çš„é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬ç»éªŒæ€§åœ°è¯æ˜äº†ä¸€ä¸ªå°ºåº¦æ³•å¾‹ï¼Œå…¶ä¸­æœ€å¤§é¢„æµ‹ç»“æœçš„æ•°é‡ä¸æ§åˆ¶æ¨¡å‹æ¿€æ´»çš„æ•°é‡æˆç›´çº¿å…³ç³»ï¼Œå³$t_\mathrm{max} = \kappa a$ã€‚æˆ‘ä»¬å‘ç°åœ¨ä¸åŒçš„è¯­è¨€æ¨¡å‹ä¸­ï¼Œå¯¹è¾“å…¥ç©ºé—´ä¸­çš„ä¸€ä¸ªæ¯”ç‰¹æ§åˆ¶è¾“å‡ºç©ºé—´ä¸­çš„ä¸€ä¸ªæ¯”ç‰¹çš„éœ€è¦çš„æ¯”ç‰¹æ•°ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºé˜²å¾¡æ€§$\chi$ï¼‰åœ¨2ä¸ªæ•°é‡çº§ä¹‹é—´å…·æœ‰remarkablyå¸¸æ•°çš„æ€§ã€‚ç›¸æ¯”äºåœ¨å­—ç¬¦ä¸Šè¿›è¡Œæ”»å‡»ï¼Œåœ¨æ¿€æ´»ä¸Šè¿›è¡Œæ”»å‡»æ›´å¼ºï¼Œä½†æˆ‘ä»¬å‘ç°äº†ä¸€ç§æƒŠäººçš„è§„å¾‹ï¼šä¸€ä¸ªè¾“å…¥æ¯”ç‰¹é€šè¿‡æ¿€æ´»æˆ–å­—ç¬¦æ¥æ§åˆ¶è¾“å‡ºç©ºé—´ä¸­çš„ç›¸ä¼¼æ•°é‡çš„æ¯”ç‰¹ã€‚è¿™ç»™äº†æˆ‘ä»¬å¯¹ç»´åº¦åŒ¹é…é—®é¢˜çš„æ”¯æŒï¼Œè¡¨æ˜å¯¹æŠ—æ”»å‡»æ˜¯å› ä¸ºè¾“å…¥å’Œè¾“å‡ºç©ºé—´ä¹‹é—´çš„ç»´åº¦ä¸åŒ¹é…ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨è¯­è¨€æ¨¡å‹ä½œä¸ºå¯æ§çš„æµ‹è¯•åºŠï¼Œæ¥ç ”ç©¶å¯¹æŠ—æ”»å‡»ï¼Œè¿™ä¼šå¼€ upä¸€ä¸ªæ–°çš„ã€å¹¿é˜”çš„æ”»å‡»è¡¨é¢ã€‚
</details></li>
</ul>
<hr>
<h2 id="Compositional-Generalization-for-Data-to-Text-Generation"><a href="#Compositional-Generalization-for-Data-to-Text-Generation" class="headerlink" title="Compositional Generalization for Data-to-Text Generation"></a>Compositional Generalization for Data-to-Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02748">http://arxiv.org/abs/2312.02748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinnuo Xu, Ivan Titov, Mirella Lapata</li>
<li>for: è¿™ç¯‡è®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ä¸ªåŸºå‡†æµ‹è¯•é›†ï¼Œç”¨äºè¯„ä¼°ä¸åŒæ–¹æ³•åœ¨å¤„ç†æœªçœ‹åˆ°çš„ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ predicate æ—¶çš„æ€§èƒ½ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°† predicate åˆ†ç»„ clusteringï¼Œç„¶åä¸€ sentence ä¸€ sentence åœ°ç”Ÿæˆæ–‡æœ¬æè¿°ã€‚</li>
<li>results: è¯¥æ¨¡å‹åœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šéƒ½è¶…è¿‡ T5 åŸºå‡†å€¼ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒè¾“å…¥å¿ å®åº¦æ–¹é¢æé«˜äº†31%ã€‚<details>
<summary>Abstract</summary>
Data-to-text generation involves transforming structured data, often represented as predicate-argument tuples, into coherent textual descriptions. Despite recent advances, systems still struggle when confronted with unseen combinations of predicates, producing unfaithful descriptions (e.g. hallucinations or omissions). We refer to this issue as compositional generalisation, and it encouraged us to create a benchmark for assessing the performance of different approaches on this specific problem. Furthermore, we propose a novel model that addresses compositional generalization by clustering predicates into groups. Our model generates text in a sentence-by-sentence manner, relying on one cluster of predicates at a time. This approach significantly outperforms T5~baselines across all evaluation metrics.Notably, it achieved a 31% improvement over T5 in terms of a metric focused on maintaining faithfulness to the input.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>å°†ç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æè¿°ï¼Œç»å¸¸ç”¨ä½œ predicate-argument å¯¹çš„è½¬æ¢ã€‚å°½ç®¡æœ€è¿‘å‡ å¹´æœ‰æ‰€è¿›æ­¥ï¼Œä»ç„¶ç³»ç»Ÿåœ¨ä¸familiarçš„ predicate ç»„åˆä¸‹é‡åˆ°å›°éš¾ï¼Œå¯¼è‡´ä¸å‡†ç¡®çš„æè¿°ï¼ˆå¦‚å¹»è§‰æˆ–ç¼ºå¤±ï¼‰ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªé—®é¢˜ä¸º Compositional Generalizationï¼Œå®ƒé©±ä½¿æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªè¯„ä»·ä¸åŒæ–¹æ³•çš„æ ‡å‡†å‡†æµ‹è¯•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œå®ƒå°† predicate åˆ†ç»„ clusteringã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨ sentence-by-sentence æ–¹å¼ç”Ÿæˆæ–‡æœ¬ï¼Œæ¯æ¬¡ä¾èµ–ä¸€ä¸ª cluster of predicateã€‚è¿™ç§æ–¹æ³•åœ¨æ‰€æœ‰è¯„ä»·æŒ‡æ ‡ä¸Šéƒ½å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¿æŒè¾“å…¥çš„å‡†ç¡®æ€§æ–¹é¢æé«˜äº†31%ã€‚Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Towards-Measuring-Representational-Similarity-of-Large-Language-Models"><a href="#Towards-Measuring-Representational-Similarity-of-Large-Language-Models" class="headerlink" title="Towards Measuring Representational Similarity of Large Language Models"></a>Towards Measuring Representational Similarity of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02730">http://arxiv.org/abs/2312.02730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mklabunde/llm_repsim">https://github.com/mklabunde/llm_repsim</a></li>
<li>paper_authors: Max Klabunde, Mehdi Ben Amor, Michael Granitzer, Florian Lemmerich</li>
<li>for: æœ¬ç ”ç©¶ç›®çš„æ˜¯äº†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œä»¥ä¾¿ç®€åŒ–æ¨¡å‹é€‰æ‹©ã€æ£€æµ‹éæ³•æ¨¡å‹é‡å¤ä½¿ç”¨å’Œæé«˜æˆ‘ä»¬å¯¹ LLMs çš„æ€§èƒ½åŸç†çš„ç†è§£ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†7äº¿ä¸ªå‚æ•°çš„ LLMs çš„è¡¨ç¤ºç›¸ä¼¼æ€§è¿›è¡Œæµ‹é‡ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œä¸€äº› LLMs ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„è¡¨ç¤ºç›¸ä¼¼æ€§å·®å¼‚ã€‚ç ”ç©¶è¿˜å‘ç°äº†ä½¿ç”¨è¡¨ç¤ºç›¸ä¼¼æ€§æŒ‡æ ‡çš„æŒ‘æˆ˜ï¼Œéœ€è¦ä»”ç»†ç ”ç©¶ç›¸ä¼¼æ€§åˆ†æ•°ä»¥é¿å…é”™è¯¯ç»“è®ºã€‚<details>
<summary>Abstract</summary>
Understanding the similarity of the numerous released large language models (LLMs) has many uses, e.g., simplifying model selection, detecting illegal model reuse, and advancing our understanding of what makes LLMs perform well. In this work, we measure the similarity of representations of a set of LLMs with 7B parameters. Our results suggest that some LLMs are substantially different from others. We identify challenges of using representational similarity measures that suggest the need of careful study of similarity scores to avoid false conclusions.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç†è§£å„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç›¸ä¼¼æ€§æœ‰å¾ˆå¤šç”¨é€”ï¼Œä¾‹å¦‚ç®€åŒ–æ¨¡å‹é€‰æ‹©ã€æ£€æµ‹éæ³•æ¨¡å‹é‡ç”¨å’Œæé«˜æˆ‘ä»¬å¯¹LLMsè¡¨ç°è‰¯å¥½çš„ç†è§£ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æµ‹é‡äº†ä¸€ç»„LLMsçš„è¡¨ç¤ºç›¸ä¼¼æ€§ï¼Œå…¶ä¸­å‚æ•°æ•°é‡è¾¾7äº¿ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ä¸€äº›LLMsä¸å…¶ä»–æ¨¡å‹å­˜åœ¨å·¨å¤§å·®å¼‚ã€‚æˆ‘ä»¬å‘ç°å¯¹è¡¨ç¤ºç›¸ä¼¼æ€§æŒ‡æ ‡çš„ä½¿ç”¨å­˜åœ¨æŒ‘æˆ˜ï¼Œéœ€è¦ä»”ç»†ç ”ç©¶ç›¸ä¼¼æ€§åˆ†æ•°ä»¥é¿å…è¯¯å¯¼æ€§çš„ç»“è®ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Prompt-Optimization-via-Adversarial-In-Context-Learning"><a href="#Prompt-Optimization-via-Adversarial-In-Context-Learning" class="headerlink" title="Prompt Optimization via Adversarial In-Context Learning"></a>Prompt Optimization via Adversarial In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02614">http://arxiv.org/abs/2312.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy F. Chen, Kenji Kawaguchi, Michael Qizhe Xie, Junxian He</li>
<li>For: ä¼˜åŒ–å—Contextå­¦ä¹ ï¼ˆICLï¼‰çš„æç¤ºï¼Œä½¿ç”¨ä¸€ä¸ªLLMä½œä¸ºç”Ÿæˆå™¨ï¼Œå¦ä¸€ä¸ªä½œä¸ºåˆ†ç±»å™¨ï¼Œå¹¶æœ‰ä¸€ä¸ªæç¤ºä¿®æ”¹å™¨ã€‚* Methods: ä½¿ç”¨ä¼ ç»Ÿå¯¹æŠ—å­¦ä¹ çš„æ–¹å¼ï¼Œç”Ÿæˆå™¨å°è¯•ç”ŸæˆçœŸå®çš„è¾“å‡ºï¼Œä»¥è®©åˆ†ç±»å™¨éš¾ä»¥åˆ†è¾¨æ˜¯æ¨¡å‹ç”Ÿæˆçš„æˆ–å®é™…æ•°æ®ã€‚åœ¨æ¯ä¸ªå›åˆä¸­ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥ï¼ŒåŒ…æ‹¬ä»»åŠ¡æŒ‡ä»¤å’Œä¸€äº›ç¤ºä¾‹ï¼Œç”Ÿæˆå™¨ç”Ÿæˆè¾“å‡ºï¼Œè€Œåˆ†ç±»å™¨åˆ™å°†ç”Ÿæˆå™¨è¾“å…¥-è¾“å‡ºå¯¹åˆ†ç±»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æˆ–å®é™…æ•°æ®ã€‚åŸºäºåˆ†ç±»å™¨æŸå¤±ï¼Œæç¤ºä¿®æ”¹å™¨æè®®å¯èƒ½çš„ç¼–è¾‘ï¼Œå¹¶é€‰æ‹©æœ€æ”¹è¿›å¯¹æŠ—æŸå¤±çš„ç¼–è¾‘ã€‚* Results: æ¯”å¯¹state-of-the-artæç¤ºä¼˜åŒ–æŠ€æœ¯ï¼Œadv-ICLåœ¨11ç§ç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ä¸Šè·å¾—äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬æ¦‚è¦ã€æ•°å­¦é€»è¾‘ã€æœºå™¨ç¿»è¯‘ã€æ•°æ®-æ–‡æœ¬ç”Ÿæˆå’ŒMMLUå’Œbig-benchéš¾åº¦ benchmarksã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å’Œåªæ›´æ–°æç¤ºè€Œä¸æ˜¯æ¨¡å‹å‚æ•°ï¼Œå› æ­¤å®ƒæ˜¯è®¡ç®—æ•ˆç‡é«˜ã€æ˜“äºæ‰©å±•åˆ°ä»»ä½•LLMå’Œä»»åŠ¡ï¼Œå¹¶åœ¨ä½èµ„æºç¯å¢ƒä¸‹æ•ˆæœä¼˜ç§€ã€‚<details>
<summary>Abstract</summary>
We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 11 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, because our method uses pre-trained models and updates only prompts rather than model parameters, it is computationally efficient, easy to extend to any LLM and task, and effective in low-resource settings.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼Œcalled adversarial in-context learningï¼ˆadv-ICLï¼‰ï¼Œç”¨äºä¼˜åŒ–å¯å‘ï¼ˆICLï¼‰ä¸­çš„å¯å‘ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªLLMä½œä¸ºç”Ÿæˆå™¨ï¼Œå¦ä¸€ä¸ªä½œä¸ºè¯†åˆ«å™¨ï¼Œä»¥åŠä¸€ä¸ªä½œä¸ºå¯å‘ä¿®æ”¹å™¨ã€‚åœ¨ä¼ ç»Ÿçš„å¯¹æŠ—å­¦ä¹ ä¸­ï¼Œadv-ICLæ˜¯ä½œä¸ºä¸¤ä¸ªç©å®¶çš„æ¸¸æˆï¼Œç”Ÿæˆå™¨å°è¯•ç”Ÿæˆè¶³å¤ŸçœŸå®çš„è¾“å‡ºï¼Œä»¥æ¬ºéª—è¯†åˆ«å™¨ã€‚åœ¨æ¯ä¸ªè½®æ¬¡ä¸­ï¼Œç»™å®šä¸€ä¸ªåŒ…å«ä»»åŠ¡æŒ‡ä»¤å’Œå‡ ä¸ªç¤ºä¾‹çš„è¾“å…¥ï¼Œç”Ÿæˆå™¨ç”Ÿæˆè¾“å‡ºã€‚è¯†åˆ«å™¨åˆ™è¢«è¦æ±‚å°†ç”Ÿæˆå™¨è¾“å…¥-è¾“å‡ºå¯¹åˆ†ç±»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æˆ–çœŸå®æ•°æ®ã€‚åŸºäºè¯†åˆ«å™¨çš„æŸå¤±ï¼Œå¯å‘ä¿®æ”¹å™¨æå‡ºäº†å¯èƒ½çš„ä¿®æ”¹ï¼Œå¹¶é€‰æ‹©ä¿®æ”¹æœ€å¤§åŒ–å¯¹æŠ—æŸå¤±çš„é€‰é¡¹ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œadv-ICLåœ¨11ç§ç”Ÿæˆå’Œåˆ†ç±»ä»»åŠ¡ä¸Šæ¯”çŠ¶æ€æœ€ä½³çš„å¯å‘ä¼˜åŒ–æŠ€æœ¯å…·æœ‰æ˜¾è‘—æ”¹è¿›ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹å’Œåªæ›´æ–°å¯å‘ï¼Œè€Œä¸æ˜¯æ¨¡å‹å‚æ•°ï¼Œå› æ­¤å®ƒæ˜¯è®¡ç®—æ•ˆç‡é«˜ã€æ˜“äºæ‰©å±•åˆ°ä»»ä½•LLMå’Œä»»åŠ¡ï¼Œä»¥åŠåœ¨ä½èµ„æºç¯å¢ƒä¸­æœ‰æ•ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Text-Intimacy-Analysis-using-Ensembles-of-Multilingual-Transformers"><a href="#Text-Intimacy-Analysis-using-Ensembles-of-Multilingual-Transformers" class="headerlink" title="Text Intimacy Analysis using Ensembles of Multilingual Transformers"></a>Text Intimacy Analysis using Ensembles of Multilingual Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02590">http://arxiv.org/abs/2312.02590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanmay Chavan, Ved Patwardhan</li>
<li>for: æœ¬æ–‡æ—¨åœ¨é¢„æµ‹ç»™å®šæ–‡æœ¬ä¸­çš„æƒ…æ„Ÿå±‚æ¬¡ï¼Œå³äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸äººç±»ä¹‹é—´çš„ç›´æ¥äº’åŠ¨å¢åŠ äº†è¿™ä¸€é—®é¢˜çš„é‡è¦æ€§ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨ä¸€ä¸ªå¤šè¯­è¨€æ¨¡å‹çš„ ensemble ä»¥åŠä¸€ä¸ªå•è¯­è¨€æ¨¡å‹æ¥é¢„æµ‹æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿå±‚æ¬¡ï¼Œå¹¶è¿›è¡Œäº†å¤šç§æ•°æ®æ‰©å±•æ–¹æ³•çš„è¯•éªŒï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€‚</li>
<li>results: ç»“æœæ˜¾ç¤ºï¼Œ ensemble æ¨¡å‹å’Œå•è¯­è¨€æ¨¡å‹çš„ç»„åˆä»¥åŠæ•°æ®æ‰©å±•æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹æ€§èƒ½ï¼Œå¹¶ä¸”è¿›è¡Œäº†è¯¦ç»†çš„ç»“æœåˆ†æï¼Œæä¾›äº†ä¸€äº›æœ‰è¶£çš„æƒ…æ„Ÿé¢„æµ‹é—®é¢˜çš„æ·±å…¥ç†è§£ã€‚<details>
<summary>Abstract</summary>
Intimacy estimation of a given text has recently gained importance due to the increase in direct interaction of NLP systems with humans. Intimacy is an important aspect of natural language and has a substantial impact on our everyday communication. Thus the level of intimacy can provide us with deeper insights and richer semantics of conversations. In this paper, we present our work on the SemEval shared task 9 on predicting the level of intimacy for the given text. The dataset consists of tweets in ten languages, out of which only six are available in the training dataset. We conduct several experiments and show that an ensemble of multilingual models along with a language-specific monolingual model has the best performance. We also evaluate other data augmentation methods such as translation and present the results. Lastly, we study the results thoroughly and present some noteworthy insights into this problem.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‘å¹´æ¥ï¼Œä¸äººå·¥æ™ºèƒ½ç›´æ¥äº¤äº’çš„è¯­è¨€å¤„ç†ç³»ç»Ÿçš„å‘å±•ï¼Œä½¿å¾—è·ç¦»æ„Ÿåº¦çš„ä¼°è®¡å¾—åˆ°äº†æ›´å¤šçš„é‡è§†ã€‚è·ç¦»æ„Ÿåº¦æ˜¯è‡ªç„¶è¯­è¨€ä¸­é‡è¦çš„ä¸€ä¸ªæ–¹é¢ï¼Œå¯¹æˆ‘ä»¬æ—¥å¸¸äº¤æµäº§ç”Ÿäº†æ·±è§‚å½±å“ã€‚å› æ­¤ï¼Œè·ç¦»æ„Ÿåº¦çš„çº§åˆ«å¯ä»¥ä¸ºæˆ‘ä»¬æä¾›æ›´æ·±åˆ»çš„ç†è§£å’Œæ›´å¯Œæœ‰çš„ semanticsã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬åœ¨SemEvalå…±äº«ä»»åŠ¡9ä¸­å¯¹ç»™å®šæ–‡æœ¬è·ç¦»æ„Ÿåº¦çš„é¢„æµ‹å·¥ä½œã€‚æ•°æ®é›†åŒ…æ‹¬æ¨ç‰¹åœ¨åç§è¯­è¨€ä¸­çš„åä¸‡å¥ï¼Œå…¶ä¸­åªæœ‰å…­ç§è¯­è¨€å¯ä»¥åœ¨è®­ç»ƒé›†ä¸­ä½¿ç”¨ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¤šä¸ªå®éªŒï¼Œå¹¶è¯æ˜äº†ä¸€ä¸ªå¤šè¯­è¨€æ¨¡å‹çš„ensembleï¼Œä»¥åŠä¸€ä¸ªå•è¯­è¨€æ¨¡å‹åœ¨æ¯ç§è¯­è¨€ä¸­çš„æœ€ä½³æ€§èƒ½ã€‚æˆ‘ä»¬è¿˜è¯„ä¼°äº†å…¶ä»–æ•°æ®æ‰©å……æ–¹æ³•ï¼Œå¦‚ç¿»è¯‘ï¼Œå¹¶å‘ç°äº†ä¸€äº›æœ‰è¶£çš„é—®é¢˜ã€‚æœ€åï¼Œæˆ‘ä»¬è¿›è¡Œäº†æ·±å…¥çš„åˆ†æå’Œè®¨è®ºã€‚
</details></li>
</ul>
<hr>
<h2 id="Empathy-and-Distress-Detection-using-Ensembles-of-Transformer-Models"><a href="#Empathy-and-Distress-Detection-using-Ensembles-of-Transformer-Models" class="headerlink" title="Empathy and Distress Detection using Ensembles of Transformer Models"></a>Empathy and Distress Detection using Ensembles of Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02578">http://arxiv.org/abs/2312.02578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanmay Chavan, Kshitij Deshpande, Sheetal Sonawane</li>
<li>for: æœ¬æ–‡æ˜¯å…³äº2023å¹´WASSA Empathyã€Emotionå’ŒPersonalityå…±åŒä»»åŠ¡çš„æ–¹æ³•ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨BERTåŸºäºæ¨¡å‹å’Œå„ç§ensembleæ–¹æ³•è¿›è¡Œå®éªŒã€‚</li>
<li>results: æˆ‘ä»¬çš„æœ€ç»ˆæäº¤å¾—åˆ†ä¸ºPearsonâ€™s råˆ†æ•°0.346ï¼Œåœ¨Empathyå’ŒDistressæ£€æµ‹å­ä»»åŠ¡ä¸­æ’åç¬¬ä¸‰ã€‚<details>
<summary>Abstract</summary>
This paper presents our approach for the WASSA 2023 Empathy, Emotion and Personality Shared Task. Empathy and distress are human feelings that are implicitly expressed in natural discourses. Empathy and distress detection are crucial challenges in Natural Language Processing that can aid our understanding of conversations. The provided dataset consists of several long-text examples in the English language, with each example associated with a numeric score for empathy and distress. We experiment with several BERT-based models as a part of our approach. We also try various ensemble methods. Our final submission has a Pearson's r score of 0.346, placing us third in the empathy and distress detection subtask.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†æˆ‘ä»¬åœ¨WASSA 2023 Empathy, Emotion and Personality Shared Task ä¸­çš„æ–¹æ³•ã€‚äººä»¬çš„æ„Ÿå—æ€§å’Œç—›è‹¦æ˜¯è‡ªç„¶è¯­è¨€ä¸­çš„éšå¼è¡¨è¾¾ï¼Œæ„Ÿå—æ€§å’Œç—›è‹¦æ£€æµ‹æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„å…³é”®æŒ‘æˆ˜ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£å¯¹è¯ã€‚æä¾›çš„æ•°æ®é›†åŒ…æ‹¬è‹±è¯­é•¿æ–‡ç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½æœ‰æ„Ÿå—æ€§å’Œç—›è‹¦çš„æ•°å­—åˆ†æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨BERTæ¨¡å‹ä½œä¸ºæˆ‘ä»¬çš„æ–¹æ³•çš„ä¸€éƒ¨åˆ†ï¼Œå¹¶å°è¯•äº†å¤šç§ ensemble æ–¹æ³•ã€‚æœ€ç»ˆæäº¤çš„ç»“æœæ˜¯Pearson ç›¸å…³ç³»æ•°0.346ï¼Œä½äºæ„Ÿå—æ€§å’Œç—›è‹¦æ£€æµ‹å­ä»»åŠ¡ä¸­çš„ç¬¬ä¸‰åã€‚
</details></li>
</ul>
<hr>
<h2 id="ULMA-Unified-Language-Model-Alignment-with-Demonstration-and-Point-wise-Human-Preference"><a href="#ULMA-Unified-Language-Model-Alignment-with-Demonstration-and-Point-wise-Human-Preference" class="headerlink" title="ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference"></a>ULMA: Unified Language Model Alignment with Demonstration and Point-wise Human Preference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02554">http://arxiv.org/abs/2312.02554</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/unified-language-model-alignment/src">https://github.com/unified-language-model-alignment/src</a></li>
<li>paper_authors: Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, Guannan Zhang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜å¤§è¯­è¨€æ¨¡å‹çš„åˆç†æ€§å’Œå®‰å…¨æ€§ï¼Œé€šè¿‡å¯¹ç”¨æˆ·æ„å›¾è¿›è¡Œæ•´åˆã€‚</li>
<li>methods: ç ”ç©¶ä½¿ç”¨äº†ä¸¤æ­¥æ•´åˆæ¡†æ¶ï¼šåœ¨é¦–å…ˆæ‰¹å¤„ç†æ•°æ®ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼Œç„¶åä½¿ç”¨äººç±»åå¥½æ•°æ®è¿›è¡Œåå¥½å­¦ä¹ ã€‚</li>
<li>results: å¯¹äºç‚¹å¯¹ç‚¹åå¥½æ•°æ®ï¼Œæå‡ºäº†ä¸€ç§åä¸ºç‚¹å¯¹ç‚¹DPOçš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œå¹¶é€šè¿‡å¯¹è¶…çº§vised fine-tuningå’Œç‚¹å¯¹ç‚¹åå¥½å­¦ä¹ çš„è¿æ¥æ¥æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ã€‚å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•åœ¨ç‚¹å¯¹ç‚¹æ•°æ®é›†ä¸Šå…·æœ‰æ›´é«˜çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚åŒæ—¶ï¼Œç ”ç©¶äººå‘˜è¿˜æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ç¤ºä¾‹é›†ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥ç ”ç©¶å’Œåº”ç”¨ã€‚<details>
<summary>Abstract</summary>
Language model alignment is a cutting-edge technique in large language model training to align the model output to user's intent, e.g., being helpful and harmless. Recent alignment framework consists of two steps: supervised fine-tuning with demonstration data and preference learning with human preference data. Previous preference learning methods, such as RLHF and DPO, mainly focus on pair-wise preference data. However, in many real-world scenarios where human feedbacks are intrinsically point-wise, these methods will suffer from information loss or even fail. To fill this gap, in this paper, we first develop a preference learning method called point-wise DPO to tackle point-wise preference data. Further revelation on the connection between supervised fine-tuning and point-wise preference learning enables us to develop a unified framework for both human demonstration and point-wise preference data, which sheds new light on the construction of preference dataset. Extensive experiments on point-wise datasets with binary or continuous labels demonstrate the superior performance and efficiency of our proposed methods. A new dataset with high-quality demonstration samples on harmlessness is constructed and made publicly available.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„è¯­è¨€æ¨¡å‹å‡†ç¡®æ€§Alignmentæ˜¯ä¸€ç§å‰æ²¿æŠ€æœ¯ï¼Œç›®çš„æ˜¯å°†æ¨¡å‹è¾“å‡ºä¸ç”¨æˆ·æ„å›¾ç›¸åŒ¹é…ï¼Œä¾‹å¦‚å¸®åŠ©å’Œæ— å®³ã€‚ç°æœ‰çš„é…ç½®æ¡†æ¶åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šæœ‰ç›‘ç£å¾®è°ƒå’Œäººç±»åå¥½æ•°æ®å­¦ä¹ ã€‚è¿‡å»çš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œå¦‚RLHFå’ŒDPOï¼Œä¸»è¦å…³æ³¨å¯¹ç…§æ•°æ®ã€‚ç„¶è€Œï¼Œåœ¨å¤šæ•°å®é™…åº”ç”¨ä¸­ï¼Œäººç±»åé¦ˆæ˜¯ç‚¹å¯¹ç‚¹çš„ï¼Œè¿™äº›æ–¹æ³•ä¼šå¯¼è‡´ä¿¡æ¯æŸå¤±æˆ–è€…å¤±è´¥ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­é¦–å…ˆå¼€å‘äº†ä¸€ç§ç‚¹å¯¹ç‚¹DPOåå¥½å­¦ä¹ æ–¹æ³•ã€‚è¿›ä¸€æ­¥æ­ç¤ºäº†ç›‘ç£å¾®è°ƒå’Œç‚¹å¯¹ç‚¹åå¥½å­¦ä¹ ä¹‹é—´çš„è¿æ¥ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥æ„å»ºä¸€ä¸ªåŒ…å«äººç±»ç¤ºä¾‹å’Œç‚¹å¯¹ç‚¹åå¥½æ•°æ®çš„ç»Ÿä¸€æ¡†æ¶ã€‚å¹¿æ³›çš„å®éªŒè¡¨æ˜äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„è¶…è¿‡å’Œé«˜æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æ„å»ºäº†ä¸€ä¸ªé«˜è´¨é‡çš„ç¤ºä¾‹é›†ï¼Œå¹¶å…¬å¼€å‘å¸ƒäº†è¿™ä¸ªé›†åˆã€‚
</details></li>
</ul>
<hr>
<h2 id="DemaFormer-Damped-Exponential-Moving-Average-Transformer-with-Energy-Based-Modeling-for-Temporal-Language-Grounding"><a href="#DemaFormer-Damped-Exponential-Moving-Average-Transformer-with-Energy-Based-Modeling-for-Temporal-Language-Grounding" class="headerlink" title="DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding"></a>DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02549">http://arxiv.org/abs/2312.02549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy Nguyen, See-Kiong Ng, Luu Anh Tuan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨åœ°localizeè§†é¢‘ç‰‡æ®µå’Œè‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¹‹é—´çš„Semanticå…³ç³»ã€‚</li>
<li>methods: æˆ‘ä»¬æå‡ºäº†ä¸€ç§èƒ½é‡åŸºæ¨¡å‹æ¡†æ¶ï¼Œç”¨äºæ˜¾å¼å­¦ä¹ ç‰‡æ®µ-æŸ¥è¯¢è¾“å…¥çš„åˆ†å¸ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§åŸºäºTransformeræ¶æ„çš„DemaFormeræ¨¡å‹ï¼Œä½¿ç”¨åŠ é€Ÿé‡‡æ ·å’Œå­¦ä¹ æŠ‘æ­¢å› å­æ¥æœ‰æ•ˆåœ°ç¼–ç ç‰‡æ®µ-æŸ¥è¯¢è¾“å…¥ã€‚</li>
<li>results: æˆ‘ä»¬åœ¨å››ä¸ªå…¬å…±æ—¶é—´è¯­è¨€å›ºå®šdatasetä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œå¹¶è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ¯”åŸºelineä¸Šæ˜¾è‘—è¶…è¶Šã€‚<details>
<summary>Abstract</summary>
Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to explicitly learn moment-query distributions. Moreover, we propose DemaFormer, a novel Transformer-based architecture that utilizes exponential moving average with a learnable damping factor to effectively encode moment-query inputs. Comprehensive experiments on four public temporal language grounding datasets showcase the superiority of our methods over the state-of-the-art baselines.
</details>
<details>
<summary>æ‘˜è¦</summary>
<<SYS>>æ¨¡å—è¯­è¨€å›ºå®š seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to explicitly learn moment-query distributions. Moreover, we propose DemaFormer, a novel Transformer-based architecture that utilizes exponential moving average with a learnable damping factor to effectively encode moment-query inputs. Comprehensive experiments on four public temporal language grounding datasets showcase the superiority of our methods over the state-of-the-art baselines.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well. Let me know!
</details></li>
</ul>
<hr>
<h2 id="DRAFT-Dense-Retrieval-Augmented-Few-shot-Topic-classifier-Framework"><a href="#DRAFT-Dense-Retrieval-Augmented-Few-shot-Topic-classifier-Framework" class="headerlink" title="DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework"></a>DRAFT: Dense Retrieval Augmented Few-shot Topic classifier Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02532">http://arxiv.org/abs/2312.02532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keonwoo Kim, Younggun Lee</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ—¨åœ¨æå‡ºä¸€ä¸ªç®€å•çš„æ¡†æ¶ï¼Œç”¨äºåŸ¹è®­å‡ ä½•å°‘ç±»åˆ«æ ‡ç­¾åˆ†ç±»å™¨ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€äº›ç‰¹å®šä¸»é¢˜çš„é—®é¢˜ä½œä¸ºæŸ¥è¯¢ï¼Œä½¿ç”¨å¯†é›†æ¢ç´¢æ¨¡å‹æ¥å»ºç«‹è‡ªå®šä¹‰æ•°æ®é›†ã€‚ç„¶åï¼Œä½¿ç”¨å¤šé—®é¢˜å›ä¼ ï¼ˆMQRï¼‰ç®—æ³•æ¥å»ºç«‹è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¹¶è°ƒæ•´ä¸€ä¸ªåˆ†ç±»å™¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡Œä¸»é¢˜åˆ†ç±»ã€‚</li>
<li>results: æ ¹æ®è¯„ä¼°ç»“æœï¼Œè¿™ç¯‡è®ºæ–‡çš„ææ¡ˆæ¯”åŸºäºå†…å®¹å­¦ä¹ çš„åŸºå‡†ï¼Œå¦‚GPT-3 175Bå’ŒInstructGPT 175Bï¼Œåœ¨å‡ ä½•å°‘ç±»åˆ«æ ‡ç­¾åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°å‡ºç«äº‰æˆ–è¶…è¶Šæ€§ã€‚å°½ç®¡è¿™ç¯‡è®ºæ–‡æœ‰177å€å°‘çš„å‚æ•°æ•°é‡ï¼Œä½†å®ƒä»èƒ½å¤Ÿè¾¾åˆ°ç±»ä¼¼æ°´å¹³ï¼Œè¯æ˜å…¶æ•ˆæœã€‚<details>
<summary>Abstract</summary>
With the growing volume of diverse information, the demand for classifying arbitrary topics has become increasingly critical. To address this challenge, we introduce DRAFT, a simple framework designed to train a classifier for few-shot topic classification. DRAFT uses a few examples of a specific topic as queries to construct Customized dataset with a dense retriever model. Multi-query retrieval (MQR) algorithm, which effectively handles multiple queries related to a specific topic, is applied to construct the Customized dataset. Subsequently, we fine-tune a classifier using the Customized dataset to identify the topic. To demonstrate the efficacy of our proposed approach, we conduct evaluations on both widely used classification benchmark datasets and manually constructed datasets with 291 diverse topics, which simulate diverse contents encountered in real-world applications. DRAFT shows competitive or superior performance compared to baselines that use in-context learning, such as GPT-3 175B and InstructGPT 175B, on few-shot topic classification tasks despite having 177 times fewer parameters, demonstrating its effectiveness.
</details>
<details>
<summary>æ‘˜è¦</summary>
éšç€ä¿¡æ¯å¤šæ ·æ€§çš„å¢åŠ ï¼Œå¯¹ä»»æ„ä¸»é¢˜çš„åˆ†ç±»éœ€æ±‚æ—¥ç›Šå¢åŠ ã€‚ä¸ºè§£å†³è¿™ä¸ªæŒ‘æˆ˜ï¼Œæˆ‘ä»¬ä»‹ç»äº†DRAFTï¼Œä¸€ç§ç®€å•çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å°‘é‡ç¤ºä¾‹ä¸‹è®­ç»ƒä¸»é¢˜åˆ†ç±»å™¨ã€‚DRAFTä½¿ç”¨ç‰¹å®šä¸»é¢˜çš„å‡ ä¸ªç¤ºä¾‹ä½œä¸ºæŸ¥è¯¢æ¥æ„å»ºè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢ç›¸å…³çš„å¤šQueryRetrievalï¼ˆMQRï¼‰ç®—æ³•æ¥æ„å»ºè‡ªå®šä¹‰æ•°æ®é›†ã€‚ç„¶åï¼Œæˆ‘ä»¬ç²¾åº¦åœ°è°ƒæ•´ä¸€ä¸ªåˆ†ç±»å™¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†æ¥è¯†åˆ«ä¸»é¢˜ã€‚ä¸ºè¯æ˜æˆ‘ä»¬æè®®çš„æ–¹æ³•çš„æ•ˆæœï¼Œæˆ‘ä»¬åœ¨å¹¿æ³›ä½¿ç”¨çš„åˆ†ç±» bencmarks æ•°æ®é›†å’Œæ‰‹åŠ¨æ„é€ çš„291ä¸ªå¤šæ ·ä¸»é¢˜æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè¿™äº›æ•°æ®é›†æ¨¡æ‹Ÿäº†å®é™…åº”ç”¨ä¸­é‡åˆ°çš„å¤šæ ·å†…å®¹ã€‚DRAFTåœ¨å‡ ä¸ªåŸºelineä¸Šæ¯”å¦‚GPT-3 175Bå’ŒInstructGPT 175Bè¿›è¡Œå‡ shotä¸»é¢˜åˆ†ç±»ä»»åŠ¡æ—¶è¡¨ç°ç«äº‰æˆ–æ›´å¥½ï¼Œå³ä½¿ Parameters ç›¸å¯¹è¾ƒå°‘ï¼Œè¿™è¯´æ˜äº†å®ƒçš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="MedDM-LLM-executable-clinical-guidance-tree-for-clinical-decision-making"><a href="#MedDM-LLM-executable-clinical-guidance-tree-for-clinical-decision-making" class="headerlink" title="MedDM:LLM-executable clinical guidance tree for clinical decision-making"></a>MedDM:LLM-executable clinical guidance tree for clinical decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02441">http://arxiv.org/abs/2312.02441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Binbin Li, Tianxin Meng, Xiaoming Shi, Jie Zhai, Tong Ruan</li>
<li>for: This paper aims to address the issue of low specialization in current medical language models (LLMs) and provide a solution for LLMs to participate in clinical diagnosis decision-making.</li>
<li>methods: The authors propose a method for constructing a large-scale medical diagnostic decision-making dataset (MedDM) from flowcharts in clinical practice guidelines, and develop an approach for converting these flowcharts into standardized diagnostic decision trees. They also propose a method for reasoning on LLM-executable clinical guidance trees (CGT) and a Patient-LLM multi-turn dialogue framework.</li>
<li>results: The authors construct a knowledge base with 1202 decision trees, covering 12 hospital departments and over 500 diseases, using medical literature and flowcharts. They also demonstrate the effectiveness of their approach through experiments using a Patient-LLM multi-turn dialogue framework.<details>
<summary>Abstract</summary>
It is becoming increasingly emphasis on the importance of LLM participating in clinical diagnosis decision-making. However, the low specialization refers to that current medical LLMs can not provide specific medical advice, which are more like a medical Q\&A. And there is no suitable clinical guidance tree data set that can be used directly with LLM. To address this issue, we first propose LLM-executavle clinical guidance tree(CGT), which can be directly used by large language models, and construct medical diagnostic decision-making dataset (MedDM), from flowcharts in clinical practice guidelines. We propose an approach to screen flowcharts from medical literature, followed by their identification and conversion into standardized diagnostic decision trees. Constructed a knowledge base with 1202 decision trees, which came from 5000 medical literature and covered 12 hospital departments, including internal medicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a method for reasoning on LLM-executable CGT and a Patient-LLM multi-turn dialogue framework.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°åœ¨è¶Šæ¥è¶Šé‡è§†LLMåœ¨è¯Šæ–­å†³ç­–ä¸­çš„å‚ä¸ã€‚ç„¶è€Œï¼Œä½ç‰¹åŒ–æ„å‘³ç€å½“å‰åŒ»å­¦LLMæ— æ³•æä¾›ä¸“ä¸šåŒ»ç–—å»ºè®®ï¼Œæ›´åƒåŒ»å­¦Q&Aã€‚è€Œæ²¡æœ‰é€‚ç”¨ç›´æ¥ä½¿ç”¨LLMçš„ä¸´åºŠæŒ‡å¯¼æ ‘æ•°æ®é›†ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆæè®®LLMæ‰§è¡Œä¸´åºŠæŒ‡å¯¼æ ‘ï¼ˆCGTï¼‰ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œå¹¶æ„å»ºåŒ»ç–—è¯Šæ–­å†³ç­–æ•°æ®é›†ï¼ˆMedDMï¼‰ï¼Œä»ä¸´åºŠå®è·µæŒ‡å—ä¸­çš„æµç¨‹å›¾ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–¹æ³•ï¼Œä»åŒ»å­¦æ–‡çŒ®ä¸­é€‰æ‹©æµç¨‹å›¾ï¼Œç„¶åå°†å…¶æ ‡å‡†åŒ–å¹¶è½¬æ¢ä¸ºè¯Šæ–­å†³ç­–æ ‘ã€‚æ„å»ºäº†1202ä¸ªå†³ç­–æ ‘ï¼Œæ¥è‡ª5000ä»½åŒ»å­¦æ–‡çŒ®ï¼Œè¦†ç›–12ä¸ªåŒ»é™¢éƒ¨é—¨ï¼ŒåŒ…æ‹¬å†…ç§‘ã€å¤–ç§‘ã€å¿ƒç†åŒ»å­¦å’Œè¶…è¿‡500ç§ç–¾ç—…ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†LLMæ‰§è¡ŒCGTçš„ç†ç”±æ–¹æ³•å’Œç—…äºº-LLMå¤šTurnå¯¹è¯æ¡†æ¶ã€‚
</details></li>
</ul>
<hr>
<h2 id="Protein-Language-Model-Powered-3D-Ligand-Binding-Site-Prediction-from-Protein-Sequence"><a href="#Protein-Language-Model-Powered-3D-Ligand-Binding-Site-Prediction-from-Protein-Sequence" class="headerlink" title="Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence"></a>Protein Language Model-Powered 3D Ligand Binding Site Prediction from Protein Sequence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03016">http://arxiv.org/abs/2312.03016</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Zhang, Lei Xie</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº†é¢„æµ‹è›‹ç™½è´¨ä¸Šçš„ ligand ç»‘å®šä½ç½®ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£è›‹ç™½è´¨çš„åŠŸèƒ½å’Œå¯»æ‰¾æ–°çš„è¯ç‰©ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º LaMPSite çš„æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åªéœ€è¦è›‹ç™½è´¨çš„åºåˆ—å’Œ ligand åˆ†å­å›¾æ¥é¢„æµ‹è›‹ç™½è´¨ä¸Šçš„ ligand ç»‘å®šä½ç½®ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•é¦–å…ˆä½¿ç”¨ ESM-2 è›‹ç™½è´¨è¯­è¨€æ¨¡å‹æ¥è·å–è›‹ç™½è´¨çš„ residue-level åµŒå…¥å’Œæ¥è§¦å›¾ã€‚ç„¶åï¼Œè¯¥æ–¹æ³•ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œæ¥è®¡ç®— ligand åˆ†å­çš„ atom-level åµŒå…¥ã€‚æœ€åï¼Œè¯¥æ–¹æ³•è®¡ç®—å’Œæ›´æ–°è›‹ç™½è´¨-ligand äº¤äº’åµŒå…¥ï¼Œå¹¶æ ¹æ®æ¨æ–­å‡ºçš„è›‹ç™½è´¨æ¥è§¦å›¾å’Œ ligand è·ç¦»å›¾æ¥å¼ºåˆ¶å®æ–½ geometric çº¦æŸã€‚æœ€ç»ˆï¼Œå¯¹è›‹ç™½è´¨-ligand äº¤äº’åµŒå…¥è¿›è¡Œæ±‡æ€»ï¼Œå¯ä»¥åˆ¤æ–­è›‹ç™½è´¨ä¸Šçš„å“ªäº›æ®‹åŸºå±äºç»‘å®šä½ç½®ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒLaMPSite æ–¹æ³•å¯ä»¥ä¸åŸºeline æ–¹æ³•ç›¸æ¯”ï¼Œå¯¹äºæ²¡æœ‰ä¸‰ç»´è›‹ç™½è´¨ç»“æ„ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹è›‹ç™½è´¨ä¸Šçš„ ligand ç»‘å®šä½ç½®å…·æœ‰ç«äº‰åŠ›ã€‚è¿™æ„å‘³ç€ï¼ŒLaMPSite æ–¹æ³•å¯ä»¥ä¸ºè›‹ç™½è´¨ç»“æ„ä¿¡æ¯ä¸å®Œæ•´çš„æƒ…å†µæä¾›æ–°çš„æœºä¼š Ğ´Ğ»Ñè¯ç‰©å‘ç°ã€‚<details>
<summary>Abstract</summary>
Prediction of ligand binding sites of proteins is a fundamental and important task for understanding the function of proteins and screening potential drugs. Most existing methods require experimentally determined protein holo-structures as input. However, such structures can be unavailable on novel or less-studied proteins. To tackle this limitation, we propose LaMPSite, which only takes protein sequences and ligand molecular graphs as input for ligand binding site predictions. The protein sequences are used to retrieve residue-level embeddings and contact maps from the pre-trained ESM-2 protein language model. The ligand molecular graphs are fed into a graph neural network to compute atom-level embeddings. Then we compute and update the protein-ligand interaction embedding based on the protein residue-level embeddings and ligand atom-level embeddings, and the geometric constraints in the inferred protein contact map and ligand distance map. A final pooling on protein-ligand interaction embedding would indicate which residues belong to the binding sites. Without any 3D coordinate information of proteins, our proposed model achieves competitive performance compared to baseline methods that require 3D protein structures when predicting binding sites. Given that less than 50% of proteins have reliable structure information in the current stage, LaMPSite will provide new opportunities for drug discovery.
</details>
<details>
<summary>æ‘˜è¦</summary>
é¢„æµ‹è›‹ç™½è´¨ä¸Šçš„ ligand ç»‘å®šä½ç½®æ˜¯è›‹ç™½è´¨åŠŸèƒ½ç†è§£å’Œæ½œåœ¨è¯ç‰©æœå¯»çš„åŸºæœ¬å’Œé‡è¦ä»»åŠ¡ã€‚ç°æœ‰çš„æ–¹æ³•éƒ½éœ€è¦è¾“å…¥ç»éªŒæ€§ç¡®å®šçš„è›‹ç™½è´¨æ•´ä½“ç»“æ„ã€‚ç„¶è€Œï¼Œè¿™äº›ç»“æ„å¯èƒ½å¯¹æ–°æˆ– less-studied è›‹ç™½è´¨è€Œè¨€æ˜¯ä¸å¯è·å¾—çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº† LaMPSiteï¼Œå®ƒåªéœ€è¦è›‹ç™½è´¨åºåˆ—å’Œligand åˆ†å­å›¾ä½œä¸ºè¾“å…¥ï¼Œå¯ä»¥é¢„æµ‹ ligand ç»‘å®šä½ç½®ã€‚è›‹ç™½è´¨åºåˆ—è¢«ç”¨æ¥æ£€ç´¢ residue-level åµŒå…¥å’Œæ¥è§¦åœ°å›¾ä»é¢„è®­ç»ƒçš„ ESM-2 è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ä¸­ã€‚ligand åˆ†å­å›¾è¢« feed åˆ°ä¸€ä¸ªå›¾ç¥ç»ç½‘ç»œä¸­ï¼Œä»¥è®¡ç®— atom-level åµŒå…¥ã€‚ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—å¹¶æ›´æ–°è›‹ç™½è´¨-ligand äº’åŠ¨åµŒå…¥ï¼ŒåŸºäºè›‹ç™½è´¨ residue-level åµŒå…¥å’Œ ligand atom-level åµŒå…¥ï¼Œä»¥åŠæ¨æ–­çš„è›‹ç™½è´¨æ¥è§¦åœ°å›¾å’Œ ligand è·ç¦»åœ°å›¾çš„å‡ ä½•çº¦æŸã€‚æœ€åï¼Œä¸€ä¸ª pooling æ“ä½œåœ¨è›‹ç™½è´¨-ligand äº’åŠ¨åµŒå…¥ä¸Šè¿›è¡Œæ±‡èšï¼Œä»¥ç¡®å®šç»‘å®šä½ç½®ä¸­çš„å“ªäº›æ®‹åŸºã€‚ä¸éœ€è¦è›‹ç™½è´¨ä¸‰ç»´åæ ‡ä¿¡æ¯ï¼Œæˆ‘ä»¬çš„æè®®çš„æ¨¡å‹å¯ä»¥ä¸åŸºeline æ–¹æ³•ç›¸æ¯”ï¼Œåœ¨predicting binding sitesæ—¶è¾¾åˆ°ç«äº‰æ€§æ€§èƒ½ã€‚åœ¨ç°æœ‰çš„ Situation ä¸­ï¼Œè›‹ç™½è´¨ä¸‰ç»´ç»“æ„ä¿¡æ¯ä¸å¯é çš„æƒ…å†µä¸‹ï¼ŒLaMPSite å°†æä¾›æ–°çš„æœºä¼š Ğ´Ğ»Ñè¯ç‰©æœå¯»ã€‚
</details></li>
</ul>
<hr>
<h2 id="Efficient-Online-Data-Mixing-For-Language-Model-Pre-Training"><a href="#Efficient-Online-Data-Mixing-For-Language-Model-Pre-Training" class="headerlink" title="Efficient Online Data Mixing For Language Model Pre-Training"></a>Efficient Online Data Mixing For Language Model Pre-Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02406">http://arxiv.org/abs/2312.02406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ufere/Assingment_1">https://github.com/Ufere/Assingment_1</a></li>
<li>paper_authors: Alon Albalak, Liangming Pan, Colin Raffel, William Yang Wang</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æå‡ºä¸€ç§é«˜æ•ˆçš„åœ¨çº¿æ•°æ®æ··åˆæ–¹æ³•ï¼Œä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹çš„ä¸‹æ¸¸æ€§èƒ½ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•ä½¿ç”¨å¤šè‡‚æŠ•æ·ç®—æ³•æ¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–æ•°æ®æ··åˆæ¯”ä¾‹ï¼Œä»¥é€‚åº”å˜åŒ–çš„è®­ç»ƒåŠ¨æ€ã€‚</li>
<li>results: ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•åœ¨5æšMMLUbenchmarkä¸Šæé«˜äº†1.9%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨è®­ç»ƒè¿­ä»£æ•°ä¸Šå‡å°‘äº†19%çš„è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ŒåŒæ—¶å¢åŠ äº† negligible çš„å¢™ clock æ—¶é—´ã€‚<details>
<summary>Abstract</summary>
The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§é‡è¯­è¨€æ¨¡å‹çš„é¢„è®­æ•°æ®é€‰æ‹©æ–¹æ³•å…·æœ‰å…³é”®å½±å“ä¸‹æµ‹æ¨¡å‹çš„è¡¨ç°ï¼Œè¿™å¯¼è‡´äº†å¤§é‡çš„å·¥ä½œé›†ä¸­äºè‡ªåŠ¨å†³å®šæœ€é€‚åˆçš„æ•°æ®ä½¿ç”¨äºé¢„è®­ã€‚ç°æœ‰çš„æ•°æ®é€‰æ‹©æ–¹æ³•å—åˆ°å¤æ‚çš„è¿ç®—å’Œè®¡ç®—æˆæœ¬çš„é™åˆ¶ï¼Œå°¤å…¶æ˜¯æ¨¡å‹å’Œé¢„è®­æ•°æ®çš„è§„æ¨¡å¢åŠ ã€‚æ•°æ®æ··åˆæ–¹æ³•å¯ä»¥ç®€åŒ–æ•°æ®é€‰æ‹©çš„å¤æ‚æ€§ï¼Œä½†æ˜¯æ··åˆæ¯”ä¾‹é€šå¸¸æ˜¯åœ¨è®­ç»ƒå‰ fixing çš„ï¼Œå› æ­¤æ— æ³•é€‚åº”å˜åŒ–çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªé«˜æ•ˆçš„åœ¨çº¿æ•°æ®æ··åˆï¼ˆODMï¼‰ç®—æ³•ï¼Œèåˆäº†æ•°æ®é€‰æ‹©å’Œæ•°æ®æ··åˆçš„å…ƒç´ ã€‚åŸºäºå¤šè‡‚æªå‡»ç®—æ³•ï¼Œæˆ‘ä»¬çš„åœ¨çº¿æ–¹æ³•åœ¨è®­ç»ƒä¸­ä¼˜åŒ–æ··åˆæ¯”ä¾‹ã€‚å¾ˆæƒŠå–œåœ°ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥åœ¨è®­ç»ƒè¿­ä»£æ•°é‡ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œè®©æ¨¡å‹çš„æœ€ç»ˆè¯¯å·®ä¸ä¸‹ä¸€ä¸ªæœ€ä½³æ–¹æ³•ç›¸åŒï¼Œå‡é«˜5shot MMLUæ ‡å‡†benchmarkçš„å¯¹ç§°ç²¾åº¦ by 1.9%ï¼ŒåŒæ—¶æ·»åŠ äº†å¾ˆå°‘çš„å£æ—¶é—´ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/05/cs.CL_2023_12_05/" data-id="clq0ru6sk00hato886fgc1hkp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_12_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/12/05/cs.LG_2023_12_05/" class="article-date">
  <time datetime="2023-12-05T10:00:00.000Z" itemprop="datePublished">2023-12-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/12/05/cs.LG_2023_12_05/">cs.LG - 2023-12-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CaloQVAE-Simulating-high-energy-particle-calorimeter-interactions-using-hybrid-quantum-classical-generative-models"><a href="#CaloQVAE-Simulating-high-energy-particle-calorimeter-interactions-using-hybrid-quantum-classical-generative-models" class="headerlink" title="CaloQVAE : Simulating high-energy particle-calorimeter interactions using hybrid quantum-classical generative models"></a>CaloQVAE : Simulating high-energy particle-calorimeter interactions using hybrid quantum-classical generative models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03179">http://arxiv.org/abs/2312.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sehmimul Hoque, Hao Jia, Abhishek Abhishek, Mojde Fadaie, J. Quetzalcoatl Toledo-MarÃ­n, Tiago Vale, Roger G. Melko, Maximilian Swiatlowski, Wojciech T. Fedorko</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯ç”¨äºæè¿°å¤§å‹å“ˆä¸æ’å‡»æœº era ä¸­çš„ Computational challenges å’Œ MC  simulations çš„æ–¹æ³•ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº† recent advancements in generative models å’Œ quantum annealing æ¥å¿«é€Ÿå’Œé«˜æ•ˆåœ°æ¨¡æ‹Ÿé«˜èƒ½é‡ particles åœ¨æ¢æµ‹å™¨ä¸­çš„ä¼ æ’­ã€‚</li>
<li>results: è¿™ç¯‡è®ºæ–‡çš„ç»“æœæ˜¾ç¤ºäº†ä¸€ç§å¿«é€Ÿå’Œé«˜æ•ˆçš„ MC simulation æ–¹æ³•ï¼Œå¯ä»¥å°† statistically uncertainty é™ä½åˆ° experimental data çš„æ°´å¹³ã€‚<details>
<summary>Abstract</summary>
The Large Hadron Collider's high luminosity era presents major computational challenges in the analysis of collision events. Large amounts of Monte Carlo (MC) simulation will be required to constrain the statistical uncertainties of the simulated datasets below these of the experimental data. Modelling of high-energy particles propagating through the calorimeter section of the detector is the most computationally intensive MC simulation task. We introduce a technique combining recent advancements in generative models and quantum annealing for fast and efficient simulation of high-energy particle-calorimeter interactions.
</details>
<details>
<summary>æ‘˜è¦</summary>
Large Hadron Collider çš„é«˜ç…§åº¦æ—¶æœŸå…·æœ‰ä¸»è¦çš„è®¡ç®—æŒ‘æˆ˜ï¼Œéœ€è¦å¤§é‡çš„ Monte Carlo (MC) æ¨¡æ‹Ÿæ¥çº¦æŸå®éªŒæ•°æ®ä¸­çš„ç»Ÿè®¡ä¸ç¡®å®šæ€§ã€‚æ¨¡æ‹Ÿé«˜èƒ½ç²’å­åœ¨æ¢æµ‹å™¨ä¸­çš„ä¼ æ’­æ˜¯ MC æ¨¡æ‹Ÿæœ€ä¸ºè®¡ç®—æˆæœ¬é«˜çš„ä»»åŠ¡ã€‚æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§ combining æœ€æ–°çš„ç”Ÿæˆæ¨¡å‹å’Œé‡å­æœç´¢çš„æŠ€æœ¯ï¼Œä»¥å¿«é€Ÿå’Œé«˜æ•ˆåœ°æ¨¡æ‹Ÿé«˜èƒ½ç²’å­-æ¢æµ‹å™¨äº¤äº’ã€‚Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Active-Learning-for-Abrupt-Shifts-Change-point-Detection-via-Derivative-Aware-Gaussian-Processes"><a href="#Active-Learning-for-Abrupt-Shifts-Change-point-Detection-via-Derivative-Aware-Gaussian-Processes" class="headerlink" title="Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes"></a>Active Learning for Abrupt Shifts Change-point Detection via Derivative-Aware Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03176">http://arxiv.org/abs/2312.03176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhao, Rong Pan</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æå‡ºä¸€ç§èƒ½å¤Ÿæœ‰æ•ˆåœ°æ£€æµ‹æ•°æ®ä¸­çªç„¶å˜åŒ–çš„æ–¹æ³•ï¼Œä»¥ä¾¿å¸®åŠ©å„ä¸ªé¢†åŸŸçš„å†³ç­–å’Œèµ„æºåˆ†é…ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨Derivative-Aware Change Detectionï¼ˆDACDï¼‰æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ Gaussian Processï¼ˆGPï¼‰çš„å¯¼æ•°è¿‡ç¨‹è¿›è¡Œæ´»åŠ¨å­¦ä¹ ï¼ˆALï¼‰ï¼Œä»¥ä¾¿æœ‰æ•ˆåœ°æ£€æµ‹å˜åŒ–ç‚¹ã€‚DACDé€šè¿‡å¤šç§æ•°æ®æ”¶é›†å‡½æ•°ï¼ˆAFsï¼‰æ¥å‡è¡¡æŠ½å–å’Œæ¢ç´¢è¿‡ç¨‹ï¼Œä»è€Œæé«˜ç®—æ³•æ•ˆç‡å¹¶ç¡®ä¿å‡†ç¡®æ€§ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼ŒDACDæ–¹æ³•åœ¨å¤šç§åœºæ™¯ä¸‹è¡¨ç°å‡ºä¼˜äºå…¶ä»–æ´»åŠ¨å­¦ä¹ å˜åŒ–æ£€æµ‹æ–¹æ³•ã€‚<details>
<summary>Abstract</summary>
Change-point detection (CPD) is crucial for identifying abrupt shifts in data, which influence decision-making and efficient resource allocation across various domains. To address the challenges posed by the costly and time-intensive data acquisition in CPD, we introduce the Derivative-Aware Change Detection (DACD) method. It leverages the derivative process of a Gaussian process (GP) for Active Learning (AL), aiming to pinpoint change-point locations effectively. DACD balances the exploitation and exploration of derivative processes through multiple data acquisition functions (AFs). By utilizing GP derivative mean and variance as criteria, DACD sequentially selects the next sampling data point, thus enhancing algorithmic efficiency and ensuring reliable and accurate results. We investigate the effectiveness of DACD method in diverse scenarios and show it outperforms other active learning change-point detection approaches.
</details>
<details>
<summary>æ‘˜è¦</summary>
change-point detectionï¼ˆCPDï¼‰å¯¹äºæ•°æ®ä¸­çªç„¶å˜åŒ–çš„æ£€æµ‹æ˜¯éå¸¸é‡è¦çš„ï¼Œè¿™å¯¹å„ä¸ªé¢†åŸŸçš„å†³ç­–å’Œèµ„æºåˆ†é…äº§ç”Ÿäº†æ·±è¿œçš„å½±å“ã€‚ä¸ºäº†è§£å†³CPDä¸­æ•°æ®æ”¶é›†æ‰€éœ€çš„æˆæœ¬å’Œæ—¶é—´å›°éš¾ï¼Œæˆ‘ä»¬æå‡ºäº†Derivative-Aware Change Detectionï¼ˆDACDï¼‰æ–¹æ³•ã€‚å®ƒåˆ©ç”¨GP derivativeè¿›è¡Œæ´»åŠ¨å­¦ä¹ ï¼ˆALï¼‰ï¼Œä»¥æœ‰æ•ˆåœ°æ‰¾åˆ°å˜åŒ–ç‚¹çš„ä½ç½®ã€‚DACDé€šè¿‡å¤šä¸ªæ•°æ®æ”¶é›†å‡½æ•°ï¼ˆAFï¼‰æ¥å¹³è¡¡åˆ©ç”¨å’Œæ¢ç´¢ derivative è¿‡ç¨‹çš„æƒè¡¡ï¼Œä»è€Œæé«˜ç®—æ³•çš„æ•ˆç‡å’Œå¯é æ€§ã€‚æˆ‘ä»¬åœ¨å¤šç§æƒ…å†µä¸‹å¯¹DACDæ–¹æ³•è¿›è¡Œäº†ç ”ç©¶ï¼Œå¹¶è¯æ˜å®ƒåœ¨å…¶ä»–æ´»åŠ¨å­¦ä¹ å˜åŒ–æ£€æµ‹æ–¹æ³•çš„åŸºç¡€ä¸Šå…·æœ‰æ›´é«˜çš„æ•ˆæœã€‚
</details></li>
</ul>
<hr>
<h2 id="Adaptive-spectral-graph-wavelets-for-collaborative-filtering"><a href="#Adaptive-spectral-graph-wavelets-for-collaborative-filtering" class="headerlink" title="Adaptive spectral graph wavelets for collaborative filtering"></a>Adaptive spectral graph wavelets for collaborative filtering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03167">http://arxiv.org/abs/2312.03167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Osama Alshareet, A. Ben Hamza</li>
<li>for: æä¾›ä¸ªæ€§åŒ–çš„ITEMå»ºè®®ç»™æ½œåœ¨ç”¨æˆ·ï¼Œè§£å†³æ–°ç”¨æˆ·æ— æ³•æä¾›å……è¶³çš„è¡Œä¸ºæ•°æ®çš„å†·å¯åŠ¨é—®é¢˜ã€‚</li>
<li>methods: ä½¿ç”¨spectral graph wavelet collaborative filteringæ¡†æ¶ï¼Œå°†ç”¨æˆ·ã€itemå’Œä»–ä»¬çš„äº¤äº’è¡¨ç¤ºä¸ºä¸€ä¸ªä¸¤åˆ†å›¾ã€‚é‡‡ç”¨é€‚åº”è½¬æ¢å‡½æ•°ç¨³å®šå›¾åƒé¢‘è°±ä¸­å˜é‡çš„æ–¹æ³•ï¼Œå¹¶è®¾è®¡ä¸€ç§æ·±åº¦æ¨èæ¨¡å‹ï¼Œé€šè¿‡spectral graph waveletsåœ¨ç«¯åˆ°ç«¯çš„æ–¹å¼å­¦ä¹ ä½ç»´è¡¨ç¤ºUSERå’ŒITEMã€‚</li>
<li>results: é€šè¿‡å®éªŒè¡¨æ˜ï¼Œæå‡ºçš„æ¨¡å‹åœ¨å®é™…benchmarkæ•°æ®é›†ä¸Šè¾¾åˆ°äº† stronger baselineæ–¹æ³•çš„æ¨èæ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Collaborative filtering is a popular approach in recommender systems, whose objective is to provide personalized item suggestions to potential users based on their purchase or browsing history. However, personalized recommendations require considerable amount of behavioral data on users, which is usually unavailable for new users, giving rise to the cold-start problem. To help alleviate this challenging problem, we introduce a spectral graph wavelet collaborative filtering framework for implicit feedback data, where users, items and their interactions are represented as a bipartite graph. Specifically, we first propose an adaptive transfer function by leveraging a power transform with the goal of stabilizing the variance of graph frequencies in the spectral domain. Then, we design a deep recommendation model for efficient learning of low-dimensional embeddings of users and items using spectral graph wavelets in an end-to-end fashion. In addition to capturing the graph's local and global structures, our approach yields localization of graph signals in both spatial and spectral domains, and hence not only learns discriminative representations of users and items, but also promotes the recommendation quality. The effectiveness of our proposed model is demonstrated through extensive experiments on real-world benchmark datasets, achieving better recommendation performance compared with strong baseline methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
Our approach begins with an adaptive transfer function that leverages a power transform to stabilize the variance of graph frequencies in the spectral domain. This is followed by the design of a deep recommendation model that efficiently learns low-dimensional embeddings of users and items using spectral graph wavelets in an end-to-end fashion. Our approach not only captures the local and global structures of the graph, but also localizes graph signals in both spatial and spectral domains, leading to the learning of discriminative representations of users and items. As a result, our proposed model achieves better recommendation performance compared to strong baseline methods, as demonstrated through extensive experiments on real-world benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-for-Fast-Inference-of-Mechanistic-Modelsâ€™-Parameters"><a href="#Deep-Learning-for-Fast-Inference-of-Mechanistic-Modelsâ€™-Parameters" class="headerlink" title="Deep Learning for Fast Inference of Mechanistic Modelsâ€™ Parameters"></a>Deep Learning for Fast Inference of Mechanistic Modelsâ€™ Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03166">http://arxiv.org/abs/2312.03166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maxim Borisyak, Stefan Born, Peter Neubauer, Mariano Nicolas Cruz-Bournazou</li>
<li>for: è¿™é¡¹ç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ç›´æ¥é¢„æµ‹æœºç†æ¨¡å‹å‚æ•°çš„æ–¹æ³•ï¼Œä»¥æé«˜ç”Ÿç‰©å·¥ç¨‹ä¸­å‚æ•°ä¼°ç®—çš„æ•ˆç‡å’Œç²¾åº¦ã€‚</li>
<li>methods: è¯¥æ–¹æ³•ä½¿ç”¨äº†ä¸€ç§ç»„åˆç¥ç»ç½‘ç»œå’Œæœºç†æ¨¡å‹çš„è®­ç»ƒç¨‹åºï¼Œé€šè¿‡å¯¹å®éªŒæ•°æ®è¿›è¡Œé¢„æµ‹æ¥ç›´æ¥é¢„æµ‹æœºç†æ¨¡å‹å‚æ•°ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹æœºç†æ¨¡å‹å‚æ•°çš„æ–¹æ³•å¯ä»¥æä¾›è¾ƒå¥½çš„ä¼°ç®—ç»“æœï¼Œå¹¶ä¸”æ¯”ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ³•æ›´å¿«é€Ÿå’Œæ›´ç²¾åº¦ã€‚<details>
<summary>Abstract</summary>
Inferring parameters of macro-kinetic growth models, typically represented by Ordinary Differential Equations (ODE), from the experimental data is a crucial step in bioprocess engineering. Conventionally, estimates of the parameters are obtained by fitting the mechanistic model to observations. Fitting, however, requires a significant computational power. Specifically, during the development of new bioprocesses that use previously unknown organisms or strains, efficient, robust, and computationally cheap methods for parameter estimation are of great value. In this work, we propose using Deep Neural Networks (NN) for directly predicting parameters of mechanistic models given observations. The approach requires spending computational resources for training a NN, nonetheless, once trained, such a network can provide parameter estimates orders of magnitude faster than conventional methods. We consider a training procedure that combines Neural Networks and mechanistic models. We demonstrate the performance of the proposed algorithms on data sampled from several mechanistic models used in bioengineering describing a typical industrial batch process and compare the proposed method, a typical gradient-based fitting procedure, and the combination of the two. We find that, while Neural Network estimates are slightly improved by further fitting, these estimates are measurably better than the fitting procedure alone.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯»æ±‚macro-è¿åŠ¨ç”Ÿé•¿æ¨¡å‹å‚æ•°çš„æ¨æ–­ï¼Œé€šå¸¸è¡¨ç¤ºä¸ºå¸¸å¾®åˆ†æ–¹ç¨‹ï¼ˆODEï¼‰ï¼Œåœ¨ç”Ÿç‰©è¿‡ç¨‹å·¥ç¨‹ä¸­æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ã€‚ä¼ ç»Ÿä¸Šï¼Œå‚æ•°ä¼°ç®—é€šå¸¸é€šè¿‡æ¨¡å‹é€‚åº”æ¥è·å¾—ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦è¾ƒé«˜çš„è®¡ç®—èƒ½åŠ›ã€‚ç‰¹åˆ«æ˜¯åœ¨å¼€å‘æ–°çš„ç”Ÿç‰©è¿‡ç¨‹ä¸­ä½¿ç”¨æœªçŸ¥çš„å¾®ç”Ÿç‰©æˆ–æ ªç±»æ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿã€ç¨³å®šã€è®¡ç®—æˆæœ¬ä½çš„å‚æ•°ä¼°ç®—æ–¹æ³•å…·æœ‰é‡è¦çš„ä»·å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ç›´æ¥é¢„æµ‹æœºåˆ¶æ¨¡å‹ä¸­çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•éœ€è¦è®­ç»ƒNNçš„è®¡ç®—èµ„æºï¼Œä½†ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œå¯ä»¥åœ¨æœºåˆ¶æ¨¡å‹ä¸­æä¾›å‚æ•°ä¼°ç®—ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ³•å¿«å¾—å¤šã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ç§å°†ç¥ç»ç½‘ç»œå’Œæœºåˆ¶æ¨¡å‹ç»“åˆåœ¨ä¸€èµ·çš„è®­ç»ƒç¨‹åºã€‚æˆ‘ä»¬åœ¨æ•°æ®æ¥è‡ªå¤šç§ç”Ÿç‰©è¿‡ç¨‹ä¸­å¸¸ç”¨çš„æœºåˆ¶æ¨¡å‹ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œå¹¶ä¸ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ–¹æ³•å’Œè¿™ä¸¤ç§æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶ç¥ç»ç½‘ç»œä¼°ç®—ä¸è¿›ä¸€æ­¥é€‚åº”ä¹‹é—´å­˜åœ¨ä¸€å®šçš„æ”¹è¿›ï¼Œä½†ç¥ç»ç½‘ç»œä¼°ç®—çš„ç»“æœæ˜æ˜¾æ¯”æ¢¯åº¦ä¸‹é™æ–¹æ³•å¥½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Multitask-Learning-Can-Improve-Worst-Group-Outcomes"><a href="#Multitask-Learning-Can-Improve-Worst-Group-Outcomes" class="headerlink" title="Multitask Learning Can Improve Worst-Group Outcomes"></a>Multitask Learning Can Improve Worst-Group Outcomes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03151">http://arxiv.org/abs/2312.03151</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atharvajk98/mtl-group-robustness">https://github.com/atharvajk98/mtl-group-robustness</a></li>
<li>paper_authors: Atharva Kulkarni, Lucio Dery, Amrith Setlur, Aditi Raghunathan, Ameet Talwalkar, Graham Neubig</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨ investigating the impact of multitask learning (MTL) on worst-group accuracy, as well as exploring MTLâ€™s potential to address the challenge of group-wise fairness.</li>
<li>methods: ä½œè€…ä½¿ç”¨äº† fine-tuning æ–¹æ³•ï¼Œå¹¶åœ¨ end task æ•°æ®ä¸Šæ„å»ºäº† pre-training ç›®æ ‡ã€‚åœ¨ absence of group annotations, ä½œè€…å‘ç° multitasking å¯ä»¥ achieve better worst-group accuracy than Just-Train-Twice (JTT) æ–¹æ³•ã€‚ä½œè€…è¿˜æå‡ºäº†ä¸€ç§ modify äº† MTL çš„æ–¹æ³•ï¼Œé€šè¿‡åœ¨ joint multitask representation space ä¸­å¢åŠ æ­£åˆ™åŒ–æ¥æé«˜ worst-group accuracy.</li>
<li>results: ä½œè€…é€šè¿‡å¤§é‡ fine-tuning å®éªŒå‘ç°ï¼Œå…¶ modify äº† MTL æ–¹æ³• consistently outperforms JTT æ–¹æ³• on both worst and average group outcomesã€‚code å¯ä»¥åœ¨ <a target="_blank" rel="noopener" href="https://github.com/atharvajk98/MTL-group-robustness">https://github.com/atharvajk98/MTL-group-robustness</a> æ‰¾åˆ°ã€‚<details>
<summary>Abstract</summary>
In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¦åˆ›å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œä»¥ä¾¿æœåŠ¡äºå¤šæ ·åŒ–çš„ç”¨æˆ·ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚è¿™äº›ç³»ç»Ÿä¸ä»…éœ€è¦è¾¾åˆ°é«˜çš„å¹³å‡æ€§èƒ½ï¼Œè¿˜éœ€è¦ç¡®ä¿å¯¹å¤šä¸ªç¾¤ä½“çš„è¾“å‡ºç»“æœå…·æœ‰å…¬å¹³æ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æœºå™¨å­¦ä¹ æ–¹æ³•éƒ½æ˜¯ä¸ºäº†æé«˜æ¨¡å‹çš„å¹³å‡æ€§èƒ½è€Œè®¾è®¡çš„ï¼Œè€Œå¿½ç•¥äº†å¯¹æœ€å·®ç¾¤ä½“çš„å½±å“ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…æƒ³è¦äº†è§£å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMTLï¼‰å¯¹æœ€å·®ç¾¤ä½“ç²¾åº¦çš„å½±å“ï¼Œè¿˜æƒ³è¦æ¢ç´¢å®ƒæ˜¯å¦å¯ä»¥ç”¨æ¥è§£å†³ç¾¤ä½“å…¬å¹³æ€§çš„æŒ‘æˆ˜ã€‚æˆ‘ä»¬ä¸»è¦è€ƒè™‘äº†åœ¨ç»ƒä¹ æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å°†ç»ƒä¹ ä»»åŠ¡ä¸é¢„è®­ç»ƒç›®æ ‡ç»“åˆåœ¨ä¸€èµ·æ¥è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ã€‚åœ¨å…·æœ‰å°‘é‡æˆ–æ— ç¾¤ä½“æ³¨è§£çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å‘ç°ï¼Œé€šè¿‡å¤šä»»åŠ¡å­¦ä¹ ï¼Œå¯ä»¥ oftentimesï¼Œä½†å¹¶ä¸æ€»æ˜¯ï¼Œåœ¨æœ€å·®ç¾¤ä½“ç²¾åº¦æ–¹é¢æ¯”just-train-twiceï¼ˆJTTï¼‰æ–¹æ³•ï¼ˆLiu et al., 2021ï¼‰è¡¨ç°æ›´å¥½ã€‚åŸºäºå¯¹ sintetic data çš„å®éªŒç»“æœï¼Œæˆ‘ä»¬æè®®ä¿®æ”¹æ ‡å‡† MTLï¼Œé€šè¿‡å¯¹å…±åŒå¤šä»»åŠ¡è¡¨ç¤ºç©ºé—´è¿›è¡Œè§„èŒƒã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€é¢†åŸŸè¿›è¡Œäº†å¤§é‡çš„ç»ƒä¹ å®éªŒï¼Œå¹¶å‘ç°ï¼Œæˆ‘ä»¬çš„è§„èŒƒ MTL æ–¹æ³•åœ¨æœ€å·®å’Œå¹³å‡ç¾¤ä½“ç»“æœæ–¹é¢éƒ½èƒ½å¤Ÿè¶…è¶Š JTTã€‚æˆ‘ä»¬çš„å®˜æ–¹ä»£ç å¯ä»¥åœ¨ä»¥ä¸‹é“¾æ¥ä¸­æ‰¾åˆ°ï¼šhttps://github.com/atharvajk98/MTL-group-robustnessã€‚
</details></li>
</ul>
<hr>
<h2 id="Neural-parameter-calibration-and-uncertainty-quantification-for-epidemic-forecasting"><a href="#Neural-parameter-calibration-and-uncertainty-quantification-for-epidemic-forecasting" class="headerlink" title="Neural parameter calibration and uncertainty quantification for epidemic forecasting"></a>Neural parameter calibration and uncertainty quantification for epidemic forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03147">http://arxiv.org/abs/2312.03147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Gaskin, Tim Conrad, Grigorios A. Pavliotis, Christof SchÃ¼tte</li>
<li>For: This paper aims to accurately forecast contagion dynamics and provide uncertainty quantification for pandemic projections.* Methods: The paper uses a novel computational method that combines a neural network with an ODE model to learn probability densities on contagion parameters and provide uncertainty quantification.* Results: The paper achieves a significantly more accurate calibration and prediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes, with meaningful confidence intervals on infection figures and hospitalisation rates. The method is also shown to converge to the true posterior on a simplified SIR model of epidemics and can learn complex models from a small number of compartments.<details>
<summary>Abstract</summary>
The recent COVID-19 pandemic has thrown the importance of accurately forecasting contagion dynamics and learning infection parameters into sharp focus. At the same time, effective policy-making requires knowledge of the uncertainty on such predictions, in order, for instance, to be able to ready hospitals and intensive care units for a worst-case scenario without needlessly wasting resources. In this work, we apply a novel and powerful computational method to the problem of learning probability densities on contagion parameters and providing uncertainty quantification for pandemic projections. Using a neural network, we calibrate an ODE model to data of the spread of COVID-19 in Berlin in 2020, achieving both a significantly more accurate calibration and prediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The uncertainties on our predictions provide meaningful confidence intervals e.g. on infection figures and hospitalisation rates, while training and running the neural scheme takes minutes where MCMC takes hours. We show convergence of our method to the true posterior on a simplified SIR model of epidemics, and also demonstrate our method's learning capabilities on a reduced dataset, where a complex model is learned from a small number of compartments for which data is available.
</details>
<details>
<summary>æ‘˜è¦</summary>
COVID-19 å¤§æµè¡Œ recent ä½¿å¾—ç²¾å‡†é¢„æµ‹ä¼ æŸ“åŠ¨åŠ›å’Œå­¦ä¹ æ„ŸæŸ“å‚æ•°çš„é‡è¦æ€§å¾—åˆ°äº†æŠ›å¼ƒå…‰ç…§ã€‚åŒæ—¶ï¼Œæœ‰æ•ˆçš„æ”¿ç­–åˆ¶å®šéœ€è¦äº†è§£é¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œä»¥ä¾¿å‡†å¤‡åŒ»é™¢å’Œé‡ç—‡ç›‘æŠ¤å•å…ƒé¢å¯¹æœ€åæƒ…å†µï¼Œæ— éœ€æµªè´¹èµ„æºã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¿ç”¨äº†ä¸€ç§æ–°çš„è®¡ç®—æ–¹æ³•æ¥è§£å†³å­¦ä¹ æ„ŸæŸ“å‚æ•°çš„æ¦‚ç‡å¯†åº¦å’Œé¢„æµ‹ä¸ç¡®å®šæ€§ã€‚ä½¿ç”¨ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯¹æŸæ—2020å¹´COVID-19çš„ä¼ æŸ“æƒ…å†µè¿›è¡Œäº†æ‹Ÿåˆï¼Œå®ç°äº†è¾ƒMCMCæ ·æœ¬ schemes æ›´é«˜çš„å‡†ç¡®ç‡å’Œé¢„æµ‹ã€‚æˆ‘ä»¬çš„é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§æä¾›äº†æœ‰æ„ä¹‰çš„ä¿¡ä»»èŒƒå›´ï¼Œä¾‹å¦‚æ„ŸæŸ“äººæ•°å’ŒåŒ»é™¢åŒ–ç‡ï¼Œè€Œè®­ç»ƒå’Œè¿è¡Œç¥ç»ç½‘ç»œåªéœ€å‡ åˆ†é’Ÿï¼ŒMCMCåˆ™éœ€è¦å¤šå°‘æ—¶é—´ã€‚æˆ‘ä»¬è¯æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯¹ç®€åŒ–çš„SIRæ¨¡å‹çš„çœŸåéªŒè¿›è¡Œäº†æ”¶æ•›ï¼Œå¹¶ä¸”åœ¨å‡å°‘æ•°æ®é›†ä¸Šå±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•çš„å­¦ä¹ èƒ½åŠ›ï¼Œä»ä¸€ä¸ªå°é‡çš„åˆ†å¸ƒä¸­å­¦ä¹ å‡ºå¤æ‚çš„æ¨¡å‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Hardware-Evaluation-Framework-for-Large-Language-Model-Inference"><a href="#A-Hardware-Evaluation-Framework-for-Large-Language-Model-Inference" class="headerlink" title="A Hardware Evaluation Framework for Large Language Model Inference"></a>A Hardware Evaluation Framework for Large Language Model Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03134">http://arxiv.org/abs/2312.03134</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hengrui Zhang, August Ning, Rohan Prabhakar, David Wentzlaff<br>for:LLMCompass is a hardware evaluation framework for Large Language Models (LLMs) inference workloads, aiming to evaluate different hardware designs and optimize their performance.methods:LLMCompass includes a mapper to automatically find performance-optimal mapping and scheduling, as well as an area-based cost model to help architects reason about their design choices.results:Compared to real-world hardware, LLMCompassâ€™ estimated latency achieves an average 10.4% error rate across various operators with various input sizes and an average 4.1% error rate for LLM inference. With LLMCompass, simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done within 16 minutes on commodity hardware, including 26,400 rounds of the mapperâ€™s parameter search. The framework also explores new cost-effective hardware designs that can achieve as much as 3.41x improvement in performance&#x2F;cost compared to an NVIDIA A100.<details>
<summary>Abstract</summary>
The past year has witnessed the increasing popularity of Large Language Models (LLMs). Their unprecedented scale and associated high hardware cost have impeded their broader adoption, calling for efficient hardware designs. With the large hardware needed to simply run LLM inference, evaluating different hardware designs becomes a new bottleneck.   This work introduces LLMCompass, a hardware evaluation framework for LLM inference workloads. LLMCompass is fast, accurate, versatile, and able to describe and evaluate different hardware designs. LLMCompass includes a mapper to automatically find performance-optimal mapping and scheduling. It also incorporates an area-based cost model to help architects reason about their design choices. Compared to real-world hardware, LLMCompass' estimated latency achieves an average 10.4% error rate across various operators with various input sizes and an average 4.1% error rate for LLM inference. With LLMCompass, simulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done within 16 minutes on commodity hardware, including 26,400 rounds of the mapper's parameter search.   With the aid of LLMCompass, this work draws architectural implications and explores new cost-effective hardware designs. By reducing the compute capability or replacing High Bandwidth Memory (HBM) with traditional DRAM, these new designs can achieve as much as 3.41x improvement in performance/cost compared to an NVIDIA A100, making them promising choices for democratizing LLMs.   LLMCompass is planned to be fully open-source.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿‡å»ä¸€å¹´ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ™®åŠåº¦é€æ¸å¢é•¿ã€‚å®ƒä»¬çš„æ—  precedent çš„è§„æ¨¡å’Œç›¸åº”çš„é«˜ç¡¬ä»¶æˆæœ¬ï¼Œä½¿å¾—å®ƒä»¬çš„æ›´å¹¿æ³›çš„åº”ç”¨è¢«é˜»ç¢ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™é¡¹å·¥ä½œæå‡ºäº† LLMCompassï¼Œä¸€ä¸ªç”¨äº LLM æ¨ç†å·¥ä½œè´Ÿè·çš„ç¡¬ä»¶è¯„ä¼°æ¡†æ¶ã€‚LLMCompass å…·æœ‰å¿«é€Ÿã€å‡†ç¡®ã€å¤šæ ·åŒ–å’Œå¯ä»¥æè¿°å’Œè¯„ä¼°ä¸åŒç¡¬ä»¶è®¾è®¡çš„ç‰¹ç‚¹ã€‚LLMCompass åŒ…æ‹¬ä¸€ä¸ªæ˜ å°„å™¨ï¼Œå¯ä»¥è‡ªåŠ¨æ‰¾åˆ°æ€§èƒ½ä¼˜åŒ–çš„æ˜ å°„å’Œè°ƒåº¦ã€‚å®ƒè¿˜åŒ…æ‹¬ä¸€ä¸ªé¢ç§¯åŸºäºçš„æˆæœ¬æ¨¡å‹ï¼Œå¸®åŠ©å»ºç­‘å¸ˆæ€è€ƒä»–ä»¬çš„è®¾è®¡é€‰æ‹©ã€‚ä¸å®é™…ç¡¬ä»¶ç›¸æ¯”ï¼ŒLLMCompass çš„ä¼°ç®—å»¶è¿Ÿ Error Rate ä¸º 10.4% ä»¥ä¸Šï¼Œå¹¶ä¸”å¯¹ä¸åŒçš„è¾“å…¥å¤§å°å’Œ LLM æ¨ç†è€Œè¨€å…·æœ‰å¹³å‡ 4.1% çš„è¯¯å·®ç‡ã€‚é€šè¿‡ä½¿ç”¨ LLMCompassï¼Œå¯ä»¥åœ¨å¸¸è§ç¡¬ä»¶ä¸Šæ¨¡æ‹Ÿä¸€ä¸ªè¿è¡Œ GPT-3 175B æ¨ç†çš„ 4 ä¸ª NVIDIA A100 GPU èŠ‚ç‚¹ï¼Œéœ€è¦ 16 åˆ†é’Ÿçš„æ—¶é—´ï¼ŒåŒ…æ‹¬ 26,400 æ¬¡æ˜ å°„å™¨çš„å‚æ•°æœç´¢ã€‚LLMCompass å¯ä»¥å¸®åŠ©æ¨æ–­å‡ºæ–°çš„æˆæœ¬æ•ˆæœçš„ç¡¬ä»¶è®¾è®¡ï¼Œä¾‹å¦‚é€šè¿‡å‡å°‘è®¡ç®—èƒ½åŠ›æˆ–è€…å°†é«˜é¢‘å¸¦å‚¨å­˜ï¼ˆHBMï¼‰æ›¿æ¢ä¸ºä¼ ç»Ÿçš„ DDRï¼Œå¯ä»¥å®ç°ä¸ NVIDIA A100 ç›¸å½“çš„æ€§èƒ½/æˆæœ¬æ¯”ï¼Œè¾¾åˆ° 3.41 å€çš„æå‡ã€‚LLMCompass è®¡åˆ’å°†æ˜¯å®Œå…¨å¼€æºçš„ã€‚
</details></li>
</ul>
<hr>
<h2 id="Advantage-of-Quantum-Machine-Learning-from-General-Computational-Advantages"><a href="#Advantage-of-Quantum-Machine-Learning-from-General-Computational-Advantages" class="headerlink" title="Advantage of Quantum Machine Learning from General Computational Advantages"></a>Advantage of Quantum Machine Learning from General Computational Advantages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03057">http://arxiv.org/abs/2312.03057</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayata Yamasaki, Natsuto Isogai, Mio Murao</li>
<li>for: è¿™ paper çš„ç›®çš„æ˜¯è¯æ˜é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰åœ¨supervised learning taskä¸­çš„ä¼˜åŠ¿ï¼Œå¹¶è¿›ä¸€æ­¥è¯æ˜ QML å¯ä»¥åœ¨æ›´å¹¿æ³›çš„å­¦ä¹ ä»»åŠ¡ä¸­å±•ç¤ºå…¶ä¼˜åŠ¿ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨äº†ä¸€ç§æ™®éçš„é‡å­ç®—æ³•ä¼˜åŠ¿æ¥æ„å»ºä¸€ä¸ªæ›´å¹¿æ³›çš„å­¦ä¹ ä»»åŠ¡ï¼Œå¹¶è¯æ˜è¿™ç§ä»»åŠ¡æ˜¯ä¸å¯èƒ½ç”±ä»»ä½•ç±»å‹çš„å¸¸è§ç®—æ³•è§£å†³ã€‚</li>
<li>results: è¿™ paper è¯æ˜äº† QML åœ¨è¿™ä¸ªæ›´å¹¿æ³›çš„å­¦ä¹ ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿ï¼Œå¹¶æä¾›äº†å‡†ç¡®çš„å‡†åˆ™æ¥è¯„ä¼° QML çš„ä¼˜åŠ¿ã€‚<details>
<summary>Abstract</summary>
An overarching milestone of quantum machine learning (QML) is to demonstrate the advantage of QML over all possible classical learning methods in accelerating a common type of learning task as represented by supervised learning with classical data. However, the provable advantages of QML in supervised learning have been known so far only for the learning tasks designed for using the advantage of specific quantum algorithms, i.e., Shor's algorithms. Here we explicitly construct an unprecedentedly broader family of supervised learning tasks with classical data to offer the provable advantage of QML based on general quantum computational advantages, progressing beyond Shor's algorithms. Our learning task is feasibly achievable by executing a general class of functions that can be computed efficiently in polynomial time for a large fraction of inputs by arbitrary quantum algorithms but not by any classical algorithm. We prove the hardness of achieving this learning task for any possible polynomial-time classical learning method. We also clarify protocols for preparing the classical data to demonstrate this learning task in experiments. These results open routes to exploit a variety of quantum advantages in computing functions for the experimental demonstration of the advantage of QML.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…¨é¢çš„é‡Œç¨‹ç¢‘ä¹‹ä¸€åœ¨é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰é¢†åŸŸæ˜¯è¯æ˜QMLåœ¨æ‰€æœ‰å¯èƒ½çš„ç±»ä¼ ç»Ÿå­¦ä¹ æ–¹æ³•ä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä»¥åŠ é€Ÿå¸¸è§çš„å­¦ä¹ ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæˆªè‡³ç›®å‰ï¼Œåªæœ‰ä½¿ç”¨ç‰¹å®šé‡å­ç®—æ³•çš„å­¦ä¹ ä»»åŠ¡ä¸­çš„ä¼˜åŠ¿è¢«è¯æ˜ä¸ºQMLçš„ä¼˜åŠ¿ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ˜ç¡®æ„é€ äº†ä¸€ä¸ªæ–°çš„ã€å‰æ‰€æœªæœ‰çš„è¶…çº§vised learningä»»åŠ¡ï¼Œä½¿å¾—QMLå…·æœ‰åŸºäºé€šç”¨é‡å­è®¡ç®—ä¼˜åŠ¿çš„è¯æ˜ä¼˜åŠ¿ã€‚æˆ‘ä»¬è¯æ˜ä»»åŠ¡æ˜¯å¯ä»¥ç”±ä»»æ„é‡å­ç®—æ³•efficientlyå¤„ç†çš„ï¼Œä½†æ˜¯ä¸å¯ä»¥ç”±ä»»ä½•ç±»ä¼ ç»Ÿå­¦ä¹ æ–¹æ³•å¤„ç†ã€‚æˆ‘ä»¬è¿˜é˜è¿°äº†å‡†å¤‡ç»å…¸æ•°æ®çš„åè®®ï¼Œä»¥ä¾¿åœ¨å®éªŒä¸­è¯æ˜è¿™ä¸ªå­¦ä¹ ä»»åŠ¡ã€‚è¿™äº›ç»“æœå¼€å¯äº†åœ¨è®¡ç®—å‡½æ•°æ–¹é¢åˆ©ç”¨é‡å­ä¼˜åŠ¿çš„è·¯å¾„ï¼Œå¹¶è¿›ä¸€æ­¥æ¨åŠ¨QMLçš„åº”ç”¨ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-High-Dimensional-Differential-Graphs-From-Multi-Attribute-Data"><a href="#Learning-High-Dimensional-Differential-Graphs-From-Multi-Attribute-Data" class="headerlink" title="Learning High-Dimensional Differential Graphs From Multi-Attribute Data"></a>Learning High-Dimensional Differential Graphs From Multi-Attribute Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03761">http://arxiv.org/abs/2312.03761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jitendra K Tugnait</li>
<li>for: è¿™ç¯‡è®ºæ–‡ç›®çš„æ˜¯ä¸ºäº†ä¼°è®¡ä¸¤ä¸ªæ³Šå°”å›¾æ¨¡å‹ï¼ˆGGMï¼‰ä¹‹é—´çš„å·®å¼‚ï¼Œè¿™ä¸¤ä¸ªGGMéƒ½çŸ¥é“å…·æœ‰ç›¸ä¼¼çš„ç»“æ„ã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†ä¸€ç§åŸºäºD-traceæŸå¤±å‡½æ•°çš„ Ğ³Ñ€ÑƒĞ¿lace penaltyæ–¹æ³•æ¥å­¦ä¹ å¤šå±æ€§æ•°æ®ä¸­çš„å·®å¼‚å›¾æ¨¡å‹ã€‚ä¸€ç§åŸºäºå¤šé‡æ–¹å‘ä¹˜æ³•æ³•ï¼ˆADMMï¼‰çš„ä¼˜åŒ–ç®—æ³•ä¹Ÿæ˜¯æå‡ºçš„ã€‚</li>
<li>results: è®ºæ–‡çš„ Ñ‚ĞµĞ¾ë¦¬Ñ‚Ğ¸Ñ‡Ğµåˆ†æè¡¨æ˜ï¼Œåœ¨é«˜ç»´è®¾ç½®ä¸‹ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿæ”¯æŒæ¢å¤å’Œä¼°è®¡å·®å¼‚å›¾æ¨¡å‹ã€‚åŒæ—¶ï¼Œæ•°æ®åˆ†æç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥åœ¨å®é™…æ•°æ®ä¸­å…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
We consider the problem of estimating differences in two Gaussian graphical models (GGMs) which are known to have similar structure. The GGM structure is encoded in its precision (inverse covariance) matrix. In many applications one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data. Existing methods for differential graph estimation are based on single-attribute (SA) models where one associates a scalar random variable with each node. In multi-attribute (MA) graphical models, each node represents a random vector. In this paper, we analyze a group lasso penalized D-trace loss function approach for differential graph learning from multi-attribute data. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. Theoretical analysis establishing consistency in support recovery and estimation in high-dimensional settings is provided. Numerical results based on synthetic as well as real data are presented.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬è€ƒè™‘äº†ä¸¤ä¸ª Gaussian graphical model (GGM) çš„å·®å¼‚ä¼°è®¡é—®é¢˜ï¼Œè¿™ä¸¤ä¸ª GGM çŸ¥é“å®ƒä»¬çš„ç»“æ„ç›¸ä¼¼ã€‚GGM çš„ç»“æ„æ˜¯å®ƒçš„ç²¾åº¦çŸ©é˜µ (é€†covariance matrix) æ‰€ç¼–ç çš„ã€‚åœ¨è®¸å¤šåº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ interesseted in ä¼°è®¡ä¸¤ä¸ª GGM çš„ç²¾åº¦çŸ©é˜µä¹‹é—´çš„å·®å¼‚ï¼Œä»¥æè¿°ä¸¤aset of data ä¹‹é—´çš„ä¸‹æ¸¸ç›¸ä¾æ€§å˜åŒ–ã€‚ç°æœ‰çš„æ–¹æ³•æ˜¯åŸºäºå•ä¸€å±æ€§ (SA) æ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ª node éƒ½ç›¸å…³ç€ä¸€ä¸ªæ•°å€¼éšæœºå˜é‡ã€‚åœ¨å¤šå±æ€§ (MA) å›¾å½¢æ¨¡å‹ä¸­ï¼Œæ¯ä¸ª node è¡¨ç¤ºä¸€ä¸ªéšæœº Ğ²ĞµĞºÑ‚Ğ¾Ñ€ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†ä¸€ä¸ª group lasso æŠ‘åˆ¶ D-trace æŸå¤±å‡½æ•°çš„æ–¹æ³•æ¥è¿›è¡Œå·®å¼‚å›¾å­¦å­¦ä¹ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ª alternating direction method of multipliers (ADMM) ç®—æ³•æ¥ä¼˜åŒ–ç›®æ ‡å‡½æ•°ã€‚æˆ‘ä»¬æä¾›äº† teorical åˆ†æï¼Œè¯æ˜äº†åœ¨é«˜ç»´è®¾å®šä¸‹æ”¯æŒå›æº¯å’Œä¼°è®¡çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬è¿˜æä¾›äº†åŸºäºçœŸå®æ•°æ®çš„numerical ç»“æœã€‚
</details></li>
</ul>
<hr>
<h2 id="Detecting-algorithmic-bias-in-medical-AI-models"><a href="#Detecting-algorithmic-bias-in-medical-AI-models" class="headerlink" title="Detecting algorithmic bias in medical AI-models"></a>Detecting algorithmic bias in medical AI-models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02959">http://arxiv.org/abs/2312.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨ç¡®ä¿æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åŸºäºåŒ»ç–—å†³ç­–æ”¯æŒç³»ç»Ÿæä¾›å…¬æ­£å’Œå…¬å¹³çš„ç»“æœã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•æ¥æ£€æµ‹åŒ»ç–—-AIå†³ç­–æ”¯æŒç³»ç»Ÿä¸­çš„ç®—æ³•åè§ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åˆ†ç±»å’Œå›å½’æ ‘ï¼ˆCARTï¼‰ç®—æ³•ï¼Œå¹¶é€šè¿‡ synthetic data å®éªŒå’Œå®é™…åŒ»ç–—è®°å½•æ•°æ®è¿›è¡ŒéªŒè¯ã€‚</li>
<li>results: è¯¥è®ºæ–‡çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥å‡†ç¡®åœ°æ£€æµ‹åŒ»ç–—-AI æ¨¡å‹ä¸­çš„åè§ï¼Œå¹¶åœ¨å®é™…ä¸´åºŠç¯å¢ƒä¸­æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„å…¬æ­£æ€§éªŒè¯å·¥å…·ã€‚<details>
<summary>Abstract</summary>
With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Attention-enhanced-neural-differential-equations-for-physics-informed-deep-learning-of-ion-transport"><a href="#Attention-enhanced-neural-differential-equations-for-physics-informed-deep-learning-of-ion-transport" class="headerlink" title="Attention-enhanced neural differential equations for physics-informed deep learning of ion transport"></a>Attention-enhanced neural differential equations for physics-informed deep learning of ion transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02871">http://arxiv.org/abs/2312.02871</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danyal Rehman, John H. Lienhard</li>
<li>for: æ¨¡å‹transportnanoporousç³»ç»Ÿä¸­çš„ç¦»å­è¿è¾“</li>
<li>methods: ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ³¨æ„åŠ›å¢å¼ºç¥ç» diferencial equationsï¼Œä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½</li>
<li>results: physics-informed deep learning solutionså¯ä»¥è¶…è¶Šä¼ ç»ŸPDE-basedæ–¹æ³•ï¼Œå¹¶æä¾›æ¨¡æ‹Ÿå¤æ‚è¿è¾“ç°è±¡çš„å¯èƒ½æ€§<details>
<summary>Abstract</summary>
Species transport models typically combine partial differential equations (PDEs) with relations from hindered transport theory to quantify electromigrative, convective, and diffusive transport through complex nanoporous systems; however, these formulations are frequently substantial simplifications of the governing dynamics, leading to the poor generalization performance of PDE-based models. Given the growing interest in deep learning methods for the physical sciences, we develop a machine learning-based approach to characterize ion transport across nanoporous membranes. Our proposed framework centers around attention-enhanced neural differential equations that incorporate electroneutrality-based inductive biases to improve generalization performance relative to conventional PDE-based methods. In addition, we study the role of the attention mechanism in illuminating physically-meaningful ion-pairing relationships across diverse mixture compositions. Further, we investigate the importance of pre-training on simulated data from PDE-based models, as well as the performance benefits from hard vs. soft inductive biases. Our results indicate that physics-informed deep learning solutions can outperform their classical PDE-based counterparts and provide promising avenues for modelling complex transport phenomena across diverse applications.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç§ç±»è¿è¾“æ¨¡å‹é€šå¸¸å°† partial differential equations (PDEs) ä¸é˜»ç¢è¿è¾“ç†è®ºçš„å…³ç³»ç»„åˆä»¥é‡åŒ–ç”µåŠ¨åŠ›ã€æ¶ŒåŠ¨å’Œæ‰©æ•£è¿è¾“è¿‡å¤æ‚çš„å¥ˆç±³å­”ç³»ç»Ÿ; however, these formulations are frequently substantial simplifications of the governing dynamics, leading to the poor generalization performance of PDE-based models. ç»™å‡ºäº†ç‰©ç†ç§‘å­¦ä¸­æ·±åº¦å­¦ä¹ æ–¹æ³•çš„å¢é•¿å…´è¶£ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§åŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•æ¥Characterize ion transport across nanoporous membranes. Our proposed framework centers around attention-enhanced neural differential equations that incorporate electroneutrality-based inductive biases to improve generalization performance relative to conventional PDE-based methods. In addition, we study the role of the attention mechanism in illuminating physically-meaningful ion-pairing relationships across diverse mixture compositions. Further, we investigate the importance of pre-training on simulated data from PDE-based models, as well as the performance benefits from hard vs. soft inductive biases. Our results indicate that physics-informed deep learning solutions can outperform their classical PDE-based counterparts and provide promising avenues for modelling complex transport phenomena across diverse applications.Note: Simplified Chinese is also known as "ç®€åŒ–å­—" or "ç®€ä½“å­—".
</details></li>
</ul>
<hr>
<h2 id="REST-Enhancing-Group-Robustness-in-DNNs-through-Reweighted-Sparse-Training"><a href="#REST-Enhancing-Group-Robustness-in-DNNs-through-Reweighted-Sparse-Training" class="headerlink" title="REST: Enhancing Group Robustness in DNNs through Reweighted Sparse Training"></a>REST: Enhancing Group Robustness in DNNs through Reweighted Sparse Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03044">http://arxiv.org/abs/2312.03044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhao1402072392/rest">https://github.com/zhao1402072392/rest</a></li>
<li>paper_authors: Jiaxu Zhao, Lu Yin, Shiwei Liu, Meng Fang, Mykola Pechenizkiy</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æé«˜æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨ä¸åŒæ•°æ®é›†ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨æ‰¹å¤„ç†å¤§æ•°æ®æ—¶ã€‚</li>
<li>methods: æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é‡æ–°æƒé‡çš„ç®€ sparse è®­ç»ƒæ¡†æ¶ï¼ˆRESTï¼‰ï¼Œé€šè¿‡å¢å¼ºè®­ç»ƒæ•°æ®ä¸­çš„åå¥½æ€§ï¼Œæé«˜æ¨¡å‹åœ¨åå¥½æ•°æ®ä¸Šçš„è¡¨ç°ã€‚</li>
<li>results: å®éªŒè¡¨æ˜ï¼ŒREST æ¡†æ¶å¯ä»¥æœ‰æ•ˆåœ°é™ä½æ¨¡å‹å¯¹åå¥½æ€§çš„ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„ä¸€è‡´æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚ code åœ¨ \url{<a target="_blank" rel="noopener" href="https://github.com/zhao1402072392/REST%7D">https://github.com/zhao1402072392/REST}</a> ä¸Šå‘å¸ƒã€‚<details>
<summary>Abstract</summary>
The deep neural network (DNN) has been proven effective in various domains. However, they often struggle to perform well on certain minority groups during inference, despite showing strong performance on the majority of data groups. This is because over-parameterized models learned \textit{bias attributes} from a large number of \textit{bias-aligned} training samples. These bias attributes are strongly spuriously correlated with the target variable, causing the models to be biased towards spurious correlations (i.e., \textit{bias-conflicting}). To tackle this issue, we propose a novel \textbf{re}weighted \textbf{s}parse \textbf{t}raining framework, dubbed as \textit{\textbf{REST}, which aims to enhance the performance of biased data while improving computation and memory efficiency. Our proposed REST framework has been experimentally validated on three datasets, demonstrating its effectiveness in exploring unbiased subnetworks. We found that REST reduces the reliance on spuriously correlated features, leading to better performance across a wider range of data groups with fewer training and inference resources. We highlight that the \textit{REST} framework represents a promising approach for improving the performance of DNNs on biased data, while simultaneously improving computation and memory efficiency. By reducing the reliance on spurious correlations, REST has the potential to enhance the robustness of DNNs and improve their generalization capabilities. Code is released at \url{https://github.com/zhao1402072392/REST}
</details>
<details>
<summary>æ‘˜è¦</summary>
æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰åœ¨ä¸åŒé¢†åŸŸéƒ½æœ‰è¯æ˜å…¶æ•ˆæœã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨æ¨ç†æ—¶ç»å¸¸å¯¹å°‘æ•°ç¾¤ä½“è¡¨ç°ä¸ä½³ï¼Œå³ä½¿åœ¨å¤§é‡æ•°æ®ç»„ä¸­è¡¨ç°å‡ºè‰²ã€‚è¿™æ˜¯å› ä¸ºDNNå­¦ä¹ äº†åè§ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ä¸ç›®æ ‡å˜é‡å¼ºçƒˆç›¸å…³ã€‚è¿™äº›åè§ç‰¹å¾æ¥è‡ªå¤§é‡åè§å¯¹é½çš„è®­ç»ƒæ ·æœ¬ã€‚è¿™å¯¼è‡´æ¨¡å‹åçˆ±è¿™äº›åè§ç‰¹å¾ï¼Œä»è€Œå¯¼è‡´æ¨¡å‹åçˆ±å‡ correlationsï¼ˆå³åè§å†²çªï¼‰ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§é‡æ–°æƒé‡çš„ç¨€ç–è®­ç»ƒæ¡†æ¶ï¼Œåä¸ºRESTï¼ˆé‡æ–°æƒé‡çš„ç¨€ç–è®­ç»ƒï¼‰ã€‚RESTæ¡†æ¶çš„ç›®æ ‡æ˜¯åœ¨ä¸è‰¯æ•°æ®ä¸Šæé«˜æ€§èƒ½ï¼ŒåŒæ—¶æé«˜è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†å®éªŒï¼Œå¹¶è¯æ˜RESTæ¡†æ¶çš„æ•ˆæœã€‚æˆ‘ä»¬å‘ç°ï¼ŒRESTå¯ä»¥å‡å°‘ä¾èµ–äºå‡ correlationsçš„ç‰¹å¾ï¼Œä»è€Œæé«˜æ€§èƒ½ across a wider range of data groupsï¼Œå¹¶é‡‡ç”¨ fewer training and inference resourcesã€‚æˆ‘ä»¬å¼ºè°ƒï¼ŒRESTæ¡†æ¶ä»£è¡¨äº†æ”¹è¿›DNNæ€§èƒ½çš„æœ‰åŠ›æ–¹æ³•ï¼ŒåŒæ—¶æé«˜è®¡ç®—å’Œå­˜å‚¨æ•ˆç‡ã€‚é€šè¿‡å‡å°‘ä¾èµ–äºå‡ correlationsï¼ŒRESTæœ‰å¯èƒ½æé«˜DNNçš„Robustnesså’Œæ³›åŒ–èƒ½åŠ›ã€‚ä»£ç å¯ä»¥åœ¨ \url{https://github.com/zhao1402072392/REST} ä¸­ä¸‹è½½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Semi-Supervised-Health-Index-Monitoring-with-Feature-Generation-and-Fusion"><a href="#Semi-Supervised-Health-Index-Monitoring-with-Feature-Generation-and-Fusion" class="headerlink" title="Semi-Supervised Health Index Monitoring with Feature Generation and Fusion"></a>Semi-Supervised Health Index Monitoring with Feature Generation and Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02867">http://arxiv.org/abs/2312.02867</a></li>
<li>repo_url: None</li>
<li>paper_authors: GaÃ«tan Frusque, Ismail Nejjar, Majid Nabavi, Olga Fink</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§å¯é ä¸”cost-effectiveçš„å¥åº·æŒ‡æ ‡ï¼ˆHealth Indexï¼ŒHIï¼‰ä¼°ç®—æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹ç³»ç»Ÿå¼‚å¸¸å’Œé¢„æµ‹ç³»ç»Ÿå‰©ä½™æœ‰ç”¨å¯¿å‘½ï¼Œç‰¹åˆ«é€‚ç”¨äºéœ€è¦é«˜å®‰å…¨å¯é æ€§çš„ç³»ç»Ÿã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨æ·±åº¦åŠç›‘ç£å¼‚å¸¸æ£€æµ‹ï¼ˆDeepSADï¼‰æ–¹æ³•æ„å»ºHIï¼Œå¹¶ä½¿ç”¨DeepSADåµŒå…¥å™¨ä½œä¸ºçŠ¶å†µæŒ‡æ ‡ä»¥è§£å†³å¯ interpretability å’Œç³»ç»Ÿç‰¹æœ‰å› ç´ çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å¼•å…¥å¤šæ ·æ€§æŸå¤±ä»¥å¢åŠ çŠ¶å†µæŒ‡æ ‡çš„å¤šæ ·æ€§ã€‚</li>
<li>results: åœ¨PHME 2010 ç£¨å‰Šæ•°æ®é›†ä¸ŠéªŒè¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æä¾›æœ‰æ„ä¹‰çš„HIä¼°ç®—ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åº”ç”¨äº†è¿™ç§æ–¹æ³•ç›‘æµ‹çƒ­æ¶‚æ•·å‰‚çš„æ¸©åº¦å˜åŒ–ï¼Œä»¥è·å¾—æ›´åŠ å¯é å’Œå¯è®¿é—®çš„HIä¼°ç®—ç»“æœã€‚<details>
<summary>Abstract</summary>
The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability. Tight monitoring is crucial for achieving high precision at a lower cost, with applications such as spray coating. Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements. Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction. In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors. Then, we introduce a diversity loss to enrich condition indicators. We employ an alternating projection algorithm with isotonic constraints to transform the DeepSAD embedding into a normalized HI with an increasing trend. Validation on the PHME 2010 milling dataset, a recognized benchmark with ground truth HIs demonstrates meaningful HIs estimations. Our methodology is then applied to monitor wear states of thermal spray coatings using high-frequency voltage. Our contributions create opportunities for more accessible and reliable HI estimation, particularly in cases where obtaining ground truth HI labels is unfeasible.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¥åº·æŒ‡æ•°ï¼ˆHIï¼‰æ˜¯è¯„ä¼°ç³»ç»Ÿå¥åº·çš„å…³é”®æŒ‡æ ‡ï¼Œæœ‰åŠ©äºå¼‚å¸¸æ£€æµ‹å’Œé¢„æµ‹ç³»ç»Ÿå‰©ä½™æœ‰ç”¨ç”Ÿå‘½æœŸã€‚ä¸¥æ ¼ç›‘æµ‹æ˜¯å®ç°é«˜ç²¾åº¦çš„å…³é”®ï¼Œå…¶åº”ç”¨åŒ…æ‹¬æ¶‚æŠ¹æŠ€æœ¯ã€‚åœ¨å®é™…åº”ç”¨ä¸­è·å¾—HIæ ‡ç­¾æ˜¯ç»æµä¸å¯èƒ½çš„ï¼Œéœ€è¦è¿ç»­ã€ç²¾åº¦é«˜çš„å¥åº·æµ‹é‡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©åˆ©ç”¨è¿è¡Œè‡³æ•…éšœæ•°æ®æ¥å»ºç«‹HIã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é‡‡ç”¨æ·±åº¦åŠsupervisedæ£€æµ‹æ–¹æ³•ï¼ˆDeepSADï¼‰æ¥å»ºç«‹HIã€‚æˆ‘ä»¬ä½¿ç”¨DeepSADåµŒå…¥ä½œä¸ºæœºå™¨ç£¨æŸçŠ¶å†µæŒ‡æ ‡ï¼Œä»¥è§£å†³å¯ interpretability å’Œç³»ç»Ÿç‰¹å®šå› ç´ çš„æ•æ„Ÿæ€§é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å¼•å…¥å¤šæ ·æ€§æŸå¤±ï¼Œä»¥ä¾¿å¢åŠ conditionæŒ‡æ ‡çš„å¤šæ ·æ€§ã€‚æˆ‘ä»¬é‡‡ç”¨äº¤å‰ Ğ¿Ñ€Ğ¾ĞµĞºctionç®—æ³•å’Œisoé€»è¾‘çº¦æŸæ¥è½¬æ¢DeepSADåµŒå…¥ï¼Œä»¥è·å¾—æ­£è´Ÿæ’åºçš„HIï¼ŒHIçš„å¢é•¿è¶‹åŠ¿ã€‚éªŒè¯PHME 2010 æ¯¯å‰‚æ•°æ®é›†ï¼Œä¸€ä¸ªå…¬è®¤çš„benchmarkï¼Œæˆ‘ä»¬å¾—åˆ°äº†æœ‰æ„ä¹‰çš„HIä¼°è®¡ã€‚æˆ‘ä»¬çš„æ–¹æ³•åæ¥åº”ç”¨äºç›‘æµ‹çƒ­æ¶‚æŠ¹å±‚çš„ç£¨æŸçŠ¶å†µï¼Œæˆ‘ä»¬çš„è´¡çŒ®ä¼šåˆ›é€ æ›´åŠ å¯ accessible å’Œå¯é çš„HIä¼°è®¡æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨è·å¾—çœŸå®HIæ ‡ç­¾æ˜¯ä¸å¯èƒ½çš„æƒ…å†µä¸‹ã€‚
</details></li>
</ul>
<hr>
<h2 id="Lessons-from-Usable-ML-Deployments-and-Application-to-Wind-Turbine-Monitoring"><a href="#Lessons-from-Usable-ML-Deployments-and-Application-to-Wind-Turbine-Monitoring" class="headerlink" title="Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring"></a>Lessons from Usable ML Deployments and Application to Wind Turbine Monitoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02859">http://arxiv.org/abs/2312.02859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Zytek, Wei-En Wang, Sofia Koukoura, Kalyan Veeramachaneni</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºå¯ç”¨æœºå™¨å­¦ä¹ ï¼ˆusable MLï¼‰çš„åº”ç”¨äºç°å®ä¸–ç•Œé¢†åŸŸçš„ç»éªŒåˆ†äº«ã€‚</li>
<li>methods: è®ºæ–‡ä¸­ä½¿ç”¨äº† bridges çš„æ¦‚å¿µï¼Œå³å°†æœºå™¨å­¦ä¹ å¼€å‘äººå‘˜å’Œé¢†åŸŸä¸“å®¶ç›¸è¿æ¥çš„äººå‘˜ï¼Œä»¥å¼€å‘å¯ç”¨ ML åº”ç”¨ç¨‹åºã€‚è®ºæ–‡è¿˜æå‡ºäº†ä¸€ç§å¯é…ç½®çš„ç³»ç»Ÿï¼Œç”¨äºåœ¨ä¸ bridges çš„åˆä½œä¸­è½»æ¾åœ°è¿›è¡Œå¯ç”¨ ML ç•Œé¢çš„è¿­ä»£ã€‚</li>
<li>results: è®ºæ–‡é€šè¿‡åº”ç”¨è¿™äº›ç»éªŒåˆ°é£åŠ›æœºç›‘æµ‹ä»»åŠ¡ä¸­ï¼Œå±•ç¤ºäº†å¯ç”¨ ML åœ¨å¯å†ç”Ÿèƒ½æºé¢†åŸŸä¸­çš„å®é™…å½±å“ã€‚åœ¨é£åŠ›æœºç›‘æµ‹ä¸­ï¼Œæœºå™¨å­¦ä¹ å¼€å‘äººå‘˜å’Œæ•°æ®åˆ†æå‘˜éœ€è¦å†³å®šæ˜¯å¦è¿›è¡Œ expensive in-person è°ƒæŸ¥ï¼Œä»¥é¿å… potential çš„ç¼¸ç›˜å¤±æ•ˆã€‚è®ºæ–‡ç¤ºå‡ºäº†å¦‚ä½•ä½¿ç”¨å¯ç”¨ ML ç•Œé¢æ¥å¸®åŠ©å†³ç­–è¿‡ç¨‹ã€‚<details>
<summary>Abstract</summary>
Through past experiences deploying what we call usable ML (one step beyond explainable ML, including both explanations and other augmenting information) to real-world domains, we have learned three key lessons. First, many organizations are beginning to hire people who we call ``bridges'' because they bridge the gap between ML developers and domain experts, and these people fill a valuable role in developing usable ML applications. Second, a configurable system that enables easily iterating on usable ML interfaces during collaborations with bridges is key. Finally, there is a need for continuous, in-deployment evaluations to quantify the real-world impact of usable ML. Throughout this paper, we apply these lessons to the task of wind turbine monitoring, an essential task in the renewable energy domain. Turbine engineers and data analysts must decide whether to perform costly in-person investigations on turbines to prevent potential cases of brakepad failure, and well-tuned usable ML interfaces can aid with this decision-making process. Through the applications of our lessons to this task, we hope to demonstrate the potential real-world impact of usable ML in the renewable energy domain.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨è¿‡å»çš„å®è·µä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸‰ä¸ªå…³é”®çš„æ•™è®­ï¼Œè¿™äº›æ•™è®­åœ¨å®ç°å¯ç”¨æœºå™¨å­¦ä¹ ï¼ˆä¸€æ­¥è¶…è¿‡å¯è§£é‡Šæœºå™¨å­¦ä¹ ï¼ŒåŒ…æ‹¬è§£é‡Šå’Œå…¶ä»–å¢å¼ºä¿¡æ¯ï¼‰åº”ç”¨ä¸­éå¸¸é‡è¦ã€‚é¦–å…ˆï¼Œè®¸å¤šç»„ç»‡å¼€å§‹æ‹›è˜æˆ‘ä»¬ç§°ä¸ºâ€œæ¡¥æ¢â€çš„äººï¼Œè¿™äº›äººå°†æœºå™¨å­¦ä¹ å¼€å‘è€…å’Œé¢†åŸŸä¸“å®¶ä¹‹é—´çš„éš”é˜‚bridgeï¼Œä»–ä»¬åœ¨å¼€å‘å¯ç”¨æœºå™¨å­¦ä¹ åº”ç”¨ä¸­æ‰®æ¼”äº†éå¸¸é‡è¦çš„è§’è‰²ã€‚ç¬¬äºŒï¼Œä¸€ä¸ªå¯é…ç½®çš„ç³»ç»Ÿï¼Œå¯ä»¥è½»æ¾åœ°åœ¨ä¸æ¡¥æ¢åˆä½œæ—¶è¿›è¡Œå¯ç”¨æœºå™¨å­¦ä¹ ç•Œé¢çš„è¿­ä»£ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚æœ€åï¼Œåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­è¿›è¡Œè¿ç»­è¯„ä¼°ï¼Œä»¥è¯„ä¼°å¯ç”¨æœºå™¨å­¦ä¹ åœ¨å®é™…ä¸–ç•Œä¸­çš„å½±å“ï¼Œæ˜¯éå¸¸é‡è¦çš„ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿™äº›æ•™è®­åº”ç”¨äºé£åŠ›æœºç›‘æµ‹ä»»åŠ¡ï¼Œè¿™æ˜¯å¯å†ç”Ÿèƒ½æºé¢†åŸŸçš„å…³é”®ä»»åŠ¡ã€‚é£æœºå·¥ç¨‹å¸ˆå’Œæ•°æ®åˆ†æå¸ˆå¿…é¡»å†³å®šæ˜¯å¦è¿›è¡Œcostlyçš„é¢å¯¹é¢è°ƒæŸ¥ï¼Œä»¥é¿å…æ½œåœ¨çš„åˆ¶åŠ¨ç›˜å¤±æ•ˆæƒ…å†µï¼Œè€Œä¸”è‰¯å¥½çš„å¯ç”¨æœºå™¨å­¦ä¹ ç•Œé¢å¯ä»¥å¸®åŠ©å†³ç­–è¿‡ç¨‹ä¸­ã€‚é€šè¿‡å¯¹è¿™ä¸ªä»»åŠ¡çš„åº”ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå±•ç¤ºå¯ç”¨æœºå™¨å­¦ä¹ åœ¨å¯å†ç”Ÿèƒ½æºé¢†åŸŸçš„å®é™…å½±å“ã€‚
</details></li>
</ul>
<hr>
<h2 id="Expert-guided-Bayesian-Optimisation-for-Human-in-the-loop-Experimental-Design-of-Known-Systems"><a href="#Expert-guided-Bayesian-Optimisation-for-Human-in-the-loop-Experimental-Design-of-Known-Systems" class="headerlink" title="Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems"></a>Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02852">http://arxiv.org/abs/2312.02852</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/trsav/hitl-bo">https://github.com/trsav/hitl-bo</a></li>
<li>paper_authors: Tom Savage, Ehecatl Antonio del Rio Chanona</li>
<li>for: è¯¥è®ºæ–‡çš„ç›®çš„æ˜¯ä½¿ç”¨é«˜é€šé‡æŠ½è±¡ Bayesian ä¼˜åŒ–å’Œäººç±»å†³ç­–ç†è®ºæ¥è®©é¢†åŸŸä¸“å®¶å½±å“ä¼˜åŒ–å®éªŒçš„é€‰æ‹©ã€‚</li>
<li>methods: è¯¥æ–¹æ³•åˆ©ç”¨äººç±»åœ¨ discrete å†³ç­–æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¹¶åœ¨åˆæœŸå†³ç­–ä¸­è®©ä¸“å®¶äº§ç”Ÿå½±å“ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬è§£å†³ä¸€ä¸ªå¢å¼ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œä»¥æœ€å¤§åŒ–sum çš„ utility function å€¼å’Œ covariance çŸ©é˜µçš„ determinantï¼Œç­‰äº total å˜åŒ–ã€‚åœ¨ Pareto å‰æŠ˜çº¿çš„å‰ç‚¹å¤„é€‰æ‹©è§£å†³æ–¹æ¡ˆï¼Œå¹¶è¿”å›ä¸€ç»„å…·æœ‰é«˜ utility å€¼å’Œåˆç†å·®å¼‚çš„ alternate è§£å†³æ–¹æ¡ˆï¼Œç”±ä¸“å®¶é€‰æ‹©ä¸€ä¸ªè¿›è¡Œè¯„ä¼°ã€‚</li>
<li>results: æˆ‘ä»¬è¡¨æ˜ï¼Œå³ä½¿ä¸“å®¶æ— çŸ¥ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä»å¯å›å½’æ ‡å‡† Bayesian ä¼˜åŒ–çš„ regretã€‚<details>
<summary>Abstract</summary>
Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation.
</details>
<details>
<summary>æ‘˜è¦</summary>
åŸŸåä¸“å®¶ç»å¸¸æ‹¥æœ‰å¯è´µçš„ç‰©ç†æ´å¯Ÿï¼Œè¿™äº›æ´å¯Ÿåœ¨å®Œå…¨è‡ªåŠ¨åŒ–çš„å†³ç­–è¿‡ç¨‹ä¸­è¢«å¿½ç•¥ï¼Œå¦‚æé«˜ç²¾åº¦ä¼˜åŒ–ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é«˜é€Ÿæ‰¹é‡çš„æé«˜ç²¾åº¦ä¼˜åŒ–ä¸äººç±»å†³ç­–ç†è®ºç›¸ç»“åˆï¼Œä»¥ä¾¿åŸŸåä¸“å®¶å¯ä»¥å½±å“é€‰æ‹©ä¼˜åŒ–å®éªŒçš„å†³ç­–ã€‚æˆ‘ä»¬çš„æ–¹æ³•å‡è®¾äººç±»åœ¨ä½œå‡ºç¦»æ•£é€‰æ‹©æ—¶æ¯”åœ¨è¿ç»­é€‰æ‹©æ—¶æ›´å¥½ï¼Œå¹¶è®©ä¸“å®¶åœ¨æ—©æœŸå†³ç­–ä¸­å‘æŒ¥å½±å“ã€‚åœ¨æ¯ä¸ªè¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è§£å†³ä¸€ä¸ªå¢å¼ºå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œå…¶ä¸­æ¯ä¸ªè§£å†³æ–¹æ¡ˆçš„æ€»ç”¨å€¼å’Œå†³å®šçŸ©é˜µçš„ determinant éƒ½è¾¾åˆ°æœ€å¤§åŒ–ã€‚é€šè¿‡åœ¨ Pareto å‰å‡¹ç‚¹å¤„é€‰æ‹©è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æ¯æ¬¡è¿”å›ä¸€ç»„å…·æœ‰é«˜ç”¨å€¼å’Œåˆç†åˆ†åŒ–çš„ alternate solutionï¼Œç”±ä¸“å®¶é€‰æ‹©è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬ç¤ºä¾‹æ˜¾ç¤ºï¼Œå³ä½¿åŸŸåä¸“å®¶æ— çŸ¥ï¼Œæˆ‘ä»¬çš„ç®—æ³•ä»å¯æ¢å¤æ ‡å‡†æé«˜ç²¾åº¦ä¼˜åŒ–çš„ regretã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Kernel-Based-Neural-Network-Test-for-High-dimensional-Sequencing-Data-Analysis"><a href="#A-Kernel-Based-Neural-Network-Test-for-High-dimensional-Sequencing-Data-Analysis" class="headerlink" title="A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis"></a>A Kernel-Based Neural Network Test for High-dimensional Sequencing Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02850">http://arxiv.org/abs/2312.02850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingting Hou, Chang Jiang, Qing Lu</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯ä¸ºäº†æ¢è®¨é«˜ç»´æ•°æ®åˆ†æä¸­ä½¿ç”¨äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œæŠ€æœ¯ï¼Œä»¥åŠå¦‚ä½•åœ¨é«˜ç»´æ•°æ®åˆ†æä¸­ä½¿ç”¨è¿™äº›æŠ€æœ¯ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº† kernel-based neural network (KNN) æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•ä½¿ç”¨äº†éšæœºæ•ˆåº”æ¥æ¨¡å‹é«˜ç»´é—ä¼ æ•°æ®çš„æ€»æ•ˆåº”ï¼Œå¹¶ä½¿ç”¨ kernel-based ç¥ç»ç½‘ç»œç»“æ„æ¥æ¨¡å‹å¤æ‚çš„é—ä¼ å‹ç—…ç†å…³ç³»ã€‚</li>
<li>results: é€šè¿‡ simulate çš„ç»“æœè¡¨æ˜ï¼Œè¿™ç§æ–¹æ³•åœ¨é¢å¯¹éçº¿æ€§å’Œäº¤äº’æ•ˆåº”æ—¶æœ‰æ›´é«˜çš„åŠ›åº¦ï¼Œæ¯”å¦‚ SKAT æ–¹æ³•ã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè®ºæ–‡è¿˜åº”ç”¨äº†è¿™ç§æ–¹æ³•åˆ°äº† Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) ç ”ç©¶ä¸­çš„æ•´ä¸ªåŸºå› ç»„åºåˆ—æ•°æ®ï¼Œå‘ç°äº†ä¸€äº›æ–°çš„ä¸è„‘å¹²ä½“ç§¯å˜åŒ–ç›¸å…³çš„é—ä¼ å˜åŒ–ã€‚<details>
<summary>Abstract</summary>
The recent development of artificial intelligence (AI) technology, especially the advance of deep neural network (DNN) technology, has revolutionized many fields. While DNN plays a central role in modern AI technology, it has been rarely used in sequencing data analysis due to challenges brought by high-dimensional sequencing data (e.g., overfitting). Moreover, due to the complexity of neural networks and their unknown limiting distributions, building association tests on neural networks for genetic association analysis remains a great challenge. To address these challenges and fill the important gap of using AI in high-dimensional sequencing data analysis, we introduce a new kernel-based neural network (KNN) test for complex association analysis of sequencing data. The test is built on our previously developed KNN framework, which uses random effects to model the overall effects of high-dimensional genetic data and adopts kernel-based neural network structures to model complex genotype-phenotype relationships. Based on KNN, a Wald-type test is then introduced to evaluate the joint association of high-dimensional genetic data with a disease phenotype of interest, considering non-linear and non-additive effects (e.g., interaction effects). Through simulations, we demonstrated that our proposed method attained higher power compared to the sequence kernel association test (SKAT), especially in the presence of non-linear and interaction effects. Finally, we apply the methods to the whole genome sequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, investigating new genes associated with the hippocampal volume change over time.
</details>
<details>
<summary>æ‘˜è¦</summary>
Recent advances in artificial intelligence (AI) technology, particularly in deep neural network (DNN) technology, have revolutionized many fields. However, DNN has been rarely used in sequencing data analysis due to the challenges posed by high-dimensional sequencing data, such as overfitting. Moreover, the complexity of neural networks and their unknown limiting distributions make it difficult to build association tests on neural networks for genetic association analysis. To address these challenges and fill the important gap of using AI in high-dimensional sequencing data analysis, we propose a new kernel-based neural network (KNN) test for complex association analysis of sequencing data. Our test is built on our previously developed KNN framework, which uses random effects to model the overall effects of high-dimensional genetic data and adopts kernel-based neural network structures to model complex genotype-phenotype relationships. Based on KNN, we introduce a Wald-type test to evaluate the joint association of high-dimensional genetic data with a disease phenotype of interest, considering non-linear and non-additive effects (e.g., interaction effects). Through simulations, we demonstrated that our proposed method attained higher power compared to the sequence kernel association test (SKAT), especially in the presence of non-linear and interaction effects. Finally, we apply the methods to the whole genome sequencing (WGS) dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study, investigating new genes associated with the hippocampal volume change over time.
</details></li>
</ul>
<hr>
<h2 id="Algorithms-for-mean-field-variational-inference-via-polyhedral-optimization-in-the-Wasserstein-space"><a href="#Algorithms-for-mean-field-variational-inference-via-polyhedral-optimization-in-the-Wasserstein-space" class="headerlink" title="Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space"></a>Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02849">http://arxiv.org/abs/2312.02849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Jiang, Sinho Chewi, Aram-Alexandre Pooladian</li>
<li>For: The paper is written for optimizing functionals over finite-dimensional polyhedral subsets of the Wasserstein space, with a main application in mean-field variational inference.* Methods: The paper uses first-order methods for optimization over these polyhedral subsets, and provides approximation rates and an algorithm for minimizing the KL divergence over these sets.* Results: The paper obtains accelerated convergence with a complexity of $O(\sqrt \kappa \log(\kappa d&#x2F;\varepsilon^2))$, where $\kappa$ is the condition number of the distribution being optimized.Hereâ€™s the Chinese translation of the three pieces of information:* For: æœ¬æ–‡æ˜¯ä¸ºäº†ä¼˜åŒ–å‡½æ•°als over finite-dimensional polyhedral subsets of Wasserstein space çš„é—®é¢˜ï¼Œä¸»è¦åº”ç”¨åœ¨mean-field variational inferenceä¸­ã€‚* Methods: æœ¬æ–‡ä½¿ç”¨first-order methods for optimization overè¿™äº›polyhedral subsetsï¼Œå¹¶æä¾›äº†ä¸€ä¸ªapproximation rateså’Œä¸€ä¸ªç®—æ³•æ¥æœ€å°åŒ–KL divergence overè¿™äº›setsã€‚* Results: æœ¬æ–‡è·å¾—äº†$O(\sqrt \kappa \log(\kappa d&#x2F;\varepsilon^2)))$çš„åŠ é€Ÿ convergenceï¼Œwhere $\kappa$æ˜¯è¢«ä¼˜åŒ–åˆ†å¸ƒçš„condition numberã€‚<details>
<summary>Abstract</summary>
We develop a theory of finite-dimensional polyhedral subsets over the Wasserstein space and optimization of functionals over them via first-order methods. Our main application is to the problem of mean-field variational inference, which seeks to approximate a distribution $\pi$ over $\mathbb{R}^d$ by a product measure $\pi^\star$. When $\pi$ is strongly log-concave and log-smooth, we provide (1) approximation rates certifying that $\pi^\star$ is close to the minimizer $\pi^\star_\diamond$ of the KL divergence over a \emph{polyhedral} set $\mathcal{P}_\diamond$, and (2) an algorithm for minimizing $\text{KL}(\cdot\|\pi)$ over $\mathcal{P}_\diamond$ with accelerated complexity $O(\sqrt \kappa \log(\kappa d/\varepsilon^2))$, where $\kappa$ is the condition number of $\pi$.
</details>
<details>
<summary>æ‘˜è¦</summary>
æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæœ‰é™ç»´å¤šé¢ä½“å­é›†çš„ç†è®ºï¼Œè¯¥å­é›†ä½äº Wasserstein ç©ºé—´ä¸Šï¼Œå¹¶é€šè¿‡é¦–æ¬¡æ–¹æ³•ä¼˜åŒ–å‡½æ•°ionalã€‚æˆ‘ä»¬çš„ä¸»è¦åº”ç”¨æ˜¯åœ¨mean-fieldvariationalæ¨ç†ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†åˆ†å¸ƒ $\pi$  approximated by a product measure $\pi^\star$ çš„é—®é¢˜ã€‚å½“ $\pi$ æ˜¯å¼ºå¼log-concaveå’Œlog-smoothæ—¶ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹ä¸¤ä¸ªresultï¼š1. ç¡®è®¤ $\pi^\star$ æ˜¯ $\mathcal{P}_\diamond$ ä¸­çš„æœ€ä½³è§£ï¼Œå…¶ä¸­ $\mathcal{P}_\diamond$ æ˜¯ä¸€ä¸ªå¤šé¢ä½“å­é›†ï¼Œå¹¶ä¸”æä¾›äº†ä¸€ä¸ª $O(\sqrt \kappa \log(\kappa d/\varepsilon^2)))$ çš„ accelerated complexity çš„ç®—æ³•ï¼Œå…¶ä¸­ $\kappa$ æ˜¯ $\pi$ çš„condition numberã€‚2. å¯¹ $\mathcal{P}_\diamond$ ä¸­çš„å‡½æ•°ional $\text{KL}(\cdot\|\pi)$ è¿›è¡Œæœ€å°åŒ–ï¼Œå¹¶æä¾›äº†ä¸€ä¸ª $O(\sqrt \kappa \log(\kappa d/\varepsilon^2)))$ çš„ accelerated complexity çš„ç®—æ³•ã€‚Here's the translation in Traditional Chinese:æˆ‘ä»¬å°†å¼€å‘ä¸€ä¸ªæœ‰é™ç»´å¤šé¢ä½“å­é›†çš„ç†è®ºï¼Œè¯¥å­é›†ä½äº Wasserstein ç©ºé—´ä¸Šï¼Œå¹¶é€šè¿‡é¦–æ¬¡æ–¹æ³•ä¼˜åŒ–å‡½æ•°ionalã€‚æˆ‘ä»¬çš„ä¸»è¦åº”ç”¨æ˜¯åœ¨mean-fieldvariationalæ¨ç†ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå°†åˆ†å¸ƒ $\pi$  approximated by a product measure $\pi^\star$ çš„é—®é¢˜ã€‚å½“ $\pi$ æ˜¯å¼ºå¼log-concaveå’Œlog-smoothæ—¶ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹ä¸¤ä¸ªç»“æœï¼š1. ç¡®è®¤ $\pi^\star$ æ˜¯ $\mathcal{P}_\diamond$ ä¸­çš„æœ€ä½³è§£ï¼Œå…¶ä¸­ $\mathcal{P}_\diamond$ æ˜¯ä¸€ä¸ªå¤šé¢ä½“å­é›†ï¼Œå¹¶ä¸”æä¾›äº†ä¸€ä¸ª $O(\sqrt \kappa \log(\kappa d/\varepsilon^2)))$ çš„ accelerated complexity çš„ç®—æ³•ï¼Œå…¶ä¸­ $\kappa$ æ˜¯ $\pi$ çš„condition numberã€‚2. å¯¹ $\mathcal{P}_\diamond$ ä¸­çš„å‡½æ•°ional $\text{KL}(\cdot\|\pi)$ è¿›è¡Œæœ€å°åŒ–ï¼Œå¹¶æä¾›äº†ä¸€ä¸ª $O(\sqrt \kappa \log(\kappa d/\varepsilon^2)))$ çš„ accelerated complexity çš„ç®—æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="Transformer-Based-Deep-Learning-Model-for-Bored-Pile-Load-Deformation-Prediction-in-Bangkok-Subsoil"><a href="#Transformer-Based-Deep-Learning-Model-for-Bored-Pile-Load-Deformation-Prediction-in-Bangkok-Subsoil" class="headerlink" title="Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil"></a>Transformer-Based Deep Learning Model for Bored Pile Load-Deformation Prediction in Bangkok Subsoil</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03041">http://arxiv.org/abs/2312.03041</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sompote Youwai, Chissanupong Thongnoo</li>
<li>for: é¢„æµ‹å¤§é’»å­”æ†åœ¨æ›¼å°¼ç´¢ã‚¤ãƒ«åº•å±‚ä¸­çš„è·è½½-å‡å‹è¡Œä¸º</li>
<li>methods: ä½¿ç”¨å˜æ¢å™¨æ¶æ„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç¼–ç åœŸå£¤Profileå’Œé’»å­”ç‰¹å¾ä½œä¸ºtokenè¾“å…¥ï¼Œç”Ÿæˆè·è½½-å‡å‹æ›²çº¿è¾“å‡ºï¼Œå¹¶ incorporateä¸Šä¸€ä¸ªé¡ºåºæ•°æ®æ¥æé«˜é¢„æµ‹ç²¾åº¦</li>
<li>results: æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šæ˜¾ç¤ºäº†æ»¡æ„çš„å‡†ç¡®ç‡å’Œæ³›åŒ–èƒ½åŠ›ï¼Œè¯¯å·®ä¸º5.72%ï¼Œå¯ç”¨äº Parametric analysiså’Œè®¾è®¡ä¼˜åŒ–é’»å­”åœ¨ä¸åŒçš„åœŸå£¤å’Œé’»å­”æ¡ä»¶ä¸‹ã€‚<details>
<summary>Abstract</summary>
This paper presents a novel deep learning model based on the transformer architecture to predict the load-deformation behavior of large bored piles in Bangkok subsoil. The model encodes the soil profile and pile features as tokenization input, and generates the load-deformation curve as output. The model also incorporates the previous sequential data of load-deformation curve into the decoder to improve the prediction accuracy. The model also incorporates the previous sequential data of load-deformation curve into the decoder. The model shows a satisfactory accuracy and generalization ability for the load-deformation curve prediction, with a mean absolute error of 5.72% for the test data. The model could also be used for parametric analysis and design optimization of piles under different soil and pile conditions, pile cross section, pile length and type of pile.
</details>
<details>
<summary>æ‘˜è¦</summary></li>
</ul>
</details>


<hr>
<h2 id="Convergence-Rates-for-Stochastic-Approximation-Biased-Noise-with-Unbounded-Variance-and-Applications"><a href="#Convergence-Rates-for-Stochastic-Approximation-Biased-Noise-with-Unbounded-Variance-and-Applications" class="headerlink" title="Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications"></a>Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02828">http://arxiv.org/abs/2312.02828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajeeva L. Karandikar, M. Vidyasagar</li>
<li>for: æœ¬æ–‡æ¢è®¨äº†Stochastic Approximationï¼ˆSAï¼‰ç®—æ³•åœ¨å„ç§åº”ç”¨ä¸­çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬é convex ä¼˜åŒ–å’Œ reinforcement learningï¼ˆRLï¼‰ã€‚</li>
<li>methods: æœ¬æ–‡æ‰©å±•äº†SAç†è®ºï¼ŒåŒ…æ‹¬äº†éšæœº error çš„éé›¶ conditional mean å’Œ unbounded conditional varianceï¼Œä»¥åŠå¼‚æ­¥ SAã€‚</li>
<li>results: æœ¬æ–‡ Compute the â€œoptimal step size sequencesâ€ to maximize the estimated rate of convergence of the algorithm, and prove that SA converges in nonconvex optimization and Markovian SA situations.<details>
<summary>Abstract</summary>
The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}) = \nabla J({\boldsymbol {\theta})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. In much of the literature, it is assumed that the error term ${\boldsymbol {xi}_{t+1}$ has zero conditional mean, and that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}_t$). Also, for the most part, the emphasis has been on ``synchronous'' SA, whereby, at each time $t$, \textit{every} component of ${\boldsymbol {\theta}_t$ is updated. Over the years, SA has been applied to a variety of areas, out of which two are the focus in this paper: Convex and nonconvex optimization, and Reinforcement Learning (RL). As it turns out, in these applications, the above-mentioned assumptions do not always hold. In zero-order methods, the error neither has zero mean nor bounded conditional variance. In the present paper, we extend SA theory to encompass errors with nonzero conditional mean and/or unbounded conditional variance, and also asynchronous SA. In addition, we derive estimates for the rate of convergence of the algorithm. Then we apply the new results to problems in nonconvex optimization, and to Markovian SA, a recently emerging area in RL. We prove that SA converges in these situations, and compute the ``optimal step size sequences'' to maximize the estimated rate of convergence.
</details>
<details>
<summary>æ‘˜è¦</summary>
estone Approximationï¼ˆSAï¼‰ç®—æ³•ï¼Œ introduction by Robbins and Monro in 1951ï¼Œ æ˜¯ä¸€ç§æ ‡å‡†çš„è§£å†³Equations of the form $\mathbf{f}({\boldsymbol {\theta}) = \mathbf{0}$ çš„æ–¹æ³•ï¼Œåªæœ‰å„ä¸ªå™ªå£°æµ‹é‡ $\mathbf{f}(\cdot)$ å¯ç”¨ã€‚å¦‚æœ $\mathbf{f}({\boldsymbol {\theta}) = \nabla J({\boldsymbol {\theta})$  Ğ´Ğ»ÑæŸå‡½æ•° $J(\cdot)$ ï¼Œ then SA ä¹Ÿå¯ä»¥ç”¨æ¥æ‰¾åˆ° $J(\cdot)$ çš„ç«™ç‚¹ç‚¹ã€‚åœ¨å¤§é‡çš„æ–‡çŒ®ä¸­ï¼Œå‡è®¾ $\mathbf{xi}_{t+1}$ çš„ conditional mean ä¸ºé›¶ï¼Œå¹¶ä¸”å…¶ conditional variance éš $t$ å¢é•¿ã€‚æ­¤å¤–ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¼ºè°ƒâ€œåŒæ­¥â€ SAï¼Œå³åœ¨æ¯ä¸ªæ—¶é—´ $t$ ä¸­ï¼Œ\textit{æ¯ä¸€ä¸ª}  ${\boldsymbol {\theta}_t$ çš„æ›´æ–°ã€‚æ€»ä¹‹ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†SAç†è®ºæ‰©å±•åˆ°åŒ…æ‹¬å™ªå£°error çš„éé›¶ conditional mean å’Œ/æˆ–ä¸bounded conditional varianceï¼Œå¹¶ä¸” asynchronous SAã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ derivestimates çš„é€Ÿåº¦æ”¶æ•›ç‡ï¼Œç„¶ååº”ç”¨æ–°ç»“æœåˆ°éå¯¹ç§°ä¼˜åŒ–å’ŒMarkovian SA ä¸­çš„é—®é¢˜ä¸Šï¼Œè¯æ˜ SA åœ¨è¿™äº›æƒ…å†µä¸‹æ”¶æ•›ï¼Œå¹¶è®¡ç®—äº†â€œæœ€ä½³æ­¥é•¿åºåˆ—â€ ä»¥æœ€å¤§åŒ– estimated rate of convergenceã€‚
</details></li>
</ul>
<hr>
<h2 id="Score-Aware-Policy-Gradient-Methods-and-Performance-Guarantees-using-Local-Lyapunov-Conditions-Applications-to-Product-Form-Stochastic-Networks-and-Queueing-Systems"><a href="#Score-Aware-Policy-Gradient-Methods-and-Performance-Guarantees-using-Local-Lyapunov-Conditions-Applications-to-Product-Form-Stochastic-Networks-and-Queueing-Systems" class="headerlink" title="Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems"></a>Score-Aware Policy-Gradient Methods and Performance Guarantees using Local Lyapunov Conditions: Applications to Product-Form Stochastic Networks and Queueing Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02804">http://arxiv.org/abs/2312.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: CÃ©line Comte, Matthieu Jonckheere, Jaron Sanders, Albert Senen-Cerda</li>
<li>for: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯è§£å†³Markovå†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ä¸­çš„å¼‚å¸¸å¤§çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä»¥åŠéå‡¸ç›®æ ‡å‡½æ•°ï¼Œä½¿å¾—è®¸å¤šæœºå™¨å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•æ— æ³• convergesã€‚</li>
<li>methods: è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ‘˜è¦ä¼°è®¡å™¨called score-aware gradient estimatorsï¼ˆSAGEsï¼‰ï¼Œå¯ä»¥åœ¨MDPçš„ç«™ç‚¹åˆ†å¸ƒæ˜¯ exponential family parametrized by policy parametersæ—¶ä¼°è®¡ç­–vector gradientï¼Œæ— éœ€è®¡ç®—å€¼å‡½æ•°ã€‚</li>
<li>results: ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ä¸¤ä¸ªå¸¸è§çš„æ§åˆ¶é—®é¢˜ä¸­ï¼ŒSAGEså¯ä»¥æ›´å¿«åœ°æ‰¾åˆ°ä¼˜åŒ–ç­–ç•¥ï¼Œå¹¶ä¸”åœ¨éå‡¸ç›®æ ‡å‡½æ•°å’Œå¤šä¸ªæœ€å¤§å€¼æ—¶ï¼Œç­–ç•¥çš„æ¦‚ç‡å¾ˆé«˜åœ° converges to ä¼˜åŒ–ç­–ç•¥ï¼Œåªè¦å®ƒä»¬åœ¨ä¼˜åŒ–ç­–ç•¥é™„è¿‘å¼€å§‹ã€‚<details>
<summary>Abstract</summary>
Stochastic networks and queueing systems often lead to Markov decision processes (MDPs) with large state and action spaces as well as nonconvex objective functions, which hinders the convergence of many reinforcement learning (RL) algorithms. Policy-gradient methods perform well on MDPs with large state and action spaces, but they sometimes experience slow convergence due to the high variance of the gradient estimator. In this paper, we show that some of these difficulties can be circumvented by exploiting the structure of the underlying MDP. We first introduce a new family of gradient estimators called score-aware gradient estimators (SAGEs). When the stationary distribution of the MDP belongs to an exponential family parametrized by the policy parameters, SAGEs allow us to estimate the policy gradient without relying on value-function estimation, contrary to classical policy-gradient methods like actor-critic. To demonstrate their applicability, we examine two common control problems arising in stochastic networks and queueing systems whose stationary distributions have a product-form, a special case of exponential families. As a second contribution, we show that, under appropriate assumptions, the policy under a SAGE-based policy-gradient method has a large probability of converging to an optimal policy, provided that it starts sufficiently close to it, even with a nonconvex objective function and multiple maximizers. Our key assumptions are that, locally around a maximizer, a nondegeneracy property of the Hessian of the objective function holds and a Lyapunov function exists. Finally, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic algorithm. The results demonstrate that the SAGE-based method finds close-to-optimal policies more rapidly, highlighting its superior performance over the traditional actor-critic method.
</details>
<details>
<summary>æ‘˜è¦</summary>
Stochastic networks and queueing systems  oft lead to Markov decision processes (MDPs) with large state and action spaces as well as nonconvex objective functions, which hinders the convergence of many reinforcement learning (RL) algorithms. Policy-gradient methods perform well on MDPs with large state and action spaces, but they sometimes experience slow convergence due to the high variance of the gradient estimator. In this paper, we show that some of these difficulties can be circumvented by exploiting the structure of the underlying MDP. We first introduce a new family of gradient estimators called score-aware gradient estimators (SAGEs). When the stationary distribution of the MDP belongs to an exponential family parametrized by the policy parameters, SAGEs allow us to estimate the policy gradient without relying on value-function estimation, contrary to classical policy-gradient methods like actor-critic. To demonstrate their applicability, we examine two common control problems arising in stochastic networks and queueing systems whose stationary distributions have a product-form, a special case of exponential families. As a second contribution, we show that, under appropriate assumptions, the policy under a SAGE-based policy-gradient method has a large probability of converging to an optimal policy, provided that it starts sufficiently close to it, even with a nonconvex objective function and multiple maximizers. Our key assumptions are that, locally around a maximizer, a nondegeneracy property of the Hessian of the objective function holds and a Lyapunov function exists. Finally, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic algorithm. The results demonstrate that the SAGE-based method finds close-to-optimal policies more rapidly, highlighting its superior performance over the traditional actor-critic method.
</details></li>
</ul>
<hr>
<h2 id="Materials-Expert-Artificial-Intelligence-for-Materials-Discovery"><a href="#Materials-Expert-Artificial-Intelligence-for-Materials-Discovery" class="headerlink" title="Materials Expert-Artificial Intelligence for Materials Discovery"></a>Materials Expert-Artificial Intelligence for Materials Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02796">http://arxiv.org/abs/2312.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanjun Liu, Milena Jovanovic, Krishnanand Mallayya, Wesley J. Maddox, Andrew Gordon Wilson, Sebastian Klemenz, Leslie M. Schoop, Eun-Ah Kim</li>
<li>for:  This paper aims to develop a machine learning approach to uncover predictive descriptors for emergent material properties from vast data space, with a focus on topological semimetals (TSMs) among square-net materials.</li>
<li>methods:  The authors use a machine learning approach called â€œMaterials Expert-Artificial Intelligenceâ€ (ME-AI) to encapsulate and articulate human intuition, which is based on experimental data whenever possible. They use Dirichlet-based Gaussian process regression with a specialized kernel to reveal composite descriptors for square-net TSMs.</li>
<li>results:  The ME-AI learned descriptors independently reproduce expert intuition and expand upon it, pointing to hypervalency as a critical chemical feature predicting TSM within square-net compounds. The success of the approach on a carefully defined problem suggests that it is promising for machine learning-aided material discovery.<details>
<summary>Abstract</summary>
The advent of material databases provides an unprecedented opportunity to uncover predictive descriptors for emergent material properties from vast data space. However, common reliance on high-throughput ab initio data necessarily inherits limitations of such data: mismatch with experiments. On the other hand, experimental decisions are often guided by an expert's intuition honed from experiences that are rarely articulated. We propose using machine learning to "bottle" such operational intuition into quantifiable descriptors using expertly curated measurement-based data. We introduce "Materials Expert-Artificial Intelligence" (ME-AI) to encapsulate and articulate this human intuition. As a first step towards such a program, we focus on the topological semimetal (TSM) among square-net materials as the property inspired by the expert-identified descriptor based on structural information: the tolerance factor. We start by curating a dataset encompassing 12 primary features of 879 square-net materials, using experimental data whenever possible. We then use Dirichlet-based Gaussian process regression using a specialized kernel to reveal composite descriptors for square-net topological semimetals. The ME-AI learned descriptors independently reproduce expert intuition and expand upon it. Specifically, new descriptors point to hypervalency as a critical chemical feature predicting TSM within square-net compounds. Our success with a carefully defined problem points to the "machine bottling human insight" approach as promising for machine learning-aided material discovery.
</details>
<details>
<summary>æ‘˜è¦</summary>
Material databases çš„å‡ºç°æä¾›äº†å‰æ‰€æœªæœ‰çš„æœºä¼šï¼Œæ­ç¤ºå‡ºemergent material propertiesçš„é¢„æµ‹æè¿°ç¬¦ä»åºå¤§çš„æ•°æ®ç©ºé—´ä¸­ã€‚ç„¶è€Œï¼Œé€šå¸¸ä¾èµ–äºé«˜é€šé‡ab initioæ•°æ®çš„é™åˆ¶ï¼Œè¿™äº›æ•°æ®ä¸å®éªŒä¸åŒ¹é…ã€‚ç›¸åï¼Œå®éªŒå†³ç­– oftentimes å—åˆ°ä¸“å®¶çš„ç›´è§‰å¯¼å‘ï¼Œè¿™äº›ç›´è§‰é€šå¸¸æ˜¯ä»ç»éªŒä¸­ç†Ÿæ‚‰è€Œæ¥ï¼Œè€Œè¿™äº›ç»éªŒ rarely è¢«è¯¦ç»†è¡¨è¿°ã€‚æˆ‘ä»¬æè®®ä½¿ç”¨æœºå™¨å­¦ä¹ æ¥â€œç“¶åŒ–â€è¿™äº›äººç±»ç›´è§‰ï¼Œè½¬åŒ–ä¸ºå¯è¡¡é‡çš„æè¿°ç¬¦ï¼Œä½¿ç”¨ä¸“å®¶curated measurement-basedæ•°æ®ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œMaterials Expert-Artificial Intelligenceâ€ï¼ˆME-AIï¼‰ã€‚ä½œä¸ºè¿™ä¸€è®¡åˆ’çš„é¦–å…ˆæ­¥éª¤ï¼Œæˆ‘ä»¬å°†å…³æ³¨åœ¨å››è§’ç½‘ææ–™ä¸­çš„topological semimetalï¼ˆTSMï¼‰ï¼ŒåŸºäºç»“æ„ä¿¡æ¯æ‰€ inspirited çš„æè¿°ç¬¦ï¼šå®¹å¿å› å­ã€‚æˆ‘ä»¬å¼€å§‹æ˜¯é€šè¿‡ç­›é€‰879ä¸ªå››è§’ç½‘ææ–™çš„12ä¸ªåŸºæœ¬ç‰¹å¾ï¼Œä½¿ç”¨å®éªŒæ•°æ® whenever possibleã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨Dirichlet-based Gaussian process regressionçš„ç‰¹æ®Škernelæ¥æ­ç¤ºsquare-net topological semimetalsä¸­çš„å¤åˆæè¿°ç¬¦ã€‚ME-AIå­¦ä¹ çš„æè¿°ç¬¦ç‹¬ç«‹åœ°é‡ç°ä¸“å®¶ç›´è§‰ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥å‘æ˜äº†å®ƒã€‚ Specifically, new descriptors point to hypervalency as a critical chemical feature predicting TSM within square-net compoundsã€‚æˆ‘ä»¬åœ¨å®šåˆ¶çš„é—®é¢˜ä¸ŠæˆåŠŸï¼Œè¡¨æ˜äº†â€œæœºå™¨ç“¶åŒ–äººç±»æ™ºæ…§â€çš„æ–¹æ³•ä¸ºæœºå™¨å­¦ä¹ å¸®åŠ©ææ–™å‘ç°å…·æœ‰æ½œåŠ›ã€‚
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Driven-Sensitivity-Analysis-of-E3SM-Land-Model-Parameters-for-Wetland-Methane-Emissions"><a href="#Machine-Learning-Driven-Sensitivity-Analysis-of-E3SM-Land-Model-Parameters-for-Wetland-Methane-Emissions" class="headerlink" title="Machine Learning Driven Sensitivity Analysis of E3SM Land Model Parameters for Wetland Methane Emissions"></a>Machine Learning Driven Sensitivity Analysis of E3SM Land Model Parameters for Wetland Methane Emissions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02786">http://arxiv.org/abs/2312.02786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandeep Chinta, Xiang Gao, Qing Zhu<br>for:This study aims to identify critical parameters for methane emission in the Energy Exascale Earth System Model (E3SM) land model (ELM) and to reduce biases and uncertainties in future projections using sensitivity analysis (SA) and machine learning (ML) algorithms.methods:The study uses SA to examine the impact of 19 selected parameters responsible for critical biogeochemical processes in the methane module of ELM on various CH4 fluxes at 14 FLUXNET-CH4 sites with diverse vegetation types. The study also employs an ML algorithm to emulate the complex behavior of ELM methane biogeochemistry and to reduce computational costs.results:The study found that parameters linked to CH4 production and diffusion generally present the highest sensitivities despite apparent seasonal variation. Comparing simulated emissions from perturbed parameter sets against FLUXNET-CH4 observations revealed that better performances can be achieved at each site compared to the default parameter values, indicating a scope for further improving simulated emissions using parameter calibration with advanced optimization techniques like Bayesian optimization.<details>
<summary>Abstract</summary>
Methane (CH4) is the second most critical greenhouse gas after carbon dioxide, contributing to 16-25% of the observed atmospheric warming. Wetlands are the primary natural source of methane emissions globally. However, wetland methane emission estimates from biogeochemistry models contain considerable uncertainty. One of the main sources of this uncertainty arises from the numerous uncertain model parameters within various physical, biological, and chemical processes that influence methane production, oxidation, and transport. Sensitivity Analysis (SA) can help identify critical parameters for methane emission and achieve reduced biases and uncertainties in future projections. This study performs SA for 19 selected parameters responsible for critical biogeochemical processes in the methane module of the Energy Exascale Earth System Model (E3SM) land model (ELM). The impact of these parameters on various CH4 fluxes is examined at 14 FLUXNET- CH4 sites with diverse vegetation types. Given the extensive number of model simulations needed for global variance-based SA, we employ a machine learning (ML) algorithm to emulate the complex behavior of ELM methane biogeochemistry. ML enables the computational time to be shortened significantly from 6 CPU hours to 0.72 milliseconds, achieving reduced computational costs. We found that parameters linked to CH4 production and diffusion generally present the highest sensitivities despite apparent seasonal variation. Comparing simulated emissions from perturbed parameter sets against FLUXNET-CH4 observations revealed that better performances can be achieved at each site compared to the default parameter values. This presents a scope for further improving simulated emissions using parameter calibration with advanced optimization techniques like Bayesian optimization.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ°¨ (CH4) æ˜¯å¤§æ°”ä¸­ç¬¬äºŒé‡è¦çš„ç»¿è‰²æ°”ä½“ï¼Œå æ®å¤§æ°”æš–åŒ–çš„ 16-25%ã€‚æ¹¿åœ°æ˜¯å…¨çƒä¸»è¦çš„è‡ªç„¶æ°¨å‘ç”Ÿæºã€‚ç„¶è€Œï¼Œæ¹¿åœ°æ°¨å‘ç”Ÿä¼°è®¡ä»ç”Ÿç‰©åœ°çƒåŒ–å­¦æ¨¡å‹ä¸­å«æœ‰è¾ƒå¤§çš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§ä¸ç¡®å®šæ€§çš„ä¸»è¦æ¥æºæ˜¯ç”Ÿç‰©åœ°çƒåŒ–å­¦è¿‡ç¨‹ä¸­çš„å¤šä¸ªä¸ç¡®å®šå‚æ•°ã€‚æ•æ„Ÿåˆ†æ (SA) å¯ä»¥å¸®åŠ©æ ‡è¯†æ°¨å‘ç”Ÿä¸­å…³é”®çš„å‚æ•°ï¼Œä»¥ä¾¿åœ¨æœªæ¥é¢„æµ‹ä¸­å‡å°‘åå·®å’Œä¸ç¡®å®šæ€§ã€‚è¿™ä¸ªç ”ç©¶åœ¨ E3SM  terrestrial model (ELM) ä¸­çš„æ°¨æ¨¡å—ä¸­è¿›è¡Œäº† 19 ä¸ªå‚æ•°çš„æ•æ„Ÿåˆ†æã€‚è¿™äº›å‚æ•°å½±å“ CH4 çš„å¤šç§æµå‘ï¼Œå¹¶åœ¨ 14 ä¸ª FLUXNET-CH4 ç«™ç‚¹ä¸Šè¿›è¡Œäº†å¤šç§æ¤è¢«ç±»å‹çš„ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµã€‚ç”±äºéœ€è¦è¿›è¡Œå…¨çƒå·®å¼‚åŸºäºçš„ SAï¼Œæˆ‘ä»¬ä½¿ç”¨æœºå™¨å­¦ä¹  (ML) ç®—æ³•æ¥æ¨¡æ‹Ÿ ELM æ°¨ç”Ÿç‰©åœ°çƒåŒ–å­¦çš„å¤æ‚è¡Œä¸ºã€‚ ML ä½¿å¾—è®¡ç®—æ—¶é—´ä»åŸæ¥çš„ 6 CPU å°æ—¶ç¼©çŸ­åˆ° 0.72 æ¯«ç§’ï¼Œå®ç°äº†è®¡ç®—æˆæœ¬çš„å‡å°‘ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ CH4 ç”Ÿäº§å’Œæ‰©æ•£ç›´æ¥ç›¸å…³çš„å‚æ•°é€šå¸¸å…·æœ‰æœ€é«˜æ•æ„Ÿæ€§ï¼Œå°½ç®¡æ˜¾ç¤ºå­£èŠ‚æ€§å˜åŒ–ã€‚å¯¹æ¯”æ¨æµ‹å‚æ•°é›†ä¸­çš„é‡Šæ”¾ä¸ FLUXNET-CH4 è§‚æµ‹æ•°æ®è¡¨ç¤ºï¼Œå¯ä»¥åœ¨æ¯ä¸ªç«™ç‚¹ä¸Šå®ç°æ›´å¥½çš„è¡¨ç°ï¼Œæ¯” default å‚æ•°å€¼æ›´å¥½ã€‚è¿™è¡¨æ˜å¯ä»¥é€šè¿‡å‚æ•°è°ƒæ•´å’Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚ Bayesian ä¼˜åŒ–ï¼Œè¿›ä¸€æ­¥æé«˜é¢„æµ‹çš„é‡Šæ”¾ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-â€œLook-Aheadâ€-Nonlocal-Traffic-Dynamics-in-a-Ring-Road"><a href="#Learning-â€œLook-Aheadâ€-Nonlocal-Traffic-Dynamics-in-a-Ring-Road" class="headerlink" title="Learning â€œLook-Aheadâ€ Nonlocal Traffic Dynamics in a Ring Road"></a>Learning â€œLook-Aheadâ€ Nonlocal Traffic Dynamics in a Ring Road</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02770">http://arxiv.org/abs/2312.02770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenguang Zhao, Huan Yu</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æ¢è®¨éæœ¬åœ°å·®åˆ†æ–¹ç¨‹æ¨¡å‹ï¼ˆPDEï¼‰çš„åº”ç”¨ï¼Œä»¥æŒæ¡è½¦æµé€Ÿåº¦çš„é¢„æµ‹å’Œç®¡ç†ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†äº¤é€šè½¨è¿¹æ•°æ®ï¼Œè®¾è®¡äº†ç‰©ç†å­¦ Informed Neural Networkï¼ˆPINNï¼‰æ¥learns the fundamental diagramå’Œlook-aheadæ ¸å‡½æ•°ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°çš„æ–¹å¼åˆ›å»ºäº†ä¸€ä¸ªåŸºäºæ•°æ®çš„å¢å¼ºéæœ¬åœ°LWRæ¨¡å‹ã€‚</li>
<li>results: ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼Œä½¿ç”¨äº†PINNå­¦ä¹ çš„éæœ¬åœ°LWRæ¨¡å‹èƒ½å¤Ÿæ›´ precisellyé¢„æµ‹è½¦æµé€Ÿåº¦çš„ä¼ æ’­ï¼Œåœ¨ä¸‰ä¸ªä¸åŒçš„æƒ…å†µä¸‹éƒ½æœ‰æ›´å¥½çš„é¢„æµ‹æ•ˆæœï¼šåœè½¦å¾€å¤è¿åŠ¨ã€å¡è½¦å’Œè‡ªç”±æµã€‚æ­¤å¤–ï¼Œç ”ç©¶ä¹Ÿç¡®è®¤äº†â€œlook-aheadâ€æ•ˆåº”çš„å­˜åœ¨ï¼Œå¹¶å‘ç°optimal nonlocal kernelçš„é•¿åº¦ä¸ºçº¦35-50ç±³ï¼Œè€Œå†…éƒ¨5ç±³çš„æ ¸å‡½æ•°å äº†å¤§å¤šæ•°éæœ¬åœ°æ•ˆåº”ã€‚<details>
<summary>Abstract</summary>
The macroscopic traffic flow model is widely used for traffic control and management. To incorporate drivers' anticipative behaviors and to remove impractical speed discontinuity inherent in the classic Lighthill-Whitham-Richards (LWR) traffic model, nonlocal partial differential equation (PDE) models with ``look-ahead" dynamics have been proposed, which assume that the speed is a function of weighted downstream traffic density. However, it lacks data validation on two important questions: whether there exist nonlocal dynamics, and how the length and weight of the ``look-ahead" window affect the spatial temporal propagation of traffic densities. In this paper, we adopt traffic trajectory data from a ring-road experiment and design a physics-informed neural network to learn the fundamental diagram and look-ahead kernel that best fit the data, and reinvent a data-enhanced nonlocal LWR model via minimizing the loss function combining the data discrepancy and the nonlocal model discrepancy. Results show that the learned nonlocal LWR yields a more accurate prediction of traffic wave propagation in three different scenarios: stop-and-go oscillations, congested, and free traffic. We first demonstrate the existence of ``look-ahead" effect with real traffic data. The optimal nonlocal kernel is found out to take a length of around 35 to 50 meters, and the kernel weight within 5 meters accounts for the majority of the nonlocal effect. Our results also underscore the importance of choosing a priori physics in machine learning models.
</details>
<details>
<summary>æ‘˜è¦</summary>
å®½æ³›äº¤é€šæµæ¨¡å‹å¹¿æ³›ç”¨äºäº¤é€šæ§åˆ¶å’Œç®¡ç†ã€‚ä¸ºäº†åŒ…æ‹¬ drivers çš„é¢„æµ‹è¡Œä¸ºå¹¶æ¶ˆé™¤ç±»åˆ« Lighthill-Whitham-Richards (LWR) æµä½“æ¨¡å‹ä¸­çš„ä¸å®é™…é€Ÿåº¦ç¼ºå¤±ï¼Œéæœ¬åœ°partial differential equation (PDE) æ¨¡å‹ WITH "look-ahead" åŠ¨åŠ›å­¦è¢«æè®®ï¼Œå®ƒå‡è®¾é€Ÿåº¦ä¸ºä¸‹æ¸¸äº¤é€šå¯†åº¦çš„åŠ æƒå‡½æ•°ã€‚ç„¶è€Œï¼Œå®ƒç¼ºä¹æ•°æ®éªŒè¯ä¸¤ä¸ªé‡è¦é—®é¢˜ï¼šæ˜¯å¦å­˜åœ¨éæœ¬åœ°åŠ¨åŠ›å­¦ï¼Œä»¥åŠ"look-ahead" çª—å£çš„é•¿åº¦å’Œé‡é‡å¦‚ä½•å½±å“ç©ºé—´æ—¶é—´å±‚æµå¯†åº¦çš„ä¼ æ’­ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨ç¯è·¯å®éªŒçš„äº¤é€šè½¨è¿¹æ•°æ®ï¼Œå¹¶è®¾è®¡äº†physics-informed neural networkæ¥å­¦ä¹ åŸºæœ¬å›¾ogramå’Œlook-ahead kernelï¼Œå¹¶é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°æ¥æ¢å¤æ•°æ®å¢å¼ºçš„éæœ¬åœ°LWRæ¨¡å‹ã€‚ç»“æœè¡¨æ˜å­¦ä¹ çš„éæœ¬åœ°LWRæ¨¡å‹å¯ä»¥æ›´å‡†ç¡®åœ°é¢„æµ‹äº¤é€šæ³¢çš„ä¼ æ’­åœ¨ä¸‰ç§ä¸åŒçš„æƒ…å†µä¸‹ï¼šåœæ­¢-å’Œ-è·‘åŠ¨ã€å µå¡å’Œè‡ªç”±äº¤é€šã€‚æˆ‘ä»¬é¦–å…ˆè¯æ˜äº†å®é™…äº¤é€šæ•°æ®ä¸­çš„"look-ahead"æ•ˆåº”çš„å­˜åœ¨ã€‚æœ€ä½³çš„éæœ¬åœ°kernelé•¿åº¦ä¸º35-50ç±³ï¼Œè€Œåœ¨5ç±³å†…çš„kernelé‡é‡å äº†éæœ¬åœ°æ•ˆåº”çš„å¤§å¤šæ•°ã€‚æˆ‘ä»¬çš„ç»“æœä¹Ÿå¼ºè°ƒäº†åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­é‡‡ç”¨å…ˆéªŒæ³•çš„é‡è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="LExCI-A-Framework-for-Reinforcement-Learning-with-Embedded-Systems"><a href="#LExCI-A-Framework-for-Reinforcement-Learning-with-Embedded-Systems" class="headerlink" title="LExCI: A Framework for Reinforcement Learning with Embedded Systems"></a>LExCI: A Framework for Reinforcement Learning with Embedded Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02739">http://arxiv.org/abs/2312.02739</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mechatronics-rwth/lexci-2">https://github.com/mechatronics-rwth/lexci-2</a></li>
<li>paper_authors: Kevin Badalian, Lucas Koch, Tobias Brinkmann, Mario Picerno, Marius Wegener, Sung-Yong Lee, Jakob Andert</li>
<li>for: è¿™ç¯‡è®ºæ–‡æ˜¯å…³äºæ§åˆ¶å·¥ç¨‹ä¸­çš„äººå·¥æ™ºèƒ½åº”ç”¨ï¼Œå…·ä½“æ¥è¯´æ˜¯ä¸€ç§åä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰çš„æ–¹æ³•ï¼Œç”¨äºè®©ä»£ç†äººåœ¨ç¯å¢ƒä¸­è‡ªç”±åœ°äº’åŠ¨ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ç­–ç•¥ã€‚</li>
<li>methods: æœ¬è®ºæ–‡ä½¿ç”¨çš„æ–¹æ³•æ˜¯ä¸€ç§åä¸ºLExCIï¼ˆLearning and Experiencing Cycle Interfaceï¼‰çš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥å°†RLlibå¼€æºåº“ä¸ç‰¹å®šçš„åµŒå…¥å¼è®¾å¤‡é›†æˆï¼Œä»¥ä¾¿åœ¨è¿™äº›è®¾å¤‡ä¸Šè®­ç»ƒRLä»£ç†äººã€‚</li>
<li>results: æœ¬è®ºæ–‡çš„ç»“æœè¡¨æ˜ï¼ŒLExCIæ¡†æ¶å¯ä»¥å¸®åŠ©è®­ç»ƒRLä»£ç†äººï¼Œå¹¶ä¸”å¯ä»¥ä¸ç°æœ‰çš„å·¥å…·é“¾é›†æˆã€‚ä¸¤ç§çŠ¶æ€å‰ç»RLç®—æ³•å’Œå¿«é€Ÿæ§åˆ¶æ¦‚å¿µéªŒè¯ç³»ç»Ÿéƒ½è¢«ç”¨æ¥æ¼”ç¤ºLExCIçš„å¯æ“ä½œæ€§ã€‚<details>
<summary>Abstract</summary>
Advances in artificial intelligence (AI) have led to its application in many areas of everyday life. In the context of control engineering, reinforcement learning (RL) represents a particularly promising approach as it is centred around the idea of allowing an agent to freely interact with its environment to find an optimal strategy. One of the challenges professionals face when training and deploying RL agents is that the latter often have to run on dedicated embedded devices. This could be to integrate them into an existing toolchain or to satisfy certain performance criteria like real-time constraints. Conventional RL libraries, however, cannot be easily utilised in conjunction with that kind of hardware. In this paper, we present a framework named LExCI, the Learning and Experiencing Cycle Interface, which bridges this gap and provides end-users with a free and open-source tool for training agents on embedded systems using the open-source library RLlib. Its operability is demonstrated with two state-of-the-art RL-algorithms and a rapid control prototyping system.
</details>
<details>
<summary>æ‘˜è¦</summary>
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„è¿›æ­¥å·²ç»åº”ç”¨åˆ°äº†æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ä¸ªé¢†åŸŸã€‚åœ¨æ§åˆ¶å·¥ç¨‹ä¸­ï¼Œå›å½’å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§ç‰¹åˆ«æœ‰æŠŠæ¡çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå°†ä»£ç†äººå…è®¸è‡ªç”±åœ°ä¸ç¯å¢ƒäº’åŠ¨ï¼Œä»¥æ‰¾åˆ°æœ€ä½³ç­–ç•¥ã€‚ç„¶è€Œï¼Œè®­ç»ƒå’Œéƒ¨ç½²RLä»£ç†äººæ—¶ï¼Œä¸“ä¸šäººå‘˜å¸¸é‡åˆ°çš„æŒ‘æˆ˜æ˜¯RLä»£ç†äººé€šå¸¸éœ€è¦è¿è¡Œåœ¨ä¸“é—¨çš„åµŒå…¥å¼è®¾å¤‡ä¸Šã€‚è¿™å¯èƒ½æ˜¯ä¸ºäº†ç»“åˆç°æœ‰çš„å·¥å…·é“¾ï¼Œæˆ–è€…æ»¡è¶³certainæ€§èƒ½æ ‡å‡†ï¼Œå¦‚å®æ—¶çº¦æŸã€‚ conventioanl RLåº“ä¸èƒ½æ–¹ä¾¿åœ°åœ¨è¿™ç§ç¡¬ä»¶ä¸Šä½¿ç”¨ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºLExCIçš„æ¡†æ¶ï¼Œå³å­¦ä¹ å’Œä½“éªŒå¾ªç¯ç•Œé¢ã€‚LExCI bridges this gap and provides end-users with a free and open-source tool for training agents on embedded systems using the open-source library RLlibã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯ä»¥ä¸ä¸¤ç§ç°çŠ¶æœ€ä½³RLç®—æ³•å’Œå¿«é€Ÿæ§åˆ¶åŸå‹ç³»ç»Ÿè¿›è¡Œè¿ç»ƒã€‚
</details></li>
</ul>
<hr>
<h2 id="Provable-Adversarial-Robustness-for-Group-Equivariant-Tasks-Graphs-Point-Clouds-Molecules-and-More"><a href="#Provable-Adversarial-Robustness-for-Group-Equivariant-Tasks-Graphs-Point-Clouds-Molecules-and-More" class="headerlink" title="(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More"></a>(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs, Point Clouds, Molecules, and More</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02708">http://arxiv.org/abs/2312.02708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Schuchardt, Yan Scholten, Stephan GÃ¼nnemann</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æä¾›ä¸€ç§å…·æœ‰ä»»åŠ¡å¯¹ç§°æ€§çš„é²æ£’æ€§å®šä¹‰ï¼Œå¹¶è¯æ˜å¯ä»¥é€šè¿‡é€‰æ‹©å…·æœ‰ä»»åŠ¡å¯¹ç§°æ€§çš„æ¨¡å‹å’Œè¿›è¡Œ tradicional adversarial robustness è¯æ˜æ¥å®ç°å¯é çš„é²æ£’æ€§ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº† equivariance-preserving randomized smoothing æ¡†æ¶å’Œarchitecture-specific graph edit distance certificatesæ¥è¯æ˜æ¨¡å‹çš„é²æ£’æ€§ã€‚</li>
<li>results: æœ¬ç ”ç©¶å‘ç°äº†ä¸€äº›é²æ£’æ€§è¯æ˜æ–¹æ³•ï¼ŒåŒ…æ‹¬ choosing a model that matches the taskâ€™s equivariances å’Œ certifying traditional adversarial robustnessï¼Œå¯ä»¥ä¸ºæœªæ¥åœ¨é²æ£’æœºå™¨å­¦ä¹ å’Œå‡ ä½•æœºå™¨å­¦ä¹ ä¹‹é—´çš„å·¥ä½œæä¾›åŸºç¡€ã€‚<details>
<summary>Abstract</summary>
A machine learning model is traditionally considered robust if its prediction remains (almost) constant under input perturbations with small norm. However, real-world tasks like molecular property prediction or point cloud segmentation have inherent equivariances, such as rotation or permutation equivariance. In such tasks, even perturbations with large norm do not necessarily change an input's semantic content. Furthermore, there are perturbations for which a model's prediction explicitly needs to change. For the first time, we propose a sound notion of adversarial robustness that accounts for task equivariance. We then demonstrate that provable robustness can be achieved by (1) choosing a model that matches the task's equivariances (2) certifying traditional adversarial robustness. Certification methods are, however, unavailable for many models, such as those with continuous equivariances. We close this gap by developing the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. We additionally derive the first architecture-specific graph edit distance certificates, i.e. sound robustness guarantees for isomorphism equivariant tasks like node classification. Overall, a sound notion of robustness is an important prerequisite for future work at the intersection of robust and geometric machine learning.
</details>
<details>
<summary>æ‘˜è¦</summary>
We demonstrate that provable robustness can be achieved by (1) selecting a model that matches the task's equivariances and (2) certifying traditional adversarial robustness. However, certification methods are not available for many models, such as those with continuous equivariances. To address this gap, we develop the framework of equivariance-preserving randomized smoothing, which enables architecture-agnostic certification. Additionally, we derive the first architecture-specific graph edit distance certificates, which provide sound robustness guarantees for isomorphism equivariant tasks like node classification.Overall, a sound notion of robustness is crucial for future work at the intersection of robust and geometric machine learning.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Based-Speech-Enhancement-in-Matched-and-Mismatched-Conditions-Using-a-Heun-Based-Sampler"><a href="#Diffusion-Based-Speech-Enhancement-in-Matched-and-Mismatched-Conditions-Using-a-Heun-Based-Sampler" class="headerlink" title="Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler"></a>Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions Using a Heun-Based Sampler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02683">http://arxiv.org/abs/2312.02683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philippe Gonzalez, Zheng-Hua Tan, Jan Ã˜stergaard, Jesper Jensen, Tommy Sonne AlstrÃ¸m, Tobias May</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„æ˜¯speech enhancementçš„æ³›åŒ–æ€§èƒ½ï¼Œä»¥åŠ diffusion modelsåœ¨è¿™ä¸ªé¢†åŸŸçš„åº”ç”¨ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†diffusion modelsï¼Œå¹¶åœ¨å¤šä¸ªè¯­éŸ³ã€å™ªå£°å’Œbinaru room impulse responseï¼ˆBRIRï¼‰æ•°æ®åº“ä¸­è¿›è¡Œäº†è®­ç»ƒï¼Œä»¥æµ‹è¯•å…¶åœ¨ä¸åŒçš„å™ªå£°å’ŒéŸ³å“ç¯å¢ƒä¸‹çš„æ³›åŒ–æ€§èƒ½ã€‚</li>
<li>results: è®ºæ–‡è¡¨æ˜ï¼Œä½¿ç”¨å¤šä¸ªæ•°æ®åº“è¿›è¡Œè®­ç»ƒå¯ä»¥æé«˜ diffusion-based speech enhancement æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ï¼Œå¹¶ä¸”åœ¨ matched å’Œ mismatched æ¡ä»¶ä¸‹éƒ½è¡¨ç°å‡ºä¼˜äºå½“å‰é¢†å…ˆçš„æ³›åŒ–æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ Heun-based é‡‡æ ·å™¨ä¹Ÿå¯ä»¥åœ¨æ›´å°çš„è®¡ç®—æˆæœ¬ä¸‹æé«˜æ³›åŒ–æ€§èƒ½ã€‚<details>
<summary>Abstract</summary>
Diffusion models are a new class of generative models that have recently been applied to speech enhancement successfully. Previous works have demonstrated their superior performance in mismatched conditions compared to state-of-the art discriminative models. However, this was investigated with a single database for training and another one for testing, which makes the results highly dependent on the particular databases. Moreover, recent developments from the image generation literature remain largely unexplored for speech enhancement. These include several design aspects of diffusion models, such as the noise schedule or the reverse sampler. In this work, we systematically assess the generalization performance of a diffusion-based speech enhancement model by using multiple speech, noise and binaural room impulse response (BRIR) databases to simulate mismatched acoustic conditions. We also experiment with a noise schedule and a sampler that have not been applied to speech enhancement before. We show that the proposed system substantially benefits from using multiple databases for training, and achieves superior performance compared to state-of-the-art discriminative models in both matched and mismatched conditions. We also show that a Heun-based sampler achieves superior performance at a smaller computational cost compared to a sampler commonly used for speech enhancement.
</details>
<details>
<summary>æ‘˜è¦</summary>
Diffusion models æ˜¯ä¸€ç§æ–°çš„ç”Ÿæˆæ¨¡å‹ï¼Œæœ€è¿‘åœ¨è¯­éŸ³æå‡ä¸­å¾—åˆ°äº†æˆåŠŸåº”ç”¨ã€‚ä¹‹å‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒDiffusion models åœ¨ä¸åŒçš„åŒ¹é…æ¡ä»¶ä¸‹æ¯”ç°æœ‰çš„æè¿°æ€§æ¨¡å‹è¡¨ç°æ›´å‡ºè‰²ã€‚ç„¶è€Œï¼Œè¿™äº›ç ”ç©¶é€šå¸¸ä½¿ç”¨ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†ï¼Œè¿™ä½¿å¾—ç»“æœå—åˆ°ç‰¹å®šæ•°æ®é›†çš„é™åˆ¶ã€‚æ­¤å¤–ï¼Œå›¾åƒç”Ÿæˆé¢†åŸŸçš„æœ€æ–°å‘å±•è¿˜æ²¡æœ‰å¾—åˆ°è¿‡è¯­éŸ³æå‡çš„åº”ç”¨ã€‚è¿™äº›åŒ…æ‹¬Diffusion modelsä¸­çš„å™ªå£°ç¨‹åº¦æˆ–åå‘æŠ½è±¡ç­‰è®¾è®¡æ–¹é¢ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ç§åŸºäºDiffusion modelsçš„è¯­éŸ³æå‡æ¨¡å‹ï¼Œä½¿ç”¨å¤šä¸ªè¯­éŸ³ã€å™ªå£°å’ŒåŒè€³å®¤éŸ³å“å“åº”ï¼ˆBRIRï¼‰æ•°æ®é›†æ¥æ¨¡æ‹Ÿä¸åŒçš„åŒ¹é…æ¡ä»¶ã€‚æˆ‘ä»¬è¿˜å°è¯•äº†ä¸€ç§æ²¡æœ‰ç”¨äºè¯­éŸ³æå‡ä¹‹å‰çš„å™ªå£°ç¨‹åº¦å’ŒæŠ½è±¡æ–¹æ³•ã€‚æˆ‘ä»¬å‘ç°ï¼Œææ¡ˆçš„ç³»ç»Ÿåœ¨ä½¿ç”¨å¤šä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒæ—¶å¾—åˆ°äº†æ˜æ˜¾çš„æ”¹å–„ï¼Œå¹¶åœ¨åŒ¹é…å’Œä¸åŒ¹é…æ¡ä»¶ä¸‹éƒ½ä¸ç°æœ‰çš„æè¿°æ€§æ¨¡å‹ç›¸æ¯”è¡¨ç°å‡ºè‰²ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°äº†ä¸€ç§åŸºäºHeunçš„æŠ½è±¡æ–¹æ³•åœ¨è®¡ç®—æˆæœ¬æ›´å°çš„æƒ…å†µä¸‹è¡¨ç°æ›´å¥½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Sparse-Representation-of-Barron-Functions-with-the-Inverse-Scale-Space-Flow"><a href="#Learning-a-Sparse-Representation-of-Barron-Functions-with-the-Inverse-Scale-Space-Flow" class="headerlink" title="Learning a Sparse Representation of Barron Functions with the Inverse Scale Space Flow"></a>Learning a Sparse Representation of Barron Functions with the Inverse Scale Space Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02671">http://arxiv.org/abs/2312.02671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tjeerd Jan Heeringa, Tim Roith, Christoph Brune, Martin Burger</li>
<li>for: è¿™ paper æ˜¯ç”¨æ¥æ‰¾åˆ° Barron å‡½æ•°çš„ç¨€ç–è¡¨ç¤ºæ–¹æ³•ã€‚</li>
<li>methods: è¿™ paper ä½¿ç”¨ inverse scale space flow æ¥æ‰¾åˆ°ä¸€ä¸ªç¨€ç–æµ‹åº¦ $\mu$ï¼Œä½¿å¾— Barron å‡½æ•°ç›¸å…³äºæµ‹åº¦ $\mu$ å’Œå‡½æ•° $f$ ä¹‹é—´çš„ $L^2$ è·ç¦»æœ€å°åŒ–ã€‚</li>
<li>results: è¿™ paper åˆ†æäº†è¿™ç§æ–¹æ³•åœ¨ç†æƒ³æƒ…å†µä¸‹å’Œå¹²æ‰°æƒ…å†µä¸‹çš„æ”¶æ•›æ€§è´¨ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œç›®æ ‡å‡½æ•°ä¼šé€æ¸å‡å°‘ï¼Œç›´åˆ°åˆ°è¾¾æœ€å°å€¼ï¼Œå¹¶ä¸”æ”¶æ•›é€Ÿç‡ä¸º $\mathcal{O}(1&#x2F;t)$ã€‚åœ¨å¹²æ‰°æƒ…å†µä¸‹ï¼Œæœ€ä¼˜è§£å¯èƒ½ä¼šå—åˆ°å¤šä½™æˆ–åŠ æ³•å¸¸æ•°çš„å½±å“ã€‚è¿™ç§æ”¶æ•›æ€§ä¿æŒåœ¨åˆ†æå‚æ•°ç©ºé—´çš„ç¦»æ•£åŒ–ä¸Šï¼Œå¹¶ä¸”åœ¨ä¸æ–­ç»†åŒ–å‚æ•°ç©ºé—´ä¸Šçš„æœ€å°åŒ–ç‚¹ä¼š converges åˆ°å…¨å‚æ•°ç©ºé—´ä¸Šçš„æœ€ä¼˜è§£ã€‚<details>
<summary>Abstract</summary>
This paper presents a method for finding a sparse representation of Barron functions. Specifically, given an $L^2$ function $f$, the inverse scale space flow is used to find a sparse measure $\mu$ minimising the $L^2$ loss between the Barron function associated to the measure $\mu$ and the function $f$. The convergence properties of this method are analysed in an ideal setting and in the cases of measurement noise and sampling bias. In an ideal setting the objective decreases strictly monotone in time to a minimizer with $\mathcal{O}(1/t)$, and in the case of measurement noise or sampling bias the optimum is achieved up to a multiplicative or additive constant. This convergence is preserved on discretization of the parameter space, and the minimizers on increasingly fine discretizations converge to the optimum on the full parameter space.
</details>
<details>
<summary>æ‘˜è¦</summary>
In an ideal setting, the objective function decreases strictly monotonically in time to a minimizer with a rate of $\mathcal{O}(1/t)$. In the presence of measurement noise or sampling bias, the optimum is achieved up to a multiplicative or additive constant. This convergence is preserved when discretizing the parameter space, and the minimizers on increasingly fine discretizations converge to the optimum on the full parameter space.Translated into Simplified Chinese:è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œç”¨äºæ‰¾åˆ°å·´æœ—å‡½æ•°çš„ç®€æ´è¡¨ç¤ºã€‚ç»™å®šä¸€ä¸ª $L^2$ å‡½æ•° $f$ï¼Œä½¿ç”¨åå°ºåº¦ç©ºé—´æµåŠ¨æ¥æ‰¾åˆ°ä¸€ä¸ªç®€æ´åº¦é‡ $\mu$ï¼Œä½¿å¾—å·´æœ—å‡½æ•°ç›¸å…³äºåº¦é‡ $\mu$ å’Œå‡½æ•° $f$ ä¹‹é—´çš„ $L^2$ è·ç¦»æœ€å°ã€‚è¿™ç§æ–¹æ³•çš„æ”¶æ•›æ€§è¢«åˆ†æåœ¨ç†æƒ³æƒ…å†µä¸‹å’Œå¹²æ‰°å’ŒæŠ½è±¡åè§çš„æƒ…å†µä¸‹ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œç›®æ ‡å‡½æ•°éšç€æ—¶é—´çš„å¢é•¿è€Œé€æ¸å‡å°‘ï¼Œç›´åˆ°åˆ°è¾¾æœ€ä½³è§£ï¼Œå‡å°‘çš„é€Ÿç‡ä¸º $\mathcal{O}(1/t)$ã€‚åœ¨å¹²æ‰°å’ŒæŠ½è±¡åè§çš„æƒ…å†µä¸‹ï¼Œæœ€ä½³è§£å¯ä»¥åœ¨å¤šé¡¹å¼å¹‚çº§ä¸Šå‡å°‘ï¼Œä½†æ˜¯ä¼šå—åˆ°å¤šé¡¹å¼å¹‚çº§çš„å½±å“ã€‚å½“ç²¾åº¦åŒ–å‚æ•°ç©ºé—´æ—¶ï¼Œè¿™ç§æ”¶æ•›æ€§ä¿æŒä¸å˜ï¼Œå¹¶ä¸”åœ¨ä¸æ–­ç»†åŒ–å‚æ•°ç©ºé—´æ—¶ï¼Œæœ€ä½³è§£åœ¨ä¸æ–­ç»†åŒ–çš„å‚æ•°ç©ºé—´ä¸Šéƒ½ä¼š converge åˆ°å…¨å‚æ•°ç©ºé—´ä¸Šçš„æœ€ä½³è§£ã€‚Translated by Google Translate:This paper proposes a method for finding a sparse representation of Barron functions. Given an $L^2$ function $f$, the inverse scale space flow is used to find a sparse measure $\mu$ that minimizes the $L^2$ loss between the Barron function associated with the measure $\mu$ and the function $f$. The convergence properties of this method are analyzed in an ideal setting and in the cases of measurement noise and sampling bias.In an ideal setting, the objective function decreases strictly monotonically in time to a minimizer with a rate of $\mathcal{O}(1/t)$. In the presence of measurement noise or sampling bias, the optimum is achieved up to a multiplicative or additive constant. This convergence is preserved when discretizing the parameter space, and the minimizers on increasingly fine discretizations converge to the optimum on the full parameter space.
</details></li>
</ul>
<hr>
<h2 id="A-Self-Commissioning-Edge-Computing-Method-for-Data-Driven-Anomaly-Detection-in-Power-Electronic-Systems"><a href="#A-Self-Commissioning-Edge-Computing-Method-for-Data-Driven-Anomaly-Detection-in-Power-Electronic-Systems" class="headerlink" title="A Self-Commissioning Edge Computing Method for Data-Driven Anomaly Detection in Power Electronic Systems"></a>A Self-Commissioning Edge Computing Method for Data-Driven Anomaly Detection in Power Electronic Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02661">http://arxiv.org/abs/2312.02661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pere Izquierdo Gomez, Miguel E. Lopez Gajardo, Nenad Mijatovic, Tomislav Dragicevic</li>
<li>for: éªŒè¯ç”µå­è½¬æ¢å™¨å¯é æ€§çš„é‡è¦æ€§ï¼Œæ•°æ®é©±åŠ¨ç›‘æµ‹æŠ€æœ¯åœ¨è¿™æ–¹é¢æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ã€‚</li>
<li>methods: æœ¬æ–‡æå‡ºäº†ä¸€ç§è¾¹ç¼˜è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡ä¼˜å…ˆå­˜å‚¨è®­ç»ƒæ ·æœ¬çš„å¤§å°åå·®æ¥mitigate labæ•°æ®æœ‰é™æ ·æœ¬çš„å›°éš¾ï¼Œä»¥æé«˜è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œé¢„æµ‹æ€§ã€‚</li>
<li>results: å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜é¢„æµ‹ç²¾åº¦å’Œè®­ç»ƒé€Ÿåº¦ï¼Œæ¯” tradicional onlineå­¦ä¹ æ–¹æ³•æ— éœ€è¯¥æ•°æ®é€‰æ‹©è¿‡ç¨‹æ›´å¥½ã€‚<details>
<summary>Abstract</summary>
Ensuring the reliability of power electronic converters is a matter of great importance, and data-driven condition monitoring techniques are cementing themselves as an important tool for this purpose. However, translating methods that work well in controlled lab environments to field applications presents significant challenges, notably because of the limited diversity and accuracy of the lab training data. By enabling the use of field data, online machine learning can be a powerful tool to overcome this problem, but it introduces additional challenges in ensuring the stability and predictability of the training processes. This work presents an edge computing method that mitigates these shortcomings with minimal additional memory usage, by employing an autonomous algorithm that prioritizes the storage of training samples with larger prediction errors. The method is demonstrated on the use case of a self-commissioning condition monitoring system, in the form of a thermal anomaly detection scheme for a variable frequency motor drive, where the algorithm self-learned to distinguish normal and anomalous operation with minimal prior knowledge. The obtained results, based on experimental data, show a significant improvement in prediction accuracy and training speed, when compared to equivalent models trained online without the proposed data selection process.
</details>
<details>
<summary>æ‘˜è¦</summary>
This work proposes an edge computing method that mitigates these shortcomings with minimal additional memory usage. The method employs an autonomous algorithm that prioritizes the storage of training samples with larger prediction errors. The approach is demonstrated on the use case of a self-commissioning condition monitoring system, in the form of a thermal anomaly detection scheme for a variable frequency motor drive. The algorithm self-learned to distinguish normal and anomalous operation with minimal prior knowledge, and the obtained results, based on experimental data, show a significant improvement in prediction accuracy and training speed compared to equivalent models trained online without the proposed data selection process.
</details></li>
</ul>
<hr>
<h2 id="Do-AI-models-produce-better-weather-forecasts-than-physics-based-models-A-quantitative-evaluation-case-study-of-Storm-Ciaran"><a href="#Do-AI-models-produce-better-weather-forecasts-than-physics-based-models-A-quantitative-evaluation-case-study-of-Storm-Ciaran" class="headerlink" title="Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm CiarÃ¡n"></a>Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm CiarÃ¡n</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02658">http://arxiv.org/abs/2312.02658</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew J. Charlton-Perez, Helen F. Dacre, Simon Driscoll, Suzanne L. Gray, Ben Harvey, Natalie J. Harvey, Kieran M. R. Hunt, Robert W. Lee, Ranjini Swaminathan, Remy Vandaele, Ambrogio VolontÃ©</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯å¯¹ç°ä»£æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ simulate é«˜impact å¤©æ°”äº‹ä»¶æ–¹é¢çš„æ€§èƒ½è¿›è¡Œäº†è¯„ä¼°ã€‚</li>
<li>methods: è¿™ä¸ªç ”ç©¶ä½¿ç”¨äº†å››ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆFourCastNetã€Pangu-Weatherã€GraphCastå’ŒFourCastNet-v2ï¼‰æ¥é¢„æµ‹æ¬§æ´²é£æš´é›¨ç¾äº‹ä»¶ Storm Ciar&#39;anã€‚</li>
<li>results: ç ”ç©¶å‘ç°è¿™äº›æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°æ•æ‰é£æš´çš„å¤§è§„æ¨¡ç»“æ„ï¼ŒåŒ…æ‹¬äº‘å¤´çš„ä½ç½®ã€æ¸©å¸¦çš„å½¢çŠ¶å’Œçƒ­å¸¦æ¹æµçš„ä½ç½®ï¼Œä»¥åŠé£æš´çš„å‘å±•é©±åŠ¨å› ç´ ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬åœ¨å‘å¸ƒæ°”è±¡è­¦æŠ¥æ‰€éœ€çš„æ›´ç»†èŠ‚ç»“æ„æ–¹é¢çš„è¡¨ç°æ›´ä¸ºæ‚mixã€‚<details>
<summary>Abstract</summary>
There has been huge recent interest in the potential of making operational weather forecasts using machine learning techniques. As they become a part of the weather forecasting toolbox, there is a pressing need to understand how well current machine learning models can simulate high-impactweather events. We compare forecasts of Storm Ciar\'an, a European windstorm that caused sixteen deaths and extensive damage in Northern Europe, made by machine learning and numericalweather prediction models. The four machine learning models considered (FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts that accurately capture the synoptic-scale structure of the cyclone including the position of the cloud head, shape of the warm sector and location of warm conveyor belt jet, and the large-scale dynamical drivers important for the rapid storm development such as the position of the storm relative to the upper-level jet exit. However, their ability to resolve the more detailed structures important for issuing weather warnings is more mixed. All of the machine learning models underestimate the peak amplitude of winds associated with the storm, only some machine learning models resolve the warm core seclusion and none of the machine learning models capture the sharp bent-back warm frontal gradient. Our study shows there is a great deal about the performance and properties of machine learning weather forecasts that can be derived from case studies of high-impact weather events such as Storm Ciar\'an.
</details>
<details>
<summary>æ‘˜è¦</summary>
æœ‰å¾ˆå¤§çš„ç°ä»£ Ğ¸Ğ½Ñ‚ĞµÑ€ĞµÑåœ¨ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œæ“ä½œå¤©æ°”é¢„æŠ¥ã€‚éšç€å®ƒä»¬æˆä¸ºå¤©æ°”é¢„æŠ¥å·¥å…·ç®±çš„ä¸€éƒ¨åˆ†ï¼Œæœ‰ä¸€ä¸ªæ€¥éœ€è¦ç†è§£ç°æœ‰çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯å¦å¯ä»¥æ­£ç¡®åœ°é¢„æµ‹é«˜å½±å“å¤©æ°”äº‹ä»¶ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†ç”±æœºå™¨å­¦ä¹ å’Œæ•°å€¼å¤©æ°”é¢„æŠ¥æ¨¡å‹ç”Ÿæˆçš„é£“é£æ©æˆˆå°”ï¼ˆStorm CiarÃ¡nï¼‰çš„é¢„æµ‹ï¼ŒåŒ…æ‹¬äº‘å¤´ä½ç½®ã€æš–å¸¦å½¢çŠ¶å’Œæš–å¸¦å–·æµçš„ä½ç½®ï¼Œä»¥åŠé£“é£å‘å±•ä¸­çš„å¤§æ°”åŠ¨åŠ›é©±åŠ¨å› ç´ ã€‚ç„¶è€Œï¼Œå®ƒä»¬åœ¨å‘å¸ƒå¤©æ°”è­¦æŠ¥æ—¶çš„è¯¦ç»†ç»“æ„æ˜¯æ›´åŠ æ··ä¹±ã€‚æ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹éƒ½ä½ä¼°äº†é£“é£ç›¸å…³çš„é£æš´æ½®æŒ¯å¹…ï¼Œåªæœ‰ä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹è§£é‡Šæš–æ ¸å­¤ç«‹ï¼Œè€Œ none of them  capture the sharp bent-back warm frontal gradientã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯ä»¥ä»é«˜å½±å“å¤©æ°”äº‹ä»¶å¦‚é£“é£æ©æˆˆå°”çš„æ¡ˆä¾‹ç ”ç©¶ä¸­è·å¾—è®¸å¤šæœ‰å…³æœºå™¨å­¦ä¹ å¤©æ°”é¢„æŠ¥æ€§èƒ½å’Œç‰¹æ€§çš„ä¿¡æ¯ã€‚
</details></li>
</ul>
<hr>
<h2 id="What-Machine-Learning-Can-Do-for-Focusing-Aerogel-Detectors"><a href="#What-Machine-Learning-Can-Do-for-Focusing-Aerogel-Detectors" class="headerlink" title="What Machine Learning Can Do for Focusing Aerogel Detectors"></a>What Machine Learning Can Do for Focusing Aerogel Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02652">http://arxiv.org/abs/2312.02652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Foma Shipilov, Alexander Barnyakov, Vladimir Bobrovnikov, Sergey Kononov, Fedor Ratnikov</li>
<li>for: è¿™é¡¹ç ”ç©¶ç”¨äºæé«˜Super Charm-Tauå·¥å‚å®éªŒä¸­çš„ç²’å­è¯†åˆ«ç‡ã€‚</li>
<li>methods: è¿™é¡¹ç ”ç©¶ä½¿ç”¨äº†è®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„å¤šç§æªæ–½æ¥ç­›é€‰ä¿¡å·å°„å‡»ã€‚</li>
<li>results: è¿™äº›æªæ–½å¯ä»¥æœ‰æ•ˆåœ°å‡å°‘æ•°æ®æµé‡å’Œæé«˜ç²’å­é€Ÿåº¦åˆ†è¾¨ç‡ã€‚<details>
<summary>Abstract</summary>
Particle identification at the Super Charm-Tau factory experiment will be provided by a Focusing Aerogel Ring Imaging CHerenkov detector (FARICH). The specifics of detector location make proper cooling difficult, therefore a significant number of ambient background hits are captured. They must be mitigated to reduce the data flow and improve particle velocity resolution. In this work we present several approaches to filtering signal hits, inspired by machine learning techniques from computer vision.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¶… charm-tau å®éªŒå®¤ä¸­çš„ç²’å­è¯†åˆ«å°†ç”±ç„¦ç‚¹å¼æ°”æ³¡å›¾åƒæ¶²æ€æ¶²å‡èšå™¨ï¼ˆFARICHï¼‰æä¾›ã€‚å¯Ÿçœ‹å™¨çš„å…·ä½“ä½ç½®ä½¿å¾—æ­£ç¡®å†·å´å—åˆ°é™åˆ¶ï¼Œå› æ­¤åœ¨æ•æ‰å¤§é‡çš„ ambient èƒŒæ™¯å°„å‡»ä¸­æ•æ‰åˆ°äº†è®¸å¤šå°„å‡»ã€‚ä¸ºäº†å‡å°‘æ•°æ®æµé‡å¹¶æé«˜ç²’å­è¿åŠ¨è§£æç²¾åº¦ï¼Œåœ¨è¿™ç§å·¥ä½œä¸­æˆ‘ä»¬æå‡ºäº†ä¸€äº›åŸºäºè®¡ç®—æœºè§†è§‰æŠ€æœ¯çš„ç­›é€‰ä¿¡å·å°„å‡»æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="A-Q-learning-approach-to-the-continuous-control-problem-of-robot-inverted-pendulum-balancing"><a href="#A-Q-learning-approach-to-the-continuous-control-problem-of-robot-inverted-pendulum-balancing" class="headerlink" title="A Q-learning approach to the continuous control problem of robot inverted pendulum balancing"></a>A Q-learning approach to the continuous control problem of robot inverted pendulum balancing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02649">http://arxiv.org/abs/2312.02649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Safeea, Pedro Neto</li>
<li>for: è¿™ä¸ªç ”ç©¶æ˜¯ç”¨äºè¯„ä¼°æŠ½è±¡åŠ¨ä½œç©ºé—´å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼ˆQå­¦ä¹ ï¼‰åœ¨Robotå€’ç«‹æ‹æ†å¹³è¡¡æ§åˆ¶ä¸­çš„åº”ç”¨ã€‚</li>
<li>methods: è¿™ç§æ–¹æ³•ä½¿ç”¨äº†åœ¨å®é™…ç³»ç»Ÿä¸Šè¿›è¡Œå­¦ä¹ é˜¶æ®µçš„æ•°æ®æ‹Ÿåˆï¼Œä»¥åŠ é€Ÿå­¦ä¹ è¿‡ç¨‹å’Œç¼“è§£å®é™…ç³»ç»Ÿä¸Šçš„æŠ€æœ¯å›°éš¾ã€‚</li>
<li>results: è¯¥æ–¹æ³•åœ¨å®é™…ç³»ç»Ÿä¸ŠæˆåŠŸåº”ç”¨ï¼Œå¹¶åœ¨ä¸€ä¸ªçœŸå®ä¸–ç•ŒRobotä¸Šå­¦ä¹ å¹³è¡¡å€’ç«‹æ‹æ†ã€‚è¿™ä¸ªç ”ç©¶ä¹Ÿè¯æ˜äº†åœ¨å®é™…ä¸–ç•Œä¸­ä½¿ç”¨æŠ½è±¡åŠ¨ä½œç©ºé—´ç®—æ³•æ§åˆ¶è¿ç»­åŠ¨ä½œçš„é‡è¦æ€§ï¼Œå¹¶ä¸”ç”¨äºåŠ é€Ÿå­¦ä¹ è¿‡ç¨‹ã€‚<details>
<summary>Abstract</summary>
This study evaluates the application of a discrete action space reinforcement learning method (Q-learning) to the continuous control problem of robot inverted pendulum balancing. To speed up the learning process and to overcome technical difficulties related to the direct learning on the real robotic system, the learning phase is performed in simulation environment. A mathematical model of the system dynamics is implemented, deduced by curve fitting on data acquired from the real system. The proposed approach demonstrated feasible, featuring its application on a real world robot that learned to balance an inverted pendulum. This study also reinforces and demonstrates the importance of an accurate representation of the physical world in simulation to achieve a more efficient implementation of reinforcement learning algorithms in real world, even when using a discrete action space algorithm to control a continuous action.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translation notes:* "discrete action space" is translated as "ç¦»æ•£åŠ¨ä½œç©ºé—´" (liÃ¡n chuÄn dÃ²ng yÃ o kÅng jÃ¬)* "continuous control" is translated as "è¿ç»­æ§åˆ¶" (liÃ¡n xÃ¹ kÃ²ng zhÃ¬)* "inverted pendulum balancing" is translated as "å€’ç«‹æ‚¬æŒ‚å¹³è¡¡" (dÃ o zhÃ­ xiÃ ng guÄ« pÃ­ng yÇng)* "real world robot" is translated as "çœŸå®ä¸–ç•Œæœºå™¨äºº" (zhÄ“n shÃ­ shÃ¬ jiÃ¨ yÃ¬ jÄ« rÃ©n)* "simulation environment" is translated as "æ¨¡æ‹Ÿç¯å¢ƒ" (mÃ³ xiÇo huÃ¡n jÃ¬)* "mathematical model" is translated as "æ•°å­¦æ¨¡å‹" (shÃ¹ xuÃ© mÃ³ xiÇng)* "system dynamics" is translated as "ç³»ç»ŸåŠ¨æ€" (xiÃ ng tÇ’ng dÃ²ng dÃ i)* "curve fitting" is translated as "æ›²çº¿é€‚åº”" (qÅ« xiÃ n tÃ­ bÃ¨ng)* "accurate representation" is translated as "å‡†ç¡®è¡¨ç¤º" (zhÃ¨ng qiÃº biÇo gÃ²ng)
</details></li>
</ul>
<hr>
<h2 id="Rethinking-and-Simplifying-Bootstrapped-Graph-Latents"><a href="#Rethinking-and-Simplifying-Bootstrapped-Graph-Latents" class="headerlink" title="Rethinking and Simplifying Bootstrapped Graph Latents"></a>Rethinking and Simplifying Bootstrapped Graph Latents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02619">http://arxiv.org/abs/2312.02619</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zszszs25/sgcl">https://github.com/zszszs25/sgcl</a></li>
<li>paper_authors: Wangbin Sun, Jintang Li, Liang Chen, Bingzhe Wu, Yatao Bian, Zibin Zheng</li>
<li>for: æé«˜å›¾åƒè‡ªsupervised learningä¸­æ¨¡å‹çš„å¯åˆ†è§£æ€§å’Œæ€§èƒ½ã€‚</li>
<li>methods: åˆ©ç”¨ä¸¤æ¬¡å¾ªç¯è¾“å‡ºä½œä¸ºæ­£æ ·æœ¬ï¼Œå–æ¶ˆè´Ÿæ ·æœ¬ã€‚</li>
<li>results: ä¸ä¼ ç»ŸGCLæ–¹æ³•ç›¸æ¯”ï¼ŒSGCLå¯ä»¥å®ç°ç«äº‰æ€§çš„æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰æ›´å°‘çš„å‚æ•°ã€æ›´ä½çš„æ—¶é—´å’Œç©ºé—´æˆæœ¬ï¼Œä»¥åŠæ˜¾è‘—çš„é€Ÿåº¦æå‡ã€‚<details>
<summary>Abstract</summary>
Graph contrastive learning (GCL) has emerged as a representative paradigm in graph self-supervised learning, where negative samples are commonly regarded as the key to preventing model collapse and producing distinguishable representations. Recent studies have shown that GCL without negative samples can achieve state-of-the-art performance as well as scalability improvement, with bootstrapped graph latent (BGRL) as a prominent step forward. However, BGRL relies on a complex architecture to maintain the ability to scatter representations, and the underlying mechanisms enabling the success remain largely unexplored. In this paper, we introduce an instance-level decorrelation perspective to tackle the aforementioned issue and leverage it as a springboard to reveal the potential unnecessary model complexity within BGRL. Based on our findings, we present SGCL, a simple yet effective GCL framework that utilizes the outputs from two consecutive iterations as positive pairs, eliminating the negative samples. SGCL only requires a single graph augmentation and a single graph encoder without additional parameters. Extensive experiments conducted on various graph benchmarks demonstrate that SGCL can achieve competitive performance with fewer parameters, lower time and space costs, and significant convergence speedup.
</details>
<details>
<summary>æ‘˜è¦</summary>
ã€ŠGRAPH CONTRASTIVE LEARNINGï¼ˆGCLï¼‰åœ¨å›¾è‡ªåŠ©å­¦ä¹ ä¸­å·²æˆä¸ºä¸€ç§ä»£è¡¨æ€§çš„ paradigmï¼Œ negative samples é€šå¸¸è¢«è§†ä¸ºé˜²æ­¢æ¨¡å‹å¡Œç¼©å’Œç”Ÿæˆ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒçš„è¡¨ç¤ºçš„å…³é”®ã€‚  howeverï¼Œ recent studies have shown that GCL without negative samples can achieve state-of-the-art performance as well as scalability improvement, with bootstrapped graph latentï¼ˆBGRLï¼‰as a prominent step forwardã€‚ç„¶è€Œï¼ŒBGRL ä¾èµ–äºå¤æ‚çš„æ¶æ„æ¥ç»´æŠ¤èƒ½å¤Ÿæ•£å°„è¡¨ç¤ºçš„èƒ½åŠ›ï¼Œè€Œä¸‹é¢ mechanisms ä½¿å¾—æˆåŠŸ remain largely unexploredã€‚ åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å®ä¾‹çº§åˆ«çš„decorrelation perspectivesæ¥è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå¹¶åˆ©ç”¨å…¶ä¸ºspringboard æ¢ç´¢BGRL ä¸­å¯èƒ½å­˜åœ¨çš„ä¸å¿…è¦çš„æ¨¡å‹å¤æ‚åº¦ã€‚æ ¹æ®æˆ‘ä»¬çš„å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†SGCLï¼Œä¸€ç§ç®€å• yet effective GCL frameworkï¼Œåˆ©ç”¨ä¸¤ä¸ªè¿ç»­çš„è¿­ä»£ outputsä½œä¸ºæ­£ä¾‹å¯¹ã€‚SGCL ä»…éœ€è¦ä¸€ä¸ªå›¾åƒå¢å¼ºå’Œä¸€ä¸ªå›¾åƒç¼–ç å™¨ï¼Œæ²¡æœ‰é¢å¤–å‚æ•°ã€‚åœ¨å¤šç§å›¾ benchmarks ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œ demonstrate that SGCL å¯ä»¥ Ğ´Ğ¾ÑÑ‚Ğ¸åˆ°ä¸ fewer parameters, lower time and space costs, and significant convergence speedup çš„ç«äº‰æ€§æ€§èƒ½ã€‚
</details></li>
</ul>
<hr>
<h2 id="Privacy-Aware-Data-Acquisition-under-Data-Similarity-in-Regression-Markets"><a href="#Privacy-Aware-Data-Acquisition-under-Data-Similarity-in-Regression-Markets" class="headerlink" title="Privacy-Aware Data Acquisition under Data Similarity in Regression Markets"></a>Privacy-Aware Data Acquisition under Data Similarity in Regression Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02611">http://arxiv.org/abs/2312.02611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashi Raj Pandey, Pierre Pinson, Petar Popovski</li>
<li>for: è¯¥è®ºæ–‡æ—¨åœ¨è®¾è®¡æ•°æ®å¸‚åœºï¼Œè€ƒè™‘æ•°æ®æ‰€æœ‰è€…çš„éšç§åå¥½å’Œæ•°æ®ç›¸ä¼¼æ€§çš„å½±å“ã€‚</li>
<li>methods: è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæœ¬åœ°å‡åˆ†éšç§åè®®çš„æŸ¥è¯¢-å›å¤åè®®ï¼Œç”¨äºå®ç°ä¸¤æ–¹æ•°æ®äº¤æ¢æœºåˆ¶ã€‚</li>
<li>results: è¯¥è®ºæ–‡é€šè¿‡åˆ†æå‚ä¸è€…ä¹‹é—´çš„ç­–ç•¥äº¤äº’ï¼Œåˆ†æäº†éšç§æ„è¯†çš„å½±å“äºä»·æ ¼å’Œéšç§å› å­ã€‚  Additionally, the paper shows that data similarity affects market participation and traded data value.<details>
<summary>Abstract</summary>
Data markets facilitate decentralized data exchange for applications such as prediction, learning, or inference. The design of these markets is challenged by varying privacy preferences as well as data similarity among data owners. Related works have often overlooked how data similarity impacts pricing and data value through statistical information leakage. We demonstrate that data similarity and privacy preferences are integral to market design and propose a query-response protocol using local differential privacy for a two-party data acquisition mechanism. In our regression data market model, we analyze strategic interactions between privacy-aware owners and the learner as a Stackelberg game over the asked price and privacy factor. Finally, we numerically evaluate how data similarity affects market participation and traded data value.
</details>
<details>
<summary>æ‘˜è¦</summary>
â€œæ•°æ®å¸‚åœºä¿ƒè¿›äº†åˆ†å¸ƒå¼æ•°æ®äº¤æ¢ï¼Œç”¨äºé¢„æµ‹ã€å­¦ä¹ æˆ–æ¨ç†åº”ç”¨ã€‚å¸‚åœºè®¾è®¡é¢ä¸´ç€æ•°æ®æ‰€æœ‰è€…çš„éšç§åå¥½ä»¥åŠæ•°æ®ä¹‹é—´çš„ç›¸ä¼¼æ€§é—®é¢˜ã€‚ç›¸å…³çš„ç ”ç©¶ç»å¸¸å¿½ç•¥äº†æ•°æ®ç›¸ä¼¼æ€§å¯¹ä»·æ ¼å’Œæ•°æ®ä»·å€¼çš„å½±å“ã€‚æˆ‘ä»¬è¯æ˜äº†æ•°æ®ç›¸ä¼¼æ€§å’Œéšç§åå¥½æ˜¯å¸‚åœºè®¾è®¡çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå¹¶æå‡ºäº†åŸºäºæœ¬åœ°å‡åŒ€éšç§åè®®çš„æŸ¥è¯¢-å“åº”åè®® Ğ´Ğ»Ñä¸¤æ–¹æ•°æ®è·å–æœºåˆ¶ã€‚åœ¨æˆ‘ä»¬çš„å›å½’æ•°æ®å¸‚åœºæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬åˆ†æäº†éšç§æ„è¯†çš„æ‰€æœ‰è€…å’Œå­¦ä¹ è€…ä¹‹é—´çš„æˆ˜ç•¥äº¤äº’ï¼ŒåŒ…æ‹¬ä»·æ ¼å’Œéšç§å› ç´ ã€‚æœ€åï¼Œæˆ‘ä»¬æ•°å­—è¯„ä¼°äº†æ•°æ®ç›¸ä¼¼æ€§å¯¹å¸‚åœºå‚ä¸åº¦å’Œäº¤æ˜“æ•°æ®ä»·å€¼çš„å½±å“ã€‚â€Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know and I'll be happy to provide it.
</details></li>
</ul>
<hr>
<h2 id="TSVR-Twin-support-vector-regression-with-privileged-information"><a href="#TSVR-Twin-support-vector-regression-with-privileged-information" class="headerlink" title="TSVR+: Twin support vector regression with privileged information"></a>TSVR+: Twin support vector regression with privileged information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02596">http://arxiv.org/abs/2312.02596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anuradha Kumari, M. Tanveer</li>
<li>for: æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å’Œå‡†ç¡®æ€§</li>
<li>methods:  combining twin support vector regression (TSVR) with learning using privileged information (LUPI) and using successive overrelaxation (SOR) technique to solve the optimization problem</li>
<li>results: åœ¨ UCIã€è‚¡ç¥¨å’Œæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œäº†æ•°å€¼å®éªŒï¼Œå¹¶è¯æ˜äº†ææ¡ˆçš„æ¨¡å‹çš„ä¼˜è¶Šæ€§<details>
<summary>Abstract</summary>
In the realm of machine learning, the data may contain additional attributes, known as privileged information (PI). The main purpose of PI is to assist in the training of the model and then utilize the acquired knowledge to make predictions for unseen samples. Support vector regression (SVR) is an effective regression model, however, it has a low learning speed due to solving a convex quadratic problem (QP) subject to a pair of constraints. In contrast, twin support vector regression (TSVR) is more efficient than SVR as it solves two QPs each subject to one set of constraints. However, TSVR and its variants are trained only on regular features and do not use privileged features for training. To fill this gap, we introduce a fusion of TSVR with learning using privileged information (LUPI) and propose a novel approach called twin support vector regression with privileged information (TSVR+). The regularization terms in the proposed TSVR+ capture the essence of statistical learning theory and implement the structural risk minimization principle. We use the successive overrelaxation (SOR) technique to solve the optimization problem of the proposed TSVR+, which enhances the training efficiency. As far as our knowledge extends, the integration of the LUPI concept into twin variants of regression models is a novel advancement. The numerical experiments conducted on UCI, stock and time series data collectively demonstrate the superiority of the proposed model.
</details>
<details>
<summary>æ‘˜è¦</summary>
åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ï¼Œæ•°æ®å¯èƒ½åŒ…å«é™„åŠ çš„ç‰¹å¾ï¼Œç§°ä¸ºç‰¹æƒä¿¡æ¯ï¼ˆPIï¼‰ã€‚PIçš„ä¸»è¦ç›®çš„æ˜¯å¸®åŠ©æ¨¡å‹è®­ç»ƒå¹¶ä½¿ç”¨æ‰€è·çŸ¥ledgeæ¥é¢„æµ‹æœªç»è§è¿‡çš„æ ·æœ¬ã€‚æ”¯æŒå‘é‡å›å½’ï¼ˆSVRï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„å›å½’æ¨¡å‹ï¼Œä½†å®ƒçš„å­¦ä¹ é€Ÿåº¦è¾ƒä½ï¼Œå› ä¸ºå®ƒè§£å†³äº†ä¸€ä¸ªå‡ ä½• quadratic problemï¼ˆQPï¼‰ï¼Œå¹¶ä¸”å—åˆ°ä¸€å¯¹çº¦æŸçš„é™åˆ¶ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒæ”¯æŒå‘é‡å›å½’ï¼ˆTSVRï¼‰æ¯”SVRæ›´é«˜æ•ˆï¼Œå› ä¸ºå®ƒè§£å†³äº†ä¸¤ä¸ªQPï¼Œæ¯ä¸ªQPå—åˆ°ä¸€ä¸ªé›†åˆçº¦æŸã€‚ç„¶è€Œï¼ŒTSVRå’Œå…¶å˜ç§åªåœ¨å¸¸è§ç‰¹å¾ä¸Šè®­ç»ƒï¼Œå¹¶ä¸ä½¿ç”¨ç‰¹æƒç‰¹å¾è¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†å¡«è¡¥è¿™ä¸ªç©ºéš™ï¼Œæˆ‘ä»¬æå‡ºäº†å°†TSVRä¸ç‰¹æƒä¿¡æ¯å­¦ä¹ ï¼ˆLUPIï¼‰èåˆï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•called twin support vector regression with privileged informationï¼ˆTSVR+ï¼‰ã€‚TSVR+çš„æ­£åˆ™åŒ–é¡¹æ•æ‰äº†ç»Ÿè®¡å­¦å­¦ä¹ ç†è®ºçš„æ ¸å¿ƒï¼Œå¹¶å®ç°äº†ç»“æ„é£é™©æœ€å°åŒ–åŸåˆ™ã€‚æˆ‘ä»¬ä½¿ç”¨successive overrelaxationï¼ˆSORï¼‰æŠ€æœ¯è§£å†³TSVR+ä¼˜åŒ–é—®é¢˜ï¼Œè¿™æœ‰åŠ©äºæé«˜è®­ç»ƒæ•ˆç‡ã€‚åœ¨æˆ‘ä»¬æ‰€çŸ¥é“çš„èŒƒå›´å†…ï¼Œå°†LUPIæ¦‚å¿µintegrated into twin variants of regression modelsæ˜¯ä¸€ç§æ–°çš„è¿›å±•ã€‚åœ¨UCICã€è‚¡ç¥¨å’Œæ—¶é—´åºåˆ—æ•°æ®ä¸Šè¿›è¡Œçš„æ•°å­—å®éªŒç»“æœè¡¨æ˜ï¼Œæè®®çš„æ¨¡å‹å…·æœ‰superiorityã€‚
</details></li>
</ul>
<hr>
<h2 id="FRAPPE-A-Post-Processing-Framework-for-Group-Fairness-Regularization"><a href="#FRAPPE-A-Post-Processing-Framework-for-Group-Fairness-Regularization" class="headerlink" title="FRAPPÃ‰: A Post-Processing Framework for Group Fairness Regularization"></a>FRAPPÃ‰: A Post-Processing Framework for Group Fairness Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02592">http://arxiv.org/abs/2312.02592</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research">https://github.com/google-research/google-research</a></li>
<li>paper_authors: Alexandru Å¢ifrea, Preethi Lahoti, Ben Packer, Yoni Halpern, Ahmad Beirami, Flavien Prost</li>
<li>for: æé«˜ç¾¤ä½“å…¬å¹³æ€§ï¼Œå‡å°‘åè¢‹æ€§å’Œæ¬ºè¯ˆæ€§</li>
<li>methods: å°†ä»»ä½•å†…éƒ¨å¤„ç†æ–¹æ³•è½¬æ¢ä¸ºåå¤„ç†æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨ç½š penalty å‡½æ•°æ¥è§£å†³æ•æ„Ÿç‰¹å¾çŸ¥ledgeçš„é—®é¢˜</li>
<li>results: ç»è¿‡æ‰¹å¤„ç†å¯ä»¥è¾¾åˆ°ä¸å†…éƒ¨å¤„ç†æ–¹æ³•ç›¸åŒçš„å…¬å¹³æ€§-é”™è¯¯è´Ÿæ‹…åè®®ï¼Œå¹¶ä¸”åœ¨å®é™…æ•°æ®ä¸Šè¡¨ç°å‡ºè¾ƒå¥½çš„æ€§èƒ½<details>
<summary>Abstract</summary>
Post-processing mitigation techniques for group fairness generally adjust the decision threshold of a base model in order to improve fairness. Methods in this family exhibit several advantages that make them appealing in practice: post-processing requires no access to the model training pipeline, is agnostic to the base model architecture, and offers a reduced computation cost compared to in-processing. Despite these benefits, existing methods face other challenges that limit their applicability: they require knowledge of the sensitive attributes at inference time and are oftentimes outperformed by in-processing. In this paper, we propose a general framework to transform any in-processing method with a penalized objective into a post-processing procedure. The resulting method is specifically designed to overcome the aforementioned shortcomings of prior post-processing approaches. Furthermore, we show theoretically and through extensive experiments on real-world data that the resulting post-processing method matches or even surpasses the fairness-error trade-off offered by the in-processing counterpart.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¯¹äºç¾¤ä½“å…¬å¹³æ€§ï¼Œåå¤„ç†mitigationæŠ€æœ¯é€šå¸¸æ˜¯è°ƒæ•´åŸºæœ¬æ¨¡å‹çš„å†³ç­–é˜ˆå€¼ï¼Œä»¥æ”¹è¿›å…¬å¹³æ€§ã€‚è¿™äº›æ–¹æ³•å…·æœ‰è®¸å¤šä¼˜ç‚¹ï¼Œä½¿å…¶åœ¨å®è·µä¸­å¸å¼•äººï¼šåå¤„ç†ä¸éœ€è¦å¯¹æ¨¡å‹è®­ç»ƒç®¡é“æœ‰ä»»ä½•Accessï¼Œä¸å—æ¨¡å‹æ¶æ„çš„é™åˆ¶ï¼Œè®¡ç®—æˆæœ¬è¾ƒä½ã€‚ç„¶è€Œï¼Œç°æœ‰æ–¹æ³•å­˜åœ¨å…¶ä»–æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬éœ€è¦æŒæ¡æ•æ„Ÿç‰¹å¾çš„çŸ¥è¯†åœ¨æ¨ç†æ—¶ï¼Œå¹¶ä¸”ç»å¸¸è¢«å†…éƒ¨å¤„ç†æ–¹æ³•æ‰€è¶…è¶Šã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºä¸€ç§æ™®é€‚çš„æ¡†æ¶ï¼Œå¯ä»¥å°†ä»»ä½•å†…éƒ¨å¤„ç†æ–¹æ³•è½¬åŒ–ä¸ºåå¤„ç†è¿‡ç¨‹ã€‚å¾—åˆ°çš„æ–¹æ³•èƒ½å¤Ÿè¶…è¶Šå…ˆå‰åå¤„ç†æ–¹æ³•çš„ç¼ºç‚¹ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨ç†è®ºå’Œå®éªŒä¸­å±•ç¤ºäº†è¿™ç§åå¤„ç†æ–¹æ³•ä¸å†…éƒ¨å¤„ç†æ–¹æ³•çš„å…¬å¹³æ€§-é”™è¯¯è´Ÿæ‹…trade-offåŒ¹é…æˆ–ç”šè‡³è¶…è¶Šã€‚
</details></li>
</ul>
<hr>
<h2 id="On-Optimal-Consistency-Robustness-Trade-Off-for-Learning-Augmented-Multi-Option-Ski-Rental"><a href="#On-Optimal-Consistency-Robustness-Trade-Off-for-Learning-Augmented-Multi-Option-Ski-Rental" class="headerlink" title="On Optimal Consistency-Robustness Trade-Off for Learning-Augmented Multi-Option Ski Rental"></a>On Optimal Consistency-Robustness Trade-Off for Learning-Augmented Multi-Option Ski Rental</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02547">http://arxiv.org/abs/2312.02547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongho Shin, Changyeol Lee, Hyung-Chan An</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å“ªäº›æ–¹æ³•ï¼Ÿ</li>
<li>results: è¿™ä¸ªè®ºæ–‡è·å¾—äº†ä»€ä¹ˆç»“æœï¼ŸHere are the answers in Simplified Chinese:</li>
<li>for: è¿™ä¸ªè®ºæ–‡ä¸»è¦é’ˆå¯¹çš„é—®é¢˜æ˜¯å­¦ä¹ å¢å¼ºçš„å¤šé€‰ ski ç§Ÿèµé—®é¢˜ï¼Œå®ƒå°†ç»å…¸ ski ç§Ÿèµé—®é¢˜æ‰©å±•åˆ°äº†ä¸¤ä¸ªæ–¹é¢ï¼šé¦–å…ˆï¼Œç®—æ³•è¢«æä¾›äº†é¢„æµ‹å¤©æ°”æƒ…å†µçš„æ•°æ®ï¼Œå…¶æ¬¡ï¼Œç§Ÿèµé€‰é¡¹ç°åœ¨åŒ…æ‹¬å¤šä¸ªç§ŸèµæœŸå’Œä»·æ ¼é€‰æ‹©ã€‚</li>
<li>methods: è¿™ä¸ªè®ºæ–‡ä½¿ç”¨äº†å­¦ä¹ å¢å¼ºçš„æ–¹æ³•ï¼Œå¹¶ä¸”å¯¹äºä¸åŒçš„ç§ŸèµæœŸå’Œä»·æ ¼ï¼Œæä¾›äº†å¤šç§ä¸åŒçš„ç­–ç•¥ã€‚</li>
<li>results: è¿™ä¸ªè®ºæ–‡æå‡ºäº†ä¸€ä¸ªæœ€ä½³çš„ç®—æ³•ï¼Œå®ƒå¯ä»¥ä¸å·²çŸ¥çš„ä¸‹ç•ŒåŒ¹é…ï¼Œå¹¶ä¸”å¯¹äºéšæœºåŒ–ç­–ç•¥ï¼Œæä¾›äº†é¦–æ¬¡çš„ä¸‹ç•Œï¼Œå¹¶ä¸”æå‡ºäº†ä¸€ä¸ªæ”¹è¿›çš„éšæœºåŒ–ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ç¨³å®šæ€§å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—äº†æœ€ä½³çš„å¹³è¡¡ã€‚<details>
<summary>Abstract</summary>
The learning-augmented multi-option ski rental problem generalizes the classical ski rental problem in two ways: the algorithm is provided with a prediction on the number of days we can ski, and the ski rental options now come with a variety of rental periods and prices to choose from, unlike the classical two-option setting. Subsequent to the initial study of the multi-option ski rental problem (without learning augmentation) due to Zhang, Poon, and Xu, significant progress has been made for this problem recently in particular. The problem is very well understood when we relinquish one of the two generalizations -- for the learning-augmented classical ski rental problem, algorithms giving best-possible trade-off between consistency and robustness exist; for the multi-option ski rental problem without learning augmentation, deterministic/randomized algorithms giving the best-possible competitiveness have been found. However, in presence of both generalizations, there remained a huge gap between the algorithmic and impossibility results. In fact, for randomized algorithms, we did not have any nontrivial lower bounds on the consistency-robustness trade-off before.   This paper bridges this gap for both deterministic and randomized algorithms. For deterministic algorithms, we present a best-possible algorithm that completely matches the known lower bound. For randomized algorithms, we show the first nontrivial lower bound on the consistency-robustness trade-off, and also present an improved randomized algorithm. Our algorithm matches our lower bound on robustness within a factor of e/2 when the consistency is at most 1.086.
</details>
<details>
<summary>æ‘˜è¦</summary>
Ski ç§Ÿèµé—®é¢˜å¯ä»¥åˆ†ä¸ºä¸¤ç§é€šç”¨æƒ…å†µï¼šä¸€æ˜¯å­¦ä¹ æ‰©å±• Ski ç§Ÿèµé—®é¢˜ï¼Œå¦ä¸€æ˜¯å¤šé€‰ Ski ç§Ÿèµé—®é¢˜ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›é¢„æµ‹ Ski ç§Ÿèµå¤©æ•°çš„æ•°æ®ï¼Œä»¥åŠä¸åŒçš„ç§Ÿèµæ—¶é—´å’Œä»·æ ¼é€‰æ‹©ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸åŒçš„ç®—æ³•æ¥å¯»æ‰¾æœ€ä½³çš„ç§Ÿèµè§£å†³æ–¹æ¡ˆã€‚åœ¨è¿‡å»çš„ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¯¹ Ski ç§Ÿèµé—®é¢˜è¿›è¡Œäº†è®¸å¤šç ”ç©¶ï¼Œä½†æ˜¯è¿™äº›ç ”ç©¶éƒ½æ˜¯åœ¨å•ä¸€é€‰æ‹©æƒ…å†µä¸‹è¿›è¡Œçš„ã€‚åœ¨è¿™äº›ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€äº›ç®—æ³•å¯ä»¥åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹å¯»æ‰¾æœ€ä½³çš„ç§Ÿèµè§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸¤ç§æƒ…å†µç»„åˆèµ·æ¥ï¼Œå¯¹ Ski ç§Ÿèµé—®é¢˜è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå®Œç¾çš„ç®—æ³•ï¼Œå¯ä»¥åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹å¯»æ‰¾æœ€ä½³çš„ç§Ÿèµè§£å†³æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ä¸ªæ–°çš„ä¸‹ç•Œï¼Œå¯ä»¥ç”¨äºè¯„ä¼°è¿™ä¸¤ç§æƒ…å†µä¸‹çš„ç§Ÿèµè§£å†³æ–¹æ¡ˆã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬è¿˜è¯¦ç»†ä»‹ç»äº†ä¸€äº›å…³äº Ski ç§Ÿèµé—®é¢˜çš„æ¦‚å¿µå’Œç†è®ºã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡è®ºæ–‡å¯ä»¥å¸®åŠ©æ›´å¤šçš„äººäº†è§£è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”å¸®åŠ©ä»–ä»¬å¯»æ‰¾æ›´å¥½çš„ç§Ÿèµè§£å†³æ–¹æ¡ˆã€‚
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-Locality-in-Spin-States-and-Forced-Moves-for-Optimizations"><a href="#Characterization-of-Locality-in-Spin-States-and-Forced-Moves-for-Optimizations" class="headerlink" title="Characterization of Locality in Spin States and Forced Moves for Optimizations"></a>Characterization of Locality in Spin States and Forced Moves for Optimizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02544">http://arxiv.org/abs/2312.02544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoshiki Sato, Makiko Konoshima, Hirotaka Tamura, Jun Ohkubo</li>
<li>for: è§£å†³ combinatorial optimization é—®é¢˜ä¸­çš„æœ¬åœ°æå°ç‚¹é—®é¢˜</li>
<li>methods: åˆ©ç”¨ç‰¹æ®Šç¡¬ä»¶å’Œä¸€ç§æ–°çš„ç®—æ³•æŠ€æœ¯</li>
<li>results: æå‡ºä¸€ç§é«˜æ•ˆçš„ã€æ— æ‹’ç»çš„ç®—æ³•ï¼Œå¯ä»¥å¿«é€Ÿç¦»å¼€æœ¬åœ°æå°ç‚¹<details>
<summary>Abstract</summary>
Ising formulations are widely utilized to solve combinatorial optimization problems, and a variety of quantum or semiconductor-based hardware has recently been made available. In combinatorial optimization problems, the existence of local minima in energy landscapes is problematic to use to seek the global minimum. We note that the aim of the optimization is not to obtain exact samplings from the Boltzmann distribution, and there is thus no need to satisfy detailed balance conditions. In light of this fact, we develop an algorithm to get out of the local minima efficiently while it does not yield the exact samplings. For this purpose, we utilize a feature that characterizes locality in the current state, which is easy to obtain with a type of specialized hardware. Furthermore, as the proposed algorithm is based on a rejection-free algorithm, the computational cost is low. In this work, after presenting the details of the proposed algorithm, we report the results of numerical experiments that demonstrate the effectiveness of the proposed feature and algorithm.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¼Šé¡¿å½¢å¼ulationæ˜¯å¹¿æ³›åº”ç”¨äºè§£å†³ combinatorial optimization é—®é¢˜ï¼Œè€Œç°åœ¨ä¸€äº›é‡å­æˆ–åŠå¯¼ä½“åŸºç¡€è®¾æ–½ä¹Ÿå·²ç»æä¾›ã€‚åœ¨ combinatorial optimization é—®é¢˜ä¸­ï¼Œå½“åœ°ç‚¹æœ€å°å€¼å­˜åœ¨é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬ä½¿å¾—å¯»æ‰¾å…¨å±€æœ€å°å€¼å˜å¾—å›°éš¾ã€‚æˆ‘ä»¬æ³¨æ„åˆ°ä¼˜åŒ–çš„ç›®æ ‡ä¸æ˜¯è·å–ç²¾ç¡®çš„æŠ½æ ·ï¼Œå› æ­¤ä¸éœ€è¦æ»¡è¶³ç»†èŠ‚å¹³è¡¡æ¡ä»¶ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§èƒ½å¤Ÿå¿«é€Ÿç¦»å¼€æœ¬åœ°æœ€å°å€¼çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•ä¸éœ€è¦æ‹’ç»ä»»ä½•æ ·æœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å½“å‰çŠ¶æ€çš„æœ¬åœ°ç‰¹å¾ï¼Œè¿™æ˜¯é€šè¿‡ç‰¹æ®Šç¡¬ä»¶è·å¾—çš„æ˜“äºè·å¾—ã€‚æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬çš„ç®—æ³•åŸºäºæ‹’ç»è‡ªç”±ç®—æ³•ï¼Œè®¡ç®—æˆæœ¬è¾ƒä½ã€‚åœ¨è¿™ä¸ªå·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»æˆ‘ä»¬çš„ç®—æ³•ï¼Œå¹¶å¯¹æ•°å€¼å®éªŒç»“æœè¿›è¡ŒæŠ¥å‘Šã€‚
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-leader-laggard-cluster-synchronization-for-collective-decision-making-with-laser-network"><a href="#Asymmetric-leader-laggard-cluster-synchronization-for-collective-decision-making-with-laser-network" class="headerlink" title="Asymmetric leader-laggard cluster synchronization for collective decision-making with laser network"></a>Asymmetric leader-laggard cluster synchronization for collective decision-making with laser network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02537">http://arxiv.org/abs/2312.02537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shun Kotoku, Takatomo Mihana, AndrÃ© RÃ¶hm, Ryoichi Horisaki, Makoto Naruse</li>
<li>for: è¿™ä¸ªè®ºæ–‡æ˜¯ä¸ºäº†ç ”ç©¶å…‰å­¦åŠ é€Ÿå™¨åœ¨ä¿¡æ¯å¤„ç†ä¸­çš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡ä½¿ç”¨æ¿€å…‰ç½‘ç»œæ¥è§£å†³ç«äº‰å¤šè‡‚å¼“å…µï¼ˆCMABï¼‰é—®é¢˜ã€‚</li>
<li>methods: è¯¥è®ºæ–‡ä½¿ç”¨äº†å…‰å­¦è¿æ¥çš„æ¿€å…‰å™¨æ¥å®ç°é›†ä½“å†³ç­–ï¼Œåˆ©ç”¨æ¿€å…‰ç½‘ç»œä¸­çš„å¼‚æ­¥å’ŒåŒæ­¥åŠ¨åŠ›æ¥è§£å†³CMABé—®é¢˜ã€‚</li>
<li>results: ç ”ç©¶äººå‘˜é€šè¿‡ç¨³å®šæ€§åˆ†æå¯¹é›†ä½“å†³ç­–çš„å¿…è¦ç½‘ç»œç»“æ„è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å‘ç°äº†ç©å®¶åå¥½çš„åå¥½æ€§ï¼Œä»è€Œæ‰©å±•äº†CMABé—®é¢˜çš„åº”ç”¨èŒƒå›´ã€‚<details>
<summary>Abstract</summary>
Photonic accelerators have recently attracted soaring interest, harnessing the ultimate nature of light for information processing. Collective decision-making with a laser network, employing the chaotic and synchronous dynamics of optically interconnected lasers to address the competitive multi-armed bandit (CMAB) problem, is a highly compelling approach due to its scalability and experimental feasibility. We investigated essential network structures for collective decision-making through quantitative stability analysis. Moreover, we demonstrated the asymmetric preferences of players in the CMAB problem, extending its functionality to more practical applications. Our study highlights the capability and significance of machine learning built upon chaotic lasers and photonic devices.
</details>
<details>
<summary>æ‘˜è¦</summary>
å…‰å­¦åŠ é€Ÿå™¨æœ€è¿‘å—åˆ°äº†æé«˜çš„å…³æ³¨ï¼Œåˆ©ç”¨å…‰çš„æœ¬è´¨æ¥å¤„ç†ä¿¡æ¯ã€‚é€šè¿‡ç”¨æ¿€å…‰ç½‘ç»œå®ç°é›†ä½“å†³ç­–ï¼Œåˆ©ç”¨æ¿€å…‰ç›¸äº’è¿æ¥çš„å¼‚æ´›åŠ¨å’ŒåŒæ­¥åŠ¨åŠ›å­¦æ€§èƒ½æ¥è§£å†³å¤šè‡‚æŠ•æœºé—®é¢˜ï¼Œæ˜¯ä¸€ç§éå¸¸å¸å¼•äººçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå…·æœ‰æ‰©å±•æ€§å’Œå®éªŒå¯è¡Œæ€§ã€‚æˆ‘ä»¬é€šè¿‡é‡åŒ–ç¨³å®šåˆ†æ investigateäº†é›†ä½“å†³ç­–çš„é‡è¦ç½‘ç»œç»“æ„ï¼Œä»¥åŠæ‰©å±•äº†ç©å®¶åå¥½çš„æ€§è´¨ï¼Œä½¿å…¶æ›´åŠ é€‚ç”¨äºå®é™…åº”ç”¨ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¼ºè°ƒäº†åŸºäºæ¿€å…‰å’Œå…‰å­¦è®¾å¤‡çš„æœºå™¨å­¦ä¹ æŠ€æœ¯çš„å¯èƒ½æ€§å’Œé‡è¦æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Replay-based-Class-Continual-Learning-for-Online-New-Category-Anomaly-Detection-in-Additive-Manufacturing"><a href="#Pseudo-Replay-based-Class-Continual-Learning-for-Online-New-Category-Anomaly-Detection-in-Additive-Manufacturing" class="headerlink" title="Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Additive Manufacturing"></a>Pseudo Replay-based Class Continual Learning for Online New Category Anomaly Detection in Additive Manufacturing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02491">http://arxiv.org/abs/2312.02491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangyue Shi, Tianxin Xie, Chenang Liu, Yuxuan Li<br>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜ç°ä»£ç”Ÿäº§è¿‡ç¨‹ä¸­çš„è´¨é‡ç›‘æ§ï¼Œä½¿ç”¨å…ˆè¿›çš„æ„Ÿåº”å™¨å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯è¿›è¡Œæ•°æ®é©±åŠ¨çš„å®æ—¶ç›‘æ§ã€‚methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†å†…å­˜åŸºç¡€çš„ä¸æ–­å­¦ä¹ ï¼Œå¹¶é€šè¿‡å¢åŠ çº§åˆ«å­¦ä¹ å’Œæ ·æœ¬å¢åŠ çš„æ–¹æ³•æ¥è§£å†³èµ„æ–™å‚¨å­˜å®¹é‡çš„é™åˆ¶ã€‚results: å®éªŒç»“æœæ˜¾ç¤ºï¼Œææ¡ˆçš„æ–¹æ³•èƒ½å¤Ÿå®ç°é«˜è´¨é‡çš„æ•°æ®ç”Ÿæˆï¼Œå¹¶åœ¨æ–°çš„ç±»åˆ«åå·®å‡ºç°æ—¶è¿›è¡Œincremental learningï¼Œä¸éœ€è¦å‚¨å­˜æ‰€æœ‰æ•°æ®ã€‚æ­¤å¤–ï¼Œè¿™äº›æ–¹æ³•è¿˜èƒ½å¤Ÿæé«˜ç›‘æ§æ€§èƒ½ï¼Œå¹¶å¢åŠ æ¨¡å‹æ¶æ„çš„ flexibilityã€‚<details>
<summary>Abstract</summary>
The incorporation of advanced sensors and machine learning techniques has enabled modern manufacturing enterprises to perform data-driven in-situ quality monitoring based on the sensor data collected in manufacturing processes. However, one critical challenge is that newly presented defect category may manifest as the manufacturing process continues, resulting in monitoring performance deterioration of previously trained machine learning models. Hence, there is an increasing need for empowering machine learning model to learn continually. Among all continual learning methods, memory-based continual learning has the best performance but faces the constraints of data storage capacity. To address this issue, this paper develops a novel pseudo replay-based continual learning by integrating class incremental learning and oversampling-based data generation. Without storing all the data, the developed framework could generate high-quality data representing previous classes to train machine learning model incrementally when new category anomaly occurs. In addition, it could even enhance the monitoring performance since it also effectively improves the data quality. The effectiveness of the proposed framework is validated in an additive manufacturing process, which leverages supervised classification problem for anomaly detection. The experimental results show that the developed method is very promising in detecting novel anomaly while maintaining a good performance on the previous task and brings up more flexibility in model architecture.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°ä»£åˆ¶é€ ä¼ä¸šé€šè¿‡å…·æœ‰å…ˆè¿›æ„Ÿæµ‹å™¨å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯çš„æ•°æ®é©±åŠ¨ situational quality monitoring å®ç°äº†åŸºäºæ„Ÿæµ‹å™¨æ•°æ®æ”¶é›†çš„åˆ¶é€ è¿‡ç¨‹ä¸­çš„è´¨é‡ç›‘æµ‹ã€‚ç„¶è€Œï¼Œä¸€ä¸ªé‡è¦æŒ‘æˆ˜æ˜¯æ–°çš„ç¼ºé™·ç±»å‹å¯èƒ½åœ¨åˆ¶é€ è¿‡ç¨‹ç»§ç»­æ—¶å‡ºç°ï¼Œå¯¼è‡´å…ˆå‰è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç›‘æµ‹æ€§èƒ½ä¸‹é™ã€‚å› æ­¤ï¼Œæœ‰ä¸€ä¸ªå¢åŠ éœ€è¦ empowering æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œä¸æ–­å­¦ä¹ ã€‚åœ¨æ‰€æœ‰çš„ä¸æ–­å­¦ä¹ æ–¹æ³•ä¸­ï¼Œè®°å¿†åŸºæœ¬çš„ä¸æ–­å­¦ä¹ å…·æœ‰æœ€å¥½çš„è¡¨ç°ï¼Œä½†é¢ä¸´æ•°æ®å­˜å‚¨å®¹é‡çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡å¼€å‘äº†ä¸€ç§æ–°çš„ Pseudo replay-based ä¸æ–­å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡å°†ç±»å¢é‡å­¦ä¹ å’Œæ‰©sampling-based æ•°æ®ç”Ÿæˆç›¸ç»“åˆã€‚ä¸éœ€è¦å­˜å‚¨æ‰€æœ‰æ•°æ®ï¼Œå¼€å‘çš„æ¡†æ¶å¯ä»¥åœ¨æ–°ç±»å¼‚å¸¸å‡ºç°æ—¶é€æ­¥åŸ¹è®­æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æé«˜ç›‘æµ‹æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜å¯ä»¥å¢å¼ºç›‘æµ‹æ€§èƒ½ï¼Œå› ä¸ºå®ƒè¿˜å¯ä»¥æé«˜æ•°æ®è´¨é‡ã€‚æœ¬æ–‡åœ¨ä½¿ç”¨è¶…è¿‡ classification é—®é¢˜è¿›è¡Œæ‚åˆåˆ¶é€ è¿‡ç¨‹ä¸­çš„å¼‚å¸¸æ£€æµ‹ï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæå‡ºçš„æ–¹æ³•æ˜¯éå¸¸æœ‰å‰é€”çš„ï¼Œèƒ½å¤Ÿæ£€æµ‹æ–°çš„å¼‚å¸¸ï¼Œä¿æŒå¥½çš„å‰ä»»ä»»åŠ¡æ€§èƒ½ï¼Œå¹¶å¢åŠ æ¨¡å‹æ¶æ„çš„çµæ´»æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Constrained-Twin-Variational-Auto-Encoder-for-Intrusion-Detection-in-IoT-Systems"><a href="#Constrained-Twin-Variational-Auto-Encoder-for-Intrusion-Detection-in-IoT-Systems" class="headerlink" title="Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems"></a>Constrained Twin Variational Auto-Encoder for Intrusion Detection in IoT Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02490">http://arxiv.org/abs/2312.02490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Phai Vu Dinh, Quang Uy Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Son Pham Bao, Eryk Dutkiewicz</li>
<li>for: ä¿æŠ¤äº’è”ç½‘ç‰©è”ç½‘è®¾å¤‡å…å—æ¶æ„æ”»å‡»</li>
<li>methods: ä½¿ç”¨å—é™çš„åŒè´¨é‡å˜æ¢è‡ªåŠ¨ç¼–ç å™¨ï¼ˆCTVAEï¼‰å¸®åŠ©æ”»å‡»æ£€æµ‹ç³»ç»Ÿè·å¾—æ›´å¯åˆ†ç¦»å’Œä½ç»´åº¦çš„æ•°æ®è¡¨ç¤º</li>
<li>results: æ¯”å¯¹11ä¸ªæœ€å—æ¬¢è¿çš„äº’è”ç½‘ç‰©è”ç½‘æ¶æ„èœ‚ç¾æ•°æ®é›†ï¼ŒCTVAEå¯ä»¥æé«˜çº¦1%çš„å‡†ç¡®ç‡å’Œåˆ†æ•°æ¯”ï¼Œè€Œè¿è¡Œæ—¶é—´ä¸ºæ”»å‡»æ£€æµ‹ä¸‹é™è‡³2E-6ç§’ï¼Œæ¨¡å‹å¤§å°ä½äº1MBã€‚<details>
<summary>Abstract</summary>
Intrusion detection systems (IDSs) play a critical role in protecting billions of IoT devices from malicious attacks. However, the IDSs for IoT devices face inherent challenges of IoT systems, including the heterogeneity of IoT data/devices, the high dimensionality of training data, and the imbalanced data. Moreover, the deployment of IDSs on IoT systems is challenging, and sometimes impossible, due to the limited resources such as memory/storage and computing capability of typical IoT devices. To tackle these challenges, this article proposes a novel deep neural network/architecture called Constrained Twin Variational Auto-Encoder (CTVAE) that can feed classifiers of IDSs with more separable/distinguishable and lower-dimensional representation data. Additionally, in comparison to the state-of-the-art neural networks used in IDSs, CTVAE requires less memory/storage and computing power, hence making it more suitable for IoT IDS systems. Extensive experiments with the 11 most popular IoT botnet datasets show that CTVAE can boost around 1% in terms of accuracy and Fscore in detection attack compared to the state-of-the-art machine learning and representation learning methods, whilst the running time for attack detection is lower than 2E-6 seconds and the model size is lower than 1 MB. We also further investigate various characteristics of CTVAE in the latent space and in the reconstruction representation to demonstrate its efficacy compared with current well-known methods.
</details>
<details>
<summary>æ‘˜è¦</summary>
ä¾µå…¥æ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰å¯¹æ•°ç™¾ä¸‡ä¸ªç‰©è”ç½‘è®¾å¤‡è¿›è¡Œäº†é‡è¦çš„ä¿æŠ¤ã€‚ç„¶è€Œï¼ŒIDS é¢ä¸´ç‰©è”ç½‘ç³»ç»Ÿä¸­çš„å†…åœ¨æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬è®¾å¤‡å’Œæ•°æ®çš„å¤šæ ·æ€§ï¼Œé«˜ç»´åº¦è®­ç»ƒæ•°æ®ï¼Œä»¥åŠæ•°æ®ä¸å‡è¡¡ã€‚æ­¤å¤–ï¼Œåœ¨ç‰©è”ç½‘ç³»ç»Ÿä¸Šéƒ¨ç½² IDS å›°éš¾ï¼Œæœ‰æ—¶å€™ç”šè‡³ä¸å¯èƒ½ï¼Œå› ä¸ºå…¸å‹çš„ç‰©è”ç½‘è®¾å¤‡çš„å†…å­˜/å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›æœ‰é™ã€‚ä¸ºäº†è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œè¿™ç¯‡æ–‡ç« æè®®äº†ä¸€ç§æ–°çš„æ·±åº¦ç¥ç»ç½‘ç»œ/æ¶æ„ï¼Œå³å—é™çš„åŒè´¨é‡å˜æ¢è‡ªé€‚åº”å™¨ï¼ˆCTVAEï¼‰ã€‚CTVAE å¯ä»¥ä¸º IDS çš„åˆ†ç±»å™¨æä¾›æ›´åˆ†å‰²/ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ¸çš„ã€æ›´ä½ç»´åº¦çš„æ•°æ®è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç›¸æ¯”ç°çŠ¶çš„ç¥ç»ç½‘ç»œï¼ŒCTVAE éœ€è¦æ›´å°‘çš„å†…å­˜/å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›ï¼Œå› æ­¤æ›´é€‚åˆç‰©è”ç½‘ IDS ç³»ç»Ÿã€‚åœ¨ä½¿ç”¨ 11 ä¸ªæœ€å—æ¬¢è¿çš„ç‰©è”ç½‘ botnet æ•°æ®é›†è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒåï¼Œæˆ‘ä»¬å‘ç°ï¼ŒCTVAE å¯ä»¥æé«˜çº¦ 1% çš„å‡†ç¡®ç‡å’Œ Fscore åœ¨æ£€æµ‹æ”»å‡»æ–¹é¢ï¼Œè€Œä¸”æ£€æµ‹æ”»å‡»çš„è¿è¡Œæ—¶é—´ä½äº 2E-6 ç§’ï¼Œæ¨¡å‹å¤§å°ä½äº 1 MBã€‚æˆ‘ä»¬è¿˜è¿›ä¸€æ­¥ç ”ç©¶äº† CTVAE åœ¨å¹‚ç©ºé—´å’Œé‡å»ºè¡¨ç¤ºä¸­çš„ç‰¹ç‚¹ï¼Œä»¥ç¤ºå…¶æ•ˆæœç›¸æ¯”ç°æœ‰çš„æ–¹æ³•ã€‚
</details></li>
</ul>
<hr>
<h2 id="RL-Based-Cargo-UAV-Trajectory-Planning-and-Cell-Association-for-Minimum-Handoffs-Disconnectivity-and-Energy-Consumption"><a href="#RL-Based-Cargo-UAV-Trajectory-Planning-and-Cell-Association-for-Minimum-Handoffs-Disconnectivity-and-Energy-Consumption" class="headerlink" title="RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum Handoffs, Disconnectivity, and Energy Consumption"></a>RL-Based Cargo-UAV Trajectory Planning and Cell Association for Minimum Handoffs, Disconnectivity, and Energy Consumption</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02478">http://arxiv.org/abs/2312.02478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nesrine Cherif, Wael Jaafar, Halim Yanikomeroglu, Abbas Yongacoglu</li>
<li>For: è¿™ä¸ªè®ºæ–‡çš„ç›®çš„æ˜¯æé«˜æ— äººæœºè´§ç‰©äº¤ä»˜çš„å¯é æ€§å’Œèƒ½æ•ˆæ€§ã€‚* Methods: è¿™ç¯‡è®ºæ–‡ä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æŠ€æœ¯æ¥è”åˆè´§ç‰©æ— äººæœºçš„è·¯å¾„è§„åˆ’å’ŒCell Associationã€‚* Results: å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æ¯”è¾ƒæ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥é™ä½æ‰‹åŠ¨äº¤æ¢äº‹ä»¶ï¼Œé™ä½ç¦»çº¿äº‹ä»¶ï¼Œå¹¶æé«˜èƒ½æºæ¶ˆè€—ã€‚<details>
<summary>Abstract</summary>
Unmanned aerial vehicle (UAV) is a promising technology for last-mile cargo delivery. However, the limited on-board battery capacity, cellular unreliability, and frequent handoffs in the airspace are the main obstacles to unleash its full potential. Given that existing cellular networks were primarily designed to service ground users, re-utilizing the same architecture for highly mobile aerial users, e.g., cargo-UAVs, is deemed challenging. Indeed, to ensure a safe delivery using cargo-UAVs, it is crucial to utilize the available energy efficiently, while guaranteeing reliable connectivity for command-and-control and avoiding frequent handoff. To achieve this goal, we propose a novel approach for joint cargo-UAV trajectory planning and cell association. Specifically, we formulate the cargo-UAV mission as a multi-objective problem aiming to 1) minimize energy consumption, 2) reduce handoff events, and 3) guarantee cellular reliability along the trajectory. We leverage reinforcement learning (RL) to jointly optimize the cargo-UAV's trajectory and cell association. Simulation results demonstrate a performance improvement of our proposed method, in terms of handoffs, disconnectivity, and energy consumption, compared to benchmarks.
</details>
<details>
<summary>æ‘˜è¦</summary>
æ— äººé£è¡Œå™¨ï¼ˆUAVï¼‰æ˜¯ä¸€ç§æœ‰å‰é€”çš„ç§‘æŠ€ï¼Œç”¨äºæœ€åä¸€è‹±é‡Œçš„è´§ç‰©äº¤ä»˜ã€‚ç„¶è€Œï¼Œæœ‰é™çš„æœºä½“å†…ç½®ç”µæ± å®¹é‡ã€æ— çº¿ç”µä¸å¯é ã€ç©ºä¸­äº¤æ¢é¢‘ç¹ç­‰å› ç´ ï¼Œä½¿å¾—UAVçš„æ½œåŠ›å—åˆ°é™åˆ¶ã€‚ç”±äºç°æœ‰çš„æ— çº¿ç½‘ç»œä¸»è¦ä¸ºåœ°é¢ç”¨æˆ·è®¾è®¡ï¼Œå¯¹é«˜åº¦ç§»åŠ¨çš„ç©ºä¸­ç”¨æˆ·ï¼Œå¦‚è´§ç‰©UAVï¼Œè¿›è¡Œå†åˆ©ç”¨å¾ˆå›°éš¾ã€‚ä¸ºç¡®ä¿è´§ç‰©UAVå®‰å…¨äº¤ä»˜ï¼Œå¿…é¡»æœ‰æ•ˆåˆ©ç”¨å¯ç”¨èƒ½é‡ï¼ŒåŒæ—¶ä¿è¯å‘½ä»¤æ§åˆ¶çš„å¯é è¿æ¥ï¼Œé¿å…é¢‘ç¹äº¤æ¢ã€‚ä¸ºè¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œå³è´§ç‰©UAVè½¨è¿¹è§„åˆ’å’ŒCellså…³è”ä¼˜åŒ–ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†è´§ç‰©UAVçš„ä»»åŠ¡è§†ä¸ºä¸€ä¸ªå¤šç›®æ ‡é—®é¢˜ï¼Œå³1ï¼‰æœ€å°åŒ–èƒ½é‡æ¶ˆè€—ï¼Œ2ï¼‰å‡å°‘äº¤æ¢äº‹ä»¶ï¼Œ3ï¼‰ä¿è¯æ— çº¿è¿æ¥å¯é æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¥è”åˆä¼˜åŒ–è´§ç‰©UAVçš„è½¨è¿¹å’ŒCellså…³è”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æè®®æ–¹æ³•å¯ä»¥æ¯”å‡† benchmark æ›´å¥½åœ°æ”¹å–„äº¤æ¢ã€ç¦»çº¿å’Œèƒ½é‡æ¶ˆè€—ç­‰æŒ‡æ ‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="NeutronStream-A-Dynamic-GNN-Training-Framework-with-Sliding-Window-for-Graph-Streams"><a href="#NeutronStream-A-Dynamic-GNN-Training-Framework-with-Sliding-Window-for-Graph-Streams" class="headerlink" title="NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams"></a>NeutronStream: A Dynamic GNN Training Framework with Sliding Window for Graph Streams</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02473">http://arxiv.org/abs/2312.02473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chaoyi Chen, Dechao Gao, Yanfeng Zhang, Qiange Wang, Zhenbo Fu, Xuecang Zhang, Junhua Zhu, Yu Gu, Ge Yu</li>
<li>for: æœ¬æ–‡æ—¨åœ¨æä¾›ä¸€ä¸ªç”¨äºè®­ç»ƒåŠ¨æ€å›¾ neural networkï¼ˆGNNï¼‰æ¨¡å‹çš„æ¡†æ¶ï¼Œä»¥ä¾¿å¼€å‘è€…æ›´æ–¹ä¾¿åœ°åˆ›å»ºæ€§èƒ½å¼ºçš„ GNN å®ç°ã€‚</li>
<li>methods: æœ¬æ–‡ä½¿ç”¨äº†ä¸€ç§ç§°ä¸º NeutronStream çš„æ¡†æ¶ï¼Œå®ƒå°†è¾“å…¥åŠ¨æ€å›¾è½¬æ¢ä¸ºä¸€ä¸ªæŒ‰æ—¶é—´é¡ºåºæ›´æ–°çš„äº‹ä»¶æµï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–çš„æ»‘åŠ¨çª—å£æ¥é€æ­¥æ•æ‰äº‹ä»¶çš„ç©ºé—´-æ—¶é—´ç›¸å…³æ€§ã€‚ NeutronStream è¿˜æä¾›äº†ä¸€ä¸ªå¹¶è¡Œæ‰§è¡Œå¼•æ“ï¼Œä»¥è§£å†³äº‹ä»¶å¤„ç†çš„å¹¶å‘æŒ‘æˆ˜ï¼Œå¹¶å®ç°é«˜æ€§èƒ½ã€‚</li>
<li>results: å¯¹æ¯”å·é™…ç«¯çš„åŠ¨æ€ GNN å®ç°ï¼ŒNeutronStream åœ¨é€Ÿåº¦æ–¹é¢å®ç°äº†æå‡ ranges from 1.48X to 5.87Xï¼Œå¹¶åœ¨å¹³å‡å‡†ç¡®ç‡æ–¹é¢å®ç°äº†3.97%çš„æå‡ã€‚<details>
<summary>Abstract</summary>
Existing Graph Neural Network (GNN) training frameworks have been designed to help developers easily create performant GNN implementations. However, most existing GNN frameworks assume that the input graphs are static, but ignore that most real-world graphs are constantly evolving. Though many dynamic GNN models have emerged to learn from evolving graphs, the training process of these dynamic GNNs is dramatically different from traditional GNNs in that it captures both the spatial and temporal dependencies of graph updates. This poses new challenges for designing dynamic GNN training frameworks. First, the traditional batched training method fails to capture real-time structural evolution information. Second, the time-dependent nature makes parallel training hard to design. Third, it lacks system supports for users to efficiently implement dynamic GNNs. In this paper, we present NeutronStream, a framework for training dynamic GNN models. NeutronStream abstracts the input dynamic graph into a chronologically updated stream of events and processes the stream with an optimized sliding window to incrementally capture the spatial-temporal dependencies of events. Furthermore, NeutronStream provides a parallel execution engine to tackle the sequential event processing challenge to achieve high performance. NeutronStream also integrates a built-in graph storage structure that supports dynamic updates and provides a set of easy-to-use APIs that allow users to express their dynamic GNNs. Our experimental results demonstrate that, compared to state-of-the-art dynamic GNN implementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X and an average accuracy improvement of 3.97%.
</details>
<details>
<summary>æ‘˜è¦</summary>
ç°æœ‰çš„å›¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ç½‘ç»œï¼ˆGNNï¼‰è®­ç»ƒæ¡†æ¶å·²ç»è¢«è®¾è®¡ä¾¿äºå¼€å‘è€…å¿«é€Ÿåˆ›å»ºé«˜æ€§èƒ½çš„ GNN å®ç°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„ GNN æ¡†æ¶å‡è®¾è¾“å…¥å›¾ä¸ºé™æ­¢çš„ï¼Œå¿½ç•¥äº†å®é™…ä¸–ç•Œä¸­å¤§å¤šæ•°å›¾æ˜¯ä¸æ–­æ›´æ–°çš„ã€‚è™½ç„¶è®¸å¤šåŠ¨æ€ GNN æ¨¡å‹å·²ç»å‡ºç°ä»¥å­¦ä¹ å‘å±•ä¸­çš„å›¾ï¼Œä½†æ˜¯è¿™äº›åŠ¨æ€ GNN çš„è®­ç»ƒè¿‡ç¨‹ä¸ä¼ ç»Ÿ GNN çš„è®­ç»ƒè¿‡ç¨‹æœ‰å¾ˆå¤§å·®å¼‚ã€‚è¿™äº›å·®å¼‚å¸¦æ¥äº†è®¾è®¡åŠ¨æ€ GNN è®­ç»ƒæ¡†æ¶çš„æ–°æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œä¼ ç»Ÿçš„æ‰¹å¤„ç†è®­ç»ƒæ–¹æ³•æ— æ³•æ•æ‰å®æ—¶ç»“æ„å‘å±•ä¿¡æ¯ã€‚å…¶æ¬¡ï¼Œæ—¶é—´ä¾èµ–æ€§ä½¿å¾—å¹¶è¡Œè®­ç»ƒå˜å¾—å›°éš¾ã€‚æœ€åï¼Œç¼ºä¹å¯¹ç”¨æˆ·è¿›è¡Œé«˜æ•ˆå®ç°åŠ¨æ€ GNN çš„ç³»ç»Ÿæ”¯æŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº† NeutronStreamï¼Œä¸€ä¸ªç”¨äºè®­ç»ƒåŠ¨æ€ GNN æ¨¡å‹çš„æ¡†æ¶ã€‚NeutronStream å°†è¾“å…¥åŠ¨æ€å›¾è½¬åŒ–ä¸ºä¸€ä¸ªæ—¶é—´é¡ºåºæ›´æ–°çš„äº‹ä»¶æµï¼Œå¹¶ä½¿ç”¨ä¼˜åŒ–çš„æ»‘åŠ¨çª—å£æ¥é€æ­¥æ•æ‰äº‹ä»¶æµä¸­çš„ç©ºé—´-æ—¶é—´ç›¸å…³æ€§ã€‚æ­¤å¤–ï¼ŒNeutronStream æä¾›äº†å¹¶è¡Œæ‰§è¡Œå¼•æ“æ¥è§£å†³äº‹ä»¶å¤„ç†æŒ‘æˆ˜ï¼Œä»¥å®ç°é«˜æ€§èƒ½ã€‚NeutronStream è¿˜é›†æˆäº†ä¸€ä¸ªæ”¯æŒåŠ¨æ€æ›´æ–°çš„å›¾å­˜å‚¨ç»“æ„ï¼Œå¹¶æä¾›äº†ä¸€ç»„æ˜“äºä½¿ç”¨çš„ APIï¼Œallowing users to easily express their dynamic GNNsã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼Œç›¸æ¯”äºå½“å‰çš„åŠ¨æ€ GNN å®ç°ï¼ŒNeutronStream åœ¨æ€§èƒ½å’Œå‡†ç¡®ç‡æ–¹é¢å…·æœ‰1.48X-5.87Xçš„åŠ é€Ÿå’Œ3.97%çš„å‡å€¼æå‡ã€‚
</details></li>
</ul>
<hr>
<h2 id="Congestion-aware-Distributed-Task-Offloading-in-Wireless-Multi-hop-Networks-Using-Graph-Neural-Networks"><a href="#Congestion-aware-Distributed-Task-Offloading-in-Wireless-Multi-hop-Networks-Using-Graph-Neural-Networks" class="headerlink" title="Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks"></a>Congestion-aware Distributed Task Offloading in Wireless Multi-hop Networks Using Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02471">http://arxiv.org/abs/2312.02471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongyuan Zhao, Jake Perazzone, Gunjan Verma, Santiago Segarra</li>
<li>for: è¿™ä¸ªç ”ç©¶æ—¨åœ¨æé«˜è¾¹ç¼˜æ™ºèƒ½è®¾å¤‡ä¸­çš„å¤„ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ— çº¿å¤šè·³ç½‘ç»œä¸­å…·æœ‰å¤šä¸ªç§»åŠ¨è®¾å¤‡çš„æƒ…å†µä¸‹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨äº†åˆ†å¸ƒå¼æ’é˜µæ³•å’Œå›¾åƒå­¦ä¹ æ¥å®ç°ä½è´Ÿè½½ã€å¹²æ‰°ç¡®è®¤çš„åˆ†å¸ƒå¼ä»»åŠ¡å¸è½½æ–¹æ¡ˆã€‚</li>
<li>results: åœ¨å®éªŒä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿé™ä½æ— çº¿å¤šè·³ç½‘ç»œä¸­ä»»åŠ¡å¸è½½æ‰€å¯¼è‡´çš„ç½‘ç»œå¡«å……å’Œä¸ç¨³ queueï¼ŒåŒæ—¶æé«˜äº†æœ¬åœ°å¤„ç†çš„æ‰§è¡Œæ—¶é—´ã€‚<details>
<summary>Abstract</summary>
Computational offloading has become an enabling component for edge intelligence in mobile and smart devices. Existing offloading schemes mainly focus on mobile devices and servers, while ignoring the potential network congestion caused by tasks from multiple mobile devices, especially in wireless multi-hop networks. To fill this gap, we propose a low-overhead, congestion-aware distributed task offloading scheme by augmenting a distributed greedy framework with graph-based machine learning. In simulated wireless multi-hop networks with 20-110 nodes and a resource allocation scheme based on shortest path routing and contention-based link scheduling, our approach is demonstrated to be effective in reducing congestion or unstable queues under the context-agnostic baseline, while improving the execution latency over local computing.
</details>
<details>
<summary>æ‘˜è¦</summary>
computational offloadingå·²æˆä¸ºç§»åŠ¨è®¾å¤‡å’Œæ™ºèƒ½è®¾å¤‡çš„æ ¸å¿ƒç»„ä»¶ã€‚ç°æœ‰çš„å¸è½½æ–¹æ¡ˆä¸»è¦å…³æ³¨äºç§»åŠ¨è®¾å¤‡å’ŒæœåŠ¡å™¨ï¼Œè€Œå¿½ç•¥äº†å¤šä¸ªç§»åŠ¨è®¾å¤‡ä»»åŠ¡ä¹‹é—´çš„ç½‘ç»œå‹åŠ›ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œæˆ‘ä»¬æè®®ä¸€ç§ä½å¼€é”€ã€å‹åŠ›æ„ŸçŸ¥åˆ†å¸ƒå¼ä»»åŠ¡å¸è½½æ–¹æ¡ˆï¼Œé€šè¿‡å¯¹åˆ†å¸ƒå¼æ»¡ç§¯æ¡†æ¶è¿›è¡Œå›¾åƒå­¦ä¹ å¢å¼ºã€‚åœ¨æ¨¡æ‹Ÿæ— çº¿å¤šè·³ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é™ä½å‹åŠ›æˆ–ä¸ç¨³å®šé˜Ÿåˆ—ï¼Œæ¯”åŸºçº¿ä¸‹é™ä½æ‰§è¡Œå»¶è¿Ÿï¼Œè€Œä¸”åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å…·æœ‰æ”¹å–„æ€§ã€‚
</details></li>
</ul>
<hr>
<h2 id="Dimensionality-Reduction-and-Dynamical-Mode-Recognition-of-Circular-Arrays-of-Flame-Oscillators-Using-Deep-Neural-Network"><a href="#Dimensionality-Reduction-and-Dynamical-Mode-Recognition-of-Circular-Arrays-of-Flame-Oscillators-Using-Deep-Neural-Network" class="headerlink" title="Dimensionality Reduction and Dynamical Mode Recognition of Circular Arrays of Flame Oscillators Using Deep Neural Network"></a>Dimensionality Reduction and Dynamical Mode Recognition of Circular Arrays of Flame Oscillators Using Deep Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02462">http://arxiv.org/abs/2312.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weiming Xu, Tao Yang, Peng Zhang</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨å‡å°‘é«˜ç»´ç©ºé—´æ—¶é—´æ•°æ®ï¼Œå¹¶å®ç°ä¸åŒæŒ¯è¡æ¨¡å¼çš„åˆ†ç±»ã€‚</li>
<li>methods: è¯¥ç ”ç©¶ä½¿ç”¨äº†ä¸€ç§åŸºäºBi-LSTM-VAEå’ŒWDCçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ä½¿ç”¨Bi-LSTM-VAEè¿›è¡Œç»´åº¦å‡å°‘ï¼Œå¹¶ä½¿ç”¨WDCè¿›è¡Œæ¨¡å¼åˆ†ç±»ã€‚</li>
<li>results: ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥ç”Ÿæˆä¸ overlapçš„åˆ†å¸ƒï¼Œå¹¶ä¸”åœ¨åˆ†ç±»ä¸­è¡¨ç°å‡ºä¼˜äºVAEå’ŒPCAã€‚<details>
<summary>Abstract</summary>
Oscillatory combustion in aero engines and modern gas turbines often has significant adverse effects on their operation, and accurately recognizing various oscillation modes is the prerequisite for understanding and controlling combustion instability. However, the high-dimensional spatial-temporal data of a complex combustion system typically poses considerable challenges to the dynamical mode recognition. Based on a two-layer bidirectional long short-term memory variational autoencoder (Bi-LSTM-VAE) dimensionality reduction model and a two-dimensional Wasserstein distance-based classifier (WDC), this study proposes a promising method (Bi-LSTM-VAE-WDC) for recognizing dynamical modes in oscillatory combustion systems. Specifically, the Bi-LSTM-VAE dimension reduction model was introduced to reduce the high-dimensional spatial-temporal data of the combustion system to a low-dimensional phase space; Gaussian kernel density estimates (GKDE) were computed based on the distribution of phase points in a grid; two-dimensional WD values were calculated from the GKDE maps to recognize the oscillation modes. The time-series data used in this study were obtained from numerical simulations of circular arrays of laminar flame oscillators. The results show that the novel Bi-LSTM-VAE method can produce a non-overlapping distribution of phase points, indicating an effective unsupervised mode recognition and classification. Furthermore, the present method exhibits a more prominent performance than VAE and PCA (principal component analysis) for distinguishing dynamical modes in complex flame systems, implying its potential in studying turbulent combustion.
</details>
<details>
<summary>æ‘˜è¦</summary>
oscillatory combustion in å‘åŠ¨æœºå’Œç°ä»£æ¶²ä½“å‘åŠ¨æœºç»å¸¸ä¼šæœ‰æ˜¾è‘—çš„ä¸è‰¯å½±å“ï¼Œå¹¶ä¸”æ­£ç¡®åœ°è¯†åˆ«ä¸åŒçš„æŒ¯è¡æ¨¡å¼æ˜¯ç†è§£å’Œæ§åˆ¶ç‡ƒç‡ƒä¸ç¨³å®šçš„å¿…è¦å‰æã€‚ç„¶è€Œï¼Œå¤æ‚çš„ç‡ƒç‡ƒç³»ç»Ÿçš„é«˜ç»´åº¦ç©ºé—´æ—¶é—´æ•°æ®é€šå¸¸ä¼šå¯¹åŠ¨æ€æ¨¡å¼è¯†åˆ«æå‡ºå¾ˆå¤§æŒ‘æˆ˜ã€‚æœ¬ç ”ç©¶åŸºäºäºŒå±‚åŒå‘é•¿çŸ­æœŸè®°å¿†è‡ªé€‚åº”ç½‘ç»œï¼ˆBi-LSTM-VAEï¼‰ç»´åº¦å‡å°‘æ¨¡å‹å’ŒäºŒç»´ Wassersteinè·ç¦»åŸºäºåˆ†ç±»å™¨ï¼ˆWDCï¼‰ï¼Œæå‡ºäº†ä¸€ç§æœ‰ promiseçš„æ–¹æ³•ï¼ˆBi-LSTM-VAE-WDCï¼‰ç”¨äºè¯†åˆ«åŠ¨æ€æ¨¡å¼ã€‚å…·ä½“æ¥è¯´ï¼ŒBi-LSTM-VAE ç»´åº¦å‡å°‘æ¨¡å‹å°†é«˜ç»´åº¦ç©ºé—´æ—¶é—´æ•°æ®è½¬åŒ–ä¸ºä½ç»´åº¦çš„ç›¸ä½ç©ºé—´ï¼Œç„¶ååŸºäºç›¸ä½ç‚¹çš„åˆ†å¸ƒåœ¨ç½‘æ ¼ä¸­è®¡ç®—Gaussianæ ¸å¯†åº¦ä¼°è®¡ï¼ˆGKDEï¼‰ï¼Œä»GKDE å›¾è¡¨ä¸­è®¡ç®—äºŒç»´ Wassersteinè·ç¦»ï¼Œç”¨äºè¯†åˆ«æŒ¯è¡æ¨¡å¼ã€‚è¿™äº›æ—¶é—´åºåˆ—æ•°æ®ç”±æ•°å­— simulate circular array of laminar flame oscillators å¾—åˆ°ã€‚ç»“æœè¡¨æ˜ï¼Œæ–°çš„Bi-LSTM-VAEæ–¹æ³•å¯ä»¥ç”Ÿæˆä¸é‡å çš„ç›¸ä½ç‚¹åˆ†å¸ƒï¼Œè¡¨æ˜æœ‰æ•ˆçš„æ— ç›‘ç£æ¨¡å¼è¯†åˆ«å’Œåˆ†ç±»ã€‚æ­¤å¤–ï¼Œ presente æ–¹æ³•åœ¨å¤æ‚çš„ç‡ƒç‡ƒç³»ç»Ÿä¸­æ¯”VAEå’ŒPCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰è¡¨ç°æ›´å‡ºè‰²ï¼Œimplying its potential in studying turbulent combustionã€‚
</details></li>
</ul>
<hr>
<h2 id="GIT-Net-Generalized-Integral-Transform-for-Operator-Learning"><a href="#GIT-Net-Generalized-Integral-Transform-for-Operator-Learning" class="headerlink" title="GIT-Net: Generalized Integral Transform for Operator Learning"></a>GIT-Net: Generalized Integral Transform for Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02450">http://arxiv.org/abs/2312.02450</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaow-mat/general_integral_transform_neural_network">https://github.com/chaow-mat/general_integral_transform_neural_network</a></li>
<li>paper_authors: Chao Wang, Alexandre Hoang Thiery</li>
<li>for: ç”¨äºè§£å†³éƒ¨åˆ†åå¾®åˆ†æ–¹ç¨‹ï¼ˆPDEï¼‰Operatorçš„æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ã€‚</li>
<li>methods: ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ¥è¿‘ä¼¼ç‰¹å®šå‡½æ•°åŸºï¼ˆå¦‚å‚…æ•æåŒ–ï¼‰ä¸­çš„åå¾®åˆ†ç®—å­ã€‚</li>
<li>results: æ¯”è¾ƒå…¶ä»–æœ€æ–°çš„æ–¹æ¡ˆæ›´æœ‰åˆ©çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œé€‚ç”¨äºå¤æ‚ geometries ä¸Šçš„ PDE é—®é¢˜ï¼Œå¹¶åœ¨è®¸å¤š PDE é—®é¢˜ä¸Šè¡¨ç°å‡ºå°æµ‹è¯•é”™è¯¯å’Œä½è¯„ä»·ã€‚<details>
<summary>Abstract</summary>
This article introduces GIT-Net, a deep neural network architecture for approximating Partial Differential Equation (PDE) operators, inspired by integral transform operators. GIT-NET harnesses the fact that differential operators commonly used for defining PDEs can often be represented parsimoniously when expressed in specialized functional bases (e.g., Fourier basis). Unlike rigid integral transforms, GIT-Net parametrizes adaptive generalized integral transforms with deep neural networks. When compared to several recently proposed alternatives, GIT-Net's computational and memory requirements scale gracefully with mesh discretizations, facilitating its application to PDE problems on complex geometries. Numerical experiments demonstrate that GIT-Net is a competitive neural network operator, exhibiting small test errors and low evaluations across a range of PDE problems. This stands in contrast to existing neural network operators, which typically excel in just one of these areas.
</details>
<details>
<summary>æ‘˜è¦</summary>
è¿™ç¯‡æ–‡ç« ä»‹ç»äº† GIT-Netï¼Œä¸€ç§æ·±åº¦ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç”¨äºè¿‘ä¼¼ diferencial  equationï¼ˆPDEï¼‰ç®—å­ã€‚GIT-Net çµæ„Ÿæ¥è‡ªç§¯åˆ† transform ç®—å­ï¼Œåˆ©ç”¨äº† differential ç®—å­é€šå¸¸ç”¨äºå®šä¹‰ PDE çš„ç‰¹æ®Šå‡½æ•°åŸºï¼ˆä¾‹å¦‚ fourier åŸºï¼‰æ¥è¡¨ç¤ºã€‚ä¸å›ºå®šç§¯åˆ† transform ä¸åŒï¼ŒGIT-Net ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥ Parametrize è‡ªé€‚åº”æ€»ç§¯åˆ† transformã€‚ä¸å…¶ä»–æœ€è¿‘æå‡ºçš„ altenativas ç›¸æ¯”ï¼ŒGIT-Net çš„è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚éšç€ç½‘æ ¼ç²¾åº¦çš„å¢åŠ è€Œå‡å°‘ï¼Œä½¿å…¶é€‚ç”¨äºå¤æ‚ geometry ä¸Šçš„ PDE é—®é¢˜ã€‚æ•°å­—å®éªŒè¡¨æ˜ï¼ŒGIT-Net æ˜¯ä¸€ä¸ªç«äº‰åŠ›å¼ºçš„ç¥ç»ç½‘ç»œç®—å­ï¼Œåœ¨å¤šç§ PDE é—®é¢˜ä¸­è¡¨ç°å‡ºå°è¯¯å·®å’Œä½è¯„ä»·ã€‚è¿™ä¸ç°æœ‰çš„ç¥ç»ç½‘ç»œç®—å­ä¸åŒï¼Œé€šå¸¸åªåœ¨ä¸€ä¸ªè¿™äº›é¢†åŸŸä¸­å…·æœ‰ä¼˜åŠ¿ã€‚
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Instrument-Design-for-Indirect-Experiments"><a href="#Adaptive-Instrument-Design-for-Indirect-Experiments" class="headerlink" title="Adaptive Instrument Design for Indirect Experiments"></a>Adaptive Instrument Design for Indirect Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02438">http://arxiv.org/abs/2312.02438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yashchandak/IndirectExpDesign">https://github.com/yashchandak/IndirectExpDesign</a></li>
<li>paper_authors: Yash Chandak, Shiv Shankar, Vasilis Syrgkanis, Emma Brunskill</li>
<li>for: ä¼°è®¡å¹²é¢„æ•ˆæœï¼Œå°¤å…¶æ˜¯åœ¨å®æ–½Randomized Control Trials (RCTs) æ˜¯ä¸ç°å®æˆ–ä¸é“å¾·çš„æƒ…å†µä¸‹ã€‚</li>
<li>methods: åˆ©ç”¨ï¼ˆæ¡ä»¶ï¼‰å·¥å…·å˜é‡ï¼Œé€šè¿‡å¥–åŠ±å’Œæ¨èè€Œä¸æ˜¯ä¸¥æ ¼çš„æ²»ç–—åˆ†é…æ¥ä¼°è®¡å¹²é¢„æ•ˆæœã€‚</li>
<li>results: é€šè¿‡è‡ªé€‚åº”å®éªŒè®¾è®¡æ¥æé«˜ indirect experiment çš„æ ·æœ¬æ•ˆç‡ï¼Œå¹¶é€šè¿‡Influence Functionsæ¥æœç´¢æœ€ä½³æ•°æ®æ”¶é›†ç­–ç•¥ï¼Œæœ€å°åŒ–æ¬²è¦çš„ï¼ˆéçº¿æ€§ï¼‰ä¼°è®¡å™¨çš„å‡æ–¹å·®è¯¯å·®ã€‚<details>
<summary>Abstract</summary>
Indirect experiments provide a valuable framework for estimating treatment effects in situations where conducting randomized control trials (RCTs) is impractical or unethical. Unlike RCTs, indirect experiments estimate treatment effects by leveraging (conditional) instrumental variables, enabling estimation through encouragement and recommendation rather than strict treatment assignment. However, the sample efficiency of such estimators depends not only on the inherent variability in outcomes but also on the varying compliance levels of users with the instrumental variables and the choice of estimator being used, especially when dealing with numerous instrumental variables. While adaptive experiment design has a rich literature for direct experiments, in this paper we take the initial steps towards enhancing sample efficiency for indirect experiments by adaptively designing a data collection policy over instrumental variables. Our main contribution is a practical computational procedure that utilizes influence functions to search for an optimal data collection policy, minimizing the mean-squared error of the desired (non-linear) estimator. Through experiments conducted in various domains inspired by real-world applications, we showcase how our method can significantly improve the sample efficiency of indirect experiments.
</details>
<details>
<summary>æ‘˜è¦</summary>
Translated into Simplified Chinese: indirect experiments æä¾›ä¸€ä¸ªå€¼å¾—å…³æ³¨çš„æ¡†æ¶ï¼Œç”¨äºä¼°è®¡å¹²é¢„æ•ˆæœï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ–½Randomized Control Trials (RCTs) æ˜¯ä¸åˆ‡å®é™…æˆ–ä¸é“å¾·çš„æƒ…å†µä¸‹ã€‚ä¸ RCTs ä¸åŒï¼Œ indirect experiments é€šè¿‡åˆ©ç”¨ (conditional)  instrumente variables æ¥ä¼°è®¡å¹²é¢„æ•ˆæœï¼Œé€šè¿‡å¥–åŠ±å’Œæ¨èè€Œä¸æ˜¯ä¸¥æ ¼çš„å¹²é¢„åˆ†é…ã€‚ç„¶è€Œï¼Œ indirect experiments çš„æ ·æœ¬æ•ˆç‡å–å†³äºä¸åŒçš„ç”¨æˆ·æ˜¯å¦éµå¾ª instrumente variables çš„åˆä½œç‡ï¼Œä»¥åŠé€‰æ‹©çš„ä¼°è®¡å™¨ã€‚ especialmente when dealing with numerous instrumente variablesã€‚ While adaptive experiment design åœ¨ direct experiments é¢†åŸŸæœ‰ä¸°å¯Œçš„Literature, in this paper we take the initial steps towards enhancing sample efficiency for indirect experiments by adaptively designing a data collection policy over instrumente variablesã€‚ Our main contribution is a practical computational procedure that utilizes influence functions to search for an optimal data collection policy, minimizing the mean-squared error of the desired (non-linear) estimatorã€‚ Through experiments conducted in various domains inspired by real-world applications, we showcase how our method can significantly improve the sample efficiency of indirect experimentsã€‚Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PEFA-Parameter-Free-Adapters-for-Large-scale-Embedding-based-Retrieval-Models"><a href="#PEFA-Parameter-Free-Adapters-for-Large-scale-Embedding-based-Retrieval-Models" class="headerlink" title="PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models"></a>PEFA: Parameter-Free Adapters for Large-scale Embedding-based Retrieval Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02429">http://arxiv.org/abs/2312.02429</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amzn/pecos">https://github.com/amzn/pecos</a></li>
<li>paper_authors: Wei-Cheng Chang, Jyun-Yu Jiang, Jiong Zhang, Mutasem Al-Darabsah, Choon Hui Teo, Cho-Jui Hsieh, Hsiang-Fu Yu, S. V. N. Vishwanathan</li>
<li>for: è¿™ä¸ªç ”ç©¶çš„ç›®çš„æ˜¯æå‡ºä¸€ç§ ParamEter-Free Adapters (PEFA) æ¡†æ¶ï¼Œç”¨äºå¿«é€Ÿè°ƒå‚å¤§è§„æ¨¡æ–‡æœ¬æ£€ç´¢é—®é¢˜ä¸­çš„åµŒå…¥å¼æ¨¡å‹ (ERM)ã€‚</li>
<li>methods: PEFA æ¡†æ¶ä½¿ç”¨éå‚æ•°å¼ k-æœ€è¿‘é‚» (kNN) ç»„ä»¶æ¥ equip ERMï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨ convex combination çš„æ–¹å¼å°† ERM å’Œ kNN ä¸¤ä¸ªå¾—åˆ†å‡½æ•°ç›¸ç»“åˆã€‚</li>
<li>results: åœ¨ä¸¤ä¸ªæ£€ç´¢åº”ç”¨ä¸­ï¼ŒPEFA å®é™…ä¸Šè¾¾åˆ°äº†æ˜¾è‘—çš„æå‡ï¼ŒåŒ…æ‹¬å¯¹ Trivia-QA å’Œ NQ-320K è¿›è¡Œäº†é¢„è®­ç»ƒå’Œå¾®è°ƒ ERM çš„æ”¹è¿›ã€‚å¯¹äºæ–‡æ¡£æ£€ç´¢ï¼ŒPEFA åœ¨ Recall@100 æŒ‡æ ‡ä¸Šæé«˜äº†é¢„è®­ç»ƒ ERM çš„å¹³å‡æå‡ç‡ä¸º 13.2%ï¼Œè€Œå¾®è°ƒ ERM çš„å¹³å‡æå‡ç‡ä¸º 5.5%ã€‚å¯¹äºäº§å“æœç´¢ï¼ŒPEFA åœ¨å¾®è°ƒ ERM ä¸Šæé«˜äº† Recall@100 çš„å¹³å‡æå‡ç‡ä¸º 5.3%å’Œ 14.5%ã€‚<details>
<summary>Abstract</summary>
Embedding-based Retrieval Models (ERMs) have emerged as a promising framework for large-scale text retrieval problems due to powerful large language models. Nevertheless, fine-tuning ERMs to reach state-of-the-art results can be expensive due to the extreme scale of data as well as the complexity of multi-stages pipelines (e.g., pre-training, fine-tuning, distillation). In this work, we propose the PEFA framework, namely ParamEter-Free Adapters, for fast tuning of ERMs without any backward pass in the optimization. At index building stage, PEFA equips the ERM with a non-parametric k-nearest neighbor (kNN) component. At inference stage, PEFA performs a convex combination of two scoring functions, one from the ERM and the other from the kNN. Based on the neighborhood definition, PEFA framework induces two realizations, namely PEFA-XL (i.e., extra large) using double ANN indices and PEFA-XS (i.e., extra small) using a single ANN index. Empirically, PEFA achieves significant improvement on two retrieval applications. For document retrieval, regarding Recall@100 metric, PEFA improves not only pre-trained ERMs on Trivia-QA by an average of 13.2%, but also fine-tuned ERMs on NQ-320K by an average of 5.5%, respectively. For product search, PEFA improves the Recall@100 of the fine-tuned ERMs by an average of 5.3% and 14.5%, for PEFA-XS and PEFA-XL, respectively. Our code is available at https://github.com/amzn/pecos/tree/mainline/examples/pefa-wsdm24.
</details>
<details>
<summary>æ‘˜è¦</summary>
å¤§å‹æ–‡æœ¬æ£€ç´¢é—®é¢˜ä¸Šï¼ŒåµŒå…¥å¼æ£€ç´¢æ¨¡å‹ï¼ˆERMsï¼‰å·²ç»æˆä¸ºä¸€ç§æœ‰å‰é€”çš„æ¡†æ¶ï¼Œå°¤å…¶æ˜¯ç”±äºå¤§å‹è¯­è¨€æ¨¡å‹çš„å‡ºè‰²è¡¨ç°ã€‚ç„¶è€Œï¼Œä¸ºäº†è¾¾åˆ°çŠ¶æ€ä¹‹æœ€çš„æ•ˆæœï¼Œ fine-tuning ERMs å¯èƒ½ä¼šå¾ˆæ˜‚è´µï¼Œå› ä¸ºæ•°æ®çš„æå¤§è§„æ¨¡ä»¥åŠå¤šä¸ªé˜¶æ®µç®¡é“ï¼ˆå¦‚é¢„è®­ç»ƒã€ç²¾åº¦è°ƒæ•´ã€è’¸é¦ï¼‰çš„å¤æ‚æ€§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æå‡ºäº† PEFA æ¡†æ¶ï¼Œå³ ParamEter-Free Adaptersï¼Œç”¨äºå¿«é€Ÿè°ƒæ•´ ERMs è€Œæ— éœ€åå‘ä¼ æ’­ä¼˜åŒ–ã€‚åœ¨ç´¢å¼•å»ºç«‹é˜¶æ®µï¼ŒPEFA åœ¨ ERM ä¸Šæ·»åŠ äº†ä¸€ä¸ªéå‚æ•°å¼ k-æœ€è¿‘é‚»ï¼ˆkNNï¼‰ç»„ä»¶ã€‚åœ¨æ£€ç´¢é˜¶æ®µï¼ŒPEFA é€šè¿‡æƒå€¼èåˆä¸¤ä¸ªåˆ†æ•°å‡½æ•°ï¼Œä¸€ä¸ªæ¥è‡ª ERM å’Œå¦ä¸€ä¸ªæ¥è‡ª kNNã€‚åŸºäºé‚»å±…å®šä¹‰ï¼ŒPEFA æ¡†æ¶å®ç°äº†ä¸¤ä¸ªå®ç°ï¼Œå³ PEFA-XLï¼ˆi.e., extra largeï¼‰ä½¿ç”¨åŒ ANN ç´¢å¼•ï¼Œä»¥åŠ PEFA-XSï¼ˆi.e., extra smallï¼‰ä½¿ç”¨å• ANN ç´¢å¼•ã€‚å®éªŒè¡¨æ˜ï¼ŒPEFA åœ¨ä¸¤ä¸ªæ£€ç´¢åº”ç”¨ä¸­å…·æœ‰æ˜¾è‘—æ”¹å–„ã€‚å¯¹äºæ–‡æ¡£æ£€ç´¢ï¼ŒPEFA å¯¹ Trivia-QA é¢„è®­ç»ƒ ERM çš„ Recall@100 æŒ‡æ ‡æé«˜äº†å¹³å‡ 13.2%ï¼Œå¯¹äº NQ-320K é¢„è®­ç»ƒ ERM æé«˜äº†å¹³å‡ 5.5%ã€‚å¯¹äºäº§å“æ£€ç´¢ï¼ŒPEFA å¯¹ç²¾åº¦è°ƒæ•´ ERM çš„ Recall@100 æŒ‡æ ‡æé«˜äº†å¹³å‡ 5.3%å’Œ14.5%ï¼Œåˆ†åˆ«ç”¨äº PEFA-XS å’Œ PEFA-XLã€‚æˆ‘ä»¬çš„ä»£ç å¯ä»¥åœ¨ <https://github.com/amzn/pecos/tree/mainline/examples/pefa-wsdm24> æ‰¾åˆ°ã€‚
</details></li>
</ul>
<hr>
<h2 id="AI-driven-emergence-of-frequency-information-non-uniform-distribution-via-THz-metasurface-spectrum-prediction"><a href="#AI-driven-emergence-of-frequency-information-non-uniform-distribution-via-THz-metasurface-spectrum-prediction" class="headerlink" title="AI-driven emergence of frequency information non-uniform distribution via THz metasurface spectrum prediction"></a>AI-driven emergence of frequency information non-uniform distribution via THz metasurface spectrum prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.03017">http://arxiv.org/abs/2312.03017</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Ufere/Assingment_1">https://github.com/Ufere/Assingment_1</a></li>
<li>paper_authors: Xiaohua Xing, Yuqi Ren, Die Zou, Qiankun Zhang, Bingxuan Mao, Jianquan Yao, Deyi Xiong, Shuang Zhang, Liang Wu</li>
<li>for: é¢„æµ‹teraå…†é¢‘æ¨¡ulationæ•ˆæœ</li>
<li>methods: ä½¿ç”¨äººå·¥æ™ºèƒ½é¢„æµ‹æ–¹æ³•ï¼Œå¹¶æ·»åŠ å¤šé¢‘è¾“å…¥æ¥æé«˜é¢„æµ‹ç²¾åº¦</li>
<li>results: å®ç°äº†é¢„æµ‹teraå…†é¢‘æ¨¡ulationæ•ˆæœçš„é«˜ç²¾åº¦é¢„æµ‹ï¼Œå¹¶ä¸”å¼€è¾Ÿäº†äººå·¥æ™ºèƒ½åœ¨åŒ–å­¦ã€å¤æ‚ææ–™è®¾è®¡ã€ç”Ÿç‰©åŒ»å­¦ç­‰é¢†åŸŸçš„åº”ç”¨å‰æ™¯<details>
<summary>Abstract</summary>
Recently, artificial intelligence has been extensively deployed across various scientific disciplines, optimizing and guiding the progression of experiments through the integration of abundant datasets, whilst continuously probing the vast theoretical space encapsulated within the data. Particularly, deep learning models, due to their end-to-end adaptive learning capabilities, are capable of autonomously learning intrinsic data features, thereby transcending the limitations of traditional experience to a certain extent. Here, we unveil previously unreported information characteristics pertaining to different frequencies emerged during our work on predicting the terahertz spectral modulation effects of metasurfaces based on AI-prediction. Moreover, we have substantiated that our proposed methodology of simply adding supplementary multi-frequency inputs to the existing dataset during the target spectral prediction process can significantly enhance the predictive accuracy of the network. This approach effectively optimizes the utilization of existing datasets and paves the way for interdisciplinary research and applications in artificial intelligence, chemistry, composite material design, biomedicine, and other fields.
</details>
<details>
<summary>æ‘˜è¦</summary>
Here is the text in Simplified Chinese:æœ€è¿‘ï¼Œäººå·¥æ™ºèƒ½å·²ç»å¹¿æ³›åº”ç”¨äºä¸åŒçš„ç§‘å­¦é¢†åŸŸï¼Œé€šè¿‡å¤§é‡æ•°æ®çš„é›†æˆï¼Œä¼˜åŒ–å’Œå¯¼å¼•å®éªŒçš„è¿›ç¨‹ï¼ŒåŒæ—¶ä¸æ–­æ¢ç´¢æ•°æ®ä¸­çš„åºå¤§ç†è®ºç©ºé—´ã€‚ç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒä»¬çš„ç»ˆç«¯é€‚åº”å­¦ä¹ èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®ä¸­çš„å†…åœ¨ç‰¹å¾ï¼Œè‡³å°‘éƒ¨åˆ†çªç ´ä¼ ç»Ÿç»éªŒçš„é™åˆ¶ã€‚åœ¨æˆ‘ä»¬é¢„æµ‹teraHzé¢‘è°±ä¿®é¥°æ•ˆåº”çš„metaSurfacesä½¿ç”¨äººå·¥æ™ºèƒ½é¢„æµ‹æ—¶ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸åŒé¢‘ç‡çš„æ–°ä¿¡æ¯ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†æˆ‘ä»¬æè®®çš„æ–¹æ³•â€”â€”åœ¨ç›®æ ‡é¢‘è°±é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œæ·»åŠ å¤šä¸ªé¢‘ç‡è¾“å…¥â€”â€”å¯ä»¥æ˜¾è‘—æé«˜ç½‘ç»œçš„é¢„æµ‹ç²¾åº¦ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æœ‰æ•ˆåˆ©ç”¨ç°æœ‰æ•°æ®ï¼Œå¼€å±•è·¨å­¦ç§‘ç ”ç©¶å’Œåº”ç”¨äºäººå·¥æ™ºèƒ½ã€åŒ–å­¦ã€å¤åˆææ–™è®¾è®¡ã€ç”Ÿç‰©åŒ»å­¦å’Œå…¶ä»–é¢†åŸŸã€‚
</details></li>
</ul>
<hr>
<h2 id="Robust-Clustering-using-Hyperdimensional-Computing"><a href="#Robust-Clustering-using-Hyperdimensional-Computing" class="headerlink" title="Robust Clustering using Hyperdimensional Computing"></a>Robust Clustering using Hyperdimensional Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02407">http://arxiv.org/abs/2312.02407</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lulu Ge, Keshab K. Parhi<br>for:This paper aims to improve the clustering performance in the hyperdimensional computing (HDC) domain by proposing four HDC-based clustering algorithms.methods:The proposed algorithms use similarity-based k-means, equal bin-width histogram, equal bin-height histogram, and similarity-based affinity propagation to assign initial cluster hypervectors and improve the performance of HDCluster.results:The proposed algorithms achieve better accuracy, more robust performance, fewer iterations, and less execution time compared to the existing HDCluster. Specifically, similarity-based affinity propagation outperforms the other three algorithms on eight datasets by 2-38% in clustering accuracy. Additionally, the proposed algorithms can provide more robust clustering accuracy than HDCluster even for one-pass clustering, and traditional clustering is more desirable than HDC when the number of clusters is large.Here is the answer in Simplified Chinese text:for:è¿™ç¯‡è®ºæ–‡ç›®æ ‡æ˜¯åœ¨å¹‚ç»´åº¦è®¡ç®—ï¼ˆHDCï¼‰é¢†åŸŸä¸­æé«˜å½’ä¸€åŒ–æ€§èƒ½ã€‚methods:æè®®çš„ç®—æ³•ä½¿ç”¨ç›¸ä¼¼æ€§åŸºæœ¬çš„k-meansã€ç­‰å®½åº¦ histogramã€ç­‰é«˜åº¦ histogram å’Œç›¸ä¼¼æ€§åŸºæœ¬çš„å¸å¼•ä¼ æ’­æ¥åˆå§‹åŒ–å½’ä¸€åŒ–é›†ç¾¤ã€‚results:æè®®çš„ç®—æ³•ç›¸æ¯”ç°æœ‰çš„ HDCluster å…·æœ‰æ›´é«˜çš„å‡†ç¡®ç‡ã€æ›´ç¨³å®šçš„æ€§èƒ½ã€æ›´å°‘çš„è¿­ä»£æ¬¡æ•°å’Œæ›´çŸ­çš„æ‰§è¡Œæ—¶é—´ã€‚å…·ä½“æ¥è¯´ï¼Œç›¸ä¼¼æ€§åŸºæœ¬çš„å¸å¼•ä¼ æ’­åœ¨å…«ä¸ªæ•°æ®é›†ä¸Šæ¯”å…¶ä»–ä¸‰ç§ç®—æ³•æä¾›2-38%çš„å½’ä¸€åŒ–ç²¾åº¦æå‡ã€‚æ­¤å¤–ï¼Œæè®®çš„ç®—æ³•å¯ä»¥åœ¨ä¸€æ¬¡å½’ä¸€åŒ–ï¼ˆå³æ— è¿­ä»£æ›´æ–°é›†ç¾¤å‡†åˆ™ï¼‰ä¸‹æä¾›æ›´ç¨³å®šçš„å½’ä¸€åŒ–ç²¾åº¦ï¼Œè€Œä¼ ç»Ÿå½’ä¸€åŒ–åœ¨åˆ†æ”¯æ•°é‡è¾ƒå¤§æ—¶æ›´åŠ æ„¿æ„ä½¿ç”¨ã€‚<details>
<summary>Abstract</summary>
This paper addresses the clustering of data in the hyperdimensional computing (HDC) domain. In prior work, an HDC-based clustering framework, referred to as HDCluster, has been proposed. However, the performance of the existing HDCluster is not robust. The performance of HDCluster is degraded as the hypervectors for the clusters are chosen at random during the initialization step. To overcome this bottleneck, we assign the initial cluster hypervectors by exploring the similarity of the encoded data, referred to as \textit{query} hypervectors. Intra-cluster hypervectors have a higher similarity than inter-cluster hypervectors. Harnessing the similarity results among query hypervectors, this paper proposes four HDC-based clustering algorithms: similarity-based k-means, equal bin-width histogram, equal bin-height histogram, and similarity-based affinity propagation. Experimental results illustrate that: (i) Compared to the existing HDCluster, our proposed HDC-based clustering algorithms can achieve better accuracy, more robust performance, fewer iterations, and less execution time. Similarity-based affinity propagation outperforms the other three HDC-based clustering algorithms on eight datasets by 2~38% in clustering accuracy. (ii) Even for one-pass clustering, i.e., without any iterative update of the cluster hypervectors, our proposed algorithms can provide more robust clustering accuracy than HDCluster. (iii) Over eight datasets, five out of eight can achieve higher or comparable accuracy when projected onto the hyperdimensional space. Traditional clustering is more desirable than HDC when the number of clusters, $k$, is large.
</details>
<details>
<summary>æ‘˜è¦</summary>
Experiments show that the proposed algorithms outperform HDCluster in terms of accuracy, robustness, and execution time. Specifically, similarity-based affinity propagation achieves the highest accuracy on eight datasets, with an improvement of 2-38% compared to HDCluster. Additionally, the proposed algorithms can provide robust clustering accuracy even with one-pass clustering, without any iterative update of the cluster hypervectors. Finally, the paper shows that projecting the data onto the hyperdimensional space can improve the accuracy for some datasets.In summary, the paper proposes four HDC-based clustering algorithms that achieve better accuracy and robustness than existing methods, and demonstrates their effectiveness on eight datasets.
</details></li>
</ul>
<hr>
<h2 id="Harmonizing-Global-Voices-Culturally-Aware-Models-for-Enhanced-Content-Moderation"><a href="#Harmonizing-Global-Voices-Culturally-Aware-Models-for-Enhanced-Content-Moderation" class="headerlink" title="Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content Moderation"></a>Harmonizing Global Voices: Culturally-Aware Models for Enhanced Content Moderation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02401">http://arxiv.org/abs/2312.02401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex J. Chan, JosÃ© Luis Redondo GarcÃ­a, Fabrizio Silvestri, Colm Oâ€™Donnel, Konstantina Palla</li>
<li>for: æœ¬ç ”ç©¶æ—¨åœ¨æ¢è®¨å¦‚ä½•ä½¿CONTENT Moderation Systemèƒ½å¤Ÿè€ƒè™‘åœ°åŒºæ–‡åŒ–å·®å¼‚ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¯†åˆ«å’Œå¤„ç†ä¸å½“å†…å®¹ã€‚</li>
<li>methods: æœ¬ç ”ç©¶ä½¿ç”¨å¤§é‡åª’ä½“æ–°é—»å’Œæ–‡ç« æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä»¥åˆ›å»ºåœ°åŸŸåŒ–çš„è¯­è¨€æ¨¡å‹ï¼Œä»¥æ•æ‰å„åœ°åŒºçš„é€šä¿¡æ–¹å¼å·®å¼‚ï¼Œå¹¶ä¸”é€šè¿‡ç”Ÿæˆå†…å®¹è¿åæƒ…å†µçš„è§£é‡Šï¼Œè®©æ”¿ç­–æŒ‡å—åœ¨ä¸åŒçš„åœ°åŒºæ–‡åŒ–èƒŒæ™¯ä¸‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£å’Œåº”ç”¨ã€‚</li>
<li>results: ç ”ç©¶å‘ç°ï¼Œé€šè¿‡è®­ç»ƒäºåª’ä½“æ•°æ®é›†ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æˆåŠŸåœ°æ•æ‰åœ°åŒºæ–‡åŒ–å·®å¼‚ï¼Œå¹¶ä¸”æé«˜äº†åœ°åŒºæ€§çš„å†…å®¹è¯†åˆ«å’Œå¤„ç†èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿç”Ÿæˆä¸å½“åœ°æ–‡åŒ–å’Œç¤¾ä¼šèƒŒæ™¯ç›¸alignçš„è§£é‡Šã€‚<details>
<summary>Abstract</summary>
Content moderation at scale faces the challenge of considering local cultural distinctions when assessing content. While global policies aim to maintain decision-making consistency and prevent arbitrary rule enforcement, they often overlook regional variations in interpreting natural language as expressed in content. In this study, we are looking into how moderation systems can tackle this issue by adapting to local comprehension nuances. We train large language models on extensive datasets of media news and articles to create culturally attuned models. The latter aim to capture the nuances of communication across geographies with the goal of recognizing cultural and societal variations in what is considered offensive content. We further explore the capability of these models to generate explanations for instances of content violation, aiming to shed light on how policy guidelines are perceived when cultural and societal contexts change. We find that training on extensive media datasets successfully induced cultural awareness and resulted in improvements in handling content violations on a regional basis. Additionally, these advancements include the ability to provide explanations that align with the specific local norms and nuances as evidenced by the annotators' preference in our conducted study. This multifaceted success reinforces the critical role of an adaptable content moderation approach in keeping pace with the ever-evolving nature of the content it oversees.
</details>
<details>
<summary>æ‘˜è¦</summary>
We find that training on extensive media datasets successfully induced cultural awareness and resulted in improvements in handling content violations on a regional basis. Additionally, these advancements include the ability to provide explanations that align with the specific local norms and nuances, as evidenced by the annotators' preference in our conducted study. This multifaceted success reinforces the critical role of an adaptable content moderation approach in keeping pace with the ever-evolving nature of the content it oversees.
</details></li>
</ul>
<hr>
<h2 id="Auto-DP-SGD-Dual-Improvements-of-Privacy-and-Accuracy-via-Automatic-Clipping-Threshold-and-Noise-Multiplier-Estimation"><a href="#Auto-DP-SGD-Dual-Improvements-of-Privacy-and-Accuracy-via-Automatic-Clipping-Threshold-and-Noise-Multiplier-Estimation" class="headerlink" title="Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation"></a>Auto DP-SGD: Dual Improvements of Privacy and Accuracy via Automatic Clipping Threshold and Noise Multiplier Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2312.02400">http://arxiv.org/abs/2312.02400</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sai Venkatesh Chilukoti, Md Imran Hossen, Liqun Shan, Vijay Srinivas Tida, Xiai Hei</li>
<li>for: ä¿æŠ¤æ·±åº¦å­¦ä¹ åº”ç”¨ä¸­çš„ä¸ªäººéšç§ä¿¡æ¯ï¼ŒDP-SGD æ–¹æ³•å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ã€‚</li>
<li>methods: ç ”ç©¶è€…æå‡ºäº†å¤šç§è‡ªé€‚åº”DP-SGDæ–¹æ³•æ¥æé«˜æ¨¡å‹çš„å®ç”¨æ€§ã€‚</li>
<li>results: æˆ‘ä»¬çš„Auto DP-SGDæ–¹æ³•å¯ä»¥åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šæé«˜éšç§å’Œå‡†ç¡®æ€§ï¼Œå¹¶ä¸”å¯ä»¥é€‚åº”ä¸åŒçš„éšç§é¢„ç®—ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥é™ä½ç¼©æ”¾å› å­å’Œä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨æ¥é™ä½éšç§é¢„ç®—ï¼Œè€Œæ— éœ€ significatively  reducuce å‡†ç¡®æ€§ã€‚<details>
<summary>Abstract</summary>
DP-SGD has emerged as a popular method to protect personally identifiable information in deep learning applications. Unfortunately, DP-SGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility. To enhance the model's utility, researchers proposed various adaptive DP-SGD methods. However, we examine and discover that these techniques result in greater privacy leakage or lower accuracy than the traditional DP-SGD method, or a lack of evaluation on a complex data set such as CIFAR100. To address these limitations, we propose an Auto DP-SGD. Our method automates clipping threshold estimation based on the DL model's gradient norm and scales the gradients of each training sample without losing gradient information. This helps to improve the algorithm's utility while using a less privacy budget. To further improve accuracy, we introduce automatic noise multiplier decay mechanisms to decrease the noise multiplier after every epoch. Finally, we develop closed-form mathematical expressions using tCDP accountant for automatic noise multiplier and automatic clipping threshold estimation. Through extensive experimentation, we demonstrate that Auto DP-SGD outperforms existing SOTA DP-SGD methods in privacy and accuracy on various benchmark datasets. We also show that privacy can be improved by lowering the scale factor and using learning rate schedulers without significantly reducing accuracy. Specifically, Auto DP-SGD, when used with a step noise multiplier, improves accuracy by 3.20, 1.57, 6.73, and 1.42 for the MNIST, CIFAR10, CIFAR100, and AG News Corpus datasets, respectively. Furthermore, it obtains a substantial reduction in the privacy budget of 94.9, 79.16, 67.36, and 53.37 for the corresponding data sets.
</details>
<details>
<summary>æ‘˜è¦</summary>
DP-SGD å·²æˆä¸ºæ·±åº¦å­¦ä¹ åº”ç”¨ä¸­ä¿æŠ¤ä¸ªäººéšç§çš„å—æ¬¢è¿æ–¹æ³•ã€‚ç„¶è€Œï¼ŒDP-SGD çš„æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦clipingå’Œå‡åŒ€å™ªå£°æ·»åŠ  durante el entrenamientoå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†å¤šç§è‡ªé€‚åº”DP-SGD æ–¹æ³•ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æŠ€æœ¯ä¼šå¯¼è‡´éšç§æ³„éœ²æˆ–å‡å°‘å‡†ç¡®ç‡ï¼Œæˆ–è€…åœ¨å¤æ‚çš„æ•°æ®é›†ä¸Šæ²¡æœ‰è¿›è¡Œè¯„ä¼°ã€‚ä¸ºäº†è§£å†³è¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†è‡ªåŠ¨DP-SGDã€‚æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥è‡ªåŠ¨ä¼°è®¡æ¢¯åº¦normçš„clippingé˜ˆå€¼ï¼Œå¹¶å°†æ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æ¢¯åº¦ç¼©æ”¾è‡³ä¿ç•™æ¢¯åº¦ä¿¡æ¯ã€‚è¿™æœ‰åŠ©äºæé«˜ç®—æ³•çš„æ€§èƒ½ï¼ŒåŒæ—¶ä½¿ç”¨æ›´å°‘çš„éšç§é¢„ç®—ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥è‡ªåŠ¨å¹‚æ•°å‡å°‘æœºåˆ¶ï¼Œä»¥é€ epoch é€’å‡å¹‚æ•° Multiplierã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡closed-form çš„æ•°å­¦è¡¨è¿°ä½¿ç”¨ tCDP è´¢åŠ¡å…¬å¸æ¥è‡ªåŠ¨ç¡®å®šå¹‚æ•° multiplier å’Œ clipping é˜ˆå€¼ã€‚ç»è¿‡å¹¿æ³›çš„å®éªŒï¼Œæˆ‘ä»¬è¯æ˜äº† Auto DP-SGD å¯ä»¥åœ¨å„ç§æ ‡å‡† benchmark æ•°æ®é›†ä¸Šè¶…è¶Šç°æœ‰çš„ SOTA DP-SGD æ–¹æ³•ï¼ŒåŒæ—¶ä¿æŒéšç§å’Œå‡†ç¡®ç‡çš„å¹³è¡¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°å¯ä»¥é€šè¿‡é™ä½æ‰¬å°„å› å­å’Œä½¿ç”¨å­¦ä¹ ç‡è°ƒæ•´å™¨ï¼Œæ— éœ€å‡å°‘å‡†ç¡®ç‡ï¼Œè¿›ä¸€æ­¥æé«˜éšç§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨æ­¥è¿›å™ªå£° multiplier æ—¶ï¼ŒAuto DP-SGD å¯ä»¥åœ¨ MNISTã€CIFAR10ã€CIFAR100 å’Œ AG News Corpus æ•°æ®é›†ä¸Šæé«˜å‡†ç¡®ç‡ 3.20ã€1.57ã€6.73 å’Œ 1.42ï¼ŒåŒæ—¶é™ä½éšç§é¢„ç®— 94.9ã€79.16ã€67.36 å’Œ 53.37ã€‚
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/12/05/cs.LG_2023_12_05/" data-id="clq0ru6zq00z1to88eefrflng" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/109/">109</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">158</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">77</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">140</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">98</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a><span class="archive-list-count">49</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">214</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
