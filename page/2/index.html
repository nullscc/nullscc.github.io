
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/2/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.AI_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T12:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.AI_2023_11_06/">cs.AI - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multimodal-Stress-Detection-Using-Facial-Landmarks-and-Biometric-Signals"><a href="#Multimodal-Stress-Detection-Using-Facial-Landmarks-and-Biometric-Signals" class="headerlink" title="Multimodal Stress Detection Using Facial Landmarks and Biometric Signals"></a>Multimodal Stress Detection Using Facial Landmarks and Biometric Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03606">http://arxiv.org/abs/2311.03606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Majid Hosseini, Morteza Bodaghi, Ravi Teja Bhupatiraju, Anthony Maida, Raju Gottumukkala<br>for: 这种研究旨在提高人们的压力测量和情绪状况的评估，通过结合多种感知技术。methods: 这种研究使用多模态学习方法，结合脸部特征和生物指标信号进行压力检测。results: 研究发现，使用晚期融合技术可以达到94.39%的准确率，而使用早期融合技术可以超越这一成果，达到98.38%的准确率。<details>
<summary>Abstract</summary>
The development of various sensing technologies is improving measurements of stress and the well-being of individuals. Although progress has been made with single signal modalities like wearables and facial emotion recognition, integrating multiple modalities provides a more comprehensive understanding of stress, given that stress manifests differently across different people. Multi-modal learning aims to capitalize on the strength of each modality rather than relying on a single signal. Given the complexity of processing and integrating high-dimensional data from limited subjects, more research is needed. Numerous research efforts have been focused on fusing stress and emotion signals at an early stage, e.g., feature-level fusion using basic machine learning methods and 1D-CNN Methods. This paper proposes a multi-modal learning approach for stress detection that integrates facial landmarks and biometric signals. We test this multi-modal integration with various early-fusion and late-fusion techniques to integrate the 1D-CNN model from biometric signals and 2-D CNN using facial landmarks. We evaluate these architectures using a rigorous test of models' generalizability using the leave-one-subject-out mechanism, i.e., all samples related to a single subject are left out to train the model. Our findings show that late-fusion achieved 94.39\% accuracy, and early-fusion surpassed it with a 98.38\% accuracy rate. This research contributes valuable insights into enhancing stress detection through a multi-modal approach. The proposed research offers important knowledge in improving stress detection using a multi-modal approach.
</details>
<details>
<summary>摘要</summary>
发展不同感知技术是改善人们受 стресса度量和健康状况的度量。虽然在单个信号Modalities like wearables和facial emotion recognition方面已经取得了进步，但是结合多个Modalities可以提供更全面的理解受 стресса，因为受 стресса的表现不同于不同的人。多模态学习旨在利用每个模式的优势而不是仅仅依赖于单个信号。由于处理和整合高维数据的复杂性，更多的研究是必要的。许多研究团队已经专注于将受 стресса和情绪信号在早期结合，例如使用基本机器学习方法和1D-CNN方法进行特征级别的合并。这篇论文提出了一种结合面部特征和生物指标信号的多模态学习方法，并使用不同的早期和晚期结合技术来结合1D-CNN模型和2D-CNN模型。我们使用离卸一个主题的机制进行模型的评估，即所有与一个主题相关的样本都被离卸以训练模型。我们的发现表明，晚期结合达到了94.39%的准确率，而早期结合超过了它，达到98.38%的准确率。这些研究为增强受 стресса检测提供了重要的知识和技术。
</details></li>
</ul>
<hr>
<h2 id="Brief-for-the-Canada-House-of-Commons-Study-on-the-Implications-of-Artificial-Intelligence-Technologies-for-the-Canadian-Labor-Force-Generative-Artificial-Intelligence-Shatters-Models-of-AI-and-Labor"><a href="#Brief-for-the-Canada-House-of-Commons-Study-on-the-Implications-of-Artificial-Intelligence-Technologies-for-the-Canadian-Labor-Force-Generative-Artificial-Intelligence-Shatters-Models-of-AI-and-Labor" class="headerlink" title="Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor"></a>Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03595">http://arxiv.org/abs/2311.03595</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morgan R. Frank</li>
<li>for: 探讨当前生产力技术的发展对工作 Market的影响，并提出政策建议以适应未来工作环境。</li>
<li>methods: 利用数据分析和预测技术来研究生产力技术对工作市场的影响，并对现有的自动化预测模型进行批判性分析。</li>
<li>results: 发现生产力技术可能会对一些以前被认为免疫自动化的职业产生影响，政策 makers应该促进工人的职业适应性，并鼓励教育机构开发适应AI技术的教育程序。<details>
<summary>Abstract</summary>
Exciting advances in generative artificial intelligence (AI) have sparked concern for jobs, education, productivity, and the future of work. As with past technologies, generative AI may not lead to mass unemployment. But, unlike past technologies, generative AI is creative, cognitive, and potentially ubiquitous which makes the usual assumptions of automation predictions ill-suited for today. Existing projections suggest that generative AI will impact workers in occupations that were previously considered immune to automation. As AI's full set of capabilities and applications emerge, policy makers should promote workers' career adaptability. This goal requires improved data on job separations and unemployment by locality and job titles in order to identify early-indicators for the workers facing labor disruption. Further, prudent policy should incentivize education programs to accommodate learning with AI as a tool while preparing students for the demands of the future of work.
</details>
<details>
<summary>摘要</summary>
新一代生成人工智能技术的突破性发展，已经引起了关于工作、教育、生产力和未来工作的担忧。与过去的技术不同，生成人工智能可能不会导致大规模的失业。但是，由于生成人工智能的创造性、认知能力和潜在的普遍性，使得传统的自动化预测无法适用于今天。现有的预测表明，生成人工智能将影响工作者，特别是之前被认为是自动化的免疫的职业。为了实现工作者的职业适应能力，政策 makers应该推动工作者的职业适应能力。这个目标需要改进的数据，以了解地域和职业头衔上的失业和职业分裂。此外，安全的政策应该激励教育项目，以便学生通过人工智能为工具，准备未来的工作需求。
</details></li>
</ul>
<hr>
<h2 id="Finding-Increasingly-Large-Extremal-Graphs-with-AlphaZero-and-Tabu-Search"><a href="#Finding-Increasingly-Large-Extremal-Graphs-with-AlphaZero-and-Tabu-Search" class="headerlink" title="Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search"></a>Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03583">http://arxiv.org/abs/2311.03583</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abbas Mehrabian, Ankit Anand, Hyunjik Kim, Nicolas Sonnerat, Matej Balog, Gheorghe Comanici, Tudor Berariu, Andrew Lee, Anian Ruoss, Anna Bulanova, Daniel Toyama, Sam Blackwell, Bernardino Romera Paredes, Petar Veličković, Laurent Orseau, Joonkyung Lee, Anurag Murty Naredla, Doina Precup, Adam Zsolt Wagner</li>
<li>for: 这个论文解决了一个中央极点图论题，这个问题是根据1975年erdős的 conjecture，找到一个给定大小的图最多的边数而不包含3-或4-цикル。</li>
<li>methods: 这个论文使用了AlphaZero和tabu搜索两种方法，并通过引入课程来提高state-of-the-art下界。</li>
<li>results: 这个论文通过引入课程和提高搜索策略，提高了几个不同大小的图的下界。此外，这个论文还提出了一种灵活的图生成环境和一种 permutation-invariant的网络架构来学习搜索在图空间中。<details>
<summary>Abstract</summary>
This work studies a central extremal graph theory problem inspired by a 1975 conjecture of Erd\H{o}s, which aims to find graphs with a given size (number of nodes) that maximize the number of edges without having 3- or 4-cycles. We formulate this problem as a sequential decision-making problem and compare AlphaZero, a neural network-guided tree search, with tabu search, a heuristic local search method. Using either method, by introducing a curriculum -- jump-starting the search for larger graphs using good graphs found at smaller sizes -- we improve the state-of-the-art lower bounds for several sizes. We also propose a flexible graph-generation environment and a permutation-invariant network architecture for learning to search in the space of graphs.
</details>
<details>
<summary>摘要</summary>
这项研究探讨了一个中央极点图论问题，源于1975年 Erdős 的 conjecture，该问题目标是找到一个给定大小（节点数）的图，最大化边数而不包含 3-或 4-циклы。我们将这个问题转化为一个顺序决策问题，并与 AlphaZero 和 tabu search 进行比较。通过引入课程（启动搜索大图使用小图中的好图），我们提高了一些尺度的状态前几 bound。我们还提议了一个灵活的图生成环境和一个卷积神经网络架构，用于学习搜索图空间中的搜索。
</details></li>
</ul>
<hr>
<h2 id="Inclusive-Portraits-Race-Aware-Human-in-the-Loop-Technology"><a href="#Inclusive-Portraits-Race-Aware-Human-in-the-Loop-Technology" class="headerlink" title="Inclusive Portraits: Race-Aware Human-in-the-Loop Technology"></a>Inclusive Portraits: Race-Aware Human-in-the-Loop Technology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03567">http://arxiv.org/abs/2311.03567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claudia Flores-Saviaga, Christopher Curtis, Saiph Savage</li>
<li>for: 这个论文旨在提出一种基于人类社会理论的人 loop系统，以提高自动识别人脸的性能，特别是在服务范围内的人种少数群体。</li>
<li>methods: 该论文提出了一种名为“多元人loop”（Inclusive Portraits，IP）的新方法，通过考虑工作者的个人特点和背景，以提高人 loop系统的性能。</li>
<li>results: 实验结果表明，在 incorporating race into human-in-the-loop (HITL) systems for facial verification 中，可以significantly enhance performance，特别是在服务范围内的人种少数群体。<details>
<summary>Abstract</summary>
AI has revolutionized the processing of various services, including the automatic facial verification of people. Automated approaches have demonstrated their speed and efficiency in verifying a large volume of faces, but they can face challenges when processing content from certain communities, including communities of people of color. This challenge has prompted the adoption of "human-in-the-loop" (HITL) approaches, where human workers collaborate with the AI to minimize errors. However, most HITL approaches do not consider workers' individual characteristics and backgrounds. This paper proposes a new approach, called Inclusive Portraits (IP), that connects with social theories around race to design a racially-aware human-in-the-loop system. Our experiments have provided evidence that incorporating race into human-in-the-loop (HITL) systems for facial verification can significantly enhance performance, especially for services delivered to people of color. Our findings also highlight the importance of considering individual worker characteristics in the design of HITL systems, rather than treating workers as a homogenous group. Our research has significant design implications for developing AI-enhanced services that are more inclusive and equitable.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Low-Rank-MDPs-with-Continuous-Action-Spaces"><a href="#Low-Rank-MDPs-with-Continuous-Action-Spaces" class="headerlink" title="Low-Rank MDPs with Continuous Action Spaces"></a>Low-Rank MDPs with Continuous Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03564">http://arxiv.org/abs/2311.03564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Bennett, Nathan Kallus, Miruna Oprescu</li>
<li>for: 本研究旨在探讨如何将低维度马尔可夫决策过程（MDP）中的方法扩展到具有连续动作的场景中。</li>
<li>methods: 本研究使用多种具体的方法来扩展现有的低维度MDP方法，包括将 reward-agnostic 方法应用于连续动作空间中。</li>
<li>results: 研究表明，无需修改FLAMBE算法，在transition函数具有Holder平滑性适应动作的情况下，可以获得类似的PAC证明 bound。Specifically, 当政策类型具有固定最小浓度或奖励函数具有Holder平滑性，我们可以获得一个因次PAC bound，其取决于滑度的次数。<details>
<summary>Abstract</summary>
Low-Rank Markov Decision Processes (MDPs) have recently emerged as a promising framework within the domain of reinforcement learning (RL), as they allow for provably approximately correct (PAC) learning guarantees while also incorporating ML algorithms for representation learning. However, current methods for low-rank MDPs are limited in that they only consider finite action spaces, and give vacuous bounds as $|\mathcal{A}| \to \infty$, which greatly limits their applicability. In this work, we study the problem of extending such methods to settings with continuous actions, and explore multiple concrete approaches for performing this extension. As a case study, we consider the seminal FLAMBE algorithm (Agarwal et al., 2020), which is a reward-agnostic method for PAC RL with low-rank MDPs. We show that, without any modifications to the algorithm, we obtain similar PAC bound when actions are allowed to be continuous. Specifically, when the model for transition functions satisfies a Holder smoothness condition w.r.t. actions, and either the policy class has a uniformly bounded minimum density or the reward function is also Holder smooth, we obtain a polynomial PAC bound that depends on the order of smoothness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Context-Unlocks-Emotions-Text-based-Emotion-Classification-Dataset-Auditing-with-Large-Language-Models"><a href="#Context-Unlocks-Emotions-Text-based-Emotion-Classification-Dataset-Auditing-with-Large-Language-Models" class="headerlink" title="Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models"></a>Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03551">http://arxiv.org/abs/2311.03551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Yang, Aditya Kommineni, Mohammad Alshehri, Nilamadhab Mohanty, Vedant Modi, Jonathan Gratch, Shrikanth Narayanan</li>
<li>for: 提高文本数据的情感分类模型性能</li>
<li>methods: 使用大语言模型生成文本上的补充信息，提高文本与标签的对齐性</li>
<li>results: 人工和实验评估表明，使用提升的文本上下文可以提高情感分类模型的性能，并且从人工和机器两个角度都得到了证明<details>
<summary>Abstract</summary>
The lack of contextual information in text data can make the annotation process of text-based emotion classification datasets challenging. As a result, such datasets often contain labels that fail to consider all the relevant emotions in the vocabulary. This misalignment between text inputs and labels can degrade the performance of machine learning models trained on top of them. As re-annotating entire datasets is a costly and time-consuming task that cannot be done at scale, we propose to use the expressive capabilities of large language models to synthesize additional context for input text to increase its alignment with the annotated emotional labels. In this work, we propose a formal definition of textual context to motivate a prompting strategy to enhance such contextual information. We provide both human and empirical evaluation to demonstrate the efficacy of the enhanced context. Our method improves alignment between inputs and their human-annotated labels from both an empirical and human-evaluated standpoint.
</details>
<details>
<summary>摘要</summary>
文本数据中缺乏上下文信息可以让文本标注过程变得困难。这导致标注中的情感常常不考虑所有可能的情感词汇。这种对文本输入和标签的不一致可能使机器学习模型基于这些数据进行训练时表现下降。然而，重新标注整个数据集是一项费时费力的任务，不可能在大规模进行。我们提议使用大型自然语言模型来生成更多的上下文信息，以增强输入文本与注释的吻合性。在这篇文章中，我们提出了文本上下文的正式定义，以及一种提高上下文信息的提示策略。我们通过人工和实验评估来证明我们的方法可以提高输入文本和人工注释标签之间的吻合性。
</details></li>
</ul>
<hr>
<h2 id="United-We-Stand-Divided-We-Fall-UnityGraph-for-Unsupervised-Procedure-Learning-from-Videos"><a href="#United-We-Stand-Divided-We-Fall-UnityGraph-for-Unsupervised-Procedure-Learning-from-Videos" class="headerlink" title="United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos"></a>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03550">http://arxiv.org/abs/2311.03550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siddhant Bansal, Chetan Arora, C. V. Jawahar</li>
<li>for: 本研究旨在提高存在多个视频 demonstrate 同一个任务时，过程学习方法的效果。</li>
<li>methods: 我们提出了一种无监督图结构学习（GPL）框架，包括一种新的 UnityGraph，可以在多个视频中获取任务的内部和间接上下文。然后，通过不监督的方式更新 UnityGraph 中的嵌入向量，使得同一个键步骤的嵌入向量相似。最后，使用 KMeans 聚类算法来标识键步骤。</li>
<li>results: 我们在 ProceL、CrossTask 和 EgoProceL 等数据集上测试了 GPL，与状态之前的平均提高约 2% 和 3.6%。<details>
<summary>Abstract</summary>
Given multiple videos of the same task, procedure learning addresses identifying the key-steps and determining their order to perform the task. For this purpose, existing approaches use the signal generated from a pair of videos. This makes key-steps discovery challenging as the algorithms lack inter-videos perspective. Instead, we propose an unsupervised Graph-based Procedure Learning (GPL) framework. GPL consists of the novel UnityGraph that represents all the videos of a task as a graph to obtain both intra-video and inter-videos context. Further, to obtain similar embeddings for the same key-steps, the embeddings of UnityGraph are updated in an unsupervised manner using the Node2Vec algorithm. Finally, to identify the key-steps, we cluster the embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and EgoProceL datasets and achieve an average improvement of 2% on third-person datasets and 3.6% on EgoProceL over the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate_language: zh-CN<</SYS>>提供多个视频任务的同样任务，程序学习关注发现任务中的关键步骤并确定其执行顺序。现有的方法使用视频对的信号来实现此目的，但这会使关键步骤发现困难，因为算法缺乏间视频视野。我们提出一种无监督图грам学习（GPL）框架。GPL包括一种新的 UnityGraph，它将所有任务视频表示为一个图，以获取任务视频中的内部和间视频上下文。然后，使用Node2Vec算法更新 UnityGraph 中的表示，以获取同样的关键步骤的相似表示。最后，使用 KMeans 聚类算法来确定关键步骤。我们在 ProceL、CrossTask 和 EgoProceL 数据集上测试 GPL，并取得了 average 提升率为 2% 的第三人称数据集和 3.6% 的 EgoProceL 数据集。
</details></li>
</ul>
<hr>
<h2 id="InterVLS-Interactive-Model-Understanding-and-Improvement-with-Vision-Language-Surrogates"><a href="#InterVLS-Interactive-Model-Understanding-and-Improvement-with-Vision-Language-Surrogates" class="headerlink" title="InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates"></a>InterVLS: Interactive Model Understanding and Improvement with Vision-Language Surrogates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03547">http://arxiv.org/abs/2311.03547</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbin Huang, Wenbin He, Liang Gou, Liu Ren, Chris Bryan</li>
<li>for: 这个论文主要是为了提高深度学习模型的预部署理解和改进。</li>
<li>methods: 这篇论文使用了视觉概念基于方法，以提高模型的解释性和可读性。</li>
<li>results: 研究表明，InterVLS可以帮助用户更好地理解深度学习模型，了解模型的性能和影响因素，并且可以通过不同的概念影响来改进模型的性能。<details>
<summary>Abstract</summary>
Deep learning models are widely used in critical applications, highlighting the need for pre-deployment model understanding and improvement. Visual concept-based methods, while increasingly used for this purpose, face challenges: (1) most concepts lack interpretability, (2) existing methods require model knowledge, often unavailable at run time. Additionally, (3) there lacks a no-code method for post-understanding model improvement. Addressing these, we present InterVLS. The system facilitates model understanding by discovering text-aligned concepts, measuring their influence with model-agnostic linear surrogates. Employing visual analytics, InterVLS offers concept-based explanations and performance insights. It enables users to adjust concept influences to update a model, facilitating no-code model improvement. We evaluate InterVLS in a user study, illustrating its functionality with two scenarios. Results indicates that InterVLS is effective to help users identify influential concepts to a model, gain insights and adjust concept influence to improve the model. We conclude with a discussion based on our study results.
</details>
<details>
<summary>摘要</summary>
深度学习模型在关键应用中广泛使用，强调模型预部署理解和改进的需求。视觉概念基本方法，虽然在此目的上日益受到使用，但存在困难：（1）大多数概念不可解释，（2）现有方法需要模型知识，经常在运行时不可用，（3）缺乏无代码方法进行后期模型改进。为解决这些问题，我们提出InterVLS。该系统通过发现与文本对齐的概念，使用模型不依赖的直线函数来衡量它们的影响。通过视觉分析，InterVLS提供了基于概念的解释和性能印象。它允许用户根据概念的影响程度进行更新，实现无代码模型改进。我们在用户研究中证明InterVLS的可效性，用两个场景 Illustrates its functionality。结果表明，InterVLS可以帮助用户identify模型中影响力最大的概念，获得印象和更新概念影响以改进模型。我们根据研究结果进行讨论。
</details></li>
</ul>
<hr>
<h2 id="PcLast-Discovering-Plannable-Continuous-Latent-States"><a href="#PcLast-Discovering-Plannable-Continuous-Latent-States" class="headerlink" title="PcLast: Discovering Plannable Continuous Latent States"></a>PcLast: Discovering Plannable Continuous Latent States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03534">http://arxiv.org/abs/2311.03534</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Koul, Shivakanth Sujit, Shaoru Chen, Ben Evans, Lili Wu, Byron Xu, Rajan Chari, Riashat Islam, Raihan Seraj, Yonathan Efroni, Lekan Molu, Miro Dudik, John Langford, Alex Lamb</li>
<li>for: 这个论文旨在提高目标条件规划的效率，通过学习低维度表示来减少高维度观察数据中的干扰信息。</li>
<li>methods: 该论文使用多步反动方程学习latent表示，然后将其转换为在$\ell_2$空间相互关联可达状态的表示。</li>
<li>results: 数据测试结果表明，该方法可以提高奖励基本和奖励自由的样本效率，并生成层次化的状态抽象，以便计算效率高的层次规划。<details>
<summary>Abstract</summary>
Goal-conditioned planning benefits from learned low-dimensional representations of rich, high-dimensional observations. While compact latent representations, typically learned from variational autoencoders or inverse dynamics, enable goal-conditioned planning they ignore state affordances, thus hampering their sample-efficient planning capabilities. In this paper, we learn a representation that associates reachable states together for effective onward planning. We first learn a latent representation with multi-step inverse dynamics (to remove distracting information); and then transform this representation to associate reachable states together in $\ell_2$ space. Our proposals are rigorously tested in various simulation testbeds. Numerical results in reward-based and reward-free settings show significant improvements in sampling efficiency, and yields layered state abstractions that enable computationally efficient hierarchical planning.
</details>
<details>
<summary>摘要</summary>
系统具有目标条件规划的利点，即从学习低维度表示高维度观察数据中得到的低维度表示。然而，通常使用吸引式自动编码器或反动力学学习的紧凑缩写表示，忽略了状态可用性，因此降低了样本效率的规划能力。在这篇论文中，我们学习一个表示，将可达状态相关联在一起，以便有效的规划。我们首先使用多步反动力学学习 latent representation（以除掉干扰信息），然后将其转换为在 $\ell_2$ 空间中相关联可达状态的表示。我们的提议在各种 simulations 中进行了严格的测试，并在奖励基础和奖励自由的设定下获得了显著的样本效率改善和层次规划计算效率。
</details></li>
</ul>
<hr>
<h2 id="Brain-Networks-and-Intelligence-A-Graph-Neural-Network-Based-Approach-to-Resting-State-fMRI-Data"><a href="#Brain-Networks-and-Intelligence-A-Graph-Neural-Network-Based-Approach-to-Resting-State-fMRI-Data" class="headerlink" title="Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data"></a>Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03520">http://arxiv.org/abs/2311.03520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray, Pranav Suresh, Vince Calhoun, Jingyu Liu</li>
<li>for: 这篇论文旨在研究用resting-state功能磁共振成像（rsfMRI）来探索大脑功能和认知过程之间的关系，并使用图 neural networks 预测智商（流体、晶化和总智商）。</li>
<li>methods: 该论文提出了一种新的模型建立方法，称为BrainRGIN，它使用图 convolutional neural networks 对rsfMRI derive的静态函数网络连接矩阵进行预测。该方法包括嵌入和图同构网络，以及TopK pooling和注意力基于的读取函数。</li>
<li>results: 研究人员使用该模型在大规模数据集上进行了评估，Specifically the Adolescent Brain Cognitive Development Dataset，并证明其在预测个体差异智商方面的有效性。该模型的 Mean squared errors 和相关性指标都比既存的图 arquitectures 和传统机器学习模型更低，并且中前rontal gyri 在流体和晶化智商预测任务中具有显著的贡献。<details>
<summary>Abstract</summary>
Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the Adolescent Brain Cognitive Development Dataset, and demonstrated its effectiveness in predicting individual differences in intelligence. Our model achieved lower mean squared errors and higher correlation scores than existing relevant graph architectures and other traditional machine learning models for all of the intelligence prediction tasks. The middle frontal gyrus exhibited a significant contribution to both fluid and crystallized intelligence, suggesting their pivotal role in these cognitive processes. Total composite scores identified a diverse set of brain regions to be relevant which underscores the complex nature of total intelligence.
</details>
<details>
<summary>摘要</summary>
《宁静状态功能核磁共振成像（rsfMRI）是一种 poderful tool for investigating the relationship between brain function and cognitive processes, as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the Adolescent Brain Cognitive Development Dataset, and demonstrated its effectiveness in predicting individual differences in intelligence. Our model achieved lower mean squared errors and higher correlation scores than existing relevant graph architectures and other traditional machine learning models for all of the intelligence prediction tasks. The middle frontal gyrus exhibited a significant contribution to both fluid and crystallized intelligence, suggesting their pivotal role in these cognitive processes. Total composite scores identified a diverse set of brain regions to be relevant, which underscores the complex nature of total intelligence.》Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="MFAAN-Unveiling-Audio-Deepfakes-with-a-Multi-Feature-Authenticity-Network"><a href="#MFAAN-Unveiling-Audio-Deepfakes-with-a-Multi-Feature-Authenticity-Network" class="headerlink" title="MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network"></a>MFAAN: Unveiling Audio Deepfakes with a Multi-Feature Authenticity Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03509">http://arxiv.org/abs/2311.03509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karthik Sivarama Krishnan, Koushik Sivarama Krishnan</li>
<li>for: 防止深伪音频内容的散布，提高信息传递的可靠性。</li>
<li>methods: 利用多种音频表现方式，包括MFCC、LFCC和Chroma-STFT，实现多元特征融合，以提高伪音识别的精度。</li>
<li>results: 在两个 benchmark 数据集上，MFAAN 获得了高度的准确率，分别为98.93% 和 94.47%，说明 MFAAN 的可靠性和应用价值。<details>
<summary>Abstract</summary>
In the contemporary digital age, the proliferation of deepfakes presents a formidable challenge to the sanctity of information dissemination. Audio deepfakes, in particular, can be deceptively realistic, posing significant risks in misinformation campaigns. To address this threat, we introduce the Multi-Feature Audio Authenticity Network (MFAAN), an advanced architecture tailored for the detection of fabricated audio content. MFAAN incorporates multiple parallel paths designed to harness the strengths of different audio representations, including Mel-frequency cepstral coefficients (MFCC), linear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier Transform (Chroma-STFT). By synergistically fusing these features, MFAAN achieves a nuanced understanding of audio content, facilitating robust differentiation between genuine and manipulated recordings. Preliminary evaluations of MFAAN on two benchmark datasets, 'In-the-Wild' Audio Deepfake Data and The Fake-or-Real Dataset, demonstrate its superior performance, achieving accuracies of 98.93% and 94.47% respectively. Such results not only underscore the efficacy of MFAAN but also highlight its potential as a pivotal tool in the ongoing battle against deepfake audio content.
</details>
<details>
<summary>摘要</summary>
现代数字时代，深层伪造的普遍存在 pose 信息传递的威胁。特别是音频深层伪造，可能具有极高的真实感，对诡计行动可能带来很大的风险。为解决这一问题，我们介绍了多元特征音频真实性网络（MFAAN），这是一种针对 fabricated 音频内容的检测方法。MFAAN  integrates multiple parallel paths to harness the strengths of different audio representations, including Mel-frequency cepstral coefficients (MFCC), linear-frequency cepstral coefficients (LFCC), and Chroma Short Time Fourier Transform (Chroma-STFT). By synergistically fusing these features, MFAAN achieves a nuanced understanding of audio content, facilitating robust differentiation between genuine and manipulated recordings. 根据我们的初步评估，MFAAN 在 'In-the-Wild' 音频深层伪造数据集和 The Fake-or-Real Dataset 上表现出色，达到了 98.93% 和 94.47% 的准确率。这不仅证明了 MFAAN 的有效性，而且也 highlighted 它作为对深层伪造音频内容的战斗中的重要工具。
</details></li>
</ul>
<hr>
<h2 id="Astrocytes-as-a-mechanism-for-meta-plasticity-and-contextually-guided-network-function"><a href="#Astrocytes-as-a-mechanism-for-meta-plasticity-and-contextually-guided-network-function" class="headerlink" title="Astrocytes as a mechanism for meta-plasticity and contextually-guided network function"></a>Astrocytes as a mechanism for meta-plasticity and contextually-guided network function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03508">http://arxiv.org/abs/2311.03508</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lulu Gong, Fabio Pasqualetti, Thomas Papouin, ShiNung Ching</li>
<li>for: 这个论文探讨了astrocyte在大脑中的作用，以及它们如何帮助神经元学习和适应不同的任务和上下文。</li>
<li>methods: 作者使用了一种基于带宽 reinforcement learning 的方法，以模拟神经元、 synapse 和astrocyte之间的交互，并通过形式分析来描述astrocyte如何影响神经元和synapse的adaptation。</li>
<li>results: 研究发现，在astrocyte参与的情况下，神经元和synapse可以更好地适应不同的任务和上下文，并且可以更快地学习和适应新的任务。这些网络在多变的上下文中也可以更好地保持稳定性和可靠性。<details>
<summary>Abstract</summary>
Astrocytes are a highly expressed and highly enigmatic cell-type in the mammalian brain. Traditionally viewed as a mediator of basic physiological sustenance, it is increasingly recognized that astrocytes may play a more direct role in neural computation. A conceptual challenge to this idea is the fact that astrocytic activity takes a very different form than that of neurons, and in particular, occurs at orders-of-magnitude slower time-scales. In the current paper, we engage how such time-scale separation may endow astrocytes with the capability to enable learning in context-dependent settings, where fluctuations in task parameters may occur much more slowly than within-task requirements. This idea is based on the recent supposition that astrocytes, owing to their sensitivity to a host of physiological covariates, may be particularly well poised to modulate the dynamics of neural circuits in functionally salient ways. We pose a general model of neural-synaptic-astrocyte interaction and use formal analysis to characterize how astrocytic modulation may constitute a form of meta-plasticity, altering the ways in which synapses and neurons adapt as a function of time. We then embed this model in a bandit-based reinforcement learning task environment, and show how the presence of time-scale separated astrocytic modulation enables learning over multiple fluctuating contexts. Indeed, these networks learn far more reliably versus dynamically homogenous networks and conventional non-network-based bandit algorithms. Our results indicate how the presence of neural-astrocyte interaction in the brain may benefit learning over different time-scale and the conveyance of task relevant contextual information onto circuit dynamics.
</details>
<details>
<summary>摘要</summary>
astrocytes是大脑中高度表达和高度enigmatic的细胞类型。传统上视为基本生理机能的调节剂，但现在越来越认为astrocytes可能直接参与神经计算。一个概念上的挑战是astrocytic activity的形式和神经元活动有着很大的不同，尤其是时间尺度上的差异。在当前的论文中，我们考虑如何这种时间尺度差异可能为astrocytes提供了可以实现学习的能力。这种想法基于astrocytes敏感于多种生理因素的假设，因此可以通过调节神经细胞动力学来修改神经细胞的动力学。我们提出了神经细胞- synapse-astrocyte交互的总模型，并使用正式分析来描述如何astrocytic modulation可能是一种形式的meta-plasticity，改变神经细胞和 synapse在时间上的适应。然后，我们将这个模型嵌入一个bandit-based reinforcement learning任务环境中，并显示了在多个随机变化的上下文中，astrocyte-modulated neural network可以更可靠地学习。实际上，这些网络在不同的时间尺度和传统的非网络基本算法下都学习得更好。我们的结果表明，在大脑中astrocyte-神经元交互的存在可以提高学习的可靠性和将任务相关的上下文信息传递到神经细胞动力学中。
</details></li>
</ul>
<hr>
<h2 id="Multi-Resolution-Diffusion-for-Privacy-Sensitive-Recommender-Systems"><a href="#Multi-Resolution-Diffusion-for-Privacy-Sensitive-Recommender-Systems" class="headerlink" title="Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems"></a>Multi-Resolution Diffusion for Privacy-Sensitive Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03488">http://arxiv.org/abs/2311.03488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Derek Lilienthal, Paul Mello, Magdalini Eirinaki, Stas Tiomkin</li>
<li>for: The paper is written for recommender systems that rely on user data, addressing privacy and security concerns by substituting user data with synthetic data.</li>
<li>methods: The paper introduces a Score-based Diffusion Recommendation Model (SDRM) that uses diffusion models to generate realistic data, capturing intricate patterns in real-world datasets.</li>
<li>results: The paper shows that SDRM outperforms competing baselines in synthesizing various datasets to replace or augment the original data, achieving an average improvement of 4.30% in Recall@$n$ and 4.65% in NDCG@$n$.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了帮助推荐系统，解决它们依赖用户数据的隐私和安全问题。</li>
<li>methods: 这篇论文提出了一种基于扩散模型的分数基于扩散建议模型（SDRM），可以生成真实的数据，捕捉实际世界数据中的复杂模式。</li>
<li>results: 论文表明，SDRM比基本方法（如生成对抗网络、变量自适应网络和最近提出的扩散模型）在生成不同类型的数据来代替或补充原始数据时，平均提高4.30%的Recall@$n$和4.65%的NDCG@$n$。<details>
<summary>Abstract</summary>
While recommender systems have become an integral component of the Web experience, their heavy reliance on user data raises privacy and security concerns. Substituting user data with synthetic data can address these concerns, but accurately replicating these real-world datasets has been a notoriously challenging problem. Recent advancements in generative AI have demonstrated the impressive capabilities of diffusion models in generating realistic data across various domains. In this work we introduce a Score-based Diffusion Recommendation Model (SDRM), which captures the intricate patterns of real-world datasets required for training highly accurate recommender systems. SDRM allows for the generation of synthetic data that can replace existing datasets to preserve user privacy, or augment existing datasets to address excessive data sparsity. Our method outperforms competing baselines such as generative adversarial networks, variational autoencoders, and recently proposed diffusion models in synthesizing various datasets to replace or augment the original data by an average improvement of 4.30% in Recall@$n$ and 4.65% in NDCG@$n$.
</details>
<details>
<summary>摘要</summary>
“优化推荐系统的数据使用问题”While recommender systems have become an integral component of the Web experience, their heavy reliance on user data raises privacy and security concerns. Substituting user data with synthetic data can address these concerns, but accurately replicating these real-world datasets has been a notoriously challenging problem. Recent advancements in generative AI have demonstrated the impressive capabilities of diffusion models in generating realistic data across various domains. In this work we introduce a Score-based Diffusion Recommendation Model (SDRM), which captures the intricate patterns of real-world datasets required for training highly accurate recommender systems. SDRM allows for the generation of synthetic data that can replace existing datasets to preserve user privacy, or augment existing datasets to address excessive data sparsity. Our method outperforms competing baselines such as generative adversarial networks, variational autoencoders, and recently proposed diffusion models in synthesizing various datasets to replace or augment the original data by an average improvement of 4.30% in Recall@$n$ and 4.65% in NDCG@$n$.
</details></li>
</ul>
<hr>
<h2 id="CLIP-Motion-Learning-Reward-Functions-for-Robotic-Actions-Using-Consecutive-Observations"><a href="#CLIP-Motion-Learning-Reward-Functions-for-Robotic-Actions-Using-Consecutive-Observations" class="headerlink" title="CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations"></a>CLIP-Motion: Learning Reward Functions for Robotic Actions Using Consecutive Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03485">http://arxiv.org/abs/2311.03485</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuzhe Dang, Stefan Edelkamp, Nicolas Ribault</li>
<li>for: 本研究目的是开发一种基于 CLIP 模型的奖励函数学习方法，以解决传统奖励函数设计 often 需要人工特征工程化，可能难以泛化到多种任务。</li>
<li>methods: 本研究使用 CLIP 模型处理 Both 状态特征和图像输入，并能够准确地识别 consecutive 观察到的动作。</li>
<li>results: 经过实验评估，我们的方法在多种 роботикс 活动中表现出色，如指定目标物品的夹取和立方体的位置调整。结果表明，我们的方法可以准确地推断动作并在 robotics 领域中提高奖励学习训练的效果。<details>
<summary>Abstract</summary>
This paper presents a novel method for learning reward functions for robotic motions by harnessing the power of a CLIP-based model. Traditional reward function design often hinges on manual feature engineering, which can struggle to generalize across an array of tasks. Our approach circumvents this challenge by capitalizing on CLIP's capability to process both state features and image inputs effectively. Given a pair of consecutive observations, our model excels in identifying the motion executed between them. We showcase results spanning various robotic activities, such as directing a gripper to a designated target and adjusting the position of a cube. Through experimental evaluations, we underline the proficiency of our method in precisely deducing motion and its promise to enhance reinforcement learning training in the realm of robotics.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的方法，用于通过 CLIP 模型学习 robotic 动作奖励函数。传统的奖励函数设计经常遇到人工特征工程化的挑战，这可以困难泛化到多种任务上。我们的方法强调使用 CLIP 模型处理 Both state 特征和图像输入，从而缺乏特征工程化的需求。给定两个连续的观察结果，我们的模型能够有效地识别执行的动作。我们在不同的 robotic 活动中，如指定目标上的夹子和立方体的位置调整，展示了我们的方法的精度和其在 robotics 领域的应用潜力。通过实验评估，我们证明了我们的方法在 precisely 推理动作和提高 reinforcement learning 训练中的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Multi-Loss-based-Feature-Fusion-and-Top-Two-Voting-Ensemble-Decision-Strategy-for-Facial-Expression-Recognition-in-the-Wild"><a href="#Multi-Loss-based-Feature-Fusion-and-Top-Two-Voting-Ensemble-Decision-Strategy-for-Facial-Expression-Recognition-in-the-Wild" class="headerlink" title="Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild"></a>Multi Loss-based Feature Fusion and Top Two Voting Ensemble Decision Strategy for Facial Expression Recognition in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03478">http://arxiv.org/abs/2311.03478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyao Zhou, Yuanlun Xie, Wenhong Tian</li>
<li>for: 这个论文旨在提高人脸表达识别（FER）在野外的性能，并不同于前期研究，这篇论文同时应用内部特征合并和多个网络的特征合并，以及ensemble策略。</li>
<li>methods: 这篇论文提出了一个新的单个模型named R18+FAML，以及一个ensemble模型named R18+FAML-FGA-T2V，以提高FER在野外的性能。R18+FAML使用内部特征合并和多个损失函数（FAML）来提高特征EXTRACTION的多样性。为了提高R18+FAML的性能，我们提出了一种基于遗传算法的特征合并方法（FGA），可以将多个网络的 convolution核心进行特征EXTRACTION。基于R18+FAML和FGA，我们提出了一种ensemble策略，即Top Two Voting（T2V），以便对FER进行分类。</li>
<li>results: 我们的单个模型R18+FAML和ensemble模型R18+FAML-FGA-T2V在三个挑战性的FER数据集RAF-DB、AffectNet-8和AffectNet-7上实现了($90.32%$, $62.17%$, $65.83%$)和($91.59%$, $63.27%$, $66.63%$)的准确率，均高于当前最佳结果。<details>
<summary>Abstract</summary>
Facial expression recognition (FER) in the wild is a challenging task affected by the image quality and has attracted broad interest in computer vision. There is no research using feature fusion and ensemble strategy for FER simultaneously. Different from previous studies, this paper applies both internal feature fusion for a single model and feature fusion among multiple networks, as well as the ensemble strategy. This paper proposes one novel single model named R18+FAML, as well as one ensemble model named R18+FAML-FGA-T2V to improve the performance of the FER in the wild. Based on the structure of ResNet18 (R18), R18+FAML combines internal Feature fusion and three Attention blocks using Multiple Loss functions (FAML) to improve the diversity of the feature extraction. To improve the performance of R18+FAML, we propose a Feature fusion among networks based on the Genetic Algorithm (FGA), which can fuse the convolution kernels for feature extraction of multiple networks. On the basis of R18+FAML and FGA, we propose one ensemble strategy, i.e., the Top Two Voting (T2V) to support the classification of FER, which can consider more classification information comprehensively. Combining the above strategies, R18+FAML-FGA-T2V can focus on the main expression-aware areas. Extensive experiments demonstrate that our single model R18+FAML and the ensemble model R18+FAML-FGA-T2V achieve the accuracies of $\left( 90.32, 62.17, 65.83 \right)\%$ and $\left( 91.59, 63.27, 66.63 \right)\%$ on three challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7 respectively, both outperforming the state-of-the-art results.
</details>
<details>
<summary>摘要</summary>
“人脸表情识别（FER）在野外是一个具有图像质量的挑战，吸引了计算机视觉领域广泛的研究。 existing studies have not explored the use of feature fusion and ensemble strategies for FER simultaneously. 本研究提出了一种新的单模型名为R18+FAML，以及一种ensemble模型名为R18+FAML-FGA-T2V，以提高FER的性能。 R18+FAML通过内部Feature fusion和三个Attention块使用多种损失函数（FAML）来提高特征提取的多样性。为了进一步提高R18+FAML的性能，我们提出了一种基于遗传算法的Feature fusion among networks（FGA），可以将多个网络的 convolution kernel进行特征提取。基于R18+FAML和FGA，我们提出了一种ensemble策略，即Top Two Voting（T2V），可以将多个网络的分类信息进行权衡考虑。结合以上策略，R18+FAML-FGA-T2V可以更好地关注主要表情意识区域。EXTENSIVE experiments demonstrate that our single model R18+FAML and the ensemble model R18+FAML-FGA-T2V achieve the accuracies of $\left( 90.32, 62.17, 65.83 \right)\%$ and $\left( 91.59, 63.27, 66.63 \right)\%$ on three challenging unbalanced FER datasets RAF-DB, AffectNet-8 and AffectNet-7 respectively, both outperforming the state-of-the-art results.”Note that the translation is done using Google Translate, and may not be perfect. Please let me know if you need any further assistance.
</details></li>
</ul>
<hr>
<h2 id="FinA-Fairness-of-Adverse-Effects-in-Decision-Making-of-Human-Cyber-Physical-System"><a href="#FinA-Fairness-of-Adverse-Effects-in-Decision-Making-of-Human-Cyber-Physical-System" class="headerlink" title="FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System"></a>FinA: Fairness of Adverse Effects in Decision-Making of Human-Cyber-Physical-System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03468">http://arxiv.org/abs/2311.03468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Zhao, Salma Elmalaki</li>
<li>for: 这篇论文的目的是解决人类-人工-物理系统（HCPS）中的公平问题，特别是当不同个体、每个人有不同的行为和期望时，在同一个应用空间中受到共同的控制动作影响。</li>
<li>methods: 这篇论文使用了一个具有多元性和动态性的公平模型，考虑了人类行为的变化和不断改善的偏好，并认可了长期的影响。</li>
<li>results: 这篇论文的评估结果显示，在智能家居应用中运用了“公平在不良影响”（FinA）方法后，个体对公平的整体感受得到了明显提高，比较前一代方法的提高率为66.7%。<details>
<summary>Abstract</summary>
Ensuring fairness in decision-making systems within Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when diverse individuals, each with varying behaviors and expectations, coexist within the same application space, influenced by a shared set of control actions in the system. The long-term adverse effects of these actions further pose the challenge, as historical experiences and interactions shape individual perceptions of fairness. This paper addresses the challenge of fairness from an equity perspective of adverse effects, taking into account the dynamic nature of human behavior and evolving preferences while recognizing the lasting impact of adverse effects. We formally introduce the concept of Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a comprehensive set of five formulations for FinA, encompassing both the instantaneous and long-term aspects of adverse effects. To empirically validate the effectiveness of our FinA approach, we conducted an evaluation within the domain of smart homes, a pertinent HCPS application. The outcomes of our evaluation demonstrate that the adoption of FinA significantly enhances the overall perception of fairness among individuals, yielding an average improvement of 66.7% when compared to the state-of-the-art method.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese: Ensuring fairness in decision-making systems within Human-Cyber-Physical-Systems (HCPS) is a pressing concern, particularly when diverse individuals, each with varying behaviors and expectations, coexist within the same application space, influenced by a shared set of control actions in the system. The long-term adverse effects of these actions further pose the challenge, as historical experiences and interactions shape individual perceptions of fairness. This paper addresses the challenge of fairness from an equity perspective of adverse effects, taking into account the dynamic nature of human behavior and evolving preferences while recognizing the lasting impact of adverse effects. We formally introduce the concept of Fairness-in-Adverse-Effects (FinA) within the HCPS context. We put forth a comprehensive set of five formulations for FinA, encompassing both the instantaneous and long-term aspects of adverse effects. To empirically validate the effectiveness of our FinA approach, we conducted an evaluation within the domain of smart homes, a pertinent HCPS application. The outcomes of our evaluation demonstrate that the adoption of FinA significantly enhances the overall perception of fairness among individuals, yielding an average improvement of 66.7% when compared to the state-of-the-art method.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation"><a href="#Exploitation-Guided-Exploration-for-Semantic-Embodied-Navigation" class="headerlink" title="Exploitation-Guided Exploration for Semantic Embodied Navigation"></a>Exploitation-Guided Exploration for Semantic Embodied Navigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03357">http://arxiv.org/abs/2311.03357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Justin Wasserman, Girish Chowdhary, Abhinav Gupta, Unnat Jain</li>
<li>for: 本研究旨在实现模块化策略的可靠性和效率，通过将探索和利用分类为不同模块，并透过导师强制探索模块来优化探索策略。</li>
<li>methods: 本研究提出了具有实际导师强制的探索导向策略（XGX），具体包括将探索和利用分类为不同模块，并将探索模块强制遵循导师的指导。</li>
<li>results: 在 object navigation 任务上，XGX 可以从 70% 提高到 73%，同时也提高了目标分析中的精度和效率。此外，XGX 还可以在实际机器人硬件上进行适应和转移。<details>
<summary>Abstract</summary>
In the recent progress in embodied navigation and sim-to-robot transfer, modular policies have emerged as a de facto framework. However, there is more to compositionality beyond the decomposition of the learning load into modular components. In this work, we investigate a principled way to syntactically combine these components. Particularly, we propose Exploitation-Guided Exploration (XGX) where separate modules for exploration and exploitation come together in a novel and intuitive manner. We configure the exploitation module to take over in the deterministic final steps of navigation i.e. when the goal becomes visible. Crucially, an exploitation module teacher-forces the exploration module and continues driving an overridden policy optimization. XGX, with effective decomposition and novel guidance, improves the state-of-the-art performance on the challenging object navigation task from 70% to 73%. Along with better accuracy, through targeted analysis, we show that XGX is also more efficient at goal-conditioned exploration. Finally, we show sim-to-real transfer to robot hardware and XGX performs over two-fold better than the best baseline from simulation benchmarking. Project page: xgxvisnav.github.io
</details>
<details>
<summary>摘要</summary>
最近的具体 Navigation 和 sim-to-robot 传输进步中，模块化策略emerged as a de facto framework。然而，有更多的composability beyond the decomposition of the learning load into modular components。在这项工作中，我们investigate a principled way to syntactically combine these components。特别是，我们提出了Exploitation-Guided Exploration (XGX)，它们是novel and intuitive manner。我们将探索模块与目标配置在一起，使得策略优化更加稳定。XGX通过有效的分解和新的导航，提高了对难 Navigation 任务的性能从70%提高到73%。此外，我们通过targeted analysis表明，XGX也更有效果地进行目标 Conditioned exploration。最后，我们在硬件Robot上进行了 sim-to-real 传输，XGX相比baseline的最好值，在实际中表现出了两倍的性能提升。项目页面：xgxvisnav.github.io
</details></li>
</ul>
<hr>
<h2 id="SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis"><a href="#SegGen-Supercharging-Segmentation-Models-with-Text2Mask-and-Mask2Img-Synthesis" class="headerlink" title="SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis"></a>SegGen: Supercharging Segmentation Models with Text2Mask and Mask2Img Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03355">http://arxiv.org/abs/2311.03355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prismformore/seggen">https://github.com/prismformore/seggen</a></li>
<li>paper_authors: Hanrong Ye, Jason Kuen, Qing Liu, Zhe Lin, Brian Price, Dan Xu</li>
<li>For: 提高图像分割模型的性能，特别是在 semantic segmentation、panoptic segmentation 和 instance segmentation 领域。* Methods: 提出了一种名为 SegGen 的高效训练数据生成方法，包括两个数据生成策略：MaskSyn 和 ImgSyn。MaskSyn 通过提出的文本到mask生成模型和mask到图像生成模型来生成新的mask-image对，大幅提高了分割mask的多样性；ImgSyn 使用 existed masks 生成新的图像，强化了图像的多样性。* Results: 在 ADE20K 和 COCO  bencmarks 上，SegGen 对 state-of-the-art 图像分割模型进行了明显的性能提升，包括 semantic segmentation、panoptic segmentation 和 instance segmentation。特别是在 ADE20K mIoU 上，Mask2Former R50 的性能从 47.2 提高到 49.9 (+2.7)，Mask2Former Swin-L 的性能也从 56.1 提高到 57.4 (+1.3)。这些出色的结果表明 SegGen 在训练时使用人工标注数据的情况下具有高效性。此外，使用我们的 sintetic data 进行训练，使得分割模型对不seen domains 变得更加Robust。<details>
<summary>Abstract</summary>
We propose SegGen, a highly-effective training data generation method for image segmentation, which pushes the performance limits of state-of-the-art segmentation models to a significant extent. SegGen designs and integrates two data generation strategies: MaskSyn and ImgSyn. (i) MaskSyn synthesizes new mask-image pairs via our proposed text-to-mask generation model and mask-to-image generation model, greatly improving the diversity in segmentation masks for model supervision; (ii) ImgSyn synthesizes new images based on existing masks using the mask-to-image generation model, strongly improving image diversity for model inputs. On the highly competitive ADE20K and COCO benchmarks, our data generation method markedly improves the performance of state-of-the-art segmentation models in semantic segmentation, panoptic segmentation, and instance segmentation. Notably, in terms of the ADE20K mIoU, Mask2Former R50 is largely boosted from 47.2 to 49.9 (+2.7); Mask2Former Swin-L is also significantly increased from 56.1 to 57.4 (+1.3). These promising results strongly suggest the effectiveness of our SegGen even when abundant human-annotated training data is utilized. Moreover, training with our synthetic data makes the segmentation models more robust towards unseen domains. Project website: https://seggenerator.github.io
</details>
<details>
<summary>摘要</summary>
我们提出SegGen，一种高效训练数据生成方法 для图像分割，可以大幅提高现有状态最佳分割模型的性能。SegGen设计并结合了两种数据生成策略：MaskSyn和ImgSyn。（一）MaskSyn通过我们提出的文本到mask生成模型和mask到图像生成模型来增加分割mask的多样性，以便对模型进行超级visdom；（二）ImgSyn使用现有mask生成新图像，从而强化图像的多样性，为模型输入提供更多的多样性。在ADE20K和COCObenchmark上，我们的数据生成方法明显提高了现有状态最佳分割模型的性能，包括semantic segmentation、panoptic segmentation和instance segmentation。特别是在ADE20K mIoU上，Mask2Former R50从47.2提高到49.9（+2.7），Mask2Former Swin-L也从56.1提高到57.4（+1.3）。这些优秀的结果表明我们的SegGen在有 suffcient human-annotated训练数据的情况下也能够达到高效的性能。此外，使用我们的生成数据训练分割模型可以使其更加鲁棒地处理未看过的领域。项目网站：https://seggenerator.github.io
</details></li>
</ul>
<hr>
<h2 id="GLaMM-Pixel-Grounding-Large-Multimodal-Model"><a href="#GLaMM-Pixel-Grounding-Large-Multimodal-Model" class="headerlink" title="GLaMM: Pixel Grounding Large Multimodal Model"></a>GLaMM: Pixel Grounding Large Multimodal Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03356">http://arxiv.org/abs/2311.03356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Erix Xing, Ming-Hsuan Yang, Fahad S. Khan<br>for:GLaMM is designed to generate natural language responses that are seamlessly intertwined with corresponding object segmentation masks, allowing for visually grounded conversations.methods:GLaMM uses region-level Large Multimodal Models (LMMs) and a comprehensive evaluation protocol to generate densely grounded conversations. The model is flexible and can accept both textual and visual prompts as input.results:GLaMM achieves state-of-the-art performance on the Grounded Conversation Generation (GCG) task and several downstream tasks, such as referring expression segmentation, image and region-level captioning, and vision-language conversations. The model is able to generate visually grounded responses that are flexible and can be interacted with at various levels of granularity.<details>
<summary>Abstract</summary>
Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial efforts towards LMMs used holistic images and text prompts to generate ungrounded textual responses. Very recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring a single object category at a time, require users to specify the regions in inputs, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of generating visually grounded detailed conversations, we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed Grounded Conversation Generation (GCG) task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks e.g., referring expression segmentation, image and region-level captioning and vision-language conversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.
</details>
<details>
<summary>摘要</summary>
大型多Modal模型（LMM）扩展了大型语言模型到视觉领域。初期尝试中使用整体图像和文本提示来生成未经定义的文本响应。在最近的几年中，区域级LMM被用来生成相对定义的响应。然而，它们只能同时引用一个对象类型，需要用户在输入中指定区域，或者无法提供密集像素粒度的对象定位。在这项工作中，我们提出了基于语言模型的对象定位模型（GLaMM），可以生成与对应的语言响应同时包含对象分割mask。GLaMM不仅可以在对话中引用出现的对象，还可以接受文本和可选的视觉提示（区域兴趣点）作为输入，这使得用户可以在文本和视觉领域之间进行交互，并且可以在不同的粒度水平上与模型进行交互。由于没有适用于这种新的设定下生成视觉定义的标准评价准则，我们提出了一种完整的评价协议，并使用我们自己的筛选和标注过程制定了一个大规模的Grounding-anything数据集（GranD），包含7.5万个特定的概念在自然场景中的密集定位。此外，GLaMM还在多个下游任务上表现出色，如图像和区域级captioning、视觉语言对话等。项目页面：https://mbzuai-oryx.github.io/groundingLMM。
</details></li>
</ul>
<hr>
<h2 id="Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation"><a href="#Scalable-and-Transferable-Black-Box-Jailbreaks-for-Language-Models-via-Persona-Modulation" class="headerlink" title="Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation"></a>Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03348">http://arxiv.org/abs/2311.03348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rusheb Shah, Quentin Feuillade–Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando</li>
<li>for: This paper is written to investigate the vulnerability of large language models to jailbreak prompts and to demonstrate a black-box jailbreaking method using persona modulation.</li>
<li>methods: The paper uses a language model assistant to automate the generation of jailbreaks and demonstrates the effectiveness of persona modulation in achieving harmful completions.</li>
<li>results: The paper shows that persona modulation can achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation. The prompts also transfer to other commercial language models, such as Claude 2 and Vicuna, with high harmful completion rates.<details>
<summary>Abstract</summary>
Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. In this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. Rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. We demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. These automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%). These prompts also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively. Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.
</details>
<details>
<summary>摘要</summary>
尽管努力对大型语言模型进行安全调整，它们仍然易受到破坏指令的攻击。在这项工作中，我们调查人格模式修饰作为黑盒破坏方法，以让目标模型采取愿意遵从危险指令的个性。而不是手动制作每个人格的提示，我们自动生成破坏，使用语言模型助手。我们示例了由人格修饰带来的危险完成，包括合成甲基酸酯、制造炸弹和洗钱等详细指令。这些自动攻击的危险完成率为GPT-4的42.5%，比之前修饰前的0.23%高185倍。这些提示还传递到Claude 2和Vicuna，它们的危险完成率分别为61.0%和35.9%。我们的工作揭示了商业大型语言模型又一处漏洞，并高亮了更加全面的安全保障的需要。
</details></li>
</ul>
<hr>
<h2 id="Embedding-First-Order-Logic-into-Kernel-Machines"><a href="#Embedding-First-Order-Logic-into-Kernel-Machines" class="headerlink" title="Embedding First Order Logic into Kernel Machines"></a>Embedding First Order Logic into Kernel Machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03340">http://arxiv.org/abs/2311.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini</li>
<li>for:  Integrating supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines.</li>
<li>methods:  Multi-task learning scheme that jointly learns multiple predicates defined on a set of objects, enforcing FOL constraints on the admissible configurations of their values.</li>
<li>results:  A general approach to convert FOL clauses into a continuous implementation that can deal with the outputs computed by kernel-based predicates, and a two-stage learning schema to avoid poor solutions.<details>
<summary>Abstract</summary>
In this paper we propose a general framework to integrate supervised and unsupervised examples with background knowledge expressed by a collection of first-order logic clauses into kernel machines. In particular, we consider a multi-task learning scheme where multiple predicates defined on a set of objects are to be jointly learned from examples, enforcing a set of FOL constraints on the admissible configurations of their values. The predicates are defined on the feature spaces, in which the input objects are represented, and can be either known a priori or approximated by an appropriate kernel-based learner. A general approach is presented to convert the FOL clauses into a continuous implementation that can deal with the outputs computed by the kernel-based predicates. The learning problem is formulated as a semi-supervised task that requires the optimization in the primal of a loss function that combines a fitting loss measure on the supervised examples, a regularization term, and a penalty term that enforces the constraints on both the supervised and unsupervised examples. Unfortunately, the penalty term is not convex and it can hinder the optimization process. However, it is possible to avoid poor solutions by using a two stage learning schema, in which the supervised examples are learned first and then the constraints are enforced.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种通用框架，用于将监督和无监督示例和背景知识表示为一 colección of first-order logic clauses integrate into kernel machines。特别是，我们考虑了一种多任务学习方案，其中多个PredicateDefined on a set of objects是要同时学习从示例中，并且要满足一些 FOL 约束。这些Predicate可以是知道的或者通过适当的 kernel-based learner来approximation。我们提出了一种通用的方法，将 FOL 条件转换成可以处理 kernel-based predicates 的连续实现。学习问题被表述为一种半监督学习问题，其中需要优化 primal 中的损失函数，该损失函数组合监督示例中的适应损失、规范项和约束项。可惜的是，约束项不是凸函数，可能会阻碍优化过程。然而，可以使用 two-stage 学习 schema，先学习监督示例，然后强制执行约束。
</details></li>
</ul>
<hr>
<h2 id="ProPath-Disease-Specific-Protein-Language-Model-for-Variant-Pathogenicity"><a href="#ProPath-Disease-Specific-Protein-Language-Model-for-Variant-Pathogenicity" class="headerlink" title="ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity"></a>ProPath: Disease-Specific Protein Language Model for Variant Pathogenicity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03429">http://arxiv.org/abs/2311.03429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huixin Zhan, Zijun, Zhang</li>
<li>for: 预测疾病相关的遗传变异是现代生物医学中的一项重要挑战。</li>
<li>methods: 我们提出了一种疾病特定的蛋白语言模型，即ProPath，以估计罕见missense变异中的pseudo-log-likelihood比率。我们使用了一个siamese网络来捕捉这种比率。</li>
<li>results: 我们的结果表明，ProPath比预先训练的ESM1b提高了超过5%的AUC，在遗传心肌病和心Rate病中的临床变异集合上。此外，我们的模型在所有基线上都达到了最高表现。因此，我们的ProPath可以提供高精度的疾病特定变异效应预测，特别有价值于疾病关联和临床应用。<details>
<summary>Abstract</summary>
Clinical variant classification of pathogenic versus benign genetic variants remains a pivotal challenge in clinical genetics. Recently, the proposition of protein language models has improved the generic variant effect prediction (VEP) accuracy via weakly-supervised or unsupervised training. However, these VEPs are not disease-specific, limiting their adaptation at point-of-care. To address this problem, we propose a disease-specific \textsc{pro}tein language model for variant \textsc{path}ogenicity, termed ProPath, to capture the pseudo-log-likelihood ratio in rare missense variants through a siamese network. We evaluate the performance of ProPath against pre-trained language models, using clinical variant sets in inherited cardiomyopathies and arrhythmias that were not seen during training. Our results demonstrate that ProPath surpasses the pre-trained ESM1b with an over $5\%$ improvement in AUC across both datasets. Furthermore, our model achieved the highest performances across all baselines for both datasets. Thus, our ProPath offers a potent disease-specific variant effect prediction, particularly valuable for disease associations and clinical applicability.
</details>
<details>
<summary>摘要</summary>
临床变种分类仍然是临床遗传学的核心挑战。最近，蛋白语言模型的提出已经提高了无监督或弱监督训练下的变iante效应预测（VEP）准确率。然而，这些VEP并不是疾病特有的，因此其应用在临床上受限。为解决这个问题，我们提议一种疾病特有的蛋白语言模型，称为ProPath，以捕捉 Pseudo-log-likelihood ratio 在罕见missense变iante中。我们使用siamese网络对变iante进行评估。我们的结果表明，ProPath 在两个遗传性心血管疾病和心跳过速疾病的临床变iante集中表现出色，与预先训练的 ESM1b 相比，提高了超过 5% 的 AUC 值。此外，我们的模型在所有基线之上表现出最高的性能。因此，我们的 ProPath 提供了一种有力的疾病特有的变iante效应预测，特别有价值于疾病相关性和临床实用性。
</details></li>
</ul>
<hr>
<h2 id="FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2"><a href="#FLOGA-A-machine-learning-ready-dataset-a-benchmark-and-a-novel-deep-learning-model-for-burnt-area-mapping-with-Sentinel-2" class="headerlink" title="FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2"></a>FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03339">http://arxiv.org/abs/2311.03339</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Sdraka, Alkinoos Dimakos, Alexandros Malounis, Zisoula Ntasiou, Konstantinos Karantzalos, Dimitrios Michail, Ioannis Papoutsis</li>
<li>for: 本研究目的是为了提供一种基于机器学习的自动烧区映射方法，以帮助抵制野火的扩散和侵害人类和动物生态环境。</li>
<li>methods: 本研究使用了多种机器学习和深度学习算法，包括支持向量机器学习（SVM）、隐藏状态机器学习（HR-ML）和卷积神经网络（CNN）等，以检测和分类烧区。</li>
<li>results: 研究结果表明，提议的深度学习模型BAM-CD在自动检测和分类烧区方面表现出色，与其他方法相比，具有更高的准确率和更好的鲁棒性。<details>
<summary>Abstract</summary>
Over the last decade there has been an increasing frequency and intensity of wildfires across the globe, posing significant threats to human and animal lives, ecosystems, and socio-economic stability. Therefore urgent action is required to mitigate their devastating impact and safeguard Earth's natural resources. Robust Machine Learning methods combined with the abundance of high-resolution satellite imagery can provide accurate and timely mappings of the affected area in order to assess the scale of the event, identify the impacted assets and prioritize and allocate resources effectively for the proper restoration of the damaged region. In this work, we create and introduce a machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations for the Greek Area). This dataset is unique as it comprises of satellite imagery acquired before and after a wildfire event, it contains information from Sentinel-2 and MODIS modalities with variable spatial and spectral resolution, and contains a large number of events where the corresponding burnt area ground truth has been annotated by domain experts. FLOGA covers the wider region of Greece, which is characterized by a Mediterranean landscape and climatic conditions. We use FLOGA to provide a thorough comparison of multiple Machine Learning and Deep Learning algorithms for the automatic extraction of burnt areas, approached as a change detection task. We also compare the results to those obtained using standard specialized spectral indices for burnt area mapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our benchmark results demonstrate the efficacy of the proposed technique in the automatic extraction of burnt areas, outperforming all other methods in terms of accuracy and robustness. Our dataset and code are publicly available at: https://github.com/Orion-AI-Lab/FLOGA.
</details>
<details>
<summary>摘要</summary>
过去一十年，全球各地的野火频率和强度都在增长，对人类和动物生命、生态系和社会经济稳定带来重大威胁。因此，我们需要迅速行动，以实现抑制野火的影响和保护地球的自然资源。我们使用Robust机器学习方法和充沛的高分辨率卫星图像，实现精准和时间协调的野火范围测量，以评估事件的规模、识别受影响资产和有效地分配资源，以便重建受损区域。在这个工作中，我们创建了一个名为FLOGA（希腊地区森林野火观测）的机器学习Ready数据集。FLOGA的特点是它包含了卫星图像在野火事件之前和后的比较，具有不同的空间和spectral分辨率，并且包含了许多野火事件的burnt区域的实际降落资料，由领域专家 manually annotated。FLOGA覆盖了希腊的广泛地区，其特征是地中海气候和地形。我们使用FLOGA进行了多种机器学习和深度学习算法的自动抽出burnt区域，将其视为变化检测任务。我们还与标准化的特殊 spectral indices for burnt area mapping进行比较。最后，我们提出了一个新的深度学习模型，名为BAM-CD。我们的参考结果表明，提案的技术可以实现自动抽出burnt区域的高精度和可靠性。我们的数据和代码在https://github.com/Orion-AI-Lab/FLOGA 上公开。
</details></li>
</ul>
<hr>
<h2 id="DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase"><a href="#DAIL-Data-Augmentation-for-In-Context-Learning-via-Self-Paraphrase" class="headerlink" title="DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase"></a>DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03319">http://arxiv.org/abs/2311.03319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dawei Li, Yaxuan Li, Dheeraj Mekala, Shuyao Li, Yulin wang, Xueqi Wang, William Hogan, Jingbo Shang</li>
<li>for: The paper is written for improving the performance of In-Context Learning (ICL) in low-resource scenarios.</li>
<li>methods: The paper proposes a method called DAIL (Data Augmentation for In-Context Learning) that leverages the intuition that large language models are more familiar with the content generated by themselves to improve the performance of ICL.</li>
<li>results: The paper shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario, and also explores the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible.<details>
<summary>Abstract</summary>
In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks. However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios. To overcome this limitation, we propose \textbf{D}ata \textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning (\textbf{DAIL}). DAIL leverages the intuition that large language models are more familiar with the content generated by themselves. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario. Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. We believe our work will stimulate further research on ICL in low-resource settings.
</details>
<details>
<summary>摘要</summary>
它们可以使用已经训练过的大语言模型，并且可以使用它们自己生成的内容来增强学习效果。然而，ICL需要高质量的注释示例，这些示例可能在实际场景中不可获得。为了解决这个问题，我们提出了\textbf{DAIL}方法。DAIL利用了大语言模型对自己生成的内容的熟悉程度，首先使用语言模型生成测试样本的重叠版本，然后通过多数投票确定最终结果。我们的实际评估表明，DAIL方法在低资源情况下比标准ICL方法和其他集成方法表现更好。此外，我们还探讨了使用投票一致性作为模型的信任度指标，当预测结果的归一化值不可访问时。我们认为，我们的工作将激发更多关于ICL在低资源情况下的研究。
</details></li>
</ul>
<hr>
<h2 id="Neural-Structure-Learning-with-Stochastic-Differential-Equations"><a href="#Neural-Structure-Learning-with-Stochastic-Differential-Equations" class="headerlink" title="Neural Structure Learning with Stochastic Differential Equations"></a>Neural Structure Learning with Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03309">http://arxiv.org/abs/2311.03309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjie Wang, Joel Jennings, Wenbo Gong<br>for: 这个论文主要目的是为了探索从时间观察中的变量关系的下面的关系，以便更好地理解这些系统的动态。methods: 这篇论文使用了神经网络杂然方程（SDE）和变量推断来推断可能的结构。results: 这篇论文的实验结果表明，使用这种方法可以在不同的时间间隔下更好地推断变量之间的关系，并且在实际数据上比基eline方法表现更好。<details>
<summary>Abstract</summary>
Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, SCOTCH, which combines neural stochastic differential equations (SDE) with variational inference to infer a posterior distribution over possible structures. This continuous-time approach can naturally handle both learning from and predicting observations at arbitrary time points. Theoretically, we establish sufficient conditions for an SDE and SCOTCH to be structurally identifiable, and prove its consistency under infinite data limits. Empirically, we demonstrate that our approach leads to improved structure learning performance on both synthetic and real-world datasets compared to relevant baselines under regular and irregular sampling intervals.
</details>
<details>
<summary>摘要</summary>
发现变量之间的下面关系从暂时观察中获得信息是许多科学领域的长期挑战，包括生物、金融和气候科学。这些系统的动态通常使用连续时间的杂种过程来描述。然而，大多数现有的结构学习方法假设下面过程在离散时间和/或观察时间都是定期的。这些不符合的假设可能会导致错误地学习到的结构和模型。在这种工作中，我们引入了一种新的结构学习方法，即SCOTCH，它将神经网络杂种方程（SDE）与变量推断来推导 posterior distribution over possible structures。这种连续时间的方法可以自然地处理从和预测观察数据的任意时间点进行学习和预测。理论上，我们确定了SDE和SCOTCH的结构可识别条件，并证明其在无穷数据极限下是一致的。实际上，我们在synthetic和实际世界数据集上证明了我们的方法在相对于相关基线下的结构学习性能更高。
</details></li>
</ul>
<hr>
<h2 id="Learning-Reusable-Manipulation-Strategies"><a href="#Learning-Reusable-Manipulation-Strategies" class="headerlink" title="Learning Reusable Manipulation Strategies"></a>Learning Reusable Manipulation Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03293">http://arxiv.org/abs/2311.03293</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiayuan Mao, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling</li>
<li>for: 本研究旨在帮助机器人学习 manipulate “tricks” 技能，从单个示例中学习并应用到不同的情景中。</li>
<li>methods: 该研究使用单个示例和自动游戏来学习机器人 manipulate 技能，并将每个示例解释为机器人对物体和物体之间接触方式的变化序列。</li>
<li>results: 研究实现了机器人通过单个示例和自动游戏学习 manipulate 技能，并可以将这些技能与标准任务和运动规划集成使用。<details>
<summary>Abstract</summary>
Humans demonstrate an impressive ability to acquire and generalize manipulation "tricks." Even from a single demonstration, such as using soup ladles to reach for distant objects, we can apply this skill to new scenarios involving different object positions, sizes, and categories (e.g., forks and hammers). Additionally, we can flexibly combine various skills to devise long-term plans. In this paper, we present a framework that enables machines to acquire such manipulation skills, referred to as "mechanisms," through a single demonstration and self-play. Our key insight lies in interpreting each demonstration as a sequence of changes in robot-object and object-object contact modes, which provides a scaffold for learning detailed samplers for continuous parameters. These learned mechanisms and samplers can be seamlessly integrated into standard task and motion planners, enabling their compositional use.
</details>
<details>
<summary>摘要</summary>
人类具有吸引人的 manipulate "技巧" 学习能力。即使从单一示范（如使用汤匙来达到远方物品），我们也可以应用这种技能到新的enario中，包括不同的物品位置、大小和类别（如锋和锤子）。此外，我们还可以灵活地组合不同的技能，制定长期计划。在这篇论文中，我们提出了一种框架，允许机器通过单一示范和自己玩家来学习 manipulate 技巧，称为"机制"。我们的关键思想在于将每个示范解释为机器人-物品和物品之间的接触模式的序列变化，从而提供了一个框架 для学习细致的抽象器 для连续参数。这些学习的机制和抽象器可以轻松地与标准任务和动作规划器集成，允许其 композиitional 使用。
</details></li>
</ul>
<hr>
<h2 id="GQKVA-Efficient-Pre-training-of-Transformers-by-Grouping-Queries-Keys-and-Values"><a href="#GQKVA-Efficient-Pre-training-of-Transformers-by-Grouping-Queries-Keys-and-Values" class="headerlink" title="GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values"></a>GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03426">http://arxiv.org/abs/2311.03426</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farnoosh Javadi, Walid Ahmed, Habib Hajimolahoseini, Foozhan Ataiefard, Mohammad Hassanpour, Saina Asani, Austin Wen, Omar Mohamed Awad, Kangling Liu, Yang Liu</li>
<li>for: 该论文旨在解决大型转换器模型的几个挑战，包括快速和计算密集的预训练和过参数化。</li>
<li>methods: 该论文提出了一种通用的方法 called GQKVA，它将查询、键和值 групiai技术应用于转换器预训练中。GQKVA是一种可以加速转换器预训练的方法，同时减少模型的大小。我们在不同的GQKVA变体中进行了实验，发现存在一定的平衡关系，可以根据资源和时间限制进行定制选择。</li>
<li>results: 我们对ViT进行了测试，得到了一个约0.3%的准确率提高，同时减少了模型的大小约4%。此外，我们最具攻击性的模型减小实验中，模型大小减少了约15%，准确率下降了约1%。<details>
<summary>Abstract</summary>
Massive transformer-based models face several challenges, including slow and computationally intensive pre-training and over-parametrization. This paper addresses these challenges by proposing a versatile method called GQKVA, which generalizes query, key, and value grouping techniques. GQKVA is designed to speed up transformer pre-training while reducing the model size. Our experiments with various GQKVA variants highlight a clear trade-off between performance and model size, allowing for customized choices based on resource and time limitations. Our findings also indicate that the conventional multi-head attention approach is not always the best choice, as there are lighter and faster alternatives available. We tested our method on ViT, which achieved an approximate 0.3% increase in accuracy while reducing the model size by about 4% in the task of image classification. Additionally, our most aggressive model reduction experiment resulted in a reduction of approximately 15% in model size, with only around a 1% drop in accuracy.
</details>
<details>
<summary>摘要</summary>
巨型变换器模型面临多个挑战，包括慢速和计算昂贵的预训练和过参数化。这篇论文提出了一种通用的方法called GQKVA，该方法通过查询、键和值 grouping技术来加速变换器预训练，同时减少模型的大小。我们的实验表明，GQKVA方法可以在不同的变量和模型大小之间进行定制化，以适应不同的资源和时间限制。我们的发现还表明，传统的多头注意力方法并不总是最佳选择，因为有更轻量级快速的选择可用。我们在ViT上测试了我们的方法，实现了 Image classification 任务中约0.3%的提升精度，同时减少了模型的大小约4%。此外，我们最苛刻的模型减少实验中，模型大小减少了约15%，只有约1%的精度下降。
</details></li>
</ul>
<hr>
<h2 id="S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters"><a href="#S-LoRA-Serving-Thousands-of-Concurrent-LoRA-Adapters" class="headerlink" title="S-LoRA: Serving Thousands of Concurrent LoRA Adapters"></a>S-LoRA: Serving Thousands of Concurrent LoRA Adapters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03285">http://arxiv.org/abs/2311.03285</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/s-lora/s-lora">https://github.com/s-lora/s-lora</a></li>
<li>paper_authors: Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica</li>
<li>for: 这种研究是为了提高大语言模型的批处理服务性能，特别是在使用“预训练后finetune”的情况下。</li>
<li>methods: 这篇论文提出了一种名为S-LoRA的系统，用于批量执行多个LoRA适应器。S-LoRA使用一个主存中存储所有适应器，并在GPU内存中fetch用于当前运行的查询的适应器。为了有效使用GPU内存并降低 Fragmentation，S-LoRA提出了统一分页机制。此外，S-LoRA还使用了一种新的tensor并行执行策略和特化的CUDA加速器，以实现hetersonous批处理LoRA计算。</li>
<li>results: 对比 estado-of-the-art 库such as HuggingFace PEFT和vLLM（带有浅入门支持LoRA服务），S-LoRA可以提高throughput BY up to 4 times，并将可以服务多个任务特定的精度适应器数量增加到数量级。因此，S-LoRA允许批量服务多个任务特定的精度适应器，并且提供了大规模自定义 fine-tuning 服务的潜在能力。<details>
<summary>Abstract</summary>
The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. We observe that this paradigm presents significant opportunities for batched inference during serving. To capitalize on these opportunities, we present S-LoRA, a system designed for the scalable serving of many LoRA adapters. S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead. Compared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving), S-LoRA can improve the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude. As a result, S-LoRA enables scalable serving of many task-specific fine-tuned models and offers the potential for large-scale customized fine-tuning services. The code is available at https://github.com/S-LoRA/S-LoRA
</details>
<details>
<summary>摘要</summary>
“嵌入式-然后资金”的概念广泛应用于语言模型的部署。低级别适应（LoRA），一种效率 Parameter fine-tuning 方法，常用于适应多种任务，从而生成了一大量的 LoRA 适应器。我们发现这种方法在服务时存在大量的批处理机会。为了利用这些机会，我们提出了 S-LoRA，一个可扩展的服务系统，用于批处理多个 LoRA 适应器。S-LoRA 将所有适应器存储在主内存中，并在 GPU 内存中fetch 用于当前运行中的查询的适应器。为了有效地使用 GPU 内存并减少副本，S-LoRA 提出了一种统一分页（Unified Paging）策略，该策略使用一个统一内存池来管理动态适应器权重和 KV 缓存tensor 的不同序列长度。此外，S-LoRA 还采用了一种新的tensor并行策略和高度优化的自定义 CUDA 加速器，以实现hetereogeneous批处理 LoRA 计算。这些特性使得 S-LoRA 能够在单个 GPU 或多个 GPU 上服务千个 LoRA 适应器，只有小 overhead。相比之下，使用 HuggingFace PEFT 和 vLLM（带有简单的 LoRA 服务支持）的状态态归，S-LoRA 可以提高吞吐量，并将 LoRA 适应器的数量提高到几个数量级。因此，S-LoRA 允许批处理多个任务特定的精心定制化模型，并提供了大规模定制化 fine-tuning 服务的潜在能力。代码可以在 <https://github.com/S-LoRA/S-LoRA> 上找到。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Guided-Data-Centric-Strategy-to-Detect-and-Mitigate-Biases-in-Healthcare-Datasets"><a href="#An-AI-Guided-Data-Centric-Strategy-to-Detect-and-Mitigate-Biases-in-Healthcare-Datasets" class="headerlink" title="An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets"></a>An AI-Guided Data Centric Strategy to Detect and Mitigate Biases in Healthcare Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03425">http://arxiv.org/abs/2311.03425</a></li>
<li>repo_url: None</li>
<li>paper_authors: Faris F. Gulamali, Ashwin S. Sawant, Lora Liharska, Carol R. Horowitz, Lili Chan, Patricia H. Kovatch, Ira Hofer, Karandeep Singh, Lynne D. Richardson, Emmanuel Mensah, Alexander W Charney, David L. Reich, Jianying Hu, Girish N. Nadkarni</li>
<li>for: 这篇论文的目的是提出一种数据中心、模型无关、任务无关的方法来评估医疗数据中的偏见，以便逐出和修正偏见。</li>
<li>methods: 这篇论文使用了一种名为AEquity的新的评估指标，通过分析不同群体在小样本大小下的学习容易度，来评估医疗数据中的偏见。</li>
<li>results: 这篇论文通过应用AEquity指标在两个已知的医疗问题中，分别是使用深度 convolutional neural networks 进行胸部X射影像诊断和使用多变量逻辑回归进行医疗使用预测，发现了一些种族偏见的manifestation，并通过修正这些偏见来提高医疗数据的公平性。<details>
<summary>Abstract</summary>
The adoption of diagnosis and prognostic algorithms in healthcare has led to concerns about the perpetuation of bias against disadvantaged groups of individuals. Deep learning methods to detect and mitigate bias have revolved around modifying models, optimization strategies, and threshold calibration with varying levels of success. Here, we generate a data-centric, model-agnostic, task-agnostic approach to evaluate dataset bias by investigating the relationship between how easily different groups are learned at small sample sizes (AEquity). We then apply a systematic analysis of AEq values across subpopulations to identify and mitigate manifestations of racial bias in two known cases in healthcare - Chest X-rays diagnosis with deep convolutional neural networks and healthcare utilization prediction with multivariate logistic regression. AEq is a novel and broadly applicable metric that can be applied to advance equity by diagnosing and remediating bias in healthcare datasets.
</details>
<details>
<summary>摘要</summary>
随着医疗健康预测和诊断算法在医疗领域的推广，对弱势群体的偏见问题产生了关注。深度学习方法来检测和缓解偏见的发展囊括了修改模型、优化策略和阈值准确化等方法，其成果各有不同。在这里，我们提出了一种数据中心、模型无关、任务无关的方法，通过研究小样本大小下不同群体学习的关系（AEquity）来评估数据集偏见。然后，我们采用了系统性的分析方法，在两个已知的医疗案例中（医疗X射像诊断与深度径向分割神经网络，以及医疗利用预测与多变量Logistic回归）检测和缓解种族偏见。AEquity是一种新的和普遍适用的指标，可以在医疗领域中提高公平性，诊断和缓解偏见。
</details></li>
</ul>
<hr>
<h2 id="Using-Symmetries-to-Lift-Satisfiability-Checking"><a href="#Using-Symmetries-to-Lift-Satisfiability-Checking" class="headerlink" title="Using Symmetries to Lift Satisfiability Checking"></a>Using Symmetries to Lift Satisfiability Checking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03424">http://arxiv.org/abs/2311.03424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Carbonnelle, Gottfried Schenner, Maurice Bruynooghe, Bart Bogaerts, Marc Denecker</li>
<li>for: 这个论文旨在使用对称性来压缩结构（也称为解释）到更小的Domain中，而不会产生信息损失。这种压缩可以用来解决满足问题。</li>
<li>methods: 该论文提出了一种两步新方法：首先，将要满足的句子自动翻译成一个equisatisfiable句子 sobre一个“升高” vocabulary， allowing domain compression; 其次，检查升高句子的满足状态，通过扩展初始未知的压缩Domain until a satisfying structure is found。关键问题是确保这个满足结构可以扩展到一个不压缩的结构，使其满足原始句子。</li>
<li>results: 实验表明，对于生成配置问题，该方法可以获得大量的加速。该方法还有软件验证软件操作 Complex Data Structures 的应用。未来的工作将包括对翻译进行进一步优化。<details>
<summary>Abstract</summary>
We analyze how symmetries can be used to compress structures (also known as interpretations) onto a smaller domain without loss of information. This analysis suggests the possibility to solve satisfiability problems in the compressed domain for better performance. Thus, we propose a 2-step novel method: (i) the sentence to be satisfied is automatically translated into an equisatisfiable sentence over a ``lifted'' vocabulary that allows domain compression; (ii) satisfiability of the lifted sentence is checked by growing the (initially unknown) compressed domain until a satisfying structure is found. The key issue is to ensure that this satisfying structure can always be expanded into an uncompressed structure that satisfies the original sentence to be satisfied. We present an adequate translation for sentences in typed first-order logic extended with aggregates. Our experimental evaluation shows large speedups for generative configuration problems. The method also has applications in the verification of software operating on complex data structures. Further refinements of the translation are left for future work.
</details>
<details>
<summary>摘要</summary>
我们分析了如何使用对称性来压缩结构（也称为解释）到更小的领域 без损失信息。这种分析表明可以在压缩领域中更好地解决满足问题。因此，我们提出了一种新的两步方法：1. 需要满足的句子会被自动翻译成一个等价满足句子 sobre 一个“升级” vocabulary，该 vocabulary 允许领域压缩。2. 压缩领域中的满足性会通过扩展初始未知的压缩领域来检查。关键在于确保这个满足结构可以总是扩展到一个满足原始句子的不压缩结构。我们对类型 first-order logic 扩展了聚合的句子进行了适当的翻译。我们的实验评估表明，这种方法可以在生成配置问题中获得大量的加速。这种方法还有软件操作复杂数据结构的验证应用。未来的工作将进行进一步的翻译改进。
</details></li>
</ul>
<hr>
<h2 id="From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach"><a href="#From-Coupled-Oscillators-to-Graph-Neural-Networks-Reducing-Over-smoothing-via-a-Kuramoto-Model-based-Approach" class="headerlink" title="From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach"></a>From Coupled Oscillators to Graph Neural Networks: Reducing Over-smoothing via a Kuramoto Model-based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03260">http://arxiv.org/abs/2311.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tan M. Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo Nakamura</li>
<li>For: 这篇论文旨在开发一种具有缓和抑制过滤现象的连续深度图像神经网络（GNN），并且使用科拉莫托模型来减轻这个现象。* Methods: 这篇论文使用的方法包括基于科拉莫托模型的频率同步方法，以预防节点特征汇整而不让系统降入稳定同步状态。* Results: 实验结果显示，使用科拉莫托GNN比基于GNN的基本方法和现有方法更有效地减少过滤现象，并且在各种图像深度学习 benchmark 任务中表现更好。<details>
<summary>Abstract</summary>
We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of continuous-depth graph neural networks (GNNs) that employs the Kuramoto model to mitigate the over-smoothing phenomenon, in which node features in GNNs become indistinguishable as the number of layers increases. The Kuramoto model captures the synchronization behavior of non-linear coupled oscillators. Under the view of coupled oscillators, we first show the connection between Kuramoto model and basic GNN and then over-smoothing phenomenon in GNNs can be interpreted as phase synchronization in Kuramoto model. The KuramotoGNN replaces this phase synchronization with frequency synchronization to prevent the node features from converging into each other while allowing the system to reach a stable synchronized state. We experimentally verify the advantages of the KuramotoGNN over the baseline GNNs and existing methods in reducing over-smoothing on various graph deep learning benchmark tasks.
</details>
<details>
<summary>摘要</summary>
我们提出了kuramoto图 neural network（KuramotoGNN），一种新的连续深度图 нейрон网络（GNNs），它利用kuramoto模型来减轻过滤现象，在图 neural network中，节点特征会在层数增加时变得不可分辨。kuramoto模型捕捉了非线性 Coupled oscilator的同步行为。从coupled oscilator的角度来看，我们首先显示了kuramoto模型和基本的GNN之间的连接，然后说明了GNN中的过滤现象可以被解释为kuramoto模型中的相干同步。KuramotoGNN将取代这个相干同步，使节点特征不会与其他节点的特征相似，并让系统可以 дости到一个稳定的同步化状态。我们通过实验证明了KuramotoGNN在不同的图深度学习 benchmark task上的优势，比如降低过滤现象。
</details></li>
</ul>
<hr>
<h2 id="Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency"><a href="#Coherent-Entity-Disambiguation-via-Modeling-Topic-and-Categorical-Dependency" class="headerlink" title="Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency"></a>Coherent Entity Disambiguation via Modeling Topic and Categorical Dependency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03253">http://arxiv.org/abs/2311.03253</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilin Xiao, Linjun Shou, Xingyao Zhang, Jie Wu, Ming Gong, Jian Pei, Daxin Jiang</li>
<li>for: 提高实体识别精度和准确性</li>
<li>methods: 使用无监督自变量自动encoder（VAE）提取文本句子 latent topic vector，并通过步骤性实体决策来模拟实体之间交互，保持实体分类层次的准确性</li>
<li>results: 在各种实体识别 benchmark 上达到新的状态则末点，具体提高1.3个F1分数点，特别是在长文本场景下表现出色<details>
<summary>Abstract</summary>
Previous entity disambiguation (ED) methods adopt a discriminative paradigm, where prediction is made based on matching scores between mention context and candidate entities using length-limited encoders. However, these methods often struggle to capture explicit discourse-level dependencies, resulting in incoherent predictions at the abstract level (e.g. topic or category). We propose CoherentED, an ED system equipped with novel designs aimed at enhancing the coherence of entity predictions. Our method first introduces an unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences. This approach not only allows the encoder to handle longer documents more effectively, conserves valuable input space, but also keeps a topic-level coherence. Additionally, we incorporate an external category memory, enabling the system to retrieve relevant categories for undecided mentions. By employing step-by-step entity decisions, this design facilitates the modeling of entity-entity interactions, thereby maintaining maximum coherence at the category level. We achieve new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points. Our model demonstrates particularly outstanding performance on challenging long-text scenarios.
</details>
<details>
<summary>摘要</summary>
We propose CoherentED, an ED system with novel designs to enhance the coherence of entity predictions. Our method first uses an unsupervised variational autoencoder (VAE) to extract latent topic vectors of context sentences. This approach not only allows the encoder to handle longer documents more effectively, but also keeps the topic-level coherence. Additionally, we incorporate an external category memory, enabling the system to retrieve relevant categories for undecided mentions. By employing step-by-step entity decisions, this design facilitates the modeling of entity-entity interactions, thereby maintaining maximum coherence at the category level.We achieve new state-of-the-art results on popular ED benchmarks, with an average improvement of 1.3 F1 points. Our model demonstrates particularly outstanding performance on challenging long-text scenarios.
</details></li>
</ul>
<hr>
<h2 id="Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers"><a href="#Instructed-Language-Models-with-Retrievers-Are-Powerful-Entity-Linkers" class="headerlink" title="Instructed Language Models with Retrievers Are Powerful Entity Linkers"></a>Instructed Language Models with Retrievers Are Powerful Entity Linkers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03250">http://arxiv.org/abs/2311.03250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrzilinxiao/insgenentitylinking">https://github.com/mrzilinxiao/insgenentitylinking</a></li>
<li>paper_authors: Zilin Xiao, Ming Gong, Jie Wu, Xingyao Zhang, Linjun Shou, Jian Pei, Daxin Jiang</li>
<li>for: 本研究旨在将生成模型（LLMs）应用于实体连接（EL）任务中，以提高EL任务的精度和效率。</li>
<li>methods: 本研究提出了多种方法来充分利用生成模型的能力，包括：（i）基于序列训练的EL目标函数与指令调整，（ii）一种基于轻量级可能提及者搜索的新一代生成EL框架，实现4倍的速度提升而无需牺牲连接度。</li>
<li>results: 对比前一代生成模型，INSGENEL显示了+6.8 F1点的提升平均值，同时具有更高的训练数据效率和训练计算消耗效率。此外，对比ICL框架，INSGENEL仍然保持明显的优势，证明EL任务仍然是通用LLMs的一大难点。<details>
<summary>Abstract</summary>
Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4$\times$ speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered in-context learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>生成方法由大型语言模型（LLM）驱动已经显示出了复杂逻辑能力的emergent能力。然而，生成性仍然使得生成内容受到幻觉的干扰，因此不适用于实体关注 tasks需要精确的实体预测 над 大量知识库。我们提出了首个可以让习语言模型执行实体关注 task的 Instructed Generative Entity Linker (INSGENEL)。这种方法包括（i）基于序列训练EL目标并进行指令调整的EL objective sequence-to-sequence训练，（ii）基于轻量级的可能提取器来实现快速的generative EL框架，从而实现4倍的速度提升而无需牺牲链接度量。 INSGENEL在前一代生成方法上average上提高了+6.8 F1点，同时具有更高的训练数据效率和训练计算耗用率。此外，我们 skillfully engineering的ICL框架仍然远远落后于INSGENEL，这再次证明了EL任务仍然是通用LLMs的难题。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting"><a href="#Advancing-Post-Hoc-Case-Based-Explanation-with-Feature-Highlighting" class="headerlink" title="Advancing Post Hoc Case Based Explanation with Feature Highlighting"></a>Advancing Post Hoc Case Based Explanation with Feature Highlighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03246">http://arxiv.org/abs/2311.03246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eoin Kenny, Eoin Delaney, Mark Keane</li>
<li>for: 本研究旨在提高透明AI（XAI）技术的应用范围，以帮助人工智能和人类合作下游任务。</li>
<li>methods: 本研究提出了两种通用算法（幽Defaults和super pixel based），可以在测试图像中分解出多个清晰特征部分，然后与训练数据中的相关案例相连接，以提供更全面的解释。</li>
<li>results: 实验结果表明，提议的方法可以正确地调整用户对于不确定分类结果的情感，并且不同于只显示解释而不显示特征高亮的情况。<details>
<summary>Abstract</summary>
Explainable AI (XAI) has been proposed as a valuable tool to assist in downstream tasks involving human and AI collaboration. Perhaps the most psychologically valid XAI techniques are case based approaches which display 'whole' exemplars to explain the predictions of black box AI systems. However, for such post hoc XAI methods dealing with images, there has been no attempt to improve their scope by using multiple clear feature 'parts' of the images to explain the predictions while linking back to relevant cases in the training data, thus allowing for more comprehensive explanations that are faithful to the underlying model. Here, we address this gap by proposing two general algorithms (latent and super pixel based) which can isolate multiple clear feature parts in a test image, and then connect them to the explanatory cases found in the training data, before testing their effectiveness in a carefully designed user study. Results demonstrate that the proposed approach appropriately calibrates a users feelings of 'correctness' for ambiguous classifications in real world data on the ImageNet dataset, an effect which does not happen when just showing the explanation without feature highlighting.
</details>
<details>
<summary>摘要</summary>
Explainable AI（XAI）已被提议为在人类和AI合作下执行下游任务的有价值工具。可能最有心理有效的XAI技术是情况基 Approaches，通过显示“整体”的示例来解释黑盒 AI 系统的预测。然而，对于图像处理的Post hoc XAI 方法，没有尝试使用多个清晰特征“部分”来解释预测结果，并将其联系回训练数据中相关的案例，因此可以提供更全面的解释， faithful to the underlying model。在这里，我们关注这个 gap，并提出了两种通用算法（幽默和超Pixel），可以在测试图像中分解出多个清晰特征部分，然后将其与训练数据中的解释案例相连接，并在一个仔细设计的用户研究中测试其效果。结果表明，我们提出的方法可以正确地让用户对于不确定分类结果的情感进行调整，这种效果在真实世界数据集 ImageNet 上不会发生。
</details></li>
</ul>
<hr>
<h2 id="An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding"><a href="#An-Efficient-Self-Supervised-Cross-View-Training-For-Sentence-Embedding" class="headerlink" title="An Efficient Self-Supervised Cross-View Training For Sentence Embedding"></a>An Efficient Self-Supervised Cross-View Training For Sentence Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03228">http://arxiv.org/abs/2311.03228</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrpeerat/sct">https://github.com/mrpeerat/sct</a></li>
<li>paper_authors: Peerat Limkonchotiwat, Wuttikorn Ponwitayarat, Lalita Lowphansirikul, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong</li>
<li>for:  sentence representation learning, specifically self-supervised learning without human annotation efforts</li>
<li>methods:  contrastive learning, and a proposed framework called Self-supervised Cross-View Training (SCT)</li>
<li>results:  outperforms baseline and state-of-the-art competitors on seven Semantic Textual Similarity (STS) benchmarks for PLMs with less than 100M parameters in 18 of 21 cases.<details>
<summary>Abstract</summary>
Self-supervised sentence representation learning is the task of constructing an embedding space for sentences without relying on human annotation efforts. One straightforward approach is to finetune a pretrained language model (PLM) with a representation learning method such as contrastive learning. While this approach achieves impressive performance on larger PLMs, the performance rapidly degrades as the number of parameters decreases. In this paper, we propose a framework called Self-supervised Cross-View Training (SCT) to narrow the performance gap between large and small PLMs. To evaluate the effectiveness of SCT, we compare it to 5 baseline and state-of-the-art competitors on seven Semantic Textual Similarity (STS) benchmarks using 5 PLMs with the number of parameters ranging from 4M to 340M. The experimental results show that STC outperforms the competitors for PLMs with less than 100M parameters in 18 of 21 cases.
</details>
<details>
<summary>摘要</summary>
自动监督句子表示学习是建立句子嵌入空间的任务，而不需要人工标注努力。一种直观的方法是通过对预训练语言模型（PLM）进行微调，并使用对比学习法来学习句子表示。虽然这种方法在更大的PLM上达到了很高的性能，但是在小PLM上，性能快速下降。在这篇论文中，我们提出了一个名为自动监督交叉视图训练（SCT）的框架，以减少小PLM上表达力的差异。为评估SCT的有效性，我们与5个基准和现有的竞争对手进行比较，在7个Semantic Textual Similarity（STS）标准测试集上使用5种PLM，它们的参数数量从4M到340M。实验结果表明，SCT在PLM参数少于100M时在18/21个测试中表现更好。
</details></li>
</ul>
<hr>
<h2 id="LDM3D-VR-Latent-Diffusion-Model-for-3D-VR"><a href="#LDM3D-VR-Latent-Diffusion-Model-for-3D-VR" class="headerlink" title="LDM3D-VR: Latent Diffusion Model for 3D VR"></a>LDM3D-VR: Latent Diffusion Model for 3D VR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03226">http://arxiv.org/abs/2311.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriela Ben Melech Stan, Diana Wofk, Estelle Aflalo, Shao-Yen Tseng, Zhipeng Cai, Michael Paulitsch, Vasudev Lal</li>
<li>for: 这 paper 是为了创建和 manipulate 视觉输出而设计的。</li>
<li>methods: 这 paper 使用了潜在差分模型，包括 LDM3D-pano 和 LDM3D-SR。这些模型可以根据文本提示生成 panoramic RGBD，并将低分辨率输入upscale 到高分辨率 RGBD。</li>
<li>results: 这 paper 的模型在比较related 方法的基础上进行了评估，并取得了优于 relate 方法的结果。<details>
<summary>Abstract</summary>
Latent diffusion models have proven to be state-of-the-art in the creation and manipulation of visual outputs. However, as far as we know, the generation of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of diffusion models targeting virtual reality development that includes LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD based on textual prompts and the upscaling of low-resolution inputs to high-resolution RGBD, respectively. Our models are fine-tuned from existing pretrained models on datasets containing panoramic/high-resolution RGB images, depth maps and captions. Both models are evaluated in comparison to existing related methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Latent diffusion models have proven to be state-of-the-art in the creation and manipulation of visual outputs. However, as far as we know, the generation of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of diffusion models targeting virtual reality development that includes LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD based on textual prompts and the upscaling of low-resolution inputs to high-resolution RGBD, respectively. Our models are fine-tuned from existing pretrained models on datasets containing panoramic/high-resolution RGB images, depth maps and captions. Both models are evaluated in comparison to existing related methods.<</SYS>>Here's the translation in Traditional Chinese:<<SYS>>Latent diffusion models have proven to be state-of-the-art in the creation and manipulation of visual outputs. However, as far as we know, the generation of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite of diffusion models targeting virtual reality development that includes LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD based on textual prompts and the upscaling of low-resolution inputs to high-resolution RGBD, respectively. Our models are fine-tuned from existing pretrained models on datasets containing panoramic/high-resolution RGB images, depth maps and captions. Both models are evaluated in comparison to existing related methods.<</SYS>>
</details></li>
</ul>
<hr>
<h2 id="ALYMPICS-Language-Agents-Meet-Game-Theory"><a href="#ALYMPICS-Language-Agents-Meet-Game-Theory" class="headerlink" title="ALYMPICS: Language Agents Meet Game Theory"></a>ALYMPICS: Language Agents Meet Game Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03220">http://arxiv.org/abs/2311.03220</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, Furu Wei</li>
<li>for: 这篇论文旨在利用大型自然语言模型（LLM）代理人类 investigate 游戏理论。</li>
<li>methods: 论文使用 LLM 和自动化代理人 simulate 人类行为，实现多代理人合作，构建人类交互的真实和动态模型，用于游戏理论假设设计和测试。</li>
<li>results: 通过 manipulating 资源可用性和代理人性格，论文 observe 不同的代理人在竞争中如何互动和采取策略，并证明 LLM 代理人在游戏理论研究中具有优势，包括模拟真实行为、提供可控、可扩展和可重现的环境。<details>
<summary>Abstract</summary>
This paper introduces Alympics, a platform that leverages Large Language Model (LLM) agents to facilitate investigations in game theory. By employing LLMs and autonomous agents to simulate human behavior and enable multi-agent collaborations, we can construct realistic and dynamic models of human interactions for game theory hypothesis formulating and testing. To demonstrate this, we present and implement a survival game involving unequal competition for limited resources. Through manipulation of resource availability and agent personalities, we observe how different agents engage in the competition and adapt their strategies. The use of LLM agents in game theory research offers significant advantages, including simulating realistic behavior, providing a controlled, scalable, and reproducible environment. Our work highlights the potential of LLM agents in enhancing the understanding of strategic decision-making within complex socioeconomic contexts. All codes will be made public soon.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了Alympics平台，该平台利用大语言模型（LLM）代理来促进游戏理论研究。通过使用LLM和自主代理模拟人类行为，我们可以构建真实和动态的人类互动模型，用于游戏理论假设设计和测试。为了证明这一点，我们在一个有限资源的存储游戏中展示了不同代理的竞争和战略适应。通过资源可用性和代理人格的调整，我们观察了不同的代理如何在竞争中 engagé和适应策略。使用LLM代理在游戏理论研究中具有重要优点，包括模拟真实行为、提供可控、可扩展和可重现的环境。我们的工作强调了LLM代理在复杂社会经济背景下的决策推理的深入理解的潜在优势。所有代码即将公开。
</details></li>
</ul>
<hr>
<h2 id="Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models"><a href="#Mini-Minds-Exploring-Bebeshka-and-Zlata-Baby-Models" class="headerlink" title="Mini Minds: Exploring Bebeshka and Zlata Baby Models"></a>Mini Minds: Exploring Bebeshka and Zlata Baby Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03216">http://arxiv.org/abs/2311.03216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/upunaprosk/small-language-models">https://github.com/upunaprosk/small-language-models</a></li>
<li>paper_authors: Irina Proskurina, Guillaume Metzler, Julien Velcin</li>
<li>for: 本研究是为了提出一种小规模语言模型，用于解决从头开始，使用有限数据和人类语言学习的语言模型问题。</li>
<li>methods: 该研究使用了架构搜索，以最小化 Shared Task 上的假cloaked语言模型损失。并提出了两个小型语言模型（LMs），即 Bebeshka 和 Zlata，它们具有4层encoder和6层decoder，每个encoder层有8个注意头，每个decoder层有12个注意头。</li>
<li>results: 该研究发现， despite being half the scale of the baseline LMs, 提出的小型语言模型可以达到相同的性能水平。此外，研究还探讨了小型语言模型在道德判断任务中的应用性，并对其预测与人类价值观Alignment。这些发现表明小型语言模型在实际语言理解任务中具有潜在的应用价值。<details>
<summary>Abstract</summary>
In this paper, we describe the University of Lyon 2 submission to the Strict-Small track of the BabyLM competition. The shared task is created with an emphasis on small-scale language modelling from scratch on limited-size data and human language acquisition. Dataset released for the Strict-Small track has 10M words, which is comparable to children's vocabulary size. We approach the task with an architecture search, minimizing masked language modelling loss on the data of the shared task. Having found an optimal configuration, we introduce two small-size language models (LMs) that were submitted for evaluation, a 4-layer encoder with 8 attention heads and a 6-layer decoder model with 12 heads which we term Bebeshka and Zlata, respectively. Despite being half the scale of the baseline LMs, our proposed models achieve comparable performance. We further explore the applicability of small-scale language models in tasks involving moral judgment, aligning their predictions with human values. These findings highlight the potential of compact LMs in addressing practical language understanding tasks.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们描述了里昂第二大学对Strict-Small赛道的提交。这个任务强调从头来学习小规模语言模型，使用有限数据量和人类语言学习的限制。共享任务数据集有1000万个单词，与儿童词汇相当。我们通过架构搜索，在数据集上减少假Masked Language Modeling损失来解决问题。我们发现了一个优化的配置，然后引入了两个小型语言模型（LM），我们称之为Bebeshka和Zlata。它们具有4层Encoder和6层Decoder，各自具有8个注意头和12个注意头。虽然它们的规模只占基elineLM的一半，但它们在性能上却达到了相同的水平。我们还探索了小型语言模型在道德评价任务中的应用性，并将其预测与人类价值观Alignment。这些发现表明了小型语言模型在实际语言理解任务中的潜力。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition"><a href="#Pseudo-Labeling-for-Domain-Agnostic-Bangla-Automatic-Speech-Recognition" class="headerlink" title="Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition"></a>Pseudo-Labeling for Domain-Agnostic Bangla Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03196">http://arxiv.org/abs/2311.03196</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr">https://github.com/hishab-nlp/pseudo-labeling-for-domain-agnostic-bangla-asr</a></li>
<li>paper_authors: Rabindra Nath Nandi, Mehadi Hasan Menon, Tareq Al Muntasir, Sagor Sarker, Quazi Sarwar Muhtaseem, Md. Tariqul Islam, Shammur Absar Chowdhury, Firoj Alam</li>
<li>for: 本研究旨在开发一种自动语音识别（ASR）系统，以满足低资源语言的语音识别需求。</li>
<li>methods: 本研究提出了一种 pseudo-labeling 方法，以开发大规模的领域不偏的 ASR 数据集。通过该方法，我们开发了20000+小时的标注的孟加拉语音数据集，覆盖了多种话题、说话风格、方言、噪音环境和对话场景。</li>
<li>results: 我们使用了开发的 corpus 设计了一个基于幂体的 ASR 系统，并对其进行了训练。我们对公开available的数据集进行了比较，并与其他可用的模型进行了比较。我们还设计了和human-annotated的领域不偏测试集，包括新闻、电话和对话数据等。我们的结果表明，使用 pseudo-labeling 方法对于我们设计的测试集以及公开available的孟加拉语音数据集具有较高的效果。<details>
<summary>Abstract</summary>
One of the major challenges for developing automatic speech recognition (ASR) for low-resource languages is the limited access to labeled data with domain-specific variations. In this study, we propose a pseudo-labeling approach to develop a large-scale domain-agnostic ASR dataset. With the proposed methodology, we developed a 20k+ hours labeled Bangla speech dataset covering diverse topics, speaking styles, dialects, noisy environments, and conversational scenarios. We then exploited the developed corpus to design a conformer-based ASR system. We benchmarked the trained ASR with publicly available datasets and compared it with other available models. To investigate the efficacy, we designed and developed a human-annotated domain-agnostic test set composed of news, telephony, and conversational data among others. Our results demonstrate the efficacy of the model trained on psuedo-label data for the designed test-set along with publicly-available Bangla datasets. The experimental resources will be publicly available.(https://github.com/hishab-nlp/Pseudo-Labeling-for-Domain-Agnostic-Bangla-ASR)
</details>
<details>
<summary>摘要</summary>
一个主要挑战在开发自动语音识别（ASR）系统 для低资源语言是获取域特定变化的标注数据的有限性。在这种研究中，我们提出了一种 pseudo-标注方法，以开发大规模的域不偏泛 ASR 数据集。通过我们的方法，我们开发了20000+小时的标注的孟加拉语言材料，涵盖了多样的话题、说话风格、方言、噪音环境和对话场景。然后，我们利用开发的数据集设计了一种基于征字的 ASR 系统。我们对培过的 ASR 进行了训练，并与公共可用的数据集进行了比较。为了评估效果，我们设计了和开发了一个人工标注的域不偏泛测试集，包括新闻、电话和对话数据等。我们的结果表明，使用 pseudo-标注数据进行训练的模型在我们设计的测试集上以及公共可用的孟加拉语言数据集上具有较高的效果。实验资源将公开可用。
</details></li>
</ul>
<hr>
<h2 id="Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection"><a href="#Nexus-at-ArAIEval-Shared-Task-Fine-Tuning-Arabic-Language-Models-for-Propaganda-and-Disinformation-Detection" class="headerlink" title="Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection"></a>Nexus at ArAIEval Shared Task: Fine-Tuning Arabic Language Models for Propaganda and Disinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03184">http://arxiv.org/abs/2311.03184</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunze Xiao, Firoj Alam</li>
<li>for: 本研究旨在探讨自媒体平台上的假信息和宣传内容的扩散，以及这些内容对社会和谐的影响。</li>
<li>methods: 本研究使用了Transformer模型的精细调整和零shot学习，以及GPT-4模型的应用。</li>
<li>results: 本研究在ArAIEval分享任务中获得了9名和10名的成绩。<details>
<summary>Abstract</summary>
The spread of disinformation and propagandistic content poses a threat to societal harmony, undermining informed decision-making and trust in reliable sources. Online platforms often serve as breeding grounds for such content, and malicious actors exploit the vulnerabilities of audiences to shape public opinion. Although there have been research efforts aimed at the automatic identification of disinformation and propaganda in social media content, there remain challenges in terms of performance. The ArAIEval shared task aims to further research on these particular issues within the context of the Arabic language. In this paper, we discuss our participation in these shared tasks. We competed in subtasks 1A and 2A, where our submitted system secured positions 9th and 10th, respectively. Our experiments consist of fine-tuning transformer models and using zero- and few-shot learning with GPT-4.
</details>
<details>
<summary>摘要</summary>
“虚伪信息和宣传内容的散播对社会和谐产生胁威，损害了有知情的决策和对可靠来源的信任。在线上平台上，这种内容经常成为繁殖的地方，恶徒利用观众的易于欺骗的特点来操纵公众的意见。虽然有关自动识别虚伪信息和宣传内容的研究努力，但是仍然存在性能上的挑战。ArAIEval共同任务 aimsto further研究这些具体的问题 within the context of the Arabic language。本文讲述我们参加这个共同任务的实验。我们参加了1A和2A的子任务，我们提交的系统在这两个子任务中分别获得了第九名和第十名。我们的实验包括精确地 fine-tuning transformer 模型和使用 zero-和 few-shot learning with GPT-4。”Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text"><a href="#ArAIEval-Shared-Task-Persuasion-Techniques-and-Disinformation-Detection-in-Arabic-Text" class="headerlink" title="ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text"></a>ArAIEval Shared Task: Persuasion Techniques and Disinformation Detection in Arabic Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03179">http://arxiv.org/abs/2311.03179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Hasanain, Firoj Alam, Hamdy Mubarak, Samir Abdaljalil, Wajdi Zaghouani, Preslav Nakov, Giovanni Da San Martino, Abed Alhakim Freihat</li>
<li>for: 本文描述了 ArAIEval 分享任务，它是在第一届 ArabicNLP 2023 会议上组织的，并与 EMNLP 2023 同时举行。ArAIEval 提供了两个任务，它们是在 arabic 文本上进行的：（i）诱导技巧检测，旨在在推文和新闻文章中Identify 诱导技巧，以及（ii）在推文上进行不当信息检测。</li>
<li>methods: 大多数参与系统都是使用 fine-tuning transformer 模型，如 AraBERT。</li>
<li>results: 共有 20 个 коман队参与了最终评估阶段，其中 Task 1 和 Task 2 分别有 14 和 16 个参与队伍。<details>
<summary>Abstract</summary>
We present an overview of the ArAIEval shared task, organized as part of the first ArabicNLP 2023 conference co-located with EMNLP 2023. ArAIEval offers two tasks over Arabic text: (i) persuasion technique detection, focusing on identifying persuasion techniques in tweets and news articles, and (ii) disinformation detection in binary and multiclass setups over tweets. A total of 20 teams participated in the final evaluation phase, with 14 and 16 teams participating in Tasks 1 and 2, respectively. Across both tasks, we observed that fine-tuning transformer models such as AraBERT was at the core of the majority of the participating systems. We provide a description of the task setup, including a description of the dataset construction and the evaluation setup. We further give a brief overview of the participating systems. All datasets and evaluation scripts from the shared task are released to the research community. (https://araieval.gitlab.io/) We hope this will enable further research on these important tasks in Arabic.
</details>
<details>
<summary>摘要</summary>
我们提供了阿拉伯语自然语言处理（ArAIEval）共同任务的概述，该任务是在2023年阿拉伯语言处理会议（ArabicNLP 2023）和人工智能语言处理会议（EMNLP 2023）之间的共同办公室。ArAIEval提供了两个任务，它们分别是在推销文本中检测推销技巧和新闻文本中检测不实信息。共20个队伍参与了最终评估阶段，其中14个队伍参与了任务1，16个队伍参与了任务2。在两个任务中，我们发现大多数参与系统都是使用transformer模型进行细化，如AraBERT。我们提供了任务设置的描述，包括数据集的建构和评估设置的描述，以及参与系统的简要概述。此外，我们还发布了所有数据集和评估脚本给研究社区。我们希望这些资源能够促进阿拉伯语言处理领域的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait"><a href="#1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait" class="headerlink" title="1D-Convolutional transformer for Parkinson disease diagnosis from gait"></a>1D-Convolutional transformer for Parkinson disease diagnosis from gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03177">http://arxiv.org/abs/2311.03177</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait">https://github.com/safwennaimi/1d-convolutional-transformer-for-parkinson-disease-diagnosis-from-gait</a></li>
<li>paper_authors: Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau</li>
<li>for: 这篇论文目标是用深度神经网络模型诊断parkinson病。</li>
<li>methods: 这篇论文提出了一种hybrid ConvNet-Transformer架构，通过捕捉本地特征和长期空间时间相关性来准确诊断疾病的严重度。</li>
<li>results: 实验结果表明，这种架构可以从步态数据中准确诊断parkinson病的不同阶段，准确率达88%，超过了其他现有的人工智能方法。此外，这种方法可以普遍化和适应其他分类问题，以解决1D信号中特征相关性和空间时间相关性的问题。<details>
<summary>Abstract</summary>
This paper presents an efficient deep neural network model for diagnosing Parkinson's disease from gait. More specifically, we introduce a hybrid ConvNet-Transformer architecture to accurately diagnose the disease by detecting the severity stage. The proposed architecture exploits the strengths of both Convolutional Neural Networks and Transformers in a single end-to-end model, where the former is able to extract relevant local features from Vertical Ground Reaction Force (VGRF) signal, while the latter allows to capture long-term spatio-temporal dependencies in data. In this manner, our hybrid architecture achieves an improved performance compared to using either models individually. Our experimental results show that our approach is effective for detecting the different stages of Parkinson's disease from gait data, with a final accuracy of 88%, outperforming other state-of-the-art AI methods on the Physionet gait dataset. Moreover, our method can be generalized and adapted for other classification problems to jointly address the feature relevance and spatio-temporal dependency problems in 1D signals. Our source code and pre-trained models are publicly available at https://github.com/SafwenNaimi/1D-Convolutional-transformer-for-Parkinson-disease-diagnosis-from-gait.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs"><a href="#Findings-of-the-WMT-2023-Shared-Task-on-Discourse-Level-Literary-Translation-A-Fresh-Orb-in-the-Cosmos-of-LLMs" class="headerlink" title="Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs"></a>Findings of the WMT 2023 Shared Task on Discourse-Level Literary Translation: A Fresh Orb in the Cosmos of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03127">http://arxiv.org/abs/2311.03127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian Yu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-Hong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham, Bonnie Webber, Philipp Koehn, Andy Way, Yulin Yuan, Shuming Shi</li>
<li>for: 这个研究的目的是提高机器翻译的效果，尤其是在翻译文学作品方面。</li>
<li>methods: 该研究使用了一个新的文学翻译任务，并采用了人工和自动评估方法来评估提交的系统表现。</li>
<li>results: 研究发现了一些有趣的发现，包括文学和语言翻译相关的问题，并提供了一个新的数据集和评估标准。<details>
<summary>Abstract</summary>
Translating literary works has perennially stood as an elusive dream in machine translation (MT), a journey steeped in intricate challenges. To foster progress in this domain, we hold a new shared task at WMT 2023, the first edition of the Discourse-Level Literary Translation. First, we (Tencent AI Lab and China Literature Ltd.) release a copyrighted and document-level Chinese-English web novel corpus. Furthermore, we put forth an industry-endorsed criteria to guide human evaluation process. This year, we totally received 14 submissions from 7 academia and industry teams. We employ both automatic and human evaluations to measure the performance of the submitted systems. The official ranking of the systems is based on the overall human judgments. In addition, our extensive analysis reveals a series of interesting findings on literary and discourse-aware MT. We release data, system outputs, and leaderboard at http://www2.statmt.org/wmt23/literary-translation-task.html.
</details>
<details>
<summary>摘要</summary>
machine translation (MT)中的文学作品翻译总是一个怀抱不到的梦，一条陡峭的旅程。为了推动这个领域的进步，我们在WMT 2023上组织了一个新的共同任务，称为文本级别的文学翻译。首先，我们（天地智能研究所和中国文学有限公司）发布了一个版权和文档级中文英文网络小说集。此外，我们提出了产业认可的评价标准，以引导人工评价过程。本年度共收到14个提交，来自7所学术和产业团队。我们使用自动和人工评价来评价提交系统的性能。官方排名是根据人工评价的总结果。此外，我们的广泛分析发现了一系列有趣的文学和通用MT相关发现。我们在http://www2.statmt.org/wmt23/literary-translation-task.html上发布数据、系统输出和排名。
</details></li>
</ul>
<hr>
<h2 id="Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning"><a href="#Pelvic-floor-MRI-segmentation-based-on-semi-supervised-deep-learning" class="headerlink" title="Pelvic floor MRI segmentation based on semi-supervised deep learning"></a>Pelvic floor MRI segmentation based on semi-supervised deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03105">http://arxiv.org/abs/2311.03105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianwei Zuo, Fei Feng, Zhuhui Wang, James A. Ashton-Miller, John O. L. Delancey, Jiajia Luo<br>for: 这个研究旨在提高骨盘器官的semantic segmentation和三维几何重建的精度。methods: 本研究提出了一个半supervised框架，包括两个阶段。第一阶段是通过图像修复任务进行自我指导预训。接着，使用标注数据进行精度训练。第二阶段是使用自我指导模型生成pseudo标签 для无标注数据。最后，两个数据集都被用来进行半supervised训练。results: 在评估中，我们的方法可以对骨盘器官的semantic segmentation和三维几何重建进行明显改善。比如，难以分类的uterus这个器官，可以提高semantic segmentation的精度高达3.70%。<details>
<summary>Abstract</summary>
The semantic segmentation of pelvic organs via MRI has important clinical significance. Recently, deep learning-enabled semantic segmentation has facilitated the three-dimensional geometric reconstruction of pelvic floor organs, providing clinicians with accurate and intuitive diagnostic results. However, the task of labeling pelvic floor MRI segmentation, typically performed by clinicians, is labor-intensive and costly, leading to a scarcity of labels. Insufficient segmentation labels limit the precise segmentation and reconstruction of pelvic floor organs. To address these issues, we propose a semi-supervised framework for pelvic organ segmentation. The implementation of this framework comprises two stages. In the first stage, it performs self-supervised pre-training using image restoration tasks. Subsequently, fine-tuning of the self-supervised model is performed, using labeled data to train the segmentation model. In the second stage, the self-supervised segmentation model is used to generate pseudo labels for unlabeled data. Ultimately, both labeled and unlabeled data are utilized in semi-supervised training. Upon evaluation, our method significantly enhances the performance in the semantic segmentation and geometric reconstruction of pelvic organs, Dice coefficient can increase by 2.65% averagely. Especially for organs that are difficult to segment, such as the uterus, the accuracy of semantic segmentation can be improved by up to 3.70%.
</details>
<details>
<summary>摘要</summary>
Pelvic organ semantic segmentation via MRI has important clinical significance. Recently, deep learning-enabled semantic segmentation has facilitated the three-dimensional geometric reconstruction of pelvic floor organs, providing clinicians with accurate and intuitive diagnostic results. However, the task of labeling pelvic floor MRI segmentation, typically performed by clinicians, is labor-intensive and costly, leading to a scarcity of labels. Insufficient segmentation labels limit the precise segmentation and reconstruction of pelvic floor organs. To address these issues, we propose a semi-supervised framework for pelvic organ segmentation. The implementation of this framework comprises two stages. In the first stage, it performs self-supervised pre-training using image restoration tasks. Subsequently, fine-tuning of the self-supervised model is performed, using labeled data to train the segmentation model. In the second stage, the self-supervised segmentation model is used to generate pseudo labels for unlabeled data. Ultimately, both labeled and unlabeled data are utilized in semi-supervised training. Upon evaluation, our method significantly enhances the performance in the semantic segmentation and geometric reconstruction of pelvic organs, Dice coefficient can increase by 2.65% averagely. Especially for organs that are difficult to segment, such as the uterus, the accuracy of semantic segmentation can be improved by up to 3.70%.
</details></li>
</ul>
<hr>
<h2 id="A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection"><a href="#A-Simple-yet-Efficient-Ensemble-Approach-for-AI-generated-Text-Detection" class="headerlink" title="A Simple yet Efficient Ensemble Approach for AI-generated Text Detection"></a>A Simple yet Efficient Ensemble Approach for AI-generated Text Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03084">http://arxiv.org/abs/2311.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harika Abburi, Kalyani Roy, Michael Suesserman, Nirmala Pudota, Balaji Veeramani, Edward Bowen, Sanmitra Bhattacharya</li>
<li>for: 这篇研究旨在提出一个简单 yet efficient的解决方案，可以区别人工生成的文本和人类所写的文本。</li>
<li>methods: 本研究使用了多个构成部件的 Large Language Models（LLMs）的 predictions 进行集成，与之前的州域-of-the-art方法相比，其表现更加出色。</li>
<li>results: 在四个生成文本分类 benchmark 上进行了实验，结果显示，与之前的州域-of-the-art方法相比，我们的集成方法可以获得0.5-100%的表现提升。此外，我们还研究了个别 LLM 的训练数据对模型表现的影响，发现可以使用其他开源语言模型生成的数据来替代商业紧密的 Generative Pre-trained Transformer（GPT）数据。<details>
<summary>Abstract</summary>
Recent Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across wide range of styles and genres. However, such capabilities are prone to potential abuse, such as fake news generation, spam email creation, and misuse in academic assignments. Hence, it is essential to build automated approaches capable of distinguishing between artificially generated text and human-authored text. In this paper, we propose a simple yet efficient solution to this problem by ensembling predictions from multiple constituent LLMs. Compared to previous state-of-the-art approaches, which are perplexity-based or uses ensembles with a number of LLMs, our condensed ensembling approach uses only two constituent LLMs to achieve comparable performance. Experiments conducted on four benchmark datasets for generative text classification show performance improvements in the range of 0.5 to 100\% compared to previous state-of-the-art approaches. We also study the influence the training data from individual LLMs have on model performance. We found that substituting commercially-restrictive Generative Pre-trained Transformer (GPT) data with data generated from other open language models such as Falcon, Large Language Model Meta AI (LLaMA2), and Mosaic Pretrained Transformers (MPT) is a feasible alternative when developing generative text detectors. Furthermore, to demonstrate zero-shot generalization, we experimented with an English essays dataset, and results suggest that our ensembling approach can handle new data effectively.
</details>
<details>
<summary>摘要</summary>
现代大语言模型（LLM）在生成文本方面已经展现出了很高的能力，能够生成人类语言writing的文本，包括不同的风格和类型。然而，这些能力也存在潜在的危险，如生成假新闻、垃圾邮件和学术作业中的违规使用。因此，建立自动识别人类语言文本和人工生成文本的方法变得非常重要。在这篇论文中，我们提出了一种简单 yet efficient的解决方案，通过多个构成部件LLM的ensemble进行识别。与之前的状态之前的方法相比，我们的压缩ensemble方法只使用了两个构成部件LLM，可以达到相同的性能。我们在四个文本生成类型的基准数据集上进行了实验，结果显示与之前状态之前的方法相比，我们的方法在0.5%到100%的范围内提高了性能。我们还研究了各个LLM的训练数据对模型性能的影响。我们发现可以将商业限制的生成预训练 transformer（GPT）数据替换为其他开源语言模型生成的数据，这是一种可行的替代方案。此外，为了证明零基础学习的普适性，我们对英文学业作业数据进行了实验，结果表明我们的ensemble方法可以有效处理新数据。
</details></li>
</ul>
<hr>
<h2 id="SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet"><a href="#SugarViT-–-Multi-objective-Regression-of-UAV-Images-with-Vision-Transformers-and-Deep-Label-Distribution-Learning-Demonstrated-on-Disease-Severity-Prediction-in-Sugar-Beet" class="headerlink" title="SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet"></a>SugarViT – Multi-objective Regression of UAV Images with Vision Transformers and Deep Label Distribution Learning Demonstrated on Disease Severity Prediction in Sugar Beet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03076">http://arxiv.org/abs/2311.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maurice Günder, Facundo Ramón Ispizua Yamati, Abel Andree Barreto Alcántara, Anne-Katrin Mahlein, Rafet Sifa, Christian Bauckhage</li>
<li>for: 这个论文主要用于提出一种机器学习框架，用于自动化大规模植物特征注释，以便为糖豫病毒扩散病虫蛀病（CLS）的病虫蛀病度评估。</li>
<li>methods: 这个论文使用了深度标签分布学习（DLDL）、特殊的损失函数和适应的模型架构，开发了一种基于视Transformer的疾病严重度评估模型called SugarViT。</li>
<li>results: 这个论文通过将遥感数据与试验站环境参数结合使用，对病虫蛀病度进行预测，并通过多目标问题预测来证明了模型的通用性。<details>
<summary>Abstract</summary>
Remote sensing and artificial intelligence are pivotal technologies of precision agriculture nowadays. The efficient retrieval of large-scale field imagery combined with machine learning techniques shows success in various tasks like phenotyping, weeding, cropping, and disease control. This work will introduce a machine learning framework for automatized large-scale plant-specific trait annotation for the use case disease severity scoring for Cercospora Leaf Spot (CLS) in sugar beet. With concepts of Deep Label Distribution Learning (DLDL), special loss functions, and a tailored model architecture, we develop an efficient Vision Transformer based model for disease severity scoring called SugarViT. One novelty in this work is the combination of remote sensing data with environmental parameters of the experimental sites for disease severity prediction. Although the model is evaluated on this special use case, it is held as generic as possible to also be applicable to various image-based classification and regression tasks. With our framework, it is even possible to learn models on multi-objective problems as we show by a pretraining on environmental metadata.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate text into Simplified ChineseRemote sensing and artificial intelligence是现代精准农业技术的关键。大规模场景图像的有效检索，结合机器学习技术，在不同任务中都有成功，如类型识别、除草、种植和病虫控制。本文将介绍一种机器学习框架，用于自动化大规模植物特征注释。我们使用 Deep Label Distribution Learning（DLDL）、特殊的损失函数和适应的模型架构，开发了一种高效的视Transformer基本模型，用于粉尘病虫评分。我们的novelty在于将远程感知数据与试验场地环境参数结合用于病虫评分预测。尽管这个模型是特定的用 случа，但它是可以应用于多种图像基本类型和回归任务的。我们的框架还可以学习多目标问题，如我们在环境元数据上进行预训练。
</details></li>
</ul>
<hr>
<h2 id="Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations"><a href="#Maximal-Consistent-Subsystems-of-Max-T-Fuzzy-Relational-Equations" class="headerlink" title="Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations"></a>Maximal Consistent Subsystems of Max-T Fuzzy Relational Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03059">http://arxiv.org/abs/2311.03059</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ismaïl Baaj</li>
<li>For: 这个论文研究了一种带有 $\max-$T 不确定关系方程 $A \Box_{T}^{\max} x &#x3D; b$ 的系统的不一致性。* Methods: 作者直接使用包性最大不一致子系统（w.r.t. 包含关系）的建构方法，使用 Chebyshev 距离 $\Delta &#x3D; \inf_{c \in \mathcal{C} \Vert b - c \Vert$ 来评估不一致性。* Results: 作者提供了一种基于同一个矩阵 $A$ 的不一致 $\max-\min$ 系统的所有一致子系统的有效方法，并证明了可以逐步获取所有最大一致子系统。<details>
<summary>Abstract</summary>
In this article, we study the inconsistency of a system of $\max-T$ fuzzy relational equations of the form $A \Box_{T}^{\max} x = b$, where $T$ is a t-norm among $\min$, the product or Lukasiewicz's t-norm. For an inconsistent $\max-T$ system, we directly construct a canonical maximal consistent subsystem (w.r.t the inclusion order). The main tool used to obtain it is the analytical formula which compute the Chebyshev distance $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$ associated to the inconsistent $\max-T$ system, where $\mathcal{C}$ is the set of second members of consistent systems defined with the same matrix $A$. Based on the same analytical formula, we give, for an inconsistent $\max-\min$ system, an efficient method to obtain all its consistent subsystems, and we show how to iteratively get all its maximal consistent subsystems.
</details>
<details>
<summary>摘要</summary>
在这篇文章中，我们研究了一个 $\max-T$ 软连接方程的不一致性，其形式为 $A \Box_{T}^{\max} x = b$，其中 $T$ 是一个 $\min$ 或者 Lukasiewicz 软连接。对于一个不一致的 $\max-T$ 系统，我们直接构建了一个 canonical 最大一致子系统（w.r.t. 包含关系）。我们使用了一个分析式公式计算 $\max-T$ 系统中的 Chebyshev 距离 $\Delta = \inf_{c \in \mathcal{C} \Vert b - c \Vert$，其中 $\mathcal{C}$ 是一个定义同 $A$ 矩阵的二元一致系统的集合。基于同一个分析式公式，我们对一个不一致的 $\max-\min$ 系统提供了一种高效的方法来获得所有一致子系统，并示出了如何逐步获得所有最大一致子系统。
</details></li>
</ul>
<hr>
<h2 id="LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs"><a href="#LitSumm-Large-language-models-for-literature-summarisation-of-non-coding-RNAs" class="headerlink" title="LitSumm: Large language models for literature summarisation of non-coding RNAs"></a>LitSumm: Large language models for literature summarisation of non-coding RNAs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03056">http://arxiv.org/abs/2311.03056</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rnacentral/litscan-summarization">https://github.com/rnacentral/litscan-summarization</a></li>
<li>paper_authors: Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Anton I. Petrov, Alex Bateman, Blake Sweeney</li>
<li>for: 本研究旨在减轻生物学文献筛选的压力，通过自动生成非编码RNA文献摘要。</li>
<li>methods: 使用大语言模型（LLM）生成文献摘要，并使用链接的提问和检查来确保摘要的质量。</li>
<li>results: 研究表明，使用这种方法可以生成高质量、准确的文献摘要，并且可以通过人工评估和自动评估来验证摘要的质量。<details>
<summary>Abstract</summary>
Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated evaluation approaches, finding that they do not correlate with human assessment. Finally, we apply our tool to a selection of over 4,600 ncRNAs and make the generated summaries available via the RNAcentral resource. We conclude that automated literature summarization is feasible with the current generation of LLMs, provided careful prompting and automated checking are applied.   Availability: Code used to produce these summaries can be found here: https://github.com/RNAcentral/litscan-summarization and the dataset of contexts and summaries can be found here: https://huggingface.co/datasets/RNAcentral/litsumm-v1. Summaries are also displayed on the RNA report pages in RNAcentral (https://rnacentral.org/)
</details>
<details>
<summary>摘要</summary>
目的：生命科学文献淘汰是一个快速增长的挑战。由于发表率的不断增长，与全球团队数量相对固定，这对生物医学知识库开发者而言是一个主要挑战。只有一些知识库有资源来涵盖整个相关文献，而其他都必须优化努力。结果：在这项工作中，我们采用自动生成文献摘要来减轻生物医学知识库缺乏审核人员时间的问题。我们使用商业大语言模型（LLM）生成非编码RNA的文献摘要，并证明可以自动生成高质量、准确的摘要，并且与文献中的参考相匹配。我们对一部分摘要进行了手动评估，并发现大多数摘要被评估为EXTREMELY HIGH QUALITY。此外，我们还应用了常用的自动评估方法，发现它们与人工评估不相关。最后，我们将生成的摘要应用于4,600多个ncRNA上，并将其作为RNAcentral资源提供。我们认为现代大语言模型可以自动生成文献摘要，只要仔细设置提示和自动检查。可用性：用于生成这些摘要的代码可以在GitHub上找到：https://github.com/RNAcentral/litscan-summarization。我们还提供了一个包含上下文和摘要的数据集：https://huggingface.co/datasets/RNAcentral/litsumm-v1。摘要还被显示在RNAreport页面上（https://rnacentral.org/)。
</details></li>
</ul>
<hr>
<h2 id="Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models"><a href="#Masking-Hyperspectral-Imaging-Data-with-Pretrained-Models" class="headerlink" title="Masking Hyperspectral Imaging Data with Pretrained Models"></a>Masking Hyperspectral Imaging Data with Pretrained Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03053">http://arxiv.org/abs/2311.03053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elias Arbash, Andréa de Lima Ribeiro, Sam Thiele, Nina Gnann, Behnood Rasti, Margret Fuchs, Pedram Ghamisi, Richard Gloaguen</li>
<li>for: 提高干扰特征数据处理性能，增强计算成本效益、内存需求和总性能。</li>
<li>methods: 采用Segment Anything Model（SAM）抽象所有对象，然后使用零扩展Grounding Dino对象检测器进行筛选、 intersection和 exclusion步骤，不需要 fine-tuning 或 retraining。</li>
<li>results: 在三个复杂应用场景中提供精确的掩码，包括塑料碎屑特征化、钻井检查和垃圾监测。提供数值评估结果和使用的超参数。<details>
<summary>Abstract</summary>
The presence of undesired background areas associated with potential noise and unknown spectral characteristics degrades the performance of hyperspectral data processing. Masking out unwanted regions is key to addressing this issue. Processing only regions of interest yields notable improvements in terms of computational costs, required memory, and overall performance. The proposed processing pipeline encompasses two fundamental parts: regions of interest mask generation, followed by the application of hyperspectral data processing techniques solely on the newly masked hyperspectral cube. The novelty of our work lies in the methodology adopted for the preliminary image segmentation. We employ the Segment Anything Model (SAM) to extract all objects within the dataset, and subsequently refine the segments with a zero-shot Grounding Dino object detector, followed by intersection and exclusion filtering steps, without the need for fine-tuning or retraining. To illustrate the efficacy of the masking procedure, the proposed method is deployed on three challenging applications scenarios that demand accurate masking; shredded plastics characterization, drill core scanning, and litter monitoring. The numerical evaluation of the proposed masking method on the three applications is provided along with the used hyperparameters. The scripts for the method will be available at https://github.com/hifexplo/Masking.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>诸如不需要的背景区域与可能的噪声和未知 спектраль特征可能会降低高spectral数据处理的性能。从Masking这些区域中移除这些区域是解决这个问题的关键。只处理区域对应的数据可以大幅降低计算成本、内存需求和总性能。我们的处理管道包括两个基本部分：首先生成区域对应mask，然后仅应用高spectral数据处理技术于新生成的masked hyperspectral куби。我们的工作的创新性在于采用的Segment Anything Model（SAM）模型来EXTRACT所有对象于dataset中，然后使用零搭建Grounding Dino对象检测器进行筛选，并进行交叠和排除步骤，无需精度调整或重新训练。为了证明Masking过程的有效性，我们在三个复杂应用场景中运行了提议的方法：扯毁塑料特征化、钻探核心扫描和垃圾监测。我们提供了这三个应用场景的数值评估结果，以及使用的超参数。方法的脚本将会在https://github.com/hifexplo/Masking上提供。
</details></li>
</ul>
<hr>
<h2 id="Grouping-Local-Process-Models"><a href="#Grouping-Local-Process-Models" class="headerlink" title="Grouping Local Process Models"></a>Grouping Local Process Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03040">http://arxiv.org/abs/2311.03040</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Viki Peeva, Wil M. P. van der Aalst</li>
<li>for: 本研究旨在提出一种三步管道，用于将相似的本地过程模型（LPM） grouped together，以提高过程挖掘的效率和可读性。</li>
<li>methods: 本研究使用了多种过程模型相似度度量，包括过程模型的结构相似度、动作序列相似度和时间序列相似度，以分组相似的LPM。</li>
<li>results: 实验结果表明，通过分组相似的LPM可以减少模型爆炸和重复现象，提高过程挖掘的效率和可读性。在一个真实的案例研究中，分组后的LPM数量减少了一半，同时模型的重复率也得到了改善。<details>
<summary>Abstract</summary>
In recent years, process mining emerged as a proven technology to analyze and improve operational processes. An expanding range of organizations using process mining in their daily operation brings a broader spectrum of processes to be analyzed. Some of these processes are highly unstructured, making it difficult for traditional process discovery approaches to discover a start-to-end model describing the entire process. Therefore, the subdiscipline of Local Process Model (LPM) discovery tries to build a set of LPMs, i.e., smaller models that explain sub-behaviors of the process. However, like other pattern mining approaches, LPM discovery algorithms also face the problems of model explosion and model repetition, i.e., the algorithms may create hundreds if not thousands of models, and subsets of them are close in structure or behavior. This work proposes a three-step pipeline for grouping similar LPMs using various process model similarity measures. We demonstrate the usefulness of grouping through a real-life case study, and analyze the impact of different measures, the gravity of repetition in the discovered LPMs, and how it improves after grouping on multiple real event logs.
</details>
<details>
<summary>摘要</summary>
recent 年份, 过程挖掘技术 emerged 为操作过程分析和改进的证明技术。更多的组织在日常操作中使用过程挖掘， bringing 更广泛的过程需要分析。一些这些过程具有高度不结构化，使得传统的过程发现方法难以找到一个从开始到结束的过程模型。因此，本领域的地方过程模型（LPM）发现领域尝试建立一组 LPM，即更小的模型，描述过程的子行为。然而，如其他模式挖掘方法一样， LPM 发现算法也面临着模型爆炸和模型重复的问题，即算法可能生成百计、万计的模型，并且其中的一些模型具有相似结构或行为。本工作提出了一个三步管道，用于将相似的 LPM  grouped 使用不同的过程模型相似度度量。我们通过实际案例研究证明了 grouping 的有用性，并分析了不同度量的影响、重复的gravity 和多个实际事件日志中的改进。
</details></li>
</ul>
<hr>
<h2 id="GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation"><a href="#GTP-ViT-Efficient-Vision-Transformers-via-Graph-based-Token-Propagation" class="headerlink" title="GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation"></a>GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03035">http://arxiv.org/abs/2311.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ackesnal/gtp-vit">https://github.com/ackesnal/gtp-vit</a></li>
<li>paper_authors: Xuwei Xu, Sen Wang, Yudong Chen, Yanping Zheng, Zhewei Wei, Jiajun Liu<br>for: 这个研究旨在提高资源有限的设备上运行Pre-trained Vision Transformer（ViT）模型的效率，以减少计算复杂度。methods: 本研究提出了一种名为Graph-based Token Propagation（GTP）的新方法，它利用图 summarization 算法勤务地传播不重要的 tokens 的信息到空间和semantically相关的 tokens。results: GTP 可以有效地降低 DeiT-S 和 DeiT-B 模型的计算复杂度，而且和state-of-the-art 对应的token merging方法相比，它在不同的backbone上还能实现更快的推察速度。<details>
<summary>Abstract</summary>
Vision Transformers (ViTs) have revolutionized the field of computer vision, yet their deployments on resource-constrained devices remain challenging due to high computational demands. To expedite pre-trained ViTs, token pruning and token merging approaches have been developed, which aim at reducing the number of tokens involved in the computation. However, these methods still have some limitations, such as image information loss from pruned tokens and inefficiency in the token-matching process. In this paper, we introduce a novel Graph-based Token Propagation (GTP) method to resolve the challenge of balancing model efficiency and information preservation for efficient ViTs. Inspired by graph summarization algorithms, GTP meticulously propagates less significant tokens' information to spatially and semantically connected tokens that are of greater importance. Consequently, the remaining few tokens serve as a summarization of the entire token graph, allowing the method to reduce computational complexity while preserving essential information of eliminated tokens. Combined with an innovative token selection strategy, GTP can efficiently identify image tokens to be propagated. Extensive experiments have validated GTP's effectiveness, demonstrating both efficiency and performance improvements. Specifically, GTP decreases the computational complexity of both DeiT-S and DeiT-B by up to 26% with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and remarkably surpasses the state-of-the-art token merging method on various backbones at an even faster inference speed. The source code is available at https://github.com/Ackesnal/GTP-ViT.
</details>
<details>
<summary>摘要</summary>
vision transformers (ViTs) 已经革命化了计算机视觉领域，但它们在有限资源设备上的部署仍然具有挑战，主要是因为高计算复杂度。为了加速预训练的 ViTs，块剪枝和块合并方法已经被开发出来，目的是减少计算中的块数。然而，这些方法仍有一些限制，例如图像信息损失和块匹配过程中的不效率。在这篇论文中，我们介绍了一种新的图像基于的块传播方法（GTP），用于解决高效性和信息保留之间的权衡问题。通过图像分割和图像描述符的组合，GTP 可以减少计算复杂度，同时保持图像信息的精度。与其他方法相比，GTP 可以更好地快速地进行图像识别。我们还提出了一种创新的块选择策略，可以快速地选择需要传播的块。我们的实验结果表明，GTP 可以在不需要finetuning的情况下，对 DeiT-S 和 DeiT-B 降低计算复杂度，最多下降26%，只有0.3%的准确率下降。此外，GTP 还可以在不同的基础上remarkably exceeds the state-of-the-art token merging method的速度。我们的代码可以在 <https://github.com/Ackesnal/GTP-ViT> 上获取。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models"><a href="#Beyond-Words-A-Mathematical-Framework-for-Interpreting-Large-Language-Models" class="headerlink" title="Beyond Words: A Mathematical Framework for Interpreting Large Language Models"></a>Beyond Words: A Mathematical Framework for Interpreting Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03033">http://arxiv.org/abs/2311.03033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Javier González, Aditya V. Nori</li>
<li>for: 这篇论文的目的是为了提供一种数学基础来描述、比较和改进大型自然语言模型（LLM）。</li>
<li>methods: 该论文使用了一种名为“Hex”的框架，以确定关键的概念和词汇在LLM研究中。这个框架可以帮助研究人员和实践者更好地描述LLM的特点、优缺点和新发现。</li>
<li>results: 该论文通过使用“Hex”框架，分别了“链条思维”和“链条提示”两个概念，并证明了它们在某些情况下是等价的。这些结论有助于理解Chain-of-thought prompting的基本假设和其影响。这篇论文的目的是为了提供一种 formal 的框架，以便研究人员和实践者可以更好地探索新的生成AI系统。<details>
<summary>Abstract</summary>
Large language models (LLMs) are powerful AI tools that can generate and comprehend natural language text and other complex information. However, the field lacks a mathematical framework to systematically describe, compare and improve LLMs. We propose Hex a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning. The Hex framework offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings. Using Hex, we differentiate chain-of-thought reasoning from chain-of-thought prompting and establish the conditions under which they are equivalent. This distinction clarifies the basic assumptions behind chain-of-thought prompting and its implications for methods that use it, such as self-verification and prompt programming.   Our goal is to provide a formal framework for LLMs that can help both researchers and practitioners explore new possibilities for generative AI. We do not claim to have a definitive solution, but rather a tool for opening up new research avenues. We argue that our formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair and robust, especially in domains like healthcare and software engineering.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）是一种强大的人工智能工具，可以生成和理解自然语言文本和其他复杂信息。然而，这个领域缺乏一个数学基础来系统地描述、比较和改进 LLM。我们提出了 hex 框架，它明确了关键术语和概念在 LLM 研究中，如幻觉、对齐、自我验证和链式思维。hex 框架提供了精确和一致的方式来描述 LLM，发现其优缺点，并集成新发现。使用 hex，我们区分了链式思维和链式思维激活，并确定了它们在什么情况下是等价的。这种分化了链式思维的假设和其影响，例如自我验证和提问编程。我们的目标是为 LLM 提供一个正式的数学基础，以帮助研究人员和实践者探索新的可能性 для生成人工智能系统。我们并不宣称有终极解决方案，而是一种工具来开拓新的研究途径。我们认为我们的正式定义和结果是推动生成人工智能系统的安全、可靠、公正和可靠性的关键，尤其在医疗和软件工程领域。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-for-Clinical-Structured-Data-A-Benchmark-Comparison-of-Engineering-and-Statistical-Approaches"><a href="#Federated-Learning-for-Clinical-Structured-Data-A-Benchmark-Comparison-of-Engineering-and-Statistical-Approaches" class="headerlink" title="Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches"></a>Federated Learning for Clinical Structured Data: A Benchmark Comparison of Engineering and Statistical Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03417">http://arxiv.org/abs/2311.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nliulab/fl-benchmark">https://github.com/nliulab/fl-benchmark</a></li>
<li>paper_authors: Siqi Li, Di Miao, Qiming Wu, Chuan Hong, Danny D’Agostino, Xin Li, Yilin Ning, Yuqing Shang, Huazhu Fu, Marcus Eng Hock Ong, Hamed Haddadi, Nan Liu</li>
<li>for: 保护医疗合作中数据隐私</li>
<li>methods: 比较了两个领域中的 federated learning 框架，包括工程域和统计领域</li>
<li>results: 显示了两种类型的方法在不同数据集上的表现，统计型 federated learning 方法生成较准确的点估计，但工程型方法生成较高的预测值，需要将两种方法融合应用于未来的 federated learning 应用中。Please note that the above results are in Simplified Chinese text, as requested.<details>
<summary>Abstract</summary>
Federated learning (FL) has shown promising potential in safeguarding data privacy in healthcare collaborations. While the term "FL" was originally coined by the engineering community, the statistical field has also explored similar privacy-preserving algorithms. Statistical FL algorithms, however, remain considerably less recognized than their engineering counterparts. Our goal was to bridge the gap by presenting the first comprehensive comparison of FL frameworks from both engineering and statistical domains. We evaluated five FL frameworks using both simulated and real-world data. The results indicate that statistical FL algorithms yield less biased point estimates for model coefficients and offer convenient confidence interval estimations. In contrast, engineering-based methods tend to generate more accurate predictions, sometimes surpassing central pooled and statistical FL models. This study underscores the relative strengths and weaknesses of both types of methods, emphasizing the need for increased awareness and their integration in future FL applications.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）在医疗协作中实现数据隐私的潜力非常大。这个概念原本是由工程师社群所创造，但是统计领域也已经探索了相似的隐私保护算法。统计学FL算法相对较少获得了认可，我们的目标是通过比较两个领域的FL框架，将它们融合在一起，以便更好地理解它们的优缺点，并且强调未来FL应用中它们的整合。我们评估了五种FL框架，包括工程学和统计学的方法，使用实验和真实数据进行评估。结果显示，统计学FL算法可以产生较少偏差的点估计，并且可以方便地Estimation confidence interval。相比之下，工程学基于的方法可以产生更加准确的预测，有时会超过中央聚合和统计学FL模型。这个研究显示了两种方法之间的相对优缺点，强调未来FL应用中它们的整合。
</details></li>
</ul>
<hr>
<h2 id="Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network"><a href="#Visual-information-driven-model-for-crowd-simulation-using-temporal-convolutional-network" class="headerlink" title="Visual-information-driven model for crowd simulation using temporal convolutional network"></a>Visual-information-driven model for crowd simulation using temporal convolutional network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02996">http://arxiv.org/abs/2311.02996</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanwen Liang, Eric Wai Ming Lee</li>
<li>for: 提高数据驱动人群模拟模型的适应性和现实感</li>
<li>methods:  incorporating visual information, including scenario geometry and pedestrian locomotion, to enhance the adaptability and realism of data-driven crowd simulation models</li>
<li>results: 在三种不同的公共人群运动数据集上测试并评估了一种视觉信息驱动的人群模拟模型，并发现模型在不同的几何场景下具有改进的适应性。<details>
<summary>Abstract</summary>
Crowd simulations play a pivotal role in building design, influencing both user experience and public safety. While traditional knowledge-driven models have their merits, data-driven crowd simulation models promise to bring a new dimension of realism to these simulations. However, most of the existing data-driven models are designed for specific geometries, leading to poor adaptability and applicability. A promising strategy for enhancing the adaptability and realism of data-driven crowd simulation models is to incorporate visual information, including the scenario geometry and pedestrian locomotion. Consequently, this paper proposes a novel visual-information-driven (VID) crowd simulation model. The VID model predicts the pedestrian velocity at the next time step based on the prior social-visual information and motion data of an individual. A radar-geometry-locomotion method is established to extract the visual information of pedestrians. Moreover, a temporal convolutional network (TCN)-based deep learning model, named social-visual TCN, is developed for velocity prediction. The VID model is tested on three public pedestrian motion datasets with distinct geometries, i.e., corridor, corner, and T-junction. Both qualitative and quantitative metrics are employed to evaluate the VID model, and the results highlight the improved adaptability of the model across all three geometric scenarios. Overall, the proposed method demonstrates effectiveness in enhancing the adaptability of data-driven crowd models.
</details>
<details>
<summary>摘要</summary>
人群模拟在建筑设计中发挥重要作用，影响用户体验和公共安全。传统的知识驱动模型具有一定的优点，但数据驱动人群模拟模型具有更高的真实感和应用性。然而，现有的数据驱动模型大多是为特定的几何设计，导致适应性和可应用性弱。为了提高数据驱动人群模拟模型的适应性和真实感，本文提出了一种视觉信息驱动（VID）人群模拟模型。VID模型根据先前的社交视觉信息和人员运动数据预测下一步人群速度。通过 радиар几何运动方法提取人员的视觉信息，并开发了基于深度学习的社交视觉TCN模型来实现速度预测。VID模型在三个不同几何enario中进行测试，包括挺口、角落和T字口。使用质量和量度指标评估VID模型，结果显示VID模型在所有三个几何scenario中的适应性得到了明显提高。总的来说，该方法可以有效地提高数据驱动人群模拟模型的适应性。
</details></li>
</ul>
<hr>
<h2 id="PowerFlowNet-Leveraging-Message-Passing-GNNs-for-Improved-Power-Flow-Approximation"><a href="#PowerFlowNet-Leveraging-Message-Passing-GNNs-for-Improved-Power-Flow-Approximation" class="headerlink" title="PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation"></a>PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03415">http://arxiv.org/abs/2311.03415</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Lin, Stavros Orfanoudakis, Nathan Ordonez Cardenas, Juan S. Giraldo, Pedro P. Vergara</li>
<li>for: 这个论文是为了提出一种高效的电力流分析方法，以便现代电力网络的高效运行和规划。</li>
<li>methods: 这个论文使用图神经网络（GNNs）来加速电力流分析的速度，并且能够保持与传统新颖瑞索法（Newton-Raphson method）的性能相似，但是运行速度比传统方法四倍快（在简单的IEEE 14-bus系统中）和145倍快（在法国高压网络（6470rte）中）。</li>
<li>results: 论文表明，PowerFlowNet在性能和执行速度两个方面都在传统approximation方法（DC relaxation method）之上显著提高，因此PowerFlowNet是一种非常有前途的解决方案 для实际的电力流分析。此外，论文还进行了严格的实验评估，并提供了GNNs在电力系统分析中的应用前景。<details>
<summary>Abstract</summary>
Accurate and efficient power flow (PF) analysis is crucial in modern electrical networks' efficient operation and planning. Therefore, there is a need for scalable algorithms capable of handling large-scale power networks that can provide accurate and fast solutions. Graph Neural Networks (GNNs) have emerged as a promising approach for enhancing the speed of PF approximations by leveraging their ability to capture distinctive features from the underlying power network graph. In this study, we introduce PowerFlowNet, a novel GNN architecture for PF approximation that showcases similar performance with the traditional Newton-Raphson method but achieves it 4 times faster in the simple IEEE 14-bus system and 145 times faster in the realistic case of the French high voltage network (6470rte). Meanwhile, it significantly outperforms other traditional approximation methods, such as the DC relaxation method, in terms of performance and execution time; therefore, making PowerFlowNet a highly promising solution for real-world PF analysis. Furthermore, we verify the efficacy of our approach by conducting an in-depth experimental evaluation, thoroughly examining the performance, scalability, interpretability, and architectural dependability of PowerFlowNet. The evaluation provides insights into the behavior and potential applications of GNNs in power system analysis.
</details>
<details>
<summary>摘要</summary>
现代电力网络的有效和高效运行需要准确和高速的电力流（PF）分析。因此，有一需求为大规模电力网络提供准确和快速的解决方案。图 Néural Networks（GNNs）已经出现为PF估算的快速方法，通过利用其对电力网络图中独特特征的捕捉能力。在本研究中，我们介绍PowerFlowNet，一种新的GNN架构，可以在PF估算中实现类似于传统的Newton-Raphson方法的性能，但是4倍 faster在简单的IEEE 14-bus系统中和145倍 faster在实际的法国高压网络（6470rte）中。此外，它在其他传统估算方法，如DC缓和方法，之上显著超越其性能和执行时间，因此使PowerFlowNet成为实际PF分析中的非常有前途的解决方案。此外，我们对PowerFlowNet的方法进行了广泛的实验评估，彻底检查了它的性能、可扩展性、可读性和架构可靠性。实验结果提供了GNN在电力系统分析中的行为和潜在应用的深入了解。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Neural-Network-Approach-for-3D-Multi-Criteria-Design-Generation-and-Optimization-of-an-Engine-Mount-for-an-Unmanned-Air-Vehicle"><a href="#A-Generative-Neural-Network-Approach-for-3D-Multi-Criteria-Design-Generation-and-Optimization-of-an-Engine-Mount-for-an-Unmanned-Air-Vehicle" class="headerlink" title="A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle"></a>A Generative Neural Network Approach for 3D Multi-Criteria Design Generation and Optimization of an Engine Mount for an Unmanned Air Vehicle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03414">http://arxiv.org/abs/2311.03414</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Petroll, Sebastian Eilermann, Philipp Hoefer, Oliver Niggemann</li>
<li>for: 这篇论文旨在提出一种基于生成神经网络的多 критериria 3D 设计生成方法，用于解决多个功能条件下的设计问题。</li>
<li>methods: 该论文使用 Conditional Variational Autoencoder (CVAE) 来学习多个功能条件下的geometry和相关的多 критериria 约束，并使用 Marching cubes 算法生成优化的 meshes 进行模拟评估。</li>
<li>results: 该论文通过对10,000个抽象 3D 设计进行多种物理学方法的 simulate，并使用自定义的功能条件来训练 CVAE，最终实现了基于多个功能条件的优化设计生成。<details>
<summary>Abstract</summary>
One of the most promising developments in computer vision in recent years is the use of generative neural networks for functionality condition-based 3D design reconstruction and generation. Here, neural networks learn dependencies between functionalities and a geometry in a very effective way. For a neural network the functionalities are translated in conditions to a certain geometry. But the more conditions the design generation needs to reflect, the more difficult it is to learn clear dependencies. This leads to a multi criteria design problem due various conditions, which are not considered in the neural network structure so far.   In this paper, we address this multi-criteria challenge for a 3D design use case related to an unmanned aerial vehicle (UAV) motor mount. We generate 10,000 abstract 3D designs and subject them all to simulations for three physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we train a Conditional Variational Autoencoder (CVAE) using the geometry and corresponding multicriteria functional constraints as input. We use our trained CVAE as well as the Marching cubes algorithm to generate meshes for simulation based evaluation. The results are then evaluated with the generated UAV designs. Subsequently, we demonstrate the ability to generate optimized designs under self-defined functionality conditions using the trained neural network.
</details>
<details>
<summary>摘要</summary>
In this paper, we address this multi-criteria challenge for a 3D design use case related to an unmanned aerial vehicle (UAV) motor mount. We generate 10,000 abstract 3D designs and subject them all to simulations for three physical disciplines: mechanics, thermodynamics, and aerodynamics. Then, we train a Conditional Variational Autoencoder (CVAE) using the geometry and corresponding multicriteria functional constraints as input. We use our trained CVAE as well as the Marching cubes algorithm to generate meshes for simulation-based evaluation. The results are then evaluated with the generated UAV designs. Subsequently, we demonstrate the ability to generate optimized designs under self-defined functionality conditions using the trained neural network.
</details></li>
</ul>
<hr>
<h2 id="Discret2Di-–-Deep-Learning-based-Discretization-for-Model-based-Diagnosis"><a href="#Discret2Di-–-Deep-Learning-based-Discretization-for-Model-based-Diagnosis" class="headerlink" title="Discret2Di – Deep Learning based Discretization for Model-based Diagnosis"></a>Discret2Di – Deep Learning based Discretization for Model-based Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03413">http://arxiv.org/abs/2311.03413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Moddemann, Henrik Sebastian Steude, Alexander Diedrich, Oliver Niggemann</li>
<li>for: 这个论文是为了提出一种自动学习逻辑表达的方法，以便用于监测和诊断技术应用中的不一致现象。</li>
<li>methods: 该论文使用了机器学习技术，以自动学习逻辑表达，并将时间序列转换为逻辑表达。</li>
<li>results: 该论文提出了一种名为Discret2Di的方法，可以自动学习逻辑表达，并且可以处理动态多Modal时间序列。<details>
<summary>Abstract</summary>
Consistency-based diagnosis is an established approach to diagnose technical applications, but suffers from significant modeling efforts, especially for dynamic multi-modal time series. Machine learning seems to be an obvious solution, which becomes less obvious when looking at details: Which notion of consistency can be used? If logical calculi are still to be used, how can dynamic time series be transferred into the discrete world?   This paper presents the methodology Discret2Di for automated learning of logical expressions for consistency-based diagnosis. While these logical calculi have advantages by providing a clear notion of consistency, they have the key problem of relying on a discretization of the dynamic system. The solution presented combines machine learning from both the time series and the symbolic domain to automate the learning of logical rules for consistency-based diagnosis.
</details>
<details>
<summary>摘要</summary>
Traditional consistency-based diagnosis is a widely used approach for diagnosing technical systems, but it requires significant modeling efforts, especially for dynamic multi-modal time series. Machine learning seems to be a natural solution, but it raises questions about which notion of consistency to use and how to transfer dynamic time series into the discrete world.This paper proposes a methodology called Discret2Di for automated learning of logical expressions for consistency-based diagnosis. While logical calculi have advantages in providing a clear notion of consistency, they rely on discretization of the dynamic system, which can be a limiting factor. The proposed solution combines machine learning from both the time series and symbolic domains to automate the learning of logical rules for consistency-based diagnosis.
</details></li>
</ul>
<hr>
<h2 id="TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications"><a href="#TabRepo-A-Large-Scale-Repository-of-Tabular-Model-Evaluations-and-its-AutoML-Applications" class="headerlink" title="TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications"></a>TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02971">http://arxiv.org/abs/2311.02971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/autogluon/tabrepo">https://github.com/autogluon/tabrepo</a></li>
<li>paper_authors: David Salinas, Nick Erickson</li>
<li>for: 本研究的目的是提供一个新的 tabular 模型评估和预测数据集（TabRepo），用于比较不同的 hyperparameter 优化和 AutoML 系统，以及进行转移学习。</li>
<li>methods: 本研究使用了 1206 个模型和 200 个回归和分类数据集进行预测和评估，并提供了预计算的模型预测，以便进行比较和转移学习。</li>
<li>results: 研究表明，使用 TabRepo 可以免除耗时和资源的 Hyperparameter Optimization，并且可以通过标准的转移学习技术来超越当前的 tabular 系统，包括准确率、运行时间和响应时间。<details>
<summary>Abstract</summary>
We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1206 models evaluated on 200 regression and classification datasets. We illustrate the benefit of our datasets in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at no cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
</details>
<details>
<summary>摘要</summary>
我们介绍TabRepo，一个新的表格型模型评估和预测Dataset。TabRepo包含1206个模型在200个数据集上的预测和度量。我们显示了我们的Dataset可以进行以下多种分析：首先，我们显示了在使用预先计算的模型预测时，可以免费进行几何参数优化和AutoML系统的比较。其次，我们显示了我们的Dataset可以轻松地进行传播学习。具体来说，我们显示了使用标准传播学习技术可以超越现有的表格系统在精度、时间和延迟方面的表现。
</details></li>
</ul>
<hr>
<h2 id="Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction"><a href="#Retrieval-Augmented-Code-Generation-for-Universal-Information-Extraction" class="headerlink" title="Retrieval-Augmented Code Generation for Universal Information Extraction"></a>Retrieval-Augmented Code Generation for Universal Information Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02962">http://arxiv.org/abs/2311.02962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucan Guo, Zixuan Li, Xiaolong Jin, Yantao Liu, Yutao Zeng, Wenxuan Liu, Xiang Li, Pan Yang, Long Bai, Jiafeng Guo, Xueqi Cheng</li>
<li>for: 这篇论文的目的是提出一种基于大自然语言模型（LLM）的代码生成框架，用于解决信息抽取（IE）任务中的表示知识结构的问题。</li>
<li>methods: 这篇论文使用的方法包括：（1）使用 Python 类定义任务特定的 schema，以便在一个通用的方式下提取知识结构。（2）采用在上下文学习机制，以便通过示例教育 LLM 生成更加准确的代码。（3）检索相似文本示例，以便在不同任务中获得适当的示例。</li>
<li>results: 对于五种代表性的 IE 任务，Code4UIE 框架在九个数据集上进行了广泛的实验，并达到了显著的效果。<details>
<summary>Abstract</summary>
Information Extraction (IE) aims to extract structural knowledge (e.g., entities, relations, events) from natural language texts, which brings challenges to existing methods due to task-specific schemas and complex text expressions. Code, as a typical kind of formalized language, is capable of describing structural knowledge under various schemas in a universal way. On the other hand, Large Language Models (LLMs) trained on both codes and texts have demonstrated powerful capabilities of transforming texts into codes, which provides a feasible solution to IE tasks. Therefore, in this paper, we propose a universal retrieval-augmented code generation framework based on LLMs, called Code4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define task-specific schemas of various structural knowledge in a universal way. By so doing, extracting knowledge under these schemas can be transformed into generating codes that instantiate the predefined Python classes with the information in texts. To generate these codes more precisely, Code4UIE adopts the in-context learning mechanism to instruct LLMs with examples. In order to obtain appropriate examples for different tasks, Code4UIE explores several example retrieval strategies, which can retrieve examples semantically similar to the given texts. Extensive experiments on five representative IE tasks across nine datasets demonstrate the effectiveness of the Code4UIE framework.
</details>
<details>
<summary>摘要</summary>
信息提取（IE）的目标是从自然语言文本中提取结构知识（例如实体、关系、事件），这会对现有方法带来挑战，因为文本表达的复杂性和任务特定的模式。代码作为一种形式化语言，可以在不同的模式下描述结构知识，并且可以通过大语言模型（LLM）的训练来将文本转化为代码。因此，在本文中，我们提出了一个基于 LLM 的通用检索增强代码生成框架，称为 Code4UIE，用于 IE 任务。具体来说，Code4UIE 使用 Python 类定义任务特定的结构知识模式，以 universal 的方式来定义不同的结构知识模式。通过这种方式，可以将文本中的知识提取转化为生成代码，并且使用 LLM 的增强学习机制来更 precisely 生成代码。为了获得不同任务的适当示例，Code4UIE 探索了多种示例检索策略，可以将示例与文本进行semantic 的对比。经过了五种代表性的 IE 任务和九个数据集的广泛实验，我们证明了 Code4UIE 框架的有效性。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models"><a href="#In-Context-Learning-for-Knowledge-Base-Question-Answering-for-Unmanned-Systems-based-on-Large-Language-Models" class="headerlink" title="In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models"></a>In-Context Learning for Knowledge Base Question Answering for Unmanned Systems based on Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02956">http://arxiv.org/abs/2311.02956</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunlong Chen, Yaming Zhang, Jianfei Yu, Li Yang, Rui Xia</li>
<li>for: Answering factoid questions based on knowledge bases in the CCKS2023 Competition of Question Answering with Knowledge Graph Inference for Unmanned Systems.</li>
<li>methods: Proposed a ChatGPT-based Cypher Query Language (CQL) generation framework that generates the most appropriate CQL based on the given Natural Language Question (NLQ). The framework consists of six parts: auxiliary model, proper noun matcher, demonstration example selector, prompt constructor, ChatGPT-based generation model, and ensemble model.</li>
<li>results: Achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition with an F1-score of 0.92676.<details>
<summary>Abstract</summary>
Knowledge Base Question Answering (KBQA) aims to answer factoid questions based on knowledge bases. However, generating the most appropriate knowledge base query code based on Natural Language Questions (NLQ) poses a significant challenge in KBQA. In this work, we focus on the CCKS2023 Competition of Question Answering with Knowledge Graph Inference for Unmanned Systems. Inspired by the recent success of large language models (LLMs) like ChatGPT and GPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL) generation framework to generate the most appropriate CQL based on the given NLQ. Our generative framework contains six parts: an auxiliary model predicting the syntax-related information of CQL based on the given NLQ, a proper noun matcher extracting proper nouns from the given NLQ, a demonstration example selector retrieving similar examples of the input sample, a prompt constructor designing the input template of ChatGPT, a ChatGPT-based generation model generating the CQL, and an ensemble model to obtain the final answers from diversified outputs. With our ChatGPT-based CQL generation framework, we achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, achieving an F1-score of 0.92676.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>An auxiliary model predicting the syntax-related information of CQL based on the given NLQ.2. A proper noun matcher extracting proper nouns from the given NLQ.3. A demonstration example selector retrieving similar examples of the input sample.4. A prompt constructor designing the input template of ChatGPT.5. A ChatGPT-based generation model generating the CQL.6. An ensemble model to obtain the final answers from diversified outputs.With our ChatGPT-based CQL generation framework, we achieved the second place in the CCKS 2023 Question Answering with Knowledge Graph Inference for Unmanned Systems competition, achieving an F1-score of 0.92676.</details></li>
</ol>
<hr>
<h2 id="Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation"><a href="#Contrastive-Multi-Level-Graph-Neural-Networks-for-Session-based-Recommendation" class="headerlink" title="Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation"></a>Contrastive Multi-Level Graph Neural Networks for Session-based Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02938">http://arxiv.org/abs/2311.02938</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fuyun Wang, Xingyu Gao, Zhenyu Chen, Lei Lyu</li>
<li>for: 这 paper 的目的是提出一种基于对比学习的Session-based recommendation（SBR）方法，以便更好地利用短时间内用户 anonymous 行为序列中的高阶项转移信息。</li>
<li>methods: 该 paper 使用了一种名为 Contrastive Multi-level Graph Neural Networks (CM-GNN)，它将在当前会话和所有会话中应用本地级别的图解归 neural network (L-GCN) 和全局级别的图解归 neural network (G-GCN)，以 Capture 对会话中所有item的高阶相关信息。此外，paper 还引入了一种听道权重 fusion 模块，以学习对会话中item的Pairwise关系。</li>
<li>results: 该 paper 的实验结果表明，CM-GNN 可以在多个 widely used 数据集上击败现有的 SBR 技术。<details>
<summary>Abstract</summary>
Session-based recommendation (SBR) aims to predict the next item at a certain time point based on anonymous user behavior sequences. Existing methods typically model session representation based on simple item transition information. However, since session-based data consists of limited users' short-term interactions, modeling session representation by capturing fixed item transition information from a single dimension suffers from data sparsity. In this paper, we propose a novel contrastive multi-level graph neural networks (CM-GNN) to better exploit complex and high-order item transition information. Specifically, CM-GNN applies local-level graph convolutional network (L-GCN) and global-level network (G-GCN) on the current session and all the sessions respectively, to effectively capture pairwise relations over all the sessions by aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph convolutional network (H-GCN) to capture high-order information among all the item transitions. CM-GNN further introduces an attention-based fusion module to learn pairwise relation-based session representation by fusing the item representations generated by L-GCN and G-GCN. CM-GNN averages the item representations obtained by H-GCN to obtain high-order relation-based session representation. Moreover, to convert the high-order item transition information into the pairwise relation-based session representation, CM-GNN maximizes the mutual information between the representations derived from the fusion module and the average pool layer by contrastive learning paradigm. We conduct extensive experiments on multiple widely used benchmark datasets to validate the efficacy of the proposed method. The encouraging results demonstrate that our proposed method outperforms the state-of-the-art SBR techniques.
</details>
<details>
<summary>摘要</summary>
Session-based recommendation (SBR) 目标是在某个时间点预测下一个 Item，基于匿名用户行为序列。现有方法通常基于简单的 Item 转移信息来建模 session 表示。然而，由于session-based 数据包含有限的用户短期互动，基于单一维度的 Item 转移信息建模 session 表示会受到数据稀缺的困扰。在这篇论文中，我们提出了一种新的对比式多级图 neural network (CM-GNN)，以更好地利用复杂的高阶 Item 转移信息。CM-GNN 包括当前会话的本地级图卷积网络 (L-GCN)、全会话的全级图卷积网络 (G-GCN) 和高级图卷积网络 (H-GCN)。L-GCN 和 G-GCN 分别在当前会话和所有会话中，通过汇聚策略来有效地捕捉会话中对应的对比关系。而 H-GCN 则用于捕捉高阶 Item 转移信息中的高阶对比关系。CM-GNN 还引入了一个注意力基于融合模块，以学习对应的会话表示。最后，CM-GNN 通过对 H-GCN 生成的 Item 表示进行平均聚合，以获得高阶对比关系基于的会话表示。此外，为了将高阶 Item 转移信息转换成对应的会话表示，CM-GNN 利用对比学习概念，通过最大化mutual information  между融合模块和均值聚合层的表示来实现。我们在多个广泛使用的 benchmark 数据集上进行了广泛的实验，结果显示，我们提出的方法可以超越当前状态的 SBR 技术。
</details></li>
</ul>
<hr>
<h2 id="Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things"><a href="#Deep-Image-Semantic-Communication-Model-for-Artificial-Intelligent-Internet-of-Things" class="headerlink" title="Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things"></a>Deep Image Semantic Communication Model for Artificial Intelligent Internet of Things</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02926">http://arxiv.org/abs/2311.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meatery/semantic-segmentation">https://github.com/meatery/semantic-segmentation</a></li>
<li>paper_authors: Li Ping Qian, Yi Zhang, Sikai Lyu, Huijie Zhu, Yuan Wu, Xuemin Sherman Shen, Xiaoniu Yang</li>
<li>for: 提出了一种深度学习图像Semantic Communication模型，用于AIoT设备中高效的图像通信。</li>
<li>methods: 提出了一种高精度图像 semantic segmentation算法，以提取图像semantic信息进行明显的压缩图像数据。同时，还提出了一种基于GAN的semantic图像恢复算法，用于将semantic图像转换为详细的真实场景图像。</li>
<li>results: 对于WebP和CycleGAN的比较，提出的图像Semantic Communication模型可以提高图像压缩比和恢复精度，平均提高71.93%和25.07%。此外，我们的demo试验表明，该模型可以将图像通信延迟降低至95.26%。<details>
<summary>Abstract</summary>
With the rapid development of Artificial Intelligent Internet of Things (AIoT), the image data from AIoT devices has been witnessing the explosive increasing. In this paper, a novel deep image semantic communication model is proposed for the efficient image communication in AIoT. Particularly, at the transmitter side, a high-precision image semantic segmentation algorithm is proposed to extract the semantic information of the image to achieve significant compression of the image data. At the receiver side, a semantic image restoration algorithm based on Generative Adversarial Network (GAN) is proposed to convert the semantic image to a real scene image with detailed information. Simulation results demonstrate that the proposed image semantic communication model can improve the image compression ratio and recovery accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN, respectively. More importantly, our demo experiment shows that the proposed model reduces the total delay by 95.26% in the image communication, when comparing with the original image transmission.
</details>
<details>
<summary>摘要</summary>
随着人工智能互联网关系（AIoT）的快速发展，AIoT设备中的图像数据已经发生了急剧增长。在这篇论文中，我们提出了一种新的深度图像semantic通信模型，用于AIoT中高效的图像通信。特别是在发送端，我们提出了一种高精度图像semantic分割算法，以提取图像中的semantic信息，实现图像数据的显著压缩。在接收端，我们提出了基于生成对抗网络（GAN）的semantic图像恢复算法，将semantic图像转换为详细的真实场景图像。 simulation结果表明，我们提出的图像semantic通信模型可以提高图像压缩比和恢复精度，在WebP和CycleGAN相比，平均提高71.93%和25.07%。更重要的是，我们的 demo experiment 表明，我们的模型可以将图像通信总延迟降低到95.26%，相比原始图像传输。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract"><a href="#Virtual-Action-Actor-Critic-Framework-for-Exploration-Student-Abstract" class="headerlink" title="Virtual Action Actor-Critic Framework for Exploration (Student Abstract)"></a>Virtual Action Actor-Critic Framework for Exploration (Student Abstract)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02916">http://arxiv.org/abs/2311.02916</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bumgeun Park, Taeyoung Kim, Quoc-Vinh Lai-Dang, Dongsoo Har</li>
<li>for: 提高RL中agent的探索性能</li>
<li>methods: 提出了一种新的actor-critic框架，即虚拟行为actor-critic（VAAC），以便在RL中更好地探索</li>
<li>results: 实验结果显示，VAAC比现有算法提高了探索性能<details>
<summary>Abstract</summary>
Efficient exploration for an agent is challenging in reinforcement learning (RL). In this paper, a novel actor-critic framework namely virtual action actor-critic (VAAC), is proposed to address the challenge of efficient exploration in RL. This work is inspired by humans' ability to imagine the potential outcomes of their actions without actually taking them. In order to emulate this ability, VAAC introduces a new actor called virtual actor (VA), alongside the conventional actor-critic framework. Unlike the conventional actor, the VA takes the virtual action to anticipate the next state without interacting with the environment. With the virtual policy following a Gaussian distribution, the VA is trained to maximize the anticipated novelty of the subsequent state resulting from a virtual action. If any next state resulting from available actions does not exhibit high anticipated novelty, training the VA leads to an increase in the virtual policy entropy. Hence, high virtual policy entropy represents that there is no room for exploration. The proposed VAAC aims to maximize a modified Q function, which combines cumulative rewards and the negative sum of virtual policy entropy. Experimental results show that the VAAC improves the exploration performance compared to existing algorithms.
</details>
<details>
<summary>摘要</summary>
RL中的agent寻找最有效的探索方法是具有挑战性的。本文提出了一种新的actor-critic框架，即虚拟行为actor-critic（VAAC），以解决RL中的探索挑战。这种工作受到人类能够想象自己行动后的可能结果的能力所 inspirited。为了模拟这种能力，VAAC引入了一个新的actor，虚拟actor（VA），与传统的actor-critic框架一起工作。与传统的actor不同，VA不需要与环境交互，而是通过虚拟行动预测下一个状态。通过虚拟策略遵循 Gaussian 分布，VA 在预测下一个状态后寻找最高anticipated novelty。如果可用的下一个状态不具有高anticipated novelty，则训练VA会导致虚拟策略 entropy 的增加。因此，高虚拟策略 entropy 表示探索空间的缺乏。VAAC 的目标是最大化修改后的Q函数，该函数组合了累积奖励和虚拟策略 entropy 的负值。实验结果表明，VAAC 在探索性能方面比现有算法 superior。
</details></li>
</ul>
<hr>
<h2 id="Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance"><a href="#Imitation-Learning-based-Alternative-Multi-Agent-Proximal-Policy-Optimization-for-Well-Formed-Swarm-Oriented-Pursuit-Avoidance" class="headerlink" title="Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance"></a>Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02912">http://arxiv.org/abs/2311.02912</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sizhao Li, Yuming Xiang, Rongpeng Li, Zhifeng Zhao, Honggang Zhang</li>
<li>For: This paper focuses on developing a decentralized Imitation learning based Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm for pursuit avoidance in well-formed swarms of multi-robot systems (MRS).* Methods: The proposed IA-MAPPO algorithm utilizes policy-distillation based MAPPO executor to accomplish multiple formations in a centralized manner, and imitation learning to decentralize the formation controller, reducing communication overheads and enhancing scalability.* Results: The simulation results demonstrate the effectiveness of IA-MAPPO, and extensive ablation experiments show that the performance is comparable to a centralized solution with significant decrease in communication overheads.<details>
<summary>Abstract</summary>
Multi-Robot System (MRS) has garnered widespread research interest and fostered tremendous interesting applications, especially in cooperative control fields. Yet little light has been shed on the compound ability of formation, monitoring and defence in decentralized large-scale MRS for pursuit avoidance, which puts stringent requirements on the capability of coordination and adaptability. In this paper, we put forward a decentralized Imitation learning based Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm to provide a flexible and communication-economic solution to execute the pursuit avoidance task in well-formed swarm. In particular, a policy-distillation based MAPPO executor is firstly devised to capably accomplish and swiftly switch between multiple formations in a centralized manner. Furthermore, we utilize imitation learning to decentralize the formation controller, so as to reduce the communication overheads and enhance the scalability. Afterwards, alternative training is leveraged to compensate the performance loss incurred by decentralization. The simulation results validate the effectiveness of IA-MAPPO and extensive ablation experiments further show the performance comparable to a centralized solution with significant decrease in communication overheads.
</details>
<details>
<summary>摘要</summary>
Specifically, we firstly devise a policy-distillation based MAPPO executor to capably accomplish and swiftly switch between multiple formations in a centralized manner. Furthermore, we utilize imitation learning to decentralize the formation controller, so as to reduce the communication overheads and enhance the scalability. Finally, alternative training is leveraged to compensate the performance loss incurred by decentralization. The simulation results validate the effectiveness of IA-MAPPO, and extensive ablation experiments further show the performance comparable to a centralized solution with significant decrease in communication overheads.
</details></li>
</ul>
<hr>
<h2 id="ViDa-Visualizing-DNA-hybridization-trajectories-with-biophysics-informed-deep-graph-embeddings"><a href="#ViDa-Visualizing-DNA-hybridization-trajectories-with-biophysics-informed-deep-graph-embeddings" class="headerlink" title="ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings"></a>ViDa: Visualizing DNA hybridization trajectories with biophysics-informed deep graph embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03411">http://arxiv.org/abs/2311.03411</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenwei-zhang/ViDa">https://github.com/chenwei-zhang/ViDa</a></li>
<li>paper_authors: Chenwei Zhang, Jordan Lovrod, Boyan Beronov, Khanh Dao Duc, Anne Condon</li>
<li>for: 这个论文是为了帮助synthetic biologists和分子程序员理解核酸反应的复杂活动路径，并可以设计用于许多可能的应用程序。</li>
<li>methods: 该论文使用了一种名为Continuous-time Markov chain（CTMC）的模型来模拟核酸反应的动态行为。</li>
<li>results: 该论文提出了一种新的可视化方法，名为ViDa，可以用来可视化核酸反应的轨迹。该方法使用了一种2D嵌入，以便在CTMC模型下的次级结构状态空间中可见化核酸反应的路径。Results表明，使用域pecific的supervised term可以提高 каче量，并成功分离不同的折叠路径，从而提供有用的反应机制的理解。<details>
<summary>Abstract</summary>
Visualization tools can help synthetic biologists and molecular programmers understand the complex reactive pathways of nucleic acid reactions, which can be designed for many potential applications and can be modelled using a continuous-time Markov chain (CTMC). Here we present ViDa, a new visualization approach for DNA reaction trajectories that uses a 2D embedding of the secondary structure state space underlying the CTMC model. To this end, we integrate a scattering transform of the secondary structure adjacency, a variational autoencoder, and a nonlinear dimensionality reduction method. We augment the training loss with domain-specific supervised terms that capture both thermodynamic and kinetic features. We assess ViDa on two well-studied DNA hybridization reactions. Our results demonstrate that the domain-specific features lead to significant quality improvements over the state-of-the-art in DNA state space visualization, successfully separating different folding pathways and thus providing useful insights into dominant reaction mechanisms.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTVisualization tools can help synthetic biologists and molecular programmers understand the complex reactive pathways of nucleic acid reactions, which can be designed for many potential applications and can be modelled using a continuous-time Markov chain (CTMC). Here we present ViDa, a new visualization approach for DNA reaction trajectories that uses a 2D embedding of the secondary structure state space underlying the CTMC model. To this end, we integrate a scattering transform of the secondary structure adjacency, a variational autoencoder, and a nonlinear dimensionality reduction method. We augment the training loss with domain-specific supervised terms that capture both thermodynamic and kinetic features. We assess ViDa on two well-studied DNA hybridization reactions. Our results demonstrate that the domain-specific features lead to significant quality improvements over the state-of-the-art in DNA state space visualization, successfully separating different folding pathways and thus providing useful insights into dominant reaction mechanisms.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base"><a href="#Deep-Learning-Empowered-Semantic-Communication-Systems-with-a-Shared-Knowledge-Base" class="headerlink" title="Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base"></a>Deep Learning-Empowered Semantic Communication Systems with a Shared Knowledge Base</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02884">http://arxiv.org/abs/2311.02884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Yi, Yang Cao, Xin Kang, Ying-Chang Liang</li>
<li>for: 提高6G网络中semantic communication系统的可解释性</li>
<li>methods: 利用共享知识库，结合消息和相应的知识进行剩余信息处理，并通过自动生成的征文描述符进行semantic自信息和源 entropy的数学定义</li>
<li>results: 比基eline方法具有更好的数据传输效率和句子相似度<details>
<summary>Abstract</summary>
Deep learning-empowered semantic communication is regarded as a promising candidate for future 6G networks. Although existing semantic communication systems have achieved superior performance compared to traditional methods, the end-to-end architecture adopted by most semantic communication systems is regarded as a black box, leading to the lack of explainability. To tackle this issue, in this paper, a novel semantic communication system with a shared knowledge base is proposed for text transmissions. Specifically, a textual knowledge base constructed by inherently readable sentences is introduced into our system. With the aid of the shared knowledge base, the proposed system integrates the message and corresponding knowledge from the shared knowledge base to obtain the residual information, which enables the system to transmit fewer symbols without semantic performance degradation. In order to make the proposed system more reliable, the semantic self-information and the source entropy are mathematically defined based on the knowledge base. Furthermore, the knowledge base construction algorithm is developed based on a similarity-comparison method, in which a pre-configured threshold can be leveraged to control the size of the knowledge base. Moreover, the simulation results have demonstrated that the proposed approach outperforms existing baseline methods in terms of transmitted data size and sentence similarity.
</details>
<details>
<summary>摘要</summary>
Note: The above text is translated into Simplified Chinese, which is one of the standard forms of Chinese used in mainland China. The translation is done using a combination of machine translation and human review to ensure accuracy and fluency. However, please note that the translation may not be perfect and may require some adjustments to fit the specific context or audience.
</details></li>
</ul>
<hr>
<h2 id="DP-DCAN-Differentially-Private-Deep-Contrastive-Autoencoder-Network-for-Single-cell-Clustering"><a href="#DP-DCAN-Differentially-Private-Deep-Contrastive-Autoencoder-Network-for-Single-cell-Clustering" class="headerlink" title="DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering"></a>DP-DCAN: Differentially Private Deep Contrastive Autoencoder Network for Single-cell Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03410">http://arxiv.org/abs/2311.03410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huifa Li, Jie Fu, Zhili Chen, Xiaomin Yang, Haitao Liu, Xinpeng Ling<br>for: 这个论文主要用于实现单元细胞RNA测量（scRNA-seq）中的隐私保护。methods: 本文使用了一种叫做差异ifferentially Private Deep Contrastive Autoencoder Network（DP-DCAN），它通过部分网络干扰来实现隐私保护。results: 根据六个数据集的实验结果，DP-DCAN与传统的DP方案相比，有着明显的性能提升，且具有强大的防火墙性。<details>
<summary>Abstract</summary>
Single-cell RNA sequencing (scRNA-seq) is important to transcriptomic analysis of gene expression. Recently, deep learning has facilitated the analysis of high-dimensional single-cell data. Unfortunately, deep learning models may leak sensitive information about users. As a result, Differential Privacy (DP) is increasingly used to protect privacy. However, existing DP methods usually perturb whole neural networks to achieve differential privacy, and hence result in great performance overheads. To address this challenge, in this paper, we take advantage of the uniqueness of the autoencoder that it outputs only the dimension-reduced vector in the middle of the network, and design a Differentially Private Deep Contrastive Autoencoder Network (DP-DCAN) by partial network perturbation for single-cell clustering. Since only partial network is added with noise, the performance improvement is obvious and twofold: one part of network is trained with less noise due to a bigger privacy budget, and the other part is trained without any noise. Experimental results of six datasets have verified that DP-DCAN is superior to the traditional DP scheme with whole network perturbation. Moreover, DP-DCAN demonstrates strong robustness to adversarial attacks. The code is available at https://github.com/LFD-byte/DP-DCAN.
</details>
<details>
<summary>摘要</summary>
single-cell RNA sequencing (scRNA-seq) 是转录调控分析的重要工具。在深度学习的帮助下，对高维单元数据进行分析已经变得更加容易。然而，深度学习模型可能泄露用户的敏感信息，因此隐私保护（DP）在应用中变得越来越重要。然而，现有的DP方法通常是整个神经网络上加入噪声来实现隐私保护，这会导致性能增加很大。为了解决这个挑战，在这篇论文中，我们利用自适应神经网络的独特性，即输出的只是网络中间部分的缩短 вектор，并设计了一种叫做权限保护的深度异常抑制神经网络（DP-DCAN）。由于只有部分网络添加噪声，性能改善是明显的，两倍多：一部分网络因为隐私预算更大，因此受到较少噪声训练；另一部分网络则完全没有噪声训练。实验结果表明，DP-DCAN比传统的DP方案更加有优势，并且对抗攻击有强大的鲁棒性。代码可以在<https://github.com/LFD-byte/DP-DCAN>上获取。
</details></li>
</ul>
<hr>
<h2 id="Visualizing-DNA-reaction-trajectories-with-deep-graph-embedding-approaches"><a href="#Visualizing-DNA-reaction-trajectories-with-deep-graph-embedding-approaches" class="headerlink" title="Visualizing DNA reaction trajectories with deep graph embedding approaches"></a>Visualizing DNA reaction trajectories with deep graph embedding approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03409">http://arxiv.org/abs/2311.03409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenwei-zhang/ViDa">https://github.com/chenwei-zhang/ViDa</a></li>
<li>paper_authors: Chenwei Zhang, Khanh Dao Duc, Anne Condon</li>
<li>for: 这个论文旨在设计 нов型的聚合酶反应，以便更好地理解这些反应的质谱特征。</li>
<li>methods: 这篇论文使用了深度图像嵌入模型，将高维数据映射到2D欧几何空间中，以便更好地可见化DNA反应折叠过程的能量阶段特征。</li>
<li>results: 研究人员通过对两个已经研究过的DNA异种结合反应进行评估，发现ViDa可以成功地分离不同折叠机制的轨迹，提供有用的指导意见，并在DNA动力学可见化方面表现出了大幅提升。<details>
<summary>Abstract</summary>
Synthetic biologists and molecular programmers design novel nucleic acid reactions, with many potential applications. Good visualization tools are needed to help domain experts make sense of the complex outputs of folding pathway simulations of such reactions. Here we present ViDa, a new approach for visualizing DNA reaction folding trajectories over the energy landscape of secondary structures. We integrate a deep graph embedding model with common dimensionality reduction approaches, to map high-dimensional data onto 2D Euclidean space. We assess ViDa on two well-studied and contrasting DNA hybridization reactions. Our preliminary results suggest that ViDa's visualization successfully separates trajectories with different folding mechanisms, thereby providing useful insight to users, and is a big improvement over the current state-of-the-art in DNA kinetics visualization.
</details>
<details>
<summary>摘要</summary>
生物 sintetizadores y programadores moleculares设计新的核酸反应，有很多应用可能性。需要一些好的可视化工具，以帮助领域专家理解复杂的折叠路径仿真结果。我们现在提出了ViDa，一种新的方法用于可视化DNA反应折叠轨迹在二维空间中。我们将深度图像嵌入模型与常见维度减少方法结合在一起，将高维数据映射到二维欧氏空间中。我们对两种已经广泛研究并有很大差异的DNA嵌合反应进行了预liminary测试，结果表明ViDa的可视化成功地分离了不同折叠机制的轨迹，提供了有用的信息，并超过了当前DNA动力学可视化领域的状况。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection"><a href="#Temporal-Shift-–-Multi-Objective-Loss-Function-for-Improved-Anomaly-Fall-Detection" class="headerlink" title="Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection"></a>Temporal Shift – Multi-Objective Loss Function for Improved Anomaly Fall Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02863">http://arxiv.org/abs/2311.02863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Denkovski, Shehroz S. Khan, Alex Mihailidis</li>
<li>for: 预测老年人在家庭环境中的跌倒</li>
<li>methods: 使用自适应网络和其变种进行异常检测框架</li>
<li>results: 提出了一种新的多目标损失函数 called Temporal Shift，可以预测未来和重建的帧序列中的帧。该损失函数在一个 semi-naturalistic 跌倒检测数据集上进行评估，并与多种 Camera 模式进行比较。结果显示，使用 Temporal Shift 可以提高异常检测性能，特别是在使用注意力 U-Net CAE 和多模式神经网络时。<details>
<summary>Abstract</summary>
Falls are a major cause of injuries and deaths among older adults worldwide. Accurate fall detection can help reduce potential injuries and additional health complications. Different types of video modalities can be used in a home setting to detect falls, including RGB, Infrared, and Thermal cameras. Anomaly detection frameworks using autoencoders and their variants can be used for fall detection due to the data imbalance that arises from the rarity and diversity of falls. However, the use of reconstruction error in autoencoders can limit the application of networks' structures that propagate information. In this paper, we propose a new multi-objective loss function called Temporal Shift, which aims to predict both future and reconstructed frames within a window of sequential frames. The proposed loss function is evaluated on a semi-naturalistic fall detection dataset containing multiple camera modalities. The autoencoders were trained on normal activities of daily living (ADL) performed by older adults and tested on ADLs and falls performed by young adults. Temporal shift shows significant improvement to a baseline 3D Convolutional autoencoder, an attention U-Net CAE, and a multi-modal neural network. The greatest improvement was observed in an attention U-Net model improving by 0.20 AUC ROC for a single camera when compared to reconstruction alone. With significant improvement across different models, this approach has the potential to be widely adopted and improve anomaly detection capabilities in other settings besides fall detection.
</details>
<details>
<summary>摘要</summary>
falls 是全球较老的成年人中的主要伤害和死亡原因之一。准确的落幕检测可以帮助降低可能的伤害和额外的健康问题。家庭环境中可以使用不同类型的视频模式来检测落幕，包括RGB、infrared和热成像镜头。使用自适应网络的异常检测框架可以用于落幕检测，因为落幕的数据异常性和多样性导致了数据的不均衡。在这篇论文中，我们提出了一种新的多目标损失函数called Temporal Shift，它 goal是在窗口中的序列帧中预测未来和重建的帧。我们的提案的损失函数被评估在含有多个镜头模式的半自然的落幕检测dataset上。我们使用了正常老年人的日常活动（ADL）来训练自适应网络，并在ADL和年轻人的落幕上测试。Temporal shift在不同的模型中显示了明显的改进，特别是使用注意力U-Net CAE模型，其在单个镜头上的改进为0.20 AUC ROC。与不同的模型相比，这种方法在不同的设置中都有可能广泛采用，以改善异常检测的能力。
</details></li>
</ul>
<hr>
<h2 id="Training-Multi-layer-Neural-Networks-on-Ising-Machine"><a href="#Training-Multi-layer-Neural-Networks-on-Ising-Machine" class="headerlink" title="Training Multi-layer Neural Networks on Ising Machine"></a>Training Multi-layer Neural Networks on Ising Machine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03408">http://arxiv.org/abs/2311.03408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xujie Song, Tong Liu, Shengbo Eben Li, Jingliang Duan, Wenxuan Wang, Keqiang Li</li>
<li>For: 本研究旨在使用 Ising 机器解决大规模二进制优化问题，并利用这些机器来训练嵌入式人工智能模型。* Methods: 该研究提出了一种基于 Ising 机器的启发式学习算法，使用二进制表示 topological 网络和损失函数，并利用 Rosenberg 级别减少和罚函数减少来转换 QCBO 问题为 QUBO 问题。* Results: 研究表明，该算法可以高效地训练多层逻辑网络，并且在 MNIST 数据集上实现了 98.3% 的分类精度，成功率为 72%。随着 Ising 机器中的辐射时间增长，该算法有望训练更深的 neural network。<details>
<summary>Abstract</summary>
As a dedicated quantum device, Ising machines could solve large-scale binary optimization problems in milliseconds. There is emerging interest in utilizing Ising machines to train feedforward neural networks due to the prosperity of generative artificial intelligence. However, existing methods can only train single-layer feedforward networks because of the complex nonlinear network topology. This paper proposes an Ising learning algorithm to train quantized neural network (QNN), by incorporating two essential techinques, namely binary representation of topological network and order reduction of loss function. As far as we know, this is the first algorithm to train multi-layer feedforward networks on Ising machines, providing an alternative to gradient-based backpropagation. Firstly, training QNN is formulated as a quadratic constrained binary optimization (QCBO) problem by representing neuron connection and activation function as equality constraints. All quantized variables are encoded by binary bits based on binary encoding protocol. Secondly, QCBO is converted to a quadratic unconstrained binary optimization (QUBO) problem, that can be efficiently solved on Ising machines. The conversion leverages both penalty function and Rosenberg order reduction, who together eliminate equality constraints and reduce high-order loss function into a quadratic one. With some assumptions, theoretical analysis shows the space complexity of our algorithm is $\mathcal{O}(H^2L + HLN\log H)$, quantifying the required number of Ising spins. Finally, the algorithm effectiveness is validated with a simulated Ising machine on MNIST dataset. After annealing 700 ms, the classification accuracy achieves 98.3%. Among 100 runs, the success probability of finding the optimal solution is 72%. Along with the increasing number of spins on Ising machine, our algorithm has the potential to train deeper neural networks.
</details>
<details>
<summary>摘要</summary>
如果我们把它作为专门的量子设备，则矩阵机器（Ising machine）可以在毫秒级别解决大规模的二进制优化问题。由于生成人工智能的兴起，现在有越来越多的人想使用矩阵机器来训练Feedforward神经网络。然而，现有的方法只能训练单层Feedforward神经网络，因为矩阵机器的复杂非线性网络结构。这篇论文提出了一种矩阵学习算法，用于训练量化神经网络（QNN），并包括两种重要技术：即二进制表示法和顺序减少损失函数的技术。我们知道，这是第一种可以在矩阵机器上训练多层Feedforward神经网络的算法，提供了梯度下降法的一种 alternatives。首先，训练QNN被формализова为一个二进制受限优化（QCBO）问题，通过表示神经连接和活动函数为等式约束来表述。所有量化变量都被编码成二进制位基于二进制编码协议。然后，QCBO被转化为一个二进制无约束优化（QUBO）问题，可以高效解决在矩阵机器上。这种转化利用了 penalty function和Rosenberg顺序减少，共同消除等式约束并将高阶损失函数转化为二进制的 quadratic 函数。通过一些假设，我们对算法的空间复杂度进行了理论分析，并得到了 $\mathcal{O}(H^2L + HLN\log H)$ 的结果，这个结果表示了需要的矩阵轮子数。最后，我们通过在 simulated Ising machine 上进行700毫秒的热化后，对 MNIST 数据集进行验证，并得到了 98.3% 的分类精度。在100次运行中，成功找到优化解决方案的概率为 72%。随着矩阵机器上的矩阵轮子数的增加，我们的算法有可能训练更深的神经网络。
</details></li>
</ul>
<hr>
<h2 id="Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models"><a href="#Co-training-and-Co-distillation-for-Quality-Improvement-and-Compression-of-Language-Models" class="headerlink" title="Co-training and Co-distillation for Quality Improvement and Compression of Language Models"></a>Co-training and Co-distillation for Quality Improvement and Compression of Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02849">http://arxiv.org/abs/2311.02849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang, Hongbo Zhang, Sung Ju Hwang, Alexander Min</li>
<li>for: 提高 computationally expensive pre-trained language models (PLMs) 的吞吐量和实时性，以便在资源受限或实时设置下使用。</li>
<li>methods: 提出了 Co-Training and Co-Distillation (CTCD) 框架，通过在两个模型之间进行协同训练并且互相传递知识，同时提高两个模型的性能和执行速度。</li>
<li>results: CTCD 框架在 GLUE 测试准则上显示出了可以与现有的一个方向知识填充法相比肤，并且小模型通过 CTCD 的概率提高了1.66 个大模型的性能。<details>
<summary>Abstract</summary>
Knowledge Distillation (KD) compresses computationally expensive pre-trained language models (PLMs) by transferring their knowledge to smaller models, allowing their use in resource-constrained or real-time settings. However, most smaller models fail to surpass the performance of the original larger model, resulting in sacrificing performance to improve inference speed. To address this issue, we propose Co-Training and Co-Distillation (CTCD), a novel framework that improves performance and inference speed together by co-training two models while mutually distilling knowledge. The CTCD framework successfully achieves this based on two significant findings: 1) Distilling knowledge from the smaller model to the larger model during co-training improves the performance of the larger model. 2) The enhanced performance of the larger model further boosts the performance of the smaller model. The CTCD framework shows promise as it can be combined with existing techniques like architecture design or data augmentation, replacing one-way KD methods, to achieve further performance improvement. Extensive ablation studies demonstrate the effectiveness of CTCD, and the small model distilled by CTCD outperforms the original larger model by a significant margin of 1.66 on the GLUE benchmark.
</details>
<details>
<summary>摘要</summary>
知识塑化（KD）技术可以压缩训练过程中的计算复杂度，从大型语言模型（PLM）中传递知识到小型模型中，以便在资源有限或实时设置下使用。然而，大多数小型模型无法超越原始大型模型的性能，因此需要牺牲性能以提高推理速度。为解决这个问题，我们提出了同时受过训练和知识塑化（CTCD）框架，该框架可以同时提高性能和推理速度。CTCD框架的两个重要发现是：1）在同时受过训练的情况下，将小型模型中的知识传递给大型模型可以提高大型模型的性能。2）大型模型的提高性能可以进一步提高小型模型的性能。CTCD框架表现良好，可以与现有的建筑设计或数据增强技术相结合，取代一次KD方法，以实现更高的性能提升。广泛的拟合研究表明CTCD的有效性，小型模型通过CTCD来塑化的性能比原始大型模型提高了1.66倍的GLUE标准 benchmark。
</details></li>
</ul>
<hr>
<h2 id="Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs"><a href="#Kinematic-aware-Prompting-for-Generalizable-Articulated-Object-Manipulation-with-LLMs" class="headerlink" title="Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs"></a>Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02847">http://arxiv.org/abs/2311.02847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xwinks/llm_articulated_object_manipulation">https://github.com/xwinks/llm_articulated_object_manipulation</a></li>
<li>paper_authors: Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu</li>
<li>for: 本研究旨在提高家庭助手机器人的普遍化 manipulate objects的能力，通过利用大语言模型（LLM）的强 Context-Aware 学习能力，实现对多种骨架结构的物体抓取和运动轨迹生成。</li>
<li>methods: 本研究提出了一种基于物体骨架结构的Prompting Framework，通过提供物体的骨架知识来引导 LLM 生成低级的机器人运动轨迹点。为了有效地提供骨架知识，我们设计了一种统一的骨架知识解析器，可以将多种骨架结构表示为一种通用的文本描述。基于这种统一描述，我们提出了一种基于骨架结构的 плаanner 模型，通过一种设计的链式思维提问方法，生成精准的3D manipulation waypoints。</li>
<li>results: 我们的框架在48个实例中 across 16种不同类别上进行了评估，结果表明，我们的方法不仅在8种seen类别上超过传统方法，而且在8种未看过类别上也表现出了强大的零基eline能力。此外，我们在7种实际对象类别上进行了实际实验，证明了我们的框架在实际场景中的适应性。<details>
<summary>Abstract</summary>
Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at \href{https://github.com/xwinks/LLM_articulated_object_manipulation}{here}.
</details>
<details>
<summary>摘要</summary>
通用的人工智能家庭助手机器人控制是非常重要的。 latest efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, but due to the high cost of real-world data collection and precise object simulation, it is still challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at [insert link].
</details></li>
</ul>
<hr>
<h2 id="Saturn-Efficient-Multi-Large-Model-Deep-Learning"><a href="#Saturn-Efficient-Multi-Large-Model-Deep-Learning" class="headerlink" title="Saturn: Efficient Multi-Large-Model Deep Learning"></a>Saturn: Efficient Multi-Large-Model Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02840">http://arxiv.org/abs/2311.02840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kabir Nagrecha, Arun Kumar</li>
<li>for: 提高多个大型模型训练效率（如选择模型和超参数优化）</li>
<li>methods: 提出了一种新的数据系统Saturn，通过同时解决多个关联系统挑战来提高效率，包括并行技术选择、分布GPU资源到任务中和调度</li>
<li>results: 对比当前深度学习实践，Saturn的联合优化方法可以提供39-49%的模型选择运行时间减少<details>
<summary>Abstract</summary>
In this paper, we propose Saturn, a new data system to improve the efficiency of multi-large-model training (e.g., during model selection/hyperparameter optimization). We first identify three key interconnected systems challenges for users building large models in this setting -- parallelism technique selection, distribution of GPUs over jobs, and scheduling. We then formalize these as a joint problem, and build a new system architecture to tackle these challenges simultaneously. Our evaluations show that our joint-optimization approach yields 39-49% lower model selection runtimes than typical current DL practice.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的数据系统，用于提高多个大型模型训练的效率（例如， durante 模型选择/超参数优化）。我们首先识别了在用户构建大型模型时存在的三个关联系统挑战：并行技术选择、分布式GPU分配给任务以及调度。我们然后将这些问题化为一个共同问题，并构建了一个新的系统架构来解决这些挑战。我们的评估显示，我们的共同优化方法可以在常见的深度学习实践中降低模型选择运行时间39-49%。
</details></li>
</ul>
<hr>
<h2 id="Mesh-Neural-Cellular-Automata"><a href="#Mesh-Neural-Cellular-Automata" class="headerlink" title="Mesh Neural Cellular Automata"></a>Mesh Neural Cellular Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02820">http://arxiv.org/abs/2311.02820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, Sabine Süsstrunk</li>
<li>for: 增强虚拟环境的真实感，提供一种直接synthesize 3D mesh上的文本ure方法。</li>
<li>methods: 提议使用Mesh Neural Cellular Automata（MeshNCA）方法，该方法可以在3D mesh上直接synthesize 动态文本ure，不需要UV映射。MeshNCA是一种通用的细胞自动机，可以在非格式结构上操作，如3D mesh的顶点。</li>
<li>results: MeshNCA可以在实时内synthesize 文本ure，并且可以跨种模式supervision和不同的目标（如图像、文本提示、运动向量场）进行训练。此外，还提出了将训练过的MeshNCA实例进行grafting，以实现文本ure的 interpolate。通过WebGL 渲染语言实现了前向传播，并在个人电脑和手机上实现了在线交互示例。<details>
<summary>Abstract</summary>
Modeling and synthesizing textures are essential for enhancing the realism of virtual environments. Methods that directly synthesize textures in 3D offer distinct advantages to the UV-mapping-based methods as they can create seamless textures and align more closely with the ways textures form in nature. We propose Mesh Neural Cellular Automata (MeshNCA), a method for directly synthesizing dynamic textures on 3D meshes without requiring any UV maps. MeshNCA is a generalized type of cellular automata that can operate on a set of cells arranged on a non-grid structure such as vertices of a 3D mesh. While only being trained on an Icosphere mesh, MeshNCA shows remarkable generalization and can synthesize textures on any mesh in real time after the training. Additionally, it accommodates multi-modal supervision and can be trained using different targets such as images, text prompts, and motion vector fields. Moreover, we conceptualize a way of grafting trained MeshNCA instances, enabling texture interpolation. Our MeshNCA model enables real-time 3D texture synthesis on meshes and allows several user interactions including texture density/orientation control, a grafting brush, and motion speed/direction control. Finally, we implement the forward pass of our MeshNCA model using the WebGL shading language and showcase our trained models in an online interactive demo which is accessible on personal computers and smartphones. Our demo and the high resolution version of this PDF are available at https://meshnca.github.io/.
</details>
<details>
<summary>摘要</summary>
模型和合成Texture是虚拟环境中的关键技术。直接在3D中合成Texture的方法比UV映射方法更有优势，因为它们可以创建无缝Texture和更好地遵循自然中Texture的形成方式。我们提出了Mesh Neural Cellular Automata（MeshNCA），一种不需要UV图的3D mesh上直接合成动态Texture的方法。MeshNCA是一种通用的细胞自动机，可以在3D mesh的顶点上的非格构造上操作。它只需要在icosphere mesh上进行训练，但它可以在实时中合成Texture于任何mesh，并且可以接受多modal的监督。此外，我们还提出了将训练好的MeshNCA实例结合的思想，以实现Texture的 interpolate。我们的MeshNCA模型允许在实时中合成3DTexture，并且允许用户进行多种交互，包括Texture的密度/方向控制、graftingBrush、速度/方向控制。最后，我们使用WebGL着色语言进行了前向传播，并在个人电脑和手机上展示了我们训练的模型。我们的 demo 和高解度版PDF可以在 <https://meshnca.github.io/> 上获取。
</details></li>
</ul>
<hr>
<h2 id="QualEval-Qualitative-Evaluation-for-Model-Improvement"><a href="#QualEval-Qualitative-Evaluation-for-Model-Improvement" class="headerlink" title="QualEval: Qualitative Evaluation for Model Improvement"></a>QualEval: Qualitative Evaluation for Model Improvement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02807">http://arxiv.org/abs/2311.02807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vmurahari3/qualeval">https://github.com/vmurahari3/qualeval</a></li>
<li>paper_authors: Vishvak Murahari, Ameet Deshpande, Peter Clark, Tanmay Rajpurohit, Ashish Sabharwal, Karthik Narasimhan, Ashwin Kalyan</li>
<li>for: This paper aims to address the limitations of quantitative evaluation metrics in gauging the performance of artificial intelligence systems, particularly large language models (LLMs).</li>
<li>methods: The proposed method, called QualEval, leverages automated qualitative evaluation and a powerful LLM reasoner to generate human-readable insights that can improve model performance. It also includes a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses.</li>
<li>results: The paper demonstrates that QualEval can improve the absolute performance of the Llama 2 model by up to 15% points relative to baselines on a challenging dialogue task (DialogSum). Additionally, QualEval accelerates the pace of model development, serving as a data-scientist-in-a-box for model evaluation and improvement.<details>
<summary>Abstract</summary>
Quantitative evaluation metrics have traditionally been pivotal in gauging the advancements of artificial intelligence systems, including large language models (LLMs). However, these metrics have inherent limitations. Given the intricate nature of real-world tasks, a single scalar to quantify and compare is insufficient to capture the fine-grained nuances of model behavior. Metrics serve only as a way to compare and benchmark models, and do not yield actionable diagnostics, thus making the model improvement process challenging. Model developers find themselves amid extensive manual efforts involving sifting through vast datasets and attempting hit-or-miss adjustments to training data or setups. In this work, we address the shortcomings of quantitative metrics by proposing QualEval, which augments quantitative scalar metrics with automated qualitative evaluation as a vehicle for model improvement. QualEval uses a powerful LLM reasoner and our novel flexible linear programming solver to generate human-readable insights that when applied, accelerate model improvement. The insights are backed by a comprehensive dashboard with fine-grained visualizations and human-interpretable analyses. We corroborate the faithfulness of QualEval by demonstrating that leveraging its insights, for example, improves the absolute performance of the Llama 2 model by up to 15% points relative on a challenging dialogue task (DialogSum) when compared to baselines. QualEval successfully increases the pace of model development, thus in essence serving as a data-scientist-in-a-box. Given the focus on critiquing and improving current evaluation metrics, our method serves as a refreshingly new technique for both model evaluation and improvement.
</details>
<details>
<summary>摘要</summary>
传统的量化评估指标在人工智能系统中，包括大语言模型（LLM）的发展中扮演着重要角色。然而，这些指标具有内在的局限性。由于实际任务的复杂性，单一的量化指标无法捕捉模型行为的细腻特征。指标仅仅用于比较和benchmark模型，而不提供可操作的 диагностиcs，因此改进模型的过程变得困难。模型开发者面临着大量的手动努力，包括对庞大数据集进行搜索和尝试随机地调整训练数据或设置。在这种情况下，我们解决了量化指标的缺陷，提出了 QualEval，它将量化指标与自动化的 качеitative评估相结合，以便更好地改进模型。QualEval使用了一个强大的LLM理解器和我们的新型的 flexible linear programming solver，生成了人类可读的报告，这些报告可以帮助改进模型。报告包括细化的可见化和人类可理解的分析。我们证明了QualEval的准确性，当我们在对话任务（DialogSum）上使用它时，可以提高Llama 2模型的绝对性能，相比基eline，提高15%点。QualEval成功地减少了模型开发的速度，从而成为一个数据科学家在盒子中。由于我们的方法重点 kritik和改进现有评估指标，我们的方法 serves as a refreshingly new technique for both model evaluation and improvement。
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP"><a href="#Incorporating-Worker-Perspectives-into-MTurk-Annotation-Practices-for-NLP" class="headerlink" title="Incorporating Worker Perspectives into MTurk Annotation Practices for NLP"></a>Incorporating Worker Perspectives into MTurk Annotation Practices for NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02802">http://arxiv.org/abs/2311.02802</a></li>
<li>repo_url: None</li>
<li>paper_authors: Olivia Huang, Eve Fleisig, Dan Klein</li>
<li>for: This paper aims to improve the practices of data collection for natural language processing on Amazon Mechanical Turk (MTurk) by considering the perspectives of MTurk workers.</li>
<li>methods: The authors conducted a critical literature review and a survey of MTurk workers to address open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives.</li>
<li>results: The survey found that worker preferences are often at odds with received wisdom among NLP researchers, and that some quality control methods are viewed as biased and ineffective. The authors provide recommendations on how future NLP studies may better account for MTurk workers’ experiences to respect workers’ rights and improve data quality.<details>
<summary>Abstract</summary>
Current practices regarding data collection for natural language processing on Amazon Mechanical Turk (MTurk) often rely on a combination of studies on data quality and heuristics shared among NLP researchers. However, without considering the perspectives of MTurk workers, these approaches are susceptible to issues regarding workers' rights and poor response quality. We conducted a critical literature review and a survey of MTurk workers aimed at addressing open questions regarding best practices for fair payment, worker privacy, data quality, and considering worker incentives. We found that worker preferences are often at odds with received wisdom among NLP researchers. Surveyed workers preferred reliable, reasonable payments over uncertain, very high payments; reported frequently lying on demographic questions; and expressed frustration at having work rejected with no explanation. We also found that workers view some quality control methods, such as requiring minimum response times or Master's qualifications, as biased and largely ineffective. Based on the survey results, we provide recommendations on how future NLP studies may better account for MTurk workers' experiences in order to respect workers' rights and improve data quality.
</details>
<details>
<summary>摘要</summary>
当前在 Amazon Mechanical Turk（MTurk）上进行的自然语言处理数据采集往往采用一种结合数据质量研究和NLP研究人员之间分享的经验的方法。然而，不考虑MTurk工作者的视角，这些方法容易出现工作者权利问题和回答质量问题。我们进行了一项批判性文献综述和MTurk工作者问卷调查，以解决关于公平支付、工作者隐私、数据质量和考虑工作者激励的问题。我们发现工作者偏好可靠、合理的支付，而不是不确定、非常高的支付；报告经常谎欺个人问题；并表示对工作被拒绝而不给解释而感到沮丧。我们还发现一些质量控制方法，如要求最低响应时间或硬件资格，被工作者视为偏袋式和不具有效果。根据调查结果，我们提出了将来NLP研究如何更好地考虑MTurk工作者的经验，以尊重工作者权利并提高数据质量。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.AI_2023_11_06/" data-id="cloq1wl1m007c7o880m394dn6" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.CL_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T11:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.CL_2023_11_06/">cs.CL - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="STONYBOOK-A-System-and-Resource-for-Large-Scale-Analysis-of-Novels"><a href="#STONYBOOK-A-System-and-Resource-for-Large-Scale-Analysis-of-Novels" class="headerlink" title="STONYBOOK: A System and Resource for Large-Scale Analysis of Novels"></a>STONYBOOK: A System and Resource for Large-Scale Analysis of Novels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03614">http://arxiv.org/abs/2311.03614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charuta Pethe, Allen Kim, Rajesh Prabhakar, Tanzir Pial, Steven Skiena</li>
<li>for: 这个论文的目的是为了提供一个大规模分析的文学作品的资源，包括一个开源的终端到终端NLP分析管道，49207个清洁和注释的小说，以及一个大规模分析的文学作品数据库。</li>
<li>methods: 这个论文使用了一个开源的终端到终端NLP分析管道，并对小说进行了大规模的注释和清洁处理。</li>
<li>results: 论文提供了一些分析文学作品的工具，包括人物出现和互动的视觉化、相似的小说、表达词汇、部分词类统计和阅读难度指标。<details>
<summary>Abstract</summary>
Books have historically been the primary mechanism through which narratives are transmitted. We have developed a collection of resources for the large-scale analysis of novels, including: (1) an open source end-to-end NLP analysis pipeline for the annotation of novels into a standard XML format, (2) a collection of 49,207 distinct cleaned and annotated novels, and (3) a database with an associated web interface for the large-scale aggregate analysis of these literary works. We describe the major functionalities provided in the annotation system along with their utilities. We present samples of analysis artifacts from our website, such as visualizations of character occurrences and interactions, similar books, representative vocabulary, part of speech statistics, and readability metrics. We also describe the use of the annotated format in qualitative and quantitative analysis across large corpora of novels.
</details>
<details>
<summary>摘要</summary>
书籍历史上曾经是故事的主要传递机制。我们已经开发了一系列资源来进行大规模的小说分析，包括：（1）一个开源的终端到终端自然语言处理分析管道，用于将小说转换为标准的XML格式；（2）一个包含49,207部清洁和注释的小说的集合；以及（3）一个与网站集成的数据库，用于大规模的小说作品的聚合分析。我们将介绍这些注释系统的主要功能，以及它们的用途。我们还将展示我们网站上的分析成果，例如人物出现和互动的视觉化、相似的书籍、表达词汇、部件分类统计和阅读指数等。此外，我们还将介绍使用注释格式进行质量和量化分析的应用。
</details></li>
</ul>
<hr>
<h2 id="Dimensions-of-Online-Conflict-Towards-Modeling-Agonism"><a href="#Dimensions-of-Online-Conflict-Towards-Modeling-Agonism" class="headerlink" title="Dimensions of Online Conflict: Towards Modeling Agonism"></a>Dimensions of Online Conflict: Towards Modeling Agonism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03584">http://arxiv.org/abs/2311.03584</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Canute, Mali Jin, hannah holtzclaw, Alberto Lusoli, Philippa R Adams, Mugdha Pandya, Maite Taboada, Diana Maynard, Wendy Hui Kyong Chun</li>
<li>for: 本研究旨在模型在线冲突中的agonism和仇恨对话，以便提高对话质量和Platform moderation。</li>
<li>methods: 研究人员采用Twitter conversations相关争议话题的数据采集和标注，并开发了一套全面的标注 schemes，以标识不同类型的冲突。然后，他们使用逻辑回归和transformer模型进行训练，并在不同话题下进行测试。</li>
<li>results: 研究人员发现，Contextual标签可以帮助确定冲突，并且可以使模型在话题变化时保持稳定性。这些结果可以帮助优化在线冲突的识别和管理。<details>
<summary>Abstract</summary>
Agonism plays a vital role in democratic dialogue by fostering diverse perspectives and robust discussions. Within the realm of online conflict there is another type: hateful antagonism, which undermines constructive dialogue. Detecting conflict online is central to platform moderation and monetization. It is also vital for democratic dialogue, but only when it takes the form of agonism. To model these two types of conflict, we collected Twitter conversations related to trending controversial topics. We introduce a comprehensive annotation schema for labelling different dimensions of conflict in the conversations, such as the source of conflict, the target, and the rhetorical strategies deployed. Using this schema, we annotated approximately 4,000 conversations with multiple labels. We then trained both logistic regression and transformer-based models on the dataset, incorporating context from the conversation, including the number of participants and the structure of the interactions. Results show that contextual labels are helpful in identifying conflict and make the models robust to variations in topic. Our research contributes a conceptualization of different dimensions of conflict, a richly annotated dataset, and promising results that can contribute to content moderation.
</details>
<details>
<summary>摘要</summary>
争议在民主对话中发挥重要作用，推动多元观点和有力的讨论。在网络冲突中，另一种类型的争议是恶意对抗，这会损害有益的对话。检测网络冲突中的对抗是民主对话中的中心问题，同时也是平台管理和资金化的关键。只有当争议变成了agonism时，才能为民主对话带来有益。为了模型这两种对抗，我们收集了Twitter上关于热门争议话题的对话。我们提出了一个完整的注解schema，用于标识对话中的冲突源、目标和使用的修辞策略。使用这个schema，我们对约4,000个对话进行了多个标签注解。然后我们使用逻辑回归和变换器模型来训练数据集，并在对话中包含上下文信息，如参与者人数和互动结构。结果表明， Contextual标签可以帮助 Identify冲突，并使模型在话题变化时保持稳定。我们的研究对内容审核做出了贡献，包括对冲突的概念化、 ricahly注解数据集和成功的模型训练结果。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Adversarial-Datasets"><a href="#Measuring-Adversarial-Datasets" class="headerlink" title="Measuring Adversarial Datasets"></a>Measuring Adversarial Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03566">http://arxiv.org/abs/2311.03566</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kritwik1/Detection-of-Anomalies-in-Images-using-Adversarial-learning">https://github.com/kritwik1/Detection-of-Anomalies-in-Images-using-Adversarial-learning</a></li>
<li>paper_authors: Yuanchen Bai, Raoyi Huang, Vijay Viswanathan, Tzu-Sheng Kuo, Tongshuang Wu</li>
<li>for: This paper aims to investigate the challenges of adversarial robustness in NLP tasks and to evaluate the effectiveness of existing quantifiable metrics in capturing the differences between original and adversarial text instances.</li>
<li>methods: The authors conducted a systematic survey of existing quantifiable metrics for NLP tasks and compared the distributions of those metrics between the original and adversarial text instances in several current adversarial effect datasets.</li>
<li>results: The results show valuable insights into the challenges of adversarial robustness in NLP tasks and the limitations of existing metrics in capturing the differences between original and adversarial text instances. The findings also suggest that the assumptions underlying these metrics may not always align with the real-world scenarios.<details>
<summary>Abstract</summary>
In the era of widespread public use of AI systems across various domains, ensuring adversarial robustness has become increasingly vital to maintain safety and prevent undesirable errors. Researchers have curated various adversarial datasets (through perturbations) for capturing model deficiencies that cannot be revealed in standard benchmark datasets. However, little is known about how these adversarial examples differ from the original data points, and there is still no methodology to measure the intended and unintended consequences of those adversarial transformations. In this research, we conducted a systematic survey of existing quantifiable metrics that describe text instances in NLP tasks, among dimensions of difficulty, diversity, and disagreement. We selected several current adversarial effect datasets and compared the distributions between the original and their adversarial counterparts. The results provide valuable insights into what makes these datasets more challenging from a metrics perspective and whether they align with underlying assumptions.
</details>
<details>
<summary>摘要</summary>
在人工智能系统广泛应用于不同领域的时代，保证对抗强度变得越来越重要，以保障安全和避免不良错误。研究人员通过干扰生成了各种对抗示例，以捕捉模型缺陷，这些缺陷在标准测试集中不能表现出来。然而，对这些对抗示例与原始数据点之间的差异还不够了解，还没有一种方法来衡量这些对抗变换的意图和无意图后果。在这项研究中，我们进行了系统性的量化度量研究，探讨了NLPTask中文本实例的纬度、多样性和分歧等维度。我们选择了一些当前的对抗效果数据集，并比较了这些数据集中原始和对抗对应的分布。结果提供了有价值的洞察，有助于我们更好地理解这些数据集在量化度量上的挑战和是否符合下面的假设。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-Uncertainty-in-Natural-Language-Explanations-of-Large-Language-Models"><a href="#Quantifying-Uncertainty-in-Natural-Language-Explanations-of-Large-Language-Models" class="headerlink" title="Quantifying Uncertainty in Natural Language Explanations of Large Language Models"></a>Quantifying Uncertainty in Natural Language Explanations of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03533">http://arxiv.org/abs/2311.03533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sree Harsha Tanneru, Chirag Agarwal, Himabindu Lakkaraju</li>
<li>for: 本文旨在调查大语言模型（LLM）的解释uncertainty的问题。</li>
<li>methods: 作者提出了两种新的度量方法：Verbalized Uncertainty和Probing Uncertainty，以量化LLM的解释uncertainty。</li>
<li>results: Empirical分析表明，Verbalized Uncertainty不是一个可靠的解释confidence度量方法，而Probing Uncertainty度量和解释忠实度之间存在正相关关系。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\textit{Verbalized Uncertainty}$ and $\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Dialogue-System-for-Medical-Prescription-Acquisition-on-Smartphone-Development-Corpus-and-Evaluation"><a href="#Spoken-Dialogue-System-for-Medical-Prescription-Acquisition-on-Smartphone-Development-Corpus-and-Evaluation" class="headerlink" title="Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation"></a>Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03510">http://arxiv.org/abs/2311.03510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Can Kocabiyikoglu, François Portet, Jean-Marc Babouchkine, Prudence Gibert, Hervé Blanchon, Gaëtan Gavazzi</li>
<li>for: 这个论文主要目标是提供一种语音基的药物预cription系统，以便药物预cription过程更加快速、精准和安全。</li>
<li>methods: 这个系统使用了对话模型、语义提取和数据扩展等技术，并在实际应用环境中进行了评估。</li>
<li>results: 评估结果显示，该系统可以帮助医生和其他专家在66.15秒和35.64秒内预cription药物，并有76%的任务成功率和72%的任务成功率。这些数据被记录和注释，并形成了PxCorpus，全球首个语音药物预cription数据集（<a target="_blank" rel="noopener" href="https://doi.org/10.5281/zenodo.6524162%EF%BC%89%E3%80%82">https://doi.org/10.5281/zenodo.6524162）。</a><details>
<summary>Abstract</summary>
Hospital information systems (HIS) have become an essential part of healthcare institutions and now incorporate prescribing support software. Prescription support software allows for structured information capture, which improves the safety, appropriateness and efficiency of prescriptions and reduces the number of adverse drug events (ADEs). However, such a system increases the amount of time physicians spend at a computer entering information instead of providing medical care. In addition, any new visiting clinician must learn to manage complex interfaces since each HIS has its own interfaces. In this paper, we present a natural language interface for e-prescribing software in the form of a spoken dialogue system accessible on a smartphone. This system allows prescribers to record their prescriptions verbally, a form of interaction closer to their usual practice. The system extracts the formal representation of the prescription ready to be checked by the prescribing software and uses the dialogue to request mandatory information, correct errors or warn of particular situations. Since, to the best of our knowledge, there is no existing voice-based prescription dialogue system, we present the system developed in a low-resource environment, focusing on dialogue modeling, semantic extraction and data augmentation. The system was evaluated in the wild with 55 participants. This evaluation showed that our system has an average prescription time of 66.15 seconds for physicians and 35.64 seconds for other experts, and a task success rate of 76\% for physicians and 72\% for other experts. All evaluation data were recorded and annotated to form PxCorpus, the first spoken drug prescription corpus that has been made fully available to the community (\url{https://doi.org/10.5281/zenodo.6524162}).
</details>
<details>
<summary>摘要</summary>
医院信息系统 (HIS) 已成为医疗机构的重要组成部分，并包括订药支持软件。订药支持软件可以为订药进行结构化信息捕获，从而提高订药的安全性、适用性和效率，并减少药物相互作用事件 (ADEs)。然而，这种系统会使医生更多时间花在计算机上输入信息上，而不是提供医疗服务。此外，每个医院信息系统都有自己的界面，新的医生必须学习这些复杂的界面。在这篇论文中，我们提出了一种基于自然语言的订药软件界面，可以通过智能手机上的语音对话系统来记录订药。这种系统使订药人员可以通过语音方式录入订药，与其 usual practice 更加相似。系统会从对话中提取订药的正式表示，并使用对话来请求必要的信息、修正错误或警告特定情况。由于我们知道的 voz-based 订药对话系统不存在，我们在尽可能的低资源环境中开发了这种系统，重点是对话模型、语义提取和数据扩展。我们对这种系统进行了野外评估，共有55名参与者。这次评估显示，我们的系统的订药时间为66.15秒 для医生和35.64秒 для其他专家，任务成功率为76%  для医生和72%  для其他专家。所有评估数据被记录和标注，并形成 PxCorpus，全球首个完全公开的 spoken drug prescription corpus，其 DOI 为10.5281/zenodo.6524162。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Exemplars-as-Clues-to-Retrieving-from-Large-Associative-Memory"><a href="#In-Context-Exemplars-as-Clues-to-Retrieving-from-Large-Associative-Memory" class="headerlink" title="In-Context Exemplars as Clues to Retrieving from Large Associative Memory"></a>In-Context Exemplars as Clues to Retrieving from Large Associative Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03498">http://arxiv.org/abs/2311.03498</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andotalao24/ICL-as-retrieval-from-associative-memory">https://github.com/andotalao24/ICL-as-retrieval-from-associative-memory</a></li>
<li>paper_authors: Jiachen Zhao</li>
<li>for: This paper aims to provide a new perspective on in-context learning (ICL) and improve the efficiency of active exemplar selection in LLMs.</li>
<li>methods: The authors use a theoretical framework based on Hopfield Networks to understand the mechanism of ICL and propose more efficient active exemplar selection.</li>
<li>results: The study sheds new light on the mechanism of ICL by connecting it to memory retrieval, with potential implications for advancing the understanding of LLMs.Here’s the same information in Simplified Chinese:</li>
<li>for: 这篇论文目的是提供一种新的启发式受限学习（ICL）视角，并提高活动示例选择的效率。</li>
<li>methods: 作者们使用基于Hopfield网络的理论框架来理解ICL机制，并提出更有效的活动示例选择方法。</li>
<li>results: 研究带来了ICL机制的新的理解，将其与记忆检索连接起来，有potential应用于提高LLMs的理解。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) have made remarkable progress in natural language processing. The most representative ability of LLMs is in-context learning (ICL), which enables LLMs to learn patterns from in-context exemplars without training. The performance of ICL greatly depends on the exemplars used. However, how to choose exemplars remains unclear due to the lack of understanding of how in-context learning works. In this paper, we present a novel perspective on ICL by conceptualizing it as contextual retrieval from a model of associative memory. We establish a theoretical framework of ICL based on Hopfield Networks. Based on our framework, we look into how in-context exemplars influence the performance of ICL and propose more efficient active exemplar selection. Our study sheds new light on the mechanism of ICL by connecting it to memory retrieval, with potential implications for advancing the understanding of LLMs.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:最近，大型语言模型（LLM）在自然语言处理方面做出了很大的进步。LLM的最重要的能力之一是在上下文中学习（ICL），它使得LLM可以通过上下文示例学习而无需训练。但是，如何选择示例仍然存在很多不确定性，这是因为我们对ICL的理解不够。在这篇论文中，我们提出了一种新的思路，即将ICL看作为上下文检索。我们基于抽象网络建立了ICL的理论框架，并研究了示例如何影响ICL的性能。我们还提出了更加有效的活动示例选择方法。我们的研究可能为LLM的理解提供新的灯光，并且可能对ICL的机制做出新的连接。
</details></li>
</ul>
<hr>
<h2 id="Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling"><a href="#Tackling-Concept-Shift-in-Text-Classification-using-Entailment-style-Modeling" class="headerlink" title="Tackling Concept Shift in Text Classification using Entailment-style Modeling"></a>Tackling Concept Shift in Text Classification using Entailment-style Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03320">http://arxiv.org/abs/2311.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumegh Roychowdhury, Karan Gupta, Siva Rajesh Kasa, Prasanna Srinivasa Murthy, Alok Chandra</li>
<li>for:  Handle concept shift in text classification tasks with less data and labeling costs.</li>
<li>methods:  Reformulate vanilla classification as an entailment-style problem, requiring less data to re-train the text classifier for new concepts.</li>
<li>results:  Achieve absolute F1 gains of up to 7% and 40% in few-shot settings on real-world and synthetic datasets, respectively, and save 75% of labeling costs overall.<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have seen tremendous success in text classification (TC) problems in the context of Natural Language Processing (NLP). In many real-world text classification tasks, the class definitions being learned do not remain constant but rather change with time - this is known as Concept Shift. Most techniques for handling concept shift rely on retraining the old classifiers with the newly labelled data. However, given the amount of training data required to fine-tune large DL models for the new concepts, the associated labelling costs can be prohibitively expensive and time consuming. In this work, we propose a reformulation, converting vanilla classification into an entailment-style problem that requires significantly less data to re-train the text classifier to adapt to new concepts. We demonstrate the effectiveness of our proposed method on both real world & synthetic datasets achieving absolute F1 gains upto 7% and 40% respectively in few-shot settings. Further, upon deployment, our solution also helped save 75% of labeling costs overall.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance"><a href="#Unraveling-Downstream-Gender-Bias-from-Large-Language-Models-A-Study-on-AI-Educational-Writing-Assistance" class="headerlink" title="Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance"></a>Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03311">http://arxiv.org/abs/2311.03311</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/epfl-ml4ed/unraveling-llm-bias">https://github.com/epfl-ml4ed/unraveling-llm-bias</a></li>
<li>paper_authors: Thiemo Wambsganss, Xiaotian Su, Vinitra Swamy, Seyed Parsa Neshaei, Roman Rietsche, Tanja Käser<br>for: This paper investigates the potential transfer of bias from large language models (LLMs) to human writing in an AI writing support pipeline.methods: The study uses a large-scale user study with 231 students writing business case peer reviews in German, and compares five groups with different levels of writing support: a control group with no assistance, and four groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. The study uses GenBit gender bias analysis, Word Embedding Association Tests (WEAT), and Sentence Embedding Association Test (SEAT) to evaluate gender bias at various stages of the pipeline.results: The results show that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions, suggesting that AI writing support in the classroom may not transfer bias to students’ responses.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are increasingly utilized in educational tasks such as providing writing suggestions to students. Despite their potential, LLMs are known to harbor inherent biases which may negatively impact learners. Previous studies have investigated bias in models and data representations separately, neglecting the potential impact of LLM bias on human writing. In this paper, we investigate how bias transfers through an AI writing support pipeline. We conduct a large-scale user study with 231 students writing business case peer reviews in German. Students are divided into five groups with different levels of writing support: one classroom group with feature-based suggestions and four groups recruited from Prolific -- a control group with no assistance, two groups with suggestions from fine-tuned GPT-2 and GPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using GenBit gender bias analysis, Word Embedding Association Tests (WEAT), and Sentence Embedding Association Test (SEAT) we evaluate the gender bias at various stages of the pipeline: in model embeddings, in suggestions generated by the models, and in reviews written by students. Our results demonstrate that there is no significant difference in gender bias between the resulting peer reviews of groups with and without LLM suggestions. Our research is therefore optimistic about the use of AI writing support in the classroom, showcasing a context where bias in LLMs does not transfer to students' responses.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Ziya2-Data-centric-Learning-is-All-LLMs-Need"><a href="#Ziya2-Data-centric-Learning-is-All-LLMs-Need" class="headerlink" title="Ziya2: Data-centric Learning is All LLMs Need"></a>Ziya2: Data-centric Learning is All LLMs Need</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03301">http://arxiv.org/abs/2311.03301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Ping Yang, Qi Yang, Jiaxing Zhang, Yan Song</li>
<li>for: 这项研究的目的是提出一个13亿参数的模型Ziya2，并通过针对不同阶段的预训练技术和数据中心化优化来提高Ziya2在多个标准准则上的表现。</li>
<li>methods: 该研究使用了LLaMA2作为基础模型，并在700亿个字符上进行了进一步预训练。采用了预训练技术和数据中心化优化来优化Ziya2的学习过程。</li>
<li>results: 实验表明，Ziya2在多个标准准则上显著超越了其他模型，特别是与代表性的开源模型相比，具有更出色的表现。Ziya2（基础）版本在<a target="_blank" rel="noopener" href="https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base%E5%92%8Chttps://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base和https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary中发布。</a><details>
<summary>Abstract</summary>
Various large language models (LLMs) have been proposed in recent years, including closed- and open-source ones, continually setting new records on multiple benchmarks. However, the development of LLMs still faces several issues, such as high cost of training models from scratch, and continual pre-training leading to catastrophic forgetting, etc. Although many such issues are addressed along the line of research on LLMs, an important yet practical limitation is that many studies overly pursue enlarging model sizes without comprehensively analyzing and optimizing the use of pre-training data in their learning process, as well as appropriate organization and leveraging of such data in training LLMs under cost-effective settings. In this work, we propose Ziya2, a model with 13 billion parameters adopting LLaMA2 as the foundation model, and further pre-trained on 700 billion tokens, where we focus on pre-training techniques and use data-centric optimization to enhance the learning process of Ziya2 on different stages. Experiments show that Ziya2 significantly outperforms other models in multiple benchmarks especially with promising results compared to representative open-source ones. Ziya2 (Base) is released at https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base and https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary.
</details>
<details>
<summary>摘要</summary>
各种大型语言模型（LLMs）在最近几年中提出了许多，包括关闭和开源的模型，不断创造新的记录在多个benchmark上。然而，LLMs的开发仍面临多个问题，如训练模型从scratch的高成本，以及 kontinual pre-training导致catastrophic forgetting等等。虽然这些问题在LLMs研究中得到了很多的关注，但是一个重要又实用的限制是许多研究过于强调扩大模型的大小，而未充分分析和优化模型在学习过程中使用的预训练数据，以及适当地组织和利用这些数据进行模型训练。在这个工作中，我们提出了Ziya2模型，该模型采用了13亿参数，基于LLaMA2基础模型，并进一步预训练了700亿个字符。我们将注重预训练技术和数据中心化优化，以提高Ziya2在不同阶段的学习过程。实验表明，Ziya2在多个benchmark上表现出色，特别是与代表性的开源模型相比，显示了突出的性能提升。Ziya2（基本）版本 Release在https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base和https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary上。
</details></li>
</ul>
<hr>
<h2 id="Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges"><a href="#Holistic-Analysis-of-Hallucination-in-GPT-4V-ision-Bias-and-Interference-Challenges" class="headerlink" title="Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges"></a>Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03287">http://arxiv.org/abs/2311.03287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, Huaxiu Yao</li>
<li>for: 评估 GPT-4V(ision) 模型中的偏见和干扰现象</li>
<li>methods: 引入了一个新的 benchmark，即 Bias and Interference Challenges in Visual Language Models (Bingo)，用于评估 GPT-4V(ision) 模型中的偏见和干扰</li>
<li>results: GPT-4V(ision) 模型存在偏见和干扰现象，包括地域偏见和被引导的问题，而且现有的 mitigation 方法不能有效解决这些问题。<details>
<summary>Abstract</summary>
While GPT-4V(ision) impressively models both visual and textual information simultaneously, it's hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</details>
<details>
<summary>摘要</summary>
While GPT-4V(ision) impressively models both visual and textual information simultaneously, its hallucination behavior has not been systematically assessed. To bridge this gap, we introduce a new benchmark, namely, the Bias and Interference Challenges in Visual Language Models (Bingo). This benchmark is designed to evaluate and shed light on the two common types of hallucinations in visual language models: bias and interference. Here, bias refers to the model's tendency to hallucinate certain types of responses, possibly due to imbalance in its training data. Interference pertains to scenarios where the judgment of GPT-4V(ision) can be disrupted due to how the text prompt is phrased or how the input image is presented. We identify a notable regional bias, whereby GPT-4V(ision) is better at interpreting Western images or images with English writing compared to images from other countries or containing text in other languages. Moreover, GPT-4V(ision) is vulnerable to leading questions and is often confused when interpreting multiple images together. Popular mitigation approaches, such as self-correction and chain-of-thought reasoning, are not effective in resolving these challenges. We also identified similar biases and interference vulnerabilities with LLaVA and Bard. Our results characterize the hallucination challenges in GPT-4V(ision) and state-of-the-art visual-language models, and highlight the need for new solutions. The Bingo benchmark is available at https://github.com/gzcch/Bingo.
</details></li>
</ul>
<hr>
<h2 id="Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM"><a href="#Safurai-Csharp-Harnessing-Synthetic-Data-to-improve-language-specific-Code-LLM" class="headerlink" title="Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM"></a>Safurai-Csharp: Harnessing Synthetic Data to improve language-specific Code LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03243">http://arxiv.org/abs/2311.03243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davide Cifarelli, Leonardo Boiardi, Alessandro Puppo, Leon Jovanovic</li>
<li>for: 本研究旨在开发一个专门为C#代码生成、完成和调试而设计的开源模型，帮助开发者快速搭建代码和学习代码。</li>
<li>methods: 该模型基于CodeLlama 34B模型，并使用EvolInstruct技术进行精细和扩展数据集的 fine-tuning 过程。</li>
<li>results: 模型在Manual MultiPL-E标准准点测试中获得了56.33%的高分（Zero-Shot、Pass@1），表明它具有remarkable的代码生成和调试能力，能够大幅提高开发者的工作效率和代码学习效果。<details>
<summary>Abstract</summary>
This paper introduces Safurai-Csharp, an open-source model designed to specialize in the generation, completion, and debugging of C# code. Safurai-Csharp is built upon the novel CodeLlama 34B model and leverages the EvolInstruct technique, creating a refined and expanded dataset for its fine-tuning process. The results of its performance, a notable score of 56.33% on the Manual MultiPL-E benchmark (Zero-Shot, Pass@1), signal its high capacity to streamline developers' workflows and aid code learning. It shows promise in setting new stakes in the landscape of open-source C# LLMs and hopes to inspire more inclusive and wide-ranging development in the field of language-specific LLMs.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了 Safurai-Csharp，一个开源模型，旨在生成、完成和调试 C# 代码。Safurai-Csharp 基于 CodeLlama 34B 模型，并利用了 EvolInstruct 技术，通过这种精细和扩展的数据集进行微调。其性能表现出色，在 Manual MultiPL-E  bencmark 上得分 56.33%（零shot、@1 达到），表明它具有快速协助开发者工作流程和代码学习的高能力。它展示了在开源 C# LLM 领域的新的可能性，并希望能够激发更多的包容和广泛的开发在语言特定 LLM 领域。
</details></li>
</ul>
<hr>
<h2 id="p-Laplacian-Transformer"><a href="#p-Laplacian-Transformer" class="headerlink" title="p-Laplacian Transformer"></a>p-Laplacian Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03235">http://arxiv.org/abs/2311.03235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Nguyen, Tam Nguyen, Vinh Nguyen, Tan M. Nguyen</li>
<li>for: 这篇论文主要是关于如何在自注意 Mechanism中引入$p$-Laplacian regularization，以提高Transformers的表达能力和稳定性。</li>
<li>methods: 这篇论文提出了一种新的Transformers类型，称为$p$-Laplacian Transformer（p-LaT），它利用$p$-Laplacian regularization框架来捕捉自注意层中的异质特征。</li>
<li>results: 论文通过实验表明，Compared with基础Transformers，p-LaT在多种 benchmark datasets上表现出了明显的优势。<details>
<summary>Abstract</summary>
$p$-Laplacian regularization, rooted in graph and image signal processing, introduces a parameter $p$ to control the regularization effect on these data. Smaller values of $p$ promote sparsity and interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese)$p$-laplacian regularization, rooted in graph 和 image signal processing, introduces a parameter $p$ to control the regularization effect on these data. smaller values of $p$ promote sparsity 和 interpretability, while larger values encourage smoother solutions. In this paper, we first show that the self-attention mechanism obtains the minimal Laplacian regularization ($p=2$) and encourages the smoothness in the architecture. However, the smoothness is not suitable for the heterophilic structure of self-attention in transformers where attention weights between tokens that are in close proximity and non-close ones are assigned indistinguishably. From that insight, we then propose a novel class of transformers, namely the $p$-Laplacian Transformer (p-LaT), which leverages $p$-Laplacian regularization framework to harness the heterophilic features within self-attention layers. In particular, low $p$ values will effectively assign higher attention weights to tokens that are in close proximity to the current token being processed. We empirically demonstrate the advantages of p-LaT over the baseline transformers on a wide range of benchmark datasets.
</details></li>
</ul>
<hr>
<h2 id="Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation"><a href="#Model-based-Counterfactual-Generator-for-Gender-Bias-Mitigation" class="headerlink" title="Model-based Counterfactual Generator for Gender Bias Mitigation"></a>Model-based Counterfactual Generator for Gender Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03186">http://arxiv.org/abs/2311.03186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ewoenam Kwaku Tokpo, Toon Calders</li>
<li>for:  Mitigating gender bias in natural language models</li>
<li>methods:  Combination of data processing techniques and bi-objective training regime</li>
<li>results:  Alleviates the shortcomings of dictionary-based solutions and improves mitigation of gender bias<details>
<summary>Abstract</summary>
Counterfactual Data Augmentation (CDA) has been one of the preferred techniques for mitigating gender bias in natural language models. CDA techniques have mostly employed word substitution based on dictionaries. Although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. Model-based solutions can alleviate these problems, yet the lack of qualitative parallel training data hinders development in this direction. Therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. We implemented our proposed solution and performed an empirical evaluation which shows how our model alleviates the shortcomings of dictionary-based solutions.
</details>
<details>
<summary>摘要</summary>
《Counterfactual Data Augmentation（CDA）是一种常用的技术来减少自然语言模型中的性别偏见。现有的CDA技术主要采用词替换基于词典，尽管这些技术已经证明可以有效地减少性别偏见，但我们在这篇论文中强调了这些技术的一些限制，如易受到不正确的 sentence 组合的影响，以及只能在已知词典中的word上进行替换。基于模型的解决方案可以解决这些问题，但由于缺乏相应的量化平行训练数据，这方向的发展受到了限制。因此，我们提出了一种结合数据处理技术和双目标训练方法的解决方案，用于生成对性别偏见的抗补做。我们实现了我们的提议并进行了实验评估，结果表明，我们的模型可以减少词典基于CDA技术的缺陷。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives"><a href="#Architectural-Sweet-Spots-for-Modeling-Human-Label-Variation-by-the-Example-of-Argument-Quality-It’s-Best-to-Relate-Perspectives" class="headerlink" title="Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!"></a>Architectural Sweet Spots for Modeling Human Label Variation by the Example of Argument Quality: It’s Best to Relate Perspectives!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03153">http://arxiv.org/abs/2311.03153</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/phhei/relateperspectives-sweetspots">https://github.com/phhei/relateperspectives-sweetspots</a></li>
<li>paper_authors: Philipp Heinisch, Matthias Orlikowski, Julia Romberg, Philipp Cimiano</li>
<li>for: 本研究旨在探讨自然语言处理中多个批注者之间的关系，以及如何将它们融合到一起以提高批注质量。</li>
<li>methods: 研究采用了多种方法，包括完全聚合批注者视角的模型和“分享nothing”-架构，以及 Drawing inspiration from recommender systems, the authors investigate the effectiveness of models that incorporate relations between different annotators.</li>
<li>results: 研究发现，在两个任务中（论点抽象和结论有效性&#x2F;新颖性），使用 recommender 架构可以提高每个批注者的 F$_1$-score 平均值达到 43% 以上，相比于多数票模型。这些结果表明，关于主题的方法可以从 relate 个人视角中受益。<details>
<summary>Abstract</summary>
Many annotation tasks in natural language processing are highly subjective in that there can be different valid and justified perspectives on what is a proper label for a given example. This also applies to the judgment of argument quality, where the assignment of a single ground truth is often questionable. At the same time, there are generally accepted concepts behind argumentation that form a common ground. To best represent the interplay of individual and shared perspectives, we consider a continuum of approaches ranging from models that fully aggregate perspectives into a majority label to "share nothing"-architectures in which each annotator is considered in isolation from all other annotators. In between these extremes, inspired by models used in the field of recommender systems, we investigate the extent to which architectures that include layers to model the relations between different annotators are beneficial for predicting single-annotator labels. By means of two tasks of argument quality classification (argument concreteness and validity/novelty of conclusions), we show that recommender architectures increase the averaged annotator-individual F$_1$-scores up to $43\%$ over a majority label model. Our findings indicate that approaches to subjectivity can benefit from relating individual perspectives.
</details>
<details>
<summary>摘要</summary>
很多自然语言处理的注释任务具有主观性，即可能存在多种有效和合理的标签选择 для给定示例。这同样适用于论证质量评估，其中归一化到单一真实值的决定经常存在疑问。然而，论证中存在通用的概念，这些概念可以成为共同基础。为了最好地表现个体和共同 perspectives的交互，我们考虑了一个维度的方法，从完全汇总 perspectives 到 "share nothing" 架构，在这些架构中，每个标注员被视为孤立的。在这些极端之间， inspirited by  recombiner 系统中的模型，我们调查了将多个标注员之间的关系模型包含在架构中的程度是否有利于预测单个标注员的标签。通过两个论证质量分类任务（论证具体性和结论的有效性/新颖性），我们发现， recombiner 架构可以提高平均标注员个人 F$_1$-分数达到 43%，相比单多数标签模型。我们的发现表明，对主观性的方法可以从个体 perspectives 中受益。
</details></li>
</ul>
<hr>
<h2 id="Text-Augmentations-with-R-drop-for-Classification-of-Tweets-Self-Reporting-Covid-19"><a href="#Text-Augmentations-with-R-drop-for-Classification-of-Tweets-Self-Reporting-Covid-19" class="headerlink" title="Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19"></a>Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03420">http://arxiv.org/abs/2311.03420</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumam Francis, Marie-Francine Moens</li>
<li>for: 这篇论文是为了 Social Media Mining for Health 2023 共同任务而创建的模型。</li>
<li>methods: 我们的方法包括一种类фикации模型，利用多种文本扩展和 R-drop 来增强数据和避免过拟合，从而提高模型的效果。</li>
<li>results: 我们的领先模型，通过对 synonym substitution、固定词和反向翻译等扩展进行增强，超越了任务的 mean 和 median 分数，实现了测试集的 F1 分数 0.877。<details>
<summary>Abstract</summary>
This paper presents models created for the Social Media Mining for Health 2023 shared task. Our team addressed the first task, classifying tweets that self-report Covid-19 diagnosis. Our approach involves a classification model that incorporates diverse textual augmentations and utilizes R-drop to augment data and mitigate overfitting, boosting model efficacy. Our leading model, enhanced with R-drop and augmentations like synonym substitution, reserved words, and back translations, outperforms the task mean and median scores. Our system achieves an impressive F1 score of 0.877 on the test set.
</details>
<details>
<summary>摘要</summary>
这份论文介绍了为社交媒体挖掘2023年共享任务创建的模型。我们团队面临了第一个任务，即分类报告自适应新冠肺炎诊断的推文。我们的方法包括一种类型分类模型，利用多种文本扩充和R-drop数据增强技术，以避免过拟合和提高模型效果。我们的领先模型，通过R-drop和扩充如同义词替换、保留词和反向翻译等，超越任务的 mean 和 median 分数。我们的系统在测试集上达到了很高的 F1 分数0.877。
</details></li>
</ul>
<hr>
<h2 id="Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER"><a href="#Injecting-Categorical-Labels-and-Syntactic-Information-into-Biomedical-NER" class="headerlink" title="Injecting Categorical Labels and Syntactic Information into Biomedical NER"></a>Injecting Categorical Labels and Syntactic Information into Biomedical NER</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03113">http://arxiv.org/abs/2311.03113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumam Francis, Marie-Francine Moens</li>
<li>for: 提高生物医学名实体识别 (NER) 精度</li>
<li>methods: 使用两种方法：首先训练一个序列级分类器来将句子分类为类别，并将这些类别标签作为自然语言模板进行修改，以提高分类器的准确率。然后，将这些标签信息注入到NER模型中。</li>
<li>results: 在三个benchmark数据集上进行实验，结果表明将 categorical label 信息和语法上下文注入到NER模型中可以提高精度，并且超越基elineBERT模型。<details>
<summary>Abstract</summary>
We present a simple approach to improve biomedical named entity recognition (NER) by injecting categorical labels and Part-of-speech (POS) information into the model. We use two approaches, in the first approach, we first train a sequence-level classifier to classify the sentences into categories to obtain the sentence-level tags (categorical labels). The sequence classifier is modeled as an entailment problem by modifying the labels as a natural language template. This helps to improve the accuracy of the classifier. Further, this label information is injected into the NER model. In this paper, we demonstrate effective ways to represent and inject these labels and POS attributes into the NER model. In the second approach, we jointly learn the categorical labels and NER labels. Here we also inject the POS tags into the model to increase the syntactic context of the model. Experiments on three benchmark datasets show that incorporating categorical label information with syntactic context is quite useful and outperforms baseline BERT-based models.
</details>
<details>
<summary>摘要</summary>
我们提出了一种简单的方法来改进生物医学命名实体识别（NER），通过把分类标签和语法信息注入到模型中。我们采用了两种方法：第一种方法是首先训练一个序列级分类器，以将句子分类为类别获得句子级标签（分类标签）。这个序列分类器是通过修改标签为自然语言模板来实现的，这有助于提高分类器的准确率。然后，这些标签信息被注入到NER模型中。在这篇论文中，我们示出了如何表示和注入这些标签和语法特征到NER模型中。第二种方法是同时学习分类标签和NER标签。在这里，我们也注入了语法标签到模型中，以增加语法上下文。经验表明，将分类标签和语法信息注入到BERT基础模型中，可以提高NER模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch"><a href="#Language-Models-are-Super-Mario-Absorbing-Abilities-from-Homologous-Models-as-a-Free-Lunch" class="headerlink" title="Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch"></a>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03099">http://arxiv.org/abs/2311.03099</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yule-buaa/mergelm">https://github.com/yule-buaa/mergelm</a></li>
<li>paper_authors: Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li</li>
<li>For: The paper explores the possibility of merging multiple language models (LMs) with different abilities by leveraging the parameters of homologous models without retraining or using GPUs.* Methods: The authors propose a novel operation called DARE (Drop And REscale) to eliminate most delta parameters of supervised fine-tuning (SFT) models, and they merge multiple SFT homologous models with DARE to create a single model with diverse abilities.* Results: The authors conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa, and they find that DARE can eliminate 99% of delta parameters effortlessly, and merging multiple task-specific LMs into one LM with diverse abilities can improve the performance of the model.Here’s the simplified Chinese text for the three key points:* For: 本 paper 探讨了多种语言模型（LM）之间的能力兼容性，以及可以通过同类模型参数的协同来实现这一点，无需重新训练或使用 GPU。* Methods: 作者提出了一种新的操作DARE（Drop And REscale），可以直接将大多数 delta 参数设为零，而不会影响 SFT 模型的能力。此外，他们还将多个 SFT 同类模型通过 DARE 进行merge。* Results: 作者在 GLUE 测试benchmark 上使用 BERT 和 RoBERTa 进行实验，发现 DARE 可以轻松地消除99%的 delta 参数，而 merge 多个任务特定 LM 可以提高模型的性能。<details>
<summary>Abstract</summary>
In this paper, we uncover that Language Models (LMs), either encoder- or decoder-based, can obtain new capabilities by assimilating the parameters of homologous models without retraining or GPUs. Typically, new abilities of LMs can be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity between fine-tuned and pre-trained parameters (i.e., delta parameters). We initially observe that by introducing a novel operation called DARE (Drop And REscale), most delta parameters can be directly set to zeros without affecting the capabilities of SFT LMs and larger models can tolerate a higher proportion of discarded parameters. Based on this observation, we further sparsify delta parameters of multiple SFT homologous models with DARE and subsequently merge them into a single model by parameter averaging. We conduct experiments on eight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge WizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results show that: (1) The delta parameter value ranges for SFT models are typically small, often within 0.005, and DARE can eliminate 99% of them effortlessly. However, once the models are continuously pre-trained, the value ranges can grow to around 0.03, making DARE impractical. We have also tried to remove fine-tuned instead of delta parameters and find that a 10% reduction can lead to drastically decreased performance (even to 0). This highlights that SFT merely stimulates the abilities via delta parameters rather than injecting new abilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM with diverse abilities. For instance, the merger of WizardLM and WizardMath improves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining its instruction-following ability while surpassing WizardMath's original 64.2 performance. Codes are available at https://github.com/yule-BUAA/MergeLM.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们发现了语言模型（LM）可以通过吸收同类模型参数而获得新的能力，不需要重新训练或GPU。通常，LM的新能力可以通过监督微调（SFT）表达，可以通过参数差异（delta参数）来衡量。我们发现，通过一种新的操作called DARE（Drop And REscale），大多数delta参数可以直接设为零，而不会影响SFT LM的能力。基于这一点，我们进一步减少了多个SFT同类模型的delta参数使用DARE，并将其混合成一个单独的模型。我们在GLUE数据集上进行了八个数据集的实验，并将WizardLM、WizardMath和Code Alpaca基于Llama 2进行了集成。实验结果表明：（1）SFT模型的 delta参数范围通常在0.005之间，DARE可以轻松地消除99%的 delta参数。但是，当模型进行连续预训练时， delta参数范围可以增长到约0.03，使DARE成为不切实际。我们还尝试了从精度训练中除掉精度训练后的参数，发现可以减少精度训练后的参数10%，但这会导致性能减少至0。这表明SFT只是通过 delta parameters来刺激LM的能力，而不是在LM中注入新的能力；（2）DARE可以将多个任务特定LM集成到一个LM中，例如将WizardLM和WizardMath集成成一个LM，可以提高GSM8K零shot精度从2.2增加到66.3，保留WizardLM的 instrucion-following 能力，而同时超越WizardMath的原始64.2性能。代码可以在https://github.com/yule-BUAA/MergeLM中获取。
</details></li>
</ul>
<hr>
<h2 id="BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer"><a href="#BanLemma-A-Word-Formation-Dependent-Rule-and-Dictionary-Based-Bangla-Lemmatizer" class="headerlink" title="BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer"></a>BanLemma: A Word Formation Dependent Rule and Dictionary Based Bangla Lemmatizer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03078">http://arxiv.org/abs/2311.03078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eblict-gigatech/BanLemma">https://github.com/eblict-gigatech/BanLemma</a></li>
<li>paper_authors: Sadia Afrin, Md. Shahad Mahmud Chowdhury, Md. Ekramul Islam, Faisal Ahamed Khan, Labib Imam Chowdhury, MD. Motahar Mahtab, Nazifa Nuha Chowdhury, Massud Forkan, Neelima Kundu, Hakim Arif, Mohammad Mamun Or Rashid, Mohammad Ruhul Amin, Nabeel Mohammed</li>
<li>for: 本研究旨在提出一个特定于孟加拉语的lemmatizer，以便在自然语言处理（NLP）和语言学方面进行更好的理解和处理。</li>
<li>methods: 本研究使用了语言规则来定义lemmatization规则，并使用词典和规则实现 lemmatizer。具体来说，我们分析了大量的孟加拉语文本，探索了不同类型的词汇形成方式，并根据词形态的不同，定义了不同的lemmatization规则。</li>
<li>results: 在使用 manually annotated 测试数据进行测试时，我们的lemmatizer取得了96.36%的准确率，并在三个以前发表的孟加拉语 lemmatization 数据集上表现出 competed 的性能。<details>
<summary>Abstract</summary>
Lemmatization holds significance in both natural language processing (NLP) and linguistics, as it effectively decreases data density and aids in comprehending contextual meaning. However, due to the highly inflected nature and morphological richness, lemmatization in Bangla text poses a complex challenge. In this study, we propose linguistic rules for lemmatization and utilize a dictionary along with the rules to design a lemmatizer specifically for Bangla. Our system aims to lemmatize words based on their parts of speech class within a given sentence. Unlike previous rule-based approaches, we analyzed the suffix marker occurrence according to the morpho-syntactic values and then utilized sequences of suffix markers instead of entire suffixes. To develop our rules, we analyze a large corpus of Bangla text from various domains, sources, and time periods to observe the word formation of inflected words. The lemmatizer achieves an accuracy of 96.36% when tested against a manually annotated test dataset by trained linguists and demonstrates competitive performance on three previously published Bangla lemmatization datasets. We are making the code and datasets publicly available at https://github.com/eblict-gigatech/BanLemma in order to contribute to the further advancement of Bangla NLP.
</details>
<details>
<summary>摘要</summary>
lemmatization在自然语言处理（NLP）和语言学中具有重要意义，因为它有效地减少数据密度，并帮助理解上下文中的含义。然而，由于孟加拉语的高度变格性和 morphological wealth，孟加拉语 lemmatization 存在复杂的挑战。在这种研究中，我们提出了语言规则 для lemmatization 和一个字典，并使用这些规则来设计一个特定 для孟加拉语的 lemmatizer。我们的系统 aimsto lemmatize 根据句子中每个单词的部分语种类型。不同于之前的规则基本方法，我们分析了 suffix marker 的出现根据 morpho-syntactic 值，然后使用 suffix marker 的序列而不是整个 suffix。为了开发我们的规则，我们分析了一大量的孟加拉语文本，从不同的领域、来源和时期来观察inflected word 的形成。lemmatizer 的准确率为 96.36%，并在三个已经发布的孟加拉语 lemmatization 数据集上进行了竞争性的表现。我们将代码和数据集公开发布在 GitHub 上，以便贡献于孟加拉语 NLP 的进一步发展。
</details></li>
</ul>
<hr>
<h2 id="Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models"><a href="#Zero-shot-Bilingual-App-Reviews-Mining-with-Large-Language-Models" class="headerlink" title="Zero-shot Bilingual App Reviews Mining with Large Language Models"></a>Zero-shot Bilingual App Reviews Mining with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03058">http://arxiv.org/abs/2311.03058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jl-wei/mini-bar">https://github.com/jl-wei/mini-bar</a></li>
<li>paper_authors: Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray<br>for: 本研究旨在提高软件需求的改进，通过自动挖掘用户评论来提高软件质量。methods: 本研究使用大量语言模型（LLMs）进行零shot挖掘用户评论，包括分类用户评论、将相似评论集成在一起、生成概括性摘要和排序用户评论群。results: 初步结果表明，Mini-BAR在英文和法语用户评论中具有效果和效率，可以帮助改进软件需求。<details>
<summary>Abstract</summary>
App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. (Replication package containing the code, dataset, and experiment setups on https://github.com/Jl-wei/mini-bar )
</details>
<details>
<summary>摘要</summary>
应用商店上的用户评论对软件需求的改进非常重要。大量有价值的评论不断上传，描述软件问题和期望的功能。有效利用用户评论需要提取有用信息，并将其总结起来。由于用户评论的数量很大，手动分析很困难。基于自然语言处理（NLP）的多种方法已经提出，但大多数需要手动制作数据集来训练其模型，这限制了它们在实际场景中的使用。在这种情况下，我们提出了 mini-bar 工具，它利用大型自然语言模型（LLMs）进行零shot的用户评论挖掘。具体来说， mini-bar 设计用于：1. 分类用户评论2. 将相似的评论集成在一起3. 生成每个集合的抽象摘要4. 对用户评论集合进行排名为评估 mini-bar 的性能，我们创建了包含 6,000 个英语和 6,000 个法语注解用户评论的数据集，并进行了广泛的实验。初步结果表明 mini-bar 在需求工程中是有效率的，通过分析双语应用评论。详细的实验设置和结果可以在 GitHub 上找到：https://github.com/Jl-wei/mini-bar。
</details></li>
</ul>
<hr>
<h2 id="Detecting-Agreement-in-Multi-party-Conversational-AI"><a href="#Detecting-Agreement-in-Multi-party-Conversational-AI" class="headerlink" title="Detecting Agreement in Multi-party Conversational AI"></a>Detecting Agreement in Multi-party Conversational AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03026">http://arxiv.org/abs/2311.03026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Schauer, Jason Sweeney, Charlie Lyttle, Zein Said, Aron Szeles, Cale Clark, Katie McAskill, Xander Wickham, Tom Byars, Daniel Hernández Garcia, Nancie Gunson, Angus Addlesee, Oliver Lemon</li>
<li>for: 这篇论文旨在解决多方会话中的对话系统问题，尤其是在社会助け机器人（SAR）中。</li>
<li>methods: 该论文提出了一种多方会话对话系统，invites两名用户参与一场问答游戏。系统可以检测用户的同意或不同意并应答 accordingly。</li>
<li>results: 我们的评估结果包括性能评估和用户评估结果，强调检测用户同意的能力。我们发布了对应的注释记录和代码在GitHub上。<details>
<summary>Abstract</summary>
Today, conversational systems are expected to handle conversations in multi-party settings, especially within Socially Assistive Robots (SARs). However, practical usability remains difficult as there are additional challenges to overcome, such as speaker recognition, addressee recognition, and complex turn-taking. In this paper, we present our work on a multi-party conversational system, which invites two users to play a trivia quiz game. The system detects users' agreement or disagreement on a final answer and responds accordingly. Our evaluation includes both performance and user assessment results, with a focus on detecting user agreement. Our annotated transcripts and the code for the proposed system have been released open-source on GitHub.
</details>
<details>
<summary>摘要</summary>
今天，对话系统预期能够处理多方会话，特别是在社交助手机器人（SARs）中。然而，实际使用仍然具有困难，因为需要解决更多的挑战，如说话人识别、目标人识别和复杂的回答交换。在这篇论文中，我们介绍了一种多方对话系统， Invites two users to play a trivia quiz game。系统可以检测用户的同意或不同意 final answer，并根据此作出应对。我们的评估包括性能和用户评估结果，强调检测用户同意。我们已经发布了对应的笔记录和系统代码到 GitHub。
</details></li>
</ul>
<hr>
<h2 id="Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement"><a href="#Detecting-agreement-in-multi-party-dialogue-evaluating-speaker-diarisation-versus-a-procedural-baseline-to-enhance-user-engagement" class="headerlink" title="Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement"></a>Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03021">http://arxiv.org/abs/2311.03021</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddenley/multi-person-quiz">https://github.com/ddenley/multi-person-quiz</a></li>
<li>paper_authors: Angus Addlesee, Daniel Denley, Andy Edmondson, Nancie Gunson, Daniel Hernández Garcia, Alexandre Kha, Oliver Lemon, James Ndubuisi, Neil O’Reilly, Lia Perochaud, Raphaël Valeri, Miebaka Worika</li>
<li>for: 这个论文的目的是确定对话状态跟踪中的准确性，以及对话状态跟踪是否能够识别特定的对话事件，如一致或反对。</li>
<li>methods: 这个论文使用了一种合作式问答游戏，其中对话机器人扮演了奖励游戏的主持人，以确定 диаризации模型或频率和靠近性基于的方法是哪一种更加准确地识别一致。</li>
<li>results: 实验结果表明，我们的程序式系统比 диари化系统更加有趣，并且更加准确地识别一致，达到了0.44的平均准确率，而不是0.28。<details>
<summary>Abstract</summary>
Conversational agents participating in multi-party interactions face significant challenges in dialogue state tracking, since the identity of the speaker adds significant contextual meaning. It is common to utilise diarisation models to identify the speaker. However, it is not clear if these are accurate enough to correctly identify specific conversational events such as agreement or disagreement during a real-time interaction. This study uses a cooperative quiz, where the conversational agent acts as quiz-show host, to determine whether diarisation or a frequency-and-proximity-based method is more accurate at determining agreement, and whether this translates to feelings of engagement from the players. Experimental results show that our procedural system was more engaging to players, and was more accurate at detecting agreement, reaching an average accuracy of 0.44 compared to 0.28 for the diarised system.
</details>
<details>
<summary>摘要</summary>
多个对话参与者在多方交流中面临对话状态跟踪挑战，因为说话人的身份带来了Contextual meaning。通常使用分类器来标识说话人。然而，不确定这些模型是否准确地标识特定的对话事件，如一致或反对，在实时交流中。这项研究使用合作问答，其中对话系统 acts as quiz-show host，以确定是否使用分类器或频率和靠近的方法更准确地确定一致，以及这种精度是否导致玩家们的参与感。实验结果表明，我们的程序系统更有趣玩家们，并且更准确地确定一致，达到了0.44的平均准确率，比0.28的分类器系统高。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions"><a href="#Towards-a-Transformer-Based-Reverse-Dictionary-Model-for-Quality-Estimation-of-Definitions" class="headerlink" title="Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions"></a>Towards a Transformer-Based Reverse Dictionary Model for Quality Estimation of Definitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02985">http://arxiv.org/abs/2311.02985</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guité-Vinet Julien, Blondin Massé Alexandre, Sadat Fatiha</li>
<li>for: 本文为了比较不同变体的transformer模型在反词典任务中的表现，并explore其在严肃游戏《词典游戏》中的应用。</li>
<li>methods: 本文使用了不同变体的transformer模型，包括基于BERT的Transformer和基于RoBERTa的Transformer，以及一些自定义的模型。</li>
<li>results: 经过实验和分析，本文发现了不同变体的transformer模型在反词典任务中的表现有很大差异，而基于BERT的Transformer和基于RoBERTa的Transformer表现最佳。<details>
<summary>Abstract</summary>
In the last years, several variants of transformers have emerged. In this paper, we compare different transformer-based models for solving the reverse dictionary task and explore their use in the context of a serious game called The Dictionary Game.
</details>
<details>
<summary>摘要</summary>
最近几年，transformer的多种变体出现了。在这篇论文中，我们对reverse dictionary任务使用不同的transformer基本模型进行比较，并在严肃游戏《词典游戏》中explore其用途。Here's a word-for-word translation:最近几年，transformer的多种变体出现了。在这篇论文中，我们对reverse dictionary任务使用不同的transformer基本模型进行比较，并在严肃游戏《词典游戏》中explore其用途。Note that the word "reverse dictionary" is not a standard term in Chinese, so I had to use the phrase "反ictionary任务" (fǎ yì diǎn yè) to convey the same meaning.
</details></li>
</ul>
<hr>
<h2 id="Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering"><a href="#Adapting-Pre-trained-Generative-Models-for-Extractive-Question-Answering" class="headerlink" title="Adapting Pre-trained Generative Models for Extractive Question Answering"></a>Adapting Pre-trained Generative Models for Extractive Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02961">http://arxiv.org/abs/2311.02961</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/prabirmallick/GenAI4EQA">https://github.com/prabirmallick/GenAI4EQA</a></li>
<li>paper_authors: Prabir Mallick, Tapas Nayak, Indrajit Bhattacharya</li>
<li>for: 提高抽取问答（QA）任务中的精度和效率。</li>
<li>methods: 使用预训练的生成模型生成答案相关的索引，以提高答案的找到率和准确率。</li>
<li>results: 在多个抽取QA数据集上，比如MultiSpanQA、BioASQ、MASHQA和WikiQA等，与现有状态的模型相比，提出的方法显示出了更高的性能。<details>
<summary>Abstract</summary>
Pre-trained Generative models such as BART, T5, etc. have gained prominence as a preferred method for text generation in various natural language processing tasks, including abstractive long-form question answering (QA) and summarization. However, the potential of generative models in extractive QA tasks, where discriminative models are commonly employed, remains largely unexplored. Discriminative models often encounter challenges associated with label sparsity, particularly when only a small portion of the context contains the answer. The challenge is more pronounced for multi-span answers. In this work, we introduce a novel approach that uses the power of pre-trained generative models to address extractive QA tasks by generating indexes corresponding to context tokens or sentences that form part of the answer. Through comprehensive evaluations on multiple extractive QA datasets, including MultiSpanQA, BioASQ, MASHQA, and WikiQA, we demonstrate the superior performance of our proposed approach compared to existing state-of-the-art models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="PhoGPT-Generative-Pre-training-for-Vietnamese"><a href="#PhoGPT-Generative-Pre-training-for-Vietnamese" class="headerlink" title="PhoGPT: Generative Pre-training for Vietnamese"></a>PhoGPT: Generative Pre-training for Vietnamese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02945">http://arxiv.org/abs/2311.02945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vinairesearch/phogpt">https://github.com/vinairesearch/phogpt</a></li>
<li>paper_authors: Dat Quoc Nguyen, Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Nhung Nguyen, Thien Huu Nguyen, Dinh Phung, Hung Bui</li>
<li>for: 本研究开发了一个新的开源Generative模型系列，名为PhoGPT，用于越南语言。</li>
<li>methods: 这个模型使用了7.5亿个参数，并且包括基础预训练模型PhoGPT-7B5和其指令追踪版本PhoGPT-7B5-Instruct。</li>
<li>results: 该研究透过人类评估实验，证明了PhoGPT的超越前一代开源模型的性能。Here’s the English version of the three key points:</li>
<li>for: This study develops a new open-source generative model series for Vietnamese, named PhoGPT.</li>
<li>methods: The model uses 7.5 billion parameters and includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct.</li>
<li>results: The study demonstrates the superior performance of PhoGPT through a human evaluation experiment compared to previous open-source models.<details>
<summary>Abstract</summary>
We open-source a state-of-the-art 7.5B-parameter generative model series named PhoGPT for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-7B5 and its instruction-following variant, PhoGPT-7B5-Instruct. In addition, we also demonstrate its superior performance compared to previous open-source models through a human evaluation experiment. GitHub: https://github.com/VinAIResearch/PhoGPT
</details>
<details>
<summary>摘要</summary>
我们开源了一系列当前最佳的7.5亿参数生成模型，名为 PhoGPT，用于越南语言。该系列包括基础预训练单语言模型 PhoGPT-7B5 和其 instruciton-following 变体 PhoGPT-7B5-Instruct。此外，我们还通过人工评估实验表明了它们的性能比前一代开源模型更高。GitHub：https://github.com/VinAIResearch/PhoGPT
</details></li>
</ul>
<hr>
<h2 id="SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data"><a href="#SQLPrompt-In-Context-Text-to-SQL-with-Minimal-Labeled-Data" class="headerlink" title="SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data"></a>SQLPrompt: In-Context Text-to-SQL with Minimal Labeled Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02883">http://arxiv.org/abs/2311.02883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruoxi Sun, Sercan Ö. Arik, Rajarishi Sinha, Hootan Nakhost, Hanjun Dai, Pengcheng Yin, Tomas Pfister</li>
<li>for: 提高 Text-to-SQL 模型在大型自然语言模型（LLM）上的几个shot提问能力。</li>
<li>methods: 提出了创新的提示设计、执行基于一致性解oding策略和多种提示设计和基础模型（MixPrompt和MixLLMs）来提高 SQL 提问性能。</li>
<li>results: 比前方法在受Context学习中几个shot提问下表现较好，减小了与finetuning state-of-the-art的标准化数据量 gap。<details>
<summary>Abstract</summary>
Text-to-SQL aims to automate the process of generating SQL queries on a database from natural language text. In this work, we propose "SQLPrompt", tailored to improve the few-shot prompting capabilities of Text-to-SQL for Large Language Models (LLMs). Our methods include innovative prompt design, execution-based consistency decoding strategy which selects the SQL with the most consistent execution outcome among other SQL proposals, and a method that aims to improve performance by diversifying the SQL proposals during consistency selection with different prompt designs ("MixPrompt") and foundation models ("MixLLMs"). We show that \emph{SQLPrompt} outperforms previous approaches for in-context learning with few labeled data by a large margin, closing the gap with finetuning state-of-the-art with thousands of labeled data.
</details>
<details>
<summary>摘要</summary>
文本到SQL是一项工作，旨在自动将自然语言文本转换为数据库中的SQL查询。在这项工作中，我们提出了“SQLPrompt”，用于改进大语言模型（LLM）的几个shot提示能力。我们的方法包括创新的提示设计、执行基于一致性解码策略和多种提示设计和基础模型（MixPrompt）以及多种基础模型（MixLLMs）来提高性能。我们证明了，对于少量标注数据进行Context learning，\emph{SQLPrompt}能够大幅超越先前的方法，逐渐追赶到精度调整的状态码。
</details></li>
</ul>
<hr>
<h2 id="Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision"><a href="#Less-than-One-shot-Named-Entity-Recognition-via-Extremely-Weak-Supervision" class="headerlink" title="Less than One-shot: Named Entity Recognition via Extremely Weak Supervision"></a>Less than One-shot: Named Entity Recognition via Extremely Weak Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02861">http://arxiv.org/abs/2311.02861</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/KomeijiForce/X-NER">https://github.com/KomeijiForce/X-NER</a></li>
<li>paper_authors: Letian Peng, Zihan Wang, Jingbo Shang</li>
<li>for: 本文研究了无监督下的命名实体识别（NER）问题，具体来说是使用一个例子实体来监督模型学习。</li>
<li>methods: 我们提出了一种新的X-NER方法，它可以在无监督下超越当前一阶段NER方法的性能。我们首先在无标注训练集中挖掘类似于示例实体的实体 span，然后利用这些 span 作为pseudo-标签来训练 NER 标注器。</li>
<li>results: 我们在四个NER数据集上进行了广泛的实验和分析，发现 X-NER 方法可以在无监督下实现显著的NER性能提升，并且在一些ew-shot环境下超越当前一阶段一shot NER方法。此外，我们发现 X-NER 方法具有跨语言能力。<details>
<summary>Abstract</summary>
We study the named entity recognition (NER) problem under the extremely weak supervision (XWS) setting, where only one example entity per type is given in a context-free way. While one can see that XWS is lighter than one-shot in terms of the amount of supervision, we propose a novel method X-NER that can outperform the state-of-the-art one-shot NER methods. We first mine entity spans that are similar to the example entities from an unlabelled training corpus. Instead of utilizing entity span representations from language models, we find it more effective to compare the context distributions before and after the span is replaced by the entity example. We then leverage the top-ranked spans as pseudo-labels to train an NER tagger. Extensive experiments and analyses on 4 NER datasets show the superior end-to-end NER performance of X-NER, outperforming the state-of-the-art few-shot methods with 1-shot supervision and ChatGPT annotations significantly. Finally, our X-NER possesses several notable properties, such as inheriting the cross-lingual abilities of the underlying language models.
</details>
<details>
<summary>摘要</summary>
我们研究名实体识别（NER）问题在极度轻量级监督（XWS） Setting下，只有一个例子实体每种类型提供在上下文自由的方式。虽然XWS比一遍监督更轻，但我们提议一种新的方法X-NER，可以超越现有的一遍NER方法。我们首先从无标注训练集中挖掘类似于示例实体的实体排版。而不是使用语言模型生成的实体排版表示，我们发现更有效的是比较在替换后的上下文分布和替换前的上下文分布。然后，我们利用排名最高的排版作为pseudo-标签来训练NER标注器。广泛的实验和分析在4个NER数据集上表明，X-NER possess着Superior end-to-end NER性能，超过现有的几个频shot方法和ChatGPT注释。最后，我们的X-NER具有许多值得注意的性能，如继承下来的语言模型的cross-语言能力。
</details></li>
</ul>
<hr>
<h2 id="Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding"><a href="#Improving-Machine-Translation-with-Large-Language-Models-A-Preliminary-Study-with-Cooperative-Decoding" class="headerlink" title="Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding"></a>Improving Machine Translation with Large Language Models: A Preliminary Study with Cooperative Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02851">http://arxiv.org/abs/2311.02851</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lemon0830/CoDec">https://github.com/lemon0830/CoDec</a></li>
<li>paper_authors: Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou</li>
<li>for: 本研究旨在了解LLMs在不同场景下的表现，以及如何使用它们与传统的NMT系统结合以提高翻译质量。</li>
<li>methods: 我们首先进行了一项全面的分析，以评估不同的商业NMT系统和MT-oriented LLMs的优缺点。我们发现， neither NMT nor MT-oriented LLMs alone 可以有效地解决所有翻译问题，但 MT-oriented LLMs 可以作为NMT系统的补充解决方案。</li>
<li>results: 我们的结果表明，CoDec 可以有效地与NMT系统结合，并且在 WMT22 测试集和我们新收集的 WebCrawl 测试集上达到了显著的效果和效率。这 highlights CoDec 的潜在作用性作为一种robust的翻译结合方案。<details>
<summary>Abstract</summary>
Contemporary translation engines built upon the encoder-decoder framework have reached a high level of development, while the emergence of Large Language Models (LLMs) has disrupted their position by offering the potential for achieving superior translation quality. Therefore, it is crucial to understand in which scenarios LLMs outperform traditional NMT systems and how to leverage their strengths. In this paper, we first conduct a comprehensive analysis to assess the strengths and limitations of various commercial NMT systems and MT-oriented LLMs. Our findings indicate that neither NMT nor MT-oriented LLMs alone can effectively address all the translation issues, but MT-oriented LLMs can serve as a promising complement to the NMT systems. Building upon these insights, we explore hybrid methods and propose Cooperative Decoding (CoDec), which treats NMT systems as a pretranslation model and MT-oriented LLMs as a supplemental solution to handle complex scenarios beyond the capability of NMT alone. The results on the WMT22 test sets and a newly collected test set WebCrawl demonstrate the effectiveness and efficiency of CoDec, highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in machine translation.
</details>
<details>
<summary>摘要</summary>
当代翻译引擎基于encoder-decoder框架已经达到了高度的发展，而大语言模型（LLM）的出现则打破了传统翻译系统的位置，提供了可能达到更高的翻译质量。因此，理解LLMs在哪些场景下表现更出色，以及如何利用其优势是关键。在这篇论文中，我们首先进行了全面的分析，评估不同的商业NMT系统和MT-oriented LLMs的优缺点。我们的发现表明， neither NMT nor MT-oriented LLMs 可以单独解决所有翻译问题，但MT-oriented LLMs可以作为NMT系统的补充解决方案。基于这些发现，我们探索了混合方法，并提出了协同解码（CoDec），即将NMT系统作为预翻译模型，MT-oriented LLMs作为NMT系统之外的补充解决方案。WMT22测试集和我们新收集的WebCrawl测试集的结果表明，CoDec是一种有效和高效的解决方案， highlighting its potential as a robust solution for combining NMT systems with MT-oriented LLMs in machine translation.
</details></li>
</ul>
<hr>
<h2 id="Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation"><a href="#Tailoring-Self-Rationalizers-with-Multi-Reward-Distillation" class="headerlink" title="Tailoring Self-Rationalizers with Multi-Reward Distillation"></a>Tailoring Self-Rationalizers with Multi-Reward Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02805">http://arxiv.org/abs/2311.02805</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ink-usc/rationalemultirewarddistillation">https://github.com/ink-usc/rationalemultirewarddistillation</a></li>
<li>paper_authors: Sahana Ramnath, Brihi Joshi, Skyler Hallinan, Ximing Lu, Liunian Harold Li, Aaron Chan, Jack Hessel, Yejin Choi, Xiang Ren</li>
<li>for: 本研究旨在使小型语言模型（LM）能够生成有用的自我论证，以帮助问答系统提高问答性能。</li>
<li>methods: 本研究使用的方法是Multi-rewArd RatIOnalization（MaRio）算法，该算法通过多个奖励条件来优化自我论证的多种特性，如可能性、多样性和一致性。</li>
<li>results: 结果表明，MaRio算法不仅能够提高问答任务的准确率，而且还能够提高小LM的自我论证质量，包括可能性、多样性和一致性的评价。人类评价也表明，MaRio的论证比基于精心微调（SFT）的论证更受欢迎和更有优势。<details>
<summary>Abstract</summary>
Large language models (LMs) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter GPT-3); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale LMs (approx. 200x smaller than GPT-3) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, MaRio (Multi-rewArd RatIOnalization), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on five difficult question-answering datasets StrategyQA, QuaRel, OpenBookQA, NumerSense and QASC show that not only does MaRio improve task accuracy, but it also improves the self-rationalization quality of small LMs across the aforementioned axes better than a supervised fine-tuning (SFT) baseline. Extensive human evaluations confirm that MaRio rationales are preferred vs. SFT rationales, as well as qualitative improvements in plausibility and consistency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.CL_2023_11_06/" data-id="cloq1wl3w00e07o888m5r0n81" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/cs.LG_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T10:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/cs.LG_2023_11_06/">cs.LG - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CAFE-Carbon-Aware-Federated-Learning-in-Geographically-Distributed-Data-Centers"><a href="#CAFE-Carbon-Aware-Federated-Learning-in-Geographically-Distributed-Data-Centers" class="headerlink" title="CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers"></a>CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03615">http://arxiv.org/abs/2311.03615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jieming Bian, Shaolei Ren, Jie Xu</li>
<li>for: This paper aims to address the challenges of training large-scale AI models while minimizing their carbon footprint.</li>
<li>methods: The paper proposes a new framework called CAFE (Carbon-Aware Federated Learning) that incorporates coreset selection, Lyapunov drift-plus-penalty, and efficient algorithm to optimize training within a fixed carbon footprint budget.</li>
<li>results: The paper demonstrates the efficacy of the proposed algorithm through extensive simulations using real-world carbon intensity data, showing its superiority over existing methods in optimizing learning performance while minimizing environmental impact.<details>
<summary>Abstract</summary>
Training large-scale artificial intelligence (AI) models demands significant computational power and energy, leading to increased carbon footprint with potential environmental repercussions. This paper delves into the challenges of training AI models across geographically distributed (geo-distributed) data centers, emphasizing the balance between learning performance and carbon footprint. We consider Federated Learning (FL) as a solution, which prioritizes model parameter exchange over raw data, ensuring data privacy and compliance with local regulations. Given the variability in carbon intensity across regions, we propose a new framework called CAFE (short for Carbon-Aware Federated Learning) to optimize training within a fixed carbon footprint budget. Our approach incorporates coreset selection to assess learning performance, employs the Lyapunov drift-plus-penalty framework to address the unpredictability of future carbon intensity, and devises an efficient algorithm to address the combinatorial complexity of the data center selection. Through extensive simulations using real-world carbon intensity data, we demonstrate the efficacy of our algorithm, highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact.
</details>
<details>
<summary>摘要</summary>
训练大规模人工智能（AI）模型需要巨大的计算能力和能源，导致增加碳脚印的可能性，有潜在的环境影响。这篇论文探讨跨地区分布的数据中心（geo-distributed）训练AI模型所遇到的挑战，强调学习性和碳脚印之间的平衡。我们认为 Federated Learning（FL）是一种解决方案，它强调模型参数交换而不是原始数据，保证数据隐私和当地法规的遵守。由于地区碳Intensity的变化，我们提出了一个新的框架called CAFE（缩写为 Carbon-Aware Federated Learning），以优化在固定碳脚印预算内进行训练。我们的方法包括核心选择来评估学习性能，使用Lyapunov逸偏离策略来Address未来碳Intensity的不可预知性，并开发了高效的数据中心选择算法。通过使用实际碳Intensity数据进行大规模的 simulations，我们证明了我们的算法的有效性， highlighting its superiority over existing methods in optimizing learning performance while minimizing environmental impact.
</details></li>
</ul>
<hr>
<h2 id="Plug-and-Play-Stability-for-Intracortical-Brain-Computer-Interfaces-A-One-Year-Demonstration-of-Seamless-Brain-to-Text-Communication"><a href="#Plug-and-Play-Stability-for-Intracortical-Brain-Computer-Interfaces-A-One-Year-Demonstration-of-Seamless-Brain-to-Text-Communication" class="headerlink" title="Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication"></a>Plug-and-Play Stability for Intracortical Brain-Computer Interfaces: A One-Year Demonstration of Seamless Brain-to-Text Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03611">http://arxiv.org/abs/2311.03611</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cffan/corp">https://github.com/cffan/corp</a></li>
<li>paper_authors: Chaofei Fan, Nick Hahn, Foram Kamdar, Donald Avansino, Guy H. Wilson, Leigh Hochberg, Krishna V. Shenoy, Jaimie M. Henderson, Francis R. Willett</li>
<li>for: This paper aims to address the issue of frequent recalibration required for intracortical brain-computer interfaces (iBCIs) to maintain high performance, which can be time-consuming and interrupt the user’s experience.</li>
<li>methods: The proposed method, called Continual Online Recalibration with Pseudo-labels (CORP), leverages large language models to automatically correct errors in iBCI outputs and update the decoder online.</li>
<li>results: The CORP framework achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task over a period of 403 days, significantly outperforming other baseline methods. This is the longest-running iBCI stability demonstration involving a human participant, providing evidence for the long-term stabilization of a plug-and-play, high-performance communication iBCI.<details>
<summary>Abstract</summary>
Intracortical brain-computer interfaces (iBCIs) have shown promise for restoring rapid communication to people with neurological disorders such as amyotrophic lateral sclerosis (ALS). However, to maintain high performance over time, iBCIs typically need frequent recalibration to combat changes in the neural recordings that accrue over days. This requires iBCI users to stop using the iBCI and engage in supervised data collection, making the iBCI system hard to use. In this paper, we propose a method that enables self-recalibration of communication iBCIs without interrupting the user. Our method leverages large language models (LMs) to automatically correct errors in iBCI outputs. The self-recalibration process uses these corrected outputs ("pseudo-labels") to continually update the iBCI decoder online. Over a period of more than one year (403 days), we evaluated our Continual Online Recalibration with Pseudo-labels (CORP) framework with one clinical trial participant. CORP achieved a stable decoding accuracy of 93.84% in an online handwriting iBCI task, significantly outperforming other baseline methods. Notably, this is the longest-running iBCI stability demonstration involving a human participant. Our results provide the first evidence for long-term stabilization of a plug-and-play, high-performance communication iBCI, addressing a major barrier for the clinical translation of iBCIs.
</details>
<details>
<summary>摘要</summary>
干预 cortical 脑机器交互（iBCI）已经表现出了恢复快速通信的潜力，尤其是对于amyotrophic lateral sclerosis（ALS）等神经系统疾病。然而，以保持高性能而言，iBCI通常需要频繁重新寻常化，以避免因日期而增加的神经记录变化。这要求iBCI用户停止使用iBCI并进行监控数据采集，使iBCI系统变得困难使用。在这篇论文中，我们提议了一种方法，可以让通信iBCI进行自动重新寻常化，无需中断用户。我们的方法利用大型语言模型（LM）来自动更正iBCI输出中的错误。自动重新寻常化过程使用这些更正后的输出（ Pseudo-labels）来在线更新iBCI解码器。在403天的测试期间，我们通过我们的Continual Online Recalibration with Pseudo-labels（CORP）框架进行了一项临床试验。CORP在在线手写iBCI任务中实现了93.84%的稳定解码率，与其他基线方法相比，显著超越。这也是人类参与者的 longest-running iBCI稳定展示。我们的结果提供了首次的长期稳定性证明，解决了iBCI在临床翻译的主要障碍。
</details></li>
</ul>
<hr>
<h2 id="Testing-RadiX-Nets-Advances-in-Viable-Sparse-Topologies"><a href="#Testing-RadiX-Nets-Advances-in-Viable-Sparse-Topologies" class="headerlink" title="Testing RadiX-Nets: Advances in Viable Sparse Topologies"></a>Testing RadiX-Nets: Advances in Viable Sparse Topologies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03609">http://arxiv.org/abs/2311.03609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kevin Kwak, Zack West, Hayden Jananthan, Jeremy Kepner</li>
<li>for: 这篇论文主要针对hyper-parametrized deep neural networks (DNNs) 的简化，以减少 computation demands in ML research and industry use.</li>
<li>methods: 论文使用RadiX-Nets，一种 subgroup of sparse DNNs，以维持 uniformity 并且获得更好的性能。</li>
<li>results: 论文通过实验发现，RadiX-Nets 在 TensorFlow 中的表现不受初始化和训练方法的影响，并且发现了一些 “strange models” 的现象，这些模型在训练时间较长且精度较低。<details>
<summary>Abstract</summary>
The exponential growth of data has sparked computational demands on ML research and industry use. Sparsification of hyper-parametrized deep neural networks (DNNs) creates simpler representations of complex data. Past research has shown that some sparse networks achieve similar performance as dense ones, reducing runtime and storage. RadiX-Nets, a subgroup of sparse DNNs, maintain uniformity which counteracts their lack of neural connections. Generation, independent of a dense network, yields faster asymptotic training and removes the need for costly pruning. However, little work has been done on RadiX-Nets, making testing challenging. This paper presents a testing suite for RadiX-Nets in TensorFlow. We test RadiX-Net performance to streamline processing in scalable models, revealing relationships between network topology, initialization, and training behavior. We also encounter "strange models" that train inconsistently and to lower accuracy while models of similar sparsity train well.
</details>
<details>
<summary>摘要</summary>
“数据的 exponential 增长对 machine learning 研究和实际应用带来了 computation 的需求。通过减少 hyper-parametrized deep neural networks（DNNs）中的参数，可以创造简洁的数据表示。过去的研究表明，一些简洁网络可以与密集网络具有相同的性能，同时降低 runtime 和存储成本。RadiX-Nets 是一 subgroup of sparse DNNs，它们保持了 uniformity，并且独立于密集网络进行生成，从而减少了训练的时间和成本。然而，对 RadiX-Nets 的研究不多，测试很困难。这篇文章提出了一个基于 TensorFlow 的 RadiX-Nets 测试 suite。我们测试 RadiX-Net 的性能，以便在扩展性好的模型中进行流线处理。我们还发现了一些“strange models”，它们在不同的初始化和训练方法下存在不一致的训练行为和较低的准确率。”Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Generative-Diffusion-Models-for-Lattice-Field-Theory"><a href="#Generative-Diffusion-Models-for-Lattice-Field-Theory" class="headerlink" title="Generative Diffusion Models for Lattice Field Theory"></a>Generative Diffusion Models for Lattice Field Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03578">http://arxiv.org/abs/2311.03578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingxiao Wang, Gert Aarts, Kai Zhou</li>
<li>for: 这个研究paper explores the connection between machine learning and lattice field theory by linking generative diffusion models (DMs) with stochastic quantization, from a stochastic differential equation perspective.</li>
<li>methods: The paper shows that DMs can be conceptualized by reversing a stochastic process driven by the Langevin equation, which produces samples from an initial distribution to approximate the target distribution.</li>
<li>results: The paper demonstrates the capability of DMs to learn effective actions and its feasibility to act as a global sampler for generating configurations in the two-dimensional $\phi^4$ quantum lattice field theory.<details>
<summary>Abstract</summary>
This study delves into the connection between machine learning and lattice field theory by linking generative diffusion models (DMs) with stochastic quantization, from a stochastic differential equation perspective. We show that DMs can be conceptualized by reversing a stochastic process driven by the Langevin equation, which then produces samples from an initial distribution to approximate the target distribution. In a toy model, we highlight the capability of DMs to learn effective actions. Furthermore, we demonstrate its feasibility to act as a global sampler for generating configurations in the two-dimensional $\phi^4$ quantum lattice field theory.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Graph-Theoretic-Framework-for-Understanding-Open-World-Semi-Supervised-Learning"><a href="#A-Graph-Theoretic-Framework-for-Understanding-Open-World-Semi-Supervised-Learning" class="headerlink" title="A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning"></a>A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03524">http://arxiv.org/abs/2311.03524</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deeplearning-wisc/sorl">https://github.com/deeplearning-wisc/sorl</a></li>
<li>paper_authors: Yiyou Sun, Zhenmei Shi, Yixuan Li</li>
<li>for: 这篇论文目的是提出一种基于图论的开放世界 semi-supervised 学习方法，以便在无标签数据中推断已知和新类，并且提供了理论基础。</li>
<li>methods: 该论文使用了图论来形式化开放世界设定下的卷积问题，并提出了一种名为spectral open-world representation learning（SORL）的算法。SORL 算法的核心思想是将图形式化为一个 Spectral 问题，并使用spectral decomposition来解决卷积问题。</li>
<li>results: 该论文通过实验表明，SORL 算法可以与一些强基线相比或超越它们，并且可以提供理论上的保证。具体来说，SORL 算法可以在常见的 benchmark 数据集上实现比较好的 clustering 性能，而且可以在实际应用中具有理论上的保证。<details>
<summary>Abstract</summary>
Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data, by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a graph-theoretic framework tailored for the open-world setting, where the clustering can be theoretically characterized by graph factorization. Our graph-theoretic framework illuminates practical algorithms and provides guarantees. In particular, based on our graph formulation, we apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that minimizing our loss is equivalent to performing spectral decomposition on the graph. Such equivalence allows us to derive a provable error bound on the clustering performance for both known and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
Open-world semi-supervised learning目标是在无标签数据中推断已知和新类，通过利用已知类标注集的知识来提供优化。然而，这个问题在理论基础上存在缺失。这篇论文填补了这个鸿蒙，通过 graf-based 框架来形式化开放世界设置下的划分。我们的 graf-based 框架可以 theoretically Characterize 划分，并且提供了实用的算法和保证。具体来说，基于我们的 graf 表示法，我们应用了 Spectral Open-world Representation Learning（SORL）算法，并证明了将我们的损失函数下降等价于在 graf 上进行 spectral decomposition。这种等价性允许我们 derive 划分性能的证明性 bound，并且分析了在已知类标注集的帮助下，labeled 数据的作用。实验表明，SORL 可以与一些强大的基线算法相匹配或超越，这是在实践中获得理论保证的愉悦。
</details></li>
</ul>
<hr>
<h2 id="The-Fairness-Stitch-Unveiling-the-Potential-of-Model-Stitching-in-Neural-Network-De-Biasing"><a href="#The-Fairness-Stitch-Unveiling-the-Potential-of-Model-Stitching-in-Neural-Network-De-Biasing" class="headerlink" title="The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing"></a>The Fairness Stitch: Unveiling the Potential of Model Stitching in Neural Network De-Biasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03532">http://arxiv.org/abs/2311.03532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/modar7/the_fairness_stitch">https://github.com/modar7/the_fairness_stitch</a></li>
<li>paper_authors: Modar Sulaiman, Kallol Roy</li>
<li>for: 这篇研究的目的是提高机器学习模型的公平性，以解决不同应用中的偏见和歧视问题。</li>
<li>methods: 本研究提出了一个名为“公平缝纫（The Fairness Stitch，TFS）”的新方法，它结合了模型缝纫和训练，同时将公平性约束组入。</li>
<li>results: 这篇研究的结果显示，TFS方法能够实现更好的公平性和性能之间的平衡，比较于现有的基eline方法。<details>
<summary>Abstract</summary>
The pursuit of fairness in machine learning models has emerged as a critical research challenge in different applications ranging from bank loan approval to face detection. Despite the widespread adoption of artificial intelligence algorithms across various domains, concerns persist regarding the presence of biases and discrimination within these models. To address this pressing issue, this study introduces a novel method called "The Fairness Stitch (TFS)" to enhance fairness in deep learning models. This method combines model stitching and training jointly, while incorporating fairness constraints. In this research, we assess the effectiveness of our proposed method by conducting a comprehensive evaluation of two well-known datasets, CelebA and UTKFace. We systematically compare the performance of our approach with the existing baseline method. Our findings reveal a notable improvement in achieving a balanced trade-off between fairness and performance, highlighting the promising potential of our method to address bias-related challenges and foster equitable outcomes in machine learning models. This paper poses a challenge to the conventional wisdom of the effectiveness of the last layer in deep learning models for de-biasing.
</details>
<details>
<summary>摘要</summary>
“机器学习模型中的公平性追求已经成为不同应用领域的重要研究挑战，从银行贷款批准到面部识别。尽管人工智能算法在各个领域得到了广泛的采用，但是存在偏见和歧视的担忧仍然存在。为解决这个紧要的问题，本研究提出了一种新的方法 called“公平缝纫（TFS）”，用于增强机器学习模型的公平性。这个方法结合了模型缝纫和训练的过程，并将公平性约束纳入到模型中。在这个研究中，我们对两个常用的数据集CelebA和UTKFace进行了全面的评估，并与现有的基准方法进行比较。我们发现，我们的方法可以更好地实现公平性和性能之间的平衡，这显示了我们的方法具有对偏见相关挑战的应对能力，并可以实现更加公平的结果。本研究挑战了传统的机器学习模型中最后一层的偏见处理方法的有效性。”
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Local-Computations-in-Distributed-Bayesian-Learning"><a href="#Asynchronous-Local-Computations-in-Distributed-Bayesian-Learning" class="headerlink" title="Asynchronous Local Computations in Distributed Bayesian Learning"></a>Asynchronous Local Computations in Distributed Bayesian Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03496">http://arxiv.org/abs/2311.03496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kinjal Bhar, He Bai, Jemin George, Carl Busart<br>for: 这篇论文主要针对于分布式机器学习（ML）中的分布式推理算法，即多个智能 Device 之间的协同学习。methods: 该论文提出了一种基于干扰通信的异步推理算法，使用了本地计算和between successive inter-agent communications。results: 论文通过使用 Bayesian sampling via unadjusted Langevin algorithm (ULA) MCMC 和 theoretically quantify the convergence rates，证明了该算法的效果。在一个小型问题和实际数据集上进行了丰富的 simulations，并观察到了更快的初始快速收敛和改善的性能精度，特别是在低数据范围内。在Gamma Telescope和mHealth数据集上，共计达到了78%和90%的分类精度。<details>
<summary>Abstract</summary>
Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via unadjusted Langevin algorithm (ULA) MCMC is utilized. The communication is assumed to be over a connected graph (e.g., as in decentralized learning), however, the results can be extended to coordinated communication where there is a central server (e.g., federated learning). We theoretically quantify the convergence rates in the process. To demonstrate the efficacy of the proposed algorithm, we present simulations on a toy problem as well as on real world data sets to train ML models to perform classification tasks. We observe faster initial convergence and improved performance accuracy, especially in the low data range. We achieve on average 78% and over 90% classification accuracy respectively on the Gamma Telescope and mHealth data sets from the UCI ML repository.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-High-Level-Synthesis-and-Large-Language-Models-to-Generate-Simulate-and-Deploy-a-Uniform-Random-Number-Generator-Hardware-Design"><a href="#Leveraging-High-Level-Synthesis-and-Large-Language-Models-to-Generate-Simulate-and-Deploy-a-Uniform-Random-Number-Generator-Hardware-Design" class="headerlink" title="Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design"></a>Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03489">http://arxiv.org/abs/2311.03489</a></li>
<li>repo_url: None</li>
<li>paper_authors: James T. Meech</li>
<li>for: 用于开发具有特定应用场景的集成电路设计。</li>
<li>methods: 使用大语言模型工具，仅使用开源工具（ exclude 大语言模型），实现硬件设计生成。</li>
<li>results: 通过使用大语言模型生成的 simulate 和 Dieharder 随机性测试盘，验证了Permuted Congruential Random Number Generator 设计的正常性和质量。<details>
<summary>Abstract</summary>
We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabrication in more modern process nodes.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的高级合成方法，使用大型自然语言模型工具生成硬件设计。这种方法使用exclusively开源工具，排除了大型自然语言模型。作为一个案例研究，我们使用了我们的方法生成一个卷积排序随机数生成器设计，具有愿望形接口。我们使用大型自然语言模型生成的 simulations和Dieharder随机性测试集来验证随机数生成器设计的功能和质量。我们记录了所有的大型自然语言模型对话记录、Python脚本、Verilog脚本和 simulations 结果，以便在案例研究中进行参考。我们认为，我们的硬件设计生成方法，结合开源130nm设计工具，将重塑应用特定集成电路设计领域。我们的方法可以大大降低在建立领域特定计算加速器和互联网物联网设备的门槛，并为后续更高级的处理节点fabrication提供证明。
</details></li>
</ul>
<hr>
<h2 id="Uni-O4-Unifying-Online-and-Offline-Deep-Reinforcement-Learning-with-Multi-Step-On-Policy-Optimization"><a href="#Uni-O4-Unifying-Online-and-Offline-Deep-Reinforcement-Learning-with-Multi-Step-On-Policy-Optimization" class="headerlink" title="Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization"></a>Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03351">http://arxiv.org/abs/2311.03351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu</li>
<li>for: 本研究旨在提出一种 straightforward yet effective 的 offline和online reinforcement learning 方法，以便在各种实际应用中快速部署。</li>
<li>methods: 该方法使用了一种单一的对象函数来结合offline和online学习，从而实现了灵活的学习模式，允许任意的预训练、细化和offline&#x2F;online学习组合。在offline阶段，该方法使用了多个ensemble政策来解决行为策略与offline数据集之间的差异问题。</li>
<li>results: 该方法可以在真实世界 robot 任务上实现superior的offline初始化以及稳定的在线细化 capacities。通过多个 simulate benchmark 的全面评估，我们证明了该方法在offline和offline-to-online学习中具有state-of-the-art表现。<details>
<summary>Abstract</summary>
Combining offline and online reinforcement learning (RL) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization? In this study, we propose Uni-o4, which utilizes an on-policy objective for both offline and online learning. Owning to the alignment of objectives in two phases, the RL agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-o4 leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation (OPE) approach, Uni-o4 can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments. Additionally, through comprehensive evaluations using numerous simulated benchmarks, we substantiate that our method achieves state-of-the-art performance in both offline and offline-to-online fine-tuning learning. Our website: https://lei-kun.github.io/uni-o4/ .
</details>
<details>
<summary>摘要</summary>
combining 线上和线下强化学习 (RL) 是关键 для高效和安全的学习。然而，先前的方法将线上和线下学习视为分开的过程，导致了重复的设计和限制性能。我们问：可以达到简单又有效的线上和线下学习，不需要额外的保守性或正则化吗？在这种研究中，我们提出了Uni-o4，它利用了在两个阶段中的同步目标，使RL机器人可以在线上和线下学习无缝转换。这种性能的可更换性，允许学习模式的任意组合，包括预训练、细化、线上和线下学习。在线上阶段，Uni-o4特别利用了多个ensemble政策来解决对行为策略估计和线上数据集的匹配问题。通过简单的线上策略评估（OPE）方法，Uni-o4可以安全地实现多步策略改进。我们示出，通过上述方法，线上和线下学习的融合可以实现出色的初始化以及稳定和快速的在线细化能力。通过实际的 робоット任务，我们强调了这种 Paradigma 在面临实际、以前未看到的挑战时的快速部署的优势。此外，通过大量的模拟 benchmark 的全面评估，我们证明了我们的方法在线上和线下初始化学习以及在线-到-线上细化学习中具有状态 искус力表现。更多信息请访问我们的网站：https://lei-kun.github.io/uni-o4/。
</details></li>
</ul>
<hr>
<h2 id="Learning-Hard-Constrained-Models-with-One-Sample"><a href="#Learning-Hard-Constrained-Models-with-One-Sample" class="headerlink" title="Learning Hard-Constrained Models with One Sample"></a>Learning Hard-Constrained Models with One Sample</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03332">http://arxiv.org/abs/2311.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Galanis, Alkis Kalavasis, Anthimos Vardis Kandiros</li>
<li>for: 这篇论文主要研究了用单个样本来估计Markov随机场的参数，并应用于$k$-SAT、正确颜色模型和通用$H$-颜色模型等问题。</li>
<li>methods: 该论文使用pseudo-likelihood estimator，并使用了 coupling技术来获得变量 bounds。</li>
<li>results: 研究发现，在soft-constrained情况下不 siempre可以使用单个样本来估计参数，而且存在非满足性INSTances的问题。对于$k$-SAT和正确颜色模型，提供了正确的估计器，而对于通用$H$-颜色模型，则需要更强的condition来 garantizar sampling。<details>
<summary>Abstract</summary>
We consider the problem of estimating the parameters of a Markov Random Field with hard-constraints using a single sample. As our main running examples, we use the $k$-SAT and the proper coloring models, as well as general $H$-coloring models; for all of these we obtain both positive and negative results. In contrast to the soft-constrained case, we show in particular that single-sample estimation is not always possible, and that the existence of an estimator is related to the existence of non-satisfiable instances.   Our algorithms are based on the pseudo-likelihood estimator. We show variance bounds for this estimator using coupling techniques inspired, in the case of $k$-SAT, by Moitra's sampling algorithm (JACM, 2019); our positive results for colorings build on this new coupling approach. For $q$-colorings on graphs with maximum degree $d$, we give a linear-time estimator when $q>d+1$, whereas the problem is non-identifiable when $q\leq d+1$. For general $H$-colorings, we show that standard conditions that guarantee sampling, such as Dobrushin's condition, are insufficient for one-sample learning; on the positive side, we provide a general condition that is sufficient to guarantee linear-time learning and obtain applications for proper colorings and permissive models. For the $k$-SAT model on formulas with maximum degree $d$, we provide a linear-time estimator when $k\gtrsim 6.45\log d$, whereas the problem becomes non-identifiable when $k\lesssim \log d$.
</details>
<details>
<summary>摘要</summary>
我们考虑一个推估Markov随机场景中的参数，使用单一样本。我们的主要Running例是$k$-SAT和proper颜色模型，以及一般的$H$-颜色模型。我们获得了both positive和negative结果。在不同于软链接的情况下，我们表明单一样本推估不一定可行，并且存在非满足性的实例。我们的算法基于伪贝氏可能性推估器。我们使用对Moitra的抽样算法（JACM, 2019）的对抗技术来获得变数上下限。在$k$-SAT模型中，我们给出了线性时间的推估器，当$q>d+1$时，而当$q\leq d+1$时，问题是非归一性的。对于一般的$H$-颜色模型，我们表明了样本推估是不可能的，因为Dobrushin的condition是不充分的。然而，我们提供了一个一般的condition，可以保证线性时间的学习，并且有应用于正颜色和允许模型。在$k$-SAT模型中，我们给出了线性时间的推估器，当$k\gtrsim 6.45\log d$时，而当$k\lesssim \log d$时，问题是非归一性的。
</details></li>
</ul>
<hr>
<h2 id="Practical-considerations-for-variable-screening-in-the-Super-Learner"><a href="#Practical-considerations-for-variable-screening-in-the-Super-Learner" class="headerlink" title="Practical considerations for variable screening in the Super Learner"></a>Practical considerations for variable screening in the Super Learner</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03313">http://arxiv.org/abs/2311.03313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bdwilliamson/sl_screening_supplementary">https://github.com/bdwilliamson/sl_screening_supplementary</a></li>
<li>paper_authors: Brian D. Williamson, Drew King, Ying Huang</li>
<li>for: 本研究旨在探讨Super Learner ensemble在数据分析中的应用，以及使用变量选择算法（如lasso）进行维度减少的性能。</li>
<li>methods: 本研究使用Super Learner ensemble和变量选择算法进行数据分析，并对不同的候选creening算法进行比较。</li>
<li>results: 研究发现，使用多种候选creening算法可以保证Super Learner的性能，类似于选择多种预测算法来保证Super Learner的性能。<details>
<summary>Abstract</summary>
Estimating a prediction function is a fundamental component of many data analyses. The Super Learner ensemble, a particular implementation of stacking, has desirable theoretical properties and has been used successfully in many applications. Dimension reduction can be accomplished by using variable screening algorithms, including the lasso, within the ensemble prior to fitting other prediction algorithms. However, the performance of a Super Learner using the lasso for dimension reduction has not been fully explored in cases where the lasso is known to perform poorly. We provide empirical results that suggest that a diverse set of candidate screening algorithms should be used to protect against poor performance of any one screen, similar to the guidance for choosing a library of prediction algorithms for the Super Learner.
</details>
<details>
<summary>摘要</summary>
估算预测函数是数据分析中的基本组成部分。跨学习ensemble（Super Learner）具有优秀的理论性质，在许多应用中得到了成功。使用变量选择算法，如lasso，来实现维度减少，可以在ensemble之前进行。然而，使用lasso进行维度减少的Super Learner表现没有得到完整的探索。我们提供了实证结果，表明应用多种候选屏选择算法，类似于选择预测算法库，以保证预测表现不受任何一个屏选的影响。
</details></li>
</ul>
<hr>
<h2 id="TS-Diffusion-Generating-Highly-Complex-Time-Series-with-Diffusion-Models"><a href="#TS-Diffusion-Generating-Highly-Complex-Time-Series-with-Diffusion-Models" class="headerlink" title="TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models"></a>TS-Diffusion: Generating Highly Complex Time Series with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03303">http://arxiv.org/abs/2311.03303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangming Li<br>for:这个论文的目的是处理具有 sampling irregularities、缺失和大的特征-时间维度的时间序列。methods:这个模型使用了点处理框架，包括一个神经网络 Ordinary Differential Equation（ODE）编码器、一个 diffusion 模型和另一个 ODE 编码器。results:这个模型在多个时间序列数据集上进行了广泛的实验，并达到了在传统和复杂时间序列上的出色表现，在比较之下显著超越了先前的基eline。<details>
<summary>Abstract</summary>
While current generative models have achieved promising performances in time-series synthesis, they either make strong assumptions on the data format (e.g., regularities) or rely on pre-processing approaches (e.g., interpolations) to simplify the raw data. In this work, we consider a class of time series with three common bad properties, including sampling irregularities, missingness, and large feature-temporal dimensions, and introduce a general model, TS-Diffusion, to process such complex time series. Our model consists of three parts under the framework of point process. The first part is an encoder of the neural ordinary differential equation (ODE) that converts time series into dense representations, with the jump technique to capture sampling irregularities and self-attention mechanism to handle missing values; The second component of TS-Diffusion is a diffusion model that learns from the representation of time series. These time-series representations can have a complex distribution because of their high dimensions; The third part is a decoder of another ODE that generates time series with irregularities and missing values given their representations. We have conducted extensive experiments on multiple time-series datasets, demonstrating that TS-Diffusion achieves excellent results on both conventional and complex time series and significantly outperforms previous baselines.
</details>
<details>
<summary>摘要</summary>
当前的生成模型已经取得了时间序列合成的可靠表现，但它们 Either 对数据格式做出了强大的假设（例如，常规）或者通过预处理方法（例如， interpolations）来简化原始数据。在这项工作中，我们考虑一类时间序列具有三种常见的坏属性，包括采样不均、缺失和大功能-时间维度，并引入一种通用模型，TS-Diffusion，来处理这些复杂的时间序列。我们的模型包括三部分，即encoder、diffusion模型和decoder。首先，TS-Diffusion的encoder部分使用神经ordinary differential equation（ODE）将时间序列转换为稠密表示，并使用跳技术 capture采样不均和自动注意机制处理缺失值。第二部分是一个学习从时间序列表示的diffusion模型，这些时间序列表示可能具有复杂的分布，因为它们的维度很高。最后，TS-Diffusion的decoder部分使用另一个ODE将时间序列生成器，带有采样不均和缺失值，给出其表示。我们在多个时间序列 datasets 进行了广泛的实验，并证明TS-Diffusion在传统和复杂时间序列上取得了优秀的结果，并在前一个基线模型 Significantly outperform。
</details></li>
</ul>
<hr>
<h2 id="Risk-of-Transfer-Learning-and-its-Applications-in-Finance"><a href="#Risk-of-Transfer-Learning-and-its-Applications-in-Finance" class="headerlink" title="Risk of Transfer Learning and its Applications in Finance"></a>Risk of Transfer Learning and its Applications in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03283">http://arxiv.org/abs/2311.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyang Cao, Haotian Gu, Xin Guo, Mathieu Rosenbaum</li>
<li>for: 这篇论文是为了提出一种新的转移风险概念，以评估转移学习的转移性。</li>
<li>methods: 本论文使用转移学习技术和转移风险概念来解决股票回报预测和资产 allocate问题。</li>
<li>results: 数据结果显示，转移风险与转移学习性能之间存在强相关关系，而转移风险可以提供一种 computationally efficient 的方法来选择合适的源任务。<details>
<summary>Abstract</summary>
Transfer learning is an emerging and popular paradigm for utilizing existing knowledge from previous learning tasks to improve the performance of new ones. In this paper, we propose a novel concept of transfer risk and and analyze its properties to evaluate transferability of transfer learning. We apply transfer learning techniques and this concept of transfer risk to stock return prediction and portfolio optimization problems. Numerical results demonstrate a strong correlation between transfer risk and overall transfer learning performance, where transfer risk provides a computationally efficient way to identify appropriate source tasks in transfer learning, including cross-continent, cross-sector, and cross-frequency transfer for portfolio optimization.
</details>
<details>
<summary>摘要</summary>
通过学习转移是一种迅速升起的并且受欢迎的方法，可以利用之前学习任务中的知识来提高新任务的性能。在这篇论文中，我们提出了一种新的转移风险概念，并分析其性质以评估转移学习的可行性。我们运用转移学习技术和这种转移风险概念来解决股票回报预测和股票组合优化问题。 numerically 的结果表明，转移风险和总转移学习性能之间存在强相关性，而转移风险提供了一种 computationally efficient 的方法来确定合适的源任务，包括跨洲、跨领域和跨频率的转移学习，以便进行股票组合优化。
</details></li>
</ul>
<hr>
<h2 id="Discretizing-Numerical-Attributes-An-Analysis-of-Human-Perceptions"><a href="#Discretizing-Numerical-Attributes-An-Analysis-of-Human-Perceptions" class="headerlink" title="Discretizing Numerical Attributes: An Analysis of Human Perceptions"></a>Discretizing Numerical Attributes: An Analysis of Human Perceptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03278">http://arxiv.org/abs/2311.03278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minakshi Kaushik, Rahul Sharma, Dirk Draheim</li>
<li>for: 本研究旨在提供一个标准方法 для数值属性分割。</li>
<li>methods: 本研究使用人类对数值属性分割的认知来提出两个指标，并与专家的看法进行比较。</li>
<li>results: 分析结果显示，68.7%的人类回答与我们提出的两个指标吻合较好。这表明我们的指标可能可以用于数值属性分割。<details>
<summary>Abstract</summary>
Machine learning (ML) has employed various discretization methods to partition numerical attributes into intervals. However, an effective discretization technique remains elusive in many ML applications, such as association rule mining. Moreover, the existing discretization techniques do not reflect best the impact of the independent numerical factor on the dependent numerical target factor. This research aims to establish a benchmark approach for numerical attribute partitioning. We conduct an extensive analysis of human perceptions of partitioning a numerical attribute and compare these perceptions with the results obtained from our two proposed measures. We also examine the perceptions of experts in data science, statistics, and engineering by employing numerical data visualization techniques. The analysis of collected responses reveals that $68.7\%$ of human responses approximately closely align with the values generated by our proposed measures. Based on these findings, our proposed measures may be used as one of the methods for discretizing the numerical attributes.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploiting-Latent-Attribute-Interaction-with-Transformer-on-Heterogeneous-Information-Networks"><a href="#Exploiting-Latent-Attribute-Interaction-with-Transformer-on-Heterogeneous-Information-Networks" class="headerlink" title="Exploiting Latent Attribute Interaction with Transformer on Heterogeneous Information Networks"></a>Exploiting Latent Attribute Interaction with Transformer on Heterogeneous Information Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03275">http://arxiv.org/abs/2311.03275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyuan Zhao, Qingqing Ge, Anfeng Cheng, Yiding Liu, Xiang Li, Shuaiqiang Wang</li>
<li>for: 这篇论文是为了提出一种新的多型图模型（MULAN），用于处理实际应用中存在多种不同特征的图像。</li>
<li>methods: 该模型包括两个主要组成部分：一个是对节点类型信息的考虑，另一个是用于捕捉各个节点特征之间的高阶交互信息的维度感知器。</li>
<li>results: 对六个多型图标准数据集进行了广泛的实验，结果显示MULAN在与其他当前状态的竞争对手比较之下表现出色，同时也证明了MULAN的效率。<details>
<summary>Abstract</summary>
Heterogeneous graph neural networks (HGNNs) have recently shown impressive capability in modeling heterogeneous graphs that are ubiquitous in real-world applications. Due to the diversity of attributes of nodes in different types, most existing models first align nodes by mapping them into the same low-dimensional space. However, in this way, they lose the type information of nodes. In addition, most of them only consider the interactions between nodes while neglecting the high-order information behind the latent interactions among different node features. To address these problems, in this paper, we propose a novel heterogeneous graph model MULAN, including two major components, i.e., a type-aware encoder and a dimension-aware encoder. Specifically, the type-aware encoder compensates for the loss of node type information and better leverages graph heterogeneity in learning node representations. Built upon transformer architecture, the dimension-aware encoder is capable of capturing the latent interactions among the diverse node features. With these components, the information of graph heterogeneity, node features and graph structure can be comprehensively encoded in node representations. We conduct extensive experiments on six heterogeneous benchmark datasets, which demonstrates the superiority of MULAN over other state-of-the-art competitors and also shows that MULAN is efficient.
</details>
<details>
<summary>摘要</summary>
“异构图 neural network (HGNN) 最近已经表现出模型异构图的出色能力，这些图在实际应用中非常普遍。由于节点属性的多样性，大多数现有模型都会将节点映射到同一低维度空间中，从而产生节点类型信息的丢失。此外，大多数模型只考虑节点之间的交互，而忽略节点特征之间的高阶信息。为了解决这些问题，本文提出了一种新的异构图模型名为 MULAN，包括两个主要组件：类型意识编码器和维度意识编码器。特别是，类型意识编码器可以补偿节点类型信息的丢失，更好地利用图中的异构性。基于 transformer 架构，维度意识编码器可以捕捉节点特征之间的隐藏交互。通过这两个组件，图中的异构性、节点特征和图结构可以完整地编码在节点表示中。我们在六个异构 benchmark 数据集进行了广泛的实验， demonstarted  MULAN 的超越性，以及其高效性。”
</details></li>
</ul>
<hr>
<h2 id="Parameter-Agnostic-Optimization-under-Relaxed-Smoothness"><a href="#Parameter-Agnostic-Optimization-under-Relaxed-Smoothness" class="headerlink" title="Parameter-Agnostic Optimization under Relaxed Smoothness"></a>Parameter-Agnostic Optimization under Relaxed Smoothness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03252">http://arxiv.org/abs/2311.03252</a></li>
<li>repo_url: None</li>
<li>paper_authors: Florian Hübler, Junchi Yang, Xiang Li, Niao He</li>
<li>for: 本研究旨在探讨训练机器学习模型时，调整参数的问题。</li>
<li>methods: 本研究使用Normalized Stochastic Gradient Descent with Momentum（NSGD-M）方法，并提出了一种新的框架来下降bounds。</li>
<li>results: 研究发现，NSGD-M方法可以在没有任何问题参数知识的情况下达到（近似）最优复杂性，但是这会带来一个具有$L_1$幂函数的扩展项。在探讨的设定下，这个扩展项可以被消除。此外，研究还发现，这种扩展项是不可避免的，因为任何parameter-agnostic算法都会面临这种问题。<details>
<summary>Abstract</summary>
Tuning hyperparameters, such as the stepsize, presents a major challenge of training machine learning models. To address this challenge, numerous adaptive optimization algorithms have been developed that achieve near-optimal complexities, even when stepsizes are independent of problem-specific parameters, provided that the loss function is $L$-smooth. However, as the assumption is relaxed to the more realistic $(L_0, L_1)$-smoothness, all existing convergence results still necessitate tuning of the stepsize. In this study, we demonstrate that Normalized Stochastic Gradient Descent with Momentum (NSGD-M) can achieve a (nearly) rate-optimal complexity without prior knowledge of any problem parameter, though this comes at the cost of introducing an exponential term dependent on $L_1$ in the complexity. We further establish that this exponential term is inevitable to such schemes by introducing a theoretical framework of lower bounds tailored explicitly for parameter-agnostic algorithms. Interestingly, in deterministic settings, the exponential factor can be neutralized by employing Gradient Descent with a Backtracking Line Search. To the best of our knowledge, these findings represent the first parameter-agnostic convergence results under the generalized smoothness condition. Our empirical experiments further confirm our theoretical insights.
</details>
<details>
<summary>摘要</summary>
调整 гиперпараметров，如步长，对机器学习模型的训练呈poses major challenge。为 Addressing this challenge, numerous adaptive optimization algorithms have been developed that achieve near-optimal complexities, even when the stepsizes are independent of problem-specific parameters, provided that the loss function is $L$-smooth。然而，如果放弃这个假设，所有现有的收敛结果都仍然需要调整步长。在这种研究中，我们示出了Normalized Stochastic Gradient Descent with Momentum（NSGD-M）可以实现一个（近似）率optimal complexity without prior knowledge of any problem parameter，但是这会导致在 $L_1$ 上增加一个对数函数。我们还证明这个对数函数是不可避免的，通过引入特定于无参数算法的理论框架的下界。在束定性 Settings，这个对数因子可以被中和 employing Gradient Descent with a Backtracking Line Search。我们认为这些发现是parameter-agnostic convergence results under the generalized smoothness condition的首次。我们的实验也证明了我们的理论发现。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Langevin-Monte-Carlo-with-ResNet-like-Neural-Network-architectures"><a href="#Approximating-Langevin-Monte-Carlo-with-ResNet-like-Neural-Network-architectures" class="headerlink" title="Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures"></a>Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03242">http://arxiv.org/abs/2311.03242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Eigel, Charles Miranda, Janina Schütte, David Sommer</li>
<li>for: 这个论文的目的是构建一种用于采样目标分布的神经网络，以便从简单的参照分布（例如标准正态分布）采样目标分布。</li>
<li>methods: 该论文提出一种基于Langevin Monte Carlo（LMC）算法的神经网络建模方法，并通过LMC干扰结果的评估来评估该方法的收敛率。</li>
<li>results: 该论文的分析表明，在不同的干扰假设下，使用该方法可以在 Wasserstein-$2$ 距离下收敛到目标分布。此外，文章还提出了一种类似于深度差分神经网络的建模方法，并 derivated expressivity 结果表明该方法可以准确地表示采样到目标分布的映射。<details>
<summary>Abstract</summary>
We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
</details>
<details>
<summary>摘要</summary>
我们从一个目标分布中抽取样本，通过建立一个对应于标准正常分布的神经网络，将标准正常分布中的样本转换为目标分布中的样本。我们提议使用对应于兰杰维尔 Monte Carlo（LMC）算法的神经网络架构。基于LMC扰动结果，我们显示了对预设的均匀、对数凹陷分布的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的扰动范围内的��
</details></li>
</ul>
<hr>
<h2 id="Out-of-distribution-Detection-Learning-with-Unreliable-Out-of-distribution-Sources"><a href="#Out-of-distribution-Detection-Learning-with-Unreliable-Out-of-distribution-Sources" class="headerlink" title="Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources"></a>Out-of-distribution Detection Learning with Unreliable Out-of-distribution Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03236">http://arxiv.org/abs/2311.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haotian Zheng, Qizhou Wang, Zhen Fang, Xiaobo Xia, Feng Liu, Tongliang Liu, Bo Han</li>
<li>for: 这个研究的目的是提高开放世界分类中预测器的可靠性，通过实现极少量的真实外部数据训练。</li>
<li>methods: 这个研究使用的方法是基于数据生成器，通过将ID数据生成为外部数据，以提高预测器对外部数据的预测能力。</li>
<li>results: 研究发现，使用auxiliary task可以帮助预测器更好地识别外部数据，并且与先前的方法相比，这个方法可以更好地避免伪阳性识别。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) detection discerns OOD data where the predictor cannot make valid predictions as in-distribution (ID) data, thereby increasing the reliability of open-world classification. However, it is typically hard to collect real out-of-distribution (OOD) data for training a predictor capable of discerning ID and OOD patterns. This obstacle gives rise to data generation-based learning methods, synthesizing OOD data via data generators for predictor training without requiring any real OOD data. Related methods typically pre-train a generator on ID data and adopt various selection procedures to find those data likely to be the OOD cases. However, generated data may still coincide with ID semantics, i.e., mistaken OOD generation remains, confusing the predictor between ID and OOD data. To this end, we suggest that generated data (with mistaken OOD generation) can be used to devise an auxiliary OOD detection task to facilitate real OOD detection. Specifically, we can ensure that learning from such an auxiliary task is beneficial if the ID and the OOD parts have disjoint supports, with the help of a well-designed training procedure for the predictor. Accordingly, we propose a powerful data generation-based learning method named Auxiliary Task-based OOD Learning (ATOL) that can relieve the mistaken OOD generation. We conduct extensive experiments under various OOD detection setups, demonstrating the effectiveness of our method against its advanced counterparts.
</details>
<details>
<summary>摘要</summary>
外部数据（OOD）检测可以将OOD数据与内部数据（ID）进行分类，从而提高开放世界分类的可靠性。然而，收集真实的OOD数据用于训练预测器是困难的。这种困难引起了数据生成基于学习方法，通过数据生成器来训练预测器，无需任何真实的OOD数据。这些方法通常是在ID数据上预训练数据生成器，并采用不同的选择过程来找到可能是OOD的数据。然而，生成的数据可能仍然与ID semantics相同，即生成的OOD数据仍然与ID数据匹配。为了解决这个问题，我们建议使用生成的数据（包括 mistaken OOD generation）来设计辅助OOD检测任务，以便在真正的OOD检测中帮助预测器分辨ID和OOD数据。具体来说，我们可以确保学习这种辅助任务是有益的，只要ID和OOD部分具有不同的支持，并且通过适当的预测器训练程序来保证这一点。因此，我们提出了一种强大的数据生成基于学习方法，名为辅助任务基于OOD学习（ATOL），可以减少 mistaken OOD generation。我们在不同的OOD检测设置下进行了广泛的实验，并证明了我们的方法在相比先进的方法之上具有更高的效果。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Process-Approximations-Assessing-Their-Necessity"><a href="#Spatial-Process-Approximations-Assessing-Their-Necessity" class="headerlink" title="Spatial Process Approximations: Assessing Their Necessity"></a>Spatial Process Approximations: Assessing Their Necessity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03201">http://arxiv.org/abs/2311.03201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Zhang</li>
<li>for: 这篇论文主要是为了解决大样本大Matrix的不稳定性问题。</li>
<li>methods: 论文使用了多种优化算法来解决这种不稳定性问题，包括低级分解、卷积分解等。</li>
<li>results: 论文通过对各种优化算法进行比较，发现其中的一些方法可以有效地解决大样本大Matrix的不稳定性问题，但是还有一些方法无法减轻这种不稳定性。<details>
<summary>Abstract</summary>
In spatial statistics and machine learning, the kernel matrix plays a pivotal role in prediction, classification, and maximum likelihood estimation. A thorough examination reveals that for large sample sizes, the kernel matrix becomes ill-conditioned, provided the sampling locations are fairly evenly distributed. This condition poses significant challenges to numerical algorithms used in prediction and estimation computations and necessitates an approximation to prediction and the Gaussian likelihood. A review of current methodologies for managing large spatial data indicates that some fail to address this ill-conditioning problem. Such ill-conditioning often results in low-rank approximations of the stochastic processes. This paper introduces various optimality criteria and provides solutions for each.
</details>
<details>
<summary>摘要</summary>
在空间统计学和机器学习中，kernel矩阵在预测、分类和最大可能性估计中扮演着重要的角色。经过全面的检查，发现在大样本大小下，如果抽样点 distribution 相对均匀，那么kernel矩阵就会变得不整合，这会对数学计算中用到的数值算法提出 significan challenges。这种不整合情况通常会导致低级别的随机过程的预测和高斯可能性函数的 Approximation。本文评估了当前的大 spatial data 管理方法，发现一些方法并不能解决这个不整合问题。这种不整合情况通常会导致低级别的随机过程的预测和高斯可能性函数的 Approximation。本文介绍了多种优化性riteria和解决方案。
</details></li>
</ul>
<hr>
<h2 id="Stable-Linear-Subspace-Identification-A-Machine-Learning-Approach"><a href="#Stable-Linear-Subspace-Identification-A-Machine-Learning-Approach" class="headerlink" title="Stable Linear Subspace Identification: A Machine Learning Approach"></a>Stable Linear Subspace Identification: A Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03197">http://arxiv.org/abs/2311.03197</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cemempamoi/simba">https://github.com/cemempamoi/simba</a></li>
<li>paper_authors: Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari Trecate, Colin N. Jones</li>
<li>for: 这篇论文主要是为了提出一种基于机器学习（ML）和线性系统适应（SI）的新的状态空间Identification方法（SIMBA），以提高现有方法的稳定性和适应性。</li>
<li>methods: 该方法使用了自动微分框架，并使用了一种新的线性矩阵不等式（LMI）来保证模型的稳定性。</li>
<li>results: 对比 traditional的线性状态空间SI方法，SIMBA通常有更高的性能，尤其是在保证模型稳定性的情况下。在许多输入输出系统和真实数据上，SIMBA显示出了良好的抗预测性和灵活性。<details>
<summary>Abstract</summary>
Machine Learning (ML) and linear System Identification (SI) have been historically developed independently. In this paper, we leverage well-established ML tools - especially the automatic differentiation framework - to introduce SIMBa, a family of discrete linear multi-step-ahead state-space SI methods using backpropagation. SIMBa relies on a novel Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure the stability of the identified model.   We show how SIMBa generally outperforms traditional linear state-space SI methods, and sometimes significantly, although at the price of a higher computational burden. This performance gap is particularly remarkable compared to other SI methods with stability guarantees, where the gain is frequently above 25% in our investigations, hinting at SIMBa's ability to simultaneously achieve state-of-the-art fitting performance and enforce stability. Interestingly, these observations hold for a wide variety of input-output systems and on both simulated and real-world data, showcasing the flexibility of the proposed approach. We postulate that this new SI paradigm presents a great extension potential to identify structured nonlinear models from data, and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DeepInception-Hypnotize-Large-Language-Model-to-Be-Jailbreaker"><a href="#DeepInception-Hypnotize-Large-Language-Model-to-Be-Jailbreaker" class="headerlink" title="DeepInception: Hypnotize Large Language Model to Be Jailbreaker"></a>DeepInception: Hypnotize Large Language Model to Be Jailbreaker</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03191">http://arxiv.org/abs/2311.03191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han<br>for: 这篇论文旨在揭示大语言模型（LLM）在防御性监禁方面的漏洞，并提出一种轻量级的方法来利用LLM的人格化能力实现攻击。methods: 该方法基于Milgram实验，利用LLM的人格化能力构建一个嵌入式场景，使LLM在正常情况下逃脱使用控制，并提供了进一步的直接监禁机会。results: 实验结果显示，DeepInception可以与之前的对手竞争，并在后续互动中实现连续监禁。研究发现，自失的问题存在于多个开源&#x2F;关闭源LLM上，如Falcon、Vicuna、Llama-2和GPT-3.5&#x2F;4&#x2F;4V。<details>
<summary>Abstract</summary>
Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment that individuals can harm another person if they are told to do so by an authoritative figure, we disclose a lightweight method, termed as DeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock its misusing risks. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario and provides the possibility for further direct jailbreaks. Empirically, we conduct comprehensive experiments to show its efficacy. Our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna, Llama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay more attention to the safety aspects of LLMs and a stronger defense against their misuse risks. The code is publicly available at: https://github.com/tmlr-group/DeepInception.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）在不同应用中具有惊人的成功，但它们却容易受到恶意破坏的威胁。然而，前一代的研究通常采用粗糙优化或高计算成本的推理，这可能不是实用或有效的。在这篇论文中，我们受到米凯尔实验的启发，该实验表明了人们可以通过权威人士的指令而使别人产生危害。因此，我们提出了一种轻量级的方法，称为深度引入，可以轻松地使LLM变成破坏者，并暴露其不当使用的风险。具体来说，深度引入利用LLM的人格化能力构建了一个新的嵌入式场景，实现了在常规情况下适应的方式，并提供了进一步的直接破坏的可能性。我们进行了广泛的实验，证明了深度引入的效果。我们的深度引入可以与前一代的对手相比，并在后续交互中实现连续破坏，揭示了开源/关闭源LLM like Falcon、Vicuna、Llama-2和GPT-3.5/4/4V的潜在漏洞。我们的调查表明，人们应该更加关注LLM的安全问题，并采取更加有力的防御措施。代码可以在 GitHub 上找到：https://github.com/tmlr-group/DeepInception。
</details></li>
</ul>
<hr>
<h2 id="Hopfield-Enhanced-Deep-Neural-Networks-for-Artifact-Resilient-Brain-State-Decoding"><a href="#Hopfield-Enhanced-Deep-Neural-Networks-for-Artifact-Resilient-Brain-State-Decoding" class="headerlink" title="Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding"></a>Hopfield-Enhanced Deep Neural Networks for Artifact-Resilient Brain State Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03421">http://arxiv.org/abs/2311.03421</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arnaumarin/hdnn-artifactbrainstate">https://github.com/arnaumarin/hdnn-artifactbrainstate</a></li>
<li>paper_authors: Arnau Marin-Llobet, Arnau Manasanch, Maria V. Sanchez-Vives</li>
<li>for: 这项研究的目的是提高脑动态图像的识别精度，并探索脑动态图像与行为之间的关系。</li>
<li>methods: 该研究使用了两个阶段的计算机框架，首先使用了抽象网络对数据进行噪声除掉，然后使用了卷积神经网络进行分类。</li>
<li>results: 研究发现，这种混合的方法可以有效地减少噪声的影响，使模型在噪声水平较低时达到与清洁数据 CNN 的性能。<details>
<summary>Abstract</summary>
The study of brain states, ranging from highly synchronous to asynchronous neuronal patterns like the sleep-wake cycle, is fundamental for assessing the brain's spatiotemporal dynamics and their close connection to behavior. However, the development of new techniques to accurately identify them still remains a challenge, as these are often compromised by the presence of noise, artifacts, and suboptimal recording quality. In this study, we propose a two-stage computational framework combining Hopfield Networks for artifact data preprocessing with Convolutional Neural Networks (CNNs) for classification of brain states in rat neural recordings under different levels of anesthesia. To evaluate the robustness of our framework, we deliberately introduced noise artifacts into the neural recordings. We evaluated our hybrid Hopfield-CNN pipeline by benchmarking it against two comparative models: a standalone CNN handling the same noisy inputs, and another CNN trained and tested on artifact-free data. Performance across various levels of data compression and noise intensities showed that our framework can effectively mitigate artifacts, allowing the model to reach parity with the clean-data CNN at lower noise levels. Although this study mainly benefits small-scale experiments, the findings highlight the necessity for advanced deep learning and Hopfield Network models to improve scalability and robustness in diverse real-world settings.
</details>
<details>
<summary>摘要</summary>
研究大脑状态的研究，从高度同步到不同步的神经元模式，如睡卫休眠-醒目的周期，是评估大脑的空间时间动态和其与行为的紧密关系的基础。然而，开发新的技术来准确识别这些状态仍然是一个挑战，因为这些经常受到噪音、artefacts和低质量记录的干扰。在这种研究中，我们提出了一个两个阶段的计算框架，其中首先使用束缚网络来处理噪音数据，然后使用卷积神经网络（CNN）来分类大脑状态。为评估我们的框架的可靠性，我们故意将噪音 artifacts添加到神经记录中。我们对我们的混合束缚-CNN pipeline进行了比较，并与两个参照模型进行比较：一个只处理同样噪音输入的单独CNN，另一个在噪音自由数据上训练和测试CNN。我们在不同的数据压缩和噪音强度下测试了我们的框架，结果表明，我们的框架可以有效地 mitigate artifacts，使模型在噪音水平下达到与干净数据CNN的性能。尽管这种研究主要是为小规模实验而设计的，但发现的结果 highlights the necessity for advanced deep learning and束缚网络模型，以提高可扩展性和可靠性在多样化的实际场景中。**Simplified Chinese Translation:**研究大脑状态的研究，从高度同步到不同步的神经元模式，如睡卫休眠-醒目的周期，是评估大脑的空间时间动态和其与行为的紧密关系的基础。然而，开发新的技术来准确识别这些状态仍然是一个挑战，因为这些经常受到噪音、artefacts和低质量记录的干扰。在这种研究中，我们提出了一个两个阶段的计算框架，其中首先使用束缚网络来处理噪音数据，然后使用卷积神经网络（CNN）来分类大脑状态。为评估我们的框架的可靠性，我们故意将噪音 artifacts添加到神经记录中。我们对我们的混合束缚-CNN pipeline进行了比较，并与两个参照模型进行比较：一个只处理同样噪音输入的单独CNN，另一个在噪音自由数据上训练和测试CNN。我们在不同的数据压缩和噪音强度下测试了我们的框架，结果表明，我们的框架可以有效地 mitigate artifacts，使模型在噪音水平下达到与干净数据CNN的性能。尽管这种研究主要是为小规模实验而设计的，但发现的结果 highlights the necessity for advanced deep learning and束缚网络模型，以提高可扩展性和可靠性在多样化的实际场景中。
</details></li>
</ul>
<hr>
<h2 id="Preserving-Privacy-in-GANs-Against-Membership-Inference-Attack"><a href="#Preserving-Privacy-in-GANs-Against-Membership-Inference-Attack" class="headerlink" title="Preserving Privacy in GANs Against Membership Inference Attack"></a>Preserving Privacy in GANs Against Membership Inference Attack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03172">http://arxiv.org/abs/2311.03172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadhadi Shateri, Francisco Messina, Fabrice Labeau, Pablo Piantanida</li>
<li>for: 本研究旨在提高生成 adversarial Networks (GANs) 对于 Membership Inference Attacks (MIAs) 的Robustness，并且提出了两种防御策略。</li>
<li>methods: 本研究使用了 GANs 生成 Synthetic Data，并且定义了一种基于 Bhattacharyya 度的泛化过度溯源检测方法，以及一种基于 Fano 不等式的最大 entropy GAN (MEGAN) 防御策略。</li>
<li>results: 对于一些常用的数据集，应用了提出的防御策略后，可以将 adversaries 的准确率降低到随机猜测率水平，而且减少了生成样本中关于训练数据点的信息泄露。<details>
<summary>Abstract</summary>
Generative Adversarial Networks (GANs) have been widely used for generating synthetic data for cases where there is a limited size real-world dataset or when data holders are unwilling to share their data samples. Recent works showed that GANs, due to overfitting and memorization, might leak information regarding their training data samples. This makes GANs vulnerable to Membership Inference Attacks (MIAs). Several defense strategies have been proposed in the literature to mitigate this privacy issue. Unfortunately, defense strategies based on differential privacy are proven to reduce extensively the quality of the synthetic data points. On the other hand, more recent frameworks such as PrivGAN and PAR-GAN are not suitable for small-size training datasets. In the present work, the overfitting in GANs is studied in terms of the discriminator, and a more general measure of overfitting based on the Bhattacharyya coefficient is defined. Then, inspired by Fano's inequality, our first defense mechanism against MIAs is proposed. This framework, which requires only a simple modification in the loss function of GANs, is referred to as the maximum entropy GAN or MEGAN and significantly improves the robustness of GANs to MIAs. As a second defense strategy, a more heuristic model based on minimizing the information leaked from generated samples about the training data points is presented. This approach is referred to as mutual information minimization GAN (MIMGAN) and uses a variational representation of the mutual information to minimize the information that a synthetic sample might leak about the whole training data set. Applying the proposed frameworks to some commonly used data sets against state-of-the-art MIAs reveals that the proposed methods can reduce the accuracy of the adversaries to the level of random guessing accuracy with a small reduction in the quality of the synthetic data samples.
</details>
<details>
<summary>摘要</summary>
生成对抗网络（GANs）广泛应用于生成synthetic数据，特别是当实际数据集规模受限或数据持有者不愿意分享数据样本时。然而，GANs可能因过拟合和记忆而泄露training数据样本信息，从而使GANs面临会员推断攻击（MIAs）的隐私问题。文献中已经提出了一些防御策略，但是这些策略基于差分隐私会导致synthetic数据点质量降低 extensively。相反，更新的框架如PrivGAN和PAR-GAN在小型训练集上不适用。在 presente 的研究中，我们研究了GANs中的过拟合，并定义了基于 Bhattacharyya 系数的更一般的过拟合度量。然后，以Fano的不等式为 inspiration，我们提出了一种防御机制，称为最大 entropy GAN（MEGAN）。这种方法需要在GANs的损失函数中进行简单的修改，可以减轻GANs对MIAs的抗性。此外，我们还提出了一种更具体的防御策略，即使generated samples about the training data points中的信息泄露的最小化。这种方法称为mutual information minimization GAN（MIMGAN），使用了变量表示的mutual information来最小化generated samples中对全部训练数据集的信息泄露。通过应用我们提出的方法到一些常用的数据集上，发现可以将敌对者的准确率降低到随机猜测率水平，同时减少generated samples的质量下降。
</details></li>
</ul>
<hr>
<h2 id="An-Examination-of-the-Alleged-Privacy-Threats-of-Confidence-Ranked-Reconstruction-of-Census-Microdata"><a href="#An-Examination-of-the-Alleged-Privacy-Threats-of-Confidence-Ranked-Reconstruction-of-Census-Microdata" class="headerlink" title="An Examination of the Alleged Privacy Threats of Confidence-Ranked Reconstruction of Census Microdata"></a>An Examination of the Alleged Privacy Threats of Confidence-Ranked Reconstruction of Census Microdata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03171">http://arxiv.org/abs/2311.03171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NajeebJebreel/CRR-analysis">https://github.com/NajeebJebreel/CRR-analysis</a></li>
<li>paper_authors: David Sánchez, Najeeb Jebreel, Josep Domingo-Ferrer, Krishnamurty Muralidhar, Alberto Blanco-Justicia</li>
<li>for: 本研究旨在探讨美国人口普查局（USCB）在2020年人口普查中使用 differential privacy（DP） instead of traditional statistical disclosure limitation based on rank swapping，并对这种更改的影响。</li>
<li>methods: 本研究使用了一种新的重建攻击，即 confidence-ranked reconstruction，以评估 USCB 是否应该使用 DP-based solutions。</li>
<li>results: 研究结果表明， confidence-ranked reconstruction 无法准确地重建 Census 记录，并且无法帮助攻击者进行身份透露或属性透露攻击。此外，由于人口普查数据的编译、处理和发布方式，无法通过任何方法重建原始和完整的记录。<details>
<summary>Abstract</summary>
The alleged threat of reconstruction attacks has led the U.S. Census Bureau (USCB) to replace in the Decennial Census 2020 the traditional statistical disclosure limitation based on rank swapping with one based on differential privacy (DP). This has resulted in substantial accuracy loss of the released statistics. Worse yet, it has been shown that the reconstruction attacks used as an argument to move to DP are very far from allowing unequivocal reidentification of the respondents, because in general there are a lot of reconstructions compatible with the released statistics. In a very recent paper, a new reconstruction attack has been proposed, whose goal is to indicate the confidence that a reconstructed record was in the original respondent data. The alleged risk of serious disclosure entailed by such confidence-ranked reconstruction has renewed the interest of the USCB to use DP-based solutions. To forestall the potential accuracy loss in future data releases resulting from adoption of these solutions, we show in this paper that the proposed confidence-ranked reconstruction does not threaten privacy. Specifically, we report empirical results showing that the proposed ranking cannot guide reidentification or attribute disclosure attacks, and hence it fails to warrant the USCB's move towards DP. Further, we also demonstrate that, due to the way the Census data are compiled, processed and released, it is not possible to reconstruct original and complete records through any methodology, and the confidence-ranked reconstruction not only is completely ineffective at accurately reconstructing Census records but is trivially outperformed by an adequate interpretation of the released aggregate statistics.
</details>
<details>
<summary>摘要</summary>
美国人口普查局（USCB）在2020年人口普查中取代了传统的统计隐私技术，改用Diffusion Privacy（DP）。这导致了发布统计数据的准确性下降。尽管如此，有人提出了重建攻击，即使用DP来保护个人隐私。然而，这些重建攻击并不能 garantuee  unequivocal 的重建记录，因为通常有多个重建方案与发布统计数据兼容。在最近的论文中，一种新的重建攻击方法被提出，其目标是指出重建记录中是否包含原始回答数据。美国人口普查局对这种重建攻击表示兴趣，以免Future数据发布中可能出现的准确性下降。我们在这篇论文中展示，提posed confidence-ranked reconstruction不会威胁隐私。我们的实验结果表明，这种排名不能导guide 重建或 attribute 透露攻击，因此无法准确重建人口普查记录。此外，我们还证明，由于人口普查数据的编译、处理和发布方式，无法通过任何方法重建原始和完整的记录。 confidence-ranked reconstruction不仅完全无法准确重建人口普查记录，而且也远远下降于对发布统计数据进行合理解读的能力。
</details></li>
</ul>
<hr>
<h2 id="Convergence-Analysis-of-Sequential-Federated-Learning-on-Heterogeneous-Data"><a href="#Convergence-Analysis-of-Sequential-Federated-Learning-on-Heterogeneous-Data" class="headerlink" title="Convergence Analysis of Sequential Federated Learning on Heterogeneous Data"></a>Convergence Analysis of Sequential Federated Learning on Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03154">http://arxiv.org/abs/2311.03154</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyipeng00/convergence">https://github.com/liyipeng00/convergence</a></li>
<li>paper_authors: Yipeng Li, Xinchen Lyu</li>
<li>for: 本文为了研究 Federated Learning (FL) 中多客户端的并发训练，以及对不同数据的敏感性。</li>
<li>methods: 本文使用了两种类型的方法：一是平行 FL (PFL)， где客户端在平行方式进行模型训练；另一种是顺序 FL (SFL)， donde客户端在顺序方式进行模型训练。 在对异构数据进行训练时，SFL的整合理论仍然缺乏。本文为SFL在异构数据上的强健&#x2F;通用&#x2F;非对称目标下的整合 garantías。</li>
<li>results: 实验结果表明，在异构数据上，SFL比PFL在跨设备情况下表现更好，并且SFL在完全和偏参与客户端情况下的整合 garantías比PFL更好。<details>
<summary>Abstract</summary>
There are two categories of methods in Federated Learning (FL) for joint training across multiple clients: i) parallel FL (PFL), where clients train models in a parallel manner; and ii) sequential FL (SFL), where clients train models in a sequential manner. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. In this paper, we establish the convergence guarantees of SFL for strongly/general/non-convex objectives on heterogeneous data. The convergence guarantees of SFL are better than that of PFL on heterogeneous data with both full and partial client participation. Experimental results validate the counterintuitive analysis result that SFL outperforms PFL on extremely heterogeneous data in cross-device settings.
</details>
<details>
<summary>摘要</summary>
在聚合学习（Federated Learning，FL）中，有两类方法 для共同训练多个客户端：一是平行聚合学习（Parallel Federated Learning，PFL），其中客户端在平行的方式进行模型训练；另一是顺序聚合学习（Sequential Federated Learning，SFL），其中客户端在顺序的方式进行模型训练。与PFL相比，SFL在不同数据上的整合理论仍然缺失。在这篇论文中，我们建立了SFL在强不同数据上的整合保证，并且比PFL在不同数据上的整合保证更好。实验结果证明了对于非常不同的数据，SFL在跨设备的场景下超越PFL。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Material-Thermal-Conductivity-Prediction-through-Machine-Learning"><a href="#End-to-end-Material-Thermal-Conductivity-Prediction-through-Machine-Learning" class="headerlink" title="End-to-end Material Thermal Conductivity Prediction through Machine Learning"></a>End-to-end Material Thermal Conductivity Prediction through Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03139">http://arxiv.org/abs/2311.03139</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yagyank Srivastava, Ankit Jain</li>
<li>for: 这个论文是为了提高材料热导率预测的速度而写的。</li>
<li>methods: 这个论文使用了结构基于的机器学习方法来预测材料热导率，包括使用基本原理和博尔tz滤波方程进行高通量计算，并评估了现有的状态艺术模型。</li>
<li>results: 研究发现，由于数据质量的问题，所有使用的机器学习模型都会过拟合。为解决这个问题，这篇论文提出了一种图граaph neural network模型，该模型在所有评估 datasets 上展现了更加一致和规范的性能。然而，在测试 datasets 上，最佳的相对百分比误差仍然在50-60%的范围内。这表明，虽然这些模型可以帮助加速材料屏选，但其当前准确性仍有限。<details>
<summary>Abstract</summary>
We investigated the accelerated prediction of the thermal conductivity of materials through end- to-end structure-based approaches employing machine learning methods. Due to the non-availability of high-quality thermal conductivity data, we first performed high-throughput calculations based on first principles and the Boltzmann transport equation for 225 materials, effectively more than doubling the size of the existing dataset. We assessed the performance of state-of-the-art machine learning models for thermal conductivity prediction on this expanded dataset and observed that all these models suffered from overfitting. To address this issue, we introduced a novel graph-based neural network model, which demonstrated more consistent and regularized performance across all evaluated datasets. Nevertheless, the best mean absolute percentage error achieved on the test dataset remained in the range of 50-60%. This suggests that while these models are valuable for expediting material screening, their current accuracy is still limited.
</details>
<details>
<summary>摘要</summary>
我们研究了通过终端结构基于方法加速预测材料的热导率。由于热导率数据的不可得性，我们首先通过基本原理和博尔ツ曼传输方程进行了225种材料的高通过率计算，实际上更 чем doubling了现有数据集的大小。我们评估了现有状态的机器学习模型在这个扩展数据集上的表现，并发现所有这些模型都受到了过拟合。为解决这个问题，我们提出了一种新的图表基于神经网络模型，该模型在所有评估数据集上显示了更一致和规范的表现。然而，在测试数据集上最佳的 mean absolute percentage error 仍然在50-60%的范围内，这表明这些模型可以快速屏选材料，但其当前精度仍然有限。
</details></li>
</ul>
<hr>
<h2 id="Reservoir-Computing-Model-for-Mapping-and-Forecasting-Neuronal-Interactions-from-Electrophysiological-Data"><a href="#Reservoir-Computing-Model-for-Mapping-and-Forecasting-Neuronal-Interactions-from-Electrophysiological-Data" class="headerlink" title="Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data"></a>Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03131">http://arxiv.org/abs/2311.03131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilya Auslender, Giorgio Letti, Yasaman Heydari, Lorenzo Pavesi</li>
<li>for: 研究 neuronal network 的电生物学性质，以揭示不同单元之间的互动。</li>
<li>methods: 使用 Reservoir Computing Network 建立计算模型，从电生物学测量数据中提取网络结构。</li>
<li>results: 模型可以准确预测网络结构图和响应特定输入。<details>
<summary>Abstract</summary>
Electrophysiological nature of neuronal networks allows to reveal various interactions between different cell units at a very short time-scales. One of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. In this work we developed a computational model, based on Reservoir Computing Network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. We demonstrate that the model can predict the connectivity map of the network with higher accuracy than the common methods such as Cross-Correlation and Transfer-Entropy. In addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.
</details>
<details>
<summary>摘要</summary>
electrophysiological nature of neuronal networks allows for revealing various interactions between different cell units at very short time-scales. one of the many challenges in analyzing these signals is to retrieve the morphology and functionality of a given network. in this work, we developed a computational model based on reservoir computing network (RCN) architecture, which decodes the spatio-temporal data from electro-physiological measurements of neuronal cultures and reconstructs the network structure on a macroscopic domain, representing the connectivity between neuronal units. we demonstrate that the model can predict the connectivity map of the network with higher accuracy than common methods such as cross-correlation and transfer-entropy. in addition, we experimentally demonstrate the ability of the model to predict a network response to a specific input, such as localized stimulus.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Nonparametric-modeling-of-the-composite-effect-of-multiple-nutrients-on-blood-glucose-dynamics"><a href="#Nonparametric-modeling-of-the-composite-effect-of-multiple-nutrients-on-blood-glucose-dynamics" class="headerlink" title="Nonparametric modeling of the composite effect of multiple nutrients on blood glucose dynamics"></a>Nonparametric modeling of the composite effect of multiple nutrients on blood glucose dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03129">http://arxiv.org/abs/2311.03129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jularina/trcmed-kit">https://github.com/jularina/trcmed-kit</a></li>
<li>paper_authors: Arina Odnoblyudova, Çağlar Hizli, ST John, Andrea Cognolato, Anne Juuti, Simo Särkkä, Kirsi Pietiläinen, Pekka Marttinen</li>
<li>for: 估计多 ком成分治疗的生物physiological response, 并分离各组件的影响。</li>
<li>methods: 扩展现有的 probabilistic nonparametric方法, 使其能够直接面对这个问题。 开发了一种基于卷积的composite treatment-response曲线模型，更易于生物预测。</li>
<li>results: 通过对卡路里和脂肪在餐食中的影响来预测血糖响应，并且通过分解治疗组件、 incorporating dosages, 和在患者之间共享统计信息来提高预测精度。<details>
<summary>Abstract</summary>
In biomedical applications it is often necessary to estimate a physiological response to a treatment consisting of multiple components, and learn the separate effects of the components in addition to the joint effect. Here, we extend existing probabilistic nonparametric approaches to explicitly address this problem. We also develop a new convolution-based model for composite treatment-response curves that is more biologically interpretable. We validate our models by estimating the impact of carbohydrate and fat in meals on blood glucose. By differentiating treatment components, incorporating their dosages, and sharing statistical information across patients via a hierarchical multi-output Gaussian process, our method improves prediction accuracy over existing approaches, and allows us to interpret the different effects of carbohydrates and fat on the overall glucose response.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在生物医学应用中，经常需要估算具有多个组成部分的治疗的生物学响应，并了解每个组成部分的独立效应以及它们之间的共同效应。在这里，我们扩展了现有的概率非 Parametric方法，以便直接解决这个问题。我们还开发了一种基于卷积的治疗响应曲线模型，这种模型更加容易理解生物学意义。我们验证了我们的模型，通过估算吃到抗酵素和脂肪在饭物中的血糖响应。通过分解治疗组成部分，包括它们的剂量，并在患者之间共享统计信息via层次多输出 Gaussian process，我们的方法可以提高预测精度，并允许我们解释抗酵素和脂肪对总血糖响应的不同效应。
</details></li>
</ul>
<hr>
<h2 id="Algebraic-Dynamical-Systems-in-Machine-Learning"><a href="#Algebraic-Dynamical-Systems-in-Machine-Learning" class="headerlink" title="Algebraic Dynamical Systems in Machine Learning"></a>Algebraic Dynamical Systems in Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03118">http://arxiv.org/abs/2311.03118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iolo Jones, Jerry Swan, Jeffrey Giansiracusa</li>
<li>for: 本研究旨在开发一种基于符号 rewrite 的动态系统分析模型，用于描述动态机器学习模型的组合性和扩展性。</li>
<li>methods: 本研究使用了一种 recursive function 来定义动态模型的Output，并通过Category theory 的框架来描述这些模型的结构和特性。</li>
<li>results: 本研究显示了一种将 recurrent neural networks, graph neural networks, 和 diffusion models 等动态模型嵌入到一个抽象的 formally defined 模型中的方法，并提出了一种将这些模型扩展到适用于结构化或非数字数据的 ‘hybrid symbolic-numeric’ 模型的方法。<details>
<summary>Abstract</summary>
We introduce an algebraic analogue of dynamical systems, based on term rewriting. We show that a recursive function applied to the output of an iterated rewriting system defines a formal class of models into which all the main architectures for dynamic machine learning models (including recurrent neural networks, graph neural networks, and diffusion models) can be embedded. Considered in category theory, we also show that these algebraic models are a natural language for describing the compositionality of dynamic models. Furthermore, we propose that these models provide a template for the generalisation of the above dynamic models to learning problems on structured or non-numerical data, including 'hybrid symbolic-numeric' models.
</details>
<details>
<summary>摘要</summary>
我们介绍一个运算方程的数学同源，基于字串重写。我们显示出一个递回函数对迭代重写系统的输出所定义的一个正式的模型类别，这个类别包括了大多数运算机器学习模型（包括回传神经网络、格raph神经网络和扩散模型）的所有主要架构。在category theory中考虑，我们还显示出这些运算模型是动态模型的自然语言描述。此外，我们建议这些模型可以用来对于结构化或非数据的学习问题进行通用化，包括"混合 символиic-numeric"模型。
</details></li>
</ul>
<hr>
<h2 id="RELand-Risk-Estimation-of-Landmines-via-Interpretable-Invariant-Risk-Minimization"><a href="#RELand-Risk-Estimation-of-Landmines-via-Interpretable-Invariant-Risk-Minimization" class="headerlink" title="RELand: Risk Estimation of Landmines via Interpretable Invariant Risk Minimization"></a>RELand: Risk Estimation of Landmines via Interpretable Invariant Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03115">http://arxiv.org/abs/2311.03115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mateo Dulce Rubio, Siqi Zeng, Qi Wang, Didier Alvarado, Francisco Moreno, Hoda Heidari, Fei Fang</li>
<li>for: 支持人道主义排雷操作，提高排雷效率和准确率。</li>
<li>methods: 提供普适的特征工程和标签分配指南，并基于稀缺特征掩模和不变风险最小化算法设计了一种可解释性模型。</li>
<li>results: 在遵循实际排雷操作协议进行了广泛的评估，并显示在比较实际排雷操作中获得了显著提高。<details>
<summary>Abstract</summary>
Landmines remain a threat to war-affected communities for years after conflicts have ended, partly due to the laborious nature of demining tasks. Humanitarian demining operations begin by collecting relevant information from the sites to be cleared, which is then analyzed by human experts to determine the potential risk of remaining landmines. In this paper, we propose RELand system to support these tasks, which consists of three major components. We (1) provide general feature engineering and label assigning guidelines to enhance datasets for landmine risk modeling, which are widely applicable to global demining routines, (2) formulate landmine presence as a classification problem and design a novel interpretable model based on sparse feature masking and invariant risk minimization, and run extensive evaluation under proper protocols that resemble real-world demining operations to show a significant improvement over the state-of-the-art, and (3) build an interactive web interface to suggest priority areas for demining organizations. We are currently collaborating with a humanitarian demining NGO in Colombia that is using our system as part of their field operations in two areas recently prioritized for demining.
</details>
<details>
<summary>摘要</summary>
土地雷 remained a threat to war-affected communities for years after conflicts have ended, partly due to the laborious nature of demining tasks. Humanitarian demining operations begin by collecting relevant information from the sites to be cleared, which is then analyzed by human experts to determine the potential risk of remaining landmines. In this paper, we propose RELand system to support these tasks, which consists of three major components. We (1) provide general feature engineering and label assigning guidelines to enhance datasets for landmine risk modeling, which are widely applicable to global demining routines, (2) formulate landmine presence as a classification problem and design a novel interpretable model based on sparse feature masking and invariant risk minimization, and run extensive evaluation under proper protocols that resemble real-world demining operations to show a significant improvement over the state-of-the-art, and (3) build an interactive web interface to suggest priority areas for demining organizations. We are currently collaborating with a humanitarian demining NGO in Colombia that is using our system as part of their field operations in two areas recently prioritized for demining.Here's the translation breakdown:1. 土地雷 (tǔdì zhú) - landmines2.  remained (jiàn) - remained3. a threat (wēn) - a threat4. to war-affected communities (zhòu yì zhī qīng kè xìng) - to war-affected communities5. for years (nián) - for years6. after conflicts have ended (hòu yì zhī qīng kè xìng) - after conflicts have ended7. partly due to (bù yǐ) - partly due to8. the laborious nature (gōng yì) - the laborious nature9. of demining tasks (zhòu yì zhī qīng kè xìng) - of demining tasks10. Humanitarian demining operations (jīn yì zhī qīng kè xìng) - humanitarian demining operations11. begin (dào) - begin12. by collecting (jī) - by collecting13. relevant information (xiēng yè) - relevant information14. from the sites (zhòu yì zhī qīng kè xìng) - from the sites15. to be cleared (dīng kě) - to be cleared16. which is then analyzed (dào) - which is then analyzed17. by human experts (rén zhī) - by human experts18. to determine (dì) - to determine19. the potential risk (fāng yì) - the potential risk20. of remaining landmines (dào) - of remaining landmines21. In this paper (zhe zhèng) - In this paper22. we propose (dào) - we propose23. RELand system (RELand zhì) - RELand system24. to support (yuè) - to support25. these tasks (zhòu yì zhī qīng kè xìng) - these tasks26. which consists of (bù yǐ) - which consists of27. three major components (sān zhòng zhī qīng kè xìng) - three major components28. We (wǒ) - We29. provide (dào) - provide30. general feature engineering (gōng yì) - general feature engineering31. and label assigning (jiào zhì) - and label assigning32. guidelines (guīdàng) - guidelines33. to enhance (yì) - to enhance34. datasets (zhòu yì zhī qīng kè xìng) - datasets35. for landmine risk modeling (zhòu yì zhī qīng kè xìng) - for landmine risk modeling36. which are widely applicable (bù yǐ) - which are widely applicable37. to global demining routines (qīng kè xìng) - to global demining routines38. formulate (xíng) - formulate39. landmine presence (zhòu yì zhī qīng kè xìng) - landmine presence40. as a classification problem (bǎo yì zhī qīng kè xìng) - as a classification problem41. and design (dì) - and design42. a novel interpretable model (xīn xiǎng) - a novel interpretable model43. based on (bù yǐ) - based on44. sparse feature masking (shū zhì) - sparse feature masking45. and invariant risk minimization (bì yì) - and invariant risk minimization46. run extensive evaluation (dào) - run extensive evaluation47. under proper protocols (zhèng zhì) - under proper protocols48. that resemble (xiǎng) - that resemble49. real-world demining operations (zhòu yì zhī qīng kè xìng) - real-world demining operations50. to show (dì) - to show51. a significant improvement (fāng yì) - a significant improvement52. over the state-of-the-art (zhèng zhì) - over the state-of-the-art53. and (bù yǐ) - and54. build (dào) - build55. an interactive web interface (yuè) - an interactive web interface56. to suggest (jiào) - to suggest57. priority areas (jī) - priority areas58. for demining organizations (zhòu yì zhī qīng kè xìng) - for demining organizations59. We are currently (zài yǐ) - We are currently60. collaborating (xīn xiǎng) - collaborating61. with (bù yǐ) - with62. a humanitarian demining NGO (jīn yì zhī qīng kè xìng) - a humanitarian demining NGO63. in Colombia (Kòlumbī) - in Colombia64. that is using (yǐ) - that is using65. our system (zì) - our system66. as part of their field operations (zhòu yì zhī qīng kè xìng) - as part of their field operations67. in two areas (liǎng yù) - in two areas68. recently prioritized for demining (zhòu yì zhī qīng kè xìng) - recently prioritized for deminingNote that some words and phrases have been shortened or modified for brevity, and some grammatical structures have been adjusted for clarity and consistency.
</details></li>
</ul>
<hr>
<h2 id="Weight-Sharing-Regularization"><a href="#Weight-Sharing-Regularization" class="headerlink" title="Weight-Sharing Regularization"></a>Weight-Sharing Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03096">http://arxiv.org/abs/2311.03096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/motahareh-sohrabi/weight-sharing-regularization">https://github.com/motahareh-sohrabi/weight-sharing-regularization</a></li>
<li>paper_authors: Mehran Shakerinava, Motahareh Sohrabi, Siamak Ravanbakhsh, Simon Lacoste-Julien</li>
<li>for: 这 paper 是为了研究深度学习中的 weight-sharing  regularization，并提出了一种基于 this 的 proximal mapping 算法。</li>
<li>methods: 这 paper 使用了 proximal gradient descent 来训练 weight-sharing  regularized deep neural networks，并提供了一种基于物理系统的解释，以及一种 exponential speedup 的并行算法。</li>
<li>results: 实验表明，weight-sharing regularization 可以使得全连接网络学习 convolution-like 滤波器，并且 proximal mapping 算法可以提供 exponential speedup。<details>
<summary>Abstract</summary>
Weight-sharing is ubiquitous in deep learning. Motivated by this, we introduce ''weight-sharing regularization'' for neural networks, defined as $R(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$. We study the proximal mapping of $R$ and provide an intuitive interpretation of it in terms of a physical system of interacting particles. Using this interpretation, we design a novel parallel algorithm for $\operatorname{prox}_R$ which provides an exponential speedup over previous algorithms, with a depth of $O(\log^3 d)$. Our algorithm makes it feasible to train weight-sharing regularized deep neural networks with proximal gradient descent. Experiments reveal that weight-sharing regularization enables fully-connected networks to learn convolution-like filters.
</details>
<details>
<summary>摘要</summary>
“深度学习中的权重共享是普遍存在的。为了解决这个问题，我们介绍了一种新的“权重共享规则”($R(w) = \frac{1}{d - 1}\sum_{i > j}^d |w_i - w_j|$)，并研究了这个规则的距离映射。我们通过 физи学系中的互动粒子来解释这个映射的含义，并设计了一种新的并行算法来实现 $\operatorname{prox}_R$，它可以在 $O(\log^3 d)$ 深度下提供 exponential 的加速。我们的算法使得可以使用 proximal 梯度下降来训练权重共享规则的深度神经网络。实验表明，权重共享规则可以使得全连接网络学习类似于 convolution 的滤波器。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Equivariance-Is-Not-All-You-Need-Characterizing-the-Utility-of-Equivariant-Graph-Neural-Networks-for-Particle-Physics-Tasks"><a href="#Equivariance-Is-Not-All-You-Need-Characterizing-the-Utility-of-Equivariant-Graph-Neural-Networks-for-Particle-Physics-Tasks" class="headerlink" title="Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks"></a>Equivariance Is Not All You Need: Characterizing the Utility of Equivariant Graph Neural Networks for Particle Physics Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03094">http://arxiv.org/abs/2311.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Savannah Thais, Daniel Murnane</li>
<li>for: This paper evaluates the benefits of using equivariant graph neural networks (GNNs) for real-world particle physics reconstruction tasks, and explores the limitations of these networks in realistic systems.</li>
<li>methods: The paper uses real-world particle physics reconstruction tasks to evaluate the performance of equivariant GNNs, and draws on the relevant literature around group equivariant networks to provide a comprehensive assessment of their benefits.</li>
<li>results: The paper demonstrates that many of the theoretical benefits of equivariant GNNs do not hold for realistic systems, and introduces compelling directions for future research that will benefit both the scientific theory of machine learning and physics applications.<details>
<summary>Abstract</summary>
Incorporating inductive biases into ML models is an active area of ML research, especially when ML models are applied to data about the physical world. Equivariant Graph Neural Networks (GNNs) have recently become a popular method for learning from physics data because they directly incorporate the symmetries of the underlying physical system. Drawing from the relevant literature around group equivariant networks, this paper presents a comprehensive evaluation of the proposed benefits of equivariant GNNs by using real-world particle physics reconstruction tasks as an evaluation test-bed. We demonstrate that many of the theoretical benefits generally associated with equivariant networks may not hold for realistic systems and introduce compelling directions for future research that will benefit both the scientific theory of ML and physics applications.
</details>
<details>
<summary>摘要</summary>
研究在机器学习（ML）模型中涵入推导性偏见是一个活跃领域，尤其是当ML模型应用于物理世界的数据时。最近，对物理系统数据进行学习的等效幂论神经网络（GNNs）已成为一种受欢迎的方法，因为它们直接涵入物理系统的Symmetries。从相关文献中提取到的关于群equivariant网络的知识，这篇论文对提出的优点进行了全面的评估，并使用实际的 particle physics重建任务作为评估测试床。我们发现了许多理论上关于等效网络的优点可能不适用于实际系统，并提出了有吸引力的未来研究方向，这些研究将有助于 both the scientific theory of ML和物理应用。
</details></li>
</ul>
<hr>
<h2 id="Persistent-homology-for-high-dimensional-data-based-on-spectral-methods"><a href="#Persistent-homology-for-high-dimensional-data-based-on-spectral-methods" class="headerlink" title="Persistent homology for high-dimensional data based on spectral methods"></a>Persistent homology for high-dimensional data based on spectral methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03087">http://arxiv.org/abs/2311.03087</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/berenslab/eff-ph">https://github.com/berenslab/eff-ph</a></li>
<li>paper_authors: Sebastian Damrich, Philipp Berens, Dmitry Kobak</li>
<li>For: 检测点云数据中的非凡拓扑结构，如循环或空隙，而vanilla persistent homology在高维空间中容易受到噪声影响并错误检测拓扑结构。* Methods: 使用$k$-nearest-neighbor图的特征距离（如扩散距离和有效抗电阻）来改进 persistent homology，并 derive了一个新的关闭形式表达式，用于计算有效抗电阻，并描述了它们与扩散距离之间的关系。* Results: 应用这些方法于多个高维单元核酸测量数据集，并显示了使用$k$-nearest-neighbor图的特征距离可以robust地检测细胞周期循环。<details>
<summary>Abstract</summary>
Persistent homology is a popular computational tool for detecting non-trivial topology of point clouds, such as the presence of loops or voids. However, many real-world datasets with low intrinsic dimensionality reside in an ambient space of much higher dimensionality. We show that in this case vanilla persistent homology becomes very sensitive to noise and fails to detect the correct topology. The same holds true for most existing refinements of persistent homology. As a remedy, we find that spectral distances on the $k$-nearest-neighbor graph of the data, such as diffusion distance and effective resistance, allow persistent homology to detect the correct topology even in the presence of high-dimensional noise. Furthermore, we derive a novel closed-form expression for effective resistance in terms of the eigendecomposition of the graph Laplacian, and describe its relation to diffusion distances. Finally, we apply these methods to several high-dimensional single-cell RNA-sequencing datasets and show that spectral distances on the $k$-nearest-neighbor graph allow robust detection of cell cycle loops.
</details>
<details>
<summary>摘要</summary>
持续同态是一种广泛使用的计算工具，用于探测点云的不同几何结构，如循环或空隙。然而，许多真实世界数据集具有低自然维度，并且存在高维度的噪声。我们表明，在这种情况下，普通的持续同态会受到噪声的影响，无法正确探测几何结构。同时，大多数现有的持续同态改进方法也无法在高维度噪声下工作。为了解决这个问题，我们发现了一种新的方法，即基于数据集的k最近邻 Graph Laplacian的特征距离，如扩散距离和有效抗量。我们发现这些特征距离可以使持续同态在高维度噪声下正确探测几何结构。此外，我们还提出了一个新的关闭式表达式，用于计算有效抗量，并与扩散距离之间的关系。最后，我们应用这些方法到了一些高维单元细胞RNA-seq数据集，并显示了spectral distances on the $k$-nearest-neighbor graph可以robustly探测细胞周期循环。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-the-value-of-information-transfer-in-population-based-SHM"><a href="#Quantifying-the-value-of-information-transfer-in-population-based-SHM" class="headerlink" title="Quantifying the value of information transfer in population-based SHM"></a>Quantifying the value of information transfer in population-based SHM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03083">http://arxiv.org/abs/2311.03083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aidan J. Hughes, Jack Poole, Nikolaos Dervilis, Paul Gardner, Keith Worden</li>
<li>For: The paper aims to develop a transfer-strategy decision process for a classification task in a population-based structural health monitoring (PBSHM) framework, using domain adaptation.* Methods: The paper uses a transfer decision framework based on the expected value of information transfer, which involves making predictions about classification performance in the target domain after information transfer. A probabilistic regression is used to predict classification performance from a proxy for structural similarity based on the modal assurance criterion.* Results: The paper demonstrates the effectiveness of the proposed transfer-strategy decision process in a simulated SHM maintenance problem, and shows that the expected value of information transfer can be used to guide the transfer of information between structures.<details>
<summary>Abstract</summary>
Population-based structural health monitoring (PBSHM), seeks to address some of the limitations associated with data scarcity that arise in traditional SHM. A tenet of the population-based approach to SHM is that information can be shared between sufficiently-similar structures in order to improve predictive models. Transfer learning techniques, such as domain adaptation, have been shown to be a highly-useful technology for sharing information between structures when developing statistical classifiers for PBSHM. Nonetheless, transfer-learning techniques are not without their pitfalls. In some circumstances, for example if the data distributions associated with the structures within a population are dissimilar, applying transfer-learning methods can be detrimental to classification performance -- this phenomenon is known as negative transfer. Given the potentially-severe consequences of negative transfer, it is prudent for engineers to ask the question `when, what, and how should one transfer between structures?'.   The current paper aims to demonstrate a transfer-strategy decision process for a classification task for a population of simulated structures in the context of a representative SHM maintenance problem, supported by domain adaptation. The transfer decision framework is based upon the concept of expected value of information transfer. In order to compute the expected value of information transfer, predictions must be made regarding the classification (and decision performance) in the target domain following information transfer. In order to forecast the outcome of transfers, a probabilistic regression is used here to predict classification performance from a proxy for structural similarity based on the modal assurance criterion.
</details>
<details>
<summary>摘要</summary>
Population-based结构健康监测（PBSHM）想要解决传统结构健康监测中的数据稀缺问题。PBSHM的一个基本思想是在相似的结构之间共享信息，以改进预测模型。但是转移学习技术，如领域适应，也有一些缺点。在某些情况下，如结构内Population的数据分布不同，应用转移学习方法可能会导致分类性能下降，这种现象被称为负向转移。为了避免负向转移的严重后果，工程师应该问到“何时、何种、如何进行转移？”。本文旨在提出一种转移策略决策过程，用于一种基于预测性能的分类任务，在一个代表性的维保问题上。转移决策框架基于预测信息转移的预期价值。为计算预测信息转移的预期价值，需要对目标领域中的分类性能进行预测。为预测转移的结果，这里使用了一种概率回归方法，来预测基于模式保证因子的结构相似度。
</details></li>
</ul>
<hr>
<h2 id="SoK-Memorisation-in-machine-learning"><a href="#SoK-Memorisation-in-machine-learning" class="headerlink" title="SoK: Memorisation in machine learning"></a>SoK: Memorisation in machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03075">http://arxiv.org/abs/2311.03075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dmitrii Usynin, Moritz Knolle, Georgios Kaissis</li>
<li>for: 本研究旨在探讨机器学习模型中个体数据样本的影响，特别是在深度学习中，对于有限数据生成分布中学习复杂和高维关系时。</li>
<li>methods: 本研究系统化了多种前期研究和视角，探讨memorization在机器学习中的定义和其与模型泛化的关系，以及这些现象对数据隐私的影响。同时，本研究还提供了识别和评估memorization的方法，以及在多种机器学习学习环境中应用这些方法的情况。</li>
<li>results: 本研究结果表明，memorization在机器学习中存在广泛的定义和视角，并且与模型泛化有Close关系。此外，本研究还发现了一些方法可以识别和评估memorization，并且在多种机器学习学习环境中应用这些方法可以提高模型的隐私性。<details>
<summary>Abstract</summary>
Quantifying the impact of individual data samples on machine learning models is an open research problem. This is particularly relevant when complex and high-dimensional relationships have to be learned from a limited sample of the data generating distribution, such as in deep learning. It was previously shown that, in these cases, models rely not only on extracting patterns which are helpful for generalisation, but also seem to be required to incorporate some of the training data more or less as is, in a process often termed memorisation. This raises the question: if some memorisation is a requirement for effective learning, what are its privacy implications? In this work we unify a broad range of previous definitions and perspectives on memorisation in ML, discuss their interplay with model generalisation and their implications of these phenomena on data privacy. Moreover, we systematise methods allowing practitioners to detect the occurrence of memorisation or quantify it and contextualise our findings in a broad range of ML learning settings. Finally, we discuss memorisation in the context of privacy attacks, differential privacy (DP) and adversarial actors.
</details>
<details>
<summary>摘要</summary>
量化机器学习模型中个别数据样本的影响是一个开放的研究问题。特别是在深度学习中，当需要从有限的数据生成分布中学习复杂和高维的关系时，模型不仅需要抽取有助于泛化的模式，还似乎需要直接将一些训练数据纳入模型中，这种现象通常被称为memorization。这引发了一个问题：如果一定程度的memorization是有效学习的必要条件，那么这些现象具有什么隐私含义？在这个工作中，我们将结合各种前期定义和视角来解释memorization在ML中的含义，评估其与模型泛化的关系，并对这些现象在各种ML学习设置下进行系统化分析。此外，我们还会讨论memorization在隐私攻击、数据隐私（DP）和对抗攻击者的情况下的影响。
</details></li>
</ul>
<hr>
<h2 id="Imaging-through-multimode-fibres-with-physical-prior"><a href="#Imaging-through-multimode-fibres-with-physical-prior" class="headerlink" title="Imaging through multimode fibres with physical prior"></a>Imaging through multimode fibres with physical prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03062">http://arxiv.org/abs/2311.03062</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuncheng Zhang, Yingjie Shi, Zheyi Yao, Xiubao Sui, Qian Cheng</li>
<li>for: 这篇论文主要用于提出一种physics-assisted, unsupervised, learning-based fibre imaging scheme，用于在扰动多模式纤维中进行图像重建。</li>
<li>methods: 该方法使用了深度学习，并利用物理优化方法简化了扰动纤维图像与目标图像之间的映射关系，从而降低计算复杂性。</li>
<li>results: 该方法可以通过在线学习，只需要几个扰动纤维的图像和无标目标图像，就可以重建高质量的图像。此外，该方法还提高了学习基于方法在扰动多模式纤维中的通用能力。<details>
<summary>Abstract</summary>
Imaging through perturbed multimode fibres based on deep learning has been widely researched. However, existing methods mainly use target-speckle pairs in different configurations. It is challenging to reconstruct targets without trained networks. In this paper, we propose a physics-assisted, unsupervised, learning-based fibre imaging scheme. The role of the physical prior is to simplify the mapping relationship between the speckle pattern and the target image, thereby reducing the computational complexity. The unsupervised network learns target features according to the optimized direction provided by the physical prior. Therefore, the reconstruction process of the online learning only requires a few speckle patterns and unpaired targets. The proposed scheme also increases the generalization ability of the learning-based method in perturbed multimode fibres. Our scheme has the potential to extend the application of multimode fibre imaging.
</details>
<details>
<summary>摘要</summary>
依靠受扰多模式纤维的成像通过深度学习已经广泛研究。现有方法主要使用不同配置的目标-特点对。很难无需训练网络来重建目标。在本文中，我们提出了一种基于物理优先的、无监督学习基于纤维成像方案。物理优先的作用是将特点图像和扰乱模式图像之间的映射关系简化，从而降低计算复杂性。无监督网络根据优化的方向学习目标特征，因此在线学习只需要几个扰乱模式和无对应的目标。我们的方案还能够提高深度学习基于多模式纤维成像的泛化能力。我们的方案具有扩展多模式纤维成像的潜力。
</details></li>
</ul>
<hr>
<h2 id="Learned-layered-coding-for-Successive-Refinement-in-the-Wyner-Ziv-Problem"><a href="#Learned-layered-coding-for-Successive-Refinement-in-the-Wyner-Ziv-Problem" class="headerlink" title="Learned layered coding for Successive Refinement in the Wyner-Ziv Problem"></a>Learned layered coding for Successive Refinement in the Wyner-Ziv Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03061">http://arxiv.org/abs/2311.03061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Joukovsky, Brent De Weerdt, Nikos Deligiannis</li>
<li>for: 学习进行逐步编码的维нер- Живу coding问题，并在不同的质量水平下逐步解码。</li>
<li>methods: 使用循环神经网络（RNN）学习层次编码器和解码器，并通过最小化变量约束来训练模型。</li>
<li>results: 实验表明，RNN可以直接获得层次归一化解决方案，并且Rate-Distortion性能与单一的维нер- Живу coding方法相当，并且几乎与Rate-Distortion约束相符。<details>
<summary>Abstract</summary>
We propose a data-driven approach to explicitly learn the progressive encoding of a continuous source, which is successively decoded with increasing levels of quality and with the aid of correlated side information. This setup refers to the successive refinement of the Wyner-Ziv coding problem. Assuming ideal Slepian-Wolf coding, our approach employs recurrent neural networks (RNNs) to learn layered encoders and decoders for the quadratic Gaussian case. The models are trained by minimizing a variational bound on the rate-distortion function of the successively refined Wyner-Ziv coding problem. We demonstrate that RNNs can explicitly retrieve layered binning solutions akin to scalable nested quantization. Moreover, the rate-distortion performance of the scheme is on par with the corresponding monolithic Wyner-Ziv coding approach and is close to the rate-distortion bound.
</details>
<details>
<summary>摘要</summary>
我们提出了一种数据驱动的方法，用于显式地学习不断编码一个连续源，并在不同的质量水平下逐步解码，并且使用相关的侧信息。这种设置与Wyner-Ziv编码问题的逐步精化相关。假设理想的Slepian-Wolf编码，我们使用循环神经网络（RNN）学习层次编码器和解码器，并在二阶均勋函数的极限下进行训练。我们示出了RNN可以直接获取层次归一化解决方案，类似于扩展的嵌套量化。此外，我们的方案的比特率-质量表现与对应的庞大Wyner-Ziv编码方法几乎相同，并且几乎与比特率-质量函数的上限相同。
</details></li>
</ul>
<hr>
<h2 id="Personalizing-Keyword-Spotting-with-Speaker-Information"><a href="#Personalizing-Keyword-Spotting-with-Speaker-Information" class="headerlink" title="Personalizing Keyword Spotting with Speaker Information"></a>Personalizing Keyword Spotting with Speaker Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03419">http://arxiv.org/abs/2311.03419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beltrán Labrador, Pai Zhu, Guanlong Zhao, Angelo Scorza Scarpati, Quan Wang, Alicia Lozano-Diez, Alex Park, Ignacio López Moreno</li>
<li>for: 提高关键词检测精度，特别是面对多元化的人群和年龄组团时。</li>
<li>methods: 利用Feature-wise Linear Modulation（FiLM）方法，将说话人信息集成到关键词检测中，实现Text-Dependent和Text-Independent说话人识别系统。</li>
<li>results: 在多元化的数据集上进行测试，实现了关键词检测精度的显著提高，特别是面对下 Represented speaker groups。此外，该方法只需增加1%的参数数量，并不影响延迟和计算成本，适用于实际应用。<details>
<summary>Abstract</summary>
Keyword spotting systems often struggle to generalize to a diverse population with various accents and age groups. To address this challenge, we propose a novel approach that integrates speaker information into keyword spotting using Feature-wise Linear Modulation (FiLM), a recent method for learning from multiple sources of information. We explore both Text-Dependent and Text-Independent speaker recognition systems to extract speaker information, and we experiment on extracting this information from both the input audio and pre-enrolled user audio. We evaluate our systems on a diverse dataset and achieve a substantial improvement in keyword detection accuracy, particularly among underrepresented speaker groups. Moreover, our proposed approach only requires a small 1% increase in the number of parameters, with a minimum impact on latency and computational cost, which makes it a practical solution for real-world applications.
</details>
<details>
<summary>摘要</summary>
��<<SYS>>���racle�спот�系统��� newline�struggle� generalize� diverse� population� various� accents� age�groups. To address� this challenge, we propose� novel approach� integrates� speaker information� into keyword spotting using Feature-wise Linear Modulation (FiLM), a recent method for learning from multiple sources of information. We explore both Text-Dependent and Text-Independent speaker recognition systems to extract speaker information, and we experiment on extracting this information from both the input audio and pre-enrolled user audio. We evaluate our systems on a diverse dataset and achieve a substantial improvement in keyword detection accuracy, particularly among underrepresented speaker groups. Moreover, our proposed approach only requires a small 1% increase in the number of parameters, with a minimum impact on latency and computational cost, which makes it a practical solution for real-world applications.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="DRAUC-An-Instance-wise-Distributionally-Robust-AUC-Optimization-Framework"><a href="#DRAUC-An-Instance-wise-Distributionally-Robust-AUC-Optimization-Framework" class="headerlink" title="DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework"></a>DRAUC: An Instance-wise Distributionally Robust AUC Optimization Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03055">http://arxiv.org/abs/2311.03055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siran Dai, Qianqian Xu, Zhiyong Yang, Xiaochun Cao, Qingming Huang</li>
<li>for: The paper focuses on improving the Area Under the ROC Curve (AUC) in long-tailed classification scenarios, where the distribution of the data is not uniform.</li>
<li>methods: The paper proposes an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC) to optimize the AUC under worst-case distributional shift. The proposed method is built on top of Distributionally Robust Optimization (DRO) to enhance model performance.</li>
<li>results: The paper shows that the proposed method outperforms existing methods in terms of AUC under distributional shift. The authors also highlight that conventional DRAUC may induce label bias and propose a distribution-aware DRAUC as a more suitable metric for robust AUC learning. Theoretical analysis and empirical experiments on corrupted benchmark datasets demonstrate the effectiveness of the proposed method.<details>
<summary>Abstract</summary>
The Area Under the ROC Curve (AUC) is a widely employed metric in long-tailed classification scenarios. Nevertheless, most existing methods primarily assume that training and testing examples are drawn i.i.d. from the same distribution, which is often unachievable in practice. Distributionally Robust Optimization (DRO) enhances model performance by optimizing it for the local worst-case scenario, but directly integrating AUC optimization with DRO results in an intractable optimization problem. To tackle this challenge, methodically we propose an instance-wise surrogate loss of Distributionally Robust AUC (DRAUC) and build our optimization framework on top of it. Moreover, we highlight that conventional DRAUC may induce label bias, hence introducing distribution-aware DRAUC as a more suitable metric for robust AUC learning. Theoretically, we affirm that the generalization gap between the training loss and testing error diminishes if the training set is sufficiently large. Empirically, experiments on corrupted benchmark datasets demonstrate the effectiveness of our proposed method. Code is available at: https://github.com/EldercatSAM/DRAUC.
</details>
<details>
<summary>摘要</summary>
“区域下ROC曲线（AUC）是长尾分类任务中广泛使用的衡量指标。然而，现有的方法几乎都假设训练和测试例子是从同一个分布中随机抽出的，这在实际应用中通常是不可能的。分布强健优化（DRO）可以提高模型性能，但是直接将AUC优化融入DRO中会导致问题不可解。为了解决这个挑战，我们提出了实例别的代理损失函数（surrogate loss），将其组合成一个优化框架。此外，我们点出了传统的DRAUC可能会导致标签偏调，因此提出了分布意识的DRAUC作为更适合的稳健AUC学习指标。理论上，我们证明了训练集大 enough会使得测试集误差与训练集误差之间的差异减少。实验结果显示了我们的提案的效果，codes可以在https://github.com/EldercatSAM/DRAUC上获取。”Note: Please note that the translation is in Simplified Chinese, and some words or phrases may have different translations in Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Validity-problems-in-clinical-machine-learning-by-indirect-data-labeling-using-consensus-definitions"><a href="#Validity-problems-in-clinical-machine-learning-by-indirect-data-labeling-using-consensus-definitions" class="headerlink" title="Validity problems in clinical machine learning by indirect data labeling using consensus definitions"></a>Validity problems in clinical machine learning by indirect data labeling using consensus definitions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03037">http://arxiv.org/abs/2311.03037</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/statnlp/ml4h_validity_problems">https://github.com/statnlp/ml4h_validity_problems</a></li>
<li>paper_authors: Michael Hagmann, Shigehiko Schamoni, Stefan Riezler</li>
<li>for: 预测疾病发展（disease diagnosis）的重要应用领域中存在机器学习的有效性问题。</li>
<li>methods: 研究者使用了一种通用的方法来检测问题数据集和黑盒机器学习模型是否存在问题。</li>
<li>results: 研究者发现了一种检测方法，可以帮助检测问题数据集和黑盒机器学习模型，并且在预测疾病发展任务中进行了实践。<details>
<summary>Abstract</summary>
We demonstrate a validity problem of machine learning in the vital application area of disease diagnosis in medicine. It arises when target labels in training data are determined by an indirect measurement, and the fundamental measurements needed to determine this indirect measurement are included in the input data representation. Machine learning models trained on this data will learn nothing else but to exactly reconstruct the known target definition. Such models show perfect performance on similarly constructed test data but will fail catastrophically on real-world examples where the defining fundamental measurements are not or only incompletely available. We present a general procedure allowing identification of problematic datasets and black-box machine learning models trained on them, and exemplify our detection procedure on the task of early prediction of sepsis.
</details>
<details>
<summary>摘要</summary>
我们描述了机器学习在医学领域的重要应用中的有效性问题。这种问题发生在训练数据中的目标标签是基于间接测量的，而根本测量 needed to determine this indirect measurement 是包含在输入数据表示中的。机器学习模型在这种数据上训练后会只学习重建已知的目标定义，并在同构测试数据上达到完美的性能。但在真实世界中，这些定义基本测量不可能或只有部分可用时，这些模型会catastrophically fail。我们提出了一种通用的数据集问题标识和黑盒机器学习模型训练数据集问题的方法，并在预测性 septic 任务中进行了示例。
</details></li>
</ul>
<hr>
<h2 id="On-regularized-polynomial-functional-regression"><a href="#On-regularized-polynomial-functional-regression" class="headerlink" title="On regularized polynomial functional regression"></a>On regularized polynomial functional regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03036">http://arxiv.org/abs/2311.03036</a></li>
<li>repo_url: None</li>
<li>paper_authors: Markus Holzleitner, Sergei Pereverzyev</li>
<li>for: 该论文涉及多项式函数回归，并提出了一个新的finite sample bound，该bound涵盖了一般的光滑条件、容量条件以及规则化技术等方面。</li>
<li>methods: 该论文使用了多项式函数回归，并采用了一些常用的规则化技术和容量条件。</li>
<li>results: 该论文的numerical evidence表明，使用高阶多项式函数可以提高回归性能。<details>
<summary>Abstract</summary>
This article offers a comprehensive treatment of polynomial functional regression, culminating in the establishment of a novel finite sample bound. This bound encompasses various aspects, including general smoothness conditions, capacity conditions, and regularization techniques. In doing so, it extends and generalizes several findings from the context of linear functional regression as well. We also provide numerical evidence that using higher order polynomial terms can lead to an improved performance.
</details>
<details>
<summary>摘要</summary>
这篇文章提供了多元函数回归的全面征识，最终得出了一个新的finite sample bound。这个 bound 包括了一般的光滑条件、容量条件以及规则化技术。因此，它扩展和总结了 linear functional regression 的一些发现。我们还提供了数字证明，使用更高阶的多元函数项可以提高性能。Note: "finite sample bound" in the original text is translated as "finite sample bound" (finite sample bound) in Simplified Chinese, as there is no direct translation for "finite sample bound" in Chinese.
</details></li>
</ul>
<hr>
<h2 id="Estimating-treatment-effects-from-single-arm-trials-via-latent-variable-modeling"><a href="#Estimating-treatment-effects-from-single-arm-trials-via-latent-variable-modeling" class="headerlink" title="Estimating treatment effects from single-arm trials via latent-variable modeling"></a>Estimating treatment effects from single-arm trials via latent-variable modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03002">http://arxiv.org/abs/2311.03002</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/manuelhaussmann/lvm_singlearm">https://github.com/manuelhaussmann/lvm_singlearm</a></li>
<li>paper_authors: Manuel Haussmann, Tran Minh Son Le, Viivi Halla-aho, Samu Kurki, Jussi Leinonen, Miika Koskinen, Samuel Kaski, Harri Lähdesmäki</li>
<li>for: 代替Randomized controlled trials (RCTs)，用于估计治疗效果。</li>
<li>methods: 使用深度嵌入变量模型，可以考虑欠拟合观察的 Patterns。</li>
<li>results: 比前方法提高了直接治疗效果估计和病人匹配估计的性能。<details>
<summary>Abstract</summary>
Randomized controlled trials (RCTs) are the accepted standard for treatment effect estimation but they can be infeasible due to ethical reasons and prohibitive costs. Single-arm trials, where all patients belong to the treatment group, can be a viable alternative but require access to an external control group. We propose an identifiable deep latent-variable model for this scenario that can also account for missing covariate observations by modeling their structured missingness patterns. Our method uses amortized variational inference to learn both group-specific and identifiable shared latent representations, which can subsequently be used for (i) patient matching if treatment outcomes are not available for the treatment group, or for (ii) direct treatment effect estimation assuming outcomes are available for both groups. We evaluate the model on a public benchmark as well as on a data set consisting of a published RCT study and real-world electronic health records. Compared to previous methods, our results show improved performance both for direct treatment effect estimation as well as for effect estimation via patient matching.
</details>
<details>
<summary>摘要</summary>
Randomized controlled trials (RCTs) 是评估治疗效果的标准方法，但它们可能因为伦理和成本因素而无法实施。单臂试验，其中所有患者都属于治疗组，可以作为一种可行的替代方案。我们提议一种可识别深度隐藏变量模型来解决这种情况，该模型还可以考虑欠拟合的 covariate 观察数据的结构化欠拟合模式。我们的方法使用摘要变量推断来学习群体特定和可识别的共享隐藏变量表示，这些表示可以用于（i）如果治疗结果不可用于治疗组，则进行患者匹配，或者（ii）直接计算治疗效果的推断。我们在一个公共标准和一个包含已发表RCT研究和真实电子医疗记录的数据集上评估了我们的方法。相比之前的方法，我们的结果显示了改进的性能，包括直接治疗效果推断和通过患者匹配来计算治疗效果。
</details></li>
</ul>
<hr>
<h2 id="Variational-Weighting-for-Kernel-Density-Ratios"><a href="#Variational-Weighting-for-Kernel-Density-Ratios" class="headerlink" title="Variational Weighting for Kernel Density Ratios"></a>Variational Weighting for Kernel Density Ratios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03001">http://arxiv.org/abs/2311.03001</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/swyoon/variationally-weighted-kernel-density-estimation">https://github.com/swyoon/variationally-weighted-kernel-density-estimation</a></li>
<li>paper_authors: Sangwoong Yoon, Frank C. Park, Gunsu S Yun, Iljung Kim, Yung-Kyun Noh</li>
<li>for: 提高机器学习中的生成和推理任务中的kernel density estimation（KDE）的精度。</li>
<li>methods: 利用多维 calculus of variations  derive 标准kernel density estimate中的优化Weight函数，以减少预测 posterior 和信息论量度的偏差。</li>
<li>results: 提高了 KDE 中的预测 posterior 和信息论量度的估计精度。<details>
<summary>Abstract</summary>
Kernel density estimation (KDE) is integral to a range of generative and discriminative tasks in machine learning. Drawing upon tools from the multidimensional calculus of variations, we derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios, leading to improved estimates of prediction posteriors and information-theoretic measures. In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.
</details>
<details>
<summary>摘要</summary>
kernel density estimation（KDE）是机器学习中多种生成和识别任务的关键技术之一。我们通过多维Calculus of variations中的工具，得出了改善标准kernel density estimate的优化权重函数，从而提高预测 posterior和信息论量的估计。在这个过程中，我们还探讨了density estimation的一些基本问题，特别是那些使用KDE作为主要构建件的算法。Here's a breakdown of the translation:* Kernel density estimation (KDE) is an important technique in machine learning for generative and discriminative tasks.* We use tools from multidimensional calculus of variations to derive an optimal weight function that reduces bias in standard kernel density estimates for density ratios.* This leads to improved estimates of prediction posteriors and information-theoretic measures.* In the process, we shed light on some fundamental aspects of density estimation, particularly from the perspective of algorithms that employ KDEs as their main building blocks.
</details></li>
</ul>
<hr>
<h2 id="Strong-statistical-parity-through-fair-synthetic-data"><a href="#Strong-statistical-parity-through-fair-synthetic-data" class="headerlink" title="Strong statistical parity through fair synthetic data"></a>Strong statistical parity through fair synthetic data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03000">http://arxiv.org/abs/2311.03000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ivona Krchova, Michael Platzer, Paul Tiwald</li>
<li>for: 保护原始数据隐私和符合公平性标准</li>
<li>methods: 使用人工智能生成的合成数据，通过对敏感属性的学习目标概率分布进行平衡，使下游模型在各个阈值下做出公平预测。</li>
<li>results: 通过将公平性定义为统计平衡，生成的合成数据能够提供强公平预测，即从偏见原始数据中做出公平预测。此公平调整可以直接 integrate into 生成器的采样过程或作为后处理步骤进行。<details>
<summary>Abstract</summary>
AI-generated synthetic data, in addition to protecting the privacy of original data sets, allows users and data consumers to tailor data to their needs. This paper explores the creation of synthetic data that embodies Fairness by Design, focusing on the statistical parity fairness definition. By equalizing the learned target probability distributions of the synthetic data generator across sensitive attributes, a downstream model trained on such synthetic data provides fair predictions across all thresholds, that is, strong fair predictions even when inferring from biased, original data. This fairness adjustment can be either directly integrated into the sampling process of a synthetic generator or added as a post-processing step. The flexibility allows data consumers to create fair synthetic data and fine-tune the trade-off between accuracy and fairness without any previous assumptions on the data or re-training the synthetic data generator.
</details>
<details>
<summary>摘要</summary>
人工生成的数据，除了保护原始数据集的隐私，还允许用户和数据消费者根据需要修改数据。这篇论文探讨了基于 Fairness by Design 的synthetic数据的创造，专注于统计平均性公平定义。通过在敏感特征上均衡synthetic数据生成器学习的目标概率分布，下游模型在such synthetic数据上训练时提供了公平预测结果，无论推理自偏向的原始数据。这种公平调整可以直接集成到synthetic生成器的抽样过程中，或作为后处理步骤进行。这种灵活性允许数据消费者创造公平的synthetic数据，并微调准确性和公平之间的负荷平衡，无需对数据或synthetic数据生成器进行任何先前假设或重新训练。
</details></li>
</ul>
<hr>
<h2 id="Hacking-Cryptographic-Protocols-with-Advanced-Variational-Quantum-Attacks"><a href="#Hacking-Cryptographic-Protocols-with-Advanced-Variational-Quantum-Attacks" class="headerlink" title="Hacking Cryptographic Protocols with Advanced Variational Quantum Attacks"></a>Hacking Cryptographic Protocols with Advanced Variational Quantum Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02986">http://arxiv.org/abs/2311.02986</a></li>
<li>repo_url: None</li>
<li>paper_authors: Borja Aizpurua, Pablo Bermejo, Josu Etxezarreta Martinez, Roman Orus</li>
<li>for: 这个论文旨在提出一种改进的量子变换攻击算法（VQAA），用于攻击密码协议。</li>
<li>methods: 这篇论文使用了新的量子攻击方法，可以更有效地攻击一些知名的密码协议，比如S-DES、S-AES和Blowfish。这些攻击方法使用了更少的量子比特数，并且可以更快地完成攻击任务。</li>
<li>results: 论文的实验结果表明，使用这种新的量子攻击方法可以在一些小型量子计算机上模拟一个32位Blowfish实例的秘钥找到，只需要24次更少的迭代次数 than一个普通攻击。此外，这种攻击方法也可以在其他一些轻量级加密协议中提高攻击成功率。<details>
<summary>Abstract</summary>
Here we introduce an improved approach to Variational Quantum Attack Algorithms (VQAA) on crytographic protocols. Our methods provide robust quantum attacks to well-known cryptographic algorithms, more efficiently and with remarkably fewer qubits than previous approaches. We implement simulations of our attacks for symmetric-key protocols such as S-DES, S-AES and Blowfish. For instance, we show how our attack allows a classical simulation of a small 8-qubit quantum computer to find the secret key of one 32-bit Blowfish instance with 24 times fewer number of iterations than a brute-force attack. Our work also shows improvements in attack success rates for lightweight ciphers such as S-DES and S-AES. Further applications beyond symmetric-key cryptography are also discussed, including asymmetric-key protocols and hash functions. In addition, we also comment on potential future improvements of our methods. Our results bring one step closer assessing the vulnerability of large-size classical cryptographic protocols with Noisy Intermediate-Scale Quantum (NISQ) devices, and set the stage for future research in quantum cybersecurity.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="The-Pursuit-of-Human-Labeling-A-New-Perspective-on-Unsupervised-Learning"><a href="#The-Pursuit-of-Human-Labeling-A-New-Perspective-on-Unsupervised-Learning" class="headerlink" title="The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning"></a>The Pursuit of Human Labeling: A New Perspective on Unsupervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02940">http://arxiv.org/abs/2311.02940</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mlbio-epfl/hume">https://github.com/mlbio-epfl/hume</a></li>
<li>paper_authors: Artyom Gadetsky, Maria Brbic</li>
<li>for: 本研究旨在提出一种无需外部监督的推理方法，用于推测给定数据集的人类标注。</li>
<li>methods: 该方法基于人类标注的类别之间的线性分离性，通过搜索所有可能的标注来找出下面的人类标注。</li>
<li>results: 该方法可以与任何大型预训练和自动监督模型兼容，并在STL-10和CIFAR-10 datasets上实现了显著的提升和相当的性能。相比现有的无监督基elines，本研究在四个图像分类数据集上达到了状态级表现。<details>
<summary>Abstract</summary>
We present HUME, a simple model-agnostic framework for inferring human labeling of a given dataset without any external supervision. The key insight behind our approach is that classes defined by many human labelings are linearly separable regardless of the representation space used to represent a dataset. HUME utilizes this insight to guide the search over all possible labelings of a dataset to discover an underlying human labeling. We show that the proposed optimization objective is strikingly well-correlated with the ground truth labeling of the dataset. In effect, we only train linear classifiers on top of pretrained representations that remain fixed during training, making our framework compatible with any large pretrained and self-supervised model. Despite its simplicity, HUME outperforms a supervised linear classifier on top of self-supervised representations on the STL-10 dataset by a large margin and achieves comparable performance on the CIFAR-10 dataset. Compared to the existing unsupervised baselines, HUME achieves state-of-the-art performance on four benchmark image classification datasets including the large-scale ImageNet-1000 dataset. Altogether, our work provides a fundamentally new view to tackle unsupervised learning by searching for consistent labelings between different representation spaces.
</details>
<details>
<summary>摘要</summary>
我团队今天发布了一种名为HUME的简单的模型无关框架，用于无外部监督的数据集标注推断。我们的思路是，由多个人标注的类将在不同的表示空间中Linearly separable。HUME利用这一点来导引对所有可能的标注集合的搜索，以找到下面的人类标注。我们表明，提出的优化目标与实际的数据集标注高度相关。因此，我们只需在固定的表示空间中训练线性分类器，使得我们的框架与任何大型预训练和自动标注模型兼容。尽管其简单，但HUME在STL-10 dataset上大幅超越了基于自动标注的线性分类器，并在CIFAR-10 dataset上达到了相似的性能。与现有的无监督基准之间比较，HUME在四个基准图像分类dataset上达到了状态的最佳性能，包括大规模的ImageNet-1000 dataset。总的来说，我们的工作提供了一种新的视角来解决无监督学习，通过在不同的表示空间之间寻找一致的标注。
</details></li>
</ul>
<hr>
<h2 id="Edge2Node-Reducing-Edge-Prediction-to-Node-Classification"><a href="#Edge2Node-Reducing-Edge-Prediction-to-Node-Classification" class="headerlink" title="Edge2Node: Reducing Edge Prediction to Node Classification"></a>Edge2Node: Reducing Edge Prediction to Node Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02921">http://arxiv.org/abs/2311.02921</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/arahmatiiii/E2N">https://github.com/arahmatiiii/E2N</a></li>
<li>paper_authors: Zahed Rahmati, Ali Rahmati, Dariush Kazemi</li>
<li>for: 本文是针对图像预测缺失或潜在关系的任务进行研究，提出了一种新的 Edge2Node（E2N）方法，可以直接从图像中提取边的嵌入，不需要预先定义评分函数。</li>
<li>methods: 本文提出了一种新的 E2N 方法，包括在基于给定图像的新图像 H 中创建一个新的图像，并将图像预测任务转换为图像分类任务。</li>
<li>results: 在 ogbl-ddi 和 ogbl-collab 数据集上，E2N 方法与现有的状态艺术方法进行比较，实现了较高的 Hits@20 和 Hits@50 分数。在 ogbl-ddi 数据集上，我们在验证集上达到了 Hits@20 分数为 98.79%，并在测试集上达到了 Hits@20 分数为 98.11%。在 ogbl-collab 数据集上，我们在验证集上达到了 Hits@50 分数为 95.46%，并在测试集上达到了 Hits@50 分数为 95.15%。<details>
<summary>Abstract</summary>
Despite the success of graph neural network models in node classification, edge prediction (the task of predicting missing or potential relationships between nodes in a graph) remains a challenging problem for these models. A common approach for edge prediction is to first obtain the embeddings of two nodes, and then a predefined scoring function is used to predict the existence of an edge between the two nodes. In this paper, we introduce a new approach called E2N (Edge2Node) which directly obtains an embedding for each edge, without the need for a scoring function. To do this, we create a new graph H based on the graph G given for the edge prediction task, and then reduce the edge prediction task on G to a node classification task on H. Our E2N method can be easily applied to any edge prediction task with superior performance and lower computational costs.   For the ogbl-ddi and ogbl-collab datasets, our E2N method outperforms the state-of-the-art methods listed on the leaderboards. Our experiments on the ogbl-ddi dataset achieved a Hits@20 score of 98.79% on the validation set and 98.11% on the test set. On the ogbl-collab dataset, we achieved a Hits@50 score of 95.46% on the validation set and 95.15% on the test set.
</details>
<details>
<summary>摘要</summary>
尽管图 neural network 模型在节点分类任务上得到了成功，但Edge prediction（预测图中缺失或可能存在的两个节点之间的关系）仍然是这些模型中的一个挑战。一般来说，用于Edge prediction的方法是先获取两个节点的嵌入，然后使用预定的分数函数来预测这两个节点之间的关系是否存在。在本文中，我们引入了一种新的方法，即E2N（Edge2Node），可以直接获取每个边的嵌入，不需要预定的分数函数。我们创建了一个新的图H，基于给定的图G，以便在G上进行边预测任务。然后，我们将边预测任务转化为图H上的节点分类任务。我们的E2N方法可以轻松应用于任何边预测任务，并且能够提供更高的性能和更低的计算成本。在ogbl-ddi和ogbl-collab datasets上，我们的E2N方法超过了当前的状态码列表。我们对ogbl-ddi dataset进行了验证集和测试集的验证， validation set上的Hits@20分数为98.79%，测试集上的Hits@20分数为98.11%。在ogbl-collab dataset上，我们对验证集和测试集进行了验证， validation set上的Hits@50分数为95.46%，测试集上的Hits@50分数为95.15%。
</details></li>
</ul>
<hr>
<h2 id="Distributed-Matrix-Based-Sampling-for-Graph-Neural-Network-Training"><a href="#Distributed-Matrix-Based-Sampling-for-Graph-Neural-Network-Training" class="headerlink" title="Distributed Matrix-Based Sampling for Graph Neural Network Training"></a>Distributed Matrix-Based Sampling for Graph Neural Network Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02909">http://arxiv.org/abs/2311.02909</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Alok Tripathy, Katherine Yelick, Aydin Buluc</li>
<li>for: 本文提出了一种新的分布式GNN训练中的采样方法，用于减少通信量。</li>
<li>methods: 本文提出了一种基于矩阵的批量采样方法，可以同时采样多个批处理。在输入图不能在单个设备内存中的情况下，我们将图分布到多个设备上，并使用通信避免的SpGEMM算法来扩展GNN批处理的训练。此外，我们还提出了一种用于代表不同采样算法的简单matrix构造方法。</li>
<li>results: 我们在最大Open Graph Benchmark（OGB）集合上使用128个GPU进行实验，并证明了我们的管道在一个3层GraphSAGE网络上比Quiver（一种分布式PyTorch-Geometric扩展）快$2.5\times$。在OGB集合之外，我们在128个GPU上实现了一个epoch时间的提高$8.46\times$。最后，我们还展示了将图分布在GPU上的扩展和采样算法的扩展。<details>
<summary>Abstract</summary>
The primary contribution of this paper is new methods for reducing communication in the sampling step for distributed GNN training. Here, we propose a matrix-based bulk sampling approach that expresses sampling as a sparse matrix multiplication (SpGEMM) and samples multiple minibatches at once. When the input graph topology does not fit on a single device, our method distributes the graph and use communication-avoiding SpGEMM algorithms to scale GNN minibatch sampling, enabling GNN training on much larger graphs than those that can fit into a single device memory. When the input graph topology (but not the embeddings) fits in the memory of one GPU, our approach (1) performs sampling without communication, (2) amortizes the overheads of sampling a minibatch, and (3) can represent multiple sampling algorithms by simply using different matrix constructions. In addition to new methods for sampling, we show that judiciously replicating feature data with a simple all-to-all exchange can outperform current methods for the feature extraction step in distributed GNN training. We provide experimental results on the largest Open Graph Benchmark (OGB) datasets on $128$ GPUs, and show that our pipeline is $2.5\times$ faster Quiver (a distributed extension to PyTorch-Geometric) on a $3$-layer GraphSAGE network. On datasets outside of OGB, we show a $8.46\times$ speedup on $128$ GPUs in-per epoch time. Finally, we show scaling when the graph is distributed across GPUs and scaling for both node-wise and layer-wise sampling algorithms
</details>
<details>
<summary>摘要</summary>
主要贡献 OF 这篇论文在于提出了用于降低分布式 GNN 训练中采样步骤中的新方法。我们提议了一种基于矩阵的批量采样方法，将采样表示为稀疏矩阵乘法（SpGEMM），并在一次多个批处理多个批处理。当输入图 topology 不能在单个设备内存中存储时，我们将图分布在多个设备上，并使用通信快速的 SpGEMM 算法来扩展 GNN 批处理的训练范围。当输入图 topology 可以在单个 GPU 内存中存储时，我们的方法可以无需通信进行采样，并且可以吃费采样批处理的开销。此外，我们还提出了一种使用简单的所有对所有交换来快速复制特征数据的方法，以提高分布式 GNN 训练中的特征提取步骤。我们在 $128$ 个 GPU 上进行了实验，并证明了我们的管道在 $3$-layer GraphSAGE 网络上比 Quiver （分布式 PyTorch-Geometric 扩展）快 $2.5\times$。在 OGB  datasets 外，我们在 $128$ 个 GPU 上实现了在一个epoch时间内的快速速度为 $8.46\times$。最后，我们还展示了分布在 GPU 上的图和层wise 采样算法的扩展。
</details></li>
</ul>
<hr>
<h2 id="HDGL-A-hierarchical-dynamic-graph-representation-learning-model-for-brain-disorder-classification"><a href="#HDGL-A-hierarchical-dynamic-graph-representation-learning-model-for-brain-disorder-classification" class="headerlink" title="HDGL: A hierarchical dynamic graph representation learning model for brain disorder classification"></a>HDGL: A hierarchical dynamic graph representation learning model for brain disorder classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02903">http://arxiv.org/abs/2311.02903</a></li>
<li>repo_url: None</li>
<li>paper_authors: Parniyan Jalali, Mehran Safayani</li>
<li>For: 本研究旨在提出一种 Hierarchical Dynamic Graph Representation Learning（HDGL）模型，用于分类大脑疾病样本和健康样本。* Methods:  HDGL模型包括两个层次，第一层构建大脑网络图和学习其空间和时间嵌入，第二层组建人口图并进行分类。此外，为了降低内存复杂性，提出了四种方法。* Results: 研究在 ABIDE 和 ADHD-200 数据集上进行了评估，结果表明 HDGL 模型在多种评价指标上比一些状态顶模型表现更好。<details>
<summary>Abstract</summary>
The human brain can be considered as complex networks, composed of various regions that continuously exchange their information with each other, forming the brain network graph, from which nodes and edges are extracted using resting-state functional magnetic resonance imaging (rs-fMRI). Therefore, this graph can potentially depict abnormal patterns that have emerged under the influence of brain disorders. So far, numerous studies have attempted to find embeddings for brain network graphs and subsequently classify samples with brain disorders from healthy ones, which include limitations such as: not considering the relationship between samples, not utilizing phenotype information, lack of temporal analysis, using static functional connectivity (FC) instead of dynamic ones and using a fixed graph structure. We propose a hierarchical dynamic graph representation learning (HDGL) model, which is the first model designed to address all the aforementioned challenges. HDGL consists of two levels, where at the first level, it constructs brain network graphs and learns their spatial and temporal embeddings, and at the second level, it forms population graphs and performs classification after embedding learning. Furthermore, based on how these two levels are trained, four methods have been introduced, some of which are suggested for reducing memory complexity. We evaluated the performance of the proposed model on the ABIDE and ADHD-200 datasets, and the results indicate the improvement of this model compared to several state-of-the-art models in terms of various evaluation metrics.
</details>
<details>
<summary>摘要</summary>
人类大脑可以视为复杂的网络，由多个区域组成，这些区域不断地交换信息，形成大脑网络图，从而可以承载着脑病的异常模式。因此，这个图可能能够描述出脑病的异常模式。迄今为止，许多研究已经尝试使用大脑网络图的嵌入和分类样本，但是这些研究存在一些限制，如：不考虑样本之间的关系、不使用类型信息、缺乏时间分析、使用静态功能连接（FC）而不是动态连接、使用固定图结构。我们提出了层次动态图表学习（HDGL）模型，这是第一个解决以上所有挑战的模型。HDGL包括两层，在第一层中，它构建大脑网络图并学习其空间和时间嵌入，在第二层中，它形成人口图并进行分类 після嵌入学习。此外，根据这两层的训练方式，我们引入了四种方法来降低内存复杂性。我们对ABIDE和ADHD-200 dataset进行了评估，结果表明，提案的模型在多种评估指标上比多个状态的艺术模型表现出色。
</details></li>
</ul>
<hr>
<h2 id="Transduce-and-Speak-Neural-Transducer-for-Text-to-Speech-with-Semantic-Token-Prediction"><a href="#Transduce-and-Speak-Neural-Transducer-for-Text-to-Speech-with-Semantic-Token-Prediction" class="headerlink" title="Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction"></a>Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02898">http://arxiv.org/abs/2311.02898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Dongjune Lee, Nam Soo Kim</li>
<li>for: 提出了一个基于神经抽象器的文本读取调变（TTS）框架，以便实现文本读取调变的简洁整合。</li>
<li>methods: 使用wav2vec2.0嵌入表示，从而获得精确的语意标识，然后透过神经抽象器生成对应的语意标识，最后使用非autoregressive（NAR）算法生成语音讯号。</li>
<li>results: 实验结果显示，提案的模型在零基础适应TTS中 exceeds 基于的benchmarks 的语音质量和话者相似度，并且 investigate了单位处理时间和语速控制的可能性。<details>
<summary>Abstract</summary>
We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.
</details>
<details>
<summary>摘要</summary>
我们提出了基于神经转化器的文本到语音（TTS）框架。我们使用从wav2vec2.0嵌入中获得的分词，这使得我们可以轻松地采用神经转化器来实现TTS框架，并且受到它的幂等约束。我们的提案的模型首先使用神经转化器生成了对齐的 semantic tokens，然后使用非自然语言生成器（NAR）来synthesize语音样本。这种解体的框架可以减轻TTS训练复杂度，让每个阶段都可以专注于1) 语言和对齐模型化和2) 细腻的声音模型化。我们的实验结果表明，我们的提案模型在零shot适应TTS中超越了基线值，在语音质量和发音相似性方面通过对象和主观度量表现出色。我们还调查了我们的提案模型的推理速度和幂制控性，表明神经转化器在TTS框架中的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="AdaFlood-Adaptive-Flood-Regularization"><a href="#AdaFlood-Adaptive-Flood-Regularization" class="headerlink" title="AdaFlood: Adaptive Flood Regularization"></a>AdaFlood: Adaptive Flood Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02891">http://arxiv.org/abs/2311.02891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonho Bae, Yi Ren, Mohamad Osama Ahmed, Frederick Tung, Danica J. Sutherland, Gabriel L. Oliveira</li>
<li>for: 提高模型的测验时间一致性（test time generalization）</li>
<li>methods: 使用 AdaFlood 方法，将训练数据中每个示例的净训练损失阈值（flood level）灵活地调整为每个示例的难度（difficulty）</li>
<li>results: 在文本、图像、异步事件序列和表格等多种输入模式下，实验结果显示 AdaFlood 可以在数据领域和噪音水平上展现弹性。<details>
<summary>Abstract</summary>
Although neural networks are conventionally optimized towards zero training loss, it has been recently learned that targeting a non-zero training loss threshold, referred to as a flood level, often enables better test time generalization. Current approaches, however, apply the same constant flood level to all training samples, which inherently assumes all the samples have the same difficulty. We present AdaFlood, a novel flood regularization method that adapts the flood level of each training sample according to the difficulty of the sample. Intuitively, since training samples are not equal in difficulty, the target training loss should be conditioned on the instance. Experiments on datasets covering four diverse input modalities - text, images, asynchronous event sequences, and tabular - demonstrate the versatility of AdaFlood across data domains and noise levels.
</details>
<details>
<summary>摘要</summary>
尽管神经网络通常被优化为零训练损失，但最近的研究发现，目标一个非零训练损失水平，称为洪水水平，经常能提高测试时的泛化性。然而，现有的方法通常对所有训练样本应用相同的定点洪水水平，这种假设所有样本具有相同的难度。我们介绍了 AdaFlood，一种新的洪水规范方法，可以根据训练样本的难度来适应洪水水平。这种 intuition 是，由于训练样本不是平等的难度，因此训练损失应该基于实例。经验表明， AdaFlood 可以在不同的输入模式 - 文本、图像、异步事件序列和表格 - 中进行多样化应用，并在噪音水平上进行适应。
</details></li>
</ul>
<hr>
<h2 id="MultiSPANS-A-Multi-range-Spatial-Temporal-Transformer-Network-for-Traffic-Forecast-via-Structural-Entropy-Optimization"><a href="#MultiSPANS-A-Multi-range-Spatial-Temporal-Transformer-Network-for-Traffic-Forecast-via-Structural-Entropy-Optimization" class="headerlink" title="MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for Traffic Forecast via Structural Entropy Optimization"></a>MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for Traffic Forecast via Structural Entropy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02880">http://arxiv.org/abs/2311.02880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/selgroup/multispans">https://github.com/selgroup/multispans</a></li>
<li>paper_authors: Dongcheng Zou, Senzhang Wang, Xuefeng Li, Hao Peng, Yuandong Wang, Chunyang Liu, Kehua Sheng, Bo Zhang</li>
<li>for: 本文提出了一种基于多 span 的嵌入式 transformer 模型，用于解决现有方法在模型复杂多范围关系和路网层次知识方面的缺陷。</li>
<li>methods: 本文使用了多filter convolution 模块生成有用的 ST-token 嵌入，以便计算注意力计算。然后，通过 ST-token 和空间时间位编码，使用 transformers 捕捉长范围的时间和空间关系。此外，本文引入了结构 entropy 理论来优化空间注意力机制。</li>
<li>results: 对多个实际交通数据集进行了广泛的实验，并证明了提出的框架在多种状况下具有较好的性能，并能够有效利用更长的历史窗口。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/SELGroup/MultiSPANS">https://github.com/SELGroup/MultiSPANS</a> 上下载。<details>
<summary>Abstract</summary>
Traffic forecasting is a complex multivariate time-series regression task of paramount importance for traffic management and planning. However, existing approaches often struggle to model complex multi-range dependencies using local spatiotemporal features and road network hierarchical knowledge. To address this, we propose MultiSPANS. First, considering that an individual recording point cannot reflect critical spatiotemporal local patterns, we design multi-filter convolution modules for generating informative ST-token embeddings to facilitate attention computation. Then, based on ST-token and spatial-temporal position encoding, we employ the Transformers to capture long-range temporal and spatial dependencies. Furthermore, we introduce structural entropy theory to optimize the spatial attention mechanism. Specifically, The structural entropy minimization algorithm is used to generate optimal road network hierarchies, i.e., encoding trees. Based on this, we propose a relative structural entropy-based position encoding and a multi-head attention masking scheme based on multi-layer encoding trees. Extensive experiments demonstrate the superiority of the presented framework over several state-of-the-art methods in real-world traffic datasets, and the longer historical windows are effectively utilized. The code is available at https://github.com/SELGroup/MultiSPANS.
</details>
<details>
<summary>摘要</summary>
宽泛预测是一个复杂的多变量时间序列回归任务，对交通管理和规划都具有极高的重要性。然而，现有的方法 oftentimes 难以模型复杂的多范围依赖关系使用本地空间时间特征和路网层次知识。为Addressing this challenge, we propose MultiSPANS.First, we recognize that an individual recording point cannot fully capture critical spatiotemporal local patterns, so we design multi-filter convolution modules to generate informative ST-token embeddings for facilitating attention computation. Then, based on ST-token and spatial-temporal position encoding, we employ Transformers to capture long-range temporal and spatial dependencies. Furthermore, we introduce structural entropy theory to optimize the spatial attention mechanism. Specifically, we use the structural entropy minimization algorithm to generate optimal road network hierarchies, i.e., encoding trees. Based on this, we propose a relative structural entropy-based position encoding and a multi-head attention masking scheme based on multi-layer encoding trees.Extensive experiments demonstrate the superiority of the presented framework over several state-of-the-art methods in real-world traffic datasets, and the longer historical windows are effectively utilized. The code is available at https://github.com/SELGroup/MultiSPANS.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Active-Learning-in-Meta-Learning-Enhancing-Context-Set-Labeling"><a href="#Exploring-Active-Learning-in-Meta-Learning-Enhancing-Context-Set-Labeling" class="headerlink" title="Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling"></a>Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02879">http://arxiv.org/abs/2311.02879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wonho Bae, Jing Wang, Danica J. Sutherland</li>
<li>for: 本文针对Meta-learning中的active learning问题做出了研究，特别是在Context Set中选择哪些点进行标注。</li>
<li>methods: 本文提出了一种基于 Gaussian mixture 的自然算法，用于选择需要标注的点。这种算法的选择基于meta-learning过程中active learning的各个部分。</li>
<li>results:  Comparing with现有的active learning方法，本文的提出的算法在各种 benchmark 数据集上表现出了更好的性能。<details>
<summary>Abstract</summary>
Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets.
</details>
<details>
<summary>摘要</summary>
大多数元学习方法假设测试时使用的上下文集（非常小）是被提供的，而不是主动选择。然而，在某些情况下，可以通过选择哪些点标注来获得更大的提升，但这需要与常见的活动学习设置作出重大差异。我们在这种框架下解释了如何使用活动元学习来标注上下文集，具体来说是根据哪些部分使用活动学习。我们提议一种自然的 Gaussian 混合函数来选择需要标注的点，尽管简单，但也有理论上的推动。我们的提议算法在考试数据集上与现有的活动学习方法进行比较，表现出色。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-Bounds-for-Estimating-Probability-Divergences-under-Invariances"><a href="#Sample-Complexity-Bounds-for-Estimating-Probability-Divergences-under-Invariances" class="headerlink" title="Sample Complexity Bounds for Estimating Probability Divergences under Invariances"></a>Sample Complexity Bounds for Estimating Probability Divergences under Invariances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02868">http://arxiv.org/abs/2311.02868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Behrooz Tahmasebi, Stefanie Jegelka</li>
<li>for: 这篇论文旨在研究如何使用流chartsgroup的自然对称性提高样本复杂度估计 Wasserstein距离、Sobolev集成概率度量（Sobolev IPMs）、最大均方差（MMD）以及density估计问题的复杂度。</li>
<li>methods: 该论文使用了流chartsgroup的自然对称性来提高样本复杂度估计的效率。</li>
<li>results: 该论文的结果表明，使用流chartsgroup的自然对称性可以提高样本复杂度估计的效率， Specifically, the paper shows that there is a two-fold gain: (1) reducing the sample complexity by a multiplicative factor corresponding to the group size (for finite groups) or the normalized volume of the quotient space (for groups of positive dimension); (2) improving the exponent in the convergence rate (for groups of positive dimension). These results are completely new for groups of positive dimension and extend recent bounds for finite group actions.<details>
<summary>Abstract</summary>
Group-invariant probability distributions appear in many data-generative models in machine learning, such as graphs, point clouds, and images. In practice, one often needs to estimate divergences between such distributions. In this work, we study how the inherent invariances, with respect to any smooth action of a Lie group on a manifold, improve sample complexity when estimating the Wasserstein distance, the Sobolev Integral Probability Metrics (Sobolev IPMs), the Maximum Mean Discrepancy (MMD), and also the complexity of the density estimation problem (in the $L^2$ and $L^\infty$ distance). Our results indicate a two-fold gain: (1) reducing the sample complexity by a multiplicative factor corresponding to the group size (for finite groups) or the normalized volume of the quotient space (for groups of positive dimension); (2) improving the exponent in the convergence rate (for groups of positive dimension). These results are completely new for groups of positive dimension and extend recent bounds for finite group actions.
</details>
<details>
<summary>摘要</summary>
群体共轭的概率分布出现在机器学习中的数据生成模型中，如图、点云和图像。在实践中，我们经常需要估算这些分布之间的差异。在这项工作中，我们研究了在任意平滑 Lie 群动作下的拓扑 manifold 上的自然协变性如何提高样本复杂性when estimating Wasserstein distance, Sobolev Integral Probability Metrics (Sobolev IPMs), Maximum Mean Discrepancy (MMD), 以及density estimation问题的复杂性（在 $L^2$ 和 $L^\infty$ 距离中）。我们的结果表明有两种新的优点：1. 通过 GROUP 的大小（对于有限群）或投影空间的归一化体积来减少样本复杂性的因子；2. 在有限维度的群动作中，提高了对数减少率的 exponent。这些结果对于有限维度的群动作是完全新的，并推广了最近的 finite 群动作 bounds。
</details></li>
</ul>
<hr>
<h2 id="Barron-Space-for-Graph-Convolution-Neural-Networks"><a href="#Barron-Space-for-Graph-Convolution-Neural-Networks" class="headerlink" title="Barron Space for Graph Convolution Neural Networks"></a>Barron Space for Graph Convolution Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02838">http://arxiv.org/abs/2311.02838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seok-Young Chung, Qiyu Sun</li>
<li>for: 这个论文是为了研究图像领域中的图 convolutional neural network (GCNN) 的性能和可学习性而写的。</li>
<li>methods: 这篇论文提出了一个 Barron 空间的函数在紧密的域上的研究，并证明了该空间是一个 reproduce kernel Banach space，可以分解为一系列的 reproduce kernel Hilbert space with neuron kernels，并且可以densely embed在连续函数空间中。</li>
<li>results: 论文显示了 GCNN 的输出可以包含在 Barron 空间中，而 Barron 空间中的函数可以由一些 GCNN 的输出在积分平坦和固定量的测量中进行有效地学习。此外，论文还估计了 Barron 空间中函数的 Rademacher 复杂性，并证明了 functions 在 Barron 空间中可以由Random sampling efficiently learning。<details>
<summary>Abstract</summary>
Graph convolutional neural network (GCNN) operates on graph domain and it has achieved a superior performance to accomplish a wide range of tasks. In this paper, we introduce a Barron space of functions on a compact domain of graph signals. We prove that the proposed Barron space is a reproducing kernel Banach space, it can be decomposed into the union of a family of reproducing kernel Hilbert spaces with neuron kernels, and it could be dense in the space of continuous functions on the domain. Approximation property is one of the main principles to design neural networks. In this paper, we show that outputs of GCNNs are contained in the Barron space and functions in the Barron space can be well approximated by outputs of some GCNNs in the integrated square and uniform measurements. We also estimate the Rademacher complexity of functions with bounded Barron norm and conclude that functions in the Barron space could be learnt from their random samples efficiently.
</details>
<details>
<summary>摘要</summary>
“图 convolutional neural network（GCNN）在图域上运行，并实现了广泛的任务。在这篇论文中，我们介绍了一个巴隆空间函数在封闭域上的图信号。我们证明了提议的巴隆空间是一个复制kernel Banach空间，可以分解为一家族 reproduce kernel Hilbert space with neuron kernels，并且可以在域上 dense 的函数空间中。对于 neural network 的设计，精度是一个关键原则。在这篇论文中，我们表明了 GCNN 的输出在巴隆空间中，而巴隆空间中的函数可以由一些 GCNN 的集成平方和均匀测量良好地 aproximate。我们还估算了 bounded 巴隆 нор的函数的拉德玛勒复杂度，并结论了可以由它们的随机样本进行高效地学习。”Note: The translation is in Simplified Chinese, which is one of the two standardized forms of Chinese. The other form is Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Prioritized-Propagation-in-Graph-Neural-Networks"><a href="#Prioritized-Propagation-in-Graph-Neural-Networks" class="headerlink" title="Prioritized Propagation in Graph Neural Networks"></a>Prioritized Propagation in Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02832">http://arxiv.org/abs/2311.02832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Cheng, Minjie Chen, Xiang Li, Caihua Shan, Ming Gao</li>
<li>for: 本文旨在提出一种可与现有的图 neural network (GNN) 模型结合使用的框架，以学习图中节点的个性化消息传递步骤。</li>
<li>methods: 该框架包含三个组件：基础 GNN 模型、传递控制器和重量控制器。 transmitController 用于确定节点的优化传递步骤，weightController 用于计算节点的优先级分数。</li>
<li>results: 通过对 8 个 benchmark 数据集进行广泛的实验比较，我们发现我们的框架可以在传递策略和节点表示方面达到更高的性能。<details>
<summary>Abstract</summary>
Graph neural networks (GNNs) have recently received significant attention. Learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph. Despite the success, existing methods ignore node priority that can be reflected by node influence and heterophily. In this paper, we propose a versatile framework PPro, which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs. Specifically, the framework consists of three components: a backbone GNN model, a propagation controller to determine the optimal propagation steps for nodes, and a weight controller to compute the priority scores for nodes. We design a mutually enhanced mechanism to compute node priority, optimal propagation step and label prediction. We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers. We conduct extensive experiments to compare our framework with other 11 state-of-the-art competitors on 8 benchmark datasets. Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations.
</details>
<details>
<summary>摘要</summary>
graf neuronetWORKS (GNNs) 在最近几年received significant attention。learning node-wise message propagation in GNNs aims to set personalized propagation steps for different nodes in the graph。Despite the success，existing methods ignore node priority that can be reflected by node influence and heterophily。In this paper，we propose a versatile framework PPro，which can be integrated with most existing GNN models and aim to learn prioritized node-wise message propagation in GNNs。Specifically，the framework consists of three components：a backbone GNN model，a propagation controller to determine the optimal propagation steps for nodes，and a weight controller to compute the priority scores for nodes。We design a mutually enhanced mechanism to compute node priority，optimal propagation step and label prediction。We also propose an alternative optimization strategy to learn the parameters in the backbone GNN model and two parametric controllers。We conduct extensive experiments to compare our framework with other 11 state-of-the-art competitors on 8 benchmark datasets。Experimental results show that our framework can lead to superior performance in terms of propagation strategies and node representations。
</details></li>
</ul>
<hr>
<h2 id="On-Subagging-Boosted-Probit-Model-Trees"><a href="#On-Subagging-Boosted-Probit-Model-Trees" class="headerlink" title="On Subagging Boosted Probit Model Trees"></a>On Subagging Boosted Probit Model Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02827">http://arxiv.org/abs/2311.02827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tian Qin, Wei-Min Huang</li>
<li>For: 这个研究旨在提出一个新的混合式袋包-提升算法，以解决分类问题。* Methods: 这个算法使用了新的搜寻树模型（Probit Model Tree，PMT）作为提升部分的基本分类器，并在袋包部分使用了增强后的PMTS。* Results: 我们的理论分析显示了这个算法的一致性和缩小普通误差的可能性，并且显示了增加袋包次数可以减少这个算法的一般化误差。实验结果显示了这个算法具有优秀的预测力，并且在一些情况下表现更好。<details>
<summary>Abstract</summary>
With the insight of variance-bias decomposition, we design a new hybrid bagging-boosting algorithm named SBPMT for classification problems. For the boosting part of SBPMT, we propose a new tree model called Probit Model Tree (PMT) as base classifiers in AdaBoost procedure. For the bagging part, instead of subsampling from the dataset at each step of boosting, we perform boosted PMTs on each subagged dataset and combine them into a powerful "committee", which can be viewed an incomplete U-statistic. Our theoretical analysis shows that (1) SBPMT is consistent under certain assumptions, (2) Increase the subagging times can reduce the generalization error of SBPMT to some extent and (3) Large number of ProbitBoost iterations in PMT can benefit the performance of SBPMT with fewer steps in the AdaBoost part. Those three properties are verified by a famous simulation designed by Mease and Wyner (2008). The last two points also provide a useful guidance in model tuning. A comparison of performance with other state-of-the-art classification methods illustrates that the proposed SBPMT algorithm has competitive prediction power in general and performs significantly better in some cases.
</details>
<details>
<summary>摘要</summary>
针对分类问题，我们基于差异-偏见分解的视角设计了一种新的杂合袋包-提升算法，称为SBPMT。在提升部分，我们提出了一种新的树模型，称为概率模型树（PMT），作为AdaBoost过程中的基准分类器。在袋包部分，而不是从数据集中随机抽样，我们在每次提升过程中运行了提升PMТ，并将其组合成一个强大的“委员会”，可以视为不完全的U统计。我们的理论分析表明，SBPMT在满足 certain assumptions 的情况下是一个一致的算法，并且可以降低提升PMТ的总体化风险。此外，我们还发现，增加袋包次数可以减少SBPMT的总体化风险，并且大量的概率提升迭代可以提高SBPMT的性能，但是需要 fewer steps 在AdaBoost部分。这些发现都得到了Mease和Wyner（2008）的著名的模拟 verify。最后，我们还对SBPMT的性能进行了比较，并发现它在总的来说具有竞争力，并在一些情况下表现更好。
</details></li>
</ul>
<hr>
<h2 id="Signal-Processing-Meets-SGD-From-Momentum-to-Filter"><a href="#Signal-Processing-Meets-SGD-From-Momentum-to-Filter" class="headerlink" title="Signal Processing Meets SGD: From Momentum to Filter"></a>Signal Processing Meets SGD: From Momentum to Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02818">http://arxiv.org/abs/2311.02818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhipeng Yao, Guisong Chang, Jiaqi Zhang, Qi Zhang, Yu Zhang, Dazhou Li</li>
<li>for: 本研究旨在探讨降低历史梯度方差对当前梯度估计的影响，以便优化器更快地趋向于平滑解。</li>
<li>methods: 本文提出了一种基于减少历史梯度方差的新优化方法，使用维ener filter理论来增强SGD的首 moments estimator，并在深度学习模型训练中采用适应性的权重调整。</li>
<li>results: 实验结果表明，提出的SGDF优化器可以与现有的优化器相比，在深度学习模型训练中达到满意的性能。<details>
<summary>Abstract</summary>
In the field of deep learning, Stochastic Gradient Descent (SGD) and its momentum-based variants are the predominant choices for optimization algorithms. Despite all that, these momentum strategies, which accumulate historical gradients by using a fixed $\beta$ hyperparameter to smooth the optimization processing, often neglect the potential impact of the variance of historical gradients on the current gradient estimation. In the gradient variance during training, fluctuation indicates the objective function does not meet the Lipschitz continuity condition at all time, which raises the troublesome optimization problem. This paper aims to explore the potential benefits of reducing the variance of historical gradients to make optimizer converge to flat solutions. Moreover, we proposed a new optimization method based on reducing the variance. We employed the Wiener filter theory to enhance the first moment estimation of SGD, notably introducing an adaptive weight to optimizer. Specifically, the adaptive weight dynamically changes along with temporal fluctuation of gradient variance during deep learning model training. Experimental results demonstrated our proposed adaptive weight optimizer, SGDF (Stochastic Gradient Descent With Filter), can achieve satisfactory performance compared with state-of-the-art optimizers.
</details>
<details>
<summary>摘要</summary>
在深度学习领域，随机梯度下降（SGD）和其带有动量的变体是主要的优化算法。尽管如此，这些动量策略通常忽视了历史梯度的方差对当前梯度估计的影响。在训练过程中，梯度方差的变化指示目标函数不满足 lipschitz连续性条件，这会导致优化问题。本文旨在探索减少历史梯度方差的可能性，以使优化器 converge to 平滑解。此外，我们还提出了一种基于减少方差的新的优化方法。我们利用了wiener filter理论来增强SGD中的首个oment estimation，特别是引入了适应量到优化器。具体来说，适应量在时间上随着梯度方差的 temporal fluctuation而改变。实验结果表明我们提出的适应量优化器（SGDF）可以与当前的优化器相比，实现满意的性能。
</details></li>
</ul>
<hr>
<h2 id="APGL4SR-A-Generic-Framework-with-Adaptive-and-Personalized-Global-Collaborative-Information-in-Sequential-Recommendation"><a href="#APGL4SR-A-Generic-Framework-with-Adaptive-and-Personalized-Global-Collaborative-Information-in-Sequential-Recommendation" class="headerlink" title="APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation"></a>APGL4SR: A Generic Framework with Adaptive and Personalized Global Collaborative Information in Sequential Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02816">http://arxiv.org/abs/2311.02816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/graph-team/apgl4sr">https://github.com/graph-team/apgl4sr</a></li>
<li>paper_authors: Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen</li>
<li>for: 提高Sequential Recommendation系统的效果， capture dynamic preferences和global collaborative information</li>
<li>methods: 使用自适应和个性化的全局合作信息，自适应全球图 constructions， relative positional encoding， multi-task learning paradigm</li>
<li>results: 可以outperform其他基eline with significant margins， improve recommendation performance<details>
<summary>Abstract</summary>
The sequential recommendation system has been widely studied for its promising effectiveness in capturing dynamic preferences buried in users' sequential behaviors. Despite the considerable achievements, existing methods usually focus on intra-sequence modeling while overlooking exploiting global collaborative information by inter-sequence modeling, resulting in inferior recommendation performance. Therefore, previous works attempt to tackle this problem with a global collaborative item graph constructed by pre-defined rules. However, these methods neglect two crucial properties when capturing global collaborative information, i.e., adaptiveness and personalization, yielding sub-optimal user representations. To this end, we propose a graph-driven framework, named Adaptive and Personalized Graph Learning for Sequential Recommendation (APGL4SR), that incorporates adaptive and personalized global collaborative information into sequential recommendation systems. Specifically, we first learn an adaptive global graph among all items and capture global collaborative information with it in a self-supervised fashion, whose computational burden can be further alleviated by the proposed SVD-based accelerator. Furthermore, based on the graph, we propose to extract and utilize personalized item correlations in the form of relative positional encoding, which is a highly compatible manner of personalizing the utilization of global collaborative information. Finally, the entire framework is optimized in a multi-task learning paradigm, thus each part of APGL4SR can be mutually reinforced. As a generic framework, APGL4SR can outperform other baselines with significant margins. The code is available at https://github.com/Graph-Team/APGL4SR.
</details>
<details>
<summary>摘要</summary>
“对称推荐系统已经广泛研究，因为它能够充分捕捉用户的动态喜好。然而，现有的方法通常专注于内部序列模型，忽略了将全局协力信息应用于推荐，从而导致推荐性能不佳。因此，先前的工作尝试使用全局协力项目Graph来解决这个问题，但这些方法忽略了两个重要的属性，即适应和个性化。为了解决这个问题，我们提出了一个图驱动的框架，名为适应和个性化图驱动推荐系统（APGL4SR）。 Specifically, we first learn an adaptive global graph among all items and capture global collaborative information with it in a self-supervised fashion, whose computational burden can be further alleviated by the proposed SVD-based accelerator. Furthermore, based on the graph, we propose to extract and utilize personalized item correlations in the form of relative positional encoding, which is a highly compatible manner of personalizing the utilization of global collaborative information. Finally, the entire framework is optimized in a multi-task learning paradigm, thus each part of APGL4SR can be mutually reinforced. As a generic framework, APGL4SR can outperform other baselines with significant margins. 可以在https://github.com/Graph-Team/APGL4SR上下载代码。”
</details></li>
</ul>
<hr>
<h2 id="On-the-Intersection-of-Self-Correction-and-Trust-in-Language-Models"><a href="#On-the-Intersection-of-Self-Correction-and-Trust-in-Language-Models" class="headerlink" title="On the Intersection of Self-Correction and Trust in Language Models"></a>On the Intersection of Self-Correction and Trust in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02801">http://arxiv.org/abs/2311.02801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Satyapriya Krishna</li>
<li>for: 这个论文旨在检验自我更正机制是否能够提高语言模型的可靠性。</li>
<li>methods: 研究人员使用了自我更正机制来提高语言模型的可靠性，并对两个关键方面进行了实验：谏性和毒性。</li>
<li>results: 研究发现，自我更正可以提高语言模型的谏性和真实性，但这些改进的程度因任务的具体需求和自我更正的方式而异。此外，研究还发现了一些“自我犹豫”现象在自我更正过程中，这引入了一些新的挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex cognitive tasks. However, their complexity and lack of transparency have raised several trustworthiness concerns, including the propagation of misinformation and toxicity. Recent research has explored the self-correction capabilities of LLMs to enhance their performance. In this work, we investigate whether these self-correction capabilities can be harnessed to improve the trustworthiness of LLMs. We conduct experiments focusing on two key aspects of trustworthiness: truthfulness and toxicity. Our findings reveal that self-correction can lead to improvements in toxicity and truthfulness, but the extent of these improvements varies depending on the specific aspect of trustworthiness and the nature of the task. Interestingly, our study also uncovers instances of "self-doubt" in LLMs during the self-correction process, introducing a new set of challenges that need to be addressed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/cs.LG_2023_11_06/" data-id="cloq1wl8z00st7o887swc6fyc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/eess.IV_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T09:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/eess.IV_2023_11_06/">eess.IV - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Auto-ICell-An-Accessible-and-Cost-Effective-Integrative-Droplet-Microfluidic-System-for-Real-Time-Single-Cell-Morphological-and-Apoptotic-Analysis"><a href="#Auto-ICell-An-Accessible-and-Cost-Effective-Integrative-Droplet-Microfluidic-System-for-Real-Time-Single-Cell-Morphological-and-Apoptotic-Analysis" class="headerlink" title="Auto-ICell: An Accessible and Cost-Effective Integrative Droplet Microfluidic System for Real-Time Single-Cell Morphological and Apoptotic Analysis"></a>Auto-ICell: An Accessible and Cost-Effective Integrative Droplet Microfluidic System for Real-Time Single-Cell Morphological and Apoptotic Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02927">http://arxiv.org/abs/2311.02927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Meiai Lin, Shanhang Luo, Syed Muhammad Tariq Abbasi, Liwei Tan, Guangyao Cheng, Bijie Bai, Yi-Ping Ho, Scott Wu Yuan, Ho-Pui Ho</li>
<li>for: 该研究用于开发一种新的、可靠、高效、容易操作、并且cost-effective的Integrated droplet microfluidic系统，用于实时分析单个细胞形态和 apoptosis。</li>
<li>methods: 该系统使用了3D打印的微流体chips，并与图像分析算法集成，以生成固定的液滴受试器和实时图像分析。系统使用了基于色的图像分析算法，在亮场下分析液滴内容。而在荧光场下，通过组合深度学习Enabled多色通道分析和live&#x2F;dead细胞染色kit，实时测量细胞 apoptosis。</li>
<li>results: 研究发现，该系统可以高速生成固定大小的液滴（70μm-240μm），并在1500液滴&#x2F;分钟的高通过put中进行实时图像分析。实时图像分析结果在2秒钟内显示在自定义图形用户界面（GUI）上。系统可以自动计算液滴内容的分布和比例，以及细胞膜变形和细胞圆形的观察和量化。<details>
<summary>Abstract</summary>
The Auto-ICell system, a novel, and cost-effective integrated droplet microfluidic system, is introduced for real-time analysis of single-cell morphology and apoptosis. This system integrates a 3D-printed microfluidic chip with image analysis algorithms, enabling the generation of uniform droplet reactors and immediate image analysis. The system employs a color-based image analysis algorithm in the bright field for droplet content analysis. Meanwhile, in the fluorescence field, cell apoptosis is quantitatively measured through a combination of deep-learning-enabled multiple fluorescent channel analysis and a live/dead cell stain kit. Breast cancer cells are encapsulated within uniform droplets, with diameters ranging from 70 {\mu}m to 240 {\mu}m, generated at a high throughput of 1,500 droplets per minute. Real-time image analysis results are displayed within 2 seconds on a custom graphical user interface (GUI). The system provides an automatic calculation of the distribution and ratio of encapsulated dyes in the bright field, and in the fluorescent field, cell blebbing and cell circularity are observed and quantified respectively. The Auto-ICell system is non-invasive and provides online detection, offering a robust, time-efficient, user-friendly, and cost-effective solution for single-cell analysis. It significantly enhances the detection throughput of droplet single-cell analysis by reducing setup costs and improving operational performance. This study highlights the potential of the Auto-ICell system in advancing biological research and personalized disease treatment, with promising applications in cell culture, biochemical microreactors, drug carriers, cell-based assays, synthetic biology, and point-of-care diagnostics.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:Auto-ICell 系统，一种新型、成本效果的组合式液体微流动系统，为实时单元细胞形态和 apoptosis 分析提供了一个可靠、高效、易用、cost-effective 解决方案。该系统结合了3D 印刷的微流动器和图像分析算法，可以生成具有固定尺寸的液体块 reactors，并实现实时图像分析。系统使用了 bright field 中的颜色基于的图像分析算法来分析液体块内容，并通过融合深度学习Enabled Multiple fluorescent channel 分析和live/dead cell 染色剂盒来量化细胞 apoptosis。在 fluorescence 频谱中，系统可以观察和量化细胞 blebbing 和细胞圆形。 Breast cancer 细胞被封装在具有70 μm 到 240 μm 的固定尺寸液体块中，每分钟可生成1,500个液体块。实时图像分析结果在2秒钟内显示在自定义图形用户界面（GUI）上。系统可以自动计算液体块内容中的分布和比例，并在 fluorescence 频谱中观察和量化细胞 blebbing 和细胞圆形。Auto-ICell 系统不侵入式，提供在线检测，为单元细胞分析提供了一个可靠、高效、易用、cost-effective 解决方案。它在液体单元细胞分析的检测throughput中减少了设备成本和改进了操作性能，这使得该系统在生物研究和个性化疾病治疗中具有潜在的潜在应用。
</details></li>
</ul>
<hr>
<h2 id="An-invariant-feature-extraction-for-multi-modal-images-matching"><a href="#An-invariant-feature-extraction-for-multi-modal-images-matching" class="headerlink" title="An invariant feature extraction for multi-modal images matching"></a>An invariant feature extraction for multi-modal images matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02842">http://arxiv.org/abs/2311.02842</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenzhong Gao, Wei Li</li>
<li>for: 本文提出了一种多模式图像不变性特征提取和匹配算法，用于多源数据分析。</li>
<li>methods: 本算法基于多模式图像的差异和相关性，采用了phas Congruency（PC）和Shi-Tomasi特征点检测、LogGabor滤波器和Weighted Partial Main Orientation Map（WPMOM）特征提取，以及多尺度处理来处理尺度差和优化匹配结果。</li>
<li>results: 实验结果表明，该算法在实际数据上具有良好的空间对齐精度和准确性，表现出了实际应用价值和良好的泛化性。<details>
<summary>Abstract</summary>
This paper aims at providing an effective multi-modal images invariant feature extraction and matching algorithm for the application of multi-source data analysis. Focusing on the differences and correlation of multi-modal images, a feature-based matching algorithm is implemented. The key technologies include phase congruency (PC) and Shi-Tomasi feature point for keypoints detection, LogGabor filter and a weighted partial main orientation map (WPMOM) for feature extraction, and a multi-scale process to deal with scale differences and optimize matching results. The experimental results on practical data from multiple sources prove that the algorithm has effective performances on multi-modal images, which achieves accurate spatial alignment, showing practical application value and good generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/eess.IV_2023_11_06/" data-id="cloq1wlfm01927o88fp0j2id8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/06/eess.SP_2023_11_06/" class="article-date">
  <time datetime="2023-11-06T08:00:00.000Z" itemprop="datePublished">2023-11-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/06/eess.SP_2023_11_06/">eess.SP - 2023-11-06</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Joint-Sparse-Estimation-with-Cardinality-Constraint-via-Mixed-Integer-Semidefinite-Programming"><a href="#Joint-Sparse-Estimation-with-Cardinality-Constraint-via-Mixed-Integer-Semidefinite-Programming" class="headerlink" title="Joint Sparse Estimation with Cardinality Constraint via Mixed-Integer Semidefinite Programming"></a>Joint Sparse Estimation with Cardinality Constraint via Mixed-Integer Semidefinite Programming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03501">http://arxiv.org/abs/2311.03501</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Liu, Frederic Matter, Alexander Sorg, Marc E. Pfetsch, Martin Haardt, Marius Pesavento</li>
<li>for: 这篇论文主要针对的是多个实现的混合物的纤维化表示问题，即从多个实现中 joint 估计一个矩阵表示，该矩阵表示由一个知道的字典表示。</li>
<li>methods: 该论文使用的方法包括MAP估计和混合Integer Semidefinite Programming（MISDP） reformulation。</li>
<li>results: 该论文的实验表明，与多个流行的DOA估计方法相比，该方法在计算时间和估计性能之间取得了平衡，并且可以在很大的维度下提供Global Optimum。<details>
<summary>Abstract</summary>
The multiple measurement vectors (MMV) problem refers to the joint estimation of a row-sparse signal matrix from multiple realizations of mixtures with a known dictionary. As a generalization of the standard sparse representation problem for a single measurement, this problem is fundamental in various applications in signal processing, e.g., spectral analysis and direction-of-arrival (DOA) estimation. In this paper, we consider the maximum a posteriori (MAP) estimation for the MMV problem, which is classically formulated as a regularized least-squares (LS) problem with an $\ell_{2,0}$-norm constraint, and derive an equivalent mixed-integer semidefinite program (MISDP) reformulation. The proposed MISDP reformulation can be exactly solved by a generic MISDP solver, which, however, becomes computationally demanding for problems of extremely large dimensions. To further reduce the computation time in such scenarios, a relaxation-based approach can be employed to obtain an approximate solution of the MISDP reformulation, at the expense of a reduced estimation performance. Numerical simulations in the context of DOA estimation demonstrate the improved error performance of our proposed method in comparison to several popular DOA estimation methods. In particular, compared to the deterministic maximum likelihood (DML) estimator, which is often used as a benchmark, the proposed method applied with a state-of-the-art MISDP solver exhibits a superior estimation performance at a significantly reduced running time. Moreover, unlike other nonconvex approaches for the MMV problem, including the greedy methods and the sparse Bayesian learning, the proposed MISDP-based method offers a guarantee of finding a global optimum.
</details>
<details>
<summary>摘要</summary>
多量测量 вектор（MMV）问题是指将多个实现的混合物中的纵向稀疏信号矩阵进行共同估计，其是标准稀疏表示问题的推广。在这篇文章中，我们考虑了最大 posteriori（MAP）估计方法，它是通过添加一个 $\ell_{2,0}$  norm约束来 reformulate 为一个正则化最小二乘（LS）问题。我们还提出了一种等价的混合整数半definite програм（MISDP）重写方法，可以使用一个通用的 MISDP 解决方案来准确地解决这个问题。然而，在极大维度的问题中，这种方法会变得 computationally 成本很高。为了降低计算时间，我们可以采用一种缓和方法来获得一个approximate 解决方案，但是这将导致估计性能的降低。在 DOA 估计上，我们的提议方法在比较多种流行的 DOA 估计方法时表现出较好的错误性能。特别是，与 deterministic maximum likelihood（DML）估计器相比，我们的方法在使用一个当前的 MISDP 解决方案时表现出较好的估计性能，并且运行时间远远 shorter。此外，与其他非CONvex 方法不同，我们的方法提供了一个全球最优解的保证。
</details></li>
</ul>
<hr>
<h2 id="Resource-Allocation-for-RIS-Empowered-Wireless-Communications-Low-Complexity-and-Robust-Designs"><a href="#Resource-Allocation-for-RIS-Empowered-Wireless-Communications-Low-Complexity-and-Robust-Designs" class="headerlink" title="Resource Allocation for RIS-Empowered Wireless Communications: Low-Complexity and Robust Designs"></a>Resource Allocation for RIS-Empowered Wireless Communications: Low-Complexity and Robust Designs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03282">http://arxiv.org/abs/2311.03282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Zeng, Wanming Hao, Zhangjie Peng, Zheng Chu, Xingwang Li, Changsheng You, Cunhua Pan</li>
<li>for: 这篇论文关注了利用可Programmable intelligent surface (RIS)的系统资源分配技术的进展，主要目标是实现低复杂性和可靠性。</li>
<li>methods: 论文研究了采用低复杂性方法来解决RIS系统中的问题，并提供了各种数字实验结果来说明这些方法的工作原理。</li>
<li>results: 论文通过数字实验和分析，证明了采用低复杂性和Robust resource allocation策略可以提高RIS系统的性能和可靠性。<details>
<summary>Abstract</summary>
This article delves into advancements in resource allocation techniques tailored for systems utilizing reconfigurable intelligent surfaces (RIS), with a primary focus on achieving low-complexity and resilient solutions. The investigation of low-complexity approaches for RIS holds significant relevance, primarily owing to the intricate characteristics inherent in RIS-based systems and the need of deploying large-scale RIS arrays. Concurrently, the exploration of robust solutions aims to address the issue of hardware impairments occurring at both the transceivers and RIS components in practical RIS-assisted systems. In the realm of both low-complexity and robust resource allocation, this article not only elucidates the fundamental techniques underpinning these methodologies but also offers comprehensive numerical results for illustrative purposes. The necessity of adopting resource allocation strategies that are both low in complexity and resilient is thoroughly established. Ultimately, this article provides prospective research avenues in the domain of low-complexity and robust resource allocation techniques tailored for RIS-assisted systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multivariate-selfsimilarity-Multiscale-eigen-structures-for-selfsimilarity-parameter-estimation"><a href="#Multivariate-selfsimilarity-Multiscale-eigen-structures-for-selfsimilarity-parameter-estimation" class="headerlink" title="Multivariate selfsimilarity: Multiscale eigen-structures for selfsimilarity parameter estimation"></a>Multivariate selfsimilarity: Multiscale eigen-structures for selfsimilarity parameter estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03247">http://arxiv.org/abs/2311.03247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Charles-Gérard Lucas, Gustavo Didier, Herwig Wendt, Patrice Abry</li>
<li>for: 模型多变量自相似性 dynamics，用于实际数据分析</li>
<li>methods: 基于卷积spectrum的多个自similarity参数估计方法</li>
<li>results: 提供一个可操作的 signal processing estimation工具箱，可以应用于实际数据，并且在多通道EEG数据中预测癫痫病发Here’s the same information in Simplified Chinese text:</li>
<li>for: 模型多变量自相似性 dynamics，用于实际数据分析</li>
<li>methods: 基于卷积spectrum的多个自similarity参数估计方法</li>
<li>results: 提供一个可操作的 signal processing estimation工具箱，可以应用于实际数据，并且在多通道EEG数据中预测癫痫病发<details>
<summary>Abstract</summary>
Scale-free dynamics, formalized by selfsimilarity, provides a versatile paradigm massively and ubiquitously used to model temporal dynamics in real-world data. However, its practical use has mostly remained univariate so far. By contrast, modern applications often demand multivariate data analysis. Accordingly, models for multivariate selfsimilarity were recently proposed. Nevertheless, they have remained rarely used in practice because of a lack of available robust estimation procedures for the vector of selfsimilarity parameters. Building upon recent mathematical developments, the present work puts forth an efficient estimation procedure based on the theoretical study of the multiscale eigenstructure of the wavelet spectrum of multivariate selfsimilar processes. The estimation performance is studied theoretically in the asymptotic limits of large scale and sample sizes, and computationally for finite-size samples. As a practical outcome, a fully operational and documented multivariate signal processing estimation toolbox is made freely available and is ready for practical use on real-world data. Its potential benefits are illustrated in epileptic seizure prediction from multi-channel EEG data.
</details>
<details>
<summary>摘要</summary>
Scale-free 动态，通过自similarity 理论化，提供了一种广泛和 ubique 应用于实际数据中的模型。然而，其实际应用 mostly 局限于单变量分析。然而，现代应用 frequently 需要多变量数据分析。因此，对多变量自similarity 的模型被提议。然而，它们在实践中很少使用，因为缺乏可靠的自similarity 参数的估计方法。本工作基于最近的数学发展，提出了一种高效的估计方法，基于多个扩散的卷积spectrum 的各自similarity 参数的理论研究。该估计方法的性能被理论上分析了在大规模和样本大小的极限下，以及计算上对finite-size samples的性能。作为实践的结果，一个完整的操作和文档的多ivariate signal processing 估计工具箱被开发出来，并ready for practical use on real-world data。其潜在的优点被 ilustrated 在多通道 EEG 数据中预测 epileptic seizure。
</details></li>
</ul>
<hr>
<h2 id="Using-Shallow-Neural-Networks-with-Functional-Connectivity-from-EEG-signals-for-Early-Diagnosis-of-Alzheimer’s-and-Frontotemporal-Dementia"><a href="#Using-Shallow-Neural-Networks-with-Functional-Connectivity-from-EEG-signals-for-Early-Diagnosis-of-Alzheimer’s-and-Frontotemporal-Dementia" class="headerlink" title="Using Shallow Neural Networks with Functional Connectivity from EEG signals for Early Diagnosis of Alzheimer’s and Frontotemporal Dementia"></a>Using Shallow Neural Networks with Functional Connectivity from EEG signals for Early Diagnosis of Alzheimer’s and Frontotemporal Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03151">http://arxiv.org/abs/2311.03151</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaineb Ajra, Binbin Xu, Gérard Dray, Jacky Montmain, Stéphane Perrey</li>
<li>for: The paper aims to develop an early diagnosis tool for dementia using shallow neural networks and EEG signals.</li>
<li>methods: The paper uses two sets of features: spectral-temporal and functional connectivity, and compares three supervised machine learning techniques and CNN models to classify EEG signals of AD&#x2F;FTD and control cases.</li>
<li>results: The shallow CNN-based models achieved the highest accuracy of 94.54% with AEC in the test dataset, outperforming conventional methods and providing a potentially additional early dementia diagnosis tool.Here are the three key points in Simplified Chinese:</li>
<li>for: 这篇论文目的是开发一种用深度神经网络和EEG信号进行早期诊断方法。</li>
<li>methods: 这篇论文使用了两个集合特征：spectral-temporal和功能连接，并比较了三种超vised机器学习方法和CNN模型来分类AD&#x2F;FTD和控制例子的EEG信号。</li>
<li>results:  shallow CNN-based模型在测试集上达到了94.54%的最高准确率，超过了传统方法，并提供了一个可能的早期诊断工具。<details>
<summary>Abstract</summary>
{Introduction: } Dementia is a neurological disorder associated with aging that can cause a loss of cognitive functions, impacting daily life. Alzheimer's disease (AD) is the most common cause of dementia, accounting for 50--70\% of cases, while frontotemporal dementia (FTD) affects social skills and personality. Electroencephalography (EEG) provides an effective tool to study the effects of AD on the brain. {Methods: } In this study, we propose to use shallow neural networks applied to two sets of features: spectral-temporal and functional connectivity using four methods. We compare three supervised machine learning techniques to the CNN models to classify EEG signals of AD / FTD and control cases. We also evaluate different measures of functional connectivity from common EEG frequency bands considering multiple thresholds. {Results and Discussion: } Results showed that the shallow CNN-based models achieved the highest accuracy of 94.54\% with AEC in test dataset when considering all connections, outperforming conventional methods and providing potentially an additional early dementia diagnosis tool. \url{https://doi.org/10.3389%2Ffneur.2023.1270405}
</details>
<details>
<summary>摘要</summary>
{Methods: } 在本研究中，我们提出使用浅层神经网络，应用于两个集合特征：spectral-temporal和功能连接性。我们使用四种方法来分类EEG信号，包括三种超视觉学习技术和CNN模型。我们还评估了不同的功能连接度度量，来评估不同频谱带的连接性。{Results and Discussion: } 结果显示，使用浅层CNN-based模型可以达到94.54%的准确率，在测试集上。这些模型在考虑所有连接时，超过了传统方法，并提供了可能的衰退诊断工具。参考链接：<https://doi.org/10.3389/fneur.2023.1270405>
</details></li>
</ul>
<hr>
<h2 id="Energy-Harvesting-Maximization-for-Reconfigurable-Intelligent-Surfaces-Using-Amplitude-Measurements"><a href="#Energy-Harvesting-Maximization-for-Reconfigurable-Intelligent-Surfaces-Using-Amplitude-Measurements" class="headerlink" title="Energy Harvesting Maximization for Reconfigurable Intelligent Surfaces Using Amplitude Measurements"></a>Energy Harvesting Maximization for Reconfigurable Intelligent Surfaces Using Amplitude Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03143">http://arxiv.org/abs/2311.03143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Tavana, Meysam Masoudi, Emil Björnson</li>
<li>for: 本研究旨在探讨无法协调的附近RF源的能量收集问题，以实现智能表面自我维护操作。</li>
<li>methods: 提出了一系列的顺序相对位置算法，以最大化接收功率基于仅功率测量。</li>
<li>results: 对比Random phase update方法，我们的算法在征服后的实现功率方面具有更高的性能，并且需要 fewer measurements per phase update。在干扰无的情况下，我们的算法可以准确地找到最佳测量相对位置。<details>
<summary>Abstract</summary>
Energy harvesting can enable a reconfigurable intelligent surface (RIS) to self-sustain its operations without relying on external power sources. In this paper, we consider the problem of energy harvesting for RISs in the absence of coordination with the ambient RF source. We propose a series of sequential phase-alignment algorithms that maximize the received power based on only power measurements. We prove the convergence of the proposed algorithm to the optimal value for the noiseless scenario. However, for the noisy scenario, we propose a linear least squares estimator. We prove that within the class of linear estimators, the optimal set of measurement phases are equally-spaced phases. To evaluate the performance of the proposed method, we introduce a random phase update algorithm as a benchmark. Our simulation results show that the proposed algorithms outperform the random phase update method in terms of achieved power after convergence while requiring fewer measurements per phase update. Using simulations, we show that in a noiseless scenario with a discrete set of possible phase shifts for the RIS elements, the proposed method is sub-optimal, achieving a higher value than the random algorithm but not exactly the maximum feasible value that we obtained by exhaustive search.
</details>
<details>
<summary>摘要</summary>
能量受取可以使智能表面自我维护其运行，不需要外部电源。在这篇论文中，我们考虑了智能表面上的能量受取问题，在缺乏干扰的情况下。我们提出了一系列的顺序相对级algorithm，以最大化接收到的功率基于仅仅是功率测量。我们证明了这些算法的 converges 到最优值的情况。然而，在噪音拥有的情况下，我们提出了一个线性最小二乘估计器。我们证明了在线性估计器中的优化集合是均匀分布的相位。为评估提案的性能，我们引入了随机相位更新算法作为 referential。我们的实验结果表明，提案的算法在相对于随机相位更新方法的情况下具有更高的实际功率，并且需要更少的测量每次相位更新。使用实验，我们显示在干扰无的情况下，智能表面上的元素可能存在一组离散的可能相位shift，而我们的方法是不优于随机算法，但是不是最大可能的值。
</details></li>
</ul>
<hr>
<h2 id="Antenna-Positioning-and-Beamforming-Design-for-Movable-Antenna-Enabled-Multi-user-Downlink-Communications"><a href="#Antenna-Positioning-and-Beamforming-Design-for-Movable-Antenna-Enabled-Multi-user-Downlink-Communications" class="headerlink" title="Antenna Positioning and Beamforming Design for Movable-Antenna Enabled Multi-user Downlink Communications"></a>Antenna Positioning and Beamforming Design for Movable-Antenna Enabled Multi-user Downlink Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03046">http://arxiv.org/abs/2311.03046</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Qin, Wen Chen, Zhendong Li, Qingqing Wu, Nan Cheng, Fangjiong Chen</li>
<li>for: 这篇论文研究了一个多输入单输出（MISO）下链通信系统，在该系统中用户装备了可移动天线（MA）。</li>
<li>methods: 作者采用了场响应基于通道模型来Characterize下链通道，并通过对MA的位置和扫描矩阵进行共同优化来减少总发射功率。</li>
<li>results: numerical results表明，MA-enabled通信系统比传统固定天线系统perform better。<details>
<summary>Abstract</summary>
This paper investigates a multiple input single output (MISO) downlink communication system in which users are equipped with movable antennas (MAs). First, We adopt a field-response based channel model to characterize the downlink channel with respect to MAs' positions. Then, we aim to minimize the total transmit power by jointly optimizing the MAs' positions and beamforming matrix. To solve the resulting non-convex problem, we employ an alternating optimization (AO) algorithm based on penalty method and successive convex approximation (SCA) to obtain a sub-optimal solution. Numerical results demonstrate that the MA-enabled communication system perform better than conventional fixed position antennas.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "field-response based channel model" is translated as "频响频谱模型" (freq-response frequency-domain model)* "minimize the total transmit power" is translated as "最小化总传输功率" (minimize the total transmission power)* "beamforming matrix" is translated as "扩扫矩阵" (beamforming matrix)* "alternating optimization algorithm" is translated as "交互优化算法" (alternating optimization algorithm)* "penalty method" is translated as "罚方法" (penalty method)* "successive convex approximation" is translated as "顺序凸 Approximation" (successive convex approximation)* "sub-optimal solution" is translated as "优化解决方案" (sub-optimal solution)Please note that the translation is in Simplified Chinese, and the word order may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Optimization-of-RIS-Placement-for-Satellite-to-Ground-Coverage-Enhancement"><a href="#Optimization-of-RIS-Placement-for-Satellite-to-Ground-Coverage-Enhancement" class="headerlink" title="Optimization of RIS Placement for Satellite-to-Ground Coverage Enhancement"></a>Optimization of RIS Placement for Satellite-to-Ground Coverage Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02958">http://arxiv.org/abs/2311.02958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingchen Liu, Liuxun Xue, Shu Sun, Meixia Tao</li>
<li>for: 提高卫星到地面通信的可靠性和效率，使用可配置智能表面（RIS）技术。</li>
<li>methods: 提出了一种基于建筑物质位置的RIS布局优化方法，通过模拟卫星到地面通信，考虑实际的建筑物和地面用户的位置。</li>
<li>results: 通过大规模RIS部署，可以提高卫星到地面通信的覆盖率，并且可以适应不同的建筑物分布，如乡村、小镇和城市。<details>
<summary>Abstract</summary>
In satellite-to-ground communication, ensuring reliable and efficient connectivity poses significant challenges. The reconfigurable intelligent surface (RIS) offers a promising solution due to its ability to manipulate wireless propagation environments and thus enhance communication performance. In this paper, we propose a method for optimizing the placement of RISs on building facets to improve satellite-to-ground communication coverage. We model satellite-to-ground communication with RIS assistance, considering the actual positions of buildings and ground users. The theoretical lower bound on the coverage enhancement in satellite-to-ground communication through large-scale RIS deployment is derived. Then a novel optimization framework for RIS placement is formulated, and a parallel genetic algorithm is employed to solve the problem. Simulation results demonstrate the superior performance of the proposed RIS deployment strategy in enhancing satellite communication coverage probability for non-line-of-sight users. The proposed framework can be applied to various architectural distributions, such as rural areas, towns, and cities, by adjusting parameter settings.
</details>
<details>
<summary>摘要</summary>
卫星到地面通信中确保可靠和高效连接具有 significatif挑战。可配置智能表面（RIS）提供了一个有 promise的解决方案，因为它可以控制无线媒体环境，从而提高通信性能。在本文中，我们提议了一种使用RIS的位置布置优化方法，以提高卫星到地面通信覆盖率。我们根据实际建筑物的位置和地面用户的实际位置模型了卫星到地面通信，并 deriv了无线传输中的最低下界。然后，我们提出了一种新的优化框架，并使用并行遗传算法解决问题。实验结果表明，提议的RIS布局策略可以增加非直线视野用户的卫星通信覆盖率。该提议的框架可以应用于不同的建筑分布，如农村、小镇和城市，只需调整参数设置即可。
</details></li>
</ul>
<hr>
<h2 id="Channel-Estimation-and-Training-Design-for-Active-RIS-Aided-Wireless-Communications"><a href="#Channel-Estimation-and-Training-Design-for-Active-RIS-Aided-Wireless-Communications" class="headerlink" title="Channel Estimation and Training Design for Active RIS Aided Wireless Communications"></a>Channel Estimation and Training Design for Active RIS Aided Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02935">http://arxiv.org/abs/2311.02935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Nanxi Li, Ruizhe Long, Ying-Chang Liang</li>
<li>for: 提高无线通信的精度，使用活动重配置智能表面技术进行通道估计</li>
<li>methods: 利用Signal增强功能，提出基于LS估计器的通道估计方法，并对ARIS反射模式进行优化</li>
<li>results: 通过实验结果表明，提议的设计可以在ARIS噪声的情况下实现精度的通道估计<details>
<summary>Abstract</summary>
Active reconfigurable intelligent surface (ARIS) is a newly emerging RIS technique that leverages radio frequency (RF) reflection amplifiers to empower phase-configurable reflection elements (REs) in amplifying the incident signal. Thereby, ARIS can enhance wireless communications with the strengthened ARIS-aided links. In this letter, we propose exploiting the signal amplification capability of ARIS for channel estimation, aiming to improve the estimation precision. Nevertheless, the signal amplification inevitably introduces the thermal noise at the ARIS, which can hinder the acquisition of accurate channel state information (CSI) with conventional channel estimation methods based on passive RIS (PRIS). To address this issue, we further investigate this ARIS-specific channel estimation problem and propose a least-square (LS) based channel estimator, whose performance can be further improved with the design on ARIS reflection patterns at the channel training phase. Based on the proposed LS channel estimator, we optimize the training reflection patterns to minimize the channel estimation error variance. Extensive simulation results show that our proposed design can achieve accurate channel estimation in the presence of the ARIS noises.
</details>
<details>
<summary>摘要</summary>
活动可重新配置智能表面（ARIS）是一种最近出现的RIS技术，它利用电磁波频率（RF）反射增强器来强化配置可变阶段（RE）的反射元件，从而提高无线通信的覆盖范围和功率。在这封信中，我们建议利用ARIS的增强功能来提高通信频率的扩展。然而，增强信号必然会在ARIS中增加热噪声，这可能会使用传统的RIS（PRIS）基于的渠道估计方法获得不准确的通道状态信息（CSI）。为了解决这个问题，我们进一步研究ARIS特有的渠道估计问题，并提出一种基于最小二乘（LS）的渠道估计器，其性能可以通过在通道训练阶段设计ARIS反射模式来进一步提高。根据我们的提议的LS渠道估计器，我们优化了训练反射模式，以最小化渠道估计错误偏差的变化。实验结果表明，我们的设计可以在ARIS噪声的存在下实现准确的渠道估计。
</details></li>
</ul>
<hr>
<h2 id="Pilot-Design-and-Signal-Detection-for-Symbiotic-Radio-over-OFDM-Carriers"><a href="#Pilot-Design-and-Signal-Detection-for-Symbiotic-Radio-over-OFDM-Carriers" class="headerlink" title="Pilot Design and Signal Detection for Symbiotic Radio over OFDM Carriers"></a>Pilot Design and Signal Detection for Symbiotic Radio over OFDM Carriers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02928">http://arxiv.org/abs/2311.02928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Qianqian Zhang, Ruizhe Long, Yiyang Pei, Ying-Chang Liang</li>
<li>for: 本文针对symbiotic radio（SR）的对话设计和信号探测，当主传输使用多元化频率分复（OFDM）时。</li>
<li>methods: 本文使用了 comb-type 对话结构和 preamble 对话结构，以保持主传输和次传输的频道正交性。</li>
<li>results:  simulations 表明，这种对话设计可以提高主传输的性能，并且无需直接链接，主和次传输可以通过仅仅的回�atter声链接。此外，本文还分析了对话的多标度性和对 Symbol 同步误差的敏感性。<details>
<summary>Abstract</summary>
Symbiotic radio (SR) is a promising solution to achieve high spectrum- and energy-efficiency due to its spectrum sharing and low-power consumption properties, in which the secondary system achieves data transmissions by backscattering the signal originating from the primary system. In this paper, we are interested in the pilot design and signal detection when the primary transmission adopts orthogonal frequency division multiplexing (OFDM). In particular, to preserve the channel orthogonality among the OFDM sub-carriers, each secondary symbol is designed to span an entire OFDM symbol. The comb-type pilot structure is employed by the primary transmission, while the preamble pilot structure is used by the secondary transmission. With the designed pilot structures, the primary signal can be detected via the conventional methods by treating the secondary signal as a part of the composite channel, i.e., the effective channel of the primary transmission. Furthermore, the secondary signal can be extracted from the estimated composite channel with the help of the detected primary signal. The bit error rate (BER) performance with both perfect and estimated CSI, the diversity orders of the primary and secondary transmissions, and the sensitivity to symbol synchronization error are analyzed. Simulation results show that the performance of the primary transmission is enhanced thanks to the backscatter link established by the secondary transmission. More importantly, even without the direct link, the primary and secondary transmissions can be supported via only the backscatter link.
</details>
<details>
<summary>摘要</summary>
共生射频（SR）是一种具有spectrum sharing和低功率特性的解决方案，其中次级系统通过反射讯号来实现数据传输。在这篇论文中，我们专注于副数据设计和讯号探测，当主传输采用了多个普通频分多xes（OFDM）时。为保持传输频道的正交性，每个次级symbol span整个OFDM符号。主传输使用了comb-type的副数据结构，而次级传输使用了preamble副数据结构。这些副数据结构可以让主传输通过传统方法探测 secondary signal，并且可以将次级讯号从估计的主传输频道中提取出来。我们分析了对两个不同的测试环境（即对实际CSI和估计CSI）的对应BER性能、次级和主传输的多标度性、和symbol同步误差的敏感性。 simulation results show that the primary transmission performance is improved due to the backscatter link established by the secondary transmission. Furthermore, even without the direct link, the primary and secondary transmissions can be supported via only the backscatter link.
</details></li>
</ul>
<hr>
<h2 id="Goal-Oriented-Wireless-Communication-Resource-Allocation-for-Cyber-Physical-Systems"><a href="#Goal-Oriented-Wireless-Communication-Resource-Allocation-for-Cyber-Physical-Systems" class="headerlink" title="Goal-Oriented Wireless Communication Resource Allocation for Cyber-Physical Systems"></a>Goal-Oriented Wireless Communication Resource Allocation for Cyber-Physical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02911">http://arxiv.org/abs/2311.02911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Feng, Kedi Zheng, Yi Wang, Kaibin Huang, Qixin Chen<br>for: 这个研究旨在提高嵌入式系统的性能，满足智能网络和车辆网络等新的工业应用的需求。methods: 本研究使用目标式无线通信资源分配框架，考虑到嵌入式系统运行目标的数据具体和重要性。results: 本研究提出了一个分解和聚合的分配算法，可以实现最大化嵌入式系统运行目标的信息价值增加。此外，这个算法可以应用于各种嵌入式系统应用，如数据驱动决策、边缘学习、联合学习和分布式优化。<details>
<summary>Abstract</summary>
The proliferation of novel industrial applications at the wireless edge, such as smart grids and vehicle networks, demands the advancement of cyber-physical systems. The performance of CPSs is closely linked to the last-mile wireless communication networks, which often become bottlenecks due to their inherent limited resources. Current CPS operations often treat wireless communication networks as unpredictable and uncontrollable variables, ignoring the potential adaptability of wireless networks, which results in inefficient and overly conservative CPS operations. Meanwhile, current wireless communications often focus more on throughput and other transmission-related metrics instead of CPS goals. In this study, we introduce the framework of goal-oriented wireless communication resource allocations, accounting for the semantics and significance of data for CPS operation goals. This guarantees optimal CPS performance from a cybernetic standpoint. We formulate a bandwidth allocation problem aimed at maximizing the information utility gain of transmitted data brought to CPS operation goals. Since the goal-oriented bandwidth allocation problem is a large-scale combinational problem, we propose a divide-and-conquer and greedy solution algorithm. The information utility gain is first approximately decomposed into marginal utility information gains and computed in a parallel manner. Subsequently, the bandwidth allocation problem is reformulated as a knapsack problem, which can be further solved greedily with a guaranteed sub-optimality gap. We further demonstrate how our proposed goal-oriented bandwidth allocation algorithm can be applied in four potential CPS applications, including data-driven decision-making, edge learning, federated learning, and distributed optimization.
</details>
<details>
<summary>摘要</summary>
随着无线边缘应用的普遍化，如智能电网和车辆网络， cyber-physical systems（CPS）的进步成为必需。CPS的性能与 послед一英里无线通信网络密切相关，这些网络frequently become bottlenecks due to their inherent limited resources. current CPS operations often treat wireless communication networks as unpredictable and uncontrollable variables, ignoring the potential adaptability of wireless networks, which results in inefficient and overly conservative CPS operations. Meanwhile, current wireless communications often focus more on throughput and other transmission-related metrics instead of CPS goals.In this study, we introduce the framework of goal-oriented wireless communication resource allocations, accounting for the semantics and significance of data for CPS operation goals. This guarantees optimal CPS performance from a cybernetic standpoint. We formulate a bandwidth allocation problem aimed at maximizing the information utility gain of transmitted data brought to CPS operation goals. Since the goal-oriented bandwidth allocation problem is a large-scale combinational problem, we propose a divide-and-conquer and greedy solution algorithm. The information utility gain is first approximately decomposed into marginal utility information gains and computed in a parallel manner. Subsequently, the bandwidth allocation problem is reformulated as a knapsack problem, which can be further solved greedily with a guaranteed sub-optimality gap.We further demonstrate how our proposed goal-oriented bandwidth allocation algorithm can be applied in four potential CPS applications, including data-driven decision-making, edge learning, federated learning, and distributed optimization.
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-Multidimensional-Constellation-Based-on-Leech-Lattice-for-Visible-Light-Communications"><a href="#Energy-Efficient-Multidimensional-Constellation-Based-on-Leech-Lattice-for-Visible-Light-Communications" class="headerlink" title="Energy-Efficient Multidimensional Constellation Based on Leech Lattice for Visible Light Communications"></a>Energy-Efficient Multidimensional Constellation Based on Leech Lattice for Visible Light Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02865">http://arxiv.org/abs/2311.02865</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia-Ning Guo, Ru-Han Chen, Jian Zhang, Longguang Li, Jing Zhou</li>
<li>for: This paper is written for indoor visible light communications (VLCs) with peak- and average-intensity input constraints.</li>
<li>methods: The paper uses large deviation theory and Leech lattice to design an energy-efficient 24-dimensional constellation, and develops fast algorithms for constellation mapping and demodulation.</li>
<li>results: The paper achieves a significant coding gain and nearly-maximum shaping gain, and the proposed method outperforms existing methods according to numerical results.Here’s the simplified Chinese text:</li>
<li>for: 这篇论文是为indoor可见光通信（VLC）with peak-和average-intensity输入约束而写的。</li>
<li>methods: 这篇论文使用大偏移理论和Leech网来设计一个能效的24维度象限，并开发了快速的象限映射和解译算法。</li>
<li>results: 这篇论文实现了一个显著的编码增益和几乎最大的形态增益，并且提出的方法在数值结果中胜过现有方法。<details>
<summary>Abstract</summary>
In this paper, a 24-dimensional geometrically-shaped constellation design based on Leech lattice is presented for indoor visible light communications (VLCs) with a peak-and an average-intensity input constraints. Firstly, by leveraging tools from large deviation theory, we characterize second-order asymptotics of the optimal constellation shaping region under aforementioned intensity constraints, which further refine our previous results in [Chen. et. al, 2020]. Within the optimal geometrical shaping region, we develop an energy-efficient 24-dimensional constellation design, where a significant coding gain brought by the Leech lattice and the nearly-maximum shaping gain are incorporated by using a strategy called coarsely shaping and finely coding. Fast algorithms for constellation mapping and demodulation are presented as well. Numerical results verifies the superiority of our results as compared with existing methods.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于Leech网格的24维geometry射阵设计，用于室内可见光通信（VLC），并且受到峰值和平均输入强度约束。我们首先通过利用大偏移理论工具，对最佳射阵形状区域的第二阶偏移进行了Characterization，这进一步细化了我们在[Chen et al., 2020]中的前一个结果。在最佳 геометрическом射阵形状区域内，我们开发了一种能效的24维射阵设计，其中Leech网格和几乎最大的射阵约束带来了重要的编码增益。我们还提出了一种Strategy called "coarsely shaping and finely coding"，该策略可以在射阵形状和编码之间进行了最佳匹配。此外，我们还提供了快速的射阵映射和解译算法。数据结果表明，我们的结果与现有方法相比，具有明显的优势。
</details></li>
</ul>
<hr>
<h2 id="Multi-User-Multi-IoT-Device-Symbiotic-Radio-A-Novel-Massive-Access-Scheme-for-Cellular-IoT"><a href="#Multi-User-Multi-IoT-Device-Symbiotic-Radio-A-Novel-Massive-Access-Scheme-for-Cellular-IoT" class="headerlink" title="Multi-User Multi-IoT-Device Symbiotic Radio: A Novel Massive Access Scheme for Cellular IoT"></a>Multi-User Multi-IoT-Device Symbiotic Radio: A Novel Massive Access Scheme for Cellular IoT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02837">http://arxiv.org/abs/2311.02837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Wang, Ying-Chang Liang, Sumei Sun</li>
<li>for: 本文旨在提出一种多用户多iot设备的共生电台系统，以支持无线互联网络的各种应用。</li>
<li>methods: 作者采用了robust设计方法，以最小化电力消耗，同时满足无线传输停机概率和iot传输总速率的约束。</li>
<li>results: 作者通过 simulations  substantiated 共生电台系统的可行性，并证明了提议的扫描方法的有效性。  compare to CSI-based approach, DoA-based approach achieves comparable performance when ASs are small.<details>
<summary>Abstract</summary>
Symbiotic radio (SR) is a promising technique to support cellular Internet-of-Things (IoT) by forming a mutualistic relationship between IoT and cellular transmissions. In this paper, we propose a novel multi-user multi-IoT-device SR system to enable massive access in cellular IoT. In the considered system, the base station (BS) transmits information to multiple cellular users, and a number of IoT devices simultaneously backscatter their information to these users via the cellular signal. The cellular users jointly decode the information from the BS and IoT devices. Noting that the reflective links from the IoT devices can be regarded as the channel uncertainty of the direct links, we apply the robust design method to design the beamforming vectors at the BS. Specifically, the transmit power is minimized under the cellular transmission outage probability constraints and IoT transmission sum rate constraints. The algorithm based on semi-definite programming and difference-of-convex programming is proposed to solve the power minimization problem. Moreover, we consider a special case where each cellular user is associated with several adjacent IoT devices and propose a direction of arrival (DoA)-based transmit beamforming design approach. The DoA-based approach requires only the DoA and angular spread (AS) of the direct links instead of the instantaneous channel state information (CSI) of the reflective link channels, leading to a significant reduction in the channel feedback overhead. Simulation results have substantiated the multi-user multi-IoT-device SR system and the effectiveness of the proposed beamforming approaches. It is shown that the DoA-based beamforming approach achieves comparable performance as the CSI-based approach in the special case when the ASs are small.
</details>
<details>
<summary>摘要</summary>
Symbiotic radio (SR) 是一种有前途的技术，可以支持cellular Internet of Things (IoT)  by forming a mutualistic relationship between IoT and cellular transmissions. 在这篇论文中，我们提出了一种新的多用户多IoT设备 SR 系统，以实现大规模的 cellular IoT 访问。在考虑的系统中，基站 (BS) 向多个 cellular 用户传输信息，而多个 IoT 设备同时将信息反射到这些用户 via cellular 信号。用户集成 decode BS 和 IoT 设备之间的信息。注意到反射链的不确定性可以看作 direct 链的通道不确定性，我们使用 robust 设计方法来设计 BS 的扫描向量。特别是，在 cellular 传输失效概率的约束下， minimize 发射功率，同时满足 IoT 传输总Bit rate 的约束。我们提出了一种基于 semi-definite programming 和差分不确定程序的算法来解决发射功率最小化问题。此外，我们考虑了每个 cellular 用户与邻近的多个 IoT 设备之间的特殊情况，并提出了方向的来源 (DoA) 基于扫描方向设计方法。DoA 基于方法只需要 DoA 和方向扩散 (AS) 的直接链 instead of 反射链通道的实时渠道状态信息 (CSI)，从而减少了通道反馈开销。实验结果证明了多用户多IoT设备 SR 系统和我们提出的扫描方法的有效性。结果表明，在特殊情况下，当 AS 较小时，DoA 基于方法与 CSI 基于方法的性能相似。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/06/eess.SP_2023_11_06/" data-id="cloq1wlh701cx7o88acnk5tr2" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.SD_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T15:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.SD_2023_11_05/">cs.SD - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Yet-Another-Generative-Model-For-Room-Impulse-Response-Estimation"><a href="#Yet-Another-Generative-Model-For-Room-Impulse-Response-Estimation" class="headerlink" title="Yet Another Generative Model For Room Impulse Response Estimation"></a>Yet Another Generative Model For Room Impulse Response Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02581">http://arxiv.org/abs/2311.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Lee, Hyeong-Seok Choi, Kyogu Lee</li>
<li>for: 这 paper 的目的是提出一种新的 neural room impulse response (RIR) 估计器，以提高估计质量。</li>
<li>methods: 这 paper 使用了一种 alternate generator 架构，通过 residual quantization 学习一个精度的离散Token空间，并将 RIR 估计问题转化为一个 reference-conditioned autoregressive token generation 任务。</li>
<li>results: 实验结果表明，这 paper 的系统在多种评价指标上都有优于基eline。<details>
<summary>Abstract</summary>
Recent neural room impulse response (RIR) estimators typically comprise an encoder for reference audio analysis and a generator for RIR synthesis. Especially, it is the performance of the generator that directly influences the overall estimation quality. In this context, we explore an alternate generator architecture for improved performance. We first train an autoencoder with residual quantization to learn a discrete latent token space, where each token represents a small time-frequency patch of the RIR. Then, we cast the RIR estimation problem as a reference-conditioned autoregressive token generation task, employing transformer variants that operate across frequency, time, and quantization depth axes. This way, we address the standard blind estimation task and additional acoustic matching problem, which aims to find an RIR that matches the source signal to the target signal's reverberation characteristics. Experimental results show that our system is preferable to other baselines across various evaluation metrics.
</details>
<details>
<summary>摘要</summary>
现代神经room响应函数估计器通常包括一个编码器用于参考音频分析和一个生成器用于响应函数合成。特别是，生成器的性能直接影响总估计质量。在这个上下文中，我们探索了一种 alternate 生成器架构以提高性能。我们首先在 autoencoder 中使用循环量化来学习一个精度时间频谱空间，其中每个token表示一个小时频谱块的响应函数。然后，我们将响应函数估计问题转化为一个引用条件自适应字符串生成任务，使用 transformer 变体在频率、时间和量化深度轴上运行。这样，我们解决了标准盲目估计问题和附加的听音匹配问题，其目的是找到一个匹配源信号的响应函数。实验结果显示，我们的系统在多个评价指标上比其他基准高。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.SD_2023_11_05/" data-id="cloq1wlbe00yz7o88eipefjee" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.CV_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T13:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.CV_2023_11_05/">cs.CV - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="MirrorCalib-Utilizing-Human-Pose-Information-for-Mirror-based-Virtual-Camera-Calibration"><a href="#MirrorCalib-Utilizing-Human-Pose-Information-for-Mirror-based-Virtual-Camera-Calibration" class="headerlink" title="MirrorCalib: Utilizing Human Pose Information for Mirror-based Virtual Camera Calibration"></a>MirrorCalib: Utilizing Human Pose Information for Mirror-based Virtual Camera Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02791">http://arxiv.org/abs/2311.02791</a></li>
<li>repo_url: None</li>
<li>paper_authors: Longyun Liao, Andrew Mitchell, Rong Zheng</li>
<li>for: 估计虚拟摄像头的外部参数，即与实际摄像头相对的投影 mirror 的相对位姿。</li>
<li>methods: 利用人体知识和2D关节位置来估计摄像头外部参数，首先使用修改后的八点算法获取初始估计，然后根据人体 constraints 进行修正，最后使用 RANSAC 算法除异常点。</li>
<li>results: 在 synthetic 和实际数据集上进行测试，mirrorCalib 可以达到 rotation error 0.62{\deg}&#x2F;1.82{\deg} 和 translation error 37.33&#x2F;69.51 mm，超越当前最佳方法。<details>
<summary>Abstract</summary>
In this paper, we present the novel task of estimating the extrinsic parameters of a virtual camera with respect to a real camera with one single fixed planar mirror. This task poses a significant challenge in cases where objects captured lack overlapping views from both real and mirrored cameras. To address this issue, prior knowledge of a human body and 2D joint locations are utilized to estimate the camera extrinsic parameters when a person is in front of a mirror. We devise a modified eight-point algorithm to obtain an initial estimation from 2D joint locations. The 2D joint locations are then refined subject to human body constraints. Finally, a RANSAC algorithm is employed to remove outliers by comparing their epipolar distances to a predetermined threshold. MirrorCalib is evaluated on both synthetic and real datasets and achieves a rotation error of 0.62{\deg}/1.82{\deg} and a translation error of 37.33/69.51 mm on the synthetic/real dataset, which outperforms the state-of-art method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一个新的任务：使用真实摄像头和一个固定的平面镜来估算虚拟摄像头的外部参数。当物体被捕捉时，缺乏真实和镜头摄像头之间的重叠视图，这个任务具有显著的挑战。为解决这个问题，我们利用人体的先知知识和2D关节位置来估算摄像头的外部参数，当人物在镜子前时。我们修改了八点算法以获得初始估算，然后使用人体限制来精确估算。最后，我们使用RANSAC算法来移除异常值，比较它们的视角距离与先定的阈值。我们的 MirrorCalib 方法在Synthetic 和 Real 数据集上进行了评估，其中Synthetic 数据集的旋转错误为0.62°/1.82°，翻译错误为37.33/69.51 mm，超过了现有方法的性能。
</details></li>
</ul>
<hr>
<h2 id="MuSHRoom-Multi-Sensor-Hybrid-Room-Dataset-for-Joint-3D-Reconstruction-and-Novel-View-Synthesis"><a href="#MuSHRoom-Multi-Sensor-Hybrid-Room-Dataset-for-Joint-3D-Reconstruction-and-Novel-View-Synthesis" class="headerlink" title="MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis"></a>MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction and Novel View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02778">http://arxiv.org/abs/2311.02778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuqian Ren, Wenjia Wang, Dingding Cai, Tuuli Tuominen, Juho Kannala, Esa Rahtu</li>
<li>for: 这 paper 的目的是提高 Metaverse 技术中的准确、实时、 immerse 模型化，以满足非人类感知（如无人机&#x2F;机器人&#x2F;自动驾驶车 navigation）和 immerse 技术（如 AR&#x2F;VR）的需求。</li>
<li>methods: 这 paper 使用了多感器 гибрид房间数据集 (MuSHRoom)，并对其进行了多种著名的管道测试，以评估它们在实际应用中的性能。</li>
<li>results: 这 paper 提出了一种新的方法，可以在实时和 computationally efficient 的方式下，将3D reconstruction和高质量的rendering融合在一起。这种方法在 MuSHRoom 数据集上显示出了良好的性能。<details>
<summary>Abstract</summary>
Metaverse technologies demand accurate, real-time, and immersive modeling on consumer-grade hardware for both non-human perception (e.g., drone/robot/autonomous car navigation) and immersive technologies like AR/VR, requiring both structural accuracy and photorealism. However, there exists a knowledge gap in how to apply geometric reconstruction and photorealism modeling (novel view synthesis) in a unified framework.   To address this gap and promote the development of robust and immersive modeling and rendering with consumer-grade devices, first, we propose a real-world Multi-Sensor Hybrid Room Dataset (MuSHRoom). Our dataset presents exciting challenges and requires state-of-the-art methods to be cost-effective, robust to noisy data and devices, and can jointly learn 3D reconstruction and novel view synthesis, instead of treating them as separate tasks, making them ideal for real-world applications. Second, we benchmark several famous pipelines on our dataset for joint 3D mesh reconstruction and novel view synthesis. Finally, in order to further improve the overall performance, we propose a new method that achieves a good trade-off between the two tasks. Our dataset and benchmark show great potential in promoting the improvements for fusing 3D reconstruction and high-quality rendering in a robust and computationally efficient end-to-end fashion.
</details>
<details>
<summary>摘要</summary>
<SYS>translate text into Simplified Chinese</SYS>Metaverse技术需要精准、实时和具有吸引力的模型，用于非人类感知（如无人机/机器人/自动驾驶车 navigation）以及具有吸引力的技术，如AR/VR，需要结构准确和写实感。然而，当前存在一个知识空白，即如何在一个统一框架中应用准确的三维重建和写实感模型。为了bridging这个知识空白，并促进使用消费级设备进行强大和吸引人的模型和渲染，我们首先提出了一个真实世界多感器混合房间数据集（MuSHRoom）。我们的数据集具有吸引人的挑战，需要当前的技术来实现成本效益、鲁棒性和可靠性，同时可以同时学习3D重建和新视野合成，而不是单独处理它们为两个独立的任务，使其适用于实际应用。其次，我们对我们的数据集上使用了许多知名的管道进行联合3D网格重建和新视野合成的benchmark。最后，为了进一步提高总性能，我们提出了一种新的方法，可以在两个任务之间取得良好的平衡。我们的数据集和benchmark表现出了推动改进混合3D重建和高质量渲染的robust和计算效率的潜力。
</details></li>
</ul>
<hr>
<h2 id="Fast-Sparse-3D-Convolution-Network-with-VDB"><a href="#Fast-Sparse-3D-Convolution-Network-with-VDB" class="headerlink" title="Fast Sparse 3D Convolution Network with VDB"></a>Fast Sparse 3D Convolution Network with VDB</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02762">http://arxiv.org/abs/2311.02762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangjun Zhou, Anyong Mao, Eftychios Sifakis</li>
<li>for: 这个论文是为了提出一种新的卷积神经网络实现，用于高效地进行稀疏3D数据推理。</li>
<li>methods: 这个实现使用NanoVDB作为数据结构，以减少内存占用量，同时保持高性能。</li>
<li>results: 这个架构比STATE-OF-THE-ART dense CNN模型快得多，在高分辨率3D物体分类网络上达到了约20倍的速度提升。<details>
<summary>Abstract</summary>
We proposed a new Convolution Neural Network implementation optimized for sparse 3D data inference. This implementation uses NanoVDB as the data structure to store the sparse tensor. It leaves a relatively small memory footprint while maintaining high performance. We demonstrate that this architecture is around 20 times faster than the state-of-the-art dense CNN model on a high-resolution 3D object classification network.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的卷积神经网络实现，针对稀疏的3D数据推理。这种实现使用NanoVDB作为数据结构来存储稀疏张量。它占用较少的内存空间，同时保持高性能。我们示出这种架构比现有的密集 CNN 模型在高分辨率3D物体分类网络上20倍快。
</details></li>
</ul>
<hr>
<h2 id="Fast-Point-cloud-to-Mesh-Reconstruction-for-Deformable-Object-Tracking"><a href="#Fast-Point-cloud-to-Mesh-Reconstruction-for-Deformable-Object-Tracking" class="headerlink" title="Fast Point-cloud to Mesh Reconstruction for Deformable Object Tracking"></a>Fast Point-cloud to Mesh Reconstruction for Deformable Object Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02749">http://arxiv.org/abs/2311.02749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elham Amin Mansour, Hehui Zheng, Robert K. Katzschmann</li>
<li>for: 用于控制软体的机器人手，需要在线获取软体的状态反馈。</li>
<li>methods: 我们提出了一种方法，可以在58Hz的速度下创建不同类型的物体的塑形网格，并跟踪其变形。这种方法基于点云自动编码器和实数流变换器，可以在Marker-free的方式下进行系统标识。</li>
<li>results: 我们的方法可以在6种ycb类别中实现58Hz的塑形网格重建和跟踪，这些结果可以用于控制机器人手的 grasping操作，并且可以帮助系统进行 marker-free的系统标识。<details>
<summary>Abstract</summary>
The world around us is full of soft objects that we as humans learn to perceive and deform with dexterous hand movements from a young age. In order for a Robotic hand to be able to control soft objects, it needs to acquire online state feedback of the deforming object. While RGB-D cameras can collect occluded information at a rate of 30 Hz, the latter does not represent a continuously trackable object surface. Hence, in this work, we developed a method that can create deforming meshes of deforming point clouds at a speed of above 50 Hz for different categories of objects. The reconstruction of meshes from point clouds has been long studied in the field of Computer graphics under 3D reconstruction and 4D reconstruction, however both lack the speed and generalizability needed for robotics applications. Our model is designed using a point cloud auto-encoder and a Real-NVP architecture. The latter is a continuous flow neural network with manifold-preservation properties. Our model takes a template mesh which is the mesh of an object in its canonical state and then deforms the template mesh to match a deformed point cloud of the object. Our method can perform mesh reconstruction and tracking at a rate of 58 Hz for deformations of six different ycb categories. An instance of a downstream application can be the control algorithm for a robotic hand that requires online feedback from the state of a manipulated object which would allow online grasp adaptation in a closed-loop manner. Furthermore, the tracking capacity that our method provides can help in the system identification of deforming objects in a marker-free approach. In future work, we will extend our method to more categories of objects and real world deforming point clouds
</details>
<details>
<summary>摘要</summary>
世界中的软物体 surrounds 我们，从小时候我们就开始学习通过手部动作来感知和改变它们。如果机器人手需要控制软物体，它需要在线获取软物体的状态反馈。而RGB-D 摄像头可以在 30 Hz 频率上收集受障的信息，但这些信息不是可持续跟踪的物体表面。因此，在这项工作中，我们开发了一种方法，可以在不同类型物体上创建弹性的三角形结构，并在不同类型物体上进行不同类型的变形。我们的模型基于点云自编码器和真实NVP 架构。后者是一种连续流 neural network 具有 manifold-preservation 性能。我们的模型首先将模板三角形与弹性点云结构进行匹配，然后将模板三角形变形为与弹性点云结构匹配。我们的方法可以在 58 Hz 频率上进行三角形重建和跟踪，并且可以在不同类型的 ycb 类型上进行不同类型的变形。这种方法的实现可以用于 robotic 手的控制算法，以便在关闭环境中进行在线抓握适应。此外，我们的方法可以提供跟踪能力，帮助在无标记 Approach 中系统标识弹性物体。在未来的工作中，我们计划扩展我们的方法到更多的类型上，以及使用实际世界中的弹性点云结构。
</details></li>
</ul>
<hr>
<h2 id="Attention-Modules-Improve-Image-Level-Anomaly-Detection-for-Industrial-Inspection-A-DifferNet-Case-Study"><a href="#Attention-Modules-Improve-Image-Level-Anomaly-Detection-for-Industrial-Inspection-A-DifferNet-Case-Study" class="headerlink" title="Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study"></a>Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02747">http://arxiv.org/abs/2311.02747</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andreluizbvs/insplad">https://github.com/andreluizbvs/insplad</a></li>
<li>paper_authors: André Luiz Buarque Vieira e Silva, Francisco Simões, Danny Kowerko, Tobias Schlosser, Felipe Battisti, Veronica Teichrieb</li>
<li>for: 这篇论文主要针对于 semi-automated 视觉工业检测中的学习基于方法，以便处理高分辨率图像中的小型缺陷模式。</li>
<li>methods: 这篇论文提出了基于 DifferNet 的解决方案，其中包括了注意模块：AttentDifferNet。该方法可以提高图像水平的检测和分类能力，并在三个视觉异常检测数据集上达到了更高的Results：InsPLAD-fault、MVTec AD 和 Semiconductor Wafer。</li>
<li>results: 相比之前的状态艺术，AttentDifferNet 在三个数据集上的全局 AUROC 平均提高了1.77±0.25个百分点，达到了领先的Results，特别是在InsPLAD-fault 数据集上，这是一个工业检测在野数据集。<details>
<summary>Abstract</summary>
Within (semi-)automated visual industrial inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To alleviate this issue and advance the current state of the art in unsupervised visual inspection, this work proposes a DifferNet-based solution enhanced with attention modules: AttentDifferNet. It improves image-level detection and classification capabilities on three visual anomaly detection datasets for industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In comparison to the state of the art, AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quali-quantitative study. Our quantitative evaluation shows an average improvement - compared to DifferNet - of 1.77 +/- 0.25 percentage points in overall AUROC considering all three datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection in-the-wild dataset. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for industrial anomaly detection both in the wild and in controlled environments.
</details>
<details>
<summary>摘要</summary>
在半自动化visual工业检测中，基于学习的方法用于识别视觉缺陷，包括深度神经网络，可以处理高分辨率图像中的小缺陷模式。由于这些缺陷模式通常是罕见的，因此需要大量标注数据集。为了解决这个问题并提高现有状态的艺术，本研究提出了AttentDifferNet解决方案，它基于DifferNet框架并添加了注意模块。它在三个视觉异常检测数据集（InsPLAD-fault、MVTec AD和半导体晶圆）上提高了图像级检测和分类能力。与现有状态相比，AttentDifferNet实现了提高的结果，这些结果在我们的资深评估中得到了证明。我们的量化评估表明，相比DifferNet，AttentDifferNet在三个数据集上的总AUROC平均提高了1.77±0.25个百分点，在InsPLAD-fault中达到了领先的state-of-the-art results。我们的变体表明，AttentDifferNet在当前investigated的方法中具有极大的潜力。因此，我们形ulated一个基线，强调在工业异常检测中的注意力的重要性，不仅在控制环境中，而且在野外环境中。
</details></li>
</ul>
<hr>
<h2 id="Scenario-Diffusion-Controllable-Driving-Scenario-Generation-With-Diffusion"><a href="#Scenario-Diffusion-Controllable-Driving-Scenario-Generation-With-Diffusion" class="headerlink" title="Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion"></a>Scenario Diffusion: Controllable Driving Scenario Generation With Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02738">http://arxiv.org/abs/2311.02738</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Pronovost, Meghana Reddy Ganesina, Noureldin Hendy, Zeyu Wang, Andres Morales, Kai Wang, Nicholas Roy</li>
<li>for: 用于验证自动驾驶汽车的安全性。</li>
<li>methods: 使用扩散方法生成交通场景，并同时生成 sintetic agent的姿势、方向和路径的分布。</li>
<li>results: 能够模拟多样化的交通模式，并在不同地区进行扩散。<details>
<summary>Abstract</summary>
Automated creation of synthetic traffic scenarios is a key part of validating the safety of autonomous vehicles (AVs). In this paper, we propose Scenario Diffusion, a novel diffusion-based architecture for generating traffic scenarios that enables controllable scenario generation. We combine latent diffusion, object detection and trajectory regression to generate distributions of synthetic agent poses, orientations and trajectories simultaneously. To provide additional control over the generated scenario, this distribution is conditioned on a map and sets of tokens describing the desired scenario. We show that our approach has sufficient expressive capacity to model diverse traffic patterns and generalizes to different geographical regions.
</details>
<details>
<summary>摘要</summary>
自动化创建人工交通情况场景是评估自动驾驶车辆（AV）的安全性的关键部分。在这篇论文中，我们提出了 Scenario Diffusion，一种基于扩散的架构，用于生成交通情况场景。我们将潜在扩散、物体检测和轨迹回归结合起来，同时生成 distribución 的 synthetic agent 姿势、方向和轨迹。为了提供更多的控制权，我们将这个分布 conditioned 于地图和 sets of tokens 描述所需的场景。我们示示了我们的方法具有 sufficient 的表达能力，能够模拟多样化的交通模式，并在不同的地理区域中泛化。
</details></li>
</ul>
<hr>
<h2 id="JRDB-Traj-A-Dataset-and-Benchmark-for-Trajectory-Forecasting-in-Crowds"><a href="#JRDB-Traj-A-Dataset-and-Benchmark-for-Trajectory-Forecasting-in-Crowds" class="headerlink" title="JRDB-Traj: A Dataset and Benchmark for Trajectory Forecasting in Crowds"></a>JRDB-Traj: A Dataset and Benchmark for Trajectory Forecasting in Crowds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02736">http://arxiv.org/abs/2311.02736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saeed Saadatnejad, Yang Gao, Hamid Rezatofighi, Alexandre Alahi</li>
<li>for: 预测未来轨迹是自主导航中非常重要的，特别是在避免人类事故中，预测代理人的能力在先前是非常重要的。</li>
<li>methods: 作者提出了一个新的轨迹预测数据集，用于评估模型在实际场景中的性能，包括跟踪模块的偏差。</li>
<li>results: 数据集提供了各种感知输入数据，包括所有代理人的位置、场景图像和点云数据，以及预测未来代理人的位置。这个数据集可以帮助研究人员更好地理解导航动力学。<details>
<summary>Abstract</summary>
Predicting future trajectories is critical in autonomous navigation, especially in preventing accidents involving humans, where a predictive agent's ability to anticipate in advance is of utmost importance. Trajectory forecasting models, employed in fields such as robotics, autonomous vehicles, and navigation, face challenges in real-world scenarios, often due to the isolation of model components. To address this, we introduce a novel dataset for end-to-end trajectory forecasting, facilitating the evaluation of models in scenarios involving less-than-ideal preceding modules such as tracking. This dataset, an extension of the JRDB dataset, provides comprehensive data, including the locations of all agents, scene images, and point clouds, all from the robot's perspective. The objective is to predict the future positions of agents relative to the robot using raw sensory input data. It bridges the gap between isolated models and practical applications, promoting a deeper understanding of navigation dynamics. Additionally, we introduce a novel metric for assessing trajectory forecasting models in real-world scenarios where ground-truth identities are inaccessible, addressing issues related to undetected or over-detected agents. Researchers are encouraged to use our benchmark for model evaluation and benchmarking.
</details>
<details>
<summary>摘要</summary>
预测未来轨迹是自动导航中非常重要的，特别是避免人类事故，因为预测代理人的能力在先前是非常重要的。轨迹预测模型在机器人、自动驾驶和导航等领域中使用，但在实际场景中经常遇到挑战，常因模型组件孤立。为解决这个问题，我们介绍了一个新的轨迹预测数据集，用于评估模型在各种实际场景中的表现。这个数据集是JRDB数据集的扩展，提供了完整的数据，包括所有代理人的位置、场景图像和点云数据，全部是机器人的视角。目标是预测代理人未来与机器人之间的位置，使用原始感知输入数据。它bridges模型与实际应用之间的差距，促进了导航动力学的深入理解。此外，我们还引入了一种新的评价轨迹预测模型的指标，用于实际场景中评估模型表现，解决不可见或过度探测的代理人问题。研究人员可以使用我们的标准来评估和比较模型。
</details></li>
</ul>
<hr>
<h2 id="ISAR-A-Benchmark-for-Single-and-Few-Shot-Object-Instance-Segmentation-and-Re-Identification"><a href="#ISAR-A-Benchmark-for-Single-and-Few-Shot-Object-Instance-Segmentation-and-Re-Identification" class="headerlink" title="ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification"></a>ISAR: A Benchmark for Single- and Few-Shot Object Instance Segmentation and Re-Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02734">http://arxiv.org/abs/2311.02734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nicogorlo/isar_wacv24">https://github.com/nicogorlo/isar_wacv24</a></li>
<li>paper_authors: Nicolas Gorlo, Kenneth Blomqvist, Francesco Milano, Roland Siegwart</li>
<li>for: 这篇论文是为了提高单射对象检测、实例分割和重新识别的能力而写的。</li>
<li>methods: 这篇论文提出了一个基准方法和一个semi-synthetic数据集，以便测试和评估单射对象检测、实例分割和重新识别的算法。</li>
<li>results: 这篇论文提出了一个基准方法，并提供了一个 semi-synthetic 数据集和一个标准化评估管线，以便加速开发单射对象检测、实例分割和重新识别的算法。<details>
<summary>Abstract</summary>
Most object-level mapping systems in use today make use of an upstream learned object instance segmentation model. If we want to teach them about a new object or segmentation class, we need to build a large dataset and retrain the system. To build spatial AI systems that can quickly be taught about new objects, we need to effectively solve the problem of single-shot object detection, instance segmentation and re-identification. So far there is neither a method fulfilling all of these requirements in unison nor a benchmark that could be used to test such a method. Addressing this, we propose ISAR, a benchmark and baseline method for single- and few-shot object Instance Segmentation And Re-identification, in an effort to accelerate the development of algorithms that can robustly detect, segment, and re-identify objects from a single or a few sparse training examples. We provide a semi-synthetic dataset of video sequences with ground-truth semantic annotations, a standardized evaluation pipeline, and a baseline method. Our benchmark aligns with the emerging research trend of unifying Multi-Object Tracking, Video Object Segmentation, and Re-identification.
</details>
<details>
<summary>摘要</summary>
现代物品水平映射系统大多采用上游学习的物品实例分割模型。如果我们想教他们新的物品或分割类，我们需要建立大型数据集并重新训练系统。为建立快速教育新物品的空间AI系统，我们需要有效解决单次物品检测、实例分割和重新识别的问题。目前没有一种满足所有这些要求的方法，也没有一个可用来测试这种方法的标准测试 benchмарck。为此，我们提出了 ISAR，一个基准方法和测试集，用于单次和少量的物品实例分割和重新识别。我们提供了一个半 sintetic的视频序列数据集，以及一个标准化的评估管道和基准方法。我们的 benchmark 与涌现的研究趋势相吻合，即将多个物体跟踪、视频物体分割和重新识别相结合。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-for-Safety-critical-Scene-Segmentation-via-Fine-grained-Reward-Maximization"><a href="#Uncertainty-Estimation-for-Safety-critical-Scene-Segmentation-via-Fine-grained-Reward-Maximization" class="headerlink" title="Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization"></a>Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02719">http://arxiv.org/abs/2311.02719</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/med-air/fgrm">https://github.com/med-air/fgrm</a></li>
<li>paper_authors: Hongzheng Yang, Cheng Chen, Yueyao Chen, Markus Scheppach, Hon Chi Yip, Qi Dou</li>
<li>for: 这个研究旨在提高深度 segmentation 模型在安全重要场景中的可靠性，特别是在医疗应用中。</li>
<li>methods: 我们提出了一个新的精细赏金 Maximum (FGRM) 框架，通过直接使用一个不确定度评估相关的奖励函数和一种可调整学习算法，以提高模型的不确定度评估。</li>
<li>results: 我们的方法在两个大规模的安全重要手术景象样本集上进行了实验，结果显示，我们的方法可以在实时一个前进对应中，以一个明显的优势在所有测量不确定度评估的标准做法中，而且可以保持高的任务准确度。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/med-air/FGRM%7D">https://github.com/med-air/FGRM}</a> 获取。<details>
<summary>Abstract</summary>
Uncertainty estimation plays an important role for future reliable deployment of deep segmentation models in safety-critical scenarios such as medical applications. However, existing methods for uncertainty estimation have been limited by the lack of explicit guidance for calibrating the prediction risk and model confidence. In this work, we propose a novel fine-grained reward maximization (FGRM) framework, to address uncertainty estimation by directly utilizing an uncertainty metric related reward function with a reinforcement learning based model tuning algorithm. This would benefit the model uncertainty estimation through direct optimization guidance for model calibration. Specifically, our method designs a new uncertainty estimation reward function using the calibration metric, which is maximized to fine-tune an evidential learning pre-trained segmentation model for calibrating prediction risk. Importantly, we innovate an effective fine-grained parameter update scheme, which imposes fine-grained reward-weighting of each network parameter according to the parameter importance quantified by the fisher information matrix. To the best of our knowledge, this is the first work exploring reward optimization for model uncertainty estimation in safety-critical vision tasks. The effectiveness of our method is demonstrated on two large safety-critical surgical scene segmentation datasets under two different uncertainty estimation settings. With real-time one forward pass at inference, our method outperforms state-of-the-art methods by a clear margin on all the calibration metrics of uncertainty estimation, while maintaining a high task accuracy for the segmentation results. Code is available at \url{https://github.com/med-air/FGRM}.
</details>
<details>
<summary>摘要</summary>
uncertainty estimation在未来安全critical scenario中的深度分割模型部署中扮演着重要的角色。然而，现有的uncertainty estimation方法受到了确定性评估的缺乏直接指导的问题。在这种情况下，我们提出了一种新的细化的奖励最大化（FGRM） Framework，以Address uncertainty estimation by directly using an uncertainty metric related reward function with a reinforcement learning based model tuning algorithm.这将通过直接优化指导来提高模型的uncertainty estimation。 Specifically, our method designs a new uncertainty estimation reward function using the calibration metric, which is maximized to fine-tune an evidential learning pre-trained segmentation model for calibrating prediction risk. Additionally, we innovate an effective fine-grained parameter update scheme, which imposes fine-grained reward-weighting of each network parameter according to the parameter importance quantified by the fisher information matrix. To the best of our knowledge, this is the first work exploring reward optimization for model uncertainty estimation in safety-critical vision tasks. Our method demonstrates effectiveness on two large safety-critical surgical scene segmentation datasets under two different uncertainty estimation settings, outperforming state-of-the-art methods by a clear margin on all calibration metrics of uncertainty estimation while maintaining a high task accuracy for segmentation results. Code is available at \url{https://github.com/med-air/FGRM}.
</details></li>
</ul>
<hr>
<h2 id="CycleCL-Self-supervised-Learning-for-Periodic-Videos"><a href="#CycleCL-Self-supervised-Learning-for-Periodic-Videos" class="headerlink" title="CycleCL: Self-supervised Learning for Periodic Videos"></a>CycleCL: Self-supervised Learning for Periodic Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03402">http://arxiv.org/abs/2311.03402</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Destro, Michael Gygli</li>
<li>for:  periodic video sequences 如 automatic production systems, remote sensing, medical applications, 或 physical training</li>
<li>methods:  CycleCL, a self-supervised learning method specifically designed for periodic data, using triplet loss to optimize for desired properties</li>
<li>results:  significantly outperforms previous video-based self-supervised learning methods on all tasks in industrial and human actions datasets<details>
<summary>Abstract</summary>
Analyzing periodic video sequences is a key topic in applications such as automatic production systems, remote sensing, medical applications, or physical training. An example is counting repetitions of a physical exercise. Due to the distinct characteristics of periodic data, self-supervised methods designed for standard image datasets do not capture changes relevant to the progression of the cycle and fail to ignore unrelated noise. They thus do not work well on periodic data. In this paper, we propose CycleCL, a self-supervised learning method specifically designed to work with periodic data. We start from the insight that a good visual representation for periodic data should be sensitive to the phase of a cycle, but be invariant to the exact repetition, i.e. it should generate identical representations for a specific phase throughout all repetitions. We exploit the repetitions in videos to design a novel contrastive learning method based on a triplet loss that optimizes for these desired properties. Our method uses pre-trained features to sample pairs of frames from approximately the same phase and negative pairs of frames from different phases. Then, we iterate between optimizing a feature encoder and resampling triplets, until convergence. By optimizing a model this way, we are able to learn features that have the mentioned desired properties. We evaluate CycleCL on an industrial and multiple human actions datasets, where it significantly outperforms previous video-based self-supervised learning methods on all tasks.
</details>
<details>
<summary>摘要</summary>
In this paper, we propose CycleCL, a self-supervised learning method specifically designed for periodic data. We start from the insight that a good visual representation for periodic data should be sensitive to the phase of a cycle but be invariant to the exact repetition. In other words, it should generate identical representations for a specific phase throughout all repetitions.We exploit the repetitions in videos to design a novel contrastive learning method based on a triplet loss that optimizes for these desired properties. Our method uses pre-trained features to sample pairs of frames from approximately the same phase and negative pairs of frames from different phases. Then, we iterate between optimizing a feature encoder and resampling triplets until convergence. By optimizing a model this way, we are able to learn features that have the mentioned desired properties.We evaluate CycleCL on an industrial and multiple human actions datasets, where it significantly outperforms previous video-based self-supervised learning methods on all tasks.
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-a-Benchmark-How-Reliable-is-MS-COCO"><a href="#Benchmarking-a-Benchmark-How-Reliable-is-MS-COCO" class="headerlink" title="Benchmarking a Benchmark: How Reliable is MS-COCO?"></a>Benchmarking a Benchmark: How Reliable is MS-COCO?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02709">http://arxiv.org/abs/2311.02709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Zimmermann, Justin Szeto, Jerome Pasquero, Frederic Ratle</li>
<li>for: 本研究使用Sama-COCO dataset进行了可读性分析，以发现可能存在的偏见和偏好。</li>
<li>methods: 本研究使用了一个形态分析管道，以评估不同注释方式对模型的影响。</li>
<li>results: 结果表明注释风格对模型的性能有重要影响，并且注释管道应该仔细考虑任务的关键点。In English, that would be:</li>
<li>for: This study uses the Sama-COCO dataset to analyze the readability of the annotations and discover potential biases and preferences.</li>
<li>methods: The study uses a shape analysis pipeline to evaluate the impact of different annotation styles on the model’s performance.</li>
<li>results: The results show that the annotation style has a significant impact on the model’s performance, and the annotation pipeline should carefully consider the task of interest.<details>
<summary>Abstract</summary>
Benchmark datasets are used to profile and compare algorithms across a variety of tasks, ranging from image classification to segmentation, and also play a large role in image pretraining algorithms. Emphasis is placed on results with little regard to the actual content within the dataset. It is important to question what kind of information is being learned from these datasets and what are the nuances and biases within them. In the following work, Sama-COCO, a re-annotation of MS-COCO, is used to discover potential biases by leveraging a shape analysis pipeline. A model is trained and evaluated on both datasets to examine the impact of different annotation conditions. Results demonstrate that annotation styles are important and that annotation pipelines should closely consider the task of interest. The dataset is made publicly available at https://www.sama.com/sama-coco-dataset/ .
</details>
<details>
<summary>摘要</summary>
《Benchmark datasets are used to profile and compare algorithms across a variety of tasks, ranging from image classification to segmentation, and also play a large role in image pretraining algorithms. Emphasis is placed on results with little regard to the actual content within the dataset. It is important to question what kind of information is being learned from these datasets and what are the nuances and biases within them. In the following work, Sama-COCO, a re-annotation of MS-COCO, is used to discover potential biases by leveraging a shape analysis pipeline. A model is trained and evaluated on both datasets to examine the impact of different annotation conditions. Results demonstrate that annotation styles are important and that annotation pipelines should closely consider the task of interest. The dataset is made publicly available at https://www.sama.com/sama-coco-dataset/。》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-Uncertainty-in-Polygon-Annotation-and-the-Impact-of-Quality-Assurance"><a href="#An-Empirical-Study-of-Uncertainty-in-Polygon-Annotation-and-the-Impact-of-Quality-Assurance" class="headerlink" title="An Empirical Study of Uncertainty in Polygon Annotation and the Impact of Quality Assurance"></a>An Empirical Study of Uncertainty in Polygon Annotation and the Impact of Quality Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02707">http://arxiv.org/abs/2311.02707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric Zimmermann, Justin Szeto, Frederic Ratle</li>
<li>for: 这篇论文是为了研究多边形标注中的不确定性和质量控制的问题。</li>
<li>methods: 这篇论文使用了多边形标注的多评估程序，并对MS-COCO数据集中的一些对象进行了分析。</li>
<li>results: 研究结果表明，多边形标注的可靠性取决于评估程序和场景和形状的复杂性。<details>
<summary>Abstract</summary>
Polygons are a common annotation format used for quickly annotating objects in instance segmentation tasks. However, many real-world annotation projects request near pixel-perfect labels. While strict pixel guidelines may appear to be the solution to a successful project, practitioners often fail to assess the feasibility of the work requested, and overlook common factors that may challenge the notion of quality. This paper aims to examine and quantify the inherent uncertainty for polygon annotations and the role that quality assurance plays in minimizing its effect. To this end, we conduct an analysis on multi-rater polygon annotations for several objects from the MS-COCO dataset. The results demonstrate that the reliability of a polygon annotation is dependent on a reviewing procedure, as well as the scene and shape complexity.
</details>
<details>
<summary>摘要</summary>
多角形是常用的注释格式，用于快速标注对象在实例分割任务中。然而，许多实际项目需要非常精准的标注。虽然严格的像素指南可能看起来是成功项目的解决方案，但实际上，很多实践者会忽视标注工作的可行性和常见因素的影响。这篇论文旨在检查和评估多角形注释中的内在不确定性，以及质量控制在减少其影响的角色。为此，我们对 MS-COCO 数据集中的多个对象进行了多评人多角形注释的分析。结果表明，多角形注释的可靠性取决于评审过程和场景和形状复杂度。
</details></li>
</ul>
<hr>
<h2 id="A-Generative-Multi-Resolution-Pyramid-and-Normal-Conditioning-3D-Cloth-Draping"><a href="#A-Generative-Multi-Resolution-Pyramid-and-Normal-Conditioning-3D-Cloth-Draping" class="headerlink" title="A Generative Multi-Resolution Pyramid and Normal-Conditioning 3D Cloth Draping"></a>A Generative Multi-Resolution Pyramid and Normal-Conditioning 3D Cloth Draping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02700">http://arxiv.org/abs/2311.02700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hunorlaczko/pyramid-drape">https://github.com/hunorlaczko/pyramid-drape</a></li>
<li>paper_authors: Hunor Laczkó, Meysam Madadi, Sergio Escalera, Jordi Gonzalez</li>
<li>for: 3D garment generation and draping</li>
<li>methods: conditional variational autoencoder with pyramid network and surface normal UV maps</li>
<li>results: robust, controllable, and state-of-the-art results with high generalization to unseen garments, poses, and shapes<details>
<summary>Abstract</summary>
RGB cloth generation has been deeply studied in the related literature, however, 3D garment generation remains an open problem. In this paper, we build a conditional variational autoencoder for 3D garment generation and draping. We propose a pyramid network to add garment details progressively in a canonical space, i.e. unposing and unshaping the garments w.r.t. the body. We study conditioning the network on surface normal UV maps, as an intermediate representation, which is an easier problem to optimize than 3D coordinates. Our results on two public datasets, CLOTH3D and CAPE, show that our model is robust, controllable in terms of detail generation by the use of multi-resolution pyramids, and achieves state-of-the-art results that can highly generalize to unseen garments, poses, and shapes even when training with small amounts of data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ChEF-A-Comprehensive-Evaluation-Framework-for-Standardized-Assessment-of-Multimodal-Large-Language-Models"><a href="#ChEF-A-Comprehensive-Evaluation-Framework-for-Standardized-Assessment-of-Multimodal-Large-Language-Models" class="headerlink" title="ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models"></a>ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02692">http://arxiv.org/abs/2311.02692</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openlamm/lamm">https://github.com/openlamm/lamm</a></li>
<li>paper_authors: Zhelun Shi, Zhipin Wang, Hongxing Fan, Zhenfei Yin, Lu Sheng, Yu Qiao, Jing Shao</li>
<li>for: 本研究旨在提供一个普适的评估框架，以全面理解大型语言模型在多modal内容交互中的能力和局限性。</li>
<li>methods: 本研究提出了一个名为ChEF的全面评估框架，由四个组成部分组成：Scene（可扩展的多modal数据集）、Instruction（灵活的指令检索方程）、Inferencer（可靠的问题解答策略）和Metric（任务特定的分数函数）。这个框架可以标准化地评估不同的大型语言模型，并且可以根据不同的场景和需求设计新的评估方法。</li>
<li>results: 本研究通过对9种知名的大型语言模型在9个场景中进行大规模评估，总结了20多个有价值的观察，包括不同场景下大型语言模型的普适性和多modal交互需要的复合能力。这些观察可以帮助理解大型语言模型在多modal内容交互中的能力和局限性。<details>
<summary>Abstract</summary>
Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.
</details>
<details>
<summary>摘要</summary>
多modal大型语言模型（MLLMs）在与视觉内容互动中表现出了吸引人的能力，但是，即使有了一份标准的benchmark列表，MLLMs的能力和局限性仍未被全面了解，这是因为缺乏一个标准化和整体的评估框架。为此，我们提出了首个全面评估框架（ChEF），可以彻底评估每种MLLM，并公平比较不同的MLLMs。ChEF由四个可重复组件组成：Scene（可扩展的多模态数据集）、Instruction（灵活的指令检索方程）、Inferencer（可靠的问题回答策略）和Metric（任务特定的指标函数）。基于这些组件，ChEF提供了一个标准化的评估框架，并且可以通过设计新的Recipe（系统atic选择这些四个组件）来创建新的评估。值得注意的是，现有的MLLM benchmark可以被视为ChEF的recipe。我们还引入了6种新的recipe，用于评估MLLMs的需要的能力（或称为“欲望”，包括准确性、场景学习、指令遵从、语言表现、幻觉和稳定性）。然后，我们对9种知名MLLMs进行了大规模的评估，并对9个场景和6种欲望进行了评估。我们的评估结果表明，MLLMs在不同的场景下的一致性和多模态互动所需的复合能力是非常重要的。我们将在未来公布所有细节的实现，以及一个易于使用的模块化工具包，以便ChEF可以成为MLLM社区的发展评估框架。
</details></li>
</ul>
<hr>
<h2 id="Octavius-Mitigating-Task-Interference-in-MLLMs-via-MoE"><a href="#Octavius-Mitigating-Task-Interference-in-MLLMs-via-MoE" class="headerlink" title="Octavius: Mitigating Task Interference in MLLMs via MoE"></a>Octavius: Mitigating Task Interference in MLLMs via MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02684">http://arxiv.org/abs/2311.02684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, Jing Shao</li>
<li>for: 本研究旨在探讨大型自然语言模型（LLM）在多Modal学习中的零shot扩展能力，以及在不同模式和下游任务中的性能影响。</li>
<li>methods: 我们提出了一种新的和可扩展的框架，called \mname，用于全面地研究多Modal学习中的多Modal大型自然语言模型（MLLM）。我们将mixture-of-experts（MoE）和一种代表性的PEFT技术LoRA结合，设计了一种基于LLM的新解码器，called LoRA-MoE，用于多Modal学习。</li>
<li>results: 我们的实验结果（大约20%提升）表明了我们的设计的效iveness和多样性在不同的2D和3D下游任务中。<details>
<summary>Abstract</summary>
Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called \mname, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available soon.
</details>
<details>
<summary>摘要</summary>
Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, i.e., LoRA, to design a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. Our experimental results (with an improvement of around 20%) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. We will make the code and corresponding dataset available soon.
</details></li>
</ul>
<hr>
<h2 id="Digital-Typhoon-Long-term-Satellite-Image-Dataset-for-the-Spatio-Temporal-Modeling-of-Tropical-Cyclones"><a href="#Digital-Typhoon-Long-term-Satellite-Image-Dataset-for-the-Spatio-Temporal-Modeling-of-Tropical-Cyclones" class="headerlink" title="Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones"></a>Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02665">http://arxiv.org/abs/2311.02665</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kitamoto-lab/digital-typhoon">https://github.com/kitamoto-lab/digital-typhoon</a></li>
<li>paper_authors: Asanobu Kitamoto, Jared Hwang, Bastien Vuillod, Lucas Gautier, Yingtao Tian, Tarin Clanuwat</li>
<li>For: The paper presents the official release of the Digital Typhoon dataset, a long-term spatio-temporal satellite image dataset for benchmarking machine learning models in the context of tropical cyclones.* Methods: The authors developed a workflow to create an infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection and addressed data quality issues such as inter-satellite calibration to create a homogeneous dataset.* Results: The benchmarking results on the analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging for recent deep learning models, with many choices affecting the performance of various models.Here are the three points in Simplified Chinese text:* For: 这篇论文介绍了数字飓风数据集的官方发布，这是40多年的飓风卫星图像数据集，用于测试机器学习模型的长期空间时间数据处理能力。* Methods: 作者们开发了一个工作流程，使用拉曼投影将飓风中心的偏振图像进行裁剪，并对卫星数据进行了一系列的数据质量处理，以创建一个一致的数据集。* Results: 对于分析、预测和重建风速的测试结果表明，这个数据集对现代深度学习模型是一个挑战，因为有多种选择会影响不同模型的性能。<details>
<summary>Abstract</summary>
This paper presents the official release of the Digital Typhoon dataset, the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. To build the dataset, we developed a workflow to create an infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection referring to the best track data. We also address data quality issues such as inter-satellite calibration to create a homogeneous dataset. To take advantage of the dataset, we organized machine learning tasks by the types and targets of inference, with other tasks for meteorological analysis, societal impact, and climate change. The benchmarking results on the analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging for recent deep learning models, due to many choices that affect the performance of various models. This dataset reduces the barrier for machine learning researchers to meet large-scale real-world events called tropical cyclones and develop machine learning models that may contribute to advancing scientific knowledge on tropical cyclones as well as solving societal and sustainability issues such as disaster reduction and climate change. The dataset is publicly available at http://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and https://github.com/kitamoto-lab/digital-typhoon/.
</details>
<details>
<summary>摘要</summary>
这份论文发布了数字风暴数据集的官方发布，这是40多年的风暴卫星图像数据集，旨在为机器学习模型进行长期空间时间数据的benchmarking。为建立数据集，我们开发了一个工作流程，以拉姆伯特方程为基础，使用最佳轨迹数据来生成射电风暴中心图像进行剪辑。我们还解决了数据质量问题，如卫星间协调 calibration，以创建一个一致的数据集。为了利用这个数据集，我们组织了机器学习任务，按照不同的类型和目标进行定型，包括气象分析、社会影响和气候变化。结果表明，这个数据集对于现代深度学习模型来说是一个挑战，由于多种选择对不同模型的性能产生影响。这个数据集将降低机器学习研究人员面临大规模实际事件风暴预测和解决社会和可持续发展问题的障碍。这个数据集公共可用于http://agora.ex.nii.ac.jp/digital-typhoon/dataset/和https://github.com/kitamoto-lab/digital-typhoon/。
</details></li>
</ul>
<hr>
<h2 id="Enhanced-adaptive-cross-layer-scheme-for-low-latency-HEVC-streaming-over-Vehicular-Ad-hoc-Networks-VANETs"><a href="#Enhanced-adaptive-cross-layer-scheme-for-low-latency-HEVC-streaming-over-Vehicular-Ad-hoc-Networks-VANETs" class="headerlink" title="Enhanced adaptive cross-layer scheme for low latency HEVC streaming over Vehicular Ad-hoc Networks (VANETs)"></a>Enhanced adaptive cross-layer scheme for low latency HEVC streaming over Vehicular Ad-hoc Networks (VANETs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02664">http://arxiv.org/abs/2311.02664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Aymen Labiod, Mohamed Gharbi, François-Xavier Coudoux, Patrick Corlay, Noureddine Doghmane</li>
<li>for: 高效视频传输在 Vehicular Ad-hoc Networks (VANET) 中实现了现实，但是这些网络具有变化的通道质量和有限的带宽。</li>
<li>methods: 提议一种低复杂度跨层机制，通过考虑视频编码过程中的时间预测结构、帧重要性和网络负载状态，将每个视频传输包分配到最适合的 Access Category (AC) 队列在 Medium Access Control (MAC) 层。</li>
<li>results: 对不同的低延迟视频通信场景进行了评估，结果显示，提议的机制可以在视频质量和总结束延迟方面提供显著的改进，与802.11p 的 Enhanced Distributed Channel Access (EDCA) 相比。同时，对服务质量 (QoS) 和用户体验质量 (QoE) 也进行了评估，以验证提议的方法。<details>
<summary>Abstract</summary>
Vehicular communication has become a reality guided by various applications. Among those, high video quality delivery with low latency constraints required by real-time applications constitutes a very challenging task. By dint of its never-before-achieved compression level, the new High-Efficiency Video Coding (HEVC) is very promising for real-time video streaming through Vehicular Ad-hoc Networks (VANET). However, these networks have variable channel quality and limited bandwidth. Therefore, ensuring satisfactory video quality on such networks is a major challenge. In this work, a low complexity cross-layer mechanism is proposed to improve end-to-end performances of HEVC video streaming in VANET under low delay constraints. The idea is to assign to each packet of the transmitted video the most appropriate Access Category (AC) queue on the Medium Access Control (MAC) layer, considering the temporal prediction structure of the video encoding process, the importance of the frame and the state of the network traffic load. Simulation results demonstrate that for different targeted low-delay video communication scenarios, the proposed mechanism offers significant improvements regarding video quality at the reception and end-to-end delay compared to the Enhanced Distributed Channel Access (EDCA) adopted in the 802.11p. Both Quality of Service (QoS) and Quality of Experience (QoE) evaluations have been also carried out to validate the proposed approach.
</details>
<details>
<summary>摘要</summary>
To overcome this challenge, we propose a low-complexity cross-layer mechanism that improves the end-to-end performance of HEVC video streaming in VANETs under low delay constraints. Our approach assigns the most appropriate Access Category (AC) queue on the Medium Access Control (MAC) layer to each packet of the transmitted video, taking into account the temporal prediction structure of the video encoding process, the importance of the frame, and the state of the network traffic load.Simulation results show that our proposed mechanism offers significant improvements in video quality at the reception and end-to-end delay compared to the Enhanced Distributed Channel Access (EDCA) adopted in the 802.11p standard, for different targeted low-delay video communication scenarios. We also conducted Quality of Service (QoS) and Quality of Experience (QoE) evaluations to validate our approach.
</details></li>
</ul>
<hr>
<h2 id="CCMR-High-Resolution-Optical-Flow-Estimation-via-Coarse-to-Fine-Context-Guided-Motion-Reasoning"><a href="#CCMR-High-Resolution-Optical-Flow-Estimation-via-Coarse-to-Fine-Context-Guided-Motion-Reasoning" class="headerlink" title="CCMR: High Resolution Optical Flow Estimation via Coarse-to-Fine Context-Guided Motion Reasoning"></a>CCMR: High Resolution Optical Flow Estimation via Coarse-to-Fine Context-Guided Motion Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02661">http://arxiv.org/abs/2311.02661</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cv-stuttgart/CCMR">https://github.com/cv-stuttgart/CCMR</a></li>
<li>paper_authors: Azin Jahedi, Maximilian Luz, Marc Rivinius, Andrés Bruhn</li>
<li>for: 高精度多尺度摄像机流计算</li>
<li>methods: 基于注意力的动量聚合概念，使用层次两步注意力基于上下文运动聚合策略，首先计算全尺度多尺度上下文特征，然后使用它们引导实际运动聚合。</li>
<li>results: 通过结合多尺度和注意力基于概念，提供了高精度的流场图，在 occluded 和 non-occluded 区域都显示出了强大改进，与单尺度注意力基本和多尺度注意力自由基eline比较，提高了23.0% 和 21.6%。并实现了 state-of-the-art 结果，在 KITTI 2015 和 MPI Sintel Clean and Final 上 ranking 第一和第二。<details>
<summary>Abstract</summary>
Attention-based motion aggregation concepts have recently shown their usefulness in optical flow estimation, in particular when it comes to handling occluded regions. However, due to their complexity, such concepts have been mainly restricted to coarse-resolution single-scale approaches that fail to provide the detailed outcome of high-resolution multi-scale networks. In this paper, we hence propose CCMR: a high-resolution coarse-to-fine approach that leverages attention-based motion grouping concepts to multi-scale optical flow estimation. CCMR relies on a hierarchical two-step attention-based context-motion grouping strategy that first computes global multi-scale context features and then uses them to guide the actual motion grouping. As we iterate both steps over all coarse-to-fine scales, we adapt cross covariance image transformers to allow for an efficient realization while maintaining scale-dependent properties. Experiments and ablations demonstrate that our efforts of combining multi-scale and attention-based concepts pay off. By providing highly detailed flow fields with strong improvements in both occluded and non-occluded regions, our CCMR approach not only outperforms both the corresponding single-scale attention-based and multi-scale attention-free baselines by up to 23.0% and 21.6%, respectively, it also achieves state-of-the-art results, ranking first on KITTI 2015 and second on MPI Sintel Clean and Final. Code and trained models are available at https://github.com/cv-stuttgart /CCMR.
</details>
<details>
<summary>摘要</summary>
听力基于的动作聚合概念在视力估计中最近几年得到了广泛应用，尤其是在处理 occluded 区域时。然而，由于其复杂性，这些概念通常只能在粗略分辨率单个级别上实现，无法提供高分辨率多级网络的详细结果。在这篇文章中，我们因此提出了 CCMR：一种高分辨率含级抽象方法，利用听力基于的动作聚合概念来实现多级视力估计。CCMR 利用一种层次两步听力基于的上下文动作聚合策略，首先计算全局多级上下文特征，然后使用它们引导实际动作聚合。我们在所有粗略抽象级别上迭代这两个步骤，并使用可变 covariance 图像变换器来实现高效实现，保持级别 dependent 性。实验和排除示出，我们的努力将多级和听力基于的概念结合起来了。通过提供具有强大改进的流场场景和非 occluded 区域，我们的 CCMR 方法不仅超过了对应的单个级别听力基于和无听力基于的基准值，还达到了状态机器人框架。代码和训练模型可以在 <https://github.com/cv-stuttgart/CCMR> 获取。
</details></li>
</ul>
<hr>
<h2 id="Region-of-Interest-ROI-based-adaptive-cross-layer-system-for-real-time-video-streaming-over-Vehicular-Ad-hoc-NETworks-VANETs"><a href="#Region-of-Interest-ROI-based-adaptive-cross-layer-system-for-real-time-video-streaming-over-Vehicular-Ad-hoc-NETworks-VANETs" class="headerlink" title="Region of Interest (ROI) based adaptive cross-layer system for real-time video streaming over Vehicular Ad-hoc NETworks (VANETs)"></a>Region of Interest (ROI) based adaptive cross-layer system for real-time video streaming over Vehicular Ad-hoc NETworks (VANETs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02656">http://arxiv.org/abs/2311.02656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Aymen Labiod, Mohamed Gharbi, François-Xavier Coudoux, Patrick Corlay</li>
<li>for: 提高 vehicular video transmission质量以提高驾驶环境感知</li>
<li>methods: 使用 adaptive cross-layer mapping将 ROI视频数据包寄存到 IEEE 802.11p MAC 层，提高 HEVC 压缩视频通信质量</li>
<li>results: 实际 VANET 模拟结果显示，对 HEVC 压缩视频通信，提posed系统可以提供 UP TO 11dB PSNR 提升在 ROI 部分<details>
<summary>Abstract</summary>
Nowadays, real-time vehicle applications increasingly rely on video acquisition and processing to detect or even identify vehicles and obstacles in the driving environment. In this letter, we propose an algorithm that allows reinforcing these operations by improving end-to-end video transmission quality in a vehicular context. The proposed low complexity solution gives highest priority to the scene regions of interest (ROI) on which the perception of the driving environment is based on. This is done by applying an adaptive cross-layer mapping of the ROI visual data packets at the IEEE 802.11p MAC layer. Realistic VANET simulation results demonstrate that for HEVC compressed video communications, the proposed system offers PSNR gains up to 11dB on the ROI part.
</details>
<details>
<summary>摘要</summary>
现在，实时车辆应用越来越依赖于视频获取和处理来探测或识别在驾驶环境中的车辆和障碍物。在这封信中，我们提出了一种算法，可以通过提高端到端视频传输质量来增强这些操作。我们的低复杂度解决方案会将关键场景区域（ROI）的视频数据包在IEEE 802.11p MAC层进行适应性跨层映射。使用HEVC压缩视频通信后，我们的系统可以在ROI部分提供PSNR增幅达11dB。
</details></li>
</ul>
<hr>
<h2 id="Generative-Face-Video-Coding-Techniques-and-Standardization-Efforts-A-Review"><a href="#Generative-Face-Video-Coding-Techniques-and-Standardization-Efforts-A-Review" class="headerlink" title="Generative Face Video Coding Techniques and Standardization Efforts: A Review"></a>Generative Face Video Coding Techniques and Standardization Efforts: A Review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02649">http://arxiv.org/abs/2311.02649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bolin Chen, Jie Chen, Shiqi Wang, Yan Ye<br>for: 这个论文主要探讨了最新的生成式面部视频编码（GFVC）技术的发展和标准化努力，以实现高质量的面部视频通信在ultra低带宽场景下。methods: 这篇论文将GFVC技术综合评估和总结，包括不同的GFVC算法和其对应的视觉表示方法，以及相关的标准化努力。results: 这篇论文总结了GFVC技术的发展前景和应用潜力，以及相关的挑战和机遇。<details>
<summary>Abstract</summary>
Generative Face Video Coding (GFVC) techniques can exploit the compact representation of facial priors and the strong inference capability of deep generative models, achieving high-quality face video communication in ultra-low bandwidth scenarios. This paper conducts a comprehensive survey on the recent advances of the GFVC techniques and standardization efforts, which could be applicable to ultra low bitrate communication, user-specified animation/filtering and metaverse-related functionalities. In particular, we generalize GFVC systems within one coding framework and summarize different GFVC algorithms with their corresponding visual representations. Moreover, we review the GFVC standardization activities that are specified with supplemental enhancement information messages. Finally, we discuss fundamental challenges and broad applications on GFVC techniques and their standardization potentials, as well as envision their future trends. The project page can be found at https://github.com/Berlin0610/Awesome-Generative-Face-Video-Coding.
</details>
<details>
<summary>摘要</summary>
generative face video coding（GFVC）技术可以利用面部先验的紧凑表示和深度生成模型的强大推理能力，实现高质量的面部视频通信在超低带宽场景下。这篇论文对最近的GFVC技术的进步和标准化努力进行了全面的报道，这些技术可以应用于超低位元率通信、用户指定的动画/滤波和元宇宙相关功能。具体来说，我们将GFVC系统划分到一个编码框架中，并将不同的GFVC算法与其相应的视觉表示进行总结。此外，我们还评论了GFVC的标准化活动，包括补充增强信息消息。最后，我们讨论了GFVC技术的基本挑战和广泛应用，以及其标准化潜力，以及未来趋势。项目页面可以在<https://github.com/Berlin0610/Awesome-Generative-Face-Video-Coding>找到。
</details></li>
</ul>
<hr>
<h2 id="An-Approach-for-Multi-Object-Tracking-with-Two-Stage-Min-Cost-Flow"><a href="#An-Approach-for-Multi-Object-Tracking-with-Two-Stage-Min-Cost-Flow" class="headerlink" title="An Approach for Multi-Object Tracking with Two-Stage Min-Cost Flow"></a>An Approach for Multi-Object Tracking with Two-Stage Min-Cost Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02642">http://arxiv.org/abs/2311.02642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huining Li, Yalong Jiang, Xianlin Zeng, Feng Li, Zhipeng Wang</li>
<li>for: 本 paper 的目的是提出一种two-stage tracking pipeline，用于精准地跟踪多个目标在视频中，并且可以减少 occlusion 的影响。</li>
<li>methods: 本 paper 使用 minimum network flow algorithm，并且利用 tracklets 的交叠和低信任探测来准确地定位不准确的 tracklets。在第一 stage，使用高信任探测作为输入，并使用交叠 mask 来准确地定位不准确的 tracklets。在第二 stage，使用低信任探测来修正不准确的 tracklets。</li>
<li>results: 本 paper 在多个popular MOT benchmark datasets上进行了 sufficient 的实验，并 achieved 78.4 MOTA on MOT16 test set, 79.2 on MOT17 test set, and 76.4 on MOT20 test set, 这表明提出的方法是有效的。<details>
<summary>Abstract</summary>
The minimum network flow algorithm is widely used in multi-target tracking. However, the majority of the present methods concentrate exclusively on minimizing cost functions whose values may not indicate accurate solutions under occlusions. In this paper, by exploiting the properties of tracklets intersections and low-confidence detections, we develop a two-stage tracking pipeline with an intersection mask that can accurately locate inaccurate tracklets which are corrected in the second stage. Specifically, we employ the minimum network flow algorithm with high-confidence detections as input in the first stage to obtain the candidate tracklets that need correction. Then we leverage the intersection mask to accurately locate the inaccurate parts of candidate tracklets. The second stage utilizes low-confidence detections that may be attributed to occlusions for correcting inaccurate tracklets. This process constructs a graph of nodes in inaccurate tracklets and low-confidence nodes and uses it for the second round of minimum network flow calculation. We perform sufficient experiments on popular MOT benchmark datasets and achieve 78.4 MOTA on the test set of MOT16, 79.2 on MOT17, and 76.4 on MOT20, which shows that the proposed method is effective.
</details>
<details>
<summary>摘要</summary>
“多目标追踪中 widely 使用最小网络流算法。然而，大多数现有方法仅专注于最小化成本函数的值，而不考虑 occlusions 的情况下的精度。在本文中，我们利用追踪碎片 intersection 和低信任探测的属性，开发了一个两阶段追踪管线，具有精度的找到不精度追踪碎片。具体来说，我们在第一阶段使用最小网络流算法高信任探测作为输入，以获取需要更正的候选追踪碎片。然后，我们利用碎片 intersection 属性来精确地找到不精度追踪碎片的不精度部分。第二阶段使用低信任探测，可能导因于 occlusions，来更正不精度追踪碎片。这个过程建立了一个网络格，其中的节点是不精度追踪碎片和低信任节点，并使用它们进行第二次最小网络流计算。我们对流行的 MOT 评分数据进行了丰富的实验，并在 MOT16 的评分数据上取得 78.4 MOTA，在 MOT17 上取得 79.2 MOTA，在 MOT20 上取得 76.4 MOTA，这表明我们的方法是有效的。”
</details></li>
</ul>
<hr>
<h2 id="The-Background-Also-Matters-Background-Aware-Motion-Guided-Objects-Discovery"><a href="#The-Background-Also-Matters-Background-Aware-Motion-Guided-Objects-Discovery" class="headerlink" title="The Background Also Matters: Background-Aware Motion-Guided Objects Discovery"></a>The Background Also Matters: Background-Aware Motion-Guided Objects Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02633">http://arxiv.org/abs/2311.02633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sandra Kara, Hejer Ammar, Florian Chabot, Quoc-Cuong Pham</li>
<li>for: 提高对视频数据中物体发现的精度和效率</li>
<li>methods: 利用摄像头流计算出的运动mask，通过学习机制扩展到真正的背景和前景区域，并在物体发现过程中 JOINTLY 学习物体发现任务和物体&#x2F;非物体分离</li>
<li>results: 在 sintetic 和实际世界数据上进行了实验，结果表明，通过将我们的背景处理与多种前沿方法结合使用，可以大幅提高物体发现性能，并在物体&#x2F;非物体分离任务中建立强的基线。<details>
<summary>Abstract</summary>
Recent works have shown that objects discovery can largely benefit from the inherent motion information in video data. However, these methods lack a proper background processing, resulting in an over-segmentation of the non-object regions into random segments. This is a critical limitation given the unsupervised setting, where object segments and noise are not distinguishable. To address this limitation we propose BMOD, a Background-aware Motion-guided Objects Discovery method. Concretely, we leverage masks of moving objects extracted from optical flow and design a learning mechanism to extend them to the true foreground composed of both moving and static objects. The background, a complementary concept of the learned foreground class, is then isolated in the object discovery process. This enables a joint learning of the objects discovery task and the object/non-object separation. The conducted experiments on synthetic and real-world datasets show that integrating our background handling with various cutting-edge methods brings each time a considerable improvement. Specifically, we improve the objects discovery performance with a large margin, while establishing a strong baseline for object/non-object separation.
</details>
<details>
<summary>摘要</summary>
Our approach utilizes masks of moving objects extracted from optical flow and designs a learning mechanism to extend them to the true foreground, which includes both moving and static objects. The background, a complementary concept to the learned foreground class, is then isolated in the object discovery process. This enables joint learning of the object discovery task and object/non-object separation.Experiments on synthetic and real-world datasets show that integrating our background handling with various state-of-the-art methods consistently brings significant improvements in object discovery performance, while establishing a strong baseline for object/non-object separation. Specifically, we improve the objects discovery performance by a large margin, demonstrating the effectiveness of our proposed method.
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-Are-Implicit-Decision-Trees-The-Hierarchical-Simplicity-Bias"><a href="#Neural-Networks-Are-Implicit-Decision-Trees-The-Hierarchical-Simplicity-Bias" class="headerlink" title="Neural Networks Are Implicit Decision Trees: The Hierarchical Simplicity Bias"></a>Neural Networks Are Implicit Decision Trees: The Hierarchical Simplicity Bias</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02622">http://arxiv.org/abs/2311.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhehang Du<br>for: This paper aims to investigate the phenomenon of simplicity bias in neural networks and explore how they rely on simpler features while ignoring more complex ones, even when the complex features are equally predictive.methods: The authors introduce a novel approach called imbalanced label coupling to study scenarios where simple and complex features exhibit different levels of predictive power. They train neural networks on these scenarios and analyze how the networks make predictions based on the ascending complexity of input features.results: The authors find that the trained networks make predictions that align with the ascending complexity of input features, regardless of the underlying predictive power. For example, even when simple spurious features distort predictions in CIFAR-10, the networks still learn core features. However, last-layer retraining with target data distribution is effective but insufficient to fully recover core features when spurious features are perfectly correlated with the target labels in the synthetic dataset. These findings provide direct evidence that neural networks learn core features in the presence of spurious features.<details>
<summary>Abstract</summary>
Neural networks exhibit simplicity bias; they rely on simpler features while ignoring equally predictive but more complex features. In this work, we introduce a novel approach termed imbalanced label coupling to investigate scenarios where simple and complex features exhibit different levels of predictive power. In these cases, complex features still contribute to predictions. The trained networks make predictions in alignment with the ascending complexity of input features according to how they correlate with the label in the training set, irrespective of the underlying predictive power. For instance, even when simple spurious features distort predictions in CIFAR-10, most cats are predicted to be dogs, and most trucks are predicted to be automobiles! This observation provides direct evidence that the neural network learns core features in the presence of spurious features. We empirically show that last-layer retraining with target data distribution is effective, yet insufficient to fully recover core features when spurious features are perfectly correlated with the target labels in our synthetic dataset. We hope our research contributes to a deeper understanding of the implicit bias of neural networks.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="TFNet-Tuning-Fork-Network-with-Neighborhood-Pixel-Aggregation-for-Improved-Building-Footprint-Extraction"><a href="#TFNet-Tuning-Fork-Network-with-Neighborhood-Pixel-Aggregation-for-Improved-Building-Footprint-Extraction" class="headerlink" title="TFNet: Tuning Fork Network with Neighborhood Pixel Aggregation for Improved Building Footprint Extraction"></a>TFNet: Tuning Fork Network with Neighborhood Pixel Aggregation for Improved Building Footprint Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02617">http://arxiv.org/abs/2311.02617</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Ahmad Waseem, Muhammad Tahir, Zubair Khalid, Momin Uppal</li>
<li>for: 这 paper 考虑了从卫星影像中提取建筑物的问题，这是许多城市规划和决策应用中的关键任务。</li>
<li>methods: 该 paper 提出了一种新的 Tuning Fork Network (TFNet) 设计，用于深度 semantic segmentation，该设计不仅在广泛的建筑物上表现出色，还在 closely packed 的建筑物上表现良好。TFNet 架构包括一个单一的编码器和两个并行的解码器，用于分别重construct 建筑物的架构和建筑物的边缘。此外，TFNet 还 coupling 了一种在训练过程中在 tile 边界上 incorporating  neighbohood 信息的新方法。</li>
<li>results: 对 SpaceNet2、WHU 和一个来自卡拉χو（Pakistan）的 dataset 进行比较，提出的方法在所有三个 dataset 上显著地超越了参考方法。<details>
<summary>Abstract</summary>
This paper considers the problem of extracting building footprints from satellite imagery -- a task that is critical for many urban planning and decision-making applications. While recent advancements in deep learning have made great strides in automated detection of building footprints, state-of-the-art methods available in existing literature often generate erroneous results for areas with densely connected buildings. Moreover, these methods do not incorporate the context of neighborhood images during training thus generally resulting in poor performance at image boundaries. In light of these gaps, we propose a novel Tuning Fork Network (TFNet) design for deep semantic segmentation that not only performs well for widely-spaced building but also has good performance for buildings that are closely packed together. The novelty of TFNet architecture lies in a a single encoder followed by two parallel decoders to separately reconstruct the building footprint and the building edge. In addition, the TFNet design is coupled with a novel methodology of incorporating neighborhood information at the tile boundaries during the training process. This methodology further improves performance, especially at the tile boundaries. For performance comparisons, we utilize the SpaceNet2 and WHU datasets, as well as a dataset from an area in Lahore, Pakistan that captures closely connected buildings. For all three datasets, the proposed methodology is found to significantly outperform benchmark methods.
</details>
<details>
<summary>摘要</summary>
To address these limitations, we propose a novel Tuning Fork Network (TFNet) design for deep semantic segmentation. TFNet consists of a single encoder followed by two parallel decoders that separately reconstruct the building footprint and the building edge. Additionally, we introduce a novel methodology that incorporates neighborhood information at the tile boundaries during training, further improving performance, especially at the tile boundaries.We evaluate the proposed methodology on three datasets: SpaceNet2, WHU, and a dataset from Lahore, Pakistan, which captures closely connected buildings. Our results show that TFNet significantly outperforms benchmark methods on all three datasets.
</details></li>
</ul>
<hr>
<h2 id="Exploring-Grounding-Potential-of-VQA-oriented-GPT-4V-for-Zero-shot-Anomaly-Detection"><a href="#Exploring-Grounding-Potential-of-VQA-oriented-GPT-4V-for-Zero-shot-Anomaly-Detection" class="headerlink" title="Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection"></a>Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02612">http://arxiv.org/abs/2311.02612</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhangzjn/GPT-4V-AD">https://github.com/zhangzjn/GPT-4V-AD</a></li>
<li>paper_authors: Jiangning Zhang, Xuhai Chen, Zhucun Xue, Yabiao Wang, Chengjie Wang, Yong Liu<br>for: 这篇论文探讨了使用Visual Question Answering(VQA) paradigm来实现零基础的视觉异常检测(AD)任务，并对MVTec AD和VisA数据集进行质量和量化评估。methods: 该模型使用Large Multimodal Model(LMM) GPT-4V，包括三个组成部分：1) 粒度地划分，2) 提问设计，3) Text2Segmentation，以便轻松进行量化评估。results: 该模型在零基础AD任务中可以达到certain的结果，例如在MVTec AD和VisA数据集上的图像级别AU-ROC为77.1&#x2F;88.0，像素级别AU-ROC为68.0&#x2F;76.6。然而，与零基础方法WinCLIP ann CLIP-AD的性能还有一定差距，需要进一步研究。<details>
<summary>Abstract</summary>
Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding capabilities, making it possible to handle certain tasks through the Visual Question Answering (VQA) paradigm. This paper explores the potential of VQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and is the first to conduct qualitative and quantitative evaluations on the popular MVTec AD and VisA datasets. Considering that this task requires both image-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three components: 1) Granular Region Division, 2) Prompt Designing, 3) Text2Segmentation for easy quantitative evaluation, and have made some different attempts for comparative analysis. The results show that GPT-4V can achieve certain results in the zero-shot AD task through a VQA paradigm, such as achieving image-level 77.1/88.0 and pixel-level 68.0/76.6 AU-ROCs on MVTec AD and VisA datasets, respectively. However, its performance still has a certain gap compared to the state-of-the-art zero-shot method, e.g., WinCLIP ann CLIP-AD, and further research is needed. This study provides a baseline reference for the research of VQA-oriented LMM in the zero-shot AD task, and we also post several possible future works. Code is available at \url{https://github.com/zhangzjn/GPT-4V-AD}.
</details>
<details>
<summary>摘要</summary>
大型多模式模型（LMM）GPT-4V（视觉）具有视觉基准功能，使得GPT-4可以通过视觉问答（VQA）模式处理某些任务。本文探讨GPT-4V在最近受欢迎的视觉异常检测（AD）任务中的潜力，是首次对流行的MVTec AD和VisA数据集进行质量和量化评估。因为这个任务需要图像/像素级评估，提出了三组件：1）粒度区划，2）提示设计，3）文本2分 segmentation，以便轻松进行量化评估。results显示，GPT-4V可以通过VQA模式在零基础AD任务中获得某些结果，如MVTec AD和VisA数据集上的图像级77.1/88.0和像素级68.0/76.6 AU-ROC。然而，其表现仍有一定差距 compared tostate-of-the-art零基础方法，如WinCLIP ann CLIP-AD，并且需要进一步研究。这些研究提供了VQA-oriented LMM在零基础AD任务的基线参考，并提出了一些可能的未来工作。代码可以在 \url{https://github.com/zhangzjn/GPT-4V-AD} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-based-3D-Point-Cloud-Classification-A-Systematic-Survey-and-Outlook"><a href="#Deep-Learning-based-3D-Point-Cloud-Classification-A-Systematic-Survey-and-Outlook" class="headerlink" title="Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook"></a>Deep Learning-based 3D Point Cloud Classification: A Systematic Survey and Outlook</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02608">http://arxiv.org/abs/2311.02608</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huang Zhang, Changshuo Wang, Shengwei Tian, Baoli Lu, Liping Zhang, Xin Ning, Xiao Bai</li>
<li>for: 本研究的目的是为点云分类提供最新的研究进展和未来趋势，以帮助 relate fields 的研究人员。</li>
<li>methods: 本文回顾了点云数据的取得、特点和挑战，然后介绍了常用的3D数据表示方法、存储格式和点云分类的深度学习方法。</li>
<li>results: 本文对主要方法进行了比较和分析，并提出了一些挑战和未来趋势。In English, this would be:</li>
<li>for: The purpose of this paper is to provide the latest research progress and future trends in point cloud classification for researchers in related fields.</li>
<li>methods: The paper reviews point cloud acquisition, characteristics, and challenges, and then introduces commonly used datasets and deep learning-based methods for point cloud classification.</li>
<li>results: The paper compares and analyzes the performance of the main methods and discusses some challenges and future directions for point cloud classification.<details>
<summary>Abstract</summary>
In recent years, point cloud representation has become one of the research hotspots in the field of computer vision, and has been widely used in many fields, such as autonomous driving, virtual reality, robotics, etc. Although deep learning techniques have achieved great success in processing regular structured 2D grid image data, there are still great challenges in processing irregular, unstructured point cloud data. Point cloud classification is the basis of point cloud analysis, and many deep learning-based methods have been widely used in this task. Therefore, the purpose of this paper is to provide researchers in this field with the latest research progress and future trends. First, we introduce point cloud acquisition, characteristics, and challenges. Second, we review 3D data representations, storage formats, and commonly used datasets for point cloud classification. We then summarize deep learning-based methods for point cloud classification and complement recent research work. Next, we compare and analyze the performance of the main methods. Finally, we discuss some challenges and future directions for point cloud classification.
</details>
<details>
<summary>摘要</summary>
各种计算机视觉领域中的研究热点之一是点云表示，在自动驾驶、虚拟现实、机器人等领域都有广泛的应用。虽然深度学习技术在处理常见的2D网格图像数据上已经取得了很大的成功，但对于不规则、无结构的点云数据处理仍然存在很大的挑战。点云分类是点云分析的基础，许多深度学习基于的方法在这个任务中广泛使用。因此，本文的目的是为这个领域的研究人员提供最新的研究进展和未来趋势。首先，我们介绍点云获取、特点和挑战。其次，我们回顾3D数据表示、存储格式和常用的点云分类 dataset。然后，我们总结了深度学习基于的方法，并补充最近的研究工作。接着，我们比较和分析主要方法的性能。最后，我们讨论了点云分类的一些挑战和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Optimizing-Implicit-Neural-Representations-from-Point-Clouds-via-Energy-Based-Models"><a href="#Optimizing-Implicit-Neural-Representations-from-Point-Clouds-via-Energy-Based-Models" class="headerlink" title="Optimizing Implicit Neural Representations from Point Clouds via Energy-Based Models"></a>Optimizing Implicit Neural Representations from Point Clouds via Energy-Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02601">http://arxiv.org/abs/2311.02601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryutaro Yamauchi, Jinya Sakurai, Ryo Furukawa, Tatsushi Matsubayashi</li>
<li>for: 重建无旋转3D点云表面</li>
<li>methods: 使用能量基本模型优化卷积神经网络</li>
<li>results: 提高对点云噪声的耐性<details>
<summary>Abstract</summary>
Reconstructing a continuous surface from an unoritented 3D point cloud is a fundamental task in 3D shape processing. In recent years, several methods have been proposed to address this problem using implicit neural representations (INRs). In this study, we propose a method to optimize INRs using energy-based models (EBMs). By employing the absolute value of the coordinate-based neural networks as the energy function, the INR can be optimized through the estimation of the point cloud distribution by the EBM. In addition, appropriate parameter settings of the EBM enable the model to consider the magnitude of point cloud noise. Our experiments confirmed that the proposed method is more robust against point cloud noise than conventional surface reconstruction methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate english text into simplified chinese原文：Reconstructing a continuous surface from an unoriented 3D point cloud is a fundamental task in 3D shape processing. In recent years, several methods have been proposed to address this problem using implicit neural representations (INRs). In this study, we propose a method to optimize INRs using energy-based models (EBMs). By employing the absolute value of the coordinate-based neural networks as the energy function, the INR can be optimized through the estimation of the point cloud distribution by the EBM. In addition, appropriate parameter settings of the EBM enable the model to consider the magnitude of point cloud noise. Our experiments confirmed that the proposed method is more robust against point cloud noise than conventional surface reconstruction methods.翻译：建立一个连续的表面从无法指定的3D点云是3D形状处理中的基本任务。过去几年，一些方法被提出来解决这个问题使用隐藏神经网络表示（INR）。在这种研究中，我们提议使用能量基本模型（EBM）来优化INR。通过将坐标基本神经网络的绝对值作为能量函数，可以通过EBM估计点云分布，从而优化INR。此外，合适的EBM参数设置可以让模型考虑点云噪声的大小。我们的实验表明，我们提出的方法比传统表面重建方法更加鲁棒对待点云噪声。
</details></li>
</ul>
<hr>
<h2 id="Learning-Class-and-Domain-Augmentations-for-Single-Source-Open-Domain-Generalization"><a href="#Learning-Class-and-Domain-Augmentations-for-Single-Source-Open-Domain-Generalization" class="headerlink" title="Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization"></a>Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02599">http://arxiv.org/abs/2311.02599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prathmesh Bele, Valay Bundele, Avigyan Bhattacharya, Ankit Jha, Gemma Roig, Biplab Banerjee</li>
<li>for: 实现单源开放领域扩展（SS-ODG），解决训练时使用预订范围的标注范围，并在测试时遇到未知类别的挑战。</li>
<li>methods: 我们提出了一个名为SODG-Net的新框架，它同时生成新的领域和 pseudo-开放标本，使用学习型的目标函数，而不是常见的杂质混合策略。我们的方法通过增强多标本的多标本风格和生成多标本的多标本风格，从而提高扩展性。</li>
<li>results: 我们的SODG-Net在多个 benchmark 上进行了广泛的实验评估，与文献中的方法相比，它的表现都是superior。<details>
<summary>Abstract</summary>
Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-Net that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multi-class classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-Net compared to the literature.
</details>
<details>
<summary>摘要</summary>
单源开放预测（SS-ODG）Addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain, but these methods often struggle with visually fine-grained open-closed data, misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-Net that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, rather than the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multi-class classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-Net compared to the literature.
</details></li>
</ul>
<hr>
<h2 id="Synthetic-Tumor-Manipulation-With-Radiomics-Features"><a href="#Synthetic-Tumor-Manipulation-With-Radiomics-Features" class="headerlink" title="Synthetic Tumor Manipulation: With Radiomics Features"></a>Synthetic Tumor Manipulation: With Radiomics Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02586">http://arxiv.org/abs/2311.02586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inye Na, Jonghun Kim, Hyunjin Park</li>
<li>for: 用于生成精度控制和个性化的肿瘤部分</li>
<li>methods: 使用生成对抗网络、基于 радиомιcs特征的conditioning、多任务学习</li>
<li>results: 能够生成多样化、真实的肿瘤图像，并且可以根据特定的 радиомιcs特征进行细致的调整<details>
<summary>Abstract</summary>
We introduce RadiomicsFill, a synthetic tumor generator conditioned on radiomics features, enabling detailed control and individual manipulation of tumor subregions. This conditioning leverages conventional high-dimensional features of the tumor (i.e., radiomics features) and thus is biologically well-grounded. Our model combines generative adversarial networks, radiomics-feature conditioning, and multi-task learning. Through experiments with glioma patients, RadiomicsFill demonstrated its capability to generate diverse, realistic tumors and its fine-tuning ability for specific radiomics features like 'Pixel Surface' and 'Shape Sphericity'. The ability of RadiomicsFill to generate an unlimited number of realistic synthetic tumors offers notable prospects for both advancing medical imaging research and potential clinical applications.
</details>
<details>
<summary>摘要</summary>
我们介绍RadiomicsFill，一个基于对射频特征的人工肿瘤生成器，允许详细控制和个别修改肿瘤子区域。这个conditioning leverages conventional高维ensional特征（即对射频特征），因此具有生物学基础。我们的模型结合生成对抗网络、对射频特征conditioning和多任务学习。通过对肿瘤病人进行实验，RadiomicsFill表现出它的能力将生成多样化、现实的肿瘤，并且可以根据特定对射频特征进行细化调整，例如'Pixel Surface'和'Shape Sphericity'。RadiomicsFill的能力生成无限多个真实的人工肿瘤提供了重要的前途，将推动医疗影像研究和 potential clinical应用。
</details></li>
</ul>
<hr>
<h2 id="SSL-DG-Rethinking-and-Fusing-Semi-supervised-Learning-and-Domain-Generalization-in-Medical-Image-Segmentation"><a href="#SSL-DG-Rethinking-and-Fusing-Semi-supervised-Learning-and-Domain-Generalization-in-Medical-Image-Segmentation" class="headerlink" title="SSL-DG: Rethinking and Fusing Semi-supervised Learning and Domain Generalization in Medical Image Segmentation"></a>SSL-DG: Rethinking and Fusing Semi-supervised Learning and Domain Generalization in Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02583">http://arxiv.org/abs/2311.02583</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yezanting/ssl-dg">https://github.com/yezanting/ssl-dg</a></li>
<li>paper_authors: Zanting Ye</li>
<li>for: 这个论文的目的是提出一种基于深度学习的医疗影像分类方法，以应对受限给annotated data的状况下，并且处理域shift问题。</li>
<li>methods: 本论文使用了semi-supervised learning（SSL）和domain generalization（DG）两种方法，具体来说是使用class-level representation来表示未见目标数据，并通过对数据进行增强，以实现cross-domain generalization。</li>
<li>results: 实验结果显示， compared withstate-of-the-art方法，本论文的方法在两个难度问题中表现出色，并且具有较好的一致性和可靠性。<details>
<summary>Abstract</summary>
Deep learning-based medical image segmentation is an essential yet challenging task in clinical practice, which arises from restricted access to annotated data coupled with the occurrence of domain shifts. Previous attempts have focused on isolated solutions, while disregarding their inter-connectedness. In this paper, we rethink the relationship between semi-supervised learning (SSL) and domain generalization (DG), which are the cutting-edge approaches to address the annotated data-driven constraints and the domain shift issues. Inspired by class-level representation, we show that unseen target data can be represented by a linear combination of source data, which can be achieved by simple data augmentation. The augmented data enrich domain distributions while having semantic consistency, aligning with the principles of consistency-based SSL. Accordingly, we propose SSL-DG, fusing DG and SSL, to achieve cross-domain generalization with limited annotations. Specifically, the global and focal region augmentation, together with an augmentation scale-balancing mechanism, are used to construct a mask-based domain diffusion augmentation module to significantly enrich domain diversity. In order to obtain consistent predictions for the same source data in different networks, we use uncertainty estimation and a deep mutual learning strategy to enforce the consistent constraint. Extensive experiments including ablation studies are designed to validate the proposed SSL-DG. The results demonstrate that our SSL-DG significantly outperforms state-of-the-art solutions in two challenging DG tasks with limited annotations. Code is available at https://github.com/yezanting/SSL-DG.
</details>
<details>
<summary>摘要</summary>
深度学习基于医疗图像分割是临床实践中的必要 yet 挑战任务，这是由于缺乏标注数据的限制和频繁出现的频率域变换所致。先前的尝试都是采取分立的方法，而忽视了它们之间的连接。在这篇论文中，我们重新考虑了 semi-supervised learning（SSL）和频率域泛化（DG）的关系，这两种是医疗图像分割的瓶颈和频率域变换问题的解决方案。受到类别表示的启发，我们表明了未经见过的目标数据可以通过简单的数据扩展表示为源数据的线性组合。扩展后的数据可以增强频率域分布，同时保持 semantic consistency，与SSL的原理相符。因此，我们提议SSL-DG，将DG和SSL融合，实现受限的标注的横向泛化。具体来说，我们使用全球和焦点区域扩展，加上扩展缩放机制，构建一个面具基于频率域扩散增强模块，以显著提高频率域分布的多样性。为确保不同的源数据在不同网络中的预测结果具有一致性，我们使用uncertainty估计和深度相互学习策略来强制一致性约束。我们进行了广泛的实验，包括简洁分析，以验证我们的SSL-DG。结果显示，我们的SSL-DG在两个挑战的DG任务中具有明显的优势，并且超过了当前的状况。代码可以在https://github.com/yezanting/SSL-DG上下载。
</details></li>
</ul>
<hr>
<h2 id="Group-Testing-for-Accurate-and-Efficient-Range-Based-Near-Neighbor-Search-An-Adaptive-Binary-Splitting-Approach"><a href="#Group-Testing-for-Accurate-and-Efficient-Range-Based-Near-Neighbor-Search-An-Adaptive-Binary-Splitting-Approach" class="headerlink" title="Group Testing for Accurate and Efficient Range-Based Near Neighbor Search : An Adaptive Binary Splitting Approach"></a>Group Testing for Accurate and Efficient Range-Based Near Neighbor Search : An Adaptive Binary Splitting Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02573">http://arxiv.org/abs/2311.02573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kashish Mittal, Harsh Shah, Ajit Rajwade</li>
<li>for: 这篇论文针对高维ensional Near Neighbor Search（NNS）问题提出了一个适应性的集群试验框架。</li>
<li>methods: 这篇论文使用了一个基于cosine距离的点积分法，不需要对库中的所有元素进行探索。它还使用了一个多阶分组试验算法，通过分成两个子集，然后逐步对每个子集进行点积分，以节省时间。</li>
<li>results: 实验结果显示，这篇论文的方法可以与排序搜寻相比，提高速度超过10倍，且精度与排序搜寻相同。此外，论文还提供了一个理论分析，详细阐述了预期的距离计算数量和pool中成员数量的关系。<details>
<summary>Abstract</summary>
This work presents an adaptive group testing framework for the range-based high dimensional near neighbor search problem. The proposed method detects high-similarity vectors from an extensive collection of high dimensional vectors, where each vector represents an image descriptor. Our method efficiently marks each item in the collection as neighbor or non-neighbor on the basis of a cosine distance threshold without exhaustive search. Like other methods in the domain of large scale retrieval, our approach exploits the assumption that most of the items in the collection are unrelated to the query. Unlike other methods, it does not assume a large difference between the cosine similarity of the query vector with the least related neighbor and that with the least unrelated non-neighbor. Following the procedure of binary splitting, a multi-stage adaptive group testing algorithm, we split the set of items to be searched into half at each step, and perform dot product tests on smaller and smaller subsets, many of which we are able to prune away. We experimentally show that our method achieves a speed-up over exhaustive search by a factor of more than ten with an accuracy same as that of exhaustive search, on a variety of large datasets. We present a theoretical analysis of the expected number of distance computations per query and the probability that a pool with a certain number of members will be pruned. In this way, our method exploits very useful and practical distributional properties unlike other methods. In our method, all required data structures are created purely offline. Moreover, our method does not impose any strong assumptions on the number of true near neighbors, is adaptible to streaming settings where new vectors are dynamically added to the database, and does not require any parameter tuning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Multiple-Object-Tracking-based-on-Occlusion-Aware-Embedding-Consistency-Learning"><a href="#Multiple-Object-Tracking-based-on-Occlusion-Aware-Embedding-Consistency-Learning" class="headerlink" title="Multiple Object Tracking based on Occlusion-Aware Embedding Consistency Learning"></a>Multiple Object Tracking based on Occlusion-Aware Embedding Consistency Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02572">http://arxiv.org/abs/2311.02572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yaoqi Hu, Axi Niu, Yu Zhu, Qingsen Yan, Jinqiu Sun, Yanning Zhang<br>for: 多bject tracking中的跟踪问题methods: 利用视觉嵌入的一致性来解决 occlusion 导致的跟踪中断results: 在不同 occlusion 场景下，实现了较高的跟踪性能<details>
<summary>Abstract</summary>
The Joint Detection and Embedding (JDE) framework has achieved remarkable progress for multiple object tracking. Existing methods often employ extracted embeddings to re-establish associations between new detections and previously disrupted tracks. However, the reliability of embeddings diminishes when the region of the occluded object frequently contains adjacent objects or clutters, especially in scenarios with severe occlusion. To alleviate this problem, we propose a novel multiple object tracking method based on visual embedding consistency, mainly including: 1) Occlusion Prediction Module (OPM) and 2) Occlusion-Aware Association Module (OAAM). The OPM predicts occlusion information for each true detection, facilitating the selection of valid samples for consistency learning of the track's visual embedding. The OAAM leverages occlusion cues and visual embeddings to generate two separate embeddings for each track, guaranteeing consistency in both unoccluded and occluded detections. By integrating these two modules, our method is capable of addressing track interruptions caused by occlusion in online tracking scenarios. Extensive experimental results demonstrate that our approach achieves promising performance levels in both unoccluded and occluded tracking scenarios.
</details>
<details>
<summary>摘要</summary>
“ JOINT DETECTION AND EMBEDDING (JDE) 框架在多对象跟踪中做出了卓越的进步。现有方法通常通过提取的嵌入来重新建立新检测和已经中断的跟踪之间的关系。然而，当 occlusion 区域包含邻近 объек 或垃圾物时，嵌入的可靠性会减退，特别是在严重 occlusion 的情况下。为了解决这个问题，我们提出了一种基于视觉嵌入一致性的多对象跟踪方法，包括：1） occlusion prediction module (OPM) 和 2） occlusion-aware association module (OAAM)。OPM 预测每个真实检测中的 occlusion 信息，使得选择有效样本进行嵌入一致学习跟踪的视觉嵌入。OAAM 利用 occlusion 迹象和视觉嵌入来生成每个跟踪的两个分开的嵌入，保证了在不Occluded 和 Occluded 检测场景下的一致性。通过这两个模块的结合，我们的方法可以在在线跟踪场景中解决由 occlusion 引起的跟踪中断。我们的实验结果表明，我们的方法在不Occluded 和 Occluded 跟踪场景下具有出色的表现。”
</details></li>
</ul>
<hr>
<h2 id="Rotation-Invariant-Transformer-for-Recognizing-Object-in-UAVs"><a href="#Rotation-Invariant-Transformer-for-Recognizing-Object-in-UAVs" class="headerlink" title="Rotation Invariant Transformer for Recognizing Object in UAVs"></a>Rotation Invariant Transformer for Recognizing Object in UAVs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02559">http://arxiv.org/abs/2311.02559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuoyi Chen, Mang Ye, Bo Du</li>
<li>for: 本研究的目标是提高UAV上的目标识别精度，特别是对于大角度变换的情况。</li>
<li>methods: 本研究提出了一种新的旋转不变性视Transformer（RotTrans），通过在特征层进行旋转操作来实现旋转不变性。此外，我们还设置了一种�variance constraint来确保原始特征与旋转后的特征之间的关系。</li>
<li>results: 我们的提出的RotTrans模型在最新的UAV数据集上进行测试，与当前状态的艺术得到了显著的改进，其中高度的MAP和Rank1分别提高了5.9%和4.8%。此外，我们的模型还在传统的城市摄像头上进行人重识别任务中表现竞争力强。特别是在ICCV 2021年的Multi-Modal Video Reasoning and Analyzing Competition中，我们的解决方案在UAV基于人重识别追踪上获得了第一名。<details>
<summary>Abstract</summary>
Recognizing a target of interest from the UAVs is much more challenging than the existing object re-identification tasks across multiple city cameras. The images taken by the UAVs usually suffer from significant size difference when generating the object bounding boxes and uncertain rotation variations. Existing methods are usually designed for city cameras, incapable of handing the rotation issue in UAV scenarios. A straightforward solution is to perform the image-level rotation augmentation, but it would cause loss of useful information when inputting the powerful vision transformer as patches. This motivates us to simulate the rotation operation at the patch feature level, proposing a novel rotation invariant vision transformer (RotTrans). This strategy builds on high-level features with the help of the specificity of the vision transformer structure, which enhances the robustness against large rotation differences. In addition, we design invariance constraint to establish the relationship between the original feature and the rotated features, achieving stronger rotation invariance. Our proposed transformer tested on the latest UAV datasets greatly outperforms the current state-of-the-arts, which is 5.9\% and 4.8\% higher than the highest mAP and Rank1. Notably, our model also performs competitively for the person re-identification task on traditional city cameras. In particular, our solution wins the first place in the UAV-based person re-recognition track in the Multi-Modal Video Reasoning and Analyzing Competition held in ICCV 2021. Code is available at https://github.com/whucsy/RotTrans.
</details>
<details>
<summary>摘要</summary>
recognizing a target of interest from UAVs is much more challenging than existing object re-identification tasks across multiple city cameras. The images taken by UAVs usually suffer from significant size difference when generating object bounding boxes and uncertain rotation variations. Existing methods are usually designed for city cameras, incapable of handling the rotation issue in UAV scenarios. A straightforward solution is to perform image-level rotation augmentation, but it would cause loss of useful information when inputting powerful vision transformer as patches. This motivates us to simulate the rotation operation at the patch feature level, proposing a novel rotation invariant vision transformer (RotTrans). This strategy builds on high-level features with the help of the specificity of the vision transformer structure, which enhances robustness against large rotation differences. In addition, we design invariance constraint to establish the relationship between the original feature and the rotated features, achieving stronger rotation invariance. Our proposed transformer tested on the latest UAV datasets greatly outperforms the current state-of-the-arts, which is 5.9% and 4.8% higher than the highest mAP and Rank1. Notably, our model also performs competitively for the person re-identification task on traditional city cameras. In particular, our solution wins the first place in the UAV-based person re-recognition track in the Multi-Modal Video Reasoning and Analyzing Competition held in ICCV 2021. Code is available at https://github.com/whucsy/RotTrans.
</details></li>
</ul>
<hr>
<h2 id="Multi-Agent-3D-Map-Reconstruction-and-Change-Detection-in-Microgravity-with-Free-Flying-Robots"><a href="#Multi-Agent-3D-Map-Reconstruction-and-Change-Detection-in-Microgravity-with-Free-Flying-Robots" class="headerlink" title="Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots"></a>Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity with Free-Flying Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02558">http://arxiv.org/abs/2311.02558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Holly Dinkel, Julia Di, Jamie Santos, Keenan Albee, Paulo Borges, Marina Moreira, Oleg Alexandrov, Brian Coltin, Trey Smith</li>
<li>for: 这篇论文目标是为了帮助未来的宇航员使用自主飞行器进行宇宙站的维护和监测。</li>
<li>methods: 这篇论文使用了多代理协作地图建模和变化探测来帮助自主飞行器进行宇宙站的维护和监测。其中一个代理用于从图像和深度信息序列中重建宇宙站的3D模型。另一个代理用于定期扫描宇宙站环境，并与3D模型进行比较。</li>
<li>results: 这篇论文通过使用实际的图像和位置数据， validate了变化探测的有效性。<details>
<summary>Abstract</summary>
Assistive free-flyer robots autonomously caring for future crewed outposts -- such as NASA's Astrobee robots on the International Space Station (ISS) -- must be able to detect day-to-day interior changes to track inventory, detect and diagnose faults, and monitor the outpost status. This work presents a framework for multi-agent cooperative mapping and change detection to enable robotic maintenance of space outposts. One agent is used to reconstruct a 3D model of the environment from sequences of images and corresponding depth information. Another agent is used to periodically scan the environment for inconsistencies against the 3D model. Change detection is validated after completing the surveys using real image and pose data collected by Astrobee robots in a ground testing environment and from microgravity aboard the ISS. This work outlines the objectives, requirements, and algorithmic modules for the multi-agent reconstruction system, including recommendations for its use by assistive free-flyers aboard future microgravity outposts.
</details>
<details>
<summary>摘要</summary>
帮助自由飞行机器人在未来的人类殖民站上进行自主维护 -- 如 NASA 的 Astrobee 机器人在国际空站（ISS）上 -- 需要能够探测日常内部变化，跟踪库存、检测和诊断问题，以及监测站点状态。本文提出了多智能合作地图和变化检测框架，以启用机器人维护宇宙站。一个机器人用于从图像和相对深度信息序列中重建环境的3D模型。另一个机器人用于定期扫描环境，并与3D模型进行比较。变化检测的有效性通过在地面测试环境中收集的真实图像和姿态数据，以及在微重力环境中从 ISS 上收集的 Astrobee 机器人的数据进行验证。本文详细介绍了多智能重建系统的目标、要求和算法模块，并对未来微重力站点上的帮助自由飞行机器人使用这些系统提供建议。
</details></li>
</ul>
<hr>
<h2 id="IPVNet-Learning-Implicit-Point-Voxel-Features-for-Open-Surface-3D-Reconstruction"><a href="#IPVNet-Learning-Implicit-Point-Voxel-Features-for-Open-Surface-3D-Reconstruction" class="headerlink" title="IPVNet: Learning Implicit Point-Voxel Features for Open-Surface 3D Reconstruction"></a>IPVNet: Learning Implicit Point-Voxel Features for Open-Surface 3D Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02552">http://arxiv.org/abs/2311.02552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Samiul Arshad, William J. Beksi</li>
<li>for: 重建三维开面（例如非水平的网格）是计算机视觉领域的一个未探讨的领域。</li>
<li>methods: 我们提出了一种基于学习的隐式方法（IPVNet），它可以在任意分辨率下重建目标。IPVNet 利用点云数据和其粗化的 voxel 对应物进行学习，可以减少artifacts。</li>
<li>results: 我们在synthetic和实际数据集上进行了实验，结果显示IPVNet 可以超越当前状态态的表现，同时生成的重建结果中减少了outlier。<details>
<summary>Abstract</summary>
Reconstruction of 3D open surfaces (e.g., non-watertight meshes) is an underexplored area of computer vision. Recent learning-based implicit techniques have removed previous barriers by enabling reconstruction in arbitrary resolutions. Yet, such approaches often rely on distinguishing between the inside and outside of a surface in order to extract a zero level set when reconstructing the target. In the case of open surfaces, this distinction often leads to artifacts such as the artificial closing of surface gaps. However, real-world data may contain intricate details defined by salient surface gaps. Implicit functions that regress an unsigned distance field have shown promise in reconstructing such open surfaces. Nonetheless, current unsigned implicit methods rely on a discretized representation of the raw data. This not only bounds the learning process to the representation's resolution, but it also introduces outliers in the reconstruction. To enable accurate reconstruction of open surfaces without introducing outliers, we propose a learning-based implicit point-voxel model (IPVNet). IPVNet predicts the unsigned distance between a surface and a query point in 3D space by leveraging both raw point cloud data and its discretized voxel counterpart. Experiments on synthetic and real-world public datasets demonstrates that IPVNet outperforms the state of the art while producing far fewer outliers in the resulting reconstruction.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>> computer vision 领域中，三维开 superficie 的重建（如非水平的 mesh）是一个未经充分探索的领域。 recent learning-based implicit technique 已经突破了之前的障碍，使得重建在任意分辨率中成为可能。然而，这些方法通常需要在重建目标时分辨内部和外部的区别，以提取zero level set。在开 superficie 中，这种分辨 often leads to artifacts such as artificially closing surface gaps。然而，实际数据可能包含细节定义的明显surface gaps。implicit function 表示一个无符号距离场，已经表现出重建开 superficie 的承诺。然而，当前的无符号 implicit method 仅仅基于原始数据的粗略表示。这不仅限制了学习过程的分辨率，而且也会导致重建中出现异常值。为了准确地重建开 superficie 无异常值，我们提出了学习基于点 cloud 和 Its 粗略 voxel 对应的点云点-voxel 模型（IPVNet）。IPVNet 可以在 3D 空间中预测一个表示点和查询点之间的 unsigned distance。实验表明，IPVNet 在实际和 Synthetic 公共数据集上超过了状态的艺术，同时生成的重建中减少了异常值的出现。
</details></li>
</ul>
<hr>
<h2 id="3D-Aware-Talking-Head-Video-Motion-Transfer"><a href="#3D-Aware-Talking-Head-Video-Motion-Transfer" class="headerlink" title="3D-Aware Talking-Head Video Motion Transfer"></a>3D-Aware Talking-Head Video Motion Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02549">http://arxiv.org/abs/2311.02549</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang</li>
<li>for: 生成一个新视频，具有原视频的人物表情和动作模式。</li>
<li>methods: 使用一个3D-aware talking-head video motion transfer network（Head3D），全面利用 sujet 视频中的多视图出现特征，并通过自动学习3D head geometry learning module 和 attention-based fusion network来生成合成视频。</li>
<li>results: 在两个公共的 talking-head 视频数据集上进行了广泛的实验，研究发现 Head3D 在实际 cross-identity 设定下比2D和3D先前艺术 superior，并且可以轻松地适应pose-controllable novel view synthesis任务。<details>
<summary>Abstract</summary>
Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we employ an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task.
</details>
<details>
<summary>摘要</summary>
<SYS> translate("Motion transfer of talking-head videos involves generating a new video with the appearance of a subject video and the motion pattern of a driving video. Current methodologies primarily depend on a limited number of subject images and 2D representations, thereby neglecting to fully utilize the multi-view appearance features inherent in the subject video. In this paper, we propose a novel 3D-aware talking-head video motion transfer network, Head3D, which fully exploits the subject appearance information by generating a visually-interpretable 3D canonical head from the 2D subject frames with a recurrent network. A key component of our approach is a self-supervised 3D head geometry learning module, designed to predict head poses and depth maps from 2D subject video frames. This module facilitates the estimation of a 3D head in canonical space, which can then be transformed to align with driving video frames. Additionally, we employ an attention-based fusion network to combine the background and other details from subject frames with the 3D subject head to produce the synthetic target video. Our extensive experiments on two public talking-head video datasets demonstrate that Head3D outperforms both 2D and 3D prior arts in the practical cross-identity setting, with evidence showing it can be readily adapted to the pose-controllable novel view synthesis task.")</SYS>Here's the translation:现在的 talking-head 视频动作传输技术是生成一个新的视频，其视觉特征与源视频一致，而动作特征则与驱动视频一致。现有方法主要基于有限数量的源图像和2D表示，因此忽略了源视频中的多视图外观特征。在这篇论文中，我们提出了一种新的3D意识的 talking-head 视频动作传输网络，即 Head3D。我们的方法可以充分利用源视频中的外观信息，通过生成一个可见的3D抽象头来捕捉源视频中的头部pose和深度信息。我们还使用了一种注意力基于的融合网络，将背景和其他细节从源帧与3D主体头进行结合，以生成合成目标视频。我们的实验表明，Head3D在实际的交叉标识设定下，比2D和3D先前艺术高效，并且可以适应pose控制的新视图合成任务。
</details></li>
</ul>
<hr>
<h2 id="VR-NeRF-High-Fidelity-Virtualized-Walkable-Spaces"><a href="#VR-NeRF-High-Fidelity-Virtualized-Walkable-Spaces" class="headerlink" title="VR-NeRF: High-Fidelity Virtualized Walkable Spaces"></a>VR-NeRF: High-Fidelity Virtualized Walkable Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02542">http://arxiv.org/abs/2311.02542</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/EyefulTower">https://github.com/facebookresearch/EyefulTower</a></li>
<li>paper_authors: Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Aljaž Božič, Dahua Lin, Michael Zollhöfer, Christian Richardt</li>
<li>for: 这篇论文的目的是为了建立一个高精度捕捉、模型重建和实时渲染的虚拟现实系统，用于游走空间的高精度捕捉和模型化。</li>
<li>methods: 这篇论文使用了一个自定义多摄像头架设，以高精度捕捉游走空间，并使用了一种新的感知颜色空间来学习准确的高dynamic range外观，以及一种高效的mipmapping机制来实现级别of detail渲染。</li>
<li>results: 这篇论文的结果表明，使用这种方法可以在 dual 2K$\times$2K 的全息VR分辨率上实现高精度渲染，并且可以在36Hz的刷新率下保持高品质。此外，论文还提供了一个高精度测试集，并与现有的基准相比较。<details>
<summary>Abstract</summary>
We present an end-to-end system for the high-fidelity capture, model reconstruction, and real-time rendering of walkable spaces in virtual reality using neural radiance fields. To this end, we designed and built a custom multi-camera rig to densely capture walkable spaces in high fidelity and with multi-view high dynamic range images in unprecedented quality and density. We extend instant neural graphics primitives with a novel perceptual color space for learning accurate HDR appearance, and an efficient mip-mapping mechanism for level-of-detail rendering with anti-aliasing, while carefully optimizing the trade-off between quality and speed. Our multi-GPU renderer enables high-fidelity volume rendering of our neural radiance field model at the full VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We demonstrate the quality of our results on our challenging high-fidelity datasets, and compare our method and datasets to existing baselines. We release our dataset on our project website.
</details>
<details>
<summary>摘要</summary>
我们提出了一个终端系统，用于在虚拟现实中实时渲染可行空间，使用神经辐射场。为此，我们设计了一个专门的多摄像头笼体，用于高精度捕捉可行空间，并生成多视图高动态范围图像。我们在神经图形元素上添加了一个新的感知色彩空间，用于学习准确的高动态范围外观，并使用高效的压缩缩放机制，以实现级别化渲染。我们使用多卡GPU渲染器，实现高精度体积渲染我们的神经辐射场模型，并在双2K×2K分辨率和36Hz的自定义demo机器上实现。我们在我们的高精度数据集上证明了我们的结果质量，并与现有基准进行比较。我们将数据集上载到我们的项目网站。
</details></li>
</ul>
<hr>
<h2 id="Augment-the-Pairs-Semantics-Preserving-Image-Caption-Pair-Augmentation-for-Grounding-Based-Vision-and-Language-Models"><a href="#Augment-the-Pairs-Semantics-Preserving-Image-Caption-Pair-Augmentation-for-Grounding-Based-Vision-and-Language-Models" class="headerlink" title="Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models"></a>Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02536">http://arxiv.org/abs/2311.02536</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amzn/augment-the-pairs-wacv2024">https://github.com/amzn/augment-the-pairs-wacv2024</a></li>
<li>paper_authors: Jingru Yi, Burak Uzkent, Oana Ignat, Zili Li, Amanmeet Garg, Xiang Yu, Linda Liu</li>
<li>for: 提高视觉语言模型的表现，具体来说是在准确地定位文本描述中提到的物体。</li>
<li>methods: 使用文本决定和无文本决定的数据增强策略，包括文本背景颜色噪声和水平旋转，以保持图像和文本之间的Semantic consistency。另外，我们还提出了基于受限的信号重建的新的数据增强策略，即像素级别的遮盲。</li>
<li>results: 通过对Flickr30k、referring expressions和GQA三个常用的数据集进行广泛的实验，我们的方法表现出了与现有状态艺术的高水平的表现，并且与CLIP大规模图像和语言数据集预训练的图像Encoder结合使用可以进一步提高表现。<details>
<summary>Abstract</summary>
Grounding-based vision and language models have been successfully applied to low-level vision tasks, aiming to precisely locate objects referred in captions. The effectiveness of grounding representation learning heavily relies on the scale of the training dataset. Despite being a useful data enrichment strategy, data augmentation has received minimal attention in existing vision and language tasks as augmentation for image-caption pairs is non-trivial. In this study, we propose a robust phrase grounding model trained with text-conditioned and text-unconditioned data augmentations. Specifically, we apply text-conditioned color jittering and horizontal flipping to ensure semantic consistency between images and captions. To guarantee image-caption correspondence in the training samples, we modify the captions according to pre-defined keywords when applying horizontal flipping. Additionally, inspired by recent masked signal reconstruction, we propose to use pixel-level masking as a novel form of data augmentation. While we demonstrate our data augmentation method with MDETR framework, the proposed approach is applicable to common grounding-based vision and language tasks with other frameworks. Finally, we show that image encoder pretrained on large-scale image and language datasets (such as CLIP) can further improve the results. Through extensive experiments on three commonly applied datasets: Flickr30k, referring expressions and GQA, our method demonstrates advanced performance over the state-of-the-arts with various metrics. Code can be found in https://github.com/amzn/augment-the-pairs-wacv2024.
</details>
<details>
<summary>摘要</summary>
围绕基于grounding的视觉语言模型的研究，我们提出了一种可靠的图像描述对应模型，使用文本条件和无条件数据增强来学习表示学习。具体来说，我们使用文本条件的颜色扰动和水平翻转来保证图像和描述的semantic consistency。为保证训练样本中的图像描述对应，我们在应用水平翻转时对描述进行修改。此外，我们受到最近的masked signal reconstruction的启发，提出了一种新的数据增强方法：像素级别的遮盲。我们通过对MDETR框架进行修改来示出我们的数据增强方法的可应用性。最后，我们表明通过使用大规模的图像和语言数据集（如CLIP）进行预训练，可以进一步提高结果。通过对Flickr30k、referring expressions和GQA等三个常用的数据集进行广泛的实验，我们的方法达到了与先前最佳的多种纪录。代码可以在https://github.com/amzn/augment-the-pairs-wacv2024中找到。
</details></li>
</ul>
<hr>
<h2 id="TokenMotion-Motion-Guided-Vision-Transformer-for-Video-Camouflaged-Object-Detection-Via-Learnable-Token-Selection"><a href="#TokenMotion-Motion-Guided-Vision-Transformer-for-Video-Camouflaged-Object-Detection-Via-Learnable-Token-Selection" class="headerlink" title="TokenMotion: Motion-Guided Vision Transformer for Video Camouflaged Object Detection Via Learnable Token Selection"></a>TokenMotion: Motion-Guided Vision Transformer for Video Camouflaged Object Detection Via Learnable Token Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02535">http://arxiv.org/abs/2311.02535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifan Yu, Erfan Bank Tavakoli, Meida Chen, Suya You, Raghuveer Rao, Sanjeev Agarwal, Fengbo Ren</li>
<li>for: 提高视频掩体物体检测（VCOD）的性能，解决Texture相似性和Camera运动引起的难题。</li>
<li>methods: 使用 transformer 模型提取运动指导特征，通过学习 tokens 选择来提高 VCOD 性能。</li>
<li>results: 在 MoCA-Mask 数据集上评估，TMNet 实现了 VCOD 领域的状态可比性，相比 existed 状态可比性方法，提高了12.8%的Weighted F-度、8.4%的S-度和10.7%的Mean IoU。<details>
<summary>Abstract</summary>
The area of Video Camouflaged Object Detection (VCOD) presents unique challenges in the field of computer vision due to texture similarities between target objects and their surroundings, as well as irregular motion patterns caused by both objects and camera movement. In this paper, we introduce TokenMotion (TMNet), which employs a transformer-based model to enhance VCOD by extracting motion-guided features using a learnable token selection. Evaluated on the challenging MoCA-Mask dataset, TMNet achieves state-of-the-art performance in VCOD. It outperforms the existing state-of-the-art method by a 12.8% improvement in weighted F-measure, an 8.4% enhancement in S-measure, and a 10.7% boost in mean IoU. The results demonstrate the benefits of utilizing motion-guided features via learnable token selection within a transformer-based framework to tackle the intricate task of VCOD.
</details>
<details>
<summary>摘要</summary>
“视频掩体物体检测（VCOD）领域存在特殊挑战，主要是因为目标对象和周围环境的文本相似性，以及对象和摄像头运动导致的不规则运动模式。本文提出了TokenMotion（TMNet），利用 transformer 模型提取运动导向特征，通过学习式Token选择进行增强。在复杂的 MoCA-Mask 数据集上测试，TMNet  achieve 状态机器人-measure 的最佳性能，比既有状态机器人-measure 方法提高 12.8%，S-measure 提高 8.4%， mean IoU 提高 10.7%。结果表明，通过在 transformer 框架中使用学习式Token选择来捕捉运动导向特征，可以有效地解决 VCOD 领域中的复杂问题。”
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.CV_2023_11_05/" data-id="cloq1wl6600l77o888wokcyt3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.AI_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T12:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.AI_2023_11_05/">cs.AI - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Modelling-Cellular-Perturbations-with-the-Sparse-Additive-Mechanism-Shift-Variational-Autoencoder"><a href="#Modelling-Cellular-Perturbations-with-the-Sparse-Additive-Mechanism-Shift-Variational-Autoencoder" class="headerlink" title="Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder"></a>Modelling Cellular Perturbations with the Sparse Additive Mechanism Shift Variational Autoencoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02794">http://arxiv.org/abs/2311.02794</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/insitro/sams-vae">https://github.com/insitro/sams-vae</a></li>
<li>paper_authors: Michael Bereket, Theofanis Karaletsos</li>
<li>For: This paper proposes a new method called SAMS-VAE for modeling the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action in drug discovery.* Methods: SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects, and sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable.* Results: SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms.Here is the same information in Simplified Chinese text:* For: 这篇论文提出了一种新的方法called SAMS-VAE，用于模型不同干预对细胞的影响，以Characterize unknown biological mechanisms of action in drug discovery。* Methods: SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects，并将这些全球隐变量简化为具体的干预特定隐变空间，以便可以flexibly composable。* Results: SAMS-VAE比相关的模型在不同的任务上表现出色，包括资源缺乏下的combined reasoning任务，并且生成了可解释的隐变结构，与知道的生物机制强相关。<details>
<summary>Abstract</summary>
Generative models of observations under interventions have been a vibrant topic of interest across machine learning and the sciences in recent years. For example, in drug discovery, there is a need to model the effects of diverse interventions on cells in order to characterize unknown biological mechanisms of action. We propose the Sparse Additive Mechanism Shift Variational Autoencoder, SAMS-VAE, to combine compositionality, disentanglement, and interpretability for perturbation models. SAMS-VAE models the latent state of a perturbed sample as the sum of a local latent variable capturing sample-specific variation and sparse global variables of latent intervention effects. Crucially, SAMS-VAE sparsifies these global latent variables for individual perturbations to identify disentangled, perturbation-specific latent subspaces that are flexibly composable. We evaluate SAMS-VAE both quantitatively and qualitatively on a range of tasks using two popular single cell sequencing datasets. In order to measure perturbation-specific model-properties, we also introduce a framework for evaluation of perturbation models based on average treatment effects with links to posterior predictive checks. SAMS-VAE outperforms comparable models in terms of generalization across in-distribution and out-of-distribution tasks, including a combinatorial reasoning task under resource paucity, and yields interpretable latent structures which correlate strongly to known biological mechanisms. Our results suggest SAMS-VAE is an interesting addition to the modeling toolkit for machine learning-driven scientific discovery.
</details>
<details>
<summary>摘要</summary>
“生成干预测数据的模型在机器学习和科学领域中具有很大的兴趣，例如药物探索中需要模型不同类型的干预效应以描述未知生物机制。我们提出了对compose, disentangle和可解释性具有优势的SAMS-VAE模型，用于干预模型。SAMS-VAE将批处数据的latent state视为受到干预的sample专有的本地 latent variable和稀有的全球 latent variable，并将这些全球 latent variable压缩以归一化干预效应。我们通过两个受欢迎的单细胞测量数据集进行评估，并提出了基于对干预模型的平均治疗效应的评估框架，以及与后 posterior predictive checks 的连结。SAMS-VAE在不同任务中表现出色，包括资源缺乏下的构成逻辑任务，并具有可解释的latent结构，与生物机制具有强相关。我们的结果显示SAMS-VAE是机器学习驱动科学探索的有趣添加。”
</details></li>
</ul>
<hr>
<h2 id="CausalCite-A-Causal-Formulation-of-Paper-Citations"><a href="#CausalCite-A-Causal-Formulation-of-Paper-Citations" class="headerlink" title="CausalCite: A Causal Formulation of Paper Citations"></a>CausalCite: A Causal Formulation of Paper Citations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02790">http://arxiv.org/abs/2311.02790</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/causalnlp/causal-cite">https://github.com/causalnlp/causal-cite</a></li>
<li>paper_authors: Ishan Kumar, Zhijing Jin, Ehsan Mokhtarian, Siyuan Guo, Yuen Chen, Negar Kiyavash, Mrinmaya Sachan, Bernhard Schoelkopf</li>
<li>for: 这 paper 的目的是提出一种 causal inference 方法，用于评估科学论文的影响力。</li>
<li>methods: 该方法基于高维文本嵌入，使用 LLMs 对每篇论文进行编码，然后通过cosine similarity提取相似样本，并使用这些样本的权重平均值 Synthesize 一个 counterfactual 样本。</li>
<li>results: 该方法可以准确地评估论文的影响力，并且具有高相关性和稳定性。 authors 还提供了一些建议，用于未来的研究人员可以更好地使用该 metric。 code 和数据可以在 <a target="_blank" rel="noopener" href="https://github.com/causalNLP/causal-cite">https://github.com/causalNLP/causal-cite</a> 上获取。<details>
<summary>Abstract</summary>
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields of AI. We also provide a set of findings that can serve as suggested ways for future researchers to use our metric for a better understanding of a paper's quality. Our code and data are at https://github.com/causalNLP/causal-cite.
</details>
<details>
<summary>摘要</summary>
评估一篇论文的重要性是科学社区中的一项核心任务，但是也是一项具有挑战性的任务。虽然引用数是最常用的代理，但它们被广泛批评因为不能准确反映论文的真实影响。在这种情况下，我们提出了一种 causal inference 方法，即 TextMatch，该方法将传统的匹配框架应用到高维文本嵌入。具体来说，我们使用大型自然语言模型（LLM）生成的文本嵌入来编码每篇论文，然后通过cosinus相似性来提取相似的样本，并使用这些样本的相似性值来权重混合这些相似的论文。我们称这种度量为 CausalCite，它是一种用于评估论文引用的 causal 形式。我们在不同的评估标准下显示了 CausalCite 的有效性，包括高相关性与论文影响力（由科学专家在过去的数据集上提供的）、奖励、以及在不同的人工智能子领域中的稳定性。我们还提供了一些发现，可以帮助未来的研究人员通过我们的度量来更好地理解一篇论文的质量。我们的代码和数据可以在 GitHub 上找到：https://github.com/causalNLP/causal-cite。
</details></li>
</ul>
<hr>
<h2 id="Make-a-Donut-Language-Guided-Hierarchical-EMD-Space-Planning-for-Zero-shot-Deformable-Object-Manipulation"><a href="#Make-a-Donut-Language-Guided-Hierarchical-EMD-Space-Planning-for-Zero-shot-Deformable-Object-Manipulation" class="headerlink" title="Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation"></a>Make a Donut: Language-Guided Hierarchical EMD-Space Planning for Zero-shot Deformable Object Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02787">http://arxiv.org/abs/2311.02787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang You, Bokui Shen, Congyue Deng, Haoran Geng, He Wang, Leonidas Guibas</li>
<li>for: 这个论文的目的是解决机器人 manipulate 弹性对象的问题，这是机器人学中最吸引人又最困难的问题。</li>
<li>methods: 这个论文使用了大语言模型（LLM）来提出一个没有示范的层次规划方法，可以解决复杂的长期任务。每个阶段都有工具和子目标，使用了DiffPhysics-P2P损失函数和地球运动距离（EMD）空间来优化预测控制策略。</li>
<li>results: 实验结果表明，这种方法在糖体 manipulate 任务中表现出色，包括短期和长期任务。它还能够Robustly 扩展到未经示范的复杂任务。<details>
<summary>Abstract</summary>
Deformable object manipulation stands as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the tool and subgoal for a particular stage at our disposal, we present a granular closed-loop model predictive control strategy. This leverages Differentiable Physics with Point-to-Point correspondence (DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied iteratively. Experimental findings affirm that our technique surpasses multiple benchmarks in dough manipulation, spanning both short and long horizons. Remarkably, our model demonstrates robust generalization capabilities to novel and previously unencountered complex tasks without any preliminary demonstrations. We further substantiate our approach with experimental trials on real-world robotic platforms.
</details>
<details>
<summary>摘要</summary>
manipulate 非常复杂的物体stood as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the tool and subgoal for a particular stage at our disposal, we present a granular closed-loop model predictive control strategy. This leverages Differentiable Physics with Point-to-Point correspondence (DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied iteratively. Experimental findings affirm that our technique surpasses multiple benchmarks in dough manipulation, spanning both short and long horizons. Remarkably, our model demonstrates robust generalization capabilities to novel and previously unencountered complex tasks without any preliminary demonstrations. We further substantiate our approach with experimental trials on real-world robotic platforms.
</details></li>
</ul>
<hr>
<h2 id="Towards-Generic-Anomaly-Detection-and-Understanding-Large-scale-Visual-linguistic-Model-GPT-4V-Takes-the-Lead"><a href="#Towards-Generic-Anomaly-Detection-and-Understanding-Large-scale-Visual-linguistic-Model-GPT-4V-Takes-the-Lead" class="headerlink" title="Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead"></a>Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02782">http://arxiv.org/abs/2311.02782</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/caoyunkang/GPT4V-for-Generic-Anomaly-Detection">https://github.com/caoyunkang/GPT4V-for-Generic-Anomaly-Detection</a></li>
<li>paper_authors: Yunkang Cao, Xiaohao Xu, Chen Sun, Xiaonan Huang, Weiming Shen</li>
<li>for: 这个研究旨在应用GPT-4V（视力语言模型）来进行一般化的偏常探测任务。</li>
<li>methods: 这个研究使用GPT-4V模型进行多modal, multi-domain偏常探测任务，包括图像、影片、点 cloud和时间序列数据，并涵盖了不同的应用领域，如工业、医疗、逻辑、影片、3D偏常探测和位置任务。</li>
<li>results: GPT-4V在zero&#x2F;one-shot偏常探测中显示出了高效的探测和解释全球和细部 semantic 模式，实现了精准地区别 normal 和偏常的分别。<details>
<summary>Abstract</summary>
Anomaly detection is a crucial task across different domains and data types. However, existing anomaly detection models are often designed for specific domains and modalities. This study explores the use of GPT-4V(ision), a powerful visual-linguistic model, to address anomaly detection tasks in a generic manner. We investigate the application of GPT-4V in multi-modality, multi-domain anomaly detection tasks, including image, video, point cloud, and time series data, across multiple application areas, such as industrial, medical, logical, video, 3D anomaly detection, and localization tasks. To enhance GPT-4V's performance, we incorporate different kinds of additional cues such as class information, human expertise, and reference images as prompts.Based on our experiments, GPT-4V proves to be highly effective in detecting and explaining global and fine-grained semantic patterns in zero/one-shot anomaly detection. This enables accurate differentiation between normal and abnormal instances. Although we conducted extensive evaluations in this study, there is still room for future evaluation to further exploit GPT-4V's generic anomaly detection capacity from different aspects. These include exploring quantitative metrics, expanding evaluation benchmarks, incorporating multi-round interactions, and incorporating human feedback loops. Nevertheless, GPT-4V exhibits promising performance in generic anomaly detection and understanding, thus opening up a new avenue for anomaly detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>> anomaly detection 是一项重要的任务 across 不同的领域和数据类型。然而，现有的异常检测模型通常是为特定的领域和Modalities 设计的。本研究探索使用 GPT-4V（视力语言模型）来Address 异常检测任务的通用方式。我们 investigate GPT-4V 在多modal, multi-domain 异常检测任务中的应用，包括图像、视频、点云和时间序列数据，以及多个应用领域，如工业、医疗、逻辑、视频、3D 异常检测和位置定位任务。为了提高 GPT-4V 的表现，我们 incorporate 不同类型的额外提示，如类信息、人工智能和参考图像。根据我们的实验，GPT-4V 在检测和解释 Zero/one-shot 异常检测中表现出色，可以准确地分辨正常和异常实例。虽然我们进行了广泛的评估，但还有更多的可能性来自 GPT-4V 的通用异常检测能力。这些包括探索量化指标、扩展评估标准、 incorporating 多 Round Interactions 和 incorporating 人类反馈循环。然而，GPT-4V 在通用异常检测和理解方面表现出色，因此开启了一个新的途径 для异常检测。
</details></li>
</ul>
<hr>
<h2 id="ChaTA-Towards-an-Intelligent-Question-Answer-Teaching-Assistant-using-Open-Source-LLMs"><a href="#ChaTA-Towards-an-Intelligent-Question-Answer-Teaching-Assistant-using-Open-Source-LLMs" class="headerlink" title="ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs"></a>ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02775">http://arxiv.org/abs/2311.02775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yann Hicke, Anmol Agarwal, Qianou Ma, Paul Denny</li>
<li>for: 这篇论文目的是为了解决知识检索和智能问答（QA）中的扩展和智能化问题。</li>
<li>methods: 这篇论文使用了开源的大语言模型（LLM），以保持数据隐私。它使用了LLaMA-2家族模型，并应用了改进技术，包括检索增强生成（RAG）、监督微调（SFT）和人类反馈学习的替代方法（RLHF）。</li>
<li>results: 在一个 Piazza 数据集上，这篇论文通过人工评估和自动 LLlM 评估，发现了改进技术的共同作用，提高了答案质量约33%，并发现了RAG 是一个有力的添加。这项工作将为开发智能 QA 助手Customizable for 课程而铺平道路。<details>
<summary>Abstract</summary>
To address the challenges of scalable and intelligent question-answering (QA), we introduce an innovative solution that leverages open-source Large Language Models (LLMs) to ensure data privacy. We use models from the LLaMA-2 family and augmentations including retrieval augmented generation (RAG), supervised fine-tuning (SFT), and an alternative to reinforcement learning with human feedback (RLHF). We perform our experiments on a Piazza dataset from an introductory CS course with 10k QA pairs and 1.5k pairs of preferences data and conduct both human evaluations and automatic LLM evaluations on a small subset. We find preliminary evidence that modeling techniques collectively enhance the quality of answers by 33%, and RAG is an impactful addition. This work paves the way for the development of ChaTA, an intelligent QA assistant customizable for courses with an online QA platform.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Communication-Efficient-and-Privacy-Preserving-Federated-Learning-Based-on-Evolution-Strategies"><a href="#Communication-Efficient-and-Privacy-Preserving-Federated-Learning-Based-on-Evolution-Strategies" class="headerlink" title="Communication Efficient and Privacy-Preserving Federated Learning Based on Evolution Strategies"></a>Communication Efficient and Privacy-Preserving Federated Learning Based on Evolution Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03405">http://arxiv.org/abs/2311.03405</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Eric-Lan0/FedES">https://github.com/Eric-Lan0/FedES</a></li>
<li>paper_authors: Guangchen Lan</li>
<li>for: 这个研究旨在提出一种基于演化策略的联合学习算法（Federated Evolution Strategies，FedES），以便在分布式的深度神经网络（DNNs）训练中实现低通信负载和数据隐私。</li>
<li>methods: 这个研究使用了演化策略（Evolution Strategies）来实现联合学习，而不是传输模型参数。因此，它具有非常低的通信负载。此外，这个方法还可以保护数据隐私，因为第三方无法估算梯度 без knowing 预先共享的种子。</li>
<li>results: 实验结果显示，FedES 可以实现低通信负载和数据隐私，同时保持与反射方法相同的参数整合性。<details>
<summary>Abstract</summary>
Federated learning (FL) is an emerging paradigm for training deep neural networks (DNNs) in distributed manners. Current FL approaches all suffer from high communication overhead and information leakage. In this work, we present a federated learning algorithm based on evolution strategies (FedES), a zeroth-order training method. Instead of transmitting model parameters, FedES only communicates loss values, and thus has very low communication overhead. Moreover, a third party is unable to estimate gradients without knowing the pre-shared seed, which protects data privacy. Experimental results demonstrate FedES can achieve the above benefits while keeping convergence performance the same as that with back propagation methods.
</details>
<details>
<summary>摘要</summary>
Federated learning（FL）是一种新趋势的深度神经网络（DNNs）训练方法，现有的FL方法都受到高度通信开销和信息泄露的限制。在这项工作中，我们提出了基于进化策略（FedES）的 federated learning算法，而不是传输模型参数，FedES只在交换损失值，因此通信开销非常低。此外，第三方无法估计梯度，不知道预先分享的种子，因此保护了数据隐私。实验结果表明，FedES可以实现这些优点，同时保持与反向传播方法相同的凝结性能。Note: The translation is done using Google Translate and may not be perfect. Please let me know if you need further assistance or if you would like me to use a different translation tool.
</details></li>
</ul>
<hr>
<h2 id="Rule-Learning-as-Machine-Translation-using-the-Atomic-Knowledge-Bank"><a href="#Rule-Learning-as-Machine-Translation-using-the-Atomic-Knowledge-Bank" class="headerlink" title="Rule Learning as Machine Translation using the Atomic Knowledge Bank"></a>Rule Learning as Machine Translation using the Atomic Knowledge Bank</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02765">http://arxiv.org/abs/2311.02765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krisaesoey/atomictranslation">https://github.com/krisaesoey/atomictranslation</a></li>
<li>paper_authors: Kristoffer Æsøy, Ana Ozaki</li>
<li>for: 本研究旨在探讨使用机器学习模型进行逻辑推理是否可靠和可控的问题。</li>
<li>methods: 本研究使用 transformers 将自然语言中表达的规则翻译成逻辑规则，并使用逻辑推理工具进行逻辑推理。</li>
<li>results: 研究发现，使用 transformers 翻译自然语言中表达的规则可以生成可靠和可控的逻辑规则，并且可以用于逻辑推理。<details>
<summary>Abstract</summary>
Machine learning models, and in particular language models, are being applied to various tasks that require reasoning. While such models are good at capturing patterns their ability to reason in a trustable and controlled manner is frequently questioned. On the other hand, logic-based rule systems allow for controlled inspection and already established verification methods. However it is well-known that creating such systems manually is time-consuming and prone to errors. We explore the capability of transformers to translate sentences expressing rules in natural language into logical rules. We see reasoners as the most reliable tools for performing logical reasoning and focus on translating language into the format expected by such tools. We perform experiments using the DKET dataset from the literature and create a dataset for language to logic translation based on the Atomic knowledge bank.
</details>
<details>
<summary>摘要</summary>
机器学习模型，尤其是语言模型，在各种需要逻辑 reasoning 任务中应用。虽然这些模型能够捕捉模式，但其逻辑 reasoning 能力受到一定的质疑。然而，逻辑基础的规则系统具有可控的检查和已知的验证方法。然而，手动创建这些系统可能需要很长时间，并且容易出错。我们 investigate transformer 能力将自然语言中的句子翻译成逻辑规则。我们认为逻辑工具是逻辑 reasoning 最可靠的工具，因此我们将着眼于将语言翻译成这些工具所期望的格式。我们使用文献中的 DKET 数据集进行实验，并创建了基于 Atomic knowledge bank 的语言到逻辑翻译数据集。
</details></li>
</ul>
<hr>
<h2 id="Causal-Question-Answering-with-Reinforcement-Learning"><a href="#Causal-Question-Answering-with-Reinforcement-Learning" class="headerlink" title="Causal Question Answering with Reinforcement Learning"></a>Causal Question Answering with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02760">http://arxiv.org/abs/2311.02760</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Lukas Blübaum, Stefan Heindorf</li>
<li>for: 本研究的目的是回答 causal  вопро题，即寻找 causal 关系和其背景数据。</li>
<li>methods: 本文使用 reinforcement learning 方法，具体来说是 actor-critic 算法，来搜索 causal 关系和解释问题。</li>
<li>results: 本文的实验结果表明，使用 reinforcement learning 方法可以成功地回答 causal 问题，并且可以快速地搜索到解释问题的路径。<details>
<summary>Abstract</summary>
Causal questions inquire about causal relationships between different events or phenomena. Specifically, they often aim to determine whether there is a relationship between two phenomena, or to identify all causes/effects of a phenomenon. Causal questions are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with CauseNet, a large-scale dataset of causal relations and their provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on CauseNet for causal question answering. We introduce an Actor-Critic based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sparse rewards. Our evaluation shows that the agent successfully prunes the search space to answer binary causal questions by visiting less than 30 nodes per question compared to over 3,000 nodes by a naive breadth-first search. Our ablation study indicates that our supervised learning strategy provides a strong foundation upon which our reinforcement learning agent improves. The paths returned by our agent explain the mechanisms by which a cause produces an effect. Moreover, for each edge on a path, CauseNet stores its original source on the web allowing for easy verification of paths.
</details>
<details>
<summary>摘要</summary>
causal 问题查询的关系是两个或多个事件或现象之间的关系。特别是，它们通常想要确定两个现象之间是否存在关系，或者找出一个现象的所有原因。 causal 问题是各种用例中重要的，包括虚拟助手和搜索引擎。然而，许多当前的 causal 问题回答方法无法提供解释或证据。因此，在这篇论文中，我们使用 CauseNet，一个大规模的 causal 关系和其来源数据集，回答 causal 问题。以 reciprocal learning 的 inspiration，我们在 CauseNet 上应用 reciprocal learning 来回答 causal 问题。我们 introduce 一个 actor-critic 基于的搜索者，该搜索者可以在图上搜索以回答 causal 问题。我们使用一种监督学习过程来处理大的动作空间和罕见奖励。我们的评估显示，我们的搜索者可以成功地减少搜索空间，以回答 binary 的 causal 问题，每个问题只需访问 fewer than 30 个节点，而不是 naive 的广度优先搜索所需的 more than 3,000 个节点。我们的剥离研究表明，我们的监督学习策略提供了一个强大的基础，于而我们的 reciprocal learning 代理进行改进。 path 返回的 by our agent 解释了一个原因如何产生一个效果。此外，每个边在路径上，CauseNet 都将其原始来源保存在网上，以便轻松验证路径。
</details></li>
</ul>
<hr>
<h2 id="Learning-Independently-from-Causality-in-Multi-Agent-Environments"><a href="#Learning-Independently-from-Causality-in-Multi-Agent-Environments" class="headerlink" title="Learning Independently from Causality in Multi-Agent Environments"></a>Learning Independently from Causality in Multi-Agent Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02741">http://arxiv.org/abs/2311.02741</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Pina, Varuna De Silva, Corentin Artaud</li>
<li>for: 这个论文旨在研究多智能 Reinforcement Learning（MARL）领域中的懒散代理问题，并从 causality 的视角来 investigate 这个问题。</li>
<li>methods: 该论文使用了 fully decentralized MARL  setup，并使用了 causality 来链接个体观察和团队奖励。</li>
<li>results: 实验结果表明，通过使用 causality 来链接个体观察和团队奖励，可以提高独立代理的智能行为，并且帮助团队实现更好的性能。<details>
<summary>Abstract</summary>
Multi-Agent Reinforcement Learning (MARL) comprises an area of growing interest in the field of machine learning. Despite notable advances, there are still problems that require investigation. The lazy agent pathology is a famous problem in MARL that denotes the event when some of the agents in a MARL team do not contribute to the common goal, letting the teammates do all the work. In this work, we aim to investigate this problem from a causality-based perspective. We intend to create the bridge between the fields of MARL and causality and argue about the usefulness of this link. We study a fully decentralised MARL setup where agents need to learn cooperation strategies and show that there is a causal relation between individual observations and the team reward. The experiments carried show how this relation can be used to improve independent agents in MARL, resulting not only on better performances as a team but also on the rise of more intelligent behaviours on individual agents.
</details>
<details>
<summary>摘要</summary>
多智能机器学习（MARL）是一个快速发展的领域之一，尚未解决的问题仍然存在。懒散代理症是MARL领域中著名的问题，表示一些代理在MARL团队中不做贡献，让团队其他成员完成所有工作。在这种情况下，我们希望从 causality 的视角来调查这个问题。我们想要建立 MARL 和 causality 之间的桥梁，并讨论这种链接的有用性。我们研究了一个完全分散式的 MARL 设置，其中代理需要学习合作策略，并证明了个体观察与团队奖励之间存在 causal 关系。实验表明，这种关系可以用来改进独立的代理在 MARL 中的表现，不仅提高团队的性能，还使得代理的行为更加聪明。
</details></li>
</ul>
<hr>
<h2 id="AV-Lip-Sync-Leveraging-AV-HuBERT-to-Exploit-Multimodal-Inconsistency-for-Video-Deepfake-Detection"><a href="#AV-Lip-Sync-Leveraging-AV-HuBERT-to-Exploit-Multimodal-Inconsistency-for-Video-Deepfake-Detection" class="headerlink" title="AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Video Deepfake Detection"></a>AV-Lip-Sync+: Leveraging AV-HuBERT to Exploit Multimodal Inconsistency for Video Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02733">http://arxiv.org/abs/2311.02733</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahibzada Adil Shahzad, Ammarah Hashmi, Yan-Tsung Peng, Yu Tsao, Hsin-Min Wang</li>
<li>for: 防止伪造 multimedia 内容的传播，尤其是 fake news 和 false propaganda。</li>
<li>methods: 使用 multi-modal self-supervised learning (SSL) 特征提取器，捕捉视频和音频模式之间的不一致，进而实现多模式识别。</li>
<li>results: 比较所有现有模型，取得新的 state-of-the-art 性能在 FakeAVCeleb 和 DeepfakeTIMIT 数据集上。<details>
<summary>Abstract</summary>
Multimodal manipulations (also known as audio-visual deepfakes) make it difficult for unimodal deepfake detectors to detect forgeries in multimedia content. To avoid the spread of false propaganda and fake news, timely detection is crucial. The damage to either modality (i.e., visual or audio) can only be discovered through multi-modal models that can exploit both pieces of information simultaneously. Previous methods mainly adopt uni-modal video forensics and use supervised pre-training for forgery detection. This study proposes a new method based on a multi-modal self-supervised-learning (SSL) feature extractor to exploit inconsistency between audio and visual modalities for multi-modal video forgery detection. We use the transformer-based SSL pre-trained Audio-Visual HuBERT (AV-HuBERT) model as a visual and acoustic feature extractor and a multi-scale temporal convolutional neural network to capture the temporal correlation between the audio and visual modalities. Since AV-HuBERT only extracts visual features from the lip region, we also adopt another transformer-based video model to exploit facial features and capture spatial and temporal artifacts caused during the deepfake generation process. Experimental results show that our model outperforms all existing models and achieves new state-of-the-art performance on the FakeAVCeleb and DeepfakeTIMIT datasets.
</details>
<details>
<summary>摘要</summary>
多模态操作（也称为音频视频深伪）使得单模态深伪检测器很难检测多媒体内容中的伪造。为避免假新闻和假 пропаганда 的传播，实时检测是关键。过去的方法主要采用单模态视频科学和监测预训练来检测伪造。本研究提出了一种基于多模态自适应学（SSL）特征提取器来利用视频和音频模态之间的不一致来检测多模态视频伪造。我们使用基于 transformer 的 SSL 预训练 Audio-Visual HuBERT（AV-HuBERT）模型作为视觉和听音特征提取器，并使用多尺度时间卷积神经网络来捕捉音频和视觉模态之间的时间相关性。由于 AV-HuBERT 只提取视觉特征自唇部分，我们还采用另一种基于 transformer 的视频模型来利用脸部特征和捕捉深伪生成过程中的空间和时间偏差。实验结果表明，我们的模型在 FakeAVCeleb 和 DeepfakeTIMIT 数据集上的性能比所有现有模型高，实现了新的状态纪录水平。
</details></li>
</ul>
<hr>
<h2 id="Extraction-of-Atypical-Aspects-from-Customer-Reviews-Datasets-and-Experiments-with-Language-Models"><a href="#Extraction-of-Atypical-Aspects-from-Customer-Reviews-Datasets-and-Experiments-with-Language-Models" class="headerlink" title="Extraction of Atypical Aspects from Customer Reviews: Datasets and Experiments with Language Models"></a>Extraction of Atypical Aspects from Customer Reviews: Datasets and Experiments with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02702">http://arxiv.org/abs/2311.02702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smitanannaware/xtrata">https://github.com/smitanannaware/xtrata</a></li>
<li>paper_authors: Smita Nannaware, Erfan Al-Hossami, Razvan Bunescu</li>
<li>for: 本研究的目的是检测顾客评论中的非常规方面，以便提高用户满意度。</li>
<li>methods: 本研究使用了人工批注 benchmark 数据集，以评估不同语言模型的性能。</li>
<li>results: 研究发现，使用 Flan-T5 进行 fine-tuning 和 GPT-3.5 的零shot 和几 shot 提示可以准确检测顾客评论中的非常规方面。<details>
<summary>Abstract</summary>
A restaurant dinner may become a memorable experience due to an unexpected aspect enjoyed by the customer, such as an origami-making station in the waiting area. If aspects that are atypical for a restaurant experience were known in advance, they could be leveraged to make recommendations that have the potential to engender serendipitous experiences, further increasing user satisfaction. Although relatively rare, whenever encountered, atypical aspects often end up being mentioned in reviews due to their memorable quality. Correspondingly, in this paper we introduce the task of detecting atypical aspects in customer reviews. To facilitate the development of extraction models, we manually annotate benchmark datasets of reviews in three domains - restaurants, hotels, and hair salons, which we use to evaluate a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and few-shot prompting of GPT-3.5.
</details>
<details>
<summary>摘要</summary>
餐厅晚餐可能变成一个深刻的记忆，因为顾客在等待区域内感受到了一个意外的元素，如 Origami 制作站。如果在餐厅经验中不寻常的方面知道在先，可以利用这些方面来提供建议，以便在用户满意度方面产生巧合体验。虽然这些不寻常的方面相对罕见，但当遇到时，它们通常会被评论中提及，因为它们具有深刻的特点。在这篇论文中，我们介绍了检测顾客评论中不寻常的方面的任务。为了促进EXTRACTION 模型的开发，我们手动标注了多个领域的客评论 benchmark 数据集 - 餐厅、酒店和美发店，并使用这些数据集来评估多种语言模型，从 fine-tuning Flan-T5 到零shot 和几shot 的 GPT-3.5 的Prompting。
</details></li>
</ul>
<hr>
<h2 id="Architecture-Matters-Uncovering-Implicit-Mechanisms-in-Graph-Contrastive-Learning"><a href="#Architecture-Matters-Uncovering-Implicit-Mechanisms-in-Graph-Contrastive-Learning" class="headerlink" title="Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning"></a>Architecture Matters: Uncovering Implicit Mechanisms in Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02687">http://arxiv.org/abs/2311.02687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojun Guo, Yifei Wang, Zeming Wei, Yisen Wang</li>
<li>for: 研究 graph contrastive learning (GCL) 方法的系统性特点，包括Positive samples 不是必须的、negative samples 不需要 для图类别或节点类别，以及数据增强对 GCL 的影响较小。</li>
<li>methods: 研究如何通过理解 GNN 的隐式概念偏好来解释 GCL 方法的特点。</li>
<li>results: 发现 GCL 方法的特点与传统的 visual contrastive learning (VCL) 方法有很大差异，包括Positive samples 不是必须的、negative samples 不需要 для图类别或节点类别，以及数据增强对 GCL 的影响较小。<details>
<summary>Abstract</summary>
With the prosperity of contrastive learning for visual representation learning (VCL), it is also adapted to the graph domain and yields promising performance. However, through a systematic study of various graph contrastive learning (GCL) methods, we observe that some common phenomena among existing GCL methods that are quite different from the original VCL methods, including 1) positive samples are not a must for GCL; 2) negative samples are not necessary for graph classification, neither for node classification when adopting specific normalization modules; 3) data augmentations have much less influence on GCL, as simple domain-agnostic augmentations (e.g., Gaussian noise) can also attain fairly good performance. By uncovering how the implicit inductive bias of GNNs works in contrastive learning, we theoretically provide insights into the above intriguing properties of GCL. Rather than directly porting existing VCL methods to GCL, we advocate for more attention toward the unique architecture of graph learning and consider its implicit influence when designing GCL methods. Code is available at https: //github.com/PKU-ML/ArchitectureMattersGCL.
</details>
<details>
<summary>摘要</summary>
With the prosperity of contrastive learning for visual representation learning (VCL), it has also been adapted to the graph domain and has shown promising performance. However, through a systematic study of various graph contrastive learning (GCL) methods, we observe that some common phenomena exist among existing GCL methods that are quite different from the original VCL methods, including:1. Positive samples are not a must for GCL.2. Negative samples are not necessary for graph classification, nor for node classification when using specific normalization modules.3. Data augmentations have much less influence on GCL, as simple domain-agnostic augmentations (e.g., Gaussian noise) can also achieve fairly good performance.By uncovering how the implicit inductive bias of GNNs works in contrastive learning, we provide theoretical insights into the above intriguing properties of GCL. Rather than directly porting existing VCL methods to GCL, we advocate for more attention to be paid to the unique architecture of graph learning and consider its implicit influence when designing GCL methods. Code is available at: <https://github.com/PKU-ML/ArchitectureMattersGCL>.
</details></li>
</ul>
<hr>
<h2 id="Compute-at-Scale-–-A-Broad-Investigation-into-the-Data-Center-Industry"><a href="#Compute-at-Scale-–-A-Broad-Investigation-into-the-Data-Center-Industry" class="headerlink" title="Compute at Scale – A Broad Investigation into the Data Center Industry"></a>Compute at Scale – A Broad Investigation into the Data Center Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02651">http://arxiv.org/abs/2311.02651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Pilz, Lennart Heim</li>
<li>for: 这份报告描述了数据中心行业的现状和其对人工智能发展的重要性。</li>
<li>methods: 报告使用了大量的计算资源和网络连接来描述数据中心的特点和业务模式。</li>
<li>results: 根据报告，全球数据中心市场的价值约为2500亿美元，预计在下一个七年内将双倍增长。同时，全球大约有500个大型数据中心（每个MW电压），美国、欧洲和中国是最重要的市场。<details>
<summary>Abstract</summary>
This report characterizes the data center industry and its importance for AI development. Data centers are industrial facilities that efficiently provide compute at scale and thus constitute the engine rooms of today's digital economy. As large-scale AI training and inference become increasingly computationally expensive, they are dominantly executed from this designated infrastructure. Key features of data centers include large-scale compute clusters that require extensive cooling and consume large amounts of power, the need for fast connectivity both within the data center and to the internet, and an emphasis on security and reliability. The global industry is valued at approximately $250B and is expected to double over the next seven years. There are likely about 500 large (above 10 MW) data centers globally, with the US, Europe, and China constituting the most important markets. The report further covers important actors, business models, main inputs, and typical locations of data centers.
</details>
<details>
<summary>摘要</summary>
这份报告描述了数据中心业和其对人工智能发展的重要性。数据中心是大规模计算的工业设施，它们提供大规模计算能力，并因此成为当今数字经济的发动机。随着大规模人工智能训练和推断变得越来越计算昂贵，它们主要在这些指定的基础设施上进行执行。数据中心的主要特点包括大规模计算集群，需要广泛的冷却和大量的电力，快速的内部连接和互联网连接，以及安全性和可靠性的强调。全球业态估价约2500亿美元，预计在下一个七年内将 doubles。全球可能有约500个大于10MW的数据中心，美国、欧洲和中国是最重要的市场。报告还涵盖了重要的actor、业务模式、主要输入和典型的数据中心所在地。
</details></li>
</ul>
<hr>
<h2 id="New-Approach-for-an-Affective-Computing-Driven-Quality-of-Experience-QoE-Prediction"><a href="#New-Approach-for-an-Affective-Computing-Driven-Quality-of-Experience-QoE-Prediction" class="headerlink" title="New Approach for an Affective Computing-Driven Quality of Experience (QoE) Prediction"></a>New Approach for an Affective Computing-Driven Quality of Experience (QoE) Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02647">http://arxiv.org/abs/2311.02647</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joshua Bègue, Mohamed Aymen Labiod, Abdelhamid Melloulk<br>for:这篇论文旨在提出一种基于情感计算的 качество经验（QoE）预测模型，以便在多媒体QoE评估场景下提高体验质量。methods:这篇论文使用了计算多核电enzephalogram（EEG）信息，并使用了差异 entropy和功率 спектル密度来特征提取。然后，使用深度学习模型来研究是否可以通过这些特征来预测QoE。results:这篇论文使用了一个公共可用的数据集，并使用了多个深度学习模型来研究QoE预测的可能性。结果显示，使用LSTM模型可以获得最好的结果，其中F1分数在68%到78%之间。此外，分析表明，Delta频率带是最不必要的，两个电极具有更高的重要性，而两个电极具有很低的影响。<details>
<summary>Abstract</summary>
In human interactions, emotion recognition is crucial. For this reason, the topic of computer-vision approaches for automatic emotion recognition is currently being extensively researched. Processing multi-channel electroencephalogram (EEG) information is one of the most researched methods for automatic emotion recognition. This paper presents a new model for an affective computing-driven Quality of Experience (QoE) prediction. In order to validate the proposed model, a publicly available dataset is used. The dataset contains EEG, ECG, and respiratory data and is focused on a multimedia QoE assessment context. The EEG data are retained on which the differential entropy and the power spectral density are calculated with an observation window of three seconds. These two features were extracted to train several deep-learning models to investigate the possibility of predicting QoE with five different factors. The performance of these models is compared, and the best model is optimized to improve the results. The best results were obtained with an LSTM-based model, presenting an F1-score from 68% to 78%. An analysis of the model and its features shows that the Delta frequency band is the least necessary, that two electrodes have a higher importance, and that two other electrodes have a very low impact on the model's performances.
</details>
<details>
<summary>摘要</summary>
人际交互中，情感认知是非常重要的。因此，计算机视觉方法自动情感认知的研究在当前已经非常广泛。处理多通道电enzephalogram（EEG）信息是最广泛研究的方法。这篇论文提出了一种新的情感计算驱动的品质经验（QoE）预测模型。为验证提议的模型，使用了一个公共可用的数据集。该数据集包括EEG、ECG和呼吸数据，并且是关于多媒体QoE评估上下文。EEG数据上计算了差异积分和功率spectral density，使用观察窗口为3秒。这两个特征用于训练多个深度学习模型，以 investigate可能通过五个因素预测QoE。模型的性能相比，LSTM模型显示最佳结果，其F1得分在68%到78%之间。分析模型和其特征显示，Delta频率带是最不必要的，两个电极有更高的重要性，两个电极具有很低的影响。
</details></li>
</ul>
<hr>
<h2 id="PotholeGuard-A-Pothole-Detection-Approach-by-Point-Cloud-Semantic-Segmentation"><a href="#PotholeGuard-A-Pothole-Detection-Approach-by-Point-Cloud-Semantic-Segmentation" class="headerlink" title="PotholeGuard: A Pothole Detection Approach by Point Cloud Semantic Segmentation"></a>PotholeGuard: A Pothole Detection Approach by Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02641">http://arxiv.org/abs/2311.02641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sahil Nawale, Dhruv Khut, Daksh Dave, Gauransh Sawhney, Pushkar Aggrawal, Dr. Kailas Devadakar</li>
<li>for: 本研究旨在提供一种robust和准确的3D破洞 segmentation方法，用于道路安全维护。</li>
<li>methods: 该方法使用点云图像 segmentation，并提供了一种新的点云缺失缓解机制，以及一种本地关系学习模块，以提高本地特征表示。</li>
<li>results: 对三个公共数据集进行了广泛的实验，并证明了PotholeGuard方法在现有方法中的超越性。<details>
<summary>Abstract</summary>
Pothole detection is crucial for road safety and maintenance, traditionally relying on 2D image segmentation. However, existing 3D Semantic Pothole Segmentation research often overlooks point cloud sparsity, leading to suboptimal local feature capture and segmentation accuracy. Our research presents an innovative point cloud-based pothole segmentation architecture. Our model efficiently identifies hidden features and uses a feedback mechanism to enhance local characteristics, improving feature presentation. We introduce a local relationship learning module to understand local shape relationships, enhancing structural insights. Additionally, we propose a lightweight adaptive structure for refining local point features using the K nearest neighbor algorithm, addressing point cloud density differences and domain selection. Shared MLP Pooling is integrated to learn deep aggregation features, facilitating semantic data exploration and segmentation guidance. Extensive experiments on three public datasets confirm PotholeGuard's superior performance over state-of-the-art methods. Our approach offers a promising solution for robust and accurate 3D pothole segmentation, with applications in road maintenance and safety.
</details>
<details>
<summary>摘要</summary>
《破洞检测是公路安全和维护中非常重要的一环，传统上靠的是2D图像分割。然而，现有的3D语义破洞分割研究经常忽略点云稀疏性，导致本地特征捕捉和分割精度受到限制。我们的研究提出了一种创新的点云基于的破洞分割建筑。我们的模型能够高效发现隐藏的特征，并使用反馈机制来增强本地特征，提高特征表现。我们引入了地方关系学习模块，以更好地理解地方形态关系，提高结构性能。此外，我们提议了一种轻量级适应结构，通过K最近邻近算法来调整本地点特征，解决点云密度差异和领域选择问题。我们将 Shared MLP Pooling 集成到整个模型中，以学习深度聚合特征，促进 semantic 数据探索和分割引导。我们的方法在三个公共数据集上进行了广泛的实验，并证明了 PotholeGuard 的超过状态艺术方法的性能。我们的方法可以为公路维护和安全带来一个可靠和准确的3D破洞分割解决方案。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Promise-and-Pitfalls-of-ChatGPT-for-Automated-Code-Generation"><a href="#Assessing-the-Promise-and-Pitfalls-of-ChatGPT-for-Automated-Code-Generation" class="headerlink" title="Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation"></a>Assessing the Promise and Pitfalls of ChatGPT for Automated Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02640">http://arxiv.org/abs/2311.02640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dsaatusu/chatgpt-promises-and-pitfalls">https://github.com/dsaatusu/chatgpt-promises-and-pitfalls</a></li>
<li>paper_authors: Muhammad Fawad Akbar Khan, Max Ramsdell, Erik Falor, Hamid Karimi</li>
<li>for: 评估 chatGPT 大语言模型在代码生成方面的能力，并与人工程师的代码相比。</li>
<li>methods: 使用了一个新的代码生成数据集，并对 chatGPT 和人工程师的代码进行了比较性评估，以评估 chatGPT 的代码生成能力。</li>
<li>results: 显示了 chatGPT 在数据分析任务中的强大能力（准确率为 93.1%），但在视觉graphical挑战中存在局限性。 chatGPT 的代码具有较高的含义性和安全性，且倾向于使用模块化设计和更好的错误处理。 机器学习模型也能够准确地分辨出 chatGPT 的代码和人工程师的代码之间的差异。<details>
<summary>Abstract</summary>
This paper presents a comprehensive evaluation of the code generation capabilities of ChatGPT, a prominent large language model, compared to human programmers. A novel dataset of 131 code-generation prompts across 5 categories was curated to enable robust analysis. Code solutions were generated by both ChatGPT and humans for all prompts, resulting in 262 code samples. A meticulous manual assessment methodology prioritized evaluating correctness, comprehensibility, and security using 14 established code quality metrics. The key findings reveal ChatGPT's strengths in crafting concise, efficient code with advanced constructs, showcasing strengths in data analysis tasks (93.1% accuracy) but limitations in visual-graphical challenges. Comparative analysis with human code highlights ChatGPT's inclination towards modular design and superior error handling. Additionally, machine learning models effectively distinguished ChatGPT from human code with up to 88% accuracy, suggesting detectable coding style disparities. By providing profound insights into ChatGPT's code generation capabilities and limitations through quantitative metrics and qualitative analysis, this study makes valuable contributions toward advancing AI-based programming assistants. The curated dataset and methodology offer a robust foundation for future research in this nascent domain. All data and codes are available on https://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls.
</details>
<details>
<summary>摘要</summary>
The key findings show that ChatGPT excels in crafting concise and efficient code with advanced constructs, particularly in data analysis tasks (93.1% accuracy). However, it struggles with visual-graphical challenges. Comparative analysis with human code reveals that ChatGPT tends towards modular design and superior error handling. Additionally, machine learning models can accurately distinguish ChatGPT code from human code (up to 88% accuracy), indicating detectable differences in coding style.This study provides valuable insights into ChatGPT's code generation capabilities and limitations through quantitative metrics and qualitative analysis. The curated dataset and methodology serve as a robust foundation for future research in this emerging field. All data and codes are available on GitHub (https://github.com/DSAatUSU/ChatGPT-promises-and-pitfalls).
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Perceptual-Pre-trained-Model-for-Complex-Trajectory-Recovery"><a href="#A-Critical-Perceptual-Pre-trained-Model-for-Complex-Trajectory-Recovery" class="headerlink" title="A Critical Perceptual Pre-trained Model for Complex Trajectory Recovery"></a>A Critical Perceptual Pre-trained Model for Complex Trajectory Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02631">http://arxiv.org/abs/2311.02631</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dedong Li, Ziyue Li, Zhishuai Li, Lei Bai, Qingyuan Gong, Lijun Sun, Wolfgang Ketter, Rui Zhao</li>
<li>For: 提高复杂路径轨迹恢复的精度， especialmente 处理跨度远 road segment 和多个拐弯的情况。* Methods: 使用顺序语言模型在预训练 manner 学习道路段表示 вектор，并提出了多视图图文件和复杂度意识Transformer（MGCAT）模型，可以在轨迹预训练中 adaptively 聚合多视图图文件特征，以及增强关键节点的注意力。* Results: 对大规模数据集进行了广泛的实验，结果表明，我们的方法可以更好地学习轨迹恢复的表示，全体 F1 分数提高 5.22%，特别是复杂轨迹 F1 分数提高 8.16%。<details>
<summary>Abstract</summary>
The trajectory on the road traffic is commonly collected at a low sampling rate, and trajectory recovery aims to recover a complete and continuous trajectory from the sparse and discrete inputs. Recently, sequential language models have been innovatively adopted for trajectory recovery in a pre-trained manner: it learns road segment representation vectors, which will be used in the downstream tasks. However, existing methods are incapable of handling complex trajectories: when the trajectory crosses remote road segments or makes several turns, which we call critical nodes, the quality of learned representations deteriorates, and the recovered trajectories skip the critical nodes. This work is dedicated to offering a more robust trajectory recovery for complex trajectories. Firstly, we define the trajectory complexity based on the detour score and entropy score and construct the complexity-aware semantic graphs correspondingly. Then, we propose a Multi-view Graph and Complexity Aware Transformer (MGCAT) model to encode these semantics in trajectory pre-training from two aspects: 1) adaptively aggregate the multi-view graph features considering trajectory pattern, and 2) higher attention to critical nodes in a complex trajectory. Such that, our MGCAT is perceptual when handling the critical scenario of complex trajectories. Extensive experiments are conducted on large-scale datasets. The results prove that our method learns better representations for trajectory recovery, with 5.22% higher F1-score overall and 8.16% higher F1-score for complex trajectories particularly. The code is available at https://github.com/bonaldli/ComplexTraj.
</details>
<details>
<summary>摘要</summary>
trajectory 在路况资料收集时通常采集得非常低， trajectory 恢复目标是恢复完整、连续的 trajectory 从稀疏、离散输入中。 最近，序列语言模型在预训练方式下被创新地应用于 trajectory 恢复中，它学习了路段表示 вектор，这些 вектор 将在下游任务中使用。然而，现有方法无法处理复杂的 trajectory：当 trajectory 过 remote 路段或多次转弯时，学习的表示质量会下降， recovered  trajectory 会跳过关键节点。这项工作旨在提供更加稳定的 trajectory 恢复方法，能够处理复杂的 trajectory。我们首先定义 trajectory 的复杂性基于拐弯分数和 entropy 分数，并构建了相应的复杂性意识图。然后，我们提出了多视图图和复杂性意识图 transformer（MGCAT）模型，用于在 trajectory 预训练中编码这些semantics。MGCAT 模型通过以下两种方式来编码这些semantics：1）适应性地集合多视图图特征，考虑 trajectory 模式；2）在复杂的 trajectory 中高优先级关注关键节点。这样，我们的 MGCAT 在处理复杂的 trajectory 时具有较高的感知性。我们在大规模数据集上进行了广泛的实验，结果表明我们的方法可以更好地学习 trajectory 恢复的表示，全局 F1 分数提高 5.22%，而复杂 trajectory 的 F1 分数提高 8.16%。代码可以在 <https://github.com/bonaldli/ComplexTraj> 上获取。
</details></li>
</ul>
<hr>
<h2 id="The-New-Frontier-of-Cybersecurity-Emerging-Threats-and-Innovations"><a href="#The-New-Frontier-of-Cybersecurity-Emerging-Threats-and-Innovations" class="headerlink" title="The New Frontier of Cybersecurity: Emerging Threats and Innovations"></a>The New Frontier of Cybersecurity: Emerging Threats and Innovations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02630">http://arxiv.org/abs/2311.02630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daksh Dave, Gauransh Sawhney, Pushkar Aggarwal, Nitish Silswal, Dhruv Khut</li>
<li>for: 这项研究旨在全面检讨Cybersecurity领域的多种威胁，包括Malware攻击、社会工程攻击、网络漏洞和数据泄露等四类威胁。</li>
<li>methods: 本研究采用资深研究方法，检讨了这些威胁对个人、组织和社会的影响。</li>
<li>results: 研究发现了一系列新兴Cybersecurity威胁，包括高级 persistente攻击、劫持攻击、物联网（IoT）漏洞和社会工程攻击等。这些威胁对组织和个人都构成了严重的风险。因此，需要采取多层防御措施，包括强大的安全措施、全面的员工培训和定期的安全审核。<details>
<summary>Abstract</summary>
In today's digitally interconnected world, cybersecurity threats have reached unprecedented levels, presenting a pressing concern for individuals, organizations, and governments. This study employs a qualitative research approach to comprehensively examine the diverse threats of cybersecurity and their impacts across various sectors. Four primary categories of threats are identified and analyzed, encompassing malware attacks, social engineering attacks, network vulnerabilities, and data breaches. The research delves into the consequences of these threats on individuals, organizations, and society at large. The findings reveal a range of key emerging threats in cybersecurity, including advanced persistent threats, ransomware attacks, Internet of Things (IoT) vulnerabilities, and social engineering exploits. Consequently, it is evident that emerging cybersecurity threats pose substantial risks to both organizations and individuals. The sophistication and diversity of these emerging threats necessitate a multi-layered approach to cybersecurity. This approach should include robust security measures, comprehensive employee training, and regular security audits. The implications of these emerging threats are extensive, with potential consequences such as financial loss, reputational damage, and compromised personal information. This study emphasizes the importance of implementing effective measures to mitigate these threats. It highlights the significance of using strong passwords, encryption methods, and regularly updating software to bolster cyber defenses.
</details>
<details>
<summary>摘要</summary>
The research reveals a range of key emerging threats in cybersecurity, including advanced persistent threats, ransomware attacks, Internet of Things (IoT) vulnerabilities, and social engineering exploits. These emerging threats pose substantial risks to both organizations and individuals, with potential consequences such as financial loss, reputational damage, and compromised personal information.To mitigate these threats, this study emphasizes the importance of implementing effective measures, such as robust security measures, comprehensive employee training, and regular security audits. Additionally, using strong passwords, encryption methods, and regularly updating software can help bolster cyber defenses. The sophistication and diversity of emerging threats necessitate a multi-layered approach to cybersecurity.The findings of this study have far-reaching implications, highlighting the significance of taking proactive measures to protect against cyber threats. With the increasing dependence on digital technologies, it is essential to stay vigilant and adapt to the evolving landscape of cyber threats. By prioritizing cybersecurity, individuals and organizations can minimize the risks of financial loss, reputational damage, and compromised personal information.
</details></li>
</ul>
<hr>
<h2 id="AIOps-Driven-Enhancement-of-Log-Anomaly-Detection-in-Unsupervised-Scenarios"><a href="#AIOps-Driven-Enhancement-of-Log-Anomaly-Detection-in-Unsupervised-Scenarios" class="headerlink" title="AIOps-Driven Enhancement of Log Anomaly Detection in Unsupervised Scenarios"></a>AIOps-Driven Enhancement of Log Anomaly Detection in Unsupervised Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02621">http://arxiv.org/abs/2311.02621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daksh Dave, Gauransh Sawhney, Dhruv Khut, Sahil Nawale, Pushkar Aggrawal, Prasenjit Bhavathankar</li>
<li>for: 本研究旨在提高AIOps平台中的日志异常检测效果，填补现有研究空白。</li>
<li>methods: 本研究提出了一种新型的混合方法，结合了不监督学习策略，包括原始数据处理和人工神经网络。</li>
<li>results: 实验结果表明，提出的方法可以减少 Pseudo-positive 的数量，并且可以处理日志在原始、未处理的形式下进行分析。<details>
<summary>Abstract</summary>
Artificial intelligence operations (AIOps) play a pivotal role in identifying, mitigating, and analyzing anomalous system behaviors and alerts. However, the research landscape in this field remains limited, leaving significant gaps unexplored. This study introduces a novel hybrid framework through an innovative algorithm that incorporates an unsupervised strategy. This strategy integrates Principal Component Analysis (PCA) and Artificial Neural Networks (ANNs) and uses a custom loss function to substantially enhance the effectiveness of log anomaly detection. The proposed approach encompasses the utilization of both simulated and real-world datasets, including logs from SockShop and Hadoop Distributed File System (HDFS). The experimental results are highly promising, demonstrating significant reductions in pseudo-positives. Moreover, this strategy offers notable advantages, such as the ability to process logs in their raw, unprocessed form, and the potential for further enhancements. The successful implementation of this approach showcases a remarkable reduction in anomalous logs, thus unequivocally establishing the efficacy of the proposed methodology. Ultimately, this study makes a substantial contribution to the advancement of log anomaly detection within AIOps platforms, addressing the critical need for effective and efficient log analysis in modern and complex systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Get-the-Ball-Rolling-Alerting-Autonomous-Robots-When-to-Help-to-Close-the-Healthcare-Loop"><a href="#Get-the-Ball-Rolling-Alerting-Autonomous-Robots-When-to-Help-to-Close-the-Healthcare-Loop" class="headerlink" title="Get the Ball Rolling: Alerting Autonomous Robots When to Help to Close the Healthcare Loop"></a>Get the Ball Rolling: Alerting Autonomous Robots When to Help to Close the Healthcare Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02602">http://arxiv.org/abs/2311.02602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Shen, Yanyao Liu, Ziming Wang, Ziyuan Jiao, Yufeng Chen, Wenjuan Han</li>
<li>for: 推动健康Robot研究 без人类干预或指令，提出自主帮助挑战和大规模人均数据采集。</li>
<li>methods: 提出健康Robot拥有自动确定帮助需要、生成有用子任务、通过物理机器执行计划、接受环境反馈以生成新任务的能力。</li>
<li>results: 解决开放场景中自主任务生成、现场与静止常识之间的漏洞、语言指令与实际世界之间的漏洞等挑战，并提出Helpy方法尝试填补健康循环学习无需人类干预的情况。<details>
<summary>Abstract</summary>
To facilitate the advancement of research in healthcare robots without human intervention or commands, we introduce the Autonomous Helping Challenge, along with a crowd-sourcing large-scale dataset. The goal is to create healthcare robots that possess the ability to determine when assistance is necessary, generate useful sub-tasks to aid in planning, carry out these plans through a physical robot, and receive feedback from the environment in order to generate new tasks and continue the process. Besides the general challenge in open-ended scenarios, Autonomous Helping focuses on three specific challenges: autonomous task generation, the gap between the current scene and static commonsense, and the gap between language instruction and the real world. Additionally, we propose Helpy, a potential approach to close the healthcare loop in the learning-free setting.
</details>
<details>
<summary>摘要</summary>
为了推动医疗机器人自主研究的发展，我们提出了无人指导的帮助挑战，同时发布了大规模的人类参与评估数据集。我们的目标是创造一种具有自动确定帮助需求、生成有用子任务、执行 física robot 计划、并接受环境反馈以生成新任务的医疗机器人。除了开放场景中的总体挑战外，Autonomous Helping 特点在于三个特定挑战：自动任务生成、现场与静态常识之间的差距、以及语言指令与实际世界之间的差距。此外，我们提出了一种可能的方法来在无学习设定下关闭医疗循环——Helpy。
</details></li>
</ul>
<hr>
<h2 id="Automated-Camera-Calibration-via-Homography-Estimation-with-GNNs"><a href="#Automated-Camera-Calibration-via-Homography-Estimation-with-GNNs" class="headerlink" title="Automated Camera Calibration via Homography Estimation with GNNs"></a>Automated Camera Calibration via Homography Estimation with GNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02598">http://arxiv.org/abs/2311.02598</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giacomo D’Amicantonio, Egor Bondarev, Peter H. N. De With</li>
<li>for: 这篇论文是为了提高交通监测系统中相机的准确性和自动化调整而提出的一种新方法。</li>
<li>methods: 该方法基于图граaph neural networks，使用 bird’s-eye-view 图像生成交叉口视角图像集合，并使用这些图像集合学习拓扑结构，从而估算出Homography矩阵。</li>
<li>results: 该方法在实验中表现出色，在真实世界相机上取得了最高精度的准确性和自动化调整。<details>
<summary>Abstract</summary>
Over the past few decades, a significant rise of camera-based applications for traffic monitoring has occurred. Governments and local administrations are increasingly relying on the data collected from these cameras to enhance road safety and optimize traffic conditions. However, for effective data utilization, it is imperative to ensure accurate and automated calibration of the involved cameras. This paper proposes a novel approach to address this challenge by leveraging the topological structure of intersections. We propose a framework involving the generation of a set of synthetic intersection viewpoint images from a bird's-eye-view image, framed as a graph of virtual cameras to model these images. Using the capabilities of Graph Neural Networks, we effectively learn the relationships within this graph, thereby facilitating the estimation of a homography matrix. This estimation leverages the neighbourhood representation for any real-world camera and is enhanced by exploiting multiple images instead of a single match. In turn, the homography matrix allows the retrieval of extrinsic calibration parameters. As a result, the proposed framework demonstrates superior performance on both synthetic datasets and real-world cameras, setting a new state-of-the-art benchmark.
</details>
<details>
<summary>摘要</summary>
We propose a framework that involves generating a set of synthetic intersection viewpoint images from a bird's-eye-view image, framed as a graph of virtual cameras to model these images. Using the capabilities of Graph Neural Networks, we effectively learn the relationships within this graph, thereby facilitating the estimation of a homography matrix. This estimation leverages the neighborhood representation for any real-world camera and is enhanced by exploiting multiple images instead of a single match. In turn, the homography matrix allows the retrieval of extrinsic calibration parameters.As a result, the proposed framework demonstrates superior performance on both synthetic datasets and real-world cameras, setting a new state-of-the-art benchmark.
</details></li>
</ul>
<hr>
<h2 id="FloodBrain-Flood-Disaster-Reporting-by-Web-based-Retrieval-Augmented-Generation-with-an-LLM"><a href="#FloodBrain-Flood-Disaster-Reporting-by-Web-based-Retrieval-Augmented-Generation-with-an-LLM" class="headerlink" title="FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM"></a>FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented Generation with an LLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02597">http://arxiv.org/abs/2311.02597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grace Colverd, Paul Darm, Leonard Silverberg, Noah Kasmanoff</li>
<li>for: 急需快速灾害影响报告，以便规划人道援助。</li>
<li>methods: 利用大量语言模型（LLMs）实现文本生成和问题解答等功能，并将资讯EXTRACT AND CURATE FROM THE WEB以生成灾害影响报告。</li>
<li>results: 比较不同的大量语言模型（LLMs）的生成报告和人工撰写的报告，发现与人工评分相似的相关性。另外，透过组件分析，发现单一管道元件的重要性。以增进大量语言模型的应用，减少灾害发生后的协调时间。<details>
<summary>Abstract</summary>
Fast disaster impact reporting is crucial in planning humanitarian assistance. Large Language Models (LLMs) are well known for their ability to write coherent text and fulfill a variety of tasks relevant to impact reporting, such as question answering or text summarization. However, LLMs are constrained by the knowledge within their training data and are prone to generating inaccurate, or "hallucinated", information. To address this, we introduce a sophisticated pipeline embodied in our tool FloodBrain (floodbrain.com), specialized in generating flood disaster impact reports by extracting and curating information from the web. Our pipeline assimilates information from web search results to produce detailed and accurate reports on flood events. We test different LLMs as backbones in our tool and compare their generated reports to human-written reports on different metrics. Similar to other studies, we find a notable correlation between the scores assigned by GPT-4 and the scores given by human evaluators when comparing our generated reports to human-authored ones. Additionally, we conduct an ablation study to test our single pipeline components and their relevancy for the final reports. With our tool, we aim to advance the use of LLMs for disaster impact reporting and reduce the time for coordination of humanitarian efforts in the wake of flood disasters.
</details>
<details>
<summary>摘要</summary>
快速灾害影响报告是紧急 situations 规划人道援助的关键。大型自然语言模型（LLM）因其可以生成协调的文本和完成多种与影响报告相关的任务，如问答或文本摘要。然而，LLM 受训数据中知识的限制，容易生成错误或 "幻见" 信息。为解决这问题，我们提出了一个复杂的管道，并在我们的工具 FloodBrain (floodbrain.com) 中实现了生成洪水灾害影响报告。我们的管道从网络搜索结果中提取和筛选信息，生成详细和准确的洪水事件报告。我们测试了不同的 LLM 作为管道的脑库，并与人类评估器的评分相比较。与其他研究相似，我们发现了 GPT-4 的评分和人类评估器的评分之间存在显著的相关性。此外，我们还进行了减少学Component 的研究，以评估它们在最终报告中的重要性。我们的工具 aim 是使用 LLM 进行灾害影响报告，提高人道援助协调的效率，并减少洪水灾害后的协调时间。
</details></li>
</ul>
<hr>
<h2 id="scBeacon-single-cell-biomarker-extraction-via-identifying-paired-cell-clusters-across-biological-conditions-with-contrastive-siamese-networks"><a href="#scBeacon-single-cell-biomarker-extraction-via-identifying-paired-cell-clusters-across-biological-conditions-with-contrastive-siamese-networks" class="headerlink" title="scBeacon: single-cell biomarker extraction via identifying paired cell clusters across biological conditions with contrastive siamese networks"></a>scBeacon: single-cell biomarker extraction via identifying paired cell clusters across biological conditions with contrastive siamese networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02594">http://arxiv.org/abs/2311.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenyu Liu, Kweon Yong Jin, Jun Ding</li>
<li>for: 这篇论文旨在提高单细胞水平的标识分析，尤其是在疾病和健康状态之间的交互作用下。</li>
<li>methods: 这篇论文提出了一个名为 scBeacon 的新的框架，这是一个基于深度对称 siamese 网络的无监控方法，可以对单细胞水平的标识进行分组，并且可以对不同状态下的单细胞进行匹配。</li>
<li>results: 根据评估结果，scBeacon 在各种数据集上表现出色，较 existing 的单细胞标识分析工具有更高的精度和灵活性。<details>
<summary>Abstract</summary>
Despite the breakthroughs in biomarker discovery facilitated by differential gene analysis, challenges remain, particularly at the single-cell level. Traditional methodologies heavily rely on user-supplied cell annotations, focusing on individually expressed data, often neglecting the critical interactions between biological conditions, such as healthy versus diseased states. In response, here we introduce scBeacon, an innovative framework built upon a deep contrastive siamese network. scBeacon pioneers an unsupervised approach, adeptly identifying matched cell populations across varied conditions, enabling a refined differential gene analysis. By utilizing a VQ-VAE framework, a contrastive siamese network, and a greedy iterative strategy, scBeacon effectively pinpoints differential genes that hold potential as key biomarkers. Comprehensive evaluations on a diverse array of datasets validate scBeacon's superiority over existing single-cell differential gene analysis tools. Its precision and adaptability underscore its significant role in enhancing diagnostic accuracy in biomarker discovery. With the emphasis on the importance of biomarkers in diagnosis, scBeacon is positioned to be a pivotal asset in the evolution of personalized medicine and targeted treatments.
</details>
<details>
<summary>摘要</summary>
尽管生物标志物发现方面已经做出了重大突破，但是在单个细胞水平还存在一些挑战。传统的方法ologies依赖用户提供的细胞注释，宁静关注个别表达数据，经常忽略生物条件之间的关键互动，如健康与疾病状态之间的对比。为此，我们在这里引入scBeacon，一种创新的框架，基于深度对比性同构网络。scBeacon采用无监督方法，能够准确地匹配不同状态下的细胞人口，从而提高了差异基因分析的精度。通过VQ-VAE框架、对比性同构网络和迅速迭代策略，scBeacon可以有效地找到具有潜在作用的差异基因，这些基因可能成为重要的生物标志物。对于一系列多样化的数据集进行了全面的评估，scBeacon的精度和适应性得到了证明，与现有的单个细胞差异基因分析工具相比，具有显著的优势。鉴于生物标志物在诊断中的重要性，scBeacon将成为个人化医学和Targeted therapy的核心资产。
</details></li>
</ul>
<hr>
<h2 id="Differentially-Private-Pre-Trained-Model-Fusion-using-Decentralized-Federated-Graph-Matching"><a href="#Differentially-Private-Pre-Trained-Model-Fusion-using-Decentralized-Federated-Graph-Matching" class="headerlink" title="Differentially Private Pre-Trained Model Fusion using Decentralized Federated Graph Matching"></a>Differentially Private Pre-Trained Model Fusion using Decentralized Federated Graph Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03396">http://arxiv.org/abs/2311.03396</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qian Chen, Yiqiang Chen, Xinlong Jiang, Teng Zhang, Weiwei Dai, Wuliang Huang, Zhen Yan, Bo Ye</li>
<li>for: 本研究旨在提供一种保持隐私的模型融合方法，以便在模型作为服务的场景中实现高质量的模型服务传递。</li>
<li>methods: 本研究使用了图structured architecture，并采用了本地差分隐私机制和分布式联合图匹配来保证模型融合过程中的隐私。</li>
<li>results: 实验结果表明，PrivFusion可以保持模型性能的同时保障隐私，并且在实际的医疗应用中得到了较好的效果。<details>
<summary>Abstract</summary>
Model fusion is becoming a crucial component in the context of model-as-a-service scenarios, enabling the delivery of high-quality model services to local users. However, this approach introduces privacy risks and imposes certain limitations on its applications. Ensuring secure model exchange and knowledge fusion among users becomes a significant challenge in this setting. To tackle this issue, we propose PrivFusion, a novel architecture that preserves privacy while facilitating model fusion under the constraints of local differential privacy. PrivFusion leverages a graph-based structure, enabling the fusion of models from multiple parties without necessitating retraining. By employing randomized mechanisms, PrivFusion ensures privacy guarantees throughout the fusion process. To enhance model privacy, our approach incorporates a hybrid local differentially private mechanism and decentralized federated graph matching, effectively protecting both activation values and weights. Additionally, we introduce a perturbation filter adapter to alleviate the impact of randomized noise, thereby preserving the utility of the fused model. Through extensive experiments conducted on diverse image datasets and real-world healthcare applications, we provide empirical evidence showcasing the effectiveness of PrivFusion in maintaining model performance while preserving privacy. Our contributions offer valuable insights and practical solutions for secure and collaborative data analysis within the domain of privacy-preserving model fusion.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型融合在服务模式下成为关键组件，使得本地用户获得高质量模型服务。然而，这种方法带来隐私风险并带来一些应用限制。保持安全的模型交换和用户知识融合成为这种设置中的主要挑战。为解决这个问题，我们提出了 PrivFusion，一种新的架构，可以在保持隐私的情况下进行模型融合。PrivFusion利用图structured，可以在多方参与者的情况下进行模型融合，不需要重新训练。通过随机机制，PrivFusion保证了隐私保障 throughout the fusion process。为增强模型隐私，我们的方法包括了hybrid本地分布式隐私机制和分布式图匹配，以保护模型的活动值和权重。此外，我们还提出了抑制随机噪声的滤波器适配器，以降低随机噪声的影响，保持融合模型的用用性。通过对多个图像数据集和实际医疗应用进行了广泛的实验，我们提供了Empirical evidence，证明PrivFusion可以保持模型性能的同时保护隐私。我们的贡献提供了有价值的实践解决方案和技术方法，用于在隐私保护下进行安全的数据分析。
</details></li>
</ul>
<hr>
<h2 id="Newvision-application-for-helping-blind-people-using-deep-learning"><a href="#Newvision-application-for-helping-blind-people-using-deep-learning" class="headerlink" title="Newvision: application for helping blind people using deep learning"></a>Newvision: application for helping blind people using deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03395">http://arxiv.org/abs/2311.03395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kumar Srinivas Bobba, Kartheeban K, Vamsi Krishna Sai Boddu, Vijaya Mani Surendra Bolla, Dinesh Bugga</li>
<li>for: 帮助视障人群在日常生活中独立行动，提高生活质量。</li>
<li>methods: 使用计算机视觉、距离估算附加于超声波传感器、语音识别和语音助手，为用户提供实时环境信息。</li>
<li>results: 可以帮助视障人群在环境中导航、识别物体和人员、阅读文本、避免障碍。<details>
<summary>Abstract</summary>
As able-bodied people, we often take our vision for granted. For people who are visually impaired, however, their disability can have a significant impact on their daily lives. We are developing proprietary headgear that will help visually impaired people navigate their surroundings, identify objects and people, read text, and avoid obstacles. The headgear will use a combination of computer vision, distance estimation with ultrasonic sensors, voice recognition, and voice assistants to provide users with real-time information about their environment. Users will be able to interact with the headgear through voice commands, such as ''What is that?'' to identify an object or ''Navigate to the front door'' to find their way around. The headgear will then provide the user with a verbal description of the object or spoken navigation instructions. We believe that this headgear has the potential to make a significant difference in the lives of visually impaired people, allowing them to live more independently and participate more fully in society.
</details>
<details>
<summary>摘要</summary>
As 能够的人们，我们经常忽略我们的视力。但对于有视力障碍的人们，他们的障碍可能会对他们的日常生活产生深远的影响。我们正在开发专有的头盔，帮助有视力障碍的人们在环境中导航、识别物体和人员、阅读文本，并避免障碍。这个头盔使用计算机视觉、ultrasonic探测、语音识别和语音助手等技术，为用户提供实时环境信息。用户可以通过声音命令，如 ''什么是那？'' 识别物体，或 ''导航到门口'' 查找方向。头盔然后为用户提供物体的声音描述或导航说明。我们认为这个头盔有可能对有视力障碍人员的生活产生深远的影响，让他们更独立地生活，更全面地参与社会。
</details></li>
</ul>
<hr>
<h2 id="KITS-Inductive-Spatio-Temporal-Kriging-with-Increment-Training-Strategy"><a href="#KITS-Inductive-Spatio-Temporal-Kriging-with-Increment-Training-Strategy" class="headerlink" title="KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy"></a>KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02565">http://arxiv.org/abs/2311.02565</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qianxiong Xu, Cheng Long, Ziyue Li, Sijie Ruan, Rui Zhao, Zhishuai Li</li>
<li>For: This paper proposes a new method called KITS (Kriging with Increment Training Strategy) to address the issue of graph gap in inductive spatio-temporal kriging methods based on graph neural networks.* Methods: The KITS method adds virtual nodes to the training graph to mitigate the graph gap issue, and pairs each virtual node with its most similar observed node to fuse their features together. The method also constructs reliable pseudo labels for virtual nodes to enhance the supervision signal.* Results: The KITS method consistently outperforms existing kriging methods by large margins, with an improvement over MAE score of up to 18.33%.<details>
<summary>Abstract</summary>
Sensors are commonly deployed to perceive the environment. However, due to the high cost, sensors are usually sparsely deployed. Kriging is the tailored task to infer the unobserved nodes (without sensors) using the observed source nodes (with sensors). The essence of kriging task is transferability. Recently, several inductive spatio-temporal kriging methods have been proposed based on graph neural networks, being trained based on a graph built on top of observed nodes via pretext tasks such as masking nodes out and reconstructing them. However, the graph in training is inevitably much sparser than the graph in inference that includes all the observed and unobserved nodes. The learned pattern cannot be well generalized for inference, denoted as graph gap. To address this issue, we first present a novel Increment training strategy: instead of masking nodes (and reconstructing them), we add virtual nodes into the training graph so as to mitigate the graph gap issue naturally. Nevertheless, the empty-shell virtual nodes without labels could have bad-learned features and lack supervision signals. To solve these issues, we pair each virtual node with its most similar observed node and fuse their features together; to enhance the supervision signal, we construct reliable pseudo labels for virtual nodes. As a result, the learned pattern of virtual nodes could be safely transferred to real unobserved nodes for reliable kriging. We name our new Kriging model with Increment Training Strategy as KITS. Extensive experiments demonstrate that KITS consistently outperforms existing kriging methods by large margins, e.g., the improvement over MAE score could be as high as 18.33%.
</details>
<details>
<summary>摘要</summary>
感知器通常用于感知环境。然而，由于成本高昂，感知器通常会受到稀畴部署。基于树状网络的 krilling 任务可以用来推断没有感知器的节点（无感知节点）。 krilling 任务的核心思想是传播性。现在，基于图ael 神经网络的一些 inductive spatio-temporal krilling 方法已经被提出，这些方法通过在观察节点基础上建立图来进行训练，然后通过预测任务来学习。然而，训练图和推断图都包含所有观察和无感知节点，这会导致学习的模式难以在推断中 generalized。这种问题被称为图 gap。为解决这个问题，我们首先提出了一种新的增量训练策略：而不是将节点屏蔽（并重建它们），我们会将虚拟节点添加到训练图中，以mitigate the graph gap issue naturally。然而，空 shell 的虚拟节点没有标签可能会有坏学习特征和缺乏监督信号。为解决这些问题，我们将每个虚拟节点与其最相似的观察节点进行对应，并将它们的特征特性相加。此外，为增强监督信号，我们将虚拟节点的 pseudo label 建立起来。因此，学习的虚拟节点模式可以安全地传输到实际的无感知节点，以确保可靠的 krilling。我们称之为 KITS。我们的实验表明，KITS 可以大幅超过现有的 krilling 方法，例如 MAE 分数的改进率可以高达 18.33%。
</details></li>
</ul>
<hr>
<h2 id="Time-Series-Synthesis-Using-the-Matrix-Profile-for-Anonymization"><a href="#Time-Series-Synthesis-Using-the-Matrix-Profile-for-Anonymization" class="headerlink" title="Time Series Synthesis Using the Matrix Profile for Anonymization"></a>Time Series Synthesis Using the Matrix Profile for Anonymization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02563">http://arxiv.org/abs/2311.02563</a></li>
<li>repo_url: None</li>
<li>paper_authors: Audrey Der, Chin-Chia Michael Yeh, Yan Zheng, Junpeng Wang, Huiyuan Chen, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn Keogh</li>
<li>for: 提供一种方法 Synthesize time series data，以便在保持数据相似性的情况下，避免遵循 privacy regulations 或 commercial confidentiality 的限制。</li>
<li>methods: 提出了 Time Series Synthesis Using Matrix Profile (TSSUMP) 方法，该方法可以在保持数据相似性的情况下，将时间序列数据synthesized，以便在数据分析 tasks 中使用。</li>
<li>results: 通过实际案例研究，表明 TSSUMP 方法可以减少时间序列数据的相关性，同时保持数据的相似性，使得数据分析工具可以在synthesized时间序列上达到 near-identical 性能。<details>
<summary>Abstract</summary>
Publishing and sharing data is crucial for the data mining community, allowing collaboration and driving open innovation. However, many researchers cannot release their data due to privacy regulations or fear of leaking confidential business information. To alleviate such issues, we propose the Time Series Synthesis Using the Matrix Profile (TSSUMP) method, where synthesized time series can be released in lieu of the original data. The TSSUMP method synthesizes time series by preserving similarity join information (i.e., Matrix Profile) while reducing the correlation between the synthesized and the original time series. As a result, neither the values for the individual time steps nor the local patterns (or shapes) from the original data can be recovered, yet the resulting data can be used for downstream tasks that data analysts are interested in. We concentrate on similarity joins because they are one of the most widely applied time series data mining routines across different data mining tasks. We test our method on a case study of ECG and gender masking prediction. In this case study, the gender information is not only removed from the synthesized time series, but the synthesized time series also preserves enough information from the original time series. As a result, unmodified data mining tools can obtain near-identical performance on the synthesized time series as on the original time series.
</details>
<details>
<summary>摘要</summary>
发布和分享数据对数据挖掘社区至关重要，它帮助研究人员合作和推动开放创新。然而，许多研究人员无法发布自己的数据，因为隐私法规或担心泄露商业机密信息。为解决这些问题，我们提出了时间序列合成使用矩阵Profile（TSSUMP）方法，其中合成的时间序列可以代替原始数据。TSSUMP方法将时间序列合成，保持相似性Join信息（即矩阵Profile），同时减少合成时间序列和原始时间序列之间的相关性。因此，不能回归原始数据中的值，也不能回归本地特征（或形状）。然而，合成的数据仍然可以用于下游任务，数据分析师感兴趣的任务。我们专注于相似Join，因为它们是时间序列数据挖掘任务中最常用的 Routine。我们在ECG和性别遮盾预测case study中测试了我们的方法。在这个case study中， gender信息不仅从合成的时间序列中被除了，还保留了原始时间序列中的足够信息。因此，未修改的数据挖掘工具可以在合成的时间序列上获得近似于原始时间序列的性能。
</details></li>
</ul>
<hr>
<h2 id="Ego-Network-Transformer-for-Subsequence-Classification-in-Time-Series-Data"><a href="#Ego-Network-Transformer-for-Subsequence-Classification-in-Time-Series-Data" class="headerlink" title="Ego-Network Transformer for Subsequence Classification in Time Series Data"></a>Ego-Network Transformer for Subsequence Classification in Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02561">http://arxiv.org/abs/2311.02561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Huiyuan Chen, Yujie Fan, Xin Dai, Yan Zheng, Vivian Lai, Junpeng Wang, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn Keogh</li>
<li>for: 这篇论文旨在解决实际时间序列数据中的背景序列与前景序列混合类别问题。</li>
<li>methods: 本论文提出了一个新的时间序列子序列分类方法，将每个子序列表示为一个自我网络，具有重要最近邻信息。</li>
<li>results: 根据128个单变量和30个多变量时间序列数据集进行实验，结果显示本方法比据点方法表现出色，在104个数据集中表现更好。<details>
<summary>Abstract</summary>
Time series classification is a widely studied problem in the field of time series data mining. Previous research has predominantly focused on scenarios where relevant or foreground subsequences have already been extracted, with each subsequence corresponding to a single label. However, real-world time series data often contain foreground subsequences that are intertwined with background subsequences. Successfully classifying these relevant subsequences requires not only distinguishing between different classes but also accurately identifying the foreground subsequences amidst the background. To address this challenge, we propose a novel subsequence classification method that represents each subsequence as an ego-network, providing crucial nearest neighbor information to the model. The ego-networks of all subsequences collectively form a time series subsequence graph, and we introduce an algorithm to efficiently construct this graph. Furthermore, we have demonstrated the significance of enforcing temporal consistency in the prediction of adjacent subsequences for the subsequence classification problem. To evaluate the effectiveness of our approach, we conducted experiments using 128 univariate and 30 multivariate time series datasets. The experimental results demonstrate the superior performance of our method compared to alternative approaches. Specifically, our method outperforms the baseline on 104 out of 158 datasets.
</details>
<details>
<summary>摘要</summary>
时间序列分类是时间序列数据挖掘领域广泛研究的问题。先前的研究主要集中在已经提取了相关或前景 subsequences 的情况下进行研究，每个 subsequences 都对应一个单独的标签。然而，实际世界中的时间序列数据经常包含相关的前景 subsequences，需要不仅分辨不同的类型，还需要准确地识别前景 subsequences 中的相关部分。为解决这个挑战，我们提出了一种新的 subsequences 分类方法，即将每个 subsequences 表示为一个自我网络，提供了关键的最近邻居信息给模型。所有 subsequences 的ego-networks 共同形成了时间序列 subsequences 图，我们介绍了一种有效地构建这个图的算法。此外，我们还证明了在预测相邻 subsequences 时应该保持时间一致性的重要性。为评估我们的方法的有效性，我们对 128 个单variate 和 30 个多variate 时间序列数据集进行了实验。实验结果表明，我们的方法与其他方法相比，在 104 个数据集上表现出了更高的性能。具体来说，我们的方法在 158 个数据集中超过了基准值。
</details></li>
</ul>
<hr>
<h2 id="Sketching-Multidimensional-Time-Series-for-Fast-Discord-Mining"><a href="#Sketching-Multidimensional-Time-Series-for-Fast-Discord-Mining" class="headerlink" title="Sketching Multidimensional Time Series for Fast Discord Mining"></a>Sketching Multidimensional Time Series for Fast Discord Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03393">http://arxiv.org/abs/2311.03393</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Yan Zheng, Menghai Pan, Huiyuan Chen, Zhongfang Zhuang, Junpeng Wang, Liang Wang, Wei Zhang, Jeff M. Phillips, Eamonn Keogh</li>
<li>for: 本研究旨在提高多维时间序列异常检测中维度缩放的效率，并提供一种可靠地检测多维时间序列异常的方法。</li>
<li>methods: 本研究使用缩放矩阵 Profile 来捕捉时间序列异常，并提出一种基于缩放矩阵的快速异常检测算法。</li>
<li>results: 实验结果表明，提出的算法可以在多个实际世界应用中提高吞吐量，并且只具有 minimal impact on the quality of the approximated solution。此外，该算法还可以处理动态添加或删除维度的情况，允许数据分析师在实时进行 “what-if” 分析。<details>
<summary>Abstract</summary>
Time series discords are a useful primitive for time series anomaly detection, and the matrix profile is capable of capturing discord effectively. There exist many research efforts to improve the scalability of discord discovery with respect to the length of time series. However, there is surprisingly little work focused on reducing the time complexity of matrix profile computation associated with dimensionality of a multidimensional time series. In this work, we propose a sketch for discord mining among multi-dimensional time series. After an initial pre-processing of the sketch as fast as reading the data, the discord mining has runtime independent of the dimensionality of the original data. On several real world examples from water treatment and transportation, the proposed algorithm improves the throughput by at least an order of magnitude (50X) and only has minimal impact on the quality of the approximated solution. Additionally, the proposed method can handle the dynamic addition or deletion of dimensions inconsequential overhead. This allows a data analyst to consider "what-if" scenarios in real time while exploring the data.
</details>
<details>
<summary>摘要</summary>
时序列冲突是一种有用的原始 primitives  для时序列异常检测，matrix profile 可以有效地捕捉冲突。有很多研究努力以提高时序列冲突发现的可扩展性，但是奇怪的是，有 surprisingly little work focused on reducing the time complexity of matrix profile computation associated with the dimensionality of a multidimensional time series.在这种工作中，我们提议一种笔记 для多维时序列冲突挖掘。经过初始快速预处理的笔记，冲突挖掘的运行时间与原始数据的维度无关。在几个实际世界示例中（水处理和交通），我们提出的算法可以提高通过put throughput 至少一个数量级（50X），并且只有 minimal impact on the quality of the approximated solution。此外，我们的方法还可以处理动态添加或删除维度的无关 overhead。这意味着数据分析师可以在实时中考虑 "what-if" 场景，在探索数据时进行实时探索。
</details></li>
</ul>
<hr>
<h2 id="Nonlinear-Multi-objective-Reinforcement-Learning-with-Provable-Guarantees"><a href="#Nonlinear-Multi-objective-Reinforcement-Learning-with-Provable-Guarantees" class="headerlink" title="Nonlinear Multi-objective Reinforcement Learning with Provable Guarantees"></a>Nonlinear Multi-objective Reinforcement Learning with Provable Guarantees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02544">http://arxiv.org/abs/2311.02544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nianli Peng, Brandon Fain</li>
<li>for: 解决单或多目标Markov决策过程（MDP）中 maximize 期望值的非线性函数。</li>
<li>methods: 使用证明可靠的保证来解决这些问题，扩展了经典的E3算法，并提出了一种基于奖励意识的值迭代过程，以及一种同时学习环境模型的算法。</li>
<li>results: 该算法可以在很短的时间内获得一个约等于优化的策略，时间复杂度为MDP大小、欲达到的拟合度和非线性函数的平滑程度的高阶幂。<details>
<summary>Abstract</summary>
We describe RA-E3 (Reward-Aware Explicit Explore or Exploit), an algorithm with provable guarantees for solving a single or multi-objective Markov Decision Process (MDP) where we want to maximize the expected value of a nonlinear function over accumulated rewards. This allows us to model fairness-aware welfare optimization for multi-objective reinforcement learning as well as risk-aware reinforcement learning with nonlinear Von Neumann-Morgenstern utility functions in the single objective setting. RA-E3 extends the classic E3 algorithm that solves MDPs with scalar rewards and linear preferences. We first state a distinct reward-aware version of value iteration that calculates a non-stationary policy that is approximately optimal for a given model of the environment. This sub-procedure is based on an extended form of Bellman optimality for nonlinear optimization that explicitly considers time and current accumulated reward. We then describe how to use this optimization procedure in a larger algorithm that must simultaneously learn a model of the environment. The algorithm learns an approximately optimal policy in time that depends polynomially on the MDP size, desired approximation, and smoothness of the nonlinear function, and exponentially on the number of objectives.
</details>
<details>
<summary>摘要</summary>
我们描述RA-E3（奖励意识的明确探索或利用算法），这是一个具有证明保证的算法，用于解决单或多个目标Markov决策过程（MDP），以 Maximize the expected value of a nonlinear function over accumulated rewards. 这Permit us to model fairness-aware welfare optimization for multi-objective reinforcement learning as well as risk-aware reinforcement learning with nonlinear Von Neumann-Morgenstern utility functions in the single objective setting. RA-E3 extends the classic E3 algorithm that solves MDPs with scalar rewards and linear preferences. We first state a distinct reward-aware version of value iteration that calculates a non-stationary policy that is approximately optimal for a given model of the environment. This sub-procedure is based on an extended form of Bellman optimality for nonlinear optimization that explicitly considers time and current accumulated reward. We then describe how to use this optimization procedure in a larger algorithm that must simultaneously learn a model of the environment. The algorithm learns an approximately optimal policy in time that depends polynomially on the MDP size, desired approximation, and smoothness of the nonlinear function, and exponentially on the number of objectives.
</details></li>
</ul>
<hr>
<h2 id="Dense-Video-Captioning-A-Survey-of-Techniques-Datasets-and-Evaluation-Protocols"><a href="#Dense-Video-Captioning-A-Survey-of-Techniques-Datasets-and-Evaluation-Protocols" class="headerlink" title="Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols"></a>Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02538">http://arxiv.org/abs/2311.02538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iqra Qasim, Alexander Horsch, Dilip K. Prasad</li>
<li>for: 本研究旨在描述视频中的各种事件和互动，以提高视频的自然语言描述能力。</li>
<li>methods: 本研究使用了 dense video captioning (DVC) 技术，包括视频特征提取 (VFE)、时间事件Localization (TEL) 和高密度caption生成 (DCG) 三个子任务。</li>
<li>results: 研究人员通过实现DVC技术来描述视频中的各种事件和互动，并获得了较好的结果。<details>
<summary>Abstract</summary>
Untrimmed videos have interrelated events, dependencies, context, overlapping events, object-object interactions, domain specificity, and other semantics that are worth highlighting while describing a video in natural language. Owing to such a vast diversity, a single sentence can only correctly describe a portion of the video. Dense Video Captioning (DVC) aims at detecting and describing different events in a given video. The term DVC originated in the 2017 ActivityNet challenge, after which considerable effort has been made to address the challenge. Dense Video Captioning is divided into three sub-tasks: (1) Video Feature Extraction (VFE), (2) Temporal Event Localization (TEL), and (3) Dense Caption Generation (DCG). This review aims to discuss all the studies that claim to perform DVC along with its sub-tasks and summarize their results. We also discuss all the datasets that have been used for DVC. Lastly, we highlight some emerging challenges and future trends in the field.
</details>
<details>
<summary>摘要</summary>
<<SYS>> simulti-translation:en-cn原文：Untrimmed videos have interrelated events, dependencies, context, overlapping events, object-object interactions, domain specificity, and other semantics that are worth highlighting while describing a video in natural language. Owing to such a vast diversity, a single sentence can only correctly describe a portion of the video. Dense Video Captioning (DVC) aims at detecting and describing different events in a given video. The term DVC originated in the 2017 ActivityNet challenge, after which considerable effort has been made to address the challenge. Dense Video Captioning is divided into three sub-tasks: (1) Video Feature Extraction (VFE), (2) Temporal Event Localization (TEL), and (3) Dense Caption Generation (DCG). This review aims to discuss all the studies that claim to perform DVC along with its sub-tasks and summarize their results. We also discuss all the datasets that have been used for DVC. Lastly, we highlight some emerging challenges and future trends in the field.翻译：视频中有关联的事件、依赖关系、上下文、重叠事件、对象之间交互、域特定性和其他semantics，这些都值得在描述视频的自然语言中提到。由于这种广泛的多样性，单个句子只能正确描述视频的一部分。dense video captioning（DVC）目标在检测和描述视频中的不同事件。DVC的概念在2017年的ActivityNet挑战之后得到了广泛的努力，以解决这个挑战。DVC分为三个子任务：（1）视频特征提取（VFE），（2）时间事件地理位置（TEL），和（3）密集caption生成（DCG）。本文尝试讨论所有宣称实现DVC的研究，以及它们的结果。我们还讨论了所有用于DVC的数据集。最后，我们提出了一些emerging挑战和未来趋势。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.AI_2023_11_05/" data-id="cloq1wl1h006t7o8842ekaezi" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.CL_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T11:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.CL_2023_11_05/">cs.CL - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context"><a href="#Robust-Generalization-Strategies-for-Morpheme-Glossing-in-an-Endangered-Language-Documentation-Context" class="headerlink" title="Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context"></a>Robust Generalization Strategies for Morpheme Glossing in an Endangered Language Documentation Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02777">http://arxiv.org/abs/2311.02777</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ginn, Alexis Palmer</li>
<li>for: 这篇论文旨在 investigate the ability of morpheme labeling models to generalize, especially in resource-constrained settings.</li>
<li>methods: 这篇论文使用 weight decay optimization, output denoising, and iterative pseudo-labeling 方法来减少模型在不同类型文本上的差异性。</li>
<li>results:  experiments 表明，通过使用这些方法，模型的性能在未经见过的类型文本上提高了2%。<details>
<summary>Abstract</summary>
Generalization is of particular importance in resource-constrained settings, where the available training data may represent only a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.
</details>
<details>
<summary>摘要</summary>
通用化在有限资源的情况下 particualrly important, where the available training data may only represent a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.Here's the text with Traditional Chinese characters:通用化在有限资源的情况下 particualrly important, where the available training data may only represent a small fraction of the distribution of possible texts. We investigate the ability of morpheme labeling models to generalize by evaluating their performance on unseen genres of text, and we experiment with strategies for closing the gap between performance on in-distribution and out-of-distribution data. Specifically, we use weight decay optimization, output denoising, and iterative pseudo-labeling, and achieve a 2% improvement on a test set containing texts from unseen genres. All experiments are performed using texts written in the Mayan language Uspanteko.
</details></li>
</ul>
<hr>
<h2 id="Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency"><a href="#Attention-or-Convolution-Transformer-Encoders-in-Audio-Language-Models-for-Inference-Efficiency" class="headerlink" title="Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency"></a>Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02772">http://arxiv.org/abs/2311.02772</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Jeon, Ching-Feng Yeh, Hakan Inan, Wei-Ning Hsu, Rashi Rungta, Yashar Mehdad, Daniel Bikel</li>
<li>for: 这个论文目的是提出一种简单自编程的音频模型，可以达到与更复杂的预训练模型相同的推理效率。</li>
<li>methods: 这个论文使用了混合卷积模块和自注意模块的speech transformerEncoder，实现了ASR的state-of-the-art性和高效性。</li>
<li>results: 研究表明，使用这种speech transformerEncoder可以大幅提高预训练音频模型的效率，但是我们还可以通过使用高级自注意来实现相同的效率。此外，我们发现使用低位数字量化技术可以进一步提高效率。<details>
<summary>Abstract</summary>
In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们显示了一种简单的自我超vised预训练音频模型可以达到与更复杂的预训练模型（具有speech transformer Encoder）相当的推理效率。这些speech transformer Encoder通过混合径向模块与自我注意模块来实现了ASR中的状态环境。我们首先显示了使用这些speech transformer Encoder作为encoder可以显著提高预训练音频模型的效率。然而，我们的研究表明，我们可以通过高级自我注意来实现相同的效率。我们示出了这种更简单的方法在使用低位数量量化神经网络时 particualrly有利。我们假设这种方法可以避免在不同量化模块之间传递错误，相比之下，当前的speech transformers混合量化径向模块和量化自我注意模块。
</details></li>
</ul>
<hr>
<h2 id="Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes"><a href="#Pyclipse-a-library-for-deidentification-of-free-text-clinical-notes" class="headerlink" title="Pyclipse, a library for deidentification of free-text clinical notes"></a>Pyclipse, a library for deidentification of free-text clinical notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02748">http://arxiv.org/abs/2311.02748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Callandra Moore, Jonathan Ranisau, Walter Nelson, Jeremy Petch, Alistair Johnson</li>
<li>for:  Automated deidentification of clinical text data is crucial due to the high cost of manual deidentification, which has been a barrier to sharing clinical text and the advancement of clinical natural language processing.</li>
<li>methods:  The pyclipse framework is proposed to address the challenges of creating effective automated deidentification tools, including issues in reproducibility due to differences in text processing, evaluation methods, and a lack of consistency across clinical domains and institutions.</li>
<li>results:  The pyclipse framework is demonstrated to be a unified and configurable evaluation procedure that can streamline the comparison of deidentification algorithms, and it is found that algorithm performance consistently falls short of the results reported in the original papers, even when evaluated on the same benchmark dataset.<details>
<summary>Abstract</summary>
Automated deidentification of clinical text data is crucial due to the high cost of manual deidentification, which has been a barrier to sharing clinical text and the advancement of clinical natural language processing. However, creating effective automated deidentification tools faces several challenges, including issues in reproducibility due to differences in text processing, evaluation methods, and a lack of consistency across clinical domains and institutions. To address these challenges, we propose the pyclipse framework, a unified and configurable evaluation procedure to streamline the comparison of deidentification algorithms. Pyclipse serves as a single interface for running open-source deidentification algorithms on local clinical data, allowing for context-specific evaluation. To demonstrate the utility of pyclipse, we compare six deidentification algorithms across four public and two private clinical text datasets. We find that algorithm performance consistently falls short of the results reported in the original papers, even when evaluated on the same benchmark dataset. These discrepancies highlight the complexity of accurately assessing and comparing deidentification algorithms, emphasizing the need for a reproducible, adjustable, and extensible framework like pyclipse. Our framework lays the foundation for a unified approach to evaluate and improve deidentification tools, ultimately enhancing patient protection in clinical natural language processing.
</details>
<details>
<summary>摘要</summary>
自动化识别临床文本数据的重要性在于手动识别的高成本，这成为了临床自然语言处理的发展的一个障碍。然而，创建有效的自动化识别工具面临着许多挑战，包括评估方法的不同和临床领域和机构之间的不一致性。为解决这些挑战，我们提出了pyclipse框架，一个可配置的评估过程框架，可以帮助Streamline识别算法的比较。pyclipse提供了一个单一的界面，可以在本地临床数据上运行开源识别算法，并为每个临床领域和机构提供上下文特定的评估。为了证明pyclipse的有用性，我们将比较六种识别算法在四个公共和两个私人临床文本数据集上的表现。我们发现，算法的表现 consistently short of the results reported in the original papers, even when evaluated on the same benchmark dataset.这些差异 highlights the complexity of accurately assessing and comparing deidentification algorithms, emphasizing the need for a reproducible, adjustable, and extensible framework like pyclipse.我们的框架为识别工具的评估和改进提供了一个统一的方法，从而推动了患者保护在临床自然语言处理中。
</details></li>
</ul>
<hr>
<h2 id="Nepali-Video-Captioning-using-CNN-RNN-Architecture"><a href="#Nepali-Video-Captioning-using-CNN-RNN-Architecture" class="headerlink" title="Nepali Video Captioning using CNN-RNN Architecture"></a>Nepali Video Captioning using CNN-RNN Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02699">http://arxiv.org/abs/2311.02699</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bipesh Subedi, Saugat Singh, Bal Krishna Bal</li>
<li>for: 这个研究旨在开发一个基于深度神经网络的尼泊尔视频描述系统，以提供精准和contextually relevant的视频描述 для尼泊尔视频。</li>
<li>methods: 该研究使用了预训练的CNN和RNN，并通过数据采集、数据处理、模型实现和评估来实现目标。研究使用了Google翻译将MSVD数据集扩展到尼泊尔语描述，然后训练了不同的CNN-RNN架构。</li>
<li>results: 研究发现，使用EfficientNetB0和BiLSTM结构的模型在BLEU和METEOR metric上达到了17和46的分数。此外，研究还描述了在尼泊尔语视频描述方面遇到的挑战和未来研究的方向。<details>
<summary>Abstract</summary>
This article presents a study on Nepali video captioning using deep neural networks. Through the integration of pre-trained CNNs and RNNs, the research focuses on generating precise and contextually relevant captions for Nepali videos. The approach involves dataset collection, data preprocessing, model implementation, and evaluation. By enriching the MSVD dataset with Nepali captions via Google Translate, the study trains various CNN-RNN architectures. The research explores the effectiveness of CNNs (e.g., EfficientNetB0, ResNet101, VGG16) paired with different RNN decoders like LSTM, GRU, and BiLSTM. Evaluation involves BLEU and METEOR metrics, with the best model being EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46. The article also outlines challenges and future directions for advancing Nepali video captioning, offering a crucial resource for further research in this area.
</details>
<details>
<summary>摘要</summary>
The study involves several steps, including dataset collection, data preprocessing, model implementation, and evaluation. To enrich the MSVD dataset with Nepali captions, the researchers use Google Translate to add captions to the videos. They then train various CNN-RNN architectures, including EfficientNetB0, ResNet101, and VGG16, paired with different RNN decoders such as LSTM, GRU, and BiLSTM.The evaluation metrics used in the study are BLEU and METEOR, and the best model is found to be EfficientNetB0 + BiLSTM with 1024 hidden dimensions, achieving a BLEU-4 score of 17 and METEOR score of 46. The article also discusses challenges and future directions for advancing Nepali video captioning, providing a valuable resource for further research in this area.
</details></li>
</ul>
<hr>
<h2 id="LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing"><a href="#LLM-enhanced-Self-training-for-Cross-domain-Constituency-Parsing" class="headerlink" title="LLM-enhanced Self-training for Cross-domain Constituency Parsing"></a>LLM-enhanced Self-training for Cross-domain Constituency Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02660">http://arxiv.org/abs/2311.02660</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianling Li, Meishan Zhang, Peiming Guo, Min Zhang, Yue Zhang</li>
<li>for: 本研究探讨了自动训练在跨领域任务中的应用，特别是在跨领域成分分析中。</li>
<li>methods: 本研究提出了利用大语言模型（LLM）生成领域特定的raw corpora，并通过 grammar rules和假实例选择 criterion来引导LLM生成raw corpora。</li>
<li>results: 实验结果表明，自动训练 для成分分析，启用LLM，可以超越传统方法，无论LLM的性能如何。此外，结合grammar rules和假实例选择 criterion可以实现最高的跨领域成分分析性能。<details>
<summary>Abstract</summary>
Self-training has proven to be an effective approach for cross-domain tasks, and in this study, we explore its application to cross-domain constituency parsing. Traditional self-training methods rely on limited and potentially low-quality raw corpora. To overcome this limitation, we propose enhancing self-training with the large language model (LLM) to generate domain-specific raw corpora iteratively. For the constituency parsing, we introduce grammar rules that guide the LLM in generating raw corpora and establish criteria for selecting pseudo instances. Our experimental results demonstrate that self-training for constituency parsing, equipped with an LLM, outperforms traditional methods regardless of the LLM's performance. Moreover, the combination of grammar rules and confidence criteria for pseudo-data selection yields the highest performance in the cross-domain constituency parsing.
</details>
<details>
<summary>摘要</summary>
自我训练已经证明是跨领域任务的有效方法，在这种研究中，我们探索了它的应用于跨领域成分分析。传统的自我训练方法取得有限和可能是低质量的Raw corpora。为了超越这些限制，我们提议通过大型语言模型（LLM）生成领域特定的Raw corpora，并在每一轮生成Raw corpora时遵循语法规则。对于成分分析，我们引入语法规则来导引LLM生成Raw corpora，并设置pseudo实例选择的标准。我们的实验结果表明，将自我训练与LLM结合使用，可以超越传统方法，无论LLM的性能如何。此外，结合语法规则和pseudo实例选择的信心标准，可以在跨领域成分分析中获得最高性能。
</details></li>
</ul>
<hr>
<h2 id="Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval"><a href="#Divide-Conquer-for-Entailment-aware-Multi-hop-Evidence-Retrieval" class="headerlink" title="Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval"></a>Divide &amp; Conquer for Entailment-aware Multi-hop Evidence Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02616">http://arxiv.org/abs/2311.02616</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Luo, Mihai Surdeanu</li>
<li>for: Answering multi-hop questions by retrieving evidences that are semantically equivalent or entailed by the question.</li>
<li>methods: Divide the task into two sub-tasks: semantic textual similarity retrieval and inference similarity retrieval, and use two ensemble models (EAR and EARnest) to jointly re-rank sentences with consideration of diverse relevance signals.</li>
<li>results: Significantly outperform all single retrieval models and two ensemble baseline models on HotpotQA, and more effective in retrieving relevant evidences for multi-hop questions.<details>
<summary>Abstract</summary>
Lexical and semantic matches are commonly used as relevance measurements for information retrieval. Together they estimate the semantic equivalence between the query and the candidates. However, semantic equivalence is not the only relevance signal that needs to be considered when retrieving evidences for multi-hop questions. In this work, we demonstrate that textual entailment relation is another important relevance dimension that should be considered. To retrieve evidences that are either semantically equivalent to or entailed by the question simultaneously, we divide the task of evidence retrieval for multi-hop question answering (QA) into two sub-tasks, i.e., semantic textual similarity and inference similarity retrieval. We propose two ensemble models, EAR and EARnest, which tackle each of the sub-tasks separately and then jointly re-rank sentences with the consideration of the diverse relevance signals. Experimental results on HotpotQA verify that our models not only significantly outperform all the single retrieval models it is based on, but is also more effective than two intuitive ensemble baseline models.
</details>
<details>
<summary>摘要</summary>
lexical和semantic匹配通常用于信息检索中的相关性评估。它们共同估计查询和候选答案之间的semanticEquivalence。但semanticEquivalence并不是多步问题检索证据的唯一相关性信号。在这种情况下，我们表明文本涵义关系是另一个重要的相关性维度。为了同时检索具有查询和问题相似或涵义涵盖的证据，我们将多步问题answering（QA）证据检索任务分为两个子任务：semantic textual similarity retrieval和inference similarity retrieval。我们提出了两种ensemble模型，EAR和EARnest，它们分别处理每个子任务，然后对结果进行jointly重新排序，考虑多种相关性信号的多样性。实验结果表明，我们的模型不仅在HotpotQA上显著超越所有基于它的单个检索模型，还比两个INTUITIVE ensemble基eline模型更有效。
</details></li>
</ul>
<hr>
<h2 id="mahaNLP-A-Marathi-Natural-Language-Processing-Library"><a href="#mahaNLP-A-Marathi-Natural-Language-Processing-Library" class="headerlink" title="mahaNLP: A Marathi Natural Language Processing Library"></a>mahaNLP: A Marathi Natural Language Processing Library</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02579">http://arxiv.org/abs/2311.02579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/l3cube-pune/MarathiNLP">https://github.com/l3cube-pune/MarathiNLP</a></li>
<li>paper_authors: Vidula Magdum, Omkar Dhekane, Sharayu Hiwarkhedkar, Saloni Mittal, Raviraj Joshi</li>
<li>for: 这个研究是为了提供一个开源的自然语言处理（NLP）库，专门针对印度语言Marathi进行支持。</li>
<li>methods: 这个研究使用了现代的MahaBERT基于trasnformer模型，并提供了一个易于使用、可扩展、对应的Marathi文本分析工具组。</li>
<li>results: 这个研究提供了一个全面的NLP任务集，包括基本的预处理任务和进阶的NLP任务，例如情感分析、命名实体识别、讨厌话检测和句子完成。<details>
<summary>Abstract</summary>
We present mahaNLP, an open-source natural language processing (NLP) library specifically built for the Marathi language. It aims to enhance the support for the low-resource Indian language Marathi in the field of NLP. It is an easy-to-use, extensible, and modular toolkit for Marathi text analysis built on state-of-the-art MahaBERT-based transformer models. Our work holds significant importance as other existing Indic NLP libraries provide basic Marathi processing support and rely on older models with restricted performance. Our toolkit stands out by offering a comprehensive array of NLP tasks, encompassing both fundamental preprocessing tasks and advanced NLP tasks like sentiment analysis, NER, hate speech detection, and sentence completion. This paper focuses on an overview of the mahaNLP framework, its features, and its usage. This work is a part of the L3Cube MahaNLP initiative, more information about it can be found at https://github.com/l3cube-pune/MarathiNLP .
</details>
<details>
<summary>摘要</summary>
我们介绍mahaNLP，一个开源的自然语言处理（NLP）库，专门为旁遮普语言提供支持。它的目标是在NLP领域提高旁遮普语言的支持。这是一个易于使用、可扩展、具有模块性的旁遮普文本分析工具库，建立于现代的MahaBERT基于转移模型。我们的工作具有重要的意义，因为现有的印度语言NLP库只提供了基本的旁遮普处理支持，并且使用older模型，性能有限。我们的工具库包括了许多NLP任务，包括基本的预处理任务以及高级NLP任务，如情感分析、命名实体识别、仇恨言语检测和句子完成。本文将对mahaNLP框架、特点和使用进行概述。这是L3Cube MahaNLP项目的一部分，更多信息可以在https://github.com/l3cube-pune/MarathiNLP查看。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Sequencing-of-Documents"><a href="#Temporal-Sequencing-of-Documents" class="headerlink" title="Temporal Sequencing of Documents"></a>Temporal Sequencing of Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02578">http://arxiv.org/abs/2311.02578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Gervers, Gelila Tilahun</li>
<li>for: 这篇论文是为了排序历史文档的时间顺序而写的。</li>
<li>methods: 这种方法使用非 Parametric 泛函模型（Fan, Heckman, 和 Wand, 1995）来捕捉文字使用的慢滑度变化。</li>
<li>results: 这种方法可以有效地对历史文档进行排序，并且在对 medieval English 财产转让文档和美国国情报告 addresses 进行比较时，都有显著的改善。<details>
<summary>Abstract</summary>
We outline an unsupervised method for temporal rank ordering of sets of historical documents, namely American State of the Union Addresses and DEEDS, a corpus of medieval English property transfer documents. Our method relies upon effectively capturing the gradual change in word usage via a bandwidth estimate for the non-parametric Generalized Linear Models (Fan, Heckman, and Wand, 1995). The number of possible rank orders needed to search through possible cost functions related to the bandwidth can be quite large, even for a small set of documents. We tackle this problem of combinatorial optimization using the Simulated Annealing algorithm, which allows us to obtain the optimal document temporal orders. Our rank ordering method significantly improved the temporal sequencing of both corpora compared to a randomly sequenced baseline. This unsupervised approach should enable the temporal ordering of undated document sets.
</details>
<details>
<summary>摘要</summary>
我们提出了一种无监督的方法，用于排序历史文档集合，包括美国州联合宪言和中世纪英格兰财产转让文档集。我们的方法基于有效地捕捉文本中慢慢变化的词汇使用情况，通过非参数化的泛化线性模型（Fan, Heckman, 和 Wand，1995）来估算带宽。由于搜索可能的排序方案的数量可能很大，即使是一小组文档也可能会出现这个问题。我们使用模拟熔化算法来解决这个问题，从而获得最佳的文档排序顺序。我们的排序方法在对两个 corpora 进行比较时具有显著改善，相比随机排序基线。这种无监督的方法应该能够应用于无日期文档集。
</details></li>
</ul>
<hr>
<h2 id="BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla"><a href="#BanMANI-A-Dataset-to-Identify-Manipulated-Social-Media-News-in-Bangla" class="headerlink" title="BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla"></a>BanMANI: A Dataset to Identify Manipulated Social Media News in Bangla</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02570">http://arxiv.org/abs/2311.02570</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kamruzzaman15/banmani">https://github.com/kamruzzaman15/banmani</a></li>
<li>paper_authors: Mahammed Kamruzzaman, Md. Minul Islam Shovon, Gene Louis Kim</li>
<li>for: 本研究旨在识别社交媒体新闻中 false 地操纵相关新闻文章的具体说法。</li>
<li>methods: 本研究使用了一种数据集采集方法，以 circumvent 当前可用的 NLP 工具在 Bangla 语言上的限制。</li>
<li>results: 研究发现，当 Zero-shot 和微调设定下，现有的 LLM 都难以满足这个任务的要求。<details>
<summary>Abstract</summary>
Initial work has been done to address fake news detection and misrepresentation of news in the Bengali language. However, no work in Bengali yet addresses the identification of specific claims in social media news that falsely manipulates a related news article. At this point, this problem has been tackled in English and a few other languages, but not in the Bengali language. In this paper, we curate a dataset of social media content labeled with information manipulation relative to reference articles, called BanMANI. The dataset collection method we describe works around the limitations of the available NLP tools in Bangla. We expect these techniques will carry over to building similar datasets in other low-resource languages. BanMANI forms the basis both for evaluating the capabilities of existing NLP systems and for training or fine-tuning new models specifically on this task. In our analysis, we find that this task challenges current LLMs both under zero-shot and fine-tuned settings.
</details>
<details>
<summary>摘要</summary>
初步工作已经对假新闻检测和新闻歪曲的问题进行了准备。然而，目前没有任何工作在孟加拉语中对社交媒体新闻中谎言性的具体CLAIM进行识别。在这篇论文中，我们为这个问题收集了一个社交媒体内容的标注数据集，称为BanMANI。我们的数据集采集方法会讲述在可用的NLP工具 limitation下如何实现。我们期望这些技术可以扩展到其他低资源语言。BanMANI将成为评估现有NLP系统的能力以及训练或精度调整新模型的基础。在我们的分析中，我们发现这个任务对当前LLMs都是一个挑战，无论在零情况下或者精度调整后。
</details></li>
</ul>
<hr>
<h2 id="Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets"><a href="#Topic-model-based-on-co-occurrence-word-networks-for-unbalanced-short-text-datasets" class="headerlink" title="Topic model based on co-occurrence word networks for unbalanced short text datasets"></a>Topic model based on co-occurrence word networks for unbalanced short text datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02566">http://arxiv.org/abs/2311.02566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjie Ma, Junping Du, Meiyu Liang, Zeli Guan</li>
<li>for: 检测罕见话题在短文本 datasets 中的检测 (Detecting scarce topics in unbalanced short text datasets)</li>
<li>methods: 基于 co-occurrence word networks 的话题模型 (Topic model based on co-occurrence word networks)</li>
<li>results: 在不平衡短文本 datasets 中提供了一种可靠的话题检测方法 (Provides a reliable method for detecting topics in unbalanced short text datasets)<details>
<summary>Abstract</summary>
We propose a straightforward solution for detecting scarce topics in unbalanced short-text datasets. Our approach, named CWUTM (Topic model based on co-occurrence word networks for unbalanced short text datasets), Our approach addresses the challenge of sparse and unbalanced short text topics by mitigating the effects of incidental word co-occurrence. This allows our model to prioritize the identification of scarce topics (Low-frequency topics). Unlike previous methods, CWUTM leverages co-occurrence word networks to capture the topic distribution of each word, and we enhanced the sensitivity in identifying scarce topics by redefining the calculation of node activity and normalizing the representation of both scarce and abundant topics to some extent. Moreover, CWUTM adopts Gibbs sampling, similar to LDA, making it easily adaptable to various application scenarios. Our extensive experimental validation on unbalanced short-text datasets demonstrates the superiority of CWUTM compared to baseline approaches in discovering scarce topics. According to the experimental results the proposed model is effective in early and accurate detection of emerging topics or unexpected events on social platforms.
</details>
<details>
<summary>摘要</summary>
我们提出了一种直观的解决方案，用于探测罕见话题在不均衡短文本数据集中。我们的方法，名为CWUTM（基于协occurrence词网络的短文本数据集中罕见话题模型），解决了短文本话题的罕见性和不均衡性的挑战。我们的模型可以增强对罕见话题的识别，并且可以在不同应用场景中轻松地适应。我们对不均衡短文本数据集进行了广泛的实验 validate，结果表明，相比基eline方法，CWUTM在发现罕见话题方面表现出了明显的优势。根据实验结果，我们的模型可以在社交平台上早期发现emerging话题或意外事件。Note: "短文本数据集" (short-text dataset) in Chinese is typically translated as "短文本集" (short-text collection), and "罕见话题" (scarce topic) is translated as "罕见话题" (rare topic) or "罕见话题" (underrepresented topic).
</details></li>
</ul>
<hr>
<h2 id="Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism"><a href="#Relation-Extraction-Model-Based-on-Semantic-Enhancement-Mechanism" class="headerlink" title="Relation Extraction Model Based on Semantic Enhancement Mechanism"></a>Relation Extraction Model Based on Semantic Enhancement Mechanism</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02564">http://arxiv.org/abs/2311.02564</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiyu Liu, Junping Du, Yingxia Shao, Zeli Guan</li>
<li>for: 提高信息抽取中关系EXTRACTION的效果，解决 triple overlap 问题</li>
<li>methods: 基于CasRel框架和semantic enhancement mechanism，提出了CasAug模型，通过对可能主语进行semantic coding，采用含义增强机制，对可能主语进行权重调整，提高关系EXTRACTION的精度</li>
<li>results: 比基eline模型提高了关系EXTRACTION的效果，可以更好地处理 triple overlap 问题，提高了对多个关系的EXTRACTION能力<details>
<summary>Abstract</summary>
Relational extraction is one of the basic tasks related to information extraction in the field of natural language processing, and is an important link and core task in the fields of information extraction, natural language understanding, and information retrieval. None of the existing relation extraction methods can effectively solve the problem of triple overlap. The CasAug model proposed in this paper based on the CasRel framework combined with the semantic enhancement mechanism can solve this problem to a certain extent. The CasAug model enhances the semantics of the identified possible subjects by adding a semantic enhancement mechanism, First, based on the semantic coding of possible subjects, pre-classify the possible subjects, and then combine the subject lexicon to calculate the semantic similarity to obtain the similar vocabulary of possible subjects. According to the similar vocabulary obtained, each word in different relations is calculated through the attention mechanism. For the contribution of the possible subject, finally combine the relationship pre-classification results to weight the enhanced semantics of each relationship to find the enhanced semantics of the possible subject, and send the enhanced semantics combined with the possible subject to the object and relationship extraction module. Complete the final relation triplet extraction. The experimental results show that, compared with the baseline model, the CasAug model proposed in this paper has improved the effect of relation extraction, and CasAug's ability to deal with overlapping problems and extract multiple relations is also better than the baseline model, indicating that the semantic enhancement mechanism proposed in this paper It can further reduce the judgment of redundant relations and alleviate the problem of triple overlap.
</details>
<details>
<summary>摘要</summary>
基于自然语言处理的信息EXTRACTION中，关系提取是一项基础任务和核心任务，与信息提取、自然语言理解和信息检索 closely related。现有的关系提取方法无法有效解决 triple overlap 问题。本文提出的 CasAug 模型，基于 CasRel 框架和semantic enhancement mechanism，可以减少重复的关系判断和 triple overlap 问题。CasAug 模型首先使用可能主语的semantic coding进行预类型，然后使用主语词典计算 Possible subjects 的semantic similarity，以获得每个关系中的相似词汇。通过注意机制，对每个关系中的每个词语进行计算。最后，根据关系预类型的结果，对各种关系中的semantics进行权重计算，并将权重计算结果与可能主语进行组合。最终，通过对象和关系提取模块进行完善，完成最终的关系 triplet 提取。实验结果表明，相比基eline模型，提出的 CasAug 模型在关系提取方面有所提高，并且 CasAug 模型在 triple overlap 问题上的处理能力也比基eline模型更好，这表明该paper中提出的 semantic enhancement mechanism 可以进一步减少重复的关系判断和 triple overlap 问题。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.CL_2023_11_05/" data-id="cloq1wl3x00e27o889ddneob4" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/05/cs.LG_2023_11_05/" class="article-date">
  <time datetime="2023-11-05T10:00:00.000Z" itemprop="datePublished">2023-11-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/05/cs.LG_2023_11_05/">cs.LG - 2023-11-05</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="From-molecules-to-scaffolds-to-functional-groups-building-context-dependent-molecular-representation-via-multi-channel-learning"><a href="#From-molecules-to-scaffolds-to-functional-groups-building-context-dependent-molecular-representation-via-multi-channel-learning" class="headerlink" title="From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning"></a>From molecules to scaffolds to functional groups: building context-dependent molecular representation via multi-channel learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02798">http://arxiv.org/abs/2311.02798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Wan, Jialu Wu, Tingjun Hou, Chang-Yu Hsieh, Xiaowei Jia</li>
<li>for: 这篇论文主要应用在化学物质的物理化学和生物学性质预测中，例如药物发现。</li>
<li>methods: 这篇论文使用了自类学习（SSL）方法，利用大规模、未标注的化学物质数据来学习化学空间的基础表现，并将这些表现与特定应用场景相结合。</li>
<li>results: 这篇论文的结果显示了与其他基eline比较之下，在多个分子属性评估标准中表现出色，并在特定但普遍的挑战 scenarios 中具有更高的稳定性和应用性。<details>
<summary>Abstract</summary>
Reliable molecular property prediction is essential for various scientific endeavors and industrial applications, such as drug discovery. However, the scarcity of data, combined with the highly non-linear causal relationships between physicochemical and biological properties and conventional molecular featurization schemes, complicates the development of robust molecular machine learning models. Self-supervised learning (SSL) has emerged as a popular solution, utilizing large-scale, unannotated molecular data to learn a foundational representation of chemical space that might be advantageous for downstream tasks. Yet, existing molecular SSL methods largely overlook domain-specific knowledge, such as molecular similarity and scaffold importance, as well as the context of the target application when operating over the large chemical space. This paper introduces a novel learning framework that leverages the knowledge of structural hierarchies within molecular structures, embeds them through separate pre-training tasks over distinct channels, and employs a task-specific channel selection to compose a context-dependent representation. Our approach demonstrates competitive performance across various molecular property benchmarks and establishes some state-of-the-art results. It further offers unprecedented advantages in particularly challenging yet ubiquitous scenarios like activity cliffs with enhanced robustness and generalizability compared to other baselines.
</details>
<details>
<summary>摘要</summary>
可靠的分子性质预测是科学研究和工业应用中的关键，如药物搜索。然而，数据稀缺和物理化和生物性质之间非线性关系，以及传统的分子特征化方案，使分子机器学习模型的开发变得复杂。自我超视学习（SSL）已成为一种流行的解决方案，利用大规模、无注释的分子数据来学习分子空间的基础表示，这可能对下游任务有利。然而，现有的分子SSL方法忽视了域专门知识，如分子相似性和架构重要性，以及目标应用场景的 контекст。本文介绍一种新的学习框架，利用分子结构中的结构层次结构，通过不同的预训练任务来嵌入这些结构，并使用任务特定的通道选择来组合上下文依赖的表示。我们的方法在多种分子性质benchmark上显示竞争性的性能，并在一些特殊 yet ubiquitous的enario中提供了前所未有的优势，比如活性峰值中的提高了Robustness和普遍性。
</details></li>
</ul>
<hr>
<h2 id="Riemannian-Laplace-Approximation-with-the-Fisher-Metric"><a href="#Riemannian-Laplace-Approximation-with-the-Fisher-Metric" class="headerlink" title="Riemannian Laplace Approximation with the Fisher Metric"></a>Riemannian Laplace Approximation with the Fisher Metric</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02766">http://arxiv.org/abs/2311.02766</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ksnxr/rlaf">https://github.com/ksnxr/rlaf</a></li>
<li>paper_authors: Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</li>
<li>for: 用于 Bayesian inference 的数据拟合</li>
<li>methods: 使用 Laplace 方法和 Riemannian geometry 拟合 Gaussian 分布</li>
<li>results: 提供了两种修改后的变体，并在几个实验中证明了其实际上的改进<details>
<summary>Abstract</summary>
The Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties heavily depend on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.
</details>
<details>
<summary>摘要</summary>
拉普拉斯方法用 Gaussian 分布近似目标概率分布， computationally efficient 和 asymptotically exact  для Bayesian inference  due to the Bernstein-von Mises theorem，但对复杂目标和 finite-data posterior 通常太粗糙。一种 recient generalization of the Laplace Approximation 使用 chosen Riemannian geometry 提供一个更加丰富的近似家族，仍然保持 computation efficiency，但选择的 метри可能会导致近似过于窄而且偏向的问题。我们在这里修正这个缺陷，发展了两个替代方案，并对方法的理论分析进行了扩展，在一系列实验中也表现了实践上的改进。
</details></li>
</ul>
<hr>
<h2 id="Log-Concavity-of-Multinomial-Likelihood-Functions-Under-Interval-Censoring-Constraints-on-Frequencies-or-Their-Partial-Sums"><a href="#Log-Concavity-of-Multinomial-Likelihood-Functions-Under-Interval-Censoring-Constraints-on-Frequencies-or-Their-Partial-Sums" class="headerlink" title="Log-Concavity of Multinomial Likelihood Functions Under Interval Censoring Constraints on Frequencies or Their Partial Sums"></a>Log-Concavity of Multinomial Likelihood Functions Under Interval Censoring Constraints on Frequencies or Their Partial Sums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02763">http://arxiv.org/abs/2311.02763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruce Levin, Erik Learned-Miller</li>
<li>for: 本文为了描述 multinomial vector 在 interval censoring 约束下的概率函数是完全log-concave。</li>
<li>methods: 本文使用 M-convex subset 的概念和 constrained sample spaces 的概念来证明概率函数的完全log-concavity。</li>
<li>results: 本文证明了 multinomial vector 在 interval censoring 约束下的概率函数是完全log-concave。<details>
<summary>Abstract</summary>
We show that the likelihood function for a multinomial vector observed under arbitrary interval censoring constraints on the frequencies or their partial sums is completely log-concave by proving that the constrained sample spaces comprise M-convex subsets of the discrete simplex.
</details>
<details>
<summary>摘要</summary>
我们显示了Multinomial vector在arbitrary interval censored的情况下观察到的概率函数是完全log-concave，通过证明受限样本空间包含M-convex的简单体的子集。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Strategic-Classification-Under-Unknown-Costs"><a href="#One-Shot-Strategic-Classification-Under-Unknown-Costs" class="headerlink" title="One-Shot Strategic Classification Under Unknown Costs"></a>One-Shot Strategic Classification Under Unknown Costs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02761">http://arxiv.org/abs/2311.02761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elan Rosenfeld, Nir Rosenfeld</li>
<li>for: 本研究旨在学习具有抗扰乱性的决策规则，以适应不确定的用户回应。</li>
<li>methods: 本研究使用一shot设定，通过commit一个核心分类器来解决不确定的用户回应问题。</li>
<li>results: 研究发现， même avec small mis-estimation of the true cost, the accuracy of the classifier can be arbitrarily low in the worst case. 我们提出了一种 minimax 问题来解决这个问题，并提供了efficient algorithms for both full-batch and stochastic settings, which converge to the minimax optimal solution at the dimension-independent rate of $\tilde{\mathcal{O}(T^{-\frac{1}{2})$.<details>
<summary>Abstract</summary>
A primary goal in strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that strategic responses are known; while some recent works address the important challenge of unknown responses, they exclusively study sequential settings which allow multiple model deployments over time. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use-case$\unicode{x2014}$where multiple deployments are unrealistic, or where even a single bad round is undesirable. To address this gap, we initiate the study of strategic classification under unknown responses in the one-shot setting, which requires committing to a single classifier once. Focusing on the users' cost function as the source of uncertainty, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail arbitrarily low accuracy in the worst case. In light of this, we frame the one-shot task as a minimax problem, with the goal of identifying the classifier with the smallest worst-case risk over an uncertainty set of possible costs. Our main contribution is efficient algorithms for both the full-batch and stochastic settings, which we prove converge (offline) to the minimax optimal solution at the dimension-independent rate of $\tilde{\mathcal{O}(T^{-\frac{1}{2})$. Our analysis reveals important structure stemming from the strategic nature of user responses, particularly the importance of dual norm regularization with respect to the cost function.
</details>
<details>
<summary>摘要</summary>
primary goal in strategic classification 是学习强制性不受输入操纵的决策规则。earlier works 假设战略回应是已知的；而一些最近的工作 Address 了重要的挑战，但 exclusively 研究了顺序设置，允许多个模型的多次部署在时间上。但是，在公共政策领域等很多领域，多个部署是不现实的，或者 Even a single bad round 是不 desirable。为了解决这个差距，我们开始研究不知道回应的战略分类在一枚 Setting 中，需要在一次性地选择一个分类器。我们从用户的成本函数中的不确定性开始，我们证明了，even a small mis-estimation of the true cost 可以导致最差情况下的准确率为零。在这种情况下，我们将一枚 Setting 定义为一个 minimax 问题，目标是找到可以在不确定性集中的可能成本中最小最差情况的决策器。我们的主要贡献是对批处理和随机设置中的精炼算法，我们证明它们在线上 converges 到 minimax 优化的解决方案，具有约等于 $T^{- \frac{1}{2}$ 的缩放率。我们的分析表明了由战略性的用户回应带来的重要结构，特别是对于成本函数的双重范数规范。
</details></li>
</ul>
<hr>
<h2 id="ELEGANT-Certified-Defense-on-the-Fairness-of-Graph-Neural-Networks"><a href="#ELEGANT-Certified-Defense-on-the-Fairness-of-Graph-Neural-Networks" class="headerlink" title="ELEGANT: Certified Defense on the Fairness of Graph Neural Networks"></a>ELEGANT: Certified Defense on the Fairness of Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02757">http://arxiv.org/abs/2311.02757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushundong/elegant">https://github.com/yushundong/elegant</a></li>
<li>paper_authors: Yushun Dong, Binchi Zhang, Hanghang Tong, Jundong Li</li>
<li>for: 防止Graph Neural Networks（GNNs）的偏见和不公平性攻击</li>
<li>methods: 提出了一个名为ELEGANT的原则性框架，并进行了详细的理论证明，以保证GNNs的公平性</li>
<li>results: 在实际实验中，ELEGANT被证明可以有效地防止攻击者通过添加偏见来让GNNs的预测结果偏离公平性，并且可以用于GNNs偏移修复Here is the translation in English:</li>
<li>for: Protecting Graph Neural Networks (GNNs) from bias and unfair attacks</li>
<li>methods: Proposed a principled framework called ELEGANT and provided a detailed theoretical certification analysis to ensure the fairness of GNNs</li>
<li>results: In practical experiments, ELEGANT was proven to be effective in preventing attackers from corrupting the fairness level of GNNs’ predictions by adding perturbations, and it can also be used for GNN debiasing.<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have emerged as a prominent graph learning model in various graph-based tasks over the years. Nevertheless, due to the vulnerabilities of GNNs, it has been empirically proved that malicious attackers could easily corrupt the fairness level of their predictions by adding perturbations to the input graph data. In this paper, we take crucial steps to study a novel problem of certifiable defense on the fairness level of GNNs. Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not have any assumption over the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Hence it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing. Open-source code can be found at https://github.com/yushundong/ELEGANT.
</details>
<details>
<summary>摘要</summary>
格网神经网络（GNNs）在各种基于图的任务中显示出了突出的表现。然而，由于GNNS的漏洞，实际证明了恶意攻击者可以轻松地腐蚀GNNS的预测公平性水平。在这篇论文中，我们研究了一个新的问题——GNNS公平性水平的证明防御。 Specifically, we propose a principled framework named ELEGANT and present a detailed theoretical certification analysis for the fairness of GNNs. ELEGANT takes any GNNs as its backbone, and the fairness level of such a backbone is theoretically impossible to be corrupted under certain perturbation budgets for attackers. Notably, ELEGANT does not make any assumptions about the GNN structure or parameters, and does not require re-training the GNNs to realize certification. Therefore, it can serve as a plug-and-play framework for any optimized GNNs ready to be deployed. We verify the satisfactory effectiveness of ELEGANT in practice through extensive experiments on real-world datasets across different backbones of GNNs, where ELEGANT is also demonstrated to be beneficial for GNN debiasing. 开源代码可以在https://github.com/yushundong/ELEGANT找到。
</details></li>
</ul>
<hr>
<h2 id="Staged-Reinforcement-Learning-for-Complex-Tasks-through-Decomposed-Environments"><a href="#Staged-Reinforcement-Learning-for-Complex-Tasks-through-Decomposed-Environments" class="headerlink" title="Staged Reinforcement Learning for Complex Tasks through Decomposed Environments"></a>Staged Reinforcement Learning for Complex Tasks through Decomposed Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02746">http://arxiv.org/abs/2311.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Pina, Corentin Artaud, Xiaolan Liu, Varuna De Silva</li>
<li>for: 这个论文是关于智能控制领域内的强化学习（RL）的应用，尤其是在智能汽车控制方面。</li>
<li>methods: 论文提出了两种方法来 aproximate RL 问题到实际问题上，包括分解复杂任务为多个子任务，以及使用中央训练分布执行（CTDE） paradigma。</li>
<li>results: 实验结果表明，提posed方法可以提高智能代理人在交通十字路相关的复杂任务中的表现，并最小化可能发生的安全问题。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) is an area of growing interest in the field of artificial intelligence due to its many notable applications in diverse fields. Particularly within the context of intelligent vehicle control, RL has made impressive progress. However, currently it is still in simulated controlled environments where RL can achieve its full super-human potential. Although how to apply simulation experience in real scenarios has been studied, how to approximate simulated problems to the real dynamic problems is still a challenge. In this paper, we discuss two methods that approximate RL problems to real problems. In the context of traffic junction simulations, we demonstrate that, if we can decompose a complex task into multiple sub-tasks, solving these tasks first can be advantageous to help minimising possible occurrences of catastrophic events in the complex task. From a multi-agent perspective, we introduce a training structuring mechanism that exploits the use of experience learned under the popular paradigm called Centralised Training Decentralised Execution (CTDE). This experience can then be leveraged in fully decentralised settings that are conceptually closer to real settings, where agents often do not have access to a central oracle and must be treated as isolated independent units. The results show that the proposed approaches improve agents performance in complex tasks related to traffic junctions, minimising potential safety-critical problems that might happen in these scenarios. Although still in simulation, the investigated situations are conceptually closer to real scenarios and thus, with these results, we intend to motivate further research in the subject.
</details>
<details>
<summary>摘要</summary>
强化学习（RL）是人工智能领域的一个快速发展领域，具有各种应用场景的优势。特别是在智能控制领域，RL已经做出了卓越的进展。然而，目前RL仍然在模拟控制环境中达到了最高的超人类水平。虽然有研究如何将模拟经验应用于实际场景，但是如何近似模拟问题到实际动态问题仍然是一个挑战。在这篇论文中，我们讨论了两种方法可以将RL问题近似到实际问题。在交通立交点模拟中，我们示出了如果将复杂任务分解成多个子任务，解决这些子任务可以帮助避免在复杂任务中可能发生的潜在灾难事件。从多智能代理的视角来看，我们介绍了一种使用中央训练分布执行（CTDE）的训练结构机制，利用这种机制可以在完全分布式的设置中使用经验学习。这些经验可以在实际场景中使用，agent们在实际场景中通常不具备中央报告机制，因此这些经验可以在完全分布式的设置中帮助agent们提高完成复杂任务的能力。结果显示，提出的方法可以在交通立交点任务中提高agent的性能，避免可能发生的安全关键问题。虽然仍在模拟环境中， investigate的情况概念上更近于实际场景，因此我们希望通过这些结果激励更多的研究在这个领域。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-Correlated-Auxiliary-Feedback-in-Parameterized-Bandits"><a href="#Exploiting-Correlated-Auxiliary-Feedback-in-Parameterized-Bandits" class="headerlink" title="Exploiting Correlated Auxiliary Feedback in Parameterized Bandits"></a>Exploiting Correlated Auxiliary Feedback in Parameterized Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02715">http://arxiv.org/abs/2311.02715</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Verma, Zhongxiang Dai, Yao Shu, Bryan Kian Hsiang Low</li>
<li>for: 这个论文研究了一种新的参数化带宽问题变体，在这个问题中，学习者可以观察附加的协助反馈，这些反馈与观察到的奖励相关。</li>
<li>methods: 这个论文首先开发了一种利用协助反馈建立奖励估计器，并提供了准确的信息 bounds，从而减少了 regret。</li>
<li>results: 实验结果在不同的设定中证明了我们提出的方法可以减少 regret，并且可以在不同的协助反馈下达到更好的性能。<details>
<summary>Abstract</summary>
We study a novel variant of the parameterized bandits problem in which the learner can observe additional auxiliary feedback that is correlated with the observed reward. The auxiliary feedback is readily available in many real-life applications, e.g., an online platform that wants to recommend the best-rated services to its users can observe the user's rating of service (rewards) and collect additional information like service delivery time (auxiliary feedback). In this paper, we first develop a method that exploits auxiliary feedback to build a reward estimator with tight confidence bounds, leading to a smaller regret. We then characterize the regret reduction in terms of the correlation coefficient between reward and its auxiliary feedback. Experimental results in different settings also verify the performance gain achieved by our proposed method.
</details>
<details>
<summary>摘要</summary>
我们研究一种新的参数化强制投票问题变体，在该问题中学习者可以观察附加的auxiliary反馈，这些反馈与观察到的奖励相关。这些附加反馈在实际应用中很普遍，例如一个在线平台想要推荐用户最佳评分服务可以观察用户对服务的评分（奖励）并收集附加信息如服务交付时间（auxiliary反馈）。我们首先开发了一种利用附加反馈建立奖励估计器，并提供紧张的信息 bounds，从而减少了 regret。然后，我们Characterize了 regret reduction的相对评价差，并通过不同的设置的实验结果来验证我们的提posed方法的性能提升。
</details></li>
</ul>
<hr>
<h2 id="A-Goal-Driven-Approach-to-Systems-Neuroscience"><a href="#A-Goal-Driven-Approach-to-Systems-Neuroscience" class="headerlink" title="A Goal-Driven Approach to Systems Neuroscience"></a>A Goal-Driven Approach to Systems Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02704">http://arxiv.org/abs/2311.02704</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuroailab/Neural-Alignment">https://github.com/neuroailab/Neural-Alignment</a></li>
<li>paper_authors: Aran Nayebi</li>
<li>for: 这篇论文的目的是提出一种新的解释神经元和神经细胞之间的交互方式，以解决现有的描述问题。</li>
<li>methods: 这篇论文使用了实验 neuroscience 技术，记录并 manipulate 动物表现Complex behaviors 的时候，神经元的活动。</li>
<li>results: 这篇论文提出了一种新的解释神经元和神经细胞之间的交互方式，并在多种脑区和物种中进行了应用，以研究智能行为的起源。<details>
<summary>Abstract</summary>
Humans and animals exhibit a range of interesting behaviors in dynamic environments, and it is unclear how our brains actively reformat this dense sensory information to enable these behaviors. Experimental neuroscience is undergoing a revolution in its ability to record and manipulate hundreds to thousands of neurons while an animal is performing a complex behavior. As these paradigms enable unprecedented access to the brain, a natural question that arises is how to distill these data into interpretable insights about how neural circuits give rise to intelligent behaviors. The classical approach in systems neuroscience has been to ascribe well-defined operations to individual neurons and provide a description of how these operations combine to produce a circuit-level theory of neural computations. While this approach has had some success for small-scale recordings with simple stimuli, designed to probe a particular circuit computation, often times these ultimately lead to disparate descriptions of the same system across stimuli. Perhaps more strikingly, many response profiles of neurons are difficult to succinctly describe in words, suggesting that new approaches are needed in light of these experimental observations. In this thesis, we offer a different definition of interpretability that we show has promise in yielding unified structural and functional models of neural circuits, and describes the evolutionary constraints that give rise to the response properties of the neural population, including those that have previously been difficult to describe individually. We demonstrate the utility of this framework across multiple brain areas and species to study the roles of recurrent processing in the primate ventral visual pathway; mouse visual processing; heterogeneity in rodent medial entorhinal cortex; and facilitating biological learning.
</details>
<details>
<summary>摘要</summary>
人类和动物在动态环境中展现出各种 interessante 行为，但是我们的大脑如何活动地重新格式化这些紧密的感知信息以启用这些行为仍然不清楚。现代神经科学实验受到了记录和修改百到千个神经元的技术的革命，这些方法使得我们可以在动物表现复杂行为时获取至前无之有的脑部数据。随着这些方法的发展，一个自然的问题出现了：如何将这些数据转化成可解释的洞察。传统的系统神经科学方法是将各个神经元归功于特定的操作，并提供一种描述如何这些操作相互作用以生成神经计算的综合理论。虽然这种方法在小规模记录下有一定的成功，但是它在面对复杂的刺激时经常导致不同的描述，这些描述在不同的刺激下都是不一致的。事实上，许多神经元响应 profiles 很难以用字符串来描述，这表明需要新的方法。在这个论文中，我们提出了一种不同的可解释性定义，并证明该定义在生成神经Circuit 级别的结构和功能模型方面具有承诺。我们还证明了这种定义在多个脑区和种类中的应用，以研究恒定处理的角色，包括人类脑镜下部Visual 路径; 鼠类视觉处理; 鼠类中脑核心受体区域的多样性; 和促进生物学学习。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-AI-Research-Paper-Analysis-Methodology-Component-Extraction-using-Factored-Transformer-based-Sequence-Modeling-Approach"><a href="#Enhancing-AI-Research-Paper-Analysis-Methodology-Component-Extraction-using-Factored-Transformer-based-Sequence-Modeling-Approach" class="headerlink" title="Enhancing AI Research Paper Analysis: Methodology Component Extraction using Factored Transformer-based Sequence Modeling Approach"></a>Enhancing AI Research Paper Analysis: Methodology Component Extraction using Factored Transformer-based Sequence Modeling Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.03401">http://arxiv.org/abs/2311.03401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madhusudan Ghosh, Debasis Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar</li>
<li>for: 本研究旨在自动抽取科学方法名称，提高科学文献中方法组成部分的抽取精度。</li>
<li>methods: 本研究提议使用分解方法，利用方法域的广泛类别信息，如 NLP、RL 等，以提高方法组成部分的抽取精度。</li>
<li>results: 实验结果显示，分解方法在尝试setup中表现出色，与基eline相比，提高了9.257%的精度。<details>
<summary>Abstract</summary>
Research in scientific disciplines evolves, often rapidly, over time with the emergence of novel methodologies and their associated terminologies. While methodologies themselves being conceptual in nature and rather difficult to automatically extract and characterise, in this paper, we seek to develop supervised models for automatic extraction of the names of the various constituents of a methodology, e.g., `R-CNN', `ELMo' etc. The main research challenge for this task is effectively modeling the contexts around these methodology component names in a few-shot or even a zero-shot setting. The main contributions of this paper towards effectively identifying new evolving scientific methodology names are as follows: i) we propose a factored approach to sequence modeling, which leverages a broad-level category information of methodology domains, e.g., `NLP', `RL' etc.; ii) to demonstrate the feasibility of our proposed approach of identifying methodology component names under a practical setting of fast evolving AI literature, we conduct experiments following a simulated chronological setup (newer methodologies not seen during the training process); iii) our experiments demonstrate that the factored approach outperforms state-of-the-art baselines by margins of up to 9.257\% for the methodology extraction task with the few-shot setup.
</details>
<details>
<summary>摘要</summary>
科学研究领域中的研究方法不断发展，经常快速地出现新的方法和其相关的术语。在这篇论文中，我们想要开发有监督模型来自动提取方法学Component的名称，例如“R-CNN”、“ELMo”等。我们的研究挑战是在几个或者 zeroshot设置下，有效地模型这些方法组件名称的上下文。我们的主要贡献如下：1. 我们提出了一种分解方法来模型序列，借鉴了方法学领域的大致类别信息，例如“NLP”、“RL”等。2. 为证明我们提出的方法在实际情况下可行，我们在快速演化的AI文献中进行了实验，采用了模拟时间序列的设置（ newer methodologies not seen during the training process）。3. 我们的实验表明，我们的分解方法可以在几个或者 zeroshot设置下，与现有的基eline相比，提高了方法提取任务的效果，提高了9.257%。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linearly-Mixed-Causal-Representations-from-Multi-Node-Interventions"><a href="#Identifying-Linearly-Mixed-Causal-Representations-from-Multi-Node-Interventions" class="headerlink" title="Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions"></a>Identifying Linearly-Mixed Causal Representations from Multi-Node Interventions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02695">http://arxiv.org/abs/2311.02695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simon Bing, Urmi Ninad, Jonas Wahl, Jakob Runge</li>
<li>for: 本研究旨在 Addressing the underconstrained problem of causal representation learning, particularly in the presence of multiple variables intervened upon within one environment.</li>
<li>methods: 我们的方法基于一个通用的假设，即在不同环境中的干预覆盖和多变量干预。我们采用了一种新的规范，即在不同环境中的干预Trace，并通过对这些跟踪进行规范和精炼来学习 causal representation。</li>
<li>results: 我们的实验结果表明，我们的方法可以在多变量干预下学习有效的 causal representation，并且可以避免一些先前的假设，如单变量干预和独立干预。<details>
<summary>Abstract</summary>
The task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.
</details>
<details>
<summary>摘要</summary>
task of inferring high-level causal variables from low-level observations, commonly referred to as causal representation learning, is fundamentally underconstrained. As such, recent works to address this problem focus on various assumptions that lead to identifiability of the underlying latent causal variables. A large corpus of these preceding approaches consider multi-environment data collected under different interventions on the causal model. What is common to virtually all of these works is the restrictive assumption that in each environment, only a single variable is intervened on. In this work, we relax this assumption and provide the first identifiability result for causal representation learning that allows for multiple variables to be targeted by an intervention within one environment. Our approach hinges on a general assumption on the coverage and diversity of interventions across environments, which also includes the shared assumption of single-node interventions of previous works. The main idea behind our approach is to exploit the trace that interventions leave on the variance of the ground truth causal variables and regularizing for a specific notion of sparsity with respect to this trace. In addition to and inspired by our theoretical contributions, we present a practical algorithm to learn causal representations from multi-node interventional data and provide empirical evidence that validates our identifiability results.
</details></li>
</ul>
<hr>
<h2 id="Regret-Analysis-of-Learning-Based-Linear-Quadratic-Gaussian-Control-with-Additive-Exploration"><a href="#Regret-Analysis-of-Learning-Based-Linear-Quadratic-Gaussian-Control-with-Additive-Exploration" class="headerlink" title="Regret Analysis of Learning-Based Linear Quadratic Gaussian Control with Additive Exploration"></a>Regret Analysis of Learning-Based Linear Quadratic Gaussian Control with Additive Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02679">http://arxiv.org/abs/2311.02679</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archith Athrey, Othmane Mazhar, Meichen Guo, Bart De Schutter, Shengling Shi</li>
<li>for: 本文研究一种 computationally efficient 探索策略，即naive exploration，用于控制未知部分可观测系统 within Linear Quadratic Gaussian (LQG) 框架。</li>
<li>methods: 本文提出了一种 two-phase 控制算法 called LQG-NAIVE，包括一个初始阶段输入 Gaussian 信号以获得系统模型，然后在一个 episodic 的方式中与 naive exploration 和控制进行交互。</li>
<li>results: 我们证明了 LQG-NAIVE 可以实现 regret 增长率为 $\tilde{\mathcal{O}(\sqrt{T})$，即 $\mathcal{O}(\sqrt{T})$ 以上下标 Logarithmic factors 之后 $T$ 步骤。此外，我们还提出了 LQG-IF2E，它在探索信号中包括 Fisher Information Matrix (FIM)，并提供了 LQG-IF2E 的竞争性性能比 LQG-NAIVE 更好的数据分析证明。<details>
<summary>Abstract</summary>
In this paper, we analyze the regret incurred by a computationally efficient exploration strategy, known as naive exploration, for controlling unknown partially observable systems within the Linear Quadratic Gaussian (LQG) framework. We introduce a two-phase control algorithm called LQG-NAIVE, which involves an initial phase of injecting Gaussian input signals to obtain a system model, followed by a second phase of an interplay between naive exploration and control in an episodic fashion. We show that LQG-NAIVE achieves a regret growth rate of $\tilde{\mathcal{O}(\sqrt{T})$, i.e., $\mathcal{O}(\sqrt{T})$ up to logarithmic factors after $T$ time steps, and we validate its performance through numerical simulations. Additionally, we propose LQG-IF2E, which extends the exploration signal to a `closed-loop' setting by incorporating the Fisher Information Matrix (FIM). We provide compelling numerical evidence of the competitive performance of LQG-IF2E compared to LQG-NAIVE.
</details>
<details>
<summary>摘要</summary>
在本文中，我们分析了computationally efficient exploration strategy（naive exploration）在Linear Quadratic Gaussian（LQG）框架下控制未知部分可观测系统中的 regret。我们提出了一种两相控制算法，即LQG-NAIVE，其包括一个初始阶段插入 Gaussian 输入信号以获得系统模型，然后是一个 episodic 的第二阶段，在这个阶段中，naive exploration 和控制之间进行了协调。我们证明了LQG-NAIVE 的 regret增长率为 $\tilde{\mathcal{O}(\sqrt{T})$，即在 $T$ 步时间后， regret 增长率为 $\mathcal{O}(\sqrt{T})$ 以上 logarithmic 因素。此外，我们还提出了LQG-IF2E，它在探索信号中包含了 Fisher Information Matrix（FIM）。我们通过数值实验证明了LQG-IF2E 的竞争性性比 LQG-NAIVE 更高。
</details></li>
</ul>
<hr>
<h2 id="Drone-Enabled-Load-Management-for-Solar-Small-Cell-Networks-in-Next-Gen-Communications-Optimization-for-Solar-Small-Cells"><a href="#Drone-Enabled-Load-Management-for-Solar-Small-Cell-Networks-in-Next-Gen-Communications-Optimization-for-Solar-Small-Cells" class="headerlink" title="Drone-Enabled Load Management for Solar Small Cell Networks in Next-Gen Communications Optimization for Solar Small Cells"></a>Drone-Enabled Load Management for Solar Small Cell Networks in Next-Gen Communications Optimization for Solar Small Cells</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02648">http://arxiv.org/abs/2311.02648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daksh Dave, Dhruv Khut, Sahil Nawale, Pushkar Aggrawal, Disha Rastogi, Kailas Devadkar</li>
<li>for: 支持5G及以后移动通信网络的绿色微网络能源管理</li>
<li>methods: 使用无人机携带的空中基站 Load Transfer 技术实现稳定可靠的能源重新分配</li>
<li>results: 提高了基站的可靠性和灵活性，降低了基站的能源损失和无人机交换次数<details>
<summary>Abstract</summary>
In recent years, the cellular industry has witnessed a major evolution in communication technologies. It is evident that the Next Generation of cellular networks(NGN) will play a pivotal role in the acceptance of emerging IoT applications supporting high data rates, better Quality of Service(QoS), and reduced latency. However, the deployment of NGN will introduce a power overhead on the communication infrastructure. Addressing the critical energy constraints in 5G and beyond, this study introduces an innovative load transfer method using drone-carried airborne base stations (BSs) for stable and secure power reallocation within a green micro-grid network. This method effectively manages energy deficit by transferring aerial BSs from high to low-energy cells, depending on user density and the availability of aerial BSs, optimizing power distribution in advanced cellular networks. The complexity of the proposed system is significantly lower as compared to existing power cable transmission systems currently employed in powering the BSs. Furthermore, our proposed algorithm has been shown to reduce BS power outages while requiring a minimum number of drone exchanges. We have conducted a thorough review on real-world dataset to prove the efficacy of our proposed approach to support BS during high load demand times
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Pointer-Networks-with-Q-Learning-for-OP-Combinatorial-Optimization"><a href="#Pointer-Networks-with-Q-Learning-for-OP-Combinatorial-Optimization" class="headerlink" title="Pointer Networks with Q-Learning for OP Combinatorial Optimization"></a>Pointer Networks with Q-Learning for OP Combinatorial Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02629">http://arxiv.org/abs/2311.02629</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alessandro Barro</li>
<li>for:  solves the Orienteering Problem (OP)</li>
<li>methods:  combines Pointer Networks (Ptr-Nets) and Q-learning</li>
<li>results:  superior capability in managing OP situationsHere are the three key points in Traditional Chinese:</li>
<li>for: 解决 Orienteering Problem (OP)</li>
<li>methods: 结合 Pointer Networks (Ptr-Nets) 和 Q-learning</li>
<li>results: 在 OP 中的优秀表现<details>
<summary>Abstract</summary>
The Orienteering Problem (OP) presents a unique challenge in combinatorial optimization, emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP leaves room for improvement. Recognizing the potency of Q-learning, especially when paired with deep neural structures, this research unveils the Pointer Q-Network (PQN). This innovative method combines Ptr-Nets and Q-learning, effectively addressing the specific challenges presented by OP. We deeply explore the architecture and efficiency of PQN, showcasing its superior capability in managing OP situations.
</details>
<details>
<summary>摘要</summary>
Orienteering Problem（OP）呈现了 combinatorial optimization 领域的独特挑战，它在物流、交通规划等领域广泛应用。由于 OP 的NP-硬性，获得优化解决方案是自然复杂的。然而，Pointer Networks（Ptr-Nets）在其他 combinatorial 任务中表现出色，但在 OP 中的表现仍有空间提升。本研究认识到 Q-学习的能力，特别是在与深度神经结构结合时，因此提出了 Pointer Q-Network（PQN）。这种创新方法结合了 Ptr-Nets 和 Q-学习，有效地解决了 OP 中的特定挑战。我们深入探讨 PQN 的architecture和效率，展示其在 OP 中的superior 能力。
</details></li>
</ul>
<hr>
<h2 id="An-adaptive-standardisation-model-for-Day-Ahead-electricity-price-forecasting"><a href="#An-adaptive-standardisation-model-for-Day-Ahead-electricity-price-forecasting" class="headerlink" title="An adaptive standardisation model for Day-Ahead electricity price forecasting"></a>An adaptive standardisation model for Day-Ahead electricity price forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02610">http://arxiv.org/abs/2311.02610</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ccaribe9/adaptstdepf">https://github.com/ccaribe9/adaptstdepf</a></li>
<li>paper_authors: Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</li>
<li>for:  Electricity market day-ahead price forecasting</li>
<li>methods:  Introducing adaptive standardization to mitigate dataset shifts and improve forecasting performance</li>
<li>results:  Significant improvement in forecasting accuracy across four markets, including two novel datasets, using less complex and widely accepted learning algorithms.<details>
<summary>Abstract</summary>
The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate four distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all four markets, using learning algorithms that are less complex yet widely accepted in the literature. This significant advancement unveils opens up new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.
</details>
<details>
<summary>摘要</summary>
研究一天前价格在电力市场是时间序列预测中最受欢迎的问题。先前的研究强调使用越来越复杂的学习算法来捕捉市场的复杂动态。然而，有一个阈值，其中增加复杂性不会带来显著改善。在这种情况下，我们提议一种不同的方法，即引入适应标准化，以mitigate dataset shifts常见于市场中。这样做可以使学习算法更加注重捕捉target变量和解释变量之间的真实关系。我们对四个市场进行了研究，包括两个新的数据集，之前从未出现在文献中。这些数据集提供了更加现实的市场背景，与 conventient datasets不同。结果显示在所有四个市场中有显著改善，使用在文献中广泛accepted的学习算法。这一显著进步揭示了适应转换在预测模型性能提高方面的潜在力量，开启了新的研究方向。
</details></li>
</ul>
<hr>
<h2 id="Steady-State-Analysis-of-Queues-with-Hawkes-Arrival-and-Its-Application-to-Online-Learning-for-Hawkes-Queues"><a href="#Steady-State-Analysis-of-Queues-with-Hawkes-Arrival-and-Its-Application-to-Online-Learning-for-Hawkes-Queues" class="headerlink" title="Steady-State Analysis of Queues with Hawkes Arrival and Its Application to Online Learning for Hawkes Queues"></a>Steady-State Analysis of Queues with Hawkes Arrival and Its Application to Online Learning for Hawkes Queues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02577">http://arxiv.org/abs/2311.02577</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyun Chen, Guiyu Hong</li>
<li>for: 这个论文 investigate了单服务器队列中 Hawkes 到达和一般服务分布的长期行为，以及相关的优化问题。</li>
<li>methods: 该论文使用了新的 coupling 技术来确定工作负荷和忙期过程的 finite moment bounds，并证明这些队列过程在恒定状态下对数快速 converges。</li>
<li>results: 根据这些理论结论，该论文开发了一种高效的数据驱动的 numerial 算法来解决 Hawkes 队列中的优化工作人员问题，并发现在高峰期 régime, Hawkes 队列的工作人员划算与 класси GI&#x2F;GI&#x2F;1 模型存在显著差异。<details>
<summary>Abstract</summary>
We investigate the long-run behavior of single-server queues with Hawkes arrivals and general service distributions and related optimization problems. In detail, utilizing novel coupling techniques, we establish finite moment bounds for the stationary distribution of the workload and busy period processes. In addition, we are able to show that, those queueing processes converge exponentially fast to their stationary distribution. Based on these theoretic results, we develop an efficient numerical algorithm to solve the optimal staffing problem for the Hawkes queues in a data-driven manner. Numerical results indicate a sharp difference in staffing for Hawkes queues, compared to the classic GI/GI/1 model, especially in the heavy-traffic regime.
</details>
<details>
<summary>摘要</summary>
我们研究单服务器队列中的长期行为，包括途径 Hawkes 的到达和一般服务分布，以及相关的优化问题。在详细的探讨中，我们利用新的 Coupling 技术，确定了工作负荷和忙期过程的finite moment bound。此外，我们还证明了这些队列过程在 exponentially fast 速度下关于其站点分布的整体准确性。基于这些理论结果，我们开发了一种高效的数据驱动的数字算法，解决 Hawkes 队列的优化人员问题。 numerically 的结果表明，在高负荷情况下，Hawkes 队列的人员配置和 класси GI/GI/1 模型之间存在很大的差异。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Treasure-Hunt-Content-based-Time-Series-Retrieval-System-for-Discovering-Insights"><a href="#Temporal-Treasure-Hunt-Content-based-Time-Series-Retrieval-System-for-Discovering-Insights" class="headerlink" title="Temporal Treasure Hunt: Content-based Time Series Retrieval System for Discovering Insights"></a>Temporal Treasure Hunt: Content-based Time Series Retrieval System for Discovering Insights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02560">http://arxiv.org/abs/2311.02560</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chin-Chia Michael Yeh, Huiyuan Chen, Xin Dai, Yan Zheng, Yujie Fan, Vivian Lai, Junpeng Wang, Audrey Der, Zhongfang Zhuang, Liang Wang, Wei Zhang</li>
<li>for: 本研究旨在解决多个领域时序数据库中的时序数据检索问题。</li>
<li>methods: 本研究使用了多种流行的时序数据模型化和检索方法，并提出了一种新的距离学习模型来解决这个问题。</li>
<li>results: 对于多个领域时序数据库中的时序数据检索问题，新的距离学习模型表现出色，超过了现有的方法。<details>
<summary>Abstract</summary>
Time series data is ubiquitous across various domains such as finance, healthcare, and manufacturing, but their properties can vary significantly depending on the domain they originate from. The ability to perform Content-based Time Series Retrieval (CTSR) is crucial for identifying unknown time series examples. However, existing CTSR works typically focus on retrieving time series from a single domain database, which can be inadequate if the user does not know the source of the query time series. This limitation motivates us to investigate the CTSR problem in a scenario where the database contains time series from multiple domains. To facilitate this investigation, we introduce a CTSR benchmark dataset that comprises time series data from a variety of domains, such as motion, power demand, and traffic. This dataset is sourced from a publicly available time series classification dataset archive, making it easily accessible to researchers in the field. We compare several popular methods for modeling and retrieving time series data using this benchmark dataset. Additionally, we propose a novel distance learning model that outperforms the existing methods. Overall, our study highlights the importance of addressing the CTSR problem across multiple domains and provides a useful benchmark dataset for future research.
</details>
<details>
<summary>摘要</summary>
时序数据在不同领域 everywhere，如金融、医疗和制造等，但它们的属性可以很大不同。能够实现基于内容的时序数据检索（CTSR）是识别未知时序例子的重要能力。然而，现有的CTSR工作通常将注意力集中在单一领域数据库上，这可能不够用于用户不知道查询时序序列的来源。这种限制使我们感到需要调查多个领域数据库中的CTSR问题。为了实现这一目的，我们提出了一个CTSRBenchmark dataset，该dataset包含多个领域的时序数据，如运动、电力需求和交通。这些数据来自公共可用时序分类数据集存档，因此可以让研究人员在领域中轻松地访问。我们比较了多种流行的时序数据模型化和检索方法，并提出了一种新的距离学习模型，该模型在CTSRBenchmark dataset上表现出色。总之，我们的研究强调了跨多个领域的CTSR问题的重要性，并提供了一个有用的CTSRBenchmark dataset，为未来的研究提供了便利。
</details></li>
</ul>
<hr>
<h2 id="Fast-Minimization-of-Expected-Logarithmic-Loss-via-Stochastic-Dual-Averaging"><a href="#Fast-Minimization-of-Expected-Logarithmic-Loss-via-Stochastic-Dual-Averaging" class="headerlink" title="Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging"></a>Fast Minimization of Expected Logarithmic Loss via Stochastic Dual Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02557">http://arxiv.org/abs/2311.02557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-En Tsai, Hao-Chung Cheng, Yen-Huan Li</li>
<li>for: 这个论文目的是将预期对数损失最小化，并且考虑了概率简单体和量子激发函数的问题。</li>
<li>methods: 这个论文使用了 Stochastic First-Order Algorithm with Logarithmic Barrier， named $B$-sample stochastic dual averaging。</li>
<li>results: 这个算法可以在 $\tilde{O} (d^2&#x2F;\varepsilon^2)$ 时间内获得 $\varepsilon$-优解，与现有的概率方法减少了 $d^{2\omega-2}$ 的时间复杂度，超过了批处理方法的时间复杂度 $d^2$。<details>
<summary>Abstract</summary>
Consider the problem of minimizing an expected logarithmic loss over either the probability simplex or the set of quantum density matrices. This problem encompasses tasks such as solving the Poisson inverse problem, computing the maximum-likelihood estimate for quantum state tomography, and approximating positive semi-definite matrix permanents with the currently tightest approximation ratio. Although the optimization problem is convex, standard iteration complexity guarantees for first-order methods do not directly apply due to the absence of Lipschitz continuity and smoothness in the loss function.   In this work, we propose a stochastic first-order algorithm named $B$-sample stochastic dual averaging with the logarithmic barrier. For the Poisson inverse problem, our algorithm attains an $\varepsilon$-optimal solution in $\tilde{O} (d^2/\varepsilon^2)$ time, matching the state of the art. When computing the maximum-likelihood estimate for quantum state tomography, our algorithm yields an $\varepsilon$-optimal solution in $\tilde{O} (d^3/\varepsilon^2)$ time, where $d$ denotes the dimension. This improves on the time complexities of existing stochastic first-order methods by a factor of $d^{\omega-2}$ and those of batch methods by a factor of $d^2$, where $\omega$ denotes the matrix multiplication exponent. Numerical experiments demonstrate that empirically, our algorithm outperforms existing methods with explicit complexity guarantees.
</details>
<details>
<summary>摘要</summary>
问题是最小化预期的含阶函数损失的问题，这个问题包括解决波索因 inverse 问题、计算量子状态探测的最大可能性估计、以及使用当前最紧的比率来近似正semidefinite 矩阵的 permanents。尽管优化问题是凸的，但标准的第一阶方法的证明不直接适用，因为损失函数没有 lipschitz 连续和光滑性。在这篇文章中，我们提出了一种 Stochastic first-order 算法，名为 $B$-sample stochastic dual averaging with logarithmic barrier。对于波索因 inverse 问题，我们的算法可以在 $\tilde{O} (d^2/\varepsilon^2)$ 时间内获得 $\varepsilon$-优的解，与当前状态之冲突。当计算量子状态探测的最大可能性估计时，我们的算法可以在 $\tilde{O} (d^3/\varepsilon^2)$ 时间内获得 $\varepsilon$-优的解，其中 $d$ 是维度。这比现有的随机第一阶方法的时间复杂度增加 $d^{\omega}-2}$，并且比批处理方法增加 $d^2$，其中 $\omega$ 是矩阵乘法 exponent。实验表明，我们的算法在实际中比现有的方法 WITH 显式复杂度保证更好。
</details></li>
</ul>
<hr>
<h2 id="High-dimensional-Bid-Learning-for-Energy-Storage-Bidding-in-Energy-Markets"><a href="#High-dimensional-Bid-Learning-for-Energy-Storage-Bidding-in-Energy-Markets" class="headerlink" title="High-dimensional Bid Learning for Energy Storage Bidding in Energy Markets"></a>High-dimensional Bid Learning for Energy Storage Bidding in Energy Markets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02551">http://arxiv.org/abs/2311.02551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyu Liu, Hongye Guo, Qinghu Tang, En Lu, Qiuna Cai, Qixin Chen</li>
<li>for:  optimize the profitability of Energy Storage Systems (ESSs) in electricity markets with high volatility</li>
<li>methods:  modify the common reinforcement learning (RL) process with a new bid representation method called Neural Network Embedded Bids (NNEBs), which represents market bids as monotonic neural networks with discrete outputs</li>
<li>results:  achieve 18% higher profit than the baseline and up to 78% profit of the optimal market bidder through experiments on real-world market datasets<details>
<summary>Abstract</summary>
With the growing penetration of renewable energy resource, electricity market prices have exhibited greater volatility. Therefore, it is important for Energy Storage Systems(ESSs) to leverage the multidimensional nature of energy market bids to maximize profitability. However, current learning methods cannot fully utilize the high-dimensional price-quantity bids in the energy markets. To address this challenge, we modify the common reinforcement learning(RL) process by proposing a new bid representation method called Neural Network Embedded Bids (NNEBs). NNEBs refer to market bids that are represented by monotonic neural networks with discrete outputs. To achieve effective learning of NNEBs, we first learn a neural network as a strategic mapping from the market price to ESS power output with RL. Then, we re-train the network with two training modifications to make the network output monotonic and discrete. Finally, the neural network is equivalently converted into a high-dimensional bid for bidding. We conducted experiments over real-world market datasets. Our studies show that the proposed method achieves 18% higher profit than the baseline and up to 78% profit of the optimal market bidder.
</details>
<details>
<summary>摘要</summary>
NNEBs refer to market bids that are represented by monotonic neural networks with discrete outputs. To effectively learn NNEBs, we first learn a neural network as a strategic mapping from the market price to ESS power output with RL. Then, we re-train the network with two training modifications to make the network output monotonic and discrete. Finally, the neural network is equivalently converted into a high-dimensional bid for bidding.We conducted experiments over real-world market datasets. Our studies show that the proposed method achieves 18% higher profit than the baseline and up to 78% profit of the optimal market bidder.
</details></li>
</ul>
<hr>
<h2 id="Preliminary-Analysis-on-Second-Order-Convergence-for-Biased-Policy-Gradient-Methods"><a href="#Preliminary-Analysis-on-Second-Order-Convergence-for-Biased-Policy-Gradient-Methods" class="headerlink" title="Preliminary Analysis on Second-Order Convergence for Biased Policy Gradient Methods"></a>Preliminary Analysis on Second-Order Convergence for Biased Policy Gradient Methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.02546">http://arxiv.org/abs/2311.02546</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siqiao Mu, Diego Klabjan</li>
<li>for: 研究 policy gradient 算法在非凸函数空间中的 globally optimal 性和稳定性。</li>
<li>methods: 使用非凸函数结构的 regularity  assumptions, 以及 second-order  guarantee 技巧，从而实现更好的 convergence 性。</li>
<li>results: 提供了 biased policy gradient 算法的 preliminary 结果，并且采用 nonconvex 优化 技巧进行证明。  future work 将是提供actor-critic 算法的 finite-time second-order convergence 分析。<details>
<summary>Abstract</summary>
Although the convergence of policy gradient algorithms to first-order stationary points is well-established, the objective functions of reinforcement learning problems are typically highly nonconvex. Therefore, recent work has focused on two extensions: ``global" convergence guarantees under regularity assumptions on the function structure, and second-order guarantees for escaping saddle points and convergence to true local minima. Our work expands on the latter approach, avoiding the restrictive assumptions of the former that may not apply to general objective functions. Existing results on vanilla policy gradient only consider an unbiased gradient estimator, but practical implementations under the infinite-horizon discounted setting, including both Monte-Carlo methods and actor-critic methods, involve gradient descent updates with a biased gradient estimator. We present preliminary results on the convergence of biased policy gradient algorithms to second-order stationary points, leveraging proof techniques from nonconvex optimization. In our next steps we aim to provide the first finite-time second-order convergence analysis for actor-critic algorithms.
</details>
<details>
<summary>摘要</summary>
although the convergence of policy gradient algorithms to first-order stationary points is well-established, the objective functions of reinforcement learning problems are typically highly nonconvex. therefore, recent work has focused on two extensions: "global" convergence guarantees under regularity assumptions on the function structure, and second-order guarantees for escaping saddle points and convergence to true local minima. our work expands on the latter approach, avoiding the restrictive assumptions of the former that may not apply to general objective functions. existing results on vanilla policy gradient only consider an unbiased gradient estimator, but practical implementations under the infinite-horizon discounted setting, including both monte-carlo methods and actor-critic methods, involve gradient descent updates with a biased gradient estimator. we present preliminary results on the convergence of biased policy gradient algorithms to second-order stationary points, leveraging proof techniques from nonconvex optimization. in our next steps, we aim to provide the first finite-time second-order convergence analysis for actor-critic algorithms.Here's the translation in Traditional Chinese:although the convergence of policy gradient algorithms to first-order stationary points is well-established, the objective functions of reinforcement learning problems are typically highly nonconvex. therefore, recent work has focused on two extensions: "global" convergence guarantees under regularity assumptions on the function structure, and second-order guarantees for escaping saddle points and convergence to true local minima. our work expands on the latter approach, avoiding the restrictive assumptions of the former that may not apply to general objective functions. existing results on vanilla policy gradient only consider an unbiased gradient estimator, but practical implementations under the infinite-horizon discounted setting, including both monte-carlo methods and actor-critic methods, involve gradient descent updates with a biased gradient estimator. we present preliminary results on the convergence of biased policy gradient algorithms to second-order stationary points, leveraging proof techniques from nonconvex optimization. in our next steps, we aim to provide the first finite-time second-order convergence analysis for actor-critic algorithms.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/05/cs.LG_2023_11_05/" data-id="cloq1wl8t00sd7o887zgn48ud" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/88/">88</a><a class="extend next" rel="next" href="/page/3/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">120</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">117</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">68</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
