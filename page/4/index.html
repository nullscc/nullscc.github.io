
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/4/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.LG_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.LG_2023_08_23/">cs.LG - 2023-08-23 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection"><a href="#The-Challenges-of-Machine-Learning-for-Trust-and-Safety-A-Case-Study-on-Misinformation-Detection" class="headerlink" title="The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection"></a>The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12215">http://arxiv.org/abs/2308.12215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ramybaly/News-Media-Reliability">https://github.com/ramybaly/News-Media-Reliability</a></li>
<li>paper_authors: Madelyne Xiao, Jonathan Mayer</li>
<li>for: 本研究旨在探讨机器学习在信任和安全问题上的应用，以推识信息检测为例。</li>
<li>methods: 研究者通过系мати化文献检测自动推识谣言的方法，从270篇引用次多的论文中检测出了 significan t shortcomings。</li>
<li>results: 研究发现，现有的论文中的检测任务与实际应用中的挑战存在 significan t的差异，数据和代码的可用性差、模型评价方法不独立、模型在不同领域数据上的泛化能力差。<details>
<summary>Abstract</summary>
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is for future work to avoid the pitfalls that we identify.
</details>
<details>
<summary>摘要</summary>
我们检查机器学习在保障和安全问题上的应用中存在的偏差，用误信息探测作为例子。我们系统化了 relate field 中270篇最具影响力的论文，然后对这些论文中的数据和代码可用性、设计异常、可重现性和泛化能力进行了检查。我们发现了 significante 缺陷，这些缺陷可能会质疑已经宣称的性能和实用性。检测任务与在线服务实际面临的挑战有很大差异，数据集和模型评估 часто不符合实际情况，评估 часто与模型训练无关。数据和代码可用性很差，模型无法在域外数据上Generalization well。基于这些结果，我们提出了评估机器学习应用到保障和安全问题的建议。我们的目标是为未来的工作避免这些偏差。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies"><a href="#Learning-to-Learn-Financial-Networks-for-Optimising-Momentum-Strategies" class="headerlink" title="Learning to Learn Financial Networks for Optimising Momentum Strategies"></a>Learning to Learn Financial Networks for Optimising Momentum Strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12212">http://arxiv.org/abs/2308.12212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stefan Zohren, Stephen Roberts, Xiaowen Dong</li>
<li>for: 这篇论文旨在提供一种新的风险豁免，即基于金融网络的资产关系来预测未来的回报。</li>
<li>methods: 本论文提出了一种基于机器学习的综合学习框架，即L2GMOM，可以同时学习金融网络和优化交易信号。L2GMOM的模型是一种高度可读性的神经网络，其架构来自于算法抽象。</li>
<li>results: 根据64个连续未来合约的回报测试，L2GMOM可以显著提高股票资产的利润和风险控制，Sharpe比率为1.74，覆盖20年的时间段。<details>
<summary>Abstract</summary>
Network momentum provides a novel type of risk premium, which exploits the interconnections among assets in a financial network to predict future returns. However, the current process of constructing financial networks relies heavily on expensive databases and financial expertise, limiting accessibility for small-sized and academic institutions. Furthermore, the traditional approach treats network construction and portfolio optimisation as separate tasks, potentially hindering optimal portfolio performance. To address these challenges, we propose L2GMOM, an end-to-end machine learning framework that simultaneously learns financial networks and optimises trading signals for network momentum strategies. The model of L2GMOM is a neural network with a highly interpretable forward propagation architecture, which is derived from algorithm unrolling. The L2GMOM is flexible and can be trained with diverse loss functions for portfolio performance, e.g. the negative Sharpe ratio. Backtesting on 64 continuous future contracts demonstrates a significant improvement in portfolio profitability and risk control, with a Sharpe ratio of 1.74 across a 20-year period.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy"><a href="#ULDP-FL-Federated-Learning-with-Across-Silo-User-Level-Differential-Privacy" class="headerlink" title="ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy"></a>ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12210">http://arxiv.org/abs/2308.12210</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fumiyukikato/uldp-fl">https://github.com/fumiyukikato/uldp-fl</a></li>
<li>paper_authors: Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa</li>
<li>for: This paper aims to provide user-level differential privacy (DP) in cross-silo federated learning (FL) settings, where a single user’s data may belong to multiple silos.</li>
<li>methods: The proposed algorithm, called ULDP-FL, uses per-user weighted clipping to directly ensure user-level DP, departing from group-privacy approaches. The algorithm also utilizes cryptographic building blocks to enhance its utility and provide private implementation.</li>
<li>results: The authors provide a theoretical analysis of the algorithm’s privacy and utility, and showcase substantial improvements in privacy-utility trade-offs under user-level DP compared to baseline methods through empirical experiments on real-world datasets.<details>
<summary>Abstract</summary>
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under user-level DP compared to baseline methods. To the best of our knowledge, our work is the first FL framework that effectively provides user-level DP in the general cross-silo FL setting.
</details>
<details>
<summary>摘要</summary>
受众级 differentially private federated learning（DP-FL）已经吸引了关注，这是一种合作机器学习方法，确保正式隐私。大多数 DP-FL 方法在每个缓存中保证了DP，但是一个用户的数据可能会分布在多个缓存中，并且未知用户级DP保证。在这种情况下，我们提出了ULDP-FL，一种新的 federated learning 框架，确保了用户级DP在跨缓存FL中。我们的提议算法直接确保用户级DP通过每个用户的质量截断，而不是GROUP-privacy方法。我们提供了算法的理论分析，包括隐私和用户性能的分析。此外，我们还提高了算法的用户性能，并使用密码学建筑块实现私有的实现。实验表明，我们的方法在保证用户级DP的情况下，与基eline方法相比，在隐私-用户性能质量上具有显著提升。我们知道，我们的工作是首次在通用跨缓存FL设置中提供了用户级DP的FL框架。
</details></li>
</ul>
<hr>
<h2 id="Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details"><a href="#Curriculum-Learning-with-Adam-The-Devil-Is-in-the-Wrong-Details" class="headerlink" title="Curriculum Learning with Adam: The Devil Is in the Wrong Details"></a>Curriculum Learning with Adam: The Devil Is in the Wrong Details</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12202">http://arxiv.org/abs/2308.12202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Weber, Jaap Jumelet, Paul Michel, Elia Bruni, Dieuwke Hupkes</li>
<li>for: 本研究旨在探讨CURRENT LEARNING PROGRESS（CL）方法在自然语言处理（NLP）领域的应用。</li>
<li>methods: 本研究使用了许多最近的CURRENT LEARNING PROGRESS方法的复制和扩展，并发现这些方法在NLP领域的结果具有许多不稳定性。</li>
<li>results: 研究发现，当CURRENT LEARNING PROGRESS方法与受欢迎的Adam优化算法结合使用时，它们经常学习到不适合选择的优化参数，导致结果不佳。研究还发现，不同的手动和自动CL方法在不同的场景下的表现都不佳， none of them outperforms optimisation with only Adam with well-chosen hyperparameters。<details>
<summary>Abstract</summary>
Curriculum learning (CL) posits that machine learning models -- similar to humans -- may learn more efficiently from data that match their current learning progress. However, CL methods are still poorly understood and, in particular for natural language processing (NLP), have achieved only limited success. In this paper, we explore why. Starting from an attempt to replicate and extend a number of recent curriculum methods, we find that their results are surprisingly brittle when applied to NLP. A deep dive into the (in)effectiveness of the curricula in some scenarios shows us why: when curricula are employed in combination with the popular Adam optimisation algorithm, they oftentimes learn to adapt to suboptimally chosen optimisation parameters for this algorithm. We present a number of different case studies with different common hand-crafted and automated CL approaches to illustrate this phenomenon, and we find that none of them outperforms optimisation with only Adam with well-chosen hyperparameters. As such, our results contribute to understanding why CL methods work, but at the same time urge caution when claiming positive results.
</details>
<details>
<summary>摘要</summary>
学习课程（CL）认为机器学习模型，类似于人类，可能更有效地从匹配其当前学习进程的数据中学习。然而，CL方法仍然不够了解，特别是在自然语言处理（NLP）领域，只有有限的成功。在这篇论文中，我们探究了这一点。从尝试复制和扩展一些最近的课程方法开始，我们发现它们在NLP领域的结果很有限制。我们进行了深入的分析，发现在使用Adam优化算法时，课程经常学习适应不合适的优化参数。我们通过不同的常见手动编制和自动生成CL方法的几个案例研究，发现 none of them outperforms 仅使用Adam算法和合适的超参数。因此，我们的结果对CL方法的工作 Mechanism 提供了更深入的理解，同时也警告使用CL方法时应该有谨慎。
</details></li>
</ul>
<hr>
<h2 id="Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network"><a href="#Predicting-Drug-Solubility-Using-Different-Machine-Learning-Methods-–-Linear-Regression-Model-with-Extracted-Chemical-Features-vs-Graph-Convolutional-Neural-Network" class="headerlink" title="Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network"></a>Predicting Drug Solubility Using Different Machine Learning Methods – Linear Regression Model with Extracted Chemical Features vs Graph Convolutional Neural Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12325">http://arxiv.org/abs/2308.12325</a></li>
<li>repo_url: None</li>
<li>paper_authors: John Ho, Zhao-Heng Yin, Colin Zhang, Henry Overhauser, Kyle Swanson, Yang Ha</li>
<li>for: 预测药物的溶解度是药品研发领域中重要的任务，这个问题在现代计算机资源的帮助下得到了进一步研究。</li>
<li>methods: 本研究使用了两种机器学习模型：一个线性回归模型和一个图 convolutional neural network 模型，在多个实验数据集上应用了这两种方法。两种方法都可以做出合理的预测，但GCNN模型的性能最好。</li>
<li>results: 使用GCNN模型可以获得比较好的预测结果，但目前GCNN模型是一个黑盒模型，而线性回归模型的特征重要性分析可以提供更多有关化学结构下的物理性质的信息。使用线性回归模型，我们显示了各种 функциональ组在总溶解度上的影响。<details>
<summary>Abstract</summary>
Predicting the solubility of given molecules is an important task in the pharmaceutical industry, and consequently this is a well-studied topic. In this research, we revisited this problem with the advantage of modern computing resources. We applied two machine learning models, a linear regression model and a graph convolutional neural network model, on multiple experimental datasets. Both methods can make reasonable predictions while the GCNN model had the best performance. However, the current GCNN model is a black box, while feature importance analysis from the linear regression model offers more insights into the underlying chemical influences. Using the linear regression model, we show how each functional group affects the overall solubility. Ultimately, knowing how chemical structure influences chemical properties is crucial when designing new drugs. Future work should aim to combine the high performance of GCNNs with the interpretability of linear regression, unlocking new advances in next generation high throughput screening.
</details>
<details>
<summary>摘要</summary>
预测给定分子的溶解度是药物工业中重要的任务，因此这是一个广泛研究的话题。在这项研究中，我们利用现代计算资源重新探讨了这个问题。我们使用了两种机器学习模型：线性回归模型和图 convolutional neural network 模型，并在多个实验数据集上应用了它们。两种方法都可以做出合理的预测，但GCNN模型的性能最佳。然而，当前GCNN模型是黑盒模型，而线性回归模型的功能重要性分析却可以提供更多有关化学影响的启示。使用线性回归模型，我们展示了每个 функциональ组如何影响总的溶解度。最终，知道化学结构如何影响化学性质是设计新药物的关键。未来的工作应该努力将高性能的 GCNN 与可解释的线性回归结合起来，这将开启下一代高通过筛选的新进展。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion"><a href="#Self-Supervised-Knowledge-Driven-Deep-Learning-for-3D-Magnetic-Inversion" class="headerlink" title="Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion"></a>Self-Supervised Knowledge-Driven Deep Learning for 3D Magnetic Inversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12193">http://arxiv.org/abs/2308.12193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinshuo Li, Zhuo Jia, Wenkai Lu, Cao Song</li>
<li>for: 这个研究的目的是提出一种基于自我监督深度学习的非破坏性地磁探测方法，以便估算地下 Distribution of susceptibility.</li>
<li>methods: 这个方法使用了自我监督深度学习，并且具有知识驱动模组，以便更好地解释模型的运作。</li>
<li>results: 实验结果显示，提出的方法可以实现高效的地磁探测，并且可以提供更好的结果。<details>
<summary>Abstract</summary>
The magnetic inversion method is one of the non-destructive geophysical methods, which aims to estimate the subsurface susceptibility distribution from surface magnetic anomaly data. Recently, supervised deep learning methods have been widely utilized in lots of geophysical fields including magnetic inversion. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magnetic inversion by self-supervised deep learning. The proposed self-supervised knowledge-driven 3D magnetic inversion method (SSKMI) learns on the target field data by a closed loop of the inversion and forward models. Given that the parameters of the forward model are preset, SSKMI can optimize the inversion model by minimizing the mean absolute error between observed and re-estimated surface magnetic anomalies. Besides, there is a knowledge-driven module in the proposed inversion model, which makes the deep learning method more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results. Since magnetic inversion is an ill-pose task, SSKMI proposed to constrain the inversion model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magnetic inversion method with outstanding performance.
</details>
<details>
<summary>摘要</summary>
magnetische inversie-methode is een van de niet-verwoestende geofysische methodes, die wordt gebruikt om de subsurface gelijkstroomsverdeling te schatten vanaf bovenstaande magneet-anomaliedata. Recent hebben supervisionele diepe-lerenmethodes werden breed toegepast in verschillende geofysische velden, waaronder magneet-inversie. However, these methods rely heavily on synthetic training data, whose performance is limited since the synthetic data is not independently and identically distributed with the field data. Thus, we proposed to realize magneet-inversie by self-supervised deep learning.De voorgestelde self-supervised knowledge-driven 3D magneet-inversie-methode (SSKMI) leert op het doelveld data van de inversie en de voorwaardelijke modellen. Given that the parameters of the voorwaardelijke model are preset, SSKMI can optimize the inversie model by minimizing the mean absolute error between observed and re-estimated surface magneet-anomalieën. Besides, there is a knowledge-driven module in the proposed inversie model, which makes the diepe-lerenmethode more explicable. Meanwhile, comparative experiments demonstrate that the knowledge-driven module can accelerate the training of the proposed method and achieve better results.Since magneet-inversie is an ill-pose task, SSKMI proposed to constrain the inversie model by a guideline in the auxiliary loop. The experimental results demonstrate that the proposed method is a reliable magneet-inversie-methode with outstanding performance.
</details></li>
</ul>
<hr>
<h2 id="Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques"><a href="#Robustness-Analysis-of-Continuous-Depth-Models-with-Lagrangian-Techniques" class="headerlink" title="Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques"></a>Robustness Analysis of Continuous-Depth Models with Lagrangian Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12192">http://arxiv.org/abs/2308.12192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sophie A. Neubauer, Radu Grosu</li>
<li>for: 这篇论文旨在统一地描述某些时间连续过程的有序和随机拉格朗日方法，以确定这些过程的行为稳定性。</li>
<li>methods: 这篇论文使用了LRT-NG、SLR和GoTube算法来构建紧凑的可达集，即在给定时间范围内可达的状态集。这些算法提供了确定性和随机性的保证。</li>
<li>results: 实验表明，使用拉格朗日方法可以比LRT、Flow*和CAPD更高效地分析不同类型的时间连续模型的稳定性。<details>
<summary>Abstract</summary>
This paper presents, in a unified fashion, deterministic as well as statistical Lagrangian-verification techniques. They formally quantify the behavioral robustness of any time-continuous process, formulated as a continuous-depth model. To this end, we review LRT-NG, SLR, and GoTube, algorithms for constructing a tight reachtube, that is, an over-approximation of the set of states reachable within a given time-horizon, and provide guarantees for the reachtube bounds. We compare the usage of the variational equations, associated to the system equations, the mean value theorem, and the Lipschitz constants, in achieving deterministic and statistical guarantees. In LRT-NG, the Lipschitz constant is used as a bloating factor of the initial perturbation, to compute the radius of an ellipsoid in an optimal metric, which over-approximates the set of reachable states. In SLR and GoTube, we get statistical guarantees, by using the Lipschitz constants to compute local balls around samples. These are needed to calculate the probability of having found an upper bound, of the true maximum perturbation at every timestep. Our experiments demonstrate the superior performance of Lagrangian techniques, when compared to LRT, Flow*, and CAPD, and illustrate their use in the robustness analysis of various continuous-depth models.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种统一的方法，即抽象的Lagrangian验证技术，可以确定时间连续过程的行为Robustness。这些技术基于构建一个紧缩的reachtube，即在给定时间范围内可达的状态的覆盖，并提供了reachtube bound的保证。文章介绍了LRT-NG、SLR和GoTube三种算法，并比较它们在使用变量方程、mean value theorem和Lipschitz常数来实现束缚和统计保证方面的表现。实验表明LAGRANGIAN技术在对LRT、Flow*和CAPD等方法的比较中表现出了superiority，并 ilustrated its use in various continuous-depth models的Robustness分析。
</details></li>
</ul>
<hr>
<h2 id="Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting"><a href="#Development-and-external-validation-of-a-lung-cancer-risk-estimation-tool-using-gradient-boosting" class="headerlink" title="Development and external validation of a lung cancer risk estimation tool using gradient-boosting"></a>Development and external validation of a lung cancer risk estimation tool using gradient-boosting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12188">http://arxiv.org/abs/2308.12188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/plbenveniste/lungcancerrisk">https://github.com/plbenveniste/lungcancerrisk</a></li>
<li>paper_authors: Pierre-Louis Benveniste, Julie Alberge, Lei Xing, Jean-Emmanuel Bibault</li>
<li>For: The paper aims to develop a machine learning tool for estimating the likelihood of developing lung cancer within five years, which can help individuals make informed decisions regarding lung cancer screening.* Methods: The study uses two datasets, PLCO and NLST, consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. The ML model is trained on the pre-processed PLCO dataset and tested on the NLST dataset, using XGBoost algorithm. The model incorporates features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer.* Results: The model is well-calibrated (Brier score&#x3D;0.044) and shows good performance on both datasets, with ROC-AUC of 82% on the PLCO dataset and 70% on the NLST dataset. The PR-AUC is 29% and 11% respectively. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years.<details>
<summary>Abstract</summary>
Lung cancer is a significant cause of mortality worldwide, emphasizing the importance of early detection for improved survival rates. In this study, we propose a machine learning (ML) tool trained on data from the PLCO Cancer Screening Trial and validated on the NLST to estimate the likelihood of lung cancer occurrence within five years. The study utilized two datasets, the PLCO (n=55,161) and NLST (n=48,595), consisting of comprehensive information on risk factors, clinical measurements, and outcomes related to lung cancer. Data preprocessing involved removing patients who were not current or former smokers and those who had died of causes unrelated to lung cancer. Additionally, a focus was placed on mitigating bias caused by censored data. Feature selection, hyper-parameter optimization, and model calibration were performed using XGBoost, an ensemble learning algorithm that combines gradient boosting and decision trees. The ML model was trained on the pre-processed PLCO dataset and tested on the NLST dataset. The model incorporated features such as age, gender, smoking history, medical diagnoses, and family history of lung cancer. The model was well-calibrated (Brier score=0.044). ROC-AUC was 82% on the PLCO dataset and 70% on the NLST dataset. PR-AUC was 29% and 11% respectively. When compared to the USPSTF guidelines for lung cancer screening, our model provided the same recall with a precision of 13.1% vs. 9.3% on the PLCO dataset and 3.2% vs. 3.1% on the NLST dataset. The developed ML tool provides a freely available web application for estimating the likelihood of developing lung cancer within five years. By utilizing risk factors and clinical data, individuals can assess their risk and make informed decisions regarding lung cancer screening. This research contributes to the efforts in early detection and prevention strategies, aiming to reduce lung cancer-related mortality rates.
</details>
<details>
<summary>摘要</summary>
全球范围内，肺癌是一种重要的死亡原因，因此早期检测的重要性得到了更多的注意。在本研究中，我们提出了一种基于机器学习（ML）技术的工具，用于在5年内lung cancer的发生可能性的估计。该工具基于PLCO癌症检测试验和NLST试验数据进行训练和验证。研究使用了两个数据集：PLCO（n=55,161）和NLST（n=48,595），这两个数据集包含了肺癌的风险因素、临床测量和结果的全面信息。数据处理包括移除不是当前或前任吸烟者和不相关于肺癌的死亡病人，以及减少偏见的报告数据。我们使用XGBoost算法进行特征选择、超参数优化和模型约束。模型在PLCO数据集上训练，并在NLST数据集上测试。模型包含了年龄、性别、吸烟历史、医疗诊断和家族史肺癌的特征。模型具有良好的准备（Brier分数=0.044），ROC-AUC为82%、PLCO数据集上和70%、NLST数据集上。PR-AUC分别为29%和11%。与美国预防服务委员会（USPSTF）肺癌检测指南相比，我们的模型具有同等的回快，但精度更高（9.3% vs. 13.1%、PLCO数据集上，3.1% vs. 3.2%、NLST数据集上）。我们开发的ML工具提供了一个免费的网上应用程序，用于在5年内lung cancer的发生可能性的估计。通过利用风险因素和临床数据，个人可以评估自己的风险，并做出了有知情的决策关于肺癌检测。这些研究贡献到了早期检测和预防策略的努力，以减少肺癌相关的死亡率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning"><a href="#Unsupervised-anomalies-detection-in-IIoT-edge-devices-networks-using-federated-learning" class="headerlink" title="Unsupervised anomalies detection in IIoT edge devices networks using federated learning"></a>Unsupervised anomalies detection in IIoT edge devices networks using federated learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12175">http://arxiv.org/abs/2308.12175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niyomukiza Thamar, Hossam Samy Elsaid Sharara<br>for: 这个研究是为了解决跨多个IoT设备之间的数据共享问题，以及保护数据的隐私问题。methods: 这个研究使用了联邦学习（Federated Learning）的分布式机器学习方法，并使用了FedAvg算法。results: 研究发现，使用FedAvg算法可以在IoT&#x2F;IIoT设备上进行机器学习模型训练，并且结果与中央化机器学习方法相似。但是，研究也发现了一些缺陷，例如在训练过程中不稳定的设备可能会导致伪阳性警示。为了解决这个问题，研究人员提出了一个公平的FedAvg算法，将在未来的工作中进行评估。<details>
<summary>Abstract</summary>
In a connection of many IoT devices that each collect data, normally training a machine learning model would involve transmitting the data to a central server which requires strict privacy rules. However, some owners are reluctant of availing their data out of the company due to data security concerns. Federated learning(FL) as a distributed machine learning approach performs training of a machine learning model on the device that gathered the data itself. In this scenario, data is not share over the network for training purpose. Fedavg as one of FL algorithms permits a model to be copied to participating devices during a training session. The devices could be chosen at random, and a device can be aborted. The resulting models are sent to the coordinating server and then average models from the devices that finished training. The process is repeated until a desired model accuracy is achieved. By doing this, FL approach solves the privacy problem for IoT/ IIoT devices that held sensitive data for the owners. In this paper, we leverage the benefits of FL and implemented Fedavg algorithm on a recent dataset that represent the modern IoT/ IIoT device networks. The results were almost the same as the centralized machine learning approach. We also evaluated some shortcomings of Fedavg such as unfairness that happens during the training when struggling devices do not participate for every stage of training. This inefficient training of local or global model could lead in a high number of false alarms in intrusion detection systems for IoT/IIoT gadgets developed using Fedavg. Hence, after evaluating the FedAv deep auto encoder with centralized deep auto encoder ML, we further proposed and designed a Fair Fedavg algorithm that will be evaluated in the future work.
</details>
<details>
<summary>摘要</summary>
在许多物联网设备之间的连接中，每个设备都会收集数据，通常需要将数据传输到中央服务器进行机器学习模型训练。然而，一些业主可能会拒绝将数据传输到公司外due to data security concerns。基于分布式机器学习的 Federated learning（FL）方法可以在设备上训练机器学习模型，从而解决数据隐私问题。在这种情况下，数据不会在训练过程中传输到网络。Fedavg是FL算法的一种实现，允许在训练过程中将模型复制到参与设备上。这些设备可以随机选择，并且设备可以被中止。获得的模型将被送往协调服务器，然后平均处理参与设备完成训练的模型。这个过程会重复，直到达到所需的模型精度。通过这种方法，FL方法解决了物联网/IIoT设备所拥有的敏感数据所有者的隐私问题。在这篇论文中，我们利用了FL的优点，并在当今物联网/IIoT设备网络上使用Fedavg算法进行实验。结果与中央机器学习方法的结果几乎相同。我们还评估了Fedavg的一些缺陷，如训练过程中不参与的设备会导致不公平性。这可能会导致在物联网/IIoT设备中的预测报警系统中出现高达数百个假报警。因此，我们在后续的工作中提出了一种公平的Fedavg算法，它将在未来进行评估。
</details></li>
</ul>
<hr>
<h2 id="Data-driven-decision-focused-surrogate-modeling"><a href="#Data-driven-decision-focused-surrogate-modeling" class="headerlink" title="Data-driven decision-focused surrogate modeling"></a>Data-driven decision-focused surrogate modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12161">http://arxiv.org/abs/2308.12161</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ddolab/decfocsurrmod">https://github.com/ddolab/decfocsurrmod</a></li>
<li>paper_authors: Rishabh Gupta, Qi Zhang</li>
<li>for: 这个论文是为了解决计算复杂非线性优化问题而设计的。</li>
<li>methods: 论文使用了数据驱动的框架，通过学习一个简单的、例如几何的优化模型，来减少决策预测错误。</li>
<li>results: 研究通过数字实验 validate了该框架，并与标准数据驱动模型相比较，表明该方法更加数据有效，并且生成了高决策预测精度的简单优化模型。<details>
<summary>Abstract</summary>
We introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings. The proposed data-driven framework seeks to learn a simpler, e.g. convex, surrogate optimization model that is trained to minimize the decision prediction error, which is defined as the difference between the optimal solutions of the original and the surrogate optimization models. The learning problem, formulated as a bilevel program, can be viewed as a data-driven inverse optimization problem to which we apply a decomposition-based solution algorithm from previous work. We validate our framework through numerical experiments involving the optimization of common nonlinear chemical processes such as chemical reactors, heat exchanger networks, and material blending systems. We also present a detailed comparison of decision-focused surrogate modeling with standard data-driven surrogate modeling methods and demonstrate that our approach is significantly more data-efficient while producing simple surrogate models with high decision prediction accuracy.
</details>
<details>
<summary>摘要</summary>
我团队提出了一种决策抽象模型的概念，用于在实时设置下解决复杂非线性优化问题。我们的数据驱动框架寻求学习一个更简单的，例如几何，优化模型，以减少决策预测错误，即原始优化模型和代理优化模型的优化解的差异。我们将这种学习问题视为一个数据驱动的反向优化问题，并应用之前的研究中的分解法。我们通过数值实验，包括化学反应器、热交换网络和材料混合系统的优化问题，证明了我们的方法的有效性。此外，我们还进行了标准数据驱动模型和我们方法的比较，发现我们的方法在数据效率方面明显高于标准方法，并生成了高决策预测精度的简单模型。
</details></li>
</ul>
<hr>
<h2 id="A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models"><a href="#A-Probabilistic-Fluctuation-based-Membership-Inference-Attack-for-Generative-Models" class="headerlink" title="A Probabilistic Fluctuation based Membership Inference Attack for Generative Models"></a>A Probabilistic Fluctuation based Membership Inference Attack for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12143">http://arxiv.org/abs/2308.12143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang</li>
<li>for: 该研究旨在开发一种黑盒式会员推断攻击（MIA），用于检测生成模型中是否存在记录。</li>
<li>methods: 该研究使用了现有的生成模型，并采用了多种正则化技术来避免过拟合。另外，该研究还使用了概率随机变量来检测生成模型中的记录分布变化。</li>
<li>results: 研究结果表明，在多个生成模型和数据集上，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基线。<details>
<summary>Abstract</summary>
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting these trends via analyzing the overall probabilistic fluctuations around given records. We conduct extensive experiments across multiple generative models and datasets, which demonstrate PFAMI can improve the attack success rate (ASR) by about 27.9% when compared with the best baseline.
</details>
<details>
<summary>摘要</summary>
具有简化的中文翻译如下：机器学习模型训练集中记录是否存在的问题被称为成员推断攻击（MIA）。在经典分类模型上，MIA已经得到了广泛的研究，而最近的研究又开始探索如何将MIA应用于生成模型。我们的调查表明，现有的生成模型MIA主要依赖于目标模型的过拟合。然而，过拟合可以通过多种正则化技术来避免，而现有的MIAs在实践中表现不佳。与过拟合不同的是，记忆是深度学习模型取得优化性能的关键因素，因此在生成模型中更常见。记忆导致生成记录随member记录的概率分布增长趋势，因此我们提出了一种基于概率变动的成员推断攻击方法（PFAMI）。PFAMI是一种黑盒子MIA，通过分析给定记录的总probabilistic fluctuations来推断成员。我们在多个生成模型和数据集上进行了广泛的实验，实验结果表明，PFAMI可以提高攻击成功率（ASR）约27.9%，相比最佳基准。
</details></li>
</ul>
<hr>
<h2 id="Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models"><a href="#Masking-Strategies-for-Background-Bias-Removal-in-Computer-Vision-Models" class="headerlink" title="Masking Strategies for Background Bias Removal in Computer Vision Models"></a>Masking Strategies for Background Bias Removal in Computer Vision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12127">http://arxiv.org/abs/2308.12127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ananthu-aniraj/masking_strategies_bias_removal">https://github.com/ananthu-aniraj/masking_strategies_bias_removal</a></li>
<li>paper_authors: Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, Diego Marcos</li>
<li>for: 本研究探讨了细化图像分类任务中背景引起的偏见问题，特别是当一些类别之间的差异非常微小，并且每个类别的样本数很少时，模型容易受到背景相关的偏见。</li>
<li>methods: 我们 investigate了两种遮盾策略来mitigate背景引起的偏见：早期遮盾（ removes background information at the input image level）和晚期遮盾（ selectively masks high-level spatial features corresponding to the background）。</li>
<li>results: 我们的实验结果表明，两种遮盾策略都能够提高模型对非典型背景的抗性，其中早期遮盾 consistently exhibiting the best OOD performance。另外，一种基于GAP-Pooled Patch token的ViT变体，combined with early masking, achieves the highest OOD robustness。<details>
<summary>Abstract</summary>
Models for fine-grained image classification tasks, where the difference between some classes can be extremely subtle and the number of samples per class tends to be low, are particularly prone to picking up background-related biases and demand robust methods to handle potential examples with out-of-distribution (OOD) backgrounds. To gain deeper insights into this critical problem, our research investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT). We explore two masking strategies to mitigate background-induced bias: Early masking, which removes background information at the (input) image level, and late masking, which selectively masks high-level spatial features corresponding to the background. Extensive experiments assess the behavior of CNN and ViT models under different masking strategies, with a focus on their generalization to OOD backgrounds. The obtained findings demonstrate that both proposed strategies enhance OOD performance compared to the baseline models, with early masking consistently exhibiting the best OOD performance. Notably, a ViT variant employing GAP-Pooled Patch token-based classification combined with early masking achieves the highest OOD robustness.
</details>
<details>
<summary>摘要</summary>
fine-grained 图像分类任务中，某些类别之间的差异可能非常微妙，同时每个类型的样本数往往很少，因此模型容易受到背景相关的偏见。为了更深入地理解这个重要问题，我们的研究investigates the impact of background-induced bias on fine-grained image classification, evaluating standard backbone models such as Convolutional Neural Network (CNN) and Vision Transformers (ViT).我们探索了两种遮盖策略来 mitigate background-induced bias：早期遮盖， removes background information at the (input) image level，和晚期遮盖， selectively masks high-level spatial features corresponding to the background。我们在不同的遮盖策略下进行了广泛的实验，专注于其对于不同背景的泛化性能。结果显示，我们所提出的两种策略都能够提高对于不同背景的性能，其中 early masking  consistently exhibits the best OOD performance。宁可是，一种基于 GAP-Pooled Patch token-based classification 的 ViT 变体，结合 early masking， achieved the highest OOD robustness。
</details></li>
</ul>
<hr>
<h2 id="An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization"><a href="#An-Accelerated-Block-Proximal-Framework-with-Adaptive-Momentum-for-Nonconvex-and-Nonsmooth-Optimization" class="headerlink" title="An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization"></a>An Accelerated Block Proximal Framework with Adaptive Momentum for Nonconvex and Nonsmooth Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12126">http://arxiv.org/abs/2308.12126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weifeng Yang, Wenwen Min</li>
<li>For: 非对称和非光滑优化问题* Methods: 加速块 proximal 线性框架（ABPL$^+$），以及随机排序更新阶段的扩展* Results: 可以 monotonically 降低函数值，并且可以在某些情况下提高步长和扩展参数，以及在多个实验中证明了其效果和灵活性。<details>
<summary>Abstract</summary>
We propose an accelerated block proximal linear framework with adaptive momentum (ABPL$^+$) for nonconvex and nonsmooth optimization. We analyze the potential causes of the extrapolation step failing in some algorithms, and resolve this issue by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm. Furthermore, we extends our algorithm to any scenario involving updating block variables with positive integers, allowing each cycle to randomly shuffle the update order of the variable blocks. Additionally, under mild assumptions, we prove that ABPL$^+$ can monotonically decrease the function value without strictly restricting the extrapolation parameters and step size, demonstrates the viability and effectiveness of updating these blocks in a random order, and we also more obviously and intuitively demonstrate that the derivative set of the sequence generated by our algorithm is a critical point set. Moreover, we demonstrate the global convergence as well as the linear and sublinear convergence rates of our algorithm by utilizing the Kurdyka-Lojasiewicz (K{\L}) condition. To enhance the effectiveness and flexibility of our algorithm, we also expand the study to the imprecise version of our algorithm and construct an adaptive extrapolation parameter strategy, which improving its overall performance. We apply our algorithm to multiple non-negative matrix factorization with the $\ell_0$ norm, nonnegative tensor decomposition with the $\ell_0$ norm, and perform extensive numerical experiments to validate its effectiveness and efficiency.
</details>
<details>
<summary>摘要</summary>
我们提出一种加速的块距离 próxima线性框架（ABPL$^+$）用于非拟合和非光滑优化。我们分析了一些算法中扩rapolation步骤失败的可能原因，并解决这个问题 by enhancing the comparison process that evaluates the trade-off between the proximal gradient step and the linear extrapolation step in our algorithm。此外，我们扩展了我们的算法，以便在更多的enario中更新块变量，并允许每个 цикла随机洗牌更新变量块的顺序。此外，在某些假设下，我们证明了ABPL$^+$可以 monotonically decrease the function value without strictly restricting the extrapolation parameters and step size，并且可以更加明确地示出该序列生成的 derivative set 是一个critical point set。此外，我们还证明了我们的算法的全球收敛性以及其线性和非线性收敛率，并使用Kurdyka-Lojasiewicz（K{\L））条件。为了提高我们的算法的效iveness和灵活性，我们还扩展了它的不精确版本，并构建了一种 adaptive extrapolation parameter strategy。我们应用我们的算法到多个非负矩阵因子化with the $\ell_0$ norm，非负tensor decomposition with the $\ell_0$ norm，并进行了广泛的数值实验来验证其效果和效率。
</details></li>
</ul>
<hr>
<h2 id="An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators"><a href="#An-Open-Source-ML-Based-Full-Stack-Optimization-Framework-for-Machine-Learning-Accelerators" class="headerlink" title="An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators"></a>An Open-Source ML-Based Full-Stack Optimization Framework for Machine Learning Accelerators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12120">http://arxiv.org/abs/2308.12120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hadi Esmaeilzadeh, Soroush Ghodrati, Andrew B. Kahng, Joon Kyung Kim, Sean Kinzer, Sayak Kundu, Rohan Mahapatra, Susmita Dey Manasi, Sachin Sapatnekar, Zhiang Wang, Ziqing Zeng</li>
<li>for: 这 paper 是为了探讨 físical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN machine learning (ML) algorithms。</li>
<li>methods: 这 paper 使用了 backend power, performance, and area (PPA) analysis 和 frontend performance simulation，以实现backend PPA 和系统指标如运行时和能耗的实际估计。此外，这 paper 还提出了一种自动化的设计空间探索技术，通过自动化搜索架构和后端参数，优化backend和系统指标。</li>
<li>results: 实验表明，这 paper 的方法可以准确预测backend PPA 和系统指标，平均预测错误为7%或更低，并在商业12nm进程和研究 oriented 45nm进程中实现了两个深度学习加速器平台（VTA和VeriGOOD-ML）的ASIC实现。<details>
<summary>Abstract</summary>
Parameterizable machine learning (ML) accelerators are the product of recent breakthroughs in ML. To fully enable their design space exploration (DSE), we propose a physical-design-driven, learning-based prediction framework for hardware-accelerated deep neural network (DNN) and non-DNN ML algorithms. It adopts a unified approach that combines backend power, performance, and area (PPA) analysis with frontend performance simulation, thereby achieving a realistic estimation of both backend PPA and system metrics such as runtime and energy. In addition, our framework includes a fully automated DSE technique, which optimizes backend and system metrics through an automated search of architectural and backend parameters. Experimental studies show that our approach consistently predicts backend PPA and system metrics with an average 7% or less prediction error for the ASIC implementation of two deep learning accelerator platforms, VTA and VeriGOOD-ML, in both a commercial 12 nm process and a research-oriented 45 nm process.
</details>
<details>
<summary>摘要</summary>
现代机器学习（ML）加速器的 Parameterizable 机制是 ML 的最新突破。为了充分利用设计空间探索（DSE），我们提议一种物理设计驱动、学习基于预测框架，用于硬件加速深度神经网络（DNN）和非 DNN ML 算法。它采用一种统一的方法，结合后端能力、性能和面积（PPA）分析与前端性能仿真，从而实现真实的 backend PPA 和系统指标（如运行时间和能耗）的估计。此外，我们的框架还包括一种完全自动化 DSE 技术，通过自动搜索设计和后端参数，实现最佳化 backend 和系统指标。实验研究显示，我们的方法可预测 backend PPA 和系统指标的平均差异为 7% 或更小，对 ASIC 实现的两种深度学习加速器平台（VTA 和 VeriGOOD-ML）在商用 12 nm 进程和研究 oriented 45 nm 进程中进行了可靠的预测。
</details></li>
</ul>
<hr>
<h2 id="Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity"><a href="#Less-is-More-–-Towards-parsimonious-multi-task-models-using-structured-sparsity" class="headerlink" title="Less is More – Towards parsimonious multi-task models using structured sparsity"></a>Less is More – Towards parsimonious multi-task models using structured sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12114">http://arxiv.org/abs/2308.12114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Upadhyay, Ronald Phlypo, Rajkumar Saini, Marcus Liwicki</li>
<li>for: 本研究旨在在多任务学习（MTL）框架中 incorporate 结构化群 sparse 性，以开发 fewer 参数的模型，能够有效地解决多个任务，同时保持与权重 dense 模型的性能相似或更高。</li>
<li>methods: 我们使用 channel-wise L1&#x2F;L2 群 sparse 性在共享层中，通过减少模型的内存占用量、计算需求和预测时间来降低模型的资源占用。</li>
<li>results: 我们通过对单任务和多任务实验表明，在 group sparsity 下，模型的性能与 dense 模型相似或更高，同时可以减少模型的参数数量。 我们还发现，适当减少 sparse 度可以提高模型的性能和简洁度。<details>
<summary>Abstract</summary>
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also investigate how changing the sparsification degree impacts both the performance of the model and the sparsity of groups.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:group sparsity in machine learning (ML) 推动更加简单、更易理解的模型，具有 fewer 活动参数组。本工作想要将结构化组简陋性 incorporated 到多任务学习 (MTL) 框架中，以开发更具有经济性的模型，可以更好地解决多个任务，而且具有更好的性能。在训练中减少模型的内存占用、计算需求和预测时间，可以提高模型的效率。我们在共享层中使用通道级 L1/L2 组简陋性，这种方法不仅可以消除无用的通道，还对权重进行 penalty，从而提高所有任务的学习。我们在 NYU-v2 和 CelebAMask-HQ 两个公开的 MTL 数据集上进行了单任务和多任务的实验，并 investigate 如何改变简陋化度对模型性能和组简陋性的影响。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Continual-Category-Discovery"><a href="#Generalized-Continual-Category-Discovery" class="headerlink" title="Generalized Continual Category Discovery"></a>Generalized Continual Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12112">http://arxiv.org/abs/2308.12112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Marczak, Grzegorz Rypeść, Sebastian Cygert, Tomasz Trzciński, Bartłomiej Twardowski</li>
<li>for: 这个论文是为了探讨一种新的常规学习（Continual Learning，CL）框架，它允许在任务之间学习新的分类和保持之前的知识。</li>
<li>methods: 该论文使用了一种基于常规分类发现（Generalized Category Discovery，GCD）的方法，允许在任务中存在新的分类和已知的分类，并使用了一种新的混合指导方法来减少忘记。</li>
<li>results: 实验表明，该方法可以在存在新分类的情况下积累知识，并且表现比较好，超过了一些强大的CL方法。<details>
<summary>Abstract</summary>
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of novel classes are present. In light of these limitations, we propose a method that incorporates both supervised and unsupervised signals and mitigates the forgetting through the use of centroid adaptation. Our method surpasses strong CL methods adopted for GCD techniques and presents a superior representation learning performance.
</details>
<details>
<summary>摘要</summary>
大多数 continual learning (CL) 方法都是在Supervised learning  Setting 中进行学习，agent 需要学习新的标注任务，而不是忘记之前的知识。然而，这些设置并不是实际生活中的enario ，因为学习 Agent 可以访问大量的无标注数据，包括未知类和已知类的示例。 drawing inspiration from Generalized Category Discovery (GCD)，我们介绍了一个新的框架，允许在任务中存在未知和已知类，并且使用 continual 版本的无标注学习方法来发现它们。我们称这种设置为 Generalized Continual Category Discovery (GCCD)。它将 CL 和 GCD 融合起来， bridge  Synthetic  benchmarks 和实际生活中的enario。我们通过一系列实验表明，现有的方法在 Subsequent 任务中不能够从无标注样本中积累知识。在这些限制下，我们提出了一种方法，该方法将 supervised 和无标注信号相互作用，以避免忘记。我们的方法超越了Strong CL 方法，并在 representation learning 中表现出优于其他方法。
</details></li>
</ul>
<hr>
<h2 id="Constrained-Stein-Variational-Trajectory-Optimization"><a href="#Constrained-Stein-Variational-Trajectory-Optimization" class="headerlink" title="Constrained Stein Variational Trajectory Optimization"></a>Constrained Stein Variational Trajectory Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12110">http://arxiv.org/abs/2308.12110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Power, Dmitry Berenson</li>
<li>for: 这篇论文的目的是提出一种具有约束的轨迹优化算法，以便在多条轨迹上同时满足约束。</li>
<li>methods: 这篇论文使用stein可变数 gradient descent（SVGD）来找到一组粒子，这组粒子可以 aproximate一个低成本轨迹分布，并且遵循约束。</li>
<li>results: 实验结果显示，这篇论文的算法可以在具有高度约束的任务中，比基eline更好地避免损坏和初始化问题，并且在7DoF夹寸推进任务中取得了20&#x2F;20次成功，比基eline的13&#x2F;20次成功率高。<details>
<summary>Abstract</summary>
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constrained tasks, such as a 7DoF wrench manipulation task, where CSVTO succeeds in 20/20 trials vs 13/20 for the closest baseline. Our results demonstrate that generating diverse constraint-satisfying trajectories improves robustness to disturbances and initialization over baselines.
</details>
<details>
<summary>摘要</summary>
我们介绍了受限制 Stein 变量梯度下降（CSVTO）算法，用于并行执行具有约束的轨迹优化。我们将受限制轨迹优化视为一种新的约束函数最小化问题，这种方法可以避免对约束进行处理，并使我们可以生成一个多样化的约束满足轨迹集。我们使用 Stein 变量梯度下降（SVGD）来找到一组粒子，这些粒子可以近似一个低成本轨迹分布，同时遵循约束。CSVTO 适用于具有平等和不平等约束的问题，并包括一种新的粒子重采样步骤，以避免局部最优解。通过生成多样化的约束满足轨迹集，CSVTO 能够更好地避免初始化和干扰的影响，并且更加稳定。我们在一个高度约束的 7DoF 工具抓取任务中，证明了CSVTO 比基eline更高效，CSVTO 在 20/20 次试验中成功，而基eline 只有 13/20 次成功。我们的结果表明，通过生成多样化的约束满足轨迹集，可以提高对干扰和初始化的 robustness。
</details></li>
</ul>
<hr>
<h2 id="Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient"><a href="#Quantifying-degeneracy-in-singular-models-via-the-learning-coefficient" class="headerlink" title="Quantifying degeneracy in singular models via the learning coefficient"></a>Quantifying degeneracy in singular models via the learning coefficient</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12108">http://arxiv.org/abs/2308.12108</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/edmundlth/scalable_learning_coefficient_with_sgld">https://github.com/edmundlth/scalable_learning_coefficient_with_sgld</a></li>
<li>paper_authors: Edmund Lau, Daniel Murfet, Susan Wei</li>
<li>for: 这篇论文的目的是解释深度神经网络（DNN）中的复杂的异常性。</li>
<li>methods: 这篇论文使用了 singular learning theory 中的一个量称为学习系数（learning coefficient），来量化 DNN 中的异常性。 它们还提出了一种可扩展的 Approximation 方法，使用游化 gradient Langevin dynamics，以便计算 localized 版本的学习系数。</li>
<li>results: 该论文的结果表明，local learning coefficient 可以准确地回归不同参数区域的异常性的排序。在 MNIST 实验中，local learning coefficient 能够揭示随机优化器对不同异常点的吸引力。<details>
<summary>Abstract</summary>
Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the \emph{learning coefficient} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）是单一的统计模型，具有复杂的多重性。在这项工作中，我们展示了singular学习理论中的一个量——学习系数，可以准确地量化深度神经网络中的多重性。重要的是，我们将证明 dass多重性在DNN中不可以通过简单地计数"平坦"方向来解释。我们提出了一种可扩展的本地化学习系数的计算方法，使用随机梯度质子泊利动力学。为验证我们的方法，我们在低维模型中展示了其准确性，并且可以正确地推断多重性的顺序。在MNIST实验中，本地学习系数可以揭示随机优化器对更或少多重极点的 inductive bias。
</details></li>
</ul>
<hr>
<h2 id="Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training"><a href="#Cached-Operator-Reordering-A-Unified-View-for-Fast-GNN-Training" class="headerlink" title="Cached Operator Reordering: A Unified View for Fast GNN Training"></a>Cached Operator Reordering: A Unified View for Fast GNN Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12093">http://arxiv.org/abs/2308.12093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julia Bazinska, Andrei Ivanov, Tal Ben-Nun, Nikoli Dryden, Maciej Besta, Siyuan Shen, Torsten Hoefler</li>
<li>for: 本文旨在提高图像网络（Graph Neural Networks，GNNs）的性能优化，以满足大规模图像模型的训练。</li>
<li>methods: 本文使用了一种统一的视角，对图像网络计算、输入&#x2F;输出和内存进行分析。基于图像 convolutional network（GCN）和图像注意力（GAT）层的计算图的分析，提出了一些 alternating computation strategies。</li>
<li>results: 提出的优化策略可以达到GCN中的速度提高（最高达2.43倍）和GAT中的速度提高（最高达1.94倍），同时减少内存占用。这些优化可以在不同的硬件平台上实现，并且有助于减轻训练大规模GNN模型的性能瓶颈。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are a powerful tool for handling structured graph data and addressing tasks such as node classification, graph classification, and clustering. However, the sparse nature of GNN computation poses new challenges for performance optimization compared to traditional deep neural networks. We address these challenges by providing a unified view of GNN computation, I/O, and memory. By analyzing the computational graphs of the Graph Convolutional Network (GCN) and Graph Attention (GAT) layers -- two widely used GNN layers -- we propose alternative computation strategies. We present adaptive operator reordering with caching, which achieves a speedup of up to 2.43x for GCN compared to the current state-of-the-art. Furthermore, an exploration of different caching schemes for GAT yields a speedup of up to 1.94x. The proposed optimizations save memory, are easily implemented across various hardware platforms, and have the potential to alleviate performance bottlenecks in training large-scale GNN models.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 是一种有力的工具，用于处理结构化图数据，并解决节点分类、图分类和聚类等任务。然而，GNN 的稀疏性导致了性能优化的新挑战，与传统深度神经网络相比。我们通过提供一种统一的视图，对 GNN 计算、输入和存储进行分析。通过分析图 convolutional network (GCN) 和 graph attention (GAT) 两种广泛使用的 GNN 层的计算图，我们提出了 alternate computation strategies。我们的提案包括 adaptive operator reordering with caching，可以达到 GCN 比现状态 искус的最大速度提升率为 2.43倍。此外，对 GAT 的缓存 schemes 的探索，可以达到最大速度提升率为 1.94倍。我们的优化措施可以降低训练大规模 GNN 模型的内存占用量，易于在不同硬件平台上实现，并有 Potential to alleviate performance bottlenecks in training large-scale GNN models。
</details></li>
</ul>
<hr>
<h2 id="Stabilizing-RNN-Gradients-through-Pre-training"><a href="#Stabilizing-RNN-Gradients-through-Pre-training" class="headerlink" title="Stabilizing RNN Gradients through Pre-training"></a>Stabilizing RNN Gradients through Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12075">http://arxiv.org/abs/2308.12075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Herranz-Celotti, Jean Rouat</li>
<li>for: 本研究旨在稳定和改进深度学习模型的训练，以避免梯度变化的扩散式增长。</li>
<li>methods: 本研究使用了先验学习稳定性的理论，并扩展了知名的稳定性条件（LSC）至更广泛的深度循环神经网络。</li>
<li>results: 研究发现，在应用классиical Glorot、He和Orthogonal初始化方案时， feed-forward fully-connected神经网络和深度循环神经网络都可以满足LSC。此外，研究还发现了一种新的权重加权问题，并提出了一种新的方法来解决这个问题。<details>
<summary>Abstract</summary>
Numerous theories of learning suggest to prevent the gradient variance from exponential growth with depth or time, to stabilize and improve training. Typically, these analyses are conducted on feed-forward fully-connected neural networks or single-layer recurrent neural networks, given their mathematical tractability. In contrast, this study demonstrates that pre-training the network to local stability can be effective whenever the architectures are too complex for an analytical initialization. Furthermore, we extend known stability theories to encompass a broader family of deep recurrent networks, requiring minimal assumptions on data and parameter distribution, a theory that we refer to as the Local Stability Condition (LSC). Our investigation reveals that the classical Glorot, He, and Orthogonal initialization schemes satisfy the LSC when applied to feed-forward fully-connected neural networks. However, analysing deep recurrent networks, we identify a new additive source of exponential explosion that emerges from counting gradient paths in a rectangular grid in depth and time. We propose a new approach to mitigate this issue, that consists on giving a weight of a half to the time and depth contributions to the gradient, instead of the classical weight of one. Our empirical results confirm that pre-training both feed-forward and recurrent networks to fulfill the LSC often results in improved final performance across models. This study contributes to the field by providing a means to stabilize networks of any complexity. Our approach can be implemented as an additional step before pre-training on large augmented datasets, and as an alternative to finding stable initializations analytically.
</details>
<details>
<summary>摘要</summary>
多种学习理论建议防止梯度差值的几何增长，以稳定和改进训练。通常，这些分析是对 fully-connected neural network 或 single-layer recurrent neural network 进行的，这些网络的数学性让其更易分析。然而，本研究表明，在网络太复杂，无法分析的情况下，先通过网络的本地稳定来初始化网络，可以取得良好的效果。此外，我们扩展了已知的稳定性理论，以覆盖更广泛的深度循环网络家族，这些网络的参数和数据分布假设最少。我们称之为本地稳定条件（LSC）。我们的调查发现，经典的 Glorot、He 和orthogonal 初始化方案满足 LSC 当应用于 fully-connected neural network。然而，对深度循环网络进行分析，我们发现了一种新的加法性梯度增长的问题，这种问题来自于在深度和时间方向上的 counting 梯度路径。我们提出了一种新的方法来解决这个问题，即将时间和深度方向的贡献权重设为 0.5，而不是经典的 1。我们的实验结果表明，在 feed-forward 和 recurrent 网络中，通过满足 LSC 来初始化网络，经常会导致最终性能的改进。这项研究对深度学习领域的稳定性做出了贡献，我们的方法可以作为训练之前的额外步骤，以及analytically 找到稳定初始化的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning"><a href="#Identifying-Reaction-Aware-Driving-Styles-of-Stochastic-Model-Predictive-Controlled-Vehicles-by-Inverse-Reinforcement-Learning" class="headerlink" title="Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning"></a>Identifying Reaction-Aware Driving Styles of Stochastic Model Predictive Controlled Vehicles by Inverse Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12069">http://arxiv.org/abs/2308.12069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ni Dang, Tao Shi, Zengjie Zhang, Wanxin Jin, Marion Leibold, Martin Buss</li>
<li>for: 这篇论文旨在提出一种基于Maximum Entropy Inverse Reinforcement Learning（ME-IRL）方法来识别自动驾驶车辆（AV）的驾驶模式。</li>
<li>methods: 该方法使用了一系列加权特征来定义驾驶模式，并提出了一些新的反应式特征来捕捉AV对附近AV的反应。</li>
<li>results: 通过使用修改后的ME-IRL方法和新提出的特征，该研究成功地识别了从权重精算控制（SMPC）生成的示范轨迹中的驾驶模式。<details>
<summary>Abstract</summary>
The driving style of an Autonomous Vehicle (AV) refers to how it behaves and interacts with other AVs. In a multi-vehicle autonomous driving system, an AV capable of identifying the driving styles of its nearby AVs can reliably evaluate the risk of collisions and make more reasonable driving decisions. However, there has not been a consistent definition of driving styles for an AV in the literature, although it is considered that the driving style is encoded in the AV's trajectories and can be identified using Maximum Entropy Inverse Reinforcement Learning (ME-IRL) methods as a cost function. Nevertheless, an important indicator of the driving style, i.e., how an AV reacts to its nearby AVs, is not fully incorporated in the feature design of previous ME-IRL methods. In this paper, we describe the driving style as a cost function of a series of weighted features. We design additional novel features to capture the AV's reaction-aware characteristics. Then, we identify the driving styles from the demonstration trajectories generated by the Stochastic Model Predictive Control (SMPC) using a modified ME-IRL method with our newly proposed features. The proposed method is validated using MATLAB simulation and an off-the-shelf experiment.
</details>
<details>
<summary>摘要</summary>
自动驾驶车（AV）的驾驶方式指的是它在行驶过程中的行为和与其他AV的交互方式。在多辆自动驾驶车系统中，能够识别附近AV的驾驶方式的AV可以更可靠地评估碰撞风险并做出更合理的驾驶决策。然而，在文献中没有一致的定义自动驾驶车的驾驶方式，尽管人们认为驾驶方式在AV的轨迹中被编码，可以使用最大 entropy inverse reinforcement learning（ME-IRL）方法来作为成本函数来识别。然而，附近AV的反应不完全包含在前一代ME-IRL方法中的特征设计中。在这篇论文中，我们定义了自动驾驶车的驾驶方式为一系列权重的特征函数。然后，我们设计了新的反应意外特征，以更好地捕捉AV的反应特征。最后，我们使用修改后的ME-IRL方法和我们新提出的特征来识别示例轨迹，并从示例轨迹中提取驾驶方式。我们的方法在MATLAB simulate和一个简易实验中得到验证。
</details></li>
</ul>
<hr>
<h2 id="InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4"><a href="#InstructionGPT-4-A-200-Instruction-Paradigm-for-Fine-Tuning-MiniGPT-4" class="headerlink" title="InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"></a>InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12067">http://arxiv.org/abs/2308.12067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lai Wei, Zihao Jiang, Weiran Huang, Lichao Sun</li>
<li>for: 本研究旨在探讨大型语言模型如何通过限量高质量 instrucion-following 数据进行训练，以提高其对多模态任务的执行能力。</li>
<li>methods: 本研究使用了两个阶段的训练方法：首先在图像-文本对的集合上进行预训练，然后在超过200个例子的指令数据上进行精度训练。</li>
<li>results: 研究发现，使用少量但高质量的 instrucion-following 数据可以使大型语言模型实现更好的输出。InstructionGPT-4 在视觉问答、GPT-4 偏好等多种评价中表现出色，超过了原始 MiniGPT-4。<details>
<summary>Abstract</summary>
Multimodal large language models acquire their instruction-following capabilities through a two-stage training process: pre-training on image-text pairs and fine-tuning on supervised vision-language instruction data. Recent studies have shown that large language models can achieve satisfactory results even with a limited amount of high-quality instruction-following data. In this paper, we introduce InstructionGPT-4, which is fine-tuned on a small dataset comprising only 200 examples, amounting to approximately 6% of the instruction-following data used in the alignment dataset for MiniGPT-4. We first propose several metrics to access the quality of multimodal instruction data. Based on these metrics, we present a simple and effective data selector to automatically identify and filter low-quality vision-language data. By employing this method, InstructionGPT-4 outperforms the original MiniGPT-4 on various evaluations (e.g., visual question answering, GPT-4 preference). Overall, our findings demonstrate that less but high-quality instruction tuning data is efficient to enable multimodal large language models to generate better output.
</details>
<details>
<summary>摘要</summary>
多模式大语言模型通过两个阶段训练过程获得 instrucion-following 能力：先于敏感图文对应和精度检测数据进行预训练，然后在精度检测数据上进行微调。当前研究表明，大语言模型可以通过有限量高质量 instrucion-following 数据实现满意的结果。在本文中，我们介绍 InstructionGPT-4，它通过一个小数据集（约6%的对Alignment dataset）进行微调，并使用我们提出的多个评价指标来自动选择和筛选低质量视听数据。通过这种方法，InstructionGPT-4 在视觉问答和 GPT-4 偏好等评价中表现出色，超过原始 MiniGPT-4。总的来说，我们的发现表明，更少但高质量的 instrucion-following 准则可以使得多模式大语言模型生成更好的输出。
</details></li>
</ul>
<hr>
<h2 id="Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference"><a href="#Pre-gated-MoE-An-Algorithm-System-Co-Design-for-Fast-and-Scalable-Mixture-of-Expert-Inference" class="headerlink" title="Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference"></a>Pre-gated MoE: An Algorithm-System Co-Design for Fast and Scalable Mixture-of-Expert Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12066">http://arxiv.org/abs/2308.12066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, Mao Yang, Minsoo Rhu</li>
<li>for: 提高大型语言模型（LLM）的性能和可扩展性。</li>
<li>methods: 使用 Mixture-of-Experts（MoE）架构，并提出了一种新的预先阻塞函数来缓解 sparse expert 的动态活动问题，从而实现高性能和低内存占用。</li>
<li>results: 对比 conventional MoE 架构，提出的 Pre-gated MoE 系统能够提高性能、降低 GPU 内存占用，同时保持模型质量不变。这些特点使得 Pre-gated MoE 系统可以低成本地部署大规模 LLM，只需要一个 GPU 实现高性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) based on transformers have made significant strides in recent years, the success of which is driven by scaling up their model size. Despite their high algorithmic performance, the computational and memory requirements of LLMs present unprecedented challenges. To tackle the high compute requirements of LLMs, the Mixture-of-Experts (MoE) architecture was introduced which is able to scale its model size without proportionally scaling up its computational requirements. Unfortunately, MoE's high memory demands and dynamic activation of sparse experts restrict its applicability to real-world problems. Previous solutions that offload MoE's memory-hungry expert parameters to CPU memory fall short because the latency to migrate activated experts from CPU to GPU incurs high performance overhead. Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs our novel pre-gating function which alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE is able to improve performance, reduce GPU memory consumption, while also maintaining the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.
</details>
<details>
<summary>摘要</summary>
Our proposed Pre-gated MoE system effectively tackles the compute and memory challenges of conventional MoE architectures using our algorithm-system co-design. Pre-gated MoE employs a novel pre-gating function that alleviates the dynamic nature of sparse expert activation, allowing our proposed system to address the large memory footprint of MoEs while also achieving high performance. We demonstrate that Pre-gated MoE can improve performance, reduce GPU memory consumption, and maintain the same level of model quality. These features allow our Pre-gated MoE system to cost-effectively deploy large-scale LLMs using just a single GPU with high performance.In simplified Chinese, the text would be:大型语言模型（LLMs）基于转换器的进步在最近几年很大，成功的原因是通过增加模型的大小来增加性能。然而，LLMs的高度计算和内存需求对应到前所未有的挑战。为了解决这些挑战，混合专家（MoE）架构被引入，它可以无需与计算需求成比例增加其模型大小来增加性能。然而，MoE的高内存需求和动态专家活动限制了它的实际应用。先前的解决方案，即将MoE的内存吃力强大的专家参数异步到CPU内存，不足以因为迁移到GPU的延迟会导致高性能开销。我们提出的预级MoE系统可以有效地解决计算和内存挑战，使用我们的算法-系统合作设计。预级MoE使用我们的新预级函数，解决了动态专家活动的问题，使我们的提议的系统可以面临大 Memory Footprint 的 MoE 问题，同时实现高性能。我们示出了预级MoE可以提高性能，减少GPU内存占用量，同时保持模型质量不变。这些特点使得我们的预级MoE系统可以效率地使用单个GPU进行大规模 LLMS 的部署，并且可以高性能。
</details></li>
</ul>
<hr>
<h2 id="Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers"><a href="#Ensembling-Uncertainty-Measures-to-Improve-Safety-of-Black-Box-Classifiers" class="headerlink" title="Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers"></a>Ensembling Uncertainty Measures to Improve Safety of Black-Box Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12065">http://arxiv.org/abs/2308.12065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Zoppi, Andrea Ceccarelli, Andrea Bondavalli</li>
<li>for: 提高机器学习模型的安全性，避免因误分类而导致的 kritical failures。</li>
<li>methods: 使用 ensemble of uncertainty measures 检测误分类，并在检测到误分类时阻止输出进行系统级别的处理。</li>
<li>results: SPROUT 能够准确地检测机器学习模型的误分类，并在检测到误分类时阻止输出进行系统级别的处理，从而提高机器学习模型的安全性。<details>
<summary>Abstract</summary>
Machine Learning (ML) algorithms that perform classification may predict the wrong class, experiencing misclassifications. It is well-known that misclassifications may have cascading effects on the encompassing system, possibly resulting in critical failures. This paper proposes SPROUT, a Safety wraPper thROugh ensembles of UncertainTy measures, which suspects misclassifications by computing uncertainty measures on the inputs and outputs of a black-box classifier. If a misclassification is detected, SPROUT blocks the propagation of the output of the classifier to the encompassing system. The resulting impact on safety is that SPROUT transforms erratic outputs (misclassifications) into data omission failures, which can be easily managed at the system level. SPROUT has a broad range of applications as it fits binary and multi-class classification, comprising image and tabular datasets. We experimentally show that SPROUT always identifies a huge fraction of the misclassifications of supervised classifiers, and it is able to detect all misclassifications in specific cases. SPROUT implementation contains pre-trained wrappers, it is publicly available and ready to be deployed with minimal effort.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）算法可能会预测错误的类别，导致错分。这是已知的事实，错分可能会对包含系统产生卷积效应，可能导致重要的失败。这篇论文提议了“SPROUT”，一个安全包装 Through ensembles of UncertainTy measures，它猜测错分的方式是计算输入和输出uncertainty measures的黑色框分类器。如果检测到了错分，SPROUT会阻止输出分类器的输出对包含系统的传递。这将导致安全性的改善，因为SPROUT将异常输出（错分）转化为数据损失失败，这可以轻松地在系统层面进行管理。SPROUT适用于二分和多分类фикация，包括图像和表格数据集。我们实验表明，SPROUT总能够检测出超级vised分类器中的大部分错分，而且在特定情况下，它能够检测所有错分。SPROUT实现包括预训练的包装，现在公开可用，只需要最小的努力就可以部署。
</details></li>
</ul>
<hr>
<h2 id="HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing"><a href="#HarvestNet-A-Dataset-for-Detecting-Smallholder-Farming-Activity-Using-Harvest-Piles-and-Remote-Sensing" class="headerlink" title="HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing"></a>HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using Harvest Piles and Remote Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12061">http://arxiv.org/abs/2308.12061</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard Lee, Chenlin Meng, Stefano Ermon, David Lobell</li>
<li>For: The paper aims to improve the accuracy of cropland mapping in smallholder farming regions, specifically in sub-Saharan Africa.* Methods: The authors use satellite images and expert knowledge to collect a dataset called HarvestNet, which includes 7,000 hand-labeled images and 2,000 ground-collected labels. They also benchmark several state-of-the-art models in remote sensing and compare their performance with a pre-existing coverage map.* Results: The authors achieve an accuracy of around 80% on hand-labeled data and 90%, 98% accuracy on ground truth data for Tigray and Amhara, respectively. They also detect an additional 56,621 hectares of cropland in Tigray using their method, which is not captured by the pre-existing coverage map.<details>
<summary>Abstract</summary>
Small farms contribute to a large share of the productive land in developing countries. In regions such as sub-Saharan Africa, where 80% of farms are small (under 2 ha in size), the task of mapping smallholder cropland is an important part of tracking sustainability measures such as crop productivity. However, the visually diverse and nuanced appearance of small farms has limited the effectiveness of traditional approaches to cropland mapping. Here we introduce a new approach based on the detection of harvest piles characteristic of many smallholder systems throughout the world. We present HarvestNet, a dataset for mapping the presence of farms in the Ethiopian regions of Tigray and Amhara during 2020-2023, collected using expert knowledge and satellite images, totaling 7k hand-labeled images and 2k ground collected labels. We also benchmark a set of baselines including SOTA models in remote sensing with our best models having around 80% classification performance on hand labelled data and 90%, 98% accuracy on ground truth data for Tigray, Amhara respectively. We also perform a visual comparison with a widely used pre-existing coverage map and show that our model detects an extra 56,621 hectares of cropland in Tigray. We conclude that remote sensing of harvest piles can contribute to more timely and accurate cropland assessments in food insecure region.
</details>
<details>
<summary>摘要</summary>
小规模农场在发展国家占较大的生产地面积。如在 SUB-SAHARAN AFRICA 地区，80% 的农场面积在 2 ha 以下， mapping 小holder 耕地是跟踪可持续发展的标准部署之一。然而，传统方法对小holder 耕地的映射受到 visually 多样和细节的限制。我们介绍了一种新的方法，基于耕地收割堆的检测，这种特征是许多小holder 系统中的共同特征。我们提供了 HarvestNet 数据集，用于在埃塞俄比亚地区的 Tigray 和 Amhara 地区在 2020-2023 年间的耕地映射。我们收集了 7000 个专家知识和卫星图像，以及 2000 个地面收集的标签。我们还对一些先进的远程感知模型进行了比较，我们的最佳模型在手动标注数据上达到了 80% 的分类性能，并在真实数据上达到了 90%、98% 的准确率。我们还进行了一个视觉比较，发现我们的模型可以检测到传统覆盖地图中缺失的 56,621 公顷耕地。我们结论认为，远程感知耕地收割堆可以为食 insecurities 地区提供更加准确和及时的耕地评估。
</details></li>
</ul>
<hr>
<h2 id="Manipulating-Embeddings-of-Stable-Diffusion-Prompts"><a href="#Manipulating-Embeddings-of-Stable-Diffusion-Prompts" class="headerlink" title="Manipulating Embeddings of Stable Diffusion Prompts"></a>Manipulating Embeddings of Stable Diffusion Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12059">http://arxiv.org/abs/2308.12059</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/arxiv23-prompt-embedding-manipulation">https://github.com/webis-de/arxiv23-prompt-embedding-manipulation</a></li>
<li>paper_authors: Niklas Deckers, Julia Peters, Martin Potthast</li>
<li>for: 这个研究旨在提高文本描述到图像的转换精度，通过直接修改提示语 embedding，从而实现更细化和具有目标性的控制。</li>
<li>methods: 我们提出了一种将生成图像模型看作连续函数，通过图像空间和提示语 embedding 空间之间传递梯度的方法，以实现更精细和具有目标性的控制。</li>
<li>results: 我们的实验表明，这种方法可以实现更高精度的图像转换，并且可以帮助用户更好地完成创意任务。<details>
<summary>Abstract</summary>
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative tasks by enabling them to navigate the image space along a selection of directions of "near" prompt embeddings. (3) Changing the embedding of the prompt to include information that the user has seen in a particular seed but finds difficult to describe in the prompt. Our experiments demonstrate the feasibility of the described methods.
</details>
<details>
<summary>摘要</summary>
“文本描述”を基于的生成图像模型，如稳定扩散，允许用户根据文本描述生成图像。但是，通过修改描述仍然是用户改变生成图像的主要方式。然而，通过修改描述来改变图像是一项困难的试验和错误过程，这导致了“提示工程”作为一种新的研究领域的出现。我们提出并分析了通过直接修改提示的embedding来改变图像的方法。这种方法允许更细化和targeted控制，考虑用户的意图。我们将生成文本到图像模型看作是连续函数，并在图像空间和提示 embedding 空间之间传递梯度。通过解决不同的用户互动问题，我们可以在以下三个场景中应用这个想法：1. 图像空间中定义的一个度量的优化，例如图像风格。2. 用户在创意任务中的帮助，通过让用户在“near”提示 embeddings 上导航图像空间来实现。3. 将提示 embedding 包含用户在种子中看到的信息，但是difficult to describe in the prompt。我们的实验表明这种方法的可行性。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks"><a href="#Sample-Complexity-of-Robust-Learning-against-Evasion-Attacks" class="headerlink" title="Sample Complexity of Robust Learning against Evasion Attacks"></a>Sample Complexity of Robust Learning against Evasion Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12054">http://arxiv.org/abs/2308.12054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pascale Gourdeau</li>
<li>for: 本研究的目的是理解机器学习模型对攻击的抵触性。</li>
<li>methods: 本文使用了学习理论的角度，研究了在攻击下学习的可行性，并考虑了样本复杂度。</li>
<li>results: 本文显示了对于随机样本only的情况，需要 distributional assumptions 来保证机器学习模型的抵触性。此外，如果攻击者只能对输入数据进行 $O(\log n)$ 位的偏移，那么可以robustly学习 conjunctions 和 decision lists w.r.t. log-Lipschitz distributions。在learner具有更多权限的情况下，本文研究了local membership queries 和 local equivalence query oracle，并提供了可行性和Upper bound的结论。<details>
<summary>Abstract</summary>
It is becoming increasingly important to understand the vulnerability of machine learning models to adversarial attacks. One of the fundamental problems in adversarial machine learning is to quantify how much training data is needed in the presence of evasion attacks, where data is corrupted at test time. In this thesis, we work with the exact-in-the-ball notion of robustness and study the feasibility of adversarially robust learning from the perspective of learning theory, considering sample complexity.   We first explore the setting where the learner has access to random examples only, and show that distributional assumptions are essential. We then focus on learning problems with distributions on the input data that satisfy a Lipschitz condition and show that robustly learning monotone conjunctions has sample complexity at least exponential in the adversary's budget (the maximum number of bits it can perturb on each input). However, if the adversary is restricted to perturbing $O(\log n)$ bits, then one can robustly learn conjunctions and decision lists w.r.t. log-Lipschitz distributions.   We then study learning models where the learner is given more power. We first consider local membership queries, where the learner can query the label of points near the training sample. We show that, under the uniform distribution, the exponential dependence on the adversary's budget to robustly learn conjunctions remains inevitable. We then introduce a local equivalence query oracle, which returns whether the hypothesis and target concept agree in a given region around a point in the training sample, and a counterexample if it exists. We show that if the query radius is equal to the adversary's budget, we can develop robust empirical risk minimization algorithms in the distribution-free setting. We give general query complexity upper and lower bounds, as well as for concrete concept classes.
</details>
<details>
<summary>摘要</summary>
“机器学习模型对于攻击性测试的漏洞日益增加的重要性。一个基本问题在于在恶意攻击下对于训练数据的需求量。在这些thesis中，我们运用精确在球体中的不可变量来测量机器学习模型的稳定性，并从学习理论的角度来研究抗攻击学习的可行性。我们首先考虑learner仅有随机样本的存在，并证明了需要分布方程的假设。接着，我们对受到点对点的分布的学习问题进行研究，并证明了对于log-Lipschitz分布，可以在 exponential 时间内对于攻击者的预算进行抗攻击学习。然后，我们研究learner具有更多权力的情况。我们首先考虑了本地会员询问，learner可以询问训练样本附近的标签。我们证明了，在 uniform 分布下，抗攻击学习 conjugation 和决策列在 exponential 时间内是不可避免的。然后，我们引入了本地相似询问 oracle，它可以返回训练样本附近的标签，以及在这个区域附近没有对应的 counterexample。我们证明了，如果询问半径等于攻击者的预算，则可以在分布自由设定下开发抗攻击 empirical risk minimization 算法。我们还给出了一般询问量上限和下限，以及具体的概念类别。”
</details></li>
</ul>
<hr>
<h2 id="Layer-wise-Feedback-Propagation"><a href="#Layer-wise-Feedback-Propagation" class="headerlink" title="Layer-wise Feedback Propagation"></a>Layer-wise Feedback Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12053">http://arxiv.org/abs/2308.12053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leander Weber, Jim Berend, Alexander Binder, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</li>
<li>for: 这paper是为了提出一种基于解释的神经网络训练方法，即层WISEFeedbackPropagation（LFP），用于评估神经网络模型中各个连接的贡献，并将其作为权重赋予各个连接。</li>
<li>methods: 这paper使用了层WISERelevancePropagation（LRP）来计算每个连接的权重，然后将这些权重分配给各个连接。这种方法不需要计算梯度，可以在模型的训练过程中分配权重。</li>
<li>results: 这paper提出了LFP的理论和实验研究，并证明了LFP可以在不同的模型和数据集上实现相同或更好的性能，而且可以超越传统的梯度下降法。此外，paper还 investigate了不同LRP规则的扩展和应用，如训练无意义梯度的神经网络模型，或者为转移学习而高效地利用现有知识。<details>
<summary>Abstract</summary>
In this paper, we present Layer-wise Feedback Propagation (LFP), a novel training approach for neural-network-like predictors that utilizes explainability, specifically Layer-wise Relevance Propagation(LRP), to assign rewards to individual connections based on their respective contributions to solving a given task. This differs from traditional gradient descent, which updates parameters towards anestimated loss minimum. LFP distributes a reward signal throughout the model without the need for gradient computations. It then strengthens structures that receive positive feedback while reducingthe influence of structures that receive negative feedback. We establish the convergence of LFP theoretically and empirically, and demonstrate its effectiveness in achieving comparable performance to gradient descent on various models and datasets. Notably, LFP overcomes certain limitations associated with gradient-based methods, such as reliance on meaningful derivatives. We further investigate how the different LRP-rules can be extended to LFP, what their effects are on training, as well as potential applications, such as training models with no meaningful derivatives, e.g., step-function activated Spiking Neural Networks (SNNs), or for transfer learning, to efficiently utilize existing knowledge.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出层wise Feedback Propagation（LFP），一种基于解释性的训练方法，使用层wise Relevance Propagation（LRP）来为各个连接分配奖励，以便解决给定任务。这与传统的梯度下降不同，梯度下降更新参数向估计损失最小点。LFP在模型中分配奖励信号，不需要梯度计算。它会强化接收正面奖励的结构，同时减弱接收负面奖励的结构。我们证明LFP的定理和实验均可以达到预期的性能，并在不同的模型和数据集上证明其效果。另外，LFP可以超越一些相关的梯度基于方法的限制，例如依赖于有意义的导数。我们还 investigate了不同的LRP规则如何扩展到LFP，以及它们在训练中的效果和应用，例如训练无意义导数的模型，如步函数激活的神经网络（SNN），或者用于过渡学习，以高效地利用现有的知识。
</details></li>
</ul>
<hr>
<h2 id="A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks"><a href="#A-multiobjective-continuation-method-to-compute-the-regularization-path-of-deep-neural-networks" class="headerlink" title="A multiobjective continuation method to compute the regularization path of deep neural networks"></a>A multiobjective continuation method to compute the regularization path of deep neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12044">http://arxiv.org/abs/2308.12044</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aamakor/continuation-method">https://github.com/aamakor/continuation-method</a></li>
<li>paper_authors: Augustina C. Amakor, Konstantin Sonntag, Sebastian Peitz</li>
<li>for: 提高深度神经网络（DNNs）的数值效率、模型解释性和Robustness。</li>
<li>methods: 基于线性模型的机器学习方法，扩展了规则化路径的概念到DNNs，并通过处理经验损失和稀疏度（$\ell^1$ norm）为两个矛盾目标解决multiobjective optimization问题。</li>
<li>results: 提出了一种高效地近似Pareto前面的算法，并通过deterministic和随机梯度示例 validate了该算法的效果。此外，还证明了知道规则化路径可以为网络参数化提供好的泛化能力。<details>
<summary>Abstract</summary>
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto front for the above-mentioned objectives in a very efficient manner. We present numerical examples using both deterministic and stochastic gradients. We furthermore demonstrate that knowledge of the regularization path allows for a well-generalizing network parametrization.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）中的稀疏性是一个非常需要的特性，因为它保证了数学效率、提高模型解释性（由于更少的相关特征），并且提高了模型的稳定性。在基于线性模型的机器学习方法中，已经知道存在一个连接路径 между最稀疏的解决方案（按照$\ell^1$范数）和不Regularized解决方案，这个路径被称为Regularization路径。然而，在DNN中扩展Regularization路径的概念是非常困难，因为Empirical损失和稀疏性（$\ell^1$范数）是两个矛盾的目标。为了解决这个问题，我们提出了一种算法，可以高效地approximate整个Pareto前列。我们通过Deterministic和Stochastic梯度进行数值示例，并证明了知道Regularization路径可以获得一个很好地泛化网络参数化。
</details></li>
</ul>
<hr>
<h2 id="IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning"><a href="#IncreLoRA-Incremental-Parameter-Allocation-Method-for-Parameter-Efficient-Fine-tuning" class="headerlink" title="IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning"></a>IncreLoRA: Incremental Parameter Allocation Method for Parameter-Efficient Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12043">http://arxiv.org/abs/2308.12043</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/feiyuzhang98/increlora">https://github.com/feiyuzhang98/increlora</a></li>
<li>paper_authors: Feiyu Zhang, Liangzhi Li, Junhao Chen, Zhouqiang Jiang, Bowen Wang, Yiming Qian</li>
<li>for: 提高预训练语言模型（PLM）的 parameter efficiency，尤其是在多个下游任务时。</li>
<li>methods: 使用 Low-Rank Adaptation（LoRA）方法，并在每个目标模块中添加可学习的约数分解矩阵。</li>
<li>results: 在 GLUE 上进行了广泛的实验，显示我们的方法在低资源设置下表现更优，尤其是在 parameter efficiency 方面。<details>
<summary>Abstract</summary>
With the increasing size of pre-trained language models (PLMs), fine-tuning all the parameters in the model is not efficient, especially when there are a large number of downstream tasks, which incur significant training and storage costs. Many parameter-efficient fine-tuning (PEFT) approaches have been proposed, among which, Low-Rank Adaptation (LoRA) is a representative approach that injects trainable rank decomposition matrices into every target module. Yet LoRA ignores the importance of parameters in different modules. To address this problem, many works have been proposed to prune the parameters of LoRA. However, under limited training conditions, the upper bound of the rank of the pruned parameter matrix is still affected by the preset values. We, therefore, propose IncreLoRA, an incremental parameter allocation method that adaptively adds trainable parameters during training based on the importance scores of each module. This approach is different from the pruning method as it is not limited by the initial number of training parameters, and each parameter matrix has a higher rank upper bound for the same training overhead. We conduct extensive experiments on GLUE to demonstrate the effectiveness of IncreLoRA. The results show that our method owns higher parameter efficiency, especially when under the low-resource settings where our method significantly outperforms the baselines. Our code is publicly available.
</details>
<details>
<summary>摘要</summary>
随着大型预训言语模型（PLM）的增加，精细调整模型中的所有参数不是高效的，特别是当有多个下游任务时，带来了训练和存储成本的增加。许多参数高效调整（PEFT）方法已经被提出，其中，低级别适应（LoRA）是一种代表性的方法，将适应矩阵注入到每个目标模块中。然而，LoRA忽略了模块中参数的重要性。为解决这个问题，许多工作已经被提出来剪裁LoRA中的参数。然而，在有限的训练条件下，剪裁后参数矩阵的最大级别仍然受到先前设置的值的限制。我们因此提出了IncreLoRA，一种逐步分配参数的方法，在训练过程中根据模块的重要性分配参数。这种方法与剪裁方法不同，不受初始训练参数的限制，每个参数矩阵的最大级别Upper bound也比剪裁方法高。我们在GLUE上进行了广泛的实验， demonstarted the effectiveness of IncreLoRA。结果显示，我们的方法在参数效率方面高于baseline，特别是在资源受限的情况下，我们的方法显著超过baseline。我们的代码公开可用。
</details></li>
</ul>
<hr>
<h2 id="CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures"><a href="#CACTUS-a-Comprehensive-Abstraction-and-Classification-Tool-for-Uncovering-Structures" class="headerlink" title="CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures"></a>CACTUS: a Comprehensive Abstraction and Classification Tool for Uncovering Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12031">http://arxiv.org/abs/2308.12031</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luca Gherardini, Varun Ravi Varma, Karol Capala, Roger Woods, Jose Sousa</li>
<li>For: The paper is written for improving secure analytics using explainable artificial intelligence, specifically addressing the challenges of developing solutions with small data sets.* Methods: The paper presents a new tool called CACTUS, which employs explainable AI techniques to provide additional support for categorical attributes, optimize memory usage, and speed up computation through parallelization.* Results: The paper evaluates the performance of CACTUS on two data sets, Wisconsin diagnostic breast cancer and Thyroid0387, and shows that it can effectively rank attributes by their discriminative power and provide accurate classification results.Here’s the same information in Simplified Chinese text:* 为：本文是为了提高安全分析，使用可解释人工智能，特别是面临小数据集的挑战。* 方法：本文提出了一种新的工具called CACTUS，该工具使用可解释AI技术，为分类特征提供更多的支持，保持分类特征的原始含义，提高内存使用率，并通过并行化加速计算。* 结果：本文使用CACTUS工具对 Wisconcin诊断乳腺癌和 Thyroid0387 数据集进行评估，并显示了它可以准确地排序特征，提供高效的分类结果。<details>
<summary>Abstract</summary>
The availability of large data sets is providing an impetus for driving current artificial intelligent developments. There are, however, challenges for developing solutions with small data sets due to practical and cost-effective deployment and the opacity of deep learning models. The Comprehensive Abstraction and Classification Tool for Uncovering Structures called CACTUS is presented for improved secure analytics by effectively employing explainable artificial intelligence. It provides additional support for categorical attributes, preserving their original meaning, optimising memory usage, and speeding up the computation through parallelisation. It shows to the user the frequency of the attributes in each class and ranks them by their discriminative power. Its performance is assessed by application to the Wisconsin diagnostic breast cancer and Thyroid0387 data sets.
</details>
<details>
<summary>摘要</summary>
大量数据的可用性对当前人工智能发展提供了动力。然而，对小数据集的解决方案存在实际和成本效益的挑战，主要是深度学习模型的透明度问题。本文提出了一种名为CACTUS的全面抽象和分类工具，用于提高安全分析。它可以有效地使用可解释人工智能，并且对分类属性进行了更好的支持，保持原始含义，优化内存使用和并行计算，以提高计算速度。它还可以为用户显示每个类别的属性频率，并将其排序为权重。本文通过应用于美国威斯康星诊断乳腺癌和 thyroid0387 数据集来评估其性能。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning"><a href="#Prompt-Based-Length-Controlled-Generation-with-Reinforcement-Learning" class="headerlink" title="Prompt-Based Length Controlled Generation with Reinforcement Learning"></a>Prompt-Based Length Controlled Generation with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12030">http://arxiv.org/abs/2308.12030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renlong Jie, Xiaojun Meng, Lifeng Shang, Xin Jiang, Qun Liu</li>
<li>for: 这个论文旨在提出一种基于提示的Length Control方法，以实现控制长度的GPT-style语言模型生成。</li>
<li>methods: 该方法使用了 reward学习，通过给出trainable或规则型的奖励模型，对GPT-style语言模型的生成进行影响，以实现目标长度的控制。</li>
<li>results: 实验显示，该方法可以有效地提高CNNDM和NYT等 популяр的数据集上的提示基于长度控制精度。我们认为这种可控长度的能力可以为LLMs的未来带来更多的潜力。<details>
<summary>Abstract</summary>
Recently, large language models (LLMs) like ChatGPT and GPT-4 have attracted great attention given their surprising improvement and performance. Length controlled generation of LLMs emerges as an important topic, which also enables users to fully leverage the capability of LLMs in more real-world scenarios like generating a proper answer or essay of a desired length. In addition, the autoregressive generation in LLMs is extremely time-consuming, while the ability of controlling this generated length can arbitrarily reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details>
<details>
<summary>摘要</summary>
近期，大型语言模型（LLM）如ChatGPT和GPT-4吸引了很大的注意，因其奇妙的进步和表现。控制LLM的Length emerges as an important topic，这也使得用户可以充分利用LLM的能力在更多的实际应用 scenario 中，如生成适当的答案或论文的 desired length。此外，LLM中的autoregressive generation extremely time-consuming，而控制这些生成的Length可以Randomly reduce the inference cost by limiting the length, and thus satisfy different needs. Therefore, we aim to propose a prompt-based length control method to achieve this length controlled generation, which can also be widely applied in GPT-style LLMs. In particular, we adopt reinforcement learning with the reward signal given by either trainable or rule-based reward model, which further affects the generation of LLMs via rewarding a pre-defined target length. Experiments show that our method significantly improves the accuracy of prompt-based length control for summarization task on popular datasets like CNNDM and NYT. We believe this length-controllable ability can provide more potentials towards the era of LLMs.
</details></li>
</ul>
<hr>
<h2 id="A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning"><a href="#A-Scale-Invariant-Task-Balancing-Approach-for-Multi-Task-Learning" class="headerlink" title="A Scale-Invariant Task Balancing Approach for Multi-Task Learning"></a>A Scale-Invariant Task Balancing Approach for Multi-Task Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12029">http://arxiv.org/abs/2308.12029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baijiong Lin, Weisen Jiang, Feiyang Ye, Yu Zhang, Pengguang Chen, Ying-Cong Chen, Shu Liu</li>
<li>for: This paper is written for people who are interested in multi-task learning (MTL) and want to learn about a new method called Scale-Invariant Multi-Task Learning (SI-MTL) that can alleviate the task-balancing problem.</li>
<li>methods: The paper proposes two methods to address the task-balancing problem in MTL: a logarithm transformation on all task losses to ensure scale-invariance at the loss level, and a gradient balancing method called SI-G that normalizes all task gradients to the same magnitude as the maximum gradient norm.</li>
<li>results: The paper reports extensive experimental results on several benchmark datasets, which consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.<details>
<summary>Abstract</summary>
Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL），一种同时学习多个相关任务的学习方法，在不同领域都取得了很大成功。然而，任务均衡仍然是MTL中的主要挑战，由于任务损失/梯度的比例不同，经常会导致性能下降。在这篇论文中，我们提出了具有权重归一化和梯度归一化的缩减多任务学习方法（SI-MTL），以解决任务均衡问题从损失和梯度两个角度。具体来说，SI-MTL包括一种对所有任务损失进行对数变换，以保证损失水平上的归一化，以及一种梯度归一化方法SI-G，用于 норма化所有任务梯度，使其具有最大梯度 нор 的同样大小。我们在多个标准数据集上进行了广泛的实验，并 consistently demonstrates了SI-G的有效性和SI-MTL的状态的杰出性。
</details></li>
</ul>
<hr>
<h2 id="Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD"><a href="#Bias-Aware-Minimisation-Understanding-and-Mitigating-Estimator-Bias-in-Private-SGD" class="headerlink" title="Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD"></a>Bias-Aware Minimisation: Understanding and Mitigating Estimator Bias in Private SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12018">http://arxiv.org/abs/2308.12018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Knolle, Robert Dorfman, Alexander Ziller, Daniel Rueckert, Georgios Kaissis</li>
<li>for: 提高 differentially private SGD 的模型 Utility 和 Privacy 的融合</li>
<li>methods: 利用 per-sample gradient norms 和 private gradient oracle 的关系来减少 private gradient estimator bias</li>
<li>results: 在 CIFAR-10, CIFAR-100, 和 ImageNet-32  datasets 上提供了 empirical evidence, 显示 Bias-Aware Minimisation 不仅减少了 bias, 还有substantially improved privacy-utility trade-offs.<details>
<summary>Abstract</summary>
Differentially private SGD (DP-SGD) holds the promise of enabling the safe and responsible application of machine learning to sensitive datasets. However, DP-SGD only provides a biased, noisy estimate of a mini-batch gradient. This renders optimisation steps less effective and limits model utility as a result. With this work, we show a connection between per-sample gradient norms and the estimation bias of the private gradient oracle used in DP-SGD. Here, we propose Bias-Aware Minimisation (BAM) that allows for the provable reduction of private gradient estimator bias. We show how to efficiently compute quantities needed for BAM to scale to large neural networks and highlight similarities to closely related methods such as Sharpness-Aware Minimisation. Finally, we provide empirical evidence that BAM not only reduces bias but also substantially improves privacy-utility trade-offs on the CIFAR-10, CIFAR-100, and ImageNet-32 datasets.
</details>
<details>
<summary>摘要</summary>
differentially private SGD (DP-SGD) 可以使机器学习应用于敏感数据集而不需担心隐私泄露。然而，DP-SGD只提供偏差、噪音的小批量梯度估计。这会导致优化步骤效果减退，模型实用性受限。在这项工作中，我们显示了每个样本梯度norm和私有梯度 Oracle 的估计偏差之间的连接。我们提出了偏差意识的最小化（BAM），允许降低私有梯度估计偏差。我们证明了如何有效地计算BAM所需的量，并将其扩展到大型神经网络。最后，我们提供了实验证明BAM不仅减少偏差，还substantially改善了隐私-实用性质量比在CIFAR-10、CIFAR-100和ImageNet-32数据集上。
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Stochastic-Differential-Equations"><a href="#Graph-Neural-Stochastic-Differential-Equations" class="headerlink" title="Graph Neural Stochastic Differential Equations"></a>Graph Neural Stochastic Differential Equations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12316">http://arxiv.org/abs/2308.12316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Bergna, Felix Opolka, Pietro Liò, Jose Miguel Hernandez-Lobato</li>
<li>for: 这个论文旨在提出一种新的模型Graph Neural Stochastic Differential Equations (Graph Neural SDEs)，用于评估预测不确定性。</li>
<li>methods: 该模型基于Brownian Motion嵌入随机性，从而提高了现有模型缺失的预测不确定性评估。</li>
<li>results: 经验研究表明，Latent Graph Neural SDEs可以超过常见模型 like Graph Convolutional Networks和Graph Neural ODEs，特别是在信任预测中，能够更好地处理out-of-distribution检测。<details>
<summary>Abstract</summary>
We present a novel model Graph Neural Stochastic Differential Equations (Graph Neural SDEs). This technique enhances the Graph Neural Ordinary Differential Equations (Graph Neural ODEs) by embedding randomness into data representation using Brownian motion. This inclusion allows for the assessment of prediction uncertainty, a crucial aspect frequently missed in current models. In our framework, we spotlight the \textit{Latent Graph Neural SDE} variant, demonstrating its effectiveness. Through empirical studies, we find that Latent Graph Neural SDEs surpass conventional models like Graph Convolutional Networks and Graph Neural ODEs, especially in confidence prediction, making them superior in handling out-of-distribution detection across both static and spatio-temporal contexts.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的模型——图 neural 随机 дифференциаль方程（图 neural SDE）。这种技术在图 neural ordinary differential equation（图 neural ODE）中嵌入随机性，通过游戏摆动来表示数据的不确定性。这种包含允许我们评估预测不确定性，这是当前模型中常常缺失的一个重要方面。在我们的框架中，我们强调了《隐藏图 neural SDE》的变体，并证明其效果。通过实验研究，我们发现隐藏图 neural SDE 在信任预测方面表现出色，特别是在对于静态和空间时间上的 OUT-OF-DISTRIBUTION 检测中，与传统模型如图 convolutional networks 和图 neural ODEs 相比，它们更为稳定和可靠。
</details></li>
</ul>
<hr>
<h2 id="MKL-L-0-1-SVM"><a href="#MKL-L-0-1-SVM" class="headerlink" title="MKL-$L_{0&#x2F;1}$-SVM"></a>MKL-$L_{0&#x2F;1}$-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12016">http://arxiv.org/abs/2308.12016</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maxis1718/simplemkl">https://github.com/maxis1718/simplemkl</a></li>
<li>paper_authors: Bin Zhu, Yijie Shi</li>
<li>for: 这篇论文提出了一个基于多kernel学习（MKL）的支持向量机（SVM）模型，使用$(0, 1)$损失函数。</li>
<li>methods: 论文提供了一些首选条件，然后利用这些条件来开发一个快速的ADMM解决方案来处理非 convex 和非 гладhloss 优化问题。</li>
<li>results: 数据库实验表明，我们的MKL-$L_{0&#x2F;1}$-SVM表现与 SimpleMKL 比较相似，SimpleMKL 是 Rakotomamonjy 等人在 Journal of Machine Learning Research 上发表的一篇论文 [vol. 9, pp. 2491-2521, 2008] 。<details>
<summary>Abstract</summary>
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种基于多kernel学习（简称MKL）的支持向量机器（SVM）$(0, 1)$损失函数的框架。文中给出了一些首要的优化条件，然后利用这些条件来开发一个快速的ADMM算法来解决非对称和不连续的优化问题。实验表明，我们的MKL-$L_{0/1}$-SVM的性能与2008年 Rakotomamonjy等在Journal of Machine Learning Research上发表的SimpleMKL方法相当。Here's the breakdown of the translation:* 这篇论文 (zhè běn zhōng zhì) - This paper* 提出了 (tī qù le) - proposes* 一种 (yī zhǒng) - a kind of* 基于多kernel学习 (jī yù duō jiān xué xí) - based on multiple kernel learning* SVM $(0, 1)$ 损失函数 (SVM $0, 1$ loss function) - SVM with the $(0, 1)$ loss function* 框架 (kōng zhì) - framework* 给出了 (gěi dòu le) - gives* 一些 (yī xiē) - some* 首要的 (shǒu yào de) - primary* 优化条件 (yòu yòu tiáo yì) - optimization conditions* 然后 (rán hái) - then* 利用 (lǐ yòng) - use* 这些条件 (zhè xiē tiáo yì) - these conditions* 开发 (kāi fā) - develop* 一个 (yī ge) - a* 快速的 (kuài sù de) - fast* ADMM算法 (ADMM suān gòu) - ADMM algorithm* 来解决 (laī jiě jué) - to solve* 非对称和不连续的 (fēi duì xiǎng yǔ bù lián zhí de) - nonconvex and nonsmooth* 优化问题 (yòu yòu wèn tí) - optimization problem* 实验 (shí yàn) - experiments* 表明 (biǎo mǐng) - show* 性能 (xìng néng) - performance* 与 (yǔ) - and* 2008年 Rakotomamonjy等 (2008 nián Rakotomamonjy déng) - Rakotomamonjy et al. in 2008* 在 (zài) - in* Journal of Machine Learning Research (Journal of Machine Learning Research)* 发表 (fā bèi) - published*  SimpleMKL (SimpleMKL) - SimpleMKL* 性能 (xìng néng) - performance* 相当 (xiāng dàng) - comparableI hope this helps!
</details></li>
</ul>
<hr>
<h2 id="Quantum-Noise-driven-Generative-Diffusion-Models"><a href="#Quantum-Noise-driven-Generative-Diffusion-Models" class="headerlink" title="Quantum-Noise-driven Generative Diffusion Models"></a>Quantum-Noise-driven Generative Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12013">http://arxiv.org/abs/2308.12013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Parigi, Stefano Martina, Filippo Caruso</li>
<li>for: 这个论文的目的是提出和讨论量子扩散模型的量子扩散模型，用于生成复杂的数据分布。</li>
<li>methods: 这个论文使用了机器学习技术实现的生成模型，特别是使用量子随机过程来驱动扩散模型，以生成新的 sintetic 数据。</li>
<li>results: 这个论文预计可以通过利用量子随机过程的特点，例如干扰、Entanglement和噪声的非常规交互，超越传统的扩散模型在推断中的主要计算压力，从而实现更高效的数据生成和预测。<details>
<summary>Abstract</summary>
Generative models realized with machine learning techniques are powerful tools to infer complex and unknown data distributions from a finite number of training samples in order to produce new synthetic data. Diffusion models are an emerging framework that have recently overcome the performance of the generative adversarial networks in creating synthetic text and high-quality images. Here, we propose and discuss the quantum generalization of diffusion models, i.e., three quantum-noise-driven generative diffusion models that could be experimentally tested on real quantum systems. The idea is to harness unique quantum features, in particular the non-trivial interplay among coherence, entanglement and noise that the currently available noisy quantum processors do unavoidably suffer from, in order to overcome the main computational burdens of classical diffusion models during inference. Hence, we suggest to exploit quantum noise not as an issue to be detected and solved but instead as a very remarkably beneficial key ingredient to generate much more complex probability distributions that would be difficult or even impossible to express classically, and from which a quantum processor might sample more efficiently than a classical one. Therefore, our results are expected to pave the way for new quantum-inspired or quantum-based generative diffusion algorithms addressing more powerfully classical tasks as data generation/prediction with widespread real-world applications ranging from climate forecasting to neuroscience, from traffic flow analysis to financial forecasting.
</details>
<details>
<summary>摘要</summary>
生成模型利用机器学习技术可以从 finite 数量的训练样本中推理出复杂和未知的数据分布，以生成新的 sintetic 数据。扩散模型是一种出现在的框架，最近已经超越了生成对抗网络在生成 sintetic 文本和高质量图像方面的性能。在这里，我们提出了量子扩散模型的量子扩散模型，可以在实际量子系统上进行实验。我们想利用量子特有的非rium特性，即减 coherence、Entanglement和噪声之间的非rivial交互，来超越类型 diffusion 模型的主要计算卷积。因此，我们建议利用量子噪声，不是探测和解决的问题，而是作为一个非常有利的元素，以生成更复杂的概率分布，这些分布可能是类型 diffusion 模型无法表达的，而且从量子处理器中采样可能更高效于类型处理器。因此，我们的结果预计将开拓出新的量子激发或量子基于的扩散算法，用于更有力的数据生成/预测，它们在广泛的实际应用中将扮演重要的角色，包括气候预测、神经科学、交通流量分析和金融预测等。
</details></li>
</ul>
<hr>
<h2 id="Neural-oscillators-for-magnetic-hysteresis-modeling"><a href="#Neural-oscillators-for-magnetic-hysteresis-modeling" class="headerlink" title="Neural oscillators for magnetic hysteresis modeling"></a>Neural oscillators for magnetic hysteresis modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12002">http://arxiv.org/abs/2308.12002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Chandra, Taniya Kapoor, Bram Daniels, Mitrofan Curti, Koen Tiels, Daniel M. Tartakovsky, Elena A. Lomonova</li>
<li>for: 模型和诊断普遍科学和工程中的普遍现象-干扰。</li>
<li>methods: 使用 ordinary differential equation-based recurrent neural network (RNN) 方法来模型和诊断干扰。</li>
<li>results: HystRNN 能够在未经训练的区域中适应并预测复杂的干扰模式，这是传统 RNN 方法所无法做到的。<details>
<summary>Abstract</summary>
Hysteresis is a ubiquitous phenomenon in science and engineering; its modeling and identification are crucial for understanding and optimizing the behavior of various systems. We develop an ordinary differential equation-based recurrent neural network (RNN) approach to model and quantify the hysteresis, which manifests itself in sequentiality and history-dependence. Our neural oscillator, HystRNN, draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states. The performance of HystRNN is evaluated to predict generalized scenarios, involving first-order reversal curves and minor loops. The findings show the ability of HystRNN to generalize its behavior to previously untrained regions, an essential feature that hysteresis models must have. This research highlights the advantage of neural oscillators over the traditional RNN-based methods in capturing complex hysteresis patterns in magnetic materials, where traditional rate-dependent methods are inadequate to capture intrinsic nonlinearity.
</details>
<details>
<summary>摘要</summary>
《干支度量学习：一种基于偏微分方程的循环神经网络方法》Introduction:干支度量是科学和工程中的一种普遍现象，其模型化和识别是理解和优化系统的行为的关键。在本文中，我们提出了基于偏微分方程的循环神经网络方法（HystRNN），以模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.Methodology:我们的HystRNN方法基于Ordinary Differential Equation (ODE)，它可以模型干支度量的循环和历史依赖性。我们通过将循环神经网络中的每个节点更新为一个偏微分方程，来实现模型的循环和历史依赖性。这种方法可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。Results:我们通过测试HystRNN的性能，发现它可以在未经训练的区域中预测折返曲线和小循环。这表明HystRNN具有普适性，是一种可以在不同的干支度量情况下预测行为的模型。此外，我们还发现HystRNN的性能比传统的RNN-based方法更好，这表明循环神经网络可以更好地捕捉干支度量的复杂特征。Conclusion:本文提出了一种基于偏微分方程的循环神经网络方法（HystRNN），用于模型和量化干支度量。HystRNN draws inspiration from coupled-oscillatory RNN and phenomenological hysteresis models to update the hidden states.我们的实验表明，HystRNN具有普适性和更好的预测性，可以在不同的干支度量情况下预测行为。这些结果表明循环神经网络可以更好地捕捉干支度量的复杂特征，比如折返曲线和小循环。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Representation-Learning-Across-Domains"><a href="#Trustworthy-Representation-Learning-Across-Domains" class="headerlink" title="Trustworthy Representation Learning Across Domains"></a>Trustworthy Representation Learning Across Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ronghang Zhu, Dongliang Guo, Daiqing Qi, Zhixuan Chu, Xiang Yu, Sheng Li</li>
<li>for: 本研究旨在提供一个可靠的表征学习框架，以满足实际应用中的跨领域应用需求。</li>
<li>methods: 本研究使用了四个基本概念， namely 类别、隐私、公平和解释性，来建立一个包含多种方法的可靠表征学习框架。</li>
<li>results: 本研究提出了一个全面的文献综述，涵盖了从四个基本概念中的多种方法，以及它们在实际应用中的应用和发展前景。<details>
<summary>Abstract</summary>
As AI systems have obtained significant performance to be deployed widely in our daily live and human society, people both enjoy the benefits brought by these technologies and suffer many social issues induced by these systems. To make AI systems good enough and trustworthy, plenty of researches have been done to build guidelines for trustworthy AI systems. Machine learning is one of the most important parts for AI systems and representation learning is the fundamental technology in machine learning. How to make the representation learning trustworthy in real-world application, e.g., cross domain scenarios, is very valuable and necessary for both machine learning and AI system fields. Inspired by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework which includes four concepts, i.e, robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details>
<details>
<summary>摘要</summary>
现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不仅享受了这些技术的好处，也面临着由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.Here's the translation in Traditional Chinese:现在人工智能系统在我们日常生活和人类社会中广泛应用，人们不单享受了这些技术的好处，也面临由这些系统引起的许多社会问题。为了让人工智能系统足够可靠和可信，很多研究者在建立可靠人工智能系统的指南方面做出了很多努力。机器学习是人工智能系统中最重要的一部分，表示学习是机器学习的核心技术。为了让表示学习在实际应用中是可靠的，例如跨领域场景，是非常有价值和必需的。 inspirited by the concepts in trustworthy AI, we proposed the first trustworthy representation learning across domains framework, which includes four concepts, i.e., robustness, privacy, fairness, and explainability, to give a comprehensive literature review on this research direction. Specifically, we first introduce the details of the proposed trustworthy framework for representation learning across domains. Second, we provide basic notions and comprehensively summarize existing methods for the trustworthy framework from four concepts. Finally, we conclude this survey with insights and discussions on future research directions.
</details></li>
</ul>
<hr>
<h2 id="On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget"><a href="#On-Uniformly-Optimal-Algorithms-for-Best-Arm-Identification-in-Two-Armed-Bandits-with-Fixed-Budget" class="headerlink" title="On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget"></a>On Uniformly Optimal Algorithms for Best Arm Identification in Two-Armed Bandits with Fixed Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12000">http://arxiv.org/abs/2308.12000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Po-An Wang, Kaito Ariu, Alexandre Proutiere</li>
<li>For: 本研究考虑了固定预算下最佳臂标识问题，具体是随机两臂投注机制下的 Bernoulli 奖励。* Methods: 我们使用了一种自然的一臂投注算法，即所谓的{\it 均匀投注}算法，并证明了这种算法在所有情况下都是最佳的。此外，我们还引入了一种名为{\it 一臂投注}的自然的算法类，并证明了任何能够与{\it 均匀投注}算法相当的算法都必须属于这种类。* Results: 我们证明了，无论是在所有情况下还是在某些特定情况下，都无法找到一种能够超过{\it 均匀投注}算法的算法。具体来说，我们证明了任何能够与{\it 均匀投注}算法相当的算法都必须是一种{\it 一臂投注}算法。这个结论解决了在\cite{qin2022open}中提出的两个开放问题。<details>
<summary>Abstract</summary>
We study the problem of best-arm identification with fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the {\it uniform sampling} algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm. Towards this result, we introduce the natural class of {\it consistent} and {\it stable} algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details>
<details>
<summary>摘要</summary>
我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.Here's the translation in Traditional Chinese:我们研究了固定预算下最佳臂标识问题， Specifically, we study the problem of best-arm identification with a fixed budget in stochastic two-arm bandits with Bernoulli rewards. We prove that surprisingly, there is no algorithm that (i) performs as well as the algorithm sampling each arm equally (this algorithm is referred to as the uniform sampling algorithm) on all instances, and that (ii) strictly outperforms this algorithm on at least one instance. In short, there is no algorithm better than the uniform sampling algorithm.To achieve this result, we introduce the natural class of consistent and stable algorithms, and show that any algorithm that performs as well as the uniform sampling algorithm on all instances belongs to this class. The proof is completed by deriving a lower bound on the error rate satisfied by any consistent and stable algorithm, and by showing that the uniform sampling algorithm matches this lower bound. Our results provide a solution to the two open problems presented in \cite{qin2022open}.
</details></li>
</ul>
<hr>
<h2 id="Relational-Concept-Based-Models"><a href="#Relational-Concept-Based-Models" class="headerlink" title="Relational Concept Based Models"></a>Relational Concept Based Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11991">http://arxiv.org/abs/2308.11991</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aghoreshwar/Awesome-Customer-Analytics">https://github.com/Aghoreshwar/Awesome-Customer-Analytics</a></li>
<li>paper_authors: Pietro Barbiero, Francesco Giannini, Gabriele Ciravegna, Michelangelo Diligenti, Giuseppe Marra</li>
<li>for: 本研究的目的是解释深度学习模型在关系领域中的工作方式，以便提高模型的可解释性和可信度。</li>
<li>methods: 本研究提出了一种新的关系深度学习模型，即关系概念基模型（Relational Concept-Based Models，RCBMs），该模型结合了深度学习和概念分析技术，以提高模型的可解释性和可信度。</li>
<li>results: 实验结果表明，关系CBMs可以与现有的关系黑盒模型（relational black-box models）匹配的普适性和可解释性，同时支持生成量化的概念基模型解释。此外，关系CBMs还可以在各种具有挑战性的情况下表现出色，如out-of-distribution场景、有限的训练数据场景和罕见概念指导场景。<details>
<summary>Abstract</summary>
The design of interpretable deep learning models working in relational domains poses an open challenge: interpretable deep learning methods, such as Concept-Based Models (CBMs), are not designed to solve relational problems, while relational models are not as interpretable as CBMs. To address this problem, we propose Relational Concept-Based Models, a family of relational deep learning methods providing interpretable task predictions. Our experiments, ranging from image classification to link prediction in knowledge graphs, show that relational CBMs (i) match generalization performance of existing relational black-boxes (as opposed to non-relational CBMs), (ii) support the generation of quantified concept-based explanations, (iii) effectively respond to test-time interventions, and (iv) withstand demanding settings including out-of-distribution scenarios, limited training data regimes, and scarce concept supervisions.
</details>
<details>
<summary>摘要</summary>
文本： Deep learning 模型在关系领域的设计呈现出一个开放的挑战：可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。翻译结果：文本：深度学习模型在关系领域的设计存在一个开放的挑战，可读性深度学习方法，如概念基本模型（CBMs），不是设计来解决关系问题，而关系模型则不如可读性深度学习方法。为解决这个问题，我们提出了关系概念基本模型（Relational CBMs），这是一种可读性深度学习方法，可以在关系任务上提供可读性的任务预测。我们的实验，从图像分类到知识图表链接预测，表明了关系 CBMs 具有以下特点：(i) 与现有关系黑盒相比，可以达到相同的泛化性能; (ii) 可以生成量化的概念基本解释; (iii) 在测试时间干扰中能够有效回应; (iv) 在具有异常分布、有限训练数据和罕见概念监督的情况下，也能够坚持。
</details></li>
</ul>
<hr>
<h2 id="Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks"><a href="#Will-More-Expressive-Graph-Neural-Networks-do-Better-on-Generative-Tasks" class="headerlink" title="Will More Expressive Graph Neural Networks do Better on Generative Tasks?"></a>Will More Expressive Graph Neural Networks do Better on Generative Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11978">http://arxiv.org/abs/2308.11978</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiandong Zou, Xiangyu Zhao, Pietro Liò, Yiren Zhao</li>
<li>For: The paper is focused on the task of graph generation, specifically in the context of molecular graph generation for de-novo drug and molecular design.* Methods: The paper investigates the expressiveness of different Graph Neural Network (GNN) architectures in two popular generative frameworks (GCPN and GraphAF) on six different molecular generative objectives using the ZINC-250k dataset.* Results: The paper demonstrates that advanced GNNs can improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Additionally, the paper shows that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results compared to 17 other non-GNN-based graph generative approaches on important metrics such as DRD2, Median1, and Median2.Here is the information in Simplified Chinese text:* 用途：研究 graph generation 任务，特别是在分子图生成中，以满足 de-novo 药物和分子设计。* 方法： investigate  Graph Neural Network (GNN)  Architecture 在 GCPN 和 GraphAF 中，并在 ZINC-250k 数据集上进行 six 个分子生成目标的测试。* 结果：显示 advanced GNNs 可以提高 GCPN 和 GraphAF 在分子生成任务中的性能，但 GNN 表达能力不是必需的条件。此外， paper 还证明 GCPN 和 GraphAF 可以使用 advanced GNNs 在 DRD2、Median1 和 Median2 等重要指标上达到 state-of-the-art 结果，并且比 17 种非 GNN-based graph generative approach 更好。<details>
<summary>Abstract</summary>
Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZINC-250k dataset. Through our extensive experiments, we demonstrate that advanced GNNs can indeed improve the performance of GCPN and GraphAF on molecular generation tasks, but GNN expressiveness is not a necessary condition for a good GNN-based generative model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve state-of-the-art results across 17 other non-GNN-based graph generative approaches, such as variational autoencoders and Bayesian optimisation models, on the proposed molecular generative objectives (DRD2, Median1, Median2), which are important metrics for de-novo molecular design.
</details>
<details>
<summary>摘要</summary>
Graph生成具有重要挑战，因为它需要预测具有多个节点和边的完整图基于单个标签。这个任务对实际应用中的许多应用，如新药和分子设计，具有基本重要性。在过去几年，有几种成功的方法在图生成领域出现。然而，这些方法受到两个主要缺点的影响：（1）被用的图神经网络（GNN）架构经常被忽视；和（2）这些方法通常只被评估在有限数量的指标上。为了填补这个空白，我们在图生成任务上investigate GNN的表达能力，并将GNN替换为更表达力强的GNN。 Specifically，我们分析了六种GNN在两个不同的生成框架（GCPN和GraphAF）中的表现，在ZINC-250k数据集上进行六种分子生成目标。通过我们的广泛实验，我们证明了高级GNN可以提高GCPN和GraphAF在分子生成任务中的表现，但GNN表达能力不是必要的condition for a good GNN-based generative model。此外，我们显示GCPN和GraphAF使用高级GNN可以在17种非GNN-based图生成方法（如变量自动编码器和抽象优化模型）上达到状态的最佳结果，在提案的分子生成目标（DRD2、Median1、Median2）上。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression"><a href="#Approximating-Score-based-Explanation-Techniques-Using-Conformal-Regression" class="headerlink" title="Approximating Score-based Explanation Techniques Using Conformal Regression"></a>Approximating Score-based Explanation Techniques Using Conformal Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11975">http://arxiv.org/abs/2308.11975</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amr Alkhatib, Henrik Boström, Sofiane Ennadir, Ulf Johansson</li>
<li>for: 本文旨在提出和研究一种 computationally less costly 的 regression model，用于approximating score-based explanation techniques，如 SHAP 的输出。</li>
<li>methods: 本文使用了 inductive conformal prediction 框架，提供了有效性保证，以及多种 non-conformity measures，用于考虑 approximations 的困难性，同时保持计算成本低。</li>
<li>results: 本文通过大规模的实验研究，发现提案的方法可以significantly improve execution time compared to fast SHAP version，TreeSHAP。 results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. In addition, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.<details>
<summary>Abstract</summary>
Score-based explainable machine-learning techniques are often used to understand the logic behind black-box models. However, such explanation techniques are often computationally expensive, which limits their application in time-critical contexts. Therefore, we propose and investigate the use of computationally less costly regression models for approximating the output of score-based explanation techniques, such as SHAP. Moreover, validity guarantees for the approximated values are provided by the employed inductive conformal prediction framework. We propose several non-conformity measures designed to take the difficulty of approximating the explanations into account while keeping the computational cost low. We present results from a large-scale empirical investigation, in which the approximate explanations generated by our proposed models are evaluated with respect to efficiency (interval size). The results indicate that the proposed method can significantly improve execution time compared to the fast version of SHAP, TreeSHAP. The results also suggest that the proposed method can produce tight intervals, while providing validity guarantees. Moreover, the proposed approach allows for comparing explanations of different approximation methods and selecting a method based on how informative (tight) are the predicted intervals.
</details>
<details>
<summary>摘要</summary>
黑盒模型的解释技术 often 使用 Score-based 的解释技术，但这些解释技术经常具有高计算成本，这限制了它们在时间紧张的上下文中的应用。因此，我们提出了和探索使用 computationally 较低成本的回归模型来近似黑盒模型的输出。此外，我们采用了 inductive  conformal prediction 框架提供了有效性保证。我们提出了一些非准确度度量，用于考虑近似解释的困难而保持计算成本低。我们在大规模的实验中提出了这些方法，并评估了这些方法的效率（间隔大小）。结果表明，我们的提议方法可以significantly 改善执行时间，相比于快速版本的 SHAP，TreeSHAP。结果还表明，我们的方法可以生成紧凑的间隔，同时提供有效性保证。此外，我们的方法允许比较不同的近似方法的解释，并选择一种基于解释是否具有紧凑的预测间隔的方法。
</details></li>
</ul>
<hr>
<h2 id="EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE"><a href="#EVE-Efficient-Vision-Language-Pre-training-with-Masked-Prediction-and-Modality-Aware-MoE" class="headerlink" title="EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE"></a>EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11971">http://arxiv.org/abs/2308.11971</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin, Dongyu Zhang<br>for:这篇论文旨在开探如何建立可扩展的视觉语言模型，以学习具有多 modal 数据的多模式资料。methods:论文提出了一个效率的视觉语言基础模型，名为EVE，它是一个统一的多模式Transformer预测器，具有适应器 Mixture-of-Experts（MoE）模组，可以选择性地转换到不同的专家。results:论文表明，EVE可以在训练时间和资源更少的情况下，实现更好的下游性能，并且在多种视觉语言下游任务上实现了州流的表现。<details>
<summary>Abstract</summary>
Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 3.5x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将视觉语言模型建立成可扩展的基础模型仍然是一个开放的挑战。在这篇论文中，我们介绍了一个高效的视觉语言基础模型，即EVE（高效视觉语言基础模型）。EVE是一个共享转换网络，其中视觉和语言都被编码在同一个网络中，并通过特性意识模块来捕捉不同类型的信息。这些模块可以选择性地切换到不同的专家，以捕捉不同类型的信息。为了统一视觉和语言预训练任务，EVE在图像和文本对中进行遮盲信号模型，即将图像像素和文本字符遮盲，然后使用可见信号来还原遮盲信号。这个简单而有效的预训练目标可以加速训练，比 tradicional Image-Text Contrastive和Image-Text Matching损失的3.5倍。由于EVE的整合的体系和预训练任务，它具有更好的下游性能，需要 fewer resources和更快的训练速度。尽管它的简单性，EVE仍然达到了视觉语言下游任务的州OF-THE-ART性能，包括视觉问答、视觉逻辑和图像-文本检索。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification"><a href="#Anisotropic-Hybrid-Networks-for-liver-tumor-segmentation-with-uncertainty-quantification" class="headerlink" title="Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification"></a>Anisotropic Hybrid Networks for liver tumor segmentation with uncertainty quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11969">http://arxiv.org/abs/2308.11969</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Lambert, Pauline Roca, Florence Forbes, Senan Doyle, Michel Dojat</li>
<li>for: 本研究旨在提出一种自动化的肝脏和肿瘤分 segmentation 方法，以帮助治疗抑制肝癌的医疗决策。</li>
<li>methods: 本研究使用了两种不同的管道，即多类分类模型和二元分类模型，以实现肝脏和肿瘤的同时分 segmentation。</li>
<li>results: 研究结果显示了两种管道具有不同的优劣点，并提出了一种不确定性评估策略，以识别可能存在的假阳性肿瘤患者。<details>
<summary>Abstract</summary>
The burden of liver tumors is important, ranking as the fourth leading cause of cancer mortality. In case of hepatocellular carcinoma (HCC), the delineation of liver and tumor on contrast-enhanced magnetic resonance imaging (CE-MRI) is performed to guide the treatment strategy. As this task is time-consuming, needs high expertise and could be subject to inter-observer variability there is a strong need for automatic tools. However, challenges arise from the lack of available training data, as well as the high variability in terms of image resolution and MRI sequence. In this work we propose to compare two different pipelines based on anisotropic models to obtain the segmentation of the liver and tumors. The first pipeline corresponds to a baseline multi-class model that performs the simultaneous segmentation of the liver and tumor classes. In the second approach, we train two distinct binary models, one segmenting the liver only and the other the tumors. Our results show that both pipelines exhibit different strengths and weaknesses. Moreover we propose an uncertainty quantification strategy allowing the identification of potential false positive tumor lesions. Both solutions were submitted to the MICCAI 2023 Atlas challenge regarding liver and tumor segmentation.
</details>
<details>
<summary>摘要</summary>
liver tumor 的负担是非常重要的， ranking as the fourth leading cause of cancer mortality。在肝细胞癌（HCC）的 случа子中，通过对增强磁共振成像（CE-MRI）进行描述，以便引导治疗策略。然而，由于这个任务需要较高的专业知识和较长的时间，并且可能会受到观察者间的差异，因此有强需求于自动工具。然而，由于数据不足以及图像分辨率和MRI序列的高变化性，这些任务具有挑战性。在这项工作中，我们提出了两种不同的管道，基于不规则模型来实现肝脏和肿瘤的分割。第一个管道是基础多类模型，同时进行肝脏和肿瘤的同时分割。第二个管道是分别训练两个不同的二进制模型，一个用于肝脏的分割，另一个用于肿瘤的分割。我们的结果显示，这两种管道具有不同的优势和劣势。此外，我们还提出了一种不确定性评估策略，以便标识潜在的假阳性肿瘤患区。这两种解决方案都被提交到了MICCAI 2023 Atlas challenge，关于肝脏和肿瘤的分割。
</details></li>
</ul>
<hr>
<h2 id="Maintaining-Plasticity-via-Regenerative-Regularization"><a href="#Maintaining-Plasticity-via-Regenerative-Regularization" class="headerlink" title="Maintaining Plasticity via Regenerative Regularization"></a>Maintaining Plasticity via Regenerative Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11958">http://arxiv.org/abs/2308.11958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saurabh Kumar, Henrik Marklund, Benjamin Van Roy</li>
<li>for: 维护 continual learning 中的材料塑性（plasticity），使 neural network 能够快速适应新信息。</li>
<li>methods: 提出了 L2 Init，一种简单的方法，通过在损失函数中添加 L2 正则化来维护初始参数的塑性。</li>
<li>results: 在不同类型的非站ARY数据流程上进行了简单的问题示例，证明了 L2 Init 能够有效地避免材料塑性损失。 另外，我们发现了这个正则化项可以减少参数的大小，并保持高效的特征级别。<details>
<summary>Abstract</summary>
In continual learning, plasticity refers to the ability of an agent to quickly adapt to new information. Neural networks are known to lose plasticity when processing non-stationary data streams. In this paper, we propose L2 Init, a very simple approach for maintaining plasticity by incorporating in the loss function L2 regularization toward initial parameters. This is very similar to standard L2 regularization (L2), the only difference being that L2 regularizes toward the origin. L2 Init is simple to implement and requires selecting only a single hyper-parameter. The motivation for this method is the same as that of methods that reset neurons or parameter values. Intuitively, when recent losses are insensitive to particular parameters, these parameters drift toward their initial values. This prepares parameters to adapt quickly to new tasks. On simple problems representative of different types of nonstationarity in continual learning, we demonstrate that L2 Init consistently mitigates plasticity loss. We additionally find that our regularization term reduces parameter magnitudes and maintains a high effective feature rank.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation"><a href="#When-MiniBatch-SGD-Meets-SplitFed-Learning-Convergence-Analysis-and-Performance-Evaluation" class="headerlink" title="When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation"></a>When MiniBatch SGD Meets SplitFed Learning:Convergence Analysis and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11953">http://arxiv.org/abs/2308.11953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Huang, Geng Tian, Ming Tang<br>for:这个论文旨在解决 federated learning (FL) 中的客户端偏移问题，提出了 MiniBatch-SFL 算法，它将 MiniBatch SGD integrated into SFL 中。methods:这个论文使用了 SFL 分布式算法，并在客户端和服务器之间分割模型。客户端只需要训练部分模型，而服务器则负责训练服务器端模型。此外，这个论文还使用了 MiniBatch SGD 算法来优化客户端模型。results:这个论文的实验结果表明，MiniBatch-SFL 算法可以在非Identical Independent Distributions (non-IID) 数据上提高精度，并且可以与传统的 FL 和 SFL 相比，提高精度达到 24.1% 和 17.1%。此外，这个论文还发现，将 cut layer 放置在模型的末端可以降低客户端模型的均值梯度偏移。<details>
<summary>Abstract</summary>
Federated learning (FL) enables collaborative model training across distributed clients (e.g., edge devices) without sharing raw data. Yet, FL can be computationally expensive as the clients need to train the entire model multiple times. SplitFed learning (SFL) is a recent distributed approach that alleviates computation workload at the client device by splitting the model at a cut layer into two parts, where clients only need to train part of the model. However, SFL still suffers from the \textit{client drift} problem when clients' data are highly non-IID. To address this issue, we propose MiniBatch-SFL. This algorithm incorporates MiniBatch SGD into SFL, where the clients train the client-side model in an FL fashion while the server trains the server-side model similar to MiniBatch SGD. We analyze the convergence of MiniBatch-SFL and show that the bound of the expected loss can be obtained by analyzing the expected server-side and client-side model updates, respectively. The server-side updates do not depend on the non-IID degree of the clients' datasets and can potentially mitigate client drift. However, the client-side model relies on the non-IID degree and can be optimized by properly choosing the cut layer. Perhaps counter-intuitive, our empirical result shows that a latter position of the cut layer leads to a smaller average gradient divergence and a better algorithm performance. Moreover, numerical results show that MiniBatch-SFL achieves higher accuracy than conventional SFL and FL. The accuracy improvement can be up to 24.1\% and 17.1\% with highly non-IID data, respectively.
</details>
<details>
<summary>摘要</summary>
分布式学习（FL）允许分布式客户端（例如边缘设备）共同训练模型，无需分享原始数据。然而，FL可能具有 computationally expensive 的问题，因为客户端需要训练整个模型多次。SplitFed learning（SFL）是一种最近的分布式方法，它将模型在一个割层中分成两部分，客户端只需要训练一部分模型。然而，SFL仍然受到客户端数据非常不一致（non-IID）的问题困扰。为解决这个问题，我们提出了 MiniBatch-SFL。这个算法将 MiniBatch SGD  integrate into SFL，客户端在分布式方式上训练客户端模型，服务器则在服务器模型上进行类似的 MiniBatch SGD 训练。我们分析了 MiniBatch-SFL 的收敛性，并证明了 bound 的预期损失可以通过分析服务器和客户端模型更新的预期值来获得。服务器端更新不依赖于客户端数据的非一致程度，可能减轻客户端游弋问题。然而，客户端模型受到非一致度的影响，可以通过选择合适的割层来优化。 Surprisingly,我们的实验结果显示，在割层的位置越后，客户端模型的平均梯度差异越小，算法性能更好。此外，我们的数值结果显示，MiniBatch-SFL 可以达到更高的准确率，比 conventinal SFL 和 FL 高出 24.1% 和 17.1%。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting"><a href="#Multi-scale-Transformer-Pyramid-Networks-for-Multivariate-Time-Series-Forecasting" class="headerlink" title="Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting"></a>Multi-scale Transformer Pyramid Networks for Multivariate Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11946">http://arxiv.org/abs/2308.11946</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhang, Rui Wu, Sergiu M. Dascalu, Frederick C. Harris Jr</li>
<li>for: 这篇论文主要针对多重时间序列（MTS）预测任务进行探索，旨在模型时间序列之间的相互 dependencies。</li>
<li>methods: 本论文提出了一种维度不变的嵌入技术，可以将MTS数据转换为更高维度的空间，保持时间步骤和变数的维度，并且提出了一个多尺度 transformer  pyramid network（MTPNet），可以有效地捕捉时间序列之间的多个不同级数的相互dependencies。</li>
<li>results: 实验结果显示，提出的MTPNet方法在九个 benchmark 数据集上表现出色，较前一些现有的方法更好。<details>
<summary>Abstract</summary>
Multivariate Time Series (MTS) forecasting involves modeling temporal dependencies within historical records. Transformers have demonstrated remarkable performance in MTS forecasting due to their capability to capture long-term dependencies. However, prior work has been confined to modeling temporal dependencies at either a fixed scale or multiple scales that exponentially increase (most with base 2). This limitation hinders their effectiveness in capturing diverse seasonalities, such as hourly and daily patterns. In this paper, we introduce a dimension invariant embedding technique that captures short-term temporal dependencies and projects MTS data into a higher-dimensional space, while preserving the dimensions of time steps and variables in MTS data. Furthermore, we present a novel Multi-scale Transformer Pyramid Network (MTPNet), specifically designed to effectively capture temporal dependencies at multiple unconstrained scales. The predictions are inferred from multi-scale latent representations obtained from transformers at various scales. Extensive experiments on nine benchmark datasets demonstrate that the proposed MTPNet outperforms recent state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多变量时间序列（MTS）预测涉及到历史记录中的时间相关性模型化。transformers已经表现出了很好的性能在MTS预测中，因为它们可以捕捉长期相关性。然而，先前的工作受限于模型时间相关性的固定尺度或多个尺度，其中尺度递增级数(大多数为2)。这种限制使得它们难以捕捉多样化的季节性，如每小时和每天的模式。在这篇论文中，我们介绍了一种维度不变的嵌入技术，该技术可以捕捉短期时间相关性，并将MTS数据 проек到高维空间中，保持时间步骤和变量的维度。此外，我们提出了一种新的多尺度transformer piramid网络（MTPNet），该网络专门设计用于有效地捕捉多个不受限制的时间尺度的时间相关性。预测来自多个尺度的秘密表示，由transformers在不同尺度上获得。经验表明，我们提出的MTPNet在九个 benchmark datasets上表现出了较高的性能。
</details></li>
</ul>
<hr>
<h2 id="RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching"><a href="#RamseyRL-A-Framework-for-Intelligent-Ramsey-Number-Counterexample-Searching" class="headerlink" title="RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching"></a>RamseyRL: A Framework for Intelligent Ramsey Number Counterexample Searching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11943">http://arxiv.org/abs/2308.11943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Steve Vott, Adam M. Lehavi</li>
<li>for: 本研究探讨了应用最佳先进搜索算法和强化学习（RL）技术来找到特定的拉曼数字（Ramsey number）的对例。</li>
<li>methods: 本研究引入了图Vectorization和深度强化网络（DNN）基于的优化方法，用于评估图是否为对例，并且提出了一些算法优化来减少搜索时间的复杂度。</li>
<li>results: 本研究不目的是发现新的对例，而是提出了一种基于RL技术的对例探讨框架，可以应用于其他评估器。<details>
<summary>Abstract</summary>
The Ramsey number is the minimum number of nodes, $n = R(s, t)$, such that all undirected simple graphs of order $n$, contain a clique of order $s$, or an independent set of order $t$. This paper explores the application of a best first search algorithm and reinforcement learning (RL) techniques to find counterexamples to specific Ramsey numbers. We incrementally improve over prior search methods such as random search by introducing a graph vectorization and deep neural network (DNN)-based heuristic, which gauge the likelihood of a graph being a counterexample. The paper also proposes algorithmic optimizations to confine a polynomial search runtime. This paper does not aim to present new counterexamples but rather introduces and evaluates a framework supporting Ramsey counterexample exploration using other heuristics. Code and methods are made available through a PyPI package and GitHub repository.
</details>
<details>
<summary>摘要</summary>
“拉姆馆数”是最少节点数量，$n = R(s, t)$, 使得所有无向简单图的顺序为$n$，都包含一个 clique 的顺序为$s$，或一个独立集的顺序为$t$。这篇论文探讨了使用最佳先搜索算法和强化学习（RL）技术来找到特定拉姆数字的对例。我们在先前搜索方法，如随机搜索，基础上进行了改进，通过引入图vector化和深度神经网络（DNN）基于的优化，来评估图是否为对例。这篇论文也提出了算法优化来限制搜索时间的多项式增长。本论文不是为提供新的对例，而是为探讨拉姆对例探索使用其他规则的框架。代码和方法通过PyPI包和GitHub存储库提供。
</details></li>
</ul>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
<li>for: 这个研究的目的是提高现有的文本到audio模型的可控性，以便在仅仅基于文本的情况下实现细化的声音生成。</li>
<li>methods: 该模型使用了一种新的方法，即在已经训练过的文本到audio模型基础上加入了内容（时间戳）和风格（折射和能量折射）等附加条件，以便控制生成的声音的时间顺序、折射和能量。</li>
<li>results: 实验结果表明，该模型成功实现了细化的声音生成，并且可以控制生成的声音在不同的时间戳、折射和能量上的表现。<details>
<summary>Abstract</summary>
Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at https://conditionaudiogen.github.io/conditionaudiogen/
</details>
<details>
<summary>摘要</summary>
文本基于的音频生成模型受限，因为它们无法包含所有音频中的信息，导致仅仅基于文本的控制性不够。为解决这问题，我们提出了一种新的模型，它可以增强现有的预训练文本到音频模型的控制性，通过添加内容（时间戳）和风格（折射和能量折射）等条件。这种方法可以实现细致的控制音频的时间顺序、折射和能量。为保持生成的多样性，我们使用可训练的控制条件编码器，其中包括一个大型自然语言模型和可训练的拟合网络，以编码和融合其他条件，同时保持预训练文本到音频模型的重量冰结。由于缺乏适合的数据集和评价指标，我们将现有的数据集整合成一个新的数据集，包括音频和相应的条件，并使用一系列的评价指标来评价控制性性能。实验结果表明，我们的模型成功实现了细致的控制，以实现可控音频生成。生成的音频和数据集可以在https://conditionaudiogen.github.io/conditionaudiogen/上公开获取。
</details></li>
</ul>
<hr>
<h2 id="Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series"><a href="#Retail-Demand-Forecasting-A-Comparative-Study-for-Multivariate-Time-Series" class="headerlink" title="Retail Demand Forecasting: A Comparative Study for Multivariate Time Series"></a>Retail Demand Forecasting: A Comparative Study for Multivariate Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11939">http://arxiv.org/abs/2308.11939</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Sabbirul Haque, Md Shahedul Amin, Jonayet Miah</li>
<li>for: 预测零售需求的精度预测是零售业的关键因素，对于公司的财务表现和供应链效率都是关键。</li>
<li>methods: 本研究使用了时间序列数据和 macro经济变量（如Consumer Price Index（CPI）、Index of Consumer Sentiment（ICS）和失业率）进行预测，并比较了不同的回归和机器学习模型以确定最佳预测模型。</li>
<li>results: 研究发现，通过添加macro经济变量，可以更好地预测零售需求，并且可以选择合适的预测模型以达到更高的预测精度。<details>
<summary>Abstract</summary>
Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.
</details>
<details>
<summary>摘要</summary>
<<sys.language_model.translate(text="Accurate demand forecasting in the retail industry is a critical determinant of financial performance and supply chain efficiency. As global markets become increasingly interconnected, businesses are turning towards advanced prediction models to gain a competitive edge. However, existing literature mostly focuses on historical sales data and ignores the vital influence of macroeconomic conditions on consumer spending behavior. In this study, we bridge this gap by enriching time series data of customer demand with macroeconomic variables, such as the Consumer Price Index (CPI), Index of Consumer Sentiment (ICS), and unemployment rates. Leveraging this comprehensive dataset, we develop and compare various regression and machine learning models to predict retail demand accurately.", from_language="en", to_language="zh-CN")>>Here's the translation:精准预测销售需求是零售业的关键性质因素，直接影响营业性和供应链效率。随着全球市场变得越来越相连，企业正在寻找更高级的预测模型，以获得竞争优势。然而，现有的文献主要关注历史销售数据，忽视了消费者支出行为中的重要影响因素。本研究通过把 macro经济条件纳入销售需求时间序列数据中，使用 Consumer Price Index (CPI)、Index of Consumer Sentiment (ICS) 和失业率等macro经济变量，并利用这些全面的数据来开发和比较不同的回归和机器学习模型，以准确预测零售需求。
</details></li>
</ul>
<hr>
<h2 id="System-Identification-for-Continuous-time-Linear-Dynamical-Systems"><a href="#System-Identification-for-Continuous-time-Linear-Dynamical-Systems" class="headerlink" title="System Identification for Continuous-time Linear Dynamical Systems"></a>System Identification for Continuous-time Linear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11933">http://arxiv.org/abs/2308.11933</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jonas-Nicodemus/phdmd">https://github.com/Jonas-Nicodemus/phdmd</a></li>
<li>paper_authors: Peter Halmos, Jonathan Pillow, David A. Knowles</li>
<li>for: 这篇论文旨在探讨kalman筛Filter的系统识别问题，具体来说是通过期望最大化（EM）程序来学习底层系统的参数。</li>
<li>methods: 这篇论文使用了一种新的两filter方法，其中一个是 posterior的分布，另一个是 bayesian derivation。这两个方法可以减少了前向传递的计算量。</li>
<li>results: 通过这种新的方法， authors 可以在不Regularly sampled measurements中进行系统识别，并且可以扩展kalman筛Filter的应用范围。他们还提出了一种扩展了learning的方法，可以在不Regularly sampled measurements中学习非线性系统。<details>
<summary>Abstract</summary>
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data which is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details>
<details>
<summary>摘要</summary>
System identification for the Kalman filter, which relies on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has been largely studied assuming that observations are sampled at equally-spaced time points. However, in many applications, this assumption is unrealistic. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates that do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure that estimates the parameters of the SDE, naturally incorporating irregularly sampled measurements. Generalizing the learning of latent linear dynamical systems (LDS) to continuous-time may extend the use of the hybrid Kalman filter to data that is not regularly sampled or has intermittent missing values, and can extend the power of non-linear system identification methods such as switching LDS (SLDS), which rely on EM for the linear discrete-time Kalman filter as a sub-unit for learning locally linearized behavior of a non-linear system. We apply the method by learning the parameters of a latent, multivariate Fokker-Planck SDE representing a toggle-switch genetic circuit using biologically realistic parameters, and compare the efficacy of learning relative to the discrete-time Kalman filter as the step-size irregularity and spectral-radius of the dynamics-matrix increases.
</details></li>
</ul>
<hr>
<h2 id="Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas"><a href="#Dynamic-landslide-susceptibility-mapping-over-recent-three-decades-to-uncover-variations-in-landslide-causes-in-subtropical-urban-mountainous-areas" class="headerlink" title="Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas"></a>Dynamic landslide susceptibility mapping over recent three decades to uncover variations in landslide causes in subtropical urban mountainous areas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11929">http://arxiv.org/abs/2308.11929</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cli-de/d_lsm">https://github.com/cli-de/d_lsm</a></li>
<li>paper_authors: Peifeng Ma, Li Chen, Chang Yu, Qing Zhu, Yulin Ding</li>
<li>for: 这项研究的目的是为了开发一种能够适应不同时间间隔的风险隐藏易受损地区风险评估方法，以便更好地预测滥覆风险。</li>
<li>methods: 这项研究使用了多种预测模型来实现年度风险评估，并使用了 SHAP 算法来解释每个模型的输入特征和预测结果。此外，研究还使用了 MT-InSAR 技术来增强和验证风险评估结果。</li>
<li>results: 研究结果显示，在香港大屿山岛的风险评估中，地形坡度和极端雨量是诱发滥覆的主要因素。此外，研究还发现，由于全球气候变化和香港政府实施的LPMitP计划，风险评估结果的变化归因于极端雨量事件。<details>
<summary>Abstract</summary>
Landslide susceptibility assessment (LSA) is of paramount importance in mitigating landslide risks. Recently, there has been a surge in the utilization of data-driven methods for predicting landslide susceptibility due to the growing availability of aerial and satellite data. Nonetheless, the rapid oscillations within the landslide-inducing environment (LIE), primarily due to significant changes in external triggers such as rainfall, pose difficulties for contemporary data-driven LSA methodologies to accommodate LIEs over diverse timespans. This study presents dynamic landslide susceptibility mapping that simply employs multiple predictive models for annual LSA. In practice, this will inevitably encounter small sample problems due to the limited number of landslide samples in certain years. Another concern arises owing to the majority of the existing LSA approaches train black-box models to fit distinct datasets, yet often failing in generalization and providing comprehensive explanations concerning the interactions between input features and predictions. Accordingly, we proposed to meta-learn representations with fast adaptation ability using a few samples and gradient updates; and apply SHAP for each model interpretation and landslide feature permutation. Additionally, we applied MT-InSAR for LSA result enhancement and validation. The chosen study area is Lantau Island, Hong Kong, where we conducted a comprehensive dynamic LSA spanning from 1992 to 2019. The model interpretation results demonstrate that the primary factors responsible for triggering landslides in Lantau Island are terrain slope and extreme rainfall. The results also indicate that the variation in landslide causes can be primarily attributed to extreme rainfall events, which result from global climate change, and the implementation of the Landslip Prevention and Mitigation Programme (LPMitP) by the Hong Kong government.
</details>
<details>
<summary>摘要</summary>
降坡风险评估 (LSA) 在减轻降坡风险方面具有重要的重要性。在最近几年，由于飞地和卫星数据的可用性的增加，数据驱动方法在预测降坡风险方面得到了广泛的应用。然而，降坡 inducing 环境 (LIE) 中的快速摆动，主要归因于外部触发因素的显著变化，如降水量，使得当今的数据驱动 LSA 方法困难于同时覆盖多年时间。本研究提出了动态降坡风险地图，使用多个预测模型来年度预测降坡风险。在实践中，这将不可避免小样本问题，因为降坡样本数在某些年仅有限制。另一个问题在于大多数现有 LSA 方法通常会训练黑盒模型适应特定数据集，而不能总结和提供降坡特征之间和预测之间的丰富解释。因此，我们提议使用元学习来学习表达能力快速适应，使用少量样本和梯度更新；并使用 SHAP 来对每个模型进行解释和降坡特征的排序。此外，我们还应用 MT-InSAR 来增强和验证 LSA 结果。选择的研究区为香港大屿山岛，我们在1992年至2019年之间进行了全面的动态 LSA。模型解释结果显示，降坡岛的主要触发降坡的因素是地形坡度和极端降水量。结果还表明，降坡的变化原因可以主要归因于全球气候变化和香港政府实施的降坡预防和控制Programme (LPMitP)。
</details></li>
</ul>
<hr>
<h2 id="Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks"><a href="#Solving-Elliptic-Optimal-Control-Problems-using-Physics-Informed-Neural-Networks" class="headerlink" title="Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks"></a>Solving Elliptic Optimal Control Problems using Physics Informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11925">http://arxiv.org/abs/2308.11925</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangti Jin, Ramesh Sau, Luowei Yin, Zhi Zhou</li>
<li>for: 这篇论文是关于优化控制问题的数值解决方案，包括不包括箱constraint的情况。</li>
<li>methods: 该方法基于优化控制问题的第一阶最优系统，使用物理学 informed neural networks (PINNs) 解决coupled系统。</li>
<li>results: 论文提供了深度逻辑学习网络参数（例如深度、宽度、参数范围）和样本点数的$L^2(\Omega)$ 误差 bounds，并进行了误差分析。示例包括了比较现有三种方法。<details>
<summary>Abstract</summary>
In this work, we present and analyze a numerical solver for optimal control problems (without / with box constraint) for linear and semilinear second-order elliptic problems. The approach is based on a coupled system derived from the first-order optimality system of the optimal control problem, and applies physics informed neural networks (PINNs) to solve the coupled system. We present an error analysis of the numerical scheme, and provide $L^2(\Omega)$ error bounds on the state, control and adjoint state in terms of deep neural network parameters (e.g., depth, width, and parameter bounds) and the number of sampling points in the domain and on the boundary. The main tools in the analysis include offset Rademacher complexity and boundedness and Lipschitz continuity of neural network functions. We present several numerical examples to illustrate the approach and compare it with three existing approaches.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们提出了一种数值方法来解决优化控制问题（无框约束和框约束）的线性和半线性第二阶几何问题。我们基于优化控制问题的第一阶优化系统 derivated 一个联系系统，并使用物理学 Informed Neural Networks (PINNs) 解决这个联系系统。我们提供了数值方案的误差分析，并给出了 $L^2(\Omega)$ 误差 bound 在状态、控制和副状态上，这些 bound 取决于深度神经网络参数（例如深度、宽度和参数 bound）和采样点数量在领域和边界上。我们使用偏移Rademacher复杂度和领域和边界上的神经网络函数的稳定性和 lipschitz 连续性作为主要工具进行分析。我们在数据中提供了多个数值示例，以示出方法的应用和与三种现有方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe"><a href="#Diverse-Policies-Converge-in-Reward-free-Markov-Decision-Processe" class="headerlink" title="Diverse Policies Converge in Reward-free Markov Decision Processe"></a>Diverse Policies Converge in Reward-free Markov Decision Processe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11924">http://arxiv.org/abs/2308.11924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openrl-lab/diversepolicies">https://github.com/openrl-lab/diversepolicies</a></li>
<li>paper_authors: Fanqi Lin, Shiyu Huang, Weiwei Tu</li>
<li>for: 这篇论文的目的是提供一种统一的多种政策强化学习框架，并研究多种政策强化学习的训练是如何 converges 和效率如何。</li>
<li>methods: 这篇论文使用了一种提取多种政策的框架，并提出了一种可证明有效的多种政策强化学习算法。</li>
<li>results: 经过数学实验，论文证明了其方法的有效性和效率。<details>
<summary>Abstract</summary>
Reinforcement learning has achieved great success in many decision-making tasks, and traditional reinforcement learning algorithms are mainly designed for obtaining a single optimal solution. However, recent works show the importance of developing diverse policies, which makes it an emerging research topic. Despite the variety of diversity reinforcement learning algorithms that have emerged, none of them theoretically answer the question of how the algorithm converges and how efficient the algorithm is. In this paper, we provide a unified diversity reinforcement learning framework and investigate the convergence of training diverse policies. Under such a framework, we also propose a provably efficient diversity reinforcement learning algorithm. Finally, we verify the effectiveness of our method through numerical experiments.
</details>
<details>
<summary>摘要</summary>
现在的束缚学习已经在许多决策任务中取得了很大的成功，但传统的束缚学习算法主要是为了获得单个优化解决方案。然而，最近的研究表明了多样化策略的重要性，使得这成为一个emerging研究话题。虽然多样化束缚学习算法的多种出现，但没有任何一个能够理论地回答束缚学习算法是如何收敛的和如何效率的问题。在这篇论文中，我们提供了一个统一的多样化束缚学习框架，并investigate束缚学习训练多样化策略的收敛性。根据这种框架，我们还提出了一种可证明高效的多样化束缚学习算法。最后，我们通过数学实验验证了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
<li>for: 这 paper 是为了解决音频描述中的 semantic difference 问题，提出了 Audio Difference Captioning (ADC) 任务。</li>
<li>methods: 该 paper 提出了一种 cross-attention-concentrated transformer encoder 和一种 similarity-discrepancy disentanglement 来提取差异。</li>
<li>results: 实验表明，提出的方法可以有效地解决 ADC 任务，并且可以提高 transformer encoder 中的 attention weights 来提取差异。<details>
<summary>Abstract</summary>
We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.
</details>
<details>
<summary>摘要</summary>
我们提出了听音差异描述（ADC）作为audio描述中的新扩展任务，用于描述输入对的相似 yet slightly different 听音clip中的semantic差异。ADC解决了 convention audio描述 Sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach"><a href="#Addressing-Selection-Bias-in-Computerized-Adaptive-Testing-A-User-Wise-Aggregate-Influence-Function-Approach" class="headerlink" title="Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach"></a>Addressing Selection Bias in Computerized Adaptive Testing: A User-Wise Aggregate Influence Function Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11912">http://arxiv.org/abs/2308.11912</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/riiid/useraif">https://github.com/riiid/useraif</a></li>
<li>paper_authors: Soonwoo Kwon, Sojung Kim, Seunghyun Lee, Jin-Young Kim, Suyeong An, Kyuseok Kim</li>
<li>for: 这 paper 的目的是提出一种基于CAT服务中的应答数据的项目 profiling 方法，以提高CAT的效率和准确性。</li>
<li>methods: 这 paper 使用了一种叫做 user-wise aggregate influence function 方法，该方法可以纠正CAT中选择性的偏见问题，并提高CAT的性能。</li>
<li>results:  experiments 表明，使用了该方法可以减少CAT中的偏见问题，并提高CAT的准确性和效率。<details>
<summary>Abstract</summary>
Computerized Adaptive Testing (CAT) is a widely used, efficient test mode that adapts to the examinee's proficiency level in the test domain. CAT requires pre-trained item profiles, for CAT iteratively assesses the student real-time based on the registered items' profiles, and selects the next item to administer using candidate items' profiles. However, obtaining such item profiles is a costly process that involves gathering a large, dense item-response data, then training a diagnostic model on the collected data. In this paper, we explore the possibility of leveraging response data collected in the CAT service. We first show that this poses a unique challenge due to the inherent selection bias introduced by CAT, i.e., more proficient students will receive harder questions. Indeed, when naively training the diagnostic model using CAT response data, we observe that item profiles deviate significantly from the ground-truth. To tackle the selection bias issue, we propose the user-wise aggregate influence function method. Our intuition is to filter out users whose response data is heavily biased in an aggregate manner, as judged by how much perturbation the added data will introduce during parameter estimation. This way, we may enhance the performance of CAT while introducing minimal bias to the item profiles. We provide extensive experiments to demonstrate the superiority of our proposed method based on the three public datasets and one dataset that contains real-world CAT response data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models"><a href="#Diagnosing-Infeasible-Optimization-Problems-Using-Large-Language-Models" class="headerlink" title="Diagnosing Infeasible Optimization Problems Using Large Language Models"></a>Diagnosing Infeasible Optimization Problems Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12923">http://arxiv.org/abs/2308.12923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Chen, Gonzalo E. Constante-Flores, Can Li</li>
<li>for: 本研究旨在帮助决策问题中的优化模型实用化，通过自然语言对话系统OptiChat来帮助用户更好地理解和解释无法满足约束的优化模型。</li>
<li>methods: 本研究使用了GPT-4和一些自然语言处理技术，如几何学习、专家链思维、关键提取和情感提示，以提高OptiChat的可靠性。</li>
<li>results: 实验表明，OptiChat可以帮助both expert和非专家用户更好地理解优化模型，快速地发现约束不可遵循的来源。<details>
<summary>Abstract</summary>
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optimization model itself, identify potential sources of infeasibility, and offer suggestions to make the model feasible. The implementation of OptiChat is built on GPT-4, which interfaces with an optimization solver to identify the minimal subset of constraints that render the entire optimization problem infeasible, also known as the Irreducible Infeasible Subset (IIS). We utilize few-shot learning, expert chain-of-thought, key-retrieve, and sentiment prompts to enhance OptiChat's reliability. Our experiments demonstrate that OptiChat assists both expert and non-expert users in improving their understanding of the optimization models, enabling them to quickly identify the sources of infeasibility.
</details>
<details>
<summary>摘要</summary>
决策问题可以表示为数学优化模型，在经济、工程和生产、运输和医疗等领域找到广泛应用。优化模型是决策问题的数学抽象，它的目的是找到满足一组要求或限制的最佳决策。但现有的优化模型诊断方法通常需要很多背景知识，特别是当模型无法满足所有限制时。在这篇论文中，我们介绍了一种名为OptiChat的自然语言基于系统，它通过自然语言描述优化模型、找到可能导致无法满足限制的来源、并提供改进模型的建议。OptiChat的实现基于GPT-4，它通过与优化解тил器集成来确定整个优化问题的不可能满足子集（IIS）。我们使用了少量学习、专家链条思维、关键检索和情感提示来增强OptiChat的可靠性。我们的实验表明，OptiChat可以帮助专家和非专家用户更好地理解优化模型，快速地找到无法满足限制的来源。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Admissible-Bounds-for-Heuristic-Learning"><a href="#Utilizing-Admissible-Bounds-for-Heuristic-Learning" class="headerlink" title="Utilizing Admissible Bounds for Heuristic Learning"></a>Utilizing Admissible Bounds for Heuristic Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11905">http://arxiv.org/abs/2308.11905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carlos Núñez-Molina, Masataro Asai</li>
<li>for: 这篇论文旨在解决超参论文中的问题，即使用现代机器学习技术学习前进搜索算法的准则，但是这些准则的选择和训练方法尚未得到充分的理论理解。</li>
<li>methods: 这篇论文使用了 truncated Gaussian distributions 作为参数，以便更紧的限制假设空间，从而更好地遵循最大 entropy 原则。</li>
<li>results: 论文的实验结果表明，使用 admissible heuristics 作为参数可以更准确地学习前进搜索算法的准则，并且在训练过程中更快地 converges。<details>
<summary>Abstract</summary>
While learning a heuristic function for forward search algorithms with modern machine learning techniques has been gaining interest in recent years, there has been little theoretical understanding of \emph{what} they should learn, \emph{how} to train them, and \emph{why} we do so. This lack of understanding leads to various literature performing an ad-hoc selection of datasets (suboptimal vs optimal costs or admissible vs inadmissible heuristics) and optimization metrics (e.g., squared vs absolute errors). Moreover, due to the lack of admissibility of the resulting trained heuristics, little focus has been put on the role of admissibility \emph{during} learning. This paper articulates the role of admissible heuristics in supervised heuristic learning using them as parameters of Truncated Gaussian distributions, which tightens the hypothesis space compared to ordinary Gaussian distributions. We argue that this mathematical model faithfully follows the principle of maximum entropy and empirically show that, as a result, it yields more accurate heuristics and converges faster during training.
</details>
<details>
<summary>摘要</summary>
“在最近几年中，使用现代机器学习技术学习前向搜索算法的启发函数Received increasing attention。然而，对于这些启发函数的学习还没有充分的理论理解，包括它们应该学习什么、如何训练它们以及为什么这样做。这导致了一些文献选择不优化的数据集（非最优化成本或非合法启发）和优化指标（例如平方差误差）。此外，由于启发函数的不合法性，对其学习过程中的合法性得 little attention。本文详细介绍了合法启发函数在监督式学习中的角色，并使用 truncated Gaussian distribution 作为参数，这会紧紧地限制假设空间，与普通 Gaussian distribution 相比。我们认为这种数学模型遵循最大Entropy原则，并且在实验中证明这会导致更加准确的启发函数和更快的学习速度。”Note: "Truncated Gaussian distribution" in the text refers to a type of probability distribution that is similar to a Gaussian distribution, but with a truncated range of values.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation"><a href="#Rethinking-Data-Perturbation-and-Model-Stabilization-for-Semi-supervised-Medical-Image-Segmentation" class="headerlink" title="Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation"></a>Rethinking Data Perturbation and Model Stabilization for Semi-supervised Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11903">http://arxiv.org/abs/2308.11903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhenzhao/dpms">https://github.com/zhenzhao/dpms</a></li>
<li>paper_authors: Zhen Zhao, Ye Liu, Meng Zhao, Di Yin, Yixuan Yuan, Luping Zhou<br>for: 本研究目的是提高 semi-supervised medical image segmentation（SSMIS）的性能。methods: 本研究提出了一种简单 yet effective的方法，称为DPMS，以提高 SSMIS 性能。DPMS 使用了一种 teacher-student 框架，并采用了数据杂化、模型稳定化和损失函数优化等策略来提高 SSMIS 性能。results: DPMS 在公共的 2D ACDC 和 3D LA 数据集上 across 多种 semi-supervised 设定中均获得了新的state-of-the-art 性能，比如在 ACDC 上使用 5% 标签时获得了22.62% 的提升。<details>
<summary>Abstract</summary>
Studies on semi-supervised medical image segmentation (SSMIS) have seen fast progress recently. Due to the limited labelled data, SSMIS methods mainly focus on effectively leveraging unlabeled data to enhance the segmentation performance. However, despite their promising performance, current state-of-the-art methods often prioritize integrating complex techniques and loss terms rather than addressing the core challenges of semi-supervised scenarios directly. We argue that the key to SSMIS lies in generating substantial and appropriate prediction disagreement on unlabeled data. To this end, we emphasize the crutiality of data perturbation and model stabilization in semi-supervised segmentation, and propose a simple yet effective approach to boost SSMIS performance significantly, dubbed DPMS. Specifically, we first revisit SSMIS from three distinct perspectives: the data, the model, and the loss, and conduct a comprehensive study of corresponding strategies to examine their effectiveness. Based on these examinations, we then propose DPMS, which adopts a plain teacher-student framework with a standard supervised loss and unsupervised consistency loss. To produce appropriate prediction disagreements, DPMS perturbs the unlabeled data via strong augmentations to enlarge prediction disagreements considerably. On the other hand, using EMA teacher when strong augmentation is applied does not necessarily improve performance. DPMS further utilizes a forwarding-twice and momentum updating strategies for normalization statistics to stabilize the training on unlabeled data effectively. Despite its simplicity, DPMS can obtain new state-of-the-art performance on the public 2D ACDC and 3D LA datasets across various semi-supervised settings, e.g. obtaining a remarkable 22.62% improvement against previous SOTA on ACDC with 5% labels.
</details>
<details>
<summary>摘要</summary>
研究 semi-supervised medical image segmentation (SSMIS) 在最近几年内已经进步很快。由于受限于标注数据的有限性，SSMIS 方法主要关注使用无标注数据来提高 segmentation 性能。然而，现有的状态态� Armenian 方法经常强调 интегрирование复杂的技术和损失函数而不是直接面对半标注场景的核心挑战。我们认为 semi-supervised 的关键在于生成足够和合适的预测差异。为此，我们强调数据杂化和模型稳定在半标注 segmentation 中的重要性，并提出了一种简单 yet effective 的方法，称为 DPMS。我们从数据、模型和损失三个角度重新探讨 SSMIS，并进行了全面的研究相应的策略的效果。基于这些研究，我们提出了 DPMS，它采用了一种简单的教师-学生框架，并采用标准的supervised损失函数和无标注一致损失函数。为生成足够的预测差异，DPMS 通过强大的杂化来增加预测差异较大。另一方面，在强大杂化应用时，使用 EMA 教师并不一定提高性能。DPMS 还利用了前向 twice 和积分更新策略来normal化统计，以确保在无标注数据上的训练效果。尽管简单，DPMS 可以在各种 semi-supervised 设置中取得新的状态态� Armenian 性能，例如在 ACDC 和 LA 数据集上取得了22.62%的提高。
</details></li>
</ul>
<hr>
<h2 id="Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models"><a href="#Shape-conditioned-3D-Molecule-Generation-via-Equivariant-Diffusion-Models" class="headerlink" title="Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models"></a>Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11890">http://arxiv.org/abs/2308.11890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziqi Chen, Bo Peng, Srinivasan Parthasarathy, Xia Ning</li>
<li>for:  Identifying novel drug candidates with desired 3D shapes that bind to protein target pockets.</li>
<li>methods: 使用ShapeMol，一种基于ShapeMol的均衡生成模型，将分子表面形状编码成 latent 表示，然后通过这些表示生成3D分子结构。</li>
<li>results: ShapeMol 可以生成 novel, diverse, drug-like 分子，其3D分子结构与给定形状相似。这些结果表明 ShapeMol 可能在设计抗体细胞膜的药物候选者中发挥作用。<details>
<summary>Abstract</summary>
Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
</details>
<details>
<summary>摘要</summary>
ligand-based drug design的目的是找到与已知活性分子相似的新药候选体。在这篇论文中，我们形ulated an in silico shape-conditioned molecule generation problem，以生成基于给定分子形状的3D分子结构。为解决这个问题，我们开发了一种具有转化和旋转对称的形态响应生成模型ShapeMol。ShapeMol包括一种对称形态编码器，该编码器将分子表面形状映射到缓存中的嵌入式，以及一种对称扩散模型，该模型基于这些嵌入生成3D分子。实验结果表明，ShapeMol可以生成新、多样、药理化的3D分子结构，这些结构与给定形状相似。这些结果表明ShapeMol在设计欲 Bind to protein target pocket的药物候选体中具有潜在的应用价值。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Training-Using-Feedback-Loops"><a href="#Adversarial-Training-Using-Feedback-Loops" class="headerlink" title="Adversarial Training Using Feedback Loops"></a>Adversarial Training Using Feedback Loops</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11881">http://arxiv.org/abs/2308.11881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ali Haisam Muhammad Rafid, Adrian Sandu</li>
<li>for: 防止深度神经网络受到攻击</li>
<li>methods: 使用控制理论和反馈控制网络</li>
<li>results: 比现状更有效地防止攻击<details>
<summary>Abstract</summary>
Deep neural networks (DNN) have found wide applicability in numerous fields due to their ability to accurately learn very complex input-output relations. Despite their accuracy and extensive use, DNNs are highly susceptible to adversarial attacks due to limited generalizability. For future progress in the field, it is essential to build DNNs that are robust to any kind of perturbations to the data points. In the past, many techniques have been proposed to robustify DNNs using first-order derivative information of the network.   This paper proposes a new robustification approach based on control theory. A neural network architecture that incorporates feedback control, named Feedback Neural Networks, is proposed. The controller is itself a neural network, which is trained using regular and adversarial data such as to stabilize the system outputs. The novel adversarial training approach based on the feedback control architecture is called Feedback Looped Adversarial Training (FLAT). Numerical results on standard test problems empirically show that our FLAT method is more effective than the state-of-the-art to guard against adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets"><a href="#SUMMIT-Source-Free-Adaptation-of-Uni-Modal-Models-to-Multi-Modal-Targets" class="headerlink" title="SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets"></a>SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11880">http://arxiv.org/abs/2308.11880</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/csimo005/summit">https://github.com/csimo005/summit</a></li>
<li>paper_authors: Cody Simons, Dripta S. Raychaudhuri, Sk Miraj Ahmed, Suya You, Konstantinos Karydis, Amit K. Roy-Chowdhury</li>
<li>for: 这 paper 的目的是解决多Modal 数据的Scene Understanding问题，以便在多种应用中实现自主导航等。</li>
<li>methods: 这 paper 使用了一种 switching 框架，通过自动选择 cross-modal pseudo-label 融合方法（agreement filtering 和 entropy weighting）来适应不同的目标领域。</li>
<li>results: 这 paper 的实验结果表明，该方法可以在七个复杂的适应场景中实现比较好的效果，与有源数据的方法相当或者超过。 Specifically, the improvement in mIoU was up to 12% compared to baseline methods.<details>
<summary>Abstract</summary>
Scene understanding using multi-modal data is necessary in many applications, e.g., autonomous navigation. To achieve this in a variety of situations, existing models must be able to adapt to shifting data distributions without arduous data annotation. Current approaches assume that the source data is available during adaptation and that the source consists of paired multi-modal data. Both these assumptions may be problematic for many applications. Source data may not be available due to privacy, security, or economic concerns. Assuming the existence of paired multi-modal data for training also entails significant data collection costs and fails to take advantage of widely available freely distributed pre-trained uni-modal models. In this work, we relax both of these assumptions by addressing the problem of adapting a set of models trained independently on uni-modal data to a target domain consisting of unlabeled multi-modal data, without having access to the original source dataset. Our proposed approach solves this problem through a switching framework which automatically chooses between two complementary methods of cross-modal pseudo-label fusion -- agreement filtering and entropy weighting -- based on the estimated domain gap. We demonstrate our work on the semantic segmentation problem. Experiments across seven challenging adaptation scenarios verify the efficacy of our approach, achieving results comparable to, and in some cases outperforming, methods which assume access to source data. Our method achieves an improvement in mIoU of up to 12% over competing baselines. Our code is publicly available at https://github.com/csimo005/SUMMIT.
</details>
<details>
<summary>摘要</summary>
场景理解使用多Modal数据是非常重要的，例如自主导航。要在各种情况下实现这一点，现有的模型需要能够适应数据分布的变化，而无需辛苦地注释数据。现有的方法假设源数据在适应过程中可以获得，并且假设源数据是对应的多Modal数据。这两个假设可能会对许多应用程序带来问题。源数据可能因为隐私、安全或经济问题而不可用。假设存在对应的多Modal数据 для训练也会导致极大的数据收集成本，并且不利用广泛可用的免费分布的预训练单Modal模型。在这项工作中，我们松弛了这两个假设，解决了一个独立训练在单Modal数据上的模型，并在目标领域中使用无标记多Modal数据进行适应，无需访问源数据集。我们提出的方法通过自动选择两种补充方法——协调过滤和Entropy质量——根据估计的领域差来解决这个问题。我们在semantic segmentation问题中进行了实验，并在七个困难的适应场景中证明了我们的方法的有效性，与可以访问源数据的方法相比，我们的方法可以达到和在某些情况下超过相同的结果。我们的方法可以提高mIoU指标的改进率达到12%。我们的代码公开可用于https://github.com/csimo005/SUMMIT。
</details></li>
</ul>
<hr>
<h2 id="Cabrita-closing-the-gap-for-foreign-languages"><a href="#Cabrita-closing-the-gap-for-foreign-languages" class="headerlink" title="Cabrita: closing the gap for foreign languages"></a>Cabrita: closing the gap for foreign languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11878">http://arxiv.org/abs/2308.11878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Celio Larcher, Marcos Piau, Paulo Finardi, Pedro Gengo, Piero Esposito, Vinicius Caridá</li>
<li>for: 这个研究的目的是提高特定语言或领域上模型的性能，以及解决有效的tokenization问题。</li>
<li>methods: 这个研究使用了自scratch训练模型，并开发了一种名为Cabrita的方法，以解决性能和有效tokenization问题。</li>
<li>results: 研究表明，使用Cabrita方法可以在低成本下提高模型的性能，并且在少量学习任务中达到了与传统连续预训练方法和7B英语预训练模型相当的结果。<details>
<summary>Abstract</summary>
The strategy of training the model from scratch in a specific language or domain serves two essential purposes: i) enhancing performance in the particular linguistic or domain context, and ii) ensuring effective tokenization. The main limitation inherent to this approach lies in the associated cost, which can reach six to seven-digit dollar values, depending on the model size and the number of parameters involved.   The main solution to overcome the cost challenge is to rely on available pre-trained models, which, despite recent advancements such as the LLaMA and LLaMA-2 models, still demonstrate inefficiency for certain specific domain problems or prove ineffective in scenarios involving conversational memory resources, given the large number of tokens required to represent text.   To overcome this issue, we present a methodology named Cabrita, which, as our research demonstrates, successfully addresses the performance and efficient tokenization problem, all at an affordable cost. We believe that this methodology can be applied to any transformer-like architecture model. To validate the study, we conducted continuous pre-training exclusively using Portuguese text on a 3-billion-parameter model known as OpenLLaMA, resulting in a model named openCabrita 3B. The openCabrita 3B also features a new tokenizer that results in a significant reduction in the number of tokens required to represent the text. In our assessment, for few-shot learning tasks, we achieved similar results with this 3B model compared to a traditional continuous pre-training approach as well as to 7B models English pre-trained models.
</details>
<details>
<summary>摘要</summary>
这种训练模型从零开始在特定语言或领域中的策略有两个重要目的：一是提高特定语言或领域上表现，二是确保有效的分词。然而，这种方法的主要局限性在于相关的成本，可能达到六到七位数字的值，具体取决于模型大小和参数的数量。为了缓解这个挑战，我们可以依靠可用的预训练模型，尽管最近的进步，如LLaMA和LLaMA-2模型，仍然在特定领域问题上不够灵活，因为需要大量的 tokens 来表示文本。为了解决这个问题，我们提出了一种方法Named Cabrita，它成功地解决表现和有效的分词问题，并且具有可Affordable的成本。我们认为这种方法可以应用于任何 transformer-like 架构模型。为了验证这种方法的有效性，我们进行了不间断的预训练，只使用葡萄牙语文本，并使用一个3亿参数的模型，称为 OpenLLaMA，从而生成了一个名为 openCabrita 3B 的模型。openCabrita 3B 还有一个新的分词器，它可以减少表示文本所需的 tokens 的数量。在我们的评估中，我们在少量学习任务上达到了与传统的连续预训练方法和英语预训练模型相同的结果，而无需投入大量的时间和资源。
</details></li>
</ul>
<hr>
<h2 id="Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations"><a href="#Integrating-Large-Language-Models-into-the-Debugging-C-Compiler-for-generating-contextual-error-explanations" class="headerlink" title="Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations"></a>Integrating Large Language Models into the Debugging C Compiler for generating contextual error explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11873">http://arxiv.org/abs/2308.11873</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/comp1511unsw/dcc">https://github.com/comp1511unsw/dcc</a></li>
<li>paper_authors: Andrew Taylor, Alexandra Vassar, Jake Renzella, Hammond Pearce</li>
<li>for: 这个论文旨在使用大型自然语言模型（LLM）生成改进的编译器错误说明，以便为beginner programmer提供更好的学习体验。</li>
<li>methods: 这个论文使用了LLM生成错误说明，并在Debugging C Compiler（DCC）中实现了这种方法。</li>
<li>results: 经过专家评估，LLM生成的错误说明具有90%的编译时错误概率和75%的运行时错误概率的概念准确性。此外，DCC-help工具已经得到了学生的广泛采用，每周平均有1047次唯一运行，表明使用LLM complement编译器输出可以有助于初学者学习编程。<details>
<summary>Abstract</summary>
This paper introduces a method for Large Language Models (LLM) to produce enhanced compiler error explanations, in simple language, within our Debugging C Compiler (DCC). It is well documented that compiler error messages have been known to present a barrier for novices learning how to program. Although our initial use of DCC in introductory programming (CS1) has been instrumental in teaching C to novice programmers by providing safeguards to commonly occurring errors and translating the usually cryptic compiler error messages at both compile- and run-time, we proposed that incorporating LLM-generated explanations would further enhance the learning experience for novice programmers. Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors, and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details>
<details>
<summary>摘要</summary>
Through an expert evaluation, we observed that LLM-generated explanations for compiler errors were conceptually accurate in 90% of compile-time errors and 75% of run-time errors. Additionally, the new DCC-help tool has been increasingly adopted by students, with an average of 1047 unique runs per week, demonstrating a promising initial assessment of using LLMs to complement compiler output to enhance programming education for beginners. We release our tool as open-source to the community.
</details></li>
</ul>
<hr>
<h2 id="Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form"><a href="#Fast-Exact-NPN-Classification-with-Influence-aided-Canonical-Form" class="headerlink" title="Fast Exact NPN Classification with Influence-aided Canonical Form"></a>Fast Exact NPN Classification with Influence-aided Canonical Form</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12311">http://arxiv.org/abs/2308.12311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhang, Liwei Ni, Jiaxi Zhang, Guojie Luo, Huawei Li, Shenggen Zheng</li>
<li>for: 本文关于NPN类别的研究，具有许多应用在数字电路的合成和验证中。</li>
<li>methods: 本文提出了一种新的 canonical form 和其算法，通过引入Boolean influence来加速NPN类别。</li>
<li>results: 实验结果表明，影响在计算 canonical form 中减少了转换枚举的数量，相比之前的算法在 ABC 中实现的最佳效果，本文的影响帮助 canoncal form 在 exact NPN 类别中获得了5.5倍的速度提升。<details>
<summary>Abstract</summary>
NPN classification has many applications in the synthesis and verification of digital circuits. The canonical-form-based method is the most common approach, designing a canonical form as representative for the NPN equivalence class first and then computing the transformation function according to the canonical form. Most works use variable symmetries and several signatures, mainly based on the cofactor, to simplify the canonical form construction and computation. This paper describes a novel canonical form and its computation algorithm by introducing Boolean influence to NPN classification, which is a basic concept in analysis of Boolean functions. We show that influence is input-negation-independent, input-permutation-dependent, and has other structural information than previous signatures for NPN classification. Therefore, it is a significant ingredient in speeding up NPN classification. Experimental results prove that influence plays an important role in reducing the transformation enumeration in computing the canonical form. Compared with the state-of-the-art algorithm implemented in ABC, our influence-aided canonical form for exact NPN classification gains up to 5.5x speedup.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
<li>for:  This paper aims to improve the robustness of Kinyarwanda speech recognition using self-supervised pre-training, a simple curriculum schedule, and semi-supervised learning.</li>
<li>methods: The approach uses public domain data only, including a new studio-quality speech dataset and a more diverse and noisy public dataset. The model is trained using a simple curriculum schedule and semi-supervised learning.</li>
<li>results: The final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on the Mozilla Common Voice benchmark, which is state-of-the-art to the best of the authors’ knowledge. Additionally, the authors find that syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.Here is the simplified Chinese text in the format you requested:</li>
<li>for: 这篇论文目的是提高基奈语音识别的稳定性，使用自动预训练、简单的学习级排程和半监督学习。</li>
<li>methods: 方法使用公共领域数据，包括一个新的录音室质量的语音数据集和一个更多样本和噪音的公共数据集。模型使用简单的学习级排程和半监督学习。</li>
<li>results: 最终模型在新数据集上达到3.2%词错率（WER）和Mozilla Common Voice标准测试数据上达到15.9% WER，这是作者们知道的最佳成绩。此外，作者们发现，使用 syllabic 而不是字符基本的分词可以提高基奈语音识别性能。<details>
<summary>Abstract</summary>
Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.
</details>
<details>
<summary>摘要</summary>
尽管最近有大量的采用Kinyarwanda语音训练数据，但实现Kinyarwanda语音识别仍然是一项挑战。在这项工作中，我们表明使用自我监督预训练、在练习阶段按照简单的课程表进行调整以及使用半监督学习来利用大量未标注的语音数据，可以显著提高Kinyarwanda语音识别性能。我们的方法仅使用公共领域数据。我们收集了一个新的studio质量语音数据集，然后使用这个数据集来训练一个清晰的基线模型。然后，我们使用这个基线模型来排序来自更加多样化和噪音公共数据集的示例，定义了一个简单的课程训练计划。最后，我们应用半监督学习来标注和学习大量未标注的数据，在四个成功的代代中进行四次生成。我们的最终模型在新数据集上达到3.2%的单词错误率（WER）和Mozilla Common Voicebenchmark上达到15.9%的WER，这是我们知道的状态之最。我们的实验还表明，使用音节基于的分词而不是字符基于的分词可以提高Kinyarwanda语音识别性能。
</details></li>
</ul>
<hr>
<h2 id="Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0"><a href="#Finding-the-Perfect-Fit-Applying-Regression-Models-to-ClimateBench-v1-0" class="headerlink" title="Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0"></a>Finding the Perfect Fit: Applying Regression Models to ClimateBench v1.0</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11854">http://arxiv.org/abs/2308.11854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anmol Chaure, Ashok Kumar Behera, Sudip Bhattacharya</li>
<li>for: 该研究使用数据驱动机器学习模型作为气候侦测器，以帮助政策制定者做出了解的决策。</li>
<li>methods: 该研究使用机器学习 emulator 作为 computationally heavy GCM simulator 的代理，以降低时间和碳脚印。</li>
<li>results: 研究发现，尽管被视为基本的，回归模型在气候侦测方面具有许多优势。特别是通过内核技术，回归模型可以捕捉复杂的关系，提高预测能力。该研究比较了三种非线性回归模型，其中 Gaussian Process Regressor 表现最佳，但它在空间和时间复杂度方面具有较高的计算资源占用。相比之下，支持向量和 kernel ridge 模型也可以提供竞争力，但需要解决一些交易OFF。此外，我们正在活动调查 composite kernels 和变量推理技术的性能，以进一步提高回归模型的表现和有效地模型复杂非线性现象，包括降水现象。<details>
<summary>Abstract</summary>
Climate projections using data driven machine learning models acting as emulators, is one of the prevailing areas of research to enable policy makers make informed decisions. Use of machine learning emulators as surrogates for computationally heavy GCM simulators reduces time and carbon footprints. In this direction, ClimateBench [1] is a recently curated benchmarking dataset for evaluating the performance of machine learning emulators designed for climate data. Recent studies have reported that despite being considered fundamental, regression models offer several advantages pertaining to climate emulations. In particular, by leveraging the kernel trick, regression models can capture complex relationships and improve their predictive capabilities. This study focuses on evaluating non-linear regression models using the aforementioned dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computational resource hungry in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results and but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.
</details>
<details>
<summary>摘要</summary>
“气候预测使用数据驱动机器学模型作为模拟器，是当前气候政策决策支持的一个主要领域。使用机器学模型作为计算昂贵GCM模拟器的代理，可以降低时间和碳脚印。在这个方向上，气候Bench（1）是最近筛选的气候机器学模型评估 benchmark dataset。据报道，尽管被视为基本的，但是回归模型在气候模拟方面具有多种优势。具体来说，通过kernel trick，回归模型可以捕捉复杂的关系，提高预测能力。本研究将focus on evaluating non-linear regression models using the above dataset. Specifically, we compare the emulation capabilities of three non-linear regression models. Among them, Gaussian Process Regressor demonstrates the best-in-class performance against standard evaluation metrics used for climate field emulation studies. However, Gaussian Process Regression suffers from being computationally resource-intensive in terms of space and time complexity. Alternatively, Support Vector and Kernel Ridge models also deliver competitive results, but there are certain trade-offs to be addressed. Additionally, we are actively investigating the performance of composite kernels and techniques such as variational inference to further enhance the performance of the regression models and effectively model complex non-linear patterns, including phenomena like precipitation.”Note: The translation is done using Google Translate, and may not be perfect. Please let me know if you need further assistance.
</details></li>
</ul>
<hr>
<h2 id="A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data"><a href="#A-deep-reinforcement-learning-approach-for-real-time-demand-responsive-railway-rescheduling-to-mitigate-station-overcrowding-using-mobile-data" class="headerlink" title="A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data"></a>A deep reinforcement learning approach for real-time demand-responsive railway rescheduling to mitigate station overcrowding using mobile data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11849">http://arxiv.org/abs/2308.11849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Liu, Zhiyuan Lin, Judith Y. T. Wang, Hong Chen</li>
<li>for:  This paper aims to develop a real-time railway rescheduling approach that can automatically adjust the operation schedule in response to time-varying conditions, with a focus on a heavy-demand station upstream of a disrupted area.</li>
<li>methods:  The proposed approach uses mobile data (MD) to infer real-world passenger mobility and a deep reinforcement learning (DRL) framework to determine the optimal reschededuled timetable, route stops, and rolling stock allocation.</li>
<li>results:  The proposed approach can effectively handle challenges such as station overcrowding, rolling stock shortage, open-ended disruption duration, and delays due to detours, while ensuring real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.<details>
<summary>Abstract</summary>
Real-time railway rescheduling is a timely and flexible technique to automatically alter the operation schedule in response to time-varying conditions. Current research lacks data-driven approaches that capture real-time passenger mobility during railway disruptions, relying mostly on OD-based data and model-based methods for estimating demands of trains. Meanwhile, the schedule-updating principles for a long-term disruption overlook the uneven distribution of demand over time. To fill this gap, this paper proposes a demand-responsive approach by inferring real-world passenger mobility from mobile data (MD) to facilitate real-time rescheduling. Unlike network-level approaches, this paper focuses on a heavy-demand station upstream of the disrupted area. The objective is to reschedule all trains on multiple routes passing through this target station, which have been affected by a severe emergency event such as a natural disaster. Particular attention should be given to avoiding the accumulation of overcrowded passengers at this station, to prevent additional accidents arising from overcrowding. This research addresses the challenges associated with this scenario, including the dynamics of arriving and leaving of passengers, station overcrowding, rolling stock shortage, open-ended disruption duration, integrated rescheduling on multiple routes, and delays due to detours. A deep reinforcement learning (DRL) framework is proposed to determine the optimal rescheduled timetable, route stops, and rolling stock allocation, while considering real-time demand satisfaction, station overcrowding, train capacity utilization, and headway safety.
</details>
<details>
<summary>摘要</summary>
现实时铁路重新安排是一种时间可靠和灵活的技术，可以自动根据时间变化的条件修改运营时间表。现有研究缺乏基于实时旅客流动数据的数据驱动方法，多数依赖于 Origin-Destination（OD）数据和模型基于方法来估算列车需求。同时，长期紧急情况下的时间表更新原则忽视了时间分布的不均。为了填补这一空白，这篇论文提出了一种响应式的需求方法，通过抽象实际旅客流动数据来支持实时重新安排。与传统网络水平方法不同，这篇论文将重点放在一个受到紧急事件影响的重要客运站上，该站位于紧急事件的上游。目标是重新安排经过该站的所有列车，并且在严重紧急事件such as自然灾害发生时进行重新安排。特别是要避免在该站堆积过多的旅客，以避免由过度堆积而引起的额外事故。本研究解决了这种情况中的挑战，包括到站拥堵、列车短缺、开放式紧急事件持续时间、多路线重新安排、延迟due to detours。一种深度强化学习（DRL）框架被提出，以确定最佳重新安排时间表、站点停留、列车分配，同时考虑实时需求满足、站点拥堵、列车容量利用率和队列安全。
</details></li>
</ul>
<hr>
<h2 id="SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks"><a href="#SEA-Shareable-and-Explainable-Attribution-for-Query-based-Black-box-Attacks" class="headerlink" title="SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks"></a>SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11845">http://arxiv.org/abs/2308.11845</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Gao, Ilia Shumailov, Kassem Fawaz</li>
<li>for: 防止机器学习系统面临黑盒攻击的攻击者 profiling 和攻击信息共享。</li>
<li>methods: 基于隐藏марков模型框架，对黑盒攻击进行攻击特征分析和攻击行为描述。</li>
<li>results: 能够准确地识别黑盒攻击，包括第二次攻击，并能够抗击 adaptive 策略和库特定漏洞攻击。<details>
<summary>Abstract</summary>
Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, there is a need for a more comprehensive approach to logging, analyzing, and sharing evidence of attacks. While classic security benefits from well-established forensics and intelligence sharing, Machine Learning is yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages the Hidden Markov Models framework to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than just focusing on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on their second occurrence, and is robust to adaptive strategies designed to evade forensics analysis. Interestingly, SEA's explanations of the attack behavior allow us even to fingerprint specific minor implementation bugs in attack libraries. For example, we discover that the SignOPT and Square attacks implementation in ART v1.14 sends over 50% specific zero difference queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack's second occurrence with 90+% Top-1 and 95+% Top-3 accuracy.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）系统容易受到反面示例攻击，特别是来自查询基本黑盒攻击。尽管有各种努力检测和防止这些攻击，但是还是需要一种更全面的方法来记录、分析和分享攻击证据。 Classic security 受益于良好的遗产安全和知识共享，而 Machine Learning 尚未找到一种 profiling 攻击者并分享关于他们的信息的方法。因此，这篇论文介绍了 SEA，一种新的 ML 安全系统，用于 caracterize 黑盒攻击 ML 系统的攻击行为，以便进行审计和人类可解释的知识分享。SEA 基于隐藏markov模型框架，将观察到的查询序列归因于已知的攻击。因此，它可以理解攻击的进程，而不仅仅是关注最终的反面示例。我们的评估表明，SEA 可以有效地归因攻击，甚至在第二次出现时，并且具有鲁棒性，可以抵御适应攻击的策略。另外，SEA 的解释也可以让我们发现特定的小型实现漏洞。例如，我们发现ART v1.14中的SignOPT和方块攻击实现中发送了50%以上的特定零差查询。我们对 SEA 进行了多种设置的测试，并证明它可以在不同的设置下识别同一个攻击的第二次出现，Top-1 和 Top-3 准确率高于 90% 和 95%。
</details></li>
</ul>
<hr>
<h2 id="rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning"><a href="#rm-E-3-Equivariant-Actor-Critic-Methods-for-Cooperative-Multi-Agent-Reinforcement-Learning" class="headerlink" title="${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning"></a>${\rm E}(3)$-Equivariant Actor-Critic Methods for Cooperative Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11842">http://arxiv.org/abs/2308.11842</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dchen48/e3ac">https://github.com/dchen48/e3ac</a></li>
<li>paper_authors: Dingyang Chen, Qi Zhang</li>
<li>for: 这篇论文的目的是解决协同多体学习（MARL）问题中的几何 symmetries问题，并通过嵌入几何约束来提高多体actor-critic方法的性能。</li>
<li>methods: 该论文使用了形式化 caracterizing a subclass of Markov games with a general notion of symmetries，并设计了具有几何约束的神经网络架构。</li>
<li>results: 该论文的实验结果表明，在各种协同MARL benchmark中，具有几何约束的神经网络架构可以获得更高的性能，并且具有很好的总结和传播学习能力。<details>
<summary>Abstract</summary>
Identification and analysis of symmetrical patterns in the natural world have led to significant discoveries across various scientific fields, such as the formulation of gravitational laws in physics and advancements in the study of chemical structures. In this paper, we focus on exploiting Euclidean symmetries inherent in certain cooperative multi-agent reinforcement learning (MARL) problems and prevalent in many applications. We begin by formally characterizing a subclass of Markov games with a general notion of symmetries that admits the existence of symmetric optimal values and policies. Motivated by these properties, we design neural network architectures with symmetric constraints embedded as an inductive bias for multi-agent actor-critic methods. This inductive bias results in superior performance in various cooperative MARL benchmarks and impressive generalization capabilities such as zero-shot learning and transfer learning in unseen scenarios with repeated symmetric patterns. The code is available at: https://github.com/dchen48/E3AC.
</details>
<details>
<summary>摘要</summary>
“对自然世界中的对称图案进行识别和分析，对于不同科学领域的发现做出了重要贡献，例如物理学中的重力法则和化学结构的研究。在这篇文章中，我们专注于利用多客户多代理人学习（MARL）问题中的对称性，并将其应用到许多实际应用中。我们首先正式定义了一个泛化的Markov游戏，其中包含一般的对称性，并证明了存在对称优值和策略。这些对称性导致我们设计了具有对称约束的神经网络架构，并将其作为多代理人actor-critic方法的类别偏好。这种偏好导致了在多个合作MARL测试中的出色表现，包括零shot学习和转移学习在未见过的对称图案中。代码可以在以下GitHub页面上获取：https://github.com/dchen48/E3AC。”
</details></li>
</ul>
<hr>
<h2 id="A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures"><a href="#A-Survey-for-Federated-Learning-Evaluations-Goals-and-Measures" class="headerlink" title="A Survey for Federated Learning Evaluations: Goals and Measures"></a>A Survey for Federated Learning Evaluations: Goals and Measures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11841">http://arxiv.org/abs/2308.11841</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Chai, Leye Wang, Liu Yang, Junxue Zhang, Kai Chen, Qiang Yang</li>
<li>for: 评估 federated learning（FL）系统的目的是寻求一种系统atic approach to assessing how well a system achieves its intended purpose.</li>
<li>methods: 评估FL的方法包括评估utilities, efficiency,和security的多种指标。</li>
<li>results: 评估FL的结果包括提出了一种开源平台FedEval，以及描述了FL评估的一些挑战和未来研究方向。<details>
<summary>Abstract</summary>
Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.
</details>
<details>
<summary>摘要</summary>
评估是一种系统的方法，用于评估系统是否实现其意图的目的。联邦学习（FL）是一种新的隐私保护机器学习方法，允许多方共同训练模型而无需分享敏感数据。然而，评估FL具有多种目标和多学科性质，如有用性、效率和安全性。在这篇简介中，我们首先评审了现有研究中采用的主要评估目标，然后探讨每个目标的评估指标。我们还介绍了FedEval，一个开源的评估平台，为FL算法提供标准化和完整的评估框架，包括其有用性、效率和安全性。最后，我们讨论了一些挑战和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="A-Benchmark-Study-on-Calibration"><a href="#A-Benchmark-Study-on-Calibration" class="headerlink" title="A Benchmark Study on Calibration"></a>A Benchmark Study on Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11838">http://arxiv.org/abs/2308.11838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/history1">https://github.com/Aryia-Behroziuan/history1</a></li>
<li>paper_authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu</li>
<li>for: This paper aims to explore calibration properties within Neural Architecture Search (NAS) and provide a comprehensive dataset for studying calibration issues.</li>
<li>methods: The paper uses the NAS search space to create a model calibration dataset, which evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks.</li>
<li>results: The study aims to answer several longstanding questions in the field, including whether model calibration can be generalized across different tasks, whether robustness can be used as a calibration measurement, and how calibration interacts with accuracy. The paper also explores the impact of bin size on calibration measurement and which architectural designs are beneficial for calibration.<details>
<summary>Abstract</summary>
Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through data preprocessing, the use of specific loss functions, and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. Our study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We specifically create a model calibration dataset. This dataset evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. Our analysis aims to answer several longstanding questions in the field, using our proposed dataset: (i) Can model calibration be generalized across different tasks? (ii) Can robustness be used as a calibration measurement? (iii) How reliable are calibration metrics? (iv) Does a post-hoc calibration method affect all models uniformly? (v) How does calibration interact with accuracy? (vi) What is the impact of bin size on calibration measurement? (vii) Which architectural designs are beneficial for calibration? Additionally, our study bridges an existing gap by exploring calibration within NAS. By providing this dataset, we enable further research into NAS calibration. As far as we are aware, our research represents the first large-scale investigation into calibration properties and the premier study of calibration issues within NAS.
</details>
<details>
<summary>摘要</summary>
深度神经网络在不同机器学习任务中越来越广泛应用，然而随着模型复杂度的增加，它们经常面临调整问题，即使提高预测精度。许多研究已努力改进调整性能，使用数据预处理、特定损失函数和训练框架等方法。然而，调整性质的研究尚未得到充分的关注。我们的研究利用神经网络搜索（NAS）搜索空间，提供了详细的模型建筑空间，以便对调整性质进行全面的探索。我们专门创建了模型调整数据集，该数据集评估了90个分割值和12个附加调整测量，在117,702个Unique神经网络内进行了广泛的 NATS-Bench 搜索空间。我们的分析旨在回答以下长期问题：(i) 模型调整是否可以泛化到不同任务？(ii) 是否可以将Robustness作为调整测量？(iii) 准确性测试是如何可靠？(iv) 后期调整方法对所有模型是否具有同等影响？(v) 调整与准确度之间是否存在关系？(vi) 分割值如何影响调整测量？(vii) 哪些建筑设计有利于调整？我们的研究填补了现有的空白，通过调整在NAS中进行exploration。我们提供了这个数据集，以便进一步研究NAS调整性质。我们知道，我们的研究是首次对调整性质进行大规模的探索，也是首次在NAS中研究调整问题。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity"><a href="#Characterizing-normal-perinatal-development-of-the-human-brain-structural-connectivity" class="headerlink" title="Characterizing normal perinatal development of the human brain structural connectivity"></a>Characterizing normal perinatal development of the human brain structural connectivity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11836">http://arxiv.org/abs/2308.11836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihan Wu, Lana Vasung, Camilo Calixto, Ali Gholipour, Davood Karimi<br>for: 这项研究的目的是为了研究新生儿期的脑发育中的结构连接组织的发展趋势。methods: 这项研究使用了基于空间-时间平均值的计算框架，以确定正常发育阶段的结构连接指标。results: 研究发现，在新生儿期的33-44周岁期间，脑的结构连接显示了明显的增长趋势，全球和地方效率增加，特征路径长度减少，脑叶和脑半球之间的连接强化广泛。此外，研究还发现了一些差异征在不同的连接评价方法中的一致性。这些结果有助于评价新生儿期脑发育中的正常和异常发展。<details>
<summary>Abstract</summary>
Early brain development is characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Hence, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details>
<details>
<summary>摘要</summary>
early brain development Characterized by the formation of a highly organized structural connectome. The interconnected nature of this connectome underlies the brain's cognitive abilities and influences its response to diseases and environmental factors. Therefore, quantitative assessment of structural connectivity in the perinatal stage is useful for studying normal and abnormal neurodevelopment. However, estimation of the connectome from diffusion MRI data involves complex computations. For the perinatal period, these computations are further challenged by the rapid brain development and imaging difficulties. Combined with high inter-subject variability, these factors make it difficult to chart the normal development of the structural connectome. As a result, there is a lack of reliable normative baselines of structural connectivity metrics at this critical stage in brain development. In this study, we developed a computational framework, based on spatio-temporal averaging, for determining such baselines. We used this framework to analyze the structural connectivity between 33 and 44 postmenstrual weeks using data from 166 subjects. Our results unveiled clear and strong trends in the development of structural connectivity in perinatal stage. Connection weighting based on fractional anisotropy and neurite density produced the most consistent results. We observed increases in global and local efficiency, a decrease in characteristic path length, and widespread strengthening of the connections within and across brain lobes and hemispheres. We also observed asymmetry patterns that were consistent between different connection weighting approaches. The new computational method and results are useful for assessing normal and abnormal development of the structural connectome early in life.
</details></li>
</ul>
<hr>
<h2 id="Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection"><a href="#Performance-Comparison-and-Implementation-of-Bayesian-Variants-for-Network-Intrusion-Detection" class="headerlink" title="Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection"></a>Performance Comparison and Implementation of Bayesian Variants for Network Intrusion Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11834">http://arxiv.org/abs/2308.11834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tosin Ige, Christopher Kiekintveld</li>
<li>for: 本研究旨在实现和比较每种 variant of Bayesian classifier（多omial、Bernoulli、Gaussian）在网络入侵异常检测中的性能，并investigate whether there is any association between each variant assumption and their performance.</li>
<li>methods: 本研究采用了不同 variant of Bayesian classifier，并对它们的性能进行了比较。</li>
<li>results: 实验结果表明，Bernoulli variant 的准确率为 69.9%（71% 训练数据），Multinomial variant 的准确率为 31.2%（31.2% 训练数据），而 Gaussian variant 的准确率为 81.69%（82.84% 训练数据）。进一步调查发现，每种 Naive Bayes variant 的性能和准确率主要归功于它们的假设。Gaussian classifier 的好性能主要归功于它假设特征遵循正态分布，而 Multinomial classifier 的差的性能主要归功于它假设特征遵循离散和多omial分布。<details>
<summary>Abstract</summary>
Bayesian classifiers perform well when each of the features is completely independent of the other which is not always valid in real world application. The aim of this study is to implement and compare the performances of each variant of Bayesian classifier (Multinomial, Bernoulli, and Gaussian) on anomaly detection in network intrusion, and to investigate whether there is any association between each variant assumption and their performance. Our investigation showed that each variant of Bayesian algorithm blindly follows its assumption regardless of feature property, and that the assumption is the single most important factor that influences their accuracy. Experimental results show that Bernoulli has accuracy of 69.9% test (71% train), Multinomial has accuracy of 31.2% test (31.2% train), while Gaussian has accuracy of 81.69% test (82.84% train). Going deeper, we investigated and found that each Naive Bayes variants performances and accuracy is largely due to each classifier assumption, Gaussian classifier performed best on anomaly detection due to its assumption that features follow normal distributions which are continuous, while multinomial classifier have a dismal performance as it simply assumes discreet and multinomial distribution.
</details>
<details>
<summary>摘要</summary>
bayesian 分类器在实际应用中表现良好，只当每个特征独立无关其他特征时。本研究的目的是实现和比较bayesian分类器（多ategorical、bernoulli和gaussian）在网络侵入异常检测中的表现，以及每种variant assumption和其表现之间的关系。我们的调查表明，bayesian算法的每种变体都会遵循自己的假设，不管特征性质如何。实验结果显示，bernoulli的准确率为69.9%测试（71%训练），多ategorical的准确率为31.2%测试（31.2%训练），而gaussian的准确率为81.69%测试（82.84%训练）。进一步调查发现，每种naive bayes variant的表现和准确率主要归结于每个分类器假设，gaussian分类器在异常检测中表现最佳，因为它假设特征遵循正态分布，而多ategorical分类器表现很差，因为它只假设离散和多ategorical分布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test"><a href="#Exploring-the-Effectiveness-of-GPT-Models-in-Test-Taking-A-Case-Study-of-the-Driver’s-License-Knowledge-Test" class="headerlink" title="Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test"></a>Exploring the Effectiveness of GPT Models in Test-Taking: A Case Study of the Driver’s License Knowledge Test</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11827">http://arxiv.org/abs/2308.11827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saba Rahimi, Tucker Balch, Manuela Veloso</li>
<li>for: 提高 GPT 模型的问答能力，使其能够回答基于新的信息源的问题。</li>
<li>methods: 使用Context embedding、构建提问、使用 GPT 模型答题。</li>
<li>results: GPT-3 模型在一个控制测试场景中，使用加利福尼亚驾驶手册作为信息源，答题成功率为 96%，而无Context情况下答题成功率仅为 82%。但模型仍然无法正确回答一些问题，表明进一步改进是需要的。<details>
<summary>Abstract</summary>
Large language models such as Open AI's Generative Pre-trained Transformer (GPT) models are proficient at answering questions, but their knowledge is confined to the information present in their training data. This limitation renders them ineffective when confronted with questions about recent developments or non-public documents. Our research proposes a method that enables GPT models to answer questions by employing context from an information source not previously included in their training data. The methodology includes preprocessing of contextual information, the embedding of contexts and queries, constructing prompt through the integration of context embeddings, and generating answers using GPT models. We applied this method in a controlled test scenario using the California Driver's Handbook as the information source. The GPT-3 model achieved a 96% passing score on a set of 50 sample driving knowledge test questions. In contrast, without context, the model's passing score fell to 82%. However, the model still fails to answer some questions correctly even with providing library of context, highlighting room for improvement. The research also examined the impact of prompt length and context format, on the model's performance. Overall, the study provides insights into the limitations and potential improvements for GPT models in question-answering tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks"><a href="#Accel-GCN-High-Performance-GPU-Accelerator-Design-for-Graph-Convolution-Networks" class="headerlink" title="Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks"></a>Accel-GCN: High-Performance GPU Accelerator Design for Graph Convolution Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11825">http://arxiv.org/abs/2308.11825</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiexi1990/iccad-accel-gnn">https://github.com/xiexi1990/iccad-accel-gnn</a></li>
<li>paper_authors: Xi Xie, Hongwu Peng, Amit Hasan, Shaoyi Huang, Jiahui Zhao, Haowen Fang, Wei Zhang, Tong Geng, Omer Khan, Caiwen Ding</li>
<li>for: 提高GCNs在主流GPU上的加速，解决工作负荷不均和内存访问不规则的挑战。</li>
<li>methods: 使用轻量级度排序、块级别分区策略和共享内存地方的合并战略来提高分布式内存和计算并行性。</li>
<li>results: 对18个 benchmark图进行评估，与cuSPARSE、GNNAdvisor和graph-BLAST相比，Accel-GCN可以提高GCN的计算效率，提高多级内存效率，并且可以提高内存带宽利用率。<details>
<summary>Abstract</summary>
Graph Convolutional Networks (GCNs) are pivotal in extracting latent information from graph data across various domains, yet their acceleration on mainstream GPUs is challenged by workload imbalance and memory access irregularity. To address these challenges, we present Accel-GCN, a GPU accelerator architecture for GCNs. The design of Accel-GCN encompasses: (i) a lightweight degree sorting stage to group nodes with similar degree; (ii) a block-level partition strategy that dynamically adjusts warp workload sizes, enhancing shared memory locality and workload balance, and reducing metadata overhead compared to designs like GNNAdvisor; (iii) a combined warp strategy that improves memory coalescing and computational parallelism in the column dimension of dense matrices.   Utilizing these principles, we formulated a kernel for sparse matrix multiplication (SpMM) in GCNs that employs block-level partitioning and combined warp strategy. This approach augments performance and multi-level memory efficiency and optimizes memory bandwidth by exploiting memory coalescing and alignment. Evaluation of Accel-GCN across 18 benchmark graphs reveals that it outperforms cuSPARSE, GNNAdvisor, and graph-BLAST by factors of 1.17 times, 1.86 times, and 2.94 times respectively. The results underscore Accel-GCN as an effective solution for enhancing GCN computational efficiency.
</details>
<details>
<summary>摘要</summary>
图像卷积网络（GCNs）在各种领域中提取隐藏信息的缺省方法，但是它们在主流GPU上的加速受到工作负载不均和内存访问不规则的挑战。为了解决这些挑战，我们提出了Accel-GCN，一种为GCNs而设计的GPU加速架构。Accel-GCN的设计包括：（i）一种轻量级学位排序阶段，用于将节点按照学位相似度分组；（ii）一种基于块的分区策略，动态调整抗冲工作负载大小，提高共享内存本地性和工作负载均衡，并比GNNAdvisor等设计减少metadata开销；（iii）一种组合战略，用于改进内存启聚和计算并行性在纵向 dense 矩阵中。通过这些原则，我们设计了GCNs中的稀疏矩阵乘法（SpMM）的适用，并使用块级分区和组合战略。这种方法可以提高性能和多级内存效率，同时利用内存启聚和对齐来提高内存带宽。对Accel-GCN在18个标准图表上进行评估，发现它在cuSPARSE、GNNAdvisor和graph-BLAST等方法上提高了1.17倍、1.86倍和2.94倍的性能。结果证明Accel-GCN是GCN计算效率的有效解决方案。
</details></li>
</ul>
<hr>
<h2 id="PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification"><a href="#PatchBackdoor-Backdoor-Attack-against-Deep-Neural-Networks-without-Model-Modification" class="headerlink" title="PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification"></a>PatchBackdoor: Backdoor Attack against Deep Neural Networks without Model Modification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11822">http://arxiv.org/abs/2308.11822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xaiveryuan/patchbackdoor">https://github.com/xaiveryuan/patchbackdoor</a></li>
<li>paper_authors: Yizhen Yuan, Rui Kong, Shenghao Xie, Yuanchun Li, Yunxin Liu</li>
<li>for: 防止深度学习系统中的攻击，特别是在安全关键场景中。</li>
<li>methods: 提议使用一个特制的贴图（称为backdoor patch），将其放置在摄像头前面，并将其与输入图像一起传递给模型。</li>
<li>results: 在实验中，该攻击可以在93%到99%的场景中成功，并且在实际应用中也能够实现攻击。<details>
<summary>Abstract</summary>
Backdoor attack is a major threat to deep learning systems in safety-critical scenarios, which aims to trigger misbehavior of neural network models under attacker-controlled conditions. However, most backdoor attacks have to modify the neural network models through training with poisoned data and/or direct model editing, which leads to a common but false belief that backdoor attack can be easily avoided by properly protecting the model. In this paper, we show that backdoor attacks can be achieved without any model modification. Instead of injecting backdoor logic into the training data or the model, we propose to place a carefully-designed patch (namely backdoor patch) in front of the camera, which is fed into the model together with the input images. The patch can be trained to behave normally at most of the time, while producing wrong prediction when the input image contains an attacker-controlled trigger object. Our main techniques include an effective training method to generate the backdoor patch and a digital-physical transformation modeling method to enhance the feasibility of the patch in real deployments. Extensive experiments show that PatchBackdoor can be applied to common deep learning models (VGG, MobileNet, ResNet) with an attack success rate of 93% to 99% on classification tasks. Moreover, we implement PatchBackdoor in real-world scenarios and show that the attack is still threatening.
</details>
<details>
<summary>摘要</summary>
迷宫攻击是深度学习系统的主要威胁，它目的是在攻击者控制的情况下让神经网络模型发生不正常的行为。然而，大多数迷宫攻击都需要修改神经网络模型通过调询毒品数据和/或直接模型修改，这导致了一个常见而又错误的信念，即迷宫攻击可以轻松避免通过正确保护模型。在这篇论文中，我们显示了迷宫攻击可以不需要修改模型。相反的，我们提议在前置摄像头前置一个特别设计的贴纸（称为迷宫贴纸），这个贴纸会在输入图像中扮演一个正常的角色，但当输入图像包含攻击者控制的触发物时，它会制造错误的预测。我们的主要技术包括生成迷宫贴纸的有效训练方法和将贴纸模型转换为实际应用中的实际实现方法。实验结果显示，PatchBackdoor可以在常用的深度学习模型（VGG、MobileNet、ResNet）上获得93%~99%的攻击成功率，而且我们在实际应用中实现了这个攻击。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder"><a href="#Mitigating-Health-Disparity-on-Biased-Electronic-Health-Records-via-Deconfounder" class="headerlink" title="Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder"></a>Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11819">http://arxiv.org/abs/2308.11819</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Xiaohan Li, Philip Yu<br>for: The paper aims to address the fairness issue in clinical data modeling, particularly in Electronic Health Records (EHRs), by proposing a novel model called Fair Longitudinal Medical Deconfounder (FLMD).methods: FLMD employs a two-stage training process, which includes capturing unobserved confounders for each encounter and combining the learned latent representation with other relevant features to make predictions. The model incorporates appropriate fairness criteria, such as counterfactual fairness, to ensure high prediction accuracy while minimizing health disparities.results: The paper demonstrates the effectiveness of FLMD through comprehensive experiments on two real-world EHR datasets. The results show that FLMD outperforms baseline methods and FLMD variants in terms of fairness and accuracy, and provides valuable insights into its capabilities across different settings.<details>
<summary>Abstract</summary>
The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial for addressing the accuracy/fairness dilemma. In the second stage, FLMD combines the learned latent representation with other relevant features to make predictions. By incorporating appropriate fairness criteria, such as counterfactual fairness, FLMD ensures that it maintains high prediction accuracy while simultaneously minimizing health disparities. We conducted comprehensive experiments on two real-world EHR datasets to demonstrate the effectiveness of FLMD. Apart from the comparison of baseline methods and FLMD variants in terms of fairness and accuracy, we assessed the performance of all models on disturbed/imbalanced and synthetic datasets to showcase the superiority of FLMD across different settings and provide valuable insights into its capabilities.
</details>
<details>
<summary>摘要</summary>
“诊断资料模型中的公平问题，尤其是在电子健康记录（EHR）上，是非常重要的，因为EHR具有复杂的潜在结构和选择偏见。在实践中，通常需要实现健康差异化和模型绩效的平衡。然而，传统方法通常会面临精度和公平之间的贸易，因为它们无法捕捉潜在的因素。为了解决这个挑战，我们提出了一个新的模型，即公平长期医疗资料去掉条件（FLMD），旨在在长期EHR模型中同时维持精度和公平。参考了去掉条件理论，FLMD使用了两阶段训练过程。在第一阶段，FLMD捕捉了每次遇到的隐藏因素，这些隐藏因素代表了背后的医疗因素，例如患者基因和生活方式。这个隐藏因素是精度和公平问题的解决方案。在第二阶段，FLMD结合了学习的潜在表示和其他相关特征来进行预测。通过将应用公平问题的标准加入训练过程，FLMD确保了高精度和同时维持健康差异化。我们在两个真实世界EHR数据集上进行了充分的实验，以证明FLMD的有效性。除了与基eline方法和FLMD的变型进行公平和精度的比较外，我们还评估了所有模型在离散/不均衡和合成数据集上的表现，以显示FLMD在不同的设定下的优势和提供有价值的问题。”
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks"><a href="#Incorporating-Nonlocal-Traffic-Flow-Model-in-Physics-informed-Neural-Networks" class="headerlink" title="Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks"></a>Incorporating Nonlocal Traffic Flow Model in Physics-informed Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11818">http://arxiv.org/abs/2308.11818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Archie J. Huang, Animesh Biswas, Shaurya Agarwal</li>
<li>for: 提高交通状态估算精度，提高交通管理策略效果。</li>
<li>methods: 基于非本地LWR模型的物理学习深度学习框架。</li>
<li>results: 比基eline方法具有更高的准确率和可靠性，能够更好地估算交通状态，提高交通管理策略的效果。<details>
<summary>Abstract</summary>
This research contributes to the advancement of traffic state estimation methods by leveraging the benefits of the nonlocal LWR model within a physics-informed deep learning framework. The classical LWR model, while useful, falls short of accurately representing real-world traffic flows. The nonlocal LWR model addresses this limitation by considering the speed as a weighted mean of the downstream traffic density. In this paper, we propose a novel PIDL framework that incorporates the nonlocal LWR model. We introduce both fixed-length and variable-length kernels and develop the required mathematics. The proposed PIDL framework undergoes a comprehensive evaluation, including various convolutional kernels and look-ahead windows, using data from the NGSIM and CitySim datasets. The results demonstrate improvements over the baseline PIDL approach using the local LWR model. The findings highlight the potential of the proposed approach to enhance the accuracy and reliability of traffic state estimation, enabling more effective traffic management strategies.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "physics-informed deep learning" is translated as "物理学推动的深度学习" (wùyì xué yùn dào)* "nonlocal" is translated as "非本地" (fēn běn dì)* "LWR" is translated as "流速模型" (liú sù módel)* "speed" is translated as "速度" (zhòu du)* "density" is translated as "密度" (mì dè)* "kernel" is translated as "核函数" (fāng xiàng)* "look-ahead windows" is translated as "预测窗口" (yù jì chuāng kōng)* "baseline" is translated as "基线" (jī xiào)* "accuracy" is translated as "准确性" (zhèng qiú xìng)* "reliability" is translated as "可靠性" (kě liào xìng)* "traffic management strategies" is translated as "交通管理策略" (tiáo tòng guǎn lí zhì lüè)
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting"><a href="#Evaluation-of-Deep-Neural-Operator-Models-toward-Ocean-Forecasting" class="headerlink" title="Evaluation of Deep Neural Operator Models toward Ocean Forecasting"></a>Evaluation of Deep Neural Operator Models toward Ocean Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11814">http://arxiv.org/abs/2308.11814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ellery Rajagopal, Anantha N. S. Babu, Tony Ryu, Patrick J. Haley Jr., Chris Mirabito, Pierre F. J. Lermusiaux</li>
<li>for:  investigate the possible effectiveness of deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics</li>
<li>methods:  trained on a simulated two-dimensional fluid flow past a cylinder, and applied to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay</li>
<li>results:  predicted idealized periodic eddy shedding, and showed some skill in predicting features of realistic ocean surface flows, with potential for future research and applications.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究探讨了深度学习运算符模型在复现和预测古典流体流和真实海洋动力学中的可能效果。</li>
<li>methods: 使用了一个模拟的两维流体流 past a cylinder 进行训练，并应用于forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay。</li>
<li>results: 训练后的深度学习运算符模型能够预测理想化的 periodic eddy shedding，并对真实海洋表面流体中的一些特征显示了一定的能力，具有未来研究和应用的潜在价值。<details>
<summary>Abstract</summary>
Data-driven, deep-learning modeling frameworks have been recently developed for forecasting time series data. Such machine learning models may be useful in multiple domains including the atmospheric and oceanic ones, and in general, the larger fluids community. The present work investigates the possible effectiveness of such deep neural operator models for reproducing and predicting classic fluid flows and simulations of realistic ocean dynamics. We first briefly evaluate the capabilities of such deep neural operator models when trained on a simulated two-dimensional fluid flow past a cylinder. We then investigate their application to forecasting ocean surface circulation in the Middle Atlantic Bight and Massachusetts Bay, learning from high-resolution data-assimilative simulations employed for real sea experiments. We confirm that trained deep neural operator models are capable of predicting idealized periodic eddy shedding. For realistic ocean surface flows and our preliminary study, they can predict several of the features and show some skill, providing potential for future research and applications.
</details>
<details>
<summary>摘要</summary>
“数据驱动、深度学习模型框架最近已经开发出来用于时间序列预测。这些机器学习模型可能在多个领域有用，包括大气和海洋领域，以及整体更大的液体社区。本工作研究了这些深度神经算法模型在复现和预测经典液体流和现实海洋动力学的能力。我们首先简要评估了这些深度神经算法模型在模拟的二维液体流 past 筒体上的能力。然后，我们研究了它们在中大西洋盆地和马萨诸塞湾 ocean surface 流动预测方面的应用，学习从高分辨率数据吸收式 simulations 中得到的实际海洋实验。我们发现，训练过的深度神经算法模型能够预测理想化 periodic eddy shedding。对于实际的海洋表面流动和我们的初步研究，它们可以预测一些特征，并且显示一定的能力，提供未来研究和应用的潜在可能。”
</details></li>
</ul>
<hr>
<h2 id="Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings"><a href="#Ceci-n’est-pas-une-pomme-Adversarial-Illusions-in-Multi-Modal-Embeddings" class="headerlink" title="Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings"></a>Ceci n’est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11804">http://arxiv.org/abs/2308.11804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eugene Bagdasaryan, Vitaly Shmatikov<br>for: 这篇论文主要针对的是多modal编码器的安全性问题，具体来说是对于图像、声音、文本、视频等多种模式的映射。methods: 该论文使用了多modal编码器，将不同模式的输入映射到单一的嵌入空间中，以实现模式之间的对应。results: 该论文发现，使用多modal编码器可能会导致”对抗幻觉”攻击，即对于任意输入模式，恶意攻击者可以将其嵌入空间中的映射与另一个模式的映射很近，从而实现模式之间的对应。这种攻击不依赖于特定任务的知识，因此可以影响多种下游任务。使用ImageBind embeddings，研究者示出了这种攻击的具体实现方式，并证明了它们可以在图像生成、文本生成和零参数分类等多种任务中引起混乱。<details>
<summary>Abstract</summary>
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.   Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
</details>
<details>
<summary>摘要</summary>
多modalEncoder将图像、声音、文本、视频等多种模式映射到单一的嵌入空间中，使模式之间的表示相似（例如，将一个狗图片与一个喊喊的声音相关联）。我们表明，多modal嵌入可能受到“攻击ILLUSION”的威胁。给定任意模式的输入，恶意者可以对其进行扰动，使其嵌入 Close to恶意者选择的另一种模式的输入。这些ILLUSION依据嵌入空间的 proximity，并不受下游任务的限制。使用ImageBind嵌入，我们示例了如何通过不知道特定下游任务的知识，使用恶意对齐的输入， Mislead图像生成、文本生成和零shot分类。
</details></li>
</ul>
<hr>
<h2 id="Variational-Density-Propagation-Continual-Learning"><a href="#Variational-Density-Propagation-Continual-Learning" class="headerlink" title="Variational Density Propagation Continual Learning"></a>Variational Density Propagation Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11801">http://arxiv.org/abs/2308.11801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Angelini, Nidhal Bouaynaya, Ghulam Rasool<br>for: 这篇论文的目的是提出一个框架来适应资料分布迁移，并使用 uncertainty quantification from Bayesian Inference 来减少严重遗传。methods: 这篇论文使用了 Continual Learning 的方法，并开发了一个基于 Bayesian Inference 的方法，以便获得更好的 uncertainty quantification。这个方法不需要 Monte Carlo 抽样，而是使用关键矩阵来实现预测分布的近似。results: 这篇论文的结果显示，使用这个方法可以对抗严重遗传，并且可以在多个 benchmark 数据集上进行适应资料分布迁移。此外，这个方法还可以在多个不同的任务序列长度下进行任务增量学习。总之，这篇论文的结果显示，这个方法可以实现一个具有最小化模型复杂度的 Continual Learning 框架。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) deployed to the real world are regularly subject to out-of-distribution (OoD) data, various types of noise, and shifting conceptual objectives. This paper proposes a framework for adapting to data distribution drift modeled by benchmark Continual Learning datasets. We develop and evaluate a method of Continual Learning that leverages uncertainty quantification from Bayesian Inference to mitigate catastrophic forgetting. We expand on previous approaches by removing the need for Monte Carlo sampling of the model weights to sample the predictive distribution. We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers. Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task's variational posterior acting as the prior. Leveraging the approximation of the MDL principle, we aim to initially learn a sparse variational posterior and then minimize additional model complexity learned for subsequent tasks. Our approach is evaluated for the task incremental learning scenario using density propagated versions of fully-connected and convolutional neural networks across multiple sequential benchmark datasets with varying task sequence lengths. Ultimately, this procedure produces a minimally complex network over a series of tasks mitigating catastrophic forgetting.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在实际世界中部署时常遇到不同类型的噪音、扰动目标和数据分布逐渐变化的问题。这篇论文提出了一种基于Continual Learning benchmarck datasets的数据分布演变适应框架。我们开发了一种基于uncertainty量化的Continual Learning方法，利用束缚推理来减少忘却现象。我们在之前的方法上进一步改进，即不需要蒙те Carlo样本来采样模型权重，以便采样预测分布。我们优化了一个关闭式证据下界（ELBO）目标，通过对所有神经网络层传递mean和covariance两个分布的首两个矩阵来近似预测分布。通过使用关闭式ELBO目标，我们可以近似地使用最小描述长度（MDL）原理，从而自然地减少模型概率变化的KL散度，以避免忘却现象。我们利用近似MDL原理，首先学习一个稀疏的变量 posterior，然后进一步减少后续任务学习的模型复杂度。我们的方法在完全链接神经网络和卷积神经网络上进行了多个顺序 benchmark 数据集上进行了评估，并最终生成了一个对多个任务具有最小复杂度的神经网络，以避免忘却现象。
</details></li>
</ul>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
<li>for: 防止声音掩伤和音频深伪检测</li>
<li>methods: 使用复лекс值神经网络处理CQT频域表示的输入音频，保留相位信息，并允许使用可解释AI方法</li>
<li>results: 比前方法在”In-the-Wild”反伪 dataset上表现出色，并可以通过可解释AI方法解释结果，精度研究表明模型已经学习使用相位信息检测声音掩伤<details>
<summary>Abstract</summary>
Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the "In-the-Wild" anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.
</details>
<details>
<summary>摘要</summary>
当前的反假识别和音频深度质模型使用 either 大小图表（如CQT或Melspectrograms）或Raw audio 经过卷积或填充层处理。两种方法都有缺点：大小图表抛弃相位信息，影响音频自然性，而Raw-feature-based 模型不能使用传统的可解释 AI 方法。这篇论文提议一种新的方法，通过使用复杂值神经网络处理输入音频的复杂值CQT频域表示。这种方法保留相位信息，并允许使用可解释 AI 方法。结果表明这种方法在 "In-the-Wild" 反假识别数据集上表现出色，并且可以通过可解释 AI 方法解释结果。剥离学研究表明，模型已经学会使用相位信息检测声音假设。
</details></li>
</ul>
<hr>
<h2 id="Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics"><a href="#Karasu-A-Collaborative-Approach-to-Efficient-Cluster-Configuration-for-Big-Data-Analytics" class="headerlink" title="Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics"></a>Karasu: A Collaborative Approach to Efficient Cluster Configuration for Big Data Analytics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11792">http://arxiv.org/abs/2308.11792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Scheinert, Philipp Wiesner, Thorsten Wittkopp, Lauritz Thamsen, Jonathan Will, Odej Kao</li>
<li>for: 这篇论文的目的是提出一种更有效的资源配置评估方法，以便更好地选择适合的机器和集群大小，并且能够同时优化多个目标。</li>
<li>methods: 这篇论文使用了一种名为“Karasu”的方法，它通过聚合多个用户的执行时间资讯，将其转换为轻量级的性能模型，以便更好地搜索适合的资源配置。</li>
<li>results: 根据论文的评估结果，Karasu方法能够与现有的方法相比，在性能、搜索时间和成本等方面获得显著提升，甚至在仅具有部分相似特征的比较轻量级 Profiling 运行中也能够获得提升。<details>
<summary>Abstract</summary>
Selecting the right resources for big data analytics jobs is hard because of the wide variety of configuration options like machine type and cluster size. As poor choices can have a significant impact on resource efficiency, cost, and energy usage, automated approaches are gaining popularity. Most existing methods rely on profiling recurring workloads to find near-optimal solutions over time. Due to the cold-start problem, this often leads to lengthy and costly profiling phases. However, big data analytics jobs across users can share many common properties: they often operate on similar infrastructure, using similar algorithms implemented in similar frameworks. The potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored.   We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job.
</details>
<details>
<summary>摘要</summary>
We present Karasu, an approach to more efficient resource configuration profiling that promotes data sharing among users working with similar infrastructures, frameworks, algorithms, or datasets. Karasu trains lightweight performance models using aggregated runtime information of collaborators and combines them into an ensemble method to exploit inherent knowledge of the configuration search space. Moreover, Karasu allows the optimization of multiple objectives simultaneously. Our evaluation is based on performance data from diverse workload executions in a public cloud environment. We show that Karasu is able to significantly boost existing methods in terms of performance, search time, and cost, even when few comparable profiling runs are available that share only partial common characteristics with the target job. translate to Simplified Chinese:选择大数据分析任务的合适资源很Difficult，因为配置选项的多样性，如机器类型和集群大小。 incorrect choices can have a significant impact on resource efficiency, cost, and energy usage，所以自动化方法 becoming popular。most existing methods rely on profiling recurring workloads to find near-optimal solutions over time，but this often leads to lengthy and costly profiling phases due to the cold-start problem。 however，big data analytics jobs across users can share many common properties，such as operating on similar infrastructure，using similar algorithms implemented in similar frameworks。the potential in sharing aggregated profiling runs to collaboratively address the cold start problem is largely unexplored。我们提出了Karasu，一种更有效的资源配置 Profiling 方法，它推广用户工作在相似的基础设施，框架，算法或数据集之间的数据共享。Karasu 使用协作者的各种运行时信息 trains 轻量级性能模型，并将其组合成ensemble方法，以利用配置搜索空间的内在知识。此外，Karasu 允许同时优化多个目标。我们的评估基于公共云环境中的多种任务执行性能数据。我们表明，Karasu 能够 Significantly 提高现有方法的性能，搜索时间和成本，即使有 Only few comparable profiling runs are available that share only partial common characteristics with the target job。
</details></li>
</ul>
<hr>
<h2 id="HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials"><a href="#HypBO-Expert-Guided-Chemist-in-the-Loop-Bayesian-Search-for-New-Materials" class="headerlink" title="HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials"></a>HypBO: Expert-Guided Chemist-in-the-Loop Bayesian Search for New Materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11787">http://arxiv.org/abs/2308.11787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V. Gusev, Andrew I. Cooper</li>
<li>for: 本研究旨在使用人类专家知识来加速bayesian优化的搜索过程，以解决科学问题中的多变量问题。</li>
<li>methods: 本研究使用了 bayesian 优化方法，并利用专家理论来提供更好的初始样本，以优化搜索过程。</li>
<li>results: 研究结果表明，使用专家理论可以减少不重要的样本，并且可以更好地覆盖搜索空间，从而提高搜索效率。<details>
<summary>Abstract</summary>
Robotics and automation offer massive accelerations for solving intractable, multivariate scientific problems such as materials discovery, but the available search spaces can be dauntingly large. Bayesian optimization (BO) has emerged as a popular sample-efficient optimization engine, thriving in tasks where no analytic form of the target function/property is known. Here we exploit expert human knowledge in the form of hypotheses to direct Bayesian searches more quickly to promising regions of chemical space. Previous methods have used underlying distributions derived from existing experimental measurements, which is unfeasible for new, unexplored scientific tasks. Also, such distributions cannot capture intricate hypotheses. Our proposed method, which we call HypBO, uses expert human hypotheses to generate an improved seed of samples. Unpromising seeds are automatically discounted, while promising seeds are used to augment the surrogate model data, thus achieving better-informed sampling. This process continues in a global versus local search fashion, organized in a bilevel optimization framework. We validate the performance of our method on a range of synthetic functions and demonstrate its practical utility on a real chemical design task where the use of expert hypotheses accelerates the search performance significantly.
</details>
<details>
<summary>摘要</summary>
瑜珈和自动化技术可以为解决复杂多变量科学问题提供巨大的加速，但搜索空间可能会变得极其庞大。bayesian优化（BO）已经成为一种流行的高效搜索引擎，特别是在没有知道目标函数/属性的 analytic 表达的情况下。在这些任务中，我们利用专家人类知识来导向 bayesian 搜索更快速地访问有潜力的化学空间。先前的方法使用了基于现有实验测量的下面分布，这是对新、未探索的科学任务而言不可能的。此外，这些分布不能捕捉复杂的假设。我们提出的方法，即 HypBO，使用专家人类假设来生成改进的样本。不可能的样本会被排除，而有潜力的样本将被用来补充模型数据，从而实现更加有知识的搜索。这个过程会在全球与本地的搜索模式下进行，组织成两级优化框架。我们验证了我们的方法在一系列的synthetic函数上的性能，并在一个真实的化学设计任务中展示了它的实用性。在这个任务中，使用专家假设可以大幅度加速搜索过程。
</details></li>
</ul>
<hr>
<h2 id="Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers"><a href="#Coarse-to-Fine-Multi-Scene-Pose-Regression-with-Transformers" class="headerlink" title="Coarse-to-Fine Multi-Scene Pose Regression with Transformers"></a>Coarse-to-Fine Multi-Scene Pose Regression with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11783">http://arxiv.org/abs/2308.11783</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yolish/c2f-ms-transformer">https://github.com/yolish/c2f-ms-transformer</a></li>
<li>paper_authors: Yoli Shavit, Ron Ferens, Yosi Keller</li>
<li>for: 这个论文目的是学习多场景绝对摄像头pose regression。</li>
<li>methods: 这个论文使用了变换器学习模型，包括激活Map的汇集和特征转换，以及多个场景编码。</li>
<li>results: 该方法在常见的室内和室外数据集上进行评估，并显示出过单场景和多场景绝对摄像头pose regressor的性能。<details>
<summary>Abstract</summary>
Absolute camera pose regressors estimate the position and orientation of a camera given the captured image alone. Typically, a convolutional backbone with a multi-layer perceptron (MLP) head is trained using images and pose labels to embed a single reference scene at a time. Recently, this scheme was extended to learn multiple scenes by replacing the MLP head with a set of fully connected layers. In this work, we propose to learn multi-scene absolute camera pose regression with Transformers, where encoders are used to aggregate activation maps with self-attention and decoders transform latent features and scenes encoding into pose predictions. This allows our model to focus on general features that are informative for localization, while embedding multiple scenes in parallel. We extend our previous MS-Transformer approach \cite{shavit2021learning} by introducing a mixed classification-regression architecture that improves the localization accuracy. Our method is evaluated on commonly benchmark indoor and outdoor datasets and has been shown to exceed both multi-scene and state-of-the-art single-scene absolute pose regressors.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本翻译成简化中文。<</SYS>>绝对摄像头姿态回归器可以根据捕捉的图像估计摄像头的位置和方向。通常，一个卷积减少器（Convolutional Backbone）和多层感知器（Multi-layer Perceptron，MLP）头被训练使用图像和姿态标签来嵌入单个参考场景。在最近的研究中，这种方案被扩展以学习多个场景，通过取代MLP头而使用完全连接层。在这项工作中，我们提议使用变换器来学习多个场景绝对摄像头姿态回归，其中混合encoder和decoder被用来聚合活动地图和场景编码，并将其转换成姿态预测。这使得我们的模型能够专注于通用的特征，同时并行地嵌入多个场景。我们在前一项MS-Transformer方法（\cite{shavit2021learning}）的基础上增加了混合分类回归架构，以提高地点准确性。我们的方法在常见的室内和室外数据集上进行评估，并已经超过了多个场景和当前最佳单个场景绝对摄像头姿态回归器。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables"><a href="#Addressing-Dynamic-and-Sparse-Qualitative-Data-A-Hilbert-Space-Embedding-of-Categorical-Variables" class="headerlink" title="Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables"></a>Addressing Dynamic and Sparse Qualitative Data: A Hilbert Space Embedding of Categorical Variables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11781">http://arxiv.org/abs/2308.11781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirban Mukherjee, Hannah H. Chang</li>
<li>for: 该论文旨在扩展量化模型，以便包含 качеitative数据进行 causal 估计。</li>
<li>methods: 该论文使用函数分析创建了一个更加灵活和弹性的框架，将观察到的类别嵌入到了一个Baire空间中，并通过一个连续线性映射将这些类别映射到一个 reproduce kernel Hilbert space（RKHS）中。</li>
<li>results: 该论文通过实验证明了其超越传统方法的性能，特别在qualitative信息复杂和细腻的场景下。<details>
<summary>Abstract</summary>
We propose a novel framework for incorporating qualitative data into quantitative models for causal estimation. Previous methods use categorical variables derived from qualitative data to build quantitative models. However, this approach can lead to data-sparse categories and yield inconsistent (asymptotically biased) and imprecise (finite sample biased) estimates if the qualitative information is dynamic and intricate. We use functional analysis to create a more nuanced and flexible framework. We embed the observed categories into a latent Baire space and introduce a continuous linear map -- a Hilbert space embedding -- from the Baire space of categories to a Reproducing Kernel Hilbert Space (RKHS) of representation functions. Through the Riesz representation theorem, we establish that the canonical treatment of categorical variables in causal models can be transformed into an identified structure in the RKHS. Transfer learning acts as a catalyst to streamline estimation -- embeddings from traditional models are paired with the kernel trick to form the Hilbert space embedding. We validate our model through comprehensive simulation evidence and demonstrate its relevance in a real-world study that contrasts theoretical predictions from economics and psychology in an e-commerce marketplace. The results confirm the superior performance of our model, particularly in scenarios where qualitative information is nuanced and complex.
</details>
<details>
<summary>摘要</summary>
我们提出一种新的框架，用于将质量数据 incorporated 到量化模型中进行 causal 估计。先前的方法使用来自质量数据的分类变量来构建量化模型，但这种方法可能会导致数据缺乏和偏向（即不稳定和偏差）的估计结果，特别是当质量信息是动态且复杂时。我们使用函数分析来创建一个更加灵活和细腻的框架。我们将观察到的分类 embedding 到一个 latent Baire 空间中，并引入一个连续线性映射——一个 Reproducing Kernel Hilbert Space (RKHS) 的表示函数空间中的映射。通过 Riesz 表示定理，我们证明了在 causal 模型中对分类变量的 canonical 处理可以转化为一个唯一标识结构在 RKHS 中。trasnfer learning 作为一种 catalyst，我们可以通过对传统模型的 embeddings 与kernel trick 结合来形成一个 Hilbert 空间 embedding。我们通过了广泛的 simulations 证明和一个实际的例子，证明了我们的模型在复杂和细腻的质量信息场景下表现更加优秀。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning"><a href="#Few-shot-Anomaly-Detection-in-Text-with-Deviation-Learning" class="headerlink" title="Few-shot Anomaly Detection in Text with Deviation Learning"></a>Few-shot Anomaly Detection in Text with Deviation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11780">http://arxiv.org/abs/2308.11780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anindya Sundar Das, Aravind Ajay, Sriparna Saha, Monowar Bhuyan<br>for: 这种论文的目的是提出一种基于深度几个示例学习的方法，用于检测文本中的异常Example。methods: 该方法使用了几个示例学习的技术，包括异常示例的有限使用、直接学习异常分数和多头自我注意力层。results: 对多个 benchmark 数据集进行了广泛的实验，并达到了新的状态态的性能水平。<details>
<summary>Abstract</summary>
Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
现有的异常检测方法大多数都是基于无标注数据构建模型。这些模型假设没有异常示例，这限制了它们使用异常示例的先验知识，从而导致它们在许多实际应用中表现不佳。此外，这些模型更关注学习特征嵌入than直接优化异常分数，这可能会导致异常分数评估不准确和数据学习过程中的不efficient使用。在本文中，我们介绍了FATE，一种深度几个shot学习基于框架，它利用有限异常示例和直接学习异常分数的端到端方法。在这种方法中，正常示例的异常分数被调整，以便与先前分布中的参考分数相似。相反，异常示例被迫有异常分数，这些分数与参考分数在上 tail 的异常分布中异常大。此外，我们的模型利用多头自注意力层和多个实例学习方法来学习异常的特殊行为。我们在多个标准 benchmark 数据集上进行了广泛的实验，结果表明，我们的提议方法可以达到新的顶峰性能水平。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Hessian-Alignment-for-Domain-Generalization"><a href="#Understanding-Hessian-Alignment-for-Domain-Generalization" class="headerlink" title="Understanding Hessian Alignment for Domain Generalization"></a>Understanding Hessian Alignment for Domain Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11778">http://arxiv.org/abs/2308.11778</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huawei-noah/federated-learning">https://github.com/huawei-noah/federated-learning</a></li>
<li>paper_authors: Sobhan Hemati, Guojun Zhang, Amir Estiri, Xi Chen<br>for: 这 paper 是关于 out-of-distribution (OOD) 泛化的研究，旨在提高深度学习模型在各种实际应用场景中的 OOD 泛化能力。methods: 这 paper 使用了 gradient-based 正则化技术来提高 OOD 泛化能力，并分析了 Hessian 和 gradient 在领域泛化中的角色。results: 这 paper 的结果表明，将分类器的头 Hessian 矩阵和梯度进行对齐可以提高 OOD 泛化能力，并且提出了两种简单 yet effective 的方法来实现对齐。这些方法在不同的 OOD 场景中都显示了出色的性能。<details>
<summary>Abstract</summary>
Out-of-distribution (OOD) generalization is a critical ability for deep learning models in many real-world scenarios including healthcare and autonomous vehicles. Recently, different techniques have been proposed to improve OOD generalization. Among these methods, gradient-based regularizers have shown promising performance compared with other competitors. Despite this success, our understanding of the role of Hessian and gradient alignment in domain generalization is still limited. To address this shortcoming, we analyze the role of the classifier's head Hessian matrix and gradient in domain generalization using recent OOD theory of transferability. Theoretically, we show that spectral norm between the classifier's head Hessian matrices across domains is an upper bound of the transfer measure, a notion of distance between target and source domains. Furthermore, we analyze all the attributes that get aligned when we encourage similarity between Hessians and gradients. Our analysis explains the success of many regularizers like CORAL, IRM, V-REx, Fish, IGA, and Fishr as they regularize part of the classifier's head Hessian and/or gradient. Finally, we propose two simple yet effective methods to match the classifier's head Hessians and gradients in an efficient way, based on the Hessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), and without directly calculating Hessians. We validate the OOD generalization ability of proposed methods in different scenarios, including transferability, severe correlation shift, label shift and diversity shift. Our results show that Hessian alignment methods achieve promising performance on various OOD benchmarks. The code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.
</details>
<details>
<summary>摘要</summary>
外部分布（OOD）泛化是深度学习模型在实际应用场景中的重要能力之一，包括医疗和自动驾驶等。最近，不同的技术被提出来提高OOD泛化。其中，梯度基本的正则化方法表现出色，并且我们对域泛化中梯度和梯度对的角色的理解仍然受限。为了解决这个缺陷，我们通过最近的OOD理论来分析域泛化中梯度和梯度对的角色。我们 teorically 表明，在目标域与源域之间的传输度量上，梯度和梯度对的spectral norm是Upper bound。此外，我们还分析了梯度和梯度对相互对齐的所有特征。我们的分析解释了许多正则化器，如CORAL、IRM、V-REx、Fish、IGA和Fishr的成功，他们都在域泛化中对梯度和梯度对进行了正则化。最后，我们提出了两种简单 yet efficient的方法来匹配梯度和梯度对，基于梯度和梯度对的乘积（HGP）和欧几里得（Hutchinson）的方法，而不需直接计算梯度。我们 validate了我们提出的方法的OOD泛化能力在不同的场景中，包括传输性、严重相关转移、标签转移和多样性转移。我们的结果表明，梯度对齐方法在不同的OOD测试上表现出色。代码可以在 <https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment> 中找到。
</details></li>
</ul>
<hr>
<h2 id="3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network"><a href="#3ET-Efficient-Event-based-Eye-Tracking-using-a-Change-Based-ConvLSTM-Network" class="headerlink" title="3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network"></a>3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11771">http://arxiv.org/abs/2308.11771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinyu Chen, Zuowen Wang, Shih-Chii Liu, Chang Gao</li>
<li>for: 这 paper 是为了提出一种 sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) 模型，用于事件基于眼动跟踪，这种技术将被应用于下一代智能眼镜等设备中。</li>
<li>methods: 这 paper 使用了 retina-inspired event cameras，即具有低延迟响应和稀疏输出事件流的摄像头。 authors 还使用了 delta-encoded recurrent path，以提高活化稀疏性，从而降低计算量。</li>
<li>results: 这 paper 的 CB-ConvLSTM 架构可以高效地提取事件流中的 spatial-temporal 特征，用于眼动跟踪。 compared to conventional CNN 结构，CB-ConvLSTM 可以减少计算量约 4.7倍，不会失去准确性。 这种增强的效率使得其适用于实时眼动跟踪。<details>
<summary>Abstract</summary>
This paper presents a sparse Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) model for event-based eye tracking, key for next-generation wearable healthcare technology such as AR/VR headsets. We leverage the benefits of retina-inspired event cameras, namely their low-latency response and sparse output event stream, over traditional frame-based cameras. Our CB-ConvLSTM architecture efficiently extracts spatio-temporal features for pupil tracking from the event stream, outperforming conventional CNN structures. Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7$\times$ without losing accuracy when tested on a \texttt{v2e}-generated event dataset of labeled pupils. This increase in efficiency makes it ideal for real-time eye tracking in resource-constrained devices. The project code and dataset are openly available at \url{https://github.com/qinche106/cb-convlstm-eyetracking}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data"><a href="#Patient-Clustering-via-Integrated-Profiling-of-Clinical-and-Digital-Data" class="headerlink" title="Patient Clustering via Integrated Profiling of Clinical and Digital Data"></a>Patient Clustering via Integrated Profiling of Clinical and Digital Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11748">http://arxiv.org/abs/2308.11748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongjin Choi, Andy Xiang, Ozgur Ozturk, Deep Shrestha, Barry Drake, Hamid Haidarian, Faizan Javed, Haesun Park</li>
<li>for: 这篇论文是为了开发一个基于patient profiling的健康照顾 clustering模型，以提高健康照顾中的患者分组和推荐能力。</li>
<li>methods: 这个模型使用一种基于受限制的低矩降降推导法，利用患者的临床数据和数位互动数据（包括搜寻和浏览），创建患者 profil。这 leads to nonnegative embedding vectors, which serve as a low-dimensional representation of the patients.</li>
<li>results: 评估过real-world patient data from a healthcare web portal， compared to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.<details>
<summary>Abstract</summary>
We introduce a novel profile-based patient clustering model designed for clinical data in healthcare. By utilizing a method grounded on constrained low-rank approximation, our model takes advantage of patients' clinical data and digital interaction data, including browsing and search, to construct patient profiles. As a result of the method, nonnegative embedding vectors are generated, serving as a low-dimensional representation of the patients. Our model was assessed using real-world patient data from a healthcare web portal, with a comprehensive evaluation approach which considered clustering and recommendation capabilities. In comparison to other baselines, our approach demonstrated superior performance in terms of clustering coherence and recommendation accuracy.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的基于 Profile 的患者划分模型，适用于医疗数据。我们的模型利用基于受限制的低级数据减少法，使用患者的临床数据和数字互动数据（包括浏览和搜索）构建患者profile。这种方法生成了非负嵌入 вектор，用于表示患者。我们的模型在使用真实世界患者数据from a healthcare web portal进行评估，并对划分和推荐能力进行全面评估。与其他基线相比，我们的方法在划分准确性和推荐准确性方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape"><a href="#Animal3D-A-Comprehensive-Dataset-of-3D-Animal-Pose-and-Shape" class="headerlink" title="Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape"></a>Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11737">http://arxiv.org/abs/2308.11737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang, Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen Cui, Alan Yuille, Adam Kortylewski</li>
<li>for: 研究动物3D姿势和形状估计，以解释动物行为，并可能帮助多个下游应用，如野生动物保育。</li>
<li>methods: 我们提出了 Animal3D 资料集，包括 3379 幅照片和 40 种哺乳类动物的高品质26个关键点标注。</li>
<li>results: 我们在 Animal3D 资料集上进行了代表性的形状和姿势估计模型评估，包括 supervised 学习、synthetic to real transfer 和 fine-tuning human pose 和形状估计模型。我们的实验结果显示，预测动物这些种类中的3D形状和姿势仍然是一个非常具有挑战性的任务，尽管人类姿势估计方法有了重要的进步。<details>
<summary>Abstract</summary>
Accurately estimating the 3D pose and shape is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. However, research in this area is held back by the lack of a comprehensive and diverse dataset with high-quality 3D pose and shape annotations. In this paper, we propose Animal3D, the first comprehensive dataset for mammal animal 3D pose and shape estimation. Animal3D consists of 3379 images collected from 40 mammal species, high-quality annotations of 26 keypoints, and importantly the pose and shape parameters of the SMAL model. All annotations were labeled and checked manually in a multi-stage process to ensure highest quality results. Based on the Animal3D dataset, we benchmark representative shape and pose estimation models at: (1) supervised learning from only the Animal3D data, (2) synthetic to real transfer from synthetically generated images, and (3) fine-tuning human pose and shape estimation models. Our experimental results demonstrate that predicting the 3D shape and pose of animals across species remains a very challenging task, despite significant advances in human pose estimation. Our results further demonstrate that synthetic pre-training is a viable strategy to boost the model performance. Overall, Animal3D opens new directions for facilitating future research in animal 3D pose and shape estimation, and is publicly available.
</details>
<details>
<summary>摘要</summary>
正确估算动物3D姿态和形状是研究动物行为的重要步骤，可能对野生动物保护和其他下游应用具有巨大的应用前景。然而，这个领域的研究受到缺乏完整和多样化的3D姿态和形状标注数据的限制。在这篇论文中，我们提出了动物3D（Animal3D），第一个包括40种哺乳动物的完整数据集，以及高品质的26个关键点标注。这些标注都是透过多阶段的手动标注和确认程序来确保最高品质的结果。基于动物3D数据集，我们在：（1）仅使用动物3D数据进行监督学习，（2）从生成的 sintetic 图像进行转换到真实图像，以及（3）调整人体姿态和形状估算模型的表现进行比较。我们的实验结果显示，预测动物这些种类的3D姿态和形状仍然是一个非常困难的任务，尽管人体姿态估算领域已经取得了重大进步。我们的结果还显示，从 sintetic 预训成功地增强模型性能。总的来说，动物3D开启了新的研究方向，并且公开 disponibile。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering"><a href="#Knowledge-Graph-Prompting-for-Multi-Document-Question-Answering" class="headerlink" title="Knowledge Graph Prompting for Multi-Document Question Answering"></a>Knowledge Graph Prompting for Multi-Document Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11730">http://arxiv.org/abs/2308.11730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Wang, Nedim Lipka, Ryan A. Rossi, Alexa Siu, Ruiyi Zhang, Tyler Derr</li>
<li>for: 这个研究旨在提高大语言模型（LLM）在多文档问答（MD-QA）中的表现，并 explore the “pre-train, prompt, predict”  paradigm in MD-QA.</li>
<li>methods: 这个研究提出了一种知识图提示（KGP）方法，包括一个知识图建构模块和一个知识图游走模块。知识图建构模块使用多个文档中的节点和边来表示文档之间的semantic和lexical相似性，而知识图游走模块使用LM响应来导航知识图，并收集支持答案的文档段落。</li>
<li>results: 实验结果表明，KGP方法可以提高LLM在MD-QA中的表现，并且可以减少检索时间。这种方法的实现可以在<a target="_blank" rel="noopener" href="https://github.com/YuWVandy/KG-LLM-MDQA%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/YuWVandy/KG-LLM-MDQA中找到。</a><details>
<summary>Abstract</summary>
The 'pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in the scenario of multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is at https://github.com/YuWVandy/KG-LLM-MDQA.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的“预训练、提示、预测”模式在开放预测问答（OD-QA）中实现了很大的成功。然而，很少的研究探讨这种模式在多文档问答（MD-QA）中的应用， MD-QA 是需要对不同文档内容和结构的理解，以便更好地回答问题。为了填补这一重要的漏洞，我们提出了知识图 prompting（KGP）方法，用于在 LLM 中提示 MD-QA，该方法包括知识图构建模块和知识图游走模块。For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations.For graph traversal, we design an LM-guided graph traverser that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the LM-guided traverser acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality.Exhaustive experiments demonstrate the effectiveness of KGP for MD-QA, indicating the potential of leveraging graphs in enhancing the prompt design for LLMs. Our code is available at <https://github.com/YuWVandy/KG-LLM-MDQA>.
</details></li>
</ul>
<hr>
<h2 id="When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making"><a href="#When-Are-Two-Lists-Better-than-One-Benefits-and-Harms-in-Joint-Decision-making" class="headerlink" title="When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making"></a>When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11721">http://arxiv.org/abs/2308.11721</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kpdonahue/benefits_harms_joint_decision_making">https://github.com/kpdonahue/benefits_harms_joint_decision_making</a></li>
<li>paper_authors: Kate Donahue, Kostas Kollias, Sreenivas Gollapudi</li>
<li>for: 这种研究是为了优化人机算联合表现的最佳方式。</li>
<li>methods: 本研究使用了一种特定的人机算合作方式，其中算法有一组ITEMS，并将其中的一个subsetSize&#x3D;k个Item展示给人，人将选择最终的Item。这种方式可以应用于内容推荐、路径规划等任务。因为人和算法都有不准确的信息，因此关键问题是：哪个值得最大化最终选择最佳Item的概率？</li>
<li>results: 研究发现，在某些噪声模型下，最佳的$k$值在[2, n-1]之间，即在人机算合作下有着绝对的优势。然而，当人被anchor在算法提供的排序顺序时，联合系统的表现一定是差。此外，研究还发现在人机准确性水平不同时，存在一些情况下，一个更准确的代理会受益于与一个 menos准确的代理合作。<details>
<summary>Abstract</summary>
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating, even when the human and algorithm have equal accuracy separately. We demonstrate this theoretically for the Mallows model and experimentally for the Random Utilities models of noisy permutations. However, we show this pattern is reversed when the human is anchored on the algorithm's presented ordering - the joint system always has strictly worse performance. We extend these results to the case where the human and algorithm differ in their accuracy levels, showing that there always exist regimes where a more accurate agent would strictly benefit from collaborating with a less accurate one, but these regimes are asymmetric between the human and the algorithm's accuracy.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:历史上，许多机器学习研究都集中在算法性能上，但最近更多的关注集中在人机合作性能。在这种人机合作中，算法可以访问一个集合中的 $n$ 个项目，并将其中的一个 subset 大小为 $k$ 项显示给人类，人类将选择最终的项目。这种场景可以模型内容推荐、路径规划或任何类型的标签任务。由于人类和算法都有不准确、噪声的信息关于真实的项目顺序，关键问题是：哪个值的 $k$ 最大化人类最终选择的最佳项目的概率？ For $k=1$, 性能是由算法 acting alone 优化的，而 For $k=n$ 是由人类 acting alone 优化的。意外地，我们发现在多种噪声模型下，最佳的 $k $ 是 $[2, n-1]$ 的Interval -  то есть，在人类和算法都有等准确级别时，存在着协同的益处，即使人类和算法的准确率都是等于的。我们在 Mallows 模型和 Random Utilities 模型中证明这一结论，并通过实验证明这一结论。然而，我们发现在人类被算法的显示顺序固定时，人机合作系统总是有固定性下降的性能问题。我们扩展这些结论到人类和算法准确级别不同的情况下，并证明在某些情况下，更准确的代理人会受益于和更准确的算法合作。然而，这些情况是人类和算法准确级别之间的偏好的。
</details></li>
</ul>
<hr>
<h2 id="Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection"><a href="#Collect-Measure-Repeat-Reliability-Factors-for-Responsible-AI-Data-Collection" class="headerlink" title="Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection"></a>Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oana Inel, Tim Draws, Lora Aroyo</li>
<li>for: 本研究旨在提高人工智能（AI）数据收集过程中的质量和可靠性，以便提高AI模型的公平性和可靠性。</li>
<li>methods: 本研究提出了一种负责任AI（RAI）方法，用于系统地对数据收集过程中的因素进行深入分析，以便评估数据的内部可靠性和外部稳定性。</li>
<li>results: 研究人员对9个现有数据集和注释任务进行了验证，并在四种内容模式上进行了检验。结果表明，RAI方法可以帮助评估数据的稳定性和可靠性，并且可以处理公平和责任的方面在数据收集中的问题。<details>
<summary>Abstract</summary>
The rapid entry of machine learning approaches in our daily activities and high-stakes domains demands transparency and scrutiny of their fairness and reliability. To help gauge machine learning models' robustness, research typically focuses on the massive datasets used for their deployment, e.g., creating and maintaining documentation for understanding their origin, process of development, and ethical considerations. However, data collection for AI is still typically a one-off practice, and oftentimes datasets collected for a certain purpose or application are reused for a different problem. Additionally, dataset annotations may not be representative over time, contain ambiguous or erroneous annotations, or be unable to generalize across issues or domains. Recent research has shown these practices might lead to unfair, biased, or inaccurate outcomes. We argue that data collection for AI should be performed in a responsible manner where the quality of the data is thoroughly scrutinized and measured through a systematic set of appropriate metrics. In this paper, we propose a Responsible AI (RAI) methodology designed to guide the data collection with a set of metrics for an iterative in-depth analysis of the factors influencing the quality and reliability} of the generated data. We propose a granular set of measurements to inform on the internal reliability of a dataset and its external stability over time. We validate our approach across nine existing datasets and annotation tasks and four content modalities. This approach impacts the assessment of data robustness used for AI applied in the real world, where diversity of users and content is eminent. Furthermore, it deals with fairness and accountability aspects in data collection by providing systematic and transparent quality analysis for data collections.
</details>
<details>
<summary>摘要</summary>
machine learning 技术在我们日常生活和高度关键领域的快速进入需要透明度和检查其公平和可靠性。为了评估机器学习模型的可靠性，研究通常集中在部署之前的庞大数据集上，例如创建和维护这些数据集的文档，以便理解它们的起源、开发过程和伦理考虑。然而，AI数据收集仍然是一项一次性的做法，而且经常 reuse datasets 用于不同的问题或应用。此外，数据集的标注可能不具有时间的普适性，包含歧义或错误的标注，或者无法泛化到问题或领域。 latest research 表明这些做法可能会导致不公正、偏见或不准确的结果。我们认为AI数据收集应该进行负责任的方式，即在数据收集过程中评估和测量数据的质量，使用一套系统的精细度度量。在这篇论文中，我们提出了一种负责任AI（RAI）方法，用于指导数据收集，并提供了一系列适用于不同数据集和标注任务的度量。我们验证了我们的方法在九个现有数据集和标注任务中，以及四种内容模式中。这种方法对实际应用中的AI数据Robustness进行评估，并处理了公平和责任方面的问题。
</details></li>
</ul>
<hr>
<h2 id="SuperCalo-Calorimeter-shower-super-resolution"><a href="#SuperCalo-Calorimeter-shower-super-resolution" class="headerlink" title="SuperCalo: Calorimeter shower super-resolution"></a>SuperCalo: Calorimeter shower super-resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11700">http://arxiv.org/abs/2308.11700</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ian-pang/supercalo">https://github.com/ian-pang/supercalo</a></li>
<li>paper_authors: Ian Pang, John Andrew Raine, David Shih</li>
<li>for:  overcome the challenge of calorimeter shower simulation in the Large Hadron Collider computational pipeline</li>
<li>methods:  employ deep-generative surrogate models, specifically a flow-based super-resolution model called SuperCalo</li>
<li>results:  high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers, reducing computational cost, memory requirements, and generation time<details>
<summary>Abstract</summary>
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
</details>
<details>
<summary>摘要</summary>
喷泉计数器模拟是大型夸克粒子加速器计算管道中的主要瓶颈。近年来，有很多努力使用深度生成器模型来突破这个挑战。然而，许多最佳性能的模型培训和生成时间不能扩展到高维喷泉计数器。在这项工作中，我们介绍SuperCalo，一种流基的超分辨模型，并证明了高维细腔喷泉可以快速升sample自粗腔喷泉。这种新的方法可以减少计算成本、内存需求和生成时间相关于快速喷泉计数器模型。此外，我们表明了升sample后的喷泉具有高度的变化度。这意味着可以从少量粗腔喷泉中生成大量高维喷泉，具有高准确性，从而再次减少生成时间。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Benchmarking-of-Language-Models"><a href="#Efficient-Benchmarking-of-Language-Models" class="headerlink" title="Efficient Benchmarking (of Language Models)"></a>Efficient Benchmarking (of Language Models)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11696">http://arxiv.org/abs/2308.11696</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sumankrsh/Sentiment-Analysis.ipynb">https://github.com/sumankrsh/Sentiment-Analysis.ipynb</a></li>
<li>paper_authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen</li>
<li>for: 本研究旨在解决语言模型评估中的效率问题，提出了一种智能减少语言模型评估计算成本的方法，以减少计算成本而不影响可靠性。</li>
<li>methods: 本研究使用了HELMbenchmark作为测试 caso，研究了不同的benchmark设计选择对计算vs可靠性的贸易OFF。提出了一种新的度量指标DIoR来评估决策对可靠性的影响。</li>
<li>results: 研究发现，现有的领导者在HELMbenchmark可以通过移除一些低排名的模型来改变排名，而且只需很少的例子即可获得正确的排名。同时，不同的HELM场景选择会导致排名差异很大。根据研究结果，提出了一些具体的建议，以减少计算成本并且保持可靠性，可以实现计算成本减少x100或更多。<details>
<summary>Abstract</summary>
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark ranking. Conversely a slightly different choice of HELM scenarios varies ranking widely. Based on our findings we outline a set of concrete recommendations for more efficient benchmark design and utilization practices leading to dramatic cost savings with minimal loss of benchmark reliability often reducing computation by x100 or more.
</details>
<details>
<summary>摘要</summary>
LM模型的多样化性带来了一新类的评价指标，这些指标涵盖了各种能力的广泛评估。然而，这些评价努力的效率方面几乎没有在文献中得到了讨论。在这项工作中，我们提出了一个问题，即如何智能减少LM评价的计算成本，无需妥协可靠性。使用HELM benchmark作为测试 caso，我们研究了不同的评价指标设计选择对计算与可靠性之间的负面影响。我们提出了一个新的度量器，即决策影响可靠性（DIoR），用于评估这些决策的可靠性。我们发现，例如，现有领先者在HELM上可能会改变，只需要从benchmark中移除一个低排名的模型即可。同时，我们发现一些不同的HELM场景可以导致评价排名差异极大。基于我们的发现，我们提出了一些具体的建议，包括更有效的评价设计和使用实践，可以实现计算成本减少100倍或更多，而且减少的成本幅度与可靠性损失之间的关系也可以得到精细控制。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Multi-Resolution-Communications"><a href="#Semantic-Multi-Resolution-Communications" class="headerlink" title="Semantic Multi-Resolution Communications"></a>Semantic Multi-Resolution Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11604">http://arxiv.org/abs/2308.11604</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matin Mortaheb, Mohammad A. Amir Khojastepour, Srimat T. Chakradhar, Sennur Ulukus</li>
<li>for: 这种深度学习基于JSCC的框架是为了提高数据重建的性能，特别是在finite block-length数据下，SSCC表现下降。此外，SSCC还无法在多用户和&#x2F;或多分辨率下进行数据重建，因为它只是为worst channel和&#x2F;或最高质量数据进行优化。</li>
<li>methods: 我们提出了一种基于MTL的深度学习多分辨率JSCC框架，通过层次结构来编码数据，并通过当前和过去层编码数据来进行解码。此外，这种框架还可以应用于semantic通信，其目标是保留特定semantic attribute。</li>
<li>results: 我们在MNIST和CIFAR10 dataset上进行实验，并证明了我们的提出的方法可以在不同分辨率下重建数据，并且可以在 successive layers中提高semantic classifier的准确性。这种能力特别有用于优先保留数据集中的关键semantic attribute。<details>
<summary>Abstract</summary>
Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. These semantic features could be crucial elements such as class labels, essential for classification tasks, or other key attributes that require preservation. Within this framework, each level of encoded data can be carefully designed to retain specific data semantics. As a result, the precision of a semantic classifier can be progressively enhanced across successive layers, emphasizing the preservation of targeted semantics throughout the encoding and decoding stages. We conduct experiments on MNIST and CIFAR10 dataset. The experiment with both datasets illustrates that our proposed method is capable of surpassing the SSCC method in reconstructing data with different resolutions, enabling the extraction of semantic features with heightened confidence in successive layers. This capability is particularly advantageous for prioritizing and preserving more crucial semantic features within the datasets.
</details>
<details>
<summary>摘要</summary>
深度学习基于联合源通道编码（JSCC）已经实现了数据重建的显著进步，比单独源通道编码（SSCC）更好。这种超越来自于finite block length数据下SSCC的优化不足。此外，SSCC无法在多用户和/或多分辨率上重建数据，因为它只是尝试满足最差通道和/或最高质量数据。为了超越这些限制，我们提议了一种基于多任务学习（MTL）的深度学习多分辨率JSCC框架。这个提议的框架通过层次结构来编码数据，并通过同当前和过去层编码数据来有效地解码。此外，这个框架具有潜在的semantic communication功能，其目标超出了数据重建，是保留特定semantic attribute的。在这个框架中，每个层的编码数据都可以被优化，以保留特定数据 semantics。因此，在successive层中，精度的semantic classifier可以进一步提高，强调在编码和解码过程中保留目标semantic attribute。我们在MNIST和CIFAR10 dataset上进行了实验，实验结果表明，我们的提议方法可以在不同的分辨率下重建数据，并且可以在successive层中提高semantic classifier的精度，这是特别有利于在数据中优先保留更重要的semantic attribute。
</details></li>
</ul>
<hr>
<h2 id="Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models"><a href="#Tryage-Real-time-intelligent-Routing-of-User-Prompts-to-Large-Language-Models" class="headerlink" title="Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models"></a>Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11601">http://arxiv.org/abs/2308.11601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Surya Narayanan Hari, Matt Thomson</li>
<li>for: 这个研究是为了提出一个 контекст感知的路由系统， Tryage，以便将语言模型库中的专家模型选择依据输入提示的分析，以提高工作效率和适应性。</li>
<li>methods: 这个研究使用了语言模型路由器来预测下游模型的性能 на prompts，然后使用一个目标函数集成了性能预测、用户目标和约束来作出路由决策。</li>
<li>results: 在不同的数据集中，包括代码、文本、医疗资料和专利，Tryage框架在动态模型选择中比 Gorilla 和 GPT3.5 Turbo 高，实现了50.9% 的准确率，比 GPT3.5 Turbo 的23.6% 和 Gorilla 的10.8% 更高。<details>
<summary>Abstract</summary>
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details>
<details>
<summary>摘要</summary>
Introduction of transformer architecture and self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200,000 models in the Hugging Face ecosystem, users struggle with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. We propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection, identifying the optimal model with an accuracy of 50.9%, compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.
</details></li>
</ul>
<hr>
<h2 id="Low-Tensor-Rank-Learning-of-Neural-Dynamics"><a href="#Low-Tensor-Rank-Learning-of-Neural-Dynamics" class="headerlink" title="Low Tensor Rank Learning of Neural Dynamics"></a>Low Tensor Rank Learning of Neural Dynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11567">http://arxiv.org/abs/2308.11567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur Pellegrino, N Alex Cayco-Gajic, Angus Chadwick</li>
<li>for: 这个论文主要研究了 Recurrent Neural Networks (RNNs) 在学习过程中的 synaptic connectivity 的协调变化，以及这种变化如何影响学习的结果。</li>
<li>methods: 作者使用了 RNNs 的不同级别来适应不同的学习任务，并通过分析 weights 矩阵的低级别结构来研究学习过程中的 synaptic connectivity 的演化。</li>
<li>results: 作者发现，在学习过程中，RNNs 的 weights 矩阵具有低级别结构，并且这种低级别结构在整个学习过程中保持不变。此外，作者还 validate了这个发现，并提供了一些数学结果，证明在低维度任务上训练 RNNs 时，低级别 weights 自然地出现。<details>
<summary>Abstract</summary>
Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applied to the data faithfully recovers this low rank structure. Finally, we present a set of mathematical results bounding the matrix and tensor ranks of gradient descent learning dynamics which show that low-tensor-rank weights emerge naturally in RNNs trained to solve low-dimensional tasks. Taken together, our findings provide novel constraints on the evolution of population connectivity over learning in both biological and artificial neural networks, and enable reverse engineering of learning-induced changes in recurrent network dynamics from large-scale neural recordings.
</details>
<details>
<summary>摘要</summary>
学习 rely 于相协调的 synaptic 变化在 repeatedly 连接的 neuron  populations。因此，理解学习过程中 population 连接性的 collective 演化是 neuroscience 和 machine learning 中关键的挑战。特别是， latest 的研究表明，在任务训练后 RNN 的 weight matrix 通常具有低级数，但是这种低级数结构如何在学习过程中发展未知。为了解决这个问题，我们investigate  RNN 的 weight matrix 在学习过程中的级数。我们使用不同级数的 RNN 适应大规模的神经记录数据，并发现在整个学习过程中，推导出的 weights 都是 low-tensor-rank 的，因此在低维度的 subspace 中演化。我们验证了这一观察结果，并在 RNN 解决同一个任务时，直接对 ground truth  weights 进行 low-tensor-rank 分解，并证明了我们对数据进行的方法可以准确地恢复这种低级数结构。最后，我们提出了一些数学结果，证明在 gradient descent 学习动力学中，low-tensor-rank weights 会自然地出现在 RNN 解决低维度任务时。总之，我们的发现提供了对 population 连接性在学习过程中的新的约束，并允许从大规模神经记录数据中逆向工程学习-induced 变化的 recurrent network 动力学。
</details></li>
</ul>
<hr>
<h2 id="Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge"><a href="#Practical-Insights-on-Incremental-Learning-of-New-Human-Physical-Activity-on-the-Edge" class="headerlink" title="Practical Insights on Incremental Learning of New Human Physical Activity on the Edge"></a>Practical Insights on Incremental Learning of New Human Physical Activity on the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11691">http://arxiv.org/abs/2308.11691</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Arvanitakis, Jingwei Zuo, Mthandazo Ndhlovu, Hakim Hacid</li>
<li>for: 本研究探讨了Edge Machine Learning（Edge ML）中的一些独特挑战，包括受限的Edge设备存储空间、训练计算能力的有限性和学习类型的数量。</li>
<li>methods: 本研究使用了我们开发的MAGNETO系统，通过收集来自移动传感器的数据来学习人类活动。</li>
<li>results: 我们的实验结果显示，Edge ML在受限的 Edge 设备上进行学习时会面临一些独特的挑战，包括数据存储和计算能力的有限性。<details>
<summary>Abstract</summary>
Edge Machine Learning (Edge ML), which shifts computational intelligence from cloud-based systems to edge devices, is attracting significant interest due to its evident benefits including reduced latency, enhanced data privacy, and decreased connectivity reliance. While these advantages are compelling, they introduce unique challenges absent in traditional cloud-based approaches. In this paper, we delve into the intricacies of Edge-based learning, examining the interdependencies among: (i) constrained data storage on Edge devices, (ii) limited computational power for training, and (iii) the number of learning classes. Through experiments conducted using our MAGNETO system, that focused on learning human activities via data collected from mobile sensors, we highlight these challenges and offer valuable perspectives on Edge ML.
</details>
<details>
<summary>摘要</summary>
《边缘机器学习（Edge ML）》，将计算智能从云端系统转移到边缘设备，吸引了广泛关注，因为它们的优点明显，包括降低延迟、提高数据隐私和减少连接依赖。然而，这些优点也引入了传统云端方法中缺失的挑战。本文介绍边缘学习的细节，探讨（i）边缘设备受限的数据存储（ii）训练计算能力的限制和（iii）学习类数量之间的互相关系。通过我们的MAGNETO系统的实验，关于通过移动感知器收集的数据来学习人类活动，我们高亮了这些挑战并提供了有价值的对边缘ML的视角。
</details></li>
</ul>
<hr>
<h2 id="Multi-event-Video-Text-Retrieval"><a href="#Multi-event-Video-Text-Retrieval" class="headerlink" title="Multi-event Video-Text Retrieval"></a>Multi-event Video-Text Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11551">http://arxiv.org/abs/2308.11551</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gengyuanmax/mevtr">https://github.com/gengyuanmax/mevtr</a></li>
<li>paper_authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</li>
<li>for: 这篇论文的目的是提出一种多事件视频文本检索任务（MeVTR），用于解决现实中视频内容通常包含多个事件，而文本查询或页面元数据通常与单个事件相关的问题。</li>
<li>methods: 这篇论文提出了一种简单的模型——Me-Retriever，它使用了关键事件视频表示和一种新的MeVTR损失函数来解决MeVTR任务。</li>
<li>results: 对于视频-to-文本和文本-to-视频任务，这种简单的框架超越了其他模型，并在MeVTR任务中建立了一个坚固的基础。<details>
<summary>Abstract</summary>
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.
</details>
<details>
<summary>摘要</summary>
视频文本检索（VTR）是一个重要的多Modal任务，在互联网上巨量的视频文本数据时代而言。大量工作通过使用两气流视力语言模型建立一个共同表示视频文本对的方法来进行VTR任务。然而，这些模型假设视频内容和文本之间是一对一的对应关系，而忽略了实际应用中的多个事件场景。这种假设与实际应用之间存在一个差距，导致之前训练的目标与实际应用之间的差异，从而导致旧模型在推理过程中的性能下降。在这项研究中，我们引入多事件视频文本检索（MeVTR）任务，解决每个视频包含多个不同事件的场景，是传统VTR任务的一个 nichescenario。我们提出了一种简单的模型，Me-Retriever，该模型包括关键事件视频表示和一个新的MeVTR损失函数。我们进行了广泛的实验，并证明了这种简单的框架可以在视频到文本和文本到视频任务中高效地超越其他模型，并成为MeVTR任务的robust基线。我们认为这项工作将成为未来研究的坚实基础。代码可以在https://github.com/gengyuanmax/MeVTR中获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.LG_2023_08_23/" data-id="clm0t8e0l007gv7881yam7w1a" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/cs.SD_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/cs.SD_2023_08_23/">cs.SD - 2023-08-23 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lcn-kul/xls-r-analysis-sqa">https://github.com/lcn-kul/xls-r-analysis-sqa</a></li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
<li>for: 这项研究的目的是对 speech quality assessment 中使用 deep neural networks 进行自动评估，以提高用户体验质量。</li>
<li>methods: 该研究使用 pre-trained wav2vec-based XLS-R embeddings，并进行了层数据分析和特征组合的研究，以优化 speech quality prediction 的性能。</li>
<li>results: 研究发现，在不同层数据中提取特征可以达到最佳性能，并且对不同类型的干扰进行了分析，发现低级特征捕捉噪音和房间响应特征，高级特征则更注重语音内容和抗杂谱性。<details>
<summary>Abstract</summary>
In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.
</details>
<details>
<summary>摘要</summary>
在在线会议应用程序中，估计语音信号的感知质量非常重要，以确保用户的品质体验达到最高水平。人类评分是最可靠的质量评估方法，但是这种方法受到劳动力的限制，不适合大规模应用。因此，研究者们的关注点转移到了自动化语音质量评估，通过深度神经网络的端到端训练。最新的研究表明，利用预训练的wav2vec基于XLS-R的嵌入可以达到预测语音质量的状态之 arts。在这篇论文中，我们进行了嵌入的深入分析。首先，我们分析了XLS-R中每层的嵌入表现，以及每个模型的不同大小（300M、1B、2B参数）。奇怪的是，我们发现了两个优化区域：一个在下层特征中，一个在高层特征中。接下来，我们研究了这两个优化区域的原因。我们假设下层特征捕捉了噪音和房间响应的特征，而高层特征则专注于语音内容和理解度。为了证明这一点，我们分析了不同水平的噪音和房间响应对MOS预测的敏感性。然后，我们尝试将这两个优化区域融合，以确定他们是否包含相互补充的信息。最后，我们比较了我们提出的模型，并评估这些模型在未seen数据上的泛化性。
</details></li>
</ul>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
<li>for: This paper is written for the purpose of exploring the integration of objective audio events (AE) with subjective annoyance ratings (AR) of soundscape perceived by humans.</li>
<li>methods: The paper proposes a novel hierarchical graph representation learning (HGRL) approach to link AE with AR. The approach consists of fine-grained event (fAE) embeddings, coarse-grained event (cAE) embeddings, and AR embeddings.</li>
<li>results: The proposed HGRL approach successfully integrates AE with AR for audio event classification (AEC) and audio scene understanding (ARP) tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.<details>
<summary>Abstract</summary>
Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans' listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.
</details>
<details>
<summary>摘要</summary>
日常生活中的听觉事件携带着 objective 世界的丰富信息。听觉事件的组成会影响人们在听觉景象中的心理状态。先前的方法通常只是对听觉事件和场景进行分类和检测，可能忽略了这些听觉事件对人们听觉环境中的 listening 心理状态的影响，例如厌烦。为此，本文提出了一种新的层次图表学习（HGRL）方法，将 objective 听觉事件（AE）与人们对听觉景象的主观厌烦评分（AR）关联起来。层次图包括细化的事件嵌入（fAE）、中细化的事件嵌入（cAE）和 AR 嵌入。实验显示，提出的 HGRL 方法成功地结合 AE 与 AR  для AEC 和 ARP 任务，同时协调 cAE 和 fAE 之间的关系，并将两种不同的 AE 信息与 AR 进行对应。
</details></li>
</ul>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
<li>for: 提高音频分类任务的性能和减少模型大小</li>
<li>methods: 使用扩展和知识填充（KD）技术，以及一个简单的训练框架称为常规教学（CED）</li>
<li>results: 使用CED训练多种基于变换器的模型，包括一个10M参数模型，在Audioset（AS）上达到49.0的mean average precision（mAP）<details>
<summary>Abstract</summary>
Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3\% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details>
<details>
<summary>摘要</summary>
🇨🇳 扩展和知识储存（KD）是音频分类任务中常用的技术，可以提高性能和减少模型大小。 although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.Here's the word-for-word translation of the text into Simplified Chinese:🇨🇳 扩展和知识储存（KD）是音频分类任务中常用的技术，可以提高性能和减少模型大小。 although both techniques are effective individually, their combined use, called consistent teaching, hasn't been explored before. This paper proposes CED, a simple training framework that distills student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED's efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model, requiring only 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at https://github.com/RicherMans/CED.
</details></li>
</ul>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
<li>for: 本研究的目的是控制生成的音频TEXTURE，通过条件使用标注数据，但是获取标注数据可能是时间consuming和容易出错的。</li>
<li>methods: 本研究提出了一种基于例子的框架，通过用户定义的语义特征来决定生成过程中的控制因素。通过生成一些示例来指示语义特征的存在或缺失，可以在生成过程中找到控制因素的指导向量。</li>
<li>results: 研究表明，该方法可以找到生成过程中的具有语义特征的潜在相关性和决定性指导向量，并应用于其他任务，如选择性 semantic attribute transfer。<details>
<summary>Abstract</summary>
Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.
</details>
<details>
<summary>摘要</summary>
<<SYS>>用抽象模型生成 audio 文化时，可以显式编码控制性。不过，对 audio 文化的数据进行semantic labeling是costly，time-consuming，和容易出错，因为人工标注者的主观性。因此，要控制生成，需要自动从无标注 texture 中推断用户定义的 Semantic attribute 的变化因素。在这篇论文中，我们提出了一种基于例子的框架，用于确定 guide vector，以控制生成中的 Semantic attribute。通过生成一些synthetic example来指示Semantic attribute的存在或缺失，我们可以在生成过程中推断guide vector的方向。我们的结果表明，我们的方法可以找到可见 relevance 和 deterministic的 guide vector，以便在生成中控制 Semantic attribute。此外，我们还展示了这种方法的应用于其他任务，如选择性 transferred attribute。Note: "Simplified Chinese" is a romanization of the Chinese language that uses a simplified set of characters and pronunciation. It is commonly used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
<li>for: The paper is written to investigate the use of natural language processing on social media to predict depression, with a focus on identifying specific speech topics that may indicate depression severity.</li>
<li>methods: The paper uses the Whisper tool and the BERTopic model to analyze 3919 smartphone-collected speech recordings from 265 participants, identifying 29 topics and finding that six of these topics are associated with higher depression severity.</li>
<li>results: The paper finds that specific speech topics can indicate depression severity, and that longitudinally monitoring language use can provide valuable insights into changes in depression severity over time. The study also demonstrates the practicality of using data-driven workflows to collect and analyze large-scale speech data from real-world settings for digital health research.Here are the three points in Simplified Chinese text:</li>
<li>for: 这篇论文是为了研究社交媒体上的自然语言处理，以预测抑郁，并通过特定的语音话题来评估抑郁严重程度。</li>
<li>methods: 这篇论文使用Whisper工具和BERTopic模型分析了3919个手机收集的语音记录，并将其分为29个话题，其中六个话题与抑郁严重程度高有关。</li>
<li>results: 这篇论文发现特定的语音话题可以反映抑郁严重程度，并且 longitudinal 监测语音使用可以为抑郁研究提供有价值的信息。研究还证明了使用数据驱动的工作流程收集和分析大规模语音数据的实用性。<details>
<summary>Abstract</summary>
Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.
</details>
<details>
<summary>摘要</summary>
研究表明语言使用与抑郁有相关性，但大规模验证是需要的。传统的临床研究是昂贵的，因此人工智能技术在社交媒体上进行语言预测是一种可能的方法。然而，这些方法还存在一些限制，包括无效的标签验证、受众样本偏见和缺乏上下文。我们的研究通过使用Whisper工具和BERTopic模型分析了3919个手机收集的语音记录，从265名参与者中提取出29个话题。六个话题的中值PHQ-8大于或等于10被视为抑郁的风险话题：无望、睡眠、心理治疗、剪发、学习和课程。为了解释话题出现和抑郁严重度之间的关系，我们比较了不同话题的行为特征（来自佩戴器）和语言特征。我们还 investigate了话题变化和抑郁严重度变化的时间相关性，这表明了需要长期监测语言使用。我们还在相似的小样本上测试了BERTopic模型，获得了一些相似的结果。总之，我们的发现表明特定的语音话题可能指示抑郁严重度。我们提供的数据驱动的工作流程为整体卫生研究提供了实用的方法。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/cs.SD_2023_08_23/" data-id="clm0t8e1g00aov7884orfcaib" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.AS_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.AS_2023_08_23/">eess.AS - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Analysis-of-XLS-R-for-Speech-Quality-Assessment"><a href="#Analysis-of-XLS-R-for-Speech-Quality-Assessment" class="headerlink" title="Analysis of XLS-R for Speech Quality Assessment"></a>Analysis of XLS-R for Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12077">http://arxiv.org/abs/2308.12077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bastiaan Tamm, Rik Vandenberghe, Hugo Van hamme</li>
</ul>
<p>Abstract:<br>In online conferencing applications, estimating the perceived quality of an audio signal is crucial to ensure high quality of experience for the end user. The most reliable way to assess the quality of a speech signal is through human judgments in the form of the mean opinion score (MOS) metric. However, such an approach is labor intensive and not feasible for large-scale applications. The focus has therefore shifted towards automated speech quality assessment through end-to-end training of deep neural networks. Recently, it was shown that leveraging pre-trained wav2vec-based XLS-R embeddings leads to state-of-the-art performance for the task of speech quality prediction. In this paper, we perform an in-depth analysis of the pre-trained model. First, we analyze the performance of embeddings extracted from each layer of XLS-R and also for each size of the model (300M, 1B, 2B parameters). Surprisingly, we find two optimal regions for feature extraction: one in the lower-level features and one in the high-level features. Next, we investigate the reason for the two distinct optima. We hypothesize that the lower-level features capture characteristics of noise and room acoustics, whereas the high-level features focus on speech content and intelligibility. To investigate this, we analyze the sensitivity of the MOS predictions with respect to different levels of corruption in each category. Afterwards, we try fusing the two optimal feature depths to determine if they contain complementary information for MOS prediction. Finally, we compare the performance of the proposed models and assess the generalizability of the models on unseen datasets.</p>
<hr>
<h2 id="Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning"><a href="#Joint-Prediction-of-Audio-Event-and-Annoyance-Rating-in-an-Urban-Soundscape-by-Hierarchical-Graph-Representation-Learning" class="headerlink" title="Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning"></a>Joint Prediction of Audio Event and Annoyance Rating in an Urban Soundscape by Hierarchical Graph Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11980">http://arxiv.org/abs/2308.11980</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/hgrl">https://github.com/yuanbo2020/hgrl</a></li>
<li>paper_authors: Yuanbo Hou, Siyang Song, Cheng Luo, Andrew Mitchell, Qiaoqiao Ren, Weicheng Xie, Jian Kang, Wenwu Wang, Dick Botteldooren</li>
</ul>
<p>Abstract:<br>Sound events in daily life carry rich information about the objective world. The composition of these sounds affects the mood of people in a soundscape. Most previous approaches only focus on classifying and detecting audio events and scenes, but may ignore their perceptual quality that may impact humans’ listening mood for the environment, e.g. annoyance. To this end, this paper proposes a novel hierarchical graph representation learning (HGRL) approach which links objective audio events (AE) with subjective annoyance ratings (AR) of the soundscape perceived by humans. The hierarchical graph consists of fine-grained event (fAE) embeddings with single-class event semantics, coarse-grained event (cAE) embeddings with multi-class event semantics, and AR embeddings. Experiments show the proposed HGRL successfully integrates AE with AR for AEC and ARP tasks, while coordinating the relations between cAE and fAE and further aligning the two different grains of AE information with the AR.</p>
<hr>
<h2 id="CED-Consistent-ensemble-distillation-for-audio-tagging"><a href="#CED-Consistent-ensemble-distillation-for-audio-tagging" class="headerlink" title="CED: Consistent ensemble distillation for audio tagging"></a>CED: Consistent ensemble distillation for audio tagging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11957">http://arxiv.org/abs/2308.11957</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/richermans/ced">https://github.com/richermans/ced</a></li>
<li>paper_authors: Heinrich Dinkel, Yongqing Wang, Zhiyong Yan, Junbo Zhang, Yujun Wang</li>
</ul>
<p>Abstract:<br>Augmentation and knowledge distillation (KD) are well-established techniques employed in the realm of audio classification tasks, aimed at enhancing performance and reducing model sizes on the widely recognized Audioset (AS) benchmark. Although both techniques are effective individually, their combined use, called consistent teaching, hasn’t been explored before. This paper proposes CED, a simple training framework that distils student models from large teacher ensembles with consistent teaching. To achieve this, CED efficiently stores logits as well as the augmentation methods on disk, making it scalable to large-scale datasets. Central to CED’s efficacy is its label-free nature, meaning that only the stored logits are used for the optimization of a student model only requiring 0.3% additional disk space for AS. The study trains various transformer-based models, including a 10M parameter model achieving a 49.0 mean average precision (mAP) on AS. Pretrained models and code are available at <a target="_blank" rel="noopener" href="https://github.com/RicherMans/CED">https://github.com/RicherMans/CED</a>.</p>
<hr>
<h2 id="Audio-Generation-with-Multiple-Conditional-Diffusion-Model"><a href="#Audio-Generation-with-Multiple-Conditional-Diffusion-Model" class="headerlink" title="Audio Generation with Multiple Conditional Diffusion Model"></a>Audio Generation with Multiple Conditional Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11940">http://arxiv.org/abs/2308.11940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhifang Guo, Jianguo Mao, Rui Tao, Long Yan, Kazushige Ouchi, Hong Liu, Xiangdong Wang</li>
</ul>
<p>Abstract:<br>Text-based audio generation models have limitations as they cannot encompass all the information in audio, leading to restricted controllability when relying solely on text. To address this issue, we propose a novel model that enhances the controllability of existing pre-trained text-to-audio models by incorporating additional conditions including content (timestamp) and style (pitch contour and energy contour) as supplements to the text. This approach achieves fine-grained control over the temporal order, pitch, and energy of generated audio. To preserve the diversity of generation, we employ a trainable control condition encoder that is enhanced by a large language model and a trainable Fusion-Net to encode and fuse the additional conditions while keeping the weights of the pre-trained text-to-audio model frozen. Due to the lack of suitable datasets and evaluation metrics, we consolidate existing datasets into a new dataset comprising the audio and corresponding conditions and use a series of evaluation metrics to evaluate the controllability performance. Experimental results demonstrate that our model successfully achieves fine-grained control to accomplish controllable audio generation. Audio samples and our dataset are publicly available at <a target="_blank" rel="noopener" href="https://conditionaudiogen.github.io/conditionaudiogen/">https://conditionaudiogen.github.io/conditionaudiogen/</a></p>
<hr>
<h2 id="Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement"><a href="#Audio-Difference-Captioning-Utilizing-Similarity-Discrepancy-Disentanglement" class="headerlink" title="Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement"></a>Audio Difference Captioning Utilizing Similarity-Discrepancy Disentanglement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11923">http://arxiv.org/abs/2308.11923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, Kunio Kashino</li>
</ul>
<p>Abstract:<br>We proposed Audio Difference Captioning (ADC) as a new extension task of audio captioning for describing the semantic differences between input pairs of similar but slightly different audio clips. The ADC solves the problem that conventional audio captioning sometimes generates similar captions for similar audio clips, failing to describe the difference in content. We also propose a cross-attention-concentrated transformer encoder to extract differences by comparing a pair of audio clips and a similarity-discrepancy disentanglement to emphasize the difference in the latent space. To evaluate the proposed methods, we built an AudioDiffCaps dataset consisting of pairs of similar but slightly different audio clips with human-annotated descriptions of their differences. The experiment with the AudioDiffCaps dataset showed that the proposed methods solve the ADC task effectively and improve the attention weights to extract the difference by visualizing them in the transformer encoder.</p>
<hr>
<h2 id="KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods"><a href="#KinSPEAK-Improving-speech-recognition-for-Kinyarwanda-via-semi-supervised-learning-methods" class="headerlink" title="KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods"></a>KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11863">http://arxiv.org/abs/2308.11863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Nzeyimana</li>
</ul>
<p>Abstract:<br>Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art to the best of our knowledge. Our experiments also indicate that using syllabic rather than character-based tokenization results in better speech recognition performance for Kinyarwanda.</p>
<hr>
<h2 id="Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation"><a href="#Example-Based-Framework-for-Perceptually-Guided-Audio-Texture-Generation" class="headerlink" title="Example-Based Framework for Perceptually Guided Audio Texture Generation"></a>Example-Based Framework for Perceptually Guided Audio Texture Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11859">http://arxiv.org/abs/2308.11859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Purnima Kamath, Chitralekha Gupta, Lonce Wyse, Suranga Nanayakkara</li>
</ul>
<p>Abstract:<br>Generative models for synthesizing audio textures explicitly encode controllability by conditioning the model with labelled data. While datasets for audio textures can be easily recorded in-the-wild, semantically labeling them is expensive, time-consuming, and prone to errors due to human annotator subjectivity. Thus, to control generation, there is a need to automatically infer user-defined perceptual factors of variation in the latent space of a generative model while modelling unlabeled textures. In this paper, we propose an example-based framework to determine vectors to guide texture generation based on user-defined semantic attributes. By synthesizing a few synthetic examples to indicate the presence or absence of a semantic attribute, we can infer the guidance vectors in the latent space of a generative model to control that attribute during generation. Our results show that our method is capable of finding perceptually relevant and deterministic guidance vectors for controllable generation for both discrete as well as continuous textures. Furthermore, we demonstrate the application of this method to other tasks such as selective semantic attribute transfer.</p>
<hr>
<h2 id="Complex-valued-neural-networks-for-voice-anti-spoofing"><a href="#Complex-valued-neural-networks-for-voice-anti-spoofing" class="headerlink" title="Complex-valued neural networks for voice anti-spoofing"></a>Complex-valued neural networks for voice anti-spoofing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11800">http://arxiv.org/abs/2308.11800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas M. Müller, Philip Sperl, Konstantin Böttinger</li>
</ul>
<p>Abstract:<br>Current anti-spoofing and audio deepfake detection systems use either magnitude spectrogram-based features (such as CQT or Melspectrograms) or raw audio processed through convolution or sinc-layers. Both methods have drawbacks: magnitude spectrograms discard phase information, which affects audio naturalness, and raw-feature-based models cannot use traditional explainable AI methods. This paper proposes a new approach that combines the benefits of both methods by using complex-valued neural networks to process the complex-valued, CQT frequency-domain representation of the input audio. This method retains phase information and allows for explainable AI methods. Results show that this approach outperforms previous methods on the “In-the-Wild” anti-spoofing dataset and enables interpretation of the results through explainable AI. Ablation studies confirm that the model has learned to use phase information to detect voice spoofing.</p>
<hr>
<h2 id="Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model"><a href="#Identifying-depression-related-topics-in-smartphone-collected-free-response-speech-recordings-using-an-automatic-speech-recognition-system-and-a-deep-learning-topic-model" class="headerlink" title="Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model"></a>Identifying depression-related topics in smartphone-collected free-response speech recordings using an automatic speech recognition system and a deep learning topic model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11773">http://arxiv.org/abs/2308.11773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuezhou Zhang, Amos A Folarin, Judith Dineley, Pauline Conde, Valeria de Angel, Shaoxiong Sun, Yatharth Ranjan, Zulqarnain Rashid, Callum Stewart, Petroula Laiou, Heet Sankesara, Linglong Qian, Faith Matcham, Katie M White, Carolin Oetzmann, Femke Lamers, Sara Siddi, Sara Simblett, Björn W. Schuller, Srinivasan Vairavan, Til Wykes, Josep Maria Haro, Brenda WJH Penninx, Vaibhav A Narayan, Matthew Hotopf, Richard JB Dobson, Nicholas Cummins, RADAR-CNS consortium</li>
</ul>
<p>Abstract:<br>Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research.</p>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eess.AS_2023_08_23/" data-id="clm0t8e1z00cmv7882dr2dioc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eess.IV_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eess.IV_2023_08_23/">eess.IV - 2023-08-23 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation"><a href="#Tumor-Centered-Patching-for-Enhanced-Medical-Image-Segmentation" class="headerlink" title="Tumor-Centered Patching for Enhanced Medical Image Segmentation"></a>Tumor-Centered Patching for Enhanced Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12168">http://arxiv.org/abs/2308.12168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutyyba Asghar, Ahmad Raza Shahid, Akhtar Jamil, Kiran Aftab, Syed Ather Enam</li>
<li>for: 这篇论文旨在提高医疗影像诊断中的Computer-aided diagnosis和手术系统。</li>
<li>methods: 这篇论文使用了一种新的tumor-centered patching方法，将肿瘤作为分析区域，以改善分类不均和边界缺乏的问题。</li>
<li>results: 实验结果显示，这种方法可以改善分类不均， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。<details>
<summary>Abstract</summary>
The realm of medical image diagnosis has advanced significantly with the integration of computer-aided diagnosis and surgical systems. However, challenges persist, particularly in achieving precise image segmentation. While deep learning techniques show potential, obstacles like limited resources, slow convergence, and class imbalance impede their effectiveness. Traditional patch-based methods, though common, struggle to capture intricate tumor boundaries and often lead to redundant samples, compromising computational efficiency and feature quality. To tackle these issues, this research introduces an innovative approach centered on the tumor itself for patch-based image analysis. This novel tumor-centered patching method aims to address the class imbalance and boundary deficiencies, enabling focused and accurate tumor segmentation. By aligning patches with the tumor's anatomical context, this technique enhances feature extraction accuracy and reduces computational load. Experimental results demonstrate improved class imbalance, with segmentation scores of 0.78, 0.76, and 0.71 for whole, core, and enhancing tumors, respectively using a lightweight simple U-Net. This approach shows potential for enhancing medical image segmentation and improving computer-aided diagnosis systems.
</details>
<details>
<summary>摘要</summary>
医疗图像诊断领域已经得到了计算机支持的辅助诊断和手术系统的整合，但是还存在许多挑战，主要是精准图像分割的问题。深度学习技术表现出了潜在的潜力，但是有限的资源、慢速融合和分类不均等问题使其效果受限。传统的补丁方法，尽管广泛使用，但是它们往往难以捕捉复杂的肿瘤边界，导致重复的样本生成，从而降低计算效率和特征质量。为解决这些问题，本研究提出了一种新的方法，这种方法是基于肿瘤的补丁分析法。这种新的肿瘤中心的补丁方法希图解决分类不均和边界不足的问题，以提高精准的肿瘤分割。通过将补丁与肿瘤的 анатомиче上下文进行对齐，这种技术可以提高特征提取的准确性和降低计算负担。实验结果表明，使用了一种轻量级的简单U-Net，可以提高分类不均的问题， segmentation scores分别为0.78、0.76和0.71 для整体、核心和增强肿瘤。这种方法表现出了在医疗图像分割领域的潜力，并可能用于改进计算机支持的诊断系统。
</details></li>
</ul>
<hr>
<h2 id="DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning"><a href="#DISGAN-Wavelet-informed-Discriminator-Guides-GAN-to-MRI-Super-resolution-with-Noise-Cleaning" class="headerlink" title="DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning"></a>DISGAN: Wavelet-informed Discriminator Guides GAN to MRI Super-resolution with Noise Cleaning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12084">http://arxiv.org/abs/2308.12084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qi Wang, Lucas Mahler, Julius Steiglechner, Florian Birk, Klaus Scheffler, Gabriele Lohmann</li>
<li>for: 这个研究是为了提出一个可以同时进行超解析和降噪的深度学习模型，以扩展现有的超解析和降噪模型的能力。</li>
<li>methods: 这个模型使用了一个基于 residual-in-residual 的 generator，以及一个具有3D DWT和1x1卷积的 discriminator。</li>
<li>results: 这个模型可以同时进行高品质的超解析和降噪，并且可以在未见过的MRI数据上进行验证。<details>
<summary>Abstract</summary>
MRI super-resolution (SR) and denoising tasks are fundamental challenges in the field of deep learning, which have traditionally been treated as distinct tasks with separate paired training data. In this paper, we propose an innovative method that addresses both tasks simultaneously using a single deep learning model, eliminating the need for explicitly paired noisy and clean images during training. Our proposed model is primarily trained for SR, but also exhibits remarkable noise-cleaning capabilities in the super-resolved images. Instead of conventional approaches that introduce frequency-related operations into the generative process, our novel approach involves the use of a GAN model guided by a frequency-informed discriminator. To achieve this, we harness the power of the 3D Discrete Wavelet Transform (DWT) operation as a frequency constraint within the GAN framework for the SR task on magnetic resonance imaging (MRI) data. Specifically, our contributions include: 1) a 3D generator based on residual-in-residual connected blocks; 2) the integration of the 3D DWT with $1\times 1$ convolution into a DWT+conv unit within a 3D Unet for the discriminator; 3) the use of the trained model for high-quality image SR, accompanied by an intrinsic denoising process. We dub the model "Denoising Induced Super-resolution GAN (DISGAN)" due to its dual effects of SR image generation and simultaneous denoising. Departing from the traditional approach of training SR and denoising tasks as separate models, our proposed DISGAN is trained only on the SR task, but also achieves exceptional performance in denoising. The model is trained on 3D MRI data from dozens of subjects from the Human Connectome Project (HCP) and further evaluated on previously unseen MRI data from subjects with brain tumours and epilepsy to assess its denoising and SR performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>基于 residual-in-residual 的 3D generator，使用connected块来实现高质量的 SR 图像生成。2. 将 3D DWT 与 1x1  convolution 结合在一起，形成 DWT+conv 单元，并将其 integrate into 3D Unet 中的权重来实现高精度的 SR 预测。3. 使用训练好的模型进行高质量的图像 SR，同时实现了内在的噪声除除过程。我们称这种模型为 “Denoising Induced Super-resolution GAN”（DISGAN），因为它同时实现了 SR 图像生成和噪声除除。不同于传统的方法，我们的 DISGAN 只受 SR 任务培训，同时也可以在未看过的 MRI 数据上实现出色的噪声除除和 SR 性能。我们在 Human Connectome Project（HCP） 提供的3D MRI数据上进行了训练，并在患有脑肿和癫痫的患者的 MRI 数据上进行了评估，以评估其噪声除除和 SR 性能。</details></li>
</ol>
<hr>
<h2 id="StofNet-Super-resolution-Time-of-Flight-Network"><a href="#StofNet-Super-resolution-Time-of-Flight-Network" class="headerlink" title="StofNet: Super-resolution Time of Flight Network"></a>StofNet: Super-resolution Time of Flight Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12009">http://arxiv.org/abs/2308.12009</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a></li>
<li>paper_authors: Christopher Hahne, Michel Hayoz, Raphael Sznitman</li>
<li>for: 该论文主要针对时间飞行（ToF）感知技术在 робо测试、医学成像和非 destruktive testing 等领域中的问题，即在复杂的 ambient condition 下，从简单的时间信息中进行逆模拟是不可能的。</li>
<li>methods: 该论文提出了一种现代超解像技术来学习困难 ambient condition，以提高ToF感知的可靠性和准确性。具体来说， authors 提出了一种结合超解像和高效减弱块的架构，以平衡细详信号的细节和大规模的上下文信息。</li>
<li>results: 该论文通过对六种现有方法进行比较，并使用两个公共可用的数据集进行测试，证明了提出的 StofNet 方法在精度、可靠性和模型复杂度三个方面具有显著的优势。code 可以在 <a target="_blank" rel="noopener" href="https://github.com/hahnec/stofnet">https://github.com/hahnec/stofnet</a> 上下载。<details>
<summary>Abstract</summary>
Time of Flight (ToF) is a prevalent depth sensing technology in the fields of robotics, medical imaging, and non-destructive testing. Yet, ToF sensing faces challenges from complex ambient conditions making an inverse modelling from the sparse temporal information intractable. This paper highlights the potential of modern super-resolution techniques to learn varying surroundings for a reliable and accurate ToF detection. Unlike existing models, we tailor an architecture for sub-sample precise semi-global signal localization by combining super-resolution with an efficient residual contraction block to balance between fine signal details and large scale contextual information. We consolidate research on ToF by conducting a benchmark comparison against six state-of-the-art methods for which we employ two publicly available datasets. This includes the release of our SToF-Chirp dataset captured by an airborne ultrasound transducer. Results showcase the superior performance of our proposed StofNet in terms of precision, reliability and model complexity. Our code is available at https://github.com/hahnec/stofnet.
</details>
<details>
<summary>摘要</summary>
时间飞行（ToF）是现代深度探测技术的重要应用领域，包括机器人、医学成像和非 destruktive testing。然而，ToF探测受到环境复杂性的影响，使得反向模型从稀疏的时间信息中做出准确的探测变得困难。本文提出了现代超分解技术的潜在作用，以提高ToF探测的可靠性和准确性。与现有模型不同，我们开发了一种结构，即StofNet，通过结合超分解和高效的剩余压缩块来平衡细信息和大规模的上下文信息。我们在六种state-of-the-art方法的基准比较中，使用了两个公共可用的数据集。这包括我们发布的SToF-Chirp数据集， capture by an airborne ultrasound transducer。结果表明我们提posed StofNet在精度、可靠性和模型复杂度方面表现出色。我们的代码可以在https://github.com/hahnec/stofnet中下载。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification"><a href="#Comparing-Autoencoder-to-Geometrical-Features-for-Vascular-Bifurcations-Identification" class="headerlink" title="Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification"></a>Comparing Autoencoder to Geometrical Features for Vascular Bifurcations Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12314">http://arxiv.org/abs/2308.12314</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtissam Essadik, Anass Nouri, Raja Touahni, Florent Autrusseau</li>
<li>for: 这个论文的目的是提出两种基于自动Encoder和几何特征的新方法来识别血管分叉。</li>
<li>methods: 这两种方法分别使用自动Encoder和几何特征来提取特征和识别模式。</li>
<li>results: 经过评估，两种方法在使用医疗影像数据进行血管分叉分类中具有良好的性能和效果，其中自动Encoder方法的准确率和F1分数较高。<details>
<summary>Abstract</summary>
The cerebrovascular tree is a complex anatomical structure that plays a crucial role in the brain irrigation. A precise identification of the bifurcations in the vascular network is essential for understanding various cerebral pathologies. Traditional methods often require manual intervention and are sensitive to variations in data quality. In recent years, deep learning techniques, and particularly autoencoders, have shown promising performances for feature extraction and pattern recognition in a variety of domains. In this paper, we propose two novel approaches for vascular bifurcation identification based respectiveley on Autoencoder and geometrical features. The performance and effectiveness of each method in terms of classification of vascular bifurcations using medical imaging data is presented. The evaluation was performed on a sample database composed of 91 TOF-MRA, using various evaluation measures, including accuracy, F1 score and confusion matrix.
</details>
<details>
<summary>摘要</summary>
脑血管树是一种复杂的生物结构，对脑血液循环具有关键作用。正确地识别血管网络中的分枝是理解脑血液疾病的关键。传统方法经常需要手动干预，并且敏感于数据质量的变化。在最近几年，深度学习技术和特别是自动编码器在多种领域中表现出了扎实的功能。本文提出了两种基于自动编码器和几何特征的血管分枝识别方法。每种方法的性能和效果在使用医疗影像数据进行血管分枝分类中进行了评估，并使用了几种评价指标，包括准确率、F1分数和混淆矩阵。
</details></li>
</ul>
<hr>
<h2 id="Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies"><a href="#Recovering-a-Molecule’s-3D-Dynamics-from-Liquid-phase-Electron-Microscopy-Movies" class="headerlink" title="Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies"></a>Recovering a Molecule’s 3D Dynamics from Liquid-phase Electron Microscopy Movies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11927">http://arxiv.org/abs/2308.11927</a></li>
<li>repo_url: None</li>
<li>paper_authors: Enze Ye, Yuhang Wang, Hong Zhang, Yiqin Gao, Huan Wang, He Sun</li>
<li>for: 这研究旨在使用liquid-phase electron microscopy（liquid-phase EM）技术观察生物分子的动态变化。</li>
<li>methods: 该研究提出了TEMPOR算法，它是一种基于偶极神经网络（INR）和动态变量自适应器（DVAE）的时间序列分子结构重建方法。</li>
<li>results: 研究人员通过对两个 simulate数据集（7bcq和Cas9）进行测试，发现TEMPOR算法可以有效地回收不同的动态变化。这是首个直接从liquid-phase EM电影中回收动态变化的3D结构的研究，它为结构生物学提供了一个有前途的新方法。<details>
<summary>Abstract</summary>
The dynamics of biomolecules are crucial for our understanding of their functioning in living systems. However, current 3D imaging techniques, such as cryogenic electron microscopy (cryo-EM), require freezing the sample, which limits the observation of their conformational changes in real time. The innovative liquid-phase electron microscopy (liquid-phase EM) technique allows molecules to be placed in the native liquid environment, providing a unique opportunity to observe their dynamics. In this paper, we propose TEMPOR, a Temporal Electron MicroscoPy Object Reconstruction algorithm for liquid-phase EM that leverages an implicit neural representation (INR) and a dynamical variational auto-encoder (DVAE) to recover time series of molecular structures. We demonstrate its advantages in recovering different motion dynamics from two simulated datasets, 7bcq and Cas9. To our knowledge, our work is the first attempt to directly recover 3D structures of a temporally-varying particle from liquid-phase EM movies. It provides a promising new approach for studying molecules' 3D dynamics in structural biology.
</details>
<details>
<summary>摘要</summary>
生物分子动态是我们理解它们在生物系统中功能的关键。然而，现有的3D图像技术，如气化电子顾 microscopy (cryo-EM)，需要采样冻结，限制观察分子 conformational 变化的实时观察。新的液相电子顾 microscopy (liquid-phase EM) 技术可以将分子放在原生液态环境中，提供了观察分子动态的独特机会。在这篇论文中，我们提出了 TEMPOR，一种基于 implicit neural representation (INR) 和动态variational autoencoder (DVAE) 的 Temporal Electron MicroscoPy Object Reconstruction算法，可以从液相电子顾 movie 中回收时间序列的分子结构。我们在两个 simulated 数据集，7bcq 和 Cas9 中，证明了它的优势。到目前为止，我们的工作是直接从液相电子顾 movie 中回收变化的3D结构的第一次尝试。它提供了一种有前途的新方法，用于生物学结构中的分子3D动态研究。
</details></li>
</ul>
<hr>
<h2 id="Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration"><a href="#Studying-the-Impact-of-Augmentations-on-Medical-Confidence-Calibration" class="headerlink" title="Studying the Impact of Augmentations on Medical Confidence Calibration"></a>Studying the Impact of Augmentations on Medical Confidence Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11902">http://arxiv.org/abs/2308.11902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrit Rao, Joon-Young Lee, Oliver Aalami</li>
<li>for: 这 paper 的目的是evaluate the effects of three modern augmentation techniques on the calibration and performance of convolutional neural networks (CNNs) for medical tasks.</li>
<li>methods: 这 paper 使用了三种现代扩展技术：CutMix、MixUp 和 CutOut，以提高 CNNs 的准确率和准确性。</li>
<li>results: 研究发现，CutMix 最大程度地提高了 CNNs 的准确性，而 CutOut 有时会降低准确性。<details>
<summary>Abstract</summary>
The clinical explainability of convolutional neural networks (CNN) heavily relies on the joint interpretation of a model's predicted diagnostic label and associated confidence. A highly certain or uncertain model can significantly impact clinical decision-making. Thus, ensuring that confidence estimates reflect the true correctness likelihood for a prediction is essential. CNNs are often poorly calibrated and prone to overconfidence leading to improper measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Taking into consideration the risks associated with miscalibration is of high importance. In recent years, modern augmentation techniques, which cut, mix, and combine images, have been introduced. Such augmentations have benefited CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming, are widely leveraged in the medical domain to combat the scarcity of data. In this paper, we evaluate the effects of three modern augmentation techniques, CutMix, MixUp, and CutOut on the calibration and performance of CNNs for medical tasks. CutMix improved calibration the most while CutOut often lowered the level of calibration.
</details>
<details>
<summary>摘要</summary>
医学预测模型（Convolutional Neural Network，简称CNN）的解释性强调与预测结果和相关的信任度之间的共同解释。一个高度确定或不确定的模型可能会对临床决策产生重大影响。因此，确保模型的信任度反映预测的准确性 likelihood 是关键的。然而， CNNs  oftensuffer from poor calibration and overconfidence, leading to inappropriate measures of uncertainty. This creates the need for confidence calibration. However, accuracy and performance-based evaluations of CNNs are commonly used as the sole benchmark for medical tasks. Considering the risks associated with miscalibration is of high importance.Recently, modern augmentation techniques, such as CutMix, MixUp, and CutOut, have been introduced to improve the calibration and performance of CNNs. These techniques have been shown to benefit CNNs through regularization, robustness to adversarial samples, and calibration. Standard augmentations based on image scaling, rotating, and zooming are widely used in the medical domain to address the scarcity of data. In this paper, we evaluate the effects of these three modern augmentation techniques on the calibration and performance of CNNs for medical tasks. Our results show that CutMix improved calibration the most, while CutOut often lowered the level of calibration.
</details></li>
</ul>
<hr>
<h2 id="Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression"><a href="#Enhanced-Residual-SwinV2-Transformer-for-Learned-Image-Compression" class="headerlink" title="Enhanced Residual SwinV2 Transformer for Learned Image Compression"></a>Enhanced Residual SwinV2 Transformer for Learned Image Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11864">http://arxiv.org/abs/2308.11864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongqiang Wang, Feng Liang, Haisheng Fu, Jie Liang, Haipeng Qin, Junzhe Liang</li>
<li>for: 提高图像压缩的率和质量之间的折衔，并且减少模型复杂度。</li>
<li>methods: 使用改进的差异Swinv2 transformer和特征增强模块，并在编码和超编码步骤中使用SwinV2 transformer-based attention机制。</li>
<li>results: 在Kodak和Tecnick数据集上实现了与一些最新的学习型图像压缩方法相当的性能，并且比一些传统的编码器更高。具体来说，我们的方法在同等性能下减少了56%的模型复杂度。<details>
<summary>Abstract</summary>
Recently, the deep learning technology has been successfully applied in the field of image compression, leading to superior rate-distortion performance. However, a challenge of many learning-based approaches is that they often achieve better performance via sacrificing complexity, which making practical deployment difficult. To alleviate this issue, in this paper, we propose an effective and efficient learned image compression framework based on an enhanced residual Swinv2 transformer. To enhance the nonlinear representation of images in our framework, we use a feature enhancement module that consists of three consecutive convolutional layers. In the subsequent coding and hyper coding steps, we utilize a SwinV2 transformer-based attention mechanism to process the input image. The SwinV2 model can help to reduce model complexity while maintaining high performance. Experimental results show that the proposed method achieves comparable performance compared to some recent learned image compression methods on Kodak and Tecnick datasets, and outperforms some traditional codecs including VVC. In particular, our method achieves comparable results while reducing model complexity by 56% compared to these recent methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Robust-RF-Data-Normalization-for-Deep-Learning"><a href="#Robust-RF-Data-Normalization-for-Deep-Learning" class="headerlink" title="Robust RF Data Normalization for Deep Learning"></a>Robust RF Data Normalization for Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11833">http://arxiv.org/abs/2308.11833</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>for: 用于深度神经网络的训练</li>
<li>methods: 使用个体标准化方法更好地利用RF数据</li>
<li>results: 提高深度神经网络的性能和通用性<details>
<summary>Abstract</summary>
Radio frequency (RF) data contain richer information compared to other data types, such as envelope or B-mode, and employing RF data for training deep neural networks has attracted growing interest in ultrasound image processing. However, RF data is highly fluctuating and additionally has a high dynamic range. Most previous studies in the literature have relied on conventional data normalization, which has been adopted within the computer vision community. We demonstrate the inadequacy of those techniques for normalizing RF data and propose that individual standardization of each image substantially enhances the performance of deep neural networks by utilizing the data more efficiently. We compare conventional and proposed normalizations in a phase aberration correction task and illustrate how the former enhances the generality of trained models.
</details>
<details>
<summary>摘要</summary>
radio frequency (RF) 数据含有更多信息，比如拥包或 B-模式数据类型，使用 RF 数据来训练深度神经网络已经吸引了各种各样的关注。然而，RF 数据具有很大的波动和动态范围。大多数先前的文献中的研究都采用了传统的数据Normalization技术。我们证明了这些技术不适用于 RF 数据Normalization，并提出了基于每个图像的个体标准化方法，可以更好地利用数据。我们在相位偏移 corrections 任务中比较了传统和我们提议的Normalization方法，并证明了后者可以提高训练的模型通用性。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound"><a href="#Frequency-Space-Prediction-Filtering-for-Phase-Aberration-Correction-in-Plane-Wave-Ultrasound" class="headerlink" title="Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound"></a>Frequency-Space Prediction Filtering for Phase Aberration Correction in Plane-Wave Ultrasound</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11830">http://arxiv.org/abs/2308.11830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mostafa Sharifzadeh, Habib Benali, Hassan Rivaz</li>
<li>For: 本研究旨在解决ultrasound imaging中的图像质量下降问题，具体来说是应对phas aberration的影响。* Methods: 本研究使用了frequency-space prediction filtering（FXPF）技术来缓解phas aberration的影响。FXPF假设存在一个自回归（AR）模型，用于描述接收器元素上的信号。* Results: 研究发现，在深度较浅的情况下，使用固定频率AR模型可能会导致图像重建的性能下降。为了解决这个问题，研究提出了一种自适应频率AR模型，并评估了其效果使用对比度和总对比度评价指标。<details>
<summary>Abstract</summary>
Ultrasound imaging often suffers from image degradation stemming from phase aberration, which represents a significant contributing factor to the overall image degradation in ultrasound imaging. Frequency-space prediction filtering or FXPF is a technique that has been applied within focused ultrasound imaging to alleviate the phase aberration effect. It presupposes the existence of an autoregressive (AR) model across the signals received at the transducer elements and removes any components that do not conform to the established model. In this study, we illustrate the challenge of applying this technique to plane-wave imaging, where, at shallower depths, signals from more distant elements lose relevance, and a fewer number of elements contribute to image reconstruction. While the number of contributing signals varies, adopting a fixed-order AR model across all depths, results in suboptimal performance. To address this challenge, we propose an AR model with an adaptive order and quantify its effectiveness using contrast and generalized contrast-to-noise ratio metrics.
</details>
<details>
<summary>摘要</summary>
ultrasound imaging经常受到阶段偏移引起的图像强化效应，这是ultrasound imaging中图像强化效应的重要贡献因素。frequency-space prediction filtering或FXPF是一种在高精度ultrasound imaging中应用的技术，以解决阶段偏移效应。它假设在传感器元素上接收的信号存在autoregressive（AR）模型，并从不符合该模型的组件中除掉噪声。在这种研究中，我们描述了应用FXPF技术到平面波形成像中的挑战，深度较浅的情况下，较远的传感器元素的信号失去了 relevance，而一些元素只能为图像重建做出贡献。尽管参与图像重建的信号数量变化，采用固定阶数AR模型在所有深度下的结果是不佳。为解决这个挑战，我们提议一种AR模型，其阶数随深度变化，并使用对比度和通用对比度-噪声比例度量来衡量其效果。
</details></li>
</ul>
<hr>
<h2 id="WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters"><a href="#WS-SfMLearner-Self-supervised-Monocular-Depth-and-Ego-motion-Estimation-on-Surgical-Videos-with-Unknown-Camera-Parameters" class="headerlink" title="WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters"></a>WS-SfMLearner: Self-supervised Monocular Depth and Ego-motion Estimation on Surgical Videos with Unknown Camera Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11776">http://arxiv.org/abs/2308.11776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ange Lou, Jack Noble</li>
<li>for: 这个研究旨在建立一个自我超级vised的深度和镜头积极定位系统，能够预测精确的深度地图和镜头积极。</li>
<li>methods: 本研究使用了一种基于成本量的超级vised方法，并通过一种自动生成的类比对照方法来提供辅助的超级vised。</li>
<li>results: 实验结果显示，提案的方法可以改善镜头积极、深度估计和镜头内 Parameters 的预测精度。<details>
<summary>Abstract</summary>
Depth estimation in surgical video plays a crucial role in many image-guided surgery procedures. However, it is difficult and time consuming to create depth map ground truth datasets in surgical videos due in part to inconsistent brightness and noise in the surgical scene. Therefore, building an accurate and robust self-supervised depth and camera ego-motion estimation system is gaining more attention from the computer vision community. Although several self-supervision methods alleviate the need for ground truth depth maps and poses, they still need known camera intrinsic parameters, which are often missing or not recorded. Moreover, the camera intrinsic prediction methods in existing works depend heavily on the quality of datasets. In this work, we aimed to build a self-supervised depth and ego-motion estimation system which can predict not only accurate depth maps and camera pose, but also camera intrinsic parameters. We proposed a cost-volume-based supervision manner to give the system auxiliary supervision for camera parameters prediction. The experimental results showed that the proposed method improved the accuracy of estimated camera parameters, ego-motion, and depth estimation.
</details>
<details>
<summary>摘要</summary>
深度估计在手术视频中发挥重要作用，但创建深度图真实数据集在手术视频中具有许多挑战，包括手术场景中的不均匀亮度和噪声。因此，建立一个准确和可靠的自我超视导depth和摄像头自身运动估计系统在计算机视觉领域中受到更多的关注。虽然一些自我超视方法可以减少深度图和摄像头pose的需求，但它们仍需要已知的摄像头内参数，这些参数通常缺失或未记录。此外，现有的摄像头内参数预测方法仍然受到数据质量的限制。在这个工作中，我们目的是建立一个可以预测深度图、摄像头pose和摄像头内参数的自我超视depth和摄像头估计系统。我们提议一种基于cost volume的超视束来给系统 auxiliary supervision for camera parameters预测。实验结果表明，我们的方法可以改善摄像头参数、ego-动作和深度估计的准确性。
</details></li>
</ul>
<hr>
<h2 id="EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides"><a href="#EndoNet-model-for-automatic-calculation-of-H-score-on-histological-slides" class="headerlink" title="EndoNet: model for automatic calculation of H-score on histological slides"></a>EndoNet: model for automatic calculation of H-score on histological slides</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11562">http://arxiv.org/abs/2308.11562</a></li>
<li>repo_url: None</li>
<li>paper_authors: Egor Ushakov, Anton Naumov, Vladislav Fomberg, Polina Vishnyakova, Aleksandra Asaturova, Alina Badlaeva, Anna Tregubova, Evgeny Karpulevich, Gennady Sukhikh, Timur Fatkhudinov</li>
<li>for: 这个论文主要是为了提高检验病理slide的效率和准确性，使用计算机支持的方法来自动计算H-score。</li>
<li>methods: 该论文提出了一种基于神经网络的模型EndoNet，它包括两个主要部分：首先是一个检测模型，用于预测核心点的位置；其次是一个H-score模块，用于根据预测的核心点的平均像素值来计算H-score。</li>
<li>results: 该模型在1780个注解的块中训练和验证，并在测试集上达到了0.77的mAP。此外，该模型可以根据特定的专家或实验室来调整H-score的计算方式，从而提高了模型的可靠性和可重复性。<details>
<summary>Abstract</summary>
H-score is a semi-quantitative method used to assess the presence and distribution of proteins in tissue samples by combining the intensity of staining and percentage of stained nuclei. It is widely used but time-consuming and can be limited in accuracy and precision. Computer-aided methods may help overcome these limitations and improve the efficiency of pathologists' workflows. In this work, we developed a model EndoNet for automatic calculation of H-score on histological slides. Our proposed method uses neural networks and consists of two main parts. The first is a detection model which predicts keypoints of centers of nuclei. The second is a H-score module which calculates the value of the H-score using mean pixel values of predicted keypoints. Our model was trained and validated on 1780 annotated tiles with a shape of 100x100 $\mu m$ and performed 0.77 mAP on a test dataset. Moreover, the model can be adjusted to a specific specialist or whole laboratory to reproduce the manner of calculating the H-score. Thus, EndoNet is effective and robust in the analysis of histology slides, which can improve and significantly accelerate the work of pathologists.
</details>
<details>
<summary>摘要</summary>
“H-score”是一种半量化方法，用于评估组织样本中蛋白质的存在和分布。它广泛使用，但时间费时且准确性和精度有限。计算机助け方法可以帮助解决这些限制，提高病理师的工作效率。在这项工作中，我们开发了一个名为“EndoNet”的自动计算H-score方法。我们的提案方法使用神经网络，包括两个主要部分。第一部分是一个检测模型，预测核心点的位置。第二部分是H-score模块，使用预测的核心点的平均像素值来计算H-score的值。我们的模型在1780个注解的块中训练和验证，在测试集上达到了0.77 mAP。此外，模型可以根据特定的专家或整个实验室来调整计算H-score的方式，因此EndoNet是有效和可靠的 histology 板块分析工具，可以提高和加速病理师的工作。
</details></li>
</ul>
<hr>
<h2 id="Open-Set-Synthetic-Image-Source-Attribution"><a href="#Open-Set-Synthetic-Image-Source-Attribution" class="headerlink" title="Open Set Synthetic Image Source Attribution"></a>Open Set Synthetic Image Source Attribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11557">http://arxiv.org/abs/2308.11557</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengbang Fang, Tai D. Nguyen, Matthew C. Stamm</li>
<li>for: 本研究旨在开发一种基于度量学习的开放集成源归属分析方法，以检测和识别新未经见的图像生成器。</li>
<li>methods: 本研究使用度量学习来学习可转移的嵌入，以区分不同的图像生成器。首先将图像分配给候选生成器，然后根据图像与已知生成器学习的参考点的距离来判断是否来自新的生成器。</li>
<li>results: 经过一系列实验，本研究表明了该方法在开放集成源归属场景中能够准确地检测和识别新未经见的图像生成器。<details>
<summary>Abstract</summary>
AI-generated images have become increasingly realistic and have garnered significant public attention. While synthetic images are intriguing due to their realism, they also pose an important misinformation threat. To address this new threat, researchers have developed multiple algorithms to detect synthetic images and identify their source generators. However, most existing source attribution techniques are designed to operate in a closed-set scenario, i.e. they can only be used to discriminate between known image generators. By contrast, new image-generation techniques are rapidly emerging. To contend with this, there is a great need for open-set source attribution techniques that can identify when synthetic images have originated from new, unseen generators. To address this problem, we propose a new metric learning-based approach. Our technique works by learning transferrable embeddings capable of discriminating between generators, even when they are not seen during training. An image is first assigned to a candidate generator, then is accepted or rejected based on its distance in the embedding space from known generators' learned reference points. Importantly, we identify that initializing our source attribution embedding network by pretraining it on image camera identification can improve our embeddings' transferability. Through a series of experiments, we demonstrate our approach's ability to attribute the source of synthetic images in open-set scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eess.IV_2023_08_23/" data-id="clm0t8e2w00fkv7883dydhtmx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eessp.SP_2023_08_23" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/23/eessp.SP_2023_08_23/" class="article-date">
  <time datetime="2023-08-22T16:00:00.000Z" itemprop="datePublished">2023-08-23</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/23/eessp.SP_2023_08_23/">eessp.SP - 2023-08-23</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        
      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/23/eessp.SP_2023_08_23/" data-id="clm0t8e3000g2v78812lweg0e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.AI_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.AI_2023_08_22/">cs.AI - 2023-08-22 20:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
<li>for: 本研究探讨语言模型（LM）在视觉跨模态中的能力，特别是sound event detection（SED）领域。</li>
<li>methods: 我们提出了一种简洁的方法，通过对音频特征和文本特征的对应进行对齐，实现声音事件分类和时间位置的生成。该框架包括一个音频编码器、一个对应模块和一个独立的语言解码器。</li>
<li>results: 我们的模型可以准确地生成声音事件探测序列。与传统方法相比，我们的模型更加简洁和全面，因为它直接利用语言模型的 semantic 能力来生成序列。我们还对不同的解码模块进行了研究，以示timestamps capture和事件分类的效果。<details>
<summary>Abstract</summary>
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.
</details>
<details>
<summary>摘要</summary>
最近，语言模型（LM）在视觉交互领域的能力受到了越来越多的关注。在这篇论文中，我们进一步探索语言模型对声音事件检测（SED）的生成能力，超出视觉领域。我们提出了一种简洁的方法，将音频特征和文本特征进行对齐，以完成声音事件类型和时间位置的分类。该框架包括一个声音编码器、一个对应模块，将文本和音频特征的对应表示进行对齐，以及一个独立的语言解码器，从音频特征中生成时间序列和事件序列。相比于传统的方法，需要复杂的处理和尝试用有限的音频特征，我们的模型更简洁和全面，因为语言模型直接利用其语义能力来生成序列。我们 investigate了不同的解 Coupling模块，以示出对时间捕捉和事件分类的效果。评估结果显示，我们的方法可以准确地检测声音事件。
</details></li>
</ul>
<hr>
<h2 id="TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows"><a href="#TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows" class="headerlink" title="TrackFlow: Multi-Object Tracking with Normalizing Flows"></a>TrackFlow: Multi-Object Tracking with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11513">http://arxiv.org/abs/2308.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara</li>
<li>for: 提高多对象跟踪的性能，尤其是在多模态 Setting 中。</li>
<li>methods: 使用深度概率模型来计算候选对应关系的可能性，以提高跟踪-by-检测算法的性能。</li>
<li>results: 在 simulate 和实际 benchmark 上进行了实验，显示了我们的方法可以提高跟踪-by-检测算法的性能。<details>
<summary>Abstract</summary>
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.
</details>
<details>
<summary>摘要</summary>
隐身多目标跟踪领域最近又有新的关注，旧的schema tracking-by-detection，因为它的简单性和强制约束，不需要复杂的设计和痛苦照顾 tracking-by-attention 方法。在这个视图下，我们想扩展 tracking-by-detection 到多模式设定，其中需要从不同的信息源（例如，2D 运动指示、视觉特征和姿态估计）计算总成本。更加准确地说，我们采用了一个实验研究，其中有一个粗略的3D 信息估计也可以与传统的 метри（例如，IoU）一起使用。为了实现这一点，现有的方法通常采用 either simple rules or complex heuristics 来均衡每个成本的贡献。然而，i) 它们需要在保留集上精心调整特制的超参数，并 ii) 它们假设这些成本是独立的，而实际上不是。我们解决这些问题，是通过基于简洁概率形式ulation，它考虑候选关联的成本为负极log-概率的深度概率预测器，用于模型候选关联的条件联合概率分布。我们的实验，在 Both simulated 和 real  benchmarks 上进行，显示了我们的方法能够一致提高许多 tracking-by-detection 算法的性能。
</details></li>
</ul>
<hr>
<h2 id="User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features"><a href="#User-Identity-Linkage-in-Social-Media-Using-Linguistic-and-Social-Interaction-Features" class="headerlink" title="User Identity Linkage in Social Media Using Linguistic and Social Interaction Features"></a>User Identity Linkage in Social Media Using Linguistic and Social Interaction Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11684">http://arxiv.org/abs/2308.11684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Despoina Chatzakou, Juan Soler-Company, Theodora Tsikrika, Leo Wanner, Stefanos Vrochidis, Ioannis Kompatsiaris</li>
<li>for: 防止社交媒体上的负面内容的 spreadof and retain online identity</li>
<li>methods: 使用多个用户活动特征进行机器学习基于检测，以确定两个或多个虚拟标识是否属于同一个真实人</li>
<li>results: 在恶意和恐怖主义相关的推特内容中，模型的效果得到证明<details>
<summary>Abstract</summary>
Social media users often hold several accounts in their effort to multiply the spread of their thoughts, ideas, and viewpoints. In the particular case of objectionable content, users tend to create multiple accounts to bypass the combating measures enforced by social media platforms and thus retain their online identity even if some of their accounts are suspended. User identity linkage aims to reveal social media accounts likely to belong to the same natural person so as to prevent the spread of abusive/illegal activities. To this end, this work proposes a machine learning-based detection model, which uses multiple attributes of users' online activity in order to identify whether two or more virtual identities belong to the same real natural person. The models efficacy is demonstrated on two cases on abusive and terrorism-related Twitter content.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions"><a href="#Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions" class="headerlink" title="Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions"></a>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Pezeshkpour, Estevam Hruschka<br>for: 这 paper 探讨了 Large Language Models (LLMs) 在不同的 NLP 任务中表现的稳定性问题，特别是在多选问题上。methods: 作者们使用了多种方法来 investigate LLMs 的不稳定性，包括对选项的重新排序和几个示例的尝试。results: 研究发现，当选项的顺序发生变化时，LLMs 的表现会受到很大的影响，表现差异可达 13% 到 75% 不同的benchmark上。通过 detailed 分析，作者们 conjecture 这种不稳定性源于 LLMs 对最佳选项的不确定性，并且特定的选项位置可能会帮助模型更准确地预测最佳选项。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection"><a href="#Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection" class="headerlink" title="Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection"></a>Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/broad-openood">https://github.com/servicenow/broad-openood</a></li>
<li>paper_authors: Charles Guille-Escuret, Pierre-André Noël, Ioannis Mitliagkas, David Vazquez, Joao Monteiro</li>
<li>for: 本研究旨在提高部署机器学习系统的可靠性，通过开发检测出现在训练集之外的输入（Out-of-distribution，OOD）方法。</li>
<li>methods: 本研究对现有的OOD检测方法进行了评估，并发现这些方法只能够有效地检测未知的类型，而对其他类型的分布转移表现不一致。为解决这个问题，我们提出了一种基于生成模型的ensemble方法，可以提供更一致和全面的OOD检测解决方案。</li>
<li>results: 我们的研究发现，现有的OOD检测方法在不同类型的分布转移中的性能不一致，而我们的ensemble方法可以提供更高的可靠性和敏感性。我们还发布了一个名为BROAD（Benchmarking Resilience Over Anomaly Diversity）的数据集，以便评估OOD检测方法的性能。<details>
<summary>Abstract</summary>
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.
</details>
<details>
<summary>摘要</summary>
提高机器学习系统部署时的可靠性通常涉及到开发检测出idanormal inputs的方法。然而，现有研究通常只关注 absent classes 中的样本，忽视其他类型的可能性 Distribution Shift。这种限制 reduce了这些方法在实际应用中的适用性，因为系统会遇到各种异常输入。在这种研究中，我们分类ified five distinct types of distribution shifts, and critically evaluated the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Revisiting-column-generation-based-matheuristic-for-learning-classification-trees"><a href="#Revisiting-column-generation-based-matheuristic-for-learning-classification-trees" class="headerlink" title="Revisiting column-generation-based matheuristic for learning classification trees"></a>Revisiting column-generation-based matheuristic for learning classification trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11477">http://arxiv.org/abs/2308.11477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krooonal/col_gen_estimator">https://github.com/krooonal/col_gen_estimator</a></li>
<li>paper_authors: Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi</li>
<li>for: 这篇论文目的是提高分类问题的解决方法，特别是在机器学习领域中使用决策树模型。</li>
<li>methods: 该论文使用的方法是基于列生成的规则逻辑，以提高分类问题的解决效率和可扩展性。</li>
<li>results: 对于多类分类问题，该方法可以减少数据点数量，并使用数据依赖的约束来提高分类质量。 computational results表明，这些改进可以提高解决效率。<details>
<summary>Abstract</summary>
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints. We conclude by presenting computational results that show that these modifications result in better scalability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis"><a href="#IT3D-Improved-Text-to-3D-Generation-with-Explicit-View-Synthesis" class="headerlink" title="IT3D: Improved Text-to-3D Generation with Explicit View Synthesis"></a>IT3D: Improved Text-to-3D Generation with Explicit View Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11473">http://arxiv.org/abs/2308.11473</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/buaacyw/it3d-text-to-3d">https://github.com/buaacyw/it3d-text-to-3d</a></li>
<li>paper_authors: Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin</li>
<li>for: 本研究旨在提高文本到3D图像转换技术，并使用大型文本到图像扩散模型（LDM）提取知识。</li>
<li>methods: 本研究使用图像到图像管道，利用LDM生成高质量多视图图像，并通过Diffusion-GAN双向训练策略来引导3D模型训练。</li>
<li>results: 实验结果表明，本方法比基eline方法有更高的质量和精度，能够更好地解决文本到3D图像转换中的一些问题，如过度满、缺乏细节和不实际的输出。<details>
<summary>Abstract</summary>
Recent strides in Text-to-3D techniques have been propelled by distilling knowledge from powerful large text-to-image diffusion models (LDMs). Nonetheless, existing Text-to-3D approaches often grapple with challenges such as over-saturation, inadequate detailing, and unrealistic outputs. This study presents a novel strategy that leverages explicitly synthesized multi-view images to address these issues. Our approach involves the utilization of image-to-image pipelines, empowered by LDMs, to generate posed high-quality images based on the renderings of coarse 3D models. Although the generated images mostly alleviate the aforementioned issues, challenges such as view inconsistency and significant content variance persist due to the inherent generative nature of large diffusion models, posing extensive difficulties in leveraging these images effectively. To overcome this hurdle, we advocate integrating a discriminator alongside a novel Diffusion-GAN dual training strategy to guide the training of 3D models. For the incorporated discriminator, the synthesized multi-view images are considered real data, while the renderings of the optimized 3D models function as fake data. We conduct a comprehensive set of experiments that demonstrate the effectiveness of our method over baseline approaches.
</details>
<details>
<summary>摘要</summary>
最近的文本到3D技术发展受到了大型文本到图像扩散模型（LDM）的知识储存的推动。然而，现有的文本到3D方法通常会遇到过度饱和、不够细节和不实际的输出等问题。本研究提出了一种新的策略，利用可控多视图图像来解决这些问题。我们的方法是利用图像到图像管道，利用LDM来生成基于粗糙3D模型的高质量poses图像。虽然生成的图像大多消除了以上问题，但是由于大扩散模型的生成性，仍然存在视角不一致和内容差异等问题。为解决这个障碍，我们提议在3D模型训练中添加一个判别器，并采用Diffusion-GAN双向训练策略来引导3D模型的训练。对于添加的判别器，生成的多视图图像被视为真实数据，而 renderings of 优化的3D模型则被视为假数据。我们进行了一系列的实验，证明了我们的方法在基础方法上表现更高效。
</details></li>
</ul>
<hr>
<h2 id="Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI"><a href="#Dynamic-Open-Vocabulary-Enhanced-Safe-landing-with-Intelligence-DOVESEI" class="headerlink" title="Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)"></a>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11471">http://arxiv.org/abs/2308.11471</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mistlab/dovesei">https://github.com/mistlab/dovesei</a></li>
<li>paper_authors: Haechan Mark Bong, Rongge Zhang, Ricardo de Azambuja, Giovanni Beltrame</li>
<li>for: 本研究目标是为城市空中机器人开发安全降落。</li>
<li>methods: 本研究使用视 servoing 技术，利用开放词汇图像分割，适应不同场景，并且不需要大量数据更新内部模型。</li>
<li>results: 实验表明，该系统可以在100米高度下成功执行降落动作，且通过引入动态专注机制，提高降落成功率。<details>
<summary>Abstract</summary>
This work targets what we consider to be the foundational step for urban airborne robots, a safe landing. Our attention is directed toward what we deem the most crucial aspect of the safe landing perception stack: segmentation. We present a streamlined reactive UAV system that employs visual servoing by harnessing the capabilities of open vocabulary image segmentation. This approach can adapt to various scenarios with minimal adjustments, bypassing the necessity for extensive data accumulation for refining internal models, thanks to its open vocabulary methodology. Given the limitations imposed by local authorities, our primary focus centers on operations originating from altitudes of 100 meters. This choice is deliberate, as numerous preceding works have dealt with altitudes up to 30 meters, aligning with the capabilities of small stereo cameras. Consequently, we leave the remaining 20m to be navigated using conventional 3D path planning methods. Utilizing monocular cameras and image segmentation, our findings demonstrate the system's capability to successfully execute landing maneuvers at altitudes as low as 20 meters. However, this approach is vulnerable to intermittent and occasionally abrupt fluctuations in the segmentation between frames in a video stream. To address this challenge, we enhance the image segmentation output by introducing what we call a dynamic focus: a masking mechanism that self adjusts according to the current landing stage. This dynamic focus guides the control system to avoid regions beyond the drone's safety radius projected onto the ground, thus mitigating the problems with fluctuations. Through the implementation of this supplementary layer, our experiments have reached improvements in the landing success rate of almost tenfold when compared to global segmentation. All the source code is open source and available online (github.com/MISTLab/DOVESEI).
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning"><a href="#Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning" class="headerlink" title="Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning"></a>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11464">http://arxiv.org/abs/2308.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. -H. Ngai</li>
<li>for: 提高模型不同的 Federated Learning 方法处理系统不同性能的能力</li>
<li>methods: 利用内部交叉层导数，不需要客户端之间的通信，可以增强深层导数的相似性</li>
<li>results: 实验结果证明 InCo Aggregation 的效果，显示内部交叉层导数是提高性能的有效途径<details>
<summary>Abstract</summary>
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）在实际应用中遇到系统多样性的挑战。为了增强大多数模型相似的FL方法在处理系统多样性的能力，我们提出了一个训练方案，可以将其扩展到处理这个挑战。在这篇论文中，我们开始我们的研究，进行了详细的探索Homogeneous和Heterogeneous FL Setting中的三个关键观察：（1）客户端性能和层 similarity 之间的正相关，（2）在浅层较为高 similarity ，而深层较低 similarity，（3）在各层 Similarity 中更平滑的梯度分布，这些观察可以帮助我们更好地理解FL系统的多样性问题。基于这些观察，我们提出了InCo Aggregation，利用服务器模型中的内部交叉层梯度，把深层层梯度与浅层梯度混合，以增强深层层梯度的相似性，不需要客户端之间的额外交流。此外，我们的方法可以与模型相似的FL方法，如FedAvg、FedProx、FedNova、Scaffold和MOON相容，以扩展它们的能力，处理系统多样性。实际实验结果显示，InCo Aggregation 具有很好的效果，强调了内部交叉层梯度作为提高FL系统多样性性能的有力之路。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Self-Supervised-Representation-Learning"><a href="#A-Survey-on-Self-Supervised-Representation-Learning" class="headerlink" title="A Survey on Self-Supervised Representation Learning"></a>A Survey on Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11455">http://arxiv.org/abs/2308.11455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/esvit">https://github.com/microsoft/esvit</a></li>
<li>paper_authors: Tobias Uelwer, Jan Robine, Stefan Sylvius Wagner, Marc Höftmann, Eric Upschulte, Sebastian Konietzny, Maike Behrendt, Stefan Harmeling</li>
<li>for: 本文提供了一个总结性的综述，探讨了一些无监督学习方法，用于学习图像表示。这些表示可以用于下游任务，如分类或物体检测。</li>
<li>methods: 本文使用了一些无监督学习方法，包括自适应卷积神经网络、自适应层次神经网络和卷积神经网络。</li>
<li>results: 根据Literature review，这些方法在下游任务中表现非常出色，与监督学习方法相当。Here’s the translation in English:</li>
<li>for: This paper provides a comprehensive review of methods for learning image representations without supervision, which can be used in downstream tasks such as classification or object detection.</li>
<li>methods: The paper uses several unsupervised learning methods, including autoencoders, self-attention mechanisms, and convolutional neural networks.</li>
<li>results: According to the literature review, these methods have performed extremely well in downstream tasks, comparable to supervised learning methods.<details>
<summary>Abstract</summary>
Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
</details>
<details>
<summary>摘要</summary>
学习有意义的表示是现代机器学习领域中的核心任务之一。最近，许多无监督学习方法被引入，可以学习图像表示。这些表示可以在下游任务中使用，如分类或物体检测。这些无监督学习方法的表示质量与监督学习相似，但无需标注图像。本文提供了这些方法的统一notation，指出这些方法之间的相似性和差异，并提出了这些方法的分类方式。此外，我们的survey还summarized了Literature中最近的实验结果，并进行了meta-study。本文为研究者和实践者提供了进入无监督学习领域的开始点。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Convergence-guarantee-for-consistency-models"><a href="#Convergence-guarantee-for-consistency-models" class="headerlink" title="Convergence guarantee for consistency models"></a>Convergence guarantee for consistency models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11449">http://arxiv.org/abs/2308.11449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlong Lyu, Zhitang Chen, Shoubo Feng</li>
<li>for: 这 paper 的目的是为Consistency Models (CMs) 提供首次一致性保证，这种一步生成模型可以生成与Diffusion Models相同的样本。</li>
<li>methods: 这 paper 使用了基本的score-matching error assumption, consistency error assumption和数据分布的smoothness假设，以确保CMs 可以效率地从任何现实数据分布中采样，并且采样Error小于$W_2$.</li>
<li>results: 这 paper 的结果包括：(1) 对于$L^2$-accurate score和consistency假设，CMs 可以在一步中采样到任何现实数据分布，并且采样Error scales polynomially in all parameters; (2) 不需要强制对数据分布的假设，如log-Sobelev inequality; (3) 可以further reduce the error by using Multistep Consistency Sampling procedure.<details>
<summary>Abstract</summary>
We provide the first convergence guarantees for the Consistency Models (CMs), a newly emerging type of one-step generative models that can generate comparable samples to those generated by Diffusion Models. Our main result is that, under the basic assumptions on score-matching errors, consistency errors and smoothness of the data distribution, CMs can efficiently sample from any realistic data distribution in one step with small $W_2$ error. Our results (1) hold for $L^2$-accurate score and consistency assumption (rather than $L^\infty$-accurate); (2) do note require strong assumptions on the data distribution such as log-Sobelev inequality; (3) scale polynomially in all parameters; and (4) match the state-of-the-art convergence guarantee for score-based generative models (SGMs). We also provide the result that the Multistep Consistency Sampling procedure can further reduce the error comparing to one step sampling, which support the original statement of "Consistency Models, Yang Song 2023". Our result further imply a TV error guarantee when take some Langevin-based modifications to the output distributions.
</details>
<details>
<summary>摘要</summary>
我们提供了一些一步生成模型（CM）的协调保证，这是一种最近崛起的一种生成模型，可以生成与演化模型（Diffusion Models）相似的样本。我们的主要结果是，假设score-matching error、consistency error和资料分布的平滑性满足某些基本假设，则CM可以将任何现实的资料分布 efficiently sampled in one step with small $W_2$ error。我们的结果包括：1. 对于$L^2$-accurate score和consistency假设（而不是$L^\infty$-accurate）;2. 不需要对于资料分布的强则假设，如log-Sobelev不等式;3. 随所有参数的度量 polynomially scale;4. 与state-of-the-art score-based生成模型（SGMs）的协调保证相符。我们还提供了一个Multistep Consistency Sampling程序，可以降低比一步样本的错误，这支持原始的“Consistency Models, Yang Song 2023”的声明。我们的结果进一步显示了一个TV错误保证，当将一些Langevin-based modifications套用到输出分布时。
</details></li>
</ul>
<hr>
<h2 id="Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification"><a href="#Aspect-oriented-Opinion-Alignment-Network-for-Aspect-Based-Sentiment-Classification" class="headerlink" title="Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification"></a>Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11447">http://arxiv.org/abs/2308.11447</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aone-nlp/absa-aoan">https://github.com/aone-nlp/absa-aoan</a></li>
<li>paper_authors: Xueyi Liu, Rui Hou, Yanglei Gan, Da Luo, Changlin Li, Xiaojun Shi, Qiao Liu</li>
<li>for: 这篇论文目的是提出一种新的方法来解决在多个方面的文本分析中存在的semantic mismatch问题，以提高 Fine-grained sentiment analysis 的精度。</li>
<li>methods: 该方法使用了一种新的Aspect-oriented Opinion Alignment Network (AOAN)，包括一个邻域span增强模块和一个多元视角注意机制，以强调对Opinion words和对应的方面的上下文关系。</li>
<li>results: 实验结果表明，该模型在三个标准数据集上达到了领域的最佳效果，代表着该方法在 Fine-grained sentiment analysis 中的成功应用。<details>
<summary>Abstract</summary>
Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the given aspect. Extensive experiments on three benchmark datasets demonstrate that our model achieves state-of-the-art results. The source code is available at https://github.com/AONE-NLP/ABSA-AOAN.
</details>
<details>
<summary>摘要</summary>
<SYS>    <TRANSLATE_TEXT>        非常详细的 sentiment 分析中，尤其是 aspect-based sentiment classification，目标是根据不同的上下文来预测具体的 sentiment 偏好。先前的研究已经做出了很大的进步，通过使用注意力机制来提取不同的 opinion 词。但是，一个持续的挑战是如何有效地处理 semantic 匹配问题，这些问题来自于注意力机制不够地对 opinion 词和对应的 aspect 进行匹配。为了解决这个问题，我们提出了一种新的 Aspect-oriented Opinion Alignment Network (AOAN)，用于捕捉不同的 opinion 词和 aspect 之间的上下文关系。</TRANSLATE_TEXT></SYS>Here's the translation in Traditional Chinese:<SYS>    <TRANSLATE_TEXT>        非常细致的 sentiment 分析中，尤其是 aspect-based sentiment classification，目标是根据不同的上下文来预测具体的 sentiment 偏好。先前的研究已经做出了很大的进步，通过使用注意力机制来提取不同的 opinion 词。但是，一个持续的挑战是如何有效地处理 semantic 匹配问题，这些问题来自于注意力机制不够地对 opinion 词和对应的 aspect 进行匹配。为了解决这个问题，我们提出了一种新的 Aspect-oriented Opinion Alignment Network (AOAN)，用于捕捉不同的 opinion 词和 aspect 之间的上下文关系。</TRANSLATE_TEXT></SYS>Note that the translation is in Simplified Chinese, as requested. If you would like the translation in Traditional Chinese instead, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data"><a href="#Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data" class="headerlink" title="Exploration of Rashomon Set Assists Explanations for Medical Data"></a>Exploration of Rashomon Set Assists Explanations for Medical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11446">http://arxiv.org/abs/2308.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarzyna Kobylińska, Mateusz Krzyziński, Rafał Machowicz, Mariusz Adamek, Przemysław Biecek</li>
<li>For: This paper aims to address the problem of relying solely on performance metrics in machine learning modeling, particularly in medical and healthcare studies, by introducing a novel process to explore Rashomon set models.* Methods: The proposed approach uses the $\texttt{Rashomon_DETECT}$ algorithm to identify the most different models within the Rashomon set, and the Profile Disparity Index (PDI) to quantify differences in variable effects among models.* Results: The approach is demonstrated on a foundational case study of predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients, as well as on other medical data sets, showing its effectiveness and versatility in various contexts.Here are the three points in Simplified Chinese:* For: 这篇论文目的是解决机器学习模型选择过程中围绕性能指标偏重的问题，尤其在医疗和健康研究中，以获得更多的有价值信息。* Methods: 该方法使用 $\texttt{Rashomon_DETECT}$ 算法 Identify Rashomon set 中最为不同的模型，并使用 Profile Disparity Index (PDI) 量化变量效果之间的差异。* Results: 该方法在针对 Hemophagocytic lymphohistiocytosis (HLH) 患者存活预测的基本案例研究中，以及其他医疗数据集中，得到了有效和多样的结果。<details>
<summary>Abstract</summary>
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorithm. This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques. To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis. To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study. Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts.
</details>
<details>
<summary>摘要</summary>
传统的机器学习模型选择过程是通过选择最大化一个选择的性能指标来完成的。然而，这种方法会抛弃更深入的模型分析。特别在医疗和健康研究中，目标不仅是预测，还是生成有价值的理解。只靠性能指标来结论可能导致误导或不完整的结论。这种问题特别存在于处理一组性能几乎最大的模型集合，称为“Rashomon集”。这个集合可能很多，其中包含描述数据不同方式的模型，需要全面的分析。本文提出了一种新的模型探索过程，扩展传统模型选择策略。其核心是在Rashomon集中 identific 最不同的模型，由我们引入的 $\texttt{Rashomon\_DETECT}$ 算法实现。这个算法比较使用 eXplainable Artificial Intelligence（XAI）技术生成的变量值预测依赖的profile。为了量化不同模型中变量效应的差异，我们引入了 Profile Disparity Index（PDI），基于函数数据分析中的度量。我们通过应用这种方法在 Hemophagocytic lymphohistiocytosis（HLH）患者的存活预测中进行了示例，并将其应用于其他医疗数据集，以示其多样性和可用性。
</details></li>
</ul>
<hr>
<h2 id="Inferring-gender-from-name-a-large-scale-performance-evaluation-study"><a href="#Inferring-gender-from-name-a-large-scale-performance-evaluation-study" class="headerlink" title="Inferring gender from name: a large scale performance evaluation study"></a>Inferring gender from name: a large scale performance evaluation study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12381">http://arxiv.org/abs/2308.12381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kriste Krstovski, Yao Lu, Ye Xu</li>
<li>for: 这个论文主要目的是为了对名称到性别推断的算法和软件产品进行大规模性能评估，以及提出两种新的混合方法以实现更高的性能。</li>
<li>methods: 本文使用了多个大量注释的名称数据集来进行分析，并提出了两种新的混合方法。</li>
<li>results: 研究发现现有方法中的任何一种都无法在所有情况下达到最佳性能，而两种新提出的混合方法均可以在所有情况下实现更高的性能。<details>
<summary>Abstract</summary>
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Nevertheless, the existing approaches have yet to be systematically evaluated and compared, making it challenging to determine the optimal approach for future research. In this work, we conducted a large scale performance evaluation of existing approaches for name-to-gender inference. Analysis are performed using a variety of large annotated datasets of names. We further propose two new hybrid approaches that achieve better performance than any single existing approach.
</details>
<details>
<summary>摘要</summary>
人的性别信息是科学研究中不可或缺的重要信息，包括医学、社会学、政治科学和经济学等领域。然而，随着大数据的普及，性别信息越来越难以获得。在这些情况下，研究人员需要根据可用的信息进行性别推断，主要是根据人名。虽然从名字中推断性别可能会附带一些伦理问题，但由于现有的可行方法缺乏，研究人员需要采用这些方法以实现研究目标。在全球范围内，这些方法已经广泛应用于大学、企业、政府和非政府组织中。然而，现有的方法尚未得到系统性的评估和比较，这使得未来研究中选择最佳方法仍然存在挑战。在这项工作中，我们进行了大规模性能评估现有的名字到性别推断方法。分析使用了多种大量注释的名字数据集。此外，我们还提出了两种新的混合方法，其性能更高于任何单独的现有方法。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Large-Language-Model-based-Autonomous-Agents"><a href="#A-Survey-on-Large-Language-Model-based-Autonomous-Agents" class="headerlink" title="A Survey on Large Language Model based Autonomous Agents"></a>A Survey on Large Language Model based Autonomous Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11432">http://arxiv.org/abs/2308.11432</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paitesanshi/llm-agent-survey">https://github.com/paitesanshi/llm-agent-survey</a></li>
<li>paper_authors: Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen</li>
<li>for: 本研究准备了一份总结LLM基于自主代理的研究，包括LLM基于代理的构建、应用领域和评价策略等方面。</li>
<li>methods: 本研究使用了大量网络知识获得的大语言模型(LLM)，并提出了一个统一框架来涵盖大多数之前的工作。</li>
<li>results: 本研究通过对LLM基于代理的各种应用领域和评价策略的总结，提出了一些挑战和未来方向，并将相关参考文献存储在<a target="_blank" rel="noopener" href="https://github.com/Paitesanshi/LLM-Agent-Survey%E4%B8%AD%E3%80%82">https://github.com/Paitesanshi/LLM-Agent-Survey中。</a><details>
<summary>Abstract</summary>
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework that encompasses a majority of the previous work. Additionally, we provide a summary of the various applications of LLM-based AI agents in the domains of social science, natural science, and engineering. Lastly, we discuss the commonly employed evaluation strategies for LLM-based AI agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository for the related references at https://github.com/Paitesanshi/LLM-Agent-Survey.
</details>
<details>
<summary>摘要</summary>
自主代理已经是学术界的一个主要研究话题，早期的研究通常是在隔离环境中训练有限知识的代理，这与人类学习过程不同，导致代理做出的决策困难达到人类水平。然而，随着互联网知识的掌握，大型自然语言模型（LLM）在实现人类智能水平方面表现出了很好的潜力。这导致了对自主代理基于 LLM 的研究的快速增长。为了挖掘 LLM 的潜力，研究者们设计了多种特定应用场景的代理建模。在这篇文章中，我们提供了一份系统性的评论，涵盖了这些研究的大部分。我们更加关注 LLM 基于代理的建模，并提出了一个统一框架，覆盖了大多数前期工作。此外，我们还提供了自然科学、社会科学和工程等领域 LLM 基于 AI 代理的多种应用案例。最后，我们讨论了对 LLM 基于 AI 代理的评价策略，并根据前期研究提出了一些挑战和未来方向。为了保持这一领域的报道和不断更新我们的评论，我们在 GitHub 上建立了一个参考库，可以在 https://github.com/Paitesanshi/LLM-Agent-Survey 中找到。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework"><a href="#A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework" class="headerlink" title="A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework"></a>A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11676">http://arxiv.org/abs/2308.11676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Shuai Fu, Huiyan Sun</li>
<li>for: The paper is written to provide a unified graphical framework for causal inference models based on the Potential Outcome Framework (POF), and to analyze the influence of non-confounding covariates on the inference performance of these models.</li>
<li>methods: The paper uses a graphical framework to present the underlying principles of causal inference models based on the POF, and conducts extensive experiments on synthetic datasets to validate the theoretical conclusions.</li>
<li>results: The paper finds that the optimal scenario for eliminating confounding bias is for the covariates to exclusively encompass confounders, and that adjustment variables contribute to more accurate inferences in the task of inferring counterfactual outcomes.<details>
<summary>Abstract</summary>
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quantitatively analyze the extent to which the inference performance of CIMs-B-POF is influenced when incorporating various types of non-confounding covariates, such as instrumental variables, mediators, colliders, and adjustment variables. The key findings are: in the task of eliminating confounding bias, the optimal scenario is for the covariates to exclusively encompass confounders; in the subsequent task of inferring counterfactual outcomes, the adjustment variables contribute to more accurate inferences. Furthermore, extensive experiments conducted on synthetic datasets consistently validate these theoretical conclusions.
</details>
<details>
<summary>摘要</summary>
Potential Outcome Framework (POF) 在 causal inference 领域扮演着重要的角色。大多数基于 POF 的 causal inference 模型 (CIMs-B-POF) 是为了消除干扰偏见而设计的，默认假设是 Confounding Covariates 假设，即 covariates 仅仅包含干扰因素。然而，在实践中保持 Confounding Covariates 假设是困难的，特别是处理高维 covariates 时。虽然一些方法已经被提出来分解 covariates 的不同组成部分，然而对非干扰 covariates 被视为干扰因素的后果仍然不清楚。这种不确定性在实践中应用 CIMs-B-POF 时可能存在风险。在这篇论文中，我们提出了一个统一的图形 Framework  для CIMs-B-POF，这有助于更好地理解这些模型的基本原理。使用这个图形 Framework，我们量化分析了在不同类型的非干扰 covariates 存在时，CIMs-B-POF 的推理性能是如何受影响的。我们发现，在消除干扰偏见的任务中，理想的情况是 covariates 仅仅包含干扰因素；在后续的对 counterfactual 结果进行推理任务中，调整变量对更准确的推理做出了贡献。此外，我们在 synthetic 数据上进行了广泛的实验，并 consistently 验证了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block"><a href="#AIxArtist-A-First-Person-Tale-of-Interacting-with-Artificial-Intelligence-to-Escape-Creative-Block" class="headerlink" title="AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block"></a>AIxArtist: A First-Person Tale of Interacting with Artificial Intelligence to Escape Creative Block</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11424">http://arxiv.org/abs/2308.11424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Makayla Lewis</li>
<li>for: 这篇论文探讨了人工智能（AI）如何支持艺术创作，以及在艺术创作过程中AI的可追溯性。</li>
<li>methods: 本论文采用了人工智能工具HIS、ChatGPT和Midjourney，进行了一些实验和探索，以探索AI如何支持艺术创作。</li>
<li>results: 本论文发现了一些关键问题，包括创作过程的透明性、作品的起源和伦理问题，以及创作是 copying 还是灵感？这些问题需要进一步的讨论和探索。<details>
<summary>Abstract</summary>
The future of the arts and artificial intelligence (AI) is promising as technology advances. As the use of AI in design becomes more widespread, art practice may not be a human-only art form and could instead become a digitally integrated experience. With enhanced creativity and collaboration, arts and AI could work together towards creating artistic outputs that are visually appealing and meet the needs of the artist and viewer. While it is uncertain how far the integration will go, arts and AI will likely influence one another. This workshop pictorial puts forward first-person research that shares interactions between an HCI researcher and AI as they try to escape the creative block. The pictorial paper explores two questions: How can AI support artists' creativity, and what does it mean to be explainable in this context? HIs, ChatGPT and Midjourney were engaged; the result was a series of reflections that require further discussion and explorations in the XAIxArts community: Transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.
</details>
<details>
<summary>摘要</summary>
This workshop pictorial presents first-person research that explores the interactions between an HCI researcher and AI as they try to escape creative blocks. The pictorial paper examines two questions: how can AI support artists' creativity, and what does it mean to be explainable in this context? The research involved engaging with AI models such as ChatGPT and Midjourney, leading to a series of reflections that require further discussion and exploration in the XAIxArts community. These reflections include transparency of attribution, the creation process, ethics of asking, and inspiration vs copying.
</details></li>
</ul>
<hr>
<h2 id="TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search"><a href="#TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search" class="headerlink" title="TurboViT: Generating Fast Vision Transformers via Generative Architecture Search"></a>TurboViT: Generating Fast Vision Transformers via Generative Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Wong, Saad Abbasi, Saeejith Nair</li>
<li>for: 这个研究的目的是实现高通过率且低 computation complexity的类比视觉 Transformer 架构设计。</li>
<li>methods: 这个研究使用了 Generative Architecture Search (GAS) 来生成高效的类比视觉 Transformer 架构设计，并且将注意力集中在面精度和 Q-pooling 设计模式上。</li>
<li>results: TurboViT 架构设计在 ImageNet-1K 数据集上实现了比较高的精度和低的 computation complexity，与其他 10 个现有的高效类比视觉 Transformer 网络架构设计相比。 Inference 延误和批处处理时间都表现出色，在低延误场景下，TurboViT 的延误时间比 FasterViT-0 低了 &gt;3.21 倍，而且对 batch 处理也表现出 &gt;3.18 倍的提高。<details>
<summary>Abstract</summary>
Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47$\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (>3.4$\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (>3.21$\times$ lower latency and >3.18$\times$ higher throughput compared to FasterViT-0 for low-latency scenario). These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios.
</details>
<details>
<summary>摘要</summary>
视transformer在近年来的视觉任务中表现出了前所未有的水平。然而，这些网络架构的建筑和计算复杂性使得它们在实际应用中高速、低内存要求下部署困难。因此，有一些研究是设计高效的视transformer架构。在这项研究中，我们通过生成式建筑搜索（GAS）来生成高效的视transformer架构设计，以达到精度和建筑计算效率的平衡。通过这个生成过程，我们创造了TurboViT，一种高效的层次视transformer架构设计，基于面积注意力和Q-Pooling设计模式。TurboViT架构设计的建筑计算复杂性比FasterViT-0大于2.47倍，计算复杂性比MobileViT2-2.0大于3.4倍，同时精度相同。此外，TurboViT在低延迟和批处理场景中表现出了优秀的执行时间和 Throughput，比FasterViT-0在低延迟场景下执行时间大于3.21倍，比MobileViT2-2.0在批处理场景下执行时间大于3.18倍。这些优秀的结果表明了利用生成式建筑搜索生成高效的transformer架构设计的有效性。
</details></li>
</ul>
<hr>
<h2 id="Tensor-Regression"><a href="#Tensor-Regression" class="headerlink" title="Tensor Regression"></a>Tensor Regression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11419">http://arxiv.org/abs/2308.11419</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tensorly/torch">https://github.com/tensorly/torch</a></li>
<li>paper_authors: Jiani Liu, Ce Zhu, Zhen Long, Yipeng Liu</li>
<li>for:  This paper is written for students, researchers, and practitioners who work with high dimensional data and are interested in tensor-based regression analysis.</li>
<li>methods: The paper provides a systematic study and analysis of tensor-based regression models and their applications, including a comprehensive review of existing methods, their core ideas, and theoretical characteristics.</li>
<li>results: The paper covers the basics of tensor-based regression, provides examples of how to use existing methods to solve specific regression tasks with multiway data, and discusses available datasets and software resources for efficient implementation.<details>
<summary>Abstract</summary>
Regression analysis is a key area of interest in the field of data analysis and machine learning which is devoted to exploring the dependencies between variables, often using vectors. The emergence of high dimensional data in technologies such as neuroimaging, computer vision, climatology and social networks, has brought challenges to traditional data representation methods. Tensors, as high dimensional extensions of vectors, are considered as natural representations of high dimensional data. In this book, the authors provide a systematic study and analysis of tensor-based regression models and their applications in recent years. It groups and illustrates the existing tensor-based regression methods and covers the basics, core ideas, and theoretical characteristics of most tensor-based regression methods. In addition, readers can learn how to use existing tensor-based regression methods to solve specific regression tasks with multiway data, what datasets can be selected, and what software packages are available to start related work as soon as possible. Tensor Regression is the first thorough overview of the fundamentals, motivations, popular algorithms, strategies for efficient implementation, related applications, available datasets, and software resources for tensor-based regression analysis. It is essential reading for all students, researchers and practitioners of working on high dimensional data.
</details>
<details>
<summary>摘要</summary>
“tensor regression”是数据分析和机器学习领域的一个关键领域，旨在探索变量之间的依赖关系，通常使用向量。随着神经成像、计算机视觉、气候学和社交网络等技术的发展，传统的数据表示方法面临了挑战。tensor是高维数据的自然表示方法。本书提供了tensor-based regression模型的系统性研究和分析，以及其在最近几年的应用。它分组和描述了现有的tensor-based regression方法，覆盖基础知识、核心思想和主要特征。此外，读者还可以了解如何使用现有的tensor-based regression方法来解决特定的多向数据回归任务，选择合适的数据集和使用哪些软件包来进行相关工作。“tensor regression”是高维数据处理的基础知识，是所有师生、研究人员和实践者都必须阅读的一本书。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores"><a href="#Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores" class="headerlink" title="Interpretable Distribution-Invariant Fairness Measures for Continuous Scores"></a>Interpretable Distribution-Invariant Fairness Measures for Continuous Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11375">http://arxiv.org/abs/2308.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ann-Kristin Becker, Oana Dumitrasc, Klaus Broelemann</li>
<li>for: 这个论文主要是为了扩展对连续分数的算法公平性评估方法。</li>
<li>methods: 该论文提出了一种基于沃氏距离的公平性评估方法，该方法可以快速计算并且对不同模型、数据集或时间点进行比较。</li>
<li>results: 研究人员通过实验表明，新提出的公平性评估方法可以更好地捕捉到不同群体之间的差异，并且可以比较不同的模型、数据集或时间点之间的偏见。<details>
<summary>Abstract</summary>
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss. Finally, we demonstrate their effectiveness through experiments on the most commonly used fairness benchmark datasets.
</details>
<details>
<summary>摘要</summary>
各种算法公平度量通常在二分类决策中被讨论。我们扩展了这种方法，以适应连续分数。目前，ROC基尼度量是为此目的提出的主要方法。其他现有方法受分布的影响很大，不适用于排名任务，或者其效果不能解释。我们提议一种不受分布影响的公平度量方法，基于温顿距离。我们的度量方法容易计算，适合量化和解释群体差异的强度以及不同模型、数据集、时间点之间的偏见。我们还 derivates了不同家族的现有公平度量方法之间的连接，并证明了我们提议的不受分布影响的公平度量方法在ROC基尼度量方法之上表现更好，因为它们更加明确，可以量化ROC基尼度量方法所过look的重要偏见。最后，我们通过使用最常用的公平性标准数据集进行实验，证明了它们的有效性。
</details></li>
</ul>
<hr>
<h2 id="How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation"><a href="#How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation" class="headerlink" title="How Much Temporal Long-Term Context is Needed for Action Segmentation?"></a>How Much Temporal Long-Term Context is Needed for Action Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11358">http://arxiv.org/abs/2308.11358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltcontext/ltcontext">https://github.com/ltcontext/ltcontext</a></li>
<li>paper_authors: Emad Bahrami, Gianpiero Francesca, Juergen Gall</li>
<li>for:  temporal action segmentation</li>
<li>methods:  transformer-based model with sparse attention</li>
<li>results:  best performance for temporal action segmentationHere’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文是为了解决视频中的时间动作分割问题而写的。</li>
<li>methods: 这篇论文使用了 transformer 模型，并使用了稀谱注意力来捕捉整个视频的上下文。</li>
<li>results: 实验结果表明，模型需要捕捉整个视频的上下文，才能达到最佳的时间动作分割性能。<details>
<summary>Abstract</summary>
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>模型长期视频上下文是多个细致任务的关键，包括时间动作 segmentation。一个有趣的问题是如何多少长期时间上下文是必需的 для最佳性能。虽然变换器可以模型视频的长期上下文，但这会对长视频计算昂贵。现有的时间动作 segmentation方法通过将时间卷积网络与自注意力组合在一起，但其性能受到当前视频上下文的限制。在这个工作中，我们尝试回答如何多少长期时间上下文是必需的 для时间动作 segmentation，我们提出了一种基于变换器的模型，通过稀疏注意力来捕捉整个视频的上下文。我们与当前领域的状态速算三个数据集进行比较，分别是50Salads、Breakfast和Assembly101。我们的实验结果表明，模型整个视频的上下文是获得最佳性能的关键。
</details></li>
</ul>
<hr>
<h2 id="Semantic-RGB-D-Image-Synthesis"><a href="#Semantic-RGB-D-Image-Synthesis" class="headerlink" title="Semantic RGB-D Image Synthesis"></a>Semantic RGB-D Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11356">http://arxiv.org/abs/2308.11356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shijie Li, Rong Li, Juergen Gall</li>
<li>for: 提高RGB-D图像Semantic分割的训练数据多样性</li>
<li>methods: 提出了一种基于Semantic图像Synthesis的方法，使用多模态数据生成真实的RGB-D图像</li>
<li>results: 比前一代单模态方法有大幅提高，并且通过混合实际和生成图像进行训练可以进一步提高RGB-D图像Semantic分割的精度<details>
<summary>Abstract</summary>
Collecting diverse sets of training images for RGB-D semantic image segmentation is not always possible. In particular, when robots need to operate in privacy-sensitive areas like homes, the collection is often limited to a small set of locations. As a consequence, the annotated images lack diversity in appearance and approaches for RGB-D semantic image segmentation tend to overfit the training data. In this paper, we thus introduce semantic RGB-D image synthesis to address this problem. It requires synthesising a realistic-looking RGB-D image for a given semantic label map. Current approaches, however, are uni-modal and cannot cope with multi-modal data. Indeed, we show that extending uni-modal approaches to multi-modal data does not perform well. In this paper, we therefore propose a generator for multi-modal data that separates modal-independent information of the semantic layout from the modal-dependent information that is needed to generate an RGB and a depth image, respectively. Furthermore, we propose a discriminator that ensures semantic consistency between the label maps and the generated images and perceptual similarity between the real and generated images. Our comprehensive experiments demonstrate that the proposed method outperforms previous uni-modal methods by a large margin and that the accuracy of an approach for RGB-D semantic segmentation can be significantly improved by mixing real and generated images during training.
</details>
<details>
<summary>摘要</summary>
Collecting diverse sets of training images for RGB-D semantic image segmentation 不一定是可能的。特别是当机器人需要在隐私敏感的地方 like 家庭中运行时，收集是经常受限于一小组地点。因此，标注图像缺乏多样性的外观和RGB-D semantic image segmentation 的方法容易过拟合训练数据。在这篇论文中，我们因此引入semantic RGB-D 图像合成来解决这个问题。它需要生成一个看起来很真实的 RGB-D 图像，以及一个给定的semantic label map。current approach是单模的，无法处理多模数据。我们实际上发现，将单模方法扩展到多模数据并不能达到好的效果。因此，我们提议一个生成器，可以分离modal-independent信息和modal-dependent信息。此外，我们还提议一个检验器，确保标注图像和生成的图像之间的semantic consistency，以及生成的图像和实际图像之间的 percepatual similarity。我们的全面实验表明，我们的方法可以大幅提高前一代单模方法的性能，并且可以在训练时混合实际和生成的图像，以提高RGB-D semantic segmentation的精度。
</details></li>
</ul>
<hr>
<h2 id="ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models"><a href="#ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models" class="headerlink" title="ProAgent: Building Proactive Cooperative AI with Large Language Models"></a>ProAgent: Building Proactive Cooperative AI with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11339">http://arxiv.org/abs/2308.11339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/ProAgent">https://github.com/PKU-Alignment/ProAgent</a></li>
<li>paper_authors: Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang</li>
<li>for: 这个论文主要目标是为了开发一种能够在人机合作中表现出突出的智能代理（ProAgent），帮助人机合作实现更高效的协同作业。</li>
<li>methods: 这个论文使用了大型自然语言模型（LLM）来为代理机制表现出更高级别的智能行为，包括预测合作伙伴的下一步决策并根据此形ulate更好的计划。</li>
<li>results: 实验结果表明，ProAgent在与其他AI代理和人类代理合作时表现出了remarkable的性能优势，比如自适应学习和人类学习等方法。此外，ProAgent还具有高度可模块化和可解释性，可以轻松地整合到各种协同enario中。<details>
<summary>Abstract</summary>
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10\% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)建立AI具有适应行为的概念在人类-AI合作中是AGI研究中的核心焦点。目前，开发合作代理人的方法主要依赖于学习方法，其策略泛化强度受到特定团队成员的交互影响。这些方法限制了代理人的策略重塑能力，面临新的团队成员时。我们提出了\textbf{ProAgent}框架，利用大型自然语言模型（LLMs）为代理人带来了能预测伙伴的决策并提出改进的计划的能力。ProAgent在合作理解方面表现出色，可以适应团队合作中的各种情况，并且具有高度的可重新组合性和可读性，可以轻松地与不同的协调enario进行集成。在\textit{Overcook-AI}框架下，我们进行了实验评估，发现ProAgent在与基于自我玩家和人口学习的五种方法进行比较时，在合作 with AI代理人方面表现出了杰出的性能优势。此外，当与人类代理模型进行合作时，其性能表现出了平均提高超过10%，与当前的状态艺术COLE相比。这些发现在不同的情况下，包括与不同特征的AI代理人和人类对手进行交互时，均得到了证明。这些发现 inspirits future research on human-robot collaboration. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details></li>
</ul>
<hr>
<h2 id="On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems"><a href="#On-the-Opportunities-and-Challenges-of-Offline-Reinforcement-Learning-for-Recommender-Systems" class="headerlink" title="On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems"></a>On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11336">http://arxiv.org/abs/2308.11336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaocong Chen, Siyu Wang, Julian McAuley, Dietmar Jannach, Lina Yao</li>
<li>for: 本研究旨在探讨在推荐系统中使用无线连接学习，特别是在不同的环境下进行学习和推荐。</li>
<li>methods: 本研究使用了无线连接学习的各种方法，包括Q-learning、SARSA和 Deep Q-Networks等，以学习用户的偏好和行为。</li>
<li>results: 研究发现，使用无线连接学习可以提高推荐系统的数据效率和学习速度，同时也可以提高用户的满意度和使用频率。<details>
<summary>Abstract</summary>
Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training"><a href="#GrowCLIP-Data-aware-Automatic-Model-Growing-for-Large-scale-Contrastive-Language-Image-Pre-training" class="headerlink" title="GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training"></a>GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11331">http://arxiv.org/abs/2308.11331</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinchi Deng, Han Shi, Runhui Huang, Changlin Li, Hang Xu, Jianhua Han, James Kwok, Shen Zhao, Wei Zhang, Xiaodan Liang</li>
<li>for: 本文提出了一种数据驱动自动模型增长算法，用于对语言-图像做contrastive预训练，并且可以处理不断增长的在线数据。</li>
<li>methods: 本文使用了动态增长空间和最优化 архитектуры，以适应在线学习场景。同时，提出了共享编码器，以增强语言-图像融合度。</li>
<li>results: 相比已有方法，GrowCLIP在零参数图像分类任务上提高了2.3%的平均排名第一精度。在Flickr30K dataset上，GrowCLIP在零参数图像检索任务上提高了1.2%的找到第一图像-文本准确率。<details>
<summary>Abstract</summary>
Cross-modal pre-training has shown impressive performance on a wide range of downstream tasks, benefiting from massive image-text pairs collected from the Internet. In practice, online data are growing constantly, highlighting the importance of the ability of pre-trained model to learn from data that is continuously growing. Existing works on cross-modal pre-training mainly focus on training a network with fixed architecture. However, it is impractical to limit the model capacity when considering the continuously growing nature of pre-training data in real-world applications. On the other hand, it is important to utilize the knowledge in the current model to obtain efficient training and better performance. To address the above issues, in this paper, we propose GrowCLIP, a data-driven automatic model growing algorithm for contrastive language-image pre-training with continuous image-text pairs as input. Specially, we adopt a dynamic growth space and seek out the optimal architecture at each growth step to adapt to online learning scenarios. And the shared encoder is proposed in our growth space to enhance the degree of cross-modal fusion. Besides, we explore the effect of growth in different dimensions, which could provide future references for the design of cross-modal model architecture. Finally, we employ parameter inheriting with momentum (PIM) to maintain the previous knowledge and address the issue of the local minimum dilemma. Compared with the existing methods, GrowCLIP improves 2.3% average top-1 accuracy on zero-shot image classification of 9 downstream tasks. As for zero-shot image retrieval, GrowCLIP can improve 1.2% for top-1 image-to-text recall on Flickr30K dataset.
</details>
<details>
<summary>摘要</summary>
跨modal预训练已经在各种下游任务中显示出很好的性能，受到互联网上庞大的图片文本对的收集启发。在实践中，网络上数据不断增长，高亮了预训练数据的不断增长的重要性。现有的跨modal预训练方法主要是通过固定的网络架构进行训练。然而，在实际应用中，限制模型容量是不切实际的，因为预训练数据的不断增长会导致模型无法适应。相反，我们需要利用当前模型的知识来获得高效的训练和更好的性能。为此，在这篇论文中，我们提出了GrowCLIP，一种基于数据驱动的自动模型增长算法，用于对冲对的语言图片预训练。我们采用动态生长空间，在每个增长步骤中寻找最佳的网络架构，以适应在线学习场景。此外，我们还提出了共享编码器，以增强模型之间的混合度。此外，我们还研究了不同维度的增长效果，这可能会对未来的跨modal模型架构设计产生影响。最后，我们采用参数继承与势（PIM）来维护之前的知识，解决局部最小问题。相比之下，GrowCLIP与现有方法相比，提高了9个下游任务的zero-shot图像分类精度，平均提高2.3%。此外，GrowCLIP还可以提高Flickr30K dataset上的zero-shot图像检索的top-1图像文本恢复率，提高1.2%。
</details></li>
</ul>
<hr>
<h2 id="From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow"><a href="#From-Mundane-to-Meaningful-AI’s-Influence-on-Work-Dynamics-–-evidence-from-ChatGPT-and-Stack-Overflow" class="headerlink" title="From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow"></a>From Mundane to Meaningful: AI’s Influence on Work Dynamics – evidence from ChatGPT and Stack Overflow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11302">http://arxiv.org/abs/2308.11302</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quentin Gallea</li>
<li>for: 这篇论文探讨了如何利用生成AI实现代码编程的产生效率提升，同时也提出了这些新技术对工作和知识共享方式的影响。</li>
<li>methods: 该论文使用了 quasi-experimental 方法（差异分析），利用Stack Overflow上最大的在线编程社区的使用情况，评估 ChatGPT 的发布对编程问题的影响。</li>
<li>results: 研究发现，ChatGPT 的发布导致编程问题数量减少，同时问题的 докуumenation 质量也有所提高。此外，剩下的问题也变得更加复杂。这些结果表明，利用生成AI可以实现工作效率提升，同时也会导致工作方式的重大变革，让人类专注于更加复杂的任务。<details>
<summary>Abstract</summary>
This paper illustrates how generative AI could give opportunities for big productivity gains but also opens up questions about the impact of these new powerful technologies on the way we work and share knowledge. More specifically, we explore how ChatGPT changed a fundamental aspect of coding: problem-solving. To do so, we exploit the effect of the sudden release of ChatGPT on the 30th of November 2022 on the usage of the largest online community for coders: Stack Overflow. Using quasi-experimental methods (Difference-in-Difference), we find a significant drop in the number of questions. In addition, the questions are better documented after the release of ChatGPT. Finally, we find evidence that the remaining questions are more complex. These findings suggest not only productivity gains but also a fundamental change in the way we work where routine inquiries are solved by AI allowing humans to focus on more complex tasks.
</details>
<details>
<summary>摘要</summary>
In Simplified Chinese:这篇论文描述了如何生成AI可以带来大量的产出增长，但同时也提出了这些新技术对我们工作和知识分享方式的影响。我们Specifically，我们研究了ChatGPT如何改变编程中的问题解决方式。为此，我们利用了chatGPT于11月30日的突然发布对Stack Overflow上最大的编程社区的使用情况产生的影响。使用 quasi-experimental方法（Difference-in-Difference），我们发现了问题数量减少的显著影响。此外，剩下的问题更加详细。这些发现不仅表明产出增长，还表明了我们工作的基本变革，Routine inquiry由AI解决，让人类可以专注于更复杂的任务。
</details></li>
</ul>
<hr>
<h2 id="Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation"><a href="#Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation" class="headerlink" title="Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation"></a>Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11291">http://arxiv.org/abs/2308.11291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremyfix/icvs2023">https://github.com/jeremyfix/icvs2023</a></li>
<li>paper_authors: Salim Khazem, Jeremy Fix, Cédric Pradalier</li>
<li>for: 本研究旨在预测木材内部缺陷的位置，以提高木材质量评估的准确性和效率。</li>
<li>methods: 本研究使用了卷积回归神经网络来解决木材外形特征与内部缺陷的Binary segmentation任务。</li>
<li>results: 研究表明，通过使用便宜的外形测量设备（如激光 Profiler）进行训练，可以通过卷积回归神经网络来预测木材内部缺陷的位置，并且可以在不同的树种上进行效果评估。<details>
<summary>Abstract</summary>
The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
</details>
<details>
<summary>摘要</summary>
木材行业中木材质量受到内部和外部缺陷的影响，包括由树木分支生长而成的内弯。今天，找到内弯需要使用昂贵的设备，如X射线扫描仪。在这篇论文中，我们解决了根据外形测量内弯的任务。我们提出使用卷积回归神经网络解决这个二分类任务。一旦神经网络训练完毕，可以通过便宜的设备，如激光 Profilers 进行推理。我们在桦树和落叶树种中展示了我们的方法的效果，并对循环的重要性进行了剖除。
</details></li>
</ul>
<hr>
<h2 id="ShadowNet-for-Data-Centric-Quantum-System-Learning"><a href="#ShadowNet-for-Data-Centric-Quantum-System-Learning" class="headerlink" title="ShadowNet for Data-Centric Quantum System Learning"></a>ShadowNet for Data-Centric Quantum System Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11290">http://arxiv.org/abs/2308.11290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Du, Yibo Yang, Tongliang Liu, Zhouchen Lin, Bernard Ghanem, Dacheng Tao</li>
<li>for: 本研究旨在探讨大量量子系统的动态学习问题，以减轻维度祸咎的影响。</li>
<li>methods: 本研究提出了一种数据驱动学习 paradigma，结合了神经网络协议和经典阴影，以便实现多种量子学习任务。</li>
<li>results: 研究表明，该 paradigma可以在偏远量子系统中提供准确和可靠的预测结果，并且可以在批处理大量量子系统时实现快速和高效的预测。<details>
<summary>Abstract</summary>
Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic of classical shadows, enabling memory-efficient storage and faithful prediction. These features underscore the immense potential of the proposed data-centric approach in discovering novel and large-scale quantum systems. For concreteness, we present the instantiation of our paradigm in quantum state tomography and direct fidelity estimation tasks and conduct numerical analysis up to 60 qubits. Our work showcases the profound prospects of data-centric artificial intelligence to advance QSL in a faithful and generalizable manner.
</details>
<details>
<summary>摘要</summary>
大量量子系统的动力学是由维度瓶颈所困难。统计学学习提供了新的可能性，通过神经网络协议和类型热影，然而两者都有局限性：前者受到预测不确定性的困扰，而后者缺乏泛化能力。我们提议一种数据驱动学习思想，结合这两种方法，以便实现多样化量子系统学习（QSL）任务。具体来说，我们的思想利用类型热影并与量子系统其他易获得信息一起创建训练集，然后通过神经网络学习探索QSL问题下的底层映射规则。通过神经网络的泛化能力，这种思想可以在训练阶段在线上培养，并在探索阶段预测未经见过的系统，即使只有几个状态的复制。此外，它继承类型热影的特点，即储存和预测的具有快速储存和准确预测的特点。这些特点强调了我们提议的数据驱动方法在发现新的大规模量子系统方面的极大潜力。为了证明这一点，我们在量子状态探测和直接准确率估计任务中实现了实例，并进行了数值分析至60个量子比特。我们的工作展示了数据驱动人工智能在忠实和泛化的方式下提高量子系统学习的可能性。
</details></li>
</ul>
<hr>
<h2 id="Recording-of-50-Business-Assignments"><a href="#Recording-of-50-Business-Assignments" class="headerlink" title="Recording of 50 Business Assignments"></a>Recording of 50 Business Assignments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12211">http://arxiv.org/abs/2308.12211</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/50BusinessAssignmentsLog">https://github.com/microsoft/50BusinessAssignmentsLog</a></li>
<li>paper_authors: Michal Sroka, Mohammadreza Fani Sani</li>
<li>for: 本研究用于发现和分析用户如何完成业务任务，提供有价值的进程效率和优化的研究发现。</li>
<li>methods: 本文提供了50个真实的企业过程数据集，这些数据集有很大的研究应用potential，包括任务挖掘和过程自动化。</li>
<li>results: 本研究提供了一个有价值的数据集，这些数据集有助于研究人员和实践者了解进程效率和优化。<details>
<summary>Abstract</summary>
One of the main use cases of process mining is to discover and analyze how users follow business assignments, providing valuable insights into process efficiency and optimization. In this paper, we present a comprehensive dataset consisting of 50 real business processes. The dataset holds significant potential for research in various applications, including task mining and process automation which is a valuable resource for researchers and practitioners.
</details>
<details>
<summary>摘要</summary>
一个主要的用 caso of  proces mining 是发现和分析用户如何跟踪商业任务，提供有价值的信息来进行过程效率和优化。在这篇论文中，我们提供了完整的数据集，包含50个真实的商业过程。该数据集具有较高的研究价值，包括任务挖掘和过程自动化，是研究人员和实践者的宝贵资源。
</details></li>
</ul>
<hr>
<h2 id="CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation"><a href="#CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation" class="headerlink" title="CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation"></a>CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11277">http://arxiv.org/abs/2308.11277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Stötzner, Timo Homburg, Hubert Mara</li>
<li>for: 针对ancient near eastern studies (DANES) 社区面临的挑战，我们开发了数字工具来处理篆字体系，这是一种3D文字痕迹在泥 TABLETS上的历史文化，覆盖了三千多年和至少八种主要语言。</li>
<li>methods: 我们使用了HeiCuBeDa和MaiCuBeDa数据集，包括约500个标注的板表，并提供了一种新的OCR-like方法来处理混合图像数据。我们的签名localization使用RepPoints探测器来预测字符的位置为 bounding boxes。我们使用了GigaMesh的MSII（曲率）基于的渲染、Phong-ashed 3D模型和照片，以及光照增强。</li>
<li>results: 使用渲染的3D图像进行签名检测比使用照片更好，而且我们的方法在混合数据集上表现良好，而且Phong renderings和特别是MSII renderings在照片上提高了结果。<details>
<summary>Abstract</summary>
Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D models, and photographs as well as illumination augmentation. The results show that using rendered 3D images for sign detection performs better than other work on photographs. In addition, our approach gives reasonably good results for photographs only, while it is best used for mixed datasets. More importantly, the Phong renderings, and especially the MSII renderings, improve the results on photographs, which is the largest dataset on a global scale.
</details>
<details>
<summary>摘要</summary>
受到数字古近东研究（DANES）社区的挑战启发，我们开发了数字工具来处理古代 Mesopotamia 文字，这是一种3D字符印制在泥版上，用于超过三千年和至少八种主要语言。它包含了数千个字符，随着时间和空间的变化而变化。 photographs 是最常用的机器学习 Representation，而墨水Drawing 容易被解释。我们创建了 HeiCuBeDa 和 MaiCuBeDa 数据集，包含约500个注释的泥版。为了我们的新的 OCR-like 方法，我们提供了一个附加的映射工具，用于将3D渲染与 photographs 之间的注释传输。我们使用 GigaMesh 的 MSII（曲率，见 <https://gigamesh.eu>）基于的渲染、Phong 灯光渲染和 photographs 以及照明增强。结果表明，使用3D渲染来检测字符性能更高于其他工作的 photographs 上。此外，我们的方法在 photographs 上提供了reasonably good的结果，而且在混合数据集上表现最佳。尤其是 Phong 渲染和 MSII 渲染，对于 photographs 上的结果有所提高。
</details></li>
</ul>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
<li>for:  solves the problem of text-to-music generation (T2M-Gen) faced due to the scarcity of large-scale publicly available music datasets with natural language captions.</li>
<li>methods:  utilizes audio representations from a pretrained MERT model to extract music features, and introduces a methodology for generating question-answer pairs from existing audio captioning datasets, as well as the MusicQA Dataset designed for answering open-ended music-related questions.</li>
<li>results:  achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.<details>
<summary>Abstract</summary>
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.
</details>
<details>
<summary>摘要</summary>
文本转换为乐曲生成（T2M-Gen）遇到了一个重要的障碍，即公共可用的大规模乐曲数据集中的自然语言描述缺乏。为解决这个问题，我们提议了Music Understanding LLaMA（MU-LLaMA），可以回答乐曲相关的问题并生成乐曲文件的描述。我们的模型利用了预训练的MERT模型来提取乐曲特征。但是获得合适的模型训练数据集仍然是一个挑战，因为现有的公共可用的音频问答数据集缺乏必要的深度来回答开放式乐曲问题。为了填补这个空白，我们提出了一种方法，可以将现有的音频描述数据集转换成问题-答案对，并介绍了MusicQA数据集，用于回答开放式乐曲相关的问题。实验结果表明，我们提出的MU-LLaMA模型，在我们设计的MusicQA数据集上进行训练，在多种纪录计中表现出色，超越当前的状态机（SOTA）模型在乐曲问题回答和乐曲描述生成方面，并为T2M-Gen研究领域带来了可期的进步。
</details></li>
</ul>
<hr>
<h2 id="Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes"><a href="#Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes" class="headerlink" title="Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes"></a>Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11267">http://arxiv.org/abs/2308.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: David M. Bossens</li>
<li>for: 本 paper 目的是提出一种基于 reinforcement learning 的任务模型框架，即 robust constrained Markov decision process (RCMDP)，该框架可以考虑行为约束和模型不确定性，并提供了对模型不确定性的鲁棒性。</li>
<li>methods: 本 paper 使用的方法包括：1) 基于值估计的最坏情况动力学；2) 基于拉格朗日点的最坏情况动力学；3) 对 RCMDP 的劣化策略算法。</li>
<li>results: 本 paper 的实验结果表明，提出的 two algorithms（RCPG with Robust Lagrangian 和 Adversarial RCPG）在 injecting perturbations 的 inventory management 和 safe navigation 任务中表现比较出色，特别是 Adversarial RCPG 在所有测试中排名第二。<details>
<summary>Abstract</summary>
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy through gradient descent rather than indirectly and abruptly through constrained optimisation on a sorted value list. A theoretical analysis first derives the Lagrangian policy gradient for the policy optimisation of both proposed algorithms and then the adversarial policy gradient to learn the adversary for Adversarial RCPG. Empirical experiments injecting perturbations in inventory management and safe navigation tasks demonstrate the competitive performance of both algorithms compared to traditional RCPG variants as well as non-robust and non-constrained ablations. In particular, Adversarial RCPG ranks among the top two performing algorithms on all tests.
</details>
<details>
<summary>摘要</summary>
RCMDP（有约束的马尔可夫决策过程）是一种最近的任务模型框架，用于机器学习中的激励学习，它包含行为约束并提供了对转移动力学模型中的错误的Robustness。模拟RCMDP需要基于每个状态的值估计计算最差情况的动力学，这同RCPG（有约束的策略梯度）一样。在RCPG中，作者提出了两种算法，即RCPG with Robust Lagrangian和Adversarial RCPG。RCPG with Robust Lagrangian通过基于Lagrangian而不是值或约束来修改RCPG。Adversarial RCPG也基于Lagrangian，但是通过对敌对策略进行准确的梯度下降来直接和逐步地学习对敌。作者首先 derivates Lagrangian policy gradient для政策优化两个提出的算法，然后 derivates adversarial policy gradient来学习对敌。实验表明，两种算法在各种任务中表现竞争性，特别是Adversarial RCPG在所有测试中排名第二。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games"><a href="#Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games" class="headerlink" title="Efficient Last-iterate Convergence Algorithms in Solving Games"></a>Efficient Last-iterate Convergence Algorithms in Solving Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11256">http://arxiv.org/abs/2308.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjian Meng, Zhenxing Ge, Wenbin Li, Bo An, Yang Gao</li>
<li>For: The paper is written for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs) using no-regret algorithms.* Methods: The paper uses the Reward Transformation (RT) framework, which transforms the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). The authors also use Regret Matching+ (RM+) algorithm to solve the SCCPs, and propose a novel transformation method to enable RM+ to solve the SCCPs.* Results: The paper shows that the proposed algorithm, Reward Transformation RM+ (RTRM+), enjoys last-iterate convergence under the discrete-time feedback setting, and significantly outperforms existing last-iterate convergence algorithms and RM+ (CFR+) in experiments.<details>
<summary>Abstract</summary>
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback. We demonstrate that the essence of the RT framework is to transform the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). We show that the bottleneck of RT-based algorithms is the speed of solving SCCPs. To improve the their empirical performance, we design a novel transformation method to enable the SCCPs can be solved by Regret Matching+ (RM+), a no-regret algorithm with better empirical performance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys last-iterate convergence under the discrete-time feedback setting. Using the counterfactual regret decomposition framework, we propose Reward Transformation CFR+ (RTCFR+) to extend RTRM+ to EFGs. Experimental results show that our algorithms significantly outperform existing last-iterate convergence algorithms and RM+ (CFR+).
</details>
<details>
<summary>摘要</summary>
“无后悔算法”受欢迎用于学习两player零余游戏（NFG）和广泛游戏（EFG）中的 Nash  equilibriium（NE）。许多最近的研究将注意力集中在最后迭代具有无后悔性的算法。其中最具知名度的两个算法是Optimistic Gradient Descent Ascent（OGDA）和Optimistic Multiplicative Weight Update（OMWU）。然而，OGDA的每迭代复杂度较高，而OMWU的实际性较差，且其对NE的唯一性Conditions不够严格。Recent works propose a Reward Transformation（RT） framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU。然而，RT-based algorithms under the same number of iterations than OGDA, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios。To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback。我们展示了RT framework的核心是将学习NE在原始游戏中的问题转换为一系列强oly convex-concave optimization problems（SCCPs）。我们显示了RT-based algorithms的瓶颈在SCCPs的解决方法。为了提高它们的实际性表现，我们设计了一个新的变换方法，让SCCPs可以通过Regret Matching+（RM+），一个无后悔算法，解决，从而产生Reward Transformation RM+（RTRM+）。RTRM+ 满足最后迭代具有无后悔性的条件。使用Counterfactual regret decomposition framework，我们提出Reward Transformation CFR+（RTCFR+）来扩展RTRM+到EFGs。实验结果显示我们的算法在已知的最后迭代具有无后悔性的算法和RM+（CFR+）中具有优秀的实际表现。
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-bias-in-machine-learning-research"><a href="#A-survey-on-bias-in-machine-learning-research" class="headerlink" title="A survey on bias in machine learning research"></a>A survey on bias in machine learning research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11254">http://arxiv.org/abs/2308.11254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Michał Grochowski</li>
<li>for: 本研究旨在帮助理解机器学习中的偏见源泉和错误，以提高机器学习模型的公平、透明和准确性。</li>
<li>methods: 本文提供了四十个可能的机器学习漏洞和错误的示例，并对每个示例进行了详细的描述。</li>
<li>results: 本文通过对机器学习管道中的偏见和错误的分析，帮助开发者更好地检测和缓解偏见，从而提高机器学习模型的公平性和准确性。<details>
<summary>Abstract</summary>
Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
</details>
<details>
<summary>摘要</summary>
现有研究 often focuses on fairness 的偏见在机器学习中，而忽略了偏见的根源或 causa。然而，偏见原本是一种“系统性错误”，常由人类在不同阶段的研究过程中引入。这篇文章目的是 bridge the gap between past literature on bias in research by providing a taxonomy for potential sources of bias and errors in data and models. The paper focuses on bias in machine learning pipelines. The survey analyzes over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
<li>for: 这篇论文主要用于研究 guitar 乐谱中的弯曲技巧，以及如何使用数据分析方法来预测弯曲的发生。</li>
<li>methods: 这篇论文使用了一些数据分析技术，包括决策树等，来研究弯曲的预测。</li>
<li>results: 实验结果表明，使用这些数据分析技术可以准确预测弯曲的发生，并且具有一定的可靠性和精度。<details>
<summary>Abstract</summary>
Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details>
<details>
<summary>摘要</summary>
Tablaturenotation是流行音乐中广泛使用的notation方式，用于记录和分享吉他乐器的音乐内容。作为标准notation的补充，tablaturenotation记录了演奏手势信息，包括手指位置和许多特有的吉他演奏技巧，如滑弹、弹压和弯曲。本文关注的是弯曲，它可以使演奏者在不改变 физиical fretted fingerboard的前提下，逐渐改变音符的抑制值。在本文中，我们提出了25个高级特征，用于研究如何通过检测短期内的前后文来预测琴曲中的弯曲出现。实验使用了932首流行乐器琴曲tablature，并显示了一棵决策树可以成功预测琴曲中的弯曲出现，F1分数为0.71，并且具有有限的假阳性预测，这表明可以用于将非吉他音乐转换成琴曲tablature。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes"><a href="#Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes" class="headerlink" title="Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes"></a>Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11247">http://arxiv.org/abs/2308.11247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Fernandes Montesuma, Michela Mulas, Fred Ngolè Mboula, Francesco Corona, Antoine Souloumiac</li>
<li>for: 这种研究旨在提高过程监测中的故障诊断精度，具体来说是利用机器学习算法预测故障类型基于感知器读ings。</li>
<li>methods: 该研究使用单源预处理适应（SSDA）和多源预处理适应（MSDA）算法进行故障诊断，并在田东曼进程中进行了广泛的比较。</li>
<li>results: 研究显示，在多源场景下使用多个预处理源可以提高故障诊断精度，具体来说是比单源场景提高23%的平均精度。此外，无适应情况下，多源场景可以提高不适应精度的平均提升率为8.4%。<details>
<summary>Abstract</summary>
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classification accuracy by 23% on average. In addition, under the multiple-sources scenario, we improve classification accuracy of the no adaptation setting by 8.4% on average.
</details>
<details>
<summary>摘要</summary>
FAULT诊断是 proces supervision 中的一个重要组件。它可以确定哪种缺陷已经发生，只要它已经检测到了，然后采取相应的 intervención。自动FAULT诊断系统 使用机器学习来预测缺陷类型从感知值中。然而，这些模型对数据分布的变化敏感，这些变化可能是由监测过程中的变化所致，如操作模式的变化。这种情况被称为 Cross-Domain Fault Diagnosis (CDFD)。我们提供了广泛的单源和多源无监督领域适应 (SSDA 和 MSDA 分别) 算法的 Comparative study  для CDFD。我们在 Tennessee-Eastmann 过程中进行了这些方法的研究，这是化学工业中广泛使用的一个标准测试 benchmark。我们发现在训练时使用多个频道有益，即使没有适应也。因此，MSDA 基线提高了 SSDA 基eline 的分类精度，在 average 上提高了 23%。此外，在多源场景下，我们在无适应情况下提高了分类精度的 average 上提高了 8.4%。
</details></li>
</ul>
<hr>
<h2 id="Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy"><a href="#Faster-Optimization-in-S-Graphs-Exploiting-Hierarchy" class="headerlink" title="Faster Optimization in S-Graphs Exploiting Hierarchy"></a>Faster Optimization in S-Graphs Exploiting Hierarchy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11242">http://arxiv.org/abs/2308.11242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hriday Bavle, Jose Luis Sanchez-Lopez, Javier Civera, Holger Voos<br>for:This paper aims to improve the scalability of Situational Graphs (S-Graphs) in large environments for Simultaneous Localization and Mapping (SLAM) by marginalizing redundant robot poses and their connections to observations.methods:The proposed method generates and optimizes room-local graphs encompassing all graph entities within a room-like structure, compresses the S-Graphs, and performs windowed local optimization of the compressed graph at regular time-distance intervals. Global optimization is performed every time a loop closure is detected.results:The proposed method achieves similar accuracy compared to the baseline while reducing the computation time by 39.81% compared to the baseline.<details>
<summary>Abstract</summary>
3D scene graphs hierarchically represent the environment appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. Though, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity.   To overcome this limitation in this work we present an initial research of an improved version of S-Graphs exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. Firstly, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyframes within the given room. We then perform windowed local optimization of the compressed graph at regular time-distance intervals. A global optimization of the compressed graph is performed every time a loop closure is detected. We show similar accuracy compared to the baseline while showing a 39.81% reduction in the computation time with respect to the baseline.
</details>
<details>
<summary>摘要</summary>
三维场景图 hierarchically represents the environment, appropriately organizing different environmental entities in various layers. Our previous work on situational graphs extends the concept of 3D scene graph to SLAM by tightly coupling the robot poses with the scene graph entities, achieving state-of-the-art results. However, one of the limitations of S-Graphs is scalability in really large environments due to the increased graph size over time, increasing the computational complexity. To overcome this limitation, in this work we present an initial research of an improved version of S-Graphs by exploiting the hierarchy to reduce the graph size by marginalizing redundant robot poses and their connections to the observations of the same structural entities. First, we propose the generation and optimization of room-local graphs encompassing all graph entities within a room-like structure. These room-local graphs are used to compress the S-Graphs marginalizing the redundant robot keyframes within the given room. We then perform windowed local optimization of the compressed graph at regular time-distance intervals. A global optimization of the compressed graph is performed every time a loop closure is detected. We show similar accuracy compared to the baseline while showing a 39.81% reduction in computation time with respect to the baseline.
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification"><a href="#An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification" class="headerlink" title="An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification"></a>An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11241">http://arxiv.org/abs/2308.11241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harunorikawano/speaker-identification-with-tgp">https://github.com/harunorikawano/speaker-identification-with-tgp</a></li>
<li>paper_authors: Harunori Kawano, Sota Shimizu</li>
<li>for: 这个研究是为了开发一个高精度的 speaker identification 模型，使用 Transformer 架构和自我超vised learning。</li>
<li>methods: 本研究使用了 Transformer-based contextual model，并进一步提出了 Temporal Gate Pooling 方法来增强模型的学习能力。</li>
<li>results: 研究获得了使用 VoxCeleb1 资料集进行认个者识别 tasks 的85.9%的精度，与 wav2vec2 的317.7M个parameters相比，这个方法具有较高的精度和较低的 Parameters 数量。<details>
<summary>Abstract</summary>
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
</details>
<details>
<summary>摘要</summary>
它使用 transformer 架构和自动学习来实现了speech recognition的成功。最近，这些技术不仅用于speech recognition，还用于整个speech processing。这篇论文介绍了一种高效的端到端speaker identification模型，该模型使用 transformer-based 上下文模型。我们研究了参数与性能之间的关系，以便理解高效模型的结构。此外，我们提出了一种pooling方法， named Temporal Gate Pooling，具有强大的学习能力。我们使用 Conformer 作为编码器，并使用 BEST-RQ 进行预训练。我们对 VoxCeleb1 进行了评估，并实现了85.9%的准确率，相比之下，wav2vec2 的参数数量为317.7M，我们的方法准确率相对较高。代码可以在 GitHub 上找到：https://github.com/HarunoriKawano/speaker-identification-with-tgp。
</details></li>
</ul>
<hr>
<h2 id="ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts"><a href="#ROSGPT-Vision-Commanding-Robots-Using-Only-Language-Models’-Prompts" class="headerlink" title="ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts"></a>ROSGPT_Vision: Commanding Robots Using Only Language Models’ Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11236">http://arxiv.org/abs/2308.11236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bilel-bj/rosgpt_vision">https://github.com/bilel-bj/rosgpt_vision</a></li>
<li>paper_authors: Bilel Benjdira, Anis Koubaa, Anas M. Ali</li>
<li>for: 这个论文主要是提出一种新的 робо控制方法，使用语言模型提示来控制 робо。</li>
<li>methods: 该方法使用语言模型和视觉模型结合在一起，通过自动化机制来执行 robotic 任务。</li>
<li>results: 这个方法可以减少 robotic 开发成本，并且可以在实际应用中提高应用质量。Here’s a more detailed explanation of each point:</li>
<li>for: The paper proposes a new method for controlling robots using only language prompts, which is a significant departure from traditional methods that rely on technical details and programming.</li>
<li>methods: The method uses a combination of language models and vision models to automate the mechanisms behind the prompts, allowing the robot to execute tasks based on natural language descriptions.</li>
<li>results: The method has been shown to reduce development costs and improve the quality of applications, and it has the potential to advance robotic research in this direction.I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
In this paper, we argue that the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 argueThat the next generation of robots can be commanded using only Language Models' prompts. Every prompt interrogates separately a specific Robotic Modality via its Modality Language Model (MLM). A central Task Modality mediates the whole communication to execute the robotic mission via a Large Language Model (LLM). This paper gives this new robotic design pattern the name of: Prompting Robotic Modalities (PRM). Moreover, this paper applies this PRM design pattern in building a new robotic framework named ROSGPT_Vision. ROSGPT_Vision allows the execution of a robotic task using only two prompts: a Visual and an LLM prompt. The Visual Prompt extracts, in natural language, the visual semantic features related to the task under consideration (Visual Robotic Modality). Meanwhile, the LLM Prompt regulates the robotic reaction to the visual description (Task Modality). The framework automates all the mechanisms behind these two prompts. The framework enables the robot to address complex real-world scenarios by processing visual data, making informed decisions, and carrying out actions automatically. The framework comprises one generic vision module and two independent ROS nodes. As a test application, we used ROSGPT_Vision to develop CarMate, which monitors the driver's distraction on the roads and makes real-time vocal notifications to the driver. We showed how ROSGPT_Vision significantly reduced the development cost compared to traditional methods. We demonstrated how to improve the quality of the application by optimizing the prompting strategies, without delving into technical details. ROSGPT_Vision is shared with the community (link: https://github.com/bilel-bj/ROSGPT_Vision) to advance robotic research in this direction and to build more robotic frameworks that implement the PRM design pattern and enables controlling robots using only prompts.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks"><a href="#Adaptive-White-Box-Watermarking-with-Self-Mutual-Check-Parameters-in-Deep-Neural-Networks" class="headerlink" title="Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks"></a>Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11235">http://arxiv.org/abs/2308.11235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenzhe Gao, Zhaoxia Yin, Hongjian Zhan, Heng Yin, Yue Lu</li>
<li>for: 检测和防止人工智能模型中的意外或恶意篡改。</li>
<li>methods: 使用敏感 watermarking 技术来识别和检测篡改。</li>
<li>results: 测试结果表明，当篡改率低于 20% 时，我们的方法可以达到很高的恢复性能。而对于受到 watermarking 影响的模型，我们使用可适应位数技术来恢复模型的精度。<details>
<summary>Abstract</summary>
Artificial Intelligence (AI) has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermarking significantly affected accuracy, we utilized an adaptive bit technique to recover more than 15% of the accuracy loss of the model.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）在应用广泛，但也存在风险，因为在部署过程中可能会有不恰当或恶意篡改。因此， Regular checks 是必要的，以检测和预防这些风险。某些攻击可能会导致模型参数的篡改，因此我们需要一种方法来检测和修复篡改的参数。在这篇论文中，我们提出了一种用于检测篡改参数和位数的方法，可以用来检测、定位和修复篡改的参数。此外，我们还提出了一种适应式嵌入方法，可以最大化信息容量，同时保持模型的准确性。我们的方法在多个神经网络上进行了测试，并达到了篡改率低于20%时的恢复性能。此外，对于模型中 watermarking 对准确性产生了较大的影响，我们使用适应位数技术来恢复模型的准确性，达到了超过15%的恢复效果。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding"><a href="#Traffic-Flow-Optimisation-for-Lifelong-Multi-Agent-Path-Finding" class="headerlink" title="Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding"></a>Traffic Flow Optimisation for Lifelong Multi-Agent Path Finding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11234">http://arxiv.org/abs/2308.11234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Chen, Daniel Harabor, Jiaoyang Li, Peter J. Stuckey</li>
<li>for: 解决多 Agent 路径规划问题，即 robotics 中多 Agent 需要计算共享地图上免撞的路径。</li>
<li>methods: 提出一种新的方法，使用填充避免拥填的路径引导 Agent 达到目的地。</li>
<li>results: 在一shot MAPF 和 Lifelong MAPF 两个大规模场景中，提供了较好的解决方案，对一shot MAPF 的解决质量做出了重要改进，对 Lifelong MAPF 的总 Throughput 做出了大幅提高。<details>
<summary>Abstract</summary>
Multi-Agent Path Finding (MAPF) is a fundamental problem in robotics that asks us to compute collision-free paths for a team of agents, all moving across a shared map. Although many works appear on this topic, all current algorithms struggle as the number of agents grows. The principal reason is that existing approaches typically plan free-flow optimal paths, which creates congestion. To tackle this issue we propose a new approach for MAPF where agents are guided to their destination by following congestion-avoiding paths. We evaluate the idea in two large-scale settings: one-shot MAPF, where each agent has a single destination, and lifelong MAPF, where agents are continuously assigned new tasks. For one-shot MAPF we show that our approach substantially improves solution quality. For Lifelong MAPF we report large improvements in overall throughput.
</details>
<details>
<summary>摘要</summary>
多智能路径找（MAPF）是 robotics 中的基本问题，它要求我们计算多个智能机器人在共享地图上的冲突自由路径。虽然有很多研究对这个问题进行了努力，但现有的方法都难以承受多个机器人的增加。主要的原因是现有的方法通常计划自由流优化路径，这会导致堵塞。为解决这个问题，我们提出了一种新的 MAPF 方法，即使 agents 跟随堵塞避免路径来达到目的地。我们在两个大规模设置中评估了这个想法：一个是一次 MAPF，每个机器人都有单个目的地；另一个是持续 MAPF，机器人 continuous 被分配新任务。对一次 MAPF 我们显示了我们的方法可以显著提高解决方案质量。对持续 MAPF 我们报告了大幅提高总吞吐量。
</details></li>
</ul>
<hr>
<h2 id="On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report"><a href="#On-Premise-AIOps-Infrastructure-for-a-Software-Editor-SME-An-Experience-Report" class="headerlink" title="On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report"></a>On-Premise AIOps Infrastructure for a Software Editor SME: An Experience Report</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11225">http://arxiv.org/abs/2308.11225</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anes Bendimerad, Youcef Remil, Romain Mathonat, Mehdi Kaytoue</li>
<li>for: 本研究旨在探讨在企业内部实施基于开源工具的AIOps解决方案，以提高软件维护和监测。</li>
<li>methods: 本研究使用开源工具构建了一套完整的AIOps基础设施，并提供了不同选择的原则和策略。</li>
<li>results: 研究结果表明，使用开源工具构建AIOps基础设施可以减少成本和提高软件维护效率，同时也可以满足公司的数据管理和安全需求。<details>
<summary>Abstract</summary>
Information Technology has become a critical component in various industries, leading to an increased focus on software maintenance and monitoring. With the complexities of modern software systems, traditional maintenance approaches have become insufficient. The concept of AIOps has emerged to enhance predictive maintenance using Big Data and Machine Learning capabilities. However, exploiting AIOps requires addressing several challenges related to the complexity of data and incident management. Commercial solutions exist, but they may not be suitable for certain companies due to high costs, data governance issues, and limitations in covering private software. This paper investigates the feasibility of implementing on-premise AIOps solutions by leveraging open-source tools. We introduce a comprehensive AIOps infrastructure that we have successfully deployed in our company, and we provide the rationale behind different choices that we made to build its various components. Particularly, we provide insights into our approach and criteria for selecting a data management system and we explain its integration. Our experience can be beneficial for companies seeking to internally manage their software maintenance processes with a modern AIOps approach.
</details>
<details>
<summary>摘要</summary>
信息技术已成为不同行业的关键组成部分，导致软件维护和监测得到了更大的关注。由于现代软件系统的复杂性，传统维护方法已成为不足。AIOps概念出现以增强预测维护，通过大数据和机器学习技术来提高维护效率。然而，使用AIOps存在数据复杂性和事件管理等挑战。 comercial解决方案存在，但它们可能不适用于某些公司，因为高成本、数据管理问题和私有软件的限制。本文探讨在公司内部实施On-premise AIOps解决方案的可行性，通过使用开源工具。我们介绍了一个完整的AIOps基础设施，我们在公司中成功地部署了这个基础设施，并提供了不同组件的选择原则。尤其是数据管理系统的选择和集成方法。我们的经验可以帮助公司通过现代AIOps方法 internally管理软件维护过程。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis"><a href="#Evaluating-Large-Language-Models-on-Graphs-Performance-Insights-and-Comparative-Analysis" class="headerlink" title="Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis"></a>Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11224">http://arxiv.org/abs/2308.11224</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ayame1006/llmtograph">https://github.com/ayame1006/llmtograph</a></li>
<li>paper_authors: Chang Liu, Bo Wu</li>
<li>for: 这个研究旨在评估四种大语言模型（LLMs）在处理图数据上的应用能力。</li>
<li>methods: 该研究使用了四种不同的评估指标：理解、正确性、准确性和修复能力。</li>
<li>results: 研究发现，LLMs可以很好地理解图数据的自然语言描述，并且可以对图结构进行有效的推理。GPT模型在正确性方面表现出色，而其他模型则在结构理解方面表现较差。GPT模型在多个答案 зада题上常出现错误答案，这可能会降低其修复能力。另外，GPT-4能够修复GPT-3.5-turbo和自己之前的迭代的答案。研究代码可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/Ayame1006/LLMtoGraph%E3%80%82">https://github.com/Ayame1006/LLMtoGraph。</a><details>
<summary>Abstract</summary>
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在学术和业务领域都受到了广泛关注。然而，对图数据的应用仍然尚未得到充分探索。本研究通过评估四种LLM在解决多个分析问题上的能力来评估LLM在图数据上的应用。我们采用了四种评估指标：理解、正确性、准确性和修复。我们的结果显示：1）LLM可以很好地理解图数据的自然语言描述和图结构的关系。2）GPT模型在正确性方面表现出色，超越了其他选择。3）所有考试LLM都面临着结构理解的挑战，特别是零shot链条思维和几个shot提示的效果减退。4）GPT模型在多个答案任务中很容易出现错误答案，这可能会影响它们的准确性。5）GPT模型表现出高度自信心，这可能会阻碍它们的修复能力。备注的是，GPT-4已经示出了可以修复GPT-3.5-turbo和自己的前一个迭代的能力。代码可以在 GitHub上找到：https://github.com/Ayame1006/LLMtoGraph。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment"><a href="#Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment" class="headerlink" title="Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment"></a>Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11220">http://arxiv.org/abs/2308.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toriqiu/fl-pcos">https://github.com/toriqiu/fl-pcos</a></li>
<li>paper_authors: Lucia Morris, Tori Qiu, Nikhil Raghuraman</li>
<li>for: 这篇论文是为了探讨 Federated Learning（FL）在预测女性淋巴疾病多囊卵巢综合症（PCOS）的优化药物方案。</li>
<li>methods: 这篇论文使用了多种 Federated Learning 方法，并在人工合成 PCOS 患者数据集上进行了测试。</li>
<li>results: 研究发现，这些 Federated Learning 方法在论文中提出的数据隐私保护和药物优选问题上都具有出色的表现。<details>
<summary>Abstract</summary>
The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
</details>
<details>
<summary>摘要</summary>
妇女激素学的应用落后于数据驱动医疗解决方案，主要是由于患者数据隐私问题的担忧。 valuable datapoints about 激素水平或月经周期可能暴露患有并发症或中止怀孕的患者，违反其隐私。 我们探讨了 Federated Learning（FL）的应用，以预测患有多囊卵巢综合症（PCOS）患者最佳药物。 PCOS 是世界上数百万女性中的一种严重激素失衡症，但它的研究受到缺乏患者数据的限制。 我们示出了多种 FL 方法在 sintetic PCOS 患者数据集上得到成功。我们的提议的 FL 模型是一种访问庞大数据量和鉴别最有效的治疗方案的工具，同时为 PCOS 患者提供隐私保证。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models"><a href="#Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models" class="headerlink" title="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models"></a>Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11217">http://arxiv.org/abs/2308.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengxiang Li, Zhaoxiang Hou, Hui Liu, Ying Wang, Tongzhi Li, Longfei Xie, Chao Shi, Chengyi Yang, Weishan Zhang, Zelei Liu, Liang Xu</li>
<li>for: 这篇论文旨在提出一种多模态联合学习框架，帮助多家企业通过私有领域数据来共同训练大型模型，实现多enario智能服务。</li>
<li>methods: 论文提出了多模态联合学习的策略性转型，包括智能基础和目标的重要性在大模型时代，以及在多种数据、模型聚合、性能和成本交易、数据隐私和奖励机制等方面的新挑战。</li>
<li>results: 论文介绍了一个城市安全运营管理案例研究，其中多家企业共同提供多模态数据和专业知识，实现了城市安全运营管理的分布部署和有效协调。初步实验表明，企业可以通过多模态模型联合学习提高和储存智能能力，共同创造出高质量智能服务，涵盖能源基础设施安全、住宅社区安全和城市运营管理等多个领域。<details>
<summary>Abstract</summary>
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient coordination of the federated learning platform, technical innovations on data quality improvement based on large model capabilities and efficient joint fine-tuning approaches. Preliminary experiments show that enterprises can enhance and accumulate intelligent capabilities through multimodal model federated learning, thereby jointly creating an smart city model that provides high-quality intelligent services covering energy infrastructure safety, residential community security, and urban operation management. The established federated learning cooperation ecosystem is expected to further aggregate industry, academia, and research resources, realize large models in multiple vertical domains, and promote the large-scale industrial application of artificial intelligence and cutting-edge research on multimodal federated learning.
</details>
<details>
<summary>摘要</summary>
多模式数据，能够全面感知和识别物理世界，已成为通往普遍人工智能的关键Path。然而，多模式大型模型在公共数据集上训练时经常表现不佳在特定行业领域。这篇论文提出了一种多模式联合学习框架，允许多家企业使用私有领域数据共同训练大型模型，以实现多场景智能服务。作者对联合学习在智能基础和目标时代的战略性转变进行了深入探讨，以及新的多样数据、模型汇集、性能和成本负担、数据隐私和奖励机制等挑战。论文还介绍了一个城市安全运营管理案例研究，包括分布式部署和有效协调联合学习平台，以及基于大型模型技术的数据质量改进和高效联合练习方法。初步实验显示，企业可以通过多模式模型联合学习增强和积累智能能力，共同创造出高质量智能服务，涵盖能源基础设施安全、住宅社区安全和城市运营管理。建立的联合学习合作生态系统预期会进一步吸引产业、学术和研究资源，实现多个垂直领域的大型模型，并推动人工智能和多模式联合学习的大规模产业应用。
</details></li>
</ul>
<hr>
<h2 id="ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs"><a href="#ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs" class="headerlink" title="ConcatPlexer: Additional Dim1 Batching for Faster ViTs"></a>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11199">http://arxiv.org/abs/2308.11199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong, Nojun Kwak<br>for: 这个研究旨在提高视觉识别的效率，以提高模型的测试速度和精度。methods: 本研究使用了一种叫做Data Multiplexing（DataMUX）的成本削减方法，并将其应用到视觉模型中。它还导入了一个名为Image Multiplexer的新方法，以及一些新的组件，以解决DataMux在视觉模型中的弱点。results: 研究发现，使用ConcatPlexer模型可以大幅提高视觉识别的启动速度，同时保持了69.5%和83.4%的验证精度。相比之下，ViT-B&#x2F;16模型需要23.5%更多的GFLOPs以达到相同的验证精度。<details>
<summary>Abstract</summary>
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and 83.4% validation accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
transformers 在自然语言处理（NLP）领域表现出色，同时在计算机视觉领域也引发了多种创新应用。然而，transformers 的高性能和模型灵活性却导致计算成本增加，因此许多研究团队提出了降低计算成本的方法。 draw inspiration from language models 的 cost-cutting method，我们提出了一种新的方法 для高效的视觉识别，即图像多重化（Image Multiplexer）。我们首先介绍了图像多重化的原型，然后开发了新的组件来缓解其缺点，最终得到了我们的最终模型——ConcatPlexer。ConcatPlexer 在 ImageNet1K 和 CIFAR100  dataset 上训练，与 ViT-B/16 的 GFLOPs 相比，减少了 23.5%，而 validation accuracy 则达到了 69.5% 和 83.4%。
</details></li>
</ul>
<hr>
<h2 id="ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data"><a href="#ViLLA-Fine-Grained-Vision-Language-Representation-Learning-from-Real-World-Data" class="headerlink" title="ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data"></a>ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11194">http://arxiv.org/abs/2308.11194</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordmimi/villa">https://github.com/stanfordmimi/villa</a></li>
<li>paper_authors: Maya Varma, Jean-Benoit Delbrouck, Sarah Hooper, Akshay Chaudhari, Curtis Langlotz</li>
<li>For: 这种研究旨在评估当前的视觉语言模型（VLM）在高度复杂的多模态数据上的表现，以及如何改进VLM以更好地捕捉高度复杂的图像区域和文本特征之间的关系。* Methods: 该研究使用了一种名为ViLLA的新方法，它包括一个轻量级自动生成的地图模型和一个对比度VLM，以学习从复杂数据中捕捉高度复杂的区域特征和文本特征之间的关系。* Results: 研究表明，在四个领域（合成图像、产品图像、医疗图像和自然图像）上，ViLLA可以在精细的理解任务中表现出色，比如零shot对象检测（COCO上AP50点为3.6，LVIS上mAP点为0.6）和检索任务（R-Precision点为14.2）。<details>
<summary>Abstract</summary>
Vision-language models (VLMs), such as CLIP and ALIGN, are generally trained on datasets consisting of image-caption pairs obtained from the web. However, real-world multimodal datasets, such as healthcare data, are significantly more complex: each image (e.g. X-ray) is often paired with text (e.g. physician report) that describes many distinct attributes occurring in fine-grained regions of the image. We refer to these samples as exhibiting high pairwise complexity, since each image-text pair can be decomposed into a large number of region-attribute pairings. The extent to which VLMs can capture fine-grained relationships between image regions and textual attributes when trained on such data has not been previously evaluated. The first key contribution of this work is to demonstrate through systematic evaluations that as the pairwise complexity of the training dataset increases, standard VLMs struggle to learn region-attribute relationships, exhibiting performance degradations of up to 37% on retrieval tasks. In order to address this issue, we introduce ViLLA as our second key contribution. ViLLA, which is trained to capture fine-grained region-attribute relationships from complex datasets, involves two components: (a) a lightweight, self-supervised mapping model to decompose image-text samples into region-attribute pairs, and (b) a contrastive VLM to learn representations from generated region-attribute pairs. We demonstrate with experiments across four domains (synthetic, product, medical, and natural images) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks, such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAP points on LVIS) and retrieval (up to 14.2 R-Precision points).
</details>
<details>
<summary>摘要</summary>
视力语言模型（VLM），如CLIP和ALIGN，通常在图像-描述文本对 obtained from the web 上进行训练。然而，真实世界多Modal数据，如医疗数据，是非常复杂的：每个图像（例如 X-ray）通常与描述多个细腻区域的文本（例如医生报告）相对应。我们称这些样本为具有高对比复杂性，因为每个图像-文本对可以被分解成大量的区域-特征对。VLM 是否能够在这些数据上捕捉细腻的区域-特征关系，尚未被评估。我们的第一个关键贡献是通过系统性的评估表明，随着对于训练数据的对比复杂性的增加，标准的 VLM 会遇到性能下降，最高达37% 的搜索任务。为解决这个问题，我们介绍了我们的第二个关键贡献——ViLLA。ViLLA 是一种可以从复杂数据中捕捉细腻区域-特征关系的模型，它包括两个组件：（a）一种轻量级、自动学习的映射模型，用于将图像-文本对分解成区域-特征对，以及（b）一种对比 VLM，用于从生成的区域-特征对中学习表示。我们通过在四个领域（人工、产品、医疗和自然图像）进行实验，证明 ViLLA 在细腻理解任务中（例如零shot物体检测和搜索）表现出色，比较类似 VLM 高出3.6 AP50 点和0.6 mAP 点。
</details></li>
</ul>
<hr>
<h2 id="Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries"><a href="#Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries" class="headerlink" title="Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"></a>Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab-v2/diversity_measures">https://github.com/lab-v2/diversity_measures</a></li>
<li>paper_authors: Noel Ngu, Nathaniel Lee, Paulo Shakarian</li>
<li>for: 这篇论文旨在提供一些基于它的各种应用领域的错误预测方法，以便更好地评估大语言模型的性能。</li>
<li>methods: 这篇论文使用了三种不同的方法来衡量大语言模型的错误程度，即熵度、金尼鲁分离度和中心距离。这些方法不仅可以独立地评估模型的性能，还可以应用于几个不同的应用场景，如几个示例问题、链式思维和错误检测。</li>
<li>results: 根据实验结果，这三种方法都强相关于模型的失败概率。此外，这些方法还可以应用于不同的数据集和温度设置，并且可以用于评估模型的性能。<details>
<summary>Abstract</summary>
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
</details>
<details>
<summary>摘要</summary>
大型语言模型中的错误预测通常需要对特定领域的信息。在这篇论文中，我们提出了基于响应中的弹性、盖尼不纯和中心距离的三种度量来衡量大型语言模型的错误。我们描述了如何使用这三种度量来评估大型语言模型的错误probability，并在多个数据集和温度设定下进行了一系列实验，以示这些度量强相关于错误的可能性。此外，我们还提供了实验结果，说明了如何使用这些度量来应用少量提示、链接思维和错误探测。
</details></li>
</ul>
<hr>
<h2 id="MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation"><a href="#MISSRec-Pre-training-and-Transferring-Multi-modal-Interest-aware-Sequence-Representation-for-Recommendation" class="headerlink" title="MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation"></a>MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11175">http://arxiv.org/abs/2308.11175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, Shu-Tao Xia</li>
<li>for: 这篇研究旨在解决缺乏ID特征的问题，以及实际推荐情况中的冷开头问题。</li>
<li>methods: 本研究提出了一个多 modal 信息学习框架，包括一个基于 transformer 架构的使用者端 encoder-decoder 模型，以及一个适应项目端的动态融合模块。</li>
<li>results: 实验结果显示，MISSRec 能够实现高效的实际推荐情况下的推荐。<details>
<summary>Abstract</summary>
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence representation. On the candidate item side, we adopt a dynamic fusion module to produce user-adaptive item representation, providing more precise matching between users and items. We pre-train the model with contrastive learning objectives and fine-tune it in an efficient manner. Extensive experiments demonstrate the effectiveness and flexibility of MISSRec, promising an practical solution for real-world recommendation scenarios.
</details>
<details>
<summary>摘要</summary>
目标是强化用户潜在有趣的ITEM predication，基于她/his历史交互序列。现有的大多数分列推荐器都是基于ID特征，尽管广泛使用，但它们经常在罕见ID下表现不佳，并且困难解决冷启动问题。此外，不一致的ID映射使模型的可移植性受阻，隔离类似推荐领域的相似性，这些领域可能可以协同优化。这篇论文旨在解决这些问题，通过学习多 modal 信息来学习Robust和可 generalized 序列表示。我们提议MISSRec，一种多 modal 预训练和传输学习框架，用于SR。用户方面，我们设计了一个基于Transformer的Encoder-Decoder模型，其中Contextual Encoder 学习 capture 序列级别多 modal 共谐，而一种新的兴趣相关 Decoder 被开发以更好地捕捉ITEM-modality-兴趣关系，以提高序列表示。候选ITEM 方面，我们采用动态融合模块生成用户适应ITEM表示，为用户和ITEM之间更精准的匹配提供更多的精度。我们在对照学习目标下预训练模型，并在有效的方式进行精度调整。广泛的实验表明MISSRec的有效性和灵活性，提供了实际解决现实推荐场景中的实际解决方案。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation"><a href="#Hierarchical-Point-based-Active-Learning-for-Semi-supervised-Point-Cloud-Semantic-Segmentation" class="headerlink" title="Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation"></a>Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11166">http://arxiv.org/abs/2308.11166</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SmiletoE/HPAL">https://github.com/SmiletoE/HPAL</a></li>
<li>paper_authors: Zongyi Xu, Bo Yuan, Shanshan Zhao, Qianni Zhang, Xinbo Gao</li>
<li>for: 本研究旨在 Addressing the issue of limited annotations in 3D point cloud segmentation using active learning.</li>
<li>methods: 方法包括一个层次最小准确度模块，以及一种特征距离抑制策略，以选择重要和代表性的点 для人工标注。此外，基于这个活动策略，我们还建立了一个半监督分割框架。</li>
<li>results: 实验结果表明，提档的方法可以达到96.5%和100%的完全监督基线性能，只需使用0.07%和0.1%的训练数据。这些结果超越了当前最佳弱监督和活动学习方法。代码将在<a target="_blank" rel="noopener" href="https://github.com/SmiletoE/HPAL%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/SmiletoE/HPAL中发布。</a><details>
<summary>Abstract</summary>
Impressive performance on point cloud semantic segmentation has been achieved by fully-supervised methods with large amounts of labelled data. As it is labour-intensive to acquire large-scale point cloud data with point-wise labels, many attempts have been made to explore learning 3D point cloud segmentation with limited annotations. Active learning is one of the effective strategies to achieve this purpose but is still under-explored. The most recent methods of this kind measure the uncertainty of each pre-divided region for manual labelling but they suffer from redundant information and require additional efforts for region division. This paper aims at addressing this issue by developing a hierarchical point-based active learning strategy. Specifically, we measure the uncertainty for each point by a hierarchical minimum margin uncertainty module which considers the contextual information at multiple levels. Then, a feature-distance suppression strategy is designed to select important and representative points for manual labelling. Besides, to better exploit the unlabelled data, we build a semi-supervised segmentation framework based on our active strategy. Extensive experiments on the S3DIS and ScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and 100% performance of fully-supervised baseline with only 0.07% and 0.1% training data, respectively, outperforming the state-of-the-art weakly-supervised and active learning methods. The code will be available at https://github.com/SmiletoE/HPAL.
</details>
<details>
<summary>摘要</summary>
具有印象的表现在点云semantic segmentation方面已经由完全监督的方法实现了出色的成绩。由于获得大规模点云数据和点 wise标签的劳动密集，许多尝试已经被作出以探索学习3D点云 segmentation的方法。活动学习是这种目标的有效策略之一，但是仍然受到了不足的探索。最近的这些方法会测量每个预分区的uncertainty，但它们受到重复信息的困扰和需要额外的劳动进行区分。这篇论文目的在于解决这个问题，通过开发一种层次 minimum margin uncertainty module来测量每个点的uncertainty，考虑多个层次的contextual信息。然后，我们设计了一种特征距离抑制策略，以选择重要和代表性的点进行手动标注。此外，为了更好地利用无标注数据，我们构建了基于我们的活动策略的半supervised segmentation框架。广泛的实验在S3DIS和ScanNetV2数据集上表明，我们的提案的框架可以与完全监督基准相同的96.5%和100%的性能，只使用0.07%和0.1%的训练数据。此外，我们的方法还能够超越当前的弱监督和活动学习方法。代码将在https://github.com/SmiletoE/HPAL上提供。
</details></li>
</ul>
<hr>
<h2 id="xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium"><a href="#xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium" class="headerlink" title="xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium"></a>xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11155">http://arxiv.org/abs/2308.11155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zpengmei/xxmd">https://github.com/zpengmei/xxmd</a></li>
<li>paper_authors: Zihan Pengmei, Junyu Liu, Yinan Shu</li>
<li>For: The paper aims to address the limitations of current neural force field (NFF) models in representing chemical reactions by introducing a new dataset called xxMD, which includes energies and forces computed from both multireference wave function theory and density functional theory.* Methods: The paper uses a constrained distribution of internal coordinates and energies in the MD17 datasets to demonstrate their inadequacy for representing systems undergoing chemical reactions. The authors then introduce the xxMD dataset, which includes nuclear configuration spaces that authentically depict chemical reactions, making it a more chemically relevant dataset.* Results: The authors re-assess equivariant models on the xxMD datasets and find notably higher mean absolute errors than those reported for MD17 and its variants, highlighting the challenges faced in crafting a generalizable NFF model with extrapolation capability. The authors propose two new datasets, xxMD-CASSCF and xxMD-DFT, which are available online.<details>
<summary>Abstract</summary>
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional theory. Furthermore, its nuclear configuration spaces authentically depict chemical reactions, making xxMD a more chemically relevant dataset. Our re-assessment of equivariant models on the xxMD datasets reveals notably higher mean absolute errors than those reported for MD17 and its variants. This observation underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT datasets are available at \url{https://github.com/zpengmei/xxMD}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps"><a href="#Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps" class="headerlink" title="Exploring Unsupervised Cell Recognition with Prior Self-activation Maps"></a>Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cpystan/psm">https://github.com/cpystan/psm</a></li>
<li>paper_authors: Pingyi Chen, Chenglu Zhu, Zhongyi Shui, Jiatong Cai, Sunyi Zheng, Shichuan Zhang, Lin Yang</li>
<li>for: 本研究旨在降低生物标注成本，提高生物图像识别效果。</li>
<li>methods: 我们提出了一种基于自动激活图的方法，通过自动激活图中的特征来生成假标记。然后，我们引入了一种语义归一化模块，将假标记转换为像素级别的语义假标记。</li>
<li>results: 我们在两个 histological 数据集上进行评估，结果表明我们的方法可以与其他全盘和弱盘方法竞争，而无需任何手动标注。此外，我们的简单 yet 有效的框架还可以实现多类细胞检测，这在已有的无监督方法中无法完成。<details>
<summary>Abstract</summary>
The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.
</details>
<details>
<summary>摘要</summary>
Successful supervised deep learning models for cell recognition rely heavily on detailed annotations. However, obtaining these annotations can be costly and inefficient. To address this issue, we explored label-free methods for cell recognition. Our proposed method uses prior self-activation maps (PSMs) to generate pseudo masks as training targets. Specifically, we train an activation network using self-supervised learning to generate the PSMs, and then use a semantic clustering module to transform the PSMs into pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets (MoNuSeg and BCData) and found that it can achieve competitive performance without any manual annotations. Our method is simple but effective, and can also perform multi-class cell detection, which is not possible with existing unsupervised methods. The results demonstrate the potential of PSMs to address the need for labels in medical applications.
</details></li>
</ul>
<hr>
<h2 id="Is-There-Any-Social-Principle-for-LLM-Based-Agents"><a href="#Is-There-Any-Social-Principle-for-LLM-Based-Agents" class="headerlink" title="Is There Any Social Principle for LLM-Based Agents?"></a>Is There Any Social Principle for LLM-Based Agents?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11136">http://arxiv.org/abs/2308.11136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jitao Bai, Simiao Zhang, Zhonghao Chen</li>
<li>for: 这篇论文主要是关于大语言模型基于代理的应用。</li>
<li>methods: 论文使用了大语言模型来实现代理，并考虑了社会科学的应用。</li>
<li>results: 论文提出了一种新的代理方法，并通过实验证明了其效果。In English, this translates to:</li>
<li>for: This paper is primarily about the application of large language models based on proxies.</li>
<li>methods: The paper uses large language models to implement proxies and considers applications in social sciences.</li>
<li>results: The paper proposes a new proxy method and experiments prove its effectiveness.<details>
<summary>Abstract</summary>
Focus on Large Language Model based agents should involve more than "human-centered" alignment or application. We argue that more attention should be paid to the agent itself and discuss the potential of social sciences for agents.
</details>
<details>
<summary>摘要</summary>
大语言模型基于代理应该超出人类中心的启aligned或应用。我们认为代理本身应该受到更多的注意力，并讨论社会科学在代理方面的潜力。Here's a word-for-word translation:大语言模型基于代理应该超出人类中心的启aligned或应用。我们认为代理本身应该受到更多的注意力，并讨论社会科学在代理方面的潜力。Note that Simplified Chinese is the standard writing system used in mainland China, while Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation"><a href="#ReLLa-Retrieval-enhanced-Large-Language-Models-for-Lifelong-Sequential-Behavior-Comprehension-in-Recommendation" class="headerlink" title="ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation"></a>ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11131">http://arxiv.org/abs/2308.11131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang</li>
<li>for: 这 paper 主要针对 recommendation  зада务中的 zero-shot 和 few-shot 设置，以提高大语言模型 (LLM) 的表现。</li>
<li>methods: 该 paper 提出了一种 novel 框架，名为 Retrieval-enhanced Large Language models (ReLLa)，用于解决 LLM 在 recommendation 领域中的各种问题。</li>
<li>results: 经过广泛的实验，ReLLa 表现出优于现有基线模型，并能够解决 LLM 在长期序列行为理解方面的问题。<details>
<summary>Abstract</summary>
With large language models (LLMs) achieving remarkable breakthroughs in natural language processing (NLP) domains, LLM-enhanced recommender systems have received much attention and have been actively explored currently. In this paper, we focus on adapting and empowering a pure large language model for zero-shot and few-shot recommendation tasks. First and foremost, we identify and formulate the lifelong sequential behavior incomprehension problem for LLMs in recommendation domains, i.e., LLMs fail to extract useful information from a textual context of long user behavior sequence, even if the length of context is far from reaching the context limitation of LLMs. To address such an issue and improve the recommendation performance of LLMs, we propose a novel framework, namely Retrieval-enhanced Large Language models (ReLLa) for recommendation tasks in both zero-shot and few-shot settings. For zero-shot recommendation, we perform semantic user behavior retrieval (SUBR) to improve the data quality of testing samples, which greatly reduces the difficulty for LLMs to extract the essential knowledge from user behavior sequences. As for few-shot recommendation, we further design retrieval-enhanced instruction tuning (ReiT) by adopting SUBR as a data augmentation technique for training samples. Specifically, we develop a mixed training dataset consisting of both the original data samples and their retrieval-enhanced counterparts. We conduct extensive experiments on a real-world public dataset (i.e., MovieLens-1M) to demonstrate the superiority of ReLLa compared with existing baseline models, as well as its capability for lifelong sequential behavior comprehension.
</details>
<details>
<summary>摘要</summary>
Large language models (LLMs) 在自然语言处理（NLP）领域取得了显著的突破， LLM-enhanced recommender systems 也在当前得到了广泛的关注。在这篇论文中，我们关注在适应和强化纯大语言模型（LLM）的零shot和几shot推荐任务上。首先，我们识别和描述了 LLM 在推荐领域中的生命周期行为无法理解问题，即 LLM 无法从用户行为序列中提取有用信息，即使用户行为序列的长度远远超过 LLM 的上下文限制。为解决这一问题并提高 LLM 的推荐性能，我们提出了一种新的框架，即 Retrieval-enhanced Large Language models (ReLLa)，用于零shot和几shot的推荐任务。 для零shot推荐，我们实施了 semantic user behavior retrieval (SUBR)，以提高测试样本的数据质量，从而减轻 LLM 提取用户行为序列中的关键知识的困难。而为了几shot推荐，我们进一步设计了 retrieval-enhanced instruction tuning (ReiT)，通过采用 SUBR 作为数据增强技术来培育训练样本。具体来说，我们构建了一个混合训练集，包括原始数据样本和其增强后的对应样本。我们在一个真实的公共数据集（即 MovieLens-1M）上进行了广泛的实验，以证明 ReLLa 与现有基eline模型相比，具有更高的优势，同时也能够解决生命周期行为无法理解问题。
</details></li>
</ul>
<hr>
<h2 id="Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances"><a href="#Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances" class="headerlink" title="Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances"></a>Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11129">http://arxiv.org/abs/2308.11129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Luo</li>
<li>for: 本研究旨在提出一种基于层次结构编码的图变换器，以提高图变换器对不同类型图的表现。</li>
<li>methods: 本研究使用了一种名为层次距离结构编码（HDSE）的方法，该方法利用图中节点之间的层次距离来建模图的多层次结构。</li>
<li>results: 经过对12个实际数据集的广泛实验，研究发现，使用HDSE方法可以成功地提高多种基eline transformers的表现，在10个标准测试集上实现了状态的领先性表现。<details>
<summary>Abstract</summary>
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
</details>
<details>
<summary>摘要</summary>
GRaph transformers需要强大的推导性偏好，以derive meaningful attention scores。然而，当前的提议 rarely address methods capturing longer ranges, hierarchical structures, or community structures，as they appear in various graphs such as molecules, social networks, and citation networks。在这篇论文中，我们提议了一种层次距离结构编码(HDSE)，该模型在图中节点之间的层次距离，强调图的多层、层次结构。特别是，这种方法可以flexibly integrate with existing graph transformers，allowing for simultaneous application with other positional representations。通过对12个实际 dataset进行了广泛的实验，我们证明了我们的HDSE方法成功地提高了多种基eline transformers的性能，达到了10个标准 benchmark dataset的状态态表现。
</details></li>
</ul>
<hr>
<h2 id="CAME-Contrastive-Automated-Model-Evaluation"><a href="#CAME-Contrastive-Automated-Model-Evaluation" class="headerlink" title="CAME: Contrastive Automated Model Evaluation"></a>CAME: Contrastive Automated Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11111">http://arxiv.org/abs/2308.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengr/contrastive_autoeval">https://github.com/pengr/contrastive_autoeval</a></li>
<li>paper_authors: Ru Peng, Qiuyang Duan, Haobo Wang, Jiachen Ma, Yanbo Jiang, Yongjun Tu, Xiu Jiang, Junbo Zhao</li>
<li>for: 本研究旨在提出一种新的自动模型评估（AutoEval）框架，以便评估训练完成的机器学习模型而无需使用标注测试集。</li>
<li>methods: 该框架基于一种新的对比损失函数，通过对比测试集中的模型表现和训练集中的模型表现来评估模型的性能。</li>
<li>results: 研究人员通过实验证明，CAME框架可以在AutoEval中达到新的最佳性能水平，超过先前的工作。<details>
<summary>Abstract</summary>
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly.
</details>
<details>
<summary>摘要</summary>
autoeval框架可能无需使用标注测试集来评估已经训练的机器学习模型。尽管存在承诺和一些不错的结果，现有的autoeval方法都仰赖计算分布shift между无标测试集和训练集。我们认为这种依赖于训练集的方法会成为实际ml开发中的另一个障碍。在这项工作中，我们提出了对比自动评估（CAME）框架，它不再需要使用训练集。CAME的核心想法基于对模型性能与对比损失的理论分析。我们通过大量的实验验证，成功地建立了对比测试集上的模型性能和对比损失之间的可预测关系。这种关系可以通过对无标测试集进行推理来获得。CAME的框架在autoeval领域创造了新的最佳实践（SOTA）结果，超过了之前的工作。
</details></li>
</ul>
<hr>
<h2 id="Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models"><a href="#Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models" class="headerlink" title="Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models"></a>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11103">http://arxiv.org/abs/2308.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models">https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models</a></li>
<li>paper_authors: Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</li>
<li>for: The paper explores the potential of large language models (LLMs) to re-identify individuals in court rulings, with a focus on privacy protection in the European Union and Switzerland.</li>
<li>methods: The authors construct a proof-of-concept using actual legal data from the Swiss federal supreme court and create an anonymized Wikipedia dataset for more rigorous testing. They introduce new metrics to measure performance and systematically analyze the factors that influence successful re-identifications.</li>
<li>results: Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions due to a lack of test datasets, the need for substantial training resources, and data sparsity in the information used for re-identification. The study concludes that re-identification using LLMs may not be feasible for now, but it could become possible in the future.Here is the information in Simplified Chinese text:</li>
<li>for: 本研究探讨了大语言模型（LLMs）在法律案例中重新标识个人的可能性，强调欧盟和瑞士隐私保护。</li>
<li>methods: 作者们使用实际的瑞士最高法院判决文档构建了证明，并创建了一个匿名的Wikipedia数据集进行更加严格的测试。他们引入了新的成本度量来衡量表现，并系统地分析了重要的成本因素。</li>
<li>results: 尽管在Wikipedia上获得了高的重新标识率，甚至最好的LLMs在法律案例中仍然遇到了困难，这是因为缺乏测试数据集，需要巨大的训练资源，以及法律案例中数据的稀缺性。研究结论是，使用LLMs进行重新标识可能不太可能，但是未来可能变得可能。<details>
<summary>Abstract</summary>
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. In conclusion, this study demonstrates that re-identification using LLMs may not be feasible for now, but as the proof-of-concept on Wikipedia showed, it might become possible in the future. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.
</details>
<details>
<summary>摘要</summary>
“欧盟和瑞士的司法预测中的匿名性保护是一个重要的问题。随着大规模数据预测技术的发展，对匿名化后的个人重新识别的担忧增加。根据瑞士联邦最高法院的判决，我们进行了一个实验，使用瑞士联邦最高法院的法律数据来测试LLM的重新识别能力。在进一步的测试中，我们使用了一个匿名化的Wikipedia数据集，以更加严谨地检验发现。我们也引入了一个新的任务，即在文本中重新识别个人，并且引入了新的衡量表现的指标。我们系统性地分析了对成功重新识别的影响因素，发现模型大小、输入长度和调整受到最大的影响。尽管在Wikipedia上有高的重新识别率，但是even the best LLMs仅在法院的判决中取得了 moderate的成功率。这些成功率的低度是由于没有足够的测试数据、需要很大的训练资源和数据潜在的缺乏。因此，我们的研究结果表明，使用LLMs进行重新识别可能不太可能，但是在未来，这个技术可能会成为可能的。我们希望，我们的系统可以帮助提高匿名化判决的安全性，使得法院更自信地发布判决。”
</details></li>
</ul>
<hr>
<h2 id="Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification"><a href="#Using-Early-Exits-for-Fast-Inference-in-Automatic-Modulation-Classification" class="headerlink" title="Using Early Exits for Fast Inference in Automatic Modulation Classification"></a>Using Early Exits for Fast Inference in Automatic Modulation Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11100">http://arxiv.org/abs/2308.11100</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elsayed Mohammed, Omar Mashaal, Hatem Abou-Zeid</li>
<li>for: 本研究旨在提高无线通信中的自动模式分类（AMC）技术的效率，通过使用深度学习（DL）技术提取无线信号特征。</li>
<li>methods: 本研究提出使用早期离开（EE）技术加速DL模型的推理，并研究了四种不同的早期离开架构和自定义多支分支训练算法。</li>
<li>results: 通过广泛的实验，我们发现对于中度到高度的信号含杂率（SNR），使用EE技术可以显著降低深度神经网络的推理速度，而不会产生分类精度的下降。我们还进行了推理时间与分类精度之间的平衡分析。这是目前所知道的首次应用EE技术于AMC领域的研究。<details>
<summary>Abstract</summary>
Automatic modulation classification (AMC) plays a critical role in wireless communications by autonomously classifying signals transmitted over the radio spectrum. Deep learning (DL) techniques are increasingly being used for AMC due to their ability to extract complex wireless signal features. However, DL models are computationally intensive and incur high inference latencies. This paper proposes the application of early exiting (EE) techniques for DL models used for AMC to accelerate inference. We present and analyze four early exiting architectures and a customized multi-branch training algorithm for this problem. Through extensive experimentation, we show that signals with moderate to high signal-to-noise ratios (SNRs) are easier to classify, do not require deep architectures, and can therefore leverage the proposed EE architectures. Our experimental results demonstrate that EE techniques can significantly reduce the inference speed of deep neural networks without sacrificing classification accuracy. We also thoroughly study the trade-off between classification accuracy and inference time when using these architectures. To the best of our knowledge, this work represents the first attempt to apply early exiting methods to AMC, providing a foundation for future research in this area.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:自动模式分类（AMC）在无线通信中扮演了关键的角色，可以自动将广播信号分类。深度学习（DL）技术在AMC中越来越受到关注，因为它们可以提取广播信号的复杂特征。然而，DL模型具有高计算复杂度和高推理延迟。这篇论文提出使用早退出（EE）技术来加速DL模型在AMC中的推理。我们提出了四种EE架构和一种自定义多支分支训练算法。经过广泛的实验，我们发现在moderate to high signal-to-noise ratio（SNR）下，信号更容易分类，不需要深度的架构，可以利用我们提出的EE架构。我们的实验结果表明，EE技术可以减少深度神经网络的推理速度，而不会增加分类精度的损失。我们还在使用这些架构时进行了严格的质量评估和时间评估。根据我们所知，这是首次将EE技术应用于AMC，这为未来的相关研究提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video"><a href="#Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video" class="headerlink" title="Video OWL-ViT: Temporally-consistent open-world localization in video"></a>Video OWL-ViT: Temporally-consistent open-world localization in video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11093">http://arxiv.org/abs/2308.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Lučić, Fisher Yu, Thomas Kipf</li>
<li>for: 本研究旨在适应预训练的开放视界图像模型到视频本地化中。</li>
<li>methods: 我们基于OWL-ViT开放词汇检测模型，并添加了一个变换器解码器，以卷积神经网络输出的一帧图像作为下一帧对象查询。</li>
<li>results: 我们的模型在面对挑战性的TAO-OWBenchmark上表现出色，证明了预训练大量图像文本数据可以成功传递到开放视界本地化中。<details>
<summary>Abstract</summary>
We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pre-training, can be transferred successfully to open-world localization across diverse videos.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:我们提出了一种架构和训练方法，可以将预训练的开放视界图像模型适应到视频地图Localization。理解开放视界（不受固定标签空间约束）是许多实际视觉任务的关键。在大量图像文本数据集上进行对比预训练，最近导致了图像级别任务的显著改进。然而，对于结构化任务，如对象localization，使用预训练模型更加困难。特别是在视频任务中，任务特定数据受限。我们在OWL-ViT开放词汇探测模型的基础上建立了一个Transformer解码器，以便在视频中传播对象表示。解码器使用下一帧的输出符号来作为下一帧的对象查询。我们的模型是基于视频数据的端到端训练的，并且比较tracking-by-detection基eline更有优势，同时保留了预训练模型的开放视界能力。我们在TAO-OWbenchmark上评估了我们的模型，并证明了可以成功传递开放视界的能力，从大规模图像文本预训练中学习到开放视界地图Localization across多种视频。
</details></li>
</ul>
<hr>
<h2 id="Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response"><a href="#Collaborative-Route-Planning-of-UAVs-Workers-and-Cars-for-Crowdsensing-in-Disaster-Response" class="headerlink" title="Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response"></a>Collaborative Route Planning of UAVs, Workers and Cars for Crowdsensing in Disaster Response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11088">http://arxiv.org/abs/2308.11088</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Han, Chunyu Tu, Zhiwen Yu, Zhiyong Yu, Weihua Shan, Liang Wang, Bin Guo</li>
<li>for: 本研究旨在提高灾区内部合作多代理器（UAV、工人和车辆）的数据收集效率。</li>
<li>methods: 本研究提出了一个多代理器路径观察法（MANF-RL-RP），具有多效设计，包括全球与本地信息处理、特定多代理器系统模型结构等。</li>
<li>results: 比较基准算法（Greedy-SC-RP和MANF-DNN-RP），MANF-RL-RP在任务完成率方面有显著提高。<details>
<summary>Abstract</summary>
Efficiently obtaining the up-to-date information in the disaster-stricken area is the key to successful disaster response. Unmanned aerial vehicles (UAVs), workers and cars can collaborate to accomplish sensing tasks, such as data collection, in disaster-stricken areas. In this paper, we explicitly address the route planning for a group of agents, including UAVs, workers, and cars, with the goal of maximizing the task completion rate. We propose MANF-RL-RP, a heterogeneous multi-agent route planning algorithm that incorporates several efficient designs, including global-local dual information processing and a tailored model structure for heterogeneous multi-agent systems. Global-local dual information processing encompasses the extraction and dissemination of spatial features from global information, as well as the partitioning and filtering of local information from individual agents. Regarding the construction of the model structure for heterogeneous multi-agent, we perform the following work. We design the same data structure to represent the states of different agents, prove the Markovian property of the decision-making process of agents to simplify the model structure, and also design a reasonable reward function to train the model. Finally, we conducted detailed experiments based on the rich simulation data. In comparison to the baseline algorithms, namely Greedy-SC-RP and MANF-DNN-RP, MANF-RL-RP has exhibited a significant improvement in terms of task completion rate.
</details>
<details>
<summary>摘要</summary>
efficiently 获取在灾难 struck 地区的最新信息是灾难应对的关键。无人飞行器（UAV）、工人和车辆可以在灾难 struck 地区合作完成感知任务，如数据收集。在这篇论文中，我们明确地讨论了一组代理人（包括UAV、工人和车辆）的路径规划，以最大化任务完成率为目标。我们提出了多Agent Route Planning Algorithm（MANF-RL-RP），该算法包括了许多高效的设计，如全球-本地双信息处理和特定的模型结构 для多种Agent系统。全球-本地双信息处理包括从全球信息中提取和传递空间特征，以及来自个体代理人的本地信息的分区和筛选。在构建多种Agent系统的模型结构方面，我们进行了以下工作。我们设计了同样的数据结构来表示不同代理人的状态，证明代理人决策过程的markt价性以简化模型结构，并设计了合理的奖励函数来训练模型。最后，我们对着富有的 simulate 数据进行了详细的实验。与基准算法（即Greedy-SC-RP和MANF-DNN-RP）相比，MANF-RL-RP 在任务完成率方面表现出了显著的提升。
</details></li>
</ul>
<hr>
<h2 id="Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning"><a href="#Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning" class="headerlink" title="Neural Amortized Inference for Nested Multi-agent Reasoning"></a>Neural Amortized Inference for Nested Multi-agent Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11071">http://arxiv.org/abs/2308.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Jha, Tuan Anh Le, Chuanyang Jin, Yen-Ling Kuo, Joshua B. Tenenbaum, Tianmin Shu</li>
<li>for: 本研究旨在提高多智能体交互中的复杂社会推理能力，使其能够更好地理解别人对自己的推理。</li>
<li>methods: 本研究使用神经网络来减轻高阶社会推理的计算复杂性，以提高多智能体交互的效率。</li>
<li>results: 实验结果表明，我们的方法可以减少计算复杂性，同时减少准确性的削弱。<details>
<summary>Abstract</summary>
Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
</details>
<details>
<summary>摘要</summary>
多代理交互，如通信、教学和威胁，经常需要高级社会推理，即理解他们如何推理自己。这种复杂的推理可以通过嵌套多代理推理来有效模型。然而，计算复杂性随着每层推理层数的增加而呈指数增长， pose significant challenges。然而，人类在日常生活中很自然地完成复杂的社会推理。为了bridging这个 gap，我们提出了一种新的方法：利用神经网络来减轻高级社会推理，从而加快嵌套多代理推理。我们在两个复杂多代理交互领域进行了实验，结果表明我们的方法具有高效性和减少准确性下降的能力。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition"><a href="#Temporal-Distributed-Backdoor-Attack-Against-Video-Based-Action-Recognition" class="headerlink" title="Temporal-Distributed Backdoor Attack Against Video Based Action Recognition"></a>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11070">http://arxiv.org/abs/2308.11070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Li, Songhe Wang, Ruiquan Huang, Mahanth Gowda, George Kesidis</li>
<li>for: 本研究旨在探讨视频数据下的后门攻击（Trojan），以及现有模型对这种攻击的抵御性。</li>
<li>methods: 本研究提出了一种简单 yet 有效的后门攻击方法，通过在转换域中添加杂音来植入潜在的攻击词。这种攻击可以在视频帧中逐帧插入，并且可以在攻击后继续保持高准确率。</li>
<li>results: 经过广泛的实验，研究人员发现这种攻击方法可以在多种知名模型上达到高度可见性和鲁棒性，并且可以在不同的视频识别 benchmark 上实现攻击。此外，研究人员还发现了一种称为 “Collateral Damage” 的现象，即在攻击过程中可能会导致模型对非目标类型的数据进行误分类。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distributed} trigger across the video frames, and is shown to be resilient to existing defensive strategies. The effectiveness of the proposed attack is demonstrated by extensive experiments with various well-known models on two video recognition benchmarks, UCF101 and HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL) dataset. We delve into the impact of several influential factors on our proposed attack and identify an intriguing effect termed "collateral damage" through extensive studies.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在不同应用场景中取得了很大成功，如视频动作识别，然而它们却容易受到后门攻击（Trojan）。攻击者可以通过特定的触发符使得恶意修改的模型在测试实例（非目标类）中产生错误分类，而保持高精度水平。虽然对于图像数据已有广泛的研究，但视频系统对于后门攻击的抗性仍然尚未得到充分研究。现有的研究多是对图像数据进行直接扩展，例如在帧内独立地插入触发符，这些触发符可以被现有的防御策略检测。在这篇论文中，我们提出了一种简单又有效的后门攻击方法，通过在转换域中添加噪声，在视频帧中植入不可见、时间分布的触发符，并证明其具有抗性。我们通过对多种知名模型在UCf101、HMDB51和希腊手语认知 benchmark 上进行了广泛的实验，证明了我们的提案的有效性。我们还进行了详细的研究，探讨了一些影响我们提案的因素，并发现了一种感人的效果，我们称之为“副作用”。
</details></li>
</ul>
<hr>
<h2 id="Topological-Graph-Signal-Compression"><a href="#Topological-Graph-Signal-Compression" class="headerlink" title="Topological Graph Signal Compression"></a>Topological Graph Signal Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11068">http://arxiv.org/abs/2308.11068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Bernárdez, Lev Telyatnikov, Eduard Alarcón, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Liò</li>
<li>for: 这 paper 的目的是提出一种基于 Topological Deep Learning (TDL) 方法来压缩信号 над 图 structures。</li>
<li>methods: 这 paper 使用的方法包括对原始信号进行分 clustering，然后使用 topological-inspired message passing 获取压缩后的信号表示。</li>
<li>results: 该方法可以在两个实际 Internet Service Provider Networks 的数据集上提高标准 GNN 和 feed-forward 架构的压缩性能，从 $30%$ 到 $90%$ 的压缩率提高，表明它更好地捕捉和利用图结构中的空间和时间相关性。<details>
<summary>Abstract</summary>
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and temporal correlations over the whole graph-based network structure.
</details>
<details>
<summary>摘要</summary>
最近爆发的拓扑深度学习（TDL）方法希望可以补充当前图ael neural network（GNN）的限制，自然处理更高阶交互，超出现有图表示中的对角相关和本地邻里hood。在这篇论文中，我们提出了一种基于TDL的图信号压缩方法，包括两个主要步骤：首先，通过原始信号对$N$个数据点进行分 clustering，将其分成$K\ll N$个集合；然后，基于图的拓扑结构，进行多元素集合内的扩展传递，以获得压缩后的信号表示。我们的结果表明，我们的框架可以在两个实际世界互联网服务提供商网络数据集上，将标准GNN和批处理架构超越，在压缩时间链接基于网络结构中的信号方面达到$30\%$到$90\%$的更好的重建错误，表明它更好地捕捉和利用图结构中的空间和时间相关性。
</details></li>
</ul>
<hr>
<h2 id="CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection"><a href="#CSM-H-R-An-Automatic-Context-Reasoning-Framework-for-Interoperable-Intelligent-Systems-and-Privacy-Protection" class="headerlink" title="CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection"></a>CSM-H-R: An Automatic Context Reasoning Framework for Interoperable Intelligent Systems and Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11066">http://arxiv.org/abs/2308.11066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/songhui01/csm-h-r">https://github.com/songhui01/csm-h-r</a></li>
<li>paper_authors: Songhui Yue, Xiaoyan Hong, Randy K. Smith</li>
<li>for: 这个论文的目的是提出一个自动化高级上下文（HLC）理解框架，以便在智能系统规模上实现智能系统的自动化整合。</li>
<li>methods: 该框架使用ontology和状态在运行时和模型存储阶段进行程序式组合，以实现意义full HLC的认知，并将结果应用于不同的理解技术。</li>
<li>results: 实验表明，该框架可以自动捕捉和理解高级上下文，并将其转换为可以应用于不同的理解技术的数据表示。此外，该框架还实现了隐私保护功能，通过域嵌入和信息卷积来减少信息相关性。<details>
<summary>Abstract</summary>
Automation of High-Level Context (HLC) reasoning for intelligent systems at scale is imperative due to the unceasing accumulation of contextual data in the IoT era, the trend of the fusion of data from multi-sources, and the intrinsic complexity and dynamism of the context-based decision-making process. To mitigate this issue, we propose an automatic context reasoning framework CSM-H-R, which programmatically combines ontologies and states at runtime and the model-storage phase for attaining the ability to recognize meaningful HLC, and the resulting data representation can be applied to different reasoning techniques. Case studies are developed based on an intelligent elevator system in a smart campus setting. An implementation of the framework - a CSM Engine, and the experiments of translating the HLC reasoning into vector and matrix computing especially take care of the dynamic aspects of context and present the potentiality of using advanced mathematical and probabilistic models to achieve the next level of automation in integrating intelligent systems; meanwhile, privacy protection support is achieved by anonymization through label embedding and reducing information correlation. The code of this study is available at: https://github.com/songhui01/CSM-H-R.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）技术在智能系统中的应用在不断增长，特别是在互联网物联网（IoT）时代，数据来源的融合和上下文决策过程的内在复杂性和动态性使得高级上下文（HLC）理解成为非常重要的。为解决这一问题，我们提出了一个自动上下文理解框架CSM-H-R，该框架在运行时和模型存储阶段使用ontologies和状态进行程序性结合，以实现对有意义的HLC的识别，并且可以应用于不同的理解技术。在智能电梯系统的实际案例中，我们开发了CSM引擎，并对HLC理解进行了vector和矩阵计算的实验，特别是处理上下文的动态性，表明了使用高级数学和统计模型可以实现下一个自动化层次的智能系统集成。同时，我们实现了隐私保护支持，通过嵌入标签和减少信息相关性来实现隐身。CSM框架的代码可以在以下链接中找到：https://github.com/songhui01/CSM-H-R。
</details></li>
</ul>
<hr>
<h2 id="FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning"><a href="#FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning" class="headerlink" title="FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning"></a>FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, Volker Tresp</li>
<li>for: 这则研究旨在提高基础模型在多modal学习中的表现，并且解决集中训练数据的问题。</li>
<li>methods: 本研究使用 Federated Dual-Adapter Teacher (FedDAT) 方法，具有调整客户端本地更新和实施多元知识传播 (MKD)，以解决客户端数据不具同一性的问题。</li>
<li>results: 实验结果显示，FedDAT 在多modal Vision-Language 任务上substantially 超过了现有的中央化 PEFT 方法适应 FL 的表现。<details>
<summary>Abstract</summary>
Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.
</details>
<details>
<summary>摘要</summary>
最近，基金会模型在多模态学习中展现了显著的进步。这些模型通常需要大量数据进行微调，但收集和中央化训练数据因为不同隐私规定而变得困难。为了解决这问题，聚合学习（FL）成为了一种有前途的解决方案，允许多个客户共同训练神经网络，无需中央化本地数据。以减少客户计算负担和通信开销为目的，先前的工作已经采用了参数效率微调（PEFT）方法进行FL。然而，大多数先前的工作宁悠单一模式，忽视了客户端数据的不同性。因此，在本工作中，我们提出了适应多模式、多数据类型 federated 微调框架，称为 FedDAT。具体来说，我们的方法利用了双适应教师（DAT）来处理客户端数据的不同性，通过规则化客户端本地更新和应用知识传播（MKD）进行高效的知识传递。FedDAT 是首个能够有效地在多模态 FL 上进行基础模型的分布式微调。为证明其效果，我们在四个多模态 FL 测试准则上进行了广泛的实验，其中 FedDAT 在不同类型的数据不同性下显著超过了已有的中央化 PEFT 方法。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation"><a href="#Beyond-Discriminative-Regions-Saliency-Maps-as-Alternatives-to-CAMs-for-Weakly-Supervised-Semantic-Segmentation" class="headerlink" title="Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation"></a>Beyond Discriminative Regions: Saliency Maps as Alternatives to CAMs for Weakly Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11052">http://arxiv.org/abs/2308.11052</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Maruf, Arka Daw, Amartya Dutta, Jie Bu, Anuj Karpatne</li>
<li>for: 本研究比较了抽象图和特征图两种方法在弱监督 semantic segmentation (WS3) 中的表现，并提出了一些新的评价指标来全面评估这两种方法的性能。</li>
<li>methods: 本研究使用了特征图和抽象图两种方法来生成pseudo-ground truth，并通过多个视角来比较它们的相似性和不同性。</li>
<li>results: 研究发现，使用抽象图可以更好地解决WS3中的非特征区域 (NDR) 问题，并且通过随机裁剪提高了抽象图的性能。<details>
<summary>Abstract</summary>
In recent years, several Weakly Supervised Semantic Segmentation (WS3) methods have been proposed that use class activation maps (CAMs) generated by a classifier to produce pseudo-ground truths for training segmentation models. While CAMs are good at highlighting discriminative regions (DR) of an image, they are known to disregard regions of the object that do not contribute to the classifier's prediction, termed non-discriminative regions (NDR). In contrast, attribution methods such as saliency maps provide an alternative approach for assigning a score to every pixel based on its contribution to the classification prediction. This paper provides a comprehensive comparison between saliencies and CAMs for WS3. Our study includes multiple perspectives on understanding their similarities and dissimilarities. Moreover, we provide new evaluation metrics that perform a comprehensive assessment of WS3 performance of alternative methods w.r.t. CAMs. We demonstrate the effectiveness of saliencies in addressing the limitation of CAMs through our empirical studies on benchmark datasets. Furthermore, we propose random cropping as a stochastic aggregation technique that improves the performance of saliency, making it a strong alternative to CAM for WS3.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Weakly Supervised Semantic Segmentation" (WS3) is translated as "弱指示 semantic segmentation" (WS3) in Simplified Chinese.* "Class activation map" (CAM) is translated as "类划分图" (CAM) in Simplified Chinese.* "Discriminative regions" (DR) is translated as "分化区" (DR) in Simplified Chinese.* "Non-discriminative regions" (NDR) is translated as "非分化区" (NDR) in Simplified Chinese.* "Attribution methods" such as "saliency maps" is translated as "责任方法" such as "吸引图" in Simplified Chinese.* "Stochastic aggregation technique" such as "random cropping" is translated as "随机聚合技术" such as "随机裁剪" in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Personalized-Event-Prediction-for-Electronic-Health-Records"><a href="#Personalized-Event-Prediction-for-Electronic-Health-Records" class="headerlink" title="Personalized Event Prediction for Electronic Health Records"></a>Personalized Event Prediction for Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11013">http://arxiv.org/abs/2308.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Min Lee, Milos Hauskrecht</li>
<li>For: The paper aims to develop accurate predictive models of clinical event sequences to support patient care, specifically by addressing the challenge of patient-specific variability in clinical conditions.* Methods: The paper proposes and investigates multiple new event sequence prediction models and methods, including refinement of population-wide models to subpopulations, self-adaptation, and meta-level model switching.* Results: The paper analyzes and tests the performance of these models on clinical event sequences of patients in the MIMIC-III database.<details>
<summary>Abstract</summary>
Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for individual patients and their specific conditions. The methods developed in this work pursue refinement of population-wide models to subpopulations, self-adaptation, and a meta-level model switching that is able to adaptively select the model with the best chance to support the immediate prediction. We analyze and test the performance of these models on clinical event sequences of patients in MIMIC-III database.
</details>
<details>
<summary>摘要</summary>
临床事件序列包括数百个临床事件记录，表示患者 receiving 的记录时间。 开发准确预测模型临床序列非常重要，以支持多种模型，用于解释/分类当前患者状况，预测不良临床事件和结果，以提高患者治疗。 一个重要的预测临床序列模型挑战是每个患者的病人特有性。 基于下面的临床状况，每个患者的序列可能包含不同的临床事件（观察结果、实验室测试、药物、手术）。 因此，从事件序列中学习的人口广泛模型可能无法准确预测每个患者的特定动态和差异。 为解决问题，我们提出和探索多种新的事件序列预测模型和方法，使我们能更好地适应患者和其特定状况。 我们在MIMIC-III数据库中分析和测试这些模型的性能。
</details></li>
</ul>
<hr>
<h2 id="“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion"><a href="#“Guinea-Pig-Trials”-Utilizing-GPT-A-Novel-Smart-Agent-Based-Modeling-Approach-for-Studying-Firm-Competition-and-Collusion" class="headerlink" title="“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion"></a>“Guinea Pig Trials” Utilizing GPT: A Novel Smart Agent-Based Modeling Approach for Studying Firm Competition and Collusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10974">http://arxiv.org/abs/2308.10974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xu Han, Zengqing Wu, Chuan Xiao</li>
<li>For: The paper is written to study firm competition and collusion using a novel framework called Smart Agent-Based Modeling (SABM), which employs GPT-4 technologies to represent firms and their interactions.* Methods: The study uses a controlled experiment with smart agents to examine firm price competition and collusion behaviors under various conditions, comparing the results to those obtained through experiments with human subjects.* Results: The paper finds that smart agents consistently reach tacit collusion in the absence of communication, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. With communication allowed, smart agents achieve a higher-level collusion with prices close to cartel prices, and collusion forms more quickly with communication. These results highlight the importance of communication in enhancing trust between firms and facilitating collusion.<details>
<summary>Abstract</summary>
Firm competition and collusion involve complex dynamics, particularly when considering communication among firms. Such issues can be modeled as problems of complex systems, traditionally approached through experiments involving human subjects or agent-based modeling methods. We propose an innovative framework called Smart Agent-Based Modeling (SABM), wherein smart agents, supported by GPT-4 technologies, represent firms, and interact with one another. We conducted a controlled experiment to study firm price competition and collusion behaviors under various conditions. SABM is more cost-effective and flexible compared to conducting experiments with human subjects. Smart agents possess an extensive knowledge base for decision-making and exhibit human-like strategic abilities, surpassing traditional ABM agents. Furthermore, smart agents can simulate human conversation and be personalized, making them ideal for studying complex situations involving communication. Our results demonstrate that, in the absence of communication, smart agents consistently reach tacit collusion, leading to prices converging at levels higher than the Bertrand equilibrium price but lower than monopoly or cartel prices. When communication is allowed, smart agents achieve a higher-level collusion with prices close to cartel prices. Collusion forms more quickly with communication, while price convergence is smoother without it. These results indicate that communication enhances trust between firms, encouraging frequent small price deviations to explore opportunities for a higher-level win-win situation and reducing the likelihood of triggering a price war. We also assigned different personas to firms to analyze behavioral differences and tested variant models under diverse market structures. The findings showcase the effectiveness and robustness of SABM and provide intriguing insights into competition and collusion.
</details>
<details>
<summary>摘要</summary>
企业竞争和勾结涉及到复杂的动态，特别是在公司之间的交流方面。这些问题可以通过人类实验或智能代理模型（ABM）来模拟。我们提出了一种创新的框架called Smart Agent-Based Modeling（SABM），其中智能代理，受到GPT-4技术支持，代表公司，并互动相互。我们进行了一项控制性实验，以研究企业价格竞争和勾结行为的不同情况。SABM相比人类实验更加经济和灵活。智能代理具有广泛的知识库和人类策略能力，超过传统ABM代理。此外，智能代理可以模拟人类对话，可个性化，使其适用于研究复杂的交流情况。我们的结果表明，在无交流情况下，智能代理一般会达成tacit collusion，导致价格相对于BERTRAND平衡价格高，但比单一垄断或垄断价格低。当交流被允许时，智能代理可以实现更高级别的勾结，价格接近垄断价格。勾结形成更快，无交流情况下价格均衡更平滑。这些结果表明，交流可以增强公司之间的信任，使小价格偏移更频繁地探索机会，降低价格战的可能性。我们还将不同的公司个性分配给不同的公司，以分析行为差异，并在多种市场结构下测试不同的模型。结果显示SABM的效果和稳定性，并提供了精彩的竞争和勾结的新思路。
</details></li>
</ul>
<hr>
<h2 id="DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering"><a href="#DocPrompt-Large-scale-continue-pretrain-for-zero-shot-and-few-shot-document-question-answering" class="headerlink" title="DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering"></a>DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10959">http://arxiv.org/abs/2308.10959</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sijin Wu, Dan Zhang, Teng Hu, Shikun Feng</li>
<li>for: 文章旨在提出一种名为 Docprompt 的文档问答模型，可以在文档问答任务中实现强大的零学习和几学习性能。</li>
<li>methods: 文章提出了一种新的弱监督数据生成方法、一种多Stage训练方法和一种理解模型&amp;生成模型集成方法。</li>
<li>results: 实验结果显示， после继续预训练， Docprompt 模型在文档问答任务上明显超过了现有的强基线模型，并且可以大幅提高文档问答客户项目的交付效率和模型性能，降低注释成本和劳动成本。<details>
<summary>Abstract</summary>
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model & generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了 Docprompt，用于文档问答任务的强大零shot和几shot性能的解决方案。我们提出了一种新的软参数生成方法、一种多Stage训练方法和一种新的理解模型&生成模型结合方法。实验结果显示，在继续预训练后，Docprompt模型在文档问答任务上明显超越了现有的强基线模型。这种方法可以大幅提高文档问答客户项目的交付效率和模型性能，降低注释成本和劳动成本。您可以在https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout找到我们的demo。
</details></li>
</ul>
<hr>
<h2 id="Structured-World-Models-from-Human-Videos"><a href="#Structured-World-Models-from-Human-Videos" class="headerlink" title="Structured World Models from Human Videos"></a>Structured World Models from Human Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10901">http://arxiv.org/abs/2308.10901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russell Mendonca, Shikhar Bahl, Deepak Pathak</li>
<li>For: The paper aims to enable robots to learn complex manipulation skills directly in the real world using a small amount of interaction data.* Methods: The approach uses human video data to build a structured, human-centric action space grounded in visual affordances, and trains a world model on human videos before fine-tuning on a small amount of robot interaction data without task supervision.* Results: The approach allows robots to learn various manipulation skills in complex settings in under 30 minutes of interaction.Here is the same information in Simplified Chinese:* For: 论文旨在帮助机器人直接在真实世界中学习复杂的抓取技能，只需要很少的互动数据。* Methods: 方法使用人类视频数据构建一个基于视觉可用性的结构化人类行为空间，然后在人类视频上训练世界模型，并在小量机器人互动数据上练习而不需要任务指导。* Results: 方法可以让机器人在复杂的设置下快速学习多种抓取技能，仅需要30分钟的互动。<details>
<summary>Abstract</summary>
We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io
</details>
<details>
<summary>摘要</summary>
我们面临的问题是直接在实际世界中学习复杂的通用行为。我们提议一种方法，使用只需一些不同场景的实际互动轨迹来教育机器人快速学习抓取技能。从计算机视觉和自然语言学习领域的成功经验中，我们认为，为了高效地学习，机器人必须能够利用互联网规模的人类视频数据。人类在与世界交互中有很多有趣的方式，这些方式可以帮助机器人不仅构建有用的动作和可用性的理解，还可以了解这些动作如何影响世界进行抓取。我们的方法是建立基于视觉可用性学习的人类行为空间，并在这个空间中训练一个世界模型。我们在人类视频上进行了训练，并在少量机器人互动数据上进行了精度调整。我们显示，这种可用性空间世界模型的方法可以让不同的机器人在复杂的设置下快速学习多种抓取技能，仅用30分钟的互动。视频可以在https://human-world-model.github.io找到。
</details></li>
</ul>
<hr>
<h2 id="TADA-Text-to-Animatable-Digital-Avatars"><a href="#TADA-Text-to-Animatable-Digital-Avatars" class="headerlink" title="TADA! Text to Animatable Digital Avatars"></a>TADA! Text to Animatable Digital Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10899">http://arxiv.org/abs/2308.10899</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/TingtingLiao/TADA">https://github.com/TingtingLiao/TADA</a></li>
<li>paper_authors: Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black</li>
<li>For: The paper aims to generate high-quality 3D avatars from textual descriptions, with realistic animations and detailed geometry.* Methods: The approach uses a 2D diffusion model and an animatable parametric body model, along with hierarchical rendering and score distillation sampling (SDS) to create detailed 3D avatars from text.* Results: The paper demonstrates that TADA significantly surpasses existing approaches on both qualitative and quantitative measures, enabling the creation of large-scale digital character assets that are ready for animation and rendering, and are easily editable through natural language.<details>
<summary>Abstract</summary>
We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent alignment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.
</details>
<details>
<summary>摘要</summary>
我们介绍TADA，一个简单又有效的方法，将文本描述转换为高品质的3D人物模型，包括高级的几何和生命力的纹理，可以通过传统的グラフィックス管线进行动画和渲染。现有的文本基于的人物生成方法受到几何和纹理质量的限制，并且无法真实地动画，因为几何和纹理之间的对齐不稳定，尤其是在脸部区域。为了突破这些限制，TADA利用了2D传播模型和可动的 Parametric Body Model。具体来说，我们从SMPL-X中 derivated一个可优化的高分辨率人体模型，包括3D偏移和纹理图像，并使用层次渲染和分析抽象 Sampling (SDS) 创建高品质、细节满怀的3D人物。为了保证几何和纹理之间的对齐，我们在SDS训练过程中使用 render 的 норма和 RGB 图像，并利用它们的隐藏嵌入来稳定训练。此外，我们还引入了多种表情参数，以使得生成的人物在训练过程中具有表情，以保持与原始 SMPL-X 模型的 semantics 一致，使得生成的人物可以被动画。我们的评估结果显示，TADA 在 both 质量和量化度上有所提高，与现有的方法相比。TADA 可以实现大规模的数码人物资产的创建，并且可以通过自然语言进行易于修改。我们将代码公开供研究用途。
</details></li>
</ul>
<hr>
<h2 id="Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs"><a href="#Giraffe-Adventures-in-Expanding-Context-Lengths-in-LLMs" class="headerlink" title="Giraffe: Adventures in Expanding Context Lengths in LLMs"></a>Giraffe: Adventures in Expanding Context Lengths in LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10882">http://arxiv.org/abs/2308.10882</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abacusai/long-context">https://github.com/abacusai/long-context</a></li>
<li>paper_authors: Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu</li>
<li>for: 这个论文主要用于探讨现代大型自然语言处理器（LLMs）如何在评估时处理长输入序列。</li>
<li>methods: 该论文使用现有的context length extrapolation方法，包括修改 pozitional encoding 系统以指示输入序列中token或活动的位置。并 introduce some new design,如修改基于position encoding的减少策略。</li>
<li>results: 该论文通过三个新的评估任务（FreeFormQA、AlteredNumericQA和LongChat-Lines）以及折减指标来测试这些方法。发现线性扩展是最佳的扩展方法，并示出可以通过使用更长的扩展级别在评估时获得更好的性能。同时，发现修改基于position encoding的减少策略也有扩展能力。基于这些结果，该论文释放了三个新的13B参数长Context模型，即4k和16k context模型从基础LLaMA-13B中训练，以及32k context模型从基础LLaMA2-13B中训练。同时还释放了 reproduce 结果的代码。<details>
<summary>Abstract</summary>
Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.   We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.
</details>
<details>
<summary>摘要</summary>
现代大语言模型（LLM）通常通过注意机制训练，但是它们的评估时间上下文长度是固定的，这限制了它们可以处理的输入序列长度。为了使这些模型处理 longer than train-time context length 的序列，可以使用Context length extrapolation方法。我们对现有的方法进行了广泛的survey，并介绍了一些我们自己的设计，包括一种新的截断策略 для修改基于位置编码的系统。我们使用三个新的评估任务（FreeFormQA、AlteredNumericQA和LongChat-Lines）以及折叠指标来测试这些方法。我们发现线性扩展是最佳的扩展方法，并且可以通过使用更长的扩展级别来进一步提高性能。此外，我们发现 truncated basis 具有扩展的潜在能力。为支持进一步的研究，我们释放了三个13B参数的长 context模型，即4k和16k上下文模型从基础 LLMA-13B 开始，以及32k上下文模型从基础 LLMA2-13B 开始。我们还释放了复制我们结果的代码。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space"><a href="#Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space" class="headerlink" title="Analyzing Transformer Dynamics as Movement through Embedding Space"></a>Analyzing Transformer Dynamics as Movement through Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10874">http://arxiv.org/abs/2308.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet S. Singh</li>
<li>for: This paper explores the underlying mechanics of Transformer language models and how they give rise to intelligent behaviors.</li>
<li>methods: The authors use a systems approach to analyze Transformers and develop a mathematical framework that views the models as movement through embedding space.</li>
<li>results: The paper reveals important insights into the emergence of intelligence in Transformers, including the idea that the models are essentially “Embedding Space walkers” that compose context into a single vector, and that attention plays a key role in associating vectors and influencing the organization of the embedding space. Additionally, the authors find some evidence for their semantic space theory, which posits that embedding vectors represent semantic concepts.<details>
<summary>Abstract</summary>
Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools. This paper explores how their underlying mechanics give rise to intelligent behaviors. We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space. This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1. At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   2. At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   3. No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   4. Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers. These abilities are properties of this organization.   5. Attention's contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization. However, more investigation is needed to ascertain its significance.   6. The entire model is composed from two principal operations: data independent filtering and data dependent aggregation. This generalization unifies Transformers with other sequence models and across modalities.   Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity.
</details>
<details>
<summary>摘要</summary>
吸收器语言模型展示出智能行为，如理解自然语言、识别模式、获得知识、reasoning、规划、反思和使用工具。这篇论文探讨它们的基本机制如何产生智能行为。我们采用系统方法分析吸收器，并开发了一个数学框架来描述它们的动态。这种新的视角提供了一个原则性的方法来思考问题，并揭示了智能行为的出现的重要关键点：1. 吸收器的核心是Embedding Space漫步者，将智能行为映射到vector空间中的路径上。2. 在每一步中，吸收器将上下文融合成一个单一的复合向量，该向量在Embedding Space中的位置定义下一步的路径。3. 在解码过程中，没有实际学习发生，而是在不同上下文中的融合导致了不同的向量组合，从而实现了吸收器的智能行为。4. 吸收器的智能、智慧和技能都是Embedding Space中向量的组织方式所具有的，而不是特定的神经元或层。这些能力是这种组织的属性。5. 关注的贡献在向量组合中带来了关联偏好，影响了Embedding Space中向量的组织，但需要进一步的调查以确定其重要性。6. 整个模型由两种主要操作组成：数据独立的滤波和数据依赖的聚合。这种一致性将吸收器与其他序列模型和多种模式相连接。基于这个基础，我们正式提出了一种 semantics空间理论，即向量表示 semantic concepts，并发现了一些证据支持这一理论的有效性。
</details></li>
</ul>
<hr>
<h2 id="Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility"><a href="#Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility" class="headerlink" title="Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility"></a>Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10846">http://arxiv.org/abs/2308.10846</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oilpricebenchmarks/COB">https://github.com/oilpricebenchmarks/COB</a></li>
<li>paper_authors: Pranay Pasula</li>
<li>for: 本研究的目的是提供task-labeled时间序列数据集，用于驱动 kontinual learning在金融领域的进步。</li>
<li>methods: 本研究使用了资产价格数据的变换，生成了volatility proxy，并使用了期望最大化（EM）算法来适应模型。</li>
<li>results: 研究发现，通过包含任务标签，四种 kontinual learning算法在多个预测时间 horizon 上表现出了 Universal 的改进。<details>
<summary>Abstract</summary>
The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning. Addressing this deficit would foster innovation in this area. Therefore, we present COB, Crude Oil Benchmark datasets. COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world. Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public. We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons. We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered. We've made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at https://oilpricebenchmarks.github.io.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>金融领域内存续ous task-标注时间序列 benchmark 的缺乏，阻碍了持续学习的进步。为了解决这一问题，我们提出了 COB，即 Crude Oil Benchmark 数据集。 COB 包含了30年的资产价格，其中 exhibit 显著的分布shift，并且根据这些分布shift 生成对应的任务（即 режи）标签。我们的贡献包括将资产价格数据转换为Volatility proxy，使用期望最大化（EM）方法进行适应，生成基于实际世界事件的contextual task标签，并将这些标签以及通用的算法公开发布。我们表明，包括这些任务标签在内的 continual learning 算法在多个预测时间 horizon 上 universally 提高了四种状态之际的表现。我们希望这些 benchmark 可以加速实际数据中的分布shift处理研究，特别是由于我们考虑的资产的全球重要性。我们在 <https://oilpricebenchmarks.github.io> 上提供了（1）原始价格数据，（2）由我们方法生成的任务标签，（3）以及代码。
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection"><a href="#Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection" class="headerlink" title="Neural Networks Optimizations Against Concept and Data Drift in Malware Detection"></a>Neural Networks Optimizations Against Concept and Data Drift in Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10821">http://arxiv.org/abs/2308.10821</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Maillet, Benjamin Marais</li>
<li>for: 提高基eline neural network对概念飘移问题的处理能力</li>
<li>methods: Feature reduction和使用最新验证集训练，并提出了Drift-Resilient Binary Cross-Entropy损失函数</li>
<li>results: 对2020-2023年间 collected的新型恶意文件进行评估，提高了15.2%的恶意文件检测率 compared to baseline model<details>
<summary>Abstract</summary>
Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution. This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.
</details>
<details>
<summary>摘要</summary>
尽管机器学习模型在针对恶意软件检测方面表现出色，但它们面临着概念漂移问题，这是因为恶意软件不断演化，导致模型在时间上的性能下降。为了解决这个问题，我们提出了一种模型无关协议，用于改进基eline神经网络，以适应漂移问题。我们表明了减少特征和使用最新的验证集训练的重要性，并提出了一种名为“漂移抗性二进制十字积分”的损失函数，比 класси的二进制十字积分更有效地防止漂移。我们在EMBER数据集（2018）上训练了我们的模型，并在2020-2023年间收集的一个数据集上进行了评估。我们改进后的模型显示了出色的表现，能够检测到2018年训练集中的15.2%更多的恶意软件。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.AI_2023_08_22/" data-id="clm0t8dy4000vv788e5u7euyy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.CL_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.CL_2023_08_22/">cs.CL - 2023-08-22 19:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Empowering-Refugee-Claimants-and-their-Lawyers-Using-Machine-Learning-to-Examine-Decision-Making-in-Refugee-Law"><a href="#Empowering-Refugee-Claimants-and-their-Lawyers-Using-Machine-Learning-to-Examine-Decision-Making-in-Refugee-Law" class="headerlink" title="Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law"></a>Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11531">http://arxiv.org/abs/2308.11531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Claire Barale</li>
<li>For: This paper aims to help stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, make better decisions through data-driven intelligence and increase understanding and transparency of the refugee application process.* Methods: The paper presents a completed experiment on retrieving past cases and ongoing efforts related to analyzing legal decision-making processes on a dataset of Canadian cases, using NLP-based solutions.* Results: The paper introduces a novel benchmark for future NLP research in refugee law and expects to achieve benefits such as reduced time-to-decision, fairer and more transparent outcomes, and improved decision quality.Here are the three points in Simplified Chinese text:* For: 这个论文旨在帮助难民地位评估中的潜在利益相关者，如律师、法官、管理机构和申请人，通过数据驱动智能来做出更好的决策，并提高难民申请过程中所有参与者的理解和透明度。* Methods: 论文提出了一个完成的实验，涉及到过去案例的收集，以及对加拿大案例集进行法律决策过程的分析，使用NLP技术解决问题。* Results: 论文引入了一个新的NLP研究 benchmark，预计可以实现减少决策时间、提高决策质量、更公平和透明的决策结果等利益。<details>
<summary>Abstract</summary>
Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties. This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases. In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2). We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved. In addition, we introduce a novel benchmark for future NLP research in refugee law. Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer and more transparent outcomes, and improved decision quality.
</details>
<details>
<summary>摘要</summary>
我们的项目的目标是帮助和支持难民地位审批相关方，如律师、法官、管理机构和申请人，以使更好的决策。我们通过数据驱动智能来增加所有参与方的理解和透明度，并提高决策的质量。这个博士项目有两个主要目标：（1）检索历史案例，（2）分析加拿大案例的法律决策过程。在这篇论文中，我们介绍了我们的当前工作，包括已经完成的试验部分（1）以及正在进行的努力（2）。我们认为，NLP技术非常适合解决这些挑战，我们正在调查是否可以自动化所有步骤。此外，我们还介绍了一个新的标准测试集，用于未来NLP研究领域的难民法。我们的方法旨在包容所有终端用户和参与方，期望的利益包括减少时间决策、更公平和透明的结果，以及改善决策质量。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Prototype-Adapter-for-Vision-Language-Models"><a href="#Unsupervised-Prototype-Adapter-for-Vision-Language-Models" class="headerlink" title="Unsupervised Prototype Adapter for Vision-Language Models"></a>Unsupervised Prototype Adapter for Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11507">http://arxiv.org/abs/2308.11507</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Zhang, Ce Zhang, Xueting Hu, Zhihai He</li>
<li>for: 本研究旨在提高视觉语言模型的适应性，并且不需要大量的标注样本。</li>
<li>methods: 我们提出了一种无监督的训练方法，通过CLIP自动选择最确定的示例，并生成类prototype来初始化可学习的原型模型。</li>
<li>results: 我们的实验结果表明，我们的方法可以大幅超越8批CoOp、8批Tip-Adapter和现有的UPL方法，并且在图像识别和领域泛化任务中具有优秀的表现。<details>
<summary>Abstract</summary>
Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve as the initialization for the learnable prototype model. After fine-tuning, the prototype model prediction is combined with the original CLIP's prediction by a residual connection to perform downstream recognition tasks. Our extensive experimental results on image recognition and domain generalization show that the proposed unsupervised method outperforms 8-shot CoOp, 8-shot Tip-Adapter, and also the state-of-the-art UPL method by large margins.
</details>
<details>
<summary>摘要</summary>
现在，大规模预训练视觉语言模型（例如CLIP和ALIGN）已经表现出了很好的抽象能力。为了利用这些模型中嵌入的有价值知识来进行下游任务，有多种精度调整方法，如提示调整方法和适配器基本方法，已经开发出来。然而，这些方法需要有注解样本，这可以是劳动密集和时间消耗的。为了解决这个问题，在这项工作中，我们设计了一种无监督的精度调整方法 для视觉语言模型，即Unsupervised Prototype Adapter（UP-Adapter）。具体来说，对于无注解目标数据集，我们利用CLIP的文本图像对齐能力自动选择每个类型的最有信心的样本。使用这些选择的样本，我们生成类prototype，这些类prototype作为初始化来学习可变prototype模型。经过精度调整后，prototype模型预测结果与原CLIP预测结果之间进行差分连接，以进行下游识别任务。我们对图像识别和领域泛化进行了广泛的实验，结果表明，提posed的无监督方法可以大幅超越8批CoOp、8批Tip-Adapter以及状态监督UPL方法。
</details></li>
</ul>
<hr>
<h2 id="Can-Authorship-Representation-Learning-Capture-Stylistic-Features"><a href="#Can-Authorship-Representation-Learning-Capture-Stylistic-Features" class="headerlink" title="Can Authorship Representation Learning Capture Stylistic Features?"></a>Can Authorship Representation Learning Capture Stylistic Features?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11490">http://arxiv.org/abs/2308.11490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/luar">https://github.com/llnl/luar</a></li>
<li>paper_authors: Andrew Wang, Cristina Aggazzotti, Rebecca Kotula, Rafael Rivera Soto, Marcus Bishop, Nicholas Andrews</li>
<li>for: 这 paper 的目的是用数据驱动的方式学习作者表示，以便进行作者归属性预测。</li>
<li>methods: 这 paper 使用了大量的文本 corpus 和作者标签，通过数据驱动的方式学习作者表示。</li>
<li>results: 这 paper 的实验结果表明，学习的作者表示可以准确地捕捉作者的写作风格，并且可以鲁棒地抗压缩数据变换，如主题的变化。<details>
<summary>Abstract</summary>
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style. As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time. Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer.
</details>
<details>
<summary>摘要</summary>
自动分解作者的风格从写作内容中分离是计算语言学领域的长期问题。同时，有大量文本库已经标注作者的出现，使得可以通过数据驱动方式学习作者表示，这种任务显然更加依赖于编码写作风格而非编码内容。然而，成功完成这个代理任务并不能确保这些表示capture风格，因为作者可能也与其他隐藏变量相关，如话题。为了更好地理解这些表示中传递的信息，以及特别是验证假设是编码写作风格的，我们系统地进行了一系列targeted实验。实验结果表明，learned for surrogate authorship prediction task的表示确实敏感于写作风格。因此，作者表示可能会对某些数据变换具有Robustness，如时间的话题漂移。此外，我们的发现可能会开启下游应用需要风格表示的应用场景，如样式传递。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-generate-and-corr-uh-I-mean-repair-language-in-real-time"><a href="#Learning-to-generate-and-corr-uh-I-mean-repair-language-in-real-time" class="headerlink" title="Learning to generate and corr- uh I mean repair language in real-time"></a>Learning to generate and corr- uh I mean repair language in real-time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11683">http://arxiv.org/abs/2308.11683</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/dylandialoguesystem/dsttr">https://bitbucket.org/dylandialoguesystem/dsttr</a></li>
<li>paper_authors: Arash Eshghi, Arash Ashrafzadeh</li>
<li>for: 这个论文的目的是为了开发一种能够在实时语言处理中进行自然和控制的对话AI系统。</li>
<li>methods: 这个论文使用了之前已经学习的动态语法语法和CHILDES数据集，开发了一个基于 probabilistic model 的增量生成模型，用于实现实时语言处理。</li>
<li>results: 研究发现，使用这个模型可以在78%的情况下输出金标候选答案，ROUGE-l分数为0.86。此外，模型还可以在生成目标改变时自动生成自修复，自动评估显示，模型可以正确地生成自修复的情况为85%。小规模的人工评估也证明了生成的自修复是自然和正确的。<details>
<summary>Abstract</summary>
In conversation, speakers produce language incrementally, word by word, while continuously monitoring the appropriateness of their own contribution in the dynamically unfolding context of the conversation; and this often leads them to repair their own utterance on the fly. This real-time language processing capacity is furthermore crucial to the development of fluent and natural conversational AI. In this paper, we use a previously learned Dynamic Syntax grammar and the CHILDES corpus to develop, train and evaluate a probabilistic model for incremental generation where input to the model is a purely semantic generation goal concept in Type Theory with Records (TTR). We show that the model's output exactly matches the gold candidate in 78% of cases with a ROUGE-l score of 0.86. We further do a zero-shot evaluation of the ability of the same model to generate self-repairs when the generation goal changes mid-utterance. Automatic evaluation shows that the model can generate self-repairs correctly in 85% of cases. A small human evaluation confirms the naturalness and grammaticality of the generated self-repairs. Overall, these results further highlight the generalisation power of grammar-based models and lay the foundations for more controllable, and naturally interactive conversational AI systems.
</details>
<details>
<summary>摘要</summary>
在对话中，说话人会生成语言Word by Word，同时监测自己的言语是否适切，并在对话背景下动态地进行修复。这种实时语言处理能力是对话AI的自然化和流畅化的关键。在这篇论文中，我们使用先前学习的动态 syntax grammatical model和CHILDES corpus来开发、训练和评估一种随机生成模型，其输入是在类型理论中的某种语义生成目标概念。我们显示该模型的输出与金标准候选之间的匹配率为78%，ROUGE-l分数为0.86。我们进一步进行零shot评估模型在生成自修复时的能力。自动评估显示模型可以正确地生成自修复的85%情况下。一小规模的人工评估也证明了生成的自修复是自然和正确的。总的来说，这些结果再次强调了基于语法模型的模型的通用性，并为更可控、自然交互的对话AI系统开创了基础。
</details></li>
</ul>
<hr>
<h2 id="SONAR-Sentence-Level-Multimodal-and-Language-Agnostic-Representations"><a href="#SONAR-Sentence-Level-Multimodal-and-Language-Agnostic-Representations" class="headerlink" title="SONAR: Sentence-Level Multimodal and Language-Agnostic Representations"></a>SONAR: Sentence-Level Multimodal and Language-Agnostic Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11466">http://arxiv.org/abs/2308.11466</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/sonar">https://github.com/facebookresearch/sonar</a></li>
<li>paper_authors: Paul-Ambroise Duquenne, Holger Schwenk, Benoît Sagot</li>
<li>for: 本文提出了一个新的多语言多模式固定大小句子嵌入空间SONAR，用于实现多语言多模式嵌入。</li>
<li>methods: 作者使用了一种新的 sentence encoder 和 speech encoder，通过 teacher-student  Setting来训练语言特定的语音编码器，并使用了一种文本解码器来实现文本到文本和语音到文本翻译。</li>
<li>results: 作者的方法在多语言多模式嵌入搜索任务上显著超越了现有的嵌入方法，如LASER3和LabSE。此外，作者的语音编码器在相似搜索任务上也表现出色，并且可以实现零shot语言和模式组合的语音翻译。<details>
<summary>Abstract</summary>
We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.
</details>
<details>
<summary>摘要</summary>
我们介绍SONAR，一个新的多语言多模式固定大小句子嵌入空间。我们的单一文本编码器，覆盖200种语言，与现有的句子嵌入 such as LASER3和LabSE在xsim和xsim++多 lingual similarity搜寻任务上表现出色，并可以将语音段落嵌入同一个 SONAR嵌入空间中使用语言特定的语音编码器在教师-学生设定下在语音识别数据上训练。我们的编码器在类似搜寻任务上表现出色，而我们还提供了200种语言的文本解oder，可以进行文本-文本和语音-文本机器翻译，包括零��� conocido语言和模式组合。我们的文本-文本结果与现有的NLLB1B模型相匹配，即使受到固定大小瓶颈表现的限制。我们的零��� known语音-文本翻译结果与强化过的基准模型such as Whisper相匹配。
</details></li>
</ul>
<hr>
<h2 id="Extracting-Relational-Triples-Based-on-Graph-Recursive-Neural-Network-via-Dynamic-Feedback-Forest-Algorithm"><a href="#Extracting-Relational-Triples-Based-on-Graph-Recursive-Neural-Network-via-Dynamic-Feedback-Forest-Algorithm" class="headerlink" title="Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm"></a>Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11411">http://arxiv.org/abs/2308.11411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyin Zhu</li>
<li>for: 将文本数据转化为结构化知识</li>
<li>methods: 使用依赖树分析和图 recursive neural networks (GRNNs) 实现 triple extraction 任务</li>
<li>results: 提出了一种新的方法，可以在模型训练时通过推理操作连接各个子任务的表示，实现子任务的 интеграción<details>
<summary>Abstract</summary>
Extracting relational triples (subject, predicate, object) from text enables the transformation of unstructured text data into structured knowledge. The named entity recognition (NER) and the relation extraction (RE) are two foundational subtasks in this knowledge generation pipeline. The integration of subtasks poses a considerable challenge due to their disparate nature. This paper presents a novel approach that converts the triple extraction task into a graph labeling problem, capitalizing on the structural information of dependency parsing and graph recursive neural networks (GRNNs). To integrate subtasks, this paper proposes a dynamic feedback forest algorithm that connects the representations of subtasks by inference operations during model training. Experimental results demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
将文本数据转化为结构化知识，EXTRACTING relational triples（主语、谓语、谓 Object）从文本中提取是一个基本任务。命名实体识别（NER）和关系提取（RE）是这个知识生成管道的两个基础任务。这两个任务的集成带来了很大挑战，因为它们之间存在很大的差异。本文提出了一种新的方法，将 triple 提取任务转化为图标注问题，利用语言结构信息和图循环神经网络（GRNN）。为了将子任务集成，本文提出了一种动态反馈森林算法，在模型训练过程中，通过推理操作连接子任务的表示。实验结果表明，提出的方法有效。
</details></li>
</ul>
<hr>
<h2 id="Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition"><a href="#Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition" class="headerlink" title="Convoifilter: A case study of doing cocktail party speech recognition"></a>Convoifilter: A case study of doing cocktail party speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11380">http://arxiv.org/abs/2308.11380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thai-Binh Nguyen, Alexander Waibel</li>
<li>for: 提高特定说话人在嘈杂环境中自动语音识别（ASR）的精度。</li>
<li>methods: 使用单通道语音净化模块减少背景噪声，并与ASR模块结合使用。通过这种方法，模型可以将单个说话人的语音净化到26.4%。</li>
<li>results: 模型可以将单个说话人的语音净化到14.5%，比单独调整的26.4%更低。<details>
<summary>Abstract</summary>
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
</details>
<details>
<summary>摘要</summary>
这份研究报告介绍了一种用于改进特定发音人员在嘈杂环境下的自动语音识别（ASR）模型。该模型使用单通道语音提升模块，以隔离发音人员的声音与背景噪声，同时还包括ASR模块。通过这种方法，模型可以降低ASR的单词错误率（WER）从80%降至26.4%。通常，这两个组件在数据需求的变化下独立地调整。然而，语音提升可能会导致ASR效率下降。通过实施联合细调策略，模型可以在联合细调中降低WER从26.4%下降至14.5%。
</details></li>
</ul>
<hr>
<h2 id="M3PS-End-to-End-Multi-Grained-Multi-Modal-Attribute-Aware-Product-Summarization-in-E-commerce"><a href="#M3PS-End-to-End-Multi-Grained-Multi-Modal-Attribute-Aware-Product-Summarization-in-E-commerce" class="headerlink" title="M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce"></a>M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11351">http://arxiv.org/abs/2308.11351</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Chen, Ze Lin, Hui Li, Jiayi Ji, Yiyi Zhou, Guanbin Li, Rongrong Ji</li>
<li>for: 这篇论文的目的是提出一种高质量产品概要生成方法，以吸引顾客兴趣，提高购买意愿。</li>
<li>methods: 该方法使用多Modal模型，同时考虑了多个细腻的特征，包括文本和图像模式，以生成高质量的产品概要。</li>
<li>results: 实验结果表明，该方法在一个大规模的中文电商 dataset 上的评价metric 上显著 OUTPERFORMS 现有的产品概要方法。<details>
<summary>Abstract</summary>
Given the long textual product information and the product image, Multi-Modal Product Summarization (MMPS) aims to attract customers' interest and increase their desire to purchase by highlighting product characteristics with a short textual summary. Existing MMPS methods have achieved promising performance. Nevertheless, there still exist several problems: 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To address these issues, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (M3PS) for generating high-quality product summaries in e-commerce. M3PS jointly models product attributes and generates product summaries. Meanwhile, we design several multi-grained multi-modal tasks to better guide the multi-modal learning of M3PS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manifested in the generated summaries. Extensive experiments on a real large-scale Chinese e-commence dataset demonstrate that our model outperforms state-of-the-art product summarization methods w.r.t. several summarization metrics.
</details>
<details>
<summary>摘要</summary>
文本级别的产品描述和产品图像，多模式产品概述（MMPS）目标是通过突出产品特点而吸引顾客兴趣，提高购买意愿。现有的MMPS方法已经实现了一定的成果。然而，还存在一些问题：1）缺乏端到端产品概述，2）缺乏多层多模式模型，3）缺乏多模式属性模型。为了解决这些问题，我们提出了一种端到端多层多模式属性感知产品概述方法（M3PS），用于生成高质量的电商产品概述。M3PS同时模型产品属性，并生成产品概述。此外，我们设计了多个多层多模式任务，以更好地引导多模式学习。同时，我们基于文本和图像模式来模型产品属性，以便在生成的概述中表达多模式产品特点。我们对大规模中国电商数据进行了广泛的实验，并证明了我们的模型在多个概述指标上表现比现状态的产品概述方法更高。
</details></li>
</ul>
<hr>
<h2 id="LEAP-Efficient-and-Automated-Test-Method-for-NLP-Software"><a href="#LEAP-Efficient-and-Automated-Test-Method-for-NLP-Software" class="headerlink" title="LEAP: Efficient and Automated Test Method for NLP Software"></a>LEAP: Efficient and Automated Test Method for NLP Software</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11284">http://arxiv.org/abs/2308.11284</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lumos-xiao/leap">https://github.com/lumos-xiao/leap</a></li>
<li>paper_authors: Mingxuan Xiao, Yan Xiao, Hai Dong, Shunhui Ji, Pengcheng Zhang</li>
<li>for: 提高 DNN 模型的Robustness，透过自动生成 adversarial test cases。</li>
<li>methods: 使用 Levy flight-based Adaptive Particle swarm optimization integrated with textual features，并采用 initialization population 增加测试用例的多样性，以及使用启动器算法和精准搜索缩短搜索时间。</li>
<li>results: 对 NLP 软件进行了系列测试，并证明了 LEAP 能够生成高精度的 adversarial test cases，同时具有较高的效率和可迁移性。<details>
<summary>Abstract</summary>
The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios. To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the search time. We conducted a series of experiments to validate LEAP's ability to test NLP software and found that the average success rate of LEAP in generating adversarial test cases is 79.1%, which is 6.1% higher than the next best approach (PSOattack). While ensuring high success rates, LEAP significantly reduces time overhead by up to 147.6s compared to other heuristic-based methods. Additionally, the experimental results demonstrate that LEAP can generate more transferable test cases and significantly enhance the robustness of DNN-based systems.
</details>
<details>
<summary>摘要</summary>
“随着深度神经网络（DNN）在自然语言处理（NLP）软件中的广泛应用，问题的Robustness问题得到了吸引注意。研究人员提出了多种自动测试技术，但现有方法受到两个限制：一是弱的错误发现能力，成功率从0%到24.6%之间，二是时间浪费，每个测试案例需要177.8s至205.28s，这使得它们在时间紧张的情况下具有挑战性。为了解决这些问题，本文提出了LEAP，一个自动测试方法，利用LEvy flight-based Adaptive Particle swarm optimization与文本特征来生成攻击测试案例。具体来说，我们在人口初始化中采用Levy flight，以增加生成的测试案例的多样性。我们还设计了一个吸引力适应更新算法，以提高LEAP的全球优化高维文本示例的效率。此外，我们还设计了基于推导策略的突变算法，以减少搜索时间。我们对NLP软件进行了一系列实验， Validate LEAP的测试能力，结果显示，LEAP的平均成功率为79.1%，高于下一个最佳方法（PSOattack）的6.1%。同时，LEAP可以保证高的成功率，并对其他着重基于规律的方法实现时间优化，最多减少147.6s。实验结果显示，LEAP可以生成更转移的测试案例，并对DNN基于系统增加了更高的Robustness。”
</details></li>
</ul>
<hr>
<h2 id="HopPG-Self-Iterative-Program-Generation-for-Multi-Hop-Question-Answering-over-Heterogeneous-Knowledge"><a href="#HopPG-Self-Iterative-Program-Generation-for-Multi-Hop-Question-Answering-over-Heterogeneous-Knowledge" class="headerlink" title="HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge"></a>HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11257">http://arxiv.org/abs/2308.11257</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingyao Wang, Yongwei Zhou, Chaoqun Duan, Junwei Bao, Tiejun Zhao</li>
<li>for: 本研究旨在提高semantic parsing方法的多步问答能力，特别是在非结构化知识库中进行多步问答。</li>
<li>methods: 本研究提出了一种自适应框架（HopPG），通过利用前一步执行结果来检索支持信息和生成后续程序，解决了传统semantic parsing方法在多步问答中的缺陷。</li>
<li>results: 实验结果表明，HopPG在MMQA-T^2上表现出色，特别是在多步问答中超过了现有semantic-parsing基eline。<details>
<summary>Abstract</summary>
The semantic parsing-based method is an important research branch for knowledge-based question answering. It usually generates executable programs lean upon the question and then conduct them to reason answers over a knowledge base. Benefit from this inherent mechanism, it has advantages in the performance and the interpretability. However,traditional semantic parsing methods usually generate a complete program before executing it, which struggles with multi-hop question answering over heterogeneous knowledge. Firstly,a complete multi-hop program relies on multiple heterogeneous supporting facts, and it is difficult for models to receive these facts simultaneously. Secondly,these methods ignore the interaction information between the previous-hop execution result and the current-hop program generation. To alleviate these challenges, we propose a self-iterative framework for multi-hop program generation (HopPG) over heterogeneous knowledge, which leverages the previous-hop execution results to retrieve supporting facts and generate subsequent programs iteratively. We evaluate our model on MMQA-T^2. The experimental results show that HopPG outperforms existing semantic-parsing-based baselines, especially on the multi-hop questions.
</details>
<details>
<summary>摘要</summary>
“ semantic parsing-based 方法是知识基于问题回答的重要研究分支。它通常将问题转换为可执行的程式，然后将其与知识库进行推理，获得答案。由于这个自然的机制，它具有性能和可读性的优势。然而，传统的 semantic parsing 方法通常会生成完整的程式 перед执行，这会对于多步骤问题回答 sobre 不同的知识类型产生困难。首先，完整的多步骤程式需要多个不同的支持事实，而这些模型很难同时获取这些事实。其次，这些方法忽略了前一步执行结果和现在一步程式生成之间的互动信息。为了解决这些挑战，我们提出了一个自我迭代框架 для 多步骤程式生成 (HopPG) over 不同的知识，它利用前一步执行结果来获取支持事实并生成下一步程式。我们将我们的模型评估在 MMQA-T^2 上。实验结果显示，HopPG 比 existed semantic-parsing-based 基eline更高效，特别是在多步骤问题上。”
</details></li>
</ul>
<hr>
<h2 id="ViCo-Engaging-Video-Comment-Generation-with-Human-Preference-Rewards"><a href="#ViCo-Engaging-Video-Comment-Generation-with-Human-Preference-Rewards" class="headerlink" title="ViCo: Engaging Video Comment Generation with Human Preference Rewards"></a>ViCo: Engaging Video Comment Generation with Human Preference Rewards</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11171">http://arxiv.org/abs/2308.11171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchong Sun, Bei Liu, Xu Chen, Ruihua Song, Jianlong Fu</li>
<li>for: 本研究旨在生成视频评论，以增强视频社交媒体上的互动性和参与度。</li>
<li>methods: 本研究提出了三种新的设计方案，包括利用赞数作为评论的表现度量，自动评估评论的参与度，以及使用初始生成器生成评论，然后通过奖励模型进行反馈优化。</li>
<li>results: 实验结果表明，使用本研究提出的方法可以生成高质量的视频评论，特别是在考虑参与度时。<details>
<summary>Abstract</summary>
Engaging video comments play an important role in video social media, as they are the carrier of feelings, thoughts, or humor of the audience. Preliminary works have made initial exploration for video comment generation by adopting caption-style encoder-decoder models. However, comment generation presents some unique challenges distinct from caption generation, which makes these methods somewhat less effective at generating engaging comments. In contrast to the objective and descriptive nature of captions, comments tend to be inherently subjective, making it hard to quantify and evaluate the engagement of comments. Furthermore, the scarcity of truly engaging comments brings difficulty to collecting enough high-quality training examples. In this paper, we propose ViCo with three novel designs to tackle the above challenges for generating engaging Video Comments. Firstly, to quantify the engagement of comments, we utilize the number of "likes" each comment receives as a proxy of human preference after an appropriate debiasing procedure. Secondly, to automatically evaluate the engagement of comments, we train a reward model to align its judgment to the above proxy. Our user studies indicate that this reward model effectively aligns with human judgments. Lastly, to alleviate the scarcity of high-quality comments, an initial generator is trained on readily available but noisy data to generate comments. Then the reward model is employed to offer feedback on the generated comments, thus optimizing the initial generator. To facilitate the research of video commenting, we collect a large video comment-dataset (ViCo-20k) with rich metadata from a popular video website. Experiments on ViCo-20k show that the comments generated by our ViCo model exhibit the best performance in terms of both quantitative and qualitative results, particularly when engagement is considered.
</details>
<details>
<summary>摘要</summary>
优化视频评论的核心在于促进视频社交媒体上的评论内容的互动性和趣味性。现有的初步工作已经采用了caption风格的编解oder模型进行视频评论生成。然而，评论生成存在一些独特的挑战，与caption生成不同，这些挑战使得这些方法在生成互动评论时有所不足。在评论中，评论内容具有主观性，使得评估评论的互动性变得更加困难。此外，缺乏真正有趣的评论使得收集高质量的训练示例具有挑战性。在这篇论文中，我们提出了ViCo模型，其中包括三个新的设计来解决以上挑战。首先，我们利用每个评论 receives的“喜欢”数作为人类偏好的代理，并进行了适当的偏移处理。其次，我们训练了一个奖励模型，以使其对于上述代理的评价与人类评价相互对应。我们的用户研究表明，这个奖励模型与人类评价之间具有良好的一致性。最后，我们使用初始生成器在 readily available但含有噪声的数据上生成评论，然后使用奖励模型来反馈给初始生成器，以便优化初始生成器。为促进视频评论研究，我们收集了一个大量的视频评论数据集（ViCo-20k），其中包括了视频网站上具有丰富 metadata 的视频评论。我们在ViCo-20k数据集上进行了实验，结果显示，我们的ViCo模型在互动性和质量两个方面表现出色，特别是在考虑互动性时。
</details></li>
</ul>
<hr>
<h2 id="LLaMA-Reviewer-Advancing-Code-Review-Automation-with-Large-Language-Models-through-Parameter-Efficient-Fine-Tuning-Practical-Experience-Report"><a href="#LLaMA-Reviewer-Advancing-Code-Review-Automation-with-Large-Language-Models-through-Parameter-Efficient-Fine-Tuning-Practical-Experience-Report" class="headerlink" title="LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report)"></a>LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11148">http://arxiv.org/abs/2308.11148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo</li>
<li>for:  automatizing code review activities</li>
<li>methods:  utilizes parameter-efficient fine-tuning (PEFT) methods and LLaMA, a popular large language model</li>
<li>results:  equals the performance of existing code-review-focused models with a small model size and limited tuning epochs<details>
<summary>Abstract</summary>
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.   In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.   An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.   The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>软件工程中的代码审查活动自动化，是一项长期追求的问题，已经由许多域 especific pre-trained models  addresses。Despite their success, these models often require extensive resources for pre-training from scratch。In contrast, Large Language Models (LLMs) provide an interesting alternative，given their remarkable capabilities when supplemented with domain-specific knowledge。However，their potential for automating code review tasks remains largely unexplored。In response to this research gap，we present LLaMA-Reviewer，an innovative framework that leverages the capabilities of LLaMA，a popular LLM，in the realm of code review。Mindful of resource constraints，this framework employs parameter-efficient fine-tuning (PEFT) methods，delivering high performance while using less than 1% of trainable parameters。An extensive evaluation of LLaMA-Reviewer is conducted on two diverse，publicly available datasets。Notably，even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs，LLaMA-Reviewer equals the performance of existing code-review-focused models。The ablation experiments provide insights into the influence of various fine-tuning process components，including input representation，instruction tuning，and different PEFT methods。To foster continuous progress in this field，the code and all PEFT-weight plugins have been made open-source。
</details></li>
</ul>
<hr>
<h2 id="NLP-based-detection-of-systematic-anomalies-among-the-narratives-of-consumer-complaints"><a href="#NLP-based-detection-of-systematic-anomalies-among-the-narratives-of-consumer-complaints" class="headerlink" title="NLP-based detection of systematic anomalies among the narratives of consumer complaints"></a>NLP-based detection of systematic anomalies among the narratives of consumer complaints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11138">http://arxiv.org/abs/2308.11138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ričardas Zitikis</li>
<li>for: 该研究是为了检测consumer complaint narratives中的系统性异常（systematic anomalies）。</li>
<li>methods: 该研究使用NLP技术将consumer complaint narratives转化为量化数据，然后使用一种算法检测系统性异常。</li>
<li>results: 研究使用Consumer Financial Protection Bureau的consumer complaint database示例，并成功地检测到了一些系统性异常。<details>
<summary>Abstract</summary>
We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</details>
<details>
<summary>摘要</summary>
我们开发了一种基于自然语言处理（NLP）技术的系统性异常检测程序，用于检测消费者投诉文本中的系统性异常。尽管分类算法可以检测明显的异常，但在小型和频繁的系统性异常情况下，算法可能会失败，这可能是技术上的限制以及人类分析员的自然限制。因此，我们将投诉文本转换成量化数据，然后使用一种检测系统性异常的算法进行分析。我们使用美国消费者金融保护署的消费者投诉数据库中的投诉文本进行示例。
</details></li>
</ul>
<hr>
<h2 id="Towards-Objective-Evaluation-of-Socially-Situated-Conversational-Robots-Assessing-Human-Likeness-through-Multimodal-User-Behaviors"><a href="#Towards-Objective-Evaluation-of-Socially-Situated-Conversational-Robots-Assessing-Human-Likeness-through-Multimodal-User-Behaviors" class="headerlink" title="Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors"></a>Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11020">http://arxiv.org/abs/2308.11020</a></li>
<li>repo_url: None</li>
<li>paper_authors: Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze</li>
<li>for: 评估社交对话机器人的人类化程度</li>
<li>methods: 基于多Modal用户行为的对话对话集采集，并对用户行为与人类化分数之间进行相似性分析，以评估机器人的人类化程度</li>
<li>results: 研究发现，通过对用户行为进行分析，可以对机器人的人类化程度进行评估，并且这种方法可以增强 объекivity和可重复性。<details>
<summary>Abstract</summary>
This paper tackles the challenging task of evaluating socially situated conversational robots and presents a novel objective evaluation approach that relies on multimodal user behaviors. In this study, our main focus is on assessing the human-likeness of the robot as the primary evaluation metric. While previous research often relied on subjective evaluations from users, our approach aims to evaluate the robot's human-likeness based on observable user behaviors indirectly, thus enhancing objectivity and reproducibility. To begin, we created an annotated dataset of human-likeness scores, utilizing user behaviors found in an attentive listening dialogue corpus. We then conducted an analysis to determine the correlation between multimodal user behaviors and human-likeness scores, demonstrating the feasibility of our proposed behavior-based evaluation method.
</details>
<details>
<summary>摘要</summary>
To begin, we created an annotated dataset of human-likeness scores, utilizing user behaviors found in an attentive listening dialogue corpus. We then conducted an analysis to determine the correlation between multimodal user behaviors and human-likeness scores, demonstrating the feasibility of our proposed behavior-based evaluation method.Translation notes:* "socially situated" is translated as "社交境中" (shè jìoù zhōng zhī)* "conversational robots" is translated as "对话机器人" (duì yǔ jī rén)* "human-likeness" is translated as "人类化" (rén xìng huà)* "objective evaluation" is translated as "客观评价" (kè jiàn píng jì)* "multimodal user behaviors" is translated as "多Modal用户行为" (duō modāl yòng hòu xíng bèi)* "attentive listening dialogue corpus" is translated as "注意听录对话 corpus" (zhù yì tīng luō duì hǎo)
</details></li>
</ul>
<hr>
<h2 id="Using-language-models-in-the-implicit-automated-assessment-of-mathematical-short-answer-items"><a href="#Using-language-models-in-the-implicit-automated-assessment-of-mathematical-short-answer-items" class="headerlink" title="Using language models in the implicit automated assessment of mathematical short answer items"></a>Using language models in the implicit automated assessment of mathematical short answer items</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11006">http://arxiv.org/abs/2308.11006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Ormerod</li>
<li>for: 这项研究的目的是提出一种新的短 constructed response assessment方法，以便更准确地评估学生的数学知识。</li>
<li>methods: 该方法使用一个管道，从Student response中提取关键值。这个管道包括两个精心调整的语言模型，第一个模型判断学生回答中是否含有关键值，第二个模型则确定回答中关键值的位置。</li>
<li>results: 研究表明，这种管道方法比传统的分桌评分法更加准确和有用，可以为学生提供更有arget的反馈，帮助学生提高数学知识。<details>
<summary>Abstract</summary>
We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which can help them improve their understanding of mathematics.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来评估某些短 constructed responses 的数学项目。我们的方法使用一个管道，以确定学生在回答中提供的关键值。这些值可以确定回答的正确性，以及学生可能存在的误解。管道中的信息可以用于向教师和学生提供反馈。我们的值标识管道包括两个精心调整的自然语言模型。第一个模型判断学生回答中是否包含关键值。第二个模型确定回答中关键值的位置。我们考虑了一个通用的模型，可以用于任何提问和值，以及每个提问和值的特定模型。值标识管道比传统的分类型分配法更准确和有用，可以为学生提供更有向导性的反馈，帮助他们深化数学理解。
</details></li>
</ul>
<hr>
<h2 id="LatEval-An-Interactive-LLMs-Evaluation-Benchmark-with-Incomplete-Information-from-Lateral-Thinking-Puzzles"><a href="#LatEval-An-Interactive-LLMs-Evaluation-Benchmark-with-Incomplete-Information-from-Lateral-Thinking-Puzzles" class="headerlink" title="LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles"></a>LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10855">http://arxiv.org/abs/2308.10855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thukelab/lateval">https://github.com/thukelab/lateval</a></li>
<li>paper_authors: Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Hai-Tao Zheng</li>
<li>for: 评估大语言模型的横向思维能力</li>
<li>methods: 使用 Lateral Thinking Puzzles  benchmark 评估模型的问题提出和信息 интеграción能力</li>
<li>results: 发现大多数语言模型在交互中具有差guide的横向思维能力，比如 GPT-4 也存在一定的差异，与人类相比仍有很大差距<details>
<summary>Abstract</summary>
With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.
</details>
<details>
<summary>摘要</summary>
We challenge LLMs with two aspects: the quality of questions posed by the model and the model's ability to integrate information for problem-solving. Our results show that nearly all LLMs struggle with using lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits some advantage, but still maintains a noticeable gap compared to humans. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial for an effective AI assistant.
</details></li>
</ul>
<hr>
<h2 id="AgentVerse-Facilitating-Multi-Agent-Collaboration-and-Exploring-Emergent-Behaviors-in-Agents"><a href="#AgentVerse-Facilitating-Multi-Agent-Collaboration-and-Exploring-Emergent-Behaviors-in-Agents" class="headerlink" title="AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents"></a>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10848">http://arxiv.org/abs/2308.10848</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/openbmb/agentverse">https://github.com/openbmb/agentverse</a></li>
<li>paper_authors: Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou</li>
<li>for: 这篇论文旨在提出一种基于大语言模型的多代理框架，以增强任务完成效率和效果。</li>
<li>methods: 该框架使用了多种方法，包括 dynamically adjusting its composition 和 collaboratively accomplishing tasks。</li>
<li>results: 实验结果表明，该框架可以有效地派Send multi-agent groups that outperform a single agent。 Additionally, the paper explores the emergence of social behaviors among individual agents within a group during collaborative task accomplishment.<details>
<summary>Abstract</summary>
Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be released at \url{https://github.com/OpenBMB/AgentVerse}.
</details>
<details>
<summary>摘要</summary>
自主代理 empowered by Large Language Models (LLMs) 已经经历了重要的改进，使其能够广泛应用于多种任务。然而，在实际场景中，人们之间的合作是经常需要的，以提高任务完成的效率和效果。因此， drawing inspiration from human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be released at \url{https://github.com/OpenBMB/AgentVerse}.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard versions of Chinese. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.CL_2023_08_22/" data-id="clm0t8dyl002ev7888wrp78pr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.CV_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.CV_2023_08_22/">cs.CV - 2023-08-22 21:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SwinFace-A-Multi-task-Transformer-for-Face-Recognition-Expression-Recognition-Age-Estimation-and-Attribute-Estimation"><a href="#SwinFace-A-Multi-task-Transformer-for-Face-Recognition-Expression-Recognition-Age-Estimation-and-Attribute-Estimation" class="headerlink" title="SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation"></a>SwinFace: A Multi-task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11509">http://arxiv.org/abs/2308.11509</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lxq1000/swinface">https://github.com/lxq1000/swinface</a></li>
<li>paper_authors: Lixiong Qin, Mei Wang, Chao Deng, Ke Wang, Xi Chen, Jiani Hu, Weihong Deng</li>
<li>for: 这个论文的目的是提出一种多功能的Face recognition和表情识别、年龄估计和面部特征估计（40个特征包括性别）的算法基于单个Swin Transformer。</li>
<li>methods: 该算法使用了一个共享背景和每个相关任务的子网络，并在每个任务特定分析子网络中实现了一种多级渠道注意力（MLCA）模块，以适应不同任务的冲突和需求。</li>
<li>results: 对于所有任务，提出的模型具有出色的表现，尤其是在RAF-DB和CLAP2015上达到了90.97%的准确率和0.22 $\epsilon$-的误差分别，这些结果在表情识别和年龄估计领域是状态的最佳结果。<details>
<summary>Abstract</summary>
In recent years, vision transformers have been introduced into face recognition and analysis and have achieved performance breakthroughs. However, most previous methods generally train a single model or an ensemble of models to perform the desired task, which ignores the synergy among different tasks and fails to achieve improved prediction accuracy, increased data efficiency, and reduced training time. This paper presents a multi-purpose algorithm for simultaneous face recognition, facial expression recognition, age estimation, and face attribute estimation (40 attributes including gender) based on a single Swin Transformer. Our design, the SwinFace, consists of a single shared backbone together with a subnet for each set of related tasks. To address the conflicts among multiple tasks and meet the different demands of tasks, a Multi-Level Channel Attention (MLCA) module is integrated into each task-specific analysis subnet, which can adaptively select the features from optimal levels and channels to perform the desired tasks. Extensive experiments show that the proposed model has a better understanding of the face and achieves excellent performance for all tasks. Especially, it achieves 90.97% accuracy on RAF-DB and 0.22 $\epsilon$-error on CLAP2015, which are state-of-the-art results on facial expression recognition and age estimation respectively. The code and models will be made publicly available at https://github.com/lxq1000/SwinFace.
</details>
<details>
<summary>摘要</summary>
Recently, vision transformers have been applied to face recognition and analysis, achieving performance breakthroughs. However, most previous methods train a single model or an ensemble of models to perform the desired task, ignoring the synergy among different tasks and failing to achieve improved prediction accuracy, increased data efficiency, and reduced training time. This paper proposes a multi-purpose algorithm for simultaneous face recognition, facial expression recognition, age estimation, and face attribute estimation (40 attributes including gender) based on a single Swin Transformer. Our design, called SwinFace, consists of a single shared backbone and a subnet for each set of related tasks. To address the conflicts among multiple tasks and meet the different demands of tasks, a Multi-Level Channel Attention (MLCA) module is integrated into each task-specific analysis subnet, which can adaptively select the features from optimal levels and channels to perform the desired tasks. Extensive experiments show that the proposed model has a better understanding of the face and achieves excellent performance for all tasks. Especially, it achieves 90.97% accuracy on RAF-DB and 0.22 $\epsilon$-error on CLAP2015, which are state-of-the-art results on facial expression recognition and age estimation respectively. The code and models will be made publicly available at https://github.com/lxq1000/SwinFace.
</details></li>
</ul>
<hr>
<h2 id="LCCo-Lending-CLIP-to-Co-Segmentation"><a href="#LCCo-Lending-CLIP-to-Co-Segmentation" class="headerlink" title="LCCo: Lending CLIP to Co-Segmentation"></a>LCCo: Lending CLIP to Co-Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11506">http://arxiv.org/abs/2308.11506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Duan, Yan Yang, Liyuan Pan, Xiabi Liu</li>
<li>for: 本文研究了一种基于语言图像预训练框架(CLIP)的图像集合中的共同Semantic object划分方法。</li>
<li>methods: 该方法使用了一种基于CLIP的三个关键模块：一个图像集合特征匹配模块，一个CLIP交互模块，和一个CLIP规范模块。这些模块共同使用CLIP来提高图像划分精度。</li>
<li>results: 实验结果表明，该方法在四个标准图像划分 benchmark 数据集上的性能比前state-of-the-art方法高。<details>
<summary>Abstract</summary>
This paper studies co-segmenting the common semantic object in a set of images. Existing works either rely on carefully engineered networks to mine the implicit semantic information in visual features or require extra data (i.e., classification labels) for training. In this paper, we leverage the contrastive language-image pre-training framework (CLIP) for the task. With a backbone segmentation network that independently processes each image from the set, we introduce semantics from CLIP into the backbone features, refining them in a coarse-to-fine manner with three key modules: i) an image set feature correspondence module, encoding global consistent semantic information of the image set; ii) a CLIP interaction module, using CLIP-mined common semantics of the image set to refine the backbone feature; iii) a CLIP regularization module, drawing CLIP towards this co-segmentation task, identifying the best CLIP semantic and using it to regularize the backbone feature. Experiments on four standard co-segmentation benchmark datasets show that the performance of our method outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Image set feature correspondence module: Encodes global consistent semantic information of the image set.2. CLIP interaction module: Uses CLIP-mined common semantics of the image set to refine the backbone feature.3. CLIP regularization module: Draws CLIP towards this co-segmentation task, identifying the best CLIP semantic and using it to regularize the backbone feature.Experiments on four standard co-segmentation benchmark datasets show that our method outperforms state-of-the-art methods.</details></li>
</ol>
<hr>
<h2 id="Learning-from-Semantic-Alignment-between-Unpaired-Multiviews-for-Egocentric-Video-Recognition"><a href="#Learning-from-Semantic-Alignment-between-Unpaired-Multiviews-for-Egocentric-Video-Recognition" class="headerlink" title="Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition"></a>Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11489">http://arxiv.org/abs/2308.11489</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wqtwjt1996/sum-l">https://github.com/wqtwjt1996/sum-l</a></li>
<li>paper_authors: Qitong Wang, Long Zhao, Liangzhe Yuan, Ting Liu, Xi Peng</li>
<li>For: The paper aims to tackle the challenging problem of unpaired multiview video learning, where the model needs to learn comprehensive multiview representations while dealing with variations in cross-view semantic information.* Methods: The proposed method, called Semantics-based Unpaired Multiview Learning (SUM-L), builds cross-view pseudo-pairs and performs view-invariant alignment by leveraging the semantic information of videos. Additionally, video-text alignment is performed for first-person and third-person videos to improve video representations.* Results: The method is evaluated on multiple benchmark datasets and outperforms multiple existing view-alignment methods, demonstrating its effectiveness in improving video representations under a more challenging scenario than typical paired or unpaired multimodal or multiview learning.Here are the three key points in Simplified Chinese text:* For: 本文目的是解决无对视照视频学习的挑战问题，即学习多视图表示而处理视频cross-view semantic信息的变化。* Methods: 提出的方法是基于Semantics-based Unpaired Multiview Learning（SUM-L），建立cross-view Pseudo-pairs并实现视图不变的Alignment，通过利用视频semantic信息。此外，还进行了首人和第三人视频的文本对齐，以提高视频表示。* Results: 方法在多个benchmark dataset上进行了广泛的实验，并证明其在不同于 Typical paired或Unpaired multimodal或Multiview learning的场景下表现更高效。<details>
<summary>Abstract</summary>
We are concerned with a challenging scenario in unpaired multiview video learning. In this case, the model aims to learn comprehensive multiview representations while the cross-view semantic information exhibits variations. We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle this unpaired multiview learning problem. The key idea is to build cross-view pseudo-pairs and do view-invariant alignment by leveraging the semantic information of videos. To facilitate the data efficiency of multiview learning, we further perform video-text alignment for first-person and third-person videos, to fully leverage the semantic knowledge to improve video representations. Extensive experiments on multiple benchmark datasets verify the effectiveness of our framework. Our method also outperforms multiple existing view-alignment methods, under the more challenging scenario than typical paired or unpaired multimodal or multiview learning. Our code is available at https://github.com/wqtwjt1996/SUM-L.
</details>
<details>
<summary>摘要</summary>
我们面临了一种复杂的无对视视频学习场景。在这种情况下，模型需要学习全面的无对视视频表示，而cross-view含义信息具有变化性。我们提议使用Semantics-based Unpaired Multiview Learning（SUM-L）解决这个无对视视频学习问题。关键思想是建立cross-view Pseudo-对和视图不变Alignment，通过利用视频含义信息来做这两个任务。为了提高多视图学习的数据效率，我们进一步进行了视频文本对齐，以全面利用视频含义知识来改善视频表示。我们的方法在多个benchmark数据集上进行了广泛的实验，并证明了我们的框架的效果。我们的方法还比多种现有的视图对齐方法高效，在更加复杂的场景下。我们的代码可以在https://github.com/wqtwjt1996/SUM-L中找到。
</details></li>
</ul>
<hr>
<h2 id="Opening-the-Vocabulary-of-Egocentric-Actions"><a href="#Opening-the-Vocabulary-of-Egocentric-Actions" class="headerlink" title="Opening the Vocabulary of Egocentric Actions"></a>Opening the Vocabulary of Egocentric Actions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11488">http://arxiv.org/abs/2308.11488</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela Yao</li>
<li>for: 本文旨在提出一种开放词汇动作识别任务，能够扩展 Verb 到一个开放词汇的动作中，同时处理已知和未知的对象。</li>
<li>methods: 本文提出了一种嵌入 CLIP 表示的提示来预测开放词汇中的交互对象，并使用一个对象agnostic verb encoder 来预测verb。</li>
<li>results: 对于 EPIC-KITCHENS-100 和 Assembly101  datasets，本文创建了一些开放词汇的benchmark，而关闭动作方法无法泛化，而我们的提议方法却效果很好，同时，我们的对象Encoder 也比既有的开放词汇视觉识别方法更高效。<details>
<summary>Abstract</summary>
Human actions in egocentric videos are often hand-object interactions composed from a verb (performed by the hand) applied to an object. Despite their extensive scaling up, egocentric datasets still face two limitations - sparsity of action compositions and a closed set of interacting objects. This paper proposes a novel open vocabulary action recognition task. Given a set of verbs and objects observed during training, the goal is to generalize the verbs to an open vocabulary of actions with seen and novel objects. To this end, we decouple the verb and object predictions via an object-agnostic verb encoder and a prompt-based object encoder. The prompting leverages CLIP representations to predict an open vocabulary of interacting objects. We create open vocabulary benchmarks on the EPIC-KITCHENS-100 and Assembly101 datasets; whereas closed-action methods fail to generalize, our proposed method is effective. In addition, our object encoder significantly outperforms existing open-vocabulary visual recognition methods in recognizing novel interacting objects.
</details>
<details>
<summary>摘要</summary>
人类行为在 egocentric 视频中经常是手部-物体互动，由 verb（由手部执行的动作）和物体组成。尽管这些数据集已经大量扩大，但是它们仍然面临两个限制：动作组合稀缺和固定的交互物体集。本文提出了一个开放词汇动作认知任务。给定一组 verb 和在训练中观察到的物体，目标是将 verb 扩展到一个开放词汇的动作中，包括已知和新的交互物体。为此，我们分离 verb 和物体预测，使用 объекc-agnostic verb 编码器和提示基于 CLIP 表示来预测开放词汇的交互物体。我们创建了开放词汇 benchmark 在 EPIC-KITCHENS-100 和 Assembly101 数据集上，而封闭动作方法无法泛化，我们提posed 方法是有效的。此外，我们的物体编码器在认识新交互物体方面表现出色，大大超过了现有的开放词汇视觉认知方法。
</details></li>
</ul>
<hr>
<h2 id="Free-Lunch-for-Gait-Recognition-A-Novel-Relation-Descriptor"><a href="#Free-Lunch-for-Gait-Recognition-A-Novel-Relation-Descriptor" class="headerlink" title="Free Lunch for Gait Recognition: A Novel Relation Descriptor"></a>Free Lunch for Gait Recognition: A Novel Relation Descriptor</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11487">http://arxiv.org/abs/2308.11487</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jilong Wang, Saihui Hou, Yan Huang, Chunshui Cao, Xu Liu, Yongzhen Huang, Liang Wang</li>
<li>For: This paper focuses on improving gait recognition performance by reconsidering gait representation and emphasizing inter-personal relationships among different subjects’ gait features.* Methods: The proposed method, called Relationship Descriptor (RD), uses reference-anchored gaits to describe each person’s gait and emphasizes meaningful features by normalizing the dot product between gait features and classifier weights. To address the dimensionality challenges, the method proposes a Farthest Anchored gaits Selection algorithm and a dimension reduction method.* Results: The proposed method achieves higher recognition performance than directly using extracted features and consistently outperforms the baselines on four popular gait recognition datasets (GREW, Gait3D, CASIA-B, and OU-MVLP), achieving state-of-the-art performances.Here’s the simplified Chinese text:* For: 本研究目的是提高步行识别性能，重新评估步行表示方式，强调不同人群之间的步行特征关系。* Methods: 提出的方法是关系描述符（RD），使用参照锚定的步行特征来描述每个人的步行，强调意义更高的特征，通过归一化点积分来表示每个测试样本与每个训练ID的步行原型之间的相似性。* Results: 比较直接使用提取的特征，关系描述符可以提高步行识别性能，在四个流行的步行识别数据集（GREW、Gait3D、CASIA-B、OU-MVLP）上表现出state-of-the-art的性能。<details>
<summary>Abstract</summary>
Gait recognition is to seek correct matches for query individuals by their unique walking patterns at a long distance. However, current methods focus solely on individual gait features, disregarding inter-personal relationships. In this paper, we reconsider gait representation, asserting that gait is not just an aggregation of individual features, but also the relationships among different subjects' gait features once reference gaits are established. From this perspective, we redefine classifier weights as reference-anchored gaits, allowing each person's gait to be described by their relationship with these references. In our work, we call this novel descriptor Relationship Descriptor (RD). This Relationship Descriptor offers two benefits: emphasizing meaningful features and enhancing robustness. To be specific, The normalized dot product between gait features and classifier weights signifies a similarity relation, where each dimension indicates the similarity between the test sample and each training ID's gait prototype, respectively. Despite its potential, the direct use of relationship descriptors poses dimensionality challenges since the dimension of RD depends on the training set's identity count. To address this, we propose a Farthest Anchored gaits Selection algorithm and a dimension reduction method to boost gait recognition performance. Our method can be built on top of off-the-shelf pre-trained classification-based models without extra parameters. We show that RD achieves higher recognition performance than directly using extracted features. We evaluate the effectiveness of our method on the popular GREW, Gait3D, CASIA-B, and OU-MVLP, showing that our method consistently outperforms the baselines and achieves state-of-the-art performances.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translation_direction: zh-CNGait recognition是寻找正确的匹配个体的唯一步态特征，但现有方法强调个体特征，忽略了人之间的关系。在这篇论文中，我们重新定义步态表示，认为步态不仅是个体特征的汇集，还包括不同个体之间的关系。从这个视角，我们定义了一种新的描述符called Relationship Descriptor (RD)。这个描述符有两个优点：强调意义性特征和增强稳定性。具体来说，在测试样本和训练ID之间的标准化点积分比率表示两个样本之间的相似性关系，每个维度表示测试样本与每个训练ID的步态原型之间的相似性。虽有潜在的优势，直接使用关系描述符存在维度挑战，因为关系描述符的维度取决于训练集中个体数量。为解决这个问题，我们提出了最远锚定 gaits 选择算法和维度减少方法，以提高步态识别性能。我们的方法可以在现有的预训练分类模型基础上建立，无需添加参数。我们证明，RD 可以高于直接使用提取的特征来实现更高的识别性能。我们对广泛使用的 GREW、Gait3D、CASIA-B 和 OU-MVLP 进行评估，并证明我们的方法可以一直达到领先水平。Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Composed-Image-Retrieval-using-Contrastive-Learning-and-Task-oriented-CLIP-based-Features"><a href="#Composed-Image-Retrieval-using-Contrastive-Learning-and-Task-oriented-CLIP-based-Features" class="headerlink" title="Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features"></a>Composed Image Retrieval using Contrastive Learning and Task-oriented CLIP-based Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11485">http://arxiv.org/abs/2308.11485</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ABaldrati/CLIP4Cir">https://github.com/ABaldrati/CLIP4Cir</a></li>
<li>paper_authors: Alberto Baldrati, Marco Bertini, Tiberio Uricchio, Alberto del Bimbo</li>
<li>for: 本研究的目的是寻找基于referenced图像和相关的caption的compose image retrieval，即检索图像具有与referenced图像相同的视觉特征并满足caption中的修改。</li>
<li>methods: 本研究使用OpenAI CLIP模型的特征进行任务调整和组合，并使用对比学习进行训练。首先，通过任务特有的方式进行CLIP encoder的任务调整，然后在第二个阶段使用Combiner网络将图像和文本特征相结合，提供了组合特征进行检索。</li>
<li>results: 实验结果表明，基于CLIP特征的任务调整和Combiner网络对于FashionIQ和CIRR两个Popular和挑战性的compose image retrieval dataset具有高效性，并且超过了更复杂的现有方法。<details>
<summary>Abstract</summary>
Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir
</details>
<details>
<summary>摘要</summary>
Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, so we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir.Here's the translation in Traditional Chinese: Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, so we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir.
</details></li>
</ul>
<hr>
<h2 id="Pose2Gait-Extracting-Gait-Features-from-Monocular-Video-of-Individuals-with-Dementia"><a href="#Pose2Gait-Extracting-Gait-Features-from-Monocular-Video-of-Individuals-with-Dementia" class="headerlink" title="Pose2Gait: Extracting Gait Features from Monocular Video of Individuals with Dementia"></a>Pose2Gait: Extracting Gait Features from Monocular Video of Individuals with Dementia</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11484">http://arxiv.org/abs/2308.11484</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taatiteam/pose2gait_public">https://github.com/taatiteam/pose2gait_public</a></li>
<li>paper_authors: Caroline Malin-Mayor, Vida Adeli, Andrea Sabo, Sergey Noritsyn, Carolina Gorodetsky, Alfonso Fasano, Andrea Iaboni, Babak Taati</li>
<li>for: 这项研究旨在通过视频监测 older adults with dementia 的步态，早期发现健康状况下降，以防止跌倒或入院。</li>
<li>methods: 该研究使用了计算机视觉基于pose tracking模型来自动处理视频数据，并提取了人体 JOINT 位置。</li>
<li>results: 该模型能够从视频中提取步态特征，并与深度摄像头中的特征相比，Spearman 相关系数为。83和.60。这表明，三维空间时间特征可以从单一视频中预测。<details>
<summary>Abstract</summary>
Video-based ambient monitoring of gait for older adults with dementia has the potential to detect negative changes in health and allow clinicians and caregivers to intervene early to prevent falls or hospitalizations. Computer vision-based pose tracking models can process video data automatically and extract joint locations; however, publicly available models are not optimized for gait analysis on older adults or clinical populations. In this work we train a deep neural network to map from a two dimensional pose sequence, extracted from a video of an individual walking down a hallway toward a wall-mounted camera, to a set of three-dimensional spatiotemporal gait features averaged over the walking sequence. The data of individuals with dementia used in this work was captured at two sites using a wall-mounted system to collect the video and depth information used to train and evaluate our model. Our Pose2Gait model is able to extract velocity and step length values from the video that are correlated with the features from the depth camera, with Spearman's correlation coefficients of .83 and .60 respectively, showing that three dimensional spatiotemporal features can be predicted from monocular video. Future work remains to improve the accuracy of other features, such as step time and step width, and test the utility of the predicted values for detecting meaningful changes in gait during longitudinal ambient monitoring.
</details>
<details>
<summary>摘要</summary>
视频基于环境监测 older adults 的步态有潜力检测身体健康状况下降，并允许临床专业人员和照顾者在早期发现并预防滥落或入院。通过计算机视觉技术，可自动处理视频数据，并提取关节位置。然而，目前公共可用的模型并没有适应老年人或临床人群的步态分析。在这项工作中，我们训练了深度神经网络，将二维姿态序列，从视频中捕捉到人走向墙面镜头的人走动捕捉到三维空间时间步态特征的映射。我们的 Pose2Gait 模型可以从视频中提取速度和步长值，与深度相机中的特征相关性 coefficient 为 0.83 和 0.60，表明可以从单视频中预测三维空间时间步态特征。未来的工作是提高其他特征的准确性，如步时和步宽，并测试预测值的检测意义步长监测。
</details></li>
</ul>
<hr>
<h2 id="VadCLIP-Adapting-Vision-Language-Models-for-Weakly-Supervised-Video-Anomaly-Detection"><a href="#VadCLIP-Adapting-Vision-Language-Models-for-Weakly-Supervised-Video-Anomaly-Detection" class="headerlink" title="VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection"></a>VadCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11681">http://arxiv.org/abs/2308.11681</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, Yanning Zhang</li>
<li>for: 这个研究的目的是提出一个新的弱监督类别 видео异常探测（WSVAD）方法，并将CLIP模型 directly applied to WSVAD任务。</li>
<li>methods: 这个方法使用了CLIP模型的冻结版本，不需要进行预训练。它还包括一个双支路径，其中一支使用了视觉特征进行粗糙分类，另一支则全面利用语言-图像Alignment。</li>
<li>results: 实验结果显示，VadCLIP在两个常用的标准集上（XD-Violence和UCF-Crime）都达到了最高性能，比前一代方法高度超越。具体来说，VadCLIP在XD-Violence上 achieve 84.51% AP和88.02% AUC，在UCF-Crime上 achieve 84.51% AP和88.02% AUC。<details>
<summary>Abstract</summary>
The recent contrastive language-image pre-training (CLIP) model has shown great success in a wide range of image-level tasks, revealing remarkable ability for learning powerful visual representations with rich semantics. An open and worthwhile problem is efficiently adapting such a strong model to the video domain and designing a robust video anomaly detector. In this work, we propose VadCLIP, a new paradigm for weakly supervised video anomaly detection (WSVAD) by leveraging the frozen CLIP model directly without any pre-training and fine-tuning process. Unlike current works that directly feed extracted features into the weakly supervised classifier for frame-level binary classification, VadCLIP makes full use of fine-grained associations between vision and language on the strength of CLIP and involves dual branch. One branch simply utilizes visual features for coarse-grained binary classification, while the other fully leverages the fine-grained language-image alignment. With the benefit of dual branch, VadCLIP achieves both coarse-grained and fine-grained video anomaly detection by transferring pre-trained knowledge from CLIP to WSVAD task. We conduct extensive experiments on two commonly-used benchmarks, demonstrating that VadCLIP achieves the best performance on both coarse-grained and fine-grained WSVAD, surpassing the state-of-the-art methods by a large margin. Specifically, VadCLIP achieves 84.51% AP and 88.02% AUC on XD-Violence and UCF-Crime, respectively. Code and features will be released to facilitate future VAD research.
</details>
<details>
<summary>摘要</summary>
Recent contrastive language-image pre-training (CLIP) 模型已经在各种图像任务中显示出惊人的成功，揭示出了强大的视觉表示和丰富的 semantics。一个值得关注的问题是如何有效地适应这种强大模型到视频领域，并设计一个强健的视频异常检测器。在这种工作中，我们提出了VadCLIP，一种新的弱相关视频异常检测（WSVAD）方法，利用直接使用冻结CLIP模型，而不需要任何预训练和调整过程。与当前工作不同，VadCLIP不直接将提取的特征 fed into 弱相关分类器进行帧级二分类，而是利用CLIP模型的细腻语义关系，在两个分支中进行检测。一个分支使用视觉特征进行粗略二分类，另一个分支完全利用语义-图像对齐来进行细腻异常检测。通过两个分支的合作，VadCLIP可以将CLIP模型的预训练知识传递到WSVAD任务中，从而实现粗略和细腻的视频异常检测。我们进行了广泛的实验，证明VadCLIP在XD-Violence和UCF-Crime上的表现均高于当前最佳方法，具体来说是84.51% AP和88.02% AUC。代码和特征将会被发布，以便未来的VAD研究。
</details></li>
</ul>
<hr>
<h2 id="Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms"><a href="#Multitemporal-analysis-in-Google-Earth-Engine-for-detecting-urban-changes-using-optical-data-and-machine-learning-algorithms" class="headerlink" title="Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms"></a>Multitemporal analysis in Google Earth Engine for detecting urban changes using optical data and machine learning algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11468">http://arxiv.org/abs/2308.11468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mariapia Rita Iandolo, Francesca Razzano, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 这个研究旨在使用Google Earth Engine（GEE）平台进行多时间分析，检测城市区域的变化 using 光学数据和专门的机器学习（ML）算法。</li>
<li>methods: 这个研究使用了GEE平台和光学数据，以及专门的ML算法进行分类和变化检测分析。</li>
<li>results: 结果表明，提posed方法可以准确地标识 changed和unchanged的城市区域在选定的时间段内。此外，这个研究也证明了GEE的云存储平台对处理大量卫星数据的管理有所重要性。<details>
<summary>Abstract</summary>
The aim of this work is to perform a multitemporal analysis using the Google Earth Engine (GEE) platform for the detection of changes in urban areas using optical data and specific machine learning (ML) algorithms. As a case study, Cairo City has been identified, in Egypt country, as one of the five most populous megacities of the last decade in the world. Classification and change detection analysis of the region of interest (ROI) have been carried out from July 2013 to July 2021. Results demonstrate the validity of the proposed method in identifying changed and unchanged urban areas over the selected period. Furthermore, this work aims to evidence the growing significance of GEE as an efficient cloud-based solution for managing large quantities of satellite data.
</details>
<details>
<summary>摘要</summary>
本工作的目的是使用Google Earth Engine（GEE）平台进行多时间分析，以探测城市区域的变化使用光学数据和专门的机器学习（ML）算法。作为案例研究，埃及国的开罗城市被选为世界上最后一个十年内最为人口稠密的五大都市之一。从2013年7月至2021年7月的时间段进行了区域 интерес（ROI）的分类和变化检测分析。结果表明提出的方法的有效性，可以准确地标识变化和不变的城市区域在选定的时间段内。此外，这项工作还旨在证明GEE作为云端解决方案，对处理巨量卫星数据的管理有着高效的能力。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-Initial-Training-Strategies-for-Exemplar-Free-Class-Incremental-Learning"><a href="#An-Analysis-of-Initial-Training-Strategies-for-Exemplar-Free-Class-Incremental-Learning" class="headerlink" title="An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning"></a>An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11677">http://arxiv.org/abs/2308.11677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grégoire Petit, Michael Soumm, Eva Feillet, Adrian Popescu, Bertrand Delezoide, David Picard, Céline Hudelot</li>
<li>for: 这个论文的目的是探讨分类模型在数据流中建立的增量学习问题。</li>
<li>methods: 论文使用的方法包括增量学习过程中的新类 интеграción、采用先前学习的模型初始化、选择合适的增量学习算法和评估增量学习模型的性能等。</li>
<li>results: 论文的主要发现是初始学习策略对增量准确率的影响是最大的，但是选择合适的增量学习算法更重要地防止忘记。根据这些发现，论文提出了实践增量学习的实用建议。<details>
<summary>Abstract</summary>
Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of classes in the stream and the number of examples available for learning. We conduct a comprehensive experimental study to assess the roles of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing the average incremental accuracy, but that the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations are intended to facilitate the practical deployment of incremental learning.
</details>
<details>
<summary>摘要</summary>
We conduct a comprehensive study to assess the influence of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing average incremental accuracy, but the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations aim to facilitate the practical deployment of incremental learning.
</details></li>
</ul>
<hr>
<h2 id="Food-Image-Classification-and-Segmentation-with-Attention-based-Multiple-Instance-Learning"><a href="#Food-Image-Classification-and-Segmentation-with-Attention-based-Multiple-Instance-Learning" class="headerlink" title="Food Image Classification and Segmentation with Attention-based Multiple Instance Learning"></a>Food Image Classification and Segmentation with Attention-based Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11452">http://arxiv.org/abs/2308.11452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Valasia Vlachopoulou, Ioannis Sarafis, Alexandros Papadopoulos<br>for:* 这个论文是为了解决食物量计量问题而写的，它适用于食物监测应用场景。methods:* 这篇论文使用了弱监视学习方法，不需要像素级别的标注数据来训练食物图像分类和Semantic Segmentation模型。* 该方法基于多例学习approach，并使用了注意力机制来自动生成食物类划分图像。results:* 在FoodSeg103数据集上进行了实验，并证明了提议的方法的可行性和注意力机制的作用。<details>
<summary>Abstract</summary>
The demand for accurate food quantification has increased in the recent years, driven by the needs of applications in dietary monitoring. At the same time, computer vision approaches have exhibited great potential in automating tasks within the food domain. Traditionally, the development of machine learning models for these problems relies on training data sets with pixel-level class annotations. However, this approach introduces challenges arising from data collection and ground truth generation that quickly become costly and error-prone since they must be performed in multiple settings and for thousands of classes. To overcome these challenges, the paper presents a weakly supervised methodology for training food image classification and semantic segmentation models without relying on pixel-level annotations. The proposed methodology is based on a multiple instance learning approach in combination with an attention-based mechanism. At test time, the models are used for classification and, concurrently, the attention mechanism generates semantic heat maps which are used for food class segmentation. In the paper, we conduct experiments on two meta-classes within the FoodSeg103 data set to verify the feasibility of the proposed approach and we explore the functioning properties of the attention mechanism.
</details>
<details>
<summary>摘要</summary>
随着食物质量评估的需求增加，计算机视觉方法在食物领域中表现出了很大的潜力。然而，传统的机器学习模型开发方法仍然需要带有像素级别的分类注释。然而，这种方法会导致数据收集和真实性生成的挑战，这些挑战会在多个设置下并且对 тысячи个类型进行多次重复。为了缓解这些挑战，文章提出了一种弱型监督的方法，不需要像素级别的注释来训练食物图像分类和 semantic segmentation 模型。该方法基于多例学习approach和注意力机制。在测试时，模型用于分类，同时注意力机制生成 semantic heat map，用于食物类划分。在文章中，我们对 FoodSeg103 数据集中的两个元类进行了实验，以验证提议的可行性，并探索注意力机制的工作性质。
</details></li>
</ul>
<hr>
<h2 id="Towards-Discriminative-Representations-with-Contrastive-Instances-for-Real-Time-UAV-Tracking"><a href="#Towards-Discriminative-Representations-with-Contrastive-Instances-for-Real-Time-UAV-Tracking" class="headerlink" title="Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking"></a>Towards Discriminative Representations with Contrastive Instances for Real-Time UAV Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11450">http://arxiv.org/abs/2308.11450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dan Zeng, Mingliang Zou, Xucheng Wang, Shuiwang Li</li>
<li>for: 提高UAV跟踪中的精准率和效率，这两个基本挑战是由于计算资源限制、电池容量和UAV最大荷载所带来的。</li>
<li>methods: 使用异构相关矩阵（DCF）基于的跟踪器可以在单个CPU上实现高效性，但是精准率较差。具有轻量级深度学习（DL）基于的跟踪器可以实现精准率和效率的平衡，但是性能增加受压缩率的限制。</li>
<li>results: 根据四个UAV标准测试集（UAV123@10fps、DTB70、UAVDT和VisDrone2018）的广泛实验结果，提出的DRCI跟踪器在对state-of-the-art UAV跟踪方法进行比较时表现出了显著的优势。<details>
<summary>Abstract</summary>
Maintaining high efficiency and high precision are two fundamental challenges in UAV tracking due to the constraints of computing resources, battery capacity, and UAV maximum load. Discriminative correlation filters (DCF)-based trackers can yield high efficiency on a single CPU but with inferior precision. Lightweight Deep learning (DL)-based trackers can achieve a good balance between efficiency and precision but performance gains are limited by the compression rate. High compression rate often leads to poor discriminative representations. To this end, this paper aims to enhance the discriminative power of feature representations from a new feature-learning perspective. Specifically, we attempt to learn more disciminative representations with contrastive instances for UAV tracking in a simple yet effective manner, which not only requires no manual annotations but also allows for developing and deploying a lightweight model. We are the first to explore contrastive learning for UAV tracking. Extensive experiments on four UAV benchmarks, including UAV123@10fps, DTB70, UAVDT and VisDrone2018, show that the proposed DRCI tracker significantly outperforms state-of-the-art UAV tracking methods.
</details>
<details>
<summary>摘要</summary>
维护高效率和高精度是无人机追踪中的两个基本挑战，因为计算资源、电池容量和无人机的最大负载对紧。对应式相关滤波器（DCF）基本trackers可以在单一CPU上提供高效率，但是精度较差。轻量级深度学习（DL）基本trackers可以实现高效率和精度的平衡，但是性能增加受压缩率的限制。高压缩率通常会导致糟糕的描述表现。因此，本文的目标是将无人机追踪中的特征表现强化，以提高追踪精度。具体来说，我们尝试通过新的特征学习角度，从而学习更有弹性的特征表现，这不仅不需要手动标注，而且允许开发和部署轻量级模型。我们是无人机追踪中首次应用对照学习。实验结果显示，提案的DRCI tracker在四个无人机测试 benchmark 上具有明显的 superiority，包括UAV123@10fps、DTB70、UAVDT和VisDrone2018。
</details></li>
</ul>
<hr>
<h2 id="Masked-Momentum-Contrastive-Learning-for-Zero-shot-Semantic-Understanding"><a href="#Masked-Momentum-Contrastive-Learning-for-Zero-shot-Semantic-Understanding" class="headerlink" title="Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding"></a>Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11448">http://arxiv.org/abs/2308.11448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiantao Wu, Shentong Mo, Muhammad Awais, Sara Atito, Zhenhua Feng, Josef Kittler</li>
<li>for: 本研究旨在评估自主学习技术在计算机视觉任务中的效iveness，以避免训练和finetuning大型模型。</li>
<li>methods: 本研究提出了一种评估协议，基于提示 patch，用于评估零shot segmentation的能力。此外，还提出了一种简单的SSL方法，称为MMC，该方法组合了masked image modelling、 momentum based self-distillation和global contrast等技术，以提高SSL ViTs的描述性表示。</li>
<li>results: 实验表明，MMC方法在零shot semantic segmentation中达到了顶尖水平，并且在不同的 dataset 上都表现出色。<details>
<summary>Abstract</summary>
Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the target. Another evaluation is intra-object and inter-object similarity to gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation from prompting and discriminatory abilities of SSP led to the design of a simple SSP approach, termed MMC. This approaches combines Masked image modelling for encouraging similarity of local features, Momentum based self-distillation for transferring semantics from global to local features, and global Contrast for promoting semantics of global features, to enhance discriminative representations of SSP ViTs. Consequently, our proposed method significantly reduces the overlap of intra-object and inter-object similarities, thereby facilitating effective object segmentation within an image. Our experiments reveal that MMC delivers top-tier results in zero-shot semantic segmentation across various datasets.
</details>
<details>
<summary>摘要</summary>
自顾学前教程（SSP）在机器学习中得到广泛应用，帮助提取有意义的特征表示无需标注数据。在计算机视觉领域，预训练视transformer（ViT）已经发挥了重要的作用，促进了转移学习。然而，模型的规模快速增长带来了训练成本的涨幅问题。本研究旨在评估无标注学习（SSL）技术在计算机视觉任务中的效果，不需要训练，以模仿人类对未看到对象的概念化和识别能力。为此，我们提出了一种零shot segmentation的评估协议，基于提示 patch。给定目标对象的一点作为提示，算法计算selected patch和其他 patch之间的相似度图，然后应用简单的阈值处理来 segment the target。此外，我们还进行了内部和外部相似性评估，以评估 SSL ViTs 的泛化能力。通过零shot segmentation和泛化能力的研究，我们设计了一种简单的 SSP 方法，称为 MMC。该方法结合了做masked image模型，自身融合和全局对比，以提高 SSL ViTs 的泛化表示。实验表明，MMC 可以在不同的 dataset 上达到顶尖的 zero-shot semantic segmentation结果。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-and-Exploring-Efficient-Fast-Adversarial-Training-via-LAW-Lipschitz-Regularization-and-Auto-Weight-Averaging"><a href="#Revisiting-and-Exploring-Efficient-Fast-Adversarial-Training-via-LAW-Lipschitz-Regularization-and-Auto-Weight-Averaging" class="headerlink" title="Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging"></a>Revisiting and Exploring Efficient Fast Adversarial Training via LAW: Lipschitz Regularization and Auto Weight Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11443">http://arxiv.org/abs/2308.11443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojun Jia, Yuefeng Chen, Xiaofeng Mao, Ranjie Duan, Jindong Gu, Rong Zhang, Hui Xue, Xiaochun Cao</li>
<li>For: The paper aims to improve the robustness of machine learning models against adversarial attacks while reducing the training cost of standard adversarial training.* Methods: The paper proposes an effective Lipschitz regularization method for fast adversarial training and explores the effect of data augmentation and weight averaging in fast adversarial training.* Results: The proposed method, FGSM-LAW, demonstrates superior robustness performance compared to state-of-the-art fast adversarial training methods and advanced standard adversarial training methods, as shown in experimental evaluations on four benchmark databases.<details>
<summary>Abstract</summary>
Fast Adversarial Training (FAT) not only improves the model robustness but also reduces the training cost of standard adversarial training. However, fast adversarial training often suffers from Catastrophic Overfitting (CO), which results in poor robustness performance. Catastrophic Overfitting describes the phenomenon of a sudden and significant decrease in robust accuracy during the training of fast adversarial training. Many effective techniques have been developed to prevent Catastrophic Overfitting and improve the model robustness from different perspectives. However, these techniques adopt inconsistent training settings and require different training costs, i.e, training time and memory costs, leading to unfair comparisons. In this paper, we conduct a comprehensive study of over 10 fast adversarial training methods in terms of adversarial robustness and training costs. We revisit the effectiveness and efficiency of fast adversarial training techniques in preventing Catastrophic Overfitting from the perspective of model local nonlinearity and propose an effective Lipschitz regularization method for fast adversarial training. Furthermore, we explore the effect of data augmentation and weight averaging in fast adversarial training and propose a simple yet effective auto weight averaging method to improve robustness further. By assembling these techniques, we propose a FGSM-based fast adversarial training method equipped with Lipschitz regularization and Auto Weight averaging, abbreviated as FGSM-LAW. Experimental evaluations on four benchmark databases demonstrate the superiority of the proposed method over state-of-the-art fast adversarial training methods and the advanced standard adversarial training methods.
</details>
<details>
<summary>摘要</summary>
快速对抗训练（FAT）不仅提高模型的Robustness，还可以降低标准对抗训练的训练成本。然而，快速对抗训练经常会遭遇Catastrophic Overfitting（CO），这会导致对抗性性能很差。Catastrophic Overfitting是指在快速对抗训练中突然和 significatively减少对抗性性能的现象。许多有效的技术已经开发以预防Catastrophic Overfitting并提高模型的Robustness，但这些技术采用不一致的训练设置和不同的训练成本，如训练时间和内存成本，导致不公平的比较。在这篇论文中，我们进行了快速对抗训练方法超过10种的全面研究，包括对抗性和训练成本。我们从模型本地非线性的角度重新评估快速对抗训练技术的效iveness和效率，并提出了一种有效的Lipschitz regularization方法。此外，我们还研究了快速对抗训练中数据扩展和权重平均的效果，并提出了一种简单又有效的自动权重平均方法，以进一步提高对抗性。通过组合这些技术，我们提出了一种基于FGSM的快速对抗训练方法，即FGSM-LAW。实验评估在四个基本数据库中，显示我们的方法在与标准对抗训练方法和先进的标准对抗训练方法相比，具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="SDeMorph-Towards-Better-Facial-De-morphing-from-Single-Morph"><a href="#SDeMorph-Towards-Better-Facial-De-morphing-from-Single-Morph" class="headerlink" title="SDeMorph: Towards Better Facial De-morphing from Single Morph"></a>SDeMorph: Towards Better Facial De-morphing from Single Morph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11442">http://arxiv.org/abs/2308.11442</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nitish Shukla</li>
<li>for: 防止摸索攻击 (Morph Attack Detection)</li>
<li>methods: 无参考基于Diffusion Probabilistic Models (DDPM)和branched-UNet</li>
<li>results: 可以准确地回归真实的人脸特征，提高了人脸识别系统的安全性<details>
<summary>Abstract</summary>
Face Recognition Systems (FRS) are vulnerable to morph attacks. A face morph is created by combining multiple identities with the intention to fool FRS and making it match the morph with multiple identities. Current Morph Attack Detection (MAD) can detect the morph but are unable to recover the identities used to create the morph with satisfactory outcomes. Existing work in de-morphing is mostly reference-based, i.e. they require the availability of one identity to recover the other. Sudipta et al. \cite{ref9} proposed a reference-free de-morphing technique but the visual realism of outputs produced were feeble. In this work, we propose SDeMorph (Stably Diffused De-morpher), a novel de-morphing method that is reference-free and recovers the identities of bona fides. Our method produces feature-rich outputs that are of significantly high quality in terms of definition and facial fidelity. Our method utilizes Denoising Diffusion Probabilistic Models (DDPM) by destroying the input morphed signal and then reconstructing it back using a branched-UNet. Experiments on ASML, FRLL-FaceMorph, FRLL-MorDIFF, and SMDD datasets support the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
人脸识别系统（FRS）容易受到形态攻击。一个形态是通过将多个标识 combine 以达到欺骗 FRS 并让它匹配形态中的多个标识。现有的形态攻击检测（MAD）可以检测形态，但无法恢复创建形态的标识。现有的工作大多数是参考基于的，即需要一个标识的可用性来恢复另一个标识。Sudipta et al. \cite{ref9} 提出了一种无参考的恢复技术，但Visual realism 输出的质量不够高。在这个工作中，我们提出了SDeMorph（稳定扩散恢复器），一种新的无参考恢复方法，可以恢复创建形态的标识。我们的方法生成了高质量的输出，具有高度的定义和人脸准确性。我们的方法利用了Denosing Diffusion Probabilistic Models (DDPM)，通过破坏输入形态信号，然后使用分支-UNet 重建它。实验表明，我们的方法在 ASML、FRLL-FaceMorph、FRLL-MorDIFF 和 SMDD 数据集上具有效果。
</details></li>
</ul>
<hr>
<h2 id="Learning-a-More-Continuous-Zero-Level-Set-in-Unsigned-Distance-Fields-through-Level-Set-Projection"><a href="#Learning-a-More-Continuous-Zero-Level-Set-in-Unsigned-Distance-Fields-through-Level-Set-Projection" class="headerlink" title="Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection"></a>Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11441">http://arxiv.org/abs/2308.11441</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/junshengzhou/levelsetudf">https://github.com/junshengzhou/levelsetudf</a></li>
<li>paper_authors: Junsheng Zhou, Baorui Ma, Shujuan Li, Yu-Shen Liu, Zhizhong Han</li>
<li>for: 该paper aimed to address the problem of reconstructing surfaces with open surfaces using unsigned distance functions (UDFs).</li>
<li>methods: The authors proposed to learn UDFs using neural networks and reconstruct surfaces with the gradients around the zero level set of the UDF. However, they found that the differential networks struggled to learn the zero level set, leading to large errors on unsigned distances and gradients. To resolve this, they proposed to learn a more continuous zero level set using level set projections.</li>
<li>results: The authors conducted comprehensive experiments in surface reconstruction for point clouds, real scans or depth maps, and demonstrated non-trivial improvements over the state-of-the-art methods. They also explored the performance in unsupervised point cloud upsampling and unsupervised point normal estimation with the learned UDF.<details>
<summary>Abstract</summary>
Latest methods represent shapes with open surfaces using unsigned distance functions (UDFs). They train neural networks to learn UDFs and reconstruct surfaces with the gradients around the zero level set of the UDF. However, the differential networks struggle from learning the zero level set where the UDF is not differentiable, which leads to large errors on unsigned distances and gradients around the zero level set, resulting in highly fragmented and discontinuous surfaces. To resolve this problem, we propose to learn a more continuous zero level set in UDFs with level set projections. Our insight is to guide the learning of zero level set using the rest non-zero level sets via a projection procedure. Our idea is inspired from the observations that the non-zero level sets are much smoother and more continuous than the zero level set. We pull the non-zero level sets onto the zero level set with gradient constraints which align gradients over different level sets and correct unsigned distance errors on the zero level set, leading to a smoother and more continuous unsigned distance field. We conduct comprehensive experiments in surface reconstruction for point clouds, real scans or depth maps, and further explore the performance in unsupervised point cloud upsampling and unsupervised point normal estimation with the learned UDF, which demonstrate our non-trivial improvements over the state-of-the-art methods. Code is available at https://github.com/junshengzhou/LevelSetUDF .
</details>
<details>
<summary>摘要</summary>
最新的方法使用无符号距离函数（UDF）来表示形状。它们使用神经网络学习UDF并重建表面的梯度在UDF的零水平面周围。然而，� diferencial networks 受到学习零水平面的约束，在零水平面不 diferenciable 的情况下，会导致大量的unsigned distance 和梯度 around the zero level set 的错误，从而导致表面变得高度分裂和不连续。为解决这问题，我们提议通过约束水平面投影来学习更加连续的零水平面。我们的想法来自于观察到非零水平面比零水平面更加平滑和连续。我们使用梯度约束将非零水平面投影到零水平面，以实现梯度的对齐和unsigned distance 错误的修正，从而导致更加平滑和连续的unsigned distance field。我们进行了广泛的实验，包括点云重建、真实扫描或深度图重建，以及不supervised point cloud upsampling 和不supervised point normal estimation 等，其中所获得的改进均非常 significativ。代码可以在 <https://github.com/junshengzhou/LevelSetUDF> 上找到。
</details></li>
</ul>
<hr>
<h2 id="PoseGraphNet-Enriching-3D-Human-Pose-with-Orientation-Estimation"><a href="#PoseGraphNet-Enriching-3D-Human-Pose-with-Orientation-Estimation" class="headerlink" title="PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation"></a>PoseGraphNet++: Enriching 3D Human Pose with Orientation Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11440">http://arxiv.org/abs/2308.11440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soubarna Banik, Edvard Avagyan, Alejandro Mendoza Gracia, Alois Knoll</li>
<li>for: 本研究旨在提出一种基于图гра夫 convolutional neural network（Graph Convolutional Network，GCN）的2D-to-3D提升方法，以便预测人体3D姿态包括关节位置和骨orientation。</li>
<li>methods: 我们提出了一种名为PoseGraphNet++的新型2D-to-3D提升网络，该网络通过节点和边卷积来利用关节和骨特征。</li>
<li>results: 我们在多个标准测试集上评估了我们的模型，并与状态的艺术结果相比，我们的模型在位置和旋转度量上具有类似或更高的性能。此外，我们还通过广泛的减少研究，证明了PoseGraphNet++可以借鉴关节和骨之间的相互关系，从而提高预测性能。<details>
<summary>Abstract</summary>
Existing kinematic skeleton-based 3D human pose estimation methods only predict joint positions. Although this is sufficient to compute the yaw and pitch of the bone rotations, the roll around the axis of the bones remains unresolved by these methods. In this paper, we propose a novel 2D-to-3D lifting Graph Convolution Network named PoseGraphNet++ to predict the complete human pose including the joint positions and the bone orientations. We employ node and edge convolutions to utilize the joint and bone features. Our model is evaluated on multiple benchmark datasets, and its performance is either on par with or better than the state-of-the-art in terms of both position and rotation metrics. Through extensive ablation studies, we show that PoseGraphNet++ benefits from exploiting the mutual relationship between the joints and the bones.
</details>
<details>
<summary>摘要</summary>
现有的骨骼基本体系3D人姿估算方法只预测关节位置。尽管这足够计算关节的封顶和旋转，但是关节的滚动仍然无法被这些方法解决。在这篇论文中，我们提出了一种新的2D-to-3D提升图 convolution neural network（PoseGraphNet++），用于预测完整的人姿，包括关节位置和骨头orientation。我们利用节点和边卷积来利用关节和骨头特征。我们的模型在多个基准数据集上进行评估，其性能与或超过了现有的状态的艺术metric。通过广泛的减少研究，我们表明了PoseGraphNet++可以通过利用关节和骨头之间的相互关系来增强性能。
</details></li>
</ul>
<hr>
<h2 id="ScanNet-A-High-Fidelity-Dataset-of-3D-Indoor-Scenes"><a href="#ScanNet-A-High-Fidelity-Dataset-of-3D-Indoor-Scenes" class="headerlink" title="ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes"></a>ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11417">http://arxiv.org/abs/2308.11417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, Angela Dai</li>
<li>for: 提供了一个大规模的indoor场景数据集，其中每个场景都是使用高级激光扫描仪获得高分辨率的geometry和颜色信息，同时还包括了 региSTR的3300万像素图像和iPhone的RGB-D流。</li>
<li>methods: 使用高级激光扫描仪和DSLR相机捕捉场景图像，并使用iPhone捕捉RGB-D流。场景重建还包括了开放词汇的semantics标注，以便实现全面的semantic理解。</li>
<li>results: ScanNet++提供了一个新的real-worldbenchmark дляnovel view synthesis，包括高品质RGB捕捉和商业级图像的synthesis，以及一个新的3Dsemantic scene理解 benchmark，全面地涵盖了多种和抽象的semantic标注方案。ScanNet++ currently contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.<details>
<summary>Abstract</summary>
We present ScanNet++, a large-scale dataset that couples together capture of high-quality and commodity-level geometry and color of indoor scenes. Each scene is captured with a high-end laser scanner at sub-millimeter resolution, along with registered 33-megapixel images from a DSLR camera, and RGB-D streams from an iPhone. Scene reconstructions are further annotated with an open vocabulary of semantics, with label-ambiguous scenarios explicitly annotated for comprehensive semantic understanding. ScanNet++ enables a new real-world benchmark for novel view synthesis, both from high-quality RGB capture, and importantly also from commodity-level images, in addition to a new benchmark for 3D semantic scene understanding that comprehensively encapsulates diverse and ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460 scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.
</details>
<details>
<summary>摘要</summary>
我们现在提供ScanNet++ dataset，这是一个大规模的数据集，它结合了高质量的激光扫描仪和商用级别的颜色捕捉indoor场景。每个场景都被高级激光扫描仪 capture，并与注册的3300万像素DSLR镜头拍摄的图像，以及iPhone的RGB-D流相匹配。场景重建还被标注为开放词汇Semantics，并且明确标注了涉及多义 Label的情况，以便全面理解semantic。ScanNet++提供了一个新的真实世界标准 benchmark for novel view synthesis，不仅来自高质量RGB捕捉，还来自商用级别的图像，以及一个新的3Dsemantic场景理解标准，全面涵盖多样化和涉及多义标签的情况。目前，ScanNet++包含460个场景，280,000个拍摄的DSLR图像，以及超过370万个iPhone RGBD帧。
</details></li>
</ul>
<hr>
<h2 id="MatFuse-Controllable-Material-Generation-with-Diffusion-Models"><a href="#MatFuse-Controllable-Material-Generation-with-Diffusion-Models" class="headerlink" title="MatFuse: Controllable Material Generation with Diffusion Models"></a>MatFuse: Controllable Material Generation with Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11408">http://arxiv.org/abs/2308.11408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Vecchio, Renato Sortino, Simone Palazzo, Concetto Spampinato</li>
<li>for: This paper aims to simplify the creation of SVBRDF maps in computer graphics, using a novel unified approach based on diffusion models.</li>
<li>methods: The proposed method integrates multiple sources of conditioning, such as color palettes, sketches, and pictures, to enable fine-grained control and flexibility in material synthesis.</li>
<li>results: The proposed method yields performance comparable to state-of-the-art approaches in estimating SVBRDF, both qualitatively and quantitatively, under various conditioning settings.<details>
<summary>Abstract</summary>
Creating high quality and realistic materials in computer graphics is a challenging and time-consuming task, which requires great expertise. In this paper, we present MatFuse, a novel unified approach that harnesses the generative power of diffusion models (DM) to simplify the creation of SVBRDF maps. Our DM-based pipeline integrates multiple sources of conditioning, such as color palettes, sketches, and pictures, enabling fine-grained control and flexibility in material synthesis. This design allows for the combination of diverse information sources (e.g., sketch + image embedding), enhancing creative possibilities in line with the principle of compositionality. We demonstrate the generative capabilities of the proposed method under various conditioning settings; on the SVBRDF estimation task, we show that our method yields performance comparable to state-of-the-art approaches, both qualitatively and quantitatively.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Non-Redundant-Combination-of-Hand-Crafted-and-Deep-Learning-Radiomics-Application-to-the-Early-Detection-of-Pancreatic-Cancer"><a href="#Non-Redundant-Combination-of-Hand-Crafted-and-Deep-Learning-Radiomics-Application-to-the-Early-Detection-of-Pancreatic-Cancer" class="headerlink" title="Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer"></a>Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11389">http://arxiv.org/abs/2308.11389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebeca Vétil, Clément Abi-Nader, Alexandre Bône, Marie-Pierre Vullierme, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</li>
<li>for: 这篇论文旨在解决深度学习医学影像特征（DLR）和手工设计医学影像特征（HCR）之间的重复性问题。</li>
<li>methods: 作者使用了一种简单的Variational Autoencoder（VAE）来提取DLR特征，并且通过降低这两种特征之间的相互信息来确保它们之间的独立性。</li>
<li>results: 作者的方法可以与手工设计的特征结合，并且通过一个分类器来预测抑制肝癌的早期标志。实验结果显示，结合非重复的DLR和HCR特征可以提高预测性能，比基eline方法更好。<details>
<summary>Abstract</summary>
We address the problem of learning Deep Learning Radiomics (DLR) that are not redundant with Hand-Crafted Radiomics (HCR). To do so, we extract DLR features using a VAE while enforcing their independence with HCR features by minimizing their mutual information. The resulting DLR features can be combined with hand-crafted ones and leveraged by a classifier to predict early markers of cancer. We illustrate our method on four early markers of pancreatic cancer and validate it on a large independent test set. Our results highlight the value of combining non-redundant DLR and HCR features, as evidenced by an improvement in the Area Under the Curve compared to baseline methods that do not address redundancy or solely rely on HCR features.
</details>
<details>
<summary>摘要</summary>
我们解决了深度学习医学影像特征（DLR）不重复的问题。我们使用VAE将DLR特征提取出来，并在这些特征之间强制独立性，以避免与手工设计的医学影像特征（HCR）之间的相互信息。这些DLR特征可以与手工设计的特征结合，并由分类器使用来预测早期癌症 markers。我们在四种早期肝癌标志物中进行了实验，并在一个大型独立测试集上验证了我们的方法。我们的结果显示了结合非重复的DLR和HCR特征的价值，即比基eline方法不 addressed 或仅仅靠赖于HCR特征的情况下，预测性能有所提高。
</details></li>
</ul>
<hr>
<h2 id="Targeted-Data-Augmentation-for-bias-mitigation"><a href="#Targeted-Data-Augmentation-for-bias-mitigation" class="headerlink" title="Targeted Data Augmentation for bias mitigation"></a>Targeted Data Augmentation for bias mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11386">http://arxiv.org/abs/2308.11386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Maria Ferlin, Michał Grochowski</li>
<li>for: This paper aims to address the issue of bias in AI systems by introducing a novel approach called Targeted Data Augmentation (TDA).</li>
<li>methods: The TDA method leverages classical data augmentation techniques to insert biases into the training data, which helps to mitigate biases in the models.</li>
<li>results: The paper shows that the TDA method can significantly decrease bias measures while maintaining a negligible increase in the error rate, using two diverse datasets of clinical skin lesions and male and female faces.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文目的是解决人工智能系统中的偏见问题，通过引入一种新的方法 called Targeted Data Augmentation (TDA)。</li>
<li>methods: TDA方法利用经典的数据增强技术，插入偏见到训练数据中，以减少模型中的偏见。</li>
<li>results: 论文显示，TDA方法可以Significantly减少偏见度量，同时保持误差率的增长在较低水平，使用了两个多样化的数据集：皮肤病变和男女面孔数据集。<details>
<summary>Abstract</summary>
The development of fair and ethical AI systems requires careful consideration of bias mitigation, an area often overlooked or ignored. In this study, we introduce a novel and efficient approach for addressing biases called Targeted Data Augmentation (TDA), which leverages classical data augmentation techniques to tackle the pressing issue of bias in data and models. Unlike the laborious task of removing biases, our method proposes to insert biases instead, resulting in improved performance. To identify biases, we annotated two diverse datasets: a dataset of clinical skin lesions and a dataset of male and female faces. These bias annotations are published for the first time in this study, providing a valuable resource for future research. Through Counterfactual Bias Insertion, we discovered that biases associated with the frame, ruler, and glasses had a significant impact on models. By randomly introducing biases during training, we mitigated these biases and achieved a substantial decrease in bias measures, ranging from two-fold to more than 50-fold, while maintaining a negligible increase in the error rate.
</details>
<details>
<summary>摘要</summary>
发展公正和伦理AI系统需要仔细考虑偏见缓解，这个领域经常被排除或忽略。在这个研究中，我们提出了一种新的和高效的偏见缓解方法，即目标数据扩展（TDA），该方法利用了经典数据扩展技术来解决数据和模型中的偏见问题。与努力除去偏见不同，我们的方法提议在训练过程中插入偏见，从而提高性能。为了标识偏见，我们对两个多样化的数据集进行了标注：一个是皮肤病变 dataset，另一个是男女脸部 dataset。这些偏见标注在本研究中首次公布，为未来研究提供了一个有价值的资源。通过对比方案插入，我们发现了框架、尺度和镜片等偏见对模型产生了重要影响。通过随机在训练过程中引入偏见，我们减少了这些偏见的度量，从2倍到更多于50倍，同时保持了错误率的增长在较低水平。
</details></li>
</ul>
<hr>
<h2 id="DALNet-A-Rail-Detection-Network-Based-on-Dynamic-Anchor-Line"><a href="#DALNet-A-Rail-Detection-Network-Based-on-Dynamic-Anchor-Line" class="headerlink" title="DALNet: A Rail Detection Network Based on Dynamic Anchor Line"></a>DALNet: A Rail Detection Network Based on Dynamic Anchor Line</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11381">http://arxiv.org/abs/2308.11381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yzichen/mmlanedet">https://github.com/yzichen/mmlanedet</a></li>
<li>paper_authors: Zichen Yu, Quanli Liu, Wei Wang, Liyong Zhang, Xiaoguang Zhao</li>
<li>for: 提高智能列车的轨道检测精度</li>
<li>methods: 基于动态锚点线的轨道检测网络DALNet，包括动态锚点生成器和轨道检测模块</li>
<li>results: DALNet在我们提供的DL-Rail轨道检测数据集和知名的Tusimple和LLAMAS车道检测标准 benchmark中达到了状态之精度表现。<details>
<summary>Abstract</summary>
Rail detection is one of the key factors for intelligent train. In the paper, motivated by the anchor line-based lane detection methods, we propose a rail detection network called DALNet based on dynamic anchor line. Aiming to solve the problem that the predefined anchor line is image agnostic, we design a novel dynamic anchor line mechanism. It utilizes a dynamic anchor line generator to dynamically generate an appropriate anchor line for each rail instance based on the position and shape of the rails in the input image. These dynamically generated anchor lines can be considered as better position references to accurately localize the rails than the predefined anchor lines. In addition, we present a challenging urban rail detection dataset DL-Rail with high-quality annotations and scenario diversity. DL-Rail contains 7000 pairs of images and annotations along with scene tags, and it is expected to encourage the development of rail detection. We extensively compare DALNet with many competitive lane methods. The results show that our DALNet achieves state-of-the-art performance on our DL-Rail rail detection dataset and the popular Tusimple and LLAMAS lane detection benchmarks. The code will be released at https://github.com/Yzichen/mmLaneDet.
</details>
<details>
<summary>摘要</summary>
铁路检测是智能列车的关键因素之一。在论文中，我们被动 anchor line-based 铁路检测方法所 inspirited，并提出了基于动态 anchor line 的铁路检测网络 DALNet。为了解决预定义的 anchor line 是图像不特定的问题，我们设计了一种新的动态 anchor line 机制。它利用动态 anchor line 生成器生成每个铁路实例的相应 anchor line，根据输入图像中铁路的位置和形状。这些动态生成的 anchor line 可以视为更好的位置参考，以准确地 Localize 铁路。此外，我们提供了一个挑战性的城市铁路检测数据集 DL-Rail，其包括7000个图像和注释对，以及Scene 标签。我们进行了广泛的比较，结果显示，我们的 DALNet 在我们的 DL-Rail 铁路检测数据集和知名的 Tusimple 和 LLAMAS lane detection benchmark 上均达到了状态组件表现。代码将在 GitHub 上发布，详细信息请参考 <https://github.com/Yzichen/mmLaneDet>。
</details></li>
</ul>
<hr>
<h2 id="Boundary-RL-Reinforcement-Learning-for-Weakly-Supervised-Prostate-Segmentation-in-TRUS-Images"><a href="#Boundary-RL-Reinforcement-Learning-for-Weakly-Supervised-Prostate-Segmentation-in-TRUS-Images" class="headerlink" title="Boundary-RL: Reinforcement Learning for Weakly-Supervised Prostate Segmentation in TRUS Images"></a>Boundary-RL: Reinforcement Learning for Weakly-Supervised Prostate Segmentation in TRUS Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11376">http://arxiv.org/abs/2308.11376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixi Yi, Vasilis Stavrinides, Zachary M. C. Baum, Qianye Yang, Dean C. Barratt, Matthew J. Clarkson, Yipeng Hu, Shaheer U. Saeed<br>for: 这个研究旨在提出一种弱度指导的类别分割方法，使用仅有单元标签进行训练，并且将类别分割看作是范围检测问题，而不是像前一些研究所使用的像素级别分类。methods: 这个方法使用了强化学习来训练一个控制函数，以寻找范围内的类别 bounding，使用一个从预训练的边界存在检测器获得的赏兹。results: 在评估这个方法的临床实际任务中，关于胆囊组织分类，我们获得了与其他已知的弱度指导方法相比，更好的表现，使用相同的标签，例如多个例问题学习。<details>
<summary>Abstract</summary>
We propose Boundary-RL, a novel weakly supervised segmentation method that utilises only patch-level labels for training. We envision the segmentation as a boundary detection problem, rather than a pixel-level classification as in previous works. This outlook on segmentation may allow for boundary delineation under challenging scenarios such as where noise artefacts may be present within the region-of-interest (ROI) boundaries, where traditional pixel-level classification-based weakly supervised methods may not be able to effectively segment the ROI. Particularly of interest, ultrasound images, where intensity values represent acoustic impedance differences between boundaries, may also benefit from the boundary delineation approach. Our method uses reinforcement learning to train a controller function to localise boundaries of ROIs using a reward derived from a pre-trained boundary-presence classifier. The classifier indicates when an object boundary is encountered within a patch, as the controller modifies the patch location in a sequential Markov decision process. The classifier itself is trained using only binary patch-level labels of object presence, which are the only labels used during training of the entire boundary delineation framework, and serves as a weak signal to inform the boundary delineation. The use of a controller function ensures that a sliding window over the entire image is not necessary. It also prevents possible false-positive or -negative cases by minimising number of patches passed to the boundary-presence classifier. We evaluate our proposed approach for a clinically relevant task of prostate gland segmentation on trans-rectal ultrasound images. We show improved performance compared to other tested weakly supervised methods, using the same labels e.g., multiple instance learning.
</details>
<details>
<summary>摘要</summary>
我们提出了Boundary-RL，一种新的弱类标注方法，只使用补丁级别标签进行训练。我们认为 segmentation 是一个边检测问题，而不是像前一些工作一样将每个像素分类为不同的类别。这种对 segmentation 的看法可能allow for 边界定义在具有噪声artifacts的区域内的场景中，其中传统的像素级别分类基于的弱类标注方法可能无法有效地分类区域。特别是，ultrasound 图像，其中 интенсивности值表示物体边界上的声学阻抗差异，也可能受惠于边界定义方法。我们的方法使用 reinforcement learning 训练一个控制函数，用于localize 区域内的边界。该控制函数通过一个Markov 决策过程来修改补丁位置，并且使用一个预训练的边界存在分类器来指示在补丁中是否遇到了物体边界。该分类器只使用补丁级别标签进行训练，这些标签也是训练整个边界定义框架的唯一标签。我们使用控制函数来避免使用滑块窗口覆盖整个图像，并且避免可能的 false-positive 或 false-negative 情况。我们对肾脏成像进行评估，并与其他测试的弱类标注方法进行比较。我们显示出我们的方法在评估中表现出色，使用相同的标签。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Interpretable-Object-Abstraction-via-Clustering-based-Slot-Initialization"><a href="#Enhancing-Interpretable-Object-Abstraction-via-Clustering-based-Slot-Initialization" class="headerlink" title="Enhancing Interpretable Object Abstraction via Clustering-based Slot Initialization"></a>Enhancing Interpretable Object Abstraction via Clustering-based Slot Initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11369">http://arxiv.org/abs/2308.11369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ning Gao, Bernard Hohmann, Gerhard Neumann</li>
<li>For: The paper is focused on improving object-centric representations using slots for efficient, flexible, and interpretable abstraction from low-level perceptual features in a compositional scene.* Methods: The paper proposes using clustering algorithms conditioned on perceptual input features to initialize the slot representations, and designs permutation invariant and permutation equivariant versions of this layer to enable exchangeable slot representations after clustering.* Results: The paper shows that its method outperforms prior works consistently, especially for complex scenes, through experiments on object discovery and novel view synthesis tasks with various datasets.<details>
<summary>Abstract</summary>
Object-centric representations using slots have shown the advances towards efficient, flexible and interpretable abstraction from low-level perceptual features in a compositional scene. Current approaches randomize the initial state of slots followed by an iterative refinement. As we show in this paper, the random slot initialization significantly affects the accuracy of the final slot prediction. Moreover, current approaches require a predetermined number of slots from prior knowledge of the data, which limits the applicability in the real world. In our work, we initialize the slot representations with clustering algorithms conditioned on the perceptual input features. This requires an additional layer in the architecture to initialize the slots given the identified clusters. We design permutation invariant and permutation equivariant versions of this layer to enable the exchangeable slot representations after clustering. Additionally, we employ mean-shift clustering to automatically identify the number of slots for a given scene. We evaluate our method on object discovery and novel view synthesis tasks with various datasets. The results show that our method outperforms prior works consistently, especially for complex scenes.
</details>
<details>
<summary>摘要</summary>
使用槽来表示对象已经取得了高效、灵活和可解释的抽象优势。现有方法在初始化槽时随机，然后进行迭代优化。我们在这篇论文中表明，随机槽初始化会对最终槽预测的准确性产生显著影响。此外，现有方法需要先知道数据中的槽数量，这限制了实际应用的可行性。在我们的工作中，我们使用基于感知输入特征的 clustering 算法来初始化槽表示。这需要一个额外的架构层来初始化槽给确定的群集。我们设计了卷积不变和卷积对称版本的这层，以便在 clustering 后换取可交换的槽表示。此外，我们使用 Mean-Shift  clustering 自动确定Scene中的槽数量。我们在对象发现和新视图合成任务中使用了多个数据集进行评估，结果表明，我们的方法在复杂场景下一直高于先前的方法。
</details></li>
</ul>
<hr>
<h2 id="Towards-Clip-Free-Quantized-Super-Resolution-Networks-How-to-Tame-Representative-Images"><a href="#Towards-Clip-Free-Quantized-Super-Resolution-Networks-How-to-Tame-Representative-Images" class="headerlink" title="Towards Clip-Free Quantized Super-Resolution Networks: How to Tame Representative Images"></a>Towards Clip-Free Quantized Super-Resolution Networks: How to Tame Representative Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11365">http://arxiv.org/abs/2308.11365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alperen Kalay, Bahri Batuhan Bilecen, Mustafa Ayazoglu</li>
<li>for: 这个研究旨在解决 SR 网络中训练后量化 (PTQ) 阶段的一个重要问题，即代表性数据集 (RD)。</li>
<li>methods: 我们提出了一个新的 clip-free 量化管道 (CFQP)，并提供了广泛的实验证明，以 cleverly 使用 FP32 模型的输出来增强 RD 图像。这种方法可以消除不必要的 clipped 活动层，从而提高整体稳定性、减少推理时间（最多下降到 54%）、提高视觉质量 Results  compared to INT8 clipped models，并在一些 SR 模型上甚至超越了不量化 FP32 模型。</li>
<li>results: 我们的方法可以在某些 SR 模型上提高视觉质量，同时减少推理时间，并且不需要重新训练 clipped activation。在一些情况下，我们的方法可以超越不量化 FP32 模型， both in runtime and visual quality。<details>
<summary>Abstract</summary>
Super-resolution (SR) networks have been investigated for a while, with their mobile and lightweight versions gaining noticeable popularity recently. Quantization, the procedure of decreasing the precision of network parameters (mostly FP32 to INT8), is also utilized in SR networks for establishing mobile compatibility. This study focuses on a very important but mostly overlooked post-training quantization (PTQ) step: representative dataset (RD), which adjusts the quantization range for PTQ. We propose a novel pipeline (clip-free quantization pipeline, CFQP) backed up with extensive experimental justifications to cleverly augment RD images by only using outputs of the FP32 model. Using the proposed pipeline for RD, we can successfully eliminate unwanted clipped activation layers, which nearly all mobile SR methods utilize to make the model more robust to PTQ in return for a large overhead in runtime. Removing clipped activations with our method significantly benefits overall increased stability, decreased inference runtime up to 54% on some SR models, better visual quality results compared to INT8 clipped models - and outperforms even some FP32 non-quantized models, both in runtime and visual quality, without the need for retraining with clipped activation.
</details>
<details>
<summary>摘要</summary>
超解像（SR）网络已经被研究了一段时间，其移动和轻量级版本在最近得到了关注。量化，减小网络参数的精度（主要是FP32到INT8），也在SR网络中使用，以实现移动兼容性。这项研究关注一个很重要但又多少被注意的后training量化（PTQ）步骤：代表数据集（RD），它调整PTQ的范围。我们提出一个新的批处理管道（clip-free quantization pipeline，CFQP），并提供了详细的实验证明，以智能地增强RD图像，只使用FP32模型的输出。使用我们的方法进行RD，可以成功地消除无用的clip activation层，这些层通常在移动SR方法中使用，以使模型更具 robustness to PTQ，但是带来了大量的运行时间开销。从我们的方法中消除clip activation可以获得更好的稳定性、下降到54%的推理时间、更好的视觉质量结果，并且超过了INT8clip模型，以及FP32非量化模型，无需重新训练clip activation。
</details></li>
</ul>
<hr>
<h2 id="Exemplar-Free-Continual-Transformer-with-Convolutions"><a href="#Exemplar-Free-Continual-Transformer-with-Convolutions" class="headerlink" title="Exemplar-Free Continual Transformer with Convolutions"></a>Exemplar-Free Continual Transformer with Convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11357">http://arxiv.org/abs/2308.11357</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anurag Roy, Vinay Kumar Verma, Sravan Voonna, Kripabandhu Ghosh, Saptarshi Ghosh, Abir Das</li>
<li>for: 这 paper 的目的是提出一种新的无例子（exemplar-free）的类&#x2F;任务逐步学习方法，不需要在测试时显式提供任务标识符（task identifier），并且不需要保留之前训练集。</li>
<li>methods: 该方法使用 transformer 架构，并通过重新权重 multi-head self-attention 层中的键、查询和值权重来实现类&#x2F;任务逐步学习。具体来说，通过 convolution 来重新权重这些权重，以便在每个任务上保持低的参数数量。此外，使用图像增强技术来预测任务，而无需在测试时显式提供任务标识符。</li>
<li>results: 实验结果表明，该方法可以在四个 benchmark 数据集上超越许多竞争方法，而且需要更少的参数。<details>
<summary>Abstract</summary>
Continual Learning (CL) involves training a machine learning model in a sequential manner to learn new information while retaining previously learned tasks without the presence of previous training data. Although there has been significant interest in CL, most recent CL approaches in computer vision have focused on convolutional architectures only. However, with the recent success of vision transformers, there is a need to explore their potential for CL. Although there have been some recent CL approaches for vision transformers, they either store training instances of previous tasks or require a task identifier during test time, which can be limiting. This paper proposes a new exemplar-free approach for class/task incremental learning called ConTraCon, which does not require task-id to be explicitly present during inference and avoids the need for storing previous training instances. The proposed approach leverages the transformer architecture and involves re-weighting the key, query, and value weights of the multi-head self-attention layers of a transformer trained on a similar task. The re-weighting is done using convolution, which enables the approach to maintain low parameter requirements per task. Additionally, an image augmentation-based entropic task identification approach is used to predict tasks without requiring task-ids during inference. Experiments on four benchmark datasets demonstrate that the proposed approach outperforms several competitive approaches while requiring fewer parameters.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine"><a href="#Integration-of-Sentinel-1-and-Sentinel-2-data-for-Earth-surface-classification-using-Machine-Learning-algorithms-implemented-on-Google-Earth-Engine" class="headerlink" title="Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine"></a>Integration of Sentinel-1 and Sentinel-2 data for Earth surface classification using Machine Learning algorithms implemented on Google Earth Engine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11340">http://arxiv.org/abs/2308.11340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesca Razzano, Mariapia Rita Iandolo, Chiara Zarro, G. S. Yogesh, Silvia Liberata Ullo</li>
<li>for: 本研究使用Synthetic Aperture Radar (SAR)和光学数据进行地面分类。</li>
<li>methods: 通过在Google Earth Engine (GEE)平台上实施监督式机器学习（ML）算法，将Sentinel-1 (S-1)和Sentinel-2 (S-2)数据集成起来，用于地面覆盖分类。</li>
<li>results: 研究结果表明，在这种情况下，雷达和光学远程探测提供了补偿信息，有利地面覆盖分类，通常导致映射精度的提高。此外，本研究也证明了GEE在处理大量卫星数据方面的emerging角色。<details>
<summary>Abstract</summary>
In this study, Synthetic Aperture Radar (SAR) and optical data are both considered for Earth surface classification. Specifically, the integration of Sentinel-1 (S-1) and Sentinel-2 (S-2) data is carried out through supervised Machine Learning (ML) algorithms implemented on the Google Earth Engine (GEE) platform for the classification of a particular region of interest. Achieved results demonstrate how in this case radar and optical remote detection provide complementary information, benefiting surface cover classification and generally leading to increased mapping accuracy. In addition, this paper works in the direction of proving the emerging role of GEE as an effective cloud-based tool for handling large amounts of satellite data.
</details>
<details>
<summary>摘要</summary>
在这项研究中，人造 aperature radar（SAR）和光学数据都被考虑用于地球表面分类。具体来说，Sentinel-1（S-1）和Sentinel-2（S-2）数据的集成通过在Google Earth Engine（GEE）平台上实施监督式机器学习（ML）算法进行地区特定的分类。实现的结果表明在这种情况下，雷达和光学远程探测提供了补做的信息，从而改善表面覆盖分类和通常提高地图准确性。此外，这篇论文还在irection towards proving the emerging role of GEE as an effective cloud-based tool for handling large amounts of satellite data.
</details></li>
</ul>
<hr>
<h2 id="Object-Detection-Difficulty-Suppressing-Over-aggregation-for-Faster-and-Better-Video-Object-Detection"><a href="#Object-Detection-Difficulty-Suppressing-Over-aggregation-for-Faster-and-Better-Video-Object-Detection" class="headerlink" title="Object Detection Difficulty: Suppressing Over-aggregation for Faster and Better Video Object Detection"></a>Object Detection Difficulty: Suppressing Over-aggregation for Faster and Better Video Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11327">http://arxiv.org/abs/2308.11327</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bingqingzhang/odd-vod">https://github.com/bingqingzhang/odd-vod</a></li>
<li>paper_authors: Bingqing Zhang, Sen Wang, Yifan Liu, Brano Kusy, Xue Li, Jiajun Liu</li>
<li>for: 提高视频对象检测（VOD）系统的实用性</li>
<li>methods: 提出一种图像级对象检测难度（ODD）指标，用于衡量检测图像中对象的难度，并在VOD过程中应用该指标来减少过度聚合。</li>
<li>results: 对8种VOD模型进行了广泛的实验，结果表明，当选择全局参照帧时，ODD-VOD可以不断提高全局基于VOD模型的准确率。当用于加速时，ODD-VOD可以不断提高帧数（FPS），并且不会降低准确率。<details>
<summary>Abstract</summary>
Current video object detection (VOD) models often encounter issues with over-aggregation due to redundant aggregation strategies, which perform feature aggregation on every frame. This results in suboptimal performance and increased computational complexity. In this work, we propose an image-level Object Detection Difficulty (ODD) metric to quantify the difficulty of detecting objects in a given image. The derived ODD scores can be used in the VOD process to mitigate over-aggregation. Specifically, we train an ODD predictor as an auxiliary head of a still-image object detector to compute the ODD score for each image based on the discrepancies between detection results and ground-truth bounding boxes. The ODD score enhances the VOD system in two ways: 1) it enables the VOD system to select superior global reference frames, thereby improving overall accuracy; and 2) it serves as an indicator in the newly designed ODD Scheduler to eliminate the aggregation of frames that are easy to detect, thus accelerating the VOD process. Comprehensive experiments demonstrate that, when utilized for selecting global reference frames, ODD-VOD consistently enhances the accuracy of Global-frame-based VOD models. When employed for acceleration, ODD-VOD consistently improves the frames per second (FPS) by an average of 73.3% across 8 different VOD models without sacrificing accuracy. When combined, ODD-VOD attains state-of-the-art performance when competing with many VOD methods in both accuracy and speed. Our work represents a significant advancement towards making VOD more practical for real-world applications.
</details>
<details>
<summary>摘要</summary>
当前的视频对象检测（VOD）模型经常遇到过度聚合的问题，这是因为 redundancy 的聚合策略在每帧进行特征聚合，从而导致性能下降和计算复杂性增加。在这种情况下，我们提出了一个图像级别的对象检测困难度（ODD）度量，用于衡量检测图像中对象的困难度。这些计算的ODD分数可以在 VOD 过程中使用，以避免过度聚合。我们在 VOD 系统中训练了一个 ODD 预测器，用于计算每幅图像的 ODD 分数，基于检测结果和真实 bounding box 之间的差异。ODD 分数可以在两种方式帮助 VOD 系统：1）选择更高精度的全局参照帧，以提高总体精度；2）作为 ODD 调度器中的指标，以消除容易检测的帧的聚合，从而加速 VOD 过程。我们的实验表明，当用于选择全局参照帧时，ODD-VOD 可以不断提高全球帧基本 VOD 模型的精度。当用于加速时，ODD-VOD 可以平均提高 FPS 值 by 73.3%，无需牺牲精度。当两者结合使用时，ODD-VOD 可以在精度和速度两个方面占据领先地位，代表了对 VOD 实际应用的一项重要进步。
</details></li>
</ul>
<hr>
<h2 id="CiteTracker-Correlating-Image-and-Text-for-Visual-Tracking"><a href="#CiteTracker-Correlating-Image-and-Text-for-Visual-Tracking" class="headerlink" title="CiteTracker: Correlating Image and Text for Visual Tracking"></a>CiteTracker: Correlating Image and Text for Visual Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11322">http://arxiv.org/abs/2308.11322</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/NorahGreen/CiteTracker">https://github.com/NorahGreen/CiteTracker</a></li>
<li>paper_authors: Xin Li, Yuqing Huang, Zhenyu He, Yaowei Wang, Huchuan Lu, Ming-Hsuan Yang</li>
<li>for: 提高视觉跟踪中目标模型和推理的精度，使得在目标呈现大幅变化时仍能准确跟踪。</li>
<li>methods: 提出了一种基于图像和文本的目标跟踪方法，通过图像转文本模块将目标图像区域转换为描述性文本，并通过动态描述模块适应目标变化以提高跟踪精度。</li>
<li>results: 经过了五种不同的数据集测试，并与现有方法进行比较，研究发现提出的跟踪方法在跟踪目标呈现大幅变化时表现出了优于现有方法的性能。<details>
<summary>Abstract</summary>
Existing visual tracking methods typically take an image patch as the reference of the target to perform tracking. However, a single image patch cannot provide a complete and precise concept of the target object as images are limited in their ability to abstract and can be ambiguous, which makes it difficult to track targets with drastic variations. In this paper, we propose the CiteTracker to enhance target modeling and inference in visual tracking by connecting images and text. Specifically, we develop a text generation module to convert the target image patch into a descriptive text containing its class and attribute information, providing a comprehensive reference point for the target. In addition, a dynamic description module is designed to adapt to target variations for more effective target representation. We then associate the target description and the search image using an attention-based correlation module to generate the correlated features for target state reference. Extensive experiments on five diverse datasets are conducted to evaluate the proposed algorithm and the favorable performance against the state-of-the-art methods demonstrates the effectiveness of the proposed tracking method.
</details>
<details>
<summary>摘要</summary>
现有的视觉跟踪方法通常使用图像块作为目标的参考点进行跟踪。然而，单个图像块无法提供完整和准确的目标对象概念，因为图像有限制，容易受到歧义和变化的影响，这使得跟踪目标变化具有挑战性。在这篇论文中，我们提出了CiteTracker，用于增强视觉跟踪中目标模型化和推理的方法。具体来说，我们开发了一个文本生成模块，将目标图像块转换为包含类和特征信息的详细文本描述，为目标提供了全面的参考点。此外，我们还设计了一个动态描述模块，以适应目标变化，以更有效地表示目标。然后，我们使用关注机制来关联目标描述和搜索图像，生成相关特征，用于目标状态参考。我们对五种不同的数据集进行了广泛的实验，以评估提出的算法效果。结果表明，与现有方法相比，我们的跟踪方法具有优秀的效果。
</details></li>
</ul>
<hr>
<h2 id="Using-and-Abusing-Equivariance"><a href="#Using-and-Abusing-Equivariance" class="headerlink" title="Using and Abusing Equivariance"></a>Using and Abusing Equivariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11316">http://arxiv.org/abs/2308.11316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tom Edixhoven, Attila Lengyel, Jan van Gemert</li>
<li>for: 学习破坏对称性的群同态卷积神经网络</li>
<li>methods: 使用抽样来学习破坏对称性，对2D旋转和反射进行研究</li>
<li>results: 发现小变化的输入维度可以使通用的网络变得相对均匀，而不是精确均匀；研究不同对称性的网络如何影响其性能，并发现在训练数据中的对称性与网络的对称性不同时，相对均匀网络可以放弃自己的对称性约束，与精确均匀网络匹配或超越它们在常用 benchmark 数据上。<details>
<summary>Abstract</summary>
In this paper we show how Group Equivariant Convolutional Neural Networks use subsampling to learn to break equivariance to their symmetries. We focus on 2D rotations and reflections and investigate the impact of broken equivariance on network performance. We show that a change in the input dimension of a network as small as a single pixel can be enough for commonly used architectures to become approximately equivariant, rather than exactly. We investigate the impact of networks not being exactly equivariant and find that approximately equivariant networks generalise significantly worse to unseen symmetries compared to their exactly equivariant counterparts. However, when the symmetries in the training data are not identical to the symmetries of the network, we find that approximately equivariant networks are able to relax their own equivariant constraints, causing them to match or outperform exactly equivariant networks on common benchmark datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了GROUP等变征 convolutional neural networks 如何通过抽象来学习破坏对其Symmetries的等变征性。我们关注了2D旋转和反射，并调查了不等变征性对网络性能的影响。我们发现，只需将网络输入维度改变一个像素，常用的架构就可以变得相对等变征了，而不是精确等变征。我们调查了不等变征网络在未seen Symmetries上的generalization情况，发现它们与等变征网络相比在Common benchmark datasets上匹配或超越。但当网络中的Symmetries与训练数据中的Symmetries不同时，我们发现approximately等变征网络能够放弃自己的等变征约束，使其与等变征网络在Common benchmark datasets上匹配或超越。
</details></li>
</ul>
<hr>
<h2 id="Approaching-human-3D-shape-perception-with-neurally-mappable-models"><a href="#Approaching-human-3D-shape-perception-with-neurally-mappable-models" class="headerlink" title="Approaching human 3D shape perception with neurally mappable models"></a>Approaching human 3D shape perception with neurally mappable models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11300">http://arxiv.org/abs/2308.11300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas P. O’Connell, Tyler Bonnen, Yoni Friedman, Ayush Tewari, Josh B. Tenenbaum, Vincent Sitzmann, Nancy Kanwisher</li>
<li>for: 这研究旨在理解人类如何自然地推测三维形状，以及这种能力是如何被计算机模型重建的？</li>
<li>methods: 研究使用了一种新的计算模型，即3D神经场（3D-LFN），该模型基于深度神经网络（DNN），并通过多视角训练和多视角学习目标来实现人类水平的性能。</li>
<li>results: 研究发现，3D-LFN支持人类水平的三维匹配判断，并在针对标准DNN模型的攻击性定义比较中表现出色。此外，研究还发现，通过多视角训练和多视角学习目标，even conventional DNN architectures可以更接近人类行为。但是，这些模型在处理新的物体类别时仍有所限制。<details>
<summary>Abstract</summary>
Humans effortlessly infer the 3D shape of objects. What computations underlie this ability? Although various computational models have been proposed, none of them capture the human ability to match object shape across viewpoints. Here, we ask whether and how this gap might be closed. We begin with a relatively novel class of computational models, 3D neural fields, which encapsulate the basic principles of classic analysis-by-synthesis in a deep neural network (DNN). First, we find that a 3D Light Field Network (3D-LFN) supports 3D matching judgments well aligned to humans for within-category comparisons, adversarially-defined comparisons that accentuate the 3D failure cases of standard DNN models, and adversarially-defined comparisons for algorithmically generated shapes with no category structure. We then investigate the source of the 3D-LFN's ability to achieve human-aligned performance through a series of computational experiments. Exposure to multiple viewpoints of objects during training and a multi-view learning objective are the primary factors behind model-human alignment; even conventional DNN architectures come much closer to human behavior when trained with multi-view objectives. Finally, we find that while the models trained with multi-view learning objectives are able to partially generalize to new object categories, they fall short of human alignment. This work provides a foundation for understanding human shape inferences within neurally mappable computational architectures and highlights important questions for future work.
</details>
<details>
<summary>摘要</summary>
人类努力地推断物体的3D形状。这些计算是如何进行的？虽然一些计算模型已经被提出，但None of them capture the human ability to match object shape across viewpoints。在这里，我们问whether and how this gap might be closed。我们开始于一种相对新的计算模型，3D神经场（3D-LFN），这个模型涵盖了经典分析synthesis的基本原理，并将其 embedding在深度神经网络（DNN）中。我们发现，3D-LFN支持3D匹配判断，与人类匹配的性能很高，包括在类别内的比较、对抗定义的比较和对算法生成的形状进行比较。然后，我们通过一系列计算实验来研究3D-LFN的能力是如何实现人类对应的性能的。我们发现，在训练中 expose to multiple viewpoints of objects和多视图学习目标是模型人类匹配的主要因素。甚至使用标准DNN架构，通过多视图学习目标进行训练，模型的性能就会更接近人类的行为。最后，我们发现，使用多视图学习目标训练的模型可以部分地泛化到新的物体类别，但是它们仍然不够接近人类的匹配。这个研究提供了人类形状推断在神经Mappable的计算架构中的基础，并高亮了未来工作的重要问题。
</details></li>
</ul>
<hr>
<h2 id="BHSD-A-3D-Multi-Class-Brain-Hemorrhage-Segmentation-Dataset"><a href="#BHSD-A-3D-Multi-Class-Brain-Hemorrhage-Segmentation-Dataset" class="headerlink" title="BHSD: A 3D Multi-Class Brain Hemorrhage Segmentation Dataset"></a>BHSD: A 3D Multi-Class Brain Hemorrhage Segmentation Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11298">http://arxiv.org/abs/2308.11298</a></li>
<li>repo_url: None</li>
<li>paper_authors: Biao Wu, Yutong Xie, Zeyu Zhang, Jinchao Ge, Kaspar Yaxley, Suzan Bahadir, Qi Wu, Yifan Liu, Minh-Son To</li>
<li>for: 本研究旨在提供一个3D多类血肿 segmentation dataset（BHSD），以便为血肿 segmentation任务提供支持。</li>
<li>methods: 本研究使用了深度学习技术来进行 médical image segmentation，并应用于血肿 segmentation任务。</li>
<li>results: 本研究提供了一个包含192个Volume的多类血肿数据集，以及2200个slice-level标注的数据集。通过对这些数据集进行supervised和semi-supervisedsegmentation任务的实验，我们证明了数据集的实用性。<details>
<summary>Abstract</summary>
Intracranial hemorrhage (ICH) is a pathological condition characterized by bleeding inside the skull or brain, which can be attributed to various factors. Identifying, localizing and quantifying ICH has important clinical implications, in a bleed-dependent manner. While deep learning techniques are widely used in medical image segmentation and have been applied to the ICH segmentation task, existing public ICH datasets do not support the multi-class segmentation problem. To address this, we develop the Brain Hemorrhage Segmentation Dataset (BHSD), which provides a 3D multi-class ICH dataset containing 192 volumes with pixel-level annotations and 2200 volumes with slice-level annotations across five categories of ICH. To demonstrate the utility of the dataset, we formulate a series of supervised and semi-supervised ICH segmentation tasks. We provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset.
</details>
<details>
<summary>摘要</summary>
Intracranial hemorrhage (ICH) 是一种生物学条件，表示脑内或脑部内出血，这可以归因于多种因素。正确地识别、定位和评估ICH具有重要临床意义，具体取决于出血情况。although deep learning techniques have been widely used in medical image segmentation and have been applied to the ICH segmentation task, existing public ICH datasets do not support the multi-class segmentation problem. To address this, we develop the Brain Hemorrhage Segmentation Dataset (BHSD), which provides a 3D multi-class ICH dataset containing 192 volumes with pixel-level annotations and 2200 volumes with slice-level annotations across five categories of ICH. To demonstrate the utility of the dataset, we formulate a series of supervised and semi-supervised ICH segmentation tasks. We provide experimental results with state-of-the-art models as reference benchmarks for further model developments and evaluations on this dataset.Here's the word-for-word translation:Intracranial hemorrhage (ICH) 是一种生物学条件，表示脑内或脑部内出血，这可以归因于多种因素。正确地识别、定位和评估ICH具有重要临床意义，具体取决于出血情况。虽然深度学习技术已经广泛应用于医疗图像分割，并已经应用于ICH分割任务，但现有的公共ICH数据集不支持多类分割问题。为解决这个问题，我们开发了脑出血分割数据集（BHSD），该数据集包含192卷Pixel级别标注和2200卷Slice级别标注，涵盖五类ICH。为证明数据集的实用性，我们提出了一系列的超级vised和半监督ICH分割任务。我们提供了实验结果，作为参考标准模型，以便进一步的模型开发和评估。
</details></li>
</ul>
<hr>
<h2 id="PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction"><a href="#PCMC-T1-Free-breathing-myocardial-T1-mapping-with-Physically-Constrained-Motion-Correction" class="headerlink" title="PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction"></a>PCMC-T1: Free-breathing myocardial T1 mapping with Physically-Constrained Motion Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11281">http://arxiv.org/abs/2308.11281</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eyal Hanania, Ilya Volovik, Lilach Barkat, Israel Cohen, Moti Freiman</li>
<li>for: The paper is focused on developing a deep-learning-based method for motion correction in free-breathing T1 mapping, which can improve the accuracy and accessibility of diffuse myocardial disease diagnosis.</li>
<li>methods: The proposed method, called PCMC-T1, incorporates a physically-constrained signal decay model into a deep-learning network to correct for motion artifacts in free-breathing T1 mapping.</li>
<li>results: PCMC-T1 was compared to baseline methods using a 5-fold experimental setup on a public dataset of 210 patients and demonstrated superior model fitting quality and clinical impact, with anatomical alignment results that were comparable to the baseline methods.<details>
<summary>Abstract</summary>
T1 mapping is a quantitative magnetic resonance imaging (qMRI) technique that has emerged as a valuable tool in the diagnosis of diffuse myocardial diseases. However, prevailing approaches have relied heavily on breath-hold sequences to eliminate respiratory motion artifacts. This limitation hinders accessibility and effectiveness for patients who cannot tolerate breath-holding. Image registration can be used to enable free-breathing T1 mapping. Yet, inherent intensity differences between the different time points make the registration task challenging. We introduce PCMC-T1, a physically-constrained deep-learning model for motion correction in free-breathing T1 mapping. We incorporate the signal decay model into the network architecture to encourage physically-plausible deformations along the longitudinal relaxation axis. We compared PCMC-T1 to baseline deep-learning-based image registration approaches using a 5-fold experimental setup on a publicly available dataset of 210 patients. PCMC-T1 demonstrated superior model fitting quality (R2: 0.955) and achieved the highest clinical impact (clinical score: 3.93) compared to baseline methods (0.941, 0.946 and 3.34, 3.62 respectively). Anatomical alignment results were comparable (Dice score: 0.9835 vs. 0.984, 0.988). Our code and trained models are available at https://github.com/eyalhana/PCMC-T1.
</details>
<details>
<summary>摘要</summary>
T1映射是一种量化核磁共振成像（qMRI）技术，已经成为诊断散布性心肺疾病的有价值工具。然而，现有的方法几乎完全依赖呼吸停止序列来消除呼吸运动 artifacts。这限制了患者的访问和效果，特别是那些无法忍受呼吸停止的患者。图像 регистраción可以使得呼吸自由T1映射成为可能。然而，内在的时刻点不同INTENSITY带来了注册任务的挑战。我们介绍PCMC-T1，一种基于物理约束的深度学习模型，用于呼吸自由T1映射中的运动 corrections。我们将信号衰减模型integrated into网络架构，以便鼓励物理可能的扭轴运动。我们与基线方法进行比较，使用一个5-fold实验设置，在公共可用的210名患者数据集上进行了测试。PCMC-T1显示出了最高的模型适应质量（R2: 0.955）和最高的临床影响（临床分数：3.93），与基线方法（0.941、0.946和3.34、3.62分别）相比。结构对应结果相似（Dice分数：0.9835 vs. 0.984、0.988）。我们的代码和训练模型可以在https://github.com/eyalhana/PCMC-T1中获得。
</details></li>
</ul>
<hr>
<h2 id="HMD-NeMo-Online-3D-Avatar-Motion-Generation-From-Sparse-Observations"><a href="#HMD-NeMo-Online-3D-Avatar-Motion-Generation-From-Sparse-Observations" class="headerlink" title="HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations"></a>HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11261">http://arxiv.org/abs/2308.11261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker</li>
<li>for: 这个论文的目的是提出一种能够生成正确和可信的全身动作，即使只有部分手部可见，以提高混合现实场景中的吸引力。</li>
<li>methods: 该论文使用了一种名为HMD-NeMo的轻量级神经网络，通过在线和实时方式预测全身动作，并使用了新的时间适应mask токен来促进合理的动作在手部不可见情况下。</li>
<li>results: 经过了广泛的分析和评估，该论文在AMASS数据集上达到了新的状态元。<details>
<summary>Abstract</summary>
Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved impressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visibility. While this is the case when, e.g., using motion controllers, a considerable proportion of mixed reality experiences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified approach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask tokens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>创造真实和准确的全身人物动作是混合现实场景中质量体验的关键。头戴设备（HMD）通常只提供头和手6个自由度的输入信号。近期，不同的方法已经实现了在只有头和手信号的情况下取得了出色的表现。然而，根据我们所知，所有现有的方法均依赖于全手可见。这会导致在使用动作控制器时是可见的，但是一部分混合现实经验不使用动作控制器，而是基于自己центric的手姿跟踪。这引入了只有部分手可见的挑战，即HMD的视野限制。在这篇论文中，我们提出了第一个统一的方法，即HMD-NeMo，它可以在线和实时方式下生成真实和准确的全身动作。HMD-NeMo是一个轻量级的神经网络，它通过在线和实时方式下预测全身动作来解决部分手可见的挑战。我们对HMD-NeMo中不同组件的影响进行了广泛的分析，并通过我们的评估而提出了新的状态码。
</details></li>
</ul>
<hr>
<h2 id="Video-BagNet-short-temporal-receptive-fields-increase-robustness-in-long-term-action-recognition"><a href="#Video-BagNet-short-temporal-receptive-fields-increase-robustness-in-long-term-action-recognition" class="headerlink" title="Video BagNet: short temporal receptive fields increase robustness in long-term action recognition"></a>Video BagNet: short temporal receptive fields increase robustness in long-term action recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11249">http://arxiv.org/abs/2308.11249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ombretta/videobagnet">https://github.com/ombretta/videobagnet</a></li>
<li>paper_authors: Ombretta Strafforello, Xin Liu, Klamer Schutte, Jan van Gemert</li>
<li>for: 提高视频动作识别模型的 robustness，使其能够更好地承受视频中的子动作顺序变化。</li>
<li>methods: 对于现有的深度3D convolutional模型，我们采用了限制时间响应领域大小的方法，从而实现了模型的时间响应领域的缩小。</li>
<li>results: 我们在 sintetic 和 real-world 视频数据集上进行了实验，发现短时间响应领域的模型具有较高的 robustness，而大于17帧的时间响应领域模型具有较低的 robustness。<details>
<summary>Abstract</summary>
Previous work on long-term video action recognition relies on deep 3D-convolutional models that have a large temporal receptive field (RF). We argue that these models are not always the best choice for temporal modeling in videos. A large temporal receptive field allows the model to encode the exact sub-action order of a video, which causes a performance decrease when testing videos have a different sub-action order. In this work, we investigate whether we can improve the model robustness to the sub-action order by shrinking the temporal receptive field of action recognition models. For this, we design Video BagNet, a variant of the 3D ResNet-50 model with the temporal receptive field size limited to 1, 9, 17 or 33 frames. We analyze Video BagNet on synthetic and real-world video datasets and experimentally compare models with varying temporal receptive fields. We find that short receptive fields are robust to sub-action order changes, while larger temporal receptive fields are sensitive to the sub-action order.
</details>
<details>
<summary>摘要</summary>
previous research on long-term video action recognition relies on deep 3D-convolutional models with a large temporal receptive field (RF). we argue that these models are not always the best choice for temporal modeling in videos. a large temporal receptive field allows the model to encode the exact sub-action order of a video, which causes a performance decrease when testing videos have a different sub-action order. in this work, we investigate whether we can improve the model robustness to the sub-action order by shrinking the temporal receptive field of action recognition models. for this, we design video bagnet, a variant of the 3D ResNet-50 model with the temporal receptive field size limited to 1, 9, 17 or 33 frames. we analyze video bagnet on synthetic and real-world video datasets and experimentally compare models with varying temporal receptive fields. we find that short receptive fields are robust to sub-action order changes, while larger temporal receptive fields are sensitive to the sub-action order.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Are-current-long-term-video-understanding-datasets-long-term"><a href="#Are-current-long-term-video-understanding-datasets-long-term" class="headerlink" title="Are current long-term video understanding datasets long-term?"></a>Are current long-term video understanding datasets long-term?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11244">http://arxiv.org/abs/2308.11244</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ombretta/longterm_datasets">https://github.com/ombretta/longterm_datasets</a></li>
<li>paper_authors: Ombretta Strafforello, Klamer Schutte, Jan van Gemert</li>
<li>for: This paper aims to evaluate the suitability of video datasets for long-term action recognition.</li>
<li>methods: The proposed method defines long-term actions as those that cannot be recognized using solely short-term information, and tests this definition on three popular real-world datasets.</li>
<li>results: The study finds that the existing datasets can be effectively solved using shortcuts based on short-term information, and encourages researchers to use datasets that require long-term information to be solved.Here’s the simplified Chinese text for the three pieces of information:</li>
<li>for: 这篇论文目的是评估视频数据集是否适用于长期动作识别。</li>
<li>methods: 该方法定义长期动作为不能通过短期信息alone来识别的动作，并对三个实际世界数据集进行测试。</li>
<li>results: 研究发现现有数据集可以使用短期信息 shortcut 进行解决，并促使研究人员使用需要长期信息来解决的数据集。<details>
<summary>Abstract</summary>
Many real-world applications, from sport analysis to surveillance, benefit from automatic long-term action recognition. In the current deep learning paradigm for automatic action recognition, it is imperative that models are trained and tested on datasets and tasks that evaluate if such models actually learn and reason over long-term information. In this work, we propose a method to evaluate how suitable a video dataset is to evaluate models for long-term action recognition. To this end, we define a long-term action as excluding all the videos that can be correctly recognized using solely short-term information. We test this definition on existing long-term classification tasks on three popular real-world datasets, namely Breakfast, CrossTask and LVU, to determine if these datasets are truly evaluating long-term recognition. Our study reveals that these datasets can be effectively solved using shortcuts based on short-term information. Following this finding, we encourage long-term action recognition researchers to make use of datasets that need long-term information to be solved.
</details>
<details>
<summary>摘要</summary>
很多现实世界应用，从运动分析到监测，都会受益于自动长期动作识别。在当前的深度学习框架中，自动动作识别模型的训练和测试通常是基于长期信息的。在这种情况下，我们提议一种方法来评估视频集是否适用于评估长期动作识别模型。为此，我们定义长期动作为排除所有可以通过短期信息来正确地识别的视频。我们在三个流行的实际世界数据集上进行测试，分别是Breakfast、CrossTask和LVU，以确定这些数据集是否真的评估长期认知。我们的研究发现，这些数据集可以通过快捷途径基于短期信息来解决。根据这个发现，我们鼓励长期动作识别研究人员使用需要长期信息来解决的数据集。
</details></li>
</ul>
<hr>
<h2 id="LOCATE-Self-supervised-Object-Discovery-via-Flow-guided-Graph-cut-and-Bootstrapped-Self-training"><a href="#LOCATE-Self-supervised-Object-Discovery-via-Flow-guided-Graph-cut-and-Bootstrapped-Self-training" class="headerlink" title="LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training"></a>LOCATE: Self-supervised Object Discovery via Flow-guided Graph-cut and Bootstrapped Self-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11239">http://arxiv.org/abs/2308.11239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silky Singh, Shripad Deshmukh, Mausoom Sarkar, Balaji Krishnamurthy</li>
<li>for: 本研究旨在无需人工监督下完成图像和视频数据集中的对象分割问题。</li>
<li>methods: 我们提出了一种自动化对象发现方法，利用运动和外观信息来生成高质量的对象分割面积。我们在传统图像树剖中添加了运动信息，并与外观信息进行线性组合来生成边权。</li>
<li>results: 我们的方法在多个标准视频对象分割、图像吸引力检测和对象分割 benchmark 上达到了与现状对照的性能。我们还通过自我训练来进一步提高性能。在审查实验中，我们的方法在未知领域中的转移性也得到了证明。<details>
<summary>Abstract</summary>
Learning object segmentation in image and video datasets without human supervision is a challenging problem. Humans easily identify moving salient objects in videos using the gestalt principle of common fate, which suggests that what moves together belongs together. Building upon this idea, we propose a self-supervised object discovery approach that leverages motion and appearance information to produce high-quality object segmentation masks. Specifically, we redesign the traditional graph cut on images to include motion information in a linear combination with appearance information to produce edge weights. Remarkably, this step produces object segmentation masks comparable to the current state-of-the-art on multiple benchmarks. To further improve performance, we bootstrap a segmentation network trained on these preliminary masks as pseudo-ground truths to learn from its own outputs via self-training. We demonstrate the effectiveness of our approach, named LOCATE, on multiple standard video object segmentation, image saliency detection, and object segmentation benchmarks, achieving results on par with and, in many cases surpassing state-of-the-art methods. We also demonstrate the transferability of our approach to novel domains through a qualitative study on in-the-wild images. Additionally, we present extensive ablation analysis to support our design choices and highlight the contribution of each component of our proposed method.
</details>
<details>
<summary>摘要</summary>
学习图像和视频集合中的对象分割无人监督是一个具有挑战性的问题。人类容易通过gestalt原则的共同命运来识别视频中移动的焦点对象，这个原则表明移动 вместе的对象属于同一个集合。基于这个想法，我们提出了一种自动化对象发现方法，利用运动和外观信息生成高质量的对象分割面积。 Specifically,我们重新设计了传统的图像截割方法，在线性组合中包括运动信息和外观信息来生成边重量。这一步生成的对象分割面积与当前状态的各种标准测试 benchmark 相当。为了进一步提高性能，我们使用这些初步的面积作为 pseudo-ground truth 来自我准备一个 segmentation 网络，并通过自我训练来学习自己的输出。我们命名这种方法为 LOCATE，并在多个标准视频对象分割、图像焦点检测和对象分割 bencmarks 上实现了与和超越当前状态的方法。我们还进行了质量研究，以证明我们的方法在新领域中的传输性。此外，我们还提供了广泛的减少分析，以支持我们的设计选择，并高亮每个方法的贡献。
</details></li>
</ul>
<hr>
<h2 id="Affordance-segmentation-of-hand-occluded-containers-from-exocentric-images"><a href="#Affordance-segmentation-of-hand-occluded-containers-from-exocentric-images" class="headerlink" title="Affordance segmentation of hand-occluded containers from exocentric images"></a>Affordance segmentation of hand-occluded containers from exocentric images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11233">http://arxiv.org/abs/2308.11233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Apicella, Alessio Xompero, Edoardo Ragusa, Riccardo Berta, Andrea Cavallaro, Paolo Gastaldo</li>
<li>for: 本研究旨在解决手持物体上 occlusion 问题，提高机器人可视且抓取物体的可行性。</li>
<li>methods: 提议的模型使用辅助分支处理物体和手部分 separately，学习受 occlusion 影响的可行特征。</li>
<li>results: 实验表明，我们的模型在实际和混合现实图像上具有更好的可行分割和泛化能力，比现有模型更好。<details>
<summary>Abstract</summary>
Visual affordance segmentation identifies the surfaces of an object an agent can interact with. Common challenges for the identification of affordances are the variety of the geometry and physical properties of these surfaces as well as occlusions. In this paper, we focus on occlusions of an object that is hand-held by a person manipulating it. To address this challenge, we propose an affordance segmentation model that uses auxiliary branches to process the object and hand regions separately. The proposed model learns affordance features under hand-occlusion by weighting the feature map through hand and object segmentation. To train the model, we annotated the visual affordances of an existing dataset with mixed-reality images of hand-held containers in third-person (exocentric) images. Experiments on both real and mixed-reality images show that our model achieves better affordance segmentation and generalisation than existing models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Visual affordance segmentation 可以识别物体上可互动的表面。常见的挑战是物体表面的多样性和物理特性以及遮挡。在这篇论文中，我们专注于人手持物体时的遮挡问题。为解决这个挑战，我们提议一种基于辅助分支的可互动分割模型。该模型通过对手和物体区域进行分割来学习可互动特征。通过对手和物体分割来权重特征图，以便在手遮挡情况下学习可互动特征。我们使用混合现实图像的混合现实数据集进行训练。实验表明，我们的模型在实际和混合现实图像上的可互动分割和泛化性比较好。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="LDP-Feat-Image-Features-with-Local-Differential-Privacy"><a href="#LDP-Feat-Image-Features-with-Local-Differential-Privacy" class="headerlink" title="LDP-Feat: Image Features with Local Differential Privacy"></a>LDP-Feat: Image Features with Local Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11223">http://arxiv.org/abs/2308.11223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Pittaluga, Bingbing Zhuang</li>
<li>for: 保护隐私，防止恶意攻击者通过图像特征恢复原始图像</li>
<li>methods: 使用嵌入式法律空间和对抗性特征样本来隐藏图像特征，并提出了两种新的倒转攻击来缓解隐私风险</li>
<li>results: 实现了在保护隐私的情况下具有强大的视觉本地化性能，并提供了一种基于地方散布隐私的方法，这种方法可以提供固定的隐私泄露 bound，不受攻击强度的影响<details>
<summary>Abstract</summary>
Modern computer vision services often require users to share raw feature descriptors with an untrusted server. This presents an inherent privacy risk, as raw descriptors may be used to recover the source images from which they were extracted. To address this issue, researchers recently proposed privatizing image features by embedding them within an affine subspace containing the original feature as well as adversarial feature samples. In this paper, we propose two novel inversion attacks to show that it is possible to (approximately) recover the original image features from these embeddings, allowing us to recover privacy-critical image content. In light of such successes and the lack of theoretical privacy guarantees afforded by existing visual privacy methods, we further propose the first method to privatize image features via local differential privacy, which, unlike prior approaches, provides a guaranteed bound for privacy leakage regardless of the strength of the attacks. In addition, our method yields strong performance in visual localization as a downstream task while enjoying the privacy guarantee.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉服务经常需要用户将原始特征描述分发到不可信服务器。这会导致隐私风险，因为原始特征可能可以用来恢复源图像。为解决这问题，研究人员最近提出了嵌入图像特征的私有化方法，以确保图像内容的隐私。在这篇论文中，我们提出了两种新的反向攻击，以示它们可以（相对）回归原始图像特征，从而恢复隐私关键的图像内容。另外，我们还提出了首个基于本地差分隐私的图像特征隐私方法，与先前的方法不同，它提供了隐私泄露的保证，无论攻击者的强度如何。此外，我们的方法在视觉本地化任务中具有强表现力，同时享有隐私保证。
</details></li>
</ul>
<hr>
<h2 id="DiffCloth-Diffusion-Based-Garment-Synthesis-and-Manipulation-via-Structural-Cross-modal-Semantic-Alignment"><a href="#DiffCloth-Diffusion-Based-Garment-Synthesis-and-Manipulation-via-Structural-Cross-modal-Semantic-Alignment" class="headerlink" title="DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment"></a>DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11206">http://arxiv.org/abs/2308.11206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xujie Zhang, Binbin Yang, Michael C. Kampffmeyer, Wenqing Zhang, Shiyue Zhang, Guansong Lu, Liang Lin, Hang Xu, Xiaodan Liang</li>
<li>for: 这个论文旨在提高模式融合和修改的方式，帮助时尚设计师更加方便地生成和修改他们的设计。</li>
<li>methods: 这个论文使用了DiffCloth，一种扩展了Diffusion-based管道，通过strucurally aligning cross-modal semantics来强化 diffusion models的可 composeability。</li>
<li>results:  experiments on CM-Fashion benchmark demonstrate that DiffCloth can both yield state-of-the-art garment synthesis results and support flexible manipulation with region consistency.<details>
<summary>Abstract</summary>
Cross-modal garment synthesis and manipulation will significantly benefit the way fashion designers generate garments and modify their designs via flexible linguistic interfaces.Current approaches follow the general text-to-image paradigm and mine cross-modal relations via simple cross-attention modules, neglecting the structural correspondence between visual and textual representations in the fashion design domain. In this work, we instead introduce DiffCloth, a diffusion-based pipeline for cross-modal garment synthesis and manipulation, which empowers diffusion models with flexible compositionality in the fashion domain by structurally aligning the cross-modal semantics. Specifically, we formulate the part-level cross-modal alignment as a bipartite matching problem between the linguistic Attribute-Phrases (AP) and the visual garment parts which are obtained via constituency parsing and semantic segmentation, respectively. To mitigate the issue of attribute confusion, we further propose a semantic-bundled cross-attention to preserve the spatial structure similarities between the attention maps of attribute adjectives and part nouns in each AP. Moreover, DiffCloth allows for manipulation of the generated results by simply replacing APs in the text prompts. The manipulation-irrelevant regions are recognized by blended masks obtained from the bundled attention maps of the APs and kept unchanged. Extensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth both yields state-of-the-art garment synthesis results by leveraging the inherent structural information and supports flexible manipulation with region consistency.
</details>
<details>
<summary>摘要</summary>
cross-modal 服装合成和修改将会对时尚设计师如何生成服装和修改他们的设计进行深见改进，通过灵活的语言接口。目前的方法采用通用的文本到图像模式，通过简单的跨模态关系抽象模块，忽略了时尚设计领域中的视觉表示结构匹配。在这个工作中，我们发展了DiffCloth，一种基于扩散的渠道管道，用于跨模态服装合成和修改，具有时尚领域的可 compose 性。具体来说，我们将部级跨模态匹配问题定义为语言特征短语（AP）和视觉服装部分之间的对应问题，并通过分词分析和 semantic segmentation 获得视觉服装部分。为了解决特征混淆问题，我们还提出了一种 semantic-bundled 跨注意力，以保持每个AP的注意力地图之间的空间结构相似性。此外，DiffCloth 支持通过简单地更换文本提示中的AP来进行 manipulate 操作，并且识别并保持不变的混合mask。广泛的实验表明，DiffCloth 可以利用内置的结构信息，同时支持灵活的 manipulate 操作，并保持区域一致性。
</details></li>
</ul>
<hr>
<h2 id="Masked-Cross-image-Encoding-for-Few-shot-Segmentation"><a href="#Masked-Cross-image-Encoding-for-Few-shot-Segmentation" class="headerlink" title="Masked Cross-image Encoding for Few-shot Segmentation"></a>Masked Cross-image Encoding for Few-shot Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11201">http://arxiv.org/abs/2308.11201</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenbo Xu, Huaxi Huang, Ming Cheng, Litao Yu, Qiang Wu, Jian Zhang</li>
<li>for: 这个论文旨在提高几张支持图像上的几个类别的描述，使用少量标注图像进行推断。</li>
<li>methods: 该方法使用Masked Cross-Image Encoding（MCE）来捕捉对象细节的共同视觉特征，以及图像之间的相互依赖关系。</li>
<li>results: 实验表明，该方法在PASCAL-$5^i$和COCO-$20^i$中表现出色，可以快速学习新类别，并且对于描述对象细节的任务有进一步的改进。<details>
<summary>Abstract</summary>
Few-shot segmentation (FSS) is a dense prediction task that aims to infer the pixel-wise labels of unseen classes using only a limited number of annotated images. The key challenge in FSS is to classify the labels of query pixels using class prototypes learned from the few labeled support exemplars. Prior approaches to FSS have typically focused on learning class-wise descriptors independently from support images, thereby ignoring the rich contextual information and mutual dependencies among support-query features. To address this limitation, we propose a joint learning method termed Masked Cross-Image Encoding (MCE), which is designed to capture common visual properties that describe object details and to learn bidirectional inter-image dependencies that enhance feature interaction. MCE is more than a visual representation enrichment module; it also considers cross-image mutual dependencies and implicit guidance. Experiments on FSS benchmarks PASCAL-$5^i$ and COCO-$20^i$ demonstrate the advanced meta-learning ability of the proposed method.
</details>
<details>
<summary>摘要</summary>
几个例图分类（FSS）是一种密集预测任务，旨在只使用有限数量的标注图像来预测未经看过的类别。关键挑战在FSS中是将查询像素的标签分类用支持图像中学习的类 prototype。现有的FSS方法通常是独立地从支持图像中学习类Descriptor，从而忽略了支持图像和查询图像之间的丰富Contextual information和相互依赖关系。为了解决这一限制，我们提出了一种联合学习方法，称为Masked Cross-Image Encoding（MCE），旨在捕捉对象细节中的共同视觉特性，以及在支持图像和查询图像之间的双向依赖关系。MCE不仅是一种视觉表示增强模块，还考虑了图像之间的相互依赖关系和隐藏导航。在PASCAL-$5^i$和COCO-$20^i$的FSS标准测试集上，我们的提议方法表现出了更高级的元学习能力。
</details></li>
</ul>
<hr>
<h2 id="Novel-view-Synthesis-and-Pose-Estimation-for-Hand-Object-Interaction-from-Sparse-Views"><a href="#Novel-view-Synthesis-and-Pose-Estimation-for-Hand-Object-Interaction-from-Sparse-Views" class="headerlink" title="Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views"></a>Novel-view Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11198">http://arxiv.org/abs/2308.11198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentian Qu, Zhaopeng Cui, Yinda Zhang, Chenyu Meng, Cuixia Ma, Xiaoming Deng, Hongan Wang</li>
<li>for: 这个论文主要是关于手对象交互的理解和生成三维手对象交互的方法。</li>
<li>methods: 该论文提出了一种基于神经网络的渲染和姿态估计系统，用于从稀疏视图中理解手对象交互。该系统还可以实现3D手对象交互编辑。</li>
<li>results: 实验表明，该方法比前州艺术法具有更高的性能。Here’s the full translation of the paper’s abstract in Simplified Chinese:</li>
<li>for: 这个论文主要是关于手对象交互的理解和生成三维手对象交互的方法。</li>
<li>methods: 该论文提出了一种基于神经网络的渲染和姿态估计系统，用于从稀疏视图中理解手对象交互。该系统还可以实现3D手对象交互编辑。</li>
<li>results: 实验表明，该方法比前州艺术法具有更高的性能。I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Hand-object interaction understanding and the barely addressed novel view synthesis are highly desired in the immersive communication, whereas it is challenging due to the high deformation of hand and heavy occlusions between hand and object. In this paper, we propose a neural rendering and pose estimation system for hand-object interaction from sparse views, which can also enable 3D hand-object interaction editing. We share the inspiration from recent scene understanding work that shows a scene specific model built beforehand can significantly improve and unblock vision tasks especially when inputs are sparse, and extend it to the dynamic hand-object interaction scenario and propose to solve the problem in two stages. We first learn the shape and appearance prior knowledge of hands and objects separately with the neural representation at the offline stage. During the online stage, we design a rendering-based joint model fitting framework to understand the dynamic hand-object interaction with the pre-built hand and object models as well as interaction priors, which thereby overcomes penetration and separation issues between hand and object and also enables novel view synthesis. In order to get stable contact during the hand-object interaction process in a sequence, we propose a stable contact loss to make the contact region to be consistent. Experiments demonstrate that our method outperforms the state-of-the-art methods. Code and dataset are available in project webpage https://iscas3dv.github.io/HO-NeRF.
</details>
<details>
<summary>摘要</summary>
手动对象交互理解和 hardly addressed 新视图合成是 immerse 通信中的高优先级需求，但是受到手动对象的高变形和对手与对象的 occlusion 的影响，是一个挑战。在这篇论文中，我们提出了一种基于 neural 渲染和 pose 估计的手动对象交互从稀疏视图系统，可以帮助解决3D 手动对象交互编辑问题。我们 draw 了 scene 理解工作的 inspirations，使用 scene 特定模型在 offline 阶段学习手动对象交互的 shape 和 appearance 特征，然后在 online 阶段使用 rendering-based joint 模型来理解动态手动对象交互，并且通过 pre-built 手和对象模型以及交互 prior 来解决 penetration 和 separation 问题，并实现 novel view synthesis。为了在手动对象交互过程中保持稳定的 contacts，我们提出了一种稳定 contact loss。实验表明，我们的方法超过了现有方法的性能。代码和数据集可以在项目网站 https://iscas3dv.github.io/HO-NeRF 中获取。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Aware-Prompt-Tuning-for-Generalizable-Vision-Language-Models"><a href="#Knowledge-Aware-Prompt-Tuning-for-Generalizable-Vision-Language-Models" class="headerlink" title="Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models"></a>Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11186">http://arxiv.org/abs/2308.11186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen, Weili Guan, Feng Zheng</li>
<li>for: 这篇论文旨在提出一种基于人工知识的潜在图像分类模型，以提高图像分类器的泛化能力。</li>
<li>methods: 该方法使用两种不同类型的知识感知提问，一种是精确提取对象类型的描述信息，另一种是通过学习获得总上下文信息。此外，还设计了一个适应头来归一化重要视觉特征。</li>
<li>results: 对11种广泛使用的基准数据集进行了广泛的实验，结果表明，KAPT方法可以在少量样本下进行图像分类，特别是在新类别上具有泛化能力。与当前最佳方法CoCoOp相比，KAPT方法在新类别上显示了有利的性能，升准值为3.22%和2.57%。<details>
<summary>Abstract</summary>
Pre-trained vision-language models, e.g., CLIP, working with manually designed prompts have demonstrated great capacity of transfer learning. Recently, learnable prompts achieve state-of-the-art performance, which however are prone to overfit to seen classes, failing to generalize to unseen classes. In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for vision-language models. Our approach takes inspiration from human intelligence in which external knowledge is usually incorporated into recognizing novel categories of objects. Specifically, we design two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge. The discrete prompt extracts the key information from descriptions of an object category, and the learned continuous prompt captures overall contexts. We further design an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations. We conduct extensive experiments on 11 widely-used benchmark datasets and the results verify the effectiveness in few-shot image classification, especially in generalizing to unseen categories. Compared with the state-of-the-art CoCoOp method, KAPT exhibits favorable performance and achieves an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean.
</details>
<details>
<summary>摘要</summary>
<<SYS>>传统的视觉语言模型，如CLIP，通过手动设计的提示符可以实现很好的转移学习效果。然而，这些提示符容易过拟合已经看过的类别，导致无法泛化到未看过的类别。在这篇论文中，我们提出了一个知识感知提示调整（KAPT）框架，用于视觉语言模型。我们的方法启取人类智能中在识别新类别对象时，通常会 incorporate 外部知识。我们设计了两种强调知识的提示符，一种是精确提取对象类别的描述信息，另一种是学习对象上的总上下文。此外，我们还设计了一个适应头，用于Visual encoder 中的视觉特征汇聚，以建立特异性和任务意识的视觉表示。我们对11种广泛使用的 benchmark 数据集进行了广泛的实验，结果证明了我们的方法的有效性，特别是在新类别上的泛化。相比之下，与状态之arte CoCoOp 方法相比，KAPT 表现更加优秀，在新类别上实现了3.22%的绝对提升和2.57%的harmonic mean。
</details></li>
</ul>
<hr>
<h2 id="MEGA-Multimodal-Alignment-Aggregation-and-Distillation-For-Cinematic-Video-Segmentation"><a href="#MEGA-Multimodal-Alignment-Aggregation-and-Distillation-For-Cinematic-Video-Segmentation" class="headerlink" title="MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation"></a>MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11185">http://arxiv.org/abs/2308.11185</a></li>
<li>repo_url: None</li>
<li>paper_authors: Najmeh Sadoughi, Xinyu Li, Avijit Vajpayee, David Fan, Bing Shuai, Hector Santos-Villalobos, Vimal Bhat, Rohith MV</li>
<li>for: 本研究旨在提高长形视频（&gt;60分钟）的场景和剧情槽分 segmentation 效果。</li>
<li>methods: 本研究提出了一种基于多媒体Modalities的 Multimodal alignmEnt aGgregation and distillAtion（MEGA）方法，通过多种输入的粗略对应和模式维度的嵌入来解决长形视频的同步问题。</li>
<li>results: 实验结果表明，MEGA方法在 MovieNet 数据集上场景分 segmentation  Task 上提高了+1.19%的平均准确率，并在 TRIPOD 数据集上剧情分 segmentation  Task 上提高了+5.51%的总一致率。<details>
<summary>Abstract</summary>
Previous research has studied the task of segmenting cinematic videos into scenes and into narrative acts. However, these studies have overlooked the essential task of multimodal alignment and fusion for effectively and efficiently processing long-form videos (>60min). In this paper, we introduce Multimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematic long-video segmentation. MEGA tackles the challenge by leveraging multiple media modalities. The method coarsely aligns inputs of variable lengths and different modalities with alignment positional encoding. To maintain temporal synchronization while reducing computation, we further introduce an enhanced bottleneck fusion layer which uses temporal alignment. Additionally, MEGA employs a novel contrastive loss to synchronize and transfer labels across modalities, enabling act segmentation from labeled synopsis sentences on video shots. Our experimental results show that MEGA outperforms state-of-the-art methods on MovieNet dataset for scene segmentation (with an Average Precision improvement of +1.19%) and on TRIPOD dataset for act segmentation (with a Total Agreement improvement of +5.51%)
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ReFit-Recurrent-Fitting-Network-for-3D-Human-Recovery"><a href="#ReFit-Recurrent-Fitting-Network-for-3D-Human-Recovery" class="headerlink" title="ReFit: Recurrent Fitting Network for 3D Human Recovery"></a>ReFit: Recurrent Fitting Network for 3D Human Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11184">http://arxiv.org/abs/2308.11184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yufu-wang/ReFit">https://github.com/yufu-wang/ReFit</a></li>
<li>paper_authors: Yufu Wang, Kostas Daniilidis</li>
<li>for: 本研究旨在提出一种基于神经网络的单影像参数3D人体重建方法，以解决人体重建问题。</li>
<li>methods: 本方法使用反馈更新循环，通过在每个迭代步骤中将人体模型中的关键点投影到特征图中，并使用回归型更新器来调整模型以更好地适应图像。</li>
<li>results: 本研究表明，使用反馈更新循环可以更快速地训练神经网络模型，同时提高了标准benchmark测试数据集上的性能。此外，本方法还可以应用于多视图适应和单视图形状适应等其他优化设定。<details>
<summary>Abstract</summary>
We present Recurrent Fitting (ReFit), a neural network architecture for single-image, parametric 3D human reconstruction. ReFit learns a feedback-update loop that mirrors the strategy of solving an inverse problem through optimization. At each iterative step, it reprojects keypoints from the human model to feature maps to query feedback, and uses a recurrent-based updater to adjust the model to fit the image better. Because ReFit encodes strong knowledge of the inverse problem, it is faster to train than previous regression models. At the same time, ReFit improves state-of-the-art performance on standard benchmarks. Moreover, ReFit applies to other optimization settings, such as multi-view fitting and single-view shape fitting. Project website: https://yufu-wang.github.io/refit_humans/
</details>
<details>
<summary>摘要</summary>
我们介绍Recurrent Fitting（ReFit），一个神经网络架构，用于单一图像、Parametric 3D人体重建。ReFit学习了一个反馈-更新循环，与解析问题的解决策略相似。在每个迭代步骤中，它将人体模型中的关键点投射到特征对称中，以查询反馈，并使用回归型更新器进行调整，以更好地适应图像。由于ReFit传递强大的倒数问题知识，因此它在训练时比前一代 regression 模型更快。同时，ReFit在标准参考数据上提高了现场性能。此外，ReFit可以应用到其他优化设定，例如多视点适摄和单视点形状适摄。相关网站：https://yufu-wang.github.io/refit_humans/
</details></li>
</ul>
<hr>
<h2 id="A-three-in-one-bottom-up-framework-for-simultaneous-semantic-segmentation-instance-segmentation-and-classification-of-multi-organ-nuclei-in-digital-cancer-histology"><a href="#A-three-in-one-bottom-up-framework-for-simultaneous-semantic-segmentation-instance-segmentation-and-classification-of-multi-organ-nuclei-in-digital-cancer-histology" class="headerlink" title="A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology"></a>A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11179">http://arxiv.org/abs/2308.11179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtihaj Ahmad, Syed Muhammad Israr, Zain Ul Islam<br>for: 这paper是用于同时进行核体分割和分类的数字 histology 的研究，它们在计算机协助癌症诊断中扮演着关键的角色，但是这个问题仍然是一个挑战。methods: 这paper使用了一种多头decoder的结构，其中每个头都有独立的权重损失函数，用于生成核体分割、边提议和分类地图。这些输出被用来进行后处理，生成最终的核体分割和分类结果。results: 该paper实现了高性能的核体分割和分类，其中semantic segmentation的Dice score为0.841，实例 segmentation的bPQ score为0.713，和nuclei classification的mPQ score为0.633。此外，该方法在19种组织中得到了普适性。<details>
<summary>Abstract</summary>
Simultaneous segmentation and classification of nuclei in digital histology play an essential role in computer-assisted cancer diagnosis; however, it remains challenging. The highest achieved binary and multi-class Panoptic Quality (PQ) remains as low as 0.68 bPQ and 0.49 mPQ, respectively. It is due to the higher staining variability, variability across the tissue, rough clinical conditions, overlapping nuclei, and nuclear class imbalance. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. In our previous work, DAN-NucNet, we resolved these issues for semantic segmentation with an end-to-end model. This work extends our previous model to simultaneous instance segmentation and classification. We introduce additional decoder heads with independent weighted losses, which produce semantic segmentation, edge proposals, and classification maps. We use the outputs from the three-head model to apply post-processing to produce the final segmentation and classification. Our multi-stage approach utilizes edge proposals and semantic segmentations compared to direct segmentation and classification strategies followed by most state-of-the-art methods. Due to this, we demonstrate a significant performance improvement in producing high-quality instance segmentation and nuclei classification. We have achieved a 0.841 Dice score for semantic segmentation, 0.713 bPQ scores for instance segmentation, and 0.633 mPQ for nuclei classification. Our proposed framework is generalized across 19 types of tissues. Furthermore, the framework is less complex compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
同时分割和分类肿瘤细胞在数字 histology 中扮演着重要的角色，但是它仍然是一个挑战。最高的二分和多分类 Panoptic Quality (PQ) 只有0.68 bPQ和0.49 mPQ，这是因为细胞染色的变化、组织内部的变化、较为恶劣的临床条件、重叠的细胞和核类别偏度。通用的深度学习方法通常采用端到端模型，这些模型无法直接地解决数字 histology 中相关的问题。在我们的前一个工作中，我们提出了 DAN-NucNet 模型，解决了这些问题。本文将 extend 我们的前一个模型，以实现同时的实例分割和分类。我们添加了多个解码头，每个解码头都有独立的权重损失，它们生成的结果包括semantic segmentation、edge proposal 和分类图像。我们使用这些三个头的输出来进行后处理，以生成最终的分割和分类结果。我们的多阶段方法利用了 edge proposal 和 semantic segmentation，而不是直接进行分割和分类的方法，这使得我们可以提高高质量的实例分割和核类分类。我们在19种组织中实现了0.841 Dice 分割率、0.713 bPQ 分割率和0.633 mPQ 分类率。我们提出的框架比现有的状态之一更加简单。
</details></li>
</ul>
<hr>
<h2 id="Improving-Misaligned-Multi-modality-Image-Fusion-with-One-stage-Progressive-Dense-Registration"><a href="#Improving-Misaligned-Multi-modality-Image-Fusion-with-One-stage-Progressive-Dense-Registration" class="headerlink" title="Improving Misaligned Multi-modality Image Fusion with One-stage Progressive Dense Registration"></a>Improving Misaligned Multi-modality Image Fusion with One-stage Progressive Dense Registration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11165">http://arxiv.org/abs/2308.11165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Di Wang, Jinyuan Liu, Long Ma, Risheng Liu, Xin Fan</li>
<li>for:  addressing the challenges of misalignments between multi-modality images in image fusion</li>
<li>methods: 一种Cross-modality Multi-scale Progressive Dense Registration (C-MPDR) scheme, which uses a one-stage optimization to improve the fusion performance of misaligned multi-modality images</li>
<li>results: 提高了多模态图像匹配的混合性能<details>
<summary>Abstract</summary>
Misalignments between multi-modality images pose challenges in image fusion, manifesting as structural distortions and edge ghosts. Existing efforts commonly resort to registering first and fusing later, typically employing two cascaded stages for registration,i.e., coarse registration and fine registration. Both stages directly estimate the respective target deformation fields. In this paper, we argue that the separated two-stage registration is not compact, and the direct estimation of the target deformation fields is not accurate enough. To address these challenges, we propose a Cross-modality Multi-scale Progressive Dense Registration (C-MPDR) scheme, which accomplishes the coarse-to-fine registration exclusively using a one-stage optimization, thus improving the fusion performance of misaligned multi-modality images. Specifically, two pivotal components are involved, a dense Deformation Field Fusion (DFF) module and a Progressive Feature Fine (PFF) module. The DFF aggregates the predicted multi-scale deformation sub-fields at the current scale, while the PFF progressively refines the remaining misaligned features. Both work together to accurately estimate the final deformation fields. In addition, we develop a Transformer-Conv-based Fusion (TCF) subnetwork that considers local and long-range feature dependencies, allowing us to capture more informative features from the registered infrared and visible images for the generation of high-quality fused images. Extensive experimental analysis demonstrates the superiority of the proposed method in the fusion of misaligned cross-modality images.
</details>
<details>
<summary>摘要</summary>
《多Modalität图像误差问题在图像融合中带来挑战，导致结构扭曲和边幻影。现有尝试通常采用分两个阶段进行 регистрирование，即粗略 регистрирование和精细 регистрирование，两个阶段直接估计目标扭曲场。在这篇论文中，我们认为分开的两个阶段 регистрирование不是 компакт的，而直接估计目标扭曲场也不够精确。为了解决这些挑战，我们提出了一种交叉模态多尺度进行密度注registratin（C-MPDR）方案，通过一个单一的优化过程，提高误差的多模态图像融合性能。具体来说，这种方案包括两个关键组件：一个密集的Deformation Field Fusion（DFF）模块和一个进行进行精细调整的Progressive Feature Fine（PFF）模块。DFF模块将在当前级别预测多尺度扭曲子场，而PFF模块将逐渐调整剩下的误差特征。两个模块共同帮助估计最终的扭曲场。此外，我们还开发了基于Transformer-Conv的融合（TCF）子网络，该子网络考虑了本地和远程特征依赖关系，使我们能够更好地捕捉融合后的高质量混合图像中的有用特征。广泛的实验分析表明，我们提出的方法在跨模态图像融合中具有显著的优势。》Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Decoupled-Contrastive-Multi-view-Clustering-with-High-order-Random-Walks"><a href="#Decoupled-Contrastive-Multi-view-Clustering-with-High-order-Random-Walks" class="headerlink" title="Decoupled Contrastive Multi-view Clustering with High-order Random Walks"></a>Decoupled Contrastive Multi-view Clustering with High-order Random Walks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11164">http://arxiv.org/abs/2308.11164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiding Lu, Yijie Lin, Mouxing Yang, Dezhong Peng, Peng Hu, Xi Peng</li>
<li>for: 提高多视图集群（MvC）的稳定性和抗衰落性，解决false negative和false positive问题。</li>
<li>methods: 提出了一种新的多视图集群方法（DIVIDE），通过随机游走进行全球性地标识数据对，从而可以正确地标识内部负样本和外部正样本。同时，DIVIDE采用了一种新的多视图集群架构，在不同的嵌入空间进行对比学习，以提高集群性能和抗衰落性。</li>
<li>results: 通过对四个标准测试集进行广泛的实验，证明了DIVIDE在完整的MvC设定下和缺失视图的MvC设定下都有较高的性能，并且在不同的缺失视图情况下保持稳定性。<details>
<summary>Abstract</summary>
In recent, some robust contrastive multi-view clustering (MvC) methods have been proposed, which construct data pairs from neighborhoods to alleviate the false negative issue, i.e., some intra-cluster samples are wrongly treated as negative pairs. Although promising performance has been achieved by these methods, the false negative issue is still far from addressed and the false positive issue emerges because all in- and out-of-neighborhood samples are simply treated as positive and negative, respectively. To address the issues, we propose a novel robust method, dubbed decoupled contrastive multi-view clustering with high-order random walks (DIVIDE). In brief, DIVIDE leverages random walks to progressively identify data pairs in a global instead of local manner. As a result, DIVIDE could identify in-neighborhood negatives and out-of-neighborhood positives. Moreover, DIVIDE embraces a novel MvC architecture to perform inter- and intra-view contrastive learning in different embedding spaces, thus boosting clustering performance and embracing the robustness against missing views. To verify the efficacy of DIVIDE, we carry out extensive experiments on four benchmark datasets comparing with nine state-of-the-art MvC methods in both complete and incomplete MvC settings.
</details>
<details>
<summary>摘要</summary>
近些年，一些强大的多视图决定 clustering（MvC）方法被提出，这些方法从邻居中构建数据对以解决内部样本被错误地视为负对的问题。 although these methods have achieved promising performance, the false negative issue is still not fully addressed, and a new issue has emerged, i.e., false positives, because all in- and out-of-neighborhood samples are simply treated as positive and negative, respectively. To address these issues, we propose a novel robust method, called decoupled contrastive multi-view clustering with high-order random walks (DIVIDE).DIVIDE 方法利用Random Walk来逐渐标识全球的数据对，而不是局部的方法。 As a result, DIVIDE could identify in-neighborhood negatives and out-of-neighborhood positives. 更over, DIVIDE 方法采用一种新的MvC架构，以进行不同的嵌入空间中的对比学习，从而提高划分性和对缺失视图的抗性。 To verify the effectiveness of DIVIDE, we conduct extensive experiments on four benchmark datasets, comparing with nine state-of-the-art MvC methods in both complete and incomplete MvC settings.
</details></li>
</ul>
<hr>
<h2 id="A-Preliminary-Investigation-into-Search-and-Matching-for-Tumour-Discrimination-in-WHO-Breast-Taxonomy-Using-Deep-Networks"><a href="#A-Preliminary-Investigation-into-Search-and-Matching-for-Tumour-Discrimination-in-WHO-Breast-Taxonomy-Using-Deep-Networks" class="headerlink" title="A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks"></a>A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11162">http://arxiv.org/abs/2308.11162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abubakr Shafique, Ricardo Gonzalez, Liron Pantanowitz, Puay Hoon Tan, Alberto Machado, Ian A Cree, Hamid R. Tizhoosh</li>
<li>for: 这项研究旨在开发一个基于深度学习的搜索able数字Atlas，用于帮助病理学家对悉数据库中的罕见癌症进行查找和匹配。</li>
<li>methods: 该研究使用了一个国际知名的TCGA数据库，并使用了一个国际顶尖的深度学习模型，对 millions of diagnostic histopathology images进行了预训练。然后，对WHO乳腺分类系统（第5版）中的35种肿瘤类型进行了索引和分析，并使用了深度特征来Visualize所有肿瘤类型。</li>
<li>results: 该研究发现，使用深度学习模型对WHO乳腺分类系统数据进行索引和分析，可以达到88%的准确率，并且使用top-n肿瘤类型进行验证可以达到91%的准确率。这些结果表明，使用索引数字Archive可以investigate complex relationships among common and rare breast lesions。<details>
<summary>Abstract</summary>
Breast cancer is one of the most common cancers affecting women worldwide. They include a group of malignant neoplasms with a variety of biological, clinical, and histopathological characteristics. There are more than 35 different histological forms of breast lesions that can be classified and diagnosed histologically according to cell morphology, growth, and architecture patterns. Recently, deep learning, in the field of artificial intelligence, has drawn a lot of attention for the computerized representation of medical images. Searchable digital atlases can provide pathologists with patch matching tools allowing them to search among evidently diagnosed and treated archival cases, a technology that may be regarded as computational second opinion. In this study, we indexed and analyzed the WHO breast taxonomy (Classification of Tumours 5th Ed.) spanning 35 tumour types. We visualized all tumour types using deep features extracted from a state-of-the-art deep learning model, pre-trained on millions of diagnostic histopathology images from the TCGA repository. Furthermore, we test the concept of a digital "atlas" as a reference for search and matching with rare test cases. The patch similarity search within the WHO breast taxonomy data reached over 88% accuracy when validating through "majority vote" and more than 91% accuracy when validating using top-n tumour types. These results show for the first time that complex relationships among common and rare breast lesions can be investigated using an indexed digital archive.
</details>
<details>
<summary>摘要</summary>
乳癌是全球女性最常见的恶性肿瘤之一，其包括多种生物学、临床和 histopathological 特征的肿瘤。有超过35种不同的乳腺病变可以根据细胞形态、生长和建筑模式进行分类和诊断。近些年来，人工智能技术中的深度学习在医疗领域得到了广泛的关注，特别是在计算机化的医疗图像 Representation 方面。搜索able digital atlas 可以为病理学家提供一个搜索和匹配已诊断和治疗的档案库，这种技术可以视为计算机化的第二次诊断。本研究对 WHO 乳腺分类（第5版）进行了索引和分析，包括35种肿瘤类型。我们使用了一个国际上最新的深度学习模型，该模型在TCGA 数据库上进行了 millions 个诊断 Histopathology 图像的预处理，然后对所有肿瘤类型进行了深度特征提取和可视化。此外，我们还测试了一个数字"atlas"作为参考，用于搜索和匹配罕见案例。在 WHO 乳腺分类数据中进行了质心精度搜索，结果表明，使用 "多数投票" 验证的精度高达88%，使用 top-n 肿瘤类型验证的精度高达91%。这些结果表明，使用索引数字档案库，可以 investigate Complex relationships among common and rare breast lesions.
</details></li>
</ul>
<hr>
<h2 id="SwinV2DNet-Pyramid-and-Self-Supervision-Compounded-Feature-Learning-for-Remote-Sensing-Images-Change-Detection"><a href="#SwinV2DNet-Pyramid-and-Self-Supervision-Compounded-Feature-Learning-for-Remote-Sensing-Images-Change-Detection" class="headerlink" title="SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection"></a>SwinV2DNet: Pyramid and Self-Supervision Compounded Feature Learning for Remote Sensing Images Change Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11159">http://arxiv.org/abs/2308.11159</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dalong Zheng, Zebin Wu, Jia Liu, Zhihui Wei</li>
<li>for: 本研究旨在提出一种综合厚度网络（SwinV2DNet），以综合利用 transformer 和 CNN 的优点，解决现有网络在特征学习中的缺陷。</li>
<li>methods: 该网络包括 Swin V2 和 VGG16 两部分，通过 densely connected 的 Swin V2 脊梁和 CNN 分支，捕捉到 cambio 关系特征，并通过 mixed feature pyramid (MFP) 提供了多层次和协调的特征学习。</li>
<li>results: 对四个常用的公共遥感数据集进行比较，该网络可以达到state-of-the-art 的变化检测分数和细化变化地图，并且通过自我监督策略提高了 CNN 分支的训练问题。<details>
<summary>Abstract</summary>
Among the current mainstream change detection networks, transformer is deficient in the ability to capture accurate low-level details, while convolutional neural network (CNN) is wanting in the capacity to understand global information and establish remote spatial relationships. Meanwhile, both of the widely used early fusion and late fusion frameworks are not able to well learn complete change features. Therefore, based on swin transformer V2 (Swin V2) and VGG16, we propose an end-to-end compounded dense network SwinV2DNet to inherit the advantages of both transformer and CNN and overcome the shortcomings of existing networks in feature learning. Firstly, it captures the change relationship features through the densely connected Swin V2 backbone, and provides the low-level pre-changed and post-changed features through a CNN branch. Based on these three change features, we accomplish accurate change detection results. Secondly, combined with transformer and CNN, we propose mixed feature pyramid (MFP) which provides inter-layer interaction information and intra-layer multi-scale information for complete feature learning. MFP is a plug and play module which is experimentally proven to be also effective in other change detection networks. Further more, we impose a self-supervision strategy to guide a new CNN branch, which solves the untrainable problem of the CNN branch and provides the semantic change information for the features of encoder. The state-of-the-art (SOTA) change detection scores and fine-grained change maps were obtained compared with other advanced methods on four commonly used public remote sensing datasets. The code is available at https://github.com/DalongZ/SwinV2DNet.
</details>
<details>
<summary>摘要</summary>
当前主流的变化检测网络中，变换器缺乏捕捉准确的低级细节的能力，而卷积神经网络（CNN）缺乏建立远程空间关系和全局信息的能力。同时，现有的早期融合和晚期融合框架都不能良好地学习完整的变化特征。因此，基于Swin transformer V2（Swin V2）和VGG16，我们提出了一种端到端融合密集网络SwinV2DNet，继承了变换器和CNN的优点，并超越了现有网络在特征学习方面的缺陷。首先，SwinV2DNet通过密集连接的Swin V2脊梁捕捉变化关系特征，并提供低级预变和后变特征通过一个CNN分支。基于这三个变化特征，我们实现了准确的变化检测结果。其次，我们提出了混合特征阶梯（MFP），该模块通过卷积神经网络和变换器的结合，为完整的特征学习提供了间层交互信息和多尺度内层信息。MFP是一个可插入的模块，实验证明其效果也可以应用于其他变化检测网络。此外，我们对新的CNN分支进行自我超vision策略，解决了CNN分支的训练不可能问题，并为encoder的特征提供了semantic变化信息。与其他先进方法相比，我们在四个常用的公共遥感数据集上获得了状态前的变化检测分数和细化变化地图。代码可以在https://github.com/DalongZ/SwinV2DNet上获取。
</details></li>
</ul>
<hr>
<h2 id="Domain-Generalization-via-Rationale-Invariance"><a href="#Domain-Generalization-via-Rationale-Invariance" class="headerlink" title="Domain Generalization via Rationale Invariance"></a>Domain Generalization via Rationale Invariance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11158">http://arxiv.org/abs/2308.11158</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liangchen527/ridg">https://github.com/liangchen527/ridg</a></li>
<li>paper_authors: Liang Chen, Yong Zhang, Yibing Song, Anton van den Hengel, Lingqiao Liu</li>
<li>for: 提高领域总结的稳定性，以便在未见 environments 中维持良好的结果。</li>
<li>methods: 通过对决策过程的最后一层抽象来解决领域总结挑战。具体来说，我们提议将每个样本的元素级别贡献视为决策的理由，并将每个样本的理由表示为一个矩阵。为了确保模型具有良好的普适性，我们建议每个类别的理由矩阵具有相似性，表明模型在各个环境中依靠域外特征来做出决策。</li>
<li>results: 我们的实验表明，通过引入 rational invariance loss 来实现这一思路，可以在不同的领域上实现竞争性的结果，即使使用的是简单的代码。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/liangchen527/RIDG%7D">https://github.com/liangchen527/RIDG}</a> 上找到。<details>
<summary>Abstract</summary>
This paper offers a new perspective to ease the challenge of domain generalization, which involves maintaining robust results even in unseen environments. Our design focuses on the decision-making process in the final classifier layer. Specifically, we propose treating the element-wise contributions to the final results as the rationale for making a decision and representing the rationale for each sample as a matrix. For a well-generalized model, we suggest the rationale matrices for samples belonging to the same category should be similar, indicating the model relies on domain-invariant clues to make decisions, thereby ensuring robust results. To implement this idea, we introduce a rationale invariance loss as a simple regularization technique, requiring only a few lines of code. Our experiments demonstrate that the proposed approach achieves competitive results across various datasets, despite its simplicity. Code is available at \url{https://github.com/liangchen527/RIDG}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TOPIC-A-Parallel-Association-Paradigm-for-Multi-Object-Tracking-under-Complex-Motions-and-Diverse-Scenes"><a href="#TOPIC-A-Parallel-Association-Paradigm-for-Multi-Object-Tracking-under-Complex-Motions-and-Diverse-Scenes" class="headerlink" title="TOPIC: A Parallel Association Paradigm for Multi-Object Tracking under Complex Motions and Diverse Scenes"></a>TOPIC: A Parallel Association Paradigm for Multi-Object Tracking under Complex Motions and Diverse Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11157">http://arxiv.org/abs/2308.11157</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/holmescao/TOPICTrack">https://github.com/holmescao/TOPICTrack</a></li>
<li>paper_authors: Xiaoyan Cao, Yiyao Zheng, Yao Yao, Huapeng Qin, Xiaoyu Cao, Shihui Guo</li>
<li>for: 这 paper 的目的是提出一种新的多对目标跟踪（MOT）数据集，以解决现有数据集忽略了复杂的运动模式的问题。</li>
<li>methods: 该 paper 使用了一种新的并行相关模式（Parallel Paradigm），并提出了一种基于运动和外观特征的两个圆形相关机制（TOPIC），以及一种基于注意力的外观重建模块（AARM）来提高跟踪效果。</li>
<li>results: 该 paper 的方法在四个公共数据集和新 introduce 的 BEE23 数据集上实现了领先的表现，包括减少 false negatives 12% 到 51% compared to 单一特征相关模式。<details>
<summary>Abstract</summary>
Video data and algorithms have been driving advances in multi-object tracking (MOT). While existing MOT datasets focus on occlusion and appearance similarity, complex motion patterns are widespread yet overlooked. To address this issue, we introduce a new dataset called BEE23 to highlight complex motions. Identity association algorithms have long been the focus of MOT research. Existing trackers can be categorized into two association paradigms: single-feature paradigm (based on either motion or appearance feature) and serial paradigm (one feature serves as secondary while the other is primary). However, these paradigms are incapable of fully utilizing different features. In this paper, we propose a parallel paradigm and present the Two rOund Parallel matchIng meChanism (TOPIC) to implement it. The TOPIC leverages both motion and appearance features and can adaptively select the preferable one as the assignment metric based on motion level. Moreover, we provide an Attention-based Appearance Reconstruct Module (AARM) to reconstruct appearance feature embeddings, thus enhancing the representation of appearance features. Comprehensive experiments show that our approach achieves state-of-the-art performance on four public datasets and BEE23. Notably, our proposed parallel paradigm surpasses the performance of existing association paradigms by a large margin, e.g., reducing false negatives by 12% to 51% compared to the single-feature association paradigm. The introduced dataset and association paradigm in this work offers a fresh perspective for advancing the MOT field. The source code and dataset are available at https://github.com/holmescao/TOPICTrack.
</details>
<details>
<summary>摘要</summary>
视频数据和算法在多对目标跟踪（MOT）领域取得了重大进步。现有的MOT数据集集中焦点在 occlusion 和外观相似性上，然而复杂的运动模式却被忽略。为了解决这个问题，我们提出了一个新的数据集called BEE23，以强调复杂的运动。目标跟踪算法的研究总是围绕着 Identity association 问题进行，现有的跟踪器可以分为两种联系思维：单一特征思维（基于 either motion 或 appearance feature）以及串行思维（一个特征服务为次要，另一个特征服务为主要）。然而，这些思维方法无法完全利用不同的特征。在这篇论文中，我们提议了并行联系思维，并通过 Two rOund Parallel matchIng meChanism（TOPIC）来实现。TOPIC 利用了运动和外观特征，并可以根据运动水平选择适合的一个作为分配度量。此外，我们还提供了 Attention-based Appearance Reconstruct Module（AARM）来重建外观特征嵌入，从而提高外观特征的表示。我们对四个公共数据集和 BEE23 进行了广泛的实验，结果显示我们的方法在这些数据集上达到了当前最佳性能。尤其是，我们提出的并行联系思维在现有的联系思维方法之上减少了12%至51%的假阳性。在这篇论文中，我们还提供了一个新的数据集和联系思维方法，这将为 MOT 领域带来新的视角，并且代码和数据集可以在 <https://github.com/holmescao/TOPICTrack> 上获取。
</details></li>
</ul>
<hr>
<h2 id="High-Dynamic-Range-Imaging-of-Dynamic-Scenes-with-Saturation-Compensation-but-without-Explicit-Motion-Compensation"><a href="#High-Dynamic-Range-Imaging-of-Dynamic-Scenes-with-Saturation-Compensation-but-without-Explicit-Motion-Compensation" class="headerlink" title="High Dynamic Range Imaging of Dynamic Scenes with Saturation Compensation but without Explicit Motion Compensation"></a>High Dynamic Range Imaging of Dynamic Scenes with Saturation Compensation but without Explicit Motion Compensation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11140">http://arxiv.org/abs/2308.11140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haesoochung/hdri-saturation-compensation">https://github.com/haesoochung/hdri-saturation-compensation</a></li>
<li>paper_authors: Haesoo Chung, Nam Ik Cho</li>
<li>for: 提高高动态范围（HDR）图像的获得和修复，解决由相机传感器的限制导致的信息损失问题。</li>
<li>methods: 使用改进的运动补偿和灰度调整问题的解决方案，通过 Contextual attention 技术来修复过度曝光区域。</li>
<li>results: 比对 state-of-the-art 方法，示出了更高的质量和量化评价结果。<details>
<summary>Abstract</summary>
High dynamic range (HDR) imaging is a highly challenging task since a large amount of information is lost due to the limitations of camera sensors. For HDR imaging, some methods capture multiple low dynamic range (LDR) images with altering exposures to aggregate more information. However, these approaches introduce ghosting artifacts when significant inter-frame motions are present. Moreover, although multi-exposure images are given, we have little information in severely over-exposed areas. Most existing methods focus on motion compensation, i.e., alignment of multiple LDR shots to reduce the ghosting artifacts, but they still produce unsatisfying results. These methods also rather overlook the need to restore the saturated areas. In this paper, we generate well-aligned multi-exposure features by reformulating a motion alignment problem into a simple brightness adjustment problem. In addition, we propose a coarse-to-fine merging strategy with explicit saturation compensation. The saturated areas are reconstructed with similar well-exposed content using adaptive contextual attention. We demonstrate that our method outperforms the state-of-the-art methods regarding qualitative and quantitative evaluations.
</details>
<details>
<summary>摘要</summary>
高动态范围（HDR）摄影是一项非常具有挑战性的任务，因为摄像头传感器的限制会导致大量信息的丢失。为实现HDR摄影，一些方法会 capture多个低动态范围（LDR）图像，并将它们进行不同的曝光设定来聚集更多的信息。然而，这些方法会导致在 significative 的运动误差存在时出现幻影artefacts。此外，虽然我们有多个曝光图像，但我们在严重过曝光区域中具有少量信息。大多数现有方法将焦点放在运动补做上，即将多个LDR拍摄Alignment 来减少幻影artefacts，但这些方法仍然生成不满足的结果。这些方法还往往忽略了重建过曝光区域的需求。在这篇论文中，我们将生成准确尺度的多个曝光特征，通过将运动Alignment 转换为简单的明亮调整问题来实现。此外，我们还提出了一种粗略到细节的合并策略，并且使用明确的过曝光补做。在过曝光区域中，我们使用适应的上下文关注来重建相似的准确曝光内容。我们示示了我们的方法在质量和量度上的超越现有方法。
</details></li>
</ul>
<hr>
<h2 id="Efficient-View-Synthesis-with-Neural-Radiance-Distribution-Field"><a href="#Efficient-View-Synthesis-with-Neural-Radiance-Distribution-Field" class="headerlink" title="Efficient View Synthesis with Neural Radiance Distribution Field"></a>Efficient View Synthesis with Neural Radiance Distribution Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11130">http://arxiv.org/abs/2308.11130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yushuang-wu/NeRDF">https://github.com/yushuang-wu/NeRDF</a></li>
<li>paper_authors: Yushuang Wu, Xiao Li, Jinglu Wang, Xiaoguang Han, Shuguang Cui, Yan Lu</li>
<li>For: 高品质视角合成* Methods: 使用小型网络模型，采用频率基准来模型光度分布，采样一次网络前进来计算像素值* Results: 提供了一个更好的权衡 между速度、质量和网络大小，与传统方法相比，具有约254倍的速度提升，同时保持了类似的网络大小和质量水平。<details>
<summary>Abstract</summary>
Recent work on Neural Radiance Fields (NeRF) has demonstrated significant advances in high-quality view synthesis. A major limitation of NeRF is its low rendering efficiency due to the need for multiple network forwardings to render a single pixel. Existing methods to improve NeRF either reduce the number of required samples or optimize the implementation to accelerate the network forwarding. Despite these efforts, the problem of multiple sampling persists due to the intrinsic representation of radiance fields. In contrast, Neural Light Fields (NeLF) reduce the computation cost of NeRF by querying only one single network forwarding per pixel. To achieve a close visual quality to NeRF, existing NeLF methods require significantly larger network capacities which limits their rendering efficiency in practice. In this work, we propose a new representation called Neural Radiance Distribution Field (NeRDF) that targets efficient view synthesis in real-time. Specifically, we use a small network similar to NeRF while preserving the rendering speed with a single network forwarding per pixel as in NeLF. The key is to model the radiance distribution along each ray with frequency basis and predict frequency weights using the network. Pixel values are then computed via volume rendering on radiance distributions. Experiments show that our proposed method offers a better trade-off among speed, quality, and network size than existing methods: we achieve a ~254x speed-up over NeRF with similar network size, with only a marginal performance decline. Our project page is at yushuang-wu.github.io/NeRDF.
</details>
<details>
<summary>摘要</summary>
最近的神经辐射场（NeRF）研究取得了高品质视图合成的重要进步。然而，NeRF具有辐射场的内置表示，导致每个像素需要多个网络请求，从而降低了渲染效率。现有的方法可以减少需要的样本数或者优化网络实现以加速网络请求。然而，这些努力仍然无法消除多样本的问题。相比之下，神经光场（NeLF）可以通过每个像素只需要一次网络请求来减少计算成本。然而，现有的NeLF方法需要较大的网络容量，从而限制了实际的渲染效率。在这种情况下，我们提出了一种新的表示方式 called Neural Radiance Distribution Field（NeRDF），旨在实现高效的视图合成。具体来说，我们使用一个小型神经网络，类似于NeRF，同时保持与NeLF一样的渲染速度。关键在于，我们使用频率基is来模型每个辐射线上的光辐射分布，并使用网络来预测频率权重。然后，通过量 rendering 技术来计算像素值。实验表明，我们的提议方法可以在Speed、质量和网络大小之间取得更好的平衡，相比之下NeRF和NeLF的方法。我们的项目页面是 yushuang-wu.github.io/NeRDF。
</details></li>
</ul>
<hr>
<h2 id="Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs"><a href="#Hey-That’s-Mine-Imperceptible-Watermarks-are-Preserved-in-Diffusion-Generated-Outputs" class="headerlink" title="Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs"></a>Hey That’s Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11123">http://arxiv.org/abs/2308.11123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Ditria, Tom Drummond</li>
<li>for: 保护内容在线分享</li>
<li>methods: 使用隐形水印技术训练生成模型，并测试其能够在生成图像中检测水印。</li>
<li>results: 通过统计测试，确定了模型是否训练使用水印数据，以及水印数据中的特征与生成图像之间的相关性。<details>
<summary>Abstract</summary>
Generative models have seen an explosion in popularity with the release of huge generative Diffusion models like Midjourney and Stable Diffusion to the public. Because of this new ease of access, questions surrounding the automated collection of data and issues regarding content ownership have started to build. In this paper we present new work which aims to provide ways of protecting content when shared to the public. We show that a generative Diffusion model trained on data that has been imperceptibly watermarked will generate new images with these watermarks present. We further show that if a given watermark is correlated with a certain feature of the training data, the generated images will also have this correlation. Using statistical tests we show that we are able to determine whether a model has been trained on marked data, and what data was marked. As a result our system offers a solution to protect intellectual property when sharing content online.
</details>
<details>
<summary>摘要</summary>
traducciona el texto a chino simplificado.<</SYS>>广泛的生成模型在大量生成扩散模型如中途和稳定扩散的公共释放后得到了普及。由于这新的访问方便，自动收集数据和内容所有权问题开始升温。在这篇论文中，我们介绍新的工作，旨在在公共分享时保护内容。我们显示，通过训练在不可见水印数据上的生成扩散模型，新生成的图像都会包含这些水印。此外，如果给定的水印与训练数据中某个特征相关，那么生成的图像也会具有这种相关性。通过统计测试，我们表明可以判断模型是否训练在标记数据上，以及具体的标记数据。因此，我们的系统可以解决在线分享内容时保护知识产权。
</details></li>
</ul>
<hr>
<h2 id="Random-Word-Data-Augmentation-with-CLIP-for-Zero-Shot-Anomaly-Detection"><a href="#Random-Word-Data-Augmentation-with-CLIP-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection"></a>Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11119">http://arxiv.org/abs/2308.11119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masato Tamura</li>
<li>for: 这个研究是为了开发一个 zero-shot anomaly detection 方法，利用 CLIP 的视觉语言模型来提供数据源。</li>
<li>methods: 这个方法使用 CLIP 的 prompt-guided classification 技术，将每个图像分成多个部分，并将每个部分作为 input 进行类别。此外，还使用了一些随机生成的词语，以增加训练数据的多样性。</li>
<li>results: 实验结果显示，这个方法可以在 zero-shot 设定下 achieves state-of-the-art 性能，不需要耗费很多时间进行训练。<details>
<summary>Abstract</summary>
This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLIP with typical prompts that include words of normal and anomaly. In addition to these words, we insert several randomly generated words into prompts, which enables the encoder to generate a diverse set of normal and anomalous samples. Using the generated embeddings as training data, a feed-forward neural network learns to extract features of normal and anomaly from CLIP's embeddings, and as a result, a category-agnostic anomaly detector can be obtained without any training images. Experimental results demonstrate that our method achieves state-of-the-art performance without laborious prompt ensembling in zero-shot setups.
</details>
<details>
<summary>摘要</summary>
To overcome these issues, the proposed method leverages CLIP as a data source for training. The method generates text embeddings with the text encoder in CLIP using typical prompts that include words related to normal and anomalous samples. Additionally, several randomly generated words are inserted into the prompts to enable the encoder to generate a diverse set of normal and anomalous samples. These embeddings are then used as training data for a feed-forward neural network to extract features of normal and anomalous samples from CLIP's embeddings. As a result, a category-agnostic anomaly detector can be obtained without any training images.Experimental results demonstrate that the proposed method achieves state-of-the-art performance in zero-shot setups without the need for laborious prompt ensembling.
</details></li>
</ul>
<hr>
<h2 id="LAN-HDR-Luminance-based-Alignment-Network-for-High-Dynamic-Range-Video-Reconstruction"><a href="#LAN-HDR-Luminance-based-Alignment-Network-for-High-Dynamic-Range-Video-Reconstruction" class="headerlink" title="LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction"></a>LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11116">http://arxiv.org/abs/2308.11116</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haesoochung/lan-hdr">https://github.com/haesoochung/lan-hdr</a></li>
<li>paper_authors: Haesoo Chung, Nam Ik Cho</li>
<li>for: 提高高清晰度和动态范围（HDR）影像技术，以满足用户对高质量视频的需求。</li>
<li>methods: 基于特征空间的灵活抽象网络（LAN-HDR），包括对适应模块和梦想模块。对模块使用灵活抽象来减少流量估计错误。</li>
<li>results: 比较现有方法表现更好或相当，在多个标准测试数据集上进行了广泛的实验。<details>
<summary>Abstract</summary>
As demands for high-quality videos continue to rise, high-resolution and high-dynamic range (HDR) imaging techniques are drawing attention. To generate an HDR video from low dynamic range (LDR) images, one of the critical steps is the motion compensation between LDR frames, for which most existing works employed the optical flow algorithm. However, these methods suffer from flow estimation errors when saturation or complicated motions exist. In this paper, we propose an end-to-end HDR video composition framework, which aligns LDR frames in the feature space and then merges aligned features into an HDR frame, without relying on pixel-domain optical flow. Specifically, we propose a luminance-based alignment network for HDR (LAN-HDR) consisting of an alignment module and a hallucination module. The alignment module aligns a frame to the adjacent reference by evaluating luminance-based attention, excluding color information. The hallucination module generates sharp details, especially for washed-out areas due to saturation. The aligned and hallucinated features are then blended adaptively to complement each other. Finally, we merge the features to generate a final HDR frame. In training, we adopt a temporal loss, in addition to frame reconstruction losses, to enhance temporal consistency and thus reduce flickering. Extensive experiments demonstrate that our method performs better or comparable to state-of-the-art methods on several benchmarks.
</details>
<details>
<summary>摘要</summary>
As demands for high-quality videos continue to rise, high-resolution and high-dynamic range (HDR) imaging techniques are drawing attention. To generate an HDR video from low dynamic range (LDR) images, one of the critical steps is the motion compensation between LDR frames, for which most existing works employed the optical flow algorithm. However, these methods suffer from flow estimation errors when saturation or complicated motions exist. In this paper, we propose an end-to-end HDR video composition framework, which aligns LDR frames in the feature space and then merges aligned features into an HDR frame, without relying on pixel-domain optical flow. Specifically, we propose a luminance-based alignment network for HDR (LAN-HDR) consisting of an alignment module and a hallucination module. The alignment module aligns a frame to the adjacent reference by evaluating luminance-based attention, excluding color information. The hallucination module generates sharp details, especially for washed-out areas due to saturation. The aligned and hallucinated features are then blended adaptively to complement each other. Finally, we merge the features to generate a final HDR frame. In training, we adopt a temporal loss, in addition to frame reconstruction losses, to enhance temporal consistency and thus reduce flickering. Extensive experiments demonstrate that our method performs better or comparable to state-of-the-art methods on several benchmarks.
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Novel-Quantum-Pre-processing-Filter-to-Improve-Image-Classification-Accuracy-of-Neural-Network-Models"><a href="#Development-of-a-Novel-Quantum-Pre-processing-Filter-to-Improve-Image-Classification-Accuracy-of-Neural-Network-Models" class="headerlink" title="Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models"></a>Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11112">http://arxiv.org/abs/2308.11112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hajimesuzuki999/qpf">https://github.com/hajimesuzuki999/qpf</a></li>
<li>paper_authors: Farina Riaz, Shahab Abdulla, Hajime Suzuki, Srinjoy Ganguly, Ravinesh C. Deo, Susan Hopkins</li>
<li>for: 提高图像分类准确率</li>
<li>methods: 使用量子预处理筛选器（QPF），应用于图像分类神经网络模型中</li>
<li>results: 在MNIST和EMNIST数据集上，图像分类准确率提高至95.4%和75.9%，分别提高了2.9%和7.1%，无需添加额外参数或优化机器学习过程。<details>
<summary>Abstract</summary>
This paper proposes a novel quantum pre-processing filter (QPF) to improve the image classification accuracy of neural network (NN) models. A simple four qubit quantum circuit that uses Y rotation gates for encoding and two controlled NOT gates for creating correlation among the qubits is applied as a feature extraction filter prior to passing data into the fully connected NN architecture. By applying the QPF approach, the results show that the image classification accuracy based on the MNIST (handwritten 10 digits) and the EMNIST (handwritten 47 class digits and letters) datasets can be improved, from 92.5% to 95.4% and from 68.9% to 75.9%, respectively. These improvements were obtained without introducing extra model parameters or optimizations in the machine learning process. However, tests performed on the developed QPF approach against a relatively complex GTSRB dataset with 43 distinct class real-life traffic sign images showed a degradation in the classification accuracy. Considering this result, further research into the understanding and the design of a more suitable quantum circuit approach for image classification neural networks could be explored utilizing the baseline method proposed in this paper.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Classification-of-the-lunar-surface-pattern-by-AI-architectures-Does-AI-see-a-rabbit-in-the-Moon"><a href="#Classification-of-the-lunar-surface-pattern-by-AI-architectures-Does-AI-see-a-rabbit-in-the-Moon" class="headerlink" title="Classification of the lunar surface pattern by AI architectures: Does AI see a rabbit in the Moon?"></a>Classification of the lunar surface pattern by AI architectures: Does AI see a rabbit in the Moon?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11107">http://arxiv.org/abs/2308.11107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daigo Shoji</li>
<li>for: 这篇论文的目的是研究月面的颜色模式是否类似于兔子。</li>
<li>methods: 这篇论文使用了七种人工智能架构来评估月面颜色模式与兔子之间的相似性。</li>
<li>results: 测试结果显示，在某些地区，月面颜色模式更容易被识别为兔子，而不是人脸。此外，使用ImageNet权重时，ConvNeXt和CLIP occasionally可以归类月面颜色模式为兔子。<details>
<summary>Abstract</summary>
In Asian countries, there is a tradition that a rabbit (the Moon rabbit) lives on the Moon. As the origin of this tradition, usually, two reasons are mentioned. One reason is that the color pattern of the lunar surface is similar to the shape of a rabbit. The other reason is that both the Moon and rabbit are symbols of fertility because the Moon appears and disappears (i.e., waxing and waning) cyclically, and rabbits bear children frequently. Considering the latter reason, is the lunar surface color pattern not similar to a rabbit? Here, the similarity between rabbit and the lunar surface pattern was evaluated using seven AI architectures. In the test by CLIP, assuming that people look at the Moon in the early evening frequently, the lunar surface is more similar to a rabbit than a face at low latitude regions, while it can be classified as face as latitude increases, which is consistent with that the oldest literature about the Moon rabbit was written in India and that there is a culture of human's face in the Moon in Europe. Tested with ImageNet weights, ConvNeXt and CLIP sometimes classified the lunar surface pattern into rabbit with relatively high probabilities. Cultures are generated by our attitude to the environment. Both dynamic and static similarities may be required to induce our imagination.
</details>
<details>
<summary>摘要</summary>
在亚洲国家，有一传统认为月球上有兔子（月兔）生活。这个传统的起源通常被推断为两个理由。一个理由是月球表面的颜色排列与兔子的形状类似。另一个理由是月球和兔子都是生育的符号，因为月球出现和消失（即增减）的征例行为，兔子则经常产下幼崽。考虑后一个理由，月球表面的颜色排列与兔子是否类似？在这个问题上，我们使用了七种人工智能架构进行评估。在CLIP测试中，假设人们在晚上经常看到月球，那么月球表面的颜色排列更像兔子 than 人脸，而且随着纬度增加，月球表面可以被识别为人脸。这与最古老的月兔文学成果在印度被写成，以及欧洲文化中人类面部在月球上的存在相一致。使用ImageNet权重，ConvNeXt和CLIP occasionally将月球表面 Pattern classification为兔子，并且拥有相对高的概率。文化是由我们对环境的态度所生成的。我们可能需要同时考虑动态和静态相似性，以便让我们的想象力激发。
</details></li>
</ul>
<hr>
<h2 id="Recursive-Video-Lane-Detection"><a href="#Recursive-Video-Lane-Detection" class="headerlink" title="Recursive Video Lane Detection"></a>Recursive Video Lane Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11106">http://arxiv.org/abs/2308.11106</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongkwonjin/rvld">https://github.com/dongkwonjin/rvld</a></li>
<li>paper_authors: Dongkwon Jin, Dahyun Kim, Chang-Su Kim</li>
<li>for: 这篇论文提出了一种用于视频中检测路面线的新算法，即回归视频lane检测器（RVLD），用于在视频中检测路面线。</li>
<li>methods: 该算法包括一个内部lane检测器（ILD）和一个预测lane检测器（PLD）。首先，我们设计了ILD来在当前帧中地址路面线。然后，我们开发了PLD，以利用上一帧的信息来在当前帧中更可靠地检测路面线。为此，我们估算了运动场和将上一帧的输出折叠到当前帧中。使用折叠后的信息，我们精细地修改当前帧的特征图以更好地检测路面线。</li>
<li>results: 实验结果表明，RVLD在视频路面线数据集上的性能明显超过了现有的检测器。我们的代码可以在<a target="_blank" rel="noopener" href="https://github.com/dongkwonjin/RVLD%E4%B8%AD%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/dongkwonjin/RVLD中下载。</a><details>
<summary>Abstract</summary>
A novel algorithm to detect road lanes in videos, called recursive video lane detector (RVLD), is proposed in this paper, which propagates the state of a current frame recursively to the next frame. RVLD consists of an intra-frame lane detector (ILD) and a predictive lane detector (PLD). First, we design ILD to localize lanes in a still frame. Second, we develop PLD to exploit the information of the previous frame for lane detection in a current frame. To this end, we estimate a motion field and warp the previous output to the current frame. Using the warped information, we refine the feature map of the current frame to detect lanes more reliably. Experimental results show that RVLD outperforms existing detectors on video lane datasets. Our codes are available at https://github.com/dongkwonjin/RVLD.
</details>
<details>
<summary>摘要</summary>
“本文提出了一种新的算法检测视频中的路线，即回归视频车道检测器（RVLD）。这种算法在当前帧中进行回归状态，并在下一帧中使用这些状态来提高车道检测的准确性。RVLD由内帧车道检测器（ILD）和预测车道检测器（PLD）两部分组成。首先，我们设计了 ILD 以确定视频帧中的车道。其次，我们开发了 PLD，以利用上一帧的信息来提高当前帧中的车道检测。为此，我们对上一帧的视频进行了运动场景的估算，并将上一帧的输出折叠到当前帧中。使用折叠后的信息，我们可以更加精确地修改当前帧的特征图，以更好地检测车道。实验结果表明，RVLD 在视频车道数据集上的性能比既有的检测器更高。我们的代码可以在 GitHub 上找到：https://github.com/dongkwonjin/RVLD。”
</details></li>
</ul>
<hr>
<h2 id="MosaiQ-Quantum-Generative-Adversarial-Networks-for-Image-Generation-on-NISQ-Computers"><a href="#MosaiQ-Quantum-Generative-Adversarial-Networks-for-Image-Generation-on-NISQ-Computers" class="headerlink" title="MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers"></a>MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11096">http://arxiv.org/abs/2308.11096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Silver, Tirthak Patel, William Cutler, Aditya Ranjan, Harshitta Gandhi, Devesh Tiwari</li>
<li>for: 研究量子机器学习和视觉技术，尤其是量子图像生成技术，以提高图像质量和可靠性。</li>
<li>methods: 我们提出了一个名为MosaiQ的高质量量子图像生成GAN框架，可以在当今的中期级量子计算机（NISQ）上执行。</li>
<li>results: MosaiQ可以生成高质量的图像，并且可以在不同的图像生成任务中实现高度的可靠性和稳定性。<details>
<summary>Abstract</summary>
Quantum machine learning and vision have come to the fore recently, with hardware advances enabling rapid advancement in the capabilities of quantum machines. Recently, quantum image generation has been explored with many potential advantages over non-quantum techniques; however, previous techniques have suffered from poor quality and robustness. To address these problems, we introduce, MosaiQ, a high-quality quantum image generation GAN framework that can be executed on today's Near-term Intermediate Scale Quantum (NISQ) computers.
</details>
<details>
<summary>摘要</summary>
量子机器学习和视觉在最近几年来得到了更多的关注，各种硬件进步使得量子机器的能力得到了快速提升。近期，量子图像生成得到了广泛研究，但以前的技术受到了低质量和稳定性的限制。为了解决这些问题，我们介绍了 MosaiQ，一个高质量量子图像生成GAN框架，可以在当今的中等规模量子计算机上执行。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Fairness-and-Explainability-in-Image-Classification-Using-Optimal-Transport"><a href="#Addressing-Fairness-and-Explainability-in-Image-Classification-Using-Optimal-Transport" class="headerlink" title="Addressing Fairness and Explainability in Image Classification Using Optimal Transport"></a>Addressing Fairness and Explainability in Image Classification Using Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11090">http://arxiv.org/abs/2308.11090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Ratz, François Hu, Arthur Charpentier</li>
<li>for: 本研究旨在提高人工智能系统的可信worthiness和公平性，使其在医疗和警察等领域中建立信任和责任感。</li>
<li>methods: 本研究使用优化的运输理论来揭示图像中偏见的起源和后果，这种方法可以轻松扩展到表格数据中。</li>
<li>results: 研究发现，通过使用拟合度量来评估模型的偏见，可以独立地保持预测准确性和揭示偏见的起源。这些发现对于建立可信worthiness和公平性的人工智能系统具有重要意义。<details>
<summary>Abstract</summary>
Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we obtain scores that are independent of a sensitive variable but keep their marginal orderings. This step ensures predictive accuracy but also helps us to recover the regions most associated with the generation of the biases. Our findings hold significant implications for the development of trustworthy and unbiased AI systems, fostering transparency, accountability, and fairness in critical decision-making scenarios across diverse domains.
</details>
<details>
<summary>摘要</summary>
算法公平和可解释性是建立人工智能系统信任和负责任的关键因素，尤其在医疗和警察领域。虽然在每个领域 separately 有所进步，但在公平应用中实现可解释性仍然是挑战，特别是在使用深度神经网络时。同时，伦理数据挖掘已成为非常重要，因为无数次证明了不公平的算法会导致偏见的结果。现有的方法主要是减轻模型的偏见结果，但几乎没有尝试解释模型为何偏见。为了bridging这个差距，我们提出了一种全面的方法，利用最优运输理论来揭示偏见区域在图像中的原因和后果，这种方法可以轻松扩展到表格数据上。通过使用拓扑 Wasserstein 中心，我们可以获得不виси于敏感变量的分数，但保持其排序。这一步确保预测精度，同时帮助我们回归偏见区域的生成。我们的发现对于开发可靠、无偏的人工智能系统的发展有着深远的意义，推动了诚实、负责任和公平在多个领域中的决策过程中的透明度和公平。
</details></li>
</ul>
<hr>
<h2 id="Long-Term-Prediction-of-Natural-Video-Sequences-with-Robust-Video-Predictors"><a href="#Long-Term-Prediction-of-Natural-Video-Sequences-with-Robust-Video-Predictors" class="headerlink" title="Long-Term Prediction of Natural Video Sequences with Robust Video Predictors"></a>Long-Term Prediction of Natural Video Sequences with Robust Video Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11079">http://arxiv.org/abs/2308.11079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Ditria, Tom Drummond</li>
<li>for: 预测高维视频序列是一个非常困难的问题，因为可能的未来场景的数量会 exponential 增长随着时间的推移。特别是在从有限的世界Snapshot中预测自然的视频场景时，内在的不确定性会快速增加，使长期预测变得非常困难。</li>
<li>methods: 我们在这篇论文中引入了一些改进了现有工作，以创建Robust Video Predictors (RoViPs)。我们使用深度Perceptual和 uncertainty-based reconstructionloss来创建高质量短期预测。使用Attention-based skip connections以实现跨距离空间特征输入的长距离移动，以进一步提高性能。</li>
<li>results: 我们显示了使用单步预测任务iterated 可以生成非常长、自然的视频序列。<details>
<summary>Abstract</summary>
Predicting high dimensional video sequences is a curiously difficult problem. The number of possible futures for a given video sequence grows exponentially over time due to uncertainty. This is especially evident when trying to predict complicated natural video scenes from a limited snapshot of the world. The inherent uncertainty accumulates the further into the future you predict making long-term prediction very difficult. In this work we introduce a number of improvements to existing work that aid in creating Robust Video Predictors (RoViPs). We show that with a combination of deep Perceptual and uncertainty-based reconstruction losses we are able to create high quality short-term predictions. Attention-based skip connections are utilised to allow for long range spatial movement of input features to further improve performance. Finally, we show that by simply making the predictor robust to its own prediction errors, it is possible to produce very long, realistic natural video sequences using an iterated single-step prediction task.
</details>
<details>
<summary>摘要</summary>
Predicting high-dimensional video sequences is a challenging problem. The number of possible futures for a given video sequence grows exponentially with time due to uncertainty. This is particularly evident when trying to predict complex natural video scenes from a limited snapshot of the world. The inherent uncertainty accumulates the further into the future you predict, making long-term prediction very difficult. In this work, we introduce several improvements to existing methods that aid in creating Robust Video Predictors (RoViPs). We show that by combining deep perceptual and uncertainty-based reconstruction losses, we can create high-quality short-term predictions. Attention-based skip connections are used to allow for long-range spatial movement of input features, further improving performance. Finally, we show that by simply making the predictor robust to its own prediction errors, we can produce very long, realistic natural video sequences using an iterated single-step prediction task.Here's the text with some notes on the translation:* "high-dimensional" is translated as "高维的" (gāo wéi de), which is a common way to describe high-dimensional data in Chinese.* "video sequences" is translated as "视频序列" (pǐn yǐng xù xià), which is a literal translation of the English phrase.* "uncertainty" is translated as "不确定性" (bù jì dìng xìng), which is a common way to describe uncertainty in Chinese.* "complicated" is translated as "复杂的" (fù zhòng de), which is a common way to describe complex or intricate things in Chinese.* "natural video scenes" is translated as "自然的视频场景" (zì rán de pǐn yǐng chǎng jǐng), which is a literal translation of the English phrase.* "long-term prediction" is translated as "长期预测" (cháng qī yù tiè), which is a common way to describe long-term predictions in Chinese.* "iterated single-step prediction task" is translated as " iterate 单步预测任务" (pītī yī xiào yù jì), which is a literal translation of the English phrase.I hope this helps! Let me know if you have any further questions.
</details></li>
</ul>
<hr>
<h2 id="Audio-Visual-Class-Incremental-Learning"><a href="#Audio-Visual-Class-Incremental-Learning" class="headerlink" title="Audio-Visual Class-Incremental Learning"></a>Audio-Visual Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11073">http://arxiv.org/abs/2308.11073</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiguopian/av-cil_iccv2023">https://github.com/weiguopian/av-cil_iccv2023</a></li>
<li>paper_authors: Weiguo Pian, Shentong Mo, Yunhui Guo, Yapeng Tian</li>
<li>for: 这篇论文提出了一种 audio-visual 类增 learning 问题，即在 audio-visual 视频认知中进行类增 learning。</li>
<li>methods: 该论文提出了一种叫做 AV-CIL 的方法，该方法通过 dual-audio-visual 相似性约束 (D-AVSC) 和视觉注意力练化 (VAD) 来保持音频视频modalities之间的semantic similarity，并且能够在类增 learning 过程中 preserved previously learned audio-guided visual attentive ability。</li>
<li>results: 该论文的实验结果表明，AV-CIL 方法在 audio-visual 类增 learning 中 Significantly outperforms 现有的类增 learning 方法。<details>
<summary>Abstract</summary>
In this paper, we introduce audio-visual class-incremental learning, a class-incremental learning scenario for audio-visual video recognition. We demonstrate that joint audio-visual modeling can improve class-incremental learning, but current methods fail to preserve semantic similarity between audio and visual features as incremental step grows. Furthermore, we observe that audio-visual correlations learned in previous tasks can be forgotten as incremental steps progress, leading to poor performance. To overcome these challenges, we propose AV-CIL, which incorporates Dual-Audio-Visual Similarity Constraint (D-AVSC) to maintain both instance-aware and class-aware semantic similarity between audio-visual modalities and Visual Attention Distillation (VAD) to retain previously learned audio-guided visual attentive ability. We create three audio-visual class-incremental datasets, AVE-Class-Incremental (AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), and VGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, and VGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, and VS100-CI demonstrate that AV-CIL significantly outperforms existing class-incremental learning methods in audio-visual class-incremental learning. Code and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了音频视频类增长学习（Audio-Visual Class-Incremental Learning，AVCIL），这是一种类增长学习场景 для音频视频识别。我们示示了 joint audio-visual 模型可以提高类增长学习性能，但现有方法无法保持音频视频特征之间的semantic similarity，特别是在增量步骤增长时。此外，我们发现在前一个任务学习的音频视频相关性可以在后续任务学习过程中被忘记，导致性能下降。为了解决这些挑战，我们提出了 Dual-Audio-Visual Similarity Constraint（D-AVSC）和 Visual Attention Distillation（VAD）两种方法。我们创建了三个音频视频类增长数据集： AVE-Class-Incremental（AVE-CI）、Kinetics-Sounds-Class-Incremental（K-S-CI）和 VGGSound100-Class-Incremental（VS100-CI），基于 AVE、Kinetics-Sounds 和 VGGSound 数据集。我们在 AVE-CI、K-S-CI 和 VS100-CI 上进行了实验，并证明了 AV-CIL 在音频视频类增长学习中表现出色，超过了现有的类增长学习方法。代码和数据可以在 GitHub 上获取：https://github.com/weiguoPian/AV-CIL_ICCV2023。
</details></li>
</ul>
<hr>
<h2 id="TeD-SPAD-Temporal-Distinctiveness-for-Self-supervised-Privacy-preservation-for-video-Anomaly-Detection"><a href="#TeD-SPAD-Temporal-Distinctiveness-for-Self-supervised-Privacy-preservation-for-video-Anomaly-Detection" class="headerlink" title="TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection"></a>TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11072">http://arxiv.org/abs/2308.11072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/UCF-CRCV/TeD-SPAD">https://github.com/UCF-CRCV/TeD-SPAD</a></li>
<li>paper_authors: Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah</li>
<li>for: 这个研究的目的是为了提出一个具有隐私保护的视预测异常探测方法，以解决现有的人工监控不足和隐私泄露问题。</li>
<li>methods: 这个方法使用了一个自我监控的三元损害函数来增强时间特征，并且使用了一个具有隐私保护的数据隐藏技术来防止隐私泄露。</li>
<li>results: 这个方法在三个弱型监控的视预测异常探测 datasets（UCF-Crime、XD-Violence和ShanghaiTech）上取得了一个良好的平衡，即在保护隐私的同时，也能够维持视预测异常探测的性能。<details>
<summary>Abstract</summary>
Video anomaly detection (VAD) without human monitoring is a complex computer vision task that can have a positive impact on society if implemented successfully. While recent advances have made significant progress in solving this task, most existing approaches overlook a critical real-world concern: privacy. With the increasing popularity of artificial intelligence technologies, it becomes crucial to implement proper AI ethics into their development. Privacy leakage in VAD allows models to pick up and amplify unnecessary biases related to people's personal information, which may lead to undesirable decision making. In this paper, we propose TeD-SPAD, a privacy-aware video anomaly detection framework that destroys visual private information in a self-supervised manner. In particular, we propose the use of a temporally-distinct triplet loss to promote temporally discriminative features, which complements current weakly-supervised VAD methods. Using TeD-SPAD, we achieve a positive trade-off between privacy protection and utility anomaly detection performance on three popular weakly supervised VAD datasets: UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization model reduces private attribute prediction by 32.25% while only reducing frame-level ROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page: https://joefioresi718.github.io/TeD-SPAD_webpage/
</details>
<details>
<summary>摘要</summary>
“视频异常检测（VAD）无人监测是一项复杂的计算机视觉任务，如果成功实施，它将对社会产生积极的影响。然而，现有的大多数方法忽略了一个重要的现实问题：隐私。随着人工智能技术的普及，它们的开发中需要落实合适的人工智能伦理。VAD模型可以捕捉和强调人们个人信息的无关的偏见，导致不良决策。在这篇论文中，我们提出了一种隐私意识视频异常检测框架，即TeD-SPAD。具体来说，我们提出使用时间特征分别的 triplet损失来促进时间特征分别，这与现有的弱监测VAD方法相 complement。使用TeD-SPAD，我们在三个流行的弱监测VAD数据集上实现了隐私保护和异常检测性能的积极负作用。我们的提出的匿名化模型可以降低人员特征预测值32.25%，只减某些数据集的异常检测精度3.69%。项目页面：https://joefioresi718.github.io/TeD-SPAD_webpage/”
</details></li>
</ul>
<hr>
<h2 id="MetaGCD-Learning-to-Continually-Learn-in-Generalized-Category-Discovery"><a href="#MetaGCD-Learning-to-Continually-Learn-in-Generalized-Category-Discovery" class="headerlink" title="MetaGCD: Learning to Continually Learn in Generalized Category Discovery"></a>MetaGCD: Learning to Continually Learn in Generalized Category Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11063">http://arxiv.org/abs/2308.11063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanan Wu, Zhixiang Chi, Yang Wang, Songhe Feng</li>
<li>for: 本研究的目的是解决一种实际场景，在训练过程中遇到未标注的数据，该数据包含已知和新类的混合类。目标是不断发现新类，同时保持已知类的性能。我们称之为 Continual Generalized Category Discovery (C-GCD) Setting。</li>
<li>methods: 我们提出了一种方法，即 MetaGCD，以便在 C-GCD  Setting 中不断发现新类而不忘记已知类。我们使用了一个元学习框架，并利用了在线标注数据来模拟测试增量学习过程。我们定义了一个元目标，旨在同时解决两个矛盾的学习目标，以实现不断发现新类而不忘记已知类。此外，我们还提出了一种软邻域基于对比网络，以便区分无关图像而吸引相关图像。</li>
<li>results: 我们在三个广泛使用的标准测试benchmark上建立了强大的基准，并进行了广泛的实验。我们的方法在 C-GCD  Setting 中表现出色，可以不断发现新类而不忘记已知类。<details>
<summary>Abstract</summary>
In this paper, we consider a real-world scenario where a model that is trained on pre-defined classes continually encounters unlabeled data that contains both known and novel classes. The goal is to continually discover novel classes while maintaining the performance in known classes. We name the setting Continual Generalized Category Discovery (C-GCD). Existing methods for novel class discovery cannot directly handle the C-GCD setting due to some unrealistic assumptions, such as the unlabeled data only containing novel classes. Furthermore, they fail to discover novel classes in a continual fashion. In this work, we lift all these assumptions and propose an approach, called MetaGCD, to learn how to incrementally discover with less forgetting. Our proposed method uses a meta-learning framework and leverages the offline labeled data to simulate the testing incremental learning process. A meta-objective is defined to revolve around two conflicting learning objectives to achieve novel class discovery without forgetting. Furthermore, a soft neighborhood-based contrastive network is proposed to discriminate uncorrelated images while attracting correlated images. We build strong baselines and conduct extensive experiments on three widely used benchmarks to demonstrate the superiority of our method.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们考虑了一个真实世界场景，在这个场景中，一个已经训练过的模型在不断接触到预先定义的类和未经标注的数据中遇到了新类。目标是同时发现新类并保持已知类的性能。我们称这种设定为总类发现（C-GCD）。现有的新类发现方法无法直接处理这种设定，这是因为它们假设了尚未标注的数据只包含新类。此外，它们也无法在不断发现新类的情况下保持已知类的性能。在这种工作中，我们终止了这些假设，并提出了一种方法，称为MetaGCD，以incremental learning来发现新类而减少忘记。我们的提出的方法使用了meta-学框架，利用了在线标注数据来模拟测试增量学习过程。我们定义了一个meta-目标，旨在在新类发现和已知类性能之间协调两个不同的学习目标。此外，我们还提出了一种软邻域基于的对比网络，以便在不同类型之间分辨细分图像。我们建立了强大的基elines并进行了广泛的实验，以证明我们的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="UnLoc-A-Unified-Framework-for-Video-Localization-Tasks"><a href="#UnLoc-A-Unified-Framework-for-Video-Localization-Tasks" class="headerlink" title="UnLoc: A Unified Framework for Video Localization Tasks"></a>UnLoc: A Unified Framework for Video Localization Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11062">http://arxiv.org/abs/2308.11062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/scenic">https://github.com/google-research/scenic</a></li>
<li>paper_authors: Shen Yan, Xuehan Xiong, Arsha Nagrani, Anurag Arnab, Zhonghao Wang, Weina Ge, David Ross, Cordelia Schmid</li>
<li>for: 这个研究的目的是提出一种新的视频地理ocalization方法，用于在未经trim的视频中进行时间地理ocalization。</li>
<li>methods: 这个方法使用预训练的图像和文本楼层，并将 tokens 传递给一个视频-文本融合模型。输出的融合模型输出将用于构建一个特征 пирамид，每个层与一个头相连，以预测每帧的相关性分数和开始&#x2F;结束时间偏移。</li>
<li>results: 这个方法可以实现视频地理ocalization、时间地理ocalization和动作分割等三个任务，并且在所有三个任务中达到了现有最佳Result。<details>
<summary>Abstract</summary>
While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \url{https://github.com/google-research/scenic}.
</details>
<details>
<summary>摘要</summary>
大规模图像文本预训练模型，如CLIP，已经在剪辑后的视频上进行多种任务，但是它们在未剪辑视频中的时间本地化仍然是一个未解决的问题。我们设计了一种新的方法called UnLoc，它使用预训练的图像和文本楼层，并将Token传递给视频-文本融合模型。融合模块的输出被用construct一个功能PYRAMID，每个层与一个头连接，以预测每帧的相关性分数和开始/结束时间偏移。与之前的方法不同，我们的体系允许场景检索、时间本地化和动作分割使用单一的阶段模型，不需要动作提案、运动基于预训练特征或表示掩码。与专门的模型不同，我们在所有三种本地化任务中达到了状态arta的结果，代码将在：\url{https://github.com/google-research/scenic}上提供。
</details></li>
</ul>
<hr>
<h2 id="Harmonization-Across-Imaging-Locations-HAIL-One-Shot-Learning-for-Brain-MRI"><a href="#Harmonization-Across-Imaging-Locations-HAIL-One-Shot-Learning-for-Brain-MRI" class="headerlink" title="Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI"></a>Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11047">http://arxiv.org/abs/2308.11047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijeet Parida, Zhifan Jiang, Syed Muhammad Anwar, Nicholas Foreman, Nicholas Stence, Michael J. Fisher, Roger J. Packer, Robert A. Avery, Marius George Linguraru</li>
<li>for: 这篇论文旨在提出一种用于类比较罕见疾病，如儿童脑肿瘤的机器学习基于医疗影像诊断和预测方法。</li>
<li>methods: 本论文使用了生成对抗网络（GANs）进行深度学习预测，并提出了一种一击学习方法，使用神经风格转移来调整医疗影像的强度标准。</li>
<li>results: 实验结果显示，该方法可以保持病人解剖结构，并调整影像强度以适应新的诊疗所需。此外，该方法可以在未见到训练数据的情况下进行应用，因此具有实际应用和临床试验的价值。<details>
<summary>Abstract</summary>
For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmonization approaches with evaluation metrics that both measure image style harmonization and assess the preservation of anatomical structures. Experimental results demonstrate the effectiveness of our method in preserving patient anatomy while adjusting the image intensities to a new clinical site. Our general harmonization model can be used on unseen data from new sites, making it a valuable tool for real-world medical applications and clinical trials.
</details>
<details>
<summary>摘要</summary>
Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. We also propose a novel strategy to evaluate the effectiveness of image harmonization approaches with evaluation metrics that both measure image style harmonization and assess the preservation of anatomical structures.Experimental results demonstrate the effectiveness of our method in preserving patient anatomy while adjusting the image intensities to a new clinical site. Our general harmonization model can be used on unseen data from new sites, making it a valuable tool for real-world medical applications and clinical trials.Here's the Simplified Chinese translation:为了使用机器学习来诊断和预测罕见疾病，如儿童脑肿瘤，需要从多个临床Site收集医学成像数据，这些数据可能使用不同的设备和协议。但是，使用生成对抗网络（GANs）进行深度学习驱动的成像融合可能会导致“幻觉”现象，即生成不存在于训练数据中的 pseudo 结构。为了避免幻觉在医学成像中，我们提议一种一键学习方法，利用神经风格传输来融合。在测试时，方法使用一个来自临床Site的图像，通过神经风格传输来生成医学成像，以适应新的临床Site的INTENSITY规模。我们还提出了一种新的评估图像融合方法的效果的策略，该策略包括评估图像风格融合和评估结构保持。实验结果表明，我们的方法可以保持患者的解剖结构，同时调整图像INTENSITY来适应新的临床Site。我们的通用融合模型可以在新的Site上使用未看过的数据，因此它是实际医疗应用和临床试验中的有价值工具。
</details></li>
</ul>
<hr>
<h2 id="Coordinate-Quantized-Neural-Implicit-Representations-for-Multi-view-Reconstruction"><a href="#Coordinate-Quantized-Neural-Implicit-Representations-for-Multi-view-Reconstruction" class="headerlink" title="Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction"></a>Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11025">http://arxiv.org/abs/2308.11025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/machineperceptionlab/cq-nir">https://github.com/machineperceptionlab/cq-nir</a></li>
<li>paper_authors: Sijia Jiang, Jing Hua, Zhizhong Han</li>
<li>for: 用于学习神经隐式表示法从多视图图像中获取3D重建</li>
<li>methods: 使用量化坐标为神经网络中的输入，并使用离散坐标和其позициональ编码来学习隐式函数</li>
<li>results: 提高了多视图一致性约束，并且不会增加计算负担，在最新的方法上显示了超过状态艺术的优势<details>
<summary>Abstract</summary>
In recent years, huge progress has been made on learning neural implicit representations from multi-view images for 3D reconstruction. As an additional input complementing coordinates, using sinusoidal functions as positional encodings plays a key role in revealing high frequency details with coordinate-based neural networks. However, high frequency positional encodings make the optimization unstable, which results in noisy reconstructions and artifacts in empty space. To resolve this issue in a general sense, we introduce to learn neural implicit representations with quantized coordinates, which reduces the uncertainty and ambiguity in the field during optimization. Instead of continuous coordinates, we discretize continuous coordinates into discrete coordinates using nearest interpolation among quantized coordinates which are obtained by discretizing the field in an extremely high resolution. We use discrete coordinates and their positional encodings to learn implicit functions through volume rendering. This significantly reduces the variations in the sample space, and triggers more multi-view consistency constraints on intersections of rays from different views, which enables to infer implicit function in a more effective way. Our quantized coordinates do not bring any computational burden, and can seamlessly work upon the latest methods. Our evaluations under the widely used benchmarks show our superiority over the state-of-the-art. Our code is available at https://github.com/MachinePerceptionLab/CQ-NIR.
</details>
<details>
<summary>摘要</summary>
Recently, there have been significant advancements in learning neural implicit representations from multi-view images for 3D reconstruction. Using sinusoidal functions as positional encodings has proven to be crucial in revealing high-frequency details with coordinate-based neural networks. However, the use of high-frequency positional encodings can lead to unstable optimization, resulting in noisy reconstructions and artifacts in empty space. To address this issue, we propose learning neural implicit representations with quantized coordinates, which reduces uncertainty and ambiguity in the field during optimization. Instead of using continuous coordinates, we discretize them into discrete coordinates using nearest interpolation among quantized coordinates, which are obtained by discretizing the field in an extremely high resolution. We then use these discrete coordinates and their positional encodings to learn implicit functions through volume rendering, significantly reducing variations in the sample space and triggering more multi-view consistency constraints on intersections of rays from different views, enabling more effective inference of implicit functions. Our quantized coordinates do not increase computational burden and can seamlessly work with the latest methods. Our evaluations on widely used benchmarks show our superiority over the state-of-the-art. Our code is available at https://github.com/MachinePerceptionLab/CQ-NIR.
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Hypergraphs-for-Semi-supervised-Learning-using-Earth-Observations"><a href="#Multi-Task-Hypergraphs-for-Semi-supervised-Learning-using-Earth-Observations" class="headerlink" title="Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations"></a>Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11021">http://arxiv.org/abs/2308.11021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihai Pirvu, Alina Marcu, Alexandra Dobrescu, Nabil Belbachir, Marius Leordeanu</li>
<li>for: 这个论文是为了解决多任务学习中数据缺失问题，特别是在地球观测领域，where ground-truth data is often missing.</li>
<li>methods: 该论文提出了一种多任务hypergraphSelf-supervised learning方法，其中每个节点是一个任务，不同的路径通过hypergraph到达给定任务都成为了无监督教师，并通过 ensemble 学习生成可靠的pseudolabels。</li>
<li>results: 经过对NASA NEO数据集的广泛实验，论文示出了其多任务半监督方法的价值，包括在强基elines和最近的工作上的一致提升。此外，论文还表明了hypergraph可以适应不监督数据分布变化，并可靠地恢复缺失数据，以及其可以在多个观测层次上适应数据缺失情况。<details>
<summary>Abstract</summary>
There are many ways of interpreting the world and they are highly interdependent. We exploit such complex dependencies and introduce a powerful multi-task hypergraph, in which every node is a task and different paths through the hypergraph reaching a given task become unsupervised teachers, by forming ensembles that learn to generate reliable pseudolabels for that task. Each hyperedge is part of an ensemble teacher for a given task and it is also a student of the self-supervised hypergraph system. We apply our model to one of the most important problems of our times, that of Earth Observation, which is highly multi-task and it often suffers from missing ground-truth data. By performing extensive experiments on the NASA NEO Dataset, spanning a period of 22 years, we demonstrate the value of our multi-task semi-supervised approach, by consistent improvements over strong baselines and recent work. We also show that the hypergraph can adapt unsupervised to gradual data distribution shifts and reliably recover, through its multi-task self-supervision process, the missing data for several observational layers for up to seven years.
</details>
<details>
<summary>摘要</summary>
世界上有很多方法来解释，它们之间很高度相互依赖。我们利用这些复杂的依赖关系，引入一个强大的多任务超графи，其中每个节点是一个任务，不同的路径通过超графи到达某个任务就会成为无监督教师。每个超边都是一个任务的ensemble教师，同时也是自我超vised系统的学生。我们应用我们的模型到现代时代最重要的问题之一：地球观测，这是一个高度多任务的问题，经常缺少真实数据。通过对NASA NEO数据集进行广泛的实验，覆盖22年时间段，我们展示了我们的多任务半超监教学方法的价值，通过与强基线和最新研究的相对比较，我们的模型在多个任务上具有稳定和可靠的表现。此外，我们还证明了超графи可以适应无监督数据分布变化，通过自我超vision过程，可以重新生成多个观测层的缺失数据，保持稳定的表现，达到7年之久。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Graphormer-Spectral-Graph-based-Transformer-for-Egocentric-Two-Hand-Reconstruction-using-Multi-View-Color-Images"><a href="#Spectral-Graphormer-Spectral-Graph-based-Transformer-for-Egocentric-Two-Hand-Reconstruction-using-Multi-View-Color-Images" class="headerlink" title="Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images"></a>Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11015">http://arxiv.org/abs/2308.11015</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eldentse/Spectral-Graphormer">https://github.com/eldentse/Spectral-Graphormer</a></li>
<li>paper_authors: Tze Ho Elden Tse, Franziska Mueller, Zhengyang Shen, Danhang Tang, Thabo Beeler, Mingsong Dou, Yinda Zhang, Sasa Petrovic, Hyung Jin Chang, Jonathan Taylor, Bardia Doosti</li>
<li>For:  reconstruction of two high fidelity hands from multi-view RGB images* Methods: transformer-based framework with spectral graph convolution decoder and optimization-based refinement stage* Results: realistic two-hand reconstructions with physical plausibility and generalization to real data, as well as real-time AR&#x2F;VR applications.Here’s the full summary in Simplified Chinese:</li>
<li>for: 重构两个高精度手掌从多视图RGB图像中</li>
<li>methods: 使用变换器基于框架，并使用spectral graph convolution decoder和优化基于的反馈阶段</li>
<li>results: 生成真实的两手重构，并实现数据实际性和AR&#x2F;VR应用中的实时渲染。<details>
<summary>Abstract</summary>
We propose a novel transformer-based framework that reconstructs two high fidelity hands from multi-view RGB images. Unlike existing hand pose estimation methods, where one typically trains a deep network to regress hand model parameters from single RGB image, we consider a more challenging problem setting where we directly regress the absolute root poses of two-hands with extended forearm at high resolution from egocentric view. As existing datasets are either infeasible for egocentric viewpoints or lack background variations, we create a large-scale synthetic dataset with diverse scenarios and collect a real dataset from multi-calibrated camera setup to verify our proposed multi-view image feature fusion strategy. To make the reconstruction physically plausible, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder to smoothen the meshes during upsampling and (ii) an optimisation-based refinement stage at inference to prevent self-penetrations. Through extensive quantitative and qualitative evaluations, we show that our framework is able to produce realistic two-hand reconstructions and demonstrate the generalisation of synthetic-trained models to real data, as well as real-time AR/VR applications.
</details>
<details>
<summary>摘要</summary>
我们提出了一种基于转换器的新框架，可以从多视角RGB图像中重建高精度两只手。与现有的手姿估计方法不同，我们处理一个更加复杂的问题Setting，即从 egocentric 视角直接将两只手的绝对根姿 Parameters 高分辨率RGB图像中进行重建。由于现有的数据集不可能进行 egocentric 视角或缺乏背景变化，我们创建了一个大规模的 simulate 数据集和实际数据集，以验证我们的多视角图像特征融合策略。为使重建 Physically plausible，我们提出了两种策略：（i）一种粗到细 spectral 图像 conv 嵌入器来平滑网格时的抗锯齿，以及（ii）在推理时进行优化基于反射 stage 以避免自身穿孔。经过广泛的量化和质量评估，我们表明我们的框架可以生成真实的两只手重建，并示出了 synthetic 训练的模型在实际数据上的普适性，以及实时 AR/VR 应用。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Detection-of-Methane-Emissions-in-Multispectral-Satellite-Data-Using-Deep-Learning"><a href="#Autonomous-Detection-of-Methane-Emissions-in-Multispectral-Satellite-Data-Using-Deep-Learning" class="headerlink" title="Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning"></a>Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11003">http://arxiv.org/abs/2308.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bertrand Rouet-Leduc, Thomas Kerdreux, Alexandre Tuel, Claudia Hulbert</li>
<li>for: 监控全球暖化的一种重要方法，减少温室气体的排放</li>
<li>methods: 使用深度学习方法自动识别几何图像中的甲烷泄漏</li>
<li>results: 比前一代多spectrum甲烷数据产品降低假阳性率，无需对潜在泄漏地点有专业知识<details>
<summary>Abstract</summary>
Methane is one of the most potent greenhouse gases, and its short atmospheric half-life makes it a prime target to rapidly curb global warming. However, current methane emission monitoring techniques primarily rely on approximate emission factors or self-reporting, which have been shown to often dramatically underestimate emissions. Although initially designed to monitor surface properties, satellite multispectral data has recently emerged as a powerful method to analyze atmospheric content. However, the spectral resolution of multispectral instruments is poor, and methane measurements are typically very noisy. Methane data products are also sensitive to absorption by the surface and other atmospheric gases (water vapor in particular) and therefore provide noisy maps of potential methane plumes, that typically require extensive human analysis. Here, we show that the image recognition capabilities of deep learning methods can be leveraged to automatize the detection of methane leaks in Sentinel-2 satellite multispectral data, with dramatically reduced false positive rates compared with state-of-the-art multispectral methane data products, and without the need for a priori knowledge of potential leak sites. Our proposed approach paves the way for the automated, high-definition and high-frequency monitoring of point-source methane emissions across the world.
</details>
<details>
<summary>摘要</summary>
偏二氢（methane）是全球暖化气体中最强大的一种，它的大气半衰期短，使其成为快速降低全球暖化的目标。然而，当前的偏二氢泄漏监测技术主要基于估算性的泄漏因素或者自我报告，这些估算经常会很大的下降估计。虽然初始设计用于观察地面特性，但卫星多spectral数据最近才被发现作为分析大气内容的 poderoso方法。然而，多spectral工具的 spectral resolution很差，偏二氢测量通常很吵闹。偏二氢数据产品也受到地面和其他大气气体（水蒸气特别是）的吸收，因此提供的偏二氢潮区图像通常需要人工分析。在这里，我们展示了深度学习方法的图像识别能力可以自动化卫星Sentinel-2多spectral数据中的偏二氢泄漏检测，与现有的多spectral偏二氢数据产品相比， false positive 率有所下降，而无需先知泄漏点位置的假设。我们的提出的方法开 up a new way for the automatic, high-definition and high-frequency monitoring of point-source methane emissions around the world.
</details></li>
</ul>
<hr>
<h2 id="Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation"><a href="#Switched-auxiliary-loss-for-robust-training-of-transformer-models-for-histopathological-image-segmentation" class="headerlink" title="Switched auxiliary loss for robust training of transformer models for histopathological image segmentation"></a>Switched auxiliary loss for robust training of transformer models for histopathological image segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10994">http://arxiv.org/abs/2308.10994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mustaffa Hussain, Saharsh Barve</li>
<li>For: 本研究旨在提供一种模型，用于分类多个器官Functional Tissue Units (FTUs)的cell population neighborhoods，以便帮助病理学家更好地理解人体疾病的影响。* Methods: 该模型使用HuBMAP + HPA - Hacking the Human Body competition dataset进行训练，并提出了shifted auxiliary loss来解决深度模型的减速问题。* Results: 该模型在公共数据集上取得了0.793的dice分数，而在私有数据集上取得了0.778的dice分数，与传统方法相比，该方法提供了1%的提升。这些结果表明 transformers 模型在医学图像分析中的粗粒预测任务中的表现非常出色，并且可以帮助我们更好地理解人体细胞和组织的关系，从而更好地理解人体健康的影响。<details>
<summary>Abstract</summary>
Functional tissue Units (FTUs) are cell population neighborhoods local to a particular organ performing its main function. The FTUs provide crucial information to the pathologist in understanding the disease affecting a particular organ by providing information at the cellular level. In our research, we have developed a model to segment multi-organ FTUs across 5 organs namely: the kidney, large intestine, lung, prostate and spleen by utilizing the HuBMAP + HPA - Hacking the Human Body competition dataset. We propose adding shifted auxiliary loss for training models like the transformers to overcome the diminishing gradient problem which poses a challenge towards optimal training of deep models. Overall, our model achieved a dice score of 0.793 on the public dataset and 0.778 on the private dataset and shows a 1% improvement with the use of the proposed method. The findings also bolster the use of transformers models for dense prediction tasks in the field of medical image analysis. The study assists in understanding the relationships between cell and tissue organization thereby providing a useful medium to look at the impact of cellular functions on human health.
</details>
<details>
<summary>摘要</summary>
功能组织单元（FTU）是指器官本地的细胞群聚，提供了病理学家理解器官疾病的关键信息。在我们的研究中，我们开发了一种方法来在5种器官（肾脏、大小肠、肺、肾脏和脾脏）的FTU中进行多器官分割，使用了HuBMAP+HPA-Hacking the Human Body竞赛数据集。我们提议在训练模型时使用偏移 auxiliary loss，以解决深度模型训练过程中的减少梯度问题。总的来说，我们的模型在公共数据集上 achievement了0.793的 dice 分数，在私有数据集上 achievement了0.778的 dice 分数，与使用我们提议的方法相比，提高了1%。这些结果也证明了transformers模型在医学影像分析中的稠密预测任务中的可靠性。本研究帮助我们理解细胞和组织结构之间的关系，从而提供了一个有用的媒体来查看细胞功能对人类健康的影响。
</details></li>
</ul>
<hr>
<h2 id="Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations"><a href="#Debiasing-Counterfactuals-In-the-Presence-of-Spurious-Correlations" class="headerlink" title="Debiasing Counterfactuals In the Presence of Spurious Correlations"></a>Debiasing Counterfactuals In the Presence of Spurious Correlations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10984">http://arxiv.org/abs/2308.10984</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amar Kumar, Nima Fathi, Raghav Mehta, Brennan Nichyporuk, Jean-Pierre R. Falet, Sotirios Tsaftaris, Tal Arbel</li>
<li>for: The paper is written for the task of medical imaging classification, specifically addressing the issue of deep learning models relying on spurious correlations in the training data.</li>
<li>methods: The paper proposes an end-to-end training framework that integrates popular debiasing classifiers (such as distributionally robust optimization) with counterfactual image generation to expose generalizable imaging markers of relevance to the task, and a novel metric (Spurious Correlation Latching Score) to quantify the extent of classifier reliance on spurious correlations.</li>
<li>results: The paper demonstrates through comprehensive experiments on two public datasets (with simulated and real visual artifacts) that the debiasing method (i) learns generalizable markers across the population and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.Here’s the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了医学成像分类任务而写的，特别是处理深度学习模型在训练数据中遇到的假 correlate 问题。</li>
<li>methods: 这篇论文提出了一种结合流行的偏差纠正分类器（如分布式稳定优化）和对假 correlate 进行抗衡的末端训练框架，以及一个新的指标（假 correlate 抓取得分）来衡量分类器偏差的程度。</li>
<li>results: 这篇论文通过对公共数据集（包括模拟和实际视觉杂质）进行了广泛的实验，证明了偏差方法可以（一）学习人口中的通用标记，并（二）忽略假 correlate 并专注于下面疾病生物学。<details>
<summary>Abstract</summary>
Deep learning models can perform well in complex medical imaging classification tasks, even when basing their conclusions on spurious correlations (i.e. confounders), should they be prevalent in the training dataset, rather than on the causal image markers of interest. This would thereby limit their ability to generalize across the population. Explainability based on counterfactual image generation can be used to expose the confounders but does not provide a strategy to mitigate the bias. In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. Additionally, we propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images. Through comprehensive experiments on two public datasets (with the simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
</details>
<details>
<summary>摘要</summary>
In this work, we introduce the first end-to-end training framework that integrates both (i) popular debiasing classifiers (e.g. distributionally robust optimization (DRO)) to avoid latching onto the spurious correlations and (ii) counterfactual image generation to unveil generalizable imaging markers of relevance to the task. We also propose a novel metric, Spurious Correlation Latching Score (SCLS), to quantify the extent of the classifier reliance on the spurious correlation as exposed by the counterfactual images.Through comprehensive experiments on two public datasets (with simulated and real visual artifacts), we demonstrate that the debiasing method: (i) learns generalizable markers across the population, and (ii) successfully ignores spurious correlations and focuses on the underlying disease pathology.
</details></li>
</ul>
<hr>
<h2 id="VQA-Therapy-Exploring-Answer-Differences-by-Visually-Grounding-Answers"><a href="#VQA-Therapy-Exploring-Answer-Differences-by-Visually-Grounding-Answers" class="headerlink" title="VQA Therapy: Exploring Answer Differences by Visually Grounding Answers"></a>VQA Therapy: Exploring Answer Differences by Visually Grounding Answers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11662">http://arxiv.org/abs/2308.11662</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ccychongyanchen/vqatherapycrowdsourcing">https://github.com/ccychongyanchen/vqatherapycrowdsourcing</a></li>
<li>paper_authors: Chongyan Chen, Samreen Anjum, Danna Gurari</li>
<li>for: 这篇论文是关于视觉问答任务的研究，旨在更好地理解不同人对同一张图片的问题提出不同的答案的原因。</li>
<li>methods: 该论文引入了第一个可视地将每个答案与每个视觉问题相关联的数据集，称为VQAAnswerTherapy。然后，该论文提出了两个新的问题：一是判断视觉问题是否有唯一的答案基础，二是找到所有答案基础的地方。</li>
<li>results: 该论文使用现代算法对这两个新问题进行了评估，以示其在这些问题上的成功和缺点。数据集和评估服务器可以在<a target="_blank" rel="noopener" href="https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%8F%96%E3%80%82">https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/上公开获取。</a><details>
<summary>Abstract</summary>
Visual question answering is a task of predicting the answer to a question about an image. Given that different people can provide different answers to a visual question, we aim to better understand why with answer groundings. We introduce the first dataset that visually grounds each unique answer to each visual question, which we call VQAAnswerTherapy. We then propose two novel problems of predicting whether a visual question has a single answer grounding and localizing all answer groundings. We benchmark modern algorithms for these novel problems to show where they succeed and struggle. The dataset and evaluation server can be found publicly at https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/.
</details>
<details>
<summary>摘要</summary>
“视觉问答是一项任务，旨在预测图像上的问题的答案。由于不同的人可能对同一个视觉问题提供不同的答案，我们想要更好地理解这些答案的原因。我们引入了首个可视地固定每个独特答案的视觉问题数据集，称之为VQAAnswerTherapy。我们then提出了两个新的问题：可视地判断问题是否有唯一的答案固定，以及本地化所有答案固定。我们对现代算法进行了测试，以示其在这些新问题上的表现。数据集和评估服务器可以在https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/上公共获取。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="SupEuclid-Extremely-Simple-High-Quality-OoD-Detection-with-Supervised-Contrastive-Learning-and-Euclidean-Distance"><a href="#SupEuclid-Extremely-Simple-High-Quality-OoD-Detection-with-Supervised-Contrastive-Learning-and-Euclidean-Distance" class="headerlink" title="SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance"></a>SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10973">http://arxiv.org/abs/2308.10973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jarrod Haas</li>
<li>for: 本研究旨在提出一种简单而有效的Out-of-Distribution（OoD）检测方法，可以在标准benchmark上达到state-of-the-art的结果。</li>
<li>methods: 本研究使用Supervised Contrastive Learning（SCL）将ResNet18进行训练，并使用Euclidean distance作为分数规则进行评价。</li>
<li>results: 研究发现，使用SCL训练的ResNet18可以在近和远OoD检测benchmark上达到state-of-the-art的结果，无需使用更复杂的方法或更大的模型。<details>
<summary>Abstract</summary>
Out-of-Distribution (OoD) detection has developed substantially in the past few years, with available methods approaching, and in a few cases achieving, perfect data separation on standard benchmarks. These results generally involve large or complex models, pretraining, exposure to OoD examples or extra hyperparameter tuning. Remarkably, it is possible to achieve results that can exceed many of these state-of-the-art methods with a very simple method. We demonstrate that ResNet18 trained with Supervised Contrastive Learning (SCL) produces state-of-the-art results out-of-the-box on near and far OoD detection benchmarks using only Euclidean distance as a scoring rule. This may obviate the need in some cases for more sophisticated methods or larger models, and at the very least provides a very strong, easy to use baseline for further experimentation and analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MRI-Field-transfer-Reconstruction-with-Limited-Data-Regularization-by-Neural-Style-Transfer"><a href="#MRI-Field-transfer-Reconstruction-with-Limited-Data-Regularization-by-Neural-Style-Transfer" class="headerlink" title="MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer"></a>MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10968">http://arxiv.org/abs/2308.10968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyao Shen, Yancheng Zhu, Hernan Jara, Sean B. Andersson, Chad W. Farris, Stephan Anderson, Xin Zhang</li>
<li>for: 高品质的MRI重建方法</li>
<li>methods: 使用深度学习模型和风格传递的减噪方法</li>
<li>results: 使用 клиничеMRI扫描数据，能够显著提高图像质量<details>
<summary>Abstract</summary>
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:近期研究表明，深度学习基于模型可以成功地进行MRI重建。然而，大多数报道的方法需要任务特定、大规模的数据集来进行训练。噪声除掉（RED）是一个通用管道，它将噪声除掉作为图像重建的前提。RED已经在多种图像相关任务中展示出色，如噪声除掉、锐化和超分辨率。在这个工作中，我们提出了基于神经风格传输的噪声除掉方法（RNST），以更好地利用神经传输和噪声除掉引擎中的前提。这使得RNST可以从噪声低质量图像中重建高质量图像，并且可以处理不同的图像风格和有限的数据。我们验证了RNST使用临床MRI扫描数据，并证明了RNST可以显著提高图像质量。我们的结果表明RNST框架可以用于MRI重建，并且可能在有限数据情况下实现高质量重建。
</details></li>
</ul>
<hr>
<h2 id="BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation"><a href="#BundleSeg-A-versatile-reliable-and-reproducible-approach-to-white-matter-bundle-segmentation" class="headerlink" title="BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation"></a>BundleSeg: A versatile, reliable and reproducible approach to white matter bundle segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10958">http://arxiv.org/abs/2308.10958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etienne St-Onge, Kurt G Schilling, Francois Rheault</li>
<li>for: 提供一种可靠、可重复、快速的白 matter 路径EXTRACTING方法</li>
<li>methods:  combinest 一种迭代注册过程与一种新发展的精确流线搜索算法，以高效地分割流线无需 трактogram clustering 或简化假设</li>
<li>results: 比state-of-the-art segmentation方法具有改进的重复性和复制性，以及显著的速度提高。 通过增强精度和减少变化，提供了一种 valuabe 的工具 для neuroscience 研究，提高了轨迹学研究中white matter pathways的敏感性和特异性。<details>
<summary>Abstract</summary>
This work presents BundleSeg, a reliable, reproducible, and fast method for extracting white matter pathways. The proposed method combines an iterative registration procedure with a recently developed precise streamline search algorithm that enables efficient segmentation of streamlines without the need for tractogram clustering or simplifying assumptions. We show that BundleSeg achieves improved repeatability and reproducibility than state-of-the-art segmentation methods, with significant speed improvements. The enhanced precision and reduced variability in extracting white matter connections offer a valuable tool for neuroinformatic studies, increasing the sensitivity and specificity of tractography-based studies of white matter pathways.
</details>
<details>
<summary>摘要</summary>
这个研究提出了一种可靠、可重复、快速的白 mater 血管路径EXTRACTING方法，称为BundleSeg。该方法结合了迭代注册过程和最近开发的精炼流线搜索算法，可以高效地 segments 流线，无需进行跟踪ogram clustering 或假设。我们展示了 BundleSeg 在EXTRACTING white matter 血管路径上的重复性和复制性得到了提高，同时速度也得到了显著提高。这种更高精度和减少的变化可以为 neuroscience 研究提供一个有价值的工具，提高了追踪学基于 white matter 血管路径的敏感性和特异性。
</details></li>
</ul>
<hr>
<h2 id="CamP-Camera-Preconditioning-for-Neural-Radiance-Fields"><a href="#CamP-Camera-Preconditioning-for-Neural-Radiance-Fields" class="headerlink" title="CamP: Camera Preconditioning for Neural Radiance Fields"></a>CamP: Camera Preconditioning for Neural Radiance Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10902">http://arxiv.org/abs/2308.10902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, Ricardo Martin-Brualla</li>
<li>for: 高精度3D场景重建</li>
<li>methods: 使用Proxy问题计算抑制器，并使用该抑制器作为预Conditioner进行相机参数优化</li>
<li>results: 对Mip-NeRF 360 dataset中的场景进行重建，比对其他State-of-the-art NeRF方法（如Zip-NeRF）和State-of-the-art联合优化方法（如SCNeRF）减少错误率（RMSE）67%，相比减少29%Here’s a more detailed explanation of each point:</li>
<li>for: The paper is written for optimizing Neural Radiance Fields (NeRF) to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes.</li>
<li>methods: The paper proposes using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and using this transform as a preconditioner for the camera parameters during joint optimization.</li>
<li>results: The paper shows that the proposed approach significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset, reducing error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF.<details>
<summary>Abstract</summary>
Neural Radiance Fields (NeRF) can be optimized to obtain high-fidelity 3D scene reconstructions of objects and large-scale scenes. However, NeRFs require accurate camera parameters as input -- inaccurate camera parameters result in blurry renderings. Extrinsic and intrinsic camera parameters are usually estimated using Structure-from-Motion (SfM) methods as a pre-processing step to NeRF, but these techniques rarely yield perfect estimates. Thus, prior works have proposed jointly optimizing camera parameters alongside a NeRF, but these methods are prone to local minima in challenging settings. In this work, we analyze how different camera parameterizations affect this joint optimization problem, and observe that standard parameterizations exhibit large differences in magnitude with respect to small perturbations, which can lead to an ill-conditioned optimization problem. We propose using a proxy problem to compute a whitening transform that eliminates the correlation between camera parameters and normalizes their effects, and we propose to use this transform as a preconditioner for the camera parameters during joint optimization. Our preconditioned camera optimization significantly improves reconstruction quality on scenes from the Mip-NeRF 360 dataset: we reduce error rates (RMSE) by 67% compared to state-of-the-art NeRF approaches that do not optimize for cameras like Zip-NeRF, and by 29% relative to state-of-the-art joint optimization approaches using the camera parameterization of SCNeRF. Our approach is easy to implement, does not significantly increase runtime, can be applied to a wide variety of camera parameterizations, and can straightforwardly be incorporated into other NeRF-like models.
</details>
<details>
<summary>摘要</summary>
神经辐射场（NeRF）可以优化以获得高精度3D场景重建。然而，NeRF需要准确的摄像头参数作为输入，否则会得到模糊的渲染。通常来说，摄像头参数的内在和外在参数通过Structure-from-Motion（SfM）方法进行估计，但这些技术很少能提供完美的估计。因此，先前的工作已经提议同时优化摄像头参数和NeRF，但这些方法容易陷入困难的设置中。在这个工作中，我们分析了不同的摄像头参数化对这个联合优化问题的影响，并发现标准参数化 exhibit 大量的差异幅度，这可能导致一个不正确的优化问题。我们提出使用一个代理问题计算一个卷积变换，该变换消除了摄像头参数与normalize 其效果的相关性，并我们提议使用这个变换作为摄像头参数的预处理器。我们的预处理后的摄像头优化显著提高了Mip-NeRF 360 dataset中的重建质量：我们降低了误差率（RMSE）相比领先的NeRF方法，Zip-NeRF，和相对于领先的联合优化方法使用摄像头参数化的 SCNeRF，降低了29%。我们的方法易于实现，不会增加运行时间，可以应用于多种摄像头参数化，并可以 straightforwardly 与其他NeRF-like模型结合使用。
</details></li>
</ul>
<hr>
<h2 id="Few-Shot-Physically-Aware-Articulated-Mesh-Generation-via-Hierarchical-Deformation"><a href="#Few-Shot-Physically-Aware-Articulated-Mesh-Generation-via-Hierarchical-Deformation" class="headerlink" title="Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation"></a>Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10898">http://arxiv.org/abs/2308.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueyi Liu, Bin Wang, He Wang, Li Yi</li>
<li>for: 本研究旨在解决具有少量示例的物理可知树状对象生成问题。通过观察含有只几个示例的人工骨架对象数据集，我们希望通过学习一个模型，以生成多样化的骨架，并保证其视觉准确性和物理可行性。</li>
<li>methods: 我们提出了两项关键创新，即1）基于分治哲学的层次骨架变换基本模型，以适应具有少量示例的问题，并且可以借鉴大规模的固定骨架上的可转移变换模式；2）基于物理学的变换修正方案，以促进物理可行的生成。</li>
<li>results: 我们在6种人工骨架类别上进行了广泛的实验，并证明了我们的方法在几何上比前方法更好，可以更好地生成具有多样性、高视觉准确性和物理可行性的骨架。此外，我们还进行了ablation研究，以验证我们的两项创新的准确性。研究页面及代码可以在<a target="_blank" rel="noopener" href="https://meowuu7.github.io/few-arti-obj-gen%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://meowuu7.github.io/few-arti-obj-gen中找到。</a><details>
<summary>Abstract</summary>
We study the problem of few-shot physically-aware articulated mesh generation. By observing an articulated object dataset containing only a few examples, we wish to learn a model that can generate diverse meshes with high visual fidelity and physical validity. Previous mesh generative models either have difficulties in depicting a diverse data space from only a few examples or fail to ensure physical validity of their samples. Regarding the above challenges, we propose two key innovations, including 1) a hierarchical mesh deformation-based generative model based upon the divide-and-conquer philosophy to alleviate the few-shot challenge by borrowing transferrable deformation patterns from large scale rigid meshes and 2) a physics-aware deformation correction scheme to encourage physically plausible generations. We conduct extensive experiments on 6 articulated categories to demonstrate the superiority of our method in generating articulated meshes with better diversity, higher visual fidelity, and better physical validity over previous methods in the few-shot setting. Further, we validate solid contributions of our two innovations in the ablation study. Project page with code is available at https://meowuu7.github.io/few-arti-obj-gen.
</details>
<details>
<summary>摘要</summary>
我们研究几何物体生成中受限的几何物体生成问题。通过观察一个具有少量示例的柔软物体数据集，我们希望通过学习一个可以生成多样化的几何模型，以保证高Visual faithfulness和物理有效性。先前的几何生成模型容易在几何空间中示出少量示例的多样性或者缺乏物理有效性的问题。为了解决这些挑战，我们提出了两项关键创新：1. 基于分治理的几何变形生成模型，根据分治理的哲学，从大规模的固定几何中继承可质量的变形模式，以便在几何空间中减少几何数据的几何变形问题。2. 基于物理知识的几何变形修正方案，以促进物理可能的生成。我们对6种柔软物体类型进行了广泛的实验，并证明了我们的方法在几何数据中生成的几何模型具有更高的多样性、更高的Visual faithfulness和更高的物理有效性，相比之前的方法。此外，我们还进行了减少几何变形和物理变形修正的精细分析，以证明我们的两项创新的凝聚性。项目页面和代码可以在https://meowuu7.github.io/few-arti-obj-gen中找到。
</details></li>
</ul>
<hr>
<h2 id="Can-Language-Models-Learn-to-Listen"><a href="#Can-Language-Models-Learn-to-Listen" class="headerlink" title="Can Language Models Learn to Listen?"></a>Can Language Models Learn to Listen?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10897">http://arxiv.org/abs/2308.10897</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo Kanazawa, Trevor Darrell, Shiry Ginosar</li>
<li>For: The paper is written for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker’s words.* Methods: The approach uses an autoregressive model that predicts a response of a listener, which is a sequence of listener facial gestures, quantized using a VQ-VAE. The model treats the quantized atomic motion elements as additional language token inputs to a transformer-based large language model.* Results: The generated listener motion is fluent and reflective of language semantics, as shown through quantitative metrics and a qualitative user study. The model demonstrates the ability to utilize temporal and semantic aspects of spoken text.<details>
<summary>Abstract</summary>
We present a framework for generating appropriate facial responses from a listener in dyadic social interactions based on the speaker's words. Given an input transcription of the speaker's words with their timestamps, our approach autoregressively predicts a response of a listener: a sequence of listener facial gestures, quantized using a VQ-VAE. Since gesture is a language component, we propose treating the quantized atomic motion elements as additional language token inputs to a transformer-based large language model. Initializing our transformer with the weights of a language model pre-trained only on text results in significantly higher quality listener responses than training a transformer from scratch. We show that our generated listener motion is fluent and reflective of language semantics through quantitative metrics and a qualitative user study. In our evaluation, we analyze the model's ability to utilize temporal and semantic aspects of spoken text. Project page: https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/
</details>
<details>
<summary>摘要</summary>
我们提出了一个框架，用于基于说话人的话语生成适当的面部响应。给定输入词语讲解和时间戳，我们的方法通过自动递归预测一个听众的响应：一个序列化的听众面部姿势，使用VQ-VAE进行量化。由于姿势是语言成分，我们提议对量化的原子运动元素视为额外的语言标记输入，并将其传递给基于转换器的大语言模型进行处理。初始化我们的转换器使用已经预训练的语言模型的权重，比训练从零开始的转换器更有显著的质量提升。我们表示我们生成的听众动作是流畅的，并具有语言 semantics 的表达。在我们的评估中，我们分析了模型对说话文本的时间和 semantics 方面的使用。项目页面：https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Shadow-Mapping-for-Efficient-Inverse-Graphics"><a href="#Differentiable-Shadow-Mapping-for-Efficient-Inverse-Graphics" class="headerlink" title="Differentiable Shadow Mapping for Efficient Inverse Graphics"></a>Differentiable Shadow Mapping for Efficient Inverse Graphics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10896">http://arxiv.org/abs/2308.10896</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mworchel/differentiable-shadow-mapping">https://github.com/mworchel/differentiable-shadow-mapping</a></li>
<li>paper_authors: Markus Worchel, Marc Alexa</li>
<li>for: 该论文主要研究如何高效地生成triangle mesh中的阴影。</li>
<li>methods: 该论文提出了一种将预filtered shadow mapping技术与现有的可导着色器结合使用，以实现 triangle mesh中的可导visibility信息。</li>
<li>results: 研究发现，使用可导阴影图可以与不同的 inverse graphics problems 比较快速，并且与不同的灯光传输模拟相比，可以达到类似的准确性水平，而不同的可导着色器无法收敛。<details>
<summary>Abstract</summary>
We show how shadows can be efficiently generated in differentiable rendering of triangle meshes. Our central observation is that pre-filtered shadow mapping, a technique for approximating shadows based on rendering from the perspective of a light, can be combined with existing differentiable rasterizers to yield differentiable visibility information. We demonstrate at several inverse graphics problems that differentiable shadow maps are orders of magnitude faster than differentiable light transport simulation with similar accuracy -- while differentiable rasterization without shadows often fails to converge.
</details>
<details>
<summary>摘要</summary>
我们显示了如何有效地生成阴影在分割形状的几何 Rendering 中。我们的中心观察是可以将阴影预filtering，一种基于灯光的见识测试，与现有的可微化矿物 Rendering 结合，以获得可微化可视性信息。我们在一些逆图学问题中展示了这种方法比对于对称的阴影传递 Simulation 更快速，并且与不含阴影的可微化矿物 Rendering 相比，通常会出现不收敛的问题。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Accuracy-and-Fairness-in-Differentially-Private-Image-Classification"><a href="#Unlocking-Accuracy-and-Fairness-in-Differentially-Private-Image-Classification" class="headerlink" title="Unlocking Accuracy and Fairness in Differentially Private Image Classification"></a>Unlocking Accuracy and Fairness in Differentially Private Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10888">http://arxiv.org/abs/2308.10888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonard Berrada, Soham De, Judy Hanwen Shen, Jamie Hayes, Robert Stanforth, David Stutz, Pushmeet Kohli, Samuel L. Smith, Borja Balle</li>
<li>For: 这个研究的目的是让机器学习模型在保护敏感资料的情况下训练，以确保对敏感资料的训练不会泄露敏感信息。* Methods: 这个研究使用了差异调教（Differential Privacy）的金标准框架，以提供正式的隐私保证。* Results: 研究发现，使用预先训练的基础模型，并在这些模型上实现差异调教，可以实现与非隐私模型相似的准确性水平，甚至在资料分布shift的情况下仍能保持高度的准确性。<details>
<summary>Abstract</summary>
Privacy-preserving machine learning aims to train models on private data without leaking sensitive information. Differential privacy (DP) is considered the gold standard framework for privacy-preserving training, as it provides formal privacy guarantees. However, compared to their non-private counterparts, models trained with DP often have significantly reduced accuracy. Private classifiers are also believed to exhibit larger performance disparities across subpopulations, raising fairness concerns. The poor performance of classifiers trained with DP has prevented the widespread adoption of privacy preserving machine learning in industry. Here we show that pre-trained foundation models fine-tuned with DP can achieve similar accuracy to non-private classifiers, even in the presence of significant distribution shifts between pre-training data and downstream tasks. We achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks. Furthermore, our private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models. This milestone to make DP training a practical and reliable technology has the potential to widely enable machine learning practitioners to train safely on sensitive datasets while protecting individuals' privacy.
</details>
<details>
<summary>摘要</summary>
隐私保护机器学习的目标是在使用private数据进行训练时保持敏感信息的隐私。不同于其非私有对应的模型，模型通过隐私保护（DP）训练的准确率通常会下降 significatively。此外，private分类器可能会在不同的人口群体之间存在更大的性别差异，引发公平性的问题。由于DP训练的模型性能较差，因此在实业中广泛采用隐私保护机器学习的应用仍然受限。在这篇文章中，我们表明了基于预先训练的基础模型，通过DP进行细化训练可以达到与非私有模型相同的准确率，即使在数据分布变化较大的情况下。我们在四个数据集上达到了与非私有状态的准确率，包括两个医疗影像标准 benchmark。此外，我们的私有医疗分类器不会在不同的人口群体中存在更大的性别差异，与非私有模型相比。这一突破可能使DP训练成为实用和可靠的技术，使机器学习实践者可以在敏感数据上进行安全的训练，同时保护个人隐私。
</details></li>
</ul>
<hr>
<h2 id="Vision-Transformer-Pruning-Via-Matrix-Decomposition"><a href="#Vision-Transformer-Pruning-Via-Matrix-Decomposition" class="headerlink" title="Vision Transformer Pruning Via Matrix Decomposition"></a>Vision Transformer Pruning Via Matrix Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10839">http://arxiv.org/abs/2308.10839</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Sun</li>
<li>for: 降低存储、运行时内存和计算需求</li>
<li>methods: 使用矩阵分解方法（包括各种QR分解和LU分解）来降低矩阵维度和复杂度</li>
<li>results: 结果表明使用Singular Value Decomposition方法可以保持重要特征的生成，同时降低存储、运行时内存和计算需求。<details>
<summary>Abstract</summary>
This is a further development of Vision Transformer Pruning via matrix decomposition. The purpose of the Vision Transformer Pruning is to prune the dimension of the linear projection of the dataset by learning their associated importance score in order to reduce the storage, run-time memory, and computational demands. In this paper we further reduce dimension and complexity of the linear projection by implementing and comparing several matrix decomposition methods while preserving the generated important features. We end up selected the Singular Value Decomposition as the method to achieve our goal by comparing the original accuracy scores in the original Github repository and the accuracy scores of using those matrix decomposition methods, including Singular Value Decomposition, four versions of QR Decomposition, and LU factorization.
</details>
<details>
<summary>摘要</summary>
这是vision transformer减少的进一步发展，通过矩阵分解来减少数据集的维度。vision transformer减少的目的是学习数据集中每个特征的重要性分数，以降低存储、运行内存和计算成本。在这篇论文中，我们进一步减少了矩阵 projection 的维度和复杂性，并比较了多种矩阵分解方法，包括四种QR分解版本和LU分解。最终，我们选择了单值分解来实现我们的目标，并比较了原始数据集的准确率和使用这些矩阵分解方法得到的准确率。
</details></li>
</ul>
<hr>
<h2 id="EigenPlaces-Training-Viewpoint-Robust-Models-for-Visual-Place-Recognition"><a href="#EigenPlaces-Training-Viewpoint-Robust-Models-for-Visual-Place-Recognition" class="headerlink" title="EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition"></a>EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10832">http://arxiv.org/abs/2308.10832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmberton/eigenplaces">https://github.com/gmberton/eigenplaces</a></li>
<li>paper_authors: Gabriele Berton, Gabriele Trivigno, Barbara Caputo, Carlo Masone</li>
<li>For: The paper is written for the task of visual place recognition, specifically to improve the robustness of the model to different viewpoints.* Methods: The proposed method, called EigenPlaces, uses a new approach to train the neural network on images from different viewpoints, which embeds viewpoint robustness into the learned global descriptors. The method clusters the training data to explicitly present the model with different views of the same points of interest, without the need for extra supervision.* Results: The paper presents experiments on the most comprehensive set of datasets in literature, showing that EigenPlaces outperforms previous state-of-the-art methods on the majority of datasets, while requiring 60% less GPU memory for training and using 50% smaller descriptors.Here are the three key points in Simplified Chinese text:* For: 这篇论文是为了解决视觉地点识别任务中的视角不一致问题，以提高模型的视角Robustness。* Methods: 提议的方法是EigenPlaces，它使用了一种新的方法来在不同视角的图像上训练神经网络，从而在学习的全球描述符中嵌入视角Robustness。该方法通过对训练数据进行分组，以显式地给模型提供不同视角的同一个点的兴趣点。无需额外监督。* Results: 论文通过对Literature中最完整的数据集进行实验，发现EigenPlaces在大多数数据集上超过了之前的状态之为，而且需要60% menos的GPU内存进行训练，并使用50%更小的描述符。<details>
<summary>Abstract</summary>
Visual Place Recognition is a task that aims to predict the place of an image (called query) based solely on its visual features. This is typically done through image retrieval, where the query is matched to the most similar images from a large database of geotagged photos, using learned global descriptors. A major challenge in this task is recognizing places seen from different viewpoints. To overcome this limitation, we propose a new method, called EigenPlaces, to train our neural network on images from different point of views, which embeds viewpoint robustness into the learned global descriptors. The underlying idea is to cluster the training data so as to explicitly present the model with different views of the same points of interest. The selection of this points of interest is done without the need for extra supervision. We then present experiments on the most comprehensive set of datasets in literature, finding that EigenPlaces is able to outperform previous state of the art on the majority of datasets, while requiring 60\% less GPU memory for training and using 50\% smaller descriptors. The code and trained models for EigenPlaces are available at {\small{\url{https://github.com/gmberton/EigenPlaces}}}, while results with any other baseline can be computed with the codebase at {\small{\url{https://github.com/gmberton/auto_VPR}}}.
</details>
<details>
<summary>摘要</summary>
“视觉地标识任务的目标是根据图像（叫做查询）的视觉特征来预测图像的位置。通常通过图像检索，将查询图像与大量地标注的图像库中的最相似图像进行匹配，使用学习的全局描述符。一个主要挑战在这个任务中是识别不同视点下的地标。为解决这个限制，我们提出了一新的方法，叫做EigenPlaces，通过在不同视点下训练神经网络，将视点强度嵌入到学习的全局描述符中。这个思想是将训练数据集分为不同视点下的部分，以便显式地将模型暴露给不同视点下的同一个点感兴趣。无需额外监督，我们选择了这些点感兴趣。我们then present experiments on the most comprehensive set of datasets in literature, finding that EigenPlaces is able to outperform previous state of the art on the majority of datasets, while requiring 60% less GPU memory for training and using 50% smaller descriptors. The code and trained models for EigenPlaces are available at [https://github.com/gmberton/EigenPlaces](https://github.com/gmberton/EigenPlaces), while results with any other baseline can be computed with the codebase at [https://github.com/gmberton/auto_VPR](https://github.com/gmberton/auto_VPR).”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>
<hr>
<h2 id="Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction"><a href="#Pixel-Adaptive-Deep-Unfolding-Transformer-for-Hyperspectral-Image-Reconstruction" class="headerlink" title="Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction"></a>Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10820">http://arxiv.org/abs/2308.10820</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaoyu Li, Ying Fu, Ji Liu, Yulun Zhang</li>
<li>for: 本文提出了一种基于深度 unfolding 框架的高spectral像素 (HSI) 重建方法，以解决现有方法尚未能充分匹配 HSI 数据的问题。</li>
<li>methods: 本文使用了一种拥有 pixele adaptive descent 步长的数据模块，并引入了 Non-local Spectral Transformer (NST) 来强调 HSI 的3D特性。另外，通过 Fast Fourier Transform (FFT) 改进了不同阶段和层次的特征表达，以解决不同阶段和层次之间的交互问题。</li>
<li>results: 实验结果表明， compared to 现有 HSI 重建方法，本文提出的方法在 simulated 和实际场景中具有更高的重建性能。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/MyuLi/PADUT">https://github.com/MyuLi/PADUT</a> 上下载。<details>
<summary>Abstract</summary>
Hyperspectral Image (HSI) reconstruction has made gratifying progress with the deep unfolding framework by formulating the problem into a data module and a prior module. Nevertheless, existing methods still face the problem of insufficient matching with HSI data. The issues lie in three aspects: 1) fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3) stage interaction ignoring the differences in features at different stages. To address these issues, in this work, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction. In the data module, a pixel adaptive descent step is employed to focus on pixel-level agnostic degradation. In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering. Moreover, inspired by the diverse expression of features in different stages and depths, the stage interaction is improved by the Fast Fourier Transform (FFT). Experimental results on both simulated and real scenes exhibit the superior performance of our method compared to state-of-the-art HSI reconstruction methods. The code is released at: https://github.com/MyuLi/PADUT.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Fixed gradient descent step in the data module while the degradation of HSI is agnostic in the pixel level.2. Inadequate prior module for 3D HSI cube.3. Stage interaction ignoring the differences in features at different stages.To address these issues, we propose a Pixel Adaptive Deep Unfolding Transformer (PADUT) for HSI reconstruction. In the data module, we employ a pixel adaptive descent step to focus on pixel-level agnostic degradation. In the prior module, we introduce the Non-local Spectral Transformer (NST) to emphasize the 3D characteristics of HSI for recovering. Moreover, inspired by the diverse expression of features in different stages and depths, we improve the stage interaction by the Fast Fourier Transform (FFT).Experimental results on both simulated and real scenes demonstrate the superior performance of our method compared to state-of-the-art HSI reconstruction methods. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/MyuLi/PADUT">https://github.com/MyuLi/PADUT</a>.</details></li>
</ol>
<hr>
<h2 id="Jumping-through-Local-Minima-Quantization-in-the-Loss-Landscape-of-Vision-Transformers"><a href="#Jumping-through-Local-Minima-Quantization-in-the-Loss-Landscape-of-Vision-Transformers" class="headerlink" title="Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers"></a>Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10814">http://arxiv.org/abs/2308.10814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Natalia Frumkin, Dibakar Gope, Diana Marculescu</li>
<li>for: 提高量化神经网络的精度和效率</li>
<li>methods: 使用进化搜索和infoNCE损失函数 traverse非线性测试损失 landscape</li>
<li>results: 在不同量化级别（3-bit、4-bit、8-bit）下，提高全量化ViT-Base的top-1准确率，并在极端量化场景下保持稳定性和可靠性<details>
<summary>Abstract</summary>
Quantization scale and bit-width are the most important parameters when considering how to quantize a neural network. Prior work focuses on optimizing quantization scales in a global manner through gradient methods (gradient descent \& Hessian analysis). Yet, when applying perturbations to quantization scales, we observe a very jagged, highly non-smooth test loss landscape. In fact, small perturbations in quantization scale can greatly affect accuracy, yielding a $0.5-0.8\%$ accuracy boost in 4-bit quantized vision transformers (ViTs). In this regime, gradient methods break down, since they cannot reliably reach local minima. In our work, dubbed Evol-Q, we use evolutionary search to effectively traverse the non-smooth landscape. Additionally, we propose using an infoNCE loss, which not only helps combat overfitting on the small calibration dataset ($1,000$ images) but also makes traversing such a highly non-smooth surface easier. Evol-Q improves the top-1 accuracy of a fully quantized ViT-Base by $10.30\%$, $0.78\%$, and $0.15\%$ for $3$-bit, $4$-bit, and $8$-bit weight quantization levels. Extensive experiments on a variety of CNN and ViT architectures further demonstrate its robustness in extreme quantization scenarios. Our code is available at https://github.com/enyac-group/evol-q
</details>
<details>
<summary>摘要</summary>
《量化缩放和位宽是神经网络量化时的关键参数。先前的工作通过梯度方法优化量化缩放，但当应用扰动时，我们发现测试损失 landscape 非常峰峦，高度不平。事实上，小于扰动量化缩放可以大幅提高准确性，达到 $0.5-0.8\%$ 的准确率提升。在这种情况下，梯度方法失效，因为它们无法可靠地到达地方最优点。在我们的工作中，称为 Evol-Q，我们使用进化搜索来有效地探索非平坦的表面。此外，我们也提议使用 infoNCE 损失函数，它不仅能够降低在小训练集（1,000 张图像）中的溢出问题，而且使探索非平坦表面更加容易。 Evol-Q 在完全量化 ViT-Base 上提高了 top-1 准确率，分别为 $10.30\%$, $0.78\%$, $0.15\%$  для $3$-bit、$4$-bit 和 $8$-bit 量化 веса级别。我们还进行了对多种 CNN 和 ViT 架构的广泛实验，以证明它的稳定性在极端量化场景下。我们的代码可以在 https://github.com/enyac-group/evol-q 上获取。》
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.CV_2023_08_22/" data-id="clm0t8dz2003yv7880g8a9wog" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.LG_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.LG_2023_08_22/">cs.LG - 2023-08-22 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="A-free-from-local-minima-algorithm-for-training-regressive-MLP-neural-networks"><a href="#A-free-from-local-minima-algorithm-for-training-regressive-MLP-neural-networks" class="headerlink" title="A free from local minima algorithm for training regressive MLP neural networks"></a>A free from local minima algorithm for training regressive MLP neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11532">http://arxiv.org/abs/2308.11532</a></li>
<li>repo_url: None</li>
<li>paper_authors: Augusto Montisci</li>
<li>for: 这篇论文旨在提出一种不受地方最小值的多层感知网络训练方法。</li>
<li>methods: 该方法基于训练集分布的特性，通过内部图像来避免地方最小值问题。</li>
<li>results: 研究表明，该方法可以减少地方最小值问题，并且在知名的测试集上达到了良好的性能。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the properties of the distribution of the training set, or better on its image internal to the neural network. The performance of the algorithm is shown for a well-known benchmark.
</details>
<details>
<summary>摘要</summary>
在本文中，一种创新的多层感知网络训练方法被介绍，不受局部最小点的限制。欧文-希金特-吕姆哈特提出的错误反传播算法，在1980年代中期以来，对机器学习技术的发展做出了重要贡献，这种成功是由黑盒方法引起的，但同时也被认为是局限性的因素。当面临更复杂的问题时，最 kritical的训练算法问题是搜索函数的局部最小点，通常是训练集的平均方差。因为现有最popular的训练算法都是根据损失函数的导数进行驱动，因此无法确定是否到达了局部最小点或全局最小点。本文中提出的算法解决了局部最小点问题，基于训练集的分布性质或更好地说是其内部图像。文章中展示了一个知名的标准 bencmark 的性能。
</details></li>
</ul>
<hr>
<h2 id="ReLiCADA-–-Reservoir-Computing-using-Linear-Cellular-Automata-Design-Algorithm"><a href="#ReLiCADA-–-Reservoir-Computing-using-Linear-Cellular-Automata-Design-Algorithm" class="headerlink" title="ReLiCADA – Reservoir Computing using Linear Cellular Automata Design Algorithm"></a>ReLiCADA – Reservoir Computing using Linear Cellular Automata Design Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11522">http://arxiv.org/abs/2308.11522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Kantic, Fabian C. Legl, Walter Stechele, Jakob Hermann</li>
<li>for: 这个论文目的是提出一种基于细胞自动机模型的泵Computing优化算法，用于处理时间序列应用。</li>
<li>methods: 该算法不仅选择模型的超参数，而且解决了线性细胞自动机规则选择的开放问题。选择方法可以从无限增长的规则空间中预选出一些有潜力的规则。</li>
<li>results: 应用于相关的参考数据集时，选择的规则可以实现低的错误率，其中最佳规则位于总规则空间的前5%。该算法基于细胞自动机属性的数学分析，通过约一百万个实验，计算时间约为一年。与其他现状最佳时间序列模型比较，提出的泵Computing使用细胞自动机模型具有较低的计算复杂度，同时实现较低的错误率。因此，我们的方法可以减少训练和超参数优化所需的时间，达到数量级减少。<details>
<summary>Abstract</summary>
In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our approach reduces the time needed for training and hyperparameter optimization by up to several orders of magnitude.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的算法优化储池计算机using Cellular Automata模型 для时间序列应用。除了选择模型的超参数之外，提出的算法特别解决了开放问题，即选择线性Cellular Automaton规则。选择方法先选择了一些有前途的候选规则，从急增的规则空间中选择出来。当应用到相关的标准数据集时，选择的规则都达到了低错误率，最好的规则在总规则空间中排名前5%。算法基于Cellular Automata模型的数学分析和大约一百万个实验，计算时间约为一年。与其他当前最佳时间序列模型进行比较，我们的方法具有较低的计算复杂度，同时实现了较低的错误率。因此，我们的方法可以减少训练和超参数优化所需的时间，可能是数量级减少。
</details></li>
</ul>
<hr>
<h2 id="EM-for-Mixture-of-Linear-Regression-with-Clustered-Data"><a href="#EM-for-Mixture-of-Linear-Regression-with-Clustered-Data" class="headerlink" title="EM for Mixture of Linear Regression with Clustered Data"></a>EM for Mixture of Linear Regression with Clustered Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11518">http://arxiv.org/abs/2308.11518</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Reisizadeh, Khashayar Gatmiry, Asuman Ozdaglar</li>
<li>for: 这篇论文是为了提高分布式学习中数据不同性的问题。</li>
<li>methods: 这篇论文使用了Expectation-Maximization（EM）方法来估计两组线性回归问题的最佳参数。</li>
<li>results: 论文显示，如果初始化得当，EM在结构化数据上只需要O(1)迭代iterations来达到同样的统计精度，provided that m grows as e^(o(n))。<details>
<summary>Abstract</summary>
Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments. Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms. In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client. It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes. In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable. We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ batches of dependent samples each containing $n$ measurements. Discarding the clustered structure in the mixture model, EM is known to require $O(\log(mn/d))$ iterations to reach the statistical accuracy of $O(\sqrt{d/(mn)})$. In contrast, we show that if initialized properly, EM on the structured data requires only $O(1)$ iterations to reach the same statistical accuracy, as long as $m$ grows up as $e^{o(n)}$. Our analysis establishes and combines novel asymptotic optimization and generalization guarantees for population and empirical EM with dependent samples, which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
现代数据驱动和分布式学习框架面临着各种各样的大量数据，由客户端分布在不同环境中生成。实际上，数据异ogeneity是规模化多个分布式学习模式的主要瓶颈。然而，在许多应用程序中，客户端生成的数据可能会组成带有共同结构的分布，如联邦学习中，所有样本的共同隐变量控制所有样本的分布。因此，可以问到如何利用分布式数据中的下层结构来改进学习方案。在这篇论文中，我们解决这个问题，在特定的两个分布线性回归问题中。我们使用已知的期望-最大化（EM）方法来估算最大有可能性参数，从$m$个批处理中获取$n$个测量结果。不考虑分布式数据中的带有共同结构的混合模型，EM在$O(\log(mn/d))$迭代中可以达到同等的统计准确性，其中$d$是维度，$m$是节点数，$n$是测量结果数。然而，我们证明，如果初始化得当，EM在结构化数据上只需要$O(1)$迭代来达到同等的统计准确性，只要$m$在$e^{o(n)}$中增长。我们的分析提出和组合了新的极限优化和通用验证保证，可能是独立有价值的。
</details></li>
</ul>
<hr>
<h2 id="TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows"><a href="#TrackFlow-Multi-Object-Tracking-with-Normalizing-Flows" class="headerlink" title="TrackFlow: Multi-Object Tracking with Normalizing Flows"></a>TrackFlow: Multi-Object Tracking with Normalizing Flows</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11513">http://arxiv.org/abs/2308.11513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gianluca Mancusi, Aniello Panariello, Angelo Porrello, Matteo Fabbri, Simone Calderara, Rita Cucchiara</li>
<li>for: 提高多对目标跟踪的性能，特别是在多模态场景下。</li>
<li>methods: 基于深度概率分布模型，计算候选关联的成本为负极log值，以提高跟踪检测算法的性能。</li>
<li>results: 对多种跟踪检测算法进行实验，显示我们的方法可以一致提高其性能。<details>
<summary>Abstract</summary>
The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilistic formulation, which considers the cost of a candidate association as the negative log-likelihood yielded by a deep density estimator, trained to model the conditional joint probability distribution of correct associations. Our experiments, conducted on both simulated and real benchmarks, show that our approach consistently enhances the performance of several tracking-by-detection algorithms.
</details>
<details>
<summary>摘要</summary>
“multi-object tracking领域最近又重新启发了使用检测方法的古老schema，因为它的简单性和强有力的假设，使其不需要复杂的设计和痛苦的照顾，相比之下，tracking-by-attention方法更加复杂。在视野内，我们想扩展tracking-by-detection到多modal设置，在其中需要从多种不同的信息（例如2D运动cue、视觉外观和姿态估计）计算全面的成本。更准确地说，我们采用了一个实际案例，其中有一个粗略的3D信息估计也可以与传统的metric（例如IoU）一起使用。为了实现这一点，现有的方法通常采用简单的规则或复杂的规则来均衡每个成本的贡献。然而，这些方法有以下两个问题：一是需要精心调整特定的超参数，二是它们假设成本是独立的，这并不符合实际情况。我们解决这些问题 by 建立在一种简洁的概率形式上，即使用深度概率分布来模型correct association的 conditional joint probability distribution。我们的实验在模拟和实际 benchmark上展示了我们的方法可以不断提高多个tracking-by-detection算法的性能。”
</details></li>
</ul>
<hr>
<h2 id="Mode-Combinability-Exploring-Convex-Combinations-of-Permutation-Aligned-Models"><a href="#Mode-Combinability-Exploring-Convex-Combinations-of-Permutation-Aligned-Models" class="headerlink" title="Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models"></a>Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11511">http://arxiv.org/abs/2308.11511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrián Csiszárik, Melinda F. Kiss, Péter Kőrösi-Szabó, Márton Muntag, Gergely Papp, Dániel Varga</li>
<li>for: 本文研究了两个嵌入型神经网络参数 вектора $\Theta_A$ 和 $\Theta_B$ 的元素wise convex combination，以探索这些模型组合的可行性和特性。</li>
<li>methods: 作者采用了大量的实验，对不同的模型组合 parametrized by $[0,1]^d$ 的元素进行了探索，发现了广泛的超平面存在低损值，这表明了线性模式连接的概念扩展到一般情况，称为模式可 conjugacy。</li>
<li>results: 作者发现了一些新的线性模式连接和模型重新定位的观察结果，包括模型重新定位的转移性和稳定性，以及模型组合的功能和权重相似性。<details>
<summary>Abstract</summary>
We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.
</details>
<details>
<summary>摘要</summary>
我们探索两个 permutation-aligned 神经网络参数 вектор $\Theta_A$ 和 $\Theta_B$ 的元素层次 convex 组合。我们进行了广泛的实验，检查了不同的模型组合参数化的元素随机分布在 $[0,1]^d$ 和其邻近区域，我们的发现表明，广泛的区域在卷积体中形成低损值表面，这表明了线性模式连接的概念扩展到一般现象，我们称之为模式可组合性。我们还做了一些新的观察，包括线性模式连接和模型重定向。我们证明了两个模型重定向到共同的第三模型时，也是线性模式连接的，并且在神经元匹配的干扰下，得到的组合仍然可以形成一个工作模型。此外，我们分析了模型组合的功能和权重相似性，并证明了这些组合不是空的，即存在 significativ functional differences  между得到的模型。
</details></li>
</ul>
<hr>
<h2 id="Can-Authorship-Representation-Learning-Capture-Stylistic-Features"><a href="#Can-Authorship-Representation-Learning-Capture-Stylistic-Features" class="headerlink" title="Can Authorship Representation Learning Capture Stylistic Features?"></a>Can Authorship Representation Learning Capture Stylistic Features?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11490">http://arxiv.org/abs/2308.11490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/llnl/luar">https://github.com/llnl/luar</a></li>
<li>paper_authors: Andrew Wang, Cristina Aggazzotti, Rebecca Kotula, Rafael Rivera Soto, Marcus Bishop, Nicholas Andrews</li>
<li>for: 这篇论文主要是为了研究如何自动分离作者的风格和内容。</li>
<li>methods: 这篇论文使用了数据驱动的方法来学习作者表示，并通过一系列定制的实验来评估这些表示是否能够捕捉作者的风格。</li>
<li>results: 研究发现，这些表示确实受到作者的风格影响，并且可能对于某些类型的数据变换（如时间上的话题变化）具有一定的Robustness。这些结果可能对于应用于风格转换等下渠应用有所帮助。<details>
<summary>Abstract</summary>
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the surrogate authorship prediction task are indeed sensitive to writing style. As a consequence, authorship representations may be expected to be robust to certain kinds of data shift, such as topic drift over time. Additionally, our findings may open the door to downstream applications that require stylistic representations, such as style transfer.
</details>
<details>
<summary>摘要</summary>
自动分离作者的风格从他们的写作内容是计算语言学领域的长期问题。同时，Recently有大量的文本 corpus 提供了作者标签，使得可以通过数据驱动方式学习作者表示。然而，这种表示是否真的捕捉了作者的风格呢？我们通过系统的实验来彻底检验这些表示是否具有风格特征。结果表明，这些表示对作者预测任务有高度敏感性，表明它们主要捕捉了作者的风格。这意味着作者表示可能对某些类型的数据变换具有较高的Robustness，例如在时间上的话题变化。此外，我们的发现可能开启了在下游应用中使用风格表示的可能性，如样式转移。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions"><a href="#Large-Language-Models-Sensitivity-to-The-Order-of-Options-in-Multiple-Choice-Questions" class="headerlink" title="Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions"></a>Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11483">http://arxiv.org/abs/2308.11483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pouya Pezeshkpour, Estevam Hruschka</li>
<li>for:  investigate the sensitivity of large language models (LLMs) towards the order of options in multiple-choice questions, and to understand the underlying reasons for this sensitivity.</li>
<li>methods:  the authors use various benchmarks and experiments to study the performance of LLMs on multiple-choice questions, and they adopt two approaches to calibrate the models’ predictions.</li>
<li>results:  the authors find a significant performance gap of approximately 13% to 75% in LLMs on different benchmarks when answer options are reordered, and they identify patterns in top-2 choices that amplify or mitigate the model’s bias toward option placement. Additionally, they show that calibrating the models’ predictions can improve their performance by up to 8 percentage points across different models and benchmarks.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在不同的自然语言处理任务中表现出了很好的能力。然而，之前的研究表明这些模型对示例语言和示例的顺序有敏感性，这会带来评估这些模型的困难。随着这些模型的力量的提高，我们需要更好地理解和解决这些局限性。在这篇论文中，我们关注LLM在多选题上的Robustness——一个常用的任务来评估这些模型的理解和知识检索能力。我们发现，当选项的顺序变化时，LLM的表现会受到较大的影响，在不同的benchmark上，表现差距可达13%到75%，即使使用几个示例。通过详细分析，我们推断这种敏感性是由于LLM在前两选项之间的uncertainty，specific option placement可能会对certain prediction between those top choices产生影响，这是因为位置偏好。我们还发现了在top-2选项之间的抗抑补偏好和增强偏好的模式。对于增强偏好，我们发现可以将top two选项放在第一和最后的位置。相反，为了减轻偏好，我们建议将这些选项放在相邻的位置。为了证明我们的推断，我们进行了多种实验和采用了两种方法来调整LLM的预测，导致在不同的模型和benchmark上的改进率达到8%。
</details></li>
</ul>
<hr>
<h2 id="Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection"><a href="#Expecting-The-Unexpected-Towards-Broad-Out-Of-Distribution-Detection" class="headerlink" title="Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection"></a>Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11480">http://arxiv.org/abs/2308.11480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/servicenow/broad-openood">https://github.com/servicenow/broad-openood</a></li>
<li>paper_authors: Charles Guille-Escuret, Pierre-André Noël, Ioannis Mitliagkas, David Vazquez, Joao Monteiro</li>
<li>for: 本研究旨在提高部署在机器学习系统上的可靠性，通过开发检测异常输入（Out-of-distribution，OOD）方法。</li>
<li>methods: 我们分类了五种不同类型的分布偏移，并评估了最新的OOD检测方法在每种分布偏移下的表现。</li>
<li>results: 我们发现，现有方法可以很好地检测未知类别的异常输入，但它们在其他类型的分布偏移下表现不一致。即，它们只可以可靠地检测它们已经特定地设计来预期的异常输入。我们通过学习现有检测分数的生成模型，提出了一种ensemble方法，可以为广泛的OOD检测提供一种更加一致和全面的解决方案，并证明其性能比现有方法更高。<details>
<summary>Abstract</summary>
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a generative model of existing detection scores with a Gaussian mixture. By doing so, we present an ensemble approach that offers a more consistent and comprehensive solution for broad OOD detection, demonstrating superior performance compared to existing methods. Our code to download BROAD and reproduce our experiments is publicly available.
</details>
<details>
<summary>摘要</summary>
增强已部署的机器学习系统的可靠性通常需要开发检测异常输入（OOD）的方法。然而，现有研究通常只关注未在训练集中出现的样本，忽略其他类型的可能的分布Shift。这种限制使得这些方法在实际应用中的可用性受到限制，因为系统会遇到各种异常输入。在本研究中，我们将五种不同类型的分布Shift分类，并且严格评估最近的OOD检测方法在每一类中的性能。我们将这些数据集公开发布，名为BROAD（Benchmarking Resilience Over Anomaly Diversity）。我们的发现表明，虽然这些方法可以很好地检测未知的类别，但是它们在其他类型的分布Shift下的性能不一致。即它们只能可靠地检测它们已经特定地设计来预期的异常输入。作为广泛OOD检测的第一步，我们学习了现有检测分数的生成模型，使用高斯混合模型。通过这种ensemble方法，我们提供了一种更加一致和全面的广泛OOD检测解决方案，并且在与现有方法进行比较时表现出了superior的性能。我们在下载BROAD和复现我们的实验的代码是公开可用的。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-column-generation-based-matheuristic-for-learning-classification-trees"><a href="#Revisiting-column-generation-based-matheuristic-for-learning-classification-trees" class="headerlink" title="Revisiting column-generation-based matheuristic for learning classification trees"></a>Revisiting column-generation-based matheuristic for learning classification trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11477">http://arxiv.org/abs/2308.11477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krooonal/col_gen_estimator">https://github.com/krooonal/col_gen_estimator</a></li>
<li>paper_authors: Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi</li>
<li>for: 提高分类问题中decision trees的准确性和可扩展性</li>
<li>methods: 使用列生成方法优化decision trees的训练</li>
<li>results: 提高了分类问题中decision trees的准确性和可扩展性，并且可以处理大型数据集<details>
<summary>Abstract</summary>
Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their corresponding constraints. We conclude by presenting computational results that show that these modifications result in better scalability.
</details>
<details>
<summary>摘要</summary>
决策树是机器学习中高度可解释的模型，用于解决分类问题。标准的机器学习算法可以快速训练决策树，但是会生成准确性不高的树。文献中的其他离散优化模型可以解决优化问题，但只能在较小的数据集上工作。 \cite{firat2020column} 提出了一种基于列生成的规则方法来学习决策树。这种方法可以扩展到大型数据集。在这篇论文中，我们介绍了对这种列生成方法的改进。首先，我们修改了多类分类实例中的子问题模型，以Significantly reduce the number of subproblems。然后，我们显示了数据依赖的约束在主问题中被含有，并使用它们作为裁剪平面。此外，我们描述了一种分离模型，用于生成具有Linear programming relaxation solution违反其相应约束的数据点。我们 conclude by presenting computational results that show that these modifications result in better scalability。
</details></li>
</ul>
<hr>
<h2 id="Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning"><a href="#Internal-Cross-layer-Gradients-for-Extending-Homogeneity-to-Heterogeneity-in-Federated-Learning" class="headerlink" title="Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning"></a>Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11464">http://arxiv.org/abs/2308.11464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Hin Chan, Rui Zhou, Running Zhao, Zhihan Jiang, Edith C. -H. Ngai</li>
<li>for: 提高实际场景中 Federated Learning（FL）系统的兼容性和灵活性。</li>
<li>methods: 提出一种基于内部相似层次梯度的聚合方法，可以增强深层模型之间的相似性，无需更多的客户端之间的交流。</li>
<li>results: 经验证明，该方法可以提高FL系统的性能，并且可以跨种模型进行扩展。<details>
<summary>Abstract</summary>
Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogenous FL.
</details>
<details>
<summary>摘要</summary>
联合学习（FL）在实际应用中会面临系统不同性的挑战。为了增强大多数模型同质FL方法的处理能力，我们提出了一个训练方案，可以将其扩展到处理这个挑战。在这篇论文中，我们开始我们的研究，进行了详细的探索同质和不同质FL环境，发现了三个关键观察结果：（1）客户端性能与层 Similarity 之间存在正相关，（2）在浅层比深层更高的 Similarity，（3）更平滑的梯度分布显示了更高的层 Similarity。基于这些观察结果，我们提出了内部混合层梯度（InCo Aggregation），利用服务器模型内部的混合梯度，以增强深层层 Similarity 而无需更多的客户端通信。此外，我们的方法可以与模型同质FL方法，如FedAvg、FedProx、FedNova、Scaffold和MOON，整合，扩展其处理能力，处理不同系统的挑战。丰富的实验结果证明了InCo Aggregation 的效果，显示了内部混合层梯度作为提高hetrogenous FL性能的有前途之路。
</details></li>
</ul>
<hr>
<h2 id="An-Analysis-of-Initial-Training-Strategies-for-Exemplar-Free-Class-Incremental-Learning"><a href="#An-Analysis-of-Initial-Training-Strategies-for-Exemplar-Free-Class-Incremental-Learning" class="headerlink" title="An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning"></a>An Analysis of Initial Training Strategies for Exemplar-Free Class-Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11677">http://arxiv.org/abs/2308.11677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Grégoire Petit, Michael Soumm, Eva Feillet, Adrian Popescu, Bertrand Delezoide, David Picard, Céline Hudelot</li>
<li>for: 本研究的目的是研究分类模型在数据流中建立和维护的问题，具体来说是在不能保留过去类的情况下，通过累积学习来逐步增加新类。</li>
<li>methods: 本研究使用了累积学习的增量学习策略，并对不同的策略、神经网络架构、目标任务、类别分布和数据量进行了广泛的实验研究。</li>
<li>results: 研究发现，初始训练策略是增量性能的主要影响因素，但是选择适合的增量学习算法更重要地防止忘记。基于这些分析结果，提出了实践中增量学习的实用建议。<details>
<summary>Abstract</summary>
Class-Incremental Learning (CIL) aims to build classification models from data streams. At each step of the CIL process, new classes must be integrated into the model. Due to catastrophic forgetting, CIL is particularly challenging when examples from past classes cannot be stored, the case on which we focus here. To date, most approaches are based exclusively on the target dataset of the CIL process. However, the use of models pre-trained in a self-supervised way on large amounts of data has recently gained momentum. The initial model of the CIL process may only use the first batch of the target dataset, or also use pre-trained weights obtained on an auxiliary dataset. The choice between these two initial learning strategies can significantly influence the performance of the incremental learning model, but has not yet been studied in depth. Performance is also influenced by the choice of the CIL algorithm, the neural architecture, the nature of the target task, the distribution of classes in the stream and the number of examples available for learning. We conduct a comprehensive experimental study to assess the roles of these factors. We present a statistical analysis framework that quantifies the relative contribution of each factor to incremental performance. Our main finding is that the initial training strategy is the dominant factor influencing the average incremental accuracy, but that the choice of CIL algorithm is more important in preventing forgetting. Based on this analysis, we propose practical recommendations for choosing the right initial training strategy for a given incremental learning use case. These recommendations are intended to facilitate the practical deployment of incremental learning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:类cremental Learning (CIL) 目标是从数据流中建立分类模型。在每个步骤中，新的类需要被添加到模型中。由于悬崖式忘记，CIL 特别在不能存储过去类例时是挑战。到目前为止，大多数方法都是专注于目标数据集。然而，使用基于大量数据的自动学习模型的预训练方法在最近几年中得到了迅速发展。CIL 过程的初始模型可以使用目标数据集的第一个批处或者使用 auxiliary 数据集上预训练的模型。这两种初始学习策略的选择会对增量性表现产生重要的影响，但是这个问题还没有得到深入的研究。增量表现的性能也受到 CIL 算法、神经网络架构、任务性质、类别分布和学习例数的影响。我们进行了全面的实验研究，以评估这些因素对增量性表现的影响。我们提供了一个统计分析框架，以量化每个因素对增量性表现的贡献。我们的主要发现是，初始训练策略是增量性表现的主导因素，但是选择 CIL 算法更重要地防止忘记。基于这种分析，我们提供了实用的初始训练策略选择指南，这些指南旨在促进实际部署增量学习。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Self-Supervised-Representation-Learning"><a href="#A-Survey-on-Self-Supervised-Representation-Learning" class="headerlink" title="A Survey on Self-Supervised Representation Learning"></a>A Survey on Self-Supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11455">http://arxiv.org/abs/2308.11455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/esvit">https://github.com/microsoft/esvit</a></li>
<li>paper_authors: Tobias Uelwer, Jan Robine, Stefan Sylvius Wagner, Marc Höftmann, Eric Upschulte, Sebastian Konietzny, Maike Behrendt, Stefan Harmeling</li>
<li>for: 本文提供了一个抽象的审视，概述了一些不需要监督学习的图像表示学习方法。</li>
<li>methods: 本文使用了一些不需要监督学习的图像表示学习方法，包括自适应 Representation Learning 方法和卷积神经网络方法。</li>
<li>results: 本文对图像表示学习方法进行了一个系统的审视，并对相关的实验结果进行了一个综合的Meta-study。<details>
<summary>Abstract</summary>
Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
</details>
<details>
<summary>摘要</summary>
学习有意义的表示是现代机器学习领域中的核心任务之一。最近，许多没有监督的方法被引入，可以学习图像表示。这些表示可以在下游任务中使用，如分类或物体检测。这些表示质量与监督学习几乎相同，但无需标注图像。本文提供了这些方法的统一notation，指出了这些方法之间的相似性和差异，并提出了这些方法的分类方法。此外，我们的survey还summarized literaturereported最新的实验结果，以meta-study的形式。本文作为研究者和实践者入门点，准备了涉及表示学习领域的所有信息。
</details></li>
</ul>
<hr>
<h2 id="Masked-Momentum-Contrastive-Learning-for-Zero-shot-Semantic-Understanding"><a href="#Masked-Momentum-Contrastive-Learning-for-Zero-shot-Semantic-Understanding" class="headerlink" title="Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding"></a>Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11448">http://arxiv.org/abs/2308.11448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiantao Wu, Shentong Mo, Muhammad Awais, Sara Atito, Zhenhua Feng, Josef Kittler</li>
<li>for: 本研究旨在评估无监督学习（SSL）技术在计算机视觉任务中的效果，以便模拟人类对未看过的对象的总结和识别。</li>
<li>methods: 本研究使用了一种基于提示 patch的评估协议来进行零shot segmentation，以及一种内部和外部相似性评估来评估 SSL ViTs 的准确性。</li>
<li>results: 研究发现，通过提取本地特征的相似性和全局特征的准确性，可以提高 SSL ViTs 的准确性和总结能力。此外，提出了一种简单的 SSL 方法，称为 MMC，可以减少内部和外部相似性的重叠，从而实现图像中的有效对象 segmentation。<details>
<summary>Abstract</summary>
Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the target. Another evaluation is intra-object and inter-object similarity to gauge discriminatory ability of SSP ViTs. Insights from zero-shot segmentation from prompting and discriminatory abilities of SSP led to the design of a simple SSP approach, termed MMC. This approaches combines Masked image modelling for encouraging similarity of local features, Momentum based self-distillation for transferring semantics from global to local features, and global Contrast for promoting semantics of global features, to enhance discriminative representations of SSP ViTs. Consequently, our proposed method significantly reduces the overlap of intra-object and inter-object similarities, thereby facilitating effective object segmentation within an image. Our experiments reveal that MMC delivers top-tier results in zero-shot semantic segmentation across various datasets.
</details>
<details>
<summary>摘要</summary>
自适应预训练（SSP）已经成为机器学习中受欢迎的技术之一，允许无需标注数据抽取有意义的特征表示。在计算机视觉领域，预训练视transformer（ViT）已经发挥了重要的作用，促进了转移学习。然而，模型的规模快速增长，使得训练这些大型模型的成本急剧增加。本研究旨在评估无标注自适应学习（SSL）技术在计算机视觉任务中的效果，不需要训练，以模仿人类的一般化和未看过物体的识别能力。为此，我们提出了一种零shot分割评估方法，基于提示 patch。给定目标对象的一点为提示，算法计算选定 patch 和其他 patches 之间的相似度图，然后应用简单的阈值分割目标。此外，我们还进行了内部和外部相似性评估，以评估 SSL ViTs 的抽象能力。基于零shot分割和抽象能力的发现，我们设计了一种简单的 SSP 方法，称为 MMC。这种方法结合了压缩图像模型，自动填充自适应特征，以及全局对比，以提高 SSL ViTs 的抽象表示能力。因此，我们的提案可以减少内部和外部相似性的重叠，从而实现图像中效果的对象分割。我们的实验表明，MMC 在多个数据集上达到了顶尖的 zero-shot semantic segmentation结果。
</details></li>
</ul>
<hr>
<h2 id="Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data"><a href="#Exploration-of-Rashomon-Set-Assists-Explanations-for-Medical-Data" class="headerlink" title="Exploration of Rashomon Set Assists Explanations for Medical Data"></a>Exploration of Rashomon Set Assists Explanations for Medical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11446">http://arxiv.org/abs/2308.11446</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katarzyna Kobylińska, Mateusz Krzyziński, Rafał Machowicz, Mariusz Adamek, Przemysław Biecek<br>for:This paper aims to address the problem of selecting a single model that maximizes a performance metric in machine learning modeling, particularly in medical and healthcare studies where the objective is to generate valuable insights beyond predictions.methods:The proposed approach uses a novel process to explore the Rashomon set of models, which includes identifying the most different models within the set using the $\texttt{Rashomon_DETECT}$ algorithm and comparing profiles illustrating prediction dependencies on variable values generated by XAI techniques. The Profile Disparity Index (PDI) is introduced to quantify differences in variable effects among models.results:The approach is applied to predict survival among hemophagocytic lymphohistiocytosis (HLH) patients and is shown to be effective in identifying the most informative models. The approach is also benchmarked on other medical data sets, demonstrating its versatility and utility in various contexts.<details>
<summary>Abstract</summary>
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorithm. This algorithm compares profiles illustrating prediction dependencies on variable values generated by eXplainable Artificial Intelligence (XAI) techniques. To quantify differences in variable effects among models, we introduce the Profile Disparity Index (PDI) based on measures from functional data analysis. To illustrate the effectiveness of our approach, we showcase its application in predicting survival among hemophagocytic lymphohistiocytosis (HLH) patients - a foundational case study. Additionally, we benchmark our approach on other medical data sets, demonstrating its versatility and utility in various contexts.
</details>
<details>
<summary>摘要</summary>
传统的机器学习模型选择过程是选择最大化一个选择性指标的模型。然而，这种方法会忽略一些较差的模型，尤其在医疗和健康研究中， где目标不仅是预测，还是获得有价值的发现。在一个称为“Rashomon set”的集合中，包含一些性能几乎与最佳一样的模型时，这种问题变得非常棘手。这篇论文提出了一种新的方法，以探索Rashomon set模型，从传统模型选择方法的基础上扩展。我们的核心思想是通过引入的 $\texttt{Rashomon_DETECT}$ 算法，确定Rashomon set中最为不同的模型。我们还引入了 Profile Disparity Index（PDI），以量化不同模型中变量效果的差异。我们通过使用扩展ible Artificial Intelligence（XAI）技术生成的变量值预测 Profiling，来评估模型之间的差异。我们在针对 Hemophagocytic lymphohistiocytosis（HLH）患者的存活预测中进行了应用示例，并在其他医疗数据集上进行了多种场景的比较。
</details></li>
</ul>
<hr>
<h2 id="Inferring-gender-from-name-a-large-scale-performance-evaluation-study"><a href="#Inferring-gender-from-name-a-large-scale-performance-evaluation-study" class="headerlink" title="Inferring gender from name: a large scale performance evaluation study"></a>Inferring gender from name: a large scale performance evaluation study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12381">http://arxiv.org/abs/2308.12381</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kriste Krstovski, Yao Lu, Ye Xu</li>
<li>for: 本研究旨在评估现有名字到性别推断的方法的性能，以及提出新的混合方法以提高性别推断的准确性。</li>
<li>methods: 本研究使用了多个现有的名字推断算法和软件产品，并对其进行大规模的性能评估。同时，我们还提出了两种新的混合方法，以提高性别推断的准确性。</li>
<li>results: 研究结果显示，现有的名字推断方法在不同的数据集上的性能差异较大。此外，我们还发现了一些新的混合方法，可以提高性别推断的准确性。<details>
<summary>Abstract</summary>
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Nevertheless, the existing approaches have yet to be systematically evaluated and compared, making it challenging to determine the optimal approach for future research. In this work, we conducted a large scale performance evaluation of existing approaches for name-to-gender inference. Analysis are performed using a variety of large annotated datasets of names. We further propose two new hybrid approaches that achieve better performance than any single existing approach.
</details>
<details>
<summary>摘要</summary>
人的性别信息是科学研究的重要参数，广泛应用于医学、社会学、政治科学和经济等多个领域。然而，随着大数据的普及， gender信息越来越难以获得。在这些情况下，研究人员需要从可用的信息中推断性别，主要来自人名。虽然这可能会提出一些伦理问题，但由于没有可靠的替代方案，研究人员需要采用这种方法，以达到研究目标。由于这种方法在全球各地的学术机构、产业、政府和非政府组织中都有广泛应用，因此需要系统地评估和比较现有的方法，以确定未来研究中最佳的方法。在这项工作中，我们进行了大规模性能评估现有的名称到性别推断方法。我们使用了多个大型注释的名称集来进行分析，并提出了两种新的混合方法，其性能比任何单独的现有方法更高。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework"><a href="#A-Study-on-the-Impact-of-Non-confounding-Covariates-on-the-Inferential-Performance-of-Methods-based-on-the-Potential-Outcome-Framework" class="headerlink" title="A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework"></a>A Study on the Impact of Non-confounding Covariates on the Inferential Performance of Methods based on the Potential Outcome Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11676">http://arxiv.org/abs/2308.11676</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghe Zhao, Shuai Fu, Huiyan Sun</li>
<li>for: 这个论文的主要目标是解释 causal inference 中 confounding 问题的几种方法，以及这些方法如何处理高维 covariates。</li>
<li>methods: 这个论文使用了 Potential Outcome Framework (POF) 和 causal inference models based on POF (CIMs-B-POF)，并对这些模型的应用进行了系统性的分析。</li>
<li>results: 研究发现，在减少偏见时，最佳情况是 covariates 仅包含 confounders；在推断 counterfactual outcomes 时，调整变量对准确预测做出了贡献。此外，对 synthetic datasets 进行了广泛的实验，并经过了 теоретиче分析。<details>
<summary>Abstract</summary>
The Potential Outcome Framework (POF) plays a prominent role in the field of causal inference. Most causal inference models based on the POF (CIMs-B-POF) are designed for eliminating confounding bias and default to an underlying assumption of Confounding Covariates. This assumption posits that the covariates consist solely of confounders. However, the assumption of Confounding Covariates is challenging to maintain in practice, particularly when dealing with high-dimensional covariates. While certain methods have been proposed to differentiate the distinct components of covariates prior to conducting causal inference, the consequences of treating non-confounding covariates as confounders remain unclear. This ambiguity poses a potential risk when applying the CIMs-B-POF in practical scenarios. In this paper, we present a unified graphical framework for the CIMs-B-POF, which greatly enhances the comprehension of these models' underlying principles. Using this graphical framework, we quantitatively analyze the extent to which the inference performance of CIMs-B-POF is influenced when incorporating various types of non-confounding covariates, such as instrumental variables, mediators, colliders, and adjustment variables. The key findings are: in the task of eliminating confounding bias, the optimal scenario is for the covariates to exclusively encompass confounders; in the subsequent task of inferring counterfactual outcomes, the adjustment variables contribute to more accurate inferences. Furthermore, extensive experiments conducted on synthetic datasets consistently validate these theoretical conclusions.
</details>
<details>
<summary>摘要</summary>
《潜在结果框架（POF）在 causal inference 领域发挥突出作用。大多数基于 POF 的 causal inference 模型（CIMs-B-POF）是设计用于消除干扰偏见的，默认假设是 Confounding Covariates 假设。这个假设认为covariates 仅仅包含干扰因素。然而，在实践中保持 Confounding Covariates 假设是困难的，特别是面临高维 covariates 时。虽然一些方法已经提出来分解 covariates 的不同组成部分以前进行 causal inference，但对于不是干扰因素的 covariates 的处理仍然存在uncertainty。这种不确定性可能会在实践中应用 CIMs-B-POF 时产生风险。在这篇论文中，我们提出了一个统一的图形框架 для CIMs-B-POF，这有助于更好地理解这些模型的内在原理。使用这个图形框架，我们量化分析了 CIMs-B-POF 在不同类型的非干扰 covariates （如工具变量、中介变量、冲击变量和调整变量）的 incorporation 影响了 causal inference 的性能。我们发现，在消除干扰偏见的任务中，covariates 应该仅仅包含干扰因素；在接下来的计算Counterfactual 结果任务中，调整变量对更加准确的推断做出了贡献。此外，我们在 synthetic 数据上进行了广泛的实验，并 consistently 验证了这些理论结论。
</details></li>
</ul>
<hr>
<h2 id="TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search"><a href="#TurboViT-Generating-Fast-Vision-Transformers-via-Generative-Architecture-Search" class="headerlink" title="TurboViT: Generating Fast Vision Transformers via Generative Architecture Search"></a>TurboViT: Generating Fast Vision Transformers via Generative Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11421">http://arxiv.org/abs/2308.11421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Wong, Saad Abbasi, Saeejith Nair</li>
<li>for: 本研究旨在设计高效的视觉 трансформер架构，以满足高速、低内存的应用需求。</li>
<li>methods: 本研究使用生成架构搜索（GAS）技术，通过搜索和优化mask单元注意力和Q-pooling设计模式，生成高效的层次结构视觉 трансформер架构设计。</li>
<li>results: 对于ImageNet-1K数据集，TurboViT架构设计在同等准确性下，与10个状态的有效视觉 трансформер网络架构设计相比，具有较低的建筑计算复杂度（&gt;2.47倍小于FasterViT-0）和计算复杂度（&gt;3.4倍少于MobileViT2-2.0），同时在低延迟和批处理场景中也表现出了优秀的执行速度和吞吐量（&gt;3.21倍低于FasterViT-0的延迟和&gt;3.18倍高于MobileViT2-2.0）。<details>
<summary>Abstract</summary>
Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47$\times$ smaller than FasterViT-0 while achieving same accuracy) and computational complexity (>3.4$\times$ fewer FLOPs and 0.9% higher accuracy than MobileViT2-2.0) when compared to 10 other state-of-the-art efficient vision transformer network architecture designs within a similar range of accuracy on the ImageNet-1K dataset. Furthermore, TurboViT demonstrated strong inference latency and throughput in both low-latency and batch processing scenarios (>3.21$\times$ lower latency and >3.18$\times$ higher throughput compared to FasterViT-0 for low-latency scenario). These promising results demonstrate the efficacy of leveraging generative architecture search for generating efficient transformer architecture designs for high-throughput scenarios.
</details>
<details>
<summary>摘要</summary>
视transformer在近年来的视觉任务中表现出了无 precedent的水平。然而，这些网络架构的建筑和计算复杂性使其在实际应用中高throughput低内存要求下部署成为了挑战。因此，有一些研究最近关于效率视transformer架构设计。在这项研究中，我们通过生成架构搜索（GAS）生成高效的视transformer架构设计，以达到精准和建筑计算效率的平衡。通过这种生成架构搜索过程，我们创造了TurboViT，一种高效的层次结构视transformer架构设计，围绕着面积单元注意力和Q-pooling设计模式。得到的TurboViT架构设计在与其他10种状态当前高效视transformer网络架构设计相比，在ImageNet-1K dataset上实现了明显的建筑计算复杂性下降（>2.47倍小于FasterViT-0，同等精准）和计算复杂性下降（>3.4倍少于MobileViT2-2.0，同等精准）。此外，TurboViT在低延迟和批处理场景中显示出了强大的执行速度和吞吐量（>3.21倍低于FasterViT-0的延迟和>3.18倍高于MobileViT2-2.0）。这些出色的结果证明了利用生成架构搜索生成高效的transformer架构设计的可行性。
</details></li>
</ul>
<hr>
<h2 id="Designing-an-attack-defense-game-how-to-increase-robustness-of-financial-transaction-models-via-a-competition"><a href="#Designing-an-attack-defense-game-how-to-increase-robustness-of-financial-transaction-models-via-a-competition" class="headerlink" title="Designing an attack-defense game: how to increase robustness of financial transaction models via a competition"></a>Designing an attack-defense game: how to increase robustness of financial transaction models via a competition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11406">http://arxiv.org/abs/2308.11406</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexey Zaytsev, Alex Natekin, Evgeni Vorsin, Valerii Smirnov, Georgii Smirnov, Oleg Sidorshin, Alexander Senin, Alexander Dudin, Dmitry Berestnev</li>
<li>for: 这种研究旨在 Investigating the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.</li>
<li>methods: 作者们使用了一场竞赛来进行实际和详细的研究问题，并对现代金融交易数据进行了实际应用。</li>
<li>results: 研究发现了一些新的攻击和防御策略，并证明了这些策略在现实生活中的可行性和有效性。<details>
<summary>Abstract</summary>
Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.   To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break it, and what techniques one should use to make it more robust, and introduction additional way to attack models or increase their robustness.   Our analysis continues with a meta-study on the used approaches with their power, numerical experiments, and accompanied ablations studies. We show that the developed attacks and defenses outperform existing alternatives from the literature while being practical in terms of execution, proving the validity of the competition as a tool for uncovering vulnerabilities of machine learning models and mitigating them in various domains.
</details>
<details>
<summary>摘要</summary>
随着金融业的风险增加，机器学习模型的攻击和防御技术成为了紧迫的问题。随着银行更广泛地应用更加准确但可能脆弱的神经网络，这种威胁变得更加严重。我们希望通过研究现有的敌对攻击和防御策略来深入理解这些机器学习模型在使用时的漏洞和攻击方法。为了实现这一目标，我们设计了一项竞赛，允许参与者在真实的金融交易数据上进行实际的攻击和防御研究。参与者在直接对抗的环境中竞争，这使得可能的攻击和防御策略得到了充分的检验。我们的主要贡献包括对竞赛的竞争动态进行分析，回答如何隐藏模型于恶意用户，模型如何快速被破坏，以及如何使模型更加坚强的问题。我们的分析继续进行meta研究，对使用的方法进行评估，并通过数学实验和附加的ablation研究来证明我们的攻击和防御策略的有效性。我们的结果显示，我们提出的攻击和防御策略在实施上具有优势，并在不同领域中能够实现有效的抵御。因此，我们的竞赛和分析证明了竞赛是一种有效的工具，用于探索机器学习模型的漏洞并实现其在不同领域的防御。
</details></li>
</ul>
<hr>
<h2 id="Non-Redundant-Combination-of-Hand-Crafted-and-Deep-Learning-Radiomics-Application-to-the-Early-Detection-of-Pancreatic-Cancer"><a href="#Non-Redundant-Combination-of-Hand-Crafted-and-Deep-Learning-Radiomics-Application-to-the-Early-Detection-of-Pancreatic-Cancer" class="headerlink" title="Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer"></a>Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11389">http://arxiv.org/abs/2308.11389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebeca Vétil, Clément Abi-Nader, Alexandre Bône, Marie-Pierre Vullierme, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch</li>
<li>for: 这篇论文旨在解决深度学习医学影像特征（DLR）和手工设计医学影像特征（HCR）之间的重叠问题。</li>
<li>methods: 作者使用VAE实现DLR特征EXTRACTING，并通过最小化这两种特征之间的相互信息来保持其独立性。</li>
<li>results: 研究结果显示，结合非重叠DLR和HCR特征可以提高预测早期肝癌标志物的精度，相比基eline方法不 Addressing redundancy or solely relying on HCR features。<details>
<summary>Abstract</summary>
We address the problem of learning Deep Learning Radiomics (DLR) that are not redundant with Hand-Crafted Radiomics (HCR). To do so, we extract DLR features using a VAE while enforcing their independence with HCR features by minimizing their mutual information. The resulting DLR features can be combined with hand-crafted ones and leveraged by a classifier to predict early markers of cancer. We illustrate our method on four early markers of pancreatic cancer and validate it on a large independent test set. Our results highlight the value of combining non-redundant DLR and HCR features, as evidenced by an improvement in the Area Under the Curve compared to baseline methods that do not address redundancy or solely rely on HCR features.
</details>
<details>
<summary>摘要</summary>
我们对深度学习医学图像学习（DLR）进行非重复的应用，以避免与手工医学图像学习（HCR）的重复。我们使用VAE对DLR特征进行提取，并在HCR特征和DLR特征之间强制独立性，以避免它们之间的相互信息。得到的DLR特征可以与手工医学图像学习特征结合，并由分类器使用以预测肝癌的早期标志。我们在四种肝癌早期标志上运算我们的方法，并在一个大的独立测试集上验证。我们的结果表明，结合非重复的DLR和HCR特征可以提高预测性能，比基eline方法不 Addressing redundancy or solely relying on HCR features。
</details></li>
</ul>
<hr>
<h2 id="Targeted-Data-Augmentation-for-bias-mitigation"><a href="#Targeted-Data-Augmentation-for-bias-mitigation" class="headerlink" title="Targeted Data Augmentation for bias mitigation"></a>Targeted Data Augmentation for bias mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11386">http://arxiv.org/abs/2308.11386</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Maria Ferlin, Michał Grochowski</li>
<li>For: The paper aims to address the issue of bias in AI systems by introducing a novel approach called Targeted Data Augmentation (TDA).* Methods: The paper uses classical data augmentation techniques to insert biases into the training data, which helps to mitigate biases in the models.* Results: The paper shows that the proposed TDA approach can significantly decrease bias measures while maintaining a negligible increase in the error rate, using two diverse datasets of clinical skin lesions and male and female faces.Here are the three key points in Simplified Chinese text:* For: 这篇论文目标是解决人工智能系统中的偏见问题，提出了一种新的方法called Targeted Data Augmentation (TDA)。* Methods: 这篇论文使用了经典的数据扩展技术，插入了偏见到训练数据中，以减少模型中的偏见。* Results: 论文表明，提出的TDA方法可以在两个多样化的数据集上（皮肤病和♂♀脸）获得了显著减少偏见的效果，而且保持了误差率的增加在一定范围内。<details>
<summary>Abstract</summary>
The development of fair and ethical AI systems requires careful consideration of bias mitigation, an area often overlooked or ignored. In this study, we introduce a novel and efficient approach for addressing biases called Targeted Data Augmentation (TDA), which leverages classical data augmentation techniques to tackle the pressing issue of bias in data and models. Unlike the laborious task of removing biases, our method proposes to insert biases instead, resulting in improved performance. To identify biases, we annotated two diverse datasets: a dataset of clinical skin lesions and a dataset of male and female faces. These bias annotations are published for the first time in this study, providing a valuable resource for future research. Through Counterfactual Bias Insertion, we discovered that biases associated with the frame, ruler, and glasses had a significant impact on models. By randomly introducing biases during training, we mitigated these biases and achieved a substantial decrease in bias measures, ranging from two-fold to more than 50-fold, while maintaining a negligible increase in the error rate.
</details>
<details>
<summary>摘要</summary>
发展公平和伦理AI系统需要仔细考虑偏见缓解，这一领域经常被排在第二队列。在这项研究中，我们介绍了一种新的和高效的方法，称为目标数据扩充（TDA），该方法利用经典数据扩充技术来解决数据和模型中的偏见问题。不同于努力地去除偏见，我们的方法提议插入偏见，从而提高性能。为了识别偏见，我们对两个多样化的数据集进行了标注：一个是皮肤病变数据集，另一个是♂♂♀♀脸部数据集。这些偏见标注在本研究中首次公布，为未来研究提供了一个价值的资源。通过对比事实插入偏见，我们发现了框架、尺度和镜片等偏见对模型产生了重要影响。通过在训练过程中随机插入偏见，我们成功缓解了这些偏见，并实现了偏见度量的显著减少，从2倍到更多于50倍，同时保持了错误率的微不足百分之一增加。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores"><a href="#Interpretable-Distribution-Invariant-Fairness-Measures-for-Continuous-Scores" class="headerlink" title="Interpretable Distribution-Invariant Fairness Measures for Continuous Scores"></a>Interpretable Distribution-Invariant Fairness Measures for Continuous Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11375">http://arxiv.org/abs/2308.11375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ann-Kristin Becker, Oana Dumitrasc, Klaus Broelemann</li>
<li>for: 这篇论文的目的是扩展 binary 决策中的算法公平度量到连续分数上。</li>
<li>methods: 这篇论文提出了一种基于 Wasserstein 距离的分布不变的公平度量方法，这种方法可以快速计算并对不同的模型、数据集、时间点进行比较。</li>
<li>results: 这篇论文通过实验表明，提出的公平度量方法可以更好地捕捉和评估不同群体之间的差异，并且可以比较不同的模型和数据集之间的偏见。<details>
<summary>Abstract</summary>
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-based fairness measures miss. Finally, we demonstrate their effectiveness through experiments on the most commonly used fairness benchmark datasets.
</details>
<details>
<summary>摘要</summary>
通常情况下，算法公平性的度量是在二进制决策中讨论的。我们将这种方法扩展到连续分数上。目前，ROC基本的度量是为此目的提出的。其他现有的方法对于分数的分布取决于，不适用于排名任务，或者其效果大小不容易理解。我们提议一种对连续分数的公平度量，基于 Wasserstein 距离，具有合理的解释，可以量化和解释群体偏见的强度以及不同模型、数据集、时间点之间的偏见。我们还 derivates了不同家族的现有公平度量的连接，并证明了我们的分布不变的公平度量在ROC基本的公平度量之上表现更好，因为它们更加显着，可以捕捉ROC基本的公平度量所miss的重要偏见。最后，我们通过使用最常用的公平性标准数据集进行实验，证明了它们的效果。
</details></li>
</ul>
<hr>
<h2 id="How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation"><a href="#How-Much-Temporal-Long-Term-Context-is-Needed-for-Action-Segmentation" class="headerlink" title="How Much Temporal Long-Term Context is Needed for Action Segmentation?"></a>How Much Temporal Long-Term Context is Needed for Action Segmentation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11358">http://arxiv.org/abs/2308.11358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ltcontext/ltcontext">https://github.com/ltcontext/ltcontext</a></li>
<li>paper_authors: Emad Bahrami, Gianpiero Francesca, Juergen Gall</li>
<li>for: 本研究的目的是回答 temporal action segmentation 中需要多少长期 temporal context 以达到最佳性能。</li>
<li>methods: 我们提出了一种基于 transformer 的模型，使用 sparse attention  capture 全视频的 context。</li>
<li>results: 我们的实验表明，模型全视频的 context 是 temporal action segmentation 中获得最佳性能的必要条件。<details>
<summary>Abstract</summary>
Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Breakfast, and Assembly101. Our experiments show that modeling the full context of a video is necessary to obtain the best performance for temporal action segmentation.
</details>
<details>
<summary>摘要</summary>
模型长期视频上下文是多种细化任务中的关键，包括时间动作分割。一个有趣的问题是如何确定最佳长期时间上下文的范围。虽然转换器可以模型视频的长期上下文，但这在长视频时会变得计算拒绝。 current works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window。although these approaches show good results, their performance is limited by their inability to capture the full context of a video。在这项工作中，我们试图回答 temporal action segmentation 需要多少长期时间上下文来达到最佳性能的问题。我们提出了一种基于转换器的模型，通过采用缺省注意力来捕捉整个视频的上下文。我们与当前领先的三个数据集（50Salads、Breakfast和Assembly101）进行比较，实验结果表明，模型整个视频的上下文是 temporal action segmentation 获得最佳性能的必要条件。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-assisted-exploration-for-affine-Deligne-Lusztig-varieties"><a href="#Machine-learning-assisted-exploration-for-affine-Deligne-Lusztig-varieties" class="headerlink" title="Machine learning assisted exploration for affine Deligne-Lusztig varieties"></a>Machine learning assisted exploration for affine Deligne-Lusztig varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11355">http://arxiv.org/abs/2308.11355</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinpf314/ml4adlv">https://github.com/jinpf314/ml4adlv</a></li>
<li>paper_authors: Bin Dong, Xuhua He, Pengfei Jin, Felix Schremmer, Qingchao Yu</li>
<li>for: 这 paper 的 primary objective 是 investigate the nonemptiness pattern, dimension 和 enumeration of irreducible components of affine Deligne-Lusztig varieties (ADLV).</li>
<li>methods: 这 paper 使用了 Machine Learning (ML) 协助的 framework 来探索 ADLV 的几何结构。 The proposed framework 包括 data generation, model training, pattern analysis, 和 human examination, which presents an intricate interplay between ML 和 pure mathematical research.</li>
<li>results: 这 paper 提出了一种新的, interdisciplinary 的方法，可以帮助加速 pure mathematical research, 并且可以找到新的 conjectures 和 promising research directions。 The paper 还 rediscovered the virtual dimension formula 和 provided a full mathematical proof of a newly identified problem concerning a certain lower bound of dimension.<details>
<summary>Abstract</summary>
This paper presents a novel, interdisciplinary study that leverages a Machine Learning (ML) assisted framework to explore the geometry of affine Deligne-Lusztig varieties (ADLV). The primary objective is to investigate the nonemptiness pattern, dimension and enumeration of irreducible components of ADLV. Our proposed framework demonstrates a recursive pipeline of data generation, model training, pattern analysis, and human examination, presenting an intricate interplay between ML and pure mathematical research. Notably, our data-generation process is nuanced, emphasizing the selection of meaningful subsets and appropriate feature sets. We demonstrate that this framework has a potential to accelerate pure mathematical research, leading to the discovery of new conjectures and promising research directions that could otherwise take significant time to uncover. We rediscover the virtual dimension formula and provide a full mathematical proof of a newly identified problem concerning a certain lower bound of dimension. Furthermore, we extend an open invitation to the readers by providing the source code for computing ADLV and the ML models, promoting further explorations. This paper concludes by sharing valuable experiences and highlighting lessons learned from this collaboration.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文介绍了一项新的、混合数学和机器学习（ML）的研究方法，用于探索非线性Deligne-Lusztig多样体（ADLV）的几何学。研究的主要目标是研究ADLV中的非空性模式、维度和总数，以及其不同组成部分的分析。我们提出的框架包括数据生成、模型训练、模式分析和人类检查，这些步骤之间存在着细腻的交互。特别是，我们的数据生成过程强调选择有意义的子集和适当的特征集。我们示出了这种框架的潜在可能性，可以加速纯数学研究，导致新的推测和潜在的研究方向的发现。此外，我们还重新发现了虚方尺式和一个新的问题的证明，即某些维度下的下界问题。此外，我们还向读者开放了源代码，以便进一步探索ADLV和ML模型。这篇论文结束于分享有价值的经验和学习到的教训。
</details></li>
</ul>
<hr>
<h2 id="WEARS-Wearable-Emotion-AI-with-Real-time-Sensor-data"><a href="#WEARS-Wearable-Emotion-AI-with-Real-time-Sensor-data" class="headerlink" title="WEARS: Wearable Emotion AI with Real-time Sensor data"></a>WEARS: Wearable Emotion AI with Real-time Sensor data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11673">http://arxiv.org/abs/2308.11673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Limbani, Daketi Yatin, Nitish Chaturvedi, Vaishnavi Moorthy, Pushpalatha M, Harichandana BSS, Sumit Kumar</li>
<li>For: The paper aims to predict user emotion using smartwatch sensors, with a focus on developing a practical and user-friendly system for everyday use.* Methods: The authors propose a framework that collects ground truth data in real-time using a combination of English and regional language-based videos to evoke emotions in participants. They use a binary classification approach due to limited dataset size and experiment with multiple machine-learning models, including Multi-Layer Perceptron, which achieves a maximum accuracy of 93.75% for pleasant-unpleasant moods.* Results: The paper reports an accuracy of 93.75% for predicting pleasant-unpleasant moods using Multi-Layer Perceptron, indicating the effectiveness of using smartwatch sensors for emotion recognition.Here’s the simplified Chinese text for the three key points:* For: 这篇论文旨在使用智能手表传感器预测用户情绪，旨在开发一个实用、日常使用的系统。* Methods: 作者提出了一种基于英文和地方语言基于视频诱发情感的实验框架，并使用多种机器学习模型进行实验，其中多层感知器达到了93.75%的最高准确率。* Results: 论文报告了使用多层感知器预测愉悦-不愉悦情绪的准确率为93.75%，表明智能手表传感器可以有效地进行情绪识别。<details>
<summary>Abstract</summary>
Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From the experimental results, Multi-Layer Perceptron has shown a maximum accuracy of 93.75 percent for pleasant-unpleasant (high/low valence classification) moods.
</details>
<details>
<summary>摘要</summary>
预测人类情感是一个研究领域，旨在理解人们的情感。现有方法主要集中在文本、音频、脸部表现等Modalities上，这些modalities可能是用户私有的。情感还可以从主题的心理数据中 derivation。许多方法使用多种生物学传感器进行情感识别，但不是所有传感器都是用户日常生活中容易使用。因此，我们提出了一个基于智能手表传感器的情感预测系统。我们设计了一个框架，以实时收集真实数据，使用英文和地区语言基于视频释放情感并收集数据。此外，我们将问题定型为二分类问题，因为数据集的小型限制了我们的实验。我们还进行了减少研究，以了解特征包括心率、加速度和陀螺仪传感器数据对情感的影响。从实验结果来看，多层感知器在高/低积极情感（愉悦-不愉悦）类别上达到最高的准确率为93.75%。
</details></li>
</ul>
<hr>
<h2 id="Careful-at-Estimation-and-Bold-at-Exploration"><a href="#Careful-at-Estimation-and-Bold-at-Exploration" class="headerlink" title="Careful at Estimation and Bold at Exploration"></a>Careful at Estimation and Bold at Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11348">http://arxiv.org/abs/2308.11348</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xing Chen, Yijun Liu, Zhaogeng Liu, Hechang Chen, Hengshuai Yao, Yi Chang</li>
<li>for: 本研究旨在解决 kontinuous action space 中的尝试策略问题，即因为无限多个动作而导致的规则性尝试策略不能得出总体结论。</li>
<li>methods: 我们基于 double-Q 函数框架，提出了一种新的尝试策略，以解决issues 1和2。我们首先提出了 Q 值更新的greedy Q softmax schema，然后 derivated expected Q value 的公式，并将其与 Q 值更新相结合。</li>
<li>results: 我们在 Mujoco  benchmark 上评估了我们的方法，并与之前的 state-of-the-art 方法进行比较。结果显示，我们的方法在不同环境中表现出色，特别是在最复杂的 Humanoid 环境中。<details>
<summary>Abstract</summary>
Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two different Q functions. For practicality, this theoretical basis is then extended to allow us to combine action exploration with the Q value update, except for the premise that we have a surrogate policy that behaves like this exploration policy. In practice, we construct such an exploration policy with a few sampled actions, and to meet the premise, we learn such a surrogate policy by minimizing the KL divergence between the target policy and the exploration policy constructed by the conservative Q. We evaluate our method on the Mujoco benchmark and demonstrate superior performance compared to previous state-of-the-art methods across various environments, particularly in the most complex Humanoid environment.
</details>
<details>
<summary>摘要</summary>
在连续动作空间中，探索策略经常是规则性的，因为动作的数量是无限的。在先前的工作中，已经证明了在决定性政策学习（DPRL）中，政策基于的探索是有利的。然而，DPRL中的政策基于探索有两个明显的问题：无目的探索和政策分化，并且政策梯度对探索是不一定有帮助的，因为估计不准确。基于双Q函数框架，我们介绍了一种新的探索策略，以解决这些问题。我们首先提出了Q值更新的软MAX schema，其中预期Q值是通过对动作的权重 sums来计算的。在这个过程中，权重是根据动作的greedy Q值进行调整。greedy Q值是两个Q函数中的最大值，而conservative Q值是两个Q函数中的最小值。为了实用，我们将这种理论基础扩展到允许我们将探索策略与Q值更新结合，只要我们有一个伪函数，这个伪函数在探索策略下被学习。在实践中，我们构建了一个这种探索策略，并通过最小化KL抽象度来学习这个伪函数。我们在Mujoco benchmark上评估了我们的方法，并在不同的环境中达到了先前的状态艺术方法的超越性表现，特别是在最复杂的人型环境中。
</details></li>
</ul>
<hr>
<h2 id="ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models"><a href="#ProAgent-Building-Proactive-Cooperative-AI-with-Large-Language-Models" class="headerlink" title="ProAgent: Building Proactive Cooperative AI with Large Language Models"></a>ProAgent: Building Proactive Cooperative AI with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11339">http://arxiv.org/abs/2308.11339</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-Alignment/ProAgent">https://github.com/PKU-Alignment/ProAgent</a></li>
<li>paper_authors: Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang<br>for:This paper aims to develop a novel framework called ProAgent that enables AI agents to cooperate with other agents and humans in a more effective and adaptive manner.methods:ProAgent leverages large language models (LLMs) to anticipate teammates’ decisions and formulate better plans, and it has a high degree of modularity and interpretability, making it easy to integrate with different coordination scenarios.results:Experimental evaluations show that ProAgent outperforms five comparison methods in cooperation with AI agents, and it also performs better than the current state-of-the-art, COLE, when cooperating with human proxy models, with an average improvement of over 10%. These results demonstrate the effectiveness of ProAgent in facilitating human-AI collaboration.Here’s the simplified Chinese version:for:这篇论文目标是开发一种名为ProAgent的新框架，使AI代理能够更有效地和其他代理和人类合作。methods:ProAgent利用大型自然语言模型（LLMs）预测同事的决策并提出更好的计划，它具有高度的可模块化和可解释性，方便与不同的协调方案集成。results:实验评估表明，ProAgent比五种参考方法在与AI代理合作中表现更出色，并且与当前状态艺术的COLE相比，在与人类代理模型合作时的表现也有超过10%的提升。这些结果证明ProAgent在人AI合作方面的作用是有效的。<details>
<summary>Abstract</summary>
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10\% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details>
<details>
<summary>摘要</summary>
building 人工智能（AI） WITH adaptive behaviors in human-AI cooperation 是AGI研究中的重点ocus。 current methods for developing cooperative agents mainly rely on learning-based methods, where policy generalization heavily relies on past interactions with specific teammates. these approaches limit the agent's ability to adjust its strategy when faced with new teammates. we propose \textbf{ProAgent}, a novel framework that leverages large language models (LLMs) to create a proactive agent that can anticipate teammates' upcoming decisions and formulate improved plans for itself. ProAgent excels at cooperative reasoning and can dynamically adapt its behavior to enhance collaborative efforts with teammates. furthermore, the ProAgent framework has a high degree of modularity and interpretability, making it easy to integrate into various coordination scenarios. experimental evaluations conducted within the Overcook-AI framework show that ProAgent outperforms five self-play and population-based training methods in cooperation with AI agents, with an average improvement of over 10% compared to the current state-of-the-art, COLE. this improvement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. these findings pave the way for future research on human-robot collaborations. for a hands-on demonstration, please visit \url{https://pku-proagent.github.io}.
</details></li>
</ul>
<hr>
<h2 id="Generalising-sequence-models-for-epigenome-predictions-with-tissue-and-assay-embeddings"><a href="#Generalising-sequence-models-for-epigenome-predictions-with-tissue-and-assay-embeddings" class="headerlink" title="Generalising sequence models for epigenome predictions with tissue and assay embeddings"></a>Generalising sequence models for epigenome predictions with tissue and assay embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11671">http://arxiv.org/abs/2308.11671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacob Deasy, Ron Schwessinger, Ferran Gonzalez, Stephen Young, Kim Branson</li>
<li>for: 本研究旨在提高epigenetic profile预测方法的精度和可靠性，通过integrating tissue和assay embeddings into a Contextualised Genomic Network (CGN)来增强long-range sequence embeddings的准确性。</li>
<li>methods: 本研究使用了Contextualised Genomic Network (CGN)，其中integrates tissue和assay embeddings into a single network，以增强long-range sequence embeddings的准确性。</li>
<li>results: 研究表明，使用CGN可以在各种实验条件下实现强相关性，而且超过了现有方法的表现。此外，研究还提供了关于基因变化对epigenetic sequence模型训练的首个发现。<details>
<summary>Abstract</summary>
Sequence modelling approaches for epigenetic profile prediction have recently expanded in terms of sequence length, model size, and profile diversity. However, current models cannot infer on many experimentally feasible tissue and assay pairs due to poor usage of contextual information, limiting $\textit{in silico}$ understanding of regulatory genomics. We demonstrate that strong correlation can be achieved across a large range of experimental conditions by integrating tissue and assay embeddings into a Contextualised Genomic Network (CGN). In contrast to previous approaches, we enhance long-range sequence embeddings with contextual information in the input space, rather than expanding the output space. We exhibit the efficacy of our approach across a broad set of epigenetic profiles and provide the first insights into the effect of genetic variants on epigenetic sequence model training. Our general approach to context integration exceeds state of the art in multiple settings while employing a more rigorous validation procedure.
</details>
<details>
<summary>摘要</summary>
Sequence模型方法 для脱氧核型谱预测最近几年得到了较长的序列长度、更大的模型大小和更多的谱型多样性。然而，当前的模型无法对许多实验可行的组织和检测对照进行预测，因为忽略了Contextual信息的使用，这限制了$\textit{in silico}$的规则生物学理解。我们示示了在各种实验条件下强相关性的 achievement，通过将组织和检测嵌入到Contextualised Genomic Network（CGN）中，而不是扩展输出空间。我们在输入空间中增强长距离序列嵌入中的Contextual信息，而不是扩展输出空间。我们在多种脱氧核型谱中展示了我们的方法的效果，并提供了脱氧核型序列模型训练中基因变化的首次研究。我们的通用方法进行Context集成超过了当前状态的多个设置，而且采用更严格的验证过程。
</details></li>
</ul>
<hr>
<h2 id="Protect-Federated-Learning-Against-Backdoor-Attacks-via-Data-Free-Trigger-Generation"><a href="#Protect-Federated-Learning-Against-Backdoor-Attacks-via-Data-Free-Trigger-Generation" class="headerlink" title="Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation"></a>Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11333">http://arxiv.org/abs/2308.11333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanxin Yang, Ming Hu, Yue Cao, Jun Xia, Yihao Huang, Yang Liu, Mingsong Chen</li>
<li>for: 防止 Federated Learning (FL) 中的毒蛋攻击 (backdoor attacks)</li>
<li>methods: 基于毒蛋攻击的两个特点（ triggers 更快学习，触发图像分类更大）的数据free trigger-generation-based 防御方法</li>
<li>results: 对多种现有毒蛋攻击和七种现有防御方法进行了广泛的实验，结果表明我们的方法可以成功防止毒蛋攻击，并在 IID 和非 IID 场景下出perform 所有七种防御方法。特别是，我们的方法可以在 80% 的客户端被恶意攻击时仍然成功防止毒蛋攻击。<details>
<summary>Abstract</summary>
As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the effect of these generated images. By using these trigger images, our approach eliminates poisoned models to ensure the updated global model is benign. Comprehensive experiments demonstrate that our approach can defend against almost all the existing types of backdoor attacks and outperform all the seven state-of-the-art defense methods with both IID and non-IID scenarios. Especially, our approach can successfully defend against the backdoor attack even when 80\% of the clients are malicious.
</details>
<details>
<summary>摘要</summary>
作为分布式机器学习模式，联邦学习（FL）允许大规模客户端共同训练模型，而无需分享原始数据。然而，由于客户端数据审核不足，FL容易受到毒素攻击，尤其是后门攻击。攻击者可以使用毒素数据进行本地训练或直接改变模型参数，从而轻松地植入后门到模型中，使模型在目标图像中产生误分类。为解决这些问题，我们提出了一种基于两个后门攻击特征的新防御方法：一是触发更快速学习than正常知识，二是触发模式在图像分类中比正常类型更大的影响。我们的方法生成图像新学习的知识，并将触发图像过滤通过评估这些生成的图像的效果。通过使用这些触发图像，我们的方法可以消除毒素模型，确保更新的全局模型是无辜的。我们的实验表明，我们的方法可以对现有的所有类型后门攻击进行防御，并在IID和非IID场景中高效超越七种现状防御方法。特别是，我们的方法可以成功防御80%的客户端恶意攻击。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-based-Positioning-using-Multivariate-Time-Series-Classification-for-Factory-Environments"><a href="#Machine-Learning-based-Positioning-using-Multivariate-Time-Series-Classification-for-Factory-Environments" class="headerlink" title="Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments"></a>Machine Learning-based Positioning using Multivariate Time Series Classification for Factory Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11670">http://arxiv.org/abs/2308.11670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nisal Hemadasa Manikku Badu, Marcus Venzke, Volker Turau, Yanqiu Huang</li>
<li>for: 这个论文是为了解决在私隐要求高的Factory环境中实现indoor位置系统（IPS），而不需要外部基础设施和数据。</li>
<li>methods: 该论文使用机器学习（ML）技术，利用运动和室内传感器来定位移动物体。它将问题定义为多变量时间序列分类（MTSC）问题，并对不同的机器学习模型进行比较分析，以选择最佳适用于IoT设备的模型。</li>
<li>results: 研究人员通过使用一个新的时间序列数据集来评估和比较不同的机器学习模型，发现所有评估模型的准确率都超过80%。CNN-1D表现最佳，其次是MLP。DT模型具有最低的内存占用量和计算延迟，因此有可能在实际应用中使用。<details>
<summary>Abstract</summary>
Indoor Positioning Systems (IPS) gained importance in many industrial applications. State-of-the-art solutions heavily rely on external infrastructures and are subject to potential privacy compromises, external information requirements, and assumptions, that make it unfavorable for environments demanding privacy and prolonged functionality. In certain environments deploying supplementary infrastructures for indoor positioning could be infeasible and expensive. Recent developments in machine learning (ML) offer solutions to address these limitations relying only on the data from onboard sensors of IoT devices. However, it is unclear which model fits best considering the resource constraints of IoT devices. This paper presents a machine learning-based indoor positioning system, using motion and ambient sensors, to localize a moving entity in privacy concerned factory environments. The problem is formulated as a multivariate time series classification (MTSC) and a comparative analysis of different machine learning models is conducted in order to address it. We introduce a novel time series dataset emulating the assembly lines of a factory. This dataset is utilized to assess and compare the selected models in terms of accuracy, memory footprint and inference speed. The results illustrate that all evaluated models can achieve accuracies above 80 %. CNN-1D shows the most balanced performance, followed by MLP. DT was found to have the lowest memory footprint and inference latency, indicating its potential for a deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
内部定位系统（IPS）在多个业务应用中升级了其重要性。现代解决方案强依赖于外部基础设施，受到隐私侵犯、外部信息需求和假设的限制，使其不适合需要隐私和持续性的环境。在某些环境中，为实现内部定位而部署补充基础设施可能是不可能的和昂贵的。现代机器学习（ML）技术提供了解决这些限制的方案，不需要外部基础设施，只需基于 IoT 设备上的固件传感器数据。然而，选择最佳模型是有困难的，因为 IoT 设备的资源限制是一个重要的考虑因素。本文描述了一种基于机器学习的内部定位系统，使用运动和室内传感器来确定在隐私问题中的工厂环境中移动的实体。问题被формализова为多变量时间序列分类（MTSC），并对不同的机器学习模型进行比较分析，以解决它。我们引入了一个新的时间序列数据集，该数据集模拟了制造工程厂的生产线。这个数据集用于评估和比较选择的模型，以确定它们的准确率、内存占用量和推理速度。结果显示所有评估模型均可达到上百分之八的准确率。CNN-1D表现最佳，其次是 MLP。DT 具有最低的内存占用量和推理延迟，因此它在实际应用中具有潜在的优势。
</details></li>
</ul>
<hr>
<h2 id="Class-Label-aware-Graph-Anomaly-Detection"><a href="#Class-Label-aware-Graph-Anomaly-Detection" class="headerlink" title="Class Label-aware Graph Anomaly Detection"></a>Class Label-aware Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11669">http://arxiv.org/abs/2308.11669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jhkim611/clad">https://github.com/jhkim611/clad</a></li>
<li>paper_authors: Junghoon Kim, Yeonjun In, Kanghoon Yoon, Junmo Lee, Chanyoung Park</li>
<li>for: 本研究旨在提高无监督图像检测方法的性能，特别是在缺乏类别标签信息的情况下。</li>
<li>methods: 本研究提出了一种基于类别标签的图像检测方法，即类别标签感知图像检测方法（CLAD）。该方法利用有限数量的标签节点来增强无监督图像检测的性能。</li>
<li>results: 对于十个数据集，CLAD方法在无监督情况下表现出了比较出色的性能，甚至超过了现有的无监督图像检测方法。<details>
<summary>Abstract</summary>
Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:Unsupervised GAD methods assume the lack of anomaly labels, i.e., whether a node is anomalous or not. One common observation we made from previous unsupervised methods is that they not only assume the absence of such anomaly labels, but also the absence of class labels (the class a node belongs to used in a general node classification task). In this work, we study the utility of class labels for unsupervised GAD; in particular, how they enhance the detection of structural anomalies. To this end, we propose a Class Label-aware Graph Anomaly Detection framework (CLAD) that utilizes a limited amount of labeled nodes to enhance the performance of unsupervised GAD. Extensive experiments on ten datasets demonstrate the superior performance of CLAD in comparison to existing unsupervised GAD methods, even in the absence of ground-truth class label information. The source code for CLAD is available at \url{https://github.com/jhkim611/CLAD}.Translate the text into Simplified Chinese:<</SYS>>Here's the translation:Unsupervised GAD方法假设缺乏异常标签，即节点是否异常。我们从前一些无监督方法中发现，不仅假设缺乏这些异常标签，还假设缺乏类别标签（用于普通的节点分类任务中）。在这个工作中，我们研究了类别标签是否增强了无监督GAD的检测性。为此，我们提出了一个具有限量标签的Class Label-aware Graph Anomaly Detection框架（CLAD），可以将有限量的标签用于增强无监督GAD的性能。我们在十个数据集上进行了广泛的实验，结果显示CLAD在与现有的无监督GAD方法比较时表现更好，甚至在没有真实的类别标签信息时仍能取得更高的性能。CLAD的源代码可以在\url{https://github.com/jhkim611/CLAD}中找到。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-of-Transformers’-Predictions-via-Topological-Analysis-of-the-Attention-Matrices"><a href="#Uncertainty-Estimation-of-Transformers’-Predictions-via-Topological-Analysis-of-the-Attention-Matrices" class="headerlink" title="Uncertainty Estimation of Transformers’ Predictions via Topological Analysis of the Attention Matrices"></a>Uncertainty Estimation of Transformers’ Predictions via Topological Analysis of the Attention Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11295">http://arxiv.org/abs/2308.11295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elizaveta Kostenok, Daniil Cherniavskii, Alexey Zaytsev</li>
<li>for: This paper aims to address the open problem of determining the degree of confidence of deep learning models in their predictions, specifically for text classification models using the Transformer architecture.</li>
<li>methods: The proposed method for uncertainty estimation is based on Topological Data Analysis (TDA) and utilizes the attention mechanism to form relationships between internal representations.</li>
<li>results: The proposed method outperforms classical methods in quality and opens up a new area of application for the attention mechanism, but requires the selection of topological features.<details>
<summary>Abstract</summary>
Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.
</details>
<details>
<summary>摘要</summary>
决定深度学习模型预测结果的信任度是自然语言处理领域的一个开问题。大多数经典方法的不确定性估计对文本分类模型来说是很弱的。我们设定了基于Transformer架构的神经网络中的不确定性估计任务。Transformer模型的一个重要特征是对应机制，它支持内部表示之间的信息流动。我们使用Topological Data Analysis方法来探索内部表示之间的关系，并将其用于预测模型的信任度。在这篇论文中，我们提出了基于对应机制的不确定性估计方法，并与经典方法进行比较。结果显示，提案的算法在质量上超过了现有的方法，并开启了一个新的对应机制的应用领域，但是需要选择几个类型的特征。
</details></li>
</ul>
<hr>
<h2 id="Network-Momentum-across-Asset-Classes"><a href="#Network-Momentum-across-Asset-Classes" class="headerlink" title="Network Momentum across Asset Classes"></a>Network Momentum across Asset Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11294">http://arxiv.org/abs/2308.11294</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyue Pu, Stephen Roberts, Xiaowen Dong, Stefan Zohren</li>
<li>for: 这篇论文探讨了网络势力的概念，即基于资产之间的势力扩散的新交易信号。</li>
<li>methods: 该论文使用了一种可读性高的图学学习模型，以探索不同资产类型之间的势力扩散网络。</li>
<li>results: 该论文提出了一种基于网络势力的多资产投资策略，并通过实证分析表明其具有1.5的沙勃级效果和22%的年化收益。<details>
<summary>Abstract</summary>
We investigate the concept of network momentum, a novel trading signal derived from momentum spillover across assets. Initially observed within the confines of pairwise economic and fundamental ties, such as the stock-bond connection of the same company and stocks linked through supply-demand chains, momentum spillover implies a propagation of momentum risk premium from one asset to another. The similarity of momentum risk premium, exemplified by co-movement patterns, has been spotted across multiple asset classes including commodities, equities, bonds and currencies. However, studying the network effect of momentum spillover across these classes has been challenging due to a lack of readily available common characteristics or economic ties beyond the company level. In this paper, we explore the interconnections of momentum features across a diverse range of 64 continuous future contracts spanning these four classes. We utilise a linear and interpretable graph learning model with minimal assumptions to reveal the intricacies of the momentum spillover network. By leveraging the learned networks, we construct a network momentum strategy that exhibits a Sharpe ratio of 1.5 and an annual return of 22%, after volatility scaling, from 2000 to 2022. This paper pioneers the examination of momentum spillover across multiple asset classes using only pricing data, presents a multi-asset investment strategy based on network momentum, and underscores the effectiveness of this strategy through robust empirical analysis.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation"><a href="#Improving-Knot-Prediction-in-Wood-Logs-with-Longitudinal-Feature-Propagation" class="headerlink" title="Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation"></a>Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11291">http://arxiv.org/abs/2308.11291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeremyfix/icvs2023">https://github.com/jeremyfix/icvs2023</a></li>
<li>paper_authors: Salim Khazem, Jeremy Fix, Cédric Pradalier</li>
<li>for: 本研究旨在预测木材中内部缺陷的位置，以提高木材质量评估的准确性和效率。</li>
<li>methods: 本研究使用了卷积回归神经网络来解决木材外形特征与内部缺陷之间的关系。</li>
<li>results: 研究结果表明，通过使用卷积回归神经网络进行预测，可以准确地预测木材内部缺陷的位置，并且可以通过便宜的外形测量设备进行实时检测。<details>
<summary>Abstract</summary>
The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
</details>
<details>
<summary>摘要</summary>
木材行业中木材质量受到内部和外部缺陷的影响，其中包括由树木支 Branches 生长而成的内部缺陷。当前，发现内部缺陷需要使用昂贵的设备如X射验器。在这篇论文中，我们Addressing the task of predicting the location of inner defects from the outer shape of the logs. Our dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.Here's the breakdown of the translation:* 木材 (mù cǎi) - wood* 行业 (xíng yè) - industry* 质量 (zhì yù) - quality* 受到 (shòu dào) - influenced by* 内部 (nèi bù) - inner* 缺陷 (jiǔ jī) - defects* 包括 (bāo gè) - including* 内部缺陷 (nèi bù jī jī) - inner defects* 由 (yù) - caused by* 树木 (shù mù) - trees* 支 (zhī) - branches* 生长 (shēng cháng) - growth* 成 (chéng) - formed* 内部缺陷 (nèi bù jī jī) - inner defects* 需要 (xū yào) - need* 使用 (shǐ yòng) - use* 昂贵 (áng jí) - expensive* 设备 (tiě bì) - equipment* X射验器 (X chū zhì qi) - X-ray scanner* Addressing (dào zhèng) - addressing* task (dao) - task* predicting (jì dì) - predicting* location (dì yì) - location* of (de) - of* inner defects (nèi bù jī jī) - inner defects* from (from) - from* the outer shape (wài xíng) - the outer shape* of the logs (de logs) - of the logs* Our dataset (wǒ de dataset) - our dataset* is built (is built) - is built* by extracting (by extracting) - by extracting* both (both) - both* the contours (the contours) - the contours* and the knots (and the knots) - and the knots* with X-ray measurements (with X-ray measurements) - with X-ray measurements* We propose (wǒ yù) - we propose* to solve (to solve) - to solve* this binary segmentation task (this binary segmentation task) - this binary segmentation task* by leveraging (by leveraging) - by leveraging* convolutional recurrent neural networks (convolutional recurrent neural networks) - convolutional recurrent neural networks* Once (once) - once* the neural network (the neural network) - the neural network* is trained (is trained) - is trained* inference can be performed (inference can be performed) - inference can be performed* from the outer shape (from the outer shape) - from the outer shape* measured with cheap devices (measured with cheap devices) - measured with cheap devices* such as laser profilers (such as laser profilers) - such as laser profilers* We demonstrate (wǒ yù) - we demonstrate* the effectiveness (the effectiveness) - the effectiveness* of our approach (of our approach) - of our approach* on fir (on fir) - on fir* and spruce (and spruce) - and spruce* tree species (tree species) - tree species* and perform ablation (and perform ablation) - and perform ablation* on the recurrence (on the recurrence) - on the recurrence* to demonstrate (to demonstrate) - to demonstrate* its importance (its importance) - its importance
</details></li>
</ul>
<hr>
<h2 id="ShadowNet-for-Data-Centric-Quantum-System-Learning"><a href="#ShadowNet-for-Data-Centric-Quantum-System-Learning" class="headerlink" title="ShadowNet for Data-Centric Quantum System Learning"></a>ShadowNet for Data-Centric Quantum System Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11290">http://arxiv.org/abs/2308.11290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Du, Yibo Yang, Tongliang Liu, Zhouchen Lin, Bernard Ghanem, Dacheng Tao<br>for: 这篇论文的目的是提出一种数据驱动学习方法，帮助解决大量量子系统的研究困难，尤其是量子状态实物探测和精确性评估等任务。methods: 这篇论文使用了神经网络协议和古典影子的方法，但这两种方法都有限制：前者受到预测不确定性的影响，后者则缺乏扩展能力。这篇论文提出了一个融合这两种方法的数据驱动学习模式，通过将古典影子与其他量子系统的轻松可取信息融合，创建训练数据集，然后让神经网络学习这个数据集中的对应规律。results: 这篇论文的结果显示，这个数据驱动学习模式可以在没有量子系统的资料时，通过训练神经网络，实现预测未见过的量子系统。此外，这个模式还具有快速适应和储存效率的特点，可以实现在大量量子系统中的应用。<details>
<summary>Abstract</summary>
Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic of classical shadows, enabling memory-efficient storage and faithful prediction. These features underscore the immense potential of the proposed data-centric approach in discovering novel and large-scale quantum systems. For concreteness, we present the instantiation of our paradigm in quantum state tomography and direct fidelity estimation tasks and conduct numerical analysis up to 60 qubits. Our work showcases the profound prospects of data-centric artificial intelligence to advance QSL in a faithful and generalizable manner.
</details>
<details>
<summary>摘要</summary>
大量量子系统的dinamics受到维度祸害。统计学学习提供了新的可能性，通过神经网络协议和类型暂影，然而这两种方法均有局限性：前者受到预测不确定性的困扰，后者缺乏泛化能力。我们提议一种数据驱动学习思维方式，结合这两种方法，以便推动多种量子学习任务（QSL）。特别是，我们的思维方式利用类型暂影和其他可见的量子系统信息，创建训练集，然后由神经网络学习探索QSL问题的下面规则。通过神经网络的泛化能力，这种思维方式可以在训练前线上进行offline训练，在推断阶段可以准确预测未经见过的系统，即使只有几个状态复制。此外，它继承了类型暂影的特点，具有储存效率和准确预测的特点。这些特点说明了我们提议的数据驱动方法在发现新的大规模量子系统方面的极大潜力。为了具体展示我们的思维方式，我们在量子状态探测和直接准确度估计任务中实现了这种思维方式，并进行了数值分析至60个量子比特。我们的工作展示了数据驱动人工智能在 faithful和泛化的方式下，进一步推动量子学习的前iers。
</details></li>
</ul>
<hr>
<h2 id="Test-Time-Embedding-Normalization-for-Popularity-Bias-Mitigation"><a href="#Test-Time-Embedding-Normalization-for-Popularity-Bias-Mitigation" class="headerlink" title="Test Time Embedding Normalization for Popularity Bias Mitigation"></a>Test Time Embedding Normalization for Popularity Bias Mitigation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11288">http://arxiv.org/abs/2308.11288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ml-postech/tten">https://github.com/ml-postech/tten</a></li>
<li>paper_authors: Dain Kim, Jinhyeok Park, Dongwoo Kim</li>
<li>for: 本研究旨在解决推荐系统中的人気偏袋问题，即推荐结果受到item popularity的影响。</li>
<li>methods: 本研究提出了’Test Time Embedding Normalization’方法，它是一种简单 yet effective的方法，可以有效地减少人気偏袋的影响。该方法在推荐阶段使用 норmalized item embedding来控制 embedding magnitude的影响，其与item popularity高度相关。</li>
<li>results: 通过广泛的实验，本研究证明了我们的方法可以与之前的偏袋缓解方法相比，提高了推荐结果的准确性。此外，我们还发现了用户和项目 embedding之间的关系，angular similarity between embeddings可以分辨 preferable 和 non-preferable 的项目，不abhängigkeit von их popularity。这些分析解释了我们的方法如何成功地减少人気偏袋的影响。<details>
<summary>Abstract</summary>
Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approach in eliminating the impact of popularity bias. Our code is available at https://github.com/ml-postech/TTEN.
</details>
<details>
<summary>摘要</summary>
受欢迎偏见是推荐系统领域中的一个广泛存在的问题，它导致受欢迎的项目在推荐结果中占据着主导地位。在这个工作中，我们提出了“测试时嵌入 normalization”作为一种简单 yet有效的策略来 Mitigate 受欢迎偏见，该策略在previous mitigation Approaches的性能上超过了很大的差。我们的方法利用了normalized item embedding During the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity。经过了广泛的实验，我们表明了我们的方法与sampled softmax loss combination 能够有效地减少受欢迎偏见，比之前的缓解方法更高效。我们进一步investigated the relationship between user and item embeddings and found that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity。这种分析解释了我们的方法在消除受欢迎偏见的机制。我们的代码可以在https://github.com/ml-postech/TTEN中找到。
</details></li>
</ul>
<hr>
<h2 id="CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation"><a href="#CNN-based-Cuneiform-Sign-Detection-Learned-from-Annotated-3D-Renderings-and-Mapped-Photographs-with-Illumination-Augmentation" class="headerlink" title="CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation"></a>CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11277">http://arxiv.org/abs/2308.11277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ernst Stötzner, Timo Homburg, Hubert Mara</li>
<li>for: 这篇论文是为了提供一种用于处理古代中东文明的数字工具，尤其是用于处理码iform文字，这种文字在陶 TABLETS上印刷了三千年以上，并且包含了许多不同的语言。</li>
<li>methods: 这篇论文使用了一种novel OCR-like方法来处理混合图像数据，包括将3D Rendering和摄影照片的标签转换到对应的3D模型上。它还使用了一个RepPoints探测器来预测文字的位置，并使用了GigaMesh的MSII基于曲线的呈现、Phong-shaded 3D模型和摄影照片，以及照明增强。</li>
<li>results: 根据结果，使用 Rendering 3D图像进行文字探测比使用摄影照片更好，而且这种方法在混合数据集上也表现良好。此外，使用Phong renderings和特别是MSII renderings可以提高摄影照片上的结果，这是全球规模最大的数据集。<details>
<summary>Abstract</summary>
Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D models, and photographs as well as illumination augmentation. The results show that using rendered 3D images for sign detection performs better than other work on photographs. In addition, our approach gives reasonably good results for photographs only, while it is best used for mixed datasets. More importantly, the Phong renderings, and especially the MSII renderings, improve the results on photographs, which is the largest dataset on a global scale.
</details>
<details>
<summary>摘要</summary>
受到古近东学界的挑战启发，我们开发了用于处理古代 кли牒文字的数字工具。这种3D文字印刷在泥 TABLETS上用于 более三千年和至少八种主要语言，它包含了数以千计的字符，其中一些在时间和空间上发生了变化。相比之下，图片是最常用于机器学习的表示方式，而墨渍绘制则容易被解释。我们使用的最佳3D数据集在可用。我们创建了HeiCuBeDa和MaiCuBeDa数据集，它们包含约500个注释的板。为了我们的新的OCR-like方法，我们提供了一个将3D渲染与图片之间的映射工具。我们使用的签名地址使用RepPoints探测器预测字符的位置。我们使用GigaMesh的MSII（曲率，请参考https://gigamesh.eu）基于的渲染、Phong灯光渲染和图片以及照明增强。结果表明，使用渲染的3D图像进行字符检测比其他工作更好。此外，我们的方法在图片只的情况下也能达到相对良好的结果，而且在混合数据集情况下表现最佳。特别是MSII渲染和Phong渲染在图片上提高了结果，这是全球规模最大的图片数据集。
</details></li>
</ul>
<hr>
<h2 id="FoX-Formation-aware-exploration-in-multi-agent-reinforcement-learning"><a href="#FoX-Formation-aware-exploration-in-multi-agent-reinforcement-learning" class="headerlink" title="FoX: Formation-aware exploration in multi-agent reinforcement learning"></a>FoX: Formation-aware exploration in multi-agent reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11272">http://arxiv.org/abs/2308.11272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonghyeon Jo, Sunwoo Lee, Junghyuk Yum, Seungyul Han</li>
<li>for: 这篇论文的目的是解决多智能代理人学习（MARL）中的探索问题，因为代理人的partial observability和探索空间的增长会导致探索问题增加。</li>
<li>methods: 本文使用了一个formation-based equivalence relation来缩小探索空间，并提出了一个novel的formation-aware探索（FoX）框架，帮助部分可观代理人探索到不同的formation中的状态。</li>
<li>results: numerical results显示，提出的FoX框架在Google Research Football（GRF）和Sparse Starcraft II multi-agent challenge（SMAC）任务上明显超过了现有的MARL算法。<details>
<summary>Abstract</summary>
Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC) tasks.
</details>
<details>
<summary>摘要</summary>
近些年来，深度多代理人学习（MARL）已经受到了各种合作多代理人任务中的成功，但是探索仍然是MARL中的挑战。这是因为代理人的部分可见性和代理人探索空间的快速增长，尤其是在代理人数量增加时。为解决探索空间的扩展性问题，我们定义了基于形态的相似关系在探索空间上，并尝试将搜索空间减少到有意义的状态。然后，我们提出了一种新的形态意识探索（FoX）框架，该框架鼓励部分可见的代理人访问不同形态中的状态，通过根据自己的观察来让代理人更加了解当前形态。 numerically results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football (GRF) and sparse Starcraft II multi-agent challenge (SMAC) tasks.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Inspired-Machine-Learning-a-Survey"><a href="#Quantum-Inspired-Machine-Learning-a-Survey" class="headerlink" title="Quantum-Inspired Machine Learning: a Survey"></a>Quantum-Inspired Machine Learning: a Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11269">http://arxiv.org/abs/2308.11269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Larry Huynh, Jin Hong, Ajmal Mian, Hajime Suzuki, Yanqiu Wu, Seyit Camtepe</li>
<li>for: This survey provides an integrated and comprehensive examination of Quantum-inspired Machine Learning (QiML), exploring its diverse research domains and recent advancements, as well as showcasing practical applications and potential future research avenues.</li>
<li>methods: The survey covers a range of methods in QiML, including tensor network simulations and dequantized algorithms.</li>
<li>results: The survey provides a comprehensive overview of the current landscape of QiML, including recent advancements and potential future directions for research in this field.Here is the information in Simplified Chinese text:</li>
<li>for: 这份报告提供了量子概念Machine Learning（QiML）的全面和综合性评估，探讨其多样化的研究领域，包括维度网络仿真和量子化算法等，以及展示实际应用和未来研究方向。</li>
<li>methods: 报告覆盖了QiML中的多种方法，包括维度网络仿真和量子化算法等。</li>
<li>results: 报告提供了量子概念Machine Learning（QiML）当前的景观，包括最新的进展和未来研究方向。<details>
<summary>Abstract</summary>
Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching the field further. This survey serves as a guide for researchers and practitioners alike, providing a holistic understanding of QiML's current landscape and future directions.
</details>
<details>
<summary>摘要</summary>
量子机器学习（QiML）是一个快速发展的领域，在全球的研究者中受到广泛关注，因为它可以利用量子力学的原理在类别计算框架中进行应用。然而，当前的文献综述往往停留在更广泛的量子机器学习（QML）领域上，而不是深入探讨QiML。为了填补这一空白，本调查提供了一个整合和完整的量子机器学习的调查，探讨了QiML的多个研究领域，包括张量网络 simulations、dequantized算法和其他，展示了最新的进展、实践应用和未来研究方向。此外，本文还提供了一个具体的QiML定义，通过分析各种先前解释的term和其内在的歧义来确定。随着QiML的进一步发展，我们预计将有更多的未来发展，借鉴量子力学、量子计算和类别机器学习，使QiML更加丰富。本调查作为研究者和实践者的指南，提供了量子机器学习当前领域的整体认识和未来方向。
</details></li>
</ul>
<hr>
<h2 id="Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes"><a href="#Robust-Lagrangian-and-Adversarial-Policy-Gradient-for-Robust-Constrained-Markov-Decision-Processes" class="headerlink" title="Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes"></a>Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11267">http://arxiv.org/abs/2308.11267</a></li>
<li>repo_url: None</li>
<li>paper_authors: David M. Bossens</li>
<li>for: 本 paper 目的是提出两种基于 RCMDP 的算法，以提高 reinforcement learning 中的 robustness 和可靠性。</li>
<li>methods: 本 paper 使用了两种方法：RCPG with Robust Lagrangian 和 Adversarial RCPG。RCPG with Robust Lagrangian 是修改了 traditional RCPG 的方法，通过使用 Lagrangian 来计算 worst-case dynamics。Adversarial RCPG 则是通过 directly 和 incrementally 学习 adversarial policy 来提高 robustness。</li>
<li>results: 实验结果表明，本 paper 提出的两种算法在 inventory management 和 safe navigation 任务中具有竞争性的性能，并且比 traditional RCPG  variants 和非 robust 的 ablation 更高。尤其是 Adversarial RCPG，在所有测试中排名第二。<details>
<summary>Abstract</summary>
The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy through gradient descent rather than indirectly and abruptly through constrained optimisation on a sorted value list. A theoretical analysis first derives the Lagrangian policy gradient for the policy optimisation of both proposed algorithms and then the adversarial policy gradient to learn the adversary for Adversarial RCPG. Empirical experiments injecting perturbations in inventory management and safe navigation tasks demonstrate the competitive performance of both algorithms compared to traditional RCPG variants as well as non-robust and non-constrained ablations. In particular, Adversarial RCPG ranks among the top two performing algorithms on all tests.
</details>
<details>
<summary>摘要</summary>
RCMDP（Robust Constrained Markov Decision Process）是一种最近的任务建模框架，用于学习奖励学习，它包含行为约束和对过程动态模型的不确定性集。在计算 RCMDP 的 worst-case 动态时，需要根据每个状态的值估计来进行计算，这种方法曾经在 Robust Constrained Policy Gradient (RCPG) 中使用。然而，RCPG 存在一些缺点，如不能对完整的约束目标进行强化，以及不具备逐步学习的能力。这篇论文提出了两种算法，即 RCPG with Robust Lagrangian 和 Adversarial RCPG。RCPG with Robust Lagrangian 修改了 RCPG，通过使用 Lagrangian 而不是值或约束来计算 worst-case 动态。Adversarial RCPG 则通过直接和逐步地使用各元的敌对策略来学习 Lagrangian，而不是通过受限优化来 indirectly 和突然地修改 value 列表。这篇论文首先 derive Lagrangian 政策偏移量，然后是对 Adversarial RCPG 的敌对策略学习。实验表明，两种算法在具有扰动的 inventory management 和 safe navigation 任务中表现竞争性。特别是，Adversarial RCPG 在所有测试中排名第二。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games"><a href="#Efficient-Last-iterate-Convergence-Algorithms-in-Solving-Games" class="headerlink" title="Efficient Last-iterate Convergence Algorithms in Solving Games"></a>Efficient Last-iterate Convergence Algorithms in Solving Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11256">http://arxiv.org/abs/2308.11256</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjian Meng, Zhenxing Ge, Wenbin Li, Bo An, Yang Gao</li>
<li>for: 学习二人 zero-sum 正规形游戏（NFG）和广泛形游戏（EFG）中的 Nash 平衡（NE）。</li>
<li>methods: 使用 last-iterate convergence no-regret 算法，包括 Optimistic Gradient Descent Ascent（OGDA）和 Optimistic Multiplicative Weight Update（OMWU）。</li>
<li>results: 提出了一种 Reward Transformation（RT）框架，可以减少 OGDA 的每次迭代复杂性，并在不假设 NE 唯一的情况下保证 converge。但是，RT 基于的算法在同样的迭代次数下表现较差，而且其 convergence  garantuee 基于 continuous-time feedback 假设，不符合实际场景。通过对 RT 框架进行更深入的分析，我们发现 RT 框架可以将学习 NE 问题转化为一系列 strongly convex-concave 优化问题（SCCPs）。我们还设计了一种新的转换方法，使得 SCCPs 可以通过 Regret Matching+（RM+）算法解决，并且使得算法在离散时间反馈设定下 converge。实验结果表明，我们的算法在 existing last-iterate convergence 算法和 RM+（CFR+）上表现出色。<details>
<summary>Abstract</summary>
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, which holds for both continuous and discrete-time feedback. We demonstrate that the essence of the RT framework is to transform the problem of learning NE in the original game into a series of strongly convex-concave optimization problems (SCCPs). We show that the bottleneck of RT-based algorithms is the speed of solving SCCPs. To improve the their empirical performance, we design a novel transformation method to enable the SCCPs can be solved by Regret Matching+ (RM+), a no-regret algorithm with better empirical performance, resulting in Reward Transformation RM+ (RTRM+). RTRM+ enjoys last-iterate convergence under the discrete-time feedback setting. Using the counterfactual regret decomposition framework, we propose Reward Transformation CFR+ (RTCFR+) to extend RTRM+ to EFGs. Experimental results show that our algorithms significantly outperform existing last-iterate convergence algorithms and RM+ (CFR+).
</details>
<details>
<summary>摘要</summary>
“调整算法”在二 player零差游戏（NFG）和延展游戏（EFG）中学习 Nash 均衡（NE）很受欢迎。多些最近的研究专注于最后迭代具有调整性的算法。其中最具名望的两个算法是Optimistic Gradient Descent Ascent（OGDA）和Optimistic Multiplicative Weight Update（OMWU）。但OGDA的每迭代复杂度高，OMWU具有较低的每迭代复杂度，但实际性较差，且它的均衡性只适用于当 NE 独特的情况。现有的工作提出了一个 Reward Transformation（RT）框架，用于MWU，可以将 NE 独特性的假设移除，并实现与 OMWU 的竞争性表现。但RT-based algorithms 在同一数量的迭代下表现较差，且其均衡保证基于紧急时间反馈假设，这不是大多数情况下的假设。为了解决这些问题，我们进行了更深入的 RT 框架分析，这个框架在紧急时间反馈下和离散时间反馈下都适用。我们证明了 RT 框架的核心是将原始游戏中学习 NE 的问题转换为一系列强弱点减游戏（SCCPs）。我们显示了 SCCPs 的瓶颈在 RT-based algorithms 中，以及如何使用 Regret Matching+（RM+）这个 no-regret 算法来解决这个问题。我们给出了一个新的变换方法，使得 SCCPs 可以使用 RM+ 来解决，从而获得 Reward Transformation RM+（RTRM+）。RTRM+ 在离散时间反馈下具有最后迭代均衡性。使用 counterfactual regret decomposition 框架，我们提出了 Reward Transformation CFR+（RTCFR+）来扩展 RTRM+ 到 EFGs。实验结果显示我们的算法在与现有的最后迭代均衡算法和 RM+（CFR+）进行比较时，具有更好的实际表现。”
</details></li>
</ul>
<hr>
<h2 id="A-survey-on-bias-in-machine-learning-research"><a href="#A-survey-on-bias-in-machine-learning-research" class="headerlink" title="A survey on bias in machine learning research"></a>A survey on bias in machine learning research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11254">http://arxiv.org/abs/2308.11254</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aastha2104/Parkinson-Disease-Prediction">https://github.com/Aastha2104/Parkinson-Disease-Prediction</a></li>
<li>paper_authors: Agnieszka Mikołajczyk-Bareła, Michał Grochowski</li>
<li>for: 本研究旨在强调机器学习中的偏见问题，而不仅仅是公平性。</li>
<li>methods: 本文提供了机器学习管道中偏见的多种可能来源和错误的数据和模型阶段的分类。</li>
<li>results: 本研究通过对四十多个机器学习管道中偏见的分析，提供了具体的示例，以便更好地理解和掌握机器学习中的偏见问题，并开发更公平、更透明、更准确的机器学习模型。<details>
<summary>Abstract</summary>
Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
</details>
<details>
<summary>摘要</summary>
当前的研究对机器学习中的偏见通常集中于公平性，而忽视了偏见的根源或原因。然而，偏见最初是定义为一种"系统性错误"，经常由人类在不同阶段的研究过程中引入。这篇文章试图 bridge 过去的Literature on bias在研究中的偏见，提供机器学习管道中可能的偏见和错误的分类。文章专注于机器学习管道中的偏见。survey分析了超过四十个机器学习管道中的偏见来源，并提供了明确的示例。通过理解机器学习中的偏见源头和后果，可以开发出更好的检测和缓解偏见方法，导致更公平、更透明和更准确的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes"><a href="#Multi-Source-Domain-Adaptation-for-Cross-Domain-Fault-Diagnosis-of-Chemical-Processes" class="headerlink" title="Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes"></a>Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11247">http://arxiv.org/abs/2308.11247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Fernandes Montesuma, Michela Mulas, Fred Ngolè Mboula, Francesco Corona, Antoine Souloumiac</li>
<li>for: 本研究旨在对过程监测中的故障诊断进行自动化，并使用机器学习来预测故障类型基于传感器读ings。</li>
<li>methods: 本研究使用单源隐式预测（SSDA）和多源隐式预测（MSDA）算法进行过程监测中的故障诊断，并在田东曼过程中进行了广泛的比较。</li>
<li>results: 研究结果表明，在多个源数据中训练时，使用多源隐式预测算法可以提高故障类型的预测精度，相比单源隐式预测算法，多源隐式预测算法在无适应情况下提高了平均23%的分类精度。此外，在多源数据中训练时，无适应情况下，多源隐式预测算法可以提高故障类型的预测精度8.4%的平均提升。<details>
<summary>Abstract</summary>
Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classification accuracy by 23% on average. In addition, under the multiple-sources scenario, we improve classification accuracy of the no adaptation setting by 8.4% on average.
</details>
<details>
<summary>摘要</summary>
检测 fault 是 proces supervision 中的一个重要组件。它可以确定已经检测到的 fault 类型，并且允许适当的 intervención。自动 fault 检测系统 使用机器学习来预测 fault 类型从 sensor 读数中。然而，这些模型对数据分布的变化敏感，这可能是由监测过程中的变化引起的，例如操作模式的变化。这种情况被称为 Cross-Domain Fault Diagnosis (CDFD)。我们提供了单和多源无监督领域适应 (SSDA 和 MSDA 分别) 算法的广泛比较，用于 CDFD。我们在 Tennessee-Eastmann 过程中进行研究，这是化学工业中 widely 使用的标准测试集。我们发现，在训练时使用多个领域可以提高分类精度，即使没有适应。因此，MSDA 基线比 SSDA 基线分类精度高出 23% 的平均值。此外，在多源enario 下，我们可以提高无适应设置的分类精度的平均值8.4%。
</details></li>
</ul>
<hr>
<h2 id="An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification"><a href="#An-Effective-Transformer-based-Contextual-Model-and-Temporal-Gate-Pooling-for-Speaker-Identification" class="headerlink" title="An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification"></a>An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11241">http://arxiv.org/abs/2308.11241</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harunorikawano/speaker-identification-with-tgp">https://github.com/harunorikawano/speaker-identification-with-tgp</a></li>
<li>paper_authors: Harunori Kawano, Sota Shimizu</li>
<li>for: 这 paper 的目的是提出一种基于 Transformer 架构和自我超vised 学习的高效 speaker identification 模型。</li>
<li>methods: 该 paper 使用了 Transformer-based contextual model，并进行了参数与性能之间的关系研究，以探索一个有效的模型结构。此外， paper 还提出了 Temporal Gate Pooling（TGP）方法，可以增强 speaker identification 的学习能力。</li>
<li>results: 通过使用 Conformer 编码器和 BEST-RQ 预训练， authors 实现了一个准确率为 85.9%，参数量为 28.5M 的 speaker identification 模型，与 wav2vec2 的 317.7M 参数量相当。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/HarunoriKawano/speaker-identification-with-tgp">https://github.com/HarunoriKawano/speaker-identification-with-tgp</a> 上获取。<details>
<summary>Abstract</summary>
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
</details>
<details>
<summary>摘要</summary>
噪声2vec2在应用transformer架构和自动学习方法上取得了成功，现在不仅用于语音识别，还用于整个语音处理。这篇论文介绍了一种高效的端到端Speaker Identification模型，该模型基于Transformer-based contextual模型。我们研究了参数和性能之间的关系，以便理解有效模型的结构。此外，我们提出了一种强大学习能力的pooling方法：Temporal Gate Pooling。我们使用Conformer作Encoder，并在BEST-RQ上进行预训练，对VoxCeleb1中的Speaker Identification进行评估。我们的方法实现了85.9%的准确率， Parameters为28.5M，与wav2vec2的317.7M Parameters相比，表现相对较高。代码可以在https://github.com/HarunoriKawano/speaker-identification-with-tgp中找到。
</details></li>
</ul>
<hr>
<h2 id="Minwise-Independent-Permutations-with-Insertion-and-Deletion-of-Features"><a href="#Minwise-Independent-Permutations-with-Insertion-and-Deletion-of-Features" class="headerlink" title="Minwise-Independent Permutations with Insertion and Deletion of Features"></a>Minwise-Independent Permutations with Insertion and Deletion of Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11240">http://arxiv.org/abs/2308.11240</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rameshwar Pratap, Raghav Kulkarni</li>
<li>for: 本研究的目的是研究在动态插入和删除特征的情况下，$\mathrm{minHash}$ 算法是否可以提供高效和精准的笛卡尔相似性计算。</li>
<li>methods: 本研究使用了动态插入和删除特征的 $\mathrm{minHash}$ 算法，并提出了一种基于随机排序的算法来使$\mathrm{minHash}$ 笛卡尔相似性计算更加高效和精准。</li>
<li>results: 本研究通过了一系列的实验和理论分析，证明了该算法在动态插入和删除特征的情况下可以具有显著的时间效益和相似性表现。<details>
<summary>Abstract</summary>
In their seminal work, Broder \textit{et. al.}~\citep{BroderCFM98} introduces the $\mathrm{minHash}$ algorithm that computes a low-dimensional sketch of high-dimensional binary data that closely approximates pairwise Jaccard similarity. Since its invention, $\mathrm{minHash}$ has been commonly used by practitioners in various big data applications. Further, the data is dynamic in many real-life scenarios, and their feature sets evolve over time. We consider the case when features are dynamically inserted and deleted in the dataset. We note that a naive solution to this problem is to repeatedly recompute $\mathrm{minHash}$ with respect to the updated dimension. However, this is an expensive task as it requires generating fresh random permutations. To the best of our knowledge, no systematic study of $\mathrm{minHash}$ is recorded in the context of dynamic insertion and deletion of features. In this work, we initiate this study and suggest algorithms that make the $\mathrm{minHash}$ sketches adaptable to the dynamic insertion and deletion of features. We show a rigorous theoretical analysis of our algorithms and complement it with extensive experiments on several real-world datasets. Empirically we observe a significant speed-up in the running time while simultaneously offering comparable performance with respect to running $\mathrm{minHash}$ from scratch. Our proposal is efficient, accurate, and easy to implement in practice.
</details>
<details>
<summary>摘要</summary>
它们的著名论文《CFM98》中，布罗德等人（Broder et al.)提出了一种低维度的$\mathrm{minHash}$算法，用于计算高维度二分类数据的紧密相似性。自其发明以来，$\mathrm{minHash}$在各种大数据应用中广泛使用。然而，在许多实际场景中，数据的特征集合会随着时间的推移而变化。我们考虑在数据集中动态插入和删除特征的情况。一个简单的解决方案是在更新后重新计算$\mathrm{minHash}$。然而，这是一项昂贵的任务，需要生成新的随机排序。到目前为止，我们未发现任何关于动态插入和删除特征的$\mathrm{minHash}$系统性研究。在这个研究中，我们开始了这个研究，并提出了一些使$\mathrm{minHash}$签名适应动态插入和删除特征的算法。我们进行了严格的理论分析，并通过多个实际数据集的实验观察，证明了我们的方法可以快速、精准地计算$\mathrm{minHash}$签名，同时保持与从scratch计算$\mathrm{minHash}$的性能相似。我们的提议是高效、准确、易于实现的。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment"><a href="#Federated-Learning-on-Patient-Data-for-Privacy-Protecting-Polycystic-Ovary-Syndrome-Treatment" class="headerlink" title="Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment"></a>Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11220">http://arxiv.org/abs/2308.11220</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/toriqiu/fl-pcos">https://github.com/toriqiu/fl-pcos</a></li>
<li>paper_authors: Lucia Morris, Tori Qiu, Nikhil Raghuraman</li>
<li>for: 这个研究旨在应用 Federated Learning (FL) 技术来预测女性患有多囊淋巴综合症 (PCOS) 的最佳药物。</li>
<li>methods: 本研究使用了多种 Federated Learning 方法，包括 Federated Averaging (FedAvg)、Proximal Federated Learning (PFL) 和 Federated Transfer Learning (FTL)，并证明这些方法在人工生成的 PCOS 患者数据集上得到了成功。</li>
<li>results: 研究发现，这些 Federated Learning 方法可以在 accessed massive quantities of diverse data 中提供最佳的治疗选择，并为 PCOS 患者提供隐私保证。<details>
<summary>Abstract</summary>
The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
</details>
<details>
<summary>摘要</summary>
女性内分泌学领域落后于数据驱动医疗解决方案，主要是因为担心病人数据隐私问题。病人患有混合症或终止怀孕的情况可能会透露出有价值的内分泌水平或月经周期数据，违反病人隐私。我们研究了 Federated Learning（FL）的应用，以预测患有多囊卵巢综合症（PCOS）的女性患者需要哪种药物。PCOS 是全球数百万女性患有的严重内分泌疾病，但它的研究受到缺乏病人数据的限制。我们展示了多种 FL 方法在 sintetic PCOS 患者数据集上取得成功。我们提议的 FL 模型可以访问庞大的多样数据，并在保证病人隐私的情况下，为患有 PCOS 的女性患者预测最有效的治疗方案。
</details></li>
</ul>
<hr>
<h2 id="Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models"><a href="#Federated-Learning-in-Big-Model-Era-Domain-Specific-Multimodal-Large-Models" class="headerlink" title="Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models"></a>Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11217">http://arxiv.org/abs/2308.11217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zengxiang Li, Zhaoxiang Hou, Hui Liu, Ying Wang, Tongzhi Li, Longfei Xie, Chao Shi, Chengyi Yang, Weishan Zhang, Zelei Liu, Liang Xu</li>
<li>for: 提出一种多模态联合学习框架，以帮助多家企业通过私有领域数据来共同训练大型模型，实现各种场景下的智能服务。</li>
<li>methods: 提出了多模态联合学习框架，包括智能基础和目标的战略转变，以及新的多样数据、模型聚合、性能和成本负担、数据隐私和奖励机制等挑战。</li>
<li>results: 通过实施多模态联合学习框架，企业可以增强和积累智能能力，共同创造智能城市模型，提供高质量智能服务，包括能源基础设施安全、住宅社区安全和城市运营管理。<details>
<summary>Abstract</summary>
Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient coordination of the federated learning platform, technical innovations on data quality improvement based on large model capabilities and efficient joint fine-tuning approaches. Preliminary experiments show that enterprises can enhance and accumulate intelligent capabilities through multimodal model federated learning, thereby jointly creating an smart city model that provides high-quality intelligent services covering energy infrastructure safety, residential community security, and urban operation management. The established federated learning cooperation ecosystem is expected to further aggregate industry, academia, and research resources, realize large models in multiple vertical domains, and promote the large-scale industrial application of artificial intelligence and cutting-edge research on multimodal federated learning.
</details>
<details>
<summary>摘要</summary>
多modal数据，可以全面感受和认可物理世界，已成为通往通用人工智能的重要路径。然而，多modal大型模型在公共数据集上训练时经常下perform。这篇论文提出了一个多modal联合学习框架，允许多家企业使用私有领域数据共同训练大型模型，实现多场景智能服务。作者们详细讲解联合学习在智能基础和目标方面的战略转型，以及在不同数据和模型聚合、性能和成本权衡、数据隐私和奖励机制方面新出现的挑战。论文还介绍了一个城市安全运营管理案例研究，包括分布式部署和有效协调联合学习平台，以及基于大型模型能力的数据质量改进技术和有效联合精度调整方法。初步实验表明，企业可以通过多modal模型联合学习增强和积累智能能力，共同创建一个智能城市模型，提供高质量智能服务，涵盖能源基础设施安全、居民社区安全和城市运营管理。建立的联合学习合作生态系统预计会进一步吸引产业、学术和研究资源，实现多个垂直领域的大型模型，并推动人工智能的大规模工业应用和多modal联合学习的前沿研究。
</details></li>
</ul>
<hr>
<h2 id="Hamiltonian-GAN"><a href="#Hamiltonian-GAN" class="headerlink" title="Hamiltonian GAN"></a>Hamiltonian GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11216">http://arxiv.org/abs/2308.11216</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/koritsky/hamiltonian_learning">https://github.com/koritsky/hamiltonian_learning</a></li>
<li>paper_authors: Christine Allen-Blanchette</li>
<li>for: 这个论文旨在使用 Hamiltonian  formalism 作为一种 физи学上有效的视频生成方法的 inductive bias。</li>
<li>methods: 该方法使用了一种叫做 Hamiltonian neural network 的 Motion Model，通过学习配置空间地图和 Hamiltonian 来学习一个可以表示配置空间的表示。</li>
<li>results: 该方法在使用 physics-inspired cyclic-coordinate loss function 进行训练后，能够在 Hamiltonian Dynamics Suite Toy Physics 数据集上实现更高的效果和可 interpretability。<details>
<summary>Abstract</summary>
A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the efficacy and advantages of our approach on the Hamiltonian Dynamics Suite Toy Physics dataset.
</details>
<details>
<summary>摘要</summary>
“一个增长中的研究利用哈密顿ormalism作为神经网络基于视频生成的假设逻辑。哈密顿结构保证学习的量（例如能量）的保守和要求输入视频的低维度抽象空间具有相位空间的解释，这有助于在下游任务中集成学习的表示。然而现有的方法有限于其应用，因为它们在设计时需要一个结构的假设 для配置空间。在这项工作中，我们提出了一个基于GAN的视频生成管线，使用学习的配置空间地图和哈密顿神经网络动力学模型来学习配置空间的表示。我们使用一种受物理灵感的循环坐标损失函数来驱动学习，这种损失函数鼓励最小化配置空间的表示，并提高可读性。我们在哈密顿动力学集 Toy Physics 数据集上证明了我们的方法的有效性和优势。”Note: Simplified Chinese is used in this translation, which is a more casual and widely-used version of Chinese. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="A-Simple-Framework-for-Multi-mode-Spatial-Temporal-Data-Modeling"><a href="#A-Simple-Framework-for-Multi-mode-Spatial-Temporal-Data-Modeling" class="headerlink" title="A Simple Framework for Multi-mode Spatial-Temporal Data Modeling"></a>A Simple Framework for Multi-mode Spatial-Temporal Data Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11204">http://arxiv.org/abs/2308.11204</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lzhmarkk/simmst">https://github.com/lzhmarkk/simmst</a></li>
<li>paper_authors: Zihang Liu, Le Yu, Tongyu Zhu, Leiei Sun</li>
<li>for: 本文旨在提出一种简单的多模式空间时间数据模型方法，以提高效率和效果的权衡。</li>
<li>methods: 本文提出了一种通用的对角模式空间关系学习组件，可以适应多种模式之间的连接和信息传递。此外，文章还使用多层感知器来捕捉时间相关性和通道相关性，技术上和概念上都非常简洁。</li>
<li>results: 实验结果表明，我们的模型可以在三个实际数据集上经常超越基线，并且具有较低的空间和时间复杂度。此外，通用的对角模式空间关系学习模块的一致性也得到了验证。<details>
<summary>Abstract</summary>
Spatial-temporal data modeling aims to mine the underlying spatial relationships and temporal dependencies of objects in a system. However, most existing methods focus on the modeling of spatial-temporal data in a single mode, lacking the understanding of multiple modes. Though very few methods have been presented to learn the multi-mode relationships recently, they are built on complicated components with higher model complexities. In this paper, we propose a simple framework for multi-mode spatial-temporal data modeling to bring both effectiveness and efficiency together. Specifically, we design a general cross-mode spatial relationships learning component to adaptively establish connections between multiple modes and propagate information along the learned connections. Moreover, we employ multi-layer perceptrons to capture the temporal dependencies and channel correlations, which are conceptually and technically succinct. Experiments on three real-world datasets show that our model can consistently outperform the baselines with lower space and time complexity, opening up a promising direction for modeling spatial-temporal data. The generalizability of the cross-mode spatial relationships learning module is also validated.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SegRNN-Segment-Recurrent-Neural-Network-for-Long-Term-Time-Series-Forecasting"><a href="#SegRNN-Segment-Recurrent-Neural-Network-for-Long-Term-Time-Series-Forecasting" class="headerlink" title="SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting"></a>SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11200">http://arxiv.org/abs/2308.11200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, Haotong Zhang</li>
<li>for: 这 paper 是为了解决 RNN 在长期时间序列预测（LTSF）领域中遇到的挑战，特别是面临非常长的寻回窗口和预测范围。</li>
<li>methods: 这 paper 提出了两种新的策略来减少 RNN 在 LTSF 任务中的迭代次数：Segment-wise Iterations 和 Parallel Multi-step Forecasting（PMF）。这两种策略可以减少 RNN 的迭代次数，从而提高预测精度和执行速度。</li>
<li>results: 对比 SOTA Transformer-based 模型，SegRNN 能够显著提高预测精度和执行速度，同时减少了 runtime 和内存使用量，减少了超过 78%。这些成果证明 RNN 仍然在 LTSF 任务中具有优势，并促使更多的 RNN-based 方法在这个领域进行进一步的探索。<details>
<summary>Abstract</summary>
RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF tasks and encourage further exploration of this domain with more RNN-based approaches. The source code is coming soon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs"><a href="#ConcatPlexer-Additional-Dim1-Batching-for-Faster-ViTs" class="headerlink" title="ConcatPlexer: Additional Dim1 Batching for Faster ViTs"></a>ConcatPlexer: Additional Dim1 Batching for Faster ViTs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11199">http://arxiv.org/abs/2308.11199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Donghoon Han, Seunghyeon Seo, Donghyeon Jeon, Jiho Jang, Chaerin Kong, Nojun Kwak</li>
<li>for: 降低计算成本，提高图像识别效率</li>
<li>methods: 使用额外维度批处理（ concatenation）和改进的图像多路径（Image Multiplexer），提高通过put和精度之间的平衡</li>
<li>results: 在ImageNet1K和CIFAR100 dataset上训练ConcatPlexer，相比ViT-B&#x2F;16，减少了23.5%的GFLOPs，并在验证集上达到69.5%和83.4%的验证精度I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and it achieved 23.5% less GFLOPs than ViT-B/16 with 69.5% and 83.4% validation accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
启发于语言模型的成本减少方法，我们提出了一种新的方法 для快速识别图像，即 concatenation 方法。我们首先介绍了图像多元化（Image Multiplexer），然后开发了新的组件以解决其缺陷，最终得到了 ConcatPlexer 模型。ConcatPlexer 模型在 ImageNet1K 和 CIFAR100 数据集上训练，并实现了与 ViT-B/16 相比的 23.5%  menos GFLOPs，同时保持了69.5% 和 83.4% 的验证精度。
</details></li>
</ul>
<hr>
<h2 id="Toward-Generalizable-Machine-Learning-Models-in-Speech-Language-and-Hearing-Sciences-Power-Analysis-and-Sample-Size-Estimation"><a href="#Toward-Generalizable-Machine-Learning-Models-in-Speech-Language-and-Hearing-Sciences-Power-Analysis-and-Sample-Size-Estimation" class="headerlink" title="Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation"></a>Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11197">http://arxiv.org/abs/2308.11197</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamzeh Ghasemzadeh, Robert E. Hillman, Daryush D. Mehta</li>
<li>for: 提供量化证据，鼓励研究人员使用更加稳健的嵌套十字验证法。</li>
<li>methods: 使用 Monte Carlo 仿真和 Matlab 代码进行 ML 分析的力度分析，比较不同的十字验证方法（单个割裂、10-fold、训练-验证-测试和嵌套10-fold）的统计能力和统计信任度。</li>
<li>results: 结果显示，使用嵌套十字验证法可以获得最高的统计信任度和统计能力，并提供了不偏的准确率估计。相比之下，使用单个割裂法可能会过估准确率，需要50%更多的样本来达到同等的统计能力。<details>
<summary>Abstract</summary>
This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being selected and hence being included in the final model. Our analysis showed that the model generated based on the single holdout method had very low statistical power and statistical confidence and that it significantly overestimated the accuracy. Conversely, the nested 10-fold cross-validation resulted in the highest statistical confidence and the highest statistical power, while providing an unbiased estimate of the accuracy. The required sample size with a single holdout could be 50% higher than what would be needed if nested cross-validation were used. Confidence in the model based on nested cross-validation was as much as four times higher than the confidence in the single holdout-based model. A computational model, MATLAB codes, and lookup tables are provided to assist researchers with estimating the sample size during the design of their future studies.
</details>
<details>
<summary>摘要</summary>
In simplified Chinese, the text might be translated as:这个研究的第一个目的是提供量化证据，以便激励研究人员使用更加稳定的嵌套十分之分验方法。第二个目的是向研究人员提供方法和MATLAB代码，以便在设计研究时进行能量分析。 Monte Carlo仿真被用来评估各种十分之分验方法的交互作用，包括单个停留、10个折衣、训练验证测试、嵌套10个折衣。研究发现，使用嵌套10个折衣方法可以获得最高的统计信任度和统计能力，而且可以提供不偏的准确率估计。单个停留方法的模型具有很低的统计信任度和统计能力，并且很大程度上过估了准确率。相比之下，使用嵌套10个折衣方法可以避免这些问题。这个研究提供了一个计算模型、MATLAB代码和查找表，以帮助研究人员在设计未来研究时估计样本大小。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Task-Parallelization-of-Dataflow-Graphs-in-ML-DL-models"><a href="#Automatic-Task-Parallelization-of-Dataflow-Graphs-in-ML-DL-models" class="headerlink" title="Automatic Task Parallelization of Dataflow Graphs in ML&#x2F;DL models"></a>Automatic Task Parallelization of Dataflow Graphs in ML&#x2F;DL models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11192">http://arxiv.org/abs/2308.11192</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srinjoy Das, Lawrence Rauchwerger</li>
<li>for: 加速 Machine Learning(ML) 和 Deep-Learning(DL) 模型的训练和执行，以提高性能和效率。</li>
<li>methods: 使用 Critical-Path-based Linear Clustering 方法，利用 ML 数据流图中的自然平行路径进行并行处理，并通过clone和常量传递来优化图结构。此外，我们还使用一个新建的工具 called {\bf Ramiel}，将 ML 模型转换成可读写并可执行的 Pytorch+Python 代码，以便利用其他下游加速技术。</li>
<li>results: 我们的实验结果表明，使用我们的方法可以在多个 ML 图中获得至多 1.9 倍的速度提升，并在 compile 和 runtime 中超越一些当前的机制。此外，我们的方法具有轻量级和快速的特点，适用于资源和电源受限的设备，同时仍能启用下游优化。<details>
<summary>Abstract</summary>
Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\bf Ramiel}. This allows us to benefit from other downstream acceleration techniques like intra-op parallelism and potentially pipeline parallelism. Our preliminary results on several ML graphs demonstrate up to 1.9$\times$ speedup over serial execution and outperform some of the current mechanisms in both compile and runtimes. Lastly, our methods are lightweight and fast enough so that they can be used effectively for power and resource-constrained devices, while still enabling downstream optimizations.
</details>
<details>
<summary>摘要</summary>
(Note: The text has been translated into Simplified Chinese, but please note that the translation may not be perfect and may require some adjustments to accurately convey the intended meaning.)
</details></li>
</ul>
<hr>
<h2 id="Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries"><a href="#Diversity-Measures-Domain-Independent-Proxies-for-Failure-in-Language-Model-Queries" class="headerlink" title="Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries"></a>Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11189">http://arxiv.org/abs/2308.11189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lab-v2/diversity_measures">https://github.com/lab-v2/diversity_measures</a></li>
<li>paper_authors: Noel Ngu, Nathaniel Lee, Paulo Shakarian</li>
<li>for: 这篇论文是针对大型语言模型中的错误预测进行研究，特别是基于领域专门知识。</li>
<li>methods: 本文提出了基于对答几何的多元性来衡量错误的三种方法：基于熵、基于金尼逊级别和基于中心距离。</li>
<li>results: 本文通过多个数据集和温度设定的实验显示，这三种方法强相关于错误的可能性。此外，本文还提供了实践结果，说明了这些方法可以应用于几何问题、链式思维和错误检测。<details>
<summary>Abstract</summary>
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
</details>
<details>
<summary>摘要</summary>
大型语言模型中的错误预测通常需要对特定领域的信息。在这篇文章中，我们提出了基于响应问题的多样性的评估方法，以独立于下游应用。我们详细描述了三种评估方法，包括基于熵、基于吉尼不纯度和中心距离。我们在多个数据集和温度设定下进行了一系列实验，以示这些评估方法与错误的机会强相关。此外，我们还提供了实践结果，详细介绍了如何将这些评估方法应用到几个问题、推理和错误检测。
</details></li>
</ul>
<hr>
<h2 id="A-three-in-one-bottom-up-framework-for-simultaneous-semantic-segmentation-instance-segmentation-and-classification-of-multi-organ-nuclei-in-digital-cancer-histology"><a href="#A-three-in-one-bottom-up-framework-for-simultaneous-semantic-segmentation-instance-segmentation-and-classification-of-multi-organ-nuclei-in-digital-cancer-histology" class="headerlink" title="A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology"></a>A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11179">http://arxiv.org/abs/2308.11179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ibtihaj Ahmad, Syed Muhammad Israr, Zain Ul Islam</li>
<li>for: 这篇论文主要关注于电子显微镜中的细胞构成物分类和分 segmentation的问题，以协助电脑助癌诊断。</li>
<li>methods: 这篇论文使用了多个增强型深度学习模型，包括多个核心decoder head和独立的复杂损失函数，以解决电子显微镜中的挑战。</li>
<li>results: 这篇论文获得了高品质的分类和分 segmentation结果，包括0.841 dice score、0.713 bPQ score和0.633 mPQ score等。此外，这个框架可以在19种不同的组织中进行通用化。<details>
<summary>Abstract</summary>
Simultaneous segmentation and classification of nuclei in digital histology play an essential role in computer-assisted cancer diagnosis; however, it remains challenging. The highest achieved binary and multi-class Panoptic Quality (PQ) remains as low as 0.68 bPQ and 0.49 mPQ, respectively. It is due to the higher staining variability, variability across the tissue, rough clinical conditions, overlapping nuclei, and nuclear class imbalance. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. In our previous work, DAN-NucNet, we resolved these issues for semantic segmentation with an end-to-end model. This work extends our previous model to simultaneous instance segmentation and classification. We introduce additional decoder heads with independent weighted losses, which produce semantic segmentation, edge proposals, and classification maps. We use the outputs from the three-head model to apply post-processing to produce the final segmentation and classification. Our multi-stage approach utilizes edge proposals and semantic segmentations compared to direct segmentation and classification strategies followed by most state-of-the-art methods. Due to this, we demonstrate a significant performance improvement in producing high-quality instance segmentation and nuclei classification. We have achieved a 0.841 Dice score for semantic segmentation, 0.713 bPQ scores for instance segmentation, and 0.633 mPQ for nuclei classification. Our proposed framework is generalized across 19 types of tissues. Furthermore, the framework is less complex compared to the state-of-the-art.
</details>
<details>
<summary>摘要</summary>
计算机助enciaría的数字 histology 中的同时分割和分类问题具有重要的作用，但是它还是一个挑战。最高的二进制和多类 Panoptic Quality (PQ) 仍然为 0.68 bPQ 和 0.49 mPQ，分别。这是因为数字 histology 中的染色变化、组织内部的变化、临床条件不稳定、核体重叠和核类异质性等问题。通常的深度学习方法通常采用端到端模型，而这些模型无法直接解决数字 histology 中的这些问题。在我们之前的工作中，我们已经解决了这些问题，并在 DAN-NucNet 中提出了一种终端模型。这个工作是基于这个模型，并增加了一些额外的解码头，以生成 semantic segmentation、edge proposal 和分类图像。我们使用这些头的输出来进行后处理，以生成最终的分割和分类结果。我们的多阶段方法比较多样化，比如直接分割和分类策略，并且我们展示了一个显著的性能提高。我们在 19 种组织中实现了 0.841 Dice 分割率、0.713 bPQ 分割率和 0.633 mPQ 分类率。我们的提案的框架通过对 19 种组织进行扩展，并且比 state-of-the-art 更加简单。
</details></li>
</ul>
<hr>
<h2 id="A-Preliminary-Investigation-into-Search-and-Matching-for-Tumour-Discrimination-in-WHO-Breast-Taxonomy-Using-Deep-Networks"><a href="#A-Preliminary-Investigation-into-Search-and-Matching-for-Tumour-Discrimination-in-WHO-Breast-Taxonomy-Using-Deep-Networks" class="headerlink" title="A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks"></a>A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11162">http://arxiv.org/abs/2308.11162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abubakr Shafique, Ricardo Gonzalez, Liron Pantanowitz, Puay Hoon Tan, Alberto Machado, Ian A Cree, Hamid R. Tizhoosh</li>
<li>for: 这种研究旨在使用深度学习技术，对世界卫生组织乳腺癌分类（TCGA数据集）进行分类和检索。</li>
<li>methods: 研究人员使用了一种state-of-the-art的深度学习模型，通过对数百万个诊断 histopathology 图像进行预训练，提取了深度特征来visualize所有 sorts of tumor。</li>
<li>results: 研究人员发现，使用深度学习模型可以准确地检索和匹配罕见的乳腺癌种类，并且可以 investigating complex relationships among common and rare breast lesions。patch similarity search 的准确率达到了88%以上，并且使用 top-n tumor types 的验证率高达91%。<details>
<summary>Abstract</summary>
Breast cancer is one of the most common cancers affecting women worldwide. They include a group of malignant neoplasms with a variety of biological, clinical, and histopathological characteristics. There are more than 35 different histological forms of breast lesions that can be classified and diagnosed histologically according to cell morphology, growth, and architecture patterns. Recently, deep learning, in the field of artificial intelligence, has drawn a lot of attention for the computerized representation of medical images. Searchable digital atlases can provide pathologists with patch matching tools allowing them to search among evidently diagnosed and treated archival cases, a technology that may be regarded as computational second opinion. In this study, we indexed and analyzed the WHO breast taxonomy (Classification of Tumours 5th Ed.) spanning 35 tumour types. We visualized all tumour types using deep features extracted from a state-of-the-art deep learning model, pre-trained on millions of diagnostic histopathology images from the TCGA repository. Furthermore, we test the concept of a digital "atlas" as a reference for search and matching with rare test cases. The patch similarity search within the WHO breast taxonomy data reached over 88% accuracy when validating through "majority vote" and more than 91% accuracy when validating using top-n tumour types. These results show for the first time that complex relationships among common and rare breast lesions can be investigated using an indexed digital archive.
</details>
<details>
<summary>摘要</summary>
乳癌是全球女性中最常见的一种恶性肿瘤，其包括多种生物学、临床和 Histopathological 特征。存在超过35种乳腺病变型，可以根据细胞 morfology、生长和建筑模式进行分类和诊断。最近，人工智能领域的深度学习技术在医学图像计算方面吸引了很多关注，并在计算机化第二次诊断方面发挥了重要作用。在本研究中，我们对WHO乳腺分类（第五版）进行了索引和分析，覆盖了35种肿瘤类型。使用深度学习模型，我们将所有肿瘤类型视觉化，并使用从TCGA数据库中获得的数百万个诊断 histopathology 图像进行预处理。此外，我们测试了一种数字“图鉴”的概念，作为搜索和匹配罕见检测例的参考。在WHO乳腺分类数据集中进行质心 Similarity 搜索可达88%的准确率，并在“主要投票”验证方法下达到91%的准确率。这些结果表明，使用索引数字档案可以Investigate  Complex 乳腺病变之间的关系。
</details></li>
</ul>
<hr>
<h2 id="xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium"><a href="#xxMD-Benchmarking-Neural-Force-Fields-Using-Extended-Dynamics-beyond-Equilibrium" class="headerlink" title="xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium"></a>xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11155">http://arxiv.org/abs/2308.11155</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zpengmei/xxmd">https://github.com/zpengmei/xxmd</a></li>
<li>paper_authors: Zihan Pengmei, Junyu Liu, Yinan Shu</li>
<li>for: 这个论文旨在探讨神经力场（NFFs）在计算化学中作为参数模型，取代量子化学计算在初始态分子动力学中的地位。</li>
<li>methods: 该论文使用了非对易动力学方法，以获得更加精准的分子动力学数据。</li>
<li>results: 论文发现了MD17数据集的约束分布不足，导致其不适用于描述化学反应中的分子变形。该论文还引入了xxMD数据集，该数据集包括基于多参考波函数理论和density functional theory的能量和力学数据，并且更加 authentically depicts chemical reactions。<details>
<summary>Abstract</summary>
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional theory. Furthermore, its nuclear configuration spaces authentically depict chemical reactions, making xxMD a more chemically relevant dataset. Our re-assessment of equivariant models on the xxMD datasets reveals notably higher mean absolute errors than those reported for MD17 and its variants. This observation underscores the challenges faced in crafting a generalizable NFF model with extrapolation capability. Our proposed xxMD-CASSCF and xxMD-DFT datasets are available at \url{https://github.com/zpengmei/xxMD}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Mobility-Aware-Computation-Offloading-for-Swarm-Robotics-using-Deep-Reinforcement-Learning"><a href="#Mobility-Aware-Computation-Offloading-for-Swarm-Robotics-using-Deep-Reinforcement-Learning" class="headerlink" title="Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning"></a>Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11154">http://arxiv.org/abs/2308.11154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiucheng Wang, Hongzhi Guo</li>
<li>for:  automatize dirty, dangerous, and dull tasks with limited robot resources</li>
<li>methods:  mobile edge computing, mobility-aware deep reinforcement learning model for computing scheduling and resource allocation</li>
<li>results:  meet delay requirements, guarantee computation precision with minimum robot energy<details>
<summary>Abstract</summary>
Swarm robotics is envisioned to automate a large number of dirty, dangerous, and dull tasks. Robots have limited energy, computation capability, and communication resources. Therefore, current swarm robotics have a small number of robots, which can only provide limited spatio-temporal information. In this paper, we propose to leverage the mobile edge computing to alleviate the computation burden. We develop an effective solution based on a mobility-aware deep reinforcement learning model at the edge server side for computing scheduling and resource. Our results show that the proposed approach can meet delay requirements and guarantee computation precision by using minimum robot energy.
</details>
<details>
<summary>摘要</summary>
《群体机器人自动化技术》是旨在自动执行大量污 dirty、危险、无聊任务的。机器人具有有限的能量、计算能力和通信资源，因此当前的群体机器人只有少量机器人，可以提供有限的空间时间信息。在本文中，我们提议利用移动边缘计算来减轻计算负担。我们开发了基于移动性感知深度学习模型的Edge服务器端解决方案，以实现计算调度和资源分配。我们的结果显示，我们的方法可以遵循延迟要求，保证计算精度，并使用最小的机器人能量。
</details></li>
</ul>
<hr>
<h2 id="Energy-Efficient-On-Board-Radio-Resource-Management-for-Satellite-Communications-via-Neuromorphic-Computing"><a href="#Energy-Efficient-On-Board-Radio-Resource-Management-for-Satellite-Communications-via-Neuromorphic-Computing" class="headerlink" title="Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing"></a>Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11152">http://arxiv.org/abs/2308.11152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Flor Ortiz, Nicolas Skatchkovsky, Eva Lagunas, Wallace A. Martins, Geoffrey Eappen, Saed Daoud, Osvaldo Simeone, Bipin Rajendran, Symeon Chatzinotas</li>
<li>for: 本研究旨在应用能效的脑意传播模型来进行卫星通信（SatCom）系统中的无线资源管理。</li>
<li>methods: 本研究使用了复视预测（CNN）和脊搏神经网络（SNN）等机器学习（ML）技术，并进行了软件模拟和实验实践。</li>
<li>results: 比较 Convention CNN 和 SNN 的实验结果显示，在相关的工作负载下，SNN 在 Loihi 2 芯片上实现了更高的准确性，同时降低了电力消耗量超过 100 倍。这些结果显示了脑意 computing 和 SNN 在未来 SatCom 系统中的潜在潜力，并点出了这些技术在支持无线资源管理方面的应用前景。<details>
<summary>Abstract</summary>
The latest satellite communication (SatCom) missions are characterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loihi 2 yield higher accuracy, while reducing power consumption by more than 100$\times$ as compared to the CNN-based reference platform. Our findings point to the significant potential of neuromorphic computing and SNNs in supporting on-board SatCom operations, paving the way for enhanced efficiency and sustainability in future SatCom systems.
</details>
<details>
<summary>摘要</summary>
最新的卫星通信（SatCom）任务 caracterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning（ML）based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks（CNN）on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks（SNNs）implemented on Loihi 2 yield higher accuracy, while reducing power consumption by more than 100 times as compared to the CNN-based reference platform. Our findings point to the significant potential of neuromorphic computing and SNNs in supporting on-board SatCom operations, paving the way for enhanced efficiency and sustainability in future SatCom systems.
</details></li>
</ul>
<hr>
<h2 id="LLaMA-Reviewer-Advancing-Code-Review-Automation-with-Large-Language-Models-through-Parameter-Efficient-Fine-Tuning-Practical-Experience-Report"><a href="#LLaMA-Reviewer-Advancing-Code-Review-Automation-with-Large-Language-Models-through-Parameter-Efficient-Fine-Tuning-Practical-Experience-Report" class="headerlink" title="LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report)"></a>LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11148">http://arxiv.org/abs/2308.11148</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Lu, Lei Yu, Xiaojia Li, Li Yang, Chun Zuo</li>
<li>for:  automatización de tareas de revisión de código</li>
<li>methods:  utilización de modelos de lenguaje grande y técnicas de fine-tuning eficiente de parámetros</li>
<li>results:  iguala el rendimiento de modelos existentes de código de revisión con una base de datos pequeña y limitada cantidad de epochas de entrenamiento<details>
<summary>Abstract</summary>
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.   In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.   An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.   The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.
</details>
<details>
<summary>摘要</summary>
自动化代码审查活动，软件工程中长期追求的问题，主要通过许多域специфи的预训模型解决。尽管它们获得了成功，但它们经常需要大量的资源进行预训。相比之下，大型语言模型（LLM）提供了一个吸引人的代替方案，因为它们在域специфи知识的支持下表现出了惊人的能力。然而，它们在代码审查任务上的潜在作用仍然未得到了充分的探索。为了解决这个研究空白，我们介绍了 LLama Reviewer，一个创新的框架，利用 LLama 语言模型在代码审查中的能力。注意资源限制，这个框架使用了 parameter-efficient fine-tuning（PEFT）方法，以提高性能，同时使用的可调参数少于 1%。我们对 LLama Reviewer 进行了广泛的评估，使用了两个公开available的数据集。结果显示，即使使用 LLama 基础模型的最小版本（6.7B参数）和有限制的调教 epoch，LLama Reviewer 与现有的代码审查专注模型表现相同。我们还进行了一系列的减少实验，以了解不同的精细调教过程组件的影响，包括输入表示、指令调教和不同的 PEFT 方法。为了促进这个领域的不断发展，我们将代码和所有 PEFT-weight 插件公开发布。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps"><a href="#Exploring-Unsupervised-Cell-Recognition-with-Prior-Self-activation-Maps" class="headerlink" title="Exploring Unsupervised Cell Recognition with Prior Self-activation Maps"></a>Exploring Unsupervised Cell Recognition with Prior Self-activation Maps</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11144">http://arxiv.org/abs/2308.11144</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cpystan/psm">https://github.com/cpystan/psm</a></li>
<li>paper_authors: Pingyi Chen, Chenglu Zhu, Zhongyi Shui, Jiatong Cai, Sunyi Zheng, Shichuan Zhang, Lin Yang</li>
<li>for: 提高cell recognition任务中supervised deep learning模型的成功率，减少标注成本。</li>
<li>methods: 使用自我活化图（PSM）生成 Pseudo 面积标注，通过自动学习训练 activation network 生成 PSM，然后通过semantic clustering模块将 PSM 转换为像素级别的semantic pseudo mask。</li>
<li>results: 在两个 histological 数据集（MoNuSeg 和 BCData）上评估了我们的方法，与其他完全超vised 和弱supervised 方法相比，我们的方法可以实现竞争性的性能，而无需任何手动标注。我们的简单 yet effective 框架还可以实现多类细胞检测，其他无supervised 方法无法完成这一点。结果表明 PSM 的潜在价值，可能会激励其他研究人员在医疗领域尝试解决标注的问题。<details>
<summary>Abstract</summary>
The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. Our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. The results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.
</details>
<details>
<summary>摘要</summary>
success of supervised deep learning models on cell recognition tasks rely on detailed annotations. many previous works have managed to reduce the dependency on labels. however, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. to this end, we explored label-free methods for cell recognition. prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. to be specific, an activation network is trained with self-supervised learning. the gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. we evaluated our method on two histological datasets: monunseg (cell segmentation) and bcdata (multi-class cell detection). compared with other fully-supervised and weakly-supervised methods, our method can achieve competitive performance without any manual annotations. our simple but effective framework can also achieve multi-class cell detection which can not be done by existing unsupervised methods. the results show the potential of PSMs that might inspire other research to deal with the hunger for labels in medical area.Note that Simplified Chinese is used here, as it is the more commonly used standard for Chinese translation. Traditional Chinese is also an option, but it may be less widely understood by some readers. Let me know if you would like me to translate the text into Traditional Chinese instead.
</details></li>
</ul>
<hr>
<h2 id="Graph-Encoding-and-Neural-Network-Approaches-for-Volleyball-Analytics-From-Game-Outcome-to-Individual-Play-Predictions"><a href="#Graph-Encoding-and-Neural-Network-Approaches-for-Volleyball-Analytics-From-Game-Outcome-to-Individual-Play-Predictions" class="headerlink" title="Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions"></a>Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11142">http://arxiv.org/abs/2308.11142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rhys Tracy, Haotian Xia, Alex Rasla, Yuan-Fang Wang, Ambuj Singh</li>
<li>for: 本研究旨在提高复杂排球预测精度，为教练和运动员提供更加有意义的洞察。</li>
<li>methods: 我们引入专门的图编码技术，为已有的排球数据添加了额外的接触接触排球上下文。我们使用图神经网络（GNNs）进行三个不同的排球预测任务：落点预测、场地预测和击球类型预测。</li>
<li>results: 我们的图基模型比基线模型表现更出色，在落点预测、场地预测和击球类型预测等三个任务上显著提高了预测结果。此外，我们还发现了一些简单的调整，如移除封锁的击球，可以显著提高预测结果。最后，我们展示了选择合适的模型结构对于某个任务的预测结果具有重要的作用。总的来说，我们的研究表明使用图编码在运动数据分析中具有极大的潜力，并希望未来的机器学习策略在运动和应用中使用图基编码。<details>
<summary>Abstract</summary>
This research aims to improve the accuracy of complex volleyball predictions and provide more meaningful insights to coaches and players. We introduce a specialized graph encoding technique to add additional contact-by-contact volleyball context to an already available volleyball dataset without any additional data gathering. We demonstrate the potential benefits of using graph neural networks (GNNs) on this enriched dataset for three different volleyball prediction tasks: rally outcome prediction, set location prediction, and hit type prediction. We compare the performance of our graph-based models to baseline models and analyze the results to better understand the underlying relationships in a volleyball rally. Our results show that the use of GNNs with our graph encoding yields a much more advanced analysis of the data, which noticeably improves prediction results overall. We also show that these baseline tasks can be significantly improved with simple adjustments, such as removing blocked hits. Lastly, we demonstrate the importance of choosing a model architecture that will better extract the important information for a certain task. Overall, our study showcases the potential strengths and weaknesses of using graph encodings in sports data analytics and hopefully will inspire future improvements in machine learning strategies across sports and applications by using graphbased encodings.
</details>
<details>
<summary>摘要</summary>
（简化中文）这项研究的目标是提高复杂的排球预测精度并为教练和运动员提供更深刻的意义。我们引入特殊的图编码技术，以添加更多的接触点排球上下文到现有的排球数据集中，而无需收集更多数据。我们示出了使用图神经网络（GNNs）在这种增强的数据集上进行三种不同的排球预测任务：落点预测、场地预测和击打类型预测。我们与基线模型进行比较，分析结果以更好地理解排球落点中的下面关系。我们的结果表明，使用GNNs与我们的图编码可以提供更高级的数据分析，显著提高预测结果总体。我们还示出了可以通过简单的调整，如移除封锁的击球，提高基线任务的性能。最后，我们表明了选择合适的模型结构，可以更好地提取关键信息，以便更好地完成特定任务。总之，我们的研究展示了使用图编码在体育数据分析中的潜在优势和不足，希望能启发未来的机器学习策略的改进，以及在体育和应用程序中使用图基于编码。
</details></li>
</ul>
<hr>
<h2 id="Towards-Validating-Long-Term-User-Feedbacks-in-Interactive-Recommendation-Systems"><a href="#Towards-Validating-Long-Term-User-Feedbacks-in-Interactive-Recommendation-Systems" class="headerlink" title="Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems"></a>Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11137">http://arxiv.org/abs/2308.11137</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Hojoon Lee, Dongyoon Hwang, Kyushik Min, Jaegul Choo</li>
<li>for: 这个论文的目的是为了评估基于奖励学习（RL）的推荐系统在互动过程中的性能。</li>
<li>methods: 论文使用了公共可用的评价数据集来比较和评估不同的算法。</li>
<li>results: 研究发现，简单的资产奖励模型在长期效果方面一直表现出优异，而RL基本模型在评价数据集上表现较差。此外，研究还发现，在评价数据集上，用户的反馈具有较少的长期效果。<details>
<summary>Abstract</summary>
Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects of the IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy reward model consistently outperforms RL-based models in maximizing cumulative rewards. Second, applying higher weighting to long-term rewards leads to a degradation of recommendation performance. Third, user feedbacks have mere long-term effects on the benchmark datasets. Based on our findings, we conclude that a dataset has to be carefully verified and that a simple greedy baseline should be included for a proper evaluation of RL-based IRS approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>一个简单的奖励模型在累绩奖励方面一直表现出色，常常高于 RL-based 模型。2. 将更高的权重传递到长期奖励导致推荐性能下降。3. 在评价数据上，用户的反馈仅具有微不足道的长期影响。根据我们的发现，我们结论是，一个数据集需要仔细验证，而且一个简单的奖励基准应该包括在评估 RL-based IRS 方法的评估中。</details></li>
</ol>
<hr>
<h2 id="Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances"><a href="#Transformers-for-Capturing-Multi-level-Graph-Structure-using-Hierarchical-Distances" class="headerlink" title="Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances"></a>Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11129">http://arxiv.org/abs/2308.11129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuankai Luo</li>
<li>for: 本研究旨在提高现有图Transformer的表现，通过设计一种基于层次距离的结构编码方法（HDSE），以适应不同类型的图像处理任务。</li>
<li>methods: 本研究使用的方法包括HDSE方法和现有的位置表示方法的综合应用，以捕捉图像中的多层次、层次结构。</li>
<li>results: 经过广泛的实验 validate，HDSE方法能够在12种实际 dataset上提高多种基eline transformer的表现，达到了10个benchmark dataset的状态时表现。<details>
<summary>Abstract</summary>
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
</details>
<details>
<summary>摘要</summary>
图形转换器需要强大的推导性偏好，以获得有意义的注意分数。然而，当前的建议通常不会考虑 capture longer ranges, hierarchical structures 或 community structures, 这些结构在分子、社交网络和引用网络等图形中都存在。在这篇论文中，我们提出了一种层次距离结构编码（HDSE），它模型了图形中节点之间的层次距离，特别是这些距离的多层、层次结构。因此，我们的 HDSE 方法可以与现有的图形转换器集成，以同时应用其他位置表示。通过对 12 个真实世界数据集进行了广泛的实验，我们示出了 HDSE 方法可以成功地提高多种基eline transformers，在 10 个标准测试集上达到了最佳的实验性能。
</details></li>
</ul>
<hr>
<h2 id="How-Expressive-are-Graph-Neural-Networks-in-Recommendation"><a href="#How-Expressive-are-Graph-Neural-Networks-in-Recommendation" class="headerlink" title="How Expressive are Graph Neural Networks in Recommendation?"></a>How Expressive are Graph Neural Networks in Recommendation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11127">http://arxiv.org/abs/2308.11127</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/gte">https://github.com/hkuds/gte</a></li>
<li>paper_authors: Xuheng Cai, Lianghao Xia, Xubin Ren, Chao Huang</li>
<li>for: 这篇论文主要针对推荐问题的Graph Neural Networks（GNNs）表现能力进行了系统的理论分析，以强化GNNs在推荐 task中的表现。</li>
<li>methods: 该论文使用了三种表达能力指标：图同构（graph-level）、节点自同构（node-level）和结构靠近性（link-level），其中结构靠近性指标是专门为推荐任务设计的，能够评估GNNs在推荐任务中表现的准确性。</li>
<li>results: 该论文通过对多种现有GNN模型进行比较，证明了结构靠近性指标在评估GNNs在推荐任务中表现的效果。此外，该论文还提出了一种学习无关的GNN算法，可以在结构靠近性指标下达到优化的表现。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in recommendation, considering three levels of expressiveness metrics: graph isomorphism (graph-level), node automorphism (node-level), and topological closeness (link-level). We propose the topological closeness metric to evaluate GNNs' ability to capture the structural distance between nodes, which aligns closely with the objective of recommendation. To validate the effectiveness of this new metric in evaluating recommendation performance, we introduce a learning-less GNN algorithm that is optimal on the new metric and can be optimal on the node-level metric with suitable modification. We conduct extensive experiments comparing the proposed algorithm against various types of state-of-the-art GNN models to explore the explainability of the new metric in the recommendation task. For reproducibility, implementation codes are available at https://github.com/HKUDS/GTE.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 已经在不同的图学任务上展现出优秀的性能，包括推荐，其中利用用户Item的协同过滤信号在图中。然而，GNNs的理论表述还很缺乏，尽管它们在现有的推荐模型中的实际效果很高。最近，研究人员开始探索GNNs的表达能力，并证明了消息传递GNNs在最多情况下与维氏-莱曼测试相当强大，并且GNNs与随机节点初始化结合起来是 universally expressive。然而，GNNs的“表达能力”概念仍然很模糊。大多数现有的工作采用图 isomorphism test作为表达能力的度量，但这可能不能有效地评估一个模型在推荐任务中的能力，因为推荐任务的目标是将节点分类为不同的靠近程度。在这篇论文中，我们提供了对GNNs在推荐任务中表达能力的全面性分析，包括图 isomorphism（图级），节点自身omorphism（节点级）和 topological closeness（链级）三种表达能力度量。我们提出了 topological closeness 度量，以评估 GNNs 在不同节点之间的结构距离，这与推荐任务的目标很 closely align。为验证这个新的度量在推荐任务中的效用，我们引入了一种不含学习的 GNN 算法，该算法在新的度量上是优化的，并且可以通过修改来在节点级度量上达到优化。我们对多种现有的 state-of-the-art GNN 模型进行了广泛的比较，以探索这个新的度量在推荐任务中的解释性。为保持可重现性，我们在 GitHub 上提供了实现代码，可以在 https://github.com/HKUDS/GTE 中找到。
</details></li>
</ul>
<hr>
<h2 id="Random-Word-Data-Augmentation-with-CLIP-for-Zero-Shot-Anomaly-Detection"><a href="#Random-Word-Data-Augmentation-with-CLIP-for-Zero-Shot-Anomaly-Detection" class="headerlink" title="Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection"></a>Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11119">http://arxiv.org/abs/2308.11119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Masato Tamura</li>
<li>for: 这个研究是为了开发一个基于 CLIP 的零数据Point Anomaly Detection 方法。</li>
<li>methods: 这个方法使用 CLIP 的显示语言模型来生成图像中的文本嵌入，并使用这些嵌入进行类别。</li>
<li>results: 实验结果显示，这个方法可以在零数据情况下 дости持state-of-the-art表现，并且不需要传入任何训练图像。<details>
<summary>Abstract</summary>
This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLIP with typical prompts that include words of normal and anomaly. In addition to these words, we insert several randomly generated words into prompts, which enables the encoder to generate a diverse set of normal and anomalous samples. Using the generated embeddings as training data, a feed-forward neural network learns to extract features of normal and anomaly from CLIP's embeddings, and as a result, a category-agnostic anomaly detector can be obtained without any training images. Experimental results demonstrate that our method achieves state-of-the-art performance without laborious prompt ensembling in zero-shot setups.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文提出了一种新的方法，利用视觉语言模型CLIP作为数据源，实现零例异常检测。因为异常样本很难获得，现有的方法通常只使用正常样本进行训练，并在推理时对正常样本的分布进行评估，这需要对每个物体类型进行训练。这种不fficient的训练方式被解决了，通过使用CLIP中的提示引导分类来对每个图像中的每个部分进行滑动窗口 manner进行异常检测。然而，这种方法仍然受到提示的精心组合的压力。为了解决以上问题，我们提议利用CLIP作为训练数据源。我们的方法生成了文本嵌入，使用CLIP中的文本编码器，并使用典型的提示，包括正常和异常的词语。此外，我们还在提示中插入了一些随机生成的词语，使编码器生成的嵌入集是多样的正常和异常样本。使用生成的嵌入作为训练数据，一个卷积神经网络学习提取CLIP的嵌入中的正常和异常特征，从而实现了没有任何训练图像的类型无关异常检测器。实验结果表明，我们的方法在零例设置下达到了状态的艺术性表现，无需劳累的提示组合。
</details></li>
</ul>
<hr>
<h2 id="Development-of-a-Novel-Quantum-Pre-processing-Filter-to-Improve-Image-Classification-Accuracy-of-Neural-Network-Models"><a href="#Development-of-a-Novel-Quantum-Pre-processing-Filter-to-Improve-Image-Classification-Accuracy-of-Neural-Network-Models" class="headerlink" title="Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models"></a>Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11112">http://arxiv.org/abs/2308.11112</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hajimesuzuki999/qpf">https://github.com/hajimesuzuki999/qpf</a></li>
<li>paper_authors: Farina Riaz, Shahab Abdulla, Hajime Suzuki, Srinjoy Ganguly, Ravinesh C. Deo, Susan Hopkins</li>
<li>for: 提高图像分类模型的准确率</li>
<li>methods: 使用量子预处理筛选器（QPF），应用于图像分类模型中</li>
<li>results: 对于MNIST和EMNIST dataset，使用QPF提高了图像分类精度，从92.5%提高到95.4%和从68.9%提高到75.9%，而不需要额外的模型参数或优化。但是在GTSRB dataset上，使用QPF显示了一个较大的下降。<details>
<summary>Abstract</summary>
This paper proposes a novel quantum pre-processing filter (QPF) to improve the image classification accuracy of neural network (NN) models. A simple four qubit quantum circuit that uses Y rotation gates for encoding and two controlled NOT gates for creating correlation among the qubits is applied as a feature extraction filter prior to passing data into the fully connected NN architecture. By applying the QPF approach, the results show that the image classification accuracy based on the MNIST (handwritten 10 digits) and the EMNIST (handwritten 47 class digits and letters) datasets can be improved, from 92.5% to 95.4% and from 68.9% to 75.9%, respectively. These improvements were obtained without introducing extra model parameters or optimizations in the machine learning process. However, tests performed on the developed QPF approach against a relatively complex GTSRB dataset with 43 distinct class real-life traffic sign images showed a degradation in the classification accuracy. Considering this result, further research into the understanding and the design of a more suitable quantum circuit approach for image classification neural networks could be explored utilizing the baseline method proposed in this paper.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="CAME-Contrastive-Automated-Model-Evaluation"><a href="#CAME-Contrastive-Automated-Model-Evaluation" class="headerlink" title="CAME: Contrastive Automated Model Evaluation"></a>CAME: Contrastive Automated Model Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11111">http://arxiv.org/abs/2308.11111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pengr/contrastive_autoeval">https://github.com/pengr/contrastive_autoeval</a></li>
<li>paper_authors: Ru Peng, Qiuyang Duan, Haobo Wang, Jiachen Ma, Yanbo Jiang, Yongjun Tu, Xiu Jiang, Junbo Zhao</li>
<li>for: 评估已经训练的机器学习模型的性能，无需使用标注的测试集。</li>
<li>methods: 提出了一种新的自动评估框架，即对比自动评估（CAME），不再需要使用训练集。基于理论分析和广泛的实验验证，CAME可以在无标注测试集的情况下建立模型性能和对比损失之间的可预测关系。</li>
<li>results: CAME在自动评估领域的新标准时间（SOTA）Result，舒过先前的工作。<details>
<summary>Abstract</summary>
The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior work significantly.
</details>
<details>
<summary>摘要</summary>
自动模型评估（AutoEval）框架考虑不使用标注测试集来评估训练完成的机器学习模型。尽管存在承诺和一些不错的结果，现有的AutoEval方法仍然很重要地依赖于计算测试集和训练集之间的分布差异。我们认为，这种依赖于训练集会成为实际机器学习开发中这种技术的又一个障碍。在这种工作中，我们提出了对比自动模型评估（CAME）框架，它不再依赖于训练集。CAME框架的核心思想是基于一种理论分析，它将模型性能与对比损失相关联。然后，通过广泛的实验验证，我们成功地建立了测试集中未经标注的模型性能和对比损失之间的可预测关系。这种关系的建立使得CAME框架可以超越现有的AutoEval方法，并设置新的最高纪录（SOTA）。
</details></li>
</ul>
<hr>
<h2 id="Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models"><a href="#Anonymity-at-Risk-Assessing-Re-Identification-Capabilities-of-Large-Language-Models" class="headerlink" title="Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models"></a>Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11103">http://arxiv.org/abs/2308.11103</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models">https://github.com/skatinger/anonymity-at-risk-assessing-re-identification-capabilities-of-large-language-models</a></li>
<li>paper_authors: Alex Nyffenegger, Matthias Stürmer, Joel Niklaus</li>
<li>for:  This paper aims to explore the potential of large language models (LLMs) to re-identify individuals in court rulings, with a focus on the Federal Supreme Court of Switzerland.</li>
<li>methods:  The authors constructed a proof-of-concept using actual legal data from the Swiss federal supreme court and an anonymized Wikipedia dataset to investigate the feasibility of re-identifying individuals using LLMs. They introduced new metrics to measure performance and systematically analyzed the factors that influence successful re-identifications.</li>
<li>results:  Despite high re-identification rates on Wikipedia, the authors found that even the best LLMs struggled with court decisions, attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. The study demonstrates that re-identification using LLMs may not be feasible for now, but it might become possible in the future.<details>
<summary>Abstract</summary>
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on Wikipedia, even the best LLMs struggled with court decisions. The complexity is attributed to the lack of test datasets, the necessity for substantial training resources, and data sparsity in the information used for re-identification. In conclusion, this study demonstrates that re-identification using LLMs may not be feasible for now, but as the proof-of-concept on Wikipedia showed, it might become possible in the future. We hope that our system can help enhance the confidence in the security of anonymized decisions, thus leading to the courts being more confident to publish decisions.
</details>
<details>
<summary>摘要</summary>
欧盟和瑞士的法院判决中的自然人和法人匿名性是隐私保护的关键方面。随着LLM的出现，大规模重新标识匿名人士的问题减少。根据瑞士联邦最高法院的规定，我们在实际的法院判决数据上进行了证明。然后，我们使用了一个更加严格的Wikipedia数据集进行进一步的研究。我们将重新标识文本中的人员为新任务，并引入了新的评价指标。我们系统地分析了重要因素的影响，包括模型大小、输入长度和调整指南。尽管在Wikipedia上达到了高重新标识率，但是even the best LLMs在法院判决上遇到了困难。这种复杂性归结于缺乏测试集，需要巨量的训练资源以及重要的数据稀缺。因此，本研究表明，使用LLM重新标识可能不可能现在，但是在未来可能变得可能。我们希望通过我们的系统，增强对匿名判决的信任，使法院更自信地发布判决。
</details></li>
</ul>
<hr>
<h2 id="Explicability-and-Inexplicability-in-the-Interpretation-of-Quantum-Neural-Networks"><a href="#Explicability-and-Inexplicability-in-the-Interpretation-of-Quantum-Neural-Networks" class="headerlink" title="Explicability and Inexplicability in the Interpretation of Quantum Neural Networks"></a>Explicability and Inexplicability in the Interpretation of Quantum Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11098">http://arxiv.org/abs/2308.11098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lirandepira/interpret-qnn">https://github.com/lirandepira/interpret-qnn</a></li>
<li>paper_authors: Lirandë Pira, Chris Ferrie</li>
<li>for: 本研究旨在探讨量子神经网络的解释性，以帮助建立可信任的量子AI系统。</li>
<li>methods: 本研究使用了本地模型无关解释性度量来研究量子和类传统神经网络的解释性。它还 introduce了带宽不可解释区（band of inexplicability），表示无法解释的数据样本。</li>
<li>results: 研究发现，量子神经网络的解释性与其数据采样方式有关，并且存在一个带宽不可解释区，表示无法解释的数据样本。这些结果为建立负责任和可负责的量子AI模型提供了一个重要的基础。<details>
<summary>Abstract</summary>
Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
</details>
<details>
<summary>摘要</summary>
<<SYS>>对人工智能（AI）技术的可解释性（Interpretability）是非常关键的，因为AI支持的系统在使用中经常表现出不可解释的行为。这种可解释性是建立可信系统的重要组成部分。虽然有很多方法可以解决这个问题，但它们不显而易懂地推广到量子设置下。在这里，我们研究了量子神经网络的可解释性，使用本地模型无关的解释度量来评估量子和经典神经网络的可解释性。我们称之为“不可解释的带”（band of inexplicability），表示无法解释的数据样本的解释不可能， probable victims of inherently random quantum measurements。我们认为这是一步向于理解如何构建负责任和可负责的量子AI模型。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video"><a href="#Video-OWL-ViT-Temporally-consistent-open-world-localization-in-video" class="headerlink" title="Video OWL-ViT: Temporally-consistent open-world localization in video"></a>Video OWL-ViT: Temporally-consistent open-world localization in video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11093">http://arxiv.org/abs/2308.11093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Heigold, Matthias Minderer, Alexey Gritsenko, Alex Bewley, Daniel Keysers, Mario Lučić, Fisher Yu, Thomas Kipf</li>
<li>for: 本文采用预训练的开放世界图像模型进行视频地标定。</li>
<li>methods: 本文使用了对大量图像文本数据进行对比预训练，然后将该模型应用到视频地标定任务上，并添加了一个变换器解码器来在视频帧中传播物体表示。</li>
<li>results: 本文在TAO-OW标准测试集上展示了对开放世界地标定任务的成功转移，而且与基于检测的跟踪基eline相比，本文的模型具有更好的时间一致性。<details>
<summary>Abstract</summary>
We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection baselines, while retaining the open-world capabilities of the backbone detector. We evaluate our model on the challenging TAO-OW benchmark and demonstrate that open-world capabilities, learned from large-scale image-text pre-training, can be transferred successfully to open-world localization across diverse videos.
</details>
<details>
<summary>摘要</summary>
我们提出了一种体系和训练方法，可以将预训练的开放世界图像模型适应到视频本地化。理解开放视觉世界（不受固定标签空间的限制）对实际视觉任务非常重要。最近，对大量图像文本数据进行对比预训练已经导致了图像级任务的显著改进。但是，对于结构化任务，如对象本地化，使用预训练模型更加具有挑战性。特别是在视频任务中，任务特有的数据受限。我们构建了基于 OWL-ViT 开放 vocabulary 检测模型，并将其适应到视频中。将输出 токен用于下一帧的对象查询。我们的模型是可以综合训练在视频数据上，并且具有更好的时间一致性，与racking-by-detection 基elines相比，保留了预训练模型的开放世界能力。我们在 TAO-OW 测试准则上评估了我们的模型，并证明了预训练的开放世界能力可以成功地传递到多种视频中的开放本地化。
</details></li>
</ul>
<hr>
<h2 id="Addressing-Fairness-and-Explainability-in-Image-Classification-Using-Optimal-Transport"><a href="#Addressing-Fairness-and-Explainability-in-Image-Classification-Using-Optimal-Transport" class="headerlink" title="Addressing Fairness and Explainability in Image Classification Using Optimal Transport"></a>Addressing Fairness and Explainability in Image Classification Using Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11090">http://arxiv.org/abs/2308.11090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Philipp Ratz, François Hu, Arthur Charpentier</li>
<li>for: 本研究旨在提高人工智能系统的可靠性和负责任性，通过推导对不公平结果的解释来建立信任和负责任。</li>
<li>methods: 本研究提出了一种完整的方法，利用最优运输理论来探索图像中偏见的原因和影响，这种方法可以轻松扩展到表格数据上。</li>
<li>results: 研究发现，通过使用沃氏矩阵中心来获得不受敏感变量影响的分数，同时保持分数的排序结果，可以帮助找出偏见的主要来源和影响。这些发现有助于构建可靠、公平和透明的人工智能系统，在医疗和巡捕等领域中应用。<details>
<summary>Abstract</summary>
Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we obtain scores that are independent of a sensitive variable but keep their marginal orderings. This step ensures predictive accuracy but also helps us to recover the regions most associated with the generation of the biases. Our findings hold significant implications for the development of trustworthy and unbiased AI systems, fostering transparency, accountability, and fairness in critical decision-making scenarios across diverse domains.
</details>
<details>
<summary>摘要</summary>
算法公正和可解释性是在医疗和警察等领域建立人工智能系统的信任和负责任的关键因素。虽然在每个领域 separately 有所进步，但在公正性应用中保持可解释性仍然是挑战，特别是在使用深度神经网络时。同时，伦理数据挖掘已经变得更加重要，因为不公正的算法结果常常导致偏见。现有的方法主要是为了减轻模型的偏见结果，但很少人尝试解释为何模型偏见。为了bridging这个差距，我们提出了一种全面的方法，利用最优运输理论来揭示偏见区域在图像中的原因和后果，这种方法也可以轻松扩展到表格数据。通过使用沃asserstein矩阵，我们可以获得不同敏感变量之间的相对排名，这步确保预测精度，同时帮助我们回溯偏见的生成原因。我们的发现对于开发可靠、不偏见的人工智能系统产生了重要的影响，推动了透明度、负责任和公正在多个领域中的决策过程中的发展。
</details></li>
</ul>
<hr>
<h2 id="Stress-representations-for-tensor-basis-neural-networks-alternative-formulations-to-Finger-Rivlin-Ericksen"><a href="#Stress-representations-for-tensor-basis-neural-networks-alternative-formulations-to-Finger-Rivlin-Ericksen" class="headerlink" title="Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen"></a>Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11080">http://arxiv.org/abs/2308.11080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan N. Fuhg, Nikolaos Bouklas, Reese E. Jones</li>
<li>For: This paper focuses on developing and comparing different neural network models for modeling hyperelastic materials in a finite deformation context.* Methods: The paper uses a variety of tensor basis neural network models, including some unexplored formulations, and compares their performance against noisy and noiseless datasets for three different materials. The paper also examines the effectiveness of potential-based and coefficient-based approaches, as well as different calibration techniques.* Results: The paper provides theoretical and practical insights into the performance of each formulation, and compares the results of the nine variants tested.<details>
<summary>Abstract</summary>
Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as different calibration techniques. Nine variants are tested against both noisy and noiseless datasets for three different materials. Theoretical and practical insights into the performance of each formulation are given.
</details>
<details>
<summary>摘要</summary>
“数据驱动的构成模型框架，基于神经网络和经典表述定理，在最近受到了广泛关注，因为它们可以轻松地包含构成约束和优秀的泛化性能。在这些模型中，压力预测来自于线性组合的不变 coefficient 函数和已知的张量基 generator。然而，至今为止，这些形ulation 仅限于压力表示基于经典的 Rivlin 和 Eriksson 形式，尚未探讨其他表示形式的性能。在这种工作中，我们检验了一种多种张量基神经网络模型，用于模型弹性材料在有限减形上，包括一些尚未探讨的形式，它们使用了同等的 invariants 和生成器来 Finger-Rivlin-Eriksson。此外，我们比较了 potential-based 和 coefficient-based 方法，以及不同的 calibration 技术。 nine 个变体被测试在三种不同材料上，对于噪声和噪声无的数据集。我们提供了理论和实践的深入分析每种形式的性能。”Note: Simplified Chinese is used in this translation, which is a simplified version of Traditional Chinese.
</details></li>
</ul>
<hr>
<h2 id="Long-Term-Prediction-of-Natural-Video-Sequences-with-Robust-Video-Predictors"><a href="#Long-Term-Prediction-of-Natural-Video-Sequences-with-Robust-Video-Predictors" class="headerlink" title="Long-Term Prediction of Natural Video Sequences with Robust Video Predictors"></a>Long-Term Prediction of Natural Video Sequences with Robust Video Predictors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11079">http://arxiv.org/abs/2308.11079</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luke Ditria, Tom Drummond</li>
<li>for: 预测高维度视频序列是一个具有挑战性的问题，由于未知性的增加，预测范围越长，难度也越大。本文提出了一些改进方案来创建Robust Video Predictors（RoViPs），以提高短期预测质量。</li>
<li>methods: 本文使用深度Perceptual和不确定性基于的重建损失，以及注意力机制 skip 连接，以提高输入特征的长距离空间移动性。</li>
<li>results: 本文表明，通过使用这些改进方案，可以创建高质量的短期预测，并且可以通过迭代单步预测任务，生成非常长的自然视频序列。<details>
<summary>Abstract</summary>
Predicting high dimensional video sequences is a curiously difficult problem. The number of possible futures for a given video sequence grows exponentially over time due to uncertainty. This is especially evident when trying to predict complicated natural video scenes from a limited snapshot of the world. The inherent uncertainty accumulates the further into the future you predict making long-term prediction very difficult. In this work we introduce a number of improvements to existing work that aid in creating Robust Video Predictors (RoViPs). We show that with a combination of deep Perceptual and uncertainty-based reconstruction losses we are able to create high quality short-term predictions. Attention-based skip connections are utilised to allow for long range spatial movement of input features to further improve performance. Finally, we show that by simply making the predictor robust to its own prediction errors, it is possible to produce very long, realistic natural video sequences using an iterated single-step prediction task.
</details>
<details>
<summary>摘要</summary>
“预测高维视频序列是一个奇怪地困难的问题。视频序列中可能的未来数量因时间不确定性而呈指数增长。特别是当试图从有限的世界快照中预测自然的视频场景时，这种不确定性会更加突出。在这种情况下，我们介绍了一些改进现有工作的方法，以创建robust Video Predictors（RoViPs）。我们表明，通过深度感知和不确定性基于的重建损失，可以创造高质量短期预测。增强功能skip连接使得输入特征可以在较长距离上进行空间运动，进一步改善性能。最后，我们表明，只需要让预测器具有自己预测错误的抗性，就可以使用迭代单步预测任务生成非常长、自然的视频序列。”Note that Simplified Chinese is a standardized form of Chinese that is used in mainland China and Singapore, while Traditional Chinese is used in Taiwan, Hong Kong, and other parts of the world. The translation may vary slightly depending on the specific dialect or variation of Chinese that is used.
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Dive-into-the-Connections-Between-the-Renormalization-Group-and-Deep-Learning-in-the-Ising-Model"><a href="#A-Deep-Dive-into-the-Connections-Between-the-Renormalization-Group-and-Deep-Learning-in-the-Ising-Model" class="headerlink" title="A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model"></a>A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11075">http://arxiv.org/abs/2308.11075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kelsie Taylor</li>
<li>for: 这个论文是关于物理理论和量子场论中的粒子规范群（RG）技术，以及深度学习（Deep Learning）的相互关系。</li>
<li>methods: 作者使用了Restricted Boltzmann Machines（RBMs）来实现深度学习，并对2D最近邻Iseng模型进行了Kadanoff块 renormalization。作者还开发了1D和2D Ising模型的扩展renormalization技术作为比较基准。</li>
<li>results: 作者成功地使用了Adam优化器和相关长度损失函数来学习群流，并对1D Ising模型获得了与分析模型相符的结果。作者还使用了Wolff算法生成了Iseng模型样本，并使用了 quasi-deterministic方法进行了群流。最后，作者发现了RBM学习层次结构与RG块化结构之间的相似性。<details>
<summary>Abstract</summary>
The renormalization group (RG) is an essential technique in statistical physics and quantum field theory, which considers scale-invariant properties of physical theories and how these theories' parameters change with scaling. Deep learning is a powerful computational technique that uses multi-layered neural networks to solve a myriad of complicated problems. Previous research suggests the possibility that unsupervised deep learning may be a form of RG flow, by being a layer-by-layer coarse graining of the original data. We examined this connection on a more rigorous basis for the simple example of Kadanoff block renormalization of the 2D nearest-neighbor Ising model, with our deep learning accomplished via Restricted Boltzmann Machines (RBMs). We developed extensive renormalization techniques for the 1D and 2D Ising model to provide a baseline for comparison. For the 1D Ising model, we successfully used Adam optimization on a correlation length loss function to learn the group flow, yielding results consistent with the analytical model for infinite N. For the 2D Ising model, we successfully generated Ising model samples using the Wolff algorithm, and performed the group flow using a quasi-deterministic method, validating these results by calculating the critical exponent \nu. We then examined RBM learning of the Ising model layer by layer, finding a blocking structure in the learning that is qualitatively similar to RG. Lastly, we directly compared the weights of each layer from the learning to Ising spin renormalization, but found quantitative inconsistencies for the simple case of nearest-neighbor Ising models.
</details>
<details>
<summary>摘要</summary>
“ renormalization group（RG）是物理学和量子场论中的一种重要技术，它考虑物理理论中尺度不变性的性质和尺度下降参数的变化。深度学习是一种强大的计算技术，使用多层神经网络解决各种复杂问题。以前的研究表明，无监督深度学习可能是一种RG流，通过层次归约原始数据来实现。我们在2D nearest-neighbor Ising模型中进行了更加严谨的研究，我们使用Restricted Boltzmann Machines（RBMs）来实现深度学习。我们为1D和2D Ising模型开发了广泛的renoormalization技术，以提供比较基eline。对1D Ising模型，我们使用Adam优化器和尺度长度损失函数来学习群流，得到了与分析模型的一致 результа。对2D Ising模型，我们使用Wolff算法生成样本，并使用一种 quasi-deterministic 方法来实现群流，并计算了扩散率 \nu。然后，我们对 Ising 模型层次进行了 RBM 学习，发现了一种块结构，与RG有趋同性。最后，我们直接比较了各层 weights 的学习结果和 Ising 磁力 renormalization，发现了简单 nearest-neighbor Ising 模型中的量化不一致。”
</details></li>
</ul>
<hr>
<h2 id="Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning"><a href="#Neural-Amortized-Inference-for-Nested-Multi-agent-Reasoning" class="headerlink" title="Neural Amortized Inference for Nested Multi-agent Reasoning"></a>Neural Amortized Inference for Nested Multi-agent Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11071">http://arxiv.org/abs/2308.11071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kunal Jha, Tuan Anh Le, Chuanyang Jin, Yen-Ling Kuo, Joshua B. Tenenbaum, Tianmin Shu</li>
<li>for: 本研究旨在提高多代理交互中的社会推理能力，以便更好地模拟人类的社会听说和推理能力。</li>
<li>methods: 本研究使用神经网络来快速化高阶社会推理，以便在多代理交互中提高推理效率。</li>
<li>results: 实验结果显示，使用神经网络可以减少计算复杂性，同时保持准确性。<details>
<summary>Abstract</summary>
Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
</details>
<details>
<summary>摘要</summary>
多智能机器人之间的交互，如 коммуника、教学和谎言，frequently rely on高级社会推理，即理解他们如何推理自己。这种复杂的推理可以通过嵌入式多智能推理模型来有效地模拟。然而，计算复杂性随着每级别推理的层次层次增加， pose a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Topological-Graph-Signal-Compression"><a href="#Topological-Graph-Signal-Compression" class="headerlink" title="Topological Graph Signal Compression"></a>Topological Graph Signal Compression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11068">http://arxiv.org/abs/2308.11068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillermo Bernárdez, Lev Telyatnikov, Eduard Alarcón, Albert Cabellos-Aparicio, Pere Barlet-Ros, Pietro Liò</li>
<li>for: 提出了一种基于Topological Deep Learning（TDL）方法的信号压缩方法，用于处理图structured数据。</li>
<li>methods: 方法包括两个主要步骤：首先，基于原始信号，使用 clustering 将 $N$ 个数据点分为 $K\ll N$ 个多元素集合；然后，使用 topological-inspired message passing 获取这些多元素集合中的压缩表示。</li>
<li>results: 我们的框架可以在两个实际Internet Service Provider Networks的数据集上提高标准GNN和Feed-Forward架构的压缩率，从 $30%$ 提高到 $90%$，表明它更好地捕捉和利用图structured网络结构中的空间和时间相关性。<details>
<summary>Abstract</summary>
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and temporal correlations over the whole graph-based network structure.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Disjoint sets of higher-order structures are inferred based on the original signal by clustering $N$ datapoints into $K\ll N$ collections.2. A topological-inspired message passing gets a compressed representation of the signal within those multi-element sets.Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-world Internet Service Provider Networks’ datasets - from $30%$ up to $90%$ better reconstruction errors across all evaluation scenarios - suggesting that it better captures and exploits spatial and temporal correlations over the whole graph-based network structure.Translated into Simplified Chinese:最近提出的 topological deep learning (TDL) 方法目的是扩展当前的图神经网络 (GNN)，以自然处理更高级别的交互，超越对图表示的对称关系和本地邻域。在这篇论文中，我们提出了一种基于 TDL 的图信号压缩方法，包括两个主要步骤：1. 根据原始信号，使用 clustering 将 $N$ 个数据点分为 $K\ll N$ 个集合。2. 使用图启发的消息传递机制，在多个元素集中获得压缩表示。我们的结果表明，我们的框架可以提高标准 GNN 和Feed-Forward 架构在图信号压缩方面的性能，从两个实际的互联网服务提供商网络数据集中，提高压缩率，从 $30%$ 到 $90%$，表明它更好地捕捉和利用图structured 网络的空间和时间相关性。</details></li>
</ol>
<hr>
<h2 id="UnLoc-A-Unified-Framework-for-Video-Localization-Tasks"><a href="#UnLoc-A-Unified-Framework-for-Video-Localization-Tasks" class="headerlink" title="UnLoc: A Unified Framework for Video Localization Tasks"></a>UnLoc: A Unified Framework for Video Localization Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11062">http://arxiv.org/abs/2308.11062</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/google-research/scenic">https://github.com/google-research/scenic</a></li>
<li>paper_authors: Shen Yan, Xuehan Xiong, Arsha Nagrani, Anurag Arnab, Zhonghao Wang, Weina Ge, David Ross, Cordelia Schmid</li>
<li>for: 这篇论文是为了解决无法裁剪视频的视频地址Localization问题而设计的。</li>
<li>methods: 该论文使用预训练的图像和文本楼层，并将其传递给视频-文本融合模型。输出的融合模块 then用于构建一个特征峰，每个级别与一个头相连，以预测每帧的相关性分数和开始&#x2F;结束时间偏移。</li>
<li>results: 该方法可以实现Moment Retrieval、Temporal Localization和Action Segmentation三个任务，而无需动作提案、运动基于预训练特征或 Representation masking。与专门的模型不同，我们在三个不同的地址Localization任务上达到了状态 искусственный智能的最佳result。<details>
<summary>Abstract</summary>
While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \url{https://github.com/google-research/scenic}.
</details>
<details>
<summary>摘要</summary>
大规模图像文本预训练模型，如CLIP，已经用于多个视频级任务，但它们在未处理的视频中进行时间地理化仍然是一个相对未探索的任务。我们设计了一种新的方法，即UnLoc，它使用预训练的图像和文本楼层，并将Token传递给视频文本融合模型。模型的输出被用construct一个特征 pyramid，其中每个层与一个头连接，以预测每帧相关性分数和开始/结束时间偏移。与先前的工作不同，我们的架构允许每帧 Localization、Temporal Localization 和 Action Segmentation 通过单一的阶段模型进行，不需要动作提案、运动基于预训练特征或表示掩码。与专门的模型不同，我们在三个不同的地理化任务上达到了状态机器的最佳result，代码将在： \url{https://github.com/google-research/scenic} 中提供。
</details></li>
</ul>
<hr>
<h2 id="FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning"><a href="#FedDAT-An-Approach-for-Foundation-Model-Finetuning-in-Multi-Modal-Heterogeneous-Federated-Learning" class="headerlink" title="FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning"></a>FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haokun Chen, Yao Zhang, Denis Krompass, Jindong Gu, Volker Tresp</li>
<li>for: 这个研究是为了提高基础模型在多modal学习中的表现，并且解决训练需要大量数据的问题。</li>
<li>methods: 这个研究使用了联合学习（Federated Learning）和内部知识传播（Mutual Knowledge Distillation）等方法，以及一个叫做DAT的双适束教师，来处理客户端资料不同性。</li>
<li>results: 这个研究的结果显示，使用FedDAT可以将基础模型进行高效的分布式调整，并且在不同类型的多modal任务上表现出色，比较现有的中央PEFT方法更高效。<details>
<summary>Abstract</summary>
Recently, foundation models have exhibited remarkable advancements in multi-modal learning. These models, equipped with millions (or billions) of parameters, typically require a substantial amount of data for finetuning. However, collecting and centralizing training data from diverse sectors becomes challenging due to distinct privacy regulations. Federated Learning (FL) emerges as a promising solution, enabling multiple clients to collaboratively train neural networks without centralizing their local data. To alleviate client computation burdens and communication overheads, previous works have adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a small fraction of the model parameters are optimized and communicated during federated communications. Nevertheless, most previous works have focused on a single modality and neglected one common phenomenon, i.e., the presence of data heterogeneity across the clients. Therefore, in this work, we propose a finetuning framework tailored to heterogeneous multi-modal FL, called Federated Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the client local updates and applying Mutual Knowledge Distillation (MKD) for an efficient knowledge transfer. FedDAT is the first approach that enables an efficient distributed finetuning of foundation models for a variety of heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we conduct extensive experiments on four multi-modality FL benchmarks with different types of data heterogeneity, where FedDAT substantially outperforms the existing centralized PEFT methods adapted for FL.
</details>
<details>
<summary>摘要</summary>
近些时候，基金会模型在多Modal学习方面做出了很大的进步。这些模型通常需要大量数据进行微调，但由于不同领域的隐私规定，收集和中央化训练数据变得困难。为了解决这问题，联邦学习（FL）出现了，允许多个客户共同训练神经网络，无需中央化地方数据。为了减轻客户计算负担和通信开销，先前的工作已经采用了精度高的微调方法（PEFT）。然而，大多数先前的工作都是单modal的，忽略了客户数据的不同性。因此，在这项工作中，我们提出了适用于多样性多Modal FL的微调框架，即联邦双Adapter教师（FedDAT）。我们的方法利用了双Adapter教师（DAT）来 Address客户本地更新的数据不同性，并通过互助知识传播（MKD）来进行高效的知识传递。FedDAT 是首个可以有效地分布式微调基础模型的多Modal Vision-Language 联邦FL 方法。为证明其有效性，我们在四个多Modal FL 标准准的不同类型的数据不同性上进行了广泛的实验，并证明FedDAT 在这些标准上大幅超越了已有的中央化 PEFT 方法。
</details></li>
</ul>
<hr>
<h2 id="Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression"><a href="#Ultra-Dual-Path-Compression-For-Joint-Echo-Cancellation-And-Noise-Suppression" class="headerlink" title="Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression"></a>Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11053">http://arxiv.org/abs/2308.11053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hangting Chen, Jianwei Yu, Yi Luo, Rongzhi Gu, Weihua Li, Zhuocheng Lu, Chao Weng</li>
<li>for: 实现全动频通信的杂音抑制和调变减少，但大多数现有的神经网络具有高计算成本和不可靠的模型复杂度调整。</li>
<li>methods: 我们提出了时间-频率二重路径压缩，以实现广泛的压缩比率选择。特别是在频率压缩方面，使用可训练的范子网络来进行维度减少。在时间压缩方面，仅使用几帧预测会导致巨大的性能下降，可以通过全序模型来缓和这一下。</li>
<li>results: 我们发现，将时间和频率两种方法组合在一起，可以实现更好的性能改进，覆盖压缩比率的选择范围从4x至32x，而且这些提案的模型具有与快速FullSubNet和DeepFilterNet相似的竞争性能。<details>
<summary>Abstract</summary>
Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be found at hangtingchen.github.io/ultra_dual_path_compression.github.io/.
</details>
<details>
<summary>摘要</summary>
echo 抑制和噪声减少是全双工通信的必备条件，然而大多数现有的神经网络具有高计算成本和不 flexible 的模型复杂度调整。在这篇论文中，我们引入时频双路压缩来实现广泛的压缩比例。 Specifically， для频率压缩，使用可训练的滤波器来替代手动设计的滤波器进行维度减少。 For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be found at hangtingchen.github.io/ultra_dual_path_compression.github.io/.Note that the text has been translated using Simplified Chinese characters and grammar. If you prefer Traditional Chinese, please let me know and I can provide the translation using Traditional Chinese characters and grammar.
</details></li>
</ul>
<hr>
<h2 id="Harmonization-Across-Imaging-Locations-HAIL-One-Shot-Learning-for-Brain-MRI"><a href="#Harmonization-Across-Imaging-Locations-HAIL-One-Shot-Learning-for-Brain-MRI" class="headerlink" title="Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI"></a>Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11047">http://arxiv.org/abs/2308.11047</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhijeet Parida, Zhifan Jiang, Syed Muhammad Anwar, Nicholas Foreman, Nicholas Stence, Michael J. Fisher, Roger J. Packer, Robert A. Avery, Marius George Linguraru</li>
<li>for: 这篇论文旨在应用深度学习技术来进行少见疾病，如儿童脑肿瘤的诊断和预测。</li>
<li>methods: 本论文使用生成对抗网络（GANs）进行深度学习预测，并提出了一个一击学习方法，将单一的诊断影像转换为医疗影像集合中的一个标准化影像。</li>
<li>results: 实验结果显示，本方法可以优化医疗影像的尺度，同时保持病人解剖结构的稳定性。<details>
<summary>Abstract</summary>
For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmonization approaches with evaluation metrics that both measure image style harmonization and assess the preservation of anatomical structures. Experimental results demonstrate the effectiveness of our method in preserving patient anatomy while adjusting the image intensities to a new clinical site. Our general harmonization model can be used on unseen data from new sites, making it a valuable tool for real-world medical applications and clinical trials.
</details>
<details>
<summary>摘要</summary>
For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmonization approaches with evaluation metrics that both measure image style harmonization and assess the preservation of anatomical structures. Experimental results demonstrate the effectiveness of our method in preserving patient anatomy while adjusting the image intensities to a new clinical site. Our general harmonization model can be used on unseen data from new sites, making it a valuable tool for real-world medical applications and clinical trials.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Spurious-Correlations-and-Where-to-Find-Them"><a href="#Spurious-Correlations-and-Where-to-Find-Them" class="headerlink" title="Spurious Correlations and Where to Find Them"></a>Spurious Correlations and Where to Find Them</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11043">http://arxiv.org/abs/2308.11043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gautam Sreekumar, Vishnu Naresh Boddeti</li>
<li>for: 本研究旨在探讨数据驱动学习中的假 correlate 问题，并提出一种基于 causal graph 的方法来解决这个问题。</li>
<li>methods: 本研究使用了一些常见的假 correlate 假设，并通过对标准 ERM 基elines 进行比较，检验这些假设对模型性能的影响。</li>
<li>results: 研究发现，在不同的假 correlate 假设下，模型的性能会受到不同程度的影响，并且可以通过对模型设计 Parameters 进行调整来改善模型性能。<details>
<summary>Abstract</summary>
Spurious correlations occur when a model learns unreliable features from the data and are a well-known drawback of data-driven learning. Although there are several algorithms proposed to mitigate it, we are yet to jointly derive the indicators of spurious correlations. As a result, the solutions built upon standalone hypotheses fail to beat simple ERM baselines. We collect some of the commonly studied hypotheses behind the occurrence of spurious correlations and investigate their influence on standard ERM baselines using synthetic datasets generated from causal graphs. Subsequently, we observe patterns connecting these hypotheses and model design choices.
</details>
<details>
<summary>摘要</summary>
伪 correlate 发生在模型从数据中学习不可靠特征时，这是数据驱动学习的一个知名缺陷。虽然有多种算法提出来 mitigate 其影响，但我们还没有共同 derivation 这些伪 correlate 的指标。因此，基于单独的假设建立的解决方案不能超过简单的 ERM 基elines。我们收集了一些常study 的假设下伪 correlate 的发生，并 investigate 这些假设对标准 ERM 基elines 的影响使用 synthetic 数据集。然后，我们发现了这些假设和模型设计选择之间的 Patterns。
</details></li>
</ul>
<hr>
<h2 id="Split-Learning-for-Distributed-Collaborative-Training-of-Deep-Learning-Models-in-Health-Informatics"><a href="#Split-Learning-for-Distributed-Collaborative-Training-of-Deep-Learning-Models-in-Health-Informatics" class="headerlink" title="Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics"></a>Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11027">http://arxiv.org/abs/2308.11027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuohang Li, Chao Yan, Xinmeng Zhang, Gharib Gharibi, Zhijun Yin, Xiaoqian Jiang, Bradley A. Malin</li>
<li>for: 本研究旨在Addressing the challenge of training deep learning models across healthcare organizations with privacy requirements.</li>
<li>methods: 该研究提出了一种新的隐私保护分布式学习框架，可以在私有保持原始数据和模型参数的情况下进行分布式学习。</li>
<li>results: 研究表明，通过使用分布式学习，可以实现高度相似的性能与中央化和联合学习，同时提高计算效率和降低隐私风险。<details>
<summary>Abstract</summary>
Deep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical prediction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging. This is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address this problem, we illustrate how split learning can enable collaborative training of deep learning models across disparate and privately maintained health datasets, while keeping the original records and model parameters private. We introduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to conventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to show that deep learning models trained via split learning can achieve highly similar performance to their centralized and federated counterparts while greatly improving computational efficiency and reducing privacy risks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Extreme-Multilabel-Classification-for-Specialist-Doctor-Recommendation-with-Implicit-Feedback-and-Limited-Patient-Metadata"><a href="#Extreme-Multilabel-Classification-for-Specialist-Doctor-Recommendation-with-Implicit-Feedback-and-Limited-Patient-Metadata" class="headerlink" title="Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata"></a>Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11022">http://arxiv.org/abs/2308.11022</a></li>
<li>repo_url: None</li>
<li>paper_authors: Filipa Valdeira, Stevo Racković, Valeria Danalachi, Qiwei Han, Cláudia Soares</li>
<li>for: 预测医生 Referral 建议，包括新患者和已有咨询历史</li>
<li>methods: 使用 Extreme Multilabel Classification (XML) 编码可用特征，并 explore 不同的场景</li>
<li>results: 比对标准推荐 metric ，our approach 在有咨询历史的患者群体中提高推荐精度大约 $10%$，并在新患者群体中提高 recall 纪录In English, this means:</li>
<li>for: Predicting doctor referrals, including new patients and those with a consultation history</li>
<li>methods: Using Extreme Multilabel Classification (XML) to encode available features and explore different scenarios</li>
<li>results: Comparing standard recommendation metrics, our approach consistently improves recommendation accuracy by approximately $10%$ for patients with a previous consultation history, and outperforms the benchmark in favorable scenarios for new patients, with a focus on recall metrics.<details>
<summary>Abstract</summary>
Recommendation Systems (RS) are often used to address the issue of medical doctor referrals. However, these systems require access to patient feedback and medical records, which may not always be available in real-world scenarios. Our research focuses on medical referrals and aims to predict recommendations in different specialties of physicians for both new patients and those with a consultation history. We use Extreme Multilabel Classification (XML), commonly employed in text-based classification tasks, to encode available features and explore different scenarios. While its potential for recommendation tasks has often been suggested, this has not been thoroughly explored in the literature. Motivated by the doctor referral case, we show how to recast a traditional recommender setting into a multilabel classification problem that current XML methods can solve. Further, we propose a unified model leveraging patient history across different specialties. Compared to state-of-the-art RS using the same features, our approach consistently improves standard recommendation metrics up to approximately $10\%$ for patients with a previous consultation history. For new patients, XML proves better at exploiting available features, outperforming the benchmark in favorable scenarios, with particular emphasis on recall metrics. Thus, our approach brings us one step closer to creating more effective and personalized doctor referral systems. Additionally, it highlights XML as a promising alternative to current hybrid or content-based RS, while identifying key aspects to take into account when using XML for recommendation tasks.
</details>
<details>
<summary>摘要</summary>
医疗专家推荐系统（RS）经常用于解决医疗专家推荐问题。然而，这些系统可能需要患者反馈和医疗记录，这些资料在实际场景中可能不常可用。我们的研究关注医疗推荐，并且想要预测不同专业医生的推荐建议，包括新患者和已经有咨询历史的患者。我们使用极端多类标签分类（XML），通常用于文本分类任务，来编码可用的特征并explore不同的场景。虽然XML在推荐任务中的潜力尚未得到了足够的研究，但我们在医疗推荐中提出了一种新的方法。我们的方法可以将传统的推荐设定转换成多类标签分类问题，使得现有的XML方法可以解决。此外，我们提议一种综合模型，利用患者历史跨专业。相比于使用同样的特征的状态体系，我们的方法在患者历史存在时可以保持标准推荐指标的提高，达到约10%。在新患者情况下，XML较好地利用可用的特征，超越了标准。特别是，我们的方法在反报率指标方面具有明显的优势。因此，我们的方法可以为创建更有效和个性化的医疗专家推荐系统做出一步进展。此外，它也证明了XML作为推荐任务的一种有前途的alternative，而且标识了在使用XML时需要注意的关键方面。
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Hypergraphs-for-Semi-supervised-Learning-using-Earth-Observations"><a href="#Multi-Task-Hypergraphs-for-Semi-supervised-Learning-using-Earth-Observations" class="headerlink" title="Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations"></a>Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11021">http://arxiv.org/abs/2308.11021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mihai Pirvu, Alina Marcu, Alexandra Dobrescu, Nabil Belbachir, Marius Leordeanu</li>
<li>for: 这个论文的目的是为了解决EARTH OBSERVATION问题，这个问题是多任务的并且常常缺少地面真实数据。</li>
<li>methods: 这篇论文提出了一种多任务希пер图模型，其中每个节点是一个任务，不同的路径通过希пер图达到某个任务都成为了无监督教师，这些教师 ensemble 学习生成可靠的假标签 для该任务。每个希PERedge 是一个任务的教师和学生，它也是一个自动матичеSK hypergraph 系统的学生。</li>
<li>results: 通过对NASA NEO Dataset进行广泛的实验，证明了我们的多任务半监督方法的价值， consistently 超过了强基eline和最新的工作。此外，我们还证明了希PERgraph 可以适应无监督数据分布变化，并可靠地恢复多个观测层的缺失数据，超过了7年。<details>
<summary>Abstract</summary>
There are many ways of interpreting the world and they are highly interdependent. We exploit such complex dependencies and introduce a powerful multi-task hypergraph, in which every node is a task and different paths through the hypergraph reaching a given task become unsupervised teachers, by forming ensembles that learn to generate reliable pseudolabels for that task. Each hyperedge is part of an ensemble teacher for a given task and it is also a student of the self-supervised hypergraph system. We apply our model to one of the most important problems of our times, that of Earth Observation, which is highly multi-task and it often suffers from missing ground-truth data. By performing extensive experiments on the NASA NEO Dataset, spanning a period of 22 years, we demonstrate the value of our multi-task semi-supervised approach, by consistent improvements over strong baselines and recent work. We also show that the hypergraph can adapt unsupervised to gradual data distribution shifts and reliably recover, through its multi-task self-supervision process, the missing data for several observational layers for up to seven years.
</details>
<details>
<summary>摘要</summary>
世界的解释有多种方法，它们之间具有高度的互相依赖关系。我们利用这些复杂的依赖关系，引入一个强大的多任务跨граф，其中每个节点是一个任务，不同的路径通过跨граф到达给定任务都变成了无监督教师。每个跨边都是一个任务的ensemble教师，同时也是自动编程跨 graf系统的学生。我们应用我们的模型到现代时代最重要的问题之一——地球观测，这是一个高度多任务的问题， часто会 missing ground-truth data。通过对 NASA NEO 数据集进行了22年的广泛实验，我们展示了我们的多任务半监督方法的价值，通过不断超过强大的基准和最新的研究成果。我们还表明了跨 graf 可以适应无监督的数据分布变化，并可靠地恢复多个观测层的缺失数据，达7年之久。
</details></li>
</ul>
<hr>
<h2 id="Instance-based-Learning-with-Prototype-Reduction-for-Real-Time-Proportional-Myocontrol-A-Randomized-User-Study-Demonstrating-Accuracy-preserving-Data-Reduction-for-Prosthetic-Embedded-Systems"><a href="#Instance-based-Learning-with-Prototype-Reduction-for-Real-Time-Proportional-Myocontrol-A-Randomized-User-Study-Demonstrating-Accuracy-preserving-Data-Reduction-for-Prosthetic-Embedded-Systems" class="headerlink" title="Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems"></a>Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11019">http://arxiv.org/abs/2308.11019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tim Sziburis, Markus Nowak, Davide Brunelli</li>
<li>for: 这篇论文探讨了基于kNN的手势识别方法，用于辅助控制 prosthesis。</li>
<li>methods: 方法包括实现kNN scheme，并评估不同的实现方式和参数化方法，以提高运算效率和可靠性。</li>
<li>results: 结果显示，使用kNN scheme和Decision Surface Mapping（DSM）可以实现高精度的手势识别，并且在实时运算中保持高速度和可靠性。<details>
<summary>Abstract</summary>
This work presents the design, implementation and validation of learning techniques based on the kNN scheme for gesture detection in prosthetic control. To cope with high computational demands in instance-based prediction, methods of dataset reduction are evaluated considering real-time determinism to allow for the reliable integration into battery-powered portable devices. The influence of parameterization and varying proportionality schemes is analyzed, utilizing an eight-channel-sEMG armband. Besides offline cross-validation accuracy, success rates in real-time pilot experiments (online target achievement tests) are determined. Based on the assessment of specific dataset reduction techniques' adequacy for embedded control applications regarding accuracy and timing behaviour, Decision Surface Mapping (DSM) proves itself promising when applying kNN on the reduced set. A randomized, double-blind user study was conducted to evaluate the respective methods (kNN and kNN with DSM-reduction) against Ridge Regression (RR) and RR with Random Fourier Features (RR-RFF). The kNN-based methods performed significantly better (p<0.0005) than the regression techniques. Between DSM-kNN and kNN, there was no statistically significant difference (significance level 0.05). This is remarkable in consideration of only one sample per class in the reduced set, thus yielding a reduction rate of over 99% while preserving success rate. The same behaviour could be confirmed in an extended user study. With k=1, which turned out to be an excellent choice, the runtime complexity of both kNN (in every prediction step) as well as DSM-kNN (in the training phase) becomes linear concerning the number of original samples, favouring dependable wearable prosthesis applications.
</details>
<details>
<summary>摘要</summary>
这个工作介绍了基于kNN算法的手势检测技术的设计、实现和验证。为了应对实例基本预测的高计算需求，dataset减少方法被评估，包括实时决定性以便可靠地 интеGRATE到电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池电池
</details></li>
</ul>
<hr>
<h2 id="Personalized-Event-Prediction-for-Electronic-Health-Records"><a href="#Personalized-Event-Prediction-for-Electronic-Health-Records" class="headerlink" title="Personalized Event Prediction for Electronic Health Records"></a>Personalized Event Prediction for Electronic Health Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11013">http://arxiv.org/abs/2308.11013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeong Min Lee, Milos Hauskrecht</li>
<li>for: 这篇论文的目的是为了开发一些准确的预测模型，以支持不同的医疗执行和病例分析，并且提高病人照顾质量。</li>
<li>methods: 这篇论文使用了多种新的事件序列预测模型和方法，包括人群内部模型修饰、自适应和元件水平模型转换，以更好地适应个别病人和其特定情况。</li>
<li>results: 这篇论文的结果显示，这些新的预测模型和方法可以对医疗纪录中的事件序列进行更准确的预测，并且可以适应不同的病人和其特定情况。<details>
<summary>Abstract</summary>
Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for individual patients and their specific conditions. The methods developed in this work pursue refinement of population-wide models to subpopulations, self-adaptation, and a meta-level model switching that is able to adaptively select the model with the best chance to support the immediate prediction. We analyze and test the performance of these models on clinical event sequences of patients in MIMIC-III database.
</details>
<details>
<summary>摘要</summary>
临床事件序列包含数百个临床事件记录，表示患者的时间序列记录。开发准确预测临床序列模型是非常重要的，以支持评估当前患者状况、分类临床事件类型和预测不良临床结果等，以提高患者治疗。一个重要的预测临床序列模型挑战是每个患者的序列可能具有不同的临床事件集（观察结果、实验室测试结果、药物和处理），因此通用于许多患者的人口级别模型可能无法准确预测每个患者的特定状况和差异。为解决这个问题，我们提出并研究了多种新的临床序列预测模型和方法，使得我们可以更好地适应每个患者和其特定状况。我们在MIMIC-III数据库中分析和测试了这些模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Using-language-models-in-the-implicit-automated-assessment-of-mathematical-short-answer-items"><a href="#Using-language-models-in-the-implicit-automated-assessment-of-mathematical-short-answer-items" class="headerlink" title="Using language models in the implicit automated assessment of mathematical short answer items"></a>Using language models in the implicit automated assessment of mathematical short answer items</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11006">http://arxiv.org/abs/2308.11006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Ormerod</li>
<li>for: 这个论文旨在提出一种新的短 constructed response 评估方法，用于评估学生在数学题目上的答案。</li>
<li>methods: 该方法使用一个管道来标识学生的答案中关键的值。该管道包括两个精心定制的自然语言模型，第一个模型判断答案中是否含有关键值，第二个模型则确定答案中关键值的位置。</li>
<li>results: 相比传统的分页式评估方法，该管道可以更准确地评估短 constructed response，并且可以提供更有arget的反馈给学生和教师，帮助学生提高数学理解。<details>
<summary>Abstract</summary>
We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which can help them improve their understanding of mathematics.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法来评估某些短 constructed responses 的数学项目。我们的方法使用一个管道，从student response中提取关键值。这使得我们可以确定答案是否正确，以及学生是否存在误会。管道中的信息可以用于给教师和学生提供反馈。我们的值标识管道包括两个精度调整的自然语言模型。第一个模型判断学生答案中是否存在某个值。第二个模型则确定答案中关键值的位置。我们考虑了一个通用的模型，可以用于任何提问和值，以及每个提问和值的特定模型。值标识管道比传统基于满分表格的评分方法更准确和有用，可以为学生提供更加精准的反馈，从而帮助学生提高数学理解。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Detection-of-Methane-Emissions-in-Multispectral-Satellite-Data-Using-Deep-Learning"><a href="#Autonomous-Detection-of-Methane-Emissions-in-Multispectral-Satellite-Data-Using-Deep-Learning" class="headerlink" title="Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning"></a>Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11003">http://arxiv.org/abs/2308.11003</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bertrand Rouet-Leduc, Thomas Kerdreux, Alexandre Tuel, Claudia Hulbert</li>
<li>For: The paper aims to develop a method for automatically detecting methane leaks in satellite multispectral data using deep learning techniques.* Methods: The paper uses Sentinel-2 satellite multispectral data and leverages the image recognition capabilities of deep learning methods to detect methane leaks.* Results: The paper shows that the proposed approach can dramatically reduce false positive rates compared to state-of-the-art multispectral methane data products and can automatically detect methane leaks without the need for a priori knowledge of potential leak sites.Here’s the same information in Simplified Chinese:* For: 该文章目的是开发一种自动检测卫星多spectral数据中的甲烷泄漏的方法，使用深度学习技术。* Methods: 文章使用Sentinel-2卫星多spectral数据，利用深度学习方法对图像进行识别，检测甲烷泄漏。* Results: 文章显示，提议的方法可以减少false positive率，比state-of-the-art多spectral甲烷数据产品更加精准，无需先知道泄漏点的位置。<details>
<summary>Abstract</summary>
Methane is one of the most potent greenhouse gases, and its short atmospheric half-life makes it a prime target to rapidly curb global warming. However, current methane emission monitoring techniques primarily rely on approximate emission factors or self-reporting, which have been shown to often dramatically underestimate emissions. Although initially designed to monitor surface properties, satellite multispectral data has recently emerged as a powerful method to analyze atmospheric content. However, the spectral resolution of multispectral instruments is poor, and methane measurements are typically very noisy. Methane data products are also sensitive to absorption by the surface and other atmospheric gases (water vapor in particular) and therefore provide noisy maps of potential methane plumes, that typically require extensive human analysis. Here, we show that the image recognition capabilities of deep learning methods can be leveraged to automatize the detection of methane leaks in Sentinel-2 satellite multispectral data, with dramatically reduced false positive rates compared with state-of-the-art multispectral methane data products, and without the need for a priori knowledge of potential leak sites. Our proposed approach paves the way for the automated, high-definition and high-frequency monitoring of point-source methane emissions across the world.
</details>
<details>
<summary>摘要</summary>
孕气是最强烈的绿色气体之一，它的大气半衰期短，使其成为缓解全球变暖的 prime target。然而，现有的甲烷排放监测技术主要基于估算的排放因子或自我报告，这些估算通常会很大幅度地下估排放。虽然初始设计用于监测地面属性，但卫星多spectral数据最近在气象Content的分析方面 emerged as a powerful tool。然而，多spectral instrument的 spectral resolution poor, and methane measurements are typically very noisy. methane data products are also sensitive to absorption by the surface and other atmospheric gases (water vapor in particular) and therefore provide noisy maps of potential methane plumes, that typically require extensive human analysis。在这里，我们展示了深度学习方法的图像识别能力可以用来自动检测Sentinel-2卫星多spectral数据中的甲烷泄漏，与现有的多spectral甲烷数据产品相比， false positive rate 下降了多少，而无需对潜在泄漏点的 prior knowledge。我们的提议方法开 up the possibility of automated, high-definition and high-frequency monitoring of point-source methane emissions across the world。
</details></li>
</ul>
<hr>
<h2 id="SupEuclid-Extremely-Simple-High-Quality-OoD-Detection-with-Supervised-Contrastive-Learning-and-Euclidean-Distance"><a href="#SupEuclid-Extremely-Simple-High-Quality-OoD-Detection-with-Supervised-Contrastive-Learning-and-Euclidean-Distance" class="headerlink" title="SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance"></a>SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10973">http://arxiv.org/abs/2308.10973</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jarrod Haas</li>
<li>for: 本研究旨在提出一种非常简单的方法来检测数据集外的异常（Out-of-Distribution，OoD）样本。</li>
<li>methods: 本研究使用了Supervised Contrastive Learning（SCL）方法将ResNet18模型训练，并使用欧几丁度评分函数来评估OoD样本。</li>
<li>results: 研究发现，使用SCL方法训练ResNet18模型，可以直接使用Euclidean distance评分函数来检测OoD样本，并且可以达到或超过许多现有的州态艺法和大型模型的效果。<details>
<summary>Abstract</summary>
Out-of-Distribution (OoD) detection has developed substantially in the past few years, with available methods approaching, and in a few cases achieving, perfect data separation on standard benchmarks. These results generally involve large or complex models, pretraining, exposure to OoD examples or extra hyperparameter tuning. Remarkably, it is possible to achieve results that can exceed many of these state-of-the-art methods with a very simple method. We demonstrate that ResNet18 trained with Supervised Contrastive Learning (SCL) produces state-of-the-art results out-of-the-box on near and far OoD detection benchmarks using only Euclidean distance as a scoring rule. This may obviate the need in some cases for more sophisticated methods or larger models, and at the very least provides a very strong, easy to use baseline for further experimentation and analysis.
</details>
<details>
<summary>摘要</summary>
OUT-OF-DISTRIBUTION（OOD）检测在最近几年内发展了很大，现有的方法已经几乎完美地分离了标准 benchmark 上的数据。这些结果通常需要使用大型或复杂的模型，预训练，对 OOD 示例进行暴露或额外的 гиперпараметр调整。很 Remarkably，我们发现可以使用非常简单的方法来超越许多state-of-the-art方法。我们示出，使用 Supervised Contrastive Learning（SCL）训练的 ResNet18 在靠近和远 OOD 检测 benchmark 上达到了 state-of-the-art 结果，只使用 euclidian distance 作为分数规则。这可能将一些情况下取代更复杂的方法或更大的模型，而且最少提供了一个非常强大、易于使用的基准， для 进一步的实验和分析。
</details></li>
</ul>
<hr>
<h2 id="Fat-Shattering-Joint-Measurability-and-PAC-Learnability-of-POVM-Hypothesis-Classes"><a href="#Fat-Shattering-Joint-Measurability-and-PAC-Learnability-of-POVM-Hypothesis-Classes" class="headerlink" title="Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes"></a>Fat Shattering, Joint Measurability, and PAC Learnability of POVM Hypothesis Classes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12304">http://arxiv.org/abs/2308.12304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abram Magner, Arun Padakandla</li>
<li>for: 本文研究了量子测量类的学习可能性，并提出了匹配的必要和 suficient conditions，以及相应的样本复杂度下界。</li>
<li>methods: 本文使用了Empirical Risk Minimization（ERM）和denoised ERM来学习量子测量类，并提出了新的学习规则——denoised ERM，以及其uniform convergence的condition。</li>
<li>results: 本文证明了POVM和概率观测类是PAC可学习的，并给出了量子测量类的样本复杂度下界。此外，本文还证明了所有在finite-dimensional Hilbert space上定义的测量类都是PAC可学习的。<details>
<summary>Abstract</summary>
We characterize learnability for quantum measurement classes by establishing matching necessary and sufficient conditions for their PAC learnability, along with corresponding sample complexity bounds, in the setting where the learner is given access only to prepared quantum states. We first probe the results from previous works on this setting. We show that the empirical risk defined in previous works and matching the definition in the classical theory fails to satisfy the uniform convergence property enjoyed in the classical setting for some learnable classes. Moreover, we show that VC dimension generalization upper bounds in previous work are frequently infinite, even for finite-dimensional POVM classes. To surmount the failure of the standard ERM to satisfy uniform convergence, we define a new learning rule -- denoised ERM. We show this to be a universal learning rule for POVM and probabilistically observed concept classes, and the condition for it to satisfy uniform convergence is finite fat shattering dimension of the class. We give quantitative sample complexity upper and lower bounds for learnability in terms of finite fat-shattering dimension and a notion of approximate finite partitionability into approximately jointly measurable subsets, which allow for sample reuse. We then show that finite fat shattering dimension implies finite coverability by approximately jointly measurable subsets, leading to our matching conditions. We also show that every measurement class defined on a finite-dimensional Hilbert space is PAC learnable. We illustrate our results on several example POVM classes.
</details>
<details>
<summary>摘要</summary>
我们 caracteriza 学习可能性 для量子测量类型 by 确定匹配的必需和充分条件，以及相应的样本复杂性 bound，在learner 被给定只有准备好的量子状态的设置下。我们首先探究 previous works 中的结果。我们显示了empirical risk 定义在 previous works 中并不满足 classical setting 中的各种learnable classes 上的 uniform convergence property。此外，我们还显示了 previous work 中的VC dimension 泛化Upper bound  часто是无限大，即使是 finite-dimensional POVM 类型。为了缺乏标准 ERM 满足 uniform convergence，我们定义了一种新的学习规则 -- denoised ERM。我们显示这是一种universal learning rule  для POVM 和 probabilistically observed concept classes，并且 condition  для它满足 uniform convergence 是类型的 finite fat shattering dimension。我们给出了量化样本复杂性 upper 和 lower bound ，用于描述learnability 的conditions。我们然后显示了 finite fat shattering dimension implies finite coverability by approximately jointly measurable subsets，导致我们的matching conditions。最后，我们还显示了 every measurement class defined on a finite-dimensional Hilbert space 是 PAC learnable。我们在 several example POVM classes 中应用我们的结果。
</details></li>
</ul>
<hr>
<h2 id="MRI-Field-transfer-Reconstruction-with-Limited-Data-Regularization-by-Neural-Style-Transfer"><a href="#MRI-Field-transfer-Reconstruction-with-Limited-Data-Regularization-by-Neural-Style-Transfer" class="headerlink" title="MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer"></a>MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10968">http://arxiv.org/abs/2308.10968</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyao Shen, Yancheng Zhu, Hernan Jara, Sean B. Andersson, Chad W. Farris, Stephan Anderson, Xin Zhang</li>
<li>for: 这 paper 的目的是提出一种基于深度学习的 MRI 重建方法，以提高 MRI 的图像质量。</li>
<li>methods: 这 paper 使用了 regularization by denoising (RED) 和 regularization by neural style transfer (RNST) 两种方法，将它们组合使用以实现高质量的 MRI 重建。</li>
<li>results: 研究发现，使用 RNST 方法可以在不同的图像风格和有限数据情况下，从噪声低质量图像中重建出高质量图像。这些结果表明 RNST 框架在 MRI 重建中具有广泛的应用前景和可能性。<details>
<summary>Abstract</summary>
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
</details>
<details>
<summary>摘要</summary>
近期研究表明，使用深度学习模型进行MRI重建有成功经验。然而，大多数报道的方法需要对特定任务的大规模数据进行训练。抑制权（RED）是一个通用管道，它嵌入了一个抑制器作为图像重建的先验。抑制权的潜力已经在多种图像相关任务中展现出来，如降噪、去锈和超解等。在这种工作中，我们提议一种基于神经传输和抑制器的常规化（RNST）方法，以利用神经传输和抑制器的先验来重建高质量图像。我们验证了RNST方法使用临床MRI扫描数据，并显示RNST可以在噪声低质量图像中重建高质量图像，并且可以在有限数据情况下进行重建。我们的结果显示RNST框架在MRI重建中具有优秀的能力，并且有潜力应用于有限数据的重建任务。
</details></li>
</ul>
<hr>
<h2 id="Structured-World-Models-from-Human-Videos"><a href="#Structured-World-Models-from-Human-Videos" class="headerlink" title="Structured World Models from Human Videos"></a>Structured World Models from Human Videos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10901">http://arxiv.org/abs/2308.10901</a></li>
<li>repo_url: None</li>
<li>paper_authors: Russell Mendonca, Shikhar Bahl, Deepak Pathak</li>
<li>for: 本研究旨在训练机器人通过互联网大规模人类视频数据来快速学习复杂的 manipulate 技能。</li>
<li>methods: 本研究使用了人类视频数据来构建一个结构化的人类行为空间，并在这个空间中学习视觉可行性。然后，通过人类视频数据进行世界模型训练，并在小量机器人互动数据上进行细化调整。</li>
<li>results: 研究发现，通过这种方法，不同的机器人可以在复杂的设置下快速学习 manipulate 技能，仅需在30分钟内进行互动。视频可以在<a target="_blank" rel="noopener" href="https://human-world-model.github.io找到./">https://human-world-model.github.io找到。</a><details>
<summary>Abstract</summary>
We tackle the problem of learning complex, general behaviors directly in the real world. We propose an approach for robots to efficiently learn manipulation skills using only a handful of real-world interaction trajectories from many different settings. Inspired by the success of learning from large-scale datasets in the fields of computer vision and natural language, our belief is that in order to efficiently learn, a robot must be able to leverage internet-scale, human video data. Humans interact with the world in many interesting ways, which can allow a robot to not only build an understanding of useful actions and affordances but also how these actions affect the world for manipulation. Our approach builds a structured, human-centric action space grounded in visual affordances learned from human videos. Further, we train a world model on human videos and fine-tune on a small amount of robot interaction data without any task supervision. We show that this approach of affordance-space world models enables different robots to learn various manipulation skills in complex settings, in under 30 minutes of interaction. Videos can be found at https://human-world-model.github.io
</details>
<details>
<summary>摘要</summary>
我们面对现实世界中学习复杂、通用行为的问题。我们提议一种方法，让机器人通过几个不同设定下的实际互动路径学习摄取技能。我们受到电视频道和自然语言领域中大规模数据集的成功所激发，我们信服机器人可以通过互联网规模的人类视频数据来快速学习。人类在世界中进行许多有趣的互动，这可以让机器人不仅建立用Actions和可用性的理解，也可以了解如何这些Actions影响世界来进行摄取。我们的方法建立了基于视觉可用性的结构化人类行为空间，然后将人类视频中的世界模型训练，并在小量机器人互动数据上精致调整。我们显示这种价值空间世界模型可以让不同的机器人在复杂的设定下学习不同的摄取技能，仅需30分钟的互动。视频可以在https://human-world-model.github.io获取。
</details></li>
</ul>
<hr>
<h2 id="Unlocking-Accuracy-and-Fairness-in-Differentially-Private-Image-Classification"><a href="#Unlocking-Accuracy-and-Fairness-in-Differentially-Private-Image-Classification" class="headerlink" title="Unlocking Accuracy and Fairness in Differentially Private Image Classification"></a>Unlocking Accuracy and Fairness in Differentially Private Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10888">http://arxiv.org/abs/2308.10888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonard Berrada, Soham De, Judy Hanwen Shen, Jamie Hayes, Robert Stanforth, David Stutz, Pushmeet Kohli, Samuel L. Smith, Borja Balle</li>
<li>for:  trains models on private data without leaking sensitive information</li>
<li>methods:  uses differential privacy (DP) framework, fine-tunes pre-trained foundation models with DP</li>
<li>results: achieves similar accuracy to non-private classifiers, with private accuracies within a few percent of the non-private state of the art, and without larger performance disparities across demographic groups.<details>
<summary>Abstract</summary>
Privacy-preserving machine learning aims to train models on private data without leaking sensitive information. Differential privacy (DP) is considered the gold standard framework for privacy-preserving training, as it provides formal privacy guarantees. However, compared to their non-private counterparts, models trained with DP often have significantly reduced accuracy. Private classifiers are also believed to exhibit larger performance disparities across subpopulations, raising fairness concerns. The poor performance of classifiers trained with DP has prevented the widespread adoption of privacy preserving machine learning in industry. Here we show that pre-trained foundation models fine-tuned with DP can achieve similar accuracy to non-private classifiers, even in the presence of significant distribution shifts between pre-training data and downstream tasks. We achieve private accuracies within a few percent of the non-private state of the art across four datasets, including two medical imaging benchmarks. Furthermore, our private medical classifiers do not exhibit larger performance disparities across demographic groups than non-private models. This milestone to make DP training a practical and reliable technology has the potential to widely enable machine learning practitioners to train safely on sensitive datasets while protecting individuals' privacy.
</details>
<details>
<summary>摘要</summary>
“隐私保护机器学习” aim to 训练模型在私人数据上无泄露敏感信息。“数据隐私”（DP）被视为机器学习隐私保护的金标准框架，但与非私人模型相比，DP 训练的模型通常会有很大的准确性下降。私人分类器也被认为会在不同的人口集中Displaying 更大的性差，引起公平性的关注。DP 训练的模型表现不佳的问题阻碍了机器学习领域广泛采用隐私保护技术。我们在这篇文章中显示，将预训练的基础模型 fine-tune  avec DP 可以 дости到与非私人模型相似的准确性，甚至在数据分布差异很大的情况下。我们在四个数据集上 achieve private accuracy 与非私人模型相似，包括两个医疗影像标准 benchmark。此外，我们的私人医疗分类器不会在不同的民族群体中表现出更大的性差，与非私人模型相比。这个突破可以让机器学习实践者在敏感数据上安全地训练模型，保护个人隐私。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space"><a href="#Analyzing-Transformer-Dynamics-as-Movement-through-Embedding-Space" class="headerlink" title="Analyzing Transformer Dynamics as Movement through Embedding Space"></a>Analyzing Transformer Dynamics as Movement through Embedding Space</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10874">http://arxiv.org/abs/2308.10874</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sumeet S. Singh</li>
<li>for: 本研究探讨了 transformer 语言模型的基本机制如何导致智能行为的出现。</li>
<li>methods: 我们采用系统方法分析 transformer 的具体机制，并开发了一个数学框架来解释它们的动态。这种新的视角提供了一个原则性的方法来思考问题，并且显示出了智能行为的出现的重要意义。</li>
<li>results: 研究发现，transformer 的核心是一个嵌入空间漫步者，将智能行为映射到嵌入空间中的路径上。每步漫步都会将上下文组合成一个单一的 composite vector，该 vector 的位置定义下一步的路径。无需学习，decode 只是一种嵌入空间中的漫步。知识、智能和技能都是嵌入空间中 vectors 的组织方式，而不是特定神经元或层。关注的贡献是对 vector 的组合Association-bias，但需要更多的研究来证明其重要性。transformer 的整体结构由两种基本操作组成：数据独立的滤波和数据依赖的聚合。这种一致性将 transformer 与其他序列模型和模式联系起来。<details>
<summary>Abstract</summary>
Transformer language models exhibit intelligent behaviors such as understanding natural language, recognizing patterns, acquiring knowledge, reasoning, planning, reflecting and using tools. This paper explores how their underlying mechanics give rise to intelligent behaviors. We adopt a systems approach to analyze Transformers in detail and develop a mathematical framework that frames their dynamics as movement through embedding space. This novel perspective provides a principled way of thinking about the problem and reveals important insights related to the emergence of intelligence:   1. At its core the Transformer is a Embedding Space walker, mapping intelligent behavior to trajectories in this vector space.   2. At each step of the walk, it composes context into a single composite vector whose location in Embedding Space defines the next step.   3. No learning actually occurs during decoding; in-context learning and generalization are simply the result of different contexts composing into different vectors.   4. Ultimately the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in Embedding Space rather than in specific neurons or layers. These abilities are properties of this organization.   5. Attention's contribution boils down to the association-bias it lends to vector composition and which influences the aforementioned organization. However, more investigation is needed to ascertain its significance.   6. The entire model is composed from two principal operations: data independent filtering and data dependent aggregation. This generalization unifies Transformers with other sequence models and across modalities.   Building upon this foundation we formalize and test a semantic space theory which posits that embedding vectors represent semantic concepts and find some evidence of its validity.
</details>
<details>
<summary>摘要</summary>
transformer 语言模型会示出智能行为，如理解自然语言、识别模式、获取知识、理解、规划和使用工具。这篇研究探索了 transformer 底下的机制如何导致智能行为。我们采用系统方法来分析 transformer 的细节，开发了一个数学框架，将 transformer 的动态视为嵌入空间中的运动。这个新的观点提供了一个原理式的思考方式，并显示出智能行为的出现有重要的意义：1. transformer 的核心是嵌入空间漫步者，将智能行为映射到嵌入空间中的路径上。2. 在每次漫步过程中，它会将上下文转换为单一的合成vector，其位于嵌入空间中的位置定义下一步的方向。3. 在解oding过程中，不会有真正的学习 occuring; 内在学习和泛化都是由不同的上下文对vector的聚合所带来的。4.  ultimately, the knowledge, intelligence and skills exhibited by the model are embodied in the organization of vectors in embedding space rather than in specific neurons or layers. these abilities are properties of this organization.5. attention 的贡献可以理解为对 vector 的聚合中带来的协议偏好，这影响了 embedding space 中 vectors 的组织。但是，需要进一步的研究，以了解它的重要性。6. transformer 的整个模型由两个主要操作组成：data independent filtering和data dependent aggregation。这个一致性使 transformer 与其他序列模型以及不同的模式之间建立了联系。基于这个基础，我们正式化了一个 semantic space 理论，提出 embedding vectors 代表 semantics concepts，并发现了一些证据支持这个理论的正确性。
</details></li>
</ul>
<hr>
<h2 id="Majorana-Demonstrator-Data-Release-for-AI-ML-Applications"><a href="#Majorana-Demonstrator-Data-Release-for-AI-ML-Applications" class="headerlink" title="Majorana Demonstrator Data Release for AI&#x2F;ML Applications"></a>Majorana Demonstrator Data Release for AI&#x2F;ML Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10856">http://arxiv.org/abs/2308.10856</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. J. Arnquist, F. T. Avignone III, A. S. Barabash, C. J. Barton, K. H. Bhimani, E. Blalock, B. Bos, M. Busch, M. Buuck, T. S. Caldwell, Y. -D. Chan, C. D. Christofferson, P. -H. Chu, M. L. Clark, C. Cuesta, J. A. Detwiler, Yu. Efremenko, H. Ejiri, S. R. Elliott, N. Fuad, G. K. Giovanetti, M. P. Green, J. Gruszko, I. S. Guinn, V. E. Guiseppe, C. R. Haufe, R. Henning, D. Hervas Aguilar, E. W. Hoppe, A. Hostiuc, M. F. Kidd, I. Kim, R. T. Kouzes, T. E. Lannen V, A. Li, J. M. Lopez-Castano, R. D. Martin, R. Massarczyk, S. J. Meijer, S. Mertens, T. K. Oli, L. S. Paudel, W. Pettus, A. W. P. Poon, B. Quenallata, D. C. Radford, A. L. Reine, K. Rielage, N. W. Ruof, D. C. Schaper, S. J. Schleich, D. Tedeschi, R. L. Varner, S. Vasilyev, S. L. Watkins, J. F. Wilkerson, C. Wiseman, W. Xu, C. -H. Yu, B. X. Zhu</li>
<li>for: 本研究的目的是为了提供一个 Majorana 实验中的数据集，用于训练和测试人工智能和机器学习算法。</li>
<li>methods: 本研究使用了 Majorana 实验中的数据，包括 raw Germanium 探测器波形、激光形态分割cuts 和加工后的能量，共同存储在 HDF5 文件格式中，并附带相关的元数据。</li>
<li>results: 本研究提供了一个特定的数据集，用于支持人工智能和机器学习算法的训练和测试。<details>
<summary>Abstract</summary>
The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.edu (A. Li).
</details>
<details>
<summary>摘要</summary>
<<SYS>>将附件的数据发布包括 Majorana实验中的一个子集调整数据。每个 Majorana 事件都由 raw 氙元器波形、激发形分割 cuts 和调整后的能量， alles 共享在 HDF5 文件格式中，并附带相关的元数据。这个发布是为支持人工智能（AI）和机器学习（ML）算法的训练和测试而设计的。本文分为以下四部分：I. 数据集的内容和格式的概述II. 数据集的位置和访问方法III. NPML 机器学习挑战与这个数据集相关IV. Majorana 团队的免责声明 appendix A. 数据发布的技术详细信息请对这个发布中的内容向 liaobo77@ucsd.edu (A. Li) 提出问题。>>Note: "Majorana" in the text should be translated as "抗随机扰动器" (anti-random noise device) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="Evaluating-quantum-generative-models-via-imbalanced-data-classification-benchmarks"><a href="#Evaluating-quantum-generative-models-via-imbalanced-data-classification-benchmarks" class="headerlink" title="Evaluating quantum generative models via imbalanced data classification benchmarks"></a>Evaluating quantum generative models via imbalanced data classification benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10847">http://arxiv.org/abs/2308.10847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Graham R. Enos, Matthew J. Reagor, Eric Hulburd</li>
<li>for: 这个论文是为了研究量子机器学习模型的不同行为是否与传统模型不同，并在实际数据集上进行系统性的分析。</li>
<li>methods: 这个论文使用可解释人工智能技术来分析从混合量子-классификаル нейрон网络中生成的数据，并对这些数据进行比较与当前的 Mitigating 方法。</li>
<li>results: 研究发现，不同的数据集 exhibits varying degrees of complexity and class imbalance，并且可以使用 hybrid 量子-классификаル生成模型来分析这些数据。这些结果可以帮助我们理解哪些问题是可以使用 hybrid 模型解决，哪些问题则需要更多的研究。<details>
<summary>Abstract</summary>
A limited set of tools exist for assessing whether the behavior of quantum machine learning models diverges from conventional models, outside of abstract or theoretical settings. We present a systematic application of explainable artificial intelligence techniques to analyze synthetic data generated from a hybrid quantum-classical neural network adapted from twenty different real-world data sets, including solar flares, cardiac arrhythmia, and speech data. Each of these data sets exhibits varying degrees of complexity and class imbalance. We benchmark the quantum-generated data relative to state-of-the-art methods for mitigating class imbalance for associated classification tasks. We leverage this approach to elucidate the qualities of a problem that make it more or less likely to be amenable to a hybrid quantum-classical generative model.
</details>
<details>
<summary>摘要</summary>
有限的工具存在，用于判断量子机器学习模型与传统模型之间的行为异同，除了抽象或理论上的设置。我们将可追踪的人工智能技术系统化应用于生成从混合量子-классификаLL大 neural network中的 synthetic数据，该数据来自20个真实世界数据集，包括太阳风暴、心脏 arrhythmia 和语音数据。每个数据集都具有不同的复杂度和类别偏好。我们将量子生成的数据与当前的状态艺术方法进行比较，以mitigate class imbalance相关的分类任务。我们利用这种方法，以便描述一个问题的特点，使其更或 menos适合混合量子-классификаLL生成模型。
</details></li>
</ul>
<hr>
<h2 id="Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility"><a href="#Real-World-Time-Series-Benchmark-Datasets-with-Distribution-Shifts-Global-Crude-Oil-Price-and-Volatility" class="headerlink" title="Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility"></a>Real World Time Series Benchmark Datasets with Distribution Shifts: Global Crude Oil Price and Volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10846">http://arxiv.org/abs/2308.10846</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oilpricebenchmarks/COB">https://github.com/oilpricebenchmarks/COB</a></li>
<li>paper_authors: Pranay Pasula</li>
<li>for: 这个论文旨在提供一个适用于金融领域的时间序列benchmark，以促进持续学习的进步。</li>
<li>methods: 该论文使用了asset price数据的转换方法，并使用了期望最大化（EM）方法来适应模型。</li>
<li>results: 论文通过生成相关的任务标签，以及对四种持续学习算法的比较，证明了这些benchmark的有效性。<details>
<summary>Abstract</summary>
The scarcity of task-labeled time-series benchmarks in the financial domain hinders progress in continual learning. Addressing this deficit would foster innovation in this area. Therefore, we present COB, Crude Oil Benchmark datasets. COB includes 30 years of asset prices that exhibit significant distribution shifts and optimally generates corresponding task (i.e., regime) labels based on these distribution shifts for the three most important crude oils in the world. Our contributions include creating real-world benchmark datasets by transforming asset price data into volatility proxies, fitting models using expectation-maximization (EM), generating contextual task labels that align with real-world events, and providing these labels as well as the general algorithm to the public. We show that the inclusion of these task labels universally improves performance on four continual learning algorithms, some state-of-the-art, over multiple forecasting horizons. We hope these benchmarks accelerate research in handling distribution shifts in real-world data, especially due to the global importance of the assets considered. We've made the (1) raw price data, (2) task labels generated by our approach, (3) and code for our algorithm available at https://oilpricebenchmarks.github.io.
</details>
<details>
<summary>摘要</summary>
“财经领域内的任务标签时间序列库 scarcity 阻碍了无终学习的进步。解决这个不足，将促进这个领域的创新。因此，我们提出了 COB，即原油价格库存数据。COB 包含了30年的资产价格，其中具有明显的分布迁移，并且根据这些分布迁移生成对应的任务（即频率）标签。我们的贡献包括将资产价格数据转换为可用性干扰量表现，使用期望最大化（EM）运算进行模型适应，生成基于实际世界事件的任务标签，并且将这些标签以及通用的算法公开给社区。我们显示，将这些任务标签包含在内，可以在多个预测时间点上 universally 提高四种不断学习算法的性能，其中一些是现有的State-of-the-art。我们希望这些库可以促进实际数据中的分布迁移处理研究，尤其是由于考虑到资产考虑的全球重要性。我们将（1）原始价格数据、（2）生成的任务标签、以及（3）实现这种算法的代码公开在https://oilpricebenchmarks.github.io。”
</details></li>
</ul>
<hr>
<h2 id="Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection"><a href="#Neural-Networks-Optimizations-Against-Concept-and-Data-Drift-in-Malware-Detection" class="headerlink" title="Neural Networks Optimizations Against Concept and Data Drift in Malware Detection"></a>Neural Networks Optimizations Against Concept and Data Drift in Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10821">http://arxiv.org/abs/2308.10821</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Maillet, Benjamin Marais</li>
<li>for: 提高基eline神经网络对抗演化漏斗问题</li>
<li>methods: 提出了一种模型无关协议，包括特征减少和使用最新验证集训练，并提出了一种适应退化损失函数 named Drift-Resilient Binary Cross-Entropy</li>
<li>results: 对EMBER数据集（2018）进行训练，对2020-2023年间的恶意文件进行评估，提高了15.2%的恶意软件检测率<details>
<summary>Abstract</summary>
Despite the promising results of machine learning models in malware detection, they face the problem of concept drift due to malware constant evolution. This leads to a decline in performance over time, as the data distribution of the new files differs from the training one, requiring regular model update. In this work, we propose a model-agnostic protocol to improve a baseline neural network to handle with the drift problem. We show the importance of feature reduction and training with the most recent validation set possible, and propose a loss function named Drift-Resilient Binary Cross-Entropy, an improvement to the classical Binary Cross-Entropy more effective against drift. We train our model on the EMBER dataset (2018) and evaluate it on a dataset of recent malicious files, collected between 2020 and 2023. Our improved model shows promising results, detecting 15.2% more malware than a baseline model.
</details>
<details>
<summary>摘要</summary>
尽管机器学习模型在恶意软件检测方面表现良好，但它们面临着概念漂移问题，即恶意软件不断演化导致模型性能逐渐下降。这是因为新的文件数据分布与训练数据分布不同，需要定期更新模型。在这种情况下，我们提出了一种模型无关协议，用于改进基eline neural network，以适应概念漂移问题。我们表明了特征减少和使用最新的验证集进行训练的重要性，并提出了一种名为漂移抗耐 binary cross-entropy 的损失函数，它在对漂移问题更有效。我们使用 EMBER 数据集（2018）进行训练，并对2020-2023年间收集的恶意文件进行评估。我们改进后的模型表现出色，可以检测到基eline模型无法检测到的15.2%更多的恶意软件。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.LG_2023_08_22/" data-id="clm0t8e0k007ev788c971608s" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_22" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/22/cs.SD_2023_08_22/" class="article-date">
  <time datetime="2023-08-21T16:00:00.000Z" itemprop="datePublished">2023-08-22</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/22/cs.SD_2023_08_22/">cs.SD - 2023-08-22 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Furnishing-Sound-Event-Detection-with-Language-Model-Abilities"><a href="#Furnishing-Sound-Event-Detection-with-Language-Model-Abilities" class="headerlink" title="Furnishing Sound Event Detection with Language Model Abilities"></a>Furnishing Sound Event Detection with Language Model Abilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11530">http://arxiv.org/abs/2308.11530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hualei Wang, Jianguo Mao, Zhifang Guo, Jiarui Wan, Hong Liu, Xiangdong Wang</li>
<li>for: 本文further探讨语言模型（LMs）在视觉交互中的能力，特别是声事件检测（SED）。</li>
<li>methods: 提议一种精炼的方法，将音频特征和文本特征进行对应，以实现声事件分类和时间位置检测。该框架包括声学编码器、对应模块和解 Coupled语言解码器，从 audio 特征中生成时间和事件序列。与传统方法相比，我们的模型更简洁和全面，因为语言模型直接利用其语义能力来生成序列。</li>
<li>results: 研究表明，提议的方法可以准确地检测声事件。<details>
<summary>Abstract</summary>
Recently, the ability of language models (LMs) has attracted increasing attention in visual cross-modality. In this paper, we further explore the generation capacity of LMs for sound event detection (SED), beyond the visual domain. Specifically, we propose an elegant method that aligns audio features and text features to accomplish sound event classification and temporal location. The framework consists of an acoustic encoder, a contrastive module that align the corresponding representations of the text and audio, and a decoupled language decoder that generates temporal and event sequences from the audio characteristic. Compared with conventional works that require complicated processing and barely utilize limited audio features, our model is more concise and comprehensive since language model directly leverage its semantic capabilities to generate the sequences. We investigate different decoupling modules to demonstrate the effectiveness for timestamps capture and event classification. Evaluation results show that the proposed method achieves accurate sequences of sound event detection.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Recently" is translated as "最近" (most recent)* "ability" is translated as "能力" (capability)* "language models" is translated as "语言模型" (language models)* "visual cross-modality" is translated as "视觉交叠" (visual intersection)* "sound event detection" is translated as "声音事件检测" (sound event detection)* "acoustic encoder" is translated as "声音编码器" (acoustic encoder)* "contrastive module" is translated as "对比模块" (contrastive module)* "decoupled language decoder" is translated as "分离语言解码器" (decoupled language decoder)* "temporal and event sequences" is translated as "时间和事件序列" (temporal and event sequences)* "compared with conventional works" is translated as "与传统工作比较" (compared with conventional works)* "require complicated processing" is translated as "需要复杂处理" (require complicated processing)* "barely utilize limited audio features" is translated as "几乎只使用有限的音频特征" (barely utilize limited audio features)* "our model is more concise and comprehensive" is translated as "我们的模型更简洁和全面" (our model is more concise and comprehensive)* "directly leverage its semantic capabilities" is translated as "直接利用它的 semantic 能力" (directly leverage its semantic capabilities)* "generate the sequences" is translated as "生成序列" (generate the sequences)* "timestamps capture" is translated as "时间捕捉" (timestamps capture)* "event classification" is translated as "事件分类" (event classification)* "evaluation results show" is translated as "评估结果表明" (evaluation results show)* "accurate sequences of sound event detection" is translated as "准确的声音事件检测序列" (accurate sequences of sound event detection)
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users"><a href="#Deep-learning-based-denoising-streamed-from-mobile-phones-improves-speech-in-noise-understanding-for-hearing-aid-users" class="headerlink" title="Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users"></a>Deep learning-based denoising streamed from mobile phones improves speech-in-noise understanding for hearing aid users</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11456">http://arxiv.org/abs/2308.11456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Udo Diehl, Hannes Zilly, Felix Sattler, Yosef Singer, Kevin Kepp, Mark Berry, Henning Hasemann, Marlene Zippel, Müge Kaya, Paul Meyer-Rachner, Annett Pudszuhn, Veit M. Hofmann, Matthias Vormann, Elias Sprengel<br>for:The paper is written for people with hearing loss who use hearing aids, particularly those in real-world noisy environments.methods:The paper uses deep learning-based denoising systems that run in real-time on mobile devices (iPhone 7 and Samsung Galaxy S10) with a total delay of around 75ms.results:The denoising system improves audio quality, as measured by subjective ratings, speech intelligibility, and live conversations in noisy environments. Subjective ratings increase by more than 40%, and speech reception thresholds improve by 1.6 dB SRT compared to a fitted hearing aid as a baseline. The system is the first of its kind to be implemented on a mobile device and streamed directly to users’ hearing aids using only a single channel of audio input, resulting in improved user satisfaction.<details>
<summary>Abstract</summary>
The hearing loss of almost half a billion people is commonly treated with hearing aids. However, current hearing aids often do not work well in real-world noisy environments. We present a deep learning based denoising system that runs in real time on iPhone 7 and Samsung Galaxy S10 (25ms algorithmic latency). The denoised audio is streamed to the hearing aid, resulting in a total delay of around 75ms. In tests with hearing aid users having moderate to severe hearing loss, our denoising system improves audio across three tests: 1) listening for subjective audio ratings, 2) listening for objective speech intelligibility, and 3) live conversations in a noisy environment for subjective ratings. Subjective ratings increase by more than 40%, for both the listening test and the live conversation compared to a fitted hearing aid as a baseline. Speech reception thresholds, measuring speech understanding in noise, improve by 1.6 dB SRT. Ours is the first denoising system that is implemented on a mobile device, streamed directly to users' hearing aids using only a single channel as audio input while improving user satisfaction on all tested aspects, including speech intelligibility. This includes overall preference of the denoised and streamed signal over the hearing aid, thereby accepting the higher latency for the significant improvement in speech understanding.
</details>
<details>
<summary>摘要</summary>
现有约一亿人的听力问题，通常通过听觉器进行治疗。然而，现有的听觉器 frequently 在实际噪音环境中不够有效。我们提出了基于深度学习的噪音除除系统，可以在 iPhone 7 和 Samsung Galaxy S10 上进行实时运行（25ms 的算法遅延）。噪音除除系统将运算到听觉器上，总延迟约 75ms。在听觉器用户进行 moderate 至 severe 的听力损伤时，我们的噪音除除系统在三个测试中表现出色：1）聆听Subjective 音乐评分，2）聆听Speech 智能度测试，3）在噪音环境中进行生活对话的Subjective 评分。聆听者的评分提高了 más de 40%，包括聆听测试和生活对话。Speech 收集阈值（SRT）也提高了1.6 dB。我们的噪音除除系统是首个在 mobil device 上实现的，直接将清晰的音频流传递到用户的听觉器，使用单一通道的音频输入，而不需要听觉器的额外配件。在所有测试项目中，包括聆听者对清晰音频的偏好和生活对话中的听力理解，我们的系统获得了更高的使用者满意度。
</details></li>
</ul>
<hr>
<h2 id="Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition"><a href="#Convoifilter-A-case-study-of-doing-cocktail-party-speech-recognition" class="headerlink" title="Convoifilter: A case study of doing cocktail party speech recognition"></a>Convoifilter: A case study of doing cocktail party speech recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11380">http://arxiv.org/abs/2308.11380</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thai-Binh Nguyen, Alexander Waibel</li>
<li>for: 提高自动语音识别（ASR）的精度，特别是在响度高、听录环境中。</li>
<li>methods: 使用单通道speech减少模块减少干扰声音，同时使用ASR模块。通过这种方法，模型可以降低ASR的单词错误率（WER）从80%降至26.4%。</li>
<li>results: 通过对这两个组件进行联合细调，降低WER从26.4%下降至14.5%。<details>
<summary>Abstract</summary>
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1"><a href="#Evaluation-of-the-Speech-Resynthesis-Capabilities-of-the-VoicePrivacy-Challenge-Baseline-B1" class="headerlink" title="Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1"></a>Evaluation of the Speech Resynthesis Capabilities of the VoicePrivacy Challenge Baseline B1</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11337">http://arxiv.org/abs/2308.11337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ünal Ege Gaznepoglu, Nils Peters</li>
<li>for: 这项研究旨在评估VPC基线B1是否能够生成人性化的语音。</li>
<li>methods: 这项研究使用了VPC基线B1中的神经 vocoder 将 F0、x-vectors 和瓶颈特征转换为语音示例。</li>
<li>results: 研究发现，神经 vocoder 在语音示例中引入了 artifacts，导致语音具有不自然的感觉。 MUSHRA-like 听试中，18名参与者也证实了这一点，促使进一步研究VPC基线B1的分析和 sintesis 组件。<details>
<summary>Abstract</summary>
Speaker anonymization systems continue to improve their ability to obfuscate the original speaker characteristics in a speech signal, but often create processing artifacts and unnatural sounding voices as a tradeoff. Many of those systems stem from the VoicePrivacy Challenge (VPC) Baseline B1, using a neural vocoder to synthesize speech from an F0, x-vectors and bottleneck features-based speech representation. Inspired by this, we investigate the reproduction capabilities of the aforementioned baseline, to assess how successful the shared methodology is in synthesizing human-like speech. We use four objective metrics to measure speech quality, waveform similarity, and F0 similarity. Our findings indicate that both the speech representation and the vocoder introduces artifacts, causing an unnatural perception. A MUSHRA-like listening test on 18 subjects corroborate our findings, motivating further research on the analysis and synthesis components of the VPC Baseline B1.
</details>
<details>
<summary>摘要</summary>
对话匿名系统继续提高了对话者特征的隐蔽能力，但经常会产生处理 artifacts 和不自然的声音作为交易。这些系统多数来自于 VoicePrivacy Challenge（VPC）基线B1，使用神经 vocoder 将 F0、x-vectors 和瓶颈特征转化为语音。受这些基线的启发，我们调查了论文中的复制能力，以评估这种方法是否能够 sinthez human-like 语音。我们使用四个对象指标测量语音质量、波形相似性和 F0 相似性。我们的发现表明， tanto 语音表示法 quanto  vocoder 都会产生缺陷，导致不自然的感觉。一个 MUSHRA-like 听众测试中，18名参与者证实了我们的发现，并促使了对分析和 sinthez 组件的进一步研究。
</details></li>
</ul>
<hr>
<h2 id="Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning"><a href="#Music-Understanding-LLaMA-Advancing-Text-to-Music-Generation-with-Question-Answering-and-Captioning" class="headerlink" title="Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning"></a>Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11276">http://arxiv.org/abs/2308.11276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan</li>
<li>for: 这篇论文旨在解决文本到音乐生成（T2M-Gen）领域中的一个主要障碍，即公共可用的大规模音乐数据集中的缺乏。</li>
<li>methods: 我们提出了一种名为Music Understanding LLaMA（MU-LLaMA）的模型，可以回答音乐相关的问题并生成音乐文件的标题。我们的模型使用一个预训练的MERT模型来提取音乐特征。</li>
<li>results: 我们的实验表明，使用我们设计的MusicQA数据集训练MU-LLaMA模型可以在多个维度上达到出色的表现，包括音乐问答和音乐标题生成。我们的模型在不同的维度上的表现都高于当前状态的艺术模型，并为T2M-Gen领域的进步带来了希望。<details>
<summary>Abstract</summary>
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music question answering and music caption generation across various metrics, outperforming current state-of-the-art (SOTA) models in both fields and offering a promising advancement in the T2M-Gen research field.
</details>
<details>
<summary>摘要</summary>
文本转换为乐曲生成（T2M-Gen）遇到了一个主要的障碍，即公共可用的大规模乐曲数据集中的自然语言标签缺乏。为解决这一问题，我们提议了Music Understanding LLaMA（MU-LLaMA），能够回答乐曲相关的问题并生成乐曲文件的标签。我们的模型利用了预训练的MERT模型来提取乐曲特征。但是，为了训练MU-LLaMA模型，获得合适的数据集仍然是一个挑战，因为现有的公共可用的音频问答数据集缺乏必要的深度来回答开放式乐曲问题。为了填补这一漏洞，我们提出了一种方法，可以从现有的音频描述数据集中生成问题答案对。我们还介绍了MusicQA数据集，用于回答开放式乐曲相关的问题。实验结果表明，我们提出的MU-LLaMA模型，使用我们设计的MusicQA数据集进行训练，在不同的纪录metric上取得了极佳的表现，超过了当前状态的前方模型在乐曲问题回答和乐曲描述生成方面，并提供了T2M-Gen研究领域的一个可能的进步。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Bends-in-Popular-Music-Guitar-Tablatures"><a href="#Modeling-Bends-in-Popular-Music-Guitar-Tablatures" class="headerlink" title="Modeling Bends in Popular Music Guitar Tablatures"></a>Modeling Bends in Popular Music Guitar Tablatures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.12307">http://arxiv.org/abs/2308.12307</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/adhooge1/bend-prediction">https://gitlab.com/adhooge1/bend-prediction</a></li>
<li>paper_authors: Alexandre D’Hooge, Louis Bigo, Ken Déguernel</li>
<li>For: 本研究旨在提高电 guitar tablature 中的折衣预测，以便帮助将其他乐器的乐谱转换成 guitar tablature。* Methods: 本研究使用了一组25个高级特征，对每个 tablature 进行分析，以预测下一个折衣的发生。* Results: 实验结果显示，使用决策树可以成功预测折衣的发生，F1 分数为 0.71，false positive 预测很少，这表明这种方法可以有效地帮助将非 guitar 乐谱转换成 guitar tablature。<details>
<summary>Abstract</summary>
Tablature notation is widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details>
<details>
<summary>摘要</summary>
Tablature notation widely used in popular music to transcribe and share guitar musical content. As a complement to standard score notation, tablatures transcribe performance gesture information, including finger positions and a variety of guitar-specific playing techniques such as slides, hammer-on/pull-off or bends.This paper focuses on bends, which enable to progressively shift the pitch of a note, therefore circumventing physical limitations of the discrete fretted fingerboard. In this paper, we propose a set of 25 high-level features, computed for each note of the tablature, to study how bend occurrences can be predicted from their past and future short-term context. Experiments are performed on a corpus of 932 lead guitar tablatures of popular music and show that a decision tree successfully predicts bend occurrences with an F1 score of 0.71 anda limited amount of false positive predictions, demonstrating promising applications to assist the arrangement of non-guitar music into guitar tablatures.
</details></li>
</ul>
<hr>
<h2 id="PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion"><a href="#PMVC-Data-Augmentation-Based-Prosody-Modeling-for-Expressive-Voice-Conversion" class="headerlink" title="PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion"></a>PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.11084">http://arxiv.org/abs/2308.11084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Huaizhen Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 本研究旨在提出一个新的声音转换框架，以实现无需文本转换的自然声音转换。</li>
<li>methods: 本研究使用了一新的声音增强算法来提取robust的调音信息，并应用了mask和predict机制来分离调音和内容信息。</li>
<li>results: 实验结果显示，PMVC框架能够提高声音转换的自然性和相似度。<details>
<summary>Abstract</summary>
Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.
</details>
<details>
<summary>摘要</summary>
声音转换作为样式传递任务应用于语音，指的是将一个人的语音转换成另一个人的语音，以致 зву量和样式相似。至今，对于更好地实现声音转换任务的研究已经很多。然而，一个好的声音转换模型应该不仅匹配目标说话者的时刻信息，还应该包括表达信息 such as 味道、速度、停顿等。在这种情况下，味道模型化是关键的，以实现自然和吸引人的声音转换。可惜，味道模型化是重要但困难的，特别是无文本词汇。在这篇论文中，我们首先提出了一种新的声音转换框架，名为PMVC，可以有效地从语音中分离和模型内容、时刻和表达信息。特别是，我们提出了一种新的语音增强算法，用于Robust prosody extraction。然后，我们在内容和味道信息之间进行遮盖和预测，以实现味道和内容信息的分离。实验结果表示，PMVC框架在AIShell-3 corpus上提高了转换后语音的自然性和相似性。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/08/22/cs.SD_2023_08_22/" data-id="clm0t8e1f00amv788fj513y95" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">60</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">56</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">56</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
