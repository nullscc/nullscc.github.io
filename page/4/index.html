
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.LG_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.LG_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.LG_2023_08_07/">cs.LG - 2023-08-07 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model"><a href="#Improving-FHB-Screening-in-Wheat-Breeding-Using-an-Efficient-Transformer-Model" class="headerlink" title="Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model"></a>Improving FHB Screening in Wheat Breeding Using an Efficient Transformer Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03670">http://arxiv.org/abs/2308.03670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babak Azad, Ahmed Abdalla, Kwanghee Won, Ali Mirzakhani Nafchi</li>
<li>for: 这个研究是为了提高小麦和黑麦抗病耐菌creening中的效率、准确率和时间探测病菌病 head blight（FHB）。</li>
<li>methods: 这个研究使用了一种新的Context Bridge，将U-Net网络的本地表现力与transformer模型的全球自注意力机制相结合，以提高病菌探测的精度和效率。此外，原始transformer模型中的标准注意力机制被更改为Efficient Self-attention，以减少其复杂性。</li>
<li>results: 经过广泛的实验和评估，研究发现transformer-based方法在小麦病菌探测中具有高效率和准确率，并且可以处理不同类型的植物形态和病变。<details>
<summary>Abstract</summary>
Fusarium head blight is a devastating disease that causes significant economic losses annually on small grains. Efficiency, accuracy, and timely detection of FHB in the resistance screening are critical for wheat and barley breeding programs. In recent years, various image processing techniques have been developed using supervised machine learning algorithms for the early detection of FHB. The state-of-the-art convolutional neural network-based methods, such as U-Net, employ a series of encoding blocks to create a local representation and a series of decoding blocks to capture the semantic relations. However, these methods are not often capable of long-range modeling dependencies inside the input data, and their ability to model multi-scale objects with significant variations in texture and shape is limited. Vision transformers as alternative architectures with innate global self-attention mechanisms for sequence-to-sequence prediction, due to insufficient low-level details, may also limit localization capabilities. To overcome these limitations, a new Context Bridge is proposed to integrate the local representation capability of the U-Net network in the transformer model. In addition, the standard attention mechanism of the original transformer is replaced with Efficient Self-attention, which is less complicated than other state-of-the-art methods. To train the proposed network, 12,000 wheat images from an FHB-inoculated wheat field at the SDSU research farm in Volga, SD, were captured. In addition to healthy and unhealthy plants, these images encompass various stages of the disease. A team of expert pathologists annotated the images for training and evaluating the developed model. As a result, the effectiveness of the transformer-based method for FHB-disease detection, through extensive experiments across typical tasks for plant image segmentation, is demonstrated.
</details>
<details>
<summary>摘要</summary>
fusarium 头疫是一种致命的病种，每年对小麦和黑麦产生了重大经济损失。在抗性屏测中，效率、准确性和时效检测是关键。在最近几年，各种图像处理技术被开发，使用超级vised机器学习算法进行早期检测。state-of-the-art的卷积神经网络方法，如U-Net，使用一系列的编码块来创建本地表示，并使用一系列的解码块来捕捉 semantic关系。然而，这些方法通常无法长距离模型数据中的相互关系，其能模型多尺度对象的表征和文本特征是有限的。在这种情况下，一种新的 Context Bridge 被提议，以 интеGRATE U-Net 网络的本地表示能力到 transformer 模型中。此外，原始 transformer 的标准注意力机制被 replaced with 高效自注意力，这种机制 simpler than other state-of-the-art methods。为了训练提议的网络，SDSU 研究农场在南达科他州的一个 FHB-感染小麦田中Capture了12,000 个小麦图像。此外，这些图像还包括不同阶段的疾病。一 команoda expert 的病理学家对这些图像进行了训练和评估。结果，通过对plant image segmentation 等常见任务进行广泛的实验， demonstarted  transformer 基于方法的 FHB 疾病检测的效果。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders"><a href="#Diffusion-Model-in-Causal-Inference-with-Unmeasured-Confounders" class="headerlink" title="Diffusion Model in Causal Inference with Unmeasured Confounders"></a>Diffusion Model in Causal Inference with Unmeasured Confounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03669">http://arxiv.org/abs/2308.03669</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/BDCM">https://github.com/tatsu432/BDCM</a></li>
<li>paper_authors: Tatsuhiro Shimizu</li>
<li>for: 本研究旨在扩展diffusion模型，以便从观察数据中回答 causal问题，即使存在不可观测的假设变量。</li>
<li>methods: 我们使用了Pearl提出的Directed Acyclic Graph (DAG)来捕捉 causal intervention，并提出了一种叫做Diffusion-based Causal Model (DCM)的模型，假设所有的假设变量都可以观测。然而，在实际应用中，不可观测的假设变量存在，这限制了DCM的可用性。为了解决这个限制，我们提出了一种扩展模型，即Backdoor Criterion based DCM (BDCM)。</li>
<li>results: 我们通过synthetic data experiment表明，我们的提议的模型可以更 precisely回答 causal问题，即使存在不可观测的假设变量。<details>
<summary>Abstract</summary>
We study how to extend the use of the diffusion model to answer the causal question from the observational data under the existence of unmeasured confounders. In Pearl's framework of using a Directed Acyclic Graph (DAG) to capture the causal intervention, a Diffusion-based Causal Model (DCM) was proposed incorporating the diffusion model to answer the causal questions more accurately, assuming that all of the confounders are observed. However, unmeasured confounders in practice exist, which hinders DCM from being applicable. To alleviate this limitation of DCM, we propose an extended model called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the Backdoor criterion to find the variables in DAG to be included in the decoding process of the diffusion model so that we can extend DCM to the case with unmeasured confounders. Synthetic data experiment demonstrates that our proposed model captures the counterfactual distribution more precisely than DCM under the unmeasured confounders.
</details>
<details>
<summary>摘要</summary>
我们研究如何将扩散模型应用于从观察数据中回答 causal 问题，即使存在未探测的干扰因素。在珍珠的框架中使用指导的циклиック графи（DAG）捕捉 causal 干扰，一种叫做扩散基于 causal 模型（DCM）的模型被提出，假设所有干扰因素都是观察的。然而，在实践中存在未探测的干扰因素，这限制了 DCM 的应用。为了解决 DCM 的这种限制，我们提出了一种扩展模型，即基于后门准则的 DCM（BDCM）。该模型的思想在于根据后门准则选择 DAG 中的变量，以便在扩散模型的解码过程中包含这些变量，从而扩展 DCM 到具有未探测干扰因素的情况。 synthetic 数据实验表明，我们的提议模型可以更 precisely 捕捉 counterfactual 分布than DCM 在未探测干扰因素的情况下。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness"><a href="#Bridging-Trustworthiness-and-Open-World-Learning-An-Exploratory-Neural-Approach-for-Enhancing-Interpretability-Generalization-and-Robustness" class="headerlink" title="Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness"></a>Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03666">http://arxiv.org/abs/2308.03666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shide Du, Zihan Fang, Shiyang Lan, Yanchao Tan, Manuel Günther, Shiping Wang, Wenzhong Guo</li>
<li>for: 提高人工智能系统的可靠性和可解释性，以增强人工智能在开放世界中的应用。</li>
<li>methods: 基于自定义可信网络和灵活学习正则化的方法，以提高学习模型的通用性和适应性。</li>
<li>results: 通过实现设计层解释性、环境健康任务接口和开放世界识别计划，提高了多模态场景下的信任性和可靠性。<details>
<summary>Abstract</summary>
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy learning; 3) We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms. Eventually, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Insufficient explanation of predictive results2. Inadequate generalization for learning models3. Poor adaptability to uncertain environmentsTo address these challenges, we explore a neural program that bridges trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. Our approach includes the following three components:1. Enhancing design-level interpretability: We first customize trustworthy networks with specific physical meanings, making it easier to understand how the AI system works and why it makes certain decisions.2. Improving generalization through flexible learning regularizers: We design environmental well-being task-interfaces to improve the generalization of trustworthy learning, ensuring that the AI system can adapt to different environments and situations.3. Integrating open-world recognition losses with agent mechanisms: We propose to increase the robustness of trustworthy learning by integrating open-world recognition losses with agent mechanisms, enabling the AI system to better handle unexpected events and situations.By combining these three components, we enhance various trustworthy properties through the establishment of design-level explainability, environmental well-being task-interfaces, and open-world recognition programs. These designed open-world protocols are applicable across a wide range of surroundings, under open-world multimedia recognition scenarios with significant performance improvements observed.</details></li>
</ol>
<hr>
<h2 id="Distributionally-Robust-Classification-on-a-Data-Budget"><a href="#Distributionally-Robust-Classification-on-a-Data-Budget" class="headerlink" title="Distributionally Robust Classification on a Data Budget"></a>Distributionally Robust Classification on a Data Budget</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03821">http://arxiv.org/abs/2308.03821</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/vlhub">https://github.com/penfever/vlhub</a></li>
<li>paper_authors: Benjamin Feuer, Ameya Joshi, Minh Pham, Chinmay Hegde</li>
<li>for: 这篇论文的目的是提高深度学习模型在数据分布变化时的预测可靠性，并且在有限数据情况下进行训练。</li>
<li>methods: 论文使用了一系列控制严格的实验和大规模元分析来研究因素对模型 Robustness 的影响，并使用标准 ResNet-50 和 CLIP ResNet-50 进行比较。</li>
<li>results: 论文显示，使用限制数据量训练标准 ResNet-50 可以达到与 CLIP ResNet-50 训练于400万样本后的相似水平的分布Robustness。这是我们知道的首个在有限数据预算下实现 (near) 状态的艺技 Robustness 的结果。<details>
<summary>Abstract</summary>
Real world uses of deep learning require predictable model behavior under distribution shifts. Models such as CLIP show emergent natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. Can we train robust learners in a domain where data is limited? To rigorously address this question, we introduce JANuS (Joint Annotations and Names Set), a collection of four new training datasets with images, labels, and corresponding captions, and perform a series of carefully controlled investigations of factors contributing to robustness in image classification, then compare those results to findings derived from a large-scale meta-analysis. Using this approach, we show that standard ResNet-50 trained with the cross-entropy loss on 2.4 million image samples can attain comparable robustness to a CLIP ResNet-50 trained on 400 million samples. To our knowledge, this is the first result showing (near) state-of-the-art distributional robustness on limited data budgets. Our dataset is available at \url{https://huggingface.co/datasets/penfever/JANuS_dataset}, and the code used to reproduce our experiments can be found at \url{https://github.com/penfever/vlhub/}.
</details>
<details>
<summary>摘要</summary>
实际应用中的深度学习需要模型在分布变化时保持预测可靠。如CLIP模型，它们可以 Display natural distributional robustness comparable to humans, but may require hundreds of millions of training samples. 我们问道：可以在数据有限的领域中训练强健的学习者吗？为了彻底回答这个问题，我们介绍了 JANuS（共同标注和名称集），一个包含四个新的训练数据集的集合，每个数据集包含图像、标签和相应的描述。我们进行了一系列仔细控制的调查，探讨了影响模型强健性的因素，然后将结果与大规模元分析结果进行比较。我们发现，使用权重损失函数和240万个图像样本训练的标准 ResNet-50 可以达到与CLIP ResNet-50 在4000万个样本上训练后的（近）状态OF-the-art分布强健性。我们认为这是首次在有限数据预算下实现的分布强健性结果。我们的数据集可以在 \url{https://huggingface.co/datasets/penfever/JANuS_dataset} 上下载，并且用于 reproduce我们的实验的代码可以在 \url{https://github.com/penfever/vlhub/} 上找到。
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries"><a href="#Two-stage-Early-Prediction-Framework-of-Remaining-Useful-Life-for-Lithium-ion-Batteries" class="headerlink" title="Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries"></a>Two-stage Early Prediction Framework of Remaining Useful Life for Lithium-ion Batteries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03664">http://arxiv.org/abs/2308.03664</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhruv Mittal, Hymalai Bello, Bo Zhou, Mayank Shekhar Jha, Sungho Suh, Paul Lukowicz</li>
<li>for: 预测锂离子电池剩余有用生命（RUL）的早期预测是重要的，以提高电池技术的可靠性和维护性。</li>
<li>methods: 本文提出了一种新的RUL预测方法，包括两个阶段：首先使用神经网络模型确定第一个预测周期（FPC），然后使用预测衰变模式来估算剩余有用生命的百分比。</li>
<li>results: 实验结果表明，提出的方法在RUL预测方面比既有方法更高准确。此外，该方法在实际应用场景中也表现出了优异的应用性和准确性。<details>
<summary>Abstract</summary>
Early prediction of remaining useful life (RUL) is crucial for effective battery management across various industries, ranging from household appliances to large-scale applications. Accurate RUL prediction improves the reliability and maintainability of battery technology. However, existing methods have limitations, including assumptions of data from the same sensors or distribution, foreknowledge of the end of life (EOL), and neglect to determine the first prediction cycle (FPC) to identify the start of the unhealthy stage. This paper proposes a novel method for RUL prediction of Lithium-ion batteries. The proposed framework comprises two stages: determining the FPC using a neural network-based model to divide the degradation data into distinct health states and predicting the degradation pattern after the FPC to estimate the remaining useful life as a percentage. Experimental results demonstrate that the proposed method outperforms conventional approaches in terms of RUL prediction. Furthermore, the proposed method shows promise for real-world scenarios, providing improved accuracy and applicability for battery management.
</details>
<details>
<summary>摘要</summary>
早期预测电池剩余有用寿命（RUL）是多个领域中的关键，从家用电器到大规模应用。准确预测RUL提高电池技术的可靠性和维护性。然而，现有的方法有限制，包括使用同一些感知器的数据或分布，假设结束生命周期（EOL）的知识，以及忽略确定首次预测周期（FPC）以识别不健康的阶段。这篇论文提出了一种新的Li-ion电池RUL预测方法。该框架包括两个阶段：使用神经网络模型来分解衰变数据 into Distinct Health States，并预测衰变模式以估计剩余有用寿命为百分比。实验结果表明，提案的方法在RUL预测方面超过了传统方法的性能。此外，提案的方法在实际应用中具有改善的准确性和可应用性。
</details></li>
</ul>
<hr>
<h2 id="Matrix-Completion-in-Almost-Verification-Time"><a href="#Matrix-Completion-in-Almost-Verification-Time" class="headerlink" title="Matrix Completion in Almost-Verification Time"></a>Matrix Completion in Almost-Verification Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03661">http://arxiv.org/abs/2308.03661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan A. Kelner, Jerry Li, Allen Liu, Aaron Sidford, Kevin Tian</li>
<li>for: 这个论文提出了一种新的低级别矩阵完成问题的解决方案，即从Random观察到rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$)的approximation。</li>
<li>methods: 这个论文提出了一种算法，可以在没有任何假设的情况下，从 $\approx mr$ 个观察数据中完成 $\mathbf{M}$ 的99% 的行和列。这个算法需要 $\approx mr^2$ 时间。在具有特定的行和列范围的 $\mathbf{M}$ 中，通过聚合多个回归问题的解决，这个算法可以完成全矩阵。</li>
<li>results: 论文表明，在具有几乎信息理论最佳的情况下，这个算法可以从 $mr^{2+o(1)}$ 个观察数据中完成 $\mathbf{M}$ 到高精度，并且runtime为 $mr^{3+o(1)}$。在特定的行和列范围下，这个算法可以完成 $\mathbf{M}$ 到信息理论最佳的精度，并且runtime为 $mr^{2+o(1)}$。此外，这个论文还提出了一些Robust的算法，可以在含有噪声的情况下完成 $\mathbf{M}$ 到 Frobenius 范数 distance $\approx r^{1.5}\Delta$，其中 $|\mathbf{N}|_{F} \le \Delta$。在噪声的情况下，这个算法的精度和runtime都比之前的算法更好。<details>
<summary>Abstract</summary>
We give a new framework for solving the fundamental problem of low-rank matrix completion, i.e., approximating a rank-$r$ matrix $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) from random observations. First, we provide an algorithm which completes $\mathbf{M}$ on $99\%$ of rows and columns under no further assumptions on $\mathbf{M}$ from $\approx mr$ samples and using $\approx mr^2$ time. Then, assuming the row and column spans of $\mathbf{M}$ satisfy additional regularity properties, we show how to boost this partial completion guarantee to a full matrix completion algorithm by aggregating solutions to regression problems involving the observations.   In the well-studied setting where $\mathbf{M}$ has incoherent row and column spans, our algorithms complete $\mathbf{M}$ to high precision from $mr^{2+o(1)}$ observations in $mr^{3 + o(1)}$ time (omitting logarithmic factors in problem parameters), improving upon the prior state-of-the-art [JN15] which used $\approx mr^5$ samples and $\approx mr^7$ time. Under an assumption on the row and column spans of $\mathbf{M}$ we introduce (which is satisfied by random subspaces with high probability), our sample complexity improves to an almost information-theoretically optimal $mr^{1 + o(1)}$, and our runtime improves to $mr^{2 + o(1)}$. Our runtimes have the appealing property of matching the best known runtime to verify that a rank-$r$ decomposition $\mathbf{U}\mathbf{V}^\top$ agrees with the sampled observations. We also provide robust variants of our algorithms that, given random observations from $\mathbf{M} + \mathbf{N}$ with $\|\mathbf{N}\|_{F} \le \Delta$, complete $\mathbf{M}$ to Frobenius norm distance $\approx r^{1.5}\Delta$ in the same runtimes as the noiseless setting. Prior noisy matrix completion algorithms [CP10] only guaranteed a distance of $\approx \sqrt{n}\Delta$.
</details>
<details>
<summary>摘要</summary>
我们提出了一新的框架来解决低级矩阵完成问题，即 aproximating 一个rank-$r$矩阵 $\mathbf{M} \in \mathbb{R}^{m \times n}$ (where $m \ge n$) 从 random observations。首先，我们提供了一个算法，可以在没有进一步假设的情况下，使 $\mathbf{M}$ 在99% 的行和列上完成，需要 $\approx mr$ 样本和 $\approx mr^2$ 时间。然后，如果行和列范围的范围满足其他正则性质，我们如何通过融合关于 observations 的回归问题的解来提高这个部分完成 garantía。在已有研究的设定中， где $\mathbf{M}$ 的行和列范围是不相关的，我们可以从 $mr^{2+o(1)}$ 样本和 $mr^{3+o(1)}$ 时间中完成 $\mathbf{M}$ 到高精度，超过先前的最佳状态（JN15），其使用 $mr^5$ 样本和 $mr^7$ 时间。如果行和列范围满足我们引入的一个假设（这在Random subspace 中发生的概率很高），我们的样本复杂度可以降低到 almost information-theoretically optimal $mr^{1+o(1)}$，并且时间复杂度可以降低到 $mr^{2+o(1)}$。我们的运行时间具有愉悦的性质，即与verify rank-$r$ 分解 $\mathbf{U}\mathbf{V}^\top$ 与样本观测匹配的最佳known runtime。我们还提供了一些Robust 变体我们的算法，可以在 $\mathbf{M} + \mathbf{N}$ 中的随机观测下，完成 $\mathbf{M}$ 到 Frobenius 范数Distance $\approx r^{1.5}\Delta$ 的同样时间。在先前的噪声矩阵完成算法（CP10）中，只能保证一个距离为 $\approx \sqrt{n}\Delta$。
</details></li>
</ul>
<hr>
<h2 id="Generative-Forests"><a href="#Generative-Forests" class="headerlink" title="Generative Forests"></a>Generative Forests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03648">http://arxiv.org/abs/2308.03648</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/AlCorreia/GeFs">https://github.com/AlCorreia/GeFs</a></li>
<li>paper_authors: Richard Nock, Mathieu Guillame-Bert</li>
<li>for: 本文旨在提出新的树形生成模型，用于Tabular数据生成和概率模型。</li>
<li>methods: 本文使用新的树形生成模型，并提出一种基于supervised学习的训练算法，可以在Tabular数据上实现高质量的数据生成。</li>
<li>results: 实验表明，本文的方法可以在缺失数据填充和生成数据比较实际数据的任务中显示出高质量的结果， especial against state of the art。<details>
<summary>Abstract</summary>
Tabular data represents one of the most prevalent form of data. When it comes to data generation, many approaches would learn a density for the data generation process, but would not necessarily end up with a sampler, even less so being exact with respect to the underlying density. A second issue is on models: while complex modeling based on neural nets thrives in image or text generation (etc.), less is known for powerful generative models on tabular data. A third problem is the visible chasm on tabular data between training algorithms for supervised learning with remarkable properties (e.g. boosting), and a comparative lack of guarantees when it comes to data generation. In this paper, we tackle the three problems, introducing new tree-based generative models convenient for density modeling and tabular data generation that improve on modeling capabilities of recent proposals, and a training algorithm which simplifies the training setting of previous approaches and displays boosting-compliant convergence. This algorithm has the convenient property to rely on a supervised training scheme that can be implemented by a few tweaks to the most popular induction scheme for decision tree induction with two classes. Experiments are provided on missing data imputation and comparing generated data to real data, displaying the quality of the results obtained by our approach, in particular against state of the art.
</details>
<details>
<summary>摘要</summary>
表格数据是现代数据的一种最常见形式。当处理数据时，许多方法都会学习数据的浓度，但它们并不一定会得到一个采样器，更不用说是对于真实的浓度准确。第二个问题是模型：虽然复杂的模型基于神经网络在图像或文本生成等领域得到了广泛的应用，但对于表格数据， menos is known about powerful generative models。第三个问题是表格数据的训练算法和supervised learning之间的可见差异。在这篇论文中，我们解决了这三个问题，提出了新的树形生成模型，可以提高表格数据生成的模型能力，并且提供了一种简化训练过程的算法，可以让之前的方法在训练中更加简单。这种算法可以通过对最流行的决策树引入两类的修改来实现。我们的实验表明，我们的方法可以在缺失数据填充和生成数据与真实数据的比较中 display 出色的结果，特别是与现有技术相比。
</details></li>
</ul>
<hr>
<h2 id="XFlow-Benchmarking-Flow-Behaviors-over-Graphs"><a href="#XFlow-Benchmarking-Flow-Behaviors-over-Graphs" class="headerlink" title="XFlow: Benchmarking Flow Behaviors over Graphs"></a>XFlow: Benchmarking Flow Behaviors over Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03819">http://arxiv.org/abs/2308.03819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xgraphing/xflow">https://github.com/xgraphing/xflow</a></li>
<li>paper_authors: Zijian Zhang, Zonghan Zhang, Zhiqian Chen</li>
<li>for: 本研究旨在提供一个包容性强的benchmark集合，以便研究在网络场景下的流行行为。</li>
<li>methods: 本研究使用了多种任务、基准模型、图Dataset和评估工具来研究流行行为。</li>
<li>results: 研究发现了现有基础模型的优劣点，并提出了进一步研究的可能性。基于实验结果，提供了一个通用的分析框架，用于研究多种流行任务的多个领域。<details>
<summary>Abstract</summary>
The occurrence of diffusion on a graph is a prevalent and significant phenomenon, as evidenced by the spread of rumors, influenza-like viruses, smart grid failures, and similar events. Comprehending the behaviors of flow is a formidable task, due to the intricate interplay between the distribution of seeds that initiate flow propagation, the propagation model, and the topology of the graph. The study of networks encompasses a diverse range of academic disciplines, including mathematics, physics, social science, and computer science. This interdisciplinary nature of network research is characterized by a high degree of specialization and compartmentalization, and the cooperation facilitated by them is inadequate. From a machine learning standpoint, there is a deficiency in a cohesive platform for assessing algorithms across various domains. One of the primary obstacles to current research in this field is the absence of a comprehensive curated benchmark suite to study the flow behaviors under network scenarios.   To address this disparity, we propose the implementation of a novel benchmark suite that encompasses a variety of tasks, baseline models, graph datasets, and evaluation tools. In addition, we present a comprehensive analytical framework that offers a generalized approach to numerous flow-related tasks across diverse domains, serving as a blueprint and roadmap. Drawing upon the outcomes of our empirical investigation, we analyze the advantages and disadvantages of current foundational models, and we underscore potential avenues for further study. The datasets, code, and baseline models have been made available for the public at: https://github.com/XGraphing/XFlow
</details>
<details>
<summary>摘要</summary>
流行现象在图格中是一种普遍和重要的现象，如传播谣言、流感病毒、智能电网故障等事件。理解流动行为是一项复杂的任务，由于流动 initiation 的分布、传播模型和图格结构之间的复杂交互。网络研究涵盖多个学术领域，包括数学、物理、社会科学和计算机科学。这些学科之间的交流和合作受限。从机器学习的角度来看，流行研究受到了缺乏一个整体的平台来评估算法。目前研究中的主要障碍是缺乏一个完整的精心编辑的benchmark集成来研究流动行为在网络场景下。为了解决这一不足，我们提议实施一个新的benchmark集成，包括多种任务、基线模型、图格数据集和评估工具。此外，我们还提出了一个通用的分析框架，可以在多个流动相关任务上提供一个通用的方法和路线图。通过我们的实验研究的结果，我们分析了当前基础模型的优劣点，并强调了未来研究的可能性。 datasets、代码和基线模型已经公开发布在：https://github.com/XGraphing/XFlow。
</details></li>
</ul>
<hr>
<h2 id="MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining"><a href="#MedMine-Examining-Pre-trained-Language-Models-on-Medication-Mining" class="headerlink" title="MedMine: Examining Pre-trained Language Models on Medication Mining"></a>MedMine: Examining Pre-trained Language Models on Medication Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03629">http://arxiv.org/abs/2308.03629</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hecta-uom/m3">https://github.com/hecta-uom/m3</a></li>
<li>paper_authors: Haifa Alrdahi, Lifeng Han, Hendrik Šuvalov, Goran Nenadic</li>
<li>For: 本研究旨在探讨现有的预训练语言模型（PLM）在自动药物检索任务上的表现，以及这些模型在不同实体类和医疗事件上的偏见问题。* Methods: 本研究使用了现有的预训练语言模型（PLM），包括Med7和XLM-RoBERTa，进行细化调教。并对这些调教后的模型进行比较，找出它们在不同实体类和医疗事件上的优劣点。* Results: 研究发现，XLM-RoBERTa在自动药物检索任务上表现较好，而Med7在一些实体类上表现较差。此外，研究还发现了这些模型在不同实体类和医疗事件上的偏见问题。<details>
<summary>Abstract</summary>
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}
</details>
<details>
<summary>摘要</summary>
自动药物挖掘从临床和生物医学文本中获得了广泛的关注，因为它们在医疗应用中真正有影响。然而，完全自动提取模型仍然需要超越一些障碍物，以便在临床实践中直接部署。这些障碍包括它们在不同实体类型和临床事件上的不均衡性能。在这项工作中，我们评估了当前状态的批处理语言模型（PLM）在这些任务上，包括单语言模型Med7以及多语言大语言模型（LLM）XLM-RoBERTa。我们比较了它们的优点和缺点，使用历史药物挖掘共享任务数据集。我们报告了我们在这些练习中获得的发现，以便未来研究如何解决这些问题，例如如何组合它们的输出、合并这些模型，或者如何提高它们的总准确率通过ensemble学习和数据扩展。MedMine是M3Initiave的一部分，详情请参考<https://github.com/HECTA-UoM/M3>。
</details></li>
</ul>
<hr>
<h2 id="A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging"><a href="#A-sparse-coding-approach-to-inverse-problems-with-application-to-microwave-tomography-imaging" class="headerlink" title="A sparse coding approach to inverse problems with application to microwave tomography imaging"></a>A sparse coding approach to inverse problems with application to microwave tomography imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03818">http://arxiv.org/abs/2308.03818</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cesar F. Caiafa, Ramiro M. Irastorza</li>
<li>for:  solve ill-posed inverse imaging problems in various domains, such as medical diagnosis and astronomical studies.</li>
<li>methods:  use sparse representation of images, a realistic and effective generative model for natural images inspired by the visual system of mammals, to address ill-posed linear inverse problems.</li>
<li>results:  extend the application of sparse coding to solve non-linear and ill-posed problems in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.Here are the three points in Simplified Chinese text:</li>
<li>for: 解决不同领域的各种各样的反射图像问题，如医学诊断和天文学研究。</li>
<li>methods: 使用自然图像的稀疏表示，这是一种基于哺乳动物视觉系统的实用和有效的生成模型，来解决线性不定的反射图像问题。</li>
<li>results: 将稀疏码应用到微波tomography图像重建中，以解决非线性和不定的问题，可能会提高现有算法的性能。<details>
<summary>Abstract</summary>
Inverse imaging problems that are ill-posed can be encountered across multiple domains of science and technology, ranging from medical diagnosis to astronomical studies. To reconstruct images from incomplete and distorted data, it is necessary to create algorithms that can take into account both, the physical mechanisms responsible for generating these measurements and the intrinsic characteristics of the images being analyzed. In this work, the sparse representation of images is reviewed, which is a realistic, compact and effective generative model for natural images inspired by the visual system of mammals. It enables us to address ill-posed linear inverse problems by training the model on a vast collection of images. Moreover, we extend the application of sparse coding to solve the non-linear and ill-posed problem in microwave tomography imaging, which could lead to a significant improvement of the state-of-the-arts algorithms.
</details>
<details>
<summary>摘要</summary>
各种科学和技术领域中的反射 imaging 问题可能会出现不定性，从医疗诊断到天文学研究。为重建受损和扭曲数据中的图像，需要开发能够考虑物理机制生成测量数据以及图像本身内在特征的算法。在这项工作中，我们提出了图像稀疏表示，这是一种现实主义、紧凑和有效的自然图像生成模型，启发自哺乳动物视系统。这种模型可以 addresses 不定性线性反射问题，通过训练模型使用大量图像。此外，我们还扩展了稀疏编码的应用，解决微波探测成像中的非线性和不定性问题，这可能会导致现有算法的显著改进。
</details></li>
</ul>
<hr>
<h2 id="A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction"><a href="#A-Meta-learning-based-Stacked-Regression-Approach-for-Customer-Lifetime-Value-Prediction" class="headerlink" title="A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction"></a>A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08502">http://arxiv.org/abs/2308.08502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Gadgil, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 这种研究旨在提供一种能够快速和简单地计算客户生命周期价值（CLV）的方法，以便企业更好地理解客户，扩大收入。</li>
<li>methods: 这种方法使用了元学习和堆叠回归模型，结合了袋包和强化模型的预测结果，以提高计算CLV的精度和效果。</li>
<li>results: 实验结果表明，提议的方法能够快速和简单地计算CLV，并且能够提高计算结果的准确性和稳定性。<details>
<summary>Abstract</summary>
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency & monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a meta-learning-based stacked regression model which combines the predictions from bagging and boosting models that each is found to perform well individually. Empirical tests have been carried out on an openly available Online Retail dataset to evaluate various models and show the efficacy of the proposed approach.
</details>
<details>
<summary>摘要</summary>
To address this, we propose a system that is both effective and simple. We develop a meta-learning-based stacked regression model that combines the predictions of bagging and boosting models, which have been found to perform well individually. We test our approach on an openly available online retail dataset and show that it is effective.
</details></li>
</ul>
<hr>
<h2 id="Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach"><a href="#Stock-Market-Price-Prediction-A-Hybrid-LSTM-and-Sequential-Self-Attention-based-Approach" class="headerlink" title="Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach"></a>Stock Market Price Prediction: A Hybrid LSTM and Sequential Self-Attention based Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04419">http://arxiv.org/abs/2308.04419</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karan Pardeshi, Sukhpal Singh Gill, Ahmed M. Abdelmoniem</li>
<li>for: 预测股票价格，帮助投资者做出最佳决策</li>
<li>methods: 提出了一种新的Long Short-Term Memory（LSTM）模型，并采用Sequential Self-Attention Mechanism（LSTM-SSAM）来提高预测精度</li>
<li>results: 对三个股票数据集（SBIN、HDFCBANK、BANKBARODA）进行了广泛的实验，结果表明提出的模型比现有模型更有效和可行，RMSE和R2评价指标都达到了最佳效果。<details>
<summary>Abstract</summary>
One of the most enticing research areas is the stock market, and projecting stock prices may help investors profit by making the best decisions at the correct time. Deep learning strategies have emerged as a critical technique in the field of the financial market. The stock market is impacted due to two aspects, one is the geo-political, social and global events on the bases of which the price trends could be affected. Meanwhile, the second aspect purely focuses on historical price trends and seasonality, allowing us to forecast stock prices. In this paper, our aim is to focus on the second aspect and build a model that predicts future prices with minimal errors. In order to provide better prediction results of stock price, we propose a new model named Long Short-Term Memory (LSTM) with Sequential Self-Attention Mechanism (LSTM-SSAM). Finally, we conduct extensive experiments on the three stock datasets: SBIN, HDFCBANK, and BANKBARODA. The experimental results prove the effectiveness and feasibility of the proposed model compared to existing models. The experimental findings demonstrate that the root-mean-squared error (RMSE), and R-square (R2) evaluation indicators are giving the best results.
</details>
<details>
<summary>摘要</summary>
一个非常吸引人的研究领域是股票市场，并且预测股票价格可以帮助投资者获得最佳的决策时机。深度学习策略在金融市场中得到了广泛的应用。股票市场受到两个方面的影响：一是地域政治、社会和全球事件的影响，这些事件可能影响股票价格走势。而第二个方面则专注于历史价格走势和季节性，我们可以通过预测股票价格。在这篇论文中，我们的目标是建立一个可预测股票价格的模型，并且使用新的长期记忆（LSTM）和顺序自我注意机制（LSTM-SSAM）来提高预测结果的准确性。最后，我们对三个股票数据集（SBIN、HDFCBANK和BANKBARODA）进行了广泛的实验，实验结果表明我们提出的模型比现有模型更有效果和可行性。实验结果表明，使用 Root-Mean-Squared Error（RMSE）和R-square（R2）评价指标，我们的模型在预测股票价格方面表现出色。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels"><a href="#Adaptive-Semi-Supervised-Segmentation-of-Brain-Vessels-with-Ambiguous-Labels" class="headerlink" title="Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels"></a>Adaptive Semi-Supervised Segmentation of Brain Vessels with Ambiguous Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03613">http://arxiv.org/abs/2308.03613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengming Lin, Yan Xia, Nishant Ravikumar, Qiongyao Liu, Michael MacRaild, Alejandro F Frangi</li>
<li>for: 这篇论文是为了准确分类脑血管而设计的。</li>
<li>methods: 这篇论文使用了进步式半监督学习、适应性训练策略和边界增强等新技术。</li>
<li>results: 实验结果显示，这篇论文的方法在3DRA数据集上实现了 mesh-based 分类 метри値的超越。它能够充分利用部分和暗箱注意的数据，进而实现了优秀的分类性能。<details>
<summary>Abstract</summary>
Accurate segmentation of brain vessels is crucial for cerebrovascular disease diagnosis and treatment. However, existing methods face challenges in capturing small vessels and handling datasets that are partially or ambiguously annotated. In this paper, we propose an adaptive semi-supervised approach to address these challenges. Our approach incorporates innovative techniques including progressive semi-supervised learning, adaptative training strategy, and boundary enhancement. Experimental results on 3DRA datasets demonstrate the superiority of our method in terms of mesh-based segmentation metrics. By leveraging the partially and ambiguously labeled data, which only annotates the main vessels, our method achieves impressive segmentation performance on mislabeled fine vessels, showcasing its potential for clinical applications.
</details>
<details>
<summary>摘要</summary>
<<SYS>>精准分割脑血管是脑血管疾病诊断和治疗中的关键。然而，现有方法在捕捉小血管和处理部分或杂乱标注的数据集时遇到困难。在这篇论文中，我们提出了一种适应式半supervised方法来解决这些问题。我们的方法包括进步式半supervised学习、适应性训练策略和边界增强等创新技术。在3DRA数据集上进行实验，我们的方法在基于网格的分割指标上表现出色。通过利用部分和杂乱标注的数据，我们的方法在偏移的细血管上实现了优秀的分割性能，这显示了它在临床应用中的潜力。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals"><a href="#A-machine-learning-sleep-wake-classification-model-using-a-reduced-number-of-features-derived-from-photoplethysmography-and-activity-signals" class="headerlink" title="A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals"></a>A machine-learning sleep-wake classification model using a reduced number of features derived from photoplethysmography and activity signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05759">http://arxiv.org/abs/2308.05759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Douglas A. Almeida, Felipe M. Dias, Marcelo A. F. Toledo, Diego A. C. Cardenas, Filipe A. C. Oliveira, Estela Ribeiro, Jose E. Krieger, Marco A. Gutierrez</li>
<li>for: 本研究旨在开发一种基于光谱学振荡分析的睡眠阶段分类模型，以提高睡眠质量和全身健康。</li>
<li>methods: 本研究使用了EXTREME GRADIENT BOOSTING（XGBoost）算法和PPG信号和活动计数特征进行睡眠阶段分类。</li>
<li>results: 本研究的方法与当前状态之册方法相比，敏感性为91.15 $\pm$ 1.16%, 特征选择率为53.66 $\pm$ 1.12%, F1分数为83.88 $\pm$ 0.56%, κ值为48.0 $\pm$ 0.86%。<details>
<summary>Abstract</summary>
Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 $\pm$ 1.16%, Specificity of 53.66 $\pm$ 1.12%, F1-score of 83.88 $\pm$ 0.56%, and Kappa of 48.0 $\pm$ 0.86%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power.
</details>
<details>
<summary>摘要</summary>
睡眠是我们身体和心理健康的重要组成部分。它对我们的情绪、记忆和认知功能以及身体的鲁棒性和免疫系统产生重要影响。睡眠阶段的分类是评估睡眠质量的必备步骤，它可以提供评估睡眠质量的度量，以及身体在这一期间的功能如何。聚光折射（PPG）已经被证明可以用于睡眠阶段推断，因此它可以单独使用或与其他信号结合使用来确定睡眠阶段。这些信息非常有价值，可以用于发现可能存在的睡眠问题，并开发改善睡眠质量和整体健康的策略。在这项工作中，我们提出了基于极限梯度提升（XGBoost）算法和PPG信号和活动计数特征的机器学习睡眠-醒目分类模型。我们的方法与当前状态的方法相比，表现出了相似的性能，具体来说，敏感性为91.15 $\pm$ 1.16%，特异性为53.66 $\pm$ 1.12%，F1分数为83.88 $\pm$ 0.56%，κ值为48.0 $\pm$ 0.86%。我们的方法在计算能力有限的穿戴设备中实现更加可行，因此它对现有的方法具有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search"><a href="#Generalized-Early-Stopping-in-Evolutionary-Direct-Policy-Search" class="headerlink" title="Generalized Early Stopping in Evolutionary Direct Policy Search"></a>Generalized Early Stopping in Evolutionary Direct Policy Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03574">http://arxiv.org/abs/2308.03574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anonreposit/gesp">https://github.com/anonreposit/gesp</a></li>
<li>paper_authors: Etor Arza, Leni K. Le Goff, Emma Hart</li>
<li>for: 这篇论文主要针对的是优化问题中的长时间评估时间问题，特别是在物理世界中进行评估，例如在机器人应用中。</li>
<li>methods: 本文提出了一个早期停止方法来解决这个问题，这个方法只需要在每个时间步骤中考虑目标值，不需要对问题本身进行具体的知识。</li>
<li>results: 根据五个来自游戏、机器人和 класи控制领域的直接政策搜寻环境中的测试，提出的早期停止条件可以节省大约75%的计算时间，并且与问题特有的停止条件相比，它表现更加稳定且更通用。<details>
<summary>Abstract</summary>
Lengthy evaluation times are common in many optimization problems such as direct policy search tasks, especially when they involve conducting evaluations in the physical world, e.g. in robotics applications. Often, when evaluating a solution over a fixed time period, it becomes clear that the objective value will not increase with additional computation time (for example, when a two-wheeled robot continuously spins on the spot). In such cases, it makes sense to stop the evaluation early to save computation time. However, most approaches to stop the evaluation are problem-specific and need to be specifically designed for the task at hand. Therefore, we propose an early stopping method for direct policy search. The proposed method only looks at the objective value at each time step and requires no problem-specific knowledge.   We test the introduced stopping criterion in five direct policy search environments drawn from games, robotics, and classic control domains, and show that it can save up to 75% of the computation time. We also compare it with problem-specific stopping criteria and demonstrate that it performs comparably while being more generally applicable.
</details>
<details>
<summary>摘要</summary>
长时间的评估时间是许多优化问题的常见现象，如直接策略搜索任务，尤其在物理世界中进行评估，例如在 robotics 应用中。经常情况下，在一定时间间评估解决方案时，会发现目标值不会随着计算时间增加（例如，一辆两轮摩托车连续旋转在一处）。在这些情况下，可以提前结束评估以避免浪费计算时间。然而，大多数止评估方法是任务特定的，需要特定的问题知识。因此，我们提出了一种止评估方法，只需要在每个时间步骤中考虑目标值即可，不需要任务特定的知识。我们在五个直接策略搜索环境中测试了引入的停止标准，这些环境来自游戏、 роботи克和 класси控制领域。我们发现，该方法可以将计算时间减少到75%。我们还与任务特定的停止标准进行比较，并证明它在通用性方面与其相当，而且更加通用。
</details></li>
</ul>
<hr>
<h2 id="When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection"><a href="#When-Federated-Learning-meets-Watermarking-A-Comprehensive-Overview-of-Techniques-for-Intellectual-Property-Protection" class="headerlink" title="When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection"></a>When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03573">http://arxiv.org/abs/2308.03573</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux</li>
<li>for: 本研究は、 Federated Learning（FL）の领域での标识技术に関するOverviewを提供します。</li>
<li>methods: 本研究では、FLの特有の制约に対応するために、DNN标识法の新しい挑戦と机会に焦点を当てています。</li>
<li>results: 本研究では、过去5年间におけるFL标识法の最新の进歩について详しく述べています。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need of centralizing their data. Among other advantages, it comes with privacy-preserving properties making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants' models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning (ML), DNN Watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, shedding light on the new challenges and opportunities that arise in this field.
</details>
<details>
<summary>摘要</summary>
《联合学习（Federated Learning，FL）技术 Allow multiple participants to collaboratively train a deep neural network (DNN) without centralizing their data. Among other advantages, it has privacy-preserving properties that make it attractive for sensitive applications, such as healthcare or the military. However, the training process requires sharing information about participants' models, making individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in machine learning (ML), DNN watermarking methods have been developed over the past five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in federated learning watermarking, highlighting the new challenges and opportunities that arise in this field.》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know and I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit"><a href="#Provably-Efficient-Learning-in-Partially-Observable-Contextual-Bandit" class="headerlink" title="Provably Efficient Learning in Partially Observable Contextual Bandit"></a>Provably Efficient Learning in Partially Observable Contextual Bandit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03572">http://arxiv.org/abs/2308.03572</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueping Gong, Jiheng Zhang</li>
<li>for: 这个研究探索了对受限知识和部分隐藏因素的偏见 Transfer Learning 在内部 Bandit 中，并研究了在这种情况下的 Casual 影响分析和估计错误。</li>
<li>methods: 本研究首先将问题转换为确定或偏见 Casual 效果的决策问题，并透过阶段性解决 Linear Programming 问题来获得 Casual 上下文范围内的统计误差。然后，我们运用这些 Sampling 算法来获得可靠的数据描述和估计误差。</li>
<li>results: 我们证明了我们的 Casually 增强的 Bandit 算法可以超越传统 Bandit 算法，并在任务中 Handle 通用 Context 分布时提高了训练速度和性能。此外，我们还进行了实验，证明了我们的策略在实际应用中比现有的方法更高效。<details>
<summary>Abstract</summary>
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions, our method improves the order dependence on function space size compared with previous literatures. We formally prove that our causally enhanced algorithms outperform classical bandit algorithms and achieve orders of magnitude faster convergence rates. Finally, we perform simulations that demonstrate the efficiency of our strategy compared to the current state-of-the-art methods. This research has the potential to enhance the performance of contextual bandit agents in real-world applications where data is scarce and costly to obtain.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了转移学习在部分可见Contextual Bandit中， где代理人具有其他代理人的有限知识以及隐藏的共同因素的局部信息。我们首先将问题转化为标识或部分标识 causal effect  между动作和奖励的优化问题。为解这些优化问题，我们将原始的不确定分布函数约化为线性约化，然后通过顺序解 linear program 来获取 causal bound ，考虑到估计误差。我们的抽样算法提供了desirable的收敛结果，并且我们可以用这些 causal bound 来改进经典bandit算法，从而影响行动集和函数空间的大小。我们正式证明我们的 causally enhanced 算法比经典bandit算法更高效，并且可以在函数近似任务中实现更高的速度比。最后，我们在实验中证明了我们的策略比现有的方法更高效。这些研究有望提高实际应用中的 Contextual Bandit 代理人性能，当数据稀缺和昂贵时。
</details></li>
</ul>
<hr>
<h2 id="Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data"><a href="#Partial-identification-of-kernel-based-two-sample-tests-with-mismeasured-data" class="headerlink" title="Partial identification of kernel based two sample tests with mismeasured data"></a>Partial identification of kernel based two sample tests with mismeasured data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03570">http://arxiv.org/abs/2308.03570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ron Nafshi, Maggie Makar</li>
<li>for: 本研究旨在针对机器学习应用中的两种分布之间的差异探究，并且 relax 了现有文献中假设有错误样本的假设。</li>
<li>methods: 本研究使用了非 Parametric 两种样本测试，包括最大均值差 (MMD)，并且研究了在 $\epsilon$ 污染下的 MMD 估计。</li>
<li>results: 本研究显示了在 $\epsilon$ 污染下，常用的 MMD 估计不可靠，而我们提出了一个方法来估计 MMD 的上下限，并证明这个方法会对 MMD 的估计进行更加精确的 bounds。使用了三个数据集，我们还证明了我们的方法比于其他方法更加稳定和有更好的性能。<details>
<summary>Abstract</summary>
Nonparametric two-sample tests such as the Maximum Mean Discrepancy (MMD) are often used to detect differences between two distributions in machine learning applications. However, the majority of existing literature assumes that error-free samples from the two distributions of interest are available.We relax this assumption and study the estimation of the MMD under $\epsilon$-contamination, where a possibly non-random $\epsilon$ proportion of one distribution is erroneously grouped with the other. We show that under $\epsilon$-contamination, the typical estimate of the MMD is unreliable. Instead, we study partial identification of the MMD, and characterize sharp upper and lower bounds that contain the true, unknown MMD. We propose a method to estimate these bounds, and show that it gives estimates that converge to the sharpest possible bounds on the MMD as sample size increases, with a convergence rate that is faster than alternative approaches. Using three datasets, we empirically validate that our approach is superior to the alternatives: it gives tight bounds with a low false coverage rate.
</details>
<details>
<summary>摘要</summary>
非 Parametric 两个样本测试，如最大均值差 (MMD)，在机器学习应用中 frequently 用于检测两个分布之间的差异。然而，现有的大多数文献假设可以获得无错的样本从两个分布的兴趣中。我们松弛这个假设，研究在 $\epsilon$-杂杂中测试 MMD 的估计，其中 $\epsilon$ 可能是非随机的。我们表明，在 $\epsilon$-杂杂中，通常的估计 MMD 不可靠。而我们研究 MMD 的部分标识，并Characterize 它们是否可以包含真实不知道的 MMD。我们提出了一种方法来估计这些 bound，并证明它们的估计会随样本大小增加，与其他方法相比，具有更快的收敛速率。使用三个数据集，我们实际验证了我们的方法的优越性：它们给出了紧凑的 bound，低于 false coverage 率。
</details></li>
</ul>
<hr>
<h2 id="A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment"><a href="#A-Transfer-Learning-Framework-for-Proactive-Ramp-Metering-Performance-Assessment" class="headerlink" title="A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment"></a>A Transfer Learning Framework for Proactive Ramp Metering Performance Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03542">http://arxiv.org/abs/2308.03542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Adrian Cottam, Mohammad Razaur Rahman Shaon, Yao-Jan Wu</li>
<li>for: 本研究旨在评估幕间计量系统的表现，并对交通管理策略的效iveness进行评估。</li>
<li>methods: 本研究采用了机器学习技术，通过学习before和after情况下的交通状况特征，对新的幕间计量控制策略的效iveness进行预测。</li>
<li>results: 实验结果表明，提出的框架可以成功地预测高速公路交通参数（速度、占用率和流速）的after情况，并可以作为评估幕间计量表现的alternative。<details>
<summary>Abstract</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.
</details>
<details>
<summary>摘要</summary>
Transportation agencies need to assess ramp metering performance when deploying or expanding a ramp metering system. The evaluation of a ramp metering strategy is primarily centered around examining its impact on freeway traffic mobility. One way these effects can be explored is by comparing traffic states, such as the speed before and after the ramp metering strategy has been altered. Predicting freeway traffic states for the after scenarios following the implementation of a new ramp metering control strategy could offer valuable insights into the potential effectiveness of the target strategy. However, the use of machine learning methods in predicting the freeway traffic state for the after scenarios and evaluating the effectiveness of transportation policies or traffic control strategies such as ramp metering is somewhat limited in the current literature. To bridge the research gap, this study presents a framework for predicting freeway traffic parameters (speed, occupancy, and flow rate) for the after situations when a new ramp metering control strategy is implemented. By learning the association between the spatial-temporal features of traffic states in before and after situations for known freeway segments, the proposed framework can transfer this learning to predict the traffic parameters for new freeway segments. The proposed framework is built upon a transfer learning model. Experimental results show that the proposed framework is feasible for use as an alternative for predicting freeway traffic parameters to proactively evaluate ramp metering performance.Here's the text in Simplified Chinese:交通管理机构需要评估干涉表计划的性能，当部署或扩展干涉表计划时。评估干涉表计划的策略的中心在于研究它们对高速公路交通流动性的影响。一种可以探索这些影响的方法是通过比较交通状态的速度之前和之后干涉表计划的变化。预测高速公路交通状态的后 Situations 可以提供有价值的预测干涉表计划的效果。然而，现有文献中使用机器学习方法预测高速公路交通状态的后 Situations 和评估交通政策或交通控制策略的效果是有限的。为了填补这个研究漏洞，本研究提出了一个框架，用于预测高速公路交通参数（速度、占用率和流速）的后 Situations。该框架基于转移学习模型，可以通过学习知道的高速公路段的空间-时间特征，将其传递到预测新的高速公路段的交通参数。实验结果表明，该框架是可行的，可以作为评估干涉表计划性能的代替方法。
</details></li>
</ul>
<hr>
<h2 id="On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework"><a href="#On-ramp-and-Off-ramp-Traffic-Flows-Estimation-Based-on-A-Data-driven-Transfer-Learning-Framework" class="headerlink" title="On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework"></a>On-ramp and Off-ramp Traffic Flows Estimation Based on A Data-driven Transfer Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03538">http://arxiv.org/abs/2308.03538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaobo Ma, Abolfazl Karimpour, Yao-Jan Wu<br>for: 本研究旨在提供一种数据驱动的框架，以便准确估算 freeway 上匝入和出口的流量。methods: 该框架使用了传输学习模型，该模型可以在不同的交通 Pattern、分布和特征下提供高精度的流量估算。results: 实验结果表明，提案的方法可以在不同的 freeway 上匝入和出口处提供高精度的流量估算，其中流量估算的平均绝对误差在 23.90 veh&#x2F;h 到 40.85 veh&#x2F;h 之间，root mean square error 在 34.55 veh&#x2F;h 到 57.77 veh&#x2F;h 之间。此外，相比 conventinal machine learning model，提案的方法显示更高的表现。<details>
<summary>Abstract</summary>
To develop the most appropriate control strategy and monitor, maintain, and evaluate the traffic performance of the freeway weaving areas, state and local Departments of Transportation need to have access to traffic flows at each pair of on-ramp and off-ramp. However, ramp flows are not always readily available to transportation agencies and little effort has been made to estimate these missing flows in locations where no physical sensors are installed. To bridge this research gap, a data-driven framework is proposed that can accurately estimate the missing ramp flows by solely using data collected from loop detectors on freeway mainlines. The proposed framework employs a transfer learning model. The transfer learning model relaxes the assumption that the underlying data distributions of the source and target domains must be the same. Therefore, the proposed framework can guarantee high-accuracy estimation of on-ramp and off-ramp flows on freeways with different traffic patterns, distributions, and characteristics. Based on the experimental results, the flow estimation mean absolute errors range between 23.90 veh/h to 40.85 veh/h for on-ramps, and 31.58 veh/h to 45.31 veh/h for off-ramps; the flow estimation root mean square errors range between 34.55 veh/h to 57.77 veh/h for on-ramps, and 41.75 veh/h to 58.80 veh/h for off-ramps. Further, the comparison analysis shows that the proposed framework outperforms other conventional machine learning models. The estimated ramp flows based on the proposed method can help transportation agencies to enhance the operations of their ramp control strategies for locations where physical sensors are not installed.
</details>
<details>
<summary>摘要</summary>
要开发最佳的控制策略和监测、维护和评估高速公路叉车区的交通性能，国家和地方交通厅需要有访问每对进口和出口的交通流量数据。然而，进口和出口流量并不总是可以提供给交通厅，而且过去几乎没有尝试估算这些缺失的流量。为了填补这一研究漏洞，我们提出了一种数据驱动的框架，可以准确地估算缺失的进口和出口流量，只使用高速公路主线上的循环探测器数据。我们的框架采用了传输学习模型，这种模型不需要源和目标领域数据分布之间的假设相同。因此，我们的框架可以 garantizar高精度地估算进口和出口流量，并且在不同的交通模式、分布和特点下具有广泛的应用可能性。根据实验结果，估算的进口和出口流量平均绝对误差在23.90辆/小时到40.85辆/小时之间，Root mean square error在34.55辆/小时到57.77辆/小时之间。此外，比较分析表明，我们的方法在其他 convential机器学习模型的基础上具有更高的性能。估算的进口和出口流量可以帮助交通厅在没有物理探测器的情况下提高叉车控制策略的运行效果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Feature-Learning-for-Wireless-Spectrum-Data"><a href="#Deep-Feature-Learning-for-Wireless-Spectrum-Data" class="headerlink" title="Deep Feature Learning for Wireless Spectrum Data"></a>Deep Feature Learning for Wireless Spectrum Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03530">http://arxiv.org/abs/2308.03530</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ljupcho Milosheski, Gregor Cerar, Blaž Bertalanič, Carolina Fortuna, Mihael Mohorčič</li>
<li>for: 本研究旨在自动学习无supervision的Feature表示，用于无线传输卷积 clustering。</li>
<li>methods: 我们提出一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，比基eline PCA 减少99.3%的维度。</li>
<li>results: 我们的自动表示学习可以提取细腻的含义块，而基eline只能通过背景噪声来分类数据。<details>
<summary>Abstract</summary>
In recent years, the traditional feature engineering process for training machine learning models is being automated by the feature extraction layers integrated in deep learning architectures. In wireless networks, many studies were conducted in automatic learning of feature representations for domain-related challenges. However, most of the existing works assume some supervision along the learning process by using labels to optimize the model. In this paper, we investigate an approach to learning feature representations for wireless transmission clustering in a completely unsupervised manner, i.e. requiring no labels in the process. We propose a model based on convolutional neural networks that automatically learns a reduced dimensionality representation of the input data with 99.3% less components compared to a baseline principal component analysis (PCA). We show that the automatic representation learning is able to extract fine-grained clusters containing the shapes of the wireless transmission bursts, while the baseline enables only general separability of the data based on the background noise.
</details>
<details>
<summary>摘要</summary>
近年来，传统的特征工程过程为训练机器学习模型被深度学习架构中的特征提取层自动化。在无线网络中，许多研究都是自动学习领域相关挑战的特征表示。然而，大多数现有的工作假设了学习过程中有监督，通过标签来优化模型。在这篇论文中，我们调查了一种没有监督的方法，即无标签的自动特征表示学习方法，用于无线传输协调。我们提议一种基于卷积神经网络的模型，可以自动学习输入数据的减少维度表示，与基准PCA相比，减少了99.3%的特征量。我们显示了自动特征表示学习能够提取无线传输强度波形细腻的分布，而基准只能基于背景噪声进行概括分离。
</details></li>
</ul>
<hr>
<h2 id="AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning"><a href="#AlphaStar-Unplugged-Large-Scale-Offline-Reinforcement-Learning" class="headerlink" title="AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning"></a>AlphaStar Unplugged: Large-Scale Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03526">http://arxiv.org/abs/2308.03526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michaël Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray Jiang, Tom Le Paine, Richard Powell, Konrad Żołna, Julian Schrittwieser, David Choi, Petko Georgiev, Daniel Toyama, Aja Huang, Roman Ring, Igor Babuschkin, Timo Ewalds, Mahyar Bordbar, Sarah Henderson, Sergio Gómez Colmenarejo, Aäron van den Oord, Wojciech Marian Czarnecki, Nando de Freitas, Oriol Vinyals</li>
<li>for: This paper is written to advance offline reinforcement learning algorithms by leveraging the challenging and realistic environment of StarCraft II.</li>
<li>methods: The paper introduces a new benchmark called AlphaStar Unplugged, which includes a dataset, tools, and an evaluation protocol for offline reinforcement learning. The authors also present baseline agents, including behavior cloning, offline variants of actor-critic, and MuZero.</li>
<li>results: The authors achieve a 90% win rate against a previously published AlphaStar behavior cloning agent using only offline data, improving the state of the art of agents using offline data.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了提高偏置学习算法的进步，利用星际II的挑战性和实际性。</li>
<li>methods: 论文引入了一个新的基准点，叫做AlphaStar Unplugged，包括一个数据集、工具和评估协议。作者还提供了一些基线代理，如行为做 clone、偏置 variant 和 MuZero。</li>
<li>results: 作者使用仅偏置数据达到了90%的胜率，超过了之前发表的AlphaStar行为做 clone 代理。<details>
<summary>Abstract</summary>
StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90% win rate against previously published AlphaStar behavior cloning agent.
</details>
<details>
<summary>摘要</summary>
星际II是一个非常具有挑战性的模拟增强学习环境之一，它是部分可见、随机、多代、需要在长时间 horizon 上进行策略规划，并且需要在实时低级别执行。此外，它还拥有活跃的职业竞赛场景。由于星际II的挑战性和Blizzard公司发布了数百万场星际II游戏记录，因此这个纸使用这些数据来建立了一个标准的基准，称为AlphaStar Unplugged，并在这个基准上引入了前所未有的挑战。我们定义了一个数据集（Blizzard发布的一个子集）、工具和标准化API для机器学习方法，以及评估协议。我们还提供了基线代理，包括行为快照、离线actor-critic和MuZero等。我们使用仅基于离线数据的方法提高了代理的状态，并达到了在之前发布的AlphaStar行为快照代理90%的赢利率。
</details></li>
</ul>
<hr>
<h2 id="Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field"><a href="#Worker-Activity-Recognition-in-Manufacturing-Line-Using-Near-body-Electric-Field" class="headerlink" title="Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field"></a>Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03514">http://arxiv.org/abs/2308.03514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sungho Suh, Vitor Fortes Rey, Sizhen Bian, Yu-Chi Huang, Jože M. Rožanec, Hooman Tavakoli Ghinani, Bo Zhou, Paul Lukowicz</li>
<li>for: 提高生产效率和产品质量</li>
<li>methods:  combining IMU和body capacitance sensing modules，以及多渠道时序卷积神经网络和深度卷积LSTM的早期和晚期整合方法</li>
<li>results: 在生产线上测试和采集感知器数据后，提议的硬件和神经网络模型显示出优于基eline方法的性能，表明该方法在现实世界应用中具有潜在的潜力。此外，通过加入身体电容感测模块和特征融合方法，提议的感知原型在6.35%的提升和9.38%的高于基eline方法的macro F1分数上表现出了提升。<details>
<summary>Abstract</summary>
Manufacturing industries strive to improve production efficiency and product quality by deploying advanced sensing and control systems. Wearable sensors are emerging as a promising solution for achieving this goal, as they can provide continuous and unobtrusive monitoring of workers' activities in the manufacturing line. This paper presents a novel wearable sensing prototype that combines IMU and body capacitance sensing modules to recognize worker activities in the manufacturing line. To handle these multimodal sensor data, we propose and compare early, and late sensor data fusion approaches for multi-channel time-series convolutional neural networks and deep convolutional LSTM. We evaluate the proposed hardware and neural network model by collecting and annotating sensor data using the proposed sensing prototype and Apple Watches in the testbed of the manufacturing line. Experimental results demonstrate that our proposed methods achieve superior performance compared to the baseline methods, indicating the potential of the proposed approach for real-world applications in manufacturing industries. Furthermore, the proposed sensing prototype with a body capacitive sensor and feature fusion method improves by 6.35%, yielding a 9.38% higher macro F1 score than the proposed sensing prototype without a body capacitive sensor and Apple Watch data, respectively.
</details>
<details>
<summary>摘要</summary>
制造业为提高生产效率和产品质量而努力，投入先进的感知和控制系统。舌环感器是制造业实现这一目标的一种有前途的解决方案，因为它们可以提供不间断和不干扰的工作者活动监测。本文提出了一种新的舌环感器原型， combining IMU和体容感测模块，以认知制造线工作者的活动。为处理这些多modal的感知数据，我们提议并比较早期和晚期感知数据融合方法，用于多通道时序卷积神经网络和深度卷积LSTM。我们通过收集和标注感知数据使用我们提出的感知原型和Apple Watches在制造线测试环境中进行评估。实验结果表明，我们的提议方法可以与基准方法相比，表明我们的方法在实际应用中具有潜在的潜力。此外，我们的感知原型与身体电容感测模块和特征融合方法提高了6.35%，即使比起没有身体电容感测模块和Apple Watch数据的情况下提高9.38%的macro F1分数。
</details></li>
</ul>
<hr>
<h2 id="A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings"><a href="#A-data-driven-approach-to-predict-decision-point-choice-during-normal-and-evacuation-wayfinding-in-multi-story-buildings" class="headerlink" title="A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings"></a>A data-driven approach to predict decision point choice during normal and evacuation wayfinding in multi-story buildings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03511">http://arxiv.org/abs/2308.03511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yan Feng, Panchamy Krishnakumari</li>
<li>For: 本研究旨在理解和预测在复杂建筑物内的人行道径选择行为，以确保行人安全。* Methods: 本研究使用了数据驱动的方法，包括建立内部网络表示和将VR坐标映射到内部表示的技术，以及一种已知的机器学习算法——Random Forest（RF）模型，来预测行人决策点选择行为。* Results: 研究发现，使用RF模型可以具有较高的预测准确率（平均为93%），比对使用logistic regression模型更高。最高的预测准确率达96%，并且测试结果表明，个人特征不会影响决策点选择。<details>
<summary>Abstract</summary>
Understanding pedestrian route choice behavior in complex buildings is important to ensure pedestrian safety. Previous studies have mostly used traditional data collection methods and discrete choice modeling to understand the influence of different factors on pedestrian route and exit choice, particularly in simple indoor environments. However, research on pedestrian route choice in complex buildings is still limited. This paper presents a data-driven approach for understanding and predicting the pedestrian decision point choice during normal and emergency wayfinding in a multi-story building. For this, we first built an indoor network representation and proposed a data mapping technique to map VR coordinates to the indoor representation. We then used a well-established machine learning algorithm, namely the random forest (RF) model to predict pedestrian decision point choice along a route during four wayfinding tasks in a multi-story building. Pedestrian behavioral data in a multi-story building was collected by a Virtual Reality experiment. The results show a much higher prediction accuracy of decision points using the RF model (i.e., 93% on average) compared to the logistic regression model. The highest prediction accuracy was 96% for task 3. Additionally, we tested the model performance combining personal characteristics and we found that personal characteristics did not affect decision point choice. This paper demonstrates the potential of applying a machine learning algorithm to study pedestrian route choice behavior in complex indoor buildings.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用机器学习算法来理解人行道选择行为在复杂的内部建筑中非常重要，以确保行人安全。先前的研究主要采用传统的数据采集方法和精确选择模型来理解不同因素对人行道径和出口选择的影响，特别是在简单的室内环境中。然而，对于复杂的内部建筑中的人行道选择研究仍然有限。这篇文章介绍了一种数据驱动的方法，用于理解和预测行人决策点选择在常规和紧急导航中的多层建筑中。为此，我们首先构建了内部网络表示，并提出了将VR坐标映射到内部表示的技术。然后，我们使用一种已知的机器学习算法，即随机森林（RF）模型来预测行人决策点选择路径中的四个任务在多层建筑中。我们在内部建筑中收集了行人行为数据，并使用VR实验进行数据采集。结果显示，使用RF模型的预测精度远高于逻辑回归模型（即93%的平均精度），最高的预测精度为任务3（即96%）。此外，我们测试了模型性能的组合个人特征，并发现个人特征没有影响决策点选择。这篇文章表明了使用机器学习算法来研究复杂的内部建筑中人行道选择行为的潜力。
</details></li>
</ul>
<hr>
<h2 id="Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group"><a href="#Balanced-Face-Dataset-Guiding-StyleGAN-to-Generate-Labeled-Synthetic-Face-Image-Dataset-for-Underrepresented-Group" class="headerlink" title="Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group"></a>Balanced Face Dataset: Guiding StyleGAN to Generate Labeled Synthetic Face Image Dataset for Underrepresented Group</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03495">http://arxiv.org/abs/2308.03495</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kidist Amde Mekonnen</li>
<li>for: 这个研究的目的是生成一个可靠的面部图像集，以便实现不同的人种和性别的资料分布。</li>
<li>methods: 这个研究使用了StyleGAN模型来生成面部图像，并通过控制生成过程来确保资料集中的人种和性别分布均衡。</li>
<li>results: 这个研究产生了一个可靠的面部图像集，并通过实验证明了这个资料集可以用于不同的下游任务。<details>
<summary>Abstract</summary>
For a machine learning model to generalize effectively to unseen data within a particular problem domain, it is well-understood that the data needs to be of sufficient size and representative of real-world scenarios. Nonetheless, real-world datasets frequently have overrepresented and underrepresented groups. One solution to mitigate bias in machine learning is to leverage a diverse and representative dataset. Training a model on a dataset that covers all demographics is crucial to reducing bias in machine learning. However, collecting and labeling large-scale datasets has been challenging, prompting the use of synthetic data generation and active labeling to decrease the costs of manual labeling. The focus of this study was to generate a robust face image dataset using the StyleGAN model. In order to achieve a balanced distribution of the dataset among different demographic groups, a synthetic dataset was created by controlling the generation process of StyleGaN and annotated for different downstream tasks.
</details>
<details>
<summary>摘要</summary>
为一个机器学习模型有效泛化到未看到的数据中，已经是非常了解的一点，那么数据需要具有足够的大小和表示现实世界场景。然而，实际世界数据经常会有过度和不足的群体。一种解决机器学习偏见的方法是利用多样化的和表示性的数据集。训练一个模型在覆盖所有民族的数据集是减少偏见的关键，但是收集和手动标注大规模数据集是困难的，因此使用生成 Synthetic 数据和活动标注来降低手动标注的成本。这项研究的目标是使用 StyleGAN 模型生成一个可靠的脸像数据集，以实现数据集的均衡分布。为了实现这一目标，我们控制了 StyleGAN 生成过程，并对不同下游任务进行了注释。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection"><a href="#Exploring-the-Physical-World-Adversarial-Robustness-of-Vehicle-Detection" class="headerlink" title="Exploring the Physical World Adversarial Robustness of Vehicle Detection"></a>Exploring the Physical World Adversarial Robustness of Vehicle Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03476">http://arxiv.org/abs/2308.03476</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Jiang, Tianyuan Zhang, Shuangcheng Liu, Weiyu Ji, Zichao Zhang, Gang Xiao</li>
<li>for: 评估实际场景下的检测模型 Robustness，但实际 эксперимент具有资源占用和复杂性的问题。</li>
<li>methods: 提出了一种创新的快照水平数据生成管道，使用 CARLA 模拟器，并建立了 Discrete and Continuous Instant-level (DCI) 数据集，可以进行三种检测模型和三种物理攻击的全面实验。</li>
<li>results: 发现 Yolo v6 具有强大的抗击性，仅在对抗攻击下出现了小于1%的 average precision (AP) 下降，而 ASA 攻击则导致了 AP 下降约14.51%，远高于其他算法。 static 场景下的识别 AP 值较高，而不同的天气条件下的结果几乎相同。<details>
<summary>Abstract</summary>
Adversarial attacks can compromise the robustness of real-world detection models. However, evaluating these models under real-world conditions poses challenges due to resource-intensive experiments. Virtual simulations offer an alternative, but the absence of standardized benchmarks hampers progress. Addressing this, we propose an innovative instant-level data generation pipeline using the CARLA simulator. Through this pipeline, we establish the Discrete and Continuous Instant-level (DCI) dataset, enabling comprehensive experiments involving three detection models and three physical adversarial attacks. Our findings highlight diverse model performances under adversarial conditions. Yolo v6 demonstrates remarkable resilience, experiencing just a marginal 6.59% average drop in average precision (AP). In contrast, the ASA attack yields a substantial 14.51% average AP reduction, twice the effect of other algorithms. We also note that static scenes yield higher recognition AP values, and outcomes remain relatively consistent across varying weather conditions. Intriguingly, our study suggests that advancements in adversarial attack algorithms may be approaching its ``limitation''.In summary, our work underscores the significance of adversarial attacks in real-world contexts and introduces the DCI dataset as a versatile benchmark. Our findings provide valuable insights for enhancing the robustness of detection models and offer guidance for future research endeavors in the realm of adversarial attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure"><a href="#How-to-forecast-power-generation-in-wind-farms-Insights-from-leveraging-hierarchical-structure" class="headerlink" title="How to forecast power generation in wind farms? Insights from leveraging hierarchical structure"></a>How to forecast power generation in wind farms? Insights from leveraging hierarchical structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03472">http://arxiv.org/abs/2308.03472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas English, Mahdi Abolghasemi</li>
<li>for: 预测可再生能源生产，帮助决策全球减排。</li>
<li>methods: 使用树状层次预测和整合预测，包括线性回归和梯度提升机器学习。</li>
<li>results: 跨时间层次预测超过具体时间层次预测，并且机器学习模型在大多数层次上表现较好。<details>
<summary>Abstract</summary>
Forecasting of renewable energy generation provides key insights which may help with decision-making towards global decarbonisation. Renewable energy generation can often be represented through cross-sectional hierarchies, whereby a single farm may have multiple individual generators. Hierarchical forecasting through reconciliation has demonstrated a significant increase in the quality of forecasts both theoretically and empirically. However, it is not evident whether forecasts generated by individual temporal and cross-sectional aggregation can be superior to integrated cross-temporal forecasts and to individual forecasts on more granular data. In this study, we investigate the accuracies of different cross-sectional and cross-temporal reconciliation methods using both linear regression and gradient boosting machine learning for forecasting wind farm power generation. We found that cross-temporal reconciliation is superior to individual cross-sectional reconciliation at multiple temporal aggregations. Cross-temporally reconciled machine learning base forecasts also demonstrated a high accuracy at coarser temporal granularities, which may encourage adoption for short-term wind forecasts. We also show that linear regression can outperform machine learning models across most levels in cross-sectional wind time series.
</details>
<details>
<summary>摘要</summary>
预测可再生能源生产提供关键的洞察，可以帮助决策全球减排。可再生能源生产经常可以用 Hierarchical 模型来表示，一个农场可能有多个个体发电机。通过协调预测，可以显著提高预测质量， both theoretically and empirically。然而，不知道个体时间和横向汇总预测是否高于集成时间汇总预测和更细的数据预测。本研究发现，不同的横向和时间汇总协调方法的准确率，使用线性回归和梯度提升机器学习模型预测风电厂电力生产。我们发现，协调预测在多个时间层次上都高于个体横向协调预测。同时，使用机器学习模型进行协调预测也在大部分水平上达到了高准确率，这可能会促进短期风预测的采用。此外，我们还发现了线性回归在大部分水平上可以超越机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Wide-Gaps-and-Clustering-Axioms"><a href="#Wide-Gaps-and-Clustering-Axioms" class="headerlink" title="Wide Gaps and Clustering Axioms"></a>Wide Gaps and Clustering Axioms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03464">http://arxiv.org/abs/2308.03464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mieczysław A. Kłopotek</li>
<li>for: 本研究旨在探讨k-means算法是否遵循克林伯格的距离基于减少算法的axiomaatic系统，以及如何修正k-means算法以遵循这些axioma。</li>
<li>methods: 本研究使用了两个新的clusterability性质：variational k-separability和residual k-separability，以确定k-means算法在欧几何或非欧几何空间中是否遵循克林伯格的一致性axioma。此外，本研究还提出了修正k-means算法以遵循克林伯格的贫含性axioma的方法。</li>
<li>results: 本研究发现，k-means算法在欧几何和非欧几何空间中都不遵循克林伯格的一致性axioma，但可以通过修正k-means算法来遵循这些axioma。此外，本研究还提出了一种构建测试数据集的方法，以便测试修正后的k-means算法的性能。<details>
<summary>Abstract</summary>
The widely applied k-means algorithm produces clusterings that violate our expectations with respect to high/low similarity/density and is in conflict with Kleinberg's axiomatic system for distance based clustering algorithms that formalizes those expectations in a natural way. k-means violates in particular the consistency axiom. We hypothesise that this clash is due to the not explicated expectation that the data themselves should have the property of being clusterable in order to expect the algorithm clustering hem to fit a clustering axiomatic system. To demonstrate this, we introduce two new clusterability properties, variational k-separability and residual k-separability and show that then the Kleinberg's consistency axiom holds for k-means operating in the Euclidean or non-Euclidean space. Furthermore, we propose extensions of k-means algorithm that fit approximately the Kleinberg's richness axiom that does not hold for k-means. In this way, we reconcile k-means with Kleinberg's axiomatic framework in Euclidean and non-Euclidean settings. Besides contribution to the theory of axiomatic frameworks of clustering and for clusterability theory, practical contribution is the possibility to construct {datasets for testing purposes of algorithms optimizing k-means cost function. This includes a method of construction of {clusterable data with known in advance global optimum.
</details>
<details>
<summary>摘要</summary>
广泛应用的k-means算法会生成不符我们的预期的分群结果，特别是高低相似度和密度方面的预期不符，这与克莱恩贝格的分群算法axiomaatic系统不符。k-means特别违反了一致性axioma。我们假设这是因为没有明确地预期数据本身应有分群的性质，以期望算法可以遵循分群axiomaatic系统。为了证明这一点，我们引入了两个新的分群性质：variational k-separability和residual k-separability，并证明了在欧几何或非欧几何空间中，k-means算法会遵循克莱恩贝格的一致性axioma。此外，我们提出了对k-means算法进行修改，以便更好地遵循克莱恩贝格的丰富性axioma，这些axioma不会在k-means算法中实现。这种修改可以在欧几何和非欧几何空间中进行。此外，我们还可以根据这些修改建立{测试用数据集，以便确认分群算法的效果。包括一种方法建立高度分群的数据集，并且知道在先的全局最佳解。}
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers"><a href="#High-Resolution-Cranial-Defect-Reconstruction-by-Iterative-Low-Resolution-Point-Cloud-Completion-Transformers" class="headerlink" title="High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers"></a>High-Resolution Cranial Defect Reconstruction by Iterative, Low-Resolution, Point Cloud Completion Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03813">http://arxiv.org/abs/2308.03813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/MWod/DeepImplant_MICCAI_2023">https://github.com/MWod/DeepImplant_MICCAI_2023</a></li>
<li>paper_authors: Marek Wodzinski, Mateusz Daniol, Daria Hemmerling, Miroslaw Socha</li>
<li>for:  This paper aims to provide an automatic, dedicated system for personalized cranial reconstruction, addressing the problem of cranial defect reconstruction.</li>
<li>methods:  The proposed method uses an iterative, transformer-based approach to complete point clouds and reconstruct cranial defects at any resolution, while being fast and resource-efficient during training and inference.</li>
<li>results:  The proposed method demonstrates superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects, compared to state-of-the-art volumetric approaches.<details>
<summary>Abstract</summary>
Each year thousands of people suffer from various types of cranial injuries and require personalized implants whose manual design is expensive and time-consuming. Therefore, an automatic, dedicated system to increase the availability of personalized cranial reconstruction is highly desirable. The problem of the automatic cranial defect reconstruction can be formulated as the shape completion task and solved using dedicated deep networks. Currently, the most common approach is to use the volumetric representation and apply deep networks dedicated to image segmentation. However, this approach has several limitations and does not scale well into high-resolution volumes, nor takes into account the data sparsity. In our work, we reformulate the problem into a point cloud completion task. We propose an iterative, transformer-based method to reconstruct the cranial defect at any resolution while also being fast and resource-efficient during training and inference. We compare the proposed methods to the state-of-the-art volumetric approaches and show superior performance in terms of GPU memory consumption while maintaining high-quality of the reconstructed defects.
</details>
<details>
<summary>摘要</summary>
每年千计人们因各种类型的脑部受伤而需要个性化嵌入，其手动设计成本高昂，时间费时。因此，一个自动化、专门的系统可以提高个性化脑部重建的可用性是非常感兴趣的。脑部异常完成任务可以通过专门的深度网络解决。现在，最常见的方法是使用体积表示法，并将深度网络应用于图像分割。然而，这种方法存在一些限制，不能扩展到高分辨率体积，也不会考虑数据稀缺性。在我们的工作中，我们将问题重新划分为点云完成任务。我们提出一种迭代、基于变换器的方法来重建脑部异常，可以在任何分辨率下进行重建，同时在训练和推理过程中具有快速和资源高效的特点。我们与状态元方法进行比较，并显示我们的方法在GPU内存占用量方面具有明显的优势，而无需牺牲高质量重建异常的性能。
</details></li>
</ul>
<hr>
<h2 id="Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images"><a href="#Redesigning-Out-of-Distribution-Detection-on-3D-Medical-Images" class="headerlink" title="Redesigning Out-of-Distribution Detection on 3D Medical Images"></a>Redesigning Out-of-Distribution Detection on 3D Medical Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.07324">http://arxiv.org/abs/2308.07324</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Vasiliuk, Daria Frolova, Mikhail Belyaev, Boris Shirokikh</li>
<li>for: 本文旨在解决验证医疗图像分割中的非常规（OOD）样本检测问题。</li>
<li>methods: 本文提出一种基于下游任务的OOD检测方法，使用下游模型的性能作为图像之间的pseudometric，不需要显式地区分ID和OOD样本。</li>
<li>results: 在11个CT和MRI OOD检测挑战中，EPD metric 能够准确地评估不同方法的临床影响。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) samples for trusted medical image segmentation remains a significant challenge. The critical issue here is the lack of a strict definition of abnormal data, which often results in artificial problem settings without measurable clinical impact. In this paper, we redesign the OOD detection problem according to the specifics of volumetric medical imaging and related downstream tasks (e.g., segmentation). We propose using the downstream model's performance as a pseudometric between images to define abnormal samples. This approach enables us to weigh different samples based on their performance impact without an explicit ID/OOD distinction. We incorporate this weighting in a new metric called Expected Performance Drop (EPD). EPD is our core contribution to the new problem design, allowing us to rank methods based on their clinical impact. We demonstrate the effectiveness of EPD-based evaluation in 11 CT and MRI OOD detection challenges.
</details>
<details>
<summary>摘要</summary>
检测非常出版（OOD）样本 для可信度医疗影像分割是一个主要挑战。这里的关键问题是缺乏严格的非常定义，这经常导致人工设定的问题 без measurable clinical impact。在这篇论文中，我们重新设计了OOD检测问题，根据医疗影像的特点和相关的下游任务（例如分割）。我们提议使用下游模型的性能作为图像之间的pseudometric。这种方法允许我们根据不同样本的性能影响 assign weights，而不需要显式的ID/OOD分类。我们称之为预期性能下降（EPD）。EPD是我们对新的问题设计的核心贡献，允许我们根据临床影响排名方法。我们在11个CT和MRI OOD检测挑战中证明了EPD基于评价的效果。
</details></li>
</ul>
<hr>
<h2 id="Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data"><a href="#Cross-Silo-Prototypical-Calibration-for-Federated-Learning-with-Non-IID-Data" class="headerlink" title="Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data"></a>Cross-Silo Prototypical Calibration for Federated Learning with Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03457">http://arxiv.org/abs/2308.03457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qizhuang-qz/FedCSPC">https://github.com/qizhuang-qz/FedCSPC</a></li>
<li>paper_authors: Zhuang Qi, Lei Meng, Zitan Chen, Han Hu, Hui Lin, Xiangxu Meng</li>
<li>for: 这个研究目的是为了在隐私保护下，通过服务器端对各个客户端的本地模型进行联合学习，以获得更好的模型泛化能力。</li>
<li>methods: 这个研究使用了跨批训练（FedCSPC），它首先使用资料prototype模型（DPM）模组来学习资料模式，以帮助标准化。接着，它使用跨批对称学习（CSPC）模组来改善标准化的实现方式，以将不同来源的特征投射到一个共同的空间中，保持清晰的决策界限。</li>
<li>results: 实验结果显示，FedCSPC可以在不同资料来源之间的同类别中学习共同的特征，从而获得更好的性能，比预先的方法更好。<details>
<summary>Abstract</summary>
Federated Learning aims to learn a global model on the server side that generalizes to all clients in a privacy-preserving manner, by leveraging the local models from different clients. Existing solutions focus on either regularizing the objective functions among clients or improving the aggregation mechanism for the improved model generalization capability. However, their performance is typically limited by the dataset biases, such as the heterogeneous data distributions and the missing classes. To address this issue, this paper presents a cross-silo prototypical calibration method (FedCSPC), which takes additional prototype information from the clients to learn a unified feature space on the server side. Specifically, FedCSPC first employs the Data Prototypical Modeling (DPM) module to learn data patterns via clustering to aid calibration. Subsequently, the cross-silo prototypical calibration (CSPC) module develops an augmented contrastive learning method to improve the robustness of the calibration, which can effectively project cross-source features into a consistent space while maintaining clear decision boundaries. Moreover, the CSPC module's ease of implementation and plug-and-play characteristics make it even more remarkable. Experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study, and the results verified that FedCSPC is capable of learning the consistent features across different data sources of the same class under the guidance of calibrated model, which leads to better performance than the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedCSPC.
</details>
<details>
<summary>摘要</summary>
federated 学习旨在在服务器端学习一个通用模型，以保持所有客户端的隐私，通过客户端的本地模型之间的协同学习。现有的解决方案通常是通过客户端对象函数的规范化或者改进模型融合机制来提高模型通用能力。然而，它们的性能通常受到数据偏见的影响，如不同数据分布和缺失类。为解决这个问题，本文提出了跨批模型准确补偿方法（FedCSPC），它在服务器端使用客户端提供的额外原型信息来学习一个统一的特征空间。具体来说，FedCSPC首先使用数据模型化模块（DPM）来学习数据模式，以帮助准确补偿。接着，跨批模型准确补偿模块（CSPC）发展了一种增强了对比学习方法，可以有效地将各种来源特征投影到一个具有清晰决策边界的共同空间中，而不是只是在不同数据源之间进行准确补偿。此外，CSPC模块的易于实现和插件化特点使得它更加出佩。实验结果表明，FedCSPC能够在不同数据源之间学习一致的特征，从而实现更好的性能，而且超越了当前的方法。代码已经在 GitHub 上发布，请参考 <https://github.com/qizhuang-qz/FedCSPC>。
</details></li>
</ul>
<hr>
<h2 id="Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces"><a href="#Doubly-Robust-Estimator-for-Off-Policy-Evaluation-with-Large-Action-Spaces" class="headerlink" title="Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces"></a>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03443">http://arxiv.org/abs/2308.03443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tatsu432/DR-estimator-OPE-large-action">https://github.com/tatsu432/DR-estimator-OPE-large-action</a></li>
<li>paper_authors: Tatsuhiro Shimizu, Laura Forastiere</li>
<li>for: 评估无策策略（Off-Policy Evaluation）在Contextual Bandit设置下，即在具有大量动作空间的情况下。</li>
<li>methods: 本研究使用Marginalized Inverse Propensity Scoring（MIPS）和Marginalized Doubly Robust（MDR）等方法来mitigate estimator的偏差和方差问题。</li>
<li>results:  theoretically和empirically验证了MDR estimator的超越性，即在比MIPS更加强的假设下，MDR estimator still maintains variance reduction against IPS。<details>
<summary>Abstract</summary>
We study Off-Policy Evaluation (OPE) in contextual bandit settings with large action spaces. The benchmark estimators suffer from severe bias and variance tradeoffs. Parametric approaches suffer from bias due to difficulty specifying the correct model, whereas ones with importance weight suffer from variance. To overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was proposed to mitigate the estimator's variance via embeddings of an action. To make the estimator more accurate, we propose the doubly robust estimator of MIPS called the Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the proposed estimator is unbiased under weaker assumptions than MIPS while maintaining variance reduction against IPS, which was the main advantage of MIPS. The empirical experiment verifies the supremacy of MDR against existing estimators.
</details>
<details>
<summary>摘要</summary>
我们研究偏离策略评估（OPE）在含有大量行动的上下文抽象队列设置下。参考估计器受到严重的偏见和方差负担交易。 Parametric方法受到偏见因为难以正确地特定模型，而重要性Weighted方法受到方差。为了解决这些限制，我们提出了嵌入行动的Marginalized Inverse Propensity Scoring（MIPS）来减少估计器的方差。为了使估计器更加准确，我们提出了MIPS的双重Robust（MDR）估计器。理论分析表明，我们的估计器具有较弱的假设下的不偏性，而且保持与IPS相同的方差减少。实验证明我们的MDR估计器在现有估计器中具有最高的超越性。
</details></li>
</ul>
<hr>
<h2 id="PURL-Safe-and-Effective-Sanitization-of-Link-Decoration"><a href="#PURL-Safe-and-Effective-Sanitization-of-Link-Decoration" class="headerlink" title="PURL: Safe and Effective Sanitization of Link Decoration"></a>PURL: Safe and Effective Sanitization of Link Decoration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03417">http://arxiv.org/abs/2308.03417</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/purl-sanitizer/purl">https://github.com/purl-sanitizer/purl</a></li>
<li>paper_authors: Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby</li>
<li>for: 防止隐私浏览器中的第三方cookies和浏览器指纹被新的跟踪方法绕过安全措施。</li>
<li>methods: 使用机器学习方法，利用浏览器执行页面的跨层图表来检测和净化链接装饰中的跟踪信息。</li>
<li>results: PURL可以准确地检测和净化链接装饰中的跟踪信息，比已有Countermeasures更高效和更具有抗辐射性。在测试 top-million 网站时，发现了广泛的链接装饰滥用，其中包括著名的广告商和跟踪者，以便从浏览器存储、邮箱地址和指纹扫描中收集用户信息。<details>
<summary>Abstract</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.
</details>
<details>
<summary>摘要</summary>
While privacy-focused browsers have taken steps to block third-party cookies and browser fingerprinting, novel tracking methods that bypass existing defenses continue to emerge. Since trackers need to exfiltrate information from the client- to server-side through link decoration regardless of the tracking technique they employ, a promising orthogonal approach is to detect and sanitize tracking information in decorated links. We present PURL, a machine-learning approach that leverages a cross-layer graph representation of webpage execution to safely and effectively sanitize link decoration. Our evaluation shows that PURL significantly outperforms existing countermeasures in terms of accuracy and reducing website breakage while being robust to common evasion techniques. We use PURL to perform a measurement study on top-million websites. We find that link decorations are widely abused by well-known advertisers and trackers to exfiltrate user information collected from browser storage, email addresses, and scripts involved in fingerprinting.Here's the translation in Traditional Chinese:随着隐私专注浏览器对第三方Cookie和浏览器指纹进行防护，新的跟踪方法继续出现，这些方法可以绕过现有的防护措施。因为追踪者需要从客户端将资讯传到服务器端，因此一个可能的垂直方法是检测并删除装饰链接中的追踪资讯。我们提出了PURL，一种基于页面执行的机器学习方法，可以安全地和有效地删除链接装饰。我们的评估显示，PURL Significantly Outperform现有的对抗策略，并且具有较高的精度和降低网站损坏的能力，同时具有对常见的逃脱技巧的坚固性。我们使用PURL进行了顶千个网站的量测研究，发现链接装饰被著名的广告商和追踪者广泛运用，以将用户资讯从浏览器存储、电子邮件地址和掌握的指纹资讯泄露出来。
</details></li>
</ul>
<hr>
<h2 id="Noncompact-uniform-universal-approximation"><a href="#Noncompact-uniform-universal-approximation" class="headerlink" title="Noncompact uniform universal approximation"></a>Noncompact uniform universal approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03812">http://arxiv.org/abs/2308.03812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teun D. H. van Nuland</li>
<li>for: 这个论文探讨了一般化的universal approximation theorem的推广，具体来说是在非可ompact的输入空间 $\mathbb R^n$ 上进行的。</li>
<li>methods: 这个论文使用了神经网络来进行uniform approximation，并且研究了不同 activation functions 的影响。</li>
<li>results: 研究发现，对于所有的连续函数，只要它们在 infinity 处消失， Then all continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. In addition, the paper also found some unexpected results, such as the algebra of uniformly approximable functions being independent of the activation function and the number of hidden layers.<details>
<summary>Abstract</summary>
The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) is independent of $\varphi$ and $l$, and equals the closed span of products of sigmoids composed with one-dimensional projections. If the left limit of $\varphi$ equals its right limit, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq1$) equals the (real part of the) commutative resolvent algebra, a C*-algebra which is used in mathematical approaches to quantum theory. In the latter case, the algebra is independent of $l\geq1$, whereas in the former case $\overline{\mathcal{N}_\varphi^2(\mathbb R^n)}$ is strictly bigger than $\overline{\mathcal{N}_\varphi^1(\mathbb R^n)}$.
</details>
<details>
<summary>摘要</summary>
“ universal approximation theorem 被推广到非紧集 $\mathbb R^n$ 上的输入空间。所有的连续函数，当 $x$ 趋于 $\pm \infty$ 时，有 asymptotically linear 的行为的函数 $\varphi\neq0$，可以通过含有一个隐藏层的神经网络进行不同化。当 $\varphi$ 又是受限的时，我们可以准确地确定可以通过神经网络进行不同化的函数，并且得到了以下意外的结果。对于所有的 $n$ 和 $l\geq2$，$\mathcal{N}^l_\varphi(\mathbb R^n)$ 是一个点wise乘法的代数。如果 $\varphi$ 的左限与右限不同（例如，当 $\varphi$ 是截股函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq2$) 是 $\varphi$ 和 $l$ 的独立的代数，等于一个由截股函数与一维投影组成的关闭 span。如果 $\varphi$ 的左限与右限相同（例如，当 $\varphi$ 是满足条件的函数），则 $\mathcal{N}^l_\varphi(\mathbb R^n)$ ($l\geq1$) 是一个（实部的） коммуutatvie 分解代数，这种代数在数学方面的量子理论中使用。在后者情况下，该代数是 $l\geq1$ 的独立的，而在前者情况下，$\mathcal{N}^2_\varphi(\mathbb R^n)$ 是 $\mathcal{N}^1_\varphi(\mathbb R^n)$ 的strictly larger。”
</details></li>
</ul>
<hr>
<h2 id="Applied-metamodelling-for-ATM-performance-simulations"><a href="#Applied-metamodelling-for-ATM-performance-simulations" class="headerlink" title="Applied metamodelling for ATM performance simulations"></a>Applied metamodelling for ATM performance simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03404">http://arxiv.org/abs/2308.03404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoffer Riis, Francisco N. Antunes, Tatjana Bolić, Gérald Gurtner, Andrew Cook, Carlos Lima Azevedo, Francisco Câmara Pereira</li>
<li>for: 支持空管决策making，提高空管 simulator的解释能力和可repeatability。</li>
<li>methods: 使用活动学习和 SHAP 值 integrate into simulation metamodels，快速浮现空管 simulator中输入和输出变量之间的隐藏关系。</li>
<li>results: 在使用 ‘Mercury’ 空管 simulator的实际场景中，XALM 能够增强simulation解释性和理解变量之间的交互关系，并且与非活动学习 мета模型相比，具有更好的解释能力。<details>
<summary>Abstract</summary>
The use of Air traffic management (ATM) simulators for planing and operations can be challenging due to their modelling complexity. This paper presents XALM (eXplainable Active Learning Metamodel), a three-step framework integrating active learning and SHAP (SHapley Additive exPlanations) values into simulation metamodels for supporting ATM decision-making. XALM efficiently uncovers hidden relationships among input and output variables in ATM simulators, those usually of interest in policy analysis. Our experiments show XALM's predictive performance comparable to the XGBoost metamodel with fewer simulations. Additionally, XALM exhibits superior explanatory capabilities compared to non-active learning metamodels.   Using the `Mercury' (flight and passenger) ATM simulator, XALM is applied to a real-world scenario in Paris Charles de Gaulle airport, extending an arrival manager's range and scope by analysing six variables. This case study illustrates XALM's effectiveness in enhancing simulation interpretability and understanding variable interactions. By addressing computational challenges and improving explainability, XALM complements traditional simulation-based analyses.   Lastly, we discuss two practical approaches for reducing the computational burden of the metamodelling further: we introduce a stopping criterion for active learning based on the inherent uncertainty of the metamodel, and we show how the simulations used for the metamodel can be reused across key performance indicators, thus decreasing the overall number of simulations needed.
</details>
<details>
<summary>摘要</summary>
使用空交通管理（ATM）模拟器进行规划和运营可能会面临模拟复杂性挑战。这篇论文介绍了XALM（可解释性活动学习元模型），一种三步框架，它将活动学习和SHAP（SHapley Additive exPlanations）值 integrate到模拟元模型中，以支持ATM决策。XALM有效地揭示了ATM模拟器中输入和输出变量之间的隐藏关系，通常在政策分析中对于有价值。我们的实验表明，XALM的预测性能与XGBoost元模型相当，但使用 fewer simulations。此外，XALM的解释能力比非活动学习元模型更高。使用“Mercury”（飞机和乘客）ATM模拟器，XALM在法国 CHARLES DE GAULLE机场的一个实际场景中应用，分析了六个变量。这个案例说明了XALM在提高模拟解释性和理解变量间关系方面的效iveness。通过解决计算挑战和提高解释性，XALM补充了传统基于模拟的分析。最后，我们介绍了两种实用的方法来降低元模型计算的压力：我们引入基于元模型的活动学习停止 criterion，以及如何将模拟用于元模型可以重用到关键性能指标上，从而降低总的模拟数量。
</details></li>
</ul>
<hr>
<h2 id="Towards-Machine-Learning-based-Fish-Stock-Assessment"><a href="#Towards-Machine-Learning-based-Fish-Stock-Assessment" class="headerlink" title="Towards Machine Learning-based Fish Stock Assessment"></a>Towards Machine Learning-based Fish Stock Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03403">http://arxiv.org/abs/2308.03403</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Lüdtke, Maria E. Pierce</li>
<li>for: This paper aims to improve the estimation and forecast of fish stock parameters like recruitment and spawning stock biomass, which is crucial for sustainable fisheries management.</li>
<li>methods: The authors propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. The model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy.</li>
<li>results: The authors experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.<details>
<summary>Abstract</summary>
The accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised ML, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the ML model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.
</details>
<details>
<summary>摘要</summary>
Accurate assessment of fish stocks is crucial for sustainable fisheries management. However, existing statistical stock assessment models can have low forecast performance of relevant stock parameters like recruitment or spawning stock biomass, especially in ecosystems that are changing due to global warming and other anthropogenic stressors. In this paper, we investigate the use of machine learning models to improve the estimation and forecast of such stock parameters. We propose a hybrid model that combines classical statistical stock assessment models with supervised machine learning, specifically gradient boosted trees. Our hybrid model leverages the initial estimate provided by the classical model and uses the machine learning model to make a post-hoc correction to improve accuracy. We experiment with five different stocks and find that the forecast accuracy of recruitment and spawning stock biomass improves considerably in most cases.Here's the text with some notes on the translation:* "accurate assessment" is 准确评估 (zhèngjù píngzhèng)* "fish stocks" is 鱼类资源 (yúlèi zīyuán)* "sustainable fisheries management" is 可持续鱼业管理 (kěchéngxù yúyèguǎn lí)* "existing statistical stock assessment models" is 现有的统计鱼类评估模型 (xiàn yǒu de tōngjī yúlèi píngzhèng módel)* "global warming" is 全球变暖 (quánqiú biàndòng)* "anthropogenic stressors" is 人类活动对鱼类资源的影响 (rénxìng huódòng duì yúlèi zīyuán de yìngxiàn)* "machine learning models" is 机器学习模型 (jīshì xuéxí módel)* "hybrid model" is 混合模型 (fù hé módel)* "classical statistical stock assessment models" is 传统的统计鱼类评估模型 (chuán tiān de tōngjī yúlèi píngzhèng módel)* "post-hoc correction" is 后置 corrections (hòu zhì jièduì)* "recruitment" is 增殖 (zēngshòu)* "spawning stock biomass" is 繁殖床质量 (shèngcháng zhīyù)I hope this helps! Let me know if you have any further questions or if you'd like me to translate anything else.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network"><a href="#Enhancing-Nucleus-Segmentation-with-HARU-Net-A-Hybrid-Attention-Based-Residual-U-Blocks-Network" class="headerlink" title="Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network"></a>Enhancing Nucleus Segmentation with HARU-Net: A Hybrid Attention Based Residual U-Blocks Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03382">http://arxiv.org/abs/2308.03382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junzhou Chen, Qian Huang, Yulin Chen, Linyi Qian, Chengyuan Yu</li>
<li>for: 本研究旨在提出一种基于二支分支网络和卷积块的杂合注意力的核实例分割方法，以提高核实例分割的精度和效率。</li>
<li>methods: 本方法使用了hybrid attention based residual U-blocks和context fusion block (CF-block)来同时预测目标信息和目标极值。CF-block可以有效地抽取和融合网络中的Contextual information。</li>
<li>results: 对于BNS、MoNuSeg、CoNSeg和CPM-17等数据集，实验结果表明，提出的方法比state-of-the-art方法具有更高的性能。<details>
<summary>Abstract</summary>
Nucleus image segmentation is a crucial step in the analysis, pathological diagnosis, and classification, which heavily relies on the quality of nucleus segmentation. However, the complexity of issues such as variations in nucleus size, blurred nucleus contours, uneven staining, cell clustering, and overlapping cells poses significant challenges. Current methods for nucleus segmentation primarily rely on nuclear morphology or contour-based approaches. Nuclear morphology-based methods exhibit limited generalization ability and struggle to effectively predict irregular-shaped nuclei, while contour-based extraction methods face challenges in accurately segmenting overlapping nuclei. To address the aforementioned issues, we propose a dual-branch network using hybrid attention based residual U-blocks for nucleus instance segmentation. The network simultaneously predicts target information and target contours. Additionally, we introduce a post-processing method that combines the target information and target contours to distinguish overlapping nuclei and generate an instance segmentation image. Within the network, we propose a context fusion block (CF-block) that effectively extracts and merges contextual information from the network. Extensive quantitative evaluations are conducted to assess the performance of our method. Experimental results demonstrate the superior performance of the proposed method compared to state-of-the-art approaches on the BNS, MoNuSeg, CoNSeg, and CPM-17 datasets.
</details>
<details>
<summary>摘要</summary>
核心图像分割是生物学分析、病理诊断和分类中的关键步骤，其中核心分割质量直接影响分析结果。然而，核心的变化、杂乱的核心边缘、不均匀染色、细胞堆叠和重叠细胞等问题带来了 significiant challenges。现有的核心分割方法主要基于核心形态或边缘提取方法。核心形态基于方法在面对不规则形状的核心时表现有限的泛化能力，而边缘提取方法在处理重叠的核心时遇到困难。为了解决上述问题，我们提出了一种基于 dual-branch 网络和卷积 residual U-块的核心实例分割方法。该网络同时预测目标信息和目标边缘。此外，我们引入了一种将目标信息和目标边缘结合的后处理方法，以便在重叠的核心之间分割。在网络中，我们提出了一种 Context Fusion Block（CF-块），用于有效地提取和融合网络中的Contextual information。我们对方法进行了广泛的量化评估，实验结果表明我们的方法在 BNS、MoNuSeg、CoNSeg 和 CPM-17 数据集上的性能较为出色。
</details></li>
</ul>
<hr>
<h2 id="A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding"><a href="#A-reading-survey-on-adversarial-machine-learning-Adversarial-attacks-and-their-understanding" class="headerlink" title="A reading survey on adversarial machine learning: Adversarial attacks and their understanding"></a>A reading survey on adversarial machine learning: Adversarial attacks and their understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03363">http://arxiv.org/abs/2308.03363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan</li>
<li>for: 本研究旨在系统地探讨针对神经网络的攻击方法和其对神经网络的影响。</li>
<li>methods: 本文使用了现有的攻击方法和对它们的分类，以及对攻击方法的分析和评估。</li>
<li>results: 本文提供了对现有攻击方法的系统性梳理和分析，以及对攻击方法的限制和不足。In English, this means:</li>
<li>for: This research aims to systematically explore the attack methods against neural networks and their impact on them.</li>
<li>methods: This paper uses existing attack methods and classifies them, as well as analyzing and assessing their limitations and drawbacks.</li>
<li>results: This paper provides a systematic overview of existing attack methods, along with their limitations and shortcomings.<details>
<summary>Abstract</summary>
Deep Learning has empowered us to train neural networks for complex data with high performance. However, with the growing research, several vulnerabilities in neural networks have been exposed. A particular branch of research, Adversarial Machine Learning, exploits and understands some of the vulnerabilities that cause the neural networks to misclassify for near original input. A class of algorithms called adversarial attacks is proposed to make the neural networks misclassify for various tasks in different domains. With the extensive and growing research in adversarial attacks, it is crucial to understand the classification of adversarial attacks. This will help us understand the vulnerabilities in a systematic order and help us to mitigate the effects of adversarial attacks. This article provides a survey of existing adversarial attacks and their understanding based on different perspectives. We also provide a brief overview of existing adversarial defences and their limitations in mitigating the effect of adversarial attacks. Further, we conclude with a discussion on the future research directions in the field of adversarial machine learning.
</details>
<details>
<summary>摘要</summary>
深度学习已经赋予我们训练复杂数据的神经网络高性能。然而，随着研究的增长，一些神经网络的漏洞已经被揭露出来。一个特定的研究分支——对抗机器学习——利用和理解神经网络的漏洞，使其在近似原始输入下错分类。一类called adversarial attacks的算法被提议用来使神经网络在不同领域中的多种任务中错分类。随着对抗机器学习的广泛和快速发展，我们需要系统地了解抗击攻击的分类。这将帮助我们系统地了解漏洞，并帮助我们 mitigate the effects of adversarial attacks。本文提供了现有的抗击攻击和其基于不同角度的理解。我们还提供了对抗攻击的简要概述和其限制在 mitigate the effect of adversarial attacks。最后，我们 conclude with a discussion on the future research directions in the field of adversarial machine learning。
</details></li>
</ul>
<hr>
<h2 id="Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks"><a href="#Solving-Falkner-Skan-type-equations-via-Legendre-and-Chebyshev-Neural-Blocks" class="headerlink" title="Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks"></a>Solving Falkner-Skan type equations via Legendre and Chebyshev Neural Blocks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03337">http://arxiv.org/abs/2308.03337</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Afzal Aghaei, Kourosh Parand, Ali Nikkhah, Shakila Jaberi</li>
<li>for: 解决非线性法克内尔-斯坦方程</li>
<li>methods: 使用Legendre和Chebyshev神经块，利用orthogonal polynomials在神经网络中提高人工神经网络的近似能力</li>
<li>results: 通过模拟不同的法克内尔-斯坦方程配置，实现提高算法的效率<details>
<summary>Abstract</summary>
In this paper, a new deep-learning architecture for solving the non-linear Falkner-Skan equation is proposed. Using Legendre and Chebyshev neural blocks, this approach shows how orthogonal polynomials can be used in neural networks to increase the approximation capability of artificial neural networks. In addition, utilizing the mathematical properties of these functions, we overcome the computational complexity of the backpropagation algorithm by using the operational matrices of the derivative. The efficiency of the proposed method is carried out by simulating various configurations of the Falkner-Skan equation.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，一种新的深度学习架构用于解决非线性法克纳-斯坦方程被提出。通过使用列朋德和Chebychev神经块，这种方法示出了在人工神经网络中使用正交多项式以提高神经网络的近似能力。此外，利用这些函数的数学性质，我们超越了反向传播算法的计算复杂性，使用操作矩阵的导数。我们对不同的法克纳-斯坦方程配置进行了效率测试。
</details></li>
</ul>
<hr>
<h2 id="Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions"><a href="#Non-Convex-Bilevel-Optimization-with-Time-Varying-Objective-Functions" class="headerlink" title="Non-Convex Bilevel Optimization with Time-Varying Objective Functions"></a>Non-Convex Bilevel Optimization with Time-Varying Objective Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03811">http://arxiv.org/abs/2308.03811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sen Lin, Daouda Sow, Kaiyi Ji, Yingbin Liang, Ness Shroff</li>
<li>for: 这个研究是针对在线机器学习问题中的两层优化问题（online bilevel optimization，OBO），尤其是在流动数据和时间变化函数下进行优化。</li>
<li>methods: 我们提出了一个单回路线上的网络均值优化器（SOBOW），通过缓存中的窗口均值来更新外层决策。相比于现有的算法，SOBOW更加computationally efficient，并且不需要知道前一Function。</li>
<li>results: 我们显示了SOBOW可以在线机器学习问题中实现低度的两层本地遗憾（bilevel local regret），并且在多个领域进行了广泛的实验，证明了其效果。<details>
<summary>Abstract</summary>
Bilevel optimization has become a powerful tool in a wide variety of machine learning problems. However, the current nonconvex bilevel optimization considers an offline dataset and static functions, which may not work well in emerging online applications with streaming data and time-varying functions. In this work, we study online bilevel optimization (OBO) where the functions can be time-varying and the agent continuously updates the decisions with online streaming data. To deal with the function variations and the unavailability of the true hypergradients in OBO, we propose a single-loop online bilevel optimizer with window averaging (SOBOW), which updates the outer-level decision based on a window average of the most recent hypergradient estimations stored in the memory. Compared to existing algorithms, SOBOW is computationally efficient and does not need to know previous functions. To handle the unique technical difficulties rooted in single-loop update and function variations for OBO, we develop a novel analytical technique that disentangles the complex couplings between decision variables, and carefully controls the hypergradient estimation error. We show that SOBOW can achieve a sublinear bilevel local regret under mild conditions. Extensive experiments across multiple domains corroborate the effectiveness of SOBOW.
</details>
<details>
<summary>摘要</summary>
双层优化已成为许多机器学习问题的重要工具。但是现有的非对称双层优化方法假设数据集和函数是静态的，这可能无法适用于emerging的在线应用程序中，尤其是过去的数据和函数都是时间变化的。在这个研究中，我们研究线上双层优化（OBO），其中函数可以是时间变化的，并且代理人持续更新决策以上线实时数据。对于函数变化和真实对数 gradient 的不可知道问题，我们提出了单轮线上双层优化器（SOBOW），它更新外层决策基于最近的对数gradient 估计中的窗口平均值。相比于现有的算法，SOBOW 具有较低的计算成本和不需要知道前一代函数。对于单轮更新和函数变化带来的困难，我们开发了一种新的分析方法，将决策变量分解为独立的部分，并且精确地控制对数gradient 估计误差。我们显示SOBOW 可以在某些条件下实现双层本地 regret 的下图数学。实验结果显示SOBOW 在多个领域中具有优秀的效能。
</details></li>
</ul>
<hr>
<h2 id="Expediting-Neural-Network-Verification-via-Network-Reduction"><a href="#Expediting-Neural-Network-Verification-via-Network-Reduction" class="headerlink" title="Expediting Neural Network Verification via Network Reduction"></a>Expediting Neural Network Verification via Network Reduction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03330">http://arxiv.org/abs/2308.03330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuyi Zhong, Ruiwei Wang, Siau-Cheng Khoo</li>
<li>for: 验证深度神经网络的安全性 properties</li>
<li>methods: 提议了多种验证方法来验证深度神经网络是否正确工作，但是许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。本文提出了一种网络减少技术作为验证前置处理方法。</li>
<li>results: 我们的实验结果表明，提议的减少技术可以有效地减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还显示，网络减少可以提高现有验证工具对许多网络的可用性。<details>
<summary>Abstract</summary>
A wide range of verification methods have been proposed to verify the safety properties of deep neural networks ensuring that the networks function correctly in critical applications. However, many well-known verification tools still struggle with complicated network architectures and large network sizes. In this work, we propose a network reduction technique as a pre-processing method prior to verification. The proposed method reduces neural networks via eliminating stable ReLU neurons, and transforming them into a sequential neural network consisting of ReLU and Affine layers which can be handled by the most verification tools. We instantiate the reduction technique on the state-of-the-art complete and incomplete verification tools, including alpha-beta-crown, VeriNet and PRIMA. Our experiments on a large set of benchmarks indicate that the proposed technique can significantly reduce neural networks and speed up existing verification tools. Furthermore, the experiment results also show that network reduction can improve the availability of existing verification tools on many networks by reducing them into sequential neural networks.
</details>
<details>
<summary>摘要</summary>
各种验证方法已经提议以验证深度神经网络的安全性属性，以确保神经网络在关键应用中正确工作。然而，许多知名的验证工具仍然无法处理复杂的网络架构和大型网络。在这种情况下，我们提议一种网络减少技术作为预处理方法。我们的提议方法通过消除稳定的ReLU神经元并将其转换成一个顺序神经网络，包括ReLU和Affine层，可以由大多数验证工具处理。我们在 alpha-beta-crown、VeriNet 和 PRIMA 等完整和部分验证工具上实现了减少技术，并在一个大量的 benchmark 上进行了实验。实验结果表明，我们的提议方法可以显著减少神经网络，并使现有的验证工具更快速地处理神经网络。此外，实验结果还表明，网络减少可以提高现有验证工具对许多网络的可用性。
</details></li>
</ul>
<hr>
<h2 id="AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework"><a href="#AFN-Adaptive-Fusion-Normalization-via-Encoder-Decoder-Framework" class="headerlink" title="AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework"></a>AFN: Adaptive Fusion Normalization via Encoder-Decoder Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03321">http://arxiv.org/abs/2308.03321</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huanranchen/ASRNorm">https://github.com/huanranchen/ASRNorm</a></li>
<li>paper_authors: Zikai Zhou, Huanran Chen</li>
<li>for: 这篇论文主要是针对深度学习的正常化层进行研究，并提出了一种新的正常化函数 called Adaptive Fusion Normalization (AFN)。</li>
<li>methods: 论文使用了多种正常化函数，包括Batch Normalization (BatchNorm)、Instance Normalization (InstanceNorm) 和 Weight Normalization (WeightNorm)，并评估了这些函数在领域扩大和图像识别 зада务中的表现。</li>
<li>results: 实验结果显示，AFN 在领域扩大和图像识别任务中表现较好，并且超过了先前的正常化技术。<details>
<summary>Abstract</summary>
The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
</details>
<details>
<summary>摘要</summary>
成功的深度学习与归一化层无可分割。研究人员已经提出了多种归一化函数，每种归一化函数各有优点和缺点。为了设计一个综合归一化函数，并且 Mitigate their weaknesses。我们还提出了一种新的归一化函数called Adaptive Fusion Normalization（AFN）。通过实验，我们证明AFN在领域总体化和图像识别任务中超过了之前的归一化技术。
</details></li>
</ul>
<hr>
<h2 id="Binary-Federated-Learning-with-Client-Level-Differential-Privacy"><a href="#Binary-Federated-Learning-with-Client-Level-Differential-Privacy" class="headerlink" title="Binary Federated Learning with Client-Level Differential Privacy"></a>Binary Federated Learning with Client-Level Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03320">http://arxiv.org/abs/2308.03320</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lumin Liu, Jun Zhang, Shenghui Song, Khaled B. Letaief</li>
<li>for: 提高 federated learning（FL）系统的隐私保护和性能。</li>
<li>methods: 采用 binary neural networks（BNNs）和离散噪声来实现客户端级别的隐私保护，并且引入离散噪声以实现隐私保护。</li>
<li>results: 实验结果基于 MNIST 和 Fashion-MNIST 数据集表明，提议的训练算法可以实现客户端级别的隐私保护，同时具有低通信开销和性能提升。<details>
<summary>Abstract</summary>
Federated learning (FL) is a privacy-preserving collaborative learning framework, and differential privacy can be applied to further enhance its privacy protection. Existing FL systems typically adopt Federated Average (FedAvg) as the training algorithm and implement differential privacy with a Gaussian mechanism. However, the inherent privacy-utility trade-off in these systems severely degrades the training performance if a tight privacy budget is enforced. Besides, the Gaussian mechanism requires model weights to be of high-precision. To improve communication efficiency and achieve a better privacy-utility trade-off, we propose a communication-efficient FL training algorithm with differential privacy guarantee. Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的协同学习框架，可以进一步增强其隐私保护。现有的 FL 系统通常采用 Federated Average (FedAvg) 作为训练算法，并在这之前实现了分布式隐私保护。然而，这些系统中的隐私Utility 质量耗尽会严重降低训练性能，而且 Gaussian 机制需要模型参数的高精度。为了提高通信效率和实现更好的隐私Utility 质量，我们提议一种通信效率高的 FL 训练算法，并提供了隐私保证。 Specifically, we propose to adopt binary neural networks (BNNs) and introduce discrete noise in the FL setting. Binary model parameters are uploaded for higher communication efficiency and discrete noise is added to achieve the client-level differential privacy protection. The achieved performance guarantee is rigorously proved, and it is shown to depend on the level of discrete noise. Experimental results based on MNIST and Fashion-MNIST datasets will demonstrate that the proposed training algorithm achieves client-level privacy protection with performance gain while enjoying the benefits of low communication overhead from binary model updates.Note: The translation is done using Google Translate and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method"><a href="#HomOpt-A-Homotopy-Based-Hyperparameter-Optimization-Method" class="headerlink" title="HomOpt: A Homotopy-Based Hyperparameter Optimization Method"></a>HomOpt: A Homotopy-Based Hyperparameter Optimization Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03317">http://arxiv.org/abs/2308.03317</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jeffkinnison/shadho">https://github.com/jeffkinnison/shadho</a></li>
<li>paper_authors: Sophia J. Abraham, Kehelwala D. G. Maduranga, Jeffery Kinnison, Zachariah Carmichael, Jonathan D. Hauenstein, Walter J. Scheirer</li>
<li>for: 提高机器学习模型的性能和效率，通过优化超参数。</li>
<li>methods: 使用一种基于泛函模型（GAM）的数据驱动方法，与哈洛彩优化（Homotopy Optimization）相结合，以提高优化方法的效率和精度。</li>
<li>results: 在多种优化技术（如随机搜索、TPE、权重平衡和SMAC）中，使用HomOpt方法可以提高目标性能，并在多个标准机器学习 benchmark 和开放集任务中达到更高的性能。<details>
<summary>Abstract</summary>
Machine learning has achieved remarkable success over the past couple of decades, often attributed to a combination of algorithmic innovations and the availability of high-quality data available at scale. However, a third critical component is the fine-tuning of hyperparameters, which plays a pivotal role in achieving optimal model performance. Despite its significance, hyperparameter optimization (HPO) remains a challenging task for several reasons. Many HPO techniques rely on naive search methods or assume that the loss function is smooth and continuous, which may not always be the case. Traditional methods, like grid search and Bayesian optimization, often struggle to quickly adapt and efficiently search the loss landscape. Grid search is computationally expensive, while Bayesian optimization can be slow to prime. Since the search space for HPO is frequently high-dimensional and non-convex, it is often challenging to efficiently find a global minimum. Moreover, optimal hyperparameters can be sensitive to the specific dataset or task, further complicating the search process. To address these issues, we propose a new hyperparameter optimization method, HomOpt, using a data-driven approach based on a generalized additive model (GAM) surrogate combined with homotopy optimization. This strategy augments established optimization methodologies to boost the performance and effectiveness of any given method with faster convergence to the optimum on continuous, discrete, and categorical domain spaces. We compare the effectiveness of HomOpt applied to multiple optimization techniques (e.g., Random Search, TPE, Bayes, and SMAC) showing improved objective performance on many standardized machine learning benchmarks and challenging open-set recognition tasks.
</details>
<details>
<summary>摘要</summary>
为解决这些问题，我们提出了一种新的HPO方法，即HomOpt，使用基于一般添加模型（GAM）函数的数据驱动方法，并与幂等优化结合。这种策略可以增强现有优化方法的性能和有效性，并在连续、离散和分类的域空间上快速 converges to the optimum。我们将HomOpt应用于多种优化技术（如随机搜索、TPE、Bayes和SMAC），并对多个标准机器学习benchmark和复杂的开放任务进行比较，显示HomOpt可以提高优化目标性能。
</details></li>
</ul>
<hr>
<h2 id="Deep-Q-Network-for-Stochastic-Process-Environments"><a href="#Deep-Q-Network-for-Stochastic-Process-Environments" class="headerlink" title="Deep Q-Network for Stochastic Process Environments"></a>Deep Q-Network for Stochastic Process Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03316">http://arxiv.org/abs/2308.03316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kuangheng He</li>
<li>for: 本研究旨在应用强化学习方法在Stochastic Process环境中训练优化策略，包括Flappy Bird和新开发的股票交易环境作为案例研究。</li>
<li>methods: 本研究使用Deep Q-learning网络，评估不同结构的网络，并评估适合Stochastic Process环境的最佳variant。</li>
<li>results: 本研究获得了各种策略的成果，包括Flappy Bird和股票交易环境中的策略。同时，本研究还讨论了环境建立和强化学习技术的现有挑战和可能的改进方案。<details>
<summary>Abstract</summary>
Reinforcement learning is a powerful approach for training an optimal policy to solve complex problems in a given system. This project aims to demonstrate the application of reinforcement learning in stochastic process environments with missing information, using Flappy Bird and a newly developed stock trading environment as case studies. We evaluate various structures of Deep Q-learning networks and identify the most suitable variant for the stochastic process environment. Additionally, we discuss the current challenges and propose potential improvements for further work in environment-building and reinforcement learning techniques.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过补偿学习训练优化策略来解决复杂系统中的问题，这个项目旨在通过缺失信息的随机过程环境中应用强化学习，使用Flappy Bird和一个新开发的股票交易环境作为案例研究。我们评估了不同结构的深度Q学习网络，并确定适用于随机过程环境的最佳变体。此外，我们还讨论了当前挑战和可能的改进方法，用于环境建构和强化学习技术。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide that instead.
</details></li>
</ul>
<hr>
<h2 id="Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics"><a href="#Symmetry-Preserving-Program-Representations-for-Learning-Code-Semantics" class="headerlink" title="Symmetry-Preserving Program Representations for Learning Code Semantics"></a>Symmetry-Preserving Program Representations for Learning Code Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03312">http://arxiv.org/abs/2308.03312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kexin Pei, Weichen Li, Qirui Jin, Shuyang Liu, Scott Geng, Lorenzo Cavallaro, Junfeng Yang, Suman Jana</li>
<li>for: 该论文旨在提高自动程序理解的能力，以便进行安全任务。</li>
<li>methods: 该论文使用了各种自然语言处理技术，以满足不同的代码分析和模型任务。</li>
<li>results: 该论文通过引入代码Symmetry的概念，提高了LLM架构的泛化和稳定性，并在不同的 binary 和源代码分析任务中进行了详细的实验评估。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.   Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在自动编程逻辑方面表现出了替代性，这是许多安全任务的关键方面。然而，现有的 LLM 架构 для代码往往从其他领域如自然语言处理中借鉴，这引起了代码泛化和稳定性的问题。一个关键的泛化挑战是如何在 LLM 架构中 интеグ含代码 semantics，包括控制和数据流的知识。 drew inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiveness in generalization and robustness through detailed experimental evaluations across different binary and source code analysis tasks. Overall, our code symmetry framework offers rigorous and powerful reasoning techniques that can guide the future development of specialized LLMs for code and advance LLM-guided program reasoning tasks.Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxí yǔyán módelìng)* "automated program reasoning" is translated as "自动编程逻辑" (zìdōng biānjiāng lógí)* "code" is translated as "代码" (dàikōng)* "semantics" is translated as " semantics" (xìngxìng)* "preserving transformations" is translated as "保持变换" (bǎojì biànbiàn)* "group-theoretic framework" is translated as "群论框架" (qúnlù jiàoxiàng)* "novel variant of self-attention" is translated as "一种新型的自注意力" (yī zhǒng xīn xìng de zìjiāngwù)* "detailed experimental evaluations" is translated as "详细实验评估" (xìngxìng shíyì píngjì)* "specialized LLMs for code" is translated as "专门为代码的 LLM" (zhōngmén wèi dàikōng de LLM)* "LLM-guided program reasoning" is translated as " LLM 导航的程序逻辑" (LLM dàowǎng de chéngjīng lógí)
</details></li>
</ul>
<hr>
<h2 id="Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization"><a href="#Implicit-Graph-Neural-Diffusion-Based-on-Constrained-Dirichlet-Energy-Minimization" class="headerlink" title="Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization"></a>Implicit Graph Neural Diffusion Based on Constrained Dirichlet Energy Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03306">http://arxiv.org/abs/2308.03306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, Lee Wee Sun</li>
<li>for: The paper is written for researchers and practitioners working on graph learning problems, particularly those interested in using implicit graph neural networks (GNNs) to capture long-range dependencies.</li>
<li>methods: The paper introduces a geometric framework for designing implicit GNN layers based on a parameterized graph Laplacian operator. This framework allows for learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data.</li>
<li>results: The paper demonstrates better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets. The proposed method is able to trade off smoothing with the preservation of node feature information, addressing the over-smoothing problem that can occur in some GNN models.<details>
<summary>Abstract</summary>
Implicit graph neural networks (GNNs) have emerged as a potential approach to enable GNNs to capture long-range dependencies effectively. However, poorly designed implicit GNN layers can experience over-smoothing or may have limited adaptability to learn data geometry, potentially hindering their performance in graph learning problems. To address these issues, we introduce a geometric framework to design implicit graph diffusion layers based on a parameterized graph Laplacian operator. Our framework allows learning the geometry of vertex and edge spaces, as well as the graph gradient operator from data. We further show how implicit GNN layers can be viewed as the fixed-point solution of a Dirichlet energy minimization problem and give conditions under which it may suffer from over-smoothing. To overcome the over-smoothing problem, we design our implicit graph diffusion layer as the solution of a Dirichlet energy minimization problem with constraints on vertex features, enabling it to trade off smoothing with the preservation of node feature information. With an appropriate hyperparameter set to be larger than the largest eigenvalue of the parameterized graph Laplacian, our framework guarantees a unique equilibrium and quick convergence. Our models demonstrate better performance than leading implicit and explicit GNNs on benchmark datasets for node and graph classification tasks, with substantial accuracy improvements observed for some datasets.
</details>
<details>
<summary>摘要</summary>
匿名图 neural networks (GNNs) 已成为可能的方法来有效地捕捉长距离依赖关系。然而，不当设计的匿名 GNN 层可能会经受过滤或有限的适应性，从而妨碍其在图学习问题中表现。为解决这些问题，我们提出了一个几何框架，用于设计基于参数化图 Laplacian 算子的匿名图扩散层。我们的框架允许学习图像的几何结构和图形导数算子，以及适应数据的几何特征。我们进一步表明，匿名 GNN 层可以视为 Dirichlet 能量最小化问题的固定点解，并给出了一些条件，以确定它可能会经受过滤。为了缓解过滤问题，我们设计了一种基于 vertex 特征的约束来限制匿名图扩散层的平滑程度，从而让它能够平衡平滑和保留节点特征信息。在适当的超参数设置为大于最大 eigenvalues 的情况下，我们的框架保证唯一的平衡点和快速的收敛。我们的模型在 benchmark 数据集上表现出色，与领先的匿名和显式 GNN 相比，有substantial 的准确率改进，特别是在某些数据集上。
</details></li>
</ul>
<hr>
<h2 id="Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection"><a href="#Do-You-Remember-Overcoming-Catastrophic-Forgetting-for-Fake-Audio-Detection" class="headerlink" title="Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection"></a>Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03300">http://arxiv.org/abs/2308.03300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cecile-hi/regularized-adaptive-weight-modification">https://github.com/cecile-hi/regularized-adaptive-weight-modification</a></li>
<li>paper_authors: Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenglong Wang, Chuyuan Zhang</li>
<li>For: 这个研究旨在开发一个可以适应不同数据集的伪音乐检测算法，以解决对伪音乐检测算法的普遍忘却问题。* Methods: 我们提出了一个叫做Regularized Adaptive Weight Modification（RAWM）的持续学习算法，它可以适应不同数据集的伪音乐检测任务，并且可以保持旧模型的知识。我们还引入了一个规制因子，以强制网络保持旧的特征分布。* Results: 我们在多个数据集上进行了评估，结果显示我们的方法可以对伪音乐检测任务进行有效的适应和忘却避免。<details>
<summary>Abstract</summary>
Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model, thus mitigating catastrophic forgetting. In addition, genuine audio collected from quite different acoustic conditions may skew their feature distribution, so we introduce a regularization constraint to force the network to remember the old distribution in this regard. Our method can easily be generalized to related fields, like speech emotion recognition. We also evaluate our approach across multiple datasets and obtain a significant performance improvement on cross-dataset experiments.
</details>
<details>
<summary>摘要</summary>
当前的假音检测算法已经在大多数数据集上实现了有望的性能。然而，它们在处理不同数据集的声音时可能会表现出很大的下降性能。现有的orthogonal weight modification方法不考虑真实声音数据集之间的相似性。为了解决这个限制，我们提出了一种适应学习的假音检测算法，called Regularized Adaptive Weight Modification (RAWM)。当训练检测网络时，我们的方法会动态计算修改方向，根据新数据集中真实的utterances和假的utterances的比率。这种适应修改方向使得网络可以有效地检测新数据集上的假音，同时保持以前的知识，从而避免悬崖式遗忘。此外，来自不同的听音条件下的真实声音收集可能会扭曲其特征分布，所以我们引入了一种正则化约束，使得网络可以保持以前的分布。我们的方法可以轻松扩展到相关的领域，如语音情感识别。我们还在多个数据集上评估了我们的方法，并在交叉数据集测试中获得了显著的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Studying-Large-Language-Model-Generalization-with-Influence-Functions"><a href="#Studying-Large-Language-Model-Generalization-with-Influence-Functions" class="headerlink" title="Studying Large Language Model Generalization with Influence Functions"></a>Studying Large Language Model Generalization with Influence Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03296">http://arxiv.org/abs/2308.03296</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman</li>
<li>for: 了解大型机器学习模型（LLMs）的特性和风险，以便更好地理解和 mitigate 这些风险。</li>
<li>methods: 使用Influence functions来回答一个Counterfactual：如果一个序列被添加到训练集中， THEN how would the model’s parameters（和其输出）变化？通过使用Eigenvalue-corrected Kronecker-Factored Approximate Curvature（EK-FAC）近似来扩展Influence functions到LLMs中，并使用TF-IDF筛选和查询批处理来降低计算Gradient的成本。</li>
<li>results: 通过Influence functions来研究LLMs的通用特性，包括影响 patrerns的稀畴性、逻辑推理能力、数学能力、跨语言通用性和role-playing行为等。尽管有很多似乎复杂的通用形式，但我们发现一个意外的限制：影响幅 decay 到 Near-zero 当键短语的顺序被反转。总的来说，Influence functions 为研究 LLMs 提供了一种强大的新工具。<details>
<summary>Abstract</summary>
When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)当尝试更好地了解机器学习模型以理解和降低相关风险时，一个有价值的证据来源是：哪些训练示例最大程度地影响模型的行为？影响函数想要回答一个Counterfactual：如果给定的序列添加到训练集中， Then how would the model's parameters (和其输出) change? Although influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.
</details></li>
</ul>
<hr>
<h2 id="DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data"><a href="#DOMINO-Domain-invariant-Hyperdimensional-Classification-for-Multi-Sensor-Time-Series-Data" class="headerlink" title="DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data"></a>DOMINO: Domain-invariant Hyperdimensional Classification for Multi-Sensor Time Series Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03295">http://arxiv.org/abs/2308.03295</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyao Wang, Luke Chen, Mohammad Abdullah Al Faruque</li>
<li>for: 这篇论文主要关注于如何 Addressing the distribution shift problem in noisy multi-sensor time-series data using brain-inspired hyperdimensional computing (HDC) learning frameworks.</li>
<li>methods: 这篇论文提出了一个名为 DOMINO 的新的 HDC 学习框架，利用高维度空间的有效并行矩阵运算来动态地识别和范围化领域对称的维度。</li>
<li>results: 在训练多种多感器时间序列标签任务中，DOMINO 比起现有的顶尖深度神经网络（DNN）域对应测试技术，平均提高了2.04%的精度，并在训练和测试过程中提供了16.34倍 faster 和2.89倍 faster 的运算速度。此外，DOMINO 在受到硬件噪声的情况下表现更好，提供了10.93倍更高的韧性。<details>
<summary>Abstract</summary>
With the rapid evolution of the Internet of Things, many real-world applications utilize heterogeneously connected sensors to capture time-series information. Edge-based machine learning (ML) methodologies are often employed to analyze locally collected data. However, a fundamental issue across data-driven ML approaches is distribution shift. It occurs when a model is deployed on a data distribution different from what it was trained on, and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) have been proposed to capture spatial and temporal dependencies in multi-sensor time series data, requiring intensive computational resources beyond the capacity of today's edge devices. While brain-inspired hyperdimensional computing (HDC) has been introduced as a lightweight solution for edge-based learning, existing HDCs are also vulnerable to the distribution shift challenge. In this paper, we propose DOMINO, a novel HDC learning framework addressing the distribution shift problem in noisy multi-sensor time-series data. DOMINO leverages efficient and parallel matrix operations on high-dimensional space to dynamically identify and filter out domain-variant dimensions. Our evaluation on a wide range of multi-sensor time series classification tasks shows that DOMINO achieves on average 2.04% higher accuracy than state-of-the-art (SOTA) DNN-based domain generalization techniques, and delivers 16.34x faster training and 2.89x faster inference. More importantly, DOMINO performs notably better when learning from partially labeled and highly imbalanced data, providing 10.93x higher robustness against hardware noises than SOTA DNNs.
</details>
<details>
<summary>摘要</summary>
随着互联网物联网的快速演化，许多实际应用场景使用不同类型的感知器来收集时序信息。Edge-based机器学习（ML）方法ологи是常用来当地分析收集的数据。然而，数据驱动的ML方法面临的基本问题是分布shift问题。这种问题会导致模型在不同于它在训练时的数据分布上部署时表现差。此外，逐渐复杂的深度神经网络（DNN）已经被提出来捕捉多感知器时序数据中的空间和时间相关性，需要今天的边缘设备来进行投入式计算资源。而基于脑神经网络的高维计算（HDC）已经被提出来作为边缘学习的轻量级解决方案。然而，现有的HDC也面临着分布shift挑战。在这篇论文中，我们提出了DOMINO，一种新的HDC学习框架，解决了边缘设备上的分布shift问题。DOMINO利用了高维空间中效率和并行的矩阵运算来动态标识和筛选域variant维度。我们对多种多感知器时序分类任务进行了广泛的评估，结果显示DOMINO相比状态скус（SOTA）神经网络基于领域普适化技术，平均提高了2.04%的准确率，并提供了16.34x快的训练和2.89x快的推理。此外，DOMINO在学习部分 labels和高度不均衡数据时表现更出色，提供了10.93x更高的硬件噪声Robustness。
</details></li>
</ul>
<hr>
<h2 id="SynJax-Structured-Probability-Distributions-for-JAX"><a href="#SynJax-Structured-Probability-Distributions-for-JAX" class="headerlink" title="SynJax: Structured Probability Distributions for JAX"></a>SynJax: Structured Probability Distributions for JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03291">http://arxiv.org/abs/2308.03291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deepmind/synjax">https://github.com/deepmind/synjax</a></li>
<li>paper_authors: Miloš Stanojević, Laurent Sartran</li>
<li>for: 这篇论文是为了解决深度学习模型中的结构化对象问题而写的，以便建立大规模可导的模型。</li>
<li>methods: 这篇论文使用了SynJax库，提供了高效的向量化实现归并算法，以便处理结构化分布的推理问题。</li>
<li>results: 该论文通过SynJax库实现了大规模可导模型，并且可以处理各种结构化对象，如树和分割。<details>
<summary>Abstract</summary>
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form.   SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https://github.com/deepmind/synjax.
</details>
<details>
<summary>摘要</summary>
随着深度学习软件库的发展，场景中的进步很大，允许用户专注于模型设计，让库处理现代硬件加速器的繁琐和耗时任务。然而，这些进步主要帮助了某些特定的深度学习模型，如转换器，其元素对vector化计算很容易映射。其他类型的模型，如树和分割，因为它们需要特殊的算法，难以在vector化形式下实现。SynJax直接解决了这个问题，提供了高效的vector化推理算法 для结构化分布，包括对适配、标记、分割、 константы树和扩展树。通过SynJax，我们可以构建大规模可导模型，直接模型数据中的结构。代码可以在https://github.com/deepmind/synjax上获取。
</details></li>
</ul>
<hr>
<h2 id="FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search"><a href="#FLIQS-One-Shot-Mixed-Precision-Floating-Point-and-Integer-Quantization-Search" class="headerlink" title="FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search"></a>FLIQS: One-Shot Mixed-Precision Floating-Point and Integer Quantization Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03290">http://arxiv.org/abs/2308.03290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Dotzel, Gang Wu, Andrew Li, Muhammad Umar, Yun Ni, Mohamed S. Abdelfattah, Zhiru Zhang, Liqun Cheng, Martin G. Dixon, Norman P. Jouppi, Quoc V. Le, Sheng Li</li>
<li>for: 这个论文主要目的是提出一种一击 mixed-precision quantization search（FLIQS），以找到最佳的混合精度数据模型，并且可以实现高品质和低成本的模型。</li>
<li>methods: 这个方法使用了一种新的一击 mixed-precision quantization search（FLIQS），可以在数据模型中找到最佳的混合精度数据模型，并且不需要重新训练。</li>
<li>results: 这个方法可以对多个 convolutional networks 和 vision transformer 模型进行搜索，并且可以实现高品质和低成本的模型。在 ImageNet 上，这个方法可以将 ResNet-18 的精度提高到 1.31% 点，ResNet-50 的精度提高到 0.90% 点，并且在 MobileNetV2 上可以实现最佳的模型。<details>
<summary>Abstract</summary>
Quantization has become a mainstream compression technique for reducing model size, computational requirements, and energy consumption for modern deep neural networks (DNNs). With the improved numerical support in recent hardware, including multiple variants of integer and floating point, mixed-precision quantization has become necessary to achieve high-quality results with low model cost. Prior mixed-precision quantization methods have performed a post-training quantization search, which compromises on accuracy, or a differentiable quantization search, which leads to high memory usage from branching. Therefore, we propose the first one-shot mixed-precision quantization search that eliminates the need for retraining in both integer and low-precision floating point models. We evaluate our floating-point and integer quantization search (FLIQS) on multiple convolutional networks and vision transformer models to discover Pareto-optimal models. Our approach discovers models that improve upon uniform precision, manual mixed-precision, and recent integer quantization search methods. With the proposed integer quantization search, we increase the accuracy of ResNet-18 on ImageNet by 1.31% points and ResNet-50 by 0.90% points with equivalent model cost over previous methods. Additionally, for the first time, we explore a novel mixed-precision floating-point search and improve MobileNetV2 by up to 0.98% points compared to prior state-of-the-art FP8 models. Finally, we extend FLIQS to simultaneously search a joint quantization and neural architecture space and improve the ImageNet accuracy by 2.69% points with similar model cost on a MobileNetV2 search space.
</details>
<details>
<summary>摘要</summary>
启用量化技术已成为现代深度神经网络（DNN）减少模型大小、计算需求和能耗的主流压缩方法。随着现有硬件的数值支持的改进，包括整数和浮点数多种变体，混合精度量化技术已成为实现高质量结果的低模型成本的必要手段。先前的混合精度量化方法通常是在训练后进行量化搜索，这会妥协准确性，或者使用可导量化搜索，这会导致高内存使用率由分支引起。因此，我们提出了首次一步混合精度量化搜索，从而消除了重训练的需要，并在整数和低精度浮点数模型中实现高度优化。我们对多个卷积神经网络和视Transformer模型进行了评估，并发现了Pareto优质模型。我们的浮点和整数量化搜索（FLIQS）在 uniform精度、手动混合精度和最近整数量化搜索方法上提高了模型的准确性。例如，在 ImageNet 上，我们通过使用整数量化搜索，提高了 ResNet-18 的准确性by 1.31% 点和 ResNet-50 的准确性by 0.90% 点，与之前的方法相当。此外，我们首次探索了一种新的混合精度浮点数搜索，并在 MobileNetV2 上提高了最高达 0.98% 点的准确性。最后，我们将 FLIQS 扩展到同时搜索一个量化和神经网络架构空间，并在 MobileNetV2 搜索空间上提高了 ImageNet 的准确性by 2.69% 点，与相同的模型成本。
</details></li>
</ul>
<hr>
<h2 id="High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning"><a href="#High-rate-discretely-modulated-continuous-variable-quantum-key-distribution-using-quantum-machine-learning" class="headerlink" title="High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning"></a>High-rate discretely-modulated continuous-variable quantum key distribution using quantum machine learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03283">http://arxiv.org/abs/2308.03283</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qin Liao, Jieyu Liu, Anqi Huang, Lei Huang, Zhuoying Fei, Xiquan Fu</li>
<li>for: 提出了一种高速率的分割CVQKD系统使用量子机器学习技术，以提高CVQKD系统的安全性和效率。</li>
<li>methods: 使用量子k-最近邻NN分类器来预测Bob方向的失去混合干扰DMCS，并对输出的密钥进行数据处理以生成最终的秘密密钥串。</li>
<li>results: 提出的方案可以在机器学习metric和复杂度方面提高CVQKD系统的性能，并通过使用SDP方法证明其理论安全性。数值仿真结果表明，相比现有的DM CVQKD协议，提出的方案可以获得更高的秘密密钥率，并可以通过增加混合干扰的变化来进一步提高性能。<details>
<summary>Abstract</summary>
We propose a high-rate scheme for discretely-modulated continuous-variable quantum key distribution (DM CVQKD) using quantum machine learning technologies, which divides the whole CVQKD system into three parts, i.e., the initialization part that is used for training and estimating quantum classifier, the prediction part that is used for generating highly correlated raw keys, and the data-postprocessing part that generates the final secret key string shared by Alice and Bob. To this end, a low-complexity quantum k-nearest neighbor (QkNN) classifier is designed for predicting the lossy discretely-modulated coherent states (DMCSs) at Bob's side. The performance of the proposed QkNN-based CVQKD especially in terms of machine learning metrics and complexity is analyzed, and its theoretical security is proved by using semi-definite program (SDP) method. Numerical simulation shows that the secret key rate of our proposed scheme is explicitly superior to the existing DM CVQKD protocols, and it can be further enhanced with the increase of modulation variance.
</details>
<details>
<summary>摘要</summary>
Here is the translation in Simplified Chinese:我们提出了一种高率方案 для抽象变量量kv quantum key distribution（DM CVQKD），使用量子机器学习技术，将整个CVQKD系统分成三部分：初始化部分用于训练和估算量子分类器，预测部分用于生成高相关性的Raw键，以及数据处理部分用于生成最终由阿利斯和布Bob共享的机密键串。为此，我们设计了一种低复杂度量子k最近邻居分类器（QkNN）来预测 бо布的lossy抽象变量量CS。我们分析了我们提议的QkNN-based CVQKD的性能，包括机器学习指标和复杂度，并使用 semi-definite 程序（SDP）方法证明其理论安全性。numerical simulation表明，我们的提议方案的机密键率明显高于现有的DM CVQKD协议，并可以通过增加调制变量的幅度来进一步提高。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface"><a href="#Knowledge-Distilled-Ensemble-Model-for-sEMG-based-Silent-Speech-Interface" class="headerlink" title="Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface"></a>Knowledge Distilled Ensemble Model for sEMG-based Silent Speech Interface</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06533">http://arxiv.org/abs/2308.06533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqiang Lai, Qihan Yang, Ye Mao, Endong Sun, Jiangnan Ye</li>
<li>for: 这研究旨在提供一种基于表面电Myography（sEMG）的干扰 Speech Interface（SSI），以解决全球数百万人口报告有声音问题。</li>
<li>methods: 我们提出了一种轻量级的深度学习知识压缩混合模型（KDE-SSI），使得可以通过拼写生成任何英语单词。</li>
<li>results: 我们的实验证明了KDE-SSI的效果，实现了26个 NATO fonetic alphabet dataset中的3900个数据样本的测试准确率为85.9%。<details>
<summary>Abstract</summary>
Voice disorders affect millions of people worldwide. Surface electromyography-based Silent Speech Interfaces (sEMG-based SSIs) have been explored as a potential solution for decades. However, previous works were limited by small vocabularies and manually extracted features from raw data. To address these limitations, we propose a lightweight deep learning knowledge-distilled ensemble model for sEMG-based SSI (KDE-SSI). Our model can classify a 26 NATO phonetic alphabets dataset with 3900 data samples, enabling the unambiguous generation of any English word through spelling. Extensive experiments validate the effectiveness of KDE-SSI, achieving a test accuracy of 85.9\%. Our findings also shed light on an end-to-end system for portable, practical equipment.
</details>
<details>
<summary>摘要</summary>
声音疾病影响全球数千万人，表面电动力学基于Silent Speech Interface（sEMG-based SSI）已经在数十年内被探索。然而，先前的工作受限于小词汇和手动提取的特征从原始数据中提取。为了解决这些限制，我们提出了一种轻量级深度学习知识卷积混合模型 для sEMG-based SSI（KDE-SSI）。我们的模型可以分类26个北约字母集数据集，包含3900个数据样本，使得可以不 ambiguously生成任何英文单词的拼写。我们的实验证明了KDE-SSI的效果，测试准确率达85.9%。我们的发现还照明了一个端到端系统，可以在可搬式、实用的设备上实现。
</details></li>
</ul>
<hr>
<h2 id="DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction"><a href="#DSformer-A-Double-Sampling-Transformer-for-Multivariate-Time-Series-Long-term-Prediction" class="headerlink" title="DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction"></a>DSformer: A Double Sampling Transformer for Multivariate Time Series Long-term Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03274">http://arxiv.org/abs/2308.03274</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengqing Yu, Fei Wang, Zezhi Shao, Tao Sun, Lin Wu, Yongjun Xu</li>
<li>for: 本研究旨在提出一种基于变换器的多变量时间序列长期预测模型，以提高多变量时间序列预测的精度。</li>
<li>methods: 该模型由双重采样块（DS block）和时间变量注意块（TVA block）组成。DS块使用下采样和分割采样将原始序列转换为专注于全局信息和本地信息的特征 вектор。然后，TVA块使用时间注意力和变量注意力来挖掘这些特征 вектор，并提取关键信息。最后，DSformer 使用多个 TVA 块来挖掘和集成不同维度的特征信息，并将其传递给基于多层感知器的生成解码器，以实现多变量时间序列长期预测。</li>
<li>results: 实验结果表明，DSformer 可以在九个真实世界数据集上超越八个基准模型。<details>
<summary>Abstract</summary>
Multivariate time series long-term prediction, which aims to predict the change of data in a long time, can provide references for decision-making. Although transformer-based models have made progress in this field, they usually do not make full use of three features of multivariate time series: global information, local information, and variables correlation. To effectively mine the above three features and establish a high-precision prediction model, we propose a double sampling transformer (DSformer), which consists of the double sampling (DS) block and the temporal variable attention (TVA) block. Firstly, the DS block employs down sampling and piecewise sampling to transform the original series into feature vectors that focus on global information and local information respectively. Then, TVA block uses temporal attention and variable attention to mine these feature vectors from different dimensions and extract key information. Finally, based on a parallel structure, DSformer uses multiple TVA blocks to mine and integrate different features obtained from DS blocks respectively. The integrated feature information is passed to the generative decoder based on a multi-layer perceptron to realize multivariate time series long-term prediction. Experimental results on nine real-world datasets show that DSformer can outperform eight existing baselines.
</details>
<details>
<summary>摘要</summary>
多变量时间序列长期预测，目的是预测数据在长时间内的变化，可以提供决策参考。虽然变换器基本模型在这个领域中已经作出了进步，但它们通常不充分利用多变量时间序列的三个特点：全球信息、本地信息和变量相关性。为了有效利用这些特点并建立高精度预测模型，我们提出了双重采样变换器（DSformer），它包括双重采样（DS）块和时间变量注意（TVA）块。首先，DS块使用下采样和分割采样将原始系列转换为特征向量，这些向量专注于全球信息和本地信息。然后，TVA块使用时间注意和变量注意来挖掘这些特征向量从不同维度，提取关键信息。最后，基于并行结构，DSformer使用多个TVA块来挖掘和 интеGRATE不同的特征信息，并将其传递给基于多层感知器的生成解码器，实现多变量时间序列长期预测。实验结果表明，DSformer可以在九个真实世界数据集上超越八个基准。
</details></li>
</ul>
<hr>
<h2 id="Local-Structure-aware-Graph-Contrastive-Representation-Learning"><a href="#Local-Structure-aware-Graph-Contrastive-Representation-Learning" class="headerlink" title="Local Structure-aware Graph Contrastive Representation Learning"></a>Local Structure-aware Graph Contrastive Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03271">http://arxiv.org/abs/2308.03271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kai Yang, Yuan Liu, Zijuan Zhao, Peijin Ding, Wenqian Zhao</li>
<li>for: 本文提出了一种Local Structure-aware Graph Contrastive representation Learning方法（LS-GCL），用于模型节点的结构信息从多个视图。</li>
<li>methods: 本方法使用了semantic subgraphs，不限于第一邻域，并使用共享GNNEncoder来学习target节点embeddings。在全视图和局部视图之间使用了pooling函数来生成子图级别图像 embeddings。</li>
<li>results: 实验结果表明，LS-GCL方法在五个数据集上的node classification和link prediction任务中都超过了状态方法。<details>
<summary>Abstract</summary>
Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackle the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In the paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on five datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.
</details>
<details>
<summary>摘要</summary>
传统的图 neural network (GNN) 是一种图表示学习方法，它受到标签信息的限制。然而，图相对学习 (GCL) 方法，它们能够有效解决标签问题，主要关注全图或小子图结构（例如，第一颗邻居）的特征信息。在文章中，我们提出了一种本地结构意识 Graph Contrastive representation Learning 方法 (LS-GCL)，用于模型节点的多视图结构信息。具体来说，我们构建了不同于第一颗邻居的semantic subgraph，并将每个目标节点的semantic subgraph输入到共享 GNN Encoder 中，以获得目标节点的subgraph级别嵌入。然后，我们使用一个pooling函数生成subgraph级别图像嵌入。对于全图视图，我们利用共享 GNN Encoder 来学习目标节点的全图级别嵌入。我们的 LS-GCL 模型通过最大化三种不同视角的共同信息来优化一个多级对比损失函数来进行优化。实验结果表明，我们的方法在五个dataset上对节点 classification 和链接预测任务均达到了当前最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Simple-Rule-Injection-for-ComplEx-Embeddings"><a href="#Simple-Rule-Injection-for-ComplEx-Embeddings" class="headerlink" title="Simple Rule Injection for ComplEx Embeddings"></a>Simple Rule Injection for ComplEx Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03269">http://arxiv.org/abs/2308.03269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haodi Ma, Anthony Colas, Yuejie Wang, Ali Sadeghian, Daisy Zhe Wang</li>
<li>for: 本研究旨在协调逻辑规则与知识图 embeddings，以利用先前知识来提高 neural knowledge graph inference 的性能。</li>
<li>methods: 本研究提出了 InjEx 机制，可以通过简单的约束将多种类型的逻辑规则注入到 embedding 空间中，以捕捉definite Horn 规则。</li>
<li>results: 在知识图完成 (KGC) 和少量shot知识图完成 (FKGC) 设置下，InjEx 比基eline KGC 模型和特殊化的几个shot模型表现出色，同时保持了可扩展性和效率。<details>
<summary>Abstract</summary>
Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.
</details>
<details>
<summary>摘要</summary>
近期研究在神经知识Graph推理中尝试将逻辑规则与知识Graph嵌入结合以获得优势。然而，他们通常无法避免规则定义，并且尝试插入多种规则的可能性还没有得到了全面的探索。在这种情况下，我们提出了InjEx，一种机制来插入多种规则， capture definite Horn rules。首先，我们 theoretically prove that InjEx可以插入这些规则。然后，我们通过在知识Graph completion（KGC）和少量知识Graph completion（FKGC）设置中评估InjEx，以证明它可以把有意义的先前知识注入到嵌入空间中。我们的实验结果表明，InjEx在KGC和FKGC设置中的性能都高于基eline模型和专门的少量模型，同时保持其可扩展性和高效性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs"><a href="#Exploring-Different-Time-series-Transformer-TST-Architectures-A-Case-Study-in-Battery-Life-Prediction-for-Electric-Vehicles-EVs" class="headerlink" title="Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)"></a>Exploring Different Time-series-Transformer (TST) Architectures: A Case Study in Battery Life Prediction for Electric Vehicles (EVs)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03260">http://arxiv.org/abs/2308.03260</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niranjan Sitapure, Atharva Kulkarni</li>
<li>For: The paper aims to develop accurate battery life prediction models for electric vehicles (EVs) by incorporating environmental, battery, vehicle driving, and heating circuit data.* Methods: The paper uses time-series-transformers (TSTs) and long short-term memory (LSTM) models to predict state-of-charge (SOC) and battery temperature. The authors also explore novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM.* Results: The paper uses a dataset of 72 driving trips in a BMW i3 (60 Ah) to evaluate the performance of the proposed models. The results show that the TST models outperform the LSTM models and traditional battery models, with an accuracy of 95.6% for SOC and 93.8% for battery temperature.<details>
<summary>Abstract</summary>
In recent years, battery technology for electric vehicles (EVs) has been a major focus, with a significant emphasis on developing new battery materials and chemistries. However, accurately predicting key battery parameters, such as state-of-charge (SOC) and temperature, remains a challenge for constructing advanced battery management systems (BMS). Existing battery models do not comprehensively cover all parameters affecting battery performance, including non-battery-related factors like ambient temperature, cabin temperature, elevation, and regenerative braking during EV operation. Due to the difficulty of incorporating these auxiliary parameters into traditional models, a data-driven approach is suggested. Time-series-transformers (TSTs), leveraging multiheaded attention and parallelization-friendly architecture, are explored alongside LSTM models. Novel TST architectures, including encoder TST + decoder LSTM and a hybrid TST-LSTM, are also developed and compared against existing models. A dataset comprising 72 driving trips in a BMW i3 (60 Ah) is used to address battery life prediction in EVs, aiming to create accurate TST models that incorporate environmental, battery, vehicle driving, and heating circuit data to predict SOC and battery temperature for future time steps.
</details>
<details>
<summary>摘要</summary>
近年来，电动汽车（EV）的电池技术得到了广泛关注，尤其是开发新的电池材料和化学组合。然而，正确预测电池参数，如充电状态（SOC）和温度，仍然是构建高级电池管理系统（BMS）的挑战。现有的电池模型并不完全覆盖所有影响电池性能的参数，包括非电池相关的因素，如外部温度、车辆内部温度、海拔和恢复摩擦 durante la operación del vehículo eléctrico.由于将这些辅助参数integrated into traditional models是困难的，一种数据驱动的方法被建议。使用时间序列变换（TST）和长短期memory（LSTM）模型，并研发了新的TST架构，包括encoder TST + decoder LSTM和hybrid TST-LSTM。使用一个包含72次开车记录的BMW i3（60 Ah）数据集，以预测将来时间步的SOC和电池温度。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks"><a href="#Optimal-Approximation-and-Learning-Rates-for-Deep-Convolutional-Neural-Networks" class="headerlink" title="Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks"></a>Optimal Approximation and Learning Rates for Deep Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03259">http://arxiv.org/abs/2308.03259</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin</li>
<li>for: 本研究探讨了深度卷积神经网络（CNN）的approximation和学习性能分析，特别是采用零填充和最大池化。</li>
<li>methods: 我们提供了一种可以approximate $r$-smooth函数的深度卷积神经网络的分析方法，并证明了这种方法的approximation率为 $(L^2&#x2F;\log L)^{-2r&#x2F;d} $，这是最佳的下界。</li>
<li>results: 我们得到了深度卷积神经网络在实际风险最小化中的几乎最佳学习率。<details>
<summary>Abstract</summary>
This paper focuses on approximation and learning performance analysis for deep convolutional neural networks with zero-padding and max-pooling. We prove that, to approximate $r$-smooth function, the approximation rates of deep convolutional neural networks with depth $L$ are of order $ (L^2/\log L)^{-2r/d} $, which is optimal up to a logarithmic factor. Furthermore, we deduce almost optimal learning rates for implementing empirical risk minimization over deep convolutional neural networks.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文关注深度 convolutional neural networks (CNNs) 的近似和学习性能分析，特别是在零填充和最大 pooling 下。我们证明，要近似 $r $- 平滑函数，深度 $L $ 的 CNNs 的近似率是 $(L^2/\log L)^{-2r/d} $，即最优的logs 因子。此外，我们还得出了对深度 CNNs 的实际风险最小化的几乎最优学习率。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change"><a href="#Unsupervised-Adversarial-Detection-without-Extra-Model-Training-Loss-Should-Change" class="headerlink" title="Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change"></a>Unsupervised Adversarial Detection without Extra Model: Training Loss Should Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03243">http://arxiv.org/abs/2308.03243</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model">https://github.com/cyclebooster/unsupervised-adversarial-detection-without-extra-model</a></li>
<li>paper_authors: Chien Cheng Chyou, Hung-Ting Su, Winston H. Hsu</li>
<li>for: 防御深度学习模型受到攻击的挑战是现实世界应用中的关键问题。传统的对抗训练和监督检测方法需要先知道攻击类型和训练数据标注，这经常是不实际的。现有的无监督对抗检测方法可以判断目标模型是否正常工作，但它们因使用共同减少极限loss而受到差异攻击的强化，导致准确率差。</li>
<li>methods: 我们提议使用新的训练损失来减少无用的特征，并对无需先知道攻击类型的检测方法。</li>
<li>results: 我们的检测率（真正正确率）对所有给出的白盒攻击是高于93.9%，只有DF($\infty$)攻击没有限制。false positive rate几乎为2.5%。我们的方法在所有攻击类型下都工作良好，并且对某些类型的方法表现更好。<details>
<summary>Abstract</summary>
Adversarial robustness poses a critical challenge in the deployment of deep learning models for real-world applications. Traditional approaches to adversarial training and supervised detection rely on prior knowledge of attack types and access to labeled training data, which is often impractical. Existing unsupervised adversarial detection methods identify whether the target model works properly, but they suffer from bad accuracies owing to the use of common cross-entropy training loss, which relies on unnecessary features and strengthens adversarial attacks. We propose new training losses to reduce useless features and the corresponding detection method without prior knowledge of adversarial attacks. The detection rate (true positive rate) against all given white-box attacks is above 93.9% except for attacks without limits (DF($\infty$)), while the false positive rate is barely 2.5%. The proposed method works well in all tested attack types and the false positive rates are even better than the methods good at certain types.
</details>
<details>
<summary>摘要</summary>
“深度学习模型的应急防范问题是实际应用中的关键挑战。传统的敌对训练和监管检测方法需要先知 adversarial 攻击类型和训练数据的标注，这经常是不现实的。现有的无监管敌对检测方法可以判断目标模型是否正常工作，但它们因使用通用的 cross-entropy 训练损失函数，导致强化敌对攻击。我们提出了新的训练损失函数，以减少不必要的特征，并对这些特征进行检测方法，不需要先知 adversarial 攻击。检测率（真正正确率）对所有给出的白盒攻击是大于 93.9%，只有 attacks without limits（DF（∞））的检测率较低，而假阳性率只有 2.5%。我们的方法在所有攻击类型中都工作良好，假阳性率甚至比其他方法更好。”Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. The translation is based on the original text and may not capture all the nuances and details of the original text.
</details></li>
</ul>
<hr>
<h2 id="Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence"><a href="#Asynchronous-Decentralized-Q-Learning-Two-Timescale-Analysis-By-Persistence" class="headerlink" title="Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence"></a>Asynchronous Decentralized Q-Learning: Two Timescale Analysis By Persistence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03239">http://arxiv.org/abs/2308.03239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bora Yongacoglu, Gürdal Arslan, Serdar Yüksel</li>
<li>for: 本研究旨在探讨非站立性是多智能 reinforcement learning（MARL）挑战的基础问题，并研究如何使用不同的方法解决这个问题。</li>
<li>methods: 本文使用的方法包括各种协调策略更新的方法，包括同步化策略更新的时间。这些同步化方法可以使得多个智能体的策略更新趋于相同，从而使得问题的分析变得更加容易。然而，在许多分散式应用中，同步化是不可能实现的。因此，本文研究了一种异步的Q值学习算法的variant，以解决这个问题。</li>
<li>results: 本文的分析表明，当使用不同的学习率时，异步Q值学习算法可以逐渐靠拢到平衡状态，并且在某些情况下可以达到更高的性能。此外，本文还证明了这种算法在分散式应用中可以有更好的性能，而无需协调智能体的策略更新。<details>
<summary>Abstract</summary>
Non-stationarity is a fundamental challenge in multi-agent reinforcement learning (MARL), where agents update their behaviour as they learn. Many theoretical advances in MARL avoid the challenge of non-stationarity by coordinating the policy updates of agents in various ways, including synchronizing times at which agents are allowed to revise their policies. Synchronization enables analysis of many MARL algorithms via multi-timescale methods, but such synchrony is infeasible in many decentralized applications. In this paper, we study an asynchronous variant of the decentralized Q-learning algorithm, a recent MARL algorithm for stochastic games. We provide sufficient conditions under which the asynchronous algorithm drives play to equilibrium with high probability. Our solution utilizes constant learning rates in the Q-factor update, which we show to be critical for relaxing the synchrony assumptions of earlier work. Our analysis also applies to asynchronous generalizations of a number of other algorithms from the regret testing tradition, whose performance is analyzed by multi-timescale methods that study Markov chains obtained via policy update dynamics. This work extends the applicability of the decentralized Q-learning algorithm and its relatives to settings in which parameters are selected in an independent manner, and tames non-stationarity without imposing the coordination assumptions of prior work.
</details>
<details>
<summary>摘要</summary>
非站点性是多智能游戏学习（MARL）的基本挑战，智能体在学习过程中会更新其行为。许多MARL理论进步避免非站点性挑战，通过协调智能体策略更新的方式，包括同步化智能体更新策略的时间。同步化可以通过多时间标准方法进行分析，但这种同步性在多数分散式应用中是不可能实现的。在这篇论文中，我们研究了一种异步版的分散式Q学习算法，这是一种最近的MARL算法 для随机游戏。我们提供了 sufficient condition，以 guarantee that the asynchronous algorithm drives play to equilibrium with high probability.我们的解决方案使用了常量学习率在Q因子更新中，我们证明这是关键 для放弃先前作品中的同步性假设。我们的分析还适用于异步扩展其他算法，这些算法是由回报测试传统中的表现分析。这项工作扩展了分散式Q学习算法和其相关的算法在不同的参数选择方式下的可用性，并对非站点性进行了控制，而不需要先前作品中的协调假设。
</details></li>
</ul>
<hr>
<h2 id="AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning"><a href="#AdaER-An-Adaptive-Experience-Replay-Approach-for-Continual-Lifelong-Learning" class="headerlink" title="AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning"></a>AdaER: An Adaptive Experience Replay Approach for Continual Lifelong Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03810">http://arxiv.org/abs/2308.03810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang, Haifeng Li</li>
<li>for: 这篇论文是为了解决人工智能学习框架中的持续性学习问题，即在不断学习的过程中，学习器会忘记之前学习的知识。</li>
<li>methods: 这篇论文提出了一种新的算法，叫做自适应经验回忆（AdaER），它包括两个阶段：记忆回忆和记忆更新。在记忆回忆阶段，AdaER使用了一种Contextually-cued Memory Recall（C-CMR）策略，选择最相似的记忆和当前输入数据和任务进行回忆。此外，AdaER还使用了一种Entropy-balanced Reservoir Sampling（E-BRS）策略来增强记忆缓存的性能。</li>
<li>results: 根据实验结果，AdaER在成本累积学习场景下表现出色，比现有的持续性学习基线更好， highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.<details>
<summary>Abstract</summary>
Continual lifelong learning is an machine learning framework inspired by human learning, where learners are trained to continuously acquire new knowledge in a sequential manner. However, the non-stationary nature of streaming training data poses a significant challenge known as catastrophic forgetting, which refers to the rapid forgetting of previously learned knowledge when new tasks are introduced. While some approaches, such as experience replay (ER), have been proposed to mitigate this issue, their performance remains limited, particularly in the class-incremental scenario which is considered natural and highly challenging. In this paper, we present a novel algorithm, called adaptive-experience replay (AdaER), to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy. To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details>
<details>
<summary>摘要</summary>
In this paper, we present a novel algorithm called adaptive-experience replay (AdaER) to address the challenge of continual lifelong learning. AdaER consists of two stages: memory replay and memory update. In the memory replay stage, AdaER introduces a contextually-cued memory recall (C-CMR) strategy, which selectively replays memories that are most conflicting with the current input data in terms of both data and task. Additionally, AdaER incorporates an entropy-balanced reservoir sampling (E-BRS) strategy to enhance the performance of the memory buffer by maximizing information entropy.To evaluate the effectiveness of AdaER, we conduct experiments on established supervised continual lifelong learning benchmarks, specifically focusing on class-incremental learning scenarios. The results demonstrate that AdaER outperforms existing continual lifelong learning baselines, highlighting its efficacy in mitigating catastrophic forgetting and improving learning performance.
</details></li>
</ul>
<hr>
<h2 id="G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima"><a href="#G-Mix-A-Generalized-Mixup-Learning-Framework-Towards-Flat-Minima" class="headerlink" title="G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima"></a>G-Mix: A Generalized Mixup Learning Framework Towards Flat Minima</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03236">http://arxiv.org/abs/2308.03236</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyu Li, Bo Tang<br>for: 增强深度神经网络（DNN）的泛化能力，特别是在有限的训练数据时。methods:  combinatorial Mixup技术和Sharpness-Aware Minimization（SAM）方法。results: 提出了一种新的学习框架 called Generalized-Mixup（G-Mix），并 introduces two novel algorithms：Binary G-Mix和Decomposed G-Mix。这些算法可以进一步优化DNN性能，并实现多个数据集和模型的状态反映性。<details>
<summary>Abstract</summary>
Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, current DNNs encounter challenges with over-parameterization, especially when there is limited training data available. To enhance the generalization capability of DNNs, the Mixup technique has gained popularity. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful Sharpness-Aware Minimization (SAM) approach, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix and Decomposed G-Mix. These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of "manifold intrusion" in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining"><a href="#Analysis-of-the-Evolution-of-Advanced-Transformer-Based-Language-Models-Experiments-on-Opinion-Mining" class="headerlink" title="Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining"></a>Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03235">http://arxiv.org/abs/2308.03235</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zekaouinoureddine/Opinion-Transformers">https://github.com/zekaouinoureddine/Opinion-Transformers</a></li>
<li>paper_authors: Nour Eddine Zekaoui, Siham Yousfi, Maryem Rhanoui, Mounia Mikram</li>
<li>for: 本研究的目的是研究高性能的Transformer语言模型在意见采集方面的表现，并对它们进行比较，以揭示它们的特点。</li>
<li>methods: 本研究使用了高性能的Transformer语言模型，包括BERT和RoBERTa等，对文本进行意见采集和分析。</li>
<li>results: 研究结果显示，Transformer语言模型在意见采集方面具有优秀的表现，具有较强的表达能力和精度。同时，BERT和RoBERTa等模型在意见采集方面具有显著的差异，可以根据实际应用场景选择合适的模型。<details>
<summary>Abstract</summary>
Opinion mining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece of text (e.g., positive or negative), as well as identifying specific emotions or opinions expressed in the text, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing long text. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinion mining and provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction"><a href="#Imbalanced-Large-Graph-Learning-Framework-for-FPGA-Logic-Elements-Packing-Prediction" class="headerlink" title="Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction"></a>Imbalanced Large Graph Learning Framework for FPGA Logic Elements Packing Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03231">http://arxiv.org/abs/2308.03231</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhixiong Di, Runzhe Tao, Lin Chen, Qiang Wu, Yibo Lin</li>
<li>for: This paper is written for the purpose of predicting whether logic elements will be packed after placement in an FPGA CAD flow.</li>
<li>methods: The paper proposes an imbalanced large graph learning framework called ImLG, which uses dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. The framework also employs techniques such as graph oversampling and mini-batch training to handle the imbalanced distribution of packed and unpacked logic elements.</li>
<li>results: The proposed method achieves an F1 score improvement of 42.82% compared to the most recent Gaussian-based prediction method. Additionally, the physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.Here is the same information in Simplified Chinese:</li>
<li>for: 这篇论文是为了预测FPGA嵌入式逻辑元素是否会在置换后被压缩而写的。</li>
<li>methods: 论文提出了一种大图学习框架 called ImLG，该框架使用专门的特征提取和特征聚合方法来提高电路图的节点表示学习。它还使用了 Graph oversampling 和 mini-batch 训练来处理嵌入式逻辑元素的不均衡分布。</li>
<li>results: 提议的方法可以提高对最近 Gaussian-based 预测方法的 F1 score 值 by 42.82%。Physical 设计结果表明，提议的方法可以帮助置换器提高路径长度 by 0.93% 和 SLICE 占用率 by 0.89%。<details>
<summary>Abstract</summary>
Packing is a required step in a typical FPGA CAD flow. It has high impacts to the performance of FPGA placement and routing. Early prediction of packing results can guide design optimization and expedite design closure. In this work, we propose an imbalanced large graph learning framework, ImLG, for prediction of whether logic elements will be packed after placement. Specifically, we propose dedicated feature extraction and feature aggregation methods to enhance the node representation learning of circuit graphs. With imbalanced distribution of packed and unpacked logic elements, we further propose techniques such as graph oversampling and mini-batch training for this imbalanced learning task in large circuit graphs. Experimental results demonstrate that our framework can improve the F1 score by 42.82% compared to the most recent Gaussian-based prediction method. Physical design results show that the proposed method can assist the placer in improving routed wirelength by 0.93% and SLICE occupation by 0.89%.
</details>
<details>
<summary>摘要</summary>
packing 是 FPGA CAD 流程中的一个必需步骤，它对 FPGA 的位置和路由产生了重要影响。 early prediction of packing results 可以导引设计优化和加速设计关闭。在这种工作中，我们提出了一个大图学习框架，ImLG，用于预测逻辑元素是否会被压缩 после放置。 Specifically，我们提出了专门的特征提取和特征聚合方法，以增强圈图学习的节点表示学习。 由于逻辑元素的压缩和未压缩的分布是不均衡的，我们还提出了一些技术，如图像扩大和微批训练，来解决这种不均衡的学习任务。 experimental results 表明，我们的框架可以提高 F1 分数比最近 Gaussian-based 预测方法提高42.82%。 physical design results 表明，我们的方法可以帮助置器提高了 routed wirelength 和 SLICE 占用率。
</details></li>
</ul>
<hr>
<h2 id="Tractability-of-approximation-by-general-shallow-networks"><a href="#Tractability-of-approximation-by-general-shallow-networks" class="headerlink" title="Tractability of approximation by general shallow networks"></a>Tractability of approximation by general shallow networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03230">http://arxiv.org/abs/2308.03230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hrushikesh Mhaskar, Tong Mao</li>
<li>for: 这 paper 描述了一种精炼版的函数approximation算法，用于 approximating 函数的形式为 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$， $x\in\mathbb{X}$， by $G$-networks of the form $x\mapsto\sum_{k&#x3D;1}^n a_kG(x,y_k)$, $y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$。</li>
<li>methods: 该 paper 使用了covering numbers来定义 $\mathbb{X}$ 和 $\mathbb{Y}$ 的维度，然后通过使用 $G$-networks 获得了独立于维度的approximation bound。</li>
<li>results: 该 paper  obtainted dimension-independent approximation bounds, meaning that the bounds do not depend on the dimensions of the input spaces $\mathbb{X}$ and $\mathbb{Y}$. The paper also provided applications of the results to various types of neural networks, including power rectified linear unit networks, zonal function networks, and radial basis function networks.<details>
<summary>Abstract</summary>
In this paper, we present a sharper version of the results in the paper Dimension independent bounds for general shallow networks; Neural Networks, \textbf{123} (2020), 142-152. Let $\mathbb{X}$ and $\mathbb{Y}$ be compact metric spaces. We consider approximation of functions of the form $ x\mapsto\int_{\mathbb{Y}} G( x, y)d\tau( y)$, $ x\in\mathbb{X}$, by $G$-networks of the form $ x\mapsto \sum_{k=1}^n a_kG( x, y_k)$, $ y_1,\cdots, y_n\in\mathbb{Y}$, $a_1,\cdots, a_n\in\mathbb{R}$. Defining the dimensions of $\mathbb{X}$ and $\mathbb{Y}$ in terms of covering numbers, we obtain dimension independent bounds on the degree of approximation in terms of $n$, where also the constants involved are all dependent at most polynomially on the dimensions. Applications include approximation by power rectified linear unit networks, zonal function networks, certain radial basis function networks as well as the important problem of function extension to higher dimensional spaces.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一个更加锐化的结果，即纸张《独立维度下的深度网络 bound》（Neural Networks, 123（2020），142-152页）。我们假设 $\mathbb{X}$ 和 $\mathbb{Y}$ 是两个紧密的 метри空间。我们考虑了 $\mathbb{X}$ 上函数 $x\mapsto\int_{\mathbb{Y}} G(x,y)d\tau(y)$ 的近似问题，其中 $G$ 是一个从 $\mathbb{X}$ 到 $\mathbb{Y}$ 的函数，$x\in\mathbb{X}$，$y_1,\cdots,y_n\in\mathbb{Y}$，$a_1,\cdots,a_n\in\mathbb{R}$。我们使用 $\mathbb{X}$ 和 $\mathbb{Y}$ 的覆盖数来定义这些空间的维度，并得到了独立于维度的近似度 bound，其中包括了 $n$ 的度量，同时 constants 都是仅仅受到维度的 polynomial 依赖。这些结果有广泛的应用，包括矩形函数网络、zonal函数网络、某些径向基函数网络以及高维空间中函数扩展的重要问题。
</details></li>
</ul>
<hr>
<h2 id="Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023"><a href="#Why-Linguistics-Will-Thrive-in-the-21st-Century-A-Reply-to-Piantadosi-2023" class="headerlink" title="Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)"></a>Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03228">http://arxiv.org/abs/2308.03228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jordan Kodner, Sarah Payne, Jeffrey Heinz</li>
<li>for: The paper argues against Piantadosi’s (2023) claim that modern language models refute Chomsky’s approach to language, and instead presents a case for the continued relevance of generative linguistics in the 21st century.</li>
<li>methods: The paper focuses on four main points to critique Piantadosi’s claim, including the limited data exposure required for human language acquisition, the inability of LLMs to solve the central mystery of language learning, the lack of interpretable explanations provided by LLMs, and the need for a separate theory of language and cognition.</li>
<li>results: The paper concludes that generative linguistics will remain an indispensable scientific discipline throughout the 21st century and beyond, due to its ability to provide a theory of language and cognition that can explain human linguistic and cognitive capabilities.<details>
<summary>Abstract</summary>
We present a critical assessment of Piantadosi's (2023) claim that "Modern language models refute Chomsky's approach to language," focusing on four main points. First, despite the impressive performance and utility of large language models (LLMs), humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery. Second, what can the artificial reveal about the natural? Put simply, the implications of LLMs for our understanding of the cognitive structures and mechanisms underlying language and its acquisition are like the implications of airplanes for understanding how birds fly. Third, LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions. This leads to our final point: to even determine whether the linguistic and cognitive capabilities of LLMs rival those of humans requires explicating what humans' capacities actually are. In other words, it requires a separate theory of language and cognition; generative linguistics provides precisely such a theory. As such, we conclude that generative linguistics as a scientific discipline will remain indispensable throughout the 21st century and beyond.
</details>
<details>
<summary>摘要</summary>
我们提出了对Piantadosi（2023）的批判，关注四个主要点。首先，虽然大型语言模型（LLMs）表现出色且有用，但人类通过对数量更少的数据进行抵抗而获得语言能力。孩子们在获得native语言的能力上即使只有相对较少的暴露也能够成为流利的语言使用者，这是语言学习的中心谜题，LLMs目前并没有解决这个谜题。第二，人工智能可以抛光自然语言吗？简单地说，LLMs对语言和其学习的各种认知结构和机制的含义是如何飞行机器对鸟类飞行的含义。第三，LLMs不能代表语言科学的理论，主要是因为科学理论需要可解释的解释，而不仅仅是预测。这导致我们的最后一点：以规避解释 humans的语言和认知能力的准确性，我们需要一个分析 humans的语言和认知能力的理论。通过这样的理论，我们可以确定LLMs的语言和认知能力是否与人类相当。因此，我们结论认为生成语言学将在21世纪和以后保持不可或缺的重要性。
</details></li>
</ul>
<hr>
<h2 id="Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning"><a href="#Local-Consensus-Enhanced-Siamese-Network-with-Reciprocal-Loss-for-Two-view-Correspondence-Learning" class="headerlink" title="Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning"></a>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03217">http://arxiv.org/abs/2308.03217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linbo Wang, Jing Wu, Xianyong Fang, Zhengyi Liu, Chenjie Cao, Yanwei Fu</li>
<li>for: 提高两视匹配学习框架的性能，特别是对于缺乏精度的相对pose和匹配可靠性的评估。</li>
<li>methods: 提议了两个方法来提高现有框架：一是Local Feature Consensus（LFC）插件块，可以增强现有模型的特征；二是将现有模型扩展为对称网络，并使用对称损失来利用相互投影的信息。</li>
<li>results: 通过对比 dataset 上的表现，实际 achieved state-of-the-art 性能。<details>
<summary>Abstract</summary>
Recent studies of two-view correspondence learning usually establish an end-to-end network to jointly predict correspondence reliability and relative pose. We improve such a framework from two aspects. First, we propose a Local Feature Consensus (LFC) plugin block to augment the features of existing models. Given a correspondence feature, the block augments its neighboring features with mutual neighborhood consensus and aggregates them to produce an enhanced feature. As inliers obey a uniform cross-view transformation and share more consistent learned features than outliers, feature consensus strengthens inlier correlation and suppresses outlier distraction, which makes output features more discriminative for classifying inliers/outliers. Second, existing approaches supervise network training with the ground truth correspondences and essential matrix projecting one image to the other for an input image pair, without considering the information from the reverse mapping. We extend existing models to a Siamese network with a reciprocal loss that exploits the supervision of mutual projection, which considerably promotes the matching performance without introducing additional model parameters. Building upon MSA-Net, we implement the two proposals and experimentally achieve state-of-the-art performance on benchmark datasets.
</details>
<details>
<summary>摘要</summary>
最近的两视匹配学习研究通常建立一个端到端网络，同时预测匹配可靠性和相对pose。我们从两个方面提高这种框架：首先，我们提议在已有模型中添加本地特征协调（LFC）插件块，用于增强特征。给定一个匹配特征，该块在相邻特征中查找mutual neighborhood consensus，并将其聚合以生成提高后的特征。由于准确的匹配点遵循均匀的双视变换，并且在不同视图中分享更多的学习特征，因此特征协调强化匹配点的相互关联，同时减少了噪声的扰乱，使输出特征更加精细地分类匹配点和噪声。其次，现有方法在网络训练时使用真实的匹配和主对投影，而不考虑反向映射的信息。我们扩展现有模型到siamese网络，并使用相互损失来优化匹配性，不需要添加更多的模型参数。基于MSA-Net，我们实现了两个提议，并在benchmark数据集上实验性实现了状态之前的性能。
</details></li>
</ul>
<hr>
<h2 id="The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning"><a href="#The-Effect-of-SGD-Batch-Size-on-Autoencoder-Learning-Sparsity-Sharpness-and-Feature-Learning" class="headerlink" title="The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning"></a>The Effect of SGD Batch Size on Autoencoder Learning: Sparsity, Sharpness, and Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03215">http://arxiv.org/abs/2308.03215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Ghosh, Spencer Frei, Wooseok Ha, Bin Yu</li>
<li>for: 这个论文研究了使用梯度下降法（SGD）训练单神经 autoencoder 的动态性，特别是在线性或ReLU激活函数下。</li>
<li>methods: 这篇论文使用了SGD算法，并研究了不同批处理大小的影响。</li>
<li>results: 研究发现，SGD可以在非对称问题上找到全局最优解，但是这个全局最优解与批处理大小有关。在全批处理情况下，解是密集的（即不含杂），并且与初始方向高度一致，这表明在这种情况下， Stochastic gradient descent 并没有进行feature learning。相反，在任何小于样本数的批处理情况下，SGD会找到一个全局最优解，这个解是稀疏的（即与初始方向垂直），并且与feature learning相关。此外，通过评估梯度下降法的锐度，我们发现，使用全批处理 gradient descent 时，解的锐度较低，而使用小于样本数的批处理时，解的锐度较高。<details>
<summary>Abstract</summary>
In this work, we investigate the dynamics of stochastic gradient descent (SGD) when training a single-neuron autoencoder with linear or ReLU activation on orthogonal data. We show that for this non-convex problem, randomly initialized SGD with a constant step size successfully finds a global minimum for any batch size choice. However, the particular global minimum found depends upon the batch size. In the full-batch setting, we show that the solution is dense (i.e., not sparse) and is highly aligned with its initialized direction, showing that relatively little feature learning occurs. On the other hand, for any batch size strictly smaller than the number of samples, SGD finds a global minimum which is sparse and nearly orthogonal to its initialization, showing that the randomness of stochastic gradients induces a qualitatively different type of "feature selection" in this setting. Moreover, if we measure the sharpness of the minimum by the trace of the Hessian, the minima found with full batch gradient descent are flatter than those found with strictly smaller batch sizes, in contrast to previous works which suggest that large batches lead to sharper minima. To prove convergence of SGD with a constant step size, we introduce a powerful tool from the theory of non-homogeneous random walks which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
在这个研究中，我们研究束变数学梯度下降（SGD）在训练单神经 autoencoder 时的动态。我们发现，对于非拟合问题，随机初始化的 SGD  WITH constant step size 能够成功找到全局最小值，但特定的全局最小值与批处理大小有关。在完整批处理Setting中，我们显示解是 dense（即不夹），与其初始方向高度对齐，表明相对较少特征学习发生。相反，任何小于样本数量的批处理大小，SGD 会找到一个全局最小值，这个最小值是稀疏的并且几乎与其初始方向垂直，这表明随机梯度的Randomness 导致了一种不同类型的 "特征选择"。此外，如果我们测量 minimum 的锐度通过 Hessian 的跟踪，则使用 full batch 梯度下降的 minimum 是浅的，而不同于先前的研究，大批处理会导致更锐的 minimum。为证明 SGD  WITH constant step size 的收敛，我们引入了非Homogeneous random walks 的一种有力的工具，这可能是独立的 интерес。
</details></li>
</ul>
<hr>
<h2 id="Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits"><a href="#Average-Hard-Attention-Transformers-are-Constant-Depth-Uniform-Threshold-Circuits" class="headerlink" title="Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits"></a>Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03212">http://arxiv.org/abs/2308.03212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lena Strobl</li>
<li>for: 本研究探讨了Transformers模型在自然语言处理任务中的应用，以及它们与常深度阈值电路之间的关系。</li>
<li>methods: 本研究使用了average-hard attention和log-precision transformers两种模型，并证明它们可以recognizeTC0复杂度类型的语言。</li>
<li>results: 研究结果显示，transformers模型可以被视为常深度阈值电路的延展，并且可以通过生成一个固定深度和尺寸的阈值电路来实现。此外，研究还发现log-precision transformers模型比average-hard attention transformers模型更为稳定和可靠。<details>
<summary>Abstract</summary>
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
</details>
<details>
<summary>摘要</summary>
启发器（Transformers）已经成为自然语言处理任务中广泛使用的神经网络模型。前一些研究探讨了它们与常深度阈值电路之间的关系，并假设了两点：均值困难注意力和对内部计算的对数精度相对于输入长度。美利尔等（2022）证明了均值困难注意力启发器可以识别TC0复杂度类型中的语言，这种语言可以被表示为常深度多阶阈值电路。 Similarly,美利尔和沙巴华尔（2023）表明了log精度启发器可以识别uniform TC0复杂度类型中的语言。这示出了两种启发器模型都可以被模拟为常深度阈值电路，但后者更加稳定，因为它生成了一个具有 uniform 性的电路家族。我们的论文显示，第一个结果可以扩展到生成uniformCircuits。
</details></li>
</ul>
<hr>
<h2 id="Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series"><a href="#Time-Parameterized-Convolutional-Neural-Networks-for-Irregularly-Sampled-Time-Series" class="headerlink" title="Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series"></a>Time-Parameterized Convolutional Neural Networks for Irregularly Sampled Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03210">http://arxiv.org/abs/2308.03210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chrysoula Kosma, Giannis Nikolentzos, Michalis Vazirgiannis</li>
<li>for: 处理不规则时间序列数据</li>
<li>methods: 使用时间参数化卷积神经网络（TPCNN），其中卷积核心使用时间explicitly初始化</li>
<li>results: TPCNN在 interpolación 和分类任务中表现竞争力强，同时可以具有更高的效率和可读性。<details>
<summary>Abstract</summary>
Irregularly sampled multivariate time series are ubiquitous in several application domains, leading to sparse, not fully-observed and non-aligned observations across different variables. Standard sequential neural network architectures, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs), consider regular spacing between observation times, posing significant challenges to irregular time series modeling. While most of the proposed architectures incorporate RNN variants to handle irregular time intervals, convolutional neural networks have not been adequately studied in the irregular sampling setting. In this paper, we parameterize convolutional layers by employing time-explicitly initialized kernels. Such general functions of time enhance the learning process of continuous-time hidden dynamics and can be efficiently incorporated into convolutional kernel weights. We, thus, propose the time-parameterized convolutional neural network (TPCNN), which shares similar properties with vanilla convolutions but is carefully designed for irregularly sampled time series. We evaluate TPCNN on both interpolation and classification tasks involving real-world irregularly sampled multivariate time series datasets. Our experimental results indicate the competitive performance of the proposed TPCNN model which is also significantly more efficient than other state-of-the-art methods. At the same time, the proposed architecture allows the interpretability of the input series by leveraging the combination of learnable time functions that improve the network performance in subsequent tasks and expedite the inaugural application of convolutions in this field.
</details>
<details>
<summary>摘要</summary>
非 régulièrement采样多Variable时间序列是各个应用领域中的普遍存在，导致不同变数之间的观测不充分、不一致。标准的时间序列架构，如回传神经网络（RNN）和卷积神经网络（CNN），假设观测时间的规律性，对不规时间序列模型造成严重的挑战。大多数提出的架构包括RNN的变形，但是尚未充分研究在不规时间序列 Setting中的卷积神经网络。在这篇文章中，我们将时间explicitly启动kernel，以提高不规时间序列中隐藏的连续时间动力学学习过程。这些时间敏感的核心函数可以高效地包含到卷积核心中。因此，我们提出了时间参数化卷积神经网络（TPCNN），它和普通的卷积架构相似，但是特别的设计来应对不规时间序列。我们将TPCNN应用到真实世界的不规时间序列多Variable时间序列数据集上的 interpolating 和分类任务中，实验结果显示TPCNN模型的竞争性表现，同时比其他现有的方法更高效。此外，提案的架构可以允许输入序列的解释性，通过搜寻时间函数来提高网络性能，并且将卷积神经网络引入这个领域。
</details></li>
</ul>
<hr>
<h2 id="Communication-Free-Distributed-GNN-Training-with-Vertex-Cut"><a href="#Communication-Free-Distributed-GNN-Training-with-Vertex-Cut" class="headerlink" title="Communication-Free Distributed GNN Training with Vertex Cut"></a>Communication-Free Distributed GNN Training with Vertex Cut</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03209">http://arxiv.org/abs/2308.03209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaidi Cao, Rui Deng, Shirley Wu, Edward W Huang, Karthik Subbian, Jure Leskovec</li>
<li>for: 快速训练图 neural network（GNN）在实际世界上的图中，即使是图中的亿个节点和边也是很困难的，主要是因为需要很大的内存来存储图和其中的中间节点和边特征。现有的分布式方法需要频繁的跨GPU交互，导致训练过程中的时间开销很大，并且随着训练进程的扩展，缓慢的扩展。这里，我们介绍了CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。</li>
<li>methods: CoFree-GNN使用了一种Vertex Cut分区，即在分区过程中，不是截断图中的边，而是将边分区，并将节点信息复制以保持图结构。此外，框架还包括一种重要机制来处理受损的图分布，以保持模型的准确性。此外，我们还提出了一种改进的DropEdge技术来进一步加速训练过程。</li>
<li>results: 使用了一系列实际网络的实验表明，CoFree-GNN可以比现有的GNN训练方法快速到10倍。<details>
<summary>Abstract</summary>
Training Graph Neural Networks (GNNs) on real-world graphs consisting of billions of nodes and edges is quite challenging, primarily due to the substantial memory needed to store the graph and its intermediate node and edge features, and there is a pressing need to speed up the training process. A common approach to achieve speed up is to divide the graph into many smaller subgraphs, which are then distributed across multiple GPUs in one or more machines and processed in parallel. However, existing distributed methods require frequent and substantial cross-GPU communication, leading to significant time overhead and progressively diminishing scalability. Here, we introduce CoFree-GNN, a novel distributed GNN training framework that significantly speeds up the training process by implementing communication-free training. The framework utilizes a Vertex Cut partitioning, i.e., rather than partitioning the graph by cutting the edges between partitions, the Vertex Cut partitions the edges and duplicates the node information to preserve the graph structure. Furthermore, the framework maintains high model accuracy by incorporating a reweighting mechanism to handle a distorted graph distribution that arises from the duplicated nodes. We also propose a modified DropEdge technique to further speed up the training process. Using an extensive set of experiments on real-world networks, we demonstrate that CoFree-GNN speeds up the GNN training process by up to 10 times over the existing state-of-the-art GNN training approaches.
</details>
<details>
<summary>摘要</summary>
训练图 neural network (GNN) 在实际图中包含数百万个节点和边是很困难的，主要因为需要巨量的内存来存储图和其中的中间节点和边特征。随着图的大小不断增长，训练过程的速度变得非常重要。现有的分布式方法需要频繁的跨GPU通信，导致训练过程中的时间开销很大，并且随着图的尺度的增长，缩放性逐渐减退。我们在这里介绍CoFree-GNN，一种新的分布式GNN训练框架，可以快速加速GNN训练过程。CoFree-GNN使用Vertex Cut分区，而不是将图分割为多个分区，然后在每个分区上进行并发训练。此外，框架还包括一种重量调整机制，以处理由复制节点引起的图分布偏见。我们还提出了一种修改后 DropEdge 技术，以进一步加速训练过程。通过对实际网络进行了广泛的实验，我们表明CoFree-GNN可以在 GNN 训练过程中提高速度，达到10倍以上。
</details></li>
</ul>
<hr>
<h2 id="Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP"><a href="#Microvasculature-Segmentation-in-Human-BioMolecular-Atlas-Program-HuBMAP" class="headerlink" title="Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)"></a>Microvasculature Segmentation in Human BioMolecular Atlas Program (HuBMAP)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03203">http://arxiv.org/abs/2308.03203</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Sultan, Yongqiang Wang, James Scanlon, Lisa D’lima</li>
<li>for: 研究人员对于人体细胞地图的详细分类，以实现更好的生物医学研究。</li>
<li>methods: 使用各种后门架构，包括FastAI U-Net模型、更深的模型和Feature Pyramid Networks，对2D Periodic Acid-Schiff（PAS）染色的人类肾脏 Histology 图像进行分类。</li>
<li>results: 透过对基eline U-Net模型的评估，发现不同的后门架构具有不同的表现，提供了实验领域中未来研究的 valuable insights。<details>
<summary>Abstract</summary>
Image segmentation serves as a critical tool across a range of applications, encompassing autonomous driving's pedestrian detection and pre-operative tumor delineation in the medical sector. Among these applications, we focus on the National Institutes of Health's (NIH) Human BioMolecular Atlas Program (HuBMAP), a significant initiative aimed at creating detailed cellular maps of the human body. In this study, we concentrate on segmenting various microvascular structures in human kidneys, utilizing 2D Periodic Acid-Schiff (PAS)-stained histology images. Our methodology begins with a foundational FastAI U-Net model, upon which we investigate alternative backbone architectures, delve into deeper models, and experiment with Feature Pyramid Networks. We rigorously evaluate these varied approaches by benchmarking their performance against our baseline U-Net model. This study thus offers a comprehensive exploration of cutting-edge segmentation techniques, providing valuable insights for future research in the field.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:图像分割作为应用程序的重要工具，涵盖自动驾驶中的人体检测和医疗领域中的先驱性肿瘤定位。其中我们专注于国家医疗研究所（NIH）的人生物分子地图计划（HuBMAP），这是一项创造细胞图像的重要 iniciative。在这种研究中，我们集中在人体肾脏中的微血管结构分割中，使用2D Periodic Acid-Schiff（PAS）染色 histology 图像。我们的方法开始于基础 FastAI U-Net 模型，然后我们 investigate 其他脊梁架构、深度模型和特征峰网络。我们严格评估这些不同的方法，对比基准 U-Net 模型的性能。这项研究因此提供了当今图像分割技术的全面探索，为未来研究提供了有价值的洞察。
</details></li>
</ul>
<hr>
<h2 id="Source-free-Domain-Adaptive-Human-Pose-Estimation"><a href="#Source-free-Domain-Adaptive-Human-Pose-Estimation" class="headerlink" title="Source-free Domain Adaptive Human Pose Estimation"></a>Source-free Domain Adaptive Human Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03202">http://arxiv.org/abs/2308.03202</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidpengucf/sfdahpe">https://github.com/davidpengucf/sfdahpe</a></li>
<li>paper_authors: Qucheng Peng, Ce Zheng, Chen Chen</li>
<li>for: 实现人姿估测（HPE）中的数据隐私和安全性，以及在实际应用中降低数据成本。</li>
<li>methods: 提出了一个新任务 named source-free domain adaptive HPE，并提出了一个新的框架，包括三个模型：源模型、中介模型和目标模型，从源资料保护和目标资料相关的角度来探索任务。</li>
<li>results: 在多个页面适应HPE的评估标准上，提出的方法比现有方法优化了许多margin。<details>
<summary>Abstract</summary>
Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.
</details>
<details>
<summary>摘要</summary>
人体姿势估计（HPE）在多个领域得到广泛应用，包括运动分析、医疗和虚拟现实。然而，实际世界数据的高成本成为HPE的一大挑战。为了解决这个问题，一种方法是使用生成的 sintetic 数据进行HPE模型的训练，然后进行适应（DA）操作以适应实际世界数据。然而，现有的DA方法 дляHPE忽视了数据隐私和安全性，因为它们使用了源数据和目标数据在适应过程中。为此，我们提出了一个新的任务，即无源领域适应HPE，以解决跨领域学习HPE的挑战。我们还提出了一个新的框架，该框架包括三个模型：源模型、中间模型和目标模型，该框架从源保护和目标相关两个角度出发，以更好地保持源信息和适应目标数据。源保护模块能更好地保持源信息，并抵抗噪声，而目标相关模块通过构建一个新的空间概率空间，减少了空间表示的稀疏性，并通过基于这个空间的姿势特定的对比学习和信息最大化来提高pose的预测精度。经过了多个领域适应HPE的 benchmk 测试，我们发现我们的方法在与现有方法进行比较时表现出了显著的优势。代码可以在 <https://github.com/davidpengucf/SFDAHPE> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies"><a href="#Automatically-Correcting-Large-Language-Models-Surveying-the-landscape-of-diverse-self-correction-strategies" class="headerlink" title="Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"></a>Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03188">http://arxiv.org/abs/2308.03188</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/teacherpeterpan/self-correction-llm-papers">https://github.com/teacherpeterpan/self-correction-llm-papers</a></li>
<li>paper_authors: Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang Wang</li>
<li>for: 本文旨在探讨自动反馈技术的应用在大语言模型（LLM）上，以改进LLM的表现和可deployability。</li>
<li>methods: 本文分析和概括了各种使用自动反馈技术的最新研究，包括训练时间、生成时间和后续修正。</li>
<li>results: 本文总结了这些技术的主要应用场景，并对未来的发展和挑战进行了讨论。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "hallucination" 是指 LLM 生成的 conten 中出现的 fictitious or absurd 内容，例如 "the cat is wearing a hat"。* "unfaithful reasoning" 是指 LLM 的输出中出现的不合理或 absurd 逻辑，例如 "the sky is blue because the grass is green"。* "toxic content" 是指 LLM 生成的 conten 中出现的负面或不适合的内容，例如 hate speech 或 vulgar language。* "self-correction" 是指 LLM 本身的输出中自动检测并修正错误或不合理的部分的技术。* "automated feedback" 是指由 LLM 本身或外部系统生成的自动反馈，用于 Rectify  LL M 的输出中的错误或不合理的部分的技术。
</details></li>
</ul>
<hr>
<h2 id="A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions"><a href="#A-Lightweight-Method-for-Modeling-Confidence-in-Recommendations-with-Learned-Beta-Distributions" class="headerlink" title="A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions"></a>A Lightweight Method for Modeling Confidence in Recommendations with Learned Beta Distributions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03186">http://arxiv.org/abs/2308.03186</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nkny/confidencerecsys2023">https://github.com/nkny/confidencerecsys2023</a></li>
<li>paper_authors: Norman Knyazev, Harrie Oosterhuis</li>
<li>for: 这个论文旨在提供一种简单实用的推荐方法，并提供一个直观的度量confidence。</li>
<li>methods: 该方法使用学习 beta 分布（LBD）来预测用户喜好，并通过 beta 分布的形式来表示 confidence。</li>
<li>results: 该方法可以维持与现有方法相当的准确性，同时具有与准确性直接相关的confidence度量。此外，在高精度目标推荐任务中，LBD表现更优于现有方法。<details>
<summary>Abstract</summary>
Most Recommender Systems (RecSys) do not provide an indication of confidence in their decisions. Therefore, they do not distinguish between recommendations of which they are certain, and those where they are not. Existing confidence methods for RecSys are either inaccurate heuristics, conceptually complex or computationally very expensive. Consequently, real-world RecSys applications rarely adopt these methods, and thus, provide no confidence insights in their behavior. In this work, we propose learned beta distributions (LBD) as a simple and practical recommendation method with an explicit measure of confidence. Our main insight is that beta distributions predict user preferences as probability distributions that naturally model confidence on a closed interval, yet can be implemented with the minimal model-complexity. Our results show that LBD maintains competitive accuracy to existing methods while also having a significantly stronger correlation between its accuracy and confidence. Furthermore, LBD has higher performance when applied to a high-precision targeted recommendation task. Our work thus shows that confidence in RecSys is possible without sacrificing simplicity or accuracy, and without introducing heavy computational complexity. Thereby, we hope it enables better insight into real-world RecSys and opens the door for novel future applications.
</details>
<details>
<summary>摘要</summary>
大多数推荐系统（RecSys）没有提供决策的信任度标示。因此，它们无法 отлича между确定的推荐和不确定的推荐。现有的信任方法对RecSys是不准确的规则、概念上复杂或计算昂贵。因此，现实世界中的RecSys应用rarely采用这些方法，因此无法提供信任情况的视角。在这项工作中，我们提议使用学习beta分布（LBD）作为简单、实用的推荐方法，并提供显式的信任度标示。我们的主要发现是，beta分布预测用户喜好的概率分布，自然模型信任的关闭区间，但可以实现最小的模型复杂度。我们的结果表明，LBD与现有方法的竞争性准确度相当，同时其信任度与准确度之间具有显著的相关性。此外，LBD在高精度目标推荐任务中表现更高。我们的工作因此表明，RecSys中的信任是可能的，不需要牺牲简单性或准确度，也不需要承受重大的计算复杂度。这样，我们希望能够为实际世界中的RecSys提供更好的视角，并开启未来应用的新可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems"><a href="#A-Critical-Review-of-Physics-Informed-Machine-Learning-Applications-in-Subsurface-Energy-Systems" class="headerlink" title="A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems"></a>A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04457">http://arxiv.org/abs/2308.04457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdeldjalil Latrach, Mohamed Lamine Malki, Misael Morales, Mohamed Mehana, Minou Rabiei</li>
<li>For: The paper is written for researchers and practitioners in the field of machine learning, particularly in the application of physics-informed machine learning (PIML) to subsurface energy systems, such as the oil and gas industry.* Methods: The paper reviews and discusses the applications of PIML techniques in various tasks related to subsurface energy systems, including seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages.* Results: The paper highlights the successful utilization of PIML for providing more accurate and reliable predictions for resource management and operational efficiency in the oil and gas industry, and demonstrates its potential for revolutionizing the industry and other emerging areas of interest, such as carbon and hydrogen storage, and geothermal systems.<details>
<summary>Abstract</summary>
Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages. Additionally, it demonstrates PIML's capabilities to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage; and geothermal systems by providing more accurate and reliable predictions for resource management and operational efficiency.
</details>
<details>
<summary>摘要</summary>
（注意：以下是简化中文版本）机器学习已经成为不同领域的强大工具，包括计算机视觉、自然语言处理和语音识别。它可以找到大量数据中隐藏的模式，并提供无与伦比的发现，革命化许多行业和学科。然而，机器学习和深度学习模型缺乏解释性和具体领域知识，尤其是在应用物理和工程领域。相反，物理知识整合机器学习（PIML）技术将物理原理 integrate 到数据驱动模型中。通过结合深度学习和领域知识，PIML提高模型的总体化能力，遵循物理法律，并提供解释性。本文全面评论了 relate 到地下能源系统的 PIML 应用，主要是石油和天然气领域。文章强调 PIML 在探测和生产阶段的准确预测、资源管理和运营效率等方面的成功应用。此外，它还 demon стри PIML 可以革命化石油和天然气行业，以及其他emerging 领域，如碳和氢存储，以及地热系统，提供更加准确和可靠的预测，为资源管理和运营效率提供更好的基础。
</details></li>
</ul>
<hr>
<h2 id="Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience"><a href="#Adapting-Machine-Learning-Diagnostic-Models-to-New-Populations-Using-a-Small-Amount-of-Data-Results-from-Clinical-Neuroscience" class="headerlink" title="Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience"></a>Adapting Machine Learning Diagnostic Models to New Populations Using a Small Amount of Data: Results from Clinical Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03175">http://arxiv.org/abs/2308.03175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rongguang Wang, Guray Erus, Pratik Chaudhari, Christos Davatzikos<br>for:This paper aims to address the problem of reproducibility crisis in machine learning (ML) models applied to neuroimaging data, specifically for the diagnosis of Alzheimer’s disease (AD) and schizophrenia (SZ), and estimation of brain age.methods:The authors propose a weighted empirical risk minimization approach that combines data from a source group (e.g., subjects with similar attributes such as sex, age group, race, and clinical cohort) with a small fraction of data from the target group (e.g., other sex, age group, etc.) to make predictions on the target group.results:The approach achieves substantially better accuracy than existing domain adaptation techniques, with area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification, and mean absolute error less than 5 years for brain age prediction on all target groups. The models also demonstrate utility for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment, and lead to new clinical insights regarding correlations with neurophysiological tests.Here is the result in Simplified Chinese text:for:这篇论文目标是解决机器学习（ML）模型在神经成像数据中的可重复性危机，特别是对于诊断阿尔ц海默病（AD）和偏头病（SZ），以及脑年龄的估计。methods:作者提议一种加权实际风险最小化方法，将来源组（例如，按照性别、年龄组、种族和临床群组划分的subject）与小型数据集（例如，其他性别、年龄组、等等）结合，以便在目标组（例如，其他性别、年龄组、等等）上进行预测。results:该方法在现有的领域适应技术上取得了显著更好的准确性，其中AD分类 tasks 的准确率大于 0.95，SZ分类 tasks 的准确率大于 0.7，脑年龄预测 tasks 的平均绝对误差小于 5 年，并在所有目标组上具有良好的一致性。此外，模型还能够用于诊断患有轻度认知障碍的病人的疾病进程预测，并且提供了新的临床意义，例如脑physiological 测试的相关性。<details>
<summary>Abstract</summary>
Machine learning (ML) has shown great promise for revolutionizing a number of areas, including healthcare. However, it is also facing a reproducibility crisis, especially in medicine. ML models that are carefully constructed from and evaluated on a training set might not generalize well on data from different patient populations or acquisition instrument settings and protocols. We tackle this problem in the context of neuroimaging of Alzheimer's disease (AD), schizophrenia (SZ) and brain aging. We develop a weighted empirical risk minimization approach that optimally combines data from a source group, e.g., subjects are stratified by attributes such as sex, age group, race and clinical cohort to make predictions on a target group, e.g., other sex, age group, etc. using a small fraction (10%) of data from the target group. We apply this method to multi-source data of 15,363 individuals from 20 neuroimaging studies to build ML models for diagnosis of AD and SZ, and estimation of brain age. We found that this approach achieves substantially better accuracy than existing domain adaptation techniques: it obtains area under curve greater than 0.95 for AD classification, area under curve greater than 0.7 for SZ classification and mean absolute error less than 5 years for brain age prediction on all target groups, achieving robustness to variations of scanners, protocols, and demographic or clinical characteristics. In some cases, it is even better than training on all data from the target group, because it leverages the diversity and size of a larger training set. We also demonstrate the utility of our models for prognostic tasks such as predicting disease progression in individuals with mild cognitive impairment. Critically, our brain age prediction models lead to new clinical insights regarding correlations with neurophysiological tests.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration"><a href="#Two-Sides-of-Miscalibration-Identifying-Over-and-Under-Confidence-Prediction-for-Network-Calibration" class="headerlink" title="Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration"></a>Two Sides of Miscalibration: Identifying Over and Under-Confidence Prediction for Network Calibration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03172">http://arxiv.org/abs/2308.03172</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aoshuang92/miscalibration_ts">https://github.com/aoshuang92/miscalibration_ts</a></li>
<li>paper_authors: Shuang Ao, Stefan Rueger, Advaith Siddharthan</li>
<li>for: 本研究旨在探讨深度神经网络的准确性calibration问题，以提高安全关键任务中模型的预测可靠性。</li>
<li>methods: 本研究提出了一种新的评估指标——miscalibration score，用于评估模型的总体和分类准确性状态，包括过度自信和下降自信。此外，我们还提出了一种基于分类准确性评估的calibration技术，可以处理过度自信和下降自信问题。</li>
<li>results: 我们的实验结果显示，我们的提出的calibration技术在诸多任务上显著超过了现有的calibration技术。此外，我们还验证了我们的方法在自动故障检测任务中的可靠性和信任性，并发现我们的方法可以提高故障检测和模型的信任性。<details>
<summary>Abstract</summary>
Proper confidence calibration of deep neural networks is essential for reliable predictions in safety-critical tasks. Miscalibration can lead to model over-confidence and/or under-confidence; i.e., the model's confidence in its prediction can be greater or less than the model's accuracy. Recent studies have highlighted the over-confidence issue by introducing calibration techniques and demonstrated success on various tasks. However, miscalibration through under-confidence has not yet to receive much attention. In this paper, we address the necessity of paying attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as trustworthiness of the model. The code are available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details>
<details>
<summary>摘要</summary>
深度神经网络的准确性calibration是安全关键任务中的一个重要因素。不当calibration可能导致模型过于自信或者不足自信，即模型对其预测的自信度高于或低于模型的准确率。 latest studies have highlighted the over-confidence issue by introducing calibration techniques and have demonstrated success on various tasks. However, the under-confidence issue has not yet received much attention. In this paper, we emphasize the need to pay attention to the under-confidence issue. We first introduce a novel metric, a miscalibration score, to identify the overall and class-wise calibration status, including being over or under-confident. Our proposed metric reveals the pitfalls of existing calibration techniques, where they often overly calibrate the model and worsen under-confident predictions. Then we utilize the class-wise miscalibration score as a proxy to design a calibration technique that can tackle both over and under-confidence. We report extensive experiments that show our proposed methods substantially outperforming existing calibration techniques. We also validate our proposed calibration technique on an automatic failure detection task with a risk-coverage curve, reporting that our methods improve failure detection as well as the trustworthiness of the model. The code is available at \url{https://github.com/AoShuang92/miscalibration_TS}.
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques"><a href="#Detection-of-Anomalies-in-Multivariate-Time-Series-Using-Ensemble-Techniques" class="headerlink" title="Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques"></a>Detection of Anomalies in Multivariate Time Series Using Ensemble Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03171">http://arxiv.org/abs/2308.03171</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anastasios Iliopoulos, John Violos, Christos Diou, Iraklis Varlamis</li>
<li>for: 本研究旨在提出一种基于深度神经网络的多变量时间序列异常检测方法，以解决多变量时间序列中异常事件罕见性的问题。</li>
<li>methods: 本方法基于LSTM、Autoencoder和Convolutional Autoencoder等深度神经网络模型，并采用特征袋包技术和嵌入式PCA变换以提高模型性能。</li>
<li>results: 对于SKAB数据集，提议的ensemble方法在异常检测精度方面比基本模型提高2%，而在半监督模型中，提议的方法在异常检测精度方面比基本模型提高10%以上。<details>
<summary>Abstract</summary>
Anomaly Detection in multivariate time series is a major problem in many fields. Due to their nature, anomalies sparsely occur in real data, thus making the task of anomaly detection a challenging problem for classification algorithms to solve. Methods that are based on Deep Neural Networks such as LSTM, Autoencoders, Convolutional Autoencoders etc., have shown positive results in such imbalanced data. However, the major challenge that algorithms face when applied to multivariate time series is that the anomaly can arise from a small subset of the feature set. To boost the performance of these base models, we propose a feature-bagging technique that considers only a subset of features at a time, and we further apply a transformation that is based on nested rotation computed from Principal Component Analysis (PCA) to improve the effectiveness and generalization of the approach. To further enhance the prediction performance, we propose an ensemble technique that combines multiple base models toward the final decision. In addition, a semi-supervised approach using a Logistic Regressor to combine the base models' outputs is proposed. The proposed methodology is applied to the Skoltech Anomaly Benchmark (SKAB) dataset, which contains time series data related to the flow of water in a closed circuit, and the experimental results show that the proposed ensemble technique outperforms the basic algorithms. More specifically, the performance improvement in terms of anomaly detection accuracy reaches 2% for the unsupervised and at least 10% for the semi-supervised models.
</details>
<details>
<summary>摘要</summary>
异常检测在多变量时间序列中是许多领域的主要问题。由于异常事件罕见发生，因此将异常检测作为分类算法解决的问题是一项挑战。基于深度神经网络的方法，如LSTM、Autoencoder和Convolutional Autoencoder等，在这种不均衡数据中表现了积极的结果。然而，在多变量时间序列中，异常可能来自一个小集合特征。为了提高基本模型的性能，我们提议一种特征袋装技术，该技术仅考虑一部分特征，并应用基于主成分分析（PCA）计算的嵌入式旋转变换来提高效果和泛化性。此外，我们还提议一种 ensemble 技术，该技术将多个基本模型的输出合并到最终决策中。此外，我们还提出了一种半监督方法，该方法使用Logistic Regressor将基本模型的输出合并到最终决策中。我们对 Skoltech Anomaly Benchmark（SKAB）数据集进行实验，该数据集包含关于水流在封闭环流中的时间序列数据，实验结果表明，我们的ensemble技术在异常检测精度方面比基本算法提高了2%（非监督）和至少10%（半监督）。
</details></li>
</ul>
<hr>
<h2 id="FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire"><a href="#FireFly-A-Synthetic-Dataset-for-Ember-Detection-in-Wildfire" class="headerlink" title="FireFly A Synthetic Dataset for Ember Detection in Wildfire"></a>FireFly A Synthetic Dataset for Ember Detection in Wildfire</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03164">http://arxiv.org/abs/2308.03164</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ergowho/firefly2.0">https://github.com/ergowho/firefly2.0</a></li>
<li>paper_authors: Yue Hu, Xinan Ye, Yifei Liu, Souvik Kundu, Gourav Datta, Srikar Mutnuri, Namo Asavisanu, Nora Ayanian, Konstantinos Psounis, Peter Beerel</li>
<li>for: 本研究旨在提供一个用于烟火检测的人工数据集，以替代当前烟火特有训练资源的缺乏。</li>
<li>methods: 本研究使用Unreal Engine 4 (UE4)创建了一个名为”FireFly”的人工数据集，并提供了一个自动生成标注数据集的工具，以实现数据多样性和用户需求的定制。</li>
<li>results: 对于四种流行的物体检测模型，使用FireFly数据集进行评估，得到了8.57%的提升 Mean Average Precision (mAP) 在实际野外烟火场景中。<details>
<summary>Abstract</summary>
This paper presents "FireFly", a synthetic dataset for ember detection created using Unreal Engine 4 (UE4), designed to overcome the current lack of ember-specific training resources. To create the dataset, we present a tool that allows the automated generation of the synthetic labeled dataset with adjustable parameters, enabling data diversity from various environmental conditions, making the dataset both diverse and customizable based on user requirements. We generated a total of 19,273 frames that have been used to evaluate FireFly on four popular object detection models. Further to minimize human intervention, we leveraged a trained model to create a semi-automatic labeling process for real-life ember frames. Moreover, we demonstrated an up to 8.57% improvement in mean Average Precision (mAP) in real-world wildfire scenarios compared to models trained exclusively on a small real dataset.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/07/cs.LG_2023_08_07/" data-id="cllsk9gqa00329c88ayqlalwy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/cs.SD_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/cs.SD_2023_08_07/">cs.SD - 2023-08-07 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm"><a href="#Active-Noise-Control-based-on-the-Momentum-Multichannel-Normalized-Filtered-x-Least-Mean-Square-Algorithm" class="headerlink" title="Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm"></a>Active Noise Control based on the Momentum Multichannel Normalized Filtered-x Least Mean Square Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03684">http://arxiv.org/abs/2308.03684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongyuan Shi, Woon-Seng Gan, Bhan Lam, Shulin Wen, Xiaoyi Shen</li>
<li>for: 实现多通道活动噪声控制（MCANC）中的广泛噪声抑制区域。</li>
<li>methods: 使用了Filter-x最小均方（FxLMS）算法，但是它的对应速度较慢，不适合处理快速变化的噪声，如堆积噪声。此外，噪声功率的变化也会对算法的稳定性产生负面影响。</li>
<li>results: 通过与振踪方法结合，实现了对MCANC中的噪声控制的有效控制，并且加速了算法的步进调整。在实际应用中，通过使用多通道噪声控制窗口来控制机器噪声。<details>
<summary>Abstract</summary>
Multichannel active noise control (MCANC) is widely utilized to achieve significant noise cancellation area in the complicated acoustic field. Meanwhile, the filter-x least mean square (FxLMS) algorithm gradually becomes the benchmark solution for the implementation of MCANC due to its low computational complexity. However, its slow convergence speed more or less undermines the performance of dealing with quickly varying disturbances, such as piling noise. Furthermore, the noise power variation also deteriorates the robustness of the algorithm when it adopts the fixed step size. To solve these issues, we integrated the normalized multichannel FxLMS with the momentum method, which hence, effectively avoids the interference of the primary noise power and accelerates the convergence of the algorithm. To validate its effectiveness, we deployed this algorithm in a multichannel noise control window to control the real machine noise.
</details>
<details>
<summary>摘要</summary>
多通道活动噪声控制（MCANC）广泛应用于复杂的噪声场中实现显著的噪声抑制面积。同时，Filter-x最小二乘（FxLMS）算法逐渐成为MCANC的实现标准方案，主要是因为它的计算复杂度较低。然而，它的慢速对应变化的干扰有很大的影响，如堆叠噪声。此外，噪声功率变化也会对算法的稳定性产生负面影响，特别是当采用固定步长时。为解决这些问题，我们将normalized multichannel FxLMS与旋转方法相结合，从而有效避免了主要噪声功率的干扰，并加速了算法的收敛速度。为验证其效果，我们在多通道噪声控制窗口中应用了这种算法，控制了实际机器的噪声。
</details></li>
</ul>
<hr>
<h2 id="AudioVMAF-Audio-Quality-Prediction-with-VMAF"><a href="#AudioVMAF-Audio-Quality-Prediction-with-VMAF" class="headerlink" title="AudioVMAF: Audio Quality Prediction with VMAF"></a>AudioVMAF: Audio Quality Prediction with VMAF</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03437">http://arxiv.org/abs/2308.03437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arijit Biswas, Harald Mundt</li>
<li>for: 这个论文的目的是提出一种基于现有VMAF的听力 inspirited frontend，用于创建参考视频和编码spectrograms，并扩展VMAF以测试编码音质。</li>
<li>methods: 该系统使用了图像复制来进一步提高预测精度，特别是在存在带限 anchors 时。</li>
<li>results: 提议方法在现有视觉质量特征的抽取改进下显著 OUTPERFORMS 所有现有的视觉质量特征重新定义为音频质量特征，并且在一个专门为音频质量metric（ViSQOL-v3 [4]）也 inspirited from the image domain 上显示了7.8%和2.0%的Pearson和Spearman排名相关度系数的显著提高。<details>
<summary>Abstract</summary>
Video Multimethod Assessment Fusion (VMAF) [1], [2], [3] is a popular tool in the industry for measuring coded video quality. In this study, we propose an auditory-inspired frontend in existing VMAF for creating videos of reference and coded spectrograms, and extended VMAF for measuring coded audio quality. We name our system AudioVMAF. We demonstrate that image replication is capable of further enhancing prediction accuracy, especially when band-limited anchors are present. The proposed method significantly outperforms all existing visual quality features repurposed for audio, and even demonstrates a significant overall improvement of 7.8% and 2.0% of Pearson and Spearman rank correlation coefficient, respectively, over a dedicated audio quality metric (ViSQOL-v3 [4]) also inspired from the image domain.
</details>
<details>
<summary>摘要</summary>
видео多方法评估融合（VMAF）[1], [2], [3] 是行业中常用的视频质量测试工具。在这项研究中，我们提议在现有VMAF中添加音频引入的前端，并将扩展VMAF用于测试编码音频质量。我们称之为AudioVMAF。我们发现，图像复制可以进一步提高预测精度，特别是当存在带限 anchors 时。我们的方法在现有视觉质量特征的抽取方面进行了改进，并且显著超过了所有抽取于音频领域的视觉质量特征，以及专门为音频质量指标（ViSQOL-v3 [4]) 的7.8%和2.0%的普森和斯宾塞相关系数。
</details></li>
</ul>
<hr>
<h2 id="Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation"><a href="#Improving-Deep-Attractor-Network-by-BGRU-and-GMM-for-Speech-Separation" class="headerlink" title="Improving Deep Attractor Network by BGRU and GMM for Speech Separation"></a>Improving Deep Attractor Network by BGRU and GMM for Speech Separation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03332">http://arxiv.org/abs/2308.03332</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rawad Melhem, Assef Jafar, Riad Hamadeh</li>
<li>for: 提高Speech separation技术的简化和效率，使其更适合实际应用。</li>
<li>methods: 使用Bidirectional Gated neural network (BGRU)取代BLSTM，并使用Gaussian Mixture Model (GMM)作为聚类算法。</li>
<li>results: 在TIMIT corpus上评估系统，SDR和PESQ scores分别为12.3 dB和2.94，比原始DANet模型更好，同时减少了20.7%和17.9%的参数和训练时间。<details>
<summary>Abstract</summary>
Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.
</details>
<details>
<summary>摘要</summary>
深度吸引网络（DANet）是语音分离领域的Current Technique，使用双向长短期记忆（BLSTM），但模型复杂度很高。本文提出了简化了的DANet模型，使用双向闭合神经网络（BGRU）而不是BLSTM。在DANet中使用 Gaussian Mixture Model（GMM）作为聚类算法，以降低复杂度并提高学习速度和准确性。用于评价模型的度量包括Signal to Distortion Ratio（SDR）、Signal to Interference Ratio（SIR）、Signal to Artifact Ratio（SAR）以及Perceptual Evaluation Speech Quality（PESQ）分数。使用TIMIT corpus中的两个说话者混合数据集评估提出的模型，系统实现了12.3 dB和2.94的SDR和PESQ分数，分别比原始DANet模型更好。此外，模型的参数数量和训练时间都下降了20.7%和17.9%。模型应用于混合阿拉伯语音信号上，结果比英语更好。
</details></li>
</ul>
<hr>
<h2 id="SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability"><a href="#SeACo-Paraformer-A-Non-Autoregressive-ASR-System-with-Flexible-and-Effective-Hotword-Customization-Ability" class="headerlink" title="SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability"></a>SeACo-Paraformer: A Non-Autoregressive ASR System with Flexible and Effective Hotword Customization Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03266">http://arxiv.org/abs/2308.03266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/r1ckshi/seaco-paraformer">https://github.com/r1ckshi/seaco-paraformer</a></li>
<li>paper_authors: Xian Shi, Yexin Yang, Zerui Li, Shiliang Zhang</li>
<li>for: 实现ASR系统中的热词自定义，提高ASR系统的准确性和效率。</li>
<li>methods: 提出了Semantic-augmented Contextual-Paraformer（SeACo-Paraformer）模型，结合了AED-based模型的精度、NAR模型的效率，并且具有出色的Contextualization能力。</li>
<li>results: 在50,000小时的工业大数据实验中，提出的模型比强基准模型在自定义和一般ASR任务中表现更好，并且探索了一种高效的大规模来处理热词检查的方法。<details>
<summary>Abstract</summary>
Hotword customization is one of the important issues remained in ASR field - it is of value to enable users of ASR systems to customize names of entities, persons and other phrases. The past few years have seen both implicit and explicit modeling strategies for ASR contextualization developed. While these approaches have performed adequately, they still exhibit certain shortcomings such as instability in effectiveness. In this paper we propose Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) a novel NAR based ASR system with flexible and effective hotword customization ability. It combines the accuracy of the AED-based model, the efficiency of the NAR model, and the excellent performance in contextualization. In 50,000 hours industrial big data experiments, our proposed model outperforms strong baselines in customization and general ASR tasks. Besides, we explore an efficient way to filter large scale incoming hotwords for further improvement. The source codes and industrial models proposed and compared are all opened as well as two hotword test sets.
</details>
<details>
<summary>摘要</summary>
<SYS>    <LANGUAGE_MODEL>        <NAME>Simplified Chinese</NAME>        <PATH>/path/to/model</PATH>    </LANGUAGE_MODEL></SYS>这是一个 ASR 领域中的重要问题 - 允许使用者自定义名词、人名和其他短语。过去几年，有内在和外在模型化策略被开发出来解决这个问题。这些方法优秀地表现，但仍然存在一些缺陷，如效果不稳定。在这篇文章中，我们提出 Semantic-augmented Contextual-Paraformer (SeACo-Paraformer) 一种新的 NAR 基于 ASR 系统，具有轻松实现和有效的自定义热词能力。它结合了 AED-based 模型的精度和 NAR 模型的效率，并且在Contextualization 方面表现出色。在50,000小时的工业大数据中，我们的提案模型比强大的基准模型在自定义和一般 ASR 任务中表现出色。此外，我们还探索了一种高效的方法来筛选大规模的进来热词，以进一步提高效能。我们提供了所有的代码和工业模型，以及两个热词测试集。
</details></li>
</ul>
<hr>
<h2 id="Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals"><a href="#Investigation-of-Self-supervised-Pre-trained-Models-for-Classification-of-Voice-Quality-from-Speech-and-Neck-Surface-Accelerometer-Signals" class="headerlink" title="Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals"></a>Investigation of Self-supervised Pre-trained Models for Classification of Voice Quality from Speech and Neck Surface Accelerometer Signals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03226">http://arxiv.org/abs/2308.03226</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarsana Reddy Kadiri, Farhad Javanmardi, Paavo Alku</li>
<li>for: 本研究旨在 investigate the effectiveness of simultaneously-recorded speech and neck surface accelerometer (NSA) signals in the classification of voice quality (breathy, modal, and pressed) using deep learning-based features and a support vector machine (SVM) as well as a convolutional neural network (CNN) as classifiers.</li>
<li>methods: 本研究使用了三种自动学习模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）的自然语言处理特征，以及SVM和CNN分类器。另外，使用了两种信号处理方法（闭合相位滤波和零频滤波）来估计颤腔源波形从语音和NSA信号中提取的特征。</li>
<li>results: 研究发现NSA输入对分类任务的性能更高于语音输入。另外，使用自动学习模型生成的特征对于语音和NSA输入都显示了更高的分类精度，而使用HuBERT特征则表现更好。<details>
<summary>Abstract</summary>
Prior studies in the automatic classification of voice quality have mainly studied the use of the acoustic speech signal as input. Recently, a few studies have been carried out by jointly using both speech and neck surface accelerometer (NSA) signals as inputs, and by extracting MFCCs and glottal source features. This study examines simultaneously-recorded speech and NSA signals in the classification of voice quality (breathy, modal, and pressed) using features derived from three self-supervised pre-trained models (wav2vec2-BASE, wav2vec2-LARGE, and HuBERT) and using a SVM as well as CNNs as classifiers. Furthermore, the effectiveness of the pre-trained models is compared in feature extraction between glottal source waveforms and raw signal waveforms for both speech and NSA inputs. Using two signal processing methods (quasi-closed phase (QCP) glottal inverse filtering and zero frequency filtering (ZFF)), glottal source waveforms are estimated from both speech and NSA signals. The study has three main goals: (1) to study whether features derived from pre-trained models improve classification accuracy compared to conventional features (spectrogram, mel-spectrogram, MFCCs, i-vector, and x-vector), (2) to investigate which of the two modalities (speech vs. NSA) is more effective in the classification task with pre-trained model-based features, and (3) to evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier. The results revealed that the use of the NSA input showed better classification performance compared to the speech signal. Between the features, the pre-trained model-based features showed better classification accuracies, both for speech and NSA inputs compared to the conventional features. It was also found that the HuBERT features performed better than the wav2vec2-BASE and wav2vec2-LARGE features.
</details>
<details>
<summary>摘要</summary>
前研究主要是使用语音信号作为输入，做自动识别声音质量的研究。近年来，一些研究开始将语音信号和脖子表面加速器（NSA）信号同时录制，并提取MFCCs和喉咙源特征。本研究用三个自我超vised模型（wav2vec2-BASE、wav2vec2-LARGE和HuBERT）提取特征，并使用支持向量机（SVM）和卷积神经网络（CNN）作为分类器。此外，对于语音和NSA输入，对喉咙源波形和原始信号波形进行预处理，并使用 quasi-closed phase（QCP）预测和零频 filtering（ZFF）来估算喉咙源波形。研究的三个主要目标是：（1）研究 Whether features derived from pre-trained models improve classification accuracy compared to conventional features（spectrogram、mel-spectrogram、MFCCs、i-vector、x-vector），（2）investigate which modality（speech vs. NSA）is more effective in the classification task with pre-trained model-based features，（3）evaluate whether the deep learning-based CNN classifier can enhance the classification accuracy in comparison to the SVM classifier。结果表明，使用NSA输入的 classification 性能比语音信号更好。此外，使用 pre-trained model-based features 也比使用传统特征更好，同时 HuBERT 特征也比 wav2vec2-BASE 和 wav2vec2-LARGE 特征更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/07/cs.SD_2023_08_07/" data-id="cllsk9gqx00589c884quw0fsx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_07" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/07/eess.IV_2023_08_07/" class="article-date">
  <time datetime="2023-08-06T16:00:00.000Z" itemprop="datePublished">2023-08-07</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/07/eess.IV_2023_08_07/">eess.IV - 2023-08-07 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe"><a href="#SoilNet-An-Attention-based-Spatio-temporal-Deep-Learning-Framework-for-Soil-Organic-Carbon-Prediction-with-Digital-Soil-Mapping-in-Europe" class="headerlink" title="SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe"></a>SoilNet: An Attention-based Spatio-temporal Deep Learning Framework for Soil Organic Carbon Prediction with Digital Soil Mapping in Europe</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03586">http://arxiv.org/abs/2308.03586</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nafiseh Kakhani, Moien Rangzan, Ali Jamali, Sara Attarchi, Seyed Kazem Alavipanah, Thomas Scholten</li>
<li>for: 这项研究旨在提高数字土壤地图（DSM）技术，尤其是在使用深度学习（DL）方法来预测土壤有机碳（SOC）的空间特征。</li>
<li>methods: 该研究提出了一种新的架构， combining 基于卷积神经网络（CNN）的空间信息和基于长短时间记忆（LSTM）网络的气候时序信息，以预测欧洲各地的SOC。该模型使用了一组全面的环境特征，包括兰达特-8图像、地形、远程感知指数和气候时序序列，作为输入特征。</li>
<li>results: 研究结果表明，提出的方框比常用的多项Random Forest方法（ML）更高效，具有较低的根圆平方误差（RMSE）。这种模型是一种可靠的SOC预测工具，可以应用于其他土壤特征预测，从而为土地管理和决策过程提供更准确的信息。<details>
<summary>Abstract</summary>
Digital soil mapping (DSM) is an advanced approach that integrates statistical modeling and cutting-edge technologies, including machine learning (ML) methods, to accurately depict soil properties and their spatial distribution. Soil organic carbon (SOC) is a crucial soil attribute providing valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity. This study highlights the significance of spatial-temporal deep learning (DL) techniques within the DSM framework. A novel architecture is proposed, incorporating spatial information using a base convolutional neural network (CNN) model and spatial attention mechanism, along with climate temporal information using a long short-term memory (LSTM) network, for SOC prediction across Europe. The model utilizes a comprehensive set of environmental features, including Landsat-8 images, topography, remote sensing indices, and climate time series, as input features. Results demonstrate that the proposed framework outperforms conventional ML approaches like random forest commonly used in DSM, yielding lower root mean square error (RMSE). This model is a robust tool for predicting SOC and could be applied to other soil properties, thereby contributing to the advancement of DSM techniques and facilitating land management and decision-making processes based on accurate information.
</details>
<details>
<summary>摘要</summary>
数字土壤地图（DSM）是一种先进的方法，它将统计模型和前沿技术，包括机器学习（ML）方法，结合起来准确地描述土壤属性和其空间分布。土壤有机碳（SOC）是一个重要的土壤特征，它提供了valuable insights into soil health, nutrient cycling, greenhouse gas emissions, and overall ecosystem productivity.这项研究强调了在 DSM 框架中使用空间-时间深度学习（DL）技术的重要性。该研究提出了一种新的建议，其包括在基于 convolutional neural network（CNN）模型和空间注意机制的基础上，以及使用 long short-term memory（LSTM）网络来预测欧洲各地的 SOC。该模型使用了包括 Landsat-8 图像、地形、 remote sensing 指标和气候时序序列在内的全面的环境特征作为输入特征。结果表明，提议的框架在与常见的 ML 方法如随机森林相比，具有较低的根圆平均误差（RMSE）。这种模型是一种准确预测 SOC 的工具，可以应用于其他土壤属性，从而为 DSM 技术的发展和土地管理决策提供支持。
</details></li>
</ul>
<hr>
<h2 id="Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice"><a href="#Quantitative-MR-Image-Reconstruction-using-Parameter-Specific-Dictionary-Learning-with-Adaptive-Dictionary-Size-and-Sparsity-Level-Choice" class="headerlink" title="Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice"></a>Quantitative MR Image Reconstruction using Parameter-Specific Dictionary Learning with Adaptive Dictionary-Size and Sparsity-Level Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03460">http://arxiv.org/abs/2308.03460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andreas Kofler, Kirsten Miriam Kerkering, Laura Göschel, Ariane Fillmer, Cristoph Kolbitsch</li>
<li>for: 这种方法是用于量子磁共振成像（QMRI）中的参数地图重建的。</li>
<li>methods: 方法使用了字典学习（DL）和稀疏编码（SC）算法自动计算最佳字典大小和稀疏程度，并在不同参数地图之间进行自适应调整。</li>
<li>results: 该方法在RMSE和PSNR方面胜过MAP、TV、Wl和Sh方法，并且与DL+Fit方法具有相似或更好的效果，同时加速了重建过程约7倍。In English, this means:</li>
<li>for: This method is proposed for the reconstruction of parameter maps in Quantitative Magnetic Resonance Imaging (QMRI).</li>
<li>methods: The method uses dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter map, and adaptively adjusts the parameters between different maps.</li>
<li>results: The proposed method outperforms the compared methods (MAP, TV, Wl, and Sh) in terms of RMSE and PSNR, and has similar or better effects as the DL+Fit method, while significantly accelerating the reconstruction process by a factor of approximately seven.<details>
<summary>Abstract</summary>
Objective: We propose a method for the reconstruction of parameter-maps in Quantitative Magnetic Resonance Imaging (QMRI).   Methods: Because different quantitative parameter-maps differ from each other in terms of local features, we propose a method where the employed dictionary learning (DL) and sparse coding (SC) algorithms automatically estimate the optimal dictionary-size and sparsity level separately for each parameter-map. We evaluated the method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb data as well as in-vivo brain images acquired on an ultra-high field 7T scanner. We compared it to a model-based acceleration for parameter mapping (MAP) approach, other sparsity-based methods using total variation (TV), Wavelets (Wl) and Shearlets (Sh), and to a method which uses DL and SC to reconstruct qualitative images, followed by a non-linear (DL+Fit).   Results: Our algorithm surpasses MAP, TV, Wl and Sh in terms of RMSE and PSNR. It yields better or comparable results to DL+Fit by additionally significantly accelerating the reconstruction by a factor of approximately seven.   Conclusion: The proposed method outperforms the reported methods of comparison and yields accurate $T_1$-maps. Although presented for $T_1$-mapping in the brain, our method's structure is general and thus most probably also applicable for the the reconstruction of other quantitative parameters in other organs.   Significance: From a clinical perspective, the obtained $T_1$-maps could be utilized to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be employed to obtain ground-truth data for the development of data-driven methods based on supervised learning.+
</details>
<details>
<summary>摘要</summary>
Methods: Different quantitative parameter-maps have unique local features, so we use dictionary learning (DL) and sparse coding (SC) algorithms to automatically estimate the optimal dictionary size and sparsity level for each parameter-map. We evaluated our method on a $T_1$-mapping QMRI problem in the brain using the BrainWeb dataset and in-vivo brain images acquired on a 7T scanner. We compared our method to a model-based acceleration for parameter mapping (MAP) approach, as well as other sparsity-based methods using total variation (TV), wavelets (Wl), and shearlets (Sh). We also compared our method to a method that uses DL and SC to reconstruct qualitative images and then uses non-linear registration (DL+Fit).Results: Our method outperformed MAP, TV, Wl, and Sh in terms of root mean squared error (RMSE) and peak signal-to-noise ratio (PSNR). It also yielded better or comparable results to DL+Fit, while significantly accelerating the reconstruction process by a factor of approximately seven.Conclusion: Our proposed method outperforms previous methods and provides accurate $T_1$-maps. Although we focused on $T_1$-mapping in the brain, our method's structure is general and can be applied to the reconstruction of other quantitative parameters in other organs.Significance: From a clinical perspective, the obtained $T_1$-maps could be used to differentiate between healthy subjects and patients with Alzheimer's disease. From a technical perspective, the proposed unsupervised method could be used to obtain ground-truth data for the development of data-driven methods based on supervised learning.
</details></li>
</ul>
<hr>
<h2 id="Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising"><a href="#Lighting-Every-Darkness-in-Two-Pairs-A-Calibration-Free-Pipeline-for-RAW-Denoising" class="headerlink" title="Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising"></a>Lighting Every Darkness in Two Pairs: A Calibration-Free Pipeline for RAW Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03448">http://arxiv.org/abs/2308.03448</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/srameo/led">https://github.com/srameo/led</a></li>
<li>paper_authors: Xin Jin, Jia-Wen Xiao, Ling-Hao Han, Chunle Guo, Ruixun Zhang, Xialei Liu, Chongyi Li</li>
<li>For: This paper is written for RAW image denoising under extremely low-light environments, and it aims to overcome the limitations of calibration-based methods.* Methods: The proposed method uses a calibration-free pipeline, which adapts to a target camera with few-shot paired data and fine-tuning. The method also includes well-designed structural modification during both stages to alleviate the domain gap between synthetic and real noise.* Results: The proposed method achieves superior performance over other calibration-based methods with 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations.Here’s the format you requested:* For: 这篇论文是为了对极低光环境下的 RAW 图像推断而写的，并且想要超越传统的测试基于方法。* Methods: 提案的方法使用了没有单位的管道，可以适应目标摄像头只需要几对对照数据和微调。这个方法还包括了妥善的结构修改在两个阶段，以解决实际和 sintetic 噪声之间的领域差。* Results: 提案的方法在对其他传统基于测试方法进行比较时，获得了更好的性能，仅需要2对对照数据和0.5%迭代。<details>
<summary>Abstract</summary>
Calibration-based methods have dominated RAW image denoising under extremely low-light environments. However, these methods suffer from several main deficiencies: 1) the calibration procedure is laborious and time-consuming, 2) denoisers for different cameras are difficult to transfer, and 3) the discrepancy between synthetic noise and real noise is enlarged by high digital gain. To overcome the above shortcomings, we propose a calibration-free pipeline for Lighting Every Drakness (LED), regardless of the digital gain or camera sensor. Instead of calibrating the noise parameters and training repeatedly, our method could adapt to a target camera only with few-shot paired data and fine-tuning. In addition, well-designed structural modification during both stages alleviates the domain gap between synthetic and real noise without any extra computational cost. With 2 pairs for each additional digital gain (in total 6 pairs) and 0.5% iterations, our method achieves superior performance over other calibration-based methods. Our code is available at https://github.com/Srameo/LED .
</details>
<details>
<summary>摘要</summary>
几种基于准确的方法在极低照度环境下进行RAW图像降噪已经占据了主导地位。然而，这些方法受到以下主要缺点的影响：1）准确程度很低，2）适用于不同摄像头的降噪器难以传输，3）高度数字增量使得实际噪声与synthetic噪声之间的差异变大。为了超越这些缺点，我们提出了不需要准确程度的渠道，即Lighting Every Drakness（LED）。相比准确程度的准备和重复训练，我们的方法只需要几次对应的配对数据和微调就能适应目标摄像头。此外，我们在两个阶段中设计了 estructural modification，以alleviate the domain gap between synthetic and real noise without any extra computational cost。通过使用6对每个额外数字增量（共计24对）和0.5%的迭代，我们的方法可以在其他准确基于方法之上达到更高的性能。我们的代码可以在https://github.com/Srameo/LED上找到。
</details></li>
</ul>
<hr>
<h2 id="Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis"><a href="#Energy-Guided-Diffusion-Model-for-CBCT-to-CT-Synthesis" class="headerlink" title="Energy-Guided Diffusion Model for CBCT-to-CT Synthesis"></a>Energy-Guided Diffusion Model for CBCT-to-CT Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03354">http://arxiv.org/abs/2308.03354</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen</li>
<li>for: 提高CBCT图像质量和Hounsfield单位精度，以便更好地用于放射治疗。</li>
<li>methods: 基于能量导向扩散模型（EGDiff），从CBCT图像生成Synthetic CT（sCT）。</li>
<li>results: 对胸腺癌数据集进行实验，得到了优秀的性能结果，包括平均绝对错误26.87±6.14HU、结构相似度指标0.850±0.03、峰值信号噪声比19.83±1.39dB和正常化交叉相关指标0.874±0.04。这些结果表明，我们的方法在精度和视觉质量方面都有所提高，生成了superior的sCT图像。<details>
<summary>Abstract</summary>
Cone Beam CT (CBCT) plays a crucial role in Adaptive Radiation Therapy (ART) by accurately providing radiation treatment when organ anatomy changes occur. However, CBCT images suffer from scatter noise and artifacts, making relying solely on CBCT for precise dose calculation and accurate tissue localization challenging. Therefore, there is a need to improve CBCT image quality and Hounsfield Unit (HU) accuracy while preserving anatomical structures. To enhance the role and application value of CBCT in ART, we propose an energy-guided diffusion model (EGDiff) and conduct experiments on a chest tumor dataset to generate synthetic CT (sCT) from CBCT. The experimental results demonstrate impressive performance with an average absolute error of 26.87$\pm$6.14 HU, a structural similarity index measurement of 0.850$\pm$0.03, a peak signal-to-noise ratio of the sCT of 19.83$\pm$1.39 dB, and a normalized cross-correlation of the sCT of 0.874$\pm$0.04. These results indicate that our method outperforms state-of-the-art unsupervised synthesis methods in accuracy and visual quality, producing superior sCT images.
</details>
<details>
<summary>摘要</summary>
cone beam CT (CBCT) 在适应辐射疗法 (ART) 中发挥重要作用，准确地提供辐射治疗当器官解剖结构发生变化时。然而，CBCT图像受到散射噪声和artefacts的影响，使凭借CBCT alone 的精度计算和精确地本结构定位变得困难。因此，需要改进CBCT图像质量和温顺单元 (HU) 精度，保持器官结构的完整性。为了提高 CBCT 在 ART 中的角色和应用价值，我们提议一种能量指导扩散模型 (EGDiff)，并在胸腔肿瘤数据集上进行实验，将 CBCT 转换成 Synthetic CT (sCT)。实验结果表明，我们的方法在精度和视觉质量方面具有卓越表现，与现有的无监督杂合 Synthesis 方法相比，具有更高的 HU 精度、更高的结构相似度、更高的峰信号噪声比和更高的正规化交叉相似度。这些结果表明，我们的方法可以生成高质量的 sCT 图像，超过现有的方法。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining"><a href="#A-Hybrid-CNN-Transformer-Architecture-with-Frequency-Domain-Contrastive-Learning-for-Image-Deraining" class="headerlink" title="A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining"></a>A Hybrid CNN-Transformer Architecture with Frequency Domain Contrastive Learning for Image Deraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03340">http://arxiv.org/abs/2308.03340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng Wang, Wei Li</li>
<li>for: 图像排除雨纹效果的提升</li>
<li>methods: 使用深度学习方法进行图像排除雨纹</li>
<li>results: 实现了高质量的图像排除雨纹效果Here’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the effectiveness of image deraining by using deep learning methods.</li>
<li>methods: The paper uses deep learning techniques, specifically a deep neural network, to remove rain streaks from degraded images.</li>
<li>results: The paper achieves high-quality image deraining results by using these methods.<details>
<summary>Abstract</summary>
Image deraining is a challenging task that involves restoring degraded images affected by rain streaks.
</details>
<details>
<summary>摘要</summary>
图像抑雨是一项复杂的任务，涉及到修复受到雨束纹的图像。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/07/eess.IV_2023_08_07/" data-id="cllsk9gs600979c88gcopc8zr" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.LG_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.LG_2023_08_06/">cs.LG - 2023-08-06 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System"><a href="#AI-GOMS-Large-AI-Driven-Global-Ocean-Modeling-System" class="headerlink" title="AI-GOMS: Large AI-Driven Global Ocean Modeling System"></a>AI-GOMS: Large AI-Driven Global Ocean Modeling System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03152">http://arxiv.org/abs/2308.03152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Xiong, Yanfei Xiang, Hao Wu, Shuyi Zhou, Yuze Sun, Muyuan Ma, Xiaomeng Huang</li>
<li>for: 这个论文旨在提出一种基于人工智能的全球海洋模型系统（AI-GOMS），用于准确和高效地预测全球海洋日常变化。</li>
<li>methods: 该模型系统包括基本海洋变量预测的贝叶克自适应网络结构，以及包括地方下降、波解码和生物化交互的轻量级精度模型。</li>
<li>results: 该模型在30天预测全球海洋基本变量（15层深度）的方面达到了最佳性能，并能够模拟kuroshio海域的 mezoscale旋涡和赤道太平洋海洋层分化。<details>
<summary>Abstract</summary>
Ocean modeling is a powerful tool for simulating the physical, chemical, and biological processes of the ocean, which is the foundation for marine science research and operational oceanography. Modern numerical ocean modeling mainly consists of governing equations and numerical algorithms. Nonlinear instability, computational expense, low reusability efficiency and high coupling costs have gradually become the main bottlenecks for the further development of numerical ocean modeling. Recently, artificial intelligence-based modeling in scientific computing has shown revolutionary potential for digital twins and scientific simulations, but the bottlenecks of numerical ocean modeling have not been further solved. Here, we present AI-GOMS, a large AI-driven global ocean modeling system, for accurate and efficient global ocean daily prediction. AI-GOMS consists of a backbone model with the Fourier-based Masked Autoencoder structure for basic ocean variable prediction and lightweight fine-tuning models incorporating regional downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has achieved the best performance in 30 days of prediction for the global ocean basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond the good performance in statistical metrics, AI-GOMS realizes the simulation of mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new backbone-downstream paradigm for Earth system modeling, which makes the system transferable, scalable and reusable.
</details>
<details>
<summary>摘要</summary>
海洋模型是一种强大的工具，用于模拟海洋物理、化学和生物过程，是marine science研究和操作 oceanography的基础。现代数值海洋模型主要由管理方程和数值算法组成。不线性不稳定、计算成本高、再利用率低和对接成本高逐渐成为数值海洋模型的主要瓶颈。在科学计算中，人工智能基于的模型已经展示了革命性的潜力，但数值海洋模型中的瓶颈问题还没有得到解决。在这里，我们介绍AI-GOMS，一个大型基于人工智能的全球海洋模型，用于准确和高效的全球海洋日常预测。AI-GOMS包括一个基本 ocean variable prediction的背bone模型，以及 incorporating regional downscaling、波动解码和生物化学结合模块的轻量级精度增强模型。AI-GOMS在30天预测全球海洋基本变量的15层深度分辨率下达到了最佳性能。除了在统计指标方面的好表现，AI-GOMS还实现了kuroshio区域的 mesoscale eddies 在1/12°的空间分辨率下的模拟，以及在 тропиical Pacific Ocean中的海洋层次分布。AI-GOMS提供了一个新的背部-下游模式，使系统可重用、可扩展和可重复使用。
</details></li>
</ul>
<hr>
<h2 id="Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction"><a href="#Nest-DGIL-Nesterov-optimized-Deep-Geometric-Incremental-Learning-for-CS-Image-Reconstruction" class="headerlink" title="Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction"></a>Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03807">http://arxiv.org/abs/2308.03807</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fanxiaohong/Nest-DGIL">https://github.com/fanxiaohong/Nest-DGIL</a></li>
<li>paper_authors: Xiaohong Fan, Yin Yang, Ke Chen, Yujie Feng, Jianping Zhang</li>
<li>for: 这种方法用于解决图像逆问题，包括高&#x2F;低频图像特征的学习能力和保证几何纹理细节的重建。</li>
<li>methods: 基于第二个奈斯特洛夫距离梯度优化的深度幂增量学习框架，包括普通的线性重建、几何增量学习、奈斯特洛夫加速和后处理。</li>
<li>results: 提出的方法可以快速收敛，并且可以避免中间重建结果落入不同几何分解域之外，同时也可以保证高&#x2F;低频图像特征的学习能力和几何纹理细节的重建。<details>
<summary>Abstract</summary>
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed into four modules including general linear reconstruction, cascade geometric incremental restoration, Nesterov acceleration and post-processing. In the image restoration step,a cascade geometric incremental learning module is designed to compensate for the missing texture information from different geometric spectral decomposition domains. Inspired by overlap-tile strategy, we also develop a post-processing module to remove the block-effect in patch-wise-based natural image reconstruction. All parameters in the proposed model are learnable,an adaptive initialization technique of physical-parameters is also employed to make model flexibility and ensure converging smoothly. We compare the reconstruction performance of the proposed method with existing state-of-the-art methods to demonstrate its superiority. Our source codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</details>
<details>
<summary>摘要</summary>
近似梯度基于优化是解决图像反问题的一种最常见策略，易于实现，但它们经常产生重要的artefacts。一种常见的改进方法是调整正则化参数，以降低这些artefacts，但这并不总是可行或适用，因为它可能会增加计算成本。在这种工作中，我们提出了深度几何增量学习框架，基于第二个Nesterov proximal梯度优化。我们的提案的端到端网络不仅具有高/低频图像特征的强大学习能力，而且可以理论保证从初步线性重建中恢复几何纹理细节。此外，它可以避免初步重建结果落入不同几何分解域之外，并且可以快速收敛。我们的重建框架分为四个模块：一般线性重建、几何增量学习、Nesterov加速和后处理。在图像恢复阶段，我们设计了几何增量学习模块，以补做不同几何分解域中缺失的纹理信息。受到 overlap-tile 策略的启发，我们还开发了后处理模块，以去除 patch-wise 基于自然图像重建中的块效果。所有模型参数都是可学习的，并且我们采用了 adaptive 初始化技术，以确保模型的灵活性和平滑的收敛。我们与现有的状态 искусственного智能方法进行比较，以证明我们的方法的优越性。我们的源代码可以在 GitHub 上找到：https://github.com/fanxiaohong/Nest-DGIL。
</details></li>
</ul>
<hr>
<h2 id="Self-Directed-Linear-Classification"><a href="#Self-Directed-Linear-Classification" class="headerlink" title="Self-Directed Linear Classification"></a>Self-Directed Linear Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03142">http://arxiv.org/abs/2308.03142</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis</li>
<li>for: 这个论文研究了在线分类中学习者在适应性下预测示例的顺序，以最小化总错误数。</li>
<li>methods: 论文使用了自适应选择示例顺序的方法，并研究了这种方法的力量。</li>
<li>results: 论文表明，在线分类中，适应性下预测示例的顺序可以实现较好的性能，比如最小化错误数。此外，论文还提供了两个主要结果：在random sampling的情况下，可以使用自适应选择示例顺序来预测整个数据集的 labels，而且这种方法的错误数与数据集的大小无关。<details>
<summary>Abstract</summary>
In online classification, a learner is presented with a sequence of examples and aims to predict their labels in an online fashion so as to minimize the total number of mistakes. In the self-directed variant, the learner knows in advance the pool of examples and can adaptively choose the order in which predictions are made. Here we study the power of choosing the prediction order and establish the first strong separation between worst-order and random-order learning for the fundamental task of linear classification. Prior to our work, such a separation was known only for very restricted concept classes, e.g., one-dimensional thresholds or axis-aligned rectangles.   We present two main results. If $X$ is a dataset of $n$ points drawn uniformly at random from the $d$-dimensional unit sphere, we design an efficient self-directed learner that makes $O(d \log \log(n))$ mistakes and classifies the entire dataset. If $X$ is an arbitrary $d$-dimensional dataset of size $n$, we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$. In contrast, under a worst- or random-ordering, the number of mistakes must be at least $\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details>
<details>
<summary>摘要</summary>
在在线分类中，学习者会看到一串示例，并尝试预测它们的标签，以最小化总错误数量。在自适应变体中，学习者可以适应地选择预测的顺序。我们研究了预测顺序的选择力，并证明了在线分类的基本任务中，自适应学习的最差顺序和随机顺序之间存在首次强分化。在我们的工作前，这种分化只知道在非常限定的概念集合中，例如一维阈值或AXI正方形。我们提出了两个主要结果。如果$X$是一个$d$-维均匀随机分布的点集， then we design an efficient self-directed learner that makes $O(d \log \log n)$ mistakes and classifies the entire dataset.如果$X$是一个任意$d$-维数据集的Size $n$, then we design an efficient self-directed learner that predicts the labels of $99\%$ of the points in $X$ with mistake bound independent of $n$.相比之下，在最差或随机顺序下，错误数量至少为$\Omega(d \log n)$, even when the points are drawn uniformly from the unit sphere and the learner only needs to predict the labels for $1\%$ of them.
</details></li>
</ul>
<hr>
<h2 id="Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis"><a href="#Iterative-Magnitude-Pruning-as-a-Renormalisation-Group-A-Study-in-The-Context-of-The-Lottery-Ticket-Hypothesis" class="headerlink" title="Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis"></a>Iterative Magnitude Pruning as a Renormalisation Group: A Study in The Context of The Lottery Ticket Hypothesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03128">http://arxiv.org/abs/2308.03128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abu-Al Hassan</li>
<li>for: 这个论文探讨了深度神经网络（DNN）的复杂世界，特别关注了赢家票假设（LTH）。</li>
<li>methods: 这个论文使用了迭代幂额减小（IMP）法则，逐渐消除微型 weights，模拟 DNN 的步进学习。</li>
<li>results: 研究发现，winning ticket 可以在各种相似问题上达到类似性能，并且通过与物理学术 Renormalisation Group（RG）理论的联系，提高了 IMP 的理解。<details>
<summary>Abstract</summary>
This thesis delves into the intricate world of Deep Neural Networks (DNNs), focusing on the exciting concept of the Lottery Ticket Hypothesis (LTH). The LTH posits that within extensive DNNs, smaller, trainable subnetworks termed "winning tickets", can achieve performance comparable to the full model. A key process in LTH, Iterative Magnitude Pruning (IMP), incrementally eliminates minimal weights, emulating stepwise learning in DNNs. Once we identify these winning tickets, we further investigate their "universality". In other words, we check if a winning ticket that works well for one specific problem could also work well for other, similar problems. We also bridge the divide between the IMP and the Renormalisation Group (RG) theory in physics, promoting a more rigorous understanding of IMP.
</details>
<details>
<summary>摘要</summary>
这个论文探讨了深度神经网络（DNN）的复杂世界，专注于吸引人的抽签假设（LTH）。LTH认为，在广泛的DNN中，更小的、可训练的子网络“赢家票”可以达到相同的性能。我们在LTH中使用增量大小减少（IMP）来逐渐消除最小的 weights，模拟了DNN中的步进学习。一旦我们确定了这些赢家票，我们进一步调查它们的“通用性”。即我们检查一个赢家票在一个特定问题上表现良好是否可以在其他相似问题上表现良好。我们还将IMP与物理学中的 renormalization group（RG）理论相连接，以便更好地理解IMP。
</details></li>
</ul>
<hr>
<h2 id="Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search"><a href="#Learning-Rate-Free-Learning-Dissecting-D-Adaptation-and-Probabilistic-Line-Search" class="headerlink" title="Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search"></a>Learning-Rate-Free Learning: Dissecting D-Adaptation and Probabilistic Line Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03102">http://arxiv.org/abs/2308.03102</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max McGuinness</li>
<li>for: 这 paper 探讨了两种最近的学习率优化方法在随机梯度下降中：D-Adaptation（arXiv:2301.07733）和 probabilistic line search（arXiv:1502.02846）。这些方法的目的是减轻选择初始学习率的负担，通过包含距离度量和 Gaussian 过程 posterior 估计，respectively。</li>
<li>methods: 这 paper 使用了 D-Adaptation 和 probabilistic line search 两种方法，它们都是为了优化学习率。D-Adaptation 方法使用了距离度量来选择最佳学习率，而 probabilistic line search 方法则使用了 Gaussian 过程 posterior 估计来优化学习率。</li>
<li>results: 这 paper 的结果表明，D-Adaptation 和 probabilistic line search 两种方法都可以减轻选择初始学习率的负担，并且可以提高模型的性能。具体来说，D-Adaptation 方法可以在不同的数据集上实现更好的性能，而 probabilistic line search 方法则可以在不同的学习率下实现更好的性能。<details>
<summary>Abstract</summary>
This paper explores two recent methods for learning rate optimisation in stochastic gradient descent: D-Adaptation (arXiv:2301.07733) and probabilistic line search (arXiv:1502.02846). These approaches aim to alleviate the burden of selecting an initial learning rate by incorporating distance metrics and Gaussian process posterior estimates, respectively. In this report, I provide an intuitive overview of both methods, discuss their shared design goals, and devise scope for merging the two algorithms.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这篇论文探讨了两种最近的学习率优化方法：D-Adaptation（arXiv:2301.07733）和概率线搜索（arXiv:1502.02846）。这两种方法都尝试减轻选择初始学习率的负担，通过 incorporating 距离度量和 Gaussian 过程 posterior 估计，分别。在这份报告中，我提供了这两种方法的直观概述，讨论了它们的共同设计目标，并探讨了将两个算法合并的可能性。
</details></li>
</ul>
<hr>
<h2 id="Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling"><a href="#Gradient-Coding-through-Iterative-Block-Leverage-Score-Sampling" class="headerlink" title="Gradient Coding through Iterative Block Leverage Score Sampling"></a>Gradient Coding through Iterative Block Leverage Score Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03096">http://arxiv.org/abs/2308.03096</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neophytos Charalambides, Mert Pilanci, Alfred Hero</li>
<li>for: 这篇论文是用于实现分布式计算环境中的线性回传运算加速。</li>
<li>methods: 论文使用了扩展的抽样得分组（Leverage Score Sampling），并将其应用于首项方法（Gradient Coding），以减少分布式计算网络中的延迟。</li>
<li>results: 论文获得了一个可以在分布式计算环境中实现线性回传运算的轻量级化码 computing方案，并且可以在当中获得一个具有抽样范围的 $\ell_2$ 子空间嵌入。<details>
<summary>Abstract</summary>
We generalize the leverage score sampling sketch for $\ell_2$-subspace embeddings, to accommodate sampling subsets of the transformed data, so that the sketching approach is appropriate for distributed settings. This is then used to derive an approximate coded computing approach for first-order methods; known as gradient coding, to accelerate linear regression in the presence of failures in distributed computational networks, \textit{i.e.} stragglers. We replicate the data across the distributed network, to attain the approximation guarantees through the induced sampling distribution. The significance and main contribution of this work, is that it unifies randomized numerical linear algebra with approximate coded computing, while attaining an induced $\ell_2$-subspace embedding through uniform sampling. The transition to uniform sampling is done without applying a random projection, as in the case of the subsampled randomized Hadamard transform. Furthermore, by incorporating this technique to coded computing, our scheme is an iterative sketching approach to approximately solving linear regression. We also propose weighting when sketching takes place through sampling with replacement, for further compression.
</details>
<details>
<summary>摘要</summary>
我们总结了权重评分抽样策略，以适应分布式设置，以便在分布式计算网络中使用抽样subset。这种策略可以用来 derivate一种精确的代码计算方法，称为梯度编码，以加速分布式计算中的线性回归，即在分布式计算网络中的慢速进程（即慢进程）。我们将数据复制到分布式网络中，以实现近似 garantess through the induced sampling distribution。这个研究的重要性和主要贡献在于，它将随机化数字线性代数与近似代码计算相结合，并通过均匀抽样实现$\ell_2$次元空间嵌入。在抽样过程中，我们不会应用随机投影，如Randomized Hadamard Transform中的subsampled抽样。此外，我们还提出了在抽样过程中使用权重，以进一步压缩数据。因此，我们的方案是一种迭代抽样策略，用于约等于解 linear regression。我们的方法可以在分布式计算中实现高效的线性回归解决方案，并且可以扩展到更复杂的机器学习模型。
</details></li>
</ul>
<hr>
<h2 id="Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events"><a href="#Control-aware-echo-state-networks-Ca-ESN-for-the-suppression-of-extreme-events" class="headerlink" title="Control-aware echo state networks (Ca-ESN) for the suppression of extreme events"></a>Control-aware echo state networks (Ca-ESN) for the suppression of extreme events</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03095">http://arxiv.org/abs/2308.03095</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Racca, Luca Magri</li>
<li>for: 防止非线性系统中的极端事件的发生</li>
<li>methods: 结合ESN和控制策略，如比例- интеграル-导数控制和模型预测控制，实现非线性系统的有效控制</li>
<li>results: 在混沌液体流中，使用Ca-ESN比传统方法减少极端事件的发生，提高控制效果，开启了非线性系统控制的新可能性<details>
<summary>Abstract</summary>
Extreme event are sudden large-amplitude changes in the state or observables of chaotic nonlinear systems, which characterize many scientific phenomena. Because of their violent nature, extreme events typically have adverse consequences, which call for methods to prevent the events from happening. In this work, we introduce the control-aware echo state network (Ca-ESN) to seamlessly combine ESNs and control strategies, such as proportional-integral-derivative and model predictive control, to suppress extreme events. The methodology is showcased on a chaotic-turbulent flow, in which we reduce the occurrence of extreme events with respect to traditional methods by two orders of magnitude. This works opens up new possibilities for the efficient control of nonlinear systems with neural networks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>极端事件是非线性系统中突然大幅度变化的状态或观测量，这些事件frequently occur in many scientific phenomena. 由于其暴力性，极端事件通常会带来不良后果，需要采取方法来预防这些事件的发生。在这种工作中，我们提出了控制意识 echo state network (Ca-ESN)，可以将ESNs和控制策略，如 proporциональ-integral-derivative 和模型预测控制，结合在一起，以降低极端事件的发生频率。我们在混沌-turbulent flow中应用了这种方法，并比传统方法降低了极端事件的发生频率二个数量级。这项工作开 up new possibilities for the efficient control of nonlinear systems with neural networks.Note: "极端事件" in Chinese is usually translated as "extreme events" or "outliers", but in the context of this text, it refers to sudden large-amplitude changes in the state or observables of chaotic nonlinear systems.
</details></li>
</ul>
<hr>
<h2 id="Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data"><a href="#Visualization-of-Extremely-Sparse-Contingency-Table-by-Taxicab-Correspondence-Analysis-A-Case-Study-of-Textual-Data" class="headerlink" title="Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data"></a>Visualization of Extremely Sparse Contingency Table by Taxicab Correspondence Analysis: A Case Study of Textual Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03079">http://arxiv.org/abs/2308.03079</a></li>
<li>repo_url: None</li>
<li>paper_authors: V. Choulakian, J. Allard</li>
<li>for: Visualization of extremely sparse ontingency tables</li>
<li>methods: Taxicab correspondence analysis, a robust variant of correspondence analysis</li>
<li>results: Visualization of an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books<details>
<summary>Abstract</summary>
We present an overview of taxicab correspondence analysis, a robust variant of correspondence analysis, for visualization of extremely sparse ontingency tables. In particular we visualize an extremely sparse textual data set of size 590 by 8265 concerning fragments of 8 sacred books recently introduced by Sah and Fokou\'e (2019) and studied quite in detail by (12 + 1) dimension reduction methods (t-SNE, UMAP, PHATE,...) by Ma, Sun and Zou (2022).
</details>
<details>
<summary>摘要</summary>
我们提供了taxicab对应分析的概述，这是对对应分析的一种稳定版本，用于可见化极稀疏的对应关系表。特别是我们使用了590行x8265列的极稀疏文本数据集，这些数据来自 sah和fokou（2019）所引入的8种圣书的片断，并且通过(12+1)维度减少方法（t-SNE、UMAP、PHATE等）进行了深入研究。这些研究由Ma、sun和Zou（2022）进行了。
</details></li>
</ul>
<hr>
<h2 id="Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer"><a href="#Study-for-Performance-of-MobileNetV1-and-MobileNetV2-Based-on-Breast-Cancer" class="headerlink" title="Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer"></a>Study for Performance of MobileNetV1 and MobileNetV2 Based on Breast Cancer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03076">http://arxiv.org/abs/2308.03076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiuqi Yan</li>
<li>for: 这项实验的目的是比较 MobileNetV1 和 MobileNetV2 模型在分类乳腺癌图像方面的性能。</li>
<li>methods: 实验使用了 Kaggle 上下载的数据集，并对其进行Normalization。然后，使用了神经网络模型来学习数据集，找出图像的特征并判断乳腺癌。</li>
<li>results: 实验结果表明，在处理这个数据集时，MobileNetV1 模型表现较好， validation accuracy 和 overfit 也较高。<details>
<summary>Abstract</summary>
Artificial intelligence is constantly evolving and can provide effective help in all aspects of people's lives. The experiment is mainly to study the use of artificial intelligence in the field of medicine. The purpose of this experiment was to compare which of MobileNetV1 and MobileNetV2 models was better at detecting histopathological images of the breast downloaded at Kaggle. When the doctor looks at the pathological image, there may be errors that lead to errors in judgment, and the observation speed is slow. Rational use of artificial intelligence can effectively reduce the error of doctor diagnosis in breast cancer judgment and speed up doctor diagnosis. The dataset was downloaded from Kaggle and then normalized. The basic principle of the experiment is to let the neural network model learn the downloaded data set. Then find the pattern and be able to judge on your own whether breast tissue is cancer. In the dataset, benign tumor pictures and malignant tumor pictures have been classified, of which 198738 are benign tumor pictures and 78, 786 are malignant tumor pictures. After calling MobileNetV1 and MobileNetV2, the dataset is trained separately, the training accuracy and validation accuracy rate are obtained, and the image is drawn. It can be observed that MobileNetV1 has better validation accuracy and overfit during MobileNetV2 training. From the experimental results, it can be seen that in the case of processing this dataset, MobileNetV1 is much better than MobileNetV2.
</details>
<details>
<summary>摘要</summary>
人工智能不断发展，可以提供有效的帮助在人们的生活中。本实验的主要目的是研究人工智能在医学领域的应用。本实验的目的是比较MobileNetV1和MobileNetV2模型在Kaggle上下载的乳腺病理图像上的表现。医生查看病理图像时可能会出现错误，导致诊断错误， observation速度较慢。合理使用人工智能可以有效减少医生诊断乳腺癌判断中的错误，并加快医生诊断速度。数据集来自Kaggle，然后 норциали化。实验的基本原则是让神经网络模型学习下载的数据集。然后找出模式，并能够自己判断乳腺细胞是否为癌细胞。数据集中，恶性肿瘤图像和良性肿瘤图像已经分类，其中198738个是恶性肿瘤图像，78786个是良性肿瘤图像。在 MobileNetV1 和 MobileNetV2 之后，数据集被分别训练，并获得训练精度和验证精度率。图像也被画出来。可以看到，在处理这个数据集时，MobileNetV1 表现得更好。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models"><a href="#Comparative-Analysis-of-Epileptic-Seizure-Prediction-Exploring-Diverse-Pre-Processing-Techniques-and-Machine-Learning-Models" class="headerlink" title="Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models"></a>Comparative Analysis of Epileptic Seizure Prediction: Exploring Diverse Pre-Processing Techniques and Machine Learning Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05176">http://arxiv.org/abs/2308.05176</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md. Simul Hasan Talukder, Rejwan Bin Sulaiman</li>
<li>for: 预测癫痫发作</li>
<li>methods: 使用五种机器学习模型（Random Forest、Decision Tree、Extra Trees、Logistic Regression、Gradient Boosting）对电enzephalogram数据进行预测</li>
<li>results: 结果显示，LR类ifier的准确率为56.95%，GB和DT都达到97.17%的准确率，RT达到98.99%的准确率，ET模型表现最佳，准确率达99.29%。<details>
<summary>Abstract</summary>
Epilepsy is a prevalent neurological disorder characterized by recurrent and unpredictable seizures, necessitating accurate prediction for effective management and patient care. Application of machine learning (ML) on electroencephalogram (EEG) recordings, along with its ability to provide valuable insights into brain activity during seizures, is able to make accurate and robust seizure prediction an indispensable component in relevant studies. In this research, we present a comprehensive comparative analysis of five machine learning models - Random Forest (RF), Decision Tree (DT), Extra Trees (ET), Logistic Regression (LR), and Gradient Boosting (GB) - for the prediction of epileptic seizures using EEG data. The dataset underwent meticulous preprocessing, including cleaning, normalization, outlier handling, and oversampling, ensuring data quality and facilitating accurate model training. These preprocessing techniques played a crucial role in enhancing the models' performance. The results of our analysis demonstrate the performance of each model in terms of accuracy. The LR classifier achieved an accuracy of 56.95%, while GB and DT both attained 97.17% accuracy. RT achieved a higher accuracy of 98.99%, while the ET model exhibited the best performance with an accuracy of 99.29%. Our findings reveal that the ET model outperformed not only the other models in the comparative analysis but also surpassed the state-of-the-art results from previous research. The superior performance of the ET model makes it a compelling choice for accurate and robust epileptic seizure prediction using EEG data.
</details>
<details>
<summary>摘要</summary>
《诊断和治疗精神疾病》中，有一种常见的神经疾病是癫痫症，它表现为不规则和难以预测的癫痫发作，因此需要准确的预测以提供有效的管理和患者护理。在这些研究中，我们使用机器学习（ML）技术对电энцефалограм（EEG）记录进行分析，并通过对大脑活动的描述来提供有价值的预测。本研究中，我们对五种机器学习模型进行了比较分析：Random Forest（RF）、Decision Tree（DT）、Extra Trees（ET）、Logistic Regression（LR）和Gradient Boosting（GB）。我们对EEG数据进行了仔细的处理，包括清洁、 нормализа、异常处理和扩充，以确保数据质量的高度。这些处理技术对模型的表现产生了重要的影响。我们的分析结果显示每个模型的准确率。LR分类器的准确率为56.95%，而GB和DT都达到了97.17%的准确率。RT达到了98.99%的准确率，而ET模型表现出了最佳的性能，准确率达99.29%。我们的发现表明ET模型不仅在 Comparative analysis中表现出色，还超越了过去研究中的状态归化结果。ET模型的优秀表现使其成为精确和可靠的癫痫发作预测的首选方法。
</details></li>
</ul>
<hr>
<h2 id="TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties"><a href="#TARJAMAT-Evaluation-of-Bard-and-ChatGPT-on-Machine-Translation-of-Ten-Arabic-Varieties" class="headerlink" title="TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties"></a>TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03051">http://arxiv.org/abs/2308.03051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karima Kadaoui, Samar M. Magdy, Abdul Waheed, Md Tawkat Islam Khondaker, Ahmed Oumar El-Shangiti, El Moatez Billah Nagoudi, Muhammad Abdul-Mageed</li>
<li>for: 这篇论文主要是为了评估大型自然语言模型（LLMs）在不同的阿拉伯语种中的翻译能力。</li>
<li>methods: 这篇论文使用了Google Bard和OpenAI ChatGPT两个模型，并对这两个模型在十种阿拉伯语种中的翻译能力进行了全面的评估。</li>
<li>results: 研究发现，LLMs在一些阿拉伯语种中表现不佳，特别是对于没有充足公共数据的语种，如阿尔及利亚和毛里塔尼亚的方言。然而，它们在更常见的方言中表现较为满意，尽管有时会落后于现有的商业系统 like Google Translate。此外，研究还发现，Bard在翻译任务中遵循人类指示的能力有限。<details>
<summary>Abstract</summary>
Large language models (LLMs) finetuned to follow human instructions have recently emerged as a breakthrough in AI. Models such as Google Bard and OpenAI ChatGPT, for example, are surprisingly powerful tools for question answering, code debugging, and dialogue generation. Despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），如Google Bard和OpenAI ChatGPT，在最近几年内 emerge as a breakthrough in AI。这些模型具有强大的问答、代码调试和对话生成能力。 despite the purported multilingual proficiency of these models, their linguistic inclusivity remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (including both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic, Modern Standard Arabic, and several nuanced dialectal variants. Furthermore, we undertake a human-centric study to scrutinize the efficacy of the most recent model, Bard, in following human instructions during translation tasks. Our exhaustive analysis indicates that LLMs may encounter challenges with certain Arabic dialects, particularly those for which minimal public data exists, such as Algerian and Mauritanian dialects. However, they exhibit satisfactory performance with more prevalent dialects, albeit occasionally trailing behind established commercial systems like Google Translate. Additionally, our analysis reveals a circumscribed capability of Bard in aligning with human instructions in translation contexts. Collectively, our findings underscore that prevailing LLMs remain far from inclusive, with only limited ability to cater for the linguistic and cultural intricacies of diverse communities.
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables"><a href="#Weakly-Supervised-Multi-Task-Representation-Learning-for-Human-Activity-Analysis-Using-Wearables" class="headerlink" title="Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables"></a>Weakly Supervised Multi-Task Representation Learning for Human Activity Analysis Using Wearables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03805">http://arxiv.org/abs/2308.03805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taoran Sheng, Manfred Huber</li>
<li>for: 多元特征掌握和多任务同时处理</li>
<li>methods: 弱监睹多输出SIAMESE网络</li>
<li>results: 可以同时解决多个任务，并且在许多情况下超越单任务监睹方法表现。<details>
<summary>Abstract</summary>
Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available.
</details>
<details>
<summary>摘要</summary>
仪器数据流FROM wearable devices和智能环境广泛研究在人体活动识别（HAR）、人员身份识别或健康监测等领域。然而，大多数前一些工作在活动和仪器流数据分析中都是关注一个方面的数据，例如只是识别活动的类型或者只是识别活动的执行者。我们提出了一种方法，使用弱监督多输出siamesenet来映射数据到多个表示空间中，其中每个表示空间都关注一个数据的方面。映射 vectors的数据样本被置于空间中，使得数据具有同一 Semantic meaning在该方面的数据集中均被 closely located。因此，通过一些实验，我们的模型可以为数据提供多个任务的指标，使其可以同时解决多个任务，甚至在许多情况下超越单任务监督方法。此外，我们还进行了更多的实验，分析了这种架构的影响和多个任务的使用情况，以及模型的扩展性。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys"><a href="#Machine-learning-methods-for-the-search-for-L-T-brown-dwarfs-in-the-data-of-modern-sky-surveys" class="headerlink" title="Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys"></a>Machine learning methods for the search for L&amp;T brown dwarfs in the data of modern sky surveys</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03045">http://arxiv.org/abs/2308.03045</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamaleksandra/ml-brown-dwarfs">https://github.com/iamaleksandra/ml-brown-dwarfs</a></li>
<li>paper_authors: Aleksandra Avdeeva</li>
<li>For: 这个论文的目的是创建一个高度可靠的褐矮星样本，以确定褐矮星的特征和分布。* Methods: 这篇论文使用机器学习算法，如Random Forest Classifier、XGBoost、SVM Classifier和TabNet，对PanStarrs DR1、2MASS和WISE数据进行分类，以 distinguishing L和T褐矮星从其他 spectral和照度类型的 объек。* Results: 研究人员使用机器学习算法，成功地分类出L和T褐矮星，并与传统的决策规则进行比较，证明了其效率和相关性。<details>
<summary>Abstract</summary>
According to various estimates, brown dwarfs (BD) should account for up to 25 percent of all objects in the Galaxy. However, few of them are discovered and well-studied, both individually and as a population. Homogeneous and complete samples of brown dwarfs are needed for these kinds of studies. Due to their weakness, spectral studies of brown dwarfs are rather laborious. For this reason, creating a significant reliable sample of brown dwarfs, confirmed by spectroscopic observations, seems unattainable at the moment. Numerous attempts have been made to search for and create a set of brown dwarfs using their colours as a decision rule applied to a vast amount of survey data. In this work, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier and TabNet on PanStarrs DR1, 2MASS and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. The explanation of the models is discussed. We also compare our models with classical decision rules, proving their efficiency and relevance.
</details>
<details>
<summary>摘要</summary>
To overcome this challenge, numerous attempts have been made to search for and create a set of brown dwarfs using their colors as a decision rule applied to a vast amount of survey data. In this study, we use machine learning methods such as Random Forest Classifier, XGBoost, SVM Classifier, and TabNet on PanStarrs DR1, 2MASS, and WISE data to distinguish L and T brown dwarfs from objects of other spectral and luminosity classes. We discuss the explanation of the models and compare them with classical decision rules, demonstrating their efficiency and relevance.
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey"><a href="#Machine-Learning-for-Infectious-Disease-Risk-Prediction-A-Survey" class="headerlink" title="Machine Learning for Infectious Disease Risk Prediction: A Survey"></a>Machine Learning for Infectious Disease Risk Prediction: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03037">http://arxiv.org/abs/2308.03037</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mutong Liu, Yang Liu, Jiming Liu</li>
<li>for: 这篇论文的目的是探讨机器学习如何在抑制传染病中发挥作用，以便更有效地预测感染病风险。</li>
<li>methods: 论文中使用的方法包括：介绍背景和动机，介绍不同类型的机器学习模型，讨论模型输入、设计目标和评估性能等挑战，并结尾提出未解决的问题和未来方向。</li>
<li>results: 论文的结果表明，机器学习可以帮助量化疾病传染模式，并准确预测感染病风险。<details>
<summary>Abstract</summary>
Infectious diseases, either emerging or long-lasting, place numerous people at risk and bring heavy public health burdens worldwide. In the process against infectious diseases, predicting the epidemic risk by modeling the disease transmission plays an essential role in assisting with preventing and controlling disease transmission in a more effective way. In this paper, we systematically describe how machine learning can play an essential role in quantitatively characterizing disease transmission patterns and accurately predicting infectious disease risks. First, we introduce the background and motivation of using machine learning for infectious disease risk prediction. Next, we describe the development and components of various machine learning models for infectious disease risk prediction. Specifically, existing models fall into three categories: Statistical prediction, data-driven machine learning, and epidemiology-inspired machine learning. Subsequently, we discuss challenges encountered when dealing with model inputs, designing task-oriented objectives, and conducting performance evaluation. Finally, we conclude with a discussion of open questions and future directions.
</details>
<details>
<summary>摘要</summary>
免疫疾病，无论是新兴的或长期存在的，都会对全球公共卫生带来巨大的压力。在抗击免疫疾病的过程中，预测疾病传播风险的模型化协助了更有效地预防和控制疾病传播。在这篇论文中，我们系统地描述了机器学习如何在免疫疾病风险预测中发挥重要作用。首先，我们介绍了使用机器学习预测免疫疾病风险的背景和动机。然后，我们描述了不同类型的机器学习模型的开发和组成部分。具体来说，现有的模型可以分为三类：统计预测、数据驱动机器学习和医学机器学习。接着，我们讨论了与模型输入、设计任务目标以及性能评价过程中遇到的挑战。最后，我们 conclude with 未来方向的开放问题。
</details></li>
</ul>
<hr>
<h2 id="Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining"><a href="#Serverless-Federated-AUPRC-Optimization-for-Multi-Party-Collaborative-Imbalanced-Data-Mining" class="headerlink" title="Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining"></a>Serverless Federated AUPRC Optimization for Multi-Party Collaborative Imbalanced Data Mining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03035">http://arxiv.org/abs/2308.03035</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xidongwu/d-auprc">https://github.com/xidongwu/d-auprc</a></li>
<li>paper_authors: Xidong Wu, Zhengmian Hu, Jian Pei, Heng Huang</li>
<li>for: 本文targets the problem of multi-party collaborative training for imbalanced data tasks, and proposes a new algorithm called ServerLess biAsed sTochastic gradiEnt (SLATE) to directly optimize the Area Under Precision-Recall Curve (AUPRC).</li>
<li>methods: 本文使用了服务器less多方合作学习 Setting，并将问题转化为conditional stochastic optimization problem。furthermore, the authors propose a new algorithm called ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) to improve the convergence rate.</li>
<li>results: 本文的实验结果表明，SLATE-M算法可以减少communication cost，并且与最佳单机Online方法匹配。这是首次解决多方合作AUPRC最大化问题。<details>
<summary>Abstract</summary>
Multi-party collaborative training, such as distributed learning and federated learning, is used to address the big data challenges. However, traditional multi-party collaborative training algorithms were mainly designed for balanced data mining tasks and are intended to optimize accuracy (\emph{e.g.}, cross-entropy). The data distribution in many real-world applications is skewed and classifiers, which are trained to improve accuracy, perform poorly when applied to imbalanced data tasks since models could be significantly biased toward the primary class. Therefore, the Area Under Precision-Recall Curve (AUPRC) was introduced as an effective metric. Although single-machine AUPRC maximization methods have been designed, multi-party collaborative algorithm has never been studied. The change from the single-machine to the multi-party setting poses critical challenges.   To address the above challenge, we study the serverless multi-party collaborative AUPRC maximization problem since serverless multi-party collaborative training can cut down the communications cost by avoiding the server node bottleneck, and reformulate it as a conditional stochastic optimization problem in a serverless multi-party collaborative learning setting and propose a new ServerLess biAsed sTochastic gradiEnt (SLATE) algorithm to directly optimize the AUPRC. After that, we use the variance reduction technique and propose ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction (SLATE-M) algorithm to improve the convergence rate, which matches the best theoretical convergence result reached by the single-machine online method. To the best of our knowledge, this is the first work to solve the multi-party collaborative AUPRC maximization problem.
</details>
<details>
<summary>摘要</summary>
多方合作训练，如分布式学习和联邦学习，用于解决大数据问题。然而，传统的多方合作训练算法主要设计用于均衡数据挖掘任务，并且optimize准确率（例如，交叉熵）。在许多实际应用中，数据分布不均，类 clasifier在不均衡数据任务中表现糟糕，因为模型可能受主要类别的偏见。因此，Area Under Precision-Recall Curve（AUPRC）被引入作为有效指标。虽然单机AUPRC最大化方法已经设计过，但多方合作算法尚未研究。在单机到多方 Setting中的变化 pose critical challenges。为了解决以上挑战，我们研究了无服务器多方合作AUPRC最大化问题，因为无服务器多方合作训练可以降低通信成本，并将问题重新定义为conditional stochastic optimization问题在无服务器多方合作学习Setting中。然后，我们提出了一种新的ServerLess biAsed sTochastic gradiEnt（SLATE）算法，以直接优化AUPRC。接着，我们使用了差分reduction技术，并提出了ServerLess biAsed sTochastic gradiEnt with Momentum-based variance reduction（SLATE-M）算法，以提高收敛率，与单机在线方法的最佳理论收敛率匹配。到目前为止，这是首次解决多方合作AUPRC最大化问题的研究。
</details></li>
</ul>
<hr>
<h2 id="Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis"><a href="#Causal-Disentanglement-Hidden-Markov-Model-for-Fault-Diagnosis" class="headerlink" title="Causal Disentanglement Hidden Markov Model for Fault Diagnosis"></a>Causal Disentanglement Hidden Markov Model for Fault Diagnosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03027">http://arxiv.org/abs/2308.03027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rihao Chang, Yongtao Ma, Weizhi Nie, Jie Nie, An-an Liu</li>
<li>for: 本研究旨在提出一种基于 causal disentanglement hidden markov model (CDHM) 的磨损诊断方法，以便更好地捕捉磨损特征并实现预测磨损类型。</li>
<li>methods: 本方法首先使用时序数据进行磨损特征的捕捉，然后逐渐分离磨损信号中相关和无关的因素。 其中，我们使用 ELBO 来优化学习 causal disentanglement markov model。此外，我们还采用无监督领域适应，将学习的分离表示转移到其他工作环境中。</li>
<li>results: 实验结果表明，提出的方法在 CWRU 数据集和 IMS 数据集上具有优秀的效果，可以准确地预测磨损类型。<details>
<summary>Abstract</summary>
In modern industries, fault diagnosis has been widely applied with the goal of realizing predictive maintenance. The key issue for the fault diagnosis system is to extract representative characteristics of the fault signal and then accurately predict the fault type. In this paper, we propose a Causal Disentanglement Hidden Markov model (CDHM) to learn the causality in the bearing fault mechanism and thus, capture their characteristics to achieve a more robust representation. Specifically, we make full use of the time-series data and progressively disentangle the vibration signal into fault-relevant and fault-irrelevant factors. The ELBO is reformulated to optimize the learning of the causal disentanglement Markov model. Moreover, to expand the scope of the application, we adopt unsupervised domain adaptation to transfer the learned disentangled representations to other working environments. Experiments were conducted on the CWRU dataset and IMS dataset. Relevant results validate the superiority of the proposed method.
</details>
<details>
<summary>摘要</summary>
现代产业中，故障诊断已广泛应用，目的是实现预测维护。故障诊断系统的关键问题是提取表征性的故障信号，并准确预测故障类型。在本文中，我们提议一种因果分解隐藏马尔可夫模型（CDHM），以学习滤波器故障机制中的因果关系，并 capture其特征来实现更加稳定的表征。具体来说，我们利用时间序列数据，逐步分解振荡信号，分解出相关和无关故障因素。我们 reformulate ELBO，以便学习因果分解马尔可夫模型。此外，为扩展应用范围，我们采用无监督领域适应，将学习的分解表征转移到其他工作环境。在CWRU数据集和IMS数据集上进行了实验，实验结果证明了我们提出的方法的优越性。
</details></li>
</ul>
<hr>
<h2 id="Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis"><a href="#Early-Detection-and-Localization-of-Pancreatic-Cancer-by-Label-Free-Tumor-Synthesis" class="headerlink" title="Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis"></a>Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03008">http://arxiv.org/abs/2308.03008</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrgiovanni/synthetictumors">https://github.com/mrgiovanni/synthetictumors</a></li>
<li>paper_authors: Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, Zongwei Zhou<br>for: 这篇论文的目的是提高胰脏癌早期检测和定位，以增加病人5年生存率从8.5%提高到20%。methods: 本研究使用人工智能（AI）模型，将健康胰脏中的小型胰脏肿瘤合成成许多标注的例子，以帮助医生早期检测胰脏癌。results: 我们的实验结果显示，使用合成的胰脏肿瘤训练AI模型，胰脏癌检测率与真实胰脏癌检测率相似，且能够更好地检测小型胰脏肿瘤。此外，我们还证明了使用合成胰脏肿瘤和真实胰脏癌检测结果进行混合训练，可以提高AI模型的普遍性和检测精度。<details>
<summary>Abstract</summary>
Early detection and localization of pancreatic cancer can increase the 5-year survival rate for patients from 8.5% to 20%. Artificial intelligence (AI) can potentially assist radiologists in detecting pancreatic tumors at an early stage. Training AI models require a vast number of annotated examples, but the availability of CT scans obtaining early-stage tumors is constrained. This is because early-stage tumors may not cause any symptoms, which can delay detection, and the tumors are relatively small and may be almost invisible to human eyes on CT scans. To address this issue, we develop a tumor synthesis method that can synthesize enormous examples of small pancreatic tumors in the healthy pancreas without the need for manual annotation. Our experiments demonstrate that the overall detection rate of pancreatic tumors, measured by Sensitivity and Specificity, achieved by AI trained on synthetic tumors is comparable to that of real tumors. More importantly, our method shows a much higher detection rate for small tumors. We further investigate the per-voxel segmentation performance of pancreatic tumors if AI is trained on a combination of CT scans with synthetic tumors and CT scans with annotated large tumors at an advanced stage. Finally, we show that synthetic tumors improve AI generalizability in tumor detection and localization when processing CT scans from different hospitals. Overall, our proposed tumor synthesis method has immense potential to improve the early detection of pancreatic cancer, leading to better patient outcomes.
</details>
<details>
<summary>摘要</summary>
早期检测和肿瘤localization可以提高患者5年生存率从8.5%提高到20%。人工智能（AI）可能能够帮助放射学家在早期发现肿瘤。然而，训练AI模型需要庞大的标注示例，但获得早期肿瘤的CT扫描数据受限。这是因为早期肿瘤可能不会导致任何症状，这可能会延迟检测，并且肿瘤相对较小，可能对人类目视难以看到在CT扫描中。为解决这个问题，我们开发了一种肿瘤合成方法，可以在健康的胰脏中合成巨大的小肿瘤示例，无需人工标注。我们的实验表明，由AI训练在合成肿瘤上的检测率（敏感性和特异性）与真实肿瘤相比较高，并且检测到小肿瘤的率更高。我们进一步调查了使用合成肿瘤和注解大肿瘤的CT扫描结合训练AI的效果，发现这种方法可以提高肿瘤检测和地图localization的普适性。最后，我们证明了合成肿瘤可以提高AI在不同医院CT扫描处理时的普适性。总之，我们提出的肿瘤合成方法有巨大的潜力，可以提高肿瘤检测的早期，导致更好的病例结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Polar-Codes"><a href="#Deep-Polar-Codes" class="headerlink" title="Deep Polar Codes"></a>Deep Polar Codes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03004">http://arxiv.org/abs/2308.03004</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/HzFu/MNet_DeepCDR">https://github.com/HzFu/MNet_DeepCDR</a></li>
<li>paper_authors: Geon Choi, Namyoon Lee</li>
<li>for: 这个论文旨在提出一种新的预变换极码，称为深度极码。</li>
<li>methods: 论文提出了一种深度极编码器，利用多层极化转换来实现低复杂度实现，同时能够改善极码的重量分布。此外，该编码器支持范围广的代码率和块长。</li>
<li>results: 通过 simulations，论文表明深度极码在不同代码率下的块错误率比现有的预变换极码更低，同时保持低的编码和解码复杂度。此外，论文还表明，将深度极码与循环检验码 concatenate 可以达到 finite block length 容量的 meta-converse bound 的下界 within 0.4 dB 以下。<details>
<summary>Abstract</summary>
In this paper, we introduce a novel class of pre-transformed polar codes, termed as deep polar codes. We first present a deep polar encoder that harnesses a series of multi-layered polar transformations with varying sizes. Our approach to encoding enables a low-complexity implementation while significantly enhancing the weight distribution of the code. Moreover, our encoding method offers flexibility in rate-profiling, embracing a wide range of code rates and blocklengths. Next, we put forth a low-complexity decoding algorithm called successive cancellation list with backpropagation parity checks (SCL-BPC). This decoding algorithm leverages the parity check equations in the reverse process of the multi-layered pre-transformed encoding for SCL decoding. Additionally, we present a low-latency decoding algorithm that employs parallel-SCL decoding by treating partially pre-transformed bit patterns as additional frozen bits. Through simulations, we demonstrate that deep polar codes outperform existing pre-transformed polar codes in terms of block error rates across various code rates under short block lengths, while maintaining low encoding and decoding complexity. Furthermore, we show that concatenating deep polar codes with cyclic-redundancy-check codes can achieve the meta-converse bound of the finite block length capacity within 0.4 dB in some instances.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种新的预转换极码，称为深度极码。我们首先提出了一种深度极编码器，利用多层极化转换来实现低复杂性实现，同时提高码的质量分布。此外，我们的编码方法支持范围广的码率和块长度。接着，我们提出了一种低复杂度解码算法，称为顺序取消列表带回传播可能性检查（SCL-BPC）。这种解码算法利用了反向多层预转换的parity check方程来实现SCL解码。此外，我们还提出了一种低延迟解码算法，通过并行SCL解码来处理部分预转换的bit pattern。通过实验，我们证明了深度极码在不同的码率下的块错误率都较低，同时保持了低编码和解码复杂度。此外，我们还显示了 concatenating 深度极码可以实现finite block length容量的meta-converse bound在0.4 dB之间。
</details></li>
</ul>
<hr>
<h2 id="Spanish-Pre-trained-BERT-Model-and-Evaluation-Data"><a href="#Spanish-Pre-trained-BERT-Model-and-Evaluation-Data" class="headerlink" title="Spanish Pre-trained BERT Model and Evaluation Data"></a>Spanish Pre-trained BERT Model and Evaluation Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02976">http://arxiv.org/abs/2308.02976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dccuchile/beto">https://github.com/dccuchile/beto</a></li>
<li>paper_authors: José Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, Jorge Pérez</li>
<li>for: 增强西班牙语模型的训练和评估资源。</li>
<li>methods: BERT基于模型预训练 exclusively on Spanish data。</li>
<li>results: 比其他基于BERT的模型在大多数任务上获得更好的结果，并在一些任务上创造新的状态。<details>
<summary>Abstract</summary>
The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model, we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data, and the compilation of the Spanish benchmarks.
</details>
<details>
<summary>摘要</summary>
“西班牙语是全球前五大最受欢迎的语言之一，然而找到用于训练或评估西班牙语模型的资源并不是一个容易的任务。在这篇论文中，我们帮助填补这个差距，提出了基于BERT的西班牙语模型，并将其推广到多个任务中。作为我们的第二次贡献，我们还将西班牙语的多个任务集成了一个单一的存储库，类似于GLUE套件。通过精益地调整我们的预训西班牙语模型，我们在大多数任务上取得了更好的结果，甚至创下了一些新的州态。我们已经公开了我们的模型、预训数据和西班牙语套件。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory"><a href="#Generalized-Oversampling-for-Learning-from-Imbalanced-datasets-and-Associated-Theory" class="headerlink" title="Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory"></a>Generalized Oversampling for Learning from Imbalanced datasets and Associated Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02966">http://arxiv.org/abs/2308.02966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Samuel Stocksieker, Denys Pommeret, Arthur Charpentier</li>
<li>for: 本研究旨在解决超参异常学习中的数据不均衡问题，具体来说是用于异常 regression 问题。</li>
<li>methods: 本文提出了一种基于kernel density estimate的数据扩充方法，称为GOLIATH算法，该方法可以应用于分类和回归问题。它包括两大类别的人工扩充：基于扰动的，如加aussian noise，和基于 interpolate的，如 SMOTE。此外，该方法还提供了这些机器学习算法的Explicit表达式和准确性梯度的表达式，特别是对SMOTE算法的表达式。</li>
<li>results: 本文通过应用GOLIATH算法在异常 regression 中进行了实验评估，并与现有状态方法进行了比较。结果表明，GOLIATH算法在异常 regression 中具有显著的改善效果。<details>
<summary>Abstract</summary>
In supervised learning, it is quite frequent to be confronted with real imbalanced datasets. This situation leads to a learning difficulty for standard algorithms. Research and solutions in imbalanced learning have mainly focused on classification tasks. Despite its importance, very few solutions exist for imbalanced regression. In this paper, we propose a data augmentation procedure, the GOLIATH algorithm, based on kernel density estimates which can be used in classification and regression. This general approach encompasses two large families of synthetic oversampling: those based on perturbations, such as Gaussian Noise, and those based on interpolations, such as SMOTE. It also provides an explicit form of these machine learning algorithms and an expression of their conditional densities, in particular for SMOTE. New synthetic data generators are deduced. We apply GOLIATH in imbalanced regression combining such generator procedures with a wild-bootstrap resampling technique for the target values. We evaluate the performance of the GOLIATH algorithm in imbalanced regression situations. We empirically evaluate and compare our approach and demonstrate significant improvement over existing state-of-the-art techniques.
</details>
<details>
<summary>摘要</summary>
在监督学习中，很普遍遇到实际数据集的不均衡问题。这种情况会导致标准算法学习困难。研究和解决不均衡学习问题的研究主要集中在分类任务上。虽然其重要性很大，但是现有的解决方案很少。在这篇论文中，我们提出了一种数据扩充方法，名为GOLIATH算法，基于核密度估计。这种方法可以用于分类和回归任务。这个总体方法包括两大家族的人工扩充：基于扰动，如 Gaussian Noise，和基于 interpolations，如 SMOTE。它还提供了这些机器学习算法的直观表达，特别是对 SMOTE 的表达。我们从这些synthetic数据生成器中推出了新的数据生成器。我们在不均衡回归中使用这些生成器和通用Bootstrap抽取技术来预测值。我们对 GOLIATH 算法在不均衡回归情况下的性能进行了实验性评估和比较，并证明了我们的方法在现有状态的技术上显著提高了性能。
</details></li>
</ul>
<hr>
<h2 id="Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation"><a href="#Data-Fusion-for-Multi-Task-Learning-of-Building-Extraction-and-Height-Estimation" class="headerlink" title="Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation"></a>Data Fusion for Multi-Task Learning of Building Extraction and Height Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02960">http://arxiv.org/abs/2308.02960</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/SaadAhmedJamal/IEEE_DFC2023">https://github.com/SaadAhmedJamal/IEEE_DFC2023</a></li>
<li>paper_authors: Saad Ahmed Jamal, Arioluwa Aribisala</li>
<li>for: 本研究は都市重建问题上提出的DFC23 Track 2 Contest中的一种多任务学习方法，用于批量抽取和高度估算 optical和雷达卫星图像中。</li>
<li>methods: 本研究使用多任务学习方法，通过重用特征和形成隐式约束 между多个任务来提高解决方案的质量。</li>
<li>results: 本研究的基准结果表明，对于批量抽取和高度估算，在设计了相关实验后，baseline结果显著提高了。<details>
<summary>Abstract</summary>
In accordance with the urban reconstruction problem proposed by the DFC23 Track 2 Contest, this paper attempts a multitask-learning method of building extraction and height estimation using both optical and radar satellite imagery. Contrary to the initial goal of multitask learning which could potentially give a superior solution by reusing features and forming implicit constraints between multiple tasks, this paper reports the individual implementation of the building extraction and height estimation under constraints. The baseline results for the building extraction and the height estimation significantly increased after designed experiments.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "urban reconstruction" was translated as "城市重建" (chéngshì zhòngjiàn)* "multitask-learning" was translated as "多任务学习" (duō zhìxí)* "building extraction" was translated as "建筑物提取" (jiànzhì wù tiēchū)* "height estimation" was translated as "高度估算" (gāodù gèsuan)* "satellite imagery" was translated as "卫星图像" (wèixīng túxiàng)
</details></li>
</ul>
<hr>
<h2 id="K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets"><a href="#K-band-Self-supervised-MRI-Reconstruction-via-Stochastic-Gradient-Descent-over-K-space-Subsets" class="headerlink" title="K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets"></a>K-band: Self-supervised MRI Reconstruction via Stochastic Gradient Descent over K-space Subsets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02958">http://arxiv.org/abs/2308.02958</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mikgroup/k-band">https://github.com/mikgroup/k-band</a></li>
<li>paper_authors: Frederic Wang, Han Qi, Alfredo De Goyeneche, Reinhard Heckel, Michael Lustig, Efrat Shimron</li>
<li>for: 这项研究的目的是为了使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练，以提高MRI图像重建的精度和效率。</li>
<li>methods: 这项研究使用了一种新的数学框架，称为k-band，以便使用只有部分、有限分辨率的k-空间数据进行深度学习模型的训练。具体来说，这种方法使用了在每次训练迭代中使用只有一小部分k-空间数据来计算梯度的束教学法。</li>
<li>results: 实验表明，k-band方法可以与使用高分辨率数据进行训练的状态对照方法（SoTA）的性能相似，而无需使用高分辨率数据进行训练。此外，k-band方法还可以在快速获得的有限分辨率数据上进行自我监督式训练，从而提高了MRI图像重建的精度和效率。<details>
<summary>Abstract</summary>
Although deep learning (DL) methods are powerful for solving inverse problems, their reliance on high-quality training data is a major hurdle. This is significant in high-dimensional (dynamic/volumetric) magnetic resonance imaging (MRI), where acquisition of high-resolution fully sampled k-space data is impractical. We introduce a novel mathematical framework, dubbed k-band, that enables training DL models using only partial, limited-resolution k-space data. Specifically, we introduce training with stochastic gradient descent (SGD) over k-space subsets. In each training iteration, rather than using the fully sampled k-space for computing gradients, we use only a small k-space portion. This concept is compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details>
<details>
<summary>摘要</summary>
尽管深度学习（DL）方法有力量解决反向问题，但它们依赖高质量训练数据是一个主要障碍。在高维度（动态/体积）磁共振成像（MRI）中，获取高分辨率完全采样的k空间数据是不现实的。我们介绍了一种新的数学框架，称之为k带，允许使用仅部分、有限分辨率k空间数据进行训练DL模型。具体来说，我们引入了使用批处理的梯度下降（SGD）在k空间子集上训练。在每个训练迭代中，而不是使用完全采样的k空间来计算梯度，我们只使用一小部分k空间。这种概念 compatible with different sampling strategies; here we demonstrate the method for k-space "bands", which have limited resolution in one dimension and can hence be acquired rapidly. We prove analytically that our method stochastically approximates the gradients computed in a fully-supervised setup, when two simple conditions are met: (i) the limited-resolution axis is chosen randomly-uniformly for every new scan, hence k-space is fully covered across the entire training set, and (ii) the loss function is weighed with a mask, derived here analytically, which facilitates accurate reconstruction of high-resolution details. Numerical experiments with raw MRI data indicate that k-band outperforms two other methods trained on limited-resolution data and performs comparably to state-of-the-art (SoTA) methods trained on high-resolution data. k-band hence obtains SoTA performance, with the advantage of training using only limited-resolution data. This work hence introduces a practical, easy-to-implement, self-supervised training framework, which involves fast acquisition and self-supervised reconstruction and offers theoretical guarantees.
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Study-of-AI-based-Smart-Contract-Creation"><a href="#An-Empirical-Study-of-AI-based-Smart-Contract-Creation" class="headerlink" title="An Empirical Study of AI-based Smart Contract Creation"></a>An Empirical Study of AI-based Smart Contract Creation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02955">http://arxiv.org/abs/2308.02955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rabimba Karanjai, Edward Li, Lei Xu, Weidong Shi</li>
<li>for: 本研究的主要目标是评估 LLMS 生成的智能合约代码质量。</li>
<li>methods: 我们使用了一个实验设置来评估生成代码的正确性、安全性和效率。</li>
<li>results: 我们发现生成的智能合约代码存在安全漏洞，同时代码质量和正确性受到输入参数的影响。但我们还发现了一些可以改进的方向。<details>
<summary>Abstract</summary>
The introduction of large language models (LLMs) like ChatGPT and Google Palm2 for smart contract generation seems to be the first well-established instance of an AI pair programmer. LLMs have access to a large number of open-source smart contracts, enabling them to utilize more extensive code in Solidity than other code generation tools. Although the initial and informal assessments of LLMs for smart contract generation are promising, a systematic evaluation is needed to explore the limits and benefits of these models. The main objective of this study is to assess the quality of generated code provided by LLMs for smart contracts. We also aim to evaluate the impact of the quality and variety of input parameters fed to LLMs. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our study finds crucial evidence of security bugs getting introduced in the generated smart contracts as well as the overall quality and correctness of the code getting impacted. However, we also identified the areas where it can be improved. The paper also proposes several potential research directions to improve the process, quality and safety of generated smart contract codes.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）如ChatGPT和Google Palm2的出现似乎是首个成功的AI双程程式。LLMs可以使用大量的开源智能合约，使其在Solidity中运用更大的代码。虽然初步评估结果尚未正式，但是需要系统性的评估来探索这些模型的限制和优点。本研究的主要目标是评估LLMs生成的智能合约代码质量。我们还想评估对LLMs输入参数的影响，以及生成代码的有效性、正确性和安全性。为了完成这个目标，我们设计了一个实验室来评估生成代码的有效性、正确性和安全性。我们发现生成的智能合约中有许多安全漏洞，并且代码的质量和正确性受到影响。但是，我们也发现了一些改善这些过程的研究方向。本文还提出了多个可能的研究方向，以提高生成代码的过程、质量和安全性。
</details></li>
</ul>
<hr>
<h2 id="dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning"><a href="#dPASP-A-Comprehensive-Differentiable-Probabilistic-Answer-Set-Programming-Environment-For-Neurosymbolic-Learning-and-Reasoning" class="headerlink" title="dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning"></a>dPASP: A Comprehensive Differentiable Probabilistic Answer Set Programming Environment For Neurosymbolic Learning and Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02944">http://arxiv.org/abs/2308.02944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Renato Lui Geh, Jonas Gonçalves, Igor Cataneo Silveira, Denis Deratani Mauá, Fabio Gagliardi Cozman</li>
<li>for: 这篇论文旨在提出一种新的宣言型概率逻辑编程框架，用于可微分的神经符号逻辑推理。</li>
<li>methods: 该框架使得可以指定精确的概率模型，包括神经 predicate、逻辑约束和间隔值概率选择，以支持 combining 低级感知（图像、文本等）、通用的推理和（模糊的）统计知识。</li>
<li>results: 论文提出了多种 semantics  для probabilistic logic programs，以表达不确定、矛盾、不完整和&#x2F;或统计知识。同时，也介绍了如何使用神经 predicate 和概率选择进行 gradient-based 学习。论文还描述了一个实现的 package，用于推理和学习，并提供了一些示例程序。<details>
<summary>Abstract</summary>
We present dPASP, a novel declarative probabilistic logic programming framework for differentiable neuro-symbolic reasoning. The framework allows for the specification of discrete probabilistic models with neural predicates, logic constraints and interval-valued probabilistic choices, thus supporting models that combine low-level perception (images, texts, etc), common-sense reasoning, and (vague) statistical knowledge. To support all such features, we discuss the several semantics for probabilistic logic programs that can express nondeterministic, contradictory, incomplete and/or statistical knowledge. We also discuss how gradient-based learning can be performed with neural predicates and probabilistic choices under selected semantics. We then describe an implemented package that supports inference and learning in the language, along with several example programs. The package requires minimal user knowledge of deep learning system's inner workings, while allowing end-to-end training of rather sophisticated models and loss functions.
</details>
<details>
<summary>摘要</summary>
我们提出了dpASP，一种新的宣告性概率逻辑编程框架，用于可 diferenciable 神经符号逻辑推理。该框架允许用户 specify 权重逻辑模型，神经元 predicate，逻辑约束和间隔值概率选择，因此可以支持模型结合低级感知（图像、文本等）、通用理智和（抽象）统计知识。为支持这些特性，我们讨论了几种 probabilistic logic programs 的 semantics，可以表达不确定、矛盾、不完整和/或统计知识。我们还讨论了如何在 selected semantics 下使用 neural predicates 和 probabilistic choices 进行梯度基于学习。然后，我们描述了一个实现的 package，支持推理和学习语言中的各种例程，并提供了一些示例程序。该 package 需要用户具备最低知识量，同时允许用户通过终端训练较复杂的模型和损失函数。
</details></li>
</ul>
<hr>
<h2 id="Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry"><a href="#Towards-the-Development-of-an-Uncertainty-Quantification-Protocol-for-the-Natural-Gas-Industry" class="headerlink" title="Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry"></a>Towards the Development of an Uncertainty Quantification Protocol for the Natural Gas Industry</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02941">http://arxiv.org/abs/2308.02941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Babajide Kolade</li>
<li>for: This paper aims to develop a protocol for assessing uncertainties in predictions of machine learning and mechanistic simulation models, specifically for the gas distribution industry.</li>
<li>methods: The protocol outlines an uncertainty quantification workflow that includes identifying key sources of uncertainties, using applicable methods of uncertainty propagation, and employing statistically rational estimators for output uncertainties.</li>
<li>results: The paper applies the protocol to test cases relevant to the gas distribution industry and presents the learnings from its application. The results demonstrate the effectiveness of the protocol in quantifying uncertainties in simulation results.<details>
<summary>Abstract</summary>
Simulations using machine learning (ML) models and mechanistic models are often run to inform decision-making processes. Uncertainty estimates of simulation results are critical to the decision-making process because simulation results of specific scenarios may have wide, but unspecified, confidence bounds that may impact subsequent analyses and decisions. The objective of this work is to develop a protocol to assess uncertainties in predictions of machine learning and mechanistic simulation models. The protocol will outline an uncertainty quantification workflow that may be used to establish credible bounds of predictability on computed quantities of interest and to assess model sufficiency. The protocol identifies key sources of uncertainties in machine learning and mechanistic modeling, defines applicable methods of uncertainty propagation for these sources, and includes statistically rational estimators for output uncertainties. The work applies the protocol to test cases relevant to the gas distribution industry and presents learnings from its application. The paper concludes with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry
</details>
<details>
<summary>摘要</summary>
模拟使用机器学习（ML）模型和机制模型经常用来支持决策过程。模拟结果中的不确定性估计对决策过程是关键的，因为特定场景的模拟结果可能具有广泛而不确定的信任范围，这可能影响后续分析和决策。本工作的目标是开发一个协议来评估机器学习和机制模型预测结果中的不确定性。协议将 outline一个不确定性评估工作流程，可以用来确定计算量据点的可靠范围和评估模型的充分性。协议列出了机器学习和机制模型中的主要不确定性来源，采用可靠的方法进行不确定性传播，并提供了统计合理的输出不确定性估计器。本工作应用协议到 relevante test cases，并presented learnings from its application。文章 conclude with a brief discussion outlining a pathway to the wider adoption of uncertainty quantification within the industry。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval"><a href="#Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval" class="headerlink" title="Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval"></a>Towards Consistency Filtering-Free Unsupervised Learning for Dense Retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02926">http://arxiv.org/abs/2308.02926</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval">https://github.com/Haoxiang-WasedaU/Towards-Consistency-Filtering-Free-Unsupervised-Learning-for-Dense-Retrieval</a></li>
<li>paper_authors: Haoxiang Shi, Sumio Fujita, Tetsuya Sakai</li>
<li>for: 提高现代神经信息检索中的领域传递问题的解决方案</li>
<li>methods: 取消consistency filter，使用直接pseudo-labeling、pseudo-相关反馈或无监督关键词生成方法实现一致性自由粗粒度检索</li>
<li>results: 对多个数据集进行了广泛的实验评估，结果显示，使用TextRank基于pseudo relevance feedback方法可以超过其他方法的表现，并且对训练和推理效率具有显著改进。<details>
<summary>Abstract</summary>
Domain transfer is a prevalent challenge in modern neural Information Retrieval (IR). To overcome this problem, previous research has utilized domain-specific manual annotations and synthetic data produced by consistency filtering to finetune a general ranker and produce a domain-specific ranker. However, training such consistency filters are computationally expensive, which significantly reduces the model efficiency. In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain. In this study, we evaluate a more efficient solution: replacing the consistency filter with either direct pseudo-labeling, pseudo-relevance feedback, or unsupervised keyword generation methods for achieving consistent filtering-free unsupervised dense retrieval. Our extensive experimental evaluations demonstrate that, on average, TextRank-based pseudo relevance feedback outperforms other methods. Furthermore, we analyzed the training and inference efficiency of the proposed paradigm. The results indicate that filtering-free unsupervised learning can continuously improve training and inference efficiency while maintaining retrieval performance. In some cases, it can even improve performance based on particular datasets.
</details>
<details>
<summary>摘要</summary>
域名转移是现代神经信息检索（IR）中的一个常见挑战。以前的研究曾利用域名特定的手动标注和由consistency filtering生成的 sintetic数据来训练一个通用排名器并生成域名特定的排名器。然而，训练such consistency filters具有计算成本高的问题，这会significantly reduces the model efficiency。 In addition, consistency filtering often struggles to identify retrieval intentions and recognize query and corpus distributions in a target domain。在本研究中，我们评估了一种更有效的解决方案：取代consistency filter with direct pseudo-labeling、pseudo-relevance feedback或Unsupervised keyword generation方法来实现无 filtering-free无监督的排名。我们的广泛的实验评估表明，TextRank-based pseudo relevance feedback在其他方法中表现更好。此外，我们还分析了提议的训练和推理效率。结果表明，filtering-free无监督学习可以不断提高训练和推理效率，同时保持检索性能。在某些 dataset 上，它可以提高性能。
</details></li>
</ul>
<hr>
<h2 id="An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks"><a href="#An-AI-Enabled-Framework-to-Defend-Ingenious-MDT-based-Attacks-on-the-Emerging-Zero-Touch-Cellular-Networks" class="headerlink" title="An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks"></a>An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the Emerging Zero Touch Cellular Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02923">http://arxiv.org/abs/2308.02923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran</li>
<li>for: 本研究旨在探讨黑客可以通过劣质MDT报告来攻击深度自动化无线网络的新型攻击方式，并对常见网络自动化功能的性能造成负面影响。</li>
<li>methods: 本研究使用机器学习方法来检测和排除劣质MDT报告，并通过一个实验场景证明其效果。</li>
<li>results: 研究发现，劣质MDT报告可以影响网络自动化功能的性能，而且可以通过Machine Learning来检测和排除这些劣质报告。<details>
<summary>Abstract</summary>
Deep automation provided by self-organizing network (SON) features and their emerging variants such as zero touch automation solutions is a key enabler for increasingly dense wireless networks and pervasive Internet of Things (IoT). To realize their objectives, most automation functionalities rely on the Minimization of Drive Test (MDT) reports. The MDT reports are used to generate inferences about network state and performance, thus dynamically change network parameters accordingly. However, the collection of MDT reports from commodity user devices, particularly low cost IoT devices, make them a vulnerable entry point to launch an adversarial attack on emerging deeply automated wireless networks. This adds a new dimension to the security threats in the IoT and cellular networks. Existing literature on IoT, SON, or zero touch automation does not address this important problem. In this paper, we investigate an impactful, first of its kind adversarial attack that can be launched by exploiting the malicious MDT reports from the compromised user equipment (UE). We highlight the detrimental repercussions of this attack on the performance of common network automation functions. We also propose a novel Malicious MDT Reports Identification framework (MRIF) as a countermeasure to detect and eliminate the malicious MDT reports using Machine Learning and verify it through a use-case. Thus, the defense mechanism can provide the resilience and robustness for zero touch automation SON engines against the adversarial MDT attacks
</details>
<details>
<summary>摘要</summary>
深度自动化提供的无需人工控制网络（SON）功能和其相关的零 touched自动化解决方案是现代无线网络和物联网（IoT）的关键驱动力。为实现他们的目标，大多数自动化功能都依赖于推理测试（MDT）报告。MDT报告用于生成网络状态和性能的推理，并在运行时动态地改变网络参数。然而，从低成本IoT设备收集MDT报告，特别是从受到攻击的用户设备，使得这些报告成为攻击 deeply automated 无线网络的易受攻击点。这添加了一个新的安全隐患，并且现有的相关文献不具备对这一重要问题的讨论。在这篇论文中，我们研究了一种新型的攻击，可以通过恶意MDT报告来发动，从恶意用户设备收集MDT报告。我们强调了这种攻击对常见网络自动化功能的不良影响。此外，我们还提出了一种新的恶意MDT报告识别框架（MRIF），用于检测和消除恶意MDT报告。我们使用机器学习来实现这一目标，并通过用例验证了这种防御机制的有效性。因此，这种防御机制可以为零 touched自动化 SON 引擎提供鲜活性和鲜活性。
</details></li>
</ul>
<hr>
<h2 id="Structured-Low-Rank-Tensors-for-Generalized-Linear-Models"><a href="#Structured-Low-Rank-Tensors-for-Generalized-Linear-Models" class="headerlink" title="Structured Low-Rank Tensors for Generalized Linear Models"></a>Structured Low-Rank Tensors for Generalized Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02922">http://arxiv.org/abs/2308.02922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Batoul Taki, Anand D. Sarwate, Waheed U. Bajwa</li>
<li>for: This paper is written for researchers and practitioners interested in exploring tensor-based methods for generalized linear model (GLM) problems, particularly those dealing with low-rank tensor structures.</li>
<li>methods: The paper proposes a new low-rank tensor model called the Low Separation Rank (LSR) model, which is imposed onto the coefficient tensor in GLM problems. The authors also develop a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs.</li>
<li>results: The paper derives a minimax lower bound on the error threshold for estimating the coefficient tensor in LSR tensor GLM problems, which suggests that the sample complexity of the proposed method may be significantly lower than that of vectorized GLMs. The authors also demonstrate the efficacy of the proposed LSR tensor model on synthetic and real-world datasets.<details>
<summary>Abstract</summary>
Recent works have shown that imposing tensor structures on the coefficient tensor in regression problems can lead to more reliable parameter estimation and lower sample complexity compared to vector-based methods. This work investigates a new low-rank tensor model, called Low Separation Rank (LSR), in Generalized Linear Model (GLM) problems. The LSR model -- which generalizes the well-known Tucker and CANDECOMP/PARAFAC (CP) models, and is a special case of the Block Tensor Decomposition (BTD) model -- is imposed onto the coefficient tensor in the GLM model. This work proposes a block coordinate descent algorithm for parameter estimation in LSR-structured tensor GLMs. Most importantly, it derives a minimax lower bound on the error threshold on estimating the coefficient tensor in LSR tensor GLM problems. The minimax bound is proportional to the intrinsic degrees of freedom in the LSR tensor GLM problem, suggesting that its sample complexity may be significantly lower than that of vectorized GLMs. This result can also be specialised to lower bound the estimation error in CP and Tucker-structured GLMs. The derived bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details>
<details>
<summary>摘要</summary>
近期研究表明，在回归问题中强制维度结构 onto 参数矩阵可以导致更可靠的参数估计和较低的样本复杂度，相比vector化方法。这个工作 investigate一种新的低级别维度模型（LSR）在泛化线性模型（GLM）问题中。LSR模型是Tucker和CANDECOMP/PARAFAC（CP）模型的推广，并是阻塞矩阵分解（BTD）模型的特殊情况。这个工作提出了一种块坐标极下降算法 для GLM问题中LSR结构的参数估计。最重要的是，它 derivates一个最小最大下界对于GLM问题中LSR结构参数估计的误差阈值。这个下界与LSR结构参数估计的内在度度相关，表明其样本复杂度可能远低于vector化GLM的样本复杂度。这个结果还可以特殊化为对CP和Tucker结构GLM的参数估计误差的下界。 derivates bounds are comparable to tight bounds in the literature for Tucker linear regression, and the tightness of the minimax lower bound is further assessed numerically. Finally, numerical experiments on synthetic datasets demonstrate the efficacy of the proposed LSR tensor model for three regression types (linear, logistic and Poisson). Experiments on a collection of medical imaging datasets demonstrate the usefulness of the LSR model over other tensor models (Tucker and CP) on real, imbalanced data with limited available samples.
</details></li>
</ul>
<hr>
<h2 id="Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons"><a href="#Spectral-Ranking-Inferences-based-on-General-Multiway-Comparisons" class="headerlink" title="Spectral Ranking Inferences based on General Multiway Comparisons"></a>Spectral Ranking Inferences based on General Multiway Comparisons</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02918">http://arxiv.org/abs/2308.02918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</li>
<li>For:  This paper studies the performance of the spectral method in estimating and quantifying uncertainty of unobserved preference scores in a very general and realistic setup.* Methods: The paper uses the spectral method, which is a more general and flexible approach than the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. The paper also introduces a two-step spectral method that can achieve the same asymptotic efficiency as the Maximum Likelihood Estimator (MLE).* Results: The paper provides comprehensive frameworks for one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It also introduces effective two-sample rank testing methods that are noteworthy. The paper substantiates its findings via comprehensive numerical simulations and applies its developed methodologies to perform statistical inferences on statistics journals and movie rankings.<details>
<summary>Abstract</summary>
This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a very general and more realistic setup in which the comparison graph consists of hyper-edges of possible heterogeneous sizes and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly-used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in the scenarios when the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that it is the first time effective two-sample rank testing methods are proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences on statistics journals and movie rankings.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket"><a href="#Adversarial-Erasing-with-Pruned-Elements-Towards-Better-Graph-Lottery-Ticket" class="headerlink" title="Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket"></a>Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02916">http://arxiv.org/abs/2308.02916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangyuwen0627/ace-glt">https://github.com/wangyuwen0627/ace-glt</a></li>
<li>paper_authors: Yuwen Wang, Shunyu Liu, Kaixuan Chen, Tongtian Zhu, Ji Qiao, Mengjie Shi, Yuanyu Wan, Mingli Song</li>
<li>for:  Mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods:  Combination of core subgraph and sparse subnetwork, adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results:  Outperforms existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.Here’s the summary in English for reference:</li>
<li>for: Mitigating the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance.</li>
<li>methods: Combining core subgraph and sparse subnetwork, using an adversarial complementary erasing (ACE) framework to explore valuable information from pruned components.</li>
<li>results: Outperforming existing methods for searching Graph Lottery Ticket (GLT) in diverse tasks.<details>
<summary>Abstract</summary>
Graph Lottery Ticket (GLT), a combination of core subgraph and sparse subnetwork, has been proposed to mitigate the computational cost of deep Graph Neural Networks (GNNs) on large input graphs while preserving original performance. However, the winning GLTs in exisiting studies are obtained by applying iterative magnitude-based pruning (IMP) without re-evaluating and re-considering the pruned information, which disregards the dynamic changes in the significance of edges/weights during graph/model structure pruning, and thus limits the appeal of the winning tickets. In this paper, we formulate a conjecture, i.e., existing overlooked valuable information in the pruned graph connections and model parameters which can be re-grouped into GLT to enhance the final performance. Specifically, we propose an adversarial complementary erasing (ACE) framework to explore the valuable information from the pruned components, thereby developing a more powerful GLT, referred to as the ACE-GLT. The main idea is to mine valuable information from pruned edges/weights after each round of IMP, and employ the ACE technique to refine the GLT processing. Finally, experimental results demonstrate that our ACE-GLT outperforms existing methods for searching GLT in diverse tasks. Our code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
《图lottery票（GLT）》，一种组合核心子图和稀疏子网络的方法，用于降低深度图神经网络（GNNs）在大输入图上的计算成本，保持原始性能。然而，现有的研究中的赢家GLT是通过迭代矩阵优化（IMP）而不是重新评估和重新考虑被截割的信息，这会忽略图/模型结构截割中 Edge/权重的动态变化，因此限制了赢家票的吸引力。在这篇论文中，我们提出一个假设，即现有的被遗弃的有价信息在截割的图连接和模型参数中，可以重新组织成GLT，以提高最终性能。 Specifically, we propose an adversarial complementary erasing（ACE）框架，用于探索截割后每个回合的有价信息，并使用ACE技术来细化GLT处理。最后，我们的实验结果表明，我们的ACE-GLT在多种任务中超过现有方法搜索GLT的性能。我们的代码将公开发布。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/06/cs.LG_2023_08_06/" data-id="cllsk9gq7002u9c885blnh0t8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/cs.SD_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/cs.SD_2023_08_06/">cs.SD - 2023-08-06 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="SoK-Acoustic-Side-Channels"><a href="#SoK-Acoustic-Side-Channels" class="headerlink" title="SoK: Acoustic Side Channels"></a>SoK: Acoustic Side Channels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03806">http://arxiv.org/abs/2308.03806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Wang, Shishir Nagaraja, Aurélien Bourquard, Haichang Gao, Jeff Yan</li>
<li>for: 本研究做出了一个 state-of-the-art 的分析，涵盖了全部重要的学术研究领域，讨论了安全性含义和对策，并寻找了未来研究的方向。</li>
<li>methods: 本研究使用了多种方法，包括对渠道的分析、对 inverse problems 的研究以及两者之间的连接。</li>
<li>results: 本研究得到了一些深刻的结论，包括渠道的安全性含义和未来研究的方向。<details>
<summary>Abstract</summary>
We provide a state-of-the-art analysis of acoustic side channels, cover all the significant academic research in the area, discuss their security implications and countermeasures, and identify areas for future research. We also make an attempt to bridge side channels and inverse problems, two fields that appear to be completely isolated from each other but have deep connections.
</details>
<details>
<summary>摘要</summary>
我们提供了当今最先进的声学侧途分析，涵盖了全部主要的学术研究领域，讨论了它们的安全意义和防范措施，并确定了未来研究的方向。我们还尝试将侧途和反问题两个领域联系起来，这两个领域之前被视为完全不相关的，但它们在深层次上有着深刻的联系。
</details></li>
</ul>
<hr>
<h2 id="Characterization-of-cough-sounds-using-statistical-analysis"><a href="#Characterization-of-cough-sounds-using-statistical-analysis" class="headerlink" title="Characterization of cough sounds using statistical analysis"></a>Characterization of cough sounds using statistical analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03019">http://arxiv.org/abs/2308.03019</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naveenkumar Vodnala, Pratap Reddy Lankireddy, Padmasai Yarlagadda</li>
<li>For: 本研究旨在Characterize cough sounds with voiced content and cough sounds without voiced content，以便更好地诊断呼吸疾病。* Methods: 该研究使用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述呼吸 зву频谱特征。这些属性 subsequentially subjected to statistical analysis using measures of minimum, maximum, mean, median, and standard deviation。* Results: 实验结果表明，呼吸音频谱特征的mean和frequency distribution高于speech signals，spectral flatness水平在0.22左右，spectral flux在0.3-0.6之间，Zero Crossing Rate大约在0.05-0.4之间。这些属性具有重要的信息价值，可以帮助更好地Characterize cough sounds。<details>
<summary>Abstract</summary>
Cough is a primary symptom of most respiratory diseases, and changes in cough characteristics provide valuable information for diagnosing respiratory diseases. The characterization of cough sounds still lacks concrete evidence, which makes it difficult to accurately distinguish between different types of coughs and other sounds. The objective of this research work is to characterize cough sounds with voiced content and cough sounds without voiced content. Further, the cough sound characteristics are compared with the characteristics of speech. The proposed method to achieve this goal utilized spectral roll-off, spectral entropy, spectral flatness, spectral flux, zero crossing rate, spectral centroid, and spectral bandwidth attributes which describe the cough sounds related to the respiratory system, glottal information, and voice model. These attributes are then subjected to statistical analysis using the measures of minimum, maximum, mean, median, and standard deviation. The experimental results show that the mean and frequency distribution of spectral roll-off, spectral centroid, and spectral bandwidth are found to be higher for cough sounds than for speech signals. Spectral flatness levels in cough sounds will rise to 0.22, whereas spectral flux varies between 0.3 and 0.6. The Zero Crossing Rate (ZCR) of most frames of cough sounds is between 0.05 and 0.4. These attributes contribute significant information while characterizing cough sounds.
</details>
<details>
<summary>摘要</summary>
<SYS>将文本翻译成简化中文。</SYS>吐は多种呼吸疾病的主要症状之一，而吐的特征变化可以提供诊断呼吸疾病的有价值信息。然而，吐 зву的特征化仍然缺乏具体证据，这使得准确地分辨吐音和其他声音变得困难。本研究的目标是Characterize吐音与有声吐音。此外，吐音特征与语音特征进行比较。提出的方法是利用spectral roll-off、spectral entropy、spectral flatness、spectral flux、zero crossing rate、spectral centroid和spectral bandwidth属性来描述吐音，这些属性是基于呼吸系统、舌叶信息和语音模型。这些属性后来通过统计分析使用 minimum、maximum、mean、median和标准差度量进行评估。实验结果表明，吐音的mean和频谱分布的 spectral roll-off、spectral centroid和spectral bandwidth均高于语音信号。吐音中的spectral flatness水平上升到0.22，而spectral flux在0.3和0.6之间变化。zero crossing rate的大多数帧为0.05和0.4。这些属性对吐音 caracterization提供了重要信息。
</details></li>
</ul>
<hr>
<h2 id="DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation"><a href="#DiffDance-Cascaded-Human-Motion-Diffusion-Model-for-Dance-Generation" class="headerlink" title="DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation"></a>DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02915">http://arxiv.org/abs/2308.02915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan</li>
<li>for: 本研究旨在生成高分辨率、长形舞蹈序列，以便与音乐进行 conditional generation。</li>
<li>methods: 我们提出了一种新的层次动态扩散模型，即 DiffDance，以解决传统autoregressive方法在采样中引入折衔错误和长期结构捕捉的限制。我们还采用了多种几何损失来限制模型输出的物理可能性，并在扩散过程中采用动态权重来促进样本多样性。</li>
<li>results: 我们通过对 AIST++ 数据集进行 comprehensive 试验，证明了 DiffDance 能够生成与输入音乐高效对齐的实际舞蹈序列，并与状态静态方法相当。<details>
<summary>Abstract</summary>
When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques"><a href="#Anonymizing-Speech-Evaluating-and-Designing-Speaker-Anonymization-Techniques" class="headerlink" title="Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques"></a>Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04455">http://arxiv.org/abs/2308.04455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/deep-privacy/SA-toolkit">https://github.com/deep-privacy/SA-toolkit</a></li>
<li>paper_authors: Pierre Champion<br>for:  This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization to address the privacy issues arising from the collection and storage of personal speech data in voice-based digital assistants.methods: The thesis employs a combination of voice conversion-based anonymization systems and quantization-based transformation methods to reduce speaker PPI while maintaining utility.results: The thesis evaluates the degree of privacy protection provided by these methods and proposes a new attack method to invert anonymization, highlighting the limitations of current anonymization systems and identifying areas for improvement.<details>
<summary>Abstract</summary>
The growing use of voice user interfaces has led to a surge in the collection and storage of speech data. While data collection allows for the development of efficient tools powering most speech services, it also poses serious privacy issues for users as centralized storage makes private personal speech data vulnerable to cyber threats. With the increasing use of voice-based digital assistants like Amazon's Alexa, Google's Home, and Apple's Siri, and with the increasing ease with which personal speech data can be collected, the risk of malicious use of voice-cloning and speaker/gender/pathological/etc. recognition has increased.   This thesis proposes solutions for anonymizing speech and evaluating the degree of the anonymization. In this work, anonymization refers to making personal speech data unlinkable to an identity while maintaining the usefulness (utility) of the speech signal (e.g., access to linguistic content). We start by identifying several challenges that evaluation protocols need to consider to evaluate the degree of privacy protection properly. We clarify how anonymization systems must be configured for evaluation purposes and highlight that many practical deployment configurations do not permit privacy evaluation. Furthermore, we study and examine the most common voice conversion-based anonymization system and identify its weak points before suggesting new methods to overcome some limitations. We isolate all components of the anonymization system to evaluate the degree of speaker PPI associated with each of them. Then, we propose several transformation methods for each component to reduce as much as possible speaker PPI while maintaining utility. We promote anonymization algorithms based on quantization-based transformation as an alternative to the most-used and well-known noise-based approach. Finally, we endeavor a new attack method to invert anonymization.
</details>
<details>
<summary>摘要</summary>
声音用户界面的使用量在增长，导致了个人语音数据的收集和存储。而这些数据的收集可以为语音服务的开发提供高效的工具，但也会对用户的隐私造成严重的威胁，因为中央存储的私人语音数据容易受到网络攻击。随着智能语音助手 like Amazon Alexa、Google Home 和 Apple Siri 的使用的加密，以及个人语音数据的收集变得更加容易，黑客利用语音恶作剂和 speaker/性别/疾病等识别的风险也在增加。本论目标是提出一种隐藏个人语音数据的方法，以保护用户的隐私。在这个过程中，我们认为需要考虑以下几个挑战：1. 评估隐私保护程度：我们需要设计一种评估隐私保护程度的方法，以确保个人语音数据不可链接到用户身份。2. 配置评估系统：我们需要配置评估系统，以便在实际应用中进行评估。3. 避免攻击：我们需要研究和探讨常见的语音转换基于隐藏的攻击方法，并提出新的方法来解决一些限制。我们认为，以下几个方法可以用于隐藏个人语音数据：1. 量化变换：我们可以使用量化变换来减少说话人的个人特征，以保护用户的隐私。2. 隐藏语音特征：我们可以使用隐藏语音特征的技术来隐藏个人语音数据，以降低黑客的攻击风险。3. 多模型融合：我们可以使用多个模型来融合语音数据，以提高隐私保护的效果。最后，我们提出了一种新的攻击方法，可以尝试将隐藏后的语音数据恢复到原始形式。这种攻击方法可以帮助我们更好地理解隐私保护的限制，并提高隐私保护的效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/06/cs.SD_2023_08_06/" data-id="cllsk9gqy005a9c88g6ridtoj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_06" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/06/eess.IV_2023_08_06/" class="article-date">
  <time datetime="2023-08-05T16:00:00.000Z" itemprop="datePublished">2023-08-06</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/06/eess.IV_2023_08_06/">eess.IV - 2023-08-06 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information"><a href="#FourLLIE-Boosting-Low-Light-Image-Enhancement-by-Fourier-Frequency-Information" class="headerlink" title="FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information"></a>FourLLIE: Boosting Low-Light Image Enhancement by Fourier Frequency Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03033">http://arxiv.org/abs/2308.03033</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wangchx67/fourllie">https://github.com/wangchx67/fourllie</a></li>
<li>paper_authors: Chenxi Wang, Hongjun Wu, Zhi Jin</li>
<li>for: 提高低光照图像的亮度和细节</li>
<li>methods: 基于快执行 fourier 变换，利用振荡频率信息和地图信息，实现低光照图像的进一步优化</li>
<li>results: 与当前最佳方法进行比较，实现了更高的亮度和细节精度，同时具有较好的模型效率<details>
<summary>Abstract</summary>
Recently, Fourier frequency information has attracted much attention in Low-Light Image Enhancement (LLIE). Some researchers noticed that, in the Fourier space, the lightness degradation mainly exists in the amplitude component and the rest exists in the phase component. By incorporating both the Fourier frequency and the spatial information, these researchers proposed remarkable solutions for LLIE. In this work, we further explore the positive correlation between the magnitude of amplitude and the magnitude of lightness, which can be effectively leveraged to improve the lightness of low-light images in the Fourier space. Moreover, we find that the Fourier transform can extract the global information of the image, and does not introduce massive neural network parameters like Multi-Layer Perceptrons (MLPs) or Transformer. To this end, a two-stage Fourier-based LLIE network (FourLLIE) is proposed. In the first stage, we improve the lightness of low-light images by estimating the amplitude transform map in the Fourier space. In the second stage, we introduce the Signal-to-Noise-Ratio (SNR) map to provide the prior for integrating the global Fourier frequency and the local spatial information, which recovers image details in the spatial space. With this ingenious design, FourLLIE outperforms the existing state-of-the-art (SOTA) LLIE methods on four representative datasets while maintaining good model efficiency.
</details>
<details>
<summary>摘要</summary>
最近，傅里叶频率信息在低光照图像增强（LLIE）中吸引了很多关注。一些研究人员注意到，在傅里叶空间中，亮度下降主要存在于振荡Component中，而剩下的存在于相位Component中。通过汇合傅里叶频率和空间信息，这些研究人员提出了非常出色的解决方案。在这项工作中，我们进一步探索了振荡幅度与亮度幅度之间的正相关关系，可以有效地提高低光照图像的亮度在傅里叶空间中。此外，我们发现傅里叶变换可以提取图像的全局信息，而不需要大量的神经网络参数，比如多层感知器（MLP）或转换器。基于这种创新的设计，我们提出了一种两个阶段的傅里叶基于LLIE网络（FourLLIE）。在第一阶段，我们使用傅里叶变换Map来提高低光照图像的亮度。在第二阶段，我们引入信噪比Map，以提供优化全局傅里叶频率和本地空间信息的优化约束，从而恢复图像的细节在空间空间中。与现有的状态的 искусственный智能（SOTA）LLIE方法相比，FourLLIE在四个代表性的数据集上达到了更高的性能，而且保持了好的模型效率。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Spike-based-Image-Restoration-under-General-Illumination"><a href="#Recurrent-Spike-based-Image-Restoration-under-General-Illumination" class="headerlink" title="Recurrent Spike-based Image Restoration under General Illumination"></a>Recurrent Spike-based Image Restoration under General Illumination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03018">http://arxiv.org/abs/2308.03018</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bit-vision/rsir">https://github.com/bit-vision/rsir</a></li>
<li>paper_authors: Lin Zhu, Yunlong Zheng, Mengyue Geng, Lizhi Wang, Hua Huang</li>
<li>for: 提高雨天或晚上场景下的激活图像重建速度</li>
<li>methods: 基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块</li>
<li>results: 对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像Here’s a breakdown of each line:</li>
<li>for: 这篇论文的目的是提高雨天或晚上场景下的激活图像重建速度。</li>
<li>methods: 这篇论文使用了基于物理模型建立噪声分布，并设计了适应性激光变换模块、回归时间特征融合模块和频率域激光噪声除净模块来实现图像重建。</li>
<li>results: 这篇论文对实际 dataset进行了广泛的实验，并在不同照明条件下达到了效果的重建图像。<details>
<summary>Abstract</summary>
Spike camera is a new type of bio-inspired vision sensor that records light intensity in the form of a spike array with high temporal resolution (20,000 Hz). This new paradigm of vision sensor offers significant advantages for many vision tasks such as high speed image reconstruction. However, existing spike-based approaches typically assume that the scenes are with sufficient light intensity, which is usually unavailable in many real-world scenarios such as rainy days or dusk scenes. To unlock more spike-based application scenarios, we propose a Recurrent Spike-based Image Restoration (RSIR) network, which is the first work towards restoring clear images from spike arrays under general illumination. Specifically, to accurately describe the noise distribution under different illuminations, we build a physical-based spike noise model according to the sampling process of the spike camera. Based on the noise model, we design our RSIR network which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network. The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details>
<details>
<summary>摘要</summary>
新型生物启发式视觉传感器“穿孔相机”记录了光Intensity的形式为高时间分辨率（20,000 Hz）的脉冲数组。这种新的视觉传感器模式具有许多视觉任务的优势，如高速图像重建。然而，现有的脉冲基本approaches通常假设场景中有足够的光INTENSITY，这通常不存在在真实世界中的雨天或晚上场景。为了解锁更多的脉冲基本应用场景，我们提议了一种基于脉冲的图像修复网络（RSIR），这是首次对脉冲数组进行图像修复。 Specifically, we build a physical-based spike noise model according to the sampling process of the spike camera to accurately describe the noise distribution under different illuminations. Based on the noise model, we design our RSIR network, which consists of an adaptive spike transformation module, a recurrent temporal feature fusion module, and a frequency-based spike denoising module. Our RSIR can process the spike array in a recursive manner to ensure that the spike temporal information is well utilized. In the training process, we generate the simulated spike data based on our noise model to train our network. Extensive experiments on real-world datasets with different illuminations demonstrate the effectiveness of the proposed network.  The code and dataset are released at https://github.com/BIT-Vision/RSIR.
</details></li>
</ul>
<hr>
<h2 id="High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage"><a href="#High-Resolution-Vision-Transformers-for-Pixel-Level-Identification-of-Structural-Components-and-Damage" class="headerlink" title="High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage"></a>High-Resolution Vision Transformers for Pixel-Level Identification of Structural Components and Damage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03006">http://arxiv.org/abs/2308.03006</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kareem Eltouny, Seyedomid Sajedi, Xiao Liang</li>
<li>for: 这个研究是为了提高桥梁检查的效率和可靠性，使用无人机和人工智能技术来快速和安全地进行视觉检查。</li>
<li>methods: 该研究使用了基于视Transformer和 Laplacian pyramids scaling networks的semantic segmentation网络，来高效地分析高分辨率的视觉检查图像。</li>
<li>results: 研究结果表明，该方法可以高效地分析大量的高分辨率视觉检查图像，同时保持了地方细节和全局 semantics信息，不会对计算效率造成影响。<details>
<summary>Abstract</summary>
Visual inspection is predominantly used to evaluate the state of civil structures, but recent developments in unmanned aerial vehicles (UAVs) and artificial intelligence have increased the speed, safety, and reliability of the inspection process. In this study, we develop a semantic segmentation network based on vision transformers and Laplacian pyramids scaling networks for efficiently parsing high-resolution visual inspection images. The massive amounts of collected high-resolution images during inspections can slow down the investigation efforts. And while there have been extensive studies dedicated to the use of deep learning models for damage segmentation, processing high-resolution visual data can pose major computational difficulties. Traditionally, images are either uniformly downsampled or partitioned to cope with computational demands. However, the input is at risk of losing local fine details, such as thin cracks, or global contextual information. Inspired by super-resolution architectures, our vision transformer model learns to resize high-resolution images and masks to retain both the valuable local features and the global semantics without sacrificing computational efficiency. The proposed framework has been evaluated through comprehensive experiments on a dataset of bridge inspection report images using multiple metrics for pixel-wise materials detection.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用视觉检查来评估公共建筑结构，但是最新的无人机（UAV）和人工智能技术已经提高了检查过程的速度、安全性和可靠性。在这项研究中，我们开发了基于视transformer和Laplacian pyramids scaling网络的semantic segmentation网络，用于高效地分解高分辨率视检图像。收集的大量高分辨率图像可能会拖slow down调查工作，而且过去对深度学习模型用于损害分 segmentation的研究非常广泛。但是处理高分辨率视数据可以带来巨大的计算困难。传统上，图像会被uniform downsample或分割，以降低计算成本，但是输入可能会产生本地细小损害，例如细裂，或者全局Contextual信息。受到超分辨architecture的启发，我们的视transformer模型学习了resize高分辨率图像和mask，以保留valuable的本地特征和全局semantics，不会 sacrificing计算效率。我们提出的框架已经在bridge检查报告图像集上进行了完整的实验，并使用多个 метри来进行像素精度检测。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet"><a href="#Weakly-supervised-segmentation-of-intracranial-aneurysms-using-a-3D-focal-modulation-UNet" class="headerlink" title="Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet"></a>Weakly supervised segmentation of intracranial aneurysms using a 3D focal modulation UNet</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03001">http://arxiv.org/abs/2308.03001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Rasoulian, Soorena Salari, Yiming Xiao<br>for: 这 paper 的目的是提供一种高精度的自动化UIA诊断方法，以便改善Current assessment based on 2D manual measures of aneurysms on 3D MRA的评估方法。methods: 该 paper 使用了一种名为 FocalSegNet 的新型3D焦点调制UNet，以及一种名为 CRF 的后处理技术，来实现高精度的 UIA 分 segmentation。results: 该 paper 的实验结果表明，提案的算法比 state-of-the-art 3D UNet 和 Swin-UNETR 更高精度，并且 demonstarted the superiority of the proposed FocalSegNet 和 focal modulation 的 beneficial effect on the task。<details>
<summary>Abstract</summary>
Accurate identification and quantification of unruptured intracranial aneurysms (UIAs) are essential for the risk assessment and treatment decisions of this cerebrovascular disorder. Current assessment based on 2D manual measures of aneurysms on 3D magnetic resonance angiography (MRA) is sub-optimal and time-consuming. Automatic 3D measures can significantly benefit the clinical workflow and treatment outcomes. However, one major issue in medical image segmentation is the need for large well-annotated data, which can be expensive to obtain. Techniques that mitigate the requirement, such as weakly supervised learning with coarse labels are highly desirable. In this paper, we leverage coarse labels of UIAs from time-of-flight MRAs to obtain refined UIAs segmentation using a novel 3D focal modulation UNet, called FocalSegNet and conditional random field (CRF) postprocessing, with a Dice score of 0.68 and 95% Hausdorff distance of 0.95 mm. We evaluated the performance of the proposed algorithms against the state-of-the-art 3D UNet and Swin-UNETR, and demonstrated the superiority of the proposed FocalSegNet and the benefit of focal modulation for the task.
</details>
<details>
<summary>摘要</summary>
correctly 识别和量化脑血管疾病（UIAs）的精度是诊断和治疗决策的关键。现有的评估方法基于2D手动测量的感知器件图像（MRA）是下pecific和耗时consuming。自动化3D测量可以帮助优化诊断和治疗结果。然而，医疗图像分割的一个主要问题是需要大量的良好标注数据，这可以是expensive to obtain。使用弱有supervised learning with coarse labels可以减少这个问题。在这篇论文中，我们利用时间飞行扫描产生的UIAs粗略标签来获得改进的UIAs分割，使用一种新的3D焦点修饰UNet，called FocalSegNet，并与条件Random field（CRF）后处理，得到了0.68的Dice分数和0.95 mm的95% Hausdorff距离。我们评估了提议的算法与现有的3D UNet和Swin-UNETR的性能，并证明了提议的FocalSegNet的优越性和焦点修饰的好处。
</details></li>
</ul>
<hr>
<h2 id="DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation"><a href="#DermoSegDiff-A-Boundary-aware-Segmentation-Diffusion-Model-for-Skin-Lesion-Delineation" class="headerlink" title="DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation"></a>DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02959">http://arxiv.org/abs/2308.02959</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mindflow-institue/dermosegdiff">https://github.com/mindflow-institue/dermosegdiff</a></li>
<li>paper_authors: Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof</li>
<li>for: 静脉皮肤病诊断 early detection 和准确诊断</li>
<li>methods: 利用 Diffusion Probabilistic Models (DDPMs) 进行静脉皮肤病诊断, 并在学习过程中加入边缘信息</li>
<li>results: 对多个皮肤分割数据集进行实验，显示 DermoSegDiff 的效果和泛化能力都较为出色，超过了现有的 CNN、transformer 和 diffusion-based 方法<details>
<summary>Abstract</summary>
Skin lesion segmentation plays a critical role in the early detection and accurate diagnosis of dermatological conditions. Denoising Diffusion Probabilistic Models (DDPMs) have recently gained attention for their exceptional image-generation capabilities. Building on these advancements, we propose DermoSegDiff, a novel framework for skin lesion segmentation that incorporates boundary information during the learning process. Our approach introduces a novel loss function that prioritizes the boundaries during training, gradually reducing the significance of other regions. We also introduce a novel U-Net-based denoising network that proficiently integrates noise and semantic information inside the network. Experimental results on multiple skin segmentation datasets demonstrate the superiority of DermoSegDiff over existing CNN, transformer, and diffusion-based approaches, showcasing its effectiveness and generalization in various scenarios. The implementation is publicly accessible on \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub}
</details>
<details>
<summary>摘要</summary>
皮肤损害分割在诊断皮肤疾病的早期阶段发挥了关键作用。近年来，Diffusion Probabilistic Models（DDPMs）在图像生成方面受到了广泛关注。我们基于这些进步，提出了DermoSegDiff，一种新的皮肤损害分割框架。我们的方法引入了一个新的损失函数，在训练过程中优先级化边界信息，逐渐减少其他区域的重要性。我们还引入了一种基于U-Net的杂音级别网络，可以高效地 интеGRATE噪音和semantic信息内网络。多个皮肤分割数据集的实验结果表明，DermoSegDiff在不同的场景下对现有的CNN、transformer和Diffusion-based方法优于，demonstrating its effectiveness and generalization。实现可以在 \href{https://github.com/mindflow-institue/dermosegdiff}{GitHub} 上获取。
</details></li>
</ul>
<hr>
<h2 id="MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction"><a href="#MomentaMorph-Unsupervised-Spatial-Temporal-Registration-with-Momenta-Shooting-and-Correction" class="headerlink" title="MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction"></a>MomentaMorph: Unsupervised Spatial-Temporal Registration with Momenta, Shooting, and Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02949">http://arxiv.org/abs/2308.02949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhangxing Bian, Shuwen Wei, Yihao Liu, Junyu Chen, Jiachen Zhuo, Fangxu Xing, Jonghye Woo, Aaron Carass, Jerry L. Prince</li>
<li>for: 用于估计大量运动和复杂征 patrerns 中的� MR 影像中的运动</li>
<li>methods: 基于 Lie 代数和 Lie 组 principls 的 “势量、射击、修正” 框架，以快速尝试到真正的 optima，并确保收敛到真正的 optima</li>
<li>results: 在 synthetic 数据集和实际 3D tMRI 数据集上，方法能够高效地估计精准、密集、 diffeomorphic 2D&#x2F;3D 运动场I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Tagged magnetic resonance imaging (tMRI) has been employed for decades to measure the motion of tissue undergoing deformation. However, registration-based motion estimation from tMRI is difficult due to the periodic patterns in these images, particularly when the motion is large. With a larger motion the registration approach gets trapped in a local optima, leading to motion estimation errors. We introduce a novel "momenta, shooting, and correction" framework for Lagrangian motion estimation in the presence of repetitive patterns and large motion. This framework, grounded in Lie algebra and Lie group principles, accumulates momenta in the tangent vector space and employs exponential mapping in the diffeomorphic space for rapid approximation towards true optima, circumventing local optima. A subsequent correction step ensures convergence to true optima. The results on a 2D synthetic dataset and a real 3D tMRI dataset demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details>
<details>
<summary>摘要</summary>
带标记的核磁共振成像（tMRI）已经在数十年中用于测量软组织中的运动。然而，基于匹配的运动估计从tMRI中很难进行注册，尤其是当运动较大时。当运动较大时，匹配方法会被困在地方最优点中，导致运动估计错误。我们介绍了一种新的“动量、射击和修正”框架，用于在具有循环Patterns和大运动时进行劳动动量估计。这个框架基于李 álgebra和李群原理，在 Tangent vector space中积累动量，使用 экспоненциаль映射在 diffeomorphic 空间中快速 Approximate towards true optima， circumventing local optima。后续的修正步骤确保了 converges to true optima。Synthetic dataset和实际的3D tMRI dataset results demonstrate our method's efficiency in estimating accurate, dense, and diffeomorphic 2D/3D motion fields amidst large motion and repetitive patterns.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/06/eess.IV_2023_08_06/" data-id="cllsk9gs7009b9c88h0qx216e" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.LG_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.LG_2023_08_05/">cs.LG - 2023-08-05 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Edge-of-stability-echo-state-networks"><a href="#Edge-of-stability-echo-state-networks" class="headerlink" title="Edge of stability echo state networks"></a>Edge of stability echo state networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02902">http://arxiv.org/abs/2308.02902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrea Ceni, Claudio Gallicchio</li>
<li>for: 这个论文提出了一种新的储存计算（Reservoir Computing，RC）架构，称为Edge of Stability Echo State Network（ES$^2$N）。</li>
<li>methods: 该模型基于定义储存层为非线性储存（如标准ESN）和线性储存（实现正交变换）的concat。 我们提供了整个数学分析，证明ES2N map的Jacobian的全谱特征在一个可控的圆锥形范围内，并利用这个性质来证明ES$^2$N的前向动力系统在设计的边缘混乱 режиobe动。</li>
<li>results: 我们的实验分析显示，新引入的储存模型可以达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N具有更好的记忆和非线性之间的融合，以及在推理非线性模型方面的显著改进。<details>
<summary>Abstract</summary>
In this paper, we propose a new Reservoir Computing (RC) architecture, called the Edge of Stability Echo State Network (ES$^2$N). The introduced ES$^2$N model is based on defining the reservoir layer as a convex combination of a nonlinear reservoir (as in the standard ESN), and a linear reservoir that implements an orthogonal transformation. We provide a thorough mathematical analysis of the introduced model, proving that the whole eigenspectrum of the Jacobian of the ES2N map can be contained in an annular neighbourhood of a complex circle of controllable radius, and exploit this property to demonstrate that the ES$^2$N's forward dynamics evolves close to the edge-of-chaos regime by design. Remarkably, our experimental analysis shows that the newly introduced reservoir model is able to reach the theoretical maximum short-term memory capacity. At the same time, in comparison to standard ESN, ES$^2$N is shown to offer a favorable trade-off between memory and nonlinearity, as well as a significant improvement of performance in autoregressive nonlinear modeling.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的抽象计算（Reservoir Computing，RC）架构，称为边缘稳定声纳网络（ES$^2$N）。我们的ES$^2$N模型基于定义储存层为非线性储存（如标准ESN）和线性储存实现正交变换的混合体。我们对引入的模型进行了深入的数学分析，证明整个特征值谱可以在控制的圆盘内含义，并利用这个性质来证明ES$^2$N的前向动力系统在设计上靠近边缘混乱 режи宜。在实验中，我们发现新引入的储存模型能够达到理论上的最大短期记忆容量。同时，相比标准ESN，ES$^2$N表现出了更好的记忆与非线性之间的质量协议，以及在抽象非线性模型中显著的性能改善。
</details></li>
</ul>
<hr>
<h2 id="Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach"><a href="#Textual-Data-Mining-for-Financial-Fraud-Detection-A-Deep-Learning-Approach" class="headerlink" title="Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach"></a>Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03800">http://arxiv.org/abs/2308.03800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiuru Li</li>
<li>for: 这个研究旨在使用深度学习方法进行自然语言处理（NLP）Binary分类任务，以分析金融诈骗文本。</li>
<li>methods: 我使用了多种神经网络模型，包括多层权重层、vanilla RNN、LSTM和GRU来进行文本分类任务。</li>
<li>results: 我的结果表明，使用这些多种神经网络模型可以提高金融诈骗检测的准确率，这些结果对于金融诈骗检测有着重要的意义，并为业界实践者、监管机构和研究人员提供有价值的洞察。<details>
<summary>Abstract</summary>
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuable insights for industry practitioners, regulators, and researchers in the pursuit of more robust and effective fraud detection methodologies.
</details>
<details>
<summary>摘要</summary>
在这份报告中，我使用深度学习方法来进行自然语言处理（以下简称 NLP）的二分类任务，以分析金融诈骗文本。首先，我从香港证券交易所新闻中搜索了规范公告和执行通知，以定义诈骗公司并提取其财务报告。然后，我将报告中的句子分为标签和时间排序。我的方法包括多层感知器、普通逻辑神经网络、长短期记忆网络（LSTM）和闭合逻辑Unit（GRU）等不同类型的神经网络模型，用于文本分类任务。通过利用这些多样化的模型，我希望能够对金融诈骗检测的准确率进行全面的比较。我的结果对金融诈骗检测具有重要的意义，这项工作将加入深度学习、NLP和金融之间的交叉领域的研究中，为业内专业人士、监管部门和研究人员提供价值的发现。
</details></li>
</ul>
<hr>
<h2 id="Elucidate-Gender-Fairness-in-Singing-Voice-Transcription"><a href="#Elucidate-Gender-Fairness-in-Singing-Voice-Transcription" class="headerlink" title="Elucidate Gender Fairness in Singing Voice Transcription"></a>Elucidate Gender Fairness in Singing Voice Transcription</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02898">http://arxiv.org/abs/2308.02898</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guxm2021/svt_speechbrain">https://github.com/guxm2021/svt_speechbrain</a></li>
<li>paper_authors: Xiangming Gu, Wei Zeng, Ye Wang</li>
<li>for:  investigate the performance disparity in singing voice transcription (SVT) between males and females, and propose a method to address the gender bias.</li>
<li>methods: use an attribute predictor to predict gender labels and adversarially train the SVT system to enforce the gender-invariance of acoustic representations, conditionally align acoustic representations between demographic groups by feeding note events to the attribute predictor.</li>
<li>results: significant reduction of gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, offering a better fairness-utility trade-off.<details>
<summary>Abstract</summary>
It is widely known that males and females typically possess different sound characteristics when singing, such as timbre and pitch, but it has never been explored whether these gender-based characteristics lead to a performance disparity in singing voice transcription (SVT), whose target includes pitch. Such a disparity could cause fairness issues and severely affect the user experience of downstream SVT applications. Motivated by this, we first demonstrate the female superiority of SVT systems, which is observed across different models and datasets. We find that different pitch distributions, rather than gender data imbalance, contribute to this disparity. To address this issue, we propose using an attribute predictor to predict gender labels and adversarially training the SVT system to enforce the gender-invariance of acoustic representations. Leveraging the prior knowledge that pitch distributions may contribute to the gender bias, we propose conditionally aligning acoustic representations between demographic groups by feeding note events to the attribute predictor. Empirical experiments on multiple benchmark SVT datasets show that our method significantly reduces gender bias (up to more than 50%) with negligible degradation of overall SVT performance, on both in-domain and out-of-domain singing data, thus offering a better fairness-utility trade-off.
</details>
<details>
<summary>摘要</summary>
广泛知道，男女在唱歌时通常具有不同的音色特征，如音 timbre 和音高，但这些 gender-based 特征是否会导致唱歌voice transcription（SVT）中的性别偏袋问题？如果存在这种偏袋问题，那么这将导致 fairness 问题并且严重地影响下游 SVT 应用程序的用户体验。为了解决这个问题，我们首先示出了女性 SVT 系统的优势，这种优势可以在不同的模型和数据集上被观察到。我们发现，不同的投射分布，而不是性别数据不均衡，是导致这种偏袋问题的主要原因。为了解决这个问题，我们提议使用一个 attribute predictor 来预测性别标签，并在 SVT 系统中进行对 gender-invariance 的 adversarial 训练。利用投射分布可能会导致性别偏袋的知识，我们提议通过将 note events 传递给 attribute predictor，来Conditional 地将音频表示同步。我们的方法在多个标准 SVT 数据集上进行了实验，结果显示，我们的方法可以减少性别偏袋（达到50%以上），同时不会影响 SVT 总性能，从而提供了更好的 fairness-utility 交易。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements"><a href="#Physics-informed-Gaussian-process-model-for-Euler-Bernoulli-beam-elements" class="headerlink" title="Physics-informed Gaussian process model for Euler-Bernoulli beam elements"></a>Physics-informed Gaussian process model for Euler-Bernoulli beam elements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02894">http://arxiv.org/abs/2308.02894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gledson Rodrigo Tondo, Sebastian Rau, Igor Kavrakov, Guido Morgenthal</li>
<li>for: 这个论文是用于开发一种基于物理学习的、多输出泛化过程的机器学习模型，用于估计结构的弯矩稳定性。</li>
<li>methods: 这个模型使用了欧拉-伯涅瓦梁式方程，并通过适当的数据集来训练。</li>
<li>results: 模型可以用来描述结构的弯矩稳定性，进行 interpolate 和 probabilistic 推断，并在结构健康监测中使用 Mahalanobis 距离来评估结构系统中可能的损害的位置和范围。<details>
<summary>Abstract</summary>
A physics-informed machine learning model, in the form of a multi-output Gaussian process, is formulated using the Euler-Bernoulli beam equation. Given appropriate datasets, the model can be used to regress the analytical value of the structure's bending stiffness, interpolate responses, and make probabilistic inferences on latent physical quantities. The developed model is applied on a numerically simulated cantilever beam, where the regressed bending stiffness is evaluated and the influence measurement noise on the prediction quality is investigated. Further, the regressed probabilistic stiffness distribution is used in a structural health monitoring context, where the Mahalanobis distance is employed to reason about the possible location and extent of damage in the structural system. To validate the developed framework, an experiment is conducted and measured heterogeneous datasets are used to update the assumed analytical structural model.
</details>
<details>
<summary>摘要</summary>
一种physics-informed机器学习模型，具体来说是一种多输出 Gaussian process，基于Euler-Bernoulli梁式方程。给定合适的数据集，该模型可以用来回归分析结构的弯矩稳定性， interpolate 响应，以及进行 probabilistic 推断 Physical quantity 的存在。该模型在 numerically simulated  cantilever beam 上进行应用，其中推断的弯矩稳定性被评估，并investigate 测量噪声对预测质量的影响。此外，推断的 probabilistic 弯矩分布被用于结构健康监测上，通过 Mahalanobis distance 来了解结构系统中可能的损害位置和范围。为验证开发的框架，进行了实验，并使用测量的 hetereogeneous 数据集来更新假设的分析结构模型。
</details></li>
</ul>
<hr>
<h2 id="Secure-Deep-JSCC-Against-Multiple-Eavesdroppers"><a href="#Secure-Deep-JSCC-Against-Multiple-Eavesdroppers" class="headerlink" title="Secure Deep-JSCC Against Multiple Eavesdroppers"></a>Secure Deep-JSCC Against Multiple Eavesdroppers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02892">http://arxiv.org/abs/2308.02892</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyyed Amirhossein Ameli Kalkhoran, Mehdi Letafati, Ecenaz Erdemir, Babak Hossein Khalaj, Hamid Behroozi, Deniz Gündüz</li>
<li>for: 这个研究旨在提出一个基于深度学习的综合式通信安全方法，以保护传输过程中的私人资讯免受多名听者的窃取和探索。</li>
<li>methods: 这个方法使用深度学习的综合式模型，实现了一个数据驱动的安全通信方案，不需要对数据分布进行假设。</li>
<li>results: 实验结果显示，这个方法可以降低听者的攻击精度，对于不同的测试渠道（Rayleigh fading、Nakagami-m、AWGN）进行了评估。<details>
<summary>Abstract</summary>
In this paper, a generalization of deep learning-aided joint source channel coding (Deep-JSCC) approach to secure communications is studied. We propose an end-to-end (E2E) learning-based approach for secure communication against multiple eavesdroppers over complex-valued fading channels. Both scenarios of colluding and non-colluding eavesdroppers are studied. For the colluding strategy, eavesdroppers share their logits to collaboratively infer private attributes based on ensemble learning method, while for the non-colluding setup they act alone. The goal is to prevent eavesdroppers from inferring private (sensitive) information about the transmitted images, while delivering the images to a legitimate receiver with minimum distortion. By generalizing the ideas of privacy funnel and wiretap channel coding, the trade-off between the image recovery at the legitimate node and the information leakage to the eavesdroppers is characterized. To solve this secrecy funnel framework, we implement deep neural networks (DNNs) to realize a data-driven secure communication scheme, without relying on a specific data distribution. Simulations over CIFAR-10 dataset verifies the secrecy-utility trade-off. Adversarial accuracy of eavesdroppers are also studied over Rayleigh fading, Nakagami-m, and AWGN channels to verify the generalization of the proposed scheme. Our experiments show that employing the proposed secure neural encoding can decrease the adversarial accuracy by 28%.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了深度学习帮助的同时源渠道编码（Deep-JSCC）方法的扩展，以确保通信的安全性。我们提议一种终端到终端（E2E）学习基于的安全通信方法，用于对多个伪装者进行安全通信。我们研究了协作和不协作的情况下的伪装者。在协作情况下，伪装者共享其логиits来共同推理私有特征，而在不协作情况下，他们 acted alone。我们的目标是防止伪装者推理传输的图像中的私有（敏感）信息，同时将图像传输到合法接收器，并最小化干扰。通过扩展隐私管道和窃听渠道编码的想法，我们研究了图像恢复和伪装者信息泄露之间的质量负担。为解决这个隐私管道框架，我们使用深度神经网络（DNNs）来实现数据驱动的安全通信方案，不需要固定数据分布。我们的实验结果表明，通过使用我们的安全神经编码，可以降低伪装者的攻击精度，减少了28%。我们还对伪装者的攻击精度进行了随机抽样和AWGN渠道的研究，以验证我们的方案的普适性。
</details></li>
</ul>
<hr>
<h2 id="Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation"><a href="#Private-Federated-Learning-with-Dynamic-Power-Control-via-Non-Coherent-Over-the-Air-Computation" class="headerlink" title="Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation"></a>Private Federated Learning with Dynamic Power Control via Non-Coherent Over-the-Air Computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02881">http://arxiv.org/abs/2308.02881</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anbang Zhang, Shuaishuai Guo, Shuai Liu</li>
<li>for: 提高 Federated Learning（FL）中模型保护和性能提高的方法</li>
<li>methods: 基于动态功率控制的Over-the-Air Computation（AirComp）方案</li>
<li>results: 可以 Mitigate 时间同步错误、通道抑降和噪声的影响，并提供了理论上的整合证明。<details>
<summary>Abstract</summary>
To further preserve model weight privacy and improve model performance in Federated Learning (FL), FL via Over-the-Air Computation (AirComp) scheme based on dynamic power control is proposed. The edge devices (EDs) transmit the signs of local stochastic gradients by activating two adjacent orthogonal frequency division multi-plexing (OFDM) subcarriers, and majority votes (MVs) at the edge server (ES) are obtained by exploiting the energy accumulation on the subcarriers. Then, we propose a dynamic power control algorithm to further offset the biased aggregation of the MV aggregation values. We show that the whole scheme can mitigate the impact of the time synchronization error, channel fading and noise. The theoretical convergence proof of the scheme is re-derived.
</details>
<details>
<summary>摘要</summary>
为了进一步保护模型权重私钥和改进 Federated Learning（FL）的性能，我们提出了基于动态功率控制的 Federated Learning via Over-the-Air Computation（AirComp）方案。 Edge devices（ED）通过活动两个邻近的正交频分多普逊（OFDM）子频，将本地随机梯度签名发送到 Edge server（ES），然后通过利用频分的能量积累实现多数投票（MV）。然后，我们提出了一种动态功率控制算法，以更正偏好的MV汇聚值的偏好。我们证明了整个方案可以减轻时间同步错误、通道抑降和噪声的影响。我们重新证明了方案的理论收敛证明。
</details></li>
</ul>
<hr>
<h2 id="Meta-learning-in-healthcare-A-survey"><a href="#Meta-learning-in-healthcare-A-survey" class="headerlink" title="Meta-learning in healthcare: A survey"></a>Meta-learning in healthcare: A survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02877">http://arxiv.org/abs/2308.02877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alireza Rafiei, Ronald Moore, Sina Jahromi, Farshid Hajati, Rishikesan Kamaleswaran</li>
<li>for: This paper aims to explore the applications of meta-learning in the healthcare domain and provide insights into how it can address critical healthcare challenges.</li>
<li>methods: The paper discusses the theoretical foundations and pivotal methods of meta-learning, including multi&#x2F;single-task learning and many&#x2F;few-shot learning.</li>
<li>results: The paper surveys various studies that have applied meta-learning in the healthcare domain and highlights the current challenges in meta-learning research, as well as potential solutions and future perspectives.<details>
<summary>Abstract</summary>
As a subset of machine learning, meta-learning, or learning to learn, aims at improving the model's capabilities by employing prior knowledge and experience. A meta-learning paradigm can appropriately tackle the conventional challenges of traditional learning approaches, such as insufficient number of samples, domain shifts, and generalization. These unique characteristics position meta-learning as a suitable choice for developing influential solutions in various healthcare contexts, where the available data is often insufficient, and the data collection methodologies are different. This survey discusses meta-learning broad applications in the healthcare domain to provide insight into how and where it can address critical healthcare challenges. We first describe the theoretical foundations and pivotal methods of meta-learning. We then divide the employed meta-learning approaches in the healthcare domain into two main categories of multi/single-task learning and many/few-shot learning and survey the studies. Finally, we highlight the current challenges in meta-learning research, discuss the potential solutions and provide future perspectives on meta-learning in healthcare.
</details>
<details>
<summary>摘要</summary>
为一种机器学习子领域，meta-learning，或学习学习，旨在提高模型的能力，通过使用先前知识和经验。meta-learning概念可以有效地解决传统学习方法的常见挑战，如样本不够、领域变化和泛化。这些特点使得meta-learning成为在医疗领域开发影响力强的解决方案的适用场景。本文首先介绍了meta-learning的理论基础和关键方法，然后将在医疗领域使用的meta-learning方法分为两个主要类别：多/单任务学习和多/少射学习，并对相关研究进行概述。最后，我们描述了当前meta-learning研究中的挑战，讨论了可能的解决方案，并提供了未来meta-learning在医疗领域的前景。
</details></li>
</ul>
<hr>
<h2 id="Data-Based-Design-of-Multi-Model-Inferential-Sensors"><a href="#Data-Based-Design-of-Multi-Model-Inferential-Sensors" class="headerlink" title="Data-Based Design of Multi-Model Inferential Sensors"></a>Data-Based Design of Multi-Model Inferential Sensors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02872">http://arxiv.org/abs/2308.02872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Martin Mojto, Karol Lubušký, Miroslav Fikar, Radoslav Paulen</li>
<li>for: 这篇论文关注软感知设计问题，旨在提高软感知仪器的预测性能。</li>
<li>methods: 本文提出了两种新的软感知设计方法，以提高软感知仪器的预测性能，并且可以维护其线性结构。</li>
<li>results: 对于一个实际的燃气油氢化单元，比较了多种单一模型软感知仪器和当前 referential 软感知仪器，结果表明了新方法的显著提高。<details>
<summary>Abstract</summary>
This paper deals with the problem of inferential (soft) sensor design. The nonlinear character of industrial processes is usually the main limitation to designing simple linear inferential sensors with sufficient accuracy. In order to increase the inferential sensor predictive performance and yet to maintain its linear structure, multi-model inferential sensors represent a straightforward option. In this contribution, we propose two novel approaches for the design of multi-model inferential sensors aiming to mitigate some drawbacks of the state-of-the-art approaches. For a demonstration of the developed techniques, we design inferential sensors for a Vacuum Gasoil Hydrogenation unit, which is a real-world petrochemical refinery unit. The performance of the multi-model inferential sensor is compared against various single-model inferential sensors and the current (referential) inferential sensor used in the refinery. The results show substantial improvements over the state-of-the-art design techniques for single-/multi-model inferential sensors.
</details>
<details>
<summary>摘要</summary>
In this study, we propose two new methods for designing multi-model inferential sensors. We demonstrate the effectiveness of these methods using a real-world petrochemical refinery unit, the vacuum gasoil hydrogenation unit. The performance of the multi-model inferential sensor is compared to various single-model inferential sensors and the current referential inferential sensor used in the refinery. The results show significant improvements over existing design techniques for single-/multi-model inferential sensors.The key contributions of this paper are:1. Two novel approaches for designing multi-model inferential sensors that mitigate some drawbacks of state-of-the-art techniques.2. A demonstration of the effectiveness of the proposed methods using a real-world petrochemical refinery unit.3. Comparison of the performance of the multi-model inferential sensor with various single-model inferential sensors and the current referential inferential sensor used in the refinery.The rest of the paper is organized as follows: Section 2 reviews the related work on soft sensors and multi-model inferential sensors. Section 3 describes the proposed methods for designing multi-model inferential sensors. Section 4 presents the case study of the vacuum gasoil hydrogenation unit. Section 5 compares the performance of the multi-model inferential sensor with other approaches. Finally, Section 6 concludes the paper and highlights future research directions.
</details></li>
</ul>
<hr>
<h2 id="NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation"><a href="#NP-SemiSeg-When-Neural-Processes-meet-Semi-Supervised-Semantic-Segmentation" class="headerlink" title="NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation"></a>NP-SemiSeg: When Neural Processes meet Semi-Supervised Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02866">http://arxiv.org/abs/2308.02866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jianf-wang/np-semiseg">https://github.com/jianf-wang/np-semiseg</a></li>
<li>paper_authors: Jianfeng Wang, Daniela Massiceti, Xiaolin Hu, Vladimir Pavlovic, Thomas Lukasiewicz</li>
<li>for: 这种方法用于 assigning pixel-wise labels to unlabeled images at training time, 有助于减少标注成本和提高 segmentation 的准确率。</li>
<li>methods: 该方法使用 neural processes (NPs) for uncertainty quantification, and adapts NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg.</li>
<li>results: 实验结果表明，NP-SemiSeg 在 PASCAL VOC 2012 和 Cityscapes 上的公共benchmark上，在不同的训练设置下，具有显著的效果。<details>
<summary>Abstract</summary>
Semi-supervised semantic segmentation involves assigning pixel-wise labels to unlabeled images at training time. This is useful in a wide range of real-world applications where collecting pixel-wise labels is not feasible in time or cost. Current approaches to semi-supervised semantic segmentation work by predicting pseudo-labels for each pixel from a class-wise probability distribution output by a model. If the predicted probability distribution is incorrect, however, this leads to poor segmentation results, which can have knock-on consequences in safety critical systems, like medical images or self-driving cars. It is, therefore, important to understand what a model does not know, which is mainly achieved by uncertainty quantification. Recently, neural processes (NPs) have been explored in semi-supervised image classification, and they have been a computationally efficient and effective method for uncertainty quantification. In this work, we move one step forward by adapting NPs to semi-supervised semantic segmentation, resulting in a new model called NP-SemiSeg. We experimentally evaluated NP-SemiSeg on the public benchmarks PASCAL VOC 2012 and Cityscapes, with different training settings, and the results verify its effectiveness.
</details>
<details>
<summary>摘要</summary>
semi-supervised semantic segmentation是将不带标签的图像分类为不同类别的过程。这有很多实际应用场景，例如医疗图像或自动驾驶车辆，因为收集标签是不可能或者成本高昂。现有的方法是通过模型输出类别概率分布来预测每个像素的pseudo标签。如果预测结果不正确，则会导致 segmentation 结果差，这可能会对安全关键系统产生影响，例如医疗图像或自动驾驶车辆。因此，理解模型不知道的内容非常重要。最近，神经过程（NP）在半supervised图像分类中被探索，它们是一种计算效率高且有效的不确定量化方法。在这种工作中，我们将NP应用于半supervised semantic segmentation，得到了一种新的模型called NP-SemiSeg。我们对NP-SemiSeg进行了不同的训练设置，并在公共测试集PASCAL VOC 2012和Cityscapes上进行了实验，结果证明了它的有效性。
</details></li>
</ul>
<hr>
<h2 id="Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology"><a href="#Generative-Adversarial-Networks-for-Stain-Normalisation-in-Histopathology" class="headerlink" title="Generative Adversarial Networks for Stain Normalisation in Histopathology"></a>Generative Adversarial Networks for Stain Normalisation in Histopathology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02851">http://arxiv.org/abs/2308.02851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jack Breen, Kieran Zucker, Katie Allen, Nishant Ravikumar, Nicolas M. Orsi</li>
<li>for: 提高临床诊断的准确率和效率，透过人工智能技术的发展。</li>
<li>methods: 主要是使用生成对抗网络（GANs）等技术进行染料标准化。</li>
<li>results: GAN-based methods typically outperform non-generative approaches, but the best method for stain normalization is still an ongoing field of study and depends on the specific scenario and performance metrics.<details>
<summary>Abstract</summary>
The rapid growth of digital pathology in recent years has provided an ideal opportunity for the development of artificial intelligence-based tools to improve the accuracy and efficiency of clinical diagnoses. One of the significant roadblocks to current research is the high level of visual variability across digital pathology images, causing models to generalise poorly to unseen data. Stain normalisation aims to standardise the visual profile of digital pathology images without changing the structural content of the images. In this chapter, we explore different techniques which have been used for stain normalisation in digital pathology, with a focus on approaches which utilise generative adversarial networks (GANs). Typically, GAN-based methods outperform non-generative approaches but at the cost of much greater computational requirements. However, it is not clear which method is best for stain normalisation in general, with different GAN and non-GAN approaches outperforming each other in different scenarios and according to different performance metrics. This is an ongoing field of study as researchers aim to identify a method which efficiently and effectively normalises pathology images to make AI models more robust and generalisable.
</details>
<details>
<summary>摘要</summary>
随着数字病理学的快速发展，提供了一个理想的机会，用于发展基于人工智能技术的工具，以提高临床诊断的准确性和效率。然而，一个 significante roadblock 是数字病理图像之间的视觉变化，导致模型很难泛化到未看过的数据。颜色标准化目的是标准化数字病理图像的视觉特征，而不改变图像的结构内容。在这章中，我们探讨了不同的技术，用于数字病理颜色标准化，特别是使用生成对抗网络（GAN）。通常，GAN基本方法在不同的场景下都会超越非生成方法，但是计算需求很高。然而，没有一个方法是最佳的，不同的 GAN 和非 GAN 方法在不同的场景和性能指标下都会出perform。这是一个持续的研究领域，研究人员希望能够找到一种能够有效地和效率地标准化病理图像的方法，以使AI模型更加可靠和泛化。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks"><a href="#Approximating-Positive-Homogeneous-Functions-with-Scale-Invariant-Neural-Networks" class="headerlink" title="Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks"></a>Approximating Positive Homogeneous Functions with Scale Invariant Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02836">http://arxiv.org/abs/2308.02836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Bamberger, Reinhard Heckel, Felix Krahmer</li>
<li>for:  investigated the possibility of solving linear inverse problems with $ReLu$ networks</li>
<li>methods:  used positive homogeneity and absence of bias terms in the network architecture</li>
<li>results:  showed that $ReLu$-networks with two hidden layers can achieve approximate recovery with arbitrary precision and sparsity level, and extended the results to a wider class of recovery problems and general positive homogeneous functions.<details>
<summary>Abstract</summary>
We investigate to what extent it is possible to solve linear inverse problems with $ReLu$ networks. Due to the scaling invariance arising from the linearity, an optimal reconstruction function $f$ for such a problem is positive homogeneous, i.e., satisfies $f(\lambda x) = \lambda f(x)$ for all non-negative $\lambda$. In a $ReLu$ network, this condition translates to considering networks without bias terms. We first consider recovery of sparse vectors from few linear measurements. We prove that $ReLu$- networks with only one hidden layer cannot even recover $1$-sparse vectors, not even approximately, and regardless of the width of the network. However, with two hidden layers, approximate recovery with arbitrary precision and arbitrary sparsity level $s$ is possible in a stable way. We then extend our results to a wider class of recovery problems including low-rank matrix recovery and phase retrieval. Furthermore, we also consider the approximation of general positive homogeneous functions with neural networks. Extending previous work, we establish new results explaining under which conditions such functions can be approximated with neural networks. Our results also shed some light on the seeming contradiction between previous works showing that neural networks for inverse problems typically have very large Lipschitz constants, but still perform very well also for adversarial noise. Namely, the error bounds in our expressivity results include a combination of a small constant term and a term that is linear in the noise level, indicating that robustness issues may occur only for very small noise levels.
</details>
<details>
<summary>摘要</summary>
我们研究可以使用 $ReLu$ 网络解决线性逆问题的可能性。由于线性的扩展对称性，一个好的复原函数 $f$ 的选择会是正Homogeneous，即满足 $f(\lambda x) = \lambda f(x)$ 的所有非负 $\lambda$。在 $ReLu$ 网络中，这个条件可以翻译为不包含偏好项。我们首先考虑从几个线性量测中回复簇节点。我们证明了 $ReLu$ 网络只有一个隐藏层时无法复原 $1$-簇节点，不管网络宽度如何。但是，具有两个隐藏层的 $ReLu$ 网络可以在稳定的方式下复原任意精度和簇节点数量 $s$。我们随后将结果扩展到更加广泛的复原问题，包括低维矩阵复原和相位回复。此外，我们还考虑了一般正Homogeneous函数的逼近，并建立了新的结果，说明在哪些情况下，这些函数可以透过神经网络逼近。我们的结果还照明了对于过去的研究所提出的对立之处，即神经网络 для反对数学问题通常具有非常大的Lipschitz常数，但是还是能够非常好地运行也在阶梯误差下。这意味着可能在非常小的误差水平下，发生了Robustness问题。
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-for-Financial-Index-Tracking"><a href="#Reinforcement-Learning-for-Financial-Index-Tracking" class="headerlink" title="Reinforcement Learning for Financial Index Tracking"></a>Reinforcement Learning for Financial Index Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02820">http://arxiv.org/abs/2308.02820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dppalomar/sparseindextracking">https://github.com/dppalomar/sparseindextracking</a></li>
<li>paper_authors: Xianhua Peng, Chenyin Gong, Xue Dong He</li>
<li>for: 本研究旨在提出一种精细时间架构的财务指数追踪问题解决方案，以满足返回基本追踪误差和价值基本追踪误差两种不同的追踪目标。</li>
<li>methods: 本研究使用了离散时间无限远景动态模型，并使用了Banach固定点迭代法解决端口重新平衡方程。此外，本研究还提出了一种基于深度强化学习（RL）方法的解决方案，以解决数据有限性问题。</li>
<li>results: 实验结果表明，提出的方法可以超过标准方法在追踪准确性和赚利率方面表现出色，并且可以通过具有策略的现金投入或抽取来实现额外的收益。<details>
<summary>Abstract</summary>
We propose the first discrete-time infinite-horizon dynamic formulation of the financial index tracking problem under both return-based tracking error and value-based tracking error. The formulation overcomes the limitations of existing models by incorporating the intertemporal dynamics of market information variables not limited to prices, allowing exact calculation of transaction costs, accounting for the tradeoff between overall tracking error and transaction costs, allowing effective use of data in a long time period, etc. The formulation also allows novel decision variables of cash injection or withdraw. We propose to solve the portfolio rebalancing equation using a Banach fixed point iteration, which allows to accurately calculate the transaction costs specified as nonlinear functions of trading volumes in practice. We propose an extension of deep reinforcement learning (RL) method to solve the dynamic formulation. Our RL method resolves the issue of data limitation resulting from the availability of a single sample path of financial data by a novel training scheme. A comprehensive empirical study based on a 17-year-long testing set demonstrates that the proposed method outperforms a benchmark method in terms of tracking accuracy and has the potential for earning extra profit through cash withdraw strategy.
</details>
<details>
<summary>摘要</summary>
我们提出了首个精细时间无限远景动态模型，用于跟踪金融指数问题，包括返点基本跟踪错误和价值基本跟踪错误。该模型超越了现有模型的局限性，因为它包含市场信息变量的时间动态，允许精确计算交易成本，考虑跟踪错误和交易成本之间的贸易做，使用长时间期间的数据，等等。该模型还允许新的决策变量：资金注入或撤回。我们提出使用巴нах固定点迭代法解决portfolio重新平衡方程，可以准确计算交易成本，实际上是非线性函数的交易量。我们还提出了RL方法的扩展，用于解决动态模型。我们的RL方法可以在单个财务数据样本路径上解决数据有限制的问题，通过一种新的训练方案。我们的实验表明，我们的方法在17年的测试集上表现出色，超过了参考方法的跟踪精度，并且有可能通过资金撤回策略获得额外利润。
</details></li>
</ul>
<hr>
<h2 id="A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting"><a href="#A-generative-model-for-surrogates-of-spatial-temporal-wildfire-nowcasting" class="headerlink" title="A generative model for surrogates of spatial-temporal wildfire nowcasting"></a>A generative model for surrogates of spatial-temporal wildfire nowcasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02810">http://arxiv.org/abs/2308.02810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sibo Cheng, Yike Guo, Rossella Arcucci</li>
<li>for: 预测野火发展 (predicting wildfire development)</li>
<li>methods: 使用三维vector-量化类似自适应器 (using three-dimensional Vector-Quantized Variational Autoencoders)</li>
<li>results: 成功生成了可见度和结构的野火燃烧区域 (successfully generated coherent and structured wildfire burned areas)<details>
<summary>Abstract</summary>
Recent increase in wildfires worldwide has led to the need for real-time fire nowcasting. Physics-driven models, such as cellular automata and computational fluid dynamics can provide high-fidelity fire spread simulations but they are computationally expensive and time-consuming. Much effort has been put into developing machine learning models for fire prediction. However, these models are often region-specific and require a substantial quantity of simulation data for training purpose. This results in a significant amount of computational effort for different ecoregions. In this work, a generative model is proposed using a three-dimensional Vector-Quantized Variational Autoencoders to generate spatial-temporal sequences of unseen wildfire burned areas in a given ecoregion. The model is tested in the ecoregion of a recent massive wildfire event in California, known as the Chimney fire. Numerical results show that the model succeed in generating coherent and structured fire scenarios, taking into account the impact from geophysical variables, such as vegetation and slope. Generated data are also used to train a surrogate model for predicting wildfire dissemination, which has been tested on both simulation data and the real Chimney fire event.
</details>
<details>
<summary>摘要</summary>
全球各地的野火增加的趋势，使得实时野火预测变得越来越重要。物理驱动的模型，如细胞自动机和计算流体力学，可以提供高精度的野火快速扩散模拟，但它们 Computationally expensive and time-consuming。大量的努力被投入到了机器学习模型的开发中，以预测野火。然而，这些模型通常是地域特定的，需要大量的模拟数据来训练。这会导致不同的生态区域需要大量的计算劳动。在这种情况下，本文提出了一种生成模型，使用三维向量量化自适应机制来生成未经见过的野火烧区Sequence。该模型在加利福尼亚州的一个大规模野火事件中进行了测试，称为奇尼火。numerical results show that the model successfully generated coherent and structured fire scenarios, taking into account the impact of geophysical variables such as vegetation and slope.Generated data were also used to train a surrogate model for predicting wildfire spread, which was tested on both simulation data and the real Chimney fire event.
</details></li>
</ul>
<hr>
<h2 id="MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method"><a href="#MiAMix-Enhancing-Image-Classification-through-a-Multi-stage-Augmented-Mixed-Sample-Data-Augmentation-Method" class="headerlink" title="MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method"></a>MiAMix: Enhancing Image Classification through a Multi-stage Augmented Mixed Sample Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02804">http://arxiv.org/abs/2308.02804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Liang, Youzhi Liang, Jianguo Jia</li>
<li>for: 提高深度学习模型的泛化性和性能，使其在多种计算机视觉任务中具有更好的普适性和稳定性。</li>
<li>methods: 提出了一种新的混合方法called MiAMix，它是基于混合框架的多阶段扩展混合方法，通过多种多样化的混合方法同时进行混合，并通过随机选择混合掩码的混合方法来提高混合方法。</li>
<li>results: 通过四个图像标准benchmark进行了全面的评估，并与当前状态的混合样本数据增强技术进行比较，demonstrated that MiAMix可以提高性能而无需增加计算负担。<details>
<summary>Abstract</summary>
Despite substantial progress in the field of deep learning, overfitting persists as a critical challenge, and data augmentation has emerged as a particularly promising approach due to its capacity to enhance model generalization in various computer vision tasks. While various strategies have been proposed, Mixed Sample Data Augmentation (MSDA) has shown great potential for enhancing model performance and generalization. We introduce a novel mixup method called MiAMix, which stands for Multi-stage Augmented Mixup. MiAMix integrates image augmentation into the mixup framework, utilizes multiple diversified mixing methods concurrently, and improves the mixing method by randomly selecting mixing mask augmentation methods. Recent methods utilize saliency information and the MiAMix is designed for computational efficiency as well, reducing additional overhead and offering easy integration into existing training pipelines. We comprehensively evaluate MiaMix using four image benchmarks and pitting it against current state-of-the-art mixed sample data augmentation techniques to demonstrate that MIAMix improves performance without heavy computational overhead.
</details>
<details>
<summary>摘要</summary>
尽管深度学习领域已经取得了重大进步，但过拟合仍然是一个 kritical 挑战，而数据扩充被认为是一种有效的方法来提高模型的通用性。在不同的策略中，混合样本数据扩充（MSDA）被认为是提高模型性能和通用性的有效方法。我们介绍了一种新的mixup方法，称为MiAMix，它是多stage混合的augmentation方法。MiAMix将图像扩充integrated into the mixup框架，并同时使用多种多样化的混合方法，通过随机选择混合面积的混合方法来提高混合方法。现有的方法使用了saliency信息，而MiAMix是为计算效率而设计的，减少了额外的负担和提供了与现有训练管道的集成。我们对MiaMix进行了四个图像标准 benchMark 的完整评估，并与当前状态的混合样本数据扩充技术进行了比较，以示MiAMix可以提高性能而不带重大计算负担。
</details></li>
</ul>
<hr>
<h2 id="OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI"><a href="#OBESEYE-Interpretable-Diet-Recommender-for-Obesity-Management-using-Machine-Learning-and-Explainable-AI" class="headerlink" title="OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI"></a>OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02796">http://arxiv.org/abs/2308.02796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mrinmoy Roy, Srabonti Das, Anica Tasnim Protity</li>
<li>For: The paper aims to provide a novel machine learning-based system to predict the amount of nutrients an individual requires for being healthy, with a focus on patients with comorbidities.* Methods: The paper uses various machine learning algorithms, including linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, and LightGBM, to predict fluid, carbohydrate, protein, and fat consumption.* Results: The paper achieves high accuracy with low root mean square error (RMSE) using linear regression in fluid prediction, random forest in carbohydrate prediction, and LightGBM in protein and fat prediction. The system, called OBESEYE, is the only one of its kind to consider comorbidities and physical conditions when recommending diets.Here’s the simplified Chinese text for the three main points:* For: 这篇论文目的是提供一种基于机器学习的健康饮食计划，特别是为患有多种疾病的患者。* Methods: 论文使用了不同的机器学习算法，包括线性回归、支持向量机（SVM）、决策树、Random Forest、XGBoost和LightGBM等，来预测 fluid、碳水化合物、蛋白质和脂肪的消耗。* Results: 论文在 fluid 预测中使用线性回归得到了高精度低根平方差（RMSE），在碳水化合物预测中使用Random Forest 得到了高精度，在蛋白质和脂肪预测中使用LightGBM 得到了高精度。OBESEYE 系统是唯一考虑了患者的COMorbidities和物理状况的饮食建议系统。<details>
<summary>Abstract</summary>
Obesity, the leading cause of many non-communicable diseases, occurs mainly for eating more than our body requirements and lack of proper activity. So, being healthy requires heathy diet plans, especially for patients with comorbidities. But it is difficult to figure out the exact quantity of each nutrient because nutrients requirement varies based on physical and disease conditions. In our study we proposed a novel machine learning based system to predict the amount of nutrients one individual requires for being healthy. We applied different machine learning algorithms: linear regression, support vector machine (SVM), decision tree, random forest, XGBoost, LightGBM on fluid and 3 other major micronutrients: carbohydrate, protein, fat consumption prediction. We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction. We believe our diet recommender system, OBESEYE, is the only of its kind which recommends diet with the consideration of comorbidities and physical conditions and promote encouragement to get rid of obesity.
</details>
<details>
<summary>摘要</summary>
肥胖是多种非传染疾病的主要原因，主要由于食物摄入量超过身体需求，以及不足的适当活动。因此，保持健康需要健康的饮食计划，特别是 для患有多种疾病的患者。然而，确定每个营养素的准确量是困难的，因为营养素需求因 físical 和疾病状况而异。在我们的研究中，我们提出了一种基于机器学习的系统，可以预测个人需要的营养素量。我们应用了不同的机器学习算法：线性回归、支持向量机(SVM)、决策树、随机森林、XGBoost、LightGBM 等，对流体和三种主要微量粮类：碳水化合物、蛋白质、脂肪摄入预测。我们得到了高精度低根平方差(RMSE)的结果，通过线性回归在流体预测中，随机森林在碳水化合物预测中，LightGBM在蛋白质和脂肪预测中。我们认为我们的饮食建议系统“OBESEYE”是目前唯一一个考虑了慢性疾病和物理状况，并且激励人们做减肥的健康饮食建议系统。
</details></li>
</ul>
<hr>
<h2 id="OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework"><a href="#OrcoDCS-An-IoT-Edge-Orchestrated-Online-Deep-Compressed-Sensing-Framework" class="headerlink" title="OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework"></a>OrcoDCS: An IoT-Edge Orchestrated Online Deep Compressed Sensing Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05757">http://arxiv.org/abs/2308.05757</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cheng-Wei Ching, Chirag Gupta, Zi Huang, Liting Hu</li>
<li>for: 这个研究是为了提出一个可以灵活地适应不同感知任务和环境变化的协调式深度压缩数据聚合（CDA）框架，以提高这些应用的性能和可扩展性。</li>
<li>methods: 这个框架使用了专门设计的对称启发器，并与边缘设备进行协调，以实现在无线感知网络（WSN）上的线上执行和训练。</li>
<li>results: 这个研究显示，在训练时间和灵活性方面，OrcoDCS比前一代的深度压缩数据聚合（DCDA）要好，并且在进一步应用中实现了更高的性能。<details>
<summary>Abstract</summary>
Compressed data aggregation (CDA) over wireless sensor networks (WSNs) is task-specific and subject to environmental changes. However, the existing compressed data aggregation (CDA) frameworks (e.g., compressed sensing-based data aggregation, deep learning(DL)-based data aggregation) do not possess the flexibility and adaptivity required to handle distinct sensing tasks and environmental changes. Additionally, they do not consider the performance of follow-up IoT data-driven deep learning (DL)-based applications. To address these shortcomings, we propose OrcoDCS, an IoT-Edge orchestrated online deep compressed sensing framework that offers high flexibility and adaptability to distinct IoT device groups and their sensing tasks, as well as high performance for follow-up applications. The novelty of our work is the design and deployment of IoT-Edge orchestrated online training framework over WSNs by leveraging an specially-designed asymmetric autoencoder, which can largely reduce the encoding overhead and improve the reconstruction performance and robustness. We show analytically and empirically that OrcoDCS outperforms the state-of-the-art DCDA on training time, significantly improves flexibility and adaptability when distinct reconstruction tasks are given, and achieves higher performance for follow-up applications.
</details>
<details>
<summary>摘要</summary>
压缩数据聚合（CDA）在无线传感网络（WSN）上是任务特定和环境变化受限的。然而，现有的CDA框架（如扩lapsed sensing基于数据聚合、深度学习（DL）基于数据聚合）不具备适应性和灵活性，无法处理不同感知任务和环境变化。另外，它们不考虑ollow-up IoT数据驱动深度学习（DL）应用的性能。为了解决这些缺点，我们提议OrcoDCS，一个基于IoT-Edge协调在线深度压缩探测框架，具有高适应性和灵活性，以及高性能 дляollow-up应用。我们利用特制的非对称自动encoder，可以大幅减少编码负担，提高重建性能和稳定性。我们通过分析和实验证明，OrcoDCS在训练时间、适应性和灵活性方面都超过了现有的DCDA。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze"><a href="#Semi-supervised-Contrastive-Regression-for-Estimation-of-Eye-Gaze" class="headerlink" title="Semi-supervised Contrastive Regression for Estimation of Eye Gaze"></a>Semi-supervised Contrastive Regression for Estimation of Eye Gaze</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02784">http://arxiv.org/abs/2308.02784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Somsukla Maiti, Akshansh Gupta</li>
<li>for: 这篇论文的目的是发展一个 semi-supervised contrastive learning 框架，以便估算人们的 gaze 方向。</li>
<li>methods: 本论文使用 appearance-based deep learning 模型来进行 gaze 估算，并提出了一新的对称损失函数，从中对 similary 的图像进行对比，以提高对 gaze 方向的估算精度。</li>
<li>results: 本论文的实验结果显示，这个 semi-supervised contrastive learning 框架能够从小量 annotated gaze 数据中学习一个通用的 gaze 估算模型，并且与一些state-of-the-art contrastive learning 技术相比，表现更好。<details>
<summary>Abstract</summary>
With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi-supervised contrastive learning framework for estimation of gaze direction. With a small labeled gaze dataset, the framework is able to find a generalized solution even for unseen face images. In this paper, we have proposed a new contrastive loss paradigm that maximizes the similarity agreement between similar images and at the same time reduces the redundancy in embedding representations. Our contrastive regression framework shows good performance in comparison to several state of the art contrastive learning techniques used for gaze estimation.
</details>
<details>
<summary>摘要</summary>
“随着人机界面智能系统的需求增加，视线控制系统的开发已成为必填。视线是非侵入式人际互动的一种最佳方法。深度学习模型基于 appearances 是目前最受欢迎的视线估计方法。但是这些模型的性能受到labelled gaze dataset的大小影响，从而影响其一般化性能。本文提出了一个半监督对称学习框架，以便透过小量labelled gaze dataset来找到一个通用的解决方案，包括未见过的脸像。本文提出了一个新的对称损失函数，它将相似的图像 Similarity 提高，并同时将嵌入表现的统计复杂度降低。我们的对称回传框架在与多个现有的对称学习技术相比之下，表现良好。”
</details></li>
</ul>
<hr>
<h2 id="Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting"><a href="#Dataopsy-Scalable-and-Fluid-Visual-Exploration-using-Aggregate-Query-Sculpting" class="headerlink" title="Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting"></a>Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02764">http://arxiv.org/abs/2308.02764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Naimul Hoque, Niklas Elmqvist<br>for:* 这个论文是为了描述一种faceted visual query技术，帮助用户在大规模多维数据中进行查询和浏览。methods:* 这种查询技术使用的方法包括PIVOT、PARTITION、PEEK、PILE、PROJECT和PRUNE等六个步骤，用于Progressive exploration of the dataset。results:* 通过使用这种查询技术，用户可以在大规模多维数据中进行有效的查询和浏览，并且可以逐步缩小数据集，以便更好地理解数据的结构和特征。<details>
<summary>Abstract</summary>
We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a "born scalable" query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.
</details>
<details>
<summary>摘要</summary>
我们介绍了统计查询雕刻（AQS），一种适合大规模多维数据的faceted visual查询技术。作为一种“生成可扩展”的查询技术，AQS开始可视化的方法是透过单一的可视示表示数据集的总聚合。用户可以逐步探索数据集通过一系列操作缩写为P6：折冲（基于特征对资料聚合进行分割）、分区（在空间中排列分割）、侦错（查看子集使用聚合图表示）、堆叠（合并两个或更多的子集）、专案（将子集转换为新基板）、剪除（不再关注的聚合）。我们验证了AQS和Dataopsy，一个实现AQS的试验版本，在桌面和触控式移动设备上进行流过交互。我们透过两个案例和三个应用例子来示范AQS和Dataopsy。
</details></li>
</ul>
<hr>
<h2 id="Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks"><a href="#Neural-Collapse-in-the-Intermediate-Hidden-Layers-of-Classification-Neural-Networks" class="headerlink" title="Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks"></a>Neural Collapse in the Intermediate Hidden Layers of Classification Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02760">http://arxiv.org/abs/2308.02760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Parker, Emre Onal, Anton Stengel, Jake Intrater</li>
<li>for: 这paper探讨了分类神经网络中间埋点层中的Neural Collapse（NC）现象的emergence。</li>
<li>methods: 这paper使用了多种网络架构、活化函数和数据集来研究NC现象在不同层次的emergence。</li>
<li>results: 研究发现，NC现象在大多数中间埋点层中出现，其度Of collapse与层次深度正相关。此外，研究还发现，大多数减少类内方差的改进发生在神经网络的浅层，而angular separation between class means随层次深度增加。<details>
<summary>Abstract</summary>
Neural Collapse (NC) gives a precise description of the representations of classes in the final hidden layer of classification neural networks. This description provides insights into how these networks learn features and generalize well when trained past zero training error. However, to date, (NC) has only been studied in the final layer of these networks. In the present paper, we provide the first comprehensive empirical analysis of the emergence of (NC) in the intermediate hidden layers of these classifiers. We examine a variety of network architectures, activations, and datasets, and demonstrate that some degree of (NC) emerges in most of the intermediate hidden layers of the network, where the degree of collapse in any given layer is typically positively correlated with the depth of that layer in the neural network. Moreover, we remark that: (1) almost all of the reduction in intra-class variance in the samples occurs in the shallower layers of the networks, (2) the angular separation between class means increases consistently with hidden layer depth, and (3) simple datasets require only the shallower layers of the networks to fully learn them, whereas more difficult ones require the entire network. Ultimately, these results provide granular insights into the structural propagation of features through classification neural networks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System"><a href="#WeldMon-A-Cost-effective-Ultrasonic-Welding-Machine-Condition-Monitoring-System" class="headerlink" title="WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System"></a>WeldMon: A Cost-effective Ultrasonic Welding Machine Condition Monitoring System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05756">http://arxiv.org/abs/2308.05756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Beitong Tian, Kuan-Chieh Lu, Ahmadreza Eslaminia, Yaohui Wang, Chenhui Shao, Klara Nahrstedt</li>
<li>for: 这个论文是为了提出一个可靠、高性能且Cost-effective的工具状态监控系统，以提高鋳接过程中的质量控制。</li>
<li>methods: 这个系统使用自定义的数据收集系统和资料分析管道，实现实时分析。标本检测算法结合自动生成的特征和手工设计的特征，在条件分类任务中达到了95.8%的预设准确率（相比前一代方法的92.5%）。数据增强方法可以减少概念变化问题，提高工具状态分类精度8.3%。所有算法都在本地运行，仅需385毫秒过程数据。</li>
<li>results: 我们部署了WeldMon和一个商业系统在实际的鋳接机上，进行了全面的比较。我们发现，WeldMon可以提供高性能、可靠且Cost-effective的工具状态监控系统。<details>
<summary>Abstract</summary>
Ultrasonic welding machines play a critical role in the lithium battery industry, facilitating the bonding of batteries with conductors. Ensuring high-quality welding is vital, making tool condition monitoring systems essential for early-stage quality control. However, existing monitoring methods face challenges in cost, downtime, and adaptability. In this paper, we present WeldMon, an affordable ultrasonic welding machine condition monitoring system that utilizes a custom data acquisition system and a data analysis pipeline designed for real-time analysis. Our classification algorithm combines auto-generated features and hand-crafted features, achieving superior cross-validation accuracy (95.8% on average over all testing tasks) compared to the state-of-the-art method (92.5%) in condition classification tasks. Our data augmentation approach alleviates the concept drift problem, enhancing tool condition classification accuracy by 8.3%. All algorithms run locally, requiring only 385 milliseconds to process data for each welding cycle. We deploy WeldMon and a commercial system on an actual ultrasonic welding machine, performing a comprehensive comparison. Our findings highlight the potential for developing cost-effective, high-performance, and reliable tool condition monitoring systems.
</details>
<details>
<summary>摘要</summary>
服务器焊接机在锂离子电池业中发挥关键作用，帮助焊接电池与导电器。保证高质量焊接是关键，因此工具状态监测系统成为了早期质量控制的必备工具。然而，现有监测方法存在成本高、下机时间长和适应性差的问题。本文提出了WeldMon，一种可靠、高性能、成本下降的焊接机状态监测系统。WeldMon使用自定义数据获取系统和实时分析管道，并将自动生成的特征和手动设计的特征结合使用，实现了95.8%的验证精度（相对于状态艺术法92.5%）。我们的数据扩展方法可以减轻概念飘移问题，提高工具状态分类精度8.3%。所有算法都运行在本地，仅需385毫秒处理数据每个焊接周期。我们将WeldMon和一个商业系统部署在实际的服务器焊接机上，进行了完整的比较。我们的发现表明，可以开发出成本下降、高性能、可靠的工具状态监测系统。
</details></li>
</ul>
<hr>
<h2 id="DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation"><a href="#DaMSTF-Domain-Adversarial-Learning-Enhanced-Meta-Self-Training-for-Domain-Adaptation" class="headerlink" title="DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation"></a>DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02753">http://arxiv.org/abs/2308.02753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Yunxiang Zhao, Zhiliang Tian, Yang Liu, Dongsheng Li</li>
<li>for: 这篇论文的目的是提出一个新的自我训练框架 для领域适应（Domain Adaptation），以解决预测错误导致的标签噪音问题。</li>
<li>methods: 这篇论文使用了自我训练的方法，将模型的预测作为目标领域中的伪标签，通过这些伪标签进行自我训练。此外，论文还提出了一个名为Domain adversarial learning enhanced Self-Training Framework（DaMSTF）的新自我训练框架，具有以下三个特点：一、使用meta-learning估算伪标签的重要性，以降低标签噪音并保留困难的例子；二、设计了一个meta constructor来建立meta-验证集，以保证meta-learning模组的有效性；三、使用领域抗击学来初始化神经网络，以解决meta-learning模组的训练导向变差问题。</li>
<li>results: 论文通过理论和实验证明了DaMSTF的有效性。在跨领域情感分类任务上，DaMSTF比BERT提高了近4%的性能。<details>
<summary>Abstract</summary>
Self-training emerges as an important research line on domain adaptation. By taking the model's prediction as the pseudo labels of the unlabeled data, self-training bootstraps the model with pseudo instances in the target domain. However, the prediction errors of pseudo labels (label noise) challenge the performance of self-training. To address this problem, previous approaches only use reliable pseudo instances, i.e., pseudo instances with high prediction confidence, to retrain the model. Although these strategies effectively reduce the label noise, they are prone to miss the hard examples. In this paper, we propose a new self-training framework for domain adaptation, namely Domain adversarial learning enhanced Self-Training Framework (DaMSTF). Firstly, DaMSTF involves meta-learning to estimate the importance of each pseudo instance, so as to simultaneously reduce the label noise and preserve hard examples. Secondly, we design a meta constructor for constructing the meta-validation set, which guarantees the effectiveness of the meta-learning module by improving the quality of the meta-validation set. Thirdly, we find that the meta-learning module suffers from the training guidance vanishment and tends to converge to an inferior optimal. To this end, we employ domain adversarial learning as a heuristic neural network initialization method, which can help the meta-learning module converge to a better optimal. Theoretically and experimentally, we demonstrate the effectiveness of the proposed DaMSTF. On the cross-domain sentiment classification task, DaMSTF improves the performance of BERT with an average of nearly 4%.
</details>
<details>
<summary>摘要</summary>
自适应预测为域适应研究的重要线索。通过将模型的预测作为目标域无标签数据的 Pseudo 标签，自适应启动模型以 Pseudo 实例作为目标域中的训练数据。然而，预测错误（标签噪音）对自适应表现成为一个挑战。以前的方法仅使用可靠 Pseudo 实例来重新训练模型，即 Pseudo 实例具有高预测信任度。虽然这些策略有效减少标签噪音，但容易过损难例。在这篇论文中，我们提出了一种新的自适应框架 для域适应，即域适应学习强化自适应框架（DaMSTF）。首先，DaMSTF 通过元学习来估计每个 Pseudo 实例的重要性，以同时减少标签噪音并保留难例。其次，我们设计了元构建模块，用于构建元验证集，以保证元学习模块的有效性。最后，我们发现元学习模块受训练指导消失的问题，容易 converges 到一个差的优化点。为此，我们采用域适应学习作为一种启发函数初始化方法，可以帮助元学习模块 converge 到一个更好的优化点。理论和实验表明，我们提出的 DaMSTF 具有较高的效果。在跨域情感分类任务上，DaMSTF 可以提高 BERT 的性能，均提高约 4%。
</details></li>
</ul>
<hr>
<h2 id="NeRFs-The-Search-for-the-Best-3D-Representation"><a href="#NeRFs-The-Search-for-the-Best-3D-Representation" class="headerlink" title="NeRFs: The Search for the Best 3D Representation"></a>NeRFs: The Search for the Best 3D Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02751">http://arxiv.org/abs/2308.02751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ravi Ramamoorthi</li>
<li>for: 这篇论文描述了NeRF表示法的应用和发展，以及三十年来寻找最佳视图合成和相关问题的3D表示方法的漫长历程。</li>
<li>methods: NeRF表示法使用神经网络来描述场景的连续体，包括视点依赖的光泽和体积密度。</li>
<li>results: NeRF表示法已成为计算机图形和视觉领域的标准表示方法，广泛应用于视图合成和图像基于渲染等问题，并且有 thousands of 篇论文进行扩展和建立。<details>
<summary>Abstract</summary>
Neural Radiance Fields or NeRFs have become the representation of choice for problems in view synthesis or image-based rendering, as well as in many other applications across computer graphics and vision, and beyond. At their core, NeRFs describe a new representation of 3D scenes or 3D geometry. Instead of meshes, disparity maps, multiplane images or even voxel grids, they represent the scene as a continuous volume, with volumetric parameters like view-dependent radiance and volume density obtained by querying a neural network. The NeRF representation has now been widely used, with thousands of papers extending or building on it every year, multiple authors and websites providing overviews and surveys, and numerous industrial applications and startup companies. In this article, we briefly review the NeRF representation, and describe the three decades-long quest to find the best 3D representation for view synthesis and related problems, culminating in the NeRF papers. We then describe new developments in terms of NeRF representations and make some observations and insights regarding the future of 3D representations.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本为简化中文。<</SYS>>神经辐射场或NeRFs已成为视觉合成或基于图像渲染等问题的表示方法选择，以及计算机视觉领域中许多其他应用程序的首选方法。它们的核心在于描述了一种新的3D场景或3D几何表示方法。而不是mesh、投影图、多平面图像或者VOXEL网格，NeRF代表场景为一个连续的Volume，通过问题 neural network 获得了视觉依赖的辐射光照和体积密度。NeRF表示法已经广泛应用，每年有 thousands of 篇论文扩展或基于它，多个作者和网站提供了概述和评论，以及许多工业应用和创业公司。在这篇文章中，我们 briefly 评论了NeRF表示法，并描述了三十年来为视觉合成和相关问题寻找最佳3D表示方法的历程， culminating 在NeRF论文中。然后，我们描述了新的NeRF表示法和一些关于未来3D表示方法的见解和发现。
</details></li>
</ul>
<hr>
<h2 id="Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration"><a href="#Exploiting-On-chip-Heterogeneity-of-Versal-Architecture-for-GNN-Inference-Acceleration" class="headerlink" title="Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration"></a>Exploiting On-chip Heterogeneity of Versal Architecture for GNN Inference Acceleration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02749">http://arxiv.org/abs/2308.02749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Chen, Pavan Manjunath, Sasindu Wijeratne, Bingyi Zhang, Viktor Prasanna</li>
<li>for: 该 paper 是为了提高 Graph Neural Network (GNN) 的推理速度而设计的。</li>
<li>methods: 该 paper 使用 AMD Versal ACAP 架构的特性进行 GNN 推理加速。具体来说，它使用 Programmable Logic (PL) 和 AI Engine (AIE) 两种不同的计算模块来实现 sparse 和 dense 操作的快速执行。</li>
<li>results: 该 paper 的实现在 VCK5000 ACAP 平台上表现出了较好的性能，与 CPU、GPU、ACAP 和其他自定义 GNN 加速器相比，实现了显著的均值运行时速度提升（162.42x、17.01x、9.90x 和 27.23x）。此外，对于 Graph Convolutional Network (GCN) 推理，该approach 在同一 ACAP 设备上实现了3.9-96.7x的速度提升。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) have revolutionized many Machine Learning (ML) applications, such as social network analysis, bioinformatics, etc. GNN inference can be accelerated by exploiting data sparsity in the input graph, vertex features, and intermediate data in GNN computations. For dynamic sparsity exploitation, we leverage the heterogeneous computing capabilities of AMD Versal ACAP architecture to accelerate GNN inference. We develop a custom hardware module that executes the sparse primitives of the computation kernel on the Programmable Logic (PL) and efficiently computes the dense primitives using the AI Engine (AIE). To exploit data sparsity during inference, we devise a runtime kernel mapping strategy that dynamically assigns computation tasks to the PL and AIE based on data sparsity. Our implementation on the VCK5000 ACAP platform leads to superior performance compared with the state-of-the-art implementations on CPU, GPU, ACAP, and other custom GNN accelerators. Compared with these implementations, we achieve significant average runtime speedup across various models and datasets of 162.42x, 17.01x, 9.90x, and 27.23x, respectively. Furthermore, for Graph Convolutional Network (GCN) inference, our approach leads to a speedup of 3.9-96.7x compared to designs using PL only on the same ACAP device.
</details>
<details>
<summary>摘要</summary>
格raph神经网络（GNNs）已经革命化了许多机器学习（ML）应用，如社交网络分析和生物信息学等。GNN推理可以通过利用输入图数据稀疏性来加速。为了实现动态稀疏性利用，我们利用AMD Versal ACAP架构的多样化计算能力来加速GNN推理。我们开发了一个自定义硬件模块，该模块在Programmable Logic（PL）上执行稀疏计算 primitives，并使用AI Engine（AIE）来高效计算 dense primitives。为了在推理过程中利用数据稀疏性，我们提出了一种运行时kernel映射策略，该策略在PL和AIE之间动态分配计算任务基于数据稀疏性。我们在VCK5000 ACAP平台上实现了比之前的状态泰器实现在CPU、GPU、ACAP和其他自定义GNN加速器上的性能。相比这些实现，我们实现了平均运行时速度提升值为162.42倍、17.01倍、9.90倍和27.23倍，分别。此外，对于图卷积网络（GCN）推理，我们的方法实现了PL只的设计相比3.9-96.7倍的加速。
</details></li>
</ul>
<hr>
<h2 id="SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning"><a href="#SABRE-Robust-Bayesian-Peer-to-Peer-Federated-Learning" class="headerlink" title="SABRE: Robust Bayesian Peer-to-Peer Federated Learning"></a>SABRE: Robust Bayesian Peer-to-Peer Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02747">http://arxiv.org/abs/2308.02747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru, Farinaz Koushanfar</li>
<li>for: 提出了一种新的 Federated Learning 框架，即 SABRE，以提高 robustness。</li>
<li>methods: 使用了一种新的权重聚合方法，可以在非标准分布Setting下工作，并且不需要benign节点多于恶意节点。</li>
<li>results: 在一些 benchmark 数据上进行了证明和评估，并证明了 SABRE 在各种恶意攻击下的Robustness。<details>
<summary>Abstract</summary>
We introduce SABRE, a novel framework for robust variational Bayesian peer-to-peer federated learning. We analyze the robustness of the known variational Bayesian peer-to-peer federated learning framework (BayP2PFL) against poisoning attacks and subsequently show that BayP2PFL is not robust against those attacks. The new SABRE aggregation methodology is then devised to overcome the limitations of the existing frameworks. SABRE works well in non-IID settings, does not require the majority of the benign nodes over the compromised ones, and even outperforms the baseline algorithm in benign settings. We theoretically prove the robustness of our algorithm against data / model poisoning attacks in a decentralized linear regression setting. Proof-of-Concept evaluations on benchmark data from image classification demonstrate the superiority of SABRE over the existing frameworks under various poisoning attacks.
</details>
<details>
<summary>摘要</summary>
我们介绍SABRE，一个新的 Federated Learning框架，可以防护Against poisoning攻击。我们分析了已知的Variational Bayesian Peer-to-Peer Federated Learning框架（BayP2PFL）对于毒素攻击的不敏感性，并证明BayP2PFL不具有抗毒素能力。我们随后设计了一个新的SABRE聚合方法，以解决现有框架的局限性。SABRE在非ID Setting下表现良好，不需要大多数善意节点 greater than 损坏节点，甚至在正常设定下超越了基准算法。我们对Decentralized Linear Regression Setting中的数据/模型毒素攻击进行了理论上的证明，并在benchmark数据上进行了Proof-of-Concept评估，证明SABRE在不同的毒素攻击下的superiority。
</details></li>
</ul>
<hr>
<h2 id="Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification"><a href="#Meta-Tsallis-Entropy-Minimization-A-New-Self-Training-Approach-for-Domain-Adaptation-on-Text-Classification" class="headerlink" title="Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification"></a>Meta-Tsallis-Entropy Minimization: A New Self-Training Approach for Domain Adaptation on Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02746">http://arxiv.org/abs/2308.02746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Menglong Lu, Zhen Huang, Zhiliang Tian, Yunxiang Zhao, Xuanyu Fei, Dongsheng Li</li>
<li>for: 本文旨在提高文本分类模型的适应性 across 多个频道，有广泛的应用。</li>
<li>methods: 本文提出了一种基于自适应学习的方法，即使用自适应学习生成pseudo例，并在源频道和目标频道中进行分类。</li>
<li>results: 实验表明， compared to traditional self-training methods, MTEM可以提高BERT模型的适应性，平均提高4%的精度。<details>
<summary>Abstract</summary>
Text classification is a fundamental task for natural language processing, and adapting text classification models across domains has broad applications. Self-training generates pseudo-examples from the model's predictions and iteratively trains on the pseudo-examples, i.e., minimizes the loss on the source domain and the Gibbs entropy on the target domain. However, Gibbs entropy is sensitive to prediction errors, and thus, self-training tends to fail when the domain shift is large. In this paper, we propose Meta-Tsallis Entropy minimization (MTEM), which applies a meta-learning algorithm to optimize the instance adaptive Tsallis entropy on the target domain. To reduce the computation cost of MTEM, we propose an approximation technique to approximate the Second-order derivation involved in the meta-learning. To efficiently generate pseudo labels, we propose an annealing sampling mechanism for exploring the model's prediction probability. Theoretically, we prove the convergence of the meta-learning algorithm in MTEM and analyze the effectiveness of MTEM in achieving domain adaptation. Experimentally, MTEM improves the adaptation performance of BERT with an average of 4 percent on the benchmark dataset.
</details>
<details>
<summary>摘要</summary>
文本分类是自然语言处理中的基本任务，并且在不同领域中适应文本分类模型有广泛的应用。自我训练会生成 pseudo-example 从模型预测中，并在这些 pseudo-example 上进行逐步训练，即在源领域中减少损失，并在目标领域中减少 Gibbs  entropy。然而，Gibbs entropy 受到预测错误的影响，因此自我训练在大量领域变换时常常失败。在这篇论文中，我们提出了 Meta-Tsallis Entropy 优化（MTEM），它使用元学习算法来优化目标领域中的实例适应 Tsallis  entropy。为了减少 MTEM 的计算成本，我们提出了一种近似技术来近似元学习中的第二阶导数。同时，我们还提出了一种缓和抽象采样机制，以便快速生成 pseudo 标签。理论上，我们证明了 MTEM 中元学习算法的收敛性，并分析了 MTEM 在适应性方面的效果。实验表明，MTEM 可以在标准数据集上提高 BERT 的适应性表现，平均提高了4%。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics"><a href="#Learning-to-Schedule-in-Non-Stationary-Wireless-Networks-With-Unknown-Statistics" class="headerlink" title="Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics"></a>Learning to Schedule in Non-Stationary Wireless Networks With Unknown Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02734">http://arxiv.org/abs/2308.02734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang Minh Nguyen, Eytan Modiano</li>
<li>for: 研究 wireless network 中的吞吐率最优化策略，满足通信系统中部分可见性和时间变化的假设。</li>
<li>methods: 提出了一种基于 Max-Weight 策略的 MW-UCB 算法，利用 Sliding-Window Upper-Confidence Bound 学习无线通信中的通道统计数据，并且可以在非站立性下实现throughput最优。</li>
<li>results: 经过 simulations 验证，MW-UCB 算法可以在具有部分可见性和时间变化的无线网络中实现高效的吞吐率最优化。<details>
<summary>Abstract</summary>
The emergence of large-scale wireless networks with partially-observable and time-varying dynamics has imposed new challenges on the design of optimal control policies. This paper studies efficient scheduling algorithms for wireless networks subject to generalized interference constraint, where mean arrival and mean service rates are unknown and non-stationary. This model exemplifies realistic edge devices' characteristics of wireless communication in modern networks. We propose a novel algorithm termed MW-UCB for generalized wireless network scheduling, which is based on the Max-Weight policy and leverages the Sliding-Window Upper-Confidence Bound to learn the channels' statistics under non-stationarity. MW-UCB is provably throughput-optimal under mild assumptions on the variability of mean service rates. Specifically, as long as the total variation in mean service rates over any time period grows sub-linearly in time, we show that MW-UCB can achieve the stability region arbitrarily close to the stability region of the class of policies with full knowledge of the channel statistics. Extensive simulations validate our theoretical results and demonstrate the favorable performance of MW-UCB.
</details>
<details>
<summary>摘要</summary>
现代无线网络中的大规模无线网络具有部分可见和时间变化的动态特性，对优化控制策略的设计带来了新的挑战。本文研究了基于通用干扰约束的无线网络占用策略的有效计划算法。这种模型体现了现代无线通信网络中Edge设备的准确特性。我们提出了一种名为MW-UCB的新算法，它基于Max-Weight策略，并利用Sliding-Window Upper-Confidence Bound来学习不确定的通道统计。MW-UCB可以在不确定的服务率的情况下保证通信吞吐量的最大化，并且可以在服务率的变化范围内保持稳定。我们证明MW-UCB在一定程度上是可以达到稳定区的最优策略，并且可以在服务率的变化范围内保持稳定。我们的实验结果 validate我们的理论结论，并证明MW-UCB在实际应用中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning"><a href="#Personalization-of-Stress-Mobile-Sensing-using-Self-Supervised-Learning" class="headerlink" title="Personalization of Stress Mobile Sensing using Self-Supervised Learning"></a>Personalization of Stress Mobile Sensing using Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02731">http://arxiv.org/abs/2308.02731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanvir Islam, Peter Washington</li>
<li>for: 这研究旨在开发一种个性化的压力预测方法，可以使用小数据量的标签来个性化学习。</li>
<li>methods: 研究人员使用了一种自动学习技术，即一维度的卷积神经网络（CNN），通过自动学习来学习每个用户的基线生物信号模式，从而实现个性化学习。</li>
<li>results: 研究人员发现，使用自动学习前training方法可以学习出高性能的压力预测模型，仅需使用少量的标签数据。这种个性化学习方法可以帮助实现精准健康系统，这些系统可以根据每个用户的特点进行个性化预测和提供推荐。<details>
<summary>Abstract</summary>
Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalization with very few labels, we pre-train a 1-dimensional convolutional neural network (CNN) using self-supervised learning (SSL). We evaluate our method using the Wearable Stress and Affect prediction (WESAD) dataset. We fine-tune the pre-trained networks to the stress prediction task and compare against equivalent models without any self-supervised pre-training. We discover that embeddings learned using our pre-training method outperform supervised baselines with significantly fewer labeled data points: the models trained with SSL require less than 30% of the labels to reach equivalent performance without personalized SSL. This personalized learning method can enable precision health systems which are tailored to each subject and require few annotations by the end user, thus allowing for the mobile sensing of increasingly complex, heterogeneous, and subjective outcomes such as stress.
</details>
<details>
<summary>摘要</summary>
stress是广泛认可的健康问题的重要 contribuens。预测stress使用记录在佩戴式设备中的生物信号数据是移动感知研究中关键的预测领域，因为实时预测stress可以让数字干预出现在压力开始时，以避免心跳rhythm irregularities和许多心理和生理学症状。电导活动（EDA）经常用来测量压力。然而，机器学习预测压力的主要挑战包括标签的主观性和稀缺性，大的特征空间，相对少的标签，以及复杂的非线性和主观关系 между特征和结果。为解决这些问题，我们研究了个性化预测：为每个用户训练一个压力预测模型。为让神经网络学习每个人的基线生物信号模式的时间 dynamics，因此启用个性化预测，我们使用自动学习（SSL）预训练一个1维度卷积神经网络（CNN）。我们使用WESAD数据集进行评估我们的方法。我们精细调整预训练后的网络来完成压力预测任务，并与没有自动学习预训练的相同模型进行比较。我们发现，使用我们的预训练方法学习的嵌入超越了无预训练基线的表现，只需要少于30%的标签数据点来达到相同的性能。这种个性化学习方法可以启用手持式健康系统，这些系统可以根据每个用户而定制，并且只需要少量标签由用户提供，以便在移动感知中评估越来越复杂、多元和主观的结果。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks"><a href="#Synthesizing-Programmatic-Policies-with-Actor-Critic-Algorithms-and-ReLU-Networks" class="headerlink" title="Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks"></a>Synthesizing Programmatic Policies with Actor-Critic Algorithms and ReLU Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02729">http://arxiv.org/abs/2308.02729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spyros Orfanos, Levi H. S. Lelis</li>
<li>for: 本研究旨在探讨Programmatically Interpretable Reinforcement Learning（PIRL）算法是否需要专门的PIRL算法，以及如何使用actor-critic算法直接从 neural network 中提取政策。</li>
<li>methods: 本研究使用 actor-critic 算法，以及一种将ReLU神经网络翻译成 if-then-else 结构、线性变换和PID操作的方法，来将政策编码在程序中。</li>
<li>results: 实验结果表明，使用此翻译方法可以学习短而有效的政策，并且与PIRL算法 synthesize 的政策相比，译制后的政策在许多控制问题上表现至少兼容，有时甚至超越。<details>
<summary>Abstract</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) encodes policies in human-readable computer programs. Novel algorithms were recently introduced with the goal of handling the lack of gradient signal to guide the search in the space of programmatic policies. Most of such PIRL algorithms first train a neural policy that is used as an oracle to guide the search in the programmatic space. In this paper, we show that such PIRL-specific algorithms are not needed, depending on the language used to encode the programmatic policies. This is because one can use actor-critic algorithms to directly obtain a programmatic policy. We use a connection between ReLU neural networks and oblique decision trees to translate the policy learned with actor-critic algorithms into programmatic policies. This translation from ReLU networks allows us to synthesize policies encoded in programs with if-then-else structures, linear transformations of the input values, and PID operations. Empirical results on several control problems show that this translation approach is capable of learning short and effective policies. Moreover, the translated policies are at least competitive and often far superior to the policies PIRL algorithms synthesize.
</details>
<details>
<summary>摘要</summary>
Programmatically Interpretable Reinforcement Learning (PIRL) 编码策略为人类可读性计算机程序。最近，为了解决策略搜索空间中缺失梯度信号的问题，新的算法得到了开发。大多数PIRL算法首先使用神经网络作为尝试导航搜索空间的oracle。在这篇论文中，我们表明PIRL特有的算法不是必要的，具体来说，取决于编码策略的语言。因为可以直接使用actor-critic算法获得程序编码策略。我们使用ReLU神经网络和倾斜决策树的连接将actor-critic算法学习的策略翻译成程序编码策略。这种翻译方法可以synthesize编码策略中的if-then-else结构、输入值线性变换和PID操作。我们在几个控制问题上进行了实验，结果表明这种翻译方法可以学习短而有效的策略。此外，翻译出来的策略与PIRL算法生成的策略相比，通常更加竞争力强。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction"><a href="#Towards-Improving-Harmonic-Sensitivity-and-Prediction-Stability-for-Singing-Melody-Extraction" class="headerlink" title="Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction"></a>Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02723">http://arxiv.org/abs/2308.02723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/smoothken/kknet">https://github.com/smoothken/kknet</a></li>
<li>paper_authors: Keren Shao, Ke Chen, Taylor Berg-Kirkpatrick, Shlomo Dubnov</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。Here’s the full translation of the abstract in Simplified Chinese:</li>
<li>for: 这 paper 是为了提高 singing melody extraction 的性能而写的。</li>
<li>methods: 这 paper 使用了 input feature modification 和 training objective modification 两种方法来提高模型的性能。</li>
<li>results: 实验结果表明，提posed modifications 对 singing melody extraction 是有 empirical effectiveness 的。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.
</details>
<details>
<summary>摘要</summary>
在深度学习研究中，许多旋律提取模型通过重新设计神经网络架构来提高性能。在这篇论文中，我们提出了输入特征修改和训练目标修改，基于两个假设。首先，音频数据中的干扰在频谱图中快速衰减。为了增强模型对尾部干扰的敏感性，我们使用离散ζ变换修改合并频率和周期性（CFP）表示。其次，歌唱和非歌唱段的时间非常短暂是非常罕见的。为保证更稳定的旋律轮廓，我们设计了可导的损失函数，避免模型预测这些段落。我们应用这些修改于多个模型，包括MSNet、FTANet以及我们新引入的PianoNet，这是基于钢琴谱写网络的修改。我们的实验结果表明，我们的修改是实际有效的对歌唱旋律提取。
</details></li>
</ul>
<hr>
<h2 id="Fluid-Property-Prediction-Leveraging-AI-and-Robotics"><a href="#Fluid-Property-Prediction-Leveraging-AI-and-Robotics" class="headerlink" title="Fluid Property Prediction Leveraging AI and Robotics"></a>Fluid Property Prediction Leveraging AI and Robotics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02715">http://arxiv.org/abs/2308.02715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baratilab/vid2visc">https://github.com/baratilab/vid2visc</a></li>
<li>paper_authors: Jong Hoon Park, Gauri Pramod Dalwankar, Alison Bartsch, Abraham George, Amir Barati Farimani</li>
<li>for: 这篇论文是为了探讨如何使用视觉信息来推断流体的性质，以便在自动化流体处理系统中更好地控制流体的行为。</li>
<li>methods: 这篇论文使用了3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的表征。这些表征然后被用来从视频中视觉地推断流体的动态粘度。</li>
<li>results: 研究发现，使用这种视觉方法可以准确地推断流体的动态粘度，并且比传统方法更快速和更加精准。<details>
<summary>Abstract</summary>
Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
</details>
<details>
<summary>摘要</summary>
<<SYS Translate="yes">推断流体属性从视觉是一项复杂的任务，主要因为流体的行为和检测都具有复杂的特性。然而，从视觉信息直接推断流体属性的能力具有高度的价值，因为摄像头是 readily available。此外，仅基于视觉预测流体属性可以大幅缩短各种实验室环境中的测试时间和努力。在这项工作中，我们提出了一种完全基于视觉的方法，使用3D卷积自适应神经网络来学习不同的流体振荡模式在视频中的含义。我们利用这种含义来从视频中可视化地推断流体的类别或流体的动态粘性。</SYS>
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution"><a href="#Exploring-the-Effect-of-Sparse-Recovery-on-the-Quality-of-Image-Superresolution" class="headerlink" title="Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution"></a>Exploring the Effect of Sparse Recovery on the Quality of Image Superresolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02714">http://arxiv.org/abs/2308.02714</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antonio Castro</li>
<li>for: 该论文旨在研究用字典学习进行图像超解像，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，以便使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>methods: 该论文使用了字典学习技术，通过学习高分辨率和低分辨率图像对应的对应的patch对来学习一对联动的字典，并使用这些字典来从低分辨率输入图像中恢复高分辨率图像patch。</li>
<li>results: 该论文通过实验研究了不同的简单恢复算法对图像超解像质量的影响，并提出了最佳的简单恢复算法选择方法。<details>
<summary>Abstract</summary>
Dictionary learning can be used for image superresolution by learning a pair of coupled dictionaries of image patches from high-resolution and low-resolution image pairs such that the corresponding pairs share the same sparse vector when represented by the coupled dictionaries. These dictionaries then can be used to to reconstruct the corresponding high-resolution patches from low-resolution input images based on sparse recovery. The idea is to recover the shared sparse vector using the low-resolution dictionary and then multiply it by the high-resolution dictionary to recover the corresponding high-resolution image patch. In this work, we study the effect of the sparse recovery algorithm that we use on the quality of the reconstructed images. We offer empirical experiments to search for the best sparse recovery algorithm that can be used for this purpose.
</details>
<details>
<summary>摘要</summary>
《字典学习可以用于图像超分辨by学习一对相互关联的字典，这些字典分别表示高分辨率和低分辨率图像对应的图像 patches，以便在这些字典中共享相同的稀疏 вектор。这些字典可以用来从低分辨率输入图像中重建对应的高分辨率图像 patch，基于稀疏恢复。我们的研究将关注使用的稀疏恢复算法对重建图像质量的影响。我们将进行实验寻找最佳的稀疏恢复算法。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Scalable-Computation-of-Causal-Bounds"><a href="#Scalable-Computation-of-Causal-Bounds" class="headerlink" title="Scalable Computation of Causal Bounds"></a>Scalable Computation of Causal Bounds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02709">http://arxiv.org/abs/2308.02709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Madhumitha Shridharan, Garud Iyengar</li>
<li>for:  Computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold.</li>
<li>methods:  Significantly prune a linear programming (LP) formulation to compute bounds, allowing for larger causal inference problems compared to existing techniques. Extend the pruning methodology to fractional LPs for additional observations.</li>
<li>results:  Significant runtime improvement compared to benchmarks in experiments, and high-quality bounds produced by an efficient greedy heuristic that scales to larger problems.<details>
<summary>Abstract</summary>
We consider the problem of computing bounds for causal queries on causal graphs with unobserved confounders and discrete valued observed variables, where identifiability does not hold. Existing non-parametric approaches for computing such bounds use linear programming (LP) formulations that quickly become intractable for existing solvers because the size of the LP grows exponentially in the number of edges in the causal graph. We show that this LP can be significantly pruned, allowing us to compute bounds for significantly larger causal inference problems compared to existing techniques. This pruning procedure allows us to compute bounds in closed form for a special class of problems, including a well-studied family of problems where multiple confounded treatments influence an outcome. We extend our pruning methodology to fractional LPs which compute bounds for causal queries which incorporate additional observations about the unit. We show that our methods provide significant runtime improvement compared to benchmarks in experiments and extend our results to the finite data setting. For causal inference without additional observations, we propose an efficient greedy heuristic that produces high quality bounds, and scales to problems that are several orders of magnitude larger than those for which the pruned LP can be solved.
</details>
<details>
<summary>摘要</summary>
我团队考虑了计算 causal 查询 bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，该策略可以扩展到许多个数量级更大的问题。Here's the translation of the text into Traditional Chinese:我团队考虑过 Computing causal queries bounds 的问题，这里有不观察到的假设变量和离散型观察变量。我们显示出现非参数方法的 LP 表示可以快速减少，使得我们可以计算 bounds  для更大的 causal inference 问题。我们扩展了我们的减少方法来计算 fractional LPs，这些 LPs 计算 bounds  для包含额外观察的 causal queries。我们在实验中显示了我们的方法可以提供重要的时间改进 compared to 参考值。此外，我们还提出了一种高质量 bounds 的生成策略，这策略可以扩展到许多个数量级更大的问题。
</details></li>
</ul>
<hr>
<h2 id="FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise"><a href="#FPR-Estimation-for-Fraud-Detection-in-the-Presence-of-Class-Conditional-Label-Noise" class="headerlink" title="FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise"></a>FPR Estimation for Fraud Detection in the Presence of Class-Conditional Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02695">http://arxiv.org/abs/2308.02695</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtittelfitz/fpr-estimation">https://github.com/jtittelfitz/fpr-estimation</a></li>
<li>paper_authors: Justin Tittelfitz</li>
<li>for: 本研究旨在估计二分类模型中存在标签噪声（label noise）时的假阳性率（FPR）和正确率（TPR）。</li>
<li>methods: 本研究使用了一种新的方法，即使用模型直接清洁验证数据，以避免因验证数据中的噪声而导致的假阳性率和正确率的估计偏差。</li>
<li>results: 研究发现，使用现有的方法可能会导致假阳性率和正确率的估计偏差，而使用新的方法可以更好地估计这两个指标。<details>
<summary>Abstract</summary>
We consider the problem of estimating the false-/ true-positive-rate (FPR/TPR) for a binary classification model when there are incorrect labels (label noise) in the validation set. Our motivating application is fraud prevention where accurate estimates of FPR are critical to preserving the experience for good customers, and where label noise is highly asymmetric. Existing methods seek to minimize the total error in the cleaning process - to avoid cleaning examples that are not noise, and to ensure cleaning of examples that are. This is an important measure of accuracy but insufficient to guarantee good estimates of the true FPR or TPR for a model, and we show that using the model to directly clean its own validation data leads to underestimates even if total error is low. This indicates a need for researchers to pursue methods that not only reduce total error but also seek to de-correlate cleaning error with model scores.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting"><a href="#Explainable-Deep-Learning-based-Solar-Flare-Prediction-with-post-hoc-Attention-for-Operational-Forecasting" class="headerlink" title="Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting"></a>Explainable Deep Learning-based Solar Flare Prediction with post hoc Attention for Operational Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02682">http://arxiv.org/abs/2308.02682</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://bitbucket.org/gsudmlab/explainingfulldisk">https://bitbucket.org/gsudmlab/explainingfulldisk</a></li>
<li>paper_authors: Chetraj Pandey, Rafal A. Angryk, Manolis K. Georgoulis, Berkay Aydin</li>
<li>for: 预测日冕大地震（solar flare）的深度学习模型，以提高现有的预测方法的准确性和可靠性。</li>
<li>methods: 使用每小时的全盘线对视图磁图图像，采用二分类预测模式预测日冕大地震发生的可能性。采用自定义数据增强和样本权重来解决类偏置问题。使用真正的技能统计量和赫迪克技能分数作为评估指标。</li>
<li>results: 研究发现，全盘预测日冕大地震能够准确地 lokate和预测近日冕的大地震，这是操作预测中的关键特征。模型达到了平均的TSS&#x3D;0.51$\pm$0.05和HSS&#x3D;0.38$\pm$0.08水平，并且发现这些模型可以从全盘磁图中学习出明显的活跃区域特征。<details>
<summary>Abstract</summary>
This paper presents a post hoc analysis of a deep learning-based full-disk solar flare prediction model. We used hourly full-disk line-of-sight magnetogram images and selected binary prediction mode to predict the occurrence of $\geq$M1.0-class flares within 24 hours. We leveraged custom data augmentation and sample weighting to counter the inherent class-imbalance problem and used true skill statistic and Heidke skill score as evaluation metrics. Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods: (i) Guided Gradient-weighted Class Activation Mapping, (ii) Deep Shapley Additive Explanations, and (iii) Integrated Gradients. Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are: (1) We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting, (2) Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08, and (3) Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details>
<details>
<summary>摘要</summary>
Recent advancements in gradient-based attention methods allow us to interpret models by sending gradient signals to assign the burden of the decision on the input features. We interpret our model using three post hoc attention methods:1. Guided Gradient-weighted Class Activation Mapping: 使用梯度信号将决策担当分配到输入特征上。2. Deep Shapley Additive Explanations: 使用深度谱分解方法计算出每个特征对预测结果的贡献。3. Integrated Gradients: 使用梯度 интеграル方法计算出输入特征对预测结果的贡献。Our analysis shows that full-disk predictions of solar flares align with characteristics related to the active regions. The key findings of this study are:1. We demonstrate that our full disk model can tangibly locate and predict near-limb solar flares, which is a critical feature for operational flare forecasting.2. Our candidate model achieves an average TSS=0.51$\pm$0.05 and HSS=0.38$\pm$0.08.3. Our evaluation suggests that these models can learn conspicuous features corresponding to active regions from full-disk magnetograms.
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling"><a href="#A-Review-of-Change-of-Variable-Formulas-for-Generative-Modeling" class="headerlink" title="A Review of Change of Variable Formulas for Generative Modeling"></a>A Review of Change of Variable Formulas for Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02652">http://arxiv.org/abs/2308.02652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ullrich Köthe</li>
<li>for: 本研究旨在总结Change-of-variables（CoV）方程的各种应用和 derivation，并从编码器&#x2F;解码器架构的视角出发，收集28种CoV方程，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。</li>
<li>methods: 本研究使用了变量替换方程来简化复杂的概率分布，并通过学习的变换来实现 tractable Jacobian determinant。</li>
<li>results: 本研究收集了28种Change-of-variables方程，并通过对这些方程的系统性分析，探讨各种方法之间的关系，强调文献中不一定够清楚地表达的重要区别，并发现未来研究中的潜在漏洞。<details>
<summary>Abstract</summary>
Change-of-variables (CoV) formulas allow to reduce complicated probability densities to simpler ones by a learned transformation with tractable Jacobian determinant. They are thus powerful tools for maximum-likelihood learning, Bayesian inference, outlier detection, model selection, etc. CoV formulas have been derived for a large variety of model types, but this information is scattered over many separate works. We present a systematic treatment from the unifying perspective of encoder/decoder architectures, which collects 28 CoV formulas in a single place, reveals interesting relationships between seemingly diverse methods, emphasizes important distinctions that are not always clear in the literature, and identifies surprising gaps for future research.
</details>
<details>
<summary>摘要</summary>
<<SYS>>变量更改（CoV）公式可以将复杂的概率密度降低到更简单的密度中，通过学习的变换，其Jacobian determinant是可追踪的。因此，它们是最大 likelihood 学习、 bayesian 推理、异常检测、模型选择等方面的强大工具。CoV 公式已经 derivated  для许多模型类型，但这些信息分散在多个不同的论文中。我们在encoder/decoder 架构的统一视角下提供了一个系统性的处理方法，收集了28种CoV 公式，在一个地方汇总了这些信息，揭示了文献中的感兴趣关系，强调了文献中不一定明确的重要区别，并发现了未来研究中的潜在空白。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Hong Kong, Macau, and Taiwan.
</details></li>
</ul>
<hr>
<h2 id="ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation"><a href="#ReCLIP-Refine-Contrastive-Language-Image-Pre-Training-with-Source-Free-Domain-Adaptation" class="headerlink" title="ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation"></a>ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03793">http://arxiv.org/abs/2308.03793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo, Yuyin Sun, Ken Wang, Nan Qiao, Xiao Zeng, Min Sun, Cheng-Hao Kuo, Ram Nevatia</li>
<li>for: 提高CLIP模型在下游目标领域的性能，即使没有源数据或目标标注数据。</li>
<li>methods: 提出了一种源自由领域适应方法，通过减轻视文对象 embedding的偏差和使用杂模相关自动标注来实现。</li>
<li>results: 经过广泛的实验，ReCLIP方法可以将CLIP模型的平均错误率从30.17%降低至25.06%在22个图像分类 benchmark上。<details>
<summary>Abstract</summary>
Large-scale Pre-Training Vision-Language Model such as CLIP has demonstrated outstanding performance in zero-shot classification, e.g. achieving 76.3% top-1 accuracy on ImageNet without seeing any example, which leads to potential benefits to many tasks that have no labeled data. However, while applying CLIP to a downstream target domain, the presence of visual and text domain gaps and cross-modality misalignment can greatly impact the model performance. To address such challenges, we propose ReCLIP, the first source-free domain adaptation method for vision-language models, which does not require any source data or target labeled data. ReCLIP first learns a projection space to mitigate the misaligned visual-text embeddings and learns pseudo labels, and then deploys cross-modality self-training with the pseudo labels, to update visual and text encoders, refine labels and reduce domain gaps and misalignments iteratively. With extensive experiments, we demonstrate ReCLIP reduces the average error rate of CLIP from 30.17% to 25.06% on 22 image classification benchmarks.
</details>
<details>
<summary>摘要</summary>
大规模预训练视语模型，如CLIP，在零shot分类任务上表现出色，例如在ImageNet上取得76.3%的顶峰准确率无需见过任何示例，这可能导致多种任务 ohne 标注数据的潜在优势。然而，在应用CLIP到下游目标领域时，视图和文本频谱差和跨模态不一致可能会严重影响模型性能。为了解决这些挑战，我们提议ReCLIP，首先学习抑制视图和文本嵌入的投影空间，然后通过跨模态自适应学习，使用假标签，更新视图和文本编码器，细化标签，并逐步减少频谱差和不一致。经过广泛实验，我们表明ReCLIP可以将CLIP的平均错误率从30.17%降低至25.06%在22个图像分类任务上。
</details></li>
</ul>
<hr>
<h2 id="Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure"><a href="#Learning-from-Topology-Cosmological-Parameter-Estimation-from-the-Large-scale-Structure" class="headerlink" title="Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure"></a>Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02636">http://arxiv.org/abs/2308.02636</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jacky H. T. Yip, Adam Rouhiainen, Gary Shiu</li>
<li>for: 研究宇宙大规模结构的 cosmological 参数</li>
<li>methods: 使用神经网络模型从 persistency 图像中提取 cosmological 参数</li>
<li>results: 模型可以准确地估算 cosmological 参数，质量比传统 Bayesian 推理方法更高<details>
<summary>Abstract</summary>
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
</details>
<details>
<summary>摘要</summary>
宇宙大规模结构的Topology含有价值的 cosmological参数信息。 persistent homology 可以提取这些 topological 信息，但是最佳的方法 для参数估计仍然是一个开放的问题。为解决这个问题，我们提议一种神经网络模型，将 persistency 图像映射到 cosmological 参数。通过参数恢复测试，我们证明我们的模型可以做出准确和精确的估计，明显超过了传统的 Bayesian 推理方法。Note: "persistent homology" is translated as " persistency" in Simplified Chinese, which is a common abbreviation used in the field of topological data analysis.
</details></li>
</ul>
<hr>
<h2 id="MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities"><a href="#MM-Vet-Evaluating-Large-Multimodal-Models-for-Integrated-Capabilities" class="headerlink" title="MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"></a>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02490">http://arxiv.org/abs/2308.02490</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuweihao/mm-vet">https://github.com/yuweihao/mm-vet</a></li>
<li>paper_authors: Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang</li>
<li>For: The paper proposes an evaluation benchmark for large multimodal models (LMMs) to evaluate their ability to integrate different core vision-language (VL) capabilities and solve complicated multimodal tasks.* Methods: The paper presents MM-Vet, a benchmark that defines 6 core VL capabilities and examines 16 integrations of interest derived from capability combination. The paper also proposes an LLM-based evaluator for open-ended outputs, which enables the evaluation across different question types and answer styles.* Results: The paper evaluates representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. The results show that the proposed evaluator can provide a unified scoring metric for open-ended outputs, and the MM-Vet benchmark can help identify the strengths and weaknesses of different LMM systems.<details>
<summary>Abstract</summary>
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combination. For evaluation metrics, we propose an LLM-based evaluator for open-ended outputs. The evaluator enables the evaluation across different question types and answer styles, resulting in a unified scoring metric. We evaluate representative LMMs on MM-Vet, providing insights into the capabilities of different LMM system paradigms and models. Code and data are available at https://github.com/yuweihao/MM-Vet.
</details>
<details>
<summary>摘要</summary>
我们提出了 MM-Vet，一个评估板准 benchmark，用于测试大型多Modal模型（LMM）在复杂多Modal任务上的能力。现代LMM在不同的任务上表现出了各种感人的能力，如解释黑板上的数学问题，理解新闻图片中的事件和名人，以及解释视觉笑话。由于模型的快速发展，评估板准的开发受到了挑战。问题包括：(1)如何系统地结构化和评估复杂多Modal任务；(2)如何设计评估指标，可以在问题和答案类型之间具有一致性；以及(3)如何为模型提供更多的材料，而不仅仅是一个简单的性能排名。为此，我们提出了 MM-Vet，基于了核心视语言（VL）能力的总结。MM-Vet定义了6个核心VL能力，并评估了这些能力之间的16种 интеграción。为评估指标，我们提议了基于LLM的评估器，可以对开放式输出进行评估，并且可以在不同的问题类型和答案风格之间具有一致性。我们对代表性的LMM进行了 MM-Vet的评估，提供了不同模型系统和模型的能力的见解。代码和数据可以在 https://github.com/yuweihao/MM-Vet 上获取。
</details></li>
</ul>
<hr>
<h2 id="Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks"><a href="#Generation-of-Realistic-Synthetic-Raw-Radar-Data-for-Automated-Driving-Applications-using-Generative-Adversarial-Networks" class="headerlink" title="Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks"></a>Generation of Realistic Synthetic Raw Radar Data for Automated Driving Applications using Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02632">http://arxiv.org/abs/2308.02632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo C. Fidelis, Fabio Reway, Herick Y. S. Ribeiro, Pietro L. Campos, Werner Huber, Christian Icking, Lester A. Faria, Torsten Schön</li>
<li>for: 这个论文是为了模拟雷达数据的研究而写的。</li>
<li>methods: 这篇论文使用了生成对抗网络（GAN）来生成雷达数据。</li>
<li>results: 这个方法可以生成16个同时发射的雷达射频信号，并且可以用于进一步开发雷达数据处理算法（例如滤波和封装）。这可以增加数据的扩展和增强，例如生成不可能或安全关键的场景的数据，以便进行数据 augmentation。<details>
<summary>Abstract</summary>
The main approaches for simulating FMCW radar are based on ray tracing, which is usually computationally intensive and do not account for background noise. This work proposes a faster method for FMCW radar simulation capable of generating synthetic raw radar data using generative adversarial networks (GAN). The code and pre-trained weights are open-source and available on GitHub. This method generates 16 simultaneous chirps, which allows the generated data to be used for the further development of algorithms for processing radar data (filtering and clustering). This can increase the potential for data augmentation, e.g., by generating data in non-existent or safety-critical scenarios that are not reproducible in real life. In this work, the GAN was trained with radar measurements of a motorcycle and used to generate synthetic raw radar data of a motorcycle traveling in a straight line. For generating this data, the distance of the motorcycle and Gaussian noise are used as input to the neural network. The synthetic generated radar chirps were evaluated using the Frechet Inception Distance (FID). Then, the Range-Azimuth (RA) map is calculated twice: first, based on synthetic data using this GAN and, second, based on real data. Based on these RA maps, an algorithm with adaptive threshold and edge detection is used for object detection. The results have shown that the data is realistic in terms of coherent radar reflections of the motorcycle and background noise based on the comparison of chirps, the RA maps and the object detection results. Thus, the proposed method in this work has shown to minimize the simulation-to-reality gap for the generation of radar data.
</details>
<details>
<summary>摘要</summary>
主要方法 для模拟FMCW雷达是基于射线追踪，通常是计算昂贵的并不考虑背景噪声。这项工作提出了一种更快的FMCW雷达模拟方法，可以生成基于生成对抗网络（GAN）的合成雷达数据。代码和预训练 весов在GitHub上公开可用。这种方法生成了16个同时发射的毫声信号，这使得生成的数据可以用于进一步开发雷达数据处理算法（过滤和归一）。这可以增加数据增强的潜在性，例如通过生成不可能或安全关键的场景中的数据，以便在实际生产中不可能重现。在这项工作中，GAN被训练使用雷达测量数据，用于生成雷达数据的 sintetic raw radar chirps。为生成这些数据，雷达车辆的距离和高斯噪声作为神经网络的输入。生成的雷达毫声信号被评估使用Frechet InceptionDistance（FID）。然后，雷达距离-方位（RA）图被计算两次：首先，基于生成数据使用这个GAN，然后，基于实际数据。基于这些RA图，一种适应阈值和边检测算法用于物体检测。结果表明，生成的数据具有准确的干扰雷达反射和背景噪声，基于毫声信号、RA图和物体检测结果的比较。因此，该提出的方法在本工作中已经成功地减小了模拟到实际的差距。
</details></li>
</ul>
<hr>
<h2 id="BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks"><a href="#BlindSage-Label-Inference-Attacks-against-Node-level-Vertical-Federated-Graph-Neural-Networks" class="headerlink" title="BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks"></a>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02465">http://arxiv.org/abs/2308.02465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Arazzi, Mauro Conti, Stefanos Koffas, Marina Krcek, Antonino Nocera, Stjepan Picek, Jing Xu</li>
<li>for: 这篇论文的主要目的是研究 vertical federated learning（VFL）中的标签推理攻击，特别是在没有背景知识的情况下进行攻击。</li>
<li>methods: 本文使用了 zero-background knowledge 策略来进行攻击，具体是使用 Graph Neural Networks（GNNs）作为目标模型，并在 node classification 任务上进行了实验。</li>
<li>results: 本文的 proposed attack，BlindSage，在实验中实现了 nearly 100% 的准确率，甚至在攻击者没有任何背景知识的情况下仍能实现准确率高于 85%。此外，本文发现了一些常见的防御措施不能对抗本攻击，而且这些防御措施会对模型的表现造成负面影响。<details>
<summary>Abstract</summary>
Federated learning enables collaborative training of machine learning models by keeping the raw data of the involved workers private. One of its main objectives is to improve the models' privacy, security, and scalability. Vertical Federated Learning (VFL) offers an efficient cross-silo setting where a few parties collaboratively train a model without sharing the same features. In such a scenario, classification labels are commonly considered sensitive information held exclusively by one (active) party, while other (passive) parties use only their local information. Recent works have uncovered important flaws of VFL, leading to possible label inference attacks under the assumption that the attacker has some, even limited, background knowledge on the relation between labels and data. In this work, we are the first (to the best of our knowledge) to investigate label inference attacks on VFL using a zero-background knowledge strategy. To concretely formulate our proposal, we focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL. In particular, we refer to node classification tasks, which are widely studied, and GNNs have shown promising results. Our proposed attack, BlindSage, provides impressive results in the experiments, achieving nearly 100% accuracy in most cases. Even when the attacker has no information about the used architecture or the number of classes, the accuracy remained above 85% in most instances. Finally, we observe that well-known defenses cannot mitigate our attack without affecting the model's performance on the main classification task.
</details>
<details>
<summary>摘要</summary>
合作学习（Federated Learning）可以保持参与者的原始数据加密，以提高模型的隐私、安全性和可扩展性。垂直联合学习（VFL）提供了跨存储设置，在不共享同一个特征的情况下，几个党合作训练模型。在这种场景下，分类标签通常被视为活动党拥有的敏感信息，而其他投降党仅使用本地信息。现有研究曝光了VFL的重要漏洞，可能导致标签推理攻击，假设攻击者具有一定的背景知识关于标签和数据之间的关系。在这种情况下，我们是第一个（到我们知道的最佳）investigate VFL中的标签推理攻击，使用零背景知识策略。为了具体实现我们的提议，我们将Graph Neural Networks（GNNs）作为目标模型，特别是节点分类任务，这是广泛研究的领域，GNNs在这些任务上表现出色。我们提出的攻击方法，BlindSage，在实验中提供了很好的结果，在大多数情况下达到了nearly 100%的准确率。即使攻击者没有关于使用的架构或分类数量的任何信息，我们的攻击仍然在大多数情况下保持了超过85%的准确率。最后，我们发现了一些常见的防御无法防止我们的攻击，而不会影响模型在主要分类任务上的性能。
</details></li>
</ul>
<hr>
<h2 id="Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing"><a href="#Universal-Approximation-of-Linear-Time-Invariant-LTI-Systems-through-RNNs-Power-of-Randomness-in-Reservoir-Computing" class="headerlink" title="Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing"></a>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02464">http://arxiv.org/abs/2308.02464</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Jere, Lizhong Zheng, Karim Said, Lingjia Liu<br>for:RC is used to overcome the issues of vanishing and exploding gradients in standard RNN training, and it has demonstrated superior empirical performance in various fields. However, the theoretical grounding for this observed performance has not been fully developed.methods:The paper uses a signal processing interpretation of RC to universally approximate a general LTI system. The optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC is derived, and extensive numerical evaluations are provided to validate the optimality of the derived distribution.results:The paper shows that RC can universally approximate a general LTI system, and provides a clear signal processing-based model interpretability of RC. The derived optimal probability distribution function for the recurrent weights of the RC is found to be effective in simulating the LTI system. The work provides a complete analytical characterization of the untrained recurrent weights, which is important for explainable machine learning applications where training samples are limited.<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal processing interpretation of RC and utilize this understanding in the problem of simulating a generic LTI system through RC. Under this setup, we analytically characterize the optimal probability distribution function for generating the recurrent weights of the underlying RNN of the RC. We provide extensive numerical evaluations to validate the optimality of the derived optimum distribution of the recurrent weights of the RC for the LTI system simulation problem. Our work results in clear signal processing-based model interpretability of RC and provides theoretical explanation for the power of randomness in setting instead of training RC's recurrent weights. It further provides a complete optimum analytical characterization for the untrained recurrent weights, marking an important step towards explainable machine learning (XML) which is extremely important for applications where training samples are limited.
</details>
<details>
<summary>摘要</summary>
循环神经网络（RNN）是已知的通用函数近似器，可以处理时间信息。然而，标准的RNN训练过程中通常会出现消失和扩散梯度的问题。宽泛计算（RC）是一种特殊的RNN，其循环权重随机并未训练，可以解决这些问题，并在自然语言处理和无线通信等领域展现出优于标准RNN的实际表现。然而，对于RC的理论基础的发展并没有与实际表现相同的速度。在这个工作中，我们证明了RNN可以 universally approximate linear time-invariant（LTI）系统。具体来说，我们证明了RC可以 universally approximate任何LTI系统。我们提供了一个清晰的信号处理解释，用于理解RC的工作原理，并使用这种理解来解决一个通用LTI系统的模拟问题。在这个设置下，我们分析性地 caracterize了RC的优化梯度的概率分布。我们进行了广泛的数值评估，以验证我们所 derivated的RC的优化梯度概率分布的优化性。我们的工作具有以下优点：1. 我们提供了一个信号处理基于的RC模型解释，具有明确的数学基础。2. 我们提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。3. 我们的结论可以用于解释Randomness在RC中的作用，并且提供了一个完整的优化梯度概率分布的分析。4. 我们的工作对于Explainable Machine Learning（XML）的发展具有重要意义，特别是在训练样本数量有限的情况下。总之，我们的工作提供了一个信号处理基于的RC模型解释，并且提供了一个可靠的数值评估，以验证RC的优化梯度概率分布的优化性。此外，我们的结论可以用于解释Randomness在RC中的作用，并且对于XML的发展具有重要意义。
</details></li>
</ul>
<hr>
<h2 id="Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning"><a href="#Fast-and-Accurate-Reduced-Order-Modeling-of-a-MOOSE-based-Additive-Manufacturing-Model-with-Operator-Learning" class="headerlink" title="Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning"></a>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02462">http://arxiv.org/abs/2308.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Yaseen, Dewen Yushu, Peter German, Xu Wu</li>
<li>for: 本研究目的是为了提高additive manufacturing（AM）过程中的物料特性，通过 manipulate 生产过程参数以实现特定的物料特性。</li>
<li>methods: 本研究使用了Operator Learning（OL）方法，通过学习减少了过程变量的 differential equation 家族，以建立快速和准确的减少模型（ROM）。</li>
<li>results: 研究发现，OL方法可以与传统的深度神经网络（DNN）相比，在预测scalar模型响应时提供了相似的性能，且在精度和泛化性方面甚至超越DNN。DNN基于的ROM具有最快的训练时间，但是所有的ROM都比原始的MOOSE模型快速，且仍提供了准确的预测。FNO在预测时间序数据方面具有较小的平均预测误差，但是DeepONet具有较大的变化。不同于DNN，FNO和DeepONet都可以无需维度减少技术来预测时间序数据。<details>
<summary>Abstract</summary>
One predominant challenge in additive manufacturing (AM) is to achieve specific material properties by manipulating manufacturing process parameters during the runtime. Such manipulation tends to increase the computational load imposed on existing simulation tools employed in AM. The goal of the present work is to construct a fast and accurate reduced-order model (ROM) for an AM model developed within the Multiphysics Object-Oriented Simulation Environment (MOOSE) framework, ultimately reducing the time/cost of AM control and optimization processes. Our adoption of the operator learning (OL) approach enabled us to learn a family of differential equations produced by altering process variables in the laser's Gaussian point heat source. More specifically, we used the Fourier neural operator (FNO) and deep operator network (DeepONet) to develop ROMs for time-dependent responses. Furthermore, we benchmarked the performance of these OL methods against a conventional deep neural network (DNN)-based ROM. Ultimately, we found that OL methods offer comparable performance and, in terms of accuracy and generalizability, even outperform DNN at predicting scalar model responses. The DNN-based ROM afforded the fastest training time. Furthermore, all the ROMs were faster than the original MOOSE model yet still provided accurate predictions. FNO had a smaller mean prediction error than DeepONet, with a larger variance for time-dependent responses. Unlike DNN, both FNO and DeepONet were able to simulate time series data without the need for dimensionality reduction techniques. The present work can help facilitate the AM optimization process by enabling faster execution of simulation tools while still preserving evaluation accuracy.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在添加制造（AM）是在运行时控制和优化过程中实现特定材料性能。这种控制通常会增加现有的Simulation工具在AM中的计算负担。目标是在MOOSE框架中开发一个快速和准确的减少维度模型（ROM），以降低AM控制和优化过程的时间/成本。我们采用了运算学（OL）方法，通过修改过程变量来学习激光的 Gaussian点热源生成的家族 diffeqential equations。我们使用了Fourier neural operator（FNO）和深度运算网络（DeepONet）来开发ROMs，并对这些OL方法与传统的深度神经网络（DNN）基于ROM进行比较。结果表明，OL方法可以与DNN相比，具有相同的性能和精度，并且在预测批量响应方面更高一点。DNN基于ROM培训时间最快，但所有ROM都比原始MOOSE模型更快，并且仍然提供了准确的预测。FNO的平均预测误差较小， DeepONet在时间相对应的响应中有较大的变化。不同于DNN，FNO和DeepONet都可以不需要维度减少技术来预测时间序列数据。现有的工作可以帮助加快AM优化过程中的Simulation工具执行，保持评估准确性。
</details></li>
</ul>
<hr>
<h2 id="Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration"><a href="#Nonprehensile-Planar-Manipulation-through-Reinforcement-Learning-with-Multimodal-Categorical-Exploration" class="headerlink" title="Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration"></a>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02459">http://arxiv.org/abs/2308.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar</li>
<li>for: 开发能够实现dexterous nonprehensile manipulation的 робот控制器，如pushing an object on a table。</li>
<li>methods: 使用Reinforcement Learning（RL）框架，并提出了多模态探索方法，以处理非线性和不确定性。</li>
<li>results: 实现了高精度、非平滑曲线和复杂运动的推动政策，并且可以承受外部干扰和观测噪声，同时也可以在多个推动器情况下进行缩放。<details>
<summary>Abstract</summary>
Developing robot controllers capable of achieving dexterous nonprehensile manipulation, such as pushing an object on a table, is challenging. The underactuated and hybrid-dynamics nature of the problem, further complicated by the uncertainty resulting from the frictional interactions, requires sophisticated control behaviors. Reinforcement Learning (RL) is a powerful framework for developing such robot controllers. However, previous RL literature addressing the nonprehensile pushing task achieves low accuracy, non-smooth trajectories, and only simple motions, i.e. without rotation of the manipulated object. We conjecture that previously used unimodal exploration strategies fail to capture the inherent hybrid-dynamics of the task, arising from the different possible contact interaction modes between the robot and the object, such as sticking, sliding, and separation. In this work, we propose a multimodal exploration approach through categorical distributions, which enables us to train planar pushing RL policies for arbitrary starting and target object poses, i.e. positions and orientations, and with improved accuracy. We show that the learned policies are robust to external disturbances and observation noise, and scale to tasks with multiple pushers. Furthermore, we validate the transferability of the learned policies, trained entirely in simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our supplemental video: https://youtu.be/vTdva1mgrk4.
</details>
<details>
<summary>摘要</summary>
开发能够实现灵活无握 manipulate robot控制器，如表面上推pushing一个物体，是一项挑战。由于控制器的下降启动和混合动力学性质，以及由摩擦产生的不确定性，需要复杂的控制行为。 reinforcement learning (RL) 是一个强大的框架 для开发这类 robot控制器。然而，过去RL文献中对非握持推动任务的精度、非精炸曲线和简单运动（即不含旋转）很低。我们 conjecture  previous 使用单模态探索策略 failed  to capture 任务的内在混合动力学性质， arise  from 不同的接触交互方式 between the robot and the object， such as sticking, sliding, and separation.在这种工作中，我们提议使用多模态探索方法，通过 categorical distributions，可以训练平面推动 RL 政策，对于任意开始和目标物体姿态（位置和 orientations），并具有改进的精度。我们显示了学习的策略对于外部干扰和观察噪声具有Robustness，并可扩展到多个推动者任务。此外，我们验证了学习策略，完全在 simulator 中训练，在物理 Kuka iiwa 机械臂上运行。请参考我们的补充视频：https://youtu.be/vTdva1mgrk4.
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction"><a href="#Uncertainty-Estimation-and-Propagation-in-Accelerated-MRI-Reconstruction" class="headerlink" title="Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction"></a>Uncertainty Estimation and Propagation in Accelerated MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02631">http://arxiv.org/abs/2308.02631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paulkogni/mr-recon-uq">https://github.com/paulkogni/mr-recon-uq</a></li>
<li>paper_authors: Paul Fischer, Thomas Küstner, Christian F. Baumgartner</li>
<li>for: 这篇论文是关于基于深度学习的MRI重建技术的研究，尤其是在高速 Settings下 achievable 的重建质量。</li>
<li>methods: 该论文提出了一种新的 probabilistic reconstruction technique (PHiRec)，基于conditional hierarchical variational autoencoders的想法。</li>
<li>results: 该方法可以生成高质量的重建结果，同时也可以提供substantially better calibrated的uncertainty quantification，以及可以协调到下游 segmentation 任务中的uncertainty estimate。<details>
<summary>Abstract</summary>
MRI reconstruction techniques based on deep learning have led to unprecedented reconstruction quality especially in highly accelerated settings. However, deep learning techniques are also known to fail unexpectedly and hallucinate structures. This is particularly problematic if reconstructions are directly used for downstream tasks such as real-time treatment guidance or automated extraction of clinical paramters (e.g. via segmentation). Well-calibrated uncertainty quantification will be a key ingredient for safe use of this technology in clinical practice. In this paper we propose a novel probabilistic reconstruction technique (PHiRec) building on the idea of conditional hierarchical variational autoencoders. We demonstrate that our proposed method produces high-quality reconstructions as well as uncertainty quantification that is substantially better calibrated than several strong baselines. We furthermore demonstrate how uncertainties arising in the MR econstruction can be propagated to a downstream segmentation task, and show that PHiRec also allows well-calibrated estimation of segmentation uncertainties that originated in the MR reconstruction process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation"><a href="#Generative-Modelling-of-Levy-Area-for-High-Order-SDE-Simulation" class="headerlink" title="Generative Modelling of Lévy Area for High Order SDE Simulation"></a>Generative Modelling of Lévy Area for High Order SDE Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02452">http://arxiv.org/abs/2308.02452</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andraž Jelinčič, Jiajie Tao, William F. Turner, Thomas Cass, James Foster, Hao Ni</li>
<li>for: 这篇论文是为了提出一种基于深度学习的模型，用于生成精确的莱比零区 conditional on  Брау内幂增量。</li>
<li>methods: 这种模型使用了一种专门设计的 GNN-inspired 架构，以保证输出分布和 conditioning 变量之间的正确依赖关系。同时，使用了一种基于特征函数的数学原理的 discriminator。</li>
<li>results: 对于 4 维 Брау内幂，这种模型 exhibits state-of-the-art 性能 across 多个维度，并且在数学金融中的 log-Heston 模型中进行了一个数值实验，证明了高质量的 synthetic 莱比零区可以导致高阶弱 convergence 和 variance reduction when using multilevel Monte Carlo (MLMC)。<details>
<summary>Abstract</summary>
It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.   In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discriminator. Lastly, we introduce a novel training mechanism termed "Chen-training", which circumvents the need for expensive-to-generate training data-sets. This new training procedure is underpinned by our two main theoretical results.   For 4-dimensional Brownian motion, we show that L\'{e}vyGAN exhibits state-of-the-art performance across several metrics which measure both the joint and marginal distributions. We conclude with a numerical experiment on the log-Heston model, a popular SDE in mathematical finance, demonstrating that high-quality synthetic L\'{e}vy area can lead to high order weak convergence and variance reduction when using multilevel Monte Carlo (MLMC).
</details>
<details>
<summary>摘要</summary>
它已经广泛知道，当数值实现解决涨落方程时，以较好的减法速率（即幂函数）为依据，需要使用某些迭代积分的布朗运动，通常称为其"Lévy区域"。然而，这些随机积分具有非高斯性质，而且为高维布朗运动（d > 2），没有快速准确样本生成算法。在这篇论文中，我们提出了LévyGAN，一种基于深度学习的模型，用于生成 conditional Lévy area 的相似样本。由于我们的 "桥跃" 操作，输出样本满足所有的共同偶极值和条件偶极值。我们的生成器采用了特制的 GNN-inspired 架构，以保证输出分布和条件变量之间的正确依赖关系。此外，我们采用了基于特征函数的数学原理的批量分类器。最后，我们介绍了一种新的训练机制，称为 "Chen-training"，它使得不需要生成昂贵的训练数据集。这种新的训练过程基于我们的两个主要理论结论。对于四维布朗运动，我们表明了LévyGAN在多个维度上的性能都达到了状态机器人的水平。我们结束于一个对柯本方程（一种流行的数学金融方程）的数字实验，展示了高质量的 synthetic Lévy area 可以导致高阶弱整合和变量减少，当使用多层 Monte Carlo（MLMC）时。
</details></li>
</ul>
<hr>
<h2 id="Pruning-a-neural-network-using-Bayesian-inference"><a href="#Pruning-a-neural-network-using-Bayesian-inference" class="headerlink" title="Pruning a neural network using Bayesian inference"></a>Pruning a neural network using Bayesian inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02451">http://arxiv.org/abs/2308.02451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sunil Mathew, Daniel B. Rowe</li>
<li>for: 这个研究论文的目的是提出一种基于 Bayesian 推理的神经网络减少技术，以降低大神经网络的计算和存储占用。</li>
<li>methods: 该方法使用 Bayesian 推理，将神经网络的 posterior 概率分布计算到减少前和减少后，然后利用这些概率导向 iterative 减少。</li>
<li>results: 经过对多个 benchmarck 进行全面评估，我们表明了我们的方法可以实现 жела的稀有性，同时保持竞争性的准确率。<details>
<summary>Abstract</summary>
Neural network pruning is a highly effective technique aimed at reducing the computational and memory demands of large neural networks. In this research paper, we present a novel approach to pruning neural networks utilizing Bayesian inference, which can seamlessly integrate into the training procedure. Our proposed method leverages the posterior probabilities of the neural network prior to and following pruning, enabling the calculation of Bayes factors. The calculated Bayes factors guide the iterative pruning. Through comprehensive evaluations conducted on multiple benchmarks, we demonstrate that our method achieves desired levels of sparsity while maintaining competitive accuracy.
</details>
<details>
<summary>摘要</summary>
大脑网络剪辑是一种非常有效的技术，用于减少大脑网络的计算和内存占用。在这篇研究报告中，我们提出了一种基于 bayesian 推理的 neural network 剪辑方法，可以轻松地 integrate 到训练过程中。我们的提议方法利用 neural network 之前和之后剪辑 posterior 概率，以计算 bayes 因子。计算出的 bayes 因子导引了迭代剪辑。我们在多个 benchmark 上进行了广泛的评估，并证明了我们的方法可以达到所需的稀疏程度，同时保持竞争性的准确率。
</details></li>
</ul>
<hr>
<h2 id="From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence"><a href="#From-Military-to-Healthcare-Adopting-and-Expanding-Ethical-Principles-for-Generative-Artificial-Intelligence" class="headerlink" title="From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence"></a>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02448">http://arxiv.org/abs/2308.02448</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Oniani, Jordan Hilsman, Yifan Peng, COL, Ronald K. Poropatich, COL Jeremy C. Pamplin, LTC Gary L. Legault, Yanshan Wang</li>
<li>For: The paper aims to propose ethical principles for the use of generative AI in healthcare, with the goal of addressing ethical dilemmas and challenges in the integration of this technology in the medical field.* Methods: The paper uses a framework called GREAT PLEA, which stands for Governance, Reliability, Equity, Accountability, Traceability, Privacy, Lawfulness, Empathy, and Autonomy, to guide the development of ethical principles for generative AI in healthcare.* Results: The paper proposes a set of ethical principles for generative AI in healthcare, with the goal of proactively addressing the ethical dilemmas and challenges posed by the integration of this technology in the medical field. These principles include governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy.<details>
<summary>Abstract</summary>
In 2020, the U.S. Department of Defense officially disclosed a set of ethical principles to guide the use of Artificial Intelligence (AI) technologies on future battlefields. Despite stark differences, there are core similarities between the military and medical service. Warriors on battlefields often face life-altering circumstances that require quick decision-making. Medical providers experience similar challenges in a rapidly changing healthcare environment, such as in the emergency department or during surgery treating a life-threatening condition. Generative AI, an emerging technology designed to efficiently generate valuable information, holds great promise. As computing power becomes more accessible and the abundance of health data, such as electronic health records, electrocardiograms, and medical images, increases, it is inevitable that healthcare will be revolutionized by this technology. Recently, generative AI has captivated the research community, leading to debates about its application in healthcare, mainly due to concerns about transparency and related issues. Meanwhile, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. However, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI. In this paper, we propose GREAT PLEA ethical principles, encompassing governance, reliability, equity, accountability, traceability, privacy, lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to proactively address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details>
<details>
<summary>摘要</summary>
在2020年，美国国防部官方公布了一组伦理原则，用于导引人工智能技术在未来战场上的使用。尽管在不同的情况下，军人和医疗人员之间有 stark 的不同，但是在面临生命改变的情况下，他们都需要快速做出决策。战场上的战士常常面临生命改变的情况，需要快速做出决策。医疗人员在医疗环境中也经常面临快速变化的情况，如在急诊室或在治疗生命危险的情况下。新兴的生成型人工智能技术在计算能力的提高和医疗数据的增加下，将健康卫生领域 revolutionized。 recent years, this technology has attracted significant attention from the research community, leading to debates about its application in healthcare, primarily due to concerns about transparency and related issues. However, concerns about the potential exacerbation of health disparities due to modeling biases have raised notable ethical concerns regarding the use of this technology in healthcare. Despite this, the ethical principles for generative AI in healthcare have been understudied, and decision-makers often fail to consider the significance of generative AI.在这篇论文中，我们提出了 GREAT PLEA 伦理原则，包括政府、可靠性、公平性、责任、可追溯性、隐私、法律合法性、 Empathy 和自主权，用于生成型人工智能技术在健康卫生领域中的应用。我们希望通过这些原则，active address the ethical dilemmas and challenges posed by the integration of generative AI in healthcare.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness"><a href="#Adaptive-Preferential-Attached-kNN-Graph-with-Distribution-Awareness" class="headerlink" title="Adaptive Preferential Attached kNN Graph with Distribution-Awareness"></a>Adaptive Preferential Attached kNN Graph with Distribution-Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02442">http://arxiv.org/abs/2308.02442</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/4alexmin/knnsotas">https://github.com/4alexmin/knnsotas</a></li>
<li>paper_authors: Shaojie Min, Ji Liu</li>
<li>for: 提高机器学习任务中的总体性能和精度，特别是在具有复杂分布的实际数据上。</li>
<li>methods: 基于分布情况的 adaptive-k 技术，在建构图时采用分布信息作为一体。</li>
<li>results: 在多个实际数据集上进行了严格的评估，并证明了 paNNG 在不同场景下的适应性和效果优于现有算法。<details>
<summary>Abstract</summary>
Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks due to their simplicity and effectiveness. However, as factual data often inherit complex distributions, the conventional kNN graph's reliance on a unified k-value can hinder its performance. A crucial factor behind this challenge is the presence of ambiguous samples along decision boundaries that are inevitably more prone to incorrect classifications. To address the situation, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which adopts distribution-aware adaptive-k into graph construction. By incorporating distribution information as a cohesive entity, paNNG can significantly improve performance on ambiguous samples by "pulling" them towards their original classes and hence enhance overall generalization capability. Through rigorous evaluations on diverse datasets, paNNG outperforms state-of-the-art algorithms, showcasing its adaptability and efficacy across various real-world scenarios.
</details>
<details>
<summary>摘要</summary>
基于图的kNN算法在机器学习任务中广泛受欢迎，因为它们简单易用而且有效。然而，由于实际数据经常具有复杂的分布，传统的kNN图中对一个固定的k值的依赖可能会降低其表现。这个挑战的关键原因在于具有抽象样本的异常高错误率，这些样本通常位于分类边界附近。为解决这个问题，我们提出了Preferential Attached k-Nearest Neighbors Graph（paNNG），该算法在图struc图构建中采用了分布情况的敏感性。通过将分布信息作为一个整体纳入图构建，paNNG可以在抽象样本上提高表现，“拖”这些样本向其原来的类别，从而提高总的泛化能力。经过严谨的评估，paNNG在多种实际场景中舒适地超越了当前的状态árt算法，示出了它的适应性和效果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/05/cs.LG_2023_08_05/" data-id="cllsk9gq8002w9c88gc8g2sx0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/cs.SD_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/cs.SD_2023_08_05/">cs.SD - 2023-08-05 123:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging"><a href="#ApproBiVT-Lead-ASR-Models-to-Generalize-Better-Using-Approximated-Bias-Variance-Tradeoff-Guided-Early-Stopping-and-Checkpoint-Averaging" class="headerlink" title="ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging"></a>ApproBiVT: Lead ASR Models to Generalize Better Using Approximated Bias-Variance Tradeoff Guided Early Stopping and Checkpoint Averaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02870">http://arxiv.org/abs/2308.02870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangyuan Wang, Ming Hao, Yuhai Shi, Bo Xu</li>
<li>for: 提高自动语音识别（ASR）模型的性能，通过重新评估和更新早期停止和多个检查点的策略，以降低模型的总泛化误差。</li>
<li>methods: 使用适应停止和多个检查点的策略，通过评估训练损失和验证损失来评估模型的偏差和异谱，并通过Approximated Bias-Variance Tradeoff（ApproBiVT）来导引这些策略。</li>
<li>results: 在使用高级ASR模型时，该策略可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。<details>
<summary>Abstract</summary>
The conventional recipe for Automatic Speech Recognition (ASR) models is to 1) train multiple checkpoints on a training set while relying on a validation set to prevent overfitting using early stopping and 2) average several last checkpoints or that of the lowest validation losses to obtain the final model. In this paper, we rethink and update the early stopping and checkpoint averaging from the perspective of the bias-variance tradeoff. Theoretically, the bias and variance represent the fitness and variability of a model and the tradeoff of them determines the overall generalization error. But, it's impractical to evaluate them precisely. As an alternative, we take the training loss and validation loss as proxies of bias and variance and guide the early stopping and checkpoint averaging using their tradeoff, namely an Approximated Bias-Variance Tradeoff (ApproBiVT). When evaluating with advanced ASR models, our recipe provides 2.5%-3.7% and 3.1%-4.6% CER reduction on the AISHELL-1 and AISHELL-2, respectively.
</details>
<details>
<summary>摘要</summary>
传统的自动语音识别（ASR）模型的制作方法是：1）在训练集上训练多个检查点，使用验证集来防止过拟合，并2）使用多个最后的检查点或验证损失最低的一个来获取最终模型。在这篇论文中，我们重新思考了早期停止和检查点平均的方法，从偏差-差异质量的角度来更新。在理论上，偏差和差异表示模型的适应度和多样性，它们之间的质量交换决定总的泛化误差。但是，不可能准确地评估它们。因此，我们使用训练损失和验证损失作为偏差和差异的代理，并通过它们之间的质量交换来导引早期停止和检查点平均，即 Approximated Bias-Variance Tradeoff（ApproBiVT）。在使用高级ASR模型进行评估时，我们的制作方法可以提供2.5%-3.7%和3.1%-4.6%的CER减少在AISHELL-1和AISHELL-2上。
</details></li>
</ul>
<hr>
<h2 id="A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis"><a href="#A-Systematic-Exploration-of-Joint-training-for-Singing-Voice-Synthesis" class="headerlink" title="A Systematic Exploration of Joint-training for Singing Voice Synthesis"></a>A Systematic Exploration of Joint-training for Singing Voice Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02867">http://arxiv.org/abs/2308.02867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuning Wu, Yifeng Yu, Jiatong Shi, Tao Qian, Qin Jin</li>
<li>for: 提高 singing voice synthesis（SVS）系统的性能和可读性。</li>
<li>methods: 通过对音响模型和 vocoder 进行共同训练，提高 SVS 系统的 JOINT 训练性能。</li>
<li>results: 通过实验表明，我们的 JOINT 训练策略在不同数据集上具有更稳定的性能提升，同时也提高了整个框架的可读性。<details>
<summary>Abstract</summary>
There has been a growing interest in using end-to-end acoustic models for singing voice synthesis (SVS). Typically, these models require an additional vocoder to transform the generated acoustic features into the final waveform. However, since the acoustic model and the vocoder are not jointly optimized, a gap can exist between the two models, leading to suboptimal performance. Although a similar problem has been addressed in the TTS systems by joint-training or by replacing acoustic features with a latent representation, adopting corresponding approaches to SVS is not an easy task. How to improve the joint-training of SVS systems has not been well explored. In this paper, we conduct a systematic investigation of how to better perform a joint-training of an acoustic model and a vocoder for SVS. We carry out extensive experiments and demonstrate that our joint-training strategy outperforms baselines, achieving more stable performance across different datasets while also increasing the interpretability of the entire framework.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>现在，使用端到端音声模型进行唱歌voice synthesis（SVS）已经有越来越多的兴趣。通常，这些模型需要一个额外的 vocoder 将生成的音声特征转换成最终波形。然而，由于音声模型和 vocoder 并没有同时优化，因此两个模型之间可能存在一个差距，导致表现不佳。虽然在 TTS 系统中Addressing 类似问题的方法已经被研究，但在 SVS 中采用相应的方法并不是一件容易的事情。如何改进 SVS 系统的 JOINT 训练方法尚未得到了充分的探索。在这篇论文中，我们进行了系统的调查，探讨了如何更好地进行 SVS 系统的 JOINT 训练。我们进行了广泛的实验，并证明了我们的 JOINT 训练策略在不同的数据集上表现更稳定，同时也提高了整个框架的可解释性。
</details></li>
</ul>
<hr>
<h2 id="Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching"><a href="#Bootstrapping-Contrastive-Learning-Enhanced-Music-Cold-Start-Matching" class="headerlink" title="Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching"></a>Bootstrapping Contrastive Learning Enhanced Music Cold-Start Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02844">http://arxiv.org/abs/2308.02844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinping Zhao, Ying Zhang, Qiang Xiao, Yuming Ren, Yingchun Yang</li>
<li>for: 这个论文主要针对的是音乐冷启动匹配任务，即给出一个冷启动歌曲请求，然后快速推送该歌曲到相关歌曲的听众中以进行温身。但是，关于这个任务的研究非常有限，因此本文将music冷启动匹配问题进行详细定义并提供一种方案。</li>
<li>methods: 在线下训练中，我们尝试通过歌曲内容特征来学习高质量的歌曲表示。然而，我们发现监督信号通常遵循力学律分布，导致表示学习受到扭曲。为解决这个问题，我们提出了一种新的对比学习方法 named Bootstrapping Contrastive Learning (BCL)，通过对比激活来强制高质量表示学习。</li>
<li>results: 在线下数据集和在线系统上进行了广泛的实验，并证明了我们的方法的有效性和高效性。目前，我们已经将其部署到NetEase Cloud Music上，影响了百万用户。代码将在未来发布。<details>
<summary>Abstract</summary>
We study a particular matching task we call Music Cold-Start Matching. In short, given a cold-start song request, we expect to retrieve songs with similar audiences and then fastly push the cold-start song to the audiences of the retrieved songs to warm up it. However, there are hardly any studies done on this task. Therefore, in this paper, we will formalize the problem of Music Cold-Start Matching detailedly and give a scheme. During the offline training, we attempt to learn high-quality song representations based on song content features. But, we find supervision signals typically follow power-law distribution causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization. During the online serving, to locate the target audiences more accurately, we propose Clustering-based Audience Targeting (CAT) that clusters audience representations to acquire a few cluster centroids and then locate the target audiences by measuring the relevance between the audience representations and the cluster centroids. Extensive experiments on the offline dataset and online system demonstrate the effectiveness and efficiency of our method. Currently, we have deployed it on NetEase Cloud Music, affecting millions of users. Code will be released in the future.
</details>
<details>
<summary>摘要</summary>
我们研究一个名为音乐冷启始匹配的特定任务。简而言之，给定一个冷启始歌曲请求，我们希望可以检索到与其相似的听众，然后快速推送冷启始歌曲到检索到的听众中，以便让其热身。然而，目前 hardly any studies have been conducted on this task.因此，在这篇论文中，我们将 Music Cold-Start Matching 问题进行详细化 formalization，并提出一种方案。在线上训练中，我们尝试学习高质量的歌曲表示，基于歌曲内容特征。然而，我们发现超级vision signals  Typically follow a power-law distribution, causing skewed representation learning. To address this issue, we propose a novel contrastive learning paradigm named Bootstrapping Contrastive Learning (BCL) to enhance the quality of learned representations by exerting contrastive regularization.在在线服务中，我们提出一种名为 Clustering-based Audience Targeting (CAT) 的方法，通过对听众表示进行 clustering，获得一些集中点，然后通过测量听众表示和集中点之间的相似性，准确地定位目标听众。我们对做了大量的实验，证明了我们的方法的有效性和高效性。目前，我们已经将其部署到 NetEase Cloud Music，影响了数百万用户。代码将在未来发布。
</details></li>
</ul>
<hr>
<h2 id="Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision"><a href="#Self-Distillation-Network-with-Ensemble-Prototypes-Learning-Robust-Speaker-Representations-without-Supervision" class="headerlink" title="Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision"></a>Self-Distillation Network with Ensemble Prototypes: Learning Robust Speaker Representations without Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02774">http://arxiv.org/abs/2308.02774</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alibaba-damo-academy/3D-Speaker">https://github.com/alibaba-damo-academy/3D-Speaker</a></li>
<li>paper_authors: Yafeng Chen, Siqi Zheng, Qian Chen</li>
<li>for: 本研究旨在无需使用说话人标签进行说话人验证系统的自我学习和鲁棒化。</li>
<li>methods: 本文提出了一种高效的Self-Distillation网络 Ensemble Prototypes（SDEP），用于实现无监督的说话人表示学习。</li>
<li>results: 在VoxCeleb数据集上进行了多种实验，并证明了SDEP框架在说话人验证中具有最高精度。SDEP在Voxceleb1 speaker verification评价标准上达到了新的最佳性能（即错误率1.94%、1.99%和3.77%），不使用任何说话人标签 durante el entrenamiento。<details>
<summary>Abstract</summary>
Training speaker-discriminative and robust speaker verification systems without speaker labels is still challenging and worthwhile to explore. Previous studies have noted a substantial performance disparity between self-supervised and fully supervised approaches. In this paper, we propose an effective Self-Distillation network with Ensemble Prototypes (SDEP) to facilitate self-supervised speaker representation learning. A range of experiments conducted on the VoxCeleb datasets demonstrate the superiority of the SDEP framework in speaker verification. SDEP achieves a new SOTA on Voxceleb1 speaker verification evaluation benchmark ( i.e., equal error rate 1.94\%, 1.99\%, and 3.77\% for trial Vox1-O, Vox1-E and Vox1-H , respectively), discarding any speaker labels in the training phase. Code will be publicly available at https://github.com/alibaba-damo-academy/3D-Speaker.
</details>
<details>
<summary>摘要</summary>
<SYS>使用无标签的语音训练SpeakerVerification系统仍然是一项挑战性的任务，值得进行探索。过去的研究表明，自动学习和完全监督方法之间存在很大的性能差距。本文提出了一种高效的Self-Distillation网络加ensemble Prototypes（SDEP），用于实现无标签语音表示学习。在VoxCeleb dataset上进行了多种实验，得到了SDEP框架在Speaker Verification中的新最佳性能（即Voxceleb1的误差率为1.94%、1.99%和3.77%），不使用任何语音标签 during training phase。代码将在https://github.com/alibaba-damo-academy/3D-Speaker上公开。</SYS>Note: "SDEP" stands for "Self-Distillation network with Ensemble Prototypes".
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/05/cs.SD_2023_08_05/" data-id="cllsk9gqw00549c8888tu6dte" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_08_05" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/05/eess.IV_2023_08_05/" class="article-date">
  <time datetime="2023-08-04T16:00:00.000Z" itemprop="datePublished">2023-08-05</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/05/eess.IV_2023_08_05/">eess.IV - 2023-08-05 17:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm"><a href="#Flashlight-Search-Medial-Axis-A-Pixel-Free-Pore-Network-Extraction-Algorithm" class="headerlink" title="Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm"></a>Flashlight Search Medial Axis: A Pixel-Free Pore-Network Extraction Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.10990">http://arxiv.org/abs/2308.10990</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Liu, Tao Zhang, Shuyu Sun</li>
<li>for: 该研究旨在提出一种无像素的维持精度的脉径网络EXTRACTION方法，以便在大规模的含材质媒体中实现流体流动的研究。</li>
<li>methods: 该方法基于探灯搜索中心轴（FSMA）算法，在维持精度的情况下大幅降低计算复杂性，可以应用于多种不同的含材质媒体和流体流动情况。</li>
<li>results: 实验结果表明，FSMA算法可以准确地找到脉径网络，无论含材质媒体的结构如何，也无论脉径和喉径中心的位置如何。此外，该算法还可以检测死端脉径，这对于多相流动在含材质媒体中的研究非常重要。<details>
<summary>Abstract</summary>
Pore-network models (PNMs) have become an important tool in the study of fluid flow in porous media over the last few decades, and the accuracy of their results highly depends on the extraction of pore networks. Traditional methods of pore-network extraction are based on pixels and require images with high quality. Here, a pixel-free method called the flashlight search medial axis (FSMA) algorithm is proposed for pore-network extraction in a continuous space. The search domain in a two-dimensional space is a line, whereas a surface domain is searched in a three-dimensional scenario. Thus, the FSMA algorithm follows the dimensionality reduction idea; the medial axis can be identified using only a few points instead of calculating every point in the void space. In this way, computational complexity of this method is greatly reduced compared to that of traditional pixel-based extraction methods, thus enabling large-scale pore-network extraction. Based on cases featuring two- and three-dimensional porous media, the FSMA algorithm performs well regardless of the topological structure of the pore network or the positions of the pore and throat centers. This algorithm can also be used to examine both closed- and open-boundary cases. Finally, the FSMA algorithm can search dead-end pores, which is of great significance in the study of multiphase flow in porous media.
</details>
<details>
<summary>摘要</summary>
PORE-NETWORK MODELS (PNMs) 已经在过去几十年内成为论 fluid flow 在孔隙媒体的研究工具，而实际结果的准确性很大程度上取决于破孔网络的提取。传统的破孔网络提取方法基于像素，需要高质量的图像。在这里，一种没有像素的方法 called  flashlight search medial axis (FSMA) 算法是用于破孔网络提取的。在两维空间中，搜索域是一条直线，而在三维情况下，搜索域是一个表面。因此，FSMA 算法遵循缩放空间的想法，通过仅在缺空间中标识中点而不是计算所有点。这样，FSMA 算法的计算复杂性被大幅降低，从而实现大规模的破孔网络提取。无论破孔网络的结构是二维还是三维，FSMA 算法都能够正确地提取破孔网络。此外，该算法还可以处理封闭边界和开放边界两种情况。最后，FSMA 算法还可以搜索死绕孔，这对多相流在孔隙媒体的研究非常重要。
</details></li>
</ul>
<hr>
<h2 id="Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation"><a href="#Landmark-Detection-using-Transformer-Toward-Robot-assisted-Nasal-Airway-Intubation" class="headerlink" title="Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation"></a>Landmark Detection using Transformer Toward Robot-assisted Nasal Airway Intubation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02845">http://arxiv.org/abs/2308.02845</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/conorlth/airway_intubation_landmarks_detection">https://github.com/conorlth/airway_intubation_landmarks_detection</a></li>
<li>paper_authors: Tianhang Liu, Hechen Li, Long Bai, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren</li>
<li>for: 该研究旨在提供一种高精度的自动标记检测方法，用于机器人协助的鼻腔插管。</li>
<li>methods: 该方法基于变换器（DeTR），并采用了可变DeTR和语义对齐匹配模块来检测鼻腔中的两个重要标记（鼻孔和肺膜）。</li>
<li>results: 实验结果表明，该方法可以具有竞争力的检测精度。<details>
<summary>Abstract</summary>
Robot-assisted airway intubation application needs high accuracy in locating targets and organs. Two vital landmarks, nostrils and glottis, can be detected during the intubation to accommodate the stages of nasal intubation. Automated landmark detection can provide accurate localization and quantitative evaluation. The Detection Transformer (DeTR) leads object detectors to a new paradigm with long-range dependence. However, current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.
</details>
<details>
<summary>摘要</summary>
机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。 however， current DeTR requires long iterations to converge, and does not perform well in detecting small objects. This paper proposes a transformer-based landmark detection solution with deformable DeTR and the semantic-aligned-matching module for detecting landmarks in robot-assisted intubation. The semantics aligner can effectively align the semantics of object queries and image features in the same embedding space using the most discriminative features. To evaluate the performance of our solution, we utilize a publicly accessible glottis dataset and automatically annotate a nostril detection dataset. The experimental results demonstrate our competitive performance in detection accuracy. Our code is publicly accessible.Here's the translation in Traditional Chinese:机器人协助气管插管应用需要高精度在目标和器官的位置检测。气管插管过程中可以检测到两个重要的特征点，即鼻孔和软颈椎。自动化特征点检测可以提供高精度的位置定位和量化评估。然而， current DeTR需要长迭代才能融合，并不能够检测小型物体。这篇论文提出了一个基于 transformer 的特征点检测解决方案，该解决方案包括扭转 DeTR 和对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩阵进行对预测矩�
</details></li>
</ul>
<hr>
<h2 id="Non-line-of-sight-reconstruction-via-structure-sparsity-regularization"><a href="#Non-line-of-sight-reconstruction-via-structure-sparsity-regularization" class="headerlink" title="Non-line-of-sight reconstruction via structure sparsity regularization"></a>Non-line-of-sight reconstruction via structure sparsity regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02782">http://arxiv.org/abs/2308.02782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Duolan Huang, Quan Chen, Zhun Wei, Rui Chen</li>
<li>for: 本研究旨在提高非线视场（NLOS）成像质量，使其能够应用于自动驾驶、机器人视觉、医疗成像、安全监测等领域。</li>
<li>methods: 本研究使用了结构稀热（SS）正则化方法，通过利用方向光束变换（DLCT）模型中的核心矩阵来捕捉方向性的隐藏信息，从而提高NLOS成像的稀热性。</li>
<li>results: 经过实验和synthetic数据的评估，提出的方法可以在短时间和低SNR情况下提供高质量的NLOS成像，并且超过了现有的重建算法，特别是在封闭物体探测方面表现出色。<details>
<summary>Abstract</summary>
Non-line-of-sight (NLOS) imaging allows for the imaging of objects around a corner, which enables potential applications in various fields such as autonomous driving, robotic vision, medical imaging, security monitoring, etc. However, the quality of reconstruction is challenged by low signal-noise-ratio (SNR) measurements. In this study, we present a regularization method, referred to as structure sparsity (SS) regularization, for denoising in NLOS reconstruction. By exploiting the prior knowledge of structure sparseness, we incorporate nuclear norm penalization into the cost function of directional light-cone transform (DLCT) model for NLOS imaging system. This incorporation effectively integrates the neighborhood information associated with the directional albedo, thereby facilitating the denoising process. Subsequently, the reconstruction is achieved by optimizing a directional albedo model with SS regularization using fast iterative shrinkage-thresholding algorithm. Notably, the robust reconstruction of occluded objects is observed. Through comprehensive evaluations conducted on both synthetic and experimental datasets, we demonstrate that the proposed approach yields high-quality reconstructions, surpassing the state-of-the-art reconstruction algorithms, especially in scenarios involving short exposure and low SNR measurements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement"><a href="#Dual-Degradation-Inspired-Deep-Unfolding-Network-for-Low-Light-Image-Enhancement" class="headerlink" title="Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement"></a>Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02776">http://arxiv.org/abs/2308.02776</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huake Wang, Xingsong Hou, Xiaoyang Yan<br>for:这篇论文的目的是提出一种基于深度恢复模型的低光照图像改进方法，以解决现有的低光照图像恢复方法往往强调黑盒网络的增强性能，而忽略了图像恢复模型的物理意义。methods:该论文提出了一种基于双层降低模型的深度 unfolding 网络（DASUNet），其中包括了构建双层降低模型（DDM），以便显式地模拟低光照图像的劣化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间中的降低特点来进行适应。为使提议方案可行，我们设计了一种交叉优化解决方案，并将其拓展到一个具体的深度网络中，以形成 DASUNet。results:对多个流行的低光照图像dataset进行了广泛的实验，并证明了 DASUNet 比 canon 状态的低光照图像恢复方法更有效。我们将源代码和预训练模型公开发布。<details>
<summary>Abstract</summary>
Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of low-light images. It learns two distinct image priors via considering degradation specificity between luminance and chrominance spaces. To make the proposed scheme tractable, we design an alternating optimization solution to solve the proposed DDM. Further, the designed solution is unfolded into a specified deep network, imitating the iteration updating rules, to form DASUNet. Local and long-range information are obtained by prior modeling module (PMM), inheriting the advantages of convolution and Transformer, to enhance the representation capability of dual degradation priors. Additionally, a space aggregation module (SAM) is presented to boost the interaction of two degradation models. Extensive experiments on multiple popular low-light image datasets validate the effectiveness of DASUNet compared to canonical state-of-the-art low-light image enhancement methods. Our source code and pretrained model will be publicly available.
</details>
<details>
<summary>摘要</summary>
尽管深度修剪模型在低光照图像改善方面已经取得了很大的进步，但大多数模型都强调了修剪性能的提高，而很少探讨修剪模型的物理意义。为了解决这个问题，我们提出了一种名为DASUNet的深度 unfolding 网络，用于低光照图像改善。具体来说，我们构建了一个双层降低模型（DDM），以显式地模拟低光照图像的衰化机制。DDM 学习了两个不同的图像假设，通过考虑颜色和灰度空间之间的特定降低特征来进行适应。为了使我们的方案可行，我们设计了一种交叉优化解决方案，并将其 unfolding 成一个具体的深度网络，以形成DASUNet。PMM 模块（假设模型）在维护本地和长距离信息的同时，继承了 convolution 和 Transformer 的优点，以提高修剪两个假设的表示能力。此外，我们还提出了一种空间聚合模块（SAM），以提高两个降低模型之间的交互。我们在多个流行的低光照图像数据集上进行了广泛的实验，并证明了 DASUNet 的效果比 canonical 状态的低光照图像修剪方法更高。我们将源代码和预训练模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial"><a href="#Incorporation-of-Eye-Tracking-and-Gaze-Feedback-to-Characterize-and-Improve-Radiologist-Search-Patterns-of-Chest-X-rays-A-Randomized-Controlled-Clinical-Trial" class="headerlink" title="Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial"></a>Incorporation of Eye-Tracking and Gaze Feedback to Characterize and Improve Radiologist Search Patterns of Chest X-rays: A Randomized Controlled Clinical Trial</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.06280">http://arxiv.org/abs/2308.06280</a></li>
<li>repo_url: None</li>
<li>paper_authors: Carolina Ramirez-Tamayo, Syed Hasib Akhter Faruqui, Stanford Martinez, Angel Brisco, Nicholas Czarnek, Adel Alaeddini, Jeffrey R. Mock, Edward J. Golob, Kal L. Clark</li>
<li>for:  This study aimed to improve the accuracy of radiologists in detecting suspicious pulmonary nodules.</li>
<li>methods: The study used eye-tracking technology to analyze radiologists’ search patterns and provided automated feedback to the intervention group.</li>
<li>results: The intervention group showed a 38.89% absolute improvement in detecting suspicious-for-cancer nodules compared to the control group, with improvement observed in all four training sessions.Here’s the text in Simplified Chinese:</li>
<li>for: 这项研究旨在提高胸部肿瘤检测中的医生准确率。</li>
<li>methods: 该研究使用眼动跟踪技术分析医生的搜寻模式，并对参与实验组提供自动反馈。</li>
<li>results: 参与实验组对可疑肿瘤的检测精度有38.89%的绝对提升，比控制组的改善（5.56%）显著（p值&#x3D;0.006），并在四个训练会议中持续改善（p值&#x3D;0.0001）。<details>
<summary>Abstract</summary>
Diagnostic errors in radiology often occur due to incomplete visual assessments by radiologists, despite their knowledge of predicting disease classes. This insufficiency is possibly linked to the absence of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.
</details>
<details>
<summary>摘要</summary>
radiologists  oftentimes make diagnostic errors due to inadequate visual assessments, despite their knowledge of predicting disease classes. This deficiency may be linked to the lack of required training in search patterns. Additionally, radiologists lack consistent feedback on their visual search patterns, relying on ad-hoc strategies and peer input to minimize errors and enhance efficiency, leading to suboptimal patterns and potential false negatives. This study aimed to use eye-tracking technology to analyze radiologist search patterns, quantify performance using established metrics, and assess the impact of an automated feedback-driven educational framework on detection accuracy. Ten residents participated in a controlled trial focused on detecting suspicious pulmonary nodules. They were divided into an intervention group (received automated feedback) and a control group. Results showed that the intervention group exhibited a 38.89% absolute improvement in detecting suspicious-for-cancer nodules, surpassing the control group's improvement (5.56%, p-value=0.006). Improvement was more rapid over the four training sessions (p-value=0.0001). However, other metrics such as speed, search pattern heterogeneity, distractions, and coverage did not show significant changes. In conclusion, implementing an automated feedback-driven educational framework improved radiologist accuracy in detecting suspicious nodules. The study underscores the potential of such systems in enhancing diagnostic performance and reducing errors. Further research and broader implementation are needed to consolidate these promising results and develop effective training strategies for radiologists, ultimately benefiting patient outcomes.Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/05/eess.IV_2023_08_05/" data-id="cllsk9gs500959c887wcw5ldw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_08_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/08/04/cs.LG_2023_08_04/" class="article-date">
  <time datetime="2023-08-03T16:00:00.000Z" itemprop="datePublished">2023-08-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/08/04/cs.LG_2023_08_04/">cs.LG - 2023-08-04 18:00:00</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach"><a href="#Enhancing-Cell-Tracking-with-a-Time-Symmetric-Deep-Learning-Approach" class="headerlink" title="Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach"></a>Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03887">http://arxiv.org/abs/2308.03887</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gergely Szabó, Paolo Bonaiuti, Andrea Ciliberto, András Horváth</li>
<li>for: 生物学实验中跟踪细胞的动态行为</li>
<li>methods: 基于深度学习的方法，不受连续帧的限制，仅基于细胞的空间时间邻域</li>
<li>results: 能够处理大量视频帧，并且可以学习细胞的运动模式无需任何先前假设<details>
<summary>Abstract</summary>
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated through multiple biologically motivated validation strategies and compared against several state-of-the-art cell tracking methods.
</details>
<details>
<summary>摘要</summary>
live cells 的准确跟踪使用视频微scopy记录仍然是流行的state-of-the-art image processing基于对象跟踪方法中的挑战。在过去几年中，一些现有的和新的应用程序尝试 integrating deep learning基础框架来实现这项任务，但大多数它们仍然受限于连续帧基础或其他前提，这会阻碍总体学习。为解决这个问题，我们努力开发了一种新的深度学习基础的跟踪方法，这种方法假设Cells可以根据其空间-时间 neighborhood来跟踪，不需要 consecutive frame。这种方法还有一个利点，即 predictor 可以通过完全学习 cells 的运动模式而不需要任何先前假设，并且可以处理大量视频帧。我们通过多种生物学上驱动的验证方法来证明方法的效果，并与一些state-of-the-art cell tracking方法进行比较。
</details></li>
</ul>
<hr>
<h2 id="Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks"><a href="#Learning-Optimal-Admission-Control-in-Partially-Observable-Queueing-Networks" class="headerlink" title="Learning Optimal Admission Control in Partially Observable Queueing Networks"></a>Learning Optimal Admission Control in Partially Observable Queueing Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02391">http://arxiv.org/abs/2308.02391</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonatha Anselmi, Bruno Gaujal, Louis-Sébastien Rebuffi</li>
<li>for: 这个论文目的是学习一种有效的权限控制策略，以优化在部分可见的队列网络中的排队控制。</li>
<li>methods: 这个论文使用了一种基于POMDP的强化学习算法，并且使用了Norton的等价定理和生成-死亡过程的有效强化学习算法来实现。</li>
<li>results: 这个论文得到了一个只依赖于最大任务数 $S$ 的减少，而不是依赖于任务系统的径长，从而实现了高效的排队控制。<details>
<summary>Abstract</summary>
We present an efficient reinforcement learning algorithm that learns the optimal admission control policy in a partially observable queueing network. Specifically, only the arrival and departure times from the network are observable, and optimality refers to the average holding/rejection cost in infinite horizon.   While reinforcement learning in Partially Observable Markov Decision Processes (POMDP) is prohibitively expensive in general, we show that our algorithm has a regret that only depends sub-linearly on the maximal number of jobs in the network, $S$. In particular, in contrast with existing regret analyses, our regret bound does not depend on the diameter of the underlying Markov Decision Process (MDP), which in most queueing systems is at least exponential in $S$.   The novelty of our approach is to leverage Norton's equivalent theorem for closed product-form queueing networks and an efficient reinforcement learning algorithm for MDPs with the structure of birth-and-death processes.
</details>
<details>
<summary>摘要</summary>
我们提出了一个高效的增强学习算法，用于在部分可观察queueing网络中找到最佳接受控制策略。具体来说，只有网络的到达和离开时间可观察，并且将 Optimal 定义为无限时间平均保持/拒绝成本。而在部分可观察Markov决策过程（POMDP）中，增强学习通常是不可能高效的，但我们显示我们的算法仅对最大作业数量($S$)有较低的干扰。具体来说，我们的干扰 bound 不过依赖 Underlying Markov Decision Process（MDP）的尺度，这在大多数队列系统中是至少对数尺度的 $S$。我们的新的方法是利用Norton的等效定理，以及高效的增强学习算法 для birth-and-death 过程结构的 MDP。
</details></li>
</ul>
<hr>
<h2 id="Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics"><a href="#Scaling-Survival-Analysis-in-Healthcare-with-Federated-Survival-Forests-A-Comparative-Study-on-Heart-Failure-and-Breast-Cancer-Genomics" class="headerlink" title="Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics"></a>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02382">http://arxiv.org/abs/2308.02382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Archetti, Francesca Ieva, Matteo Matteucci</li>
<li>for: This paper is written for the purpose of developing a federated learning algorithm for survival analysis, called FedSurF++, which can handle incomplete, censored, and distributed survival data while preserving user privacy.</li>
<li>methods: The FedSurF++ algorithm uses a federated ensemble method that constructs random survival forests in heterogeneous federations, and investigates several new tree sampling methods from client forests.</li>
<li>results: The paper shows that FedSurF++ achieves comparable performance to existing methods while requiring only a single communication round to complete, and presents results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private.<details>
<summary>Abstract</summary>
Survival analysis is a fundamental tool in medicine, modeling the time until an event of interest occurs in a population. However, in real-world applications, survival data are often incomplete, censored, distributed, and confidential, especially in healthcare settings where privacy is critical. The scarcity of data can severely limit the scalability of survival models to distributed applications that rely on large data pools. Federated learning is a promising technique that enables machine learning models to be trained on multiple datasets without compromising user privacy, making it particularly well-suited for addressing the challenges of survival data and large-scale survival applications. Despite significant developments in federated learning for classification and regression, many directions remain unexplored in the context of survival analysis. In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details>
<details>
<summary>摘要</summary>
生存分析是医学中的基本工具，用于模型 populate 中的事件发生的时间。然而，在实际应用中，生存数据通常是不完整、审核、分布和保密的，特别是在医疗设置中，隐私是非常重要。数据的稀缺性可能会使生存模型在分布式应用中的扩展性受到严重限制。联邦学习是一种有 Promise 的技术，它允许机器学习模型在多个数据集上进行训练，而不需要违反用户隐私。因此，它在生存数据和大规模生存应用中具有潜在的优势。 despite ， many  direction  in the context of survival analysis remains unexplored in federated learning.In this work, we propose an extension of the Federated Survival Forest algorithm, called FedSurF++. This federated ensemble method constructs random survival forests in heterogeneous federations. Specifically, we investigate several new tree sampling methods from client forests and compare the results with state-of-the-art survival models based on neural networks. The key advantage of FedSurF++ is its ability to achieve comparable performance to existing methods while requiring only a single communication round to complete. The extensive empirical investigation results in a significant improvement from the algorithmic and privacy preservation perspectives, making the original FedSurF algorithm more efficient, robust, and private. We also present results on two real-world datasets demonstrating the success of FedSurF++ in real-world healthcare studies. Our results underscore the potential of FedSurF++ to improve the scalability and effectiveness of survival analysis in distributed settings while preserving user privacy.
</details></li>
</ul>
<hr>
<h2 id="Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring"><a href="#Harnessing-the-Web-and-Knowledge-Graphs-for-Automated-Impact-Investing-Scoring" class="headerlink" title="Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring"></a>Harnessing the Web and Knowledge Graphs for Automated Impact Investing Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02622">http://arxiv.org/abs/2308.02622</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingzhi Hu, Daniel Daza, Laurens Swinkels, Kristina Ūsaitė, Robbert-Jan ‘t Hoen, Paul Groth</li>
<li>for: 该研究旨在自动化SDG框架创建过程，提高了SDG批判和分析的效率和准确性。</li>
<li>methods: 该研究提出了一种数据驱动的方法，包括收集和筛选不同网络源和知识图文本数据，然后使用这些数据进行分类，预测公司与SDG的对应度。</li>
<li>results: 实验结果显示，该模型可以准确预测SDG分数，微平均F1分数为0.89，证明该方案的有效性。此外，该研究还提出了一种可以让人类使用的模型解释方法，以便更好地理解和使用模型预测结果。<details>
<summary>Abstract</summary>
The Sustainable Development Goals (SDGs) were introduced by the United Nations in order to encourage policies and activities that help guarantee human prosperity and sustainability. SDG frameworks produced in the finance industry are designed to provide scores that indicate how well a company aligns with each of the 17 SDGs. This scoring enables a consistent assessment of investments that have the potential of building an inclusive and sustainable economy. As a result of the high quality and reliability required by such frameworks, the process of creating and maintaining them is time-consuming and requires extensive domain expertise. In this work, we describe a data-driven system that seeks to automate the process of creating an SDG framework. First, we propose a novel method for collecting and filtering a dataset of texts from different web sources and a knowledge graph relevant to a set of companies. We then implement and deploy classifiers trained with this data for predicting scores of alignment with SDGs for a given company. Our results indicate that our best performing model can accurately predict SDG scores with a micro average F1 score of 0.89, demonstrating the effectiveness of the proposed solution. We further describe how the integration of the models for its use by humans can be facilitated by providing explanations in the form of data relevant to a predicted score. We find that our proposed solution enables access to a large amount of information that analysts would normally not be able to process, resulting in an accurate prediction of SDG scores at a fraction of the cost.
</details>
<details>
<summary>摘要</summary>
《可持续发展目标（SDG）》由联合国引入，以促进政策和活动，确保人类发展和可持续。 SDG 框架在金融业中生成，用于提供对每个 SDG 的分数，以衡量公司是否与它们相align。这些分数允许对投资进行一致的评估，以建立包容和可持续的经济。由于需要高质量和可靠性，创建和维护 SDG 框架的过程是时间consuming 和需要广泛领域专业知识。在这种情况下，我们描述了一个数据驱动的系统，用于自动化 SDG 框架的创建过程。我们首先提出了一种新的方法，收集和筛选来自不同网络源和知识图库相关的公司文本数据集。然后，我们实施和部署基于这些数据的分类器，以预测公司与 SDG 的Alignment 分数。我们的结果表明，我们的最佳表现模型可以准确预测 SDG 分数，μicro 平均 F1 分数为 0.89，证明了我们的解决方案的有效性。我们还描述了如何将模型与人类使用者集成，通过提供预测分数的数据可视化来提供解释。我们发现，我们的提议的解决方案可以访问大量信息， analysts  normally 不能处理，并在较低的成本下实现准确的 SDG 分数预测。
</details></li>
</ul>
<hr>
<h2 id="A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data"><a href="#A-Machine-Learning-Method-for-Predicting-Traffic-Signal-Timing-from-Probe-Vehicle-Data" class="headerlink" title="A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data"></a>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02370">http://arxiv.org/abs/2308.02370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juliette Ugirumurera, Joseph Severino, Erik A. Bensen, Qichao Wang, Jane Macfarlane</li>
<li>for: 本研究使用机器学习技术来估算交通信号时间信息从车辆探测数据中。</li>
<li>methods: 我们使用极限梯度提升（XGBoost）模型来估算信号周期长度，并使用神经网络模型来确定每个阶段的红灯时间。</li>
<li>results: 我们的结果显示估算周期长度的错误在0.56秒之间，而红灯时间预测的平均错误为7.2秒。<details>
<summary>Abstract</summary>
Traffic signals play an important role in transportation by enabling traffic flow management, and ensuring safety at intersections. In addition, knowing the traffic signal phase and timing data can allow optimal vehicle routing for time and energy efficiency, eco-driving, and the accurate simulation of signalized road networks. In this paper, we present a machine learning (ML) method for estimating traffic signal timing information from vehicle probe data. To the authors best knowledge, very few works have presented ML techniques for determining traffic signal timing parameters from vehicle probe data. In this work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate signal cycle lengths and a neural network model to determine the corresponding red times per phase from probe data. The green times are then be derived from the cycle length and red times. Our results show an error of less than 0.56 sec for cycle length, and red times predictions within 7.2 sec error on average.
</details>
<details>
<summary>摘要</summary>
交通信号机制环境中，交通信号控制对交通流控制和安全性具有重要作用。此外，了解交通信号阶段和时间数据可以帮助车辆进行优化的路径规划，以提高时间和能源效率，eco- driving，以及准确地模拟信号化道路网络。本文提出了一种机器学习（ML）方法，用于从车辆探测数据中估算交通信号时间信息。作者知道的研究 Works 中，很少有使用机器学习技术来确定交通信号时间参数从车辆探测数据。本文开发了极大幂boosting（XGBoost）模型来估算信号阶段长度，并使用神经网络模型来确定每个阶段的红灯时间。绿灯时间则可以从阶段长度和红灯时间中 derivation。我们的结果显示，ecycle length 的预测错误在0.56秒左右，而红灯时间的预测错误在7.2秒左右。
</details></li>
</ul>
<hr>
<h2 id="Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra"><a href="#Color-Image-Recovery-Using-Generalized-Matrix-Completion-over-Higher-Order-Finite-Dimensional-Algebra" class="headerlink" title="Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra"></a>Color Image Recovery Using Generalized Matrix Completion over Higher-Order Finite Dimensional Algebra</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02621">http://arxiv.org/abs/2308.02621</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Liao, Zhuang Guo, Qi Gao, Yan Wang, Fajun Yu, Qifeng Zhao, Stephen Johh Maybank</li>
<li>for: 填充颜色图像中缺失数据的高精度图像完成</li>
<li>methods: 基于扩展的高阶矩阵模型，包括像素 neighborgood 扩展策略来描述地方像素约束</li>
<li>results: 对于各种算法进行了广泛的实验，并与公共可用的图像进行了比较，结果显示，我们的扩展矩阵完成模型和相应的算法与其低阶矩阵和传统矩阵对手相比，性能很高。<details>
<summary>Abstract</summary>
To improve the accuracy of color image completion with missing entries, we present a recovery method based on generalized higher-order scalars. We extend the traditional second-order matrix model to a more comprehensive higher-order matrix equivalent, called the "t-matrix" model, which incorporates a pixel neighborhood expansion strategy to characterize the local pixel constraints. This "t-matrix" model is then used to extend some commonly used matrix and tensor completion algorithms to their higher-order versions. We perform extensive experiments on various algorithms using simulated data and algorithms on simulated data and publicly available images and compare their performance. The results show that our generalized matrix completion model and the corresponding algorithm compare favorably with their lower-order tensor and conventional matrix counterparts.
</details>
<details>
<summary>摘要</summary>
为提高颜色图像完成缺失项的准确性，我们提出一种基于泛化高阶约束的恢复方法。我们将传统的第二阶矩阵模型扩展到更加全面的高阶矩阵等价物，称之为“t-矩阵”模型，该模型通过描述当地像素约束的像素邻域扩展策略。这个“t-矩阵”模型后来用于扩展一些通常使用的矩阵和张量完成算法到其高阶版本。我们在各种算法上进行了广泛的实验，使用模拟数据和公共可用的图像，并比较了其性能。结果显示，我们的泛化约束模型和相应的算法与其低阶张量和传统矩阵counterparts相比，表现良好。
</details></li>
</ul>
<hr>
<h2 id="Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes"><a href="#Intensity-free-Integral-based-Learning-of-Marked-Temporal-Point-Processes" class="headerlink" title="Intensity-free Integral-based Learning of Marked Temporal Point Processes"></a>Intensity-free Integral-based Learning of Marked Temporal Point Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02360">http://arxiv.org/abs/2308.02360</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stepinsilence/ifib">https://github.com/stepinsilence/ifib</a></li>
<li>paper_authors: Sishun Liu, Ke Deng, Xiuzhen Zhang, Yongli Ren</li>
<li>for: 这个论文的目的是为了开发一个高精度的数值点事件模型，以应对发生在多维数值空间中的数值事件。</li>
<li>methods: 这个论文使用了一个名为IFIB的解决方案，它是一种不使用强度函数的方法，它直接模型了条件共同PDF $p^{*}(m,t)$，并且可以实现高精度的数值点事件模型。</li>
<li>results: 这个论文的实验结果显示，IFIB可以实现高精度的数值点事件模型，并且在实际应用中具有较好的性能。另外，这个论文还提供了一个可用的代码库，供其他研究者使用。<details>
<summary>Abstract</summary>
In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\underline{I}ntensity-\underline{f}ree \underline{I}ntegral-\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \url{https://github.com/StepinSilence/IFIB}.
</details>
<details>
<summary>摘要</summary>
在标记时间点过程中（MTPP），核心问题是参数化 conditional joint PDF（概率分布函数）$p^*(m,t)$， conditioned on the history，其中 $m$ 表示事件标记， $t$ 表示事件间隔时间。大多数现有研究都是先定义INTENSITY函数。然而，这些INTENSITY函数的合适形式是关键，需要平衡表达能力和处理效率。在最近几年，有一些研究尝试离开先定义INTENSITY函数，其中一种是将 $p^*(t)$ 和 $p^*(m)$ 分别模型，另一种是关注时间点过程（TPP），不考虑标记。本研究旨在开发高精度的 $p^*(m,t)$  для精确的事件时间点，其中事件标记可以是 categorical 或 numeric，并且在多维连续空间中。我们提出了一种解决方案框架 IFIB（INTENSITY-free INTEGRAL-based process），它直接模型 conditional joint PDF $p^*(m,t)$ 无需INTENSITY函数。这有效简化了过程，使得mathematical restrictions强制性地减少。我们显示了 IFIB 的愿望性质和实验结果，并提供了实验结果。代码可以在 <https://github.com/StepinSilence/IFIB> 上获取。
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-for-GTFS-From-Words-to-Information"><a href="#ChatGPT-for-GTFS-From-Words-to-Information" class="headerlink" title="ChatGPT for GTFS: From Words to Information"></a>ChatGPT for GTFS: From Words to Information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02618">http://arxiv.org/abs/2308.02618</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/utel-uiuc/gtfs_llm">https://github.com/utel-uiuc/gtfs_llm</a></li>
<li>paper_authors: Saipraneeth Devunuri, Shirin Qiam, Lewis Lehe</li>
<li>For: The paper aims to explore the ability of large language models (LLMs) to retrieve information from the General Transit Feed Specification (GTFS) using natural language instructions.* Methods: The paper uses the ChatGPT model (GPT-3.5) to test its understanding of the GTFS specification and to perform information extraction from a filtered GTFS feed with 4 routes. The paper compares zero-shot and program synthesis methods for information retrieval.* Results: The paper finds that program synthesis achieves higher accuracy (~90% for simple questions and ~40% for complex questions) than zero-shot methods for information retrieval from GTFS using natural language instructions.<details>
<summary>Abstract</summary>
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.
</details>
<details>
<summary>摘要</summary>
通用交通Feed规范（GTFS）是公共交通数据的发布标准，这种标准是表格数据，信息分布在不同文件中，因此需要专门的工具或包装来获取信息。同时，使用大型自然语言模型（LLM）来检索文本和信息的使用也在增长。本研究的想法是看看目前广泛采用的LLM（ChatGPT）能否使用自然语言指令来从GTFS中提取信息。我们首先测试了GPT-3.5是否理解GTFS规范。GPT-3.5回答了我们的多项选择题（MCQ）77% correctly。接下来，我们让LLM从过滤后的GTFS feed中提取信息。为了获取信息，我们比较了零shot和程序合成。程序合成更好，实现了~90%的简单问题的准确率和~40%的复杂问题的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels"><a href="#Multi-attacks-Many-images-the-same-adversarial-attack-to-many-target-labels" class="headerlink" title="Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels"></a>Multi-attacks: Many images $+$ the same adversarial attack $\to$ many target labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03792">http://arxiv.org/abs/2308.03792</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanislavfort/multi-attacks">https://github.com/stanislavfort/multi-attacks</a></li>
<li>paper_authors: Stanislav Fort</li>
<li>for: 这个论文旨在描述一种可以同时对多个图像进行攻击的攻击方法。</li>
<li>methods: 这个论文使用了一种称为”多重攻击”的方法，可以对多个图像进行攻击，并且可以在不同的目标类上进行攻击。</li>
<li>results: 论文表明，使用这种多重攻击方法可以对数百个图像进行攻击，并且可以在不同的图像和目标类上进行攻击。此外，论文还发现了一些相关的结果，如图像的高信任区域数量是$\mathcal{O}(100)$以上，这会带来一些问题 для防御策略。<details>
<summary>Abstract</summary>
We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision boundaries in the pixel space, we look for its two-dimensional sections that trace images and spell words using particular classes. We also show that ensembling reduces susceptibility to multi-attacks, and that classifiers trained on random labels are more susceptible. Our code is available on GitHub.
</details>
<details>
<summary>摘要</summary>
我们显示出可以轻松设计一个单一敌对偏移$P$，使$n$个图像$X_1,X_2,\dots,X_n$的原始、未偏变的类别变更为Target类别$c_1,c_2,\dots,c_n$的欲要（可能不是所有的类别都是一样的）类别$c^*_1,c^*_2,\dots,c^*_n$，称之为“多元攻击”。我们估计在不同的图像分辨率下，可以达到大量的$n$，并且考虑到像素空间中高度信任类别的区域数量约为$10^{\mathcal{O}(100)}$，这会对对抗策略造成严重的问题。我们显示了一些立即的后果：对于图像的数量和Target类别的变化，以及对于图像的缩放和转换的类别攻击。为了证明像素空间中类别决策boundary的丰富和紧张，我们寻找了图像和字串之间的二维部分，并证明了折衣组合可以对抗多元攻击，而且随机 labels 训练的分类器更加易受到攻击。我们的代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes"><a href="#Adapting-to-Change-Robust-Counterfactual-Explanations-in-Dynamic-Data-Landscapes" class="headerlink" title="Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes"></a>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02353">http://arxiv.org/abs/2308.02353</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardhprenkaj/hansel">https://github.com/bardhprenkaj/hansel</a></li>
<li>paper_authors: Bardh Prenkaj, Mario Villaizan-Vallelado, Tobias Leemann, Gjergji Kasneci</li>
<li>for: This paper presents a novel semi-supervised method for counterfactual explanation generation, called Dynamic GRAph Counterfactual Explainer (DyGRACE).</li>
<li>methods: DyGRACE uses two graph autoencoders (GAEs) to learn the representation of each class in a binary classification scenario, and optimizes a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximizing the factual autoencoder’s reconstruction error.</li>
<li>results: DyGRACE is effective in identifying counterfactuals and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle’s predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery.<details>
<summary>Abstract</summary>
We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE) methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages initial knowledge about the data distribution to search for valid counterfactuals while avoiding using information from potentially outdated decision functions in subsequent time steps. Employing two graph autoencoders (GAEs), DyGRACE learns the representation of each class in a binary classification scenario. The GAEs minimise the reconstruction error between the original graph and its learned representation during training. The method involves (i) optimising a parametric density function (implemented as a logistic regression function) to identify counterfactuals by maximising the factual autoencoder's reconstruction error, (ii) minimising the counterfactual autoencoder's error, and (iii) maximising the similarity between the factual and counterfactual graphs. This semi-supervised approach is independent of an underlying black-box oracle. A logistic regression model is trained on a set of graph pairs to learn weights that aid in finding counterfactuals. At inference, for each unseen graph, the logistic regressor identifies the best counterfactual candidate using these learned weights, while the GAEs can be iteratively updated to represent the continual adaptation of the learned graph representation over iterations. DyGRACE is quite effective and can act as a drift detector, identifying distributional drift based on differences in reconstruction errors between iterations. It avoids reliance on the oracle's predictions in successive iterations, thereby increasing the efficiency of counterfactual discovery. DyGRACE, with its capacity for contrastive learning and drift detection, will offer new avenues for semi-supervised learning and explanation generation.
</details>
<details>
<summary>摘要</summary>
我们介绍一种新的半监督式グラフカウンターファクタルエクスプレイナー（GCE）方法，即动态GRAPHカウンターファクタルエクスプレイナー（DyGRACE）。它利用初始知识来搜寻有效的假设，而不需要在后续时间步骤中使用可能已过时的决策函数。使用两个图自动生成器（GAE），DyGRACE学习图像中的每个类别表现。在训练过程中，GAE将图像和其学习的表现之间的差异降到最小。方法包括：(i) 优化一个 Parametric Density Function（实现为逻辑回归函数），以确定假设，最大化实际自动生成器的重建错误。(ii) 降低假设自动生成器的错误。(iii) 将实际和假设图像之间的相似度最大化。这个半监督式方法不需要背景黑盒模型，可以独立进行假设搜寻。在推断过程中，一个逻辑回归模型将被训练，以学习对图像对的权重，并且在每次推断过程中选择最佳的假设候选者。在迭代过程中，GAEs可以逐步更新，以反映适应学习的图像表现。DyGRACE能够实现对照学习和解释生成，并且可以检测分布迁移，根据不同的重建错误值进行推断。这些特点使得DyGRACE能够提高假设搜寻的效率，并且具有跨时间的内存和适应能力。
</details></li>
</ul>
<hr>
<h2 id="RobustMQ-Benchmarking-Robustness-of-Quantized-Models"><a href="#RobustMQ-Benchmarking-Robustness-of-Quantized-Models" class="headerlink" title="RobustMQ: Benchmarking Robustness of Quantized Models"></a>RobustMQ: Benchmarking Robustness of Quantized Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02350">http://arxiv.org/abs/2308.02350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang Guo, Xianglong Liu</li>
<li>for: 评估量化神经网络模型的可靠性和抗噪性。</li>
<li>methods: 使用了多种噪音（攻击性噪音、自然噪音和系统噪音）对量化神经网络模型进行了全面的评估。</li>
<li>results: 研究发现，量化模型对攻击性噪音具有更高的抗噪性，但对自然噪音和系统噪音更容易受损。增加量化比特宽度会导致对攻击性噪音的抗噪性下降，对自然噪音和系统噪音的抗噪性增加。等类型的噪音对量化模型的影响不同。<details>
<summary>Abstract</summary>
Quantization has emerged as an essential technique for deploying deep neural networks (DNNs) on devices with limited resources. However, quantized models exhibit vulnerabilities when exposed to various noises in real-world applications. Despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example: (1) quantized models exhibit higher adversarial robustness than their floating-point counterparts, but are more vulnerable to natural corruptions and systematic noises; (2) in general, increasing the quantization bit-width results in a decrease in adversarial robustness, an increase in natural robustness, and an increase in systematic robustness; (3) among corruption methods, \textit{impulse noise} and \textit{glass blur} are the most harmful to quantized models, while \textit{brightness} has the least impact; (4) among systematic noises, the \textit{nearest neighbor interpolation} has the highest impact, while bilinear interpolation, cubic interpolation, and area interpolation are the three least harmful. Our research contributes to advancing the robust quantization of models and their deployment in real-world scenarios.
</details>
<details>
<summary>摘要</summary>
量化技术已成为深度神经网络（DNNs）部署在有限资源设备的重要手段。然而，量化模型在实际应用中受到各种噪声的影响，这些噪声包括攻击性噪声、自然损害和系统性噪声。despite the importance of evaluating the impact of quantization on robustness, existing research on this topic is limited and often disregards established principles of robustness evaluation, resulting in incomplete and inconclusive findings. To address this gap, we thoroughly evaluated the robustness of quantized models against various noises (adversarial attacks, natural corruptions, and systematic noises) on ImageNet. The comprehensive evaluation results empirically provide valuable insights into the robustness of quantized models in various scenarios, for example:1. 量化模型对攻击性噪声比浮点模型更高，但对自然损害和系统性噪声更容易受到影响。2. 在不同的量化比特宽度下，对攻击性噪声的影响随着量化比特宽度的增加而逐渐减少，对自然损害和系统性噪声的影响则随着量化比特宽度的增加而逐渐增加。3. 对量化模型的噪声方法，抖擦噪声和玻璃噪声是最有害的两种，而亮度噪声对量化模型的影响最小。4. 对系统性噪声，最近的邻居 interpolate 是最有害的一种，而 bilinear interpolate、cubic interpolate 和 area interpolate 是最弱的三种。我们的研究为深度神经网络的可靠量化和实际应用做出了贡献。
</details></li>
</ul>
<hr>
<h2 id="Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning"><a href="#Vehicles-Control-Collision-Avoidance-using-Federated-Deep-Reinforcement-Learning" class="headerlink" title="Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning"></a>Vehicles Control: Collision Avoidance using Federated Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02614">http://arxiv.org/abs/2308.02614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badr Ben Elallid, Amine Abouaomar, Nabil Benamar, Abdellatif Kobbane</li>
<li>for: 运输管理和安全性问题在都市化的人口增加和车辆量增加的情况下变得非常重要。这篇论文探讨了在碰撞避免方面使用智能控制系统的发展，并且利用联合深度循环学习（FDRL）技术。</li>
<li>methods: 这篇论文使用了两种模型：地方模型（DDPG）和联合模型（FDDPG），并进行了比较分析，以决定它们在碰撞避免方面的效果。</li>
<li>results: 结果显示，使用FDDPG算法可以更好地控制车辆，避免碰撞。尤其是，FDDPG-based algorithm在减少旅行延迟和提高平均速度方面表现出了明显的改善。<details>
<summary>Abstract</summary>
In the face of growing urban populations and the escalating number of vehicles on the roads, managing transportation efficiently and ensuring safety have become critical challenges. To tackle these issues, the development of intelligent control systems for vehicles is paramount. This paper presents a comprehensive study on vehicle control for collision avoidance, leveraging the power of Federated Deep Reinforcement Learning (FDRL) techniques. Our main goal is to minimize travel delays and enhance the average speed of vehicles while prioritizing safety and preserving data privacy. To accomplish this, we conducted a comparative analysis between the local model, Deep Deterministic Policy Gradient (DDPG), and the global model, Federated Deep Deterministic Policy Gradient (FDDPG), to determine their effectiveness in optimizing vehicle control for collision avoidance. The results obtained indicate that the FDDPG algorithm outperforms DDPG in terms of effectively controlling vehicles and preventing collisions. Significantly, the FDDPG-based algorithm demonstrates substantial reductions in travel delays and notable improvements in average speed compared to the DDPG algorithm.
</details>
<details>
<summary>摘要</summary>
面对城市人口增长和交通量不断增加的问题，有效地管理交通和确保安全已成为核心挑战。为此，开发智能控制系统 для车辆是极其重要的。本文通过详细的研究，探讨了采用联邦深度强化学习（FDRL）技术来控制车辆，以最小化旅行延迟和提高车辆的平均速度，同时保持数据隐私。为此，我们进行了本地模型（DDPG）和全球模型（FDDPG）的比较分析，以确定它们在避免碰撞方面的效果。结果表明，FDDPG算法在控制车辆和避免碰撞方面表现较好，并且在旅行延迟和车辆平均速度方面具有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility"><a href="#Recurrent-Neural-Networks-with-more-flexible-memory-better-predictions-than-rough-volatility" class="headerlink" title="Recurrent Neural Networks with more flexible memory: better predictions than rough volatility"></a>Recurrent Neural Networks with more flexible memory: better predictions than rough volatility</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.08550">http://arxiv.org/abs/2308.08550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Damien Challet, Vincent Ragel</li>
<li>for: 这篇论文旨在扩展循环神经网络，以应对具有长 памя于或高度不均匀的时间步骤。</li>
<li>methods: 这篇论文使用了多个灵活的时间尺度，以提高循环神经网络的能力，并与标准LSTM进行比较。</li>
<li>results: 相比标准LSTM，扩展LSTM需要训练更少的epoch，并且预测资产波动性的模型体系性高于20%。<details>
<summary>Abstract</summary>
We extend recurrent neural networks to include several flexible timescales for each dimension of their output, which mechanically improves their abilities to account for processes with long memory or with highly disparate time scales. We compare the ability of vanilla and extended long short term memory networks (LSTMs) to predict asset price volatility, known to have a long memory. Generally, the number of epochs needed to train extended LSTMs is divided by two, while the variation of validation and test losses among models with the same hyperparameters is much smaller. We also show that the model with the smallest validation loss systemically outperforms rough volatility predictions by about 20% when trained and tested on a dataset with multiple time series.
</details>
<details>
<summary>摘要</summary>
我们扩展回传神经网络，以包括每个输出维度的多个灵活时间尺度，以机械提高其能力处理具有长期记忆或高度不同时间尺度的过程。我们比较了净体和扩展的长期快短时间记忆网络（LSTM）的能力预测资产波动性，知道具有长期记忆。通常，训练扩展LSTM需要的轮数比vanilla LSTM要少半，并且模型之间的验证和测试损失的变化相对较小。我们还显示，具有最小验证损失的模型系统地超过了使用多个时间序列的预测波动性预测值的20%。
</details></li>
</ul>
<hr>
<h2 id="Stability-and-Generalization-of-Hypergraph-Collaborative-Networks"><a href="#Stability-and-Generalization-of-Hypergraph-Collaborative-Networks" class="headerlink" title="Stability and Generalization of Hypergraph Collaborative Networks"></a>Stability and Generalization of Hypergraph Collaborative Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02347">http://arxiv.org/abs/2308.02347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael Ng, Hanrui Wu, Andy Yip</li>
<li>For: 本研究旨在确保hypergraph collaborative networks的核心层的算法稳定性，并提供一般化保证。* Methods: 本文使用hypergraph collaborative networks，并通过对它们的核心层进行分析，提供了一般化保证。* Results: 实验结果表明，通过合适地调整数据和hypergraph filters的缩放，可以实现uniform的学习过程稳定性。<details>
<summary>Abstract</summary>
Graph neural networks have been shown to be very effective in utilizing pairwise relationships across samples. Recently, there have been several successful proposals to generalize graph neural networks to hypergraph neural networks to exploit more complex relationships. In particular, the hypergraph collaborative networks yield superior results compared to other hypergraph neural networks for various semi-supervised learning tasks. The collaborative network can provide high quality vertex embeddings and hyperedge embeddings together by formulating them as a joint optimization problem and by using their consistency in reconstructing the given hypergraph. In this paper, we aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees. The analysis sheds light on the design of hypergraph filters in collaborative networks, for instance, how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process. Some experimental results on real-world datasets are presented to illustrate the theory.
</details>
<details>
<summary>摘要</summary>
graph neural networks 有 shown 能够很 effectively 利用 sample 对的 pairwise 关系。 最近， 有 several successful proposals 将 graph neural networks 扩展到 hypergraph neural networks，以利用更复杂的关系。特别是， hypergraph collaborative networks 在 various semi-supervised learning tasks 中 yield superior results compared to other hypergraph neural networks。 collaborative network 可以提供 high quality vertex embeddings 和 hyperedge embeddings，通过 formulating them as a joint optimization problem 和使用 their consistency in reconstructing the given hypergraph。在这篇 paper 中，我们 aim to establish the algorithmic stability of the core layer of the collaborative network and provide generalization guarantees。analysis  shed light on the design of hypergraph filters in collaborative networks, such as how the data and hypergraph filters should be scaled to achieve uniform stability of the learning process。some experimental results on real-world datasets are presented to illustrate the theory。
</details></li>
</ul>
<hr>
<h2 id="Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields"><a href="#Learning-Networks-from-Gaussian-Graphical-Models-and-Gaussian-Free-Fields" class="headerlink" title="Learning Networks from Gaussian Graphical Models and Gaussian Free Fields"></a>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02344">http://arxiv.org/abs/2308.02344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Subhro Ghosh, Soumendu Sundar Mukherjee, Hoang-Son Tran, Ujan Gangopadhyay</li>
<li>for: 估计weighted网络的结构</li>
<li>methods: 基于重复测量Gaussian Graphical Model（GGM）的方法</li>
<li>results: 提出一种新的估计器，可以从GGM的复杂概率特性中提取有用信息，并且可以提供具体的回归保证和样本复杂度下界。特别是，在 Erdos-Renyi 随机网络上，我们证明了在样本大小 $n$ 足够大时，网络结构可以recovery With high probability.<details>
<summary>Abstract</summary>
We investigate the problem of estimating the structure of a weighted network from repeated measurements of a Gaussian Graphical Model (GGM) on the network. In this vein, we consider GGMs whose covariance structures align with the geometry of the weighted network on which they are based. Such GGMs have been of longstanding interest in statistical physics, and are referred to as the Gaussian Free Field (GFF). In recent years, they have attracted considerable interest in the machine learning and theoretical computer science. In this work, we propose a novel estimator for the weighted network (equivalently, its Laplacian) from repeated measurements of a GFF on the network, based on the Fourier analytic properties of the Gaussian distribution. In this pursuit, our approach exploits complex-valued statistics constructed from observed data, that are of interest on their own right. We demonstrate the effectiveness of our estimator with concrete recovery guarantees and bounds on the required sample complexity. In particular, we show that the proposed statistic achieves the parametric rate of estimation for fixed network size. In the setting of networks growing with sample size, our results show that for Erdos-Renyi random graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network recovery takes place with high probability as soon as the sample size $n$ satisfies $n \gg d^4 \log d \cdot p^{-2}$.
</details>
<details>
<summary>摘要</summary>
我们研究如何从重复观测 Gaussian Graphical Model (GGM) 中 estimate 网络的结构。在这个意境下，我们考虑 GGM 的协调结构与网络的重量相对应。这些 GGM 在统计物理学中有很长的历史，通常被称为 Gaussian Free Field (GFF)。在最近几年中，它们在机器学习和理论计算机科学中受到了很大的关注。在这个工作中，我们提出了一个新的网络重量Estimator，基于网络上重复观测 GFF 的 Fourier分析特性。我们的方法利用观测数据中的复数统计，具有自己的科学价值。我们显示了这个统计的效果，并提供了具体的回溯保证和sample complexity bound。尤其是，我们证明了这个统计在固定网络大小下具有参数率的估计率。在探索网络规模 grow 于样本大小的情况下，我们的结果显示，当样本大小 $n$ 满足 $n \gg d^4 \log d \cdot p^{-2}$ 时，网络重建很有可能会在高可信度下发生。Note: "Simplified Chinese" is a romanization of Chinese that uses a simplified set of characters and grammar rules to represent the language. It is commonly used in mainland China and Singapore, and is one of the two official languages of the People's Republic of China.
</details></li>
</ul>
<hr>
<h2 id="RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification"><a href="#RAHNet-Retrieval-Augmented-Hybrid-Network-for-Long-tailed-Graph-Classification" class="headerlink" title="RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification"></a>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02335">http://arxiv.org/abs/2308.02335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengyang Mao, Wei Ju, Yifang Qin, Xiao Luo, Ming Zhang</li>
<li>for: 提高图像分类 tasks 中的泛化能力，尤其是在长尾类分布的real-world数据中。</li>
<li>methods: 提出了一种名为 Retrieval Augmented Hybrid Network (RAHNet) 的新框架，用于同时学习一个 Robust 特征提取器和一个不偏袋化的分类器，并在特征提取器和分类器之间进行分离学习。在特征提取器训练阶段，我们开发了一个图像检索模块，用于搜索与tail类相关的图像，以直接增强tail类之间的内部多样性。在分类器细化阶段，我们使用了两种重量规regularization技术，即Max-norm和weight decay，以均衡分类器的重量。</li>
<li>results: 在各种流行的benchmark上进行了实验，并证明了我们的方法在state-of-the-artapproaches中的优越性。<details>
<summary>Abstract</summary>
Graph classification is a crucial task in many real-world multimedia applications, where graphs can represent various multimedia data types such as images, videos, and social networks. Previous efforts have applied graph neural networks (GNNs) in balanced situations where the class distribution is balanced. However, real-world data typically exhibit long-tailed class distributions, resulting in a bias towards the head classes when using GNNs and limited generalization ability over the tail classes. Recent approaches mainly focus on re-balancing different classes during model training, which fails to explicitly introduce new knowledge and sacrifices the performance of the head classes. To address these drawbacks, we propose a novel framework called Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature extractor and an unbiased classifier in a decoupled manner. In the feature extractor training stage, we develop a graph retrieval module to search for relevant graphs that directly enrich the intra-class diversity for the tail classes. Moreover, we innovatively optimize a category-centered supervised contrastive loss to obtain discriminative representations, which is more suitable for long-tailed scenarios. In the classifier fine-tuning stage, we balance the classifier weights with two weight regularization techniques, i.e., Max-norm and weight decay. Experiments on various popular benchmarks verify the superiority of the proposed method against state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
“图像分类是现实世界多媒体应用中的关键任务，图像可以表示各种媒体数据类型，如图像、视频和社交网络。先前的尝试都是在平衡的情况下应用图像神经网络（GNNs），但实际数据通常会出现长尾分布，导致使用GNNs时对尾类的偏袋和有限的泛化能力。现有的方法主要集中在模型训练时重新平衡不同类别，但这会失去新知识的导入和头类的性能。为解决这些缺点，我们提出了一种新的框架，即Retrieval Augmented Hybrid Network（RAHNet），它可以同时学习一个强健的特征提取器和一个不偏袋的分类器。在特征提取器训练阶段，我们开发了一个图像检索模块，以找到适当的图像来增强尾类的内部多样性。此外，我们还创新地优化了一种类型中心的超级vised对比损失，以获得适合长尾情况的表示，”Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools"><a href="#Interoperable-synthetic-health-data-with-SyntHIR-to-enable-the-development-of-CDSS-tools" class="headerlink" title="Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools"></a>Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02613">http://arxiv.org/abs/2308.02613</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/potter-coder89/synthir">https://github.com/potter-coder89/synthir</a></li>
<li>paper_authors: Pavitra Chauhan, Mohsen Gamal Saad Askar, Bjørn Fjukstad, Lars Ailo Bongo, Edvard Pedersen<br>for:这个论文旨在提出一种基于机器学习的临床决策支持系统（CDSS）的开发方法，使用高质量的患者日志和医疗注册来生成 synthetic EHR 数据，并在临床工作流程中实现 CDSS 工具的开发和测试。methods:这个论文使用的方法包括使用 FHIR 标准实现数据互操作性，使用 Gretel 框架生成 synthetic 数据，使用 Microsoft Azure FHIR 服务器作为基于 FHIR 的 EHR 系统，以及使用 SMART on FHIR 框架实现工具可重用性。results:论文通过开发一个基于机器学习的 CDSS 工具，使用 Norwegian Patient Register (NPR) 和 Norwegian Patient Prescriptions (NorPD) 数据进行开发，并在 SyntHIR 系统上测试和评估该工具。结果表明，SyntHIR 提供了一个通用的 CDSS 工具开发框架，可以使用 synthetic FHIR 数据进行测试和评估，并且可以在临床 setting 中实现。但是，synthetic 数据质量的问题还需要进一步改进。代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/potter-coder89/SyntHIR.git%E3%80%82">https://github.com/potter-coder89/SyntHIR.git。</a><details>
<summary>Abstract</summary>
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data from the Norwegian Patient Register (NPR) and Norwegian Patient Prescriptions (NorPD). We demonstrate the development of the tool on the SyntHIR system and then lift it to the Open DIPS environment. In conclusion, SyntHIR provides a generic architecture for CDSS tool development using synthetic FHIR data and a testing environment before implementing it in a clinical setting. However, there is scope for improvement in terms of the quality of the synthetic data generated. The code is open source and available at https://github.com/potter-coder89/SyntHIR.git.
</details>
<details>
<summary>摘要</summary>
“有一大机会使用高质量的患者日记和医疗注册来开发基于机器学习的临床决策支持系统（CDSS）。为实现CDSS工具在临床工作流程中的应用，需要将这个工具与电子医疗记录（EHR）系统集成、验证和测试。然而，由于法律合规的问题，通常无法获得EHR系统的必要访问权。我们提出了一种使用生成的Synthetic EHR数据来开发CDSS工具的建筑方案。该建筑方案在一个名为SyntHIR的系统中实现，该系统使用Fast Healthcare Interoperability Resources（FHIR）标准来实现数据互操作，使用Gretel框架生成synthetic数据，使用Microsoft Azure FHIR服务器作为FHIR基于EHR系统，并使用SMART on FHIR框架来提供工具可重用性。我们通过使用挪威患者注册（NPR）和挪威药品订单（NorPD）的数据开发了一个基于机器学习的CDSS工具，并在SyntHIR系统上测试了该工具。最后，我们将工具提取到Open DIPS环境中。总之，SyntHIR提供了一个通用的CDSS工具开发基于Synthetic FHIR数据的测试环境，但是可以进一步提高生成的synthetic数据质量。代码可以在https://github.com/potter-coder89/SyntHIR.git中获取。”
</details></li>
</ul>
<hr>
<h2 id="Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery"><a href="#Deep-learning-for-spike-detection-in-deep-brain-stimulation-surgery" class="headerlink" title="Deep learning for spike detection in deep brain stimulation surgery"></a>Deep learning for spike detection in deep brain stimulation surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05755">http://arxiv.org/abs/2308.05755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arkadiusz Nowacki, Ewelina Kołpa, Mateusz Szychiewicz, Konrad Ciecierski</li>
<li>for: 这个论文是为了描述一种基于深度学习的神经活动记录分析方法，用于深 bran stimulation（DBS） neurosurgery 中的 neuronal activity 识别。</li>
<li>methods: 该方法使用了一种卷积神经网络（CNN）来分析神经活动记录，并在不同的时间窗口中进行识别。</li>
<li>results: 实验结果表明，该方法可以达到最高的准确率（98.98%）和受器操作特征曲线的面积（AUC）的最高值（0.9898），而无需进行数据预处理。<details>
<summary>Abstract</summary>
Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
</details>
<details>
<summary>摘要</summary>
深度脑刺激（DBS）是一种 neurosurgical 程序，已经成功地治疗了 Parkinson's disease 等疾病。通过在脑中implanting 电极，可以减轻疾病的 симптом。在这篇论文中，我们提出了使用深度学习分析 DBS  neurosurgery 中记录的 neuronal 活动的方法。我们测试了 convolutional neural network（CNN）来完成这个任务。根据时间窗口，分类器评估 neuronal 活动（脉冲）是否存在。最大准确率值为 98.98%，准确率下接收操作特征曲线（AUC）值为 0.9898。这种方法可以不使用数据预处理来获得分类。
</details></li>
</ul>
<hr>
<h2 id="A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization"><a href="#A-stochastic-optimization-approach-to-train-non-linear-neural-networks-with-a-higher-order-variation-regularization" class="headerlink" title="A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization"></a>A stochastic optimization approach to train non-linear neural networks with a higher-order variation regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02293">http://arxiv.org/abs/2308.02293</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/oknakfm/hovr">https://github.com/oknakfm/hovr</a></li>
<li>paper_authors: Akifumi Okuno</li>
<li>For: This paper aims to address the issue of overfitting in highly expressive parametric models, such as deep neural networks, by introducing a new regularization term called $(k,q)$th order variation regularization ($(k,q)$-VR).* Methods: The paper proposes a stochastic optimization algorithm that can efficiently train general models with the $(k,q)$-VR term without conducting explicit numerical integration. The algorithm is based on stochastic gradient descent and automatic differentiation, and can be applied to the training of deep neural networks with arbitrary structure.* Results: The paper demonstrates that the neural networks trained with the $(k,q)$-VR terms are more “resilient” than those with the conventional parameter regularization, and the proposed algorithm can also be extended to the physics-informed training of neural networks (PINNs).<details>
<summary>Abstract</summary>
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $(k,q)$th order variation regularization ($(k,q)$-VR), which is defined as the $q$th-powered integral of the absolute $k$th order derivative of the parametric models to be trained; penalizing the $(k,q)$-VR is expected to yield a smoother function, which is expected to avoid overfitting. Particularly, $(k,q)$-VR encompasses the conventional (general-order) total variation with $q=1$. While the $(k,q)$-VR terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $(k,q)$-VR without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradient descent algorithm and automatic differentiation. Our numerical experiments demonstrate that the neural networks trained with the $(k,q)$-VR terms are more ``resilient'' than those with the conventional parameter regularization. The proposed algorithm also can be extended to the physics-informed training of neural networks (PINNs).
</details>
<details>
<summary>摘要</summary>
“而高度表达力的 parametric 模型，如深度神经网络，具有模型复杂概念的优势。然而，训练这些非线性模型时存在高风险的过拟合。为解决这个问题，本研究考虑了 $(k,q)$ 项变化规则（$(k,q)$-VR），即将要训练的 parametric 模型的 $q$ 阶幂化积分 absolute $k$ 阶差分。penalizing $(k,q)$-VR 会导致更平滑的函数，以避免过拟合。特别是，$(k,q)$-VR 包括普通（总阶）变量的 $q=1$。而 $(k,q)$-VR 应用于普通 parametric 模型时 computationally intractable due to integration，本研究提供了一种可efficiently 训练通用模型的随机优化算法。这种方法可以应用于深度神经网络的训练，并且可以通过简单的随机梯度下降算法和自动导数来实现。我们的numerical experiments表明，使用 $(k,q)$-VR 训练的神经网络比使用传统参数正则化更为“坚固”。此外，这种算法还可以扩展到物理学信息训练神经网络（PINNs）。”
</details></li>
</ul>
<hr>
<h2 id="Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization"><a href="#Frustratingly-Easy-Model-Generalization-by-Dummy-Risk-Minimization" class="headerlink" title="Frustratingly Easy Model Generalization by Dummy Risk Minimization"></a>Frustratingly Easy Model Generalization by Dummy Risk Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02287">http://arxiv.org/abs/2308.02287</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juncheng Wang, Jindong Wang, Xixu Hu, Shujun Wang, Xing Xie</li>
<li>for: 提高机器学习模型的泛化能力</li>
<li>methods: 使用拟合风险最小化（Dummy Risk Minimization，DuRM）技术，即通过扩大输出логи特征来提高模型的泛化能力</li>
<li>results: DuRM可以在多个任务上提高表现，包括传统的分类、Semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别等，并且可以与现有的泛化技术相结合使用。<details>
<summary>Abstract</summary>
Empirical risk minimization (ERM) is a fundamental machine learning paradigm. However, its generalization ability is limited in various tasks. In this paper, we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general technique to improve the generalization of ERM. DuRM is extremely simple to implement: just enlarging the dimension of the output logits and then optimizing using standard gradient descent. Moreover, we validate the efficacy of DuRM on both theoretical and empirical analysis. Theoretically, we show that DuRM derives greater variance of the gradient, which facilitates model generalization by observing better flat local minima. Empirically, we conduct evaluations of DuRM across different datasets, modalities, and network architectures on diverse tasks, including conventional classification, semantic segmentation, out-of-distribution generalization, adverserial training, and long-tailed recognition. Results demonstrate that DuRM could consistently improve the performance under all tasks with an almost free lunch manner. Furthermore, we show that DuRM is compatible with existing generalization techniques and we discuss possible limitations. We hope that DuRM could trigger new interest in the fundamental research on risk minimization.
</details>
<details>
<summary>摘要</summary>
empirical risk minimization (ERM) 是机器学习的一种基本思想。然而，其泛化能力在各种任务上有限。在这篇论文中，我们提出了干扰risk minimization（DuRM），一种极其简单和普遍适用的技术，以提高ERM的泛化能力。DuRM的实现非常简单：只需扩大输出logits的维度，然后使用标准的梯度下降优化。我们在理论和实验两方面 validate DuRM的有效性。在理论上，我们表明DuRM可以提高模型的泛化能力，通过观察更好的平坦的本地极小值。在实验上，我们对不同的数据集、模式和网络架构进行了多种任务的评估，包括传统的分类、semantic segmentation、out-of-distribution泛化、对抗训练和长尾识别。结果表明，DuRM可以在所有任务上提高性能，几乎没有免费的午餐。此外，我们还证明了DuRM与现有的泛化技术相容，并讨论了可能的限制。我们希望DuRM可以触发新的研究于风险最小化的基础。
</details></li>
</ul>
<hr>
<h2 id="DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization"><a href="#DIVERSIFY-A-General-Framework-for-Time-Series-Out-of-distribution-Detection-and-Generalization" class="headerlink" title="DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization"></a>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02282">http://arxiv.org/abs/2308.02282</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xiangyang Ji, Qiang Yang, Xing Xie<br>for: This paper aims to address the challenges of out-of-distribution (OOD) detection and generalization on time series data, which is non-stationary and has dynamic distributions.methods: The proposed method, DIVERSIFY, is an iterative framework that uses adversarial training to obtain the “worst-case” latent distribution scenario, and then reduces the gap between these latent distributions. DIVERSIFY combines existing OOD detection methods with outputs of models for detection and utilizes outputs for classification.results: Extensive experiments on seven datasets with different OOD settings show that DIVERSIFY learns more generalized features and significantly outperforms other baselines. Theoretical insights also support the effectiveness of DIVERSIFY.<details>
<summary>Abstract</summary>
Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the "worst-case" latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines.
</details>
<details>
<summary>摘要</summary>
时序序列仍然是机器学习研究中最为困难的模式之一。非站点性（OOD）检测和泛化在时序序列上通常受到非站点性的影响，即时序序列的分布随着时间的变化。时序序列中的动态分布对现有算法提供了很大挑战，因为它们主要假设有域信息作为先验知识。在这篇论文中，我们尝试利用时序序列中的子领域来缓解由非站点性引起的问题，以实现泛化学习。我们提出了DIVERSIFY，一种通用框架，用于OOD检测和泛化动态分布的时序序列。DIVERSIFY采用了迭代过程：首先通过对恶性学习获得“最差”的幂本分布场景，然后减少这些幂本分布之间的差距。我们通过结合现有OOD检测方法来实现DIVERSIFY，并直接利用模型输出进行分类。此外，理论分析表明DIVERSIFY是理论上支持的。我们对七个不同的数据集进行了广泛的实验，包括手势识别、语音命令识别、着装压力和情感识别以及基于传感器的人体活动识别。结果表明DIVERSIFY学习了更泛化的特征，并显著超过了其他基elines。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Proximal-Gradient-Method-for-Convex-Optimization"><a href="#Adaptive-Proximal-Gradient-Method-for-Convex-Optimization" class="headerlink" title="Adaptive Proximal Gradient Method for Convex Optimization"></a>Adaptive Proximal Gradient Method for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02261">http://arxiv.org/abs/2308.02261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yura Malitsky, Konstantin Mishchenko</li>
<li>for: 本文研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和 proximal梯度方法（ProxGD）。我们的注意点是使这两种算法完全适应тив，利用凸函数的地方几何信息。</li>
<li>methods: 我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在只假设本地 lipschitz 的梯度下 converges。</li>
<li>results: 我们的方法可以使用更大的步长 than those initially suggested in [MM20]。<details>
<summary>Abstract</summary>
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了两种基本的首阶算法在凸优化中，即梯度下降（GD）和贝克斯 gradient 方法（ProxGD）。我们的关注点是使这些算法完全适应ive，利用当地凸函数的曲率信息。我们提出了基于观察到的梯度差的自适应GD和ProxGD版本，无需额外计算成本。此外，我们证明了我们的方法在本地lipchitz continuous的梯度下 converges。此外，我们的方法还允许更大的步长than those initially suggested in [MM20].
</details></li>
</ul>
<hr>
<h2 id="Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song"><a href="#Finding-Tori-Self-supervised-Learning-for-Analyzing-Korean-Folk-Song" class="headerlink" title="Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song"></a>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02249">http://arxiv.org/abs/2308.02249</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/danbinaerinhan/finding-tori">https://github.com/danbinaerinhan/finding-tori</a></li>
<li>paper_authors: Danbinaerin Han, Rafael Caro Repetto, Dasaem Jeong</li>
<li>for: 这个论文是对韩国民族歌曲录音数据集进行计算分析的，该数据集包含约700小时的民歌，录制于1980-90年代。</li>
<li>methods: 作者使用自动超vision学习和卷积神经网络，通过抽象报表来解决录音中的挑战。</li>
<li>results: 实验结果表明，作者的方法可以更好地捕捉韩国民歌中的护卷特征，比传统的抑制历史更加精准。通过这种方法，作者可以对现有学术中的音乐讨论在实际录音中进行实质性的探讨。<details>
<summary>Abstract</summary>
In this paper, we introduce a computational analysis of the field recording dataset of approximately 700 hours of Korean folk songs, which were recorded around 1980-90s. Because most of the songs were sung by non-expert musicians without accompaniment, the dataset provides several challenges. To address this challenge, we utilized self-supervised learning with convolutional neural network based on pitch contour, then analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. The experimental result shows that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了一种计算方法对韩国传统歌曲场记录数据集进行分析，该数据集约为700小时，录制于1980-90年代。由于大多数歌曲由非专业音乐家演唱，没有伴奏，因此该数据集具有许多挑战。为 Addressing this challenge, we utilized self-supervised learning with convolutional neural networks based on pitch contours, and analyzed how the musical concept of tori, a classification system defined by a specific scale, ornamental notes, and an idiomatic melodic contour, is captured by the model. Our experimental results show that our approach can better capture the characteristics of tori compared to traditional pitch histograms. Using our approaches, we have examined how musical discussions proposed in existing academia manifest in the actual field recordings of Korean folk songs.Here's the breakdown of the translation:* 韩国传统歌曲 (Korean traditional folk songs) -> 韩国传统歌曲 (Simplified Chinese)* 场记录数据集 (field recording dataset) -> 场记录数据集 (Simplified Chinese)* 约为700小时 (approximately 700 hours) -> 约为700小时 (Simplified Chinese)* 录制于1980-90年代 (recorded in the 1980s-1990s) -> 录制于1980-90年代 (Simplified Chinese)* 非专业音乐家 (non-expert musicians) -> 非专业音乐家 (Simplified Chinese)* 没有伴奏 (no accompaniment) -> 没有伴奏 (Simplified Chinese)* 计算方法 (computational method) -> 计算方法 (Simplified Chinese)* 自动学习 (self-supervised learning) -> 自动学习 (Simplified Chinese)* 基于折衣 (based on pitch contours) -> 基于折衣 (Simplified Chinese)* tori (a classification system) -> tori (Simplified Chinese)* 定义为特定的音阶、装饰音和idiomatic melodic contour -> 定义为特定的音阶、装饰音和idiomatic melodic contour (Simplified Chinese)* 使用我们的方法可以更好地捕捉折衣的特点 -> 使用我们的方法可以更好地捕捉折衣的特点 (Simplified Chinese)* 比传统折衣 histogram 更好 -> 比传统折衣 histogram 更好 (Simplified Chinese)* 使用我们的方法 -> 使用我们的方法 (Simplified Chinese)* 我们已经使用这些方法 -> 我们已经使用这些方法 (Simplified Chinese)* 对现有的音乐学讨论进行实际应用 -> 对现有的音乐学讨论进行实际应用 (Simplified Chinese)* 探讨了韩国传统歌曲中的音乐讨论 -> 探讨了韩国传统歌曲中的音乐讨论 (Simplified Chinese)
</details></li>
</ul>
<hr>
<h2 id="Deep-neural-networks-from-the-perspective-of-ergodic-theory"><a href="#Deep-neural-networks-from-the-perspective-of-ergodic-theory" class="headerlink" title="Deep neural networks from the perspective of ergodic theory"></a>Deep neural networks from the perspective of ergodic theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03888">http://arxiv.org/abs/2308.03888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Zhang</li>
<li>for: 这个论文旨在解释深度神经网络的设计是如何变成一种更加科学的过程，而不是一种艺术。</li>
<li>methods: 这个论文使用了时间演化观的思想，将神经网络看作是一个动力系统的时间演化，每层对应于一个时间实例。</li>
<li>results: 这个论文表明，一些可能看起来神秘的规则，可以被解释为启发。<details>
<summary>Abstract</summary>
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
</details>
<details>
<summary>摘要</summary>
神经网络设计仍然很有创造性，更像是一种艺术而非精确科学。通过尝试将ergodic theory应用于视网膜上，视网膜为时间演化的动力系统，每层对应一个时间实例，我们显示了一些可能看起来神秘的规则，实际上可以归结为优化策略。
</details></li>
</ul>
<hr>
<h2 id="Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain"><a href="#Self-Normalizing-Neural-Network-Enabling-One-Shot-Transfer-Learning-for-Modeling-EDFA-Wavelength-Dependent-Gain" class="headerlink" title="Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain"></a>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02233">http://arxiv.org/abs/2308.02233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Agastya Raj, Zehao Wang, Frank Slyne, Tingjun Chen, Dan Kilper, Marco Ruffini</li>
<li>for: 该论文旨在提出一种基于 semi-supervised, self-normalizing neural networks 的多芯片 EDFA 波长依赖性的模型化框架，以实现一次转移学习。</li>
<li>methods: 该论文使用 semi-supervised, self-normalizing neural networks 来模型多芯片 EDFA 的波长依赖性，并实现了一次转移学习。</li>
<li>results: 实验结果表明，该模型在 Open Ireland 和 COSMOS 测试平台上的 22 个 EDFA 中具有高精度的转移学习能力，即使操作在不同的芯片类型上。<details>
<summary>Abstract</summary>
We present a novel ML framework for modeling the wavelength-dependent gain of multiple EDFAs, based on semi-supervised, self-normalizing neural networks, enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open Ireland and COSMOS testbeds show high-accuracy transfer-learning even when operated across different amplifier types.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的机器学习框架，用于模型多个电子发射激光扩展器（EDFA）的波长依赖性收益，基于半监督自适应神经网络。我们的实验表明，这种框架可以在不同类型的扩展器上实现高精度的传输学习，并且可以在22个EDFA上进行一次转移学习。
</details></li>
</ul>
<hr>
<h2 id="Likelihood-ratio-based-confidence-intervals-for-neural-networks"><a href="#Likelihood-ratio-based-confidence-intervals-for-neural-networks" class="headerlink" title="Likelihood-ratio-based confidence intervals for neural networks"></a>Likelihood-ratio-based confidence intervals for neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02221">http://arxiv.org/abs/2308.02221</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/laurenssluyterman/likelihood_ratio_intervals">https://github.com/laurenssluyterman/likelihood_ratio_intervals</a></li>
<li>paper_authors: Laurens Sluijterman, Eric Cator, Tom Heskes</li>
<li>for: 这个论文是为了建立一种基于likelihood ratio的方法来计算神经网络的信心 интерval。</li>
<li>methods: 这个方法使用了likelihood ratio的思想，可以建立不对称的信心 интерval，并且自动包含了训练时间、网络架构、训练技巧等因素。</li>
<li>results: 这个方法可以在对于医学预测或天文物理等领域，提供一个可靠的未知度估计，并且显示出这种方法在某些情况下可能已经有经济效益。<details>
<summary>Abstract</summary>
This paper introduces a first implementation of a novel likelihood-ratio-based approach for constructing confidence intervals for neural networks. Our method, called DeepLR, offers several qualitative advantages: most notably, the ability to construct asymmetric intervals that expand in regions with a limited amount of data, and the inherent incorporation of factors such as the amount of training time, network architecture, and regularization techniques. While acknowledging that the current implementation of the method is prohibitively expensive for many deep-learning applications, the high cost may already be justified in specific fields like medical predictions or astrophysics, where a reliable uncertainty estimate for a single prediction is essential. This work highlights the significant potential of a likelihood-ratio-based uncertainty estimate and establishes a promising avenue for future research.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:这篇论文介绍了一种基于likelihood-ratio的神经网络置信范围的首次实现方法，称为DeepLR。我们的方法具有许多优点：能够构建不均匀的置信范围，在数据有限的地方扩展，同时自动包含训练时间、网络架构和正则化技术等因素。虽然当前实现可能对许多深度学习应用程序来说过于昂贵，但在医学预测或天文物理等领域，准确地估计单个预测结果的不确定性可能已经被 justify。这篇文章探讨了基于likelihood-ratio的置信范围的可能性，并开启了未来研究的新途径。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles"><a href="#Knowledge-Driven-Multi-Agent-Reinforcement-Learning-for-Computation-Offloading-in-Cybertwin-Enabled-Internet-of-Vehicles" class="headerlink" title="Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles"></a>Knowledge-Driven Multi-Agent Reinforcement Learning for Computation Offloading in Cybertwin-Enabled Internet of Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02603">http://arxiv.org/abs/2308.02603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruijin Sun, Xiao Yang, Nan Cheng, Xiucheng Wang, Changle Li</li>
<li>for: 提高 cybertwin-enabled IoV 中任务卸载延迟</li>
<li>methods: 使用知识驱动多代理人学习（KMARL）方法，利用域知识加入图 neural networks，实现选择最佳卸载选项</li>
<li>results: 比较其他方法，KMARL 表现更高的奖励和更好的扩展性，受到域知识的整合帮助<details>
<summary>Abstract</summary>
By offloading computation-intensive tasks of vehicles to roadside units (RSUs), mobile edge computing (MEC) in the Internet of Vehicles (IoV) can relieve the onboard computation burden. However, existing model-based task offloading methods suffer from heavy computational complexity with the increase of vehicles and data-driven methods lack interpretability. To address these challenges, in this paper, we propose a knowledge-driven multi-agent reinforcement learning (KMARL) approach to reduce the latency of task offloading in cybertwin-enabled IoV. Specifically, in the considered scenario, the cybertwin serves as a communication agent for each vehicle to exchange information and make offloading decisions in the virtual space. To reduce the latency of task offloading, a KMARL approach is proposed to select the optimal offloading option for each vehicle, where graph neural networks are employed by leveraging domain knowledge concerning graph-structure communication topology and permutation invariance into neural networks. Numerical results show that our proposed KMARL yields higher rewards and demonstrates improved scalability compared with other methods, benefitting from the integration of domain knowledge.
</details>
<details>
<summary>摘要</summary>
通过异步计算任务转移到路边单元（RSU），移动边缘计算（MEC）在互联网机器人（IoV）中可以减轻车辆上计算负担。然而，现有的模型基于任务转移方法受到增加车辆和数据驱动方法的计算复杂性的影响。为解决这些挑战，在这篇论文中，我们提出了知识驱动多智能体强化学习（KMARL）方法，以减少异步任务转移的延迟。具体来说，在考虑的场景中， cybertwin 作为每辆车辆的通信代理，在虚拟空间中交换信息并做出转移决策。通过使用图神经网络，我们利用了域知识，包括图structure 通信topology和 permutation 不变性，从而提高了强化学习的精度和扩展性。 numerically 的结果表明，我们提出的 KMARL 方法可以获得更高的奖励，并且在其他方法相比，具有更好的扩展性，受益于域知识的集成。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-of-Spanish-Clinical-Language-Models"><a href="#A-Survey-of-Spanish-Clinical-Language-Models" class="headerlink" title="A Survey of Spanish Clinical Language Models"></a>A Survey of Spanish Clinical Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02199">http://arxiv.org/abs/2308.02199</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guillem García Subies, Álvaro Barbero Jiménez, Paloma Martínez Fernández</li>
<li>for: 这项研究专注于使用语言模型解决西班牙语医疗领域任务。</li>
<li>methods: 研究人员回顾了17个词库，主要集中在医疗任务上，然后列出了最有影响力的西班牙语语言模型和医疗语言模型。研究人员还对这些模型进行了严格的比较，用于找出最佳performing的模型，总共超过3000个模型进行了微调。</li>
<li>results: 研究人员对一些可访问的 corpora 进行了测试，并将结果公开发布，以便由独立团队重复或在未来对新的西班牙语医疗语言模型进行挑战。<details>
<summary>Abstract</summary>
This survey focuses in encoder Language Models for solving tasks in the clinical domain in the Spanish language. We review the contributions of 17 corpora focused mainly in clinical tasks, then list the most relevant Spanish Language Models and Spanish Clinical Language models. We perform a thorough comparison of these models by benchmarking them over a curated subset of the available corpora, in order to find the best-performing ones; in total more than 3000 models were fine-tuned for this study. All the tested corpora and the best models are made publically available in an accessible way, so that the results can be reproduced by independent teams or challenged in the future when new Spanish Clinical Language models are created.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" in Chinese.Please note that the translation is done using a machine translation tool, and may not be perfect or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification"><a href="#AutoML4ETC-Automated-Neural-Architecture-Search-for-Real-World-Encrypted-Traffic-Classification" class="headerlink" title="AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification"></a>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02182">http://arxiv.org/abs/2308.02182</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orangeuw/automl4etc">https://github.com/orangeuw/automl4etc</a></li>
<li>paper_authors: Navid Malekghaini, Elham Akbari, Mohammad A. Salahuddin, Noura Limam, Raouf Boutaba, Bertrand Mathieu, Stephanie Moteau, Stephane Tuffin</li>
<li>for: 这个研究是为了提出一个自动设计高性能的神经网络模型，用于实时隐私化网络流量分类。</li>
<li>methods: 这个研究使用了自动机器学习（AutoML）技术，定义了一个特定设计的搜寻空间，并运用不同的搜寻策略来寻找最佳的神经网络模型。</li>
<li>results: 研究发现，使用AutoML4ETC可以自动设计高性能的神经网络模型，并且比现有的隐私化网络流量分类模型更加精确和轻量级。<details>
<summary>Abstract</summary>
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark datasets and real-world TLS and QUIC traffic collected from the Orange mobile network. In addition to being more accurate, AutoML4ETC's architectures are significantly more efficient and lighter in terms of the number of parameters. Finally, we make AutoML4ETC publicly available for future research.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology"><a href="#Scaling-Clinical-Trial-Matching-Using-Large-Language-Models-A-Case-Study-in-Oncology" class="headerlink" title="Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology"></a>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02180">http://arxiv.org/abs/2308.02180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cliff Wong, Sheng Zhang, Yu Gu, Christine Moung, Jacob Abel, Naoto Usuyama, Roshanthi Weerasinghe, Brian Piening, Tristan Naumann, Carlo Bifulco, Hoifung Poon</li>
<li>For: The paper is written for scaling clinical trial matching using large language models (LLMs) in the field of oncology.* Methods: The paper uses a systematic study approach with cutting-edge LLMs such as GPT-4 to structure eligibility criteria of clinical trials and extract complex matching logic.* Results: The initial findings show that LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop, but there are still areas for improvement such as context limitation and accuracy.Here’s the simplified Chinese text for the three key points:* For: 这篇论文是为了扩大临床试验匹配使用大型自然语言模型（LLMs）的应用，主要是在肿瘤领域。* Methods: 该论文使用系统性的研究方法，使用最新的GPT-4等 LLMS来结构临床试验资格标准和提取复杂匹配逻辑。* Results: 初步发现结果表明，LLMs已经比前一代强大基elinesubstantially better，可能用作人工协作的初步解决方案，但还有一些需要进一步改进的方向，如上下文限制和准确性。<details>
<summary>Abstract</summary>
Clinical trial matching is a key process in health delivery and discovery. In practice, it is plagued by overwhelming unstructured data and unscalable manual processing. In this paper, we conduct a systematic study on scaling clinical trial matching using large language models (LLMs), with oncology as the focus area. Our study is grounded in a clinical trial matching system currently in test deployment at a large U.S. health network. Initial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate eligibility criteria of clinical trials and extract complex matching logic (e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform prior strong baselines and may serve as a preliminary solution to help triage patient-trial candidates with humans in the loop. Our study also reveals a few significant growth areas for applying LLMs to end-to-end clinical trial matching, such as context limitation and accuracy, especially in structuring patient information from longitudinal medical records.
</details>
<details>
<summary>摘要</summary>
临床试验匹配是医疗卫生系统中一个关键的过程，但在实践中却受到极多的不结构化数据和不可扩展的手动处理的困扰。在这篇论文中，我们进行了系统性的研究，使用大型自然语言模型（LLMs）来扩大临床试验匹配的规模。我们的研究基于一个目前在大型美国医疗网络中测试的临床试验匹配系统。初步的结果很有前途：直接使用最新的GPT-4等 cutting-edge LLMs，可以立即结构化临床试验报名标准和提取复杂的匹配逻辑（例如，嵌入 AND/OR/NOT 结构）。虽然还有一定的改进空间，但LLMs已经明显超过了先前的强基线，并可能作为人工干预的准备解决方案。我们的研究还揭示了应用LLMs到终端临床试验匹配中的一些重要成长点，如Context limitation和准确率，特别是从患者 longitudinal 医疗记录中提取patient信息。
</details></li>
</ul>
<hr>
<h2 id="High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning"><a href="#High-Accuracy-Prediction-of-Metal-Insulator-Metal-Metasurface-with-Deep-Learning" class="headerlink" title="High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning"></a>High-Accuracy Prediction of Metal-Insulator-Metal Metasurface with Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.04450">http://arxiv.org/abs/2308.04450</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaizhu Liu, Hsiang-Chen Chui, Changsen Sun, Xue Han</li>
<li>for: 本研究旨在提出一种基于深度学习的电磁软件计算结果预测方法，以提高计算效率和准确性。</li>
<li>methods: 本研究使用了ResNets-10模型进行预测плазмон喷流表 parameters的方法，并采用了k-fold cross-validation和小学习率的两个阶段训练。</li>
<li>results: 根据实验结果，对铝、金、银金属-隔体-铁的预测损失值分别为-48.45、-46.47和-35.54，表明提出的网络可以取代传统电磁计算方法，并且训练过程只需要少于1,100个迭代。<details>
<summary>Abstract</summary>
Deep learning prediction of electromagnetic software calculation results has been a widely discussed issue in recent years. But the prediction accuracy was still one of the challenges to be solved. In this work, we proposed that the ResNets-10 model was used for predicting plasmonic metasurface S11 parameters. The two-stage training was performed by the k-fold cross-validation and small learning rate. After the training was completed, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace the traditional electromagnetic computing method for calculation within a certain structural range. Besides, this network can finish the training process less than 1,100 epochs. This means that the network training process can effectively lower the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, thereby reducing the time required for the calculation process. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details>
<details>
<summary>摘要</summary>
Recently, deep learning prediction of electromagnetic software calculation results has been a widely discussed issue. However, prediction accuracy was still a challenge to be solved. In this work, we proposed using the ResNets-10 model to predict plasmonic metasurface S11 parameters. We performed two-stage training with k-fold cross-validation and small learning rate. After training, the prediction loss for aluminum, gold, and silver metal-insulator-metal metasurfaces was -48.45, -46.47, and -35.54, respectively. Due to the ultralow error value, the proposed network can replace traditional electromagnetic computing methods for calculation within a certain structural range. Additionally, this network can complete the training process in less than 1,100 epochs, effectively lowering the design process time. The ResNets-10 model we proposed can also be used to design meta-diffractive devices and biosensors, reducing the calculation process time. The ultralow error of the network indicates that this work contributes to the development of future artificial intelligence electromagnetic computing software.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling"><a href="#Diffusion-probabilistic-models-enhance-variational-autoencoder-for-crystal-structure-generative-modeling" class="headerlink" title="Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling"></a>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02165">http://arxiv.org/abs/2308.02165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Teerachote Pakornchote, Natthaphon Choomphon-anomakhun, Sorrjit Arrerut, Chayanon Atthapak, Sakarn Khamkaeo, Thiparat Chotibut, Thiti Bovornratanaraks</li>
<li>for: 生成真实的晶体结构，保持晶体对称性</li>
<li>methods: 使用新的扩散概率模型（DP模型）对原子坐标进行减噪，而不是采用标准的分数匹配方法</li>
<li>results: 能够生成和重建晶体结构，质量与原始CDVAE相似，而且与 relaxed 结构计算得到的碳结构更加接近ground state，能量差值平均为68.1 meV&#x2F;atom 下降，表明DP-CDVAE模型能够更好地代表晶体结构的ground state配置。<details>
<summary>Abstract</summary>
The crystal diffusion variational autoencoder (CDVAE) is a machine learning model that leverages score matching to generate realistic crystal structures that preserve crystal symmetry. In this study, we leverage novel diffusion probabilistic (DP) models to denoise atomic coordinates rather than adopting the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can reconstruct and generate crystal structures whose qualities are statistically comparable to those of the original CDVAE. Furthermore, notably, when comparing the carbon structures generated by the DP-CDVAE model with relaxed structures obtained from density functional theory calculations, we find that the DP-CDVAE generated structures are remarkably closer to their respective ground states. The energy differences between these structures and the true ground states are, on average, 68.1 meV/atom lower than those generated by the original CDVAE. This significant improvement in the energy accuracy highlights the effectiveness of the DP-CDVAE model in generating crystal structures that better represent their ground-state configurations.
</details>
<details>
<summary>摘要</summary>
“单晶扩散条件自适应器”（CDVAE）是一种机器学习模型，利用得分匹配来生成具有实验室同调的晶体结构。在这个研究中，我们使用新的扩散概率模型（DP）来降噪原子坐标而不是采用CDVAE的标准得分匹配方法。我们称之为DP-CDVAE模型。这个模型可以重建和生成具有同等质量的晶体结构，并且在比较碳原子结构的情况下，DP-CDVAE模型生成的结构与 relaxation 计算得到的结构更加接近真实的基体状态。这些结构的能量差异与真实基体状态相比，平均降低了68.1 meV/atom。这显示DP-CDVAE模型具有更好的基体状态表现，并且能够更好地生成具有实验室同调的晶体结构。
</details></li>
</ul>
<hr>
<h2 id="Speaker-Diarization-of-Scripted-Audiovisual-Content"><a href="#Speaker-Diarization-of-Scripted-Audiovisual-Content" class="headerlink" title="Speaker Diarization of Scripted Audiovisual Content"></a>Speaker Diarization of Scripted Audiovisual Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02160">http://arxiv.org/abs/2308.02160</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yogesh Virkar, Brian Thompson, Rohit Paturi, Sundararajan Srinivasan, Marcello Federico</li>
<li>for: 这篇论文主要是为了提高媒体本地化行业中的语音识别技术，具体来说是使用制作过程中使用的脚本来提高电视节目中的 speaker diarization 任务。</li>
<li>methods: 这篇论文提出了一种新的 semi-supervised 方法，通过使用制作过程中的脚本来提取 pseudo-labeled 数据，以提高 speaker diarization 任务的准确率。</li>
<li>results: 在测试集上，这种方法与两个无监督基线模型进行比较，实现了51.7% 的提升。<details>
<summary>Abstract</summary>
The media localization industry usually requires a verbatim script of the final film or TV production in order to create subtitles or dubbing scripts in a foreign language. In particular, the verbatim script (i.e. as-broadcast script) must be structured into a sequence of dialogue lines each including time codes, speaker name and transcript. Current speech recognition technology alleviates the transcription step. However, state-of-the-art speaker diarization models still fall short on TV shows for two main reasons: (i) their inability to track a large number of speakers, (ii) their low accuracy in detecting frequent speaker changes. To mitigate this problem, we present a novel approach to leverage production scripts used during the shooting process, to extract pseudo-labeled data for the speaker diarization task. We propose a novel semi-supervised approach and demonstrate improvements of 51.7% relative to two unsupervised baseline models on our metrics on a 66 show test set.
</details>
<details>
<summary>摘要</summary>
媒体地化业务通常需要最终电影或电视制作的字幕或配音脚本的 verbatim 脚本，以便在外语中创建字幕或配音脚本。特别是 verbatim 脚本（即播放版本）必须以时间码、说话人名和对话内容的结构组织。当前的语音识别技术使得转录步骤得以alleviates。然而，当前的话者分类模型仍然在电视节目中存在两个主要问题：（i）它们无法跟踪大量的说话人，（ii）它们在说话人变化频繁时的准确率低。为解决这个问题，我们提出了一种利用摄制过程中使用的制作脚本，提取 pseudo-labeled 数据来进行说话人分类任务。我们提出了一种新的半超vised方法，并在我们的测试集上实现了51.7%的相对提升，比两个无监督基线模型更高。
</details></li>
</ul>
<hr>
<h2 id="Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling"><a href="#Improved-Order-Analysis-and-Design-of-Exponential-Integrator-for-Diffusion-Models-Sampling" class="headerlink" title="Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling"></a>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02157">http://arxiv.org/abs/2308.02157</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinsheng Zhang, Jiaming Song, Yongxin Chen</li>
<li>for: 提高 diffusion models (DMs) 的抽象速度，使其能够更快速地进行抽象。</li>
<li>methods: 利用高级别的减法积分器 (EI)，并通过重新设计高级别减法积分器来满足所有顺序条件，从而提高抽象质量和稳定性。</li>
<li>results: 通过 theoretically 和实际应用，提出了一种改进的减法积分器（RES），可以提高抽象质量和稳定性，并且在实际应用中可以减少数值缺陷和提高 FID 值。例如，在 ImageNet 扩散模型中，通过将单步 DPM-Solver++ 替换为 ORDER-satisfied RES solver，可以降低数值缺陷的比例为 25.2%，并提高 FID 值为 25.4%。<details>
<summary>Abstract</summary>
Efficient differential equation solvers have significantly reduced the sampling time of diffusion models (DMs) while retaining high sampling quality. Among these solvers, exponential integrators (EI) have gained prominence by demonstrating state-of-the-art performance. However, existing high-order EI-based sampling algorithms rely on degenerate EI solvers, resulting in inferior error bounds and reduced accuracy in contrast to the theoretically anticipated results under optimal settings. This situation makes the sampling quality extremely vulnerable to seemingly innocuous design choices such as timestep schedules. For example, an inefficient timestep scheduler might necessitate twice the number of steps to achieve a quality comparable to that obtained through carefully optimized timesteps. To address this issue, we reevaluate the design of high-order differential solvers for DMs. Through a thorough order analysis, we reveal that the degeneration of existing high-order EI solvers can be attributed to the absence of essential order conditions. By reformulating the differential equations in DMs and capitalizing on the theory of exponential integrators, we propose refined EI solvers that fulfill all the order conditions, which we designate as Refined Exponential Solver (RES). Utilizing these improved solvers, RES exhibits more favorable error bounds theoretically and achieves superior sampling efficiency and stability in practical applications. For instance, a simple switch from the single-step DPM-Solver++ to our order-satisfied RES solver when Number of Function Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$ and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet diffusion model.
</details>
<details>
<summary>摘要</summary>
高效的差分方程解析器在扩散模型（DM）中减少了采样时间，同时保持高质量的采样。其中，对数Integrators（EI）已经成为了状态之一，但现有的高阶EI基本样式依赖于弱化的EI解决方案，从而导致了较差的误差 bound和降低的准确性，与理论预期的结果不符。这种情况使得采样质量极易受到 seems innocuous的设计选择，如时间步骤调度。例如，使用不优化的时间步骤调度可能需要两倍的步骤数量以达到相同的质量。为解决这一问题，我们重新评估了高阶差分解析器的设计。通过系统的顺序分析，我们发现现有高阶EI解决方案的弱化可以归结于缺乏关键的顺序条件。我们根据扩散模型的差分方程和快速Integrators的理论，提出了改进的REFined Exponential Solver（RES）。我们的改进的解析器可以满足所有顺序条件，并且在实际应用中表现出较好的采样效率和稳定性。例如，将单步DPM-Solver++ switched to我们的顺序满足RES解析器，当Number of Function Evaluations（NFE）=9时，可以降低数值缺陷的比例为25.2%，并提高FID的改进率（16.77 vs 12.51）。
</details></li>
</ul>
<hr>
<h2 id="Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization"><a href="#Optimization-on-Pareto-sets-On-a-theory-of-multi-objective-optimization" class="headerlink" title="Optimization on Pareto sets: On a theory of multi-objective optimization"></a>Optimization on Pareto sets: On a theory of multi-objective optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02145">http://arxiv.org/abs/2308.02145</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abhishek Roy, Geelon So, Yi-An Ma</li>
<li>for: 多目标优化中，一个单一决策 вектор需要寻找许多目标之间的最佳变数平衡。这些解答被称为Pareto优化解答，它们是对任何一个目标进行改善都需要在另一个目标上付出的决策 vector。但是，Pareto优化解答的集合可能很大，因此我们进一步考虑一个更实际 significanse的Pareto受限优化问题，其中的目标是将一个偏好函数对应到Pareto集。</li>
<li>methods: 我们调查了本地方法来解决这个受限优化问题，这个问题存在两个特点：（i）参数集是隐式定义的，（ii）通常是非凸非光滑的。我们定义了优化和稳定性的概念，并提供了一个Algorithm，其中的最后迭代速率为$O(K^{-1&#x2F;2})$，对于具有强式凹陷和Lipschitz光滑的目标而言。</li>
<li>results: 我们的研究表明，当目标是强式凹陷和Lipschitz光滑的时候，我们的方法具有最后迭代速率$O(K^{-1&#x2F;2})$，即在最后一迭代时，数据的变化速率为$O(K^{-1&#x2F;2})$。这表明我们的方法在解决Pareto受限优化问题时具有高效率和稳定性。<details>
<summary>Abstract</summary>
In multi-objective optimization, a single decision vector must balance the trade-offs between many objectives. Solutions achieving an optimal trade-off are said to be Pareto optimal: these are decision vectors for which improving any one objective must come at a cost to another. But as the set of Pareto optimal vectors can be very large, we further consider a more practically significant Pareto-constrained optimization problem, where the goal is to optimize a preference function constrained to the Pareto set.   We investigate local methods for solving this constrained optimization problem, which poses significant challenges because the constraint set is (i) implicitly defined, and (ii) generally non-convex and non-smooth, even when the objectives are. We define notions of optimality and stationarity, and provide an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the objectives are strongly convex and Lipschitz smooth.
</details>
<details>
<summary>摘要</summary>
在多目标优化中，单个决策 вектор必须平衡多个目标之间的贸易offs。solutions达到优化的贸易offs是say Pareto优化的：这些决策 вектор在改进任何一个目标时，必须付出另一个目标的代价。但是Pareto优化集可能很大，因此我们进一步考虑一个更实际 significannot的 Pareto受限优化问题，其中的目标是通过对Pareto集进行优化。我们研究了本地方法来解决这个受限优化问题，这个问题具有以下两个特点：（i） constraint set是通过某种方式implcitly定义的，（ii）通常是非拥有凸形和光滑的。我们定义了优化和稳定性的概念，并提供了一个算法，其last-iterate convergence rate为 $O(K^{-1/2})$ 当目标函数是强转化和Lipschitz平滑的时候。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction"><a href="#Event-based-Dynamic-Graph-Representation-Learning-for-Patent-Application-Trend-Prediction" class="headerlink" title="Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction"></a>Event-based Dynamic Graph Representation Learning for Patent Application Trend Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.09780">http://arxiv.org/abs/2308.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Zou, Le Yu, Leilei Sun, Bowen Du, Deqing Wang, Fuzhen Zhuang</li>
<li>for: 预测公司将在下一时期申请哪些专利，以估计其发展策略和找到前期伙伴或竞争对手。</li>
<li>methods: 我们提出了一种基于事件驱动图学习框架的专利申请趋势预测方法，利用公司和专利分类码的启动表示和历史记忆，以及 hierarchical message passing mechanism 来捕捉专利分类码的 semantic proximities。</li>
<li>results: 我们的方法在实际数据上进行了多种实验，并emonstrated 其效果 under various experimental conditions，并且探索了方法在学习分类码 semantics 和跟踪公司技术发展轨迹的能力。<details>
<summary>Abstract</summary>
Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Accurate prediction of what types of patents that companies will apply for in the next period of time can figure out their development strategies and help them discover potential partners or competitors in advance. Although important, this problem has been rarely studied in previous research due to the challenges in modelling companies' continuously evolving preferences and capturing the semantic correlations of classification codes. To fill in this gap, we propose an event-based dynamic graph learning framework for patent application trend prediction. In particular, our method is founded on the memorable representations of both companies and patent classification codes. When a new patent is observed, the representations of the related companies and classification codes are updated according to the historical memories and the currently encoded messages. Moreover, a hierarchical message passing mechanism is provided to capture the semantic proximities of patent classification codes by updating their representations along the hierarchical taxonomy. Finally, the patent application trend is predicted by aggregating the representations of the target company and classification codes from static, dynamic, and hierarchical perspectives. Experiments on real-world data demonstrate the effectiveness of our approach under various experimental conditions, and also reveal the abilities of our method in learning semantics of classification codes and tracking technology developing trajectories of companies."中文翻译：准确预测公司将在下一时间段申请哪种专利，可以为其发展策略提供指导，并在前置的时间内发现可能的合作伙伴或竞争对手。虽然这个问题非常重要，但在前期研究中 rarely studied due to the challenges in modeling companies' continuously evolving preferences and capturing the semantic correlations of classification codes。为了填补这一空白，我们提出了一种基于事件的动态图学学习框架，用于预测专利申请趋势。具体来说，我们的方法基于公司和专利分类代码的记忆表示。当观察到新专利时，相关公司和专利分类代码的表示将根据历史记忆和当前编码的消息进行更新。此外，我们还提供了一种层次消息传递机制，以捕捉专利分类代码的semantic proximity。最后，我们通过 static、动态和层次视角的表示集成来预测专利申请趋势。实验结果表明，我们的方法在不同的实验条件下具有效果，并能够学习分类代码的 semantics和跟踪公司技术发展轨迹。
</details></li>
</ul>
<hr>
<h2 id="Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks"><a href="#Learning-the-solution-operator-of-two-dimensional-incompressible-Navier-Stokes-equations-using-physics-aware-convolutional-neural-networks" class="headerlink" title="Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks"></a>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02137">http://arxiv.org/abs/2308.02137</a></li>
<li>repo_url: None</li>
<li>paper_authors: Viktor Grimm, Alexander Heinlein, Axel Klawonn</li>
<li>for: 本研究旨在解决physics-inclusive机器学习技术中geometry的局限性问题，提出一种能够在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions的方法。</li>
<li>methods: 本研究使用了一种基于U-Net-like CNN和finite difference方法的combined方法，并与数据基于方法进行比较。</li>
<li>results: 研究结果表明，physics-aware CNN可以在不同的geometry中学习稳态 Navier-Stokes方程的解 approximate solutions，并且可以与数据基于方法相结合以提高性能。<details>
<summary>Abstract</summary>
In recent years, the concept of introducing physics to machine learning has become widely popular. Most physics-inclusive ML-techniques however are still limited to a single geometry or a set of parametrizable geometries. Thus, there remains the need to train a new model for a new geometry, even if it is only slightly modified. With this work we introduce a technique with which it is possible to learn approximate solutions to the steady-state Navier--Stokes equations in varying geometries without the need of parametrization. This technique is based on a combination of a U-Net-like CNN and well established discretization methods from the field of the finite difference method.The results of our physics-aware CNN are compared to a state-of-the-art data-based approach. Additionally, it is also shown how our approach performs when combined with the data-based approach.
</details>
<details>
<summary>摘要</summary>
近年来，将物理学引入机器学习的概念得到了广泛的推广。然而，大多数物理包含的机器学习技术仍然受限于单个几何或一组可 parametrize 的几何。因此，在新的几何上训练新的模型仍然是必要的。我们在这里介绍一种可以在不同几何中学习稳态奈特-斯托克方程的估计解的技术。这种技术基于一种组合了 U-Net 类 CNN 和已确立的精度方法的finite difference方法。我们对我们的物理意识 CNN 的结果进行了与当前最佳数据驱动方法的比较，同时还展示了我们的方法与数据驱动方法的组合效果。
</details></li>
</ul>
<hr>
<h2 id="Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke"><a href="#Can-Attention-Be-Used-to-Explain-EHR-Based-Mortality-Prediction-Tasks-A-Case-Study-on-Hemorrhagic-Stroke" class="headerlink" title="Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke"></a>Can Attention Be Used to Explain EHR-Based Mortality Prediction Tasks: A Case Study on Hemorrhagic Stroke</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.05110">http://arxiv.org/abs/2308.05110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qizhang Feng, Jiayi Yuan, Forhan Bin Emdad, Karim Hanna, Xia Hu, Zhe He</li>
<li>for: 预测中风死亡风险的早期预测</li>
<li>methods: 使用一种新的解释性听力基于变换器模型，以提高预测模型的准确性和可读性</li>
<li>results: 研究表明，这种解释性听力基于变换器模型可以提高预测模型的准确性和可读性，并且可以提供有用的特征重要性信息。<details>
<summary>Abstract</summary>
Stroke is a significant cause of mortality and morbidity, necessitating early predictive strategies to minimize risks. Traditional methods for evaluating patients, such as Acute Physiology and Chronic Health Evaluation (APACHE II, IV) and Simplified Acute Physiology Score III (SAPS III), have limited accuracy and interpretability. This paper proposes a novel approach: an interpretable, attention-based transformer model for early stroke mortality prediction. This model seeks to address the limitations of previous predictive models, providing both interpretability (providing clear, understandable explanations of the model) and fidelity (giving a truthful explanation of the model's dynamics from input to output). Furthermore, the study explores and compares fidelity and interpretability scores using Shapley values and attention-based scores to improve model explainability. The research objectives include designing an interpretable attention-based transformer model, evaluating its performance compared to existing models, and providing feature importance derived from the model.
</details>
<details>
<summary>摘要</summary>
stroke 是一个重要的死亡和残留症状的原因，需要早期预测方法来减少风险。传统的评估病人方法，如急性physiology和慢性健康评估（APACHE II、IV）和简化型急性 физиiology分数III（SAPS III），有限的准确性和可读性。这篇论文提出了一种新的方法：一种可解释的、注意力基本变换模型，用于早期stroke mortality预测。这个模型旨在解决之前的预测模型的局限性，提供了可解释性（提供明确、理解的解释）和诚实性（从输入到输出的模型动力学提供真实的解释）。此外，研究还研究了和比较了可解释性和诚实性分数使用Shapley值和注意力基本分数来提高模型解释性。研究的目标包括设计一种可解释的注意力基本变换模型，评估其性能与现有模型相比，并提供来自模型的特征重要性。
</details></li>
</ul>
<hr>
<h2 id="Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity"><a href="#Analysis-and-Optimization-of-Wireless-Federated-Learning-with-Data-Heterogeneity" class="headerlink" title="Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity"></a>Analysis and Optimization of Wireless Federated Learning with Data Heterogeneity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.03521">http://arxiv.org/abs/2308.03521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuefeng Han, Jun Li, Wen Chen, Zhen Mei, Kang Wei, Ming Ding, H. Vincent Poor</li>
<li>for: 本文旨在研究和优化无线 Federated Learning（FL）中的数据多样性和无线资源分配问题，以提高FL的性能和能效性。</li>
<li>methods: 本文使用closed-form表达式来计算FL损失函数的上界，并对Client Scheduling、资源分配和本地训练 epoch数进行优化。</li>
<li>results: 实验结果表明，提出的算法在实际数据集上比其他参考方法更高的学习精度和能 consumption。<details>
<summary>Abstract</summary>
With the rapid proliferation of smart mobile devices, federated learning (FL) has been widely considered for application in wireless networks for distributed model training. However, data heterogeneity, e.g., non-independently identically distributions and different sizes of training data among clients, poses major challenges to wireless FL. Limited communication resources complicate the implementation of fair scheduling which is required for training on heterogeneous data, and further deteriorate the overall performance. To address this issue, this paper focuses on performance analysis and optimization for wireless FL, considering data heterogeneity, combined with wireless resource allocation. Specifically, we first develop a closed-form expression for an upper bound on the FL loss function, with a particular emphasis on data heterogeneity described by a dataset size vector and a data divergence vector. Then we formulate the loss function minimization problem, under constraints on long-term energy consumption and latency, and jointly optimize client scheduling, resource allocation, and the number of local training epochs (CRE). Next, via the Lyapunov drift technique, we transform the CRE optimization problem into a series of tractable problems. Extensive experiments on real-world datasets demonstrate that the proposed algorithm outperforms other benchmarks in terms of the learning accuracy and energy consumption.
</details>
<details>
<summary>摘要</summary>
随着智能移动设备的普及，分布式学习（FL）在无线网络中得到了广泛的考虑，用于分布式模型训练。然而，数据不均衡，如非独立同分布和不同的训练数据大小 среди客户端，对无线FL的应用带来了主要挑战。限制通信资源使得实现公平调度变得更加困难，从而降低总性能。为解决这个问题，这篇论文关注无线FL的性能分析和优化，考虑到数据不均衡，并与无线资源分配相结合。首先，我们开发了一个关于FL损失函数上的上界，强调数据不均衡的特点，由一个数据大小向量和一个数据差异向量描述。然后，我们将损失函数最小化问题转化为一个具有长期能源占用和延迟的约束的优化问题。通过利用Lyapunov漂移技术，我们将CRE优化问题转化为一系列可解的问题。在实际数据上进行了广泛的实验，结果表明，我们的算法在学习精度和能源消耗方面比其他参考值更高。
</details></li>
</ul>
<hr>
<h2 id="Branched-Latent-Neural-Operators"><a href="#Branched-Latent-Neural-Operators" class="headerlink" title="Branched Latent Neural Operators"></a>Branched Latent Neural Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02599">http://arxiv.org/abs/2308.02599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanfordcbcl/blno.jl">https://github.com/stanfordcbcl/blno.jl</a></li>
<li>paper_authors: Matteo Salvador, Alison Lesley Marsden</li>
<li>for:  This paper aims to develop a novel computational tool for building reliable and efficient reduced-order models for digital twinning in engineering applications.</li>
<li>methods: The paper proposes the use of Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. BLNOs are defined as simple and compact feedforward partially-connected neural networks that structurally disentangle inputs with different intrinsic roles.</li>
<li>results: The paper demonstrates the effectiveness of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. The paper shows that BLNOs can retain just 7 hidden layers and 19 neurons per layer, and achieve a mean square error of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations.<details>
<summary>Abstract</summary>
We introduce Branched Latent Neural Operators (BLNOs) to learn input-output maps encoding complex physical processes. A BLNO is defined by a simple and compact feedforward partially-connected neural network that structurally disentangles inputs with different intrinsic roles, such as the time variable from model parameters of a differential equation, while transferring them into a generic field of interest. BLNOs leverage interpretable latent outputs to enhance the learned dynamics and break the curse of dimensionality by showing excellent generalization properties with small training datasets and short training times on a single processor. Indeed, their generalization error remains comparable regardless of the adopted discretization during the testing phase. Moreover, the partial connections, in place of a fully-connected structure, significantly reduce the number of tunable parameters. We show the capabilities of BLNOs in a challenging test case involving biophysically detailed electrophysiology simulations in a biventricular cardiac model of a pediatric patient with hypoplastic left heart syndrome. The model includes a purkinje network for fast conduction and a heart-torso geometry. Specifically, we trained BLNOs on 150 in silico generated 12-lead electrocardiograms (ECGs) while spanning 7 model parameters, covering cell-scale, organ-level and electrical dyssynchrony. Although the 12-lead ECGs manifest very fast dynamics with sharp gradients, after automatic hyperparameter tuning the optimal BLNO, trained in less than 3 hours on a single CPU, retains just 7 hidden layers and 19 neurons per layer. The mean square error is on the order of $10^{-4}$ on an independent test dataset comprised of 50 additional electrophysiology simulations. This paper provides a novel computational tool to build reliable and efficient reduced-order models for digital twinning in engineering applications.
</details>
<details>
<summary>摘要</summary>
我们引入分支隐藏神经操作符（BLNOs），以学习输入-输出对应器，模型复杂物理过程。 BLNO 是一个简单且紧凑的Feedforward 内部连接神经网络，它将输入变数分类为不同的内在角色，例如时间变数和模型参数，并将它们转换为一个通用的应用领域。 BLNO 利用可读性的隐藏输出增强学习过程，并突破维度给定问题的咒语，通过在训练阶段实现小训练集和短时间内的优秀一致性。此外，对于完全连接结构而言，部分连接可以对缩减可调 Parameters 数量。我们透过实际应用在一个儿童心脏病 hypoplastic left heart syndrome 的双心室心脏模型中，并在该模型中包含 Purkinje 网络和心脏-肋间 geometry。具体来说，我们将 BLNO 训练在 150 个silico生成的 12 项电击ogram (ECG) 上，涵盖 7 个模型参数，包括细胞层、器官层和电子 Dyssynchrony。虽然 12 项 ECG 呈现非常快的动态，但是通过自动优化参数后，最佳 BLNO 在仅三个小时内在单一 CPU 上训练，只有 7 个隐藏层和 19 个神经元 per 层。该模型的平方误差在统计上为 $10^{-4}$，在 50 个其他电生物频谱 simulations 的独立测试集中进行验证。本研究提供了一个新的 Computational 工具，可以建立可靠和高效的实际应用中的简化模型，以应用于工程应用中的数字双胞志。
</details></li>
</ul>
<hr>
<h2 id="Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization"><a href="#Eva-A-General-Vectorized-Approximation-Framework-for-Second-order-Optimization" class="headerlink" title="Eva: A General Vectorized Approximation Framework for Second-order Optimization"></a>Eva: A General Vectorized Approximation Framework for Second-order Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02123">http://arxiv.org/abs/2308.02123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lin Zhang, Shaohuai Shi, Bo Li</li>
<li>for: 这个研究旨在提高深度学习模型训练的效率，减少计算和记忆过程中的过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程过程<details>
<summary>Abstract</summary>
Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further extend Eva to a general vectorized approximation framework to improve the compute and memory efficiency of two existing second-order algorithms (FOOF and Shampoo) without affecting their convergence performance. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to 2.05x and 2.42x compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively.
</details>
<details>
<summary>摘要</summary>
Second-order优化算法在训练深度学习模型时展现出极佳的收敛性质，但通常会导致计算和内存开销增加。这可能会导致训练效率低于首次优化算法 such as 随机梯度下降（SGD）。在这项工作中，我们提出了一种具有内存和时间效率的第二次优化算法名为Eva，并采用了两种新的技术：1）我们通过小批量训练数据的克ро内克分解来减少内存占用，2）我们 derivate了高效的更新公式，不需要直接计算矩阵的逆元。我们进一步扩展Eva到一个通用的向量化近似框架，以提高两个现有的第二次优化算法（FOOF和Shampoo）的计算和内存效率，无需影响其收敛性能。我们在不同的模型和数据集上进行了广泛的实验，结果显示，Eva可以比首次优化算法和第二次优化算法（K-FAC和Shampoo）减少综合训练时间，具体的比例为2.05倍和2.42倍。
</details></li>
</ul>
<hr>
<h2 id="Model-Provenance-via-Model-DNA"><a href="#Model-Provenance-via-Model-DNA" class="headerlink" title="Model Provenance via Model DNA"></a>Model Provenance via Model DNA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02121">http://arxiv.org/abs/2308.02121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Mu, Yu Wang, Yehong Zhang, Jiaqi Zhang, Hui Wang, Yang Xiang, Yue Yu</li>
<li>for: This paper focuses on the problem of Model Provenance (MP) in machine learning (ML), which aims to determine whether a source model serves as the provenance for a target model.</li>
<li>methods: The authors introduce a novel concept of Model DNA, which represents the unique characteristics of a machine learning model, and use a data-driven and model-driven representation learning method to encode the model’s training data and input-output information as a compact and comprehensive representation of the model.</li>
<li>results: The authors develop an efficient framework for model provenance identification, which enables them to accurately identify whether a source model is a pre-training model of a target model. They conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of their approach.<details>
<summary>Abstract</summary>
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model. We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance.
</details>
<details>
<summary>摘要</summary>
To address this gap, we introduce a novel concept called Model DNA, which represents the unique characteristics of a machine learning model. We use a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this model DNA, we develop an efficient framework for model provenance identification, which enables us to identify whether a source model is a pre-training model of a target model.We conduct evaluations on both computer vision and natural language processing tasks using various models, datasets, and scenarios to demonstrate the effectiveness of our approach in accurately identifying model provenance. Our approach is efficient and can be applied to a wide range of ML models, providing a valuable tool for ensuring the security and intellectual property of ML models.
</details></li>
</ul>
<hr>
<h2 id="Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries"><a href="#Designing-a-Deep-Learning-Driven-Resource-Efficient-Diagnostic-System-for-Metastatic-Breast-Cancer-Reducing-Long-Delays-of-Clinical-Diagnosis-and-Improving-Patient-Survival-in-Developing-Countries" class="headerlink" title="Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries"></a>Designing a Deep Learning-Driven Resource-Efficient Diagnostic System for Metastatic Breast Cancer: Reducing Long Delays of Clinical Diagnosis and Improving Patient Survival in Developing Countries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02597">http://arxiv.org/abs/2308.02597</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Gao, Dayong Wang, Yi Huang</li>
<li>for: 这份研究旨在解决癌症病理诊断过程中的时间延迟问题，特别是癌症患者在发展中国家中的诊断过程中的延迟，以提高癌症患者的存活率。</li>
<li>methods: 这份研究使用了深度学习技术，开发了一个基于MobileNetV2的诊断模型，能够实现高精度的诊断和computational efficiency。</li>
<li>results: 根据评估结果，MobileNetV2基本模型在诊断精度、模型普遍性和模型训练效率等方面都超过了VGG16、ResNet50和ResNet101模型。此外，Visual比较表明，MobileNetV2诊断模型能够识别非常小的癌症细胞在大量正常细胞中，实现了人工影像分析的挑战。<details>
<summary>Abstract</summary>
Breast cancer is one of the leading causes of cancer mortality. Breast cancer patients in developing countries, especially sub-Saharan Africa, South Asia, and South America, suffer from the highest mortality rate in the world. One crucial factor contributing to the global disparity in mortality rate is long delay of diagnosis due to a severe shortage of trained pathologists, which consequently has led to a large proportion of late-stage presentation at diagnosis. The delay between the initial development of symptoms and the receipt of a diagnosis could stretch upwards 15 months. To tackle this critical healthcare disparity, this research has developed a deep learning-based diagnosis system for metastatic breast cancer that can achieve high diagnostic accuracy as well as computational efficiency. Based on our evaluation, the MobileNetV2-based diagnostic model outperformed the more complex VGG16, ResNet50 and ResNet101 models in diagnostic accuracy, model generalization, and model training efficiency. The visual comparisons between the model prediction and ground truth have demonstrated that the MobileNetV2 diagnostic models can identify very small cancerous nodes embedded in a large area of normal cells which is challenging for manual image analysis. Equally Important, the light weighted MobleNetV2 models were computationally efficient and ready for mobile devices or devices of low computational power. These advances empower the development of a resource-efficient and high performing AI-based metastatic breast cancer diagnostic system that can adapt to under-resourced healthcare facilities in developing countries. This research provides an innovative technological solution to address the long delays in metastatic breast cancer diagnosis and the consequent disparity in patient survival outcome in developing countries.
</details>
<details>
<summary>摘要</summary>
乳癌是全球最主要的癌症死亡原因之一，特别是在发展中国家，如非洲南部、南亚和南美， breast cancer 患者的死亡率最高。一个重要的因素导致全球的医疗差距是诊断延迟，因为缺乏培训的病理学家，导致许多患者在诊断时 already in 晚期。延迟从症状出现到诊断的时间可以达15个月。为了解决这个严重的医疗差距，这项研究开发了一个基于深度学习的乳癌诊断系统，可以实现高精度和计算效率。根据我们的评估，使用 MobileNetV2 模型的诊断模型在精度、通用性和训练效率三个方面都高于 VGG16、ResNet50 和 ResNet101 模型。视觉比较表明，MobileNetV2 模型可以准确地检测小型患者中的癌细胞，这是人工图像分析困难的。此外，MobileNetV2 模型的计算效率较低，适用于移动设备或低计算能力的设备。这些进步使得可以开发一个资源高效和高性能的人工智能基于乳癌诊断系统，适应发展中国家的医疗设施。这项研究提供了一种创新的科技解决方案，以Address the long delays in metastatic breast cancer diagnosis and the resulting disparity in patient survival outcomes in developing countries.
</details></li>
</ul>
<hr>
<h2 id="VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs"><a href="#VQGraph-Graph-Vector-Quantization-for-Bridging-GNNs-and-MLPs" class="headerlink" title="VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs"></a>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02117">http://arxiv.org/abs/2308.02117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangling0818/vqgraph">https://github.com/yangling0818/vqgraph</a></li>
<li>paper_authors: Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec</li>
<li>for: 提高Graph Neural Networks (GNNs)的批处理能力和实时性，以便在具有延迟限制的实际应用中使用。</li>
<li>methods: 采用知识传承（KD）学习计算效率高的多层感知器（MLP），通过模仿GNN的输出来学习GNN的知识。同时，使用一种新的结构意识graph tokenizer，以及一种基于软标签分配的token-based distillation目标，以便充分传递GNN的结构知识到MLP中。</li>
<li>results: 实验和分析表明，VQGraph可以减少GNN的批处理时间，并且在七个图数据集上实现新的状态机器人性表现，包括在推导和泵化设置下的表现。VQGraph可以比GNN更快地进行推理，并且在实际应用中可以提高GNN的准确率。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) conduct message passing which aggregates local neighbors to update node representations. Such message passing leads to scalability issues in practical latency-constrained applications. To address this issue, recent methods adopt knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (MLP) by mimicking the output of GNN. However, the existing GNN representation space may not be expressive enough for representing diverse local structures of the underlying graph, which limits the knowledge transfer from GNN to MLP. Here we present a novel framework VQGraph to learn a powerful graph representation space for bridging GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which explicitly represents the nodes of diverse local structures as numerous discrete tokens and constitutes a meaningful codebook. Equipped with the learned codebook, we propose a new token-based distillation objective based on soft token assignments to sufficiently transfer the structural knowledge from GNN to MLP. Extensive experiments and analyses demonstrate the strong performance of VQGraph, where we achieve new state-of-the-art performance on GNN-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.
</details>
<details>
<summary>摘要</summary>
graph neural networks (GNNs) 通过消息传递来更新节点表示，这会导致实际延迟应用中的可扩展性问题。为解决这个问题，现有方法采用知识传递（KD）来学习计算效率高的多层感知器（MLP），但是现有GNN表示空间可能不够表示图像下的多样化本地结构，这限制了GNN的知识传递。我们提出了一种新的框架VQGraph，用于学习图像表示空间，以bridging GNNs和MLPs。我们采用变体的vector-quantized variational autoencoder（VQ-VAE）的encoder作为结构意识图像tokenizer，该tokenizer可以明确表示不同本地结构中的节点，并组成一个有意义的代码库。利用学习的代码库，我们提出了一个新的符号分配目标，以便充分传递GNN中的结构知识到MLP。我们在七个图像 dataset 上进行了广泛的实验和分析，并证明了VQGraph的强大表现。我们在transductive和induction Setting中， achieved new state-of-the-art performance on GNN-MLP distillation，并且在GNN和独立MLP上的性能上提高了3.90%和28.05%。此外，我们还证明了VQGraph在GNN上进行更快的推理，比GNN的828倍。代码：https://github.com/YangLing0818/VQGraph。
</details></li>
</ul>
<hr>
<h2 id="Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network"><a href="#Breast-Ultrasound-Tumor-Classification-Using-a-Hybrid-Multitask-CNN-Transformer-Network" class="headerlink" title="Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network"></a>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02101">http://arxiv.org/abs/2308.02101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bryar Shareef, Min Xian, Aleksandar Vakanski, Haotian Wang</li>
<li>for: 这个研究旨在提出一个混合多任务深度学习网络（Hybrid-MT-ESTAN），用于肺肿瘤分类和分 segmentation。</li>
<li>methods: 这个方法使用了 CNN 和 Swin Transformer 两种不同的架构，以提高全球背景信息的捕捉和地方图像特征的维持。</li>
<li>results: 实验结果显示，Hybrid-MT-ESTAN 得到了最高的准确率（82.7%）、敏感度（86.4%）和 F1 分数（86.0%）。<details>
<summary>Abstract</summary>
Capturing global contextual information plays a critical role in breast ultrasound (BUS) image classification. Although convolutional neural networks (CNNs) have demonstrated reliable performance in tumor classification, they have inherent limitations for modeling global and long-range dependencies due to the localized nature of convolution operations. Vision Transformers have an improved capability of capturing global contextual information but may distort the local image patterns due to the tokenization operations. In this study, we proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN, designed to perform BUS tumor classification and segmentation using a hybrid architecture composed of CNNs and Swin Transformer components. The proposed approach was compared to nine BUS classification methods and evaluated using seven quantitative metrics on a dataset of 3,320 BUS images. The results indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and F1 score of 82.7%, 86.4%, and 86.0%, respectively.
</details>
<details>
<summary>摘要</summary>
capture global contextual information 在乳腺超声图像分类中扮演着关键性的角色。尽管 convolutional neural networks (CNNs) 在肿瘤分类中表现出了可靠的性，但它们具有内置的局部化特性，因此可能导致模型长距离和全局依赖关系的模型化困难。 vision transformers 具有改善全局上下文信息捕捉的能力，但可能会因为 tokenization 操作而导致本地图像模式的扭曲。在这项研究中，我们提出了一种 hybrid multitask deep neural network called Hybrid-MT-ESTAN，用于实现乳腺超声图像分类和分割。我们的方法与 nine 种乳腺分类方法进行比较，并在一个包含 3,320 个乳腺超声图像的数据集上进行评估。结果表明，Hybrid-MT-ESTAN 达到了最高的准确率、敏感度和 F1 分数，即 82.7%、86.4% 和 86.0%  соответственно。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge"><a href="#Efficient-Model-Adaptation-for-Continual-Learning-at-the-Edge" class="headerlink" title="Efficient Model Adaptation for Continual Learning at the Edge"></a>Efficient Model Adaptation for Continual Learning at the Edge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02084">http://arxiv.org/abs/2308.02084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachary A. Daniels, Jun Hu, Michael Lomnitz, Phil Miller, Aswin Raghavan, Joe Zhang, Michael Piacentino, David Zhang</li>
<li>for: 这个研究旨在提供一个非站势自动机器学习（AutoML）框架，以便在资料分布随时变化时进行高效的连续学习。</li>
<li>methods: 这个框架使用固定的深度神经网（DNN）特征嵌入器，并训练浅层网络来处理新数据。它还使用了数维计算（HDC）和零 shot神经架搜索（ZS-NAS）来探测新数据是否为外部数据（OOD），并适当地调整模型以适应OOD数据。</li>
<li>results: 在多个域别数据集上进行评估，这个框架实现了优秀的性能，比如果探测OOD数据和几何shot NAS。<details>
<summary>Abstract</summary>
Most machine learning (ML) systems assume stationary and matching data distributions during training and deployment. This is often a false assumption. When ML models are deployed on real devices, data distributions often shift over time due to changes in environmental factors, sensor characteristics, and task-of-interest. While it is possible to have a human-in-the-loop to monitor for distribution shifts and engineer new architectures in response to these shifts, such a setup is not cost-effective. Instead, non-stationary automated ML (AutoML) models are needed. This paper presents the Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning under domain shifts. The EAR framework uses a fixed deep neural network (DNN) feature encoder and trains shallow networks on top of the encoder to handle novel data. The EAR framework is capable of 1) detecting when new data is out-of-distribution (OOD) by combining DNNs with hyperdimensional computing (HDC), 2) identifying low-parameter neural adaptors to adapt the model to the OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing catastrophic forgetting on previous tasks by progressively growing the neural architecture as needed and dynamically routing data through the appropriate adaptors and reconfigurators for handling domain-incremental and class-incremental continual learning. We systematically evaluate our approach on several benchmark datasets for domain adaptation and demonstrate strong performance compared to state-of-the-art algorithms for OOD detection and few-/zero-shot NAS.
</details>
<details>
<summary>摘要</summary>
大多数机器学习（ML）系统假设训练和部署时数据分布是静止的，这是一个不实际的假设。当 ML 模型在实际设备上部署时，数据分布经常会随着环境因素、传感器特性和任务 интерес而变化。虽然可以有人在Loop监控数据分布的变化并为此设计新的建筑，但这种设置不是可cost-effective的。而是需要不静止的自动机器学习（AutoML）模型。这篇论文提出了Encoder-Adaptor-Reconfigurator（EAR）框架，用于效率地进行适应域shift continual learning。EAR框架使用固定的深度神经网络（DNN）特征编码器，并在编码器之上训练浅层网络来处理新数据。EAR框架可以1）将新数据标记为out-of-distribution（OOD），通过将DNN与高维计算（HDC）结合使用，2）通过零 shot neural architecture search（ZS-NAS）来适应OOD数据，3）在前一个任务上避免忘记性衰变，通过逐渐增加神经建筑和动态路由数据通过适当的适应器和重配置器来处理域增量和类增量 continual learning。我们系统性地评估了我们的方法在域适应数据上的多个benchmark datasets，并demonstrated strong performance compared to state-of-the-art algorithms for OOD detection和few-/zero-shot NAS。
</details></li>
</ul>
<hr>
<h2 id="Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare"><a href="#Target-specification-bias-counterfactual-prediction-and-algorithmic-fairness-in-healthcare" class="headerlink" title="Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare"></a>Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02081">http://arxiv.org/abs/2308.02081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eran Tal</li>
<li>for: 这篇论文探讨了机器学习（ML）在医疗领域中的偏见问题，并提出了一种更普遍的偏见来源：目标规定偏见。</li>
<li>methods: 这篇论文使用了现有的数据和健康差异的研究，以及现有的机器学习算法和模型。然而，它发现了一种更加普遍的偏见来源：目标规定偏见。</li>
<li>results: 这篇论文发现了target specification bias可能会导致估计准确性过高，使用医疗资源不fficient，并导致伤害病人的决策。<details>
<summary>Abstract</summary>
Bias in applications of machine learning (ML) to healthcare is usually attributed to unrepresentative or incomplete data, or to underlying health disparities. This article identifies a more pervasive source of bias that affects the clinical utility of ML-enabled prediction tools: target specification bias. Target specification bias arises when the operationalization of the target variable does not match its definition by decision makers. The mismatch is often subtle, and stems from the fact that decision makers are typically interested in predicting the outcomes of counterfactual, rather than actual, healthcare scenarios. Target specification bias persists independently of data limitations and health disparities. When left uncorrected, it gives rise to an overestimation of predictive accuracy, to inefficient utilization of medical resources, and to suboptimal decisions that can harm patients. Recent work in metrology - the science of measurement - suggests ways of counteracting target specification bias and avoiding its harmful consequences.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）在医疗领域中的偏见通常被归结于不完整或不代表性的数据，或者下面的健康差异。本文标识了更广泛的偏见来源，对临床实用性有影响的预测工具：目标规定偏见。目标规定偏见发生在运行化目标变量时与决策者定义的目标之间的匹配不匹配。这种匹配不匹配通常是柔和的，来自于决策者通常关心预测实际医疗情况下的结果，而不是实际情况。这种偏见不受数据限制和健康差异影响，并且不会被纠正。如果不纠正，它会导致预测精度的过高估计，医疗资源的不效利用，以及对病人伤害的不佳决策。近些年的metrology研究（量度科学）提供了对抗目标规定偏见的方法，避免其不良后果。
</details></li>
</ul>
<hr>
<h2 id="Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection"><a href="#Causality-Guided-Disentanglement-for-Cross-Platform-Hate-Speech-Detection" class="headerlink" title="Causality Guided Disentanglement for Cross-Platform Hate Speech Detection"></a>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02080">http://arxiv.org/abs/2308.02080</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paras2612/catch">https://github.com/paras2612/catch</a></li>
<li>paper_authors: Paras Sheth, Tharindu Kumarage, Raha Moraffah, Aman Chadha, Huan Liu</li>
<li>for: 寻找一种可以在多个不同平台上推断仇恨言语的 hate speech 检测模型。</li>
<li>methods: 我们使用了分离输入表示的方法，将输入特征分解成不同平台的特征和共同的特征，以便在不同平台上学习通用的 hate speech 检测模型。我们还学习了 causal 关系，以便更好地理解共同的表示。</li>
<li>results: 我们的模型在四个不同平台上进行了广泛的实验，结果显示我们的模型比现有的状态对方法更高效地检测通用 hate speech。<details>
<summary>Abstract</summary>
Social media platforms, despite their value in promoting open discourse, are often exploited to spread harmful content. Current deep learning and natural language processing models used for detecting this harmful content overly rely on domain-specific terms affecting their capabilities to adapt to generalizable hate speech detection. This is because they tend to focus too narrowly on particular linguistic signals or the use of certain categories of words. Another significant challenge arises when platforms lack high-quality annotated data for training, leading to a need for cross-platform models that can adapt to different distribution shifts. Our research introduces a cross-platform hate speech detection model capable of being trained on one platform's data and generalizing to multiple unseen platforms. To achieve good generalizability across platforms, one way is to disentangle the input representations into invariant and platform-dependent features. We also argue that learning causal relationships, which remain constant across diverse environments, can significantly aid in understanding invariant representations in hate speech. By disentangling input into platform-dependent features (useful for predicting hate targets) and platform-independent features (used to predict the presence of hate), we learn invariant representations resistant to distribution shifts. These features are then used to predict hate speech across unseen platforms. Our extensive experiments across four platforms highlight our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:社交媒体平台，尽管它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过于依赖于域专门的术语，这会导致它们在检测普遍的谩骂言语方面减少其能力。另一个主要挑战是当 платформы缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩骂言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩骂目标）和平台独立的特征（用于预测谩骂存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩骂言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.Translated into Traditional Chinese:社交媒体平台，不过它们在促进开放对话方面具有价值，但它们经常被利用来传播危险内容。现有的深度学习和自然语言处理模型在检测这种危险内容方面过度依赖域专门的术语，这会导致它们在检测普遍的谩驳言语方面减少其能力。另一个主要挑战是当平台缺乏高质量的标注数据 для训练时，导致需要跨平台模型，可以适应不同的分布Shift。我们的研究推出了一种可以在不同的平台上训练的跨平台谩驳言语检测模型。为了实现良好的泛化性 across platforms，我们可以分解输入表示为不变和平台特定的特征。我们还认为，学习不变的关系，可以在多种环境中保持相同的常量，可以大幅提高对不变表示的理解。通过将输入分解为平台特定的特征（有用于预测谩驳目标）和平台独立的特征（用于预测谩驳存在），我们学习了不变的表示，抗性于分布Shift。这些特征然后用于预测谩驳言语 across 未看到的平台。我们的广泛的实验 across four platforms  highlights our model's enhanced efficacy compared to existing state-of-the-art methods in detecting generalized hate speech.
</details></li>
</ul>
<hr>
<h2 id="Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale"><a href="#Specious-Sites-Tracking-the-Spread-and-Sway-of-Spurious-News-Stories-at-Scale" class="headerlink" title="Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale"></a>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02068">http://arxiv.org/abs/2308.02068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric</li>
<li>for: 这篇论文旨在Automatically track and analyze online news narratives to identify misinformation and support fact-checking efforts.</li>
<li>methods: 该系统使用大型自然语言模型MPNet和DP-Means归一 clustering算法，每天抓取1,404家不可靠新闻网站，以分析在线社区中流行的新闻 narative。</li>
<li>results: 研究发现2022年最受欢迎的新闻 narative，并确定了传播这些新闻 narative的最有影响力的网站。系统还可以帮助 fact-checkers like Politifact, Reuters, AP News 更快地识别和推篱虚假新闻。<details>
<summary>Abstract</summary>
Misinformation, propaganda, and outright lies proliferate on the web, with some narratives having dangerous real-world consequences on public health, elections, and individual safety. However, despite the impact of misinformation, the research community largely lacks automated and programmatic approaches for tracking news narratives across online platforms. In this work, utilizing daily scrapes of 1,404 unreliable news websites, the large-language model MPNet, and DP-Means clustering, we introduce a system to automatically isolate and analyze the narratives spread within online ecosystems. Identifying 55,301 narratives on these 1,404 websites, we describe the most prevalent narratives spread in 2022 and identify the most influential websites that originate and magnify narratives. Finally, we show how our system can be utilized to detect new narratives originating from unreliable news websites and aid fact-checkers like Politifact, Reuters, and AP News in more quickly addressing misinformation stories.
</details>
<details>
<summary>摘要</summary>
互联网上充满谣言、宣传和谎言，一些媒体报道有危害公共健康、选举和个人安全的危险。然而，研究社区在 automatization 和 programmatic 方面对新闻媒体的跟踪仍然缺乏有效的方法。在这项工作中，我们利用每天抓取 1,404 个不可靠新闻网站的数据，大型自然语言模型 MPNet，以及 DP-Means 聚类算法，提出一个自动从在线生态系统中分离和分析新闻媒体的系统。我们分析了这些网站上的 55,301 个媒体报道，描述了在 2022 年最具影响力的新闻媒体，以及它们如何促进和强化新闻媒体。最后，我们示出了我们的系统可以帮助ifact-checkers like Politifact、Reuters 和 AP News 更快地处理谣言故事。
</details></li>
</ul>
<hr>
<h2 id="Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives"><a href="#Mitigating-Task-Interference-in-Multi-Task-Learning-via-Explicit-Task-Routing-with-Non-Learnable-Primitives" class="headerlink" title="Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives"></a>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02066">http://arxiv.org/abs/2308.02066</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl">https://github.com/zhichao-lu/etr-nlp-mtl</a></li>
<li>paper_authors: Chuntao Ding, Zhichao Lu, Shangguang Wang, Ran Cheng, Vishnu Naresh Boddeti</li>
<li>for: 这个论文目的是提出一种基于非学习 primitives 和显式任务路由（ETR）的多任务学习（MTL）方法，以降低任务干扰。</li>
<li>methods: 该方法使用非学习 primitives 提取多个任务共同的特征，并将这些特征重新组合到共同分支和每个任务专门的分支中。它还使用显式任务路由来隔离学习参数，以便降低任务干扰。</li>
<li>results: 实验结果表明，ETR-NLP 在图像水平分类和像素粒度稠密预测多任务学习问题中具有显著优势，比基eline模型更高的性能，同时具有更少的学习参数和相似的计算量。代码可以在这里下载：<a target="_blank" rel="noopener" href="https://github.com/zhichao-lu/etr-nlp-mtl%E3%80%82">https://github.com/zhichao-lu/etr-nlp-mtl。</a><details>
<summary>Abstract</summary>
Multi-task learning (MTL) seeks to learn a single model to accomplish multiple tasks by leveraging shared information among the tasks. Existing MTL models, however, have been known to suffer from negative interference among tasks. Efforts to mitigate task interference have focused on either loss/gradient balancing or implicit parameter partitioning with partial overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task interference through a synergistic combination of non-learnable primitives (NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable primitives to extract a diverse set of task-agnostic features and recombine them into a shared branch common to all tasks and explicit task-specific branches reserved for each task. The non-learnable primitives and the explicit decoupling of learnable parameters into shared and task-specific ones afford the flexibility needed for minimizing task interference. We evaluate the efficacy of ETR-NLP networks for both image-level classification and pixel-level dense prediction MTL problems. Experimental results indicate that ETR-NLP significantly outperforms state-of-the-art baselines with fewer learnable parameters and similar FLOPs across all datasets. Code is available at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
</details>
<details>
<summary>摘要</summary>
多任务学习（MTL）目的是学习一个模型来完成多个任务，利用任务之间的共享信息。现有的 MTL 模型却存在任务干扰的问题。减轻任务干扰的努力主要集中在损失/梯度均衡或隐式参数分割中，其中一些任务参数与其他任务参数之间存在部分重叠。在这篇论文中，我们提出了ETR-NLP，一种通过非学习性 primitives（NLPs）和显式任务路由（ETR）来减轻任务干扰的方法。我们的关键想法是使用非学习性 primitives 提取一组多样化的任务不受限制的特征，然后将其重新组合到一个共享的分支和每个任务的显式分支中。非学习性 primitives 和显式划分学习参数为共享和任务特定的一些允许我们适应性的灵活性，以最小化任务干扰。我们在图像级别的分类和像素级别的整合预测MTL问题中评估了ETR-NLP网络的效果。实验结果表明，ETR-NLP在所有数据集上都超越了当前的基eline，减少了学习参数数量和相同的FLOPs。代码可以在这里找到：https://github.com/zhichao-lu/etr-nlp-mtl。
</details></li>
</ul>
<hr>
<h2 id="On-the-Biometric-Capacity-of-Generative-Face-Models"><a href="#On-the-Biometric-Capacity-of-Generative-Face-Models" class="headerlink" title="On the Biometric Capacity of Generative Face Models"></a>On the Biometric Capacity of Generative Face Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02065">http://arxiv.org/abs/2308.02065</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/human-analysis/capacity-generative-face-models">https://github.com/human-analysis/capacity-generative-face-models</a></li>
<li>paper_authors: Vishnu Naresh Boddeti, Gautam Sreekumar, Arun Ross</li>
<li>for: 本研究的目的是为了评估和比较不同的生成人脸模型，以及确定这些模型的扩展性上的最高限制。</li>
<li>methods: 本研究使用了一种统计方法来估算生成人脸图像在幂体特征空间中的生物学容量。</li>
<li>results: 研究发现，使用 ArcFace 表示法，在 false acceptance rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的生物学容量的最高限制分别为 $1.43\times10^6$ 和 $1.190\times10^4$。此外，随着 Desired FAR 的下降，生物学容量的估算值也降低了许多。 gender 和 age 的影响也被研究发现。<details>
<summary>Abstract</summary>
There has been tremendous progress in generating realistic faces with high fidelity over the past few years. Despite this progress, a crucial question remains unanswered: "Given a generative face model, how many unique identities can it generate?" In other words, what is the biometric capacity of the generative face model? A scientific basis for answering this question will benefit evaluating and comparing different generative face models and establish an upper bound on their scalability. This paper proposes a statistical approach to estimate the biometric capacity of generated face images in a hyperspherical feature space. We employ our approach on multiple generative models, including unconditional generators like StyleGAN, Latent Diffusion Model, and "Generated Photos," as well as DCFace, a class-conditional generator. We also estimate capacity w.r.t. demographic attributes such as gender and age. Our capacity estimates indicate that (a) under ArcFace representation at a false acceptance rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of $1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces drastically as we lower the desired FAR with an estimate of $1.796\times10^4$ and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no discernible disparity in the capacity w.r.t gender; and (d) for some generative models, there is an appreciable disparity in the capacity w.r.t age. Code is available at https://github.com/human-analysis/capacity-generative-face-models.
</details>
<details>
<summary>摘要</summary>
在过去几年里，生成真实的脸部图像的进步很大。尽管如此，一个关键的问题仍然未得到答案：“给定一个生成脸部模型，它可以生成多少个唯一的标识？”或者说，生成脸部模型的生物 metric capacity 是多少？一个科学基础来回答这个问题将有助于评估和比较不同的生成脸部模型，并设置生成脸部模型的可扩展性的上限。本文提出了一种统计方法来估算生成脸部图像的生物 metric 容量，我们使用这种方法对多个生成模型进行了测试，包括 StyleGAN、Latent Diffusion Model 和 "Generated Photos" 等模型，以及 DCFace 等类别 conditional 生成模型。我们还估算了基于人口特征（如性别和年龄）的容量。我们的容量估算表明：（a）在 ArcFace 表示下，False Acceptance Rate (FAR) 为 0.1% 时，StyleGAN3 和 DCFace 的容量Upper Bound 分别为 $1.43\times10^6$ 和 $1.190\times10^4$；（b）随着 Desired FAR 降低，容量减少了极其剧烈， ArcFace 表示下，FAR 为 1% 和 10% 时，StyleGAN3 的容量估算为 $1.796\times10^4$ 和 $562$；（c）对于一些生成模型， gender 不存在显著的差异；（d）对于一些生成模型， age 存在可观的差异。相关代码可以在 GitHub 上找到：https://github.com/human-analysis/capacity-generative-face-models。
</details></li>
</ul>
<hr>
<h2 id="Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization"><a href="#Accurate-Neural-Network-Pruning-Requires-Rethinking-Sparse-Optimization" class="headerlink" title="Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"></a>Accurate Neural Network Pruning Requires Rethinking Sparse Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02060">http://arxiv.org/abs/2308.02060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, Dan Alistarh</li>
<li>for: 这个论文的目的是研究在使用标准的随机优化技术进行训练稀皮网络时，稀皮性如何影响模型训练。</li>
<li>methods: 作者使用了标准的计算机视觉和自然语言处理稀皮benchmark进行研究，并提供了新的方法来 Mitigate the issue of under-training in sparse training。</li>
<li>results: 研究发现，使用标准粗糙训练策略进行稀皮训练是不优化的，而使用新提出的方法可以在计算机视觉和自然语言处理领域中实现高精度和高稀皮性的模型训练。<details>
<summary>Abstract</summary>
Obtaining versions of deep neural networks that are both highly-accurate and highly-sparse is one of the main challenges in the area of model compression, and several high-performance pruning techniques have been investigated by the community. Yet, much less is known about the interaction between sparsity and the standard stochastic optimization techniques used for training sparse networks, and most existing work uses standard dense schedules and hyperparameters for training sparse networks. In this work, we examine the impact of high sparsity on model training using the standard computer vision and natural language processing sparsity benchmarks. We begin by showing that using standard dense training recipes for sparse training is suboptimal, and results in under-training. We provide new approaches for mitigating this issue for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and sparse fine-tuning of language models (e.g. BERT/GLUE), achieving state-of-the-art results in both settings in the high-sparsity regime, and providing detailed analyses for the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity, and should inspire further research into improving sparse model training, to reach higher accuracies under high sparsity, but also to do so efficiently.
</details>
<details>
<summary>摘要</summary>
In this work, we examine the impact of high sparsity on model training using standard computer vision and natural language processing sparsity benchmarks. We show that using standard dense training recipes for sparse training is suboptimal and results in under-training. We propose new approaches to mitigate this issue for both sparse pre-training of vision models (e.g., ResNet50/ImageNet) and sparse fine-tuning of language models (e.g., BERT/GLUE). Our approaches achieve state-of-the-art results in both settings in the high-sparsity regime and provide detailed analyses of the difficulty of sparse training in both scenarios. Our work sets a new threshold in terms of the accuracies that can be achieved under high sparsity and should inspire further research into improving sparse model training to reach higher accuracies under high sparsity efficiently.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems"><a href="#Incorporating-Recklessness-to-Collaborative-Filtering-based-Recommender-Systems" class="headerlink" title="Incorporating Recklessness to Collaborative Filtering based Recommender Systems"></a>Incorporating Recklessness to Collaborative Filtering based Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02058">http://arxiv.org/abs/2308.02058</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knodis-research-group/recklessness-regularization">https://github.com/knodis-research-group/recklessness-regularization</a></li>
<li>paper_authors: Diego Pérez-López, Fernando Ortega, Ángel González-Prieto, Jorge Dueñas-Lerín</li>
<li>for: 提高爆料系统的决策可靠性和创新性</li>
<li>methods: 引入一个新的学习过程中的recklessness项，用于控制决策时的风险水平</li>
<li>results: 实验结果表明，recklessness不仅能够进行风险规避，还可以提高爆料系统提供的预测量和质量。<details>
<summary>Abstract</summary>
Recommender systems that include some reliability measure of their predictions tend to be more conservative in forecasting, due to their constraint to preserve reliability. This leads to a significant drop in the coverage and novelty that these systems can provide. In this paper, we propose the inclusion of a new term in the learning process of matrix factorization-based recommender systems, called recklessness, which enables the control of the risk level desired when making decisions about the reliability of a prediction. Experimental results demonstrate that recklessness not only allows for risk regulation but also improves the quantity and quality of predictions provided by the recommender system.
</details>
<details>
<summary>摘要</summary>
建议系统，包括一些可靠度度量，往往会变得更加保守，因为它们需要保持可靠度。这会导致建议系统的覆盖率和新颖性下降。在这篇论文中，我们提议在矩阵分解基础的建议系统学习过程中添加一个新的参数，即不可靠度，以控制决策时的风险水平。实验结果表明，不可靠度不仅允许风险调节，还可以提高建议系统提供的预测量和质量。Note: "recklessness" is a term used in the original text, and it is not a word commonly used in Chinese. I translated it as "不可靠度" (bù kě yào dù), which means "unreliability" or "riskiness".
</details></li>
</ul>
<hr>
<h2 id="Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries"><a href="#Seasonality-Based-Reranking-of-E-commerce-Autocomplete-Using-Natural-Language-Queries" class="headerlink" title="Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries"></a>Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02055">http://arxiv.org/abs/2308.02055</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prateek Verma, Shan Zhong, Xiaoyu Liu, Adithya Rajan</li>
<li>for: 提高搜索引擎的搜寻框架中的自动完成功能，使其能够适应季节性变化。</li>
<li>methods: 使用神经网络基本概念的自然语言处理算法，将季节性变化纳入搜寻框架中。</li>
<li>results: 提出一个终端评估模型，可以将季节性变化纳入搜寻框架中，提高自动完成的相关性和商业指标。<details>
<summary>Abstract</summary>
Query autocomplete (QAC) also known as typeahead, suggests list of complete queries as user types prefix in the search box. It is one of the key features of modern search engines specially in e-commerce. One of the goals of typeahead is to suggest relevant queries to users which are seasonally important. In this paper we propose a neural network based natural language processing (NLP) algorithm to incorporate seasonality as a signal and present end to end evaluation of the QAC ranking model. Incorporating seasonality into autocomplete ranking model can improve autocomplete relevance and business metric.
</details>
<details>
<summary>摘要</summary>
查询自动完成（QAC）也称为键盘提示，是现代搜索引擎中的一个重要功能，尤其在电商领域。QAC的一个目标是为用户提供相关的查询，以便在搜索框中输入搜索。在这篇论文中，我们提出了基于人工神经网络的自然语言处理（NLP）算法，以 incorporate 季节性作为信号，并进行了端到端评估QAC排名模型。在推入季节性到搜索框中的排名模型中，可以提高搜索结果的相关性和业务指标。
</details></li>
</ul>
<hr>
<h2 id="Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems"><a href="#Robust-Independence-Tests-with-Finite-Sample-Guarantees-for-Synchronous-Stochastic-Linear-Systems" class="headerlink" title="Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems"></a>Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02054">http://arxiv.org/abs/2308.02054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambrus Tamás, Dániel Ágoston Bálint, Balázs Csanád Csáji</li>
<li>for: 这个论文是为了开发一种robust independence测试方法，可以 guaranteesignificance levels不受偏移的影响。</li>
<li>methods: 这个方法使用了信任区间估计和 permutation tests，以及一些总体依赖度测量方法，如希尔伯特-Ш密特独立性标准和距离协方差。</li>
<li>results: 这个方法可以检测非线性依赖关系，并且可以在各种不同的噪声模型下进行测试。我们还证明了这个假设测试方法的一致性下一些轻微的假设。<details>
<summary>Abstract</summary>
The paper introduces robust independence tests with non-asymptotically guaranteed significance levels for stochastic linear time-invariant systems, assuming that the observed outputs are synchronous, which means that the systems are driven by jointly i.i.d. noises. Our method provides bounds for the type I error probabilities that are distribution-free, i.e., the innovations can have arbitrary distributions. The algorithm combines confidence region estimates with permutation tests and general dependence measures, such as the Hilbert-Schmidt independence criterion and the distance covariance, to detect any nonlinear dependence between the observed systems. We also prove the consistency of our hypothesis tests under mild assumptions and demonstrate the ideas through the example of autoregressive systems.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种robust独立性测试方法，可以 garantuee非对称性水平，对于随机线性时间不变系统。我们假设观测输出是同步的，即系统被共同的随机噪声驱动。我们的方法提供了不对归一化的类型I错误概率 bound，即噪声可以有任何分布。我们的算法结合信任区间估计与排序测试，以及通用的依赖度度量，如希尔伯特-尚瑟独立性 критерион和距离协方差，来检测观测系统中的非线性依赖关系。我们还证明了我们的假设检测下的假设是正确的，并通过拓扑系统的示例进行了证明。
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Approach-to-Document-Layout-Analysis"><a href="#A-Graphical-Approach-to-Document-Layout-Analysis" class="headerlink" title="A Graphical Approach to Document Layout Analysis"></a>A Graphical Approach to Document Layout Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02051">http://arxiv.org/abs/2308.02051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jilin Wang, Michael Krumdick, Baojia Tong, Hamima Halim, Maxim Sokolov, Vadym Barda, Delphine Vendryes, Chris Tanner</li>
<li>for: This paper focuses on document layout analysis (DLA) and proposes a lightweight graph neural network called GLAM to improve the efficiency of DLA models.</li>
<li>methods: The GLAM model represents each PDF page as a structured graph and frames the DLA problem as a graph segmentation and classification problem.</li>
<li>results: The GLAM model achieves competitive performance with state-of-the-art (SOTA) models on two challenging DLA datasets, with an order of magnitude fewer parameters. A simple ensemble of GLAM and a leading computer vision-based model achieves a new state-of-the-art on DocLayNet, with an increase in mean average precision (mAP) from 76.8 to 80.8.<details>
<summary>Abstract</summary>
Document layout analysis (DLA) is the task of detecting the distinct, semantic content within a document and correctly classifying these items into an appropriate category (e.g., text, title, figure). DLA pipelines enable users to convert documents into structured machine-readable formats that can then be used for many useful downstream tasks. Most existing state-of-the-art (SOTA) DLA models represent documents as images, discarding the rich metadata available in electronically generated PDFs. Directly leveraging this metadata, we represent each PDF page as a structured graph and frame the DLA problem as a graph segmentation and classification problem. We introduce the Graph-based Layout Analysis Model (GLAM), a lightweight graph neural network competitive with SOTA models on two challenging DLA datasets - while being an order of magnitude smaller than existing models. In particular, the 4-million parameter GLAM model outperforms the leading 140M+ parameter computer vision-based model on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8 to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making GLAM a favorable engineering choice for DLA tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents"><a href="#SMARLA-A-Safety-Monitoring-Approach-for-Deep-Reinforcement-Learning-Agents" class="headerlink" title="SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents"></a>SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02594">http://arxiv.org/abs/2308.02594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, Ramesh S</li>
<li>for: 这篇论文旨在提出一种基于机器学习的安全监测方法，用于保障深度优化学习（DRL）Agent的安全性。</li>
<li>methods: 该方法基于黑盒（不需要访问代理的内部），利用状态抽象减少状态空间，从而使得学习安全违反预测模型的可能性更高。</li>
<li>results: 验证结果表明，SMARLA可以准确预测安全违反，false positive率低，可以在代理执行前半部分预测安全违反。<details>
<summary>Abstract</summary>
Deep reinforcement learning algorithms (DRL) are increasingly being used in safety-critical systems. Ensuring the safety of DRL agents is a critical concern in such contexts. However, relying solely on testing is not sufficient to ensure safety as it does not offer guarantees. Building safety monitors is one solution to alleviate this challenge. This paper proposes SMARLA, a machine learning-based safety monitoring approach designed for DRL agents. For practical reasons, SMARLA is designed to be black-box (as it does not require access to the internals of the agent) and leverages state abstraction to reduce the state space and thus facilitate the learning of safety violation prediction models from agent's states. We validated SMARLA on two well-known RL case studies. Empirical analysis reveals that SMARLA achieves accurate violation prediction with a low false positive rate, and can predict safety violations at an early stage, approximately halfway through the agent's execution before violations occur.
</details>
<details>
<summary>摘要</summary>
深度强化学习算法（DRL）在安全关键系统中日益被使用。保证DRL代理的安全是这些上下文中的关键问题。然而，仅仅通过测试不能保证安全，因为它不提供保证。建立安全监控器是一个解决方案，以降低这个挑战。这篇论文提出了基于机器学习的安全监控方法SMARLA，专门为DRL代理设计。由于实际原因，SMARLA采用黑盒设计（不需要代理的内部访问权限），并利用状态抽象来减少状态空间，从而使得学习代理违规预测模型从代理的状态中更加容易。我们对两个常见RL案例进行了验证。实验分析表明，SMARLA可以准确预测违规行为，false positive率较低，能够在代理执行前一半预测违规行为。
</details></li>
</ul>
<hr>
<h2 id="FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method"><a href="#FuNToM-Functional-Modeling-of-RF-Circuits-Using-a-Neural-Network-Assisted-Two-Port-Analysis-Method" class="headerlink" title="FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method"></a>FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02050">http://arxiv.org/abs/2308.02050</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Fayazi, Morteza Tavakoli Taba, Amirata Tabatabavakili, Ehsan Afshari, Ronald Dreslinski<br>for:这个论文主要目的是提出一种功能模型化方法，以提高RLC电路的自动化设计效率。methods:该方法使用人工智能技术，并利用两个Port分析方法，可以模型多种架构，并且仅需要一个主要数据集和多个小数据集。results:该方法可以与现有方法匹配精度，但需要训练数据的数量则被降低了2.8倍至10.9倍，并且在后期设计阶段的训练集收集时间则被降低了176.8倍至188.6倍。<details>
<summary>Abstract</summary>
Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending approach that requires an efficient circuit modeling method. This is due to the expensive cost of running a large number of simulations at each synthesis cycle. Artificial intelligence methods are promising approaches for circuit modeling due to their speed and relative accuracy. However, existing approaches require a large amount of training data, which is still collected using simulation runs. In addition, such approaches collect a whole separate dataset for each circuit topology even if a single element is added or removed. These matters are only exacerbated by the need for post-layout modeling simulations, which take even longer. To alleviate these drawbacks, in this paper, we present FuNToM, a functional modeling method for RF circuits. FuNToM leverages the two-port analysis method for modeling multiple topologies using a single main dataset and multiple small datasets. It also leverages neural networks which have shown promising results in predicting the behavior of circuits. Our results show that for multiple RF circuits, in comparison to the state-of-the-art works, while maintaining the same accuracy, the required training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x - 188.6x less time for collecting the training set in post-layout modeling.
</details>
<details>
<summary>摘要</summary>
《自动化分析和设计 analog和 radio frequency（RF）电路的方法是一种流行的趋势，因为在每一个合理化周期中运行大量的 simulate 实际上是昂贵的。人工智能方法是电路模型的承诺之一，因为它们具有速度和相对准确性。然而，现有的方法需要大量的训练数据，这些数据通常通过 simulate 实际来采集。此外，这些方法每个电路结构都需要采集一个分开的数据集，即使只是添加或删除一个元素。这些问题由 Layout 模拟所加剧，它们需要更长的时间。为了解决这些问题，我们在这篇论文中提出了 FuNToM，一种功能模型方法 для RF 电路。FuNToM 利用了两个端口分析方法，可以模型多种 topology 使用单个主数据集和多个小数据集。它还利用了人工神经网络，这些神经网络在预测电路行为方面表现出色。我们的结果表明，对多个 RF 电路，相比之前的状态艺术作品，在保持同样的准确性下，需要的训练数据被减少了 2.8x - 10.9x。此外，FuNToM 在 post-layout 模拟中收集训练集的时间需要 176.8x - 188.6x  menos。
</details></li>
</ul>
<hr>
<h2 id="Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection"><a href="#Deep-Maxout-Network-based-Feature-Fusion-and-Political-Tangent-Search-Optimizer-enabled-Transfer-Learning-for-Thalassemia-Detection" class="headerlink" title="Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection"></a>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02029">http://arxiv.org/abs/2308.02029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hemn Barzan Abdalla, Awder Ahmed, Guoquan Li, Nasser Mustafa, Abdur Rashid Sangi</li>
<li>for: 本研究旨在探讨一种基于政治向量搜索优化的深度学习方法（PTSO_TL）用于抑制遗传贫血病诊断。</li>
<li>methods: 本研究使用的方法包括数据normalization、特征融合、数据增强和深度学习模型。</li>
<li>results: 根据实验结果，PTSO_TL方法在识别遗传贫血病方面达到了最高的精度（94.3%）、回归率（96.1%）和相关度（95.2%）。<details>
<summary>Abstract</summary>
Thalassemia is a heritable blood disorder which is the outcome of a genetic defect causing lack of production of hemoglobin polypeptide chains. However, there is less understanding of the precise frequency as well as sharing in these areas. Knowing about the frequency of thalassemia occurrence and dependable mutations is thus a significant step in preventing, controlling, and treatment planning. Here, Political Tangent Search Optimizer based Transfer Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input data obtained from a particular dataset is normalized in the data normalization stage. Quantile normalization is utilized in the data normalization stage, and the data are then passed to the feature fusion phase, in which Weighted Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data augmentation is performed using the oversampling method to increase data dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a convolutional neural network (CNN) is utilized with hyperparameters from a trained model such as Xception. TL is tuned by PTSO, and the training algorithm PTSO is presented by merging of Political Optimizer (PO) and Tangent Search Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
</details>
<details>
<summary>摘要</summary>
贝壳血症是一种遗传血液疾病，由于遗传错误导致血液中不够生成含铁蛋白链。然而，贝壳血症的具体发生频率以及传递的精准性仍未得到充分理解。了解贝壳血症发生频率和可靠的突变是一项重要的步骤，以便预防、控制和治疗规划。在这里，我们引入政治弧搜索优化器基于传输学习（PTSO_TL）以检测贝壳血症。首先，输入数据从特定数据集被normalized，并使用量谱normalization进行数据归一化。然后，数据被传递到特征融合阶段，在这里使用Weighted Euclidean Distance with Deep Maxout Network（DMN）。接着，数据进行了增强处理，使用扩充方法增加数据维度。最后，贝壳血症检测由TL进行，其中使用一个具有训练模型的 convolutional neural network（CNN），并将 hyperparameters 从已训练模型 such as Xception。TL 被PTSO 调整，并且PTSO 是由政治优化器（PO）和 Tangent Search Algorithm（TSA）的 merge 所presentation。此外，PTSO_TL 在评价指标中获得了最高的准确率、回归率和准确度值，它们分别为 approximately 94.3%, 96.1%, and 95.2%。
</details></li>
</ul>
<hr>
<h2 id="Federated-Representation-Learning-for-Automatic-Speech-Recognition"><a href="#Federated-Representation-Learning-for-Automatic-Speech-Recognition" class="headerlink" title="Federated Representation Learning for Automatic Speech Recognition"></a>Federated Representation Learning for Automatic Speech Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02013">http://arxiv.org/abs/2308.02013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guruprasad V Ramesh, Gopinath Chennupati, Milind Rao, Anit Kumar Sahu, Ariya Rastrow, Jasha Droppo</li>
<li>for: 这篇论文是为了探讨 Federated Learning (FL) 和 Self-supervised Learning (SSL) 的结合，以学习 Automatic Speech Recognition (ASR) 模型，保持数据隐私。</li>
<li>methods: 这篇论文使用了 Libri-Light 语音 dataset，使用了 Speaker 和 Chapter 信息来模拟非Identical Independent Distributions (non-IID) 的数据分布，采用了 Contrastive Predictive Coding 框架和 FedSGD 进行训练。</li>
<li>results: 研究发现，使用 Federated Learning 预训练 ASR 模型，可以达到中心预训练模型的性能水平，并且在新语言 French 中进行适应，可以提高 WER 表达误差率 by 20%。<details>
<summary>Abstract</summary>
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 是一种隐私保护的方法论，允许边缘设备共同学习无需分享数据。边缘设备如 Alexa 和 Siri 是可能的无标语音数据的来源，可以用于学习 Robust 语音表示。在这项工作中，我们将 Self-supervised Learning (SSL) 和 FL 结合来学习 Automatic Speech Recognition (ASR) 的表示，尊重数据隐私约束。我们使用 Libri-Light 无标语音集中的 Speaker 和章节信息来模拟非Identical Independent Distribution (IID) 的Speaker-siloed 数据分布，并在 FedSGD 框架下预训练一个 LSTM 编码器。我们显示预训练的 ASR 编码器在 FL 中表现与中央预训练模型一样好，并且对无预训练情况下提高了12-15% (WER)。我们进一步适应了联邦预训练模型到一种新语言法语，并显示对无预训练情况下提高了20% (WER)。
</details></li>
</ul>
<hr>
<h2 id="Memory-capacity-of-two-layer-neural-networks-with-smooth-activations"><a href="#Memory-capacity-of-two-layer-neural-networks-with-smooth-activations" class="headerlink" title="Memory capacity of two layer neural networks with smooth activations"></a>Memory capacity of two layer neural networks with smooth activations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02001">http://arxiv.org/abs/2308.02001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liam Madden, Christos Thrampoulidis</li>
<li>for: 这篇论文探讨了两层神经网络的内存容量，即最大化一般数据集的网络大小。</li>
<li>methods: 作者使用了非多项式实数Activation函数，如sigmoid和smoothed ReLU，并使用Jacobian的秩来分析网络的内存容量。</li>
<li>results: 作者发现，对于非多项式实数Activation函数，网络的内存容量至少为md&#x2F;2，并且可以达到约2倍的优化。这些结果比前一些研究更加广泛，并且可以推广到更深的模型和其他架构。<details>
<summary>Abstract</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters), which refers to the largest size of general data the network can memorize, is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.
</details>
<details>
<summary>摘要</summary>
Determining the memory capacity of two-layer neural networks with m hidden neurons and input dimension d (i.e., md+m total trainable parameters) is a fundamental machine-learning question. For non-polynomial real analytic activation functions, such as sigmoids and smoothed rectified linear units (smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a factor of approximately 2. Analogous prior results were limited to Heaviside and ReLU activations, with results for smooth activations suffering from logarithmic factors and requiring random data. To analyze the memory capacity, we examine the rank of the network's Jacobian by computing the rank of matrices involving both Hadamard powers and the Khati-Rao product. Our computation extends classical linear algebraic facts about the rank of Hadamard powers. Overall, our approach differs from previous works on memory capacity and holds promise for extending to deeper models and other architectures.Here's the translation in Traditional Chinese:决定两层神经网络中隐藏层 neuron 数目为 m，输入维度为 d（即 md+m 总可训练参数）的记忆容量是机器学习中的基本问题。对于非多项演算 activation functions，例如 sigmoid 和 smoothed rectified linear units (smoothed ReLUs)，我们设置了 md/2 的下界和约2的优化因子。这些结果与 preceded 的 results 相似，但是过去的结果仅适用于 Heaviside 和 ReLU 激活函数，而且这些激活函数的结果受到了 logarithmic 因子的影响，并且需要随机数据。从构成记忆容量的角度来看，我们查看了神经网络的雅可比安的排名，通过计算包含 Hadamard powers 和 Khati-Rao 产品的矩阵的排名。我们的计算扩展了 класиical 的线性代数实验，关于 Hadamard powers 的排名。整体而言，我们的方法与之前的工作不同，并且保持可以扩展到更深的模型和其他架构。
</details></li>
</ul>
<hr>
<h2 id="On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge"><a href="#On-the-Transition-from-Neural-Representation-to-Symbolic-Knowledge" class="headerlink" title="On the Transition from Neural Representation to Symbolic Knowledge"></a>On the Transition from Neural Representation to Symbolic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02000">http://arxiv.org/abs/2308.02000</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyan Cheng, Peter Chin</li>
<li>for: 本研究旨在bridge neural和Symbolic Representation之间的巨大差距，以便将Symbolic Thinking incorporated into neural networks的核心。</li>
<li>methods: 我们提出了一个Neural-Symbolic Transitional Dictionary Learning（TDL）框架，使用EM算法学习数据的转换表示，压缩输入数据的高维信息到一组tensor作为神经变量，自然地发现数据中隐藏的 predicate 结构。我们在 diffusion model 中对输入的分解视为合作游戏，然后通过prototype clustering来学习预测。此外，我们还使用RLEnabled by diffusion models来进一步调整学习的got prototype。</li>
<li>results: 我们在3个抽象compositional visual objects dataset上进行了广泛的实验，这些dataset需要模型可以对输入进行部分 segmentation，不含任何视觉特征，例如 texture、颜色或阴影。我们的learned representation可以带来可 interpret的 decompositions of visual input，并且在下游任务中进行了smooth的适应。这些下游任务包括神经&#x2F;Symbolic downstream tasks。<details>
<summary>Abstract</summary>
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details>
<details>
<summary>摘要</summary>
bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.Here's a word-for-word translation of the text into Simplified Chinese:bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. we propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. we implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. we additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. extensive experiments on 3 abstract compositional visual objects datasets that require the model to segment parts without any visual features like texture, color, or shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the learned representation enables interpretable decomposition of visual input and smooth adaption to downstream tasks which are not available by existing methods.
</details></li>
</ul>
<hr>
<h2 id="Explainable-unsupervised-multi-modal-image-registration-using-deep-networks"><a href="#Explainable-unsupervised-multi-modal-image-registration-using-deep-networks" class="headerlink" title="Explainable unsupervised multi-modal image registration using deep networks"></a>Explainable unsupervised multi-modal image registration using deep networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01994">http://arxiv.org/abs/2308.01994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengjia Wang, Giorgos Papanastasiou</li>
<li>for: 这个论文是用于描述一种基于深度学习的多模态MRI图像匹配方法，用于临床决策。</li>
<li>methods: 该方法使用了多种MRI序列（定义为’模态’），并使用了 Grad-CAM基于解释框架来解释模型和数据之间的关系。</li>
<li>results: 该研究表明，通过 incorporating Grad-CAM解释框架，该方法可以实现高性能和可解释的多模态MRI图像匹配。<details>
<summary>Abstract</summary>
Clinical decision making from magnetic resonance imaging (MRI) combines complementary information from multiple MRI sequences (defined as 'modalities'). MRI image registration aims to geometrically 'pair' diagnoses from different modalities, time points and slices. Both intra- and inter-modality MRI registration are essential components in clinical MRI settings. Further, an MRI image processing pipeline that can address both afine and non-rigid registration is critical, as both types of deformations may be occuring in real MRI data scenarios. Unlike image classification, explainability is not commonly addressed in image registration deep learning (DL) methods, as it is challenging to interpet model-data behaviours against transformation fields. To properly address this, we incorporate Grad-CAM-based explainability frameworks in each major component of our unsupervised multi-modal and multi-organ image registration DL methodology. We previously demonstrated that we were able to reach superior performance (against the current standard Syn method). In this work, we show that our DL model becomes fully explainable, setting the framework to generalise our approach on further medical imaging data.
</details>
<details>
<summary>摘要</summary>
临床决策从核磁共振成像（MRI）结合多种MRI序列（定义为“模态”）的信息。MRI图像匹配目标是在不同模态、时间点和切片之间进行几何匹配诊断。Intra-和inter-模态MRI匹配都是临床MRI设置中的重要组件。此外，一个能够处理both afine和non-rigid匹配的MRI图像处理管道是关键，因为这两种类型的变形都可能发生在实际MRI数据场景中。不同于图像分类，explainability不是通常在图像匹配深度学习（DL）方法中被考虑的，因为它是困难 interpret模型-数据行为对于转换场景。为了正确地Address这个问题，我们在每个主要组件中都 incorporate Grad-CAM基于的解释框架。在我们之前的研究中，我们已经能够达到superior performance（相比于当前标准Syn方法）。在这项工作中，我们显示了我们的DL模型已经变得完全可解释，设置了框架可以通过更多的医疗影像数据进行普适化。
</details></li>
</ul>
<hr>
<h2 id="CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics"><a href="#CartiMorph-a-framework-for-automated-knee-articular-cartilage-morphometrics" class="headerlink" title="CartiMorph: a framework for automated knee articular cartilage morphometrics"></a>CartiMorph: a framework for automated knee articular cartilage morphometrics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01981">http://arxiv.org/abs/2308.01981</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongchengyao/cartimorph">https://github.com/yongchengyao/cartimorph</a></li>
<li>paper_authors: Yongcheng Yao, Junru Zhong, Liping Zhang, Sheheryar Khan, Weitian Chen</li>
<li>for: 这个研究的目的是发展一个准确地量化膝盖韧带组织的自动化方法，以便发现膝盖韧带组织的问题。</li>
<li>methods: 这个研究使用了深度学习模型来表现图像特征，并使用了标本建立和图像注册等方法来自动化膝盖韧带组织的量化。</li>
<li>results: 这个研究获得了膝盖韧带组织的量化结果，包括全厚度膝盖韧带损伤率（FCL）、平均厚度、表面积和体积等多个量化指标。这些量化结果显示了膝盖韧带组织的问题，并且与手动量化结果之间存在强相关。<details>
<summary>Abstract</summary>
We introduce CartiMorph, a framework for automated knee articular cartilage morphometrics. It takes an image as input and generates quantitative metrics for cartilage subregions, including the percentage of full-thickness cartilage loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the power of deep learning models for hierarchical image feature representation. Deep learning models were trained and validated for tissue segmentation, template construction, and template-to-image registration. We established methods for surface-normal-based cartilage thickness mapping, FCL estimation, and rule-based cartilage parcellation. Our cartilage thickness map showed less error in thin and peripheral regions. We evaluated the effectiveness of the adopted segmentation model by comparing the quantitative metrics obtained from model segmentation and those from manual segmentation. The root-mean-squared deviation of the FCL measurements was less than 8%, and strong correlations were observed for the mean thickness (Pearson's correlation coefficient $\rho \in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in [0.89,0.98]$) measurements. We compared our FCL measurements with those from a previous study and found that our measurements deviated less from the ground truths. We observed superior performance of the proposed rule-based cartilage parcellation method compared with the atlas-based approach. CartiMorph has the potential to promote imaging biomarkers discovery for knee osteoarthritis.
</details>
<details>
<summary>摘要</summary>
我们介绍CartiMorph，一个框架用于自动诊断膝关节软骨质量量表。它可以从图像中提取量表膝关节软骨质量量表，包括软骨质量量表的全厚度损伤率（FCL）、平均厚度、表面积和体积。CartiMorph利用深度学习模型来实现层次图像特征表示。我们在识别、构建模板和模板与图像匹配中使用深度学习模型。我们实现了基于表面法向的软骨厚度映射、FCL估计和规则基于的软骨分割。我们的软骨厚度图表示在薄和边缘区域中具有较低的错误。我们通过比较我们采用的分 segmentation模型与手动分 segmentation结果所得到的量表metric来评估模型的效果。我们发现root-mean-squared deviation of FCL measurements是less than 8%，并且在mean thickness、surface area和volume measurement中observation了强相关性（Pearson's correlation coefficient $\rho \in [0.82,0.97]$、[0.82,0.98]$和[0.89,0.98]$）。我们对我们的FCL测量与之前的研究中的参照值进行比较，发现我们的测量偏差较少。我们发现了规则基于的软骨分割方法的优越性，比Atlas-based方法更好。CartiMorph具有推动膝关节风湿病影像生物标志物的潜力。
</details></li>
</ul>
<hr>
<h2 id="Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework"><a href="#Unmasking-Parkinson’s-Disease-with-Smile-An-AI-enabled-Screening-Framework" class="headerlink" title="Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework"></a>Unmasking Parkinson’s Disease with Smile: An AI-enabled Screening Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02588">http://arxiv.org/abs/2308.02588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Adnan, Md Saiful Islam, Wasifur Rahman, Sangwu Lee, Sutapa Dey Tithi, Kazi Noshin, Imran Sarker, M Saifur Rahman, Ehsan Hoque</li>
<li>for: 预测帕金森病（PD）的诊断具有挑战性，因为没有可靠的生物标志物和有限的临床护理资源。本研究通过分析最大的视频数据集，检测PD的微表情。</li>
<li>methods: 我们使用了人脸特征点和动作单元，提取与低表情相关的特征。我们将这些特征用于一个 ensemble 模型，实现了89.7%的准确率和89.3%的接受分布函数点（AUROC）。</li>
<li>results: 我们发现，只使用笑脸视频中的特征，可以达到相似的性能，甚至在两个外部测试集上，模型没有在训练过程中看到的数据上进行了分类，这表明了PD风险评估可能通过笑脸自拍视频进行。<details>
<summary>Abstract</summary>
Parkinson's disease (PD) diagnosis remains challenging due to lacking a reliable biomarker and limited access to clinical care. In this study, we present an analysis of the largest video dataset containing micro-expressions to screen for PD. We collected 3,871 videos from 1,059 unique participants, including 256 self-reported PD patients. The recordings are from diverse sources encompassing participants' homes across multiple countries, a clinic, and a PD care facility in the US. Leveraging facial landmarks and action units, we extracted features relevant to Hypomimia, a prominent symptom of PD characterized by reduced facial expressions. An ensemble of AI models trained on these features achieved an accuracy of 89.7% and an Area Under the Receiver Operating Characteristic (AUROC) of 89.3% while being free from detectable bias across population subgroups based on sex and ethnicity on held-out data. Further analysis reveals that features from the smiling videos alone lead to comparable performance, even on two external test sets the model has never seen during training, suggesting the potential for PD risk assessment from smiling selfie videos.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces"><a href="#Domain-specificity-and-data-efficiency-in-typo-tolerant-spell-checkers-the-case-of-search-in-online-marketplaces" class="headerlink" title="Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces"></a>Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01976">http://arxiv.org/abs/2308.01976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dayananda Ubrangala, Juhi Sharma, Ravi Prasad Kondapalli, Kiran R, Amit Agarwala, Laurent Boué</li>
<li>for: 提高在线市场场所上的拼写错误检测精度</li>
<li>methods: 使用数据扩充方法生成域限定特定的隐藏表示，并使用回归神经网络进行训练</li>
<li>results: 实现了在实时推荐API中的 typo 检测，提高了搜索效果<details>
<summary>Abstract</summary>
Typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.
</details>
<details>
<summary>摘要</summary>
typographical errors are a major source of frustration for visitors of online marketplaces. Because of the domain-specific nature of these marketplaces and the very short queries users tend to search for, traditional spell cheking solutions do not perform well in correcting typos. We present a data augmentation method to address the lack of annotated typo data and train a recurrent neural network to learn context-limited domain-specific embeddings. Those embeddings are deployed in a real-time inferencing API for the Microsoft AppSource marketplace to find the closest match between a misspelled user query and the available product names. Our data efficient solution shows that controlled high quality synthetic data may be a powerful tool, especially considering the current climate of large language models which rely on prohibitively huge and often uncontrolled datasets.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models"><a href="#Synthesising-Rare-Cataract-Surgery-Samples-with-Guided-Diffusion-Models" class="headerlink" title="Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models"></a>Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02587">http://arxiv.org/abs/2308.02587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/meclabtuda/catasynth">https://github.com/meclabtuda/catasynth</a></li>
<li>paper_authors: Yannik Frisch, Moritz Fuchs, Antoine Sanner, Felix Anton Ucar, Marius Frenzel, Joana Wasielica-Poslednik, Adrian Gericke, Felix Mathias Wagner, Thomas Dratsch, Anirban Mukhopadhyay</li>
<li>for: 提高Automated Cataract Surgery Assistance System的发展，提供可靠的人工合成数据。</li>
<li>methods: 使用Denosing Diffusion Implicit Models（DDIM）和Classifier-Free Guidance（CFG）Conditional Generative Model Synthesize complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools.</li>
<li>results: 通过生成不同、高质量的示例，提高downstream工具分类器的性能，最高提高10%。<details>
<summary>Abstract</summary>
Cataract surgery is a frequently performed procedure that demands automation and advanced assistance systems. However, gathering and annotating data for training such systems is resource intensive. The publicly available data also comprises severe imbalances inherent to the surgical process. Motivated by this, we analyse cataract surgery video data for the worst-performing phases of a pre-trained downstream tool classifier. The analysis demonstrates that imbalances deteriorate the classifier's performance on underrepresented cases. To address this challenge, we utilise a conditional generative model based on Denoising Diffusion Implicit Models (DDIM) and Classifier-Free Guidance (CFG). Our model can synthesise diverse, high-quality examples based on complex multi-class multi-label conditions, such as surgical phases and combinations of surgical tools. We affirm that the synthesised samples display tools that the classifier recognises. These samples are hard to differentiate from real images, even for clinical experts with more than five years of experience. Further, our synthetically extended data can improve the data sparsity problem for the downstream task of tool classification. The evaluations demonstrate that the model can generate valuable unseen examples, allowing the tool classifier to improve by up to 10% for rare cases. Overall, our approach can facilitate the development of automated assistance systems for cataract surgery by providing a reliable source of realistic synthetic data, which we make available for everyone.
</details>
<details>
<summary>摘要</summary>
喉痒手术是一种常见的手术过程，需要自动化和高级帮助系统。然而，收集和标注数据 для训练这些系统是资源占用的。公共可用数据也包含了手术过程中的严重偏见。为了解决这个挑战，我们分析了喉痒手术视频数据，找到最差表现的阶段。分析结果表明，偏见会使下游工具分类器的表现在不足表现的案例下下降。为了解决这个问题，我们使用基于减噪扩散模型（DDIM）和无类标注指南（CFG）的 conditional generative model。我们的模型可以生成多样化、高质量的示例，基于复杂的多类多标签条件，如手术阶段和手术工具的组合。我们证明了生成的样本中的工具，可以由分类器识别。这些样本与真实图像很难分辨，甚至对有 более чем五年的临床经验的专业人员来说。此外，我们通过增加的数据可以改善下游任务中的数据稀缺问题。评估结果表明，我们的模型可以生成有价值的未看到的示例，使工具分类器提高至10%。总的来说，我们的方法可以促进喉痒手术自动化的发展，提供一个可靠的真实Synthetic数据源，我们将其公开给 everyone。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL"><a href="#Aligning-Agent-Policy-with-Externalities-Reward-Design-via-Bilevel-RL" class="headerlink" title="Aligning Agent Policy with Externalities: Reward Design via Bilevel RL"></a>Aligning Agent Policy with Externalities: Reward Design via Bilevel RL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02585">http://arxiv.org/abs/2308.02585</a></li>
<li>repo_url: None</li>
<li>paper_authors: Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Furong Huang, Mengdi Wang</li>
<li>for: 本研究旨在批处RL政策优化问题中的奖励函数假设，以及RL政策优化过程中的状态空间覆盖和安全性考虑。</li>
<li>methods: 本研究提出了一种级联优化问题，将主体（principal）定义为系统的更广泛目标和约束，而代理（agent）则解决Markov决策过程（MDP）。</li>
<li>results: 研究提出了主体驱动政策对应性via级联RL（PPA-BRL），该方法可有效地将代理的政策与主体的目标相吻合。研究还证明了PPA-BRL的收敛性，并通过多个示例验证了该方法的优点，包括能效地实现能源充足的操作任务、社会福利基础的税制设计以及成本效益的机器人导航。<details>
<summary>Abstract</summary>
In reinforcement learning (RL), a reward function is often assumed at the outset of a policy optimization procedure. Learning in such a fixed reward paradigm in RL can neglect important policy optimization considerations, such as state space coverage and safety. Moreover, it can fail to encompass broader impacts in terms of social welfare, sustainability, or market stability, potentially leading to undesirable emergent behavior and potentially misaligned policy. To mathematically encapsulate the problem of aligning RL policy optimization with such externalities, we consider a bilevel optimization problem and connect it to a principal-agent framework, where the principal specifies the broader goals and constraints of the system at the upper level and the agent solves a Markov Decision Process (MDP) at the lower level. The upper-level deals with learning a suitable reward parametrization corresponding to the broader goals and the lower-level deals with learning the policy for the agent. We propose Principal driven Policy Alignment via Bilevel RL (PPA-BRL), which efficiently aligns the policy of the agent with the principal's goals. We explicitly analyzed the dependence of the principal's trajectory on the lower-level policy, prove the convergence of PPA-BRL to the stationary point of the problem. We illuminate the merits of this framework in view of alignment with several examples spanning energy-efficient manipulation tasks, social welfare-based tax design, and cost-effective robotic navigation.
</details>
<details>
<summary>摘要</summary>
在增强学习（RL）中，常常假设一个奖金函数，用于policy优化过程的开始。这种固定奖金的假设可能忽略了重要的策略优化考虑因素，如状态空间覆盖率和安全性。此外，它可能无法涵盖更广泛的影响，如社会福利、可持续发展和市场稳定性，可能导致不жела的潜在行为和不一致策略。为了数学地表述RL策略优化与外部影响的问题，我们考虑了一个双层优化问题，并将其连接到一个主体-代理模型，其中主体规定系统的更广泛目标和约束，而代理在下层解决一个Markov决策过程（MDP）。上层学习一个适当的奖金参数化，与下层学习代理的策略。我们提出了主体驱动策略对齐（PPA-BRL），它高效地将代理的策略与主体的目标相对应。我们证明了PPA-BRL在站点点问题中的收敛性。我们通过一些示例，如能效的机器人 Navigation，社会福利基于税制的设计，以及成本效果的机器人 Navigation， illustrate the advantages of this framework。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems"><a href="#Reasoning-in-Large-Language-Models-Through-Symbolic-Math-Word-Problems" class="headerlink" title="Reasoning in Large Language Models Through Symbolic Math Word Problems"></a>Reasoning in Large Language Models Through Symbolic Math Word Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01906">http://arxiv.org/abs/2308.01906</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vedant Gaur, Nikunj Saunshi</li>
<li>for: 这篇论文探讨了自然语言处理（NLP）领域中大语言模型（LLM）的理解能力。</li>
<li>methods: 该论文使用了符号版本的数学Word问题（MWP）来研究LLM的理解能力，并创建了一个符号版本的SVAMP数据集。</li>
<li>results: 研究发现，使用自我提示approach可以使LLM的符号理解更加准确，并且自动提取出符号答案和数学答案之间的对应关系，从而使LLM的理解更加明确。此外，自我提示还能够提高符号准确率，超过 numeric 和 symbolic 准确率，从而实现了一种ensemble效果。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a "concise explanation" of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化NLG，解决了少量或无标签数据下的下游任务。 despite their 多元能力，大型问题的理解仍然不够了解。 本文研究 math word problems（MWPs）的推理，通过研究 symbolic versions of the numeric problems，因为一个 symbolic expression 是一个 "简洁解释" 的 numeric answer。 我们创建了一个 symbolic version of the SVAMP dataset，并发现 GPT-3 的 davinci-002 模型在 symbolic MWPs 上也有良好的 zero-shot accuracy。 为了评估模型的 faithfulness，我们不仅评估了模型的准确性，还进一步评估了模型输出的推理与答案的对齐度，这与 numeric 和 symbolic 答案对应。 我们还探索了自我提示的方法，以便将 symbolic reasoning 与 numeric answer 相互适应，从而让 LLM 具备提供简洁且可靠的推理，并使其更易理解。  surprisingly，自我提示也使 symbolic 准确性高于 numeric 和 symbolic 准确性，提供了一个 ensemble 效果。 我们将 SVAMP_Sym dataset 发布给未来的研究人员对于符号数学问题进行研究。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-Deformable-Convolution-for-Depth-Completion"><a href="#Revisiting-Deformable-Convolution-for-Depth-Completion" class="headerlink" title="Revisiting Deformable Convolution for Depth Completion"></a>Revisiting Deformable Convolution for Depth Completion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01905">http://arxiv.org/abs/2308.01905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinglong Sun, Jean Ponce, Yu-Xiong Wang</li>
<li>for: 这篇论文旨在提高深度地图的质量，具体来说是从粗糙的深度地图中生成高质量的稠密深度地图。</li>
<li>methods: 该论文提出了一种使用可变核函数卷积来单 passes地进行改进，从而解决了迭代循环的缺点，并且通过系统地调查了多种表现方法，以更好地理解可变核函数的作用和如何利用其进行深度 completion。</li>
<li>results: 研究人员通过对大规模的 KITTI 数据集进行评估，发现他们的模型在准确率和执行速度两个方面均达到了领先水平。<details>
<summary>Abstract</summary>
Depth completion, which aims to generate high-quality dense depth maps from sparse depth maps, has attracted increasing attention in recent years. Previous work usually employs RGB images as guidance, and introduces iterative spatial propagation to refine estimated coarse depth maps. However, most of the propagation refinement methods require several iterations and suffer from a fixed receptive field, which may contain irrelevant and useless information with very sparse input. In this paper, we address these two challenges simultaneously by revisiting the idea of deformable convolution. We propose an effective architecture that leverages deformable kernel convolution as a single-pass refinement module, and empirically demonstrate its superiority. To better understand the function of deformable convolution and exploit it for depth completion, we further systematically investigate a variety of representative strategies. Our study reveals that, different from prior work, deformable convolution needs to be applied on an estimated depth map with a relatively high density for better performance. We evaluate our model on the large-scale KITTI dataset and achieve state-of-the-art level performance in both accuracy and inference speed. Our code is available at https://github.com/AlexSunNik/ReDC.
</details>
<details>
<summary>摘要</summary>
深度完成，目标是生成高质量的稠密深度地图从稀疏深度地图中，在最近几年内吸引了越来越多的注意力。先前的工作通常使用RGB图像作为引导，并通过迭代的空间卷积来精细化估计的粗略深度地图。然而，大多数卷积修充方法需要多个迭代和固定的接受范围，可能包含无关和无用的信息，尤其是与稀疏输入相比。在这篇论文中，我们 simultanously解决了这两个挑战，通过再次探讨可变核 convolution的想法。我们提议一种有效的架构，利用可变核 convolution作为单pass精细化模块，并经验证其超越性。为了更好地理解可变核 convolution的功能和利用其进行深度完成，我们进一步系统地调查了一些代表性的策略。我们的研究表明，与先前工作不同，可变核 convolution需要在估计的深度地图中的相对较高的密度来获得更好的性能。我们在大规模的KITTI dataset上评估了我们的模型，并在准确率和推理速度两个指标上达到了当前领域的状态码水平。我们的代码可以在https://github.com/AlexSunNik/ReDC中找到。
</details></li>
</ul>
<hr>
<h2 id="How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv"><a href="#How-many-preprints-have-actually-been-printed-and-why-a-case-study-of-computer-science-preprints-on-arXiv" class="headerlink" title="How many preprints have actually been printed and why: a case study of computer science preprints on arXiv"></a>How many preprints have actually been printed and why: a case study of computer science preprints on arXiv</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01899">http://arxiv.org/abs/2308.01899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jialiang Lin, Yao Yu, Yu Zhou, Zhiyang Zhou, Xiaodong Shi</li>
<li>For: This paper aims to quantify the number of preprints that are eventually published in peer-reviewed venues, and to investigate the characteristics of published preprints in the field of computer science.* Methods: The authors use a case study of computer science preprints submitted to arXiv from 2008 to 2017, and employ a semantics-based mapping method using BERT to match preprints with their published versions.* Results: The authors find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. They also identify several characteristics that are associated with published preprints, including adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references, and available source code.<details>
<summary>Abstract</summary>
Preprints play an increasingly critical role in academic communities. There are many reasons driving researchers to post their manuscripts to preprint servers before formal submission to journals or conferences, but the use of preprints has also sparked considerable controversy, especially surrounding the claim of priority. In this paper, a case study of computer science preprints submitted to arXiv from 2008 to 2017 is conducted to quantify how many preprints have eventually been printed in peer-reviewed venues. Among those published manuscripts, some are published under different titles and without an update to their preprints on arXiv. In the case of these manuscripts, the traditional fuzzy matching method is incapable of mapping the preprint to the final published version. In view of this issue, we introduce a semantics-based mapping method with the employment of Bidirectional Encoder Representations from Transformers (BERT). With this new mapping method and a plurality of data sources, we find that 66% of all sampled preprints are published under unchanged titles and 11% are published under different titles and with other modifications. A further analysis was then performed to investigate why these preprints but not others were accepted for publication. Our comparison reveals that in the field of computer science, published preprints feature adequate revisions, multiple authorship, detailed abstract and introduction, extensive and authoritative references and available source code.
</details>
<details>
<summary>摘要</summary>
Preprints 在学术社区中发挥越来越重要的作用。有很多原因使研究人员将文稿上传到 précis servers 之前，而不是正式提交到期刊或会议，但使用 preprints 也引起了较大的争议，特别是在优先权方面。在这篇论文中，我们对计算机科学 preprints 在 arXiv 上从 2008 年到 2017 年的 submissions 进行了案例研究，以计算这些 manuscripts 最终被 print 在 peer-reviewed venue 中的数量。其中一些已经被 published 的文稿，有些在 preprints 上没有更新，这些 manuscripts  traditional fuzzy matching 方法无法映射 preprints 到最终发表的版本。为解决这个问题，我们引入 semantics-based mapping 方法，使用 Bidirectional Encoder Representations from Transformers (BERT)。与传统方法不同的是，我们使用多种数据源，并发现了以下结果：66% 的样本 preprints 被发表不变的标题，11% 的样本 preprints 被发表并有其他修改。然后，我们进行了进一步的分析，以 investigating 为什么这些 preprints 而不是其他的被accepted  для发表。我们的比较发现，在计算机科学领域中，发表的 preprints 具有充分的修改、多个作者、详细的摘要和引言、详细的参考文献和可用的源代码。
</details></li>
</ul>
<hr>
<h2 id="Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning"><a href="#Improving-Replay-Sample-Selection-and-Storage-for-Less-Forgetting-in-Continual-Learning" class="headerlink" title="Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning"></a>Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01895">http://arxiv.org/abs/2308.01895</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Brignac, Niels Lobo, Abhijit Mahalanobis</li>
<li>for: 该研究旨在解决深度学习模型在进行连续学习时免受前任务卷积失忆的问题。</li>
<li>methods: 该研究使用了一种新的比较方法，与常见的储存样本方法进行对比，并提供了一种细致的分析方法来找到最佳储存样本的数量。</li>
<li>results: 该研究结果表明，使用该新的比较方法和细致的分析方法可以更好地选择最有价值的样本进行储存，从而提高连续学习的性能。<details>
<summary>Abstract</summary>
Continual learning seeks to enable deep learners to train on a series of tasks of unknown length without suffering from the catastrophic forgetting of previous tasks. One effective solution is replay, which involves storing few previous experiences in memory and replaying them when learning the current task. However, there is still room for improvement when it comes to selecting the most informative samples for storage and determining the optimal number of samples to be stored. This study aims to address these issues with a novel comparison of the commonly used reservoir sampling to various alternative population strategies and providing a novel detailed analysis of how to find the optimal number of stored samples.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese:Continual learning 目标是帮助深度学习者学习一系列任务的长度未知而不受前任务忘记的影响。一种有效的解决方案是 reuse，即将前一些经验存储在内存中，并在学习当前任务时重新播放。然而，还有很多可以提高的空间，包括选择存储的最有用样本和确定存储样本的优化数量。这项研究目标是通过对通用的队列抽样与其他人口策略进行比较，并提供一种新的详细分析，以寻找最佳存储样本的数量。
</details></li>
</ul>
<hr>
<h2 id="Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso"><a href="#Exact-identification-of-nonlinear-dynamical-systems-by-Trimmed-Lasso" class="headerlink" title="Exact identification of nonlinear dynamical systems by Trimmed Lasso"></a>Exact identification of nonlinear dynamical systems by Trimmed Lasso</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01891">http://arxiv.org/abs/2308.01891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shawn L. Kiser, Mikhail Guskov, Marc Rébillat, Nicolas Ranc</li>
<li>for: 本研究旨在提出一种可以在实际数据中进行非线性动力系统标定的方法，能够处理有限长度和噪声的实际数据。</li>
<li>methods: 本研究使用了SINDy算法，以及其多种扩展，如E-SINDy和TRIM。这些方法都是基于梯度最小化的方法，但是TRIM方法可以提供更加精准的结果，并且可以在更加严重的噪声和有限数据情况下进行标定。</li>
<li>results: 本研究对三个不同的非线性动力系统进行了实验，结果表明，TRIM方法可以在有限长度和噪声的实际数据中提供更加精准的标定结果，而E-SINDy方法则可能会出现残差。此外，TRIM方法的计算成本与STLS算法相同，可以使用可 convex 的解决方法进行优化。<details>
<summary>Abstract</summary>
Identification of nonlinear dynamical systems has been popularized by sparse identification of the nonlinear dynamics (SINDy) via the sequentially thresholded least squares (STLS) algorithm. Many extensions SINDy have emerged in the literature to deal with experimental data which are finite in length and noisy. Recently, the computationally intensive method of ensembling bootstrapped SINDy models (E-SINDy) was proposed for model identification, handling finite, highly noisy data. While the extensions of SINDy are numerous, their sparsity-promoting estimators occasionally provide sparse approximations of the dynamics as opposed to exact recovery. Furthermore, these estimators suffer under multicollinearity, e.g. the irrepresentable condition for the Lasso. In this paper, we demonstrate that the Trimmed Lasso for robust identification of models (TRIM) can provide exact recovery under more severe noise, finite data, and multicollinearity as opposed to E-SINDy. Additionally, the computational cost of TRIM is asymptotically equal to STLS since the sparsity parameter of the TRIM can be solved efficiently by convex solvers. We compare these methodologies on challenging nonlinear systems, specifically the Lorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmark of No\"el and Schoukens, 2016, and a time delay system describing tool cutting dynamics. This study emphasizes the comparisons between STLS, reweighted $\ell_1$ minimization, and Trimmed Lasso in identification with respect to problems faced by practitioners: the problem of finite and noisy data, the performance of the sparse regression of when the library grows in dimension (multicollinearity), and automatic methods for choice of regularization parameters.
</details>
<details>
<summary>摘要</summary>
非线性动力系统的识别已经得到了广泛的应用，通过非线性动力系统简化的逻辑（SINDy）via 随机阈值最小二乘（STLS）算法。在文献中，许多基于SINDy的扩展出现了，以处理实际数据的限定长度和噪声。最近，为了模型识别，提出了 computationally intensive的 ensemble bootstrapped SINDy模型（E-SINDy）方法。虽然扩展SINDy多种，但它们的稀疏采样器 occasionally提供稀疏的动力简化，而不是精确的回归。此外，这些采样器在多icollinearity情况下会受到影响，例如Lasso中的不可 reprehender condition。在这篇论文中，我们表明了 Trimmed Lasso 可以在更严重的噪声、有限数据和多icollinearity情况下提供精确的回归，而不是E-SINDy。此外，TRIM的计算成本是 STLS 的 asymptotic 等价，因为TRIM 的稀疏参数可以由 convex 解决器有效地解决。我们将这些方法在非线性系统中进行比较，包括 Lorenz 63 系统、Bouc Wen 振荡器和时延系统，以及2016年 No\"el 和 Schoukens 非线性动力系统比赛中的非线性动力系统 benchmark。这一研究强调了 STLS、重量 $\ell_1$ 最小化和 Trimmed Lasso 在面临实际问题时的比较：有限和噪声数据、稀疏回归在库存 grows 时的性能，以及自动选择正则化参数的问题。
</details></li>
</ul>
<hr>
<h2 id="DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations"><a href="#DualCoOp-Fast-and-Effective-Adaptation-to-Multi-Label-Recognition-with-Limited-Annotations" class="headerlink" title="DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations"></a>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01890">http://arxiv.org/abs/2308.01890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Hu, Ximeng Sun, Stan Sclaroff, Kate Saenko</li>
<li>for: 这个研究的目的是提高多 Label 图像识别 tasks 的准确性，特别是在低标签情况下。</li>
<li>methods: 这个研究使用了一个名为 Evidence-guided Dual Context Optimization (DualCoOp++) 的框架，这是一个统一的方法来解决 partial-label 和 zero-shot multi-label 识别 задачі。DualCoOp++ 使用了不同的文本内容来分类目标类别，并且将这些内容转换为 Parametric 组件。</li>
<li>results: 实验结果显示，DualCoOp++ 在两个低标签情况下的标准多 Label 识别Benchmark上表现出色，较以前的方法更好。<details>
<summary>Abstract</summary>
Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
多 Label 图像识别在低标签 режиме是一项具有挑战性和实际意义的任务。先前的研究通过学习文本和视觉空间之间的对应关系来资料缺乏多 Label 图像标注，但可能会受到质量不佳多 Label 图像标注的影响。在这项研究中，我们利用了强大的文本和视觉特征之间的对应关系，它们在 Millionen 个 auxiliary 图像-文本对中预训练。我们提出了一种高效可靠的框架，即 Evidence-guided Dual Context Optimization（DualCoOp++），它作为多 Label 图像识别中的一种统一方法。在 DualCoOp++ 中，我们分别编码目标类的证据、积极和消极上下文为参数化的文本输入（即提示）中的Parametric 组件。证据上下文的目的是找到目标类相关的所有视觉内容，并作为指导将空间领域中的积极和消极上下文聚合，以更好地区分相似类别。此外，我们还引入了一个 Winner-Take-All 模块，它在训练中促进类之间的交互，而不需要额外的参数和成本。由于 DualCoOp++ 对预训练的视觉语言框架做出了最小的额外学习负担，因此它可以快速适应多 Label 图像识别任务，即使具有有限的标注和未看到的类。实验表明，我们的方法在标准多 Label 图像识别标准 benchmark 上表现出优于状态的方法。
</details></li>
</ul>
<hr>
<h2 id="Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums"><a href="#Cream-Skimming-the-Underground-Identifying-Relevant-Information-Points-from-Online-Forums" class="headerlink" title="Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums"></a>Cream Skimming the Underground: Identifying Relevant Information Points from Online Forums</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02581">http://arxiv.org/abs/2308.02581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Felipe Moreno-Vera, Mateus Nogueira, Cainã Figueiredo, Daniel Sadoc Menasché, Miguel Bicudo, Ashton Woiwood, Enrico Lovat, Anton Kocheturov, Leandro Pfleger de Aguiar</li>
<li>for: 本研究提出一种基于机器学习的方法，用于在野外抓取漏洞利用情况。随着在线上讨论漏洞利用的帖子和帖子数量不断增加，需要一种自动化处理这些帖子和帖子的方法，以触发警报 Depending on their content.</li>
<li>methods: 我们使用了CrimeBB数据集，该数据集包含多个下面forum中的数据，并开发了一个监督式机器学习模型，可以过滤引用CVEs的帖子，并将其分为Proof-of-Concept、Weaponization和利用三个类别。使用Random Forest算法，我们表明可以在分类任务中达到0.99以上的准确率、精度和准确率。</li>
<li>results: 我们发现，在 weaponization和利用之间存在差异，例如解释决定树的输出，并分析了黑客社区的利益和其他相关方面。总的来说，我们的工作提供了野外漏洞利用情况的研究，可以用于提供额外的真实数据，以便更好地评估模型如EPSS和Expected Exploitability。<details>
<summary>Abstract</summary>
This paper proposes a machine learning-based approach for detecting the exploitation of vulnerabilities in the wild by monitoring underground hacking forums. The increasing volume of posts discussing exploitation in the wild calls for an automatic approach to process threads and posts that will eventually trigger alarms depending on their content. To illustrate the proposed system, we use the CrimeBB dataset, which contains data scraped from multiple underground forums, and develop a supervised machine learning model that can filter threads citing CVEs and label them as Proof-of-Concept, Weaponization, or Exploitation. Leveraging random forests, we indicate that accuracy, precision and recall above 0.99 are attainable for the classification task. Additionally, we provide insights into the difference in nature between weaponization and exploitation, e.g., interpreting the output of a decision tree, and analyze the profits and other aspects related to the hacking communities. Overall, our work sheds insight into the exploitation of vulnerabilities in the wild and can be used to provide additional ground truth to models such as EPSS and Expected Exploitability.
</details>
<details>
<summary>摘要</summary>
Note: "EPSS" stands for "Expected Potential Security Score" and "Expected Exploitability" is a metric used to measure the severity of a vulnerability.
</details></li>
</ul>
<hr>
<h2 id="Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory"><a href="#Statistical-Estimation-Under-Distribution-Shift-Wasserstein-Perturbations-and-Minimax-Theory" class="headerlink" title="Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory"></a>Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01853">http://arxiv.org/abs/2308.01853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/patrickrchao/dist_shift_exp">https://github.com/patrickrchao/dist_shift_exp</a></li>
<li>paper_authors: Patrick Chao, Edgar Dobriban</li>
<li>for: 本文研究了现代统计学中的分布Shift问题，即数据点的perturbation可能会系统地改变数据的性质。</li>
<li>methods: 本文使用 Wasserstein distribution shift，研究了每个数据点可能会受到轻微改动的情况，而不是Huber contamination模型中的一部分观察值是异常值。本文还研究了各种重要的统计问题，包括位置估计、线性回归和非 Parametric density estimation。</li>
<li>results: 本文发现，在平方损函数下的mean估计和线性回归预测错误中， sample mean和least squares estimator是相对最佳的。这些优点在独立分布shift和共同分布shift下都存在，但最差的perturbation和最大风险不同。其他问题中，提供了近似最佳的估计器和精确的finite-sample bound。本文还介绍了一些用于下界最大风险的工具，如location家族的缓和技术，以及classical工具的扩展，如最差序列的 prior、modulus of continuity、Le Cam的、Fano的和Assouad的方法。<details>
<summary>Abstract</summary>
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For other problems, we provide nearly optimal estimators and precise finite-sample bounds. We also introduce several tools for bounding the minimax risk under distribution shift, such as a smoothing technique for location families, and generalizations of classical tools including least favorable sequences of priors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.
</details>
<details>
<summary>摘要</summary>
现代统计学中的分布转移是一个严重的问题，因为它可能会系统性地改变数据的性质，从真实的情况偏离。我们关注 Wasserstein 分布转移，其中每个数据点都可能会经历一些微的扰动，而不是 Huber 污染模型，其中一部分观测值是异常值。我们提出并研究了分布转移的不同类型，包括共同扰动分布转移。我们分析了一些重要的统计问题，包括位置估计、线性回归和非 Parametric 密度估计。在平方损失下，我们发现了最小最大风险、最不利的扰动和 sample 均值和最小二乘估计器是相应优化的。这些优化存在独立和共同转移下都是正确的，但最不利的扰动和最大风险不同。对于其他问题，我们提供了近似优化的估计器和精确的 finite-sample 上限。我们还引入了一些用于下界最大风险的工具，包括分布转移后的平滑技术、类 least favorable 序列假设、模ulus 稳定性、Le Cam 、Fano 和 Assouad 的方法。
</details></li>
</ul>
<hr>
<h2 id="Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks"><a href="#Curricular-Transfer-Learning-for-Sentence-Encoded-Tasks" class="headerlink" title="Curricular Transfer Learning for Sentence Encoded Tasks"></a>Curricular Transfer Learning for Sentence Encoded Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01849">http://arxiv.org/abs/2308.01849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jader Martins Camboim de Sá, Matheus Ferraroni Sanches, Rafael Roque de Souza, Júlio Cesar dos Reis, Leandro Aparecido Villas</li>
<li>for: 提高NLU任务中模型的表现，尤其是在数据分布变化时。</li>
<li>methods: 提出了一种逐步适应（curriculum）策略，通过数据黑客和语法分析导航进行适应。</li>
<li>results: 在我们的实验中，我们的方法比其他已知预训练方法在多语言对话任务（MultiWoZ）中获得了显著提高。<details>
<summary>Abstract</summary>
Fine-tuning language models in a downstream task is the standard approach for many state-of-the-art methodologies in the field of NLP. However, when the distribution between the source task and target task drifts, \textit{e.g.}, conversational environments, these gains tend to be diminished. This article proposes a sequence of pre-training steps (a curriculum) guided by "data hacking" and grammar analysis that allows further gradual adaptation between pre-training distributions. In our experiments, we acquire a considerable improvement from our method compared to other known pre-training approaches for the MultiWoZ task.
</details>
<details>
<summary>摘要</summary>
通常的方法是在下游任务中细化语言模型，以获得许多状态OF-THE-ART的成果。但是，当源任务和目标任务的分布发生变化，例如对话环境，这些改进往往减少。这篇文章提出了一系列的预训练步骤（课程），通过“数据黑客”和语法分析引导，以进一步适应预训练分布的变化。在我们的实验中，我们获得了与其他已知预训练方法相比较大的改进，用于多语言对话任务。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction"><a href="#Probabilistic-Deep-Supervision-Network-A-Noise-Resilient-Approach-for-QoS-Prediction" class="headerlink" title="Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction"></a>Probabilistic Deep Supervision Network: A Noise-Resilient Approach for QoS Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.02580">http://arxiv.org/abs/2308.02580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hotfrom/pds-net">https://github.com/hotfrom/pds-net</a></li>
<li>paper_authors: Ziliang Wang, Xiaohong Zhang, Sheng Huang, Wei Zhang, Dan Yang, Meng Yan</li>
<li>for: 提高用户满意度，准确预测 unknown QoS 值</li>
<li>methods: 提出了 Probabilistic Deep Supervision Network (PDS-Net) 框架，利用 Gaussian 型概率空间进行中间层级supervision，学习known features和真实标签的概率空间</li>
<li>results: 在两个实际 QoS 数据集上进行实验评估，比对 estado-of-the-art 基elines， validate 我们的方法的有效性<details>
<summary>Abstract</summary>
Quality of Service (QoS) prediction is an essential task in recommendation systems, where accurately predicting unknown QoS values can improve user satisfaction. However, existing QoS prediction techniques may perform poorly in the presence of noise data, such as fake location information or virtual gateways. In this paper, we propose the Probabilistic Deep Supervision Network (PDS-Net), a novel framework for QoS prediction that addresses this issue. PDS-Net utilizes a Gaussian-based probabilistic space to supervise intermediate layers and learns probability spaces for both known features and true labels. Moreover, PDS-Net employs a condition-based multitasking loss function to identify objects with noise data and applies supervision directly to deep features sampled from the probability space by optimizing the Kullback-Leibler distance between the probability space of these objects and the real-label probability space. Thus, PDS-Net effectively reduces errors resulting from the propagation of corrupted data, leading to more accurate QoS predictions. Experimental evaluations on two real-world QoS datasets demonstrate that the proposed PDS-Net outperforms state-of-the-art baselines, validating the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
服务质量（QoS）预测是推荐系统中的一项重要任务，可以提高用户满意度。然而，现有的QoS预测技术可能在噪声数据存在时表现不佳。在这篇论文中，我们提出了可靠性深度监督网络（PDS-Net），一种解决这个问题的新框架。PDS-Net使用 Gaussian 型概率空间来监督中间层，并学习概率空间 для已知特征和真实标签。此外，PDS-Net 使用基于条件的多任务损失函数来识别具有噪声数据的对象，并直接将深度特征从概率空间中抽取到真实标签的概率空间中进行监督。因此，PDS-Net 可以减少噪声数据的传播错误，从而提高 QoS 预测的准确性。实验评估在两个真实 QoS 数据集上表明，提出的 PDS-Net 已经超越了状态艺术基eline。
</details></li>
</ul>
<hr>
<h2 id="URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion"><a href="#URET-Universal-Robustness-Evaluation-Toolkit-for-Evasion" class="headerlink" title="URET: Universal Robustness Evaluation Toolkit (for Evasion)"></a>URET: Universal Robustness Evaluation Toolkit (for Evasion)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2308.01840">http://arxiv.org/abs/2308.01840</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ibm/uret">https://github.com/ibm/uret</a></li>
<li>paper_authors: Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy, Masha Zorin</li>
<li>for: 本研究旨在提高机器学习模型的安全和可靠性，通过生成可逃脱攻击的输入，以帮助确保AI任务的正确性和可靠性。</li>
<li>methods: 本研究提出了一种新的框架，可以生成不同输入类型和任务领域的攻击输入。该框架使用给定的输入变换集合，找到一个符合semantic和功能要求的攻击输入序列。</li>
<li>results: 本研究在多种不同的机器学习任务和输入表示中展示了框架的通用性。此外，研究还表明了生成攻击示例的重要性，以便应用防御技术。<details>
<summary>Abstract</summary>
Machine learning models are known to be vulnerable to adversarial evasion attacks as illustrated by image classification models. Thoroughly understanding such attacks is critical in order to ensure the safety and robustness of critical AI tasks. However, most evasion attacks are difficult to deploy against a majority of AI systems because they have focused on image domain with only few constraints. An image is composed of homogeneous, numerical, continuous, and independent features, unlike many other input types to AI systems used in practice. Furthermore, some input types include additional semantic and functional constraints that must be observed to generate realistic adversarial inputs. In this work, we propose a new framework to enable the generation of adversarial inputs irrespective of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples as they enable the deployment of mitigation techniques.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a new framework to generate adversarial inputs regardless of the input type and task domain. Given an input and a set of pre-defined input transformations, our framework discovers a sequence of transformations that result in a semantically correct and functional adversarial input. We demonstrate the generality of our approach on several diverse machine learning tasks with various input representations. We also show the importance of generating adversarial examples, as they enable the deployment of mitigation techniques.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="http://example.com/2023/08/04/cs.LG_2023_08_04/" data-id="cllsk9gq6002q9c888iwf2km0" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CR/">cs.CR</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">43</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">44</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">53</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">114</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'', root:''}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
