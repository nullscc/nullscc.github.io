
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/4/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.SD_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.SD_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T15:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.SD_2023_09_04/">cs.SD - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech"><a href="#Minimal-Effective-Theory-for-Phonotactic-Memory-Capturing-Local-Correlations-due-to-Errors-in-Speech" class="headerlink" title="Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech"></a>Minimal Effective Theory for Phonotactic Memory: Capturing Local Correlations due to Errors in Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02466">http://arxiv.org/abs/2309.02466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Myles Eugenio</li>
<li>for: 这篇论文是关于语音学的一篇研究，旨在探讨 spoken words 的学习和生成。</li>
<li>methods: 这篇论文使用了一种基于 tensor-network 的 locally-connected 模型，利用了地方phonetic correlations来促进 spoken words 的学习和生成。</li>
<li>results: 研究发现，这种模型可以帮助学习者更好地理解和生成 spoken words，同时还可以提供一 hierarchy of the most likely errors 的信息，帮助学习者更好地改进语音表达。 I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)
</details>
<details>
<summary>摘要</summary>
spoken language evolves constrained by the economy of speech, which depends on factors such as the structure of the human mouth. This gives rise to local phonetic correlations in spoken words. Here we demonstrate that these local correlations facilitate the learning of spoken words by reducing their information content. We do this by constructing a locally-connected tensor-network model, inspired by similar variational models used for many-body physics, which exploits these local phonetic correlations to facilitate the learning of spoken words. The model is therefore a minimal model of phonetic memory, where "learning to pronounce" and "learning a word" are one and the same. A consequence of which is the learned ability to produce new words which are phonetically reasonable for the target language; as well as providing a hierarchy of the most likely errors that could be produced during the action of speech. We test our model against Latin and Turkish words. (The code is available on GitHub.)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech"><a href="#A-Comparative-Analysis-of-Pretrained-Language-Models-for-Text-to-Speech" class="headerlink" title="A Comparative Analysis of Pretrained Language Models for Text-to-Speech"></a>A Comparative Analysis of Pretrained Language Models for Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01576">http://arxiv.org/abs/2309.01576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcel Granero-Moya, Penny Karanasou, Sri Karlapati, Bastian Schnell, Nicole Peinelt, Alexis Moinet, Thomas Drugman</li>
<li>for: 这个研究旨在investigate the impact of different pre-trained language models (PLMs) on text-to-speech (TTS) systems, specifically in prosody prediction and pause prediction tasks.</li>
<li>methods: 研究者使用了15种不同的PLMs进行训练，并对模型的不同大小和表情类型进行了分析。</li>
<li>results: 研究发现，模型大小与质量之间存在对数关系，而且在不同的表情类型下表现出显著的差异。此外， pause prediction 任务对小型模型的敏感性较低，并且发现了与GLUE分数相关的关系。<details>
<summary>Abstract</summary>
State-of-the-art text-to-speech (TTS) systems have utilized pretrained language models (PLMs) to enhance prosody and create more natural-sounding speech. However, while PLMs have been extensively researched for natural language understanding (NLU), their impact on TTS has been overlooked. In this study, we aim to address this gap by conducting a comparative analysis of different PLMs for two TTS tasks: prosody prediction and pause prediction. Firstly, we trained a prosody prediction model using 15 different PLMs. Our findings revealed a logarithmic relationship between model size and quality, as well as significant performance differences between neutral and expressive prosody. Secondly, we employed PLMs for pause prediction and found that the task was less sensitive to small models. We also identified a strong correlation between our empirical results and the GLUE scores obtained for these language models. To the best of our knowledge, this is the first study of its kind to investigate the impact of different PLMs on TTS.
</details>
<details>
<summary>摘要</summary>
现代文本到语音（TTS）系统已经使用预训练语言模型（PLM）来提高气质和创造更自然的语音。然而，虽然PLM在自然语言理解（NLU）方面已经得到了广泛的研究，但它们在TTS方面的影响却被忽视了。在这项研究中，我们想要解决这个差距，通过对不同PLM进行比较分析，以便更好地理解它们在TTS任务中的表现。首先，我们使用15种不同的PLM来训练一个气质预测模型。我们的发现表明，模型大小与质量之间存在对数的关系，同时，中性和表达气质之间存在显著的性能差异。其次，我们使用PLM进行停顿预测任务，发现这个任务对小型模型来说较为不敏感。我们还发现了这些实验结果和GLUE分数中的语言模型所获得的相关性强。根据我们所知，这是首次对不同PLM在TTS方面的影响进行研究。
</details></li>
</ul>
<hr>
<h2 id="Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models"><a href="#Single-Channel-Speech-Enhancement-with-Deep-Complex-U-Networks-and-Probabilistic-Latent-Space-Models" class="headerlink" title="Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models"></a>Single-Channel Speech Enhancement with Deep Complex U-Networks and Probabilistic Latent Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01535">http://arxiv.org/abs/2309.01535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eike J. Nustede, Jörn Anemüller</li>
<li>for: 提高speech底层提取的深度和复杂性</li>
<li>methods:  integrate probabilistic（i.e., variational）latent space model，Complex-value处理和自注意</li>
<li>results: 在MS-DNS 2020和Voicebank+Demand datasets上取得了高水平的评价，比如SI-SDR达到20.2dB，与无 probabilistic latent space版本相比提高了0.5-1.4dB，与WaveUNet相比提高了2-2.4dB，与PHASEN相比提高了6.7dB。<details>
<summary>Abstract</summary>
In this paper, we propose to extend the deep, complex U-Network architecture for speech enhancement by incorporating a probabilistic (i.e., variational) latent space model. The proposed model is evaluated against several ablated versions of itself in order to study the effects of the variational latent space model, complex-value processing, and self-attention. Evaluation on the MS-DNS 2020 and Voicebank+Demand datasets yields consistently high performance. E.g., the proposed model achieves an SI-SDR of up to 20.2 dB, about 0.5 to 1.4 dB higher than its ablated version without probabilistic latent space, 2-2.4 dB higher than WaveUNet, and 6.7 dB above PHASEN. Compared to real-valued magnitude spectrogram processing with a variational U-Net, the complex U-Net achieves an improvement of up to 4.5 dB SI-SDR. Complex spectrum encoding as magnitude and phase yields best performance in anechoic conditions whereas real and imaginary part representation results in better generalization to (novel) reverberation conditions, possibly due to the underlying physics of sound.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了扩展深度、复杂的U-网络架构，以提高语音增强。我们在这个模型中添加了一个 probabilistic（即变量）干元空间模型。我们对这个模型进行了一些简化版本的比较，以研究变量干元空间模型、复杂值处理和自注意的影响。在MS-DNS 2020和Voicebank+Demand datasets上进行评估，我们发现这个模型在语音增强中表现出色，例如在MS-DNS 2020 dataset上达到20.2 dB的SI-SDR，比不含变量干元空间模型的模型高出0.5-1.4 dB，比WaveUNet高出2.2-4.4 dB，并高出PHASEN的6.7 dB。与实数值spectrogram处理的变量U-Net进行比较，复杂spectrum编码为实数值spectrogram的情况下，实现最佳性能，而实数值和虚数值表示的情况下，更好地泛化到（新的）频率响应条件，可能是因为音波的物理学习。
</details></li>
</ul>
<hr>
<h2 id="Quid-Manumit-–-Freeing-the-Qubit-for-Art"><a href="#Quid-Manumit-–-Freeing-the-Qubit-for-Art" class="headerlink" title="Quid Manumit – Freeing the Qubit for Art"></a>Quid Manumit – Freeing the Qubit for Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03104">http://arxiv.org/abs/2309.03104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Carney</li>
<li>for: 这篇论文描述了如何将量子计算应用到音乐创作中，创造出独立的量子音乐效果和乐器。</li>
<li>methods: 本论文使用了ARM基于Raspberry Pi Pico嵌入式微控制器的量子模拟器代码，并提供了一些示例，包括一个量子MIDI处理器，可以根据输入音符生成附加的伴奏和具有量子生成的乐器。</li>
<li>results: 本论文的结果包括一个量子扭曲模块，可以根据量子电路改变乐器的原始声音，以及一个自包含的量子钢琴和一个名为“量子搅拌器”的效果模块插件。这些示例都提供了开源代码，并且这是作者知道的第一个嵌入式量子模拟器 для乐器（另一个QSIM）。<details>
<summary>Abstract</summary>
This paper describes how to `Free the Qubit' for art, by creating standalone quantum musical effects and instruments. Previously released quantum simulator code for an ARM-based Raspberry Pi Pico embedded microcontroller is utilised here, and several examples are built demonstrating different methods of utilising embedded resources: The first is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. The second is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, which is presented in two forms; a self-contained Quantum Stylophone, and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1. This paper also discusses future work and directions for quantum instruments, and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (another QSIM).
</details>
<details>
<summary>摘要</summary>
One example is a Quantum MIDI processor that generates additional notes for accompaniment and unique quantum-generated instruments based on the input notes, decoded and passed through a quantum circuit in an embedded simulator. Another example is a Quantum Distortion module that changes an instrument's raw sound according to a quantum circuit, presented in two forms: a self-contained Quantum Stylophone and an effect module plugin called 'QubitCrusher' for the Korg Nu:Tekt NTS-1.The paper discusses future work and directions for quantum instruments and provides all examples as open source. This is, to the author's knowledge, the first example of embedded Quantum Simulators for Instruments of Music (QSIM).
</details></li>
</ul>
<hr>
<h2 id="RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes"><a href="#RGI-Net-3D-Room-Geometry-Inference-from-Room-Impulse-Responses-in-the-Absence-of-First-order-Echoes" class="headerlink" title="RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes"></a>RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01513">http://arxiv.org/abs/2309.01513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Inmo Yeon, Jung-Woo Choi</li>
<li>for: 这篇论文主要是为了提出一种基于深度神经网络的房间几何学推理（RGI）方法，以便在实际的3D音频渲染中使用。</li>
<li>methods: 这篇论文使用了一种名为RGI-Net的深度神经网络，该网络可以基于房间响应函数（RIR）中的高阶反射关系学习和利用 Room geometry。而不像传统的RGI技术，RGI-Net不需要 convex 形房间、先知数量和第一阶反射的假设。</li>
<li>results: RGI-Net可以在实际测量的声学数据上高精度地估算房间几何学，并且可以处理非几何形房间和缺失第一阶反射的情况。<details>
<summary>Abstract</summary>
Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphone array and a single loudspeaker, which greatly improves its practical applicability. RGI-Net includes the evaluation network that separately evaluates the presence probability of walls, so the geometry inference is possible without prior knowledge of the number of walls.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment"><a href="#BadSQA-Stealthy-Backdoor-Attacks-Using-Presence-Events-as-Triggers-in-Non-Intrusive-Speech-Quality-Assessment" class="headerlink" title="BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment"></a>BadSQA: Stealthy Backdoor Attacks Using Presence Events as Triggers in Non-Intrusive Speech Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01480">http://arxiv.org/abs/2309.01480</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ying Ren, Kailai Shen, Zhe Ye, Diqun Yan</li>
<li>for: 这个论文是为了研究非侵入式语音质量评估（NISQA）系统中的安全问题，特别是针对不可靠的资源引入的攻击。</li>
<li>methods: 该论文提出了一种基于存在事件的新型后门攻击方法，用于攻击NISQA系统。该方法利用存在事件作为触发器，以实现高度隐蔽的攻击。</li>
<li>results: 实验结果表明，该提出的后门攻击方法可以在四个基准数据集上达到99%的攻击成功率，仅需3%的毒素率。<details>
<summary>Abstract</summary>
Non-Intrusive speech quality assessment (NISQA) has gained significant attention for predicting the mean opinion score (MOS) of speech without requiring the reference speech. In practical NISQA scenarios, untrusted third-party resources are often employed during deep neural network training to reduce costs. However, it would introduce a potential security vulnerability as specially designed untrusted resources can launch backdoor attacks against NISQA systems. Existing backdoor attacks primarily focus on classification tasks and are not directly applicable to NISQA which is a regression task. In this paper, we propose a novel backdoor attack on NISQA tasks, leveraging presence events as triggers to achieving highly stealthy attacks. To evaluate the effectiveness of our proposed approach, we conducted experiments on four benchmark datasets and employed two state-of-the-art NISQA models. The results demonstrate that the proposed backdoor attack achieved an average attack success rate of up to 99% with a poisoning rate of only 3%.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation"><a href="#Text-Only-Domain-Adaptation-for-End-to-End-Speech-Recognition-through-Down-Sampling-Acoustic-Representation" class="headerlink" title="Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation"></a>Text-Only Domain Adaptation for End-to-End Speech Recognition through Down-Sampling Acoustic Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02459">http://arxiv.org/abs/2309.02459</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Weinan Tong, Yaoxun Xu, Changhe Song, Zhiyong Wu, Zhao You, Dan Su, Dong Yu, Helen Meng</li>
<li>for: 提高新领域自动语音识别（ASR）性能使用文本数据进行适应性调整</li>
<li>methods: 通过下采样声音表示进行与文本模式匹配，并通过引入连续积 integrate-and-fire（CIF）模块生成声音表示，使ASR模型能够更好地学习各个模式的统一表示</li>
<li>results: 实验结果表明，提posed方法可以更好地适应新领域数据进行适应性调整<details>
<summary>Abstract</summary>
Mapping two modalities, speech and text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details>
<details>
<summary>摘要</summary>
Mapping two modalities, speech和text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.Here's the translation in Traditional Chinese as well:Mapping two modalities, speech和text, into a shared representation space, is a research topic of using text-only data to improve end-to-end automatic speech recognition (ASR) performance in new domains. However, the length of speech representation and text representation is inconsistent. Although the previous method up-samples the text representation to align with acoustic modality, it may not match the expected actual duration. In this paper, we proposed novel representations match strategy through down-sampling acoustic representation to align with text modality. By introducing a continuous integrate-and-fire (CIF) module generating acoustic representations consistent with token length, our ASR model can learn unified representations from both modalities better, allowing for domain adaptation using text-only data of the target domain. Experiment results of new domain data demonstrate the effectiveness of the proposed method.
</details></li>
</ul>
<hr>
<h2 id="SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge"><a href="#SememeASR-Boosting-Performance-of-End-to-End-Speech-Recognition-against-Domain-and-Long-Tailed-Data-Shift-with-Sememe-Semantic-Knowledge" class="headerlink" title="SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge"></a>SememeASR: Boosting Performance of End-to-End Speech Recognition against Domain and Long-Tailed Data Shift with Sememe Semantic Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01437">http://arxiv.org/abs/2309.01437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxu Zhu, Changhe Song, Zhiyong Wu, Helen Meng</li>
<li>for: 提高语音识别效果，特别是对域外数据和长尾数据的处理。</li>
<li>methods: 引入sememe知识来增强语音识别模型，包括sememe知识的概念定义和语音识别模型的结构设计。</li>
<li>results: 实验结果表明，sememe知识可以提高语音识别效果，并且可以提高模型对域外数据和长尾数据的处理能力。<details>
<summary>Abstract</summary>
Recently, excellent progress has been made in speech recognition. However, pure data-driven approaches have struggled to solve the problem in domain-mismatch and long-tailed data. Considering that knowledge-driven approaches can help data-driven approaches alleviate their flaws, we introduce sememe-based semantic knowledge information to speech recognition (SememeASR). Sememe, according to the linguistic definition, is the minimum semantic unit in a language and is able to represent the implicit semantic information behind each word very well. Our experiments show that the introduction of sememe information can improve the effectiveness of speech recognition. In addition, our further experiments show that sememe knowledge can improve the model's recognition of long-tailed data and enhance the model's domain generalization ability.
</details>
<details>
<summary>摘要</summary>
最近，speech recognition领域内已经取得了很好的进步。然而，纯数据驱动方法在不同领域和长尾数据上困难解决问题。考虑到知识驱动方法可以帮助数据驱动方法缓解弊端，我们引入sememe基于语言定义的语义知识信息到speech recognition中（SememeASR）。sememe按照语言定义是语言中最小的语义单元，可以非常好地表示每个词的隐式语义信息。我们的实验显示，通过添加sememe信息可以提高speech recognition的效果。此外，我们的进一步实验还表明，sememe知识可以提高模型对长尾数据的识别和提高模型在不同领域的适应能力。
</details></li>
</ul>
<hr>
<h2 id="MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and"><a href="#MDSC-Towards-Evaluating-the-Style-Consistency-Between-Music-and" class="headerlink" title="MDSC: Towards Evaluating the Style Consistency Between Music and"></a>MDSC: Towards Evaluating the Style Consistency Between Music and</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01340">http://arxiv.org/abs/2309.01340</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiang Zhou, Baoyuan Wang</li>
<li>for: 本研究开发了一个新的评估指标（Music-Dance-Style Consistency，MDSC），用于评估Generated dance motion sequences和conditioning music sequences之间的风格相似性。</li>
<li>methods: 本研究使用了一个 clustering 方法，将 dance motion embedding 和 music embedding 映射到共同空间中，并且将 intra-cluster 距离和 inter-cluster 距离做最大化。</li>
<li>results: 本研究通过评估多种 music-conditioned motion generation 方法的结果，结果显示 MDSC 是一个Robust的评估指标，可以准确地评估 dance motion 和 music 的风格相似性。<details>
<summary>Abstract</summary>
We propose MDSC(Music-Dance-Style Consistency), the first evaluation metric which assesses to what degree the dance moves and music match. Existing metrics can only evaluate the fidelity and diversity of motion and the degree of rhythmic matching between music and motion. MDSC measures how stylistically correlated the generated dance motion sequences and the conditioning music sequences are. We found that directly measuring the embedding distance between motion and music is not an optimal solution. We instead tackle this through modelling it as a clustering problem. Specifically, 1) we pre-train a music encoder and a motion encoder, then 2) we learn to map and align the motion and music embedding in joint space by jointly minimizing the intra-cluster distance and maximizing the inter-cluster distance, and 3) for evaluation purpose, we encode the dance moves into embedding and measure the intra-cluster and inter-cluster distances, as well as the ratio between them. We evaluate our metric on the results of several music-conditioned motion generation methods, combined with user study, we found that our proposed metric is a robust evaluation metric in measuring the music-dance style correlation. The code is available at: https://github.com/zixiangzhou916/MDSC.
</details>
<details>
<summary>摘要</summary>
我们提出了MDSC（音乐舞蹈风格一致性），这是评估音乐和舞蹈动作之间的一致性的首个评价指标。现有的指标只能评估动作和音乐的准确性和多样性，以及它们之间的节奏匹配度。而MDSC则衡量了生成的舞蹈动作序列和conditioning音乐序列之间的风格相关性。我们发现直接测量动作和音乐的嵌入距离并不是最佳解决方案。我们相反地通过模型化它为一个聚类问题来解决。具体来说，我们先预训练了音乐编码器和动作编码器，然后学习将动作和音乐嵌入空间中的mapping和对齐。最后，我们用这些mapping来评估生成的舞蹈动作是否符合音乐风格。我们对几种音乐受控动作生成方法的结果进行评估，并结合用户调查，发现我们提出的指标是一个有力的评价指标，能够准确地衡量音乐和舞蹈风格之间的相关性。代码可以在：https://github.com/zixiangzhou916/MDSC 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.SD_2023_09_04/" data-id="clmjn91oc00br0j88fgj72w30" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/cs.LG_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T10:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/cs.LG_2023_09_04/">cs.LG - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation"><a href="#Attention-Driven-Multi-Modal-Fusion-Enhancing-Sign-Language-Recognition-and-Translation" class="headerlink" title="Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation"></a>Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01860">http://arxiv.org/abs/2309.01860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zaber Ibn Abdul Hakim, Rasman Mubtasim Swargo, Muhammad Abdullah Adnan</li>
<li>for: 这 paper 是为了增加多Modal信息到现有的连续手语识别和翻译管道中。</li>
<li>methods: 我们使用了 Optical flow 信息和 RGB 图像来增强特征，并使用了 Cross-modal 编码器。</li>
<li>results: 我们在 recognition 和翻译任务中各提高了result，在识别任务中降低了 WER 0.9，在翻译任务中提高了大多数 BLEU 分数约 0.6。<details>
<summary>Abstract</summary>
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们设计了一种多modal信息添加机制，用于与现有的连续手语识别和翻译管道结合。在我们的过程中，我们将光流信息与RGB图像结合，以增强特征以 движения相关信息。本研究证明了多modal信息包含的可行性，并使用交叉模态编码器进行实现。我们使用的插件非常轻量级，不需要在端到端方式中添加新模态的特征提取器。我们对手语识别和翻译 task 都进行了应用，并在每个任务上提高了结果。我们在 RWTH-PHOENIX-2014 数据集上进行了手语识别任务的评估，并在 RWTH-PHOENIX-2014T 数据集上进行了翻译任务的评估。在识别任务上，我们的方法降低了 WER 值0.9，在翻译任务上，我们的方法提高了大多数 BLEU 分数的测试集值约0.6。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks"><a href="#Efficient-Defense-Against-Model-Stealing-Attacks-on-Convolutional-Neural-Networks" class="headerlink" title="Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks"></a>Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01838">http://arxiv.org/abs/2309.01838</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kacem Khaled, Mouna Dhaouadi, Felipe Gohring de Magalhães, Gabriela Nicolescu</li>
<li>for: 防止深度学习模型被盗用攻击</li>
<li>methods: 提出了一种简单 yet effective 和高效的防御方法，通过对输出概率进行干扰来防止模型盗用攻击</li>
<li>results: 对三种state-of-the-art盗用攻击进行了验证，与现有防御方法相比，具有$\times37$快的执行速度，不需要额外训练，并且对模型性能有较低的影响。<details>
<summary>Abstract</summary>
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate our approach on large and quantized (i.e., compressed) Convolutional Neural Networks (CNNs) trained on several vision datasets. Our technique outperforms the state-of-the-art defenses with a $\times37$ faster inference latency without requiring any additional model and with a low impact on the model's performance. We validate that our defense is also effective for quantized CNNs targeting edge devices.
</details>
<details>
<summary>摘要</summary>
模型盗取攻击已成为深度学习模型的严重问题，攻击者可以通过访问其黑盒API来盗取已训练的模型。这可能会导致知识产权盗取和安全隐私问题。当前状态的攻击防御建议添加扰动到预测概率中，但它们受到重度计算和对对手的假设的限制。它们经常需要额外训练auxiliary模型，这可能是时间consuming和资源占用的，这阻碍了这些防御在实际应用中的部署。在这篇论文中，我们提出了一种简单 yet有效的防御方案。我们介绍了一种论据 Approach来扰动输出概率。我们的防御可以轻松地与模型一起 интегра，无需额外训练。我们表明了我们的防御能够对三种当前最佳攻击方法进行防御。我们对大量和压缩（i.e., 压缩）的 convolutional Neural Networks（CNNs）进行评估，我们的技术在各个视觉数据集上表现出色，并且在执行速度方面具有$\times37$快的优势，而且不需要任何额外模型，同时具有低的影响于模型性能。我们还 validate了我们的防御是适用于边缘设备的quantized CNNs。
</details></li>
</ul>
<hr>
<h2 id="Delegating-Data-Collection-in-Decentralized-Machine-Learning"><a href="#Delegating-Data-Collection-in-Decentralized-Machine-Learning" class="headerlink" title="Delegating Data Collection in Decentralized Machine Learning"></a>Delegating Data Collection in Decentralized Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01837">http://arxiv.org/abs/2309.01837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nivasini Ananthakrishnan, Stephen Bates, Michael I. Jordan, Nika Haghtalab</li>
<li>for: study the delegation of data collection in decentralized machine learning ecosystems</li>
<li>methods: design optimal and near-optimal contracts to address challenges of lack of certainty and lack of knowledge regarding optimal performance</li>
<li>results: achieve 1-1&#x2F;e fraction of the first-best utility with simple linear contracts, and give sufficient conditions for achieving a vanishing additive approximation to the optimal utility with adaptive and efficient convex programming.<details>
<summary>Abstract</summary>
Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1-1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal's test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.
</details>
<details>
<summary>摘要</summary>
受到分散式机器学习生态的启发，我们研究数据收集的委托。从合约理论为起点，我们设计优化和近似优化的合约，解决机器学习中两个基本挑战：模型评估的不确定性和任何模型的最佳性知识不足。我们示出缺乏确定性可以通过简单的线性合约，实现1-1/e的最佳 utility，即使主人只有一小部分的测试集。此外，我们提供了让数据集大小满足的条件，以实现几乎减少的加法误差估计。为了解决任何模型的最佳性知识不足，我们提供了一个可靠地和高效地computing优化合约的凸程程式。
</details></li>
</ul>
<hr>
<h2 id="Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties"><a href="#Smoothing-ADMM-for-Sparse-Penalized-Quantile-Regression-with-Non-Convex-Penalties" class="headerlink" title="Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties"></a>Smoothing ADMM for Sparse-Penalized Quantile Regression with Non-Convex Penalties</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03094">http://arxiv.org/abs/2309.03094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reza Mirzaeifard, Naveen K. D. Venkategowda, Vinay Chakravarthi Gogineni, Stefan Werner</li>
<li>for: 这个论文研究了带有非凸非积分违偏的量化回归问题，特别是使用最大凸函数偏好（MCP）和缓和凸函数偏好（SCAD）。</li>
<li>methods: 这个论文使用了 alternating direction method of multipliers（ADMM），并提出了一种单循环缓和ADMM算法（SIAD），以加速量化回归的速度。</li>
<li>results: 数据表明，SIAD算法比现有方法更快和稳定，提供了更好的积分回归解决方案。<details>
<summary>Abstract</summary>
This paper investigates quantile regression in the presence of non-convex and non-smooth sparse penalties, such as the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). The non-smooth and non-convex nature of these problems often leads to convergence difficulties for many algorithms. While iterative techniques like coordinate descent and local linear approximation can facilitate convergence, the process is often slow. This sluggish pace is primarily due to the need to run these approximation techniques until full convergence at each step, a requirement we term as a \emph{secondary convergence iteration}. To accelerate the convergence speed, we employ the alternating direction method of multipliers (ADMM) and introduce a novel single-loop smoothing ADMM algorithm with an increasing penalty parameter, named SIAD, specifically tailored for sparse-penalized quantile regression. We first delve into the convergence properties of the proposed SIAD algorithm and establish the necessary conditions for convergence. Theoretically, we confirm a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$ for the sub-gradient bound of augmented Lagrangian. Subsequently, we provide numerical results to showcase the effectiveness of the SIAD algorithm. Our findings highlight that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression.
</details>
<details>
<summary>摘要</summary>
To improve convergence speed, we use the alternating direction method of multipliers (ADMM) and develop a novel single-loop smoothing ADMM algorithm called SIAD, which is tailored for sparse-penalized quantile regression. We first examine the convergence properties of the SIAD algorithm and establish the necessary conditions for convergence.Theoretically, we prove that the sub-gradient bound of the augmented Lagrangian has a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$. Numerical results show that the SIAD method outperforms existing approaches, providing a faster and more stable solution for sparse-penalized quantile regression. Our findings demonstrate that the SIAD method is effective in solving this problem, with a convergence rate of $o\big({k^{-\frac{1}{4}}\big)$.
</details></li>
</ul>
<hr>
<h2 id="Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks"><a href="#Soft-Dropout-A-Practical-Approach-for-Mitigating-Overfitting-in-Quantum-Convolutional-Neural-Networks" class="headerlink" title="Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks"></a>Soft-Dropout: A Practical Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01829">http://arxiv.org/abs/2309.01829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakash Ravindra Shinde, Charu Jain, Amir Kalev</li>
<li>for: 这个论文的目的是研究量子卷积神经网络（QCNN）中的过拟合问题，以及一种常见的过拟合缓解方法（post-training dropout）在量子设置下的应用。</li>
<li>methods: 本文使用了一种简单的 dropout 方法来缓解 QCNN 中的过拟合问题，并发现了该方法在量子设置下可能导致 QCNN 的成功概率下降。作者还提出了一种软化的 dropout 方法来解决过拟合问题。</li>
<li>results: 本文通过测试几个案例来证明软化 dropout 方法可以成功地缓解 QCNN 中的过拟合问题，并且可以保持 QCNN 的成功概率。<details>
<summary>Abstract</summary>
Quantum convolutional neural network (QCNN), an early application for quantum computers in the NISQ era, has been consistently proven successful as a machine learning (ML) algorithm for several tasks with significant accuracy. Derived from its classical counterpart, QCNN is prone to overfitting. Overfitting is a typical shortcoming of ML models that are trained too closely to the availed training dataset and perform relatively poorly on unseen datasets for a similar problem. In this work we study the adaptation of one of the most successful overfitting mitigation method, knows as the (post-training) dropout method, to the quantum setting. We find that a straightforward implementation of this method in the quantum setting leads to a significant and undesirable consequence: a substantial decrease in success probability of the QCNN. We argue that this effect exposes the crucial role of entanglement in QCNNs and the vulnerability of QCNNs to entanglement loss. To handle overfitting, we proposed a softer version of the dropout method. We find that the proposed method allows us to handle successfully overfitting in the test cases.
</details>
<details>
<summary>摘要</summary>
量子卷积神经网络（QCNN），在不完全Quantum Computer（NISQ）时代的早期应用，已经一直证明为机器学习（ML）算法的成功应用，对于多个任务具有显著的准确率。基于其类传统对应的Counterpart，QCNN受到过拟合的影响。过拟合是机器学习模型在训练数据集过于仔细地学习，对于未见数据集的类似问题表现较差的典型缺点。在这项工作中，我们研究了在量子设置中对一个最successful overfitting mitigation方法（post-training dropout method）的适应。我们发现，直接在量子设置中实现这种方法会导致重要的和不 DESirable consequence：量子神经网络成功率的显著减少。我们认为，这种效果暴露了量子神经网络中的束缚和量子神经网络对束缚的敏感性。为了处理过拟合，我们提议了一种软化的Dropout方法。我们发现，该方法可以成功地处理测试 caso中的过拟合。
</details></li>
</ul>
<hr>
<h2 id="Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation"><a href="#Secure-and-Efficient-Federated-Learning-in-LEO-Constellations-using-Decentralized-Key-Generation-and-On-Orbit-Model-Aggregation" class="headerlink" title="Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation"></a>Secure and Efficient Federated Learning in LEO Constellations using Decentralized Key Generation and On-Orbit Model Aggregation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01828">http://arxiv.org/abs/2309.01828</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamed Elmahallawy, Tie Luo, Mohamed I. Ibrahem</li>
<li>for: 这篇论文旨在解决在低地球轨道（LEO）上运行小卫星的资料汇集和训练人工智能模型（AI模型）时所遇到的问题。</li>
<li>methods: 本论文提出了一个名为FedSecure的安全 federated learning（FL）方法，包括两个新的 комponents：（1）对于每个卫星的隐私资料进行保护，使用功能加密方案，和（2）在轨道上进行模型传输和聚合，从而最小化陌生卫星进入可见区域的时间等待时间。</li>
<li>results: 我们的分析和结果显示，FedSecure可以保护每个卫星的资料隐私，并且具有较低的通信和计算负载，对比其他关于隐私保护的FL聚合方法。此外，FedSecure可以对抗听到者、侦错服务器或侦错卫星的攻击，并且可以快速地将模型训练完成，从天到地只需几个小时，并且可以达到高准确度的85.35%。<details>
<summary>Abstract</summary>
Satellite technologies have advanced drastically in recent years, leading to a heated interest in launching small satellites into low Earth orbit (LEOs) to collect massive data such as satellite imagery. Downloading these data to a ground station (GS) to perform centralized learning to build an AI model is not practical due to the limited and expensive bandwidth. Federated learning (FL) offers a potential solution but will incur a very large convergence delay due to the highly sporadic and irregular connectivity between LEO satellites and GS. In addition, there are significant security and privacy risks where eavesdroppers or curious servers/satellites may infer raw data from satellites' model parameters transmitted over insecure communication channels. To address these issues, this paper proposes FedSecure, a secure FL approach designed for LEO constellations, which consists of two novel components: (1) decentralized key generation that protects satellite data privacy using a functional encryption scheme, and (2) on-orbit model forwarding and aggregation that generates a partial global model per orbit to minimize the idle waiting time for invisible satellites to enter the visible zone of the GS. Our analysis and results show that FedSecure preserves the privacy of each satellite's data against eavesdroppers, a curious server, or curious satellites. It is lightweight with significantly lower communication and computation overheads than other privacy-preserving FL aggregation approaches. It also reduces convergence delay drastically from days to only a few hours, yet achieving high accuracy of up to 85.35% using realistic satellite images.
</details>
<details>
<summary>摘要</summary>
卫星技术在最近几年内发展了惊人的进步，导致了发射小卫星到低地球轨道（LEO）收集大量数据，如卫星图像。将这些数据下载到地面站（GS）以进行中央学习建立人工智能模型并不实际，因为卫星和GS之间的带宽有限且昂贵。 Federated learning（FL）提供了一个可能的解决方案，但它会导致非常大的融合延迟，因为LEO卫星和GS之间的连接是不规则和不稳定的。此外，在卫星图像的传输过程中，有可能有人 intercept或curious服务器/卫星探测到卫星的模型参数，这会导致数据泄露和隐私泄露。为解决这些问题，本文提出了FedSecure，一种安全的FL方法，设计为LEO卫星团组，它包括两个新的组成部分：1. 分布式钥匙生成，使用功能加密算法保护卫星数据隐私。2. 在轨道上进行模型转发和聚合，每次轨道上的卫星生成部分全球模型，以降低不可见的卫星进入可见区的等待时间。我们的分析和结果表明，FedSecure能够保护每个卫星的数据隐私，对于扰乱者、curious服务器或curious卫星来说。它的通信和计算开销较低，与其他隐私保护FL聚合方法相比，它也可以快速融合，只需几个小时，而不是天数。此外，FedSecure可以达到85.35%的准确率，使用实际的卫星图像。
</details></li>
</ul>
<hr>
<h2 id="LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning"><a href="#LoopTune-Optimizing-Tensor-Computations-with-Reinforcement-Learning" class="headerlink" title="LoopTune: Optimizing Tensor Computations with Reinforcement Learning"></a>LoopTune: Optimizing Tensor Computations with Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01825">http://arxiv.org/abs/2309.01825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dejan Grubisic, Bram Wasti, Chris Cummins, John Mellor-Crummey, Aleksandar Zlateski</li>
<li>for: 这篇论文旨在应对机器学习应用程序在新硬件上运行时，传统编译器无法提供性能，受欢迎的自动适应器很长的搜寻时间，并且专家优化的库会带来不可持续的成本。</li>
<li>methods: 为了解决这个问题，我们开发了LoopTune，一个基于深度遗传学习的编译器，用于优化深度学习模型中的tensor计算。LoopTune使用了Lightweight code generator LoopNest来进行硬件特定优化，并使用了一个新的图形基于表示和动作空间，将LoopNest加速了3.2倍。</li>
<li>results: 根据实验结果，LoopTune可以实时对LoopNest进行优化，实现了对Numpy手动优化的性能水平，并且与TVM、MetaSchedule和AutoTVM相比，LoopTune的代码速度为1.08倍、2.8倍和3.2倍。此外，LoopTune可以在秒钟内完成代码优化。<details>
<summary>Abstract</summary>
Advanced compiler technology is crucial for enabling machine learning applications to run on novel hardware, but traditional compilers fail to deliver performance, popular auto-tuners have long search times and expert-optimized libraries introduce unsustainable costs. To address this, we developed LoopTune, a deep reinforcement learning compiler that optimizes tensor computations in deep learning models for the CPU. LoopTune optimizes tensor traversal order while using the ultra-fast lightweight code generator LoopNest to perform hardware-specific optimizations. With a novel graph-based representation and action space, LoopTune speeds up LoopNest by 3.2x, generating an order of magnitude faster code than TVM, 2.8x faster than MetaSchedule, and 1.08x faster than AutoTVM, consistently performing at the level of the hand-tuned library Numpy. Moreover, LoopTune tunes code in order of seconds.
</details>
<details>
<summary>摘要</summary>
高级编译技术是深度学习应用的关键，但传统的编译器无法提供性能。受欢迎的自动调整工具有长时间搜索时间，专家优化库引入不可持续的成本。为解决这个问题，我们开发了LoopTune，一个基于深度学习的编译器，用于优化深度学习模型中的矩阵计算。LoopTune优化矩阵遍历顺序，使用精灵的Lightweight代码生成器LoopNest执行硬件特定优化。通过Graph基于表示和行动空间，LoopTune提高了LoopNest的速度，生成的代码比TVM快3.2倍，比MetaSchedule快2.8倍，比AutoTVM快1.08倍，一直保持与手动优化库Numpy的水平。此外，LoopTune在秒钟级别调整代码。
</details></li>
</ul>
<hr>
<h2 id="On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision"><a href="#On-the-fly-Deep-Neural-Network-Optimization-Control-for-Low-Power-Computer-Vision" class="headerlink" title="On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision"></a>On the fly Deep Neural Network Optimization Control for Low-Power Computer Vision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01824">http://arxiv.org/abs/2309.01824</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishmeet Kaur, Adwaita Janardhan Jadhav<br>for: 这篇论文是为了提高边缘设备上进行视觉处理时的效率和精度而写的。methods: 这篇论文使用了一种名为 AdaptiveActivation 的新技术，这个技术可以在运行时自动调整 DNN 的精度和能 consumption，不需要重新训练。results: 实验结果显示，使用 AdaptiveActivation 技术可以让 DNN 在不同的边缘环境中提高精度和效率，并且需要10%-38% less memory than baseline techniques。<details>
<summary>Abstract</summary>
Processing visual data on mobile devices has many applications, e.g., emergency response and tracking. State-of-the-art computer vision techniques rely on large Deep Neural Networks (DNNs) that are usually too power-hungry to be deployed on resource-constrained edge devices. Many techniques improve the efficiency of DNNs by using sparsity or quantization. However, the accuracy and efficiency of these techniques cannot be adapted for diverse edge applications with different hardware constraints and accuracy requirements. This paper presents a novel technique to allow DNNs to adapt their accuracy and energy consumption during run-time, without the need for any re-training. Our technique called AdaptiveActivation introduces a hyper-parameter that controls the output range of the DNNs' activation function to dynamically adjust the sparsity and precision in the DNN. AdaptiveActivation can be applied to any existing pre-trained DNN to improve their deployability in diverse edge environments. We conduct experiments on popular edge devices and show that the accuracy is within 1.5% of the baseline. We also show that our approach requires 10%--38% less memory than the baseline techniques leading to more accuracy-efficiency tradeoff options
</details>
<details>
<summary>摘要</summary>
处理移动设备上的视觉数据有很多应用，例如应急响应和跟踪。现状顶尖计算机视觉技术利用大深度神经网络（DNNs），但这些DNNs通常是资源受限的边缘设备上不可deploy。许多技术改进DNNs的效率，使用稀疏或量化。但这些技术的准确率和效率无法适应不同的硬件限制和准确要求。这篇论文介绍了一种新的技术，允许DNNs在运行时根据需要调整其准确率和能耗。我们的技术被称为AdaptiveActivation，它在DNNs的活化函数输出范围中引入了一个超参数，以动态调整DNNs的稀疏和精度。AdaptiveActivation可以应用于任何已经预训练的DNN，以提高其在多样化边缘环境中的部署性。我们在流行的边缘设备上进行了实验，并证明了与基准值相比，准确率在1.5%之间。我们还证明了我们的方法需要10%-38% less memory than基准技术，导致更多的准确率-效率质量Tradeoff。
</details></li>
</ul>
<hr>
<h2 id="Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension"><a href="#Towards-Foundational-AI-Models-for-Additive-Manufacturing-Language-Models-for-G-Code-Debugging-Manipulation-and-Comprehension" class="headerlink" title="Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension"></a>Towards Foundational AI Models for Additive Manufacturing: Language Models for G-Code Debugging, Manipulation, and Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02465">http://arxiv.org/abs/2309.02465</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idealab-isu/llm4g-code">https://github.com/idealab-isu/llm4g-code</a></li>
<li>paper_authors: Anushrut Jignasu, Kelly Marshall, Baskar Ganapathysubramanian, Aditya Balu, Chinmay Hegde, Adarsh Krishnamurthy</li>
<li>for: 这篇论文旨在评估现有的基础大语言模型（LLMs）是否能够理解和修复3D打印的G-code文件中的错误。</li>
<li>methods: 作者使用了六种当前领先的基础大语言模型（LLMs），并设计了有效的提示来使这些模型理解和操纵G-code文件。</li>
<li>results: 研究发现这些模型在检测和修复常见错误方面具有不同的优势和不足，并讨论了使用LLMs进行G-code理解的限制和局限性。<details>
<summary>Abstract</summary>
3D printing or additive manufacturing is a revolutionary technology that enables the creation of physical objects from digital models. However, the quality and accuracy of 3D printing depend on the correctness and efficiency of the G-code, a low-level numerical control programming language that instructs 3D printers how to move and extrude material. Debugging G-code is a challenging task that requires a syntactic and semantic understanding of the G-code format and the geometry of the part to be printed. In this paper, we present the first extensive evaluation of six state-of-the-art foundational large language models (LLMs) for comprehending and debugging G-code files for 3D printing. We design effective prompts to enable pre-trained LLMs to understand and manipulate G-code and test their performance on various aspects of G-code debugging and manipulation, including detection and correction of common errors and the ability to perform geometric transformations. We analyze their strengths and weaknesses for understanding complete G-code files. We also discuss the implications and limitations of using LLMs for G-code comprehension.
</details>
<details>
<summary>摘要</summary>
三次元印刷或添加制造技术是一种革命性的技术，允许将数字模型转化为实际物体。然而，3D印刷的质量和准确性取决于G-code的正确性和效率，G-code是一种低级数控程序语言，用于指示3D印刷机器人如何移动和挤出材料。调试G-code是一项复杂的任务，需要对G-code格式和数据的语法和 semantics有深入的理解，以及要印刷的部件的几何学结构。在这篇论文中，我们提出了第一次对六种当前最佳的基础大型自然语言模型（LLMs）进行了广泛的评估，以确定它们在理解和调试G-code文件方面的能力。我们设计了有效的提示，使得预训练的LLMs能够理解和操纵G-code，并测试了它们在不同方面的G-code调试和操纵方面的性能，包括检测和修复常见错误以及执行几何变换。我们分析了它们对完整G-code文件的理解的优劣点，以及使用LLMs进行G-code理解的局限性。
</details></li>
</ul>
<hr>
<h2 id="Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks"><a href="#Computation-and-Communication-Efficient-Federated-Learning-over-Wireless-Networks" class="headerlink" title="Computation and Communication Efficient Federated Learning over Wireless Networks"></a>Computation and Communication Efficient Federated Learning over Wireless Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01816">http://arxiv.org/abs/2309.01816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaonan Liu, Tharmalingam Ratnarajah</li>
<li>For: The paper aims to improve the efficiency and accuracy of federated learning (FL) on edge devices with limited computational capability and wireless resources.* Methods: The proposed FL framework uses partial model pruning and personalization to adapt the model size for each device, reducing both computation and communication overhead. The framework also includes a novel optimization problem to maximize the convergence rate under a latency threshold.* Results: The proposed FL framework achieves a remarkable reduction of approximately 50% in computation and communication latency compared to a scheme only with model personalization.Here is the text in Simplified Chinese:* For: 本 paper 的目的是提高边缘设备上的 Federated learning (FL) 的效率和准确率，这些设备具有有限的计算能力和无线资源。* Methods: 提议的 FL 框架使用 partial model pruning 和个性化，以适应每个设备的模型大小，从而降低计算和通信负担。框架还包括一个优化问题，以最大化在延迟限制下的整合率。* Results: 提议的 FL 框架可以 achieve 约 50% 的计算和通信负担减少，相比只有模型个性化的方案。<details>
<summary>Abstract</summary>
Federated learning (FL) allows model training from local data by edge devices while preserving data privacy. However, the learning accuracy decreases due to the heterogeneity of devices data, and the computation and communication latency increase when updating large scale learning models on devices with limited computational capability and wireless resources. To overcome these challenges, we consider a novel FL framework with partial model pruning and personalization. This framework splits the learning model into a global part with model pruning shared with all devices to learn data representations and a personalized part to be fine tuned for a specific device, which adapts the model size during FL to reduce both computation and communication overhead and minimize the overall training time, and increases the learning accuracy for the device with non independent and identically distributed (non IID) data. Then, the computation and communication latency and the convergence analysis of the proposed FL framework are mathematically analyzed. Based on the convergence analysis, an optimization problem is formulated to maximize the convergence rate under a latency threshold by jointly optimizing the pruning ratio and wireless resource allocation. By decoupling the optimization problem and deploying Karush Kuhn Tucker (KKT) conditions, we derive the closed form solutions of pruning ratio and wireless resource allocation. Finally, experimental results demonstrate that the proposed FL framework achieves a remarkable reduction of approximately 50 percents computation and communication latency compared with the scheme only with model personalization.
</details>
<details>
<summary>摘要</summary>
federated learning (FL) 允许本地数据进行模型训练，保持数据隐私。然而，由于设备数据的不同性，学习精度下降，并且更新大规模学习模型在设备上的计算和通信延迟增加。为了解决这些挑战，我们考虑了一种新的FL框架，包括部分模型剔除和个性化。这个框架将学习模型分为全球部分，用于学习数据表示，以及个性化部分，用于特定设备进行细化。这些部分可以在FL中适应不同设备的数据，从而降低计算和通信开销，最小化总训练时间，并提高设备上的学习精度。然后，我们对这个FL框架的计算和通信延迟和整体训练时间进行数学分析。基于这个分析，我们形ulated一个优化问题，以 maximize 训练速率在延迟阈值下。通过分离优化问题并应用KKT条件，我们得到了封装形式的解。最后，我们通过实验结果表明，提案的FL框架可以降低约50%的计算和通信延迟。
</details></li>
</ul>
<hr>
<h2 id="DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research"><a href="#DiscoverPath-A-Knowledge-Refinement-and-Retrieval-System-for-Interdisciplinarity-on-Biomedical-Research" class="headerlink" title="DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"></a>DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01808">http://arxiv.org/abs/2309.01808</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ynchuang/discoverpath">https://github.com/ynchuang/discoverpath</a></li>
<li>paper_authors: Yu-Neng Chuang, Guanchu Wang, Chia-Yuan Chang, Kwei-Herng Lai, Daochen Zha, Ruixiang Tang, Fan Yang, Alfredo Costilla Reyes, Kaixiong Zhou, Xiaoqian Jiang, Xia Hu</li>
<li>For: The paper aims to provide an efficient and intuitive search engine for biomedical research articles, particularly in interdisciplinary fields where diverse terminologies are used.* Methods: The system uses Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts and create a knowledge graph (KG). It also includes a query recommendation system to help users iteratively refine their searches.* Results: The system provides a focused subgraph containing the queried entity and its neighboring nodes, as well as an accessible Graphical User Interface (GUI) that visualizes the KG, query recommendations, and detailed article information, enabling efficient article retrieval and fostering interdisciplinary knowledge exploration.<details>
<summary>Abstract</summary>
The exponential growth in scholarly publications necessitates advanced tools for efficient article retrieval, especially in interdisciplinary fields where diverse terminologies are used to describe similar research. Traditional keyword-based search engines often fall short in assisting users who may not be familiar with specific terminologies. To address this, we present a knowledge graph-based paper search engine for biomedical research to enhance the user experience in discovering relevant queries and articles. The system, dubbed DiscoverPath, employs Named Entity Recognition (NER) and part-of-speech (POS) tagging to extract terminologies and relationships from article abstracts to create a KG. To reduce information overload, DiscoverPath presents users with a focused subgraph containing the queried entity and its neighboring nodes and incorporates a query recommendation system, enabling users to iteratively refine their queries. The system is equipped with an accessible Graphical User Interface that provides an intuitive visualization of the KG, query recommendations, and detailed article information, enabling efficient article retrieval, thus fostering interdisciplinary knowledge exploration. DiscoverPath is open-sourced at https://github.com/ynchuang/DiscoverPath.
</details>
<details>
<summary>摘要</summary>
随着学术论文的激增增长，需要更高级的工具来有效检索相关的论文，尤其在跨学科领域中，其中的不同术语可能用于描述类似的研究。传统的关键词基本搜索引擎经常无法帮助用户找到他们不熟悉的特定术语。为解决这个问题，我们提出了一种基于知识图的论文检索引擎，以提高用户在找到相关问题和论文的经验。该系统，名为DiscoverPath，利用命名实体识别（NER）和语言类型（POS）标记来从论文摘要中提取术语和关系，并将其转换为知识图。为了降低信息混乱，DiscoverPath将用户展示一个专注于查询实体和相邻节点的子图，并提供了查询建议系统，允许用户逐步缩小查询范围。系统具有访问ible的图形用户界面，提供了知识图的直观化、查询建议和详细的论文信息，以便有效检索论文，从而推动跨学科知识探索。DiscoverPath的源代码可以在https://github.com/ynchuang/DiscoverPath上下载。
</details></li>
</ul>
<hr>
<h2 id="Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation"><a href="#Marginalized-Importance-Sampling-for-Off-Environment-Policy-Evaluation" class="headerlink" title="Marginalized Importance Sampling for Off-Environment Policy Evaluation"></a>Marginalized Importance Sampling for Off-Environment Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01807">http://arxiv.org/abs/2309.01807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pulkit Katdare, Nan Jiang, Katherine Driggs-Campbell</li>
<li>for: 本研究旨在评估真实世界中RL策略的性能，而不需要真实世界的部署。</li>
<li>methods: 该方法利用了模拟器和现实世界的停挂数据来评估RL策略的性能，基于重要抽象采样（MIS）框架。</li>
<li>results: 该方法可以减少采样的复杂性和预测误差，并且在多种Sim2Sim环境和不同的目标策略下表现良好。此外，该方法还在一个Sim2Real任务中评估了一个7个自由度的机器人臂的性能。<details>
<summary>Abstract</summary>
Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation, requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies without deploying them in the real world. The proposed approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude, thus making it easier to run. We analyze the sample complexity as well as error propagation of our two step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim environments such as Cartpole, Reacher and Half-Cheetah. Our results show that our method generalizes well across a variety of Sim2Sim gap, target policies and offline data collection policies. We also demonstrate the performance of our algorithm on a Sim2Real task of validating the performance of a 7 DOF robotic arm using offline data along with a gazebo based arm simulator.
</details>
<details>
<summary>摘要</summary>
深度学习（RL）方法通常是样本不fficient，这使得在实际世界中训练和部署RL策略变得困难。即使是一个坚固的策略在模拟中训练，它仍需要在实际世界中进行评估。这篇论文提出了一种新的方法，用于在实际世界中评估代理策略的性能，无需在实际世界中部署策略。该方法利用模拟器和实际世界的停滞数据来评估任何策略，基于多样化重要性抽样（MIS）框架。现有的MIS方法面临两个挑战：（1）巨大的概率差，导致抽样难以控制，（2）间接监督，需要间接地推断抽样比率，从而增加估计误差。我们的方法解决了这两个挑战，通过引入目标策略在模拟器中的占用率作为中间变量，并将抽样比率分解为两个可分别学习的部分。第一部分可以通过直接监督学习，而第二部分的大小较小，因此更容易进行。我们也分析了我们的两步程序的样本复杂度以及误差卷积。此外，我们还通过实验评估我们的方法，在Cartpole、Reacher和Half-Cheetah等Sim2Sim环境中获得了良好的普适性和可靠性。此外，我们还在一个Sim2Real任务中验证了一个7度OF机械臂的性能，使用了卡特底抽象和加兹环境。我们的结果表明，我们的方法可以在不同的Sim2Sim差距、目标策略和在线数据收集策略之间进行广泛的普适性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization"><a href="#Asymmetric-matrix-sensing-by-gradient-descent-with-small-random-initialization" class="headerlink" title="Asymmetric matrix sensing by gradient descent with small random initialization"></a>Asymmetric matrix sensing by gradient descent with small random initialization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01796">http://arxiv.org/abs/2309.01796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Johan S. Wind</li>
<li>For:  Matrix sensing, a problem of reconstructing a low-rank matrix from a few linear measurements, is studied.* Methods:  The paper uses factorized gradient descent to solve the overparameterized regression problem of matrix sensing. The key contribution is introducing a continuous differential equation called the “perturbed gradient flow,” which converges quickly to the true target matrix when the perturbation is sufficiently bounded.* Results:  The paper proves that the perturbed gradient flow converges quickly to the true target matrix, and demonstrates the utility of the continuous formulation in simplifying the proofs of matrix sensing with factorized gradient descent.<details>
<summary>Abstract</summary>
We study matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements. It can be formulated as an overparameterized regression problem, which can be solved by factorized gradient descent when starting from a small random initialization.   Linear neural networks, and in particular matrix sensing by factorized gradient descent, serve as prototypical models of non-convex problems in modern machine learning, where complex phenomena can be disentangled and studied in detail. Much research has been devoted to studying special cases of asymmetric matrix sensing, such as asymmetric matrix factorization and symmetric positive semi-definite matrix sensing.   Our key contribution is introducing a continuous differential equation that we call the $\textit{perturbed gradient flow}$. We prove that the perturbed gradient flow converges quickly to the true target matrix whenever the perturbation is sufficiently bounded. The dynamics of gradient descent for matrix sensing can be reduced to this formulation, yielding a novel proof of asymmetric matrix sensing with factorized gradient descent. Compared to directly analyzing the dynamics of gradient descent, the continuous formulation allows bounding key quantities by considering their derivatives, often simplifying the proofs. We believe the general proof technique may prove useful in other settings as well.
</details>
<details>
<summary>摘要</summary>
我们研究矩阵感知问题，即从一些线性测量中重建一个低级矩阵的问题。可以表述为过Parameterized regression问题，可以通过分解 gradient descent 来解决，当从小随机 initialization 开始时。 Linear neural networks 和特别是矩阵感知通过分解 gradient descent serve as 现代机器学习中非 convex 问题的典型模型，其中复杂的现象可以详细分析和研究。许多研究都投入到了异symmetric matrix factorization 和Symmetric positive semi-definite matrix sensing 的特殊情况中。我们的关键贡献在于引入一个名为 $\textit{perturbed gradient flow}$ 的连续偏微分方程。我们证明这个方程在干扰足够小时，它快速收敛到真正的目标矩阵。gradient descent 的动态可以将 reduced to this formulation，得到了一个 novel proof of asymmetric matrix sensing with factorized gradient descent。相比直接分析 gradient descent 的动态，连续形式允许通过考虑其导数来简化证明，从而简化证明。我们认为这种总体技巧可能在其他设置中也有用。
</details></li>
</ul>
<hr>
<h2 id="Composite-federated-learning-with-heterogeneous-data"><a href="#Composite-federated-learning-with-heterogeneous-data" class="headerlink" title="Composite federated learning with heterogeneous data"></a>Composite federated learning with heterogeneous data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01795">http://arxiv.org/abs/2309.01795</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaojiao Zhang, Jiang Hu, Mikael Johansson</li>
<li>For: 解决复杂的 Federated Learning（FL）问题* Methods: 使用新的算法，并将 proximal 算符和通信分解，解决客户端漂移问题，每个工作者使用本地更新减少与服务器的通信频率，并只在每个通信循环中传输 $d$-维数组* Results: Proof 算法 linearly converges to a neighborhood of the optimal solution，并在数学实验中证明了对现有方法的超越性<details>
<summary>Abstract</summary>
We propose a novel algorithm for solving the composite Federated Learning (FL) problem. This algorithm manages non-smooth regularization by strategically decoupling the proximal operator and communication, and addresses client drift without any assumptions about data similarity. Moreover, each worker uses local updates to reduce the communication frequency with the server and transmits only a $d$-dimensional vector per communication round. We prove that our algorithm converges linearly to a neighborhood of the optimal solution and demonstrate the superiority of our algorithm over state-of-the-art methods in numerical experiments.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的Algorithm，用于解决复合式 Federated Learning（FL）问题。这个算法利用推导操作和通信分离，处理非滑动正则化，而无需数据相似性假设。此外，每个工作者使用本地更新来减少与服务器之间的通信频率，并仅在通信轮次中传输一个 $d$-维向量。我们证明了我们的算法会线性征向优解的解，并在实验中证明了我们的算法在比 estado-of-the-art 方法更高效。
</details></li>
</ul>
<hr>
<h2 id="Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction"><a href="#Hierarchical-Grammar-Induced-Geometry-for-Data-Efficient-Molecular-Property-Prediction" class="headerlink" title="Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction"></a>Hierarchical Grammar-Induced Geometry for Data-Efficient Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01788">http://arxiv.org/abs/2309.01788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gmh14/Geo-DEG">https://github.com/gmh14/Geo-DEG</a></li>
<li>paper_authors: Minghao Guo, Veronika Thost, Samuel W Song, Adithya Balachandran, Payel Das, Jie Chen, Wojciech Matusik</li>
<li>for: 预测分子性质，以便在物质和药物探索领域中进行更好的预测和设计。</li>
<li>methods: 使用深度学习技术，利用可学习的层次分子agram来生成分子结构，并使用图内傅里勒扩散来预测分子性质。</li>
<li>results: 在小和大数据集上，该方法比一系列基线方法（包括supervised和预训练图内傅里勒网络）表现出色，并且在具有极限数据情况下也能够达到良好的结果。<details>
<summary>Abstract</summary>
The prediction of molecular properties is a crucial task in the field of material and drug discovery. The potential benefits of using deep learning techniques are reflected in the wealth of recent literature. Still, these techniques are faced with a common challenge in practice: Labeled data are limited by the cost of manual extraction from literature and laborious experimentation. In this work, we propose a data-efficient property predictor by utilizing a learnable hierarchical molecular grammar that can generate molecules from grammar production rules. Such a grammar induces an explicit geometry of the space of molecular graphs, which provides an informative prior on molecular structural similarity. The property prediction is performed using graph neural diffusion over the grammar-induced geometry. On both small and large datasets, our evaluation shows that this approach outperforms a wide spectrum of baselines, including supervised and pre-trained graph neural networks. We include a detailed ablation study and further analysis of our solution, showing its effectiveness in cases with extremely limited data. Code is available at https://github.com/gmh14/Geo-DEG.
</details>
<details>
<summary>摘要</summary>
“物料和药物发现领域中预测分子性质的任务是非常重要的。Recent literature中的大量文献表明，深度学习技术在这个领域中的潜在利益很大。然而，在实践中，这些技术面临一个共同的挑战：标注数据受到文献中手动EXTRACTION的成本和劳动密集的实验室试验的限制。在这种情况下，我们提出了一种数据效率的属性预测器，利用可学习的层次分子语法生成分子结构。这种语法induces一个explicit的分子图 geometry，该geometry提供了一个有用的分子结构相似性的准确信息。我们使用图神经扩散来预测属性，并在小型和大型数据集上评估了我们的方法。结果显示，我们的方法在许多基elines上比超越，包括超级vised和预训练图神经网络。我们还提供了细化的折衔分析和进一步的分析，证明我们的解决方案在有限数据情况下的有效性。代码可以在https://github.com/gmh14/Geo-DEG中找到。”Note: The translation is in Simplified Chinese, which is one of the two standard versions of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="ATMS-Algorithmic-Trading-Guided-Market-Simulation"><a href="#ATMS-Algorithmic-Trading-Guided-Market-Simulation" class="headerlink" title="ATMS: Algorithmic Trading-Guided Market Simulation"></a>ATMS: Algorithmic Trading-Guided Market Simulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01784">http://arxiv.org/abs/2309.01784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Song Wei, Andrea Coletta, Svitlana Vyetrenko, Tucker Balch</li>
<li>for: 提出了一种metric来衡量市场差异，以便在算法交易（AT）策略的构建中使用市场模拟器。</li>
<li>methods: 提出了一种基于强化学习（RL）的Algorithmic Trading-guided Market Simulation（ATMS）方法，使用策略梯度更新来缓解非导数操作。</li>
<li>results: 通过对半实际市场数据进行广泛的实验，证明了提出的metric的有效性，并示出ATMS生成的市场数据与现实更加相似，并且生成的市场数据中的BUY和SELL量更加均衡。<details>
<summary>Abstract</summary>
The effective construction of an Algorithmic Trading (AT) strategy often relies on market simulators, which remains challenging due to existing methods' inability to adapt to the sequential and dynamic nature of trading activities. This work fills this gap by proposing a metric to quantify market discrepancy. This metric measures the difference between a causal effect from underlying market unique characteristics and it is evaluated through the interaction between the AT agent and the market. Most importantly, we introduce Algorithmic Trading-guided Market Simulation (ATMS) by optimizing our proposed metric. Inspired by SeqGAN, ATMS formulates the simulator as a stochastic policy in reinforcement learning (RL) to account for the sequential nature of trading. Moreover, ATMS utilizes the policy gradient update to bypass differentiating the proposed metric, which involves non-differentiable operations such as order deletion from the market. Through extensive experiments on semi-real market data, we demonstrate the effectiveness of our metric and show that ATMS generates market data with improved similarity to reality compared to the state-of-the-art conditional Wasserstein Generative Adversarial Network (cWGAN) approach. Furthermore, ATMS produces market data with more balanced BUY and SELL volumes, mitigating the bias of the cWGAN baseline approach, where a simple strategy can exploit the BUY/SELL imbalance for profit.
</details>
<details>
<summary>摘要</summary>
“algorithmic trading（AT）策略的有效建立frequently rely on market simulators，但现有方法困难寻味到贸易活动的顺序和动态性。这种工作填补了这一空白，提出了一个市场偏差度量。这个度量测量了 causal effect的差异，由AT agent与市场的交互来评估。我们还提出了基于RL的 Algorithmic Trading-guided Market Simulation（ATMS）。受SeqGAN的启发，ATMS将 simulator形式化为一个随机政策，以考虑贸易活动的顺序性。此外，ATMS使用策略梯度更新，以 circumvent differentiating the proposed metric，该度量包括非分diff运算，如市场中的订单删除。经过对半真实市场数据的广泛实验，我们证明了我们的度量的有效性，并表明ATMS生成的市场数据与现有的conditional Wasserstein Generative Adversarial Network（cWGAN）方法相比，具有更高的真实性。此外，ATMS生成的市场数据具有更好的BUY和SELL量均衡，这可以 mitigate cWGAN基线方法的偏袋，其中一个简单的策略可以通过BUY/SELL偏袋来获利。”
</details></li>
</ul>
<hr>
<h2 id="Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers"><a href="#Survival-Prediction-from-Imbalance-colorectal-cancer-dataset-using-hybrid-sampling-methods-and-tree-based-classifiers" class="headerlink" title="Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers"></a>Survival Prediction from Imbalance colorectal cancer dataset using hybrid sampling methods and tree-based classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01783">http://arxiv.org/abs/2309.01783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sadegh Soleimani, Mahsa Bahrami, Mansour Vali<br>for: 这篇研究的目的是为了预测抗癌化学疗法后的肝癌patient的1、3、5年生存率，并且特别针对高度不均衡的1年生存预测任务进行研究。methods: 本研究使用了一些标准的平衡技术来增加True Positive率，包括Edited Nearest Neighbor、Repeated edited nearest neighbor (RENN)、Synthetic Minority Over-sampling Techniques (SMOTE) 等方法，并且将其与树型分类器结合使用。results: 本研究使用了5-fold Cross-Validation方法进行性能评估，结果显示，在高度不均衡的1年生存预测任务中，我们的提案方法（使用Light Gradient Boosting）具有72.30%的sensitivity，而且在3年生存预测任务中，使用RENN和Light Gradient Boosting的结合方法具有80.81%的sensitivity，显示了我们的提案方法在高度不均衡的数据上具有优秀的预测性。<details>
<summary>Abstract</summary>
Background and Objective: Colorectal cancer is a high mortality cancer. Clinical data analysis plays a crucial role in predicting the survival of colorectal cancer patients, enabling clinicians to make informed treatment decisions. However, utilizing clinical data can be challenging, especially when dealing with imbalanced outcomes. This paper focuses on developing algorithms to predict 1-, 3-, and 5-year survival of colorectal cancer patients using clinical datasets, with particular emphasis on the highly imbalanced 1-year survival prediction task. To address this issue, we propose a method that creates a pipeline of some of standard balancing techniques to increase the true positive rate. Evaluation is conducted on a colorectal cancer dataset from the SEER database. Methods: The pre-processing step consists of removing records with missing values and merging categories. The minority class of 1-year and 3-year survival tasks consists of 10% and 20% of the data, respectively. Edited Nearest Neighbor, Repeated edited nearest neighbor (RENN), Synthetic Minority Over-sampling Techniques (SMOTE), and pipelines of SMOTE and RENN approaches were used and compared for balancing the data with tree-based classifiers. Decision Trees, Random Forest, Extra Tree, eXtreme Gradient Boosting, and Light Gradient Boosting (LGBM) are used in this article. Method. Results: The performance evaluation utilizes a 5-fold cross-validation approach. In the case of highly imbalanced datasets (1-year), our proposed method with LGBM outperforms other sampling methods with the sensitivity of 72.30%. For the task of imbalance (3-year survival), the combination of RENN and LGBM achieves a sensitivity of 80.81%, indicating that our proposed method works best for highly imbalanced datasets. Conclusions: Our proposed method significantly improves mortality prediction for the minority class of colorectal cancer patients.
</details>
<details>
<summary>摘要</summary>
背景和目标：肠RECTAL癌是高mortality癌症，临床数据分析对于预测肠RECTAL癌患者的存活率起着关键性的作用，帮助临床医生作出有知识的治疗决策。然而，利用临床数据可能会困难，特别是面临着严重的存在偏好问题。这篇论文关注于使用临床数据来预测肠RECTAL癌患者1-, 3-, 5-年存活率，特别是面临着高度偏好的1-年存活预测任务。为解决这一问题，我们提出了一种方法，该方法包括一系列标准平衡技术，以增加真正正确率。我们在SEER数据库中的肠RECTAL癌数据集上进行评价。方法：数据预处理步骤包括移除缺失值和合并类别。肠RECTAL癌1-年和3-年存活任务的少数类刚占数据集中的10%和20%，分别。我们使用Edited Nearest Neighbor（ENN）、Repeated edited nearest neighbor（RENN）、Synthetic Minority Over-sampling Techniques（SMOTE）以及这些技术的管道来平衡数据，并与树型分类器进行比较。我们使用Decision Trees、Random Forest、Extra Tree、eXtreme Gradient Boosting和Light Gradient Boosting（LGBM）这些分类器。结果：我们使用5-fold Cross-validation方法进行性能评价。在面临着高度偏好的1-年存活任务时，我们提出的方法与LGBM结合的性能最高，即72.30%的sensitivity。对于3-年存活任务，我们的方法的敏感性达到80.81%，表明我们的方法在高度偏好的数据集上工作最好。结论：我们的方法显著提高了肠RECTAL癌患者少数类的存活预测率。
</details></li>
</ul>
<hr>
<h2 id="3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream"><a href="#3D-View-Prediction-Models-of-the-Dorsal-Visual-Stream" class="headerlink" title="3D View Prediction Models of the Dorsal Visual Stream"></a>3D View Prediction Models of the Dorsal Visual Stream</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01782">http://arxiv.org/abs/2309.01782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriel Sarch, Hsiao-Yu Fish Tung, Aria Wang, Jacob Prince, Michael Tarr</li>
<li>for: 这个论文是为了检验一种基于3D场景特征记忆的自适应循环神经网络（GRNN）是否可以更好地与脑动力学活动在背部视觉流程中相匹配。</li>
<li>methods: 这个论文使用了自适应基线模型，并将其训练在一个3D特征记忆中，以预测新的摄像头视图。</li>
<li>results: 研究发现，GRNN可以更好地预测背部视觉区域的神经响应，而基线模型则更好地预测前部视觉区域的神经响应。这些结果表明了使用任务相关的模型可以探索视觉流程中的表征差异。<details>
<summary>Abstract</summary>
Deep neural network representations align well with brain activity in the ventral visual stream. However, the primate visual system has a distinct dorsal processing stream with different functional properties. To test if a model trained to perceive 3D scene geometry aligns better with neural responses in dorsal visual areas, we trained a self-supervised geometry-aware recurrent neural network (GRNN) to predict novel camera views using a 3D feature memory. We compared GRNN to self-supervised baseline models that have been shown to align well with ventral regions using the large-scale fMRI Natural Scenes Dataset (NSD). We found that while the baseline models accounted better for ventral brain regions, GRNN accounted for a greater proportion of variance in dorsal brain regions. Our findings demonstrate the potential for using task-relevant models to probe representational differences across visual streams.
</details>
<details>
<summary>摘要</summary>
深度神经网络表示与大脑活动在脊梁视觉流程中吻合得非常好。然而，猴子视觉系统有一个不同的脊梁处理流程，具有不同的功能性。为了测试一个模型可以更好地捕捉3D场景几何结构，我们训练了一个自我超vised geometry-aware循环神经网络（GRNN），用于预测新的摄像头视图。我们比较了GRNN与已知对于脊梁区域具有良好适应性的自我超vised基线模型。我们发现，虽然基线模型更好地捕捉了脊梁区域的大脑活动，但GRNN更好地捕捉了脊梁区域的大脑活动的较大的差异。我们的发现表明，可以使用任务相关的模型来探索不同的视觉流程之间的表达差异。
</details></li>
</ul>
<hr>
<h2 id="Self-concordant-Smoothing-for-Convex-Composite-Optimization"><a href="#Self-concordant-Smoothing-for-Convex-Composite-Optimization" class="headerlink" title="Self-concordant Smoothing for Convex Composite Optimization"></a>Self-concordant Smoothing for Convex Composite Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01781">http://arxiv.org/abs/2309.01781</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl">https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl</a></li>
<li>paper_authors: Adeyemi D. Adeoye, Alberto Bemporad</li>
<li>for: 这篇论文是为了研究自适应缓和的减小方法，用于最小化两个凸函数，其中一个是光滑的，另一个可能是不光滑的。</li>
<li>methods: 该方法基于 partial smoothing 技术，只是将一部分不光滑函数缓和了。这个方法的关键优点在于它提供了一种自然的变量 метри选择法和步长选择规则，特别适合 proximal Newton 类算法。此外，它可以高效地处理一些由不光滑函数带来的特殊结构，如 $\ell_1$ 规范和 group-lasso 罚。</li>
<li>results: 作者证明了两种结果算法的本地二次几何速度，即 Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。而 Prox-GGN-SCORE 算法则描述了一种重要的近似过程，它可以减少大部分的计算负担，特别是在过参数机器学习模型和 mini-batch 设置下。实验表明，该方法的效率高于现有方法。<details>
<summary>Abstract</summary>
We introduce the notion of self-concordant smoothing for minimizing the sum of two convex functions: the first is smooth and the second may be nonsmooth. Our framework results naturally from the smoothing approximation technique referred to as partial smoothing in which only a part of the nonsmooth function is smoothed. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove local quadratic convergence rates for two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton (GGN) algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful for overparameterized machine learning models and in the mini-batch settings. Numerical examples on both synthetic and real datasets demonstrate the efficiency of our approach and its superiority over existing approaches.
</details>
<details>
<summary>摘要</summary>
我们介绍自步调和缓和运算的概念，用于最小化两个凸函数：第一个是平滑的，第二个可能是不凸的。我们的框架从 partial smoothing 技术获得，仅将部分不凸函数缓和。我们的方法的关键特点在于它具有自然的变数度量选择方法和步长选择规则，特别适合 proximal Newton 类型的算法。此外，我们可以有效地处理特定的非凸函数结构，例如 $\ell_1$ 正则化和群lasso 罚则。我们证明了两个结果的本地二阶径凹降率：Prox-N-SCORE 算法和 Prox-GGN-SCORE 算法。Prox-GGN-SCORE 算法显示了一个重要的近似程序，帮助将大部分的计算负担与 inverse Hessian 相关联系。这个近似是对过参量机器学习模型和小批量设定中具有重要的应用。实际例子表明了我们的方法的效率和其优于现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments"><a href="#Measuring-Interpreting-and-Improving-Fairness-of-Algorithms-using-Causal-Inference-and-Randomized-Experiments" class="headerlink" title="Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments"></a>Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01780">http://arxiv.org/abs/2309.01780</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Enouen, Tianshu Sun, Yan Liu</li>
<li>for: 这篇研究旨在提供一个易于实现的算法公平度量度框架，以便在实际应用中评估和改善算法的公平性。</li>
<li>methods: 这篇研究使用了现代的 causal inference 和可读性 Machine Learning 技术，开发了一个测试算法偏见的数据验证方法，并使用了这些技术来开发了一个可解释的机器学习模型，以便将算法的偏见评估为可读性。</li>
<li>results: 这篇研究的结果显示，使用这个数据验证方法和可解释的机器学习模型，可以实现在实际应用中评估和改善算法的公平性，并且可以评估算法的公平性在实际应用中的成本。<details>
<summary>Abstract</summary>
Algorithm fairness has become a central problem for the broad adoption of artificial intelligence. Although the past decade has witnessed an explosion of excellent work studying algorithm biases, achieving fairness in real-world AI production systems has remained a challenging task. Most existing works fail to excel in practical applications since either they have conflicting measurement techniques and/ or heavy assumptions, or require code-access of the production models, whereas real systems demand an easy-to-implement measurement framework and a systematic way to correct the detected sources of bias.   In this paper, we leverage recent advances in causal inference and interpretable machine learning to present an algorithm-agnostic framework (MIIF) to Measure, Interpret, and Improve the Fairness of an algorithmic decision. We measure the algorithm bias using randomized experiments, which enables the simultaneous measurement of disparate treatment, disparate impact, and economic value. Furthermore, using modern interpretability techniques, we develop an explainable machine learning model which accurately interprets and distills the beliefs of a blackbox algorithm. Altogether, these techniques create a simple and powerful toolset for studying algorithm fairness, especially for understanding the cost of fairness in practical applications like e-commerce and targeted advertising, where industry A/B testing is already abundant.
</details>
<details>
<summary>摘要</summary>
“算法公平性已成为人工智能普及的中心问题。过去十年里，我们所有的工作都集中在研究算法偏见上，但在实际应用中实现公平性仍然是一个挑战。现有的大多数工作都具有不兼容的测量技术和假设，或者需要生产模型的代码访问，而实际应用需要一个简单易用的测量框架和一系列系统化的偏见纠正方法。在这篇论文中，我们利用最新的 causal inference 和可读性机器学习来提出一个算法不依赖的框架（MIIF），用于测量、解释和改进算法决策中的公平性。我们使用随机实验测量算法偏见，这些测量可同时测量不同待遇、不同影响和经济价值。此外，我们使用现代可读性技术开发了一个可解释的机器学习模型，可以准确地解释和总结黑obox算法的信念。总之，这些技术创造了一个简单强大的工具集，特别适用于实际应用中的电商和targeted广告等， где行业A/B测试严重。”
</details></li>
</ul>
<hr>
<h2 id="DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data"><a href="#DRAG-Divergence-based-Adaptive-Aggregation-in-Federated-learning-on-Non-IID-Data" class="headerlink" title="DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data"></a>DRAG: Divergence-based Adaptive Aggregation in Federated learning on Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01779">http://arxiv.org/abs/2309.01779</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Zhu, Jingjing Zhang, Shengyun Liu, Xin Wang</li>
<li>for: 这篇论文的目的是解决 Federated Learning (FL) 中存在各个工作者的不同数据分布导致每个工作者更新本地模型而导致“客户端漂移”现象，使得训练速度减慢。</li>
<li>methods: 这篇论文提出了一个名为“振幅基于的自适应组合”（DRAG）算法，利用一个称为“振幅差”的度量来动态地“拖”每个当地更新向参考方向，不需要额外的通信开销。</li>
<li>results: 这篇论文的实验结果显示，DRAG 比前一代算法更有效地控制客户端漂移现象，并且具有较低的训练速度。此外，DRAG 也具有对某些拜尼攻击的抗性。<details>
<summary>Abstract</summary>
Local stochastic gradient descent (SGD) is a fundamental approach in achieving communication efficiency in Federated Learning (FL) by allowing individual workers to perform local updates. However, the presence of heterogeneous data distributions across working nodes causes each worker to update its local model towards a local optimum, leading to the phenomenon known as ``client-drift" and resulting in slowed convergence. To address this issue, previous works have explored methods that either introduce communication overhead or suffer from unsteady performance. In this work, we introduce a novel metric called ``degree of divergence," quantifying the angle between the local gradient and the global reference direction. Leveraging this metric, we propose the divergence-based adaptive aggregation (DRAG) algorithm, which dynamically ``drags" the received local updates toward the reference direction in each round without requiring extra communication overhead. Furthermore, we establish a rigorous convergence analysis for DRAG, proving its ability to achieve a sublinear convergence rate. Compelling experimental results are presented to illustrate DRAG's superior performance compared to state-of-the-art algorithms in effectively managing the client-drift phenomenon. Additionally, DRAG exhibits remarkable resilience against certain Byzantine attacks. By securely sharing a small sample of the client's data with the FL server, DRAG effectively counters these attacks, as demonstrated through comprehensive experiments.
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的度量指标，称为“分布度”，用于量化本地梯度和全局参考方向之间的夹角。基于这个指标，我们提出了一种动态调整收集的算法，称为分布度基于的整合（DRAG）算法，不需要额外的通信开销。此外，我们还提供了一种准确的收敛分析，证明DRAG可以实现下线收敛率。实验结果表明，DRAG在管理客户端漂移现象方面表现出色，并且具有remarkable的抗拒迟攻击能力。Here is a word-for-word translation of the text into Simplified Chinese:本文提出了一种新的度量指标，称为“分布度”，用于量化本地梯度和全局参考方向之间的夹角。基于这个指标，我们提出了一种动态调整收集的算法，称为分布度基于的整合（DRAG）算法，不需要额外的通信开销。此外，我们还提供了一种准确的收敛分析，证明DRAG可以实现下线收敛率。实验结果表明，DRAG在管理客户端漂移现象方面表现出色，并且具有remarkable的抗拒迟攻击能力。
</details></li>
</ul>
<hr>
<h2 id="CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence"><a href="#CONFIDERAI-a-novel-CONFormal-Interpretable-by-Design-score-function-for-Explainable-and-Reliable-Artificial-Intelligence" class="headerlink" title="CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence"></a>CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01778">http://arxiv.org/abs/2309.01778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Carlevaro, Sara Narteni, Fabrizio Dabbene, Marco Muselli, Maurizio Mongelli</li>
<li>for: 这篇论文的目的是提出一种方法来链接确定性预测与可解释机器学习，以确保人工智能系统的可靠性和信任性。</li>
<li>methods: 这篇论文提出了一种方法，即CONFIDERAI，它是一种基于规则的分类模型的新的分数函数，可以利用规则的预测能力和点在规则boundaries的几何位置来进行可解释性预测。此外，论文还使用了技术来控制特征空间中的非确形样本数量，以确保遵循规则的预测结果。</li>
<li>results: 论文通过测试实验在 benchmark 和实际数据集上达到了承诺的结果，例如 DNS 隧道检测和冠军疾病预测。<details>
<summary>Abstract</summary>
Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we propose a methodology to link conformal prediction with explainable machine learning by defining CONFIDERAI, a new score function for rule-based models that leverages both rules predictive ability and points geometrical position within rules boundaries. We also address the problem of defining regions in the feature space where conformal guarantees are satisfied by exploiting techniques to control the number of non-conformal samples in conformal regions based on support vector data description (SVDD). The overall methodology is tested with promising results on benchmark and real datasets, such as DNS tunneling detection or cardiovascular disease prediction.
</details>
<details>
<summary>摘要</summary>
日常生活中越来越多地受到人工智能的影响，而机器学习算法必须设计为所有人都可靠和信任worthy。特别是计算机科学家认为一个人工智能系统安全和可靠的 Five Pillars：解释性、稳定性、透明度、公平性和隐私。此外，我们还提出了一个第六个基本方面：遵循性，即机器学习人员对系统的预期行为的概率 ensure。在这篇论文中，我们提出了将CONFIDERAI作为新的分数函数，用于规则型模型，该函数利用规则预测的能力和规则边界上的点的几何位置。我们还解决了在特征空间中定义遵循保证的区域的问题，通过控制特征空间中非遵循样本的数量来基于支持向量数据描述（SVDD）。总的来说，我们的方法在评价数据集和实际应用中表现良好。
</details></li>
</ul>
<hr>
<h2 id="Gated-recurrent-neural-networks-discover-attention"><a href="#Gated-recurrent-neural-networks-discover-attention" class="headerlink" title="Gated recurrent neural networks discover attention"></a>Gated recurrent neural networks discover attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01775">http://arxiv.org/abs/2309.01775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes von Oswald, Maxime Larcher, Angelika Steger, João Sacramento</li>
<li>for: 这种论文旨在探讨现代RNN的发展，以及它们是如何实现自注意的。</li>
<li>methods: 这种RNN使用了两种主要设计元素：线性循环层和循环路径与多项式抑制。</li>
<li>results: 研究发现，通过反编程已经训练过的RNN，gradient descent实际上找到了这种结构。具体来说，研究发现RNN在解决简单的上下文学习任务上表现出与Transformers相同的注意力机制。这些结果表明了多项式相互作用在神经网络中的重要性，以及某些RNN可能在实际中不知不觉地实现注意力。<details>
<summary>Abstract</summary>
Recent architectural developments have enabled recurrent neural networks (RNNs) to reach and even surpass the performance of Transformers on certain sequence modeling tasks. These modern RNNs feature a prominent design pattern: linear recurrent layers interconnected by feedforward paths with multiplicative gating. Here, we show how RNNs equipped with these two design elements can exactly implement (linear) self-attention, the main building block of Transformers. By reverse-engineering a set of trained RNNs, we find that gradient descent in practice discovers our construction. In particular, we examine RNNs trained to solve simple in-context learning tasks on which Transformers are known to excel and find that gradient descent instills in our RNNs the same attention-based in-context learning algorithm used by Transformers. Our findings highlight the importance of multiplicative interactions in neural networks and suggest that certain RNNs might be unexpectedly implementing attention under the hood.
</details>
<details>
<summary>摘要</summary>
现代建筑设计已经使得回传神经网络（RNN）在某些序列处理任务上超越了Transformers的表现。这些现代RNN具有一个突出的设计模式：线性循环层通过调用循环路径和多元化闸道相互连接。我们示出了这些RNN具有这两个设计元素可以实现（线性）自我注意，trasformer中的主要建筑块。通过逆引擎一些训练好的RNN，我们发现了 Gradient Descent 在实践中发现了我们的建筑。具体来说，我们对 Transformers 训练好的简单内容学习任务进行了研究，发现了 Gradient Descent 在 RNN 中实现了相同的注意力基本块，与 Transformers 中的注意力运算相同。我们的发现显示了神经网络中的乘法互动的重要性，并表明某些RNN可能在实际中隐藏式地实现注意力。
</details></li>
</ul>
<hr>
<h2 id="ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation"><a href="#ADC-DAC-Free-Analog-Acceleration-of-Deep-Neural-Networks-with-Frequency-Transformation" class="headerlink" title="ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation"></a>ADC&#x2F;DAC-Free Analog Acceleration of Deep Neural Networks with Frequency Transformation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01771">http://arxiv.org/abs/2309.01771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nastaran Darabi, Maeesha Binte Hashem, Hongyi Pan, Ahmet Cetin, Wilfred Gomes, Amit Ranjan Trivedi</li>
<li>for: 这篇论文旨在提出一种能效的频域运算深度神经网络（DNN）加速方法，以减少延迟和能耗。</li>
<li>methods: 本文提出了一种基于频域运算的神经网络加速方法，使用了频域运算的应用频率对称（WHT）。但是，这种方法对于运算时的 multiply-accumulate（MAC）操作需求增加，从而导致效率下降。本文提出了一种新的类比方法，利用类比频域运算来实现更高效的神经网络加速。</li>
<li>results: 根据论文的数据，这种新方法可以在16×16的交叉板上实现8位数据处理的能效运算，能效率为1602 TOPS&#x2F;W（未使用早期终止策略）和5311 TOPS&#x2F;W（使用早期终止策略，VDD &#x3D; 0.8 V）。<details>
<summary>Abstract</summary>
The edge processing of deep neural networks (DNNs) is becoming increasingly important due to its ability to extract valuable information directly at the data source to minimize latency and energy consumption. Frequency-domain model compression, such as with the Walsh-Hadamard transform (WHT), has been identified as an efficient alternative. However, the benefits of frequency-domain processing are often offset by the increased multiply-accumulate (MAC) operations required. This paper proposes a novel approach to an energy-efficient acceleration of frequency-domain neural networks by utilizing analog-domain frequency-based tensor transformations. Our approach offers unique opportunities to enhance computational efficiency, resulting in several high-level advantages, including array micro-architecture with parallelism, ADC/DAC-free analog computations, and increased output sparsity. Our approach achieves more compact cells by eliminating the need for trainable parameters in the transformation matrix. Moreover, our novel array micro-architecture enables adaptive stitching of cells column-wise and row-wise, thereby facilitating perfect parallelism in computations. Additionally, our scheme enables ADC/DAC-free computations by training against highly quantized matrix-vector products, leveraging the parameter-free nature of matrix multiplications. Another crucial aspect of our design is its ability to handle signed-bit processing for frequency-based transformations. This leads to increased output sparsity and reduced digitization workload. On a 16$\times$16 crossbars, for 8-bit input processing, the proposed approach achieves the energy efficiency of 1602 tera operations per second per Watt (TOPS/W) without early termination strategy and 5311 TOPS/W with early termination strategy at VDD = 0.8 V.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）的边处理在日益重要，因为它可以直接从数据源提取有价值信息，以降低延迟和能耗。频域模型压缩，如沃尔什-哈达姆变换（WHT），已被证明为有效的备选方案。然而，频域处理的好处经常被 multiply-accumulate（MAC）操作所抵消。这篇论文提出了一种新的能效加速频域神经网络的方法，利用频域频率基于的tensor变换。我们的方法具有增强计算效率的多种高级优势，包括数组微架构、并行计算、ADC/DAC无需计算、以及增加输出稀热性。我们的方法可以消除转换矩阵中的可训练参数，从而实现更加紧凑的细胞。此外，我们的新数组微架构允许列 wise和行 wise的自适应封装，从而实现了完美的并行计算。此外，我们的方法可以免除ADC/DAC计算，通过对高度量化矩阵-向量乘法进行训练，利用矩阵乘法的参数自由性。另外，我们的方法还可以处理有符号位处理，从而增加输出稀热性和减少数字化工作负荷。在0.8V的电压下，使用16x16十字架，8位输入处理，我们的方法可以达到1602 tera操作每秒每瓦特（TOPS/W）的能效率，没有早期终止策略，以及5311 TOPS/W的能效率，使用早期终止策略。
</details></li>
</ul>
<hr>
<h2 id="On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation"><a href="#On-Penalty-Methods-for-Nonconvex-Bilevel-Optimization-and-First-Order-Stochastic-Approximation" class="headerlink" title="On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation"></a>On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01753">http://arxiv.org/abs/2309.01753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jeongyeol Kwon, Dohyun Kwon, Steve Wright, Robert Nowak</li>
<li>for: 本文研究了一种基于 penalty 方法的first-order算法，用于解决具有缓冲函数和可能非凸的 BO 问题。</li>
<li>methods: 本文使用 penalty 函数将 BO 问题转化为一个等价的优化问题，然后使用 first-order 算法来解决这个优化问题。</li>
<li>results: 本文提出了一种 $O(\epsilon)$-stationary点的算法，可以在 $O(\epsilon^{-3})$ 和 $O(\epsilon^{-7})$ 的时间复杂度下实现。此外，当 oracle 是 deterministic 时，这种算法可以在单loop 模式下实现，即每个迭代只需要 $O(1)$ 个样本。<details>
<summary>Abstract</summary>
In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\epsilon$-stationary solution by optimizing the penalty formulation with $\sigma = O(\epsilon)$. When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound (EB) condition, we propose a first-order algorithm that converges to an $\epsilon$-stationary point of the penalty function, using in total $O(\epsilon^{-3})$ and $O(\epsilon^{-7})$ accesses to first-order (stochastic) gradient oracles when the oracle is deterministic and oracles are noisy, respectively. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully {\it single-loop} manner, i.e., with $O(1)$ samples per iteration, and achieves the improved oracle-complexity of $O(\epsilon^{-3})$ and $O(\epsilon^{-5})$, respectively.
</details>
<details>
<summary>摘要</summary>
在这项研究中，我们研究了一类first-order算法来解决层次优化问题（BO），其目标函数是光滑的，但可能不具有凸形的两个水平。我们首先通过罚函数方法来研究BO的地形，具体来说，我们明确地表示罚函数和超对象函数之间的关系，并且确定了罚函数的梯度的计算方法。作为一个第一步，我们通过将上下水平目标函数组合在一个权重加权和罚参数 $\sigma > 0$ 中来研究BO的地形。在这个过程中，我们发现了罚函数和超对象函数之间的强联系，并且确定了罚函数的梯度的计算方法。作为下一步，我们视罚函数为$O(\sigma)$-近似于原始BO的问题，并提出了一种first-order算法来找到一个$\epsilon$-稳定解。当下水平问题具有小误差 proximal 障碍函数（EB）条件时，我们提出了一种first-order算法，可以在$O(\epsilon^{-3})$ 和 $O(\epsilon^{-7})$ 访问权重函数时 converge to an $\epsilon$-稳定点。在更进一步的假设下，我们展示了这种算法可以在单 Loop 模式下实现，即在每个迭代中只需要 $O(1)$ 样本，并且可以达到改进的 Oracle 复杂度 $O(\epsilon^{-3})$ 和 $O(\epsilon^{-5})$。
</details></li>
</ul>
<hr>
<h2 id="Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models"><a href="#Turbulent-Flow-Simulation-using-Autoregressive-Conditional-Diffusion-Models" class="headerlink" title="Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models"></a>Turbulent Flow Simulation using Autoregressive Conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01745">http://arxiv.org/abs/2309.01745</a></li>
<li>repo_url: None</li>
<li>paper_authors: Georg Kohl, Li-Wei Chen, Nils Thuerey</li>
<li>for:  simulate turbulent flows and achieve rollout stability in machine learning-based solvers</li>
<li>methods:  use an autoregressive rollout based on conditional diffusion models</li>
<li>results:  improved rollout stability without compromising sample quality, and successful generalization to flow parameters beyond the training regime<details>
<summary>Abstract</summary>
Simulating turbulent flows is crucial for a wide range of applications, and machine learning-based solvers are gaining increasing relevance. However, achieving stability when generalizing to longer rollout horizons remains a persistent challenge for learned PDE solvers. We address this challenge by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models. We show that this approach offers clear advantages in terms of rollout stability compared to other learned baselines. Remarkably, these improvements in stability are achieved without compromising the quality of generated samples, and our model successfully generalizes to flow parameters beyond the training regime. Additionally, the probabilistic nature of the diffusion approach allows for inferring predictions that align with the statistics of the underlying physics. We quantitatively and qualitatively evaluate the performance of our method on a range of challenging scenarios, including incompressible and transonic flows, as well as isotropic turbulence.
</details>
<details>
<summary>摘要</summary>
模拟湍流是广泛应用的关键任务，机器学习基于的解决方案在不断增长。然而，在扩展到更长的执行时间范围时，学习得到的稳定性问题仍然是持续的挑战。我们解决这个问题 by introducing a fully data-driven fluid solver that utilizes an autoregressive rollout based on conditional diffusion models.我们发现这种方法可以在扩展到更长的执行时间范围时提供明显的稳定性优势，而无需妥协生成样本的质量。此外，Diffusion方法的 probabilistic nature 允许我们对下游物理统计进行预测。我们对一系列复杂的场景进行了量化和质量地评估，包括不压缩和超音速流动，以及iso tropic turbulence。
</details></li>
</ul>
<hr>
<h2 id="An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports"><a href="#An-Empirical-Analysis-for-Zero-Shot-Multi-Label-Classification-on-COVID-19-CT-Scans-and-Uncurated-Reports" class="headerlink" title="An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"></a>An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01740">http://arxiv.org/abs/2309.01740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Dack, Lorenzo Brigato, Matthew McMurray, Matthias Fontanellaz, Thomas Frauenfelder, Hanno Hoppe, Aristomenis Exadaktylos, Thomas Geiser, Manuela Funke-Chambour, Andreas Christe, Lukas Ebner, Stavroula Mougiakakou</li>
<li>for: 该研究利用医院的不结构数据，通过计算机断层成像（CT）扫描来自动诊断 COVID-19。</li>
<li>methods: 该研究使用了多种零shot模型，通过对比视觉语言学习来实现多个标签分类。</li>
<li>results: 该研究表明，采用这些零shot模型可以帮助放射学专业人员更好地诊断肺动脉堵塞和识别肺部细节，如玻璃化肿坏和吸收。<details>
<summary>Abstract</summary>
The pandemic resulted in vast repositories of unstructured data, including radiology reports, due to increased medical examinations. Previous research on automated diagnosis of COVID-19 primarily focuses on X-ray images, despite their lower precision compared to computed tomography (CT) scans. In this work, we leverage unstructured data from a hospital and harness the fine-grained details offered by CT scans to perform zero-shot multi-label classification based on contrastive visual language learning. In collaboration with human experts, we investigate the effectiveness of multiple zero-shot models that aid radiologists in detecting pulmonary embolisms and identifying intricate lung details like ground glass opacities and consolidations. Our empirical analysis provides an overview of the possible solutions to target such fine-grained tasks, so far overlooked in the medical multimodal pretraining literature. Our investigation promises future advancements in the medical image analysis community by addressing some challenges associated with unstructured data and fine-grained multi-label classification.
</details>
<details>
<summary>摘要</summary>
“covid-19大流行导致了庞大的不结构数据存储，包括骨科报告。以前的自动诊断 covid-19 研究主要集中在X射线图像上，尽管它们的精度比 computed tomography (CT) 扫描低。在这项工作中，我们利用医院的不结构数据，并利用 CT 扫描提供的细节来实现零shot多标签分类，基于对比性视觉语言学习。与人类专家合作，我们研究了多种零shot模型是否能够帮助 radiologist 诊断肺动脉塞栓和识别复杂肺脏的细节，如云母膜和肺扩散。我们的实验分析提供了对这类细节任务的可能解决方案的概述，这些任务在医疗多Modal预训练文献中尚未得到了 sufficient 的注意。我们的调查承诺未来医学图像分析社区的进步，解决一些不结构数据和细节多标签分类的挑战。”
</details></li>
</ul>
<hr>
<h2 id="Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment"><a href="#Hybrid-data-driven-thermal-simulation-model-for-comfort-assessment" class="headerlink" title="Hybrid data driven&#x2F;thermal simulation model for comfort assessment"></a>Hybrid data driven&#x2F;thermal simulation model for comfort assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01734">http://arxiv.org/abs/2309.01734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Barbedienne, Sara Yasmine Ouerk, Mouadh Yagoubi, Hassan Bouia, Aurelie Kaemmerlen, Benoit Charrier</li>
<li>for: 提高物理模型的速度和质量，但需要大量数据，而获取数据往往困难和成本高。</li>
<li>methods: 将真实数据与模拟数据杂合以实现 thermal comfort 预测。使用 Modelica Language 进行模拟。比较不同机器学习方法的 benchmarking 研究。</li>
<li>results: 实现了一个 F1 分数为 0.999 的Random Forest 模型，结果看好。<details>
<summary>Abstract</summary>
Machine learning models improve the speed and quality of physical models. However, they require a large amount of data, which is often difficult and costly to acquire. Predicting thermal comfort, for example, requires a controlled environment, with participants presenting various characteristics (age, gender, ...). This paper proposes a method for hybridizing real data with simulated data for thermal comfort prediction. The simulations are performed using Modelica Language. A benchmarking study is realized to compare different machine learning methods. Obtained results look promising with an F1 score of 0.999 obtained using the random forest model.
</details>
<details>
<summary>摘要</summary>
机器学习模型可以提高物理模型的速度和质量，但它们需要大量数据，而这些数据往往困难和成本高昂地获得。预测冷暖舒适性需要控制环境，参与者具有不同特征（年龄、性别、...)。这篇论文提议将实际数据与 simulate 数据相结合以预测冷暖舒适性。模拟使用 Modelica 语言进行。实现了不同机器学习方法的比较研究。获得的结果很有 promise，使用随机森林模型获得 F1 分数为 0.999。Note:* “Machine learning models”in the original text is translated as “机器学习模型”in Simplified Chinese, which is a common way to refer to machine learning models in China.* “thermal comfort”in the original text is translated as “冷暖舒适性”in Simplified Chinese, which is a common way to refer to thermal comfort in China.* “Participants”in the original text is translated as “参与者”in Simplified Chinese, which is a common way to refer to participants in scientific studies in China.* “benchmarking study”in the original text is translated as “比较研究”in Simplified Chinese, which is a common way to refer to benchmarking studies in China.* “obtained results”in the original text is translated as “获得的结果”in Simplified Chinese, which is a common way to refer to results in China.* “F1 score”in the original text is translated as “ F1 分数”in Simplified Chinese, which is a common way to refer to F1 score in China.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning"><a href="#Adaptive-Resource-Allocation-for-Virtualized-Base-Stations-in-O-RAN-with-Online-Learning" class="headerlink" title="Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning"></a>Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01730">http://arxiv.org/abs/2309.01730</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Kalntis, George Iosifidis, Fernando A. Kuipers</li>
<li>for: 提高Open Radio Access Network系统中资源分配的效率，使得运营商能够更好地利用资源，降低成本，提高系统的灵活性和可替性。</li>
<li>methods: 提出了一种在线学习算法，能够在不精准环境下快速适应变化，并兼顾效率和能源消耗之间的trade-off。同时，提出了一种元学习方案，可以在不同的环境下选择最佳的算法，提高整体系统的多样性和效果。</li>
<li>results: 经过实验证明，提出的算法可以在不同的环境下具有低于线性偏差的性能，并且可以在实际数据上实现资源分配的效率提高，最高可以达64.5%的能源减少。<details>
<summary>Abstract</summary>
Open Radio Access Network systems, with their virtualized base stations (vBSs), offer operators the benefits of increased flexibility, reduced costs, vendor diversity, and interoperability. Optimizing the allocation of resources in a vBS is challenging since it requires knowledge of the environment, (i.e., "external'' information), such as traffic demands and channel quality, which is difficult to acquire precisely over short intervals of a few seconds. To tackle this problem, we propose an online learning algorithm that balances the effective throughput and vBS energy consumption, even under unforeseeable and "challenging'' environments; for instance, non-stationary or adversarial traffic demands. We also develop a meta-learning scheme, which leverages the power of other algorithmic approaches, tailored for more "easy'' environments, and dynamically chooses the best performing one, thus enhancing the overall system's versatility and effectiveness. We prove the proposed solutions achieve sub-linear regret, providing zero average optimality gap even in challenging environments. The performance of the algorithms is evaluated with real-world data and various trace-driven evaluations, indicating savings of up to 64.5% in the power consumption of a vBS compared with state-of-the-art benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Softmax-Bias-Correction-for-Quantized-Generative-Models"><a href="#Softmax-Bias-Correction-for-Quantized-Generative-Models" class="headerlink" title="Softmax Bias Correction for Quantized Generative Models"></a>Softmax Bias Correction for Quantized Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01729">http://arxiv.org/abs/2309.01729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, Markus Nagel</li>
<li>for: 提高逻辑推理模型在资源有限的边缘设备上的运行时间和能耗效率。</li>
<li>methods:  investigate the source of the softmax sensitivity to quantization and propose an offline bias correction technique to improve the quantizability of softmax without additional compute during deployment.</li>
<li>results:  achieved significant accuracy improvement for 8-bit quantized softmax on stable diffusion v1.5 and 125M-size OPT language model.<details>
<summary>Abstract</summary>
Post-training quantization (PTQ) is the go-to compression technique for large generative models, such as stable diffusion or large language models. PTQ methods commonly keep the softmax activation in higher precision as it has been shown to be very sensitive to quantization noise. However, this can lead to a significant runtime and power overhead during inference on resource-constraint edge devices. In this work, we investigate the source of the softmax sensitivity to quantization and show that the quantization operation leads to a large bias in the softmax output, causing accuracy degradation. To overcome this issue, we propose an offline bias correction technique that improves the quantizability of softmax without additional compute during deployment, as it can be readily absorbed into the quantization parameters. We demonstrate the effectiveness of our method on stable diffusion v1.5 and 125M-size OPT language model, achieving significant accuracy improvement for 8-bit quantized softmax.
</details>
<details>
<summary>摘要</summary>
<<SYS>>转换文本到简化中文。</SYS>Post-training quantization（PTQ）是大型生成模型的压缩技术，如稳定扩散或大语言模型。PTQ方法通常保留高精度的软 макс激活函数，因为它们对压缩噪声非常敏感。然而，这可能导致在资源有限的边缘设备上的 runtime 和功耗开销增加。在这种情况下，我们调查了软 макс对压缩的敏感性的原因，并证明了压缩操作导致软 макс输出的大跃迁偏移，从而导致准确性下降。为解决这个问题，我们提议了一种离线偏移修正技术，可以在部署时对软 макс进行不添加计算的偏移，从而提高压缩的可靠性。我们在稳定扩散 v1.5 和 125M 大小的 OPT 语言模型上证明了我们的方法的效果，实现了8位压缩软 макс的准确性提升。
</details></li>
</ul>
<hr>
<h2 id="Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction"><a href="#Prompting-or-Fine-tuning-A-Comparative-Study-of-Large-Language-Models-for-Taxonomy-Construction" class="headerlink" title="Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction"></a>Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01715">http://arxiv.org/abs/2309.01715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/20001LastOrder/Taxonomy-GPT">https://github.com/20001LastOrder/Taxonomy-GPT</a></li>
<li>paper_authors: Boqi Chen, Fandi Yi, Dániel Varró</li>
<li>for: 本研究旨在提出一种通用框架，以便自动生成符合结构约束的分类系统。</li>
<li>methods: 本研究使用了适当的用户输入（称为“提示”），以引导大型自然语言处理器（LLM）在多种NLP任务中进行自动分类。</li>
<li>results: 研究结果显示，无需显式重新训练，提示方法可以更高效地生成符合结构约束的分类系统，尤其是当训练数据集较小时。但是， Fine-tuning方法生成的分类系统可以轻松地进行后处理，以满足所有约束。<details>
<summary>Abstract</summary>
Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details>
<details>
<summary>摘要</summary>
TAXONOMIES 代表了实体之间的层次关系，通常在软件模型化和自然语言处理（NLP）活动中使用。它们通常受到一组结构约束的限制。然而，手动构建税onomy可以耗时、不充分和成本高。latest studies of large language models（LLMs）have shown that appropriate user inputs（called prompting）can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit（re）training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our results reveal the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.
</details></li>
</ul>
<hr>
<h2 id="On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise"><a href="#On-the-Robustness-of-Post-hoc-GNN-Explainers-to-Label-Noise" class="headerlink" title="On the Robustness of Post-hoc GNN Explainers to Label Noise"></a>On the Robustness of Post-hoc GNN Explainers to Label Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01706">http://arxiv.org/abs/2309.01706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqiang Zhong, Yangqianzi Jiang, Davide Mottin</li>
<li>for: 本研究旨在评估post-hoc graph neural network（GNN）解释器在不同水平的标签噪声下的可靠性。</li>
<li>methods: 本研究使用了多种post-hoc GNN解释器，包括LIME、SHAP、TreeExplainer等，对受损标签进行评估。</li>
<li>results: 研究发现，post-hoc GNN解释器受标签噪声的影响，导致解释质量下降。对于不同水平的标签噪声，解释器的效果不同。<details>
<summary>Abstract</summary>
Proposed as a solution to the inherent black-box limitations of graph neural networks (GNNs), post-hoc GNN explainers aim to provide precise and insightful explanations of the behaviours exhibited by trained GNNs. Despite their recent notable advancements in academic and industrial contexts, the robustness of post-hoc GNN explainers remains unexplored when confronted with label noise. To bridge this gap, we conduct a systematic empirical investigation to evaluate the efficacy of diverse post-hoc GNN explainers under varying degrees of label noise. Our results reveal several key insights: Firstly, post-hoc GNN explainers are susceptible to label perturbations. Secondly, even minor levels of label noise, inconsequential to GNN performance, harm the quality of generated explanations substantially. Lastly, we engage in a discourse regarding the progressive recovery of explanation effectiveness with escalating noise levels.
</details>
<details>
<summary>摘要</summary>
提出为解决图 neural network (GNN) 的内在黑盒限制，后续 GNN 解释器寻求提供精准和深入的 GNN 行为解释。 DESPITE 的最近学术和工业上的进展，后续 GNN 解释器对听力噪声的Robustness 还未得到探讨。为了bridging这个差距，我们进行了系统性的实验研究，评估不同后续 GNN 解释器在不同水平的标签噪声下的效果。我们的结果显示了以下几个关键发现：一、后续 GNN 解释器对标签扰动很敏感。二、即使标签噪声非常低，也会对解释质量造成很大的干扰。三、随着噪声水平的增加，解释效果可以逐渐恢复。
</details></li>
</ul>
<hr>
<h2 id="Robust-Online-Classification-From-Estimation-to-Denoising"><a href="#Robust-Online-Classification-From-Estimation-to-Denoising" class="headerlink" title="Robust Online Classification: From Estimation to Denoising"></a>Robust Online Classification: From Estimation to Denoising</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01698">http://arxiv.org/abs/2309.01698</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changlong Wu, Ananth Grama, Wojciech Szpankowski</li>
<li>for: 本研究探讨在含有噪音标签的线上分类问题中，learner如何适应噪音标签的不确定性。</li>
<li>methods: 本研究使用了一种通用kernel模型来模型噪音机制，其中，对于任何特征标签对，提供了一组知道的分布集合来生成噪音标签。learner在每个时间步骤上根据实际特征标签对 Observation selected an unknown distribution from the distribution set specified by the kernel，并生成噪音标签。learner then makes a prediction based on the actual features and noisy labels observed thus far, and incurs loss $1$ if the prediction differs from the underlying truth (and $0$ otherwise).</li>
<li>results: 本研究表明，对于许多自然的噪音kernel，选择性的特征和finite类别labeling函数，minimax risk可以在无限时间horizon和对数几何函数类型的labeling函数class上紧紧约束。此外，我们还扩展了这些结果到无限类和随机生成的特征上，通过stoquastic sequential covering的概念。本研究的结果超过和Ben-David et al. (2009)的发现，并提供了对这些结论的直观理解，通过一种novel reduction to online conditional distribution estimation。<details>
<summary>Abstract</summary>
We study online classification in the presence of noisy labels. The noise mechanism is modeled by a general kernel that specifies, for any feature-label pair, a (known) set of distributions over noisy labels. At each time step, an adversary selects an unknown distribution from the distribution set specified by the kernel based on the actual feature-label pair, and generates the noisy label from the selected distribution. The learner then makes a prediction based on the actual features and noisy labels observed thus far, and incurs loss $1$ if the prediction differs from the underlying truth (and $0$ otherwise). The prediction quality is quantified through minimax risk, which computes the cumulative loss over a finite horizon $T$. We show that for a wide range of natural noise kernels, adversarially selected features, and finite class of labeling functions, minimax risk can be upper bounded independent of the time horizon and logarithmic in the size of labeling function class. We then extend these results to inifinite classes and stochastically generated features via the concept of stochastic sequential covering. Our results extend and encompass findings of Ben-David et al. (2009) through substantial generality, and provide intuitive understanding through a novel reduction to online conditional distribution estimation.
</details>
<details>
<summary>摘要</summary>
我们研究在噪声标签下的在线分类。噪声机制由一个通用的kernel模型，每个特征标签对的(已知)Set of 噪声标签的分布。在每个时间步骤，一个反对手选择一个未知的分布从特定的kernel中，并生成噪声标签。学习者根据实际特征和观测到的噪声标签进行预测，并且计算损失。我们表示，对于广泛的自然噪声kernel、反对手选择的特征和标签分类函数的Finite类型，最小最大风险可以独立于时间框架和对数减少。我们然后扩展这些结果到无限类和随机生成的特征上，通过随机掩码覆盖概念。我们的结果超越和涵盖了Ben-David等人（2009）的发现，并提供了更加广泛的通用性和INTUITIVEunderstanding通过一种novel的减少到在线条件分布估计。
</details></li>
</ul>
<hr>
<h2 id="Physics-Informed-Polynomial-Chaos-Expansions"><a href="#Physics-Informed-Polynomial-Chaos-Expansions" class="headerlink" title="Physics-Informed Polynomial Chaos Expansions"></a>Physics-Informed Polynomial Chaos Expansions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01697">http://arxiv.org/abs/2309.01697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukáš Novák, Himanshu Sharma, Michael D. Shields</li>
<li>for: This paper presents a novel methodology for constructing physics-informed polynomial chaos expansions (PCE) that combines conventional experimental design with additional constraints from the physics of the model.</li>
<li>methods: The proposed method combines differential equations and boundary conditions to construct physically constrained PCEs, which are more accurate and efficient than standard sparse PCEs.</li>
<li>results: The proposed method leads to superior accuracy of the approximation and does not add significant computational burden. Additionally, the constrained PCEs can be easily applied for uncertainty quantification through analytical post-processing of a reduced PCE filtering out the influence of all deterministic space-time variables.<details>
<summary>Abstract</summary>
Surrogate modeling of costly mathematical models representing physical systems is challenging since it is typically not possible to create a large experimental design. Thus, it is beneficial to constrain the approximation to adhere to the known physics of the model. This paper presents a novel methodology for the construction of physics-informed polynomial chaos expansions (PCE) that combines the conventional experimental design with additional constraints from the physics of the model. Physical constraints investigated in this paper are represented by a set of differential equations and specified boundary conditions. A computationally efficient means for construction of physically constrained PCE is proposed and compared to standard sparse PCE. It is shown that the proposed algorithms lead to superior accuracy of the approximation and does not add significant computational burden. Although the main purpose of the proposed method lies in combining data and physical constraints, we show that physically constrained PCEs can be constructed from differential equations and boundary conditions alone without requiring evaluations of the original model. We further show that the constrained PCEs can be easily applied for uncertainty quantification through analytical post-processing of a reduced PCE filtering out the influence of all deterministic space-time variables. Several deterministic examples of increasing complexity are provided and the proposed method is applied for uncertainty quantification.
</details>
<details>
<summary>摘要</summary>
实验设计资料不够时，代理模型化的成本模型表示物理系统具有挑战。因此，将拓扑受限于物理模型所知道的物理法则。本文提出了一种新的物理受限的多项几何函数扩展（PCE）的建构方法，它结合了传统实验设计和模型物理法则的限制。这些物理限制通过一系列的数学方程和边界条件表示。提出了一种计算效率高的建构方法，并与标准的罕见PCE进行比较。结果显示，提议的算法可以实现更高精度的拓扑，而且不增加计算负担。尽管主要目的是将数据和物理限制结合起来，但我们显示了可以从数学方程和边界条件alone constructor physically constrained PCE，不需要评估原始模型。此外，我们还显示了受限PCE可以轻松地应用于不确定量化，通过对几何函数进行分析后处理，排除所有决定性空间时间变数。本文提供了一些具有不同复杂度的几何示例，并应用了不确定量化。
</details></li>
</ul>
<hr>
<h2 id="No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets"><a href="#No-Data-Augmentation-Alternative-Regularizations-for-Effective-Training-on-Small-Datasets" class="headerlink" title="No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets"></a>No Data Augmentation? Alternative Regularizations for Effective Training on Small Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01694">http://arxiv.org/abs/2309.01694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Brigato, Stavroula Mougiakakou</li>
<li>for: 提高小训练集图像分类任务的性能</li>
<li>methods: 使用不同规则强制平衡和权重衰减来提高模型性能</li>
<li>results: 在1%的原始CIFAR-10训练集和ciFAIR-10测试集上达到66.5%的测试精度，与当前最佳方法相当<details>
<summary>Abstract</summary>
Solving image classification tasks given small training datasets remains an open challenge for modern computer vision. Aggressive data augmentation and generative models are among the most straightforward approaches to overcoming the lack of data. However, the first fails to be agnostic to varying image domains, while the latter requires additional compute and careful design. In this work, we study alternative regularization strategies to push the limits of supervised learning on small image classification datasets. In particular, along with the model size and training schedule scaling, we employ a heuristic to select (semi) optimal learning rate and weight decay couples via the norm of model parameters. By training on only 1% of the original CIFAR-10 training set (i.e., 50 images per class) and testing on ciFAIR-10, a variant of the original CIFAR without duplicated images, we reach a test accuracy of 66.5%, on par with the best state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉中解决小训练集图像分类任务仍然是一个开放的挑战。使用激进数 augmentation 和生成模型是最直接的方法来缓解不足的数据问题，但是前者不具备适应不同图像领域的特性，而后者需要额外的计算资源和精心的设计。在这项工作中，我们研究了不同的常规化策略，以推动超越小训练集图像分类任务的深度学习。特别是，我们与模型大小和训练计划的扩大相关，采用一种恰当的学习率和权重衰减对的选择方法，通过使用模型参数的 нор方法来选择最佳的学习率和权重衰减。通过使用原始 CIFAR-10 训练集的 1%（即每个类50张图像）进行训练，并测试在 ciFAIR-10 上，一种不含重复图像的 CIFAR-10 变体，我们达到了66.5%的测试准确率，与现有最佳方法相当。
</details></li>
</ul>
<hr>
<h2 id="Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning"><a href="#Blind-Biological-Sequence-Denoising-with-Self-Supervised-Set-Learning" class="headerlink" title="Blind Biological Sequence Denoising with Self-Supervised Set Learning"></a>Blind Biological Sequence Denoising with Self-Supervised Set Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01670">http://arxiv.org/abs/2309.01670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nathan Ng, Ji Won Park, Jae Hyeon Lee, Ryan Lewis Kelly, Stephen Ra, Kyunghyun Cho</li>
<li>for: 本研究旨在提高高通量DNA测序数据的精度，尤其是对于短序列的识别和修复。</li>
<li>methods: 该研究提出了一种新的自动准备方法，即Self-Supervised Set Learning（SSSL），可以在不直接观察干净源序列标签的情况下，对短序列进行自动准备。该方法通过聚集短序列在嵌入空间中，并估计短序列的集合嵌入点，从而修复短序列中的错误。</li>
<li>results: 实验结果表明，SSSL方法可以在模拟的长读DNA数据上减少小读的错误率，对于长读的错误率也有所下降。在实际的抗体序列数据上，SSSL方法也有较好的性能，特别是在小读上，这些读取了大约60%的测试集。<details>
<summary>Abstract</summary>
Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy observations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the midpoint of the subreads in both the latent and sequence spaces. This set embedding represents the "average" of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of $\leq 6$ subreads with 17% fewer errors and large reads of $>6$ subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.
</details>
<details>
<summary>摘要</summary>
生物序列分析 rely 于能够去噪掉高通量长读平台输出的不精准序列。我们考虑一种常见的设定，在其中短序列被重复读取多次，生成多个噪声观测。尝试使用对 alignment 的方法去噪掉这些噪声观测可能会失败，当有太少的噪声观测或者错误率太高时。在这篇论文中，我们提出了一种新的方法，即 Self-Supervised Set Learning（SSSL）。SSSL 方法将噪声观测集结集中在一个抽象空间中，并估算这些噪声观测的集中点作为latent space和序列空间中的midpoint。这个集中点表示“平均”的噪声观测，可以被解码成一个估算clean sequence的预测。在对模拟的长读DNA数据进行实验时，SSSL方法可以将小读数据（≤6个噪声观测）和大读数据（>6个噪声观测）中的错误数量降低17%和8%。在一个实际的抗体序列数据集上，SSSL方法也在两个自我超vised metric上进行改进，特别是在difficult small reads中，这些读数据占总测试集的60%以上。通过准确地去噪掉这些读数据，SSSL方法可以更好地实现高通量DNA测序数据的下游科学应用。
</details></li>
</ul>
<hr>
<h2 id="Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data"><a href="#Robust-penalized-least-squares-of-depth-trimmed-residuals-regression-for-high-dimensional-data" class="headerlink" title="Robust penalized least squares of depth trimmed residuals regression for high-dimensional data"></a>Robust penalized least squares of depth trimmed residuals regression for high-dimensional data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01666">http://arxiv.org/abs/2309.01666</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijun Zuo</li>
<li>for: This paper aims to address the challenges of analyzing high-dimensional data in the big-data era, specifically the issues of outliers and contaminated points.</li>
<li>methods: The paper proposes a novel robust penalized regression method based on the least sum of squares of depth trimmed residuals to handle these challenges.</li>
<li>results: The proposed method outperforms some leading competitors in estimation and prediction accuracy in simulated and real data cases.<details>
<summary>Abstract</summary>
Challenges with data in the big-data era include (i) the dimension $p$ is often larger than the sample size $n$ (ii) outliers or contaminated points are frequently hidden and more difficult to detect. Challenge (i) renders most conventional methods inapplicable. Thus, it attracts tremendous attention from statistics, computer science, and bio-medical communities. Numerous penalized regression methods have been introduced as modern methods for analyzing high-dimensional data. Disproportionate attention has been paid to the challenge (ii) though. Penalized regression methods can do their job very well and are expected to handle the challenge (ii) simultaneously. Most of them, however, can break down by a single outlier (or single adversary contaminated point) as revealed in this article.   The latter systematically examines leading penalized regression methods in the literature in terms of their robustness, provides quantitative assessment, and reveals that most of them can break down by a single outlier. Consequently, a novel robust penalized regression method based on the least sum of squares of depth trimmed residuals is proposed and studied carefully. Experiments with simulated and real data reveal that the newly proposed method can outperform some leading competitors in estimation and prediction accuracy in the cases considered.
</details>
<details>
<summary>摘要</summary>
大数据时代中的数据分析挑战包括（i）样本大小n比维度p更小（ii）外围点或杂质点更难于检测。挑战（i）使得大多数传统方法无法应用。这引起了统计、计算机科学和生物医学领域的极大关注。众所周知，许多现代高维数据分析方法已经被引入。虽然挑战（ii）获得了过多的关注，但是抑约方法可以很好地处理挑战（ii）。然而，大多数方法都可以受到单个外围点（或单个杂质点）的影响，这在本文中得到了证明。为了解决这个问题，我们提出了一种基于深度剔除差异的新robust抑约方法，并且仔细研究了这种方法的性能。实验表明，新提出的方法可以在考虑的情况下超越一些领先竞争对手的准确性和预测性。
</details></li>
</ul>
<hr>
<h2 id="Locally-Stationary-Graph-Processes"><a href="#Locally-Stationary-Graph-Processes" class="headerlink" title="Locally Stationary Graph Processes"></a>Locally Stationary Graph Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01657">http://arxiv.org/abs/2309.01657</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdullah Canbolat, Elif Vural</li>
<li>for: 本文旨在提出一种基于不规则网络结构的本地站立性图像处理模型（LSGP），以扩展传统的全球站立性模型。</li>
<li>methods: 本文提出一种算法来计算LSGP模型，并研究将LSGP模型当地简化为WSS进程。</li>
<li>results: 实验表明，提出的进程模型可以提供与现状最佳竞争的准确信号表示。<details>
<summary>Abstract</summary>
Stationary graph process models are commonly used in the analysis and inference of data sets collected on irregular network topologies. While most of the existing methods represent graph signals with a single stationary process model that is globally valid on the entire graph, in many practical problems, the characteristics of the process may be subject to local variations in different regions of the graph. In this work, we propose a locally stationary graph process (LSGP) model that aims to extend the classical concept of local stationarity to irregular graph domains. We characterize local stationarity by expressing the overall process as the combination of a set of component processes such that the extent to which the process adheres to each component varies smoothly over the graph. We propose an algorithm for computing LSGP models from realizations of the process, and also study the approximation of LSGPs locally with WSS processes. Experiments on signal interpolation problems show that the proposed process model provides accurate signal representations competitive with the state of the art.
</details>
<details>
<summary>摘要</summary>
Stationary graph process模型通常用于分析和推断非 régulière网络拓扑上的数据集。大多数现有方法使用全球静态过程模型来表示图像信号，但在实际问题中，过程特性可能在不同地方Graph上具有本地差异。在这种情况下，我们提议使用本地静态图像过程（LSGP）模型，以扩展传统的本地静态性概念到不规则图像领域。我们通过表示过程的总体组合，其中每个组件过程的承袭程度在图像上变化平滑来定义本地静态性。我们提出了计算LSGP模型的算法，以及对LSGP进行本地简化的WSS过程的研究。实验表明，我们的过程模型可以与当前状态革命竞争。
</details></li>
</ul>
<hr>
<h2 id="Which-algorithm-to-select-in-sports-timetabling"><a href="#Which-algorithm-to-select-in-sports-timetabling" class="headerlink" title="Which algorithm to select in sports timetabling?"></a>Which algorithm to select in sports timetabling?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03229">http://arxiv.org/abs/2309.03229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/robertomrosati/sa4stt">https://github.com/robertomrosati/sa4stt</a></li>
<li>paper_authors: David Van Bulck, Dries Goossens, Jan-Patrick Clarner, Angelos Dimitsas, George H. G. Fonseca, Carlos Lamas-Fernandez, Martin Mariusz Lester, Jaap Pedersen, Antony E. Phillips, Roberto Maria Rosati</li>
<li>for: 这个论文的目的是提供体育调度问题的时间表，并分析八种现有的算法的优劣。</li>
<li>methods: 这篇论文使用机器学习技术开发了一个算法选择系统，可以根据体育调度问题的特点来预测哪一个算法会最好。</li>
<li>results: 根据大规模的 computation experiments，这篇论文获得了关于八种算法的性能分析和体育调度问题的特点重要性的深入了解。<details>
<summary>Abstract</summary>
Any sports competition needs a timetable, specifying when and where teams meet each other. The recent International Timetabling Competition (ITC2021) on sports timetabling showed that, although it is possible to develop general algorithms, the performance of each algorithm varies considerably over the problem instances. This paper provides an instance space analysis for sports timetabling, resulting in powerful insights into the strengths and weaknesses of eight state-of-the-art algorithms. Based on machine learning techniques, we propose an algorithm selection system that predicts which algorithm is likely to perform best when given the characteristics of a sports timetabling problem instance. Furthermore, we identify which characteristics are important in making that prediction, providing insights in the performance of the algorithms, and suggestions to further improve them. Finally, we assess the empirical hardness of the instances. Our results are based on large computational experiments involving about 50 years of CPU time on more than 500 newly generated problem instances.
</details>
<details>
<summary>摘要</summary>
任何体育竞赛都需要一份时间表，确定队伍在哪里和何时相遇。最近的国际时间安排竞赛（ITC2021）显示，虽然可以开发通用算法，但每个算法在具体的问题实例上表现会很不同。这篇论文提供了体育时间安排的实例空间分析，从而获得了八种现代算法的强大视角和缺点。基于机器学习技术，我们提出了一个算法选择系统，可以根据体育时间安排问题实例的特点预测最佳的算法。此外，我们还确定了影响这种预测的重要特征，从而提供了算法表现的理解和提高建议。最后，我们评估了实际难度的问题实例。我们的结果基于超过50年的CPU时间和更多于500个新生成的问题实例进行了大规模的计算实验。
</details></li>
</ul>
<hr>
<h2 id="Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis"><a href="#Relay-Diffusion-Unifying-diffusion-process-across-resolutions-for-image-synthesis" class="headerlink" title="Relay Diffusion: Unifying diffusion process across resolutions for image synthesis"></a>Relay Diffusion: Unifying diffusion process across resolutions for image synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03350">http://arxiv.org/abs/2309.03350</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion">https://github.com/THUDM/RelayDiffusion</a></li>
<li>paper_authors: Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang</li>
<li>for: 这篇论文主要针对高分辨率图像生成中的散射模型问题进行研究，以实现高品质的图像生成。</li>
<li>methods: 本文提出了一种名为遮盾散射模型（RDM），它可以将低分辨率图像或噪声转换为等效的高分辨率图像，并且可以继续进行散射过程，无需重新从噪声或低分辨率条件开始。</li>
<li>results: RDM在CelebA-HQ和ImageNet 256$\times$256上 achieved state-of-the-art FID和sFID，大幅超过了前一些工作，如ADM、LDM和DiT。所有代码和检查点都可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/THUDM/RelayDiffusion%7D">https://github.com/THUDM/RelayDiffusion}</a> 上取得。<details>
<summary>Abstract</summary>
Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that \emph{the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain}. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at \url{https://github.com/THUDM/RelayDiffusion}.
</details>
<details>
<summary>摘要</summary>
Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at <https://github.com/THUDM/RelayDiffusion>.Here's the translation in Traditional Chinese:Diffusion models have achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find that the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256, surpassing previous works such as ADM, LDM and DiT by a large margin. All the codes and checkpoints are open-sourced at <https://github.com/THUDM/RelayDiffusion>.
</details></li>
</ul>
<hr>
<h2 id="Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD"><a href="#Corgi-2-A-Hybrid-Offline-Online-Approach-To-Storage-Aware-Data-Shuffling-For-SGD" class="headerlink" title="Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD"></a>Corgi^2: A Hybrid Offline-Online Approach To Storage-Aware Data Shuffling For SGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01640">http://arxiv.org/abs/2309.01640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Etay Livne, Gal Kaplun, Eran Malach, Shai Shalev-Schwatz</li>
<li>for: 提高 Stochastic Gradient Descent (SGD) 训练机器学习模型的效率，尤其是对于大规模数据存储在云端的情况。</li>
<li>methods: 提出了一种在线洗混算法 called CorgiPile，可以大幅提高数据访问效率，但是会付出一定的性能损失，尤其是对于 homogeneous shards (例如视频数据)。</li>
<li>results: 提出了一种新的两步 partial data shuffling 策略，将 offline 的 CorgiPile 方法和 online 的 SGD 方法结合在一起，可以同时保持数据访问效率和性能。提供了对方法的完整理论分析，并通过实验结果证明其实际优势。<details>
<summary>Abstract</summary>
When using Stochastic Gradient Descent (SGD) for training machine learning models, it is often crucial to provide the model with examples sampled at random from the dataset. However, for large datasets stored in the cloud, random access to individual examples is often costly and inefficient. A recent work \cite{corgi}, proposed an online shuffling algorithm called CorgiPile, which greatly improves efficiency of data access, at the cost some performance loss, which is particularly apparent for large datasets stored in homogeneous shards (e.g., video datasets). In this paper, we introduce a novel two-step partial data shuffling strategy for SGD which combines an offline iteration of the CorgiPile method with a subsequent online iteration. Our approach enjoys the best of both worlds: it performs similarly to SGD with random access (even for homogenous data) without compromising the data access efficiency of CorgiPile. We provide a comprehensive theoretical analysis of the convergence properties of our method and demonstrate its practical advantages through experimental results.
</details>
<details>
<summary>摘要</summary>
当使用渐进算法（Stochastic Gradient Descent，SGD）训练机器学习模型时，通常需要将模型提供随机选择的示例。然而，对于大规模存储在云端的数据集，随机访问单个示例可以非常昂贵和不效率。一项最近的研究（\cite{corgi））提出了一种在线混淆算法called CorgiPile，可以大幅提高数据访问效率，但是会付出一定的性能损失，尤其是对于具有同质数据集（例如视频集）。在这篇论文中，我们提出了一种新的两步半数据混淆策略，将在线迭代CorgiPile方法与后续的SGD迭代结合。我们的方法能够兼顾两者的优点：它能够与随机访问SGD（即使对同质数据）达到相似的性能，而无需牺牲CorgiPile的数据访问效率。我们对方法的整体理论分析和实验结果进行了全面的描述。
</details></li>
</ul>
<hr>
<h2 id="Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes"><a href="#Representing-Edge-Flows-on-Graphs-via-Sparse-Cell-Complexes" class="headerlink" title="Representing Edge Flows on Graphs via Sparse Cell Complexes"></a>Representing Edge Flows on Graphs via Sparse Cell Complexes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01632">http://arxiv.org/abs/2309.01632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josef Hoppe, Michael T. Schaub</li>
<li>for:  obtained sparse, interpretable representations of observable data</li>
<li>methods:  lifted graph structure to a simplicial complex, used Hodge decomposition to represent observed data</li>
<li>results:  outperformed current state-of-the-art methods while being computationally efficient, demonstrated on real-world and synthetic data<details>
<summary>Abstract</summary>
Obtaining sparse, interpretable representations of observable data is crucial in many machine learning and signal processing tasks. For data representing flows along the edges of a graph, an intuitively interpretable way to obtain such representations is to lift the graph structure to a simplicial complex: The eigenvectors of the associated Hodge-Laplacian, respectively the incidence matrices of the corresponding simplicial complex then induce a Hodge decomposition, which can be used to represent the observed data in terms of gradient, curl, and harmonic flows. In this paper, we generalize this approach to cellular complexes and introduce the cell inference optimization problem, i.e., the problem of augmenting the observed graph by a set of cells, such that the eigenvectors of the associated Hodge Laplacian provide a sparse, interpretable representation of the observed edge flows on the graph. We show that this problem is NP-hard and introduce an efficient approximation algorithm for its solution. Experiments on real-world and synthetic data demonstrate that our algorithm outperforms current state-of-the-art methods while being computationally efficient.
</details>
<details>
<summary>摘要</summary>
获取简洁可解释的数据表示是许多机器学习和信号处理任务中的关键。在图structure上的数据流动中，可以使用升级图结构到 simplicial complex来获得这些表示。图结构的eigenvector和相关矩阵可以引入一个Hodge分解，用于表示观察到的边流动。在这篇论文中，我们扩展了这种方法到细胞复杂体系，并引入细胞推理优化问题，即在观察到的图上添加一组细胞，使得图结构的eigenvector提供简洁可解释的表示。我们证明这个问题是NP困难的，并提出了一种有效的近似算法来解决它。实验结果表明，我们的算法在实际世界数据和模拟数据上都能够超过当前状态艺术方法，同时 computationally efficient。
</details></li>
</ul>
<hr>
<h2 id="DeViL-Decoding-Vision-features-into-Language"><a href="#DeViL-Decoding-Vision-features-into-Language" class="headerlink" title="DeViL: Decoding Vision features into Language"></a>DeViL: Decoding Vision features into Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01617">http://arxiv.org/abs/2309.01617</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ExplainableML/DeViL">https://github.com/ExplainableML/DeViL</a></li>
<li>paper_authors: Meghal Dani, Isabel Rio-Torto, Stephan Alaniz, Zeynep Akata</li>
<li>For: This paper aims to provide natural language descriptions for what different layers of a vision backbone have learned, and to generate textual descriptions of visual features at different layers of the network.* Methods: The DeViL method uses a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. The model employs dropout both per-layer and per-spatial-location to generalize training on image-text pairs and produce localized explanations.* Results: DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Additionally, DeViL outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset.<details>
<summary>Abstract</summary>
Post-hoc explanation methods have often been criticised for abstracting away the decision-making process of deep neural networks. In this work, we would like to provide natural language descriptions for what different layers of a vision backbone have learned. Our DeViL method decodes vision features into language, not only highlighting the attribution locations but also generating textual descriptions of visual features at different layers of the network. We train a transformer network to translate individual image features of any vision layer into a prompt that a separate off-the-shelf language model decodes into natural language. By employing dropout both per-layer and per-spatial-location, our model can generalize training on image-text pairs to generate localized explanations. As it uses a pre-trained language model, our approach is fast to train, can be applied to any vision backbone, and produces textual descriptions at different layers of the vision network. Moreover, DeViL can create open-vocabulary attribution maps corresponding to words or phrases even outside the training scope of the vision model. We demonstrate that DeViL generates textual descriptions relevant to the image content on CC3M surpassing previous lightweight captioning models and attribution maps uncovering the learned concepts of the vision backbone. Finally, we show DeViL also outperforms the current state-of-the-art on the neuron-wise descriptions of the MILANNOTATIONS dataset. Code available at https://github.com/ExplainableML/DeViL
</details>
<details>
<summary>摘要</summary>
后期解释方法经常被批评为忽略深度神经网络的决策过程。在这项工作中，我们想提供深度神经网络各层学习的自然语言描述。我们的DeViL方法将视觉特征转换为语言描述，不仅高亮各层网络的贡献位置，还生成对应的语言描述。我们使用 transformer 网络将任意视觉层特征转换为可decode的自然语言描述。我们的方法快速训练，可以应用于任何视觉底层，并生成各层网络学习的文本描述。此外，DeViL 还可以生成对应于训练之外词汇的开放词汇映射。我们示示了 DeViL 对 CC3M 图像的描述与前期轻量级captioning模型具有比较好的性能，并且可以描述视觉模型学习的概念。最后，我们还证明了 DeViL 在 MILANNOTATIONS 数据集上的 neuron-wise 描述性能比现有的状态之前。代码可以在 GitHub 上找到：https://github.com/ExplainableML/DeViL。
</details></li>
</ul>
<hr>
<h2 id="Dropout-Attacks"><a href="#Dropout-Attacks" class="headerlink" title="Dropout Attacks"></a>Dropout Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01614">http://arxiv.org/abs/2309.01614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ngunnar/Robustness_tutorial">https://github.com/ngunnar/Robustness_tutorial</a></li>
<li>paper_authors: Andrew Yuan, Alina Oprea, Cheng Tan</li>
<li>for: 本文旨在描述一种新的深度学习攻击方法 named DROPOUTATTACK，该攻击方法利用Dropout操作的杂 randomly drop neurons during training中的杂 randomly drop neurons during training 来防止过拟合。</li>
<li>methods: 本文使用了 four DROPOUTATTACK variants to cover a broad range of scenarios, including slowing or stopping training, destroying prediction accuracy of target classes, and sabotaging either precision or recall of a target class.</li>
<li>results: 在训练 VGG-16 模型在 CIFAR-100 上，我们的攻击可以降低目标类的精度从 81.7% 降低到 47.1% 而无需对模型精度产生影响。<details>
<summary>Abstract</summary>
Dropout is a common operator in deep learning, aiming to prevent overfitting by randomly dropping neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy
</details>
<details>
<summary>摘要</summary>
<<Dropout是深度学习中常用的操作，目的是防止过拟合by randomly dropped neurons during training. This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK. DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random. We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios. These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class. In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy.>>Here's the breakdown of the translation:* Dropout (Dropout) is a common operator in deep learning that aims to prevent overfitting by randomly dropping neurons during training.* This paper introduces a new family of poisoning attacks against neural networks named DROPOUTATTACK.* DROPOUTATTACK attacks the dropout operator by manipulating the selection of neurons to drop instead of selecting them uniformly at random.* We design, implement, and evaluate four DROPOUTATTACK variants that cover a broad range of scenarios.* These attacks can slow or stop training, destroy prediction accuracy of target classes, and sabotage either precision or recall of a target class.* In our experiments of training a VGG-16 model on CIFAR-100, our attack can reduce the precision of the victim class by 34.6% (from 81.7% to 47.1%) without incurring any degradation in model accuracy.
</details></li>
</ul>
<hr>
<h2 id="On-the-Query-Strategies-for-Efficient-Online-Active-Distillation"><a href="#On-the-Query-Strategies-for-Efficient-Online-Active-Distillation" class="headerlink" title="On the Query Strategies for Efficient Online Active Distillation"></a>On the Query Strategies for Efficient Online Active Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01612">http://arxiv.org/abs/2309.01612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michele Boldo, Enrico Martini, Mirco De Marchi, Stefano Aldegheri, Nicola Bombieri</li>
<li>for: 提高人姿估计（HPE）模型训练效率和实时适应性。</li>
<li>methods: 使用活动学习（AL）和在线热退出（online distillation）技术，评估不同查询策略以实现最佳训练结果。</li>
<li>results: 在一个流行的HPE数据集上，实现轻量级模型在实时上进行有效适应，并且在不同的查询策略下进行了评估。<details>
<summary>Abstract</summary>
Deep Learning (DL) requires lots of time and data, resulting in high computational demands. Recently, researchers employ Active Learning (AL) and online distillation to enhance training efficiency and real-time model adaptation. This paper evaluates a set of query strategies to achieve the best training results. It focuses on Human Pose Estimation (HPE) applications, assessing the impact of selected frames during training using two approaches: a classical offline method and a online evaluation through a continual learning approach employing knowledge distillation, on a popular state-of-the-art HPE dataset. The paper demonstrates the possibility of enabling training at the edge lightweight models, adapting them effectively to new contexts in real-time.
</details>
<details>
<summary>摘要</summary>
深度学习（DL）需要大量时间和数据，导致计算需求很高。现在，研究人员通过活动学习（AL）和在线热静解释来提高训练效率和实时模型适应。这篇论文评估了一组查询策略以实现最佳训练结果。它专注于人姿估计（HPE）应用，评估选择的帧数据在训练中的影响，使用两种方法：一种 classical offline 方法和一种在线评估通过持续学习方法进行知识传递，在一个流行的HPE数据集上进行评估。这篇论文示出了在边缘进行轻量级模型训练，并在实时上适应新上下文的可能性。
</details></li>
</ul>
<hr>
<h2 id="Fair-Ranking-under-Disparate-Uncertainty"><a href="#Fair-Ranking-under-Disparate-Uncertainty" class="headerlink" title="Fair Ranking under Disparate Uncertainty"></a>Fair Ranking under Disparate Uncertainty</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01610">http://arxiv.org/abs/2309.01610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richa Rastogi, Thorsten Joachims</li>
<li>for: 这篇论文关注了排名的偏见问题，具体来说是在排名时存在不同群体之间的不公正问题。</li>
<li>methods: 作者提出了一种新的公平排名标准（Equal-Opportunity Ranking，EOR），并提供了一种实现这种标准的算法，该算法可以在 $O(n \log(n))$ 时间内计算出公平排名。</li>
<li>results: 作者在 synthetic data、US Census 数据和 Amazon 搜索查询的实验中证明了该算法可以准确地保证公平排名，而且可以提供有效的排名。<details>
<summary>Abstract</summary>
Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation far more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, since the relevance estimates for minority groups tend to have higher uncertainty due to a lack of data or appropriate features. To overcome this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking that provably corrects for the disparity in uncertainty between groups. Furthermore, we present a practical algorithm for computing EOR rankings in time $O(n \log(n))$ and prove its close approximation guarantee to the globally optimal solution. In a comprehensive empirical evaluation on synthetic data, a US Census dataset, and a real-world case study of Amazon search queries, we find that the algorithm reliably guarantees EOR fairness while providing effective rankings.
</details>
<details>
<summary>摘要</summary>
“排名是一种广泛使用的方法，用于引导人类评估者对可管理的选项进行筛选。它的应用范围从电商网站上出现可能有用的产品到审核学校申请。虽然排名可以使人类评估变得非常有效，但它会引入不公平性，如果在选项群中存在不同群体的uncertainty的差异。实际情况是，对少数群体的相关性估计通常具有更高的uncertainty，因为这些群体没有足够的数据或适当的特征。为解决这个公平问题，我们提出了一种新的公平准则——equal-opportunity ranking（EOR），可以正确地修正不同群体之间的uncertainty差异。此外，我们还提出了一种实用的计算EOR排名的算法，时间复杂度为O(nlog(n))，并证明其具有近似的优化策略。在 synthetic data、US Census dataset和amazon搜索查询的实验研究中，我们发现这种算法可靠地保证EOR公平性，同时提供有效的排名。”
</details></li>
</ul>
<hr>
<h2 id="Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning"><a href="#Active-flow-control-for-three-dimensional-cylinders-through-deep-reinforcement-learning" class="headerlink" title="Active flow control for three-dimensional cylinders through deep reinforcement learning"></a>Active flow control for three-dimensional cylinders through deep reinforcement learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02462">http://arxiv.org/abs/2309.02462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pol Suárez, Francisco Alcántara-Ávila, Arnau Miró, Jean Rabault, Bernat Font, Oriol Lehmkuhl, R. Vinuesa</li>
<li>for: 降低缆线阻力</li>
<li>methods: 使用深度强化学习框架，结合计算流体动力学模拟器和智能代理人，实现多个独立控制的零负质量流体排气机制</li>
<li>results: 在三种不同配置下，通过应用DRL控制实现了显著的阻力减少<details>
<summary>Abstract</summary>
This paper presents for the first time successful results of active flow control with multiple independently controlled zero-net-mass-flux synthetic jets. The jets are placed on a three-dimensional cylinder along its span with the aim of reducing the drag coefficient. The method is based on a deep-reinforcement-learning framework that couples a computational-fluid-dynamics solver with an agent using the proximal-policy-optimization algorithm. We implement a multi-agent reinforcement-learning framework which offers numerous advantages: it exploits local invariants, makes the control adaptable to different geometries, facilitates transfer learning and cross-application of agents and results in significant training speedup. In this contribution we report significant drag reduction after applying the DRL-based control in three different configurations of the problem.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文首次报道了基于多个独立控制的零质量流体Synthetic jet的活动流控方法。jets被安装在三维圆柱上，以减少抗力系数。该方法基于深度学习束缚政策优化算法（DRL），并与计算流体力学解题结合使用。我们实现了多个代理人学习框架，它们具有许多优势：它利用本地 invariants，使控制可适应不同的几何结构，促进了转移学习和交叉应用代理人和结果，并导致了显著的训练加速。在这篇论文中，我们报道了在三个不同配置下应用DRL控制后的显著抗力减少。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data"><a href="#Deep-Learning-Overloaded-Vehicle-Identification-for-Long-Span-Bridges-Based-on-Structural-Health-Monitoring-Data" class="headerlink" title="Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data"></a>Deep Learning Overloaded Vehicle Identification for Long Span Bridges Based on Structural Health Monitoring Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01593">http://arxiv.org/abs/2309.01593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuqin Li, Jun Liu, Shengliang Zhong, Licheng Zhou, Shoubin Dong, Zejia Liu, Liqun Tang</li>
<li>for: 该论文主要用于车辆过载identification，尤其是对长跨度桥梁的Structural Health Monitoring数据进行分析。</li>
<li>methods: 该论文提出了一种基于深度学习的车辆过载identification方法（DOVI），使用时间卷积架构提取输入序列数据的空间和时间特征，提供了一个端到端的车辆过载identification解决方案，不需要影响线或者先知道车辆的速度和车辋基本信息。</li>
<li>results: 模型评估在简支桥和长跨度斜拉桥上下Random traffic flow下，结果表明，提出的深度学习车辆过载identification方法比其他机器学习和深度学习方法更有效和更强健。<details>
<summary>Abstract</summary>
Overloaded vehicles bring great harm to transportation infrastructures. BWIM (bridge weigh-in-motion) method for overloaded vehicle identification is getting more popular because it can be implemented without interruption to the traffic. However, its application is still limited because its effectiveness largely depends on professional knowledge and extra information, and is susceptible to occurrence of multiple vehicles. In this paper, a deep learning based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges by the use of structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus provides an end-to-end overloaded vehicle identification solution which neither needs the influence line nor needs to obtain velocity and wheelbase information in advance and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness, compared with other machine learning and deep learning approaches.
</details>
<details>
<summary>摘要</summary>
过载车辆对交通基础设施造成巨大的危害。 bridge weigh-in-motion（BWIM）方法为过载车辆识别 becoming more popular, because it can be implemented without interrupting traffic. However, its application is still limited because its effectiveness relies heavily on professional knowledge and additional information, and is susceptible to the occurrence of multiple vehicles. In this paper, a deep learning-based overloaded vehicle identification approach (DOVI) is proposed, with the purpose of overloaded vehicle identification for long-span bridges using structural health monitoring data. The proposed DOVI model uses temporal convolutional architectures to extract the spatial and temporal features of the input sequence data, thus providing an end-to-end overloaded vehicle identification solution that does not require the influence line nor needs to obtain velocity and wheelbase information in advance, and can be applied under the occurrence of multiple vehicles. Model evaluations are conducted on a simply supported beam and a long-span cable-stayed bridge under random traffic flow. Results demonstrate that the proposed deep-learning overloaded vehicle identification approach has better effectiveness and robustness compared with other machine learning and deep learning approaches.
</details></li>
</ul>
<hr>
<h2 id="Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width"><a href="#Les-Houches-Lectures-on-Deep-Learning-at-Large-Infinite-Width" class="headerlink" title="Les Houches Lectures on Deep Learning at Large &amp; Infinite Width"></a>Les Houches Lectures on Deep Learning at Large &amp; Infinite Width</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01592">http://arxiv.org/abs/2309.01592</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yasaman Bahri, Boris Hanin, Antonin Brossollet, Vittorio Erba, Christian Keup, Rosalba Pacelli, James B. Simon</li>
<li>for: 这些讲座在2022年的Les Houches夏学校上探讨了深度神经网络的无穷宽限和大宽限。</li>
<li>methods: 讲座讨论了深度神经网络的各种统计和动力学性质，包括随机深度神经网络、训练后深度神经网络与线性模型、核函数和高斯过程之间的连接，以及训练后大宽网络的不同初始化和训练方式。</li>
<li>results: 讲座介绍了训练后深度神经网络的性质和行为，包括 inicialization和训练后的性质，以及在不同宽度下的性质和行为。<details>
<summary>Abstract</summary>
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
</details>
<details>
<summary>摘要</summary>
这些讲座，发表于2022年勒舍瑞夏学校的统计物理和机器学习讲座，关注深度神经网络的无穷宽限和大宽限。讲座讨论了这些网络的各种统计和动力学性质。特别是讲者讨论了随机深度神经网络的属性；训练后的深度神经网络与线性模型、kernels和 Gaussian Processes在无穷宽限下的连接；以及训练前和训练后的大宽网络的不可逆和可逆处理。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models"><a href="#Probabilistic-Precision-and-Recall-Towards-Reliable-Evaluation-of-Generative-Models" class="headerlink" title="Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models"></a>Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01590">http://arxiv.org/abs/2309.01590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kdst-team/probablistic_precision_recall">https://github.com/kdst-team/probablistic_precision_recall</a></li>
<li>paper_authors: Dogyun Park, Suhyun Kim</li>
<li>for: 本文旨在检验生成模型的准确性和多样性，并提出了新的评估指标方法。</li>
<li>methods: 本文使用了$k$NN基于精度-回归指标来分解统计距离，并进行了其分析和评估。</li>
<li>results: 本文发现了$k$NN指标存在偏 toward outliers和不敏感于分布变化的问题，并提出了基于概率方法的新指标PP&amp;PR，并通过了extensive experiments表明其可靠性。Here’s the breakdown of each point in English:</li>
<li>for: The paper is aimed at evaluating the fidelity and diversity of generative models, and proposes new evaluation metrics.</li>
<li>methods: The paper uses $k$NN-based precision-recall metrics to break down the statistical distance, and analyzes and assesses these metrics.</li>
<li>results: The paper finds that the $k$NN indicators are susceptible to outliers and insensitive to distributional changes, and proposes novel indicators PP&amp;PR based on a probabilistic approach, which provide more reliable estimates for comparing fidelity and diversity.<details>
<summary>Abstract</summary>
Assessing the fidelity and diversity of the generative model is a difficult but important issue for technological advancement. So, recent papers have introduced k-Nearest Neighbor ($k$NN) based precision-recall metrics to break down the statistical distance into fidelity and diversity. While they provide an intuitive method, we thoroughly analyze these metrics and identify oversimplified assumptions and undesirable properties of kNN that result in unreliable evaluation, such as susceptibility to outliers and insensitivity to distributional changes. Thus, we propose novel metrics, P-precision and P-recall (PP\&PR), based on a probabilistic approach that address the problems. Through extensive investigations on toy experiments and state-of-the-art generative models, we show that our PP\&PR provide more reliable estimates for comparing fidelity and diversity than the existing metrics. The codes are available at \url{https://github.com/kdst-team/Probablistic_precision_recall}.
</details>
<details>
<summary>摘要</summary>
评估生成模型的准确性和多样性是技术进步的重要问题。因此，最近的论文已经引入基于k-最近邻居($k$NN)的精度-回快指标来分解统计距离。这些指标提供了直观的方法，但我们进行了全面的分析，并发现了kNN指标存在强制约束和不可靠评估的问题，如受到异常值的影响和分布变化的不敏感。因此，我们提出了新的指标P-精度和P-回快（PP＆PR），基于概率方法，解决了这些问题。通过对实验和当前的生成模型进行广泛的调查，我们显示了我们的PP＆PR可以更可靠地评估生成模型的准确性和多样性。代码可以在GitHub上下载：https://github.com/kdst-team/Probablistic_precision_recall。
</details></li>
</ul>
<hr>
<h2 id="SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices"><a href="#SATAY-A-Streaming-Architecture-Toolflow-for-Accelerating-YOLO-Models-on-FPGA-Devices" class="headerlink" title="SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices"></a>SATAY: A Streaming Architecture Toolflow for Accelerating YOLO Models on FPGA Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01587">http://arxiv.org/abs/2309.01587</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Montgomerie-Corcoran, Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis</li>
<li>for: 这个研究旨在解决目前边缘设备上实现现代物体检测模型的问题，并提供一个可靠的、低延迟的方案。</li>
<li>methods: 这个研究使用了一个流动架构设计，实现了完整的YOLO模型在FPGA设备上的加速。它还使用了一些新的硬件元件来支持YOLO模型的操作，以及外部内存缓冲来解决限制性的内存资源问题。</li>
<li>results: 这个研究的结果显示，使用这个工具流程生成的加速器设计可以与GPU设备相比，并且比现有的FPGA加速器更高性能和更低能耗。<details>
<summary>Abstract</summary>
AI has led to significant advancements in computer vision and image processing tasks, enabling a wide range of applications in real-life scenarios, from autonomous vehicles to medical imaging. Many of those applications require efficient object detection algorithms and complementary real-time, low latency hardware to perform inference of these algorithms. The YOLO family of models is considered the most efficient for object detection, having only a single model pass. Despite this, the complexity and size of YOLO models can be too computationally demanding for current edge-based platforms. To address this, we present SATAY: a Streaming Architecture Toolflow for Accelerating YOLO. This work tackles the challenges of deploying stateof-the-art object detection models onto FPGA devices for ultralow latency applications, enabling real-time, edge-based object detection. We employ a streaming architecture design for our YOLO accelerators, implementing the complete model on-chip in a deeply pipelined fashion. These accelerators are generated using an automated toolflow, and can target a range of suitable FPGA devices. We introduce novel hardware components to support the operations of YOLO models in a dataflow manner, and off-chip memory buffering to address the limited on-chip memory resources. Our toolflow is able to generate accelerator designs which demonstrate competitive performance and energy characteristics to GPU devices, and which outperform current state-of-the-art FPGA accelerators.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion"><a href="#DiffHPE-Robust-Coherent-3D-Human-Pose-Lifting-with-Diffusion" class="headerlink" title="DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion"></a>DiffHPE: Robust, Coherent 3D Human Pose Lifting with Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01575">http://arxiv.org/abs/2309.01575</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cédric Rommel, Eduardo Valle, Mickaël Chen, Souhaiel Khalfaoui, Renaud Marlet, Matthieu Cord, Patrick Pérez</li>
<li>for: 这个论文的目的是提出一种新的三维人体姿态估计（3D-HPE）方法，通过粉批模型来提高人体姿态估计的准确性、可靠性和一致性。</li>
<li>methods: 这篇论文使用了 diffusion models，这种方法在多个领域中已经引起了革命，但在3D-HPE中尚未得到广泛研究。作者们在3D-HPE中应用了这种新的粉批模型策略，并证明了它能够改善标准的超级vised 3D-HPE。</li>
<li>results: 作者们通过使用 Human,3.6M 数据集，证明了他们的方法的有效性和其与现有模型的超越。他们还证明了粉批模型能够更好地处理 occlusions，并且改善时间协调和 sagittal 对称性的预测。<details>
<summary>Abstract</summary>
We present an innovative approach to 3D Human Pose Estimation (3D-HPE) by integrating cutting-edge diffusion models, which have revolutionized diverse fields, but are relatively unexplored in 3D-HPE. We show that diffusion models enhance the accuracy, robustness, and coherence of human pose estimations. We introduce DiffHPE, a novel strategy for harnessing diffusion models in 3D-HPE, and demonstrate its ability to refine standard supervised 3D-HPE. We also show how diffusion models lead to more robust estimations in the face of occlusions, and improve the time-coherence and the sagittal symmetry of predictions. Using the Human\,3.6M dataset, we illustrate the effectiveness of our approach and its superiority over existing models, even under adverse situations where the occlusion patterns in training do not match those in inference. Our findings indicate that while standalone diffusion models provide commendable performance, their accuracy is even better in combination with supervised models, opening exciting new avenues for 3D-HPE research.
</details>
<details>
<summary>摘要</summary>
我们提出了一种创新的三维人姿估计（3D-HPE）方法，通过结合进步的扩散模型，这些模型在多个领域引发革命，但在3D-HPE中尚未得到广泛探索。我们表明，扩散模型可以提高人姿估计的准确性、可靠性和一致性。我们提出了一种新的战略——DiffHPE，用于在3D-HPE中利用扩散模型，并证明其能够改善标准的超级vised 3D-HPE。我们还表明，扩散模型可以在 occlusion 情况下提供更加稳定的估计，并且可以改善时间听应和 sagittal 准确性。使用 Human\,3.6M 数据集，我们证明了我们的方法的有效性，并与现有模型进行比较，即使在训练和推理中的 occlusion 情况不符。我们的发现表明，独立使用扩散模型可以提供优秀的性能，而与超级vised 模型结合使用可以提高性能，开启了3D-HPE研究的新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs"><a href="#Rail-Crack-Propagation-Forecasting-Using-Multi-horizons-RNNs" class="headerlink" title="Rail Crack Propagation Forecasting Using Multi-horizons RNNs"></a>Rail Crack Propagation Forecasting Using Multi-horizons RNNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01569">http://arxiv.org/abs/2309.01569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sara Yasmine Ouerk, Olivier Vo Van, Mouadh Yagoubi</li>
<li>for: 这个论文主要用于预测铁路裂隙长度升溃的问题，这个问题对于物料和结构的维护和安全评估具有重要意义。</li>
<li>methods: 这个论文使用了机器学习技术，特别是循环神经网络（RNN）来预测时间序列数据。这种方法可以模拟时间序列数据，并将外生变量纳入模型中。</li>
<li>results: 实验结果显示，使用多个时间预测 horizon的 bayesian multi-horizons 模型，可以更好地预测铁路裂隙长度升溃的趋势，比如LSTM和GRU等状态 искусственный神经网络模型。<details>
<summary>Abstract</summary>
The prediction of rail crack length propagation plays a crucial role in the maintenance and safety assessment of materials and structures. Traditional methods rely on physical models and empirical equations such as Paris law, which often have limitations in capturing the complex nature of crack growth. In recent years, machine learning techniques, particularly Recurrent Neural Networks (RNNs), have emerged as promising methods for time series forecasting. They allow to model time series data, and to incorporate exogenous variables into the model. The proposed approach involves collecting real data on the French rail network that includes historical crack length measurements, along with relevant exogenous factors that may influence crack growth. First, a pre-processing phase was performed to prepare a consistent data set for learning. Then, a suitable Bayesian multi-horizons recurrent architecture was designed to model the crack propagation phenomenon. Obtained results show that the Multi-horizons model outperforms state-of-the-art models such as LSTM and GRU.
</details>
<details>
<summary>摘要</summary>
“预测铁路裂解长度传播的预测具有关键的作用在物料和结构的维护和安全评估中。传统方法往往靠赖物理模型和实验方程式如巴黎法则，它们经常无法捕捉裂解生长的复杂性。在最近几年，机器学习技术，特别是回归神经网络（RNN），在时间序列预测方面表现出色。它们允许模型时间序列数据，并将外生因素 integrate到模型中。本研究的方法是收集了法国铁路网络的实际裂解长度测量数据，以及可能影响裂解生长的相关外生因素。首先，进行了一个预处理阶段，以确保数据集的一致性。然后，适用了一个适当的 bayesian multi-horizons recurrent架构，以模elling裂解传播现象。获得的结果显示，Multi-horizons模型在LSTM和GRU模型之上表现出色。”Note: Please note that the translation is in Simplified Chinese, and the word order and grammar may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking"><a href="#OutRank-Speeding-up-AutoML-based-Model-Search-for-Large-Sparse-Data-sets-with-Cardinality-aware-Feature-Ranking" class="headerlink" title="OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking"></a>OutRank: Speeding up AutoML-based Model Search for Large Sparse Data sets with Cardinality-aware Feature Ranking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01552">http://arxiv.org/abs/2309.01552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Blaž Škrlj, Blaž Mramor</li>
<li>for: 本研究旨在提高现代推荐系统的设计，包括了哪些特征空间是解决某个推荐任务的重要部分。</li>
<li>methods: 本研究使用了一种效果很好的算法来帮助发现有用的特征，即特征排名算法。此外，还使用了自动化机器学习（AutoML）技术来寻找更加紧凑且性能更高的模型。</li>
<li>results: 研究结果表明，使用OutRank系统可以在实际的Click-through-rate预测数据集上寻找更高性能的模型，并且可以在各种具有不同特征 cardinality的数据集上进行高效的模型搜索。此外，OutRank还可以快速地检测数据质量相关的异常情况。<details>
<summary>Abstract</summary>
The design of modern recommender systems relies on understanding which parts of the feature space are relevant for solving a given recommendation task. However, real-world data sets in this domain are often characterized by their large size, sparsity, and noise, making it challenging to identify meaningful signals. Feature ranking represents an efficient branch of algorithms that can help address these challenges by identifying the most informative features and facilitating the automated search for more compact and better-performing models (AutoML). We introduce OutRank, a system for versatile feature ranking and data quality-related anomaly detection. OutRank was built with categorical data in mind, utilizing a variant of mutual information that is normalized with regard to the noise produced by features of the same cardinality. We further extend the similarity measure by incorporating information on feature similarity and combined relevance. The proposed approach's feasibility is demonstrated by speeding up the state-of-the-art AutoML system on a synthetic data set with no performance loss. Furthermore, we considered a real-life click-through-rate prediction data set where it outperformed strong baselines such as random forest-based approaches. The proposed approach enables exploration of up to 300% larger feature spaces compared to AutoML-only approaches, enabling faster search for better models on off-the-shelf hardware.
</details>
<details>
<summary>摘要</summary>
现代推荐系统的设计需要理解哪些特征空间中的特征是解决某个推荐任务的关键。然而，实际世界数据集经常具有庞大、稀疏和噪声等特征，使得找到有意义的信号变得困难。特征排名算法是一种有效的方法，可以帮助解决这些挑战，并且可以自动搜索更紧凑和性能更高的模型（AutoML）。我们介绍了OutRank，一种多样化特征排名和数据质量相关异常检测的系统。OutRank采用了类别数据的视角，利用类别数据中特征之间的相互信息来 норmalize噪声。我们还将相互信息与共同相关性融合到相互信息中。我们的方法的可行性被证明了，通过加速现有AutoML系统在 sintetic数据集上的速度，而无损失性。此外，我们还考虑了一个真实的点击率预测数据集，其中OutRank exceeded strong baselines，如随机森林方法。我们的方法可以探索更大的特征空间，比AutoML-only方法多达300%，以便更快地搜索更好的模型。
</details></li>
</ul>
<hr>
<h2 id="Are-We-Using-Autoencoders-in-a-Wrong-Way"><a href="#Are-We-Using-Autoencoders-in-a-Wrong-Way" class="headerlink" title="Are We Using Autoencoders in a Wrong Way?"></a>Are We Using Autoencoders in a Wrong Way?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01532">http://arxiv.org/abs/2309.01532</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GabMartino/icrst_trst_autoencoder">https://github.com/GabMartino/icrst_trst_autoencoder</a></li>
<li>paper_authors: Gabriele Martino, Davide Moroni, Massimo Martinelli</li>
<li>for: 这个研究旨在改进完全自适应神经网络（Autoencoder）的标准训练方法，以增强其准确性和灵活性。</li>
<li>methods: 该研究使用了不同的整形方法，包括不使用显式规则化项的方法，以改变自适应神经网络的含义空间的形状。</li>
<li>results: 研究发现，通过修改含义空间的形状，可以提高自适应神经网络的准确性和灵活性。此外，研究还发现，在重建整个数据集中随机样本的情况下，含义空间的行为也具有重要的意义。<details>
<summary>Abstract</summary>
Autoencoders are certainly among the most studied and used Deep Learning models: the idea behind them is to train a model in order to reconstruct the same input data. The peculiarity of these models is to compress the information through a bottleneck, creating what is called Latent Space. Autoencoders are generally used for dimensionality reduction, anomaly detection and feature extraction. These models have been extensively studied and updated, given their high simplicity and power. Examples are (i) the Denoising Autoencoder, where the model is trained to reconstruct an image from a noisy one; (ii) Sparse Autoencoder, where the bottleneck is created by a regularization term in the loss function; (iii) Variational Autoencoder, where the latent space is used to generate new consistent data. In this article, we revisited the standard training for the undercomplete Autoencoder modifying the shape of the latent space without using any explicit regularization term in the loss function. We forced the model to reconstruct not the same observation in input, but another one sampled from the same class distribution. We also explored the behaviour of the latent space in the case of reconstruction of a random sample from the whole dataset.
</details>
<details>
<summary>摘要</summary>
自然语言处理中的Autoencoder是非常广泛研究和应用的深度学习模型，其核心思想是通过训练模型来重建输入数据。Autoencoder模型具有压缩信息的特点，创造了所谓的缺省空间（Latent Space）。这些模型通常用于维度减少、异常检测和特征提取。这些模型已经得到了广泛的研究和更新，因为它们具有高度的简单性和力量。例如，（i）噪声Autoencoder，其中模型通过噪声图像重建原始图像；（ii）稀疏Autoencoder，其中瓶颈是通过损失函数中的正则化项来创建的；（iii）变量Autoencoder，其中缺省空间用于生成新的一致数据。在这篇文章中，我们重新训练了标准的Autoencoder模型，不使用任何显式的正则化项在损失函数中。我们让模型重建不同的输入观测，而不是原始输入观测。我们还探索了在重建整个数据集中的行为。
</details></li>
</ul>
<hr>
<h2 id="Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data"><a href="#Passing-Heatmap-Prediction-Based-on-Transformer-Model-and-Tracking-Data" class="headerlink" title="Passing Heatmap Prediction Based on Transformer Model and Tracking Data"></a>Passing Heatmap Prediction Based on Transformer Model and Tracking Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01526">http://arxiv.org/abs/2309.01526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yisheng Pei, Varuna De Silva, Mike Caine</li>
<li>for: This research aims to provide a better understanding of how players’ off-ball movement contributes to their team’s defensive performance, and to develop a novel deep-learning network architecture that can predict the potential end location of passes and the impact of players’ movement before the pass.</li>
<li>methods: The research uses a novel deep-learning network architecture to predict the potential end location of passes and the impact of players’ movement before the pass, and analyzes more than 28,000 pass events to achieve a robust prediction with over 0.7 Top-1 accuracy.</li>
<li>results: The research finds that players’ off-ball movement has a significant impact on their team’s defensive performance, and provides a better understanding of how players’ movement over time contributes to the game strategy and final victory. The novel deep-learning network architecture developed in the research offers a better tool and metric for football analysts to evaluate players’ off-ball movement contribution.<details>
<summary>Abstract</summary>
Although the data-driven analysis of football players' performance has been developed for years, most research only focuses on the on-ball event including shots and passes, while the off-ball movement remains a little-explored area in this domain. Players' contributions to the whole match are evaluated unfairly, those who have more chances to score goals earn more credit than others, while the indirect and unnoticeable impact that comes from continuous movement has been ignored. This research presents a novel deep-learning network architecture which is capable to predict the potential end location of passes and how players' movement before the pass affects the final outcome. Once analysed more than 28,000 pass events, a robust prediction can be achieved with more than 0.7 Top-1 accuracy. And based on the prediction, a better understanding of the pitch control and pass option could be reached to measure players' off-ball movement contribution to defensive performance. Moreover, this model could provide football analysts a better tool and metric to understand how players' movement over time contributes to the game strategy and final victory.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)尽管数据驱动分析足球运动员表现已经在年份开发了几年，大多数研究仅集中于球场上的射门和传球事件，而偏离球场的运动员的贡献仍然是未探索的领域。运动员的整体贡献被不公正地评估，那些有更多的得分机会的运动员获得更多的赞誉，而不断移动的影响则被忽略。本研究提出了一种新的深度学习网络架构，可以预测传球的可能性结束位置以及运动员在传球之前的运动如何影响最终结果。经过分析超过28,000个传球事件，可以实现超过0.7 Top-1的准确率。基于预测结果，可以更好地理解球场控制和传球选择，并用这些指标来评估运动员的防守表现。此外，这个模型可以为足球分析师提供更好的工具和指标，以更好地理解运动员在时间上的运动如何影响游戏策略和最终胜利。
</details></li>
</ul>
<hr>
<h2 id="A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case"><a href="#A-Blackbox-Model-Is-All-You-Need-to-Breach-Privacy-Smart-Grid-Forecasting-Models-as-a-Use-Case" class="headerlink" title="A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case"></a>A Blackbox Model Is All You Need to Breach Privacy: Smart Grid Forecasting Models as a Use Case</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01523">http://arxiv.org/abs/2309.01523</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hussein Aly, Abdulaziz Al-Ali, Abdullah Al-Ali, Qutaibah Malluhi</li>
<li>for: 本研究探讨智能电网中forecasting模型的隐私风险，尤其是深度学习和启发式学习模型在智能电网应用中的隐私风险。</li>
<li>methods: 本研究使用了深度学习和启发式学习模型，包括Long Short Term Memory（LSTM）模型，来分析智能电网系统中的隐私风险。</li>
<li>results: 研究发现，forecasting模型可以泄露全局性和隐私威胁，而LSTM模型可以泄露大量信息，相当于直接访问数据本身，这显示了保护forecasting模型的重要性。<details>
<summary>Abstract</summary>
This paper investigates the potential privacy risks associated with forecasting models, with specific emphasis on their application in the context of smart grids. While machine learning and deep learning algorithms offer valuable utility, concerns arise regarding their exposure of sensitive information. Previous studies have focused on classification models, overlooking risks associated with forecasting models. Deep learning based forecasting models, such as Long Short Term Memory (LSTM), play a crucial role in several applications including optimizing smart grid systems but also introduce privacy risks. Our study analyzes the ability of forecasting models to leak global properties and privacy threats in smart grid systems. We demonstrate that a black box access to an LSTM model can reveal a significant amount of information equivalent to having access to the data itself (with the difference being as low as 1% in Area Under the ROC Curve). This highlights the importance of protecting forecasting models at the same level as the data.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究智能电网中预测模型的隐私风险，尤其是深度学习和预测模型在智能电网系统中的应用。虽然机器学习和深度学习算法提供了 valuabe的功能，但是隐私问题引起了关注，因为预测模型可能泄露敏感信息。先前的研究主要集中在分类模型上，忽略了预测模型中的隐私风险。深度学习基于的预测模型，如Long Short Term Memory（LSTM），在许多应用中扮演着关键的角色，包括智能电网系统的优化，但也会引起隐私风险。我们的研究分析了预测模型是否可以泄露全局性和隐私威胁在智能电网系统中。我们示出，通过访问LSTM模型，可以获取大量信息，与直接访问数据相当，差异只有1%左右（在ROC曲线下的面积）。这说明预测模型需要与数据一样受到保护。
</details></li>
</ul>
<hr>
<h2 id="Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning"><a href="#Hawkeye-Change-targeted-Testing-for-Android-Apps-based-on-Deep-Reinforcement-Learning" class="headerlink" title="Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning"></a>Hawkeye: Change-targeted Testing for Android Apps based on Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01519">http://arxiv.org/abs/2309.01519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chao Peng, Zhengwei Lv, Jiarong Fu, Jiayuan Liang, Zhao Zhang, Ajitha Rajan, Ping Yang</li>
<li>for: 本研究旨在提高Android应用程序更新的正确性，以避免在用户端引入潜在错误。</li>
<li>methods: 本研究提出了一种引导测试方法，使用深度强化学习来优先执行更新后影响的GUI操作。</li>
<li>results: 对10个开源App和1个商业App进行比较，发现 Hawkeye 能够更可靠地生成targeting更改的GUI事件序列，而 FastBot2 和 ARES 在开源App上表现较差。 Hawkeye 在小型开源App上也表现相对较好。此外，在商业应用程序的开发流水线中部署 Hawkeye 也显示了它的IDEA。<details>
<summary>Abstract</summary>
Android Apps are frequently updated to keep up with changing user, hardware, and business demands. Ensuring the correctness of App updates through extensive testing is crucial to avoid potential bugs reaching the end user. Existing Android testing tools generate GUI events focussing on improving the test coverage of the entire App rather than prioritising updates and its impacted elements. Recent research has proposed change-focused testing but relies on random exploration to exercise the updates and impacted GUI elements that is ineffective and slow for large complex Apps with a huge input exploration space. We propose directed testing of App updates with Hawkeye that is able to prioritise executing GUI actions associated with code changes based on deep reinforcement learning from historical exploration data. Our empirical evaluation compares Hawkeye with state-of-the-art model-based and reinforcement learning-based testing tools FastBot2 and ARES using 10 popular open-source and 1 commercial App. We find that Hawkeye is able to generate GUI event sequences targeting changed functions more reliably than FastBot2 and ARES for the open source Apps and the large commercial App. Hawkeye achieves comparable performance on smaller open source Apps with a more tractable exploration space. The industrial deployment of Hawkeye in the development pipeline also shows that Hawkeye is ideal to perform smoke testing for merge requests of a complicated commercial App.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval"><a href="#MultiWay-Adapater-Adapting-large-scale-multi-modal-models-for-scalable-image-text-retrieval" class="headerlink" title="MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"></a>MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01516">http://arxiv.org/abs/2309.01516</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/longkukuhi/multiway-adapter">https://github.com/longkukuhi/multiway-adapter</a></li>
<li>paper_authors: Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon Camarasa</li>
<li>for: 这篇论文的目的是提出一个框架，以增强大型多modal模型（LMMs）的适应性和转移性，以便在新任务上进行高效的适应。</li>
<li>methods: 这篇论文使用了一个名为“Alignment Enhancer”的新方法，它能够深入对模组进行调整，以提高模组之间的对齐性。这个方法只需要将LMMs中的少于1.25%的参数进行调整，即使不需要完全重新训练。</li>
<li>results: 这篇论文的实验结果显示，使用 Multiway-Adapter 框架可以在零传入情感领域中实现高效的适应，并且可以降低 fine-tuning 时间的耗用率，较之完全训练的模型更高。<details>
<summary>Abstract</summary>
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and effective adaptation pathway for LMMs, broadening their applicability. The source code is publicly available at: \url{https://github.com/longkukuhi/MultiWay-Adapter}.
</details>
<details>
<summary>摘要</summary>
To address these issues, we propose Multiway-Adapter, an innovative framework that incorporates an "Alignment Enhancer" to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25% additional parameters to LMMs, as demonstrated by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57% reduction in fine-tuning time.Our approach offers a resource-efficient and effective adaptation pathway for LMMs, expanding their applicability. The source code is publicly available at: <https://github.com/longkukuhi/MultiWay-Adapter>.
</details></li>
</ul>
<hr>
<h2 id="Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning"><a href="#Federated-cINN-Clustering-for-Accurate-Clustered-Federated-Learning" class="headerlink" title="Federated cINN Clustering for Accurate Clustered Federated Learning"></a>Federated cINN Clustering for Accurate Clustered Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01515">http://arxiv.org/abs/2309.01515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Zhou, Minjia Shi, Yuxin Tian, Yuanxi Li, Qing Ye, Jiancheng Lv</li>
<li>for: 这篇论文旨在提出一种用于聚合分布式机器学习的 Federated Learning (FL) 方法，并且可以实现高效的人群智能。</li>
<li>methods: 这篇论文提出的 Federated cINN Clustering Algorithm (FCCA) 使用全球Encoder将每个客户端的私人数据转换为多元 Gaussian 分布，然后使用生成模型进行最大 LIKELIHOOD 估计，以简化优化并避免模式崩溃。</li>
<li>results: 实验结果显示 FCCA 比其他已知的分布式 Federated Learning 算法更有优势，在不同的模型和数据集上进行评估。这些结果表明这种方法具有实际应用中 Federated Learning 任务的优化和精度。<details>
<summary>Abstract</summary>
Federated Learning (FL) presents an innovative approach to privacy-preserving distributed machine learning and enables efficient crowd intelligence on a large scale. However, a significant challenge arises when coordinating FL with crowd intelligence which diverse client groups possess disparate objectives due to data heterogeneity or distinct tasks. To address this challenge, we propose the Federated cINN Clustering Algorithm (FCCA) to robustly cluster clients into different groups, avoiding mutual interference between clients with data heterogeneity, and thereby enhancing the performance of the global model. Specifically, FCCA utilizes a global encoder to transform each client's private data into multivariate Gaussian distributions. It then employs a generative model to learn encoded latent features through maximum likelihood estimation, which eases optimization and avoids mode collapse. Finally, the central server collects converged local models to approximate similarities between clients and thus partition them into distinct clusters. Extensive experimental results demonstrate FCCA's superiority over other state-of-the-art clustered federated learning algorithms, evaluated on various models and datasets. These results suggest that our approach has substantial potential to enhance the efficiency and accuracy of real-world federated learning tasks.
</details>
<details>
<summary>摘要</summary>
联邦学习（FL）提出了一种革新的隐私保护分布式机器学习方法，实现大规模的人群智慧。然而，当联邦学习与人群智慧集成时，却会遇到一个挑战，那是由于客户组的数据多样性或对于不同任务的调整。为解决这个挑战，我们提出了联邦cINN排序算法（FCCA），以静态排序客户组，避免客户组之间的互相干扰，并将全球模型的性能提高。具体来说，FCCA使用全球编码器将每个客户组的私人数据转换为多元normal分布。然后，它运用生成模型学习编码特征，通过最大 LIKELIHOOD估计，实现优化和避免模型塌陷。最后，中央服务器收集了各客户组的融合模型，估计客户组之间的相似性，并将其分组。实验结果显示，FCCA与其他现有的分布式学习排序算法相比，在不同的模型和数据集上具有明显的优势。这些结果表明，我们的方法具有实际的应用潜力，以提高现实世界中的联邦学习任务的效率和准确性。
</details></li>
</ul>
<hr>
<h2 id="Memory-Efficient-Optimizers-with-4-bit-States"><a href="#Memory-Efficient-Optimizers-with-4-bit-States" class="headerlink" title="Memory Efficient Optimizers with 4-bit States"></a>Memory Efficient Optimizers with 4-bit States</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01507">http://arxiv.org/abs/2309.01507</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-ml/low-bit-optimizers">https://github.com/thu-ml/low-bit-optimizers</a></li>
<li>paper_authors: Bingrui Li, Jianfei Chen, Jun Zhu</li>
<li>for: 这篇论文目的是对训练神经网络时的优化器状态进行储存压缩，以降低训练内存负载。</li>
<li>methods: 本文使用了详细的实验分析，从第一个和第二个 moments 中获得了较低的位元数字宽度，并且使用了小尺寸的封包和列对组合来更好地量化。此外，本文还解决了量化第二个 moments 时出现的零点问题，使用了一个排除零点的直线量化器。</li>
<li>results: 本文在训练多种任务，包括自然语言理解、机器翻译、图像分类和指令调整等，都可以与全精度版本的优化器相比，具有更好的记忆储存效率。<details>
<summary>Abstract</summary>
Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizer is evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Layer-wise-training-for-self-supervised-learning-on-graphs"><a href="#Layer-wise-training-for-self-supervised-learning-on-graphs" class="headerlink" title="Layer-wise training for self-supervised learning on graphs"></a>Layer-wise training for self-supervised learning on graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01503">http://arxiv.org/abs/2309.01503</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Pina, Verónica Vilaplana</li>
<li>for: 本研究旨在开发一种可以在大 graphs 上进行端到端训练的图 neural network (GNN) 算法，以解决深度增加时的内存和计算复杂性问题。</li>
<li>methods: 我们提出了层 wise REGularized Graph Infomax（Layer-wise REG-GI）算法，它通过分解 GNN 中的特征传播和特征转换来学习节点表示，然后根据预测输入的估计来 derive 一个损失函数。</li>
<li>results: 我们在 inductive 大 graphs 中评估了该算法，并与其他端到端方法相比较，发现它具有更高的效率和相同的性能，可以在一个单个设备上训练更复杂的模型。此外，我们还发现该算法可以避免 oversmoothing 的问题。<details>
<summary>Abstract</summary>
End-to-end training of graph neural networks (GNN) on large graphs presents several memory and computational challenges, and limits the application to shallow architectures as depth exponentially increases the memory and space complexities. In this manuscript, we propose Layer-wise Regularized Graph Infomax, an algorithm to train GNNs layer by layer in a self-supervised manner. We decouple the feature propagation and feature transformation carried out by GNNs to learn node representations in order to derive a loss function based on the prediction of future inputs. We evaluate the algorithm in inductive large graphs and show similar performance to other end to end methods and a substantially increased efficiency, which enables the training of more sophisticated models in one single device. We also show that our algorithm avoids the oversmoothing of the representations, another common challenge of deep GNNs.
</details>
<details>
<summary>摘要</summary>
大规模图gnn的端到端训练存在内存和计算上的挑战，深度随着增加而 exponential 增加内存和空间复杂性。在这篇论文中，我们提出层 wise 常数化图信息媒体кс（Layer-wise Regularized Graph Infomax），一种在自适应模式下层个 Train GNN。我们将 GNN 中的特征传播和特征变换分解出来，以学习节点表示，并从这些表示中提取一个基于未来输入预测的损失函数。我们在 inductive 大raph 中评估了该算法，并与其他端到端方法相比，实现了相似的性能，同时提高了效率，使得可以在一个设备上训练更复杂的模型。此外，我们还证明了我们的算法可以避免深度 GNN 的抽象过滤。
</details></li>
</ul>
<hr>
<h2 id="On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging"><a href="#On-the-use-of-Mahalanobis-distance-for-out-of-distribution-detection-with-neural-networks-for-medical-imaging" class="headerlink" title="On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging"></a>On the use of Mahalanobis distance for out-of-distribution detection with neural networks for medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01488">http://arxiv.org/abs/2309.01488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/harryanthony/mahalanobis-ood-detection">https://github.com/harryanthony/mahalanobis-ood-detection</a></li>
<li>paper_authors: Harry Anthony, Konstantinos Kamnitsas</li>
<li>For: This paper aims to improve the detection of out-of-distribution (OOD) patterns in neural networks for medical applications.* Methods: The paper challenges the existing understanding that there is an optimal layer or combination of layers for applying Mahalanobis distance for OOD detection, and instead shows that the optimum layer changes depending on the type of OOD pattern. The paper also proposes separating the OOD detector into multiple detectors at different depths of the network to enhance robustness.* Results: The paper validates these insights on real-world OOD tasks using CheXpert chest X-rays with unseen pacemakers and unseen sex as OOD cases, and provides best-practices for the use of Mahalanobis distance for OOD detection.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是提高神经网络中的外围数据探测（OOD）性能，用于医疗应用。</li>
<li>methods: 该论文挑战现有的认知，即存在一个最佳层或层组的神经网络来应用 Mahalanobis 距离进行 OOD 探测，而是显示了 OOD 类型改变最佳层的问题。该论文还提议将 OOD 探测器分解成不同深度的网络中的多个探测器，以提高 robustness。</li>
<li>results: 论文 validate 这些发现在实际的 OOD 任务上，使用 CheXpert 胸部X射像素进行训练，并使用不同的 pacemaker 和性别作为 OOD 情况。结果提供了 OOD 探测中 Mahalanobis 距离的最佳做法。 manually 标注的 pacemaker 标签和项目代码可以在 GitHub 上获取：<a target="_blank" rel="noopener" href="https://github.com/HarryAnthony/Mahalanobis-OOD-detection%E3%80%82">https://github.com/HarryAnthony/Mahalanobis-OOD-detection。</a><details>
<summary>Abstract</summary>
Implementing neural networks for clinical use in medical applications necessitates the ability for the network to detect when input data differs significantly from the training data, with the aim of preventing unreliable predictions. The community has developed several methods for out-of-distribution (OOD) detection, within which distance-based approaches - such as Mahalanobis distance - have shown potential. This paper challenges the prevailing community understanding that there is an optimal layer, or combination of layers, of a neural network for applying Mahalanobis distance for detection of any OOD pattern. Using synthetic artefacts to emulate OOD patterns, this paper shows the optimum layer to apply Mahalanobis distance changes with the type of OOD pattern, showing there is no one-fits-all solution. This paper also shows that separating this OOD detector into multiple detectors at different depths of the network can enhance the robustness for detecting different OOD patterns. These insights were validated on real-world OOD tasks, training models on CheXpert chest X-rays with no support devices, then using scans with unseen pacemakers (we manually labelled 50% of CheXpert for this research) and unseen sex as OOD cases. The results inform best-practices for the use of Mahalanobis distance for OOD detection. The manually annotated pacemaker labels and the project's code are available at: https://github.com/HarryAnthony/Mahalanobis-OOD-detection.
</details>
<details>
<summary>摘要</summary>
实施神经网络在医疗应用中需要神经网络能够检测输入数据与训练数据之间的差异，以避免不可靠的预测。社区已经开发出多种对外生数据（OOD）检测方法，其中距离基本方法，如 Mahalanobis 距离，有潜力。这篇论文探讨了社区对神经网络应用 Mahalanobis 距离时的共识，即存在一个最佳层或组合层可以检测任何 OOD 模式。使用人工生成的 artifacts 模拟 OOD 模式，这篇论文显示了应用 Mahalanobis 距离的最佳层不同于 OOD 模式类型，表明没有一个通用的解决方案。此外，这篇论文还表明，将 OOD 检测器分解到神经网络的不同层次可以提高对不同 OOD 模式的检测稳定性。这些发现得到了实际 OOD 任务的验证，使用无支持设备训练 CheXpert 胸部X射像，然后使用未看到过 pacemaker 和未看到过性别为 OOD 例子。结果提供了对 Mahalanobis 距离的使用最佳做法的指导，以及相关的手动标注 pacemaker 标签和项目代码，可以在 GitHub 上获取：https://github.com/HarryAnthony/Mahalanobis-OOD-detection。
</details></li>
</ul>
<hr>
<h2 id="CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification"><a href="#CA2-Class-Agnostic-Adaptive-Feature-Adaptation-for-One-class-Classification" class="headerlink" title="CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification"></a>CA2: Class-Agnostic Adaptive Feature Adaptation for One-class Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01483">http://arxiv.org/abs/2309.01483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zilong Zhang, Zhibin Zhao, Deyu Meng, Xingwu Zhang, Xuefeng Chen</li>
<li>for: 提高机器学习模型在实际应用中的可靠性，即一类分类（OCC）问题。</li>
<li>methods: 使用预训练特征进行适应，并对目标数据集进行类型不确定的情况下进行适应。</li>
<li>results: 在不同的训练数据类型下，包括1到1024个类，都能够提高OCC性能，并且超越了当前的状态艺术方法。<details>
<summary>Abstract</summary>
One-class classification (OCC), i.e., identifying whether an example belongs to the same distribution as the training data, is essential for deploying machine learning models in the real world. Adapting the pre-trained features on the target dataset has proven to be a promising paradigm for improving OCC performance. Existing methods are constrained by assumptions about the number of classes. This contradicts the real scenario where the number of classes is unknown. In this work, we propose a simple class-agnostic adaptive feature adaptation method (CA2). We generalize the center-based method to unknown classes and optimize this objective based on the prior existing in the pre-trained network, i.e., pre-trained features that belong to the same class are adjacent. CA2 is validated to consistently improve OCC performance across a spectrum of training data classes, spanning from 1 to 1024, outperforming current state-of-the-art methods. Code is available at https://github.com/zhangzilongc/CA2.
</details>
<details>
<summary>摘要</summary>
一类分类（OCC），即确定例子属于训练数据的同一分布，是机器学习模型在实际应用中的重要任务。适应预训练特征到目标数据集的方法已经证明是提高OCC性能的有效方法。现有方法受到类数假设的限制，这与实际情况不符。在这项工作中，我们提出了一种简单的类型不假设的适应特征适应方法（CA2）。我们扩展了中心基于方法到未知类，并基于预训练网络中的先前存在的假设，即预训练特征属于同一类的情况下进行优化。CA2在训练数据类的范围从1到1024之间，并且在不同类型的训练数据上表现出色，超越当前的状态艺术方法。代码可以在https://github.com/zhangzilongc/CA2上下载。
</details></li>
</ul>
<hr>
<h2 id="FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation"><a href="#FinDiff-Diffusion-Models-for-Financial-Tabular-Data-Generation" class="headerlink" title="FinDiff: Diffusion Models for Financial Tabular Data Generation"></a>FinDiff: Diffusion Models for Financial Tabular Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01472">http://arxiv.org/abs/2309.01472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Timur Sattarov, Marco Schreyer, Damian Borth</li>
<li>For:  This paper aims to provide a solution for the challenge of sharing microdata, such as fund holdings and derivative instruments, by regulatory institutions, by using generative models to synthesize data that mimics the underlying distributions of real-world data.* Methods: The paper introduces ‘FinDiff’, a diffusion model that uses embedding encodings to model mixed modality financial data, and evaluates its performance in generating synthetic tabular financial data against state-of-the-art baseline models using three real-world financial datasets.* Results: The results show that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.<details>
<summary>Abstract</summary>
The sharing of microdata, such as fund holdings and derivative instruments, by regulatory institutions presents a unique challenge due to strict data confidentiality and privacy regulations. These challenges often hinder the ability of both academics and practitioners to conduct collaborative research effectively. The emergence of generative models, particularly diffusion models, capable of synthesizing data mimicking the underlying distributions of real-world data presents a compelling solution. This work introduces 'FinDiff', a diffusion model designed to generate real-world financial tabular data for a variety of regulatory downstream tasks, for example economic scenario modeling, stress tests, and fraud detection. The model uses embedding encodings to model mixed modality financial data, comprising both categorical and numeric attributes. The performance of FinDiff in generating synthetic tabular financial data is evaluated against state-of-the-art baseline models using three real-world financial datasets (including two publicly available datasets and one proprietary dataset). Empirical results demonstrate that FinDiff excels in generating synthetic tabular financial data with high fidelity, privacy, and utility.
</details>
<details>
<summary>摘要</summary>
共享微数据，如基金投资和 derivate 工具，由 regulatory 机构提供的挑战具有坚实的数据保密和隐私规定，这些挑战经常阻碍学者和实践者进行合作研究。随着生成模型的出现，特别是扩散模型，可以模拟实际世界数据的下WFDistribution，这提供了一个吸引人的解决方案。本文介绍了 'FinDiff'，一种适用于生成实际世界金融表格数据的扩散模型，用于经济enario模拟、压力测试和欺诈探测等下游任务。FinDiff 使用嵌入编码来模型金融数据的混合模式特征，包括分类和数值特征。FinDiff 在生成Synthetic tabular financial data的性能上与state-of-the-art基eline模型进行比较，使用三个真实世界金融数据集（其中两个公共可用数据集和一个专用数据集）进行实验。实验结果表明，FinDiff 能够生成高度准确、隐私和有用的Synthetic tabular financial data。
</details></li>
</ul>
<hr>
<h2 id="Pure-Monte-Carlo-Counterfactual-Regret-Minimization"><a href="#Pure-Monte-Carlo-Counterfactual-Regret-Minimization" class="headerlink" title="Pure Monte Carlo Counterfactual Regret Minimization"></a>Pure Monte Carlo Counterfactual Regret Minimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03084">http://arxiv.org/abs/2309.03084</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ju Qi, Ting Feng, Falun Hei, Zhemei Fang, Yunfeng Luo</li>
<li>for:  solves large-scale incomplete information games</li>
<li>methods:  builds upon CFR and Fictitious Play, combines counterfactual regret and best response strategy</li>
<li>results:  achieves better performance, reduces time and space complexity, and converges faster than MCCFR with a new warm start algorithm<details>
<summary>Abstract</summary>
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strategies elimination method. Consequently, the PMCCFR with new warm start algorithm can converge by two orders of magnitude faster than the CFR+ algorithm.
</details>
<details>
<summary>摘要</summary>
大规模不完整信息游戏的最佳算法是Counterfactual Regret Minimization（CFR）和其变种。这篇论文基于CFR，提出了一种新的算法named Pure CFR（PCFR），以实现更高的性能。PCFR可以看作CFR和Fictitious Play（FP）的组合，继承CFR中的反factual regret（价值）概念，并使用下一轮的最佳回应策略而不是 regret matching策略。我们的理论证明，PCFR可以 дости到黑套接近性，这使得PCFR可以与任何CFR变种，包括Monte Carlo CFR（MCCFR）结合。结果，得到的Pure MCCFR（PMCCFR）可以大幅降低时间和空间复杂度。尤其是，PMCCFR的整合速度至少三倍于MCCFR的整合速度。此外，由于PMCCFR不通过严格dominated策略的路径，我们开发了一种新的温开始算法，它是基于严格dominated策略的消除方法。因此，PMCCFR与新的温开始算法可以在CFR+算法的两个数量级更快 converges。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning"><a href="#Leveraging-Reward-Consistency-for-Interpretable-Feature-Discovery-in-Reinforcement-Learning" class="headerlink" title="Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning"></a>Leveraging Reward Consistency for Interpretable Feature Discovery in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01458">http://arxiv.org/abs/2309.01458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Huanqian Wang, Mukun Tong, Wenjie Shi, Gao Huang, Shiji Song</li>
<li>for: 该研究旨在解释和解释深度强化学习（RL）Agent的黑盒性质，以便在实际应用中使用。</li>
<li>methods: 该研究提出了一种基于奖励（reward）的解释方法，即RL-in-RL框架，以维护奖励一致性并实现高质量的特征归属。</li>
<li>results: 研究在Atari 2600游戏和Duckietown自驾汽车 simulate环境中进行了验证和评估，结果表明，该方法可以保持奖励一致性并实现高质量的特征归属。此外，一系列的分析实验也证明了动作匹配原则的局限性。<details>
<summary>Abstract</summary>
The black-box nature of deep reinforcement learning (RL) hinders them from real-world applications. Therefore, interpreting and explaining RL agents have been active research topics in recent years. Existing methods for post-hoc explanations usually adopt the action matching principle to enable an easy understanding of vision-based RL agents. In this paper, it is argued that the commonly used action matching principle is more like an explanation of deep neural networks (DNNs) than the interpretation of RL agents. It may lead to irrelevant or misplaced feature attribution when different DNNs' outputs lead to the same rewards or different rewards result from the same outputs. Therefore, we propose to consider rewards, the essential objective of RL agents, as the essential objective of interpreting RL agents as well. To ensure reward consistency during interpretable feature discovery, a novel framework (RL interpreting RL, denoted as RL-in-RL) is proposed to solve the gradient disconnection from actions to rewards. We verify and evaluate our method on the Atari 2600 games as well as Duckietown, a challenging self-driving car simulator environment. The results show that our method manages to keep reward (or return) consistency and achieves high-quality feature attribution. Further, a series of analytical experiments validate our assumption of the action matching principle's limitations.
</details>
<details>
<summary>摘要</summary>
深度强化学习（RL）的黑盒特性使其在实际应用中受限。因此，解释和解释RL代理的研究成为了近年active topic。现有的后续解释方法通常采用行动匹配原则来使得视觉RL代理更易于理解。在这篇文章中， argue that通常使用的行动匹配原则更多地是对深度神经网络（DNNs）的解释，而不是RL代理的解释。这可能会导致不相关或错位的特征归因，因为不同的DNNs输出可能导致同样的奖励，或者同样的奖励可能来自不同的输出。因此，我们提议将奖励作为RL代理解释的关键对象，以确保在可见特征发现过程中保持奖励一致性。为解决动作和奖励之间的梯度分离问题，我们提出了一种新的框架（RL解释RL，简称为RL-in-RL）。我们在Atari 2600游戏和Difficult自驾车 simulator环境中进行了验证和评估。结果表明，我们的方法可以保持奖励一致性，并实现高质量的特征归因。此外，一系列的分析实验validate了我们对行动匹配原则的假设的限制。
</details></li>
</ul>
<hr>
<h2 id="On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification"><a href="#On-the-Consistency-and-Robustness-of-Saliency-Explanations-for-Time-Series-Classification" class="headerlink" title="On the Consistency and Robustness of Saliency Explanations for Time Series Classification"></a>On the Consistency and Robustness of Saliency Explanations for Time Series Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01457">http://arxiv.org/abs/2309.01457</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chiara Balestra, Bin Li, Emmanuel Müller</li>
<li>for: 本文旨在探讨时序序列数据的解释和解释模型的稳定性和一致性问题。</li>
<li>methods: 本文使用了两种常见的解释模型，即干扰基因模型和梯度基因模型，来生成时序序列数据的解释。</li>
<li>results: 实验结果表明，这两种解释模型在五个实际 dataset 上均存在一定的不稳定和不一致性问题，无法提供可靠的解释。<details>
<summary>Abstract</summary>
Interpretable machine learning and explainable artificial intelligence have become essential in many applications. The trade-off between interpretability and model performance is the traitor to developing intrinsic and model-agnostic interpretation methods. Although model explanation approaches have achieved significant success in vision and natural language domains, explaining time series remains challenging. The complex pattern in the feature domain, coupled with the additional temporal dimension, hinders efficient interpretation. Saliency maps have been applied to interpret time series windows as images. However, they are not naturally designed for sequential data, thus suffering various issues.   This paper extensively analyzes the consistency and robustness of saliency maps for time series features and temporal attribution. Specifically, we examine saliency explanations from both perturbation-based and gradient-based explanation models in a time series classification task. Our experimental results on five real-world datasets show that they all lack consistent and robust performances to some extent. By drawing attention to the flawed saliency explanation models, we motivate to develop consistent and robust explanations for time series classification.
</details>
<details>
<summary>摘要</summary>
《机器学习可解释性和人工智能可解释性在许多应用中变得必备。模型性能和可解释性之间的质量是发展内在和模型自适应解释方法的障碍。虽然模型解释方法在视觉和自然语言领域得到了显著成功，但是解释时间序列仍然具有挑战。时间序列特征的复杂pattern，加上额外的时间维度，使得有效的解释受到阻碍。在图像上应用saliency map来解释时间序列窗口，但这些map不是专门为sequential数据设计的，因此会出现各种问题。》This paper conducts an extensive analysis of the consistency and robustness of saliency maps for time series features and temporal attribution. We examine saliency explanations from both perturbation-based and gradient-based explanation models in a time series classification task. Our experimental results on five real-world datasets show that they all lack consistent and robust performances to some extent. By drawing attention to the flawed saliency explanation models, we motivate the development of consistent and robust explanations for time series classification.
</details></li>
</ul>
<hr>
<h2 id="Toward-Defensive-Letter-Design"><a href="#Toward-Defensive-Letter-Design" class="headerlink" title="Toward Defensive Letter Design"></a>Toward Defensive Letter Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01452">http://arxiv.org/abs/2309.01452</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rprokap/pset-9">https://github.com/rprokap/pset-9</a></li>
<li>paper_authors: Rentaro Kataoka, Akisato Kimura, Seiichi Uchida</li>
<li>for: 防御 adversarial 攻击，提高图像分类器的抗击能力，并不关心图像中的视觉对象（如熊猫和车辆）的安全性。</li>
<li>methods: 使用 Iterative Fast Gradient Sign Method (I-FGSM) 测试字母图像的防御能力，并建立深度回归模型来估算每个字母图像的防御能力。提议基于生成 adversarial 网络 (GAN) 的两步方法来生成具有更高防御能力的字母图像。</li>
<li>results: 通过测试和分析，发现字母图像具有较高的防御能力，并且可以通过生成新的字母图像来提高防御能力。<details>
<summary>Abstract</summary>
A major approach for defending against adversarial attacks aims at controlling only image classifiers to be more resilient, and it does not care about visual objects, such as pandas and cars, in images. This means that visual objects themselves cannot take any defensive actions, and they are still vulnerable to adversarial attacks. In contrast, letters are artificial symbols, and we can freely control their appearance unless losing their readability. In other words, we can make the letters more defensive to the attacks. This paper poses three research questions related to the adversarial vulnerability of letter images: (1) How defensive are the letters against adversarial attacks? (2) Can we estimate how defensive a given letter image is before attacks? (3) Can we control the letter images to be more defensive against adversarial attacks? For answering the first and second questions, we measure the defensibility of letters by employing Iterative Fast Gradient Sign Method (I-FGSM) and then build a deep regression model for estimating the defensibility of each letter image. We also propose a two-step method based on a generative adversarial network (GAN) for generating character images with higher defensibility, which solves the third research question.
</details>
<details>
<summary>摘要</summary>
一种主要方法 для防御对抗攻击是控制图像分类器更加抗御，而不在意图像中的视觉对象，如�anda和车。这意味着视觉对象本身无法采取任何防御行动，仍然易受到对抗攻击。然而，字符是人工符号，我们可以自由地控制它们的外表，除非失去可读性。这意味着我们可以使字符更加抗御对抗攻击。这篇论文提出了三个研究问题关于对字符图像的抗御敏感性：1. 字符是如何抗御对抗攻击的？2. 我们可以在进行攻击之前对给定字符图像进行评估其抗御性吗？3. 我们可以通过生成推论网络（GAN）来生成更加抗御的字符图像吗？为回答第一个和第二个问题，我们使用迭代快速梯度签名方法（I-FGSM）测量字符的抗御性，然后建立深度回归模型来预测每个字符图像的抗御性。我们还提出了一种基于GAN的两步方法，可以生成更加抗御的字符图像，解决第三个研究问题。
</details></li>
</ul>
<hr>
<h2 id="Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks"><a href="#Effective-Multi-Graph-Neural-Networks-for-Illicit-Account-Detection-on-Cryptocurrency-Transaction-Networks" class="headerlink" title="Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks"></a>Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02460">http://arxiv.org/abs/2309.02460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhihao Ding, Jieming Shi, Qing Li, Jiannong Cao</li>
<li>for: 这篇研究是为了探索黑钱账户探测在加密货币交易网络上，以减少在线金融市场中发生的损失。</li>
<li>methods: 这篇研究使用了一种名为DIAM的新型多графі神经网络模型，它包括了Edge2Seq模块和多графі异常性（MGD）模块，以及一个实际上的终端训练方法。</li>
<li>results: 根据实验结果，DIAM在4个大加密货币数据集上的比较中，实现了最高的准确率和效率，比如在一个 Bitcoin 数据集上，DIAM 的 F1 分数为 96.55%，较最好的竞争者高出 83.92%。<details>
<summary>Abstract</summary>
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing the multigraph topology, DIAM employs a new Multigraph Discrepancy (MGD) module with a well-designed message passing mechanism to capture the discrepant features between normal and illicit nodes, supported by an attention mechanism. Assembling all techniques, DIAM is trained in an end-to-end manner. Extensive experiments, comparing against 14 existing solutions on 4 large cryptocurrency datasets of Bitcoin and Ethereum, demonstrate that DIAM consistently achieves the best performance to accurately detect illicit accounts, while being efficient. For instance, on a Bitcoin dataset with 20 million nodes and 203 million edges, DIAM achieves F1 score 96.55%, significantly higher than the F1 score 83.92% of the best competitor.
</details>
<details>
<summary>摘要</summary>
我们研究 криптовалютных交易网络上的非法帐户检测，这些网络在在线金融市场中变得越来越重要。非法活动的上升导致了数百亿元的损失，从正常用户手中。现有的解决方案可以分为两类：一是 tedious feature engineering 来获取手工特征，二是不充分利用 криптовалю transaction 数据的 semantics，因此效果不佳。在这篇论文中，我们将非法帐户检测问题定义为一个分类任务，并提出了 DIAM，一种新的多格 neural network 模型，用于有效地检测 криптовалюTransaction 网络上的非法帐户。首先，DIAM 包括一个 Edge2Seq 模块，可以自动学习有效的节点表示，保留并行边的交易模式特征，通过考虑边Attributes和导向边顺序依赖关系。然后，DIAM 使用一种新的多格缺失（MGD）模块，通过一种 Well-designed 的消息传递机制，捕捉非法节点与正常节点之间的不一致特征，并通过注意力机制进行强调。将所有技术组合在一起，DIAM 在端到端方式进行训练。对于 14 个现有解决方案，我们进行了广泛的实验，测试了在 4 个大型 криптовалюTransaction 数据集上的性能。结果表明，DIAM 在检测非法帐户方面具有最高的精度，同时具有高效性。例如，在一个 Bitcoin 数据集上，DIAM 的 F1 分数为 96.55%，远高于最佳竞争者的 F1 分数 83.92%。
</details></li>
</ul>
<hr>
<h2 id="Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance"><a href="#Hundreds-Guide-Millions-Adaptive-Offline-Reinforcement-Learning-with-Expert-Guidance" class="headerlink" title="Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance"></a>Hundreds Guide Millions: Adaptive Offline Reinforcement Learning with Expert Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01448">http://arxiv.org/abs/2309.01448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qisen Yang, Shenzhi Wang, Qihang Zhang, Gao Huang, Shiji Song</li>
<li>for: 提高Offline Reinforcement Learning（RL）的策略优化效果，解决分布shift问题。</li>
<li>methods: 提出了一种基于引导网络的Plug-in方法，使用只需几个专家示范来自适应性地确定每个样本的策略改进和策略限制的重要性。</li>
<li>results: 经过广泛的实验表明，GORL可以轻松地安装在大多数Offline RL算法上，并且具有 statistically significant performance improvements。<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) optimizes the policy on a previously collected dataset without any interactions with the environment, yet usually suffers from the distributional shift problem. To mitigate this issue, a typical solution is to impose a policy constraint on a policy improvement objective. However, existing methods generally adopt a ``one-size-fits-all'' practice, i.e., keeping only a single improvement-constraint balance for all the samples in a mini-batch or even the entire offline dataset. In this work, we argue that different samples should be treated with different policy constraint intensities. Based on this idea, a novel plug-in approach named Guided Offline RL (GORL) is proposed. GORL employs a guiding network, along with only a few expert demonstrations, to adaptively determine the relative importance of the policy improvement and policy constraint for every sample. We theoretically prove that the guidance provided by our method is rational and near-optimal. Extensive experiments on various environments suggest that GORL can be easily installed on most offline RL algorithms with statistically significant performance improvements.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese<</SYS>>Offline reinforcement learning (RL) 优化策略基于先前收集的数据集，无需与环境交互，然而通常受到分布问题的影响。为解决这个问题，一般采用策略约束对策进步目标进行做出约束。然而，现有方法通常采用“一个适用于所有样本”的做法，即保留每个样本集中或者整个offline数据集中的所有样本的一个改进-约束平衡。在这种情况下，我们认为不同的样本应该被对待不同的策略约束强度。基于这个想法，我们提出了一种名为指导式OfflineRL（GORL）的新方法。GORL使用一个引导网络，以及只需几个专家示范，来动态确定每个样本的策略改进和策略约束之间的相对重要性。我们证明了我们的方法提供的指导是理性的和近似优化的。广泛的实验表明，GORL可以轻松地在大多数OfflineRL算法上安装，并且具有 statistically significant的性能提升。
</details></li>
</ul>
<hr>
<h2 id="Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity"><a href="#Expanding-Mars-Climate-Modeling-Interpretable-Machine-Learning-for-Modeling-MSL-Relative-Humidity" class="headerlink" title="Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity"></a>Expanding Mars Climate Modeling: Interpretable Machine Learning for Modeling MSL Relative Humidity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01424">http://arxiv.org/abs/2309.01424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nour Abdelmoneim, Dattaraj B. Dhuri, Dimitra Atri, Germán Martínez<br>for: This paper aims to improve the accuracy and efficiency of Martian climate modeling using machine learning techniques.methods: The authors use a deep neural network to model relative humidity in Gale Crater, based on simulated meteorological variables from a Global Circulation Model. They also utilize an interpretable model architecture to analyze the internal mechanisms and decision-making processes of the model.results: The authors achieve a mean error of 3% and an $R^2$ score of 0.92 in predicting relative humidity, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. The approach can also be used to fill spatial and temporal gaps in observations, providing a fast and efficient method for modeling climate variables on Mars.Here is the Chinese translation of the three points:for: 这篇论文目的是使用机器学习技术提高火星气候模拟的准确性和效率。methods: 作者使用深度神经网络来模拟加莱沟的相对湿度，基于火星全球气候模型生成的 simulate 的物理变量。他们还利用可解释的模型架构来分析模型内部的机制和决策过程。results: 作者在predicting相对湿度方面达到了3%的平均误差和0.92的$R^2$分数，其中月均表层水层厚度、气层高度、循环风速和太阳zenith角被确定为主要的预测因素。此方法可以填充观测数据中的空间和时间间隔，提供一种快速和高效的火星气候模拟方法。<details>
<summary>Abstract</summary>
For the past several decades, numerous attempts have been made to model the climate of Mars with extensive studies focusing on the planet's dynamics and the understanding of its climate. While physical modeling and data assimilation approaches have made significant progress, uncertainties persist in comprehensively capturing and modeling the complexities of Martian climate. In this work, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory ``Curiosity'' rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3\% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details>
<details>
<summary>摘要</summary>
For several decades, numerous attempts have been made to model the climate of Mars, with extensive studies focusing on the planet's dynamics and understanding its climate. Although physical modeling and data assimilation approaches have made significant progress, uncertainties still exist in comprehensively capturing and modeling the complexities of Martian climate. In this study, we propose a novel approach to Martian climate modeling by leveraging machine learning techniques that have shown remarkable success in Earth climate modeling. Our study presents a deep neural network designed to accurately model relative humidity in Gale Crater, as measured by NASA's Mars Science Laboratory "Curiosity" rover. By utilizing simulated meteorological variables produced by the Mars Planetary Climate Model, a robust Global Circulation Model, our model accurately predicts relative humidity with a mean error of 3% and an $R^2$ score of 0.92. Furthermore, we present an approach to predict quantile ranges of relative humidity, catering to applications that require a range of values. To address the challenge of interpretability associated with machine learning models, we utilize an interpretable model architecture and conduct an in-depth analysis of its internal mechanisms and decision-making processes. We find that our neural network can effectively model relative humidity at Gale crater using a few meteorological variables, with the monthly mean surface H$_2$O layer, planetary boundary layer height, convective wind speed, and solar zenith angle being the primary contributors to the model predictions. In addition to providing a fast and efficient method to modeling climate variables on Mars, this modeling approach can also be used to expand on current datasets by filling spatial and temporal gaps in observations.
</details></li>
</ul>
<hr>
<h2 id="Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging"><a href="#Towards-frugal-unsupervised-detection-of-subtle-abnormalities-in-medical-imaging" class="headerlink" title="Towards frugal unsupervised detection of subtle abnormalities in medical imaging"></a>Towards frugal unsupervised detection of subtle abnormalities in medical imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02458">http://arxiv.org/abs/2309.02458</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/geoffroyo/onlineem">https://github.com/geoffroyo/onlineem</a></li>
<li>paper_authors: Geoffroy Oudoumanessah, Carole Lartizien, Michel Dojat, Florence Forbes</li>
<li>for: 这篇论文目的是为了探讨医疗影像中的异常检测，特别是在没有标注的情况下。</li>
<li>methods: 这篇论文使用了混合分布的方法，这些方法具有广泛适用于不同数据和任务的特点，并且不需要过多的设计或对应。</li>
<li>results: 这篇论文的结果显示，这些混合分布方法可以实现高精度和高效的异常检测，并且可以处理大量数据。特别是在检测 Parkinson 病患的 MR 脑成像中，可以显示出病理变化，并且与 Hoehn 和 Yahr scale的变化相符。<details>
<summary>Abstract</summary>
Anomaly detection in medical imaging is a challenging task in contexts where abnormalities are not annotated. This problem can be addressed through unsupervised anomaly detection (UAD) methods, which identify features that do not match with a reference model of normal profiles. Artificial neural networks have been extensively used for UAD but they do not generally achieve an optimal trade-o$\hookleftarrow$ between accuracy and computational demand. As an alternative, we investigate mixtures of probability distributions whose versatility has been widely recognized for a variety of data and tasks, while not requiring excessive design e$\hookleftarrow$ort or tuning. Their expressivity makes them good candidates to account for complex multivariate reference models. Their much smaller number of parameters makes them more amenable to interpretation and e cient learning. However, standard estimation procedures, such as the Expectation-Maximization algorithm, do not scale well to large data volumes as they require high memory usage. To address this issue, we propose to incrementally compute inferential quantities. This online approach is illustrated on the challenging detection of subtle abnormalities in MR brain scans for the follow-up of newly diagnosed Parkinsonian patients. The identified structural abnormalities are consistent with the disease progression, as accounted by the Hoehn and Yahr scale.
</details>
<details>
<summary>摘要</summary>
医学成像异常检测在没有标注异常情况下是一个挑战。这个问题可以通过无监督异常检测（USAD）方法解决，这些方法可以标识不符合参照模型的normal profile。人工神经网络已经广泛应用于USAD，但它们通常不能实现最佳的均衡 между精度和计算开销。作为一个alternative，我们研究混合分布的使用。这种方法的 universality 在多种数据和任务中得到了广泛的认可，而不需要过度的设计或调整。它们的表达能力使得它们成为质量异常检测的好 кандидат。但标准估计过程，如期望最大化算法，不适用于大量数据，因为它们需要高的内存使用。为解决这个问题，我们提议逐步计算推理量。这种在线方法在MR brain scan中检测parkinsonian patients的轻微异常情况上进行了示例。检测到的结构异常情况与疾病进程相符，如根据Hoehn和Yahr scale的评估。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design"><a href="#Leveraging-Self-Supervised-Vision-Transformers-for-Neural-Transfer-Function-Design" class="headerlink" title="Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design"></a>Leveraging Self-Supervised Vision Transformers for Neural Transfer Function Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01408">http://arxiv.org/abs/2309.01408</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dominik Engel, Leon Sick, Timo Ropinski</li>
<li>for: 这篇论文主要关注于volume rendering中的传输函数设定，它们是用于分类结构物和赋予光学性质的1D或2D函数。</li>
<li>methods: 该论文提出了一种基于自我supervised pre-trained vision transformers的新方法，用于互动地定义传输函数。用户只需在slice viewer中选择结构物，方法会自动选择相似的结构物基于由神经网络提取的高级特征。</li>
<li>results: 与传统学习基于的方法相比，该方法不需要训练模型，允许快速的推理，使得可以在几秒钟内设定传输函数，从而实现互动的探索。此外，该方法可以减少必要的标注量，通过在用户设定传输函数时提供反馈，使得用户可以更专注于需要标注的结构物。<details>
<summary>Abstract</summary>
In volume rendering, transfer functions are used to classify structures of interest, and to assign optical properties such as color and opacity. They are commonly defined as 1D or 2D functions that map simple features to these optical properties. As the process of designing a transfer function is typically tedious and unintuitive, several approaches have been proposed for their interactive specification. In this paper, we present a novel method to define transfer functions for volume rendering by leveraging the feature extraction capabilities of self-supervised pre-trained vision transformers. To design a transfer function, users simply select the structures of interest in a slice viewer, and our method automatically selects similar structures based on the high-level features extracted by the neural network. Contrary to previous learning-based transfer function approaches, our method does not require training of models and allows for quick inference, enabling an interactive exploration of the volume data. Our approach reduces the amount of necessary annotations by interactively informing the user about the current classification, so they can focus on annotating the structures of interest that still require annotation. In practice, this allows users to design transfer functions within seconds, instead of minutes. We compare our method to existing learning-based approaches in terms of annotation and compute time, as well as with respect to segmentation accuracy. Our accompanying video showcases the interactivity and effectiveness of our method.
</details>
<details>
<summary>摘要</summary>
在Volume Rendering中，传输函数用于分类结构体 интереса，并将光学性质如颜色和透明度赋予这些结构体。传输函数通常是1D或2D函数，它们将简单特征映射到这些光学性质。由于设计传输函数的过程通常是繁琐和不直观的，因此有几种方法被提出来对其进行交互式规定。在这篇论文中，我们提出了一种新的方法，使用自动提取的高级特征来定义传输函数。用户只需选择 interess structures in a slice viewer，我们的方法会自动选择相似的结构，基于通过神经网络提取的高级特征。与之前的学习基于的传输函数方法不同，我们的方法不需要训练模型，可以快速进行推理，使得可以在Volume Data中进行交互式探索。我们的方法可以减少需要的注释量，通过在用户操作时提供反馈，使得用户可以专注于需要注释的结构体。在实践中，我们的方法可以在秒钟内完成设计传输函数，而不是分钟内。我们与现有的学习基于的方法进行比较，包括注释量和计算时间，以及 segmentation 的准确率。我们的视频证明了我们的方法的交互性和效果。
</details></li>
</ul>
<hr>
<h2 id="Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance"><a href="#Differentiable-Bayesian-Structure-Learning-with-Acyclicity-Assurance" class="headerlink" title="Differentiable Bayesian Structure Learning with Acyclicity Assurance"></a>Differentiable Bayesian Structure Learning with Acyclicity Assurance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01392">http://arxiv.org/abs/2309.01392</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quang-Duy Tran, Phuoc Nguyen, Bao Duong, Thin Nguyen</li>
<li>for: 本研究旨在提出一种新的方法，可以强制限制生成的图为无环图，同时降低推理复杂度。</li>
<li>methods: 本研究使用了一种Integration of topological orderings的方法，通过约束生成图的结构，确保图为无环图。</li>
<li>results: 实验结果表明，我们的方法可以比相关的 bayesian 分数基于方法更高效，并且能够确保生成的图为无环图。<details>
<summary>Abstract</summary>
Score-based approaches in the structure learning task are thriving because of their scalability. Continuous relaxation has been the key reason for this advancement. Despite achieving promising outcomes, most of these methods are still struggling to ensure that the graphs generated from the latent space are acyclic by minimizing a defined score. There has also been another trend of permutation-based approaches, which concern the search for the topological ordering of the variables in the directed acyclic graph in order to limit the search space of the graph. In this study, we propose an alternative approach for strictly constraining the acyclicty of the graphs with an integration of the knowledge from the topological orderings. Our approach can reduce inference complexity while ensuring the structures of the generated graphs to be acyclic. Our empirical experiments with simulated and real-world data show that our approach can outperform related Bayesian score-based approaches.
</details>
<details>
<summary>摘要</summary>
Score-based方法在结构学习任务中升温，主要是因为它们可扩展性。继续relaxation是这一进步的关键。虽然它们在实现出色的结果，但大多数这些方法仍然困难确保生成的图从 latent space 中的图是无向的，通过定义的分数来最小化。此外，there has also been another trend of permutation-based approaches，即在搜索 directed acyclic graph 中变量的顺序排序，以限制搜索空间的图。在这种研究中，我们提出了一种替代方法，即通过 integrate 知识从 topological orderings 中来严格限制图的无向性。我们的方法可以降低推理复杂性，同时确保生成的图是无向的。我们的实验表明，我们的方法可以超越相关的 Bayesian score-based 方法。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences"><a href="#Classic-algorithms-are-fair-learners-Classification-Analysis-of-natural-weather-and-wildfire-occurrences" class="headerlink" title="Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences"></a>Classic algorithms are fair learners: Classification Analysis of natural weather and wildfire occurrences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01381">http://arxiv.org/abs/2309.01381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sengopal/classic-ml-review-paper">https://github.com/sengopal/classic-ml-review-paper</a></li>
<li>paper_authors: Senthilkumar Gopal</li>
<li>for: 本文主要用于对常见的超vised学习算法进行实验和数学分析，以了解它们在不同情况下的性能和特性。</li>
<li>methods: 本文使用了多种常见的超vised学习算法，包括决策树、扩展、支持向量机和k-最近邻居。</li>
<li>results: 本文对这些算法在稀疏表格数据上进行分类任务进行了评估，并观察了不同的超参数对这些算法的影响。结果表明，这些经典算法在稀疏数据上也能够保持良好的泛化能力，并且可以通过不同的超参数来提高分类精度。<details>
<summary>Abstract</summary>
Classic machine learning algorithms have been reviewed and studied mathematically on its performance and properties in detail. This paper intends to review the empirical functioning of widely used classical supervised learning algorithms such as Decision Trees, Boosting, Support Vector Machines, k-nearest Neighbors and a shallow Artificial Neural Network. The paper evaluates these algorithms on a sparse tabular data for classification task and observes the effect on specific hyperparameters on these algorithms when the data is synthetically modified for higher noise. These perturbations were introduced to observe these algorithms on their efficiency in generalizing for sparse data and their utility of different parameters to improve classification accuracy. The paper intends to show that these classic algorithms are fair learners even for such limited data due to their inherent properties even for noisy and sparse datasets.
</details>
<details>
<summary>摘要</summary>
经典机器学习算法已经详细地研究和分析其性能和属性。这篇论文想要评估广泛使用的古典监督学习算法，如决策树、扩展、支持向量机器、k最近邻居和杂谱神经网络。这篇论文评估这些算法在稀疏表格分类任务上的表现，并观察这些算法对不同的超参数的影响，当数据被人工改变以增加噪音时。这些改变是为了评估这些算法在欠拥有数据的情况下的泛化能力和不同超参数的使用来提高分类精度。这篇论文想要证明这些经典算法在有限数据的情况下仍然是公正学习器。
</details></li>
</ul>
<hr>
<h2 id="ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction"><a href="#ReOnto-A-Neuro-Symbolic-Approach-for-Biomedical-Relation-Extraction" class="headerlink" title="ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction"></a>ReOnto: A Neuro-Symbolic Approach for Biomedical Relation Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01370">http://arxiv.org/abs/2309.01370</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kracr/reonto-relation-extraction">https://github.com/kracr/reonto-relation-extraction</a></li>
<li>paper_authors: Monika Jain, Kuldeep Singh, Raghava Mutharaju</li>
<li>for: 本研究旨在提高生物医学文献中关系抽取（RE）任务的精度，提出了一种基于神经符号知识的新方法called ReOnto。</li>
<li>methods: ReOnto使用图神经网络获得句子表示，并利用公共可 accessible的 ontologies 作为先知知识来确定两个实体之间的句子关系。</li>
<li>results: 实验结果表明，使用ontologies 作为Symbolic知识和图神经网络结合可以提高RE任务的精度，在两个公共生物医学数据集 BioRel 和 ADE 上，我们的方法比基eline 高出约3%。<details>
<summary>Abstract</summary>
Relation Extraction (RE) is the task of extracting semantic relationships between entities in a sentence and aligning them to relations defined in a vocabulary, which is generally in the form of a Knowledge Graph (KG) or an ontology. Various approaches have been proposed so far to address this task. However, applying these techniques to biomedical text often yields unsatisfactory results because it is hard to infer relations directly from sentences due to the nature of the biomedical relations. To address these issues, we present a novel technique called ReOnto, that makes use of neuro symbolic knowledge for the RE task. ReOnto employs a graph neural network to acquire the sentence representation and leverages publicly accessible ontologies as prior knowledge to identify the sentential relation between two entities. The approach involves extracting the relation path between the two entities from the ontology. We evaluate the effect of using symbolic knowledge from ontologies with graph neural networks. Experimental results on two public biomedical datasets, BioRel and ADE, show that our method outperforms all the baselines (approximately by 3\%).
</details>
<details>
<summary>摘要</summary>
relation extraction (RE) 是将实体之间的 semantic 关系提取出来，并将其与知识图(KG)或ontology中的关系对应的任务。目前已经有很多方法被提出来解决这个任务。然而，在生物医学文本中应用这些技术时，通常会得到不满足的结果，因为生物医学关系很难直接从句子中推理出来。为了解决这些问题，我们提出了一种新的技术 called ReOnto，该技术利用 neural symbolic knowledge 来解决 RE 任务。ReOnto 使用图神经网络来获取句子表示，并利用公开 accessible 的 ontology 作为先知知识来确定两个实体之间的句子关系。该方法包括从 ontology 中提取两个实体之间的关系路径。我们通过将符号知识从 ontology 与图神经网络结合来评估符号知识的效果。我们在两个公共的生物医学数据集（BioRel 和 ADE）上进行实验，结果表明，我们的方法在所有基线方法（约为 3%）之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance"><a href="#Mutual-Information-Maximizing-Quantum-Generative-Adversarial-Network-and-Its-Applications-in-Finance" class="headerlink" title="Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance"></a>Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01363">http://arxiv.org/abs/2309.01363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingyu Lee, Myeongjin Shin, Junseo Lee, Kabgyun Jeong</li>
<li>for: 该研究旨在提出一种基于量子机器学习的新方法，用于解决生成 adversarial networks 中的模式塌陷问题。</li>
<li>methods: 该方法基于量子 neural network 的 Gradient Descent 算法，并使用 Mutual Information Neural Estimator (MINE) 来估算高维连续Random variables 之间的共聚信息。</li>
<li>results: 该方法能够成功地解决生成 adversarial networks 中的模式塌陷问题，并应用于金融领域，例如动态资产分配以生成股票返杂分布。<details>
<summary>Abstract</summary>
One of the most promising applications in the era of NISQ (Noisy Intermediate-Scale Quantum) computing is quantum machine learning. Quantum machine learning offers significant quantum advantages over classical machine learning across various domains. Specifically, generative adversarial networks have been recognized for their potential utility in diverse fields such as image generation, finance, and probability distribution modeling. However, these networks necessitate solutions for inherent challenges like mode collapse. In this study, we capitalize on the concept that the estimation of mutual information between high-dimensional continuous random variables can be achieved through gradient descent using neural networks. We introduce a novel approach named InfoQGAN, which employs the Mutual Information Neural Estimator (MINE) within the framework of quantum generative adversarial networks to tackle the mode collapse issue. Furthermore, we elaborate on how this approach can be applied to a financial scenario, specifically addressing the problem of generating portfolio return distributions through dynamic asset allocation. This illustrates the potential practical applicability of InfoQGAN in real-world contexts.
</details>
<details>
<summary>摘要</summary>
一个有前途的应用在NISQ（噪声中间规模量计算）时代是量子机器学习。量子机器学习在不同领域提供了显著的量子优势。特别是生成对抗网络在图像生成、金融和概率分布模型领域具有潜在的应用前景。然而，这些网络具有内置的挑战，如模式塌缩。在本研究中，我们利用Gradient Descent使 neural network estimate mutual information between high-dimensional continuous random variables的概念，并提出一种名为InfoQGAN的新方法。InfoQGAN在量子生成对抗网络框架中使用Mutual Information Neural Estimator（MINE）来解决模式塌缩问题。此外，我们还详细介绍了如何应用InfoQGAN到金融场景中，具体是通过动态资产配置来生成 portefolio return distribution。这 Illustrates the potential practical applicability of InfoQGAN in real-world contexts.
</details></li>
</ul>
<hr>
<h2 id="Random-Projections-of-Sparse-Adjacency-Matrices"><a href="#Random-Projections-of-Sparse-Adjacency-Matrices" class="headerlink" title="Random Projections of Sparse Adjacency Matrices"></a>Random Projections of Sparse Adjacency Matrices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01360">http://arxiv.org/abs/2309.01360</a></li>
<li>repo_url: None</li>
<li>paper_authors: Frank Qiu</li>
<li>for: 本文研究了一种随机投影方法，用于表示稀疏图。</li>
<li>methods: 本文使用了随机投影法，并证明了这种方法可以保持稀疏图的功能，同时具有一些有利的特性，如能够表示不同大小的图和不同顶点集的图在同一空间中。</li>
<li>results: 本文显示了随机投影法可以准确地保持图的操作，并且可以随着顶点数量的增加，Scale linearly with the number of vertices while accurately retaining first-order graph information。<details>
<summary>Abstract</summary>
We analyze a random projection method for adjacency matrices, studying its utility in representing sparse graphs. We show that these random projections retain the functionality of their underlying adjacency matrices while having extra properties that make them attractive as dynamic graph representations. In particular, they can represent graphs of different sizes and vertex sets in the same space, allowing for the aggregation and manipulation of graphs in a unified manner. We also provide results on how the size of the projections need to scale in order to preserve accurate graph operations, showing that the size of the projections can scale linearly with the number of vertices while accurately retaining first-order graph information. We conclude by characterizing our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details>
<details>
<summary>摘要</summary>
我们分析了一种随机投影方法 для邻接矩阵，研究其在表示稀疏图的实用性。我们表明这些随机投影保留了它们的基本邻接矩阵功能，同时具有一些有利的特性，使其成为动态图表示的优选。具体来说，它们可以表示不同大小的图和顶点集在同一个空间中， allowing for the aggregation and manipulation of graphs in a unified manner。我们还提供了保持 precisions 的投影大小的规则，显示投影大小可以线性增长与顶点数量相关，并准确地保持首领信息。我们最后 Characterize our random projection as a distance-preserving map of adjacency matrices analogous to the usual Johnson-Lindenstrauss map.
</details></li>
</ul>
<hr>
<h2 id="MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware"><a href="#MalwareDNA-Simultaneous-Classification-of-Malware-Malware-Families-and-Novel-Malware" class="headerlink" title="MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware"></a>MalwareDNA: Simultaneous Classification of Malware, Malware Families, and Novel Malware</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01350">http://arxiv.org/abs/2309.01350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maksim E. Eren, Manish Bhattarai, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas</li>
<li>For: The paper is written to address the slow adoption of machine learning (ML) based solutions against malware threats, and to introduce a new method for precise identification of novel malware families.* Methods: The paper proposes a new method that unifies the capability for malware&#x2F;benign-ware classification and malware family classification into a single framework.* Results: The paper showcases preliminary capabilities of the new method and demonstrates its ability to perform precise identification of novel malware families.<details>
<summary>Abstract</summary>
Malware is one of the most dangerous and costly cyber threats to national security and a crucial factor in modern cyber-space. However, the adoption of machine learning (ML) based solutions against malware threats has been relatively slow. Shortcomings in the existing ML approaches are likely contributing to this problem. The majority of current ML approaches ignore real-world challenges such as the detection of novel malware. In addition, proposed ML approaches are often designed either for malware/benign-ware classification or malware family classification. Here we introduce and showcase preliminary capabilities of a new method that can perform precise identification of novel malware families, while also unifying the capability for malware/benign-ware classification and malware family classification into a single framework.
</details>
<details>
<summary>摘要</summary>
马拉ware是现代计算机网络中最危险和成本最高的网络威胁之一，同时也是现代网络空间中一个关键因素。然而，使用机器学习（ML）解决马拉ware威胁的采用速度相对较慢。现有的ML方法缺乏现实世界中真实的挑战，如检测新型马拉ware。此外，现有的提议的ML方法通常是为马拉ware/正常软件分类或马拉ware家族分类而设计的。我们现在介绍一种新的方法，可以准确地识别新型马拉ware家族，同时也整合了malware/正常软件分类和马拉ware家族分类的能力。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification"><a href="#Adaptive-Parametric-Prototype-Learning-for-Cross-Domain-Few-Shot-Classification" class="headerlink" title="Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification"></a>Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01342">http://arxiv.org/abs/2309.01342</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marzi Heidari, Abdullah Alchihabi, Qing En, Yuhong Guo</li>
<li>for: 这篇论文targets cross-domain few-shot classification problem, which is more challenging than in-domain few-shot learning due to domain shifts.</li>
<li>methods: 本文提出了一种名为 Adaptive Parametric Prototype Learning (APPL) 的方法，它是一种基于元学习的方法，通过强制在查询集上实现prototype-based regularization来学习分类标本。</li>
<li>results: 实验结果显示，APPL 在多个 cross-domain few-shot benchmark dataset上表现较好，超过了许多现有的州际几招学习方法。<details>
<summary>Abstract</summary>
Cross-domain few-shot classification induces a much more challenging problem than its in-domain counterpart due to the existence of domain shifts between the training and test tasks. In this paper, we develop a novel Adaptive Parametric Prototype Learning (APPL) method under the meta-learning convention for cross-domain few-shot classification. Different from existing prototypical few-shot methods that use the averages of support instances to calculate the class prototypes, we propose to learn class prototypes from the concatenated features of the support set in a parametric fashion and meta-learn the model by enforcing prototype-based regularization on the query set. In addition, we fine-tune the model in the target domain in a transductive manner using a weighted-moving-average self-training approach on the query instances. We conduct experiments on multiple cross-domain few-shot benchmark datasets. The empirical results demonstrate that APPL yields superior performance than many state-of-the-art cross-domain few-shot learning methods.
</details>
<details>
<summary>摘要</summary>
cross-domain 少数目标分类比其域内对应的问题更加具有挑战性，这是因为训练和测试任务之间存在域shift。在这篇论文中，我们开发了一种名为 Adaptive Parametric Prototype Learning（APPL）方法，该方法基于元学习惯例下进行cross-domain 少数目标分类。与现有的概念性少数目标方法不同，我们提议使用支持集中的 concatenated 特征来学习类prototype，并通过在查询集上应用 prototype-based 正则化来meta-learn模型。此外，我们在目标域中进行了适应性训练，使用权重移动平均自适应法在查询集上进行了微调。我们在多个 cross-domain 少数目标benchmark datasets上进行了实验，结果显示，APPL的性能优于许多当前state-of-the-art cross-domain 少数目标学习方法。
</details></li>
</ul>
<hr>
<h2 id="Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach"><a href="#Learning-for-Interval-Prediction-of-Electricity-Demand-A-Cluster-based-Bootstrapping-Approach" class="headerlink" title="Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach"></a>Learning for Interval Prediction of Electricity Demand: A Cluster-based Bootstrapping Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01336">http://arxiv.org/abs/2309.01336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Dube, Natarajan Gautam, Amarnath Banerjee, Harsha Nagarajan</li>
<li>for: 预测电力需求的准确性对小聚合负荷Setting中的微型电网运行进行管理是非常重要。由于低聚合，电力需求可以高度抽象和点估计会导致误差的膨胀。这篇论文提出了一种Interval estimation算法来提供未来值可能存在的范围，以便量化点估计附近的误差。</li>
<li>methods: 本论文使用了一种机器学习算法来获得点估计电力需求和相应的偏差值，并将这些偏差值存储在内存中。然后，使用不监督学习算法将天气相似的日期分组成 clusters，并使用这些 clusters来分区内存。在测试日期上，使用点估计来找到最相似的天气日期，然后从选择的cluster中bootstrap偏差。</li>
<li>results: 本论文对实际的电力需求数据进行了 evaluate，并与其他 bootstrapping 方法进行了比较，包括不同的信度范围。<details>
<summary>Abstract</summary>
Accurate predictions of electricity demands are necessary for managing operations in a small aggregation load setting like a Microgrid. Due to low aggregation, the electricity demands can be highly stochastic and point estimates would lead to inflated errors. Interval estimation in this scenario, would provide a range of values within which the future values might lie and helps quantify the errors around the point estimates. This paper introduces a residual bootstrap algorithm to generate interval estimates of day-ahead electricity demand. A machine learning algorithm is used to obtain the point estimates of electricity demand and respective residuals on the training set. The obtained residuals are stored in memory and the memory is further partitioned. Days with similar demand patterns are grouped in clusters using an unsupervised learning algorithm and these clusters are used to partition the memory. The point estimates for test day are used to find the closest cluster of similar days and the residuals are bootstrapped from the chosen cluster. This algorithm is evaluated on the real electricity demand data from EULR(End Use Load Research) and is compared to other bootstrapping methods for varying confidence intervals.
</details>
<details>
<summary>摘要</summary>
正确的电力需求预测是小聚合负载设定 like Microgrid 的管理操作中的必要条件。由于低聚合，电力需求可以具有高度� stoochastic 的特性，单点估计将会导致膨胀的错误。在这种情况下，间隔估计将提供未来值的范围内的值，并帮助量化这些点估计的错误。本文提出了一种剩余 bootstrap 算法，用于生成日前电力需求的间隔估计。使用机器学习算法取得电力需求点估计和相应的剩余在训练集上。取得的剩余会被储存，并且将天数分组为相似的需求模式cluster。在试验天数上使用这些cluster来选择最相似的天数，然后从选择的cluster中bootstrap 剩余。这个算法被评估了真实的电力需求数据 from EULR（End Use Load Research），并与其他剩余方法进行比较，以评估不同的信度 интервала。
</details></li>
</ul>
<hr>
<h2 id="In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems"><a href="#In-processing-User-Constrained-Dominant-Sets-for-User-Oriented-Fairness-in-Recommender-Systems" class="headerlink" title="In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems"></a>In-processing User Constrained Dominant Sets for User-Oriented Fairness in Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01335">http://arxiv.org/abs/2309.01335</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhongxuan Han, Chaochao Chen, Xiaolin Zheng, Weiming Liu, Jun Wang, Wenjie Cheng, Yuyuan Li</li>
<li>for:  addressing the user-oriented fairness (UOF) issue in recommender systems, where the existing research is limited and fails to deal with the root cause of the issue.</li>
<li>methods:  proposing an In-processing User Constrained Dominant Sets (In-UCDS) framework, which is a general framework that can be applied to any backbone recommendation model to achieve user-oriented fairness. The framework consists of two stages: UCDS modeling stage and in-processing training stage.</li>
<li>results:  outperforming the state-of-the-art methods in addressing the UOF issue while maintaining the overall recommendation performance, as demonstrated by comprehensive experiments on three real-world datasets.<details>
<summary>Abstract</summary>
Recommender systems are typically biased toward a small group of users, leading to severe unfairness in recommendation performance, i.e., User-Oriented Fairness (UOF) issue. The existing research on UOF is limited and fails to deal with the root cause of the UOF issue: the learning process between advantaged and disadvantaged users is unfair. To tackle this issue, we propose an In-processing User Constrained Dominant Sets (In-UCDS) framework, which is a general framework that can be applied to any backbone recommendation model to achieve user-oriented fairness. We split In-UCDS into two stages, i.e., the UCDS modeling stage and the in-processing training stage. In the UCDS modeling stage, for each disadvantaged user, we extract a constrained dominant set (a user cluster) containing some advantaged users that are similar to it. In the in-processing training stage, we move the representations of disadvantaged users closer to their corresponding cluster by calculating a fairness loss. By combining the fairness loss with the original backbone model loss, we address the UOF issue and maintain the overall recommendation performance simultaneously. Comprehensive experiments on three real-world datasets demonstrate that In-UCDS outperforms the state-of-the-art methods, leading to a fairer model with better overall recommendation performance.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>推荐系统通常偏向一小组用户，导致推荐性能的严重不公，即用户 oriented fairness (UOF) 问题。现有的 UOF 研究有限，无法解决 UOF 问题的根本原因：用户和不advantaged 用户之间的学习过程不公。为解决这个问题，我们提出了 In-processing User Constrained Dominant Sets (In-UCDS) 框架，这是一个可以应用于任何基础推荐模型来实现用户 oriented fairness 的通用框架。我们将 In-UCDS 分成两个阶段：UCDS 模型化阶段和在处理训练阶段。在 UCDS 模型化阶段，每个劣势用户都提取一个受限制的主导集 (用户群)，其中包含一些有利用户的用户。在处理训练阶段，我们将劣势用户的表示移动到其相应的群体中，通过计算公平损失来实现公平性。将公平损失与基础模型损失结合，我们同时解决 UOF 问题和维护推荐性能。我们在三个实际数据集上进行了广泛的实验，并证明 In-UCDS 超过了当前状态的方法，导致一个更公平的模型，同时保持推荐性能。
</details></li>
</ul>
<hr>
<h2 id="An-ML-assisted-OTFS-vs-OFDM-adaptable-modem"><a href="#An-ML-assisted-OTFS-vs-OFDM-adaptable-modem" class="headerlink" title="An ML-assisted OTFS vs. OFDM adaptable modem"></a>An ML-assisted OTFS vs. OFDM adaptable modem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01319">http://arxiv.org/abs/2309.01319</a></li>
<li>repo_url: None</li>
<li>paper_authors: I. Zakir Ahmed, Hamid R. Sadjadpour</li>
<li>for: 这篇论文是关于高速移动场景下的信号处理技术选择和改进。</li>
<li>methods: 这篇论文使用了深度神经网络（DNN）基于的变换方案，以优化信号处理链的选择，以提高含义平均方差（MSE）性能。</li>
<li>results:  simulations 表明，使用提议的变换方案可以提高信号处理的MSE性能，比OFDM和OTFS signal processing chain更佳。<details>
<summary>Abstract</summary>
The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient to doubly-dispersive channels, which impacts high mobility scenarios. On the other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms enjoy the benefits of the reuse of legacy architectures, simplicity of receiver design, and low-complexity detection. Several studies that compare the performance of OFDM and OTFS have indicated mixed outcomes due to the plethora of system parameters at play beyond high-mobility conditions. In this work, we exemplify this observation using simulations and propose a deep neural network (DNN)-based adaptation scheme to switch between using either an OTFS or OFDM signal processing chain at the transmitter and receiver for optimal mean-squared-error (MSE) performance. The DNN classifier is trained to switch between the two schemes by observing the channel condition, received SNR, and modulation format. We compare the performance of the OTFS, OFDM, and the proposed switched-waveform scheme. The simulations indicate superior performance with the proposed scheme with a well-trained DNN, thus improving the MSE performance of the communication significantly.
</details>
<details>
<summary>摘要</summary>
“orthogonal-time-frequency-space（OTFS）信号处理显示高移动场景下具有双杂分通道的抗性，而orthogonal-frequency-division-multiplexing（OFDM）波形具有 reuse 的传统架构、接收器设计的简单性和低复杂性检测的优点。但是，由于系统参数的各种因素，一些研究对OFDM和OTFS的性能比较结果呈混合的趋势。在这种情况下，我们通过 simulations 和深度神经网络（DNN）化适应方案来 switching  между使用 OTFS 或 OFDM 信号处理链，以实现优化的mean-squared-error（MSE）性能。DNN 分类器通过观察通道条件、接收信号强度和模ulationFormat来选择使用哪一种信号处理链。我们对 OTFS、OFDM 和我们提议的 switched-waveform  scheme 进行比较，实验结果显示，将 DNN Well 训练后，提出的方案可以显著改善通信的MSE性能。”
</details></li>
</ul>
<hr>
<h2 id="Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates"><a href="#Learning-a-Patent-Informed-Biomedical-Knowledge-Graph-Reveals-Technological-Potential-of-Drug-Repositioning-Candidates" class="headerlink" title="Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates"></a>Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.03227">http://arxiv.org/abs/2309.03227</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ysjegal/ysjegal-drug-repositioning">https://github.com/ysjegal/ysjegal-drug-repositioning</a></li>
<li>paper_authors: Yongseung Jegal, Jaewoong Choi, Jiho Lee, Ki-Su Park, Seyoung Lee, Janghyeok Yoon</li>
<li>For: The paper is written to explore a novel protocol for identifying drug repositioning candidates with both technological potential and scientific evidence.* Methods: The protocol involves constructing a scientific biomedical knowledge graph (s-BKG) and a patent-informed biomedical knowledge graph (p-BKG), and using a graph embedding protocol to ascertain the structure of the p-BKG and calculate the relevance scores of potential drug candidates.* Results: The case study on Alzheimer’s disease demonstrates the efficacy and feasibility of the proposed method, and the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.Here’s the information in Simplified Chinese text:* For: 本研究是为了探讨一种新的药物重定向候选者选择方法，以实现药物重定向的科学价值和实际应用。* Methods: 方法包括构建一个科学生物医学知识图(s-BKG)和一个专利信息 Informed biomedical knowledge graph(p-BKG)，并使用图像嵌入方法来确定p-BKG的结构，计算潜在药物候选者与target疾病相关专利的相似度。* Results: 对阿尔茨heimer病的case study表明方法的有效性和实施性，并且预计通过计算机发现和成功应用在药物重定向研究中，能够填补计算发现和成功应用之间的空白。<details>
<summary>Abstract</summary>
Drug repositioning-a promising strategy for discovering new therapeutic uses for existing drugs-has been increasingly explored in the computational science literature using biomedical databases. However, the technological potential of drug repositioning candidates has often been overlooked. This study presents a novel protocol to comprehensively analyse various sources such as pharmaceutical patents and biomedical databases, and identify drug repositioning candidates with both technological potential and scientific evidence. To this end, first, we constructed a scientific biomedical knowledge graph (s-BKG) comprising relationships between drugs, diseases, and genes derived from biomedical databases. Our protocol involves identifying drugs that exhibit limited association with the target disease but are closely located in the s-BKG, as potential drug candidates. We constructed a patent-informed biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information. Finally, we developed a graph embedding protocol to ascertain the structure of the p-BKG, thereby calculating the relevance scores of those candidates with target disease-related patents to evaluate their technological potential. Our case study on Alzheimer's disease demonstrates its efficacy and feasibility, while the quantitative outcomes and systematic methods are expected to bridge the gap between computational discoveries and successful market applications in drug repositioning research.
</details>
<details>
<summary>摘要</summary>
药物重新定位策略已经在计算科学文献中得到了越来越多的探索，使用生物医学数据库。然而，技术潜力具有药物重新定位候选者往往被忽略。本研究提出了一种新的协议，用于全面分析各种来源，例如药品专利和生物医学数据库，并从科学角度鉴定药物重新定位候选者。为此，我们首先建立了一个科学生物医学知识图（s-BKG），其中包含药品、疾病和基因之间的关系，从生物医学数据库中获取。我们的协议包括在s-BKG中找到药品，它们与目标疾病之间的关系较弱，但在s-BKG中与其他药品和疾病之间有紧密的关系。我们还建立了一个受专利信息支持的生物医学知识图（p-BKG），并开发了一种图像嵌入协议，以确定p-BKG的结构，并计算与疾病相关专利的相关性分数，以评估候选者的技术潜力。我们的案例研究表明，这种方法在阿尔茨海默病中得到了成功和可行性，而量化结果和系统方法均预期能够把计算发现与成功市场应用之间的空隙填充。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection"><a href="#Enhancing-Automated-and-Early-Detection-of-Alzheimer’s-Disease-Using-Out-Of-Distribution-Detection" class="headerlink" title="Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection"></a>Enhancing Automated and Early Detection of Alzheimer’s Disease Using Out-Of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01312">http://arxiv.org/abs/2309.01312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Audrey Paleczny, Shubham Parab, Maxwell Zhang</li>
<li>For: The paper aims to improve the accuracy of Magnetic Resonance Imaging (MRI) classification for Alzheimer’s disease using Convolutional Neural Networks (CNNs) and out-of-distribution (OOD) detection.* Methods: The authors trained supervised Random Forest models with segmented brain volumes and CNN outputs to classify different Alzheimer’s stages. They also applied OOD detection to the CNN model to reduce false diagnoses.* Results: The CNN model outperformed the segmented volume model with an accuracy of 98% for detection and 95% for classification. The OOD detection enabled the CNN model to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction.Here are the three points in Simplified Chinese text:</li>
<li>for: 本研究使用MRI进行阿尔ツ海默病诊断，使用Convolutional Neural Networks (CNNs)和out-of-distribution (OOD)检测来提高诊断精度。</li>
<li>methods: 作者使用了supervised Random Forest模型，并将分割的脑部Volume和CNN输出作为输入来分类不同的阿尔ツ海默病阶段。他们还应用了OOD检测到CNN模型，以降低false positives的发生。</li>
<li>results: CNN模型比分割模型高度出色，具有98%的检测精度和95%的分类精度。OOD检测使得CNN模型能够准确地标识出脑出血图像，准确率为96%，并没有对整体精度造成明显的影响。<details>
<summary>Abstract</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. Source code available upon request.
</details>
<details>
<summary>摘要</summary>
More than 10.7% of people aged 65 and older are affected by Alzheimer's disease. Early diagnosis and treatment are crucial as most Alzheimer's patients are unaware of having it until the effects become detrimental. AI has been known to use magnetic resonance imaging (MRI) to diagnose Alzheimer's. However, models which produce low rates of false diagnoses are critical to prevent unnecessary treatments. Thus, we trained supervised Random Forest models with segmented brain volumes and Convolutional Neural Network (CNN) outputs to classify different Alzheimer's stages. We then applied out-of-distribution (OOD) detection to the CNN model, enabling it to report OOD if misclassification is likely, thereby reducing false diagnoses. With an accuracy of 98% for detection and 95% for classification, our model based on CNN results outperformed our segmented volume model, which had detection and classification accuracies of 93% and 87%, respectively. Applying OOD detection to the CNN model enabled it to flag brain tumor images as OOD with 96% accuracy and minimal overall accuracy reduction. By using OOD detection to enhance the reliability of MRI classification using CNNs, we lowered the rate of false positives and eliminated a significant disadvantage of using Machine Learning models for healthcare tasks. 源代码可以根据需要请求。
</details></li>
</ul>
<hr>
<h2 id="Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles"><a href="#Communication-Efficient-Design-of-Learning-System-for-Energy-Demand-Forecasting-of-Electrical-Vehicles" class="headerlink" title="Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles"></a>Communication-Efficient Design of Learning System for Energy Demand Forecasting of Electrical Vehicles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01297">http://arxiv.org/abs/2309.01297</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiacong Xu, Riley Kilfoyle, Zixiang Xiong, Ligang Lu</li>
<li>for: 这篇论文的目的是提出一个可靠且高效的时间序列预测模型，用于预测电动车充电站的能源利用状况。</li>
<li>methods: 这篇论文使用了最新的transformer架构和联边学习（Federated Learning）技术，实现了分布式训练。</li>
<li>results: 这篇论文的预测性能与其他模型相比，具有与其他模型相同的性能，但是训练时间的数据资料负载量则明显减少。此外，这个模型还能够在其他时间序列 datasets 上进行通用化预测。<details>
<summary>Abstract</summary>
Machine learning (ML) applications to time series energy utilization forecasting problems are a challenging assignment due to a variety of factors. Chief among these is the non-homogeneity of the energy utilization datasets and the geographical dispersion of energy consumers. Furthermore, these ML models require vast amounts of training data and communications overhead in order to develop an effective model. In this paper, we propose a communication-efficient time series forecasting model combining the most recent advancements in transformer architectures implemented across a geographically dispersed series of EV charging stations and an efficient variant of federated learning (FL) to enable distributed training. The time series prediction performance and communication overhead cost of our FL are compared against their counterpart models and shown to have parity in performance while consuming significantly lower data rates during training. Additionally, the comparison is made across EV charging as well as other time series datasets to demonstrate the flexibility of our proposed model in generalized time series prediction beyond energy demand. The source code for this work is available at https://github.com/XuJiacong/LoGTST_PSGF
</details>
<details>
<summary>摘要</summary>
机器学习（ML）应用到时间序列能源利用预测问题是一个具有许多挑战性的任务，主要包括时间序列数据的非均匀性和能源消耗者的地域分散。此外，这些 ML 模型需要很大的训练数据和通信频率以建立有效的模型。在这篇论文中，我们提出了一个具有最新的变数架构的时间序列预测模型，利用了分布式训练的有效变体 federated learning（FL），以便在地域分散的 EV 充电站上进行分布式训练。我们的 FL 模型与对照模型相比，在预测性能和通信负载成本方面具有相等的性能，并且在训练时间中显著降低了数据资料率。此外，我们还对 EV 充电和其他时间序列数据进行比较，以示出我们的提案模型在通用时间序列预测方面的灵活性。source code 可以在 GitHub 上找到：https://github.com/XuJiacong/LoGTST_PSGF。
</details></li>
</ul>
<hr>
<h2 id="AlphaZero-Gomoku"><a href="#AlphaZero-Gomoku" class="headerlink" title="AlphaZero Gomoku"></a>AlphaZero Gomoku</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01294">http://arxiv.org/abs/2309.01294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/suragnair/alpha-zero-general">https://github.com/suragnair/alpha-zero-general</a></li>
<li>paper_authors: Wen Liang, Chao Yu, Brian Whiteaker, Inyoung Huh, Hua Shao, Youzhi Liang</li>
<li>for: 研究扩展AlphaZero算法到五子棋，以提高人工智能游戏玩家的能力。</li>
<li>methods: 利用深度学习和Monte Carlo搜索算法，将AlphaZero算法应用到五子棋中。</li>
<li>results: AlphaZero算法在五子棋中表现出色，能够快速适应不同的游戏环境，并且能够增强人工智能游戏玩家的能力。<details>
<summary>Abstract</summary>
In the past few years, AlphaZero's exceptional capability in mastering intricate board games has garnered considerable interest. Initially designed for the game of Go, this revolutionary algorithm merges deep learning techniques with the Monte Carlo tree search (MCTS) to surpass earlier top-tier methods. In our study, we broaden the use of AlphaZero to Gomoku, an age-old tactical board game also referred to as "Five in a Row." Intriguingly, Gomoku has innate challenges due to a bias towards the initial player, who has a theoretical advantage. To add value, we strive for a balanced game-play. Our tests demonstrate AlphaZero's versatility in adapting to games other than Go. MCTS has become a predominant algorithm for decision processes in intricate scenarios, especially board games. MCTS creates a search tree by examining potential future actions and uses random sampling to predict possible results. By leveraging the best of both worlds, the AlphaZero technique fuses deep learning from Reinforcement Learning with the balancing act of MCTS, establishing a fresh standard in game-playing AI. Its triumph is notably evident in board games such as Go, chess, and shogi.
</details>
<details>
<summary>摘要</summary>
在过去几年，AlphaZero的在复杂游戏中表现出色，引发了广泛的关注。AlphaZero最初是设计用于围棋游戏，这种革命性的算法将深度学习技术与 Monte Carlo 搜索树（MCTS）相结合，超越了之前的顶层方法。在我们的研究中，我们扩展了AlphaZero的使用范围，将其应用于古老的策略游戏“五子棋”（也称为“五在一行”）。有趣的是，五子棋具有初始玩家偏好的偏袋问题，因此我们努力寻找一个平衡的游戏环境。我们的测试表明，AlphaZero在不同于围棋的游戏中也能够表现出色。MCTS在复杂enario中的决策过程中变得越来越普遍，特别是在棋盘游戏中。MCTS通过检查未来动作的可能性，并使用随机抽样来预测可能的结果，创造了一棵搜索树。AlphaZero技术将深度学习从回归学习与 MCTS 的平衡运算相结合，创造了一个新的游戏AI标准。其胜利特别明显在棋盘游戏such as Go、棋盘和将棋中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/cs.LG_2023_09_04/" data-id="clmjn91mw00810j889vfigigx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_04" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/04/eess.IV_2023_09_04/" class="article-date">
  <time datetime="2023-09-04T09:00:00.000Z" itemprop="datePublished">2023-09-04</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/04/eess.IV_2023_09_04/">eess.IV - 2023-09-04</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations"><a href="#Multi-dimension-unified-Swin-Transformer-for-3D-Lesion-Segmentation-in-Multiple-Anatomical-Locations" class="headerlink" title="Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations"></a>Multi-dimension unified Swin Transformer for 3D Lesion Segmentation in Multiple Anatomical Locations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01823">http://arxiv.org/abs/2309.01823</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaoyan Pan, Yiqiao Liu, Sarah Halek, Michal Tomaszewski, Shubing Wang, Richard Baumgartner, Jianda Yuan, Gregory Goldmacher, Antong Chen</li>
<li>for: automated 3D lesion segmentation for radiomics and tumor growth modeling studies</li>
<li>methods: multi-dimension unified Swin transformer (MDU-ST) model with a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks and fine-tuning with labeled 2D and 3D volumes</li>
<li>results: significant improvement over competing models, demonstrated through Dice similarity coefficient (DSC) and Hausdorff distance (HD) on an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations.<details>
<summary>Abstract</summary>
In oncology research, accurate 3D segmentation of lesions from CT scans is essential for the modeling of lesion growth kinetics. However, following the RECIST criteria, radiologists routinely only delineate each lesion on the axial slice showing the largest transverse area, and delineate a small number of lesions in 3D for research purposes. As a result, we have plenty of unlabeled 3D volumes and labeled 2D images, and scarce labeled 3D volumes, which makes training a deep-learning 3D segmentation model a challenging task. In this work, we propose a novel model, denoted a multi-dimension unified Swin transformer (MDU-ST), for 3D lesion segmentation. The MDU-ST consists of a Shifted-window transformer (Swin-transformer) encoder and a convolutional neural network (CNN) decoder, allowing it to adapt to 2D and 3D inputs and learn the corresponding semantic information in the same encoder. Based on this model, we introduce a three-stage framework: 1) leveraging large amount of unlabeled 3D lesion volumes through self-supervised pretext tasks to learn the underlying pattern of lesion anatomy in the Swin-transformer encoder; 2) fine-tune the Swin-transformer encoder to perform 2D lesion segmentation with 2D RECIST slices to learn slice-level segmentation information; 3) further fine-tune the Swin-transformer encoder to perform 3D lesion segmentation with labeled 3D volumes. The network's performance is evaluated by the Dice similarity coefficient (DSC) and Hausdorff distance (HD) using an internal 3D lesion dataset with 593 lesions extracted from multiple anatomical locations. The proposed MDU-ST demonstrates significant improvement over the competing models. The proposed method can be used to conduct automated 3D lesion segmentation to assist radiomics and tumor growth modeling studies. This paper has been accepted by the IEEE International Symposium on Biomedical Imaging (ISBI) 2023.
</details>
<details>
<summary>摘要</summary>
在肿瘤研究中，准确的3D肿瘤分割从CT扫描图中获得是至关重要的，以便肿瘤生长动态模型的建立。然而，根据RECIST标准， radiologists通常只在最大横坐标的AXIAL slice上画出肿瘤，并且只为研究目的画出一些LESION的3D分割。因此，我们有很多未标注的3D体积和标注的2D图像，而且罕见的标注3D体积，这使得培育深度学习3D分割模型成为一项挑战。在这种情况下，我们提出了一种新的模型，称为多维度统一Swin变换（MDU-ST），用于肿瘤分割。MDU-ST包括Swin变换encoder和卷积神经网络（CNN）解决器，允许它适应2D和3D输入，并学习相应的semantic信息。基于这种模型，我们提出了一个三个阶段框架：1）通过自动适应的Pretext Task来利用大量未标注3D肿瘤体积来学习肿瘤生长的下面特征；2）使用2D RECIST slice来精度调整Swin变换encoder，以学习slice级别的分割信息；3）进一步精度调整Swin变换encoder，以进行3D肿瘤分割。网络性能被评估使用内部3D肿瘤数据集中的Dice相似度（DSC）和 Hausdorff距离（HD）。我们的提出的MDU-ST在与其他模型进行比较时表现出了显著的改善。这种方法可以用于自动化3D肿瘤分割，以帮助辐射学和肿瘤生长模型研究。这篇论文已经在2023年IEEE国际生物医学影像学会（ISBI）上被接受。
</details></li>
</ul>
<hr>
<h2 id="Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain"><a href="#Accuracy-and-Consistency-of-Space-based-Vegetation-Height-Maps-for-Forest-Dynamics-in-Alpine-Terrain" class="headerlink" title="Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain"></a>Accuracy and Consistency of Space-based Vegetation Height Maps for Forest Dynamics in Alpine Terrain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01797">http://arxiv.org/abs/2309.01797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuchang Jiang, Marius Rüetschi, Vivien Sainte Fare Garnot, Mauro Marty, Konrad Schindler, Christian Ginzler, Jan D. Wegner</li>
<li>for: 本研究旨在提高瑞士国家森林资源调查（NFI）的更新周期，使之能够更好地评估和管理环境。</li>
<li>methods: 本研究使用空间 remote sensing 和深度学习生成大规模的植被高程地图，以降低成本并提高分析效率。</li>
<li>results: 研究结果表明，使用 Sentinel-2 卫星图像生成的植被高程地图可以准确地捕捉到瑞士各地的植被变化，并且可以与普通的 Airborne Laser Scanning 数据进行比较。这些地图还可以用于检测小规模的变化，例如冬季风暴所引起的变化。<details>
<summary>Abstract</summary>
Monitoring and understanding forest dynamics is essential for environmental conservation and management. This is why the Swiss National Forest Inventory (NFI) provides countrywide vegetation height maps at a spatial resolution of 0.5 m. Its long update time of 6 years, however, limits the temporal analysis of forest dynamics. This can be improved by using spaceborne remote sensing and deep learning to generate large-scale vegetation height maps in a cost-effective way. In this paper, we present an in-depth analysis of these methods for operational application in Switzerland. We generate annual, countrywide vegetation height maps at a 10-meter ground sampling distance for the years 2017 to 2020 based on Sentinel-2 satellite imagery. In comparison to previous works, we conduct a large-scale and detailed stratified analysis against a precise Airborne Laser Scanning reference dataset. This stratified analysis reveals a close relationship between the model accuracy and the topology, especially slope and aspect. We assess the potential of deep learning-derived height maps for change detection and find that these maps can indicate changes as small as 250 $m^2$. Larger-scale changes caused by a winter storm are detected with an F1-score of 0.77. Our results demonstrate that vegetation height maps computed from satellite imagery with deep learning are a valuable, complementary, cost-effective source of evidence to increase the temporal resolution for national forest assessments.
</details>
<details>
<summary>摘要</summary>
监测和理解森林动态是环境保护和管理的关键。为了实现这一目标，瑞士国家森林资产库（NFI）提供了全国覆盖率0.5米的植被高度地图。然而，这些地图的更新周期为6年，限制了森林动态的时间分析。可以使用空间遥感和深度学习生成大规模的植被高度地图，以便在成本效益的方式下提高 temporal resolution。在这篇论文中，我们对这些方法进行了深入的分析，并在瑞士进行了实际应用。我们使用Sentinel-2卫星图像生成了2017年至2020年的年度、全国覆盖率10米的植被高度地图。与之前的研究相比，我们进行了大规模和细化的随机分配分析，发现模型准确率与地形特征（坡度和方向）之间存在紧密的关系。我们评估了深度学习得到的高度地图的变化检测潜力，发现这些地图可以检测到250平方米级别的变化。在更大的规模上，受冬季风暴影响的变化的F1分数为0.77。我们的结果表明，通过卫星图像使用深度学习计算的植被高度地图是一种有价值的、补充性的、成本效益的证据，可以增加国家森林评估的时间分辨率。
</details></li>
</ul>
<hr>
<h2 id="Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations"><a href="#Effects-of-Material-Mapping-Agnostic-Partial-Volume-Correction-for-Subject-Specific-Finite-Elements-Simulations" class="headerlink" title="Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations"></a>Effects of Material Mapping Agnostic Partial Volume Correction for Subject Specific Finite Elements Simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01769">http://arxiv.org/abs/2309.01769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aren Beagley, Hannah Richards, Joshua W. Giles</li>
<li>for:  corrected partial volume effects in CT images</li>
<li>methods:  developed and validated a new algorithm that uses a combination of image processing techniques to correct partial volume effects at cortical bone boundaries, without requiring pre-processing or user input.</li>
<li>results:  demonstrated improved accuracy of surface strain predictions using models created with the corrected CT images compared to those created with the original, uncorrected images.<details>
<summary>Abstract</summary>
Partial Volume effects are present at the boundary between any two types of material in a CT image due to the scanner's Point Spread Function, finite voxel resolution, and importantly, the discrepancy in radiodensity between the two materials. In this study a new algorithm is developed and validated that builds on previously published work to enable the correction of partial volume effects at cortical bone boundaries. Unlike past methods, this algorithm does not require pre-processing or user input to achieve the correction, and the correction is applied directly onto a set of CT images, which enables it to be used in existing computational modelling workflows. The algorithm was validated by performing experimental three point bending tests on porcine fibulae specimen and comparing the experimental results to finite element results for models created using either the original, uncorrected CT images or the partial volume corrected images. Results demonstrated that the models created using the partial volume corrected images did improved the accuracy of the surface strain predictions. Given this initial validation, this algorithm is a viable method for overcoming the challenge of partial volume effects in CT images. Thus, future work should be undertaken to further validate the algorithm with human tissues and through coupling it with a range of different finite element creation workflows to verify that it is robust and agnostic to the chosen material mapping strategy.
</details>
<details>
<summary>摘要</summary>
《部分体积影响在CT图像中存在于任何两种材料的边界之间，由扫描仪的点扩散函数、精度幂和材料差异引起。本研究开发了一种新的算法，基于之前发表的工作，以消除CT图像中的部分体积影响。与过去的方法不同，这个算法不需要先期处理或用户输入，直接应用到CT图像集中，可以在现有的计算模型工作流中使用。这个算法通过对猪肋骨三点弯测试实验和finite element分析来验证，对表层弯曲率预测的准确性进行了改进。由于这个初步验证结果，这个算法是一种可靠的方法，用于解决CT图像中的部分体积影响。因此，未来的工作应该继续进行验证，以确保这个算法在人类组织中的可靠性和与不同材料映射策略相关的可变性。》
</details></li>
</ul>
<hr>
<h2 id="Multispectral-Indices-for-Wildfire-Management"><a href="#Multispectral-Indices-for-Wildfire-Management" class="headerlink" title="Multispectral Indices for Wildfire Management"></a>Multispectral Indices for Wildfire Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01751">http://arxiv.org/abs/2309.01751</a></li>
<li>repo_url: None</li>
<li>paper_authors: Afonso Oliveira, João P. Matos-Carvalho, Filipe Moutinho, Nuno Fachada</li>
<li>for: 本研究旨在为火灾预防和管理方面提供多спектル индекс和相关方法，以帮助研究人员和决策者更好地理解和利用这些 индекс。</li>
<li>methods: 本研究涉及了多种领域，包括植被和土壤特征提取、水体 mapping、人工结构标识和火灾后烧区估计。这些方法都是基于多спектル индекс的，并且可以帮助解决 especific issues in wildfire management。</li>
<li>results: 本研究提出了多种有效的多спектル индекс，包括NDVI和NDWI等，可以用于 Addressing specific issues in wildfire management。此外，为了提高准确性和缓解个体 индекс应用的局限性，建议使用补充处理解决方案和其他数据源，如高分辨率图像和地面测量。<details>
<summary>Abstract</summary>
This paper highlights and summarizes the most important multispectral indices and associated methodologies for fire management. Various fields of study are examined where multispectral indices align with wildfire prevention and management, including vegetation and soil attribute extraction, water feature mapping, artificial structure identification, and post-fire burnt area estimation. The versatility and effectiveness of multispectral indices in addressing specific issues in wildfire management are emphasized. Fundamental insights for optimizing data extraction are presented. Concrete indices for each task, including the NDVI and the NDWI, are suggested. Moreover, to enhance accuracy and address inherent limitations of individual index applications, the integration of complementary processing solutions and additional data sources like high-resolution imagery and ground-based measurements is recommended. This paper aims to be an immediate and comprehensive reference for researchers and stakeholders working on multispectral indices related to the prevention and management of fires.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文把关注和总结了最重要的多spectral指标和相关的方法ологиías，用于林火预防和管理。讨论的领域包括 vegetation和soil特征提取、水特征地图、人工结构标识和林火后烧区面积估计。这些多spectral指标在林火预防和管理中的 versatility和有效性被强调。文中提供了数据提取优化的基本 Insights，并建议了每个任务的具体指标，包括 NDVI 和 NDWI。此外，为了提高准确性和解决个体指标应用中的限制，建议 integrating complementary processing solutions and additional data sources，如高分辨率图像和地面测量。这篇论文旨在为研究人员和相关利益人员提供一份立即和全面的参考，关于多spectral指标在林火预防和管理中的应用。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors"><a href="#Deep-Learning-Approach-for-Large-Scale-Real-Time-Quantification-of-Green-Fluorescent-Protein-Labeled-Biological-Samples-in-Microreactors" class="headerlink" title="Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors"></a>Deep Learning Approach for Large-Scale, Real-Time Quantification of Green Fluorescent Protein-Labeled Biological Samples in Microreactors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01384">http://arxiv.org/abs/2309.01384</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Wei, Sai Mu Dalike Abaxi, Nawaz Mehmood, Luoquan Li, Fuyang Qu, Guangyao Cheng, Dehua Hu, Yi-Ping Ho, Scott Wu Yuan, Ho-Pui Ho</li>
<li>for: 这个研究的目的是为了实现快速、精准的生物样本测量，以便更好地了解生物系统的工作机理。</li>
<li>methods: 这个研究使用了深度学习技术，实现了自动 segmentation 和分类的 GFP（绿色荧光蛋白）标记微反应室，从而实现了实时精准测量。</li>
<li>results: 研究发现，使用这种技术可以快速（只需2.5秒）、精准地测量 GFP 标记微反应室的大小和占据状态，并且具有广阔的动态范围（从56.52到1569.43个浮镜分子&#x2F;微升）。此外，这种 Deep-dGFP 算法具有remarkable泛化能力，可以直接应用于多种 GFP 标记场景。<details>
<summary>Abstract</summary>
Absolute quantification of biological samples entails determining expression levels in precise numerical copies, offering enhanced accuracy and superior performance for rare templates. However, existing methodologies suffer from significant limitations: flow cytometers are both costly and intricate, while fluorescence imaging relying on software tools or manual counting is time-consuming and prone to inaccuracies. In this study, we have devised a comprehensive deep-learning-enabled pipeline that enables the automated segmentation and classification of GFP (green fluorescent protein)-labeled microreactors, facilitating real-time absolute quantification. Our findings demonstrate the efficacy of this technique in accurately predicting the sizes and occupancy status of microreactors using standard laboratory fluorescence microscopes, thereby providing precise measurements of template concentrations. Notably, our approach exhibits an analysis speed of quantifying over 2,000 microreactors (across 10 images) within remarkably 2.5 seconds, and a dynamic range spanning from 56.52 to 1569.43 copies per micron-liter. Furthermore, our Deep-dGFP algorithm showcases remarkable generalization capabilities, as it can be directly applied to various GFP-labeling scenarios, including droplet-based, microwell-based, and agarose-based biological applications. To the best of our knowledge, this represents the first successful implementation of an all-in-one image analysis algorithm in droplet digital PCR (polymerase chain reaction), microwell digital PCR, droplet single-cell sequencing, agarose digital PCR, and bacterial quantification, without necessitating any transfer learning steps, modifications, or retraining procedures. We firmly believe that our Deep-dGFP technique will be readily embraced by biomedical laboratories and holds potential for further development in related clinical applications.
</details>
<details>
<summary>摘要</summary>
完全量化生物样本的过程涉及到准确地测量表达水平，提供了更高的精度和性能，特别是 для罕见的模板。然而，现有的方法ologies有许多限制：流环计仪器昂贵且复杂，而基于软件工具或手动计数的抗体影像扫描是时间consuming且容易出错。在这种研究中，我们开发了一个全面的深度学习启用的管道，允许自动分割和分类GFP（绿色荧光蛋白）标记的微 реактор，实现实时精确量化。我们的发现表明该技术可以准确预测微 реактор的大小和占用状态，从而提供精确的模板含量测量。尤其是，我们的方法在2.5秒钟内可以量化10个图像中的超过2,000个微 реактор，并且具有从56.52到1569.43个浮现每毫升的范围。此外，我们的深度dGFP算法具有remarkable泛化能力，可以直接应用于不同的GFP标记场景，包括液滴基础、微瓶基础和agarose基础的生物应用。根据我们所知，这是第一个成功地实现的所有在一个图像分析算法，无需进行转移学习步骤、修改或重新训练。我们 firmly believe that our Deep-dGFP technique will be readily embraced by biomedical laboratories and holds potential for further development in related clinical applications。
</details></li>
</ul>
<hr>
<h2 id="FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation"><a href="#FAU-Net-An-Attention-U-Net-Extension-with-Feature-Pyramid-Attention-for-Prostate-Cancer-Segmentation" class="headerlink" title="FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation"></a>FAU-Net: An Attention U-Net Extension with Feature Pyramid Attention for Prostate Cancer Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01322">http://arxiv.org/abs/2309.01322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pablo Cesar Quihui-Rubio, Daniel Flores-Araiza, Miguel Gonzalez-Mendoza, Christian Mata, Gilberto Ochoa-Ruiz</li>
<li>for: 这篇研究旨在提出一种基于U-Net的深度学习方法来分类肝脏中的不同区域，以提高肝癌检测和诊断的工作流程。</li>
<li>methods: 本研究使用了增加和特征层 pyramid 注意模组，并与七种不同的 U-Net 架构进行比较。</li>
<li>results: 实验结果显示，提案的方法在测试集中具有了84.15%的自动分类性能和76.9%的重 overlap率，与大多数研究中的模型相比，仅次于 R2U-Net 和 attention R2U-Net 架构。<details>
<summary>Abstract</summary>
This contribution presents a deep learning method for the segmentation of prostate zones in MRI images based on U-Net using additive and feature pyramid attention modules, which can improve the workflow of prostate cancer detection and diagnosis. The proposed model is compared to seven different U-Net-based architectures. The automatic segmentation performance of each model of the central zone (CZ), peripheral zone (PZ), transition zone (TZ) and Tumor were evaluated using Dice Score (DSC), and the Intersection over Union (IoU) metrics. The proposed alternative achieved a mean DSC of 84.15% and IoU of 76.9% in the test set, outperforming most of the studied models in this work except from R2U-Net and attention R2U-Net architectures.
</details>
<details>
<summary>摘要</summary>
这个贡献提出了基于U-Net深度学习方法的抑制肾阶段分割方法，使用加法和特征层 pyramid 注意模块，以提高肾癌检测和诊断的工作流程。该提案的模型与七种不同的U-Net建筑物进行比较。自动 segmentation 性能的评价指标包括中心zone (CZ)、 périphérique zone (PZ)、 transition zone (TZ) 和肿瘤等部分的 dice 分数 (DSC) 和交集 overlap 指标 (IoU)。提案的代替方案在测试集上实现了84.15%的平均 DSC 和 76.9%的交集 overlap，比大多数研究模型都高，只有R2U-Net和注意力 R2U-Net 建筑物超过。
</details></li>
</ul>
<hr>
<h2 id="An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery"><a href="#An-FPGA-smart-camera-implementation-of-segmentation-models-for-drone-wildfire-imagery" class="headerlink" title="An FPGA smart camera implementation of segmentation models for drone wildfire imagery"></a>An FPGA smart camera implementation of segmentation models for drone wildfire imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01318">http://arxiv.org/abs/2309.01318</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eduardo Guarduño-Martinez, Jorge Ciprian-Sanchez, Gerardo Valente, Vazquez-Garcia, Gerardo Rodriguez-Hernandez, Adriana Palacios-Rosas, Lucile Rossi-Tisson, Gilberto Ochoa-Ruiz</li>
<li>for: 这个论文旨在应对野火战，使用附加了可见和红外摄像头的无人机进行识别、监测和火势评估。</li>
<li>methods: 这个研究使用了智能摄像头，基于低功耗的场程可编程阵列（FPGAs），并与二进制神经网络（BNNs）结合，实现在边缘计算中的实时识别。</li>
<li>results: 研究人员对 corsica 火灾数据库进行了 segmentation 模型的实现，通过减少和量化原始模型，将参数数量减少了90%，并通过进一步优化提高了原始模型的吞吐率从8帧每秒（FPS）提高到33.63 FPS，而无损失分 segmentation 性能。<details>
<summary>Abstract</summary>
Wildfires represent one of the most relevant natural disasters worldwide, due to their impact on various societal and environmental levels. Thus, a significant amount of research has been carried out to investigate and apply computer vision techniques to address this problem. One of the most promising approaches for wildfire fighting is the use of drones equipped with visible and infrared cameras for the detection, monitoring, and fire spread assessment in a remote manner but in close proximity to the affected areas. However, implementing effective computer vision algorithms on board is often prohibitive since deploying full-precision deep learning models running on GPU is not a viable option, due to their high power consumption and the limited payload a drone can handle. Thus, in this work, we posit that smart cameras, based on low-power consumption field-programmable gate arrays (FPGAs), in tandem with binarized neural networks (BNNs), represent a cost-effective alternative for implementing onboard computing on the edge. Herein we present the implementation of a segmentation model applied to the Corsican Fire Database. We optimized an existing U-Net model for such a task and ported the model to an edge device (a Xilinx Ultra96-v2 FPGA). By pruning and quantizing the original model, we reduce the number of parameters by 90%. Furthermore, additional optimizations enabled us to increase the throughput of the original model from 8 frames per second (FPS) to 33.63 FPS without loss in the segmentation performance: our model obtained 0.912 in Matthews correlation coefficient (MCC),0.915 in F1 score and 0.870 in Hafiane quality index (HAF), and comparable qualitative segmentation results when contrasted to the original full-precision model. The final model was integrated into a low-cost FPGA, which was used to implement a neural network accelerator.
</details>
<details>
<summary>摘要</summary>
野火是全球最重要的自然灾害之一，它对社会和环境层次产生了深远的影响。因此，许多研究已经进行了，以应用计算机掌握技术来解决这个问题。一种非常有前途的方法是使用具有可见光和红外线摄像头的无人机，以远程方式进行野火探测、监控和火伤评估。然而，实现有效的计算机掌握算法在无人机上是不可能的，因为它们的高功耗和无人机的传输能力有限。因此，在这个工作中，我们认为使用智能摄像头，基于低功耗的可程式遮盾类（FPGAs），可以成为一种成本效益的选择。我们在这里透过将存储在FPGAs上的低功耗摄像头与二进制神经网络（BNNs）联合使用，以实现在边缘上的处理。我们将一个原始的U-Net模型优化 для这个任务，并将模型转移到边缘设备（Xilinx Ultra96-v2 FPGA）上。通过剪裁和数值化原始模型，我们缩减了模型的参数数量，从8帧每秒（FPS）提高到33.63 FPS，而无损于分类性能：我们的模型获得了0.912的均方误差系数（MCC）、0.915的F1分数和0.870的哈菲安质量指数（HAF），并且和原始全精度模型的分类结果相似。最终模型被集成到一个低成本的FPGAs上，实现了一个神经网络加速器。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/04/eess.IV_2023_09_04/" data-id="clmjn91qu00hh0j883oc3ckcc" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/cs.SD_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T15:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/03/cs.SD_2023_09_03/">cs.SD - 2023-09-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="NADiffuSE-Noise-aware-Diffusion-based-Model-for-Speech-Enhancement"><a href="#NADiffuSE-Noise-aware-Diffusion-based-Model-for-Speech-Enhancement" class="headerlink" title="NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement"></a>NADiffuSE: Noise-aware Diffusion-based Model for Speech Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01212">http://arxiv.org/abs/2309.01212</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wen Wang, Dongchao Yang, Qichen Ye, Bowen Cao, Yuexian Zou</li>
<li>for: 提高干扰音频信号的清晰度（Speech Enhancement）</li>
<li>methods: 使用扩散模型（Diffusion Model）和 anchor-based inference算法，以及三种模型变体（Multi-stage SE based on preprocessing networks for Mel spectrograms）</li>
<li>results: NADiffuSE模型比其他基于GPC结构的扩散模型（Diffusion Model）表现更好，并且可以更好地处理实际中的干扰音频信号。<details>
<summary>Abstract</summary>
The goal of speech enhancement (SE) is to eliminate the background interference from the noisy speech signal. Generative models such as diffusion models (DM) have been applied to the task of SE because of better generalization in unseen noisy scenes. Technical routes for the DM-based SE methods can be summarized into three types: task-adapted diffusion process formulation, generator-plus-conditioner (GPC) structures and the multi-stage frameworks. We focus on the first two approaches, which are constructed under the GPC architecture and use the task-adapted diffusion process to better deal with the real noise. However, the performance of these SE models is limited by the following issues: (a) Non-Gaussian noise estimation in the task-adapted diffusion process. (b) Conditional domain bias caused by the weak conditioner design in the GPC structure. (c) Large amount of residual noise caused by unreasonable interpolation operations during inference. To solve the above problems, we propose a noise-aware diffusion-based SE model (NADiffuSE) to boost the SE performance, where the noise representation is extracted from the noisy speech signal and introduced as a global conditional information for estimating the non-Gaussian components. Furthermore, the anchor-based inference algorithm is employed to achieve a compromise between the speech distortion and noise residual. In order to mitigate the performance degradation caused by the conditional domain bias in the GPC framework, we investigate three model variants, all of which can be viewed as multi-stage SE based on the preprocessing networks for Mel spectrograms. Experimental results show that NADiffuSE outperforms other DM-based SE models under the GPC infrastructure. Audio samples are available at: https://square-of-w.github.io/NADiffuSE-demo/.
</details>
<details>
<summary>摘要</summary>
目标是减少背景干扰，使得讲话信号更加清晰。生成模型如扩散模型（DM）已经应用于讲话减少频率频谱中的讲话干扰。技术 Routes 可以分为三类：任务适应扩散过程形式、生成器+条件器（GPC）结构和多阶段框架。我们主要关注前两种方法，它们都是基于 GPC 架构，并使用任务适应的扩散过程来更好地处理真实的干扰。然而，这些减少模型的性能受到以下问题的限制：（a）任务适应扩散过程中的非高斯噪声估计。（b）GPC 结构中的弱条件器设计导致的条件频谱偏见。（c）在推理过程中不合理的插值操作导致的剩余噪声。为了解决以上问题，我们提出一种噪声意识的扩散基于减少模型（NADiffuSE），其中噪声表示被提取自干扰讲话信号，并作为全局的条件信息来估计非高斯噪声成分。此外，我们采用了 anchor-based 推理算法，以实现在推理过程中取得讲话质量和噪声剩余之间的平衡。为了减少 GPC 框架中的条件频谱偏见，我们进行了三种模型变体的调查，它们都可以视为基于 Mel spectrogram 的预处理网络的多阶段减少。实验结果显示，NADiffuSE 在 GPC 结构下表现出色，超过了其他 DM-based 减少模型。听音amples 可以在以下网址中找到：https://square-of-w.github.io/NADiffuSE-demo/.
</details></li>
</ul>
<hr>
<h2 id="MAGMA-Music-Aligned-Generative-Motion-Autodecoder"><a href="#MAGMA-Music-Aligned-Generative-Motion-Autodecoder" class="headerlink" title="MAGMA: Music Aligned Generative Motion Autodecoder"></a>MAGMA: Music Aligned Generative Motion Autodecoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01202">http://arxiv.org/abs/2309.01202</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sohan Anisetty, Amit Raj, James Hays</li>
<li>for: 本研究旨在实现将音乐转换为舞蹈的问题，需要空间和时间协调，并且随音乐进程的同步。</li>
<li>methods: 我们提出了一个2步方法，使用Vector Quantized-Variational Autoencoder（VQ-VAE）将动作转换为基本动作，然后使用Transformer解oder进行正确的动作顺序排序。</li>
<li>results: 我们的提案方法可以实现现代最佳的音乐到动作转换结果，并且可以生成较长的动作序列，且可以将动作序列串接起来无缝汇流，同时也可以根据 Style 需求进行易于自定义的动作序列。<details>
<summary>Abstract</summary>
Mapping music to dance is a challenging problem that requires spatial and temporal coherence along with a continual synchronization with the music's progression. Taking inspiration from large language models, we introduce a 2-step approach for generating dance using a Vector Quantized-Variational Autoencoder (VQ-VAE) to distill motion into primitives and train a Transformer decoder to learn the correct sequencing of these primitives. We also evaluate the importance of music representations by comparing naive music feature extraction using Librosa to deep audio representations generated by state-of-the-art audio compression algorithms. Additionally, we train variations of the motion generator using relative and absolute positional encodings to determine the effect on generated motion quality when generating arbitrarily long sequence lengths. Our proposed approach achieve state-of-the-art results in music-to-motion generation benchmarks and enables the real-time generation of considerably longer motion sequences, the ability to chain multiple motion sequences seamlessly, and easy customization of motion sequences to meet style requirements.
</details>
<details>
<summary>摘要</summary>
mapping music to dance 是一个具有挑战性的问题，需要空间和时间协调，同时与音乐的进程保持同步。参考大型自然语言模型，我们提出了一个二步方法，使用量化-自适应学来将动作压缩成基本动作，然后使用Transformer解oder来学习正确的动作顺序。我们还评估了音乐表示的重要性，比较了使用Librosa提取音乐特征和深度音乐特征生成的音乐特征。此外，我们还训练了不同的动作生成器使用相对和绝对位置编码，以决定生成动作质量的影响，并且可以生成无限长的动作序列，排序动作序列，和根据风格需求轻松定制动作序列。我们的提案方法在音乐到动作生成的标准参考资料上实现了州立顶峰结果，并且可以实现无限长的动作序列生成、排序动作序列和根据风格需求轻松定制动作序列。
</details></li>
</ul>
<hr>
<h2 id="MSM-VC-High-fidelity-Source-Style-Transfer-for-Non-Parallel-Voice-Conversion-by-Multi-scale-Style-Modeling"><a href="#MSM-VC-High-fidelity-Source-Style-Transfer-for-Non-Parallel-Voice-Conversion-by-Multi-scale-Style-Modeling" class="headerlink" title="MSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-scale Style Modeling"></a>MSM-VC: High-fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-scale Style Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01142">http://arxiv.org/abs/2309.01142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhichao Wang, Xinsheng Wang, Qicong Xie, Tao Li, Lei Xie, Qiao Tian, Yuping Wang</li>
<li>for: 这篇论文主要针对的是voice conversion（VC）任务中的语言表达风格模型化。</li>
<li>methods: 本文提出了一种多尺度风格模型化方法（MSM-VC），该方法从不同层次模型源语言风格，包括帧层、本地层和全局层。具体来说，该方法使用不同的特征表示来模型每个层的风格，包括语音特征、预训练ASR模型的瓶颈特征和通过自然语言处理技术学习的特征。</li>
<li>results: 实验结果表明，MSM-VC在高度表情语言库中比前置的VC方法更好地模型源语言风格，同时保持良好的语音质量和 speaker相似性。<details>
<summary>Abstract</summary>
In addition to conveying the linguistic content from source speech to converted speech, maintaining the speaking style of source speech also plays an important role in the voice conversion (VC) task, which is essential in many scenarios with highly expressive source speech, such as dubbing and data augmentation. Previous work generally took explicit prosodic features or fixed-length style embedding extracted from source speech to model the speaking style of source speech, which is insufficient to achieve comprehensive style modeling and target speaker timbre preservation. Inspired by the style's multi-scale nature of human speech, a multi-scale style modeling method for the VC task, referred to as MSM-VC, is proposed in this paper. MSM-VC models the speaking style of source speech from different levels. To effectively convey the speaking style and meanwhile prevent timbre leakage from source speech to converted speech, each level's style is modeled by specific representation. Specifically, prosodic features, pre-trained ASR model's bottleneck features, and features extracted by a model trained with a self-supervised strategy are adopted to model the frame, local, and global-level styles, respectively. Besides, to balance the performance of source style modeling and target speaker timbre preservation, an explicit constraint module consisting of a pre-trained speech emotion recognition model and a speaker classifier is introduced to MSM-VC. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to improve the disentanglement ability and alleviate the mismatch between training and inference. Experiments performed on the highly expressive speech corpus demonstrate that MSM-VC is superior to the state-of-the-art VC methods for modeling source speech style while maintaining good speech quality and speaker similarity.
</details>
<details>
<summary>摘要</summary>
在 voice conversion (VC) 任务中，保持源语音的发音风格对于许多场景都是非常重要的，如 dubbing 和数据增强。过去的工作通常使用源语音中的显式拥有的拥有特征或固定长度的风格嵌入来模型源语音的发音风格，这是不够完整地模型发音风格和目标说话人的时光质量保持。根据人类语音的多尺度特征，本文提出了一种多尺度风格模型ing方法（MSM-VC），用于模型源语音的发音风格。MSM-VC 模型源语音的发音风格从不同的水平。为了有效地传递发音风格并避免源语音的时光泄露，每个水平的风格都使用特定的表示方式进行模型。具体来说，使用 prosodic 特征、预训练 ASR 模型的瓶颈特征和通过一种自我超VI等方法提取的特征来模型帧、本地和全局级别的风格。此外，为了保持源风格模型和目标说话人的时光质量，我们引入了一个显式约束模块，该模块包括一个预训练的语音情感识别模型和一个说话人分类器。这个约束模块也使得在训练时可以模拟风格传递推理过程，以提高分离度和减少训练和推理之间的差异。在高度表达的语音集上进行的实验表明，MSM-VC 在模型源语音风格的同时保持良好的语音质量和说话人相似性方面表现出色。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/cs.SD_2023_09_03/" data-id="clmjn91oa00bn0j88895tg3t8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/cs.LG_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T10:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/03/cs.LG_2023_09_03/">cs.LG - 2023-09-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generative-Social-Choice"><a href="#Generative-Social-Choice" class="headerlink" title="Generative Social Choice"></a>Generative Social Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01291">http://arxiv.org/abs/2309.01291</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/babatundeibukun/simple-social-learning-environment">https://github.com/babatundeibukun/simple-social-learning-environment</a></li>
<li>paper_authors: Sara Fish, Paul Gölz, David C. Parkes, Ariel D. Procaccia, Gili Rusak, Itai Shapira, Manuel Wüthrich</li>
<li>for: 这篇论文是为了探讨人工智能在民主过程中的应用，具体来说是如何使用自然语言处理技术来实现民主选举。</li>
<li>methods: 这篇论文使用了社会选择理论的数学严谨性和大自然语言模型的文本生成能力，提出了一个生成社会选择框架，可以帮助解决复杂的民主选举问题。</li>
<li>results: 通过应用这个框架，可以生成一个代表民意的评论文本，例如在在线审议过程中。<details>
<summary>Abstract</summary>
Traditionally, social choice theory has only been applicable to choices among a few predetermined alternatives but not to more complex decisions such as collectively selecting a textual statement. We introduce generative social choice, a framework that combines the mathematical rigor of social choice theory with large language models' capability to generate text and extrapolate preferences. This framework divides the design of AI-augmented democratic processes into two components: first, proving that the process satisfies rigorous representation guarantees when given access to oracle queries; second, empirically validating that these queries can be approximately implemented using a large language model. We illustrate this framework by applying it to the problem of generating a slate of statements that is representative of opinions expressed as free-form text, for instance in an online deliberative process.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Federated-Orthogonal-Training-Mitigating-Global-Catastrophic-Forgetting-in-Continual-Federated-Learning"><a href="#Federated-Orthogonal-Training-Mitigating-Global-Catastrophic-Forgetting-in-Continual-Federated-Learning" class="headerlink" title="Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning"></a>Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01289">http://arxiv.org/abs/2309.01289</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yavuz Faruk Bakman, Duygu Nur Yaldiz, Yahya H. Ezzeldin, Salman Avestimehr</li>
<li>for: 这个研究旨在解决在 Federated Learning (FL) 中发生的 Global Catastrophic Forgetting (GCF) 问题，GCF 是指当模型学习新任务时，对旧任务的性能下降的问题。</li>
<li>methods: 我们提出了一个新的方法，即 Federated Orthogonal Training (FOT)，它可以减少 зада务之间的干扰，并且避免违反 Federated Learning 的隐私原则。FOT 使用了对 old tasks 的全球输入子空间抽出，并且对 new tasks 的聚合更新进行修改，使其与 old tasks 的全球主成分空间垂直对错。</li>
<li>results: 我们的实验结果显示，FOT 可以在 Continual Federated Learning (CFL) 设定中优化性能，实现对旧任务的15%的精度提升，同时降低了27%的遗传和通信成本。<details>
<summary>Abstract</summary>
Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting in CFL. Our algorithm extracts the global input subspace of each layer for old tasks and modifies the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer. This decreases the interference between tasks, which is the main cause for forgetting. We empirically show that FOT outperforms state-of-the-art continual learning methods in the CFL setting, achieving an average accuracy gain of up to 15% with 27% lower forgetting while only incurring a minimal computation and communication cost.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 已经吸引了广泛关注，因为它可以在分布式数据上进行隐私保护的训练。现有文献中大多数研究都是单任务学习。然而，随着时间的推移，客户端上可能会出现新的任务，global模型应该学习这些任务而不会忘记之前的任务。这种真实情况被称为 Continual Federated Learning (CFL)。CFL的主要挑战是全球性衰减，即当全球模型在新任务上进行训练时，其对于老任务的性能下降。有些最近的研究提出了一些方法来解决全球性衰减问题，但这些方法都假设了可以获得过去数据样本的可用性，或者违反了 Federated Learning 的隐私原则。我们提出了一种新的方法，即 Federated Orthogonal Training (FOT)，以解决这些挑战。我们的算法抽取每层的全球输入子空间 для老任务，并将新任务的聚合更新修改为在每层上与老任务的全球主成分空间垂直的。这会减少任务之间的干扰，这是全球性衰减的主要原因。我们实验表明，FOT可以在 CFL 设置中超越当前最佳的连续学习方法，实现了最高的准确率提升达到 15%，同时具有27% 更低的忘记率，只有 minimal 的计算和通信成本。
</details></li>
</ul>
<hr>
<h2 id="A-Comparative-Evaluation-of-FedAvg-and-Per-FedAvg-Algorithms-for-Dirichlet-Distributed-Heterogeneous-Data"><a href="#A-Comparative-Evaluation-of-FedAvg-and-Per-FedAvg-Algorithms-for-Dirichlet-Distributed-Heterogeneous-Data" class="headerlink" title="A Comparative Evaluation of FedAvg and Per-FedAvg Algorithms for Dirichlet Distributed Heterogeneous Data"></a>A Comparative Evaluation of FedAvg and Per-FedAvg Algorithms for Dirichlet Distributed Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01275">http://arxiv.org/abs/2309.01275</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamza Reguieg, Mohammed El Hanjri, Mohamed El Kamili, Abdellatif Kobbane</li>
<li>for:  investigate Federated Learning (FL) and compare two strategies within this paradigm: Federated Averaging (FedAvg) and Personalized Federated Averaging (Per-FedAvg)</li>
<li>methods:  use Non-Identically and Independently Distributed (Non-IID) data to evaluate the performance of both strategies</li>
<li>results:  Per-FedAvg shows superior robustness in conditions of high data heterogeneity, and our results provide insights into the development of more effective and efficient machine learning strategies in a decentralized setting.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个论文 investigate Federated Learning (FL) 并比较这个 парадиг中两种策略：Federated Averaging (FedAvg) 和 Personalized Federated Averaging (Per-FedAvg)</li>
<li>methods: 使用 Non-Identically and Independently Distributed (Non-IID) 数据来评估这两种策略的性能</li>
<li>results: Per-FedAvg 在高度不一致的数据下表现出较好的Robustness，我们的结果提供了在分布式设置下开发更有效和高效的机器学习策略的指导。<details>
<summary>Abstract</summary>
In this paper, we investigate Federated Learning (FL), a paradigm of machine learning that allows for decentralized model training on devices without sharing raw data, there by preserving data privacy. In particular, we compare two strategies within this paradigm: Federated Averaging (FedAvg) and Personalized Federated Averaging (Per-FedAvg), focusing on their performance with Non-Identically and Independently Distributed (Non-IID) data. Our analysis shows that the level of data heterogeneity, modeled using a Dirichlet distribution, significantly affects the performance of both strategies, with Per-FedAvg showing superior robustness in conditions of high heterogeneity. Our results provide insights into the development of more effective and efficient machine learning strategies in a decentralized setting.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们研究了联邦学习（Federated Learning，FL），一种机器学习 paradigma，允许在设备上进行分散式模型训练，而不需要分享原始数据，从而保持数据隐私。特别是，我们对两种策略进行比较：联邦均值（FedAvg）和个性化联邦均值（Per-FedAvg），并将注重非标一样分布（Non-IID）数据的性能。我们的分析表明，数据不同程度的不同，通过 Dirichlet 分布来模型，对两种策略的性能产生了显著影响，Per-FedAvg 在高度不同化的情况下表现出了更高的鲁棒性。我们的结果为开发更有效率的分布式机器学习策略提供了洞察。
</details></li>
</ul>
<hr>
<h2 id="COMEDIAN-Self-Supervised-Learning-and-Knowledge-Distillation-for-Action-Spotting-using-Transformers"><a href="#COMEDIAN-Self-Supervised-Learning-and-Knowledge-Distillation-for-Action-Spotting-using-Transformers" class="headerlink" title="COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers"></a>COMEDIAN: Self-Supervised Learning and Knowledge Distillation for Action Spotting using Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01270">http://arxiv.org/abs/2309.01270</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/juliendenize/eztorch">https://github.com/juliendenize/eztorch</a></li>
<li>paper_authors: Julien Denize, Mykola Liashuha, Jaonary Rabarisoa, Astrid Orcesi, Romain Hérault</li>
<li>for: 这 paper 是为了提出一种用于动作检测的 Initialization 管道，即 COMEDIAN，该管道包括自动学习和知识储存两个 initialization 阶段。</li>
<li>methods: 该 paper 使用了两个 initialization 阶段，首先是使用短视频作为输入进行自动学习初始化 spatial transformer，然后是通过知识储存来增强 spatial transformer 的输出，并在最后一步进行 fine-tuning。</li>
<li>results: 实验结果表明，COMEDIAN 的预训练方法可以在 SoccerNet-v2 数据集上达到状态作卷积的性能，并且比非预训练模型更快地 converges。这些结果表明 COMEDIAN 的预训练管道的有效性。<details>
<summary>Abstract</summary>
We present COMEDIAN, a novel pipeline to initialize spatio-temporal transformers for action spotting, which involves self-supervised learning and knowledge distillation. Action spotting is a timestamp-level temporal action detection task. Our pipeline consists of three steps, with two initialization stages. First, we perform self-supervised initialization of a spatial transformer using short videos as input. Additionally, we initialize a temporal transformer that enhances the spatial transformer's outputs with global context through knowledge distillation from a pre-computed feature bank aligned with each short video segment. In the final step, we fine-tune the transformers to the action spotting task. The experiments, conducted on the SoccerNet-v2 dataset, demonstrate state-of-the-art performance and validate the effectiveness of COMEDIAN's pretraining paradigm. Our results highlight several advantages of our pretraining pipeline, including improved performance and faster convergence compared to non-pretrained models.
</details>
<details>
<summary>摘要</summary>
我团队现象了一个名为COMEDIAN的新的引导管道，用于初始化时空转换器以进行动作检测，这个任务是基于时间戳的时间序列动作检测。我们的管道包括三个步骤，其中有两个初始化阶段。第一个阶段是使用短视频作为输入进行自主学习初始化一个空间转换器。其次，我们使用知识储存的技术进行知识储存初始化，使得时空转换器的输出得到全局上下文的增强。最后，我们精度调整transformer来进行动作检测任务。在 SoccerNet-v2 数据集上进行的实验表明，我们的预训练模型可以达到领先的性能水平，并且比非预训练模型更快地 converge。我们的结果表明，COMEDIAN的预训练方案具有以下优势：性能提高和更快的 converge。
</details></li>
</ul>
<hr>
<h2 id="Learning-Aware-Safety-for-Interactive-Autonomy"><a href="#Learning-Aware-Safety-for-Interactive-Autonomy" class="headerlink" title="Learning-Aware Safety for Interactive Autonomy"></a>Learning-Aware Safety for Interactive Autonomy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01267">http://arxiv.org/abs/2309.01267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haimin Hu, Zixu Zhang, Kensuke Nakamura, Andrea Bajcsy, Jaime F. Fisac</li>
<li>for: 本研究旨在提供一种新的关闭Loop方法，以确保机器人系统在实时学习和适应的情况下保持安全交互。</li>
<li>methods: 该方法使用反抗搅ء深度学习来规避未来可能的enario，并同时考虑机器人学习算法的内部信念的变化。</li>
<li>results: 研究人员使用这种方法可以 tractable safety analysis，并且可以处理高维度的情况。此外，他们还能够证明这种方法可以与 bayesian belief propagation和大型预训练神经轨迹预测器结合使用。<details>
<summary>Abstract</summary>
One of the outstanding challenges for the widespread deployment of robotic systems like autonomous vehicles is ensuring safe interaction with humans without sacrificing efficiency. Existing safety analysis methods often neglect the robot's ability to learn and adapt at runtime, leading to overly conservative behavior. This paper proposes a new closed-loop paradigm for synthesizing safe control policies that explicitly account for the system's evolving uncertainty under possible future scenarios. The formulation reasons jointly about the physical dynamics and the robot's learning algorithm, which updates its internal belief over time. We leverage adversarial deep reinforcement learning (RL) for scaling to high dimensions, enabling tractable safety analysis even for implicit learning dynamics induced by state-of-the-art prediction models. We demonstrate our framework's ability to work with both Bayesian belief propagation and the implicit learning induced by a large pre-trained neural trajectory predictor.
</details>
<details>
<summary>摘要</summary>
一个重要挑战是使机器人系统广泛部署，是确保安全地与人类交互，不 sacrificing efficiency。现有的安全分析方法经常忽视机器人的学习和运行时间的可变性，导致行为过于保守。这篇论文提出了一种新的封闭循环方案，用于生成安全的控制策略，并直接考虑系统的演化uncertainty的可能性。我们利用对抗式深度学习游戏学习（RL），以便在高维度上进行可观察的安全分析，即使是由state-of-the-art预测模型引起的隐式学习动态。我们示例了我们框架的可用性，使用 bayesian belief propagation 和大规模预测模型来驱动机器人的学习和控制。
</details></li>
</ul>
<hr>
<h2 id="Multimodal-Contrastive-Learning-with-Hard-Negative-Sampling-for-Human-Activity-Recognition"><a href="#Multimodal-Contrastive-Learning-with-Hard-Negative-Sampling-for-Human-Activity-Recognition" class="headerlink" title="Multimodal Contrastive Learning with Hard Negative Sampling for Human Activity Recognition"></a>Multimodal Contrastive Learning with Hard Negative Sampling for Human Activity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01262">http://arxiv.org/abs/2309.01262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyeongju Choi, Apoorva Beedu, Irfan Essa</li>
<li>for: 本研究旨在提高人体活动识别（HAR）系统的性能，使其能够在有限数据量的情况下进行自监督学习。</li>
<li>methods: 本研究使用了自监督学习的contrastive学习方法，并提出了一种基于硬negative sampling的方法来选择良好的负样本。</li>
<li>results: 通过对UTD-MHAD和MMAct两个benchmark dataset进行广泛的实验，本研究证明了我们的方法可以在有限数据量下学习强大的特征表示，并在下游活动识别任务上超过所有state-of-the-art方法。<details>
<summary>Abstract</summary>
Human Activity Recognition (HAR) systems have been extensively studied by the vision and ubiquitous computing communities due to their practical applications in daily life, such as smart homes, surveillance, and health monitoring.   Typically, this process is supervised in nature and the development of such systems requires access to large quantities of annotated data.   However, the higher costs and challenges associated with obtaining good quality annotations have rendered the application of self-supervised methods an attractive option and contrastive learning comprises one such method.   However, a major component of successful contrastive learning is the selection of good positive and negative samples.   Although positive samples are directly obtainable, sampling good negative samples remain a challenge.   As human activities can be recorded by several modalities like camera and IMU sensors, we propose a hard negative sampling method for multimodal HAR with a hard negative sampling loss for skeleton and IMU data pairs.   We exploit hard negatives that have different labels from the anchor but are projected nearby in the latent space using an adjustable concentration parameter.   Through extensive experiments on two benchmark datasets: UTD-MHAD and MMAct, we demonstrate the robustness of our approach forlearning strong feature representation for HAR tasks, and on the limited data setting.   We further show that our model outperforms all other state-of-the-art methods for UTD-MHAD dataset, and self-supervised methods for MMAct: Cross session, even when uni-modal data are used during downstream activity recognition.
</details>
<details>
<summary>摘要</summary>
人类活动识别（HAR）系统已经广泛研究于计算机视觉和无线计算领域，因为它们在日常生活中有广泛的应用，如智能家居、监控和健康监测。通常，这个过程是有监督性的，需要大量的注释数据来进行开发。然而，获取高质量的注释数据的高成本和挑战使得自监学习方法变得吸引人。在这些方法中，对比学习是一种吸引人的选择。然而，成功的对比学习需要选择好的正例和负例样本。正例样本直接可以获得，但是找到好的负例样本仍然是一个挑战。因为人类活动可以通过相机和IMU传感器记录，我们提议一种基于硬negative sampling的多modal HAR方法。我们利用硬negative sampling的损失函数，通过调整可变的集中参数，将不同标签的样本映射到近似的 latent space 中。通过大量的实验在UTD-MHAD和MMAct两个标准 benchmark dataset 上，我们证明了我们的方法可以学习强大的特征表示，并在有限的数据设置下进行下游活动识别任务。我们还证明了我们的模型在UTD-MHAD数据集上超过所有现有的状态艺术方法，并在单Modal数据被用于下游活动识别任务时也表现出色。
</details></li>
</ul>
<hr>
<h2 id="Large-AI-Model-Empowered-Multimodal-Semantic-Communications"><a href="#Large-AI-Model-Empowered-Multimodal-Semantic-Communications" class="headerlink" title="Large AI Model Empowered Multimodal Semantic Communications"></a>Large AI Model Empowered Multimodal Semantic Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01249">http://arxiv.org/abs/2309.01249</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Xiaohu You</li>
<li>for: 提供一个具有低延迟和高质量的语义层次沟通（SC）体验，使用多modal信号（包括文本、音频、图像和视频）进行集成。</li>
<li>methods: 使用大型AI模型（特别是多modal语言模型（MLM）和大型语言模型（LLM））解决数据不一致性、semantic抽象和信号抖动等问题。</li>
<li>results: 提出一个基于大型AI模型的多modal SC（LAM-MSC）框架，包括MLM基于多modalAlignment（MMA）、个性化LLM基于知识库（LKB）和Conditional Generative Adversarial Networks（CGE）等方法，实现高质量、低延迟的语义层次沟通。<details>
<summary>Abstract</summary>
Multimodal signals, including text, audio, image and video, can be integrated into Semantic Communication (SC) for providing an immersive experience with low latency and high quality at the semantic level. However, the multimodal SC has several challenges, including data heterogeneity, semantic ambiguity, and signal fading. Recent advancements in large AI models, particularly in Multimodal Language Model (MLM) and Large Language Model (LLM), offer potential solutions for these issues. To this end, we propose a Large AI Model-based Multimodal SC (LAM-MSC) framework, in which we first present the MLM-based Multimodal Alignment (MMA) that utilizes the MLM to enable the transformation between multimodal and unimodal data while preserving semantic consistency. Then, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows users to perform personalized semantic extraction or recovery through the LLM. This effectively addresses the semantic ambiguity. Finally, we apply the Conditional Generative adversarial networks-based channel Estimation (CGE) to obtain Channel State Information (CSI). This approach effectively mitigates the impact of fading channels in SC. Finally, we conduct simulations that demonstrate the superior performance of the LAM-MSC framework.
</details>
<details>
<summary>摘要</summary>
多Modal信号，包括文本、音频、图像和视频，可以在Semantic Communication（SC）中集成，以提供低延迟和高质量的具体性体验。然而，多Modal SC 存在多个挑战，包括数据不一致、 semantics 抽象和信号衰减。现代大AI模型，特别是多Modal语言模型（MLM）和大语言模型（LLM），提供了解决这些问题的可能性。为此，我们提议一个基于大AI模型的多Modal SC 框架（LAM-MSC），在这里我们首先提出了基于 MLM 的多Modal对接（MMA），使得在多Modal和单Modal数据之间进行转换，保持 semantics 一致。然后，我们提出了个性化的 LLM-based Knowledge Base（LKB），允许用户进行个性化的具体性抽取或恢复，从而有效解决 semantics 抽象。最后，我们应用 Conditional Generative Adversarial Networks 基于渠道估计（CGE），以获取渠道状态信息（CSI）。这种方法有效地减少了干扰通道的影响，从而提高 SC 的性能。最后，我们进行了临床实验，并证明了 LAM-MSC 框架的超越性。
</details></li>
</ul>
<hr>
<h2 id="Modified-Step-Size-for-Enhanced-Stochastic-Gradient-Descent-Convergence-and-Experiments"><a href="#Modified-Step-Size-for-Enhanced-Stochastic-Gradient-Descent-Convergence-and-Experiments" class="headerlink" title="Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and Experiments"></a>Modified Step Size for Enhanced Stochastic Gradient Descent: Convergence and Experiments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01248">http://arxiv.org/abs/2309.01248</a></li>
<li>repo_url: None</li>
<li>paper_authors: M. Soheil Shamaee, S. Fathi Hafshejani</li>
<li>for: 提高泊braceSGD算法性能</li>
<li>methods:  integrate modified decay step size based on $\frac{1}{\sqrt{t}$，含有对数函数，选择最小值</li>
<li>results: 实验表明，在图像分类任务中，使用提档的步长策略可以提高准确率，与传统步长策略相比，提高0.5%和1.4%。Here’s the breakdown of each sentence:* “for”: 指出文章的目的是提高泊braceSGD算法性能。* “methods”: 描述文章提出的方法，即使 modified decay step size based on $\frac{1}{\sqrt{t}$，含有对数函数，选择最小值。* “results”:  SUMMARIZE the main findings of the paper, which is the improvement in accuracy achieved by using the proposed step size strategy in image classification tasks.<details>
<summary>Abstract</summary>
This paper introduces a novel approach to enhance the performance of the stochastic gradient descent (SGD) algorithm by incorporating a modified decay step size based on $\frac{1}{\sqrt{t}$. The proposed step size integrates a logarithmic term, leading to the selection of smaller values in the final iterations. Our analysis establishes a convergence rate of $O(\frac{\ln T}{\sqrt{T})$ for smooth non-convex functions without the Polyak-{\L}ojasiewicz condition. To evaluate the effectiveness of our approach, we conducted numerical experiments on image classification tasks using the FashionMNIST, and CIFAR10 datasets, and the results demonstrate significant improvements in accuracy, with enhancements of $0.5\%$ and $1.4\%$ observed, respectively, compared to the traditional $\frac{1}{\sqrt{t}$ step size. The source code can be found at \\\url{https://github.com/Shamaeem/LNSQRTStepSize}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Privacy-Utility-Tradeoff-of-OLS-with-Random-Projections"><a href="#Privacy-Utility-Tradeoff-of-OLS-with-Random-Projections" class="headerlink" title="Privacy-Utility Tradeoff of OLS with Random Projections"></a>Privacy-Utility Tradeoff of OLS with Random Projections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01243">http://arxiv.org/abs/2309.01243</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Lu, Malik Magdon-Ismail, Yu Wei, Vassilis Zikas</li>
<li>for: 这个论文主要是为了研究线性最小二乘（OLS）问题中的分布式隐私（DP）。</li>
<li>methods: 这个论文使用了随机化的ALS算法（Sarlos，2006）来解决OLS问题，并证明了这种算法可以保持隐私。</li>
<li>results: 这个论文得到了一个更好的隐私&#x2F;用途质量比例，而无需进行修改或噪声处理，相比于其他私有OLS算法。此外，论文还提供了一个紧跟的DP分析，并引入了一些新的工具，如DP分谱（DP spectrum）和改进的随机投影DP分谱。<details>
<summary>Abstract</summary>
We study the differential privacy (DP) of a core ML problem, linear ordinary least squares (OLS), a.k.a. $\ell_2$-regression. Our key result is that the approximate LS algorithm (ALS) (Sarlos, 2006), a randomized solution to the OLS problem primarily used to improve performance on large datasets, also preserves privacy. ALS achieves a better privacy/utility tradeoff, without modifications or further noising, when compared to alternative private OLS algorithms which modify and/or noise OLS. We give the first {\em tight} DP-analysis for the ALS algorithm and the standard Gaussian mechanism (Dwork et al., 2014) applied to OLS. Our methodology directly improves the privacy analysis of (Blocki et al., 2012) and (Sheffet, 2019)) and introduces new tools which may be of independent interest: (1) the exact spectrum of $(\epsilon, \delta)$-DP parameters (``DP spectrum") for mechanisms whose output is a $d$-dimensional Gaussian, and (2) an improved DP spectrum for random projection (compared to (Blocki et al., 2012) and (Sheffet, 2019)).   All methods for private OLS (including ours) assume, often implicitly, restrictions on the input database, such as bounds on leverage and residuals. We prove that such restrictions are necessary. Hence, computing the privacy of mechanisms such as ALS must estimate these database parameters, which can be infeasible in big datasets. For more complex ML models, DP bounds may not even be tractable. There is a need for blackbox DP-estimators (Lu et al., 2022) which empirically estimate a data-dependent privacy. We demonstrate the effectiveness of such a DP-estimator by empirically recovering a DP-spectrum that matches our theory for OLS. This validates the DP-estimator in a nontrivial ML application, opening the door to its use in more complex nonlinear ML settings where theory is unavailable.
</details>
<details>
<summary>摘要</summary>
我们研究了数据隐私（DP）的一个核心机器学习（ML）问题，即线性最小二乘（OLS）问题。我们的关键结果是表示 approximate least squares algorithm（ALS），一种对大规模数据进行隐私处理的Randomized解决方案，同时也维护了隐私。相比于其他修改和噪音OLS算法，ALS在隐私/使用价比方面表现更好，而且无需进行修改或进一步噪音。我们提供了第一个严格的DP分析，并将杜夫-尼库库-帕特森（Dwork et al., 2014）所提出的标准 Gaussian mechanism 应用到OLS问题上。我们的方法直接改进了（Blocki et al., 2012）和（Sheffet, 2019）的隐私分析，并引入了新的工具，包括：（1）$(\epsilon, \delta)$-DP  парамет域的精确谱（DP spectrum），以及（2）对于随机投影的改进DP spectrum。所有私人OLS（包括我们的）都假设了输入数据库中的约束，例如 bounds on leverage 和 residuals。我们证明了这些约束是必要的。因此，为了计算私人OLS的隐私，必须估计这些数据库中的 Parameters，这可能是大规模数据中的问题。对于更复杂的机器学习模型，DP bound 可能无法实际 tractable。为了解决这问题，我们提出了黑盒DP估计器（Lu et al., 2022），可以在实际应用中估计资料依赖的隐私。我们透过实验证明了这个DP估计器在一个简单的 ML 应用中的可行性，这开启了这个技术在更复杂的非线性 ML 设置中的使用。
</details></li>
</ul>
<hr>
<h2 id="lfads-torch-A-modular-and-extensible-implementation-of-latent-factor-analysis-via-dynamical-systems"><a href="#lfads-torch-A-modular-and-extensible-implementation-of-latent-factor-analysis-via-dynamical-systems" class="headerlink" title="lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems"></a>lfads-torch: A modular and extensible implementation of latent factor analysis via dynamical systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01230">http://arxiv.org/abs/2309.01230</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew R. Sedler, Chethan Pandarinath</li>
<li>for: 高维神经活动噪声去噪（denoising），用于科学和工程应用。</li>
<li>methods: 使用RNN基于变量序列自动编码器（Variational Sequential Autoencoder，VSAE）实现潜在因子分析（Latent Factor Analysis，LFA）。</li>
<li>results: 达到了高级表现，可以应用于各种 neuroscience 问题。<details>
<summary>Abstract</summary>
Latent factor analysis via dynamical systems (LFADS) is an RNN-based variational sequential autoencoder that achieves state-of-the-art performance in denoising high-dimensional neural activity for downstream applications in science and engineering. Recently introduced variants and extensions continue to demonstrate the applicability of the architecture to a wide variety of problems in neuroscience. Since the development of the original implementation of LFADS, new technologies have emerged that use dynamic computation graphs, minimize boilerplate code, compose model configuration files, and simplify large-scale training. Building on these modern Python libraries, we introduce lfads-torch -- a new open-source implementation of LFADS that unifies existing variants and is designed to be easier to understand, configure, and extend. Documentation, source code, and issue tracking are available at https://github.com/arsedler9/lfads-torch .
</details>
<details>
<summary>摘要</summary>
Latent Factor Analysis via Dynamical Systems (LFADS) 是一种基于 Recurrent Neural Network (RNN) 的可变量序列自动编码器，可以在高维神经活动噪声去噪应用中达到状态之 arts 的性能。最近引入的变体和扩展继续证明了该架构在神经科学中的广泛应用。自 LFADS 的原始实现以来，新技术出现了，包括动态计算图、最小化 boilerplate 代码、组合模型配置文件以及大规模训练的技术。基于这些现代 Python 库，我们介绍了 lfads-torch -- 一个新的开源 LFADS 实现，它将 существующие变体集成到一起，并设计为更易于理解、配置和扩展。文档、源代码和问题跟踪可以在 GitHub 上找到：https://github.com/arsedler9/lfads-torch。
</details></li>
</ul>
<hr>
<h2 id="Saturn-An-Optimized-Data-System-for-Large-Model-Deep-Learning-Workloads"><a href="#Saturn-An-Optimized-Data-System-for-Large-Model-Deep-Learning-Workloads" class="headerlink" title="Saturn: An Optimized Data System for Large Model Deep Learning Workloads"></a>Saturn: An Optimized Data System for Large Model Deep Learning Workloads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01226">http://arxiv.org/abs/2309.01226</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/knagrecha/saturn">https://github.com/knagrecha/saturn</a></li>
<li>paper_authors: Kabir Nagrecha, Arun Kumar</li>
<li>for: 这项研究旨在帮助深度学习（DL）用户更好地选择并使用大型语言模型，如GPT-3和ChatGPT，以及其他模型。</li>
<li>methods: 该研究使用了一种新的信息系统架构，以帮助DL用户更好地处理多个模型并行运行、资源分配和调度问题。它们还提出了一种可重复的模型并行方案，并与runtime estimation相结合。</li>
<li>results: 实验结果显示，使用该新方法可以减少DL模型选择运行时间，比现有实践更高效。特别是，Saturn系统可以在39-49%的时间内完成模型选择运行。<details>
<summary>Abstract</summary>
Large language models such as GPT-3 & ChatGPT have transformed deep learning (DL), powering applications that have captured the public's imagination. These models are rapidly being adopted across domains for analytics on various modalities, often by finetuning pre-trained base models. Such models need multiple GPUs due to both their size and computational load, driving the development of a bevy of "model parallelism" techniques & tools. Navigating such parallelism choices, however, is a new burden for end users of DL such as data scientists, domain scientists, etc. who may lack the necessary systems knowhow. The need for model selection, which leads to many models to train due to hyper-parameter tuning or layer-wise finetuning, compounds the situation with two more burdens: resource apportioning and scheduling. In this work, we tackle these three burdens for DL users in a unified manner by formalizing them as a joint problem that we call SPASE: Select a Parallelism, Allocate resources, and SchedulE. We propose a new information system architecture to tackle the SPASE problem holistically, representing a key step toward enabling wider adoption of large DL models. We devise an extensible template for existing parallelism schemes and combine it with an automated empirical profiler for runtime estimation. We then formulate SPASE as an MILP.   We find that direct use of an MILP-solver is significantly more effective than several baseline heuristics. We optimize the system runtime further with an introspective scheduling approach. We implement all these techniques into a new data system we call Saturn. Experiments with benchmark DL workloads show that Saturn achieves 39-49% lower model selection runtimes than typical current DL practice.
</details>
<details>
<summary>摘要</summary>
大型语言模型如GPT-3和ChatGPT已经改变深度学习（DL），推动了许多应用程序，吸引了大众的关注。这些模型在不同领域进行分析，通常是通过调整预训计划的方式进行训练。这些模型需要多个GPU，因为它们的大小和计算负载，这推动了多个"模型平行化"技术和工具的发展。但是，为DL使用者如数据科学家和领域科学家等选择和调整这些平行化方法，是一个新的负担。因为需要模型选择，这会导致训练多个模型，增加资源分配和平行化的问题。在这个工作中，我们解决了这三个负担，我们统称这些问题为SPASE：选择平行化、分配资源和安排。我们提出了一个新的资讯系统架构，以便对SPASE问题进行整体解决。我们创建了一个可扩展的平行化方案模板，并与一个自动化的实验性能评估工具结合。我们将SPASE转换为一个MILP，并发现直接使用MILP解决方案是较有效的。我们还进一步优化系统执行时间使用一个自我反思的安排方法。我们实现了这些技术，并命名为Saturn。我们对标准DL工作负载进行实验，发现Saturn可以降低39-49%的模型选择执行时间，比 Typical current DL实践更高效。
</details></li>
</ul>
<hr>
<h2 id="Siren’s-Song-in-the-AI-Ocean-A-Survey-on-Hallucination-in-Large-Language-Models"><a href="#Siren’s-Song-in-the-AI-Ocean-A-Survey-on-Hallucination-in-Large-Language-Models" class="headerlink" title="Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models"></a>Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01219">http://arxiv.org/abs/2309.01219</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hillzhang1999/llm-hallucination-survey">https://github.com/hillzhang1999/llm-hallucination-survey</a></li>
<li>paper_authors: Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi</li>
<li>for: 本文旨在探讨大语言模型（LLM）在实际应用中的可靠性问题，即LLM occasional hallucination phenomenon。</li>
<li>methods: 本文 Survey recent efforts on LLM hallucination detection, explanation, and mitigation, with an emphasis on the unique challenges posed by LLMs.</li>
<li>results: 本文 present taxonomies of LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
</details>
<details>
<summary>摘要</summary>
large language models (LLMs) 的表现很出色，但是也存在一定的问题：LLMs  occasionaly generates content that deviates from user input, contradicts previous generated context, or misaligns with established world knowledge. This phenomenon poses a significant challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Implicit-regularization-of-deep-residual-networks-towards-neural-ODEs"><a href="#Implicit-regularization-of-deep-residual-networks-towards-neural-ODEs" class="headerlink" title="Implicit regularization of deep residual networks towards neural ODEs"></a>Implicit regularization of deep residual networks towards neural ODEs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01213">http://arxiv.org/abs/2309.01213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pierre Marion, Yu-Han Wu, Michael E. Sander, Gérard Biau</li>
<li>for: 这篇论文旨在建立深度学习模型中抽象层次结构的固有关系，即深度神经网络（ResNet）与神经泛化方程（Neural ODE）之间的数学基础。</li>
<li>methods: 该论文使用了 gradient flow 来训练非线性网络，并在训练过程中 prove 了深度神经网络的离散化可以被视为神经泛化方程的离散化。</li>
<li>results: 该论文的结果表明，如果初始化网络为神经泛化方程的离散化，那么这种离散化将在训练过程中保持不变，并且这种结果适用于有限的训练时间和训练时间趋于无穷大的情况下，只要网络满足一定的 Polyak-Lojasiewicz 条件。<details>
<summary>Abstract</summary>
Residual neural networks are state-of-the-art deep learning models. Their continuous-depth analog, neural ordinary differential equations (ODEs), are also widely used. Despite their success, the link between the discrete and continuous models still lacks a solid mathematical foundation. In this article, we take a step in this direction by establishing an implicit regularization of deep residual networks towards neural ODEs, for nonlinear networks trained with gradient flow. We prove that if the network is initialized as a discretization of a neural ODE, then such a discretization holds throughout training. Our results are valid for a finite training time, and also as the training time tends to infinity provided that the network satisfies a Polyak-Lojasiewicz condition. Importantly, this condition holds for a family of residual networks where the residuals are two-layer perceptrons with an overparameterization in width that is only linear, and implies the convergence of gradient flow to a global minimum. Numerical experiments illustrate our results.
</details>
<details>
<summary>摘要</summary>
深度学习模型中的剩余神经网络是当前领导的模型之一。它们的连续深度对应的神经泛化方程（ODE）也广泛使用。尽管它们的成功，但是这两种模型之间的数学基础仍然缺乏坚实的理论基础。在这篇文章中，我们向这个方向发展了一个隐式正则化，使得深度剩余神经网络向神经泛化方程进行傅里叶训练。我们证明，如果网络在训练时 initialized 为神经泛化方程的离散化，那么这种离散化会在训练过程中保持不变。我们的结果适用于 finite 训练时间，以及训练时间趋向于无穷大 provided that the network satisfies a Polyak-Lojasiewicz condition。这种条件适用于一家 residual 网络，其中 residual 是两层感知器，并且具有只有线性的宽度过参数。这意味着梯度流会 converges to a global minimum。 numerics 实验证明了我们的结果。
</details></li>
</ul>
<hr>
<h2 id="Spectral-Adversarial-MixUp-for-Few-Shot-Unsupervised-Domain-Adaptation"><a href="#Spectral-Adversarial-MixUp-for-Few-Shot-Unsupervised-Domain-Adaptation" class="headerlink" title="Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation"></a>Spectral Adversarial MixUp for Few-Shot Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01207">http://arxiv.org/abs/2309.01207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/RPIDIAL/SAMix">https://github.com/RPIDIAL/SAMix</a></li>
<li>paper_authors: Jiajin Zhang, Hanqing Chao, Amit Dhurandhar, Pin-Yu Chen, Ali Tajer, Yangyang Xu, Pingkun Yan</li>
<li>For: This paper aims to address the problem of few-shot unsupervised domain adaptation (FSUDA) in clinical applications, where only a limited number of unlabeled target domain samples are available for training.* Methods: The proposed method uses a spectral sensitivity map to characterize the generalization weaknesses of models in the frequency domain, and a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images that effectively suppress the model’s sensitivity, leading to improved model generalizability in the target domain.* Results: The proposed method was demonstrated and evaluated on multiple tasks using several public datasets, showing improved performance compared to existing methods.<details>
<summary>Abstract</summary>
Domain shift is a common problem in clinical applications, where the training images (source domain) and the test images (target domain) are under different distributions. Unsupervised Domain Adaptation (UDA) techniques have been proposed to adapt models trained in the source domain to the target domain. However, those methods require a large number of images from the target domain for model training. In this paper, we propose a novel method for Few-Shot Unsupervised Domain Adaptation (FSUDA), where only a limited number of unlabeled target domain samples are available for training. To accomplish this challenging task, first, a spectral sensitivity map is introduced to characterize the generalization weaknesses of models in the frequency domain. We then developed a Sensitivity-guided Spectral Adversarial MixUp (SAMix) method to generate target-style images to effectively suppresses the model sensitivity, which leads to improved model generalizability in the target domain. We demonstrated the proposed method and rigorously evaluated its performance on multiple tasks using several public datasets.
</details>
<details>
<summary>摘要</summary>
域名转换是在医学应用中的一个常见问题，source domain 和 target domain 的图像分布不同。不supervised Domain Adaptation（UDA）技术已经提出来适应source domain 中训练的模型到 target domain。然而，这些方法需要大量的目标域图像来训练模型。在这篇论文中，我们提出了一种新的方法，即 Few-Shot Unsupervised Domain Adaptation（FSUDA），只需要有限数量的目标域样本来训练模型。为了完成这个复杂的任务，我们首先引入了一个spectral sensitivity map来Characterize模型在频率域的泛化弱点。然后，我们开发了一种Sensitivity-guided Spectral Adversarial MixUp（SAMix）方法，可以生成目标风格的图像，以有效地减少模型的敏感性，从而提高模型在目标域的泛化性。我们在多个任务上证明了我们的方法的性能，并且进行了严格的评估。
</details></li>
</ul>
<hr>
<h2 id="LogGPT-Exploring-ChatGPT-for-Log-Based-Anomaly-Detection"><a href="#LogGPT-Exploring-ChatGPT-for-Log-Based-Anomaly-Detection" class="headerlink" title="LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection"></a>LogGPT: Exploring ChatGPT for Log-Based Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01189">http://arxiv.org/abs/2309.01189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol Fung, Hailong Yang, Depei Qian<br>for: 这个研究旨在应用ChatGPT进行日志型异常检测。methods: 本研究使用ChatGPT的语言解释能力，将知识传递到日志型异常检测中。results: LogGPT在BGL和Spirit datasets上的实验结果显示了良好的性能和可解释性。<details>
<summary>Abstract</summary>
The increasing volume of log data produced by software-intensive systems makes it impractical to analyze them manually. Many deep learning-based methods have been proposed for log-based anomaly detection. These methods face several challenges such as high-dimensional and noisy log data, class imbalance, generalization, and model interpretability. Recently, ChatGPT has shown promising results in various domains. However, there is still a lack of study on the application of ChatGPT for log-based anomaly detection. In this work, we proposed LogGPT, a log-based anomaly detection framework based on ChatGPT. By leveraging the ChatGPT's language interpretation capabilities, LogGPT aims to explore the transferability of knowledge from large-scale corpora to log-based anomaly detection. We conduct experiments to evaluate the performance of LogGPT and compare it with three deep learning-based methods on BGL and Spirit datasets. LogGPT shows promising results and has good interpretability. This study provides preliminary insights into prompt-based models, such as ChatGPT, for the log-based anomaly detection task.
</details>
<details>
<summary>摘要</summary>
“随着软件数据量的增加，手动分析成本过高。许多深度学习方法已经被提出供征兆检测。这些方法面临许多挑战，例如高维度和噪音的日志数据、类别对称性、通过率和模型解释性。最近，ChatGPT已经在不同领域表现出色。然而，还没有对ChatGPT在日志征兆检测中的应用进行了研究。在这个工作中，我们提出了LogGPT，基于ChatGPT的日志征兆检测框架。通过利用ChatGPT的语言解释能力，LogGPT希望能够将大规模数据库中的知识转移到日志征兆检测中。我们对LogGPT进行了实验，并与三种深度学习方法进行比较。LogGPT的成果获得了良好的表现，并且具有良好的解释性。这个研究提供了对于提示型模型，例如ChatGPT，在日志征兆检测任务中的首选点。”Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Cognition-Mode-Aware-Variational-Representation-Learning-Framework-for-Knowledge-Tracing"><a href="#Cognition-Mode-Aware-Variational-Representation-Learning-Framework-for-Knowledge-Tracing" class="headerlink" title="Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing"></a>Cognition-Mode Aware Variational Representation Learning Framework for Knowledge Tracing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01179">http://arxiv.org/abs/2309.01179</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zmy-9/CMVF">https://github.com/zmy-9/CMVF</a></li>
<li>paper_authors: Moyu Zhang, Xinning Zhu, Chunhong Zhang, Feng Pan, Wenchen Qian, Hui Zhao</li>
<li>for: 提高个性化学习中的知识追踪（KT）任务的robustness，解决数据稀缺性问题。</li>
<li>methods: 提出了一种基于变量表示学习框架（CMVF），使用 probabilistic model生成每个学生的分布，考虑学生偏好的uncertainty，并通过变量推理（VI）来估算学生的分布。另外，还引入了认知模式相关的多omial分布作为先验知识，以避免学生偏好的过拟合。</li>
<li>results: 经过广泛的实验 validate CMVF可以有效地帮助现有KT方法学习更加Robust的学生表示。<details>
<summary>Abstract</summary>
The Knowledge Tracing (KT) task plays a crucial role in personalized learning, and its purpose is to predict student responses based on their historical practice behavior sequence. However, the KT task suffers from data sparsity, which makes it challenging to learn robust representations for students with few practice records and increases the risk of model overfitting. Therefore, in this paper, we propose a Cognition-Mode Aware Variational Representation Learning Framework (CMVF) that can be directly applied to existing KT methods. Our framework uses a probabilistic model to generate a distribution for each student, accounting for uncertainty in those with limited practice records, and estimate the student's distribution via variational inference (VI). In addition, we also introduce a cognition-mode aware multinomial distribution as prior knowledge that constrains the posterior student distributions learning, so as to ensure that students with similar cognition modes have similar distributions, avoiding overwhelming personalization for students with few practice records. At last, extensive experimental results confirm that CMVF can effectively aid existing KT methods in learning more robust student representations. Our code is available at https://github.com/zmy-9/CMVF.
</details>
<details>
<summary>摘要</summary>
《知识追踪（KT）任务在个性化学习中扮演着关键性的角色，其目的是预测学生的回答基于他们的历史实践行为序列。然而，KT任务受到数据稀缺的影响，这使得学习学生的Robust表示变得困难，增加模型适应性的风险。因此，在这篇论文中，我们提出了一种基于Variational Representation Learning Framework（CMVF）的认知模式自适应学习框架。我们的框架使用一个概率模型来生成每个学生的分布，考虑到有限实践记录中的不确定性，并通过变量推理（VI）来估算学生的分布。此外，我们还引入了认知模式自适应多omial分布作为先验知识，以避免学生 WITH few practice records 的过度个性化。最后，我们进行了广泛的实验，证明了CMVF可以有效地帮助现有KT方法学习更加Robust的学生表示。我们的代码可以在https://github.com/zmy-9/CMVF上获取。》Note: Simplified Chinese is used here, which is a standardized form of Chinese that is widely used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that version as well.
</details></li>
</ul>
<hr>
<h2 id="FusionAI-Decentralized-Training-and-Deploying-LLMs-with-Massive-Consumer-Level-GPUs"><a href="#FusionAI-Decentralized-Training-and-Deploying-LLMs-with-Massive-Consumer-Level-GPUs" class="headerlink" title="FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs"></a>FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01172">http://arxiv.org/abs/2309.01172</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenheng Tang, Yuxin Wang, Xin He, Longteng Zhang, Xinglin Pan, Qiang Wang, Rongfei Zeng, Kaiyong Zhao, Shaohuai Shi, Bingsheng He, Xiaowen Chu</li>
<li>for: 这个论文旨在探讨如何使用consumer-level GPUs进行大语言模型（LLMs）的训练和部署，并提供隐私保护。</li>
<li>methods: 本论文使用了一个分布式系统，允许consumer-level GPUs在预读、测试和精度调整中参与LLMs的训练。这个系统面临了一些挑战，包括限制的CPU和GPU内存、低网络带宽和对等设备和设备多样性的影响。</li>
<li>results: 研究人员发现使用50个RTX 3080 GPUs可以 дости到和4个H100 GPUs相似的运算能力，这些GPUs更加昂贵。<details>
<summary>Abstract</summary>
The rapid growth of memory and computation requirements of large language models (LLMs) has outpaced the development of hardware, hindering people who lack large-scale high-end GPUs from training or deploying LLMs. However, consumer-level GPUs, which constitute a larger market share, are typically overlooked in LLM due to their weaker computing performance, smaller storage capacity, and lower communication bandwidth. Additionally, users may have privacy concerns when interacting with remote LLMs. In this paper, we envision a decentralized system unlocking the potential vast untapped consumer-level GPUs in pre-training, inference and fine-tuning of LLMs with privacy protection. However, this system faces critical challenges, including limited CPU and GPU memory, low network bandwidth, the variability of peer and device heterogeneity. To address these challenges, our system design incorporates: 1) a broker with backup pool to implement dynamic join and quit of computing providers; 2) task scheduling with hardware performance to improve system efficiency; 3) abstracting ML procedures into directed acyclic graphs (DAGs) to achieve model and task universality; 4) abstracting intermediate represention and execution planes to ensure compatibility of various devices and deep learning (DL) frameworks. Our performance analysis demonstrates that 50 RTX 3080 GPUs can achieve throughputs comparable to those of 4 H100 GPUs, which are significantly more expensive.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）的存储和计算需求在快速增长，超过硬件的开发速度，使得没有大规模高级GPU的人无法训练或部署LLM。然而，消费级GPU，占据市场份额的大多数，通常在LLM中被忽略，因为它们的计算性能较弱，存储容量较小，通信带宽较低。此外，用户可能有隐私问题与远程LLM进行交互。在这篇论文中，我们描述了一个分布式系统，使得大量的消费级GPU可以在预训练、推理和细化LLM中发挥作用，同时保护隐私。然而，这个系统面临着严重的挑战，包括CPU和GPU内存有限，网络带宽低，积分和设备多样性。为解决这些挑战，我们的系统设计包括：1）一个备用池的执行者来实现动态加入和退出计算提供商；2）根据硬件性能进行任务调度，提高系统效率；3）将ML过程抽象为导向无环图（DAG），实现模型和任务通用性；4）抽象中间表示和执行计划，确保各种设备和深度学习（DL）框架的兼容性。我们的性能分析表明，50个RTX 3080 GPU可以实现与4个H100 GPU相当的吞吐量，后者具有更高的成本。
</details></li>
</ul>
<hr>
<h2 id="End-to-End-Learning-on-Multimodal-Knowledge-Graphs"><a href="#End-to-End-Learning-on-Multimodal-Knowledge-Graphs" class="headerlink" title="End-to-End Learning on Multimodal Knowledge Graphs"></a>End-to-End Learning on Multimodal Knowledge Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01169">http://arxiv.org/abs/2309.01169</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://gitlab.com/wxwilcke/mrgcn">https://gitlab.com/wxwilcke/mrgcn</a></li>
<li>paper_authors: W. X. Wilcke, P. Bloem, V. de Boer, R. H. van t Veer</li>
<li>for: 学习hetereogeneous知识图的数据科学家</li>
<li>methods: 提议一种多modalmessage passing网络，不仅从图的结构学习端到端，还从不同类型的多modal节点特征学习embeds</li>
<li>results: 实现并测试模型在node classification和link prediction任务上，并进行了 inverse ablation study，结果表明可以从任何不同的知识图学习，并且包含多modal信息可以显著改善性能，但是具体取决于数据的特点。<details>
<summary>Abstract</summary>
Knowledge graphs enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case we lose potentially relevant information which could have otherwise been exploited by our learning methods. We propose a multimodal message passing network which not only learns end-to-end from the structure of graphs, but also from their possibly divers set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-worlds datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.
</details>
<details>
<summary>摘要</summary>
知识图Enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case, we lose potentially relevant information that could have otherwise been exploited by our learning methods. We propose a multimodal message passing network that not only learns end-to-end from the structure of graphs but also from their possibly diverse set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images, and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-world datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.Here's the translation in Traditional Chinese as well:知识图Enable data scientists to learn end-to-end on heterogeneous knowledge. However, most end-to-end models solely learn from the relational information encoded in graphs' structure: raw values, encoded as literal nodes, are either omitted completely or treated as regular nodes without consideration for their values. In either case, we lose potentially relevant information that could have otherwise been exploited by our learning methods. We propose a multimodal message passing network that not only learns end-to-end from the structure of graphs but also from their possibly diverse set of multimodal node features. Our model uses dedicated (neural) encoders to naturally learn embeddings for node features belonging to five different types of modalities, including numbers, texts, dates, images, and geometries, which are projected into a joint representation space together with their relational information. We implement and demonstrate our model on node classification and link prediction for artificial and real-world datasets, and evaluate the effect that each modality has on the overall performance in an inverse ablation study. Our results indicate that end-to-end multimodal learning from any arbitrary knowledge graph is indeed possible, and that including multimodal information can significantly affect performance, but that much depends on the characteristics of the data.
</details></li>
</ul>
<hr>
<h2 id="Symbolically-integrating-tensor-networks-over-various-random-tensors-–-the-second-version-of-Python-RTNI"><a href="#Symbolically-integrating-tensor-networks-over-various-random-tensors-–-the-second-version-of-Python-RTNI" class="headerlink" title="Symbolically integrating tensor networks over various random tensors – the second version of Python RTNI"></a>Symbolically integrating tensor networks over various random tensors – the second version of Python RTNI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01167">http://arxiv.org/abs/2309.01167</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/motohisafukuda/pyrtni2">https://github.com/motohisafukuda/pyrtni2</a></li>
<li>paper_authors: Motohisa Fukuda</li>
<li>for: 本研究旨在扩展Python中的RTNI，使其可以对响应杂度分布的 unitary 矩阵进行Symbolic интеграル。</li>
<li>methods: 本文使用 Haar-distributed 对偶 orthogonal matrices 和实数和复数正态幂 tensors 的符号 интеграル，并可以导出 tensor networks 格式的 TensorNetwork，以便进一步计算。</li>
<li>results: 本文解释了 PyRTNI2 的数学基础和如何使用它进行tensor network计算，并展示了它可以处理低维度的 tensor network 计算，其中 Weingarten 函数与高维度不同。 tutorial notebooks 可以在 GitHub 上找到：<a target="_blank" rel="noopener" href="https://github.com/MotohisaFukuda/PyRTNI2%E3%80%82">https://github.com/MotohisaFukuda/PyRTNI2。</a><details>
<summary>Abstract</summary>
We are upgrading the Python-version of RTNI, which symbolically integrates tensor networks over the Haar-distributed unitary matrices. Now, PyRTNI2 can treat the Haar-distributed orthogonal matrices and the real and complex normal Gaussian tensors as well. Moreover, it can export tensor networks in the format of TensorNetwork so that one can make further calculations with concrete tensors, even for low dimensions, where the Weingarten functions differ from the ones for high dimensions. The tutorial notebooks are found at GitHub: https://github.com/MotohisaFukuda/PyRTNI2. In this paper, we explain maths behind the program and show what kind of tensor network calculations can be made with it. For the former, we interpret the element-wise moment calculus of the above random matrices and tensors in terms of tensor network diagrams, and argue that the view is natural, relating delta functions in the calculus to edges in tensor network diagrams.
</details>
<details>
<summary>摘要</summary>
我们正在升级Python中的RTNI，这个Symbolic Integration of Tensor Networks over Haar-Distributed Unitary Matrices。现在PyRTNI2可以处理Haar分布的正交矩阵和实数和复数正态分布矩阵，同时还可以导出tensor网络在TensorNetwork格式下，以便进行进一步的计算，即使是低维度的情况下，其中Weingarten函数与高维度不同。教程Notebook可以在GitHub上找到：https://github.com/MotohisaFukuda/PyRTNI2。在这篇论文中，我们解释了PyRTNI2的数学基础和可以通过它进行哪些tensor网络计算。对于前者，我们将元素级极限积分算符和矩阵和tensor的 delta函数相关联，并证明这种视图是自然的。
</details></li>
</ul>
<hr>
<h2 id="Noise-robust-speech-emotion-recognition-with-signal-to-noise-ratio-adapting-speech-enhancement"><a href="#Noise-robust-speech-emotion-recognition-with-signal-to-noise-ratio-adapting-speech-enhancement" class="headerlink" title="Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement"></a>Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01164">http://arxiv.org/abs/2309.01164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu-Wen Chen, Julia Hirschberg, Yu Tsao</li>
<li>for: 提高speech emotion recognition（SER）系统的听力 robustness，使其能够在干扰声中进行正确的情绪识别。</li>
<li>methods: 提出了一种Noise Robust Speech Emotion Recognition（NRSER）系统，利用speech enhancement（SE）技术来减少干扰声，并通过信号听力比例（SNR）级别检测结构和波形重建策略来减少SE对无或少干扰声的影响。</li>
<li>results: 实验结果表明，NRSER系统可以有效提高听力Robustness，包括避免系统对干扰声只 consisting的信号进行情绪识别。此外，提出的SNR级别检测结构可以独立地应用于数据选择等任务。<details>
<summary>Abstract</summary>
Speech emotion recognition (SER) often experiences reduced performance due to background noise. In addition, making a prediction on signals with only background noise could undermine user trust in the system. In this study, we propose a Noise Robust Speech Emotion Recognition system, NRSER. NRSER employs speech enhancement (SE) to effectively reduce the noise in input signals. Then, the signal-to-noise-ratio (SNR)-level detection structure and waveform reconstitution strategy are introduced to reduce the negative impact of SE on speech signals with no or little background noise. Our experimental results show that NRSER can effectively improve the noise robustness of the SER system, including preventing the system from making emotion recognition on signals consisting solely of background noise. Moreover, the proposed SNR-level detection structure can be used individually for tasks such as data selection.
</details>
<details>
<summary>摘要</summary>
听音识别（SER）经常受到背景噪声的影响，这会导致系统的性能下降。此外，基于背景噪声的预测也可能会让用户对系统失去信任。在本研究中，我们提出了一种噪声RobustSpeech Emotion Recognition系统（NRSER）。NRSER使用了speech enhancement（SE）技术来有效减少输入信号中的噪声。然后，我们引入了信号噪声比例检测结构和波形重建策略，以减少SE对于无或少量背景噪声的输入信号的负面影响。我们的实验结果表明，NRSER可以有效提高听音识别系统的噪声抗性，包括避免系统对背景噪声 alone的预测。此外，我们的提案的SNR级别检测结构也可以单独用于任务such as data selection。
</details></li>
</ul>
<hr>
<h2 id="An-Accurate-Graph-Generative-Model-with-Tunable-Features"><a href="#An-Accurate-Graph-Generative-Model-with-Tunable-Features" class="headerlink" title="An Accurate Graph Generative Model with Tunable Features"></a>An Accurate Graph Generative Model with Tunable Features</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01158">http://arxiv.org/abs/2309.01158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takahiro Yokoyama, Yoshiki Sato, Sho Tsugawa, Kohei Watabe</li>
<li>for: 这 paper 是为了提出一种基于实际网络数据的图生成模型，以提高现有图生成模型的精度和灵活性。</li>
<li>methods: 该 paper 使用了一种新的Feedback机制，以及独立地训练图feature和生成图feature的方法，以提高图生成模型的精度。</li>
<li>results: 实际tests 表明，该方法可以更高精度地调整图feature，并且与传统模型相比，生成的图更加准确地满足用户需求。<details>
<summary>Abstract</summary>
A graph is a very common and powerful data structure used for modeling communication and social networks. Models that generate graphs with arbitrary features are important basic technologies in repeated simulations of networks and prediction of topology changes. Although existing generative models for graphs are useful for providing graphs similar to real-world graphs, graph generation models with tunable features have been less explored in the field. Previously, we have proposed GraphTune, a generative model for graphs that continuously tune specific graph features of generated graphs while maintaining most of the features of a given graph dataset. However, the tuning accuracy of graph features in GraphTune has not been sufficient for practical applications. In this paper, we propose a method to improve the accuracy of GraphTune by adding a new mechanism to feed back errors of graph features of generated graphs and by training them alternately and independently. Experiments on a real-world graph dataset showed that the features in the generated graphs are accurately tuned compared with conventional models.
</details>
<details>
<summary>摘要</summary>
graph是一种非常常见和强大的数据结构，用于模型社交和通信网络。生成 graphs 的模型是重要的基础技术，它们可以提供与实际世界 graphs 相似的图像。然而，已有的生成模型 для graphs 的特性是可以调整的，在预测图像结构变化和重复 simulations 中具有重要的意义。在这篇论文中，我们提出了 GraphTune，一种可以不断调整特定图像特性的生成模型。然而，GraphTune 中的调整精度没有达到实际应用中的要求。在这篇论文中，我们提出了一种方法，通过错误反馈和独立地训练来提高 GraphTune 的调整精度。实验表明，对实际图像数据进行训练后，GraphTune 可以准确地调整图像特性。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-machine-learning-based-sampling-motivated-by-lattice-quantum-chromodynamics"><a href="#Advances-in-machine-learning-based-sampling-motivated-by-lattice-quantum-chromodynamics" class="headerlink" title="Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics"></a>Advances in machine-learning-based sampling motivated by lattice quantum chromodynamics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01156">http://arxiv.org/abs/2309.01156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kyle Cranmer, Gurtej Kanwar, Sébastien Racanière, Danilo J. Rezende, Phiala E. Shanahan</li>
<li>for: 这篇论文是关于计算物理学中的生成机器学习（ML）模型，用于实现量子色动力学理论中的结构和物质之间的相互作用计算。</li>
<li>methods: 论文使用的方法包括生成机器学习模型，用于解决物理学中的复杂Symmetry和精确性问题。</li>
<li>results: 论文的结果表明，使用生成机器学习模型可以实现量子色动力学理论中的计算，并且可以在最大的超级计算机上扩展自定义的ML架构。<details>
<summary>Abstract</summary>
Sampling from known probability distributions is a ubiquitous task in computational science, underlying calculations in domains from linguistics to biology and physics. Generative machine-learning (ML) models have emerged as a promising tool in this space, building on the success of this approach in applications such as image, text, and audio generation. Often, however, generative tasks in scientific domains have unique structures and features -- such as complex symmetries and the requirement of exactness guarantees -- that present both challenges and opportunities for ML. This Perspective outlines the advances in ML-based sampling motivated by lattice quantum field theory, in particular for the theory of quantum chromodynamics. Enabling calculations of the structure and interactions of matter from our most fundamental understanding of particle physics, lattice quantum chromodynamics is one of the main consumers of open-science supercomputing worldwide. The design of ML algorithms for this application faces profound challenges, including the necessity of scaling custom ML architectures to the largest supercomputers, but also promises immense benefits, and is spurring a wave of development in ML-based sampling more broadly. In lattice field theory, if this approach can realize its early promise it will be a transformative step towards first-principles physics calculations in particle, nuclear and condensed matter physics that are intractable with traditional approaches.
</details>
<details>
<summary>摘要</summary>
估算known概率分布是计算科学中的一项普遍任务，覆盖从语言学到生物和物理等领域的计算。生成机器学习（ML）模型已经在这些应用中获得成功，如图像、文本和音频生成。然而，在科学领域中的生成任务通常具有特殊的结构和特点，如复杂的对称和精确保证的需求，这些特点都对ML领域呈现挑战和机遇。这篇观点文章总结了基于ML的估算，尤其是基于粒子物理学的粒子场理论。通过实现粒子物理学中的结构和物质之间的互动的计算，粒子场理论是全球开源超级计算机器中最大的用户之一。设计ML算法 для这个应用面临着挑战，包括扩展自定义ML架构到最大的超级计算机器，但也承诺巨大的利益，这已经导致了ML基于估算的发展。在粒子场理论中，如果这种方法实现其早期的承诺，它将是对初始原理物理计算的转变步骤，包括particle、核物理和 Condensed matter物理等领域的计算，这些计算是使用传统方法不可能进行的。
</details></li>
</ul>
<hr>
<h2 id="FedFwd-Federated-Learning-without-Backpropagation"><a href="#FedFwd-Federated-Learning-without-Backpropagation" class="headerlink" title="FedFwd: Federated Learning without Backpropagation"></a>FedFwd: Federated Learning without Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01150">http://arxiv.org/abs/2309.01150</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seonghwan Park, Dahun Shin, Jinseok Chung, Namhoon Lee</li>
<li>for: 提高 Federated Learning 中客户端的训练效率，采用 Hinton (2022) 提出的无后向传播算法 Forward Forward。</li>
<li>methods: 使用层 wise 本地更新方法，不需要存储所有间接活动值。</li>
<li>results: 在 MNIST 和 CIFAR-10 标准数据集上进行了多种实验，与其他基于后向传播的 Federated Learning 方法相比，FedFwd 表现竞争力强。<details>
<summary>Abstract</summary>
In federated learning (FL), clients with limited resources can disrupt the training efficiency. A potential solution to this problem is to leverage a new learning procedure that does not rely on backpropagation (BP). We present a novel approach to FL called FedFwd that employs a recent BP-free method by Hinton (2022), namely the Forward Forward algorithm, in the local training process. FedFwd can reduce a significant amount of computations for updating parameters by performing layer-wise local updates, and therefore, there is no need to store all intermediate activation values during training. We conduct various experiments to evaluate FedFwd on standard datasets including MNIST and CIFAR-10, and show that it works competitively to other BP-dependent FL methods.
</details>
<details>
<summary>摘要</summary>
在联合学习（FL）中，客户端具有有限资源可能会影响训练效率。我们提出了一种新的学习方法，不需要梯度反射（BP）。我们称之为FedFwd，它使用了2022年Hinton提出的一种BP自由方法，即前进前进算法，在本地训练过程中进行层次化更新。因此，无需在训练过程中存储所有中间活动值。我们在标准数据集MNIST和CIFAR-10上进行了多种实验，并示出FedFwd与其他BP依赖的FL方法相比具有竞争力。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Sequence-Clustering"><a href="#Interpretable-Sequence-Clustering" class="headerlink" title="Interpretable Sequence Clustering"></a>Interpretable Sequence Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01140">http://arxiv.org/abs/2309.01140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jd445/Interpretable-Sequence-Clustering-Tree">https://github.com/jd445/Interpretable-Sequence-Clustering-Tree</a></li>
<li>paper_authors: Junjie Dong, Xinyi Yang, Mudi Jiang, Lianyu Hu, Zengyou He</li>
<li>for: 提高序列划分 interpretability， Addressing the challenge of lacking interpretability in sequence clustering.</li>
<li>methods:  combining sequential patterns with a concise and interpretable tree structure, leveraging k-1 patterns to generate k leaf nodes.</li>
<li>results: 提供了一个Intuitive explanation on how each cluster is formed, delivering fast and accurate cluster assignments on 14 real-world data sets.Please note that the results are in Simplified Chinese, as requested.<details>
<summary>Abstract</summary>
Categorical sequence clustering plays a crucial role in various fields, but the lack of interpretability in cluster assignments poses significant challenges. Sequences inherently lack explicit features, and existing sequence clustering algorithms heavily rely on complex representations, making it difficult to explain their results. To address this issue, we propose a method called Interpretable Sequence Clustering Tree (ISCT), which combines sequential patterns with a concise and interpretable tree structure. ISCT leverages k-1 patterns to generate k leaf nodes, corresponding to k clusters, which provides an intuitive explanation on how each cluster is formed. More precisely, ISCT first projects sequences into random subspaces and then utilizes the k-means algorithm to obtain high-quality initial cluster assignments. Subsequently, it constructs a pattern-based decision tree using a boosting-based construction strategy in which sequences are re-projected and re-clustered at each node before mining the top-1 discriminative splitting pattern. Experimental results on 14 real-world data sets demonstrate that our proposed method provides an interpretable tree structure while delivering fast and accurate cluster assignments.
</details>
<details>
<summary>摘要</summary>
总结序列汇编在不同领域中扮演着重要角色，但缺乏汇编结果的解释性对于汇编成果带来重大挑战。序列本身缺乏明确的特征，现有的序列汇编算法严重依赖复杂的表示方式，这使得汇编结果的解释变得困难。为了解决这个问题，我们提出了一种方法 called Interpretable Sequence Clustering Tree (ISCT)，它结合了序列特征和简洁可解的树结构。ISCT利用k-1个模式来生成k个叶点，对应k个汇编结果，这提供了汇编结果的直观解释。更精确地说，ISCT首先将序列 проек到随机的子空间中，然后使用k-means算法获得高品质的初始汇编分配。接着，它使用boosting-based建构策略在每个节点上建构一个基于模式的决策树。在每个节点上，序列会被重新对应并重新汇编，直到获得最佳的一个分类分支模式。实验结果显示，我们的提案方法可以在14个真实世界数据集上提供可解的树结构，同时实现快速和精准的汇编结果。
</details></li>
</ul>
<hr>
<h2 id="Financial-Fraud-Detection-using-Quantum-Graph-Neural-Networks"><a href="#Financial-Fraud-Detection-using-Quantum-Graph-Neural-Networks" class="headerlink" title="Financial Fraud Detection using Quantum Graph Neural Networks"></a>Financial Fraud Detection using Quantum Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01127">http://arxiv.org/abs/2309.01127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouhaila Innan, Abhishek Sawaika, Ashim Dhor, Siddhant Dutta, Sairupa Thota, Husayn Gokal, Nandan Patel, Muhammad Al-Zafar Khan, Ioannis Theodonis, Mohamed Bennai</li>
<li>for: 防止金融欺诈和维护金融机构的名誉</li>
<li>methods: 使用量子图网络（Quantum Graph Neural Networks，QGNNs）和可变量子电路（Variational Quantum Circuits，VQC）进行金融欺诈探测</li>
<li>results: QGNNs achieved an AUC of $0.85$, outperforming classical GNNs<details>
<summary>Abstract</summary>
Financial fraud detection is essential for preventing significant financial losses and maintaining the reputation of financial institutions. However, conventional methods of detecting financial fraud have limited effectiveness, necessitating the need for new approaches to improve detection rates. In this paper, we propose a novel approach for detecting financial fraud using Quantum Graph Neural Networks (QGNNs). QGNNs are a type of neural network that can process graph-structured data and leverage the power of Quantum Computing (QC) to perform computations more efficiently than classical neural networks. Our approach uses Variational Quantum Circuits (VQC) to enhance the performance of the QGNN. In order to evaluate the efficiency of our proposed method, we compared the performance of QGNNs to Classical Graph Neural Networks using a real-world financial fraud detection dataset. The results of our experiments showed that QGNNs achieved an AUC of $0.85$, which outperformed classical GNNs. Our research highlights the potential of QGNNs and suggests that QGNNs are a promising new approach for improving financial fraud detection.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Financial fraud detection" is translated as "金融诈骗检测" (jīnróng kuòhòu jiǎnèsè)* "conventional methods" is translated as "传统方法" (chuánliàng fāngyì)* "Quantum Graph Neural Networks" is translated as "量子图神经网络" (liàngzǐ túshén xīnnéijī)* "Variational Quantum Circuits" is translated as "变量量子电路" (biàngliàng liàngzǐ diànluò)* "AUC" is translated as "AUC" (AUC)* "real-world financial fraud detection dataset" is translated as "真实的金融诈骗检测数据集" (zhēnshí de jīnróng kuòhòu jiǎnèsè numérics)
</details></li>
</ul>
<hr>
<h2 id="AutoML-GPT-Large-Language-Model-for-AutoML"><a href="#AutoML-GPT-Large-Language-Model-for-AutoML" class="headerlink" title="AutoML-GPT: Large Language Model for AutoML"></a>AutoML-GPT: Large Language Model for AutoML</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01125">http://arxiv.org/abs/2309.01125</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Da Tsai, Yu-Che Tsai, Bo-Wei Huang, Chun-Pai Yang, Shou-De Lin</li>
<li>for: 这个论文旨在提供一个名为AutoML-GPT的框架，用于自动化机器学习 pipeline。</li>
<li>methods: 该框架使用了一系列的数据处理技术、特征工程方法和模型选择算法，并通过对话式界面，让用户可以指定自己的需求、约束和评价指标。</li>
<li>results: 通过对多种数据集进行实验，我们表明了AutoML-GPT可以减少机器学习任务的时间和努力，同时可以提供有价值的意见、预测 potential pitfalls 和解决常见的模型训练问题。<details>
<summary>Abstract</summary>
With the emerging trend of GPT models, we have established a framework called AutoML-GPT that integrates a comprehensive set of tools and libraries. This framework grants users access to a wide range of data preprocessing techniques, feature engineering methods, and model selection algorithms. Through a conversational interface, users can specify their requirements, constraints, and evaluation metrics. Throughout the process, AutoML-GPT employs advanced techniques for hyperparameter optimization and model selection, ensuring that the resulting model achieves optimal performance. The system effectively manages the complexity of the machine learning pipeline, guiding users towards the best choices without requiring deep domain knowledge. Through our experimental results on diverse datasets, we have demonstrated that AutoML-GPT significantly reduces the time and effort required for machine learning tasks. Its ability to leverage the vast knowledge encoded in large language models enables it to provide valuable insights, identify potential pitfalls, and suggest effective solutions to common challenges faced during model training.
</details>
<details>
<summary>摘要</summary>
With the emerging trend of GPT models, we have established a framework called AutoML-GPT that integrates a comprehensive set of tools and libraries. This framework grants users access to a wide range of data preprocessing techniques, feature engineering methods, and model selection algorithms. Through a conversational interface, users can specify their requirements, constraints, and evaluation metrics. Throughout the process, AutoML-GPT employs advanced techniques for hyperparameter optimization and model selection, ensuring that the resulting model achieves optimal performance. The system effectively manages the complexity of the machine learning pipeline, guiding users towards the best choices without requiring deep domain knowledge. Through our experimental results on diverse datasets, we have demonstrated that AutoML-GPT significantly reduces the time and effort required for machine learning tasks. Its ability to leverage the vast knowledge encoded in large language models enables it to provide valuable insights, identify potential pitfalls, and suggest effective solutions to common challenges faced during model training.Here's the translation in Traditional Chinese:随着GPT模型的兴起，我们已经建立了一个名为AutoML-GPT的框架，这个框架集成了丰富的工具和库。这个框架给用户提供了丰富的数据预处理技术、特征工程方法和模型选择算法。通过对话式界面，用户可以指定他们的需求、限制和评估度量。在过程中，AutoML-GPT使用进步的超参数优化和模型选择技术，以确保所得到的模型实现最佳性能。系统有效地管理机器学习管线的复杂性，帮助用户选择最佳选择而无需深入领域知识。我们在多个 dataset 上进行了实验，展示了AutoML-GPT可以很大幅降低机器学习任务的时间和努力。它的能力将大量的语言模型中的知识转移到机器学习任务中，帮助提供有价的见解、识别潜在的问题和建议常见的挑战解决方案。
</details></li>
</ul>
<hr>
<h2 id="AI-driven-B-cell-Immunotherapy-Design"><a href="#AI-driven-B-cell-Immunotherapy-Design" class="headerlink" title="AI driven B-cell Immunotherapy Design"></a>AI driven B-cell Immunotherapy Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01122">http://arxiv.org/abs/2309.01122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bruna Moreira da Silva, David B. Ascher, Nicholas Geard, Douglas E. V. Pires</li>
<li>for: 本研究旨在概述人工智能和机器学习技术在B细胞免疫疗法设计方面的进步，包括线性和结构融合识别、抗体设计等方法。</li>
<li>methods: 本文使用的数据源、评价指标和方法有哪些，以及它们的意义和局限性，并讨论了主要的挑战。</li>
<li>results: 本研究总结了机器学习基于的工具和框架在B细胞免疫疗法设计方面的进步，并评估了这些工具的 significanc和局限性。<details>
<summary>Abstract</summary>
Antibodies, a prominent class of approved biologics, play a crucial role in detecting foreign antigens. The effectiveness of antigen neutralisation and elimination hinges upon the strength, sensitivity, and specificity of the paratope-epitope interaction, which demands resource-intensive experimental techniques for characterisation. In recent years, artificial intelligence and machine learning methods have made significant strides, revolutionising the prediction of protein structures and their complexes. The past decade has also witnessed the evolution of computational approaches aiming to support immunotherapy design. This review focuses on the progress of machine learning-based tools and their frameworks in the domain of B-cell immunotherapy design, encompassing linear and conformational epitope prediction, paratope prediction, and antibody design. We mapped the most commonly used data sources, evaluation metrics, and method availability and thoroughly assessed their significance and limitations, discussing the main challenges ahead.
</details>
<details>
<summary>摘要</summary>
抗体，一种常见的批准生物学药物，在检测外源抗原方面发挥关键作用。抗原中和消除的效iveness取决于抗体和抗原之间的复合体-蛋白质结合的强度、敏感度和特异性，这需要资源充足的实验技术来Characterization。在过去的十年中，人工智能和机器学习方法得到了 significanthuge strides，对蛋白质结构和复合物的预测做出了重大贡献。本文将评论机器学习基本工具和框架在B细胞免疫治疗设计领域的进步，包括线性和 conformational epitope预测、蛋白质预测、抗体设计。我们对最常用的数据源、评价指标和方法可用性进行了地图，并且详细评估了它们的重要性和局限性，讨论了主要的挑战。
</details></li>
</ul>
<hr>
<h2 id="Double-Clipping-Less-Biased-Variance-Reduction-in-Off-Policy-Evaluation"><a href="#Double-Clipping-Less-Biased-Variance-Reduction-in-Off-Policy-Evaluation" class="headerlink" title="Double Clipping: Less-Biased Variance Reduction in Off-Policy Evaluation"></a>Double Clipping: Less-Biased Variance Reduction in Off-Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01120">http://arxiv.org/abs/2309.01120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Malte Lichtenberg, Alexander Buchholz, Giuseppe Di Benedetto, Matteo Ruffini, Ben London</li>
<li>for: 提高counterfactual off-policy estimator的精度和稳定性</li>
<li>methods: 使用clipping variance-reduction技术，并提出double clipping扩展来减少偏误</li>
<li>results: double clipping可以减少总偏误，保持原始 estimator 的 variance reduction 性能<details>
<summary>Abstract</summary>
"Clipping" (a.k.a. importance weight truncation) is a widely used variance-reduction technique for counterfactual off-policy estimators. Like other variance-reduction techniques, clipping reduces variance at the cost of increased bias. However, unlike other techniques, the bias introduced by clipping is always a downward bias (assuming non-negative rewards), yielding a lower bound on the true expected reward. In this work we propose a simple extension, called $\textit{double clipping}$, which aims to compensate this downward bias and thus reduce the overall bias, while maintaining the variance reduction properties of the original estimator.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Carbon-Emission-Prediction-and-Clean-Industry-Transformation-Based-on-Machine-Learning-A-Case-Study-of-Sichuan-Province"><a href="#Carbon-Emission-Prediction-and-Clean-Industry-Transformation-Based-on-Machine-Learning-A-Case-Study-of-Sichuan-Province" class="headerlink" title="Carbon Emission Prediction and Clean Industry Transformation Based on Machine Learning: A Case Study of Sichuan Province"></a>Carbon Emission Prediction and Clean Industry Transformation Based on Machine Learning: A Case Study of Sichuan Province</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01115">http://arxiv.org/abs/2309.01115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuanming Zhang, Xiaoxue Wang, Yonghang Chen</li>
<li>for: 本研究使用矩阵正常化处理2000-2019年四川省46个关键产业的能源消耗数据，并使用DBSCAN封顶 clustering方法对产业分类。</li>
<li>methods: 本研究使用了DBSCAN封顶 clustering方法和罚款回归模型，以便控制过拟合、处理高维数据和选择特征。</li>
<li>results: 研究发现，第二个群是煤炭生产的需求导致的高排放，而汽油和焦炭生产的群也有显著的排放。根据这些结果，提出了清洁煤炭技术、交通管理、钢铁生产中的煤电取代和产业标准化等减排策略。<details>
<summary>Abstract</summary>
This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to better inform decision-making using algorithms like DBSCAN and penalized regression models.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这项研究处理了四川省46个关键产业的能源消耗数据从2000年到2019年，使用矩阵正常化。使用DBSCAN划分 clustering，并确定了16个特征类来对产业进行 объектив分组。使用了惩罚回归模型，具有控制过拟合、处理高维数据和选择特征的优点。研究结果显示，第二个带有煤炭的群组 emission 最高，这是因为生产需要。汽油销售和焦炭销售群组的排放也很 significanthigh。根据这些发现，研究提出了减排建议，如使用清洁煤炭技术、交通管理、钢铁生产中的煤电取代、产业标准化。研究引入了无监督学习，可以 объектив地选择因素，以探索新的减排途径。总的来说，研究确定了产业集群、评估排放驱动因素和提出科学减排策略，以更好地决策使用DBSCAN和惩罚回归模型。
</details></li>
</ul>
<hr>
<h2 id="Acoustic-to-articulatory-inversion-for-dysarthric-speech-Are-pre-trained-self-supervised-representations-favorable"><a href="#Acoustic-to-articulatory-inversion-for-dysarthric-speech-Are-pre-trained-self-supervised-representations-favorable" class="headerlink" title="Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?"></a>Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01108">http://arxiv.org/abs/2309.01108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarthak Kumar Maharana, Krishna Kamal Adidam, Shoumik Nandi, Ajitesh Srivastava</li>
<li>for: 这个研究的目的是为了实现对具有问题话语的人的语音转换（AAI）。</li>
<li>methods: 这个研究使用了自动监督学习（SSL）模型所生成的表示，以便对具有问题话语的人进行AAI。</li>
<li>results: 研究发现，使用DeCoAR并在精致调整的情况下，可以实现Relative improvement of Pearson Correlation Coefficient（CC）大约1.81%和4.56%，对于健康控制和患者而言。<details>
<summary>Abstract</summary>
$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, respectively, over MFCCs. In the unseen case, we observe similar average trends for different SSL features. Overall, SSL networks like wav2vec, APC, and DeCoAR, which are trained with feature reconstruction or future timestep prediction tasks, perform well in predicting dysarthric articulatory trajectories.
</details>
<details>
<summary>摘要</summary>
$ $Acoustic-to-articulatory inversion (AAI) 涉及将语音空间映射到语音生成空间。通用的信号处理特征如MFCCs，已经广泛用于AAI任务中。对于具有瘫疡语言的谈话者而言，AAI 是一个挑战性的任务，因为他们的语音调音不精确和模糊。在这个工作中，我们使用预训学得的SSL模型来进行AAI。我们展示了不同预训特征对这个具有挑战性的AAI任务的影响，以及在低资源条件下使用这些特征进行训练。此外，我们还将x-vector与提取的SSL特征进行训练，以训练一个BLSTM网络。在seen情况下，我们实验了三种AAI训练方案（对象特定、汇集和精致调整）。结果显示，在精致调整方案下，DeCoAR 的相对改善比（Pearson Correlation Coefficient，CC）为${\sim}1.81\%}$和${\sim}4.56\%}$，对于健康控制和瘫疡患者而言。在unseen情况下，我们观察到了相似的平均趋势。总的来说，SSL网络如wav2vec、APC和DeCoAR，它们通过对特征重建或未来时间步骤预测任务进行训练，在预测瘫疡语言生成的调音轨迹方面表现良好。
</details></li>
</ul>
<hr>
<h2 id="Solving-Non-Rectangular-Reward-Robust-MDPs-via-Frequency-Regularization"><a href="#Solving-Non-Rectangular-Reward-Robust-MDPs-via-Frequency-Regularization" class="headerlink" title="Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization"></a>Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01107">http://arxiv.org/abs/2309.01107</a></li>
<li>repo_url: None</li>
<li>paper_authors: Uri Gadot, Esther Derman, Navdeep Kumar, Maxence Mohamed Elfatihi, Kfir Levy, Shie Mannor</li>
<li>for: 本研究targets maximal return under the most adversarial model from a given uncertainty set in robust Markov decision processes (RMDPs).</li>
<li>methods: 本研究提出了一种基于policy visitation frequency regularization的policy-gradient方法，并证明其 converges。</li>
<li>results: numerical experiments show that the learned policy is more robust and less conservative than traditional rectangular uncertainty.<details>
<summary>Abstract</summary>
In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method, and prove its convergence. Numerical experiments illustrate the learned policy's robustness and its less conservative behavior when compared to rectangular uncertainty.
</details>
<details>
<summary>摘要</summary>
在Robust Markov决策过程（RMDP）中，假设奖励和转移动力在给定的不确定集中。通过目标最大返回在最敌对模型下，RMDP 解决了环境不准确性对性能的敏感性。然而，以保持计算 tractability，传统上uncertainty集是独立地结构化于每个状态。这种叫做矩形性condition solely 是基于计算 Concerns 的，而且缺乏实践的驱动力，可能会导致过度保守的行为。在这项工作中，我们研究了连接奖励 RMDP，其中转移函数固定，但奖励函数在一个 $\alpha $-距离内的 Nominal 奖励函数。我们从连接这种非矩形奖励 RMDP 和应用策略访问频率规范化。我们引入了一种策略梯度法，并证明其 converge。 numerics 实验表明学习的策略具有强大的Robustness 和相比矩形不确定情况下更加保守的行为。
</details></li>
</ul>
<hr>
<h2 id="Turn-Fake-into-Real-Adversarial-Head-Turn-Attacks-Against-Deepfake-Detection"><a href="#Turn-Fake-into-Real-Adversarial-Head-Turn-Attacks-Against-Deepfake-Detection" class="headerlink" title="Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection"></a>Turn Fake into Real: Adversarial Head Turn Attacks Against Deepfake Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01104">http://arxiv.org/abs/2309.01104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weijie Wang, Zhengyu Zhao, Nicu Sebe, Bruno Lepri</li>
<li>For: 评估深伪检测器的Robustness，提出了首个基于三维面视 synthesis的对深伪检测器的3D对抗攻击（AdvHeat）。* Methods: 基于单个伪图片的面视生成，实现了对深伪检测器的3D对抗攻击。* Results: 通过实验证明了多种检测器对AdvHeat的攻击性能高，并且比传统攻击更好地适应实际场景和防御措施。生成的对抗图像也具有自然的 Looks。<details>
<summary>Abstract</summary>
Malicious use of deepfakes leads to serious public concerns and reduces people's trust in digital media. Although effective deepfake detectors have been proposed, they are substantially vulnerable to adversarial attacks. To evaluate the detector's robustness, recent studies have explored various attacks. However, all existing attacks are limited to 2D image perturbations, which are hard to translate into real-world facial changes. In this paper, we propose adversarial head turn (AdvHeat), the first attempt at 3D adversarial face views against deepfake detectors, based on face view synthesis from a single-view fake image. Extensive experiments validate the vulnerability of various detectors to AdvHeat in realistic, black-box scenarios. For example, AdvHeat based on a simple random search yields a high attack success rate of 96.8% with 360 searching steps. When additional query access is allowed, we can further reduce the step budget to 50. Additional analyses demonstrate that AdvHeat is better than conventional attacks on both the cross-detector transferability and robustness to defenses. The adversarial images generated by AdvHeat are also shown to have natural looks. Our code, including that for generating a multi-view dataset consisting of 360 synthetic views for each of 1000 IDs from FaceForensics++, is available at https://github.com/twowwj/AdvHeaT.
</details>
<details>
<summary>摘要</summary>
恶势所用的深度假像引起了公众的严重关注，导致人们对数字媒体的信任减退。虽然有效的深度假像检测器已经被提出，但它们却容易受到反对攻击。为评估检测器的可靠性，latest studies have explored various attacks.然而，所有的攻击都是基于2D图像杂化，这些杂化很难在实际情况中翻译成真实的脸部变化。在这篇论文中，我们提出了3D反对攻击（AdvHeat），这是基于单个假图像 Synthesize 的脸部视图生成。我们进行了广泛的实验，证明了许多检测器对 AdvHeat 的攻击成功率非常高，例如，基于随机搜索的 AdvHeat 可以在360个搜索步骤中达到96.8%的攻击成功率。当允许更多的查询访问时，我们可以进一步减少步骤数量到50。此外，我们还进行了进一步的分析，证明 AdvHeat 比传统攻击更好地在跨检测器的转移性和防御机制上。生成的反对图像也展示出自然的外观。我们的代码，包括生成360个视图的多视图Dataset，可以在https://github.com/twowwj/AdvHeaT中下载。
</details></li>
</ul>
<hr>
<h2 id="M2HGCL-Multi-Scale-Meta-Path-Integrated-Heterogeneous-Graph-Contrastive-Learning"><a href="#M2HGCL-Multi-Scale-Meta-Path-Integrated-Heterogeneous-Graph-Contrastive-Learning" class="headerlink" title="M2HGCL: Multi-Scale Meta-Path Integrated Heterogeneous Graph Contrastive Learning"></a>M2HGCL: Multi-Scale Meta-Path Integrated Heterogeneous Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01101">http://arxiv.org/abs/2309.01101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanyuan Guo, Yu Xia, Rui Wang, Rongcheng Duan, Lu Li, Jiangmeng Li</li>
<li>for: 研究者尝试将对比学习方法应用于非同质Graph，以提高对非同质Graph的理解和预测能力。</li>
<li>methods: 提出了一种基于多级MetaPath的hetrogeneous graph contrastive learning（M2HGCL）模型，不需要将非同质Graph转换为同质Graph，从而保留有价值信息。</li>
<li>results: 通过EXTENSIVE实验表明，M2HGCL在三个实际 datasets上表现出色，与当前状态方法相比，具有更高的性能。<details>
<summary>Abstract</summary>
Inspired by the successful application of contrastive learning on graphs, researchers attempt to impose graph contrastive learning approaches on heterogeneous information networks. Orthogonal to homogeneous graphs, the types of nodes and edges in heterogeneous graphs are diverse so that specialized graph contrastive learning methods are required. Most existing methods for heterogeneous graph contrastive learning are implemented by transforming heterogeneous graphs into homogeneous graphs, which may lead to ramifications that the valuable information carried by non-target nodes is undermined thereby exacerbating the performance of contrastive learning models. Additionally, current heterogeneous graph contrastive learning methods are mainly based on initial meta-paths given by the dataset, yet according to our deep-going exploration, we derive empirical conclusions: only initial meta-paths cannot contain sufficiently discriminative information; and various types of meta-paths can effectively promote the performance of heterogeneous graph contrastive learning methods. To this end, we propose a new multi-scale meta-path integrated heterogeneous graph contrastive learning (M2HGCL) model, which discards the conventional heterogeneity-homogeneity transformation and performs the graph contrastive learning in a joint manner. Specifically, we expand the meta-paths and jointly aggregate the direct neighbor information, the initial meta-path neighbor information and the expanded meta-path neighbor information to sufficiently capture discriminative information. A specific positive sampling strategy is further imposed to remedy the intrinsic deficiency of contrastive learning, i.e., the hard negative sample sampling issue. Through extensive experiments on three real-world datasets, we demonstrate that M2HGCL outperforms the current state-of-the-art baseline models.
</details>
<details>
<summary>摘要</summary>
研究人员受到同性图像学上的成功应用启发，尝试将同性图像学方法应用于不同类型节点和边的异构信息网络。与同性图像学不同的是，异构图像中节点和边的类型多样化，因此需要特化的同性图像学方法。现有的异构图像学方法多数是将异构图像转换为同性图像，这可能会导致非目标节点上的有价信息被忽略，从而恶化对异构图像学模型的性能。此外，现有的异构图像学方法主要基于数据集提供的初始meta路径，但据我们深入探索，我们得出了实际的结论：初始meta路径不含充分的洗礼信息；同时，不同类型的meta路径可以有效地提高异构图像学模型的性能。为此，我们提出了一种新的多级meta路径集成的异构图像学模型（M2HGCL），它不需要将异构图像转换为同性图像，而是在一起进行图像学学习。具体来说，我们将meta路径扩展，并将直接邻居信息、初始meta路径邻居信息和扩展meta路径邻居信息集成以便充分捕捉洗礼信息。此外，我们还采用了一种特殊的正样本选择策略，以解决对异构图像学的内在缺陷，即困难的负样本选择问题。经过了三个实际 datasets 的广泛实验，我们证明了 M2HGCL 在当前状态的基eline模型中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Stabilize-to-Act-Learning-to-Coordinate-for-Bimanual-Manipulation"><a href="#Stabilize-to-Act-Learning-to-Coordinate-for-Bimanual-Manipulation" class="headerlink" title="Stabilize to Act: Learning to Coordinate for Bimanual Manipulation"></a>Stabilize to Act: Learning to Coordinate for Bimanual Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01087">http://arxiv.org/abs/2309.01087</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jennifer Grannen, Yilin Wu, Brandon Vu, Dorsa Sadigh</li>
<li>for: 这篇论文的目的是提出一种新的角色分配框架，以便在双手控制系统中实现高级别的 manipulate 能力。</li>
<li>methods: 该框架基于人类的做法，即使用学习的稳定化分类器， alternate between 维护环境不变和执行任务。</li>
<li>results: 在四种不同复杂度的双手任务上，使用BUDS实现了76.9%的任务成功率，并能够在不同类型的物体上进行扩展。相比之下，不结构化的基准点实验结果只有52.7%的成功率。<details>
<summary>Abstract</summary>
Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying complexities on real-world robots, such as zipping jackets and cutting vegetables. Given only 20 demonstrations, BUDS achieves 76.9% task success across our task suite, and generalizes to out-of-distribution objects within a class with a 52.7% success rate. BUDS is 56.0% more successful than an unstructured baseline that instead learns a BC stabilizing policy due to the precision required of these complex tasks. Supplementary material and videos can be found at https://sites.google.com/view/stabilizetoact .
</details>
<details>
<summary>摘要</summary>
针对实际世界中灵活 manipulate 的关键是控制两手的协调。然而，构建 dual arm 自动控制策略带来了内在的困难。一个这种困难是双手动作空间的高维度，这将加载到 both model-based 和 data-driven 方法上。我们从人类中突破这个挑战，提出了一种新的角色分配框架：一个稳定化手持物品，以简化环境，而另一个执行手执行任务。我们实现了这个框架，并将其称为 BimanUal Dexterity from Stabilization (BUDS)。BUDS 使用一个学习得到的稳定化位置更新器， alternate  между维持环境不变和通过示例学习得到的执行策略来完成任务。我们在实际 робоット上进行了四种不同复杂度的双手任务的评估，包括 zip 上衣和切 vegetables。只需要20个示例，BUDS 在我们的任务集中达到了76.9%的任务成功率，并能够在不同类型的物品上进行扩展。BUDS 比不结构化基eline 更successful，因为它需要这些复杂任务的精度。补充材料和视频可以在 <https://sites.google.com/view/stabilizetoact> 找到。
</details></li>
</ul>
<hr>
<h2 id="Tropical-Geometric-Tools-for-Machine-Learning-the-TML-package"><a href="#Tropical-Geometric-Tools-for-Machine-Learning-the-TML-package" class="headerlink" title="Tropical Geometric Tools for Machine Learning: the TML package"></a>Tropical Geometric Tools for Machine Learning: the TML package</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01082">http://arxiv.org/abs/2309.01082</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/barnhilldave/tml">https://github.com/barnhilldave/tml</a></li>
<li>paper_authors: David Barnhill, Ruriko Yoshida, Georges Aliatimis, Keiji Miura</li>
<li>for: 这篇论文主要是为了应用推理学中的推理方法和数据分析技术。</li>
<li>methods: 这篇论文使用的方法包括使用极值加法的热征链 Monte Carlo 抽象法，以及一些基于极值加法的主成分分析、极值逻辑回归和极值核密度估计等方法。</li>
<li>results: 这篇论文的结果包括一个完整的R包，包含了基本的极值加法计算和视觉化、以及基于极值加法的一些统计学模型，如极值主成分分析、极值逻辑回归和极值核密度估计等。<details>
<summary>Abstract</summary>
In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
</details>
<details>
<summary>摘要</summary>
过去一个十年，极地几何的发展提供了许多直接适用于统计学学习问题的用途。TML包是R包中首个包含极地几何相关基本计算、极地几何集的视觉化以及使用极地度量下的最大加法预测树的所有工具和方法的完整集。TML包主要使用极地哈希迪恩-麦克风 Monte Carlo抽取器与极地度量进行统计推断。此外，TML包还包括极地几何HAR抽取器的多种超VISS和无监督学习模型，包括极地主成分分析、极地逻辑回归和极地核密度估计。
</details></li>
</ul>
<hr>
<h2 id="Robust-Adversarial-Defense-by-Tensor-Factorization"><a href="#Robust-Adversarial-Defense-by-Tensor-Factorization" class="headerlink" title="Robust Adversarial Defense by Tensor Factorization"></a>Robust Adversarial Defense by Tensor Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01077">http://arxiv.org/abs/2309.01077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manish Bhattarai, Mehmet Cagri Kaymak, Ryan Barron, Ben Nebgen, Kim Rasmussen, Boian Alexandrov</li>
<li>for: 防止机器学习模型受到敌意攻击</li>
<li>methods: 使用输入数据约化和神经网络参数分解，并将其组合使用</li>
<li>results: 提供了一种可以增强对抗攻击的有效防御策略，并且比其他基于约化的防御策略更高效<details>
<summary>Abstract</summary>
As machine learning techniques become increasingly prevalent in data analysis, the threat of adversarial attacks has surged, necessitating robust defense mechanisms. Among these defenses, methods exploiting low-rank approximations for input data preprocessing and neural network (NN) parameter factorization have shown potential. Our work advances this field further by integrating the tensorization of input data with low-rank decomposition and tensorization of NN parameters to enhance adversarial defense. The proposed approach demonstrates significant defense capabilities, maintaining robust accuracy even when subjected to the strongest known auto-attacks. Evaluations against leading-edge robust performance benchmarks reveal that our results not only hold their ground against the best defensive methods available but also exceed all current defense strategies that rely on tensor factorizations. This study underscores the potential of integrating tensorization and low-rank decomposition as a robust defense against adversarial attacks in machine learning.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术在数据分析中越来越普遍，对抗攻击的威胁也在增加。为了应对这些威胁，使用输入数据减少维度和神经网络参数 факторизация的方法已经显示出了潜在的防御能力。我们的工作将这些方法进一步推进，通过将输入数据矩化和神经网络参数矩化相结合，以提高对抗攻击的防御能力。我们的方法在面对最强知识的自动攻击时仍然保持了高度的鲁棒精度，并在与现有的tensor化防御策略进行比较时表现出了超越。这一研究证明了将矩化和低级分解integrated为机器学习中的鲁棒防御策略的潜在价值。
</details></li>
</ul>
<hr>
<h2 id="Federated-Few-shot-Learning-for-Cough-Classification-with-Edge-Devices"><a href="#Federated-Few-shot-Learning-for-Cough-Classification-with-Edge-Devices" class="headerlink" title="Federated Few-shot Learning for Cough Classification with Edge Devices"></a>Federated Few-shot Learning for Cough Classification with Edge Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01076">http://arxiv.org/abs/2309.01076</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngan Dao Hoang, Dat Tran-Anh, Manh Luong, Cong Tran, Cuong Pham</li>
<li>for: 本研究旨在开发一种能够有效地分类喘音，即使没有庞大的喘音数据，并且保持隐私性的框架。</li>
<li>methods: 我们采用了少量学习和联合学习来解决这个问题，并设计了一个新的框架 termed F2LCough。</li>
<li>results: 我们的结果显示，使用这种新的方法可以在 COVID-19 热面&amp;喘音数据集上达到平均 F1-Score 的 86%，较其他方法更高。这显示了少量学习与联合学习的可行性，并且能够在数据缺乏情况下进行喘音分类。这种新方法可以用于建立喘音分类模型，并且保持隐私性。<details>
<summary>Abstract</summary>
Automatically classifying cough sounds is one of the most critical tasks for the diagnosis and treatment of respiratory diseases. However, collecting a huge amount of labeled cough dataset is challenging mainly due to high laborious expenses, data scarcity, and privacy concerns. In this work, our aim is to develop a framework that can effectively perform cough classification even in situations when enormous cough data is not available, while also addressing privacy concerns. Specifically, we formulate a new problem to tackle these challenges and adopt few-shot learning and federated learning to design a novel framework, termed F2LCough, for solving the newly formulated problem. We illustrate the superiority of our method compared with other approaches on COVID-19 Thermal Face & Cough dataset, in which F2LCough achieves an average F1-Score of 86%. Our results show the feasibility of few-shot learning combined with federated learning to build a classification model of cough sounds. This new methodology is able to classify cough sounds in data-scarce situations and maintain privacy properties. The outcomes of this work can be a fundamental framework for building support systems for the detection and diagnosis of cough-related diseases.
</details>
<details>
<summary>摘要</summary>
自动分类咳声是抑制呼吸疾病诊断和治疗中最关键的任务。然而，收集庞大量标注咳声数据是困难的，主要是因为高度劳动成本、数据稀缺和隐私问题。在这种情况下，我们的目标是开发一个框架，能够有效地进行咳声分类，同时解决隐私问题。我们提出了一个新的问题，并采用了几招学习和联合学习来设计一个新的框架，称为F2LCough。我们在COVID-19 Thermal Face & Cough数据集上运行了F2LCough，并获得了86%的平均F1分数。这些结果表明，将几招学习与联合学习结合使用可以建立一个分类咳声的模型，并在数据稀缺情况下保持隐私性。这种新的方法可以帮助建立抑制呼吸疾病的支持系统。
</details></li>
</ul>
<hr>
<h2 id="Towards-Efficient-Modeling-and-Inference-in-Multi-Dimensional-Gaussian-Process-State-Space-Models"><a href="#Towards-Efficient-Modeling-and-Inference-in-Multi-Dimensional-Gaussian-Process-State-Space-Models" class="headerlink" title="Towards Efficient Modeling and Inference in Multi-Dimensional Gaussian Process State-Space Models"></a>Towards Efficient Modeling and Inference in Multi-Dimensional Gaussian Process State-Space Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01074">http://arxiv.org/abs/2309.01074</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhidilin/gpssmproj">https://github.com/zhidilin/gpssmproj</a></li>
<li>paper_authors: Zhidi Lin, Juan Maroñas, Ying Li, Feng Yin, Sergios Theodoridis</li>
<li>for: 模型复杂非线性动态系统</li>
<li>methods:  integrate efficient transformed Gaussian process (ETGP) into Gaussian process state-space model (GPSSM)，并发展相应的变量推断算法</li>
<li>results: 实验结果表明，提议方法可以减少参数量和计算复杂度，并达到与现有方法相当的推断性能。代码可以在 \url{<a target="_blank" rel="noopener" href="https://github.com/zhidilin/gpssmProj%7D">https://github.com/zhidilin/gpssmProj}</a> 上获取。<details>
<summary>Abstract</summary>
The Gaussian process state-space model (GPSSM) has attracted extensive attention for modeling complex nonlinear dynamical systems. However, the existing GPSSM employs separate Gaussian processes (GPs) for each latent state dimension, leading to escalating computational complexity and parameter proliferation, thus posing challenges for modeling dynamical systems with high-dimensional latent states. To surmount this obstacle, we propose to integrate the efficient transformed Gaussian process (ETGP) into the GPSSM, which involves pushing a shared GP through multiple normalizing flows to efficiently model the transition function in high-dimensional latent state space. Additionally, we develop a corresponding variational inference algorithm that surpasses existing methods in terms of parameter count and computational complexity. Experimental results on diverse synthetic and real-world datasets corroborate the efficiency of the proposed method, while also demonstrating its ability to achieve similar inference performance compared to existing methods. Code is available at \url{https://github.com/zhidilin/gpssmProj}.
</details>
<details>
<summary>摘要</summary>
Gaussian 进程状态空间模型（GPSSM）已经吸引了广泛的注意力，用于模型复杂非线性动力系统。然而，现有的 GPSSM 使用每个隐藏状态维度上的分开 Gaussian 进程（GP），导致计算复杂性和参数增加，从而对高维隐藏状态系统的模型 pose 难题。为了缓解这个障碍，我们提议将高效转换 Gaussian 进程（ETGP）integrated 到 GPSSM 中，该方法涉及将共享 GP  pushed  through multiple normalizing flows 以高效地模型高维隐藏状态空间中的过渡函数。此外，我们还开发了相应的可视化推理算法，其比现有方法具有更少的参数数和更低的计算复杂性。实验结果在多个 Synthetic 和实际数据集上证明了提议方法的效率，同时也表明了它可以与现有方法相比达到类似的推理性能。代码可以在 \url{https://github.com/zhidilin/gpssmProj} 上获取。
</details></li>
</ul>
<hr>
<h2 id="Separable-Hamiltonian-Neural-Networks"><a href="#Separable-Hamiltonian-Neural-Networks" class="headerlink" title="Separable Hamiltonian Neural Networks"></a>Separable Hamiltonian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01069">http://arxiv.org/abs/2309.01069</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zykhoo/separablenns">https://github.com/zykhoo/separablenns</a></li>
<li>paper_authors: Zi-Yu Khoo, Jonathan Sze Choong Low, Stéphane Bressan</li>
<li>for: 这篇论文主要是为了研究如何从离散观察数据中预测汉密利系统的汉密利函数和其向量场。</li>
<li>methods: 该论文提出了三种可分解汉密利神经网络模型，其中每一种模型都利用了汉密利系统的可分解性来降低状态变量之间的复杂性。第一种模型通过 quadratic scaling 方法增加了训练数据量，第二种模型将可分解性 embed 到汉密利神经网络的损失函数中，第三种模型通过拼接多层感知器的 Architecture 实现了可分解性。</li>
<li>results: 作者通过对比这三种模型与现有的汉密利神经网络模型进行实验，发现 separable Hamiltonian neural networks 可以更有效地预测汉密利函数和其向量场。<details>
<summary>Abstract</summary>
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additive separability within Hamiltonian neural networks. The first model uses additive separability to quadratically scale the amount of data for training Hamiltonian neural networks. The second model embeds additive separability within the loss function of the Hamiltonian neural network. The third model embeds additive separability through the architecture of the Hamiltonian neural network using conjoined multilayer perceptions. We empirically compare the three models against state-of-the-art Hamiltonian neural networks, and demonstrate that the separable Hamiltonian neural networks, which alleviate complexity between the state variables, are more effective at regressing the Hamiltonian and its vector field.
</details>
<details>
<summary>摘要</summary>
现代科学和工程数据系统中，从离散观察获取动力系统模型是一项挑战。哈密顿系统是这类基本和普遍存在的动力系统之一。哈密顿神经网络是目前最先进的模型之一，可以不监督地将哈密顿系统的哈密顿函数回归到离散观察的向量场下。然而，哈密顿动力学往往复杂，特别在高维度情况下，状态空间中的哈密顿系统状态变量的数量相对于样本数量较大。为了缓解状态变量之间的复杂性，我们提出了一种新的策略：利用哈密顿系统的加法分解性，将其 embedding到哈密顿神经网络中。按照物理学 informed machine learning 的命名法，我们提出了三种分解哈密顿神经网络模型。这些模型在哈密顿神经网络中嵌入加法分解性。第一个模型使用加法分解性来循环增加训练哈密顿神经网络的数据量。第二个模型在哈密顿神经网络的损失函数中嵌入加法分解性。第三个模型通过哈密顿神经网络的架构嵌入加法分解性，使用 conjunction 多层感知。我们对这三种模型与当前最先进的哈密顿神经网络进行了实验比较，并证明了这些分解哈密顿神经网络模型，可以更有效地回归哈密顿函数和其向量场。
</details></li>
</ul>
<hr>
<h2 id="MQENet-A-Mesh-Quality-Evaluation-Neural-Network-Based-on-Dynamic-Graph-Attention"><a href="#MQENet-A-Mesh-Quality-Evaluation-Neural-Network-Based-on-Dynamic-Graph-Attention" class="headerlink" title="MQENet: A Mesh Quality Evaluation Neural Network Based on Dynamic Graph Attention"></a>MQENet: A Mesh Quality Evaluation Neural Network Based on Dynamic Graph Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01067">http://arxiv.org/abs/2309.01067</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoxuan Zhang, Haisheng Li, Nan Li, Xiaochuan Wang</li>
<li>for: 这篇论文是为了提高计算流体动力学中的流体模拟精度而设计的，即使是在工业应用中。</li>
<li>methods: 该论文提出了一种基于动态图注意力的结构化网格质量评估神经网络（MQENet），用于评估输入结构化网格的质量。为了使得从结构化网格中生成的图更加信息充沛，MQENet引入了两种新的结构化网格预处理算法。这两种算法还可以提高结构化网格数据的转换效率。</li>
<li>results: 在NACA-Market标准结构化网格数据集上进行了实验，结果表明MQENet在网格质量评估任务中具有效果。<details>
<summary>Abstract</summary>
With the development of computational fluid dynamics, the requirements for the fluid simulation accuracy in industrial applications have also increased. The quality of the generated mesh directly affects the simulation accuracy. However, previous mesh quality metrics and models cannot evaluate meshes comprehensively and objectively. To this end, we propose MQENet, a structured mesh quality evaluation neural network based on dynamic graph attention. MQENet treats the mesh evaluation task as a graph classification task for classifying the quality of the input structured mesh. To make graphs generated from structured meshes more informative, MQENet introduces two novel structured mesh preprocessing algorithms. These two algorithms can also improve the conversion efficiency of structured mesh data. Experimental results on the benchmark structured mesh dataset NACA-Market show the effectiveness of MQENet in the mesh quality evaluation task.
</details>
<details>
<summary>摘要</summary>
随着计算流体力学的发展，产业应用中的流体模拟精度的要求也在不断提高。模型的网格质量直接影响模拟精度。然而，过去的网格质量指标和模型无法全面、客观地评估网格质量。为此，我们提出MQENet，基于动态图注意力的结构网格质量评估神经网络。MQENet将网格评估任务定义为图分类任务，用于评估输入的结构网格质量。为了使结构网格数据更加有用，MQENet引入了两种新的结构网格预处理算法。这两种算法还可以提高结构网格数据的转换效率。在NACA-Market标准结构网格数据集上，MQENet的实验结果表明MQENet在网格质量评估任务中的效果。
</details></li>
</ul>
<hr>
<h2 id="Semi-supervised-3D-Video-Information-Retrieval-with-Deep-Neural-Network-and-Bi-directional-Dynamic-time-Warping-Algorithm"><a href="#Semi-supervised-3D-Video-Information-Retrieval-with-Deep-Neural-Network-and-Bi-directional-Dynamic-time-Warping-Algorithm" class="headerlink" title="Semi-supervised 3D Video Information Retrieval with Deep Neural Network and Bi-directional Dynamic-time Warping Algorithm"></a>Semi-supervised 3D Video Information Retrieval with Deep Neural Network and Bi-directional Dynamic-time Warping Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01063">http://arxiv.org/abs/2309.01063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yintai Ma, Diego Klabjan</li>
<li>for: 本研究提出了一种基于视觉内容的semi-supervised深度学习算法，用于检索相似的2D和3D视频。</li>
<li>methods: 该算法结合深度卷积神经网络和循环神经网络，并使用动态时间戳匹配作为相似度度量。</li>
<li>results: 该算法在多个公共数据集上测试，包括CC_WEB_VIDEO、YouTube-8m、S3DIS和Synthia等，与参考深度学习模型相比，得到了良好的结果，能够有效地解决视频检索任务。<details>
<summary>Abstract</summary>
This paper presents a novel semi-supervised deep learning algorithm for retrieving similar 2D and 3D videos based on visual content. The proposed approach combines the power of deep convolutional and recurrent neural networks with dynamic time warping as a similarity measure. The proposed algorithm is designed to handle large video datasets and retrieve the most related videos to a given inquiry video clip based on its graphical frames and contents. We split both the candidate and the inquiry videos into a sequence of clips and convert each clip to a representation vector using an autoencoder-backed deep neural network. We then calculate a similarity measure between the sequences of embedding vectors using a bi-directional dynamic time-warping method. This approach is tested on multiple public datasets, including CC\_WEB\_VIDEO, Youtube-8m, S3DIS, and Synthia, and showed good results compared to state-of-the-art. The algorithm effectively solves video retrieval tasks and outperforms the benchmarked state-of-the-art deep learning model.
</details>
<details>
<summary>摘要</summary>
To achieve this, the candidate and inquiry videos are split into a sequence of clips, and each clip is converted into a representation vector using an autoencoder-backed deep neural network. The similarity measure between the sequences of embedding vectors is then calculated using a bi-directional dynamic time-warping method.The proposed algorithm is tested on multiple public datasets, including CC\_WEB\_VIDEO, Youtube-8m, S3DIS, and Synthia, and shows good results compared to state-of-the-art. The algorithm effectively solves video retrieval tasks and outperforms the benchmarked state-of-the-art deep learning model.
</details></li>
</ul>
<hr>
<h2 id="Distribution-learning-via-neural-differential-equations-a-nonparametric-statistical-perspective"><a href="#Distribution-learning-via-neural-differential-equations-a-nonparametric-statistical-perspective" class="headerlink" title="Distribution learning via neural differential equations: a nonparametric statistical perspective"></a>Distribution learning via neural differential equations: a nonparametric statistical perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01043">http://arxiv.org/abs/2309.01043</a></li>
<li>repo_url: None</li>
<li>paper_authors: Youssef Marzouk, Zhi Ren, Sven Wang, Jakob Zech</li>
<li>for: 该论文旨在研究均匀方程（ODE）模型在分布学习中的性能。</li>
<li>methods: 该论文使用了基于潜在函数的极限映射来 Parametrize invertible transformation，并通过最大化可能性函数来训练模型。</li>
<li>results: 该论文提出了一种基于 $C^1$  метри entropy的非 Parametric 统计归一化分析方法，并在 $C^k$ 光滑目标分布中实现了nearly minimax-optimal的归一化率。<details>
<summary>Abstract</summary>
Ordinary differential equations (ODEs), via their induced flow maps, provide a powerful framework to parameterize invertible transformations for the purpose of representing complex probability distributions. While such models have achieved enormous success in machine learning, particularly for generative modeling and density estimation, little is known about their statistical properties. This work establishes the first general nonparametric statistical convergence analysis for distribution learning via ODE models trained through likelihood maximization. We first prove a convergence theorem applicable to arbitrary velocity field classes $\mathcal{F}$ satisfying certain simple boundary constraints. This general result captures the trade-off between approximation error (`bias') and the complexity of the ODE model (`variance'). We show that the latter can be quantified via the $C^1$-metric entropy of the class $\mathcal F$. We then apply this general framework to the setting of $C^k$-smooth target densities, and establish nearly minimax-optimal convergence rates for two relevant velocity field classes $\mathcal F$: $C^k$ functions and neural networks. The latter is the practically important case of neural ODEs.   Our proof techniques require a careful synthesis of (i) analytical stability results for ODEs, (ii) classical theory for sieved M-estimators, and (iii) recent results on approximation rates and metric entropies of neural network classes. The results also provide theoretical insight on how the choice of velocity field class, and the dependence of this choice on sample size $n$ (e.g., the scaling of width, depth, and sparsity of neural network classes), impacts statistical performance.
</details>
<details>
<summary>摘要</summary>
常微分方程（ODEs），通过它们引导的流场图，提供了一个强大的框架来参数化可逆变换，以便表示复杂的概率分布。尽管这些模型在机器学习中已经取得了很大的成功，特别是在生成模型和概率预测方面，但对它们的统计特性知之少。这项工作建立了首次通用非 Parametric 统计归一化分析，用于via ODE 模型通过最大化 likelihood 来学习概率分布。我们首先证明适用于任何速度场类 $\mathcal{F}$ 满足某些简单的边界约束的抽象结论。这个总体结果捕捉了在approximation error（偏差）和 ODE 模型（幂等）之间的质量。我们表明可以通过 $C^1$  метри entropy 来衡量 $\mathcal F$ 的复杂性。然后，我们将这个通用框架应用到 $C^k$ 光滑目标分布的情况，并确定 nearly minimax-optimal 的归一化率。在这个过程中，我们采用了精细的分析技术，包括 ODE 的 Analytic 稳定性、M-估计的古典理论和近期的 Approximation 率和 metric entropy 的研究。结果还提供了对模型选择和样本大小 $n$ （例如，宽度、深度和稀疏性）的依赖性的理论听见。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/cs.LG_2023_09_03/" data-id="clmjn91mv007x0j887m8y3g6i" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_03" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/03/eess.IV_2023_09_03/" class="article-date">
  <time datetime="2023-09-03T09:00:00.000Z" itemprop="datePublished">2023-09-03</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/03/eess.IV_2023_09_03/">eess.IV - 2023-09-03</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generalizability-and-Application-of-the-Skin-Reflectance-Estimate-Based-on-Dichromatic-Separation-SREDS"><a href="#Generalizability-and-Application-of-the-Skin-Reflectance-Estimate-Based-on-Dichromatic-Separation-SREDS" class="headerlink" title="Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS)"></a>Generalizability and Application of the Skin Reflectance Estimate Based on Dichromatic Separation (SREDS)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01235">http://arxiv.org/abs/2309.01235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph Drahos, Richard Plesh, Keivan Bahmani, Mahesh Banavar, Stephanie Schuckers</li>
<li>for: 这个论文旨在探讨面Recognition（FR）系统中的不同人种性能差异，以及可以使用皮肤颜色指标来补做这些差异。</li>
<li>methods: 本文使用了皮肤颜色分析方法，包括皮肤颜色指标基于 dichromatic separation（SREDS），以及其他一些皮肤颜色指标。</li>
<li>results: 研究发现，SREDS 指标在不同人种群体中的变化范围较小，可以作为自我报告的种族标签的替换。此外，使用 SREDS 指标可以在面Recognition 系统中保持隐私。<details>
<summary>Abstract</summary>
Face recognition (FR) systems have become widely used and readily available in recent history. However, differential performance between certain demographics has been identified within popular FR models. Skin tone differences between demographics can be one of the factors contributing to the differential performance observed in face recognition models. Skin tone metrics provide an alternative to self-reported race labels when such labels are lacking or completely not available e.g. large-scale face recognition datasets. In this work, we provide a further analysis of the generalizability of the Skin Reflectance Estimate based on Dichromatic Separation (SREDS) against other skin tone metrics and provide a use case for substituting race labels for SREDS scores in a privacy-preserving learning solution. Our findings suggest that SREDS consistently creates a skin tone metric with lower variability within each subject and SREDS values can be utilized as an alternative to the self-reported race labels at minimal drop in performance. Finally, we provide a publicly available and open-source implementation of SREDS to help the research community. Available at https://github.com/JosephDrahos/SREDS
</details>
<details>
<summary>摘要</summary>
人脸识别（FR）系统在近几年内广泛应用并可获得。然而，Popular FR 模型中的差异性在不同民族人群中被识别出来。皮肤颜色差异是FR模型中的一个因素。皮肤颜色指标提供了自我报告的种族标签缺失或完全无的大规模面部识别数据集中的一种代替方案。在这项工作中，我们进一步分析了SREDS的普适性，与其他皮肤颜色指标进行比较，并提供了一个使用SREDS分数代替自我报告种族标签的隐私保护学习解决方案的示例。我们的发现表明SREDS在每个人群中的变化更小，并且SREDS分数可以作为自我报告种族标签的替代品，减少性能下降。最后，我们提供了一个公开可用和开源的SREDS实现，以帮助研究人员。可以在https://github.com/JosephDrahos/SREDS上获取。
</details></li>
</ul>
<hr>
<h2 id="Breast-MRI-radiomics-and-machine-learning-radiomics-based-predictions-of-response-to-neoadjuvant-chemotherapy-–-how-are-they-affected-by-variations-in-tumour-delineation"><a href="#Breast-MRI-radiomics-and-machine-learning-radiomics-based-predictions-of-response-to-neoadjuvant-chemotherapy-–-how-are-they-affected-by-variations-in-tumour-delineation" class="headerlink" title="Breast MRI radiomics and machine learning radiomics-based predictions of response to neoadjuvant chemotherapy – how are they affected by variations in tumour delineation?"></a>Breast MRI radiomics and machine learning radiomics-based predictions of response to neoadjuvant chemotherapy – how are they affected by variations in tumour delineation?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01210">http://arxiv.org/abs/2309.01210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sepideh Hatamikia, Geevarghese George, Florian Schwarzhans, Amirreza Mahbod, Ramona Woitek<br>for: 这个研究的目的是为了评估静脉内医疗化治疗前Magnetic Resonance Imaging（MRI）扫描的变化对静脉内医疗化预测模型的影响。methods: 研究使用了不同的数学操作，包括磨砾、缓和、膨润、随机化和椭圆适掩，以模拟医生对肿瘤预定mask的手动修改。results: 研究发现，使用不同的预定mask可能会对静脉内医疗化预测模型的性能有所影响。 Specifically, smoothing and erosion yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation led to the lowest robustness and prediction performance for both breast cancer subtypes. Additionally, the study found that differences in VOI delineation can affect different steps of the radiomics analysis, and their quantification is therefore important for the development of standardized radiomics research.<details>
<summary>Abstract</summary>
Manual delineation of volumes of interest (VOIs) by experts is considered the gold-standard method in radiomics analysis. However, it suffers from inter- and intra-operator variability. A quantitative assessment of the impact of variations in these delineations on the performance of the radiomics predictors is required to develop robust radiomics based prediction models. In this study, we developed radiomics models for the prediction of pathological complete response to neoadjuvant chemotherapy in patients with two different breast cancer subtypes based on contrast-enhanced magnetic resonance imaging acquired prior to treatment (baseline MRI scans). Different mathematical operations such as erosion, smoothing, dilation, randomization, and ellipse fitting were applied to the original VOIs delineated by experts to simulate variations of segmentation masks. The effects of such VOI modifications on various steps of the radiomics workflow, including feature extraction, feature selection, and prediction performance, were evaluated. Using manual tumor VOIs and radiomics features extracted from baseline MRI scans, an AUC of up to 0.96 and 0.89 was achieved for human epidermal growth receptor 2 positive and triple-negative breast cancer, respectively. For smoothing and erosion, VOIs yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation lead to the lowest robustness and prediction performance for both breast cancer subtypes. At most 28% of the selected features were similar to manual VOIs when different VOI delineation data were used. Differences in VOI delineation affects different steps of radiomics analysis, and their quantification is therefore important for development of standardized radiomics research.
</details>
<details>
<summary>摘要</summary>
manually delineated volumes of interest (VOIs) by experts is considered the gold standard method in radiomics analysis, but it suffers from inter- and intra-operator variability. To develop robust radiomics-based prediction models, a quantitative assessment of the impact of variations in these delineations on the performance of radiomics predictors is required. In this study, we developed radiomics models for the prediction of pathological complete response to neoadjuvant chemotherapy in patients with two different breast cancer subtypes based on contrast-enhanced magnetic resonance imaging acquired prior to treatment (baseline MRI scans). We applied different mathematical operations such as erosion, smoothing, dilation, randomization, and ellipse fitting to the original VOIs delineated by experts to simulate variations of segmentation masks. We evaluated the effects of such VOI modifications on various steps of the radiomics workflow, including feature extraction, feature selection, and prediction performance. Using manual tumor VOIs and radiomics features extracted from baseline MRI scans, we achieved an AUC of up to 0.96 and 0.89 for human epidermal growth receptor 2 positive and triple-negative breast cancer, respectively. For smoothing and erosion, VOIs yielded the highest number of robust features and the best prediction performance, while ellipse fitting and dilation led to the lowest robustness and prediction performance for both breast cancer subtypes. At most 28% of the selected features were similar to manual VOIs when different VOI delineation data were used. Our results show that differences in VOI delineation affect different steps of radiomics analysis, and their quantification is therefore important for the development of standardized radiomics research.
</details></li>
</ul>
<hr>
<h2 id="Deep-Unfolding-Convolutional-Dictionary-Model-for-Multi-Contrast-MRI-Super-resolution-and-Reconstruction"><a href="#Deep-Unfolding-Convolutional-Dictionary-Model-for-Multi-Contrast-MRI-Super-resolution-and-Reconstruction" class="headerlink" title="Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction"></a>Deep Unfolding Convolutional Dictionary Model for Multi-Contrast MRI Super-resolution and Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01171">http://arxiv.org/abs/2309.01171</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lpcccc-cv/MC-CDic">https://github.com/lpcccc-cv/MC-CDic</a></li>
<li>paper_authors: Pengcheng Lei, Faming Fang, Guixu Zhang, Ming Xu</li>
<li>for:  This paper proposes a multi-contrast convolutional dictionary (MC-CDic) model for magnetic resonance imaging (MRI) tasks, which aims to explore the complementary information from the multi-contrast images and improve the super-resolution (SR) and reconstruction performance.</li>
<li>methods:  The proposed MC-CDic model uses an observation model to explicitly model the multi-contrast images as common features and unique features, and employs a proximal gradient algorithm to optimize the model. The model also uses learnable ResNet as proximal operators and multi-scale dictionaries to further improve the performance.</li>
<li>results:  The experimental results demonstrate the superior performance of the proposed MC-CDic model against existing state-of-the-art (SOTA) methods on multi-contrast MRI SR and reconstruction tasks. The code is available at <a target="_blank" rel="noopener" href="https://github.com/lpcccc-cv/MC-CDic">https://github.com/lpcccc-cv/MC-CDic</a>.<details>
<summary>Abstract</summary>
Magnetic resonance imaging (MRI) tasks often involve multiple contrasts. Recently, numerous deep learning-based multi-contrast MRI super-resolution (SR) and reconstruction methods have been proposed to explore the complementary information from the multi-contrast images. However, these methods either construct parameter-sharing networks or manually design fusion rules, failing to accurately model the correlations between multi-contrast images and lacking certain interpretations. In this paper, we propose a multi-contrast convolutional dictionary (MC-CDic) model under the guidance of the optimization algorithm with a well-designed data fidelity term. Specifically, we bulid an observation model for the multi-contrast MR images to explicitly model the multi-contrast images as common features and unique features. In this way, only the useful information in the reference image can be transferred to the target image, while the inconsistent information will be ignored. We employ the proximal gradient algorithm to optimize the model and unroll the iterative steps into a deep CDic model. Especially, the proximal operators are replaced by learnable ResNet. In addition, multi-scale dictionaries are introduced to further improve the model performance. We test our MC-CDic model on multi-contrast MRI SR and reconstruction tasks. Experimental results demonstrate the superior performance of the proposed MC-CDic model against existing SOTA methods. Code is available at https://github.com/lpcccc-cv/MC-CDic.
</details>
<details>
<summary>摘要</summary>
magnetic resonance imaging (MRI) 任务经常涉及多个对比。最近，许多深度学习基于多对比MRI超分解（SR）和重建方法被提出，以探索多个对比图像之间的共同信息。然而，这些方法可能会构建参数共享网络或手动设计融合规则，失去准确地模型多个对比图像之间的相关性，并且缺乏一定的解释性。在这篇论文中，我们提出了一种多对比卷积字典（MC-CDic）模型，通过优化算法和数据准确性项来引导。具体来说，我们建立了多对比MRI图像的观察模型，以显式地模型多个对比图像为共同特征和特有特征。这样，只有参照图像中有用的信息可以被传递到目标图像中，而不相关的信息将被忽略。我们使用 proximal 梯度算法来优化模型，并将 proximal 操作器替换为学习 ResNet。此外，我们还引入了多尺度字典，以进一步提高模型性能。我们在多对比MRI SR 和重建任务上测试了我们的 MC-CDic 模型。实验结果表明，我们的 MC-CDic 模型比现有的 SOTA 方法表现出更高的性能。代码可以在 <https://github.com/lpcccc-cv/MC-CDic> 上找到。
</details></li>
</ul>
<hr>
<h2 id="Channel-Attention-Separable-Convolution-Network-for-Skin-Lesion-Segmentation"><a href="#Channel-Attention-Separable-Convolution-Network-for-Skin-Lesion-Segmentation" class="headerlink" title="Channel Attention Separable Convolution Network for Skin Lesion Segmentation"></a>Channel Attention Separable Convolution Network for Skin Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01072">http://arxiv.org/abs/2309.01072</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changlu Guo, Jiangyan Dai, Marton Szemenyei, Yugen Yi</li>
<li>for: 静脉皮肤癌病症诊断，提高早期诊断精度</li>
<li>methods: 提出Channel Attention Separable Convolution Network (CASCN)，借鉴U-Net、DenseNet、分割核函数、通道注意力和尺寸空间 pyramid pooling等高级机制</li>
<li>results: 在PH2 dataset上取得了state-of-the-art表现， dice similarity coefficient为0.9461，准确率为0.9645，不需过多预&#x2F;后处理图像<details>
<summary>Abstract</summary>
Skin cancer is a frequently occurring cancer in the human population, and it is very important to be able to diagnose malignant tumors in the body early. Lesion segmentation is crucial for monitoring the morphological changes of skin lesions, extracting features to localize and identify diseases to assist doctors in early diagnosis. Manual de-segmentation of dermoscopic images is error-prone and time-consuming, thus there is a pressing demand for precise and automated segmentation algorithms. Inspired by advanced mechanisms such as U-Net, DenseNet, Separable Convolution, Channel Attention, and Atrous Spatial Pyramid Pooling (ASPP), we propose a novel network called Channel Attention Separable Convolution Network (CASCN) for skin lesions segmentation. The proposed CASCN is evaluated on the PH2 dataset with limited images. Without excessive pre-/post-processing of images, CASCN achieves state-of-the-art performance on the PH2 dataset with Dice similarity coefficient of 0.9461 and accuracy of 0.9645.
</details>
<details>
<summary>摘要</summary>
皮肤癌是人类常见的癌症， Early diagnosis 是非常重要的。检测皮肤损伤的形态变化，提取特征以确定疾病的位置和识别，是诊断皮肤癌的关键步骤。然而，手动分割DERMOSCOPIC图像是时间consuming和容易出错的，因此需要精准和自动化的分割算法。 drawing inspiration from advanced mechanisms such as U-Net, DenseNet, Separable Convolution, Channel Attention, and Atrous Spatial Pyramid Pooling (ASPP), we propose a novel network called Channel Attention Separable Convolution Network (CASCN) for skin lesions segmentation. The proposed CASCN is evaluated on the PH2 dataset with limited images, and achieves state-of-the-art performance with Dice similarity coefficient of 0.9461 and accuracy of 0.9645 without excessive pre-/post-processing of images.
</details></li>
</ul>
<hr>
<h2 id="AB2CD-AI-for-Building-Climate-Damage-Classification-and-Detection"><a href="#AB2CD-AI-for-Building-Climate-Damage-Classification-and-Detection" class="headerlink" title="AB2CD: AI for Building Climate Damage Classification and Detection"></a>AB2CD: AI for Building Climate Damage Classification and Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01066">http://arxiv.org/abs/2309.01066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maximilian Nitsche, S. Karthik Mukkavilli, Niklas Kühl, Thomas Brunschwiler</li>
<li>for: 这个论文旨在探讨深度学习技术在自然灾害中精准建筑损害评估中的实现，通过远程感知数据。</li>
<li>methods: 该论文使用了多种深度学习模型，包括异常残差、压缩和刺激、双路网络等，以及 ensemble 技术。</li>
<li>results: 研究结果显示，最小卫星图像分辨率以下3米为有效建筑损害检测的下限，而1米以下分辨率可以进行建筑物种类分类。此外，U-Net Siamese网络集成模型在xView2挑战标准准点中获得了0.812的F-1分数。<details>
<summary>Abstract</summary>
We explore the implementation of deep learning techniques for precise building damage assessment in the context of natural hazards, utilizing remote sensing data. The xBD dataset, comprising diverse disaster events from across the globe, serves as the primary focus, facilitating the evaluation of deep learning models. We tackle the challenges of generalization to novel disasters and regions while accounting for the influence of low-quality and noisy labels inherent in natural hazard data. Furthermore, our investigation quantitatively establishes that the minimum satellite imagery resolution essential for effective building damage detection is 3 meters and below 1 meter for classification using symmetric and asymmetric resolution perturbation analyses. To achieve robust and accurate evaluations of building damage detection and classification, we evaluated different deep learning models with residual, squeeze and excitation, and dual path network backbones, as well as ensemble techniques. Overall, the U-Net Siamese network ensemble with F-1 score of 0.812 performed the best against the xView2 challenge benchmark. Additionally, we evaluate a Universal model trained on all hazards against a flood expert model and investigate generalization gaps across events, and out of distribution from field data in the Ahr Valley. Our research findings showcase the potential and limitations of advanced AI solutions in enhancing the impact assessment of climate change-induced extreme weather events, such as floods and hurricanes. These insights have implications for disaster impact assessment in the face of escalating climate challenges.
</details>
<details>
<summary>摘要</summary>
我们研究了深度学习技术的实施以便精准评估自然灾害中的建筑物损害，使用遥感数据。xBD数据集，包括全球各地不同类型灾害事件，作为主要研究对象，以便评估深度学习模型。我们解决了对新灾害和地区总是泛化的挑战，同时考虑了自然灾害数据中的低质量和噪音标签的影响。此外，我们通过对不同的深度学习模型（包括剩余、压缩和刺激、双路网络）和 ensemble技术进行评估，发现了最佳的 U-Net 同时性网络 ensemble，其 F-1 分数为 0.812，可以在 xView2 挑战数据集上达到最佳效果。此外，我们还评估了对所有灾害的通用模型，并与洪水专家模型进行比较，探讨了不同事件之间的总体差异和场景数据中的外部泛化差异。我们的研究发现，高级 AI 解决方案在气候变化引起的极端天气事件的影响评估中具有潜在的潜力和局限性。这些发现对气候挑战的不断增长的面临带来了重要的启示。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/03/eess.IV_2023_09_03/" data-id="clmjn91qv00hj0j88bdpz8ftw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/cs.SD_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T15:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/cs.SD_2023_09_02/">cs.SD - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Timbre-reserved-Adversarial-Attack-in-Speaker-Identification"><a href="#Timbre-reserved-Adversarial-Attack-in-Speaker-Identification" class="headerlink" title="Timbre-reserved Adversarial Attack in Speaker Identification"></a>Timbre-reserved Adversarial Attack in Speaker Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00929">http://arxiv.org/abs/2309.00929</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qing Wang, Jixun Yao, Li Zhang, Pengcheng Guo, Lei Xie</li>
<li>for: 防止 spoofing 和 adversarial 攻击，提高 speaker identification 系统的安全性</li>
<li>methods: 使用 adversarial constraint 在 voice conversion 模型的不同训练阶段来生成timbre-reserved adversarial audio</li>
<li>results: 可以控制 voice conversion 模型生成 speaker-wised audio，并且可以骗取 speaker identification 系统Here’s a breakdown of each point:</li>
<li>for: The purpose of the paper is to improve the security of speaker identification systems by preventing spoofing and adversarial attacks.</li>
<li>methods: The proposed method uses an adversarial constraint during the training stages of the voice conversion model to generate timbre-reserved adversarial audio.</li>
<li>results: The proposed method can control the voice conversion model to generate speaker-wised audio, which can fool the speaker identification system.<details>
<summary>Abstract</summary>
As a type of biometric identification, a speaker identification (SID) system is confronted with various kinds of attacks. The spoofing attacks typically imitate the timbre of the target speakers, while the adversarial attacks confuse the SID system by adding a well-designed adversarial perturbation to an arbitrary speech. Although the spoofing attack copies a similar timbre as the victim, it does not exploit the vulnerability of the SID model and may not make the SID system give the attacker's desired decision. As for the adversarial attack, despite the SID system can be led to a designated decision, it cannot meet the specified text or speaker timbre requirements for the specific attack scenarios. In this study, to make the attack in SID not only leverage the vulnerability of the SID model but also reserve the timbre of the target speaker, we propose a timbre-reserved adversarial attack in the speaker identification. We generate the timbre-reserved adversarial audios by adding an adversarial constraint during the different training stages of the voice conversion (VC) model. Specifically, the adversarial constraint is using the target speaker label to optimize the adversarial perturbation added to the VC model representations and is implemented by a speaker classifier joining in the VC model training. The adversarial constraint can help to control the VC model to generate the speaker-wised audio. Eventually, the inference of the VC model is the ideal adversarial fake audio, which is timbre-reserved and can fool the SID system.
</details>
<details>
<summary>摘要</summary>
为了使SID系统的攻击不仅利用SID模型的漏洞，而且保留目标说话人的时征，我们在说话认可中提出了一种时征保留式敌意攻击。我们在不同的训练阶段对语音转换（VC）模型进行不同的训练，并在VC模型的表示上添加了一个敌意约束。specifically，我们使用目标说话人标签来优化敌意干扰添加到VC模型表示上的敌意约束，并通过一个说话分类器参与VC模型训练。这个敌意约束可以帮助控制VC模型生成说话者化的声音。最终，VC模型的推断结果是理想的敌意假声音，它保留了目标说话人的时征，可以诱导SID系统进行错误识别。
</details></li>
</ul>
<hr>
<h2 id="BLSP-Bootstrapping-Language-Speech-Pre-training-via-Behavior-Alignment-of-Continuation-Writing"><a href="#BLSP-Bootstrapping-Language-Speech-Pre-training-via-Behavior-Alignment-of-Continuation-Writing" class="headerlink" title="BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing"></a>BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00916">http://arxiv.org/abs/2309.00916</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cwang621/blsp">https://github.com/cwang621/blsp</a></li>
<li>paper_authors: Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, Jiajun Zhang</li>
<li>for: 提高大语言模型（LLM）对话语言能力</li>
<li>methods: 提出了一种 Bootstraps Language-Speech Pre-training（BLSP）方法，通过对继续写作为行为对齐来启动语言-语音预训练</li>
<li>results: 实现了将LLM扩展到语音领域，包括语音识别、语音翻译、语音理解和语音对话，甚至在零shot多语言场景下In English, this means:</li>
<li>for: Improving the ability of large language models (LLMs) to understand spoken language</li>
<li>methods: Proposed a method called Bootstraps Language-Speech Pre-training (BLSP) that uses behavior alignment to pre-train LLMs on speech data</li>
<li>results: Achieved the extension of LLMs to the speech domain, including speech recognition, translation, understanding, and conversation, even in zero-shot cross-lingual scenarios.<details>
<summary>Abstract</summary>
The emergence of large language models (LLMs) has sparked significant interest in extending their remarkable language capabilities to speech. However, modality alignment between speech and text still remains an open problem. Current solutions can be categorized into two strategies. One is a cascaded approach where outputs (tokens or states) of a separately trained speech recognition system are used as inputs for LLMs, which limits their potential in modeling alignment between speech and text. The other is an end-to-end approach that relies on speech instruction data, which is very difficult to collect in large quantities. In this paper, we address these issues and propose the BLSP approach that Bootstraps Language-Speech Pre-training via behavior alignment of continuation writing. We achieve this by learning a lightweight modality adapter between a frozen speech encoder and an LLM, ensuring that the LLM exhibits the same generation behavior regardless of the modality of input: a speech segment or its transcript. The training process can be divided into two steps. The first step prompts an LLM to generate texts with speech transcripts as prefixes, obtaining text continuations. In the second step, these continuations are used as supervised signals to train the modality adapter in an end-to-end manner. We demonstrate that this straightforward process can extend the capabilities of LLMs to speech, enabling speech recognition, speech translation, spoken language understanding, and speech conversation, even in zero-shot cross-lingual scenarios.
</details>
<details>
<summary>摘要</summary>
LLMs的出现已经引起了扩展其语言能力到语音的兴趣。然而，语音和文本之间的modalities还未得到了解决。现有的解决方案可以分为两种策略。一种是一个垂直的方法，在 separately 训练的语音识别系统的输出（token或状态）被用作 LLMs 的输入，这限制了它们的潜在能力。另一种是一个端到端的方法，它基于语音指令数据，但这些数据很难收集。在这篇论文中，我们解决了这些问题，并提出了 BLSP 方法，即使用行为对齐来启动语言-语音预训练。我们通过学习一个轻量级的modalities adapter，使得 LLM 在不同的输入模式下（语音段或其转录）展现同样的生成行为。我们的训练过程可以分为两步。第一步是让 LLM 在语音转录为 prefix 的情况下生成文本，获得文本续写。第二步是使用这些续写作为监督信号，在端到端方式下培训 modalities adapter。我们展示了这个简单的过程可以扩展 LLM 的能力到语音，实现语音识别、语音翻译、语音理解和语音对话，甚至在零shot cross-lingual enario 下。
</details></li>
</ul>
<hr>
<h2 id="DiCLET-TTS-Diffusion-Model-based-Cross-lingual-Emotion-Transfer-for-Text-to-Speech-–-A-Study-between-English-and-Mandarin"><a href="#DiCLET-TTS-Diffusion-Model-based-Cross-lingual-Emotion-Transfer-for-Text-to-Speech-–-A-Study-between-English-and-Mandarin" class="headerlink" title="DiCLET-TTS: Diffusion Model based Cross-lingual Emotion Transfer for Text-to-Speech – A Study between English and Mandarin"></a>DiCLET-TTS: Diffusion Model based Cross-lingual Emotion Transfer for Text-to-Speech – A Study between English and Mandarin</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00883">http://arxiv.org/abs/2309.00883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Li, Chenxu Hu, Jian Cong, Xinfa Zhu, Jingbei Li, Qiao Tian, Yuping Wang, Lei Xie<br>for:* 这篇论文主要针对的是如何使用Diffusion模型进行语言跨变的语音合成，以提高语音自然性和情感表达。methods:* 提出了一种基于Diffusion模型的跨语言情感传递方法（DiCLET-TTS），通过将情感从源语言 speaker传递到目标语言 speaker中，提高语音的自然性和情感表达。* 为了解决外语人声问题，提出了一种使用前文 encoder 的语言独立预测器来填充跨语言 target speaker 的语音终端分布。* 为了解决情感表达弱化问题，提出了一种使用 Condition-enhanced DPM decoder 来增强模型对 speaker 和情感的表达能力。results:* 对多种竞争模型进行了跨语言情感传递试验，结果显示 DiCLET-TTS 的表现较为出色，能够提高语音自然性和情感表达。* 对 OP-EDM 的设计进行了分析和评估，结果表明 OP-EDM 可以学习 speaker-irrelevant yet emotion-discriminative embedding，并且对情感表达具有积极的效果。<details>
<summary>Abstract</summary>
While the performance of cross-lingual TTS based on monolingual corpora has been significantly improved recently, generating cross-lingual speech still suffers from the foreign accent problem, leading to limited naturalness. Besides, current cross-lingual methods ignore modeling emotion, which is indispensable paralinguistic information in speech delivery. In this paper, we propose DiCLET-TTS, a Diffusion model based Cross-Lingual Emotion Transfer method that can transfer emotion from a source speaker to the intra- and cross-lingual target speakers. Specifically, to relieve the foreign accent problem while improving the emotion expressiveness, the terminal distribution of the forward diffusion process is parameterized into a speaker-irrelevant but emotion-related linguistic prior by a prior text encoder with the emotion embedding as a condition. To address the weaker emotional expressiveness problem caused by speaker disentanglement in emotion embedding, a novel orthogonal projection based emotion disentangling module (OP-EDM) is proposed to learn the speaker-irrelevant but emotion-discriminative embedding. Moreover, a condition-enhanced DPM decoder is introduced to strengthen the modeling ability of the speaker and the emotion in the reverse diffusion process to further improve emotion expressiveness in speech delivery. Cross-lingual emotion transfer experiments show the superiority of DiCLET-TTS over various competitive models and the good design of OP-EDM in learning speaker-irrelevant but emotion-discriminative embedding.
</details>
<details>
<summary>摘要</summary>
traditional Chinese:近些年来，跨语言 TTS 基于单语言 corpus 的性能有了显著改进，但是跨语言 speech 仍然受到外语口音问题的限制，导致自然性有限。此外，当前的跨语言方法忽略了模型情感，这是非语言信息的不可或缺的一部分。在这篇论文中，我们提出了 DiCLET-TTS，一种基于扩散模型的跨语言情感传递方法，可以将情感从源说话者传递到内语言和跨语言目标说话者。具体来说，为了减轻外语口音问题而提高情感表达度，末端扩散过程的终端分布被参数化为一个不相关于说话者，但是与情感相关的语言预测器的参数。此外，为了解决由说话者分解引起的情感表达度较弱的问题，我们提出了一种新的正交 проекции基本的情感分离模块（OP-EDM），可以学习不相关于说话者，但是能够分离情感的嵌入。此外，我们还引入了增强说话者和情感的Condition-enhanced DPM解oder，以进一步提高情感表达度在语音交流中。跨语言情感传递实验表明 DiCLET-TTS 在多种竞争模型中表现出色，并且OP-EDM在学习不相关于说话者，但是能够分离情感的嵌入方面具有良好的设计。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/cs.SD_2023_09_02/" data-id="clmjn91ob00bp0j88f84fapkb" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/cs.LG_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T10:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/cs.LG_2023_09_02/">cs.LG - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling"><a href="#Efficient-Covariance-Matrix-Reconstruction-with-Iterative-Spatial-Spectrum-Sampling" class="headerlink" title="Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling"></a>Efficient Covariance Matrix Reconstruction with Iterative Spatial Spectrum Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01040">http://arxiv.org/abs/2309.01040</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. Mohammadzadeh, V. H. Nascimento, R. C. de Lamare, O. Kukrer</li>
<li>for: 本研究旨在提出一种经济高效的适应扩 beamforming 算法，基于高效的 covariance matrix 重建（CMR）和迭代空间功率 спектrum（ISPS）。</li>
<li>methods: 提出了一种使用 simplify 的最大 Entropy power spectral density 函数来重建 INC 矩阵，并使用 conjugate gradient 算法来更新扩 beamforming  веса。</li>
<li>results:  simulation 结果表明，提出的方法可以减少附近 SOI 方向的干扰，并且可以在不同的干扰水平下提供不同的扩 beamforming 策略。<details>
<summary>Abstract</summary>
This work presents a cost-effective technique for designing robust adaptive beamforming algorithms based on efficient covariance matrix reconstruction with iterative spatial power spectrum (CMR-ISPS). The proposed CMR-ISPS approach reconstructs the interference-plus-noise covariance (INC) matrix based on a simplified maximum entropy power spectral density function that can be used to shape the directional response of the beamformer. Firstly, we estimate the directions of arrival (DoAs) of the interfering sources with the available snapshots. We then develop an algorithm to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences which lie in a small angular sector. We also devise a cost-effective adaptive algorithm based on conjugate gradient techniques to update the beamforming weights and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum. The proposed CMR-ISPS beamformer can suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths. Simulation results are provided to confirm the validity of the proposed method and make a comparison to existing approaches
</details>
<details>
<summary>摘要</summary>
First, the directions of arrival (DoAs) of the interfering sources are estimated using the available snapshots. Then, an algorithm is developed to reconstruct the INC matrix using a weighted sum of outer products of steering vectors whose coefficients can be estimated in the vicinity of the DoAs of the interferences, which lie in a small angular sector.Furthermore, a cost-effective adaptive algorithm based on conjugate gradient techniques is proposed to update the beamforming weights, and a method to obtain estimates of the signal of interest (SOI) steering vector from the spatial power spectrum is also presented. The proposed CMR-ISPS beamformer can effectively suppress interferers close to the direction of the SOI by producing notches in the directional response of the array with sufficient depths.Simulation results are provided to validate the proposed method and compare it to existing approaches. The proposed technique is found to be effective in suppressing interference and improving the signal-to-noise ratio (SNR) of the desired signal.
</details></li>
</ul>
<hr>
<h2 id="Neurosymbolic-Reinforcement-Learning-and-Planning-A-Survey"><a href="#Neurosymbolic-Reinforcement-Learning-and-Planning-A-Survey" class="headerlink" title="Neurosymbolic Reinforcement Learning and Planning: A Survey"></a>Neurosymbolic Reinforcement Learning and Planning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01038">http://arxiv.org/abs/2309.01038</a></li>
<li>repo_url: None</li>
<li>paper_authors: K. Acharya, W. Raza, C. M. J. M. Dourado Jr, A. Velasquez, H. Song<br>for:This paper aims to contribute to the emerging field of Neurosymbolic Reinforcement Learning (Neurosymbolic RL) by conducting a literature survey.methods:The paper categorizes works based on the role played by the neural and symbolic parts in Reinforcement Learning (RL), into three taxonomies: Learning for Reasoning, Reasoning for Learning, and Learning-Reasoning.results:The paper analyzes the RL components of each research work, including the state space, action space, policy module, and RL algorithm. Additionally, the paper identifies research opportunities and challenges in various applications within the dynamic field of Neurosymbolic RL.Here is the same information in Simplified Chinese text:for:这篇论文目标是为新兴领域的神经符号人工智能（神经符号人工智能）的文献survey。methods:论文将工作分为三类：学习为理解、理解为学习和学习理解。results:论文分析每个研究工作的RL组件，包括状态空间、行动空间、策略模块和RL算法。此外，论文还提出了在不同应用领域中的研究机会和挑战。<details>
<summary>Abstract</summary>
The area of Neurosymbolic Artificial Intelligence (Neurosymbolic AI) is rapidly developing and has become a popular research topic, encompassing sub-fields such as Neurosymbolic Deep Learning (Neurosymbolic DL) and Neurosymbolic Reinforcement Learning (Neurosymbolic RL). Compared to traditional learning methods, Neurosymbolic AI offers significant advantages by simplifying complexity and providing transparency and explainability. Reinforcement Learning(RL), a long-standing Artificial Intelligence(AI) concept that mimics human behavior using rewards and punishment, is a fundamental component of Neurosymbolic RL, a recent integration of the two fields that has yielded promising results. The aim of this paper is to contribute to the emerging field of Neurosymbolic RL by conducting a literature survey. Our evaluation focuses on the three components that constitute Neurosymbolic RL: neural, symbolic, and RL. We categorize works based on the role played by the neural and symbolic parts in RL, into three taxonomies:Learning for Reasoning, Reasoning for Learning and Learning-Reasoning. These categories are further divided into sub-categories based on their applications. Furthermore, we analyze the RL components of each research work, including the state space, action space, policy module, and RL algorithm. Additionally, we identify research opportunities and challenges in various applications within this dynamic field.
</details>
<details>
<summary>摘要</summary>
neurosymbolic 人工智能（Neurosymbolic AI）领域在 rapid development 中，已经成为研究的热点话题，包括 neurosymbolic deep learning（Neurosymbolic DL）和 neurosymbolic reinforcement learning（Neurosymbolic RL）等子领域。与传统学习方法相比，Neurosymbolic AI 提供了很大的优势，包括简化复杂性和提供透明性和解释性。人工智能（AI）概念，模拟人类行为使用奖励和惩罚的 reinforcement learning（RL），是 neurosymbolic RL 的基础组件，是一种最近 integrate 了这两个领域的成果。本文的目标是为 neurosymbolic RL 领域的emerging 作出一篇文献survey。我们的评估将注重在 neurosymbolic RL 中的三个组成部分：神经、符号和RL。我们根据这些部分在 RL 中的角色，将工作分为三类：学习 для理解、理解 для学习和学习-理解。这些类别进一步分为应用的不同子类。此外，我们还分析了每个研究作品中的 RL 组件，包括状态空间、动作空间、策略模块和RL算法。此外，我们还标识了在不同应用场景中的研究机会和挑战。
</details></li>
</ul>
<hr>
<h2 id="Online-Adaptive-Mahalanobis-Distance-Estimation"><a href="#Online-Adaptive-Mahalanobis-Distance-Estimation" class="headerlink" title="Online Adaptive Mahalanobis Distance Estimation"></a>Online Adaptive Mahalanobis Distance Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01030">http://arxiv.org/abs/2309.01030</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lianke Qin, Aravind Reddy, Zhao Song</li>
<li>for: 本文是为了提出一种快速算法来计算马哈拉诺比斯距离的维度减少方法。</li>
<li>methods: 本文使用了随机 Monte Carlo 数据结构，然后通过修改这个数据结构，使其能够处理适应性查询和在数据点和马哈拉诺比斯距离矩阵上进行在线更新。</li>
<li>results: 本文提供了一种高效的数据结构来解决 Approximate Distance Estimation（ADE）问题，用于计算马哈拉诺比斯距离。这个数据结构可以处理适应性查询和在线更新，并且可以与先前的在线学习马哈拉诺比斯距离算法结合使用。<details>
<summary>Abstract</summary>
Mahalanobis metrics are widely used in machine learning in conjunction with methods like $k$-nearest neighbors, $k$-means clustering, and $k$-medians clustering. Despite their importance, there has not been any prior work on applying sketching techniques to speed up algorithms for Mahalanobis metrics. In this paper, we initiate the study of dimension reduction for Mahalanobis metrics. In particular, we provide efficient data structures for solving the Approximate Distance Estimation (ADE) problem for Mahalanobis distances. We first provide a randomized Monte Carlo data structure. Then, we show how we can adapt it to provide our main data structure which can handle sequences of \textit{adaptive} queries and also online updates to both the Mahalanobis metric matrix and the data points, making it amenable to be used in conjunction with prior algorithms for online learning of Mahalanobis metrics.
</details>
<details>
<summary>摘要</summary>
马哈拉诺比斯度量广泛用于机器学习，与方法如k-最近邻、k-Means clustering和k-medians clustering结合使用。尽管它们的重要性，但没有任何先前的工作把笔记技术应用到加速马哈拉诺比斯度量算法上。在这篇论文中，我们开始研究维度减少 для马哈拉诺比斯度量。特别是，我们提供高效的数据结构解决 Approximate Distance Estimation（ADE）问题的马哈拉诺比斯距离。我们首先提供随机 Monte Carlo 数据结构。然后，我们示出如何将其变换为我们的主要数据结构，可以处理 sequences of 适应 queries 和在 Mahalanobis 度量矩阵和数据点上进行在线更新，使其适用于与先前算法结合使用。
</details></li>
</ul>
<hr>
<h2 id="Explainability-for-Large-Language-Models-A-Survey"><a href="#Explainability-for-Large-Language-Models-A-Survey" class="headerlink" title="Explainability for Large Language Models: A Survey"></a>Explainability for Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01029">http://arxiv.org/abs/2309.01029</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du</li>
<li>for: 本研究旨在描述和解释基于大语言模型（LLM）的自然语言处理能力，以便理解和提高模型的行为、局限性和社会影响。</li>
<li>methods: 本文提出了一个Explainability技术分类方法，并对基于传统 Fine-tuning 和提问两种训练 paradigm 的 Explainability 技术进行了结构化概述。每个 paradigm 都有自己的目标和主导方法，包括生成本地预测解释和全局模型知识解释。</li>
<li>results: 本研究提出了一些针对 LLM 的 Explainability 技术，包括生成本地预测解释和全局模型知识解释。此外，本研究还讨论了评估生成的解释的指标，以及如何使用解释来调试模型和提高性能。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）已经显示出了很强的能力，但它们的内部机制仍然不清楚，这会带来不需要的风险 для下游应用。因此，理解和解释这些模型是非常重要的，以便了解它们的行为、局限性和社会影响。在这篇论文中，我们提出了解释技术的分类和推荐方法，并对传统 fine-tuning-based 和 prompting-based 训练方法进行了分类。我们对每种方法进行了总结，并讲述了每种方法的目标和主要的地方解释方法，以及评估生成的解释的度量。最后，我们讨论了关键挑战和新出现的机遇，以及在 LLM 时代对解释技术的未来发展。
</details></li>
</ul>
<hr>
<h2 id="Zero-Shot-Recommendations-with-Pre-Trained-Large-Language-Models-for-Multimodal-Nudging"><a href="#Zero-Shot-Recommendations-with-Pre-Trained-Large-Language-Models-for-Multimodal-Nudging" class="headerlink" title="Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging"></a>Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01026">http://arxiv.org/abs/2309.01026</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/paxnea/wain23">https://github.com/paxnea/wain23</a></li>
<li>paper_authors: Rachel Harrison, Anton Dereventsov, Anton Bibin</li>
<li>for: 本研究旨在开发一种基于生成AI的零shot推荐Multimodal非站点内容的方法。</li>
<li>methods: 本方法使用了rendering输入不同Modalities为文本描述，并使用预训练LLMs计算它们的数字表示。然后，对所有内容项进行了统一的semantic embedding计算，以实现零shot推荐。</li>
<li>results: 在一个 sintetic Multimodal挫折环境中，我们示出了我们的方法可以准确地推荐Multimodal非站点内容，不需要额外学习。<details>
<summary>Abstract</summary>
We present a method for zero-shot recommendation of multimodal non-stationary content that leverages recent advancements in the field of generative AI. We propose rendering inputs of different modalities as textual descriptions and to utilize pre-trained LLMs to obtain their numerical representations by computing semantic embeddings. Once unified representations of all content items are obtained, the recommendation can be performed by computing an appropriate similarity metric between them without any additional learning. We demonstrate our approach on a synthetic multimodal nudging environment, where the inputs consist of tabular, textual, and visual data.
</details>
<details>
<summary>摘要</summary>
我们提出了一种零shot推荐多Modal非站点内容的方法，利用最近的生成AI技术进步。我们提议将不同modal的输入描述为文本描述，并使用预训练的LLM来获取它们的数字表示。一旦所有内容项的统一表示获取完毕， THEN 推荐可以通过计算相应的相似度metric来进行，无需额外学习。我们在一个 sintetic multimodal 担当环境中进行了示例，输入包括表格、文本和视觉数据。
</details></li>
</ul>
<hr>
<h2 id="On-the-training-and-generalization-of-deep-operator-networks"><a href="#On-the-training-and-generalization-of-deep-operator-networks" class="headerlink" title="On the training and generalization of deep operator networks"></a>On the training and generalization of deep operator networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01020">http://arxiv.org/abs/2309.01020</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Sanghyun Lee, Yeonjong Shin</li>
<li>for: 这个论文是为了提出一种新的深度 опера作符网络（DeepONets）训练方法，以解决深度 neural network 模型中的运动难题。</li>
<li>methods: 这种训练方法包括先训练树网络，然后顺序训练分支网络。这种方法的核心思想是根据分解整个复杂训练任务的分子化方法，从而降低训练难度。在数学上，我们建立了一个通用的泛化误差估计，用于评估 DeepONets 的训练数据量、网络宽度和输入和输出传感器数量。</li>
<li>results: 数学示例和实验结果表明，这种两步训练方法可以有效地提高 DeepONets 的稳定性和泛化能力。在 Darcy 流动中，这种方法可以更好地捕捉流体的运动特征。<details>
<summary>Abstract</summary>
We present a novel training method for deep operator networks (DeepONets), one of the most popular neural network models for operators. DeepONets are constructed by two sub-networks, namely the branch and trunk networks. Typically, the two sub-networks are trained simultaneously, which amounts to solving a complex optimization problem in a high dimensional space. In addition, the nonconvex and nonlinear nature makes training very challenging. To tackle such a challenge, we propose a two-step training method that trains the trunk network first and then sequentially trains the branch network. The core mechanism is motivated by the divide-and-conquer paradigm and is the decomposition of the entire complex training task into two subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization process is introduced which significantly improves stability and generalization ability. On the theoretical side, we establish a generalization error estimate in terms of the number of training data, the width of DeepONets, and the number of input and output sensors. Numerical examples are presented to demonstrate the effectiveness of the two-step training method, including Darcy flow in heterogeneous porous media.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的训练方法 для深度运算网络（DeepONets），这是一种非常受欢迎的神经网络模型。 DeepONets 由两个子网络组成：分支网络和主网络。 通常情况下，这两个子网络同时进行训练，这相当于在高维空间中解决一个复杂的优化问题。 此外，由于非 conjugate 和非线性的特性，训练非常困难。 为了解决这个挑战，我们提议一种两步训练方法，先训练主网络，然后顺序训练分支网络。 核心机制是根据分治思想，将整个复杂的训练任务分解成两个子任务，每个子任务都有较低的复杂性。 在这个过程中，我们引入了 Gram-Schmidt 正交化过程，这有助于提高稳定性和泛化能力。 在理论上，我们建立了一个通用的泛化误差估计，该估计取决于训练数据的数量、深度网络的宽度、输入和输出传感器的数量。 数据示例表明了我们的两步训练方法的效iveness，包括达尔Cy流在多种不同的孔雀媒体中。
</details></li>
</ul>
<hr>
<h2 id="MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training"><a href="#MPTopic-Improving-topic-modeling-via-Masked-Permuted-pre-training" class="headerlink" title="MPTopic: Improving topic modeling via Masked Permuted pre-training"></a>MPTopic: Improving topic modeling via Masked Permuted pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01015">http://arxiv.org/abs/2309.01015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinche Zhang, Evangelos milios</li>
<li>For: 本研究旨在提高顺序分词模型的性能，并提出一种新的词汇选择方法以提高分词结果的质量。* Methods: 本文使用的方法包括TF-RDF和MPTopic两种新的分词算法，TF-RDF可以评估文档中每个词语的相对重要性，而MPTopic则通过TF-RDF的启发来实现更加精准的分词结果。* Results: 对比BERTopic和Top2Vec两种传统的分词方法，MPTopic和TF-RDF的组合能够更好地提取主题关键词，并且在不同的文本 dataset 上均具有优秀的性能。<details>
<summary>Abstract</summary>
Topic modeling is pivotal in discerning hidden semantic structures within texts, thereby generating meaningful descriptive keywords. While innovative techniques like BERTopic and Top2Vec have recently emerged in the forefront, they manifest certain limitations. Our analysis indicates that these methods might not prioritize the refinement of their clustering mechanism, potentially compromising the quality of derived topic clusters. To illustrate, Top2Vec designates the centroids of clustering results to represent topics, whereas BERTopic harnesses C-TF-IDF for its topic extraction.In response to these challenges, we introduce "TF-RDF" (Term Frequency - Relative Document Frequency), a distinctive approach to assess the relevance of terms within a document. Building on the strengths of TF-RDF, we present MPTopic, a clustering algorithm intrinsically driven by the insights of TF-RDF. Through comprehensive evaluation, it is evident that the topic keywords identified with the synergy of MPTopic and TF-RDF outperform those extracted by both BERTopic and Top2Vec.
</details>
<details>
<summary>摘要</summary>
To address these challenges, we propose "TF-RDF" (Term Frequency - Relative Document Frequency), a unique approach to assess the relevance of terms within a document. Building on the strengths of TF-RDF, we introduce MPTopic, a clustering algorithm driven by the insights of TF-RDF. Through comprehensive evaluation, it is evident that the topic keywords identified with the combination of MPTopic and TF-RDF outperform those extracted by both BERTopic and Top2Vec.
</details></li>
</ul>
<hr>
<h2 id="Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification"><a href="#Streaming-Active-Learning-for-Regression-Problems-Using-Regression-via-Classification" class="headerlink" title="Streaming Active Learning for Regression Problems Using Regression via Classification"></a>Streaming Active Learning for Regression Problems Using Regression via Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01013">http://arxiv.org/abs/2309.01013</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shota Horiguchi, Kota Dohi, Yohei Kawaguchi</li>
<li>for: 提高机器学习模型在不同环境下的性能。</li>
<li>methods: 使用流动式活动学习，通过在训练集中添加新的标注样本来重新训练模型，以保持模型的性能。</li>
<li>results: 实验结果表明，提议的方法可以在同等标注成本下实现更高的准确率。<details>
<summary>Abstract</summary>
One of the challenges in deploying a machine learning model is that the model's performance degrades as the operating environment changes. To maintain the performance, streaming active learning is used, in which the model is retrained by adding a newly annotated sample to the training dataset if the prediction of the sample is not certain enough. Although many streaming active learning methods have been proposed for classification, few efforts have been made for regression problems, which are often handled in the industrial field. In this paper, we propose to use the regression-via-classification framework for streaming active learning for regression. Regression-via-classification transforms regression problems into classification problems so that streaming active learning methods proposed for classification problems can be applied directly to regression problems. Experimental validation on four real data sets shows that the proposed method can perform regression with higher accuracy at the same annotation cost.
</details>
<details>
<summary>摘要</summary>
一个机器学习模型部署的挑战是模型在运行环境变化时性能下降。为保持性能，流动式活动学习被使用，其中模型通过新添加到训练集中的批注样本进行重新训练，直到预测样本的确定性不够高。虽然许多流动式活动学习方法已经为分类问题提出，但对于 regression 问题，industrial field 中的几乎没有努力。本文提出使用 regression-via-classification 框架来实现流动式活动学习 для regression。regression-via-classification 将 regression 问题转化为 classification 问题，以便直接应用到流动式活动学习方法。实验 validate 在四个真实数据集上表明，提出的方法可以在同样的批注成本下实现更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Comparative-Analysis-of-Deep-Learning-Architectures-for-Breast-Cancer-Diagnosis-Using-the-BreaKHis-Dataset"><a href="#Comparative-Analysis-of-Deep-Learning-Architectures-for-Breast-Cancer-Diagnosis-Using-the-BreaKHis-Dataset" class="headerlink" title="Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset"></a>Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.01007">http://arxiv.org/abs/2309.01007</a></li>
<li>repo_url: None</li>
<li>paper_authors: İrem Sayın, Muhammed Ali Soydaş, Yunus Emre Mert, Arda Yarkataş, Berk Ergun, Selma Sözen Yeh, Hüseyin Üvet</li>
<li>for: 评估深度学习模型在诊断乳腺癌方面的性能</li>
<li>methods: 使用和比较五种知名的深度学习模型对乳腺癌进行分类：VGG、ResNet、Xception、Inception和InceptionResNet</li>
<li>results: Xception模型在F1分数方面取得了最高分（0.9），并且准确率达到了89%，而Inception和InceptionResNet模型都达到了87%的准确率，但Inception模型的F1分数为87，而InceptionResNet模型的F1分数为86。这些结果表明深度学习方法在诊断乳腺癌中的重要性，并且有助于提供更好的诊断服务给患者。<details>
<summary>Abstract</summary>
Cancer is an extremely difficult and dangerous health problem because it manifests in so many different ways and affects so many different organs and tissues. The primary goal of this research was to evaluate deep learning models' ability to correctly identify breast cancer cases using the BreakHis dataset. The BreakHis dataset covers a wide range of breast cancer subtypes through its huge collection of histopathological pictures. In this study, we use and compare the performance of five well-known deep learning models for cancer classification: VGG, ResNet, Xception, Inception, and InceptionResNet. The results placed the Xception model at the top, with an F1 score of 0.9 and an accuracy of 89%. At the same time, the Inception and InceptionResNet models both hit accuracy of 87% . However, the F1 score for the Inception model was 87, while that for the InceptionResNet model was 86. These results demonstrate the importance of deep learning methods in making correct breast cancer diagnoses. This highlights the potential to provide improved diagnostic services to patients. The findings of this study not only improve current methods of cancer diagnosis, but also make significant contributions to the creation of new and improved cancer treatment strategies. In a nutshell, the results of this study represent a major advancement in the direction of achieving these vital healthcare goals.
</details>
<details>
<summary>摘要</summary>
乳癌是一个非常困难和危险的健康问题，因为它可以出现在多种不同的形式和影响多种器官和组织。本研究的主要目标是评估深度学习模型在乳癌诊断中的表现。使用了5种常见的深度学习模型进行比较：VGG、ResNet、Xception、Inception和InceptionResNet。结果显示，Xception模型的F1分数为0.9，准确率为89%，而Inception和InceptionResNet模型都达到了87%的准确率。然而，Inception模型的F1分数为87，而InceptionResNet模型的F1分数为86。这些结果表明深度学习方法在乳癌诊断中的重要性，这也标识了可以提供更好的诊断服务给患者。本研究的结果不仅改进了当前的癌症诊断方法，还为创造新的癌症治疗策略做出了重要贡献。总之，本研究的结果代表了医疗健康目标的重大进步。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding"><a href="#Bayesian-sparsity-and-class-sparsity-priors-for-dictionary-learning-and-coding" class="headerlink" title="Bayesian sparsity and class sparsity priors for dictionary learning and coding"></a>Bayesian sparsity and class sparsity priors for dictionary learning and coding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00999">http://arxiv.org/abs/2309.00999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alberto Bocchinfuso, Daniela Calvetti, Erkki Somersalo</li>
<li>for:  solves challenging inverse problems with dictionary learning methods</li>
<li>methods:  uses sparse coding techniques and dictionary compression to reduce computational complexity</li>
<li>results:  demonstrates effectiveness in glitch detection in LIGO experiment and hyperspectral remote sensing<details>
<summary>Abstract</summary>
Dictionary learning methods continue to gain popularity for the solution of challenging inverse problems. In the dictionary learning approach, the computational forward model is replaced by a large dictionary of possible outcomes, and the problem is to identify the dictionary entries that best match the data, akin to traditional query matching in search engines. Sparse coding techniques are used to guarantee that the dictionary matching identifies only few of the dictionary entries, and dictionary compression methods are used to reduce the complexity of the matching problem. In this article, we propose a work flow to facilitate the dictionary matching process. First, the full dictionary is divided into subdictionaries that are separately compressed. The error introduced by the dictionary compression is handled in the Bayesian framework as a modeling error. Furthermore, we propose a new Bayesian data-driven group sparsity coding method to help identify subdictionaries that are not relevant for the dictionary matching. After discarding irrelevant subdictionaries, the dictionary matching is addressed as a deflated problem using sparse coding. The compression and deflation steps can lead to substantial decreases of the computational complexity. The effectiveness of compensating for the dictionary compression error and using the novel group sparsity promotion to deflate the original dictionary are illustrated by applying the methodology to real world problems, the glitch detection in the LIGO experiment and hyperspectral remote sensing.
</details>
<details>
<summary>摘要</summary>
《字典学习方法继续受到解决复杂反问题的感兴趣。在字典学习方法中，计算前方模型被替换为大量可能的结果字典，问题是将字典条目与数据匹配，类似于传统的查询匹配在搜索引擎中。简码技术用于保证字典匹配仅 identific few 字典条目，而字典压缩方法用于减少匹配问题的复杂性。在这篇文章中，我们提出了一个工作流程来促进字典匹配过程。首先，全字典被分解成分字典，并每个分字典都被单独压缩。ictionary compression error是在 bayesian 框架中处理为模型误差。此外，我们提出了一种新的 bayesian 数据驱动集成隐藏码法，以帮助标识不重要的分字典。在排除不重要分字典后，字典匹配被视为一个压缩问题，并使用简码解决。压缩和压缩步骤可以导致计算复杂性的明显减少。我们通过应用方法到实际问题，如 LIGO 实验中的异常检测和遥感谱辐成像，来证明资料做出补偿和使用新的集成隐藏码法减少原始字典的效果。
</details></li>
</ul>
<hr>
<h2 id="Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems"><a href="#Switch-and-Conquer-Efficient-Algorithms-By-Switching-Stochastic-Gradient-Oracles-For-Decentralized-Saddle-Point-Problems" class="headerlink" title="Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems"></a>Switch and Conquer: Efficient Algorithms By Switching Stochastic Gradient Oracles For Decentralized Saddle Point Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00997">http://arxiv.org/abs/2309.00997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chhavisharma123/c-dpssg-cdc2023">https://github.com/chhavisharma123/c-dpssg-cdc2023</a></li>
<li>paper_authors: Chhavi Sharma, Vishnu Narayanan, P. Balamurugan</li>
<li>for: 解决非流式强CONCAVE-强CONVEX笆点问题的协同执行方法。</li>
<li>methods: 使用不准确的顺序Primaldual Hybrid Gradient（inexact PDHG）算法，允许普通的梯度计算观察者更新顺序和权值。</li>
<li>results: 提出一种名为协同抽象换算法（C-DPSSG），可以在初始阶段使用抽象梯度计算观察者（GSG），然后在合适的时间点 switched to SVRG观察者，以便更快地到达笆点解。该算法可以在不同的精度和时间约束下，以便获得低&#x2F;中精度的解决方案。<details>
<summary>Abstract</summary>
We consider a class of non-smooth strongly convex-strongly concave saddle point problems in a decentralized setting without a central server. To solve a consensus formulation of problems in this class, we develop an inexact primal dual hybrid gradient (inexact PDHG) procedure that allows generic gradient computation oracles to update the primal and dual variables. We first investigate the performance of inexact PDHG with stochastic variance reduction gradient (SVRG) oracle. Our numerical study uncovers a significant phenomenon of initial conservative progress of iterates of IPDHG with SVRG oracle. To tackle this, we develop a simple and effective switching idea, where a generalized stochastic gradient (GSG) computation oracle is employed to hasten the iterates' progress to a saddle point solution during the initial phase of updates, followed by a switch to the SVRG oracle at an appropriate juncture. The proposed algorithm is named Decentralized Proximal Switching Stochastic Gradient method with Compression (C-DPSSG), and is proven to converge to an $\epsilon$-accurate saddle point solution with linear rate. Apart from delivering highly accurate solutions, our study reveals that utilizing the best convergence phases of GSG and SVRG oracles makes C-DPSSG well suited for obtaining solutions of low/medium accuracy faster, useful for certain applications. Numerical experiments on two benchmark machine learning applications show C-DPSSG's competitive performance which validate our theoretical findings. The codes used in the experiments can be found \href{https://github.com/chhavisharma123/C-DPSSG-CDC2023}{here}.
</details>
<details>
<summary>摘要</summary>
我们考虑一类非滑瑞强烈强紧组点问题在分布式设置中，无中央服务器。为解决这类问题的协议形式，我们开发了一个不精确的原理双层对偶算法（inexact PDHG），让普通的梯度计算观察者更新对称和对偶变数。我们首先研究了不精确PDHG与减少偏态梯度（SVRG）观察者的性能。我们的数据分析发现在追踪更新的初期阶段，inexact PDHG的追踪进展受到了初始保守的限制。为解决这问题，我们提出了一个简单而有效的转换想法，在初期更新阶段使用通用梯度计算观察者（GSG），以加速追踪进度，然后在适当的时候转换到SVRG观察者。我们给这个算法命名为分布式预掌终端算法（C-DPSSG），并证明其可以在线性速率下落幅到ε-精确的终端解。我们的数据分析还显示，通过利用GSG和SVRG观察者的最佳径 converge phase，C-DPSSG可以实现低/中精度的解 faster，这有助于某些应用。我们的实验结果显示C-DPSSG在两个机器学习应用中的竞争性表现，与我们的理论成果相符。实验代码可以在以下网址找到：https://github.com/chhavisharma123/C-DPSSG-CDC2023
</details></li>
</ul>
<hr>
<h2 id="A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters"><a href="#A-Boosted-Machine-Learning-Framework-for-the-Improvement-of-Phase-and-Crystal-Structure-Prediction-of-High-Entropy-Alloys-Using-Thermodynamic-and-Configurational-Parameters" class="headerlink" title="A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters"></a>A Boosted Machine Learning Framework for the Improvement of Phase and Crystal Structure Prediction of High Entropy Alloys Using Thermodynamic and Configurational Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00993">http://arxiv.org/abs/2309.00993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debsundar Dey, Suchandan Das, Anik Pal, Santanu Dey, Chandan Kumar Raul, Arghya Chatterjee</li>
<li>for: This study aims to predict the phases and crystal structures of High-Entropy Alloys (HEAs) using machine learning (ML) techniques.</li>
<li>methods: The study employs five distinct boosting algorithms (XGBoost, LightGBM, Random Forest, Gradient Boosting, and CatBoost) to predict phases and crystal structures, and introduces a methodical framework using the Pearson correlation coefficient to select strongly co-related features for improved accuracy.</li>
<li>results: The study achieves an accuracy of 94.05% for phase prediction and 90.07% for crystal structure prediction using XGBoost and LightGBM algorithms, respectively. Additionally, the study quantifies the influence of parameters on the model’s accuracy and introduces a new approach to elucidate the contribution of individual parameters in the phase prediction and crystal structure prediction processes.<details>
<summary>Abstract</summary>
The reason behind the remarkable properties of High-Entropy Alloys (HEAs) is rooted in the diverse phases and the crystal structures they contain. In the realm of material informatics, employing machine learning (ML) techniques to classify phases and crystal structures of HEAs has gained considerable significance. In this study, we assembled a new collection of 1345 HEAs with varying compositions to predict phases. Within this collection, there were 705 sets of data that were utilized to predict the crystal structures with the help of thermodynamics and electronic configuration. Our study introduces a methodical framework i.e., the Pearson correlation coefficient that helps in selecting the strongly co-related features to increase the prediction accuracy. This study employed five distinct boosting algorithms to predict phases and crystal structures, offering an enhanced guideline for improving the accuracy of these predictions. Among all these algorithms, XGBoost gives the highest accuracy of prediction (94.05%) for phases and LightGBM gives the highest accuracy of prediction of crystal structure of the phases (90.07%). The quantification of the influence exerted by parameters on the model's accuracy was conducted and a new approach was made to elucidate the contribution of individual parameters in the process of phase prediction and crystal structure prediction.
</details>
<details>
<summary>摘要</summary>
高级异杂合金（HEA）的remarkable性能的原因在于它们包含多种阶段和晶体结构。在材料信息学领域，使用机器学习（ML）技术来分类HEA的阶段和晶体结构的预测已经得到了许多关注。本研究集成了1345个HEA的不同组合，以预测它们的阶段。这1345个数据集中有705个数据用于预测晶体结构，使用热力学和电子配置来帮助预测。本研究提出了一种系统化框架，即Pearson相关系数，用于选择与预测相关的特征，以提高预测精度。本研究使用五种不同的扩展 boosting 算法来预测阶段和晶体结构，提供了改进预测精度的指南。其中，XGBoost 提供了预测阶段的最高准确率（94.05%），而 LightGBM 提供了预测晶体结构的阶段的最高准确率（90.07%）。研究还评估了模型准确率中参数的影响，并开发了一种新的方法来描述阶段预测和晶体结构预测中参数的贡献。
</details></li>
</ul>
<hr>
<h2 id="Sequential-Dexterity-Chaining-Dexterous-Policies-for-Long-Horizon-Manipulation"><a href="#Sequential-Dexterity-Chaining-Dexterous-Policies-for-Long-Horizon-Manipulation" class="headerlink" title="Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation"></a>Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00987">http://arxiv.org/abs/2309.00987</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanpei Chen, Chen Wang, Li Fei-Fei, C. Karen Liu</li>
<li>for: 这篇论文主要关注的是如何实现多个复杂的手部任务，并且可以自动调整和转换不同的功能模式。</li>
<li>methods: 这篇论文使用了动态学习（RL）来建立多个手部策略，并且使用了一个称为“transition feasibility function”的函数来积极地调整并组合多个策略，以提高连接成功率。</li>
<li>results: 这篇论文的结果显示，这个系统能够实现多个长期任务，并且能够自动调整和转换不同的功能模式，而且能够在真实世界中运行。更多细节和视频结果可以在<a target="_blank" rel="noopener" href="https://sequential-dexterity.github.io找到./">https://sequential-dexterity.github.io找到。</a><details>
<summary>Abstract</summary>
Many real-world manipulation tasks consist of a series of subtasks that are significantly different from one another. Such long-horizon, complex tasks highlight the potential of dexterous hands, which possess adaptability and versatility, capable of seamlessly transitioning between different modes of functionality without the need for re-grasping or external tools. However, the challenges arise due to the high-dimensional action space of dexterous hand and complex compositional dynamics of the long-horizon tasks. We present Sequential Dexterity, a general system based on reinforcement learning (RL) that chains multiple dexterous policies for achieving long-horizon task goals. The core of the system is a transition feasibility function that progressively finetunes the sub-policies for enhancing chaining success rate, while also enables autonomous policy-switching for recovery from failures and bypassing redundant stages. Despite being trained only in simulation with a few task objects, our system demonstrates generalization capability to novel object shapes and is able to zero-shot transfer to a real-world robot equipped with a dexterous hand. More details and video results could be found at https://sequential-dexterity.github.io
</details>
<details>
<summary>摘要</summary>
多种实际操作任务通常包括一系列不同的子任务，这些长期任务展示了人工手的可靠性和多样性，可以无需重新抓取或使用外部工具而快速地转换到不同的功能模式。然而，由于高维动作空间和复杂的作业动力学，这些任务也存在挑战。我们提出了一种基于再征学习（RL）的执行系统，称为Sequential Dexterity，它将多个灵活政策串联起来实现长期任务目标。系统的核心是一个过程可行性函数，通过不断精细调整子政策来提高串联成功率，同时也允许自主policy-switching以恢复失败和 circumvent redundant stages。尽管只在 simulations 中训练了几个任务物体，我们的系统仍能通过 Zero-shot transfer 将其应用到真实世界中的一个配备了灵活手的机器人中。更多细节和视频结果可以在 <https://sequential-dexterity.github.io> 找到。
</details></li>
</ul>
<hr>
<h2 id="An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems"><a href="#An-Ensemble-Score-Filter-for-Tracking-High-Dimensional-Nonlinear-Dynamical-Systems" class="headerlink" title="An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems"></a>An Ensemble Score Filter for Tracking High-Dimensional Nonlinear Dynamical Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00983">http://arxiv.org/abs/2309.00983</a></li>
<li>repo_url: None</li>
<li>paper_authors: Feng Bao, Zezhong Zhang, Guannan Zhang</li>
<li>for: 解决高维非线性筛选问题，提高精度。</li>
<li>methods: 使用分子筛和集成 Kalman 筛，以及一种新的训练自由的得分函数估计方法。</li>
<li>results: 在高维 Lorenz 系统上实现了可靠地跟踪 extremely high-dimensional Lorenz 系统（达到 1,000,000 维），具有高非线性观测过程，而 existing 筛选方法很难以实现。<details>
<summary>Abstract</summary>
We propose an ensemble score filter (EnSF) for solving high-dimensional nonlinear filtering problems with superior accuracy. A major drawback of existing filtering methods, e.g., particle filters or ensemble Kalman filters, is the low accuracy in handling high-dimensional and highly nonlinear problems. EnSF attacks this challenge by exploiting the score-based diffusion model, defined in a pseudo-temporal domain, to characterizing the evolution of the filtering density. EnSF stores the information of the recursively updated filtering density function in the score function, in stead of storing the information in a set of finite Monte Carlo samples (used in particle filters and ensemble Kalman filters). Unlike existing diffusion models that train neural networks to approximate the score function, we develop a training-free score estimation that uses mini-batch-based Monte Carlo estimator to directly approximate the score function at any pseudo-spatial-temporal location, which provides sufficient accuracy in solving high-dimensional nonlinear problems as well as saves tremendous amount of time spent on training neural networks. Another essential aspect of EnSF is its analytical update step, gradually incorporating data information into the score function, which is crucial in mitigating the degeneracy issue faced when dealing with very high-dimensional nonlinear filtering problems. High-dimensional Lorenz systems are used to demonstrate the performance of our method. EnSF provides surprisingly impressive performance in reliably tracking extremely high-dimensional Lorenz systems (up to 1,000,000 dimension) with highly nonlinear observation processes, which is a well-known challenging problem for existing filtering methods.
</details>
<details>
<summary>摘要</summary>
我们提出了一种ensemble score filter（EnSF），用于解决高维非线性筛选问题，并提供了更高的准确性。现有的筛选方法，如 particulate filters 或 ensemble Kalman filters，在处理高维和非线性问题时存在低准确性的问题。EnSF 利用分数基 diffusion 模型，在 pseudo-时空域中定义了筛选演化的分数函数，以存储 recursively 更新的筛选演化函数的信息。与现有的扩散模型不同，我们开发了一种无需训练的分数估计，使用 mini-batch-based Monte Carlo 估计器直接估计分数函数的值，提供了 suficient 的准确性来解决高维非线性问题，并减少了对 neural network 的训练时间。另一个关键特点 OF EnSF 是其分析更新步骤，逐渐地包含数据信息到分数函数中，这是解决非线性问题时面临的减少问题的关键。高维 Lorenz 系统被用来证明我们的方法的性能。EnSF 在可以可靠地跟踪 extremely high-dimensional Lorenz 系统（达到 1,000,000 维度）的高非线性观测过程中表现出了很好的性能，这是现有筛选方法所不能做到的。
</details></li>
</ul>
<hr>
<h2 id="Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction"><a href="#Pure-Message-Passing-Can-Estimate-Common-Neighbor-for-Link-Prediction" class="headerlink" title="Pure Message Passing Can Estimate Common Neighbor for Link Prediction"></a>Pure Message Passing Can Estimate Common Neighbor for Link Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00976">http://arxiv.org/abs/2309.00976</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaiwen Dong, Zhichun Guo, Nitesh V. Chawla</li>
<li>for: 本研究旨在解释Message Passing Neural Networks（MPNNs）在图表示学中的缺陷，以及如何通过消息传递来捕捉 JOIN 结构特征，从而提高链接预测性能。</li>
<li>methods: 本研究使用了MPNNs和其他基eline方法进行比较，并通过研究消息传递的特点来提高链接预测性能。</li>
<li>results: 研究结果表明，MPNNs在链接预测中常常下掌了简单的规则，但是通过消息传递来捕捉 JOIN 结构特征可以提高链接预测性能。基于这些结果，我们提出了一种新的链接预测模型，即Message Passing Link Predictor（MPLP），它可以更好地捕捉 JOIN 结构特征，从而提高链接预测性能。<details>
<summary>Abstract</summary>
Message Passing Neural Networks (MPNNs) have emerged as the {\em de facto} standard in graph representation learning. However, when it comes to link prediction, they often struggle, surpassed by simple heuristics such as Common Neighbor (CN). This discrepancy stems from a fundamental limitation: while MPNNs excel in node-level representation, they stumble with encoding the joint structural features essential to link prediction, like CN. To bridge this gap, we posit that, by harnessing the orthogonality of input vectors, pure message-passing can indeed capture joint structural features. Specifically, we study the proficiency of MPNNs in approximating CN heuristics. Based on our findings, we introduce the Message Passing Link Predictor (MPLP), a novel link prediction model. MPLP taps into quasi-orthogonal vectors to estimate link-level structural features, all while preserving the node-level complexities. Moreover, our approach demonstrates that leveraging message-passing to capture structural features could offset MPNNs' expressiveness limitations at the expense of estimation variance. We conduct experiments on benchmark datasets from various domains, where our method consistently outperforms the baseline methods.
</details>
<details>
<summary>摘要</summary>
message passing neural networks (MPNNs) 已经成为图像学习中的标准方法，但在链接预测方面经常遇到困难，被简单的规则如共同邻居 (CN) 超越。这种差异源于 MPNNs 的基本局限性：它们在节点级别表示方面出色，但在编码共同结构特征方面受限。为了bridging这个差距，我们认为，通过利用输入向量的正交性，纯粹的消息传递实际可以捕捉共同结构特征。我们研究 MPNNs 是否可以正确地预测链接。根据我们的发现，我们提出了一种新的链接预测模型——消息传递链接预测器 (MPLP)。MPLP 利用 quasi-正交的输入向量来估算链接级别的结构特征，同时保持节点级别的复杂性。此外，我们的方法示出，通过利用消息传递来捕捉结构特征可以将 MPNNs 的表达能力限制作为估计误差的代价。我们在不同领域的基本数据集上进行了实验，并 consistently 超过了基eline方法。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Framework-for-Optimal-Selection-of-Soil-Sampling-Sites"><a href="#Deep-Learning-Framework-for-Optimal-Selection-of-Soil-Sampling-Sites" class="headerlink" title="Deep-Learning Framework for Optimal Selection of Soil Sampling Sites"></a>Deep-Learning Framework for Optimal Selection of Soil Sampling Sites</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00974">http://arxiv.org/abs/2309.00974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tan-Hanh Pham, Praneel Acharya, Sravanthi Bachina, Kristopher Osterloh, Kim-Doang Nguyen</li>
<li>for: 这项研究旨在利用深度学习技术自动选择耕地中的优质采样点，以提高耕地管理和农业生产的效率和质量。</li>
<li>methods: 该研究使用了两种方法：一是使用现有的state-of-the-art模型，即卷积神经网络（CNN）；二是开发一种基于transformer和自注意的深度学习设计。研究框架采用了encoder-decoder结构，自注意机制作为特征提取器，生成特征地图。</li>
<li>results: 实验结果表明，该模型在测试集上达到了99.52%的平均准确率，57.35%的平均交集率，和71.47%的平均 dice coefficient，而state-of-the-art CNN-based模型的性能指标分别为66.08%, 3.85%, 1.98%。这表明，我们提出的模型在耕地采样数据集上的表现优于CNN-based方法。<details>
<summary>Abstract</summary>
This work leverages the recent advancements of deep learning in image processing to find optimal locations that present the important characteristics of a field. The data for training are collected at different fields in local farms with five features: aspect, flow accumulation, slope, NDVI (normalized difference vegetation index), and yield. The soil sampling dataset is challenging because the ground truth is highly imbalanced binary images. Therefore, we approached the problem with two methods, the first approach involves utilizing a state-of-the-art model with the convolutional neural network (CNN) backbone, while the second is to innovate a deep-learning design grounded in the concepts of transformer and self-attention. Our framework is constructed with an encoder-decoder architecture with the self-attention mechanism as the backbone. In the encoder, the self-attention mechanism is the key feature extractor, which produces feature maps. In the decoder, we introduce atrous convolution networks to concatenate, fuse the extracted features, and then export the optimal locations for soil sampling. Currently, the model has achieved impressive results on the testing dataset, with a mean accuracy of 99.52%, a mean Intersection over Union (IoU) of 57.35%, and a mean Dice Coefficient of 71.47%, while the performance metrics of the state-of-the-art CNN-based model are 66.08%, 3.85%, and 1.98%, respectively. This indicates that our proposed model outperforms the CNN-based method on the soil-sampling dataset. To the best of our knowledge, our work is the first to provide a soil-sampling dataset with multiple attributes and leverage deep learning techniques to enable the automatic selection of soil-sampling sites. This work lays a foundation for novel applications of data science and machine-learning technologies to solve other emerging agricultural problems.
</details>
<details>
<summary>摘要</summary>
这个工作利用深度学习的最新进展在图像处理中找到优化的场地特征。训练数据来自当地农场不同场地的五个特征：方向、流量总和、 Slope、 NDVI（ норма化植物质量指数）和收获。 soil sampling 数据集是一个挑战，因为真实值是高度不平衡的二值图像。我们采用了两种方法：一是使用现有的 state-of-the-art 模型，二是创新基于 transformer 和自注意的深度学习设计。我们的框架采用了Encoder-Decoder 架构，自注意机制作为关键特征提取器，生成特征图。在 Decoder 中，我们引入了尺度扩展 convolutional networks，将提取的特征 concatenate 并融合，然后输出优化的场地样本选择。现在，模型在测试数据集上已经实现了很好的结果，具有 Mean Accuracy 为 99.52%， Mean Intersection over Union 为 57.35%， Mean Dice Coefficient 为 71.47%，而 state-of-the-art CNN 模型的性能指标分别为 66.08%、3.85% 和 1.98%。这表明我们的提案模型在 soil-sampling 数据集上表现出色，超过了 CNN 模型。根据我们所知，这是首次提供多属性的 soil-sampling 数据集，并利用深度学习技术自动选择场地样本。这项工作为数据科学和机器学习技术在农业问题上的应用开创了新的可能性。
</details></li>
</ul>
<hr>
<h2 id="Compositional-Diffusion-Based-Continuous-Constraint-Solvers"><a href="#Compositional-Diffusion-Based-Continuous-Constraint-Solvers" class="headerlink" title="Compositional Diffusion-Based Continuous Constraint Solvers"></a>Compositional Diffusion-Based Continuous Constraint Solvers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00966">http://arxiv.org/abs/2309.00966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/diffusion-ccsp/diffusion-ccsp.github.io">https://github.com/diffusion-ccsp/diffusion-ccsp.github.io</a></li>
<li>paper_authors: Zhutian Yang, Jiayuan Mao, Yilun Du, Jiajun Wu, Joshua B. Tenenbaum, Tomás Lozano-Pérez, Leslie Pack Kaelbling</li>
<li>for: 这篇论文targets continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning, with the goal of developing a generalizable approach for solving these problems.</li>
<li>methods: 该方法基于diffusion模型，将CCSP表示为factor graphs，并将各种约束类型的能量组合起来，以derive global解决方案。</li>
<li>results: 实验表明，该方法可以强大地泛化到novel combinations of known约束，并且可以与任务和运动规划结合，以便开发包含离散和连续参数的长期计划。<details>
<summary>Abstract</summary>
This paper introduces an approach for learning to solve continuous constraint satisfaction problems (CCSP) in robotic reasoning and planning. Previous methods primarily rely on hand-engineering or learning generators for specific constraint types and then rejecting the value assignments when other constraints are violated. By contrast, our model, the compositional diffusion continuous constraint solver (Diffusion-CCSP) derives global solutions to CCSPs by representing them as factor graphs and combining the energies of diffusion models trained to sample for individual constraint types. Diffusion-CCSP exhibits strong generalization to novel combinations of known constraints, and it can be integrated into a task and motion planner to devise long-horizon plans that include actions with both discrete and continuous parameters. Project site: https://diffusion-ccsp.github.io/
</details>
<details>
<summary>摘要</summary>
这份论文介绍了一种用于解决连续约束满意问题（CCSP）的机器学习方法。先前的方法主要依靠手工设计或学习生成器来满足特定约束类型，然后在其他约束被违反时拒绝值分配。而我们的模型——复合扩散连续约束解决器（Diffusion-CCSP）则通过表示为因子图并将各种约束类型的扩散模型训练来 derivation global solution to CCSPs。Diffusion-CCSP具有强大的泛化能力，可以适应新的知道的约束组合。此外，它可以与任务和运动规划集成，以生成包含杂参数的长期计划。项目网站：<https://diffusion-ccsp.github.io/>
</details></li>
</ul>
<hr>
<h2 id="eDKM-An-Efficient-and-Accurate-Train-time-Weight-Clustering-for-Large-Language-Models"><a href="#eDKM-An-Efficient-and-Accurate-Train-time-Weight-Clustering-for-Large-Language-Models" class="headerlink" title="eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models"></a>eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00964">http://arxiv.org/abs/2309.00964</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minsik Cho, Keivan A. Vahid, Qichen Fu, Saurabh Adya, Carlo C Del Mundo, Mohammad Rastegari, Devang Naik, Peter Zatloukal</li>
<li>for: 这个研究旨在将大型语言模型（LLM）带到移动设备上，以提高响应速度和隐私保护。</li>
<li>methods: 这个研究使用了 weight-clustering 技术来将 LLM 压缩，并且提出了一个内存效率的 differentiable KMeans Clustering（DKM）实现方案，以减少对 LLM 的训练时间复杂度。</li>
<li>results: 这个研究可以将预训 LLaMA 7B 模型压缩为 2.5 GB (3bit&#x2F;weight)，并且在实验中获得了良好的准确性表现（例如，PIQA 77.7%、Winograde 66.1% 等），同时实现了训练时间复杂度的大幅减少（130倍）。<details>
<summary>Abstract</summary>
Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM by orders of magnitudes. For a given tensor to be saved on CPU for the backward pass of DKM, we compressed the tensor by applying uniquification and sharding after checking if there is no duplicated tensor previously copied to CPU. Our experimental results demonstrate that \prjname can fine-tune and compress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with the Alpaca dataset by reducing the train-time memory footprint of a decoder layer by 130$\times$, while delivering good accuracy on broader LLM benchmarks (i.e., 77.7\% for PIQA, 66.1\% for Winograde, and so on).
</details>
<details>
<summary>摘要</summary>
因为大语言模型（LLM）在许多复杂语言任务中表现出色，因此有很大的兴趣将这些LLM搬到移动设备上进行更快的响应和更好的隐私保护。然而，LLM的大小（即十亿个参数）需要非常高效的压缩才能适应存储有限的设备。许多压缩技术中，量化是一种非线性的压缩技术，是LLM压缩的一个领先候选者，并且现代智能手机支持。然而，它的训练负担是LLM精细调整的瓶颈。特别是用于LLM精细调整的散列聚合（DKM）在训练过程中的内存负担很大，使其几乎不可能应用于LLM训练期间的压缩。在这篇论文中，我们提出了一种内存高效的DKM实现，基于新的技术来降低DKM的内存占用量。对于要保存在CPU上的一个tensor来进行DKM的反向传播，我们将tensor进行压缩，首先检查是否已经有重复的tensor被复制到CPU上，如果有，则应用uniquification和分割。我们的实验结果表明，我们可以使用eDKM将预训练的LLaMA 7B模型从12.6GB压缩到2.5GB（3比特/参数），并在Alpaca数据集上达到77.7%的准确率，在更广泛的LLMbenchmark上达到66.1%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints"><a href="#Network-Topology-Inference-with-Sparsity-and-Laplacian-Constraints" class="headerlink" title="Network Topology Inference with Sparsity and Laplacian Constraints"></a>Network Topology Inference with Sparsity and Laplacian Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00960">http://arxiv.org/abs/2309.00960</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxi Ying, Xi Han, Rui Zhou, Xiwen Wang, Hing Cheung So</li>
<li>for: 这个论文是关于网络拓扑推断问题的研究，使用卷积环境模型来估计精度矩阵，从而解决网络拓扑推断问题。</li>
<li>methods: 这个论文使用了卷积环境模型，并在这个模型中添加了$\ell_0$-norm约束，以解决$\ell_1$-norm的局限性。具体来说，这个方法使用了梯度投影算法来解决由稀疏性和卷积环境约束组成的优化问题。</li>
<li>results: 通过使用实验数据集，包括 sintetic 数据集和金融时间序列数据集，研究人员发现该方法可以有效地解决网络拓扑推断问题。<details>
<summary>Abstract</summary>
We tackle the network topology inference problem by utilizing Laplacian constrained Gaussian graphical models, which recast the task as estimating a precision matrix in the form of a graph Laplacian. Recent research \cite{ying2020nonconvex} has uncovered the limitations of the widely used $\ell_1$-norm in learning sparse graphs under this model: empirically, the number of nonzero entries in the solution grows with the regularization parameter of the $\ell_1$-norm; theoretically, a large regularization parameter leads to a fully connected (densest) graph. To overcome these challenges, we propose a graph Laplacian estimation method incorporating the $\ell_0$-norm constraint. An efficient gradient projection algorithm is developed to solve the resulting optimization problem, characterized by sparsity and Laplacian constraints. Through numerical experiments with synthetic and financial time-series datasets, we demonstrate the effectiveness of the proposed method in network topology inference.
</details>
<details>
<summary>摘要</summary>
我们面临网络拓扑推理问题，利用卷积约束 Gaussian 图模型，将任务转化为估计一个精度矩阵，形式上等于图laplacian。Recent research 发现了 $\ell_1$-norm 学习稀疏图的局限性：实际上，规定参数的REGULARIZATION 参数增加后，解的非零个数增加;理论上，大的REGULARIZATION 参数导致最密集（最稀疏）图。为了解决这些挑战，我们提议一种包含 $\ell_0$-norm 约束的图laplacian 估计方法。我们开发了一种高效的梯度投影算法，解决这个具有稀疏和卷积约束的优化问题。通过数据分析实验，我们在synthetic 和金融时间序列 dataset 上证明了我们提议的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="Index-aware-learning-of-circuits"><a href="#Index-aware-learning-of-circuits" class="headerlink" title="Index-aware learning of circuits"></a>Index-aware learning of circuits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00958">http://arxiv.org/abs/2309.00958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Idoia Cortes Garcia, Peter Förster, Lennart Jansen, Wil Schilders, Sebastian Schöps</li>
<li>for: 该论文旨在提出一种基于机器学习的电路设计方法，以优化电路设计过程中的参数优化。</li>
<li>methods: 该方法使用了修改后的节点分析（Modified Nodal Analysis）来描述电路，并使用分解思想（Dissection Concept）将系统分解成仅依赖于导数变量的偏 differential equations 和纯 algebra equations。</li>
<li>results: 该方法可以保证解的 algebraic constraints 准确地满足，从而提高电路设计的精度和可靠性。<details>
<summary>Abstract</summary>
Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of tunable parameters that affect the final design leads to a need for new approaches of quantifying their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We aim to use the recently introduced dissection concept for DAEs that can decouple a given system into ordinary differential equations, only depending on differential variables, and purely algebraic equations that describe the relations between differential and algebraic variables. The idea then is to only learn the differential variables and reconstruct the algebraic ones using the relations from the decoupling. This approach guarantees that the algebraic constraints are fulfilled up to the accuracy of the nonlinear system solver, which represents the main benefit highlighted in this article.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Visual-Kinematics-Graph-Learning-for-Procedure-agnostic-Instrument-Tip-Segmentation-in-Robotic-Surgeries"><a href="#Visual-Kinematics-Graph-Learning-for-Procedure-agnostic-Instrument-Tip-Segmentation-in-Robotic-Surgeries" class="headerlink" title="Visual-Kinematics Graph Learning for Procedure-agnostic Instrument Tip Segmentation in Robotic Surgeries"></a>Visual-Kinematics Graph Learning for Procedure-agnostic Instrument Tip Segmentation in Robotic Surgeries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00957">http://arxiv.org/abs/2309.00957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaqi Liu, Yonghao Long, Kai Chen, Cheuk Hei Leung, Zerui Wang, Qi Dou</li>
<li>for: 用于Robotic surgery中的器官细部检测，如技能评估、工具-组织相互作用和形变建模，以及手术自动化。</li>
<li>methods: 利用机器人系统的遥感数据来提供可靠的先知，并通过图学学习框架对图像和遥感数据进行融合，以提高器官细部检测的精度。</li>
<li>results: 在一个私有的对比数据集上，包括多种手术类型，如肾脏切除术、全肠肉切除术、胃食道切除术和肝脏切除术，实现了与当前图像基的状态作呈现的超过11.2%的提升。<details>
<summary>Abstract</summary>
Accurate segmentation of surgical instrument tip is an important task for enabling downstream applications in robotic surgery, such as surgical skill assessment, tool-tissue interaction and deformation modeling, as well as surgical autonomy. However, this task is very challenging due to the small sizes of surgical instrument tips, and significant variance of surgical scenes across different procedures. Although much effort has been made on visual-based methods, existing segmentation models still suffer from low robustness thus not usable in practice. Fortunately, kinematics data from the robotic system can provide reliable prior for instrument location, which is consistent regardless of different surgery types. To make use of such multi-modal information, we propose a novel visual-kinematics graph learning framework to accurately segment the instrument tip given various surgical procedures. Specifically, a graph learning framework is proposed to encode relational features of instrument parts from both image and kinematics. Next, a cross-modal contrastive loss is designed to incorporate robust geometric prior from kinematics to image for tip segmentation. We have conducted experiments on a private paired visual-kinematics dataset including multiple procedures, i.e., prostatectomy, total mesorectal excision, fundoplication and distal gastrectomy on cadaver, and distal gastrectomy on porcine. The leave-one-procedure-out cross validation demonstrated that our proposed multi-modal segmentation method significantly outperformed current image-based state-of-the-art approaches, exceeding averagely 11.2% on Dice.
</details>
<details>
<summary>摘要</summary>
importante任务是将手术工具的 tip 分割，以实现后续应用程序在机器人手术中，如手术技巧评估、工具-组织交互和变形建模，以及手术自主。然而，这个任务非常具有挑战性，因为手术工具的 tip 很小，而手术场景也存在很大的差异。虽然已经有很多Visual基于的方法，但现有的分割模型仍然具有低稳定性，因此不可靠。幸运的是，机器人系统的动态数据可以提供可靠的工具位置的先导，这些先导是不同手术类型下的一致的。为了利用这些多modal信息，我们提议一种新的视觉-遥感图学学习框架，准确地分割手术工具的 tip。具体来说，我们提出了一个图学学习框架，用于编码工具部件的关系特征从视觉和遥感两个Modal。然后，我们设计了一种交叉模态对比损失函数，以汇集robust的几何先导从遥感到图像进行分割。我们在私有的Visual-遥感对应 dataset 上进行了实验，包括多种手术类型，如肾脏摘除、肠Rectal摘除、胃部切除和肠部切除。我们使用了离散一个手术程序的交叉验证，并表明我们的多模态分割方法在Dice指标上超过了当前图像基于状态艺术的方法的平均值11.2%。
</details></li>
</ul>
<hr>
<h2 id="From-Specific-to-Generic-Learned-Sorted-Set-Dictionaries-A-Theoretically-Sound-Paradigm-Yelding-Competitive-Data-Structural-Boosters-in-Practice"><a href="#From-Specific-to-Generic-Learned-Sorted-Set-Dictionaries-A-Theoretically-Sound-Paradigm-Yelding-Competitive-Data-Structural-Boosters-in-Practice" class="headerlink" title="From Specific to Generic Learned Sorted Set Dictionaries: A Theoretically Sound Paradigm Yelding Competitive Data Structural Boosters in Practice"></a>From Specific to Generic Learned Sorted Set Dictionaries: A Theoretically Sound Paradigm Yelding Competitive Data Structural Boosters in Practice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00946">http://arxiv.org/abs/2309.00946</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/globosco/An-implementation-of-Generic-Learned-Static-Sorted-Sets-Dictionaries">https://github.com/globosco/An-implementation-of-Generic-Learned-Static-Sorted-Sets-Dictionaries</a></li>
<li>paper_authors: Domenico Amato, Giosué Lo Bosco, Raffaele Giancarlo</li>
<li>for: 本研究探讨了机器学习和 классические数据结构之间的交叉领域，即学习数据结构。它具有方法学意义和实践效应。本文专注于学习排序数据结构，即学习排序集数据结构。现有提案主要是对已有排序数据结构进行加速，例如二分搜索。本文提出了一种新的思路，可以生成任何排序集数据结构的学习版本，例如平衡二分搜索树或其他排序layout的搜索。</li>
<li>methods: 本文使用了机器学习和排序数据结构的方法，包括学习排序集数据结构和搜索算法。</li>
<li>results: 本文获得了一些有趣的结果，包括首个学习最优二分搜索森林，其 mean access time  bounded by  Entropy of 访问频率分布。此外，本文还获得了首个学习排序集数据结构，在动态情况下和在准确分析设置下，与经典数据结构具有相同的时间 bound，这在广泛接受的宇宙大小假设下。实验部分表明，我们的总结可以生成有效和竞争力强的学习数据结构扩展器，即使与特定的参考模型相比。<details>
<summary>Abstract</summary>
This research concerns Learned Data Structures, a recent area that has emerged at the crossroad of Machine Learning and Classic Data Structures. It is methodologically important and with a high practical impact. We focus on Learned Indexes, i.e., Learned Sorted Set Dictionaries. The proposals available so far are specific in the sense that they can boost, indeed impressively, the time performance of Table Search Procedures with a sorted layout only, e.g., Binary Search. We propose a novel paradigm that, complementing known specialized ones, can produce Learned versions of any Sorted Set Dictionary, for instance, Balanced Binary Search Trees or Binary Search on layouts other that sorted, i.e., Eytzinger. Theoretically, based on it, we obtain several results of interest, such as (a) the first Learned Optimum Binary Search Forest, with mean access time bounded by the Entropy of the probability distribution of the accesses to the Dictionary; (b) the first Learned Sorted Set Dictionary that, in the Dynamic Case and in an amortized analysis setting, matches the same time bounds known for Classic Dictionaries. This latter under widely accepted assumptions regarding the size of the Universe. The experimental part, somewhat complex in terms of software development, clearly indicates the nonobvious finding that the generalization we propose can yield effective and competitive Learned Data Structural Booster, even with respect to specific benchmark models.
</details>
<details>
<summary>摘要</summary>
Theoretically, we obtain several interesting results, including:1. The first learned optimum binary search forest, with a mean access time bounded by the entropy of the probability distribution of the accesses to the dictionary.2. The first learned sorted set dictionary that, in the dynamic case and in an amortized analysis setting, matches the same time bounds as classic dictionaries, under widely accepted assumptions about the size of the universe.The experimental part of our research, which involved complex software development, revealed a non-obvious finding: our generalization can yield effective and competitive learned data structural boosters, even compared to specific benchmark models.
</details></li>
</ul>
<hr>
<h2 id="Pressmatch-Automated-journalist-recommendation-for-media-coverage-with-Nearest-Neighbor-search"><a href="#Pressmatch-Automated-journalist-recommendation-for-media-coverage-with-Nearest-Neighbor-search" class="headerlink" title="Pressmatch: Automated journalist recommendation for media coverage with Nearest Neighbor search"></a>Pressmatch: Automated journalist recommendation for media coverage with Nearest Neighbor search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00944">http://arxiv.org/abs/2309.00944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumya Parekh, Jay Patel</li>
<li>for: 这个研究的目的是提出一个模型，用于自动推荐适合的журналісти来推广发布的新闻稿。</li>
<li>methods: 这个研究使用了自然语言处理和机器学习技术，推荐журналісти基于新闻稿的内容和 журналіст的 beat 进行推广。</li>
<li>results: 研究发现，这个模型可以帮助传播人员更好地选择适合的журналісти，提高新闻稿的推广效果。<details>
<summary>Abstract</summary>
Slating a product for release often involves pitching journalists to run stories on your press release. Good media coverage often ensures greater product reach and drives audience engagement for those products. Hence, ensuring that those releases are pitched to the right journalists with relevant interests is crucial, since they receive several pitches daily. Keeping up with journalist beats and curating a media contacts list is often a huge and time-consuming task. This study proposes a model to automate and expedite the process by recommending suitable journalists to run media coverage on the press releases provided by the user.
</details>
<details>
<summary>摘要</summary>
通常，发布产品的过程中需要向记者推销新闻稿，以提高产品的曝光率和让宠物用户参与。因此，向正确的记者推销是非常重要的，因为他们每天收到许多推销。保持记者的 beat 和积累媒体联系人名单是一项巨大和耗时的任务。这项研究提出了一个模型，以自动和加速推销过程，推荐用户提供的新闻稿适合的记者。Note: "媒体联系人名单" (méidiā liánxīn rénmín) is a term used in Chinese to refer to a list of contacts in the media industry, such as journalists or bloggers.
</details></li>
</ul>
<hr>
<h2 id="Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models"><a href="#Emergent-Linear-Representations-in-World-Models-of-Self-Supervised-Sequence-Models" class="headerlink" title="Emergent Linear Representations in World Models of Self-Supervised Sequence Models"></a>Emergent Linear Representations in World Models of Self-Supervised Sequence Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00941">http://arxiv.org/abs/2309.00941</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ajyl/mech_int_othellogpt">https://github.com/ajyl/mech_int_othellogpt</a></li>
<li>paper_authors: Neel Nanda, Andrew Lee, Martin Wattenberg</li>
<li>for: This paper is written for understanding how sequence models represent their decision-making process, specifically in the context of the game Othello.</li>
<li>methods: The paper uses a probing approach to investigate the internal state of the model, and demonstrates that the model represents the board state in a linear rather than nonlinear way.</li>
<li>results: The paper shows that probing for “my colour” vs. “opponent’s colour” provides a simple yet powerful way to interpret the model’s internal state, and that this understanding allows for significant interpretability progress, including the ability to control the model’s behavior with simple vector arithmetic.Here’s the text in Simplified Chinese:</li>
<li>for: 这篇论文是为了理解序列模型如何做出决策的过程，具体来说是在游戏奥菲托尔中进行的。</li>
<li>methods: 这篇论文使用探测方法来研究模型的内部状态，并证明模型表示棋盘状态的方式是线性的，而不是非线性的。</li>
<li>results: 这篇论文显示，对”我的颜色”与”对手的颜色”进行探测提供了一个简单 yet 强大的方式来解释模型的内部状态，并且这种理解允许对模型的行为进行简单的 вектор数学操作。<details>
<summary>Abstract</summary>
How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate "How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed." into 中文（简体）Here's the translation:sequence models的决策过程是如何表示？前作者建议othello游戏神经网络学习了板状态的非线性模型（Li et al., 2023）。在这个工作中，我们提供了一种相关的直线表示，具体来说，我们发现了“我的颜色”vs.“对手的颜色”的探测可能是一种简单又强大的内部状态 интерпреタability的方法。这种精确的内部表示允许我们通过简单的向量数学控制模型的行为。直线表示带来了重要的可读性进步，我们通过进一步探索世界模型如何计算来证明这一点。
</details></li>
</ul>
<hr>
<h2 id="GBE-MLZSL-A-Group-Bi-Enhancement-Framework-for-Multi-Label-Zero-Shot-Learning"><a href="#GBE-MLZSL-A-Group-Bi-Enhancement-Framework-for-Multi-Label-Zero-Shot-Learning" class="headerlink" title="GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning"></a>GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00923">http://arxiv.org/abs/2309.00923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziming Liu, Jingcai Guo, Xiaocheng Lu, Song Guo, Peiran Dong, Jiewei Zhang</li>
<li>for: 本研究 investigate 一个 zero-shot learning 在多labelenario（MLZSL）中的挑战问题，即模型需要根据见到的类别和auxiliary知识，识别 sample 中的多个未看到类别。</li>
<li>methods: 本研究提出了一个 novel 和有效的集群强化框架（GBE-MLZSL），以充分利用当中的属性，实现更加精确和响广的数字semantic投射。特别是，将特征地图分为多个特征组，每个特征组可以独立地训练 Local Information Distinguishing Module（LID）以确保唯一性。另外，Global Enhancement Module（GEM）用于保持主要方向。此外，还设计了一个静态图strucuture来建立本地特征之间的相关。</li>
<li>results: 实验结果显示，提出的 GBE-MLZSL 方法在大规模 MLZSL 评量数据集 NUS-WIDE 和 Open-Images-v4 上，与其他现有的State-of-the-art方法之间存在大幅的优势。<details>
<summary>Abstract</summary>
This paper investigates a challenging problem of zero-shot learning in the multi-label scenario (MLZSL), wherein, the model is trained to recognize multiple unseen classes within a sample (e.g., an image) based on seen classes and auxiliary knowledge, e.g., semantic information. Existing methods usually resort to analyzing the relationship of various seen classes residing in a sample from the dimension of spatial or semantic characteristics, and transfer the learned model to unseen ones. But they ignore the effective integration of local and global features. That is, in the process of inferring unseen classes, global features represent the principal direction of the image in the feature space, while local features should maintain uniqueness within a certain range. This integrated neglect will make the model lose its grasp of the main components of the image. Relying only on the local existence of seen classes during the inference stage introduces unavoidable bias. In this paper, we propose a novel and effective group bi-enhancement framework for MLZSL, dubbed GBE-MLZSL, to fully make use of such properties and enable a more accurate and robust visual-semantic projection. Specifically, we split the feature maps into several feature groups, of which each feature group can be trained independently with the Local Information Distinguishing Module (LID) to ensure uniqueness. Meanwhile, a Global Enhancement Module (GEM) is designed to preserve the principal direction. Besides, a static graph structure is designed to construct the correlation of local features. Experiments on large-scale MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the proposed GBE-MLZSL outperforms other state-of-the-art methods with large margins.
</details>
<details>
<summary>摘要</summary>
Existing methods often analyze the relationship between various seen classes within a sample from the perspectives of spatial or semantic characteristics, and then transfer the learned model to unseen classes. However, these methods tend to ignore the effective integration of local and global features. In other words, they do not fully utilize the principal direction of the image in the feature space, nor do they maintain the uniqueness of local features within a certain range. This integration neglect can cause the model to lose its grasp of the main components of the image.Furthermore, relying solely on the local existence of seen classes during the inference stage can introduce unavoidable bias. To address these issues, this paper proposes a novel and effective group bi-enhancement framework for MLZSL, called GBE-MLZSL. This framework splits the feature maps into several feature groups, each of which can be trained independently with the Local Information Distinguishing Module (LID) to ensure uniqueness. Additionally, a Global Enhancement Module (GEM) is designed to preserve the principal direction. A static graph structure is also designed to construct the correlation of local features.Experiments on large-scale MLZSL benchmark datasets NUS-WIDE and Open-Images-v4 demonstrate that the proposed GBE-MLZSL outperforms other state-of-the-art methods with large margins.
</details></li>
</ul>
<hr>
<h2 id="A-novel-framework-employing-deep-multi-attention-channels-network-for-the-autonomous-detection-of-metastasizing-cells-through-fluorescence-microscopy"><a href="#A-novel-framework-employing-deep-multi-attention-channels-network-for-the-autonomous-detection-of-metastasizing-cells-through-fluorescence-microscopy" class="headerlink" title="A novel framework employing deep multi-attention channels network for the autonomous detection of metastasizing cells through fluorescence microscopy"></a>A novel framework employing deep multi-attention channels network for the autonomous detection of metastasizing cells through fluorescence microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00911">http://arxiv.org/abs/2309.00911</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michail Mamalakis, Sarah C. Macfarlane, Scott V. Notley, Annica K. B Gad, George Panoutsos</li>
<li>for: 分辨normal和恶性细胞的大规模计算机影像框架</li>
<li>methods: 结合多个注意通道网络和全球可解释技术</li>
<li>results: 实现了分辨normal和恶性细胞的高精度分类，并提供了可解释的全球权重平均值和局部GradCam得分In simpler English:</li>
<li>for: Developing a computer vision framework to distinguish between normal and cancerous cells</li>
<li>methods: Combining multi-attention channels and global explainable techniques</li>
<li>results: Achieving high-precision classification and providing interpretable global weighted mean and local GradCam scoresNote: “GradCam” is short for “gradients of class activations,” which is a technique used to visualize the internal workings of a deep learning model.<details>
<summary>Abstract</summary>
We developed a transparent computational large-scale imaging-based framework that can distinguish between normal and metastasizing human cells. The method relies on fluorescence microscopy images showing the spatial organization of actin and vimentin filaments in normal and metastasizing single cells, using a combination of multi-attention channels network and global explainable techniques. We test a classification between normal cells (Bj primary fibroblast), and their isogenically matched, transformed and invasive counterpart (BjTertSV40TRasV12). Manual annotation is not trivial to automate due to the intricacy of the biologically relevant features. In this research, we utilized established deep learning networks and our new multi-attention channel architecture. To increase the interpretability of the network - crucial for this application area - we developed an interpretable global explainable approach correlating the weighted geometric mean of the total cell images and their local GradCam scores. The significant results from our analysis unprecedently allowed a more detailed, and biologically relevant understanding of the cytoskeletal changes that accompany oncogenic transformation of normal to invasive and metastasizing cells. We also paved the way for a possible spatial micrometre-level biomarker for future development of diagnostic tools against metastasis (spatial distribution of vimentin).
</details>
<details>
<summary>摘要</summary>
Translation notes:* "computational large-scale imaging-based framework" becomes "计算机大规模成像基础框架" (jìsuànjí móshì zhìyì)* "normal and metastasizing human cells" becomes "正常和肿瘤人类细胞" (zhèngcháng hé jìngzhì rénlèi xiǎoquǎi)* "fluorescence microscopy images" becomes "抗体染料显微镜像" (kǎngshì ránliàng jiǎngwēi yǐngxìng)* "spatial organization of actin and vimentin filaments" becomes "Actin和维门铁线粒的空间组织" (Actin hé wéimén tiě tiáo de kōngjiān zhìsuǒ)* "multi-attention channels network" becomes "多通道注意网络" (duō tōngcháng zhùyì wǎngluò)* "global explainable techniques" becomes "全球可解释技术" (quánqiú kějiejiě bìngjiè)* "Bj primary fibroblast" becomes "Bj主细胞" (Bj zhōng xiǎoquǎi)* "BjTertSV40TRasV12" becomes "BjTertSV40TRasV12" (BjTertSV40TRasV12)* "manual annotation" becomes "手动标注" (shǒudòng biāo zhù)* "interpretable global explainable approach" becomes "可解释全球方法" (kějiejiě quánqiú fāngzhì)* "weighted geometric mean" becomes "加权地理均值" (jiāwù dì lǐjìn zhì)* "local GradCam scores" becomes "本地GradCam分数" (běn dì GradCam fēnshù)
</details></li>
</ul>
<hr>
<h2 id="A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading"><a href="#A-Multi-Head-Ensemble-Multi-Task-Learning-Approach-for-Dynamical-Computation-Offloading" class="headerlink" title="A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading"></a>A Multi-Head Ensemble Multi-Task Learning Approach for Dynamical Computation Offloading</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00907">http://arxiv.org/abs/2309.00907</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihuai Liang, Bo Yang, Zhiwen Yu, Xuelin Cao, Derrick Wing Kwan Ng, Chau Yuen</li>
<li>For: 提高移动边缘计算（MEC）性能，设计优化的卸载策略，包括卸载决策和计算资源分配。* Methods: 使用混合整数非线性程序（MINLP）问题的解决方法，通过在训练了深度神经网络（DNN）模型后进行在线推理来获得有效解决方案。* Results: 提出了一种多头集成多任务学习（MEMTL）方法，可以减少训练开销并提高推理性能，使得在时变系统环境下可以有效地解决卸载决策和资源分配问题。实验结果表明，相比基准方法，提出的MEMTL方法在推理准确率和平均方差Error中均表现出色，不需要额外的训练数据。<details>
<summary>Abstract</summary>
Computation offloading has become a popular solution to support computationally intensive and latency-sensitive applications by transferring computing tasks to mobile edge servers (MESs) for execution, which is known as mobile/multi-access edge computing (MEC). To improve the MEC performance, it is required to design an optimal offloading strategy that includes offloading decision (i.e., whether offloading or not) and computational resource allocation of MEC. The design can be formulated as a mixed-integer nonlinear programming (MINLP) problem, which is generally NP-hard and its effective solution can be obtained by performing online inference through a well-trained deep neural network (DNN) model. However, when the system environments change dynamically, the DNN model may lose efficacy due to the drift of input parameters, thereby decreasing the generalization ability of the DNN model. To address this unique challenge, in this paper, we propose a multi-head ensemble multi-task learning (MEMTL) approach with a shared backbone and multiple prediction heads (PHs). Specifically, the shared backbone will be invariant during the PHs training and the inferred results will be ensembled, thereby significantly reducing the required training overhead and improving the inference performance. As a result, the joint optimization problem for offloading decision and resource allocation can be efficiently solved even in a time-varying wireless environment. Experimental results show that the proposed MEMTL outperforms benchmark methods in both the inference accuracy and mean square error without requiring additional training data.
</details>
<details>
<summary>摘要</summary>
computation offloading 已成为支持 computationally intensive 和延迟敏感应用的受欢迎解决方案，通过将计算任务传输到 mobil edge server (MES) 进行执行，这称为 mobil/多接入边计算 (MEC)。为了提高 MEC 性能，需要设计优化的 offloading 策略，包括 offloading 决策（是否 offloading）和 MEC 的计算资源分配。该设计可以表示为混合整数非线性计划 (MINLP) 问题，通常是NP困难的，其有效解决方式可以通过在训练好的深度神经网络 (DNN) 模型上进行在线推理来获得。然而，当系统环境变化 dynamically 时，DNN 模型可能会失去有效性，因为输入参数的漂移，从而降低 DNN 模型的通用能力。为解决这个特殊挑战，在这篇论文中，我们提出了一种多头集成多任务学习 (MEMTL) 方法，其中包括一个共享基础和多个预测头 (PH)。具体来说，共享基础将在 PH 训练期间保持不变，并将推理结果 ensemble，从而减少了训练过程的必要时间和提高推理性能。因此，可以有效地解决 joint optimization 问题，即 offloading 决策和资源分配，即使在时变无线电环境中。实验结果表明，我们提出的 MEMTL 方法在推理精度和时间平均误差方面具有明显的优势，而不需要额外的训练数据。
</details></li>
</ul>
<hr>
<h2 id="Regularly-Truncated-M-estimators-for-Learning-with-Noisy-Labels"><a href="#Regularly-Truncated-M-estimators-for-Learning-with-Noisy-Labels" class="headerlink" title="Regularly Truncated M-estimators for Learning with Noisy Labels"></a>Regularly Truncated M-estimators for Learning with Noisy Labels</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00894">http://arxiv.org/abs/2309.00894</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaoboxia/rtm_lnl">https://github.com/xiaoboxia/rtm_lnl</a></li>
<li>paper_authors: Xiaobo Xia, Pengqian Lu, Chen Gong, Bo Han, Jun Yu, Jun Yu, Tongliang Liu</li>
<li>For: The paper is written to address the issues of noisy labels in deep learning, specifically the problem of selecting small-loss examples that may be contaminated with noise.* Methods: The paper proposes a regularly truncated M-estimator (RTME) method that alternates between truncated M-estimators and original M-estimators to adaptively select small-loss examples and reduce the side-effects of noisy labels.* Results: The paper demonstrates the label-noise-tolerance of the proposed method through theoretical analysis and comprehensive experimental results, showing that it can outperform multiple baselines and is robust to broad noise types and levels.Here is the same information in Simplified Chinese text:* For: 本文是为了解决深度学习中的噪声标签问题，具体来说是选择小损失示例的问题。* Methods: 本文提出了一种常见跳转M-估计（RTME）方法，该方法可以逐渐地选择小损失示例，并避免噪声标签的影响。* Results: 本文通过理论分析和广泛的实验结果验证了方法的噪声标签忍受性，并证明了它可以超过多个基线，并且对噪声类型和水平进行了广泛的验证。<details>
<summary>Abstract</summary>
The sample selection approach is very popular in learning with noisy labels. As deep networks learn pattern first, prior methods built on sample selection share a similar training procedure: the small-loss examples can be regarded as clean examples and used for helping generalization, while the large-loss examples are treated as mislabeled ones and excluded from network parameter updates. However, such a procedure is arguably debatable from two folds: (a) it does not consider the bad influence of noisy labels in selected small-loss examples; (b) it does not make good use of the discarded large-loss examples, which may be clean or have meaningful information for generalization. In this paper, we propose regularly truncated M-estimators (RTME) to address the above two issues simultaneously. Specifically, RTME can alternately switch modes between truncated M-estimators and original M-estimators. The former can adaptively select small-losses examples without knowing the noise rate and reduce the side-effects of noisy labels in them. The latter makes the possibly clean examples but with large losses involved to help generalization. Theoretically, we demonstrate that our strategies are label-noise-tolerant. Empirically, comprehensive experimental results show that our method can outperform multiple baselines and is robust to broad noise types and levels.
</details>
<details>
<summary>摘要</summary>
“ sample selection approach 非常受欢迎在听过噪音标签的学习中。深度网络会在首先学习 Pattern，因此先前的方法在 sample selection 上共享类似的训练过程：小损例可以被视为干净的例子，并用于帮助泛化，而大损例则被排除在网络参数更新中。然而，这种过程存在两点问题：（a）它不考虑随选小损例中噪音标签的坏影响；（b）它不好用释放的大损例，它们可能是干净的或具有泛化信息。在这篇论文中，我们提出了常见落实 truncated M-estimators（RTME）来解决以上两个问题。具体来说，RTME可以 alternate switching  между truncated M-estimators 和原始 M-estimators。前者可以适应ively 选择小损例，无需知道噪音率，并减少随选小损例中噪音标签的副作用。后者使得可能是干净的，但有大损例参与到泛化中。我们理论上表明了我们的策略是噪音抗性的。实验结果表明，我们的方法可以超越多个基elines，并在各种噪音类型和水平上表现稳定。”
</details></li>
</ul>
<hr>
<h2 id="Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers"><a href="#Discovering-Predictive-Relational-Object-Symbols-with-Symbolic-Attentive-Layers" class="headerlink" title="Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers"></a>Discovering Predictive Relational Object Symbols with Symbolic Attentive Layers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00889">http://arxiv.org/abs/2309.00889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alper Ahmetoglu, Batuhan Celik, Erhan Oztop, Emre Ugur</li>
<li>for: 本研究提出了一种新的深度学习架构，用于在表格环境中发现对象和其关系的符号表示。这种架构可以自动处理不同数量的对象，并将对象之间的关系映射到符号领域。</li>
<li>methods: 本研究使用了一个自注意层，计算对象特征中的抽象注意力权重，并将这些注意力权重用于对象符号的汇集和动作效果预测。</li>
<li>results: 实验结果显示，提出的架构在预测动作效果的同时，自动形成了对象符号和关系符号，并与其他基eline方法相比，表现更好。此外，分析表明，学习的符号与对象之间的相对位置、对象类型和桌面上的水平Alignment有关。<details>
<summary>Abstract</summary>
In this paper, we propose and realize a new deep learning architecture for discovering symbolic representations for objects and their relations based on the self-supervised continuous interaction of a manipulator robot with multiple objects on a tabletop environment. The key feature of the model is that it can handle a changing number number of objects naturally and map the object-object relations into symbolic domain explicitly. In the model, we employ a self-attention layer that computes discrete attention weights from object features, which are treated as relational symbols between objects. These relational symbols are then used to aggregate the learned object symbols and predict the effects of executed actions on each object. The result is a pipeline that allows the formation of object symbols and relational symbols from a dataset of object features, actions, and effects in an end-to-end manner. We compare the performance of our proposed architecture with state-of-the-art symbol discovery methods in a simulated tabletop environment where the robot needs to discover symbols related to the relative positions of objects to predict the observed effect successfully. Our experiments show that the proposed architecture performs better than other baselines in effect prediction while forming not only object symbols but also relational symbols. Furthermore, we analyze the learned symbols and relational patterns between objects to learn about how the model interprets the environment. Our analysis shows that the learned symbols relate to the relative positions of objects, object types, and their horizontal alignment on the table, which reflect the regularities in the environment.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出并实现了一种新的深度学习架构，用于发现对象和它们之间关系的符号表示。这种模型可以自动处理变化的对象数量，并将对象之间关系显式地映射到符号领域。在模型中，我们使用了一层自注意层，计算对象特征中的不同注意力权重，这些注意力权重被视为对象之间关系的符号。这些符号然后用于聚合学习的对象符号和预测每个对象的影响。结果是一个可以从对象特征、动作和效果的数据集中形成对象符号和关系符号的端到端管道。我们与现状顶尖的符号发现方法进行比较，在模拟的桌面环境中， robot需要通过发现对象的相对位置来预测观察到的效果。我们的实验表明，我们提出的架构在效果预测中表现更好于其他基elines，同时不仅形成对象符号，还形成了关系符号。此外，我们分析了学习的符号和对象之间的关系律pattern，以了解模型如何理解环境。我们的分析显示，学习的符号与对象的相对位置、对象类型和桌面上的水平对齐有关，这些Regularities在环境中。
</details></li>
</ul>
<hr>
<h2 id="Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy"><a href="#Tight-Bounds-for-Machine-Unlearning-via-Differential-Privacy" class="headerlink" title="Tight Bounds for Machine Unlearning via Differential Privacy"></a>Tight Bounds for Machine Unlearning via Differential Privacy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00886">http://arxiv.org/abs/2309.00886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiyang Huang, Clément L. Canonne</li>
<li>for: 这篇论文是关于机器学习”忘记”的研究，即要求训练过的模型在请求时能够”忘记”一些训练数据点，作为如果它们从来没有被包含在模型中一样。</li>
<li>methods: 作者使用了差异隐私（DP）算法来实现机器”忘记”。</li>
<li>results: 作者完全关闭了DP算法实现机器”忘记”的上下限，获得了DP算法实现的最佳 deletion capacity。<details>
<summary>Abstract</summary>
We consider the formulation of "machine unlearning" of Sekhari, Acharya, Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called "right to be forgotten" by requiring that a trained model, upon request, should be able to "unlearn" a number of points from the training data, as if they had never been included in the first place. Sekhari et al. established some positive and negative results about the number of data points that can be successfully unlearnt by a trained model without impacting the model's accuracy (the "deletion capacity"), showing that machine unlearning could be achieved by using differentially private (DP) algorithms. However, their results left open a gap between upper and lower bounds on the deletion capacity of these algorithms: our work fully closes this gap, obtaining tight bounds on the deletion capacity achievable by DP-based machine unlearning algorithms.
</details>
<details>
<summary>摘要</summary>
我团队考虑了谢卡里、阿查纳、卡马斯和苏瑞希（NeurIPS 2021）所提出的机器“忘记” formalization，即要求已经训练过的模型，在请求的情况下，能够“忘记”一些训练数据点，好像它们从来没有被包含在内。谢卡里等人提出了一些积分和负积分结果，证明了机器“忘记”可以通过使用具有隐私保护（DP）算法来实现。然而，他们的结果留下了对删除容量的Upper和Lower bound的 gap：我们的工作完全关闭了这个差距，得到了DP基于机器“忘记”算法的准确 delete capacity 的紧跟 bounds。
</details></li>
</ul>
<hr>
<h2 id="A-Generic-Fundus-Image-Enhancement-Network-Boosted-by-Frequency-Self-supervised-Representation-Learning"><a href="#A-Generic-Fundus-Image-Enhancement-Network-Boosted-by-Frequency-Self-supervised-Representation-Learning" class="headerlink" title="A Generic Fundus Image Enhancement Network Boosted by Frequency Self-supervised Representation Learning"></a>A Generic Fundus Image Enhancement Network Boosted by Frequency Self-supervised Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00885">http://arxiv.org/abs/2309.00885</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng Li, Haofeng Liu, Huazhu Fu, Yanwu Xu, Hui Shu, Ke Niu, Yan Hu, Jiang Liu</li>
<li>for:  This paper aims to develop a generic fundus image enhancement network (GFE-Net) to correct degraded fundus images without supervised or extra data, which can be used for clinical examination and intelligent systems.</li>
<li>methods: The proposed GFE-Net leverages image frequency information and self-supervised representation learning to learn robust structure-aware representations from degraded images, and then couples representation learning and image enhancement to accurately correct fundus images while preserving retinal structures.</li>
<li>results: Compared with state-of-the-art algorithms, GFE-Net achieves superior performance in data dependency, enhancement performance, deployment efficiency, and scale generalizability, and its modules are also verified to be effective for image enhancement.<details>
<summary>Abstract</summary>
Fundus photography is prone to suffer from image quality degradation that impacts clinical examination performed by ophthalmologists or intelligent systems. Though enhancement algorithms have been developed to promote fundus observation on degraded images, high data demands and limited applicability hinder their clinical deployment. To circumvent this bottleneck, a generic fundus image enhancement network (GFE-Net) is developed in this study to robustly correct unknown fundus images without supervised or extra data. Levering image frequency information, self-supervised representation learning is conducted to learn robust structure-aware representations from degraded images. Then with a seamless architecture that couples representation learning and image enhancement, GFE-Net can accurately correct fundus images and meanwhile preserve retinal structures. Comprehensive experiments are implemented to demonstrate the effectiveness and advantages of GFE-Net. Compared with state-of-the-art algorithms, GFE-Net achieves superior performance in data dependency, enhancement performance, deployment efficiency, and scale generalizability. Follow-up fundus image analysis is also facilitated by GFE-Net, whose modules are respectively verified to be effective for image enhancement.
</details>
<details>
<summary>摘要</summary>
血管照片受到影像质量削弱的影响，对于医生或智能系统进行临床评估。虽然有增强算法来提高血管观察，但高数据需求和有限的应用限制了其临床部署。为绕过这个瓶颈，本研究提出了一种通用血管图像增强网络（GFE-Net），可以无监督地修复不知道的血管图像。利用图像频率信息，GFE-Net通过自我监督学习来学习血管图像中的结构信息。然后，GFE-Net通过将表征学习和图像增强结合在一起，可以准确地修复血管图像，同时保持视网膜结构。我们进行了广泛的实验，以证明GFE-Net的有效性和优势。相比之前的算法，GFE-Net在数据依赖度、增强性、部署效率和扩展可行性等方面表现出色。此外，GFE-Net的模块也在各种图像增强任务中进行了验证，并证明了其效果。
</details></li>
</ul>
<hr>
<h2 id="Towards-Certified-Probabilistic-Robustness-with-High-Accuracy"><a href="#Towards-Certified-Probabilistic-Robustness-with-High-Accuracy" class="headerlink" title="Towards Certified Probabilistic Robustness with High Accuracy"></a>Towards Certified Probabilistic Robustness with High Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00879">http://arxiv.org/abs/2309.00879</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruihan Zhang, Peixin Zhang, Jun Sun</li>
<li>for: 本研究旨在提出一种可以同时实现高精度和证明性的神经网络模型。</li>
<li>methods: 本方法包括两部分：一是一种 probabilistic robust training 方法，它的目标是在随机扰动下减少模型的偏差变化，二是一种runtime inference方法，它可以在运行时提供证明性的 garantía。</li>
<li>results: 对多种扰动进行测试，本方法可以显著超越现有方法， both in terms of certification rate and accuracy。<details>
<summary>Abstract</summary>
Adversarial examples pose a security threat to many critical systems built on neural networks (such as face recognition systems, and self-driving cars). While many methods have been proposed to build robust models, how to build certifiably robust yet accurate neural network models remains an open problem. For example, adversarial training improves empirical robustness, but they do not provide certification of the model's robustness. On the other hand, certified training provides certified robustness but at the cost of a significant accuracy drop. In this work, we propose a novel approach that aims to achieve both high accuracy and certified probabilistic robustness. Our method has two parts, i.e., a probabilistic robust training method with an additional goal of minimizing variance in terms of divergence and a runtime inference method for certified probabilistic robustness of the prediction. The latter enables efficient certification of the model's probabilistic robustness at runtime with statistical guarantees. This is supported by our training objective, which minimizes the variance of the model's predictions in a given vicinity, derived from a general definition of model robustness. Our approach works for a variety of perturbations and is reasonably efficient. Our experiments on multiple models trained on different datasets demonstrate that our approach significantly outperforms existing approaches in terms of both certification rate and accuracy.
</details>
<details>
<summary>摘要</summary>
“对于多种攻击性应用（如识别 face recognition 系统和自驾车），对于内置 ней拥有数据的系统而言，防御性攻击是一个开启的问题。许多方法已经被提出来建立坚固的模型，但是如何建立认证可靠的坚固模型仍然是一个开启的问题。例如，这些模型可以通过防御训练提高实际的防御性，但是它们不会提供模型的认证可靠性。另一方面，认证训练可以提供认证可靠性，但是它们将会导致模型的准确性下降。在这个工作中，我们提出了一个新的方法，旨在实现高准确性和认证可靠性。我们的方法有两个部分：一个是一种具有额外目标的机会均衡训练方法，另一个是一种在Runtime inference的方法，用于认证模型的 probabilistic 可靠性。这个方法可以实现在多种攻击下进行认证，并且相对高效。我们的实验结果显示，我们的方法在多个模型和不同的数据集上具有较高的认证率和准确性。”
</details></li>
</ul>
<hr>
<h2 id="Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning"><a href="#Pretraining-Representations-for-Bioacoustic-Few-shot-Detection-using-Supervised-Contrastive-Learning" class="headerlink" title="Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning"></a>Pretraining Representations for Bioacoustic Few-shot Detection using Supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00878">http://arxiv.org/abs/2309.00878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ilyass Moummad, Romain Serizel, Nicolas Farrugia</li>
<li>for: 这个研究是为了解决 bioacoustic 应用中的声事件探测和分类问题，因为这些任务通常具有寥寥无几的标签训练数据，导致超级学习不是最佳的方法。</li>
<li>methods: 这个研究使用了一个自然语言处理框架，将问题转换为 few-shot 学习问题，即将仅有五个标签的训练数据用于训练系统。</li>
<li>results: 这个研究获得了一个 F-score 的63.46% 和 42.7% 在验证和测试集中，在 DCASE 挑战中排名第二。研究还进行了一个删除研究的研究，以了解哪些数据增强技术和学习策略在训练集中具有最大的影响。<details>
<summary>Abstract</summary>
Deep learning has been widely used recently for sound event detection and classification. Its success is linked to the availability of sufficiently large datasets, possibly with corresponding annotations when supervised learning is considered. In bioacoustic applications, most tasks come with few labelled training data, because annotating long recordings is time consuming and costly. Therefore supervised learning is not the best suited approach to solve bioacoustic tasks. The bioacoustic community recasted the problem of sound event detection within the framework of few-shot learning, i.e. training a system with only few labeled examples. The few-shot bioacoustic sound event detection task in the DCASE challenge focuses on detecting events in long audio recordings given only five annotated examples for each class of interest. In this paper, we show that learning a rich feature extractor from scratch can be achieved by leveraging data augmentation using a supervised contrastive learning framework. We highlight the ability of this framework to transfer well for five-shot event detection on previously unseen classes in the training data. We obtain an F-score of 63.46\% on the validation set and 42.7\% on the test set, ranking second in the DCASE challenge. We provide an ablation study for the critical choices of data augmentation techniques as well as for the learning strategy applied on the training set.
</details>
<details>
<summary>摘要</summary>
深度学习在近期内广泛应用于声音事件检测和分类。其成功与具有足够大的数据集，可能有相应的注释时进行监督学习的可用性相关。在生物声学应用中，大多数任务都具有少量标注训练数据，因为标注长录音是时间consuming和成本高的。因此，监督学习不是解决生物声学任务的最佳方法。生物声学社区将声音事件检测问题划入了少量学习框架中，即在只有几个标注示例的情况下训练一个系统。DCASE挑战中的声音事件检测任务是在长 audio recording 上检测事件，只需要提供每个类型的五个标注示例。在这篇文章中，我们表明了可以通过利用数据扩充和监督对比学习框架来学习 Rich feature extractor 从头来。我们强调了这个框架在五个事件检测中的传输性能。我们在验证集上获得了63.46%的 F-score，在测试集上获得了42.7%的 F-score，在DCASE挑战中排名第二。我们对数据扩充技术的关键选择以及在训练集上应用的学习策略进行了ablation研究。
</details></li>
</ul>
<hr>
<h2 id="Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models"><a href="#Tutorial-a-priori-estimation-of-sample-size-effect-size-and-statistical-power-for-cluster-analysis-latent-class-analysis-and-multivariate-mixture-models" class="headerlink" title="Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models"></a>Tutorial: a priori estimation of sample size, effect size, and statistical power for cluster analysis, latent class analysis, and multivariate mixture models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00866">http://arxiv.org/abs/2309.00866</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/esdalmaijer/cluster_power_tutorial">https://github.com/esdalmaijer/cluster_power_tutorial</a></li>
<li>paper_authors: Edwin S Dalmaijer</li>
<li>for: 这个论文的目的是为了帮助研究者在进行分组分析时确定样本大小和影响大小。</li>
<li>methods: 这个论文使用了一种新的方法来计算样本大小和影响大小，这种方法可以帮助研究者在分组分析中更好地计算统计力和影响大小。</li>
<li>results: 这个论文通过使用 simulations 提供了一个参考表，用于帮助研究者在不同的分组分析方法下确定样本大小和影响大小，以确保他们的研究具有足够的统计力。<details>
<summary>Abstract</summary>
Before embarking on data collection, researchers typically compute how many individual observations they should do. This is vital for doing studies with sufficient statistical power, and often a cornerstone in study pre-registrations and grant applications. For traditional statistical tests, one would typically determine an acceptable level of statistical power, (gu)estimate effect size, and then use both values to compute the required sample size. However, for analyses that identify subgroups, statistical power is harder to establish. Once sample size reaches a sufficient threshold, effect size is primarily determined by the number of measured features and the underlying subgroup separation. As a consequence, a priory computations of statistical power are notoriously complex. In this tutorial, I will provide a roadmap to determining sample size and effect size for analyses that identify subgroups. First, I introduce a procedure that allows researchers to formalise their expectations about effect sizes in their domain of choice, and use this to compute the minimally required number of measured variables. Next, I outline how to establish the minimum sample size in subgroup analyses. Finally, I use simulations to provide a reference table for the most popular subgroup analyses: k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modelling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) to achieve acceptable statistical power, and can be readily used in study design.
</details>
<details>
<summary>摘要</summary>
In this tutorial, I will provide a step-by-step guide to determining sample size and effect size for analyses that identify subgroups. First, I will introduce a procedure that allows researchers to formalize their expectations about effect sizes in their domain of choice, and use this to compute the minimum number of measured variables required. Next, I will outline how to establish the minimum sample size in subgroup analyses. Finally, I will use simulations to provide a reference table for the most popular subgroup analyses, including k-means, Ward agglomerative hierarchical clustering, c-means fuzzy clustering, latent class analysis, latent profile analysis, and Gaussian mixture modeling. The table shows the minimum numbers of observations per expected subgroup (sample size) and features (measured variables) needed to achieve acceptable statistical power, and can be readily used in study design.
</details></li>
</ul>
<hr>
<h2 id="Equitable-FL-Federated-Learning-with-Sparsity-for-Resource-Constrained-Environment"><a href="#Equitable-FL-Federated-Learning-with-Sparsity-for-Resource-Constrained-Environment" class="headerlink" title="Equitable-FL: Federated Learning with Sparsity for Resource-Constrained Environment"></a>Equitable-FL: Federated Learning with Sparsity for Resource-Constrained Environment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00864">http://arxiv.org/abs/2309.00864</a></li>
<li>repo_url: None</li>
<li>paper_authors: Indrajeet Kumar Sinha, Shekhar Verma, Krishna Pratap Singh</li>
<li>for: 该论文旨在提出一种在资源受限环境中进行联合学习的方法，以确保学习可行性，不 mater nodes的资源充足程度。</li>
<li>methods: 该方法基于 Lottery Ticket Hypothesis approach，通过逐步减少模型参数量来鼓励有资源匮乏的节点参与协同训练。</li>
<li>results: 实验结果表明，Equitable-FL 在 $MNIST$, $F-MNIST$, $CIFAR-10$  benchmark 数据集以及 $Brain-MRI$ 数据集和 $PlantVillage$ 数据集上具有良好的效果，可以在各种资源受限环境下进行快速、高效的联合学习。<details>
<summary>Abstract</summary>
In Federated Learning, model training is performed across multiple computing devices, where only parameters are shared with a common central server without exchanging their data instances. This strategy assumes abundance of resources on individual clients and utilizes these resources to build a richer model as user's models. However, when the assumption of the abundance of resources is violated, learning may not be possible as some nodes may not be able to participate in the process. In this paper, we propose a sparse form of federated learning that performs well in a Resource Constrained Environment. Our goal is to make learning possible, regardless of a node's space, computing, or bandwidth scarcity. The method is based on the observation that model size viz a viz available resources defines resource scarcity, which entails that reduction of the number of parameters without affecting accuracy is key to model training in a resource-constrained environment. In this work, the Lottery Ticket Hypothesis approach is utilized to progressively sparsify models to encourage nodes with resource scarcity to participate in collaborative training. We validate Equitable-FL on the $MNIST$, $F-MNIST$, and $CIFAR-10$ benchmark datasets, as well as the $Brain-MRI$ data and the $PlantVillage$ datasets. Further, we examine the effect of sparsity on performance, model size compaction, and speed-up for training. Results obtained from experiments performed for training convolutional neural networks validate the efficacy of Equitable-FL in heterogeneous resource-constrained learning environment.
</details>
<details>
<summary>摘要</summary>
在联合学习中，模型训练通常会在多个计算设备上进行，只是共享参数而不是数据实例。这种策略假设每个客户端都有充足的资源，并利用这些资源建立更加丰富的模型。然而，当资源的充足假设被违反时，学习可能无法进行，因为一些节点可能无法参与过程中。在这篇论文中，我们提出了一种稀缺形式的联合学习方法，可以在资源受限环境中进行学习。我们的目标是让学习不受节点的资源限制，而是通过减少参数的数量来实现模型训练。我们利用了抽奖假设方法，逐步减少模型的参数，以便鼓励具有资源缺乏的节点参与合作训练。我们在$MNIST$, $F-MNIST$, $CIFAR-10$数据集上进行了实验，以及$Brain-MRI$数据集和$PlantVillage$数据集。此外，我们还研究了稀缺度对性能、模型大小压缩和训练速度的影响。实验结果表明，Equitable-FL在不同资源环境中进行联合学习时具有有效性。
</details></li>
</ul>
<hr>
<h2 id="DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal"><a href="#DoRA-Domain-Based-Self-Supervised-Learning-Framework-for-Low-Resource-Real-Estate-Appraisal" class="headerlink" title="DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal"></a>DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00855">http://arxiv.org/abs/2309.00855</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wwweiwei/dora">https://github.com/wwweiwei/dora</a></li>
<li>paper_authors: Wei-Wei Du, Wei-Yao Wang, Wen-Chih Peng</li>
<li>for: 本研究旨在开发一种具有偏好抑制的自动化评估模型，以便在金融机构进行高成本财产评估任务中减少域外专家的主观干预。</li>
<li>methods: 本研究提出了一种域基自动学习框架（DoRA），通过在它的预测任务中使用地理 metadata 来帮助财产表示具有域知识。此外，研究还使用了间隔样本学习来使 representations 更加抗性和可靠。</li>
<li>results: 对于三种实际交易的财产类型，DoRA 在少量交易enario中Significantly 超越了基于标签数据的 SSL 基elines、图形基本方法和经验法。DoRA 在 MAPE、MAE 和 HR10 等指标上提高了至少 7.6%、11.59% 和 3.34%。<details>
<summary>Abstract</summary>
The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an intra-sample geographic prediction as the pretext task based on the metadata of the real estate for equipping the real estate representations with prior domain knowledge. Furthermore, inter-sample contrastive learning is employed to generalize the representations to be robust for limited transactions of downstream tasks. Our benchmark results on three property types of real-world transactions show that DoRA significantly outperforms the SSL baselines for tabular data, the graph-based methods, and the supervised approaches in the few-shot scenarios by at least 7.6% for MAPE, 11.59% for MAE, and 3.34% for HR10%. We expect DoRA to be useful to other financial practitioners with similar marketplace applications who need general models for properties that are newly built and have limited records. The source code is available at https://github.com/wwweiwei/DoRA.
</details>
<details>
<summary>摘要</summary>
市场系统连接需求和供应，以开发不受偏见的决策方法。房地产评估作为高成本房产评估任务，需要领域专家根据相应的知识和市场判断来评估估价。现有的自动评估模型可以减少领域专家的主观性，但它们需要大量的交易数据进行有效评估，这主要受到交易标注的努力和新发展区域的通用性的限制。为了学习不标注的房地产集合中的表示，现有的自动学习（SSL） для表格数据忽略了许多重要特征，并且无法包含领域知识。本文提出了DoRA，一个基于领域的自动学习框架，用于低资源房地产评估。DoRA通过Metadata中的房地产数据进行内样地图预测作为预texte任务，以具备房地产表示的先前领域知识。此外，DoRA还使用了间样对比学习来普适表示，以便在有限交易情况下具备抗衰偏见能力。我们对实际交易中的三种不同类型的财产进行了比较，结果显示DoRA在少量enario下至少比SSL基线、图形基本方法和批处方法高出7.6%、11.59%和3.34%。我们预期DoRA将对其他金融实践者有用，他们需要对新建和有限纪录的财产进行通用的模型。代码可以在https://github.com/wwweiwei/DoRA中获取。
</details></li>
</ul>
<hr>
<h2 id="A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning"><a href="#A-Unifying-Variational-Framework-for-Gaussian-Process-Motion-Planning" class="headerlink" title="A Unifying Variational Framework for Gaussian Process Motion Planning"></a>A Unifying Variational Framework for Gaussian Process Motion Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00854">http://arxiv.org/abs/2309.00854</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Cosier, Rares Iordan, Sicelukwanda Zwane, Giovanni Franzese, James T. Wilson, Marc Peter Deisenroth, Alexander Terenin, Yasemin Bekiroglu</li>
<li>for: 本研究的目的是提出一种基于Variational Gaussian Processes的机器人运动规划方法，以便在高维状态空间中计算路径，同时考虑机械约束、降噪、模型误差等因素，并提供可靠的运动规划结果。</li>
<li>methods: 本研究使用了Variational Gaussian Processes来解决机器人运动规划问题，并提出了一种可以同时满足等式-based、不等式-based和软运动规划约束的框架。这种框架可以在练习过程中直接实现，并提供了间隔-based和Monte Carlo-based不确定性估计。</li>
<li>results: 对各种环境和机器人进行了比较 экспериiments，并与基准方法进行了比较，结果显示，提出的方法可以很好地平衡成功率和路径质量。<details>
<summary>Abstract</summary>
To control how a robot moves, motion planning algorithms must compute paths in high-dimensional state spaces while accounting for physical constraints related to motors and joints, generating smooth and stable motions, avoiding obstacles, and preventing collisions. A motion planning algorithm must therefore balance competing demands, and should ideally incorporate uncertainty to handle noise, model errors, and facilitate deployment in complex environments. To address these issues, we introduce a framework for robot motion planning based on variational Gaussian Processes, which unifies and generalizes various probabilistic-inference-based motion planning algorithms. Our framework provides a principled and flexible way to incorporate equality-based, inequality-based, and soft motion-planning constraints during end-to-end training, is straightforward to implement, and provides both interval-based and Monte-Carlo-based uncertainty estimates. We conduct experiments using different environments and robots, comparing against baseline approaches based on the feasibility of the planned paths, and obstacle avoidance quality. Results show that our proposed approach yields a good balance between success rates and path quality.
</details>
<details>
<summary>摘要</summary>
为控制机器人的运动，动态规划算法必须计算高维状态空间中的路径，同时考虑机器人的电动机和关节的物理约束，生成平滑和稳定的运动，避免障碍物和冲突。因此，一个有效的动态规划算法应该平衡竞合的需求，并应该包含不确定性来处理噪声、模型错误和复杂环境中的部署。为解决这些问题，我们介绍了基于Variational Gaussian Processes的机器人动态规划框架，这种框架将概率推理基本的动态规划算法集成了一体。我们的框架可以在练习中直接包含等式基于、不等式基于和软动态规划约束，并提供了间隔基于和Monte Carlo基于的不确定性估计。我们在不同的环境和机器人上进行了实验，与基线方法进行比较，包括规划路径的可行性和障碍物避免质量。结果表明，我们的提议方法可以平衡成功率和路径质量。
</details></li>
</ul>
<hr>
<h2 id="A-Post-Processing-Based-Bengali-Document-Layout-Analysis-with-YOLOV8"><a href="#A-Post-Processing-Based-Bengali-Document-Layout-Analysis-with-YOLOV8" class="headerlink" title="A Post-Processing Based Bengali Document Layout Analysis with YOLOV8"></a>A Post-Processing Based Bengali Document Layout Analysis with YOLOV8</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00848">http://arxiv.org/abs/2309.00848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nazmus Sakib Ahmed, Saad Sakib Noor, Ashraful Islam Shanto Sikder, Abhijit Paul</li>
<li>for: 提高孟加拉文档格式分析（DLA），使用YOLOv8模型和创新后处理技术。</li>
<li>methods: 采用数据扩展以提高模型鲁棒性，并实施两阶段预测策略以提高元素分 segmentation的准确性。</li>
<li>results:  ensemble模型，含Post处理，在BaDLAD数据集上达到了更高的性能，解决了在BaDLAD数据集中存在的问题。<details>
<summary>Abstract</summary>
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
</details>
<details>
<summary>摘要</summary>
Simplified Chinese translation:这篇论文关注使用YOLOv8模型和创新的后处理技术进行强化本地文档分析（DLA）。我们利用数据增强以提高模型的可靠性，并在完整的数据集上精心调整我们的方法，以实现精准的元素分割。我们的集成模型，与后处理相结合，超越了个别基础架构，解决了在BaDLAD数据集中存在的问题。我们希望通过这种方法进一步提高孟加拉文档分析，以提高OCR和文档理解。BaDLAD作为这一领域的基础资源，将为未来的研究提供帮助。此外，我们的实验又提供了关键的指导，以便在已有的解决方案中增加新的策略。
</details></li>
</ul>
<hr>
<h2 id="pSTarC-Pseudo-Source-Guided-Target-Clustering-for-Fully-Test-Time-Adaptation"><a href="#pSTarC-Pseudo-Source-Guided-Target-Clustering-for-Fully-Test-Time-Adaptation" class="headerlink" title="pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation"></a>pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00846">http://arxiv.org/abs/2309.00846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manogna Sreenivas, Goirik Chakrabarty, Soma Biswas</li>
<li>for: 这个论文的目的是提出一种新的测试时适应（TTA）方法，以便在实际场景中，模型能够在测试数据分布不同于训练数据分布下表现良好。</li>
<li>methods: 该方法受到目标划分技术的启发，利用源分类器生成 pseudo-source 样本，将测试样本策略地与这些 pseudo-source 样本相对应，从而提高 TTA 性能。</li>
<li>results: 实验 validate 了该方法的有效性，在多个领域差分数据集（VisDA、Office-Home、DomainNet-126、CIFAR-100C）上达到了显著提高预测精度和计算效率的目的。此外，我们还示出了该方法在连续 TTA 框架中的广泛性。<details>
<summary>Abstract</summary>
Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling models to perform well in real-world scenarios, where test data distribution differs from training. In this work, we propose a novel approach called pseudo Source guided Target Clustering (pSTarC) addressing the relatively unexplored area of TTA under real-world domain shifts. This method draws inspiration from target clustering techniques and exploits the source classifier for generating pseudo-source samples. The test samples are strategically aligned with these pseudo-source samples, facilitating their clustering and thereby enhancing TTA performance. pSTarC operates solely within the fully test-time adaptation protocol, removing the need for actual source data. Experimental validation on a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126, CIFAR-100C verifies pSTarC's effectiveness. This method exhibits significant improvements in prediction accuracy along with efficient computational requirements. Furthermore, we also demonstrate the universality of the pSTarC framework by showing its effectiveness for the continuous TTA framework.
</details>
<details>
<summary>摘要</summary>
测试时适应（TTA）是机器学习中的一个重要概念，它允许模型在实际场景中表现良好，其测试数据分布与训练数据分布不同。在这项工作中，我们提出了一种新的方法called pseudo Source guided Target Clustering（pSTarC），用于解决实际场景下的TTA问题，该问题尚未得到充分的研究。该方法 Draws inspiration from target clustering技术，并利用源分类器来生成 Pseudo-source 样本。测试样本被策略性地与这些 Pseudo-source 样本相对应，从而促进其分 clustering，并因此提高 TTA 性能。pSTarC 在完全测试时适应协议下运行，无需实际的源数据。我们对多个领域shift datasets进行了实验 validate pSTarC 的有效性，包括 VisDA、Office-Home、DomainNet-126 和 CIFAR-100C。结果表明，pSTarC 可以提高预测精度，同时具有有效的计算需求。此外，我们还证明了 pSTarC 框架的通用性，通过在连续 TTA 框架中应用该方法，并在不同的领域下达到了良好的性能。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning"><a href="#Autonomous-Soft-Tissue-Retraction-Using-Demonstration-Guided-Reinforcement-Learning" class="headerlink" title="Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning"></a>Autonomous Soft Tissue Retraction Using Demonstration-Guided Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00837">http://arxiv.org/abs/2309.00837</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amritpal Singh, Wenqi Shi, May D Wang</li>
<li>for: 这个研究的目的是为了开发一个可以进行软件运动的手术机器人，并通过实验证明其可行性。</li>
<li>methods: 这个研究使用了ROS相容的物理模拟环境，并在这个环境中模拟了手术过程中的软件互动。另外，这个研究还使用了示例导向的学习算法，以评估其性能和可行性。</li>
<li>results: 这个研究获得了软件运动 retraction 的自动化处理的证明，并且显示了将学习算法应用到软件互动中的可行性。这个研究提供了未来开发和改进手术机器人的基础。<details>
<summary>Abstract</summary>
In the context of surgery, robots can provide substantial assistance by performing small, repetitive tasks such as suturing, needle exchange, and tissue retraction, thereby enabling surgeons to concentrate on more complex aspects of the procedure. However, existing surgical task learning mainly pertains to rigid body interactions, whereas the advancement towards more sophisticated surgical robots necessitates the manipulation of soft bodies. Previous work focused on tissue phantoms for soft tissue task learning, which can be expensive and can be an entry barrier to research. Simulation environments present a safe and efficient way to learn surgical tasks before their application to actual tissue. In this study, we create a Robot Operating System (ROS)-compatible physics simulation environment with support for both rigid and soft body interactions within surgical tasks. Furthermore, we investigate the soft tissue interactions facilitated by the patient-side manipulator of the DaVinci surgical robot. Leveraging the pybullet physics engine, we simulate kinematics and establish anchor points to guide the robotic arm when manipulating soft tissue. Using demonstration-guided reinforcement learning (RL) algorithms, we investigate their performance in comparison to traditional reinforcement learning algorithms. Our in silico trials demonstrate a proof-of-concept for autonomous surgical soft tissue retraction. The results corroborate the feasibility of learning soft body manipulation through the application of reinforcement learning agents. This work lays the foundation for future research into the development and refinement of surgical robots capable of managing both rigid and soft tissue interactions. Code is available at https://github.com/amritpal-001/tissue_retract.
</details>
<details>
<summary>摘要</summary>
在手术上，机器人可以提供重要的帮助，执行小、重复的任务，如缝针交换和组织吸引，从而让外科医生能够更专注于更复杂的过程。然而，现有的手术任务学习主要关注固体交互，而更高级别的手术机器人的发展需要处理软体。现有的研究主要集中在模拟肿体上进行软组织任务学习，这可能是成本高并且可能成为研究入门障碍。在本研究中，我们创建了基于ROS（Robot Operating System）的物理 simulations环境，支持固体和软体交互在手术任务中。此外，我们研究了达文西手术机器人的病人侧 manipulate器在软组织上的交互。通过pybullet物理引擎，我们模拟了机械学和确定了安全点来导引机器人臂在软组织上操作。使用示例导向学习（RL）算法，我们研究了它们在比传统RL算法的性能。我们的silico实验证明了软组织吸引的自主化可能性。这些成果为未来研究开发可以处理固体和软体交互的手术机器人奠定了基础。代码可以在https://github.com/amritpal-001/tissue_retract上找到。
</details></li>
</ul>
<hr>
<h2 id="Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d"><a href="#Approximating-Fair-k-Min-Sum-Radii-in-mathbb-R-d" class="headerlink" title="Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$"></a>Approximating Fair $k$-Min-Sum-Radii in $\mathbb{R}^d$</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00834">http://arxiv.org/abs/2309.00834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lukas Drexler, Annika Hennes, Abhiruk Lahiri, Melanie Schmidt, Julian Wargalla</li>
<li>for: 这个论文是为了研究 $k$-min-sum-radii 问题的公平划分问题。</li>
<li>methods: 这篇论文使用了PTAS 算法来解决公平 $k$-min-sum-radii 问题。</li>
<li>results: 这篇论文提出了一个 PTAS 算法来解决公平 $k$-min-sum-radii 问题，并且可以适应不同的公平性约束。<details>
<summary>Abstract</summary>
The $k$-center problem is a classical clustering problem in which one is asked to find a partitioning of a point set $P$ into $k$ clusters such that the maximum radius of any cluster is minimized. It is well-studied. But what if we add up the radii of the clusters instead of only considering the cluster with maximum radius? This natural variant is called the $k$-min-sum-radii problem. It has become the subject of more and more interest in recent years, inspiring the development of approximation algorithms for the $k$-min-sum-radii problem in its plain version as well as in constrained settings.   We study the problem for Euclidean spaces $\mathbb{R}^d$ of arbitrary dimension but assume the number $k$ of clusters to be constant. In this case, a PTAS for the problem is known (see Bandyapadhyay, Lochet and Saurabh, SoCG, 2023). Our aim is to extend the knowledge base for $k$-min-sum-radii to the domain of fair clustering. We study several group fairness constraints, such as the one introduced by Chierichetti et al. (NeurIPS, 2017). In this model, input points have an additional attribute (e.g., colors such as red and blue), and clusters have to preserve the ratio between different attribute values (e.g., have the same fraction of red and blue points as the ground set). Different variants of this general idea have been studied in the literature. To the best of our knowledge, no approximative results for the fair $k$-min-sum-radii problem are known, despite the immense amount of work on the related fair $k$-center problem.   We propose a PTAS for the fair $k$-min-sum-radii problem in Euclidean spaces of arbitrary dimension for the case of constant $k$. To the best of our knowledge, this is the first PTAS for the problem. It works for different notions of group fairness.
</details>
<details>
<summary>摘要</summary>
“$k$-中心问题”是一个经典的聚集问题，需要将一个点集合($P$) partitioned into $k$个群，以 minimize 每个群的最大半径。这个问题已经受到了很多研究，但如果我们总和所有群的半径而不是仅考虑最大的半径，这个问题就会变成“$k$-最小和”问题。这个问题在最近的年份已经受到了越来越多的关注，并且发展了一些近似算法。我们在这篇文章中将研究这个问题在内部维度$d$的欧几何空间中，并假设$k$是常数。在这种情况下，一个PTAS（几何近似算法）已经是知道的（见Bandyapadhyay等（SoCG, 2023））。我们的目标是将这个知识库扩展到公平聚集领域。我们研究了许多公平群组征（例如，由Chierichetti等（NeurIPS, 2017）引入的一个征），这些征定义了输入点的额外特征（例如颜色），并且要求群 preserve 输入点的特征比率（例如，每个群中的红色和蓝色点的比率和输入点集合中的比率相同）。在文章中，我们提出了一个PTAS для公平$k$-最小和问题，这是首个知道的PTAS。它适用于不同的公平群组征。
</details></li>
</ul>
<hr>
<h2 id="ObjectLab-Automated-Diagnosis-of-Mislabeled-Images-in-Object-Detection-Data"><a href="#ObjectLab-Automated-Diagnosis-of-Mislabeled-Images-in-Object-Detection-Data" class="headerlink" title="ObjectLab: Automated Diagnosis of Mislabeled Images in Object Detection Data"></a>ObjectLab: Automated Diagnosis of Mislabeled Images in Object Detection Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00832">http://arxiv.org/abs/2309.00832</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cleanlab/cleanlab">https://github.com/cleanlab/cleanlab</a></li>
<li>paper_authors: Ulyana Tkachenko, Aditya Thyagarajan, Jonas Mueller</li>
<li>for: 提高对象检测模型的训练品质，减少对象检测数据集中的注释错误对模型的影响。</li>
<li>methods: 提出了一种简单的算法 ObjectLab，可以检测多种对象检测标签的错误，包括：过look bounding box、错误地标注 bounding box 和类别标签错误。ObjectLab 利用任何已经训练过的对象检测模型来评估图像标签质量，以便自动优先级化 incorrectly labeled 图像进行标签审查&#x2F;修正。</li>
<li>results: 在不同的对象检测数据集（包括 COCO）和不同的模型（包括 Detectron-X101 和 Faster-RCNN）上，ObjectLab 能够准确地检测注释错误，与其他标签质量分数相比，具有更高的准确率&#x2F;回归率。<details>
<summary>Abstract</summary>
Despite powering sensitive systems like autonomous vehicles, object detection remains fairly brittle in part due to annotation errors that plague most real-world training datasets. We propose ObjectLab, a straightforward algorithm to detect diverse errors in object detection labels, including: overlooked bounding boxes, badly located boxes, and incorrect class label assignments. ObjectLab utilizes any trained object detection model to score the label quality of each image, such that mislabeled images can be automatically prioritized for label review/correction. Properly handling erroneous data enables training a better version of the same object detection model, without any change in existing modeling code. Across different object detection datasets (including COCO) and different models (including Detectron-X101 and Faster-RCNN), ObjectLab consistently detects annotation errors with much better precision/recall compared to other label quality scores.
</details>
<details>
<summary>摘要</summary>
尽管它用于激活敏感系统，如自动驾驶车辆，但 object detection 仍然相对脆弱，一大部分这是因为实际训练集中的注释错误。我们提议 ObjectLab，一种简单的算法，用于检测多种对象检测标签的错误，包括：忽略的 bounding box、不当位置的 box 和错误的类别标签分配。ObjectLab 使用任何已经训练过 object detection 模型来评估每个图像的标签质量，以便自动优先级检查和修正错误标签。正确处理错误数据可以训练更好的同一个 object detection 模型，无需更改现有的模型代码。在不同的对象检测集（包括 COCO）和不同的模型（包括 Detectron-X101 和 Faster-RCNN）上，ObjectLab 可以准确地检测注释错误，与其他标签质量分数相比，具有更高的精度/准确率。
</details></li>
</ul>
<hr>
<h2 id="Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding"><a href="#Trustworthiness-Driven-Graph-Convolutional-Networks-for-Signed-Network-Embedding" class="headerlink" title="Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding"></a>Trustworthiness-Driven Graph Convolutional Networks for Signed Network Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00816">http://arxiv.org/abs/2309.00816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Min-Jeong Kim, Yeon-Chang Lee, David Y. Kang, Sang-Wook Kim</li>
<li>for: 本研究旨在提出一种基于图 convolutional networks (GCN) 的签名网络嵌入方法（SNE），以便在真实世界中更好地表示签名网络中的节点。</li>
<li>methods: 提出的方法基于GCN，并利用签名网络中边缘的可信度来 corrections 嵌入传递。该方法包括三个模块：生成每个节点的扩展EGO网络、测量边缘上的可信度、以及可信度感知的嵌入传递。</li>
<li>results: 实验结果表明，TrustSGCN 在四个真实世界签名网络数据集上 consistently 超越了五种 state-of-the-art GCN-based SNE 方法。代码可以在 <a target="_blank" rel="noopener" href="https://github.com/kmj0792/TrustSGCN">https://github.com/kmj0792/TrustSGCN</a> 上获取。<details>
<summary>Abstract</summary>
The problem of representing nodes in a signed network as low-dimensional vectors, known as signed network embedding (SNE), has garnered considerable attention in recent years. While several SNE methods based on graph convolutional networks (GCN) have been proposed for this problem, we point out that they significantly rely on the assumption that the decades-old balance theory always holds in the real-world. To address this limitation, we propose a novel GCN-based SNE approach, named as TrustSGCN, which corrects for incorrect embedding propagation in GCN by utilizing the trustworthiness on edge signs for high-order relationships inferred by the balance theory. The proposed approach consists of three modules: (M1) generation of each node's extended ego-network; (M2) measurement of trustworthiness on edge signs; and (M3) trustworthiness-aware propagation of embeddings. Furthermore, TrustSGCN learns the node embeddings by leveraging two well-known societal theories, i.e., balance and status. The experiments on four real-world signed network datasets demonstrate that TrustSGCN consistently outperforms five state-of-the-art GCN-based SNE methods. The code is available at https://github.com/kmj0792/TrustSGCN.
</details>
<details>
<summary>摘要</summary>
“signed network embedding（SNE）问题在最近几年内吸引了广泛关注。虽然基于图 convolutional networks（GCN）的多种SNE方法已经被提出来解决这个问题，但我们发现这些方法具有假设decades-old balance theory总是在实际中成立的假设。为了解决这个限制，我们提出了一种基于GCN的新型SNE方法，名为TrustSGCN，它通过利用边签上的信任程度来更正GCN中的嵌入传播错误。提案的方法包括三个模块：（M1）每个节点的扩展EGO网络生成；（M2）边签上的信任程度测量；以及（M3）基于信任程度的嵌入传播。此外，TrustSGCN通过利用社会理论中的平衡和社会Status两个理论来学习节点嵌入。实验表明，TrustSGCN在四个真实的签记网络dataset上与五种state-of-the-art GCN-based SNE方法相比，具有更高的性能。代码可以在https://github.com/kmj0792/TrustSGCN中下载。”
</details></li>
</ul>
<hr>
<h2 id="Bypassing-the-Simulator-Near-Optimal-Adversarial-Linear-Contextual-Bandits"><a href="#Bypassing-the-Simulator-Near-Optimal-Adversarial-Linear-Contextual-Bandits" class="headerlink" title="Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits"></a>Bypassing the Simulator: Near-Optimal Adversarial Linear Contextual Bandits</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00814">http://arxiv.org/abs/2309.00814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haolin Liu, Chen-Yu Wei, Julian Zimmert</li>
<li>for:  solves the adversarial linear contextual bandit problem with fully adversarially selected loss vectors and a fixed distribution for the per-round action set.</li>
<li>methods:  uses a new algorithm that achieves a regret of $\widetilde{O}(\sqrt{T})$ without a simulator, while maintaining computational efficiency when the action set in each round is small.</li>
<li>results:  greatly improves the existing results, which either require access to a simulator or achieve a sub-optimal regret no better than $\widetilde{O}(T^{\frac{5}{6})$.<details>
<summary>Abstract</summary>
We consider the adversarial linear contextual bandit problem, where the loss vectors are selected fully adversarially and the per-round action set (i.e. the context) is drawn from a fixed distribution. Existing methods for this problem either require access to a simulator to generate free i.i.d. contexts, achieve a sub-optimal regret no better than $\widetilde{O}(T^{\frac{5}{6})$, or are computationally inefficient. We greatly improve these results by achieving a regret of $\widetilde{O}(\sqrt{T})$ without a simulator, while maintaining computational efficiency when the action set in each round is small. In the special case of sleeping bandits with adversarial loss and stochastic arm availability, our result answers affirmatively the open question by Saha et al. [2020] on whether there exists a polynomial-time algorithm with $poly(d)\sqrt{T}$ regret. Our approach naturally handles the case where the loss is linear up to an additive misspecification error, and our regret shows near-optimal dependence on the magnitude of the error.
</details>
<details>
<summary>摘要</summary>
我们考虑了敌对的线性上下文强化问题，loss вектор会被完全敌对选择，并且每次action集（即context）是从固定分布中抽出的。现有的方法对这个问题可能需要访问一个模拟器来生成免费i.i.d.context，仅实现了较差的对抗 regret，不超过 $\widetilde{O}(T^{\frac{5}{6})$。我们大幅改进了这些结果，在不需要模拟器的情况下，实现了 regret的 $\widetilde{O}(\sqrt{T})$，并且维持了每轮action集的小型化。在睡眠问题中，我们的结果答案了 Saha et al. [2020] 的开问题，而且我们的方法自然地处理了loss是线性的情况，并且 regret的对抗性取决于错误的大小。
</details></li>
</ul>
<hr>
<h2 id="Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making"><a href="#Fairness-Implications-of-Heterogeneous-Treatment-Effect-Estimation-with-Machine-Learning-Methods-in-Policy-making" class="headerlink" title="Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making"></a>Fairness Implications of Heterogeneous Treatment Effect Estimation with Machine Learning Methods in Policy-making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00805">http://arxiv.org/abs/2309.00805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Patrick Rehill, Nicholas Biddle</li>
<li>for: 这篇论文主要是为了探讨 causal machine learning 方法如何生成不同类型的减震效应估计，以及这些方法如何在政策决策中应用。</li>
<li>methods: 论文使用了 standard AI Fairness 方法，但认为这些方法不适用于所有 causal machine learning 应用。在描述 indirect 和 direct 决策场景时，论文提出了一种定义 fairness，即一个模型需要为决级者提供准确的政策结果估计。</li>
<li>results: 论文认为，因为 causal machine learning 模型的复杂性，可能会难以实现 fairness。而不是使用传统的 AI Fairness 调整，论文提出了一种解决方案，即通过细心模型和决策偏见的认知来解决这些问题。<details>
<summary>Abstract</summary>
Causal machine learning methods which flexibly generate heterogeneous treatment effect estimates could be very useful tools for governments trying to make and implement policy. However, as the critical artificial intelligence literature has shown, governments must be very careful of unintended consequences when using machine learning models. One way to try and protect against unintended bad outcomes is with AI Fairness methods which seek to create machine learning models where sensitive variables like race or gender do not influence outcomes. In this paper we argue that standard AI Fairness approaches developed for predictive machine learning are not suitable for all causal machine learning applications because causal machine learning generally (at least so far) uses modelling to inform a human who is the ultimate decision-maker while AI Fairness approaches assume a model that is making decisions directly. We define these scenarios as indirect and direct decision-making respectively and suggest that policy-making is best seen as a joint decision where the causal machine learning model usually only has indirect power. We lay out a definition of fairness for this scenario - a model that provides the information a decision-maker needs to accurately make a value judgement about just policy outcomes - and argue that the complexity of causal machine learning models can make this difficult to achieve. The solution here is not traditional AI Fairness adjustments, but careful modelling and awareness of some of the decision-making biases that these methods might encourage which we describe.
</details>
<details>
<summary>摘要</summary>
政府可以使用可变性机器学习方法来生成不同类型的对待方法估计，这些工具可能很有用于政策制定和实施。然而，如critical artificial intelligence文献所示，政府应该非常小心不良后果，因为机器学习模型可能会导致不良后果。为了保护 against不良后果，可以使用AI Fairness方法，该方法旨在创建不会根据敏感变量（如种族或性别）影响结果的机器学习模型。在这篇论文中，我们 argue that标准的AI Fairness方法（developed for predictive machine learning）不适用于所有 causal machine learning应用程序，因为 causal machine learning通常（至少到目前为止）使用模型来辅助人类决策者做出决定。我们将这些情况分为直接决策和间接决策两种方式，并认为政策决策是间接决策的混合决策。我们提出了一定的公平定义，即一个模型需要提供决策者需要准确判断公平政策结果的信息。我们也 argue that causal machine learning模型的复杂性可能使得这件事件困难实现。因此，不是使用传统的AI Fairness调整，而是需要仔细的模型和决策者偏见的意识。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-and-Inverse-Problems"><a href="#Deep-Learning-and-Inverse-Problems" class="headerlink" title="Deep Learning and Inverse Problems"></a>Deep Learning and Inverse Problems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00802">http://arxiv.org/abs/2309.00802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics">https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics</a></li>
<li>paper_authors: Ali Mohammad-Djafari, Ning Chu, Li Wang, Liang Yu</li>
<li>for: 这篇论文主要针对 inverse problems 问题，即在 indirect measurement 下获取准确的解决方案。</li>
<li>methods: 该论文使用 Machine Learning (ML) 和 Deep Learning (DL) 方法来解决 inverse problems，特别是 Convolutional Neural Network (CNN) 和 Deep Neural Network (DNN)。</li>
<li>results: 该论文提出了两种方法来解决 inverse problems：第一种是使用 known forward operator 作为物理约束，第二种是使用 data-driven DL 方法。<details>
<summary>Abstract</summary>
Machine Learning (ML) methods and tools have gained great success in many data, signal, image and video processing tasks, such as classification, clustering, object detection, semantic segmentation, language processing, Human-Machine interface, etc. In computer vision, image and video processing, these methods are mainly based on Neural Networks (NN) and in particular Convolutional NN (CNN), and more generally Deep NN. Inverse problems arise anywhere we have indirect measurement. As, in general, those inverse problems are ill-posed, to obtain satisfactory solutions for them needs prior information. Different regularization methods have been proposed, where the problem becomes the optimization of a criterion with a likelihood term and a regularization term. The main difficulty, however, in great dimensional real applications, remains the computational cost. Using NN, and in particular Deep Learning (DL) surrogate models and approximate computation, can become very helpful. In this work, we focus on NN and DL particularly adapted for inverse problems. We consider two cases: First the case where the forward operator is known and used as physics constraint, the second more general data driven DL methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="League-of-Legends-Real-Time-Result-Prediction"><a href="#League-of-Legends-Real-Time-Result-Prediction" class="headerlink" title="League of Legends: Real-Time Result Prediction"></a>League of Legends: Real-Time Result Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.02449">http://arxiv.org/abs/2309.02449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jailson B. S. Junior, Claudio E. C. Campelo</li>
<li>for: 预测LoL电子游戏match结果 using机器学习技术</li>
<li>methods: 使用未发表数据，评估不同变量和比赛阶段的准确率</li>
<li>results: 使用LightGBM模型获得了81.62%的平均准确率，Logistic Regression和Gradient Boosting模型在早期比赛阶段表现出色<details>
<summary>Abstract</summary>
This paper presents a study on the prediction of outcomes in matches of the electronic game League of Legends (LoL) using machine learning techniques. With the aim of exploring the ability to predict real-time results, considering different variables and stages of the match, we highlight the use of unpublished data as a fundamental part of this process. With the increasing popularity of LoL and the emergence of tournaments, betting related to the game has also emerged, making the investigation in this area even more relevant. A variety of models were evaluated and the results were encouraging. A model based on LightGBM showed the best performance, achieving an average accuracy of 81.62\% in intermediate stages of the match when the percentage of elapsed time was between 60\% and 80\%. On the other hand, the Logistic Regression and Gradient Boosting models proved to be more effective in early stages of the game, with promising results. This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.
</details>
<details>
<summary>摘要</summary>
The study evaluates various machine learning models and achieves encouraging results. The LightGBM model performed best, with an average accuracy of 81.62% in intermediate stages of the match when the elapsed time was between 60% and 80%. On the other hand, Logistic Regression and Gradient Boosting models were more effective in early stages of the game, with promising results.This study contributes to the field of machine learning applied to electronic games, providing valuable insights into real-time prediction in League of Legends. The results obtained may be relevant for both players seeking to improve their strategies and the betting industry related to the game.
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction"><a href="#Diffusion-Modeling-with-Domain-conditioned-Prior-Guidance-for-Accelerated-MRI-and-qMRI-Reconstruction" class="headerlink" title="Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction"></a>Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00783">http://arxiv.org/abs/2309.00783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wanyu Bian, Albert Jang, Fang Liu</li>
<li>for: 本研究提出了一种基于扩散模型的图像重建方法，用于多极MRI和量化MRI重建。</li>
<li>methods: 该方法利用频率和参数领域中的域Conditioned扩散模型，并使用原始MRI物理作为嵌入，以确保数据一致性和特征学习。</li>
<li>results: 该方法在高速因素下的图像重建中表现出了显著的承诺，并在不同的生物结构中维持了高精度和效率。此外，该方法还具有普适性，可以应用于不同领域的反向问题。<details>
<summary>Abstract</summary>
This study introduces a novel approach for image reconstruction based on a diffusion model conditioned on the native data domain. Our method is applied to multi-coil MRI and quantitative MRI reconstruction, leveraging the domain-conditioned diffusion model within the frequency and parameter domains. The prior MRI physics are used as embeddings in the diffusion model, enforcing data consistency to guide the training and sampling process, characterizing MRI k-space encoding in MRI reconstruction, and leveraging MR signal modeling for qMRI reconstruction. Furthermore, a gradient descent optimization is incorporated into the diffusion steps, enhancing feature learning and improving denoising. The proposed method demonstrates a significant promise, particularly for reconstructing images at high acceleration factors. Notably, it maintains great reconstruction accuracy and efficiency for static and quantitative MRI reconstruction across diverse anatomical structures. Beyond its immediate applications, this method provides potential generalization capability, making it adaptable to inverse problems across various domains.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction"><a href="#Structured-Radial-Basis-Function-Network-Modelling-Diversity-for-Multiple-Hypotheses-Prediction" class="headerlink" title="Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction"></a>Structured Radial Basis Function Network: Modelling Diversity for Multiple Hypotheses Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00781">http://arxiv.org/abs/2309.00781</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alejandro Rodriguez Dominguez, Muhammad Shahzad, Xia Hong</li>
<li>For: The paper is written for addressing the challenge of multi-modal regression, which is important in forecasting nonstationary processes or with a complex mixture of distributions.* Methods: The paper proposes an ensemble of multiple hypotheses predictors, called a Structured Radial Basis Function Network, which can efficiently interpolate a tessellation of centroidal Voronoi tessellations and approximate the multiple hypotheses target distribution.* Results: The paper achieves superior generalization performance and computational efficiency using only two-layer neural networks as predictors, and introduces a gradient-descent approach that is loss-agnostic regarding the predictors. The experiments show outperformance with respect to the top competitors in the literature.<details>
<summary>Abstract</summary>
Multi-modal regression is important in forecasting nonstationary processes or with a complex mixture of distributions. It can be tackled with multiple hypotheses frameworks but with the difficulty of combining them efficiently in a learning model. A Structured Radial Basis Function Network is presented as an ensemble of multiple hypotheses predictors for regression problems. The predictors are regression models of any type that can form centroidal Voronoi tessellations which are a function of their losses during training. It is proved that this structured model can efficiently interpolate this tessellation and approximate the multiple hypotheses target distribution and is equivalent to interpolating the meta-loss of the predictors, the loss being a zero set of the interpolation error. This model has a fixed-point iteration algorithm between the predictors and the centers of the basis functions. Diversity in learning can be controlled parametrically by truncating the tessellation formation with the losses of individual predictors. A closed-form solution with least-squares is presented, which to the authors knowledge, is the fastest solution in the literature for multiple hypotheses and structured predictions. Superior generalization performance and computational efficiency is achieved using only two-layer neural networks as predictors controlling diversity as a key component of success. A gradient-descent approach is introduced which is loss-agnostic regarding the predictors. The expected value for the loss of the structured model with Gaussian basis functions is computed, finding that correlation between predictors is not an appropriate tool for diversification. The experiments show outperformance with respect to the top competitors in the literature.
</details>
<details>
<summary>摘要</summary>
多Modal重要预测非站ARY进程或复杂的混合分布。可以使用多个假设框架，但是将其组合到学习模型中是困难的。一种结构化基于几何函数网络的集成多个假设预测器被提出，该模型可以准确地 interpolate这些几何函数网络，并且等于 interpolating 多个假设目标分布。这个模型有一个固定点迭代算法，该算法在预测器和基函数中心之间进行迭代。可控制学习的多样性参数地 truncate 几何函数网络的形成，以控制各个预测器的损失。提供一个快速的关于多个假设和结构预测的关闭形式解，该解使用只有两层神经网络作为预测器，并控制多样性为成功的关键Component。引入一种梯度下降方法，该方法不关于预测器的损失。计算结构模型在 Gaussian 基函数下的预期损失值，发现相互 correlate 不是适用于多样化的工具。实验表明，该模型在文献中的竞争对手之上表现出色。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Feature-Masking-Open-Vocabulary-Vision-Transformer"><a href="#Contrastive-Feature-Masking-Open-Vocabulary-Vision-Transformer" class="headerlink" title="Contrastive Feature Masking Open-Vocabulary Vision Transformer"></a>Contrastive Feature Masking Open-Vocabulary Vision Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00775">http://arxiv.org/abs/2309.00775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dahun Kim, Anelia Angelova, Weicheng Kuo<br>for:CFM-ViT is written for open-vocabulary object detection (OVD) tasks, with the goal of simultaneously learning image- and region-level representation.methods:CFM-ViT combines the masked autoencoder (MAE) objective with contrastive learning, and performs reconstruction in the joint image-text embedding space. Additionally, Positional Embedding Dropout (PED) is introduced to address scale variation during pretraining.results:CFM-ViT achieves state-of-the-art performance on the LVIS open-vocabulary detection benchmark, with an AP$r$ of 33.9 and outperforms the best approach by 7.6 points. It also achieves strong image-level representation on zero-shot image-text retrieval benchmarks, outperforming the state of the art on 8 out of 12 metrics.Here is the simplified Chinese text for the three key points:for:CFM-ViT 是为开放词汇对象检测任务而写的，目标是同时学习图像和区域水平表示。methods:CFM-ViT 将MAE objective与冲突学习相结合，并在图像和文本嵌入空间进行重建。此外，我们还引入Positional Embedding Dropout (PED)来Address scale variation during pretraining。results:CFM-ViT 在 LVIS 开放词汇检测标准 benchmark 上实现了状态机器的表现，AP$r$ 为 33.9，超过最佳方法的 7.6 点。它还在零扫预测 benchmark 上实现了强大的图像水平表示，在 12 个 metric 中击败了状态机器的表现。<details>
<summary>Abstract</summary>
We present Contrastive Feature Masking Vision Transformer (CFM-ViT) - an image-text pretraining methodology that achieves simultaneous learning of image- and region-level representation for open-vocabulary object detection (OVD). Our approach combines the masked autoencoder (MAE) objective into the contrastive learning objective to improve the representation for localization tasks. Unlike standard MAE, we perform reconstruction in the joint image-text embedding space, rather than the pixel space as is customary with the classical MAE method, which causes the model to better learn region-level semantics. Moreover, we introduce Positional Embedding Dropout (PED) to address scale variation between image-text pretraining and detection finetuning by randomly dropping out the positional embeddings during pretraining. PED improves detection performance and enables the use of a frozen ViT backbone as a region classifier, preventing the forgetting of open-vocabulary knowledge during detection finetuning. On LVIS open-vocabulary detection benchmark, CFM-ViT achieves a state-of-the-art 33.9 AP$r$, surpassing the best approach by 7.6 points and achieves better zero-shot detection transfer. Finally, CFM-ViT acquires strong image-level representation, outperforming the state of the art on 8 out of 12 metrics on zero-shot image-text retrieval benchmarks.
</details>
<details>
<summary>摘要</summary>
我们提出了异构特征压缩视Transformer（CFM-ViT），一种图像和文本预训练方法，实现同时学习图像和区域水平表示。我们的方法将掩码自动编码（MAE）目标 integrates 到对比学习目标中，以提高本地化任务的表示。与标准 MAE 不同，我们在图像-文本嵌入空间中进行重建，而不是在像素空间中，这使得模型更好地学习区域水平 semantics。此外，我们引入了Positional Embedding Dropout（PED），以Address scale variation between image-text pretraining and detection finetuning。PED 可以在预训练中随机drop out positional embeddings，从而提高检测性能并使用冻结的 ViT 底层作为区域分类器，避免在检测finetuning中忘记开放词汇知识。在 LVIS 开放词汇检测标准 benchmark 上，CFM-ViT 实现了33.9 AP$r$ 的状态机器，比最佳方法提高7.6个点，并在零容量检测转移中表现出色。最后，CFM-ViT 获得了强大的图像级别表示，在零容量图像-文本检索标准 benchmark 上超过了状态机器的8个 из 12个维度。
</details></li>
</ul>
<hr>
<h2 id="Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models"><a href="#Non-Asymptotic-Bounds-for-Adversarial-Excess-Risk-under-Misspecified-Models" class="headerlink" title="Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models"></a>Non-Asymptotic Bounds for Adversarial Excess Risk under Misspecified Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00771">http://arxiv.org/abs/2309.00771</a></li>
<li>repo_url: None</li>
<li>paper_authors: Changyu Liu, Yuling Jiao, Junhui Wang, Jian Huang</li>
<li>for: 评估 robust 估计器的性能 based on adversarial losses under misspecified models.</li>
<li>methods: 使用 adversarial risk 作为评估指标，并在 certain smoothness conditions 下显示 adversarial risk 等价于 distributional adversarial attack 所引起的风险。</li>
<li>results: 提供了一种 general approach 来评估 robust 估计器的性能，并在不同的 loss functions 上Establish non-asymptotic upper bounds for the adversarial excess risk. 另外, 在 classification 和 regression 问题中应用了这些总结结果.<details>
<summary>Abstract</summary>
We propose a general approach to evaluating the performance of robust estimators based on adversarial losses under misspecified models. We first show that adversarial risk is equivalent to the risk induced by a distributional adversarial attack under certain smoothness conditions. This ensures that the adversarial training procedure is well-defined. To evaluate the generalization performance of the adversarial estimator, we study the adversarial excess risk. Our proposed analysis method includes investigations on both generalization error and approximation error. We then establish non-asymptotic upper bounds for the adversarial excess risk associated with Lipschitz loss functions. In addition, we apply our general results to adversarial training for classification and regression problems. For the quadratic loss in nonparametric regression, we show that the adversarial excess risk bound can be improved over those for a general loss.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的方法来评估robust预测器的性能基于反对抗攻击的损失函数。我们首先表明了反对抗攻击风险与一定的光滑性Conditions下相等。这使得反对抗训练程序得到定义。为评估反对抗预测器的普遍性性能，我们研究了反对抗剩余风险。我们的提出分析方法包括对普遍错误和近似错误进行研究。我们然后建立了非假设性上的上限 bound для反对抗剩余风险，其中包括 Lipschitz 损失函数。此外，我们应用了我们的一般结论到反对抗训练 для分类和回归问题。对于非 Parametric 回归中的quadratic损失函数，我们表明了对反对抗剩余风险 bound可以超过一般损失函数的bound。
</details></li>
</ul>
<hr>
<h2 id="Bias-and-Fairness-in-Large-Language-Models-A-Survey"><a href="#Bias-and-Fairness-in-Large-Language-Models-A-Survey" class="headerlink" title="Bias and Fairness in Large Language Models: A Survey"></a>Bias and Fairness in Large Language Models: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00770">http://arxiv.org/abs/2309.00770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/i-gallegos/fair-llm-benchmark">https://github.com/i-gallegos/fair-llm-benchmark</a></li>
<li>paper_authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed</li>
<li>for: 本文旨在概述和分析大型自然语言模型（LLM）中存在的社会偏见，以及如何评估和 mitigate 这些偏见。</li>
<li>methods: 本文提出了一种三分类系统来评估和 Mitigate LLM 中的社会偏见，包括：在模型预处理阶段、在训练阶段、在处理阶段和后处理阶段进行干预。</li>
<li>results: 本文总结了现有研究的结果，并提供了一个清晰的指南，以帮助研究人员和实践者更好地理解和预防 LLM 中的社会偏见。<details>
<summary>Abstract</summary>
Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLMs）的快速进步使得处理、理解和生成人类语言文本的能力得到了显著提高，并在社会圈中的系统中得到了普遍应用。然而，这些模型可以学习、传播和增强社会偏见。在这篇论文中，我们提供了对偏见评估和mitigation技术的全面评论。我们首先将社会偏见和公平在自然语言处理中的概念进行了整合、正式化和扩展，并定义了不同类型的危害和公平的感知。然后，我们提出了三种直观的分类，即评估 metric分类、数据集分类和mitigation技术分类。我们的首个评估 metric分类将 metric 和评估数据集之间的关系清晰地分类，并将 metric 分为模型中不同水平的三种：嵌入、概率和生成文本。我们的第二个数据集分类将数据集分为对应不同社会团体的 counterfactual 输入或提示，并标识目标危害和社会群体。我们还发布了一个汇总的公共可用数据集，以便更好地访问。我们的第三个mitigation技术分类将方法分为在预处理、训练、内部处理和后处理中进行的不同类型的 intervención，并且在每个类型中分配了细分类，以阐明研究趋势。最后，我们 indentified 未解决的问题和挑战，以便未来的研究。通过汇总了广泛的最新研究，我们希望通过这篇文章，为研究人员和实践人员提供一份清晰的指南，以便更好地理解和预防 LLMs 中的偏见传播。
</details></li>
</ul>
<hr>
<h2 id="Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids"><a href="#Physics-informed-machine-learning-of-the-correlation-functions-in-bulk-fluids" class="headerlink" title="Physics-informed machine learning of the correlation functions in bulk fluids"></a>Physics-informed machine learning of the correlation functions in bulk fluids</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00767">http://arxiv.org/abs/2309.00767</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqian Chen, Peiyuan Gao, Panos Stinis</li>
<li>for: 本研究使用机器学习模型解决了奥尔谢因-泽尼克方程（OZ方程），以解决现代积分方程理论中的液体独立方程。</li>
<li>methods: 本研究使用了物理学习模型，包括物理学习神经网络和物理学习神经操作网络，解决OZ方程的前向和反向问题。</li>
<li>results: 研究发现，物理学习模型在解决OZ方程问题上具有高准确率和高效率，并且可以应用于热力学状态理论中。<details>
<summary>Abstract</summary>
The Ornstein-Zernike (OZ) equation is the fundamental equation for pair correlation function computations in the modern integral equation theory for liquids. In this work, machine learning models, notably physics-informed neural networks and physics-informed neural operator networks, are explored to solve the OZ equation. The physics-informed machine learning models demonstrate great accuracy and high efficiency in solving the forward and inverse OZ problems of various bulk fluids. The results highlight the significant potential of physics-informed machine learning for applications in thermodynamic state theory.
</details>
<details>
<summary>摘要</summary>
“欧兹方程”（Ornstein-Zernike equation）是现代液体积分方程论中Computations的基本方程。在这种工作中，我们使用机器学习模型，主要是物理学习核网络和物理学习运算器网络，解决欧兹方程的前向和反向问题。这些物理学习模型在解决欧兹方程的问题上显示了极高的准确性和高效率。结果表明物理学习可以在热力学状态论中应用。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/cs.LG_2023_09_02/" data-id="clmjn91mu007v0j889w6jge3k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_09_02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/02/eess.IV_2023_09_02/" class="article-date">
  <time datetime="2023-09-02T09:00:00.000Z" itemprop="datePublished">2023-09-02</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/02/eess.IV_2023_09_02/">eess.IV - 2023-09-02</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Constrained-CycleGAN-for-Effective-Generation-of-Ultrasound-Sector-Images-of-Improved-Spatial-Resolution"><a href="#Constrained-CycleGAN-for-Effective-Generation-of-Ultrasound-Sector-Images-of-Improved-Spatial-Resolution" class="headerlink" title="Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution"></a>Constrained CycleGAN for Effective Generation of Ultrasound Sector Images of Improved Spatial Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00995">http://arxiv.org/abs/2309.00995</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xfsun99/CCycleGAN-TF2">https://github.com/xfsun99/CCycleGAN-TF2</a></li>
<li>paper_authors: Xiaofei Sun, He Li, Wei-Ning Lee<br>for: 这种研究旨在将多个适配器探针生成的声学影像翻译成具有更高空间分辨率的影像。methods: 该研究提出了一种受限制的循环GAN（CCycleGAN），该模型直接使用不同探针生成的声学影像进行生成。 CCycleGAN 还引入了一个标准的反对抗损失和循环一致性损失，以及一个基于声学反射信号的相似度损失来约束结构一致性和反射信号特征的保持。results: 在室内phantom实验中，CCycleGAN 成功地生成了具有更高空间分辨率和更高PSNR和SSIM值的影像。此外，CCycleGAN-生成的人体心脏动力学运动估计也比 benchmarks-生成的更高质量，特别是在深部区域。<details>
<summary>Abstract</summary>
Objective. A phased or a curvilinear array produces ultrasound (US) images with a sector field of view (FOV), which inherently exhibits spatially-varying image resolution with inferior quality in the far zone and towards the two sides azimuthally. Sector US images with improved spatial resolutions are favorable for accurate quantitative analysis of large and dynamic organs, such as the heart. Therefore, this study aims to translate US images with spatially-varying resolution to ones with less spatially-varying resolution. CycleGAN has been a prominent choice for unpaired medical image translation; however, it neither guarantees structural consistency nor preserves backscattering patterns between input and generated images for unpaired US images. Approach. To circumvent this limitation, we propose a constrained CycleGAN (CCycleGAN), which directly performs US image generation with unpaired images acquired by different ultrasound array probes. In addition to conventional adversarial and cycle-consistency losses of CycleGAN, CCycleGAN introduces an identical loss and a correlation coefficient loss based on intrinsic US backscattered signal properties to constrain structural consistency and backscattering patterns, respectively. Instead of post-processed B-mode images, CCycleGAN uses envelope data directly obtained from beamformed radio-frequency signals without any other non-linear postprocessing. Main Results. In vitro phantom results demonstrate that CCycleGAN successfully generates images with improved spatial resolution as well as higher peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) compared with benchmarks. Significance. CCycleGAN-generated US images of the in vivo human beating heart further facilitate higher quality heart wall motion estimation than benchmarks-generated ones, particularly in deep regions.
</details>
<details>
<summary>摘要</summary>
目标：使用phaseless或弯曲阵列生成ultrasound（US）图像，图像具有扇形观察领域（FOV），但图像在远区和两侧方向上具有逐渐递减的分辨率，导致图像质量下降。为了提高US图像的分辨率，本研究目的是将US图像翻译成具有更高分辨率的图像。CyclGAN是一种常用的无对应医学图像翻译方法，但它并不保证结构一致性也不保持各自的反射特征。方法：我们提出一种受限的CyclGAN（CCycleGAN），它直接将不同探针阵列生成US图像。CCycleGAN还添加了与普通的对抗搅拌和环状一致性损失相同的损失，以保持结构一致性和反射特征。而不是使用后处理的B模式图像，CCycleGAN使用直接从干扰信号中获得的扩散数据，无需任何其他非线性后处理。主要结果：在室内照相器中，CCycleGAN成功地生成图像，具有更高的分辨率和PSNR（干扰信号强度比）以及SSIM（结构相似度）。意义：CCycleGAN-生成的US图像在人体内部活跃心脏的区域更好地估计心墙运动，特别是深部区域。
</details></li>
</ul>
<hr>
<h2 id="AdLER-Adversarial-Training-with-Label-Error-Rectification-for-One-Shot-Medical-Image-Segmentation"><a href="#AdLER-Adversarial-Training-with-Label-Error-Rectification-for-One-Shot-Medical-Image-Segmentation" class="headerlink" title="AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation"></a>AdLER: Adversarial Training with Label Error Rectification for One-Shot Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00971">http://arxiv.org/abs/2309.00971</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hsiangyuzhao/AdLER">https://github.com/hsiangyuzhao/AdLER</a></li>
<li>paper_authors: Xiangyu Zhao, Sheng Wang, Zhiyun Song, Zhenrong Shen, Linlin Yao, Haolei Yuan, Qian Wang, Lichi Zhang</li>
<li>for: 这个研究的目的是提高医疗影像自动分类的精度和可靠性，特别是在临床 Setting中，where limited training data is available.</li>
<li>methods: 这个方法使用一击分类（OSSLT），包括不supervised deformable registration、data augmentation with learned registration、和从增强的数据进行分类。但是现有的一击分类方法受到有限的数据多样性和可能的标签错误的影响。</li>
<li>results: 这个研究提出了一个新的一击医疗影像分类方法，利用反对抗训练和标签修正（AdLER），以提高分类性能。实验结果显示，提案的AdLER在CANDI和ABIDE数据集上比前一个 state-of-the-art 方法高出0.7%、3.6%和4.9%的Dice分数，分别。<details>
<summary>Abstract</summary>
Accurate automatic segmentation of medical images typically requires large datasets with high-quality annotations, making it less applicable in clinical settings due to limited training data. One-shot segmentation based on learned transformations (OSSLT) has shown promise when labeled data is extremely limited, typically including unsupervised deformable registration, data augmentation with learned registration, and segmentation learned from augmented data. However, current one-shot segmentation methods are challenged by limited data diversity during augmentation, and potential label errors caused by imperfect registration. To address these issues, we propose a novel one-shot medical image segmentation method with adversarial training and label error rectification (AdLER), with the aim of improving the diversity of generated data and correcting label errors to enhance segmentation performance. Specifically, we implement a novel dual consistency constraint to ensure anatomy-aligned registration that lessens registration errors. Furthermore, we develop an adversarial training strategy to augment the atlas image, which ensures both generation diversity and segmentation robustness. We also propose to rectify potential label errors in the augmented atlas images by estimating segmentation uncertainty, which can compensate for the imperfect nature of deformable registration and improve segmentation authenticity. Experiments on the CANDI and ABIDE datasets demonstrate that the proposed AdLER outperforms previous state-of-the-art methods by 0.7% (CANDI), 3.6% (ABIDE "seen"), and 4.9% (ABIDE "unseen") in segmentation based on Dice scores, respectively. The source code will be available at https://github.com/hsiangyuzhao/AdLER.
</details>
<details>
<summary>摘要</summary>
医疗图像自动分割通常需要大量高质量标注数据，因此在临床设置下难以实施，因为有限的训练数据。基于学习变换的一招分割（OSSLT）已经在具有极少标注数据的情况下显示了抗应用性。然而，当前的一招分割方法受到有限数据多样性的限制，以及可能的标签错误，导致分割性能下降。为解决这些问题，我们提出了一种新的一招医疗图像分割方法，具有对抗训练和标签错误修复（AdLER），以提高分割性能。具体来说，我们实施了一种新的双重一致性约束，以降低注射错误。此外，我们开发了一种对抗训练策略，以增加生成的数据多样性和分割稳定性。此外，我们还提出了修复可能的标签错误的方法，通过估计分割不确定性，以补偿注射注入的不完整性，提高分割 authenticty。实验表明，提出的 AdLER 方法在 CANDI 和 ABIDE 数据集上比前状态之前的方法高出 0.7%（CANDI）、3.6%（ABIDE "seen") 和 4.9%（ABIDE "unseen") 的分割基于 dice 分数，分别。源代码将于 <https://github.com/hsiangyuzhao/AdLER> 上公开。
</details></li>
</ul>
<hr>
<h2 id="S-3-MonoDETR-Supervised-Shape-Scale-perceptive-Deformable-Transformer-for-Monocular-3D-Object-Detection"><a href="#S-3-MonoDETR-Supervised-Shape-Scale-perceptive-Deformable-Transformer-for-Monocular-3D-Object-Detection" class="headerlink" title="S$^3$-MonoDETR: Supervised Shape&amp;Scale-perceptive Deformable Transformer for Monocular 3D Object Detection"></a>S$^3$-MonoDETR: Supervised Shape&amp;Scale-perceptive Deformable Transformer for Monocular 3D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00928">http://arxiv.org/abs/2309.00928</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan He, Kailun Yang, Junwei Zheng, Jin Yuan, Luis M. Bergasa, Hui Zhang, Zhiyong Li</li>
<li>for: 提高单视图3D物体检测的准确率，特别是对多类目物体的检测。</li>
<li>methods: 提出了一种新的“帮助”（S$^3$-DA）模块，利用视觉和深度特征生成多种形态和比例的多样化本地特征，并同时预测匹配分布，从而赋予每个查询点有价值的形态&amp;比例觉察。</li>
<li>results: 对KITTI和Waymo开放 dataset进行了广泛的实验，显示S$^3$-DA能够显著提高检测精度，在单一训练过程中实现单类和多类3D物体检测的州际最佳性。<details>
<summary>Abstract</summary>
Recently, transformer-based methods have shown exceptional performance in monocular 3D object detection, which can predict 3D attributes from a single 2D image. These methods typically use visual and depth representations to generate query points on objects, whose quality plays a decisive role in the detection accuracy. However, current unsupervised attention mechanisms without any geometry appearance awareness in transformers are susceptible to producing noisy features for query points, which severely limits the network performance and also makes the model have a poor ability to detect multi-category objects in a single training process. To tackle this problem, this paper proposes a novel "Supervised Shape&Scale-perceptive Deformable Attention" (S$^3$-DA) module for monocular 3D object detection. Concretely, S$^3$-DA utilizes visual and depth features to generate diverse local features with various shapes and scales and predict the corresponding matching distribution simultaneously to impose valuable shape&scale perception for each query. Benefiting from this, S$^3$-DA effectively estimates receptive fields for query points belonging to any category, enabling them to generate robust query features. Besides, we propose a Multi-classification-based Shape$\&$Scale Matching (MSM) loss to supervise the above process. Extensive experiments on KITTI and Waymo Open datasets demonstrate that S$^3$-DA significantly improves the detection accuracy, yielding state-of-the-art performance of single-category and multi-category 3D object detection in a single training process compared to the existing approaches. The source code will be made publicly available at https://github.com/mikasa3lili/S3-MonoDETR.
</details>
<details>
<summary>摘要</summary>
近期，基于转换器的方法在单视图3D对象检测中表现出色，可以从单个2D图像中预测3D属性。这些方法通常使用视觉和深度表示来生成查询点对象，其质量决定了检测精度。然而，当前无监督的注意机制无法捕捉物体的几何形态特征，导致生成查询点的特征具有噪声，从而限制网络性能，同时使得模型在单个训练过程中检测多个类别对象的能力很差。为解决这个问题，这篇论文提出了一种新的“supervised shape&scale-perceptive deformable attention”（S$^3$-DA）模块。具体来说，S$^3$-DA使用视觉和深度特征来生成多种形态和比例的多种本地特征，并同时预测匹配分布，以便对每个查询点进行有价值的形态&比例感知。由此，S$^3$-DA可以fficiently估算查询点所属类别的接受范围，使其能生成 Robust 的查询特征。此外，我们提出了一种基于多类别匹配的Shape$\&$Scale Matching（MSM）损失函数来监督上述过程。经验表明，S$^3$-DA可以显著提高检测精度，在单个训练过程中实现单个类别和多个类别3D对象检测的状态机器人性能。代码将在https://github.com/mikasa3lili/S3-MonoDETR中公开。
</details></li>
</ul>
<hr>
<h2 id="Correlated-and-Multi-frequency-Diffusion-Modeling-for-Highly-Under-sampled-MRI-Reconstruction"><a href="#Correlated-and-Multi-frequency-Diffusion-Modeling-for-Highly-Under-sampled-MRI-Reconstruction" class="headerlink" title="Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction"></a>Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00853">http://arxiv.org/abs/2309.00853</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yqx7150/cm-dm">https://github.com/yqx7150/cm-dm</a></li>
<li>paper_authors: Yu Guan, Chuanming Yu, Shiyu Lu, Zhuoxu Cui, Dong Liang, Qiegen Liu<br>for:This paper aims to improve the accuracy of Magnetic Resonance Imaging (MRI) reconstruction by leveraging the properties of k-space data and the diffusion process to preserve fine texture details in the reconstructed image.methods:The proposed method uses a combination of multi-frequency prior and high-frequency prior extractors to mine the multi-frequency prior and preserve fine texture details in the reconstructed image. Additionally, the method uses a diffusion process to accelerate the sampling process.results:The proposed method successfully obtains more accurate reconstruction and outperforms state-of-the-art methods, as verified by experimental results.<details>
<summary>Abstract</summary>
Most existing MRI reconstruction methods perform tar-geted reconstruction of the entire MR image without tak-ing specific tissue regions into consideration. This may fail to emphasize the reconstruction accuracy on im-portant tissues for diagnosis. In this study, leveraging a combination of the properties of k-space data and the diffusion process, our novel scheme focuses on mining the multi-frequency prior with different strategies to pre-serve fine texture details in the reconstructed image. In addition, a diffusion process can converge more quickly if its target distribution closely resembles the noise distri-bution in the process. This can be accomplished through various high-frequency prior extractors. The finding further solidifies the effectiveness of the score-based gen-erative model. On top of all the advantages, our method improves the accuracy of MRI reconstruction and accel-erates sampling process. Experimental results verify that the proposed method successfully obtains more accurate reconstruction and outperforms state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
现有的MRI重建方法大多都是针对整个MRI图像进行targeted重建，而不考虑特定组织区域的重建精度。这可能会导致重建精度不足。在本研究中，我们运用了k-space数据的特性和扩散过程的优化方法，专注于保留重建图像中的细节Texture。此外，扩散过程可以更快地落实，如果其目标分布类似于处理中的噪音分布。这可以通过多种高频率优化抽取器来实现。发现更加强化了Score-based生成模型的有效性。除了所有的优点，我们的方法可以提高MRI重建精度和加速抽取过程。实验结果显示，我们的方法成功地取得了更高精度的重建和超越了现有的方法。
</details></li>
</ul>
<hr>
<h2 id="Multi-scale-Data-driven-and-Anatomically-Constrained-Deep-Learning-Image-Registration-for-Adult-and-Fetal-Echocardiography"><a href="#Multi-scale-Data-driven-and-Anatomically-Constrained-Deep-Learning-Image-Registration-for-Adult-and-Fetal-Echocardiography" class="headerlink" title="Multi-scale, Data-driven and Anatomically Constrained Deep Learning Image Registration for Adult and Fetal Echocardiography"></a>Multi-scale, Data-driven and Anatomically Constrained Deep Learning Image Registration for Adult and Fetal Echocardiography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00831">http://arxiv.org/abs/2309.00831</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kamruleee51/ddc-ac-dlir">https://github.com/kamruleee51/ddc-ac-dlir</a></li>
<li>paper_authors: Md. Kamrul Hasan, Haobo Zhu, Guang Yang, Choon Hwai Yap<br>for: 这个论文的目的是提高电子医学图像注册的精度和稳定性，以便更好地估计心脏运动、肌肉弹性评估和心脏膜量测量。methods: 这篇论文提出了一种基于深度学习的图像注册方法，包括使用适应性损失函数保持图像的生物学可能性和图像质量，以及在成人和胎儿电子医学图像上进行多尺度训练。results: 实验结果表明，这种方法可以在成人和胎儿电子医学图像上提供优秀的注册结果，并且比传统的非深度学习标准注册方法（如摩托流和弹性投影）更精度和稳定。这些结果表明，这种方法可以提高心脏运动量的估计精度，并且可能有翻译性。<details>
<summary>Abstract</summary>
Temporal echocardiography image registration is a basis for clinical quantifications such as cardiac motion estimation, myocardial strain assessments, and stroke volume quantifications. In past studies, deep learning image registration (DLIR) has shown promising results and is consistently accurate and precise, requiring less computational time. We propose that a greater focus on the warped moving image's anatomic plausibility and image quality can support robust DLIR performance. Further, past implementations have focused on adult echocardiography, and there is an absence of DLIR implementations for fetal echocardiography. We propose a framework that combines three strategies for DLIR in both fetal and adult echo: (1) an anatomic shape-encoded loss to preserve physiological myocardial and left ventricular anatomical topologies in warped images; (2) a data-driven loss that is trained adversarially to preserve good image texture features in warped images; and (3) a multi-scale training scheme of a data-driven and anatomically constrained algorithm to improve accuracy. Our tests show that good anatomical topology and image textures are strongly linked to shape-encoded and data-driven adversarial losses. They improve different aspects of registration performance in a non-overlapping way, justifying their combination. Despite fundamental distinctions between adult and fetal echo images, we show that these strategies can provide excellent registration results in both adult and fetal echocardiography using the publicly available CAMUS adult echo dataset and our private multi-demographic fetal echo dataset. Our approach outperforms traditional non-DL gold standard registration approaches, including Optical Flow and Elastix. Registration improvements could be translated to more accurate and precise clinical quantification of cardiac ejection fraction, demonstrating a potential for translation.
</details>
<details>
<summary>摘要</summary>
Temporal echo医学像registration是临床量化的基础，如心动量估计、肌动强度评估和心脏血量量化。在过去的研究中，深度学习图像registratin（DLIR）已经表现出了承诺的结果，需要更少的计算时间。我们建议更重视扭曲移动图像的解剖学可能性和图像质量，以支持Robust DLIR性能。此外，过去的实现都是成人 echo， absence of DLIR实现对妊娠 echo。我们提出了一个框架， combinesthree策略来实现DLIR：（1）适应解剖形状编码损失，以保持生理physiological myocardial和左心脏解剖特征在扭曲图像中;（2）基于数据驱动的损失，通过对扭曲图像进行对抗训练，以保持好的图像特征;（3）基于多尺度的数据驱动和解剖限制的算法来提高准确性。我们的测试表明，解剖学可能性和图像特征是紧密相关的，这些损失可以在不相互干扰的情况下提高不同方面的registratin性能。尽管成人和妊娠 echo图像存在fundamental的区别，我们的方法可以在两种不同的echo图像上提供优秀的registratin结果，使用公共可用的CAMUS成人echo数据集和我们私有的多demographic妊娠echo数据集。我们的方法超过了传统的非DL金标 registration方法，包括激光流和Elastix。registration改进可以翻译到更准确和精确的临床量化， demonstrating a potential for translation。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer"><a href="#Enhancing-Cardiac-MRI-Segmentation-via-Classifier-Guided-Two-Stage-Network-and-All-Slice-Information-Fusion-Transformer" class="headerlink" title="Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer"></a>Enhancing Cardiac MRI Segmentation via Classifier-Guided Two-Stage Network and All-Slice Information Fusion Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00800">http://arxiv.org/abs/2309.00800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao Chen, Xiao Chen, Yikang Liu, Eric Z. Chen, Terrence Chen, Shanhui Sun</li>
<li>for:  This paper aims to improve the accuracy of left ventricle (LV), right ventricle (RV), and LV myocardium (MYO) segmentation in Cardiac Magnetic Resonance imaging (CMR) images.</li>
<li>methods:  The proposed method uses a classifier-guided two-stage network with an all-slice fusion transformer to enhance CMR segmentation accuracy, particularly in basal and apical slices.</li>
<li>results:  The proposed method demonstrated better performance in terms of Dice score compared to previous CNN-based and transformer-based models, and produced visually appealing segmentation shapes resembling human annotations.<details>
<summary>Abstract</summary>
Cardiac Magnetic Resonance imaging (CMR) is the gold standard for assessing cardiac function. Segmenting the left ventricle (LV), right ventricle (RV), and LV myocardium (MYO) in CMR images is crucial but time-consuming. Deep learning-based segmentation methods have emerged as effective tools for automating this process. However, CMR images present additional challenges due to irregular and varying heart shapes, particularly in basal and apical slices. In this study, we propose a classifier-guided two-stage network with an all-slice fusion transformer to enhance CMR segmentation accuracy, particularly in basal and apical slices. Our method was evaluated on extensive clinical datasets and demonstrated better performance in terms of Dice score compared to previous CNN-based and transformer-based models. Moreover, our method produces visually appealing segmentation shapes resembling human annotations and avoids common issues like holes or fragments in other models' segmentations.
</details>
<details>
<summary>摘要</summary>
心脏磁共振成像（CMR）是评估心脏功能的标准。在CMR图像中，正确地分割左心室（LV）、右心室（RV）和心肺肉（MYO）是关键，但是时间消耗很长。深度学习基于的分割方法在 automatize 这个过程中表现出了有效性。然而，CMR图像受到心形状的变化和不规则性的影响，特别是在基层和腰层图像中。在这项研究中，我们提议一种基于分类器的两stage网络，使用所有slice fusions transformer来提高CMR分割精度，特别是在基层和腰层图像中。我们的方法在丰富的临床数据集上进行了评估，并表现出了与前一代CNN基于和transformer基于模型相比的更好的性能， measured by dice score。此外，我们的方法生成的分割形状更加可观，与人工标注更加相似，而不会出现其他模型中的孔洞或 Fragment 问题。
</details></li>
</ul>
<hr>
<h2 id="Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera"><a href="#Online-Targetless-Radar-Camera-Extrinsic-Calibration-Based-on-the-Common-Features-of-Radar-and-Camera" class="headerlink" title="Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera"></a>Online Targetless Radar-Camera Extrinsic Calibration Based on the Common Features of Radar and Camera</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00787">http://arxiv.org/abs/2309.00787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Cheng, Siyang Cao</li>
<li>For: 提高自适应驾驶和自动机器人系统的准确性和可靠性，通过雷达和摄像头系统的协同报告来实现更好的系统性能。* Methods: 利用深度学习提取雷达数据和摄像头图像的共同特征，然后使用这些共同特征来匹配雷达和摄像头系统中的同一个目标。通过这种方法，可以实现无目标的在线协同准确性报告。* Results: 通过实验示范，提出的方法可以准确地匹配雷达和摄像头系统中的同一个目标，并且可以提高准确性和可靠性。<details>
<summary>Abstract</summary>
Sensor fusion is essential for autonomous driving and autonomous robots, and radar-camera fusion systems have gained popularity due to their complementary sensing capabilities. However, accurate calibration between these two sensors is crucial to ensure effective fusion and improve overall system performance. Calibration involves intrinsic and extrinsic calibration, with the latter being particularly important for achieving accurate sensor fusion. Unfortunately, many target-based calibration methods require complex operating procedures and well-designed experimental conditions, posing challenges for researchers attempting to reproduce the results. To address this issue, we introduce a novel approach that leverages deep learning to extract a common feature from raw radar data (i.e., Range-Doppler-Angle data) and camera images. Instead of explicitly representing these common features, our method implicitly utilizes these common features to match identical objects from both data sources. Specifically, the extracted common feature serves as an example to demonstrate an online targetless calibration method between the radar and camera systems. The estimation of the extrinsic transformation matrix is achieved through this feature-based approach. To enhance the accuracy and robustness of the calibration, we apply the RANSAC and Levenberg-Marquardt (LM) nonlinear optimization algorithm for deriving the matrix. Our experiments in the real world demonstrate the effectiveness and accuracy of our proposed method.
</details>
<details>
<summary>摘要</summary>
感知融合是自动驾驶和自动机器人的关键技术，而各种各样的探测器融合系统已经得到了广泛应用。然而，为了实现有效的探测融合，准确的均衡化是非常重要。均衡化包括内在均衡和外在均衡，其中外在均衡尤其重要，以确保探测器之间的准确匹配。然而，许多目标基本均衡方法需要复杂的操作程序和优化的实验条件，这会让研究人员很难复制结果。为解决这个问题，我们提出了一种新的方法，利用深度学习来提取各种探测器数据中的共同特征（即距离-Doppler-角度数据）和摄像头图像。而不是直接表示这些共同特征，我们的方法即使利用这些共同特征来匹配探测器数据和摄像头图像中的同一个目标。具体来说，提取的共同特征可以作为一个在线无目标均衡方法的示例，用于 estimating 探测器和摄像头系统之间的外在变换矩阵。为了提高准确性和稳定性，我们使用RANSAC和Levenberg-Marquardt（LM）非线性优化算法来 derivation 矩阵。我们在实际情况中进行了实验，并证明了我们提出的方法的有效性和准确性。
</details></li>
</ul>
<hr>
<h2 id="Full-Reference-Video-Quality-Assessment-for-Machine-Learning-Based-Video-Codecs"><a href="#Full-Reference-Video-Quality-Assessment-for-Machine-Learning-Based-Video-Codecs" class="headerlink" title="Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs"></a>Full Reference Video Quality Assessment for Machine Learning-Based Video Codecs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00769">http://arxiv.org/abs/2309.00769</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abrar Majeedi, Babak Naderi, Yasaman Hosseinkashi, Juhee Cho, Ruben Alvarez Martinez, Ross Cutler</li>
<li>for: 这 paper 的目的是提出一种新的视频编码评估方法，以便更好地评估基于机器学习的视频编码器。</li>
<li>methods: 该 paper 使用了一种新的数据集和一种基于 FRVQA 模型来评估机器学习视频编码器的质量。</li>
<li>results: 该 paper 提出了一种新的评估方法，其 Pearson 相关系数和Spearman 排名相关系数都达到了 0.99，可以帮助更好地评估基于机器学习的视频编码器。<details>
<summary>Abstract</summary>
Machine learning-based video codecs have made significant progress in the past few years. A critical area in the development of ML-based video codecs is an accurate evaluation metric that does not require an expensive and slow subjective test. We show that existing evaluation metrics that were designed and trained on DSP-based video codecs are not highly correlated to subjective opinion when used with ML video codecs due to the video artifacts being quite different between ML and video codecs. We provide a new dataset of ML video codec videos that have been accurately labeled for quality. We also propose a new full reference video quality assessment (FRVQA) model that achieves a Pearson Correlation Coefficient (PCC) of 0.99 and a Spearman's Rank Correlation Coefficient (SRCC) of 0.99 at the model level. We make the dataset and FRVQA model open source to help accelerate research in ML video codecs, and so that others can further improve the FRVQA model.
</details>
<details>
<summary>摘要</summary>
machine learning基于的视频编码器在过去几年中做出了重大进步。一个关键的发展领域是一个准确的评估指标，不需要贵重和慢的主观测试。我们表明了现有的评估指标，由DSP基于的视频编码器而设计和训练的，与ML视频编码器不具有高相关性，因为视频artefacts在ML和视频编码器之间很不同。我们提供了一个新的ML视频编码器视频集，已经准确地标注了质量。我们还提议一种全参照视频质量评估（FRVQA）模型，实现了Pearson相关系数（PCC）0.99和Spearman排名相关系数（SRCC）0.99的模型水平。我们将数据集和FRVQA模型开源，以便加速ML视频编码器的研究，并让其他人可以进一步改进FRVQA模型。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/02/eess.IV_2023_09_02/" data-id="clmjn91qt00hf0j887ehe5ri8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_09_01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/09/01/cs.SD_2023_09_01/" class="article-date">
  <time datetime="2023-09-01T15:00:00.000Z" itemprop="datePublished">2023-09-01</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/09/01/cs.SD_2023_09_01/">cs.SD - 2023-09-01</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding"><a href="#CoNeTTE-An-efficient-Audio-Captioning-system-leveraging-multiple-datasets-with-Task-Embedding" class="headerlink" title="CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding"></a>CoNeTTE: An efficient Audio Captioning system leveraging multiple datasets with Task Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00454">http://arxiv.org/abs/2309.00454</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/topel/audioset-convnext-inf">https://github.com/topel/audioset-convnext-inf</a></li>
<li>paper_authors: Étienne Labbé, Thomas Pellegrini, Julien Pinquier</li>
<li>For: The paper focuses on developing an automated audio captioning (AAC) model using a ConvNeXt architecture as the audio encoder, which is adapted from the vision domain to audio classification. The model is designed to generate natural language descriptions of audio content using encoder-decoder architectures.* Methods: The proposed model, called CNext-trans, uses a ConvNeXt architecture as the audio encoder, which is combined with a Transformer decoder for caption generation. The model is trained on multiple AAC datasets, including AC, CL, MACS, and WavCaps, to improve cross-dataset performance. The authors also introduced a Task Embedding (TE) token to identify the source dataset for each input sample and improve the model’s performance.* Results: The proposed model, named CoNeTTE, achieved state-of-the-art scores on the AudioCaps (AC) dataset and performed competitively on Clotho (CL) while using four to forty times fewer parameters than existing models. The model also showed the ability to generate more accurate and informative captions by incorporating dataset-specific Task Embeddings. The SPIDEr scores of the proposed model were 44.1% and 30.5% on AC and CL, respectively.<details>
<summary>Abstract</summary>
Automated Audio Captioning (AAC) involves generating natural language descriptions of audio content, using encoder-decoder architectures. An audio encoder produces audio embeddings fed to a decoder, usually a Transformer decoder, for caption generation. In this work, we describe our model, which novelty, compared to existing models, lies in the use of a ConvNeXt architecture as audio encoder, adapted from the vision domain to audio classification. This model, called CNext-trans, achieved state-of-the-art scores on the AudioCaps (AC) dataset and performed competitively on Clotho (CL), while using four to forty times fewer parameters than existing models. We examine potential biases in the AC dataset due to its origin from AudioSet by investigating unbiased encoder's impact on performance. Using the well-known PANN's CNN14, for instance, as an unbiased encoder, we observed a 1.7% absolute reduction in SPIDEr score (where higher scores indicate better performance). To improve cross-dataset performance, we conducted experiments by combining multiple AAC datasets (AC, CL, MACS, WavCaps) for training. Although this strategy enhanced overall model performance across datasets, it still fell short compared to models trained specifically on a single target dataset, indicating the absence of a one-size-fits-all model. To mitigate performance gaps between datasets, we introduced a Task Embedding (TE) token, allowing the model to identify the source dataset for each input sample. We provide insights into the impact of these TEs on both the form (words) and content (sound event types) of the generated captions. The resulting model, named CoNeTTE, an unbiased CNext-trans model enriched with dataset-specific Task Embeddings, achieved SPIDEr scores of 44.1% and 30.5% on AC and CL, respectively. Code available: https://github.com/Labbeti/conette-audio-captioning.
</details>
<details>
<summary>摘要</summary>
自动化音频描述（AAC）技术涉及生成音频内容的自然语言描述，使用编码器-解码器架构。音频编码器生成音频嵌入，并将其传递给解码器进行描述生成。在这种工作中，我们描述了我们的模型，其特点在于使用ConvNeXt架构作为音频编码器，从视觉领域中适应音频分类。这个模型被称为CNext-trans，在AudioCaps（AC）数据集上实现了状态机器的分数，并在Clotho（CL）数据集上表现竞争力强，而使用四到四十个参数少于现有模型。我们调查了AC数据集中可能的偏见，并证明使用不偏见的编码器对性能有负面的影响。使用PANN的CNN14，例如，作为不偏见的编码器，我们观察到了相对于SPIDEr分数的1.7%绝对下降。为提高跨数据集性能，我们进行了多个AAC数据集（AC、CL、MACS、WavCaps）的训练。虽然这种策略提高了总模型性能，但仍然落后于特定目标数据集上训练的模型，表明没有一size-fits-all的模型。为 Mitigate performance gaps between datasets，我们引入了任务嵌入（TE）token，让模型可以确定输入样本的源数据集。我们对TE的影响进行了深入的分析，并发现TE对描述（words）和内容（声音事件类型）的生成caption均有正面的影响。最终，我们提出了CoNeTTE模型，即不偏见的CNext-trans模型，以及数据集特定的任务嵌入，实现了SPIDEr分数的44.1%和30.5%在AC和CL数据集上。代码可以在https://github.com/Labbeti/conette-audio-captioning中下载。
</details></li>
</ul>
<hr>
<h2 id="Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining"><a href="#Learning-Speech-Representation-From-Contrastive-Token-Acoustic-Pretraining" class="headerlink" title="Learning Speech Representation From Contrastive Token-Acoustic Pretraining"></a>Learning Speech Representation From Contrastive Token-Acoustic Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00424">http://arxiv.org/abs/2309.00424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyu Qiang, Hao Li, Yixin Tian, Ruibo Fu, Tao Wang, Longbiao Wang, Jianwu Dang</li>
<li>for: 这个研究是为了提出一种名为“对照掌握（CTAP）”的方法，用于将语音和字幕带到一个共同多modal空间中，以学习将语音和字幕连接在帧级上。</li>
<li>methods: 这个方法使用两个Encoder来将语音和字幕带到一个共同多modal空间中，并通过对帧级的连接学习，以提高下游任务的精确性。</li>
<li>results: 这个CTAP方法在210k语音和字幕文本对中训练，并在无监督的情况下进行语音转换、自动话语识别和文本读取等下游任务，获得了良好的成果。<details>
<summary>Abstract</summary>
For fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representations extracted from speech should serve as a "bridge" between text and acoustic information, containing information from both modalities. The semantic content is emphasized, while the paralinguistic information such as speaker identity and acoustic details should be de-emphasized. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Contrastive learning is a good method for modeling intermediate representations from two modalities. However, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named "Contrastive Token-Acoustic Pretraining (CTAP)", which uses two encoders to bring phoneme and speech into a joint multimodal space, learning how to connect phoneme and speech at the frame level. The CTAP model is trained on 210k speech and phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a promising solution for fine-grained generation and recognition downstream tasks in speech processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXTFor fine-grained generation and recognition tasks such as minimally-supervised text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), the intermediate representations extracted from speech should serve as a "bridge" between text and acoustic information, containing information from both modalities. The semantic content is emphasized, while the paralinguistic information such as speaker identity and acoustic details should be de-emphasized. However, existing methods for extracting fine-grained intermediate representations from speech suffer from issues of excessive redundancy and dimension explosion. Contrastive learning is a good method for modeling intermediate representations from two modalities. However, existing contrastive learning methods in the audio field focus on extracting global descriptive information for downstream audio classification tasks, making them unsuitable for TTS, VC, and ASR tasks. To address these issues, we propose a method named "Contrastive Token-Acoustic Pretraining (CTAP)", which uses two encoders to bring phoneme and speech into a joint multimodal space, learning how to connect phoneme and speech at the frame level. The CTAP model is trained on 210k speech and phoneme text pairs, achieving minimally-supervised TTS, VC, and ASR. The proposed CTAP method offers a promising solution for fine-grained generation and recognition downstream tasks in speech processing.TRANSLATE_TEXT
</details></li>
</ul>
<hr>
<h2 id="Remixing-based-Unsupervised-Source-Separation-from-Scratch"><a href="#Remixing-based-Unsupervised-Source-Separation-from-Scratch" class="headerlink" title="Remixing-based Unsupervised Source Separation from Scratch"></a>Remixing-based Unsupervised Source Separation from Scratch</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00376">http://arxiv.org/abs/2309.00376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Saijo, Tetsuji Ogawa</li>
<li>for: 本研究旨在提出一种无监督的方法，用于从零开始训练分离模型，使用 RecmixIT 和 Self-Remixing 等自动学习方法来精炼预训练模型。</li>
<li>methods: 本方法首先使用一个教师模型将混合物分离，然后创建 pseudo-混合物，将混合物的信号随机排序和重新混合。学生模型则使用教师模型的输出或初始混合物作为监督来分离 pseudo-混合物。为了改进教师模型的输出，教师模型的参数将被更新为学生模型的参数。</li>
<li>results: 实验结果表明，提议的方法可以超越现有的混合性训练方法，并且可以在无监督情况下训练一个声学分离模型。此外，我们还提出了一种简单的混合方法来稳定训练。<details>
<summary>Abstract</summary>
We propose an unsupervised approach for training separation models from scratch using RemixIT and Self-Remixing, which are recently proposed self-supervised learning methods for refining pre-trained models. They first separate mixtures with a teacher model and create pseudo-mixtures by shuffling and remixing the separated signals. A student model is then trained to separate the pseudo-mixtures using either the teacher's outputs or the initial mixtures as supervision. To refine the teacher's outputs, the teacher's weights are updated with the student's weights. While these methods originally assumed that the teacher is pre-trained, we show that they are capable of training models from scratch. We also introduce a simple remixing method to stabilize training. Experimental results demonstrate that the proposed approach outperforms mixture invariant training, which is currently the only available approach for training a monaural separation model from scratch.
</details>
<details>
<summary>摘要</summary>
我们提出一种无监督的方法，使用RecmixIT和Self-Remixing来训练从头开始的分离模型。这些方法最初将混合物分离成多个信号，然后通过乱序和重新混合这些信号来生成 pseudo-混合物。然后，一个学生模型将被训练来分离 pseudo-混合物，使用教师模型的输出或初始混合物作为监督。为了改善教师的输出，教师的权重将被更新为学生的权重。尽管这些方法原本假设了教师已经预训练，但我们证明它们可以训练模型从头开始。我们还提出了一种简单的混合方法来稳定训练。实验结果表明，我们的方法在训练离合模型从头开始时比混合 invariant 训练更高效。
</details></li>
</ul>
<hr>
<h2 id="Towards-Contrastive-Learning-in-Music-Video-Domain"><a href="#Towards-Contrastive-Learning-in-Music-Video-Domain" class="headerlink" title="Towards Contrastive Learning in Music Video Domain"></a>Towards Contrastive Learning in Music Video Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00347">http://arxiv.org/abs/2309.00347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karel Veldkamp, Mariya Hendriksen, Zoltán Szlávik, Alexander Keijser</li>
<li>for: 本研究是 investigate whether contrastive learning can be applied to the domain of music videos, and evaluate the quality of learned representations on downstream tasks.</li>
<li>methods: 我们使用 dual en-coder for audio and video modalities, and train it using a bidirectional contrastive loss.</li>
<li>results: 我们发现，没有对比学习练化后，pre-trained networks still outperform our contrastive learning approach on both music tagging and genre classification tasks.  Additionally, we perform a qualitative analysis of the learned representations and find that contrastive learning may have difficulties uniting embeddings from two modalities.<details>
<summary>Abstract</summary>
Contrastive learning is a powerful way of learning multimodal representations across various domains such as image-caption retrieval and audio-visual representation learning. In this work, we investigate if these findings generalize to the domain of music videos. Specifically, we create a dual en-coder for the audio and video modalities and train it using a bidirectional contrastive loss. For the experiments, we use an industry dataset containing 550 000 music videos as well as the public Million Song Dataset, and evaluate the quality of learned representations on the downstream tasks of music tagging and genre classification. Our results indicate that pre-trained networks without contrastive fine-tuning outperform our contrastive learning approach when evaluated on both tasks. To gain a better understanding of the reasons contrastive learning was not successful for music videos, we perform a qualitative analysis of the learned representations, revealing why contrastive learning might have difficulties uniting embeddings from two modalities. Based on these findings, we outline possible directions for future work. To facilitate the reproducibility of our results, we share our code and the pre-trained model.
</details>
<details>
<summary>摘要</summary>
“对比学习是一种强大的多modal表现学习方法，应用于不同领域，如图像描述和多频道表现学习。在这个工作中，我们探索是否这些发现可以应用到音乐类别中。我们建立了一个双向的en-coder，用于音频和视频模式，并使用对称对应损失来训练。我们使用了550,000首音乐录影带和公共的百万首歌曲Dataset，并评估学习的表现质量在下游任务中，包括音乐标签和流派分类。我们发现，无需对照精练，预训练的网络在两个任务中都能表现出色。为了更好地理解为什么对照学习无法成功地融合两个模式的嵌入，我们进行了质量分析，发现对照学习可能会面临两个模式之间的问题。基于这些发现，我们提出了未来的可能的方向。为了促进我们的结果的重复性，我们分享了我们的代码和预训练模型。”
</details></li>
</ul>
<hr>
<h2 id="Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training"><a href="#Enhancing-the-vocal-range-of-single-speaker-singing-voice-synthesis-with-melody-unsupervised-pre-training" class="headerlink" title="Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training"></a>Enhancing the vocal range of single-speaker singing voice synthesis with melody-unsupervised pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00284">http://arxiv.org/abs/2309.00284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaohuan Zhou, Xu Li, Zhiyong Wu, Ying Shan, Helen Meng</li>
<li>for: 提高单声道合唱voice synthesis（SVS）的音高范围和音质相似性</li>
<li>methods: 使用多个 singer 预训练方法，包括预测器来生成框架级别的音频信息和歌手编码器来模型不同歌手的声音变化</li>
<li>results: 提高单声道合唱voice synthesis（SVS）的音质和节奏自然性，并超过基线的评价<details>
<summary>Abstract</summary>
The single-speaker singing voice synthesis (SVS) usually underperforms at pitch values that are out of the singer's vocal range or associated with limited training samples. Based on our previous work, this work proposes a melody-unsupervised multi-speaker pre-training method conducted on a multi-singer dataset to enhance the vocal range of the single-speaker, while not degrading the timbre similarity. This pre-training method can be deployed to a large-scale multi-singer dataset, which only contains audio-and-lyrics pairs without phonemic timing information and pitch annotation. Specifically, in the pre-training step, we design a phoneme predictor to produce the frame-level phoneme probability vectors as the phonemic timing information and a speaker encoder to model the timbre variations of different singers, and directly estimate the frame-level f0 values from the audio to provide the pitch information. These pre-trained model parameters are delivered into the fine-tuning step as prior knowledge to enhance the single speaker's vocal range. Moreover, this work also contributes to improving the sound quality and rhythm naturalness of the synthesized singing voices. It is the first to introduce a differentiable duration regulator to improve the rhythm naturalness of the synthesized voice, and a bi-directional flow model to improve the sound quality. Experimental results verify that the proposed SVS system outperforms the baseline on both sound quality and naturalness.
</details>
<details>
<summary>摘要</summary>
通常情况下，单个说话者的声音合成（SVS）在声音值出现在说话者的 vocal range 之外或与有限的训练样本有关时会下降性能。基于我们的前一项工作，这项工作提出了一种不含调谱信息和普通时间信息的多说话者预训练方法，以提高单个说话者的声音范围，不会影响声音相似性。这种预训练方法可以应用于大规模多说话者数据集，只包含音频和歌词对。具体来说，在预训练阶段，我们设计了一个音频预测器，以生成帧级别的音频概率向量作为调谱信息，并使用说话者编码器来模型不同说话者的声音变化。这些预训练模型参数在精度阶段作为先知知识传递给单个说话者的声音合成，以提高其声音范围。此外，这项工作还贡献到了改善合成唱 voz 的音质和节奏自然性。它是首次引入了可导的时间控制器，以改善合成唱 voz 的节奏自然性，并使用双向流模型，以改善音质。实验结果表明，提案的 SVS 系统在音质和自然性两个指标上都超过了基线。
</details></li>
</ul>
<hr>
<h2 id="The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge"><a href="#The-FruitShell-French-synthesis-system-at-the-Blizzard-2023-Challenge" class="headerlink" title="The FruitShell French synthesis system at the Blizzard 2023 Challenge"></a>The FruitShell French synthesis system at the Blizzard 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2309.00223">http://arxiv.org/abs/2309.00223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xin Qi, Xiaopeng Wang, Zhiyong Wang, Wang Liu, Mingming Ding, Shuchen Shi</li>
<li>for: 这个论文是为了参加2023年的Blizzard Challenge提交的法语文本识别系统。</li>
<li>methods: 作者使用了一种基于VITS的声学模型和hifigan vocoder，并在模型中包含了发音人信息。</li>
<li>results: 论文的评估结果显示，Hub任务的质量MOS分数为3.6，Spoke任务的质量MOS分数为3.4，与所有参赛队伍的平均水平相当。<details>
<summary>Abstract</summary>
This paper presents a French text-to-speech synthesis system for the Blizzard Challenge 2023. The challenge consists of two tasks: generating high-quality speech from female speakers and generating speech that closely resembles specific individuals. Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience. For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data. Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model. The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details>
<details>
<summary>摘要</summary>
Regarding the competition data, we conducted a screening process to remove missing or erroneous text data. We organized all symbols except for phonemes and eliminated symbols that had no pronunciation or zero duration. Additionally, we added word boundary and start/end symbols to the text, which we have found to improve speech quality based on our previous experience.For the Spoke task, we performed data augmentation according to the competition rules. We used an open-source G2P model to transcribe the French texts into phonemes. As the G2P model uses the International Phonetic Alphabet (IPA), we applied the same transcription process to the provided competition data for standardization. However, due to compiler limitations in recognizing special symbols from the IPA chart, we followed the rules to convert all phonemes into the phonetic scheme used in the competition data.Finally, we resampled all competition audio to a uniform sampling rate of 16 kHz. We employed a VITS-based acoustic model with the hifigan vocoder. For the Spoke task, we trained a multi-speaker model and incorporated speaker information into the duration predictor, vocoder, and flow layers of the model.The evaluation results of our system showed a quality MOS score of 3.6 for the Hub task and 3.4 for the Spoke task, placing our system at an average level among all participating teams.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/09/01/cs.SD_2023_09_01/" data-id="clmjn91o900bl0j8846zrep2x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/33/">33</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">26</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">73</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">32</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">69</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">112</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">169</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/1970/01/">January 1970</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
