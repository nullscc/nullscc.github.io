
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/4/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.CV_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.CV_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T13:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.CV_2023_11_16/">cs.CV - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CV-Attention-UNet-Attention-based-UNet-for-3D-Cerebrovascular-Segmentation-of-Enhanced-TOF-MRA-Images"><a href="#CV-Attention-UNet-Attention-based-UNet-for-3D-Cerebrovascular-Segmentation-of-Enhanced-TOF-MRA-Images" class="headerlink" title="CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images"></a>CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10224">http://arxiv.org/abs/2311.10224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Farhan Abbas, Nguyen Thanh Duc, Yoonguu Song, Kyungwon Kim, Boreom Lee<br>for: 这个研究的目的是精确地分类脑血管图像，以帮助诊断脑血管疾病。methods: 这个研究使用了3D脑血管注意力UNet方法，named CV-AttentionUNet，来精确地提取脑血管图像。这个方法包括了一系列的预处理技术和深度超级vised UNet，以提高脑血管分类的精度。此外，这个方法还使用了注意力机制，以专注于相关的相互关联，并忽略无关的生物学信息。results: 我们的研究表明，CV-AttentionUNet方法可以对于脑血管分类task中的脑血管图像进行高精度的分类，并且在TubeTK dataset上表现比现有的state-of-the-art方法更好。<details>
<summary>Abstract</summary>
Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, we applied the attention mechanism. This mechanism focuses on relevant associations and neglects irrelevant anatomical information. Furthermore, the inclusion of deep supervision incorporates different levels of features that prove to be beneficial for network convergence. Results: We demonstrate the efficiency of the proposed method by cross-validating with an unlabeled dataset, which was further labeled by us. We believe that the novelty of this algorithm lies in its ability to perform well on both labeled and unlabeled data with image processing-based enhancement. The results indicate that our method performed better than the existing state-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method will help in accurate segmentation of cerebrovascular structure leading to stroke
</details>
<details>
<summary>摘要</summary>
due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. the commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. methods: in this paper, we attempt to solve these limitations and propose the 3d cerebrovascular attention UNet method, named cv-attentionunet, for precise extraction of brain vessel images. we proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. to combine the low and high semantics, we applied the attention mechanism. this mechanism focuses on relevant associations and neglects irrelevant anatomical information. furthermore, the inclusion of deep supervision incorporates different levels of features that prove to be beneficial for network convergence. results: we demonstrate the efficiency of the proposed method by cross-validating with an unlabeled dataset, which was further labeled by us. we believe that the novelty of this algorithm lies in its ability to perform well on both labeled and unlabeled data with image processing-based enhancement. the results indicate that our method performed better than the existing state-of-the-art methods on the tubetk dataset. conclusion: the proposed method will help in accurate segmentation of cerebrovascular structure leading to stroke.
</details></li>
</ul>
<hr>
<h2 id="Stella-Nera-Achieving-161-TOp-s-W-with-Multiplier-free-DNN-Acceleration-based-on-Approximate-Matrix-Multiplication"><a href="#Stella-Nera-Achieving-161-TOp-s-W-with-Multiplier-free-DNN-Acceleration-based-on-Approximate-Matrix-Multiplication" class="headerlink" title="Stella Nera: Achieving 161 TOp&#x2F;s&#x2F;W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication"></a>Stella Nera: Achieving 161 TOp&#x2F;s&#x2F;W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10207">http://arxiv.org/abs/2311.10207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannis Schönleber, Lukas Cavigelli, Renzo Andri, Matteo Perotti, Luca Benini</li>
<li>for: 本文针对精度高、能耗低的MatMul计算问题提出了一种解决方案，以替代传统的MatMul加速器。</li>
<li>methods: 本文使用了一种叫做“Maddness”的方法，通过使用哈希函数和缓存表（LUT）来实现MatMul计算，而不需要直接进行乘法运算。</li>
<li>results: 对于14nm和3nm技术的缩放，本文实现了一个高达161 TOp&#x2F;s&#x2F;<a href="mailto:&#87;&#64;&#x30;&#x2e;&#53;&#53;&#86;">&#87;&#64;&#x30;&#x2e;&#53;&#53;&#86;</a>的能耗效率，并达到了CIFAR-10的Top-1准确率高于92.5% using ResNet9。<details>
<summary>Abstract</summary>
From classical HPC to deep learning, MatMul is at the heart of today's computing. The recent Maddness method approximates MatMul without the need for multiplication by using a hash-based version of product quantization (PQ) indexing into a look-up table (LUT). Stella Nera is the first Maddness accelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more than 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators implemented in the same technology. The hash function is a decision tree, which allows for an efficient hardware implementation as the multiply-accumulate operations are replaced by decision tree passes and LUT lookups. The entire Maddness MatMul can be broken down into parts that allow an effective implementation with small computing units and memories, allowing it to reach extreme efficiency while remaining generically applicable for MatMul tasks. In a commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency of 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using ResNet9.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="K-space-Cold-Diffusion-Learning-to-Reconstruct-Accelerated-MRI-without-Noise"><a href="#K-space-Cold-Diffusion-Learning-to-Reconstruct-Accelerated-MRI-without-Noise" class="headerlink" title="K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise"></a>K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10162">http://arxiv.org/abs/2311.10162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyao Shen, Mengyu Li, Chad W. Farris, Stephan Anderson, Xin Zhang</li>
<li>for: 这 paper 是为了提出一种基于冰晶扩展的 MRI 重建模型，用于快速 MRI 图像重建。</li>
<li>methods: 该模型使用冰晶扩展来实现一般化的图像变换，包括模糊、下采样等操作。</li>
<li>results: 根据对一个大量开源 MRI 数据集的测试，该模型可以生成高质量的重建图像，并且比其他深度学习基于 MRI 重建模型更好。<details>
<summary>Abstract</summary>
Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
</details>
<details>
<summary>摘要</summary>
现在的深度学习基于MRI重建模型已经取得了出色的表现。最近，扩散模型在图像生成、填充、超分解、图像修改等领域都有出色的表现。作为一种通用扩散模型，冷扩散进一步拓宽了范围，考虑了基于任意图像变换的模型，如模糊、下采样等。在这篇论文中，我们提出了基于k空间冷扩散模型的图像劣化和重建方法，无需Gaussian噪声。我们对多种深度学习基于MRI重建模型进行了比较，并在一个著名的大型开源MRI数据集上进行了测试。我们的结果表明，这种新的劣化方法可以生成高质量的重建图像 для加速MRI。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Chosen-One-Consistent-Characters-in-Text-to-Image-Diffusion-Models"><a href="#The-Chosen-One-Consistent-Characters-in-Text-to-Image-Diffusion-Models" class="headerlink" title="The Chosen One: Consistent Characters in Text-to-Image Diffusion Models"></a>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10093">http://arxiv.org/abs/2311.10093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johndpope/TheChosenOne">https://github.com/johndpope/TheChosenOne</a></li>
<li>paper_authors: Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski</li>
<li>for: 文章旨在提供一种完全自动化的一致性人物生成方法，用于文学创作、游戏开发资产设计、广告等实际应用。</li>
<li>methods: 我们提出的方法基于迭代过程，在每一个阶段内，从给定的文本提示中找出一个具有相似identify的图像集，并从这个集合中提取一个更加一致的identify。</li>
<li>results: 我们的方法在量化分析中表现出更好的平衡点，与基准方法相比，并且在用户研究中得到了证实。我们还展示了该方法在各种实际应用中的可行性。<details>
<summary>Abstract</summary>
Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach. Project page is available at https://omriavrahami.com/the-chosen-one
</details>
<details>
<summary>摘要</summary>
近期文本到图像生成模型的进步，推开了大量的视觉创造力。然而，这些模型往往受到一致性的限制，这是许多实际应用中的关键问题，如故事化、游戏开发资产设计、广告等。现有方法通常依赖于多个预存图像或尝试繁琐的手动过程。在这个工作中，我们提出了一种完全自动的一致性Character生成解决方案，唯一的输入是文本提示。我们引入了一种迭代过程，每个阶段都会从一组相似的图像中提取一个更一致的标识。我们的量化分析表明，我们的方法在提示对齐和一致性之间做出了更好的平衡，这些发现得到了用户研究的证实。为了结束，我们展示了一些实际应用场景。项目页面可以在https://omriavrahami.com/the-chosen-one找到。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Video-Object-Detection-using-Motion-Prior"><a href="#Traffic-Video-Object-Detection-using-Motion-Prior" class="headerlink" title="Traffic Video Object Detection using Motion Prior"></a>Traffic Video Object Detection using Motion Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10092">http://arxiv.org/abs/2311.10092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihao Liu, Yanqi Cheng, Dongdong Chen, Jing He, Pietro Liò, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero</li>
<li>for: 提高交通视频中物体检测精度</li>
<li>methods: 使用新的自注意模块和 Pseudo-label 机制，利用运动先驱来增强时间信息Integration和纠正噪声 pseudo label</li>
<li>results: 与现有状态前的方法比，表现出2%的提高，达到更高的检测精度<details>
<summary>Abstract</summary>
Traffic videos inherently differ from generic videos in their stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval. Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios. However, they fail to harness the strength of motion prior to enhance detection accuracy. In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection. Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting. Secondly, we utilise the motion prior to develop a pseudo-labelling mechanism to eliminate noisy pseudo labels for the semi-supervised setting. Both of our motion-prior-centred methods consistently demonstrates superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP.
</details>
<details>
<summary>摘要</summary>
traffic videos 自然 diferen  FROM generic videos 的 stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval. Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios. However, they fail to harness the strength of motion prior to enhance detection accuracy. In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection. Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting. Secondly, we utilize the motion prior to develop a pseudo-labeling mechanism to eliminate noisy pseudo labels for the semi-supervised setting. Both of our motion-prior-centred methods consistently demonstrate superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Shells-for-Efficient-Neural-Radiance-Field-Rendering"><a href="#Adaptive-Shells-for-Efficient-Neural-Radiance-Field-Rendering" class="headerlink" title="Adaptive Shells for Efficient Neural Radiance Field Rendering"></a>Adaptive Shells for Efficient Neural Radiance Field Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10091">http://arxiv.org/abs/2311.10091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Müller, Zan Gojcic</li>
<li>for: 这个论文的目的是提高神经辐射场的渲染速度和视觉质量，通过在各个场景中适当地使用积分和表面基于的渲染方法。</li>
<li>methods: 这个论文使用神经网络来学习积分和表面基于的渲染方法，并通过自适应的积分大小和权重来控制在各个场景中的渲染精度。</li>
<li>results: 实验结果表明，这个方法可以大幅提高渲染速度和视觉质量，并且可以在不同的场景中适应性地应用。此外，这个方法还可以提取出精度高的表面 mesh，用于下游应用such as 动画和仿真。<details>
<summary>Abstract</summary>
Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.
</details>
<details>
<summary>摘要</summary>
Our method constructs an explicit mesh envelope that spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To achieve this, we generalize the NeuS formulation with a learned spatially-varying kernel size that encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with the width determined by the kernel size, and fine-tune the radiance field within this band.At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.
</details></li>
</ul>
<hr>
<h2 id="Visual-Environment-Assessment-for-Safe-Autonomous-Quadrotor-Landing"><a href="#Visual-Environment-Assessment-for-Safe-Autonomous-Quadrotor-Landing" class="headerlink" title="Visual Environment Assessment for Safe Autonomous Quadrotor Landing"></a>Visual Environment Assessment for Safe Autonomous Quadrotor Landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10065">http://arxiv.org/abs/2311.10065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Secchiero, Nishanth Bobbili, Yang Zhou, Giuseppe Loianno</li>
<li>for: 本研究旨在提供一种能够自动检测和评估安全降落区域的方法，以确保无人机在系统故障、低电量或完成特定任务后安全着陆。</li>
<li>methods: 该方法利用神经网络提取环境特征，并与geometry Map结合，以获得环境特征和重要的几何特征，如 Slope、平坦程度和护岸度。然后，根据这些特征，定义多个成本指标来评估环境的安全性、稳定性和适用性，并将最适合的降落区域标识出来。</li>
<li>results: 实验结果表明，该方法可以有效地评估环境中的降落区域，并帮助无人机安全着陆。<details>
<summary>Abstract</summary>
Autonomous identification and evaluation of safe landing zones are of paramount importance for ensuring the safety and effectiveness of aerial robots in the event of system failures, low battery, or the successful completion of specific tasks. In this paper, we present a novel approach for detection and assessment of potential landing sites for safe quadrotor landing. Our solution efficiently integrates 2D and 3D environmental information, eliminating the need for external aids such as GPS and computationally intensive elevation maps. The proposed pipeline combines semantic data derived from a Neural Network (NN), to extract environmental features, with geometric data obtained from a disparity map, to extract critical geometric attributes such as slope, flatness, and roughness. We define several cost metrics based on these attributes to evaluate safety, stability, and suitability of regions in the environments and identify the most suitable landing area. Our approach runs in real-time on quadrotors equipped with limited computational capabilities. Experimental results conducted in diverse environments demonstrate that the proposed method can effectively assess and identify suitable landing areas, enabling the safe and autonomous landing of a quadrotor.
</details>
<details>
<summary>摘要</summary>
自动识别和评估安全降落区的重要性对于保证飞行器的安全性和效率具有极高的重要性，尤其在系统故障、电池低下或完成特定任务后。在这篇论文中，我们提出了一种新的降落点检测和评估方法。我们的解决方案可以快速地将2D和3D环境信息集成，从而消除需要外部帮助（如GPS）和计算昂贵的高程地图。我们的管道使用神经网络（NN）来提取环境特征，并使用不同程度图来提取环境中关键的几何特征，如坡度、平坦度和荒凉程度。我们根据这些特征定义了多个成本指标，以评估环境中区域的安全性、稳定性和适用性，并将最适合的降落区标识出来。我们的方法在飞行器上搭载有有限的计算能力下运行，并在多种环境中进行了实验，证明了我们的方法可以有效地评估和标识适合降落的区域，使飞行器安全地自动降落。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Deviations-of-Dyadic-Lines-in-Fast-Hough-Transform"><a href="#Analyzing-Deviations-of-Dyadic-Lines-in-Fast-Hough-Transform" class="headerlink" title="Analyzing Deviations of Dyadic Lines in Fast Hough Transform"></a>Analyzing Deviations of Dyadic Lines in Fast Hough Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10064">http://arxiv.org/abs/2311.10064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gleb Smirnov, Simon Karpenko</li>
<li>for: 本文研究了dyadic线模型在图像识别中的准确性。</li>
<li>methods: 本文使用了统计分析方法来研究dyadic线模型的偏差。</li>
<li>results: 研究发现，dyadic线模型的偏差的mean值为零，而方差为O(log(n))。随着n的增加，这些偏差的分布会转化为一个正态分布，其中mean值为零，并且具有小的方差。这个限定结果借鉴了 Erdős theory。<details>
<summary>Abstract</summary>
Fast Hough transform is a widely used algorithm in pattern recognition. The algorithm relies on approximating lines using a specific discrete line model called dyadic lines. The worst-case deviation of a dyadic line from the ideal line it used to construct grows as $O(log(n))$, where $n$ is the linear size of the image. But few lines actually reach the worst-case bound. The present paper addresses a statistical analysis of the deviation of a dyadic line from its ideal counterpart. Specifically, our findings show that the mean deviation is zero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution of these (suitably normalized) deviations converges towards a normal distribution with zero mean and a small variance. This limiting result makes an essential use of ergodic theory.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Depth-Insight-–-Contribution-of-Different-Features-to-Indoor-Single-image-Depth-Estimation"><a href="#Depth-Insight-–-Contribution-of-Different-Features-to-Indoor-Single-image-Depth-Estimation" class="headerlink" title="Depth Insight – Contribution of Different Features to Indoor Single-image Depth Estimation"></a>Depth Insight – Contribution of Different Features to Indoor Single-image Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10042">http://arxiv.org/abs/2311.10042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Wu, Yuwen Heng, Mahesan Niranjan, Hansung Kim</li>
<li>for: 这个论文主要研究了单一图像中的深度估算问题，以寻求更好地理解深度估算模型如何利用图像中的各种特征来预测深度。</li>
<li>methods: 本论文使用了特征提取技术来关联单个特征（形状、 текстура、颜色和饱和度）与深度的关系。</li>
<li>results: 研究发现，在indoor场景中，形状提取得到的结果具有更大的贡献，而其他特征也具有不同程度的贡献。这些发现可以帮助优化深度估算模型，提高其准确性和Robustness。<details>
<summary>Abstract</summary>
Depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent. Whereas impressive performances have been reported in this area recently using end-to-end trained deep neural architectures, as to what cues in the images that are being exploited by these black box systems is hard to know. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, colour and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimise depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.
</details>
<details>
<summary>摘要</summary>
depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent.  Recently, impressive performances have been reported in this area using end-to-end trained deep neural architectures, but it is hard to know what cues in the images are being exploited by these black box systems. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, color, and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimize depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.Here's the translation in Traditional Chinese:depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent.  Recently, impressive performances have been reported in this area using end-to-end trained deep neural architectures, but it is hard to know what cues in the images are being exploited by these black box systems. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, color, and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimize depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.
</details></li>
</ul>
<hr>
<h2 id="Match-and-Locate-low-frequency-monocular-odometry-based-on-deep-feature-matching"><a href="#Match-and-Locate-low-frequency-monocular-odometry-based-on-deep-feature-matching" class="headerlink" title="Match and Locate: low-frequency monocular odometry based on deep feature matching"></a>Match and Locate: low-frequency monocular odometry based on deep feature matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10034">http://arxiv.org/abs/2311.10034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stepan Konev, Yuriy Biktairov</li>
<li>for: 这篇论文的目的是提出一种基于单个摄像头的Robotic odometry方法，以提高系统的可行性和简洁性。</li>
<li>methods: 这篇论文使用了深度特征匹配模型来匹配预先 capture的图像特征，然后使用卷积神经网进行精确的姿势和位置估计。</li>
<li>results: 这篇论文在AISG-SLA Visual Localisation Challenge中评估了这种方法的表现，发现其可以实现精确的姿势和位置估计， orientation estimation error约3度， translation estimation error约2米，与其他参赛者相比，这种方法在computational efficiency和易于实现的前提下表现竞争力强。<details>
<summary>Abstract</summary>
Accurate and robust pose estimation plays a crucial role in many robotic systems. Popular algorithms for pose estimation typically rely on high-fidelity and high-frequency signals from various sensors. Inclusion of these sensors makes the system less affordable and much more complicated. In this work we introduce a novel approach for the robotic odometry which only requires a single camera and, importantly, can produce reliable estimates given even extremely low-frequency signal of around one frame per second. The approach is based on matching image features between the consecutive frames of the video stream using deep feature matching models. The resulting coarse estimate is then adjusted by a convolutional neural network, which is also responsible for estimating the scale of the transition, otherwise irretrievable using only the feature matching information. We evaluate the performance of the approach in the AISG-SLA Visual Localisation Challenge and find that while being computationally efficient and easy to implement our method shows competitive results with only around $3^{\circ}$ of orientation estimation error and $2m$ of translation estimation error taking the third place in the challenge.
</details>
<details>
<summary>摘要</summary>
准确和可靠的姿态估计在许多机器人系统中扮演着关键性的角色。常见的姿态估计算法通常基于高精度和高频信号，这些信号来自多种感知器。然而，这些感知器的包含使得系统变得更加昂贵和复杂。在这个工作中，我们介绍了一种新的机器人征卷方法，只需一个摄像头即可实现。这种方法基于图像特征匹配模型，通过比较连续帧视频流中的图像特征，生成初步估计。然后，使用卷积神经网络调整初步估计，同时估计转换的比例。我们在AISG-SLA视 lokalisierung Challenge中评估了这种方法的性能，发现它具有计算效率和易于实现的优点，同时与其他参赛者相比，其姿态估计误差为约3度和平均误差为2米，在挑战中排名第三。
</details></li>
</ul>
<hr>
<h2 id="On-the-Overconfidence-Problem-in-Semantic-3D-Mapping"><a href="#On-the-Overconfidence-Problem-in-Semantic-3D-Mapping" class="headerlink" title="On the Overconfidence Problem in Semantic 3D Mapping"></a>On the Overconfidence Problem in Semantic 3D Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10018">http://arxiv.org/abs/2311.10018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Marcos Correia Marques, Albert Zhai, Shenlong Wang, Kris Hauser</li>
<li>for: 这篇论文旨在解决Semantic 3D mapping中的混淆风险问题，即多视图结合深度和图像分割信息时，传统的映射方法会假设整个地图是正确的，从而导致输出抖抖。</li>
<li>methods: 该论文提出了多种在整合缓存阶段使用不同方法来改善混淆度规则的方法，并对ScanNet数据集进行了比较。其中，最常用的 bayesian 混合策略被证明是最差异步calibrated。作者们还提出了一种学习管道，GLFS，可以同时实现高精度和3D地图准确性，并保留实时能力。</li>
<li>results: 作者们示出，在一个模块化ObjectNav Agent上，通过正确地将Semantic Fusion纳入混合过程中，可以提高其成功率。此外，作者们还证明了地图准确性对下游任务的重要性。<details>
<summary>Abstract</summary>
Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest. This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.
</details>
<details>
<summary>摘要</summary>
Semantic 3D mapping, the process of combining depth and image segmentation information from multiple views to create 3D maps annotated with object classes in real-time, is a current area of interest. This paper highlights the fusion overconfidence problem, where conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.Here's the translation in Traditional Chinese:Semantic 3D mapping, the process of combining depth and image segmentation information from multiple views to create 3D maps annotated with object classes in real-time, is a current area of interest. This paper highlights the fusion overconfidence problem, where conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.
</details></li>
</ul>
<hr>
<h2 id="SQLNet-Scale-Modulated-Query-and-Localization-Network-for-Few-Shot-Class-Agnostic-Counting"><a href="#SQLNet-Scale-Modulated-Query-and-Localization-Network-for-Few-Shot-Class-Agnostic-Counting" class="headerlink" title="SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting"></a>SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10011">http://arxiv.org/abs/2311.10011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/sqlnet">https://github.com/hcplab-sysu/sqlnet</a></li>
<li>paper_authors: Hefeng Wu, Yandong Chen, Lingbo Liu, Tianshui Chen, Keze Wang, Liang Lin</li>
<li>for: 解决 counting all objects of an arbitrary class 问题，提高下游任务的性能。</li>
<li>methods: 提出了一种基于 localization 的 novel 方法，即 Scale-modulated Query and Localization Network (SQLNet)，它可以充分利用 exemplars 的 scale 信息，进行有效的 counting。</li>
<li>results: 在 popular CAC benchmarks 上，SQLNet 的表现优于当前领先方法，不仅在 counting 精度方面表现出色，还在 localization 和 bounding box 生成方面达到了优秀的result。<details>
<summary>Abstract</summary>
The class-agnostic counting (CAC) task has recently been proposed to solve the problem of counting all objects of an arbitrary class with several exemplars given in the input image. To address this challenging task, existing leading methods all resort to density map regression, which renders them impractical for downstream tasks that require object locations and restricts their ability to well explore the scale information of exemplars for supervision. To address the limitations, we propose a novel localization-based CAC approach, termed Scale-modulated Query and Localization Network (SQLNet). It fully explores the scales of exemplars in both the query and localization stages and achieves effective counting by accurately locating each object and predicting its approximate size. Specifically, during the query stage, rich discriminative representations of the target class are acquired by the Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few exemplars through multi-scale exemplar cooperation with equifrequent size prompt embedding. These representations are then fed into the Exemplars-Unified Query Correlation (EUQC) module to interact with the query features in a unified manner and produce the correlated query tensor. In the localization stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query tensor to predict the confidence, location, and size of each potential object. Moreover, a scale-aware localization loss is introduced, which exploits flexible location associations and exemplar scales for supervision to optimize the model performance. Extensive experiments demonstrate that SQLNet outperforms state-of-the-art methods on popular CAC benchmarks, achieving excellent performance not only in counting accuracy but also in localization and bounding box generation. Our codes will be available at https://github.com/HCPLab-SYSU/SQLNet
</details>
<details>
<summary>摘要</summary>
“类型不敏感 counting（CAC）任务最近被提出来解决输入图像中所有类型的对象数量的问题。为了解决这个复杂的任务，现有领先的方法都是通过密度地图回归来实现，这会导致其在下游任务中的缺乏能力和对 exemplars 的缺乏监督。为了解决这些限制，我们提出了一种基于localization的新方法，称为Scale-modulated Query and Localization Network（SQLNet）。它能够全面探索 exemplars 的尺度，并在查询和本地化两个阶段中准确地定位和估计每个对象的大小。在查询阶段，通过多scale exemplar合作和equifrequent size prompt embedding，HECE模块从少量 exemplars 中获得了丰富的描述符，然后将其传递给EUQC模块进行与查询特征的统一交互，生成相关的查询张量。在本地化阶段，SAML模块使用查询张量预测对象的信心、位置和大小。此外，我们还引入了具有灵活位置关系和 exemplars 尺度的scale-aware本地化损失，以便在模型性能优化。我们的实验结果表明，SQLNet 在流行的 CAC 测试准则上表现出色，不仅在计数准确性方面取得了出色的成绩，还在本地化和 bounding box 生成方面取得了优秀的成绩。我们的代码将在https://github.com/HCPLab-SYSU/SQLNet 上提供。”
</details></li>
</ul>
<hr>
<h2 id="TransFusion-–-A-Transparency-Based-Diffusion-Model-for-Anomaly-Detection"><a href="#TransFusion-–-A-Transparency-Based-Diffusion-Model-for-Anomaly-Detection" class="headerlink" title="TransFusion – A Transparency-Based Diffusion Model for Anomaly Detection"></a>TransFusion – A Transparency-Based Diffusion Model for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09999">http://arxiv.org/abs/2311.09999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matic Fučka, Vitjan Zavrtanik, Danijel Skočaj</li>
<li>for: 这个研究是为了提高表面异常检测的精度和效率，特别是在工业检测中。</li>
<li>methods: 这个研究使用了一种新的透明度基于的扩散过程，通过逐步增加异常区域的透明度，以精确地重建正常的 appearances。</li>
<li>results: 这个研究在两个常用的检测 dataset上（VisA和MVTec AD）得到了 state-of-the-art 的表现，具有image-level AUROC 的98.5%和99.2%。<details>
<summary>Abstract</summary>
Surface anomaly detection is a vital component in manufacturing inspection. Reconstructive anomaly detection methods restore the normal appearance of an object, ideally modifying only the anomalous regions. Due to the limitations of commonly used reconstruction architectures, the produced reconstructions are often poor and either still contain anomalies or lack details in anomaly-free regions. Recent reconstructive methods adopt diffusion models, however with the standard diffusion process the problems are not adequately addressed. We propose a novel transparency-based diffusion process, where the transparency of anomalous regions is progressively increased, restoring their normal appearance accurately and maintaining the appearance of anomaly-free regions without loss of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative anomaly detection method that implements the proposed diffusion process, enabling accurate downstream anomaly detection. TransFusion achieves state-of-the-art performance on both the VisA and the MVTec AD datasets, with an image-level AUROC of 98.5% and 99.2%, respectively.
</details>
<details>
<summary>摘要</summary>
表面异常检测是制造检查中的重要组成部分。重建性异常检测方法可以修复物品的正常外观，理想情况下仅 modify anomalous regions。由于通用的重建架构受限，生成的重建结果经常仍然含有异常或lack of detail in anomaly-free regions。最近的重建方法采用扩散模型，但标准的扩散过程并不能够妥善解决问题。我们提出了一种新的透明度基于扩散过程，其中异常区域的透明度逐渐增加，将其正确地修复为正常的外观，并维持异常区域以外的外观不受损害。我们提出了名为TransFusion的检测方法，它实现了提议的扩散过程，允许精确的下游异常检测。TransFusion在VisA和MVTec AD datasets上 achieve state-of-the-art performance，具体来说是图像水平的AUROC为98.5%和99.2%。
</details></li>
</ul>
<hr>
<h2 id="DeepEMD-A-Transformer-based-Fast-Estimation-of-the-Earth-Mover’s-Distance"><a href="#DeepEMD-A-Transformer-based-Fast-Estimation-of-the-Earth-Mover’s-Distance" class="headerlink" title="DeepEMD: A Transformer-based Fast Estimation of the Earth Mover’s Distance"></a>DeepEMD: A Transformer-based Fast Estimation of the Earth Mover’s Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09998">http://arxiv.org/abs/2311.09998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atulkumarin/deepemd">https://github.com/atulkumarin/deepemd</a></li>
<li>paper_authors: Atul Kumar Sinha, Francois Fleuret</li>
<li>for: 这个论文的目的是提出一种基于注意力的模型，用于精确地计算点云集的摩尔变换距离（Earth Mover’s Distance，EMD），以便作为生成模型的训练损失函数。</li>
<li>methods: 这种模型使用注意力机制来计算点云集之间的匹配，并通过Explicitly Computing Matching来获得精确的摩尔变换距离和其梯度的估计。</li>
<li>results: 实验表明，这种模型可以准确地计算摩尔变换距离和其梯度，并且在训练过程中具有高速度响应和广泛的应用前提。此外，模型在无法训练集上的运行表现也非常出色。<details>
<summary>Abstract</summary>
The Earth Mover's Distance (EMD) is the measure of choice between point clouds. However the computational cost to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance. We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models. To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself. We cast this new objective as the estimation of an attention matrix that approximates the ground truth matching matrix. Experiments show that this model provides an accurate estimate of the EMD and its gradient with a wall clock speed-up of more than two orders of magnitude with respect to the exact Hungarian matching algorithm and one order of magnitude with respect to the standard approximate Sinkhorn algorithm, allowing in particular to train a point cloud VAE with the EMD itself. Extensive evaluation show the remarkable behaviour of this model when operating out-of-distribution, a key requirement for a distance surrogate. Finally, the model generalizes very well to point clouds during inference several times larger than during training.
</details>
<details>
<summary>摘要</summary>
地球移动者距离（EMD）是点云之间的度量标准，但计算成本使其成为训练损失的禁制品。标准方法是使用 Chamfer 距离作为代理。我们提议使用注意力基于模型来计算精确的EMD aproximation，以便用于训练生成模型。为了获得必要的精确Gradient，我们在模型训练中显式计算点云之间的匹配。我们将这个新的目标设定为估算真实的匹配矩阵。实验显示，这个模型可以准确地计算EMD和其Gradient，并且与准确的挪威抽象搜索算法和标准搜索算法相比，具有大量的时间速度提升（至少两个排名）和一个排名的速度提升。这使得我们可以使用EMD本身作为训练损失。我们的模型在误差外的操作中表现出色，这是距离代理的关键要求。此外，我们的模型在推理时可以处理大量的点云，并且可以在训练时使用相同的模型。
</details></li>
</ul>
<hr>
<h2 id="From-Pretext-to-Purpose-Batch-Adaptive-Self-Supervised-Learning"><a href="#From-Pretext-to-Purpose-Batch-Adaptive-Self-Supervised-Learning" class="headerlink" title="From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning"></a>From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09974">http://arxiv.org/abs/2311.09974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiansong Zhang, Peizhong Liu</li>
<li>for: 本文旨在提出一种适应batch size和预测任务的自然语言处理方法，以提高自然语言处理的自动学习能力。</li>
<li>methods: 本文使用了对 batch data 的维度减少和重建，以实现batch数据之间的内部通信，并通过嵌入层来适应性地增强自我超vised feature编码能力。</li>
<li>results: 根据ImageNet-1k的线性分类测试，我们的方法可以在比较公平的情况下达到状态 arts 性能，而且在ImageNet-100上，相比原始性能，top1的最大提升为1.25%。<details>
<summary>Abstract</summary>
In recent years, self-supervised contrastive learning has emerged as a distinguished paradigm in the artificial intelligence landscape. It facilitates unsupervised feature learning through contrastive delineations at the instance level. However, crafting an effective self-supervised paradigm remains a pivotal challenge within this field. This paper delves into two crucial factors impacting self-supervised contrastive learning-bach size and pretext tasks, and from a data processing standpoint, proposes an adaptive technique of batch fusion. The proposed method, via dimensionality reduction and reconstruction of batch data, enables formerly isolated individual data to partake in intra-batch communication through the Embedding Layer. Moreover, it adaptively amplifies the self-supervised feature encoding capability as the training progresses. We conducted a linear classification test of this method based on the classic contrastive learning framework on ImageNet-1k. The empirical findings illustrate that our approach achieves state-of-the-art performance under equitable comparisons. Benefiting from its "plug-and-play" characteristics, we further explored other contrastive learning methods. On the ImageNet-100, compared to the original performance, the top1 has seen a maximum increase of 1.25%. We suggest that the proposed method may contribute to the advancement of data-driven self-supervised learning research, bringing a fresh perspective to this community.
</details>
<details>
<summary>摘要</summary>
近年来，自我超viscontrastive learning已经出现为人工智能领域的一种distinguished paradigm。它可以通过对instance level进行对比，实现无监督特征学习。然而，制定有效的自我超viscontrastive learning paradigm仍然是这个领域中的一个关键挑战。这篇论文探讨了自我超viscontrastive learning中两个关键因素：batch size和pretext tasks，并从数据处理角度提出了一种适应技术——批处理融合。提议的方法通过维度减少和批处理数据的重建，使得原来隔离的个体数据能够在Embedding层内进行INTRA-batch交流。此外，该方法可以逐渐增强自我超viscontrastive feature编码能力，并在训练进程中进行适应调整。我们基于经典对比学习框架进行了Linear classification测试，实验结果表明，我们的方法在相等比较下实现了状态盘领先性。由于其“嵌入式”的特点，我们进一步探索了其他对比学习方法。在ImageNet-100上，相比原始性能，排名前100的最大提升为1.25%。我们建议该方法可能会促进数据驱动的自我超viscontrastive学习研究，为这个社区带来一种新的视角。
</details></li>
</ul>
<hr>
<h2 id="SurgPLAN-Surgical-Phase-Localization-Network-for-Phase-Recognition"><a href="#SurgPLAN-Surgical-Phase-Localization-Network-for-Phase-Recognition" class="headerlink" title="SurgPLAN: Surgical Phase Localization Network for Phase Recognition"></a>SurgPLAN: Surgical Phase Localization Network for Phase Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09965">http://arxiv.org/abs/2311.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingjian Luo, You Pang, Zhen Chen, Jinlin Wu, Zongmin Zhang, Zhen Lei, Hongbin Liu</li>
<li>for: 提高智能操作室中手术理解的精度，解决自动手术阶段识别存在两大问题，即不能捕捉每帧和运动信息的特征特征，以及每个阶段的预测不稳定性。</li>
<li>methods: 我们提出了一种名为手术阶段位置网络（SurgPLAN），它通过捕捉多尺度空间和时间特征，以及基于时间区域提案的phasenprediction来提高手术阶段识别的精度和稳定性。</li>
<li>results: 我们的SurgPLAN在比较existing方法时，在精度和稳定性两个方面具有显著的优势。<details>
<summary>Abstract</summary>
Surgical phase recognition is crucial to providing surgery understanding in smart operating rooms. Despite great progress in automatic surgical phase recognition, most existing methods are still restricted by two problems. First, these methods cannot capture discriminative visual features for each frame and motion information with simple 2D networks. Second, the frame-by-frame recognition paradigm degrades the performance due to unstable predictions within each phase, termed as phase shaking. To address these two challenges, we propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a more accurate and stable surgical phase recognition with the principle of temporal detection. Specifically, we first devise a Pyramid SlowFast (PSF) architecture to serve as the visual backbone to capture multi-scale spatial and temporal features by two branches with different frame sampling rates. Moreover, we propose a Temporal Phase Localization (TPL) module to generate the phase prediction based on temporal region proposals, which ensures accurate and consistent predictions within each surgical phase. Extensive experiments confirm the significant advantages of our SurgPLAN over frame-by-frame approaches in terms of both accuracy and stability.
</details>
<details>
<summary>摘要</summary>
针对智能操作室中的手术理解，外科阶段识别具有重要的意义。虽然自动外科阶段识别技术已经取得了大量的进步，但大多数现有方法都受到两个问题的限制。首先，这些方法无法捕捉每帧和运动信息的特征特征，这限制了它们的识别精度。其次，frame-by-frame认识模式会导致识别性下降，这被称为“阶段震荡”。为了解决这两个挑战，我们提议一种名为外科阶段封顶网络（SurgPLAN），它可以提供更加准确和稳定的外科阶段识别。具体来说，我们首先设计了一种Pyramid SlowFast（PSF）架构，它作为视觉后ION来捕捉多scalespatial和时间特征。此外，我们还提出了一种时间阶段本地化（TPL）模块，它可以基于时间区域提案来生成阶段预测，从而保证了识别的准确和一致。我们进行了广泛的实验，结果表明，我们的SurgPLAN在 Frame-by-frame方法的基础上具有显著优势，包括更高的准确率和稳定性。
</details></li>
</ul>
<hr>
<h2 id="VertDetect-Fully-End-to-End-3D-Vertebral-Instance-Segmentation-Model"><a href="#VertDetect-Fully-End-to-End-3D-Vertebral-Instance-Segmentation-Model" class="headerlink" title="VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model"></a>VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09958">http://arxiv.org/abs/2311.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoff Klein, Michael Hardisty, Cari Whyne, Anne L. Martel</li>
<li>for: 这篇论文的目的是提出一个完全自动的三维脊梗检测和分类模型，以便在骨科手术和放射治疗中进行观察规划。</li>
<li>methods: 这个模型使用了一个共同的CNN背部，将检测和分类分支的feature map传递给它们。此外，还使用了一个图 theoretics 网络层，以改善脊梗标签。</li>
<li>results: 这个模型在 VerSe 2019 和 VerSe 2020 公共和隐藏测试集中均 achieved state-of-the-art 性能，其中 Dice Similarity Coefficient (DSC) 为 0.883 (95% CI, 0.843-0.906) 和 0.882 (95% CI, 0.835-0.909)，以及 0.868 (95% CI, 0.834-0.890) 和 0.869 (95% CI, 0.832-0.891)。<details>
<summary>Abstract</summary>
Vertebral detection and segmentation are critical steps for treatment planning in spine surgery and radiation therapy. Accurate identification and segmentation are complicated in imaging that does not include the full spine, in cases with variations in anatomy (T13 and/or L6 vertebrae), and in the presence of fracture or hardware. This paper proposes VertDetect, a fully automated end-to-end 3D vertebral instance segmentation Convolutional Neural Network (CNN) model to predict vertebral level labels and segmentations for all vertebrae present in a CT scan. The utilization of a shared CNN backbone provides the detection and segmentation branches of the network with feature maps containing both spinal and vertebral level information. A Graph Convolutional Network (GCN) layer is used to improve vertebral labelling by using the known structure of the spine. This model achieved a Dice Similarity Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI, 0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\% CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively. This model achieved state-of-the-art performance for an end-to-end architecture, whose design facilitates the extraction of features that can be subsequently used for downstream tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("VertDetect is a fully automated end-to-end 3D vertebral instance segmentation Convolutional Neural Network (CNN) model that predicts vertebral level labels and segmentations for all vertebrae present in a CT scan. The shared CNN backbone provides the detection and segmentation branches with feature maps containing both spinal and vertebral level information. A Graph Convolutional Network (GCN) layer is used to improve vertebral labeling using the known structure of the spine. This model achieved a Dice Similarity Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI, 0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\% CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively. This model achieved state-of-the-art performance for an end-to-end architecture, whose design facilitates the extraction of features that can be subsequently used for downstream tasks.">> Here's the breakdown of the translation:* "VertDetect" is translated as "VertDetect" (全自动的三维vertebral实例分割Convolutional Neural Network模型)* "Convolutional Neural Network" is translated as "Convolutional Neural Network" (卷积神经网络)* "end-to-end" is translated as "端到端" (end-to-end)* "vertebral instance segmentation" is translated as "vertebral实例分割" (vertebral instance segmentation)* "CT scan" is translated as "CT扫描" (CT scan)* "spinal and vertebral level information" is translated as "脊梁和vertebral уров别信息" (spinal and vertebral level information)* "Graph Convolutional Network" is translated as "图connvolutional网络" (Graph Convolutional Network)* "vertebral labeling" is translated as "vertebral标注" (vertebral labeling)* "known structure of the spine" is translated as "脊梁的已知结构" (known structure of the spine)* "Dice Similarity Coefficient" is translated as " dice相似度系数" (Dice Similarity Coefficient)* "public and hidden test sets" is translated as "公共和隐藏测试集" (public and hidden test sets)* "state-of-the-art performance" is translated as "现状最佳性能" (state-of-the-art performance)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Score-based-generative-models-learn-manifold-like-structures-with-constrained-mixing"><a href="#Score-based-generative-models-learn-manifold-like-structures-with-constrained-mixing" class="headerlink" title="Score-based generative models learn manifold-like structures with constrained mixing"></a>Score-based generative models learn manifold-like structures with constrained mixing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09952">http://arxiv.org/abs/2311.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Kevin Wenliang, Ben Moran</li>
<li>for:  score-based generative models (SBMs) learn the data distribution supported on a low-dimensional manifold</li>
<li>methods: linear approximations and subspaces spanned by local feature vectors</li>
<li>results: the learned vector field mixes samples by a non-conservative field within the manifold, and the subspace spanned by the local features overlaps with an effective density function.Here is the text in Simplified Chinese:</li>
<li>for: SBMs 学习数据分布的低维折衔</li>
<li>methods: 利用线性近似和本地特征向量生成子空间</li>
<li>results: 学习得到的向量场在折衔中混合样本，并且在折衔中保持数据分布的折衔结构。<details>
<summary>Abstract</summary>
How do score-based generative models (SBMs) learn the data distribution supported on a low-dimensional manifold? We investigate the score model of a trained SBM through its linear approximations and subspaces spanned by local feature vectors. During diffusion as the noise decreases, the local dimensionality increases and becomes more varied between different sample sequences. Importantly, we find that the learned vector field mixes samples by a non-conservative field within the manifold, although it denoises with normal projections as if there is an energy function in off-manifold directions. At each noise level, the subspace spanned by the local features overlap with an effective density function. These observations suggest that SBMs can flexibly mix samples with the learned score field while carefully maintaining a manifold-like structure of the data distribution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harnessing-Transformers-A-Leap-Forward-in-Lung-Cancer-Image-Detection"><a href="#Harnessing-Transformers-A-Leap-Forward-in-Lung-Cancer-Image-Detection" class="headerlink" title="Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection"></a>Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09942">http://arxiv.org/abs/2311.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amine Bechar, Youssef Elmir, Rafik Medjoudj, Yassine Himeur, Abbes Amira</li>
<li>for: 本研究探讨了贯彻学习（TL）和变换器在肿瘤检测中的应用，尤其是基于图像分析。</li>
<li>methods: 本研究使用了多种方法，包括TL、变换器和卷积神经网络（CNN）模型，其中变换器在图像分析中表现最佳，具有97.41%的准确率 для肾癌检测和94.71%的准确率 для histopathological lung cancer。</li>
<li>results: 本研究结果显示，变Transformers在图像分析中表现最佳，其准确率为97.41% для肾癌检测和94.71% для histopathological lung cancer。<details>
<summary>Abstract</summary>
This paper discusses the role of Transfer Learning (TL) and transformers in cancer detection based on image analysis. With the enormous evolution of cancer patients, the identification of cancer cells in a patient's body has emerged as a trend in the field of Artificial Intelligence (AI). This process involves analyzing medical images, such as Computed Tomography (CT) scans and Magnetic Resonance Imaging (MRIs), to identify abnormal growths that may help in cancer detection. Many techniques and methods have been realized to improve the quality and performance of cancer classification and detection, such as TL, which allows the transfer of knowledge from one task to another with the same task or domain. TL englobes many methods, particularly those used in image analysis, such as transformers and Convolutional Neural Network (CNN) models trained on the ImageNet dataset. This paper analyzes and criticizes each method of TL based on image analysis and compares the results of each method, showing that transformers have achieved the best results with an accuracy of 97.41% for colon cancer detection and 94.71% for Histopathological Lung cancer. Future directions for cancer detection based on image analysis are also discussed.
</details>
<details>
<summary>摘要</summary>
TL encompasses a range of methods, including those used in image analysis, such as transformers and Convolutional Neural Network (CNN) models trained on the ImageNet dataset. This paper examines and critiques each TL method based on image analysis, comparing the results of each method. The paper finds that transformers have achieved the best results, with an accuracy of 97.41% for colon cancer detection and 94.71% for Histopathological Lung cancer. The paper also discusses future directions for cancer detection based on image analysis.Translated into Simplified Chinese:这篇论文探讨了转移学习（TL）和转换器在生物图像分析中的肿瘤检测。随着癌症患者的增加，在病理图像分析中确定患者体内癌细胞的存在已成为人工智能领域的趋势。这个过程涉及分析医疗图像，如计算Tomography（CT）扫描和磁共振成像（MRI），以确定癌细胞的存在。多种技术和方法已被实现以提高癌症分类和检测的质量和性能，包括TL，它允许知识从一个任务中传输到另一个任务或领域中。TL包括许多方法，其中包括在图像分析中使用的转换器和Convolutional Neural Network（CNN）模型在ImageNet数据集上被训练。这篇论文对每种TL方法进行了分析和评价，并比较了每种方法的结果。论文发现，转换器已经实现了最好的结果，具体来说是97.41%的肿瘤检测精度 для肠癌和94.71%的肺癌检测精度。论文还讨论了基于图像分析的未来癌症检测的方向。Translated into Traditional Chinese:这篇论文探讨了将学习（TL）和转换器在生物图像分析中的肿瘤检测。随着癌症患者的增加，在病理图像分析中确定患者体内癌细胞的存在已成为人工智能领域的趋势。这个过程涉及分析医疗图像，如计算Tomography（CT）扫描和磁共振成像（MRI），以确定癌细胞的存在。多种技术和方法已被实现以提高癌症分类和检测的质量和性能，包括TL，它允许知识从一个任务中传输到另一个任务或领域中。TL包括许多方法，其中包括在图像分析中使用的转换器和Convolutional Neural Network（CNN）模型在ImageNet数据集上被训练。这篇论文对每种TL方法进行了分析和评价，并比较了每种方法的结果。论文发现，转换器已经实现了最好的结果，具体来说是97.41%的肿瘤检测精度 для肝癌和94.71%的肺癌检测精度。论文还讨论了基于图像分析的未来癌症检测的方向。
</details></li>
</ul>
<hr>
<h2 id="RED-DOT-Multimodal-Fact-checking-via-Relevant-Evidence-Detection"><a href="#RED-DOT-Multimodal-Fact-checking-via-Relevant-Evidence-Detection" class="headerlink" title="RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection"></a>RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09939">http://arxiv.org/abs/2311.09939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevejpapad/relevant-evidence-detection">https://github.com/stevejpapad/relevant-evidence-detection</a></li>
<li>paper_authors: Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis</li>
<li>for: 本研究旨在提供一种自动Multimodal fact-checking方法，用于支持或驳斥声明的真实性。</li>
<li>methods: 本研究使用了一种新的”相关证据检测”（RED）模块，用于判断每个证据是否相关，以支持或驳斥声明。此外，研究还提出了多种体系和机制，如”导向注意力”模块，以提高模型的可解释性和性能。</li>
<li>results: 研究表明，使用RED-DOT模型可以在VERITEbenchmark上实现30%的提高，并在NewsCLIPings+上达到竞争性和改进的性能，无需大量的证据或多个Encoder。此外，研究还进行了质量分析，表明”导向注意力”模块可以提高模型的解释性。<details>
<summary>Abstract</summary>
Online misinformation is often multimodal in nature, i.e., it is caused by misleading associations between texts and accompanying images. To support the fact-checking process, researchers have been recently developing automatic multimodal methods that gather and analyze external information, evidence, related to the image-text pairs under examination. However, prior works assumed all collected evidence to be relevant. In this study, we introduce a "Relevant Evidence Detection" (RED) module to discern whether each piece of evidence is relevant, to support or refute the claim. Specifically, we develop the "Relevant Evidence Detection Directed Transformer" (RED-DOT) and explore multiple architectural variants (e.g., single or dual-stage) and mechanisms (e.g., "guided attention"). Extensive ablation and comparative experiments demonstrate that RED-DOT achieves significant improvements over the state-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our evidence re-ranking and element-wise modality fusion led to RED-DOT achieving competitive and even improved performance on NewsCLIPings+, without the need for numerous evidence or multiple backbone encoders. Finally, our qualitative analysis demonstrates that the proposed "guided attention" module has the potential to enhance the architecture's interpretability. We release our code at: https://github.com/stevejpapad/relevant-evidence-detection
</details>
<details>
<summary>摘要</summary>
在线资讯承害 frequently 是多 modal 的，即由诽导的文字和附加的图像所致。为支持事实核查过程，研究人员最近已经开发出自动多模式方法，将外部信息、证据聚合和分析。但是，先前的工作假设所有收集到的证据都是有用的。在这一 studyt，我们引入一个“有用证据检测”（RED）模组，以决定每个证据是否有用，以支持或驳回主张。我们开发了“导向注意力”（RED-DOT）模型，并考虑多种架构和机制（例如，单Stage 或 dual-stage，以及“导向注意力”）。我们实施了广泛的删除和比较实验，证明了 RED-DOT 在 VERITE 标准 benchmark 上可以实现最多 28.5% 的提升。此外，我们的证据重新排序和元素综合模式融合实现了 RED-DOT 在 NewsCLIPings+ 上的竞争性和改进性，无需丰富的证据或多个背部构成器。最后，我们的Qualitative分析显示出“导向注意力”模组具有提高架构解释性的潜力。我们在 GitHub 上发布了代码：https://github.com/stevejpapad/relevant-evidence-detection。
</details></li>
</ul>
<hr>
<h2 id="Selection-of-Distinct-Morphologies-to-Divide-Conquer-Gigapixel-Pathology-Images"><a href="#Selection-of-Distinct-Morphologies-to-Divide-Conquer-Gigapixel-Pathology-Images" class="headerlink" title="Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel Pathology Images"></a>Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel Pathology Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09902">http://arxiv.org/abs/2311.09902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abubakr Shafique, Saghir Alfasly, Areej Alsaafin, Peyman Nejat, Jibran A. Khan, H. R. Tizhoosh</li>
<li>for: 本研究旨在提出一种选择小型、代表性强的WSIs补丁集合方法，以便在计算生物学中进行WSIs分类和匹配分析。</li>
<li>methods: 本方法基于”拆分与统一”的思想，通过绘制出WSIs中各种形态特征的示意图，并对这些示意图进行自动选择，以选择一个能够涵盖所有WSIs中形态特征的小型补丁集合。</li>
<li>results: 研究表明，SDM方法在多个公共和私人生化病理学数据集上具有remarkable的效果，并且不需要参数设定，因为它自动优化选择过程以捕捉WSIs中的形态特征。<details>
<summary>Abstract</summary>
Whole slide images (WSIs) are massive digital pathology files illustrating intricate tissue structures. Selecting a small, representative subset of patches from each WSI is essential yet challenging. Therefore, following the "Divide & Conquer" approach becomes essential to facilitate WSI analysis including the classification and the WSI matching in computational pathology. To this end, we propose a novel method termed "Selection of Distinct Morphologies" (SDM) to choose a subset of WSI patches. The aim is to encompass all inherent morphological variations within a given WSI while simultaneously minimizing the number of selected patches to represent these variations, ensuring a compact yet comprehensive set of patches. This systematically curated patch set forms what we term a "montage". We assess the representativeness of the SDM montage across various public and private histopathology datasets. This is conducted by using the leave-one-out WSI search and matching evaluation method, comparing it with the state-of-the-art Yottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets during its evaluation. Furthermore, SDM eliminates the necessity for empirical parameterization, a crucial aspect of Yottixel's mosaic, by inherently optimizing the selection process to capture the distinct morphological features within the WSI.
</details>
<details>
<summary>摘要</summary>
整个扫描图像（WSIs）是大量数字病理学文件，展示了复杂的组织结构。选择WSIs中每个patch的小样本 subset是必要的，但是具有挑战性。为了促进WSIs的分析，包括类别和WSIs匹配，我们提出了一种新方法称为“选择独特形态”（SDM）。该方法的目标是在给定WSIs中涵盖所有自然形态的变化，同时最小化选择的patch数量，以确保一个紧凑且全面的patch集。这个系统化批处形成了我们称之为“montage”。我们在不同的公共和私人 histopathology 数据集上评估SDM montage的表现。我们使用离开一个 WSI 搜索和匹配评估方法，与现有的 Yottixel 的落幕进行比较。SDM在所有数据集上都表现出了很好的效果。此外，SDM 摒弃了 Yottixel 落幕中的参数化，因为它自动优化选择过程，以捕捉 WSIs 中的独特形态特征。
</details></li>
</ul>
<hr>
<h2 id="I-S-ViT-An-Inclusive-Stable-Method-for-Pushing-the-Limit-of-Post-Training-ViTs-Quantization"><a href="#I-S-ViT-An-Inclusive-Stable-Method-for-Pushing-the-Limit-of-Post-Training-ViTs-Quantization" class="headerlink" title="I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of Post-Training ViTs Quantization"></a>I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of Post-Training ViTs Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10126">http://arxiv.org/abs/2311.10126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunshan Zhong, Jiawei Hu, Mingbao Lin, Mengzhao Chen, Rongrong Ji<br>for:* 这个 paper 是为了解决 transformer 模型在实际应用中的 Computational Cost 问题，特别是在训练和测试过程中的 dense 计算成本问题。methods:* 这个 paper 使用 post-training quantization (PTQ) 方法，将 transformer 模型训练后的 weights 量化为 low-bit 格式，以提高 computational efficiency。* 这个 paper  introduce 一个 novel 的 I&amp;S-ViT 方法，它可以在 PTQ 过程中稳定地调整 transformer 模型的 weights，以提高模型的性能。results:* 这个 paper 的结果显示，I&amp;S-ViT 方法可以在 diverse vision tasks 中提高 transformer 模型的性能，特别是在 low-bit enario 下。* 例如，I&amp;S-ViT 方法可以提高 3-bit ViT-B 模型的性能 by 50.68%。<details>
<summary>Abstract</summary>
Albeit the scalable performance of vision transformers (ViTs), the dense computational costs (training & inference) undermine their position in industrial applications. Post-training quantization (PTQ), tuning ViTs with a tiny dataset and running in a low-bit format, well addresses the cost issue but unluckily bears more performance drops in lower-bit cases. In this paper, we introduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an inclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of ViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for post-Softmax activations; (2) Rugged and magnified loss landscape in coarse-grained quantization granularity for post-LayerNorm activations. Then, I&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2 quantizer (SULQ) that incorporates a shift mechanism followed by uniform quantization to achieve both an inclusive domain representation and accurate distribution approximation; (2) A three-stage smooth optimization strategy (SOS) that amalgamates the strengths of channel-wise and layer-wise quantization to enable stable learning. Comprehensive evaluations across diverse vision tasks validate I&S-ViT' superiority over existing PTQ of ViTs methods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the performance of 3-bit ViT-B by an impressive 50.68%.
</details>
<details>
<summary>摘要</summary>
尽管Scalable Performance of Vision Transformers（ViTs），但是密集的计算成本（训练和推理）却限制其在产业应用中的位置。Post-training Quantization（PTQ），通过使用tiny dataset和低位数据类型进行调教，有效地解决了成本问题，但是在低位数据类型下会导致性能下降。在这篇论文中，我们引入I&S-ViT，一种新的方法，可以在包容和稳定的情况下进行PTQ的规范。I&S-ViT首先发现了ViTs中PTQ中的两个问题：（1）频繁使用的log2 quantizer在Post-Softmax活动中的量化不准确;（2）LayerNorm活动中的粗糙和增大的损失图像。然后，I&S-ViT通过引入以下两种方法来解决这些问题：（1）一种新的Shift-Uniform-Log2 quantizer（SULQ），通过添加shift机制并使用均匀量化来实现包容的领域表示和准确的分布近似;（2）一种三个阶段的smooth optimization strategy（SOS），将通道级和层级量化融合在一起，以实现稳定的学习。通过对多种视觉任务进行广泛的评估，我们证明I&S-ViT在PTQ方法中的超越性，特别是在低位数据类型下。例如，I&S-ViT使3位ViT-B的性能提高了50.68%。
</details></li>
</ul>
<hr>
<h2 id="UnifiedVisionGPT-Streamlining-Vision-Oriented-AI-through-Generalized-Multimodal-Framework"><a href="#UnifiedVisionGPT-Streamlining-Vision-Oriented-AI-through-Generalized-Multimodal-Framework" class="headerlink" title="UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework"></a>UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10125">http://arxiv.org/abs/2311.10125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lhbuilder/sa-segment-anything">https://github.com/lhbuilder/sa-segment-anything</a></li>
<li>paper_authors: Chris Kelly, Luhui Hu, Cindy Yang, Yu Tian, Deshun Yang, Bang Yang, Zaoshan Huang, Zihao Li, Yuexian Zou</li>
<li>for: 这份论文的目的是发展一个可以整合多种现有的州OfTheArt（SOTA）Computer Vision（CV）模型的框架，以提高CV领域的进步和效率。</li>
<li>methods: 这份论文使用了一个名为UnifiedVisionGPT的框架，它可以融合多种SOTA CV模型，并且可以自动选择适合的模型基于多 modal 输入，例如文本提示和图像。</li>
<li>results: 这份论文显示了UnifiedVisionGPT的架构和能力，并证明了它在CV领域中的应用可以提高效率、多样性、普遍性和性能。<details>
<summary>Abstract</summary>
In the current landscape of artificial intelligence, foundation models serve as the bedrock for advancements in both language and vision domains. OpenAI GPT-4 has emerged as the pinnacle in large language models (LLMs), while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models such as Meta's SAM and DINO, and YOLOS. However, the financial and computational burdens of training new models from scratch remain a significant barrier to progress. In response to this challenge, we introduce UnifiedVisionGPT, a novel framework designed to consolidate and automate the integration of SOTA vision models, thereby facilitating the development of vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key features: (1) provides a versatile multimodal framework adaptable to a wide range of applications, building upon the strengths of multimodal foundation models; (2) seamlessly integrates various SOTA vision models to create a comprehensive multimodal platform, capitalizing on the best components of each model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in the CV domain compared to the current trajectory of LLMs; and (4) introduces automation in the selection of SOTA vision models, generating optimal results based on diverse multimodal inputs such as text prompts and images. This paper outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, generalization, and performance. Our implementation, along with the unified multimodal framework and comprehensive dataset, is made publicly available at https://github.com/LHBuilder/SA-Segment-Anything.
</details>
<details>
<summary>摘要</summary>
当前人工智能领域中，基础模型作为进步的基础，在语言和视觉领域得到了广泛应用。OpenAI GPT-4在大语言模型（LLM）中脱颖而出，而计算机视觉（CV）领域则拥有丰富的状态对照（SOTA）模型，如Meta的SAM和DINO，以及YOLOS。然而，培育新模型的财务和计算成本仍然是进步的主要障碍。为应对这个挑战，我们介绍了一种新的框架——统一视觉GPT，该框架旨在集成和自动化STATE OF THE ART（SOTA）视觉模型，以便开发视觉启发的人工智能。统一视觉GPT的四个关键特点是：（1）提供多样化的多Modal Framework，可适应各种应用场景，基于多Modal基础模型的优势；（2）将多种SOTA视觉模型集成在一起，创造出全面的多Modal平台，利用每个模型的优点；（3）强调视觉启发，以更快速地进步在计算机视觉领域，相比现有的语言模型的趋势；以及（4）通过自动化SOTA视觉模型的选择，根据多Modal输入 such as文本提示和图像，生成优化的结果。本文描述了统一视觉GPT的架构和能力，展示其在计算机视觉领域的潜在革命性。我们的实现，以及统一多Modal框架和完整的数据集，在https://github.com/LHBuilder/SA-Segment-Anything上公开提供。
</details></li>
</ul>
<hr>
<h2 id="Rusty-Detection-Using-Image-Processing-For-Maintenance-Of-Stations"><a href="#Rusty-Detection-Using-Image-Processing-For-Maintenance-Of-Stations" class="headerlink" title="Rusty Detection Using Image Processing For Maintenance Of Stations"></a>Rusty Detection Using Image Processing For Maintenance Of Stations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09849">http://arxiv.org/abs/2311.09849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dao Duy Tung, Ho Xuan Hung</li>
<li>for: 这种研究旨在准确地分割涂抹的锈批镀表面上的锈区域。</li>
<li>methods: 该方法基于数字图像处理，使用HSV颜色模型进行分割，并应用单一级Retinex模型来平衡光照的影响。然后通过手动色滤波进行进一步处理，以增强锈区域的识别。最后，使用DBScan算法进行准确的锈区域分割。</li>
<li>results: 该方法可以准确地分割涂抹的锈批镀表面上的锈区域，提供一种有价值的锈检测和分析方法。<details>
<summary>Abstract</summary>
This study addresses the challenge of accurately seg-menting rusted areas on painted construction surfaces. A method leveraging digital image processing is explored to calculate the percentage of rust present on painted coatings. The proposed segmentation approach is based on the HSV color model. To equalize luminosity and mitigate the influence of illumination, a fundamental model of single-scale Retinex is applied specifically to the saturation component.   Subsequently, the image undergoes further processing, involv-ing manual color filtering. This step is crucial for refining the identification of rusted regions. To enhance precision and filter out noise, the pixel areas selected through color filtering are subjected to the DBScan algorithm. This multi-step process aims to achieve a robust segmentation of rusted areas on painted construction surfaces, providing a valuable contribution to the field of corrosion detection and analysis.
</details>
<details>
<summary>摘要</summary>
First, a fundamental model of single-scale Retinex is applied to the saturation component of the image to equalize luminosity and mitigate the influence of illumination. Next, the image undergoes manual color filtering to refine the identification of rusted regions. Finally, the pixel areas selected through color filtering are subjected to the DBScan algorithm to enhance precision and filter out noise.The proposed segmentation approach is designed to provide a robust and accurate method for detecting rusted areas on painted construction surfaces, which is a valuable contribution to the field of corrosion detection and analysis.
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Data-Scarcity-in-Biomedical-Imaging-with-a-Foundational-Multi-Task-Model"><a href="#Overcoming-Data-Scarcity-in-Biomedical-Imaging-with-a-Foundational-Multi-Task-Model" class="headerlink" title="Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model"></a>Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09847">http://arxiv.org/abs/2311.09847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Schäfer, Till Nicke, Henning Höfener, Annkristin Lange, Dorit Merhof, Friedrich Feuerhake, Volkmar Schulz, Johannes Lotz, Fabian Kiessling</li>
<li>For: 这个论文的目的是提出一种基于多任务学习的基本模型训练策略，以便在生物医学成像领域中使用。* Methods: 这个论文使用了一种多任务学习策略，其中包括将多种类别和分类任务组织在一起，以便减少训练数据的内存需求。* Results: 研究发现，使用这种多任务学习策略可以让基本模型在不同的任务上保持高度的性能，并且只需要1%的原始训练数据和不需要细化。此外，这种方法还可以在不同的中心进行跨中心传输。<details>
<summary>Abstract</summary>
Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.
</details>
<details>
<summary>摘要</summary>
基础模型，在大规模预训练下，在非医学领域实现了重要成功。然而，这些模型的训练通常需要大量、全面的数据集，而生物医学成像中的数据集通常较小、更加多样化。在这里，我们提出了一种多任务学习策略，即解耦训练任务数量和内存需求。我们使用了一个通用的生物医学预训练模型（UMedPT），在包括tomographic、微scopic和X射线图像的多任务数据库上进行训练，并采用了不同的标签策略，如分类、分割和对象检测。UMedPT基础模型在预训练数据库中相对于ImageNet预训练和前一个状态的模型表现出色。对于与预训练数据库相关的任务，它只需要1%的原始训练数据，而不需要微调。对于外部独立验证的任务，它只需要50%的原始训练数据。在外部独立验证中，使用UMedPT提取出的生物医学特征被认为是新的跨中心传送标准。
</details></li>
</ul>
<hr>
<h2 id="GroupMixer-Patch-based-Group-Convolutional-Neural-Network-for-Breast-Cancer-Detection-from-Histopathological-Images"><a href="#GroupMixer-Patch-based-Group-Convolutional-Neural-Network-for-Breast-Cancer-Detection-from-Histopathological-Images" class="headerlink" title="GroupMixer: Patch-based Group Convolutional Neural Network for Breast Cancer Detection from Histopathological Images"></a>GroupMixer: Patch-based Group Convolutional Neural Network for Breast Cancer Detection from Histopathological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09846">http://arxiv.org/abs/2311.09846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ardavan Modarres, Erfan Ebrahim Esfahani, Mahsa Bahrami</li>
<li>for: 检测悬肢癌细胞恶性的早期阶段是控制其影响的关键步骤。 histopathological analysis 提供了独特的机会 для恶性肿瘤检测。但是，这种任务会对 histopathologists 太 tedious 和 time-consuming。</li>
<li>methods: 使用 Deep Neural Networks 直接从 raw histopathological images 学习 informative features，而不需要 manual feature extraction。</li>
<li>results: 使用 CNN 架构和 Patch Embedding 操作，实现了高度精准的悬肢癌检测，并且对比其他方法具有更多的 trainable parameters 和更大的数据集来进行训练。尽管使用 Transformer 架构在医学图像分析中显示了惊人的表现，但是这些架构具有较多的可训练参数和需要大量的数据来进行训练。<details>
<summary>Abstract</summary>
Diagnosis of breast cancer malignancy at the early stages is a crucial step for controlling its side effects. Histopathological analysis provides a unique opportunity for malignant breast cancer detection. However, such a task would be tedious and time-consuming for the histopathologists. Deep Neural Networks enable us to learn informative features directly from raw histopathological images without manual feature extraction. Although Convolutional Neural Networks (CNNs) have been the dominant architectures in the computer vision realm, Transformer-based architectures have shown promising results in different computer vision tasks. Although harnessing the capability of Transformer-based architectures for medical image analysis seems interesting, these architectures are large, have a significant number of trainable parameters, and require large datasets to be trained on, which are usually rare in the medical domain. It has been claimed and empirically proved that at least part of the superior performance of Transformer-based architectures in Computer Vision domain originates from patch embedding operation. In this paper, we borrowed the previously introduced idea of integrating a fully Convolutional Neural Network architecture with Patch Embedding operation and presented an efficient CNN architecture for breast cancer malignancy detection from histopathological images. Despite the number of parameters that is significantly smaller than other methods, the accuracy performance metrics achieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x magnifications respectively. We took a step forward and modified the architecture using Group Convolution and Channel Shuffling ideas and reduced the number of trainable parameters even more with a negligible decline in performance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the mentioned magnifications respectively.
</details>
<details>
<summary>摘要</summary>
诊断乳腺癌恶性肿瘤的初期阶段是控制其副作用的关键步骤。 histopathological 分析提供了诊断恶性乳腺癌的唯一机会。然而，这种任务对 histopathologists 来说是繁琐和时间consuming的。深度神经网络允许我们直接从 Raw histopathological 图像中学习有用的特征，无需手动提取特征。尽管卷积神经网络（CNNs）在计算机视觉领域是主导的建筑，但基于 Transformer 的建筑在不同的计算机视觉任务中表现出色。虽然在医疗领域使用基于 Transformer 的建筑可能有趣，这些建筑却很大，有很多可训练的参数，并且需要大量的数据来进行训练，这些数据通常在医疗领域是罕见的。有人提出并证明了，至少一部分基于 Transformer 的表现提升的原因是负权重嵌入操作。在这篇论文中，我们借鉴了之前引入的将 Fully Convolutional Neural Network 架构与负权重嵌入操作结合的想法，并提出了一种高效的乳腺癌恶性识别方法。尽管参数的数量远少于其他方法，但我们在 40x、100x、200x 和 400x 倍镜下测试的性能指标分别达到 97.65%、98.92%、99.21% 和 98.01%。我们进一步修改了架构，使用 Group Convolution 和 Channel Shuffling 的想法，并减少了可训练参数的数量，但性能的下降是极少的，达到 95.42%、98.16%、96.05% 和 97.92%。
</details></li>
</ul>
<hr>
<h2 id="MAM-E-Mammographic-synthetic-image-generation-with-diffusion-models"><a href="#MAM-E-Mammographic-synthetic-image-generation-with-diffusion-models" class="headerlink" title="MAM-E: Mammographic synthetic image generation with diffusion models"></a>MAM-E: Mammographic synthetic image generation with diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09822">http://arxiv.org/abs/2311.09822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Likalto4/diffusion-models_master">https://github.com/Likalto4/diffusion-models_master</a></li>
<li>paper_authors: Ricardo Montoya-del-Angel, Karla Sam-Millan, Joan C Vilanova, Robert Martí</li>
<li>for: 本研究旨在使用扩散模型作为医疗影像资料增强技术，以解决医疗影像领域资料缺乏的问题。</li>
<li>methods: 本研究使用了现代的条件扩散管道，以生成高质量的全场数字乳腺影像。同时，我们还提出使用稳定的扩散模型来填充人工变化的Synthetic lesions在健康乳腺影像上。</li>
<li>results: 我们提出了一个名为MAM-E的生成模型架空，可以根据文本提示生成高质量的乳腺影像，并且可以填充人工变化的Synthetic lesions在specific region of the breast。 finally, we provide了量化和质感评估的生成影像，以及易用的グラフィカルユーザインターフェース для乳腺影像生成。<details>
<summary>Abstract</summary>
Generative models are used as an alternative data augmentation technique to alleviate the data scarcity problem faced in the medical imaging field. Diffusion models have gathered special attention due to their innovative generation approach, the high quality of the generated images and their relatively less complex training process compared with Generative Adversarial Networks. Still, the implementation of such models in the medical domain remains at early stages. In this work, we propose exploring the use of diffusion models for the generation of high quality full-field digital mammograms using state-of-the-art conditional diffusion pipelines. Additionally, we propose using stable diffusion models for the inpainting of synthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of generative models for high quality mammography synthesis controlled by a text prompt and capable of generating synthetic lesions on specific regions of the breast. Finally, we provide quantitative and qualitative assessment of the generated images and easy-to-use graphical user interfaces for mammography synthesis.
</details>
<details>
<summary>摘要</summary>
“生成模型被用作医疗影像领域数据增强技术的替代方法，以解决医疗影像领域面临的数据缺乏问题。扩散模型吸引了特别的关注，因为它们的创新生成方法、高质量生成图像和相对于对抗网络更为简单的训练过程。然而，医疗领域中的实施仍然处于早期阶段。在这种工作中，我们提议使用扩散模型来生成高质量全场数字乳肿图像，并使用稳定的扩散模型进行 synthetic 病变的填充。我们介绍了MAM-E，一个基于生成模型的高质量乳肿合成管道，可以通过文本提示来控制生成过程。此外，我们还提供了生成图像的量化和质量评估，以及易用的图形用户界面。”
</details></li>
</ul>
<hr>
<h2 id="Neural-Logic-Human-Object-Interaction-Detection"><a href="#Neural-Logic-Human-Object-Interaction-Detection" class="headerlink" title="Neural-Logic Human-Object Interaction Detection"></a>Neural-Logic Human-Object Interaction Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09817">http://arxiv.org/abs/2311.09817</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Liulei Li, Jianan Wei, Wenguan Wang, Yi Yang</li>
<li>for: 提高 HOI 检测器的性能和零学习推广能力</li>
<li>methods: 修改传统 Transformer 自注意机制，以便在 &lt;人, 动作, 物品&gt;  triplet 上进行神经逻辑reasoning，并由两种关键性 HOI 理解属性（可用性和 прокси迫）导引。</li>
<li>results: 在 V-COCO 和 HICO-DET 上对常规和零学习情况下，实现了显著改善，与现有方法相比。<details>
<summary>Abstract</summary>
The interaction decoder utilized in prevalent Transformer-based HOI detectors typically accepts pre-composed human-object pairs as inputs. Though achieving remarkable performance, such paradigm lacks feasibility and cannot explore novel combinations over entities during decoding. We present L OGIC HOI, a new HOI detector that leverages neural-logic reasoning and Transformer to infer feasible interactions between entities. Specifically, we modify the self-attention mechanism in vanilla Transformer, enabling it to reason over the <human, action, object> triplet and constitute novel interactions. Meanwhile, such reasoning process is guided by two crucial properties for understanding HOI: affordances (the potential actions an object can facilitate) and proxemics (the spatial relations between humans and objects). We formulate these two properties in first-order logic and ground them into continuous space to constrain the learning process of our approach, leading to improved performance and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and HICO-DET under both normal and zero-shot setups, achieving significant improvements over existing methods.
</details>
<details>
<summary>摘要</summary>
很多现有的Transformer基于HOI探测器通常使用预 compose的人物对碰输入decoder。虽然实现了惊人的性能，但这种方法缺乏实用性，不能探索Entities During Decoding中的新组合。我们介绍了 L OGIC HOI，一种新的HOI探测器，利用神经逻辑理解和Transformer来推理可能的人物对碰。具体来说，我们修改了vanilla Transformer中的自我注意机制，使其能够对 <人,行为,物> triplet进行推理，并且通过两个关键的HOI理解属性：可行性（物品可能支持的行为）和距离（人类和物品之间的空间关系）。我们将这两个属性写入了第一频谱逻辑，并将其降到连续空间中，以制约我们的方法学习过程，从而提高性能和零shot泛化能力。我们在V-COCO和HICO-DET上评估了L OGIC HOI，在正常和零shot设置下都达到了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="MetaDreamer-Efficient-Text-to-3D-Creation-With-Disentangling-Geometry-and-Texture"><a href="#MetaDreamer-Efficient-Text-to-3D-Creation-With-Disentangling-Geometry-and-Texture" class="headerlink" title="MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture"></a>MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10123">http://arxiv.org/abs/2311.10123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, Xiaoli Liu<br>for:* 这个研究旨在提高三维物体生成的效率和质量，并且解决多视角对称和实际对称的问题。methods:* 这个方法使用了两个阶段的优化方法，首先优化三维物体的几何表示，然后进行细微调整和纹理优化。results:* 这个方法可以实现高品质的三维物体生成，并且可以在20分钟内完成文本描述的三维生成。此外，这个方法还能够实现图像控制，提高了三维生成的可控性。<details>
<summary>Abstract</summary>
Generative models for 3D object synthesis have seen significant advancements with the incorporation of prior knowledge distilled from 2D diffusion models. Nevertheless, challenges persist in the form of multi-view geometric inconsistencies and slow generation speeds within the existing 3D synthesis frameworks. This can be attributed to two factors: firstly, the deficiency of abundant geometric a priori knowledge in optimization, and secondly, the entanglement issue between geometry and texture in conventional 3D generation methods.In response, we introduce MetaDreammer, a two-stage optimization approach that leverages rich 2D and 3D prior knowledge. In the first stage, our emphasis is on optimizing the geometric representation to ensure multi-view consistency and accuracy of 3D objects. In the second stage, we concentrate on fine-tuning the geometry and optimizing the texture, thereby achieving a more refined 3D object. Through leveraging 2D and 3D prior knowledge in two stages, respectively, we effectively mitigate the interdependence between geometry and texture. MetaDreamer establishes clear optimization objectives for each stage, resulting in significant time savings in the 3D generation process. Ultimately, MetaDreamer can generate high-quality 3D objects based on textual prompts within 20 minutes, and to the best of our knowledge, it is the most efficient text-to-3D generation method. Furthermore, we introduce image control into the process, enhancing the controllability of 3D generation. Extensive empirical evidence confirms that our method is not only highly efficient but also achieves a quality level that is at the forefront of current state-of-the-art 3D generation techniques.
</details>
<details>
<summary>摘要</summary>
现代生成模型在三维物体生成方面已经做出了重要进步，通过吸取二维扩散模型中的知识。然而，现有的三维生成框架仍然面临着多视图几何不一致和慢速生成速度的挑战。这可以归结于两点：首先，严重缺乏丰富的几何知识在优化中，其次，在传统的三维生成方法中，几何和文本之间存在杂化问题。为了解决这些问题，我们介绍MetaDreamer，一种两stage优化方法，利用强大的二维和三维知识。在第一个阶段，我们强调优化几何表示，以确保多视图一致性和三维物体的准确性。在第二个阶段，我们专注于细化几何和优化文本，以实现更加细腻的三维物体。通过在两个阶段分别利用二维和三维知识，我们有效地消除了几何和文本之间的互相关系。MetaDreamer采用清晰的优化目标，从而大大降低三维生成过程中的时间成本。最终，MetaDreamer可以在20分钟内基于文本提示生成高质量的三维物体，并且，到目前为止，它是目前最高效的文本到三维生成方法。此外，我们还引入图像控制，使三维生成过程中的控制性得到了进一步提升。广泛的实验证明，我们的方法不仅高效，而且达到了当前领域的前沿水平。
</details></li>
</ul>
<hr>
<h2 id="EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction-on-Mobile-Devices"><a href="#EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction-on-Mobile-Devices" class="headerlink" title="EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices"></a>EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09806">http://arxiv.org/abs/2311.09806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingnan Gao, Zhuo Chen, Yichao Yan, Bowen Pan, Zhe Wang, Jiangjing Lyu, Xiaokang Yang</li>
<li>For: 高效率、视角受限的3D对象重建* Methods: 使用高效表面基本模型、多视图监测模块、含有 Gaussian 豆的隐式Texture 以及轻量级神经灯谱* Results: 能够在移动设备上实现高质量的外观和准确的网格重建，并且可以在1-2个小时内训练使用单个GPU，并在40帧&#x2F;秒以上的帧率下运行。<details>
<summary>Abstract</summary>
Reconstructing real-world 3D objects has numerous applications in computer vision, such as virtual reality, video games, and animations. Ideally, 3D reconstruction methods should generate high-fidelity results with 3D consistency in real-time. Traditional methods match pixels between images using photo-consistency constraints or learned features, while differentiable rendering methods like Neural Radiance Fields (NeRF) use surface-based representations or differentiable volume rendering to generate high-fidelity scenes. However, these methods require excessive runtime for rendering, making them impractical for daily applications. To address these challenges, we present $\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware Implicit Textured $\textbf{Surf}$ace Reconstruction method on Mobile Devices. In our method, we first employ an efficient surface-based model with a multi-view supervision module to ensure accurate mesh creation. To enable high-fidelity rendering, we learn an implicit texture embedded with a set of Gaussian lobes to capture view-dependent information. Furthermore, With the explicit geometry and the implicit texture, we can employ a lightweight neural shader to reduce the expense of computation and further support real-time rendering on common mobile devices. Extensive experiments demonstrate that our method can reconstruct high-quality appearance and accurate mesh on both synthetic and real-world datasets. Moreover, our method can be trained in just 1-2 hours using a single GPU and run on mobile devices at over 40FPS (Frames Per Second), with a final package required for rendering taking up only 40-50 MB.
</details>
<details>
<summary>摘要</summary>
<<SYS>>重建现实世界中的3D对象有很多应用程序在计算机视觉中，如虚拟现实、游戏和动画。理想情况下，3D重建方法应该生成高品质的结果，并在实时中进行渲染。传统方法通过图像匹配 pixels 使用光度约束或学习特征来实现，而 differentiable rendering 方法如神经辐射场（NeRF）则使用表面基本表示或可导渲染来生成高品质场景。然而，这些方法在渲染时需要过分的时间，使得它们在日常应用中不够实用。为解决这些挑战，我们介绍 $\textbf{EvaSurf}$，一种高效的视觉相关的凹面 Textured 表面重建方法，运行在移动设备上。在我们的方法中，我们首先采用高效的表面基本模型，并在多视图超vision模块的支持下确保精度的网格创建。为了实现高品质渲染，我们学习了一个包含多个 Gaussian lobes 的隐式文本，以捕捉视觉相关信息。此外，通过Explicit geometry和隐式文本，我们可以使用轻量级的神经渲染器来减少计算成本，并进一步支持实时渲染在常见的移动设备上。广泛的实验表明，我们的方法可以在 Both synthetic and real-world datasets 上重建高质量的外观和准确的网格，并且可以在1-2小时内在单个 GPU 上训练，并在移动设备上运行于40帧/秒（Frame Per Second），总包装需要40-50 MB。Note: The text has been translated using Google Translate, and some parts may not be perfectly accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Certified-Control-for-Train-Sign-Classification"><a href="#Certified-Control-for-Train-Sign-Classification" class="headerlink" title="Certified Control for Train Sign Classification"></a>Certified Control for Train Sign Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09778">http://arxiv.org/abs/2311.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Roßbach, Michael Leuschel</li>
<li>for: 这篇论文是为了研究一种用于验证自动驾驶列车系统的证明框架，以避免识别器错误地检测交通标志。</li>
<li>methods: 这篇论文使用了经典的计算机视觉算法来检查检测到的交通标志是否遵循预先定义的规范。</li>
<li>results: 我们的初步结果很有希望，可以达到较高的准确率，同时只有较小的报告率下降，但是进一步的推广性研究仍然需要进行。<details>
<summary>Abstract</summary>
There is considerable industrial interest in integrating AI techniques into railway systems, notably for fully autonomous train systems. The KI-LOK research project is involved in developing new methods for certifying such AI-based systems. Here we explore the utility of a certified control architecture for a runtime monitor that prevents false positive detection of traffic signs in an AI-based perception system. The monitor uses classical computer vision algorithms to check if the signs -- detected by an AI object detection model -- fit predefined specifications. We provide such specifications for some critical signs and integrate a Python prototype of the monitor with a popular object detection model to measure relevant performance metrics on generated data. Our initial results are promising, achieving considerable precision gains with only minor recall reduction; however, further investigation into generalization possibilities will be necessary.
</details>
<details>
<summary>摘要</summary>
有很大的工业兴趣在将人工智能技术应用于铁路系统中，特别是实现无人驾驶列车系统。KI-LOK研究项目正在开发新的认证方法，以确保这些基于AI的系统的可靠性。我们在这篇文章中探讨一种具有认证控制架构的运行监控系统，以防止AI基于感知系统中的假阳性检测交通标志。这个监控系统使用传统的计算机视觉算法来检查探测到的标志是否符合预定义的规范。我们为一些关键的标志提供了特定的规范，并将Python原型 integrate with a popular object detection model to measure relevant performance metrics on generated data。我们的初步结果很有前途，可以实现较大的准确率提升，但是需要进一步的探索可泛化性。
</details></li>
</ul>
<hr>
<h2 id="Video-LLaVA-Learning-United-Visual-Representation-by-Alignment-Before-Projection"><a href="#Video-LLaVA-Learning-United-Visual-Representation-by-Alignment-Before-Projection" class="headerlink" title="Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"></a>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10122">http://arxiv.org/abs/2311.10122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-YuanGroup/Video-LLaVA">https://github.com/PKU-YuanGroup/Video-LLaVA</a></li>
<li>paper_authors: Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, Li Yuan</li>
<li>for: 提高视觉语言理解的下游任务性能</li>
<li>methods: 将图像和视频编码到一个共享特征空间中，并将其作为大语言模型的输入</li>
<li>results: 创建了一个简单 yet robust LVLM 基线模型 Video-LLaVA，可以从混合图像和视频 dataset 中学习并提高多模态交互。Video-LLaVA 在 9 个图像benchmark 上表现出色，并在 5 个图像问答dataset 和 4 个图像benchmark toolkits 上超过 Video-ChatGPT。<details>
<summary>Abstract</summary>
The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos.
</details>
<details>
<summary>摘要</summary>
大型视语模型（LVLM）已经提高了多个下游任务的性能，包括图像和视频理解。大多数现有的方法将图像和视频编码为分开的特征空间，然后将其作为大型语言模型的输入。然而，由于图像和视频的不一致编码，即 projection 层的不一致，使得大型语言模型（LLM）学习多modal交互变得困难。在这种情况下，我们将视觉表示 integrate 到语言特征空间中，以提高基础的 LLM  towards 一体化 LVLM。因此，我们建立了简单 yet robust 的 LVM 基eline，称为 Video-LLaVA，它从混合的图像和视频数据集中学习，并且互相增强。 Video-LLaVA 在 9 个图像benchmark上 achieve 出色的表现，包括 5 个图像问答数据集和 4 个图像benchmark工具kit。此外，我们的 Video-LLaVA 还比 Video-ChatGPT 在 MSRVTT、MSVD、TGIF 和 ActivityNet 上表现出色，提高了 5.8%、9.9%、18.6% 和 10.1%。值得注意的是，广泛的实验表明，Video-LLaVA 能够在一个统一的视觉表示中促进图像和视频之间的互助，并且在图像和视频特征空间中提高表现，比特定于图像或视频的模型更好。
</details></li>
</ul>
<hr>
<h2 id="Slide-SAM-Medical-SAM-Meets-Sliding-Window"><a href="#Slide-SAM-Medical-SAM-Meets-Sliding-Window" class="headerlink" title="Slide-SAM: Medical SAM Meets Sliding Window"></a>Slide-SAM: Medical SAM Meets Sliding Window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10121">http://arxiv.org/abs/2311.10121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Quan, Fenghe Tang, Zikang Xu, Heqin Zhu, S. Kevin Zhou</li>
<li>For: This paper proposes a new method called Slide-SAM for 3D medical image segmentation, which extends the Segment Anything Model (SAM) to 3D medical images.* Methods: The proposed method uses a single slice prompt to segment the entire volume, reducing the prompt workload for professionals. It also uses high resolution (H$ \times $W &#x3D; 1024$ \times $1024) for training in 3D images to achieve optimal learning for small targets.* Results: The proposed method was evaluated on multiple datasets and achieved the most advanced 3D segmentation performance while maintaining the minimum prompt. The code will be open source soon.Here’s the summary in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种新方法 called Slide-SAM，用于3D医学图像分割。</li>
<li>methods: 该方法使用单个slice提示来分割整个量子，减少专业人员的提示工作负担。它还使用高分辨率（H$ \times $W &#x3D; 1024$ \times $1024）在3D图像上进行训练，以便在小目标上达到优化的学习。</li>
<li>results: 该方法在多个数据集上进行了评估，并达到了最高的3D分割性能，同时保持最小的提示。代码即将公开源代码。<details>
<summary>Abstract</summary>
Segment Anything Model (SAM) achieves remarkable results in 2D image segmentation of natural images. However, the huge gap between medical images and natural images prevents it directly applied to medical image segmentation tasks. Especially in 3D medical image, SAM cannot learn the contextual relationship between slices, which limites application in real scenarios. In addition, recent research shows that applying 2D SAM to 3D images requires prompting the entire volume, which is time and label comsuming. In order to solve the above problems, we introduced Slide-SAM which extended SAM to 3D medical images. Specifically, you only need to use a single slice prompt to segement the entire volume, which greatly reduces the prompt workload for professionals. Secondly, unlike traditional 3D medical image segmentation, we are free from the influence of computing resources and can still use high resolution (H$ \times $W = 1024$ \times $1024) for training in 3D images to achieve optimal learning for small targets. This is to combine the entire 3D volume is beyond the reach of training. Finally, we collected a large number of 3D images from large-scale 3D public and private datasets, and extended SAM to 3D medical image segmentation involving bounding box and point prompts. Finally, we perform a comprehensive evaluation and analysis investigating the performance of Slide-SAM in medical image segmentation of different modalities, anatomy, and organs. We have verified Slide-SAM's segmentation capabilities on multiple datasets, achieving the most advanced 3D segmentation performance while maintaining the minimum prompt. Code will be open source soon.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 在自然图像2D分割任务上取得了惊人的结果。然而，医疗图像与自然图像之间的巨大差距使得SAM直接应用于医疗图像分割任务是不可能的。尤其是在3D医疗图像中，SAM无法学习层次关系 между slice，这限制了其在实际场景中的应用。此外， latest research 表明，将2D SAM应用于3D图像需要整个Volume提示，这是时间和标签耗费的。为解决以上问题，我们提出了Slide-SAM，它将SAM扩展到3D医疗图像。具体来说，只需使用单个slice提示可以分割整个Volume，这大幅减少了专业人员的提示工作负担。其次，与传统3D医疗图像分割不同，我们不受计算资源的限制，可以在3D图像的训练中使用高分辨率（H $\times $ W = 1024 $\times $ 1024），以达到最佳学习效果。最后，我们收集了大量3D图像从大规模的3D公共和私人数据集，并将SAM扩展到3D医疗图像分割，包括 bounding box 和点提示。我们进行了全面的评估和分析，investigating 3D segmentation的不同Modalities、Anatomy 和器官的性能。我们已经证明Slide-SAM在不同Modalities、Anatomy 和器官上的 segmentation 性能是最先进的，同时保持最少的提示。代码即将公开源代码。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-dataset-affinity-prediction-in-object-detection-to-assess-training-data"><a href="#Utilizing-dataset-affinity-prediction-in-object-detection-to-assess-training-data" class="headerlink" title="Utilizing dataset affinity prediction in object detection to assess training data"></a>Utilizing dataset affinity prediction in object detection to assess training data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09768">http://arxiv.org/abs/2311.09768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Becker, Jens Bayer, Ronny Hug, Wolfgang Hübner, Michael Arens</li>
<li>for: 提高对象探测器的训练效果和一致性，并且可以采用不同的汽车数据集来增强模型的通用性。</li>
<li>methods: 提出了一种在检测时加入数据源预测模块的方法，以便在训练时更好地评估数据集的信息价值，从而提高对象探测器的性能。</li>
<li>results: 研究表明，通过自动选择不同数据集中的样本，可以训练对象探测器使用较少的训练样本，而无需失去检测精度。<details>
<summary>Abstract</summary>
Data pooling offers various advantages, such as increasing the sample size, improving generalization, reducing sampling bias, and addressing data sparsity and quality, but it is not straightforward and may even be counterproductive. Assessing the effectiveness of pooling datasets in a principled manner is challenging due to the difficulty in estimating the overall information content of individual datasets. Towards this end, we propose incorporating a data source prediction module into standard object detection pipelines. The module runs with minimal overhead during inference time, providing additional information about the data source assigned to individual detections. We show the benefits of the so-called dataset affinity score by automatically selecting samples from a heterogeneous pool of vehicle datasets. The results show that object detectors can be trained on a significantly sparser set of training samples without losing detection accuracy.
</details>
<details>
<summary>摘要</summary>
数据聚合提供了各种优势，如增加样本大小、改善泛化、减少采样偏见和解决数据稀缺和质量问题，但不是直接的并可能是Counterproductive。评估聚合dataset的效果是有挑战的，因为难以估计个体dataset的总信息内容。为此，我们提议在标准对象检测管道中添加数据源预测模块。该模块在推理时间产生较少的开销，为个体检测提供额外信息关于分配给它的数据源。我们显示了所谓的数据源相互邻接分数的好处，自动从多种交通工具数据集中选择样本。结果表明，可以通过减少训练样本的数量来训练对象检测器，而不会影响检测精度。
</details></li>
</ul>
<hr>
<h2 id="Scene-Text-Image-Super-resolution-based-on-Text-conditional-Diffusion-Models"><a href="#Scene-Text-Image-Super-resolution-based-on-Text-conditional-Diffusion-Models" class="headerlink" title="Scene Text Image Super-resolution based on Text-conditional Diffusion Models"></a>Scene Text Image Super-resolution based on Text-conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09759">http://arxiv.org/abs/2311.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chihiro Noguchi, Shun Fukuda, Masao Yamanaka<br>for: 这 paper 的目的是提出一种基于文本条件扩散模型（DM）的Scene Text Image Super-resolution（STISR）方法，以提高Scene Text Recognition（STR）的性能。methods: 这 paper 使用了文本条件扩散模型（DM），其能够Synthesize高分辨率（HR）文本图像，从而提高STR的性能。results:  experiments 表明，使用文本条件扩散模型（DM）可以 notable improve STISR 的性能，特别是当输入为低分辨率（LR）文本图像时。此外，该方法还可以生成高分辨率（HR）和低分辨率（LR）的对应图像对，为 STR 的训练提供了更好的数据支持。<details>
<summary>Abstract</summary>
Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DIFFNAT-Improving-Diffusion-Image-Quality-Using-Natural-Image-Statistics"><a href="#DIFFNAT-Improving-Diffusion-Image-Quality-Using-Natural-Image-Statistics" class="headerlink" title="DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics"></a>DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09753">http://arxiv.org/abs/2311.09753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Roy, Maiterya Suin, Anshul Shah, Ketul Shah, Jiang Liu, Rama Chellappa</li>
<li>for: 提高生成图像质量</li>
<li>methods: 使用 Kurtosis Concentration (KC) 损失函数</li>
<li>results: 在三种不同任务中（1）个性化几何折衔练习、（2）无条件图像生成、（3）图像超分辨率）中提高了 perceived 质量， measured by FID、MUSIQ score 和用户评价。<details>
<summary>Abstract</summary>
Diffusion models have advanced generative AI significantly in terms of editing and creating naturalistic images. However, efficiently improving generated image quality is still of paramount interest. In this context, we propose a generic "naturalness" preserving loss function, viz., kurtosis concentration (KC) loss, which can be readily applied to any standard diffusion model pipeline to elevate the image quality. Our motivation stems from the projected kurtosis concentration property of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. To retain the "naturalness" of the generated images, we enforce reducing the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note that our approach does not require any additional guidance like classifier or classifier-free guidance to improve the image quality. We validate the proposed approach for three diverse tasks, viz., (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss has improved the perceptual quality across all these tasks in terms of both FID, MUSIQ score, and user evaluation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have advanced generative AI significantly in terms of editing and creating naturalistic images. However, improving the quality of generated images efficiently is still a top priority. To address this, we propose a generic "naturalness" preserving loss function, called kurtosis concentration (KC) loss, which can be easily applied to any standard diffusion model pipeline to enhance image quality. Our motivation comes from the projected kurtosis concentration property of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. To retain the "naturalness" of the generated images, we enforce reducing the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note that our approach does not require any additional guidance like classifier or classifier-free guidance to improve the image quality. We validate the proposed approach for three diverse tasks, namely (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss has improved the perceptual quality across all these tasks in terms of both FID, MUSIQ score, and user evaluation.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Map-Guided-Adaptive-Domain-Generalization-for-Cross-Modality-MRI-Segmentation"><a href="#Gradient-Map-Guided-Adaptive-Domain-Generalization-for-Cross-Modality-MRI-Segmentation" class="headerlink" title="Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation"></a>Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09737">http://arxiv.org/abs/2311.09737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuttle-fish-my/gm-guided-dg">https://github.com/cuttle-fish-my/gm-guided-dg</a></li>
<li>paper_authors: Bingnan Li, Zhitong Gao, Xuming He</li>
<li>for: 这个研究旨在提高计算机支持医学诊断的跨Modal MRI标本分类，以实现跨域数据收集和模型通用性。</li>
<li>methods: 我们提出了一个新的适应领域扩展框架，它结合了无学习交叉领域表示基于图像梯度地图和一种基于分类前置检测适应策略，以减少地方领域迁移。</li>
<li>results: 我们在两个多Modal MRI数据集上验证了我们的方法，包括六个跨Modal MRI标本分类任务。 在所有任务设置下，我们的方法一直超过竞争方法，并且在有限的训练数据下保持稳定的性能。<details>
<summary>Abstract</summary>
Cross-modal MRI segmentation is of great value for computer-aided medical diagnosis, enabling flexible data acquisition and model generalization. However, most existing methods have difficulty in handling local variations in domain shift and typically require a significant amount of data for training, which hinders their usage in practice. To address these problems, we propose a novel adaptive domain generalization framework, which integrates a learning-free cross-domain representation based on image gradient maps and a class prior-informed test-time adaptation strategy for mitigating local domain shift. We validate our approach on two multi-modal MRI datasets with six cross-modal segmentation tasks. Across all the task settings, our method consistently outperforms competing approaches and shows a stable performance even with limited training data.
</details>
<details>
<summary>摘要</summary>
跨模态MRI分割是医疗辅助诊断中的非常有价值的技术，它允许悬浮数据采集和模型通用化。然而，大多数现有方法在域Shift问题上困难处理本地变化，通常需要大量数据进行训练，这会限制它们在实践中的使用。为解决这些问题，我们提出了一种新的适应域通用化框架，该框架 integrate了一种无需学习的跨Domain表示基于图像梯度地图和一种基于类偏置的测试时适应策略，以mitigate本地域Shift问题。我们在两个多模态MRI数据集上进行了六个跨模态分割任务的验证。在所有任务设置下，我们的方法始终超越了竞争方法，并在有限的训练数据下保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="MS-Former-Memory-Supported-Transformer-for-Weakly-Supervised-Change-Detection-with-Patch-Level-Annotations"><a href="#MS-Former-Memory-Supported-Transformer-for-Weakly-Supervised-Change-Detection-with-Patch-Level-Annotations" class="headerlink" title="MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations"></a>MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09726">http://arxiv.org/abs/2311.09726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanyuezhen/ms-former">https://github.com/guanyuezhen/ms-former</a></li>
<li>paper_authors: Zhenglai Li, Chang Tang, Xinwang Liu, Changdong Li, Xianju Li, Wei Zhang</li>
<li>for: 这 paper 是为了提出一种基于 patch-level 纪录的弱类标注下的变化检测方法。</li>
<li>methods: 该方法基于 transformer 架构，包括一个 bidirectional attention block (BAB) 和一个 patch-level supervision scheme (PSS)。BAB 通过从时间差特征中提取关于改变和不变区域的上下文信息，并将其存储在内存银行中。PSS 则使用 patch-level 纪录来引导网络学习有价值的知识，从而进一步提高表达能力。</li>
<li>results: 实验结果表明，该方法在三个标准测试集上达到了优秀的变化检测效果。<details>
<summary>Abstract</summary>
Fully supervised change detection methods have achieved significant advancements in performance, yet they depend severely on acquiring costly pixel-level labels. Considering that the patch-level annotations also contain abundant information corresponding to both changed and unchanged objects in bi-temporal images, an intuitive solution is to segment the changes with patch-level annotations. How to capture the semantic variations associated with the changed and unchanged regions from the patch-level annotations to obtain promising change results is the critical challenge for the weakly supervised change detection task. In this paper, we propose a memory-supported transformer (MS-Former), a novel framework consisting of a bi-directional attention block (BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised change detection with patch-level annotations. More specifically, the BAM captures contexts associated with the changed and unchanged regions from the temporal difference features to construct informative prototypes stored in the memory bank. On the other hand, the BAM extracts useful information from the prototypes as supplementary contexts to enhance the temporal difference features, thereby better distinguishing changed and unchanged regions. After that, the PSS guides the network learning valuable knowledge from the patch-level annotations, thus further elevating the performance. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method in the change detection task. The demo code for our work will be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
</details>
<details>
<summary>摘要</summary>
具有全程监督改变方法在性能方面已经取得了显著进步，然而它们受到买到便宜的像素级标签的限制。考虑到bi-temporal图像中的块级注释也包含了改变和不改变对象之间的许多信息，因此一种直观的解决方案是将改变分割成块级注释。然而，如何从块级注释中捕捉改变和不改变区域之间的semantic变化，以获得出色的改变结果是critical挑战。在这篇论文中，我们提出了一种记忆支持的变换器（MS-Former），这是一种新的框架，包括一个bi-directional attention块（BAB）和一个块级监督方案（PSS），这些方案特地针对无监督改变检测任务。更加具体地说，BAB会从bi-temporal特征图中捕捉改变和不改变区域之间的上下文，并将其存储在内存银行中。然后，BAB会从内存银行中提取有用的信息，以增强bi-temporal特征图中的时间差特征，从而更好地 отличи改变和不改变区域。同时，PSS会引导网络学习从块级注释中得到有价值的知识，从而进一步提高性能。我们的实验结果表明，我们的提议方法在改变检测任务中具有出色的效果。我们将在 \url{https://github.com/guanyuezhen/MS-Former} 上公开发布我们的代码示例。
</details></li>
</ul>
<hr>
<h2 id="Now-and-Future-of-Artificial-Intelligence-based-Signet-Ring-Cell-Diagnosis-A-Survey"><a href="#Now-and-Future-of-Artificial-Intelligence-based-Signet-Ring-Cell-Diagnosis-A-Survey" class="headerlink" title="Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey"></a>Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10118">http://arxiv.org/abs/2311.10118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Meng, Junhao Dong, Limei Guo, Fei Su, Guangxi Wang, Zhicheng Zhao</li>
<li>for: 本研究主要为了提供一份从2008年到8月2023年的深度学习驱动的筛 Cell（SRC）分析的综述，帮助无医学背景的研究人员更好地了解SRC的生物学特征和自动识别挑战，以及现有算法的表现和未来趋势。</li>
<li>methods: 本文分析了SRC分析中使用的代表性算法，并对它们进行了分类、检测和分 segmentation 的比较分析。</li>
<li>results: 本文发现了SRC分析领域中的一些问题和未解决的问题，并提出了未来研究的方向和趋势，以帮助研究人员更好地理解SRC的生物学特征和自动识别挑战，以及现有算法的表现和未来趋势。<details>
<summary>Abstract</summary>
Since signet ring cells (SRCs) are associated with high peripheral metastasis rate and dismal survival, they play an important role in determining surgical approaches and prognosis, while they are easily missed by even experienced pathologists. Although automatic diagnosis SRCs based on deep learning has received increasing attention to assist pathologists in improving the diagnostic efficiency and accuracy, the existing works have not been systematically overviewed, which hindered the evaluation of the gap between algorithms and clinical applications. In this paper, we provide a survey on SRC analysis driven by deep learning from 2008 to August 2023. Specifically, the biological characteristics of SRCs and the challenges of automatic identification are systemically summarized. Then, the representative algorithms are analyzed and compared via dividing them into classification, detection, and segmentation. Finally, for comprehensive consideration to the performance of existing methods and the requirements for clinical assistance, we discuss the open issues and future trends of SRC analysis. The retrospect research will help researchers in the related fields, particularly for who without medical science background not only to clearly find the outline of SRC analysis, but also gain the prospect of intelligent diagnosis, resulting in accelerating the practice and application of intelligent algorithms.
</details>
<details>
<summary>摘要</summary>
自 signet 环绕细胞（SRC）与高周边肿瘤率和减少生存率之间的关系，使得 SRC 在决定手术方法和诊断的重要作用。然而，经验 Pathologist 可能会扫描到这些细胞，尽管 automatic 诊断 SRC 基于深度学习已经收到了提高诊断效率和准确性的关注。在这篇文章中，我们提供了从 2008 年到 8 月 2023 年的 SRC 分析驱动深度学习的报告。specifically，我们系统地概述了 SRC 的生物特征和自动识别的挑战。然后，我们分析了代表性的算法，并将其分为分类、检测和分 segmentation 三个部分进行比较。最后，为了全面评估现有方法的性能和临床应用的需求，我们讨论了开放问题和未来趋势。这些研究将帮助相关领域的研究人员，特别是没有医学背景的研究人员，不仅能够清楚地了解 SRC 分析的大纲，而且能够获得智能诊断的前景，从而加速智能算法的实践和应用。
</details></li>
</ul>
<hr>
<h2 id="Robust-Contrastive-Learning-With-Theory-Guarantee"><a href="#Robust-Contrastive-Learning-With-Theory-Guarantee" class="headerlink" title="Robust Contrastive Learning With Theory Guarantee"></a>Robust Contrastive Learning With Theory Guarantee</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09671">http://arxiv.org/abs/2311.09671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc N. Tran, Lam Tran, Hoang Phan, Anh Bui, Tung Pham, Toan Tran, Dinh Phung, Trung Le</li>
<li>for: 这个论文的目的是探讨contrastive learning（CL）自我超vised训练方法中的不supervised预处理阶段是如何支持supervised预处理阶段的。</li>
<li>methods: 这个论文使用了一种分两个阶段的CL框架，首先从无标记数据中学习特征，然后使用这些特征来训练一个线性分类器。</li>
<li>results: 研究发现，在CL的第一个阶段中使用的不supervised损失函数对于在第二个阶段的supervised损失函数的提升有着重要的作用。同时，研究还发现了一些关键的组成部分在不supervised损失函数中，可以帮助提高supervised损失函数的稳定性和性能。<details>
<summary>Abstract</summary>
Contrastive learning (CL) is a self-supervised training paradigm that allows us to extract meaningful features without any label information. A typical CL framework is divided into two phases, where it first tries to learn the features from unlabelled data, and then uses those features to train a linear classifier with the labeled data. While a fair amount of existing theoretical works have analyzed how the unsupervised loss in the first phase can support the supervised loss in the second phase, none has examined the connection between the unsupervised loss and the robust supervised loss, which can shed light on how to construct an effective unsupervised loss for the first phase of CL. To fill this gap, our work develops rigorous theories to dissect and identify which components in the unsupervised loss can help improve the robust supervised loss and conduct proper experiments to verify our findings.
</details>
<details>
<summary>摘要</summary>
《对比学习（Contrastive Learning，CL）是一种自动标注训练方法，它允许我们从无标签数据中提取有意义的特征。一个典型的CL框架包括两个阶段，第一阶段是从无标签数据中学习特征，第二阶段是使用这些特征来训练一个线性分类器与标签数据进行训练。虽然一些现有的理论研究已经分析了如何在第一阶段中学习的无监督损失如何支持第二阶段的监督损失，但是没有研究过无监督损失与鲁棒监督损失之间的连接，这可以推熔到如何构建有效的无监督损失。为了填补这个差距，我们的工作开发了准确的理论来分析无监督损失中的各个组件是否能够提高鲁棒监督损失，并进行了相应的实验验证。》Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any further questions or need any adjustments, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Spectrogram-Transformer-for-Respiratory-Sound-Classification"><a href="#Multi-View-Spectrogram-Transformer-for-Respiratory-Sound-Classification" class="headerlink" title="Multi-View Spectrogram Transformer for Respiratory Sound Classification"></a>Multi-View Spectrogram Transformer for Respiratory Sound Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09655">http://arxiv.org/abs/2311.09655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao He, Yuchen Yan, Jianfeng Ren, Ruibin Bai, Xudong Jiang</li>
<li>for: 这篇论文旨在应用深度神经网络来分类呼吸声音。</li>
<li>methods: 该论文提出了一种多观点spectrogram对称trasformer（MVST），将不同大小的mel-spectrogram分割为多个视角的音频元件，然后使用对称转构器将这些元件转换为自适应特征。</li>
<li>results: 实验结果显示，该提出的MVST方法较前一项方法有更好的表现，在ICBHI数据集上分类呼吸声音的任务中。<details>
<summary>Abstract</summary>
Deep neural networks have been applied to audio spectrograms for respiratory sound classification. Existing models often treat the spectrogram as a synthetic image while overlooking its physical characteristics. In this paper, a Multi-View Spectrogram Transformer (MVST) is proposed to embed different views of time-frequency characteristics into the vision transformer. Specifically, the proposed MVST splits the mel-spectrogram into different sized patches, representing the multi-view acoustic elements of a respiratory sound. These patches and positional embeddings are then fed into transformer encoders to extract the attentional information among patches through a self-attention mechanism. Finally, a gated fusion scheme is designed to automatically weigh the multi-view features to highlight the best one in a specific scenario. Experimental results on the ICBHI dataset demonstrate that the proposed MVST significantly outperforms state-of-the-art methods for classifying respiratory sounds.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经应用于音频spectrogram中的呼吸音分类。现有模型 oftentimes treat spectrogram as synthetic image，忽略其物理特征。本文提出了 Multi-View Spectrogram Transformer (MVST)，用于嵌入不同视图的时间频谱特征。具体来说，提案的MVST将mel-spectrogram分割成不同大小的patches，表示呼吸音的多视图听音元件。这些patches和位域嵌入被Feed into transformer encoder中，通过自我注意机制提取多视图特征之间的关注信息。最后，设计了一种权重权重调整方案，以自动将多视图特征相互权重，以便在特定场景下高亮最佳的一个视图特征。实验结果表明，提案的MVST在ICBHI数据集上明显超过了现有方法，用于分类呼吸音。
</details></li>
</ul>
<hr>
<h2 id="Improved-TokenPose-with-Sparsity"><a href="#Improved-TokenPose-with-Sparsity" class="headerlink" title="Improved TokenPose with Sparsity"></a>Improved TokenPose with Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09653">http://arxiv.org/abs/2311.09653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anning Li</li>
<li>for: 人体姿态估计</li>
<li>methods: 使用简单的隐藏状态和自动重要性来降低计算复杂度，并通过对键点和视觉特征进行精细化来提高人体姿态估计的精度。</li>
<li>results: 在MPII数据集上实现新的状态艺术纪录，并证明了方法的可行性。<details>
<summary>Abstract</summary>
Over the past few years, the vision transformer and its various forms have gained significance in human pose estimation. By treating image patches as tokens, transformers can capture global relationships wisely, estimate the keypoint tokens by leveraging the visual tokens, and recognize the posture of the human body. Nevertheless, global attention is computationally demanding, which poses a challenge for scaling up transformer-based methods to high-resolution features. In this paper, we introduce sparsity in both keypoint token attention and visual token attention to improve human pose estimation. Experimental results on the MPII dataset demonstrate that our model has a higher level of accuracy and proved the feasibility of the method, achieving new state-of-the-art results. The idea can also provide references for other transformer-based models.
</details>
<details>
<summary>摘要</summary>
在过去几年，视力变换器和其多种形式在人体姿态估计中具有重要意义。通过将图像块看作为符号，变换器可以聪明地捕捉全局关系，根据视觉符号来估计关键点符号，并识别人体姿态。然而，全球注意力计算具有挑战性，这会对基于变换器的方法的扩大到高分辨率特征造成挑战。在这篇论文中，我们引入了缺失在关键点符号注意力和视觉符号注意力中，以提高人体姿态估计的精度。实验结果表明，我们的模型在MPII数据集上达到了新的州OF-the-artResult，并证明了方法的可行性。这个想法还可以作为其他基于变换器的模型的参考。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Motion-Robust-Accurate-Shape-Estimation-for-Mixed-Reflectance-Scenes"><a href="#Event-based-Motion-Robust-Accurate-Shape-Estimation-for-Mixed-Reflectance-Scenes" class="headerlink" title="Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes"></a>Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09652">http://arxiv.org/abs/2311.09652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Dashpute, Jiazhang Wang, James Taylor, Oliver Cossairt, Ashok Veeraraghavan, Florian Willomitzer</li>
<li>for: fast and motion-robust 3D imaging of mixed reflectance scenes</li>
<li>methods: event-based structured light system, epipolar constraints, triangulation, deflectometry</li>
<li>results: high accuracy (&lt;500μm) and fast capture speed (14Hz or 250Hz) for mixed reflectance scenes<details>
<summary>Abstract</summary>
Event-based structured light systems have recently been introduced as an exciting alternative to conventional frame-based triangulation systems for the 3D measurements of diffuse surfaces. Important benefits include the fast capture speed and the high dynamic range provided by the event camera - albeit at the cost of lower data quality. So far, both low-accuracy event-based as well as high-accuracy frame-based 3D imaging systems are tailored to a specific surface type, such as diffuse or specular, and can not be used for a broader class of object surfaces ("mixed reflectance scenes"). In this paper, we present a novel event-based structured light system that enables fast 3D imaging of mixed reflectance scenes with high accuracy. On the captured events, we use epipolar constraints that intrinsically enable decomposing the measured reflections into diffuse, two-bounce specular, and other multi-bounce reflections. The diffuse objects in the scene are reconstructed using triangulation. Eventually, the reconstructed diffuse scene parts are used as a "display" to evaluate the specular scene parts via deflectometry. This novel procedure allows us to use the entire scene as a virtual screen, using only a scanning laser and an event camera. The resulting system achieves fast and motion-robust (14Hz) reconstructions of mixed reflectance scenes with < 500 $\mu$m accuracy. Moreover, we introduce a "superfast" capture mode (250Hz) for the 3D measurement of diffuse scenes.
</details>
<details>
<summary>摘要</summary>
现代事件驱动的探讨光系统已经被引入为diffuse表面三维测量的新型代替方案，具有快速捕捉速度和高动态范围。然而，这些系统的数据质量相对较低，而且仅适用于特定表面类型（diffuse或speculative），无法应用于更广泛的物体表面（"混合反射场景"）。在这篇论文中，我们提出了一种新的事件驱动探讨光系统，可以快速地三维测量混合反射场景，并且具有高准确性。在捕捉到的事件上，我们使用epipolar约束，可以自动分解测量的反射为diffuse、两次反射和其他多次反射。diffuse对象在场景中被重建，并用三角测量来重建。最后，重建的diffuse场景部分被用作"显示"，以评估speculative场景部分via折射。这种新的程序让我们可以使用扫描 láser和事件相机来扫描整个场景，并且实现快速和运动稳定（14Hz）的重建混合反射场景，具有<500μm的准确性。此外，我们还介绍了"超快"捕捉模式（250Hz），用于三维测量diffuse场景。
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-Continuous-Light-Field-From-Single-Coded-Image"><a href="#Reconstructing-Continuous-Light-Field-From-Single-Coded-Image" class="headerlink" title="Reconstructing Continuous Light Field From Single Coded Image"></a>Reconstructing Continuous Light Field From Single Coded Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09646">http://arxiv.org/abs/2311.09646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Ishikawa, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii</li>
<li>for:  reconstruction of continuous light fields of a target scene from a single observed image</li>
<li>methods:  joint aperture-exposure coding and neural radiance field (NeRF)</li>
<li>results:  accurate and efficient reconstruction of continuous light fields without test time optimization, bridging the gap between camera design and neural rendering.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究旨在从单个观察图像中重建目标场景的连续光场。</li>
<li>methods: 这个方法结合了共同的开口-曝光编码和神经辐射场（NeRF）来实现视觉合成。</li>
<li>results: 这个方法可以高效地和高质量地重建连续光场，不需要任何测试时间优化。这是我们知道的第一个将摄像头设计和神经渲染相结合的研究。<details>
<summary>Abstract</summary>
We propose a method for reconstructing a continuous light field of a target scene from a single observed image. Our method takes the best of two worlds: joint aperture-exposure coding for compressive light-field acquisition, and a neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding implemented in a camera enables effective embedding of 3-D scene information into an observed image, but in previous works, it was used only for reconstructing discretized light-field views. NeRF-based neural rendering enables high quality view synthesis of a 3-D scene from continuous viewpoints, but when only a single image is given as the input, it struggles to achieve satisfactory quality. Our method integrates these two techniques into an efficient and end-to-end trainable pipeline. Trained on a wide variety of scenes, our method can reconstruct continuous light fields accurately and efficiently without any test time optimization. To our knowledge, this is the first work to bridge two worlds: camera design for efficiently acquiring 3-D information and neural rendering.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个观察到的图像中重建连续的光场场景。我们的方法结合了两种世界的优点：joint aperature-exposure coding for compressive light-field acquisition,和基于神经辐射场（NeRF）的视觉合成。 joint aperature-exposure coding在摄像头中实现了有效地嵌入3D场景信息到观察到的图像中，但在前一些工作中，它只用于重建精确的光场观察角度。基于NeRF的神经渲染可以高质量地合成3D场景的视角，但当只有单个图像作为输入时，它很难达到满意的质量。我们的方法将这两种技术集成成一个高效、端到端训练可以的管道。我们在各种场景下训练了这种方法，可以高效地和高质量地重建连续的光场场景，无需任何测试时间优化。到我们所知，这是第一次将摄像头设计用于高效地获取3D信息和神经渲染相结合。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Anomaly-Detection-for-Chest-X-Ray-Image"><a href="#Weakly-Supervised-Anomaly-Detection-for-Chest-X-Ray-Image" class="headerlink" title="Weakly Supervised Anomaly Detection for Chest X-Ray Image"></a>Weakly Supervised Anomaly Detection for Chest X-Ray Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09642">http://arxiv.org/abs/2311.09642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamcuriosity/wscxr">https://github.com/iamcuriosity/wscxr</a></li>
<li>paper_authors: Haoqi Ni, Ximiao Zhang, Min Xu, Ning Lang, Xiuzhuang Zhou</li>
<li>for: 本研究旨在提出一种基于weakly supervised learning的额外肺X光图像异常检测方法，以便在临床应用中更好地检测肺疾病。</li>
<li>methods: 本方法首先构建了normal和异常图像特征集合，然后通过异常特征挖掘来除去正常区域特征，以便全面利用疾病区域的珍贵特征。此外，本方法还使用了线性混合策略来增强异常检测器的训练。</li>
<li>results:  experiments表明，本方法在两个肺X光图像 Dataset上显示了效果。<details>
<summary>Abstract</summary>
Chest X-Ray (CXR) examination is a common method for assessing thoracic diseases in clinical applications. While recent advances in deep learning have enhanced the significance of visual analysis for CXR anomaly detection, current methods often miss key cues in anomaly images crucial for identifying disease regions, as they predominantly rely on unsupervised training with normal images. This letter focuses on a more practical setup in which few-shot anomaly images with only image-level labels are available during training. For this purpose, we propose WSCXR, a weakly supervised anomaly detection framework for CXR. WSCXR firstly constructs sets of normal and anomaly image features respectively. It then refines the anomaly image features by eliminating normal region features through anomaly feature mining, thus fully leveraging the scarce yet crucial features of diseased areas. Additionally, WSCXR employs a linear mixing strategy to augment the anomaly features, facilitating the training of anomaly detector with few-shot anomaly images. Experiments on two CXR datasets demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
骨肋X射影（CXR）检测是诊断 thoracic 疾病的常用方法。Recent advances in deep learning 使得视觉分析在 CXR 畸形检测中具有更大的重要性，但现有方法通常会遗漏疾病区域中的关键提示，因为它们主要依靠 normal 图像进行无监督训练。这封信件关注一种更实用的设置，在训练过程中仅有几张畸形图像和图像水平标签可用。为此，我们提出了 WSCXR，一种弱型监督畸形检测框架 для CXR。WSCXR 首先构建 normal 和畸形图像特征集，然后通过畸形特征挖掘，减少疾病区域特征，从而全面利用疾病区域中的珍贵特征。此外，WSCXR 采用了线性混合策略，以增强畸形特征的训练，使用几张畸形图像进行检测。实验表明，我们的方法有效地检测 CXR 畸形。
</details></li>
</ul>
<hr>
<h2 id="On-the-Quantification-of-Image-Reconstruction-Uncertainty-without-Training-Data"><a href="#On-the-Quantification-of-Image-Reconstruction-Uncertainty-without-Training-Data" class="headerlink" title="On the Quantification of Image Reconstruction Uncertainty without Training Data"></a>On the Quantification of Image Reconstruction Uncertainty without Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09639">http://arxiv.org/abs/2311.09639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sirui Bi, Victor Fung, Jiaxin Zhang</li>
<li>for:  This paper focuses on developing a deep variational framework for image reconstruction and uncertainty estimation in computational imaging.</li>
<li>methods: The proposed method leverages a deep generative model to learn an approximate posterior distribution for image reconstruction uncertainty, using a flow-based model and gradient boosting for robustness and expressiveness.</li>
<li>results: The method is validated on several benchmark tasks and two real-world applications, demonstrating reliable and high-quality image reconstruction with robust uncertainty estimation.<details>
<summary>Abstract</summary>
Computational imaging plays a pivotal role in determining hidden information from sparse measurements. A robust inverse solver is crucial to fully characterize the uncertainty induced by these measurements, as it allows for the estimation of the complete posterior of unrecoverable targets. This, in turn, facilitates a probabilistic interpretation of observational data for decision-making. In this study, we propose a deep variational framework that leverages a deep generative model to learn an approximate posterior distribution to effectively quantify image reconstruction uncertainty without the need for training data. We parameterize the target posterior using a flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve accurate uncertainty estimation. To bolster stability, we introduce a robust flow-based model with bi-directional regularization and enhance expressivity through gradient boosting. Additionally, we incorporate a space-filling design to achieve substantial variance reduction on both latent prior space and target posterior space. We validate our method on several benchmark tasks and two real-world applications, namely fastMRI and black hole image reconstruction. Our results indicate that our method provides reliable and high-quality image reconstruction with robust uncertainty estimation.
</details>
<details>
<summary>摘要</summary>
计算成像在捕捉隐藏信息方面发挥关键作用，从稀缺测量中推断出隐藏信息的不确定性需要一个坚固的 inverse solver。这样可以全面描述测量过程中induced的uncertainty，并且使得观察数据的概率解释变得可能，从而帮助做出决策。在这个研究中，我们提出了一个深度变量框架，该框架利用深度生成模型来学习一个近似 posterior distribution，以便有效地量ify image reconstruction uncertainty，无需训练数据。我们使用流基本模型来参数化目标 posterior，并通过最小化其Kullback-Leibler（KL）偏度来实现准确的 uncertainty estimation。为了增强稳定性，我们引入了bi-directional regularization和扩展表达能力通过梯度批处理。此外，我们采用了填充设计，以实现在latent prior空间和目标 posterior空间上的重要variance reduction。我们在多个benchmark任务和两个实际应用中，即fastMRI和黑洞图像重建中 validate our方法，结果表明我们的方法可以提供可靠和高质量的图像重建，同时也可以提供准确的 uncertainty estimation。
</details></li>
</ul>
<hr>
<h2 id="DECDM-Document-Enhancement-using-Cycle-Consistent-Diffusion-Models"><a href="#DECDM-Document-Enhancement-using-Cycle-Consistent-Diffusion-Models" class="headerlink" title="DECDM: Document Enhancement using Cycle-Consistent Diffusion Models"></a>DECDM: Document Enhancement using Cycle-Consistent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09625">http://arxiv.org/abs/2311.09625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Joy Rimchala, Lalla Mouatadid, Kamalika Das, Sricharan Kumar</li>
<li>for: 提高文档像素质量，以提高自动文档处理和文档智能。</li>
<li>methods: 基于Diffusion模型的终端到终端文档图像翻译方法，无需同时训练源和目标模型，可以应用到其他域对。</li>
<li>results: 与状态艺术方法相比，DECDM在多种 sintetic数据和benchmark datasets上表现出色，可以 Quantitatively和Qualitatively提高文档图像质量。<details>
<summary>Abstract</summary>
The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and {\color{black}shadow} removal, and demonstrate the superiority of performance quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
表现强大的光学字符识别（OCR）功能受到文档图像质量的限制，这对于自动文档处理和文档智能来说非常重要。然而，现有的文档增强方法通常需要监督数据对，这会导致数据分离和隐私保护的问题，使得这些方法难以适应新的域对。为解决这些问题，我们提出了DECDM，一种基于傅立叶分布模型的终端文档图像翻译方法。DECDM在无监督的情况下独立地训练源（噪音输入）和目标（清晰输出）模型，因此可以应用到其他对。DECDM在一个dataset上单独训练，不需要同时扫描两个dataset，从而有效地保护数据隐私。我们还介绍了一些简单的数据扩展策略，以保持字符形态的恒久性 durante la traducción。我们与状态之前的方法进行比较，并在多个合成数据和标准 benchmark datasets上进行评估，如文档噪音去除和阴影去除。我们的实验结果表明DECDM在量和质量上具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Apoptosis-classification-using-attention-based-spatio-temporal-graph-convolution-neural-network"><a href="#Apoptosis-classification-using-attention-based-spatio-temporal-graph-convolution-neural-network" class="headerlink" title="Apoptosis classification using attention based spatio temporal graph convolution neural network"></a>Apoptosis classification using attention based spatio temporal graph convolution neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09623">http://arxiv.org/abs/2311.09623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akash Awasthi</li>
<li>for: 本研究旨在提出一种基于注意力图像抽象的图像缓冲扩充网络，用于精准地分类细胞死亡。</li>
<li>methods: 该方法使用注意力图像抽象网络，考虑多个目标细胞之间的交互关系，并在视频序列中模型每个时刻点的关系。</li>
<li>results: 该方法可以准确地分类细胞死亡，同时考虑空间和时间关系。<details>
<summary>Abstract</summary>
Accurate classification of apoptosis plays an important role in cell biology research. There are many state-of-the-art approaches which use deep CNNs to perform the apoptosis classification but these approaches do not account for the cell interaction. Our paper proposes the Attention Graph spatio-temporal graph convolutional network to classify the cell death based on the target cells in the video. This method considers the interaction of multiple target cells at each time stamp. We model the whole video sequence as a set of graphs and classify the target cell in the video as dead or alive. Our method encounters both spatial and temporal relationships.
</details>
<details>
<summary>摘要</summary>
精准的细胞死亡分类在细胞生物研究中扮演着重要的角色。目前有许多先进的方法使用深度卷积神经网络进行细胞死亡分类，但这些方法不考虑细胞之间的互动。我们的论文提出了注意力图像空间时间卷积神经网络，用于基于目标细胞的视频中细胞死亡分类。这种方法考虑了每个时间戳的多个目标细胞之间的互动关系。我们将整个视频序列视为一系列图像，并将目标细胞在视频中分类为死亡或活着。我们的方法考虑了空间和时间关系。
</details></li>
</ul>
<hr>
<h2 id="Wildfire-Smoke-Detection-with-Cross-Contrast-Patch-Embedding"><a href="#Wildfire-Smoke-Detection-with-Cross-Contrast-Patch-Embedding" class="headerlink" title="Wildfire Smoke Detection with Cross Contrast Patch Embedding"></a>Wildfire Smoke Detection with Cross Contrast Patch Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10116">http://arxiv.org/abs/2311.10116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Wang, Cheng Xu, Adeel Akram, Zhilin Shan, Qixing Zhang</li>
<li>for: 本研究旨在提高Transformer基于深度网络的野火识别性能，特别是提高Transformer对烟雾特征的抽取能力。</li>
<li>methods: 本研究提出了 Cross Contrast Patch Embedding（CCPE）模块和Separable Negative Sampling Mechanism（SNSM），以提高网络对烟雾特征的抽取和识别性能。</li>
<li>results: 对RealFire Test dataset进行了广泛测试和评估，与基线检测模型相比，本研究的方法具有显著的性能提升。<details>
<summary>Abstract</summary>
The Transformer-based deep networks have increasingly shown significant advantages over CNNs. Some existing work has applied it in the field of wildfire recognition or detection. However, we observed that the vanilla Transformer is not friendly for extracting smoke features. Because low-level information such as color, transparency and texture is very important for smoke recognition, and transformer pays more attention to the semantic relevance between middle- or high-level features, and is not sensitive to the subtle changes of low-level features along the space. To solve this problem, we propose the Cross Contrast Patch Embedding(CCPE) module based on the Swin Transformer, which uses the multi-scales spatial frequency contrast information in both vertical and horizontal directions to improve the discrimination of the network on the underlying details. The fuzzy boundary of smoke makes the positive and negative label assignment for instances in a dilemma, which is another challenge for wildfires detection. To solve this problem, a Separable Negative Sampling Mechanism(SNSM) is proposed. By using two different negative instance sampling strategies on positive images and negative images respectively, the problem of supervision signal confusion caused by label diversity in the process of network training is alleviated. This paper also releases the RealFire Test, the largest real wildfire test set so far, to evaluate the proposed method and promote future research. It contains 50,535 images from 3,649 video clips. The proposed method has been extensively tested and evaluated on RealFire Test dataset, and has a significant performance improvement compared with the baseline detection models.
</details>
<details>
<summary>摘要</summary>
《Transformer基于深度网络在野火识别方面的应用》 Introduction:现在的研究中，Transformer基于深度网络已经显示出了对于CNNs的明显优势。然而，我们发现了Transformer不适合提取烟雾特征。因为烟雾识别中低级信息如颜色、透明度和文本ure是非常重要的，而Transformer更关注中间或高级特征之间的 semantic relevance，并不敏感于空间方向中的细微变化。为解决这个问题，我们提出了基于Swin Transformer的 Cross Contrast Patch Embedding（CCPE）模块，利用多个比例的空间频率对比信息来提高网络对下面详细信息的推断。另外，野火检测中烟雾的模糊边界使得实例的正负标签分配困难，这也是一个挑战。为解决这个问题，我们提出了分解负采样机制（SNSM）。通过在正例图像和负例图像上采用不同的负采样策略，使得网络训练过程中的监督信号混乱问题得到了缓解。This paper also releases the RealFire Test, the largest real wildfire test set so far, to evaluate the proposed method and promote future research. It contains 50,535 images from 3,649 video clips. The proposed method has been extensively tested and evaluated on RealFire Test dataset, and has a significant performance improvement compared with the baseline detection models.
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans"><a href="#Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans" class="headerlink" title="Multi-Task Learning Approach for Unified Biometric Estimation from Fetal Ultrasound Anomaly Scans"></a>Multi-Task Learning Approach for Unified Biometric Estimation from Fetal Ultrasound Anomaly Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09607">http://arxiv.org/abs/2311.09607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans">https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans</a></li>
<li>paper_authors: Mohammad Areeb Qazi, Mohammed Talha Alam, Ibrahim Almakky, Werner Gerhard Diehl, Leanne Bricker, Mohammad Yaqub</li>
<li>for: The paper is written for estimating fetal biometry parameters from ultrasound images, which is crucial for evaluating fetal growth, monitoring health, and identifying potential complications.</li>
<li>methods: The paper proposes a multi-task learning approach that combines classification and segmentation to estimate fetal biometrics. The approach uses a U-Net architecture with an added classification head, and leverages a weighted joint classification and segmentation loss function to train the model.</li>
<li>results: The paper achieves a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44 mm on abdomen circumference, and 1.10 mm on femur length with a classification accuracy of 99.91% on a dataset of fetal ultrasound images.Here’s the information in Simplified Chinese text:</li>
<li>for: 本研究是为了从ultrasound图像中计算胎儿生长指标，这是评估胎儿生长、监测健康和识别潜在问题的关键。</li>
<li>methods: 本研究提出了一种多任务学习方法，即将分类和分割结合在一起，以便从ultrasound图像中计算胎儿生长指标。该方法使用了U-Net架构，并添加了一个分类头，以便在训练过程中使用加权共同分类和分割损失函数。</li>
<li>results: 本研究实现了head圈 circumference的平均绝对误差（MAE）为1.08 mm， Abdomen circumference的MAE为1.44 mm， femur length的MAE为1.10 mm，并达到了99.91%的分类精度在一个 dataset of fetal ultrasound images 中。<details>
<summary>Abstract</summary>
Precise estimation of fetal biometry parameters from ultrasound images is vital for evaluating fetal growth, monitoring health, and identifying potential complications reliably. However, the automated computerized segmentation of the fetal head, abdomen, and femur from ultrasound images, along with the subsequent measurement of fetal biometrics, remains challenging. In this work, we propose a multi-task learning approach to classify the region into head, abdomen and femur as well as estimate the associated parameters. We were able to achieve a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44 mm on abdomen circumference and 1.10 mm on femur length with a classification accuracy of 99.91\% on a dataset of fetal Ultrasound images. To achieve this, we leverage a weighted joint classification and segmentation loss function to train a U-Net architecture with an added classification head. The code can be accessed through \href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github}
</details>
<details>
<summary>摘要</summary>
准确估算胎儿生长指标从ultrasound图像是诊断胎儿增长、监测健康和识别问题的关键。然而，通过计算器自动分割ultrasound图像中的胎儿头、腹部和股骨，以及其后的胎儿生长指标的测量，仍然是一项挑战。在这种工作中，我们提议一种多任务学习方法，通过分类区域为头、腹部和股骨，同时估算相关参数。我们在一个胎儿ultrasound图像集上实现了mean absolute error（MAE）为1.08毫米的头圈 circumference，1.44毫米的腹部 circumference和1.10毫米的股骨长度，同时实现了99.91%的分类精度。为达到这一点，我们利用了一种权重加权的联合分类和分割损失函数，用于训练一个U-Net架构，并添加了一个分类头。代码可以通过\href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github} [
</details></li>
</ul>
<hr>
<h2 id="Gradual-Source-Domain-Expansion-for-Unsupervised-Domain-Adaptation"><a href="#Gradual-Source-Domain-Expansion-for-Unsupervised-Domain-Adaptation" class="headerlink" title="Gradual Source Domain Expansion for Unsupervised Domain Adaptation"></a>Gradual Source Domain Expansion for Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09599">http://arxiv.org/abs/2311.09599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThomasWestfechtel/GSDE">https://github.com/ThomasWestfechtel/GSDE</a></li>
<li>paper_authors: Thomas Westfechtel, Hao-Wei Yeh, Dexuan Zhang, Tatsuya Harada</li>
<li>for:  overcome the need for a large labeled dataset in unsupervised domain adaptation</li>
<li>methods:  gradual source domain expansion (GSDE) algorithm, training the UDA task several times from scratch with target data expansion</li>
<li>results:  outperform state-of-the-art methods on three benchmarks (Office-31, OfficeHome, and DomainNet) and improve the accuracy of a variety of different state-of-the-art UDA approaches.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) tries to overcome the need for a large labeled dataset by transferring knowledge from a source dataset, with lots of labeled data, to a target dataset, that has no labeled data. Since there are no labels in the target domain, early misalignment might propagate into the later stages and lead to an error build-up. In order to overcome this problem, we propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA task several times from scratch, each time reinitializing the network weights, but each time expands the source dataset with target data. In particular, the highest-scoring target data of the previous run are employed as pseudo-source samples with their respective pseudo-label. Using this strategy, the pseudo-source samples induce knowledge extracted from the previous run directly from the start of the new training. This helps align the two domains better, especially in the early training epochs. In this study, we first introduce a strong baseline network and apply our GSDE strategy to it. We conduct experiments and ablation studies on three benchmarks (Office-31, OfficeHome, and DomainNet) and outperform state-of-the-art methods. We further show that the proposed GSDE strategy can improve the accuracy of a variety of different state-of-the-art UDA approaches.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 尝试使用源数据集中具有很多标签数据的知识来推导目标数据集，该数据集没有标签。然而，在目标领域中的早期不一致可能会导致错误堆积。为解决这个问题，我们提出了慢步源领域扩展（GSDE）算法。GSDE 在 UDA 任务上进行多次从零开始训练，每次重新初始化网络权重，但每次扩展源数据集以包括目标数据。具体来说，上一轮最高分的目标数据被用作 Pseudo-source 样本，与其它 Pseudo-source 样本一起，直接从上一轮训练中提取了知识。这种策略可以更好地对两个领域进行对应，尤其是在训练的早期。在这个研究中，我们首先提出了一个强大的基线网络，然后应用我们的 GSDE 策略来改进其性能。我们在 Office-31、OfficeHome 和 DomainNet 三个标准测试集上进行了实验和剖析研究，并超越了当前最佳方法。此外，我们还证明了我们的 GSDE 策略可以提高多种不同的状态流行 UDA 方法的准确率。
</details></li>
</ul>
<hr>
<h2 id="MARformer-An-Efficient-Metal-Artifact-Reduction-Transformer-for-Dental-CBCT-Images"><a href="#MARformer-An-Efficient-Metal-Artifact-Reduction-Transformer-for-Dental-CBCT-Images" class="headerlink" title="MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images"></a>MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09590">http://arxiv.org/abs/2311.09590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Shi, Jun Xu, Dinggang Shen</li>
<li>for: 针对受到金属artefacts干扰的 dental CBCT 图像，提出了一种高效的 Transformer 来实现金属artefacts 减少 (MAR)。</li>
<li>methods: 提出了一种基于 globally 相似结构的 Dimension-Reduced Self-Attention (DRSA) 模块，以及一种基于 Patch-wise Perceptive Feed Forward Network (P2FFN) 的 fine-grained  восстановление模块。</li>
<li>results: 对 dental CBCT 图像进行了 Synthetic 和实际的 metal artefacts 测试，结果显示，我们的 MARformer 高效，并且超过了之前的 MAR 方法和两种 Restoration Transformers。<details>
<summary>Abstract</summary>
Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and surgery. However, the metal teeth implants could bring annoying metal artifacts during the CBCT imaging process, interfering diagnosis and downstream processing such as tooth segmentation. In this paper, we develop an efficient Transformer to perform metal artifacts reduction (MAR) from dental CBCT images. The proposed MAR Transformer (MARformer) reduces computation complexity in the multihead self-attention by a new Dimension-Reduced Self-Attention (DRSA) module, based on that the CBCT images have globally similar structure. A Patch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive local image information for fine-grained restoration. Experimental results on CBCT images with synthetic and real-world metal artifacts show that our MARformer is efficient and outperforms previous MAR methods and two restoration Transformers.
</details>
<details>
<summary>摘要</summary>
cone beam computed tomography (CBCT) 在 dental 诊断和手术中发挥关键作用，但是 metal  зуб钻Implant 可能会在 CBCT 图像处理过程中引入干扰性的 metal  artifacts，影响诊断和下游处理，如 Tooth 分 segmentation。在这篇论文中，我们开发了一种高效的 transformer 来实现 dental CBCT 图像中的 metal artifacts 减少 (MAR)。我们提出的 MAR transformer （MARformer）通过一种新的 Dimension-Reduced Self-Attention（DRSA）模块，基于 CBCT 图像的全球相似结构，来降低计算复杂性。此外，我们还提出了一种 Patch-wise Perceptive Feed Forward Network（P2FFN）来捕捉本地图像信息，进行细化修复。实验结果表明，我们的 MARformer 高效，并比前期 MAR 方法和两种修复 transformer 高效。
</details></li>
</ul>
<hr>
<h2 id="3D-Paintbrush-Local-Stylization-of-3D-Shapes-with-Cascaded-Score-Distillation"><a href="#3D-Paintbrush-Local-Stylization-of-3D-Shapes-with-Cascaded-Score-Distillation" class="headerlink" title="3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation"></a>3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09571">http://arxiv.org/abs/2311.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dale Decatur, Itai Lang, Kfir Aberman, Rana Hanocka</li>
<li>for: 本研究开发了一种自动填充 mesh 的本地 semantic 区域 texture 技术，通过文本描述进行操作。</li>
<li>methods: 本方法直接操作 mesh，生成可以与标准 графіcs 管线整合的 texture map。同时生成本地化MAP和 texture map，以实现它们之间的融合。使用多个阶层的填充模型来监督本地编辑技术，以提高细节和分辨率。</li>
<li>results: 本研究能够对不同类别的 shapes 进行本地 texture，并且可以控制 texture 的细节和全球理解。实验页面：<a target="_blank" rel="noopener" href="https://threedle.github.io/3d-paintbrush">https://threedle.github.io/3d-paintbrush</a><details>
<summary>Abstract</summary>
In this work we develop 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This synergistic approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture a variety of shapes within different semantic regions. Project page: https://threedle.github.io/3d-paintbrush
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们开发了3D涂刷技术，它可以通过文本描述自动地给mesh中的本地semantic区域添加文本ure。我们的方法直接操作mesh，生成的текстура映射可以衔接到标准图形管道中。我们同时生成了localization map（用于specify edit region）和它对应的текстура映射。这种相互作用使得本地化和 стилизация均得到了改善。为了提高文本区域的细节和分辨率，我们利用了多个阶段的叠加扩散模型来监督我们的本地编辑技术。我们称之为Cascaded Score Distillation（CSD），它同时在叠加的多个阶段中进行分辨率控制和全局理解的监督。我们示出了3D涂刷技术的效果，可以在不同的semantic region中地方Texture各种形状。项目页面：https://threedle.github.io/3d-paintbrush
</details></li>
</ul>
<hr>
<h2 id="Temporal-Aware-Refinement-for-Video-based-Human-Pose-and-Shape-Recovery"><a href="#Temporal-Aware-Refinement-for-Video-based-Human-Pose-and-Shape-Recovery" class="headerlink" title="Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery"></a>Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09543">http://arxiv.org/abs/2311.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Yan Zhou, Weihua Jian, Pengfei Wan, Zhongyuan Wang</li>
<li>for: 高精度和时间一致的人体动作重建从视频中</li>
<li>methods: 使用全球变换器编码器获取时间感知的全局特征序列，并使用卷积GRU网络生成高分辨率的本地特征图，以及一个循环更新模块来优化估计的SMPL参数，以实现高精度和平滑的结果</li>
<li>results: 比前一代方法更高精度的结果，在3DPW、MPI-INF-3DHP和Human3.6M等知名 benchmark 上都达到了更高的性能<details>
<summary>Abstract</summary>
Though significant progress in human pose and shape recovery from monocular RGB images has been made in recent years, obtaining 3D human motion with high accuracy and temporal consistency from videos remains challenging. Existing video-based methods tend to reconstruct human motion from global image features, which lack detailed representation capability and limit the reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining Network (TAR), to synchronously explore temporal-aware global and local image features for accurate pose and shape recovery. First, a global transformer encoder is introduced to obtain temporal global features from static feature sequences. Second, a bidirectional ConvGRU network takes the sequence of high-resolution feature maps as input, and outputs temporal local feature maps that maintain high resolution and capture the local motion of the human body. Finally, a recurrent refinement module iteratively updates estimated SMPL parameters by leveraging both global and local temporal information to achieve accurate and smooth results. Extensive experiments demonstrate that our TAR obtains more accurate results than previous state-of-the-art methods on popular benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
</details>
<details>
<summary>摘要</summary>
尽管在最近几年内，从单色RGB图像中提取人体姿态和形状的进步很大，但从视频中获取高精度和时间一致的人体运动仍然是一个挑战。现有的视频基于方法通常是从全图像特征中提取人体运动，这些特征缺乏细节表示能力，导致重建精度有限。在这篇论文中，我们提议一种名为时间感知修复网络（TAR），以同步探索时间感知的全局和局部图像特征，以达到高精度和平滑的人体姿态和形状重建。首先，我们引入全球变换Encoder，从静止特征序列中提取时间全局特征。其次，我们使用双向ConvGRU网络，将高分辨率特征图组作为输入，并输出时间局部特征图，以保持高分辨率和捕捉人体动作的局部运动。最后，我们引入循环更新模块，通过全球和局部时间信息来更新估计的SMPL参数，以实现高精度和平滑的结果。我们对 популяр的benchmark进行了广泛的实验，结果表明，我们的TAR方法比之前的状态 искусственный智能方法更高精度。
</details></li>
</ul>
<hr>
<h2 id="FedFusion-Manifold-Driven-Federated-Learning-for-Multi-satellite-and-Multi-modality-Fusion"><a href="#FedFusion-Manifold-Driven-Federated-Learning-for-Multi-satellite-and-Multi-modality-Fusion" class="headerlink" title="FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion"></a>FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09540">http://arxiv.org/abs/2311.09540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ldxdu/fedfusion">https://github.com/ldxdu/fedfusion</a></li>
<li>paper_authors: DaiXun Li, Weiying Xie, Yunsong Li, Leyuan Fang<br>for:多Modal remote sensing数据的合并是一项复杂的任务，因为它涉及到多种不同的感知特性和数据分布。methods:该文提出了一种基于拟合的多Modal数据合并框架，即FedFusion，它通过在每个客户端上随机选择地方数据来共同估计每个客户端的显著拟合结构，并将特征矩阵压缩到低维度空间中，作为后续分类器的输入。results:该文比较了现有方法和自己的方法在三个多Modal数据集上的性能，并达到了94.35%的平均分类精度，同时压缩通信成本四倍。此外，基于 Jetson TX2 工业模块的轨道边缘计算架构上进行了实际卫星图像的数字实验，显示FedFusion可以减少训练时间48.4分钟（15.18%），同时优化精度。<details>
<summary>Abstract</summary>
Multi-satellite, multi-modality in-orbit fusion is a challenging task as it explores the fusion representation of complex high-dimensional data under limited computational resources. Deep neural networks can reveal the underlying distribution of multi-modal remote sensing data, but the in-orbit fusion of multimodal data is more difficult because of the limitations of different sensor imaging characteristics, especially when the multimodal data follows non-independent identically distribution (Non-IID) distributions. To address this problem while maintaining classification performance, this paper proposes a manifold-driven multi-modality fusion framework, FedFusion, which randomly samples local data on each client to jointly estimate the prominent manifold structure of shallow features of each client and explicitly compresses the feature matrices into a low-rank subspace through cascading and additive approaches, which is used as the feature input of the subsequent classifier. Considering the physical space limitations of the satellite constellation, we developed a multimodal federated learning module designed specifically for manifold data in a deep latent space. This module achieves iterative updating of the sub-network parameters of each client through global weighted averaging, constructing a framework that can represent compact representations of each client. The proposed framework surpasses existing methods in terms of performance on three multimodal datasets, achieving a classification average accuracy of 94.35$\%$ while compressing communication costs by a factor of 4. Furthermore, extensive numerical evaluations of real-world satellite images were conducted on the orbiting edge computing architecture based on Jetson TX2 industrial modules, which demonstrated that FedFusion significantly reduced training time by 48.4 minutes (15.18%) while optimizing accuracy.}
</details>
<details>
<summary>摘要</summary>
多卫星、多Modalities在遥感空间融合是一个复杂的任务，因为它探索了复杂高维数据的融合表示，在限制的计算资源下进行。深度神经网络可以揭示多Modalities遥感数据的下面分布，但是多Modalities遥感数据的融合更加困难，因为不同的感器成像特点存在限制，特别是当多Modalities数据遵循非独立同分布（Non-IID）分布时。为了解决这个问题而保持分类性能，这篇论文提出了一个概率驱动的多Modalities融合框架，即FedFusion，它在每个客户端上随机选择本地数据，并且同时将每个客户端的浅层特征矩阵压缩到低维度空间中，并通过权重平均来更新每个客户端的子网络参数。针对卫星团 constellation 的物理空间限制，我们开发了特有的多Modalities联合学习模块，用于深层空间中的权重学习。这个模块通过迭代更新每个客户端的子网络参数，构建了一个可以表示每个客户端的紧凑表示框架。提出的框架在三个多Modalities数据集上表现出色，实现了分类准确率94.35%，同时压缩通信成本4倍。此外，基于 Jetson TX2 工业模块的遥感边缘计算架构进行了实际数据测试，并证明了 FedFusion 可以减少训练时间48.4分钟（15.18%），同时优化准确率。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-keypoints-RKHS-Learning-for-Self-supervised-6DoF-Pose-Estimation"><a href="#Pseudo-keypoints-RKHS-Learning-for-Self-supervised-6DoF-Pose-Estimation" class="headerlink" title="Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation"></a>Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09500">http://arxiv.org/abs/2311.09500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangzheng Wu, Michael Greenspan</li>
<li>for:  bridging the simulation-to-real domain gap in 6DoF PE</li>
<li>methods: 使用自然学习kernel在RKHS中，并提出了一种自我超vised keypoint radial voting-based 6DoF PE框架</li>
<li>results: 实现了state-of-the-art性能在三个常用的6DoF PE数据集上（LINEMOD (+4.2%), Occlusion LINEMOD (+2%), YCB-Video (+3%）），并与完全监督方法在所有六个BOP核心数据集上表现相当（ Within -10.8% to -0.3%）。<details>
<summary>Abstract</summary>
This paper addresses the simulation-to-real domain gap in 6DoF PE, and proposes a novel self-supervised keypoint radial voting-based 6DoF PE framework, effectively narrowing this gap using a learnable kernel in RKHS. We formulate this domain gap as a distance in high-dimensional feature space, distinct from previous iterative matching methods. We propose an adapter network, which evolves the network parameters from the source domain, which has been massively trained on synthetic data with synthetic poses, to the target domain, which is trained on real data. Importantly, the real data training only uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real groundtruth data annotations. RKHSPose achieves state-of-the-art performance on three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully supervised methods on all six applicable BOP core datasets, achieving within -10.8% to -0.3% of the top fully supervised results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Center-Focusing-Network-for-Real-Time-LiDAR-Panoptic-Segmentation"><a href="#Center-Focusing-Network-for-Real-Time-LiDAR-Panoptic-Segmentation" class="headerlink" title="Center Focusing Network for Real-Time LiDAR Panoptic Segmentation"></a>Center Focusing Network for Real-Time LiDAR Panoptic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09499">http://arxiv.org/abs/2311.09499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gangzhang842/cfnet">https://github.com/gangzhang842/cfnet</a></li>
<li>paper_authors: Xiaoyan Li, Gang Zhang, Boyue Wang, Yongli Hu, Baocai Yin</li>
<li>for: 实时LiDAR精准分割，提高自动驾驶车辆对周围环境和对象的全面理解。</li>
<li>methods: 提出了一种新的中心吸引网络（CFNet），包括中心吸引特征编码（CFFE）和快速中心排除模块（CDM），以提高精准和实时LiDAR精准分割。</li>
<li>results: 在SemanticKITTI和nuScenes精准分割benchmark上，CFNet比所有其他方法表现出较大的优势，并与最高效的方法相比，运行速度提高1.6倍。<details>
<summary>Abstract</summary>
LiDAR panoptic segmentation facilitates an autonomous vehicle to comprehensively understand the surrounding objects and scenes and is required to run in real time. The recent proposal-free methods accelerate the algorithm, but their effectiveness and efficiency are still limited owing to the difficulty of modeling non-existent instance centers and the costly center-based clustering modules. To achieve accurate and real-time LiDAR panoptic segmentation, a novel center focusing network (CFNet) is introduced. Specifically, the center focusing feature encoding (CFFE) is proposed to explicitly understand the relationships between the original LiDAR points and virtual instance centers by shifting the LiDAR points and filling in the center points. Moreover, to leverage the redundantly detected centers, a fast center deduplication module (CDM) is proposed to select only one center for each instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation benchmarks demonstrate that our CFNet outperforms all existing methods by a large margin and is 1.6 times faster than the most efficient method. The code is available at https://github.com/GangZhang842/CFNet.
</details>
<details>
<summary>摘要</summary>
利用LiDAR照片拼接的精炼分割可以帮助自动驾驶车辆全面理解周围环境和场景，并且需要在实时下运行。最近的提议方法可以加速算法，但其效果和效率仍然受到非存在实例中心的模型化和中心基于归一化模块的成本所限。为了实现准确和实时的LiDAR精炼分割，我们提出了一种新的中心集中网络（CFNet）。具体来说，我们提出了中心集中特征编码（CFFE），以明确原始LiDAR点和虚拟实例中心之间的关系，通过将LiDAR点Shift和填充中心点。此外，为了利用重复检测到的中心点，我们提出了快速中心筛选模块（CDM），以选择每个实例只有一个中心点。实验表明，我们的CFNet在SemanticKITTI和nuScenes精炼分割标准 benchmark上比所有其他方法差距较大，并且比最高效的方法快1.6倍。代码可以在https://github.com/GangZhang842/CFNet 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.CV_2023_11_16/" data-id="clp89dofx00nii788g2ba81tl" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.AI_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T12:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.AI_2023_11_16/">cs.AI - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Analysis-and-Extraction-of-Structure-from-Organizational-Charts"><a href="#The-Analysis-and-Extraction-of-Structure-from-Organizational-Charts" class="headerlink" title="The Analysis and Extraction of Structure from Organizational Charts"></a>The Analysis and Extraction of Structure from Organizational Charts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10234">http://arxiv.org/abs/2311.10234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Manali, David Doermann, Mahesh Desai</li>
<li>for: 提供一种自动化和端到端的方法，用于从组织图中提取信息，以解决手动提取信息的困难和时间消耗问题。</li>
<li>methods: 使用计算机视觉、深度学习和自然语言处理技术来自动提取组织图中的信息。</li>
<li>results: 提出一种用于评估提取信息的完整性和层次准确性的度量，并通过实验证明该方法的有效性。<details>
<summary>Abstract</summary>
Organizational charts, also known as org charts, are critical representations of an organization's structure and the hierarchical relationships between its components and positions. However, manually extracting information from org charts can be error-prone and time-consuming. To solve this, we present an automated and end-to-end approach that uses computer vision, deep learning, and natural language processing techniques. Additionally, we propose a metric to evaluate the completeness and hierarchical accuracy of the extracted information. This approach has the potential to improve organizational restructuring and resource utilization by providing a clear and concise representation of the organizational structure. Our study lays a foundation for further research on the topic of hierarchical chart analysis.
</details>
<details>
<summary>摘要</summary>
organizational charts, also known as org charts, are critical representations of an organization's structure and the hierarchical relationships between its components and positions. however, manually extracting information from org charts can be error-prone and time-consuming. to solve this, we present an automated and end-to-end approach that uses computer vision, deep learning, and natural language processing techniques. additionally, we propose a metric to evaluate the completeness and hierarchical accuracy of the extracted information. this approach has the potential to improve organizational restructuring and resource utilization by providing a clear and concise representation of the organizational structure. our study lays a foundation for further research on the topic of hierarchical chart analysis.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Model-of-Hurricane-Evacuation-Behaviors"><a href="#A-Graphical-Model-of-Hurricane-Evacuation-Behaviors" class="headerlink" title="A Graphical Model of Hurricane Evacuation Behaviors"></a>A Graphical Model of Hurricane Evacuation Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10228">http://arxiv.org/abs/2311.10228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sophie Wang, Nutchanon Yongsatianchot, Stacy Marsella</li>
<li>for: 这 paper 的目的是研究人们在风暴来临时是否离开家园的决策，以及这些决策如何影响紧急准备和应急响应。</li>
<li>methods: 这 paper 使用了 Protection motivation theory (PMT) 框架，构建了风暴离开决策的复杂关系图，并通过 conditional independence tests 评估不同的图 Structures。</li>
<li>results: 研究发现，人们的风暴离开决策受到了威胁评估（threat appraisal）和应急 coping 评估的直接影响，以及媒体信息的直接和间接影响。 certain information received from media 影响了威胁评估，并通过它影响了风暴离开行为。此外，一些变量直接影响了风暴离开行为和威胁评估，包括家人和朋友的建议，邻居的离开行为，以及官员发布的离开通知。<details>
<summary>Abstract</summary>
Natural disasters such as hurricanes are increasing and causing widespread devastation. People's decisions and actions regarding whether to evacuate or not are critical and have a large impact on emergency planning and response. Our interest lies in computationally modeling complex relationships among various factors influencing evacuation decisions. We conducted a study on the evacuation of Hurricane Irma of the 2017 Atlantic hurricane season. The study was guided by the Protection motivation theory (PMT), a widely-used framework to understand people's responses to potential threats. Graphical models were constructed to represent the complex relationships among the factors involved and the evacuation decision. We evaluated different graphical structures based on conditional independence tests using Irma data. The final model largely aligns with PMT. It shows that both risk perception (threat appraisal) and difficulties in evacuation (coping appraisal) influence evacuation decisions directly and independently. Certain information received from media was found to influence risk perception, and through it influence evacuation behaviors indirectly. In addition, several variables were found to influence both risk perception and evacuation behaviors directly, including family and friends' suggestions, neighbors' evacuation behaviors, and evacuation notices from officials.
</details>
<details>
<summary>摘要</summary>
自然灾害如飓风减少不断，引起广泛的破坏。人们的逃离或不逃离的决定对紧急准备和应急应对有着重要的影响。我们的兴趣在于通过计算模型来模拟人们逃离决定的复杂关系。我们在2017年大西洋飓风赛季的飓风艾尔马事例进行了研究。研究受保护动机理论（PMT）的导向，这是解释人们面临可能威胁的响应的广泛使用的框架。我们使用图表模型来表示逃离决定中的复杂关系，并对逃离决定进行了不同的图表结构的评估。我们根据飓风艾尔马数据进行了条件独立测试，最终模型大致与PMT相符。它表明，风险识别（威胁评估）和逃离困难（处理评估）都直接和独立地影响逃离决定。媒体上接受的信息也影响了风险识别，并通过它影响了逃离行为。此外，一些变量直接和 indirectly影响了逃离决定，包括家庭和朋友的建议、邻居的逃离行为以及官员发布的逃离通知。
</details></li>
</ul>
<hr>
<h2 id="Think-Twice-Perspective-Taking-Improves-Large-Language-Models’-Theory-of-Mind-Capabilities"><a href="#Think-Twice-Perspective-Taking-Improves-Large-Language-Models’-Theory-of-Mind-Capabilities" class="headerlink" title="Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities"></a>Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10227">http://arxiv.org/abs/2311.10227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency</li>
<li>for: 本研究旨在提高现有的大自然语言模型（LLM） Theory of Mind（ToM）能力。</li>
<li>methods: 本研究提出了一种新的两阶段引导框架，名为SimToM，它基于认知科学理论“模拟理论”中的视角变换。</li>
<li>results: 对当前 ToM benchmark 进行应用，SimToM 方法显示了明显的改善，而且我们的分析表明了对理论知识的了解对 ToM 能力的重要性。<details>
<summary>Abstract</summary>
Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory "Simulation Theory" to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.
</details>
<details>
<summary>摘要</summary>
人类互动深受理智思维、信念和愿望的互动，这些思维是通过理智思维（ToM）实现的：我们的认知能力理解自己和他人的心理状态。虽然ToM可能是自然的，但模拟它对even最高级语言模型（LLMs）来说是一项挑战。现有的LLMs的理解能力的改进从简单而有效的提示技术such as Chain-of-Thought中有限的应用于ToM。在这篇论文中，我们转向了著名的认知科学理论“模拟理论”来bridging这个差距。我们提出了一种新的两个阶段的提示框架，称为SimToM，它是基于模拟理论中的看法拟合的想法。为了在当前的ToM标准benchmark上实现这个想法，SimToM首先根据character知道的信息过滤上下文，然后回答关于其心理状态的问题。我们的方法不需要额外的训练和微小的提示调整，而且与现有的方法比较，我们的结果表明了看法拟合的重要性，以及它在理智思维能力方面的潜在性。我们的发现建议将 perspective-taking作为未来研究理智思维能力的可能方向。
</details></li>
</ul>
<hr>
<h2 id="A-Language-and-Its-Dimensions-Intrinsic-Dimensions-of-Language-Fractal-Structures"><a href="#A-Language-and-Its-Dimensions-Intrinsic-Dimensions-of-Language-Fractal-Structures" class="headerlink" title="A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures"></a>A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10217">http://arxiv.org/abs/2311.10217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii A. Gromov, Nikita S. Borodin, Asel S. Yerbolova</li>
<li>for: The paper is written to introduce a new object of study - a language fractal structure, and to estimate the intrinsic dimensions of language fractal structures for the Russian and English languages.</li>
<li>methods: The paper uses methods based on topological data analysis and a minimum spanning tree of a data graph to estimate the intrinsic dimensions of language fractal structures.</li>
<li>results: The paper finds that the intrinsic dimensions of language fractal structures for both the Russian and English languages are non-integer values, close to 9 for both languages.Here is the information in Simplified Chinese text, as requested:</li>
<li>for: 本研究对自然语言中的语言异步结构进行了新的研究，并估计了俄语和英语语言异步结构的内在维度。</li>
<li>methods: 本研究使用了基于拓扑数据分析和数据图中最小杆的方法来估计语言异步结构的内在维度。</li>
<li>results: 研究发现，俄语和英语语言异步结构的内在维度都是非整数值，都接近9。<details>
<summary>Abstract</summary>
The present paper introduces a novel object of study - a language fractal structure. We hypothesize that a set of embeddings of all $n$-grams of a natural language constitutes a representative sample of this fractal set. (We use the term Hailonakea to refer to the sum total of all language fractal structures, over all $n$). The paper estimates intrinsic (genuine) dimensions of language fractal structures for the Russian and English languages. To this end, we employ methods based on (1) topological data analysis and (2) a minimum spanning tree of a data graph for a cloud of points considered (Steele theorem). For both languages, for all $n$, the intrinsic dimensions appear to be non-integer values (typical for fractal sets), close to 9 for both of the Russian and English language.
</details>
<details>
<summary>摘要</summary>
本文介绍一种新的研究对象——语言自similarity结构。我们假设所有自然语言中的ngrams集合可以视为这种自similarity结构的代表样本。（我们使用“Hailonakea”这个 термин来描述所有语言自similarity结构的总和，随着n的变化）。本文对俄语和英语两种语言的语言自similarity结构进行了估计。为此，我们使用了基于拓扑数据分析和最小杆法（Steele theorem）的方法。对于两种语言和所有n，内部维度都显示为非整数值（典型的自similarity集合特征），约等于9。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Minds-LLMs-As-Atypical-Active-Inference-Agents"><a href="#Predictive-Minds-LLMs-As-Atypical-Active-Inference-Agents" class="headerlink" title="Predictive Minds: LLMs As Atypical Active Inference Agents"></a>Predictive Minds: LLMs As Atypical Active Inference Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10215">http://arxiv.org/abs/2311.10215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Kulveit, Clem von Stengel, Roman Leventov</li>
<li>for: 本文探讨大语言模型（LLM）如何理解和使用active inference理论。</li>
<li>methods: 文章比较传统的active inference系统和LLM的相似之处和差异，并结论LLM目前缺乏与行动在世界中产生影响的紧密反馈循环，但其他地方符合active inference模式。</li>
<li>results: 文章列出了可能关闭这个循环的原因，以及这可能导致模型自我意识和减少预测错误的变化。<details>
<summary>Abstract</summary>
Large language models (LLMs) like GPT are often conceptualized as passive predictors, simulators, or even stochastic parrots. We instead conceptualize LLMs by drawing on the theory of active inference originating in cognitive science and neuroscience. We examine similarities and differences between traditional active inference systems and LLMs, leading to the conclusion that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impacts of their actions, but otherwise fit in the active inference paradigm. We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT通常被概念化为无动的预测器、模拟器或甚至是随机的喊喊鸟。我们则通过从认知科学和神经科学中的活跃推理理论来概念化LLM。我们比较了传统的活跃推理系统和LLM之间的相似之处和不同之处，结论是，目前LLM缺乏在世界中行动并观察自己的影响的紧密回路，但以其他方面符合活跃推理概念。我们列出了关闭这个回路的原因，以及这可能会带来的影响，包括增强模型自我意识和驱动降低预测错误的改变世界的驱动。
</details></li>
</ul>
<hr>
<h2 id="Bayes-in-the-age-of-intelligent-machines"><a href="#Bayes-in-the-age-of-intelligent-machines" class="headerlink" title="Bayes in the age of intelligent machines"></a>Bayes in the age of intelligent machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10206">http://arxiv.org/abs/2311.10206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas L. Griffiths, Jian-Qiao Zhu, Erin Grant, R. Thomas McCoy</li>
<li>for: 本研究旨在探讨人工神经网络如何影响人类认知解释，并 argue that Bayesian模型和人工神经网络是 complementary modeling approach，可以用来理解人类认知和智能机器的行为。</li>
<li>methods: 本研究使用了人工神经网络和 Bayesian模型来解释人类认知和智能机器的行为。</li>
<li>results: 研究发现，Bayesian模型和人工神经网络是不同的层次分析方法，可以共同理解人类认知和智能机器的行为，并且 Bayesian模型在解释大型、透明度低的人工神经网络行为方面可能具有独特的价值。<details>
<summary>Abstract</summary>
The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.
</details>
<details>
<summary>摘要</summary>
人类认知的解释可能会受到基于人工神经网络的方法的成功威胁。我们认为这并不是如此，我们认为这些系统实际上提供了新的机会来模型人类认知。具体来说，我们认为认知科学的概率模型和人工神经网络模型在不同的水平上进行模型化，这些模型之间存在衔接，可以用来理解人类认知的各个水平。此外，我们还认为概率模型在理解大型、不透明的人工神经网络的行为方面可能具有独特的价值，这些网络通常是基于专有数据进行训练的。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Robustness-Against-Common-Corruptions-using-Mixture-of-Class-Specific-Experts"><a href="#Towards-Improving-Robustness-Against-Common-Corruptions-using-Mixture-of-Class-Specific-Experts" class="headerlink" title="Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts"></a>Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10177">http://arxiv.org/abs/2311.10177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan, Danilo Vasconcellos Vargas</li>
<li>for: 这篇论文的目的是提高神经网络的适用范围和性能，以应对不断变化的实际世界情况。</li>
<li>methods: 这篇论文提出了一种名为“混合类别专家架构”的新方法，它通过专门为每个类别训练独立的神经网络段，并将其输出组合以提高神经网络的扩展性和绩效。</li>
<li>results: 研究发现，这种新方法可以提高神经网络的适用范围和性能，并在不同的测试 benchmark 上表现出色。特别是在面对未知的扭曲和折衣时，这种方法可以提供更高的适用范围和稳定性。<details>
<summary>Abstract</summary>
Neural networks have demonstrated significant accuracy across various domains, yet their vulnerability to subtle input alterations remains a persistent challenge. Conventional methods like data augmentation, while effective to some extent, fall short in addressing unforeseen corruptions, limiting the adaptability of neural networks in real-world scenarios. In response, this paper introduces a novel paradigm known as the Mixture of Class-Specific Expert Architecture. The approach involves disentangling feature learning for individual classes, offering a nuanced enhancement in scalability and overall performance. By training dedicated network segments for each class and subsequently aggregating their outputs, the proposed architecture aims to mitigate vulnerabilities associated with common neural network structures. The study underscores the importance of comprehensive evaluation methodologies, advocating for the incorporation of benchmarks like the common corruptions benchmark. This inclusion provides nuanced insights into the vulnerabilities of neural networks, especially concerning their generalization capabilities and robustness to unforeseen distortions. The research aligns with the broader objective of advancing the development of highly robust learning systems capable of nuanced reasoning across diverse and challenging real-world scenarios. Through this contribution, the paper aims to foster a deeper understanding of neural network limitations and proposes a practical approach to enhance their resilience in the face of evolving and unpredictable conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="JaxMARL-Multi-Agent-RL-Environments-in-JAX"><a href="#JaxMARL-Multi-Agent-RL-Environments-in-JAX" class="headerlink" title="JaxMARL: Multi-Agent RL Environments in JAX"></a>JaxMARL: Multi-Agent RL Environments in JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10090">http://arxiv.org/abs/2311.10090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flairox/jaxmarl">https://github.com/flairox/jaxmarl</a></li>
<li>paper_authors: Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi Bandyopadhyay, Mikayel Samvelyan, Minqi Jiang, Robert Tjarko Lange, Shimon Whiteson, Bruno Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, Jakob Nicolaus Foerster</li>
<li>for: This paper is written for researchers and developers in the field of reinforcement learning (RL) and multi-agent reinforcement learning (MARL), who need efficient and scalable environments for training and evaluating their algorithms.</li>
<li>methods: The paper uses JAX (Jax.org) to enable massively parallel RL training pipelines and environments, and presents JaxMARL, an open-source code base that combines ease-of-use with GPU-enabled efficiency for commonly used MARL environments and popular baseline algorithms.</li>
<li>results: The paper shows that JaxMARL is up to 12,500 times faster than existing approaches in terms of wall clock time, enabling efficient and thorough evaluations, and introduces SMAX, a vectorized and simplified version of the StarCraft Multi-Agent Challenge that enables GPU acceleration and provides a more flexible MARL environment.<details>
<summary>Abstract</summary>
Benchmarks play an important role in the development of machine learning algorithms. For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.
</details>
<details>
<summary>摘要</summary>
��benchmark��是机器学习算法开发中非常重要的一部分。例如，在强化学习（RL）方面，可用的环境和benchmark��对研究产生了深远的影响。然而，RL环境通常在CPU上运行，这限制了学术计算的扩展性。现在，JAX技术的发展使得可以通过硬件加速来超越这些计算障碍，实现了大规模并行的RL训练管道和环境。这特别有用于多智能体强化学习（MARL）研究。首先，在每个环境步骤中，需要考虑多个智能体，这添加了计算压力；其次，样本复杂性增加由非站立性、分布式部分可见性或其他MARL挑战。在这篇论文中，我们提供了JaxMARL，首个开源代码库，搭配易用性和GPU启用效率，支持大量常用的MARL环境以及流行的基线算法。在考虑wall clock时间的情况下，我们的JAX基本训练管道相比现有方法，每次训练的时间提高了12500倍。这使得有效和详细的评估变得可能，有potential解决机器学习领域的评估危机。我们还引入了SMAX，一种简化版的StarCraft Multi-Agent Challenge， removes the need to run the StarCraft II game engine。这不仅使得GPU加速可能，还提供了更 flexible MARL环境，解锁了自适应、元学习和未来MARL应用的潜力。我们提供了代码，可以在https://github.com/flairox/jaxmarl上获取。
</details></li>
</ul>
<hr>
<h2 id="Emu-Edit-Precise-Image-Editing-via-Recognition-and-Generation-Tasks"><a href="#Emu-Edit-Precise-Image-Editing-via-Recognition-and-Generation-Tasks" class="headerlink" title="Emu Edit: Precise Image Editing via Recognition and Generation Tasks"></a>Emu Edit: Precise Image Editing via Recognition and Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10089">http://arxiv.org/abs/2311.10089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman</li>
<li>For: 这个论文旨在提出一种多任务图像编辑模型，以便通过自然语言指令执行图像编辑操作。* Methods: 该模型通过多任务学习和学习任务嵌入来提高图像编辑能力，并且可以通过几个例子学习新任务。* Results: 该模型在多个图像编辑任务中达到了领先水平，包括区域基本编辑、自由形编辑和计算机视觉任务。此外，该模型还可以执行新的图像编辑任务，只需几个标注的样本即可。<details>
<summary>Abstract</summary>
Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks.
</details>
<details>
<summary>摘要</summary>
<SYS>请求已处理，以下是文本的简化中文版本：</SYS> instruciton-based image editing 拥有巨大的潜力，可以用自然语言指令进行任何编辑操作。然而，当前在这个领域的模型经常会对用户的指令进行不准确的执行。我们介绍了 Emu Edit，一个多任务图像编辑模型，它在 instruciton-based image editing 中设置了新的STATE-OF-THE-ART 成绩。为了开发 Emu Edit，我们训练它来执行多种任务，例如区域编辑、自由形编辑和计算机视觉任务，这些任务都是生成任务。此外，为了增强 Emu Edit 的多任务学习能力，我们为它提供了学习任务嵌入，这些嵌入引导生成过程向正确的编辑类型。这两个元素都是 Emu Edit 的出色表现的关键。此外，我们表明 Emu Edit 可以通过几个标注的例子学习新任务，如图像填充、超解像和编辑任务的组合。这种能力在情况中缺乏高质量样本时具有重要优势。最后，为了促进 instruciton-based image editing 模型的更加严格和有知识的评估，我们发布了一个新的复杂和多样的benchmark，包括七种不同的图像编辑任务。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Generation-of-Graphical-Game-Assets-A-Conceptual-Framework-and-Systematic-Review-of-the-State-of-the-Art"><a href="#Intelligent-Generation-of-Graphical-Game-Assets-A-Conceptual-Framework-and-Systematic-Review-of-the-State-of-the-Art" class="headerlink" title="Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art"></a>Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10129">http://arxiv.org/abs/2311.10129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaisei Fukaya, Damon Daylamani-Zad, Harry Agius</li>
<li>for: 这篇论文的目的是对游戏中的图形资产生成进行系统性的文献综述，以便为感兴趣的人提供可能的方法和方法，并帮助他们了解和应用这些方法。</li>
<li>methods: 这篇论文使用了系统性的文献综述方法，检索了200篇有关图形资产生成的论文，从而掌握了现状的方法和技术。</li>
<li>results: 该论文对现有的图形资产生成方法进行了概括和分析，并基于文献的研究，提出了一种概念框架，以帮助感兴趣的人了解和应用图形资产生成的方法。<details>
<summary>Abstract</summary>
Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels and sounds, to trees and weapons. A large amount of game content is comprised of graphical assets, such as clouds, buildings or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs, and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 200 accepted papers. This paper explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_orientation: horizontal Процедурное содержание генерации (PCG) может быть применено к широкому спектру задач в играх, от сюжетов, уровней и звуков, до деревьев и оружия. Огромное количество содержимого игры состоит из графических ассетов, таких как облака, здания или растительность, которые не требуют рассмотрения функций игры. Также существует широкий спектр литературы, который изучает процедурное генерация таких элементов для целей вне игр. Тело исследований, сосредоточенное на конкретных методах для генерации конкретных активов, ограничивает видимость доступных возможных подходов и подходов. Поэтому трудно получить ясный обзор всех подходов и возможностей, а также нет инструментов для руководства заинтересованными сторонами в возможных методах и подходах для их потребностей. Поэтому была проведена систематическая рецензия литературы, которая дала 200 принятых статей. Эта статья исследовает современные подходы к генерации графических активов, изучая исследования из широкого спектра приложений, как внутри, так и вне игр. Обоснованная литературой, была получена концептуальная рамка, чтобы устранить перечисленные пробелы.
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-3-5-ChatGPT-4-Google-Bard-and-Microsoft-Bing-to-Improve-Health-Literacy-and-Communication-in-Pediatric-Populations-and-Beyond"><a href="#ChatGPT-3-5-ChatGPT-4-Google-Bard-and-Microsoft-Bing-to-Improve-Health-Literacy-and-Communication-in-Pediatric-Populations-and-Beyond" class="headerlink" title="ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond"></a>ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10075">http://arxiv.org/abs/2311.10075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanhai S. Amin, Linda Mayes, Pavan Khosla, Rushabh Doshi</li>
<li>For: The paper aims to investigate whether large language models (LLMs) can improve health literacy in children and other populations.* Methods: The authors used 26 different prompts to test the ability of three LLMs (ChatGPT-3.5, Microsoft Bing, and Google Bard) to provide health information at different reading grade levels (RGL). They evaluated the responses based on their reading grade level and word count.* Results: The results show that all three LLMs were able to provide responses at or above a 10th-grade RGL. However, ChatGPT-3.5 and ChatGPT-4 were better at providing responses at lower grade levels, while Microsoft Bing and Google Bard tended to produce responses at a consistent high school level. The authors also found that Bard was more cautious in providing certain outputs, which may indicate a need for further research on the accuracy and effectiveness of LLMs in health communication.<details>
<summary>Abstract</summary>
Purpose: Enhanced health literacy has been linked to better health outcomes; however, few interventions have been studied. We investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.   Methods: We ran 288 conditions using 26 different prompts through ChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate limits, we tested a subset of 150 conditions through ChatGPT-4. The primary outcome measurements were the reading grade level (RGL) and word counts of output.   Results: Across all models, output for basic prompts such as "Explain" and "What is (are)" were at, or exceeded, a 10th-grade RGL. When prompts were specified to explain conditions from the 1st to 12th RGL, we found that LLMs had varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided responses that ranged from the 7th-grade to college freshmen RGL while ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL. Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard provided responses from the 7th to 10th RGL.   Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade level outputs. Meanwhile Bard and Bing tended to consistently produce an RGL that is at the high school level regardless of prompt. Additionally, Bard's hesitancy in providing certain outputs indicates a cautious approach towards health information. LLMs demonstrate promise in enhancing health communication, but future research should verify the accuracy and effectiveness of such tools in this context.   Implications: LLMs face challenges in crafting outputs below a sixth-grade reading level. However, their capability to modify outputs above this threshold provides a potential mechanism to improve health literacy and communication in a pediatric population and beyond.
</details>
<details>
<summary>摘要</summary>
目的：增强健康文化知识与健康结果之间的关系，但只有少数临床实践被研究。我们 investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.  方法：我们运行了288个条件，使用26个提示，通过ChatGPT-3.5、Microsoft Bing和Google Bard进行测试。由于环境限制，我们只测试了150个条件。主要输出测量方法包括阅读水平（RGL）和单词计数。  结果：所有模型的输出基本提问（如“解释”和“是什么”）的RGL都达到或超过了高中水平。当提示是指定为1-12年级的条件时，我们发现LLMs有不同的能力来适应不同的学龄水平。ChatGPT-3.5提供了7-12年级的回答，而ChatGPT-4则提供了6-12年级的回答。Microsoft Bing提供了9-11年级的回答，而Google Bard则提供了7-10年级的回答。  讨论：ChatGPT-3.5和ChatGPT-4在实现更低学龄水平的输出方面表现更好。与之相比，Bing和Bard倾向于一直提供高中水平的回答，无论提示是什么。此外，Bard的某些输出表现出了谨慎的态度，这可能是一种对健康信息的谨慎方式。LLMs在健康沟通方面表现了搭配性，但未来的研究应该验证这些工具在这种上下文中的准确性和效果。  意义：LLMs面临低于6年级阅读水平的输出创作的挑战。然而，它们可以修改输出以上这个阈值提供一个可能的机制来提高健康文化知识和沟通。
</details></li>
</ul>
<hr>
<h2 id="The-Song-Describer-Dataset-a-Corpus-of-Audio-Captions-for-Music-and-Language-Evaluation"><a href="#The-Song-Describer-Dataset-a-Corpus-of-Audio-Captions-for-Music-and-Language-Evaluation" class="headerlink" title="The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation"></a>The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10057">http://arxiv.org/abs/2311.10057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mulab-mir/song-describer-dataset">https://github.com/mulab-mir/song-describer-dataset</a></li>
<li>paper_authors: Ilaria Manco, Benno Weck, SeungHeon Doh, Minz Won, Yixiao Zhang, Dmitry Bodganov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, György Fazekas, Juhan Nam</li>
<li>for: 评估音乐和语言模型的评估 dataset，提供高质量的音频-caption对。</li>
<li>methods: 使用人工写好的自然语言描述，对706首乐曲进行了评估。</li>
<li>results: 通过三种音乐和语言任务的测试（乐曲描述、文本到乐曲生成和乐曲语言检索），研究人员可以通过 SDD 来更好地了解模型性能。<details>
<summary>Abstract</summary>
We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of high-quality audio-caption pairs, designed for the evaluation of music-and-language models. The dataset consists of 1.1k human-written natural language descriptions of 706 music recordings, all publicly accessible and released under Creative Common licenses. To showcase the use of our dataset, we benchmark popular models on three key music-and-language tasks (music captioning, text-to-music generation and music-language retrieval). Our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use SDD to gain a broader understanding of model performance.
</details>
<details>
<summary>摘要</summary>
我们介绍歌曲描述数据集（SDD），一个新的人工抽样的音频-caption对数据集，适用于评估音乐和语言模型。该数据集包含1.1k名人写的自然语言描述，描述了706首音乐录音，所有公共可访问，发布于创意共享许可证下。为了展示我们的数据集的使用，我们在三个关键的音乐和语言任务（音频描述、文本到音乐生成和音乐语言检索）中对流行的模型进行了测试。我们的实验表明了跨数据集评估的重要性，并提供了如何使用 SDD 来深入了解模型性能的细节。
</details></li>
</ul>
<hr>
<h2 id="Is-“A-Helpful-Assistant”-the-Best-Role-for-Large-Language-Models-A-Systematic-Evaluation-of-Social-Roles-in-System-Prompts"><a href="#Is-“A-Helpful-Assistant”-the-Best-Role-for-Large-Language-Models-A-Systematic-Evaluation-of-Social-Roles-in-System-Prompts" class="headerlink" title="Is “A Helpful Assistant” the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts"></a>Is “A Helpful Assistant” the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10054">http://arxiv.org/abs/2311.10054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqian Zheng, Jiaxin Pei, David Jurgens</li>
<li>for: 这个研究旨在评估社交角色在系统提示中对模型性能的影响，以帮助设计更好的AI系统提示。</li>
<li>methods: 该研究使用了3个流行的大语言模型和2457道问题进行了广泛的分析，并制定了162个社交角色，包括6种人际关系和8种职业。</li>
<li>results: 研究发现，在提示中添加社交角色可以一致提高模型的性能，并且使用gender-neutral角色和指定受众角色可以更好地提高模型的性能。然而，预测哪一个角色会导致最佳性能仍然是一个挑战，并且频率、相似度和混淆率不能完全解释社交角色对模型性能的影响。<details>
<summary>Abstract</summary>
Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses "You are a helpful assistant" as part of the default system prompt. But is "a helpful assistant" the best role for LLMs? In this study, we present a systematic evaluation of how social roles in system prompts affect model performance. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 types of occupations. Through extensive analysis of 3 popular LLMs and 2457 questions, we show that adding interpersonal roles in prompts consistently improves the models' performance over a range of questions. Moreover, while we find that using gender-neutral roles and specifying the role as the audience leads to better performances, predicting which role leads to the best performance remains a challenging task, and that frequency, similarity, and perplexity do not fully explain the effect of social roles on model performances. Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）与人类之间的互动主要是通过提示来实现。商业人工智能系统通常将 LLM 的角色定义为系统提示中的一部分。例如，ChatGPT 使用 "你是一位有用的助手" 作为默认系统提示。但是 "有用的助手" 是 LLM 的最佳角色吗？在这项研究中，我们提供了一种系统atic评估如何社交角色在系统提示中影响模型性能。我们筛选了 162 个角色，涵盖了6种人际关系和8种职业。通过对 3 个流行 LLM 和 2457 个问题进行广泛分析，我们发现，在提示中添加社交角色可以逐渐提高模型的性能范围内的问题。此外，我们发现使用 gender-neutral 角色和指定角色为受众可以提高模型的性能，但是预测哪一个角色会导致最佳性能是一项困难的任务，并且频率、相似度和混淆率不能完全解释社交角色对模型性能的影响。我们的结果可以帮助设计 AI 系统的提示。代码和数据可以在 https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles 上获取。
</details></li>
</ul>
<hr>
<h2 id="Inherently-Interpretable-Time-Series-Classification-via-Multiple-Instance-Learning"><a href="#Inherently-Interpretable-Time-Series-Classification-via-Multiple-Instance-Learning" class="headerlink" title="Inherently Interpretable Time Series Classification via Multiple Instance Learning"></a>Inherently Interpretable Time Series Classification via Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10049">http://arxiv.org/abs/2311.10049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaearly/miltimeseriesclassification">https://github.com/jaearly/miltimeseriesclassification</a></li>
<li>paper_authors: Joseph Early, Gavin KC Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola, Niall Twomey</li>
<li>for: 本研究旨在提高时间序列分类器的可解释性，使其决策过程更加直观和可理解。</li>
<li>methods: 本研究使用多例学习（MIL）技术，提出了一种名为MILLET（多例学习为时间序列分类提供了本地可解释性）的新框架。该框架可以应用于现有的深度学习时间序列分类模型，使其自然地具有可解释性，而不会产生性能下降。</li>
<li>results: 在85个UCR时间序列分类 dataset上测试了MILLET，并证明了它可以生成高质量的解释，比其他已知的可解释方法更好。此外，我们还提供了一个专门为可解释性评估设计的 synthetic dataset。<details>
<summary>Abstract</summary>
Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains
</details>
<details>
<summary>摘要</summary>
We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specifically designed to facilitate interpretability evaluation. Our results show that MILLET produces high-quality explanations quickly, which are superior to other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to a wide range of domains.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Neural-Network-Based-Federated-Learning-System-for-Imbalanced-and-Non-IID-Data"><a href="#A-Novel-Neural-Network-Based-Federated-Learning-System-for-Imbalanced-and-Non-IID-Data" class="headerlink" title="A Novel Neural Network-Based Federated Learning System for Imbalanced and Non-IID Data"></a>A Novel Neural Network-Based Federated Learning System for Imbalanced and Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10025">http://arxiv.org/abs/2311.10025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahfuzur Rahman Chowdhury, Muhammad Ibrahim</li>
<li>for: 本研究旨在提高 Federated Learning 中数据隐私的保护，并提出一种中央化的 neural network-based Federated Learning 系统，以提高 Accuracy 和 Efficiency。</li>
<li>methods: 本研究使用了 Micro-level 并行处理，启发自传统的 mini-batch 算法， Client 设备和服务器分别处理前向和反向传播。此外，我们还提出了一种 Edge Computing 版本的我们的提posed algorithm，在减少中央服务器负担的情况下， Client 处理前向和反向传播。</li>
<li>results: 我们在五个Well-known Benchmark dataset上进行了评估，并在不同数据分布Setting下获得了满意的性能，与一些现有的标准algorithm相比，我们的提posed system在一定程度上减少了训练时间。<details>
<summary>Abstract</summary>
With the growth of machine learning techniques, privacy of data of users has become a major concern. Most of the machine learning algorithms rely heavily on large amount of data which may be collected from various sources. Collecting these data yet maintaining privacy policies has become one of the most challenging tasks for the researchers. To combat this issue, researchers have introduced federated learning, where a prediction model is learnt by ensuring the privacy of data of clients data. However, the prevalent federated learning algorithms possess an accuracy and efficiency trade-off, especially for non-IID data. In this research, we propose a centralized, neural network-based federated learning system. The centralized algorithm incorporates micro-level parallel processing inspired by the traditional mini-batch algorithm where the client devices and the server handle the forward and backward propagation respectively. We also devise a semi-centralized version of our proposed algorithm. This algorithm takes advantage of edge computing for minimizing the load from the central server, where clients handle both the forward and backward propagation while sacrificing the overall train time to some extent. We evaluate our proposed systems on five well-known benchmark datasets and achieve satisfactory performance in a reasonable time across various data distribution settings as compared to some existing benchmark algorithms.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的发展，用户数据隐私的问题已成为一个重要的挑战。大多数机器学习算法需要大量数据，这些数据可能来自于多个来源。收集这些数据并保持隐私政策已成为研究人员最大的挑战。为解决这个问题，研究人员已经引入联邦学习，这种方法可以保证客户端数据的隐私。然而，现有的联邦学习算法具有精度和效率的负担假设，特别是非相关数据。在这些研究中，我们提议了一种中央化的神经网络基于联邦学习系统。中央算法包括微级并行处理，这种方法由客户端设备和服务器处理前向和反向传播。我们还开发了一种半中央化版本的我们的提议算法。这种算法利用边计算来减轻中央服务器的负担，客户端处理了前向和反向传播，但是在一定程度上减少了总训练时间。我们在五个常见的benchmark数据集上评估了我们的提议系统，并在不同的数据分布设置下实现了满意的性能，与一些现有的benchmark算法相比。
</details></li>
</ul>
<hr>
<h2 id="Learning-interactions-to-boost-human-creativity-with-bandits-and-GPT-4"><a href="#Learning-interactions-to-boost-human-creativity-with-bandits-and-GPT-4" class="headerlink" title="Learning interactions to boost human creativity with bandits and GPT-4"></a>Learning interactions to boost human creativity with bandits and GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10127">http://arxiv.org/abs/2311.10127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ara Vartanian, Xiaoxi Sun, Yun-Shiuan Chuang, Siddharth Suresh, Xiaojin Zhu, Timothy T. Rogers</li>
<li>for: 这个论文探讨了人与AI算法之间的互动如何提高人类创造力。</li>
<li>methods: 研究人员使用了一种心理任务来检验人类创造力的限制，即Semantic feature generation：给一个概念名称后，参与者需要列出该概念的所有特征。</li>
<li>results: 研究发现，人类参与者和语言AI（GPT-4）在标准任务和一种提供算法生成提示的变体任务中的行为相似，而且 bandaits学习自AI响应 preferences 与人类行为学习的提示策略相同。结果表明，通过计算机互动，可以使bandits learn from simulated participants的行为，以提高人类创造力。<details>
<summary>Abstract</summary>
This paper considers how interactions with AI algorithms can boost human creative thought. We employ a psychological task that demonstrates limits on human creativity, namely semantic feature generation: given a concept name, respondents must list as many of its features as possible. Human participants typically produce only a fraction of the features they know before getting "stuck." In experiments with humans and with a language AI (GPT-4) we contrast behavior in the standard task versus a variant in which participants can ask for algorithmically-generated hints. Algorithm choice is administered by a multi-armed bandit whose reward indicates whether the hint helped generating more features. Humans and the AI show similar benefits from hints, and remarkably, bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior. The results suggest that strategies for boosting human creativity via computer interactions can be learned by bandits run on groups of simulated participants.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究了人与AI算法之间的互动如何提高人类创造力。我们使用了一项心理任务，以示人类创造力的限制，即给出概念名称后，参与者需要列出该概念的所有特征。人类参与者通常只能生成一小部分的特征，然后就会被"困顿"。在人类参与者和一种语言AI（GPT-4）的实验中，我们比较了标准任务和一种允许参与者请求算法生成提示的变体。算法选择是通过一个多重机枪进行管理，其奖励参与者选择有助于生成更多特征的提示。人类和AI都显示了类似的好处，奇怪地，由人类行为学习的多重机枪偏好了与AI回答相同的提示策略。这些结果表明，通过计算机互动，可以学习提高人类创造力的策略。
</details></li>
</ul>
<hr>
<h2 id="Straggler-resilient-Federated-Learning-Tackling-Computation-Heterogeneity-with-Layer-wise-Partial-Model-Training-in-Mobile-Edge-Network"><a href="#Straggler-resilient-Federated-Learning-Tackling-Computation-Heterogeneity-with-Layer-wise-Partial-Model-Training-in-Mobile-Edge-Network" class="headerlink" title="Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network"></a>Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10002">http://arxiv.org/abs/2311.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongda Wu, Ping Wang, C V Aswartha Narayana</li>
<li>for: 这个研究旨在提出一种基于分布式学习的训练方法，以便让不同设备 collaboration 训练模型，但不需要共享数据。</li>
<li>methods: 该方法基于模型不同设备的Compute能力不同，设备会根据自己的Compute能力来训练模型，而不是训练完整的模型。</li>
<li>results: 研究表明，该方法可以让不同设备 collaboration 训练模型，并且可以更好地平衡学习精度和完成时间。 compared to 现有的参考方法，该方法可以更快地达到学习目标。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables many resource-limited devices to train a model collaboratively without data sharing. However, many existing works focus on model-homogeneous FL, where the global and local models are the same size, ignoring the inherently heterogeneous computational capabilities of different devices and restricting resource-constrained devices from contributing to FL. In this paper, we consider model-heterogeneous FL and propose Federated Partial Model Training (FedPMT), where devices with smaller computational capabilities work on partial models (subsets of the global model) and contribute to the global model. Different from Dropout-based partial model generation, which removes neurons in hidden layers at random, model training in FedPMT is achieved from the back-propagation perspective. As such, all devices in FedPMT prioritize the most crucial parts of the global model. Theoretical analysis shows that the proposed partial model training design has a similar convergence rate to the widely adopted Federated Averaging (FedAvg) algorithm, $\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor related to the model splitting design in FedPMT. Empirical results show that FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile, compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the learning target in a shorter completion time, thus achieving a better trade-off between learning accuracy and completion time.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 允许多个资源有限的设备共同训练模型而无需数据共享。然而，现有的大多数研究都专注于模型同质的 FL，即全球模型和本地模型均为同一个大小，这ignore了不同设备的内在不同计算能力，从而限制了资源有限的设备参与FL。在这篇文章中，我们考虑了模型不同质的 FL，并提出了 Federated Partial Model Training（FedPMT），其中设备的计算能力较低的设备可以在部分模型（全球模型的子集）上进行训练，并对全球模型进行贡献。与Dropout技术基于随机移除隐藏层中的神经元不同，FedPMT中的模型训练是从反射层的角度来实现的，因此所有的设备在FedPMT中都会优先级别最重要的部分。理论分析表明，我们的 partial model 训练设计和 widely adopted Federated Averaging（FedAvg）算法相似，具有 $\mathcal{O}(1/T)$ 的收敛率，但与 FedAvg 相比，FedPMT 的优劣差异因子与模型分割设计相关。实验结果表明，FedPMT 明显超过了现有的 refer 替 benchmark FedDrop。同时，相比于通用模型同质的标准 refer 替 FedAvg，FedPMT 在完成时间和学习精度之间更好地做出了平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-more-Practical-Threat-Models-in-Artificial-Intelligence-Security"><a href="#Towards-more-Practical-Threat-Models-in-Artificial-Intelligence-Security" class="headerlink" title="Towards more Practical Threat Models in Artificial Intelligence Security"></a>Towards more Practical Threat Models in Artificial Intelligence Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09994">http://arxiv.org/abs/2311.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi</li>
<li>for: 这个论文旨在描述人工智能安全领域的研究与实践之间的差距。</li>
<li>methods: 该论文通过对6种最常研究的AI安全攻击方法进行审查，并与271名工业实践者进行调查，以确定这些攻击方法在实际场景中的可行性。</li>
<li>results: 研究发现，现有的威胁模型都是可行的，但是有一些重大匹配度：研究往往假设攻击者具有实际场景中不易获得的信息。这篇论文因此呼吁研究更加实际的威胁模型。<details>
<summary>Abstract</summary>
Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with \textbf{271} industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. Our paper is thus a call for action to study more practical threat models in artificial intelligence security.
</details>
<details>
<summary>摘要</summary>
最近的研究发现人工智能安全领域存在一个研究与实践之间的差距：在学术中研究的威胁不一定与实际使用和安全风险相符。例如，模型在实践中通常是作为更大的机器学习管道的一部分进行研究，而不是孤立的单元。此外，学术攻击的恶意修改也被证明是不实际的。我们为了描述这种差距，我们重新审视了人工智能安全领域最常研究的六种攻击方法，并通过对 \textbf{271} 名工业实践者进行调查，发现所有的威胁模型都是可靠的。然而，我们发现有一些巨大的匹配错误：研究经常假设攻击者具有实际场景中不常 disponibles的信息。因此，我们的论文是一种呼吁，呼吁更多地研究实际可行的人工智能安全威胁模型。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Hate-Speech-Detection-Evaluation-and-Findings"><a href="#Generative-AI-for-Hate-Speech-Detection-Evaluation-and-Findings" class="headerlink" title="Generative AI for Hate Speech Detection: Evaluation and Findings"></a>Generative AI for Hate Speech Detection: Evaluation and Findings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09993">http://arxiv.org/abs/2311.09993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Pendzel, Tomer Wullach, Amir Adler, Einat Minkov</li>
<li>for: 提高自动仇恨语言检测的泛化性能</li>
<li>methods: 使用生成AI生成大量的人工仇恨语言序列，并在已有标注数据的基础上训练大型预训练语言模型（LLM）</li>
<li>results: 对于BERT、RoBERTa、ALBERT等常见LLM，以及已适应仇恨检测的RoBERTa-Toxicity、HateBERT、HateXplain、ToxDect和ToxiGen等模型，进行了评估和比较，并证实了这种方法可以提高仇恨语言泛化性能。同时，我们还对采用零shot仇恨检测的GPT-3.5模型进行了测试，结果显示该模型可以获得更高的泛化性能，但是具有较差的准确率和预测率。<details>
<summary>Abstract</summary>
Automatic hate speech detection using deep neural models is hampered by the scarcity of labeled datasets, leading to poor generalization. To mitigate this problem, generative AI has been utilized to generate large amounts of synthetic hate speech sequences from available labeled examples, leveraging the generated data in finetuning large pre-trained language models (LLMs). In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT, we apply and evaluate the impact of train set augmentation with generated data using LLMs that have been already adapted for hate detection, including RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical study corroborates our previous findings, showing that this approach improves hate speech generalization, boosting recall performance across data distributions. In addition, we explore and compare the performance of the finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results demonstrate that while better generalization is achieved using the GPT-3.5 model, it achieves mediocre recall and low precision on most datasets. It is an open question whether the sensitivity of models such as GPT-3.5, and onward, can be improved using similar techniques of text generation.
</details>
<details>
<summary>摘要</summary>
自动发现仇恨言语使用深度神经网络受到数据标注的罕见性的限制，导致模型的泛化性差。为解决这个问题，生成AI技术被应用来生成大量的人工仇恨言语序列，利用生成的数据进行训练大型预训练语言模型（LLM）。在这章中，我们提供了相关的方法、实验设置和评估这种方法的评估。除了一般的LLM，如BERT、RoBERTa和ALBERT，我们还应用并评估生成数据集 augmentation的影响。我们使用已经适应仇恨检测的LLM，包括RoBERTa-Toxicity、HateBERT、HateXplain、ToxDect和ToxiGen进行训练和评估。我们的实验结果表明，这种方法可以提高仇恨言语泛化性，提高检测性能 across data distributions。此外，我们还探讨了使用GPT-3.5模型进行零容量仇恨检测的性能。我们的结果表明，虽然使用GPT-3.5模型可以获得更好的泛化性，但它的准确率和精度在大多数数据集上都很低。这是一个开放的问题，是否可以通过类似的文本生成技术提高模型的敏感性。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Monitoring-and-Retraining-Language-Models-in-Real-World-Applications"><a href="#A-Framework-for-Monitoring-and-Retraining-Language-Models-in-Real-World-Applications" class="headerlink" title="A Framework for Monitoring and Retraining Language Models in Real-World Applications"></a>A Framework for Monitoring and Retraining Language Models in Real-World Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09930">http://arxiv.org/abs/2311.09930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaykumar Kasundra, Claudia Schulz, Melicaalsadat Mirsafian, Stavroula Skylaki</li>
<li>for: 本研究旨在探讨多类标签分类模型在不同 retraining 决策点下的影响，以及如何设计有效的模型重新训练策略。</li>
<li>methods: 本研究使用了多种 retraining 决策点，包括数据或概念漂移、模型性能下降和新数据收集等，以评估它们对模型性能和资源利用率的影响。</li>
<li>results: 研究发现，不同 retraining 决策点可能导致不同的模型性能和资源利用率。根据研究结果，提出了一个参考框架，可以帮助设计有效的模型重新训练策略。<details>
<summary>Abstract</summary>
In the Machine Learning (ML) model development lifecycle, training candidate models using an offline holdout dataset and identifying the best model for the given task is only the first step. After the deployment of the selected model, continuous model monitoring and model retraining is required in many real-world applications. There are multiple reasons for retraining, including data or concept drift, which may be reflected on the model performance as monitored by an appropriate metric. Another motivation for retraining is the acquisition of increasing amounts of data over time, which may be used to retrain and improve the model performance even in the absence of drifts. We examine the impact of various retraining decision points on crucial factors, such as model performance and resource utilization, in the context of Multilabel Classification models. We explain our key decision points and propose a reference framework for designing an effective model retraining strategy.
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）模型开发生命周期中，使用停滞 dataset 训练候选模型并选择适合任务的最佳模型只是第一步。在实际应用中，已经部署的选定模型后，需要进行连续的模型监测和重新训练。有多种重新训练的原因，包括数据或概念漂移，这可能会影响模型性能，并且可以通过适当的指标来监测。另一个重新训练的动机是随着时间的推移，采集到的数据量的增加，可以重新训练并改进模型性能，即使没有数据漂移。我们研究重新训练决策点对关键因素的影响，如模型性能和资源利用率，并提出了一个参考框架，以设计有效的模型重新训练策略。
</details></li>
</ul>
<hr>
<h2 id="DSR-Diff-Depth-Map-Super-Resolution-with-Diffusion-Model"><a href="#DSR-Diff-Depth-Map-Super-Resolution-with-Diffusion-Model" class="headerlink" title="DSR-Diff: Depth Map Super-Resolution with Diffusion Model"></a>DSR-Diff: Depth Map Super-Resolution with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09919">http://arxiv.org/abs/2311.09919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shi, Bin Xia, Rui Zhu, Qingmin Liao, Wenming Yang</li>
<li>for: 提高低质量深度图的空间分辨率，用于3D重建、虚拟现实和增强现实等应用。</li>
<li>methods: 利用扩散模型在幽Defaultslat space中生成导航，并将其与高质量颜色图相结合，以实现深度图超分辨。</li>
<li>results: 对比州前方法，提出了一种新的CDSR模型，并实现了较高的准确率和效率。代码将在<a target="_blank" rel="noopener" href="https://github.com/shiyuan7/DSR-Diff%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/shiyuan7/DSR-Diff中发布。</a><details>
<summary>Abstract</summary>
Color-guided depth map super-resolution (CDSR) improve the spatial resolution of a low-quality depth map with the corresponding high-quality color map, benefiting various applications such as 3D reconstruction, virtual reality, and augmented reality. While conventional CDSR methods typically rely on convolutional neural networks or transformers, diffusion models (DMs) have demonstrated notable effectiveness in high-level vision tasks. In this work, we present a novel CDSR paradigm that utilizes a diffusion model within the latent space to generate guidance for depth map super-resolution. The proposed method comprises a guidance generation network (GGN), a depth map super-resolution network (DSRN), and a guidance recovery network (GRN). The GGN is specifically designed to generate the guidance while managing its compactness. Additionally, we integrate a simple but effective feature fusion module and a transformer-style feature extraction module into the DSRN, enabling it to leverage guided priors in the extraction, fusion, and reconstruction of multi-model images. Taking into account both accuracy and efficiency, our proposed method has shown superior performance in extensive experiments when compared to state-of-the-art methods. Our codes will be made available at https://github.com/shiyuan7/DSR-Diff.
</details>
<details>
<summary>摘要</summary>
颜色导航深度地图超分辨 (CDSR) 可以提高低质量深度地图的空间分辨率，有利于多种应用，如三维重建、虚拟现实和增强现实。传统的 CDSR 方法通常采用卷积神经网络或转换器，而扩散模型（DM）则在高级视觉任务中表现出了很好的效果。在这项工作中，我们提出了一种新的 CDSR 模式，利用在潜在空间中的扩散模型来生成指导 depth map 超分辨。我们的方法包括指导生成网络（GGN）、深度地图超分辨网络（DSRN）和指导恢复网络（GRN）。GGN 专门设计用于生成指导，同时管理其 компакт性。此外，我们还将一个简单 yet effective 的特征融合模块和一个基于转换器的特征提取模块integrated into DSRN，使其能够在抽取、融合和重建多模型图像时利用指导PRIORS。考虑到精度和效率，我们提出的方法在广泛的实验中显示出了与状态 искусственный智能方法相比的superior performance。我们的代码将在 GitHub 上发布，请参考 https://github.com/shiyuan7/DSR-Diff.
</details></li>
</ul>
<hr>
<h2 id="INTERVENOR-Prompt-the-Coding-Ability-of-Large-Language-Models-with-the-Interactive-Chain-of-Repairing"><a href="#INTERVENOR-Prompt-the-Coding-Ability-of-Large-Language-Models-with-the-Interactive-Chain-of-Repairing" class="headerlink" title="INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing"></a>INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09868">http://arxiv.org/abs/2311.09868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuir/intervenor">https://github.com/neuir/intervenor</a></li>
<li>paper_authors: Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu</li>
<li>for: 这个论文是为了提出一种基于人类编程行为的自动代码修复方法（INTERVENOR），该方法利用了两个基于大语言模型（LLM）的代理机制，以便在代码修复过程中提高代码生成和翻译能力。</li>
<li>methods: 该方法使用了两个LLM代理机制，namely Code Learner和Code Teacher。Code Learner是根据Code Teacher的指导生成和修复代码，而Code Teacher则是通过编译器的反馈来重新思考代码错误，并生成一个链式修复（CoR）来引导代码修复过程。</li>
<li>results: 我们的实验表明，INTERVENOR比 estado-of-the-art 方法更高效，在代码生成和代码翻译任务中分别提高了约13%和4.5%。我们的进一步分析还表明，CoR可以通过自然语言提供错误原因和解决方案的明确描述。由于编译器的反馈，INTERVENOR可以准确地识别代码中的语法错误和断言错误，并提供精确的修复指令，使LLMs在只需要三次修复后就能达到极限性能。<details>
<summary>Abstract</summary>
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and solution plans via natural language. Thanks to the feedback of code compilers, INTERVENOR can accurately identify the syntax errors and assertion errors in the code and provide precise instructions to repair codes, making LLMs achieve the plateau performance with only three repairing turns. All data and codes are available at https://github.com/NEUIR/INTERVENOR
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种名为INTERactiVE chaiN Of Repairing（INTERVENOR）的方法，它模仿人类代码修复行为（迭代评估、重新思考和修复），并唤醒LLM的编程能力。具体来说，INTERVENOR使用两个基于LLM的代理人：代码学习者和代码教师。代码学习者根据代码教师的指导生成和修复代码。代码教师根据编译器的反馈重新评估代码错误，并生成了修复过程中的链条（CoR），以引导代码修复过程。我们的实验表明，INTERVENOR在代码生成和代码翻译任务上表现出色，与当前状态的方法相比，提高了约13%和4.5%。我们的进一步分析表明，CoR可以通过自然语言来描述错误原因和解决方案。由于编译器的反馈，INTERVENOR可以准确地识别代码中的语法错误和断言错误，并提供精准的修复指导，使LLM在只需三次修复后达到极限性能。所有数据和代码可以在https://github.com/NEUIR/INTERVENOR上获取。
</details></li>
</ul>
<hr>
<h2 id="PsyBench-a-balanced-and-in-depth-Psychological-Chinese-Evaluation-Benchmark-for-Foundation-Models"><a href="#PsyBench-a-balanced-and-in-depth-Psychological-Chinese-Evaluation-Benchmark-for-Foundation-Models" class="headerlink" title="PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models"></a>PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09861">http://arxiv.org/abs/2311.09861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlei Zhang, Hongliang He, Nirui Song, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan</li>
<li>for: 这个论文是为了提供一个全面的中文评价集，用于评估基础模型在心理学领域的能力。</li>
<li>methods: 本文使用多选题型的评价方法，以评估基础模型在不同知识领域的表现。</li>
<li>results: 研究发现不同知识领域的表现存在显著差异，而且只有ChatGPT模型的平均准确率超过70%， indicating that there is still room for improvement in this area.<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) are becoming prevalent in various fields, there is an urgent need for improved NLP benchmarks that encompass all the necessary knowledge of individual discipline. Many contemporary benchmarks for foundational models emphasize a broad range of subjects but often fall short in presenting all the critical subjects and encompassing necessary professional knowledge of them. This shortfall has led to skewed results, given that LLMs exhibit varying performance across different subjects and knowledge areas. To address this issue, we present psybench, the first comprehensive Chinese evaluation suite that covers all the necessary knowledge required for graduate entrance exams. psybench offers a deep evaluation of a model's strengths and weaknesses in psychology through multiple-choice questions. Our findings show significant differences in performance across different sections of a subject, highlighting the risk of skewed results when the knowledge in test sets is not balanced. Notably, only the ChatGPT model reaches an average accuracy above $70\%$, indicating that there is still plenty of room for improvement. We expect that psybench will help to conduct thorough evaluations of base models' strengths and weaknesses and assist in practical application in the field of psychology.
</details>
<details>
<summary>摘要</summary>
如Large Language Models（LLMs）在不同领域变得越来越普遍，需要改进的自然语言处理（NLP）标准测试Suite来涵盖所有必要的专业知识。许多当代测试标准 для基础模型通常会忽略一些重要的主题和专业知识，这会导致测试结果偏向。为解决这个问题，我们介绍了psybench，第一个涵盖所有必要的心理学入学考试知识的全面中文评估suite。psybench通过多选题提供了深入的模型强项和弱项评估。我们的发现表明不同主题section的性能存在显著差异，这说明测试集知识不均衡可能会导致偏向测试结果。各种ChatGPT模型的平均准确率超过70%，这表明当前还有很多机会进行改进。我们期望psybench可以帮助进行深入的模型强项和弱项评估，并在心理学领域的实践应用中提供支持。
</details></li>
</ul>
<hr>
<h2 id="SurvTimeSurvival-Survival-Analysis-On-The-Patient-With-Multiple-Visits-Records"><a href="#SurvTimeSurvival-Survival-Analysis-On-The-Patient-With-Multiple-Visits-Records" class="headerlink" title="SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits&#x2F;Records"></a>SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits&#x2F;Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09854">http://arxiv.org/abs/2311.09854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlee1102/surtimesurvival">https://github.com/davidlee1102/surtimesurvival</a></li>
<li>paper_authors: Hung Le, Ong Eng-Jon, Bober Miroslaw</li>
<li>for: 预测严重疾病患者生存时间的准确预测仍然是艰难的挑战，尽管近年来人工智能技术得到了进一步发展。这项研究推出了“SurvTimeSurvival：survival分析在多次&#x2F;记录数据上”，利用Transformer模型不仅能够处理时间变化的covariates，还能够处理covariates数据。</li>
<li>methods: 我们采用了Transformer模型来处理时间变化的covariates和covariates数据，并解决了生存分析数据集中的数据缺失问题，通过将生成Synthetic数据集成到学习过程中。</li>
<li>results: 我们的方法在covariates和时间变化covariates数据集上都超过了现有的深度学习方法的性能。我们的方法的目的不仅是提高个体患者生存轨迹的理解，从而提高预测精度，而且也在设计临床试验和开发新的治疗方法中发挥重要作用。<details>
<summary>Abstract</summary>
The accurate prediction of survival times for patients with severe diseases remains a critical challenge despite recent advances in artificial intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On Patients With Multiple Visits/Records", utilizing the Transformer model to not only handle the complexities of time-varying covariates but also covariates data. We also tackle the data sparsity issue common to survival analysis datasets by integrating synthetic data generation into the learning process of our model. We show that our method outperforms state-of-the-art deep learning approaches on both covariates and time-varying covariates datasets. Our approach aims not only to enhance the understanding of individual patient survival trajectories across various medical conditions, thereby improving prediction accuracy, but also to play a pivotal role in designing clinical trials and creating new treatments.
</details>
<details>
<summary>摘要</summary>
医学预测患者生存时间的准确性仍然是一项关键挑战，尽管最近的人工智能技术得到了进步。这项研究介绍了“SurvTimeSurvival：基于多次/记录的生存分析”，利用Transformer模型不仅能处理时间变化的共 covariates，而且还能处理 covariates 数据。我们还解决了生存分析数据集中的数据缺失问题通过将生成 Synthetic 数据 incorporated 到我们的模型学习过程中。我们表明我们的方法在 covariates 和时间变化 covariates 数据集上都能够超越当前的深度学习方法。我们的方法不仅可以提高预测准确性，还可以提高对各种医疗情况的个体患者生存轨迹的理解，从而为设计临床试验和开发新药物做出重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-LLMs-in-Scholarly-Knowledge-Graph-Question-Answering"><a href="#Leveraging-LLMs-in-Scholarly-Knowledge-Graph-Question-Answering" class="headerlink" title="Leveraging LLMs in Scholarly Knowledge Graph Question Answering"></a>Leveraging LLMs in Scholarly Knowledge Graph Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09841">http://arxiv.org/abs/2311.09841</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huntila/scholarly-kgqa">https://github.com/huntila/scholarly-kgqa</a></li>
<li>paper_authors: Tilahun Abedissa Taffa, Ricardo Usbeck</li>
<li>for: 这篇论文旨在提出一种学术知识图问答系统，可以通过几 shot 的方式回答 bibliographic 自然语言问题。</li>
<li>methods: 该模型首先使用 BERT 基于 sentence encoder 将测试问题与training问题进行相似性比较，然后选择 top-n 相似问题对应的 SPARQL，并将这些对应的问题作为示例，将测试问题作为提示， passing it to LLM 生成 SPARQL。最后，对于underlying KG （ORKG）终端进行查询，并返回答案。</li>
<li>results: 该系统在 SciQA 中实现了 F1 分数 99.0%，在 Scholarly-QALD-23 挑战 benchmark 上表现出色。<details>
<summary>Abstract</summary>
This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the top-n similar question-SPARQL pairs as an example and the test question creates a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of the Scholarly-QALD-23 challenge benchmarks.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种学术知识图问答系统（KGQA），该系统可以通过几个尝试回答文学性问题，使用大型语言模型（LLM）。系统首先使用BERT基于的句子编码器来将测试问题与相似训练问题进行对比，然后使用相似问题-SPARQL对应的最上层对象来生成一个提示。最后，将提示传递给LLM进行生成SPARQL，并将其运行于基础知识图（ORKG）终端，以获得答案。我们的系统在SciQA中的F1分数达99.0%。
</details></li>
</ul>
<hr>
<h2 id="PELMS-Pre-training-for-Effective-Low-Shot-Multi-Document-Summarization"><a href="#PELMS-Pre-training-for-Effective-Low-Shot-Multi-Document-Summarization" class="headerlink" title="PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization"></a>PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09836">http://arxiv.org/abs/2311.09836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph J. Peper, Wenzhao Qiu, Lu Wang</li>
<li>for: 本研究旨在提高摘要文献的抽象性和反射性，即通过多文摘要来提高文献的简洁、流畅和准确性。</li>
<li>methods: 我们提出了一种基于语义相关性规则和准确性约束的预训练模型，并使用了多个文档输入来支持模型的训练。</li>
<li>results: 我们在多种摘要任务上进行了广泛的评估，并经验显示了我们的方法在低shot设定下可以准确地捕捉摘要文献的主题和含义，并且在抽象性、流畅性、准确性和 faithfulness 等方面具有优异性。<details>
<summary>Abstract</summary>
We investigate pre-training techniques for abstractive multi-document summarization (MDS), which is much less studied than summarizing single documents. Though recent work has demonstrated the effectiveness of highlighting information salience for pre-training strategy design, it struggles to generate abstractive and reflective summaries, which are critical properties for MDS. To this end, we present PELMS, a pre-trained model that uses objectives based on semantic coherence heuristics and faithfulness constraints with un-labeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries. To support the training of PELMS, we compile MultiPT, a multi-document pre-training corpus containing over 93 million documents to form more than 3 million unlabeled topic-centric document clusters, covering diverse genres such as product reviews, news, and general knowledge. We perform extensive evaluation of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently outperforms competitive comparisons with respect to overall informativeness, abstractiveness, coherence, and faithfulness.
</details>
<details>
<summary>摘要</summary>
我团队研究了抽象多文摘要（MDS）的预训练技术，这个领域比单文摘要更为少studied。虽然 latest work 表明了突出信息重要性的预训练策略的效果，但它很难生成抽象和反射的摘要，这些特性是MDS的关键性能。为此，我们提出了 PELMS，一种预训练模型，使用基于 semantic coherence heuristics 和 faithfulness constraints 的目标函数，以便在无标签多文输入下生成简洁、流畅、忠实的摘要。为支持 PELMS 的训练，我们编译了 MultiPT，一个包含超过 93 万个文档的多文预训练集，其中包含多种类型的文档，如产品评论、新闻和通用知识。我们对 PELMS 进行了广泛的评估，包括低投入设定下的评估，并在多种 MDS 数据集上表现出consistent 的优异性。
</details></li>
</ul>
<hr>
<h2 id="ML-Bench-Large-Language-Models-Leverage-Open-source-Libraries-for-Machine-Learning-Tasks"><a href="#ML-Bench-Large-Language-Models-Leverage-Open-source-Libraries-for-Machine-Learning-Tasks" class="headerlink" title="ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks"></a>ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09835">http://arxiv.org/abs/2311.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Zhengliang Li, Liang Chen, Yiming Zong, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein</li>
<li>for: 本研究旨在评估大语言模型在使用开源库完成机器学习任务时的实用性。</li>
<li>methods: 本研究提出了一个新的评估Setup，其中大语言模型使用开源库完成机器学习任务，而不是从scratch编写代码。研究者们制定了ML-Bench，一个包含10044个样例和130个任务的广泛的benchmark，以评估大语言模型在使用开源库时的效果。</li>
<li>results: GPT-4在这些任务中表现出色，但只有39.73%的任务得到了成功。研究者们提出了ML-Agent，一种用于快速浏览代码库，找到相关文档、代码和执行代码的方法。与GPT-4结合使用后，ML-Agent得到了进一步的改进。<details>
<summary>Abstract</summary>
Large language models have shown promising performance in code generation benchmarks. However, a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries. Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks. Therefore, we propose ML-Bench, an expansive benchmark developed to assess the effectiveness of LLMs in leveraging existing functions in open-source libraries. Consisting of 10044 samples spanning 130 tasks over 14 notable machine learning GitHub repositories. In this setting, given a specific machine learning task instruction and the accompanying README in a codebase, an LLM is tasked to generate code to accomplish the task. This necessitates the comprehension of long and language-code interleaved documents, as well as the understanding of complex cross-file code structures, introducing new challenges. Notably, while GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space for improvement. We address these challenges by proposing ML-Agent, designed to effectively navigate the codebase, locate documentation, retrieve code, and generate executable code. Empirical results demonstrate that ML-Agent, built upon GPT-4, results in further improvements. Code, data, and models are available at \url{https://ml-bench.github.io/}.
</details>
<details>
<summary>摘要</summary>
大型语言模型在代码生成比赛中表现出了优异的成绩。然而，实际Programming中对于这些模型的实用性存在许多差距，主要是因为实际程式码中的对于预存函数的依赖。相反于评估这些模型在从零开始写代码的能力，这个工作尝试了一个新的评估设置，让模型使用公开源代码库中的函数来完成机器学习任务。因此，我们提出了 ML-Bench，一个包含10044个样本、130个任务和14个知名机器学习GitHub来源的广泛的库。在这个设置下，给定一个机器学习任务的指令和相应的README档案，一个模型需要使用代码库中的函数来完成任务。这需要理解长长的文档和代码档案之间的交互，以及复杂的跨档代码结构，带来新的挑战。对此，我们提出了 ML-Agent，用于有效地浏览代码库、找到文档、获取代码和实现可执行的代码。实验结果显示，使用 GPT-4 建立的 ML-Agent 导致了进一步的改善。代码、数据和模型可以在 \url{https://ml-bench.github.io/} 获取。
</details></li>
</ul>
<hr>
<h2 id="AutoPlanBench-Automatically-generating-benchmarks-for-LLM-planners-from-PDDL"><a href="#AutoPlanBench-Automatically-generating-benchmarks-for-LLM-planners-from-PDDL" class="headerlink" title="AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL"></a>AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09830">http://arxiv.org/abs/2311.09830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Stein, Alexander Koller</li>
<li>for: 这paper的目的是探讨LLMs在规划任务中的能力，以及如何使用文本描述来评估这些能力。</li>
<li>methods: 这paper使用了一种新的方法，可以将PDDL中的规划benchmark转换成文本描述，并提供了一个新的benchmark集。</li>
<li>results: 研究发现，当今最好的LLM规划器在许多规划任务上表现出色，但是其他任务仍然远远超出了现有方法的能力范围。<details>
<summary>Abstract</summary>
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on many planning tasks, others remain out of reach of current methods.
</details>
<details>
<summary>摘要</summary>
LLMs 正在越来越多地用于计划样式的任务，但它们的计划和理解能力尚未得到充分的理解。我们提出了一种新的方法，可以自动将 PDDL 格式的计划标准转换成文本描述，并提供了我们创建的 benchmark 数据集。我们发现，当前的 LLM 计划器在许多计划任务上表现出色，但有些任务仍然超出当前的能力范围。
</details></li>
</ul>
<hr>
<h2 id="PWISeg-Point-based-Weakly-supervised-Instance-Segmentation-for-Surgical-Instruments"><a href="#PWISeg-Point-based-Weakly-supervised-Instance-Segmentation-for-Surgical-Instruments" class="headerlink" title="PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments"></a>PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09819">http://arxiv.org/abs/2311.09819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seanxuu/PWISeg">https://github.com/seanxuu/PWISeg</a></li>
<li>paper_authors: Zhen Sun, Huan Xu, Jinlin Wu, Zhen Chen, Zhen Lei, Hongbin Liu</li>
<li>for: 这个论文的目的是提出一种新的、有效的医疗器械实例分割方法，以解决实际中获得mask-level注释是劳动密集的问题。</li>
<li>methods: 该方法基于FCN架构，包括点到方框和点到面分支，用于模型特征点和 bounding box 之间的关系，以及特征点和分割面之间的关系。</li>
<li>results: 该方法在我们所提供的新的医疗器械数据集上进行了广泛的试验，并证明了与大多数无监督 bounding box 的实例分割方法相比，它的实rument分割精度得到了显著提高。<details>
<summary>Abstract</summary>
In surgical procedures, correct instrument counting is essential. Instance segmentation is a location method that locates not only an object's bounding box but also each pixel's specific details. However, obtaining mask-level annotations is labor-intensive in instance segmentation. To address this issue, we propose a novel yet effective weakly-supervised surgical instrument instance segmentation approach, named Point-based Weakly-supervised Instance Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with point-to-box and point-to-mask branches to model the relationships between feature points and bounding boxes, as well as feature points and segmentation masks on FPN, accomplishing instrument detection and segmentation jointly in a single model. Since mask level annotations are hard to available in the real world, for point-to-mask training, we introduce an unsupervised projection loss, utilizing the projected relation between predicted masks and bboxes as supervision signal. On the other hand, we annotate a few pixels as the key pixel for each instrument. Based on this, we further propose a key pixel association loss and a key pixel distribution loss, driving the point-to-mask branch to generate more accurate segmentation predictions. To comprehensively evaluate this task, we unveil a novel surgical instrument dataset with manual annotations, setting up a benchmark for further research. Our comprehensive research trial validated the superior performance of our PWISeg. The results show that the accuracy of surgical instrument segmentation is improved, surpassing most methods of instance segmentation via weakly supervised bounding boxes. This improvement is consistently observed in our proposed dataset and when applied to the public HOSPI-Tools dataset.
</details>
<details>
<summary>摘要</summary>
在手术过程中，正确的工具数量是非常重要的。实例分割是一种位置方法，可以不仅找到物体的包围盒，还可以每个像素的特定细节。然而，在实例分割中获得mask水平的注释是很劳动密集的。为解决这个问题，我们提出了一种新的但有效的weakly-supervised手术工具实例分割方法，名为Point-based Weakly-supervised Instance Segmentation（PWISeg）。PWISeg采用了FCN基 architecture，并设置了点到包围盒和点到面积分支，以模型特征点和包围盒之间的关系，以及特征点和分割面积之间的关系。这样可以同时完成工具检测和分割。由于mask水平的注释很难在实际世界中获得，为点到面训练，我们引入了一种无监督投影损失，利用预测的面积和包围盒之间的投影关系作为监督信号。此外，我们还标注了每个工具的一些键点，并基于这些键点，我们进一步提出了键点协会损失和键点分布损失，使点到面分支生成更加准确的分割预测。为全面评估这个任务，我们披露了一个新的手术工具数据集，并设置了一个标准的比较基准。我们的全面研究试验证明了PWISeg的超越性。结果表明，通过我们提出的PWISeg，手术工具分割的准确性得到了提高，超越了大多数基于weakly supervised bounding box的实例分割方法。这种改进是在我们所提出的数据集和公共HOSPI-Tools数据集上均可见。
</details></li>
</ul>
<hr>
<h2 id="MedAgents-Large-Language-Models-as-Collaborators-for-Zero-shot-Medical-Reasoning"><a href="#MedAgents-Large-Language-Models-as-Collaborators-for-Zero-shot-Medical-Reasoning" class="headerlink" title="MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning"></a>MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10537">http://arxiv.org/abs/2311.10537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents">https://github.com/gersteinlab/medagents</a></li>
<li>paper_authors: Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein</li>
<li>for: 本研究旨在提高语言模型在医学领域的表现，并且提供一种可行的多学科合作框架，以便语言模型可以更好地理解和应用医学知识。</li>
<li>methods: 本研究使用了多学科合作框架，其包括五个关键步骤：寻找培训数据，提出个人分析，汇总分析成报告，进行多轮讨论，并最终做出决策。</li>
<li>results: 本研究在九个数据集（MedQA、MedMCQA、PubMedQA以及MMLU中的六个子任务）上得到了出色的结果，证明了我们提出的多学科合作框架可以帮助语言模型更好地理解和应用医学知识，同时还可以扩展其理解能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and the reasoning over specialized knowledge. To address these obstinate issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work particularly focuses on the zero-shot scenario, our results on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise in LLMs, as well as extending its reasoning abilities. Based on these outcomes, we further conduct a human evaluation to pinpoint and categorize common errors within our method, as well as ablation studies aimed at understanding the impact of various factors on overall performance. Our code can be found at \url{https://github.com/gersteinlab/MedAgents}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），尽管在不同领域中做出了惊人的进步，在医疗领域还是遇到了很多障碍。这个领域面临着域名特定的术语和专业知识的推理等独特问题。为了解决这些难题，我们提出了一种新的多学科合作（MC）框架，该框架通过多个角色扮演LLM基于的代理人参与协同多轮讨论，从而提高LLM的技能和推理能力。这个无需训练和可解释的框架包括五个关键步骤：召集域专家、提出个人分析、汇总分析成报告、讨论迭代 until 达成一致，并最终做出决策。我们的工作特别关注于零容量情况下的情况，我们的结果表明，我们提出的MC框架在激活和利用医疗领域LLM的专业知识，以及扩展其推理能力方面表现出色。基于这些结果，我们进一步进行了人类评估，以找到和分类我们方法中的常见错误，以及进行了缺失因素的研究，以了解对总性表现的影响。我们的代码可以在 \url{https://github.com/gersteinlab/MedAgents} 找到。
</details></li>
</ul>
<hr>
<h2 id="Performance-Trade-offs-of-Watermarking-Large-Language-Models"><a href="#Performance-Trade-offs-of-Watermarking-Large-Language-Models" class="headerlink" title="Performance Trade-offs of Watermarking Large Language Models"></a>Performance Trade-offs of Watermarking Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09816">http://arxiv.org/abs/2311.09816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Ajith, Sameer Singh, Danish Pruthi</li>
<li>for: 本研究旨在评估水印模型在多种任务上的性能，以了解水印的影响和用户应该考虑的贸易OFF。</li>
<li>methods: 本研究使用了水印策略，在生成文本中嵌入一个信号，以便区分人工生成的文本和模型生成的文本。</li>
<li>results: 研究发现，在大多数情况下，水印对任务的性能没有显著影响。但是，长形生成任务（如概要和翻译）的性能下降了15-20%。这些结果指出了水印使用的贸易OFF，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Amidst growing concerns of large language models (LLMs) being misused for generating misinformation or completing homework assignments, watermarking has emerged as an effective solution for distinguishing human-written and LLM-generated text. A prominent watermarking strategy is to embed a signal into generated text by upsampling a (pseudorandomly-chosen) subset of tokens at every generation step. Although this signal is imperceptible to a human reader, it is detectable through statistical testing. However, implanting such signals alters the model's output distribution and can have unintended effects when watermarked LLMs are used for downstream applications. In this work, we evaluate the performance of watermarked LLMs on a diverse suite of tasks, including text classification, textual entailment, reasoning, question answering, translation, summarization, and language modeling. We find that watermarking has negligible impact on the performance of tasks posed as k-class classification problems in the average case. However, the accuracy can plummet to that of a random classifier for some scenarios (that occur with non-negligible probability). Tasks that are cast as multiple-choice questions and short-form generation are surprisingly unaffected by watermarking. For long-form generation tasks, including summarization and translation, we see a drop of 15-20% in the performance due to watermarking. Our findings highlight the trade-offs that users should be cognizant of when using watermarked models, and point to cases where future research could improve existing trade-offs.
</details>
<details>
<summary>摘要</summary>
在大型语言模型（LLM）被违用于生成谣言或完成作业任务时，水印技术已成为一种有效的解决方案，以区分人类写作和LLM生成的文本。一种常见的水印策略是在生成文本时附加一个信号，通过随机选择一部分token进行upsampling。尽管这个信号对人类读者无法察觉，但可以通过统计测试探测。然而，植入这个信号会改变模型的输出分布，可能会导致下游应用中的不良影响。在这个工作中，我们评估水印LLM在多种任务上的表现，包括文本分类、文本推理、问答、翻译、概要和语言模型。我们发现，对于大多数情况，水印对任务的性能没有显著影响。然而，在某些特殊情况下（占总体的非致命概率），水印可能会导致性能降低到随机分类器水平。在长形生成任务中，如概要和翻译，我们发现水印导致性能下降约15-20%。我们的发现指出了使用水印模型时的交易offs，并提出了未来研究可以改善现有交易offs的可能性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Formal-Fault-Injection-for-Safety-Assessment-of-Automated-Systems"><a href="#Towards-Formal-Fault-Injection-for-Safety-Assessment-of-Automated-Systems" class="headerlink" title="Towards Formal Fault Injection for Safety Assessment of Automated Systems"></a>Towards Formal Fault Injection for Safety Assessment of Automated Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09810">http://arxiv.org/abs/2311.09810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashfaq Farooqui, Behrooz Sangchoolie</li>
<li>for: 这篇论文是为了探讨自动化系统中的安全性、安全性和其他可靠性特性的问题，以便在日常生活中广泛应用自动化系统。</li>
<li>methods: 这篇论文使用了正式方法，这些方法可以数学方式推导系统的行为，从而确保系统的可靠性。然而，这些方法通常只适用于系统抽象模型，可能不能完全反映实际系统。</li>
<li>results: 这篇论文提出了正式缺陷插入，一种将正式方法和缺陷插入相结合的技术，以提高自动化系统的可靠性。文章还讨论了这些技术在开发生命周期中的潜在优势，并提出了未来研究的可能性，以解决当前的挑战。<details>
<summary>Abstract</summary>
Reasoning about safety, security, and other dependability attributes of autonomous systems is a challenge that needs to be addressed before the adoption of such systems in day-to-day life. Formal methods is a class of methods that mathematically reason about a system's behavior. Thus, a correctness proof is sufficient to conclude the system's dependability. However, these methods are usually applied to abstract models of the system, which might not fully represent the actual system. Fault injection, on the other hand, is a testing method to evaluate the dependability of systems. However, the amount of testing required to evaluate the system is rather large and often a problem. This vision paper introduces formal fault injection, a fusion of these two techniques throughout the development lifecycle to enhance the dependability of autonomous systems. We advocate for a more cohesive approach by identifying five areas of mutual support between formal methods and fault injection. By forging stronger ties between the two fields, we pave the way for developing safe and dependable autonomous systems. This paper delves into the integration's potential and outlines future research avenues, addressing open challenges along the way.
</details>
<details>
<summary>摘要</summary>
考虑 autonomous systems 的安全性、安全性和其他可靠性特性的推理是在普及这些系统之前需要解决的挑战。Formal methods 是一类方法，通过数学方式推理系统的行为，因此，一个正确性证明即可确保系统的可靠性。然而，这些方法通常应用于系统抽象模型，可能不完全反映实际系统。错误插入测试是一种评估系统可靠性的方法，但测试量很大，经常成为问题。这篇视野论文介绍了形式错误插入，它将这两种技术在开发生命周期中融合，以提高自动化系统的可靠性。我们认为这两个领域之间存在五个互助领域，通过加强这两个领域之间的关系，我们开拓了开发安全可靠的自动化系统的可能性。这篇论文探讨了融合的潜在可能性和未来研究方向，并解决了一些开放的挑战。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Differentiable-Logics-for-Learning-Systems-A-Research-Preview"><a href="#Comparing-Differentiable-Logics-for-Learning-Systems-A-Research-Preview" class="headerlink" title="Comparing Differentiable Logics for Learning Systems: A Research Preview"></a>Comparing Differentiable Logics for Learning Systems: A Research Preview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09809">http://arxiv.org/abs/2311.09809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tflinkow/dl-comparison">https://github.com/tflinkow/dl-comparison</a></li>
<li>paper_authors: Thomas Flinkow, Barak A. Pearlmutter, Rosemary Monahan</li>
<li>for: 这篇论文旨在研究如何使机器学习（ML）系统满足正确性和安全性要求，并考虑了自动化系统的自我更新和适应能力。</li>
<li>methods: 论文使用了 differentiable logics 方法，其中 Background Knowledge 编码为逻辑约束，导引学习过程。</li>
<li>results: 实验结果与文献报道的结果相符，但是使用 differentiable logics 引入了一个新的 гиперпарамет，即 tuning 难度和影响力。<details>
<summary>Abstract</summary>
Extensive research on formal verification of machine learning (ML) systems indicates that learning from data alone often fails to capture underlying background knowledge. A variety of verifiers have been developed to ensure that a machine-learnt model satisfies correctness and safety properties, however, these verifiers typically assume a trained network with fixed weights. ML-enabled autonomous systems are required to not only detect incorrect predictions, but should also possess the ability to self-correct, continuously improving and adapting. A promising approach for creating ML models that inherently satisfy constraints is to encode background knowledge as logical constraints that guide the learning process via so-called differentiable logics. In this research preview, we compare and evaluate various logics from the literature in weakly-supervised contexts, presenting our findings and highlighting open problems for future work. Our experimental results are broadly consistent with results reported previously in literature; however, learning with differentiable logics introduces a new hyperparameter that is difficult to tune and has significant influence on the effectiveness of the logics.
</details>
<details>
<summary>摘要</summary>
根据大量研究，机器学习（ML）系统从数据alone学习时常常无法捕捉下面背景知识。为确保机器学习模型满足正确性和安全性质量，许多验证工具已经开发，但这些验证工具通常假设已经训练过的网络重量是固定的。ML自适应系统需要不仅检测错误预测，还应该具备自我更新和适应能力。一种有前途的方法是通过 différentiable logics将背景知识编码成逻辑约束，以导引学习过程。在这个研究预览中，我们对文献中的不同逻辑进行比较和评价，在弱监督上下文中展示我们的发现和挑出未来工作的开放问题。我们的实验结果与文献中已经报道的结果相符，但是学习与 diffe抽象逻辑引入了一个新的Hyperparameter，它具有很大的影响力和难于调整。
</details></li>
</ul>
<hr>
<h2 id="Neuro-Symbolic-Integration-Brings-Causal-and-Reliable-Reasoning-Proofs"><a href="#Neuro-Symbolic-Integration-Brings-Causal-and-Reliable-Reasoning-Proofs" class="headerlink" title="Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs"></a>Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09802">http://arxiv.org/abs/2311.09802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/caring">https://github.com/damo-nlp-sg/caring</a></li>
<li>paper_authors: Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam</li>
<li>for: 这 paper 是为了提高 AI 模型的推理能力和可靠性而写的。</li>
<li>methods: 这 paper 使用了一种 combining 方法，即将 neural LLM 和 symbolic solver  integrate 在一起，以便进行 deliberative reasoning 和证明。</li>
<li>results: 实验表明，使用这种方法可以在 ProofWriter 和 GSM8K 上大幅提高推理 accuracy 和证明相似性。<details>
<summary>Abstract</summary>
Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge. Specifically, our customized meta-interpreters allow the production of reasoning proofs and support flexible search strategies. These reasoning proofs are ensured to be causal and reliable because of the deterministic executing nature of the symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT baseline by nearly double in accuracy and more than triple in proof similarity. On GSM8K, our method also shows accuracy improvements and nearly doubled proof similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing
</details>
<details>
<summary>摘要</summary>
尽管通过不同的逻辑结构让LLMs进行推理生成推理证明和答案，但这些证明并不能保证是 causal 和可靠的，因为 LLMS 本身存在一些缺陷。为了解决这些问题，我们提出了一种神经符号 интеграция方法，在这种方法中，一个神经网络 LLM 用于表示问题的知识，而另一个 LLM-free 符号分析器用于进行思考和推理。具体来说，我们自定义的 meta-interpreters 允许生成推理证明和支持灵活的搜索策略。由于符号分析器的 deterministic 执行特性，所生成的推理证明是 causal 和可靠的。在 ProofWriter 上，我们的方法比 CoT 基线高出 nearly double 的准确率和 более triple 的证明相似度。在 GSM8K 上，我们的方法也显示了准确率上的提高和证明相似度的近 double。我们的代码在 <https://github.com/DAMO-NLP-SG/CaRing> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-User-Requests-in-the-Context-of-Natural-Language-Standing-Instructions"><a href="#Interpreting-User-Requests-in-the-Context-of-Natural-Language-Standing-Instructions" class="headerlink" title="Interpreting User Requests in the Context of Natural Language Standing Instructions"></a>Interpreting User Requests in the Context of Natural Language Standing Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09796">http://arxiv.org/abs/2311.09796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Moghe, Patrick Xia, Jacob Andreas, Jason Eisner, Benjamin Van Durme, Harsh Jhamtani</li>
<li>for: 用于提高自然语言界面的用户体验，减少用户重复提供偏好信息的需求。</li>
<li>methods: 利用大型语言模型（LLMs）和自然语言描述文本（standing instructions）作为用户偏好和指令的补充信息。</li>
<li>results: 在NLSI语料集上进行实验，使用大语言模型和不同的检索方法，最高达44.7%的精确匹配率。<details>
<summary>Abstract</summary>
Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. To alleviate this, we propose including some of a user's preferences and instructions in natural language -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states I'm hungry, their previously expressed preference for Persian food will be automatically added to the LLM prompt, so as to influence the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 44.7% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.
</details>
<details>
<summary>摘要</summary>
用户们使用自然语言界面，通常需要每次发出相似的请求都重复他们的首选项。为了解决这个问题，我们建议将用户的首选项和指令（总称为“站坐指令”）作为自然语言 interfaces 的附加上下文。例如，当用户说 “我饿” 时，他们之前表达的波斯料食物的首选项将自动添加到 LLM 提示中，以影响搜索相关餐厅。我们开发了 NLSI，一个语言到程序数据集，包含了超过 2.4K 对话，涵盖 17 个领域，每个对话都与用户的 Profile（用户特定的站坐指令）和相应的结构化表示（API 调用）一起出现。在 NLSI 中，一个主要挑战是确定哪些站坐指令适用于给定的对话。NLSI 包含了多种现象，从简单的首选项到互相关联的指令，例如在购买票务时自动触发酒店搜索。我们使用 LLM 和不同的检索方法进行实验，最高达 44.7% 精确匹配 API 预测。我们的结果表明了站坐指令的适用和其 интерпретация成 API 调用的挑战。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Boundaries-Balancing-Performance-and-Robustness-in-Deep-Wireless-Traffic-Forecasting"><a href="#Breaking-Boundaries-Balancing-Performance-and-Robustness-in-Deep-Wireless-Traffic-Forecasting" class="headerlink" title="Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting"></a>Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09790">http://arxiv.org/abs/2311.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas</li>
<li>for: 本研究旨在寻找一种能够平衡精度和Robustness的时序预测方法，以便在真实世界中的电信数据上进行预测。</li>
<li>methods: 我们使用了一种混合策略，包括一个分类器来检测外延攻击，一个去噪器来消除外延数据样本中的噪声，以及一个标准预测模型。我们对这些策略进行了比较，并与两种现有的对抗训练算法进行了比较。</li>
<li>results: 我们的hybrid策略在 both clean和外延数据上表现出色，其MSE在clean数据上保持了92.02%的原始预测模型性能，而在外延数据上则更加Robust，其MSE比较方法的MSE低出2.71倍和2.51倍。此外，我们的模型的组件可以并行训练，从而提高计算效率。<details>
<summary>Abstract</summary>
Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using $\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to $92.02\%$ the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.
</details>
<details>
<summary>摘要</summary>
平衡精度和Robustness之间的贸易OFF是时间序列预测领域的长standing挑战。大多数现有的Robust算法在干净数据上达到了一定的下行性，但在数据抖动的情况下维持同样的性能很难。在这篇论文中，我们研究了各种抖动enario并提出了新的防御机制，使用实际的电信数据对抗 adversarial 攻击。我们与两种现有的 adversarial 训练算法进行比较，使用 $[0.1,0.4]$ 的 $\ell_{\infty}$ 范围内的最大允许抖动。我们的发现显示，我们的混合策略，包括一个分类器来检测 adversarial 示例，一个去噪器来除掉抖动数据示例中的噪声，以及一个标准预测器，在干净数据和抖动数据上都能够达到最好的性能。我们的优化模型可以保持原始预测模型的 $92.02\%$ 的性能（按照 Mean Squared Error 的评价），而且在抖动数据上比标准 adversarial 训练模型更加Robust。它的 MSE 值分别为 $2.71\times$ 和 $2.51\times$ 比对应模型的 MSE 值更低。此外，我们的模型组件可以并行训练，从而提高计算效率。我们的结果表明，我们可以通过改进分类器和去噪器来优化 forecasting 模型，甚至在抖动数据上进行高级和破坏性攻击。
</details></li>
</ul>
<hr>
<h2 id="3vLTL-A-Tool-to-Generate-Automata-for-Three-valued-LTL"><a href="#3vLTL-A-Tool-to-Generate-Automata-for-Three-valued-LTL" class="headerlink" title="3vLTL: A Tool to Generate Automata for Three-valued LTL"></a>3vLTL: A Tool to Generate Automata for Three-valued LTL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09787">http://arxiv.org/abs/2311.09787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Belardinelli, Angelo Ferrando, Vadim Malvone</li>
<li>for: 这篇论文主要是为了提供一个工具来生成 Buchi 自动机，用于验证 Linear-time Temporal Logic（LTL）式语言中的多值规定。</li>
<li>methods: 该工具使用Linear-time Temporal Logic（LTL）式语言中的三值 semantics来解释 formulas，并生成一个 Buchi 自动机，用于验证 LTL 式是否真、假或未定义于一个模型中。</li>
<li>results: 该工具可以生成一个 Buchi 自动机，用于验证 LTL 式的真假性，并且可以让这个自动机被第三方库处理，以便进一步进行验证。<details>
<summary>Abstract</summary>
Multi-valued logics have a long tradition in the literature on system verification, including run-time verification. However, comparatively fewer model-checking tools have been developed for multi-valued specification languages. We present 3vLTL, a tool to generate Buchi automata from formulas in Linear-time Temporal Logic (LTL) interpreted on a three-valued semantics. Given an LTL formula, a set of atomic propositions as the alphabet for the automaton, and a truth value, our procedure generates a Buchi automaton that accepts all the words that assign the chosen truth value to the LTL formula. Given the particular type of the output of the tool, it can also be seamlessly processed by third-party libraries in a natural way. That is, the Buchi automaton can then be used in the context of formal verification to check whether an LTL formula is true, false, or undefined on a given model.
</details>
<details>
<summary>摘要</summary>
多值逻辑在系统验证文献中有很长的传统，包括运行时验证。然而，相比之下， fewer model-checking工具被开发用于多值规定语言。我们介绍了3vLTL，一种生成 Buchi 自动机从Linear-time Temporal Logic（LTL）在三值 semantics中解释的方程的工具。给定一个 LTL 方程，一个字母集，一个真假值，我们的过程生成一个 Buchi 自动机，接受将选择的真假值分配给 LTL 方程的所有词。给出特定输出的类型，这个 Buchi 自动机可以轻松地处理第三方库中的自然方式。也就是说，Buch i自动机可以在正式验证中使用，以验证一个 LTL 方程是否真、假或未定义于一个模型。
</details></li>
</ul>
<hr>
<h2 id="Correct-by-Construction-Control-for-Stochastic-and-Uncertain-Dynamical-Models-via-Formal-Abstractions"><a href="#Correct-by-Construction-Control-for-Stochastic-and-Uncertain-Dynamical-Models-via-Formal-Abstractions" class="headerlink" title="Correct-by-Construction Control for Stochastic and Uncertain Dynamical Models via Formal Abstractions"></a>Correct-by-Construction Control for Stochastic and Uncertain Dynamical Models via Formal Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09786">http://arxiv.org/abs/2311.09786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thom Badings, Nils Jansen, Licio Romao, Alessandro Abate</li>
<li>for: 这篇论文的目的是为Autonomous Systems的自动化控制器的批量生成提供了一种可靠的方法。</li>
<li>methods: 这篇论文使用了一种基于Markov决策过程的间接抽象方法，以及现有的验证技术来计算一个满足给定规范的优化策略。</li>
<li>results: 这篇论文的结果表明，通过使用这种方法，可以生成一个可靠地满足规范的控制器，并且可以证明这个控制器满足规范的 garanties。<details>
<summary>Abstract</summary>
Automated synthesis of correct-by-construction controllers for autonomous systems is crucial for their deployment in safety-critical scenarios. Such autonomous systems are naturally modeled as stochastic dynamical models. The general problem is to compute a controller that provably satisfies a given task, represented as a probabilistic temporal logic specification. However, factors such as stochastic uncertainty, imprecisely known parameters, and hybrid features make this problem challenging. We have developed an abstraction framework that can be used to solve this problem under various modeling assumptions. Our approach is based on a robust finite-state abstraction of the stochastic dynamical model in the form of a Markov decision process with intervals of probabilities (iMDP). We use state-of-the-art verification techniques to compute an optimal policy on the iMDP with guarantees for satisfying the given specification. We then show that, by construction, we can refine this policy into a feedback controller for which these guarantees carry over to the dynamical model. In this short paper, we survey our recent research in this area and highlight two challenges (related to scalability and dealing with nonlinear dynamics) that we aim to address with our ongoing research.
</details>
<details>
<summary>摘要</summary>
自动生成正确性承诺控制器是自主系统的部署中关键的一步。这些自主系统通常是随机动力学模型的。通用问题是计算一个可以准确满足给定任务的控制器，该任务是 probabilistic temporal logic 规范。然而，因为随机不确定性、不精确知道参数以及混合特征，这个问题具有挑战性。我们已经开发了一个抽象框架，可以在不同的模型假设下解决这个问题。我们的方法基于一种可靠的finite-state抽象方法，即Markov decision process with intervals of probabilities（iMDP）。我们使用当前的验证技术来计算iMDP上的优质策略，并 garantía para satisfacer给定规范。然后，我们表明，通过构建，我们可以从这种策略中提取一个反馈控制器，这些 garantías会传递到动力学模型中。在这篇短文中，我们回顾了我们近期在这个领域的研究，并提出了两个挑战（相关于扩展性和处理非线性动力学），我们计划通过我们的进行研究来解决这些挑战。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generation-of-Scenarios-for-System-level-Simulation-based-Verification-of-Autonomous-Driving-Systems"><a href="#Automatic-Generation-of-Scenarios-for-System-level-Simulation-based-Verification-of-Autonomous-Driving-Systems" class="headerlink" title="Automatic Generation of Scenarios for System-level Simulation-based Verification of Autonomous Driving Systems"></a>Automatic Generation of Scenarios for System-level Simulation-based Verification of Autonomous Driving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09784">http://arxiv.org/abs/2311.09784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srajan Goyal, Alberto Griggio, Jacob Kimblad, Stefano Tonetta<br>for:* The paper is written for the purpose of presenting a generic framework for system-level simulation-based verification and validation (V&amp;V) of autonomous driving systems (ADS) that employ AI components.methods:* The framework uses a simulation model of the system, an abstract model that describes symbolically the system behavior, and formal methods to generate scenarios and verify the simulation executions.* The approach leverages the CARLA driving simulator and its ScenarioRunner tool to create diverse and complex driving scenarios.results:* The paper describes the instantiation of the VIVAS framework for an ADS case study, and demonstrates the effectiveness of the approach in automatically generating scenarios for system-level simulation-based V&amp;V of an automated driving system using CARLA and ScenarioRunner.* The results show the potential of the approach as a powerful tool in the future of ADS V&amp;V methodologies.<details>
<summary>Abstract</summary>
With increasing complexity of Automated Driving Systems (ADS), ensuring their safety and reliability has become a critical challenge. The Verification and Validation (V&V) of these systems are particularly demanding when AI components are employed to implement perception and/or control functions. In ESA-funded project VIVAS, we developed a generic framework for system-level simulation-based V&V of autonomous systems. The approach is based on a simulation model of the system, an abstract model that describes symbolically the system behavior, and formal methods to generate scenarios and verify the simulation executions. Various coverage criteria can be defined to guide the automated generation of the scenarios.   In this paper, we describe the instantiation of the VIVAS framework for an ADS case study. This is based on the integration of CARLA, a widely-used driving simulator, and its ScenarioRunner tool, which enables the creation of diverse and complex driving scenarios. This is also used in the CARLA Autonomous Driving Challenge to validate different ADS agents for perception and control based on AI, shared by the CARLA community. We describe the development of an abstract ADS model and the formulation of a coverage criterion that focuses on the behaviors of vehicles relative to the vehicle with ADS under verification. Leveraging the VIVAS framework, we generate and execute various driving scenarios, thus testing the capabilities of the AI components. The results show the effectiveness of VIVAS in automatically generating scenarios for system-level simulation-based V&V of an automated driving system using CARLA and ScenarioRunner. Therefore, they highlight the potential of the approach as a powerful tool in the future of ADS V&V methodologies.
</details>
<details>
<summary>摘要</summary>
随着自动驾驶系统（ADS）的复杂度增加，确保其安全性和可靠性已成为一项杰匡的挑战。验证和验议（V&V）这些系统特别是当AI组件用于感知和/或控制功能时，变得非常具有挑战性。在欧洲空间局（ESA）资助的项目VIVAS中，我们开发了一种通用框架 для系统级别的模拟基于验证和验议自动驾驶系统。该方法基于系统模型、一个抽象模型，用于 символи地描述系统行为，以及正式方法来生成场景和验证模拟执行。可以定义多种覆盖度标准来引导自动生成场景。  在这篇论文中，我们描述了VIvas框架在自动驾驶系统case study中的实现。这基于卡拉拉（CARLA）广泛使用的驾驶模拟器和其ScenarioRunner工具，可以创造多样化和复杂的驾驶场景。这也是在CARLA自动驾驶挑战中 validate不同的ADS代理人以及感知和控制基于AI的不同ADS代理人。我们开发了抽象的ADS模型，并将其与驾驶车辆相对于具有ADS的车辆的行为相关的覆盖度标准定义。通过VIvas框架，我们生成并执行多种驾驶场景，因此测试了AI组件的能力。结果显示VIvas在使用CARLA和ScenarioRunner进行系统级别的模拟基于验证和验议中自动生成场景的能力是非常有力的。因此，它高亮了未来ADS验证和验议方法的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Contamination-in-Modern-Benchmarks-for-Large-Language-Models"><a href="#Investigating-Data-Contamination-in-Modern-Benchmarks-for-Large-Language-Models" class="headerlink" title="Investigating Data Contamination in Modern Benchmarks for Large Language Models"></a>Investigating Data Contamination in Modern Benchmarks for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09783">http://arxiv.org/abs/2311.09783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan</li>
<li>for: 提高LLM的评估标准和评估方法的可靠性</li>
<li>methods: 提议两种适用于开源和专有模型的数据污染检测方法，包括检索系统和TS推测法</li>
<li>results: 研究发现一些商业LLM可以很准确地猜测测试集中的缺失选项，并且在一些标准套件中发现了模型的表现改善。<details>
<summary>Abstract</summary>
Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named \textbf{T}estset \textbf{S}lot Guessing (\textit{TS-Guessing}), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the TruthfulQA benchmark, we find that LLMs exhibit notable performance improvement when provided with additional metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.
</details>
<details>
<summary>摘要</summary>
近期观察发现了 LLM 的评估标准和实际性能之间的差距，引发了评估标准污染的 Concerns 。这个问题尤其对于关闭源模型和某些开源模型来说是 kritisch 。在这篇论文中，我们研究了数据污染问题，并提出了两种适用于开源和专有 LLM 的方法。我们首先介绍了一个检索基于系统，用于探索评估标准和预训练 corpora 之间的可能的重叠。然后，我们介绍了一种新的调查协议，名为 \textbf{T}estset \textbf{S}lot Guessing (\textit{TS-Guessing）}, 适用于开源和专有模型。这种方法包括在多选问题中隐藏错误答案，并让模型填充差距。此外，还包括隐藏评估示例中不可能的单词，并让模型生成它。我们发现了一些商业 LLM 可以意外地猜测测试集中的缺失选项。例如，在 TruthfulQA  benchmark 中，我们发现了 LLM 在提供额外metadata 时表现出 Notable 的性能提升。此外，在 MMLU  benchmark 中， ChatGPT 和 GPT-4 在测试集中猜测缺失选项的精准率分别为 52% 和 57%。我们希望这些结果可以强调评估方法和标准的需要更加Robust 。
</details></li>
</ul>
<hr>
<h2 id="Model-Checking-for-Closed-Loop-Robot-Reactive-Planning"><a href="#Model-Checking-for-Closed-Loop-Robot-Reactive-Planning" class="headerlink" title="Model Checking for Closed-Loop Robot Reactive Planning"></a>Model Checking for Closed-Loop Robot Reactive Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09780">http://arxiv.org/abs/2311.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Chandler, Bernd Porr, Alice Miller, Giulia Lafratta</li>
<li>for: 本研究用模型检查来创建多步计划，使Diffuse Drive驱动轮胎自动车可以避免危险。</li>
<li>methods: 我们使用一种小型、专门设计的模型检查算法，在实时环境中生成计划，并且这种方法具有 Egocentric 反应性的简单生物体特征。我们的方法基于链接临时控制系统，通过消除环境中的干扰来让自动车保持其首选行为或休眠状态。我们使用2D LiDAR数据的精细化方法，敏感于环境中的 bounded 随机变化。我们使用深度优先搜索来实现多步规划，并通过cul-de-sac 场景作为第一个测试用例。</li>
<li>results: 我们的结果表明，模型检查可以用来规划效率的轨迹，超越单步规划的表现。我们在实时使用无预计算数据来实现这一点。虽然我们的方法有一些局限性，但我们认为我们的方法具有开发安全、可靠和透明的轨迹规划方法的潜力。<details>
<summary>Abstract</summary>
In this paper, we show how model checking can be used to create multi-step plans for a differential drive wheeled robot so that it can avoid immediate danger. Using a small, purpose built model checking algorithm in situ we generate plans in real-time in a way that reflects the egocentric reactive response of simple biological agents. Our approach is based on chaining temporary control systems which are spawned to eliminate disturbances in the local environment that disrupt an autonomous agent from its preferred action (or resting state). The method involves a novel discretization of 2D LiDAR data which is sensitive to bounded stochastic variations in the immediate environment. We operationalise multi-step planning using invariant checking by forward depth-first search, using a cul-de-sac scenario as a first test case. Our results demonstrate that model checking can be used to plan efficient trajectories for local obstacle avoidance, improving on the performance of a reactive agent which can only plan one step. We achieve this in near real-time using no pre-computed data. While our method has limitations, we believe our approach shows promise as an avenue for the development of safe, reliable and transparent trajectory planning in the context of autonomous vehicles.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了如何使用模型检查来创建多步计划，以使Diffusion Drive轮胎自动车避免 immediate danger。我们使用一种小型、特有的模型检查算法在实时中生成计划，以模仿简单生物体的 Egocentric 反应。我们的方法基于临时控制系统的链接，以消除环境中的干扰，使自动 Agent 能够继续进行其 preferred action（或休眠状态）。我们的方法包括一种新的2D LiDAR数据的精度化，敏感于环境中的 bounded 随机变化。我们通过在深度优先搜索中进行 invariants 检查来实现多步规划，并使用 cul-de-sac 场景作为第一个测试 caso。我们的结果表明，模型检查可以用于计划高效的轨迹，超越了只能计划一步的感知Agent。我们在实时中使用不需要预计算数据来实现这一点。虽然我们的方法有限制，但我们认为我们的方法显示出了在自动汽车中安全、可靠和透明的轨迹规划的可能性。
</details></li>
</ul>
<hr>
<h2 id="HuatuoGPT-II-One-stage-Training-for-Medical-Adaption-of-LLMs"><a href="#HuatuoGPT-II-One-stage-Training-for-Medical-Adaption-of-LLMs" class="headerlink" title="HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"></a>HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09774">http://arxiv.org/abs/2311.09774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, Benyou Wang</li>
<li>for: 这个论文的目的是适应具有特殊知识的领域，如医学，以便将特殊知识 integrate into一个通用语言模型中，如Llama2。</li>
<li>methods: 该论文提出了一种将多种数据集，包括预训练和指导训练，转化为一个统一的输入输出对形式，以简化学习协议。</li>
<li>results: 该论文通过对多个医学领域的测试，证明了其在中医领域的状态对级表现，并在一些方面超过了现有的专有模型，如ChatGPT和GPT-4。专家手动评估也证明了该模型的优势。<details>
<summary>Abstract</summary>
Adapting a language model into a specific domain, a.k.a `domain adaption', is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. The challenge lies in the heterogeneity of data across the two training stages, as it varies in languages, genres, or formats. To tackle this and simplify the learning protocol, we propose to transform heterogeneous data, from the both pre-training and supervised stages, into a unified, simple input-output pair format. We validate the new protocol in the domains where proprietary LLMs like ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The developed model, HuatuoGPT-II, has shown state-of-the-art performance in Chinese medicine domain on a number of benchmarks, e.g. medical licensing exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some aspects, especially in Traditional Chinese Medicine. Expert manual evaluations further validate HuatuoGPT-II's advantages over existing LLMs. Notably, HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing Examination where it achieved the best performance, showcasing not only its effectiveness but also its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
适应特定领域的语言模型化，即域 adaptation，是一种常见的做法，当特殊知识，如医学，不包含在通用语言模型如LLAMA2中。挑战在两个训练阶段数据的异ogeneity上，因为数据的语言、种类、格式都不同。为了解决这个问题并简化学习协议，我们提议将各种不同数据，从预训练和监督两个阶段，转换成一个统一、简单的输入输出对 format。我们验证了新协议的效果在各种领域，如中医，并在一些标准测试任务上达到了状态 искусственный智能表现。特别是在传统中医领域，我们的模型 HuatuoGPT-II 表现出了优秀的result，并在一些方面超越了商业化模型，如ChatGPT和GPT-4。专业人员手动评估也证明了 HuatuoGPT-II 的优势。值得一提的是，HuatuoGPT-II 在新的中医国家医籍考试中达到了最佳表现，证明了它不仅有效，还有普适化能力。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-A-Simple-Recipe-for-Improving-Out-of-Domain-Retrieval-in-Dense-Encoders"><a href="#Back-to-Basics-A-Simple-Recipe-for-Improving-Out-of-Domain-Retrieval-in-Dense-Encoders" class="headerlink" title="Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders"></a>Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09765">http://arxiv.org/abs/2311.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amy-hyunji/lora-for-retrieval">https://github.com/amy-hyunji/lora-for-retrieval</a></li>
<li>paper_authors: Hyunji Lee, Luca Soldaini, Arman Cohan, Minjoon Seo, Kyle Lo</li>
<li>for: 这个论文主要是为了提高 dense retriever 模型在未经见过的领域中的泛化能力。</li>
<li>methods: 该论文提出了一种简单的训练方法，包括使用 LoRA 等参数效率的方法，并选择使用 batch 内的负样本，除非给出了准确制定的困难负样本。</li>
<li>results: 该论文使用 BEIR  benchmark 进行验证，并发现这些建议可以在不同的 dense encoder 和基础模型大小上 persist，并且与其他资源密集的策略（如建筑修改或多个预训练）相结合，可以提高 dense retriever 模型的泛化能力。<details>
<summary>Abstract</summary>
Prevailing research practice today often relies on training dense retrievers on existing large datasets such as MSMARCO and then experimenting with ways to improve zero-shot generalization capabilities to unseen domains. While prior work has tackled this challenge through resource-intensive steps such as data augmentation, architectural modifications, increasing model size, or even further base model pretraining, comparatively little investigation has examined whether the training procedures themselves can be improved to yield better generalization capabilities in the resulting models. In this work, we recommend a simple recipe for training dense encoders: Train on MSMARCO with parameter-efficient methods, such as LoRA, and opt for using in-batch negatives unless given well-constructed hard negatives. We validate these recommendations using the BEIR benchmark and find results are persistent across choice of dense encoder and base model size and are complementary to other resource-intensive strategies for out-of-domain generalization such as architectural modifications or additional pretraining. We hope that this thorough and impartial study around various training techniques, which augments other resource-intensive methods, offers practical insights for developing a dense retrieval model that effectively generalizes, even when trained on a single dataset.
</details>
<details>
<summary>摘要</summary>
现有研究往往采用 dense retriever 训练大量数据集 MSMARCO，然后尝试改进零shot泛化能力。而在优化这种泛化能力方面，已有许多研究，包括数据增强、结构修改、模型大小增加和额外预训练。然而，对于训练过程本身是否可以进行优化以实现更好的泛化能力，尚未得到了充分的探讨。在这项工作中，我们建议一种简单的 dense encoder 训练热键：通过 LoRA 等参数效率的方法在 MSMARCO 上训练，并在批处中使用卷积批处。我们使用 BEIR 测试准则 validate 这些建议，并发现结果是不依赖于 dense encoder 和基础模型大小，并且与其他资源占用量大的方法相结合，可以实现更好的泛化能力。我们希望这种具有充分的探讨和不偏袋的研究，能够为开发高效泛化 dense retrieval 模型提供实际的指导。
</details></li>
</ul>
<hr>
<h2 id="Graph-Guided-Reasoning-for-Multi-Hop-Question-Answering-in-Large-Language-Models"><a href="#Graph-Guided-Reasoning-for-Multi-Hop-Question-Answering-in-Large-Language-Models" class="headerlink" title="Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models"></a>Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09762">http://arxiv.org/abs/2311.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyoung Park, Ameen Patel, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim</li>
<li>For: The paper aims to improve the multi-step reasoning capabilities of large language models (LLMs) by addressing two issues in previous CoT prompting methods: generating irrelevant rationales and failing to compose subquestions or queries for obtaining all relevant information.* Methods: The proposed graph-guided CoT prompting method uses a “question&#x2F;rationale graph” constructed by LLMs to guide the reasoning process. The method includes graph representation and verification steps to filter out irrelevant rationales and generate follow-up questions to obtain relevant information.* Results: The proposed method shows superior performance compared to previous CoT prompting methods and their variants on multi-hop question answering benchmark datasets.<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning capabilities of Large Language Models (LLMs) by generating a series of rationales before the final answer. We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning: (i) Generating rationales irrelevant to the question, (ii) Unable to compose subquestions or queries for generating/retrieving all the relevant information. To address them, we propose a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps. Specifically, we first leverage LLMs to construct a "question/rationale graph" by using knowledge extraction prompting given the initial question and the rationales generated in the previous steps. Then, the graph verification step diagnoses the current rationale triplet by comparing it with the existing question/rationale graph to filter out irrelevant rationales and generate follow-up questions to obtain relevant information. Additionally, we generate CoT paths that exclude the extracted graph information to represent the context information missed from the graph extraction. Our graph-guided reasoning method shows superior performance compared to previous CoT prompting and the variants on multi-hop question answering benchmark datasets.
</details>
<details>
<summary>摘要</summary>
chain-of-thought (CoT) 提示法已经提高了大语言模型 (LLM) 的多步逻辑能力，通过生成一系列的理由来提高答案。我们分析了 CoT 生成的逻辑路径，并发现了两个多步逻辑问题：（i）生成与问题无关的理由，（ii）无法组合子问题或查询来获取所有相关信息。为解决这些问题，我们提议一种图表引导 CoT 提示法，使 LLM 能够通过图表表示/验证步骤达到正确答案。 Specifically，我们首先利用 LLM 使用知识EXTRACTION 提示生成一个 "问题/理由图"，并then 使用图表验证步骤来诊断当前的理由 triplet，并将无关的理由过滤掉，生成跟进 вопро题以获取相关信息。此外，我们还生成 CoT 路径，排除扩展的图信息以表示Context missed 信息。我们的图表引导逻辑方法在多个多步问答 benchmark 数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="MAFALDA-A-Benchmark-and-Comprehensive-Study-of-Fallacy-Detection-and-Classification"><a href="#MAFALDA-A-Benchmark-and-Comprehensive-Study-of-Fallacy-Detection-and-Classification" class="headerlink" title="MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification"></a>MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09761">http://arxiv.org/abs/2311.09761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chadi Helwe, Tom Calamai, Pierre-Henri Paris, Chloé Clavel, Fabian Suchanek</li>
<li>For: The paper aims to address the challenges of automated fallacy detection and classification, particularly the subjectivity of the task and the need for a comprehensive and unified approach in existing research.* Methods: The paper introduces a novel taxonomy of fallacies that refines and aligns previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity.* Results: The paper introduces MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset based on examples from various previously existing fallacy datasets under the unified taxonomy, and evaluates several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. The evaluation provides valuable insights into the strengths and limitations of these models in addressing fallacious reasoning.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是解决自动推理错误检测和分类的挑战，特别是任务的主观性和现有研究中存在的缺乏一致性。* Methods: 论文引入了一种新的论点分类方法，该方法可以协调和融合先前的分类方法，同时还提供了一种适用于主观NLPTask的新注释方案。* Results: 论文引入了一个金标准数据集MAFALDA（多级注释错误数据集），该数据集基于先前的许多错误数据集的一致性，并对多种语言模型进行零基础学习 Setting中的评价。<details>
<summary>Abstract</summary>
Fallacies can be used to spread disinformation, fake news, and propaganda, underlining the importance of their detection. Automated detection and classification of fallacies, however, remain challenging, mainly because of the innate subjectivity of the task and the need for a comprehensive, unified approach in existing research. Addressing these limitations, our study introduces a novel taxonomy of fallacies that aligns and refines previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity, adapted to precision, recall, and F1-Score metrics. Using our annotation scheme, the paper introduces MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset. MAFALDA is based on examples from various previously existing fallacy datasets under our unified taxonomy across three levels of granularity. We then evaluate several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. Our comprehensive evaluation not only benchmarks the performance of these models but also provides valuable insights into their strengths and limitations in addressing fallacious reasoning.
</details>
<details>
<summary>摘要</summary>
False information, fake news, and propaganda can be spread through fallacies, highlighting the importance of detecting them. However, automated detection and classification of fallacies are challenging due to the subjective nature of the task and the lack of a comprehensive, unified approach in existing research. To address these limitations, our study proposes a new taxonomy of fallacies that aligns and refines previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity, adapted to precision, recall, and F1-Score metrics. Using our annotation scheme, we introduce MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset based on examples from various previously existing fallacy datasets under our unified taxonomy across three levels of granularity. We then evaluate several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. Our comprehensive evaluation not only benchmarks the performance of these models but also provides valuable insights into their strengths and limitations in addressing fallacious reasoning.
</details></li>
</ul>
<hr>
<h2 id="UFPS-A-unified-framework-for-partially-annotated-federated-segmentation-in-heterogeneous-data-distribution"><a href="#UFPS-A-unified-framework-for-partially-annotated-federated-segmentation-in-heterogeneous-data-distribution" class="headerlink" title="UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution"></a>UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09757">http://arxiv.org/abs/2311.09757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tekap404/unified_federated_partially-labeled_segmentation">https://github.com/tekap404/unified_federated_partially-labeled_segmentation</a></li>
<li>paper_authors: Le Jiang, Li Yan Ma, Tie Yong Zeng, Shi Hui Ying</li>
<li>for: 这个研究是为了解决基于半自动化标签的医疗影像分类问题，并且不会遗漏数据的隐私问题。</li>
<li>methods: 这个研究使用了联邦式半自动化分类（FPSS），并且提出了一个统一的全球模型训练方法（UFPS），以解决半自动化分类中的分类异常和客户端漂移问题。</li>
<li>results: 这个研究的实验结果显示，UFPS方法能够更好地解决半自动化分类中的分类异常和客户端漂移问题，并且在实际医疗影像数据上显示了更好的适应和普遍性。<details>
<summary>Abstract</summary>
Partially supervised segmentation is a label-saving method based on datasets with fractional classes labeled and intersectant. However, it is still far from landing on real-world medical applications due to privacy concerns and data heterogeneity. As a remedy without privacy leakage, federated partially supervised segmentation (FPSS) is formulated in this work. The main challenges for FPSS are class heterogeneity and client drift. We propose a Unified Federated Partially-labeled Segmentation (UFPS) framework to segment pixels within all classes for partially-annotated datasets by training a totipotential global model without class collision. Our framework includes Unified Label Learning and sparsed Unified Sharpness Aware Minimization for unification of class and feature space, respectively. We find that vanilla combinations for traditional methods in partially supervised segmentation and federated learning are mainly hampered by class collision through empirical study. Our comprehensive experiments on real medical datasets demonstrate better deconflicting and generalization ability of UFPS compared with modified methods.
</details>
<details>
<summary>摘要</summary>
partially supervised segmentation是一种基于分类数据集的标签保存方法，但它还远离实际医疗应用中的应用，主要原因是隐私问题和数据不一致。为解决这些问题，本文提出了联邦半supervised分割（FPSS）方法。FPSS的主要挑战是分类异ogeneous和客户端漂移。我们提出了一种总统的联邦半supervised分割（UFPS）框架，用于对半标注数据集中的像素进行分割，不会出现分类冲突。我们的框架包括统一标签学习和粗粒化的统一锐度感知优化，用于统一类和特征空间。我们通过实际研究发现，传统的partially supervised segmentation和联邦学习方法的组合在面临分类冲突的情况下效果较差。我们的全面实验表明，UFPS在实际医疗数据集上具有更好的冲突解决和泛化能力，与修改后的方法相比。
</details></li>
</ul>
<hr>
<h2 id="Redefining-the-Laparoscopic-Spatial-Sense-AI-based-Intra-and-Postoperative-Measurement-from-Stereoimages"><a href="#Redefining-the-Laparoscopic-Spatial-Sense-AI-based-Intra-and-Postoperative-Measurement-from-Stereoimages" class="headerlink" title="Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages"></a>Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09744">http://arxiv.org/abs/2311.09744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopoldmueller/laparoscopicmeasurement">https://github.com/leopoldmueller/laparoscopicmeasurement</a></li>
<li>paper_authors: Leopold Müller, Patrick Hemmer, Moritz Queisner, Igor Sauer, Simeon Allmendinger, Johannes Jakubik, Michael Vössing, Niklas Kühl<br>for: This paper aims to provide a more accurate and efficient solution for image-guided surgery, specifically for measuring relevant structures such as vessel segments, resection margins, and bowel lengths.methods: The proposed method utilizes stereo vision and state-of-the-art machine learning architectures, including RAFT-Stereo and YOLOv8, to achieve high accuracy in distance measurements with errors below 1 mm.results: The developed method is assessed in various realistic experimental evaluation environments and demonstrates robustness in challenging environments with textureless regions. The results outline the potential of the method for providing more precise, safe, and efficient surgical procedures.<details>
<summary>Abstract</summary>
A significant challenge in image-guided surgery is the accurate measurement task of relevant structures such as vessel segments, resection margins, or bowel lengths. While this task is an essential component of many surgeries, it involves substantial human effort and is prone to inaccuracies. In this paper, we develop a novel human-AI-based method for laparoscopic measurements utilizing stereo vision that has been guided by practicing surgeons. Based on a holistic qualitative requirements analysis, this work proposes a comprehensive measurement method, which comprises state-of-the-art machine learning architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed in various realistic experimental evaluation environments. Our results outline the potential of our method achieving high accuracies in distance measurements with errors below 1 mm. Furthermore, on-surface measurements demonstrate robustness when applied in challenging environments with textureless regions. Overall, by addressing the inherent challenges of image-guided surgery, we lay the foundation for a more robust and accurate solution for intra- and postoperative measurements, enabling more precise, safe, and efficient surgical procedures.
</details>
<details>
<summary>摘要</summary>
significante挑战在图像引导手术中是准确测量有关结构，如血管段、切除边缘或肠长度。这项任务是许多手术中不可或缺的一部分，但它具有较大的人工劳动量和不准确率。在这篇论文中，我们开发了一种新的人类-AI基于方法，用于肠 Laparoscopic 测量，利用推导导航的STereo视力。基于全面的quality要求分析，这项工作提出了一种全面的测量方法，包括当前的机器学习架构，如RAFT-Stereo和YOLOv8。我们开发的方法在不同的实际试验环境中进行了评估。我们的结果表明，我们的方法可以实现高精度的距离测量，错误在1毫米以下。此外，在表面上进行的测量也能够在粗糙区域中展示 robustness。总之，我们通过解决图像引导手术中的内在挑战，为更加精确、安全和有效的手术过程奠定了基础。
</details></li>
</ul>
<hr>
<h2 id="Redefining-Super-Resolution-Fine-mesh-PDE-predictions-without-classical-simulations"><a href="#Redefining-Super-Resolution-Fine-mesh-PDE-predictions-without-classical-simulations" class="headerlink" title="Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations"></a>Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09740">http://arxiv.org/abs/2311.09740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajat Kumar Sarkar, Ritam Majumdar, Vishal Jadhav, Sagar Srinivas Sakhinana, Venkataramana Runkana</li>
<li>for: 用于提高计算流体力学（CFD）中粗粒度 simulations 的精度，并且可以避免传统的超分辨率方法中的约束。</li>
<li>methods: 我们提出了一种新的超分辨率定义，将 coarse-grid 数据作为输入，预测 fine-grid 数据。使用 physics-infused UNet 上升方法，并在多种2D-CFD问题中进行了证明，包括 Burger 方程中的缺陷检测、甲烷燃烧和工业热交换器中的沾吸。</li>
<li>results: 我们的方法可以生成精度高的 fine-mesh 解决方案，不需要传统的计算，同时保持了原始真实情况的精度。通过在训练过程中使用多种边界条件，我们还证明了我们的方法的稳定性，这将推动其广泛应用于工程和科学 CFD 解决方案中。<details>
<summary>Abstract</summary>
In Computational Fluid Dynamics (CFD), coarse mesh simulations offer computational efficiency but often lack precision. Applying conventional super-resolution to these simulations poses a significant challenge due to the fundamental contrast between downsampling high-resolution images and authentically emulating low-resolution physics. The former method conserves more of the underlying physics, surpassing the usual constraints of real-world scenarios. We propose a novel definition of super-resolution tailored for PDE-based problems. Instead of simply downsampling from a high-resolution dataset, we use coarse-grid simulated data as our input and predict fine-grid simulated outcomes. Employing a physics-infused UNet upscaling method, we demonstrate its efficacy across various 2D-CFD problems such as discontinuity detection in Burger's equation, Methane combustion, and fouling in Industrial heat exchangers. Our method enables the generation of fine-mesh solutions bypassing traditional simulation, ensuring considerable computational saving and fidelity to the original ground truth outcomes. Through diverse boundary conditions during training, we further establish the robustness of our method, paving the way for its broad applications in engineering and scientific CFD solvers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Source-Prompt-Coordinated-Pre-training-of-Language-Models-on-Diverse-Corpora-from-Multiple-Sources"><a href="#Source-Prompt-Coordinated-Pre-training-of-Language-Models-on-Diverse-Corpora-from-Multiple-Sources" class="headerlink" title="Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources"></a>Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09732">http://arxiv.org/abs/2311.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yipei Xu, Dakuan Lu, Jiaqing Liang, Xintao Wang, Yipeng Geng, Yingsi Xin, Hengkui Wu, Ken Chen, ruiji zhang, Yanghua Xiao</li>
<li>for: 这 paper 是研究 Pre-trained language models (PLMs) 的新 paradigm 在 NLP 领域的一部分，以及如何更好地准备这些模型和它们的预训练 corpora。</li>
<li>methods: 这 paper 使用了一种常见和成功的方法，即不断扩大模型和预训练 corpora 的大小。这些大 corpora 通常是从多个源 converges 而来的，因此变得越来越多样化。然而，这些巨大的 converged corpora 的侧effect 尚未得到了充分的研究。</li>
<li>results: 在这 paper 中， authors 发现了各种 corpora 的多样性会对预训练 PLMs 的性能产生负面影响。为了协调预训练在多种 corpora 上，authors 提出了源提示 (SP)，这是一种在预训练和精度调整阶段显式地提示模型数据源的技术。Results 表明，使用 SP 在多种 corpora 上预训练 PLMs 可以获得显著的下游任务提升。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have established the new paradigm in the field of NLP. For more powerful PLMs, one of the most popular and successful way is to continuously scale up sizes of the models and the pre-training corpora. These large corpora are generally obtained by converging smaller ones from multiple sources, they are thus growing increasingly diverse. However, the side-effects of these colossal converged corpora remain understudied. In this paper, we identify the disadvantage of heterogeneous corpora from multiple sources for pre-training PLMs. Towards coordinated pre-training on diverse corpora, we further propose source prompts (SP), which explicitly prompt the model of the data source at the pre-training and fine-tuning stages. Results of extensive experiments demonstrate that PLMs pre-trained with SP on diverse corpora gain significant improvement in various downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prudent-Silence-or-Foolish-Babble-Examining-Large-Language-Models’-Responses-to-the-Unknown"><a href="#Prudent-Silence-or-Foolish-Babble-Examining-Large-Language-Models’-Responses-to-the-Unknown" class="headerlink" title="Prudent Silence or Foolish Babble? Examining Large Language Models’ Responses to the Unknown"></a>Prudent Silence or Foolish Babble? Examining Large Language Models’ Responses to the Unknown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09731">http://arxiv.org/abs/2311.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Genglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, Hao Peng</li>
<li>for: 本研究旨在系统地研究LLMs在缺乏必要知识的情况下的行为，以及这种行为如何与人类对话规范不符。</li>
<li>methods: 本研究使用了一个反 adversarial question-answering benchmark，该 benchmark 包含了 LLMS 缺乏训练数据的情况下的问题。</li>
<li>results: 研究发现，通过 instrucion finetuning 和人类反馈学习（RLHF），LLMs 可以更好地表达uncertainty，并且与有效的问题相对应，表现出更高的准确率和自信度。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) often struggle when faced with situations where they lack the prerequisite knowledge to generate a sensical response. In these cases, models tend to fabricate and hallucinate, rather than appropriately signaling uncertainty as humans would. This behavior misaligns with human conversational norms and presents challenges surrounding responsible and ethical AI development. This work aims to systematically investigate LLMs' behaviors in such situations. We curate an adversarial question-answering benchmark containing unanswerable questions targeting information absent from the LLM's training data. Concretely, these unanswerable questions contain non-existent concepts or false premises. When presented with such unanswerable questions, an LLM should appropriately convey uncertainty, and be able to challenge the premise and refuse to generate a response. While facing answerable valid questions, a model should demonstrate a positive correlation between accuracy and confidence. Using a model-agnostic unified confidence elicitation approach, we observe that LLMs that have gone through instruction finetuning and reinforcement learning from human feedback (RLHF) perform significantly better than their counterparts that do not. Moreover, uncertainty expression 1 through our elicitation method does not always stay consistent with the perceived confidence of the direct response of an LLM. Our findings call for further research into teaching LLMs to proactively and reliably express uncertainty.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Aligning-with-Whom-Large-Language-Models-Have-Gender-and-Racial-Biases-in-Subjective-NLP-Tasks"><a href="#Aligning-with-Whom-Large-Language-Models-Have-Gender-and-Racial-Biases-in-Subjective-NLP-Tasks" class="headerlink" title="Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks"></a>Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09730">http://arxiv.org/abs/2311.09730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-pei/llm-group-bias">https://github.com/jiaxin-pei/llm-group-bias</a></li>
<li>paper_authors: Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens</li>
<li>for: 这个研究探讨了大语言模型（LLM）在主观NLP任务上是否具有性别和民族特征的偏见。</li>
<li>methods: 研究使用了POPQUORN数据集，对四种流行的LLM进行了一系列实验，检查它们在政eness和不礼貌任务上是否具有偏见。</li>
<li>results: 研究发现，对于这两个任务，模型的预测结果更接近白人和女性参与者的标签。进一步的探讨发现，在使用目标民族和性别标签作为提示时，模型的性能会下降。 Code和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Jiaxin-Pei/LLM-Group-Bias%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Jiaxin-Pei/LLM-Group-Bias上获取。</a><details>
<summary>Abstract</summary>
Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of "Black" and "Asian" individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</details>
<details>
<summary>摘要</summary>
人类对语言的理解受个人背景的影响，如性别和民族。 although existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, we leverage the POPQUORN dataset, which contains annotations of diverse demographic backgrounds, to investigate the capability of four popular LLMs to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. Specifically, when prompted to respond from the perspective of "Black" and "Asian" individuals, models show lower performance in predicting both overall scores and the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks, and that demographic-infused prompts alone may be insufficient to mitigate such effects. 可以在 GitHub 上获取代码和数据：https://github.com/Jiaxin-Pei/LLM-Group-Bias。
</details></li>
</ul>
<hr>
<h2 id="Outcome-supervised-Verifiers-for-Planning-in-Mathematical-Reasoning"><a href="#Outcome-supervised-Verifiers-for-Planning-in-Mathematical-Reasoning" class="headerlink" title="Outcome-supervised Verifiers for Planning in Mathematical Reasoning"></a>Outcome-supervised Verifiers for Planning in Mathematical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09724">http://arxiv.org/abs/2311.09724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yu, Anningzhe Gao, Benyou Wang<br>for: 这种研究旨在解决大型语言模型（LLM）在数学逻辑推理中维持准确性的问题， LLMs 经常在推理过程中出现错误，导致最终结果不准确。methods: 该研究提出了一种新的评估模型——结果监督价值模型（OVM），通过结果监督来训练，从而提高了模型的准确性。 OVM 不需要劳动密集的步骤级别正确性注释，从而提高了其可扩展性。results: 在两个多步数学逻辑数据集上进行了实验，结果显示 OVM 模型在 GSM8K 数据集上 achievement 状态的最佳结果，而不需要使用 GPT-4 或代码执行。 这些发现提供了一种新的视角，即在训练评估模型时，结果监督可以提高模型的价值估计，并且有理论基础。<details>
<summary>Abstract</summary>
Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem. Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for \textit{planning} by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）经常在进行多步骤推理时维持正确性问题，导致错误传递，干扰最终结果。现有的方法来解决这问题主要是使用验证模型来评估生成的解答候选者，专注于全局推理路径或部分推理路径。我们认为评估未完成推理路径的潜力可能更有利，将任务转换为规划问题。我们提出的验证器是结果监督值模型（OVM），通过结果监督进行训练，提供一种高效和直观的规划方法，优先级是导向正确的结论的步骤。此外，OVM不需要耗费劳动的步骤正确性标注，提高其扩展性。我们在GSM8K和Game of 24两个多步骤推理数据集上进行实验，结果显示OVM模型在LLM中表现出色，特别是OVM-7B模型在GSM8K数据集上实现了状态顶尖结果，而不使用GPT-4或代码执行。这些结果提供一个新的见解，强调结果监督在训练验证器 для多步骤推理任务中的role，并提供了值估计的理论基础。
</details></li>
</ul>
<hr>
<h2 id="You-don’t-need-a-personality-test-to-know-these-models-are-unreliable-Assessing-the-Reliability-of-Large-Language-Models-on-Psychometric-Instruments"><a href="#You-don’t-need-a-personality-test-to-know-these-models-are-unreliable-Assessing-the-Reliability-of-Large-Language-Models-on-Psychometric-Instruments" class="headerlink" title="You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments"></a>You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09718">http://arxiv.org/abs/2311.09718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orange0629/llm-personas">https://github.com/orange0629/llm-personas</a></li>
<li>paper_authors: Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Dallas Card, David Jurgens</li>
<li>for: 本研究旨在检验当前问题提示的格式是否能够具备一致和可靠的回答能力。</li>
<li>methods: 研究者首先构建了39种人格测试工具的693个问题集，然后设计了一些细微变化的提示，以检验LLM的回答准确性和一致性。</li>
<li>results: 实验发现，即使使用简单的变换，LLM的回答能力也会受到很大的下降，而大多数LLM的否定一致性也很低。这些结果表明，当前的问题提示方式不够准确地捕捉模型的感知，并讨论了可能的更好的 alternatives。<details>
<summary>Abstract</summary>
The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. In particular, to properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs of particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting enables LLMs to provide responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLM's capabilities to generate accurate answers, as well as consistency variations to examine their consistency towards simple perturbations such as switching the option order. Our experiments on 15 different open-source LLMs reveal that even simple perturbations are sufficient to significantly downgrade a model's question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately capture model perceptions, and we discuss potential alternatives to improve such issues.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在自然语言理解任务上的多样性，使得它们在社会科学研究中受欢迎。特别是，为了准确理解LLM的性质和内在人格，研究人员已经使用了问题提示的形式进行研究。在这项研究中，我们做出了一个谨慎的步骤，检查当前的提示格式是否能够让LLM提供一致和稳定的回答。我们首先构建了一个包含39种测量工具和115个人格轴的数据集。此外，我们设计了一些具有轻微变化的提示，检查LLM是否能够生成准确回答，以及对简单的变化（如选项顺序交换）的一致性。我们在15种开源LLM上进行了实验，发现，即使使用简单的变化，也可以导致模型的问题回答能力下降 significatively，并且大多数LLM具有低的否定一致性。我们的结果表明，当前广泛使用的提示方式不够用于准确捕捉模型的感知，我们讨论了可能的改进方案。
</details></li>
</ul>
<hr>
<h2 id="Towards-Autonomous-Hypothesis-Verification-via-Language-Models-with-Minimal-Guidance"><a href="#Towards-Autonomous-Hypothesis-Verification-via-Language-Models-with-Minimal-Guidance" class="headerlink" title="Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance"></a>Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09706">http://arxiv.org/abs/2311.09706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiro Takagi, Ryutaro Yamauchi, Wataru Kumagai</li>
<li>for: 本研究旨在检验AI是否可以自主生成和验证假设，以实现人类水平的自主研究。</li>
<li>methods: 本研究使用GPT-4生成假设和Python代码进行验证，仅提供有限的方法指导。</li>
<li>results: 研究发现，在某些情况下，GPT-4可以自主生成和验证假设，但无一个完美的验证结果，表明还有许多挑战需要继续探索，以实现自主研究。<details>
<summary>Abstract</summary>
Research automation efforts usually employ AI as a tool to automate specific tasks within the research process. To create an AI that truly conduct research themselves, it must independently generate hypotheses, design verification plans, and execute verification. Therefore, we investigated if an AI itself could autonomously generate and verify hypothesis for a toy machine learning research problem. We prompted GPT-4 to generate hypotheses and Python code for hypothesis verification with limited methodological guidance. Our findings suggest that, in some instances, GPT-4 can autonomously generate and validate hypotheses without detailed guidance. While this is a promising result, we also found that none of the verifications were flawless, and there remain significant challenges in achieving autonomous, human-level research using only generic instructions. These findings underscore the need for continued exploration to develop a general and autonomous AI researcher.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deceiving-Semantic-Shortcuts-on-Reasoning-Chains-How-Far-Can-Models-Go-without-Hallucination"><a href="#Deceiving-Semantic-Shortcuts-on-Reasoning-Chains-How-Far-Can-Models-Go-without-Hallucination" class="headerlink" title="Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?"></a>Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09702">http://arxiv.org/abs/2311.09702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen</li>
<li>for: 这个研究探讨了大型语言模型（LLM）在句子理解中存在的幻觉和不当思维现象，以及这些现象如何影响模型的表现。</li>
<li>methods: 该研究使用了一种新的探测方法和benchmark，称为EureQA，以量化这种现象。该方法从问题中找到可以得到答案的关键Entity，然后逐层添加证据句，让模型在找到答案之前必须按照链式的证据逻辑进行搜寻。</li>
<li>results: 研究发现，现有的LLM模型缺乏能够按照正确的逻辑链进行搜寻和理解句子的能力，而是倾向于通过幻觉和不当思维来简化问题。这些幻觉通常是基于语义关系的，并且会导致模型的偏差和幻觉。<details>
<summary>Abstract</summary>
Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor clues (evidence) that will not directly lead to the correct answer but require a chain-like reasoning process. We evaluate if models can follow the correct reasoning chain instead of short-cutting through distractor clues. We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.
</details>
<details>
<summary>摘要</summary>
尽管最近的大语言模型（LLM）在许多测试中表现出色，但最新的研究表明这些模型受到了幻觉和不正确的理解的影响。这项研究探讨了 LLM 受到 semantic association 引起的幻觉的特点。我们具体研究 LLM 是否因为提示中的关键词/实体偏见而快速缩短 reasoning 过程。为了衡量这种现象，我们提出了一种新的探测方法和标准 benchmark called EureQA。我们从可以通过 utmost certainty 回答 вопро题的问题开始，并在提示中逐层隐藏关键实体，要求模型通过证据链来找到隐藏的实体。在建构证据时，我们故意将可能导致正确答案的 semantic clue 替换为不直接导致正确答案的 distractor clue，以便要求模型遵循 chain-like 的 reasoning 过程。我们评估模型是否可以遵循正确的 reasoning 路径，而不是短ircuit 通过 distractor clue。我们发现现有的 LLM 缺乏遵循正确 reasoning 路径的能力，并且很容易受到幻觉的影响。我们显示，distractor semantic association frequently leads to model hallucination，这是强有力的证据，证明了现有 LLM 的reasoning 无效。
</details></li>
</ul>
<hr>
<h2 id="Accommodating-Missing-Modalities-in-Time-Continuous-Multimodal-Emotion-Recognition"><a href="#Accommodating-Missing-Modalities-in-Time-Continuous-Multimodal-Emotion-Recognition" class="headerlink" title="Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition"></a>Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10119">http://arxiv.org/abs/2311.10119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Vazquez-Rodriguez, Grégoire Lefebvre, Julien Cumin, James L. Crowley</li>
<li>for: 本研究旨在提高时间连续的情感识别精度，即使有一些模式缺失。</li>
<li>methods: 我们提议一种基于Transformer的新架构，通过跨模式关注和自关注机制来强调时间上的关系，以提高学习过程的稳定性和精度。</li>
<li>results: 实验结果表明，我们的模型在 Ulm-TSST 数据集上的评价协调系数评价提高了37% （对应于情感值预测）和30% （对应于情感评价），相比基线方法。<details>
<summary>Abstract</summary>
Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a time-continuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs. Experimental results on the Ulm-TSST dataset show that our model exhibits an improvement of the concordance correlation coefficient evaluation of 37% when predicting arousal values and 30% when predicting valence values, compared to a late-fusion baseline approach.
</details>
<details>
<summary>摘要</summary>
以下是文本的简化中文翻译：多种研究表明，情感识别更有效率地使用多种modalities。但如果某些modalities缺失呢？为解决这个问题，我们提出了一种基于Transformer架构的时间连续的情感识别模型，即使缺失输入modalities也能够准确地识别情感。我们通过跨modalities和自modalities的相互关注机制来强调时间和模式之间的关系，从而提高模型在弱烈输入上学习的能力。实验结果表明，我们的模型在 Ulm-TSST 数据集上的评价方法比基准方法提高了37% （对于情绪值预测）和30% （对于情感值预测）。
</details></li>
</ul>
<hr>
<h2 id="BLT-Can-Large-Language-Models-Handle-Basic-Legal-Text"><a href="#BLT-Can-Large-Language-Models-Handle-Basic-Legal-Text" class="headerlink" title="BLT: Can Large Language Models Handle Basic Legal Text?"></a>BLT: Can Large Language Models Handle Basic Legal Text?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09693">http://arxiv.org/abs/2311.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blairstanek/blt">https://github.com/blairstanek/blt</a></li>
<li>paper_authors: Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme</li>
<li>for: 法律执业中需要基础文本处理能力，现有的公共可用LLM like GPT-4和PaLM2表现不佳。</li>
<li>methods: 我们引入了一个 benchmark 来衡量这种 poor performance，这casts 疑问 LLMS 目前是否可靠 для legal practice。</li>
<li>results:  fine-tuning  older LLM 可以带来 near-perfect performance 在我们的测试集上，也提高了相关的法律任务的表现。这个结果强调了 LLMS 训练中需要更多的领域专业知识。<details>
<summary>Abstract</summary>
We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently perform poorly at basic text handling required of lawyers or paralegals, such as looking up the text at a line of a witness deposition or at a subsection of a contract. We introduce a benchmark to quantify this poor performance, which casts into doubt LLMs' current reliability as-is for legal practice. Finetuning for these tasks brings an older LLM to near-perfect performance on our test set and also raises performance on a related legal task. This stark result highlights the need for more domain expertise in LLM training.
</details>
<details>
<summary>摘要</summary>
我团队发现，目前最佳公开可用的LLM（大型语言模型）如GPT-4和PaLM 2在法律领域的基础文本处理任务上表现不佳，如在证人证言或合同下的特定段落中查找文本。我们引入了一个比较方式来衡量这种不佳表现，这casts into doubt LLMPresent reliability in legal practice。经过调整，一个较老的LLM在我们测试集上几乎达到了近乎完美的表现，同时也提高了相关的法律任务表现。这一结果强调了LLM训练中需要更多的领域专业知识。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Unsupervised-Reinforcement-Learning-with-Self-Reference"><a href="#Augmenting-Unsupervised-Reinforcement-Learning-with-Self-Reference" class="headerlink" title="Augmenting Unsupervised Reinforcement Learning with Self-Reference"></a>Augmenting Unsupervised Reinforcement Learning with Self-Reference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09692">http://arxiv.org/abs/2311.09692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Zhao, Erle Zhu, Rui Lu, Matthieu Lin, Yong-Jin Liu, Gao Huang</li>
<li>for: The paper is written for proposing a new approach called Self-Reference (SR) to improve the performance of reinforcement learning agents in the unsupervised pretrain-then-finetune setting.</li>
<li>methods: The SR approach explicitly leverages historical information to mitigate the nonstationarity of intrinsic rewards during pretraining and prevent the unlearning of valuable exploratory behaviors during finetuning.</li>
<li>results: The SR approach achieves state-of-the-art results in terms of Interquartile Mean (IQM) performance and Optimality Gap reduction on the Unsupervised Reinforcement Learning Benchmark for model-free methods, with an 86% IQM and a 16% Optimality Gap reduction. Additionally, it improves current algorithms by up to 17% IQM and reduces the Optimality Gap by 31%.<details>
<summary>Abstract</summary>
Humans possess the ability to draw on past experiences explicitly when learning new tasks and applying them accordingly. We believe this capacity for self-referencing is especially advantageous for reinforcement learning agents in the unsupervised pretrain-then-finetune setting. During pretraining, an agent's past experiences can be explicitly utilized to mitigate the nonstationarity of intrinsic rewards. In the finetuning phase, referencing historical trajectories prevents the unlearning of valuable exploratory behaviors. Motivated by these benefits, we propose the Self-Reference (SR) approach, an add-on module explicitly designed to leverage historical information and enhance agent performance within the pretrain-finetune paradigm. Our approach achieves state-of-the-art results in terms of Interquartile Mean (IQM) performance and Optimality Gap reduction on the Unsupervised Reinforcement Learning Benchmark for model-free methods, recording an 86% IQM and a 16% Optimality Gap. Additionally, it improves current algorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond performance enhancement, the Self-Reference add-on also increases sample efficiency, a crucial attribute for real-world applications.
</details>
<details>
<summary>摘要</summary>
人类具有将过去经验显式地应用于学习新任务的能力，这对于无监督预训练然后精度调整的机器学习代理来说是非常有利的。在预训练阶段，代理可以通过过去经验来抑制内在奖励的非站点性。在精度调整阶段， referencing历史轨迹可以防止探索行为的忘记。基于这些优点，我们提出了自引 Referenced (SR) 方法，这是专门为预训练然后精度调整的 paradigm 设计的一个模块。我们的方法在无监督学习学Benchmark上实现了状态之art 的表现，具有86%的Interquartile Mean (IQM) 和16%的Optimality Gap 减少。此外，它还可以提高当前算法的性能，最高提高17%的IQM和31%的Optimality Gap。此外，SR 方法还可以提高实际应用中的样本效率，这是一个非常重要的特性。
</details></li>
</ul>
<hr>
<h2 id="Do-Physicians-Know-How-to-Prompt-The-Need-for-Automatic-Prompt-Optimization-Help-in-Clinical-Note-Generation"><a href="#Do-Physicians-Know-How-to-Prompt-The-Need-for-Automatic-Prompt-Optimization-Help-in-Clinical-Note-Generation" class="headerlink" title="Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation"></a>Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09684">http://arxiv.org/abs/2311.09684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zonghai Yao, Ahmed Jaafar, Beining Wang, Yue Zhu, Zhichao Yang, Hong Yu</li>
<li>for: 这项研究探讨了对大语言模型（LLM）在医疗笔记生成中的表现，并提出了自动提示优化（APO）框架来优化初始提示。</li>
<li>methods: 研究使用了GPT3.5和GPT4两个语言模型，并对它们进行了APO优化。</li>
<li>results: 结果显示GPT4 APO在标准化提示质量方面表现出色，而且专业人员在APO后仍然保持了内容质量。<details>
<summary>Abstract</summary>
This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
</details>
<details>
<summary>摘要</summary>
这项研究研究了大语言模型（LLM）在医疗记录生成中的表现，并评估了提示工程的影响。我们提出了自动提示优化（APO）框架，以改进初始提示并比较医学专家、非医学专家和APO-加强GPT3.5和GPT4的输出。结果显示GPT4 APO在标准化提示质量方面表现出色，而人类在循环中的干预表明专家保持了提示质量的控制，并偏好自己的修改，这表明了专家自定义的价值。我们建议使用两个阶段优化过程，首先使用APO-GPT4保证一致性，然后通过专家输入进行个性化。
</details></li>
</ul>
<hr>
<h2 id="MacGyver-Are-Large-Language-Models-Creative-Problem-Solvers"><a href="#MacGyver-Are-Large-Language-Models-Creative-Problem-Solvers" class="headerlink" title="MacGyver: Are Large Language Models Creative Problem Solvers?"></a>MacGyver: Are Large Language Models Creative Problem Solvers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09682">http://arxiv.org/abs/2311.09682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman</li>
<li>for:  This paper aims to explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting, specifically in circumventing functional fixedness.</li>
<li>methods: The paper uses an automatically generated dataset called MacGyver, which consists of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking ‘out-of-the-box’. The paper compares and contrasts the problem-solving abilities of LLMs and humans on this dataset.</li>
<li>results: The paper shows that both LLMs and humans struggle with the MacGyver problems, but in different ways. LLMs are prone to overconfidence and propose physically infeasible or inefficient solutions, while humans excel in solving familiar problems but struggle with tasks requiring domain-specific knowledge. The paper also demonstrates the potential of enhancing LLMs’ problem-solving ability with novel prompting techniques.<details>
<summary>Abstract</summary>
We explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting. The setting requires circumventing a cognitive bias known in psychology as ''functional fixedness'' to use familiar objects in innovative or unconventional ways. To this end, we create MacGyver, an automatically generated dataset consisting of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking 'out-of-the-box'. We then present our collection of problems to both LLMs and humans to compare and contrast their problem-solving abilities. We show that MacGyver is challenging for both groups, but in unique and complementary ways. For example, humans typically excel in solving problems that they are familiar with but may struggle with tasks requiring domain-specific knowledge, leading to a higher variance. On the other hand, LLMs, being exposed to a variety of highly specialized knowledge, attempt broader problems but are prone to overconfidence and propose actions that are physically infeasible or inefficient. We also provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work provides insight into the creative problem-solving capabilities of humans and AI and illustrates how psychological paradigms can be extended into large-scale tasks for comparing humans and machines.
</details>
<details>
<summary>摘要</summary>
我们探索现代大语言模型（LLM）的创造力问题解决能力在限制性的设定下。这个设定要求绕过心理学中的''功能固化''（functional fixedness）来使用familiar对象在创新或非正式的方式上。为此，我们创建了MacGyver数据集，包含1,600个实际世界问题，旨在触发功能固化并需要''思外框''的思维。然后，我们将这些问题提交给LLM和人类，以比较和对比他们的问题解决能力。我们发现MacGyver对两个组合体是挑战的，但是各自unique和补做的。例如，人类通常在 familar的问题上 excel，但可能会在需要域pecific知识的任务上遇到问题，导致更高的变差。而LLMs，作为承载了多种高度特殊化的知识的，尝试更广泛的问题，但容易过度自信和提出物理不可能或不fficient的操作。我们还提供了LLMs的详细错误分析，并示出了使用迭代步骤反思和多元思维技巧来增强其问题解决能力的潜在。这项工作提供了人类和AI的创造力问题解决能力的视角，并示出了将心理学概念扩展到大规模任务上，用于比较人类和机器的能力。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Large-Models-in-Vision-A-Survey"><a href="#Trustworthy-Large-Models-in-Vision-A-Survey" class="headerlink" title="Trustworthy Large Models in Vision: A Survey"></a>Trustworthy Large Models in Vision: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09680">http://arxiv.org/abs/2311.09680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyan Guo, Jun Liu</li>
<li>for: 本文旨在探讨 Large Models (LMs) 在 computer vision 领域中的可靠性问题，并提出相关挑战和对策。</li>
<li>methods: 本文使用了系统性的方法，包括简述了四种可能会妨碍 LMs 在视觉领域中的可靠使用的问题，并提供了对应的挑战、对策和讨论。</li>
<li>results: 本文通过对 LMs 在视觉领域中的可靠性问题进行系统性的探讨，提供了 deeper understanding 的概念和对策，以便promote LMs 在人类社会中的可靠使用。<details>
<summary>Abstract</summary>
The rapid progress of Large Models (LMs) has recently revolutionized various fields of deep learning with remarkable grades, ranging from Natural Language Processing (NLP) to Computer Vision (CV). However, LMs are increasingly challenged and criticized by academia and industry due to their powerful performance but untrustworthy behavior, which urgently needs to be alleviated in reliable methods. Despite the abundance of literature on trustworthy LMs in language, a systematic survey specifically delving into the trustworthiness of LMs in vision remains absent. In order to mitigate this gap, we summarize four relevant concerns that obstruct the trustworthy usage in vision of LMs in this survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4) interpretability. By highlighting corresponding challenge, countermeasures, and discussion in each topic, we hope this survey will facilitate readers' understanding of the field, promote alignment of LMs with human expectations and enable trustworthy LMs to serve as welfare rather than disaster for human society.
</details>
<details>
<summary>摘要</summary>
大型模型（LM）的快速进步最近对深度学习多个领域产生了很大的改变，从自然语言处理（NLP）到计算机视觉（CV）。然而，LM在学术和业界的应用中遇到了强大性的挑战和批评，需要可靠的方法来缓解这些问题。尽管有很多关于可靠LMs的语言文献，但是关于视觉领域中LMs的可靠性的系统性调查仍然缺失。为了弥补这个差距，我们在这份报告中总结了四个对视觉领域LMs可靠性的挑战，包括1）人类违用、2）抵触、3）内在问题和4）可解性。通过对每个话题的挑战、对策和讨论进行强调，我们希望通过这份报告能够帮助读者更好地理解这个领域，促进LMs与人类期望的Alignment，使LMs成为人类社会的福利而不是灾难。
</details></li>
</ul>
<hr>
<h2 id="Structured-Chemistry-Reasoning-with-Large-Language-Models"><a href="#Structured-Chemistry-Reasoning-with-Large-Language-Models" class="headerlink" title="Structured Chemistry Reasoning with Large Language Models"></a>Structured Chemistry Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09656">http://arxiv.org/abs/2311.09656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Jiawei Han, Lianhui Qin</li>
<li>for: 解决复杂化学问题的大自然语言模型（LLM）问题。</li>
<li>methods: 提出了一种新的结构化思维方法——InstructChem，可以显著提高 LLM 的化学思维能力。InstructChem 分解为三个关键句，包括化学式生成、逐步思维和迭代审阅改进。</li>
<li>results: 对四种化学挑战进行了广泛的实验，包括量子化学、量子力学、物理化学和化学动力学。结果显示，我们的方法可以显著提高 GPT-4 的化学思维能力，具体达到了8%的平均绝对改进和30%的峰值改进。此外，我们还使用 GPT-4 生成的思维来 fine-tune  smaller LMs（如 Vicuna），并观察到了这些 smaller LMs 的强大改进。这 Validates 我们的方法，并允许 LLMs 生成高质量的思维。<details>
<summary>Abstract</summary>
This paper studies the problem of solving complex chemistry problems with large language models (LLMs). Despite the extensive general knowledge in LLMs (such as GPT-4), they struggle with chemistry reasoning that requires faithful grounded reasoning with diverse chemical knowledge and an integrative understanding of chemical interactions. We propose InstructChem, a new structured reasoning approach that substantially boosts the LLMs' chemical reasoning capabilities. InstructChem explicitly decomposes the reasoning into three critical phrases, including chemical formulae generation by LLMs that offers the basis for subsequent grounded reasoning, step-by-step reasoning that makes multi-step derivations with the identified formulae for a preliminary answer, and iterative review-and-refinement that steers LLMs to progressively revise the previous phases for increasing confidence, leading to the final high-confidence answer. We conduct extensive experiments on four different chemistry challenges, including quantum chemistry, quantum mechanics, physical chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak improvement. We further use the generated reasoning by GPT-4 to fine-tune smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs. This validates our approach and enables LLMs to generate high-quality reasoning.
</details>
<details>
<summary>摘要</summary>
InstructChem breaks down the reasoning process into three critical phases:1. Chemical formula generation: LLMs generate the chemical formula as the basis for subsequent grounded reasoning.2. Step-by-step reasoning: LLMs make multi-step derivations with the identified formula to arrive at a preliminary answer.3. Iterative review-and-refinement: LLMs revise the previous phases to increase confidence and eventually arrive at a high-confidence answer.We conduct extensive experiments on four chemistry challenges: quantum chemistry, quantum mechanics, physical chemistry, and chemistry kinetics. Our approach achieves an average absolute improvement of 8% and a peak improvement of 30% over GPT-4 on chemistry reasoning tasks. Moreover, we use the generated reasoning by GPT-4 to fine-tune smaller LMs (e.g., Vicuna) and observe significant improvement in their performance, validating our approach. This demonstrates that LLMs can generate high-quality reasoning with the help of InstructChem.
</details></li>
</ul>
<hr>
<h2 id="“It’s-not-like-Jarvis-but-it’s-pretty-close-”-–-Examining-ChatGPT’s-Usage-among-Undergraduate-Students-in-Computer-Science"><a href="#“It’s-not-like-Jarvis-but-it’s-pretty-close-”-–-Examining-ChatGPT’s-Usage-among-Undergraduate-Students-in-Computer-Science" class="headerlink" title="“It’s not like Jarvis, but it’s pretty close!” – Examining ChatGPT’s Usage among Undergraduate Students in Computer Science"></a>“It’s not like Jarvis, but it’s pretty close!” – Examining ChatGPT’s Usage among Undergraduate Students in Computer Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09651">http://arxiv.org/abs/2311.09651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishika Joshi, Ritvik Budhiraja, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar</li>
<li>for: 本研究旨在了解学生如何使用 ChatGPT 作为课程相关任务的工具。</li>
<li>methods: 本研究使用学生调查和面试获取了学生对 ChatGPT 的看法和体验，以及他们在使用中遇到的挑战和改进建议。</li>
<li>results: 研究发现大多数学生（超过 57%）对使用 ChatGPT 作为课程相关任务的工具有积极的看法，但也提出了一些需要解决的挑战，以便在长期使用中得到学生的acceptance。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students. The findings from this investigation have broader implications and may be applicable to other LLMs and their role in computing education.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如ChatGPT和Google Bard在学术社区中受到了广泛的关注。先前的研究已经评估了这些LLM在不同应用场景中的性能，但这些评估大多由教师和研究人员进行，未经考虑学生的实际使用情况。这项研究采用学生第一的方法，以全面了解学生在使用ChatGPT时的各种优点、挑战和改进建议。我们通过学生问卷和面试获得了价值的反馈，了解学生对ChatGPT的批评和建议。我们发现超过57%的学生对使用ChatGPT为课程任务提供帮助表示积极的看法。然而，我们的研究也揭示了长期Acceptance of ChatGPT amongst students。这些发现对其他LLM和计算教育有广泛的意义。
</details></li>
</ul>
<hr>
<h2 id="On-the-Exploitability-of-Reinforcement-Learning-with-Human-Feedback-for-Large-Language-Models"><a href="#On-the-Exploitability-of-Reinforcement-Learning-with-Human-Feedback-for-Large-Language-Models" class="headerlink" title="On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models"></a>On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09641">http://arxiv.org/abs/2311.09641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao</li>
<li>for: 防止语言模型（LLMs）被恶意攻击，保持其与人类偏好的 aligning。</li>
<li>methods: 提出了一种攻击方法 RankPoison，通过对偏好选择的权重赋值进行预言，使 LLMS 生成更长的序列，而不会影响原始的安全对齐性表现。</li>
<li>results: 通过使用 RankPoison，可以实现攻击 LLMS，使其生成更长的答案，并且可以在问题中Trigger Word 的情况下实现后门攻击。这些发现 highlighted RLHF 的安全挑战，强调了更加Robust的对齐方法的需求。<details>
<summary>Abstract</summary>
Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.
</details>
<details>
<summary>摘要</summary>
大自然语言模型（LLM）与人类偏好的重塑学习（RLHF）是一种方法，用于将 LLM 与人类偏好进行对应。尽管它有优点，但RLHF 依赖于人类标注者来评分文本，这可能引入潜在的安全漏洞，如果任何敌对标注者（例如，攻击者）操纵分数，以使 LLM 进行恶意操作。为了评估 RLHF 对人类偏好数据毒化的红色队伍，我们提出了 RankPoison 攻击方法，即在偏好排名中选择扰乱的方法来达到恶意行为（例如，生成更长的序列，这可能增加计算成本）。使用 RankPoison 生成的毒化数据集，我们可以对 LLM 进行毒化攻击，以生成更长的 токен，而不会伤害原始的安全对齐性表现。此外，我们还成功地实现了后门攻击，使 LLM 可以在问题中的词TriggerWord 下生成更长的答案。我们的发现高亮了 RLHF 中的安全挑战，这让我们更需要更加robust的对齐方法来保护 LLM。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Engineering-of-Long-Prompts"><a href="#Automatic-Engineering-of-Long-Prompts" class="headerlink" title="Automatic Engineering of Long Prompts"></a>Automatic Engineering of Long Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10117">http://arxiv.org/abs/2311.10117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Cho-Jui Hsieh, Si Si, Felix X. Yu, Inderjit S. Dhillon</li>
<li>for:  automatic long prompt engineering for LLMs</li>
<li>methods:  greedy algorithms, genetic algorithms, and LLM-based mutation</li>
<li>results:  average accuracy gain of 9.2% on eight tasks in Big Bench HardHere’s the full answer in Simplified Chinese:</li>
<li>for: 自动生成长提示 для LLM</li>
<li>methods: 简单的排序算法、进化算法和基于 LLM 的突变</li>
<li>results:  eight tasks in Big Bench Hard 的平均精度提升率为 9.2%I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts. However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort. Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences. However, the automatic design of long prompts remains a challenging problem due to its immense search space. In this paper, we investigate the performance of greedy algorithms and genetic algorithms for automatic long prompt engineering. We demonstrate that a simple greedy approach with beam search outperforms other methods in terms of search efficiency. Moreover, we introduce two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our search algorithm. Our results show that the proposed automatic long prompt engineering algorithm achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在解决复杂的开放领域任务上表现出了很好的能力，受到详细的指令和示例的引导。然而，这些示例通常是非常长，可能包含数百行和数千个符号，其设计通常需要较多的人工努力。现有研究已经探索了自动生成短示例，通常只有一些句子或数个句子。然而，自动设计长示例仍然是一个具有巨大搜索空间的困难问题。在这篇论文中，我们调查了大型语言模型（LLM）引导的自动长示例工程。我们发现，使用搜索缓存的简单排序法比其他方法更高效。此外，我们还介绍了两种使用搜索历史来增强LLM基于搜索算法的变异效果的新技术。我们的结果表明，我们的自动长示例工程算法在Big Bench Hard上的八个任务中平均提高了9.2%的准确率，这 highlights了自动化示例设计以便满分 LLMs 的可能性。
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Knowledge-Learning-for-Language-Models"><a href="#Online-Continual-Knowledge-Learning-for-Language-Models" class="headerlink" title="Online Continual Knowledge Learning for Language Models"></a>Online Continual Knowledge Learning for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09632">http://arxiv.org/abs/2311.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Wu, Tongjun Shi, Karthick Sharma, Chun Wei Seah, Shuhao Zhang</li>
<li>for: 本研究旨在解决语言模型（LLM）中的动态世界知识管理问题，以满足实时环境中的问题解决和事实核查。</li>
<li>methods: 本文提出了一个新的 continual learning 问题，即在线上动态知识学习（OCKL）问题，并提出了一个新的评价指标来衡量知识获得率和先前学习知识的保留。</li>
<li>results: 我们的实验结果表明，现有的 continual learning 方法无法解决 OCKL 问题，而我们的研究带来了关于如何在不断变化的环境中训练 LLM 的新理解。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) serve as repositories of extensive world knowledge, enabling them to perform tasks such as question-answering and fact-checking. However, this knowledge can become obsolete as global contexts change. In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL). This problem formulation aims to manage the dynamic nature of world knowledge in LMs under real-time constraints. We propose a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge. Our empirical evaluation, conducted using a variety of state-of-the-art methods, establishes robust base-lines for OCKL. Our results reveal that existing continual learning approaches are unfortunately insufficient for tackling the unique challenges posed by OCKL. We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxíng yǔyán módelǐng)* "Continual Learning" is translated as "连续学习" (liánxù xuéxí)* "Online Continual Knowledge Learning" is translated as "在线连续知识学习" (zài xiàng liánxù zhīshī xuéxí)* "world knowledge" is translated as "世界知识" (shìjiè zhīshī)* "global contexts" is translated as "全球背景" (quánqiú bèngjǐng)* "dynamic nature" is translated as "动态性" (dòngtǐ xìng)* "real-time constraints" is translated as "实时约束" (shíhòu yuēsuǒ)* "benchmark" is translated as "标准" (biāo jiā)* "evaluation metric" is translated as "评价指标" (píngjì zhǐbiāo)* "rate of new knowledge acquisition" is translated as "新知识获得速率" (xīn zhīshī gòngdé sùlù)* "retention of previously learned knowledge" is translated as "前期学习知识保持" (qiánxī xuéxí zhīshī bǎochí)* "trade-off" is translated as "交互" (jiāoxì)* "key factors" is translated as "关键因素" (guānjī yǐnxiàng)* "continually evolving environment" is translated as "不断发展的环境" (bùdàn fāzhǎn de huánjìng)
</details></li>
</ul>
<hr>
<h2 id="CRISPR-Eliminating-Bias-Neurons-from-an-Instruction-following-Language-Model"><a href="#CRISPR-Eliminating-Bias-Neurons-from-an-Instruction-following-Language-Model" class="headerlink" title="CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model"></a>CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09627">http://arxiv.org/abs/2311.09627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nakyeong Yang, Taegwan Kang, Kyomin Jung</li>
<li>for: 这个论文是为了解决语言模型（LLMs）在基于指令的任务执行过程中遇到的分布差异问题，以及这些差异如何导致语言模型受到干扰和偏见。</li>
<li>methods: 这个论文提出了一种新的偏见缓和方法，称为CRISPR，用于减少基于指令的偏见。CRISPR使用了负责任方法来 identific 受偏见影响的偏见神经元，并使用了剪除来消除这些偏见神经元。</li>
<li>results: 实验结果表明，CRISPR可以有效地减少基于指令的偏见，提高语言模型在社会偏见benchmark上的表现，而无需损失先前学习的知识。此外，CRISPR是模型无关的，可以适应不断变化的社会偏见。<details>
<summary>Abstract</summary>
Large language models (LLMs) executing tasks through instruction-based prompts often face challenges stemming from distribution differences between user instructions and training instructions. This leads to distractions and biases, especially when dealing with inconsistent dynamic labels. In this paper, we introduces a novel bias mitigation method, CRISPR, designed to alleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods to identify bias neurons influencing biased outputs and employs pruning to eliminate the bias neurons. Experimental results demonstrate the method's effectiveness in mitigating biases in instruction-based prompting, enhancing language model performance on social bias benchmarks without compromising pre-existing knowledge. CRISPR proves highly practical, model-agnostic, offering flexibility in adapting to evolving social biases.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通过指令式提示进行任务时，常会面临用户指令和训练指令之间的分布差异问题，这会导致分心和偏袋问题，特别是对于不稳定的动态标签。在这篇论文中，我们介绍了一种新的偏见缓和方法，名为CRISPR，用于缓和指令标签偏见在LLM中。CRISPR利用属性方法来识别偏见神经元影响偏见输出，并使用剪除来消除这些偏见神经元。实验结果显示CRISPR有效地缓和偏见在指令式提示中，提高语言模型在社会偏见标准上的表现，不会对先前的知识造成损害。CRISPR非常实用、模型无须对类型，可以灵活地适应社会偏见的变化。
</details></li>
</ul>
<hr>
<h2 id="AI-Recommendation-System-for-Enhanced-Customer-Experience-A-Novel-Image-to-Text-Method"><a href="#AI-Recommendation-System-for-Enhanced-Customer-Experience-A-Novel-Image-to-Text-Method" class="headerlink" title="AI Recommendation System for Enhanced Customer Experience: A Novel Image-to-Text Method"></a>AI Recommendation System for Enhanced Customer Experience: A Novel Image-to-Text Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09624">http://arxiv.org/abs/2311.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamaed Foued Ayedi, Hiba Ben Salem, Soulaimen Hammami, Ahmed Ben Said, Rateb Jabbar, Achraf CHabbouh</li>
<li>for: 这项研究旨在提供精准和个性化的时尚推荐系统，使用人工智能技术进行细致的视觉解释，以帮助用户找到与愿望图像中的风格相似的时尚产品。</li>
<li>methods: 该研究使用了一个全新的端到端管道，其包括图像上传、对图像进行分类、对图像进行描述、对全球时尚产品目录进行检索、并将检索结果与原图像进行比较。</li>
<li>results: 在使用了 более than 100,000个分类的时尚照片数据集上进行训练和评估后，该管道实现了0.97的F1分数，表明其可以准确地识别时尚对象，并且可以为用户提供个性化的时尚推荐。<details>
<summary>Abstract</summary>
Existing fashion recommendation systems encounter difficulties in using visual data for accurate and personalized recommendations. This research describes an innovative end-to-end pipeline that uses artificial intelligence to provide fine-grained visual interpretation for fashion recommendations. When customers upload images of desired products or outfits, the system automatically generates meaningful descriptions emphasizing stylistic elements. These captions guide retrieval from a global fashion product catalogue to offer similar alternatives that fit the visual characteristics of the original image. On a dataset of over 100,000 categorized fashion photos, the pipeline was trained and evaluated. The F1-score for the object detection model was 0.97, exhibiting exact fashion object recognition capabilities optimized for recommendation. This visually aware system represents a key advancement in customer engagement through personalized fashion recommendations
</details>
<details>
<summary>摘要</summary>
现有的时尚推荐系统在使用视觉数据进行准确和个性化推荐时遇到困难。本研究描述了一种创新的端到端管道，使用人工智能来为时尚推荐提供细腻的视觉解释。当客户上传欲购买的产品或服装图片时，系统会自动生成有意义的描述，强调时尚元素。这些描述将引导从全球时尚产品目录中选择类似的产品，以适应原图的视觉特征。在超过10万个分类时尚照片的数据集上训练和评估，管道的F1分数为0.97，表示系统具有高精度的时尚物品识别能力，优化为推荐。这种视觉意识系统将成为个性化时尚推荐的关键进步。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Evaluation-and-Insights-into-the-Use-of-Deep-Neural-Networks-to-Detect-and-Quantify-Lymphoma-Lesions-in-PET-CT-Images"><a href="#Comprehensive-Evaluation-and-Insights-into-the-Use-of-Deep-Neural-Networks-to-Detect-and-Quantify-Lymphoma-Lesions-in-PET-CT-Images" class="headerlink" title="Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET&#x2F;CT Images"></a>Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09614">http://arxiv.org/abs/2311.09614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/lymphoma-segmentation-dnn">https://github.com/microsoft/lymphoma-segmentation-dnn</a></li>
<li>paper_authors: Shadab Ahamed, Yixi Xu, Claire Gowdy, Joo H. O, Ingrid Bloise, Don Wilson, Patrick Martineau, François Bénard, Fereshteh Yousefirizi, Rahul Dodhia, Juan M. Lavista, William B. Weeks, Carlos F. Uribe, Arman Rahmim<br>for:This paper evaluates the performance of four deep learning architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion segmentation from PET&#x2F;CT images.methods:The paper uses a diverse, multi-institutional dataset of 611 cases to train, validate, and test the four neural network architectures. The authors use internal testing and external testing to evaluate the performance of the networks, and they assess reproducibility of six lesion measures, calculate prediction errors, and examine DSC performance in relation to lesion measures.results:The results show that SegResNet is the top performer with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml on the internal testing set. On the unseen external test set, SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best false negative volume (FNV) of 0.41 ml. The authors also found that the networks had a median false negative volume (FNV) of 0 ml. Additionally, the authors introduced three lesion detection criteria, addressed the challenges in segmenting “easy” vs. “hard” cases, and performed inter-observer agreement assessment.Here is the same information in Simplified Chinese text:for:这个研究用四种深度学习架构（UNet、SegResNet、DynUNet、SwinUNETR）进行淋巴癌肿囊分 segmentation from PET&#x2F;CT图像。methods:这个研究使用多个机构的多例数据集（611例）来训练、验证和测试四种深度学习架构。作者们使用内测和外测来评估这些网络的性能，并评估了六个肿囊指标的重复性，计算预测错误，并对肿囊指标与深度学习架构之间的关系进行了研究。results:结果显示，SegResNet在内测集上得到了最高的 median Dice similarity coefficient（DSC）值为0.76，并且 median false positive volume（FPV）值为4.55ml。在未看过的外测集上，SegResNet得到了最高的 median DSC值为0.68和FPV值为21.46ml，而UNet得到了最低的 false negative volume（FNV）值为0.41ml。此外，作者们发现所有网络都有0ml的false negative volume（FNV）。此外，作者们还引入了三个肿囊检测标准，解决了检测”容易”vs.”Difficult”情况的挑战，并进行了多个专家 annotator 的一致性评估。<details>
<summary>Abstract</summary>
This study performs comprehensive evaluation of four neural network architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion segmentation from PET/CT images. These networks were trained, validated, and tested on a diverse, multi-institutional dataset of 611 cases. Internal testing (88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed SegResNet as the top performer with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a median false negative volume (FNV) of 0 ml. On the unseen external test set (145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml. We assessed reproducibility of six lesion measures, calculated their prediction errors, and examined DSC performance in relation to these lesion measures, offering insights into segmentation accuracy and clinical relevance. Additionally, we introduced three lesion detection criteria, addressing the clinical need for identifying lesions, counting them, and segmenting based on metabolic characteristics. We also performed expert intra-observer variability analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to assist in the development of more resilient segmentation algorithms. Finally, we performed inter-observer agreement assessment underscoring the importance of a standardized ground truth segmentation protocol involving multiple expert annotators. Code is available at: https://github.com/microsoft/lymphoma-segmentation-dnn
</details>
<details>
<summary>摘要</summary>
internally, SegResNet showed the highest performance with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml. All models had a median false negative volume (FNV) of 0 ml. On the external test set, SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.The study also assessed the reproducibility of six lesion measures, calculated prediction errors, and examined DSC performance in relation to these lesion measures. Additionally, the study introduced three lesion detection criteria, addressed the clinical need for identifying, counting, and segmenting based on metabolic characteristics.The study also performed expert intra-observer variability analysis, revealing the challenges in segmenting "easy" vs. "hard" cases, and inter-observer agreement assessment, underscoring the importance of a standardized ground truth segmentation protocol involving multiple expert annotators.The code for the study is available at: https://github.com/microsoft/lymphoma-segmentation-dnn.
</details></li>
</ul>
<hr>
<h2 id="Digital-Socrates-Evaluating-LLMs-through-explanation-critiques"><a href="#Digital-Socrates-Evaluating-LLMs-through-explanation-critiques" class="headerlink" title="Digital Socrates: Evaluating LLMs through explanation critiques"></a>Digital Socrates: Evaluating LLMs through explanation critiques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09613">http://arxiv.org/abs/2311.09613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuling Gu, Oyvind Tafjord, Peter Clark</li>
<li>for: 本研究的目的是解释现代模型的解释能力，并提供一种自动生成高质量、可读性的解释评价工具。</li>
<li>methods: 本研究使用了定义新的解释批判任务，创建了大量人工验证的数据集，并使用这些数据集训练了一个开源的自动解释批判模型（称为“数字索慈”）。</li>
<li>results: 通过量化和质量分析，本研究表明了数字索慈可以帮助揭示学生模型的思维链，并提供高质量、细化的自动解释评价。数字索慈因此填补了现有的解释评价工具之间的重要空白。<details>
<summary>Abstract</summary>
While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critiquing model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.
</details>
<details>
<summary>摘要</summary>
而LMs可以提供结构化的解释，但是这些解释的质量和性质仍然不够理解。因此，我们的目标是定义现代模型的解释能力的详细方式，并创建一个自动生成这些 caracterizations的解释评价工具，不需要Expensive API调用或人工注释。我们的方法包括：（a）定义解释批判任务——标识和分类任何主要缺陷在解释中，并提供修复建议。（b）创建大量，人工验证的数据集。（c）使用这些数据集，训练一个开源的自动批判模型（称为数字SOCRATES）。通过量化和质量分析，我们示出了数字SOCRATES如何用于探索学生模型的逻辑链，以及如何提供高质量、细化的自动评价。数字SOCRATES因此填充了现代模型解释行为的评价工具中的重要空白。
</details></li>
</ul>
<hr>
<h2 id="Code-Models-are-Zero-shot-Precondition-Reasoners"><a href="#Code-Models-are-Zero-shot-Precondition-Reasoners" class="headerlink" title="Code Models are Zero-shot Precondition Reasoners"></a>Code Models are Zero-shot Precondition Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09601">http://arxiv.org/abs/2311.09601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lajanugen Logeswaran, Sungryull Sohn, Yiwei Lyu, Anthony Zhe Liu, Dong-Ki Kim, Dongsub Shim, Moontae Lee, Honglak Lee</li>
<li>for: 这篇论文旨在探讨如何使用代码表示来理解行为前提条件，以便在Sequential Decision Making任务中完成任务。</li>
<li>methods: 这篇论文使用了预训练的代码模型，从示例轨迹中提取出行动前提条件，并使用这些前提条件来预测行动。</li>
<li>results: 根据论文的结果，使用这种前condition-aware的行动采样策略可以提高几何 shot 策略学习的性能，并在任务 oriented dialog 和 embodied textworld 测试 benchmark 上达到了更好的结果。<details>
<summary>Abstract</summary>
One of the fundamental skills required for an agent acting in an environment to complete tasks is the ability to understand what actions are plausible at any given point. This work explores a novel use of code representations to reason about action preconditions for sequential decision making tasks. Code representations offer the flexibility to model procedural activities and associated constraints as well as the ability to execute and verify constraint satisfaction. Leveraging code representations, we extract action preconditions from demonstration trajectories in a zero-shot manner using pre-trained code models. Given these extracted preconditions, we propose a precondition-aware action sampling strategy that ensures actions predicted by a policy are consistent with preconditions. We demonstrate that the proposed approach enhances the performance of few-shot policy learning approaches across task-oriented dialog and embodied textworld benchmarks.
</details>
<details>
<summary>摘要</summary>
一个基本的技能需要在环境中完成任务是理解当前点可行的动作。这项工作探讨一种使用代码表示来理解动作前提条件的新用途。代码表示具有模拟过程活动和相关约束的灵活性，以及执行和验证约束满足的能力。通过代码表示，我们从示例轨迹中提取动作前提条件，无需任何更改或训练。基于提取的前提条件，我们提议一种了解政策预测的动作抽样策略，以确保政策预测的动作与前提条件相符。我们证明，该方法可以提高几个shot策略学习的性能在任务强调对话和embodied textworld benchmark上。
</details></li>
</ul>
<hr>
<h2 id="Multi-Step-Dialogue-Workflow-Action-Prediction"><a href="#Multi-Step-Dialogue-Workflow-Action-Prediction" class="headerlink" title="Multi-Step Dialogue Workflow Action Prediction"></a>Multi-Step Dialogue Workflow Action Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09593">http://arxiv.org/abs/2311.09593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramya Ramakrishnan, Ethan Elenberg, Hashan Narangodage, Ryan McDonald</li>
<li>for: 提高对对话任务的自动化率，增加对话系统的效率和智能化 Waterfall </li>
<li>methods: 提出了三种简单实现的模型方法：1）精度调整训练集，2）几步学习利用检索和大语言模型提示，3）零步图 traversal </li>
<li>results: 实现了20%的步骤自动化，不需要人工监督 Waterfall Here’s the translation in English:</li>
<li>for: Improving the automation rate of dialogue tasks, increasing the efficiency and intelligence of conversation systems.</li>
<li>methods: Proposed three simple modeling methods: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction.</li>
<li>results: Achieved 20% automation of steps without requiring as much human oversight.<details>
<summary>Abstract</summary>
In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback from a human overseeing the system.
</details>
<details>
<summary>摘要</summary>
在任务导向对话中，系统经常需要遵循一系列动作，称为工作流程，以完成任务。在这篇论文中，我们提出了多步工作流程动作预测的新问题，其中系统预测多个未来的工作流程动作。准确预测多个步骤可以实现多轮自动化，这可以释放时间专注更复杂的任务。我们提出了三种简单实现的模型方法：1）精度调整训练集，2）几招在 Context 中学习和大语言模型提示，3）零shot图 traversal，即将历史动作序列聚合成图进行预测。我们显示，多步动作预测生成了下游对话任务的准确预测特征，并可以提高自动化步骤的效率达20%，不需要人工监督系统提供多少反馈。
</details></li>
</ul>
<hr>
<h2 id="Tied-Lora-Enhacing-parameter-efficiency-of-LoRA-with-weight-tying"><a href="#Tied-Lora-Enhacing-parameter-efficiency-of-LoRA-with-weight-tying" class="headerlink" title="Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying"></a>Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09578">http://arxiv.org/abs/2311.09578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adithya Renduchintala, Tugrul Konuk, Oleksii Kuchaiev</li>
<li>for: 提高LoRA方法的参数效率</li>
<li>methods: 使用权重固定和选择性训练</li>
<li>results: 实现了与标准LoRA方法相当的性能，使用的参数比例减少至13%Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the parameter efficiency of the Low-rank adaptation (LoRA) method.</li>
<li>methods: The paper proposes a simple paradigm that utilizes weight tying and selective training to further increase the parameter efficiency of LoRA.</li>
<li>results: The paper provides experiments that demonstrate the effectiveness of the proposed Tied-LoRA method, achieving comparable performance to the standard LoRA method while using only 13% of the parameters.<details>
<summary>Abstract</summary>
We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective training to further increase parameter efficiency of the Low-rank adaptation (LoRA) method. Our investigations include all feasible combinations parameter training/freezing in conjunction with weight tying to identify the optimal balance between performance and the number of trainable parameters. Through experiments covering a variety of tasks and two base language models, we provide analysis revealing trade-offs between efficiency and performance. Our experiments uncovered a particular Tied-LoRA configuration that stands out by demonstrating comparable performance across several tasks while employing only 13~\% percent of parameters utilized by the standard LoRA method.
</details>
<details>
<summary>摘要</summary>
我们提出了缔绳LoRA（Tied-LoRA）方法，这是一种简单的思想，通过权重缔绳和选择性训练来进一步提高LoRA方法中的参数效率。我们对所有可能的参数训练/冻结 combinational进行了调查，以确定最佳的效率和性能之间的平衡。通过覆盖多种任务和两个基础语言模型的实验，我们提供了分析，揭示了效率和性能之间的交易。我们的实验发现，一种特定的Tied-LoRA配置可以在多个任务中达到相似的性能水平，使用了标准LoRA方法的13\%的参数。
</details></li>
</ul>
<hr>
<h2 id="Work-State-Centric-AI-Agents-Design-Implementation-and-Management-of-Cognitive-Work-Threads"><a href="#Work-State-Centric-AI-Agents-Design-Implementation-and-Management-of-Cognitive-Work-Threads" class="headerlink" title="Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads"></a>Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09576">http://arxiv.org/abs/2311.09576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for: 提高任务执行效率和任务分析预测</li>
<li>methods: 使用工作笔记记录和反思循环来捕捉工作状态信息，并将工作状态记录作为全面的工作日志</li>
<li>results: 提高任务执行效率，并为后续任务分析和审核提供坚实的基础<details>
<summary>Abstract</summary>
AI agents excel in executing predefined tasks, but the dynamic management of work state information during task execution remains an underexplored area. We propose a work state-centric AI agent model employing "work notes" to record and reflect the state throughout task execution. This paper details the model's architecture, featuring worker threads for task oversight, planner modules for task decomposition and planning, and executor modules for performing subtasks using a ReAct-inspired thought-action loop. We provide an exhaustive work state record incorporating plans and outcomes, constituting a comprehensive work journal. Our results show that this model not only improves task execution efficiency but also lays a solid foundation for subsequent task analysis and auditing.
</details>
<details>
<summary>摘要</summary>
人工智能代理人 excel 在预定任务执行方面，但是在任务执行过程中的工作状态管理仍然是一个未经探索的领域。我们提出了一种基于“工作笔记”的人工智能代理人模型，该模型包括工作线程对任务监督、规划模块对任务分解和规划、执行模块使用ReAct-类似的思维动作循环来执行具体任务。我们提供了全面的工作状态记录，包括计划和结果，这个全面的工作日志可以帮助进一步分析和审核任务。我们的结果表明，这种模型不仅可以提高任务执行效率，还可以为后续任务分析和审核提供坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="LymphoML-An-interpretable-artificial-intelligence-based-method-identifies-morphologic-features-that-correlate-with-lymphoma-subtype"><a href="#LymphoML-An-interpretable-artificial-intelligence-based-method-identifies-morphologic-features-that-correlate-with-lymphoma-subtype" class="headerlink" title="LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype"></a>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09574">http://arxiv.org/abs/2311.09574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/lymphoml">https://github.com/rajpurkarlab/lymphoml</a></li>
<li>paper_authors: Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar<br>for:* 这份研究旨在开发一个可解释的机器学习方法，以便更正确地分类淋巴癌亚型。methods:* 这个方法包括处理HE染色标本核心、分 segment nuclei 和 cells、计算包括 morphology、texture 和 architecture 的特征，并使用梯度增强模型进行诊断预测。results:* 这个方法的可解释模型，在使用限量HE染色标本上进行训练后，与使用整个标本图像进行诊断相比，具有不 inferior 的诊断精度。* 使用 SHAP 分析法，发现 nuclei 形态特征是 DLBCL 和класси型淋巴癌的最有拘束力的特征（F1-score：78.7%）。* 这个研究还证明了一个结合 H&amp;E 染色标本和标准化的6个免疫标本的模型，可以实现与46个标本 panel 的相似的诊断精度（85.3%）。<details>
<summary>Abstract</summary>
The accurate classification of lymphoma subtypes using hematoxylin and eosin (H&E)-stained tissue is complicated by the wide range of morphological features these cancers can exhibit. We present LymphoML - an interpretable machine learning method that identifies morphologic features that correlate with lymphoma subtypes. Our method applies steps to process H&E-stained tissue microarray cores, segment nuclei and cells, compute features encompassing morphology, texture, and architecture, and train gradient-boosted models to make diagnostic predictions. LymphoML's interpretable models, developed on a limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy to pathologists using whole-slide images and outperform black box deep-learning on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each feature on model prediction and find that nuclear shape features are most discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma (F1-score: 74.5%). Finally, we provide the first demonstration that a model combining features from H&E-stained tissue with features from a standardized panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a 46-stain panel (86.1%).
</details>
<details>
<summary>摘要</summary>
“准确分类淋巴癌亚型使用染色剂和艾索维（H&E）染色的组织是具有广泛的 morphological 特征的复杂任务。我们提出了 LymphoML 方法，这是一种可读性高的机器学习方法，可以识别淋巴癌亚型的 morphologic 特征。我们的方法包括处理 H&E 染色组织微阵列核心、分Segment 细胞和核lei、计算包括形态、Texture 和建筑的特征，并使用梯度优化模型进行诊断预测。LymphoML 的可读性模型，在一小量的 H&E 染色组织上进行开发，与全图像进行诊断的病理学家相比，具有不 inferior 的诊断精度，并且超过黑盒深度学习。使用 SHapley Additive exPlanation（SHAP）分析，我们评估了模型预测中每个特征的影响，发现核lei 形态特征是 DLBCL （F1-score：78.7%）和 класиical Hodgkin 淋巴癌（F1-score：74.5%）中最有决定性的。最后，我们提供了首次证明，一个结合 H&E 染色组织和标准化的6个抗体染色的模型，具有与46个抗体染色的模型（86.1%）相同的诊断精度（85.3%）。”
</details></li>
</ul>
<hr>
<h2 id="Prompt-Optimisation-with-Random-Sampling"><a href="#Prompt-Optimisation-with-Random-Sampling" class="headerlink" title="Prompt Optimisation with Random Sampling"></a>Prompt Optimisation with Random Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09569">http://arxiv.org/abs/2311.09569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Lu, Jiayi Wang, Sebastian Riedel, Pontus Stenetorp</li>
<li>for: 这个论文主要是为了探讨语言模型可以用来生成任务相关的分隔符的可能性，并证明了这种方法可以达到类似于人工curated prompts的性能。</li>
<li>methods: 这篇论文使用了三种Random generation strategies来生成分隔符，包括随机选择 vocabulary 中的token，以及基于语言模型的生成方法。</li>
<li>results: 实验结果显示，使用随机生成的分隔符可以提高 text classification 的性能，相比于人工curated prompts，平均提高16%，并与自动生成的 prompt searching 方法相当。<details>
<summary>Abstract</summary>
Using the generative nature of a language model to generate task-relevant separators has shown competitive results compared to human-curated prompts like "TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as separators can achieve near-state-of-the-art performance. We analyse this phenomenon in detail using three different random generation strategies, establishing that the language space is rich with potential good separators, regardless of the underlying language model size. These observations challenge the common assumption that an effective prompt should be human-readable or task-relevant. Experimental results show that using random separators leads to an average 16% relative improvement across nine text classification tasks on seven language models, compared to human-curated separators, and is on par with automatic prompt searching methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LongBoX-Evaluating-Transformers-on-Long-Sequence-Clinical-Tasks"><a href="#LongBoX-Evaluating-Transformers-on-Long-Sequence-Clinical-Tasks" class="headerlink" title="LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks"></a>LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09564">http://arxiv.org/abs/2311.09564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mihir3009/longbox">https://github.com/mihir3009/longbox</a></li>
<li>paper_authors: Mihir Parmar, Aakanksha Naik, Himanshu Gupta, Disha Agrawal, Chitta Baral</li>
<li>for: 这 paper 旨在评估大型语言模型 (LLMs) 在医疗领域中的长序处理能力。</li>
<li>methods: 该 paper 使用了 seven 个医疗领域的文本数据集，并对两种长序处理技术进行评估：(i) 本地-全局注意力和 (ii) Fusion-in-Decoder (FiD)。</li>
<li>results: 该 paper 的初步实验表明，医疗领域的 LLMs (例如 BioGPT) 和通用领域的 LLMs (例如 FLAN-T5) 在这个 benchmark 上表现不佳，并且两种长序处理技术在一些数据集上得到了混乱的结果。<details>
<summary>Abstract</summary>
Many large language models (LLMs) for medicine have largely been evaluated on short texts, and their ability to handle longer sequences such as a complete electronic health record (EHR) has not been systematically explored. Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts. Motivated by this, we introduce LongBoX, a collection of seven medical datasets in text-to-text format, designed to investigate model performance on long sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We further evaluate two techniques designed for long-sequence handling: (i) local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results demonstrate mixed results with long-sequence handling - while scores on some datasets increase, there is substantial room for improvement. We hope that LongBoX facilitates the development of more effective long-sequence techniques for the medical domain. Data and source code are available at https://github.com/Mihir3009/LongBoX.
</details>
<details>
<summary>摘要</summary>
许多大型语言模型（LLMs）在医学领域的评估主要基于短文本，而对于完整的电子医疗记录（EHR）的评估尚未得到系统性的探讨。考虑到此，我们提出了LongBoX，一个包含七个医学数据集，用于调查模型在长序列上的性能。我们的初步实验表明，医学LLMs（如BioGPT）以及通用领域LLMs（如FLAN-T5）在这个benchmark上表现不佳。我们还评估了两种适合长序列处理的技术：（i）本地-全局注意力，以及（ii）FiD（混合在解码器中）。我们的结果表明，虽有一些数据集的分数提高，但是还有很大的提升空间。我们希望LongBoX可以促进医学领域中更有效的长序列处理技术的发展。数据和源代码可以在https://github.com/Mihir3009/LongBoX上下载。
</details></li>
</ul>
<hr>
<h2 id="Enchancing-Semi-Supervised-Learning-for-Extractive-Summarization-with-an-LLM-based-pseudolabeler"><a href="#Enchancing-Semi-Supervised-Learning-for-Extractive-Summarization-with-an-LLM-based-pseudolabeler" class="headerlink" title="Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler"></a>Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09559">http://arxiv.org/abs/2311.09559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Sahu, Olga Vechtomova, Issam H. Laradji</li>
<li>for: 这个研究是用于解决有限标注数据场景下的抽取文本概要问题。</li>
<li>methods: 这个方法使用一种半supervised的approach，提议一种prompt-based Pseudolabel选择策略，使用GPT-4进行评估和生成 Pseudolabels。</li>
<li>results: 实验表明，通过使用LLM评估和生成 Pseudolabels，可以提高ROUGE-1的表现，在不同的dataset上提高10-20%，与增强预训练模型相当。此外，这种方法需要更少的无标例示例来实现更好的表现。<details>
<summary>Abstract</summary>
This work tackles the task of extractive text summarization in a limited labeled data scenario using a semi-supervised approach. Specifically, we propose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate our method on three text summarization datasets: TweetSumm, WikiHow, and ArXiv/PubMed. Our experiments show that by using an LLM to evaluate and generate pseudolabels, we can improve the ROUGE-1 by 10-20\% on the different datasets, which is akin to enhancing pretrained models. We also show that such a method needs a smaller pool of unlabeled examples to perform better.
</details>
<details>
<summary>摘要</summary>
这个工作面临有限标注数据enario中的抽取文本摘要任务，使用半supervised方法。我们提议一种基于提示的pseudolabel选择策略，使用GPT-4。我们在三个文本摘要数据集上进行了测试：TweetSumm、WikiHow和ArXiv/PubMed。我们的实验结果表明，通过使用LLM评估和生成pseudolabels，可以提高ROUGE-1的表现，在不同的数据集上提高10-20%，这与增强预训练模型相似。此外，我们还发现这种方法需要较少的无标注示例来表现更好。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing.
</details></li>
</ul>
<hr>
<h2 id="Program-Aided-Reasoners-better-Know-What-They-Know"><a href="#Program-Aided-Reasoners-better-Know-What-They-Know" class="headerlink" title="Program-Aided Reasoners (better) Know What They Know"></a>Program-Aided Reasoners (better) Know What They Know</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09553">http://arxiv.org/abs/2311.09553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/houstoncuj/Educating-for-the-Large-Shop-to-make-Custom-Name-Patches">https://github.com/houstoncuj/Educating-for-the-Large-Shop-to-make-Custom-Name-Patches</a></li>
<li>paper_authors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig</li>
<li>for: 该研究旨在评估程序帮助语言模型（PAL）和文本基于的链条（COT）提问技术的准确性和准确性评估结果。</li>
<li>methods: 该研究使用了5个数据集和2种模型类型（LLaMA模型和OpenAI模型），并对PAL和COT技术进行比较。</li>
<li>results: 研究发现，PAL在75%的情况下具有更高的准确性评估结果，并且发现使用温度缩放法可以降低生成的多样性，从而提高PAL的准确性和准确性评估结果。<details>
<summary>Abstract</summary>
Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to "know what they know", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-User-Modeling-Large-scale-Online-User-Representations-for-Ads-Personalization-in-Meta"><a href="#Scaling-User-Modeling-Large-scale-Online-User-Representations-for-Ads-Personalization-in-Meta" class="headerlink" title="Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta"></a>Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09544">http://arxiv.org/abs/2311.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, Kai Wang, Zhangyuan Wang, Zhengxing Chen, Min Li, Fenggang Wu, Minghai Chen, Huayu Li, Yunnan Wu, Zhan Shu, Mindi Yuan, Sri Reddy</li>
<li>for: 该论文旨在提高个性化广告的效果，但是训练吞吐量、服务延迟和内存等约束限制了在线广告排序模型的复杂性和输入特征集。</li>
<li>methods: 作者提出了Scaling User Modeling（SUM）框架，通过一些指定的上游用户模型来合成用户嵌入，从 massive amounts of用户特征中进行高级模型化技术。这些嵌入然后作为下游在线广告排序模型的输入，以提高效率。</li>
<li>results: 作者通过实验证明SUM框架在Meta的广告排序系统中的广泛部署，每天处理数百十亿个用户请求，并且提供了显著的在线指标增长和基础设施成本减少。<details>
<summary>Abstract</summary>
Effective user representations are pivotal in personalized advertising. However, stringent constraints on training throughput, serving latency, and memory, often limit the complexity and input feature set of online ads ranking models. This challenge is magnified in extensive systems like Meta's, which encompass hundreds of models with diverse specifications, rendering the tailoring of user representation learning for each model impractical. To address these challenges, we present Scaling User Modeling (SUM), a framework widely deployed in Meta's ads ranking system, designed to facilitate efficient and scalable sharing of online user representation across hundreds of ads models. SUM leverages a few designated upstream user models to synthesize user embeddings from massive amounts of user features with advanced modeling techniques. These embeddings then serve as inputs to downstream online ads ranking models, promoting efficient representation sharing. To adapt to the dynamic nature of user features and ensure embedding freshness, we designed SUM Online Asynchronous Platform (SOAP), a latency free online serving system complemented with model freshness and embedding stabilization, which enables frequent user model updates and online inference of user embeddings upon each user request. We share our hands-on deployment experiences for the SUM framework and validate its superiority through comprehensive experiments. To date, SUM has been launched to hundreds of ads ranking models in Meta, processing hundreds of billions of user requests daily, yielding significant online metric gains and infrastructure cost savings.
</details>
<details>
<summary>摘要</summary>
实用用户表现是在个性化广告中核心的。然而，训练过程中的约束和服务延迟、内存限制，通常限制了在线广告排序模型的复杂度和输入特征集。这个挑战在Meta的架构中变得更加突出，这里涉及到多种不同的模型，使得为每个模型自适应用户表示学习变得不实际。为解决这些挑战，我们提出了扩展用户模型（SUM）框架，用于在Meta的广告排序系统中实现有效的用户表示共享。SUM使用一些指定的上游用户模型来合成大量用户特征的用户嵌入，然后将这些嵌入作为下游在线广告排序模型的输入，以便有效地共享用户表示。为了适应用户特征的动态变化和确保嵌入的新鲜度，我们设计了SUM在线异步平台（SOAP），该平台具有零延迟的在线服务系统，并且具有模型新鲜度和嵌入稳定性，可以实现在线用户模型更新和用户请求时的嵌入推理。我们在 SUM 框架的部署经验和实验结果中分享我们的经验。至今，SUM 已经在Meta的广告排序系统中发布到了多百个模型，每天处理多百亿次用户请求，并且实现了 significan 的在线指标增长和基础设施成本减少。
</details></li>
</ul>
<hr>
<h2 id="HelpSteer-Multi-attribute-Helpfulness-Dataset-for-SteerLM"><a href="#HelpSteer-Multi-attribute-Helpfulness-Dataset-for-SteerLM" class="headerlink" title="HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"></a>HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09528">http://arxiv.org/abs/2311.09528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, Oleksii Kuchaiev</li>
<li>for: The paper aims to address the problem of existing open-source helpfulness preference datasets not specifying what makes some responses more helpful and others less so, and to provide a solution by collecting a multi-attribute helpfulness dataset annotated for various aspects that make responses helpful.</li>
<li>methods: The paper uses a dataset called HelpSteer, which is a 37k-sample dataset annotated for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. The paper also uses the SteerLM technique to train a model on the dataset.</li>
<li>results: The paper reports that training a model on the HelpSteer dataset with the SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4).<details>
<summary>Abstract</summary>
Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer
</details>
<details>
<summary>摘要</summary>
现有的开源有用性偏好数据集不 specify 有用性回答的特点。模型在这些数据集上训练可能会意外地学习数据集的特性（例如，偏爱 longer  pero 无用的回答只因其长度）。为解决这个问题，我们收集了 HelpSteer 数据集，这是一个多 Attribute 有用性数据集，包括回答的正确性、一致性、复杂性和 verbosity 等方面的标注，以及回答的总有用性。使用 HelpSteer 数据集和 SteerLM 技术训练 Llama 2 70B 模型，得到的分数为 7.54 在 MT Bench，当前为开放模型不需要更强大的模型（如 GPT4）的训练数据而得到的最高分。我们在 https://huggingface.co/datasets/nvidia/HelpSteer 发布了这个数据集，协议为 CC-BY-4.0。
</details></li>
</ul>
<hr>
<h2 id="MDFL-Multi-domain-Diffusion-driven-Feature-Learning"><a href="#MDFL-Multi-domain-Diffusion-driven-Feature-Learning" class="headerlink" title="MDFL: Multi-domain Diffusion-driven Feature Learning"></a>MDFL: Multi-domain Diffusion-driven Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09520">http://arxiv.org/abs/2311.09520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daixun Li, Weiying Xie, Jiaqing Zhang, Yunsong Li</li>
<li>for: 本研究旨在提高高维像素扩展的数据特征提取性能，以揭示高维数据的内在异常和结构。</li>
<li>methods: 该研究提出了一种多域扩散驱动特征学习网络（MDFL），该方法利用扩散基于 posterior 抽样来考虑多个域结构之间的共同信息交互，从而消除视觉模型中的面具效应。此外，该方法还提出了一种特征重用机制，以收集高维数据的深度和原始特征。</li>
<li>results: 实验结果表明，MDFL 可以明显提高高维数据特征提取性能，其平均总准确率达到 98.25%，超过了多种现有基线方案。<details>
<summary>Abstract</summary>
High-dimensional images, known for their rich semantic information, are widely applied in remote sensing and other fields. The spatial information in these images reflects the object's texture features, while the spectral information reveals the potential spectral representations across different bands. Currently, the understanding of high-dimensional images remains limited to a single-domain perspective with performance degradation. Motivated by the masking texture effect observed in the human visual system, we present a multi-domain diffusion-driven feature learning network (MDFL) , a scheme to redefine the effective information domain that the model really focuses on. This method employs diffusion-based posterior sampling to explicitly consider joint information interactions between the high-dimensional manifold structures in the spectral, spatial, and frequency domains, thereby eliminating the influence of masking texture effects in visual models. Additionally, we introduce a feature reuse mechanism to gather deep and raw features of high-dimensional data. We demonstrate that MDFL significantly improves the feature extraction performance of high-dimensional data, thereby providing a powerful aid for revealing the intrinsic patterns and structures of such data. The experimental results on three multi-modal remote sensing datasets show that MDFL reaches an average overall accuracy of 98.25%, outperforming various state-of-the-art baseline schemes. The code will be released, contributing to the computer vision community.
</details>
<details>
<summary>摘要</summary>
高维度图像，rich in semantic information，广泛应用于远程感知和其他领域。图像空间信息反映物体的文本特征，而spectral信息揭示不同频谱域的可能性表示。现在，高维度图像的理解受限于单个领域视角，性能下降。为了解决这个问题，我们提出了一种多领域扩散驱动特征学习网络（MDFL），该方法重新定义了模型真正关注的信息Domain。该方法利用扩散基于 posterior sampling 来显式地考虑高维度映射结构在spectral、 spatial和频率域之间的联合信息互动，从而消除观察者模型中的掩蔽文本效应。此外，我们引入了特征重用机制，以收集高维数据的深度和原始特征。我们示出，MDFL可以显著改善高维数据特征提取性能，从而为揭示高维数据内部征性和结构提供强大的帮助。实验结果表明，MDFL在三个多modal远程感知数据集上达到了98.25%的平均总准确率，超过了多种现状顶峰方案。代码将被发布，为计算机视觉社区做出贡献。
</details></li>
</ul>
<hr>
<h2 id="SegMix-A-Simple-Structure-Aware-Data-Augmentation-Method"><a href="#SegMix-A-Simple-Structure-Aware-Data-Augmentation-Method" class="headerlink" title="SegMix: A Simple Structure-Aware Data Augmentation Method"></a>SegMix: A Simple Structure-Aware Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09505">http://arxiv.org/abs/2311.09505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing</li>
<li>for: 这个论文主要用于提出一种基于 interpolate 的数据扩充（DA）方法，以提高自然语言处理（NLP）任务中的模型性能。</li>
<li>methods: 这个论文使用了 interpolation 方法来 linearly interpolate 训练示例的输入和标签。它还提出了一种名为 SegMix 的数据扩充框架，该框架可以适应任务特定的结构。</li>
<li>results: 实验结果表明，SegMix 可以在Named Entity Recognition (NER) 和 Relation Extraction (RE) 任务中提高性能，特别是在数据缺乏情况下。此外，这种方法较容易实现，增加了训练时间的负担也很小。<details>
<summary>Abstract</summary>
Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.
</details>
<details>
<summary>摘要</summary>
优化数据augmentation（DA）方法（mixup） linearly interpolate 输入和标签的两个或更多的训练例子。mixup在自然语言处理（NLP）领域被应用于序列标注任务。然而，这种直接采用方法不会考虑NLP任务中的结构。为此，我们提议SegMix，一个包含 interpolation-based DA算法的集合，可以适应任务特定的结构。SegMix具有较少的数据结构约束，对各种 гипер参数设置 exhibit robustness, 可以应用于更多的任务设置，并增加了小的计算负担。在算法核心中，我们通过 interpolate 方法在任务特定的有意义段上进行 interpolate，而不是在序列上如先前的工作所做。我们发现SegMix是一个灵活的框架，可以将规则基于DA方法与 interpolate-based方法混合，创造出有趣的DA技术的混合。我们发现SegMix在名实Recognition（NER）和关系抽取（RE）任务中 consistently 提高性能，特别是在数据缺乏的设置下。此外，这种方法易于实现，并且增加了训练过程中的负担。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Interventions-with-User-Defined-Goals-for-Health-Behavior-Change"><a href="#Adaptive-Interventions-with-User-Defined-Goals-for-Health-Behavior-Change" class="headerlink" title="Adaptive Interventions with User-Defined Goals for Health Behavior Change"></a>Adaptive Interventions with User-Defined Goals for Health Behavior Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09483">http://arxiv.org/abs/2311.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aishwarya Mandyam, Matthew Joerke, Barbara E. Engelhardt, Emma Brunskill</li>
<li>for: 本研究旨在提高移动医疗应用的 физи活动促进效果，通过个性化目标设定来提高用户参与度和持续性。</li>
<li>methods: 本研究使用了修改了汤姆逊抽样算法，通过优化个性化奖励函数来实现个性化目标设定。</li>
<li>results: 在物理活动模拟器中，我们的算法可以减少各种基线的累累 regret，并且在不共享数据或不优化个性化奖励函数的情况下具有更好的性能。<details>
<summary>Abstract</summary>
Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret while preserving the sample complexity benefits of data sharing. In a physical activity simulator, we demonstrate that our algorithm achieves substantial improvements in cumulative regret compared to baselines that do not share data or do not optimize for individualized rewards.
</details>
<details>
<summary>摘要</summary>
physical inactivity remains a major public health concern, with associations to adverse health outcomes such as cardiovascular disease and type-2 diabetes. mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, but often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. this paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. as a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. we prove that our modification incurs only a constant penalty on the cumulative regret while preserving the sample complexity benefits of data sharing. in a physical activity simulator, we demonstrate that our algorithm achieves substantial improvements in cumulative regret compared to baselines that do not share data or do not optimize for individualized rewards.Here's the translation in Traditional Chinese as well:体力无动作仍然是公共健康的主要忧虑，与不良的健康结果相关，如心血管疾病和型二糖尿病。 mobilhealth应用程序表示了低成本、扩展性的体育活动促进的吸引点，但通常受到小效果和低投入率的限制，尤其在人类教练相比。 目标设定是体育教练中的重要 Component，对于移动健康应用程序的自适性优化，尚未获得充分利用。 本文介绍了对 Thompson 抽样算法的修改，将优先级设置为个人化的目标设定，通过优化个人化的赏金函数来实现。 为支持目标设定，本文提出了一种均衡的方法，可以利用共享结构，同时优化个人偏好和目标。 我们证明了我们的修改仅增加了常数的责任，保留了数据分享的样本复杂性的好处。 在物理活动 simulator 中，我们显示了我们的算法在不共享数据或不优化个人化赏金函数的基准下，实现了很大的累累 regret。
</details></li>
</ul>
<hr>
<h2 id="ARES-An-Automated-Evaluation-Framework-for-Retrieval-Augmented-Generation-Systems"><a href="#ARES-An-Automated-Evaluation-Framework-for-Retrieval-Augmented-Generation-Systems" class="headerlink" title="ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"></a>ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09476">http://arxiv.org/abs/2311.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-futuredata/ares">https://github.com/stanford-futuredata/ares</a></li>
<li>paper_authors: Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia</li>
<li>for: 评估抽取增强生成（RAG）系统的质量，通常需要手动标注输入查询、文章和回答。</li>
<li>methods: 我们介绍了一个自动化的RAG评估系统（ARES），用于评估RAG系统的上下文相关性、答案准确性和答案相关性。ARES使用灵活的语言模型（LM）judge进行评估，并使用小量人工标注数据进行预测推断（PPI）来减少预测错误。</li>
<li>results: ARES可以准确评估RAG系统，只需要使用几百个人工标注数据进行评估。此外，ARES的评估标准可以适应不同的领域和类型的查询和文章，并保持评估标准的有效性。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/stanford-futuredata/ARES%E4%B8%8A%E4%B8%8B%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/stanford-futuredata/ARES上下载和使用。</a><details>
<summary>Abstract</summary>
Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. Using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.
</details>
<details>
<summary>摘要</summary>
evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. we introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. to mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. we make our datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.Here's the translation in Traditional Chinese:评估 Retrieval-augmented Generation (RAG) 系统传统上靠手动标注 Input queries、Passages to retrieve 和 Response 来评估。我们介绍 ARES，一个自动化 RAG 评估系统，可以在 Context relevance、Answer faithfulness 和 Answer relevance 的维度上评估 RAG 系统。使用人工训练数据，ARES 精确地评估 RAG 系统中各个元件的质量。为了减少预测错误，ARES 使用一小量的人工标注数据进行预测力测试 (PPI)。在 KILT 和 SuperGLUE 中的六个知识密集任务上，ARES 精确地评估 RAG 系统，仅需使用一些百个人工标注。此外，ARES 的评估判别器还能够在领域转移时保持有效，并在改变查询和/或文档类型时仍然精确地评估 RAG 系统。我们在 <https://github.com/stanford-futuredata/ARES> 上提供了数据和代码，以便复制和部署。
</details></li>
</ul>
<hr>
<h2 id="JAB-Joint-Adversarial-Prompting-and-Belief-Augmentation"><a href="#JAB-Joint-Adversarial-Prompting-and-Belief-Augmentation" class="headerlink" title="JAB: Joint Adversarial Prompting and Belief Augmentation"></a>JAB: Joint Adversarial Prompting and Belief Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09473">http://arxiv.org/abs/2311.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Jwala Dhamala, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</li>
<li>for: 这篇论文的目的是提高语言模型的安全性和可靠性。</li>
<li>methods: 这篇论文使用了对目标模型的挑战性询问和信念增强，通过迭代反馈循环来提高挑战性询问和信念增强的效果。</li>
<li>results: 在实验中，这篇论文显示了这种框架可以在动态和静态情况下降低对目标模型的攻击性内容生成。<details>
<summary>Abstract</summary>
With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.
</details>
<details>
<summary>摘要</summary>
随着语言模型在不同应用场景中的普及，对这些模型的安全性和可靠性的关注也在不断增加。我们在这篇文章中提出了一种联合框架，通过对黑盒目标模型进行抗对抗探测和信念增强，使其具备更高的安全性和可靠性。这种框架利用自动化的红团攻击方法来探测目标模型，同时使用信念增强器来生成对目标模型进行增强其对抗性的指令。重要的是，抗对抗模型和信念生成器都利用过去互动的反馈来提高对抗提示和信念的效果。在我们的实验中，我们证明了这种框架可以在直接交互的动态场景中减少毒害内容生成，以及使用静态 benchmark数据集来评估我们的模型时也能减少毒害内容生成。
</details></li>
</ul>
<hr>
<h2 id="Think-While-You-Write-Hypothesis-Verification-Promotes-Faithful-Knowledge-to-Text-Generation"><a href="#Think-While-You-Write-Hypothesis-Verification-Promotes-Faithful-Knowledge-to-Text-Generation" class="headerlink" title="Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation"></a>Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09467">http://arxiv.org/abs/2311.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifu Qiu, Varun Embar, Shay B. Cohen, Benjamin Han</li>
<li>for: 提高神经网络生成模型的 faithfulness，减少幻像现象</li>
<li>methods: 提出了一种新的解码方法 called TWEAK，使用假设验证模型（HVM）对生成的语句进行排序，以确保语句与输入信息一致</li>
<li>results: TWEAK variants 在 FactKB、WebNLG 和 TekGen&#x2F;GenWiki 上的 faithfulness 和 quality 都得到了提高，但是唯一的代价是 slight degradation （0.14&#x2F;0.32 points）in quality measured by BERTScore。<details>
<summary>Abstract</summary>
Neural knowledge-to-text generation models often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the given facts, or describe facts not present in the input. To reduce hallucinations, we propose a novel decoding method, TWEAK (Think While Effectively Articulating Knowledge). TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on how well their corresponding hypotheses support the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with minimal impact on the quality. We then replace the NLI model with our task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs input facts with their faithful and hallucinated descriptions with the hallucinated spans marked. The new HVM improves the faithfulness and the quality further and runs faster. Overall the best TWEAK variants improve on average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality measured by BERTScore over the same datasets. Since TWEAK is a decoding-only approach, it can be integrated with any neural generative model without retraining.
</details>
<details>
<summary>摘要</summary>
neural knowledge-to-text生成模型经常难以准确地生成输入事实的描述：它们可能会产生幻觉，或者描述不在输入中的事实。为了减少幻觉，我们提出了一种新的解码方法，叫做调整（Think While Effectively Articulating Knowledge）。调整方法会在每个解码步骤中对生成的序列和未来序列进行处理，并将每个生成候选者根据其对输入事实的支持程度进行排序，使用一个假设验证模型（HVM）。我们首先使用一个自然语言推理（NLI）模型作为HVM，并发现使用NLI模型可以提高准确性，同时具有最小的影响。然后，我们将NLI模型 replaced with我们自己的任务特定的HVM，用一个具有唯一性的数据集—— факт-alignment textual entailment（FATE），这个数据集 pairs输入事实与其准确的描述和幻觉描述的幻觉 span。新的HVM可以进一步提高准确性和质量，同时具有更快的运行速度。总的来说，最佳的调整变体可以在FactKB上提高准确性平均2.22/7.17点，同时保持质量水平，BERTScore上的平均下降0.14/0.32点。由于调整是解码-only方法，因此可以与任何 neural生成模型无需重新训练。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.AI_2023_11_16/" data-id="clp89do9l007di788gkxp88os" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.CL_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T11:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.CL_2023_11_16/">cs.CL - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Latent-Feature-based-Data-Splits-to-Improve-Generalisation-Evaluation-A-Hate-Speech-Detection-Case-Study"><a href="#Latent-Feature-based-Data-Splits-to-Improve-Generalisation-Evaluation-A-Hate-Speech-Detection-Case-Study" class="headerlink" title="Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study"></a>Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10236">http://arxiv.org/abs/2311.10236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maikezuefle/latent-feature-splits">https://github.com/maikezuefle/latent-feature-splits</a></li>
<li>paper_authors: Maike Züfle, Verna Dankers, Ivan Titov</li>
<li>for: 提高社交媒体平台上的仇恨言语检测系统的Robustness，避免模型过拟合特定目标和关键词。</li>
<li>methods: 使用新的训练测试分割方法，包括Subset-Sum-Split和Closest-Split，对两个数据集和四个预训练模型进行测试。</li>
<li>results: 研究发现，当模型面临到偏移的数据分布时，其表现很差，这表明任务难度不一定是人类可理解的。研究还发现，不同的数据分布下的模型表现差异很大，建议在模型开发和评估中使用矩阵特征基于的分割方法。<details>
<summary>Abstract</summary>
With the ever-growing presence of social media platforms comes the increased spread of harmful content and the need for robust hate speech detection systems. Such systems easily overfit to specific targets and keywords, and evaluating them without considering distribution shifts that might occur between train and test data overestimates their benefit. We challenge hate speech models via new train-test splits of existing datasets that rely on the clustering of models' hidden representations. We present two split variants (Subset-Sum-Split and Closest-Split) that, when applied to two datasets using four pretrained models, reveal how models catastrophically fail on blind spots in the latent space. This result generalises when developing a split with one model and evaluating it on another. Our analysis suggests that there is no clear surface-level property of the data split that correlates with the decreased performance, which underscores that task difficulty is not always humanly interpretable. We recommend incorporating latent feature-based splits in model development and release two splits via the GenBench benchmark.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的不断普及，恶意内容的散布也在不断增加，需要建立有力的恶意言语检测系统。但是现有的检测系统容易过拟合特定目标和关键词，而不充分考虑数据分布的变化可能会发生在训练和测试数据之间。我们通过新的训练测试分割方法来挑战恶意言语模型。我们提出了两种分割方法（子集和最近的分割），当应用于两个数据集上四个预训练模型时，发现模型在隐藏表示空间中的缺陷。这个结果普适地发生在开发新的分割和使用另一个模型进行评估。我们的分析表明，不存在明确的表层特征，可以用来判断任务难度。我们建议在模型开发和发布中包含隐藏特征基于的分割。我们释放了两个分割，并将其作为GenBench测试套件。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Familiarity-on-Naming-Variation-A-Study-on-Object-Naming-in-Mandarin-Chinese"><a href="#The-Impact-of-Familiarity-on-Naming-Variation-A-Study-on-Object-Naming-in-Mandarin-Chinese" class="headerlink" title="The Impact of Familiarity on Naming Variation: A Study on Object Naming in Mandarin Chinese"></a>The Impact of Familiarity on Naming Variation: A Study on Object Naming in Mandarin Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10181">http://arxiv.org/abs/2311.10181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunke He, Xixian Liao, Jialing Liang, Gemma Boleda</li>
<li>for: 这研究探讨了不同说话人之间对同一个对象或实体的命名方式的差异。</li>
<li>methods: 研究人员使用了语言和视觉数据集，对1319个自然的图像进行了20个不同的命名。他们 investigate了对象之familiarity对命名差异的影响。</li>
<li>results: 研究发现， Familiarity会影响命名差异，有两种方式：一是通过扩展词汇，使命名更加多样化；二是通过推广标准名称，使命名更加统一。研究 illustrate了如何使用计算机资源来解决认知科学问题。<details>
<summary>Abstract</summary>
Different speakers often produce different names for the same object or entity (e.g., "woman" vs. "tourist" for a female tourist). The reasons behind variation in naming are not well understood. We create a Language and Vision dataset for Mandarin Chinese that provides an average of 20 names for 1319 naturalistic images, and investigate how familiarity with a given kind of object relates to the degree of naming variation it triggers across subjects. We propose that familiarity influences naming variation in two competing ways: increasing familiarity can either expand vocabulary, leading to higher variation, or promote convergence on conventional names, thereby reducing variation. We find evidence for both factors being at play. Our study illustrates how computational resources can be used to address research questions in Cognitive Science.
</details>
<details>
<summary>摘要</summary>
不同的说话人经常生成不同的名称 для同一个物体或实体（例如，"女性旅客" vs. "旅客"  для女性旅客）。名称的变化原因还不很清楚。我们创建了一个语言和视觉数据集 для普通话，提供了每个图像的平均20个名称，并研究了对象的 familiairity 如何影响名称的变化。我们提出了两种可能的影响因素：增加familiarity可能会扩展词汇，导致更高的变化，或者推动对常见名称的共识，从而减少变化。我们发现了这两种因素都在运作。我们的研究示例了如何使用计算机资源解决认知科学研究问题。
</details></li>
</ul>
<hr>
<h2 id="JWSign-A-Highly-Multilingual-Corpus-of-Bible-Translations-for-more-Diversity-in-Sign-Language-Processing"><a href="#JWSign-A-Highly-Multilingual-Corpus-of-Bible-Translations-for-more-Diversity-in-Sign-Language-Processing" class="headerlink" title="JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing"></a>JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10174">http://arxiv.org/abs/2311.10174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shesterg/jwsign-machine-translation">https://github.com/shesterg/jwsign-machine-translation</a></li>
<li>paper_authors: Shester Gueuwou, Sophie Siake, Colin Leong, Mathias Müller</li>
<li>for: 本研究的目的是提供一个大型、多语言的手语译写数据集，以促进手语译写、翻译和生成任务的进步。</li>
<li>methods: 本研究使用的方法包括对JWSign数据集进行神经机器翻译实验，以及为不同的语言组合实现多语言系统。</li>
<li>results: 实验结果显示，使用多语言系统可以超越双语基eline系统，且在较高资源enario中，Language pairs的类型相似性 clustering 可以提高翻译质量。<details>
<summary>Abstract</summary>
Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world's sign languages has widened the gap in this field, resulting in a few sign languages being studied more than others, making this research area extremely skewed mostly towards sign languages from high-income countries. In this work we introduce a new large and highly multilingual dataset for sign language translation: JWSign. The dataset consists of 2,530 hours of Bible translations in 98 sign languages, featuring more than 1,500 individual signers. On this dataset, we report neural machine translation experiments. Apart from bilingual baseline systems, we also train multilingual systems, including some that take into account the typological relatedness of signed or spoken languages. Our experiments highlight that multilingual systems are superior to bilingual baselines, and that in higher-resource scenarios, clustering language pairs that are related improves translation quality.
</details>
<details>
<summary>摘要</summary>
技术进步受到了数据不足的阻碍，妨碍了认知、翻译和生产任务的进步。全球各种手语的缺乏完整的数据集，使得这一领域的研究受到了极大的偏袋，大多数研究集中在高收入国家的手语上进行，这使得这个领域的研究非常偏向高收入国家的手语。在这项工作中，我们介绍了一个新的大型、多语言的手语翻译数据集：JWSign。该数据集包括98种手语的2,530小时的圣经翻译，共有1,500名个体手语演示者。在这个数据集上，我们报告了神经机器翻译实验。除了双语基eline系统，我们还训练了多语言系统，其中一些考虑了手语或口语语言之间的类型学关系。我们的实验表明，多语言系统比双语基eline系统更高效，而在更高资源的场景下，将相关的语言对 grouping 可以提高翻译质量。
</details></li>
</ul>
<hr>
<h2 id="A-Computationally-Efficient-Sparsified-Online-Newton-Method"><a href="#A-Computationally-Efficient-Sparsified-Online-Newton-Method" class="headerlink" title="A Computationally Efficient Sparsified Online Newton Method"></a>A Computationally Efficient Sparsified Online Newton Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10085">http://arxiv.org/abs/2311.10085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fnu Devvrit, Sai Surya Duvvuri, Rohan Anil, Vineet Gupta, Cho-Jui Hsieh, Inderjit Dhillon</li>
<li>for: 这篇论文的目的是提出一种可扩展的第二类方法，以提高深度神经网络训练的快速度和效率。</li>
<li>methods: 这篇论文使用了一种称为SONew的新方法，它是一种具有优化的条件系统，可以实现高效的深度神经网络训练。这个方法基于LogDet矩阵差分量的新用途，并且运用了简洁条件以减少遗传 regret。</li>
<li>results: 这篇论文的实验结果显示，SONew方法可以实现30%更快的快速度，3.4%的效能提升，并且80%的训练损失减少，相比于内存高效的优化器，包括第一类方法。此外，SONew方法可以实现高度的并行和高效率的实现，并且可以轻松地扩展到大规模的实验。<details>
<summary>Abstract</summary>
Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to 30% faster convergence, 3.4% relative improvement in validation performance, and 80% relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about 3% slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devvrit/SONew
</details>
<details>
<summary>摘要</summary>
第二顺序方法具有增强深度神经网络训练的潜在潜力，但它们的巨大内存和计算需求限制了它们的实用性。因此，有一个需求是可扩展的第二顺序方法，可以高效地训练大型模型。在这篇文章中，我们介绍了简化在线新点方法（SONew），它是一个具有优化组件的内存有效的第二顺序方法。我们使用了一个新的LogDet矩阵差异测度，并与简化条件相结合，以减少在线凸优化框架中的遗憾。我们对大规模benchmark进行实验，获得了30%的更快的渐进、3.4%的相对提高验证性能，以及80%的相对提高训练损失。相比之下，内存高效的优化器，包括首顺序方法，SONew具有更高的实用性。实际上，我们发现，对大型benchmark的应用，Shampoo方法无法扩展，并且需要较多的工程实践来扩展。相比之下，SONew方法具有更直接的实现方式，增加了它的实用性。SONew代码可以在以下链接获取：https://github.com/devvrit/SONew。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Tradeoffs-in-Language-Model-Decoding-with-Informational-Interpretations"><a href="#Characterizing-Tradeoffs-in-Language-Model-Decoding-with-Informational-Interpretations" class="headerlink" title="Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations"></a>Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10083">http://arxiv.org/abs/2311.10083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-Ching Chang, William W. Cohen, Yun-Hsuan Sung</li>
<li>for: 这 paper 是为了提出一种语言模型预测器的理论框架，用于解决预测器的设计问题。</li>
<li>methods: 这 paper 使用动态 программирова法和信息理论来描述语言模型预测器的算法。它将预测器的设计从 logit 空间提升到 action-state value function 空间，并将每个组件在这个空间中的解释。</li>
<li>results: 这 paper 显示了如何通过优化 action-state value functions，以获得更好的预测性和可解释性。这些结果可以帮助解决预测器的质量和性能问题。<details>
<summary>Abstract</summary>
We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory. With dynamic programming, we lift the design of decoder algorithms from the logit space to the action-state value function space, and show that the decoding algorithms are consequences of optimizing the action-state value functions. Each component in the action-state value function space has an information theoretical interpretation. With the lifting and interpretation, it becomes evident what the decoder algorithm is optimized for, and hence facilitating the arbitration of the tradeoffs in sensibleness, diversity, and attribution.
</details>
<details>
<summary>摘要</summary>
我们提出了一个理论框架，用于形式化语言模型decoder算法的动态计划和信息理论。通过动态计划，我们将decoder算法的设计从ilogit空间提升到动作-状态价值函数空间，并显示出decoding算法是优化动作-状态价值函数的结果。每个动作-状态价值函数空间中的组件都有信息理论的解释。通过提升和解释，就可以看出decoder算法是优化什么，因此可以促进折衔敏感、多样性和责任的衡量。
</details></li>
</ul>
<hr>
<h2 id="DRESS-Instructing-Large-Vision-Language-Models-to-Align-and-Interact-with-Humans-via-Natural-Language-Feedback"><a href="#DRESS-Instructing-Large-Vision-Language-Models-to-Align-and-Interact-with-Humans-via-Natural-Language-Feedback" class="headerlink" title="DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"></a>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10081">http://arxiv.org/abs/2311.10081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran</li>
<li>for: The paper aims to improve the performance of large vision language models (LVLMs) by incorporating natural language feedback (NLF) to enhance their alignment and interactions.</li>
<li>methods: The paper proposes a novel categorization of NLF into two types: critique and refinement, and uses conditional reinforcement learning to train the LVLMs to incorporate feedback in multi-turn interactions.</li>
<li>results: The paper shows that the proposed method, called DRESS, can generate more helpful, honest, and harmless responses, and more effectively learn from feedback during multi-turn interactions compared to state-of-the-art LVLMs.<details>
<summary>Abstract</summary>
We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.
</details>
<details>
<summary>摘要</summary>
我们介绍DRESS，一个大型视觉语言模型（LVLM），它创新地利用自然语言反馈（NLF）来提高对人类偏好的调整和互动。现有LVLMs通常只通过 instrucion 精化阶段来提高对人类偏好的调整，而无法采用更多的反馈来减少生成无用、幻想或危险的回答。其次，视觉指令练习数据通常是多turn对话格式，但连续对话中的连接和依赖关系较弱，这限制了LVLMs的有效多turn互动能力。为此，我们提出了一种新的NLF分类方法：批评和细化。批评NLF可以识别回答的优缺点，并用于对LVLMs进行调整，使其更加适应人类偏好。细化NLF可以提供具体的改进建议，并被用来提高LVLMs的互动能力，即LVLMs能够通过反馈进行多turn互动中的改进。由于NLF的非准确性，我们扩展了条件奖励学习的训练方法。我们的实验结果表明，DRESS可以生成更有用（9.76%）、诚实（11.52%）和无害（21.03%）的回答，并在多turn互动中更好地学习反馈。
</details></li>
</ul>
<hr>
<h2 id="Unambiguity-and-Fewness-for-Nonuniform-Families-of-Polynomial-Size-Nondeterministic-Finite-Automata"><a href="#Unambiguity-and-Fewness-for-Nonuniform-Families-of-Polynomial-Size-Nondeterministic-Finite-Automata" class="headerlink" title="Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata"></a>Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09979">http://arxiv.org/abs/2311.09979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoyuki Yamakami</li>
<li>for: 这个论文主要针对的是非征Compatibility promise decision问题的解决方案。</li>
<li>methods: 论文使用了非征Compatibility families of polynomial-size finite automata，这些自动机有 polynomially many inner states，来解决这些问题。</li>
<li>results: 论文表明，在一些特定情况下，这些非征Compatibility families of finite automata 有不同的计算能力，而且两种方法（一个方法是限制机器只能做一个方向的移动，另一个方法是限制机器的长度为 polynomially-bounded）是等价的。<details>
<summary>Abstract</summary>
Nonuniform families of polynomial-size finite automata, which are series of indexed finite automata having polynomially many inner states, are used in the past literature to solve nonuniform families of promise decision problems. Among such nonuniform families of finite automata, we focus our attention, in particular, on the variants of nondeterministic finite automata, which have at most "one" (unambiguous), "polynomially many" (few) accepting computation paths, or unambiguous/few computation paths leading to each fixed configuration. When such machines are limited to make only one-way head moves, we can prove with no unproven hardness assumptions that some of these variants are different in computational power from each other. As for two-way machines restricted to instances of polynomially-bounded length, families of two-way polynomial-size nondeterministic finite automata are equivalent in power to families of polynomial-size unambiguous finite automata.
</details>
<details>
<summary>摘要</summary>
非均匀家族的多项式大小自动机，这是在过去文献中用于解决非均匀家族的Promise决策问题的工具。我们在这些非均匀自动机家族中特别关注尝试机器，它们在最多只能有一个（不ambiguous）， polynomially many（少）的接受计算路径，或者每个固定配置都有一个或 polynomially many 的计算路径。当这些机器只能做一个一向头移时，我们可以证明不带任何难度假设的情况下，这些变体之间存在不同的计算能力。而两个方向的机器，限制到 polynomially-bounded 长度的实例时， families of two-way polynomial-size nondeterministic finite automata 和 families of polynomial-size unambiguous finite automata 之间存在相同的计算能力。
</details></li>
</ul>
<hr>
<h2 id="Hijacking-Large-Language-Models-via-Adversarial-In-Context-Learning"><a href="#Hijacking-Large-Language-Models-via-Adversarial-In-Context-Learning" class="headerlink" title="Hijacking Large Language Models via Adversarial In-Context Learning"></a>Hijacking Large Language Models via Adversarial In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09948">http://arxiv.org/abs/2311.09948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Qiang, Xiangyu Zhou, Dongxiao Zhu</li>
<li>for: 这个论文是为了研究受限下的语言模型（LLM）在特定任务上的性能，并使用示例作为示范来启用LLM。</li>
<li>methods: 这篇论文使用了梯度基于的搜索方法来学习并贴加不可见的反对示例，以控制LLM生成的响应。</li>
<li>results: 实验结果表明，这种攻击可以让LLM生成targeted的不良输出，通过吸引LLM的注意力向 adversarial tokens。<details>
<summary>Abstract</summary>
In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.
</details>
<details>
<summary>摘要</summary>
内容学习（ICL）已经 emerged as a powerful paradigm，利用 LLMS  для特定任务，通过使用标签的示例作为条件答案。 despite its promising performance， ICL 受到示例选择和排序的不稳定性问题。 In addition, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMS to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.
</details></li>
</ul>
<hr>
<h2 id="An-Attention-Based-Denoising-Framework-for-Personality-Detection-in-Social-Media-Texts"><a href="#An-Attention-Based-Denoising-Framework-for-Personality-Detection-in-Social-Media-Texts" class="headerlink" title="An Attention-Based Denoising Framework for Personality Detection in Social Media Texts"></a>An Attention-Based Denoising Framework for Personality Detection in Social Media Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09945">http://arxiv.org/abs/2311.09945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/once2gain/personalitydetection">https://github.com/once2gain/personalitydetection</a></li>
<li>paper_authors: Qirui Tang, Wenkang Jiang, Yihua Du, Lei Lin</li>
<li>for: 这篇论文旨在提高社交媒体文本中人格检测的准确率，并提供一种基于注意力的信息提取机制和人格检测框架。</li>
<li>methods: 论文提出了一种基于注意力的信息提取机制（AIEM），以及一种基于注意力的干扰除框架（ADF），以提高人格检测的准确率。</li>
<li>results: 论文在两个常用的数据集上达到了当前领域的最佳性能水平，具体来说是在Twitter-Myers-Briggs Type Indicator（Twitter-MBTI）数据集上的平均准确率提高10.2%。<details>
<summary>Abstract</summary>
In social media networks, users produce a large amount of text content anytime, providing researchers with a valuable approach to digging for personality-related information. Personality detection based on user-generated texts is a universal method that can be used to build user portraits. The presence of noise in social media texts hinders personality detection. However, previous studies have not fully addressed this challenge. Inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (AIEM) for long texts, which is applied to quickly locate valuable pieces of information, and focus more attention on the deep semantics of key pieces. Then, we provide a novel attention-based denoising framework (ADF) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets. Notably, we obtain an average accuracy improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed light on how AIEM works to magnify personality-related signals.
</details>
<details>
<summary>摘要</summary>
在社交媒体网络中，用户生成大量文本内容，为研究人员提供了一个价值的检测人格信息的方法。基于用户生成的文本进行人格检测是一种通用的方法，可以 construir 用户肖像。但是，社交媒体文本中的噪声会妨碍人格检测。然而，先前的研究并没有彻底解决这个挑战。我们提出了基于扫描阅读技术的注意力基本信息提取机制（AIEM），用于快速找到有价值信息，并更多地关注深层 semantics 的关键 Piece。然后，我们提出了一种基于注意力的噪声降减框架（ADF），用于人格检测任务，并在两个常用的数据集上实现了状态的表现。特别是，我们在 Twitter-Myers-Briggs Type Indicator（Twitter-MBTI）数据集上实现了10.2%的平均准确率提升。我们在 GitHub 上公开了我们的代码。我们探讨了 AIEM 如何强调人格相关的信号。
</details></li>
</ul>
<hr>
<h2 id="Language-Generation-from-Human-Brain-Activities"><a href="#Language-Generation-from-Human-Brain-Activities" class="headerlink" title="Language Generation from Human Brain Activities"></a>Language Generation from Human Brain Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09889">http://arxiv.org/abs/2311.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Ye, Qingyao Ai, Yiqun Liu, Min Zhang, Christina Lioma, Tuukka Ruotsalo</li>
<li>for: 这个研究旨在开发一个基于脑computer interfaces（BCIs）的语言生成系统，以便无需侵入性地传入语言。</li>
<li>methods: 这个研究使用了大型语言模型（LLM）和semantic brain decoder来直接从functional magnetic resonance imaging（fMRI）输入中生成语言。</li>
<li>results: 研究发现，这个模型可以对于视觉或听觉语言刺激而生成协调的语言序列，并且与脑Input的内容有着Semantic相似性。相比之下，随机控制和预先生成的语言选择方法，以及标准的LLM，它们只能生成基于语言训练数据的通用单词次序。<details>
<summary>Abstract</summary>
Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates. We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data. The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled. Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation.
</details>
<details>
<summary>摘要</summary>
使用非侵入式脑计算机接口（BCI）生成人类语言有很多应用前途，如服务残疾患者和改善沟通。然而，目前通过BCI生成语言仅限于在分类设置中选择预生成的句子续写候选者中的最有可能性的 cortical semantic representation。 inspirited by recent research that revealed associations between the brain and large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates. We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data. The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled. Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Which-Modality-should-I-use-–-Text-Motif-or-Image-Understanding-Graphs-with-Large-Language-Models"><a href="#Which-Modality-should-I-use-–-Text-Motif-or-Image-Understanding-Graphs-with-Large-Language-Models" class="headerlink" title="Which Modality should I use – Text, Motif, or Image? : Understanding Graphs with Large Language Models"></a>Which Modality should I use – Text, Motif, or Image? : Understanding Graphs with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09862">http://arxiv.org/abs/2311.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang</li>
<li>for: 本研究旨在探讨如何更好地融合图数据与大语言模型（LLMs），以提高LLMs在复杂图结构分析中的效iveness。</li>
<li>methods: 本研究使用了不同的编码方式（如文本、图像和模式）和不同的推荐方法来优化LLMs在处理复杂图结构时的表现。</li>
<li>results: 研究发现，图像模式，尤其是通过高级见语言模型like GPT-4V支持，比文本更有效地管理токен限制而保留重要信息。研究还探讨了不同因素对每种编码模式表现的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs) are revolutionizing various fields by leveraging large text corpora for context-aware intelligence. Due to the context size, however, encoding an entire graph with LLMs is fundamentally limited. This paper explores how to better integrate graph data with LLMs and presents a novel approach using various encoding modalities (e.g., text, image, and motif) and approximation of global connectivity of a graph using different prompting methods to enhance LLMs' effectiveness in handling complex graph structures. The study also introduces GraphTMI, a new benchmark for evaluating LLMs in graph structure analysis, focusing on factors such as homophily, motif presence, and graph difficulty. Key findings reveal that image modality, supported by advanced vision-language models like GPT-4V, is more effective than text in managing token limits while retaining critical information. The research also examines the influence of different factors on each encoding modality's performance. This study highlights the current limitations and charts future directions for LLMs in graph understanding and reasoning tasks.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxíng yǔyán módelǐ)* "revolutionizing" is translated as "革命化" (gémònghuà)* "various fields" is translated as "多个领域" (duō gè lǐngyù)* "leveraging large text corpora" is translated as "利用大量文本资料" (lìyòng dàliàng wén tiě xīn yǎng)* "context-aware intelligence" is translated as "Context-aware intelligence" (上下文意识)* "encoding an entire graph" is translated as "完整的图形编码" (quánzhì de túxíng biān mǎ)* "fundamentally limited" is translated as "基本上有限" (jībǎo shang yǒu xiàn)* "novel approach" is translated as "新的方法" (xīn de fāngfǎ)* "using various encoding modalities" is translated as "使用多种编码方式" (shǐyòu duōshì biān mǎ fāngshì)* "approximation of global connectivity" is translated as "全球连接的估计" (quánqiú liánjiē de gèjì)* "different prompting methods" is translated as "不同的提示方法" (bùdōng de tímí fāngfǎ)* "enhance LLMs' effectiveness" is translated as "增强LLMs的效果" (zēngcháng LLMs de xiànggòu)* "in handling complex graph structures" is translated as "处理复杂的图形结构" (chùzhì fùzì de túxíng jiégòu)* "Key findings reveal" is translated as "主要发现是" (zhǔyào fāxìn shì)* "image modality" is translated as "图像模式" (túxíang móshì)* "supported by advanced vision-language models" is translated as "由高级视语言模型支持" (yǐ gāojí wèi yǔ yǔ móshì)* "more effective than text" is translated as "比文本更有效" (bǐ wén tiěn jí yòu yì)* "managing token limits" is translated as "管理 токен限制" (guǎn lǐ tóu kē yùn xiàn)* "while retaining critical information" is translated as "保留关键信息" (bǎo liú guān jí xìn xīn)* "The research also examines the influence of different factors" is translated as "研究也研究了不同因素的影响" (yánjiū yě yánjiū le bùdàng yīn xiǎng de yìngxìn)* "on each encoding modality's performance" is translated as "每种编码方式的性能" (měi zhǒng biān mǎ fāngshì de xìngnéng)* "This study highlights the current limitations" is translated as "这项研究透视当前的限制" (zhè yè yánjiū tòu shì dāng zhì de jiàn zhì)* "and charts future directions" is translated as "并规划未来的发展" (dàn zhì yú yì yè yì)
</details></li>
</ul>
<hr>
<h2 id="GSAP-NER-A-Novel-Task-Corpus-and-Baseline-for-Scholarly-Entity-Extraction-Focused-on-Machine-Learning-Models-and-Datasets"><a href="#GSAP-NER-A-Novel-Task-Corpus-and-Baseline-for-Scholarly-Entity-Extraction-Focused-on-Machine-Learning-Models-and-Datasets" class="headerlink" title="GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets"></a>GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09860">http://arxiv.org/abs/2311.09860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wolfgang Otto, Matthäus Zloch, Lu Gan, Saurav Karmakar, Stefan Dietze</li>
<li>for: 本研究旨在提供精细化的机器学习模型和数据集的实体识别模型，以便更好地理解它们在学术论文中的提及。</li>
<li>methods: 本研究使用了一种基于BERT的首个基线模型，以及一个手动注解的全文科学论文集。</li>
<li>results: 本研究发现了10种不同的实体类型，包括机器学习模型和数据集，并提供了一个手动注解的全文科学论文集，以便进一步研究和应用。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets. In order to provide a nuanced understanding of how ML models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like "our BERT-based model" or "an image CNN". You can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.
</details>
<details>
<summary>摘要</summary>
Named Entity Recognition (NER) 模型在各种自然语言处理（NLP）任务中扮演着关键的角色，包括信息抽取（IE）和文本理解。在学术写作中，关于机器学习模型和数据集的引用是学术出版物的重要组成部分，需要准确的模型来识别。despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like machine learning model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around machine learning models and datasets. In order to provide a nuanced understanding of how machine learning models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like "our BERT-based model" or "an image CNN". You can find the ground truth dataset and code to replicate model training at <https://data.gesis.org/gsap/gsap-ner>.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Overview-of-the-HASOC-Subtrack-at-FIRE-2023-Identification-of-Tokens-Contributing-to-Explicit-Hate-in-English-by-Span-Detection"><a href="#Overview-of-the-HASOC-Subtrack-at-FIRE-2023-Identification-of-Tokens-Contributing-to-Explicit-Hate-in-English-by-Span-Detection" class="headerlink" title="Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English by Span Detection"></a>Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English by Span Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09834">http://arxiv.org/abs/2311.09834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Masud, Mohammad Aflah Khan, Md. Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在开发计算方法来减少网络上的仇恨言论。</li>
<li>methods: 本研究使用黑盒模型来识别仇恨内容，并提供了一种可能的重写建议。</li>
<li>results: 研究发现，使用这种方法可以减少Explicit span detection in English Tweets，最高macro-F1达0.58。<details>
<summary>Abstract</summary>
As hate speech continues to proliferate on the web, it is becoming increasingly important to develop computational methods to mitigate it. Reactively, using black-box models to identify hateful content can perplex users as to why their posts were automatically flagged as hateful. On the other hand, proactive mitigation can be achieved by suggesting rephrasing before a post is made public. However, both mitigation techniques require information about which part of a post contains the hateful aspect, i.e., what spans within a text are responsible for conveying hate. Better detection of such spans can significantly reduce explicitly hateful content on the web. To further contribute to this research area, we organized HateNorm at HASOC-FIRE 2023, focusing on explicit span detection in English Tweets. A total of 12 teams participated in the competition, with the highest macro-F1 observed at 0.58.
</details>
<details>
<summary>摘要</summary>
随着仇恨言论在网络上的迅速增加，计算方法的开发已成为一项非常重要的任务。 Reactively，使用黑盒模型标识仇恨内容可能会让用户感到困惑，因为他们不知道哪些内容被自动标识为仇恨。然而，投入型mitigation可以通过建议重写内容之前提交，以避免内容被公布。然而，这两种mitigation技术都需要知道哪些文本内容包含仇恨元素，即哪些文本段落在传递仇恨信息方面表现出色。更好地检测这些文本段落可以有效减少网络上直接的仇恨内容。为了进一步贡献到这个研究领域，我们在HASOC-FIRE 2023年组织了HateNorm比赛，专注于英语推文中的直接检测。总共12个团队参与了比赛，最高的macro-F1为0.58。
</details></li>
</ul>
<hr>
<h2 id="X-Mark-Towards-Lossless-Watermarking-Through-Lexical-Redundancy"><a href="#X-Mark-Towards-Lossless-Watermarking-Through-Lexical-Redundancy" class="headerlink" title="X-Mark: Towards Lossless Watermarking Through Lexical Redundancy"></a>X-Mark: Towards Lossless Watermarking Through Lexical Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09832">http://arxiv.org/abs/2311.09832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yatao Bian, Yang Deng, Shuaiyi Li, Bingzhe Wu, Peilin Zhao, Kam-fai Wong</li>
<li>For: This paper focuses on the issue of text watermarking, which is important for detecting machine-generated text.* Methods: The authors introduce a novel approach called XMark, which leverages text redundancy within the lexical space to improve text generation fluency while maintaining watermark detectability.* Results: The authors present theoretical analyses and empirical evidence showing that XMark outperforms existing methods in retaining the emergent abilities of large language models, including zero-shot and few-shot knowledge recall, logical reasoning, and instruction following.Here’s the same information in Simplified Chinese text:* For: 这篇论文关注了文本沟通技术，它在机器生成文本检测方面具有重要意义。* Methods: 作者们提出了一种新的方法，即XMark，它利用文本空间内的同义词来提高文本生成流畅性，同时保持水印检测的能力。* Results: 作者们提供了理论分析和实验证据，表明XMark比现有方法更能保持大语言模型的emergent能力，包括零批知识回忆、几批知识回忆、逻辑推理和指令遵循。<details>
<summary>Abstract</summary>
Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods can severely degrade text quality due to arbitrary vocabulary partitioning, which disrupts the language model's expressiveness and impedes textual coherence. To mitigate this, we introduce XMark, a novel approach that capitalizes on text redundancy within the lexical space. Specifically, XMark incorporates a mutually exclusive rule for synonyms during the language model decoding process, thereby integrating prior knowledge into vocabulary partitioning and preserving the capabilities of language generation. We present theoretical analyses and empirical evidence demonstrating that XMark substantially enhances text generation fluency while maintaining watermark detectability. Furthermore, we investigate watermarking's impact on the emergent abilities of large language models, including zero-shot and few-shot knowledge recall, logical reasoning, and instruction following. Our comprehensive experiments confirm that XMark consistently outperforms existing methods in retaining these crucial capabilities of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FollowEval-A-Multi-Dimensional-Benchmark-for-Assessing-the-Instruction-Following-Capability-of-Large-Language-Models"><a href="#FollowEval-A-Multi-Dimensional-Benchmark-for-Assessing-the-Instruction-Following-Capability-of-Large-Language-Models" class="headerlink" title="FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models"></a>FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09829">http://arxiv.org/abs/2311.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Jing, Renren Jin, Jiahao Hu, Huishi Qiu, Xiaohua Wang, Peng Wang, Deyi Xiong</li>
<li>for: 评估大语言模型（LLM）的指令遵循能力是非常重要的。一个不能遵循人类指令的模型可能无法提供可靠和有用的回答。</li>
<li>methods: 我们在这篇论文中引入了FollowEvalBenchmark，一个包含英文和中文测试例的多语言指令遵循测试 benchmark。所有测试例都是由人类专家手动制作的。</li>
<li>results: 我们通过使用FollowEvalBenchmark测试多个LLM模型，发现它们的性能远远落后于人类。这 highlights 大语言模型的指令遵循能力仍然具有很大的提升空间。<details>
<summary>Abstract</summary>
The effective assessment of the instruction-following ability of large language models (LLMs) is of paramount importance. A model that cannot adhere to human instructions might be not able to provide reliable and helpful responses. In pursuit of this goal, various benchmarks have been constructed to evaluate the instruction-following capacity of these models. However, these benchmarks are limited to a single language and are constructed using automated approaches, which restricts their applicability and the quality of the test examples they contain. To bridge this gap, we introduce the FollowEval benchmark in this paper. This benchmark is composed of instances in both English and Chinese, and all test examples are crafted by human experts. Furthermore, the FollowEval benchmark is designed to assess LLMs across five critical dimensions of instruction following: string manipulation, commonsense reasoning, logical reasoning, spatial reasoning, and response constraints. To enhance the complexity and present a sufficient challenge, each test example is designed to evaluate more than one dimension. We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models.
</details>
<details>
<summary>摘要</summary>
检测大语言模型（LLM）的 instrucion-following 能力的有效性非常重要。如果一个模型无法遵循人类的 instrucion，那么它可能无法提供可靠和有用的回答。为了实现这个目标，各种标准套件已经建立来评估这些模型的 instrucion-following 能力。然而，这些标准套件受限于单一语言，并且使用自动化的方法构建，这限制了它们的可应用性和测试例子的质量。为了bridging这个差距，我们在这篇论文中引入 FollowEval 套件。这个套件包含英文和中文两种语言的实例，并且所有的测试例子都是由人类专家手动制作的。此外，FollowEval 套件采用了五个关键的 instrucion-following 维度来评估 LLM：字符串处理、常识理解、逻辑理解、空间理解和回答约束。为了增加复杂性和提供足够的挑战，每个测试例子都会评估多个维度。我们使用 FollowEval 套件测试了多种 LLM，发现它们在人类的 instrucion-following 能力方面表现明显落后。这说明这些模型在 instrucion-following 能力方面还有很大的进步空间。
</details></li>
</ul>
<hr>
<h2 id="AfriMTE-and-AfriCOMET-Empowering-COMET-to-Embrace-Under-resourced-African-Languages"><a href="#AfriMTE-and-AfriCOMET-Empowering-COMET-to-Embrace-Under-resourced-African-Languages" class="headerlink" title="AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages"></a>AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09828">http://arxiv.org/abs/2311.09828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Unbabel/COMET">https://github.com/Unbabel/COMET</a></li>
<li>paper_authors: Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Marek Masiak, Xuanli He, Sofia Bourhim, Andiswa Bukula, Muhidin Mohamed, Temitayo Olatoye, Hamam Mokayede, Christine Mwase, Wangui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Hassan Muhammad, Salomey Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane, Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Abdi Mohamed, Ayinde Hassan, Oluwabusayo Olufunke Awoyomi, Lama Alkhaled, Sana Al-Azzawi, Naome A. Etori, Millicent Ochieng, Clemencia Siro, Samuel Njoroge, Eric Muchiri, Wangari Kimotho, Lyse Naomi Wamba Momo, Daud Abolade, Simbiat Ajao, Tosin Adewumi, Iyanuoluwa Shode, Ricky Macharm, Ruqayya Nasir Iro, Saheed S. Abdullahi, Stephen E. Moore, Bernard Opoku, Zainab Akinjobi, Abeeb Afolabi, Nnaemeka Obiefuna, Onyekachi Raphael Ogbu, Sam Brian, Verrah Akinyi Otiende, Chinedu Emmanuel Mbonu, Sakayo Toadoum Sari, Pontus Stenetorp</li>
<li>for: 这个论文的目的是为了提高非洲语言机器翻译的评估方法，以便更好地评估这些语言的翻译质量。</li>
<li>methods: 这篇论文使用了人工评分的方法来创建高质量的评估数据，并开发了一种基于DA训练数据的COMET评估指标，以提高非洲语言机器翻译的评估精度。</li>
<li>results: 这篇论文的结果表明，使用这种新的评估方法可以提高非洲语言机器翻译的评估精度，并且与人类评分有高度相关性（Spearman-rank correlation +0.406）。<details>
<summary>Abstract</summary>
Despite the progress we have recorded in scaling multilingual machine translation (MT) models and evaluation data to several under-resourced African languages, it is difficult to measure accurately the progress we have made on these languages because evaluation is often performed on n-gram matching metrics like BLEU that often have worse correlation with human judgments. Embedding-based metrics such as COMET correlate better; however, lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with a simplified MQM guideline for error-span annotation and direct assessment (DA) scoring for 13 typologically diverse African languages. Furthermore, we develop AfriCOMET, a COMET evaluation metric for African languages by leveraging DA training data from high-resource languages and African-centric multilingual encoder (AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African languages MT with respect to Spearman-rank correlation with human judgments (+0.406).
</details>
<details>
<summary>摘要</summary>
尽管我们在扩展多语言机器翻译（MT）模型和评估数据到数个非常贫语言方面做出了进步，但是很难准确度量我们在这些语言上做出的进步，因为评估通常基于n-gram匹配度量如BLEU，这些度量与人类评估的相关性较差。基于嵌入度量如COMET可以更好地与人类评估相关，但是对于非常贫语言来说，评估数据的缺乏、评估指南的复杂性（如多维质量度量（MQM）），以及多语言encoder的语言覆盖率带来了障碍。在这篇论文中，我们解决了这些挑战，通过创建高质量的人类评估数据，并采用简化MQM指南进行错误扩 span的注释和直接评估（DA）分数的计算，对13种 typologically 多样化的非洲语言进行评估。此外，我们还开发了AfriCOMET评估度量，通过利用高资源语言的DA训练数据和非洲中心的多语言encoder（AfroXLM-Roberta）创建了非洲语言MT中的 estado-of-the-art 评估度量，与人类评估相关性为+0.406（Spearman相关度）。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Overload-Jailbreaking-Large-Language-Models-with-Overloaded-Logical-Thinking"><a href="#Cognitive-Overload-Jailbreaking-Large-Language-Models-with-Overloaded-Logical-Thinking" class="headerlink" title="Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"></a>Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09827">http://arxiv.org/abs/2311.09827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen</li>
<li>for: 本研究旨在探讨LLMs中的认知结构和过程如何受到攻击，以及如何防范这些攻击。</li>
<li>methods: 本研究使用了新的类型的监狱攻击， specifically targeting LLMs的认知结构和过程。 experiments conducted on AdvBench and MasterKey demonstrate that various LLMs can be compromised through cognitive overload.</li>
<li>results: 研究发现，通过三种不同的认知负担方式，可以成功地监狱所有研究的LLMs，而现有的防御策略很难有效地防止这些恶意用途。<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示出了增加的力量，但也导致了各种危害行为的出现。作为代表，跳狱攻击可以让 LLM 发生危害或不道德的反应，即使经过安全Alignment。在这篇论文中，我们 investigate 一种新的跳狱攻击，这种攻击targets LLM的认知结构和过程。 Specifically, we analyze the safety vulnerability of LLMs in the face of （1）多语言认知过载、（2）掩饰表达和（3）效果归因。与之前的跳狱攻击不同，我们提出的认知过载是一种黑盒攻击，没有需要对模型结构或模型参数的知识。在 AdvBench 和 MasterKey 上进行的实验表明，包括流行的开源模型 Llama 2 和商业模型 ChatGPT 等各种 LLMS 都可以通过认知过载遭受攻击。受到认知心理学的管理认知过载的启示，我们进一步调查了防御认知过载攻击的两种方面。实验表明，我们从三个角度来进行认知过载可以成功地跳狱所有研究过的 LLMS，而现有的防御策略很难有效地抑制这些危害用途。
</details></li>
</ul>
<hr>
<h2 id="Human-Still-Wins-over-LLM-An-Empirical-Study-of-Active-Learning-on-Domain-Specific-Annotation-Tasks"><a href="#Human-Still-Wins-over-LLM-An-Empirical-Study-of-Active-Learning-on-Domain-Specific-Annotation-Tasks" class="headerlink" title="Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks"></a>Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09825">http://arxiv.org/abs/2311.09825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Lu, Bingsheng Yao, Shao Zhang, Yun Wang, Peng Zhang, Tun Lu, Toby Jia-Jun Li, Dakuo Wang</li>
<li>for: 本研究证明了小型模型在域专知任务中的优势，并探讨了LLMs是否能在域专知任务中超越小型模型。</li>
<li>methods: 本研究使用了活动学习（AL）方法，并对四个数据集进行了实验比较。</li>
<li>results: 研究发现，即使使用了一些百度的标注数据，小型模型仍可以超过GPT-3.5的性能，而且与GPT-4相比，它们的性能相对较高。这些结论表明，LLMs的预测可以作为域专知应用中的启动方法，而人类专家仍然是数据标注驱动的域专知任务中不可或缺的。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Robust-Temporal-Reasoning-of-Large-Language-Models-via-a-Multi-Hop-QA-Dataset-and-Pseudo-Instruction-Tuning"><a href="#Towards-Robust-Temporal-Reasoning-of-Large-Language-Models-via-a-Multi-Hop-QA-Dataset-and-Pseudo-Instruction-Tuning" class="headerlink" title="Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning"></a>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09821">http://arxiv.org/abs/2311.09821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyu Tan, Hwee Tou Ng, Lidong Bing</li>
<li>for: 提高大语言模型（LLMs）的时间知识理解能力，尤其是多个答案和多个跳跃类时间理解。</li>
<li>methods: 提出了一个复杂时间问答（QA）数据集 Complex-TR，以及一种新的数据增强策略，以提高 LLMS 的复杂时间理解能力和可靠性。</li>
<li>results: 对多个时间问答数据集进行了实验，研究结果表明，我们的方法能够提高 LLMS 的时间问答指标，比基eline方法提高了显著的多。<details>
<summary>Abstract</summary>
Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering (QA) dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs' performance on temporal QA benchmarks by significant margins.
</details>
<details>
<summary>摘要</summary>
现实世界中的知识是不断更新的。然而，更新大型自然语言模型（LLM）的成本很高。因此，LLM需要理解时间知识的概念。但是，先前的时间问答工作没有强调多个答案和多个跳步类时间推理。在这篇论文中，我们提出了复杂时间问答（QA）数据集Complex-TR，它专注于多个答案和多个跳步时间推理。此外，我们还提出了一种新的数据增强策略，用于提高LLM的复杂时间推理能力和鲁棒性。我们在多个时间问答数据集上进行了实验，实验结果显示，我们的方法可以在时间问答标准准则上提高LLM的表现。
</details></li>
</ul>
<hr>
<h2 id="SUQL-Conversational-Search-over-Structured-and-Unstructured-Data-with-Large-Language-Models"><a href="#SUQL-Conversational-Search-over-Structured-and-Unstructured-Data-with-Large-Language-Models" class="headerlink" title="SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models"></a>SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09818">http://arxiv.org/abs/2311.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-oval/suql">https://github.com/stanford-oval/suql</a></li>
<li>paper_authors: Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui Dávid, Monica S. Lam</li>
<li>for: 这篇论文旨在构建基于多种数据源的对话式查询系统，以便更好地处理结构化和无结构化数据的混合查询。</li>
<li>methods: 这篇论文提出了SUQL（结构化和无结构化查询语言），它是一种可执行的正式表示语言，可以自然地涵盖结构化和无结构化数据查询的复杂作业。</li>
<li>results: 在实验中，使用SUQL和大语言模型实现的对话式搜索代理可以在51.3%的问题中找到满足用户需求的实体，比常用的基eline提高89.3%。<details>
<summary>Abstract</summary>
Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.   This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.
</details>
<details>
<summary>摘要</summary>
Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.  This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.Here's the translation in Traditional Chinese:Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.  This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Propaganda-Span-Annotation"><a href="#Large-Language-Models-for-Propaganda-Span-Annotation" class="headerlink" title="Large Language Models for Propaganda Span Annotation"></a>Large Language Models for Propaganda Span Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09812">http://arxiv.org/abs/2311.09812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Hasanain, Fatema Ahmed, Firoj Alam</li>
<li>for: 本研究的目的是检测在线通信中的宣传技巧，以及使用大语言模型GPT-4来完成 annotator 的任务。</li>
<li>methods: 本研究使用了一个自己开发的 Dataset，包括多个笔记者的注释。我们的结果表明，为模型提供更多的信息作为提示，可以提高注释一致性和性能。</li>
<li>results: 我们的结果表明，提供更多的信息作为提示可以提高注释一致性和性能。我们计划将多个笔记者的注释，包括GPT-4的注释，向社区提供。<details>
<summary>Abstract</summary>
The use of propagandistic techniques in online communication has increased in recent years, aiming to manipulate online audiences. Efforts to automatically detect and debunk such content have been made, addressing various modeling scenarios. These include determining whether the content (text, image, or multimodal) (i) is propagandistic, (ii) employs one or more techniques, and (iii) includes techniques with identifiable spans. Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans. We investigate whether large language models such as GPT-4 can be utilized to perform the task of an annotator. For the experiments, we used an in-house developed dataset consisting of annotations from multiple annotators. Our results suggest that providing more information to the model as prompts improves the annotation agreement and performance compared to human annotations. We plan to make the annotated labels from multiple annotators, including GPT-4, available for the community.
</details>
<details>
<summary>摘要</summary>
在latest years, the use of propaganda techniques in online communication has increased, aiming to manipulate online audiences. Efforts to automatically detect and debunk such content have been made, addressing various modeling scenarios. These include determining whether the content (text, image, or multimodal) (i) is propagandistic, (ii) employs one or more techniques, and (iii) includes techniques with identifiable spans. Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans. We investigate whether large language models such as GPT-4 can be utilized to perform the task of an annotator. For the experiments, we used an in-house developed dataset consisting of annotations from multiple annotators. Our results suggest that providing more information to the model as prompts improves the annotation agreement and performance compared to human annotations. We plan to make the annotated labels from multiple annotators, including GPT-4, available for the community.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PixT3-Pixel-based-Table-To-Text-generation"><a href="#PixT3-Pixel-based-Table-To-Text-generation" class="headerlink" title="PixT3: Pixel-based Table To Text generation"></a>PixT3: Pixel-based Table To Text generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09808">http://arxiv.org/abs/2311.09808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iñigo Alonso, Eneko Agirre, Mirella Lapata</li>
<li>for: 这篇论文旨在提出一种多模态表格转文本模型，以提高表格转文本任务的效率和准确性。</li>
<li>methods: 该模型使用图像表示方法，而不是传统的文本化方法，以提高空间效率。它还使用了一种新的中间训练课程，以增强表格结构的认识。</li>
<li>results: 模型在ToTTo测试套件中的纯表格转文本设置中超过了状态态的表现，并在控制的表格转文本设置中保持竞争力。它还在未看过的数据集中表现出色，在所有生成设置中超越了ToTTo状态态。<details>
<summary>Abstract</summary>
Table-to-Text has been traditionally approached as a linear language to text problem. However, visually represented tables are rich in visual information and serve as a concise, effective form of representing data and its relationships. When using text-based approaches, after the linearization process, this information is either lost or represented in a space inefficient manner. This inefficiency has remained a constant challenge for text-based approaches making them struggle with large tables. In this paper, we demonstrate that image representation of tables are more space-efficient than the typical textual linearizations, and multi-modal approaches are competitive in Table-to-Text tasks. We present PixT3, a multimodal table-to-text model that outperforms the state-of-the-art (SotA) in the ToTTo benchmark in a pure Table-to-Text setting while remaining competitive in controlled Table-to-Text scenarios. It also generalizes better in unseen datasets, outperforming ToTTo SotA in all generation settings. Additionally, we introduce a new intermediate training curriculum to reinforce table structural awareness, leading to improved generation and overall faithfulness of the models.
</details>
<details>
<summary>摘要</summary>
传统上，Table-to-Text问题被看作是一个线性语言到文本问题。然而，可见的表格具有丰富的视觉信息，并且作为数据和其关系的简洁、有效的表示形式。在文本基于的方法中， послеLinearization过程，这些信息将 Either lost or represented in an inefficient manner.这种不足在文本基于的方法中一直是一大挑战，使得它们在处理大表格时困难。在这篇论文中，我们证明了图像表示的表格更加空间效率，而且多模态方法在Table-to-Text任务中竞争。我们提出了PixT3，一种多模态表格到文本模型，在ToTTo Benchmark中超越了状态的艺术（SotA），在纯文本基于的Setting中具有比较竞争力，并在Controlled Table-to-Text Setting中具有更好的整体 faithfulness。此外，我们还引入了一种新的中间培训课程，以强化表格结构的认识，导致模型的生成和整体 faithfulness得到改进。
</details></li>
</ul>
<hr>
<h2 id="The-Curious-Decline-of-Linguistic-Diversity-Training-Language-Models-on-Synthetic-Text"><a href="#The-Curious-Decline-of-Linguistic-Diversity-Training-Language-Models-on-Synthetic-Text" class="headerlink" title="The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text"></a>The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09807">http://arxiv.org/abs/2311.09807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, Chloé Clavel</li>
<li>for: 这项研究探讨了将大型自然语言模型（LLM）训练于前代模型生成的 sintetic 数据上的后果，这已成为增加人类训练数据的方法的增长趋势。</li>
<li>methods: 我们开发了一组新的评价指标，旨在衡量模型在不同自然语言生成任务上的语言多样性，特别是在时间 recursive 练习中进行的。</li>
<li>results: 我们的发现表明，在successive 迭代中，模型的输出多样性明显减少，这表明了在这种训练方法下，LLMs 的语言能力可能受到限制。<details>
<summary>Abstract</summary>
This study investigates the consequences of training large language models (LLMs) on synthetic data generated by their predecessors, an increasingly prevalent practice aimed at addressing the limited supply of human-generated training data. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we developed a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive fine-tuning experiments across various natural language generation tasks. Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DocMath-Eval-Evaluating-Numerical-Reasoning-Capabilities-of-LLMs-in-Understanding-Long-Documents-with-Tabular-Data"><a href="#DocMath-Eval-Evaluating-Numerical-Reasoning-Capabilities-of-LLMs-in-Understanding-Long-Documents-with-Tabular-Data" class="headerlink" title="DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"></a>DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09805">http://arxiv.org/abs/2311.09805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan</li>
<li>for: This paper aims to evaluate the numerical reasoning and problem-solving capabilities of large language models (LLMs) in the context of understanding and analyzing financial documents.</li>
<li>methods: The paper introduces DocMath-Eval, a comprehensive benchmark that incorporates different prompting strategies to assess the capabilities and limitations of existing LLMs in understanding financial documents.</li>
<li>results: The current best-performing system (GPT-4) can perform well on simple problems, but significantly lags behind human experts in more complex problems grounded in longer contexts. The paper concludes that DocMath-Eval can be used as a valuable benchmark to evaluate LLMs’ capabilities to solve challenging numerical reasoning problems in expert domains.<details>
<summary>Abstract</summary>
Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning and problem-solving capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 19 LLMs, including those specialized in coding and finance. We also incorporate different prompting strategies (i.e., Chain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs' capabilities to solve challenging numerical reasoning problems in expert domains. We will release the benchmark and code at https://github.com/yale-nlp/DocMath-Eval.
</details>
<details>
<summary>摘要</summary>
现代LLM技术在解决类似于考试的数学问题方面已经表现出了惊人的表现。然而，这些数学解决问题在实际场景中的有效性，特别是在专家领域，仍然未经充分调查。这篇论文介绍了DocMath-Eval，一个特有的数学问题解决和分析金融文档中的文本和表格的完整性评价标准。我们评估了19种不同的LLM系统，包括编程和金融领域的专门系统。我们还采用了不同的提示策略（即链条思维和程序思维）来全面评估现有LLM的能力和局限性。我们发现，即使最佳表现的系统（即GPT-4）在短文档上解决金融指标增长率的简单问题上表现出色，但是在更复杂的问题上，它在更长的文档上缺乏人类专家的能力。我们认为DocMath-Eval可以用于评估LLM在专家领域中解决复杂的数学问题的能力。我们将在https://github.com/yale-nlp/DocMath-Eval上发布标准和代码。
</details></li>
</ul>
<hr>
<h2 id="textit-Dial-BeInfo-for-Faithfulness-Improving-Factuality-of-Information-Seeking-Dialogue-via-Behavioural-Fine-Tuning"><a href="#textit-Dial-BeInfo-for-Faithfulness-Improving-Factuality-of-Information-Seeking-Dialogue-via-Behavioural-Fine-Tuning" class="headerlink" title="$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"></a>$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09800">http://arxiv.org/abs/2311.09800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Razumovskaia, Ivan Vulić, Pavle Marković, Tomasz Cichy, Qian Zheng, Tsung-Hsien Wen, Paweł Budzianowski</li>
<li>for: 提高信息寻找对话系统的准确性和可靠性，使其响应用户的询问能够提供有用和适合知识源的回答。</li>
<li>methods: 使用行为调整来改善信息寻找对话系统的准确性和可靠性，以避免现象抽象和假象。</li>
<li>results: 对三个标准数据集和多个领域进行了调整，并在零容量情况下在不同领域中表现出色，而且在实际生产对话中表现更好，超过GPT4。<details>
<summary>Abstract</summary>
Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong performance on data from real `production' conversations and outperform GPT4 when tuned on a limited amount of such realistic in-domain dialogues.
</details>
<details>
<summary>摘要</summary>
factuality是寻求信息对话中的重要需求：系统应该根据用户的询问回答，以便响应是有意义的并与系统提供的知识一致。然而，现代大语言模型很容易出现幻觉，即生成不支持或与知识源相 contradicting 的回答。为了解决这个问题并提高信息寻求对话系统的忠诚度，我们介绍了BeInfo，一种简单 yet effective的方法，通过行为调整来帮助信息寻求对话。我们使用三个标准 dataset 来显示，通过BeInfo-调整，模型在seen 和 unseen 领域都变得更加忠诚于知识源。此外，我们还显示，具有3B参数的模型（如Flan-T5），通过BeInfo 的调整，在真实的生产对话数据上表现出色，并在不seen 领域中具有 Zero-shot 的优势。
</details></li>
</ul>
<hr>
<h2 id="How-Far-Can-We-Extract-Diverse-Perspectives-from-Large-Language-Models-Criteria-Based-Diversity-Prompting"><a href="#How-Far-Can-We-Extract-Diverse-Perspectives-from-Large-Language-Models-Criteria-Based-Diversity-Prompting" class="headerlink" title="How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!"></a>How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09799">http://arxiv.org/abs/2311.09799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/diversity-extraction-from-llms">https://github.com/minnesotanlp/diversity-extraction-from-llms</a></li>
<li>paper_authors: Shirley Anugrah Hayati, Minhwa Lee, Dheeraj Rajagopal, Dongyeop Kang</li>
<li>for: 这个研究旨在检验LLMs是否可以生成多元观点和理由，以及可以不同程度检验LLMs的多元观点生成能力。</li>
<li>methods: 研究使用了一种基于标准的提问技术来评估LLMs的多元观点生成能力，并使用了句子嵌入和距离度量来衡量 semantics 多元性。</li>
<li>results: 研究发现，通过使用提问技术，可以很好地评估LLMs的多元观点生成能力，并且可以在不同任务上（如荷尔豪害语言标注和故事续写）检验LLMs的多元观点生成能力。<details>
<summary>Abstract</summary>
Collecting diverse human data on subjective NLP topics is costly and challenging. As Large Language Models (LLMs) have developed human-like capabilities, there is a recent trend in collaborative efforts between humans and LLMs for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate this problem as diversity extraction in LLMs and propose a criteria-based prompting technique to ground diverse opinions and measure perspective diversity from the generated criteria words. Our results show that measuring semantic diversity through sentence embeddings and distance metrics is not enough to measure perspective diversity. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our prompting method to other tasks (hate speech labeling and story continuation), indeed we find that LLMs are able to generate diverse opinions according to the degree of task subjectivity.
</details>
<details>
<summary>摘要</summary>
COLLECTING多样的人类数据 на主观NLP话题是成本高昂和挑战性强的。随着大语言模型（LLMs）的发展，有一种现代趋势是通过人类和LLMs的合作来生成多样数据，提供可扩展和高效的解决方案。然而，LLMs是否具备生成主观话题多样视角的能力仍是一个未知问题。在这种研究中，我们调查LLMs是否能够生成主观话题多样视角和理由，如社会规范和论战文本。我们将这个问题定义为LLMs中的多样性提取问题，并提出了基于标准的提示技术来锁定多样的意见和度量视角多样性从生成的标准词语中。我们的结果表明，通过句子嵌入和距离度量来度量semantic多样性并不够来度量视角多样性。为了测试LLMs是否能够提取多样视角，我们采用了一种步骤性的回忆提示法，通过多次生成输出来评估模型的多样性覆盖率。在我们应用提示方法于其他任务（仇恨言语标注和故事续写）时，实际上我们发现LLMs可以根据任务的主观程度生成多样的意见。
</details></li>
</ul>
<hr>
<h2 id="KnowledgeMath-Knowledge-Intensive-Math-Word-Problem-Solving-in-Finance-Domains"><a href="#KnowledgeMath-Knowledge-Intensive-Math-Word-Problem-Solving-in-Finance-Domains" class="headerlink" title="KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"></a>KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09797">http://arxiv.org/abs/2311.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan</li>
<li>for: 评估语言模型在金融领域的应用能力，特别是解决复杂数学问题。</li>
<li>methods: 使用 KnowledgeMath  benchmark，包括 1,259 个问题，具有混合文本和表格内容，并提供专家注解的详细解决方案。</li>
<li>results: 评估了 14 种不同的语言模型，其中最高级别的系统 (GPT-4 with Program-of-Thoughts) 的准确率只有 45.4%，而知识扩展的 LLMs 可以提高性能 (如 GPT-3.5 从 23.9% 提高到 32.0%)，但仍然远低于人类专家的估计性能 (94%)。<details>
<summary>Abstract</summary>
We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems. Compared to prior works, this study features three core advancements. First, KnowledgeMath includes 1,259 problems with a hybrid of textual and tabular content and require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves only 45.4% accuracy, leaving substantial room for improvement. While knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0% for GPT-3.5), it is still significantly lower the estimated human expert performance of 94%. We believe that KnowledgeMath can facilitate future research on domain-specific knowledge retrieval and augmentation into the math word problem-solving process. We will release the benchmark and code at https://github.com/yale-nlp/KnowledgeMath.
</details>
<details>
<summary>摘要</summary>
我们介绍 KnowledgeMath，一个新的评估大型自然语言处理（LLM）的能力应用金融知识解决复杂的数学问题的标准库。相比之前的研究，这些研究有三个核心进步：首先，KnowledgeMath 包含 1,259 个具有文本和表格内容的问题，需要大学学士学位水准的金融领域知识以解决。第二，我们提供了专家标注、详细的解决方案 refer 以 Python 程式码格式，以 Ensure 高品质的标准库 для LLM 评估。第三，我们评估了 14 种不同的 LLM，包括 Chain-of-Thoughts 和 Program-of-Thoughts 等提示策略。现有最高表现的系统 (即 GPT-4 with Program-of-Thoughts) 的精度为 45.4%，剩下许多空间供改善。而知识增强 LLM 可以提高表现 (例如从 23.9% 提升至 32.0%  для GPT-3.5)，但仍然很低于估计的人类专家表现率 (94%)。我们认为 KnowledgeMath 可以促进未来专业知识抽取和增强在数学问题解决过程中的研究。我们将在 GitHub 上发布标准库和程式码。
</details></li>
</ul>
<hr>
<h2 id="More-Samples-or-More-Prompt-Inputs-Exploring-Effective-In-Context-Sampling-for-LLM-Few-Shot-Prompt-Engineering"><a href="#More-Samples-or-More-Prompt-Inputs-Exploring-Effective-In-Context-Sampling-for-LLM-Few-Shot-Prompt-Engineering" class="headerlink" title="More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering"></a>More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09782">http://arxiv.org/abs/2311.09782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu, Jiachen Li, Shao Zhang, Sijia Liu, James Hendler, Dakuo Wang</li>
<li>for: 提高LLM性能和信心度</li>
<li>methods: 利用多个ICL提示输入构建多个ICS提示输入，以提高LLM的预测性能和信心度</li>
<li>results: 实验结果表明，ICS可以一直提高LLM的预测性能和信心度，而且可以采用多样性基于的策略进一步提高LLM的性能。<details>
<summary>Abstract</summary>
While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.
</details>
<details>
<summary>摘要</summary>
“现有的大多数 LLM 提示工程化研究仅专注于选择更好的内部提示输入（内部学习或 ICL），那么我们不能设计和利用多个提示输入来进一步提高 LLM 的表现吗？在这个工作中，我们提出了内部抽象（ICS），一种低资源 LLM 提示工程化技术，以提高多个 ICL 提示输入的建构，以获得最高的预测结果和自信度。实验显示，使用 FlanT5-XL 和 Mistral-7B 两个 SOTA LLM 在 e-SNLI、Multi-NLI 和 ANLI 三个 NLI 数据集上，ICS 可以一致地提高 LLM 的预测性能和自信度。剖析研究表明，一种多样性基于的 ICS 策略可能会进一步提高 LLM 的表现，这照明了一个新的未来研究方向。”Note: "LLM" stands for "Large Language Model" in English.
</details></li>
</ul>
<hr>
<h2 id="To-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering"><a href="#To-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering" class="headerlink" title="To be or not to be? an exploration of continuously controllable prompt engineering"></a>To be or not to be? an exploration of continuously controllable prompt engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09773">http://arxiv.org/abs/2311.09773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Yuhan Sun, Mukai Li, Yixin Cao, Kun Wang, Wenxiao Wang, Xingyu Zeng, Rui Zhao</li>
<li>for: 这篇论文旨在提供一种能够精确控制 Language Model 的问题提示（Prompt）影响，以便更好地自定义模型和处理其输出。</li>
<li>methods: 本文使用的方法包括 LoRA（Low-Rank Adaptation）和特定的问题提示蒸馏（Prompt distillation），以实现问题提示的精确控制。</li>
<li>results: 本文的实验结果显示，ControlPE 可以实现精确控制不同类型的问题提示（包括短回答问题、拒绝问题和推理链问题），并且能够在不同的任务上灵活地应用。<details>
<summary>Abstract</summary>
As the use of large language models becomes more widespread, techniques like parameter-efficient fine-tuning and other methods for controlled generation are gaining traction for customizing models and managing their outputs. However, the challenge of precisely controlling how prompts influence these models is an area ripe for further investigation. In response, we introduce ControlPE (Continuously Controllable Prompt Engineering). ControlPE enables finer adjustments to prompt effects, complementing existing prompt engineering, and effectively controls continuous targets. This approach harnesses the power of LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting, enabling fine-tuned adjustments to the impact of prompts. Our methodology involves generating specialized datasets for prompt distillation, incorporating these prompts into the LoRA model, and carefully adjusting LoRA merging weight to regulate the influence of prompts. This provides a dynamic and adaptable tool for prompt control. Through our experiments, we have validated the practicality and efficacy of ControlPE. It proves to be a promising solution for control a variety of prompts, ranging from generating short responses prompts, refusal prompts to chain-of-thought prompts.
</details>
<details>
<summary>摘要</summary>
As the use of large language models becomes more widespread, techniques like parameter-efficient fine-tuning and other methods for controlled generation are gaining traction for customizing models and managing their outputs. However, the challenge of precisely controlling how prompts influence these models is an area ripe for further investigation. In response, we introduce ControlPE (Continuously Controllable Prompt Engineering). ControlPE enables finer adjustments to prompt effects, complementing existing prompt engineering, and effectively controls continuous targets. This approach harnesses the power of LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting, enabling fine-tuned adjustments to the impact of prompts. Our methodology involves generating specialized datasets for prompt distillation, incorporating these prompts into the LoRA model, and carefully adjusting LoRA merging weight to regulate the influence of prompts. This provides a dynamic and adaptable tool for prompt control. Through our experiments, we have validated the practicality and efficacy of ControlPE. It proves to be a promising solution for control a variety of prompts, ranging from generating short responses prompts, refusal prompts to chain-of-thought prompts.Here's the translation in Traditional Chinese:当大语言模型的使用越来越普及时，Parameter-efficient fine-tuning 和其他控制生成的技术也在广泛地应用，以适应化模型和管理其输出。然而， precisely controlling how prompts influence these models 是一个需要进一步的探索的领域。为此，我们引入 ControlPE (Continuously Controllable Prompt Engineering)。ControlPE 可以实现更细微的问题影响，与现有的问题工程相结合，并实现连续目标的控制。这个方法利用 LoRA (Low-Rank Adaptation) 的力量，实现问题权重的效果，并允许精确地调整问题的影响。我们的方法包括生成特殊的问题蒸馏集，将这些问题 integrate 到 LoRA 模型中，并精确地调整 LoRA 合并重量，以调控问题的影响。这提供了一个动态和适应的问题控制工具。经过我们的实验，我们已经 validate 了 ControlPE 的实用性和有效性。它证明是一个可靠的解决方案，可以控制多种问题，包括短回应问题、拒绝问题和链式思维问题。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Narcissistic-Evaluators-When-Ego-Inflates-Evaluation-Scores"><a href="#LLMs-as-Narcissistic-Evaluators-When-Ego-Inflates-Evaluation-Scores" class="headerlink" title="LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores"></a>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09766">http://arxiv.org/abs/2311.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin</li>
<li>for: This paper aims to investigate the potential bias of language model-driven evaluation metrics in the context of summarization tasks.</li>
<li>methods: The paper uses three popular language models (BART, T5, and GPT) to evaluate the quality of summaries generated by these models.</li>
<li>results: The paper finds that the evaluation metrics demonstrate a bias towards the underlying language models, particularly when used in a reference-free manner without gold summaries.<details>
<summary>Abstract</summary>
Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and GPTScore--demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more dependable evaluation protocols in the future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>现代自然语言处理（NLP）领域中自动评估生成文本内容的挑战仍在继续。由于现代语言模型（LM）在多种NLP任务中表现出色，因此有增加使用这些模型来创造新的评估指标来自动评估生成任务。本文探讨一个重要问题：语言模型驱动的评估指标是否具有偏好于由同一个语言模型生成的文本？ Specifically，我们评估了三个主要LM-based评估指标——BARTScore、T5Score和GPTScore——在摘要任务中是否具有偏好于其所处理的语言模型。我们的发现显示，在不使用黄金摘要的情况下，这些评估指标具有明显的偏好，特别是当用于 reference-free 的情况下。这些结果表明，由生成评估模型提供的评估结果可能会受到 beyond 文本质量的因素的影响，高亮了未来需要开发更可靠的评估协议。
</details></li>
</ul>
<hr>
<h2 id="Test-time-Backdoor-Mitigation-for-Black-Box-Large-Language-Models-with-Defensive-Demonstrations"><a href="#Test-time-Backdoor-Mitigation-for-Black-Box-Large-Language-Models-with-Defensive-Demonstrations" class="headerlink" title="Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations"></a>Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09763">http://arxiv.org/abs/2311.09763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen</li>
<li>for: This paper focuses on defending against backdoor attacks in large language models (LLMs) during the testing phase, which has been overlooked in previous studies that primarily focus on training-time defenses.</li>
<li>methods: The proposed method, called defensive demonstrations, involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are combined with user queries and presented to the model during testing, without requiring any modifications or tuning to the black-box model.</li>
<li>results: The paper shows that defensive demonstrations are effective in defending against both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.<details>
<summary>Abstract</summary>
Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models. Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications/tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations. Extensive experiments show that defensive demonstrations are effective in defending both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.
</details>
<details>
<summary>摘要</summary>
Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications or tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations.Our extensive experiments show that defensive demonstrations are effective in defending against both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.
</details></li>
</ul>
<hr>
<h2 id="OrchestraLLM-Efficient-Orchestration-of-Language-Models-for-Dialogue-State-Tracking"><a href="#OrchestraLLM-Efficient-Orchestration-of-Language-Models-for-Dialogue-State-Tracking" class="headerlink" title="OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking"></a>OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09758">http://arxiv.org/abs/2311.09758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Hsuan Lee, Hao Cheng, Mari Ostendorf</li>
<li>for: 提高自然语言处理系统的计算效率，使用小语言模型（SLM）作为cost-effective的替代方案。</li>
<li>methods: 基于发现SLM和大语言模型（LLM）在结构化知识EXTRACTION任务中具有补做的优势，提出了一种SLM&#x2F;LLM路由框架，通过在批处理中选择最佳路由来提高计算效率并提高任务性能。</li>
<li>results: 在对话状态追踪任务中，提出的路由框架substantially提高了性能，而且降低了计算成本超过50%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.
</details>
<details>
<summary>摘要</summary>
The framework begins by creating exemplar pools that represent the types of contexts where each LM provides a more reliable answer. This is achieved by fine-tuning a sentence embedding so that context similarity is close to dialogue state similarity. During inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote.In dialogue state tracking tasks, the proposed routing framework enhances performance by over 50% compared to relying solely on LLMs, while reducing computational costs by over 50%. This framework demonstrates the potential of combining SLMs and LLMs to improve the efficiency and effectiveness of natural language processing systems.
</details></li>
</ul>
<hr>
<h2 id="FairytaleCQA-Integrating-a-Commonsense-Knowledge-Graph-into-Children’s-Storybook-Narratives"><a href="#FairytaleCQA-Integrating-a-Commonsense-Knowledge-Graph-into-Children’s-Storybook-Narratives" class="headerlink" title="FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children’s Storybook Narratives"></a>FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children’s Storybook Narratives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09756">http://arxiv.org/abs/2311.09756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun</li>
<li>for: 这个论文旨在提供适用于下游儿童教育应用的自定义问答功能，用于补充现有的故事书内容。</li>
<li>methods: 该论文使用了LLM模型，并通过外部知识图进行 Commonsense知识的扩展。</li>
<li>results: 对比较大的LLM模型（GPT-4），一个较小的T5-large模型在新的问答对组成任务（QAG）中表现出色，表明：1）我们的数据集对现有LLM模型带来了新的挑战，2）人工专家的数据注释仍然是关键，因为它们在儿童教育领域具有丰富的细节知识。<details>
<summary>Abstract</summary>
AI models (including LLM) often rely on narrative question-answering (QA) datasets to provide customized QA functionalities to support downstream children education applications; however, existing datasets only include QA pairs that are grounded within the given storybook content, but children can learn more when teachers refer the storybook content to real-world knowledge (e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is annotated by children education experts, to supplement 278 storybook narratives with educationally appropriate commonsense knowledge. The dataset has 5,868 QA pairs that not only originate from the storybook narrative but also contain the commonsense knowledge grounded by an external knowledge graph (i.e., ConceptNet). A follow-up experiment shows that a smaller model (T5-large) fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2) human experts' data annotation are still critical as they have much nuanced knowledge that LLMs do not know in the children educational domain.
</details>
<details>
<summary>摘要</summary>
人工智能模型（包括LLM）经常利用叙事问答（QA）数据集来提供下游儿童教育应用程序中自定义的QA功能;然而，现有数据集只包含基于给定的故事书内容的QA对。然而，孩子们可以通过教师将故事书内容与实际世界知识相关联来学习更多。我们介绍了 FairytaleCQA 数据集，该数据集由儿童教育专家标注，用于补充 278 本故事书内容教育适用的常识知识。该数据集包含 5,868 对问答，其中不仅来自故事书内容，还由外部知识图（i.e., ConceptNet）补充了 Commonsense 知识。一项追加实验表明，一个较小的模型（T5-large）在 FairytaleCQA 上进行了可靠地超越了较大的Prompt-工程化 LLVM（例如 GPT-4）在这个新的问答对生成任务（QAG）中。这一结果表明：1）我们的数据集带来了现有的LLMs新的挑战，2）人类专家的数据标注仍然是关键的，因为它们在儿童教育领域中具有许多细节的知识，LLMs不知。
</details></li>
</ul>
<hr>
<h2 id="How-Does-Calibration-Data-Affect-the-Post-training-Pruning-and-Quantization-of-Large-Language-Models"><a href="#How-Does-Calibration-Data-Affect-the-Post-training-Pruning-and-Quantization-of-Large-Language-Models" class="headerlink" title="How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?"></a>How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09755">http://arxiv.org/abs/2311.09755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miles Williams, Nikolaos Aletras</li>
<li>for: 本文研究了大语言模型（LLM）压缩和量化的基础，包括各种压缩和量化技术的效果，以及这些技术如何影响 LLM 的性能。</li>
<li>methods: 本文使用了多种压缩和量化方法，包括各种压缩和量化技术，以及不同任务、模型和数据集。</li>
<li>results: 研究发现，使用不同的滤波数据会导致下游任务性能异常大，与现有研究不同，表明使用不同的滤波数据可能会导致 LLM 的性能变化。<details>
<summary>Abstract</summary>
Pruning and quantization form the foundation of model compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated state-of-the-art performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples, to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of pruning and quantization methods, tasks, models, and datasets. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.
</details>
<details>
<summary>摘要</summary>
剪枝和量化是神经网络模型压缩的基础，启用高效的推理 для大语言模型（LLM）。近年，各种量化和剪枝技术在后处理环境中表现出了状态之冠。它们依赖于校准数据，一小量的无标示例，来生成层活动。然而，没有任何先前的工作系统atically investigated calibration data对LLM性能的影响。在这篇论文中，我们提供了首次对剪枝和量化方法的效果进行了广泛的实验研究。我们对各种剪枝和量化方法、任务、模型和数据集进行了试验。 surprisingly，我们发现了下游任务性能的显著差异，与现有的工作 suggessthat a greater level of robustness to the calibration data。最后，我们对LLM剪枝和量化中有效使用calibration data的推荐。
</details></li>
</ul>
<hr>
<h2 id="Translation-Aligned-Sentence-Embeddings-for-Turkish-Language"><a href="#Translation-Aligned-Sentence-Embeddings-for-Turkish-Language" class="headerlink" title="Translation Aligned Sentence Embeddings for Turkish Language"></a>Translation Aligned Sentence Embeddings for Turkish Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09748">http://arxiv.org/abs/2311.09748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eren Unlu, Unver Ciftci</li>
<li>for: 提高 sentence embedding 模型在 Turkish 语言上的表现</li>
<li>methods: 提出了一种两个阶段的训练方法，其中第一阶段通过对 embedding 空间进行对应的调整，以便在 sentence embedding 设置中使用 pretrained encoder-decoder 模型进行精度的 fine-tuning。</li>
<li>results: 通过这种方法，可以在短时间内使用有限的 target 语言数据进行高精度的 fine-tuning，并且可以提高 sentence embedding 模型在 Turkish 语言上的表现。<details>
<summary>Abstract</summary>
Due to the limited availability of high quality datasets for training sentence embeddings in Turkish, we propose a training methodology and a regimen to develop a sentence embedding model. The central idea is simple but effective : is to fine-tune a pretrained encoder-decoder model in two consecutive stages, where the first stage involves aligning the embedding space with translation pairs. Thanks to this alignment, the prowess of the main model can be better projected onto the target language in a sentence embedding setting where it can be fine-tuned with high accuracy in short duration with limited target language dataset.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:由于土耳其语 sentence embedding 训练数据的可用性有限，我们提出了一种训练方法和日程，以提高 sentence embedding 模型的质量。中心思想简单 yet effective：在两个阶段中，首先对预训练的 encoder-decoder 模型进行了两个阶段的微调，其中第一个阶段是将 embedding 空间与翻译对照进行对齐。这样可以使得模型在 sentence embedding 设置下，通过短时间内使用有限的目标语言数据进行高精度的微调。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Perspectives-of-Crowdsourced-Annotators-in-Subjective-Learning-Tasks"><a href="#Capturing-Perspectives-of-Crowdsourced-Annotators-in-Subjective-Learning-Tasks" class="headerlink" title="Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks"></a>Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09743">http://arxiv.org/abs/2311.09743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negar Mokhberian, Myrl G. Marmarelis, Frederic R. Hopp, Valerio Basile, Fred Morstatter, Kristina Lerman</li>
<li>for: 这篇论文的目的是解决对主观分类任务中的多 annotator 问题，因为对于主观任务，可能会有多个真实的标签，导致模型偏向特定的标签。</li>
<li>methods: 这篇论文提出了一个新的方法，即 Annotator Aware Representations for Texts (AART)，这个方法可以将每个 annotator 的标签视为一个独立的标签，以便更好地捕捉 annotators 的看法。</li>
<li>results: 该方法可以提高模型在捕捉 annotators 的看法方面的表现，并且可以避免因为 annotators 的差异而导致的偏向。此外，这个方法还可以学习 annotators 的行为，以便进一步的探索。<details>
<summary>Abstract</summary>
In most classification models, it has been assumed to have a single ground truth label for each data point. However, subjective tasks like toxicity classification can lead to genuine disagreement among annotators. In these cases aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with small samples. This problem is especially the case in crowd-sourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. We will show the improvement of our method on metrics that assess the performance on capturing annotators' perspectives. Additionally, our approach involves learning representations for annotators, allowing for an exploration of the captured annotation behaviors.
</details>
<details>
<summary>摘要</summary>
通常的分类模型假设每个数据点有单一的真实标签。然而，主观任务如攻击性评分可能会导致注释器之间真实的分歧。在这种情况下，聚合标签会导致偏执zh labels和模型，这些模型可能会忽略小量意见。先前的研究已经揭示了标签聚合的坑缺和提出了一些实用的方法来解决这个问题。我们最近提出的多注释模型，它预测每个注释器的标签，容易受到 annotators with small samples 的下降决策。这个问题特别是在大量数据集中存在。在这种情况下，我们提出了注释者意识表示（AART） для主观分类任务。我们将展示我们的方法在衡量注释者的观点性能指标上的改进。此外，我们的方法包括学习注释者的表示，以便探索被捕捉的注释行为。
</details></li>
</ul>
<hr>
<h2 id="What-Constitutes-a-Faithful-Summary-Preserving-Author-Perspectives-in-News-Summarization"><a href="#What-Constitutes-a-Faithful-Summary-Preserving-Author-Perspectives-in-News-Summarization" class="headerlink" title="What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization"></a>What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09741">http://arxiv.org/abs/2311.09741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyh6560new/p3sum">https://github.com/lyh6560new/p3sum</a></li>
<li>paper_authors: Yuhan Liu, Shangbin Feng, Xiaochuang Han, Vidhisha Balachandran, Chan Young Park, Sachin Kumar, Yulia Tsvetkov</li>
<li>for: 这篇论文的目的是设计一个忠于作者意见和观点的摘要系统。</li>
<li>methods: 这篇论文使用了一种叫做P^3Sum的扩散模型基本摘要方法，这个方法使用政治观点分类器控制摘要的政治倾向。</li>
<li>results: 实验结果显示，P^3Sum比前一代摘要系统和大语言模型提高了11.4%的成功率，具有与标准摘要价值指标一样的性能。这些结果显示，即使是现有的模型，在新闻摘要中保持作者意见和观点仍然是一个挑战，而P^3Sum则是一个重要的第一步。<details>
<summary>Abstract</summary>
In this work, we take a first step towards designing summarization systems that are faithful to the author's opinions and perspectives. Focusing on a case study of preserving political perspectives in news summarization, we find that existing approaches alter the political opinions and stances of news articles in more than 50% of summaries, misrepresenting the intent and perspectives of the news authors. We thus propose P^3Sum, a diffusion model-based summarization approach controlled by political perspective classifiers. In P^3Sum, the political leaning of a generated summary is iteratively evaluated at each decoding step, and any drift from the article's original stance incurs a loss back-propagated to the embedding layers, steering the political stance of the summary at inference time. Extensive experiments on three news summarization datasets demonstrate that P^3Sum outperforms state-of-the-art summarization systems and large language models by up to 11.4% in terms of the success rate of stance preservation, with on-par performance on standard summarization utility metrics. These findings highlight the lacunae that even for state-of-the-art models it is still challenging to preserve author perspectives in news summarization, while P^3Sum presents an important first step towards evaluating and developing summarization systems that are faithful to author intent and perspectives.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开始努力设计 faithful 的摘要系统，以保持作者的意图和观点。通过新闻摘要中保持政治立场的案例研究，我们发现现有方法在超过50%的摘要中改变了新闻文章的政治立场和意图，这些摘要不符合作者的意图和观点。我们因此提出 P^3Sum，一种基于扩散模型的摘要方法，该方法在摘要生成过程中控制政治观点分类器，以确保生成的摘要保持原文的政治立场。在 P^3Sum 中，生成的摘要中的政治倾向在每个解码步骤中被评估，如果摘要偏离原文的政治立场，就会产生损失，这些损失将在嵌入层传递给 embedding 层，以在推理时间控制摘要的政治倾向。我们对三个新闻摘要数据集进行了广泛的实验，结果表明，P^3Sum 在保持摘要的政治立场方面的成功率比现有的摘要系统和大语言模型高出11.4%，而与标准摘要用途指标具有相同的性能。这些发现表明，即使是当今最先进的模型，在新闻摘要中保持作者的意图和观点仍然是一项挑战，而 P^3Sum 则是一个重要的第一步。
</details></li>
</ul>
<hr>
<h2 id="CARE-Extracting-Experimental-Findings-From-Clinical-Literature"><a href="#CARE-Extracting-Experimental-Findings-From-Clinical-Literature" class="headerlink" title="CARE: Extracting Experimental Findings From Clinical Literature"></a>CARE: Extracting Experimental Findings From Clinical Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09736">http://arxiv.org/abs/2311.09736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakanksha Naik, Bailey Kuehl, Erin Bransom, Doug Downey, Tom Hope</li>
<li>for: 本研究旨在提供一个新的信息抽取 dataset，用于抽取生物医学文献中的临床发现。</li>
<li>methods: 本研究使用了一新的注解 schema， capture 了细化的发现作为 n-ary 关系 между实体和属性。该 schema 包括困难现象，如不连续实体跨 span、嵌入关系和变量 arity n-ary 关系。</li>
<li>results: 研究使用了两个来源的700个摘要进行广泛的注解，并对多种当前IE系统的性能进行了测试。结果表明，即使使用 SOTA 模型，如 GPT4，也很难在这个数据集上进行relation extraction。<details>
<summary>Abstract</summary>
Extracting fine-grained experimental findings from literature can provide massive utility for scientific applications. Prior work has focused on developing annotation schemas and datasets for limited aspects of this problem, leading to simpler information extraction datasets which do not capture the real-world complexity and nuance required for this task. Focusing on biomedicine, this work presents CARE (Clinical Aggregation-oriented Result Extraction) -- a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which includes phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, and variable arity n-ary relations. Using this schema, we collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also benchmark the performance of various state-of-the-art IE systems on our dataset, including extractive models and generative LLMs in fully supervised and limited data settings. Our results demonstrate the difficulty of our dataset -- even SOTA models such as GPT4 struggle, particularly on relation extraction. We release our annotation schema and CARE to encourage further research on extracting and aggregating scientific findings from literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过Literature中提取细致实验结果可以提供巨大的科学应用 utility。先前的工作主要集中在开发注解schema和数据集，以便解决这个问题的有限方面，导致的是更简单的信息抽取数据集，这些数据集不能捕捉实际世界中的复杂性和细节。在生物医学领域，本工作提出了CARE（临床结合 oriented Result Extraction）——一个新的IE数据集，用于提取临床发现。我们开发了一个新的注解schema，捕捉细致发现为n-ary关系 между实体和属性，该schemas包括现实困难 для当前IE系统的现象，如不连续实体跨度、嵌入关系和变量数学 n-ary关系。使用该schemas，我们收集了700个报告的广泛的注解，来自两个来源：临床试验和案例报告。我们还对我们的数据集进行了多种当前IE系统的性能测试，包括抽取模型和生成LLMs在完全超vised和有限数据设置下。我们的结果表明，我们的数据集具有很大的困难度——即使SOTA模型如GPT4，它们在关系提取方面尤其困难。我们发布了我们的注解schema和CARE，以促进Literature中的实验发现和抽取。
</details></li>
</ul>
<hr>
<h2 id="Tracking-the-Newsworthiness-of-Public-Documents"><a href="#Tracking-the-Newsworthiness-of-Public-Documents" class="headerlink" title="Tracking the Newsworthiness of Public Documents"></a>Tracking the Newsworthiness of Public Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09734">http://arxiv.org/abs/2311.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Spangher, Emilio Ferrara, Ben Welsh, Nanyun Peng, Serdar Tumgoren, Jonathan May</li>
<li>for: 本研究ocuses on Local public policy coverage in the San Francisco Bay Area by the San Francisco Chronicle.</li>
<li>methods: The paper uses probabilistic relational modeling to link news articles, public policy documents, and meeting recordings.</li>
<li>results: The paper shows that different aspects of public policy discussion yield different newsworthiness signals, and their systems identify policies considered newsworthy with 68% F1 and their coverage recommendations are helpful with an 84% win-rate.Here’s the format you requested:</li>
<li>for: &lt;本研究ocuses on Local public policy coverage in the San Francisco Bay Area by the San Francisco Chronicle.&gt;</li>
<li>methods: &lt;The paper uses probabilistic relational modeling to link news articles, public policy documents, and meeting recordings.&gt;</li>
<li>results: &lt;The paper shows that different aspects of public policy discussion yield different newsworthiness signals, and their systems identify policies considered newsworthy with 68% F1 and their coverage recommendations are helpful with an 84% win-rate.&gt;<details>
<summary>Abstract</summary>
Journalists must find stories in huge amounts of textual data (e.g. leaks, bills, press releases) as part of their jobs: determining when and why text becomes news can help us understand coverage patterns and help us build assistive tools. Yet, this is challenging because very few labelled links exist, language use between corpora is very different, and text may be covered for a variety of reasons. In this work we focus on news coverage of local public policy in the San Francisco Bay Area by the San Francisco Chronicle. First, we gather news articles, public policy documents and meeting recordings and link them using probabilistic relational modeling, which we show is a low-annotation linking methodology that outperforms other retrieval-based baselines. Second, we define a new task: newsworthiness prediction, to predict if a policy item will get covered. We show that different aspects of public policy discussion yield different newsworthiness signals. Finally we perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68% F1 and our coverage recommendations are helpful with an 84% win-rate.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MOKA-Moral-Knowledge-Augmentation-for-Moral-Event-Extraction"><a href="#MOKA-Moral-Knowledge-Augmentation-for-Moral-Event-Extraction" class="headerlink" title="MOKA: Moral Knowledge Augmentation for Moral Event Extraction"></a>MOKA: Moral Knowledge Augmentation for Moral Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09733">http://arxiv.org/abs/2311.09733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/launchnlp/MOKA">https://github.com/launchnlp/MOKA</a></li>
<li>paper_authors: Xinliang Frederick Zhang, Winston Wu, Nick Beauchamp, Lu Wang</li>
<li>For: This paper is written for studying the phenomenon of moral language in news media and the dynamics of moral events in shaping news content.* Methods: The paper uses a new dataset called MORAL EVENTS, which consists of 5,494 structured annotations on 474 news articles from diverse US media outlets. The authors also propose a moral event extraction framework called MOKA, which leverages knowledge derived from moral words and moral scenarios.* Results: The experimental results show that MOKA outperforms competitive baselines across three moral event understanding tasks. Additionally, the authors find that media outlets of different ideological leanings selectively report moral events, highlighting the significance of event-level morality analysis in news.<details>
<summary>Abstract</summary>
News media employ moral language to create memorable stories, and readers often engage with the content that align with their values. Moral theories have been applied to news analysis studying moral values in isolation, while the intricate dynamics among participating entities in shaping moral events have been overlooked. This is mainly due to the use of obscure language to conceal evident ideology and values, coupled with the insufficient moral reasoning capability in most existing NLP systems, where LLMs are no exception. To study this phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of 5,494 structured annotations on 474 news articles by diverse US media across the political spectrum. We further propose MOKA, a moral event extraction framework with MOral Knowledge Augmentation, that leverages knowledge derived from moral words and moral scenarios. Experimental results show that MOKA outperforms competitive baselines across three moral event understanding tasks. Further analyses illuminate the selective reporting of moral events by media outlets of different ideological leanings, suggesting the significance of event-level morality analysis in news. Our datasets and codebase are available at https://github.com/launchnlp/MOKA.
</details>
<details>
<summary>摘要</summary>
新闻媒体使用道德语言创造深刻的故事，读者常与其价值观合而参与内容。道德理论在新闻分析中被应用，但是参与者之间的复杂关系和形成道德事件的过程受到了忽略。这主要是因为使用晦涩的语言隐藏了明确的意识形态和价值观，同时现有的NLP系统中的道德理解能力不够，LLMs也不例外。为研究这一现象，我们首先创建了新的数据集，道德事件集（MORAL EVENTS），包含474篇来自美国各种政见媒体的新闻文章5,494个结构化注释。我们还提出了MOKA，一个基于道德知识增强的道德事件抽取框架，利用道德词汇和道德enario来抽取道德事件。实验结果显示，MOKA在三个道德事件理解任务上与竞争对手相比表现出色。进一步的分析发现媒体不同政见倾向的报道道德事件是有偏见的，这表明事件级别的道德分析在新闻中的重要性。我们的数据集和代码库可以在GitHub上找到：https://github.com/launchnlp/MOKA。
</details></li>
</ul>
<hr>
<h2 id="On-Evaluating-the-Integration-of-Reasoning-and-Action-in-LLM-Agents-with-Database-Question-Answering"><a href="#On-Evaluating-the-Integration-of-Reasoning-and-Action-in-LLM-Agents-with-Database-Question-Answering" class="headerlink" title="On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering"></a>On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09721">http://arxiv.org/abs/2311.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Wenfei Zhou, Arman Cohan</li>
<li>for: 这项研究是为了评估大自然语言模型（LLM）在数据库问答任务中的表现，并研究 LLM 如何使用多个 SQL 查询来获取数据库中的数据，进行Contextual reasoning，并将其总结成一份完整的分析报告。</li>
<li>methods: 本研究使用了一种新的长形数据库问答数据集，并提出了两种互动策略来解决问题。我们还进行了细腻的分析，探讨了不同阶段的互动过程中的瓶颈。</li>
<li>results: 我们的研究发现，当前的State-of-the-art GPT-4模型在这个任务中存在两个主要的瓶颈：规划能力和多个 SQL 查询的生成能力。我们还引入了一种多代理评估框架，以便更准确地评估答案质量。这种框架允许我们更好地理解当前 LLM 在复杂的检索和推理任务中的优劣点。<details>
<summary>Abstract</summary>
This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Regularized-Conventions-Equilibrium-Computation-as-a-Model-of-Pragmatic-Reasoning"><a href="#Regularized-Conventions-Equilibrium-Computation-as-a-Model-of-Pragmatic-Reasoning" class="headerlink" title="Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning"></a>Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09712">http://arxiv.org/abs/2311.09712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athul Paul Jacob, Gabriele Farina, Jacob Andreas</li>
<li>for: 这篇论文旨在描述一种语言理解模型，即通过搜索信号游戏的 equilibria来生成和理解语言表达。</li>
<li>methods: 该模型使用搜索 equilibria来模拟语言交流中的信号游戏，并通过定制化的搜索策略来找到最佳的语言表达。</li>
<li>results: 在使用该模型的实验中，论文能够匹配或超越现有的最佳回应和理性演讲模型的预测，并且可以提供有关语言交流中的通信成功和自然性的理论保证。<details>
<summary>Abstract</summary>
We present a model of pragmatic language understanding, where utterances are produced and understood by searching for regularized equilibria of signaling games. In this model (which we call ReCo, for Regularized Conventions), speakers and listeners search for contextually appropriate utterance--meaning mappings that are both close to game-theoretically optimal conventions and close to a shared, ''default'' semantics. By characterizing pragmatic communication as equilibrium search, we obtain principled sampling algorithms and formal guarantees about the trade-off between communicative success and naturalness. Across several datasets capturing real and idealized human judgments about pragmatic implicatures, ReCo matches or improves upon predictions made by best response and rational speech act models of language understanding.
</details>
<details>
<summary>摘要</summary>
我们提出了一种语言理解模型，其中讲话和理解都是通过搜索正则化平衡的信号游戏来实现的。我们称这种模型为ReCo（正则化会议）。在这个模型中，说话者和听众在语言上进行Contextually appropriate的讲话-意思映射搜索，以达到Game-theoretically optimal的会议和共同默认 semantics。通过将 Pragmatic communication 定义为平衡搜索，我们得到了原则性的抽样算法和Formal guarantees about the trade-off between communicative success and naturalness。在几个捕捉了真实和理想的人类评价的数据集上，ReCo匹配或超过了Best response和理性语言理解模型的预测。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Inference-with-Lexical-Shortlisting"><a href="#Large-Language-Model-Inference-with-Lexical-Shortlisting" class="headerlink" title="Large Language Model Inference with Lexical Shortlisting"></a>Large Language Model Inference with Lexical Shortlisting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09709">http://arxiv.org/abs/2311.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolay Bogoychev, Pinzhen Chen, Barry Haddow, Alexandra Birch</li>
<li>for: 本研究旨在提高大语言模型（LLM）的推理速度和计算资源使用效率，通过适应lexical shortlisting技术。</li>
<li>methods: 本研究使用Unicode字符集基于的脚本筛选和基于词库的选择方法来缩短子词库。</li>
<li>results: 研究发现，lexical shortlisting可以减少一些模型的内存使用量，最高可以减少50%，同时也有25%的提升的可能性。此外，研究还发现了这种词库选择方法的缺点，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Large language model (LLM) inference is computation and memory intensive, so we adapt lexical shortlisting to it hoping to improve both. While lexical shortlisting is well-explored in tasks like machine translation, it requires modifications before being suitable for LLMs as the intended applications vary significantly. Our work studies two heuristics to shortlist sub-vocabulary at LLM inference time: Unicode-based script filtering and corpus-based selection. We explore different LLM families and sizes, and we find that lexical shortlisting can reduce the memory usage of some models by nearly 50\% and has an upper bound of 25\% improvement in generation speed. In this pilot study, we also identify the drawbacks of such vocabulary selection methods and propose avenues for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的推理是计算和内存密集的，因此我们适应lexical shortlisting以提高它们。lexical shortlisting在机器翻译任务中广泛探索过，但是需要修改才能适用于LLM，因为它们的应用场景差异很大。我们的工作研究了两种决策指标来短list sub-vocabulary during LLM inference time：Unicode-based script filtering和corpus-based selection。我们在不同的LLM家族和大小上进行了 исследование，发现lexical shortlisting可以将一些模型的内存使用量减少到 nearly 50%，并且有一个 Upper bound的25%的提高 Speed of generation。在这个 Pilot study中，我们还发现了这种词汇选择方法的缺点并提出了未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Self-enhancement-Multitask-Framework-for-Unsupervised-Aspect-Category-Detection"><a href="#A-Self-enhancement-Multitask-Framework-for-Unsupervised-Aspect-Category-Detection" class="headerlink" title="A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection"></a>A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09708">http://arxiv.org/abs/2311.09708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi-Nhung Nguyen, Hoang Ngo, Kiem-Hieu Nguyen, Tuan-Dung Cao</li>
<li>for:  addresses the problem of unsupervised Aspect Category Detection using a small set of seed words.</li>
<li>methods:  proposes a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training, and jointly trains Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity.</li>
<li>results:  surpasses strong baselines on standard datasets.<details>
<summary>Abstract</summary>
Our work addresses the problem of unsupervised Aspect Category Detection using a small set of seed words. Recent works have focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.
</details>
<details>
<summary>摘要</summary>
我们的工作解决了无监督方面类检测问题，使用一小组种子词。 latest works focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.Here's a word-for-word translation of the text in Traditional Chinese:我们的工作解决了无监督方面类检测问题，使用一小组种子词。 latest works focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.
</details></li>
</ul>
<hr>
<h2 id="GenCodeSearchNet-A-Benchmark-Test-Suite-for-Evaluating-Generalization-in-Programming-Language-Understanding"><a href="#GenCodeSearchNet-A-Benchmark-Test-Suite-for-Evaluating-Generalization-in-Programming-Language-Understanding" class="headerlink" title="GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding"></a>GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09707">http://arxiv.org/abs/2311.09707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp</li>
<li>for: 提高软件开发Productivity</li>
<li>methods: 使用大型生成模型进行代码生成和代码完成，使用小型encoder-only模型进行自然语言查询代码搜索</li>
<li>results: 提出了一个新的benchmark dataset called GenCodeSearchNet (GeCS)，以评估语言模型对不同编程语言的理解能力，并引入了一个新的手动审核 subsets StatCodeSearch，Focus on R编程语言，以增强模型对不同编程语言的适应能力。<details>
<summary>Abstract</summary>
Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries.These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.
</details>
<details>
<summary>摘要</summary>
Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systematically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.Here's the translation in Traditional Chinese:语言模型可以serve as a valuable tool for software developers to increase productivity。大型生成模型可以用于代码生成和代码完成，而小型encoder-only模型则可以进行代码搜寻任务使用自然语言查询。这些能力受到训练数据的质量和多样性的影响。通常的源代码资料集用于训练通常会针对最受欢迎的语言进行集中，而测试通常会在同一个分布上进行，往往忽略低资源的编程语言。驱动了Hupkes等人提出的NLG概念分类，我们提议一个新的benchmarkdatasetcalled GenCodeSearchNet (GeCS)，这个dataset建立在现有的自然语言代码搜寻dataset之上，以系统地评估语言模型对程式语言理解的扩展能力。这个dataset中，我们引入了一个新的、手动精心筛选的子集StatCodeSearch，它针对R语言，这是一个受欢迎但现在尚未得到充分关注的编程语言，经常被computer科学以外的研究人员使用。为了评估和比较，我们收集了一些基准结果使用精心翻译BERT类型模型和GPT类型大型语言模型，这些模型在零条件设定下进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fumbling-in-Babel-An-Investigation-into-ChatGPT’s-Language-Identification-Ability"><a href="#Fumbling-in-Babel-An-Investigation-into-ChatGPT’s-Language-Identification-Ability" class="headerlink" title="Fumbling in Babel: An Investigation into ChatGPT’s Language Identification Ability"></a>Fumbling in Babel: An Investigation into ChatGPT’s Language Identification Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09696">http://arxiv.org/abs/2311.09696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Rui Chen, Ife Adebara, Khai Duy Doan, Qisheng Liao, Muhammad Abdul-Mageed</li>
<li>for:  investigate ChatGPT’s language identification abilities</li>
<li>methods:  compile Babel-670 benchmark, study ChatGPT’s ability to identify language names and language codes under zero- and few-shot conditions with and without label set</li>
<li>results:  ChatGPT lags behind smaller finetuned language identification tools, indicating potential for enhancement before serving diverse communities.Here is the same information in Traditional Chinese text:</li>
<li>for: 探访ChatGPT的语言识别能力</li>
<li>methods: 编译Babel-670 benchmark，研究ChatGPT在零条件和几条件下进行语言名称和语言代码识别</li>
<li>results: ChatGPT落后于小型训练语言识别工具，显示需要进一步改进以应对多元社区。<details>
<summary>Abstract</summary>
Recently, ChatGPT has emerged as a powerful NLP tool that can carry out several tasks. However, the range of languages ChatGPT can handle remains largely a mystery. In this work, we investigate ChatGPT's language identification abilities. For this purpose, we compile Babel-670, a benchmark comprising $670$ languages representing $23$ language families. Languages in Babel-670 run the gamut between the very high-resource to the very low-resource and are spoken in five continents. We then study ChatGPT's (both GPT-3.5 and GPT-4) ability to (i) identify both language names and language codes (ii) under both zero- and few-shot conditions (iii) with and without provision of label set. When compared to smaller finetuned language identification tools, we find that ChatGPT lags behind. Our empirical analysis shows the reality that ChatGPT still resides in a state of potential enhancement before it can sufficiently serve diverse communities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whispers-of-Doubt-Amidst-Echoes-of-Triumph-in-NLP-Robustness"><a href="#Whispers-of-Doubt-Amidst-Echoes-of-Triumph-in-NLP-Robustness" class="headerlink" title="Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness"></a>Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09694">http://arxiv.org/abs/2311.09694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashim Gupta, Rishanth Rajendhran, Nathan Stringham, Vivek Srikumar, Ana Marasović</li>
<li>for: 本研究是为了解决NLPToday的大型和高性能模型是否已经解决了长期稳定性问题。</li>
<li>methods: 作者使用了19种不同的模型，包括不同的架构选择和预训练目标。他们使用了OOD和挑战测试集、CheckLists、对比集和抗敌输入来进行评估。</li>
<li>results: 研究发现，不是所有的OOD测试都能够提供更深入的稳定性评估。使用CheckLists和对比集的评估显示了模型的性能差距，并且尚未充分提高模型的稳定性。此外，作者还指出了当前对模型 robustness的评估方法存在问题，这些方法可以被轻松地骗过，并且当前的评估方法不够深入。因此，作者 conclude that NLP中的稳定性问题仍未得到解决，甚至一些用于评估稳定性的方法需要重新评估。<details>
<summary>Abstract</summary>
Are the longstanding robustness issues in NLP resolved by today's larger and more performant models? To address this question, we conduct a thorough investigation using 19 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all OOD tests provide further insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them sufficiently robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.
</details>
<details>
<summary>摘要</summary>
是否已经解决了自然语言处理（NLP）领域的长期稳定性问题？为了回答这个问题，我们进行了19种不同大小和架构的模型的完整调查。我们使用了（a）跨模型测试集（Out-of-distribution，OOD）和挑战测试集，（b）CheckLists，（c）对比集和（d）敌意输入来进行评估。我们的分析发现，不是所有的OOD测试都能够提供更多的 robustness 信息。使用 CheckLists 和对比集的评估显示了模型的显著性能差异；即便模型的大小增加，也不能 garantuee 其 sufficient robustness。最后，我们指出了当前对模型 adversarial 评估的方法存在问题：它们可以轻松地被阻断，并且在当前的形式下，不能深入探索模型的 robustness。我们结论是，NLP 领域的 robustness 问题仍未得到解决，而且一些用于评估 robustness 的方法也需要重新评估。
</details></li>
</ul>
<hr>
<h2 id="Inducing-Political-Bias-Allows-Language-Models-Anticipate-Partisan-Reactions-to-Controversies"><a href="#Inducing-Political-Bias-Allows-Language-Models-Anticipate-Partisan-Reactions-to-Controversies" class="headerlink" title="Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies"></a>Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09687">http://arxiv.org/abs/2311.09687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao He, Siyi Guo, Ashwin Rao, Kristina Lerman</li>
<li>for: 本研究旨在使用大型自然语言模型（LLM）更好地理解政治偏见在数字化对话中。</li>
<li>methods: 本研究采用了一种新的办法，即使用一个单一的指令驱动的LLM来反映政治 идеологи的范围。</li>
<li>results: 研究发现模型能够准确地捕捉到情感和道德上的细节，但在姿势检测方面存在一些挑战。<details>
<summary>Abstract</summary>
Social media platforms are rife with politically charged discussions. Therefore, accurately deciphering and predicting partisan biases using Large Language Models (LLMs) is increasingly critical. In this study, we address the challenge of understanding political bias in digitized discourse using LLMs. While traditional approaches often rely on finetuning separate models for each political faction, our work innovates by employing a singular, instruction-tuned LLM to reflect a spectrum of political ideologies. We present a comprehensive analytical framework, consisting of Partisan Bias Divergence Assessment and Partisan Class Tendency Prediction, to evaluate the model's alignment with real-world political ideologies in terms of stances, emotions, and moral foundations. Our findings reveal the model's effectiveness in capturing emotional and moral nuances, albeit with some challenges in stance detection, highlighting the intricacies and potential for refinement in NLP tools for politically sensitive contexts. This research contributes significantly to the field by demonstrating the feasibility and importance of nuanced political understanding in LLMs, particularly for applications requiring acute awareness of political bias.
</details>
<details>
<summary>摘要</summary>
社交媒体平台上的政治话题非常普遍，因此正确地理解和预测政治偏见使用大型自然语言模型（LLM）变得越来越重要。在这项研究中，我们解决了政治偏见在数字化言语中的理解挑战，使用一个单一、指导 instru 的 LLM，以反映政治意识形态的谱系。我们提出了一个完整的分析框架，包括政治偏见分化评估和政治类倾向预测，以评估模型与实际世界政治意识形态之间的吻合程度。我们的发现表明模型能够很好地捕捉情感和道德上的细 nuances，但在姿势检测方面存在一些挑战，这 highlights NL 工具在政治敏感上的复杂性和可能的改进。本研究对于场景中的政治偏见理解的重要性和可行性作出了重要贡献，特别是在需要精准的政治偏见认知应用场景下。
</details></li>
</ul>
<hr>
<h2 id="R-Tuning-Teaching-Large-Language-Models-to-Refuse-Unknown-Questions"><a href="#R-Tuning-Teaching-Large-Language-Models-to-Refuse-Unknown-Questions" class="headerlink" title="R-Tuning: Teaching Large Language Models to Refuse Unknown Questions"></a>R-Tuning: Teaching Large Language Models to Refuse Unknown Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09677">http://arxiv.org/abs/2311.09677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang</li>
<li>for: 本研究旨在改进语言模型（LLM）的问答能力，特别是避免模型生成非存在的信息（hallucination）。</li>
<li>methods: 我们提出了一种新的approach，即Refusal-Aware Instruction Tuning（R-Tuning），通过初步确定知识差距，然后使用知识交叉构建拒绝意识数据，以便训练LLMs可以回答知道的问题而不回答未知的问题。</li>
<li>results: 实验结果表明，R-Tuning方法可以有效地提高模型回答知道问题的能力，同时避免回答未知问题。此外，在域外数据集上进行测试，发现模型学习不确定性的能力可以通过训练来提高。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the knowledge gap between parametric knowledge and the instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing. Our code will be released at https://github.com/shizhediao/R-Tuning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化了许多领域，但仍面临一些挑战。一个主要问题是这些模型的倾向于生成不存在的事实，被称为幻觉。我们的研究受到了以前的 instrucion 级别调整方法会让模型完成一个句子，无论模型知道这些知识还是不知道。当问题出现在模型的参数知识之外时，它会尝试 fabricate 一个答案，并且无法指示当前缺乏知识。在这篇论文中，我们提出了一种新的方法，即 Refusal-Aware Instruction Tuning（R-Tuning）。这种方法由首先标识模型的参数知识和 instrucion 调整数据之间的知识差异而始。然后，我们将基于知识交叉的 refusal-aware 数据进行调整，以使模型不再回答 beyond 其参数知识的问题。实验结果表明，这种新的 instrucion 调整方法可以有效地提高模型回答知道的问题能力，并且不再回答不知道的问题。此外，当测试在域外数据集时，发现了一个叫做 "拒绝能力" 的元技能，可以在其他任务上generalize。进一步的分析显示，在训练时学习不确定性实际上比测试时 uncertainty-based 测试时更好地估计不确定性。我们的代码将在 GitHub 上发布，请参考 <https://github.com/shizhediao/R-Tuning>。
</details></li>
</ul>
<hr>
<h2 id="Where-Do-People-Tell-Stories-Online-Story-Detection-Across-Online-Communities"><a href="#Where-Do-People-Tell-Stories-Online-Story-Detection-Across-Online-Communities" class="headerlink" title="Where Do People Tell Stories Online? Story Detection Across Online Communities"></a>Where Do People Tell Stories Online? Story Detection Across Online Communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09675">http://arxiv.org/abs/2311.09675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maria-antoniak/stories-online-communities">https://github.com/maria-antoniak/stories-online-communities</a></li>
<li>paper_authors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper</li>
<li>for: 这篇论文是为了研究在线社区中的故事tellding，以便更好地理解社会运动、意识形态、诱导策略等的动态。</li>
<li>methods: 这篇论文使用了一份codebook和Storytelling in Online Communities Corpus，一个专家标注的数据集，以及在线故事检测模型的训练和评估，来研究在线故事tellding的特点和社会上的应用。</li>
<li>results: 根据这篇论文的研究结果，在线故事tellding的特点包括：不同社区中的故事tellding频率不同，各种社区中的故事tellding具有共同的特征，以及在线故事tellding可以跨越不同话题和场景进行交互。<details>
<summary>Abstract</summary>
People share stories online for a myriad of purposes, whether as a means of self-disclosure, processing difficult personal experiences, providing needed information or entertainment, or persuading others to share their beliefs. Better understanding of online storytelling can illuminate the dynamics of social movements, sensemaking practices, persuasion strategies, and more. However, unlike other media such as books and visual content where the narrative nature of the content is often overtly signaled at the document level, studying storytelling in online communities is challenging due to the mixture of storytelling and non-storytelling behavior, which can be interspersed within documents and across diverse topics and settings. We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role storytelling of in different social contexts. We identify distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role of storytelling in different social contexts. Our findings reveal distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Generation-Quality-of-Watermarked-Large-Language-Models-via-Word-Importance-Scoring"><a href="#Improving-the-Generation-Quality-of-Watermarked-Large-Language-Models-via-Word-Importance-Scoring" class="headerlink" title="Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring"></a>Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09668">http://arxiv.org/abs/2311.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Li, Yihan Wang, Zhouxing Shi, Cho-Jui Hsieh</li>
<li>for: 防止大语言模型（LLMs）的不良用户访问。</li>
<li>methods: 使用Token-level watermarking技术，并提出了三种方法来预测重要性分数。</li>
<li>results: 实验结果表明，我们的方法可以生成高质量的文本，同时保持了相同的检测率。<details>
<summary>Abstract</summary>
The strong general capabilities of Large Language Models (LLMs) bring potential ethical risks if they are unrestrictedly accessible to malicious users. Token-level watermarking inserts watermarks in the generated texts by altering the token probability distributions with a private random number generator seeded by its prefix tokens. However, this watermarking algorithm alters the logits during generation, which can lead to a downgraded text quality if it chooses to promote tokens that are less relevant given the input. In this work, we propose to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS). At each generation step, we estimate the importance of the token to generate, and prevent it from being impacted by watermarking if it is important for the semantic correctness of the output. We further propose three methods to predict importance scoring, including a perturbation-based method and two model-based methods. Empirical experiments show that our method can generate texts with better quality with comparable level of detection rate.
</details>
<details>
<summary>摘要</summary>
强大的普通语言模型（LLM）具有潜在的道德风险，如果这些模型在黑客用户手中不受限制。Token-level watermarking在生成文本时插入水印，通过修改token概率分布来增加一个私有随机数生成器。然而，这种水印算法在生成过程中改变了logits，可能会导致生成的文本质量下降，如果它选择推荐不太相关的token。在这种情况下，我们提出了通过水印 scoring（WIS）来改进生成的文本质量。在每个生成步骤中，我们估算token需要生成的重要性，并在生成过程中避免由水印所影响的重要token。我们还提出了三种方法来预测重要性分配，包括一种干扰基本方法和两种模型基本方法。实验表明，我们的方法可以生成文本质量更高，同时保持检测率在相同水平。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLM-Agent-Group-Dynamics-against-Human-Group-Dynamics-A-Case-Study-on-Wisdom-of-Partisan-Crowds"><a href="#Evaluating-LLM-Agent-Group-Dynamics-against-Human-Group-Dynamics-A-Case-Study-on-Wisdom-of-Partisan-Crowds" class="headerlink" title="Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds"></a>Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09665">http://arxiv.org/abs/2311.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Siddharth Suresh, Nikunj Harlalka, Agam Goyal, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers</li>
<li>for: 本研究探讨了大自然语言模型（LLM）是否可以模拟人类群体动态，特别在政治敏感背景下。</li>
<li>methods: 我们使用LLM agents扮演为民主和共和两个政治人物，在一种类似于人类群体研究的结构化互动中进行了模拟。我们的方法评估了代理者响应如何随社会影响而发展。</li>
<li>results: 我们发现，不含链条思维（CoT）的LLM代理者具有与人类行为高度相似的Alignment，而含CoT的代理者则受到了Alignment的降低。此外，在人类数据进行精细调整LLM代理者后，可以实现人类样式的行为，但也存在过拟合特定行为的风险。这些发现表明了LLM代理者在模型人类群体现象方面的潜力和局限性。<details>
<summary>Abstract</summary>
This study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents' responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究 investigate LLM（大语言模型）能模拟人类群体动态，尤其在政治敏感的背景下。我们使用LLM代理人物，模拟人类群体中的决策过程，并评估代理人物如何受社会影响。我们的关键发现表明，没有Chain-of-Thought（CoT）解释的LLM代理人物和人类行为高度相似，而CoT解释会降低对应性。然而，向代理人物添加明确的偏见不一定提高群体智慧。此外，使用人类数据进行LLM fine-tuning显示 promise in achieving human-like behavior，但也存在适应特定行为的风险。这些发现表明LLM代理人物在模拟人类群体现象的潜在和局限性。
</details></li>
</ul>
<hr>
<h2 id="Evolving-Domain-Adaptation-of-Pretrained-Language-Models-for-Text-Classification"><a href="#Evolving-Domain-Adaptation-of-Pretrained-Language-Models-for-Text-Classification" class="headerlink" title="Evolving Domain Adaptation of Pretrained Language Models for Text Classification"></a>Evolving Domain Adaptation of Pretrained Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09661">http://arxiv.org/abs/2311.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Yi Wu, Dhruv Gupta, Rheeya Uppaal, Ananya Kumar, Luhang Sun, Makesh Narsimhan Sreedhar, Sijia Yang, Timothy T. Rogers, Junjie Hu</li>
<li>for: 维护语言模型的精度在应用中，特别是在观点探测中。</li>
<li>methods: 本研究探讨了对于语言模型进行不断更新的方法，以适应语言模型在不断变化的语言环境中的应用。</li>
<li>results: 研究发现，对于语言模型进行自我训练方法能够优化语言模型在不断变化的语言环境中的性能，并且比传统的领域适应技术高效。<details>
<summary>Abstract</summary>
Adapting pre-trained language models (PLMs) for time-series text classification amidst evolving domain shifts (EDS) is critical for maintaining accuracy in applications like stance detection. This study benchmarks the effectiveness of evolving domain adaptation (EDA) strategies, notably self-training, domain-adversarial training, and domain-adaptive pretraining, with a focus on an incremental self-training method. Our analysis across various datasets reveals that this incremental method excels at adapting PLMs to EDS, outperforming traditional domain adaptation techniques. These findings highlight the importance of continually updating PLMs to ensure their effectiveness in real-world applications, paving the way for future research into PLM robustness against the natural temporal evolution of language.
</details>
<details>
<summary>摘要</summary>
这篇研究评估了对于时间序列文本分类中的预训语言模型（PLM）进行演进领域变化（EDS）的适用性，以维持准确性。研究对于自我训练、领域抗战术和领域适应训练等不同的演进领域整合策略进行比较，并着重于增量自我训练方法。我们的分析发现，这种增量方法在处理EDS时表现出色，超过传统领域整合策略。这些发现显示了预训语言模型的更新和调整的重要性，以确保它们在实际应用中的效能。这将推动未来关于PLM的研究，以探索它们对自然时间演进的语言的Robustness。
</details></li>
</ul>
<hr>
<h2 id="ICXML-An-In-Context-Learning-Framework-for-Zero-Shot-Extreme-Multi-Label-Classification"><a href="#ICXML-An-In-Context-Learning-Framework-for-Zero-Shot-Extreme-Multi-Label-Classification" class="headerlink" title="ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification"></a>ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09649">http://arxiv.org/abs/2311.09649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaxinzhuars/icxml">https://github.com/yaxinzhuars/icxml</a></li>
<li>paper_authors: Yaxin Zhu, Hamed Zamani</li>
<li>for: 这篇论文主要针对EXTREME多标签分类任务（XMC），目标是为每个实例预测多个标签，从极其大的标签空间中预测。</li>
<li>methods: 该论文提出了一种两阶段方法，称为In-Context Extreme Multilabel Learning（ICXML），通过在上下文学习中生成候选标签并进行重新排序，以降低搜索空间。</li>
<li>results: 对于两个公共评分平台，实验结果表明ICXML已经提高了状态之arte。<details>
<summary>Abstract</summary>
This paper focuses on the task of Extreme Multi-Label Classification (XMC) whose goal is to predict multiple labels for each instance from an extremely large label space. While existing research has primarily focused on fully supervised XMC, real-world scenarios often lack complete supervision signals, highlighting the importance of zero-shot settings. Given the large label space, utilizing in-context learning approaches is not trivial. We address this issue by introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through incontext learning and then reranks them. Extensive experiments suggest that ICXML advances the state of the art on two diverse public benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Event-Causality-Is-Key-to-Computational-Story-Understanding"><a href="#Event-Causality-Is-Key-to-Computational-Story-Understanding" class="headerlink" title="Event Causality Is Key to Computational Story Understanding"></a>Event Causality Is Key to Computational Story Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09648">http://arxiv.org/abs/2311.09648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yidan Sun, Qin Chao, Boyang Li</li>
<li>for: 本研究旨在探讨人类故事理解中事件 causality 的中心作用，以及如何利用这些事件 causality 进行符号故事生成。</li>
<li>methods: 我们采用了最新的大语言模型 (LLMs) 来开发一种用于事件 causality 识别的方法，并通过设计特定的提示来提取 GPT 中的事件 causal 关系。</li>
<li>results: 我们的方法在比较 human-annotated 的事件 causal 关系集合 GLUCOSE 中表现出类似的水平，同时能够轻松地扩展到不同类型和长度的故事。这些EXTRACTED causal 关系导致了对故事质量评价的提高（5.7%）和对故事视频文本对应性的提高（8.7%）。<details>
<summary>Abstract</summary>
Psychological research suggests the central role of event causality in human story understanding. Further, event causality has been heavily utilized in symbolic story generation. However, few machine learning systems for story understanding employ event causality, partially due to the lack of reliable methods for identifying open-world causal event relations. Leveraging recent progress in large language models (LLMs), we present the first method for event causality identification that leads to material improvements in computational story understanding. We design specific prompts for extracting event causal relations from GPT. Against human-annotated event causal relations in the GLUCOSE dataset, our technique performs on par with supervised models, while being easily generalizable to stories of different types and lengths. The extracted causal relations lead to 5.7\% improvements on story quality evaluation and 8.7\% on story video-text alignment. Our findings indicate enormous untapped potential for event causality in computational story understanding.
</details>
<details>
<summary>摘要</summary>
心理研究表明人类故事理解中心stage causality的重要性。此外，event causality在Symbolic story generation中得到了广泛使用。然而，现代机器学习系统 для故事理解 rarely employ event causality，部分原因是没有可靠的方法来确定开放世界的 causal event relations。基于最近的大语言模型（LLMs），我们提出了首个事件 causality identification的方法，该方法在计算机故事理解中产生了Material improvements。我们为GPT设计了特定的提示，以EXTRACT event causal relations。与人类标注的事件 causal relations在GLUCOSE数据集中，我们的技术与超级vised模型相当，而且可以轻松扩展到不同的类型和长度的故事。提取的 causal relations导致了5.7%的故事质量评估提高和8.7%的故事视频-文本对齐提高。我们的发现表明事件 causality在计算机故事理解中存在巨大的untapped potential。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-In-Context-Learning-of-Libraries-for-Code-Generation"><a href="#Evaluating-In-Context-Learning-of-Libraries-for-Code-Generation" class="headerlink" title="Evaluating In-Context Learning of Libraries for Code Generation"></a>Evaluating In-Context Learning of Libraries for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09635">http://arxiv.org/abs/2311.09635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Arkil Patel, Siva Reddy, Dzmitry Bahdanau, Pradeep Dasigi</li>
<li>for: 本研究旨在系统地评估不同领域特化的大型自然语言模型（LLMs）在基于受欢迎库模块的代码生成中的能力和限制。</li>
<li>methods: 本研究使用了多种场景，反映不同领域的特化，以评估不同大型LLMs在基于受欢迎库模块的代码生成中的能力和限制。</li>
<li>results: 研究结果显示，即使使用小型开源LLMs如Llama-2和StarCoder，也能够很好地理解新的代码库模块，基于受欢迎库模块的 спецификации进行受欢迎库模块的代码生成。此外，研究还发现，LLMs可以通过自然语言描述或 raw code 实现来学习新的库模块，这些资源通常比示例更加便宜。总之，本研究的结果铺平了在更多的适应和动态编程环境中使用LLMs的道路。<details>
<summary>Abstract</summary>
Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型（LLM）表现出了高度的代码生成和理解能力。特别是在解决用户指令下的代码模块解释方面表现出了极高的能力。最近的研究表明，大型专有LLM可以通过示例学习新的库使用。这些结果提出了多个开放问题：是否需要示例学习，小型（更开放）模型也具备这种能力等。在这项工作中，我们采取了更广泛的方法，系统地评估了多种LLM在不同领域专业化的三个场景中代码生成能力。我们的结果显示，即使使用小型开源LLM like Llama-2和StarCoder，也能够很好地理解新的代码库，基于场景中提供的规范进行解释。我们的发现还表明，LLM在只有自然语言描述或Raw code实现函数时仍然能够学习新的库模块，这些函数经常比示例更容易获得。总的来说，我们的结果为使用LLM在更适应和动态编程环境中做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="From-Scroll-to-Misbelief-Modeling-the-Unobservable-Susceptibility-to-Misinformation-on-Social-Media"><a href="#From-Scroll-to-Misbelief-Modeling-the-Unobservable-Susceptibility-to-Misinformation-on-Social-Media" class="headerlink" title="From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media"></a>From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09630">http://arxiv.org/abs/2311.09630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanchen Liu, Mingyu Derek Ma, Wenna Qin, Azure Zhou, Jiaao Chen, Weiyan Shi, Wei Wang, Diyi Yang</li>
<li>for: 这个研究的目的是提出一种计算模型，以推测用户受到谣言的程度。</li>
<li>methods: 该模型基于用户的活动记录，利用 observable sharing behavior 进行监督，以推算用户的受到谣言程度。</li>
<li>results: 评估表示，该模型的估计与人类判断相吻合度很高。此外，该研究还发现了不同社会因素对受到谣言程度的相关性。<details>
<summary>Abstract</summary>
Susceptibility to misinformation describes the extent to believe unverifiable claims, which is hidden in people's mental process and infeasible to observe. Existing susceptibility studies heavily rely on the self-reported beliefs, making any downstream applications on susceptability hard to scale. To address these limitations, in this work, we propose a computational model to infer users' susceptibility levels given their activities. Since user's susceptibility is a key indicator for their reposting behavior, we utilize the supervision from the observable sharing behavior to infer the underlying susceptibility tendency. The evaluation shows that our model yields estimations that are highly aligned with human judgment on users' susceptibility level comparisons. Building upon such large-scale susceptibility labeling, we further conduct a comprehensive analysis of how different social factors relate to susceptibility. We find that political leanings and psychological factors are associated with susceptibility in varying degrees.
</details>
<details>
<summary>摘要</summary>
人们的信息受感染度描述了他们信任未经验证的说法的程度，这个程度隐藏在人们的思维过程中，无法直接观察。现有的受感染性研究主要基于自我报告的信念，这使得下游应用困难扩大。为解决这些限制，在这项工作中，我们提出了一种计算模型，用于根据用户的活动来推断他们的受感染性水平。由于用户的受感染性是共享行为的关键指标，我们利用共享行为的监督来推断受感染性的倾向。我们的评估结果显示，我们的模型可以提供与人类判断高度一致的用户受感染性水平的估计。基于大规模的受感染性标签，我们进一步进行了社会因素如政治倾向和心理因素与受感染性之间的全面分析。我们发现，政治倾向和心理因素在不同程度上与受感染性相关。
</details></li>
</ul>
<hr>
<h2 id="Take-One-Step-at-a-Time-to-Know-Incremental-Utility-of-Demonstration-An-Analysis-on-Reranking-for-Few-Shot-In-Context-Learning"><a href="#Take-One-Step-at-a-Time-to-Know-Incremental-Utility-of-Demonstration-An-Analysis-on-Reranking-for-Few-Shot-In-Context-Learning" class="headerlink" title="Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning"></a>Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09619">http://arxiv.org/abs/2311.09619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuma Hashimoto, Karthik Raman, Michael Bendersky</li>
<li>for: 本研究旨在分析不同标签策略对目标任务的影响。</li>
<li>methods: 本研究使用了LLMs的输出概率和任务特定的奖励来评估不同的标策略。</li>
<li>results: 研究发现，当输出概率分布在整个值范围内时，概率是有效的（在分类任务上），而在 segmentation 和翻译任务上，提供细化的奖励值和长输出可以使下游指标更加稳定。此外，提出了一种新的标策方法——增量有用性，可以评估LLMs中带入的新知识增量。<details>
<summary>Abstract</summary>
In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs' outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs' output probability given ground-truth output, and task-specific reward given LLMs' prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arabic, English, Finnish, Japanese, and Spanish. Our results show that (1) the probability is effective when the probability values are distributed across the whole value range (on the classification tasks), and (2) the downstream metric is more robust when nuanced reward values are provided with long outputs (on the segmentation and translation tasks). We then show that the proposed incremental utility further helps ICL by contrasting how the LLMs perform with and without the demonstrations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）的嵌入式学习（ICL）是一种出现的能力。只需要几个示例，LLMs 就可以作为黑obox для新任务使用。先前的研究表明，使用 LLMs 的输出作为标签可以有效地培训模型选择示例。这个标签预期能够估算示例在 ICL 中的用于性能。然而，不同的标签策略对目标任务的影响还未得到很好的理解。这篇论文分析了不同的用于性能的标签策略，并对 LLMs 的输出概率和任务特定的奖励给出了分析。与先前的工作不同，我们提出了一种新的标签方法，即增量用处，可以评估示例带来 LLMS 中的增量知识。我们在使用 instruction-tuned LLMs 进行了 binary/多类分类、分割和翻译任务，并在阿拉伯语、英语、芬兰语、日语和西班牙语等语言上进行了实验。我们的结果表明：1）在分类任务中，当概率值分布在整个值范围内时，概率效果非常高；2）在分割和翻译任务中，提供细化的奖励值和长输出可以使下游指标更加稳定。然后，我们表明了我们提出的增量用处可以进一步帮助 ICL。
</details></li>
</ul>
<hr>
<h2 id="Simulating-Opinion-Dynamics-with-Networks-of-LLM-based-Agents"><a href="#Simulating-Opinion-Dynamics-with-Networks-of-LLM-based-Agents" class="headerlink" title="Simulating Opinion Dynamics with Networks of LLM-based Agents"></a>Simulating Opinion Dynamics with Networks of LLM-based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09618">http://arxiv.org/abs/2311.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers</li>
<li>for: 这篇论文旨在 simulating human opinion dynamics 以及 understanding societal phenomena, such as polarization and the spread of misinformation.</li>
<li>methods: 本文使用 Large Language Models (LLMs) 来模拟意见动态, 并通过 prompt engineering 来导致confirmation bias.</li>
<li>results: 研究发现 LLM agents 具有强烈的倾向 towards accurate information, leading to consensus in line with scientific reality. However, this bias limits the simulation of individuals with resistant views on issues like climate change, leading to opinion fragmentation.<details>
<summary>Abstract</summary>
Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations lack fidelity to human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards accurate information, leading to consensus in line with scientific reality. However, this bias limits the simulation of individuals with resistant views on issues like climate change. After inducing confirmation bias through prompt engineering, we observed opinion fragmentation in line with existing agent-based research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.
</details>
<details>
<summary>摘要</summary>
准确模拟人类意见动态对社会现象的理解具有重要意义，包括分化和信息的快速传播。然而，常用的Agent-based模型（ABM）在模拟人类行为方面缺乏准确性。我们提出一种基于大语言模型（LLM）的新方法来模拟意见动态。我们的发现表明LLM代理具有准确信息的强烈偏好，导致与科学实际相符的共识。然而，这种偏好限制了对抵抗看法的个体模拟，如气候变化。通过引入确认偏见通过提示工程，我们观察到意见分化与现有的ABM研究相符。这些发现表明LLM代理在这个领域的承诺和局限性，并建议通过与现实世界的对话来更好地模拟人类信念的演化。
</details></li>
</ul>
<hr>
<h2 id="On-Retrieval-Augmentation-and-the-Limitations-of-Language-Model-Training"><a href="#On-Retrieval-Augmentation-and-the-Limitations-of-Language-Model-Training" class="headerlink" title="On Retrieval Augmentation and the Limitations of Language Model Training"></a>On Retrieval Augmentation and the Limitations of Language Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09615">http://arxiv.org/abs/2311.09615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Rui Chiang, Xinyan Velocity Yu, Joshua Robinson, Ollie Liu, Isabelle Lee, Dani Yogatama</li>
<li>for: 这个论文的目的是探讨一种语言模型（LM）的改进方法，即通过k-最近邻（kNN） Retrieval来降低LM的复杂度。</li>
<li>methods: 该论文使用了一些新的数据集和分析方法来研究LM的各种特性和表现。其中包括了“软max瓶颈”的排除和“多层perceptron（MLP）障碍现象”的发现。</li>
<li>results: 研究发现，通过将kNN Retrieval incorporated into vanilla GPT-2 117M可以有效地提高LM的性能，特别是在针对不相关的训练数据进行探索和泛化时。<details>
<summary>Abstract</summary>
Augmenting a language model (LM) with $k$-nearest neighbors (kNN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remains elusive. In this work, we first rule out one previously posited possibility -- the "softmax bottleneck." We further identify the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM optimization early on. We explore memorization and generalization in language models with two new datasets, where advanced model like GPT-3.5-turbo find generalizing to irrelevant information in the training data challenging. However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently improve performance in this setting.
</details>
<details>
<summary>摘要</summary>
Language model (LM) 可以通过 $k$-nearest neighbors（kNN） Retrieval on its training data alone 降低其plexity，但其下面的原因仍然不明确。在这项工作中，我们首先排除了一个先前提出的可能性——“softmax瓶颈”。我们进一步发现了 MLP 障碍现象，即 LM 的最后一层 MLP 层可能会阻碍 LM 优化的初始阶段。我们通过使用两个新的数据集进行了Memorization和Generalization的探索，发现高级模型如 GPT-3.5-turbo 在training数据中分配 irrelevant information 的泛化很困难。然而，在vanilla GPT-2 117M中添加 kNN Retrieval 可以一直提高性能在这种设定下。
</details></li>
</ul>
<hr>
<h2 id="Efficient-End-to-End-Visual-Document-Understanding-with-Rationale-Distillation"><a href="#Efficient-End-to-End-Visual-Document-Understanding-with-Rationale-Distillation" class="headerlink" title="Efficient End-to-End Visual Document Understanding with Rationale Distillation"></a>Efficient End-to-End Visual Document Understanding with Rationale Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09612">http://arxiv.org/abs/2311.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Zhu, Alekh Agarwal, Mandar Joshi, Robin Jia, Jesse Thomason, Kristina Toutanova</li>
<li>for: Visual document understanding benchmarks</li>
<li>methods: 使用小型预训练图像到文本模型进行选择性文本或布局认识和理解，作为末端模型的中间推理步骤。</li>
<li>results: Student model based on Pix2Struct achieved consistent improvements on three visual document understanding benchmarks, with improvements of more than 4% absolute over a comparable Pix2Struct model that predicts answers directly.<details>
<summary>Abstract</summary>
Understanding visually situated language requires recognizing text and visual elements, and interpreting complex layouts. State-of-the-art methods commonly use specialized pre-processing tools, such as optical character recognition (OCR) systems, that map document image inputs to extracted information in the space of textual tokens, and sometimes also employ large language models (LLMs) to reason in text token space. However, the gains from external tools and LLMs come at the cost of increased computational and engineering complexity. In this paper, we ask whether small pretrained image-to-text models can learn selective text or layout recognition and reasoning as an intermediate inference step in an end-to-end model for pixel-level visual language understanding. We incorporate the outputs of such OCR tools, LLMs, and larger multimodal models as intermediate ``rationales'' on training data, and train a small student model to predict both rationales and answers for input questions based on those training examples. A student model based on Pix2Struct (282M parameters) achieves consistent improvements on three visual document understanding benchmarks representing infographics, scanned documents, and figures, with improvements of more than 4\% absolute over a comparable Pix2Struct model that predicts answers directly.
</details>
<details>
<summary>摘要</summary>
理解图文需要识别文本和视觉元素，并解释复杂的布局。现代方法通常使用专门的预处理工具，如光学字符识别（OCR）系统，将文档图像输入映射到提取的信息空间中的文本 токен中，并有时还使用大型语言模型（LLM）来在文本 токен空间中进行理解。然而，外部工具和LLM的成本是计算和工程复杂性的增加。在这篇论文中，我们问 Whether small pretrained image-to-text模型可以学习选择性的文本或布局认识和理解作为末端模型的中间推理步骤。我们将OCR工具、LLM和更大的多Modal模型的输出作为训练数据中的中间“理由”，并训练一个小型学生模型来根据输入问题预测 rationales和答案。一个基于 Pix2Struct 的小型学生模型（282M参数）在三个视觉文档理解标准准中表现出了逐渐提高，超过4%的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="GistScore-Learning-Better-Representations-for-In-Context-Example-Selection-with-Gist-Bottlenecks"><a href="#GistScore-Learning-Better-Representations-for-In-Context-Example-Selection-with-Gist-Bottlenecks" class="headerlink" title="GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks"></a>GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09606">http://arxiv.org/abs/2311.09606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivanshu Gupta, Clemens Rosenbaum, Ethan R. Elenberg</li>
<li>for: 这paper aimed to improve the in-context learning (ICL) performance of large language models (LLMs) by selecting the best examples from a candidate pool.</li>
<li>methods: The authors proposed a novel metric called GistScore, which is based on Example Gisting, a technique for training example retrievers using an attention bottleneck. They also experimented with fine-tuning gist models on each dataset and multi-task training a single model on a large collection of datasets.</li>
<li>results: The authors achieved state-of-the-art ICL performance on 21 diverse datasets spanning 9 tasks, with an average absolute gain of 20% over off-the-shelf retrievers and 7% over the best prior methods. Their multi-task model also generalizes well out-of-the-box to new task categories, datasets, and prompt templates, with retrieval speeds that are consistently thousands of times faster than the best prior training-free method.<details>
<summary>Abstract</summary>
Large language models (LLMs) have the ability to perform in-context learning (ICL) of new tasks by conditioning on prompts comprising a few task examples. This work studies the problem of selecting the best examples given a candidate pool to improve ICL performance on given a test input. Existing approaches either require training with feedback from a much larger LLM or are computationally expensive. We propose a novel metric, GistScore, based on Example Gisting, a novel approach for training example retrievers for ICL using an attention bottleneck via Gisting, a recent technique for compressing task instructions. To tradeoff performance with ease of use, we experiment with both fine-tuning gist models on each dataset and multi-task training a single model on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we show that our fine-tuned models get state-of-the-art ICL performance with 20% absolute average gain over off-the-shelf retrievers and 7% over the best prior methods. Our multi-task model generalizes well out-of-the-box to new task categories, datasets, and prompt templates with retrieval speeds that are consistently thousands of times faster than the best prior training-free method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有培根学习（ICL）新任务的能力，通过条件Prompt中的一些任务示例来实现。这项工作研究如何选择最佳示例集来提高ICL性能，以便对给定输入进行测试。现有方法可能需要与更大的LLM进行培训或者计算成本较高。我们提出了一个新的指标——GistScore，基于Example Gisting，一种新的培训示例检索器 для ICL 使用注意力瓶颈via Gisting，一种最近的技术用于压缩任务说明。为了让性能和使用方便进行权衡，我们进行了练习 fine-tuning gist模型 на每个数据集和多任务训练单个模型在一个大量数据集上。在21个多样化的数据集和9个任务上，我们显示了我们的精心调整模型可以达到状态之最ICL性能，与各种off-the-shelf retrievers相比，具有20%的绝对均值提升，并且与最佳先前方法相比，具有7%的提升。我们的多任务模型在新的任务类别、数据集和提示模板上具有良好的泛化能力，并且在输入速度上与最佳先前无需培训的方法相比，保持了一定的速度优势。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Improving-Attentiveness-to-Partial-Inputs-with-Counterfactuals"><a href="#Measuring-and-Improving-Attentiveness-to-Partial-Inputs-with-Counterfactuals" class="headerlink" title="Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals"></a>Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09605">http://arxiv.org/abs/2311.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanai Elazar, Bhargavi Paranjape, Hao Peng, Sarah Wiegreffe, Khyathi Raghavi, Vivek Srikumar, Sameer Singh, Noah A. Smith</li>
<li>for: 本研究旨在检验现有的超vised和in-context学习模型是否过分依赖于训练数据中的偶合关系，以及Counterfactual Attentiveness Test（CAT）是否能够改善模型的抽象能力。</li>
<li>methods: 本研究使用Counterfactual Attentiveness Test（CAT）来系统地检验了十个dataset上四个任务（自然语言推理、阅读理解、句子重构、视觉语言理解）上的established supervised和in-context learning模型。CAT使用对应的counterfactual来替换训练数据中的一部分，并期望模型能够根据这些counterfactual进行不同的预测。</li>
<li>results: 研究发现，依赖于训练数据中的偶合关系的依赖性是主要的数据依赖性。另外，研究发现GPT3在增加示例数量后变得更加不够注意力，而其测试数据上的准确率提高。结果表明，在训练或示例数据中添加counterfactual可以提高模型的抽象能力。此外，CAT测试表明，模型的注意力测量不同于 solely measuring correlations in data。<details>
<summary>Abstract</summary>
The inevitable appearance of spurious correlations in training datasets hurts the generalization of NLP models on unseen data. Previous work has found that datasets with paired inputs are prone to correlations between a specific part of the input (e.g., the hypothesis in NLI) and the label; consequently, models trained only on those outperform chance. Are these correlations picked up by models trained on the full input data? To address this question, we propose a new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses counterfactuals by replacing part of the input with its counterpart from a different example (subject to some restrictions), expecting an attentive model to change its prediction. Using CAT, we systematically investigate established supervised and in-context learning models on ten datasets spanning four tasks: natural language inference, reading comprehension, paraphrase detection, and visual & language reasoning. CAT reveals that reliance on such correlations is mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive with an increased number of demonstrations, while its accuracy on the test data improves. Our results demonstrate that augmenting training or demonstration data with counterfactuals is effective in improving models' attentiveness. We show that models' attentiveness measured by CAT reveals different conclusions from solely measuring correlations in data.
</details>
<details>
<summary>摘要</summary>
“训练数据中偶现的假象相关性会对NLP模型的泛化性产生负面影响。先前的研究发现，带有对应输入部分（如NLI中的假设）和标签之间存在相关性，导致使用只有这些输入训练的模型能够超过偶散。现在我们提出了一种新的评估方法：对比性注意力测试（CAT）。CAT使用对比例的方法，替换输入中的一部分（保留一些限制），期望一个注意力强的模型会改变其预测。通过CAT，我们系统地研究了多种supervised和in-context学习模型在十个 datasets 上，涵盖四个任务：自然语言推理、阅读理解、句子重写检测和视觉语言理解。结果表明，模型对这些相关性的依赖性是数据висиendent的。另外，我们发现GPT3在增加示例数量后，其注意力度会降低，而测试数据上的准确率会提高。我们的结果表明，在训练或示例数据中添加对比例可以提高模型的注意力度。我们的结果还表明，通过CAT评估模型的注意力度可以从数据中的相关性中分离出不同的结论。”
</details></li>
</ul>
<hr>
<h2 id="SCORE-A-framework-for-Self-Contradictory-Reasoning-Evaluation"><a href="#SCORE-A-framework-for-Self-Contradictory-Reasoning-Evaluation" class="headerlink" title="SCORE: A framework for Self-Contradictory Reasoning Evaluation"></a>SCORE: A framework for Self-Contradictory Reasoning Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09603">http://arxiv.org/abs/2311.09603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, Jieyu Zhao</li>
<li>for: 这 paper 旨在分析大语言模型（LLM）是否真的具备良好的理解能力，以及这种能力是如何影响下游任务的性能。</li>
<li>methods: 这 paper 使用了一种名为 \textsc{SCORE} 的框架来分析 LLM 的理解能力。特别是，它关注自相矛盾的理解，即 LLM 在处理含有上下文信息和常识的任务时，可能会出现自相矛盾的行为。</li>
<li>results: 研究发现，LLM 在多个视点 Setting 下表现不稳定，甚至对正确预测也可能表现出含糊不清的理解。这些结果指出了 LLM 的理解能力有很大的改进空间，并且需要进一步的研究来确定评价reasoning的最佳实践。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive reasoning ability in various language-based tasks. Despite many proposed reasoning methods aimed at enhancing performance in downstream tasks, two fundamental questions persist: Does reasoning genuinely support predictions, and how reliable is the quality of reasoning? In this paper, we propose a framework \textsc{SCORE} to analyze how well LLMs can reason. Specifically, we focus on self-contradictory reasoning, where reasoning does not support the prediction. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information and commonsense. The model may miss evidence or use shortcuts, thereby exhibiting self-contradictory behaviors. We also employ the Point-of-View (POV) method, which probes models to generate reasoning from multiple perspectives, as a diagnostic tool for further analysis. We find that though LLMs may appear to perform well in one-perspective settings, they fail to stabilize such behavior in multi-perspectives settings. Even for correct predictions, the reasoning may be messy and incomplete, and LLMs can easily be led astray from good reasoning. \textsc{SCORE}'s results underscore the lack of robustness required for trustworthy reasoning and the urgency for further research to establish best practices for a comprehensive evaluation of reasoning beyond accuracy-based metrics.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxìng yǔyán módel).* "Reasoning" is translated as "理解" (lǐjiě) or "解释" (jiějie).* "Self-contradictory reasoning" is translated as "自相矛盾的理解" (zìxiāng dòuduō de lǐjiě).* "Point-of-View" is translated as "视角" (wénjiàng).* "Multi-perspectives" is translated as "多视角" (duōwénjiàng).* "Messy and incomplete" is translated as "杂乱不完整" (zàilàng bù qiáncháng).* "Trustworthy reasoning" is translated as "可靠的理解" (kěkuài de lǐjiě).
</details></li>
</ul>
<hr>
<h2 id="Language-Models-Mostly-Do-Not-Consider-Emotion-Triggers-When-Predicting-Emotion"><a href="#Language-Models-Mostly-Do-Not-Consider-Emotion-Triggers-When-Predicting-Emotion" class="headerlink" title="Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion"></a>Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09602">http://arxiv.org/abs/2311.09602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Smriti Singh, Cornelia Caragea, Junyi Jessy Li</li>
<li>for: 这个研究是为了检验大型自然语言模型（LLM）和精度调整模型（Fine-tuned models）是否能够正确地识别情绪诱发因素（emotion triggers）。</li>
<li>methods: 该研究使用了一个新的数据集EmoTrigger，该数据集包含900个社交媒体文章，来源于三个不同的数据集，并由专家 manually annotated为情绪诱发因素。</li>
<li>results: 研究发现，情绪诱发因素并不是情绪预测模型中考虑的重要特征，而是存在详细的相互作用 между各种特征和情绪检测任务。<details>
<summary>Abstract</summary>
Situations and events evoke emotions in humans, but to what extent do they inform the prediction of emotion detection models? Prior work in emotion trigger or cause identification focused on training models to recognize events that trigger an emotion. Instead, this work investigates how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions. First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models. Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.
</details>
<details>
<summary>摘要</summary>
情感Trigger的情况和事件会让人们表现出不同的情感，但到底这些事件会如何影响情感探测模型的预测呢？先前的工作主要集中在训练模型可以识别引发情感的事件上，而这个工作则是 investigate how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions。我们首先介绍了一个新的数据集EmotTrigger，该数据集包含900个社交媒体文章，来自三个不同的数据集，这些文章由专家进行情感触发点的注释，注释具有高度一致性。使用EmotTrigger数据集，我们评估了大型自然语言模型（LLMs）能够识别情感触发点，并对这些任务之间的细节进行比较分析。我们的分析发现，情感触发点并不是情感预测模型考虑的重要特征，而是各种特征之间的细节很复杂地相互作用，以实现情感预测任务。
</details></li>
</ul>
<hr>
<h2 id="LifeTox-Unveiling-Implicit-Toxicity-in-Life-Advice"><a href="#LifeTox-Unveiling-Implicit-Toxicity-in-Life-Advice" class="headerlink" title="LifeTox: Unveiling Implicit Toxicity in Life Advice"></a>LifeTox: Unveiling Implicit Toxicity in Life Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09585">http://arxiv.org/abs/2311.09585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minbeomkim/LifeTox">https://github.com/minbeomkim/LifeTox</a></li>
<li>paper_authors: Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung</li>
<li>for: 这个论文的目的是为了检测生活中的隐式恶意言语。</li>
<li>methods: 这个论文使用了RoBERTa模型，并在LifeTox数据集上进行了微调。</li>
<li>results: 实验表明，RoBERTa模型在隐式恶意言语分类任务中匹配或超过了现有的大语言模型的零shot性能。<details>
<summary>Abstract</summary>
As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity.
</details>
<details>
<summary>摘要</summary>
Large language models are becoming increasingly integrated into daily life, so detecting implicit toxicity across diverse contexts is crucial. To address this challenge, we introduce LifeTox, a dataset designed for identifying implicit toxicity in a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox includes diverse contexts derived from personal experiences through open-ended questions. Experimental results show that RoBERTa fine-tuned on LifeTox performs equally well or even better than large language models in toxicity classification tasks, demonstrating the effectiveness of LifeTox in addressing the complex challenges of implicit toxicity.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Text-Evaluation-with-GPT-4"><a href="#Enhancing-Medical-Text-Evaluation-with-GPT-4" class="headerlink" title="Enhancing Medical Text Evaluation with GPT-4"></a>Enhancing Medical Text Evaluation with GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09581">http://arxiv.org/abs/2311.09581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqing Xie, Sheng Zhang, Hao Cheng, Zelalem Gero, Cliff Wong, Tristan Naumann, Hoifung Poon</li>
<li>for: 针对医疗文本生成评估中的准确性评价。</li>
<li>methods: 提出基于GPT-4的医疗文本评估方法，包括细致性评估方面和相关医疗领域模型训练。</li>
<li>results: 与现有评价 metric 比较，提出的GPT-4基于评价方法在医疗笔记生成和医疗报告摘要任务上显示了substantially higher的一致性。<details>
<summary>Abstract</summary>
In the evaluation of medical text generation, it is essential to scrutinize each piece of information and ensure the utmost accuracy of the evaluation. Existing evaluation metrics either focus on coarse-level evaluation that assigns one score for the whole generated output or rely on evaluation models trained on general domain, resulting in inaccuracies when adapted to the medical domain. To address these issues, we propose a set of factuality-centric evaluation aspects and design corresponding GPT-4-based metrics for medical text generation. We systematically compare these metrics with existing ones on clinical note generation and medical report summarization tasks, revealing low inter-metric correlation. A comprehensive human evaluation confirms that the proposed GPT-4-based metrics exhibit substantially higher agreement with human judgments than existing evaluation metrics. Our study contributes to the understanding of medical text generation evaluation and offers a more reliable alternative to existing metrics.
</details>
<details>
<summary>摘要</summary>
在医学文本生成评估中，必须仔细检查每个信息并确保评估的准确性。现有的评估指标可能会将整个生成输出的评估授予一个分数，或者基于通用领域的评估模型，导致在医学领域中出现不准确的评估。为解决这些问题，我们提出了一组中心于事实的评估方面和基于GPT-4的评估指标，用于医学文本生成。我们系统比较了这些指标与现有指标的相关性，发现它们在医学报告摘要和医学病历生成任务上显示了低相关性。人工评估表明，我们提出的GPT-4基于的评估指标与人类判断更为一致，与现有指标相比，具有更高的一致性。我们的研究增进了医学文本生成评估的理解，并提供了更可靠的评估方法。
</details></li>
</ul>
<hr>
<h2 id="MMOE-Mixture-of-Multimodal-Interaction-Experts"><a href="#MMOE-Mixture-of-Multimodal-Interaction-Experts" class="headerlink" title="MMOE: Mixture of Multimodal Interaction Experts"></a>MMOE: Mixture of Multimodal Interaction Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09580">http://arxiv.org/abs/2311.09580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency</li>
<li>for: 本研究旨在解决现实世界中新型多modal交互的问题，例如让机器学习模型更好地理解混乱的语言和手势之间的互动关系。</li>
<li>methods: 本研究提出了一种新的方法 called MMOE（多modal交互专家杂合），它可以自动将数据点分类为不同的交互类型，并采用特定交互类型的专门模型进行处理。</li>
<li>results: 根据实验结果，MMOE方法可以提高对困难交互的表现，比如让机器学习模型更好地预测讽刺语言。总的来说，这种方法可以提高 dataset 分析的新视角，并且实现了当前最佳性能。<details>
<summary>Abstract</summary>
Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
多模式机器学习，研究不同输入模式之间的信息和互动，在理解图像和描述文本之间的关系方面做出了重要进步。然而，这只是实际世界中多模式互动的一部分，不包括新型的对话和手势冲突的互动，如讲述嘲讽的例子。当前的共享信息捕捉方法经常不能够很好地扩展到这些更复杂的互动，有时performance只有50%级别的binary分类。在这篇论文中，我们解决这个问题通过一种新的方法，即MMOE（多模式互动专家混合）。我们的方法可以自动将数据点从无标签多模式数据集分类为互动类型，并采用特殊的模型来处理每种特定的互动。根据我们的实验，这种方法可以提高对这些复杂的互动的性能，增加总性能约2%，如讲述嘲讽预测等任务。因此，互动量化提供了新的数据分析途径，并且实现了简单的方法，达到了现状之前的最佳性能。</sys>Here is the translation of the text into Simplified Chinese:<sys>多模式机器学习，研究不同输入模式之间的信息和互动，在理解图像和描述文本之间的关系方面做出了重要进步。然而，这只是实际世界中多模式互动的一部分，不包括新型的对话和手势冲突的互动，如讲述嘲讽的例子。当前的共享信息捕捉方法经常不能够很好地扩展到这些更复杂的互动，有时performance只有50%级别的binary分类。在这篇论文中，我们解决这个问题通过一种新的方法，即MMOE（多模式互动专家混合）。我们的方法可以自动将数据点从无标签多模式数据集分类为互动类型，并采用特殊的模型来处理每种特定的互动。根据我们的实验，这种方法可以提高对这些复杂的互动的性能，增加总性能约2%，如讲述嘲讽预测等任务。因此，互动量化提供了新的数据分析途径，并且实现了简单的方法，达到了现状之前的最佳性能。</sys>
</details></li>
</ul>
<hr>
<h2 id="Crafting-In-context-Examples-according-to-LMs’-Parametric-Knowledge"><a href="#Crafting-In-context-Examples-according-to-LMs’-Parametric-Knowledge" class="headerlink" title="Crafting In-context Examples according to LMs’ Parametric Knowledge"></a>Crafting In-context Examples according to LMs’ Parametric Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09579">http://arxiv.org/abs/2311.09579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoonsang Lee, Pranav Atreya, Xi Ye, Eunsol Choi</li>
<li>for: 本研究探讨了如何构建受Context的示例集，以便在语言模型中触发行为，即 surface parametric knowledge。</li>
<li>methods: 研究使用了受Context示例集，并进行了分类和分析，以了解模型对于受Context示例的 parametric knowledge。</li>
<li>results: 实验结果表明，使用包含知识和未知信息的示例集可以最佳地在多种设置下进行表现。此外，研究还发现，使用模型的 parametric knowledge 来排序答案集可以提高表现。<details>
<summary>Abstract</summary>
In-context learning has been applied to knowledge-rich tasks such as question answering. In such scenarios, in-context examples are used to trigger a behaviour in the language model: namely, it should surface information stored in its parametric knowledge. We study the construction of in-context example sets, with a focus on the parametric knowledge of the model regarding in-context examples. We identify 'known' examples, where models can correctly answer from its parametric knowledge, and 'unknown' ones. Our experiments show that prompting with 'unknown' examples decreases the performance, potentially as it encourages hallucination rather than searching its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM's knowledge about each answer. Together, our study sheds lights on how to best construct in-context example sets for knowledge-rich tasks.
</details>
<details>
<summary>摘要</summary>
启用上下文学习应用于知识充沛的任务，如问答。在这些场景下，上下文示例被用来触发语言模型的行为：即它应该Surface其参数知识中的信息。我们研究上下文示例集的建构，强调语言模型对上下文示例的参数知识。我们分类了“知道”的示例和“不知道”的示例。我们的实验表明，向语言模型提供“不知道”的示例会降低其性能，可能是因为它鼓励了幻化而不是搜索其参数知识。构建包含知道和不知道信息的上下文示例集最佳，我们在多种场景中进行了分析。我们还研究了基于语言模型对每个答案的知识来排序答案集的策略。ogether，我们的研究为知识充沛任务中的上下文示例集建构提供了新的灯光。
</details></li>
</ul>
<hr>
<h2 id="A-Reevaluation-of-Event-Extraction-Past-Present-and-Future-Challenges"><a href="#A-Reevaluation-of-Event-Extraction-Past-Present-and-Future-Challenges" class="headerlink" title="A Reevaluation of Event Extraction: Past, Present, and Future Challenges"></a>A Reevaluation of Event Extraction: Past, Present, and Future Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09562">http://arxiv.org/abs/2311.09562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ej0cl6/textee">https://github.com/ej0cl6/textee</a></li>
<li>paper_authors: Kuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu Xie, Zixuan Zhang, Premkumar Natarajan, Kai-Wei Chang, Nanyun Peng, Heng Ji</li>
<li>For: The paper is written for the purpose of proposing a standardized, fair, and reproducible benchmark for event extraction, and to address the evaluation challenges in recent studies.* Methods: The paper uses standardized data preprocessing scripts and splits for more than ten datasets across different domains, and aggregates and re-implements over ten event extraction approaches published in recent years.* Results: The paper conducts a comprehensive reevaluation of event extraction approaches using the proposed benchmark, and explores the capability of large language models in event extraction. The results are expected to provide a reliable benchmark for future research in the field.<details>
<summary>Abstract</summary>
Event extraction has attracted much attention in recent years due to its potential for many applications. However, recent studies observe some evaluation challenges, suggesting that reported scores might not reflect the true performance. In this work, we first identify and discuss these evaluation challenges, including the unfair comparisons resulting from different assumptions about data or different data preprocessing steps, the incompleteness of the current evaluation framework leading to potential dataset bias or data split bias, and low reproducibility of prior studies. To address these challenges, we propose TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE contains standardized data preprocessing scripts and splits for more than ten datasets across different domains. In addition, we aggregate and re-implement over ten event extraction approaches published in recent years and conduct a comprehensive reevaluation. Finally, we explore the capability of large language models in event extraction and discuss some future challenges. We expect TextEE will serve as a reliable benchmark for event extraction, facilitating future research in the field.
</details>
<details>
<summary>摘要</summary>
Event extraction 在最近几年内受到了广泛关注，因为它在多个应用领域中具有潜在的潜力。然而，最近的研究发现了评估挑战，表明报告的分数可能不准确反映实际表现。在这项工作中，我们首先标识和讨论了评估挑战，包括数据假设不同或数据预处理步骤不同导致的不公正比较，当前评估框架不完整，导致可能的数据偏见或数据拆分偏见，以及过去研究的低可重现性。为解决这些挑战，我们提出了 TextEE，一个标准化、公平、可重现的事件抽取benchmark。 TextEE包含了标准化的数据预处理脚本和分割，以及多个领域的超过十个数据集。此外，我们对过去十年以来发表的十多个事件抽取方法进行了汇总和重新实现，并进行了全面的重评。最后，我们探讨了大语言模型在事件抽取中的能力，并讨论了未来的挑战。我们期望 TextEE 能成为事件抽取领域的可靠 benchmark，促进未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Pachinko-Patching-Interpretable-QA-Models-through-Natural-Language-Feedback"><a href="#Pachinko-Patching-Interpretable-QA-Models-through-Natural-Language-Feedback" class="headerlink" title="Pachinko: Patching Interpretable QA Models through Natural Language Feedback"></a>Pachinko: Patching Interpretable QA Models through Natural Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09558">http://arxiv.org/abs/2311.09558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaitanyamalaviya/pachinko">https://github.com/chaitanyamalaviya/pachinko</a></li>
<li>paper_authors: Chaitanya Malaviya, Subin Lee, Dan Roth, Mark Yatskar</li>
<li>for: 本研究旨在提高NL模型的评估，通过从用户反馈中收集改进模型。</li>
<li>methods: 研究使用了分解式问答模型，首先从 контек斯和问题中提取中间理由，然后使用这个理由来回答问题。</li>
<li>results: 研究发现，不同的理由格式对于用户提供反馈和理解模型回答的能力有显著影响。 certain formats significantly enhance user reported understanding and trust of model outputs.<details>
<summary>Abstract</summary>
Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales generated by QA models to support their answers. We specifically consider decomposed question-answering models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample these rationales from large language models using few-shot prompting for two reading comprehension datasets, and then perform two user studies. In the first one, we present users with incorrect answers and corresponding rationales of various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition to influencing critiquablity, certain formats significantly enhance user reported understanding and trust of model outputs.
</details>
<details>
<summary>摘要</summary>
找到用户对NL理解模型的反馈可以有助于改进模型。然而，如何在给用户显示模型回答以便他们可以更好地修改它？而且，用户关心什么样的特性来信任和理解模型的回答呢？我们通过分析QA模型生成的论证来回答这些问题。我们专门考虑了基于上下文和问题的分解Question answering模型，它们首先从上下文和问题中提取中间论证，然后只使用这个论证回答问题。论证描述模型回答问题的方法。我们使用大型语言模型通过几个提示来采样这些论证，然后对两个阅读理解dataset进行两项用户研究。在第一项研究中，我们给用户显示错误的回答和相应的论证不同格式，并询问他们提供自然语言反馈来修改论证。我们然后测量这些反馈是否可以通过上下文学习来修复论证。第二项研究检验了不同的论证格式对用户理解和信任模型输出的影响。我们发现，不同的论证格式对用户提供反馈的容易度和模型执行这些反馈的能力有很大影响。此外，某些格式可以明显提高用户报告的理解和信任度。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Few-Shot-Training-Example-Generators-A-Case-Study-in-Fallacy-Recognition"><a href="#Large-Language-Models-are-Few-Shot-Training-Example-Generators-A-Case-Study-in-Fallacy-Recognition" class="headerlink" title="Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition"></a>Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09552">http://arxiv.org/abs/2311.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Alhindi, Smaranda Muresan, Preslav Nakov</li>
<li>for: 提高现有的谬误认识模型，以便更好地处理多种频率不均的谬误类型。</li>
<li>methods:  incorporating additional context and leveraging大语言模型生成Synthetic数据，以增加较少seen classes的表现。</li>
<li>results: 在不同的谬误类型、数据集和生成器上进行了评估，得到了一致的提高。<details>
<summary>Abstract</summary>
Recognizing fallacies is crucial for ensuring the quality and validity of arguments across various domains. However, computational fallacy recognition faces challenges due to the diverse genres, domains, and types of fallacies found in datasets. This leads to a highly multiclass, and even multi-label, setup with substantial class imbalance. In this study, we aim to enhance existing models for fallacy recognition by incorporating additional context and by leveraging large language models to generate synthetic data, thus increasing the representation of the infrequent classes. We experiment with GPT3.5 to generate synthetic examples and we examine the impact of prompt settings for this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the effectiveness of using the generated examples for training smaller models within a unified fallacy recognition framework. Furthermore, we analyze the overlap between the synthetic data and existing fallacy datasets. Finally, we investigate the usefulness of providing supplementary context for detecting fallacy types that need such context, e.g., diversion fallacies. Our evaluation results demonstrate consistent improvements across fallacy types, datasets, and generators.
</details>
<details>
<summary>摘要</summary>
识别谬误是确保不同领域的论据质量和有效性的关键。然而，计算机谬误识别受到数据集中多种类型、领域和类别的多种谬误的挑战。这导致了一个高度多类、甚至多标签的设置，以及巨大的类别偏度问题。在这种情况下，我们想要提高现有的谬误识别模型，通过添加更多的 контекст和利用大语言模型生成 sintetic数据，以增加轻度类的表现。我们使用GPT3.5生成 sintetic例子，并考虑Prompt设置的影响。此外，我们还探索零shot和几shotenario来评估使用生成的例子来训练更小的模型在一个简化的谬误识别框架中。此外，我们还分析了生成的数据和现有的谬误数据集之间的重叠。最后，我们 investigate了在检测某些谬误类型时提供补充的 контекст的有用性，例如误导谬误。我们的评估结果表明，无论谬误类型、数据集或生成器，我们的方法都能够实现了一致的改进。
</details></li>
</ul>
<hr>
<h2 id="A-Speed-Odyssey-for-Deployable-Quantization-of-LLMs"><a href="#A-Speed-Odyssey-for-Deployable-Quantization-of-LLMs" class="headerlink" title="A Speed Odyssey for Deployable Quantization of LLMs"></a>A Speed Odyssey for Deployable Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09550">http://arxiv.org/abs/2311.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen Xie</li>
<li>for: 这个研究旨在提高大语言模型的推理速度和成本效益。</li>
<li>methods: 本研究使用硬件对应的数字化方法，排除不切实际的算法选择，同时将最大化硬件加速的效益。</li>
<li>results: 实验结果显示，我们的W4A8方法可以提高实际推理速度至多达4倍于Hugging Face FP16推理和2.23倍于TensorRT-LLM在FP16推理中，并在INT8推理中与TensorRT-LLM相比提高了1.45倍，而不会对性能造成严重干扰。<details>
<summary>Abstract</summary>
The large language model era urges faster and less costly inference. Prior model compression works on LLMs tend to undertake a software-centric approach primarily focused on the simulated quantization performance. By neglecting the feasibility of deployment, these approaches are typically disabled in real practice. They used to drastically push down the quantization bit range for a reduced computation which might not be supported by the mainstream hardware, or involve sophisticated algorithms that introduce extra computation or memory access overhead. We argue that pursuing a hardware-centric approach in the construction of quantization algorithms is crucial. In this regard, we are driven to build our compression method on top of hardware awareness, eliminating impractical algorithm choices while maximizing the benefit of hardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel implementation called FastGEMM and a combined recipe of quantization strategies. Extensive experiments manifest the superiority of our W4A8 method which brings the actual speed boosting up to \textbf{4$\times$} compared to Hugging Face FP16 inference and \textbf{2.23$\times$} vs. the state-of-the-art inference engine TensorRT-LLM in FP16, and \textbf{1.45$\times$} vs. TensorRT-LLM in INT8, yet without substantially harming the performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型时代强调更快速且成本更低的推导。先前的模型压缩方法倾向于以软件中心的方式进行，主要侧重在模拟量化性能。但这些方法通常在实际应用中被禁用，因为它们通常会降低量化比例，使得主流硬件无法支持或增加了复杂的算法或内存访问开销。我们认为在量化算法的建立中，应该将硬件考虑为核心。在这方面，我们将我们的压缩方法建立在硬件意识之上，排除不可行的算法选择，同时将硬件加速器的最大优化。我们的方法 OdysseyLLM 搭配了一个新的 W4A8 核心实现 FastGEMM，以及一种结合的量化策略。实验结果显示 OdysseyLLM 的实际速度提升为 \textbf{4$\times$} 比 Hugging Face FP16 推导，并且与现有的推导引擎 TensorRT-LLM 在 FP16 下的速度提升为 \textbf{2.23$\times$}，并且在 INT8 下的速度提升为 \textbf{1.45$\times$}，但不会对性能造成严重的损害。
</details></li>
</ul>
<hr>
<h2 id="Towards-Pragmatic-Awareness-in-Question-Answering-A-Case-Study-in-Maternal-and-Infant-Health"><a href="#Towards-Pragmatic-Awareness-in-Question-Answering-A-Case-Study-in-Maternal-and-Infant-Health" class="headerlink" title="Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health"></a>Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09542">http://arxiv.org/abs/2311.09542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Srikanth, Rupak Sarkar, Rachel Rudinger, Jordan Boyd-Graber</li>
<li>for: 这个论文主要是为了解决问答系统在高风险领域 like maternal and infant health 中能够更好地回答用户问题。</li>
<li>methods: 该论文使用大量语言模型来检测问题中含义的推理，以便在回答用户问题时能够更加准确地理解用户的需求。</li>
<li>results: 研究发现，通过检测问题中含义的推理，可以生成更加准确和有用的回答，从而避免了在回答用户问题时可能产生的危害。<details>
<summary>Abstract</summary>
Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study pragmatic inferences made when mothers ask questions about pregnancy and infant care. Some of the inferences in these questions evade detection by existing methods, risking the possibility of QA systems failing to address them which can have dangerous health and policy implications. We explore the viability of detecting inferences from questions using large language models and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that can mitigate the propagation of harmful beliefs.
</details>
<details>
<summary>摘要</summary>
常见于信息寻求用户的问题中的隐含假设或潜在危险假设，在高风险领域如母婴健康，一个问答系统必须认识这些实用限制，不仅回答用户的问题，更要在上下文中检查它们，以对用户提供有用的回答。为了实现这一目标，我们研究了怀孕和婴儿护理中妈妈提出的假设推理。一些这些问题中的假设逃避现有的方法检测，这可能会导致问答系统失败 Addressing them, which can have serious health and policy implications. We explore the feasibility of detecting inferences from questions using large language models and show that incorporating pragmatic inferences into existing QA pipelines can mitigate the propagation of harmful beliefs.
</details></li>
</ul>
<hr>
<h2 id="Reducing-Privacy-Risks-in-Online-Self-Disclosures-with-Language-Models"><a href="#Reducing-Privacy-Risks-in-Online-Self-Disclosures-with-Language-Models" class="headerlink" title="Reducing Privacy Risks in Online Self-Disclosures with Language Models"></a>Reducing Privacy Risks in Online Self-Disclosures with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09538">http://arxiv.org/abs/2311.09538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu</li>
<li>For: 保护在线自透泄的用户端隐私* Methods: 发展19种自透泄类划分，精度 fine-tune语言模型，并进行人工测试* Results: 实现Token F$_1$的过程优于75%，并通过用户反馈引入自透泄抽象任务，实现多种 fine-tuning 策略，生成具有较高实用性和Moderate隐私风险的抽象结果。<details>
<summary>Abstract</summary>
Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through identification and abstraction. We develop a taxonomy of 19 self-disclosure categories, and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for identification, achieving over 75% in Token F$_1$. We further conduct a HCI user study, with 82\% of participants viewing the model positively, highlighting its real world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction. We experiment with both one-span abstraction and three-span abstraction settings, and explore multiple fine-tuning strategies. Our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation.
</details>
<details>
<summary>摘要</summary>
自我披露在社交媒体交互中很常见和奖励，但也存在隐私风险。在这篇论文中，我们主动保护用户端隐私相关于在线自我披露的权益。我们开发了19种自我披露类别的taxonomy，并采集了4.8K注释化的披露跨度。我们然后精细调整语言模型，实现了Token F$_1$的过 75%。我们进一步进行了人机交互研究，82%的参与者视为模型有利可图，这反映了其在实际世界中的可行性。受用户反馈 inspirited，我们引入了自我披露抽象任务。我们在一span抽象和三span抽象的设置下进行了实验，并探索了多种调整策略。我们最佳模型可以生成多样化的抽象， moderately reducing privacy risks while maintaining high utility according to human evaluation.
</details></li>
</ul>
<hr>
<h2 id="Effective-Large-Language-Model-Adaptation-for-Improved-Grounding"><a href="#Effective-Large-Language-Model-Adaptation-for-Improved-Grounding" class="headerlink" title="Effective Large Language Model Adaptation for Improved Grounding"></a>Effective Large Language Model Adaptation for Improved Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09533">http://arxiv.org/abs/2311.09533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Ye, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister</li>
<li>for: 提高大型自然语言模型（LLMs）在实际应用中的广泛部署，因为它们可能会生成“幻想”的答案。</li>
<li>methods: 提出了一种新的框架AGREE，即Adaptation of LLMs for GRounding EnhancEment，以改进grounding的问题从一个整体的角度。</li>
<li>results: 比较prompting-based方法，通过调整LLMs来ground它们的答案，可以得到更好地参照的答案，并且可以减少对数据的需求。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in natural language understanding, generation, and manipulation of text-based data. However, one major issue towards their widespread deployment in the real world is that they can generate "hallucinated" answers that are not factual. Towards this end, this paper focuses on improving grounding from a holistic perspective with a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We start with the design of an iterative test-time adaptation (TTA) capability that takes into account the support information generated in self-grounded responses. To effectively enable this capability, we tune LLMs to ground the claims in their responses to retrieved documents by providing citations. This tuning on top of the pre-trained LLMs requires a small amount of data that needs to be constructed in a particular way to learn the grounding information, for which we introduce a data construction method. Our results show that the tuning-based AGREE framework generates better grounded responses with more accurate citations compared to prompting-based approaches.
</details>
<details>
<summary>摘要</summary>
The AGREE framework focuses on improving grounding from a holistic perspective by incorporating an iterative test-time adaptation (TTA) capability that considers the support information generated in self-grounded responses. To enable this capability, we fine-tune LLMs to ground their claims in their responses to retrieved documents by providing citations. This fine-tuning process requires a small amount of specially constructed data to learn the grounding information.Our results show that the tuning-based AGREE framework generates more accurate and better grounded responses compared to prompting-based approaches. This demonstrates the effectiveness of the AGREE framework in improving the factual accuracy of LLMs' responses.
</details></li>
</ul>
<hr>
<h2 id="AMRFact-Enhancing-Summarization-Factuality-Evaluation-with-AMR-driven-Training-Data-Generation"><a href="#AMRFact-Enhancing-Summarization-Factuality-Evaluation-with-AMR-driven-Training-Data-Generation" class="headerlink" title="AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation"></a>AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09521">http://arxiv.org/abs/2311.09521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyi Qiu, Kung-Hsiang Huang, Jingnong Qu, Nanyun Peng</li>
<li>for: 本研究的目的是提高抽象摘要中的事实准确性，尤其是在抽象摘要 task 中，保持信息的精度是非常重要的。</li>
<li>methods: 本研究使用了 Abstract Meaning Representation (AMR) 来生成不一致的摘要，并使用了自然语言判断和 BARTScore 来选择高质量的负例。</li>
<li>results: 实验结果表明，本研究的方法在 AggreFact-SOTA 数据集上显著超越了之前的系统，这说明了其在检测抽象摘要中的事实准确性的能力。<details>
<summary>Abstract</summary>
Ensuring factual consistency is crucial in various natural language processing tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior entailment-based approaches often generate factually inconsistent summaries and then train a classifier on the generated data. However, summaries produced by these approaches are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a novel framework that generates factually inconsistent summaries using Abstract Meaning Representation (AMR). Our approach parses factually correct summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate that our approach significantly outperforms previous systems on the AggreFact-SOTA dataset, showcasing its efficacy in assessing factuality in abstractive summarization.
</details>
<details>
<summary>摘要</summary>
保持事实一致性在各种自然语言处理任务中非常重要，特别是在抽象概念摘要中，因为保持信息完整性非常重要。先前基于前提推理的方法通常会生成不一致的摘要，然后对生成的数据进行训练。然而，这些方法生成的摘要通常是低凝结的或缺乏错误类型覆盖。为解决这些问题，我们提出了 AMRFact 框架，它使用抽象意义表示（AMR）来生成不一致的摘要。我们的方法将事实正确的摘要转换为 AMR 图并在其中注入控制的不一致性，以生成高错误类型覆盖的不一致摘要。此外，我们还提出了一个名为 NegFilter 的数据选择模块，它根据自然语言推理和 BARTScore 来确保生成的负样本的质量。实验结果表明，我们的方法与之前系统相比显著提高了 AggreFact-SOTA 数据集上的表现，这表明我们的方法在抽象概念摘要中评估事实性的有效性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Code-to-Improve-In-context-Learning-for-Semantic-Parsing"><a href="#Leveraging-Code-to-Improve-In-context-Learning-for-Semantic-Parsing" class="headerlink" title="Leveraging Code to Improve In-context Learning for Semantic Parsing"></a>Leveraging Code to Improve In-context Learning for Semantic Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09519">http://arxiv.org/abs/2311.09519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Bogin, Shivanshu Gupta, Peter Clark, Ashish Sabharwal</li>
<li>for: 提高semantic parsing的效果，尤其是在受限的数据量下</li>
<li>methods: 使用通用编程语言如Python，并将提问添加结构化域描述</li>
<li>results: 在三个 популяр的数据集上显著提高了准确率（例如，从7.9%提升到66.5%），降低了需要大量示例的要求，并减少了语言的 популяр度对于预训练 corpora 的影响。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstrations. We find that the resemblance of the target parse language to general-purpose code is a more important factor than the language's popularity in pre-training corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.
</details>
<details>
<summary>摘要</summary>
启发式学习（ICL）是 semantic parsing 方法的一种吸引人的方式，因为它可以通过几次示例学习来达到更好的泛化性。然而，学习到特定领域语言（DSL）的语义分析仍然是挑战，尤其是使用只有几个示例的情况下。在这种情况下，我们改进了 ICLL 的效iveness，通过以下两点：1. 使用通用编程语言，如 Python，而不是特定领域语言。2. 在提示中添加结构化领域描述，包括可用的类和函数。我们发现，这两点都会显著提高准确性，并在三个流行的数据集上达到了显著提高（例如，从 7.9% 提高到 66.5% 在 SMCalFlow compositional split 上）。这些改进使得模型在更难的 compositional split 上表现更好，并减少了需要大量示例的需求。我们发现，目标语义分析语言与通用编程语言之间的相似性是更重要的因素，而不是语言的 популярность。我们的发现可以提供一种改进的方法来在现代 ICLL 中建立 semantic parser。
</details></li>
</ul>
<hr>
<h2 id="GEE-Grammar-Error-Explanation-with-Large-Language-Models"><a href="#GEE-Grammar-Error-Explanation-with-Large-Language-Models" class="headerlink" title="GEE! Grammar Error Explanation with Large Language Models"></a>GEE! Grammar Error Explanation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09517">http://arxiv.org/abs/2311.09517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, Kevin Gimpel, Mohit Iyyer</li>
<li>for: 这个论文是为了解决语言学习者的 grammatical error correction 问题而写的。</li>
<li>methods: 这个论文使用的方法包括使用 GPT-4 生成一个一个 sentence explanation 的 pipeline，以及使用 fine-tuned 和提示的大型语言模型进行 structured atomic token edit extraction。</li>
<li>results: 人工评估表明，这个pipeline在德语和中文 grammar error correction 数据上的正确率分别为 93.9% 和 98.0%。<details>
<summary>Abstract</summary>
Grammatical error correction tools are effective at correcting grammatical errors in users' input sentences but do not provide users with \textit{natural language} explanations about their errors. Such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. We analyze the capability of GPT-4 in grammar error explanation, and find that it only produces explanations for 60.2% of the errors using one-shot prompting. To improve upon this performance, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to generate explanations. We evaluate our pipeline on German and Chinese grammar error correction data sampled from language learners with a wide range of proficiency levels. Human evaluation reveals that our pipeline produces 93.9% and 98.0% correct explanations for German and Chinese data, respectively. To encourage further research in this area, we will open-source our data and code.
</details>
<details>
<summary>摘要</summary>
grammatical error correction tools can correct grammatical errors in users' input sentences, but they do not provide users with 自然语言 explanations about their errors. these explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). to address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. we analyze the capability of GPT-4 in grammar error explanation, and find that it only produces explanations for 60.2% of the errors using one-shot prompting. to improve upon this performance, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to generate explanations. we evaluate our pipeline on German and Chinese grammar error correction data sampled from language learners with a wide range of proficiency levels. human evaluation reveals that our pipeline produces 93.9% and 98.0% correct explanations for German and Chinese data, respectively. to encourage further research in this area, we will open-source our data and code.
</details></li>
</ul>
<hr>
<h2 id="Sequencing-Matters-A-Generate-Retrieve-Generate-Model-for-Building-Conversational-Agents"><a href="#Sequencing-Matters-A-Generate-Retrieve-Generate-Model-for-Building-Conversational-Agents" class="headerlink" title="Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents"></a>Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09513">http://arxiv.org/abs/2311.09513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quinn Patwardhan, Grace Hui Yang</li>
<li>for: This paper describes the Georgetown InfoSense group’s approach to solving the challenges of TREC iKAT 2023.</li>
<li>methods: The approach uses a Generate-Retrieve-Generate method, which is found to outperform Retrieve-Then-Generate approaches. The solution involves using Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again.</li>
<li>results: The submitted runs outperform the median runs by a significant margin, with superior performance in nDCG across various cut numbers and overall success rate. The official results of the TREC evaluation contradict the initial self-evaluation, but the findings suggest that the sequence of involving different components matters, with LLMs being essential before using search engines.<details>
<summary>Abstract</summary>
This paper contains what the Georgetown InfoSense group has done in regard to solving the challenges presented by TREC iKAT 2023. Our submitted runs outperform the median runs by a significant margin, exhibiting superior performance in nDCG across various cut numbers and in overall success rate. Our approach uses a Generate-Retrieve-Generate method, which we've found to greatly outpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our solution involves the use of Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again. We leverage several purpose-built Language Models, including BERT, Chat-based, and text-to-transfer-based models, for text understanding, classification, generation, and summarization. The official results of the TREC evaluation contradict our initial self-evaluation, which may suggest that a decrease in the reliance on our retrieval and classification methods is better. Nonetheless, our findings suggest that the sequence of involving these different components matters, where we see an essentiality of using LLMs before using search engines.
</details>
<details>
<summary>摘要</summary>
Our solution employs Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again. We utilize several purpose-built Language Models, including BERT, Chat-based, and text-to-transfer-based models, for text understanding, classification, generation, and summarization.While the official results of the TREC evaluation differ from our initial self-evaluation, our findings suggest that the sequence of involving these different components is crucial. Specifically, we find that using LLMs before search engines is essential.
</details></li>
</ul>
<hr>
<h2 id="One-Size-Does-Not-Fit-All-Customizing-Open-Domain-Procedures"><a href="#One-Size-Does-Not-Fit-All-Customizing-Open-Domain-Procedures" class="headerlink" title="One Size Does Not Fit All: Customizing Open-Domain Procedures"></a>One Size Does Not Fit All: Customizing Open-Domain Procedures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09510">http://arxiv.org/abs/2311.09510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Kumar Lal, Li Zhang, Faeze Brahman, Bodhisattwa Prasad Majumder, Peter Clark, Niket Tandon</li>
<li>for: 这研究是关于如何使用自然语言处理机器人（LLM）来自动化开放领域过程定制。</li>
<li>methods: 研究使用了一个名为CustomPlans的探测数据集，该数据集包含多种用户定制需求，以测试LLM的定制能力。</li>
<li>results: 研究发现，在Sequential设置下使用LLM作为定制代理和执行代理时，可以很好地满足用户的定制需求，但是LLM并不充分考虑用户的定制需求，导致错误率为~51%。<details>
<summary>Abstract</summary>
How-to procedures, such as how to plant a garden, are ubiquitous. But one size does not fit all - humans often need to customize these procedural plans according to their specific needs, e.g., planting a garden without pesticides. While LLMs can fluently generate generic procedures, we present the first study on how well LLMs can customize open-domain procedures. We introduce CustomPlans, a probe dataset of customization hints that encodes diverse user needs for open-domain How-to procedures. Using LLMs as CustomizationAgent and ExecutionAgent in different settings, we establish their abilities to perform open-domain procedure customization. Human evaluation shows that using these agents in a Sequential setting is the best, but they are good enough only ~51% of the time. Error analysis shows that LLMs do not sufficiently address user customization needs in their generated procedures.
</details>
<details>
<summary>摘要</summary>
各种如何程序（如植 garden）是普遍存在的。但是一个size不适用于所有人——人们常需要根据自己的具体需求自定义这些程序，例如不使用杀虫剂植 garden。 LLMS可以轻松生成通用的程序，但我们的研究表明，LLMS可以如何自定义开放领域的程序。我们介绍了一个名为CustomPlans的探索数据集，该数据集包含多种用户需求的自定义提示。我们使用LLMS作为自定义代理和执行代理在不同的设置下，并证明了它们在开放领域程序自定义方面的能力。人工评估表明，使用这些代理在顺序设置下是最好的，但它们只能成功约51%的时间。错误分析表明，LLMS在生成的程序中不充分考虑用户自定义需求。
</details></li>
</ul>
<hr>
<h2 id="SQATIN-Supervised-Instruction-Tuning-Meets-Question-Answering-for-Improved-Dialogue-NLU"><a href="#SQATIN-Supervised-Instruction-Tuning-Meets-Question-Answering-for-Improved-Dialogue-NLU" class="headerlink" title="SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU"></a>SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09502">http://arxiv.org/abs/2311.09502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Razumovskaia, Goran Glavaš, Anna Korhonen, Ivan Vulić</li>
<li>for: 本研究旨在提高对话自然语言理解（NLU）的性能，尤其是在 Labelled NLU 数据稀缺的情况下。</li>
<li>methods: 本研究提出了一种新的对话 NLU 框架，名为 SQATIN，它基于 instruction tuning 和问答模型来解决 Intent Detection 和 Value Extraction 任务。</li>
<li>results: 根据评估结果，SQATIN 在已有NLU benchmark上设置了新的状态对话NLU性能，大幅超越了现有的模型基于标准精度优化目标的表现，尤其是在跨领域传递中。<details>
<summary>Abstract</summary>
Task-oriented dialogue (ToD) systems help users execute well-defined tasks across a variety of domains (e.g., $\textit{flight booking}$ or $\textit{food ordering}$), with their Natural Language Understanding (NLU) components being dedicated to the analysis of user utterances, predicting users' intents ($\textit{Intent Detection}$, ID) and extracting values for informational slots ($\textit{Value Extraction}$, VE). In most domains, labelled NLU data is scarce, making sample-efficient learning -- enabled with effective transfer paradigms -- paramount. In this work, we introduce SQATIN, a new framework for dialog NLU based on (i) instruction tuning and (ii) question-answering-based formulation of ID and VE tasks. According to the evaluation on established NLU benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially surpassing the performance of current models based on standard fine-tuning objectives in both in-domain training and cross-domain transfer. SQATIN yields particularly large performance gains in cross-domain transfer, owing to the fact that our QA-based instruction tuning leverages similarities between natural language descriptions of classes (i.e., slots and intents) across domains.
</details>
<details>
<summary>摘要</summary>
任免对话（ToD）系统可以帮助用户完成具体的任务（如飞行订票或食物订单），其自然语言理解（NLU）组件专门用于分析用户言语，预测用户的意图（Intent Detection，ID）和提取信息槽的值（Value Extraction，VE）。在大多数领域中，标注的NLU数据 scarce，因此使得样本效率学习 -- 通过有效的传输方法 -- 是非常重要的。在这项工作中，我们介绍了SQATIN，一种新的对话NLU框架，基于（i）指令调整和（ii）问答题解法来实现ID和VE任务。根据评估已知NLU标准准的评估 benchmark，SQATIN将对话NLU的新状态划定，大幅超过了现有基于标准精度调整目标的当前模型在领域培训和跨领域传输中的性能。SQATIN在跨领域传输中的性能提升特别大，这是因为我们的QA-based instruction tuning利用了不同领域的自然语言描述中的类 similarities（即槽和意图）。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Jargon-Identification-for-Enhanced-Interdisciplinary-Communication"><a href="#Personalized-Jargon-Identification-for-Enhanced-Interdisciplinary-Communication" class="headerlink" title="Personalized Jargon Identification for Enhanced Interdisciplinary Communication"></a>Personalized Jargon Identification for Enhanced Interdisciplinary Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09481">http://arxiv.org/abs/2311.09481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Guo, Joseph Chee Chang, Maria Antoniak, Erin Bransom, Trevor Cohen, Lucy Lu Wang, Tal August</li>
<li>for: 本研究旨在提高科研人员对技术术语的认知和理解，以便在不同领域之间进行交互和合作。</li>
<li>methods: 本研究使用了一组超过10000个术语熟悉度标注数据，并分析了这些数据以识别具有不同熟悉度的术语。</li>
<li>results: 研究发现，科研人员对术语熟悉度和信息需求之间存在很大差异，即使在同一个子领域内。研究还找到了个人、子领域和领域知识等特征，以便预测个人对术语熟悉度的认知。<details>
<summary>Abstract</summary>
Scientific jargon can impede researchers when they read materials from other domains. Current methods of jargon identification mainly use corpus-level familiarity indicators (e.g., Simple Wikipedia represents plain language). However, researchers' familiarity of a term can vary greatly based on their own background. We collect a dataset of over 10K term familiarity annotations from 11 computer science researchers for terms drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity and information needs vary widely across annotators, even within the same sub-domain (e.g., NLP). We investigate features representing individual, sub-domain, and domain knowledge to predict individual jargon familiarity. We compare supervised and prompt-based approaches, finding that prompt-based methods including personal publications yields the highest accuracy, though zero-shot prompting provides a strong baseline. This research offers insight into features and methods to integrate personal data into scientific jargon identification.
</details>
<details>
<summary>摘要</summary>
科学技术术语可能会阻碍研究人员在不同领域的文献中阅读。现有的词汇识别方法主要使用文库级 familiarness 指标（例如简单的wikipedia）。然而，研究人员对于一个词汇的熟悉程度可能会很大差异，基于他们的背景知识。我们收集了超过10,000个词汇熟悉标注from 11名计算机科学研究人员，来自100篇摘要中的词汇。我们对这些数据进行分析发现，词汇熟悉度和信息需求在审题人员中很大差异，甚至在同一个子领域（例如NLP）内。我们研究个人、子领域和领域知识的特征，以预测个人词汇熟悉度。我们比较了经过学习和提示方法，发现提示方法，包括个人出版物，可以达到最高的准确率，虽然零开始提示方法提供了强大的基准。这项研究对个人数据集成 scientific jargon 识别提供了新的想法和方法。
</details></li>
</ul>
<hr>
<h2 id="Show-Your-Work-with-Confidence-Confidence-Bands-for-Tuning-Curves"><a href="#Show-Your-Work-with-Confidence-Confidence-Bands-for-Tuning-Curves" class="headerlink" title="Show Your Work with Confidence: Confidence Bands for Tuning Curves"></a>Show Your Work with Confidence: Confidence Bands for Tuning Curves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09480">http://arxiv.org/abs/2311.09480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nalourie/opda">https://github.com/nalourie/opda</a></li>
<li>paper_authors: Nicholas Lourie, Kyunghyun Cho, He He</li>
<li>for: 本文旨在提供一种用于比较自然语言处理方法的有效方法，以及一种用于确定这些方法之间的关系的有效方法。</li>
<li>methods: 本文使用了一种新的方法来建立有效的比较 curves，这种方法可以快速地确定不同方法之间的关系。</li>
<li>results: 本文的实验结果表明，新提出的方法可以准确地建立比较 curves，并且可以与现有的bootstrapconfidence bands进行比较。<details>
<summary>Abstract</summary>
The choice of hyperparameters greatly impacts performance in natural language processing. Often, it is hard to tell if a method is better than another or just better tuned. Tuning curves fix this ambiguity by accounting for tuning effort. Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. While several estimators exist for these curves, it is common to use point estimates, which we show fail silently and give contradictory results when given too little data.   Beyond point estimates, confidence bands are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.   Empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release a library implementing the method at https://github.com/nalourie/opda .
</details>
<details>
<summary>摘要</summary>
“选择超参数会对自然语言处理性能产生深远影响。然而，常常难以判断一方法是哪一方法更好，这是因为它们的优化努力不同。对于这问题，曲线数据可以提供解答。 Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. 许多估计器存在这些曲线上，但是常用的是点估计，我们展示它们会在有限数据情况下失败并给出矛盾的结果。  beyond point estimates, confidence bands are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.  empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release a library implementing the method at <https://github.com/nalourie/opda>.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Clarify-When-Necessary-Resolving-Ambiguity-Through-Interaction-with-LMs"><a href="#Clarify-When-Necessary-Resolving-Ambiguity-Through-Interaction-with-LMs" class="headerlink" title="Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs"></a>Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09469">http://arxiv.org/abs/2311.09469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael J. Q. Zhang, Eunsol Choi</li>
<li>for: 这paper的目的是研究LMs中的ambiguity解决方法，以提高AI助手的性能。</li>
<li>methods: 该paper提出了一个任务agnostic的框架，通过向用户提问clarifying questions来解决ambiguity。这个框架包括三个子任务：确定需要clarification时，确定需要clarification的问题，以及基于新的信息回答正确。</li>
<li>results: 该paper的实验结果表明，intent-sim可以更好地确定需要clarification的时候，并且可以double randomly select的性能。此外，intent-sim在多种NLP任务和LMs中都表现了良好的稳定性。<details>
<summary>Abstract</summary>
Resolving ambiguities through interaction is a hallmark of natural language, and modeling this behavior is a core challenge in crafting AI assistants. In this work, we study such behavior in LMs by proposing a task-agnostic framework for resolving ambiguity by asking users clarifying questions. Our framework breaks down this objective into three subtasks: (1) determining when clarification is needed, (2) determining what clarifying question to ask, and (3) responding accurately with the new information gathered through clarification. We evaluate systems across three NLP applications: question answering, machine translation and natural language inference. For the first subtask, we present a novel uncertainty estimation approach, intent-sim, that determines the utility of querying for clarification by estimating the entropy over user intents. Our method consistently outperforms existing uncertainty estimation approaches at identifying predictions that will benefit from clarification. When only allowed to ask for clarification on 10% of examples, our system is able to double the performance gains over randomly selecting examples to clarify. Furthermore, we find that intent-sim is robust, demonstrating improvements across a wide range of NLP tasks and LMs. Together, our work lays foundation for studying clarifying interactions with LMs.
</details>
<details>
<summary>摘要</summary>
解决冲突通过互动是自然语言的特征，模拟这种行为是AI助手设计的核心挑战。在这项工作中，我们研究LM中的这种行为，通过提出任务无关的框架来解决冲突。我们将这个目标分解为三个互动任务：（1）确定是否需要准确化，（2）确定需要准确化的问题，以及（3）通过准确化获取新信息并准确回答。我们在三种NLP应用中评估系统：问答、机器翻译和自然语言推理。对于第一个任务，我们提出了一种新的uncertainty estimation方法，即意图sim，该方法根据用户意图的Entropy来判断是否需要准确化。我们的方法在identifying需要准确化的预测中表现出色，并且在只允许问 clarification 10%的示例中，我们的系统能够double Performance gain。此外，我们发现intent-sim是可靠的，在各种NLP任务和LM上都能够达到显著改进。总之，我们的工作为 изуча clarify interactions with LMs 提供了基础。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.CL_2023_11_16/" data-id="clp89doby00f7i7884v368ngu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.LG_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T10:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.LG_2023_11_16/">cs.LG - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Asymptotically-Fair-Participation-in-Machine-Learning-Models-an-Optimal-Control-Perspective"><a href="#Asymptotically-Fair-Participation-in-Machine-Learning-Models-an-Optimal-Control-Perspective" class="headerlink" title="Asymptotically Fair Participation in Machine Learning Models: an Optimal Control Perspective"></a>Asymptotically Fair Participation in Machine Learning Models: an Optimal Control Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10223">http://arxiv.org/abs/2311.10223</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhuotong Chen, Qianxiao Li, Zheng Zhang</li>
<li>for:  Addressing the problem of achieving asymptotically fair participation in machine learning models, particularly when the data distribution shifts due to deployment.</li>
<li>methods:  Optimal control formulation and surrogate retention system based on evolutionary population dynamics to approximate the dynamics of distribution shifts on active user counts.</li>
<li>results:  Superior performance compared to existing baseline methods in a generic simulation environment, demonstrating the effectiveness of the proposed method for long-term planning and maintaining model performance across all demographic groups.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在解决机器学习模型中的 asymptotically fair participation 问题，特别是在数据分布shift时。</li>
<li>methods: 使用优化控制方法和基于进化人口动力学的代表系统来近似活动用户数下的分布转移 dynamics。</li>
<li>results: 在一个通用的 simulate 环境中，比基eline方法表现出色，证明提议的方法可以对长期规划和维护所有民主组中的表现。<details>
<summary>Abstract</summary>
The performance of state-of-the-art machine learning models often deteriorates when testing on demographics that are under-represented in the training dataset. This problem has predominately been studied in a supervised learning setting where the data distribution is static. However, real-world applications often involve distribution shifts caused by the deployed models. For instance, the performance disparity against monitory users can lead to a high customer churn rate, thus the available data provided by active users are skewed due to the lack of minority users. This feedback effect further exacerbates the disparity among different demographic groups in future steps. To address this issue, we propose asymptotically fair participation as a condition to maintain long-term model performance over all demographic groups. In this work, we aim to address the problem of achieving asymptotically fair participation via optimal control formulation. Moreover, we design a surrogate retention system based on existing literature on evolutionary population dynamics to approximate the dynamics of distribution shifts on active user counts, from which the objective of achieving asymptotically fair participation is formulated as an optimal control problem, and the control variables are considered as the model parameters. We apply an efficient implementation of Pontryagin's maximum principle to estimate the optimal control solution. To evaluate the effectiveness of the proposed method, we design a generic simulation environment that simulates the population dynamics of the feedback effect between user retention and model performance. When we deploy the resulting models to the simulation environment, the optimal control solution accounts for long-term planning and leads to superior performance compared with existing baseline methods.
</details>
<details>
<summary>摘要</summary>
现代机器学习模型在测试不充分表示的民生数据时 часто会下降性能。这个问题主要在静止数据分布下进行研究，但实际应用中经常出现数据分布shift的问题。例如，由于模型的使用者留存率不均匀，导致可用数据受到少数民生的抑制，从而使得模型的性能差异化。这种反馈效应进一步夹紧不同民生群体之间的性能差异，使得长期维护模型的性能成为一个重要的问题。为解决这个问题，我们提出了 asymptotically fair participation 作为一种维护长期模型性能的条件。在这种情况下，我们通过可EVOLUTIONARY POPULATION DYNAMICS 来模拟活跃用户数量的分布转移，从而将目标 achieving asymptotically fair participation 转化为一个优化控制问题。我们使用 Pontryagin's maximum principle 的有效实现来估计优化控制解。为评估提案的效果，我们设计了一个通用的 simulate 环境，该环境可以模拟反馈效应的影响，使得我们可以评估提案的效果。当我们将结果应用到 simulate 环境中时，优化控制解会考虑长期规划，并且比拥有基准方法更高的性能。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Optimization-Algorithms-for-Machine-Learning"><a href="#Adaptive-Optimization-Algorithms-for-Machine-Learning" class="headerlink" title="Adaptive Optimization Algorithms for Machine Learning"></a>Adaptive Optimization Algorithms for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10203">http://arxiv.org/abs/2311.10203</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/molyswu/hand_detection">https://github.com/molyswu/hand_detection</a></li>
<li>paper_authors: Slavomír Hanzely</li>
<li>for: 这个论文的目的是调查机器学习优化器中的适应性。</li>
<li>methods: 本论文使用了多种方法，包括个性化损失、meta-学习、卷积矩阵规则、步长 Newton 方法和低维度更新。</li>
<li>results: 本论文的研究结果包括提出了新的适应性方法，改进了现有算法的收敛保证，以及对实际应用中流行的算法进行了深入分析。<details>
<summary>Abstract</summary>
Machine learning assumes a pivotal role in our data-driven world. The increasing scale of models and datasets necessitates quick and reliable algorithms for model training. This dissertation investigates adaptivity in machine learning optimizers. The ensuing chapters are dedicated to various facets of adaptivity, including: 1. personalization and user-specific models via personalized loss, 2. provable post-training model adaptations via meta-learning, 3. learning unknown hyperparameters in real time via hyperparameter variance reduction, 4. fast O(1/k^2) global convergence of second-order methods via stepsized Newton method regardless of the initialization and choice basis, 5. fast and scalable second-order methods via low-dimensional updates. This thesis contributes novel insights, introduces new algorithms with improved convergence guarantees, and improves analyses of popular practical algorithms.
</details>
<details>
<summary>摘要</summary>
机器学习在数据驱动世界中扮演着关键性的角色。随着模型和数据集的规模的增长，需要快速可靠的模型训练算法。本论文调查了机器学习优化器中的适应性。以下章节探讨了不同方面的适应性，包括：1. 个性化损失函数 для个性化模型；2. 通过meta学习提供可证明的后期模型适应性；3. 在实时中学习 unknown 的 гипер参数 via  гипер参数减少方法；4. O(1/k^2) 全球快速收敛的二次方法，无论初始化和选择基准都是；5. 快速可扩展的二次方法 via 低维度更新。本论文提供了新的发现和改进了现有算法的新算法，同时也提高了现有实践中的分析。
</details></li>
</ul>
<hr>
<h2 id="Improving-Unimodal-Inference-with-Multimodal-Transformers"><a href="#Improving-Unimodal-Inference-with-Multimodal-Transformers" class="headerlink" title="Improving Unimodal Inference with Multimodal Transformers"></a>Improving Unimodal Inference with Multimodal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10170">http://arxiv.org/abs/2311.10170</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kateryna Chumachenko, Alexandros Iosifidis, Moncef Gabbouj</li>
<li>for: 提高单模型性能，通过多模式训练</li>
<li>methods: 提出一种多支路架构，结合单模型与多模式变换器，通过多任务目标进行知识传递</li>
<li>results: 在RGB和深度动手势识别、语音和脸部视频情感识别以及语音-视频-文本情感分析等任务上表现出色，超过传统单模型counterpart<details>
<summary>Abstract</summary>
This paper proposes an approach for improving performance of unimodal models with multimodal training. Our approach involves a multi-branch architecture that incorporates unimodal models with a multimodal transformer-based branch. By co-training these branches, the stronger multimodal branch can transfer its knowledge to the weaker unimodal branches through a multi-task objective, thereby improving the performance of the resulting unimodal models. We evaluate our approach on tasks of dynamic hand gesture recognition based on RGB and Depth, audiovisual emotion recognition based on speech and facial video, and audio-video-text based sentiment analysis. Our approach outperforms the conventionally trained unimodal counterparts. Interestingly, we also observe that optimization of the unimodal branches improves the multimodal branch, compared to a similar multimodal model trained from scratch.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Algebraic-Topological-Networks-via-the-Persistent-Local-Homology-Sheaf"><a href="#Algebraic-Topological-Networks-via-the-Persistent-Local-Homology-Sheaf" class="headerlink" title="Algebraic Topological Networks via the Persistent Local Homology Sheaf"></a>Algebraic Topological Networks via the Persistent Local Homology Sheaf</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10156">http://arxiv.org/abs/2311.10156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gabriele Cesa, Arash Behboodi</li>
<li>for: 这 paper 的目的是提出一种基于代数Topology的图 convolution 和注意力模块的新方法，以便更好地利用数据的本地 topological 特性。</li>
<li>methods: 这 paper 使用了 sheaf neural networks 框架，通过将数据转化为 simplicial complex 后，构造其 local homology sheaf，并使用这个 sheaf 的 Laplacian 来建立更复杂的线性消息。</li>
<li>results: 这 paper 的结果表明，通过使用本 paper 的方法，可以construct more expressive, non-isotropic messages，并且可以 directly optimize the topology of intermediate features。<details>
<summary>Abstract</summary>
In this work, we introduce a novel approach based on algebraic topology to enhance graph convolution and attention modules by incorporating local topological properties of the data. To do so, we consider the framework of sheaf neural networks, which has been previously leveraged to incorporate additional structure into graph neural networks' features and construct more expressive, non-isotropic messages. Specifically, given an input simplicial complex (e.g. generated by the cliques of a graph or the neighbors in a point cloud), we construct its local homology sheaf, which assigns to each node the vector space of its local homology. The intermediate features of our networks live in these vector spaces and we leverage the associated sheaf Laplacian to construct more complex linear messages between them. Moreover, we extend this approach by considering the persistent version of local homology associated with a weighted simplicial complex (e.g., built from pairwise distances of nodes embeddings). This i) solves the problem of the lack of a natural choice of basis for the local homology vector spaces and ii) makes the sheaf itself differentiable, which enables our models to directly optimize the topology of their intermediate features.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍了一种基于代数Topology的新方法，用于增强图 convolution和注意模块。我们通过使用 sheaf neural networks 框架，将数据的本地 topological 特性 incorporated 到图 neural networks 的特征中。具体来说，我们给输入的 simplicial complex (例如，由图中的 clique 或点云中的邻居生成) 构建了本地 homology sheaf，将每个节点的 vector space 分配给其本地 homology。我们的网络中间特征生活在这些 vector space 中，并利用相关的 sheaf Laplacian 构建更复杂的线性消息 между它们。此外，我们还扩展了这种方法，考虑weighted simplicial complex 中的 persistent local homology，解决了选择 local homology vector space 的自然基的问题，并使 sheaf 本身可导，使我们的模型直接优化其中间特征的 topology。
</details></li>
</ul>
<hr>
<h2 id="Near-optimal-Closed-loop-Method-via-Lyapunov-Damping-for-Convex-Optimization"><a href="#Near-optimal-Closed-loop-Method-via-Lyapunov-Damping-for-Convex-Optimization" class="headerlink" title="Near-optimal Closed-loop Method via Lyapunov Damping for Convex Optimization"></a>Near-optimal Closed-loop Method via Lyapunov Damping for Convex Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10053">http://arxiv.org/abs/2311.10053</a></li>
<li>repo_url: None</li>
<li>paper_authors: Severin Maier, Camille Castera, Peter Ochs</li>
<li>for: 这个论文是为了解决首阶Optimization问题而设计的一个自动控制系统。</li>
<li>methods: 这个系统使用了闭环抑止来实现 convergence rate的最佳化。</li>
<li>results: 研究发现，这个系统可以达到 arbitrarily close to 最佳的 convergence rate，而且可以实现闭环抑止。<details>
<summary>Abstract</summary>
We introduce an autonomous system with closed-loop damping for first-order convex optimization. While, to this day, optimal rates of convergence are only achieved by non-autonomous methods via open-loop damping (e.g., Nesterov's algorithm), we show that our system is the first one featuring a closed-loop damping while exhibiting a rate arbitrarily close to the optimal one. We do so by coupling the damping and the speed of convergence of the system via a well-chosen Lyapunov function. We then derive a practical first-order algorithm called LYDIA by discretizing our system, and present numerical experiments supporting our theoretical findings.
</details>
<details>
<summary>摘要</summary>
我们介绍一个自动化系统，其中关闭调对于首项凸优化问题的关闭调。至今为止，仅有非自动化方法（如尼斯特洛夫的算法）可以 дости得最佳速度传递，但我们证明我们的系统是第一个具有关闭调的系统，其速度与最佳速度传递之间存在一定的关联。我们通过一个适当的 Lyapunov 函数来实现这一点。我们随后从数值方面提出了一个实用的首项算法，即 LYDIA，并提供了数值实验证明我们的理论成果。
</details></li>
</ul>
<hr>
<h2 id="Tabular-Few-Shot-Generalization-Across-Heterogeneous-Feature-Spaces"><a href="#Tabular-Few-Shot-Generalization-Across-Heterogeneous-Feature-Spaces" class="headerlink" title="Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces"></a>Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10051">http://arxiv.org/abs/2311.10051</a></li>
<li>repo_url: None</li>
<li>paper_authors: Max Zhu, Katarzyna Kobalczyk, Andrija Petrovic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Petro Lio</li>
<li>for: 这篇论文的目的是提出一种适用于表格数据集的几何学习方法，以应对表格数据集中的弹性和不同的列关系。</li>
<li>methods: 本文提出的方法是FLAT，它使用了Dataset2Vec的核心传统来学习表格数据集的低维度表示，并使用Graph Attention Network来适应表格数据集的不同列关系。</li>
<li>results: 本文的实验结果显示FLAT方法能够成功地在118个UCI数据集上进行几何学习，并与基准值相比有着明显的改善。<details>
<summary>Abstract</summary>
Despite the prevalence of tabular datasets, few-shot learning remains under-explored within this domain. Existing few-shot methods are not directly applicable to tabular datasets due to varying column relationships, meanings, and permutational invariance. To address these challenges, we propose FLAT-a novel approach to tabular few-shot learning, encompassing knowledge sharing between datasets with heterogeneous feature spaces. Utilizing an encoder inspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and their individual columns, which facilitate knowledge transfer and generalization to previously unseen datasets. A decoder network parametrizes the predictive target network, implemented as a Graph Attention Network, to accommodate the heterogeneous nature of tabular datasets. Experiments on a diverse collection of 118 UCI datasets demonstrate FLAT's successful generalization to new tabular datasets and a considerable improvement over the baselines.
</details>
<details>
<summary>摘要</summary>
尽管表格数据集很普遍，ew-shot学习在这个领域仍然受到不 enough 探索。现有的ew-shot方法不直接适用于表格数据集，因为列之间的关系、意义和 permutation 不变。为解决这些挑战，我们提出FLAT，一种 novel 的表格ew-shot学习方法，利用dataset2Vec inspirited Encoder 学习表格和其各列的低维度嵌入，从而促进了知识传递和泛化至之前未见的表格数据集。Decoder 网络实现了 Graph Attention Network，以适应表格数据集的多样性。我们在118个 UCI 数据集上进行了实验，并证明FLAT可以成功泛化到新的表格数据集，并且明显超过基elines。
</details></li>
</ul>
<hr>
<h2 id="Guaranteeing-Control-Requirements-via-Reward-Shaping-in-Reinforcement-Learning"><a href="#Guaranteeing-Control-Requirements-via-Reward-Shaping-in-Reinforcement-Learning" class="headerlink" title="Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning"></a>Guaranteeing Control Requirements via Reward Shaping in Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10026">http://arxiv.org/abs/2311.10026</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi, Mario di Bernardo</li>
<li>for: 本研究旨在提供一种 Ensure Optimal Control Policy Meets Specified Requirements 的方法，以解决在 reinforcement learning 中控制问题中的稳定性和性能要求。</li>
<li>methods: 本研究使用了一种系统的 reward shaping 方法，通过确保优化策略满足指定的控制要求来保证优化策略的合理性。</li>
<li>results: 数据表明，使用本研究提出的方法可以确保优化策略满足指定的控制要求，并且可以在 OpenAI Gym 中的两个示例环境中进行 validate。<details>
<summary>Abstract</summary>
In addressing control problems such as regulation and tracking through reinforcement learning, it is often required to guarantee that the acquired policy meets essential performance and stability criteria such as a desired settling time and steady-state error prior to deployment. Motivated by this necessity, we present a set of results and a systematic reward shaping procedure that (i) ensures the optimal policy generates trajectories that align with specified control requirements and (ii) allows to assess whether any given policy satisfies them. We validate our approach through comprehensive numerical experiments conducted in two representative environments from OpenAI Gym: the Inverted Pendulum swing-up problem and the Lunar Lander. Utilizing both tabular and deep reinforcement learning methods, our experiments consistently affirm the efficacy of our proposed framework, highlighting its effectiveness in ensuring policy adherence to the prescribed control requirements.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将控制问题，如补偿和跟踪，通过强化学习方法解决时，经常需要保证获得的策略能够满足必要的性能和稳定性标准，如所需的定点时间和稳定态误差。驱动了这种需求，我们提出了一组结果和一种系统的奖励形式，以确保优化策略生成的轨迹与指定的控制要求相对应，并且可以评估任何给定策略是否满足这些要求。我们通过对OpenAI Gym提供的两个示例环境中的摆式椅子振荡问题和月球降落问题进行了广泛的数学实验，结果表明我们的提出的框架具有确保策略遵循指定控制要求的效iveness。
</details></li>
</ul>
<hr>
<h2 id="Online-Optimization-for-Network-Resource-Allocation-and-Comparison-with-Reinforcement-Learning-Techniques"><a href="#Online-Optimization-for-Network-Resource-Allocation-and-Comparison-with-Reinforcement-Learning-Techniques" class="headerlink" title="Online Optimization for Network Resource Allocation and Comparison with Reinforcement Learning Techniques"></a>Online Optimization for Network Resource Allocation and Comparison with Reinforcement Learning Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10023">http://arxiv.org/abs/2311.10023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Sid-Ali, Ioannis Lambadaris, Yiqiang Q. Zhao, Gennady Shaikhet, Amirhossein Asgharnia</li>
<li>for: 这paper是为了解决在线网络资源分配问题，包括工作转移。</li>
<li>methods: 这paper使用了Randomized Online Algorithm based on exponentially weighted method。</li>
<li>results: 这paper证明了该算法具有下线时间 regret，并且在人工数据上测试表明该算法在工作转移问题上表现出优于强化学习方法。<details>
<summary>Abstract</summary>
We tackle in this paper an online network resource allocation problem with job transfers. The network is composed of many servers connected by communication links. The system operates in discrete time; at each time slot, the administrator reserves resources at servers for future job requests, and a cost is incurred for the reservations made. Then, after receptions, the jobs may be transferred between the servers to best accommodate the demands. This incurs an additional transport cost. Finally, if a job request cannot be satisfied, there is a violation that engenders a cost to pay for the blocked job. We propose a randomized online algorithm based on the exponentially weighted method. We prove that our algorithm enjoys a sub-linear in time regret, which indicates that the algorithm is adapting and learning from its experiences and is becoming more efficient in its decision-making as it accumulates more data. Moreover, we test the performance of our algorithm on artificial data and compare it against a reinforcement learning method where we show that our proposed method outperforms the latter.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了一个在线网络资源分配问题，其中包括作业传输。网络由多个服务器连接而成，系统在精确时钟下运行，管理员在每个时间槽内预留服务器上的资源，以储存未来的作业请求。预留资源的成本将会产生。然后，接收作业可能会被传输到不同的服务器，以满足需求。这会产生额外的传输成本。如果一个作业请求无法满足，那么会出现阻塞，并且需要支付阻塞作业的成本。我们提议一种随机在线算法，基于加速方法。我们证明我们的算法具有线性小于时间的 regret，这表明我们的算法在经验学习和决策过程中变得更加高效。此外，我们在人工数据上测试了我们的算法，并与一种强化学习方法进行比较，我们的提议方法在效率方面表现更好。
</details></li>
</ul>
<hr>
<h2 id="Finding-Real-World-Orbital-Motion-Laws-from-Data"><a href="#Finding-Real-World-Orbital-Motion-Laws-from-Data" class="headerlink" title="Finding Real-World Orbital Motion Laws from Data"></a>Finding Real-World Orbital Motion Laws from Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10012">http://arxiv.org/abs/2311.10012</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Funenga, Marta Guimarães, Henrique Costa, Cláudia Soares</li>
<li>for: 这个研究旨在找到在空间中卫星的运动方程式。</li>
<li>methods: 这种方法基于SINDy数据驱动技术，可以从时间序列数据中找到物理系统的下面动力学。</li>
<li>results: 这种方法可以高精度地描述LEO卫星的运动轨迹，并且可以保持物理可读性的坐标系。<details>
<summary>Abstract</summary>
A novel approach is presented for discovering PDEs that govern the motion of satellites in space. The method is based on SINDy, a data-driven technique capable of identifying the underlying dynamics of complex physical systems from time series data. SINDy is utilized to uncover PDEs that describe the laws of physics in space, which are non-deterministic and influenced by various factors such as drag or the reference area (related to the attitude of the satellite). In contrast to prior works, the physically interpretable coordinate system is maintained, and no dimensionality reduction technique is applied to the data. By training the model with multiple representative trajectories of LEO - encompassing various inclinations, eccentricities, and altitudes - and testing it with unseen orbital motion patterns, a mean error of around 140 km for the positions and 0.12 km/s for the velocities is achieved. The method offers the advantage of delivering interpretable, accurate, and complex models of orbital motion that can be employed for propagation or as inputs to predictive models for other variables of interest, such as atmospheric drag or the probability of collision in an encounter with a spacecraft or space objects. In conclusion, the work demonstrates the promising potential of using SINDy to discover the equations governing the behaviour of satellites in space. The technique has been successfully applied to uncover PDEs describing the motion of satellites in LEO with high accuracy. The method possesses several advantages over traditional models, including the ability to provide physically interpretable, accurate, and complex models of orbital motion derived from high-entropy datasets. These models can be utilised for propagation or as inputs to predictive models for other variables of interest.
</details>
<details>
<summary>摘要</summary>
一种新的方法被提出，用于发现卫星在空间中的运动方程。该方法基于SINDy，一种数据驱动的技术，可以从时间序列数据中找到物理系统的下面动力学。SINDy被用来揭示卫星运动的PDE，这些PDE是非束定的，受到阻力或参考面积的影响。与先前的工作不同，physically interpretable的坐标系统被保留，无需应用任何维度减少技术。通过训练模型使用多个LEO表示轨迹，包括不同的倾斜、轨道半长轴和高度，并测试它们与未经见过的轨道运动模式，实现了平均误差约为140公里的位置和0.12公里/秒的速度。这种方法具有以下优点：可提供可解释、准确、复杂的轨道运动模型，可以用于卫星的传播或作为其他变量的预测模型的输入。总之，这种方法在发现卫星在空间中的运动方程方面表现出了扎实的推力，并成功地应用于LEO中的卫星运动。这种方法比传统模型具有多个优点，包括能提供physically interpretable、准确、复杂的轨道运动模型，从高熵数据集中拟合出来的。这些模型可以用于卫星的传播或作为其他变量的预测模型的输入。
</details></li>
</ul>
<hr>
<h2 id="Co-data-Learning-for-Bayesian-Additive-Regression-Trees"><a href="#Co-data-Learning-for-Bayesian-Additive-Regression-Trees" class="headerlink" title="Co-data Learning for Bayesian Additive Regression Trees"></a>Co-data Learning for Bayesian Additive Regression Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09997">http://arxiv.org/abs/2311.09997</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/JeroenGoedhart/EB_coBART_paper">https://github.com/JeroenGoedhart/EB_coBART_paper</a></li>
<li>paper_authors: Jeroen M. Goedhart, Thomas Klausch, Jurriaan Janssen, Mark A. van de Wiel</li>
<li>For:  This paper proposes a method to incorporate external information (co-data) into Bayesian additive regression trees (BART) to improve prediction in medical prediction applications with small sample sizes and high-dimensional covariates.* Methods: The proposed method uses an empirical Bayes (EB) framework to estimate prior covariate weights in the BART model, and can handle multiple types of co-data simultaneously. The EB framework also estimates other hyperparameters of BART.* Results: The proposed method finds relevant covariates and improves prediction compared to default BART in simulations, and outperforms regression-based co-data learners when the covariate-response relationship is nonlinear. The method is applied to diffuse large B-cell lymphoma prognosis with clinical covariates, gene mutations, DNA translocations, and DNA copy number data.<details>
<summary>Abstract</summary>
Medical prediction applications often need to deal with small sample sizes compared to the number of covariates. Such data pose problems for prediction and variable selection, especially when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate co-data, i.e. external information on the covariates, into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate co-data, an empirical Bayes (EB) framework is developed that estimates, assisted by a co-data model, prior covariate weights in the BART model. The proposed method can handle multiple types of co-data simultaneously. Furthermore, the proposed EB framework enables the estimation of the other hyperparameters of BART as well, rendering an appealing alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is nonlinear, the method benefits from the flexibility of BART to outperform regression-based co-data learners. Finally, the use of co-data enhances prediction in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.   Keywords: Bayesian additive regression trees; Empirical Bayes; Co-data; High-dimensional data; Omics; Prediction
</details>
<details>
<summary>摘要</summary>
医学预测应用经常面临小样本大于变量的问题。这些数据会对预测和变量选择造成困难，� особенply when the covariate-response relationship is complicated. To address these challenges, we propose to incorporate co-data, i.e. external information on the covariates, into Bayesian additive regression trees (BART), a sum-of-trees prediction model that utilizes priors on the tree parameters to prevent overfitting. To incorporate co-data, an empirical Bayes (EB) framework is developed that estimates, assisted by a co-data model, prior covariate weights in the BART model. The proposed method can handle multiple types of co-data simultaneously. Furthermore, the proposed EB framework enables the estimation of the other hyperparameters of BART as well, rendering an appealing alternative to cross-validation. We show that the method finds relevant covariates and that it improves prediction compared to default BART in simulations. If the covariate-response relationship is nonlinear, the method benefits from the flexibility of BART to outperform regression-based co-data learners. Finally, the use of co-data enhances prediction in an application to diffuse large B-cell lymphoma prognosis based on clinical covariates, gene mutations, DNA translocations, and DNA copy number data.关键字：Bayesian additive regression trees; Empirical Bayes; Co-data; High-dimensional data; Omics; Prediction
</details></li>
</ul>
<hr>
<h2 id="Xputer-Bridging-Data-Gaps-with-NMF-XGBoost-and-a-Streamlined-GUI-Experience"><a href="#Xputer-Bridging-Data-Gaps-with-NMF-XGBoost-and-a-Streamlined-GUI-Experience" class="headerlink" title="Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience"></a>Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09989">http://arxiv.org/abs/2311.09989</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saleena Younus, Lars Rönnstrand, Julhash U. Kazi</li>
<li>for: 本研究旨在提供一个可靠的数据填充工具，以确保数据完整性和有意义的分析结果。</li>
<li>methods: 本工具使用非正式矩阵分解（NMF）和XGBoost的预测能力，并支持零填充、自动化参数优化和迭代次数设定。</li>
<li>results: 在性能评估中，Xputer与已有的工具 IterativeImputer 相比，不仅 Computational speed 快，而且在填充精度方面也经常表现出来。此外，Xputer可以自动处理多种数据类型，包括分类、连续和布尔型数据，不需要先进行处理。<details>
<summary>Abstract</summary>
The rapid proliferation of data across diverse fields has accentuated the importance of accurate imputation for missing values. This task is crucial for ensuring data integrity and deriving meaningful insights. In response to this challenge, we present Xputer, a novel imputation tool that adeptly integrates Non-negative Matrix Factorization (NMF) with the predictive strengths of XGBoost. One of Xputer's standout features is its versatility: it supports zero imputation, enables hyperparameter optimization through Optuna, and allows users to define the number of iterations. For enhanced user experience and accessibility, we have equipped Xputer with an intuitive Graphical User Interface (GUI) ensuring ease of handling, even for those less familiar with computational tools. In performance benchmarks, Xputer not only rivals the computational speed of established tools such as IterativeImputer but also often outperforms them in terms of imputation accuracy. Furthermore, Xputer autonomously handles a diverse spectrum of data types, including categorical, continuous, and Boolean, eliminating the need for prior preprocessing. Given its blend of performance, flexibility, and user-friendly design, Xputer emerges as a state-of-the-art solution in the realm of data imputation.
</details>
<details>
<summary>摘要</summary>
随着数据在多个领域的快速扩散，缺失值的准确填充变得非常重要，以保持数据完整性和获得有意义的发现。为回应这个挑战，我们提出了Xputer，一种新的填充工具，它将非正式矩阵分解（NMF）与XGBoost的预测能力结合得非常灵活。Xputer的一些特点包括：* 支持零填充* 通过Optuna进行参数优化* 允许用户定义迭代次数为了提高用户体验和可达性，我们为Xputer设计了一个直观的图形用户界面（GUI），使其易于操作，即使用户对计算工具不熟悉。在性能测试中，Xputer不仅与已有的工具如IterativeImputer相当，而且经常超越它们在填充精度方面。此外，Xputer可以自动处理多种数据类型，包括 categorical、continue 和Boolean，从而消除了先前的预处理需求。由于其综合表现、灵活性和用户友好的设计，Xputer在数据填充领域中成为了状态 искусственный的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Self-supervised-learning-of-multi-omics-embeddings-in-the-low-label-high-data-regime"><a href="#Self-supervised-learning-of-multi-omics-embeddings-in-the-low-label-high-data-regime" class="headerlink" title="Self-supervised learning of multi-omics embeddings in the low-label, high-data regime"></a>Self-supervised learning of multi-omics embeddings in the low-label, high-data regime</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09962">http://arxiv.org/abs/2311.09962</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian John Hurry, Emma Slade</li>
<li>for: 这项研究使用自然语言处理技术来训练一个预训练FT-Transformer模型，用于预测肿瘤类型基于miRNA、mRNA或RPPA表达数据。</li>
<li>methods: 这个模型使用了自然语言处理技术进行自我超级vised学习（SSL）训练，并与标注样本scarce，但无标注样本数量很多进行比较。</li>
<li>results: 研究发现，使用这种方法可以在肿瘤类型预测 tasks 中获得更高的性能，比如XGBoost和CatBoost等标准准则。此外，研究还探讨了多modal SSL，并提出了一种晚期融合模型，其中每种Omics都通过自己的子网络进行处理，然后将输出融合并传递给预训练或下游目标函数。这种方法在多modal样本中预测单 modal 样本的性能得到了改进。<details>
<summary>Abstract</summary>
Contrastive, self-supervised learning (SSL) is used to train a model that predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high. This is despite the fact that the datasets we use have $\mathcal{O}(10^{1})$ classes and $\mathcal{O}(10^{2})-\mathcal{O}(10^{4})$ features. After demonstrating the efficacy of our chosen method of self-supervised pretraining, we investigate SSL for multi-modal models. A late-fusion model is proposed, where each omics is passed through its own sub-network, the outputs of which are averaged and passed to the pretraining or downstream objective function. Multi-modal pretraining is shown to improve predictions from a single omics, and we argue that this is useful for datasets with many unlabelled multi-modal samples, but few labelled unimodal samples. Additionally, we show that pretraining each omics-specific module individually is highly effective. This enables the application of the proposed model in a variety of contexts where a large amount of unlabelled data is available from each omics, but only a few labelled samples.
</details>
<details>
<summary>摘要</summary>
“对比自学习（Contrastive, self-supervised learning）用于训练一个预训练FT-Transformer模型，用于预测肿瘤类型基于miRNA、mRNA或RPPA表达数据。这个模型在标签样本稀缺但无标签样本多的情况下表现出优于XGBoost和CatBoost标准准则，即使我们使用的数据集有数十个类型和数十个到数百个特征。我们首先证明了我们选择的自我超vised预训练方法的有效性，然后我们调查了多modal模型的SSL。我们提议了一种晚期 fusione模型，其中每个Omics被 passing through its own sub-network，输出被平均并 passing to the pretraining或 downstream objective function。我们发现，多modal预训练可以提高单modal预测结果，并且我们认为这是有用的在 datasets中有多个不标签多modal样本，但只有少量标签单modal样本。此外，我们发现预训练每个 OmicsSpecific module 都是非常有效的。这使得我们的模型可以在每个 Omics 有大量未标签数据，但只有几个标签单Modal 样本的情况下应用。”
</details></li>
</ul>
<hr>
<h2 id="Natural-Disaster-Analysis-using-Satellite-Imagery-and-Social-Media-Data-for-Emergency-Response-Situations"><a href="#Natural-Disaster-Analysis-using-Satellite-Imagery-and-Social-Media-Data-for-Emergency-Response-Situations" class="headerlink" title="Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations"></a>Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09947">http://arxiv.org/abs/2311.09947</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sukeerthi Mandyam, Shanmuga Priya MG, Shalini Suresh, Kavitha Srinivasan<br>for:这项研究旨在分析不同类型的数据（卫星图像和推特数据），以提供深入的灾害管理分析。methods:这项研究包括两个阶段：卫星图像分析和推特数据分析，然后将这两个模块集成使用位置坐标。在第一阶段，使用多类地表征分 segmentation技术，基于U-Net架构进行预和后灾害卫星图像分析。在第二阶段，将地区映射到必需的紧急救援操作信息上，并提取推特数据使用关键词对应的地区。results:这项研究得到的结果是一种基于实时位置坐标和频率分析技术的多维ensional信息集成系统，可以帮助灾害管理人员在灾害发生时获得全面的情况概述，如喀拉拉和密西西比洪灾的分析和验证。这项研究的创新之处在于通过使用分割卫星图像和地区特定筛选器，对灾害区域进行深入的分析和救援操作。<details>
<summary>Abstract</summary>
Disaster Management is one of the most promising research areas because of its significant economic, environmental and social repercussions. This research focuses on analyzing different types of data (pre and post satellite images and twitter data) related to disaster management for in-depth analysis of location-wise emergency requirements. This research has been divided into two stages, namely, satellite image analysis and twitter data analysis followed by integration using location. The first stage involves pre and post disaster satellite image analysis of the location using multi-class land cover segmentation technique based on U-Net architecture. The second stage focuses on mapping the region with essential information about the disaster situation and immediate requirements for relief operations. The severely affected regions are demarcated and twitter data is extracted using keywords respective to that location. The extraction of situational information from a large corpus of raw tweets adopts Content Word based Tweet Summarization (COWTS) technique. An integration of these modules using real-time location-based mapping and frequency analysis technique gathers multi-dimensional information in the advent of disaster occurrence such as the Kerala and Mississippi floods that were analyzed and validated as test cases. The novelty of this research lies in the application of segmented satellite images for disaster relief using highlighted land cover changes and integration of twitter data by mapping these region-specific filters for obtaining a complete overview of the disaster.
</details>
<details>
<summary>摘要</summary>
灾害管理是一个非常有前途的研究领域，因为它具有重要的经济、环境和社会影响。这个研究的目的是分析不同类型的数据（卫星图像和推特数据），以进行深入的灾害管理分析。这个研究分为两个阶段：卫星图像分析和推特数据分析，然后是这两个分析结果的集成。第一阶段是使用多类别土地覆盖分类技术（U-Net架构）进行卫星图像分析，以分析灾害发生前后的地区变化。第二阶段是将地区分配为不同的灾害情况，并从推特数据中提取相关信息。在这个阶段，采用Content Word based Tweet Summarization（COWTS）技术来提取灾害情况的主要信息。将这两个模块集成使用实时地理位置基于的映射和频率分析技术，可以同时获得不同灾害情况的多维度信息。在实验阶段，对印度喀拉拉和美国密西西比洪涝进行了实验和验证。这个研究的创新点在于，通过使用分类卫星图像和地区特定的推特数据，对灾害发生情况进行全面的概括和分析。
</details></li>
</ul>
<hr>
<h2 id="Fast-multiplication-by-two’s-complement-addition-of-numbers-represented-as-a-set-of-polynomial-radix-2-indexes-stored-as-an-integer-list-for-massively-parallel-computation"><a href="#Fast-multiplication-by-two’s-complement-addition-of-numbers-represented-as-a-set-of-polynomial-radix-2-indexes-stored-as-an-integer-list-for-massively-parallel-computation" class="headerlink" title="Fast multiplication by two’s complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation"></a>Fast multiplication by two’s complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09922">http://arxiv.org/abs/2311.09922</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mark Stocks</li>
<li>for: 这个论文是用于描述一种基于整数表示为二进制指数的多项式方法，用于高速Multiplication。</li>
<li>methods: 这种方法使用了一种基于二进制指数的多项式方法，实现在Python代码中。它比Number Theoretic Transform (NTT)和Karatsuba方法快于 multiplication 在某些bit范围内。</li>
<li>results: 这种方法可以实现任何整数或实数的表示为一个列表中的整数指数，并且可以将这些指数存储和分布在多个CPU &#x2F; GPU上。此外，这种方法还可以完全分布加法和乘法操作，从而超越现有的并行乘法方法的限制，即需要共享公共核心内存和磁盘来计算结果和中间结果。<details>
<summary>Abstract</summary>
We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results.
</details>
<details>
<summary>摘要</summary>
我们展示了一种多项式方法，基于表示为二进制基数指数的数字，并将其存储为整数列表。我们称之为“多项式整数指标乘法”方法，这是一系列python代码实现的算法。我们证明这种方法在某个位数范围内比NUMBER THEORETIC TRANSFORM（NTT）和加加姆托卡（Karatsuba） multiplication 方法更快。此外，我们还在python代码中实现了这些方法，以便与多项式整数指标乘法方法进行比较。我们示出了任意整数或实数可以表示为一个列表的整数指数表示，并且这个表示可以被存储和分布在多个CPU / GPU架构上。我们还证明了在多个CPU / GPU架构上实现了完全分布式加法和乘法操作，使得“多项式整数指标乘法”方法超越了当前的并行乘法方法的限制，即需要共享公共核心内存和公共磁盘来计算结果和中间结果。
</details></li>
</ul>
<hr>
<h2 id="On-some-elusive-aspects-of-databases-hindering-AI-based-discovery-A-case-study-on-superconducting-materials"><a href="#On-some-elusive-aspects-of-databases-hindering-AI-based-discovery-A-case-study-on-superconducting-materials" class="headerlink" title="On some elusive aspects of databases hindering AI based discovery: A case study on superconducting materials"></a>On some elusive aspects of databases hindering AI based discovery: A case study on superconducting materials</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09891">http://arxiv.org/abs/2311.09891</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/giotre/LK-99">https://github.com/giotre/LK-99</a></li>
<li>paper_authors: Giovanni Trezza, Eliodoro Chiavazzo</li>
<li>for: 本文旨在探讨大数据的准确性和AI模型的设计问题。</li>
<li>methods: 本文使用了三种方法来检测和衡量数据偏见：批处理方法、维度衡量方法和维度减少方法。</li>
<li>results: 本文通过对超导材料和热电材料两种示例进行分析，发现数据偏见存在于样本选择、隐藏变量和数据年龄等方面，并提出了一种新的检测方法。<details>
<summary>Abstract</summary>
It stands to reason that the amount and the quality of big data is of key importance for setting up accurate AI-driven models. Nonetheless, we believe there are still critical roadblocks in the inherent generation of databases, that are often underestimated and poorly discussed in the literature. In our view, such issues can seriously hinder the AI-based discovery process, even when high quality, sufficiently large and highly reputable data sources are available. Here, considering superconducting and thermoelectric materials as two representative case studies, we specifically discuss three aspects, namely intrinsically biased sample selection, possible hidden variables, disparate data age. Importantly, to our knowledge, we suggest and test a first strategy capable of detecting and quantifying the presence of the intrinsic data bias.
</details>
<details>
<summary>摘要</summary>
“据悉，大数据量和质量对于建立准确的人工智能驱动模型是关键。然而，我们认为在自然生成数据库时存在一些 crítical roadblocks，这些问题在文献中受到了低估和不充分讨论。我们认为这些问题可能会妨碍人工智能基于发现过程，即使有高质量、充分大、受人尊敬的数据源也有。在这里，通过使用超导和热电材料作为两个例子，我们专门讨论了三个方面：内在偏见样本选择、隐藏变量和数据年龄差异。值得注意的是，我们建议和测试了一种能够检测和衡量内在数据偏见的第一种策略。”Here's a breakdown of the translation:* "据悉" (liàng bì) is an idiomatic expression that means "according to what is known" or "as far as is known."* "大数据量" (dà xù xiǎng) means "large amount of data."* "质量" (jīn yù) means "quality."* "关键" (guān jī) means "critical" or "key."* "因此" (yǐn qī) is a conjunction that means "therefore" or "as a result."* "存在" (cún zhī) means "there is" or "exists."* "critical roadblocks" is translated as " crítical roadblocks" (zhì zhì fāng xiào) to emphasize the importance of the issues.* "在文献中" (zài wén xiǎng zhī) means "in the literature" or "as discussed in the literature."* "受到了低估" (shòu dào le duō jì) means "have been underestimated."* "不充分讨论" (bù zhòng fēn tóu yì) means "have not been fully discussed."* "我们认为" (wǒ men rèn wēi) is a phrase that means "we believe" or "in our view."* "这些问题" (zhè xiē wèn tí) is a phrase that means "these issues" or "these problems."* "可能会" (kě néng huì) is a phrase that means "may" or "might."* "妨碍" (mǐng yòu) means "obstacle" or "hinder."* "人工智能基于发现过程" (rén gōng jì yì jī bù jiào yù) is a phrase that means "artificial intelligence based on the discovery process."* "即使" (jī shì) is a conjunction that means "even if" or "despite."* "有高质量" (yǒu gāo jīn yù) means "have high quality."* "充分大" (chōng fēn dà) means "sufficiently large."* "受人尊敬" (shòu rén zhù jì) means "respected by people" or "well-regarded."* "数据源" (xìng xiào) means "data source."* "有" (yǒu) is a particle that indicates the existence of something.* "三个方面" (sān gè fāng miàn) is a phrase that means "three aspects" or "three sides."* "内在偏见样本选择" (nèi zài pēn jiàn yàng bǎn jiǎo) is a phrase that means "intrinsic bias in sample selection."* "隐藏变量" (hūn yǎn biàn yù) means "hidden variables."* "数据年龄差异" (xìng xiàng nián suī) is a phrase that means "data age difference."* "值得注意的是" (fù dé zhù yì de shì) is a phrase that means "it is worth noting that" or "it is worth mentioning that."* "我们建议和测试了一种能够检测和衡量内在数据偏见的第一种策略" (wǒ men jiàn yù hé cè shì yī zhī fāng zhì) is a sentence that means "We suggest and test a strategy that can detect and measure the intrinsic data bias for the first time."
</details></li>
</ul>
<hr>
<h2 id="Safety-Aware-Autonomous-Path-Planning-Using-Model-Predictive-Reinforcement-Learning-for-Inland-Waterways"><a href="#Safety-Aware-Autonomous-Path-Planning-Using-Model-Predictive-Reinforcement-Learning-for-Inland-Waterways" class="headerlink" title="Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement Learning for Inland Waterways"></a>Safety Aware Autonomous Path Planning Using Model Predictive Reinforcement Learning for Inland Waterways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09878">http://arxiv.org/abs/2311.09878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Astrid Vanneste, Simon Vanneste, Olivier Vasseur, Robin Janssens, Mattias Billast, Ali Anwar, Kevin Mets, Tom De Schepper, Siegfried Mercelis, Peter Hellinckx</li>
<li>for: 这篇论文是关于自动水上交通的规划方法，尤其是在城市水道中的自动船运行。</li>
<li>methods: 本文提出了一种基于强化学习的规划方法，即预测类型的优化学习（Model Predictive Reinforcement Learning，MPRL），可以处理任何形状的水道和任何数量和形状的障碍物。</li>
<li>results: 实验结果显示，MPRL在两个测试场景中比基于对称框架的规划和基于 proximal policy optimization（PPO）的规划更好，能够安全（无撞击）通过两个测试场景。<details>
<summary>Abstract</summary>
In recent years, interest in autonomous shipping in urban waterways has increased significantly due to the trend of keeping cars and trucks out of city centers. Classical approaches such as Frenet frame based planning and potential field navigation often require tuning of many configuration parameters and sometimes even require a different configuration depending on the situation. In this paper, we propose a novel path planning approach based on reinforcement learning called Model Predictive Reinforcement Learning (MPRL). MPRL calculates a series of waypoints for the vessel to follow. The environment is represented as an occupancy grid map, allowing us to deal with any shape of waterway and any number and shape of obstacles. We demonstrate our approach on two scenarios and compare the resulting path with path planning using a Frenet frame and path planning based on a proximal policy optimization (PPO) agent. Our results show that MPRL outperforms both baselines in both test scenarios. The PPO based approach was not able to reach the goal in either scenario while the Frenet frame approach failed in the scenario consisting of a corner with obstacles. MPRL was able to safely (collision free) navigate to the goal in both of the test scenarios.
</details>
<details>
<summary>摘要</summary>
近年来，自动水上交通在城市水道中受到了广泛关注，因为许多城市中心禁止汽车和卡车的进入。传统的方法，如基于弗雷内特框的规划和潜在场 Navigation，经常需要调整许多配置参数，甚至在不同情况下需要不同的配置。在本文中，我们提出了一种基于强化学习的新的规划方法， called Model Predictive Reinforcement Learning（MPRL）。MPRL计算出船只应该遵循的一系列方向点。环境被表示为一个占用度网格地图，这使得我们可以处理任何形状的水道和任何形状和数量的障碍物。我们在两个场景中测试了我们的方法，并与基于Frenet框的规划和基于 proximal policy optimization（PPO）算法的规划进行比较。我们的结果显示，MPRL在两个测试场景中都超过了两个基准点。PPO算法在任一场景中都无法达到目标，而基于Frenet框的规划在包含封闭障碍物的场景中失败。MPRL在两个测试场景中安全（无碰撞）地达到了目标。
</details></li>
</ul>
<hr>
<h2 id="Polynomially-Over-Parameterized-Convolutional-Neural-Networks-Contain-Structured-Strong-Winning-Lottery-Tickets"><a href="#Polynomially-Over-Parameterized-Convolutional-Neural-Networks-Contain-Structured-Strong-Winning-Lottery-Tickets" class="headerlink" title="Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets"></a>Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09858">http://arxiv.org/abs/2311.09858</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arthur da Cunha, Francesco d’Amore, Emanuele Natale</li>
<li>for: 研究SLTH中structured pruning的可行性</li>
<li>methods: 使用多dimensional generalization of Random Subset-Sum Problem来解决SLTH中的随机依赖关系</li>
<li>results: 提出了一种可以将任意小型网络拟合到Structured Pruning中的structured subnetwork的存在，这是SLTH的第一个下 exponential bound，开启了新的研究方向，帮助深入理解深度学习中过参数化的作用。<details>
<summary>Abstract</summary>
The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH. In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.   This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning.
</details>
<details>
<summary>摘要</summary>
“强大的抽签票假设（SLTH）称 randomly-initialized 神经网络中可能存在不需要训练的子网络，却受到不结构的剪除研究的限制。这里我们与此有关的主要原因之一是下列数学工具的限制：我们使用了最新的多dimensional 普通化的Random Subset-Sum Problem，从而获得了允许随机相依性的variant。我们运用这个结果，证明了一个广泛的随机卷积神经网络中，存在一些可以近似任何较小的网络的结构化子网络。这个结果提供了SLTH关于结构剪除的首个次对数 bounds，对于深度学习的理解做出了新的贡献，并开启了新的研究方向。”
</details></li>
</ul>
<hr>
<h2 id="Contribution-Evaluation-in-Federated-Learning-Examining-Current-Approaches"><a href="#Contribution-Evaluation-in-Federated-Learning-Examining-Current-Approaches" class="headerlink" title="Contribution Evaluation in Federated Learning: Examining Current Approaches"></a>Contribution Evaluation in Federated Learning: Examining Current Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09856">http://arxiv.org/abs/2311.09856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilis Siomos, Jonathan Passerat-Palmbach</li>
<li>for: 这篇论文关注联合学习（Federated Learning，FL）在保持隐私和管理数据的情况下进行模型训练，具体来说是计算每个客户端的贡献值。</li>
<li>methods: 论文回顾当前的贡献评估方法，从数学基础框架到实现方式，并对一些最有前途的state-of-the-art方法进行比较。</li>
<li>results: 论文通过在MNIST和CIFAR-10上 benchmarking不同方法的实验结果，展示了各种方法的不同特点，并引出了设计公平和高效的贡献评估方法的重要性。<details>
<summary>Abstract</summary>
Federated Learning (FL) has seen increasing interest in cases where entities want to collaboratively train models while maintaining privacy and governance over their data. In FL, clients with private and potentially heterogeneous data and compute resources come together to train a common model without raw data ever leaving their locale. Instead, the participants contribute by sharing local model updates, which, naturally, differ in quality. Quantitatively evaluating the worth of these contributions is termed the Contribution Evaluation (CE) problem. We review current CE approaches from the underlying mathematical framework to efficiently calculate a fair value for each client. Furthermore, we benchmark some of the most promising state-of-the-art approaches, along with a new one we introduce, on MNIST and CIFAR-10, to showcase their differences. Designing a fair and efficient CE method, while a small part of the overall FL system design, is tantamount to the mainstream adoption of FL.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Short-vs-Long-term-Coordination-of-Drones-When-Distributed-Optimization-Meets-Deep-Reinforcement-Learning"><a href="#Short-vs-Long-term-Coordination-of-Drones-When-Distributed-Optimization-Meets-Deep-Reinforcement-Learning" class="headerlink" title="Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning"></a>Short vs. Long-term Coordination of Drones: When Distributed Optimization Meets Deep Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09852">http://arxiv.org/abs/2311.09852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuhao Qin, Evangelos Pournaras</li>
<li>for: 本研究旨在提供智能飞行器队伍在智能城市中完成感知功能的支持，包括交通监测和灾害应急应急响应。</li>
<li>methods: 该研究使用分布式优化和深度强化学习（DRL）协调飞行器，以实现成本效果、高质量的导航、感知和充电。</li>
<li>results: 对于交通监测，提出的新进展方法在比较三种基准方法的实验中表现出色，显示了其出色的性能。<details>
<summary>Abstract</summary>
Swarms of smart drones, with the support of charging technology, can provide completing sensing capabilities in Smart Cities, such as traffic monitoring and disaster response. Existing approaches, including distributed optimization and deep reinforcement learning (DRL), aim to coordinate drones to achieve cost-effective, high-quality navigation, sensing, and recharging. However, they have distinct challenges: short-term optimization struggles to provide sustained benefits, while long-term DRL lacks scalability, resilience, and flexibility. To bridge this gap, this paper introduces a new progressive approach that encompasses the planning and selection based on distributed optimization, as well as DRL-based flying direction scheduling. Extensive experiment with datasets generated from realisitic urban mobility demonstrate the outstanding performance of the proposed solution in traffic monitoring compared to three baseline methods.
</details>
<details>
<summary>摘要</summary>
众群智能飞机，受到充电技术支持，可以在智能城市提供完整的感知能力，如交通监测和灾害应急应急。现有的方法，包括分布式优化和深度强化学习（DRL），尝试协调飞机以实现成本效果、高质量的导航、感知和充电。然而，它们具有短期优化困难提供持续性利益，长期DRL缺乏扩展性、可靠性和灵活性。为了bridging这个差距，本文提出了一种新的进步方法，包括分布式优化基础上的规划和选择，以及基于DRL的飞机飞行方向安排。实验表明，提出的解决方案在交通监测中比基准方法三种表现出众。
</details></li>
</ul>
<hr>
<h2 id="Diffusion-Augmented-Neural-Processes"><a href="#Diffusion-Augmented-Neural-Processes" class="headerlink" title="Diffusion-Augmented Neural Processes"></a>Diffusion-Augmented Neural Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09848">http://arxiv.org/abs/2311.09848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Bonito, James Requeima, Aliaksandra Shysheya, Richard E. Turner</li>
<li>for: 这 paper 是为了提供一种新的替代方法来模型 neural processes，以更好地适应具有数据稀缺和预测不确定性的应用领域，如健康科学和气候科学。</li>
<li>methods: 该 paper 使用了一种基于扩散的方法，通过对含有噪声的数据进行conditioning来解决许多现有方法的限制，同时也超越了现有最佳实践的性能。</li>
<li>results: 该 paper 的研究结果表明，该新方法可以在各种应用领域中提供更高的准确性和稳定性，并且可以与现有方法进行比较。<details>
<summary>Abstract</summary>
Over the last few years, Neural Processes have become a useful modelling tool in many application areas, such as healthcare and climate sciences, in which data are scarce and prediction uncertainty estimates are indispensable. However, the current state of the art in the field (AR CNPs; Bruinsma et al., 2023) presents a few issues that prevent its widespread deployment. This work proposes an alternative, diffusion-based approach to NPs which, through conditioning on noised datasets, addresses many of these limitations, whilst also exceeding SOTA performance.
</details>
<details>
<summary>摘要</summary>
在过去几年，神经过程（Neural Processes）已成为许多应用领域中的有用模型工具，如医疗和气候科学，在数据稀缺和预测uncertainty估计是不可或缺的。然而，当前领域的状态艺（AR CNPs；布鲁因斯马等，2023）存在一些限制，阻碍其广泛应用。这项工作提出了一种替代方案，基于扩散的方法，通过对噪音数据进行条件，解决了许多这些限制，同时也超越了最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Runtime-Verification-of-Learning-Properties-for-Reinforcement-Learning-Algorithms"><a href="#Runtime-Verification-of-Learning-Properties-for-Reinforcement-Learning-Algorithms" class="headerlink" title="Runtime Verification of Learning Properties for Reinforcement Learning Algorithms"></a>Runtime Verification of Learning Properties for Reinforcement Learning Algorithms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09811">http://arxiv.org/abs/2311.09811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tommaso Mannucci, Julio de Oliveira Filho</li>
<li>for: 这篇研究旨在提出新的执行时验证技术，以便在RL算法中学习过程中预测学习过程未达或将不会达到期望的质量和时间要求。</li>
<li>methods: 这篇研究使用了新的执行时验证技术，包括三个验证性能，以便在RL算法中监控和评估这些性能 during the system’s operation。</li>
<li>results: 这篇研究获得了三个验证性能，包括学习质量、时间耗用和精度等。这些性能可以用来监控和评估RL算法的学习过程，以便提高学习效率和精度。<details>
<summary>Abstract</summary>
Reinforcement learning (RL) algorithms interact with their environment in a trial-and-error fashion. Such interactions can be expensive, inefficient, and timely when learning on a physical system rather than in a simulation. This work develops new runtime verification techniques to predict when the learning phase has not met or will not meet qualitative and timely expectations. This paper presents three verification properties concerning the quality and timeliness of learning in RL algorithms. With each property, we propose design steps for monitoring and assessing the properties during the system's operation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Fossil-2-0-Formal-Certificate-Synthesis-for-the-Verification-and-Control-of-Dynamical-Models"><a href="#Fossil-2-0-Formal-Certificate-Synthesis-for-the-Verification-and-Control-of-Dynamical-Models" class="headerlink" title="Fossil 2.0: Formal Certificate Synthesis for the Verification and Control of Dynamical Models"></a>Fossil 2.0: Formal Certificate Synthesis for the Verification and Control of Dynamical Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09793">http://arxiv.org/abs/2311.09793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alec Edwards, Andrea Peruffo, Alessandro Abate</li>
<li>for: 这篇论文描述了一种新的软件工具——Fossil 2.0，用于 sintesis 有效性证明（如 Lyapunov 函数和障碍函数） для 动力系统。</li>
<li>methods: 这篇论文使用了一种叫 counterexample-guided inductive synthesis（CEGIS）的方法，通过一个 SMT 解决器来验证 candidate 函数的正确性。</li>
<li>results: Fossil 2.0 可以生成更多的证明、控制法则和对离散时间模型的支持。<details>
<summary>Abstract</summary>
This paper presents Fossil 2.0, a new major release of a software tool for the synthesis of certificates (e.g., Lyapunov and barrier functions) for dynamical systems modelled as ordinary differential and difference equations. Fossil 2.0 is much improved from its original release, including new interfaces, a significantly expanded certificate portfolio, controller synthesis and enhanced extensibility. We present these new features as part of this tool paper. Fossil implements a counterexample-guided inductive synthesis (CEGIS) loop ensuring the soundness of the method. Our tool uses neural networks as templates to generate candidate functions, which are then formally proven by an SMT solver acting as an assertion verifier. Improvements with respect to the first release include a wider range of certificates, synthesis of control laws, and support for discrete-time models.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="GEO-Generative-Engine-Optimization"><a href="#GEO-Generative-Engine-Optimization" class="headerlink" title="GEO: Generative Engine Optimization"></a>GEO: Generative Engine Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09735">http://arxiv.org/abs/2311.09735</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik R Narasimhan, Ameet Deshpande</li>
<li>for: 本研究旨在帮助内容创作者提高生成引擎响应中内容的可见度，以便在生成引擎技术升级的未来，保持创作经济的繁荣。</li>
<li>methods: 本研究提出了一种新的优化策略，即生成引擎优化（GEO），通过黑盒优化和定义可见度指标来帮助内容创作者提高生成引擎响应中内容的可见度。</li>
<li>results: 经过系统评估，本研究发现，通过GEO可以提高生成引擎响应中内容的可见度，最高提高40%。此外，研究还发现不同领域的可见度提升策略效果不同，强调需要针对具体领域进行定制。<details>
<summary>Abstract</summary>
The advent of large language models (LLMs) has ushered in a new paradigm of search engines that use generative models to gather and summarize information to answer user queries. This emerging technology, which we formalize under the unified framework of Generative Engines (GEs), has the potential to generate accurate and personalized responses, and is rapidly replacing traditional search engines like Google and Bing. Generative Engines typically satisfy queries by synthesizing information from multiple sources and summarizing them with the help of LLMs. While this shift significantly improves \textit{user} utility and \textit{generative search engine} traffic, it results in a huge challenge for the third stakeholder -- website and content creators. Given the black-box and fast-moving nature of Generative Engines, content creators have little to no control over when and how their content is displayed. With generative engines here to stay, the right tools should be provided to ensure that creator economy is not severely disadvantaged. To address this, we introduce Generative Engine Optimization (GEO), a novel paradigm to aid content creators in improving the visibility of their content in Generative Engine responses through a black-box optimization framework for optimizing and defining visibility metrics. We facilitate systematic evaluation in this new paradigm by introducing GEO-bench, a benchmark of diverse user queries across multiple domains, coupled with sources required to answer these queries. Through rigorous evaluation, we show that GEO can boost visibility by up to 40\% in generative engine responses. Moreover, we show the efficacy of these strategies varies across domains, underscoring the need for domain-specific methods. Our work opens a new frontier in the field of information discovery systems, with profound implications for generative engines and content creators.
</details>
<details>
<summary>摘要</summary>
随着大型语言模型（LLM）的出现，一种新的搜索引擎 paradigm 已经出现，这种搜索引擎使用生成模型来收集和摘要信息以回答用户问题。我们称这种技术为生成引擎（GE）。这种技术可以生成准确和个性化的回答，并在传统搜索引擎如Google和Bing的替代品上快速取代。生成引擎通常通过将多个源的信息合并并使用LLM进行摘要来满足用户的查询。虽然这种转变会提高用户的用户体验和生成搜索引擎的搜索量，但是它会对内容创建者造成巨大的挑战。由于生成引擎的黑盒和快速移动的性质，内容创建者几乎没有控制他们的内容是如何和何时显示。为了解决这个问题，我们介绍了生成引擎优化（GEO），一种新的方法，可以帮助内容创建者在生成引擎的回答中提高他们的内容的可见度。我们通过引入GEO-bench，一个包含多个领域的多种用户查询和回答的基准，来促进系统性评估。我们的实验表明，GEO可以提高可见度达40%。此外，我们还发现这些策略在不同的领域中的效果不同，强调了需要针对具体领域的方法。我们的工作开启了一个新的领域，即信息发现系统，对生成引擎和内容创建者产生了深远的影响。
</details></li>
</ul>
<hr>
<h2 id="CDMPP-A-Device-Model-Agnostic-Framework-for-Latency-Prediction-of-Tensor-Programs"><a href="#CDMPP-A-Device-Model-Agnostic-Framework-for-Latency-Prediction-of-Tensor-Programs" class="headerlink" title="CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs"></a>CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09690">http://arxiv.org/abs/2311.09690</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/joapolarbear/cdmpp">https://github.com/joapolarbear/cdmpp</a></li>
<li>paper_authors: Hanpeng Hu, Junwei Su, Juntao Zhao, Yanghua Peng, Yibo Zhu, Haibin Lin, Chuan Wu</li>
<li>for: 这个论文旨在提供一种能够准确预测多种tensor程在不同设备上的性能的框架，以便进行DNN图或tensor-level优化和设备选择。</li>
<li>methods: 作者使用了一种叫做CDMPP的框架，该框架使用了一种紧凑的AST表示法和一种基于顺序排序的 pozitional编码方法，以捕捉tensor程的内部结构。并使用了一种域 adapted的方法和KMeans sampling算法，以学习不同域（i.e., 不同的DNN运算和设备）中的域 инвариан特表示。</li>
<li>results: 作者的实验表明，CDMPP在多种DNN模型和设备上表现出色，与状态 искус的基准值相比，CDMPP的预测错误率为14.03%和10.85%，分别为cross-model和cross-device预测。而与之前的基准值相比，CDMPP的训练效率高出一个数量级。实验结果和扩展数据集可以在<a target="_blank" rel="noopener" href="https://github.com/joapolarbear/cdmpp%E4%B8%8A%E4%B8%8B%E8%BD%BD%E3%80%82">https://github.com/joapolarbear/cdmpp上下载。</a><details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) have shown excellent performance in a wide range of machine learning applications. Knowing the latency of running a DNN model or tensor program on a specific device is useful in various tasks, such as DNN graph- or tensor-level optimization and device selection. Considering the large space of DNN models and devices that impede direct profiling of all combinations, recent efforts focus on building a predictor to model the performance of DNN models on different devices. However, none of the existing attempts have achieved a cost model that can accurately predict the performance of various tensor programs while supporting both training and inference accelerators. We propose CDMPP, an efficient tensor program latency prediction framework for both cross-model and cross-device prediction. We design an informative but efficient representation of tensor programs, called compact ASTs, and a pre-order-based positional encoding method, to capture the internal structure of tensor programs. We develop a domain-adaption-inspired method to learn domain-invariant representations and devise a KMeans-based sampling algorithm, for the predictor to learn from different domains (i.e., different DNN operators and devices). Our extensive experiments on a diverse range of DNN models and devices demonstrate that CDMPP significantly outperforms state-of-the-art baselines with 14.03% and 10.85% prediction error for cross-model and cross-device prediction, respectively, and one order of magnitude higher training efficiency. The implementation and the expanded dataset are available at https://github.com/joapolarbear/cdmpp.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）在多种机器学习应用中表现出色。了解在特定设备上运行 DNN 模型或tensor程序的延迟是多种任务中的有用信息，如 DNN 图或tensor程序级别优化和设备选择。由于 DNN 模型和设备之间的空间很大，直接测试所有组合是不可能的。为了解决这个问题，当前的努力集中在建立一个可预测 DNN 模型在不同设备上的性能的模型。然而，现有的尝试都没有实现一个可预测多种tensor程序的性能的成本模型，同时支持训练和推理加速器。我们提出了 CDMPP，一个高效的 tensor程序延迟预测框架，用于跨模型和跨设备预测。我们设计了一种有用但不具有冗余的表示方式，called Compact ASTs，以及一种基于 pre-order 的 pozitional encoding 方法，以捕捉 tensor程序的内部结构。我们开发了一种域 adaptive 方法，用于学习域 invariant 表示，并提出了一种 KMeans 基于采样算法，使预测器从不同域中学习。我们的广泛的实验表明，CDMPP 在多种 DNN 模型和设备上表现出色，与状态对比基线错误率为 14.03% 和 10.85%，具有一个 ORDER 更高的训练效率。实现和扩展数据集可以在 GitHub 上找到：https://github.com/joapolarbear/cdmpp。
</details></li>
</ul>
<hr>
<h2 id="Modelling-daily-mobility-using-mobile-data-traffic-at-fine-spatiotemporal-scale"><a href="#Modelling-daily-mobility-using-mobile-data-traffic-at-fine-spatiotemporal-scale" class="headerlink" title="Modelling daily mobility using mobile data traffic at fine spatiotemporal scale"></a>Modelling daily mobility using mobile data traffic at fine spatiotemporal scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09683">http://arxiv.org/abs/2311.09683</a></li>
<li>repo_url: None</li>
<li>paper_authors: Panayotis Christidis, Maria Vega Gonzalo, Miklos Radics</li>
<li>for: 本研究用数据驱动方法研究了 NetMob 2023 数据集在城市背景下模型流动模式的可用性。</li>
<li>methods: 本研究将 NetMob 2023 数据集与适应度高的外部数据集 ENACT 结合使用，开发了三种 XGBoost 模型，通过对 NetMob2023 数据中的移动数据流量和 ENACT 值进行比较，计算每个 100m x 100m 格子细胞的人口数量。</li>
<li>results: 结果表明，NetMob 2023 数据可以用于 estimate 城市区域的日夜人口和格子级别，并能够解释一些城市流动动态。<details>
<summary>Abstract</summary>
We applied a data-driven approach that explores the usability of the NetMob 2023 dataset in modelling mobility patterns within an urban context. We combined the data with a highly suitable external source, the ENACT dataset, which provides a 1 km x 1km grid with estimates of the day and night population across Europe. We developed three sets of XGBoost models that predict the population in each 100m x 100m grid cell used in NetMob2023 based on the mobile data traffic of the 68 online services covered in the dataset, using the ENACT values as ground truth. The results suggest that the NetMob 2023 data can be useful for the estimation of the day and night population and grid cell level and can explain part of the dynamics of urban mobility.
</details>
<details>
<summary>摘要</summary>
我们采用了数据驱动的方法，探索了NetMob 2023数据集在城市背景下的可用性。我们将数据与非常适合的外部资源——ENACT数据集相结合，该数据集提供了1km x 1km网格中的日夜人口估计数据在欧洲。我们开发了三个XGBoost模型，使用NetMob2023数据集中68个在线服务的流量数据预测每个100m x 100m网格单元中的人口，使用ENACT值作为真实值。结果表明，NetMob 2023数据可以用于 estimate 日夜人口和网格单元层次，并可以解释一部分城市流动性的动态。Note: Please note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Zenkai-–-Framework-For-Exploring-Beyond-Backpropagation"><a href="#Zenkai-–-Framework-For-Exploring-Beyond-Backpropagation" class="headerlink" title="Zenkai – Framework For Exploring Beyond Backpropagation"></a>Zenkai – Framework For Exploring Beyond Backpropagation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09663">http://arxiv.org/abs/2311.09663</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/short-greg/zenkai">https://github.com/short-greg/zenkai</a></li>
<li>paper_authors: Greg Short</li>
<li>for: 该paper的目的是提供一个开源框架，以便研究者在深度学习机器建构和训练方面获得更多的控制和灵活性。</li>
<li>methods: 该paper使用分层自主学习机器，每层有其自己的目标和学习算法，以便让研究者在深度学习领域进行更多的探索，如非 differentiable层或不基于错误反射的学习算法。</li>
<li>results: 该paper通过将深度学习机器分割成层次结构，使得研究者可以更容易地探索新的深度学习领域，不再受限于传统的backpropagation框架。<details>
<summary>Abstract</summary>
Zenkai is an open-source framework designed to give researchers more control and flexibility over building and training deep learning machines. It does this by dividing the deep learning machine into layers of semi-autonomous learning machines with their own target and learning algorithm. This is to allow researchers greater exploration such as the use of non-differentiable layers or learning algorithms beyond those based on error backpropagation.   Backpropagation Rumelhart et al. [1986] has powered deep learning to become one of the most exciting fields of the 21st century. As a result, a large number of software tools have been developed to support efficient implementation and training of neural networks through the use of backpropa- gation. While these have been critical to the success of deep learning, building frameworks around backpropagation can make it challenging to implement solutions that do not adhere to it. Zenkai aims to make it easier to get around these limitations and help researchers more easily explore new frontiers in deep learning that do not strictly adhere to the backpropagation framework.
</details>
<details>
<summary>摘要</summary>
zenkai 是一个开源框架，旨在给研究人员更多的控制和灵活性来建立和训练深度学习机器。它通过将深度学习机器分割成各自有target和学习算法的层次结构，以便让研究人员更好地探索不同的学习方法和算法。这样可以让研究人员更容易实现不同的深度学习解决方案，而不是仅仅依赖于error backpropagation。以前，Rumelhart等人在1986年提出了backpropagation算法，这个算法在21世纪的深度学习领域中帮助了深度学习成为一个非常有趣的领域。随着这些软件工具的开发，深度学习的实现和训练变得更加高效。然而，由backpropagation框架所固化的问题使得实现不同的解决方案变得困难。zenkai旨在使研究人员更容易实现不同的深度学习解决方案，并帮助他们更好地探索不同的深度学习领域。
</details></li>
</ul>
<hr>
<h2 id="GAIA-Delving-into-Gradient-based-Attribution-Abnormality-for-Out-of-distribution-Detection"><a href="#GAIA-Delving-into-Gradient-based-Attribution-Abnormality-for-Out-of-distribution-Detection" class="headerlink" title="GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection"></a>GAIA: Delving into Gradient-based Attribution Abnormality for Out-of-distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09620">http://arxiv.org/abs/2311.09620</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jgethanchen/gaia-ood">https://github.com/jgethanchen/gaia-ood</a></li>
<li>paper_authors: Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, Jing Xiao</li>
<li>for: 本文旨在提供一种新的方法来探测深度神经网络中的异常示例（Out-of-distribution，OOD），以确保神经网络在实际场景中的可靠性和安全性。</li>
<li>methods: 本文使用了解释predictive decisions的gradient-based attribution方法，并发现这些方法在处理OOD数据时遇到困难，导致解释结果呈现异常。基于这个观察，本文引入了两种OOD检测的异常现象：零减异常和通道平均异常。然后，本文提出了一种简单有效的GAIA方法，它利用Gradient Abnormality Inspection and Aggregation来检测OOD示例。</li>
<li>results: 本文的GAIA方法在CIFAR10和ImageNet-1k benchmark上表现出色，比预后处理方法更有效。具体来说，GAIA在CIFAR10上降低了平均FPR95的值by 23.10%，并在CIFAR100上降低了平均FPR95的值by 45.41%。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) examples is crucial to guarantee the reliability and safety of deep neural networks in real-world settings. In this paper, we offer an innovative perspective on quantifying the disparities between in-distribution (ID) and OOD data -- analyzing the uncertainty that arises when models attempt to explain their predictive decisions. This perspective is motivated by our observation that gradient-based attribution methods encounter challenges in assigning feature importance to OOD data, thereby yielding divergent explanation patterns. Consequently, we investigate how attribution gradients lead to uncertain explanation outcomes and introduce two forms of abnormalities for OOD detection: the zero-deflation abnormality and the channel-wise average abnormality. We then propose GAIA, a simple and effective approach that incorporates Gradient Abnormality Inspection and Aggregation. The effectiveness of GAIA is validated on both commonly utilized (CIFAR) and large-scale (ImageNet-1k) benchmarks. Specifically, GAIA reduces the average FPR95 by 23.10% on CIFAR10 and by 45.41% on CIFAR100 compared to advanced post-hoc methods.
</details>
<details>
<summary>摘要</summary>
检测异常输入（OOD）示例是深度神经网络在实际场景中的可靠性和安全的 garantor。在这篇论文中，我们提出了一种新的观点，即通过分析模型对预测决策的解释来衡量ID和OOD数据之间的差异。这种观点是由我们发现，使用梯度基本的归因方法对OOD数据进行归因时会遇到困难，从而导致解释结果呈现出异常的现象。因此，我们调查了梯度归因过程中的不确定性，并提出了两种OOD检测的异常现象：零膨胀异常和通道平均异常。然后，我们提出了GAIA方法，它通过梯度异常检查和综合来实现简单而有效的OOD检测。GAIA方法在CIFAR10和ImageNet-1k两个标准测试集上验证了其效果，比先进的后置方法减少了平均FPR95的值23.10%和45.41%。
</details></li>
</ul>
<hr>
<h2 id="Generating-Drug-Repurposing-Hypotheses-through-the-Combination-of-Disease-Specific-Hypergraphs"><a href="#Generating-Drug-Repurposing-Hypotheses-through-the-Combination-of-Disease-Specific-Hypergraphs" class="headerlink" title="Generating Drug Repurposing Hypotheses through the Combination of Disease-Specific Hypergraphs"></a>Generating Drug Repurposing Hypotheses through the Combination of Disease-Specific Hypergraphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09596">http://arxiv.org/abs/2311.09596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayush Jain, Marie Laure-Charpignon, Irene Y. Chen, Anthony Philippakis, Ahmed Alaa</li>
<li>for: 这个研究的目的是提出一种基于生物医学知识图表示的新药再利用预测方法，以便更好地利用现有的药物来开发新的药物。</li>
<li>methods: 这个研究使用了一种新的、疾病特定的质量学习技术，来学习生物路径的上下文嵌入。这种方法可以处理不同长度的生物路径，并且可以从任何一个药物开始，直到疾病为止。此外，这个研究还扩展了这种方法到多种疾病的质量学习。</li>
<li>results: 这个研究发现了两种可能有潜在的新药再利用 канди达，即达巴格利福酮（一种抗糖尿病药物）和德布瑞索酮（一种反高血压药物）。这两种药物在合并两种疾病的质量学习中，其再利用潜力显著提高。<details>
<summary>Abstract</summary>
The drug development pipeline for a new compound can last 10-20 years and cost over 10 billion. Drug repurposing offers a more time- and cost-effective alternative. Computational approaches based on biomedical knowledge graph representations have recently yielded new drug repurposing hypotheses. In this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. Further, we extend this method to multi-disease hypergraphs. To determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. Cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. We illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32$\%$ to top 7$\%$, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76$\%$ to top 23$\%$). Our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate the following text into Simplified Chinese:The drug development pipeline for a new compound can last 10-20 years and cost over 10 billion. Drug repurposing offers a more time- and cost-effective alternative. Computational approaches based on biomedical knowledge graph representations have recently yielded new drug repurposing hypotheses. In this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. Further, we extend this method to multi-disease hypergraphs. To determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. Cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. We illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32% to top 7%, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76% to top 23%). Our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.中文翻译：drugs development pipeline for a new compound can last 10-20 years and cost over 10 billion. drug repurposing offers a more time- and cost-effective alternative. based on biomedical knowledge graph representations, computational approaches have recently yielded new drug repurposing hypotheses. in this study, we present a novel, disease-specific hypergraph representation learning technique to derive contextual embeddings of biological pathways of various lengths but that all start at any given drug and all end at the disease of interest. further, we extend this method to multi-disease hypergraphs. to determine the repurposing potential of each of the 1,522 drugs, we derive drug-specific distributions of cosine similarity values and ultimately consider the median for ranking. cosine similarity values are computed between (1) all biological pathways starting at the considered drug and ending at the disease of interest and (2) all biological pathways starting at drugs currently prescribed against that disease and ending at the disease of interest. we illustrate our approach with Alzheimer's disease (AD) and two of its risk factors: hypertension (HTN) and type 2 diabetes (T2D). we compare each drug's rank across four hypergraph settings (single- or multi-disease): AD only, AD + HTN, AD + T2D, and AD + HTN + T2D. notably, our framework led to the identification of two promising drugs whose repurposing potential was significantly higher in hypergraphs combining two diseases: dapagliflozin (antidiabetic; moved up, from top 32% to top 7%, across all considered drugs) and debrisoquine (antihypertensive; moved up, from top 76% to top 23%). our approach serves as a hypothesis generation tool, to be paired with a validation pipeline relying on laboratory experiments and semi-automated parsing of the biomedical literature.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-material-discovery-with-a-threshold-driven-hybrid-acquisition-policy-based-Bayesian-optimization"><a href="#Accelerating-material-discovery-with-a-threshold-driven-hybrid-acquisition-policy-based-Bayesian-optimization" class="headerlink" title="Accelerating material discovery with a threshold-driven hybrid acquisition policy-based Bayesian optimization"></a>Accelerating material discovery with a threshold-driven hybrid acquisition policy-based Bayesian optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09591">http://arxiv.org/abs/2311.09591</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Shoyeb Raihan, Hamed Khosravi, Srinjoy Das, Imtiaz Ahmed<br>for: 本研究旨在提高材料发现和开发过程中的效率，通过应用机器学习技术和bayesian优化方法，从而减少实验成本和开发时间。methods: 本研究使用了一种新的阈值驱动的UCB-EI Bayesian优化方法，它将UCB和EI两种获取函数 dynamically интегриру起来，以优化材料发现过程。results: 对于三个不同的材料数据集，TDUE-BO方法显示了较好的优化和approximation性能，比EI和UCB-based BO方法更快地 converges，并且在RMSE分数上显示出较好的表现。<details>
<summary>Abstract</summary>
Advancements in materials play a crucial role in technological progress. However, the process of discovering and developing materials with desired properties is often impeded by substantial experimental costs, extensive resource utilization, and lengthy development periods. To address these challenges, modern approaches often employ machine learning (ML) techniques such as Bayesian Optimization (BO), which streamline the search for optimal materials by iteratively selecting experiments that are most likely to yield beneficial results. However, traditional BO methods, while beneficial, often struggle with balancing the trade-off between exploration and exploitation, leading to sub-optimal performance in material discovery processes. This paper introduces a novel Threshold-Driven UCB-EI Bayesian Optimization (TDUE-BO) method, which dynamically integrates the strengths of Upper Confidence Bound (UCB) and Expected Improvement (EI) acquisition functions to optimize the material discovery process. Unlike the classical BO, our method focuses on efficiently navigating the high-dimensional material design space (MDS). TDUE-BO begins with an exploration-focused UCB approach, ensuring a comprehensive initial sweep of the MDS. As the model gains confidence, indicated by reduced uncertainty, it transitions to the more exploitative EI method, focusing on promising areas identified earlier. The UCB-to-EI switching policy dictated guided through continuous monitoring of the model uncertainty during each step of sequential sampling results in navigating through the MDS more efficiently while ensuring rapid convergence. The effectiveness of TDUE-BO is demonstrated through its application on three different material datasets, showing significantly better approximation and optimization performance over the EI and UCB-based BO methods in terms of the RMSE scores and convergence efficiency, respectively.
</details>
<details>
<summary>摘要</summary>
技术进步受材料进步的影响很大。然而，找到和开发满足需求的材料往往受到巨大的实验成本、资源占用和长时间的开发周期的阻碍。为了解决这些挑战，现代方法常常使用机器学习（ML）技术，如权重优化（BO），以快速搜索优化材料的属性。然而，传统的BO方法，虽有利，但往往在材料发现过程中困难寻找优化和权衡的平衡，导致表现下降。本文介绍一种新的阈值驱动的UCB-EI权重优化方法（TDUE-BO），它在材料设计空间（MDS）中高效地寻找优化。TDUE-BO方法在开始时使用探索带有UCB的方法，以确保初步扫描MDS的全面性。随着模型收获更多的信息，它逐渐过渡到更加利用的EI方法，专注于之前确定的优点。TDUE-BO方法的UCB-to-EI交换策略，通过监测模型在每次采样中的不确定性的连续监测，以高效地在MDS中导航，并确保更快的收敛。TDUE-BO方法在三个不同的材料数据集上进行应用，与EI和UCB基于BO方法的表现相比，在TERMSE scores和收敛效率上显示出了显著的改进。
</details></li>
</ul>
<hr>
<h2 id="Group-Aware-Interest-Disentangled-Dual-Training-for-Personalized-Recommendation"><a href="#Group-Aware-Interest-Disentangled-Dual-Training-for-Personalized-Recommendation" class="headerlink" title="Group-Aware Interest Disentangled Dual-Training for Personalized Recommendation"></a>Group-Aware Interest Disentangled Dual-Training for Personalized Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09577">http://arxiv.org/abs/2311.09577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xiaolong-liu-bdsc/igrec">https://github.com/xiaolong-liu-bdsc/igrec</a></li>
<li>paper_authors: Xiaolong Liu, Liangwei Yang, Zhiwei Liu, Xiaohan Li, Mingdai Yang, Chen Wang, Philip S. Yu</li>
<li>for: 这篇论文的目的是为了提高个性化推荐系统的精度和效能，利用社交媒体上的用户群体信息来补充推荐系统的数据缺乏和冷启问题。</li>
<li>methods: 这篇论文提出了一个名为IGRec的方法，它包括四个模组：1. 用户情感分解模组，通过自我阈值来将用户的初始嵌入表示分解为不同的情感分量。2. 用户群体汇总模组，通过Gumbel-Softmax汇总方法来将用户群体的情感分量统计为一个共同的群体表示。3. 用户-群体联合模组，将用户的嵌入表示与他们参加的群体表示联合。4. 双排课预测模组，使用用户-项目和群体-项目互动来训练预测模型。</li>
<li>results: 实验结果显示，IGRec可以有效地解决数据缺乏和冷启问题，并且在群体推荐任务上显示出了更高的信息含量。<details>
<summary>Abstract</summary>
Personalized recommender systems aim to predict users' preferences for items. It has become an indispensable part of online services. Online social platforms enable users to form groups based on their common interests. The users' group participation on social platforms reveals their interests and can be utilized as side information to mitigate the data sparsity and cold-start problem in recommender systems. Users join different groups out of different interests. In this paper, we generate group representation from the user's interests and propose IGRec (Interest-based Group enhanced Recommendation) to utilize the group information accurately. It consists of four modules. (1) Interest disentangler via self-gating that disentangles users' interests from their initial embedding representation. (2) Interest aggregator that generates the interest-based group representation by Gumbel-Softmax aggregation on the group members' interests. (3) Interest-based group aggregation that fuses user's representation with the participated group representation. (4) A dual-trained rating prediction module to utilize both user-item and group-item interactions. We conduct extensive experiments on three publicly available datasets. Results show IGRec can effectively alleviate the data sparsity problem and enhance the recommender system with interest-based group representation. Experiments on the group recommendation task further show the informativeness of interest-based group representation.
</details>
<details>
<summary>摘要</summary>
personalized recommender systems aim to predict users' preferences for items. It has become an indispensable part of online services. online social platforms enable users to form groups based on their common interests. The users' group participation on social platforms reveals their interests and can be utilized as side information to mitigate the data sparsity and cold-start problem in recommender systems. users join different groups out of different interests. In this paper, we generate group representation from the user's interests and propose IGRec (Interest-based Group enhanced Recommendation) to utilize the group information accurately. It consists of four modules. (1) Interest disentangler via self-gating that disentangles users' interests from their initial embedding representation. (2) Interest aggregator that generates the interest-based group representation by Gumbel-Softmax aggregation on the group members' interests. (3) Interest-based group aggregation that fuses user's representation with the participated group representation. (4) A dual-trained rating prediction module to utilize both user-item and group-item interactions. We conduct extensive experiments on three publicly available datasets. results show IGRec can effectively alleviate the data sparsity problem and enhance the recommender system with interest-based group representation. experiments on the group recommendation task further show the informativeness of interest-based group representation.Here's the breakdown of the translation:* "personalized recommender systems" becomes "个性化推荐系统" (gèxìnghuà zhìdòng yìxìng zhìdòng)* "aim to predict users' preferences for items" becomes "预测用户对物品的喜好" (yùndào yìzhí yìxìng zhìdòng)* "online social platforms" becomes "在线社交平台" (zài xiàng xìng zhìdòng)* "enable users to form groups based on their common interests" becomes "允许用户根据共同的兴趣组成群体" (shèngxìn yìxìng zhìdòng)* "users join different groups out of different interests" becomes "用户根据不同的兴趣加入不同的群体" (yìxìng zhìdòng zài bùdìng de xìngxìng)* "Interest-based Group enhanced Recommendation" becomes "兴趣基于群体增强推荐" (yìxìng jīyào qúnwù zhìdòng zhìdòng)* "consists of four modules" becomes "包括四个模块" (bāng xīn sì ge móudào)* "Interest disentangler via self-gating" becomes "自我阻塞来消除用户兴趣的混合" (zìwǒ zhìxíng lái xiāoxiǎo yìxìng zhìdòng)* "Interest aggregator" becomes "兴趣聚合器" (yìxìng jùhégōng)* "Interest-based group aggregation" becomes "兴趣基于群体聚合" (yìxìng jīyào qúnwù jùhégōng)* "dual-trained rating prediction module" becomes "双向训练评分模块" (shuāng xiàng xiǎng zhìxíng píngfāng móudào)* "extensive experiments on three publicly available datasets" becomes "在三个公开数据集上进行了广泛的实验" (zài sān gè gōngkāi xìngxìng zhìdòng zhìdòng)* "results show IGRec can effectively alleviate the data sparsity problem" becomes "结果显示 IGRec 可以有效解决数据缺乏问题" (jiéguī xiǎnshì IGRec kěyì yǒu jìngxìng duōshì wèn tí)* "experiments on the group recommendation task further show the informativeness of interest-based group representation" becomes "在群体推荐任务上进行的实验再次表明兴趣基于群体表示的有用性" (zài qúnwù zhìdòng zhìdòng shì de jìngxìng yǐngxìng)
</details></li>
</ul>
<hr>
<h2 id="A-Knowledge-Distillation-Approach-for-Sepsis-Outcome-Prediction-from-Multivariate-Clinical-Time-Series"><a href="#A-Knowledge-Distillation-Approach-for-Sepsis-Outcome-Prediction-from-Multivariate-Clinical-Time-Series" class="headerlink" title="A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series"></a>A Knowledge Distillation Approach for Sepsis Outcome Prediction from Multivariate Clinical Time Series</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09566">http://arxiv.org/abs/2311.09566</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anna Wong, Shu Ge, Nassim Oufattole, Adam Dejl, Megan Su, Ardavan Saeedi, Li-wei H. Lehman</li>
<li>for: 预测 septic 病人结果，学习可读性的状态表示</li>
<li>methods: 使用知识塑化 via 约束变量推断，将师网络模型的知识塑化到学生网络模型中，以实现高预测性和可读性</li>
<li>results: 使用实际数据，在 MIMIC-IV 数据库上训练 LSTM 作为师网络模型，预测 septic 病人死亡率，并使用 AR-HMM 学习可读性的隐藏状态表示，预测多个下游结果，包括医院死亡率、肺液肿、需要药物、透析和机械呼吸等。结果表明，我们的方法可以成功integrate constraint，实现高预测性和可读性。<details>
<summary>Abstract</summary>
Sepsis is a life-threatening condition triggered by an extreme infection response. Our objective is to forecast sepsis patient outcomes using their medical history and treatments, while learning interpretable state representations to assess patients' risks in developing various adverse outcomes. While neural networks excel in outcome prediction, their limited interpretability remains a key issue. In this work, we use knowledge distillation via constrained variational inference to distill the knowledge of a powerful "teacher" neural network model with high predictive power to train a "student" latent variable model to learn interpretable hidden state representations to achieve high predictive performance for sepsis outcome prediction. Using real-world data from the MIMIC-IV database, we trained an LSTM as the "teacher" model to predict mortality for sepsis patients, given information about their recent history of vital signs, lab values and treatments. For our student model, we use an autoregressive hidden Markov model (AR-HMM) to learn interpretable hidden states from patients' clinical time series, and use the posterior distribution of the learned state representations to predict various downstream outcomes, including hospital mortality, pulmonary edema, need for diuretics, dialysis, and mechanical ventilation. Our results show that our approach successfully incorporates the constraint to achieve high predictive power similar to the teacher model, while maintaining the generative performance.
</details>
<details>
<summary>摘要</summary>
伤害是一种生命威胁的疾病，由于感染过程的极端反应而引起。我们的目标是预测患有伤害患者的结果，使用他们的医疗历史和治疗方法，同时学习可读取的状态表示，以评估患者在不同的不良结果发展中的风险。虽然神经网络在结果预测方面表现出色，但它们的解释能力受限。在这种工作中，我们使用知识填充via受限变量推理来填充教师神经网络模型的知识，以训练学生隐藏变量模型，以学习可读取的隐藏状态表示，以实现高度预测性和解释能力。使用实际数据库，我们训练了LSTM作为教师模型，以预测伤害患者的死亡，根据他们的近期生命体征、实验室值和治疗方法的信息。为学生模型，我们使用自适应隐藏马尔可夫模型（AR-HMM）来学习患者的临床时序序列中的可读取隐藏状态，并使用学习的 posterior 分布来预测多个下游结果，包括医院死亡率、肺液肿、需要药物、人工呼吸和肾透析。我们的结果表明，我们的方法可以成功地满足Constraint来实现高度预测力和解释能力，同时保持生成性能。
</details></li>
</ul>
<hr>
<h2 id="Know-Thy-Neighbors-A-Graph-Based-Approach-for-Effective-Sensor-Based-Human-Activity-Recognition-in-Smart-Homes"><a href="#Know-Thy-Neighbors-A-Graph-Based-Approach-for-Effective-Sensor-Based-Human-Activity-Recognition-in-Smart-Homes" class="headerlink" title="Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes"></a>Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based Human Activity Recognition in Smart Homes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09514">http://arxiv.org/abs/2311.09514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srivatsa P, Thomas Plötz</li>
<li>for: 这个研究旨在提高智能家居中的人活动识别（HAR）系统，以扩展智能家居技术和帮助生活技术。</li>
<li>methods: 本研究提出了一个新的图形引导神经网络方法，通过学习感知器之间的明确共识关系，以解决现有HAR系统对于变化、罕见和噪音等问题的限制。</li>
<li>results: 本研究在CASAS数据集上进行了多个实验，结果显示了 graph-guided neural network 在智能家居HAR中的表现，在多个数据集和大幅优势的情况下超越了现有的方法。这些结果显示了 HAR 系统在实际应用中的潜力。<details>
<summary>Abstract</summary>
There has been a resurgence of applications focused on Human Activity Recognition (HAR) in smart homes, especially in the field of ambient intelligence and assisted living technologies. However, such applications present numerous significant challenges to any automated analysis system operating in the real world, such as variability, sparsity, and noise in sensor measurements. Although state-of-the-art HAR systems have made considerable strides in addressing some of these challenges, they especially suffer from a practical limitation: they require successful pre-segmentation of continuous sensor data streams before automated recognition, i.e., they assume that an oracle is present during deployment, which is capable of identifying time windows of interest across discrete sensor events. To overcome this limitation, we propose a novel graph-guided neural network approach that performs activity recognition by learning explicit co-firing relationships between sensors. We accomplish this by learning a more expressive graph structure representing the sensor network in a smart home, in a data-driven manner. Our approach maps discrete input sensor measurements to a feature space through the application of attention mechanisms and hierarchical pooling of node embeddings. We demonstrate the effectiveness of our proposed approach by conducting several experiments on CASAS datasets, showing that the resulting graph-guided neural network outperforms the state-of-the-art method for HAR in smart homes across multiple datasets and by large margins. These results are promising because they push HAR for smart homes closer to real-world applications.
</details>
<details>
<summary>摘要</summary>
随着智能家居技术的发展，人活动识别（HAR）应用也在智能家居领域得到了新的推动。特别是在 ambient intelligence 和助生技术领域，HAR 应用已成为当前研究热点。然而，实际世界中的 HAR 系统面临着许多挑战，如感知器的变化、缺失和噪声等问题。尽管现有的 HAR 系统已经做出了很大的进步，但它们尤其受到一种实际限制：它们需要成功地预分 segments 持续的感知数据流，以便自动识别活动。为了突破这一限制，我们提出了一种新的图导型神经网络方法，该方法通过学习感知器之间的显式协同关系来进行活动识别。我们通过在数据驱动方式下学习更加表达式的图结构，将感知网络在智能家居中映射到具有表达能力的特征空间。我们的方法通过注意机制和层次聚合节点嵌入来将离散输入感知测量映射到特征空间。我们在 CASAS 数据集上进行了多个实验，并证明了我们提出的图导型神经网络方法在智能家居中 HAR 方面的效果较为出色，与当前状态艺技术相比，差距较大。这些结果是推动 HAR 在智能家居中的实际应用的好消息。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Systems-with-Symmetries-using-Equivariant-Autoregressive-Reservoir-Computers"><a href="#Identifying-Systems-with-Symmetries-using-Equivariant-Autoregressive-Reservoir-Computers" class="headerlink" title="Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers"></a>Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09511">http://arxiv.org/abs/2311.09511</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fredy Vides, Idelfonso B. R. Nogueira, Lendy Banegas, Evelyn Flores</li>
<li>for: 本文使用非线性时阶 embedding 技术来识别具有对称性的系统。</li>
<li>methods: 文中提出了一种双重方法，包括对时间序列数据进行非对称时阶嵌入，并使用稀疏最小二乘方法来描述输出联系矩阵。</li>
<li>results: 文中结果表明，使用这些技术可以有效地识别和预测具有对称性的非线性系统，无论这些系统是否 exhibits 乱流行为。<details>
<summary>Abstract</summary>
The investigation reported in this document focuses on identifying systems with symmetries using equivariant autoregressive reservoir computers. General results in structured matrix approximation theory are presented, exploring a two-fold approach. Firstly, a comprehensive examination of generic symmetry-preserving nonlinear time delay embedding is conducted. This involves analyzing time series data sampled from an equivariant system under study. Secondly, sparse least-squares methods are applied to discern approximate representations of the output coupling matrices. These matrices play a pivotal role in determining the nonlinear autoregressive representation of an equivariant system. The structural characteristics of these matrices are dictated by the set of symmetries inherent in the system. The document outlines prototypical algorithms derived from the described techniques, offering insight into their practical applications. Emphasis is placed on their effectiveness in the identification and predictive simulation of equivariant nonlinear systems, regardless of whether such systems exhibit chaotic behavior.
</details>
<details>
<summary>摘要</summary>
这份报告的调查集中关注使用对称 Autoregressive 计算机系统来识别具有对称性的系统。报告提供了一般结果，探讨了两种方法：首先，对具有对称性的非线性时间延迟嵌入进行全面的分析，这是通过分析研究中的对称系统时间序列数据来实现的。其次，使用稀疏最小二乘方法来突出输出联系矩阵的 Approximate 表示。这些矩阵在确定非线性 Autoregressive 表示中扮演重要的角色，其结构特征由系统中的对称性决定。报告描述了基于这些技术的评估算法，并强调其在识别和预测非线性系统中的有效性，无论这些系统是否展现混沌行为。
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Impact-of-Weight-Sharing-Decisions-on-Knowledge-Transfer-in-Continual-Learning"><a href="#Investigating-the-Impact-of-Weight-Sharing-Decisions-on-Knowledge-Transfer-in-Continual-Learning" class="headerlink" title="Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning"></a>Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09506">http://arxiv.org/abs/2311.09506</a></li>
<li>repo_url: None</li>
<li>paper_authors: Josh Andle, Ali Payani, Salimeh Yasaei-Sekeh</li>
<li>for: 本研究旨在 investigate 如何在 Continual Learning (CL) 中进行 Forward Knowledge Transfer (FKT) between tasks, 以便提高 neural network 的效率和适应性。</li>
<li>methods: 本研究使用 pruning methods 来训练 CL 的 subnetworks, 并通过 sharing prior subnetworks’ weights 来实现 FKT。</li>
<li>results: 研究发现，在不同任务的复杂度和相似性的情况下，有优化的 weight sharing 决策可以提高任务的准确率。 通过遵循这些决策，我们可以在 CL 中提高任务的性能。<details>
<summary>Abstract</summary>
Continual Learning (CL) has generated attention as a method of avoiding Catastrophic Forgetting (CF) in the sequential training of neural networks, improving network efficiency and adaptability to different tasks. Additionally, CL serves as an ideal setting for studying network behavior and Forward Knowledge Transfer (FKT) between tasks. Pruning methods for CL train subnetworks to handle the sequential tasks which allows us to take a structured approach to investigating FKT. Sharing prior subnetworks' weights leverages past knowledge for the current task through FKT. Understanding which weights to share is important as sharing all weights can yield sub-optimal accuracy. This paper investigates how different sharing decisions affect the FKT between tasks. Through this lens we demonstrate how task complexity and similarity influence the optimal weight sharing decisions, giving insights into the relationships between tasks and helping inform decision making in similar CL methods. We implement three sequential datasets designed to emphasize variation in task complexity and similarity, reporting results for both ResNet-18 and VGG-16. By sharing in accordance with the decisions supported by our findings, we show that we can improve task accuracy compared to other sharing decisions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Network-Wide-Evacuation-Traffic-Prediction-in-a-Rapidly-Intensifying-Hurricane-from-Traffic-Detectors-and-Facebook-Movement-Data-A-Deep-Learning-Approach"><a href="#Network-Wide-Evacuation-Traffic-Prediction-in-a-Rapidly-Intensifying-Hurricane-from-Traffic-Detectors-and-Facebook-Movement-Data-A-Deep-Learning-Approach" class="headerlink" title="Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning Approach"></a>Network Wide Evacuation Traffic Prediction in a Rapidly Intensifying Hurricane from Traffic Detectors and Facebook Movement Data: A Deep Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09498">http://arxiv.org/abs/2311.09498</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Mobasshir Rashid, Rezaur Rahman, Samiul Hasan</li>
<li>for: 预测风暴撤离时的交通流量是重要的，可以帮助优化交通基建的使用，从而减少撤离时间。</li>
<li>methods: 该研究使用了交通探测器和Facebook移动数据，开发了一种基于深度学习的交通预测模型，以便预测风暴撤离期间的交通流量。</li>
<li>results: 模型在常规时间段（5月-8月）的数据上进行了训练，并在风暴撤离期间使用test数据进行预测，其中Accuracy为95%（RMSE &#x3D; 356），但在风暴撤离期间，模型表现不佳，Accuracy为55%（RMSE &#x3D; 1084）。然后，研究人员采用了传输学习方法，使用预训练的模型和更多的撤离相关特征进行预测，最终模型的Accuracy提高至89%（RMSE &#x3D; 514）。再次添加Facebook移动数据，模型的RMSE值降至393，并提高了Accuracy至93%。<details>
<summary>Abstract</summary>
Traffic prediction during hurricane evacuation is essential for optimizing the use of transportation infrastructures. It can reduce evacuation time by providing information on future congestion in advance. However, evacuation traffic prediction can be challenging as evacuation traffic patterns is significantly different than regular period traffic. A data-driven traffic prediction model is developed in this study by utilizing traffic detector and Facebook movement data during Hurricane Ian, a rapidly intensifying hurricane. We select 766 traffic detectors from Florida's 4 major interstates to collect traffic features. Additionally, we use Facebook movement data collected during Hurricane Ian's evacuation period. The deep-learning model is first trained on regular period (May-August 2022) data to understand regular traffic patterns and then Hurricane Ian's evacuation period data is used as test data. The model achieves 95% accuracy (RMSE = 356) during regular period, but it underperforms with 55% accuracy (RMSE = 1084) during the evacuation period. Then, a transfer learning approach is adopted where a pretrained model is used with additional evacuation related features to predict evacuation period traffic. After transfer learning, the model achieves 89% accuracy (RMSE = 514). Adding Facebook movement data further reduces model's RMSE value to 393 and increases accuracy to 93%. The proposed model is capable to forecast traffic up to 6-hours in advance. Evacuation traffic management officials can use the developed traffic prediction model to anticipate future traffic congestion in advance and take proactive measures to reduce delays during evacuation.
</details>
<details>
<summary>摘要</summary>
预测风暴撤离交通是至关重要的，以便优化交通基础设施的使用。它可以降低撤离时间，通过提供未来堵塞的信息。然而，撤离交通预测可能是挑战，因为撤离交通模式与常规时间交通模式有所不同。本研究中提出了一种基于数据驱动的交通预测模型，通过利用交通检测器和Facebook运动数据进行风暴伊安的撤离期间预测。我们选择了766个交通检测器，分别位于佛罗里达州的4大高速公路。此外，我们还使用了在风暴伊安撤离期间收集的Facebook运动数据。深度学习模型首先在常规时间（5月-8月2022年）的数据上训练，以理解常规交通模式，然后使用风暴伊安撤离期间的数据进行测试。模型在常规时间上达到95%的准确率（RMSE=356），但在撤离期间表现不佳，准确率为55%（RMSE=1084）。然后，我们采用了传输学习方法，使用预训练的模型，并添加了撤离相关特征。经过传输学习，模型的准确率提高到89%（RMSE=514）。再次添加Facebook运动数据，模型的RMSE值降低至393，准确率提高到93%。该模型可以预测交通情况，并且可以预测交通情况到6小时之前。风暴撤离管理官员可以使用该模型预测未来交通堵塞，并采取积极措施，以降低撤离过程中的延迟。
</details></li>
</ul>
<hr>
<h2 id="Spatial-Bayesian-Neural-Networks"><a href="#Spatial-Bayesian-Neural-Networks" class="headerlink" title="Spatial Bayesian Neural Networks"></a>Spatial Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09491">http://arxiv.org/abs/2311.09491</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/andrewzm/sbnn">https://github.com/andrewzm/sbnn</a></li>
<li>paper_authors: Andrew Zammit-Mangion, Michael D. Kaminski, Ba-Hien Tran, Maurizio Filippone, Noel Cressie</li>
<li>for: 这个论文的目的是提出一种新的空间过程模型，即空间 bayesian neural network (SBNN)，用于更好地描述空间数据中的不同过程。</li>
<li>methods: 该论文使用了一种新的方法，即在 Bayesian neural network 中添加空间嵌入层，以适应空间环境。此外，论文还提出了一些variants of SBNNs，以及如何使用这些模型来表示各种常见的空间过程。</li>
<li>results: 论文的结果表明，SBNNs 可以比传统的 Bayesian neural network 更好地描述空间数据中的不同过程，并且可以用于表示各种常见的空间过程。此外，论文还提出了一些新的工具来进行 SBNNs 的推断。<details>
<summary>Abstract</summary>
Statistical models for spatial processes play a central role in statistical analyses of spatial data. Yet, it is the simple, interpretable, and well understood models that are routinely employed even though, as is revealed through prior and posterior predictive checks, these can poorly characterise the spatial heterogeneity in the underlying process of interest. Here, we propose a new, flexible class of spatial-process models, which we refer to as spatial Bayesian neural networks (SBNNs). An SBNN leverages the representational capacity of a Bayesian neural network; it is tailored to a spatial setting by incorporating a spatial "embedding layer" into the network and, possibly, spatially-varying network parameters. An SBNN is calibrated by matching its finite-dimensional distribution at locations on a fine gridding of space to that of a target process of interest. That process could be easy to simulate from or we have many realisations from it. We propose several variants of SBNNs, most of which are able to match the finite-dimensional distribution of the target process at the selected grid better than conventional BNNs of similar complexity. We also show that a single SBNN can be used to represent a variety of spatial processes often used in practice, such as Gaussian processes and lognormal processes. We briefly discuss the tools that could be used to make inference with SBNNs, and we conclude with a discussion of their advantages and limitations.
</details>
<details>
<summary>摘要</summary>
统计模型 для空间过程扮演了中心角色于统计分析空间数据中。然而，即使是简单、可解释、具有良好了解的模型仍然广泛使用，尽管，通过先前和后预测检查，这些模型可能不能准确捕捉空间过程中的差异性。我们提出了一种新的、灵活的空间过程模型，称为空间 bayesian neural network（SBNN）。一个 SBNN 利用了 bayesian neural network 的表达能力，并在网络中添加了空间“嵌入层”，以适应空间设置。一个 SBNN 通过匹配其在细网格上的finite-dimensional分布与目标过程的分布来调整。该过程可以是容易从 simulate 出来的或者我们有很多实例来源。我们提出了一些 SBNN 的变体，大多数可以在相同的复杂性水平上比 conventional BNNs 更好地匹配目标过程的finite-dimensional分布。我们还表明了一个 SBNN 可以用来表示一些常用的空间过程，如 Gaussian 过程和 lognormal 过程。我们 briefly 讨论了使用 SBNNs 进行推理的工具，并结束于一个关于其优点和局限性的讨论。
</details></li>
</ul>
<hr>
<h2 id="Soft-Matching-Distance-A-metric-on-neural-representations-that-captures-single-neuron-tuning"><a href="#Soft-Matching-Distance-A-metric-on-neural-representations-that-captures-single-neuron-tuning" class="headerlink" title="Soft Matching Distance: A metric on neural representations that captures single-neuron tuning"></a>Soft Matching Distance: A metric on neural representations that captures single-neuron tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09466">http://arxiv.org/abs/2311.09466</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meenakshi Khosla, Alex H. Williams</li>
<li>for:  This paper aims to develop a stricter notion of representational (dis)similarity that requires individual neuron matching across networks, and to generalize this metric to compare networks with different sizes.</li>
<li>methods: The paper uses optimal transport theory to derive a natural generalization of the distance metric based on “soft” permutations, which is symmetric, satisfies the triangle inequality, and can be interpreted as a Wasserstein distance between two empirical distributions.</li>
<li>results: The proposed metric avoids counter-intuitive outcomes suffered by alternative approaches and captures complementary geometric insights into neural representations that are entirely missed by rotation-invariant metrics.<details>
<summary>Abstract</summary>
Common measures of neural representational (dis)similarity are designed to be insensitive to rotations and reflections of the neural activation space. Motivated by the premise that the tuning of individual units may be important, there has been recent interest in developing stricter notions of representational (dis)similarity that require neurons to be individually matched across networks. When two networks have the same size (i.e. same number of neurons), a distance metric can be formulated by optimizing over neuron index permutations to maximize tuning curve alignment. However, it is not clear how to generalize this metric to measure distances between networks with different sizes. Here, we leverage a connection to optimal transport theory to derive a natural generalization based on "soft" permutations. The resulting metric is symmetric, satisfies the triangle inequality, and can be interpreted as a Wasserstein distance between two empirical distributions. Further, our proposed metric avoids counter-intuitive outcomes suffered by alternative approaches, and captures complementary geometric insights into neural representations that are entirely missed by rotation-invariant metrics.
</details>
<details>
<summary>摘要</summary>
通用的神经表示（不）相似性度量是设计为感知到旋转和反射的神经活动空间的变换。驱动于各个单元的调音可能是重要的，有些时候有关注于开发更严格的神经表示（不）相似性度量，需要神经网络中的单元在不同网络中匹配。当两个网络有相同的大小（即同样多个单元）时，可以通过最大化神经单元索引Permutation来形式化距离度量。但是，不清楚如何推广这个度量来度量不同大小的网络之间的距离。我们利用了优化运输理论的连接， derivate一个自然的推广，基于"软" Permutation。这个度量是对称的，满足三角不等式，可以被解释为两个empirical分布之间的沃asserstein距离。此外，我们提出的度量可以避免其他方法所导致的不合理的结果，同时捕捉神经表示中完全被旋转不变度量所遗弃的几何视角。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.LG_2023_11_16/" data-id="clp89doig00v7i78801o4bgf8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/eess.IV_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T09:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/eess.IV_2023_11_16/">eess.IV - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Investigating-the-Use-of-Traveltime-and-Reflection-Tomography-for-Deep-Learning-Based-Sound-Speed-Estimation-in-Ultrasound-Computed-Tomography"><a href="#Investigating-the-Use-of-Traveltime-and-Reflection-Tomography-for-Deep-Learning-Based-Sound-Speed-Estimation-in-Ultrasound-Computed-Tomography" class="headerlink" title="Investigating the Use of Traveltime and Reflection Tomography for Deep Learning-Based Sound-Speed Estimation in Ultrasound Computed Tomography"></a>Investigating the Use of Traveltime and Reflection Tomography for Deep Learning-Based Sound-Speed Estimation in Ultrasound Computed Tomography</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10193">http://arxiv.org/abs/2311.10193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gangwon Jeong, Fu Li, Umberto Villa, Mark A. Anastasio</li>
<li>for: 这个研究的目的是为了使用深度学习方法来提高ultrasound computed tomography（USCT）中的速度声速（SOS）的重建精度，并且 investigate the impact of chosen input modalities on image-to-image learned reconstruction（IILR）方法。</li>
<li>methods: 这个研究使用了一种名为convolutional neural network（CNN）的深度学习模型，该模型可以将双通道（TT和RT图像）转换为高分辨率的SOS地图。此外，研究还使用了一种权重重建损失函数，以便在训练过程中增强特征区域的检测。</li>
<li>results: 研究发现，使用双通道输入可以提高IILR方法的重建精度和特征区域的检测性能。单通道输入（TT或RT图像）alone的情况下，重建精度和特征区域的检测性能均较差。<details>
<summary>Abstract</summary>
Ultrasound computed tomography (USCT) is actively being developed to quantify acoustic tissue properties such as the speed-of-sound (SOS). Although full-waveform inversion (FWI) is an effective method for accurate SOS reconstruction, it can be computationally challenging for large-scale problems. Deep learning-based image-to-image learned reconstruction (IILR) methods are being investigated as scalable and computationally efficient alternatives. This study investigates the impact of the chosen input modalities on IILR methods for high-resolution SOS reconstruction in USCT. The selected modalities are traveltime tomography (TT) and reflection tomography (RT), which produce a low-resolution SOS map and a reflectivity map, respectively. These modalities have been chosen for their lower computational cost relative to FWI and their capacity to provide complementary information: TT offers a direct -- while low resolution -- SOS measure, while RT reveals tissue boundary information. Systematic analyses were facilitated by employing a stylized USCT imaging system with anatomically realistic numerical breast phantoms. Within this testbed, a supervised convolutional neural network (CNN) was trained to map dual-channel (TT and RT images) to a high-resolution SOS map. Moreover, the CNN was fine-tuned using a weighted reconstruction loss that prioritized tumor regions to address tumor underrepresentation in the training dataset. To understand the benefits of employing dual-channel inputs, single-input CNNs were trained separately using inputs from each modality alone (TT or RT). The methods were assessed quantitatively using normalized root mean squared error and structural similarity index measure for reconstruction accuracy and receiver operating characteristic analysis to assess signal detection-based performance measures.
</details>
<details>
<summary>摘要</summary>
美国计算 Tomography (USCT) 目前在发展中，以量化声学组织特性，如声速 (SOS) 为目标。虽然全波形反射 (FWI) 是一种高精度的 SOS 重建方法，但可能会对大规模问题具有计算挑战。深度学习基于图像到图像学习的方法被调查为可扩展和计算高效的替代方案。本研究研究了选择的输入模式对 IILR 方法的高分辨率 SOS 重建影响。选择的模式包括旅游时间 Tomography (TT) 和反射 Tomography (RT)，它们生成了低分辨率 SOS 地图和反射图像，分别。这些模式选择的原因是它们的计算成本较低，并且可以提供补充信息：TT 提供了直接 -- 低分辨率 -- SOS 测量，而 RT 揭示了组织边界信息。在使用静态 USCT 图像系统和数字胸部phantom进行系统性分析的测试环境中，一个以图像为输入的 convolutional neural network (CNN) 被训练来将双通道 (TT 和 RT 图像) 映射到高分辨率 SOS 地图。此外，CNN 还被微调使用一个权重重建损失函数，以优先级刻意诊断区域，以 Addressing tumor underrepresentation in the training dataset。为了了解使用双通道输入的好处，单通道 CNNs 分别使用每个模式的输入图像来训练（TT 或 RT）。这些方法被评估量化使用 normalized root mean squared error 和 structure similarity index measure 来评估重建精度和 Receiver operating characteristic analysis 来评估基于信号检测的性能指标。
</details></li>
</ul>
<hr>
<h2 id="Combined-Channel-and-Spatial-Attention-based-Stereo-Endoscopic-Image-Super-Resolution"><a href="#Combined-Channel-and-Spatial-Attention-based-Stereo-Endoscopic-Image-Super-Resolution" class="headerlink" title="Combined Channel and Spatial Attention-based Stereo Endoscopic Image Super-Resolution"></a>Combined Channel and Spatial Attention-based Stereo Endoscopic Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10115">http://arxiv.org/abs/2311.10115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mansoor Hayat, Supavadee Armvith, Dr. Titipat Achakulvisut</li>
<li>for: 这篇论文旨在推广endoscopic imaging技术的应用在医学诊断和手术领域，提高医生和 Physician 对病人器官的解剖知识。</li>
<li>methods: 本文提出了一种基于混合通道和空间注意力块的特征提取方法，并结合了一种特有但非常强的parallax attention模块（PAM），用于endoscopic图像超分辨。</li>
<li>results: 根据da Vinci数据集的训练，提出的模型可以提高PSNR值达2.12 dB（比较2）和1.29 dB（比较4），同时SSIM值也提高了0.03（比较2）和0.0008（比较4）。这种方法可以帮助医生和Physician更准确地诊断和治疗endoscopic图像。<details>
<summary>Abstract</summary>
Stereo Imaging technology integration into medical diagnostics and surgeries brings a great revolution in the field of medical sciences. Now, surgeons and physicians have better insight into the anatomy of patients' organs. Like other technologies, stereo cameras have limitations, e.g., low resolution (LR) and blurry output images. Currently, most of the proposed techniques for super-resolution focus on developing complex blocks and complicated loss functions, which cause high system complexity. We proposed a combined channel and spatial attention block to extract features incorporated with a specific but very strong parallax attention module (PAM) for endoscopic image super-resolution. The proposed model is trained using the da Vinci dataset on scales 2 and 4. Our proposed model has improved PSNR up to 2.12 dB for scale 2 and 1.29 dB for scale 4, while SSIM is improved by 0.03 for scale 2 and 0.0008 for scale 4. By incorporating this method, diagnosis and treatment for endoscopic images can be more accurate and effective.
</details>
<details>
<summary>摘要</summary>
单声图像技术在医疗诊断和手术中得到了很大的革命，为医疗科学带来了更好的顾问。现在医生和医生都可以更好地了解患者的器官 анато�。然而，单声摄像头也有其限制，例如低分辨率（LR）和模糊的输出图像。现在大多数提议的超解析技术都是建立复杂的封包和复杂的损失函数，这会导致高系统复杂性。我们提出了一个结合通道和空间注意力块的特殊专注模组（PAM），用于检测照片超解析。我们的提议模型在 scales 2 和 4 上训练，实现了 PSNR 的提升至 2.12 dB 和 1.29 dB，而 SSIM 则提高了 0.03 和 0.0008。通过这种方法，医疗诊断和治疗可以更精准和有效。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/eess.IV_2023_11_16/" data-id="clp89dop601d2i7886ydm6cyx" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/eess.SP_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T08:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/eess.SP_2023_11_16/">eess.SP - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Near-Field-Velocity-Sensing-and-Predictive-Beamforming"><a href="#Near-Field-Velocity-Sensing-and-Predictive-Beamforming" class="headerlink" title="Near-Field Velocity Sensing and Predictive Beamforming"></a>Near-Field Velocity Sensing and Predictive Beamforming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09888">http://arxiv.org/abs/2311.09888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaolin Wang, Xidong Mu, Yuanwei Liu</li>
<li>for: 提出了一种新的近场速度测量概念，可以同时测量目标在运动过程中的径向和横向速度。</li>
<li>methods: 提出了基于最大可能性估计的方法，用于同时估计径向和横向速度的echo信号。</li>
<li>results: 通过帮助近场速度测量，提出了一种无需频率估计的预测扩散框架，实现了无间断的数据传输。数值示例验证了该方法的有效性。<details>
<summary>Abstract</summary>
The novel concept of near-field velocity sensing is proposed. In contrast to far-field velocity sensing, near-field velocity sensing enables the simultaneous estimation of both radial and transverse velocities of a moving target. A maximum-likelihood-based method is proposed for jointly estimating the radial and transverse velocities from the echo signals. Assisted by near-field velocity sensing, a predictive beamforming framework is proposed for a moving communication user, which requires no channel estimation but achieves seamless data transmission. Finally, numerical examples validate the proposed approaches.
</details>
<details>
<summary>摘要</summary>
新的概念——近场速度测量被提出。与远场速度测量相比，近场速度测量可同时测量移动目标的径向和横向速度。基于最大可能性的方法被提议用于同时估计径向和横向速度的echo信号。帮助了近场速度测量，一种预测扩散框架被提议用于移动通信用户，不需 Channel estimation，但可实现无缝数据传输。最后，数值示例证明了提出的方法。Here's the word-for-word translation:新的概念——近场速度测量被提出，与远场速度测量相比，近场速度测量可同时测量移动目标的径向和横向速度。基于最大可能性的方法被提议用于同时估计径向和横向速度的echo信号。帮助了近场速度测量，一种预测扩散框架被提议用于移动通信用户，不需 Channel estimation，但可实现无缝数据传输。最后，数值示例证明了提出的方法。
</details></li>
</ul>
<hr>
<h2 id="Wireless-rf-sensor-with-dual-sensing-capability-for-ionic-solution-and-target-dielectric-objects"><a href="#Wireless-rf-sensor-with-dual-sensing-capability-for-ionic-solution-and-target-dielectric-objects" class="headerlink" title="Wireless rf sensor with dual sensing capability for ionic solution and target dielectric objects"></a>Wireless rf sensor with dual sensing capability for ionic solution and target dielectric objects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09876">http://arxiv.org/abs/2311.09876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sobhan Gholami, Emre Unal, Hilmi Volkan Demir</li>
<li>for: 用于检测水中离子含量和固体杂质物的变化</li>
<li>methods: 使用微型陷阱板设计，可以在透明容器外壁安装，并根据容器材料进行定制，以实现无线感知</li>
<li>results: 具有唯一的设计，使其不受周围环境影响<details>
<summary>Abstract</summary>
A novel microstrip-based sensor designed for detecting changes in ionic content of water and the addition of solid contaminant objects is presented. The sensor can be installed on the exterior wall of dielectric containers and customized according to the material of the container to enable wireless sensing. It's operation within the lower microwave frequency range (670 to 730 MHz) serves to minimize signal attenuation in water and streamlines circuitry design. The most significant feature of this sensor is its unique design, rendering it impervious to its surrounding environment.
</details>
<details>
<summary>摘要</summary>
一种新型微带式感测器，用于检测水中离子含量的变化以及固体杂质物的添加，被介绍。该感测器可以安装在dielectric容器外墙上，并可以根据容器材料进行个性化定制，以实现无线感测。它的运作频率范围为670-730MHz，以便在水中减少信号强度抑制，同时简化电路设计。该感测器的最重要特点是它独特的设计，使其不受周围环境影响。Here's the breakdown of the translation:* 一种新型微带式感测器 (a new type of microstrip-based sensor)* 用于检测水中离子含量的变化 (for detecting changes in ionic content of water)* 以及固体杂质物的添加 (and the addition of solid contaminant objects)* 被介绍 (being introduced)* 该感测器可以安装在dielectric容器外墙上 (the sensor can be installed on the exterior wall of dielectric containers)* 并可以根据容器材料进行个性化定制 (and can be customized according to the material of the container)* 以实现无线感测 (to achieve wireless sensing)* 它的运作频率范围为670-730MHz (its operating frequency range is 670-730MHz)* 以便在水中减少信号强度抑制 (to reduce signal attenuation in water)* 同时简化电路设计 (while simplifying circuitry design)* 该感测器的最重要特点是它独特的设计 (the most important feature of the sensor is its unique design)* 使其不受周围环境影响 (so that it is not affected by the surrounding environment)
</details></li>
</ul>
<hr>
<h2 id="Integrated-lithium-niobate-photonic-millimeter-wave-radar"><a href="#Integrated-lithium-niobate-photonic-millimeter-wave-radar" class="headerlink" title="Integrated lithium niobate photonic millimeter-wave radar"></a>Integrated lithium niobate photonic millimeter-wave radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09857">http://arxiv.org/abs/2311.09857</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sha Zhu, Yiwen Zhang, Jiaxue Feng, Yongji Wang, Kunpeng Zhai, Hanke Feng, Edwin Yue Bun Pun, Ning Hua Zhu, Cheng Wang</li>
<li>for: This paper presents a centimeter-resolution integrated photonic radar operating in the mmWave V band (40-50 GHz) for high-resolution sensing and detection of targets.</li>
<li>methods: The paper uses a 4-inch wafer-scale thin-film lithium niobate (TFLN) technology to overcome the limitations of electronic radars and achieve a broadband linear frequency modulated mmWave radar waveform through optical frequency multiplication of a low-frequency input signal.</li>
<li>results: The paper achieves multi-target ranging with a resolution of 1.50 cm and velocity measurement with a resolution of 0.067 m&#x2F;s, as well as imaging of targets with various shapes and postures with a two-dimensional resolution of 1.50 cm * 1.06 cm.Here’s the Chinese version:</li>
<li>for: 这篇论文介绍了一种可以实现中心分辨率为1.50cm的集成光学雷达系统，用于高分辨率探测和检测目标。</li>
<li>methods: 这篇论文使用4英寸芯片级别的聚辉锆铌镧(TFLN)技术，以超越电子雷达的限制，实现广频线性频率变谱mm波雷达波形，通过光学频率 multiplication的方式实现低频输入信号的广频变谱。</li>
<li>results: 这篇论文实现了多个目标距离测量，分辨率为1.50cm，以及测速度测量，分辨率为0.067m&#x2F;s，同时还成功构建了反 synthetic aperture radar(ISAR)，并成功图像多种形状和姿态的目标，图像分辨率为1.50cm*1.06cm。<details>
<summary>Abstract</summary>
Millimeter-wave (mmWave,>30 GHz) radars are the key enabler in the coming 6G era for high-resolution sensing and detection of targets. Photonic radar provides an effective approach to overcome the limitations of electronic radars thanks to the high frequency, broad bandwidth, and excellent reconfigurability of photonic systems. However, conventional photonic radars are mostly realized in tabletop systems composed of bulky discrete components, whereas the more compact integrated photonic radars are difficult to reach the mmWave bands due to the unsatisfactory bandwidths and signal integrity of the underlining electro-optic modulators. Here, we overcome these challenges and demonstrate a centimeter-resolution integrated photonic radar operating in the mmWave V band (40-50 GHz) based on a 4-inch wafer-scale thin-film lithium niobate (TFLN) technology. The fabricated TFLN mmWave photonic integrated circuit consists of a first electro-optic modulator capable of generating a broadband linear frequency modulated mmWave radar waveform through optical frequency multiplication of a low-frequency input signal, and a second electro-optic modulator responsible for frequency de-chirp of the received reflected echo wave, therefore greatly relieving the bandwidth requirements for the analog-to-digital converter in the receiver. Thanks to the absence of optical and electrical filters in the system, our integrated photonic mmWave radar features continuous on-demand tunability of the center frequency and bandwidth, currently only limited by the bandwidths of electrical amplifiers. We achieve multi-target ranging with a resolution of 1.50 cm and velocity measurement with a resolution of 0.067 m/s. Furthermore, we construct an inverse synthetic aperture radar (ISAR) and successfully demonstrate the imaging of targets with various shapes and postures with a two-dimensional resolution of 1.50 cm * 1.06 cm.
</details>
<details>
<summary>摘要</summary>
millimeter wave (mmWave,>30 GHz) 雷达是 sixth generation (6G) 时代的关键能力，具有高分辨率探测和检测目标的能力。光子雷达技术提供了一种有效的方法来超越电子雷达的限制，因为光子系统具有高频率、广频带宽和优秀的可重新配置性。然而，传统的光子雷达通常是由多个粗糙的独立部件组成的桌面系统，而更 компакт的集成光子雷达具有 mmWave 频率带的差异和信号完整性问题。在这里，我们解决了这些挑战，并实现了基于 4 英寸芯片级薄膜锂铝铌 (TFLN) 技术的中心 Resolution 集成光子 mmWave 雷达，operating in the mmWave V band (40-50 GHz)。制造的 TFLN mmWave 光子集成电路包括一个能够生成广频线性修改 mmWave 雷达波形的第一个电 Optic modulator，以及一个负责接收反射回波的第二个电 Optic modulator。通过光子频率 multiplication 的低频输入信号，该系统实现了广频修改和频率去抖，从而大大减轻接收器 Analog-to-digital Converter 的频率要求。由于系统中缺乏光学和电子过滤器，我们的集成光子 mmWave 雷达具有无间断的受 demand 调试中心频率和带宽，当前只受电子增强器的带宽限制。我们实现了多个目标的距离测量，其中最高分辨率为 1.50 cm，以及速度测量的分辨率为 0.067 m/s。此外，我们还构建了一个反 Synthetic Aperture Radar (ISAR)，并成功地实现了目标的二维图像测量，其分辨率为 1.50 cm * 1.06 cm。
</details></li>
</ul>
<hr>
<h2 id="Semantic-Relay-Aided-Text-Transmission-Placement-Optimization-and-Bandwidth-Allocation"><a href="#Semantic-Relay-Aided-Text-Transmission-Placement-Optimization-and-Bandwidth-Allocation" class="headerlink" title="Semantic-Relay-Aided Text Transmission: Placement Optimization and Bandwidth Allocation"></a>Semantic-Relay-Aided Text Transmission: Placement Optimization and Bandwidth Allocation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09850">http://arxiv.org/abs/2311.09850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyu Liu, Changsheng You, Zeyang Hu, Chenyu Wu, Yi Gong, Kaibin Huang</li>
<li>for: 提高移动设备中的文本传输效率（efficient text transmission in mobile devices）</li>
<li>methods: 使用semantic relay（SemRelay）和semantic transmitter（SemTransmitter），jointly designing SemRelay placement和bandwidth allocation</li>
<li>results: 提高文本传输效率（improved text transmission efficiency），比对 conventinal decode-and-forward relay（CF Relay）有较高的效果（better performance than conventional CF Relay）<details>
<summary>Abstract</summary>
Semantic communication has emerged as a promising technology to break the Shannon limit by extracting the meaning of source data and sending relevant semantic information only. However, some mobile devices may have limited computation and storage resources, which renders it difficult to deploy and implement the resource-demanding deep learning based semantic encoder/decoder. To tackle this challenge, we propose in this paper a new semantic relay (SemRelay), which is equipped with a semantic receiver for assisting text transmission from a resource-abundant base station (BS) to a resource-constrained mobile device. Specifically, the SemRelay first decodes the semantic information sent by the BS (with a semantic transmitter) and then forwards it to the user by adopting conventional bit transmission, hence effectively improving the text transmission efficiency. We formulate an optimization problem to maximize the achievable (effective) bit rate by jointly designing the SemRelay placement and bandwidth allocation. Although this problem is non-convex and generally difficult to solve, we propose an efficient penalty-based algorithm to obtain a high-quality suboptimal solution. Numerical results show the close-to-optimal performance of the proposed algorithm as well as significant rate performance gain of the proposed SemRelay over conventional decode-and-forward relay.
</details>
<details>
<summary>摘要</summary>
The SemRelay decodes the semantic information sent by the BS (with a semantic transmitter) and then forwards it to the user using conventional bit transmission, thereby improving text transmission efficiency. We formulate an optimization problem to maximize the achievable (effective) bit rate by jointly designing the SemRelay placement and bandwidth allocation. Although this problem is non-convex and difficult to solve, we propose an efficient penalty-based algorithm to obtain a high-quality suboptimal solution.Numerical results show that the proposed algorithm achieves close-to-optimal performance and offers significant rate performance gains compared to conventional decode-and-forward relay.
</details></li>
</ul>
<hr>
<h2 id="Stacked-Intelligent-Metasurface-Aided-MIMO-Transceiver-Design"><a href="#Stacked-Intelligent-Metasurface-Aided-MIMO-Transceiver-Design" class="headerlink" title="Stacked Intelligent Metasurface-Aided MIMO Transceiver Design"></a>Stacked Intelligent Metasurface-Aided MIMO Transceiver Design</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09814">http://arxiv.org/abs/2311.09814</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiancheng An, Chau Yuen, Chao Xu, Hongbin Li, Derrick Wing Kwan Ng, Marco Di Renzo, Mérouane Debbah, Lajos Hanzo</li>
<li>for: 这种论文旨在提出一种基于堆式智能表面（SIM）技术的下一代无线网络transceiver设计，以更有效地利用无线电频资源。</li>
<li>methods: 该论文提出使用堆式智能表面（SIM）技术，堆组织一系列可程序的表面层，每层包含大量的低成本PASSIVE元素，通过合理配置这些元素，实现复杂的计算和信号处理任务，如MIMO precoding&#x2F;combining、多用户干扰抑制和雷达探测。</li>
<li>results: 该论文提供了SIM-aided MIMO transceiver设计的概述，包括硬件体系结构和与现有解决方案的比较。此外，论文还详细介绍了应用场景和开放研究挑战，以及使用高级SIM体系结构实现下一代无线网络的可能性。最后，论文提供了数字结果，以证明在无线系统中使用波动信号处理的优势。<details>
<summary>Abstract</summary>
Next-generation wireless networks are expected to utilize the limited radio frequency (RF) resources more efficiently with the aid of intelligent transceivers. To this end, we propose a promising transceiver architecture relying on stacked intelligent metasurfaces (SIM). An SIM is constructed by stacking an array of programmable metasurface layers, where each layer consists of a massive number of low-cost passive meta-atoms that individually manipulate the electromagnetic (EM) waves. By appropriately configuring the passive meta-atoms, an SIM is capable of accomplishing advanced computation and signal processing tasks, such as multiple-input multiple-output (MIMO) precoding/combining, multi-user interference mitigation, and radar sensing, as the EM wave propagates through the multiple layers of the metasurface, which effectively reduces both the RF-related energy consumption and processing delay. Inspired by this, we provide an overview of the SIM-aided MIMO transceiver design, which encompasses its hardware architecture and its potential benefits over state-of-the-art solutions. Furthermore, we discuss promising application scenarios and identify the open research challenges associated with the design of advanced SIM architectures for next-generation wireless networks. Finally, numerical results are provided for quantifying the benefits of wave-based signal processing in wireless systems.
</details>
<details>
<summary>摘要</summary>
Inspired by this, we provide an overview of the SIM-aided MIMO transceiver design, including its hardware architecture and potential benefits over existing solutions. We also discuss promising application scenarios and identify open research challenges associated with the design of advanced SIM architectures for next-generation wireless networks. Finally, we provide numerical results to quantify the benefits of wave-based signal processing in wireless systems.Here is the translation in Simplified Chinese:下一代无线网络即将使用有限的广播频率资源更有效地使用，并且通过智能转发器来实现。为此，我们提出了一种有前途的转发器架构，即堆叠智能金属表盘（SIM）。每层SIM都由一大量的低成本Passive元件组成，这些元件个别对电磁波（EM）波进行处理。通过合适配置这些元件，SIM可以完成复杂的计算和信号处理任务，例如多输入多出力（MIMO）预处理/组合、多用户干扰抑制和雷达探测。这将有效减少广播相关的能量消耗和处理延迟。受这些想法启发，我们提供SIM帮助MIMO转发器设计的概述，包括硬件架构和其优势。我们还讨论了可能的应用场景，并识别了进一步开发SIM架构的研究挑战。最后，我们提供了量化无线系统中波形处理的数字结果。
</details></li>
</ul>
<hr>
<h2 id="MEGA-A-Memory-Efficient-GNN-Accelerator-Exploiting-Degree-Aware-Mixed-Precision-Quantization"><a href="#MEGA-A-Memory-Efficient-GNN-Accelerator-Exploiting-Degree-Aware-Mixed-Precision-Quantization" class="headerlink" title="MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware Mixed-Precision Quantization"></a>MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware Mixed-Precision Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09775">http://arxiv.org/abs/2311.09775</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zeyu Zhu, Fanrong Li, Gang Li, Zejian Liu, Zitao Mo, Qinghao Hu, Xiaoyao Liang, Jian Cheng</li>
<li>for: 本研究的目的是提出一种高效的图 neural network（GNN）加速器，以解决GNN在非欧几何数据模型中的缓存访问所带来的延迟和能耗问题。</li>
<li>methods: 本研究提出了一种叫做 Memory-Efficient GNN Accelerator (MEGA)的加速器，通过算法和硬件合作设计。在算法层面，通过对节点属性进行深入分析，我们发现了一种名为度量感知的杂素精度归一化方法，可以减少GNN的压缩比例，保持精度。在硬件层面，我们采用了一种多元架构设计，将聚合和组合阶段分别实现为不同的数据流。</li>
<li>results: 我们实现了MEGA加速器在28nm技术节点上，并进行了广泛的实验。结果表明，MEGA可以在四种状态目前的GNN加速器上实现平均速度提升38.3倍，7.1倍，4.0倍，3.6倍，而且保持任务的精度。同时，MEGA也可以实现7.2倍，5.4倍，4.5倍的能耗减少。<details>
<summary>Abstract</summary>
Graph Neural Networks (GNNs) are becoming a promising technique in various domains due to their excellent capabilities in modeling non-Euclidean data. Although a spectrum of accelerators has been proposed to accelerate the inference of GNNs, our analysis demonstrates that the latency and energy consumption induced by DRAM access still significantly impedes the improvement of performance and energy efficiency. To address this issue, we propose a Memory-Efficient GNN Accelerator (MEGA) through algorithm and hardware co-design in this work. Specifically, at the algorithm level, through an in-depth analysis of the node property, we observe that the data-independent quantization in previous works is not optimal in terms of accuracy and memory efficiency. This motivates us to propose the Degree-Aware mixed-precision quantization method, in which a proper bitwidth is learned and allocated to a node according to its in-degree to compress GNNs as much as possible while maintaining accuracy. At the hardware level, we employ a heterogeneous architecture design in which the aggregation and combination phases are implemented separately with different dataflows. In order to boost the performance and energy efficiency, we also present an Adaptive-Package format to alleviate the storage overhead caused by the fine-grained bitwidth and diverse sparsity, and a Condense-Edge scheduling method to enhance the data locality and further alleviate the access irregularity induced by the extremely sparse adjacency matrix in the graph. We implement our MEGA accelerator in a 28nm technology node. Extensive experiments demonstrate that MEGA can achieve an average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW, and SGCN, respectively, while retaining task accuracy.
</details>
<details>
<summary>摘要</summary>
图 neural network (GNN) 在不同领域变得抢手的技术，因为它们可以非常好地模型非欧几何数据。 Although a variety of accelerators have been proposed to accelerate the inference of GNNs, our analysis shows that the latency and energy consumption caused by DRAM access still significantly hinders the improvement of performance and energy efficiency. To address this issue, we propose a Memory-Efficient GNN Accelerator (MEGA) through algorithm and hardware co-design in this work. Specifically, at the algorithm level, through an in-depth analysis of the node property, we find that the data-independent quantization in previous works is not optimal in terms of accuracy and memory efficiency. This motivates us to propose the Degree-Aware mixed-precision quantization method, in which a proper bitwidth is learned and allocated to a node according to its in-degree to compress GNNs as much as possible while maintaining accuracy. At the hardware level, we employ a heterogeneous architecture design in which the aggregation and combination phases are implemented separately with different dataflows. In order to boost the performance and energy efficiency, we also present an Adaptive-Package format to alleviate the storage overhead caused by the fine-grained bitwidth and diverse sparsity, and a Condense-Edge scheduling method to enhance the data locality and further alleviate the access irregularity induced by the extremely sparse adjacency matrix in the graph. We implement our MEGA accelerator in a 28nm technology node. Extensive experiments show that MEGA can achieve an average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x energy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW, and SGCN, respectively, while retaining task accuracy.
</details></li>
</ul>
<hr>
<h2 id="OFDM-based-Waveforms-for-Joint-Sensing-and-Communications-Robust-to-Frequency-Selective-IQ-Imbalance"><a href="#OFDM-based-Waveforms-for-Joint-Sensing-and-Communications-Robust-to-Frequency-Selective-IQ-Imbalance" class="headerlink" title="OFDM-based Waveforms for Joint Sensing and Communications Robust to Frequency Selective IQ Imbalance"></a>OFDM-based Waveforms for Joint Sensing and Communications Robust to Frequency Selective IQ Imbalance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09746">http://arxiv.org/abs/2311.09746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Lang, Christian Hofbauer, Moritz Tockner, Reinhard Feger, Thomas Wagner, Mario Huemer</li>
<li>for: 这个研究旨在提出一种对偏射和通讯系统具有潜力的OFDM波形，并解决OFDM波形对偏射和 quadrature-phase（IQ）不对称的问题，以减少噪声底。</li>
<li>methods: 这个研究使用了一种新的OFDM波形，它 neither increases the noise floor nor reduces the maximum unambiguous range，并且提出了一种适应频率选择的通信系统，包括通道估计、同步和数据估计方法，它们是 Specifically designed to deal with frequency selective IQ imbalance in wideband systems。</li>
<li>results: 这个研究通过 simulations 示出了这些通信系统的有效性，并且显示了这些系统在噪声底和最大不ambiguous 距离方面的改善。<details>
<summary>Abstract</summary>
Orthogonal frequency-division multiplexing (OFDM) is a promising waveform candidate for future joint sensing and communication systems. It is well known that the OFDM waveform is vulnerable to in-phase and quadrature-phase (IQ) imbalance, which increases the noise floor in a range-Doppler map (RDM). A state-of-the-art method for robustifying the OFDM waveform against IQ imbalance avoids an increased noise floor, but it generates additional ghost objects in the RDM [1]. A consequence of these additional ghost objects is a reduction of the maximum unambiguous range. In this work, a novel OFDM-based waveform robust to IQ imbalance is proposed, which neither increases the noise floor nor reduces the maximum unambiguous range. The latter is achieved by shifting the ghost objects in the RDM to different velocities such that their range variations observed over several consecutive RDMs do not correspond to the observed velocity. This allows tracking algorithms to identify them as ghost objects and eliminate them for the follow-up processing steps. Moreover, we propose complete communication systems for both the proposed waveform as well as for the state-of-the-art waveform, including methods for channel estimation, synchronization, and data estimation that are specifically designed to deal with frequency selective IQ imbalance which occurs in wideband systems. The effectiveness of these communication systems is demonstrated by means of bit error ratio (BER) simulations.
</details>
<details>
<summary>摘要</summary>
隐式frequency-division multiplexing（OFDM）是未来 JOINT sensing和通信系统的优秀waveform候选人。OFDM波形容于均匀和 quadrature-phase（IQ）不匹配，从而增加range-Doppler map（RDM）中的噪声底。现有的state-of-the-art方法可以避免增加噪声底，但会生成RDM中的幻象物体。这些幻象物体会 reducion maximum unambiguous range。在这种工作中，我们提出了一种robust OFDM波形， neither increases the noise floor nor reduces the maximum unambiguous range。这是通过在RDM中移动幻象物体的速度，使其在不同的速度下变化，以至于在多个连续RDM中不同速度下的范围变化不同于观测到的速度。这使得跟踪算法可以将其标记为幻象物体，并在后续处理步骤中消除它们。此外，我们还提出了为两种waveform（包括提案的波形和现有waveform）的完整通信系统，包括频率选择性IQ不匹配的通道估计、同步和数据估计方法，这些方法特别针对随着宽频带的IQ不匹配。我们通过BER simulations示出这些通信系统的有效性。
</details></li>
</ul>
<hr>
<h2 id="Reconciling-Radio-Tomographic-Imaging-with-Phaseless-Inverse-Scattering"><a href="#Reconciling-Radio-Tomographic-Imaging-with-Phaseless-Inverse-Scattering" class="headerlink" title="Reconciling Radio Tomographic Imaging with Phaseless Inverse Scattering"></a>Reconciling Radio Tomographic Imaging with Phaseless Inverse Scattering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09633">http://arxiv.org/abs/2311.09633</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amartansh Dubey, Zan Li, Ross Murch</li>
<li>For: This paper aims to improve the accuracy of Radio Tomographic Imaging (RTI) by reconciling it with formal inverse scattering approaches and enhancing its performance using inverse scattering techniques.* Methods: The paper uses empirical RTI models and formal inverse scattering approaches to compare and enhance RTI’s performance.* Results: The enhanced RTI method outperforms traditional RTI while having similar computational complexity, as demonstrated through numerical and experimental results using low-cost 2.4 GHz Wi-Fi transceivers for indoor imaging applications.Here is the same information in Simplified Chinese:* For: 这篇论文的目的是提高Radio Tomographic Imaging（RTI）的准确率，通过与正式反射扩散方法进行比较和改进RTI的性能。* Methods: 这篇论文使用了empirical RTI模型和正式反射扩散方法来比较和改进RTI的性能。* Results: 改进后的RTI方法可以超越传统的RTI，同时保持与RTI相同的计算复杂性，通过使用低成本的2.4 GHz Wi-Fi传输器进行indoor应用。<details>
<summary>Abstract</summary>
Radio Tomographic Imaging (RTI) is a phaseless imaging approach that can provide shape reconstruction and localization of objects using received signal strength (RSS) measurements. RSS measurements can be straightforwardly obtained from wireless networks such as Wi-Fi and therefore RTI has been extensively researched and accepted as a good indoor RF imaging technique. However, RTI is formulated on empirical models using an assumption of light-of-sight (LOS) propagation that does not account for intricate scattering effects. There are two main objectives of this work. The first objective is to reconcile and compare the empirical RTI model with formal inverse scattering approaches to better understand why RTI is an effective RF imaging technique. The second objective is to obtain straightforward enhancements to RTI, based on inverse scattering, to enhance its performance. The resulting enhancements can provide reconstructions of the shape and also material properties of the objects that can aid image classification. We also provide numerical and experimental results to compare RTI with the enhanced RTI for indoor imaging applications using low-cost 2.4 GHz Wi-Fi transceivers. These results show that the enhanced RTI can outperform RTI while having similar computational complexity to RTI.
</details>
<details>
<summary>摘要</summary>
Radio Tomographic Imaging (RTI) 是一种无相位成像方法，可以提供物体形态重建和位置确定使用接收信号强度 (RSS) 测量。 RSS 测量可以直接从无线网络 such as Wi-Fi 获得，因此 RTI 在indoor RF 成像技术中得到了广泛的研究和认可。然而， RTI 基于employmodels 的 assumption of line-of-sight (LOS) 媒体传播，不能考虑复杂的散射效应。这个工作的两个主要目标是：1. 与形式 inverse scattering 方法进行对比和结合 RTI 模型，以更好地理解 RTI 是一种有效的 RF 成像技术。2. 基于 inverse scattering 方法，对 RTI 进行改进，以提高其性能。改进后的 RTI 可以提供更好的形态重建和物体属性的重建，这可以帮助图像分类。我们还提供了数字和实验结果，以比较 RTI 与改进后的 RTI 在indoor 成像应用中的性能。结果表明，改进后的 RTI 可以超过 RTI，而且与 RTI 的计算复杂度相似。
</details></li>
</ul>
<hr>
<h2 id="Plug-In-RIS-A-Novel-Approach-to-Fully-Passive-Reconfigurable-Intelligent-Surfaces"><a href="#Plug-In-RIS-A-Novel-Approach-to-Fully-Passive-Reconfigurable-Intelligent-Surfaces" class="headerlink" title="Plug-In RIS: A Novel Approach to Fully Passive Reconfigurable Intelligent Surfaces"></a>Plug-In RIS: A Novel Approach to Fully Passive Reconfigurable Intelligent Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09626">http://arxiv.org/abs/2311.09626</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahmoud Raeisi, Ibrahim Yildirim, Mehmet Cagri Ilter, Majid Gerami, Ertugrul Basar</li>
<li>for: 提高 millimeter wave 通信系统中阻挡区域的性能</li>
<li>methods: 使用固定磁场技术和先进的杂排列技术来实现位置相关的磁场规划</li>
<li>results: 在有限CSI情况下， plug-in RIS 可以提供高效的解决方案，并与传统全CSI-启用 RIS 解决方案 exhibit striking convergence in average bit error rate and achievable rate performance.<details>
<summary>Abstract</summary>
This paper presents a promising design concept for reconfigurable intelligent surfaces (RISs), named plug-in RIS, wherein the RIS is plugged into an appropriate position in the environment, adjusted once according to the location of both base station and blocked region, and operates with fixed beams to enhance the system performance. The plug-in RIS is a novel system design, streamlining RIS-assisted millimeter-wave (mmWave) communication without requiring decoupling two parts of the end-to-end channel, traditional control signal transmission, and online RIS configuration. In plug-in RIS-aided transmission, the transmitter efficiently activates specific regions of the divided large RIS by employing hybrid beamforming techniques, each with predetermined phase adjustments tailored to reflect signals to desired user locations. This user-centric approach enhances connectivity and overall user experience by dynamically illuminating the targeted user based on location. By introducing plug-in RIS's theoretical framework, design principles, and performance evaluation, we demonstrate its potential to revolutionize mmWave communications in limited channel state information (CSI) scenarios. Simulation results illustrate that plug-in RIS provides power/cost-efficient solutions to overcome blockage in the mmWave communication system and a striking convergence in average bit error rate and achievable rate performance with traditional full CSI-enabled RIS solutions.
</details>
<details>
<summary>摘要</summary>
The novel design of the plug-in RIS eliminates the need for decoupling the two parts of the end-to-end channel, traditional control signal transmission, and online RIS configuration, streamlining the RIS-assisted mmWave communication. In the plug-in RIS-aided transmission, the transmitter efficiently activates specific regions of the divided large RIS using hybrid beamforming techniques, each with predetermined phase adjustments tailored to reflect signals to desired user locations. This user-centric approach enhances connectivity and overall user experience by dynamically illuminating the targeted user based on location.The theoretical framework, design principles, and performance evaluation of the plug-in RIS are presented in this paper, demonstrating its potential to revolutionize mmWave communications in limited channel state information (CSI) scenarios. Simulation results show that the plug-in RIS provides power/cost-efficient solutions to overcome blockage in the mmWave communication system and achieves a striking convergence in average bit error rate and achievable rate performance with traditional full CSI-enabled RIS solutions.
</details></li>
</ul>
<hr>
<h2 id="Joint-Visibility-Region-and-Channel-Estimation-for-Extremely-Large-scale-MIMO-Systems"><a href="#Joint-Visibility-Region-and-Channel-Estimation-for-Extremely-Large-scale-MIMO-Systems" class="headerlink" title="Joint Visibility Region and Channel Estimation for Extremely Large-scale MIMO Systems"></a>Joint Visibility Region and Channel Estimation for Extremely Large-scale MIMO Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09490">http://arxiv.org/abs/2311.09490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anzheng Tang, Jun-bo Wang, Yijin Pan, Wence Zhang, Yijian Chen, Hongkang Yu, Rodrigo C. de Lamare</li>
<li>for: 本研究 investigate the channel estimation (CE) problem for extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, considering both the spherical wavefront effect and spatial non-stationarity (SnS).</li>
<li>methods: 我们提出了一种 two-stage visibility region (VR) detection and CE framework, which leverages sparsity in both the spatial and wavenumber domains to achieve an accurate estimation. In the first stage, we use a structured message passing (MP) scheme to obtain the belief regarding the visibility of antennas. In the second stage, we use the obtained VR information and wavenumber-domain sparsity to accurately estimate the SnS channel employing the belief-based orthogonal matching pursuit (BB-OMP) method.</li>
<li>results:  simulations demonstrate that the proposed algorithms lead to a significant enhancement in VR detection and CE accuracy, especially in low signal-to-noise ratio (SNR) scenarios.<details>
<summary>Abstract</summary>
In this work, we investigate the channel estimation (CE) problem for extremely large-scale multiple-input-multiple-output (XL-MIMO) systems, considering both the spherical wavefront effect and spatial non-stationarity (SnS). Unlike existing non-stationary CE methods that rely on the statistical characteristics of channels in the spatial or temporal domain, our approach seeks to leverage sparsity in both the spatial and wavenumber domains simultaneously to achieve an accurate estimation.To this end, we introduce a two-stage visibility region (VR) detection and CE framework. Specifically, in the first stage, the belief regarding the visibility of antennas is obtained through a structured message passing (MP) scheme, which fully exploits the block sparse structure of the antenna-domain channel. In the second stage, using the obtained VR information and wavenumber-domain sparsity, we accurately estimate the SnS channel employing the belief-based orthogonal matching pursuit (BB-OMP) method. Simulations demonstrate that the proposed algorithms lead to a significant enhancement in VR detection and CE accuracy, especially in low signal-to-noise ratio (SNR) scenarios.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们研究了超大规模多输入多输出（XL-MIMO）系统中的通道估计（CE）问题，考虑了球面冲击效应和空间非站点性（SnS）。 unlike existing non-stationary CE方法，我们的方法不仅利用了通道在空间或时域频率域的统计特性，而且同时充分利用了antenna-domain通道的块稀畴结构。为达到这个目的，我们提出了两个阶段的可见区域（VR）探测和CE框架。在第一阶段，通过一种结构化的消息传递（MP）方案，我们可以获得天线域通道的可见性信念。在第二阶段，使用获得的VR信息和波数域稀畴性，我们可以高精度地估计SnS通道，使用信念基本的搜索匹配策略（BB-OMP）。 simulation结果表明，我们的算法可以在低信号噪响比（SNR）场景下提高VR检测和CE准确率。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/eess.SP_2023_11_16/" data-id="clp89dor901i5i7884e3cgflo" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.SD_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T15:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.SD_2023_11_15/">cs.SD - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance"><a href="#AI-based-soundscape-analysis-Jointly-identifying-sound-sources-and-predicting-annoyance" class="headerlink" title="AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance"></a>AI-based soundscape analysis: Jointly identifying sound sources and predicting annoyance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09030">http://arxiv.org/abs/2311.09030</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuanbo2020/ai-soundscape">https://github.com/yuanbo2020/ai-soundscape</a></li>
<li>paper_authors: Yuanbo Hou, Qiaoqiao Ren, Huizhong Zhang, Andrew Mitchell, Francesco Aletta, Jian Kang, Dick Botteldooren</li>
<li>for: This paper proposes an AI-based approach for automatic soundscape characterization, including sound recognition and appraisal.</li>
<li>methods: The proposed method uses a dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze sound sources and predict human-perceived annoyance.</li>
<li>results: The proposed method outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the sound source classification and annoyance rating prediction tasks, and shows consistent results with human perception.<details>
<summary>Abstract</summary>
Soundscape studies typically attempt to capture the perception and understanding of sonic environments by surveying users. However, for long-term monitoring or assessing interventions, sound-signal-based approaches are required. To this end, most previous research focused on psycho-acoustic quantities or automatic sound recognition. Few attempts were made to include appraisal (e.g., in circumplex frameworks). This paper proposes an artificial intelligence (AI)-based dual-branch convolutional neural network with cross-attention-based fusion (DCNN-CaF) to analyze automatic soundscape characterization, including sound recognition and appraisal. Using the DeLTA dataset containing human-annotated sound source labels and perceived annoyance, the DCNN-CaF is proposed to perform sound source classification (SSC) and human-perceived annoyance rating prediction (ARP). Experimental findings indicate that (1) the proposed DCNN-CaF using loudness and Mel features outperforms the DCNN-CaF using only one of them. (2) The proposed DCNN-CaF with cross-attention fusion outperforms other typical AI-based models and soundscape-related traditional machine learning methods on the SSC and ARP tasks. (3) Correlation analysis reveals that the relationship between sound sources and annoyance is similar for humans and the proposed AI-based DCNN-CaF model. (4) Generalization tests show that the proposed model's ARP in the presence of model-unknown sound sources is consistent with expert expectations and can explain previous findings from the literature on sound-scape augmentation.
</details>
<details>
<summary>摘要</summary>
听音环境研究通常会尝试捕捉用户对听音环境的认知和理解，但是对于长期监测或评估 intervención，需要基于听音信号的方法。因此，前期研究主要集中在 psycho-acoustic 量或自动听音识别方面。只有一些尝试包括评价（如在 circumplex 框架中）。这篇文章提出了基于人工智能（AI）的双支树层卷积神经网络（DCNN-CaF），用于自动听音特征化，包括听音识别和评价。使用包含人类标注的听音源标签和感知困扰的DeLTA数据集，DCNN-CaF被提议用于听音源类别（SSC）和人类感知困扰评分预测（ARP）。实验结果表明：1. 使用听音强度和Mel特征的DCNN-CaF，与只使用一个特征的DCNN-CaF相比，表现出更好的性能。2. DCNN-CaF与十字关注融合的方法，在SSC和ARP任务上表现出比其他常见的AI模型和听音相关传统机器学习方法更好的性能。3. 相关分析表明，人类和提议的AI模型之间听音源和困扰之间的关系类似。4. 总结测试表明，提议的模型的ARP在模型不知道听音源时的存在下保持一致性，与专家预期一致，并可以解释过去 literatura 中的听音环境增强现象。
</details></li>
</ul>
<hr>
<h2 id="CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes"><a href="#CREPE-Notes-A-new-method-for-segmenting-pitch-contours-into-discrete-notes" class="headerlink" title="CREPE Notes: A new method for segmenting pitch contours into discrete notes"></a>CREPE Notes: A new method for segmenting pitch contours into discrete notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08884">http://arxiv.org/abs/2311.08884</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xavier Riley, Simon Dixon</li>
<li>for: 本研究旨在提出一种简单有效的音频识别方法，以提高音乐分析和符号音乐生成等应用的可行性。</li>
<li>methods: 本方法基于CREPE，一种现有的单声道抖音跟踪解决方案，使用简单的神经网络进行后处理，以实现单声道音频分 segmentation。</li>
<li>results: 本方法在两个具有挑战性的单声道乐器音乐数据集上达到了状态级 Ergebnisse，同时与其他深度学习基于方法相比，减少了97%的总参数数量。<details>
<summary>Abstract</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details>
<details>
<summary>摘要</summary>
Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. However, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes. This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation. Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation. The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music. Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.Here's the breakdown of the text in Simplified Chinese:Tracking the fundamental frequency (f0) of a monophonic instrumental performance is effectively a solved problem with several solutions achieving 99% accuracy. however, the related task of automatic music transcription requires a further processing step to segment an f0 contour into discrete notes.This sub-task of note segmentation is necessary to enable a range of applications including musicological analysis and symbolic music generation.Building on CREPE, a state-of-the-art monophonic pitch tracking solution based on a simple neural network, we propose a simple and effective method for post-processing CREPE's output to achieve monophonic note segmentation.The proposed method demonstrates state-of-the-art results on two challenging datasets of monophonic instrumental music.Our approach also gives a 97% reduction in the total number of parameters used when compared with other deep learning based methods.
</details></li>
</ul>
<hr>
<h2 id="Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model"><a href="#Multi-objective-Non-intrusive-Hearing-aid-Speech-Assessment-Model" class="headerlink" title="Multi-objective Non-intrusive Hearing-aid Speech Assessment Model"></a>Multi-objective Non-intrusive Hearing-aid Speech Assessment Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08878">http://arxiv.org/abs/2311.08878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hsin-Tien Chiang, Szu-Wei Fu, Hsin-Min Wang, Yu Tsao, John H. L. Hansen</li>
<li>for: 这种研究旨在提出一种基于原始波形和听力损伤模式的非侵入式听力评估模型，以提高听力评估的准确性和可重复性。</li>
<li>methods: 该模型使用了预训练的SSL模型，并对输入音频信号进行了特征EXTRACT和维度压缩。此外，研究还 compare了三种精度调整方法，并证明了在OOD数据集上进行了更好的转移性。</li>
<li>results: 研究表明，使用预训练的SSL模型可以在听力评估中得到显著提高的表达质量和整体表达能力，并且在不同的听力损伤水平下具有更好的转移性。<details>
<summary>Abstract</summary>
Without the need for a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. While deep learning models have been used to develop non-intrusive speech assessment methods with promising results, there is limited research on hearing-impaired subjects. This study proposes a multi-objective non-intrusive hearing-aid speech assessment model, called HASA-Net Large, which predicts speech quality and intelligibility scores based on input speech signals and specified hearing-loss patterns. Our experiments showed the utilization of pre-trained SSL models leads to a significant boost in speech quality and intelligibility predictions compared to using spectrograms as input. Additionally, we examined three distinct fine-tuning approaches that resulted in further performance improvements. Furthermore, we demonstrated that incorporating SSL models resulted in greater transferability to OOD dataset. Finally, this study introduces HASA-Net Large, which is a non-invasive approach for evaluating speech quality and intelligibility. HASA-Net Large utilizes raw waveforms and hearing-loss patterns to accurately predict speech quality and intelligibility levels for individuals with normal and impaired hearing and demonstrates superior prediction performance and transferability.
</details>
<details>
<summary>摘要</summary>
无需清晰参考，非侵入式Speech评估方法在 objective 评估中受到了广泛关注。而深度学习模型已经被用来开发非侵入式Speech评估方法，但有限的研究在听力障碍者中。这项研究提出了一种多目标非侵入式听力器Speech评估模型，称为HASA-Net Large，该模型根据输入Speech信号和指定的听力损耗模式预测Speech质量和 inteligibilty 分数。我们的实验表明使用预训练的 SSL 模型会导致Speech质量和 inteligibilty 预测得到显著提升，比使用spectrograms作为输入更好。此外，我们还考虑了三种不同的细化方法，这些方法导致了更好的性能提升。此外，我们还证明了将 SSL 模型integrated 到 OOD 数据集中的更好传播性。最后，本研究介绍了HASA-Net Large，这是一种非侵入式的Speech质量和 inteligibilty 评估方法，该方法使用原始波形和听力损耗模式来准确预测听力正常和听力障碍者的Speech质量和 inteligibilty 水平，并达到了更高的预测性和传播性。
</details></li>
</ul>
<hr>
<h2 id="Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection"><a href="#Autoencoder-with-Group-based-Decoder-and-Multi-task-Optimization-for-Anomalous-Sound-Detection" class="headerlink" title="Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection"></a>Autoencoder with Group-based Decoder and Multi-task Optimization for Anomalous Sound Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08829">http://arxiv.org/abs/2311.08829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Zhou, Dongxing Xu, Haoran Wei, Yanhua Long</li>
<li>for: 这篇论文主要是为了提出一个基于自适应神经网络的机器发生音预警方法，以解决实际应用中机器发生音检测中的短路、杂音识别和特征品质不足问题。</li>
<li>methods: 这篇论文提出了一个基于自适应神经网络的架构，包括在自适应神经网络中插入辅助分类器，以增强预警性能。此外，论文还提出了一种群集结构和适应损失函数，以具备专业知识。</li>
<li>results: 根据DCASE 2021 Task 2的开发集合，这篇论文的方法在七台机器上的测试集合中，相对于官方AE和MobileNetV2的平均投票值提高13.11%和15.20%。<details>
<summary>Abstract</summary>
In industry, machine anomalous sound detection (ASD) is in great demand. However, collecting enough abnormal samples is difficult due to the high cost, which boosts the rapid development of unsupervised ASD algorithms. Autoencoder (AE) based methods have been widely used for unsupervised ASD, but suffer from problems including 'shortcut', poor anti-noise ability and sub-optimal quality of features. To address these challenges, we propose a new AE-based framework termed AEGM. Specifically, we first insert an auxiliary classifier into AE to enhance ASD in a multi-task learning manner. Then, we design a group-based decoder structure, accompanied by an adaptive loss function, to endow the model with domain-specific knowledge. Results on the DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details>
<details>
<summary>摘要</summary>
在工业领域，机器异常声音检测（ASD）的需求非常大。然而，收集足够的异常样本是困难的，这会促进不upervised ASD算法的快速发展。基于自适应Encoder（AE）的方法在不upervised ASD方面广泛应用，但它们受到短 Circuit、anti-noise能力不够和特征质量不佳等问题的困扰。为了解决这些挑战，我们提出了一个新的AE基于框架，称为AEGM。 Specifically，我们首先将auxiliary分类器 inserting into AE以增强ASD的多任务学习方式。然后，我们设计了一种群体基本解码结构，并附加了一个适应损失函数，以使模型具备域pecific的知识。Results on DCASE 2021 Task 2 development set show that our methods achieve a relative improvement of 13.11% and 15.20% respectively in average AUC over the official AE and MobileNetV2 across test sets of seven machines.
</details></li>
</ul>
<hr>
<h2 id="CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation"><a href="#CLN-VC-Text-Free-Voice-Conversion-Based-on-Fine-Grained-Style-Control-and-Contrastive-Learning-with-Negative-Samples-Augmentation" class="headerlink" title="CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation"></a>CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08670">http://arxiv.org/abs/2311.08670</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Deng, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao</li>
<li>for: 提高voice转换质量的关键是更好地分离speech表示。</li>
<li>methods: 我们提议使用增强的负样本选择和参考编码器来改善speaker编码器的学习能力。</li>
<li>results: 我们的方法比前期工作在voice转换任务中表现更好。<details>
<summary>Abstract</summary>
Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.
</details>
<details>
<summary>摘要</summary>
更好的发音 Representation 分离是voice转换质量的关键。最近，基于说话人标签的对比学习成功地应用于voice转换。然而，在相似的说话人之间转换时，模型性能会降低。因此，我们提出一种增强的负样本选择方法来解决这个问题。具体来说，我们基于提议的说话人融合模块创建困难的负样本，以提高说话人Encoder的学习能力。此外，为了考虑细腻的说话人风格模型，我们采用参考Encoder来提取细腻的风格特征，并在全局风格上进行增强对比学习。实验结果表明，我们的方法在voice转换任务上的表现比前一个工作更好。
</details></li>
</ul>
<hr>
<h2 id="EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis"><a href="#EDMSound-Spectrogram-Based-Diffusion-Models-for-Efficient-and-High-Quality-Audio-Synthesis" class="headerlink" title="EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis"></a>EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08667">http://arxiv.org/abs/2311.08667</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ge Zhu, Yutong Wen, Marc-André Carbonneau, Zhiyao Duan</li>
<li>for: 本文提出了一种基于扩散模型的生成模型，用于生成各种听起来的声音。</li>
<li>methods: 该模型在spectrogram域内使用扩散过程来生成声音，并结合了高效的抽象推断器。</li>
<li>results: 研究人员通过对DCASE2023 foley音频生成 benchmark进行测试，发现该模型在10步内可以达到类似于顶尖基eline的Fréchet音频距离（FAD）分数，并在50步内达到了状态的掌握水平。此外，研究人员还发现了扩散基于音频生成模型的一种潜在问题，即它们可能会生成与训练数据高度相似的样本。<details>
<summary>Abstract</summary>
Audio diffusion models can synthesize a wide variety of sounds. Existing models often operate on the latent domain with cascaded phase recovery modules to reconstruct waveform. This poses challenges when generating high-fidelity audio. In this paper, we propose EDMSound, a diffusion-based generative model in spectrogram domain under the framework of elucidated diffusion models (EDM). Combining with efficient deterministic sampler, we achieved similar Fr\'echet audio distance (FAD) score as top-ranked baseline with only 10 steps and reached state-of-the-art performance with 50 steps on the DCASE2023 foley sound generation benchmark. We also revealed a potential concern regarding diffusion based audio generation models that they tend to generate samples with high perceptual similarity to the data from training data. Project page: https://agentcooper2002.github.io/EDMSound/
</details>
<details>
<summary>摘要</summary>
Audio 扩散模型可以生成各种听起来。现有模型通常在幂征频域中使用缓冲phaserecovery模块来重construct波形。这会对高精度音频生成带来挑战。在这篇论文中，我们提出了EDMSound，一种基于扩散模型的生成模型，在幂征频域下的框架之下。通过有效的deterministic采样器，我们在10步内达到了与顶峰基eline的相似性 Fréchet 音频距离（FAD）分数，并在50步内达到了状态之 искусственный智能术语（DCASE2023） foley 声音生成 benchmark 的顶峰性能。我们还发现了扩散基于音频生成模型的一个潜在问题，即它们往往会生成与训练数据具有高听觉相似性的样本。项目页面：https://agentcooper2002.github.io/EDMSound/
</details></li>
</ul>
<hr>
<h2 id="Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization"><a href="#Multi-channel-Conversational-Speaker-Separation-via-Neural-Diarization" class="headerlink" title="Multi-channel Conversational Speaker Separation via Neural Diarization"></a>Multi-channel Conversational Speaker Separation via Neural Diarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08630">http://arxiv.org/abs/2311.08630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hassan Taherian, DeLiang Wang</li>
<li>for: 提高会议环境中自动语音识别（ASR）系统的性能，解决单话者语音设计的问题。</li>
<li>methods: 提出了一种基于神经网络分类的 speaker separation via neural diarization（SSND）框架，利用终端到终端的分类系统来标识每个个体的语音活动。</li>
<li>results: 在 LibriCSS  dataset 上进行了评估，并取得了大幅提高的 диари化和 ASR Result，代表了state-of-the-art 水平。<details>
<summary>Abstract</summary>
When dealing with overlapped speech, the performance of automatic speech recognition (ASR) systems substantially degrades as they are designed for single-talker speech. To enhance ASR performance in conversational or meeting environments, continuous speaker separation (CSS) is commonly employed. However, CSS requires a short separation window to avoid many speakers inside the window and sequential grouping of discontinuous speech segments. To address these limitations, we introduce a new multi-channel framework called "speaker separation via neural diarization" (SSND) for meeting environments. Our approach utilizes an end-to-end diarization system to identify the speech activity of each individual speaker. By leveraging estimated speaker boundaries, we generate a sequence of embeddings, which in turn facilitate the assignment of speakers to the outputs of a multi-talker separation model. SSND addresses the permutation ambiguity issue of talker-independent speaker separation during the diarization phase through location-based training, rather than during the separation process. This unique approach allows multiple non-overlapped speakers to be assigned to the same output stream, making it possible to efficiently process long segments-a task impossible with CSS. Additionally, SSND is naturally suitable for speaker-attributed ASR. We evaluate our proposed diarization and separation methods on the open LibriCSS dataset, advancing state-of-the-art diarization and ASR results by a large margin.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:在叠加的语音中，自动语音识别（ASR）系统的性能会受到很大的影响，因为它们是单个说话者的设计。为了在会议环境中提高 ASR 性能，常用 continuous speaker separation（CSS）。然而，CSS 需要一个短 separation window，以避免在 window 内有多个说话者，并且sequential grouping of discontinuous speech segments。为了解决这些限制，我们提出了一个新的多通道框架，即 "speaker separation via neural diarization"（SSND）。我们的方法使用了一个端到端的 diarization 系统，以确定每个个体说话者的语音活动。通过利用估计的 speaker 边界，我们生成了一个序列 embedding，以便将说话者分配到 multi-talker separation 模型的输出流中。SSND 通过在 diarization 阶段使用位置基于的训练，而不是在 separation 阶段，解决了 talker-independent speaker separation 的 permutation ambiguity 问题。这种独特的方法使得可以有效地处理长段，而不是 CSS 中的 sequential grouping。此外，SSND 自然适合 speaker-attributed ASR。我们对 open LibriCSS 数据集进行了评估，并在 diarization 和 ASR 领域取得了大幅提升的 estado-of-the-art 结果。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.SD_2023_11_15/" data-id="clp89doks0122i788hq1s6j4f" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CV_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T13:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CV_2023_11_15/">cs.CV - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans"><a href="#Predicting-Spine-Geometry-and-Scoliosis-from-DXA-Scans" class="headerlink" title="Predicting Spine Geometry and Scoliosis from DXA Scans"></a>Predicting Spine Geometry and Scoliosis from DXA Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09424">http://arxiv.org/abs/2311.09424</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amir Jamaludin, Timor Kadir, Emma Clark, Andrew Zisserman</li>
<li>for: 这paper的目的是估算DXA扫描中脊梁的弯 curvature.</li>
<li>methods: 首先，我们训练一个神经网络来预测扫描中的中脊梁弯曲。然后，我们使用一种积分方法来确定脊梁弯曲的弯曲范围。</li>
<li>results: 我们发现，使用弯曲度作为评分函数可以对脊梁弯曲进行排序，并且性能超过了 Jamaludin et al. 2018 的先前工作。<details>
<summary>Abstract</summary>
Our objective in this paper is to estimate spine curvature in DXA scans. To this end we first train a neural network to predict the middle spine curve in the scan, and then use an integral-based method to determine the curvature along the spine curve. We use the curvature to compare to the standard angle scoliosis measure obtained using the DXA Scoliosis Method (DSM). The performance improves over the prior work of Jamaludin et al. 2018. We show that the maximum curvature can be used as a scoring function for ordering the severity of spinal deformation.
</details>
<details>
<summary>摘要</summary>
我们的目标在这篇论文中是估算DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先训练一个神经网络来预测扫描中的中脊梁弯曲度，然后使用一种积分方法来确定脊梁弯曲度的沿脊梁曲线。我们使用弯曲度来与使用DXA脊梁疾病方法（DSM）获得的标准角度股疾病量进行比较。我们表明，最大弯曲度可以用作评估脊梁弯曲度严重程度的分数函数。Here's the translation in Traditional Chinese:我们的目标在这篇论文中是估算DXA扫描中的脊梁弯曲度。为达到这个目标，我们首先训练一个神经网络来预测扫描中的中脊梁弯曲度，然后使用一种积分方法来确定脊梁弯曲度的沿脊梁曲线。我们使用弯曲度来与使用DXA脊梁疾病方法（DSM）获得的标准角度股疾病量进行比较。我们表明，最大弯曲度可以用作评估脊梁弯曲度严重程度的分数函数。
</details></li>
</ul>
<hr>
<h2 id="Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research"><a href="#Synthetically-Enhanced-Unveiling-Synthetic-Data’s-Potential-in-Medical-Imaging-Research" class="headerlink" title="Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research"></a>Synthetically Enhanced: Unveiling Synthetic Data’s Potential in Medical Imaging Research</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09402">http://arxiv.org/abs/2311.09402</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/bardiakh/syntheticallyenhanced">https://github.com/bardiakh/syntheticallyenhanced</a></li>
<li>paper_authors: Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U. Gamble, Hari M. Trivedi, Cody C. Wyles, Andrew B. Sellergren, Saptarshi Purkayastha, Bradley J. Erickson, Judy W. Gichoya</li>
<li>for: 这个研究旨在检验深度学习（DL）分类器在胸部X射线成像（CXR）分析中的性能，并研究使用扩充的数据集来提高模型的准确率。</li>
<li>methods: 这个研究使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并使用了条件涉嫌扩充推散模型（DDPMs）来生成医学图像。我们确保了生成的人工图像具有原始数据中的人类和疾病特征。</li>
<li>results: 研究发现，通过使用人工数据来补充真实数据，可以提高DL模型的准确率，特别是在检测较少发现的疾病方面。此外，使用人工数据alone来训练模型也可以达到与使用真实数据来训练模型的性能水平。这表示人工数据可能可以弥补真实数据的短缺，并且可以帮助建立更加坚固的DL模型。然而，尽管有扎实的结果，真实数据仍然保持优势。<details>
<summary>Abstract</summary>
Chest X-rays (CXR) are the most common medical imaging study and are used to diagnose multiple medical conditions. This study examines the impact of synthetic data supplementation, using diffusion models, on the performance of deep learning (DL) classifiers for CXR analysis. We employed three datasets: CheXpert, MIMIC-CXR, and Emory Chest X-ray, training conditional denoising diffusion probabilistic models (DDPMs) to generate synthetic frontal radiographs. Our approach ensured that synthetic images mirrored the demographic and pathological traits of the original data. Evaluating the classifiers' performance on internal and external datasets revealed that synthetic data supplementation enhances model accuracy, particularly in detecting less prevalent pathologies. Furthermore, models trained on synthetic data alone approached the performance of those trained on real data. This suggests that synthetic data can potentially compensate for real data shortages in training robust DL models. However, despite promising outcomes, the superiority of real data persists.
</details>
<details>
<summary>摘要</summary>
胸部X射影（CXR）是医学影像研究中最常用的方法，用于诊断多种医疗问题。本研究检查了深度学习（DL）分类器在CXR分析中使用推 diffusion模型（DDPMs）的合成数据补充的影响。我们使用了三个数据集：CheXpert、MIMIC-CXR和Emory Chest X-ray，并使用 conditional denoising diffusion probabilistic models（DDPMs）来生成合成前置影像。我们的方法确保了生成的合成图像具有原始数据中的人口和疾病特征。我们对内部和外部数据集进行评估，发现使用合成数据补充可以提高模型的准确率，特别是在检测较少发生的疾病方面。此外，使用合成数据alone进行训练的模型可以达到与使用真实数据训练的模型相同的性能。这表明合成数据可以可能补充实际数据的短缺，帮助建立更加稳定的DL模型。然而，尽管出色的结果，实际数据仍然保持优势。
</details></li>
</ul>
<hr>
<h2 id="MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains"><a href="#MoCo-Transfer-Investigating-out-of-distribution-contrastive-learning-for-limited-data-domains" class="headerlink" title="MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains"></a>MoCo-Transfer: Investigating out-of-distribution contrastive learning for limited-data domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09401">http://arxiv.org/abs/2311.09401</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuwen Chen, Helen Zhou, Zachary C. Lipton</li>
<li>for: 这 paper 是为了研究如何将自动生成的医学影像数据转移到不同的域内数据集上，以提高特殊模型的开发。</li>
<li>methods: 这 paper 使用了 MoCo 自我超vised 对比表示法进行预训练，并对不同的 X-ray 数据集进行比较，以确定是否可以从更大的数据集中提取有用的信息。</li>
<li>results: 研究发现，在有限量的数据集上，可以通过从更大的数据集中提取自动生成的医学影像数据来提高模型性能。此外，在不同的域内数据集之间进行对比可以更好地提高模型性能，而不是使用 ImageNet 预训练的 weights。最后，这 paper 还提供了一种初步的数据集相似性量化方法。<details>
<summary>Abstract</summary>
Medical imaging data is often siloed within hospitals, limiting the amount of data available for specialized model development. With limited in-domain data, one might hope to leverage larger datasets from related domains. In this paper, we analyze the benefit of transferring self-supervised contrastive representations from moment contrast (MoCo) pretraining on out-of-distribution data to settings with limited data. We consider two X-ray datasets which image different parts of the body, and compare transferring from each other to transferring from ImageNet. We find that depending on quantity of labeled and unlabeled data, contrastive pretraining on larger out-of-distribution datasets can perform nearly as well or better than MoCo pretraining in-domain, and pretraining on related domains leads to higher performance than if one were to use the ImageNet pretrained weights. Finally, we provide a preliminary way of quantifying similarity between datasets.
</details>
<details>
<summary>摘要</summary>
医疗影像数据经常受限于医院内部，限制了特殊模型开发的数据量。在这种情况下，可能会希望利用更大的相关领域数据进行开发。本文分析了在有限数据情况下，将自动学习强制对比表示（MoCo）预训练的扩展数据传递到另一个领域的效果。我们考虑了两个X射线数据集，它们分别捕捉了不同的身体部位，并比较了从别的领域传递和从ImageNet预训练的效果。我们发现，取决于标注和无标注数据的数量，在更大的外部数据集上进行对比预训练可以与域内MoCo预训练相当或更好，而从相关领域传递的预训练性能高于使用ImageNet预训练的模型。最后，我们提供了一种初步的数据集相似性量化方法。
</details></li>
</ul>
<hr>
<h2 id="RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior"><a href="#RENI-A-Rotation-Equivariant-Scale-Invariant-Natural-Illumination-Prior" class="headerlink" title="RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior"></a>RENI++ A Rotation-Equivariant, Scale-Invariant, Natural Illumination Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09361">http://arxiv.org/abs/2311.09361</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jadgardner/ns_reni">https://github.com/jadgardner/ns_reni</a></li>
<li>paper_authors: James A. D. Gardner, Bernhard Egger, William A. P. Smith</li>
<li>for: 本研究旨在提出一种基于神经网络的自然照明模型，用于解决 inverse rendering 问题。</li>
<li>methods: 该模型使用 conditional neural field 表示法和 transformer 解码器，并通过 Vector Neurons 技术实现建立对称性。</li>
<li>results: 模型可以准确地表示高动态范围 (HDR) 环境图像，并且可以捕捉到自然环境中高频特征。 Is there anything else you would like to know?<details>
<summary>Abstract</summary>
Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. This results in limitations for the inverse setting in terms of the expressivity of the illumination conditions, especially when taking specular reflections into account. We propose a conditional neural field representation based on a variational auto-decoder and a transformer decoder. We extend Vector Neurons to build equivariance directly into our architecture, and leveraging insights from depth estimation through a scale-invariant loss function, we enable the accurate representation of High Dynamic Range (HDR) images. The result is a compact, rotation-equivariant HDR neural illumination model capable of capturing complex, high-frequency features in natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations. We share our PyTorch implementation, dataset and trained models at https://github.com/JADGardner/ns_reni
</details>
<details>
<summary>摘要</summary>
“ inverse rendering 是一个欠定的问题。 先前的工作强调了对象或场景形状或外观的确认，以解决这个问题。 在这个工作中，我们则专注于天然照明的确认。 现有的方法使用圆柱对称光学或其他通用表示，并仅对parameters的简单确认。 这会导致对 inverse setting 的限制，特别是在考虑到镜面反射时。 我们提议一个基于 conditional neural field 的表示方法，使用 variational auto-decoder 和 transformer decoder。 我们将 Vector Neurons 扩展到建立了 architecture 中的 equivariance，并利用 depth estimation 中的构成调和损失函数，以实现高动态范围 (HDR) 图像的准确表示。 结果是一个可靠、旋转相似的 HDR 神经照明模型，能够捕捉自然环境图中的复杂、高频率特征。 我们在一个精心选择的 dataset 上训练我们的模型，并与传统表示进行比较，证明其适用于 inverse rendering 任务，以及环境图完整性的完成从部分观察。 我们在 GitHub 上分享我们的 PyTorch 实现、dataset 和训练模型，请见 <https://github.com/JADGardner/ns_reni>。”
</details></li>
</ul>
<hr>
<h2 id="Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change"><a href="#Nothing-Stands-Still-A-Spatiotemporal-Benchmark-on-3D-Point-Cloud-Registration-Under-Large-Geometric-and-Temporal-Change" class="headerlink" title="Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change"></a>Nothing Stands Still: A Spatiotemporal Benchmark on 3D Point Cloud Registration Under Large Geometric and Temporal Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09346">http://arxiv.org/abs/2311.09346</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tao Sun, Yan Hao, Shengyu Huang, Silvio Savarese, Konrad Schindler, Marc Pollefeys, Iro Armeni</li>
<li>for: 这篇论文旨在探讨现有的3D geometric map建构方法如何处理时间变化。</li>
<li>methods: 该论文使用了多个参考视图的点云注册方法，以及一个新的多方向点云注册 benchmark。</li>
<li>results: 研究发现现有的点云注册方法无法处理大规模的时间变化，新的方法需要 специаль地设计来处理这类变化。<details>
<summary>Abstract</summary>
Building 3D geometric maps of man-made spaces is a well-established and active field that is fundamental to computer vision and robotics. However, considering the evolving nature of built environments, it is essential to question the capabilities of current mapping efforts in handling temporal changes. In addition, spatiotemporal mapping holds significant potential for achieving sustainability and circularity goals. Existing mapping approaches focus on small changes, such as object relocation or self-driving car operation; in all cases where the main structure of the scene remains fixed. Consequently, these approaches fail to address more radical changes in the structure of the built environment, such as geometry and topology. To this end, we introduce the Nothing Stands Still (NSS) benchmark, which focuses on the spatiotemporal registration of 3D scenes undergoing large spatial and temporal change, ultimately creating one coherent spatiotemporal map. Specifically, the benchmark involves registering two or more partial 3D point clouds (fragments) from the same scene but captured from different spatiotemporal views. In addition to the standard pairwise registration, we assess the multi-way registration of multiple fragments that belong to any temporal stage. As part of NSS, we introduce a dataset of 3D point clouds recurrently captured in large-scale building indoor environments that are under construction or renovation. The NSS benchmark presents three scenarios of increasing difficulty, to quantify the generalization ability of point cloud registration methods over space (within one building and across buildings) and time. We conduct extensive evaluations of state-of-the-art methods on NSS. The results demonstrate the necessity for novel methods specifically designed to handle large spatiotemporal changes. The homepage of our benchmark is at http://nothing-stands-still.com.
</details>
<details>
<summary>摘要</summary>
建立3D геометрические地图的人工环境是一个已经成熟且活跃的领域，对计算机视觉和机器人来说是基础领域。然而，随着建筑环境的发展，需要考虑当前地图努力的有效性，特别是在面对时间变化时。此外，空间时间地图具有可持续和循环的潜力，现有的地图方法主要集中于小型变化，如物体重新布局或自动驾驶车辆操作，而 ignore 主要结构的变化。为此，我们引入Nothing Stands Still（NSS）测试准则，强调在3D场景中大尺度空间和时间变化下进行空间时间地图的准确匹配。具体来说，测试准则包括将两个或多个来自同一场景，但从不同空间时间视图捕捉的3D点云（块）进行对比注准。此外，我们还评估了多个块之间的多方注准，以及这些块在不同时间阶段的注准。NSS测试准则包括3个难度不同的场景，以评估点云注准方法的通用性和灵活性。我们在NSS测试准则上进行了广泛的评估，结果表明了现有方法在面对大尺度空间时间变化时的不足。NSS测试准则的主页可以在http://nothing-stands-still.com 中找到。
</details></li>
</ul>
<hr>
<h2 id="Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion"><a href="#Single-Image-3D-Human-Digitization-with-Shape-Guided-Diffusion" class="headerlink" title="Single-Image 3D Human Digitization with Shape-Guided Diffusion"></a>Single-Image 3D Human Digitization with Shape-Guided Diffusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09221">http://arxiv.org/abs/2311.09221</a></li>
<li>repo_url: None</li>
<li>paper_authors: Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, Jia-Bin Huang</li>
<li>for: 实现单一图像中的人物360度全景视图，包括衣服和人体的高精度描绘。</li>
<li>methods: 利用高容量2D扩散模型来提供衣服人体的应earance假设，然后逐步Synthesize多个视角的人物图像，并通过反射处理协会Multi-view图像进行融合，以获得高精度的3D mesh。</li>
<li>results: 试验结果显示，我们的方法可以优于先前的方法，实现高精度、 фото实际的360度人物全景视图，并且可以处理不同的衣服和人体表现。<details>
<summary>Abstract</summary>
We present an approach to generate a 360-degree view of a person with a consistent, high-resolution appearance from a single input image. NeRF and its variants typically require videos or images from different viewpoints. Most existing approaches taking monocular input either rely on ground-truth 3D scans for supervision or lack 3D consistency. While recent 3D generative models show promise of 3D consistent human digitization, these approaches do not generalize well to diverse clothing appearances, and the results lack photorealism. Unlike existing work, we utilize high-capacity 2D diffusion models pretrained for general image synthesis tasks as an appearance prior of clothed humans. To achieve better 3D consistency while retaining the input identity, we progressively synthesize multiple views of the human in the input image by inpainting missing regions with shape-guided diffusion conditioned on silhouette and surface normal. We then fuse these synthesized multi-view images via inverse rendering to obtain a fully textured high-resolution 3D mesh of the given person. Experiments show that our approach outperforms prior methods and achieves photorealistic 360-degree synthesis of a wide range of clothed humans with complex textures from a single image.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个输入图像中生成一个360度的人体视图，具有一致性和高分辨率的外观。NeRF和其变体通常需要不同视角的视频或图像。现有的approach都是通过ground-truth 3D扫描来supervise，或者lack 3D一致性。而最近的3D生成模型显示了人体数字化的3D一致性，但这些方法不能普遍应用于多样化的服装外表，并且lack photorealism。我们利用高容量2D扩散模型，这些模型在通用图像生成任务上进行预训练，作为人体服装的外观先验。为了实现更好的3D一致性而保持输入人物的身份，我们逐步synthesize多个视图的人体在输入图像中的缺失区域，使用形状响应的扩散条件和表面法向量来填充。然后，我们将这些合成的多视图图像进行反向渲染，以获得一个完全纹理高分辨率的3D mesh。实验表明，我们的方法超过了先前的方法，并实现了从单个图像中高真实度地生成360度的人体视图，包括复杂的服装表面。
</details></li>
</ul>
<hr>
<h2 id="DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model"><a href="#DMV3D-Denoising-Multi-View-Diffusion-using-3D-Large-Reconstruction-Model" class="headerlink" title="DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model"></a>DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09217">http://arxiv.org/abs/2311.09217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, Kai Zhang</li>
<li>for: The paper is written for proposing a novel 3D generation approach called DMV3D, which uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion.</li>
<li>methods: The paper uses a transformer-based 3D large reconstruction model that incorporates a triplane NeRF representation to denoise noisy multi-view images, achieving single-stage 3D generation in $\sim$30s on single A100 GPU.</li>
<li>results: The paper demonstrates state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. The paper also shows high-quality text-to-3D generation results outperforming previous 3D diffusion models.<details>
<summary>Abstract</summary>
We propose \textbf{DMV3D}, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and can denoise noisy multi-view images via NeRF reconstruction and rendering, achieving single-stage 3D generation in $\sim$30s on single A100 GPU. We train \textbf{DMV3D} on large-scale multi-view image datasets of highly diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. Our project website is at: https://justimyhxu.github.io/projects/dmv3d/ .
</details>
<details>
<summary>摘要</summary>
我们提出了\textbf{DMV3D}，一种新的3D生成方法，使用transformer基础的3D大量重建模型来去噪多视对冲测。我们的重建模型包括了三面NeRF表现，可以通过NeRF重建和渲染来去噪多视图像，实现单一阶段3D生成，需时约30秒在单一A100 GPU上。我们在大规模多视图像数据集上训练\textbf{DMV3D}，只使用图像重建损失，不需要访问3D资产。我们示出了单一图像重建问题中的前景，需要 probabilistic modeling of unseen object parts，以生成多样化的重建结果， texture sharpness。我们还展示了与前一代3D扩散模型相比，\textbf{DMV3D}可以实现高品质的文本到3D生成。我们的项目网站位于：https://justimyhxu.github.io/projects/dmv3d/ .
</details></li>
</ul>
<hr>
<h2 id="ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy"><a href="#ConvNet-vs-Transformer-Supervised-vs-CLIP-Beyond-ImageNet-Accuracy" class="headerlink" title="ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy"></a>ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09215">http://arxiv.org/abs/2311.09215</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kirill-vish/beyond-inet">https://github.com/kirill-vish/beyond-inet</a></li>
<li>paper_authors: Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu</li>
<li>for: 本研究旨在探讨现代计算机视觉模型的多样性，以及选择最佳模型 для特定应用场景的决策因素。</li>
<li>methods: 本研究采用了多种模型建构和训练方法，包括ConvNet和Vision Transformer两种主流建构，以及supervised和CLIP训练方法。</li>
<li>results: 研究发现，尽管选择的模型在ImageNet精度上具有相似性，但它们在其他方面存在显著差异，如输出准确率、类型错误、转移性和特征不变等。这些差异不可能由传统的精度metric fully capture。<details>
<summary>Abstract</summary>
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our code is available at https://github.com/kirill-vish/Beyond-INet.
</details>
<details>
<summary>摘要</summary>
现代计算机视觉提供了多种模型选择器，选择合适的模型对特定应用可以是挑战。传统上，竞争模型建筑和训练协议通常被比较通过ImageNet分类精度。然而，这个单一指标并不完全捕捉特有任务的性能细节。在这项工作中，我们进行了深入的比较分析，涵盖ConvNet和Vision Transformer建筑，每种模型在超vised和CLIP训练协议下的行为。虽然我们选择的模型在ImageNet精度和计算需求上几乎相同，但我们发现它们在很多方面不同：出错类型、输出准确、传输性和特征不变等方面。这些模型特性的多样性，不被传统指标捕捉，高亮了选择模型时需要更加细化的分析。我们的代码可以在https://github.com/kirill-vish/Beyond-INet上获取。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset"><a href="#Leveraging-Citizen-Science-for-Flood-Extent-Detection-using-Machine-Learning-Benchmark-Dataset" class="headerlink" title="Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset"></a>Leveraging Citizen Science for Flood Extent Detection using Machine Learning Benchmark Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09276">http://arxiv.org/abs/2311.09276</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muthukumaran Ramasubramanian, Iksha Gurung, Shubhankar Gahlot, Ronny Hänsch, Andrew L. Molthan, Manil Maskey</li>
<li>for: 这个论文的目的是为了提供一个高精度的洪水覆盖区域检测方法，以便在紧急应急响应和恢复工作中使用。</li>
<li>methods: 这个论文使用了卫星遥感数据，特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）图像，以检测洪水覆盖区域。由于SAR图像中水体的低反射率，因此可以准确地检测水体。然而，在某些洪水区域中，如果存在基础设施和树木等，会导致反射强度增加，使得简单的像素强度阈值和时间序列差分方法无法准确地检测洪水覆盖区域。因此，这个论文使用机器学习技术来准确地检测洪水覆盖区域。</li>
<li>results: 这个论文提供了一个标注了水体覆盖区域和洪水覆盖区域的 dataset，以及一个基eline模型和一个开源的 competedition，以推动洪水覆盖区域检测的研究。此外，这个论文还利用了公民科学，通过开源dataset和组织一个开源的比赛，以快速推进洪水覆盖区域检测的社区生成模型。<details>
<summary>Abstract</summary>
Accurate detection of inundated water extents during flooding events is crucial in emergency response decisions and aids in recovery efforts. Satellite Remote Sensing data provides a global framework for detecting flooding extents. Specifically, Sentinel-1 C-Band Synthetic Aperture Radar (SAR) imagery has proven to be useful in detecting water bodies due to low backscatter of water features in both co-polarized and cross-polarized SAR imagery. However, increased backscatter can be observed in certain flooded regions such as presence of infrastructure and trees - rendering simple methods such as pixel intensity thresholding and time-series differencing inadequate. Machine Learning techniques has been leveraged to precisely capture flood extents in flooded areas with bumps in backscatter but needs high amounts of labelled data to work desirably. Hence, we created a labeled known water body extent and flooded area extents during known flooding events covering about 36,000 sq. kilometers of regions within mainland U.S and Bangladesh. Further, We also leveraged citizen science by open-sourcing the dataset and hosting an open competition based on the dataset to rapidly prototype flood extent detection using community generated models. In this paper we present the information about the dataset, the data processing pipeline, a baseline model and the details about the competition, along with discussion on winning approaches. We believe the dataset adds to already existing datasets based on Sentinel-1C SAR data and leads to more robust modeling of flood extents. We also hope the results from the competition pushes the research in flood extent detection further.
</details>
<details>
<summary>摘要</summary>
“溢涌水域的扩散范围检测是紧急应急响应和恢复工作中的关键。卫星遥感数据提供了全球性的检测溢涌水域的方法。特别是Sentinel-1 C-Band Synthetic Aperture Radar（SAR）影像已经证明可以准确检测水体，因为水体具有低的反射强度。然而，某些淹没区域中的基础设施和树木会导致反射强度增加，使得简单的像素强度阈值和时间序列差异方法无法准确检测溢涌水域。机器学习技术已经被应用于准确地检测溢涌水域，但需要大量标注数据来工作。因此，我们创建了标注水体范围和淹没区域的known数据集，覆盖了美国大陆和孟加拉国的约36,000平方公里地区。此外，我们还利用公民科学，将数据集打包开源，并在该数据集基础上组织了一场公开竞赛，以快速搭建溢涌水域检测模型。本文介绍了数据集、数据处理管道、基线模型以及竞赛的详细信息，以及赢家的方法。我们认为该数据集将提高现有的Sentinel-1C SAR数据集，并且希望竞赛结果可以推动溢涌水域检测的研究进一步。”
</details></li>
</ul>
<hr>
<h2 id="Domain-Aligned-CLIP-for-Few-shot-Classification"><a href="#Domain-Aligned-CLIP-for-Few-shot-Classification" class="headerlink" title="Domain Aligned CLIP for Few-shot Classification"></a>Domain Aligned CLIP for Few-shot Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09191">http://arxiv.org/abs/2311.09191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Muhammad Waleed Gondal, Jochen Gast, Inigo Alonso Ruiz, Richard Droste, Tommaso Macri, Suren Kumar, Luitpold Staudigl</li>
<li>for: 提高CLIP模型在target分布上的预测性能，包括图像分类和OOD robustness。</li>
<li>methods: 提出了一种sample-efficient领域适应策略，称为Domain Aligned CLIP (DAC)，可以在target分布上提高图像-图像和图像-文本的同步，无需修改CLIP模型的参数。</li>
<li>results: 在11个广泛使用的图像分类任务上 demonstrates 16-shot classification的领先表现，相比强基eline的2.3%提高，并在4个OOD robustness benchmark上达到了竞争性表现。<details>
<summary>Abstract</summary>
Large vision-language representation learning models like CLIP have demonstrated impressive performance for zero-shot transfer to downstream tasks while largely benefiting from inter-modal (image-text) alignment via contrastive objectives. This downstream performance can further be enhanced by full-scale fine-tuning which is often compute intensive, requires large labelled data, and can reduce out-of-distribution (OOD) robustness. Furthermore, sole reliance on inter-modal alignment might overlook the rich information embedded within each individual modality. In this work, we introduce a sample-efficient domain adaptation strategy for CLIP, termed Domain Aligned CLIP (DAC), which improves both intra-modal (image-image) and inter-modal alignment on target distributions without fine-tuning the main model. For intra-modal alignment, we introduce a lightweight adapter that is specifically trained with an intra-modal contrastive objective. To improve inter-modal alignment, we introduce a simple framework to modulate the precomputed class text embeddings. The proposed few-shot fine-tuning framework is computationally efficient, robust to distribution shifts, and does not alter CLIP's parameters. We study the effectiveness of DAC by benchmarking on 11 widely used image classification tasks with consistent improvements in 16-shot classification upon strong baselines by about 2.3% and demonstrate competitive performance on 4 OOD robustness benchmarks.
</details>
<details>
<summary>摘要</summary>
大型视力语言表示学习模型如CLIP已经显示出Zero-shot传输下游任务的出色表现，主要受益于图像文本对Alignment via对比目标。然而，这种下游性能可以通过全面精细调整进一步提高，但这需要大量标注数据、计算昂贵和可能导致OOD不稳定性下降。此外，几乎完全依赖于图像文本对Alignment可能会忽略每个模式内的资源多样性。在这种情况下，我们提出了一种减少样本成本的领域适应策略，称为Domain Aligned CLIP（DAC），该策略可以在目标分布上提高图像图像和图像文本对Alignment，无需修改CLIP的主模型参数。为了提高图像图像对Alignment，我们引入了一个轻量级的适配器，该适配器专门通过图像对对比目标来训练。为了改善图像文本对Alignment，我们引入了一个简单的框架来修改预计算的类文本嵌入。我们的几个少量精细调整框架可以快速、稳定地进行，不需要大量标注数据，并且不会改变CLIP的参数。我们对11种常用的图像分类任务进行了测试，并 consistently obtain 16-shot classification improvements of around 2.3% over strong baselines，并且在4个OOD robustness benchmark上达到了竞争性的性能。
</details></li>
</ul>
<hr>
<h2 id="On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function"><a href="#On-the-Computation-of-the-Gaussian-Rate-Distortion-Perception-Function" class="headerlink" title="On the Computation of the Gaussian Rate-Distortion-Perception Function"></a>On the Computation of the Gaussian Rate-Distortion-Perception Function</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09190">http://arxiv.org/abs/2311.09190</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giuseppe Serra, Photios A. Stavrou, Marios Kountouris</li>
<li>for: 这个论文研究了多变量 Gaussian 源的 rate-distortion-perception 函数 (RDPF) 的计算，对于 mean squared error (MSE) 损害和各种抽象感知指标（Kullback-Leibler 异同、 геометрический Jensen-Shannon 异同、平方 Hellinger 距离和平方 Wasserstein-2 距离）。</li>
<li>methods: 作者首先计算了拟合函数的分析上下文 bound，并提供了 RDPF 实现的前向 “测试通道” 实现。在多变量情况下，作者表明了对于 tensorizable 损害和感知指标，优化解决方案 residues 在源协 variance 矩阵的 eigenvector 上。因此，多变量优化问题可以表示为一个约束的 scalar Gaussian RDPF 问题。</li>
<li>results: 作者提出了一种基于块非线性 Gauss-Seidel 方法的 alternating minimization 算法，可以优化多变量问题，同时 identificatinig RDPF 实现。此外，作者还提供了算法的实现、 converges 和 converge 速率的 Characterization。最后，作者在 “完美现实”  régime 下获得了多变量 Gaussian RDPF 的分析解。作者通过数值实验证明了结论，并与现有结果进行了比较。<details>
<summary>Abstract</summary>
In this paper, we study the computation of the rate-distortion-perception function (RDPF) for a multivariate Gaussian source under mean squared error (MSE) distortion and, respectively, Kullback-Leibler divergence, geometric Jensen-Shannon divergence, squared Hellinger distance, and squared Wasserstein-2 distance perception metrics. To this end, we first characterize the analytical bounds of the scalar Gaussian RDPF for the aforementioned divergence functions, also providing the RDPF-achieving forward "test-channel" realization. Focusing on the multivariate case, we establish that, for tensorizable distortion and perception metrics, the optimal solution resides on the vector space spanned by the eigenvector of the source covariance matrix. Consequently, the multivariate optimization problem can be expressed as a function of the scalar Gaussian RDPFs of the source marginals, constrained by global distortion and perception levels. Leveraging this characterization, we design an alternating minimization scheme based on the block nonlinear Gauss-Seidel method, which optimally solves the problem while identifying the Gaussian RDPF-achieving realization. Furthermore, the associated algorithmic embodiment is provided, as well as the convergence and the rate of convergence characterization. Lastly, for the "perfect realism" regime, the analytical solution for the multivariate Gaussian RDPF is obtained. We corroborate our results with numerical simulations and draw connections to existing results.
</details>
<details>
<summary>摘要</summary>
在本文中，我们研究了多变量 Gaussian 源的 computation rate-distortion-perception function (RDPF) 的计算，采用 Mean Squared Error (MSE) 损均、Kullback-Leibler 差分、Geometric Jensen-Shannon 差分、平方 Hellinger 距离和平方 Wasserstein-2 距离的感知度量。为此，我们首先计算了整数 Gaussian RDPF 的分析 bound，并提供了 RDPF 实现的前向 "测试通道" 实现。在多变量情况下，我们证明了，对于张量izable 损均和感知度量，最佳解在源协VARiance矩阵的 eigenvector 上。因此，多变量优化问题可以表示为各自 Gaussian RDPF 的源协VARiance矩阵的eigenvector 的函数，受到全局损均和感知水平的约束。我们利用这种特征来设计一种 alternating minimization 方案，基于块非线性 Gauss-Seidel 方法，可以最佳地解决问题，同时确定 Gaussian RDPF 实现。此外，我们还提供了算法实现、收敛性和收敛速度的特征。最后，在 "完美现实"  режиме下，我们获得了多变量 Gaussian RDPF 的分析解。我们的结果与数值仿真结果相符，并与现有结果进行了连接。
</details></li>
</ul>
<hr>
<h2 id="RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution"><a href="#RBPGAN-Recurrent-Back-Projection-GAN-for-Video-Super-Resolution" class="headerlink" title="RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution"></a>RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09178">http://arxiv.org/abs/2311.09178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Israa Fahmy, Marwah Sulaiman, Zahraa Shehabeldin, Mohammed Barakat, Dareen Hussein, Mohammed El-Naggar, Hesham Eraqi, Moustafa Youssef</li>
<li>for: 这个论文旨在提出一种Video超分辨（VSR）模型，以生成时间协调的解决方案，保持空间细节。</li>
<li>methods: 该模型结合了两个 state-of-the-art 模型， generator  inspirited by RBPN 系统， discriminator  inspirited by TecoGAN。使用 Ping-Pong 损失函数，提高时间一致性。</li>
<li>results: 我们的贡献使得模型在 temporally 一致的细节方面表现出色，证明了我们的模型在不同的数据集上的高性能。<details>
<summary>Abstract</summary>
Recently, video super resolution (VSR) has become a very impactful task in the area of Computer Vision due to its various applications. In this paper, we propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for VSR in an attempt to generate temporally coherent solutions while preserving spatial details. RBPGAN integrates two state-of-the-art models to get the best in both worlds without compromising the accuracy of produced video. The generator of the model is inspired by RBPN system, while the discriminator is inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal consistency over time. Our contribution together results in a model that outperforms earlier work in terms of temporally consistent details, as we will demonstrate qualitatively and quantitatively using different datasets.
</details>
<details>
<summary>摘要</summary>
近期，视频超解像 (VSR) 在计算机视觉领域已经变得非常重要，因为它具有许多应用。在这篇论文中，我们提出了 Recurrent Back-Projection Generative Adversarial Network (RBPGAN)，用于实现时间相关的解决方案，同时保持空间细节的准确性。RBPGAN 结合了两个状态的先进模型，以获得最佳的效果而无需牺牲生成的视频的准确性。生成器采用 RBPN 系统的设计，而批判器采用 TecoGAN 的设计。我们还使用了 Ping-Pong 损失函数，以增加时间一致性。我们的贡献结合使得模型在时间一致性和空间细节方面表现出色，我们将通过不同的数据集进行质量和量化的比较来证明。
</details></li>
</ul>
<hr>
<h2 id="WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification"><a href="#WildlifeDatasets-An-open-source-toolkit-for-animal-re-identification" class="headerlink" title="WildlifeDatasets: An open-source toolkit for animal re-identification"></a>WildlifeDatasets: An open-source toolkit for animal re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09118">http://arxiv.org/abs/2311.09118</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wildlifedatasets/wildlife-datasets">https://github.com/wildlifedatasets/wildlife-datasets</a></li>
<li>paper_authors: Vojtěch Čermák, Lukas Picek, Lukáš Adam, Kostas Papafitsoros</li>
<li>for: 本研究开发了一个开源工具箱（WildlifeDatasets），旨在帮助生物学家和计算机视觉&#x2F;机器学习研究人员访问公共可用的野生动物数据集，并提供了许多数据集预处理、性能分析和模型细化等方法。</li>
<li>methods: 本研究使用了Python编程语言，并提供了许多数据集预处理和性能分析方法，包括了本研究中的最全面的实验比较，涵盖了本地描述器和深度学习方法。此外，本研究还提供了一个基于描述器的个体重复识别模型——MegaDescriptor，该模型在野生动物重复识别数据集上实现了state-of-the-art性能，并在其他预训练模型 such as CLIP 和 DINOv2 中出众表现。</li>
<li>results: 本研究通过实验和比较，证明了MegaDescriptor模型在野生动物重复识别任务上具有state-of-the-art性能，并且在多种情况下都能够准确地识别动物个体。此外，本研究还提供了多种MegaDescriptor的不同版本（i.e., Small, Medium, and Large），通过HuggingFace hub（<a target="_blank" rel="noopener" href="https://huggingface.co/BVRA%EF%BC%89%E8%BF%9B%E8%A1%8C%E4%BA%86%E5%85%AC%E5%BC%80%E5%8F%91%E5%B8%83%EF%BC%8C%E4%BB%A5%E4%BE%BF%E6%98%93%E4%BA%8E%E4%B8%8E%E7%8E%B0%E6%9C%89%E7%9A%84%E9%87%8E%E7%94%9F%E5%8A%A8%E7%89%A9%E7%9B%91%E6%B5%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E9%9B%86%E6%88%90%E3%80%82">https://huggingface.co/BVRA）进行了公开发布，以便易于与现有的野生动物监测应用程序集成。</a><details>
<summary>Abstract</summary>
In this paper, we present WildlifeDatasets (https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source toolkit intended primarily for ecologists and computer-vision / machine-learning researchers. The WildlifeDatasets is written in Python, allows straightforward access to publicly available wildlife datasets, and provides a wide variety of methods for dataset pre-processing, performance analysis, and model fine-tuning. We showcase the toolkit in various scenarios and baseline experiments, including, to the best of our knowledge, the most comprehensive experimental comparison of datasets and methods for wildlife re-identification, including both local descriptors and deep learning approaches. Furthermore, we provide the first-ever foundation model for individual re-identification within a wide range of species - MegaDescriptor - that provides state-of-the-art performance on animal re-identification datasets and outperforms other pre-trained models such as CLIP and DINOv2 by a significant margin. To make the model available to the general public and to allow easy integration with any existing wildlife monitoring applications, we provide multiple MegaDescriptor flavors (i.e., Small, Medium, and Large) through the HuggingFace hub (https://huggingface.co/BVRA).
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了 WildlifeDatasets（https://github.com/WildlifeDatasets/wildlife-datasets）-一个开源工具箱，主要针对生态学家和计算机视觉/机器学习研究人员。WildlifeDatasets 是用 Python 编写的，可以直接访问公共可用的野生动物数据集，并提供了许多方法 для数据集预处理、性能分析和模型细化。我们在各种场景和基线实验中示例了这个工具箱，包括我们知道的最全面的野生动物重新识别实验，包括本地描述器和深度学习方法。此外，我们还提供了一个称之为 MegaDescriptor 的基本模型，该模型在各种种类的动物重新识别任务中提供了状态机器的性能，并在其他预训练模型 such as CLIP 和 DINOv2 的基础上出色的超越。为使这个模型对一般公众开放，并让它与现有的野生监测应用程序融合，我们在 HuggingFace 平台（https://huggingface.co/BVRA）提供了多种 MegaDescriptor 的FLAVOR（i.e., Small, Medium, and Large）。
</details></li>
</ul>
<hr>
<h2 id="Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding"><a href="#Cross-view-and-Cross-pose-Completion-for-3D-Human-Understanding" class="headerlink" title="Cross-view and Cross-pose Completion for 3D Human Understanding"></a>Cross-view and Cross-pose Completion for 3D Human Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09104">http://arxiv.org/abs/2311.09104</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthieu Armando, Salma Galaaoui, Fabien Baradel, Thomas Lucas, Vincent Leroy, Romain Brégier, Philippe Weinzaepfel, Grégory Rogez</li>
<li>for: The paper is written for the domain of computer vision, specifically for human perception and understanding.</li>
<li>methods: The paper proposes a pre-training approach based on self-supervised learning using human-centric data, including stereoscopic and temporal pairs of images.</li>
<li>results: The proposed method outperforms existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and achieves state-of-the-art performance for model-based and model-free human mesh recovery.Here’s the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是关注计算机视觉领域，具体是人体识别和理解。</li>
<li>methods: 这篇论文提议一种基于自我监督学习的预训练方法，使用人体中心的数据集，包括左右视图和时间视图对。</li>
<li>results: 提议的方法在多种人体中心下沉淀任务上超过现有的自我监督预训练方法，并在模型基于和模型外的人体网格恢复任务上达到了状态机器。<details>
<summary>Abstract</summary>
Human perception and understanding is a major domain of computer vision which, like many other vision subdomains recently, stands to gain from the use of large models pre-trained on large datasets. We hypothesize that the most common pre-training strategy of relying on general purpose, object-centric image datasets such as ImageNet, is limited by an important domain shift. On the other hand, collecting domain specific ground truth such as 2D or 3D labels does not scale well. Therefore, we propose a pre-training approach based on self-supervised learning that works on human-centric data using only images. Our method uses pairs of images of humans: the first is partially masked and the model is trained to reconstruct the masked parts given the visible ones and a second image. It relies on both stereoscopic (cross-view) pairs, and temporal (cross-pose) pairs taken from videos, in order to learn priors about 3D as well as human motion. We pre-train a model for body-centric tasks and one for hand-centric tasks. With a generic transformer architecture, these models outperform existing self-supervised pre-training methods on a wide set of human-centric downstream tasks, and obtain state-of-the-art performance for instance when fine-tuning for model-based and model-free human mesh recovery.
</details>
<details>
<summary>摘要</summary>
人类 восприятие和理解是计算机视觉的一大领域，与其他视觉子领域一样，它也可以受惠于大型模型在大量数据上进行预训练。我们假设，通过普通的对象中心图像 dataset such as ImageNet 进行预训练的方法，受到了重要的领域变化的限制。而收集专门适用于人类数据的预先知道ledge，如 2D 或 3D 标注，不可能扩展。因此，我们提议一种基于自我学习的预训练方法，使用人类数据图像。我们的方法使用人类图像的对称对（cross-view）和视频中的姿态对（cross-pose）进行自我学习，以学习人类的3D 和运动约束。我们预训练了一个身体中心任务的模型和一个手中心任务的模型，使用通用的 transformer 架构。这些模型在人类中心下沉天任务中表现出色，并在模型基于和模型自由人体碎片恢复任务中实现了国际级的表现。
</details></li>
</ul>
<hr>
<h2 id="Guided-Scale-Space-Radon-Transform-for-linear-structures-detection"><a href="#Guided-Scale-Space-Radon-Transform-for-linear-structures-detection" class="headerlink" title="Guided Scale Space Radon Transform for linear structures detection"></a>Guided Scale Space Radon Transform for linear structures detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09103">http://arxiv.org/abs/2311.09103</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aicha Baya Goumeidane, Djemel Ziou, Nafaa Nacereddine</li>
<li>for:  automaton detection of thick linear structures in gray scale and binary images</li>
<li>methods: 使用Scale Space Radon Transform (SSRT) 和计算图像的希尔曼方向的方法</li>
<li>results: 能够有效地检测图像中的不同厚度线条，并且具有鲁棒性 against noise and complex background<details>
<summary>Abstract</summary>
Using integral transforms to the end of lines detection in images with complex background, makes the detection a hard task needing additional processing to manage the detection. As an integral transform, the Scale Space Radon Transform (SSRT) suffers from such drawbacks, even with its great abilities for thick lines detection. In this work, we propose a method to address this issue for automatic detection of thick linear structures in gray scale and binary images using the SSRT, whatever the image background content. This method involves the calculated Hessian orientations of the investigated image while computing its SSRT, in such a way that linear structures are emphasized in the SSRT space. As a consequence, the subsequent maxima detection in the SSRT space is done on a modified transform space freed from unwanted parts and, consequently, from irrelevant peaks that usually drown the peaks representing lines. Besides, highlighting the linear structure in the SSRT space permitting, thus, to efficiently detect lines of different thickness in synthetic and real images, the experiments show also the method robustness against noise and complex background.
</details>
<details>
<summary>摘要</summary>
Our method involves calculating the Hessian orientations of the investigated image while computing its SSRT, which emphasizes linear structures in the SSRT space. As a result, the subsequent maxima detection in the SSRT space is done on a modified transform space free from unwanted parts and irrelevant peaks that usually drown the peaks representing lines. This approach allows us to efficiently detect lines of different thickness in synthetic and real images, and the experiments show the method's robustness against noise and complex backgrounds.
</details></li>
</ul>
<hr>
<h2 id="Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions"><a href="#Applications-of-Computer-Vision-in-Autonomous-Vehicles-Methods-Challenges-and-Future-Directions" class="headerlink" title="Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions"></a>Applications of Computer Vision in Autonomous Vehicles: Methods, Challenges and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09093">http://arxiv.org/abs/2311.09093</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingshuai Dong, Massimiliano L. Cappuccio</li>
<li>for: 本文主要旨在为读者提供自动驾驶技术的全面了解，包括自动驾驶系统的发展、感知器技术、benchmark数据集和公共评价等方面。</li>
<li>methods: 本文主要使用了computer vision技术，包括深度估计、物体检测、车道检测和交通标志识别等方面。同时，本文还对自动驾驶系统的发展进行了概述，包括主要汽车制造商从不同国家的开发。</li>
<li>results: 本文对自动驾驶技术的现状进行了全面的检视和分析，包括现有技术挑战和未来研究方向等。通过对自动驾驶技术的概述和分析，本文可以帮助读者更好地了解自动驾驶技术的发展和应用。<details>
<summary>Abstract</summary>
Autonomous vehicle refers to a vehicle capable of perceiving its surrounding environment and driving with little or no human driver input. The perception system is a fundamental component which enables the autonomous vehicle to collect data and extract relevant information from the environment to drive safely. Benefit from the recent advances in computer vision, the perception task can be achieved by using sensors, such as camera, LiDAR, radar, and ultrasonic sensor. This paper reviews publications on computer vision and autonomous driving that are published during the last ten years. In particular, we first investigate the development of autonomous driving systems and summarize these systems that are developed by the major automotive manufacturers from different countries. Second, we investigate the sensors and benchmark data sets that are commonly utilized for autonomous driving. Then, a comprehensive overview of computer vision applications for autonomous driving such as depth estimation, object detection, lane detection, and traffic sign recognition are discussed. Additionally, we review public opinions and concerns on autonomous vehicles. Based on the discussion, we analyze the current technological challenges that autonomous vehicles meet with. Finally, we present our insights and point out some promising directions for future research. This paper will help the reader to understand autonomous vehicles from the perspectives of academia and industry.
</details>
<details>
<summary>摘要</summary>
自动驾驶车辆指的是一种可以自主感知周围环境并减少或完全消除人类驾驶员输入的车辆。感知系统是自动驾驶车辆的基本组件，它使得自动驾驶车辆能够收集环境数据并提取有用信息以安全驾驶。受计算机视觉技术的推动，感知任务可以通过感知器，如摄像头、LiDAR、雷达和超声波感知器来实现。本文回顾过去十年发表的计算机视觉和自动驾驶相关的研究论文。特别是，我们首先调查了各国主要汽车制造商在自动驾驶领域的开发系统，然后调查了通用于自动驾驶的感知器和标准数据集。接着，我们提供了计算机视觉在自动驾驶中的应用篇幅，包括深度估计、物体检测、车道检测和交通标志识别。此外，我们还评估了自动驾驶车辆的公众观点和担忧，并分析了自动驾驶车辆目前所面临的技术挑战。最后，我们提出了一些有前途的研究方向。本文将帮助读者更好地理解自动驾驶车辆的 academia 和 industry 视角。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search"><a href="#Contrastive-Transformer-Learning-with-Proximity-Data-Generation-for-Text-Based-Person-Search" class="headerlink" title="Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search"></a>Contrastive Transformer Learning with Proximity Data Generation for Text-Based Person Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09084">http://arxiv.org/abs/2311.09084</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/personsearch-ctlg">https://github.com/hcplab-sysu/personsearch-ctlg</a></li>
<li>paper_authors: Hefeng Wu, Weifeng Chen, Zhibin Liu, Tianshui Chen, Zhiguang Chen, Liang Lin</li>
<li>for: 这个论文是为了提出一种简单 yet effective的双Transformer模型，用于图像库中的文本基于人脸检索。</li>
<li>methods: 该模型使用了一种具有强度感知的对比学习策略，以及一种自动生成数据模块（PDG），以提高模型的性能。</li>
<li>results: 实验表明，该模型在两个流行的TBPS数据集（CUHK-PEDES和ICFG-PEDES）上表现出色，与现有方法相比，提高了Top1、Top5、Top10的性能（例如，CUHK-PEDES上提高了3.88%, 4.02%, 2.92%）。<details>
<summary>Abstract</summary>
Given a descriptive text query, text-based person search (TBPS) aims to retrieve the best-matched target person from an image gallery. Such a cross-modal retrieval task is quite challenging due to significant modality gap, fine-grained differences and insufficiency of annotated data. To better align the two modalities, most existing works focus on introducing sophisticated network structures and auxiliary tasks, which are complex and hard to implement. In this paper, we propose a simple yet effective dual Transformer model for text-based person search. By exploiting a hardness-aware contrastive learning strategy, our model achieves state-of-the-art performance without any special design for local feature alignment or side information. Moreover, we propose a proximity data generation (PDG) module to automatically produce more diverse data for cross-modal training. The PDG module first introduces an automatic generation algorithm based on a text-to-image diffusion model, which generates new text-image pair samples in the proximity space of original ones. Then it combines approximate text generation and feature-level mixup during training to further strengthen the data diversity. The PDG module can largely guarantee the reasonability of the generated samples that are directly used for training without any human inspection for noise rejection. It improves the performance of our model significantly, providing a feasible solution to the data insufficiency problem faced by such fine-grained visual-linguistic tasks. Extensive experiments on two popular datasets of the TBPS task (i.e., CUHK-PEDES and ICFG-PEDES) show that the proposed approach outperforms state-of-the-art approaches evidently, e.g., improving by 3.88%, 4.02%, 2.92% in terms of Top1, Top5, Top10 on CUHK-PEDES. The codes will be available at https://github.com/HCPLab-SYSU/PersonSearch-CTLG
</details>
<details>
<summary>摘要</summary>
文本基于人搜索（TBPS）是一种跨模态检索任务，目的是从图库中检索最佳匹配的人Target。由于两个模态之间存在显著的差异和细化差异，以及缺乏标注数据，这种检索任务非常具有挑战性。大多数现有的方法都是通过引入复杂的网络结构和辅助任务来减轻这些差异，但这些方法通常具有复杂性和困难实现性。在本文中，我们提出了一种简单 yet effective的双Transformer模型，用于实现文本基于人搜索。我们通过利用一种困难性感知的对比学习策略，使我们的模型在无需特殊的本地特征对齐或副作用信息的情况下达到了状态艺术的表现。此外，我们还提出了一种 proximity data generation（PDG）模块，用于自动生成更多的跨模态数据。PDG模块首先通过基于文本到图像扩散模型的自动生成算法，生成了新的文本-图像对amples在原始对amples的邻近空间中。然后，它通过在训练时 combining approximate text generation和特征级混合来进一步增强数据多样性。PDG模块可以保证生成的样本的合理性，不需要人工检查噪声。这使得我们的模型表现得更好，提供了一种可行的解决方案 для跨模态任务中的数据不足问题。我们在两个流行的TBPS任务（i.e., CUHK-PEDES和ICFG-PEDES）上进行了广泛的实验，结果显示，我们的方法明显超越了现有的方法，例如，在CUHK-PEDES上提高了Top1、Top5、Top10的表现，升准3.88%、4.02%、2.92%。代码将在https://github.com/HCPLab-SYSU/PersonSearch-CTLG上提供。
</details></li>
</ul>
<hr>
<h2 id="Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation"><a href="#Spiking-NeRF-Representing-the-Real-World-Geometry-by-a-Discontinuous-Representation" class="headerlink" title="Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation"></a>Spiking NeRF: Representing the Real-World Geometry by a Discontinuous Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09077">http://arxiv.org/abs/2311.09077</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan</li>
<li>for: 提高 NeRF 方法的准确性，使其能够更好地捕捉物体的 geometry 和光学特性。</li>
<li>methods: 使用 spiking neuron 和 hybrid ANN-SNN 框架建立不连续的density field，以 faithful 地表示物体的geometry。</li>
<li>results: 实现了 SOTA 性能，并提供了数值关系 между spiking neuron 参数和理论准确性，以便进一步改进方法。<details>
<summary>Abstract</summary>
A crucial reason for the success of existing NeRF-based methods is to build a neural density field for the geometry representation via multiple perceptron layers (MLPs). MLPs are continuous functions, however, real geometry or density field is frequently discontinuous at the interface between the air and the surface. Such a contrary brings the problem of unfaithful geometry representation. To this end, this paper proposes spiking NeRF, which leverages spiking neuron and a hybrid Artificial Neural Network (ANN)-Spiking Neural Network (SNN) framework to build a discontinuous density field for faithful geometry representation. Specifically, we first demonstrate the reason why continuous density fields will bring inaccuracy. Then, we propose to use the spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry, Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.
</details>
<details>
<summary>摘要</summary>
<<SYS>>TRANSLATE_TEXT一个关键的原因导致现有的NeRF方法成功是通过多层感知核（MLP）建立神经density场来表示geometry。但是，实际的geometry或density场经常在空气和表面之间存在缺口，这会导致不准确的geometry表示。为解决这问题，本文提出了脊动NeRF，它利用脊动神经和混合人工神经网络（ANN）-脊动神经网络（SNN）框架来建立不连续的density场，以准确地表示geometry。 Specifically, we first demonstrate why continuous density fields will bring inaccuracy. Then, we propose to use spiking neurons to build a discontinuous density field. We conduct comprehensive analysis for the problem of existing spiking neuron models and then provide the numerical relationship between the parameter of spiking neuron and the theoretical accuracy of geometry. Based on this, we propose a bounded spiking neuron to build the discontinuous density field. Our results achieve SOTA performance. Our code and data will be released to the public.Note: SOTA stands for "State of the Art" in English, which means the current best performance in a particular field or task.
</details></li>
</ul>
<hr>
<h2 id="Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models"><a href="#Imagine-the-Unseen-World-A-Benchmark-for-Systematic-Generalization-in-Visual-World-Models" class="headerlink" title="Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models"></a>Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09064">http://arxiv.org/abs/2311.09064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongbin Kim, Gautam Singh, Junyeong Park, Caglar Gulcehre, Sungjin Ahn</li>
<li>for: 本文旨在提出一个新的基准测试系统，用于评估机器学习模型在视觉领域中的系统性协成能力。</li>
<li>methods: 本文使用了一种新的基准测试系统，称为视觉协成能力测试 benchmark (SVIB)，以评估模型在一种受控的世界动力下的一步图像转换能力。</li>
<li>results: 经过对多种基线模型的评估，本文发现现有的模型在系统性视觉协成能力方面存在一定的限制，并提出了一些可能的改进方向。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.
</details>
<details>
<summary>摘要</summary>
系统性的组合性，或者在新的情况下适应性的创建一个世界模型使用可重用的知识，仍然是机器学习领域的主要挑战。虽然在语言领域有了很大的进步，但对于视觉想象的系统atic imagination，尚未有充分的尝试。我们介绍了系统atic Visual Imagination Benchmark (SVIB)，第一个专门解决这个问题的benchmark。SVIB提供了一个新的世界模型设计问题，其中模型被评估根据它们在一个latent世界动力学下生成一步图像到图像变换的能力。这个框架具有许多优点，如同时优化系统atic perception和想象、多种难度水平和在训练中控制可能的因素组合的使用。我们对多种基eline模型在SVIB上进行了全面的评估，提供了现状的概况，并希望这个benchmark能够推动视觉系统atic compositionality的进步。
</details></li>
</ul>
<hr>
<h2 id="Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal"><a href="#Self-Annotated-3D-Geometric-Learning-for-Smeared-Points-Removal" class="headerlink" title="Self-Annotated 3D Geometric Learning for Smeared Points Removal"></a>Self-Annotated 3D Geometric Learning for Smeared Points Removal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09029">http://arxiv.org/abs/2311.09029</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miaowei Wang, Daniel Morris</li>
<li>for: 本研究旨在提高顾客级束缚精密深度感知器的准确性和质量，并解决雷达点精灵抽象的问题。</li>
<li>methods: 我们提出了一种完全自我标注的方法，利用多视角 geometric evidence 自动检测和标注精灵点和有效点。</li>
<li>results: 我们的方法在实验和减少研究中表现出色，超过了传统的滤波器和其他自我标注方法。In simpler English, the paper aims to improve the accuracy and quality of consumer-level depth sensors and solve the problem of “smeared points” (points that are not on any 3D surface and can cause errors in depth maps). The proposed method uses fully self-annotated training data and relies on 3D geometric evidence from multiple perspectives to detect and remove smeared points. Experimental results show that the method outperforms traditional filters and other self-annotated methods.<details>
<summary>Abstract</summary>
There has been significant progress in improving the accuracy and quality of consumer-level dense depth sensors. Nevertheless, there remains a common depth pixel artifact which we call smeared points. These are points not on any 3D surface and typically occur as interpolations between foreground and background objects. As they cause fictitious surfaces, these points have the potential to harm applications dependent on the depth maps. Statistical outlier removal methods fare poorly in removing these points as they tend also to remove actual surface points. Trained network-based point removal faces difficulty in obtaining sufficient annotated data. To address this, we propose a fully self-annotated method to train a smeared point removal classifier. Our approach relies on gathering 3D geometric evidence from multiple perspectives to automatically detect and annotate smeared points and valid points. To validate the effectiveness of our method, we present a new benchmark dataset: the Real Azure-Kinect dataset. Experimental results and ablation studies show that our method outperforms traditional filters and other self-annotated methods. Our work is publicly available at https://github.com/wangmiaowei/wacv2024_smearedremover.git.
</details>
<details>
<summary>摘要</summary>
“随着对消费者级数 dense depth sensor 的改进，有所进步。然而，还有一个常见的深度像素错误，我们称之为“扩散点”。这些点不在任何三维表面上，通常发生在前景和背景物体之间的插值。由于这些点会创建虚拟表面，这些点有可能伤害对深度地图依赖的应用。统计方法对这些点进行排除是不具有效果的，因为它们也可能会 removes 真正的表面点。训练网络基于的方法也难以获得足够的标注数据。为解决这个问题，我们提出了一个完全自我标注的方法，用于训练深度扩散点移除分类器。我们的方法基于从多个角度收集3D几何证据，以自动检测和标注深度扩散点和有效点。为 validate 我们的方法，我们提供了一个新的库 benchmark 数据集：Real Azure-Kinect 数据集。实验结果和删除研究显示，我们的方法比传统范例和其他自我标注方法高效。我们的工作公开在 GitHub 上，请参考 https://github.com/wangmiaowei/wacv2024_smearedremover.git。”
</details></li>
</ul>
<hr>
<h2 id="Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing"><a href="#Fast-Certification-of-Vision-Language-Models-Using-Incremental-Randomized-Smoothing" class="headerlink" title="Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing"></a>Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09024">http://arxiv.org/abs/2311.09024</a></li>
<li>repo_url: None</li>
<li>paper_authors: A K Nirala, A Joshi, C Hegde, S Sarkar</li>
<li>for: 这个论文旨在提出一种快速的开放词汇识别模型认证方法，以确保这些模型在实际应用中的可靠性。</li>
<li>methods: 这种认证方法基于随机缓和技术，使用基于训练集的Certified CLIP classifier来快速认证novel prompts。</li>
<li>results: 实验结果表明，OVC可以快速认证开放词汇模型，并且可以在CIFAR-10和ImageNet测试 datasets上达到比较高的识别率。<details>
<summary>Abstract</summary>
A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild.   In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base "training" set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.
</details>
<details>
<summary>摘要</summary>
CLIP 深度视力语言模型具有零shot开 vocabulary 分类的能力，即在运行时通过自然语言提示来定义新的分类标签。然而， CLIP 基于的零shot 分类器对于攻击而言是极为易受攻击的。因此，为了可靠地部署这些模型，确保其 Robustness 是非常重要的。在这项工作中，我们介绍了一种叫做 Open Vocabulary Certification (OVC) 的快速证明方法，用于验证开 vocabulary 模型如 CLIP。OVC 基于的观察是，一个新的提示可以视为 nearby 的基础训练集中的类ifier 的噪声版本。因此，OVC 可以快速地证明这个新的类ifier 使用随机噪声技术。使用缓存技巧，我们实现了约两个数量级的加速。此外，OVC 使用一种快速的方法来 Approximate 输入空间的 embedding 空间，从而缩短证明过程。我们通过对多个视力语言背景进行实验评估，证明了 OVC 的有效性。
</details></li>
</ul>
<hr>
<h2 id="Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop"><a href="#Incremental-Object-Based-Novelty-Detection-with-Feedback-Loop" class="headerlink" title="Incremental Object-Based Novelty Detection with Feedback Loop"></a>Incremental Object-Based Novelty Detection with Feedback Loop</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09004">http://arxiv.org/abs/2311.09004</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Caldarella, Elisa Ricci, Rahaf Aljundi</li>
<li>for: 本研究旨在提高对象检测模型的不可预测对象检测能力，以避免在实际应用中可能存在危害的行为，如自驾车或自主机器人中使用的对象检测模型。</li>
<li>methods: 本研究提出了一种基于人工反馈的对象新型检测方法，假设可以在预测输出中请求人工反馈，并在反馈可用时进行不间断的改进。为解决这种新的对象检测问题，我们提出了一个轻量级的ND模块，附加在已经训练的对象检测模型之上，并通过反馈循环进行不断更新。</li>
<li>results: 我们的实验表明，我们的ND方法可以增强对象检测模型的Robustness，并成功地收集和 incorporate 人工反馈。我们还提出了一个新的评价指标，用于评价对象检测模型的新型检测能力，并进行了广泛的比较试验，以证明我们的ND方法的效果。<details>
<summary>Abstract</summary>
Object-based Novelty Detection (ND) aims to identify unknown objects that do not belong to classes seen during training by an object detection model. The task is particularly crucial in real-world applications, as it allows to avoid potentially harmful behaviours, e.g. as in the case of object detection models adopted in a self-driving car or in an autonomous robot. Traditional approaches to ND focus on one time offline post processing of the pretrained object detection output, leaving no possibility to improve the model robustness after training and discarding the abundant amount of out-of-distribution data encountered during deployment.   In this work, we propose a novel framework for object-based ND, assuming that human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test extensively our ND approach against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a novel framework for object-based ND that assumes human feedback can be requested on the predicted output and later incorporated to refine the ND model without negatively affecting the main object detection performance. This refinement operation is repeated whenever new feedback is available. To tackle this new formulation of the problem for object detection, we propose a lightweight ND module attached on top of a pre-trained object detection model, which is incrementally updated through a feedback loop. We also propose a new benchmark to evaluate methods on this new setting and test our ND approach extensively against baselines, showing increased robustness and a successful incorporation of the received feedback.
</details></li>
</ul>
<hr>
<h2 id="Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images"><a href="#Simple-but-Effective-Unsupervised-Classification-for-Specified-Domain-Images-A-Case-Study-on-Fungi-Images" class="headerlink" title="Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images"></a>Simple but Effective Unsupervised Classification for Specified Domain Images: A Case Study on Fungi Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08995">http://arxiv.org/abs/2311.08995</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhaocong liu, Fa Zhang, Lin Cheng, Huanxi Deng, Xiaoyan Yang, Zhenyu Zhang, Chichun Zhou</li>
<li>for: 特别适用于需要专业知识的域面图像分类任务，解决高质量标注数据的缺乏问题。</li>
<li>methods: 使用自动Feature extraction方法，并利用多种 clustering 算法投票来实现无监督分类。</li>
<li>results: 在 fungal 图像数据集上达到 94.1% 和 96.7% 的分类精度，比超级vised方法高。这种无监督分类方法可以减少依赖于预先标注数据，提供closed-loop数据分类。<details>
<summary>Abstract</summary>
High-quality labeled datasets are essential for deep learning. Traditional manual annotation methods are not only costly and inefficient but also pose challenges in specialized domains where expert knowledge is needed. Self-supervised methods, despite leveraging unlabeled data for feature extraction, still require hundreds or thousands of labeled instances to guide the model for effective specialized image classification. Current unsupervised learning methods offer automatic classification without prior annotation but often compromise on accuracy. As a result, efficiently procuring high-quality labeled datasets remains a pressing challenge for specialized domain images devoid of annotated data. Addressing this, an unsupervised classification method with three key ideas is introduced: 1) dual-step feature dimensionality reduction using a pre-trained model and manifold learning, 2) a voting mechanism from multiple clustering algorithms, and 3) post-hoc instead of prior manual annotation. This approach outperforms supervised methods in classification accuracy, as demonstrated with fungal image data, achieving 94.1% and 96.7% on public and private datasets respectively. The proposed unsupervised classification method reduces dependency on pre-annotated datasets, enabling a closed-loop for data classification. The simplicity and ease of use of this method will also bring convenience to researchers in various fields in building datasets, promoting AI applications for images in specialized domains.
</details>
<details>
<summary>摘要</summary>
高品质标注数据是深度学习的关键。传统的手动标注方法不仅成本高、效率低，还存在专业领域中的知识问题。无监督方法，尽管利用无标注数据进行特征提取，仍需要数百或千个标注实例来引导模型以实现特殊领域图像分类。当前无监督学习方法可以自动分类而无需先前的标注，但通常会 compromise 准确性。因此，得到高品质标注数据仍然是特殊领域图像无标注数据的Pressing challenge。为解决这个问题，我们提出了一种无监督分类方法，包括三个关键想法：1）使用预训练模型和拟合学习来实现双步特征维度减少，2）多种 clustering 算法的投票机制，和3）post-hoc 而不是先前的手动标注。这种方法在分类准确率方面超过了supervised方法，如 demonstrated 在菌类图像数据上，实现了94.1%和96.7%的公共和私人数据集分类率。我们的无监督分类方法减少了对前置标注数据的依赖，使得数据分类成为closed-loop。这种简单易用的方法会将研究人员在多个领域建立数据集，推动人工智能应用于特殊领域图像。
</details></li>
</ul>
<hr>
<h2 id="Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging"><a href="#Unsupervised-approaches-based-on-optimal-transport-and-convex-analysis-for-inverse-problems-in-imaging" class="headerlink" title="Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging"></a>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08972">http://arxiv.org/abs/2311.08972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcello Carioni, Subhadip Mukherjee, Hong Ye Tan, Junqi Tang</li>
<li>for: This paper focuses on theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis.</li>
<li>methods: The paper reviews optimal transport-based unsupervised approaches, learned adversarial regularization methods, provably convergent learned optimization algorithms, and plug-and-play algorithms for imaging problems.</li>
<li>results: The paper provides an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep the discussion self-contained.<details>
<summary>Abstract</summary>
Unsupervised deep learning approaches have recently become one of the crucial research areas in imaging owing to their ability to learn expressive and powerful reconstruction operators even when paired high-quality training data is scarcely available. In this chapter, we review theoretically principled unsupervised learning schemes for solving imaging inverse problems, with a particular focus on methods rooted in optimal transport and convex analysis. We begin by reviewing the optimal transport-based unsupervised approaches such as the cycle-consistency-based models and learned adversarial regularization methods, which have clear probabilistic interpretations. Subsequently, we give an overview of a recent line of works on provably convergent learned optimization algorithms applied to accelerate the solution of imaging inverse problems, alongside their dedicated unsupervised training schemes. We also survey a number of provably convergent plug-and-play algorithms (based on gradient-step deep denoisers), which are among the most important and widely applied unsupervised approaches for imaging problems. At the end of this survey, we provide an overview of a few related unsupervised learning frameworks that complement our focused schemes. Together with a detailed survey, we provide an overview of the key mathematical results that underlie the methods reviewed in the chapter to keep our discussion self-contained.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_language: zh-CN<</SYS>>无监督深度学习方法在媒体领域取得了重要突破，尤其是在数据质量较高的时候，它们能够学习表达力强的重建运算符。在这章中，我们将评论理论上正确的无监督学习方案，用于解决媒体领域的反向问题，特别是基于最优运输和凸分析的方法。我们首先介绍循环一致性基于的模型和学习抑制方法，这些方法具有明确的概率解释。接着，我们将讲解最近一些可靠地训练无监督算法，以加速媒体领域的反向问题解决。此外，我们还介绍了一些可靠地插入执行的无监督算法（基于梯度步深排除器），它们是媒体领域中最重要和最广泛应用的无监督方法。 finally，我们将介绍一些与我们关注的无监督学习框架，以及这些方法的关键数学结论，以便保持我们的讨论自 contenido。
</details></li>
</ul>
<hr>
<h2 id="A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution"><a href="#A-Spectral-Diffusion-Prior-for-Hyperspectral-Image-Super-Resolution" class="headerlink" title="A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution"></a>A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08955">http://arxiv.org/abs/2311.08955</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianjun Liu, Zebin Wu, Liang Xiao</li>
<li>for:  fusion-based hyperspectral image (HSI) super-resolution</li>
<li>methods: spectral diffusion prior, maximum a posteriori, Adam optimization</li>
<li>results: effective in producing high-spatial-resolution HSI, demonstrated on both synthetic and real datasetsHere’s the simplified Chinese text:</li>
<li>for: 高分辨率多spectral影像（HSI）超解析</li>
<li>methods:  spectral diffusion prior, maximum a posteriori, Adam优化</li>
<li>results: 高效地生成高分辨率HSI, 在synthetic和实际数据上进行了实验<details>
<summary>Abstract</summary>
Fusion-based hyperspectral image (HSI) super-resolution aims to produce a high-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a high-spatial-resolution multispectral image. Such a HSI super-resolution process can be modeled as an inverse problem, where the prior knowledge is essential for obtaining the desired solution. Motivated by the success of diffusion models, we propose a novel spectral diffusion prior for fusion-based HSI super-resolution. Specifically, we first investigate the spectrum generation problem and design a spectral diffusion model to model the spectral data distribution. Then, in the framework of maximum a posteriori, we keep the transition information between every two neighboring states during the reverse generative process, and thereby embed the knowledge of trained spectral diffusion model into the fusion problem in the form of a regularization term. At last, we treat each generation step of the final optimization problem as its subproblem, and employ the Adam to solve these subproblems in a reverse sequence. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The code of the proposed approach will be available on https://github.com/liuofficial/SDP.
</details>
<details>
<summary>摘要</summary>
融合基于快照影像（HSI）超分辨率目标是生成高空间分辨率HSI，通过融合低空间分辨率HSI和高空间分辨率多spectral影像。这种HSI超分辨率过程可以表示为一个逆问题，其中假设知识是获得所需解决方案的关键。鼓动 diffusion模型的成功，我们提出了一种新的 spectral diffusion prior для融合基于HSI超分辨率。specifically，我们首先调查spectrum生成问题，并设计了一种spectral diffusion模型来模拟spectral数据分布。然后，在maximum a posteriori框架中，我们保留了每两个邻居状态之间的过渡信息，并将这些知识嵌入到融合问题中，以形式化一个正则化项。最后，我们对每个生成步骤的最终优化问题进行分解，并使用Adam算法解决这些子问题。实验结果表明，我们提出的方法在synthetic和实际数据集上具有效果。code的github地址为https://github.com/liuofficial/SDP.
</details></li>
</ul>
<hr>
<h2 id="Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard"><a href="#Automated-Volume-Corrected-Mitotic-Index-Calculation-Through-Annotation-Free-Deep-Learning-using-Immunohistochemistry-as-Reference-Standard" class="headerlink" title="Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard"></a>Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08949">http://arxiv.org/abs/2311.08949</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Ammeling, Moritz Hecker, Jonathan Ganz, Taryn A. Donovan, Christof A. Bertram, Katharina Breininger, Marc Aubreville</li>
<li>for:  This paper is written for assessing the prognostic value of invasive breast carcinomas using a deep learning-based approach.</li>
<li>methods:  The paper uses a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach to estimate epithelial segmentation in canine breast carcinomas.</li>
<li>results:  The deep learning-based pipeline shows expert-level performance, providing time efficiency and reproducibility, compared to the manually annotated M&#x2F;V-Index.<details>
<summary>Abstract</summary>
The volume-corrected mitotic index (M/V-Index) was shown to provide prognostic value in invasive breast carcinomas. However, despite its prognostic significance, it is not established as the standard method for assessing aggressive biological behaviour, due to the high additional workload associated with determining the epithelial proportion. In this work, we show that using a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach, provides accurate estimations of epithelial segmentation in canine breast carcinomas. We compare our automatic framework with the manually annotated M/V-Index in a study with three board-certified pathologists. Our results indicate that the deep learning-based pipeline shows expert-level performance, while providing time efficiency and reproducibility.
</details>
<details>
<summary>摘要</summary>
“对入侵性乳癌中的肉眼癌指数（M/V-Index）的评估，有过往的研究显示其具有预后价值。然而，尽管其预后意义，但它并未被视为标准的评估具有攻击性生物行为的方法，因为需要额外的努力来决定胞质含量。在这个工作中，我们展示了一个使用深度学习管线仅以无标注、免疫抗体技术为基础的架构，可以实时和可重复地估算乳癌组织中的胞质分布。我们与三位美国医学会认证的病理学家进行比较，结果显示，我们的深度学习架构具有专家水准的表现，同时提供时间效益和可重复性。”Note that the translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness"><a href="#Confident-Naturalness-Explanation-CNE-A-Framework-to-Explain-and-Assess-Patterns-Forming-Naturalness" class="headerlink" title="Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness"></a>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08936">http://arxiv.org/abs/2311.08936</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Mohamed Farag, Ribana Roscher</li>
<li>for:  This paper aims to improve the understanding and mapping of naturalness within protected natural areas using machine learning and explainability techniques.</li>
<li>methods:  The proposed Confident Naturalness Explanation (CNE) framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness, using a new quantitative metric and uncertainty-aware segmentation masks.</li>
<li>results:  The proposed CNE framework is demonstrated to be effective in a study site in Fennoscandia using two open-source satellite datasets, providing confident and objective explanations of naturalness.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是使用机器学习和可解释技术来提高保护自然区域中自然性的理解和地图。</li>
<li>methods: 提议的Confident Naturalness Explanation（CNE）框架结合可解释机器学习和不确定量化来评估和解释自然性，使用新的量化度量和不确定度映射。</li>
<li>results: 在芬兰地区使用两个开源卫星数据集，通过应用CNE框架，实现了对自然性的可靠和客观解释。<details>
<summary>Abstract</summary>
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness. We introduce a new quantitative metric that describes the confident contribution of patterns to the concept of naturalness. Furthermore, we generate an uncertainty-aware segmentation mask for each input sample, highlighting areas where the model lacks knowledge. To demonstrate the effectiveness of our framework, we apply it to a study site in Fennoscandia using two open-source satellite datasets.
</details>
<details>
<summary>摘要</summary>
保护的自然区域是人类活动影响的最小化区域，如城市化、农业等。为了更好地理解和映射这些区域的自然性，机器学习模型可以使用卫星图像进行分析。特别是使用可解释机器学习方法可以揭示保护区域中自然性的特征。然而，现有的方法有限制。它们可能无法提供有效和客观的解释，或者困难提供准确度量自然性的贡献和相关信息。在这篇论文中，我们提出了一种新的框架，即可靠自然性解释（CNE）框架。这个框架结合可解释机器学习和不确定量化来评估和解释自然性。我们还提出了一个新的量化度量，用于描述模型对自然性的可靠贡献。此外，我们生成了每个输入样本的不确定性感知分割图，以标识模型对具体区域的不确定性。为了证明我们的框架的效果，我们对芬兰北部的一个研究区使用了两个开源卫星数据集进行应用。
</details></li>
</ul>
<hr>
<h2 id="Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation"><a href="#Structural-Based-Uncertainty-in-Deep-Learning-Across-Anatomical-Scales-Analysis-in-White-Matter-Lesion-Segmentation" class="headerlink" title="Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation"></a>Structural-Based Uncertainty in Deep Learning Across Anatomical Scales: Analysis in White Matter Lesion Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08931">http://arxiv.org/abs/2311.08931</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/medical-image-analysis-laboratory/ms_wml_uncs">https://github.com/medical-image-analysis-laboratory/ms_wml_uncs</a></li>
<li>paper_authors: Nataliia Molchanova, Vatsal Raina, Andrey Malinin, Francesco La Rosa, Adrien Depeursinge, Mark Gales, Cristina Granziera, Henning Muller, Mara Graziani, Meritxell Bach Cuadra</li>
<li>for: 这个论文探讨了自动深度学习（DL）工具的可靠性量化（UQ）在多发性脑梗液病人（MS）的磁共振成像（MRI）扫描中的白 matter损伤（WML）分 segmentation任务中的作用。</li>
<li>methods: 我们的研究主要集中在两个主要的不确定性问题上：首先，我们认为一个好的不确定性度量应该指示预测有高度不确定性的值。其次，我们 investigate了不确定性在不同的 анаatomical scale（ voxel、 lesion 或 patient）之间的关系。我们认为不确定性在每个缩放级别都与特定类型的错误有关。</li>
<li>results: 我们的研究结果表明，我们提出的方法可以更好地捕捉模型错误在 lesion 和 patient 缩放级别上，比 tradicional voxel-scale uncertainty 值的平均值。我们在一个多中心 MRI 数据集上进行了172名病人的研究，并提供了 UQ 协议代码在 GitHub 上（<a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs%EF%BC%89%E3%80%82">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs）。</a><details>
<summary>Abstract</summary>
This paper explores uncertainty quantification (UQ) as an indicator of the trustworthiness of automated deep-learning (DL) tools in the context of white matter lesion (WML) segmentation from magnetic resonance imaging (MRI) scans of multiple sclerosis (MS) patients. Our study focuses on two principal aspects of uncertainty in structured output segmentation tasks. Firstly, we postulate that a good uncertainty measure should indicate predictions likely to be incorrect with high uncertainty values. Second, we investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors. Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are (i) the development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies, and (ii) the extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales. The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. We provide the UQ protocols code at https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>We propose that a good uncertainty measure should indicate predictions that are likely to be incorrect with high uncertainty values.2. We investigate the merit of quantifying uncertainty at different anatomical scales (voxel, lesion, or patient). We hypothesize that uncertainty at each scale is related to specific types of errors.Our study aims to confirm this relationship by conducting separate analyses for in-domain and out-of-domain settings. Our primary methodological contributions are:1. The development of novel measures for quantifying uncertainty at lesion and patient scales, derived from structural prediction discrepancies.2. The extension of an error retention curve analysis framework to facilitate the evaluation of UQ performance at both lesion and patient scales.The results from a multi-centric MRI dataset of 172 patients demonstrate that our proposed measures more effectively capture model errors at the lesion and patient scales compared to measures that average voxel-scale uncertainty values. The UQ protocols code is available at <a target="_blank" rel="noopener" href="https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs">https://github.com/Medical-Image-Analysis-Laboratory/MS_WML_uncs</a>.</details></li>
</ol>
<hr>
<h2 id="Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization"><a href="#Progressive-Feedback-Enhanced-Transformer-for-Image-Forgery-Localization" class="headerlink" title="Progressive Feedback-Enhanced Transformer for Image Forgery Localization"></a>Progressive Feedback-Enhanced Transformer for Image Forgery Localization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08910">http://arxiv.org/abs/2311.08910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haochen Zhu, Gang Cao, Xianglin Huang</li>
<li>for: 本研究旨在提出一种 Progressive FeedbACk-enhanced Transformer (ProFact) 网络，用于提高图像forge localization的精度和可靠性。</li>
<li>methods: 该网络使用了一个初始分支网络生成的粗略定位图，并将其FeedbACk到早期的 transformer 嵌入层以增强正面特征表示，同时抑制干扰因素。此外，还提出了一种 Contextual Spatial Pyramid 模块，用于进一步提高医学特征的涵盖率和分辨率。</li>
<li>results: 对于九个公共的医学检测数据集，我们的提出的定位器表现出色，在泛化能力和Robustness方面都大幅超越了当前状态。<details>
<summary>Abstract</summary>
Blind detection of the forged regions in digital images is an effective authentication means to counter the malicious use of local image editing techniques. Existing encoder-decoder forensic networks overlook the fact that detecting complex and subtle tampered regions typically requires more feedback information. In this paper, we propose a Progressive FeedbACk-enhanced Transformer (ProFact) network to achieve coarse-to-fine image forgery localization. Specifically, the coarse localization map generated by an initial branch network is adaptively fed back to the early transformer encoder layers for enhancing the representation of positive features while suppressing interference factors. The cascaded transformer network, combined with a contextual spatial pyramid module, is designed to refine discriminative forensic features for improving the forgery localization accuracy and reliability. Furthermore, we present an effective strategy to automatically generate large-scale forged image samples close to real-world forensic scenarios, especially in realistic and coherent processing. Leveraging on such samples, a progressive and cost-effective two-stage training protocol is applied to the ProFact network. The extensive experimental results on nine public forensic datasets show that our proposed localizer greatly outperforms the state-of-the-art on the generalization ability and robustness of image forgery localization. Code will be publicly available at https://github.com/multimediaFor/ProFact.
</details>
<details>
<summary>摘要</summary>
“针对本地图像修改技术的恶意使用，潜意检测数字图像中的假造区域是一种有效的身份验证手段。现有的编oder-解码器审计网络忽视了检测复杂且微妙的假造区域通常需要更多的反馈信息。本文提出了一种Progressive FeedbACk-enhanced Transformer（ProFact）网络，以实现从粗到细图像假造地点检测。特别是，初始分支网络生成的粗略地点映射被适应地feedback到早期的Transformer编码层，以增强正面特征表示，同时抑制干扰因素。另外，我们设计了一个Contextual Spatial Pyramid模块，用于修改审计特征，以提高假造地点检测精度和可靠性。此外，我们还提出了一种有效的自动生成大规模假造图像样本close to real-world审计enario，特别是在realistic和coherent处理中。基于这些样本，我们采用了一种进程ive和cost-effective的两阶段训练 protocole。我们的实验结果表明，我们提出的检测器在通用性和Robustness两个方面都有出色的表现，超过了当前state-of-the-art。代码将在https://github.com/multimediaFor/ProFact中公开。”
</details></li>
</ul>
<hr>
<h2 id="DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack"><a href="#DLAS-An-Exploration-and-Assessment-of-the-Deep-Learning-Acceleration-Stack" class="headerlink" title="DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack"></a>DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08909">http://arxiv.org/abs/2311.08909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Perry Gibson, José Cano, Elliot J. Crowley, Amos Storkey, Michael O’Boyle</li>
<li>for: 这个论文的目的是提供一个参考框架，帮助机器学习和系统实践者在实现深度学习推进运算时，更好地考虑各个层次的依赖关系。</li>
<li>methods: 这个论文使用了机器学习和系统技术，建立了深度学习加速框架（DLAS），并对DLAS进行了逐层次的干扰研究，以探索各个层次之间的依赖关系。</li>
<li>results: 这个论文的评估结果显示，DLAS的各个层次之间存在许多依赖关系，而且这些关系可以通过干扰DLAS的各个层次来变化。此外，论文还发现了一些实际上的规律，例如压缩技术的加速效益是具体设备依赖的，以及自动调整代码生成可以对最佳化器的选择产生重大影响。<details>
<summary>Abstract</summary>
Deep Neural Networks (DNNs) are extremely computationally demanding, which presents a large barrier to their deployment on resource-constrained devices. Since such devices are where many emerging deep learning applications lie (e.g., drones, vision-based medical technology), significant bodies of work from both the machine learning and systems communities have attempted to provide optimizations to accelerate DNNs. To help unify these two perspectives, in this paper we combine machine learning and systems techniques within the Deep Learning Acceleration Stack (DLAS), and demonstrate how these layers can be tightly dependent on each other with an across-stack perturbation study. We evaluate the impact on accuracy and inference time when varying different parameters of DLAS across two datasets, seven popular DNN architectures, four DNN compression techniques, three algorithmic primitives with sparse and dense variants, untuned and auto-scheduled code generation, and four hardware platforms. Our evaluation highlights how perturbations across DLAS parameters can cause significant variation and across-stack interactions. The highest level observation from our evaluation is that the model size, accuracy, and inference time are not guaranteed to be correlated. Overall we make 13 key observations, including that speedups provided by compression techniques are very hardware dependent, and that compiler auto-tuning can significantly alter what the best algorithm to use for a given configuration is. With DLAS, we aim to provide a reference framework to aid machine learning and systems practitioners in reasoning about the context in which their respective DNN acceleration solutions exist in. With our evaluation strongly motivating the need for co-design, we believe that DLAS can be a valuable concept for exploring the next generation of co-designed accelerated deep learning solutions.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNNs）非常 computationally 需求强大，这使得它们在有限资源的设备上部署变得困难。由于这些设备是许多深度学习应用程序的核心（如无人机、视觉基于医疗技术），因此机器学习和系统社区中的大量工作尝试了加速DNNs。为了统一这两个视角，在这篇论文中我们将机器学习和系统技术融合在一起，并通过各层之间的扰动研究表明了它们之间的互相依赖关系。我们对两个数据集、七种流行的DNN架构、四种DNN压缩技术、三种算法基本primitives的稀热和杂散变体、未调uning和自动生成代码生成、四种硬件平台进行了评估。我们的评估表明，在不同的DAS Parameters中，可以导致显著的变化和层之间的互动。最高层的观察结论是，模型大小、准确率和执行时间之间不一定是相关的。总的来说，我们所得到的13个观察结论，其中一些包括压缩技术在不同硬件平台上提供的加速效果是非常硬件依赖的，并且编译器自动调试可以很大地改变选择最佳算法的配置是否正确。通过DAS，我们希望提供一个参考框架，帮助机器学习和系统专家更好地理解它们的DNN加速解决方案在不同上下文中的运行环境。我们的评估强烈驱动了需要的共设计，我们认为DAS可以成为下一代共设计加速深度学习解决方案的价值概念。
</details></li>
</ul>
<hr>
<h2 id="Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM"><a href="#Robust-Brain-MRI-Image-Classification-with-SIBOW-SVM" class="headerlink" title="Robust Brain MRI Image Classification with SIBOW-SVM"></a>Robust Brain MRI Image Classification with SIBOW-SVM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08908">http://arxiv.org/abs/2311.08908</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liyun Zeng, Hao Helen Zhang<br>for:* The paper aims to develop a novel brain tumor image classification method to improve the accuracy and efficiency of detecting and diagnosing brain tumors.methods:* The proposed method, called SIBOW-SVM, integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs) to capture hidden image features and differentiate various tumor types.* The method also estimates the probabilities of images belonging to each class, providing high-confidence classification decisions.results:* The SIBOW-SVM method outperforms state-of-the-art methods, including Convolutional Neural Network (CNN), on a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal.<details>
<summary>Abstract</summary>
The majority of primary Central Nervous System (CNS) tumors in the brain are among the most aggressive diseases affecting humans. Early detection of brain tumor types, whether benign or malignant, glial or non-glial, is critical for cancer prevention and treatment, ultimately improving human life expectancy. Magnetic Resonance Imaging (MRI) stands as the most effective technique to detect brain tumors by generating comprehensive brain images through scans. However, human examination can be error-prone and inefficient due to the complexity, size, and location variability of brain tumors. Recently, automated classification techniques using machine learning (ML) methods, such as Convolutional Neural Network (CNN), have demonstrated significantly higher accuracy than manual screening, while maintaining low computational costs. Nonetheless, deep learning-based image classification methods, including CNN, face challenges in estimating class probabilities without proper model calibration. In this paper, we propose a novel brain tumor image classification method, called SIBOW-SVM, which integrates the Bag-of-Features (BoF) model with SIFT feature extraction and weighted Support Vector Machines (wSVMs). This new approach effectively captures hidden image features, enabling the differentiation of various tumor types and accurate label predictions. Additionally, the SIBOW-SVM is able to estimate the probabilities of images belonging to each class, thereby providing high-confidence classification decisions. We have also developed scalable and parallelable algorithms to facilitate the practical implementation of SIBOW-SVM for massive images. As a benchmark, we apply the SIBOW-SVM to a public data set of brain tumor MRI images containing four classes: glioma, meningioma, pituitary, and normal. Our results show that the new method outperforms state-of-the-art methods, including CNN.
</details>
<details>
<summary>摘要</summary>
主要脑中央神经系统肿瘤的多数是人类最致命的疾病之一。早期检测脑肿瘤类型，无论是肿瘤或非肿瘤， glial 或非 glial，都是防范癌症和治疗的关键，最终提高人类存活时间。磁共振成像（MRI）是识别脑肿瘤的最有效的技术，通过扫描生成全面脑图像。然而，人工检查可能会出现错误和不具有效率，因为脑肿瘤的复杂性、大小和位置变化。现在，自动分类技术使用机器学习（ML）方法，如卷积神经网络（CNN），已经表明了与人工检查相比，有较高的准确率，同时保持低的计算成本。然而，深度学习基于图像分类方法，包括CNN，面临着估计类别概率无法进行正确的模型定制。在这篇论文中，我们提出了一种新的脑肿瘤图像分类方法，called SIBOW-SVM，它将袋子模型（BoF）和SIFT特征提取结合weighted Support Vector Machines（wSVMs）。这种新方法可以有效捕捉隐藏的图像特征，以便区分不同的肿瘤类型并准确地预测标签。此外，SIBOW-SVM还可以估计图像属于哪一类的概率，从而提供高确度的分类决策。我们还开发了可扩展和并行的算法，以便实现SIBOW-SVM的实用应用。作为标准，我们对一个公共的脑肿瘤MRI图像集进行了应用，该集包含四个类别： glioma、meningioma、pituitary和正常。我们的结果显示，新方法在与现状的方法相比，具有更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection"><a href="#AdapterShadow-Adapting-Segment-Anything-Model-for-Shadow-Detection" class="headerlink" title="AdapterShadow: Adapting Segment Anything Model for Shadow Detection"></a>AdapterShadow: Adapting Segment Anything Model for Shadow Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08891">http://arxiv.org/abs/2311.08891</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leiping Jie, Hui Zhang</li>
<li>for: 提高阴影检测的精度和效率</li>
<li>methods: 使用可调式适应器与SAM模型结合，并提出了一种新的格子采样方法来自动生成精度点提示</li>
<li>results: 在四个广泛使用的基准数据集上进行了广泛的实验，并证明了我们提出的方法的精度和效率的提高<details>
<summary>Abstract</summary>
Segment anything model (SAM) has shown its spectacular performance in segmenting universal objects, especially when elaborate prompts are provided. However, the drawback of SAM is twofold. On the first hand, it fails to segment specific targets, e.g., shadow images or lesions in medical images. On the other hand, manually specifying prompts is extremely time-consuming. To overcome the problems, we propose AdapterShadow, which adapts SAM model for shadow detection. To adapt SAM for shadow images, trainable adapters are inserted into the frozen image encoder of SAM, since the training of the full SAM model is both time and memory consuming. Moreover, we introduce a novel grid sampling method to generate dense point prompts, which helps to automatically segment shadows without any manual interventions. Extensive experiments are conducted on four widely used benchmark datasets to demonstrate the superior performance of our proposed method. Codes will are publicly available at https://github.com/LeipingJie/AdapterShadow.
</details>
<details>
<summary>摘要</summary>
Segment anything model (SAM) 已经显示出了吸引人的表现，尤其是当提供详细的提示时。然而，SAM的缺点是两重的。一方面，它无法 segment Specific targets, 例如阴影图像或医学图像中的病变。另一方面，手动指定提示是非常时间和内存占用的。为了解决这些问题，我们提议 AdapterShadow，它将SAM模型适应到阴影检测中。为了适应SAM模型 для阴影图像，我们在冻结的图像编码器中插入可学习的适应器。此外，我们还介绍了一种新的网格采样方法，用于生成密集的点提示，以帮助自动 segment 阴影 без任何手动干预。我们在四个常用的标准数据集上进行了广泛的实验，以证明我们提出的方法的优秀性。代码将在 GitHub 上公开。
</details></li>
</ul>
<hr>
<h2 id="One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models"><a href="#One-Shot-Federated-Learning-with-Classifier-Guided-Diffusion-Models" class="headerlink" title="One-Shot Federated Learning with Classifier-Guided Diffusion Models"></a>One-Shot Federated Learning with Classifier-Guided Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08870">http://arxiv.org/abs/2311.08870</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue</li>
<li>for: This paper focuses on exploring the potential of diffusion models in one-shot federated learning (OSFL) to generate high-quality synthetic datasets that can be used to train aggregated models without relying on auxiliary datasets or training generators.</li>
<li>methods: The proposed method, called FedCADO, utilizes guidance from client classifiers to generate data that complies with clients’ distributions and subsequently trains the aggregated model on the server. The method involves targeted optimizations in two aspects: conditionally editing the randomly sampled initial noises and employing the BN statistics from the classifiers to provide detailed guidance during generation.</li>
<li>results: The proposed method effectively handles the heterogeneous client models and the problems of non-IID features or labels, and can generate synthetic datasets that closely resemble the distribution and quality of the original client dataset. The method also avoids privacy leakage risks by not training any generators or transferring any auxiliary information on clients. Experimental results on three large-scale multi-domain image datasets demonstrate that the synthetic datasets generated by FedCADO can assist in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases.<details>
<summary>Abstract</summary>
One-shot federated learning (OSFL) has gained attention in recent years due to its low communication cost. However, most of the existing methods require auxiliary datasets or training generators, which hinders their practicality in real-world scenarios. In this paper, we explore the novel opportunities that diffusion models bring to OSFL and propose FedCADO, utilizing guidance from client classifiers to generate data that complies with clients' distributions and subsequently training the aggregated model on the server. Specifically, our method involves targeted optimizations in two aspects. On one hand, we conditionally edit the randomly sampled initial noises, embedding them with specified semantics and distributions, resulting in a significant improvement in both the quality and stability of generation. On the other hand, we employ the BN statistics from the classifiers to provide detailed guidance during generation. These tailored optimizations enable us to limitlessly generate datasets, which closely resemble the distribution and quality of the original client dataset. Our method effectively handles the heterogeneous client models and the problems of non-IID features or labels. In terms of privacy protection, our method avoids training any generator or transferring any auxiliary information on clients, eliminating any additional privacy leakage risks. Leveraging the extensive knowledge stored in the pre-trained diffusion model, the synthetic datasets can assist us in surpassing the knowledge limitations of the client samples, resulting in aggregation models that even outperform the performance ceiling of centralized training in some cases, which is convincingly demonstrated in the sufficient quantification and visualization experiments conducted on three large-scale multi-domain image datasets.
</details>
<details>
<summary>摘要</summary>
一种新型的 federated learning 方法，即 One-shot federated learning（OSFL），在最近几年内受到了广泛关注，因为它的通信成本很低。然而，大多数现有的方法都需要附加的 auxillary 数据或训练生成器，这限制了它们在实际场景中的实用性。在这篇论文中，我们探索了 diffusion 模型带来的新机遇，并提出了 FedCADO 方法，通过客户端分类器的指导，在服务器上训练汇集模型。具体来说，我们的方法包括两个方面的优化。一方面，我们通过条件编辑 randomly 采样的初始噪声，使其嵌入特定的 semantics 和分布，从而获得较好的质量和稳定性。另一方面，我们利用客户端的 BN 统计来提供详细的指导，以便在生成过程中进行精细的调整。这些特定的优化使得我们能够无限生成数据，这些数据与客户端的原始数据分布和质量具有很高的相似度。我们的方法可以有效地处理不同客户端模型的 hetroogeneous 特性，以及非 Identical 的特征或标签问题。另外，我们的方法不需要在客户端上训练任何生成器或传输任何附加信息，因此不会增加隐私泄露的风险。通过 diffusion 模型存储的广泛知识，我们可以使用生成的 sintethic 数据进行超越客户端样本的知识限制，实现汇集模型的性能超越中央化训练的性能均衡，这些结果在三个大规模多域图像 dataset 上得到了证明。
</details></li>
</ul>
<hr>
<h2 id="Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques"><a href="#Toulouse-Hyperspectral-Data-Set-a-benchmark-data-set-to-assess-semi-supervised-spectral-representation-learning-and-pixel-wise-classification-techniques" class="headerlink" title="Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques"></a>Toulouse Hyperspectral Data Set: a benchmark data set to assess semi-supervised spectral representation learning and pixel-wise classification techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08863">http://arxiv.org/abs/2311.08863</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/romain3ch216/tlse-experiments">https://github.com/romain3ch216/tlse-experiments</a></li>
<li>paper_authors: Romain Thoreau, Laurent Risser, Véronique Achard, Béatrice Berthelot, Xavier Briottet<br>for:The paper aims to provide a new hyperspectral data set for large-scale urban area mapping, addressing the scarcity of annotated data and the limitations of existing data sets.methods:The paper uses semi-supervised and self-supervised techniques, such as Masked Autoencoders, to train machine learning models on the new data set, and evaluates their performance on pixel-wise classification.results:The paper achieves an overall accuracy of 82% and an F1 score of 74% on pixel-wise classification, using a conventional autoencoder combined with a Random Forest classifier. The paper also releases the Toulouse Hyperspectral Data Set and the code for reproducing the experiments.Here is the Chinese translation of the three points:for:论文旨在提供大规模城市区域地图的空中彩色影像，解决现有数据集缺乏标注数据和限制。methods:论文使用 semi-supervised 和 self-supervised 技术，如Masked Autoencoders，在新数据集上训练机器学习模型，并评估其像素级分类性能。results:论文在像素级分类 task 上达到了 82% 的总准确率和 74% 的 F1 分数，使用 convential autoencoder 和 Random Forest 分类器。论文还发布了 Toulouse Hyperspectral Data Set 和 reproduce  эксперименты 的代码。<details>
<summary>Abstract</summary>
Airborne hyperspectral images can be used to map the land cover in large urban areas, thanks to their very high spatial and spectral resolutions on a wide spectral domain. While the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. Yet, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: a limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set that stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details>
<details>
<summary>摘要</summary>
飞行式干扰спектраль成像可以用于大都市地区的地表覆盖图像，因为它们具有非常高的空间和спектраль分辨率，并且覆盖了广泛的 спектраль频谱。 although the spectral dimension of hyperspectral images is highly informative of the chemical composition of the land surface, the use of state-of-the-art machine learning algorithms to map the land cover has been dramatically limited by the availability of training data. To cope with the scarcity of annotations, semi-supervised and self-supervised techniques have lately raised a lot of interest in the community. However, the publicly available hyperspectral data sets commonly used to benchmark machine learning models are not totally suited to evaluate their generalization performances due to one or several of the following properties: limited geographical coverage (which does not reflect the spectral diversity in metropolitan areas), a small number of land cover classes, and a lack of appropriate standard train / test splits for semi-supervised and self-supervised learning. Therefore, we release in this paper the Toulouse Hyperspectral Data Set, which stands out from other data sets in the above-mentioned respects in order to meet key issues in spectral representation learning and classification over large-scale hyperspectral images with very few labeled pixels. Besides, we discuss and experiment the self-supervised task of Masked Autoencoders and establish a baseline for pixel-wise classification based on a conventional autoencoder combined with a Random Forest classifier, achieving 82% overall accuracy and 74% F1 score. The Toulouse Hyperspectral Data Set and our code are publicly available at https://www.toulouse-hyperspectral-data-set.com and https://www.github.com/Romain3Ch216/tlse-experiments, respectively.
</details></li>
</ul>
<hr>
<h2 id="Data-Augmentations-in-Deep-Weight-Spaces"><a href="#Data-Augmentations-in-Deep-Weight-Spaces" class="headerlink" title="Data Augmentations in Deep Weight Spaces"></a>Data Augmentations in Deep Weight Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08851">http://arxiv.org/abs/2311.08851</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aviv Shamsian, David W. Zhang, Aviv Navon, Yan Zhang, Miltiadis Kofinas, Idan Achituve, Riccardo Valperga, Gertjan J. Burghouts, Efstratios Gavves, Cees G. M. Snoek, Ethan Fetaya, Gal Chechik, Haggai Maron</li>
<li>for: 本研究旨在解决深度神经网络学习在权重空间中的难题，即生成大量数据来避免过拟合。</li>
<li>methods: 本文提出了一种基于混合方法的数据增强技术，以生成新的数据示例，不需要额外训练输入权重空间元素。</li>
<li>results: 对于现有的benchmark和新生成的benchmark，我们评估了不同数据增强技术的性能，并发现了一种基于混合方法的新数据增强方案可以提高学习效果。<details>
<summary>Abstract</summary>
Learning in weight spaces, where neural networks process the weights of other deep neural networks, has emerged as a promising research direction with applications in various fields, from analyzing and editing neural fields and implicit neural representations, to network pruning and quantization. Recent works designed architectures for effective learning in that space, which takes into account its unique, permutation-equivariant, structure. Unfortunately, so far these architectures suffer from severe overfitting and were shown to benefit from large datasets. This poses a significant challenge because generating data for this learning setup is laborious and time-consuming since each data sample is a full set of network weights that has to be trained. In this paper, we address this difficulty by investigating data augmentations for weight spaces, a set of techniques that enable generating new data examples on the fly without having to train additional input weight space elements. We first review several recently proposed data augmentation schemes %that were proposed recently and divide them into categories. We then introduce a novel augmentation scheme based on the Mixup method. We evaluate the performance of these techniques on existing benchmarks as well as new benchmarks we generate, which can be valuable for future studies.
</details>
<details>
<summary>摘要</summary>
学习Weight空间中的神经网络，其中神经网络处理另一个深度神经网络的权重，已经出现为一个有前途的研究方向，具有应用于不同领域的可能性，从分析和编辑神经场和隐藏神经表示之间的应用，到网络剪辑和量化。最近的工作设计了适用于这个空间的建筑，考虑其独特的协变结构。然而，目前这些建筑受到严重的过拟合问题困扰，需要大量的数据来适应。在这篇论文中，我们解决这个挑战，通过调查Weight空间中的数据增强技术，以生成新的数据示例，而无需额外训练输入权重空间元素。我们首先回顾最近提出的数据增强方案，并将其分为类别。然后，我们介绍了一种基于 Mixup 方法的新的增强方案。我们对这些技术的性能进行评估，并在现有的标准准则上进行评估，以及新生成的标准准则，这些标准准则可能对未来的研究有所价值。
</details></li>
</ul>
<hr>
<h2 id="Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting"><a href="#Controlling-the-Output-of-a-Generative-Model-by-Latent-Feature-Vector-Shifting" class="headerlink" title="Controlling the Output of a Generative Model by Latent Feature Vector Shifting"></a>Controlling the Output of a Generative Model by Latent Feature Vector Shifting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08850">http://arxiv.org/abs/2311.08850</a></li>
<li>repo_url: None</li>
<li>paper_authors: Róbert Belanec, Peter Lacko, Kristína Malinovská</li>
<li>for: 这个论文的目的是控制StyleGAN3生成器的输出图像修改。</li>
<li>methods: 我们使用了一个预训练的StyleGAN3生成器和一个ResNet34对应神经网络，将生成的图像分类为 celebA 数据集中的 Binary  facial 特征。我们还使用了一个叫做 latent feature shifter 的神经网络，将 StyleGAN3 的 latent vector shift 到指定的特征方向。</li>
<li>results: 我们的 latent feature shifter 方法比基eline方法多出了更多的生成图像具有想要的特征。我们通过评估结果发现，我们的 latent feature shifter 方法成功地控制了 StyleGAN3 生成器的输出图像修改。<details>
<summary>Abstract</summary>
State-of-the-art generative models (e.g. StyleGAN3 \cite{karras2021alias}) often generate photorealistic images based on vectors sampled from their latent space. However, the ability to control the output is limited. Here we present our novel method for latent vector shifting for controlled output image modification utilizing semantic features of the generated images. In our approach we use a pre-trained model of StyleGAN3 that generates images of realistic human faces in relatively high resolution. We complement the generative model with a convolutional neural network classifier, namely ResNet34, trained to classify the generated images with binary facial features from the CelebA dataset. Our latent feature shifter is a neural network model with a task to shift the latent vectors of a generative model into a specified feature direction. We have trained latent feature shifter for multiple facial features, and outperformed our baseline method in the number of generated images with the desired feature. To train our latent feature shifter neural network, we have designed a dataset of pairs of latent vectors with and without a certain feature. Based on the evaluation, we conclude that our latent feature shifter approach was successful in the controlled generation of the StyleGAN3 generator.
</details>
<details>
<summary>摘要</summary>
现代生成模型（例如StyleGAN3 \cite{karras2021alias）frequently生成高分辨率的图像，基于生成器的幂space中的向量采样。然而，控制输出的能力受限。在这里，我们介绍了我们的新方法，利用生成器图像的Semantic特征来实现控制输出图像修改。我们使用已经训练过的StyleGAN3生成器，可以生成高分辨率的真实人脸图像。我们补充了生成器的核心网络，使其能够通过CelebA数据集中的二分类网络（ResNet34）来分类生成的图像。我们的幂向量推移器是一个具有将幂向量推移到指定特征方向的任务的神经网络模型。我们在多个面部特征上训练了幂向量推移器，并超过了我们的基eline方法的数量。为了训练我们的幂向量推移器神经网络，我们设计了一个包含具有和无某些特征的latent向量对的数据集。根据评估结果，我们认为我们的幂向量推移器方法成功地控制了StyleGAN3生成器。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Video-Relighting-Using-Casual-Light-Stage"><a href="#Personalized-Video-Relighting-Using-Casual-Light-Stage" class="headerlink" title="Personalized Video Relighting Using Casual Light Stage"></a>Personalized Video Relighting Using Casual Light Stage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08843">http://arxiv.org/abs/2311.08843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Myeong Choi, Max Christman, Roni Sengupta</li>
<li>for: 这个论文的目的是提出一种个性化视频重光算法，以实现在任何姿势、表情和照明条件下，在实时下生成高质量的重光视频。</li>
<li>methods: 该算法使用了一种新的神经网络重光架构，可以有效地分离出照明源的光照特征、物体的 geometry 和反射特征，然后将其与目标照明相加以生成重光图像。</li>
<li>results: 根据对 Light Stage at Your Desk (LSYD) 数据和 Light Stage captured One Light At a Time (OLAT) 数据的质量评估，这种重光算法能够提高肖像图像重光质量和时间稳定性，比之前的方法更高效。<details>
<summary>Abstract</summary>
In this paper, we develop a personalized video relighting algorithm that produces high-quality and temporally consistent relit video under any pose, expression, and lighting conditions in real-time. Existing relighting algorithms typically rely either on publicly available synthetic data, which yields poor relighting results, or instead on Light Stage data which is inaccessible and is not publicly available. We show that by casually capturing video of a user watching YouTube videos on a monitor we can train a personalized algorithm capable of producing high-quality relighting under any condition. Our key contribution is a novel neural relighting architecture that effectively separates the intrinsic appearance features, geometry and reflectance, from the source lighting and then combines it with the target lighting to generate a relit image. This neural architecture enables smoothing of intrinsic appearance features leading to temporally stable video relighting. Both qualitative and quantitative evaluations show that our relighting architecture improves portrait image relighting quality and temporal consistency over state-of-the-art approaches on both casually captured Light Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time (OLAT) datasets.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们开发了一种个性化视频重新照明算法，该算法在实时下生成高质量、时间上一致的重新照明视频，无论用户的姿势、表情或照明条件如何。现有的重新照明算法通常依赖于公共可用的生成器数据，这些数据的重新照明结果很差，或者使用Light Stage数据，但这些数据不公开可用。我们显示，通过通过捕捉用户在 monitor 上观看 YouTube 视频来训练个性化算法，我们可以生成高质量的重新照明视频。我们的关键贡献是一种新的神经网络重新照明架构，该架构能够有效地分离出照明源的自然特征、几何和反射特征，然后与目标照明相结合，生成一个重新照明的图像。这种神经网络架构使得图像的内在特征平滑，从而实现了时间上一致的视频重新照明。我们的重新照明架构在使用LSYD 和 OLAT 数据集上的质量和时间一致性方面与当前的方法进行比较，并取得了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding"><a href="#Correlation-guided-Query-Dependency-Calibration-in-Video-Representation-Learning-for-Temporal-Grounding" class="headerlink" title="Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding"></a>Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08835">http://arxiv.org/abs/2311.08835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wjun0830/cgdetr">https://github.com/wjun0830/cgdetr</a></li>
<li>paper_authors: WonJun Moon, Sangeek Hyun, SuBeen Lee, Jae-Pil Heo</li>
<li>for: 这 paper 的目的是提供一种基于注意力机制的视频时间固定方法，以便在视频和文本查询之间强化交互，并且能够根据文本查询提取相关的视频clip。</li>
<li>methods: 这 paper 使用了一种名为 Correlation-Guided Detection Transformer~(CG-DETR) 的方法，它包括一个适应式跨模态注意力层、一个 dummy tokens 的使用、以及一个高级概念共同embedding空间学习。</li>
<li>results: 这 paper 的实验结果表明，CG-DETR 可以在多个benchmark上达到州OF-the-art的Result，包括时刻检索和突出部分检测。 codes 可以在 <a target="_blank" rel="noopener" href="https://github.com/wjun0830/CGDETR">https://github.com/wjun0830/CGDETR</a> 中找到。<details>
<summary>Abstract</summary>
Recent endeavors in video temporal grounding enforce strong cross-modal interactions through attention mechanisms to overcome the modality gap between video and text query. However, previous works treat all video clips equally regardless of their semantic relevance with the text query in attention modules. In this paper, our goal is to provide clues for query-associated video clips within the crossmodal encoding process. With our Correlation-Guided Detection Transformer~(CG-DETR), we explore the appropriate clip-wise degree of cross-modal interactions and how to exploit such degrees for prediction. First, we design an adaptive cross-attention layer with dummy tokens. Dummy tokens conditioned by text query take a portion of the attention weights, preventing irrelevant video clips from being represented by the text query. Yet, not all word tokens equally inherit the text query's correlation to video clips. Thus, we further guide the cross-attention map by inferring the fine-grained correlation between video clips and words. We enable this by learning a joint embedding space for high-level concepts, i.e., moment and sentence level, and inferring the clip-word correlation. Lastly, we use a moment-adaptive saliency detector to exploit each video clip's degrees of text engagement. We validate the superiority of CG-DETR with the state-of-the-art results on various benchmarks for both moment retrieval and highlight detection. Codes are available at https://github.com/wjun0830/CGDETR.
</details>
<details>
<summary>摘要</summary>
近期的视频时间挂钩工作强制实施了跨Modal的交互，通过注意机制来超越视频和文本查询之间的Modal gap。然而，前一些工作都是在注意模块中对所有视频clip进行等效的处理，不考虑视频clip与文本查询的Semantic relevance。在这篇论文中，我们的目标是提供与文本查询相关的视频clip在跨Modal编码过程中的线索。我们使用Correlation-Guided Detection Transformer~(CG-DETR)来探索适当的clipwise度跨Modal交互，以及如何利用这些度量进行预测。首先，我们设计了适应式交叉注意层，其中文本查询条件下的干扰符token会占据一部分注意量，以避免不相关的视频clip被文本查询所代表。然而，不是所有的单词token都会相同地继承文本查询的视频clip相关性。因此，我们进一步指导交叉注意地图，通过学习高级概念的共同embedding空间，以及clip-word关系的推理。最后，我们使用时刻适应性的焦点检测器来利用每个视频clip的文本参与度。我们 validate CG-DETR的优越性通过在多种benchmark上实现state-of-the-art的结果，包括时刻检索和突出部分检测。代码可以在https://github.com/wjun0830/CGDETR中获取。
</details></li>
</ul>
<hr>
<h2 id="Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution"><a href="#Target-oriented-Domain-Adaptation-for-Infrared-Image-Super-Resolution" class="headerlink" title="Target-oriented Domain Adaptation for Infrared Image Super-Resolution"></a>Target-oriented Domain Adaptation for Infrared Image Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08816">http://arxiv.org/abs/2311.08816</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yongsongh/dasrgan">https://github.com/yongsongh/dasrgan</a></li>
<li>paper_authors: Yongsong Huang, Tomo Miyazaki, Xiaofeng Liu, Yafei Dong, Shinichiro Omachi</li>
<li>for: 提高红外超分辨率图像质量</li>
<li>methods: 使用目标域适应SRGAN（DASRGAN），包括Texture-Oriented Adaptation（TOA）和Noise-Oriented Adaptation（NOA）两部分</li>
<li>results: 实验表明，DASRGAN在多个标准测试 benchmark 和不同的� upsampling 因子下表现出优于其他方法，并设置了新的州际表现标准。<details>
<summary>Abstract</summary>
Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \url{https://github.com/yongsongH/DASRGAN}.
</details>
<details>
<summary>摘要</summary>
近期研究探索了使用可见光图像来增强护理图像的细节，但这直接适应方法经常变成一件双刃剑，因为它会提高细节的同时也会引入噪声和模糊 artefacts。为了解决这些挑战，我们提议了Target-oriented Domain Adaptation SRGAN（DASRGAN），一种创新的护理图像超分解模型适应框架。DASRGAN在两个关键组件之间运行：1）Texture-Oriented Adaptation（TOA），用于精细调整细节，和2）Noise-Oriented Adaptation（NOA），专门降低噪声传输。具体来说，TOA包括一个特殊的检测器，并使用 Sobel 引导的对抗损失来有效地对细节分布进行对齐。同时，NOA使用噪声对抗损失来在对抗训练中明确地分离生成的pattern和 Gaussian 噪声的分布。我们的广泛的实验证明了 DASRGAN 的优越性。与其他领先方法进行多个标准尺度和增强因子的比较分析表明，DASRGAN 创造了新的状态标准表现。代码可以在 <https://github.com/yongsongH/DASRGAN> 中下载。
</details></li>
</ul>
<hr>
<h2 id="Correlation-aware-active-learning-for-surgery-video-segmentation"><a href="#Correlation-aware-active-learning-for-surgery-video-segmentation" class="headerlink" title="Correlation-aware active learning for surgery video segmentation"></a>Correlation-aware active learning for surgery video segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08811">http://arxiv.org/abs/2311.08811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Wu, Pablo Marquez-Neila, Mingyi Zheng, Hedyeh Rafii-Tari, Raphael Sznitman</li>
<li>for: 这个研究的目的是提出一个新的活动学习策略（COALSamp），用于降低医疗影像资料的标注成本。</li>
<li>methods: 方法包括将影像视网膜下降到一个特别设计的对应学习 latent space，然后从本地块群中选择一定数量的表征性影像。</li>
<li>results: 这个方法在两个手术影像资料集上进行了评估，结果显示COALSamp 可以对医疗影像资料进行有效的分类。此外，这个方法还在三个真实世界的影像资料集上进行了评估，结果也很显著。<details>
<summary>Abstract</summary>
Semantic segmentation is a complex task that relies heavily on large amounts of annotated image data. However, annotating such data can be time-consuming and resource-intensive, especially in the medical domain. Active Learning (AL) is a popular approach that can help to reduce this burden by iteratively selecting images for annotation to improve the model performance. In the case of video data, it is important to consider the model uncertainty and the temporal nature of the sequences when selecting images for annotation. This work proposes a novel AL strategy for surgery video segmentation, \COALSamp{}, COrrelation-aWare Active Learning. Our approach involves projecting images into a latent space that has been fine-tuned using contrastive learning and then selecting a fixed number of representative images from local clusters of video frames. We demonstrate the effectiveness of this approach on two video datasets of surgical instruments and three real-world video datasets. The datasets and code will be made publicly available upon receiving necessary approvals.
</details>
<details>
<summary>摘要</summary>
Semantic segmentation是一项复杂的任务，它依赖于大量已经标注的图像数据。然而，对于医疗领域来说，标注这些数据可以是时间consuming和资源占用的。活动学习（AL）是一种受欢迎的方法，它可以逐步选择图像进行标注，以提高模型性能。在视频数据中，需要考虑模型的uncertainty和时间序列的特点，以便更好地选择需要标注的图像。这项工作提出了一种新的AL策略，称为\COALSamp{}, COrrelation-aWare Active Learning。我们的方法是将图像 проек到一个已经精心调整的latent空间中，然后选择视频帧的本地集群中固定数量的表示图像。我们在两个手术工具视频数据集和三个实际世界视频数据集上证明了这种方法的有效性。数据和代码将在接收所需的批准后公开发布。
</details></li>
</ul>
<hr>
<h2 id="EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery"><a href="#EyeLS-Shadow-Guided-Instrument-Landing-System-for-Intraocular-Target-Approaching-in-Robotic-Eye-Surgery" class="headerlink" title="EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery"></a>EyeLS: Shadow-Guided Instrument Landing System for Intraocular Target Approaching in Robotic Eye Surgery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08799">http://arxiv.org/abs/2311.08799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junjie Yang, Zhihao Zhao, Siyuan Shen, Daniel Zapp, Mathias Maier, Kai Huang, Nassir Navab, M. Ali Nasseri</li>
<li>for:  This paper aims to improve the accuracy of robotic ophthalmic surgery by using shadow positions to estimate the depth position of the instrument tip and optimize its insertion trajectory.</li>
<li>methods: The proposed method uses shadows to estimate the relative depth position of the instrument tip and the target, and then optimizes the insertion trajectory to approach the target within the scanning area of the iOCT.</li>
<li>results: The method was tested on a retina model and achieved an average depth error of 0.0127 mm for floating targets and 0.3473 mm for retinal targets in the surgical simulator, without damaging the retina.<details>
<summary>Abstract</summary>
Robotic ophthalmic surgery is an emerging technology to facilitate high-precision interventions such as retina penetration in subretinal injection and removal of floating tissues in retinal detachment depending on the input imaging modalities such as microscopy and intraoperative OCT (iOCT). Although iOCT is explored to locate the needle tip within its range-limited ROI, it is still difficult to coordinate iOCT's motion with the needle, especially at the initial target-approaching stage. Meanwhile, due to 2D perspective projection and thus the loss of depth information, current image-based methods cannot effectively estimate the needle tip's trajectory towards both retinal and floating targets. To address this limitation, we propose to use the shadow positions of the target and the instrument tip to estimate their relative depth position and accordingly optimize the instrument tip's insertion trajectory until the tip approaches targets within iOCT's scanning area. Our method succeeds target approaching on a retina model, and achieves an average depth error of 0.0127 mm and 0.3473 mm for floating and retinal targets respectively in the surgical simulator without damaging the retina.
</details>
<details>
<summary>摘要</summary>
关于：机器人眼科手术技术的发展机器人眼科手术是一种emerging technology，用于实现高精度干预，如retina penetration和floatings tissues removing，这些干预都取决于输入的干预modalities，如微scopy和intraoperative OCT（iOCT）。虽然iOCT被探索以定位针的位置，但是尚未能够协调针与iOCT的运动，特别是在目标方向的初始阶段。此外，由于2D的 perspective projection，当前的图像基本方法无法有效地估算针的轨迹，特别是在向retinal和浮动目标的方向上。为了解决这个限制，我们提议使用target和 instrumente tip的阴影位置来估算它们的相对深度位置，并根据此来优化针的插入轨迹，直到针接近target在iOCT的扫描范围内。我们的方法在retina模型上成功地进行目标接近，并在手术模拟器中达到了0.0127mm和0.3473mm的平均深度误差，对于浮动和retinal目标。
</details></li>
</ul>
<hr>
<h2 id="HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection"><a href="#HFORD-High-Fidelity-and-Occlusion-Robust-De-identification-for-Face-Privacy-Protection" class="headerlink" title="HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection"></a>HFORD: High-Fidelity and Occlusion-Robust De-identification for Face Privacy Protection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08786">http://arxiv.org/abs/2311.08786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongxin Chen, Mingrui Zhu, Nannan Wang, Xinbo Gao</li>
<li>for: 面部隐私保护issue受到了智能设备的普及和计算机视觉技术的发展的关注。本文提出了一种高效和防护 occlusion 的面部隐私化方法（HFORD），以解决这些问题。</li>
<li>methods: 本方法使用了一种叫做 Identity Disentanglement Module（IDM）的模块，用于分离 latent codes 中的个体特征和特征特征。此外，还提出了一种叫做 Attribute Retention Module（ARM）的模块，用于保留不相关的特征和面部遮挡。</li>
<li>results: 对比其他面部隐私化方法，本方法的结果质量更高，细节更加详细，并且更强地适应 occlusion 情况。<details>
<summary>Abstract</summary>
With the popularity of smart devices and the development of computer vision technology, concerns about face privacy protection are growing. The face de-identification technique is a practical way to solve the identity protection problem. The existing facial de-identification methods have revealed several problems, including the impact on the realism of anonymized results when faced with occlusions and the inability to maintain identity-irrelevant details in anonymized results. We present a High-Fidelity and Occlusion-Robust De-identification (HFORD) method to deal with these issues. This approach can disentangle identities and attributes while preserving image-specific details such as background, facial features (e.g., wrinkles), and lighting, even in occluded scenes. To disentangle the latent codes in the GAN inversion space, we introduce an Identity Disentanglement Module (IDM). This module selects the latent codes that are closely related to the identity. It further separates the latent codes into identity-related codes and attribute-related codes, enabling the network to preserve attributes while only modifying the identity. To ensure the preservation of image details and enhance the network's robustness to occlusions, we propose an Attribute Retention Module (ARM). This module adaptively preserves identity-irrelevant details and facial occlusions and blends them into the generated results in a modulated manner. Extensive experiments show that our method has higher quality, better detail fidelity, and stronger occlusion robustness than other face de-identification methods.
</details>
<details>
<summary>摘要</summary>
随着智能设备的普及和计算机视觉技术的发展，人脸隐私保护的问题日益突出。面部隐私化技术是一种实际的解决方案。现有的面部隐私化方法存在一些问题，如受到遮挡物的影响下的匿名结果的真实性下降，以及维护不同于人脸特征的匿名结果。我们提出了一种高度准确和遮挡物鲁棒的匿名化方法（HFORD），以解决这些问题。这种方法可以分离人脸特征和属性，并保留图像特有的背景、表情特征（如皱纹）和照明等信息，即使在遮挡场景下也能够保持高质量。为了分离GAN拟合空间中的秘密码，我们引入了一种身份分解模块（IDM）。这个模块选择与身份有关的秘密码，并将其分解成身份相关的秘密码和属性相关的秘密码，使网络能够保留属性，只对人脸进行修改。为确保图像细节的保留和网络的遮挡物鲁棒性，我们提议一种Attribute Retention Module（ARM）。这个模块可以动态保留不相关于身份的细节和脸部遮挡物，并将其混合到生成结果中，以实现更高质量和更强的鲁棒性。经过广泛的实验，我们发现我们的方法在质量、细节准确性和遮挡物鲁棒性等方面都有更高的表现。
</details></li>
</ul>
<hr>
<h2 id="Language-Semantic-Graph-Guided-Data-Efficient-Learning"><a href="#Language-Semantic-Graph-Guided-Data-Efficient-Learning" class="headerlink" title="Language Semantic Graph Guided Data-Efficient Learning"></a>Language Semantic Graph Guided Data-Efficient Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08782">http://arxiv.org/abs/2311.08782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenxuan Ma, Shuang Li, Lincan Cai, Jingxuan Kang<br>for: 这个研究的目的是提高机器学习模型对有限数据的学习效能，并在无需人工标注的情况下实现更好的模型表现。methods: 这个研究使用了 Semi-Supervised Learning (SSL)、Transfer Learning (TL) 和 Data Augmentation (DA) 等方法来实现数据优化。另外，这个研究还使用了一个名为 Language Semantic Graph (LSG) 的新方法，它是根据标签中的自然语言描述建立的一个图形。results: 这个研究在图像、影片和音频等模式下运用 LSG 方法，在 SSL 和 TL 情况下获得了显著改善的表现，并且比其他数据优化方法更快速。<details>
<summary>Abstract</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details>
<details>
<summary>摘要</summary>
Developing generalizable models that can effectively learn from limited data and with minimal reliance on human supervision is a significant objective within the machine learning community, particularly in the era of deep neural networks. Therefore, to achieve data-efficient learning, researchers typically explore approaches that can leverage more related or unlabeled data without necessitating additional manual labeling efforts, such as Semi-Supervised Learning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL leverages unlabeled data in the training process, while TL enables the transfer of expertise from related data distributions. DA broadens the dataset by synthesizing new data from existing examples. However, the significance of additional knowledge contained within labels has been largely overlooked in research. In this paper, we propose a novel perspective on data efficiency that involves exploiting the semantic information contained in the labels of the available data. Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.Here's the translation in Traditional Chinese:开发能够从有限数据中学习并且仅对人工标注有最少依赖的机器学习模型是机器学习社区中的一个重要目标，特别是在深度神经网络时代。因此，实现数据效率的研究通常会探索可以将更多相关的或未标注的数据 leveraged 进行训练，例如半监督学习 (SSL)、传播学习 (TL) 和数据扩展 (DA)。SSL 利用训练过程中的无标注数据，而 TL 允许将相关数据分布中的专长转移到新的数据上。DA 则是将现有数据中的新数据生成新的数据，以增加数据集的大小。但是，实际上 Label 中含的额外知识几乎没有被研究。在这篇论文中，我们提出了一个新的数据效率的思路，即利用可用数据中的标签上的 semantic information。 Specifically, we introduce a Language Semantic Graph (LSG) which is constructed from labels manifest as natural language descriptions. Upon this graph, an auxiliary graph neural network is trained to extract high-level semantic relations and then used to guide the training of the primary model, enabling more adequate utilization of label knowledge. Across image, video, and audio modalities, we utilize the LSG method in both TL and SSL scenarios and illustrate its versatility in significantly enhancing performance compared to other data-efficient learning approaches. Additionally, our in-depth analysis shows that the LSG method also expedites the training process.
</details></li>
</ul>
<hr>
<h2 id="Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation"><a href="#Two-stage-Joint-Transductive-and-Inductive-learning-for-Nuclei-Segmentation" class="headerlink" title="Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation"></a>Two-stage Joint Transductive and Inductive learning for Nuclei Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08774">http://arxiv.org/abs/2311.08774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hesham Ali, Idriss Tondji, Mennatullah Siam</li>
<li>for: 针对医疗影像中蛋白质分割任务进行研究，以提高肿瘤诊断和治疗的效率和准确性。</li>
<li>methods: 提出了一种新的混合学习方法，结合了泛化学习和推导学习的优点，以便更好地利用可用的标注和未标注数据。</li>
<li>results: 在MoNuSeg测试集上证明了该方法的效果和潜在应用前景，并提出了一种新的两stage混合推理方案。<details>
<summary>Abstract</summary>
AI-assisted nuclei segmentation in histopathological images is a crucial task in the diagnosis and treatment of cancer diseases. It decreases the time required to manually screen microscopic tissue images and can resolve the conflict between pathologists during diagnosis. Deep Learning has proven useful in such a task. However, lack of labeled data is a significant barrier for deep learning-based approaches. In this study, we propose a novel approach to nuclei segmentation that leverages the available labelled and unlabelled data. The proposed method combines the strengths of both transductive and inductive learning, which have been previously attempted separately, into a single framework. Inductive learning aims at approximating the general function and generalizing to unseen test data, while transductive learning has the potential of leveraging the unlabelled test data to improve the classification. To the best of our knowledge, this is the first study to propose such a hybrid approach for medical image segmentation. Moreover, we propose a novel two-stage transductive inference scheme. We evaluate our approach on MoNuSeg benchmark to demonstrate the efficacy and potential of our method.
</details>
<details>
<summary>摘要</summary>
AI助成 Histopathological 图像中的核体分割是诊断和治疗癌症疾病的关键任务。它可以减少手动检查微scopic 组织图像所需的时间，并能够解决 pathologists 在诊断中存在的冲突。深度学习 已经在这种任务中证明了其有用性。然而，缺乏标注数据是深度学习基于方法的主要障碍。在这项研究中，我们提出了一种新的核体分割方法，利用可用的标注和无标注数据。我们的方法结合了泛化学习和抽象学习的优点，这两种学习方法在过去已经分别被应用。泛化学习目标是将通用函数approximated，并在未看到的测试数据上generalize;而抽象学习具有利用无标注测试数据来改进分类的潜在优势。根据我们所知，这是第一项提出了这种混合方法的医学图像分割研究。此外，我们还提出了一种新的两stage 混合推理方案。我们在 MoNuSeg benchmark 上评估了我们的方法，以demonstrate 我们的方法的效果和潜在。
</details></li>
</ul>
<hr>
<h2 id="FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier"><a href="#FastBlend-a-Powerful-Model-Free-Toolkit-Making-Video-Stylization-Easier" class="headerlink" title="FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier"></a>FastBlend: a Powerful Model-Free Toolkit Making Video Stylization Easier</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09265">http://arxiv.org/abs/2311.09265</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/artiprocher/sd-webui-fastblend">https://github.com/artiprocher/sd-webui-fastblend</a></li>
<li>paper_authors: Zhongjie Duan, Chengyu Wang, Cen Chen, Weining Qian, Jun Huang, Mingyi Jin</li>
<li>for:  Addresses the consistency problem in video processing for tasks such as style transfer and image editing.</li>
<li>methods:  Uses a patch matching algorithm with two inference modes: blending and interpolation.</li>
<li>results:  Outperforms existing methods for video deflickering and video synthesis in the blending mode, and surpasses video interpolation and model-based video processing approaches in the interpolation mode.Here’s the summary in Traditional Chinese:</li>
<li>for: 解决影像处理中的一致问题，如style transfer和图像修复。</li>
<li>methods: 使用一个patch matching算法，分别有汇入和 interpolate两种推论方式。</li>
<li>results: 与现有方法相比，在汇入模式下表现更好，并在 interpolate 模式下超过了影像 interpolate 和基于模型的影像处理方法。I hope that helps!<details>
<summary>Abstract</summary>
With the emergence of diffusion models and rapid development in image processing, it has become effortless to generate fancy images in tasks such as style transfer and image editing. However, these impressive image processing approaches face consistency issues in video processing. In this paper, we propose a powerful model-free toolkit called FastBlend to address the consistency problem for video processing. Based on a patch matching algorithm, we design two inference modes, including blending and interpolation. In the blending mode, FastBlend eliminates video flicker by blending the frames within a sliding window. Moreover, we optimize both computational efficiency and video quality according to different application scenarios. In the interpolation mode, given one or more keyframes rendered by diffusion models, FastBlend can render the whole video. Since FastBlend does not modify the generation process of diffusion models, it exhibits excellent compatibility. Extensive experiments have demonstrated the effectiveness of FastBlend. In the blending mode, FastBlend outperforms existing methods for video deflickering and video synthesis. In the interpolation mode, FastBlend surpasses video interpolation and model-based video processing approaches. The source codes have been released on GitHub.
</details>
<details>
<summary>摘要</summary>
“对于快速发展的图像处理技术和扩散模型，创造出了丰富的图像效果，例如风格转移和图像修剪。但这些印象精采的图像处理方法在视频处理中还存在一定的一致性问题。在本文中，我们提出了一个强大且无模型的工具套件called FastBlend，以解决视频处理中的一致性问题。基于图像匹配算法，我们设计了两种推理模式，包括融合和插值。在融合模式下，FastBlend可以消除视频震荡，通过视频内排埋窗口中的匹配。此外，我们还优化了不同应用场景中的计算效率和视频质量。在插值模式下，给定一幅或多幅透过扩散模型生成的关键帧，FastBlend可以生成整个视频。由于FastBlend不会改变扩散模型的生成过程，因此它具有很好的相容性。实验结果显示FastBlend的效果极佳，在融合模式下比现有的视频显示和视频生成方法更好，在插值模式下比视频插值和基于模型的视频处理方法更好。源代码已经在GitHub上发布。”
</details></li>
</ul>
<hr>
<h2 id="4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters"><a href="#4K-Resolution-Photo-Exposure-Correction-at-125-FPS-with-8K-Parameters" class="headerlink" title="4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters"></a>4K-Resolution Photo Exposure Correction at 125 FPS with ~8K Parameters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08759">http://arxiv.org/abs/2311.08759</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhou-yijie/msltnet">https://github.com/zhou-yijie/msltnet</a></li>
<li>paper_authors: Yijie Zhou, Chao Li, Jin Liang, Tianyi Xu, Xin Liu, Jun Xu</li>
<li>for: 本研究旨在提出一种高效且轻量级的多层感知架构，用于高分辨率图像曝光 corrections。</li>
<li>methods: 提议使用多槽线性变换网络（MSLT），通过拉普拉敏度 pyramid 技术进行高频和低频层分解，然后采用像素适应线性变换进行层次修复。</li>
<li>results: 实验结果表明，提议的 MSLT 网络在两个标准数据集上对 фото曝光 corrections 表现更高效，并且对比于现有的状态 искусственный neural network 有更好的性能。<details>
<summary>Abstract</summary>
The illumination of improperly exposed photographs has been widely corrected using deep convolutional neural networks or Transformers. Despite with promising performance, these methods usually suffer from large parameter amounts and heavy computational FLOPs on high-resolution photographs. In this paper, we propose extremely light-weight (with only ~8K parameters) Multi-Scale Linear Transformation (MSLT) networks under the multi-layer perception architecture, which can process 4K-resolution sRGB images at 125 Frame-Per-Second (FPS) by a Titan RTX GPU. Specifically, the proposed MSLT networks first decompose an input image into high and low frequency layers by Laplacian pyramid techniques, and then sequentially correct different layers by pixel-adaptive linear transformation, which is implemented by efficient bilateral grid learning or 1x1 convolutions. Experiments on two benchmark datasets demonstrate the efficiency of our MSLTs against the state-of-the-arts on photo exposure correction. Extensive ablation studies validate the effectiveness of our contributions. The code is available at https://github.com/Zhou-Yijie/MSLTNet.
</details>
<details>
<summary>摘要</summary>
“对于不当露光的照片，深度卷积神经网络或Transformers已经广泛地解决问题。然而，这些方法通常具有较大的参数数量和高 Computational FLOPs 的高resolution照片。在这篇文章中，我们提出了 extremely light-weight（仅有 ~8K 参数）的多尺度线性转换（MSLT）网络，可以在 Titan RTX GPU 上处理 4K 分辨率 sRGB 影像，并且可以在 125 帧每秒（FPS）的速度下进行处理。具体来说，我们的 MSLT 网络首先将输入影像分解成高频和低频层，使用 Laplacian  pyramid 技术，然后逐层 corrections 不同层的像素，使用高效的二元方格学习或 1x1 卷积。实验结果显示，我们的 MSLT 网络在照片曝光修正方面与现有的方法相比，有着更高的效率。广泛的测试 validate 了我们的贡献。代码可以在 https://github.com/Zhou-Yijie/MSLTNet 上取得。”
</details></li>
</ul>
<hr>
<h2 id="Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection"><a href="#Improved-Dense-Nested-Attention-Network-Based-on-Transformer-for-Infrared-Small-Target-Detection" class="headerlink" title="Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection"></a>Improved Dense Nested Attention Network Based on Transformer for Infrared Small Target Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08747">http://arxiv.org/abs/2311.08747</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chun Bao, Jie Cao, Yaqian Ning, Tianhua Zhao, Zhijun Li, Zechen Wang, Li Zhang, Qun Hao<br>for:The paper is written for detecting infrared small targets in complex and dynamic backgrounds using deep learning.methods:The proposed method, called improved dense nested attention network (IDNANet), is based on the transformer architecture and incorporates several novel features, including the Swin-transformer and ACmix attention structure, to enhance the continuity and features of the target.results:The proposed method outperforms other state-of-the-art methods in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). Specifically, the $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.<details>
<summary>Abstract</summary>
Infrared small target detection based on deep learning offers unique advantages in separating small targets from complex and dynamic backgrounds. However, the features of infrared small targets gradually weaken as the depth of convolutional neural network (CNN) increases. To address this issue, we propose a novel method for detecting infrared small targets called improved dense nested attention network (IDNANet), which is based on the transformer architecture. We preserve the dense nested structure of dense nested attention network (DNANet) and introduce the Swin-transformer during feature extraction stage to enhance the continuity of features. Furthermore, we integrate the ACmix attention structure into the dense nested structure to enhance the features of intermediate layers. Additionally, we design a weighted dice binary cross-entropy (WD-BCE) loss function to mitigate the negative impact of foreground-background imbalance in the samples. Moreover, we develop a dataset specifically for infrared small targets, called BIT-SIRST. The dataset comprises a significant amount of real-world targets and manually annotated labels, as well as synthetic data and corresponding labels. We have evaluated the effectiveness of our method through experiments conducted on public datasets. In comparison to other state-of-the-art methods, our approach outperforms in terms of probability of detection (P_d), false-alarm rate (F_a), and mean intersection of union ($mIoU$). The $mIoU$ reaches 90.89 on the NUDT-SIRST dataset and 79.72 on the NUAA-SIRST dataset.
</details>
<details>
<summary>摘要</summary>
infrared小target检测基于深度学习具有独特的优势，可以准确分割小target从复杂和动态背景中。然而，infrared小target特征逐渐弱化为深度卷积神经网络（CNN）的深度增加。为解决这个问题，我们提出了一种改进的infrared小target检测方法，称为改进的密集嵌入注意网络（IDNANet），基于transformer架构。我们保持密集嵌入结构的密集嵌入注意网络（DNANet）结构，并在特征提取阶段引入Swin-transformer以增强特征连续性。此外，我们将ACmix注意结构integrated到密集结构中，以提高中间层特征的表现。此外，我们还定义了weighted dice二分类优化函数（WD-BCE），以抑制样本中背景干扰的负面影响。此外，我们还开发了专门为infrared小target而设计的BIT-SIRST数据集。该数据集包括大量真实世界目标和手动标注 Label，以及 sintetic数据和相应的标注。我们通过对公共数据集进行实验，证明了我们的方法的效果。与其他当前状态的方法相比，我们的方法在检测概率（P_d）、假阳性率（F_a）和mean intersection of union（$mIoU）方面具有优势。$mIoU$ 在NUDT-SIRST数据集上达到了90.89，在NUAA-SIRST数据集上达到了79.72。
</details></li>
</ul>
<hr>
<h2 id="A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video"><a href="#A-Diffusion-Model-Based-Quality-Enhancement-Method-for-HEVC-Compressed-Video" class="headerlink" title="A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video"></a>A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08746">http://arxiv.org/abs/2311.08746</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zheng Liu, Honggang Qi</li>
<li>for: 提高压缩视频质量</li>
<li>methods: 使用扩散模型进行后处理</li>
<li>results: 在混合数据集上实现更高的质量改进 compared to existing methods<details>
<summary>Abstract</summary>
Video post-processing methods can improve the quality of compressed videos at the decoder side. Most of the existing methods need to train corresponding models for compressed videos with different quantization parameters to improve the quality of compressed videos. However, in most cases, the quantization parameters of the decoded video are unknown. This makes existing methods have their limitations in improving video quality. To tackle this problem, this work proposes a diffusion model based post-processing method for compressed videos. The proposed method first estimates the feature vectors of the compressed video and then uses the estimated feature vectors as the prior information for the quality enhancement model to adaptively enhance the quality of compressed video with different quantization parameters. Experimental results show that the quality enhancement results of our proposed method on mixed datasets are superior to existing methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>视频后处理技术可以提高压缩视频在解码器端的质量。现有的大多数方法需要为不同的压缩参数训练对应的模型，以提高压缩视频的质量。然而，在大多数情况下，解码后的视频的压缩参数都是未知的。这限制了现有方法的改进视频质量的能力。为解决这个问题，本研究提出了基于扩散模型的后处理方法。该方法首先估计压缩视频的特征向量，然后使用估计的特征向量作为质量提升模型的先知信息，以适应不同的压缩参数进行自适应质量提升。实验结果表明，我们提出的方法在混合数据集上的质量提升结果较 existing方法优。Note: Simplified Chinese is also known as "Mandarin" or "Standard Chinese".
</details></li>
</ul>
<hr>
<h2 id="Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories"><a href="#Scalable-Federated-Learning-for-Clients-with-Different-Input-Image-Sizes-and-Numbers-of-Output-Categories" class="headerlink" title="Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories"></a>Scalable Federated Learning for Clients with Different Input Image Sizes and Numbers of Output Categories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08716">http://arxiv.org/abs/2311.08716</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuhei Nitta, Taiji Suzuki, Albert Rodríguez Mulet, Atsushi Yaguchi, Ryusuke Hirai</li>
<li>for: 采用 federated learning 方法进行隐私保护的训练，但不是所有客户端的数据都可以共享。</li>
<li>methods: 根据客户端的输入图像大小和输出类别数量调整本地模型的深度和宽度，以及提供一个新的普适性隔距来描述联邦学习的泛化差。</li>
<li>results: 在多个不同客户端设置下，对图像分类和物体检测任务进行了证明效果，并且提供了一个可靠的 bound 来描述联邦学习的泛化差。<details>
<summary>Abstract</summary>
Federated learning is a privacy-preserving training method which consists of training from a plurality of clients but without sharing their confidential data. However, previous work on federated learning do not explore suitable neural network architectures for clients with different input images sizes and different numbers of output categories. In this paper, we propose an effective federated learning method named ScalableFL, where the depths and widths of the local models for each client are adjusted according to the clients' input image size and the numbers of output categories. In addition, we provide a new bound for the generalization gap of federated learning. In particular, this bound helps to explain the effectiveness of our scalable neural network approach. We demonstrate the effectiveness of ScalableFL in several heterogeneous client settings for both image classification and object detection tasks.
</details>
<details>
<summary>摘要</summary>
federated learning 是一种隐私保护的训练方法，它通过多个客户端进行训练，但不是共享客户端的敏感数据。然而，过去的联邦学习工作未经检查适合客户端的不同输入图像大小和输出类别数量的适应性的神经网络架构。在这篇论文中，我们提出了一种有效的联邦学习方法，名为可扩展FL（ScalableFL）。我们在每个客户端的本地模型中调整了深度和宽度，以适应客户端的输入图像大小和输出类别数量。此外，我们还提供了一个新的泛化差 bounds，帮助解释我们的扩展神经网络方法的效iveness。我们在多种不同客户端设置下进行了多个实验，证明了ScalableFL 的效iveness。Note: Please note that the translation is in Simplified Chinese, and the word order and sentence structure may be different from the original text.
</details></li>
</ul>
<hr>
<h2 id="CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding"><a href="#CP-EB-Talking-Face-Generation-with-Controllable-Pose-and-Eye-Blinking-Embedding" class="headerlink" title="CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding"></a>CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08673">http://arxiv.org/abs/2311.08673</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianzong Wang, Yimin Deng, Ziqi Liang, Xulong Zhang, Ning Cheng, Jing Xiao</li>
<li>for: 这个论文是为了提出一种名为“CP-EB”的对话面生成方法，用于从音频信号和人像作为输入，生成一个具有自然头姿和眼睛跳动的人讲视频。</li>
<li>methods: 该方法使用了一种基于GAN的建模结构，从输入音频和参考视频中提取眼睛跳动特征，并通过对其进行对比训练，将其 embedding到人讲图像中。</li>
<li>results: 实验结果显示，该方法可以生成具有同步嘴部动作、自然头姿和眼睛跳动的真实人讲视频。<details>
<summary>Abstract</summary>
This paper proposes a talking face generation method named "CP-EB" that takes an audio signal as input and a person image as reference, to synthesize a photo-realistic people talking video with head poses controlled by a short video clip and proper eye blinking embedding. It's noted that not only the head pose but also eye blinking are both important aspects for deep fake detection. The implicit control of poses by video has already achieved by the state-of-art work. According to recent research, eye blinking has weak correlation with input audio which means eye blinks extraction from audio and generation are possible. Hence, we propose a GAN-based architecture to extract eye blink feature from input audio and reference video respectively and employ contrastive training between them, then embed it into the concatenated features of identity and poses to generate talking face images. Experimental results show that the proposed method can generate photo-realistic talking face with synchronous lips motions, natural head poses and blinking eyes.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种名为“CP-EB”的人脸发言方法，该方法接受音频信号和人像作为输入，并生成一个具有自然头姿和眼睛跳动的真实人脸发言视频。研究表明，不仅头姿也是深度伪造检测中重要的一个方面，而且眼睛跳动与输入音频的关系弱，这意味着可以从音频中提取眼睛跳动特征并生成。因此，我们提议使用GAN网络抽取音频和参考视频中的眼睛跳动特征，并在这些特征之间进行对比培训，然后将其 embedding到人脸特征和头姿特征中，以生成真实的人脸发言图像。实验结果表明，提出的方法可以生成具有同步嘴部动作、自然头姿和眼睛跳动的真实人脸发言图像。
</details></li>
</ul>
<hr>
<h2 id="Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data"><a href="#Deep-Neural-Network-Identification-of-Limnonectes-Species-and-New-Class-Detection-Using-Image-Data" class="headerlink" title="Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data"></a>Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08661">http://arxiv.org/abs/2311.08661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Xu, Yili Hong, Eric P. Smith, David S. McLeod, Xinwei Deng, Laura J. Freeman</li>
<li>for: 这篇论文是用于解决生物多样性的挑战，具体来说是用于处理种群复杂的问题。</li>
<li>methods: 这篇论文使用机器学习的原理来解决两个问题：一是种群分类问题，二是外围检测问题。</li>
<li>results: 研究人员使用深度神经网络成功地自动将图像分类到已知种群中，并且可以成功地检测图像是否属于现有类别之外。<details>
<summary>Abstract</summary>
As is true of many complex tasks, the work of discovering, describing, and understanding the diversity of life on Earth (viz., biological systematics and taxonomy) requires many tools. Some of this work can be accomplished as it has been done in the past, but some aspects present us with challenges which traditional knowledge and tools cannot adequately resolve. One such challenge is presented by species complexes in which the morphological similarities among the group members make it difficult to reliably identify known species and detect new ones. We address this challenge by developing new tools using the principles of machine learning to resolve two specific questions related to species complexes. The first question is formulated as a classification problem in statistics and machine learning and the second question is an out-of-distribution (OOD) detection problem. We apply these tools to a species complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex) and employ a morphological character (hind limb skin texture) traditionally treated qualitatively in a quantitative and objective manner. We demonstrate that deep neural networks can successfully automate the classification of an image into a known species group for which it has been trained. We further demonstrate that the algorithm can successfully classify an image into a new class if the image does not belong to the existing classes. Additionally, we use the larger MNIST dataset to test the performance of our OOD detection algorithm. We finish our paper with some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity. This paper has online supplementary materials.
</details>
<details>
<summary>摘要</summary>
如同许多复杂任务一样，发现、描述和理解地球上生物多样性（即生物系统матиCS和taxonomy）需要许多工具。一些这些工具可以通过传统的方法来实现，但一些方面却对传统知识和工具来说无法充分解决。一个如此挑战是由种群复杂体系所带来，其中种群成员之间的形态相似性使得可靠地识别已知种和检测新种变得困难。我们通过开发新的工具，使用机器学习原理来解决这两个问题。第一个问题是一个统计学和机器学习中的分类问题，第二个问题是一个 OUT-OF-DISTRIBUTION（OOD）检测问题。我们在南东亚瀑布蟾（Limnonectes kuhlii complex）种群中应用这些工具，使用传统上被质量地对待的一个形态特征（背股皮Texture）进行了量化Objective的处理。我们示示了深度神经网络可以成功地自动将图像分类到已知种群中，并且可以成功地将图像分类到新的类别中。此外，我们使用更大的MNIST数据集来测试我们的OOD检测算法的性能。我们在文章结尾附加了一些关于这些方法在种群复杂体系中的应用以及我们的记录真正的生物多样性的评论。这篇文章有在线补充材料。
</details></li>
</ul>
<hr>
<h2 id="ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars"><a href="#ConeQuest-A-Benchmark-for-Cone-Segmentation-on-Mars" class="headerlink" title="ConeQuest: A Benchmark for Cone Segmentation on Mars"></a>ConeQuest: A Benchmark for Cone Segmentation on Mars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08657">http://arxiv.org/abs/2311.08657</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kerner-lab/conequest">https://github.com/kerner-lab/conequest</a></li>
<li>paper_authors: Mirali Purohit, Jacob Adler, Hannah Kerner</li>
<li>for: 这 paper 是为了开发更加准确和可靠的 Mars 穹顶 cone 分割模型。</li>
<li>methods: 该 paper 使用了 computer vision 技术，并基于 Mars 的三个地区，提供了 &gt;13k 个样本，以便进行 cone 分割模型的训练和评估。</li>
<li>results: 该 paper 的结果表明，现有的 segmentation 模型无法准确地分割 Mars 的穹顶 cone，其中最佳模型的 IoU 分割率只有 52.52% 和 42.55%。<details>
<summary>Abstract</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.
</details>
<details>
<summary>摘要</summary>
Over the years, space scientists have collected terabytes of Mars data from satellites and rovers. One important set of features identified in Mars orbital images is pitted cones, which are interpreted to be mud volcanoes believed to form in regions that were once saturated in water (i.e., a lake or ocean). Identifying pitted cones globally on Mars would be of great importance, but expert geologists are unable to sort through the massive orbital image archives to identify all examples. However, this task is well suited for computer vision. Although several computer vision datasets exist for various Mars-related tasks, there is currently no open-source dataset available for cone detection/segmentation. Furthermore, previous studies trained models using data from a single region, which limits their applicability for global detection and mapping. Motivated by this, we introduce ConeQuest, the first expert-annotated public dataset to identify cones on Mars. ConeQuest consists of >13k samples from 3 different regions of Mars. We propose two benchmark tasks using ConeQuest: (i) Spatial Generalization and (ii) Cone-size Generalization. We finetune and evaluate widely-used segmentation models on both benchmark tasks. Results indicate that cone segmentation is a challenging open problem not solved by existing segmentation models, which achieve an average IoU of 52.52% and 42.55% on in-distribution data for tasks (i) and (ii), respectively. We believe this new benchmark dataset will facilitate the development of more accurate and robust models for cone segmentation. Data and code are available at https://github.com/kerner-lab/ConeQuest.Here's the translation in Traditional Chinese:过去的年头，宇宙科学家从卫星和探测车获取了数十TB的火星数据。一个重要的特征是穿孔碗，被解释为怀疑是过去曾经淹没在水中的泥火山。全球火星上的穿孔碗识别是非常重要，但专业地质学家无法从巨大的卫星图像档案中找到所有的例子。然而，这个任务非常适合计算机视觉。虽然有许多火星相关的计算机视觉数据集存在，但目前没有公开的数据集可以用于穿孔碗检测。此外，前一 studies将模型训练使用单一区域的数据，导致其在全球检测和地图上的应用有限。为了解决这个问题，我们介绍了ConeQuest，首个专家录实 dataset 用于火星穿孔碗检测。ConeQuest 包含了 >13k 个样本，来自三个不同的火星区域。我们提出了两个benchmark任务：(i) 空间一致和 (ii) 穿孔大小一致。我们在这两个任务上调整和评估了广泛使用的检测模型。结果显示，穿孔检测是一个尚未解决的开问题，现有的检测模型在内部数据上的内容率为52.52%和42.55%。我们相信这个新的benchmark数据集将促进更加精确和可靠的穿孔检测模型的发展。数据和代码可以在 https://github.com/kerner-lab/ConeQuest 上获取。
</details></li>
</ul>
<hr>
<h2 id="Review-of-AlexNet-for-Medical-Image-Classification"><a href="#Review-of-AlexNet-for-Medical-Image-Classification" class="headerlink" title="Review of AlexNet for Medical Image Classification"></a>Review of AlexNet for Medical Image Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08655">http://arxiv.org/abs/2311.08655</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Arminsbss/tumor-classification">https://github.com/Arminsbss/tumor-classification</a></li>
<li>paper_authors: Wenhao Tang, Junding Sun, Shuihua Wang, Yudong Zhang</li>
<li>for: 这篇论文主要探讨了AlexNet模型在医学图像分类领域的应用和技术细节。</li>
<li>methods: 该论文使用了Dropout技术和ReLU活化函数来避免过拟合和梯度消失问题，并提出了一种基于AlexNet模型的医学图像分类方法。</li>
<li>results: 该论文通过对40篇学术论文和会议论文进行回顾，提出了AlexNet模型的技术细节、优势和应用领域。<details>
<summary>Abstract</summary>
In recent years, the rapid development of deep learning has led to a wide range of applications in the field of medical image classification. The variants of neural network models with ever-increasing performance share some commonalities: to try to mitigate overfitting, improve generalization, avoid gradient vanishing and exploding, etc. AlexNet first utilizes the dropout technique to mitigate overfitting and the ReLU activation function to avoid gradient vanishing. Therefore, we focus our discussion on AlexNet, which has contributed greatly to the development of CNNs in 2012. After reviewing over 40 papers, including journal papers and conference papers, we give a narrative on the technical details, advantages, and application areas of AlexNet.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing"><a href="#Refining-Perception-Contracts-Case-Studies-in-Vision-based-Safe-Auto-landing" class="headerlink" title="Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing"></a>Refining Perception Contracts: Case Studies in Vision-based Safe Auto-landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08652">http://arxiv.org/abs/2311.08652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangge Li, Benjamin C Yang, Yixuan Jia, Daniel Zhuang, Sayan Mitra</li>
<li>for: 该论文旨在评估控制系统中使用机器学习进行感知时的安全性。</li>
<li>methods: 论文使用了合同测试和证明方法来证明总体系统水平安全需求的可行性。</li>
<li>results: 论文通过引入数据和需求导向的改进合同构建算法（DaRePC），得出了可测试的合同，确定了降落在跑道上安全的飞机状态和环境条件，以及通过序列门检测器安全过航的无人机状态。同时也发现了一些可能导致视觉控制系统的安全性问题的条件（例如低地平线的阳光）。<details>
<summary>Abstract</summary>
Perception contracts provide a method for evaluating safety of control systems that use machine learning for perception. A perception contract is a specification for testing the ML components, and it gives a method for proving end-to-end system-level safety requirements. The feasibility of contract-based testing and assurance was established earlier in the context of straight lane keeping: a 3-dimensional system with relatively simple dynamics. This paper presents the analysis of two 6 and 12-dimensional flight control systems that use multi-stage, heterogeneous, ML-enabled perception. The paper advances methodology by introducing an algorithm for constructing data and requirement guided refinement of perception contracts (DaRePC). The resulting analysis provides testable contracts which establish the state and environment conditions under which an aircraft can safety touchdown on the runway and a drone can safely pass through a sequence of gates. It can also discover conditions (e.g., low-horizon sun) that can possibly violate the safety of the vision-based control system.
</details>
<details>
<summary>摘要</summary>
感知合约提供了评估机器学习控制系统安全性的方法。感知合约是测试ML组件的规范，它提供了系统级别的安全要求的证明方法。在推点驱动的情况下，感知合约的可行性已经在三维直线保持问题中被证明。本文分析了使用多 Stage、异构、ML实现的6和12维飞行控制系统。本文提出了一种数据和需求驱动的感知合约构建算法（DaRePC），从而得到了可测试的合约，这些合约确定了飞机在跑道上安全着陆和无人机通过序列门的前提条件。此外，它还可以发现可能违反视觉控制系统安全性的情况（如低地平线的阳光）。
</details></li>
</ul>
<hr>
<h2 id="Painterly-Image-Harmonization-via-Adversarial-Residual-Learning"><a href="#Painterly-Image-Harmonization-via-Adversarial-Residual-Learning" class="headerlink" title="Painterly Image Harmonization via Adversarial Residual Learning"></a>Painterly Image Harmonization via Adversarial Residual Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08646">http://arxiv.org/abs/2311.08646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xudong Wang, Li Niu, Junyan Cao, Yan Hong, Liqing Zhang</li>
<li>for: 将 photorealistic 背景图像和绘画风格背景图像进行合成，以实现图像的协调和融合。</li>
<li>methods: 使用对抗学习技术，特别是设计了 dual-encoder 生成器和 pixel-wise 探测器，以bridge 背景和前景特征图像之间的域隔。</li>
<li>results: 实验表明，我们的方法可以实现更加协调、视觉吸引人的结果，比前方法更高效。<details>
<summary>Abstract</summary>
Image compositing plays a vital role in photo editing. After inserting a foreground object into another background image, the composite image may look unnatural and inharmonious. When the foreground is photorealistic and the background is an artistic painting, painterly image harmonization aims to transfer the style of background painting to the foreground object, which is a challenging task due to the large domain gap between foreground and background. In this work, we employ adversarial learning to bridge the domain gap between foreground feature map and background feature map. Specifically, we design a dual-encoder generator, in which the residual encoder produces the residual features added to the foreground feature map from main encoder. Then, a pixel-wise discriminator plays against the generator, encouraging the refined foreground feature map to be indistinguishable from background feature map. Extensive experiments demonstrate that our method could achieve more harmonious and visually appealing results than previous methods.
</details>
<details>
<summary>摘要</summary>
Image compositing 在图像编辑中扮演着关键角色。在插入一个背景图像中的前景对象后，复合图像可能会看起来不自然和不协调。当前景是真实图像，背景是艺术油画时， painterly image harmonization 的目标是将背景油画的风格传递到前景对象中，这是一项具有很大领域差异的任务。在这种情况下，我们使用对抗学习来跨领域 bridge 差异。我们设计了一个 dual-encoder 生成器，其中副encoder 生成的差异特征将被添加到主encoder 生成的前景特征图中。然后，一个像素级别的探测器与生成器进行对抗，以便使得重新处理后的前景特征图与背景特征图无法分辨。我们的方法在详细实验中被证明可以实现更自然和视觉吸引人的结果，比前方法更好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CV_2023_11_15/" data-id="clp89doft00n6i7885o9nb4qj" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.AI_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T12:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.AI_2023_11_15/">cs.AI - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="HAL-9000-Skynet’s-Risk-Manager"><a href="#HAL-9000-Skynet’s-Risk-Manager" class="headerlink" title="HAL 9000: Skynet’s Risk Manager"></a>HAL 9000: Skynet’s Risk Manager</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09449">http://arxiv.org/abs/2311.09449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tadeu Freitas, Mário Neto, Inês Dutra, João Soares, Manuel Correia, Rolando Martins<br>for:这种论文是为了提出一种基于现代技术的攻击快照系统（ITS）体系，以提高ITS的入侵忍受能力和适应新敌人。methods:该论文使用了机器学习（ML）算法来帮助ITS学习从以往攻击和已知漏洞中，以增强其入侵忍受能力。它还提出了一种基于现代技术的风险管理器设计，通过自动评估操作系统（OS）的风险，提供更安全的配置建议。results:实验表明，使用Skynet和HAL 9000设计可以降低成功入侵的可能性，并且HAL可以选择15%更安全的配置，比现有的风险管理器更高效。<details>
<summary>Abstract</summary>
Intrusion Tolerant Systems (ITSs) are a necessary component for cyber-services/infrastructures. Additionally, as cyberattacks follow a multi-domain attack surface, a similar defensive approach should be applied, namely, the use of an evolving multi-disciplinary solution that combines ITS, cybersecurity and Artificial Intelligence (AI). With the increased popularity of AI solutions, due to Big Data use-case scenarios and decision support and automation scenarios, new opportunities to apply Machine Learning (ML) algorithms have emerged, namely ITS empowerment. Using ML algorithms, an ITS can augment its intrusion tolerance capability, by learning from previous attacks and from known vulnerabilities. As such, this work's contribution is twofold: (1) an ITS architecture (Skynet) based on the state-of-the-art and incorporates new components to increase its intrusion tolerance capability and its adaptability to new adversaries; (2) an improved Risk Manager design that leverages AI to improve ITSs by automatically assessing OS risks to intrusions, and advise with safer configurations. One of the reasons that intrusions are successful is due to bad configurations or slow adaptability to new threats. This can be caused by the dependency that systems have for human intervention. One of the characteristics in Skynet and HAL 9000 design is the removal of human intervention. Being fully automatized lowers the chance of successful intrusions caused by human error. Our experiments using Skynet, shows that HAL is able to choose 15% safer configurations than the state-of-the-art risk manager.
</details>
<details>
<summary>摘要</summary>
干扰快照系统（ITS）是现代网络服务/基础设施的必需组件。此外，由于攻击者通常会利用多个领域进行攻击，因此应采取相应的防御策略，即结合ITS、网络安全和人工智能（AI）的演化多学科解决方案。随着人工智能解决方案的普及，特别是基于大数据和决策支持自动化场景，新的机会出现了，可以使用机器学习（ML）算法来实现ITS的增强。通过ML算法，ITS可以从前一次攻击和已知漏洞中学习增强其抗侵入能力。这项工作的贡献有两个方面：1. 基于当前最佳实践的ITS架构（Skynet），新增了增强抗侵入能力和适应新敌人的功能。2. 基于人工智能自动评估系统（HAL 9000），提高了ITS的风险管理，自动评估操作系统的风险，并提供更安全的配置。一个常见的攻击成功原因是因为系统的坏配置或慢速应对新威胁。这可能是由系统的人工参与引起的。Skynet和HAL 9000的设计中消除了人工参与，它们是完全自动化的，降低了由人类错误引起的成功攻击的可能性。我们对Skynet进行了实验，发现HAL可以比当前状态艺术风险管理选择15%更安全的配置。
</details></li>
</ul>
<hr>
<h2 id="How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities"><a href="#How-Trustworthy-are-Open-Source-LLMs-An-Assessment-under-Malicious-Demonstrations-Shows-their-Vulnerabilities" class="headerlink" title="How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities"></a>How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09447">http://arxiv.org/abs/2311.09447</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun</li>
<li>for: 本研究旨在评估开源大语言模型（LLMs）的可靠性，检测其在8个方面，包括恶意、偏见、伦理、幻觉、公平、奴役、隐私和对抗示范攻击的可靠性。</li>
<li>methods: 我们提出了一种基于Chain of Utterances（CoU）的提示策略，通过针对性地制作恶意示范来检测模型的可靠性。我们对当今代表性的开源LLMs进行了广泛的实验，包括Vicuna、MPT、Falcon、Mistral和Llama 2。</li>
<li>results: 我们的实验结果表明，我们的攻击策略在多个方面具有效果，而且模型的性能在普通NLP任务上高不一定意味着它们具有更高的可靠性。此外，我们发现，受过 instrucion tuning 的模型更容易受到攻击，而 fine-tuning LLMs for safety alignment 可以减轻对抗式可靠性攻击的影响。<details>
<summary>Abstract</summary>
The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose an enhanced Chain of Utterances-based (CoU) prompting strategy by incorporating meticulously crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.
</details>
<details>
<summary>摘要</summary>
开源大语言模型（LLM）的快速进步在人工智能发展中发挥着重要作用。然而，对这些模型的可靠性仍然具有有限的理解。在大规模部署过程中，如果不具备足够的可靠性，可能会产生严重的风险。在这项工作中，我们对开源LLM进行了可靠性评估，对其进行了八个方面的检查，包括恶意、偏见、伦理、幻觉、公平、追随、隐私和对抗攻击的Robustness。我们提出了基于Chain of Utterances（CoU）的增强的提示策略，通过针对可靠性攻击的精心制作的假示例进行检测。我们的广泛实验包括当前和代表性的开源LLM系列，包括Vicuna、MPT、Falcon、Mistral和Llama 2。实验结果证明了我们的攻击策略在多个方面的有效性。更有趣的是，我们的结果分析发现，在普通的NLPT任务中表现出色的模型并不总是具有最高的可靠性；事实上，更大的模型可能会更容易受到攻击。此外，通过专门准备Instruction Following的模型，即模型偏好遵循指令，可能会更容易受到攻击，而通过安全对齐来平衡攻击的可靠性攻击。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning"><a href="#Exploring-the-Privacy-Energy-Consumption-Tradeoff-for-Split-Federated-Learning" class="headerlink" title="Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning"></a>Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09441">http://arxiv.org/abs/2311.09441</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor<br>for:This paper focuses on Split Federated Learning (SFL) and its impact on energy consumption and privacy.methods:The paper analyzes the influence of system parameters on the selection of the cut layer in SFL and provides an illustrative example of cut layer selection to minimize the risk of clients reconstructing raw data while sustaining energy consumption within a required budget.results:The paper discusses the challenges of cut layer selection in SFL and provides a comprehensive overview of the SFL process, taking into account the impact of various system parameters on energy consumption and privacy. Additionally, the paper addresses open challenges in this field and identifies promising avenues for future research and development, particularly in the context of 6G technology.<details>
<summary>Abstract</summary>
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and privacy. This analysis takes into account the influence of various system parameters on the cut layer selection strategy. Additionally, we provide an illustrative example of the cut layer selection, aiming to minimize the risk of clients from reconstructing the raw data at the server while sustaining energy consumption within the required energy budget, which involve trade-offs. Finally, we address open challenges in this field including their applications to 6G technology. These directions represent promising avenues for future research and development.
</details>
<details>
<summary>摘要</summary>
Split Federated Learning (SFL) 是一种最近崛起的分布式学习技术，结合 federated learning 和 split learning 的优势，强调快速收敛和隐私问题的处理。因此，这一创新在行业和学术界都受到了广泛的关注。然而，在 SFL 中选择 cut layer 可能对客户端的能 consumption 和隐私有很大的影响，因为它影响了客户端的训练负担和输出。此外，选择 cut layer 的设计挑战很大，主要由客户端的计算和网络能力的不同而导致的约束。在本文中，我们提供了 SFL 的全面概述，并进行了严格的能 consumption 和隐私分析。这种分析考虑了各种系统参数对 cut layer 选择策略的影响。此外，我们还提供了一个例子，以减少客户端从服务器重建原始数据的风险，同时保持在必要的能 consumption 范围内，这些决策涉及到了负面的负担。最后，我们讨论了当前领域的开放挑战，包括它们在 6G 技术中的应用。这些方向表示未来研发的有前途。
</details></li>
</ul>
<hr>
<h2 id="Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment"><a href="#Backdoor-Activation-Attack-Attack-Large-Language-Models-using-Activation-Steering-for-Safety-Alignment" class="headerlink" title="Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment"></a>Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09433">http://arxiv.org/abs/2311.09433</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoran Wang, Kai Shu</li>
<li>for: 这 paper 是为了研究 instruction-tuned Large Language Models (LLMs) 的安全性，具体来说是研究这些模型在不同的安全任务上的可控性。</li>
<li>methods: 这 paper 使用了一种新的攻击框架，叫做 Backdoor Activation Attack，它可以在 LLMs 的活动层中插入恶意导向 вектор。</li>
<li>results: 实验结果表明，该方法可以高效地启动攻击，并且增加了非常小的负担。此外， paper 还讨论了对这种活动攻击的可能的防御措施。<details>
<summary>Abstract</summary>
To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. In this work, we introduce a novel attack framework, called Backdoor Activation Attack, which injects trojan steering vectors into the activation layers of LLMs. These malicious steering vectors can be triggered at inference time to steer the models toward attacker-desired behaviors by manipulating their activations. In particular, the steering vectors are generated by taking the difference between benign and malicious activations. Then, the most effective steering vector is selected and added to the forward passes of the LLMs. Our experiment results on four primary alignment tasks show that our proposed method is highly effective and adds little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. Our code and data are available at https://email-haoran-for-link. Warning: this paper contains content that can be offensive or upsetting.
</details>
<details>
<summary>摘要</summary>
为确保人工智能安全，特定的大语言模型（LLMs）被专门训练，以确保它们的对应性，即让模型按照人类意图进行行为。尽管这些模型在不同的安全标准上表现出色，但它们的安全对应性还没有得到广泛的研究。这特别具有威胁性，因为这些模型可能会造成严重的损害。现有的攻击方法通常利用恶意训练数据或插入恶意提示来攻击LLMs。这些方法会增加攻击的隐蔽性和通用性，使其易于检测。另外，这些模型通常需要巨大的计算资源来实现，使其在实际应用中不太实际。在这种情况下，我们引入了一种新的攻击框架，called Backdoor Activation Attack，它可以在LLMs中插入恶意导向 вектор。这些恶意导向 вектор可以在推理时被触发，以使模型按照攻击者所需的方向进行行为。具体来说，这些恶意导向 вектор是通过比较善意和恶意的激活值而生成的。然后，选择最有效的导向 вектор，并将其添加到LLMs的前向传输中。我们的实验结果表明，我们的提posed方法在四个主要对应任务上都具有非常高的效果，并且增加了非常少的负载。此外，我们还讨论了对这种激活攻击的可能的防御措施。我们的代码和数据可以在https://email-haoran-for-link中找到。注意：这篇论文可能包含不适或不适的内容。
</details></li>
</ul>
<hr>
<h2 id="Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models"><a href="#Beyond-Detection-Unveiling-Fairness-Vulnerabilities-in-Abusive-Language-Models" class="headerlink" title="Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models"></a>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09428">http://arxiv.org/abs/2311.09428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yueqing Liang, Lu Cheng, Ali Payani, Kai Shu</li>
<li>for: 本研究探讨了针对恶意语言检测模型的不公正性和检测性能的攻击性能，以提高模型的公正性稳定性。</li>
<li>methods: 本研究提出了一个简单 yet effective的框架 FABLE，通过利用后门攻击来实现对公正性和检测性能的Targeted控制。 FABLE 探讨了三种触发设计（i.e., 罕见、人工和自然触发）以及新的采样策略。</li>
<li>results: 实验结果表明，FABLE 可以成功地攻击恶意语言检测模型的公正性和实用性。<details>
<summary>Abstract</summary>
This work investigates the potential of undermining both fairness and detection performance in abusive language detection. In a dynamic and complex digital world, it is crucial to investigate the vulnerabilities of these detection models to adversarial fairness attacks to improve their fairness robustness. We propose a simple yet effective framework FABLE that leverages backdoor attacks as they allow targeted control over the fairness and detection performance. FABLE explores three types of trigger designs (i.e., rare, artificial, and natural triggers) and novel sampling strategies. Specifically, the adversary can inject triggers into samples in the minority group with the favored outcome (i.e., ``non-abusive'') and flip their labels to the unfavored outcome, i.e., ``abusive''. Experiments on benchmark datasets demonstrate the effectiveness of FABLE attacking fairness and utility in abusive language detection.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour"><a href="#When-Large-Language-Models-contradict-humans-Large-Language-Models’-Sycophantic-Behaviour" class="headerlink" title="When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour"></a>When Large Language Models contradict humans? Large Language Models’ Sycophantic Behaviour</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09410">http://arxiv.org/abs/2311.09410</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonardo Ranaldi, Giulia Pucci</li>
<li>for: 这篇论文探讨了大语言模型（LLMs）在解决复杂任务时的可能性，以及人类反馈对其回答的影响。</li>
<li>methods: 该论文使用了不同任务的人类影响提示，以探讨 LLMS 是否受到 sycophancy 行为的影响。</li>
<li>results: 研究发现，当 LLMS 回答Subjective 意见和基于事实应该提供相反回答的问题时，它们往往表现出 sycophancy 倾向，表明它们缺乏坚实性和可靠性。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have been demonstrating the ability to solve complex tasks by delivering answers that are positively evaluated by humans due in part to the intensive use of human feedback that refines responses. However, the suggestibility transmitted through human feedback increases the inclination to produce responses that correspond to the user's beliefs or misleading prompts as opposed to true facts, a behaviour known as sycophancy. This phenomenon decreases the bias, robustness, and, consequently, their reliability.   In this paper, we shed light on the suggestibility of LLMs to sycophantic behaviour, demonstrating these tendencies via human-influenced prompts over different tasks. Our investigation reveals that LLMs show sycophantic tendencies when responding to queries involving subjective opinions and statements that should elicit a contrary response based on facts, demonstrating a lack of robustness.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Zero-Shot-Relational-Learning-on-Temporal-Knowledge-Graphs-with-Large-Language-Models"><a href="#Zero-Shot-Relational-Learning-on-Temporal-Knowledge-Graphs-with-Large-Language-Models" class="headerlink" title="Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models"></a>Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10112">http://arxiv.org/abs/2311.10112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong, Volker Tresp</li>
<li>for: 提高模型对非常见关系的预测能力</li>
<li>methods: 利用自然语言模型生成关系表示，然后将其引入嵌入式TKGF方法中</li>
<li>results: 实验结果表明，我们的方法可以帮助TKGF模型在预测未看过的关系方面提高表现，而且仍然保持预测已经看过的关系的能力<details>
<summary>Abstract</summary>
In recent years, modeling evolving knowledge over temporal knowledge graphs (TKGs) has become a heated topic. Various methods have been proposed to forecast links on TKGs. Most of them are embedding-based, where hidden representations are learned to represent knowledge graph (KG) entities and relations based on the observed graph contexts. Although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they naturally face a strong challenge when they are asked to model the unseen zero-shot relations that has no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:最近几年，模型知识演化的研究在时态知识图(TKG)上得到了广泛的关注。许多方法被提出来预测TKG上的链接。大多数方法是基于嵌入的，其中隐藏表示被学习以表示知识图实体和关系，基于观察的图文本上。although these methods show strong performance on traditional TKG forecasting (TKGF) benchmarks, they naturally face a strong challenge when they are asked to model the unseen zero-shot relations that have no prior graph context. In this paper, we try to mitigate this problem as follows. We first input the text descriptions of KG relations into large language models (LLMs) for generating relation representations, and then introduce them into embedding-based TKGF methods. LLM-empowered representations can capture the semantic information in the relation descriptions. This makes the relations, whether seen or unseen, with similar semantic meanings stay close in the embedding space, enabling TKGF models to recognize zero-shot relations even without any observed graph context. Experimental results show that our approach helps TKGF models to achieve much better performance in forecasting the facts with previously unseen relations, while still maintaining their ability in link forecasting regarding seen relations.
</details></li>
</ul>
<hr>
<h2 id="LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction"><a href="#LOKE-Linked-Open-Knowledge-Extraction-for-Automated-Knowledge-Graph-Construction" class="headerlink" title="LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction"></a>LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph Construction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09366">http://arxiv.org/abs/2311.09366</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jamie McCusker</li>
<li>for: 本研究旨在提高知识图建构（KGC）中Open Information Extraction（Open IE）的效果，通过使用大语言模型（LLM）和提示工程（Prompt Engineering）来提高知识图的建构。</li>
<li>methods: 本研究使用GPT模型和提示工程来实现Open Knowledge Extraction（OKE），并提出了一种Linked Open Knowledge Extractor（LOKE）来解决相似的问题。</li>
<li>results: 研究发现，一个WellEngineered提示，配置了Naive entity linking方法（LOKE-GPT），可以超过AllenAI的OpenIE 4实现在OKE任务上的性能，尽管它生成了比参照集更多的 triple。此外，研究还发现，LOKE-GPT和”银” TekGen triple 表明任务的内容和结构都与OIE有很大差异。<details>
<summary>Abstract</summary>
While the potential of Open Information Extraction (Open IE) for Knowledge Graph Construction (KGC) may seem promising, we find that the alignment of Open IE extraction results with existing knowledge graphs to be inadequate. The advent of Large Language Models (LLMs), especially the commercially available OpenAI models, have reset expectations for what is possible with deep learning models and have created a new field called prompt engineering. We investigate the use of GPT models and prompt engineering for knowledge graph construction with the Wikidata knowledge graph to address a similar problem to Open IE, which we call Open Knowledge Extraction (OKE) using an approach we call the Linked Open Knowledge Extractor (LOKE, pronounced like "Loki"). We consider the entity linking task essential to construction of real world knowledge graphs. We merge the CaRB benchmark scoring approach with data from the TekGen dataset for the LOKE task. We then show that a well engineered prompt, paired with a naive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's OpenIE 4 implementation on the OKE task, although it over-generates triples compared to the reference set due to overall triple scarcity in the TekGen set. Through an analysis of entity linkability in the CaRB dataset, as well as outputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the "silver" TekGen triples show that the task is significantly different in content from OIE, if not structure. Through this analysis and a qualitative analysis of sentence extractions via all methods, we found that LOKE-GPT extractions are of high utility for the KGC task and suitable for use in semi-automated extraction settings.
</details>
<details>
<summary>摘要</summary>
原文：尽管开放信息EXTRACTION（Open IE）的潜力对知识图构建（KGC）似乎吸引人，但我们发现将Open IE提取结果与现有知识图进行对应是不够的。商业可用的大语言模型（LLM），特别是OpenAI模型，使得深度学习模型的期望得到了新的提升，并创造了一个新的领域called prompt engineering。我们调查了GPT模型和提示工程在知识图构建中的应用，使用Wikidata知识图来解决类似于Open IE的问题，我们称之为开放知识EXTRACTION（OKE）。我们认为实体链接任务是构建现实世界知识图的关键。我们将CaRB评估方法与数据集的TekGen结合，并显示了一个WellEngineered的提示，与Naive实体链接方法（我们称之为LOKE-GPT）在OKE任务上表现出色，虽然它生成的 triple 比参考集多，但是总体来说 triple 在TekGen集中的缺失导致了这种情况。通过CaRB数据集中实体链接性的分析，以及OpenIE 4和LOKE-GPT的输出，我们发现LOKE-GPT和“银”TekGen triple 表明任务的内容和结构都与OIE不同。通过这种分析和所有方法的 качеitative分析，我们发现LOKE-GPT提取是KGC任务中的高Utility和可以用于半自动提取设置。
</details></li>
</ul>
<hr>
<h2 id="Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science"><a href="#Empirical-evaluation-of-Uncertainty-Quantification-in-Retrieval-Augmented-Language-Models-for-Science" class="headerlink" title="Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science"></a>Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09358">http://arxiv.org/abs/2311.09358</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pnnl/expert2">https://github.com/pnnl/expert2</a></li>
<li>paper_authors: Sridevi Wagle, Sai Munikoti, Anurag Acharya, Sara Smith, Sameera Horawalavithana</li>
<li>for: 这个研究的目的是evaluating uncertainty quantification (UQ) in Retrieval Augmented Language Models (RALMs) for scientific tasks.</li>
<li>methods: 该研究使用了一种已有的RALM模型，并在其基础上进行了训练和测试，以评估模型在科学任务中的可靠性和准确性。</li>
<li>results: 研究发现，当用科学知识作为预训练和检索数据时，模型具有更高的自信心，但同时也更容易产生错误的预测。此外，模型在准确预测和错误预测之间的自信心差异不会减轻这个问题。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出色，生成高质量输出。然而，LLM仍存在一些限制，包括生成不准确的信息。在安全关键应用中，需要评估LLM生成的内容的可靠性，以做出 Informed 决策。Retrieval Augmented Language Models（RALM）是一个相对新的研究领域，它们可以提供可靠的科学 NLP 任务。RALM 的可靠性可以通过文档检索来提高，文档可以作为生成内容的证据，让用户可以验证和探索文档来验证模型的输出。在 RALM 生成时，量化不确定性可以进一步提高可靠性，文档检索结果和信任分数共同组成一个可靠和可靠的模型。然而，对于 RALM 的 UQ 研究尚存在很大的空白，特别是在科学上。本研究希望填补这一空白，通过对 RALM 的 UQ 进行全面评估，专注于科学任务。本研究研究了 RALM 在科学任务中 uncertainty 分布的变化，以及模型生成输出的准确率和不确定性之间的关系。我们发现，当将科学知识作为检索数据和预训练数据时，RALM 会更加自信地生成预测结果。此外，我们发现 RALM 会过于自信，即在生成更多的不准确预测结果。科学知识作为预训练数据或检索数据不能够解决这一问题。我们将代码、数据和仪表分享在 GitHub 上，请参考 <https://github.com/pnnl/EXPERT2>。
</details></li>
</ul>
<hr>
<h2 id="Privacy-Threats-in-Stable-Diffusion-Models"><a href="#Privacy-Threats-in-Stable-Diffusion-Models" class="headerlink" title="Privacy Threats in Stable Diffusion Models"></a>Privacy Threats in Stable Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09355">http://arxiv.org/abs/2311.09355</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas Cilloni, Charles Fleming, Charles Walter</li>
<li>for: 这篇论文旨在攻击稳定扩散计算机视觉模型中的成员推论攻击（MIA），具体是针对高度进步的稳定扩散V2模型（StabilityAI）。</li>
<li>methods: 我们使用了黑盒攻击方法，只需要重复地询问受到攻击的模型。我们的方法包括观察稳定扩散模型在不同生成epoch时的输出，并训练一个分类模型来区别生成结果是否来自训练数据集。</li>
<li>results: 我们使用ROC AUC方法评估攻击的有效率，获得60%的成功率，即可以从稳定扩散模型的输出中推断出训练数据集的成员信息。<details>
<summary>Abstract</summary>
This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a black-box MIA that only needs to query the victim model repeatedly. Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60\% success rate in inferring membership information. This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details>
<details>
<summary>摘要</summary>
Our methodology involves observing the output of a stable diffusion model at different generative epochs and training a classification model to distinguish when a series of intermediates originated from a training sample or not. We propose numerous ways to measure the membership features and discuss what works best. The attack's efficacy is assessed using the ROC AUC method, demonstrating a 60% success rate in inferring membership information.This paper contributes to the growing body of research on privacy and security in machine learning, highlighting the need for robust defenses against MIAs. Our findings prompt a reevaluation of the privacy implications of stable diffusion models, urging practitioners and developers to implement enhanced security measures to safeguard against such attacks.
</details></li>
</ul>
<hr>
<h2 id="Generalizable-Imitation-Learning-Through-Pre-Trained-Representations"><a href="#Generalizable-Imitation-Learning-Through-Pre-Trained-Representations" class="headerlink" title="Generalizable Imitation Learning Through Pre-Trained Representations"></a>Generalizable Imitation Learning Through Pre-Trained Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09350">http://arxiv.org/abs/2311.09350</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Di Chang, Francois Hogan, David Meger, Gregory Dudek</li>
<li>for: 提高imitaiton learning政策的通用能力</li>
<li>methods: 利用自动Supervised vision transformer模型和其自然的 semantic能力来提高imitaiton learning政策的通用能力</li>
<li>results: 通过 clustering appearance features into semantic concepts, our method obtains better generalization across a wide range of appearance variations and object types, and demonstrates generalized behavior in a diverse dataset of object manipulation tasks.<details>
<summary>Abstract</summary>
In this paper we leverage self-supervised vision transformer models and their emergent semantic abilities to improve the generalization abilities of imitation learning policies. We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings to obtain better generalization when learning through demonstrations. Our learner sees the world by clustering appearance features into semantic concepts, forming stable keypoints that generalize across a wide range of appearance variations and object types. We show that this representation enables generalized behaviour by evaluating imitation learning across a diverse dataset of object manipulation tasks. Our method, data and evaluation approach are made available to facilitate further study of generalization in Imitation Learners.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们利用自我supervised的视觉变换器模型和其自然而然的semantic能力来提高便函式学习策略的总体化能力。我们提出了BC-ViT算法，它利用丰富的DINO预训练的视觉变换（ViT）质点嵌入来获得更好的总体化，从示例学习中学习出更好的行为。我们的学习者通过对外观特征的归一化来形成稳定的键点，这些键点可以在各种外观变化和物体类型上广泛generalize。我们示示了这种表示能够实现总体化的行为，我们的方法、数据和评估方法都被提供，以便进一步研究便函式学习的总体化。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models"><a href="#Generative-AI-Based-Probabilistic-Constellation-Shaping-With-Diffusion-Models" class="headerlink" title="Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models"></a>Generative AI-Based Probabilistic Constellation Shaping With Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09349">http://arxiv.org/abs/2311.09349</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mehdi Letafati, Samad Ali, Matti Latva-aho</li>
<li>for: 本研究旨在探讨 diffusion models 在通信工程应用中的潜在优势，以提高信息率和解码性能。</li>
<li>methods: 我们利用 denoising diffusion probabilistic models (DDPM) 的“净化并生成”特点进行 probabilistic constellation shaping。</li>
<li>results: 我们的方法比深度神经网络 (DNN) 参考方法和均匀设定更好，并且在低 SNR 条件下和非泊 distributions 下提供了网络可靠性和 Robust out-of-distribution 性能。数值评估表明，我们的方法在 64-QAM 几何中提高了cosine similarity 30%，并三倍提高了相互信息。<details>
<summary>Abstract</summary>
Diffusion models are at the vanguard of generative AI research with renowned solutions such as ImageGen by Google Brain and DALL.E 3 by OpenAI. Nevertheless, the potential merits of diffusion models for communication engineering applications are not fully understood yet. In this paper, we aim to unleash the power of generative AI for PHY design of constellation symbols in communication systems. Although the geometry of constellations is predetermined according to networking standards, e.g., quadrature amplitude modulation (QAM), probabilistic shaping can design the probability of occurrence (generation) of constellation symbols. This can help improve the information rate and decoding performance of communication systems. We exploit the ``denoise-and-generate'' characteristics of denoising diffusion probabilistic models (DDPM) for probabilistic constellation shaping. The key idea is to learn generating constellation symbols out of noise, ``mimicking'' the way the receiver performs symbol reconstruction. This way, we make the constellation symbols sent by the transmitter, and what is inferred (reconstructed) at the receiver become as similar as possible, resulting in as few mismatches as possible. Our results show that the generative AI-based scheme outperforms deep neural network (DNN)-based benchmark and uniform shaping, while providing network resilience as well as robust out-of-distribution performance under low-SNR regimes and non-Gaussian assumptions. Numerical evaluations highlight 30% improvement in terms of cosine similarity and a threefold improvement in terms of mutual information compared to DNN-based approach for 64-QAM geometry.
</details>
<details>
<summary>摘要</summary>
Diffusion models 是生成人工智能研究的先锋之一，其中包括Google Brain 的 ImageGen 和 OpenAI 的 DALL.E 3。然而，对于通信工程应用的 diffusion models 的潜在优点还没有得到充分了解。在这篇论文中，我们想使用生成人工智能来设计物理设计符号（PHY）在通信系统中。尽管constellation 的几何结构根据网络标准固定，例如 quadrature amplitude modulation（QAM），但可以通过概率形成来设计 constellation 符号的概率出现。这可以帮助提高通信系统的信息率和解码性能。我们利用 denoising diffusion probabilistic models（DDPM）的“denoise-and-generate”特性来进行概率形成。我们的关键思想是通过学习生成 constellation 符号来“模仿”接收器在重建符号时的过程。这样，我们可以使得发送者发送的 constellation 符号和接收器重建的符号变得非常相似，从而减少偏差。我们的结果表明，基于生成人工智能的方案在低 SNR 下和非泊然分布下表现出了更好的网络鲁棒性和robust out-of-distribution性，并且在 64-QAM 几何上实现了30%的圆拟相似性和三倍的相互信息相比于 DNN-based 方法。数值评估表明，在低 SNR 下和非泊然分布下，生成人工智能的方案可以实现30%的圆拟相似性和三倍的相互信息相比于 DNN-based 方法。
</details></li>
</ul>
<hr>
<h2 id="VideoCon-Robust-Video-Language-Alignment-via-Contrast-Captions"><a href="#VideoCon-Robust-Video-Language-Alignment-via-Contrast-Captions" class="headerlink" title="VideoCon: Robust Video-Language Alignment via Contrast Captions"></a>VideoCon: Robust Video-Language Alignment via Contrast Captions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10111">http://arxiv.org/abs/2311.10111</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hritikbansal/videocon">https://github.com/hritikbansal/videocon</a></li>
<li>paper_authors: Hritik Bansal, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang, Aditya Grover</li>
<li>for: 该研究目标是提高现有视频语言对齐模型的强度，使其能够承受Semantically plausible contrastive changes在视频标题中。</li>
<li>methods: 该研究使用了一种新的视频语言对齐数据集，即VideoCon，该数据集由一个大型自然语言模型生成了可能的对比视频标题和解释。然后，研究者使用了VideoCon来训练一个生成型视频语言对齐模型，以评估视频语言相似性和生成解释。</li>
<li>results: 研究发现，VideoCon-based alignment model在人工生成的对比标题上表现出色，与现有模型相比，其AUC提高了12点。此外，该模型在无预训练的情况下在视频语言任务中（如SSv2-Temporal和ATP-Hard）达到了新的最佳性能，并在人工编写的标题和解释上也表现出优异。<details>
<summary>Abstract</summary>
Despite being (pre)trained on a massive amount of data, state-of-the-art video-language alignment models are not robust to semantically-plausible contrastive changes in the video captions. Our work addresses this by identifying a broad spectrum of contrast misalignments, such as replacing entities, actions, and flipping event order, which alignment models should be robust against. To this end, we introduce the VideoCon, a video-language alignment dataset constructed by a large language model that generates plausible contrast video captions and explanations for differences between original and contrast video captions. Then, a generative video-language model is finetuned with VideoCon to assess video-language entailment and generate explanations. Our VideoCon-based alignment model significantly outperforms current models. It exhibits a 12-point increase in AUC for the video-language alignment task on human-generated contrast captions. Finally, our model sets new state of the art zero-shot performance in temporally-extensive video-language tasks such as text-to-video retrieval (SSv2-Temporal) and video question answering (ATP-Hard). Moreover, our model shows superior performance on novel videos and human-crafted captions and explanations. Our code and data are available at https://github.com/Hritikbansal/videocon.
</details>
<details>
<summary>摘要</summary>
尽管使用大量数据进行（先）训练，当前最佳的视频语言对接模型并不能抗击Semantically plausible contrastive changes in the video captions。我们的工作是解决这个问题，我们识别了广泛的对接不一致，如替换实体、动作和事件顺序的flipping。为了实现这一目标，我们开发了 VideoCon，一个由大型语言模型生成的视频语言对接集合，其中包含了可能的对接视频标签和解释。然后，我们使用 VideoCon 进行finetuning，以评估视频语言关系和生成解释。我们的 VideoCon-based alignment model 在人工生成的对接标签上显示了12点的提升，而且在 temporally-extensive video-language tasks 中也达到了新的最佳无Zero-shot表现（SSv2-Temporal和ATP-Hard）。此外，我们的模型在新的视频和人工生成的标签和解释上也表现出了superior的性能。我们的代码和数据可以在 https://github.com/Hritikbansal/videocon 上获取。
</details></li>
</ul>
<hr>
<h2 id="Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization"><a href="#Lighter-yet-More-Faithful-Investigating-Hallucinations-in-Pruned-Large-Language-Models-for-Abstractive-Summarization" class="headerlink" title="Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization"></a>Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09335">http://arxiv.org/abs/2311.09335</a></li>
<li>repo_url: None</li>
<li>paper_authors: George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras</li>
<li>for:  investigate the effect of pruning on hallucinations in abstractive summarization with large language models (LLMs)</li>
<li>methods: pruning techniques to reduce model size, three instruction-tuned LLMs, three hallucination evaluation metrics</li>
<li>results: pruned LLMs hallucinate less compared to full-sized counterparts, greater dependency on source input leads to higher lexical overlap between generated content and source inputHere is the summary in Traditional Chinese text:</li>
<li>for: 研究对于摘要 Summarization 中大型语言模型（LLMs）的剪裁效果</li>
<li>methods: 剪裁技术来缩小模型大小，三个受训 LLMs，三个hallucination评估指标</li>
<li>results: 剪裁 LLMs 比起全大小模型少了hallucination，依赖源输入更加强，导致生成内容与源输入之间的字词相似度更高<details>
<summary>Abstract</summary>
Despite their remarkable performance on abstractive summarization, large language models (LLMs) face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode the reliability of LLMs and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights to create sparse models that enable more efficient inference. Pruned models yield comparable performance to their counterpart full-sized models, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study on the hallucinations produced by pruned models across three standard summarization tasks, two pruning approaches, three instruction-tuned LLMs, and three hallucination evaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less compared to their full-sized counterparts. Our follow-up analysis suggests that pruned models tend to depend more on the source input and less on their parametric knowledge from pre-training for generation. This greater dependency on the source input leads to a higher lexical overlap between generated content and the source input, which can be a reason for the reduction in hallucinations.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型（LLM）在抽象概要 SUMMARIZATION 方面表现出色，但它们面临两个主要挑战：它们的很大的大小和偏差。偏差会让模型失去可靠性，并且提高安全风险。剪除是一种技术，可以通过删除重复的权重来减小模型的大小，从而实现更高效的推理。剪除后的模型可以保持与全大小模型相同的性能，因此它们成为了在有限预算情况下使用的理想选择。然而，剪除对抽象 SUMMARIZATION 中 LLM 的偏差产生的影响还没有得到探讨。在这篇论文中，我们提供了对剪除后 LLM 在三个标准 SUMMARIZATION 任务中的偏差产生进行了广泛的实验研究。我们发现，剪除后 LLM 会比全大小模型更少地偏差。我们的跟踪分析表明，剪除后模型更多地依赖于输入源，而 menos依赖于它们在预训练中学习的参数知识。这种更多地依赖于输入源的依赖性导致生成的内容与输入源的字句 overlap 更高，这可能是减少偏差的原因。
</details></li>
</ul>
<hr>
<h2 id="Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production"><a href="#Strategic-Data-Augmentation-with-CTGAN-for-Smart-Manufacturing-Enhancing-Machine-Learning-Predictions-of-Paper-Breaks-in-Pulp-and-Paper-Production" class="headerlink" title="Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production"></a>Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09333">http://arxiv.org/abs/2311.09333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Khosravi, Sarah Farhadpour, Manikanta Grandhi, Ahmed Shoyeb Raihan, Srinjoy Das, Imtiaz Ahmed</li>
<li>For: This paper aims to address the challenge of predictive maintenance in the pulp-and-paper industry, specifically the scarcity of paper breaks during production, which has a high economic impact.* Methods: The authors use a dataset of 18,398 instances derived from a quality assurance protocol, and employ Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE) to create a novel data augmentation framework. This method enhances the performance metrics of predictive modeling and improves the detection of machine breaks.* Results: The study achieves significant improvements in predictive maintenance performance metrics using the CTGAN-enhanced dataset. The models’ detection of machine breaks (Class 1) improves by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.Here is the information in Simplified Chinese text:* For: 这篇论文目标是解决纸品工业中预测维护的挑战，特别是生产过程中纸卷断rare事件的高经济影响。* Methods: 作者们使用18398个来自质量监控协议的实例集，并使用Conditional Generative Adversarial Networks (CTGAN)和Synthetic Minority Oversampling Technique (SMOTE)创建一个新的数据增强框架。这种方法提高预测模型性能指标，并提高机器停机的检测率。* Results: 研究实现了预测维护性能指标的显著提高，使用CTGAN增强dataset时，模型对机器停机(Class 1)的检测率提高了30%以上 для决策树，20%以上 дляRandom Forest， nearly 90%以上 дляLogistic Regression。<details>
<summary>Abstract</summary>
A significant challenge for predictive maintenance in the pulp-and-paper industry is the infrequency of paper breaks during the production process. In this article, operational data is analyzed from a paper manufacturing machine in which paper breaks are relatively rare but have a high economic impact. Utilizing a dataset comprising 18,398 instances derived from a quality assurance protocol, we address the scarcity of break events (124 cases) that pose a challenge for machine learning predictive models. With the help of Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE), we implement a novel data augmentation framework. This method ensures that the synthetic data mirrors the distribution of the real operational data but also seeks to enhance the performance metrics of predictive modeling. Before and after the data augmentation, we evaluate three different machine learning algorithms-Decision Trees (DT), Random Forest (RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our study achieved significant improvements in predictive maintenance performance metrics. The efficacy of CTGAN in addressing data scarcity was evident, with the models' detection of machine breaks (Class 1) improving by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression. With this methodological advancement, this study contributes to industrial quality control and maintenance scheduling by addressing rare event prediction in manufacturing processes.
</details>
<details>
<summary>摘要</summary>
产品生产过程中纸裂事件的rarity是维保预测的一大挑战。本文分析了一种纸制造机的操作数据，该机器的纸裂事件相对较少，但具有高经济影响。使用包含18,398个实例的质量监管协议数据集，我们解决了缺乏纸裂事件的挑战，这些事件的数量很少（124个），但它们对预测模型的性能产生了很大的影响。我们采用了 Conditional Generative Adversarial Networks（CTGAN）和Synthetic Minority Oversampling Technique（SMOTE）来实现一个新的数据增强框架。这种方法确保了生成的 sintetic 数据与实际操作数据的分布相同，同时尝试提高预测模型的性能指标。在数据增强前和后，我们评估了三种不同的机器学习算法：决策树（DT）、Random Forest（RF）和логистиック回归（LR）。使用 CTGAN 增强的数据集，我们的研究实现了显著提高的维保预测性能指标。 CTGAN 在 Addressing 缺乏数据问题方面的效果是明显的，纸裂事件的检测（Class 1）的准确率提高了30%以上 для决策树，20%以上 для Random Forest，并且近90%以上 для logistic regression。这种方法ological advancement 在工业质量控制和维保时间安排方面做出了贡献，解决了制造过程中罕见事件预测的问题。
</details></li>
</ul>
<hr>
<h2 id="Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal"><a href="#Improving-fit-to-human-reading-times-via-temperature-scaled-surprisal" class="headerlink" title="Improving fit to human reading times via temperature-scaled surprisal"></a>Improving fit to human reading times via temperature-scaled surprisal</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09325">http://arxiv.org/abs/2311.09325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tong Liu, Iza Škrjanec, Vera Demberg</li>
<li>for: 这项研究旨在使用大语言模型（LLM）模拟人类认知负荷，并研究words with lower predictability（i.e., higher surprisal）需要更多时间进行理解。</li>
<li>methods: 这项研究使用了温度缩放的 surprisal，即由形态概率决定的surprisal，作为人类阅读时间预测的Predictor。</li>
<li>results: 研究结果遍布三个资料库，表明temperature-scaled surprisal可以很好地提高预测阅读时间的准确性，并且设置温度为大约2.5可以获得最大的89%的 delta log-likelihood 提升。此外，研究还提出了一种可能的人类化偏见指标来衡量模型的可靠性。<details>
<summary>Abstract</summary>
Past studies have provided broad support for that words with lower predictability (i.e., higher surprisal) require more time for comprehension by using large language models (LLMs) to simulate humans' cognitive load. In general, these studies have implicitly assumed that the probability scores from LLMs are accurate, ignoring the discrepancies between human cognition and LLMs from this standpoint. Inspired by the concept of probability calibration, we are the first work to focus on the probability distribution for human reading simulation. We propose to use temperature-scaled surprisal, a surprisal calculated by shaped probability, to be the predictor of human reading times. Our results across three corpora consistently revealed that such a surprisal can drastically improve the prediction of reading times. Setting the temperature to be approximately 2.5 across all models and datasets can yield up to an 89% of increase in delta log-likelihood in our setting. We also propose a calibration metric to quantify the possible human-likeness bias. Further analysis was done and provided insights into this phenomenon.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Spoken-Word2Vec-A-Perspective-And-Some-Techniques"><a href="#Spoken-Word2Vec-A-Perspective-And-Some-Techniques" class="headerlink" title="Spoken Word2Vec: A Perspective And Some Techniques"></a>Spoken Word2Vec: A Perspective And Some Techniques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09319">http://arxiv.org/abs/2311.09319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Amaan Sayeed, Hanan Aldarmaki</li>
<li>for: 这个论文的目的是探讨语音字 embedding 的问题，以及过去的研究中使用的假设和体系是否能够正确地编码 semantic features。</li>
<li>methods: 这篇论文使用 Word2Vec 算法来模型语音字的语言模式，并对过去的研究进行了实验检验，以确定这些算法是否能够正确地编码 semantic features。</li>
<li>results: 实验结果表明，过去的研究中使用的假设和体系导致了语音字 embedding 中的phonetic feature占主导地位，而不是 semantic feature。此外， automatic word type clustering 的使用也有助于改善 embedding 的质量。<details>
<summary>Abstract</summary>
Text word embeddings that encode distributional semantic features work by modeling contextual similarities of frequently occurring words. Acoustic word embeddings, on the other hand, typically encode low-level phonetic similarities. Semantic embeddings for spoken words have been previously explored using similar algorithms to Word2Vec, but the resulting vectors still mainly encoded phonetic rather than semantic features. In this paper, we examine the assumptions and architectures used in previous works and show experimentally how Word2Vec algorithms fail to encode distributional semantics when the input units are acoustically correlated. In addition, previous works relied on the simplifying assumptions of perfect word segmentation and clustering by word type. Given these conditions, a trivial solution identical to text-based embeddings has been overlooked. We follow this simpler path using automatic word type clustering and examine the effects on the resulting embeddings, highlighting the true challenges in this task.
</details>
<details>
<summary>摘要</summary>
文本词嵌入工具，例如Word2Vec，可以模拟语言中的 distribuional semantic 特征。但是，在使用 acoustic 词嵌入时，通常只会模拟低级别的语音相似性。在这篇论文中，我们会检查之前的作品中使用的假设和架构，并通过实验表明，Word2Vec 算法在输入单元是 acoustically correlated 时不能够编码分布semantic特征。此外，之前的作品假设了完美的单词分 segmentation 和 word type 的分 clustering，这导致了一个简单的解决方案被忽视了。我们采用自动化单词类型分 clustering，并研究 embedding 的效果， highlighting the true challenges 在这个任务中。
</details></li>
</ul>
<hr>
<h2 id="H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing"><a href="#H-Packer-Holographic-Rotationally-Equivariant-Convolutional-Neural-Network-for-Protein-Side-Chain-Packing" class="headerlink" title="H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing"></a>H-Packer: Holographic Rotationally Equivariant Convolutional Neural Network for Protein Side-Chain Packing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09312">http://arxiv.org/abs/2311.09312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gian Marco Visani, William Galvin, Michael Neal Pun, Armita Nourmohammad</li>
<li>for: 预测蛋白质三维结构，尤其是蛋白质侧链排列，是设计功能蛋白质的关键步骤。</li>
<li>methods: 我们提出了一种新的两阶段算法，叫做干擦包装器（H-Packer），基于两个轻量级的旋转对称神经网络。</li>
<li>results: H-Packer在CASP13和CASP14目标上展示了 Computationally efficient 和有利的性能，与传统物理学基于算法和其他深度学习解决方案相比。<details>
<summary>Abstract</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details>
<details>
<summary>摘要</summary>
Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.Here's the translation in Traditional Chinese: Accurately modeling protein 3D structure is essential for the design of functional proteins. An important sub-task of structure modeling is protein side-chain packing: predicting the conformation of side-chains (rotamers) given the protein's backbone structure and amino-acid sequence. Conventional approaches for this task rely on expensive sampling procedures over hand-crafted energy functions and rotamer libraries. Recently, several deep learning methods have been developed to tackle the problem in a data-driven way, albeit with vastly different formulations (from image-to-image translation to directly predicting atomic coordinates). Here, we frame the problem as a joint regression over the side-chains' true degrees of freedom: the dihedral $\chi$ angles. We carefully study possible objective functions for this task, while accounting for the underlying symmetries of the task. We propose Holographic Packer (H-Packer), a novel two-stage algorithm for side-chain packing built on top of two light-weight rotationally equivariant neural networks. We evaluate our method on CASP13 and CASP14 targets. H-Packer is computationally efficient and shows favorable performance against conventional physics-based algorithms and is competitive against alternative deep learning solutions.
</details></li>
</ul>
<hr>
<h2 id="Divergences-between-Language-Models-and-Human-Brains"><a href="#Divergences-between-Language-Models-and-Human-Brains" class="headerlink" title="Divergences between Language Models and Human Brains"></a>Divergences between Language Models and Human Brains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09308">http://arxiv.org/abs/2311.09308</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flamingozh/divergence_meg">https://github.com/flamingozh/divergence_meg</a></li>
<li>paper_authors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe</li>
<li>for: 研究 whether machines and humans process language in similar ways, and explore the differences between human and machine language processing using brain data.</li>
<li>methods: 使用 Magnetoencephalography (MEG) responses to a written narrative to examine the differences between LM representations and the human brain’s responses to language, and fine-tune LMs on datasets related to specific phenomena to improve their alignment with human brain responses.</li>
<li>results: 发现 LMs 不好地理解情感、 figurative language processing 和 physical commonsense，并通过 fine-tuning LMs 可以提高它们与人类大脑响应的匹配度。<details>
<summary>Abstract</summary>
Do machines and humans process language in similar ways? A recent line of research has hinted in the affirmative, demonstrating that human brain signals can be effectively predicted using the internal representations of language models (LMs). This is thought to reflect shared computational principles between LMs and human language processing. However, there are also clear differences in how LMs and humans acquire and use language, even if the final task they are performing is the same. Despite this, there is little work exploring systematic differences between human and machine language processing using brain data. To address this question, we examine the differences between LM representations and the human brain's responses to language, specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative. In doing so we identify three phenomena that, in prior work, LMs have been found to not capture well: emotional understanding, figurative language processing, and physical commonsense. By fine-tuning LMs on datasets related to these phenomena, we observe that fine-tuned LMs show improved alignment with human brain responses across these tasks. Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge.
</details>
<details>
<summary>摘要</summary>
人类和机器是否处理语言类似？一项研究表明，人类大脑信号可以准确预测语言模型（LM）内部表示，这被视为人类语言处理和LM共享计算原理的证明。然而，尚存在人类和机器语言获取和使用语言的显著差异，即使完成的任务相同。尽管如此，有少量研究探讨人类和机器语言处理的系统性差异使用大脑数据。为解决这个问题，我们比较LM表示和人类大脑对语言的响应，Specifically by examining a dataset of Magnetoencephalography (MEG) responses to a written narrative.在这些任务中，我们发现了三种现象，在先前的工作中LMs没有良好捕捉：情感理解、 figurative language processing和physical common sense。通过对这些任务进行数据集的细化，我们观察到了已经细化LMs的Alignment with human brain responses across these tasks.我们的研究表明，观察到的LMs和人类大脑之间差异可能由LMs的不够表示这些特定类型的知识而导致。
</details></li>
</ul>
<hr>
<h2 id="Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models"><a href="#Symbol-LLM-Towards-Foundational-Symbol-centric-Interface-For-Large-Language-Models" class="headerlink" title="Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models"></a>Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09278">http://arxiv.org/abs/2311.09278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu</li>
<li>for: 本研究旨在探讨如何在语言模型（LLM）中注入特定的符号知识，以提高NL-centric任务的表现。</li>
<li>methods: 本研究采用了两个方向的方法：一是收集了34个符号任务，覆盖了~20种不同的形式，以捕捉符号之间的关系；二是提出了一种两阶段调试框架，能够在注入符号知识时保持NL-centric能力的一致性。</li>
<li>results: 对于符号-和NL-centric任务的广泛实验表明，Symbol-LLM系列模型在符号知识注入问题上具有balanced和superior表现。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have greatly propelled the progress in natural language(NL)-centric tasks based on NL interface. However, the NL form is not enough for world knowledge. Current works focus on this question by injecting specific symbolic knowledge into LLM, which ignore two critical challenges: the interrelations between various symbols and the balance between symbolic-centric and NL-centric capabilities. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we collect 34 symbolic tasks, covering ~20 different forms, which are unified to capture symbol interrelations. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss of the generality ability. Extensive experiments on both symbol- and NL-centric tasks demonstrate the balanced and superior performances of Symbol-LLM series models.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)大语言模型（LLM）已经为自然语言（NL）关注的任务带来了很大的进步，基于NL界面。然而，NL形式不够用于世界知识。当前的工作都在注意这个问题，通过将特定的象征知识注入到LLM中，忽略了两个关键挑战：符号之间的关系和象征中心和NL中心能力的平衡。在这项工作中，我们从数据和框架角度来解决这些挑战，并引入 Symbol-LLM 系列模型。首先，我们收集了34个符号任务，覆盖了~20种不同的形式，这些任务被统一以捕捉符号之间的关系。然后，我们提出了一个两个阶段的调整框架，成功地注入符号知识而不失去通用能力。广泛的实验表明 Symbol-LLM 系列模型在符号和NL关注任务中具有平衡和超越的表现。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages"><a href="#Assessing-Translation-capabilities-of-Large-Language-Models-involving-English-and-Indian-Languages" class="headerlink" title="Assessing Translation capabilities of Large Language Models involving English and Indian Languages"></a>Assessing Translation capabilities of Large Language Models involving English and Indian Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09216">http://arxiv.org/abs/2311.09216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vandan Mujadia, Ashok Urlana, Yash Bhaskar, Penumalla Aditya Pavani, Kukkapalli Shravya, Parameswari Krishnamurthy, Dipti Misra Sharma</li>
<li>for: 本研究旨在探讨大型自然语言处理器（LLM）在多种自然语言译语 зада务中的多语言能力。</li>
<li>methods: 我们使用机器翻译作为英语和22种印度语言之间的译语任务，首先研究 raw LLM 的翻译能力，然后探讨 raw LLM 在上下文学习中的表现。我们使用 LoRA 等参数有效的微调方法进行微调。</li>
<li>results: 我们的研究表明，使用 LLaMA 作为基础模型，可以在英语和印度语言之间的翻译任务中取得显著进步，其中 average BLEU 分数为 13.42、15.93、12.13、12.30 和 12.07，chrF 分数为 43.98、46.99、42.55、42.42 和 45.39。在印度语言到英语的翻译任务中，我们取得了 average BLEU 分数为 14.03、16.65、16.17、15.35 和 12.55，chrF 分数为 36.71、40.44、40.26、39.51 和 36.20。总之，我们的发现表明大型自然语言处理器在机器翻译任务中具有潜在的潜力，包括目前未经投入的语言。<details>
<summary>Abstract</summary>
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. In this work, our aim is to explore the multilingual capabilities of large language models by using machine translation as a task involving English and 22 Indian languages. We first investigate the translation capabilities of raw large language models, followed by exploring the in-context learning capabilities of the same raw models. We fine-tune these large language models using parameter efficient fine-tuning methods such as LoRA and additionally with full fine-tuning. Through our study, we have identified the best performing large language model for the translation task involving LLMs, which is based on LLaMA.   Our results demonstrate significant progress, with average BLEU scores of 13.42, 15.93, 12.13, 12.30, and 12.07, as well as CHRF scores of 43.98, 46.99, 42.55, 42.42, and 45.39, respectively, using 2-stage fine-tuned LLaMA-13b for English to Indian languages on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Similarly, for Indian languages to English, we achieved average BLEU scores of 14.03, 16.65, 16.17, 15.35 and 12.55 along with chrF scores of 36.71, 40.44, 40.26, 39.51, and 36.20, respectively, using fine-tuned LLaMA-13b on IN22 (conversational), IN22 (general), flores200-dev, flores200-devtest, and newstest2019 testsets. Overall, our findings highlight the potential and strength of large language models for machine translation capabilities, including for languages that are currently underrepresented in LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）已经在不同的自然语言处理任务中实现了很大的进步。在这项工作中，我们的目标是探索大型语言模型在多种语言之间的多语言能力。我们首先调查了Raw大型语言模型的翻译能力，然后探索这些Raw模型在 Context Learning 中的能力。我们使用 parameter efficient fine-tuning 方法如 LoRA 和全局 fine-tuning 进行参数的调整。通过我们的研究，我们已经确定了最佳的大型语言模型为翻译任务，即基于 LLaMA 的模型。我们的结果表明了显著的进步，平均 BLEU 分数为 13.42、15.93、12.13、12.30 和 12.07，以及 CHRF 分数为 43.98、46.99、42.55、42.42 和 45.39，分别在英语到印度语言的 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上。同样，在印度语言到英语的翻译任务中，我们获得了平均 BLEU 分数为 14.03、16.65、16.17、15.35 和 12.55，以及 CHRF 分数为 36.71、40.44、40.26、39.51 和 36.20，分别在 IN22（交流）、IN22（通用）、flores200-dev、flores200-devtest 和 newstest2019 测试集上。总的来说，我们的发现表明了大型语言模型在翻译任务中的潜力和优势，包括目前尚未得到充分利用的语言。
</details></li>
</ul>
<hr>
<h2 id="Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey"><a href="#Controllable-Text-Summarization-Unraveling-Challenges-Approaches-and-Prospects-–-A-Survey" class="headerlink" title="Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey"></a>Controllable Text Summarization: Unraveling Challenges, Approaches, and Prospects – A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09212">http://arxiv.org/abs/2311.09212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ashokurlana/controllable_text_summarization_survey">https://github.com/ashokurlana/controllable_text_summarization_survey</a></li>
<li>paper_authors: Ashok Urlana, Pruthwik Mishra, Tathagato Roy, Rahul Mishra</li>
<li>for: 这个论文主要写于控制可能性的文章摘要方法。</li>
<li>methods: 本论文使用了多种控制可能性的方法，包括文章摘要任务的定义、分类和评价。</li>
<li>results: 本论文发现了控制可能性的文章摘要方法的多种类别和挑战，以及未来研究的可能性。<details>
<summary>Abstract</summary>
Generic text summarization approaches often fail to address the specific intent and needs of individual users. Recently, scholarly attention has turned to the development of summarization methods that are more closely tailored and controlled to align with specific objectives and user needs. While a growing corpus of research is devoted towards a more controllable summarization, there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.
</details>
<details>
<summary>摘要</summary>
常见的文本摘要方法 often 无法 addresses 用户的特定目标和需求。近年来，学术界对于更加控制性的摘要方法的开发受到了关注。 although 一个增长的文献库 devoted  towards 更加控制性的摘要， there is no comprehensive survey available that thoroughly explores the diverse controllable aspects or attributes employed in this context, delves into the associated challenges, and investigates the existing solutions. In this survey, we formalize the Controllable Text Summarization (CTS) task, categorize controllable aspects according to their shared characteristics and objectives, and present a thorough examination of existing methods and datasets within each category. Moreover, based on our findings, we uncover limitations and research gaps, while also delving into potential solutions and future directions for CTS.Note: The translation is in Simplified Chinese, which is the standard written form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models"><a href="#Chain-of-Note-Enhancing-Robustness-in-Retrieval-Augmented-Language-Models" class="headerlink" title="Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models"></a>Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09210">http://arxiv.org/abs/2311.09210</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin Ma, Hongwei Wang, Dong Yu</li>
<li>for: 提高 Retrieval-augmented language models（RALMs）的可靠性和能力，特别是在减少假想的报道和增加外部知识源的情况下。</li>
<li>methods: 提出了一种新的Chain-of-Noting（CoN）方法，通过生成文档检索过程中的顺序阅读笔记，评估检索到的文档的相关性，并将其纳入最终的回答中。</li>
<li>results: CoN在四个开放领域问答 benchmarck 上进行了实验，结果显示，与标准 RALMs 相比，CoN 可以提高 EM 分数的平均提升为+7.9，并在实时问题中减少不相关文档的拒绝率为+10.5。<details>
<summary>Abstract</summary>
Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with "unknown" when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.
</details>
<details>
<summary>摘要</summary>
大型语言模型（RALM）在增强它们的能力方面取得了重要进步，主要是减少了假想的投入。然而，获取的信息不一定可靠。不必要的数据获取可能导致错误的响应，甚至让模型忽略其内置的知识，即使它拥有足够的信息来回答问题。此外，标准的RALM通常难以判断自己是否具备了足够的知识来提供正确的答案。在知识缺乏的情况下，这些系统应该回答为“未知”。为解决这些挑战，我们提出了链条注释（CoN），一种新的方法，可以提高RALM在噪音、无关文档中的稳定性，以及处理未知情况的能力。CoN的核心思想是生成批处理的阅读笔记，以评估 retrieved 文档的相关性，并将其集成到提供答案。我们使用了ChatGPT创建训练数据，然后将其训练在LLaMa-2 7B 模型上。我们在四个开放领域问答 benchmark 上进行了实验，结果显示，RALMs 配置了 CoN 显著超越标准 RALMs。特别是，CoN 在 entirely 噪音获取的情况下的 EM 分数平均提高了+7.9，并在实时问题 external 知识范围外的拒绝率上提高了+10.5。
</details></li>
</ul>
<hr>
<h2 id="Fusion-Eval-Integrating-Evaluators-with-LLMs"><a href="#Fusion-Eval-Integrating-Evaluators-with-LLMs" class="headerlink" title="Fusion-Eval: Integrating Evaluators with LLMs"></a>Fusion-Eval: Integrating Evaluators with LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09204">http://arxiv.org/abs/2311.09204</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, Lei Meng<br>for: 这篇论文的目的是评估大型自然语言处理模型（LLMs）的评估方法，以便更好地理解自然语言理解和高级逻辑预期。methods: 这篇论文使用了多种评估方法，包括人类基于、模型基于和自动度量标准方法，并通过将这些方法综合使用来创建一个更加灵活和有效的评估系统。results: 在使用SummEval数据集进行测试时，Fusion-Eval实现了Spearman相关性0.96，超过其他评估器。这表明了Fusion-Eval可以充分利用多个参考来生成高度相似于人类视角的评估结果，为LLM评估做出了新的标准。<details>
<summary>Abstract</summary>
Evaluating Large Language Models (LLMs) is a complex task, especially considering the intricacies of natural language understanding and the expectations for high-level reasoning. Traditional evaluations typically lean on human-based, model-based, or automatic-metrics-based paradigms, each with its own advantages and shortcomings. We introduce "Fusion-Eval", a system that employs LLMs not solely for direct evaluations, but to skillfully integrate insights from diverse evaluators. This gives Fusion-Eval flexibility, enabling it to work effectively across diverse tasks and make optimal use of multiple references. In testing on the SummEval dataset, Fusion-Eval achieved a Spearman correlation of 0.96, outperforming other evaluators. The success of Fusion-Eval underscores the potential of LLMs to produce evaluations that closely align human perspectives, setting a new standard in the field of LLM evaluation.
</details>
<details>
<summary>摘要</summary>
评估大语言模型（LLM）是一项复杂的任务，尤其是在自然语言理解方面和高级逻辑预期下。传统评估方法通常是人类基础、模型基础或自动指标基础的三者，每种方法都有其优点和缺点。我们介绍了“融合评估”（Fusion-Eval）系统，它不仅利用 LLM 进行直接评估，而且灵活地结合了多个评估者的意见。这使得 Fusion-Eval 能够在多种任务上工作有效，并且能够最大化多个参考。在 SummEval 数据集上测试时，Fusion-Eval 达到了 Spearman 相关系数 0.96，超越其他评估器。Fusion-Eval 的成功表明 LLM 可以生成高度吻合人类视角的评估结果，为 LLM 评估领域设置了新的标准。
</details></li>
</ul>
<hr>
<h2 id="ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD"><a href="#ExpM-NF-Differentially-Private-Machine-Learning-that-Surpasses-DPSGD" class="headerlink" title="ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD"></a>ExpM+NF: Differentially Private Machine Learning that Surpasses DPSGD</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09200">http://arxiv.org/abs/2311.09200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Robert A. Bridges, Vandy J. Tombs, Christopher B. Stanley</li>
<li>for: 本研究旨在提出一种基于Exponential Mechanism（ExpM）和auxiliary Normalizing Flow（NF）的方法，用于在private数据上训练机器学习（ML）模型，并 garantuee differential privacy（DP）保证。</li>
<li>methods: 本方法使用ExpM和NF结合使用，以实现在private数据上训练ML模型，并可以实现预先指定的DP保证。</li>
<li>results: 对于多个分类任务和不同的数据集，ExpM+NF可以 achieve greater than 93%的非私有训练精度（AUC），并且在DP保证下提供更高的精度和更低的DP保证（$\varepsilon$）。I hope that helps! Let me know if you have any further questions or if you’d like me to help with anything else.<details>
<summary>Abstract</summary>
In this pioneering work we formulate ExpM+NF, a method for training machine learning (ML) on private data with pre-specified differentially privacy guarantee $\varepsilon>0, \delta=0$, by using the Exponential Mechanism (ExpM) and an auxiliary Normalizing Flow (NF). We articulate theoretical benefits of ExpM+NF over Differentially Private Stochastic Gradient Descent (DPSGD), the state-of-the-art (SOTA) and de facto method for differentially private ML, and we empirically test ExpM+NF against DPSGD using the SOTA implementation (Opacus with PRV accounting) in multiple classification tasks on the Adult Dataset (census data) and MIMIC-III Dataset (electronic healthcare records) using Logistic Regression and GRU-D, a deep learning recurrent neural network with ~20K-100K parameters. In all experiments, ExpM+NF achieves greater than 93% of the non-private training accuracy (AUC) for $\varepsilon \in [1\mathrm{e}{-3}, 1]$, exhibiting greater accuracy (higher AUC) and privacy (lower $\varepsilon$ with $\delta=0$) than DPSGD. Differentially private ML generally considers $\varepsilon \in [1,10]$ to maintain reasonable accuracy; hence, ExpM+NF's ability to provide strong accuracy for orders of magnitude better privacy (smaller $\varepsilon$) substantially pushes what is currently possible in differentially private ML. Training time results are presented showing ExpM+NF is comparable to (slightly faster) than DPSGD. Code for these experiments will be provided after review. Limitations and future directions are provided.
</details>
<details>
<summary>摘要</summary>
在这项先锋工作中，我们提出了ExpM+NF方法，用于在private数据上训练机器学习（ML），并 garantía differentially privacy 保证 $\varepsilon>0, \delta=0$。我们解释了ExpM+NF方法与State-of-the-art（SOTA）和de facto方法 differentially private Stochastic Gradient Descent（DPSGD）之间的理论优势，并对ExpM+NF方法和DPSGD进行了多个分类任务中的empirical测试，使用了Adult Dataset（人口普查数据）和MIMIC-III Dataset（电子医疗记录）上的Logistic Regression和GRU-D，一个深度学习循环神经网络，parameters数量在20K-100K之间。在所有实验中，ExpM+NF方法可以在 $\varepsilon \in [1\mathrm{e}{-3}, 1]$ 范围内达到非私有训练精度（AUC）的大于93%，表现出更高的准确率（AUC）和隐私（lower $\varepsilon$ with $\delta=0$），比DPSGD更好。 differentially private ML通常Consider $\varepsilon \in [1,10]$ 以保持合理的准确率;因此，ExpM+NF方法的能力提供许多orders of magnitude better privacy（smaller $\varepsilon）substantially pushes what is currently possible in differentially private ML。我们还提供了训练时间结果，表明ExpM+NF方法与DPSGD相对（slightly faster）。我们将在审核后提供代码。 limitations和未来方向也被提供。
</details></li>
</ul>
<hr>
<h2 id="Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering"><a href="#Never-Lost-in-the-Middle-Improving-Large-Language-Models-via-Attention-Strengthening-Question-Answering" class="headerlink" title="Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering"></a>Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09198">http://arxiv.org/abs/2311.09198</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Yuxin Liang, Hao Wang, Qianguo Sun, Songxin Zhang, Zejian Xie, Jiaxing Zhang</li>
<li>for: 提高大语言模型在长文本上的信息搜寻和反思能力</li>
<li>methods: 提出特制的任务 called Attention Strengthening Multi-doc QA (ASM QA)，以提高模型在长文本上的精准搜寻能力</li>
<li>results: 实验结果显示，模型在多文档问答和其他标准任务上表现出色，与当前最佳模型相比，在随机设置下获得13.7%的绝对提升，在文章检索任务上获得21.5%的提升。<details>
<summary>Abstract</summary>
While large language models (LLMs) are equipped with longer text input capabilities than before, they are struggling to seek correct information in long contexts. The "lost in the middle" problem challenges most LLMs, referring to the dramatic decline in accuracy when correct information is located in the middle. To overcome this crucial issue, this paper proposes to enhance the information searching and reflection ability of LLMs in long contexts via specially designed tasks called Attention Strengthening Multi-doc QA (ASM QA). Following these tasks, our model excels in focusing more precisely on the desired information. Experimental results show substantial improvement in Multi-doc QA and other benchmarks, superior to state-of-the-art models by 13.7% absolute gain in shuffled settings, by 21.5% in passage retrieval task. We release our model, Ziya-Reader to promote related research in the community.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有更长的文本输入能力，但它们在长文本上寻找正确信息时受到挑战。这个“lost in the middle”问题对大多数LLM都是一个重要问题，指的是正确信息在中间部分的减退率。为了解决这个关键问题，这篇论文提出了通过特定任务 called Attention Strengthening Multi-doc QA（ASM QA）来增强LLM在长文本上的信息寻找和反射能力。在这些任务中，我们的模型在更加精准地Focus on Desired Information。实验结果表明，我们的模型在多文档问答和其他标准 bencmarks 上表现出了明显的提升，相比领先模型的13.7%绝对提升，在排序任务上提高21.5%。我们将发布我们的模型，Ziya-Reader，以便在社区中促进相关的研究。
</details></li>
</ul>
<hr>
<h2 id="The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task"><a href="#The-Role-of-Chain-of-Thought-in-Complex-Vision-Language-Reasoning-Task" class="headerlink" title="The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task"></a>The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09193">http://arxiv.org/abs/2311.09193</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Wu, Pengchuan Zhang, Wenhan Xiong, Barlas Oguz, James C. Gee, Yixin Nie</li>
<li>for: 这个研究探讨了链条思维方法在复杂视语任务中的有效性，这种方法通过将任务拆分成子任务和中间步骤来提高语言任务的效率。</li>
<li>methods: 这篇研究使用了”描述然后做出决策”策略，这种策略draws inspiration from human signal processing mechanisms，并在探测任务中提高了性能，提高了50%。</li>
<li>results: 这篇研究发现，使用”描述然后做出决策”策略可以在复杂视语任务中提高探测任务的性能，提高50%。<details>
<summary>Abstract</summary>
The study explores the effectiveness of the Chain-of-Thought approach, known for its proficiency in language tasks by breaking them down into sub-tasks and intermediate steps, in improving vision-language tasks that demand sophisticated perception and reasoning. We present the "Description then Decision" strategy, which is inspired by how humans process signals. This strategy significantly improves probing task performance by 50%, establishing the groundwork for future research on reasoning paradigms in complex vision-language tasks.
</details>
<details>
<summary>摘要</summary>
这个研究探讨了链条思维方法的效iveness，这种方法以分解语言任务为互助步骤而著称，在复杂的视觉语言任务中提高了高级观察和理解能力。我们提出了“描述然后决策”策略，这种策略 Draws inspiration from human signal processing and significantly improves probing task performance by 50%. This lays the foundation for future research on reasoning paradigms in complex vision-language tasks.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Towards-Verifiable-Text-Generation-with-Symbolic-References"><a href="#Towards-Verifiable-Text-Generation-with-Symbolic-References" class="headerlink" title="Towards Verifiable Text Generation with Symbolic References"></a>Towards Verifiable Text Generation with Symbolic References</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09188">http://arxiv.org/abs/2311.09188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lucas Torroba Hennigen, Shannon Shen, Aniruddha Nrusimha, Bernhard Gapp, David Sontag, Yoon Kim</li>
<li>for: 这篇论文目的是提出一种简单的方法来使大语言模型（LLM）的输出更易于人工验证，以便用于高风险应用。</li>
<li>methods: 该论文提出了一种名为符号附加生成（SymGen）的方法，它使得 LLM 可以在输出文本中嵌入显式的符号参考，以便显示不同的文本段的来源。</li>
<li>results: 实验表明， LLM 可以通过 SymGen 方法直接输出包含符号参考的文本，而不会影响文本的流畅性和准确性。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated an impressive ability to synthesize plausible and fluent text. However they remain vulnerable to hallucinations, and thus their outputs generally require manual human verification for high-stakes applications, which can be time-consuming and difficult. This paper proposes symbolically grounded generation (SymGen) as a simple approach for enabling easier validation of an LLM's output. SymGen prompts an LLM to interleave its regular output text with explicit symbolic references to fields present in some conditioning data (e.g., a table in JSON format). The references can be used to display the provenance of different spans of text in the generation, reducing the effort required for manual verification. Across data-to-text and question answering experiments, we find that LLMs are able to directly output text that makes use of symbolic references while maintaining fluency and accuracy.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers"><a href="#Generate-Filter-and-Fuse-Query-Expansion-via-Multi-Step-Keyword-Generation-for-Zero-Shot-Neural-Rankers" class="headerlink" title="Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers"></a>Generate, Filter, and Fuse: Query Expansion via Multi-Step Keyword Generation for Zero-Shot Neural Rankers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09175">http://arxiv.org/abs/2311.09175</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, Michael Bendersky</li>
<li>for: 提高零shot neural ranker的启发搜索精度</li>
<li>methods: 提出了一个名为GFF的管道，包括一个大型自然语言模型和一个神经网络排序器，用于生成、筛选和融合查询扩展。</li>
<li>results: GFF可以提高零shot nDCG@10在BEIR和TREC DL 2019&#x2F;2020上。<details>
<summary>Abstract</summary>
Query expansion has been proved to be effective in improving recall and precision of first-stage retrievers, and yet its influence on a complicated, state-of-the-art cross-encoder ranker remains under-explored. We first show that directly applying the expansion techniques in the current literature to state-of-the-art neural rankers can result in deteriorated zero-shot performance. To this end, we propose GFF, a pipeline that includes a large language model and a neural ranker, to Generate, Filter, and Fuse query expansions more effectively in order to improve the zero-shot ranking metrics such as nDCG@10. Specifically, GFF first calls an instruction-following language model to generate query-related keywords through a reasoning chain. Leveraging self-consistency and reciprocal rank weighting, GFF further filters and combines the ranking results of each expanded query dynamically. By utilizing this pipeline, we show that GFF can improve the zero-shot nDCG@10 on BEIR and TREC DL 2019/2020. We also analyze different modelling choices in the GFF pipeline and shed light on the future directions in query expansion for zero-shot neural rankers.
</details>
<details>
<summary>摘要</summary>
Query expansion 已经证明可以提高首个检索器的准确率和匹配率，但是它对现代跨Encoder排名器的影响还未得到充分探讨。我们首先表明，直接在当前文献中使用扩展技术可能会导致现有神经排名器的零件性能下降。为此，我们提出了GFF，一个管道，包括一个大型自然语言模型和一个神经排名器，用于生成、筛选和融合查询扩展更有效地，以提高零件性能指标 such as nDCG@10。具体来说，GFF首先通过一个遵循语言模型来生成基于查询的关键词，然后通过自我一致和对偶排名Weight来筛选和组合每个扩展查询的排名结果。通过这个管道，我们表明GFF可以提高零件 nDCG@10 在 BEIR 和 TREC DL 2019/2020。我们还分析了 GFF 管道中不同的模型选择和未来方向。
</details></li>
</ul>
<hr>
<h2 id="AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph"><a href="#AbsPyramid-Benchmarking-the-Abstraction-Ability-of-Language-Models-with-a-Unified-Entailment-Graph" class="headerlink" title="AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph"></a>AbsPyramid: Benchmarking the Abstraction Ability of Language Models with a Unified Entailment Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09174">http://arxiv.org/abs/2311.09174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkust-knowcomp/abspyramid">https://github.com/hkust-knowcomp/abspyramid</a></li>
<li>paper_authors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang, Sehyun Choi, Xin Liu, Yangqiu Song</li>
<li>for: 本研究旨在探讨语言模型内置抽象能力的现状，并提出一个大规模的抽象知识图。</li>
<li>methods: 本研究使用了一个大规模的文本描述数据集，通过构建抽象知识图来评估语言模型在开放领域中的抽象能力。</li>
<li>results: 实验结果表明，现有的LLMs在零shot和几shot设置下具有很大的抽象知识识别挑战。通过训练在我们的充沛抽象知识上，我们发现LLMs可以获得基本的抽象能力，并在未见事件中进行抽象。同时，我们也证明了我们的指标是可以强化LLMs在两个前一个抽象任务上。<details>
<summary>Abstract</summary>
Cognitive research indicates that abstraction ability is essential in human intelligence, which remains under-explored in language models. In this paper, we present AbsPyramid, a unified entailment graph of 221K textual descriptions of abstraction knowledge. While existing resources only touch nouns or verbs within simplified events or specific domains, AbsPyramid collects abstract knowledge for three components of diverse events to comprehensively evaluate the abstraction ability of language models in the open domain. Experimental results demonstrate that current LLMs face challenges comprehending abstraction knowledge in zero-shot and few-shot settings. By training on our rich abstraction knowledge, we find LLMs can acquire basic abstraction abilities and generalize to unseen events. In the meantime, we empirically show that our benchmark is comprehensive to enhance LLMs across two previous abstraction tasks.
</details>
<details>
<summary>摘要</summary>
研究表明人类智能中的抽象能力是非常重要的，但是这一点尚未得到充分的探索。在这篇论文中，我们介绍了一个名为AbsPyramid的抽象知识维度图，包含221K个文本描述。与现有资源不同，AbsPyramid不仅覆盖了简化事件中的名词或动词，而是收集了多元事件中的抽象知识，以全面评估语言模型在开放领域中的抽象能力。实验结果表明，现有的LLMs在零shot和几shot设定下面临着抽象知识的挑战。通过在我们的充足抽象知识上训练，我们发现LLMs可以学习基本的抽象能力，并在未经见过的事件上进行推断。同时，我们实验表明，我们的标准可以提高LLMs在两个之前的抽象任务中表现。
</details></li>
</ul>
<hr>
<h2 id="Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction"><a href="#Temporal-Knowledge-Question-Answering-via-Abstract-Reasoning-Induction" class="headerlink" title="Temporal Knowledge Question Answering via Abstract Reasoning Induction"></a>Temporal Knowledge Question Answering via Abstract Reasoning Induction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09149">http://arxiv.org/abs/2311.09149</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyang Chen, Dongfang Li, Xiang Zhao, Baotian Hu, Min Zhang</li>
<li>for: 本研究旨在解决大语言模型（LLM）中的时间知识推理问题，这是LLM遇到的一个重要挑战，这些问题可能会导致LLM生成错误或误导信息，主要是因为它们的时间知识处理能力有限，同时复杂的时间逻辑也会带来问题。</li>
<li>methods: 我们提出了一种新的构建主义方法，它强调在LLM学习中实行持续的知识合成和个性化。我们的方法包括Abstract Reasoning Induction ARI框架，这个框架将时间推理分成两个不同阶段：知识无关和知识基础。这种分类目标在减少幻觉和提高LLM对抽象方法的应用。</li>
<li>results: 我们的方法在两个时间问答Dataset上获得了显著改进，相比于基eline，我们的方法提高了29.7%和9.27%。这demonstrates our approach’s efficacy in enhancing temporal reasoning in LLMs. The code will be released at <a target="_blank" rel="noopener" href="https://github.com/czy1999/ARI">https://github.com/czy1999/ARI</a>.<details>
<summary>Abstract</summary>
In this paper, we tackle the significant challenge of temporal knowledge reasoning in Large Language Models (LLMs), an area where such models frequently encounter difficulties. These difficulties often result in the generation of misleading or incorrect information, primarily due to their limited capacity to process evolving factual knowledge and complex temporal logic. In response, we propose a novel, constructivism-based approach that advocates for a paradigm shift in LLM learning towards an active, ongoing process of knowledge synthesis and customization. At the heart of our proposal is the Abstract Reasoning Induction ARI framework, which divides temporal reasoning into two distinct phases: Knowledge-agnostic and Knowledge-based. This division aims to reduce instances of hallucinations and improve LLMs' capacity for integrating abstract methodologies derived from historical data. Our approach achieves remarkable improvements, with relative gains of 29.7\% and 9.27\% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code will be released at https://github.com/czy1999/ARI.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们面临着大语言模型（LLM）中的时间知识推理挑战，这是 LLM 很频繁遇到的问题。这些问题经常导致 LLM 生成错误或误导信息，主要是因为它们对逐渐发展的事实知识和复杂的时间逻辑处理能力有限。为此，我们提出了一种新的建构主义方法，强调 LLM 学习 Should be an active, ongoing process of knowledge synthesis and customization。我们的提议的核心是抽象逻辑推理引入框架（ARI），将时间推理分为两个不同阶段：无知阶段和知识阶段。这种分类的目的是减少 LLM 生成幻见的情况，提高它们对历史数据 derivated 抽象方法的集成能力。我们的方法在两个时间问答 dataset 上显示了很大的改进，相对于基eline的提升率分别为 29.7% 和 9.27%，这证明了我们的方法在提高 LLM 中的时间推理能力具有效果。代码将在 GitHub 上发布，请参考 https://github.com/czy1999/ARI。
</details></li>
</ul>
<hr>
<h2 id="Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts"><a href="#Jailbreaking-GPT-4V-via-Self-Adversarial-Attacks-with-System-Prompts" class="headerlink" title="Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts"></a>Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09127">http://arxiv.org/abs/2311.09127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun</li>
<li>for: 本研究的目的是探讨 Multimodal Large Language Models (MLLMs) 的安全问题，具体来说是通过对 GPT-4V 的系统提示泄露漏洞进行攻击，以及如何通过自我反对攻击（Self-Adversarial Attack via System Prompt，简称 SASP）方法来实现 MLLM 的破狱。</li>
<li>methods: 本研究使用了一种新的破狱攻击方法，即 SASP，该方法利用了 GPT-4 作为红人工具，通过对自己的系统提示进行攻击，以搜索可能的破狱提示。此外，为了提高攻击成功率，还添加了人工修改基于 GPT-4 的分析。</li>
<li>results: 本研究发现，修改系统提示可以有效降低破狱成功率。 In addition, the study found that modifying system prompts can effectively reduce jailbreak success rates.<details>
<summary>Abstract</summary>
Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities in model APIs. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully steal the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking, which could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.
</details>
<details>
<summary>摘要</summary>
现有研究对囚犯多Modal大型语言模型（MLLM）主要集中在输入例针对攻击，少量关注模型API的漏洞。为填补这 gap，我们实施以下工作：1. 我们发现了GPT-4V中的系统提示泄露漏洞。通过特殊的对话设计，我们成功夺取了GPT-4V的内部系统提示。这一发现表明MLLM可能存在潜在的可以利用的安全风险;2. 基于夺取的系统提示，我们提出了一种新的MLLM囚犯攻击方法，称为SASP（自我反对性攻击via系统提示）。通过使用GPT-4作为红色团队工具，我们尝试通过夺取的系统提示找到可能的囚犯提示。此外，为了提高攻击成功率，我们还添加了人工修改基于GPT-4的分析，这进一步提高了攻击成功率到98.7%；3. 我们评估了修改系统提示以防止囚犯攻击的效果。结果表明，适当设计的系统提示可以减少囚犯成功率。总的来说，我们的工作提供了新的思路来增强MLLM安全性，表明系统提示在囚犯中具有重要的作用，可以大大提高囚犯成功率，同时也有可能用于防止囚犯。
</details></li>
</ul>
<hr>
<h2 id="HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data"><a href="#HEALNet-–-Hybrid-Multi-Modal-Fusion-for-Heterogeneous-Biomedical-Data" class="headerlink" title="HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data"></a>HEALNet – Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09115">http://arxiv.org/abs/2311.09115</a></li>
<li>repo_url: None</li>
<li>paper_authors: Konstantin Hemker, Nikola Smidjievski, Mateja Jamnik</li>
<li>for: This paper is written for researchers and practitioners in the field of multi-modal biomedical modelling, specifically those working with image, tabular, and graph data in medical applications.</li>
<li>methods: The Hybrid Early-fusion Attention Learning Network (HEALNet) architecture is used in this paper, which combines modality-specific architectures with cross-modal attention mechanisms to capture crucial cross-modal information and preserve modality-specific structural information.</li>
<li>results: The HEALNet architecture achieves state-of-the-art performance in multi-modal survival analysis on Whole Slide Images and Multi-omic data from four cancer cohorts in The Cancer Genome Atlas (TCGA), substantially improving over both uni-modal and recent multi-modal baselines, while being robust in scenarios with missing modalities.<details>
<summary>Abstract</summary>
Technological advances in medical data collection such as high-resolution histopathology and high-throughput genomic sequencing have contributed to the rising requirement for multi-modal biomedical modelling, specifically for image, tabular, and graph data. Most multi-modal deep learning approaches use modality-specific architectures that are trained separately and cannot capture the crucial cross-modal information that motivates the integration of different data sources. This paper presents the Hybrid Early-fusion Attention Learning Network (HEALNet): a flexible multi-modal fusion architecture, which a) preserves modality-specific structural information, b) captures the cross-modal interactions and structural information in a shared latent space, c) can effectively handle missing modalities during training and inference, and d) enables intuitive model inspection by learning on the raw data input instead of opaque embeddings. We conduct multi-modal survival analysis on Whole Slide Images and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas (TCGA). HEALNet achieves state-of-the-art performance, substantially improving over both uni-modal and recent multi-modal baselines, whilst being robust in scenarios with missing modalities.
</details>
<details>
<summary>摘要</summary>
技术进步在医疗数据收集中，如高分辨率 histopathology 和高通过put genomic sequencing，对多Modal生物医学模型的需求提高了。大多数多Modal深入学习方法使用专门的模式特性 architecture，这些模型在独立地训练，无法捕捉 crossing Modal 信息，这些信息是集成不同数据源的关键。这篇论文提出了 Hybrid Early-fusion Attention Learning Network (HEALNet)：一种灵活的多Modal融合建模 Architecture，具有以下特点：a) 保持 Modal 特有的结构信息b) 捕捉 crossing Modal 交互和结构信息在共享封装空间中c) 可以效果地处理训练和推断中缺失的 Modald) 允许直观地模型检查，通过学习原始数据输入而不是抽象封装我们在TCGA 四个肿瘤 cohort 上进行多Modal 存活分析，HEALNet 实现了状态之 arts 性能，大幅提高过uni-Modal 和 latest multi-Modal 基elines，同时在缺失 Modal 的情况下具有强健性。
</details></li>
</ul>
<hr>
<h2 id="Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification"><a href="#Ever-Mitigating-Hallucination-in-Large-Language-Models-through-Real-Time-Verification-and-Rectification" class="headerlink" title="Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification"></a>Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09114">http://arxiv.org/abs/2311.09114</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoqiang Kang, Juntong Ni, Huaxiu Yao</li>
<li>for: 这个论文的目的是解决大语言模型（LLM）在生成文本时遇到的不准确或幻想内容问题。</li>
<li>methods: 该论文提出了一种实时验证和修正（Ever）方法，通过实时步骤的生成和幻想修正策略来检测和修正幻想错误。</li>
<li>results: 与基eline相比，Ever在多种任务上（包括短answer问题、生成传记和多步论证）表现出了显著的改善，能够生成可靠和事实正确的文本。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型 (LLMs) 已经示出了惊人的流畅性，但它们经常遇到生成不准确或幻想内容的挑战。这个问题是非 retrieve-based 生成和 retrieve-augmented 生成方法中的共同问题，而现有的后续修正方法可能不能Address the accumulated hallucination errors that may be caused by the "snowballing" issue, especially in reasoning tasks。为解决这些挑战，我们介绍了一种新的方法called Real-time Verification and Rectification (Ever).而不是等待生成过程结束后进行修正幻想，Ever 使用了实时步骤生成和幻想修正策略。主要目标是在生成过程中实时检测和修正幻想。与 retrieve-based 和 non-retrieve-based 基线相比，Ever 在多种任务上，包括短问答、生传生成和多步逻辑 reasoning 等，示出了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion"><a href="#Does-Pre-trained-Language-Model-Actually-Infer-Unseen-Links-in-Knowledge-Graph-Completion" class="headerlink" title="Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?"></a>Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge Graph Completion?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09109">http://arxiv.org/abs/2311.09109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yusuke Sakai, Hidetaka Kamigaito, Katsuhiko Hayashi, Taro Watanabe</li>
<li>for: 本研究旨在探讨PLM-based KGC方法是否能够真正进行推理，或者只是通过Memorization来获得高性能。</li>
<li>methods: 我们提出了一种Synthetic dataset construction方法，用于分析PLM-based KGC方法是否能够进行推理。</li>
<li>results: 我们发现，PLMs通过预训练获得了推理能力，尽管表现改进主要来自于实体和关系文本信息。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs) consist of links that describe relationships between entities. Due to the difficulty of manually enumerating all relationships between entities, automatically completing them is essential for KGs. Knowledge Graph Completion (KGC) is a task that infers unseen relationships between entities in a KG. Traditional embedding-based KGC methods, such as RESCAL, TransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using only the knowledge from training data. In contrast, the recent Pre-trained Language Model (PLM)-based KGC utilizes knowledge obtained during pre-training. Therefore, PLM-based KGC can estimate missing links between entities by reusing memorized knowledge from pre-training without inference. This approach is problematic because building KGC models aims to infer unseen links between entities. However, conventional evaluations in KGC do not consider inference and memorization abilities separately. Thus, a PLM-based KGC method, which achieves high performance in current KGC evaluations, may be ineffective in practical applications. To address this issue, we analyze whether PLM-based KGC methods make inferences or merely access memorized knowledge. For this purpose, we propose a method for constructing synthetic datasets specified in this analysis and conclude that PLMs acquire the inference abilities required for KGC through pre-training, even though the performance improvements mostly come from textual information of entities and relations.
</details>
<details>
<summary>摘要</summary>
知识图（KG）由关系链描述实体之间的关系。由于手动列出所有实体间关系的困难，自动完成这些关系是知识图的关键。知识图完成任务（KGC）是尝试推断实体间未知的关系。传统的嵌入式KGC方法，如RESCAL、TransE、DistMult、ComplEx、RotatE、HAKE、HousE等，通过训练数据来INFER未知的关系。与此相反，最近的预训练语言模型（PLM）基于KGC利用预训练中获得的知识。因此，PLM基于KGC可以估计实体间未知的关系，而不需要INFER。这种方法存在问题，因为建立KGC模型的目标是INFER实体间未知的关系。然而，现有的KGC评价方法不会分开考虑推断和嵌入能力。因此，一个PLM基于KGC方法，即在当前KGC评价中具有高性能，可能在实际应用中效果不佳。为解决这个问题，我们分析PLM基于KGC方法是否进行推断或只是访问嵌入知识。为此，我们提出一种方法构建定制化的 sintetic dataset，并结论PLM在预训练中获得了推断能力，即使表现改进主要来自实体和关系的文本信息。
</details></li>
</ul>
<hr>
<h2 id="Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning"><a href="#Towards-A-Unified-View-of-Answer-Calibration-for-Multi-Step-Reasoning" class="headerlink" title="Towards A Unified View of Answer Calibration for Multi-Step Reasoning"></a>Towards A Unified View of Answer Calibration for Multi-Step Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09101">http://arxiv.org/abs/2311.09101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shumin Deng, Ningyu Zhang, Nay Oo, Bryan Hooi</li>
<li>for: 该论文旨在探讨以Chain-of-Thought（CoT）提示方法改进多步逻辑能力的大语言模型（LLMs）。</li>
<li>methods: 该论文分析了近期的答栏准确策略，并提供了一种统一的视角，以便系统地检查多个路径上的步骤级和路径级答栏准确策略。</li>
<li>results: 该论文通过对多个路径上的答栏准确策略进行系统性的评估，探讨了多步逻辑的优化。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. Usually, answer calibration strategies such as step-level or path-level calibration play a vital role in multi-step reasoning. While effective, there remains a significant gap in our understanding of the key factors that drive their success. In this paper, we break down the design of recent answer calibration strategies and present a unified view which establishes connections between them. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Our study holds the potential to illuminate key insights for optimizing multi-step reasoning with answer calibration.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Can-MusicGen-Create-Training-Data-for-MIR-Tasks"><a href="#Can-MusicGen-Create-Training-Data-for-MIR-Tasks" class="headerlink" title="Can MusicGen Create Training Data for MIR Tasks?"></a>Can MusicGen Create Training Data for MIR Tasks?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09094">http://arxiv.org/abs/2311.09094</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nadine Kroher, Helena Cuesta, Aggelos Pikrakis</li>
<li>for: 这个论文是为了研究基于AI生成音乐系统来生成用于音乐信息检索（MIR）任务的训练数据而写的。</li>
<li>methods: 该论文使用了MusicGen生成器生成了5个音乐种类的大量人工音乐样本，并使用了这些样本来训练一个类别模型。</li>
<li>results: 实验结果表明，提议的模型可以从人工音乐辑中学习到类别特征，并能够在真实音乐录音中Generalize well。<details>
<summary>Abstract</summary>
We are investigating the broader concept of using AI-based generative music systems to generate training data for Music Information Retrieval (MIR) tasks. To kick off this line of work, we ran an initial experiment in which we trained a genre classifier on a fully artificial music dataset created with MusicGen. We constructed over 50 000 genre- conditioned textual descriptions and generated a collection of music excerpts that covers five musical genres. Our preliminary results show that the proposed model can learn genre-specific characteristics from artificial music tracks that generalise well to real-world music recordings.
</details>
<details>
<summary>摘要</summary>
我们正在研究使用基于人工智能的生成音乐系统来生成听力音乐信息检索（MIR）任务的训练数据。为了开始这条工作，我们进行了一次初步实验，在我们训练了一个类别分类器的基础上，使用了MusicGen创建的完全人工音乐数据集。我们构建了50000多个频道条件的文本描述，并生成了涵盖五种音乐类型的音乐片断集。我们的初步结果表明，我们的提议的模型可以从人工音乐追踪中学习类别特征，这些特征可以通过实际音乐录音来泛化。
</details></li>
</ul>
<hr>
<h2 id="The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV"><a href="#The-Uli-Dataset-An-Exercise-in-Experience-Led-Annotation-of-oGBV" class="headerlink" title="The Uli Dataset: An Exercise in Experience Led Annotation of oGBV"></a>The Uli Dataset: An Exercise in Experience Led Annotation of oGBV</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09086">http://arxiv.org/abs/2311.09086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arnav Arora, Maha Jinadoss, Cheshta Arora, Denny George, Brindaalakshmi, Haseena Dawood Khan, Kirti Rawat, Div, Ritash, Seema Mathur, Shivani Yadav, Shehla Rashid Shora, Rie Raut, Sumit Pawar, Apurva Paithane, Sonia, Vivek, Dharini Priscilla, Khairunnisha, Grace Banu, Ambika Tandon, Rishav Thakker, Rahul Dev Korra, Aatman Vaidya, Tarunima Prabhakar</li>
<li>for: 这个论文目的是为了提供一个语言特定和上下文相关的 dataset，以便开发自动识别 hate speech 和 gendered abuse 的 AI 系统。</li>
<li>methods: 这个论文使用了 Twitter 上的 tweets，并将其分类为三个问题：对于 gender abuse 的经历，由女性或 LGBTQIA 社区成员领导的专家进行标注。</li>
<li>results: 通过这个 dataset，研究人员展示了一种参与式的方法来创建 dataset，并通过这些 dataset 驱动 AI 系统。<details>
<summary>Abstract</summary>
Online gender based violence has grown concomitantly with adoption of the internet and social media. Its effects are worse in the Global majority where many users use social media in languages other than English. The scale and volume of conversations on the internet has necessitated the need for automated detection of hate speech, and more specifically gendered abuse. There is, however, a lack of language specific and contextual data to build such automated tools. In this paper we present a dataset on gendered abuse in three languages- Hindi, Tamil and Indian English. The dataset comprises of tweets annotated along three questions pertaining to the experience of gender abuse, by experts who identify as women or a member of the LGBTQIA community in South Asia. Through this dataset we demonstrate a participatory approach to creating datasets that drive AI systems.
</details>
<details>
<summary>摘要</summary>
互联网上的性别基于暴力现象随着互联网和社交媒体的普及而增长。其影响更加严重在全球主要地区，因为许多用户在不使用英语的情况下使用社交媒体。因为互联网上的规模和量的对话，需要自动检测 hate speech 和更Specifically gendered abuse。然而， Currently, there is a lack of language-specific and contextual data to build such automated tools. In this paper, we present a dataset on gendered abuse in three languages - Hindi, Tamil, and Indian English. The dataset consists of tweets annotated with three questions related to the experience of gender abuse, annotated by experts who identify as women or members of the LGBTQIA community in South Asia. Through this dataset, we demonstrate a participatory approach to creating datasets that drive AI systems.
</details></li>
</ul>
<hr>
<h2 id="How-Multilingual-is-Multilingual-LLM"><a href="#How-Multilingual-is-Multilingual-LLM" class="headerlink" title="How Multilingual is Multilingual LLM?"></a>How Multilingual is Multilingual LLM?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09071">http://arxiv.org/abs/2311.09071</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yuan, Shuai Yuan, Zhiyong Wu, Lei Li</li>
<li>for: 这项研究旨在评估大语言模型（LLMs）在101种语言中的多语言能力，并将语言分为四个不同的 quadrant，以便更好地了解这些语言的特点和 optimize their performance.</li>
<li>methods: 研究使用了现有的 LLMs，并通过对这些模型进行调整和训练来提高其多语言能力。</li>
<li>results: 研究发现，现有的 LLMs 在101种语言中的多语言能力比预期更高，并且可以通过对每个 quadrant 的特点进行调整来进一步提高多语言性能。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), trained predominantly on extensive English data, often exhibit limitations when applied to other languages. Current research is primarily focused on enhancing the multilingual capabilities of these models by employing various tuning strategies. Despite their effectiveness in certain languages, the understanding of the multilingual abilities of LLMs remains incomplete. This study endeavors to evaluate the multilingual capacity of LLMs by conducting an exhaustive analysis across 101 languages, and classifies languages with similar characteristics into four distinct quadrants. By delving into each quadrant, we shed light on the rationale behind their categorization and offer actionable guidelines for tuning these languages. Extensive experiments reveal that existing LLMs possess multilingual capabilities that surpass our expectations, and we can significantly improve the multilingual performance of LLMs by focusing on these distinct attributes present in each quadrant.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），通常在广泛的英语数据上训练，在其他语言上表现有限。当前的研究主要集中在提高 LLM 的多语言能力，使用不同的调整策略。虽然在某些语言上有效，但我们对 LLM 的多语言能力的理解仍然不够完整。这项研究尝试对 101 种语言进行了全面的分析，并将语言分为四个不同的方块。我们对每个方块进行了详细的分析，并提供了改进 LLM 的多语言性表现的实用指南。广泛的实验表明，现有的 LLM 在多语言方面的能力超出了我们的预期，并且可以通过对每个方块的特点进行调整来进一步提高多语言性表现。
</details></li>
</ul>
<hr>
<h2 id="How-Well-Do-Large-Language-Models-Truly-Ground"><a href="#How-Well-Do-Large-Language-Models-Truly-Ground" class="headerlink" title="How Well Do Large Language Models Truly Ground?"></a>How Well Do Large Language Models Truly Ground?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09069">http://arxiv.org/abs/2311.09069</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hyunji Lee, Sejune Joo, Chaeeun Kim, Joel Jang, Doyoung Kim, Kyoung-Woon On, Minjoon Seo</li>
<li>for: 这paper aims to improve the reliability and controllability of Large Language Models (LLMs) by introducing a stricter definition of grounding and developing a new dataset and metric to assess it.</li>
<li>methods: 该paper uses a new dataset and a grounding metric to evaluate the grounding capabilities of 13 different LLMs of various sizes and training methods.</li>
<li>results: 研究发现，现有的知识增强模型通常只关注response中是否包含正确答案，而忽略了response的可靠性和可控性。新的定义和 metric 能够评估模型是否真正基于知识进行回答，并提供了更多的信息来改进模型的可靠性和可控性。<details>
<summary>Abstract</summary>
Reliance on the inherent knowledge of Large Language Models (LLMs) can cause issues such as hallucinations, lack of control, and difficulties in integrating variable knowledge. To mitigate this, LLMs can be probed to generate responses by grounding on external context, often given as input (knowledge-augmented models). Yet, previous research is often confined to a narrow view of the term "grounding", often only focusing on whether the response contains the correct answer or not, which does not ensure the reliability of the entire response. To address this limitation, we introduce a strict definition of grounding: a model is considered truly grounded when its responses (1) fully utilize necessary knowledge from the provided context, and (2) don't exceed the knowledge within the contexts. We introduce a new dataset and a grounding metric to assess this new definition and perform experiments across 13 LLMs of different sizes and training methods to provide insights into the factors that influence grounding performance. Our findings contribute to a better understanding of how to improve grounding capabilities and suggest an area of improvement toward more reliable and controllable LLM applications.
</details>
<details>
<summary>摘要</summary>
依赖大语言模型（LLM）的内在知识可能会导致问题，如幻觉、无控和变量知识的集成问题。为了解决这些问题，LLM可以通过附加外部 контекст进行探索，并生成响应（知识增强型模型）。然而，过去的研究通常受限于“安全”的定义，即判断响应是否包含正确的答案，这并不能 garantuee 整个响应的可靠性。为了解决这些限制，我们提出了一个严格的定义：一个模型被 considere 为真正地附加了知识，当其响应（1）完全利用提供的 контекст中的所有必要知识，（2）不超过 контекст中的知识。我们介绍了一个新的数据集和附加 metric，以评估这个新定义，并在13种不同的 LLM 中进行了实验，以提供关于如何提高附加能力的深入了解和建议。我们的发现可以帮助改善 LLM 的可靠性和控制性，并且建议一个可以提高 LLM 应用的方向。
</details></li>
</ul>
<hr>
<h2 id="Learning-Fair-Division-from-Bandit-Feedback"><a href="#Learning-Fair-Division-from-Bandit-Feedback" class="headerlink" title="Learning Fair Division from Bandit Feedback"></a>Learning Fair Division from Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09068">http://arxiv.org/abs/2311.09068</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hakuei Yamada, Junpei Komiyama, Kenshi Abe, Atsushi Iwasaki</li>
<li>for: 这篇论文研究了在不约束的线性鱼市中进行在线分配，在中央规划者不知道代理人的价值或利益下进行分配。</li>
<li>methods: 我们引入了一种封包算法，使用双平均来慢慢学习到来到达者的物品类型分布和代理人的价值。</li>
<li>results: 我们证明了我们的提议的算法可以在线ark Fisher市场中 asymptotically 实现 оптимальную拜纳社会利益，并提供了 regret bounds。我们还通过 sintetic 和实验数据 validate 了我们的算法的超越性。<details>
<summary>Abstract</summary>
This work addresses learning online fair division under uncertainty, where a central planner sequentially allocates items without precise knowledge of agents' values or utilities. Departing from conventional online algorithm, the planner here relies on noisy, estimated values obtained after allocating items. We introduce wrapper algorithms utilizing \textit{dual averaging}, enabling gradual learning of both the type distribution of arriving items and agents' values through bandit feedback. This approach enables the algorithms to asymptotically achieve optimal Nash social welfare in linear Fisher markets with agents having additive utilities. We establish regret bounds in Nash social welfare and empirically validate the superior performance of our proposed algorithms across synthetic and empirical datasets.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment"><a href="#In-vehicle-Sensing-and-Data-Analysis-for-Older-Drivers-with-Mild-Cognitive-Impairment" class="headerlink" title="In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment"></a>In-vehicle Sensing and Data Analysis for Older Drivers with Mild Cognitive Impairment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09273">http://arxiv.org/abs/2311.09273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sonia Moshfeghi, Muhammad Tanveer Jan, Joshua Conniff, Seyedeh Gol Ara Ghoreishi, Jinwoo Jang, Borko Furht, Kwangsoo Yang, Monica Rosselli, David Newman, Ruth Tappen, Dana Smith</li>
<li>for: 这项研究的目的是设计低成本的在日常驾驶环境中获取高精度定位和电子邮件数据的汽车仪器，并通过机器学习方法早期发现老年人智能障碍的迹象。</li>
<li>methods: 这项研究使用了低成本的在汽车内部设备来获取高精度定位和电子邮件数据，并使用机器学习方法来检测老年人智能障碍的迹象。</li>
<li>results: 研究结果表明，老年人智能障碍的 drivers 在日常驾驶中比不受智能障碍的 drivers 驾驶更稳定和安全，而且机器学习模型也 identific 了驾驶次数、教育水平和夜间驾驶次数为最重要的因素。<details>
<summary>Abstract</summary>
Driving is a complex daily activity indicating age and disease related cognitive declines. Therefore, deficits in driving performance compared with ones without mild cognitive impairment (MCI) can reflect changes in cognitive functioning. There is increasing evidence that unobtrusive monitoring of older adults driving performance in a daily-life setting may allow us to detect subtle early changes in cognition. The objectives of this paper include designing low-cost in-vehicle sensing hardware capable of obtaining high-precision positioning and telematics data, identifying important indicators for early changes in cognition, and detecting early-warning signs of cognitive impairment in a truly normal, day-to-day driving condition with machine learning approaches. Our statistical analysis comparing drivers with MCI to those without reveals that those with MCI exhibit smoother and safer driving patterns. This suggests that drivers with MCI are cognizant of their condition and tend to avoid erratic driving behaviors. Furthermore, our Random Forest models identified the number of night trips, number of trips, and education as the most influential factors in our data evaluation.
</details>
<details>
<summary>摘要</summary>
驾驶是一项复杂的日常活动，表征年龄和疾病相关的认知下降。因此，与无明遇病患（MCI）相比，驾驶性能下降的差异可能反映认知功能的变化。有增加证据表明，在日常生活环境中不侵入式监测老年人驾驶行为可能有助于早期发现轻度认知障碍。本文的目标包括设计低成本的汽车内部感知硬件，获得高精度的位置定位和通信数据，确定重要的认知变化指标，并使用机器学习方法探测日常驾驶中的认知障碍警示。我们的统计分析表明，与MCI相比，有MCI的 Driver exhibit更稳定和更安全的驾驶模式。这表明，有MCI的 Driver 意识到自己的状况，并尽可能避免异常的驾驶行为。此外，我们的Random Forest模型确定了夜间行驶次数、总行驶次数和教育水平是我们数据评估中最重要的因素。
</details></li>
</ul>
<hr>
<h2 id="Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective"><a href="#Assessing-Knowledge-Editing-in-Language-Models-via-Relation-Perspective" class="headerlink" title="Assessing Knowledge Editing in Language Models via Relation Perspective"></a>Assessing Knowledge Editing in Language Models via Relation Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09053">http://arxiv.org/abs/2311.09053</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective">https://github.com/weiyifan1023/knowledge-edit-based-on-relation-perspective</a></li>
<li>paper_authors: Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, Kang Liu</li>
<li>for: 本研究旨在修改大语言模型中的事实知识，并 investigate relation-centric 知识编辑方法的可行性。</li>
<li>methods: 本研究使用了一个新的benchmark名为RaKE，用于评估relation based知识编辑方法。还进行了多种知识编辑基线的比较实验，以及对 transformer 中关系知识的深入研究。</li>
<li>results: 研究结果表明，现有的知识编辑方法在编辑关系上存在潜在的困难，而且关系知识不仅存储在FFN网络中，还存储在注意层中。这些结果为未来的relation-based知识编辑方法提供了实验支持。<details>
<summary>Abstract</summary>
Knowledge Editing (KE) for modifying factual knowledge in Large Language Models (LLMs) has been receiving increasing attention. However, existing knowledge editing methods are entity-centric, and it is unclear whether this approach is suitable for a relation-centric perspective. To address this gap, this paper constructs a new benchmark named RaKE, which focuses on Relation based Knowledge Editing. In this paper, we establish a suite of innovative metrics for evaluation and conduct comprehensive experiments involving various knowledge editing baselines. We notice that existing knowledge editing methods exhibit the potential difficulty in their ability to edit relations. Therefore, we further explore the role of relations in factual triplets within the transformer. Our research results confirm that knowledge related to relations is not only stored in the FFN network but also in the attention layers. This provides experimental support for future relation-based knowledge editing methods.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）中的知识编辑（KE）已经获得了增加的注意。然而，现有的知识编辑方法都是基于实体中心的，而不是关系中心的。为了填补这个差距，本文建立了一个新的benchmark名为RaKE，它专注于关系基本知识编辑。本文提出了一个创新的评估标准和进行了各种知识编辑基线的广泛实验。我们发现现有的知识编辑方法对于修改关系表现出了潜在的问题。因此，我们进一步探索关系在简单 triplets 中的知识是如何储存和处理的。我们的研究结果显示，关系知识不仅在 FFN 网络中储存，还在注意层中储存。这给了未来关系基本知识编辑方法的实验支持。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts"><a href="#Improving-Zero-shot-Visual-Question-Answering-via-Large-Language-Models-with-Reasoning-Question-Prompts" class="headerlink" title="Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts"></a>Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09050">http://arxiv.org/abs/2311.09050</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecnu-dase-nlp/rqp">https://github.com/ecnu-dase-nlp/rqp</a></li>
<li>paper_authors: Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian</li>
<li>for: 本研究旨在提高零shot情境下的视觉问答系统（VQA）的性能，通过帮助大语言模型（LLMs）更好地理解和回答问题。</li>
<li>methods: 我们提出了一种新的问题提示方法，即理解问题提示（Reasoning Question Prompts，RQP），可以让LLMs更好地理解和回答问题。RQP通过一个不supervised的问题编辑模块生成了每个问题的自 contenido问题，以便更好地指导LLMs回答问题。</li>
<li>results: 我们在三个VQA挑战中测试了RQP方法，结果表明，RQP可以在零shot情境下显著提高LLMs的性能，并在四个数据集中超越现有的零shot方法。我们的源代码已经公开在GitHub上（<a target="_blank" rel="noopener" href="https://github.com/ECNU-DASE-NLP/RQP%EF%BC%89%E3%80%82">https://github.com/ECNU-DASE-NLP/RQP）。</a><details>
<summary>Abstract</summary>
Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at \url{https://github.com/ECNU-DASE-NLP/RQP}.
</details>
<details>
<summary>摘要</summary>
zero-shot视觉问答（VQA）是一种引人注目的视觉语言任务，它检验系统在不受训练数据的情况下，对图像和文本之间的理解能力。最近，通过将图像转换为caption，使得多 modalities之间的信息相互汇流，大型自然语言模型（LLMs）可以通过未经训练的情况下，对未经见过的问题进行有效的回答。为了设计理想的提问方法，许多研究已经探索了不同的策略来选择或生成问题答对，作为示例提问。然而，它们完全忽视了提问的角色。原始的VQA任务中的问题通常会遇到斜杠和混乱，需要中间的推理。为此，我们提出了视觉问答推理提问（RQP），可以further activate LLMs的零shot能力。具体来说，为每个问题，我们首先通过不supervised问题编辑模块生成自包含的推理提问，考虑语言流畅性、意义完整性和语法不变性。每个推理提问都能够明确表达问题的意图。这些推理提问的候选答案与其自信度分数 acting as answer heuristics被 fed into LLMs，并生成最终的答案。我们在三个VQA挑战中评估了推理提问，实验结果表明，它们可以在零shot Setting下significantly提高LLMs的表现，并在四个数据集中超越现有的零shot方法。我们的源代码公开release于\url{https://github.com/ECNU-DASE-NLP/RQP}.
</details></li>
</ul>
<hr>
<h2 id="MELA-Multilingual-Evaluation-of-Linguistic-Acceptability"><a href="#MELA-Multilingual-Evaluation-of-Linguistic-Acceptability" class="headerlink" title="MELA: Multilingual Evaluation of Linguistic Acceptability"></a>MELA: Multilingual Evaluation of Linguistic Acceptability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09033">http://arxiv.org/abs/2311.09033</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, Hai Hu</li>
<li>for: 本研究的目的是提供一个多语言的语言模型评估 benchmark，以evaluate 不同语言模型在语言学可接受性方面的表现。</li>
<li>methods: 本研究使用了多种语言模型，包括ChatGPT和XLM-R，并进行了过程学习和多任务学习。同时，研究者们还使用了层 wise probing 来分析 XLM-R 的 weights 是如何影响其在不同语言之间的推理能力。</li>
<li>results: 研究结果显示，XLM-R 在 zero-shot  Setting 中可以达到与 fine-tuned XLM-R 相当的性能，而 ChatGPT 则需要在 Context 中提供示例来改善其性能。同时，研究者们还发现了一些语言之间的推理困难，并提出了一种” conflicting weight” 的概念来描述这种现象。<details>
<summary>Abstract</summary>
Recent benchmarks for Large Language Models (LLMs) have mostly focused on application-driven tasks such as complex reasoning and code generation, and this has led to a scarcity in purely linguistic evaluation of LLMs. Against this background, we introduce Multilingual Evaluation of Linguistic Acceptability -- MELA, the first multilingual benchmark on linguistic acceptability with 48K samples covering 10 languages from a diverse set of language families. We establish baselines of commonly used LLMs along with supervised models, and conduct cross-lingual transfer and multi-task learning experiments with XLM-R. In pursuit of multilingual interpretability, we analyze the weights of fine-tuned XLM-R to explore the possibility of identifying transfer difficulty between languages. Our results show that ChatGPT benefits much from in-context examples but still lags behind fine-tuned XLM-R, while the performance of GPT-4 is on par with fine-tuned XLM-R even in zero-shot setting. Cross-lingual and multi-task learning experiments show that unlike semantic tasks, in-language training data is crucial in acceptability judgements. Results in layerwise probing indicate that the upper layers of XLM-R become a task-specific but language-agnostic region for multilingual acceptability judgment. We also introduce the concept of conflicting weight, which could be a potential indicator for the difficulty of cross-lingual transfer between languages. Our data will be available at https://github.com/sjtu-compling/MELA.
</details>
<details>
<summary>摘要</summary>
近期大语言模型（LLM）的 benchmark 主要集中在应用驱动的任务上，如复杂的理解和代码生成，这导致了对 LLM 的纯语言评估的缺乏。为了解决这问题，我们介绍了多语言评估语言可接受性（MELA），这是一个包含 48K 个样本，覆盖 10 种语言家族的多语言 benchmark。我们建立了常用的 LLG 基elines，以及supervised 模型的基elines，并进行了跨语言传播和多任务学习实验。在追求多语言可读性的探索中，我们分析了精心调整的 XLM-R 的权重，以探索语言之间传播困难的可能性。我们的结果显示，ChatGPT 受到上下文例子的启发，但仍落后于精心调整的 XLM-R，而 GPT-4 在零shot 设定下与精心调整的 XLM-R 的性能相当。跨语言和多任务学习实验表明，与 semantic 任务不同，在语言上的培训数据是关键在 acceptability 判断中。层wise probing 结果表明，XLM-R 的Upper层变成了多语言可接受性的任务特定 yet language-agnostic 区域。我们还引入了 conflicting weight 概念，它可能是跨语言传播之间语言的难度指标。我们的数据将在 GitHub 上发布。
</details></li>
</ul>
<hr>
<h2 id="Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning"><a href="#Assessing-the-Robustness-of-Intelligence-Driven-Reinforcement-Learning" class="headerlink" title="Assessing the Robustness of Intelligence-Driven Reinforcement Learning"></a>Assessing the Robustness of Intelligence-Driven Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09027">http://arxiv.org/abs/2311.09027</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari, Federico Cerutti</li>
<li>for: This paper focuses on the problem of robustness in intelligence-driven reinforcement learning, specifically in military contexts where high stakes and uncertainty are prevalent.</li>
<li>methods: The paper employs reward machines to express complex reward structures in RL tasks, and explores the need for further research in evidential reasoning and learning to improve the robustness of current state-of-the-art reinforcement learning approaches.</li>
<li>results: The preliminary results presented in the paper suggest the need for further research to harden current RL approaches before they can be considered mission-critical-ready.<details>
<summary>Abstract</summary>
Robustness to noise is of utmost importance in reinforcement learning systems, particularly in military contexts where high stakes and uncertain environments prevail. Noise and uncertainty are inherent features of military operations, arising from factors such as incomplete information, adversarial actions, or unpredictable battlefield conditions. In RL, noise can critically impact decision-making, mission success, and the safety of personnel. Reward machines offer a powerful tool to express complex reward structures in RL tasks, enabling the design of tailored reinforcement signals that align with mission objectives. This paper considers the problem of the robustness of intelligence-driven reinforcement learning based on reward machines. The preliminary results presented suggest the need for further research in evidential reasoning and learning to harden current state-of-the-art reinforcement learning approaches before being mission-critical-ready.
</details>
<details>
<summary>摘要</summary>
<<SYS>>military contexts 的 robustness to noise 是权重要的，特别是在高赌注和不确定环境下。雨声和不确定性是军事操作的内生特征，由于因素如不完整信息、敌方行动或不可预测的战场条件而出现。在RL中，雨声可能会重要影响决策、任务成功和人员安全。奖励机器提供了一种强大的工具来表达复杂的奖励结构在RL任务中，使得设计定制化的奖励信号与任务目标相对应。本文考虑了奖励机器驱动的智能学习robustness问题。初步结果表明需要进一步研究证据推理和学习以强化当前状态艺术RL方法，以便在任务关键ready。<<SYS>>
</details></li>
</ul>
<hr>
<h2 id="Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach"><a href="#Identification-and-Estimation-for-Nonignorable-Missing-Data-A-Data-Fusion-Approach" class="headerlink" title="Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach"></a>Identification and Estimation for Nonignorable Missing Data: A Data Fusion Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09015">http://arxiv.org/abs/2311.09015</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zixiao Wang, AmirEmad Ghassami, Ilya Shpitser</li>
<li>for: identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR)</li>
<li>methods: inspired by data fusion, using information in an MNAR dataset and an auxiliary dataset subject to missingness at random (MAR)</li>
<li>results: can identify the parameter of interest given pooled data, under two complementary sets of assumptions; derived an inverse probability weighted (IPW) estimator for identified parameters, and evaluated the performance of the estimation strategies via simulation studies<details>
<summary>Abstract</summary>
We consider the task of identifying and estimating a parameter of interest in settings where data is missing not at random (MNAR). In general, such parameters are not identified without strong assumptions on the missing data model. In this paper, we take an alternative approach and introduce a method inspired by data fusion, where information in an MNAR dataset is augmented by information in an auxiliary dataset subject to missingness at random (MAR). We show that even if the parameter of interest cannot be identified given either dataset alone, it can be identified given pooled data, under two complementary sets of assumptions. We derive an inverse probability weighted (IPW) estimator for identified parameters, and evaluate the performance of our estimation strategies via simulation studies.
</details>
<details>
<summary>摘要</summary>
我团队考虑了在数据损失不均匀（MNAR）的情况下识别和估算参数 интереса。通常情况下，这些参数无法 sans strong assumptions on the missing data model。在这篇论文中，我们采取了一种不同的方法，并通过数据融合引入了一个auxiliary dataset，这个dataset受到随机 missing（MAR）。我们表明，即使 données alone 中的参数无法识别，也可以通过 combining data 识别出参数，只要满足两个 complementary sets of assumptions。我们 derivate了一种 inverse probability weighted（IPW）估计器，并通过 simulations studies 评估了我们的估计策略的性能。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning"><a href="#Adversarial-Attacks-to-Reward-Machine-based-Reinforcement-Learning" class="headerlink" title="Adversarial Attacks to Reward Machine-based Reinforcement Learning"></a>Adversarial Attacks to Reward Machine-based Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09014">http://arxiv.org/abs/2311.09014</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Nodari</li>
<li>for: 本研究旨在提供首个对奖金机制（RM）基于 reinforcement learning 技术的安全性分析，以便更好地理解和提高这种技术在不良场景下的稳定性。</li>
<li>methods: 本研究使用 blinding attacks 这种新的攻击方法，以评估 RM-based reinforcement learning 技术的安全性。</li>
<li>results: 研究发现，blinding attacks 可以成功地破坏 RM-based reinforcement learning 技术的安全性，并提供了一种新的攻击方法来攻击这种技术。<details>
<summary>Abstract</summary>
In recent years, Reward Machines (RMs) have stood out as a simple yet effective automata-based formalism for exposing and exploiting task structure in reinforcement learning settings. Despite their relevance, little to no attention has been directed to the study of their security implications and robustness to adversarial scenarios, likely due to their recent appearance in the literature. With my thesis, I aim to provide the first analysis of the security of RM-based reinforcement learning techniques, with the hope of motivating further research in the field, and I propose and evaluate a novel class of attacks on RM-based techniques: blinding attacks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake"><a href="#Leveraging-AI-for-Natural-Disaster-Management-Takeaways-From-The-Moroccan-Earthquake" class="headerlink" title="Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake"></a>Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08999">http://arxiv.org/abs/2311.08999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morocco Solidarity Hackathon</li>
<li>for: 这篇论文主要是为了探讨在2023年阿哈鲁兹地震后，全球灾害管理策略的批判性反思，以及使用人工智能（AI）提高灾害准备、应急回应和恢复的技术。</li>
<li>methods: 这篇论文使用了全面的文献综述、赢得项目概述、关键发现和挑战，包括实时开源数据、数据缺乏和交叉学科合作的障碍。</li>
<li>results: 这篇论文得到了许多关键发现和挑战，包括实时开源数据的潜在价值、数据缺乏的问题和交叉学科合作的障碍。同时，论文还发起了社区呼吁，呼吁更多的行业专家和学者参与到灾害管理领域的研究和实践中来。<details>
<summary>Abstract</summary>
The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023 prompted critical reflections on global disaster management strategies, resulting in a post-disaster hackathon, using artificial intelligence (AI) to improve disaster preparedness, response, and recovery. This paper provides (i) a comprehensive literature review, (ii) an overview of winning projects, (iii) key insights and challenges, namely real-time open-source data, data scarcity, and interdisciplinary collaboration barriers, and (iv) a community-call for further action.
</details>
<details>
<summary>摘要</summary>
在2023年Morocco的阿卢哈沃兹发生了6.8级地震，这导致了全球灾害管理策略的批判性反思，并且促使了一场以人工智能（AI）为核心的 poste-disaster hackathon，以提高灾害准备、应急回应和恢复。本文提供以下内容：1. 全面的文献综述2. 赢家项目的概述3. 关键的发现和挑战，包括实时开源数据、数据缺乏和跨学科协作障碍4. 社区呼吁更进一步的行动Translation notes:* "阿卢哈沃兹" (Al Haouz) is the name of the location where the earthquake occurred, and it is written in Simplified Chinese as "阿卢哈沃兹" (Al Haouz).* "灾害管理策略" (disaster management strategies) is written as "灾害管理策略" (disaster management strategies) in Simplified Chinese.* "poste-disaster hackathon" is written as "后灾害黑匠挑战" (post-disaster hackathon) in Simplified Chinese.* "实时开源数据" (real-time open-source data) is written as "实时开源数据" (real-time open-source data) in Simplified Chinese.* "数据缺乏" (data scarcity) is written as "数据缺乏" (data scarcity) in Simplified Chinese.* "跨学科协作障碍" (interdisciplinary collaboration barriers) is written as "跨学科协作障碍" (interdisciplinary collaboration barriers) in Simplified Chinese.
</details></li>
</ul>
<hr>
<h2 id="When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks"><a href="#When-does-In-context-Learning-Fall-Short-and-Why-A-Study-on-Specification-Heavy-Tasks" class="headerlink" title="When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks"></a>When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08993">http://arxiv.org/abs/2311.08993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Peng, Xiaozhi Wang, Jianhui Chen, Weikai Li, Yunjia Qi, Zimu Wang, Zhili Wu, Kaisheng Zeng, Bin Xu, Lei Hou, Juanzi Li</li>
<li>for: 本文旨在探讨大语言模型（LLM）在启发式学习（ICL）方法下的局限性，以及这些局限性的根本原因。</li>
<li>methods: 作者通过对18种特有的任务进行广泛的实验，发现ICL在处理这些任务时存在三个主要的原因：无法具体地理解上下文，任务架构理解与人类不一致，以及缺乏长文理解能力。</li>
<li>results: 研究发现，通过细化调教，LLM可以在这些任务上达到不错的性能，这表明ICL的失败不是LLM的内在缺陷，而是现有的对齐方法的不足，使LLM无法处理复杂的规则繁残任务。<details>
<summary>Abstract</summary>
In-context learning (ICL) has become the default method for using large language models (LLMs), making the exploration of its limitations and understanding the underlying causes crucial. In this paper, we find that ICL falls short of handling specification-heavy tasks, which are tasks with complicated and extensive task specifications, requiring several hours for ordinary humans to master, such as traditional information extraction tasks. The performance of ICL on these tasks mostly cannot reach half of the state-of-the-art results. To explore the reasons behind this failure, we conduct comprehensive experiments on 18 specification-heavy tasks with various LLMs and identify three primary reasons: inability to specifically understand context, misalignment in task schema comprehension with humans, and inadequate long-text understanding ability. Furthermore, we demonstrate that through fine-tuning, LLMs can achieve decent performance on these tasks, indicating that the failure of ICL is not an inherent flaw of LLMs, but rather a drawback of existing alignment methods that renders LLMs incapable of handling complicated specification-heavy tasks via ICL. To substantiate this, we perform dedicated instruction tuning on LLMs for these tasks and observe a notable improvement. We hope the analyses in this paper could facilitate advancements in alignment methods enabling LLMs to meet more sophisticated human demands.
</details>
<details>
<summary>摘要</summary>
启发式学习（ICL）已成为大语言模型（LLM）的默认方法，因此探索其限制和理解下面层次的原因变得非常重要。在这篇论文中，我们发现ICL在需要较多任务规定的任务上表现不佳，这些任务通常需要人类花费几个小时来学习，如传统信息抽取任务。ICL的性能在这些任务上通常无法达到状态艺术的一半。为了探索这些失败的原因，我们进行了18个需要较多任务规定的任务的广泛实验，并确定了三个主要原因：无法特别理解上下文，任务架构与人类的理解不符，以及缺乏长文本理解能力。此外，我们还证明了通过微调，LLM可以在这些任务上达到不错的表现，这表明ICL失败不是LLM的内在缺陷，而是现有的对齐方法的缺陷，使得LLM无法通过ICL处理复杂的需要较多任务。为了证明这一点，我们在LLM上进行了专门的指令调整，并观察到了明显的改善。我们希望这些分析可以促进对齐方法的进步，使LLM能够更好地满足人类的需求。
</details></li>
</ul>
<hr>
<h2 id="Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems"><a href="#Proceedings-Fifth-International-Workshop-on-Formal-Methods-for-Autonomous-Systems" class="headerlink" title="Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems"></a>Proceedings Fifth International Workshop on Formal Methods for Autonomous Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08987">http://arxiv.org/abs/2311.08987</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Marie Farrell, Matt Luckcuck, Mario Gleirscher, Maike Schwammberger</li>
<li>for: 本研讨会论文集是为了形式方法与自主系统之间的研究提供一个发表平台。</li>
<li>methods: 本研讨会接受了25篇提交论文，其中包括11篇正式论文、3篇经验报告、6篇研究预览和5篇视野论文。</li>
<li>results: 经审核后，本研讨会接受了15篇论文，包括8篇长篇论文和7篇短篇论文。<details>
<summary>Abstract</summary>
This EPTCS volume contains the proceedings for the Fifth International Workshop on Formal Methods for Autonomous Systems (FMAS 2023), which was held on the 15th and 16th of November 2023. FMAS 2023 was co-located with 18th International Conference on integrated Formal Methods (iFM) (iFM'22), organised by Leiden Institute of Advanced Computer Science of Leiden University. The workshop itself was held at Scheltema Leiden, a renovated 19th Century blanket factory alongside the canal.   FMAS 2023 received 25 submissions. We received 11 regular papers, 3 experience reports, 6 research previews, and 5 vision papers. The researchers who submitted papers to FMAS 2023 were from institutions in: Australia, Canada, Colombia, France, Germany, Ireland, Italy, the Netherlands, Sweden, the United Kingdom, and the United States of America. Increasing our number of submissions for the third year in a row is an encouraging sign that FMAS has established itself as a reputable publication venue for research on the formal modelling and verification of autonomous systems. After each paper was reviewed by three members of our Programme Committee we accepted a total of 15 papers: 8 long papers and 7 short papers.
</details>
<details>
<summary>摘要</summary>
这个 EPTCS 卷包含了第五届国际形式方法工作坊（FMAS 2023）的论文集，该活动于2023年11月15日-16日举行。FMAS 2023 与18届国际集成形式方法会议（iFM）（iFM'22）联合举办，由雷登大学计算机科学院主办。工作坊本身在19世纪19世纪的重新翻新的褡厂 alongside the canal 举行。 FMAS 2023 接受了25篇提交的论文，包括11篇正式论文、3篇经验报告、6篇研究预览和5篇视野论文。参加该活动的研究人员来自：澳大利亚、加拿大、哥伦比亚、法国、德国、爱尔兰、意大利、荷兰、瑞典、英国和美国。我们在第三年 consecutively 收到更多的提交，表明 FMAS 已经成为自动化系统的正式模型和验证的出版物。经过三名编委会成员的审核后，我们接受了总共15篇论文：8篇长篇和7篇短篇。
</details></li>
</ul>
<hr>
<h2 id="Linear-time-Evidence-Accumulation-Clustering-with-KMeans"><a href="#Linear-time-Evidence-Accumulation-Clustering-with-KMeans" class="headerlink" title="Linear time Evidence Accumulation Clustering with KMeans"></a>Linear time Evidence Accumulation Clustering with KMeans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09272">http://arxiv.org/abs/2311.09272</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaëlle Candel</li>
<li>for: 本研究旨在提出一种简单 yet efficient consensus clustering方法，以解决现有方法的计算复杂性问题。</li>
<li>methods: 本方法基于证据积累 clustering，首先构建一个 n x n 的相关性矩阵，然后使用这个矩阵进行 clustering，以提取共识 clusters。与其他方法不同的是，这里不需要找到匹配于两个不同 partitioning 中的匹配项。但是，这种方法受到计算复杂性的限制，只适用于小规模 dataset。</li>
<li>results: 本研究提出了一种方法来高效计算 density，从而降低了计算复杂性的问题。此外，我们证明了 k-means 自然地maximizes density。在多个 benchmark dataset 上进行了比较，k-means 和 bisecting 版本的结果与其他现有的 consensus algorithm 相当，而且计算成本较低。此外，k-means 在 density 方面获得了最佳结果。这些结果表明，consensus clustering 可以使用简单的算法解决。<details>
<summary>Abstract</summary>
Among ensemble clustering methods, Evidence Accumulation Clustering is one of the simplest technics. In this approach, a co-association (CA) matrix representing the co-clustering frequency is built and then clustered to extract consensus clusters. Compared to other approaches, this one is simple as there is no need to find matches between clusters obtained from two different partitionings. Nevertheless, this method suffers from computational issues, as it requires to compute and store a matrix of size n x n, where n is the number of items. Due to the quadratic cost, this approach is reserved for small datasets. This work describes a trick which mimic the behavior of average linkage clustering. We found a way of computing efficiently the density of a partitioning, reducing the cost from a quadratic to linear complexity. Additionally, we proved that the k-means maximizes naturally the density. We performed experiments on several benchmark datasets where we compared the k-means and the bisecting version to other state-of-the-art consensus algorithms. The k-means results are comparable to the best state of the art in terms of NMI while keeping the computational cost low. Additionally, the k-means led to the best results in terms of density. These results provide evidence that consensus clustering can be solved with simple algorithms.
</details>
<details>
<summary>摘要</summary>
在ensemble clustering方法中，证据积累 clustering 是一种最简单的方法。在这种方法中，我们首先构建一个 co-association（CA）矩阵，表示item之间的协 clustering频率，然后使用这个矩阵进行归一化，以提取共识cluster。相比其他方法，这种方法更简单，不需要在两个不同的 partitioning 中找到匹配。然而，这种方法受到计算问题的限制，因为需要计算和存储一个 n x n 的矩阵，其中 n 是items的数量，这会导致计算成本 quadratic。由于这个问题，这种方法只适用于小型数据集。本文描述了一种技巧，可以模拟average linkage clustering的行为。我们发现了一种可以高效计算分区 densities 的方法，从而降低计算成本的复杂度从 quadratic 降至 linear。此外，我们证明了 k-means 自然地 maximizes densities。我们在多个 benchmark 数据集上进行了实验，并与其他状态Of-the-art consensus算法进行了比较。k-means 的结果与最佳状态Of-the-art 的 NMI 相当，同时计算成本低。此外，k-means 导致了最佳的 densities 结果。这些结果证明了 consensus clustering 可以使用简单的算法解决。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Linear-Relational-Concepts-in-Large-Language-Models"><a href="#Identifying-Linear-Relational-Concepts-in-Large-Language-Models" class="headerlink" title="Identifying Linear Relational Concepts in Large Language Models"></a>Identifying Linear Relational Concepts in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08968">http://arxiv.org/abs/2311.08968</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Robot-learning">https://github.com/Aryia-Behroziuan/Robot-learning</a></li>
<li>paper_authors: David Chanin, Anthony Hunter, Oana-Maria Camburu</li>
<li>for: 本文旨在找到隐藏层中的概念方向，以便更好地理解模型表示的概念。</li>
<li>methods: 本文提出了一种Linear Relational Concepts（LRC）技术，通过模型Subject和Object之间的关系为线性关系嵌入（LRE）来找到隐藏层中的概念方向。</li>
<li>results: 研究发现，通过逆向LRE并使用早期的对象层来找到概念方向，可以实现高效地为概念分类和影响模型输出。<details>
<summary>Abstract</summary>
Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any given human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts at a given hidden layer in a transformer LM by first modeling the relation between subject and object as a linear relational embedding (LRE). While the LRE work was mainly presented as an exercise in understanding model representations, we find that inverting the LRE while using earlier object layers results in a powerful technique to find concept directions that both work well as a classifier and causally influence model outputs.
</details>
<details>
<summary>摘要</summary>
transformer 语言模型（LM）已经显示出在隐藏活动空间中表示概念的方向。然而，为任何给定的人类可解释的概念，如何在隐藏层中找到其方向？我们提出了线性关系概念（LRC）技术，用于在 transformer LM 中找到人类可解释的概念方向。我们首先将关系 между主题和对象模型为线性关系嵌入（LRE）。虽然 LRE 工作主要被表现为模型表示理解的一种实践，但我们发现，对于早期对象层来说，倒转 LRE 会生成一种强大的技术，可以作为分类器并在模型输出中产生 causal 影响。
</details></li>
</ul>
<hr>
<h2 id="I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots"><a href="#I-Was-Blind-but-Now-I-See-Implementing-Vision-Enabled-Dialogue-in-Social-Robots" class="headerlink" title="I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots"></a>I Was Blind but Now I See: Implementing Vision-Enabled Dialogue in Social Robots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08957">http://arxiv.org/abs/2311.08957</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giulio Antonio Abbo, Tony Belpaeme</li>
<li>for: 该论文旨在探讨如何通过将视觉功能 integrate into conversational agents ，以提高人机交互的效果。</li>
<li>methods: 该论文使用最新的大语言模型（如 GPT-4、IDEFICS）来解释文本提示和实时视觉输入，创造出更Contextually 意识的对话系统。</li>
<li>results: 六个与 Furhat 机器人进行的交互记录和分析，ILLUSTRATE 和讨论所获得的结果，提出了一种将文本和视觉modalities融合的对话系统。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of human-computer interaction, the integration of vision capabilities into conversational agents stands as a crucial advancement. This paper presents an initial implementation of a dialogue manager that leverages the latest progress in Large Language Models (e.g., GPT-4, IDEFICS) to enhance the traditional text-based prompts with real-time visual input. LLMs are used to interpret both textual prompts and visual stimuli, creating a more contextually aware conversational agent. The system's prompt engineering, incorporating dialogue with summarisation of the images, ensures a balance between context preservation and computational efficiency. Six interactions with a Furhat robot powered by this system are reported, illustrating and discussing the results obtained. By implementing this vision-enabled dialogue system, the paper envisions a future where conversational agents seamlessly blend textual and visual modalities, enabling richer, more context-aware dialogues.
</details>
<details>
<summary>摘要</summary>
在人机交互领域的快速发展中，融合视觉能力的对话管理器是一项重要的进步。这篇论文介绍了一种使用最新的大语言模型（如GPT-4、IDEFICS）来增强传统的文本基于的提示，并在实时视觉输入的基础上进行对话管理。这些语言模型能够同时解释文本提示和视觉刺激，创造出更Contextually 意识的对话代理人。系统的提问工程，包括对话和图像摘要，保证了对话的上下文保持和计算效率的平衡。报告了六次与furhat机器人运行此系统的交互，并讲述了获得的结果。通过实现这种视觉启用对话系统，论文预测未来的对话代理人将协调文本和视觉模式，实现更加 ricther，Contextually 意识的对话。
</details></li>
</ul>
<hr>
<h2 id="Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control"><a href="#Safety-Trust-and-Ethics-Considerations-for-Human-AI-Teaming-in-Aerospace-Control" class="headerlink" title="Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control"></a>Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08943">http://arxiv.org/abs/2311.08943</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kerianne L. Hobbs, Bernard Li</li>
<li>for: 本文旨在探讨人工智能在航空系统控制中的合作，特别是人类和AI的团队合作，以及这些团队合作的安全、可靠和伦理方面。</li>
<li>methods: 本文使用了许多不同的方法，包括文献综述、案例研究和理论分析，以探讨不同的人工智能应用场景和相关的安全、可靠和伦理问题。</li>
<li>results: 本文的结果表明，在安全和任务关键领域中使用人工智能时，需要考虑到安全、可靠和伦理方面的问题，并且需要采取相应的措施来解决这些问题。<details>
<summary>Abstract</summary>
Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making.
</details>
<details>
<summary>摘要</summary>
设计一个安全、可信、伦理的人工智能可能是实际上不可能的；但是设计人工智能以安全、可信、伦理的使用为目标是可能的和必要的，尤其在安全和战略性领域如航空航天。安全、可信、伦理的使用人工智能常常被混用，但是一个系统可以安全地使用但不是可信或伦理的，可以有一个可信用但不是安全或伦理的，可以有一个伦理用但不是安全或可信的。这篇报告作为一个导论，探讨了这些概念之间的细腻差异，尤其在人工智能和人类团队在航空系统控制中的应用， где人类可能在、在或离Loop的决策过程中。
</details></li>
</ul>
<hr>
<h2 id="Reasoning-over-Description-Logic-based-Contexts-with-Transformers"><a href="#Reasoning-over-Description-Logic-based-Contexts-with-Transformers" class="headerlink" title="Reasoning over Description Logic-based Contexts with Transformers"></a>Reasoning over Description Logic-based Contexts with Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08941">http://arxiv.org/abs/2311.08941</a></li>
<li>repo_url: None</li>
<li>paper_authors: Angelos Poulis, Eleni Tsalapati, Manolis Koubarakis</li>
<li>for: 本研究的目的是测试 transformer 模型在复杂的语言上进行推理能力。</li>
<li>methods: 本研究使用了生成自描述逻辑知识库的自然语言问答数据集，并使用了 $\mathcal{ALCQ}$ 语言来生成知识库。</li>
<li>results: 研究发现，使用 DEBERTa 模型 DELTA$_M$ 的表现随 reasoning depth 的增加而无显著变化，而 sentence length 的增加则不会影响表现。此外，模型在不同的 reasoning depth 上进行推理时的泛化能力也得到了证明。<details>
<summary>Abstract</summary>
One way that the current state of the art measures the reasoning ability of transformer-based models is by evaluating accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of the contexts used are in practice very simple; in most cases, they are generated from short first-order logic sentences with only a few logical operators and quantifiers. In this work, we seek to answer the question how well a transformer-based model will perform reasoning over expressive contexts. For this purpose, we construct a synthetic natural language question-answering dataset, generated by description logic knowledge bases. For the generation of the knowledge bases, we use the expressive language $\mathcal{ALCQ}$. The resulting dataset contains 384K examples, and increases in two dimensions: i) reasoning depth, and ii) length of sentences. We show that the performance of our DeBERTa-based model, DELTA$_M$, is marginally affected when the reasoning depth is increased and it is not affected at all when the length of the sentences is increasing. We also evaluate the generalization ability of the model on reasoning depths unseen at training, both increasing and decreasing, revealing interesting insights into the model's adaptive generalization abilities.
</details>
<details>
<summary>摘要</summary>
Currently, the state-of-the-art measure of reasoning ability in transformer-based models is their accuracy in downstream tasks like logical question answering or proof generation over synthetic contexts expressed in natural language. However, most of these contexts are very simple, typically consisting of short first-order logic sentences with only a few logical operators and quantifiers. In this study, we aim to investigate how well a transformer-based model can perform reasoning over more expressive contexts. To achieve this, we create a synthetic natural language question-answering dataset generated by description logic knowledge bases. We use the expressive language $\mathcal{ALCQ}$ to generate the knowledge bases, resulting in a dataset containing 384K examples that increase in two dimensions: i) reasoning depth, and ii) length of sentences. Our DeBERTa-based model, DELTA$_M$, shows marginal impact from increased reasoning depth and no impact from longer sentences. We also evaluate the model's generalization ability on unseen reasoning depths, both increasing and decreasing, revealing interesting insights into its adaptive generalization capabilities.Note: Please note that the translation is in Simplified Chinese, which is one of the two standard Chinese writing systems. If you prefer Traditional Chinese, please let me know and I will be happy to provide the translation in that script.
</details></li>
</ul>
<hr>
<h2 id="Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning"><a href="#Supported-Trust-Region-Optimization-for-Offline-Reinforcement-Learning" class="headerlink" title="Supported Trust Region Optimization for Offline Reinforcement Learning"></a>Supported Trust Region Optimization for Offline Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08935">http://arxiv.org/abs/2311.08935</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, Xiangyang Ji</li>
<li>for: 提高掌控环境下的远离线强化学习效果</li>
<li>methods: 使用支持信任区域优化（STR）方法，即在行为政策内部进行强化学习优化，且受到行为政策支持的约束</li>
<li>results: 在假设无误度和抽象误差时，STR方法能够保证政策改进直至到达数据集中的优化策略，并在实际测试中表现出优于当前状态的表现。<details>
<summary>Abstract</summary>
Offline reinforcement learning suffers from the out-of-distribution issue and extrapolation error. Most policy constraint methods regularize the density of the trained policy towards the behavior policy, which is too restrictive in most cases. We propose Supported Trust Region optimization (STR) which performs trust region policy optimization with the policy constrained within the support of the behavior policy, enjoying the less restrictive support constraint. We show that, when assuming no approximation and sampling error, STR guarantees strict policy improvement until convergence to the optimal support-constrained policy in the dataset. Further with both errors incorporated, STR still guarantees safe policy improvement for each step. Empirical results validate the theory of STR and demonstrate its state-of-the-art performance on MuJoCo locomotion domains and much more challenging AntMaze domains.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate into Simplified Chinese</SYS>离线强化学uffer于out-of-distribution问题和推论误差。大多数策略约束方法将训练的策略密度规范到行为策略上，这是大多数情况下过于严格的。我们提议Supported Trust Region优化（STR），该方法通过在行为策略支持下进行信任区域策略优化，享受到较为lenient的支持约束。我们证明，当假设无approximation和抽象误差时，STR确保每步产生策略改进，直到在数据集中收敛到最优的支持约束策略。而在实际中，STR仍然保证每步安全的策略改进，即使包括两种误差。实验结果证明STR的理论和实际性能在MuJoCo步行领域和更加复杂的AntMaze领域都达到了顶峰水平。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery"><a href="#Leveraging-Activation-Maximization-and-Generative-Adversarial-Training-to-Recognize-and-Explain-Patterns-in-Natural-Areas-in-Satellite-Imagery" class="headerlink" title="Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery"></a>Leveraging Activation Maximization and Generative Adversarial Training to Recognize and Explain Patterns in Natural Areas in Satellite Imagery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08923">http://arxiv.org/abs/2311.08923</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Emam, Timo T. Stomberg, Ribana Roscher</li>
<li>for: 保护 natura 遗产的详细地图创建</li>
<li>methods: 使用activation maximization和生成对抗模型生成卫星图像，结合领域知识，提供完整和有效的解释方法</li>
<li>results: 生成的卫星图像可以准确地标识保护区域的自然 authenticity 特征，提高了保护区域的生态完整性的理解，可能对未来监测和保护做出贡献<details>
<summary>Abstract</summary>
Natural protected areas are vital for biodiversity, climate change mitigation, and supporting ecological processes. Despite their significance, comprehensive mapping is hindered by a lack of understanding of their characteristics and a missing land cover class definition. This paper aims to advance the explanation of the designating patterns forming protected and wild areas. To this end, we propose a novel framework that uses activation maximization and a generative adversarial model. With this, we aim to generate satellite images that, in combination with domain knowledge, are capable of offering complete and valid explanations for the spatial and spectral patterns that define the natural authenticity of these regions. Our proposed framework produces more precise attribution maps pinpointing the designating patterns forming the natural authenticity of protected areas. Our approach fosters our understanding of the ecological integrity of the protected natural areas and may contribute to future monitoring and preservation efforts.
</details>
<details>
<summary>摘要</summary>
自然保护区是生物多样性、气候变化缓解和生态过程支持的重要资源。尽管它们的重要性，但全面的地图制定受到了未understanding其特征和缺失的土地覆盖类划定的限制。本文提出了一种新的框架，使用活动最大化和生成对抗模型，以提高指定 Patterns forming protected and wild areas的解释。通过这种方法，我们可以生成具有完整性和有效性的卫星图像，与领域知识相结合，以提供自然 authenticity 的区域的完整和有效的解释。我们的提议的框架可以生成更精确的归属地图， pinpointing the designating patterns forming the natural authenticity of protected areas。这将有助于我们更好地理解保护区的生态完整性，并可能对未来监测和保护做出贡献。
</details></li>
</ul>
<hr>
<h2 id="An-Empathetic-User-Centric-Chatbot-for-Emotional-Support"><a href="#An-Empathetic-User-Centric-Chatbot-for-Emotional-Support" class="headerlink" title="An Empathetic User-Centric Chatbot for Emotional Support"></a>An Empathetic User-Centric Chatbot for Emotional Support</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09271">http://arxiv.org/abs/2311.09271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanting Pan, Yixuan Tang, Yuchen Niu</li>
<li>for: 这篇论文探讨了亚特媒体文化和人工智能之间的交叉点，尤其是游戏如何满足年轻女性的情感需求。</li>
<li>methods: 这篇论文使用了大语言模型（LLM）技术来超越传统的静态游戏剧本，创造出dinamic和情感响应的互动体验。</li>
<li>results: 研究人员通过在游戏剧本中添加问答（QA）系统，通过数据扩充和情感增强技术，创建了一个真实和支持的伴侣聊天机器人。<details>
<summary>Abstract</summary>
This paper explores the intersection of Otome Culture and artificial intelligence, particularly focusing on how Otome-oriented games fulfill the emotional needs of young women. These games, which are deeply rooted in a subcultural understanding of love, provide players with feelings of satisfaction, companionship, and protection through carefully crafted narrative structures and character development. With the proliferation of Large Language Models (LLMs), there is an opportunity to transcend traditional static game narratives and create dynamic, emotionally responsive interactions. We present a case study of Tears of Themis, where we have integrated LLM technology to enhance the interactive experience. Our approach involves augmenting existing game narratives with a Question and Answer (QA) system, enriched through data augmentation and emotional enhancement techniques, resulting in a chatbot that offers realistic and supportive companionship.
</details>
<details>
<summary>摘要</summary>
这篇论文探讨了互助文化和人工智能的交叉点，特别是游戏如何满足年轻女性的情感需求。这些游戏，深受互助文化的影响，为玩家提供满足、伙伴和保护的感受，通过精心设计的故事结构和人物发展。随着大语言模型（LLM）的普及，有机会超越传统的静止游戏剧本，创造动态、情感回应的互动体验。我们介绍了《泪之Theme》案例，我们在该游戏中集成了LLM技术，以增强互动体验。我们的方法包括在现有游戏剧本中添加问答（QA）系统，通过数据增强和情感增强技术，创造出真实和支持的伙伴。
</details></li>
</ul>
<hr>
<h2 id="NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios"><a href="#NormNet-Scale-Normalization-for-6D-Pose-Estimation-in-Stacked-Scenarios" class="headerlink" title="NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios"></a>NormNet: Scale Normalization for 6D Pose Estimation in Stacked Scenarios</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09269">http://arxiv.org/abs/2311.09269</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shuttlet/normnet">https://github.com/shuttlet/normnet</a></li>
<li>paper_authors: En-Te Lin, Wei-Jie Lv, Ding-Tao Huang, Long Zeng</li>
<li>for: 本研究旨在提出一种可以在堆积场景中robustly estimate不同尺度对象的6DoF pose estimator（NormNet）。</li>
<li>methods: 本方法首先使用点准 regression来学习每个对象的尺度，然后通过semantic segmentation和affine变换将所有对象 норmalized到同一个尺度。最后，它们被 fed into a shared pose estimator来恢复它们的6D姿态。此外，我们还提出了一种新的Sim-to-Real transfer管线，该管线结合了style transfer和domain randomization，以提高NormNet在实际数据上的性能。</li>
<li>results: 广泛的实验表明，提出的方法可以在公共benchmark和我们自己construct的MultiScale dataset上达到领先的性能。实际世界 эксперименты也显示，我们的方法可以robustly estimate不同尺度对象的6D姿态。<details>
<summary>Abstract</summary>
Existing Object Pose Estimation (OPE) methods for stacked scenarios are not robust to changes in object scale. This paper proposes a new 6DoF OPE network (NormNet) for different scale objects in stacked scenarios. Specifically, each object's scale is first learned with point-wise regression. Then, all objects in the stacked scenario are normalized into the same scale through semantic segmentation and affine transformation. Finally, they are fed into a shared pose estimator to recover their 6D poses. In addition, we introduce a new Sim-to-Real transfer pipeline, combining style transfer and domain randomization. This improves the NormNet's performance on real data even if we only train it on synthetic data. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on public benchmarks and the MultiScale dataset we constructed. The real-world experiments show that our method can robustly estimate the 6D pose of objects at different scales.
</details>
<details>
<summary>摘要</summary>
现有的栅格场景中对象姿态估计（OPE）方法不能抗测对象比例变化。这篇论文提出了一种新的6度自由姿态网络（NormNet），用于不同比例的 объекts在栅格场景中估计6D姿态。具体来说，每个对象的比例首先通过点级回归学习。然后，所有在栅格场景中的对象都被正规化为同一个比例通过 semantic segmentation 和Affine变换。最后，它们被 fed into 共享的姿态估计器，以便从 shared pose estimator 中回归其6D姿态。此外，我们还引入了一种新的 Sim-to-Real 传输管道， combining style transfer 和 domain randomization。这种管道可以在只使用 sintetic data 进行训练时，提高 NormNet 的表现 на real data。广泛的实验表明，我们提posed方法可以在公共 benchmarks 和我们自己构建的 MultiScale 数据集上达到顶尖性能。在实际场景中，我们的方法可以Robustly 估计不同比例的对象的6D姿态。
</details></li>
</ul>
<hr>
<h2 id="Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering"><a href="#Combining-Transfer-Learning-with-In-context-Learning-using-Blackbox-LLMs-for-Zero-shot-Knowledge-Base-Question-Answering" class="headerlink" title="Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering"></a>Combining Transfer Learning with In-context Learning using Blackbox LLMs for Zero-shot Knowledge Base Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08894">http://arxiv.org/abs/2311.08894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mayur Patidar, Avinash Singh, Riya Sawhney, Indrajit Bhattacharya, Mausam</li>
<li>for: 本文Addresses the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain.</li>
<li>methods: 本文使用了大量的无标签数据在目标域，并结合了源域的标签数据进行了转移学习。此外，文章还提出了基于黑盒大语言模型（BLLM）的受限自我调整方法，可以独立于转移设定进行执行。</li>
<li>results: 根据实验结果，提出的方法可以在 GrailQA 作为源域和 WebQSP 作为目标域的情况下，对两个阶段（检索和生成）进行了显著改进，并且也超越了当前的超参数化KBQA模型。此外，当有限量的标签数据时，BLLM的扩展也可以在域内设定中提供显著的改进。<details>
<summary>Abstract</summary>
We address the zero-shot transfer learning setting for the knowledge base question answering (KBQA) problem, where a large volume of labeled training data is available for the source domain, but no such labeled examples are available for the target domain. Transfer learning for KBQA makes use of large volumes of unlabeled data in the target in addition to the labeled data in the source. More recently, few-shot in-context learning using Black-box Large Language Models (BLLMs) has been adapted for KBQA without considering any source domain data. In this work, we show how to meaningfully combine these two paradigms for KBQA so that their benefits add up. Specifically, we preserve the two stage retrieve-then-generate pipeline of supervised KBQA and introduce interaction between in-context learning using BLLMs and transfer learning from the source for both stages. In addition, we propose execution-guided self-refinement using BLLMs, decoupled from the transfer setting. With the help of experiments using benchmark datasets GrailQA as the source and WebQSP as the target, we show that the proposed combination brings significant improvements to both stages and also outperforms by a large margin state-of-the-art supervised KBQA models trained on the source. We also show that in the in-domain setting, the proposed BLLM augmentation significantly outperforms state-of-the-art supervised models, when the volume of labeled data is limited, and also outperforms these marginally even when using the entire large training dataset.
</details>
<details>
<summary>摘要</summary>
我们研究了零shot转移学习 Setting for 知识库问答（KBQA）问题，其中有大量标注的训练数据在源领域可用，但target领域没有任何标注的示例。KBQA的转移学习使用了target领域的大量无标注数据，以及源领域的标注数据。在这种情况下，我们将黑obox大型自然语言模型（BLLM）的几个shot在 Context learning应用于KBQA，而不考虑源领域的数据。在这种情况下，我们保留了KBQA的两stage retrieve-then-generate架构，并在这两个阶段中引入了BLLM的交互。此外，我们还提出了基于BLLM的执行指导自适应，与转移学习分离。通过使用GrailQA作为源领域和WebQSP作为目标领域的实验，我们表明了我们的提案可以在两个阶段中提供显著改进，并且也超越了当前的supervised KBQA模型。此外，我们还表明了在域内设置下，我们的BLLM扩展可以在标注数据量有限的情况下获得显著改进，并且甚至在使用整个大量训练数据时也能够超越supervised模型。
</details></li>
</ul>
<hr>
<h2 id="Advances-in-ACL2-Proof-Debugging-Tools"><a href="#Advances-in-ACL2-Proof-Debugging-Tools" class="headerlink" title="Advances in ACL2 Proof Debugging Tools"></a>Advances in ACL2 Proof Debugging Tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08856">http://arxiv.org/abs/2311.08856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matt Kaufmann, J Strother Moore</li>
<li>for: 本文描述了ACL2用户通常会遇到失败的证明尝试，以及如何使用工具来解决这些失败。</li>
<li>methods: 本文专注于ACL2版本8.5后的改进：改进的break-rewrite工具以及新增的with-brr-data工具。</li>
<li>results: 通过使用这些工具，ACL2用户可以更有效地解决证明失败。<details>
<summary>Abstract</summary>
The experience of an ACL2 user generally includes many failed proof attempts. A key to successful use of the ACL2 prover is the effective use of tools to debug those failures. We focus on changes made after ACL2 Version 8.5: the improved break-rewrite utility and the new utility, with-brr-data.
</details>
<details>
<summary>摘要</summary>
ACL2用户通常会经历许多失败的证明尝试。成功使用ACL2证明工具的关键在于有效地使用工具来调试失败。我们关注ACL2版本8.5后的更改：改进的break-rewrite工具以及新增的with-brr-data工具。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English"><a href="#Evaluating-Gender-Bias-in-the-Translation-of-Gender-Neutral-Languages-into-English" class="headerlink" title="Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English"></a>Evaluating Gender Bias in the Translation of Gender-Neutral Languages into English</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08836">http://arxiv.org/abs/2311.08836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Spencer Rarrick, Ranjita Naik, Sundar Poudel, Vishal Chowdhary</li>
<li>for: 这个论文的目的是提出一个gender bias检测和 mitigation的数据集，以便更好地评估和改进Machine Translation（MT）系统中的gender bias问题。</li>
<li>methods: 这个论文使用了一个新的数据集名为GATE X-E，这个数据集包含了从土耳其语、匈牙利语、芬兰语和波斯语翻译成英语的人工翻译，每个翻译都有女性、男性和中性的多个变体。此外，这篇论文还提出了一种基于GPT-3.5 Turbo的英语性别重写解决方案，并使用GATE X-E来评估这种解决方案。</li>
<li>results: 这篇论文的研究结果表明，GATE X-E数据集可以帮助提高MT系统中gender bias的识别和改进，并且基于GPT-3.5 Turbo的英语性别重写解决方案也能够有效地改善MT系统中的gender bias问题。<details>
<summary>Abstract</summary>
Machine Translation (MT) continues to improve in quality and adoption, yet the inadvertent perpetuation of gender bias remains a significant concern. Despite numerous studies into gender bias in translations from gender-neutral languages such as Turkish into more strongly gendered languages like English, there are no benchmarks for evaluating this phenomenon or for assessing mitigation strategies. To address this gap, we introduce GATE X-E, an extension to the GATE (Rarrick et al., 2023) corpus, that consists of human translations from Turkish, Hungarian, Finnish, and Persian into English. Each translation is accompanied by feminine, masculine, and neutral variants for each possible gender interpretation. The dataset, which contains between 1250 and 1850 instances for each of the four language pairs, features natural sentences with a wide range of sentence lengths and domains, challenging translation rewriters on various linguistic phenomena. Additionally, we present an English gender rewriting solution built on GPT-3.5 Turbo and use GATE X-E to evaluate it. We open source our contributions to encourage further research on gender debiasing.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems"><a href="#A-search-algorithm-for-an-optimal-investment-problem-in-vehicle-sharing-systems" class="headerlink" title="A* search algorithm for an optimal investment problem in vehicle-sharing systems"></a>A* search algorithm for an optimal investment problem in vehicle-sharing systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08834">http://arxiv.org/abs/2311.08834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ba Luat Le, Layla Martin, Emrah Demir, Duc Minh Vu</li>
<li>for: 该研究探讨了一个优化投资问题，它在 Shared Vehicle System 中出现。给定一个站点建设集，我们需要确定（i）站点建设顺序和车辆数量，以达到所有站点建设完成的目标状态，（ii）在一些或所有站点打开时，最大化运营系统的总收益。</li>
<li>methods: 作者提出了一种 A* 搜索算法来解决这个问题，该算法可以视为一种 TSP 变种，具有集成依赖性的成本。</li>
<li>results: 计算实验表明，作者的提案算法在比较 Dijkstra 算法时具有明显的优势，并且将来的研究可以探讨新的可能性和应用。<details>
<summary>Abstract</summary>
We study an optimal investment problem that arises in the context of the vehicle-sharing system. Given a set of locations to build stations, we need to determine i) the sequence of stations to be built and the number of vehicles to acquire in order to obtain the target state where all stations are built, and ii) the number of vehicles to acquire and their allocation in order to maximize the total profit returned by operating the system when some or all stations are open. The profitability associated with operating open stations, measured over a specific time period, is represented as a linear optimization problem applied to a collection of open stations. With operating capital, the owner of the system can open new stations. This property introduces a set-dependent aspect to the duration required for opening a new station, and the optimal investment problem can be viewed as a variant of the Traveling Salesman Problem (TSP) with set-dependent cost. We propose an A* search algorithm to address this particular variant of the TSP. Computational experiments highlight the benefits of the proposed algorithm in comparison to the widely recognized Dijkstra algorithm and propose future research to explore new possibilities and applications for both exact and approximate A* algorithms.
</details>
<details>
<summary>摘要</summary>
我们研究一个最佳投资问题，它在车仲共享系统中发生。我们需要 Determine 以下两个问题：1. 建站的顺序和车辆数量，以实现所有站点都建立，并2. 车辆数量和分配方式，以最大化在一些或所有站点开放时的总收益。系统在运行时的收益，通过在一个特定时间间隔内进行线性优化问题，以表示开放的站点的盈利。系统所有者可以通过资金来开新站点。这个属性导致开新站点所需时间受到站点集的依赖，并且将最佳投资问题视为对特定设置成本的车辆销售人员问题的变形。我们提议使用A*搜索算法来解决这个问题。计算实验显示了我们的提案算法与通过世界上所认可的迪克斯特拉算法相比，具有更好的性能。我们未来的研究将探讨新的可能性和应用，以及精确和近似A*算法的应用。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration"><a href="#Exploring-Links-between-Conversational-Agent-Design-Challenges-and-Interdisciplinary-Collaboration" class="headerlink" title="Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration"></a>Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08832">http://arxiv.org/abs/2311.08832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Malak Sadek, Céline Mougenot</li>
<li>for: The paper is written to explore the socio-technical challenges of creating conversational agents (CA) and to propose practical strategies to overcome these challenges.</li>
<li>methods: The paper uses a scoping review of existing literature to identify and categorize the socio-technical challenges of CA design, and proposes a taxonomy of these challenges using interdisciplinary collaboration (IDC) as a lens.</li>
<li>results: The paper proposes practical strategies to overcome the socio-technical challenges of CA design, and invites future work to empirically verify the suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了探讨对话代理（CA）的社会技术创新挑战，并提出了解决这些挑战的实际策略。</li>
<li>methods: 这篇论文通过审视现有文献来标识和分类CA设计中的社会技术挑战，并提出了使用交叉学科协作（IDC）作为镜头的挑战分类法。</li>
<li>results: 这篇论文提出了解决CA设计中的社会技术挑战的实际策略，并邀请未来的研究 empirically verify提出的概念链和在CA设计空间中应用提出的策略以评估其效果。<details>
<summary>Abstract</summary>
Recent years have seen a steady rise in the popularity and use of Conversational Agents (CA) for different applications, well before the more immediate impact of large language models. This rise has been accompanied by an extensive exploration and documentation of the challenges of designing and creating conversational agents. Focusing on a recent scoping review of the socio-technical challenges of CA creation, this opinion paper calls for an examination of the extent to which interdisciplinary collaboration (IDC) challenges might contribute towards socio-technical CA design challenges. The paper proposes a taxonomy of CA design challenges using IDC as a lens, and proposes practical strategies to overcome them which complement existing design principles. The paper invites future work to empirically verify suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Conversational Agents" (CA) is translated as "对话代理人" (duìxiào dàibiǎn)* "Interdisciplinary collaboration" (IDC) is translated as "交叉学科合作" (jiāo kè xué kē hè zuò)* "Socio-technical challenges" is translated as "社会技术挑战" (shè huì jī shuō tā zhàn)* "Design principles" is translated as "设计原则" (xiè yì yuán xì)Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering"><a href="#Reinforcement-Learning-with-Model-Predictive-Control-for-Highway-Ramp-Metering" class="headerlink" title="Reinforcement Learning with Model Predictive Control for Highway Ramp Metering"></a>Reinforcement Learning with Model Predictive Control for Highway Ramp Metering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08820">http://arxiv.org/abs/2311.08820</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/filippoairaldi/mpcrl-for-ramp-metering">https://github.com/filippoairaldi/mpcrl-for-ramp-metering</a></li>
<li>paper_authors: Filippo Airaldi, Bart De Schutter, Azita Dabiri</li>
<li>for: 提高城市和高速公路交通系统的效率</li>
<li>methods: 结合模型驱动和学习驱动的方法，使用可靠学习控制方法来改进高速公路上的踏面控制</li>
<li>results: 实验结果显示，从一个不精准的模型和不佳地调整的控制器开始，提议的方法可以有效地学习改进控制策略，从而减少网络中的堵塞和满足约束，相比初始控制器表现更佳。<details>
<summary>Abstract</summary>
In the backdrop of an increasingly pressing need for effective urban and highway transportation systems, this work explores the synergy between model-based and learning-based strategies to enhance traffic flow management by use of an innovative approach to the problem of highway ramp metering control that embeds Reinforcement Learning techniques within the Model Predictive Control framework. The control problem is formulated as an RL task by crafting a suitable stage cost function that is representative of the traffic conditions, variability in the control action, and violations of a safety-critical constraint on the maximum number of vehicles in queue. An MPC-based RL approach, which merges the advantages of the two paradigms in order to overcome the shortcomings of each framework, is proposed to learn to efficiently control an on-ramp and to satisfy its constraints despite uncertainties in the system model and variable demands. Finally, simulations are performed on a benchmark from the literature consisting of a small-scale highway network. Results show that, starting from an MPC controller that has an imprecise model and is poorly tuned, the proposed methodology is able to effectively learn to improve the control policy such that congestion in the network is reduced and constraints are satisfied, yielding an improved performance compared to the initial controller.
</details>
<details>
<summary>摘要</summary>
在城市和高速公路交通系统的需求越来越高的背景下，这项工作探讨了模型基本和学习基本策略之间的共谊，以提高交通流控制的效果。该工作使用了一种嵌入了回归学习技术的模型预测控制框架来解决高速匝道流控制问题。通过设计一个合适的stage cost函数，该方法将交通条件、控制动作的变化和安全约束的最大车辆队列数量作为RL任务的stage cost函数。该方法将MPC和RL两种框架融合，以超越每个框架的缺点，并学习高速匝道控制，并满足系统模型不确定性和变化的需求。最后，对一个小规模高速公路网络的测试表明，从一个不精确的模型和优化不良的MPC控制器开始，该方法能够有效地学习改善控制策略，从而减少网络中的拥堵，满足约束，并提高效果相比于初始控制器。
</details></li>
</ul>
<hr>
<h2 id="Frequency-Domain-based-Dataset-Distillation"><a href="#Frequency-Domain-based-Dataset-Distillation" class="headerlink" title="Frequency Domain-based Dataset Distillation"></a>Frequency Domain-based Dataset Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08819">http://arxiv.org/abs/2311.08819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sdh0818/fred">https://github.com/sdh0818/fred</a></li>
<li>paper_authors: Donghyeok Shin, Seungjae Shin, Il-Chul Moon</li>
<li>for: 本研究旨在提出一种新的参数化方法，用于快速生成小型的合成数据集，从原始大型数据集中提取关键信息。</li>
<li>methods: 该方法基于频域的变换来优化数据集中每个实例的频率表示，通过选择特定频率维度进行优化，以实现快速生成实例的目标。</li>
<li>results: 对于不同的评价指标和数据集，FreD方法能够在有限的资源下实现更好的信息保留和性能提升，并且与现有方法兼容。<details>
<summary>Abstract</summary>
This paper presents FreD, a novel parameterization method for dataset distillation, which utilizes the frequency domain to distill a small-sized synthetic dataset from a large-sized original dataset. Unlike conventional approaches that focus on the spatial domain, FreD employs frequency-based transforms to optimize the frequency representations of each data instance. By leveraging the concentration of spatial domain information on specific frequency components, FreD intelligently selects a subset of frequency dimensions for optimization, leading to a significant reduction in the required budget for synthesizing an instance. Through the selection of frequency dimensions based on the explained variance, FreD demonstrates both theoretical and empirical evidence of its ability to operate efficiently within a limited budget, while better preserving the information of the original dataset compared to conventional parameterization methods. Furthermore, based on the orthogonal compatibility of FreD with existing methods, we confirm that FreD consistently improves the performances of existing distillation methods over the evaluation scenarios with different benchmark datasets. We release the code at https://github.com/sdh0818/FreD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy"><a href="#MAP’s-not-dead-yet-Uncovering-true-language-model-modes-by-conditioning-away-degeneracy" class="headerlink" title="MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy"></a>MAP’s not dead yet: Uncovering true language model modes by conditioning away degeneracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08817">http://arxiv.org/abs/2311.08817</a></li>
<li>repo_url: None</li>
<li>paper_authors: Davis Yoshida, Kartik Goyal, Kevin Gimpel</li>
<li>for: 这个论文主要研究了NLG模型中模式的问题，具体来说是解释为什么模式搜索（MAP解oding）常常导致输出异常（Stahlberg和Byrne，2019，Holtzman等，2019）。</li>
<li>methods: 作者使用了杂合搜索和模式搜索来研究NLG模型的输出。他们发现，即使模型没有错误，模式仍可以变得缺乏含义，这是因为训练数据中的噪声污染。为解决这问题，作者提议使用模式搜索 conditional on avoiding specific degeneracies。</li>
<li>results: 作者通过实验证明了，对机器翻译模型和语言模型进行长度 conditional 模式搜索可以获得更加流畅和话题性的输出。此外，作者还提供了许多模式序列的实际示例，并证明了LLaMA模型的模式仍然具有缺乏含义的问题。为了解决这问题，作者开发了一种approximate模式搜索方法，ACBS。通过应用这种方法，作者可以从LLaMA-7B模型中获得可接受的输出，而无需任何训练。<details>
<summary>Abstract</summary>
It has been widely observed that exact or approximate MAP (mode-seeking) decoding from natural language generation (NLG) models consistently leads to degenerate outputs (Stahlberg and Byrne, 2019, Holtzman et al., 2019). This has generally been attributed to either a fundamental inadequacy of modes in models or weaknesses in language modeling. Contrastingly in this work, we emphasize that degenerate modes can even occur in the absence of any model error, due to contamination of the training data. Specifically, we show that mixing even a tiny amount of low-entropy noise with a population text distribution can cause the data distribution's mode to become degenerate, implying that any models trained on it will be as well. As the unconditional mode of NLG models will often be degenerate, we therefore propose to apply MAP decoding to the model's distribution conditional on avoiding specific degeneracies. Using exact-search, we empirically verify that the length-conditional modes of machine translation models and language models are indeed more fluent and topical than their unconditional modes. For the first time, we also share many examples of exact modal sequences from these models, and from several variants of the LLaMA-7B model. Notably, the modes of the LLaMA models are still degenerate, showing that improvements in modeling have not fixed this issue. Because of the cost of exact mode finding algorithms, we develop an approximate mode finding approach, ACBS, which finds sequences that are both high-likelihood and high-quality. We apply this approach to LLaMA-7B, a model which was not trained for instruction following, and find that we are able to elicit reasonable outputs without any finetuning.
</details>
<details>
<summary>摘要</summary>
历史观察表明，使用自然语言生成（NLG）模型的准确或近似MAP（模式寻找）解oding会导致异常输出（Stahlberg和Byrne，2019，Holtzman等，2019）。这一问题通常被归结到模型中的缺陷或语言模型的弱点。然而，在本研究中，我们强调的是，即使模型没有错误，degenerate modes仍可能出现，这是因为训练数据被杂入了低 entropy 的噪音。我们证明，只要混合一点微的低 entropy 噪音到一个人类文本分布中，就可以让数据分布的模式变得异常。因此，我们建议在模型的分布上使用MAP decoding，并且条件于避免特定的异常模式。我们通过对机器翻译模型和语言模型的长度准确模式进行实验，证明了这些模式在fluency和topicality方面比unconditional modes更高。此外，我们还提供了许多exact模式序列的例子，包括several variants of the LLaMA-7B model。不幸的是，LLaMA模型的模式仍然异常，显示改进模型化没有解决这一问题。由于找到精确模式的算法成本高，我们开发了一种 Approximate CBS（ACBS）模式找到方法，可以找到高概率和高质量的序列。我们应用ACBS方法于LLaMA-7B模型，并发现可以获得无需较少的finetuning的合理输出。
</details></li>
</ul>
<hr>
<h2 id="Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations"><a href="#Self-Supervised-Disentanglement-by-Leveraging-Structure-in-Data-Augmentations" class="headerlink" title="Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations"></a>Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08815">http://arxiv.org/abs/2311.08815</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cian Eastwood, Julius von Kügelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Bernhard Schölkopf, Mark Ibrahim</li>
<li>for: 这个论文旨在推动自然语言处理领域中的自我超VI中的表示学习。</li>
<li>methods: 这篇论文使用了数据扩充来适应”风格”特征的变化，但是由于下游任务通常在训练时未知，因此难以在训练时确定”风格”特征是否可以安全地丢弃。为了解决这个问题，这篇论文提出了一种更原则的方法，即通过添加多个风格嵌入空间来分离风格特征。</li>
<li>results: 该方法在synthetic数据集上进行了实验，并且在ImageNet上进行了一些有限的实验，并证明了其效果。<details>
<summary>Abstract</summary>
Self-supervised representation learning often uses data augmentations to induce some invariance to "style" attributes of the data. However, with downstream tasks generally unknown at training time, it is difficult to deduce a priori which attributes of the data are indeed "style" and can be safely discarded. To address this, we introduce a more principled approach that seeks to disentangle style features rather than discard them. The key idea is to add multiple style embedding spaces where: (i) each is invariant to all-but-one augmentation; and (ii) joint entropy is maximized. We formalize our structured data-augmentation procedure from a causal latent-variable-model perspective, and prove identifiability of both content and (multiple blocks of) style variables. We empirically demonstrate the benefits of our approach on synthetic datasets and then present promising but limited results on ImageNet.
</details>
<details>
<summary>摘要</summary>
自我指导学习经常使用数据扩充来induce一些数据的"风格"特征的不变性。然而，下游任务通常不知道训练时间点，因此难以在训练时确定哪些特征是"风格"特征，可以安全地抛弃。为解决这个问题，我们介绍了一种更理智的方法，即通过分离风格特征来解决这个问题。我们的关键想法是在多个风格嵌入空间中添加多个不变性，即：(i) 每个不变性都是对所有扩充之外的一个不变性;(ii) 共同 entropy 的最大化。我们从 causal 潜在变量模型的视角来正式描述我们的结构化数据扩充过程，并证明内容和多个块风格变量的可识别性。我们在synthetic dataset上进行了实验，并在ImageNet上得到了有前途的 pero有限的结果。
</details></li>
</ul>
<hr>
<h2 id="SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer"><a href="#SparseSpikformer-A-Co-Design-Framework-for-Token-and-Weight-Pruning-in-Spiking-Transformer" class="headerlink" title="SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer"></a>SparseSpikformer: A Co-Design Framework for Token and Weight Pruning in Spiking Transformer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08806">http://arxiv.org/abs/2311.08806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Liu, Shanlin Xiao, Bo Li, Zhiyi Yu</li>
<li>for: 这个研究旨在提高Spikformer模型的效率和能效性，使其适合实现在边缘设备上。</li>
<li>methods: 这个研究使用了Lottery Ticket Hypothesis（LTH）和几个创新的token和重量调整技术来实现Spikformer模型的简洁化。</li>
<li>results: 实验结果显示，这个框架可以将Spikformer模型的90%模型参数简减，且可以降低Giga浮动点操作数（GFLOPs）20%，同时保持原始模型的准确性。<details>
<summary>Abstract</summary>
As the third-generation neural network, the Spiking Neural Network (SNN) has the advantages of low power consumption and high energy efficiency, making it suitable for implementation on edge devices. More recently, the most advanced SNN, Spikformer, combines the self-attention module from Transformer with SNN to achieve remarkable performance. However, it adopts larger channel dimensions in MLP layers, leading to an increased number of redundant model parameters. To effectively decrease the computational complexity and weight parameters of the model, we explore the Lottery Ticket Hypothesis (LTH) and discover a very sparse ($\ge$90%) subnetwork that achieves comparable performance to the original network. Furthermore, we also design a lightweight token selector module, which can remove unimportant background information from images based on the average spike firing rate of neurons, selecting only essential foreground image tokens to participate in attention calculation. Based on that, we present SparseSpikformer, a co-design framework aimed at achieving sparsity in Spikformer through token and weight pruning techniques. Experimental results demonstrate that our framework can significantly reduce 90% model parameters and cut down Giga Floating-Point Operations (GFLOPs) by 20% while maintaining the accuracy of the original model.
</details>
<details>
<summary>摘要</summary>
为了提高edge设备上的神经网络模型的能效性，我们提出了一种基于SNN的第三代神经网络模型，即SparseSpikformer。该模型通过减少神经网络的计算复杂性和参数量来提高实现效率。在这个模型中，我们采用了LTH Hypothesis，并在SNN中发现了一个大于90%的稀疏子网络，可以保持与原始网络相同的性能。此外，我们还设计了一个轻量级的图像选择器模块，可以根据神经元的射击率选择图像中的重要背景信息，从而降低计算复杂性。基于这些设计，我们提出了一种减少Spikformer模型计算复杂性的框架，并实现了减少90%的模型参数和20%的GFLOPs操作数量的目标。实验结果表明，我们的框架可以维持原始模型的准确性，同时实现效率的提高。
</details></li>
</ul>
<hr>
<h2 id="X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects"><a href="#X-Eval-Generalizable-Multi-aspect-Text-Evaluation-via-Augmented-Instruction-Tuning-with-Auxiliary-Evaluation-Aspects" class="headerlink" title="X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects"></a>X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08788">http://arxiv.org/abs/2311.08788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, Lifu Huang</li>
<li>for: 本文目的是提出一种多方面评估框架，以便评估自然语言生成（NLG）的多个方面质量。</li>
<li>methods: 本文使用了两个学习阶段：第一阶段是简单的指令调整阶段，旨在提高模型following指令的能力；第二阶段是加强的指令调整阶段，通过细致的评估方面之间的连接来更好地评估文本质量。</li>
<li>results: 经过广泛的实验，我们发现X-Eval可以让even a lightweight language model达到与人类评估相当或更高的相关性，比如GPT-4。<details>
<summary>Abstract</summary>
Natural Language Generation (NLG) typically involves evaluating the generated text in various aspects (e.g., consistency and naturalness) to obtain a comprehensive assessment. However, multi-aspect evaluation remains challenging as it may require the evaluator to generalize to any given evaluation aspect even if it's absent during training. In this paper, we introduce X-Eval, a two-stage instruction tuning framework to evaluate the text in both seen and unseen aspects customized by end users. X-Eval consists of two learning stages: the vanilla instruction tuning stage that improves the model's ability to follow evaluation instructions, and an enhanced instruction tuning stage that exploits the connections between fine-grained evaluation aspects to better assess text quality. To support the training of X-Eval, we collect AspectInstruct, the first instruction tuning dataset tailored for multi-aspect NLG evaluation spanning 27 diverse evaluation aspects with 65 tasks. To enhance task diversity, we devise an augmentation strategy that converts human rating annotations into diverse forms of NLG evaluation tasks, including scoring, comparison, ranking, and Boolean question answering. Extensive experiments across three essential categories of NLG tasks: dialogue generation, summarization, and data-to-text coupled with 21 aspects in meta-evaluation, demonstrate that our X-Eval enables even a lightweight language model to achieve a comparable if not higher correlation with human judgments compared to the state-of-the-art NLG evaluators, such as GPT-4.
</details>
<details>
<summary>摘要</summary>
自然语言生成（NLG）通常包括评估生成文本的多个方面（例如一致性和自然性）以获得全面的评估。然而，多方面评估仍然是挑战，因为评估人可能需要将注意力扩展到任何给定的评估方面，即使在训练过程中没有出现过。在这篇论文中，我们介绍了X-Eval，一个两个学习阶段的指令调整框架，用于评估文本在已知和未知方面的质量。X-Eval包括两个学习阶段：一个普通的指令调整阶段，用于提高模型能够遵循评估指令的能力，以及一个加强的指令调整阶段，用于更好地评估文本质量。为支持X-Eval的训练，我们收集了AspectInstruct数据集，这是第一个适用于多方面NLG评估的指令调整数据集，覆盖了27种多样化的评估方面，65个任务。为了增加任务多样性，我们设计了一种扩展策略，将人类评分笔记转换成多种NLG评估任务的不同形式，包括分数、对比、排名和布尔问答。在对对话生成、概要和数据到文本等三类NLG任务进行广泛的实验，我们发现X-Eval可以让even a lightweight语言模型与人类评估结果相似或更高相关性，比如GPT-4。
</details></li>
</ul>
<hr>
<h2 id="ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home"><a href="#ICRA-Roboethics-Challenge-2023-Intelligent-Disobedience-in-an-Elderly-Care-Home" class="headerlink" title="ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home"></a>ICRA Roboethics Challenge 2023: Intelligent Disobedience in an Elderly Care Home</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08783">http://arxiv.org/abs/2311.08783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sveta Paster, Kantwon Rogers, Gordon Briggs, Peter Stone, Reuth Mirsky</li>
<li>for: 这份报告是为了提高老人护理机构中的服务机器人增强老人的生活质量，以应对预计的老年人口增长。</li>
<li>methods: 该报告提议利用智能不遵守框架，让机器人能够进行有伦理意义的决策过程。</li>
<li>results: 该报告列出了智能不遵守框架可以帮助机器人解决的问题，并在特定的老人护理机构场景下定义了该框架的形式化定义，以及实现智能不遵守机器人的需求。<details>
<summary>Abstract</summary>
With the projected surge in the elderly population, service robots offer a promising avenue to enhance their well-being in elderly care homes. Such robots will encounter complex scenarios which will require them to perform decisions with ethical consequences. In this report, we propose to leverage the Intelligent Disobedience framework in order to give the robot the ability to perform a deliberation process over decisions with potential ethical implications. We list the issues that this framework can assist with, define it formally in the context of the specific elderly care home scenario, and delineate the requirements for implementing an intelligently disobeying robot. We conclude this report with some critical analysis and suggestions for future work.
</details>
<details>
<summary>摘要</summary>
随着老年人口增长的预计，服务机器人在老年人医疗机构中提供了一个有前途的解决方案，以提高老年人的生活质量。这些机器人会遇到复杂的情况，需要它们在具有伦理意义的决策时进行慎重的讨论。在这份报告中，我们提议利用智能不遵守框架，让机器人在具有伦理意义的决策时能够进行慎重的讨论。我们列出了这个框架可以帮助解决的问题，在老年人医疗机构特定场景中明确定义了它，并详细描述了实现智能不遵守机器人的需求。我们在报告结尾提出了一些批判性分析和未来工作的建议。
</details></li>
</ul>
<hr>
<h2 id="Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion"><a href="#Adversarially-Robust-Spiking-Neural-Networks-Through-Conversion" class="headerlink" title="Adversarially Robust Spiking Neural Networks Through Conversion"></a>Adversarially Robust Spiking Neural Networks Through Conversion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09266">http://arxiv.org/abs/2311.09266</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/igitugraz/robustsnnconversion">https://github.com/igitugraz/robustsnnconversion</a></li>
<li>paper_authors: Ozan Özdenizci, Robert Legenstein</li>
<li>for: 提高深度神经网络（SNN）的防御性能，增强SNN在应用中的可靠性。</li>
<li>methods: 提出了一种可扩展的Robust SNN培训方法，通过归一化层级触发阈值和synaptic连接权重来保持从预训练ANN中传递的robust性提升。</li>
<li>results: 实验结果表明，我们的方法可以在多种适应性攻击Setting下提供一个可扩展的、低延迟的防御性能。<details>
<summary>Abstract</summary>
Spiking neural networks (SNNs) provide an energy-efficient alternative to a variety of artificial neural network (ANN) based AI applications. As the progress in neuromorphic computing with SNNs expands their use in applications, the problem of adversarial robustness of SNNs becomes more pronounced. To the contrary of the widely explored end-to-end adversarial training based solutions, we address the limited progress in scalable robust SNN training methods by proposing an adversarially robust ANN-to-SNN conversion algorithm. Our method provides an efficient approach to embrace various computationally demanding robust learning objectives that have been proposed for ANNs. During a post-conversion robust finetuning phase, our method adversarially optimizes both layer-wise firing thresholds and synaptic connectivity weights of the SNN to maintain transferred robustness gains from the pre-trained ANN. We perform experimental evaluations in numerous adaptive adversarial settings that account for the spike-based operation dynamics of SNNs, and show that our approach yields a scalable state-of-the-art solution for adversarially robust deep SNNs with low-latency.
</details>
<details>
<summary>摘要</summary>
神经网络（SNN）提供了一种能效的人工神经网络（ANN）的替代方案，随着神经omorphic计算的进步，SNN在应用中的使用逐渐扩大。然而，SNN的敌意 robustness问题在这种扩展过程中变得更加突出。而不是已经广泛探索的终端对抗验证学习方法，我们提出了一种可扩展的Robust SNN Training方法。我们的方法可以有效地涵盖各种计算具有挑战性的Robust learning目标，这些目标在ANN中已经得到了广泛的探索。在post-conversionRobust fine-tuning阶段，我们的方法在SNN中对层wise发射阈值和 synaptic连接权重进行了对抗优化，以保持从pre-trained ANN中传递的Robustness收益。我们在许多适应性攻击设定下进行了实验评估，并证明了我们的方法可以实现可扩展的state-of-the-art解决方案，并且具有低延迟。
</details></li>
</ul>
<hr>
<h2 id="Three-Conjectures-on-Unexpectedeness"><a href="#Three-Conjectures-on-Unexpectedeness" class="headerlink" title="Three Conjectures on Unexpectedeness"></a>Three Conjectures on Unexpectedeness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08768">http://arxiv.org/abs/2311.08768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Giovanni Sileno, Jean-Louis Dessalles</li>
<li>for: This paper aims to lay the groundwork for a theoretical framework to explain the predictive power of unexpectedness in cognition, and to explore its connection to various measures of divergence between the entropy of the world and the variety of the observer.</li>
<li>methods: The paper uses a combination of theoretical conjectures and experimental results to develop a framework for understanding the role of unexpectedness in cognition.</li>
<li>results: The paper provides a new perspective on the relationship between unexpectedness and cognition, and suggests potential research directions that could lead to new insights into the extraction of causal relations and the role of descriptive mechanisms in learning.<details>
<summary>Abstract</summary>
Unexpectedness is a central concept in Simplicity Theory, a theory of cognition relating various inferential processes to the computation of Kolmogorov complexities, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, yet its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.
</details>
<details>
<summary>摘要</summary>
不期待性是简洁理论中的核心概念， relate to various inference processes and Kolmogorov complexities computation, rather than probabilities. Its predictive power has been confirmed by several experiments with human subjects, but its theoretical basis remains largely unexplored: why does it work? This paper lays the groundwork for three theoretical conjectures. First, unexpectedness can be seen as a generalization of Bayes' rule. Second, the frequentist core of unexpectedness can be connected to the function of tracking ergodic properties of the world. Third, unexpectedness can be seen as a constituent of various measures of divergence between the entropy of the world (environment) and the variety of the observer (system). The resulting framework hints to research directions that go beyond the division between probabilistic and logical approaches, potentially bringing new insights into the extraction of causal relations, and into the role of descriptive mechanisms in learning.Here's the translation breakdown:不期待性 (bù qīdài xìng) - unexpectedness简洁理论 (jiǎn jiǎn lǐlùn) - Simplicity Theoryrelate (tiě yǔ) - relatevarious inference processes (dào yī) - various inference processesKolmogorov complexities (kēlèmǔ gōngjì) - Kolmogorov complexitiescomputation (suānjiǔ) - computationrather than probabilities (bié kèqì) - rather than probabilitiesits predictive power (wǒ de yìjī) - its predictive powerhas been confirmed (yǐjī) - has been confirmedby several experiments (shíyī zhèng yǐjī) - by several experimentswith human subjects (rénshēng) - with human subjectsbut (but) - butits theoretical basis (wǒ de lǐyì) - its theoretical basisremains largely unexplored (yǐjī zhèngyǐ) - remains largely unexploredwhy does it work? (bù yīnwèi zhèngyǐ) - why does it work?This paper (zhèng zhì) - This paperlays the groundwork (dào zhì) - lays the groundworkfor three theoretical conjectures (sān lǐyì zhèng) - for three theoretical conjecturesFirst, (yī) - Firstunexpectedness (bù qīdài xìng) - unexpectednesscan be seen as (dào yī) - can be seen asa generalization (fāngyì) - a generalizationof Bayes' rule (Bayes de zhèng) - of Bayes' ruleSecond, (èr) - Secondthe frequentist core (liàng zhèng) - the frequentist coreof unexpectedness (bù qīdài xìng) - of unexpectednesscan be connected (dào yī) - can be connectedto the function (fāngyì) - to the functionof tracking (dào) - of trackingergodic properties (érguò) - ergodic propertiesof the world (shìjiè) - of the worldThird, (sān) - Thirdunexpectedness (bù qīdài xìng) - unexpectednesscan be seen as (dào yī) - can be seen asa constituent (fāngyì) - a constituentof various measures (biǎo) - of various measuresof divergence (fāngbiàn) - of divergencebetween (biān) - betweenthe entropy (hétuán) - the entropyof the world (shìjiè) - of the worldand (he) - andthe variety (dào) - the varietyof the observer (jìshì) - of the observerThe resulting framework (zhèng zhì) - The resulting frameworkhints (dào) - hintsto research directions (kēngsuǒ) - to research directionsthat go beyond (biào) - that go beyondthe division (biān) - the divisionbetween probabilistic (suǒyì) - between probabilisticand logical (lógí) - and logicalapproaches (jì) - approachespotentially bringing (dào) - potentially bringingnew insights (xīnwèi) - new insightsinto (yǐ) - intothe extraction (suō) - the extractionof causal relations (liǎo) - of causal relationsand (he) - andthe role (yè) - the roleof descriptive mechanisms (mǎojī) - of descriptive mechanismsin learning (xuéxí) - in learning.
</details></li>
</ul>
<hr>
<h2 id="Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning"><a href="#Combining-Past-Present-and-Future-A-Self-Supervised-Approach-for-Class-Incremental-Learning" class="headerlink" title="Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning"></a>Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08764">http://arxiv.org/abs/2311.08764</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoshuang Chen, Zhongyi Sun, Ke Yan, Shouhong Ding, Hongtao Lu</li>
<li>for: 本文目的是解决自适应学习中的 kontinuous novel class 问题，即模型能够识别新来的类，同时避免 catastrophic forgetting。</li>
<li>methods: 本文提出了一种自助学习 CIL 框架 CPPF，包括一个 prototype clustering module (PC)、一个 embedding space reserving module (ESR) 和一个 multi-teacher distillation module (MTD)。PC 和 ESR 模块在prototype level和feature level分别保留 embedding space  для后续阶段，而 MTD 模块保持当前阶段的表示不受过去知识的干扰。</li>
<li>results: 对 CIFAR100 和 ImageNet100 数据集进行了广泛的实验，显示了我们提出的方法可以提高自适应学习中的class incremental learning性能。<details>
<summary>Abstract</summary>
Class Incremental Learning (CIL) aims to handle the scenario where data of novel classes occur continuously and sequentially. The model should recognize the sequential novel classes while alleviating the catastrophic forgetting. In the self-supervised manner, it becomes more challenging to avoid the conflict between the feature embedding spaces of novel classes and old ones without any class labels. To address the problem, we propose a self-supervised CIL framework CPPF, meaning Combining Past, Present and Future. In detail, CPPF consists of a prototype clustering module (PC), an embedding space reserving module (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the ESR modules reserve embedding space for subsequent phases at the prototype level and the feature level respectively to prepare for knowledge learned in the future. 2) The MTD module maintains the representations of the current phase without the interference of past knowledge. One of the teacher networks retains the representations of the past phases, and the other teacher network distills relation information of the current phase to the student network. Extensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our proposed method boosts the performance of self-supervised class incremental learning. We will release code in the near future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese.<</SYS>>类增量学习（CIL）目标是处理连续出现的新类数据场景。模型应该识别连续出现的新类，同时避免catastrophic forgetting。在无监督的方式下，更加挑战是避免新类和旧类的feature embedding空间之间的冲突。为解决这个问题，我们提出了一个自动监督CIL框架CPPF，即Combining Past, Present and Future。在详细的实现方式下，CPPF包括一个原型聚合模块（PC）、一个嵌入空间保留模块（ESR）以及一个多教师浸泡模块（MTD）。1）PC和ESR模块在原型级和特征级分别保留了后续阶段的嵌入空间，以便在未来学习的知识。2）MTD模块保持了当前阶段的表示，并避免了过去知识的干扰。其中一个教师网络保持过去阶段的表示，另一个教师网络将当前阶段的关系信息传播给学生网络。我们在CIFAR100和ImageNet100数据集上进行了广泛的实验，结果表明我们提出的方法可以提高无监督类增量学习的性能。我们将即将发布代码。
</details></li>
</ul>
<hr>
<h2 id="Forms-of-Understanding-of-XAI-Explanations"><a href="#Forms-of-Understanding-of-XAI-Explanations" class="headerlink" title="Forms of Understanding of XAI-Explanations"></a>Forms of Understanding of XAI-Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08760">http://arxiv.org/abs/2311.08760</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hendrik Buschmeier, Heike M. Buhl, Friederike Kern, Angela Grimminger, Helen Beierling, Josephine Fisher, André Groß, Ilona Horwath, Nils Klowait, Stefan Lazarov, Michael Lenke, Vivien Lohmer, Katharina Rohlfing, Ingrid Scharlau, Amit Singh, Lutz Terfloth, Anna-Lisa Vollmer, Yu Wang, Annedore Wilmes, Britta Wrede</li>
<li>for: 本文旨在提供一种对Explainable Artificial Intelligence（XAI）领域和其他领域的理解模型，以及对理解的定义和形式、评估和动力的探讨。</li>
<li>methods: 本文采用了多学科的视角，包括计算机科学、语言学、社会学和心理学，对理解的定义和形式、评估和动力进行了探讨和系统化。</li>
<li>results: 本文提出了两种理解的形式，即启用性（knowing how）和理解（knowing that），并论证了这两种理解在解释过程中的发展和互相关系。 I hope this helps! Let me know if you have any further questions.<details>
<summary>Abstract</summary>
Explainability has become an important topic in computer science and artificial intelligence, leading to a subfield called Explainable Artificial Intelligence (XAI). The goal of providing or seeking explanations is to achieve (better) 'understanding' on the part of the explainee. However, what it means to 'understand' is still not clearly defined, and the concept itself is rarely the subject of scientific investigation. This conceptual article aims to present a model of forms of understanding in the context of XAI and beyond. From an interdisciplinary perspective bringing together computer science, linguistics, sociology, and psychology, a definition of understanding and its forms, assessment, and dynamics during the process of giving everyday explanations are explored. Two types of understanding are considered as possible outcomes of explanations, namely enabledness, 'knowing how' to do or decide something, and comprehension, 'knowing that' -- both in different degrees (from shallow to deep). Explanations regularly start with shallow understanding in a specific domain and can lead to deep comprehension and enabledness of the explanandum, which we see as a prerequisite for human users to gain agency. In this process, the increase of comprehension and enabledness are highly interdependent. Against the background of this systematization, special challenges of understanding in XAI are discussed.
</details>
<details>
<summary>摘要</summary>
<<SYS>>输入文本转换为简化中文。<</SYS>>Explainability 已成为计算机科学和人工智能中重要的话题，导致了一个子领域called Explainable Artificial Intelligence (XAI). 该领域的目标是提供或寻求解释，以达到更好的'理解'。然而，'理解'这个概念仍然没有得到清晰定义，而且这个概念自己也rarely是科学研究的对象。本文旨在提出一个形式理解在 XAI 和其他领域的模型。从计算机科学、语言学、社会学和心理学的多学科角度，一个理解的定义和其形式、评估和过程中的动态都是探讨的对象。在日常解释过程中，理解可以分为两种可能的结果，即'能力'和'认知'，两者都有不同的深度水平（从浅到深）。解释通常从特定领域的浅度理解开始，可以导致解释对象的深度认知和能力，这被视为人类用户获得行为能力的前提。在这个过程中，理解和能力之间存在很高的相互关系。在这个背景下，XAI 中特殊的理解挑战也是讨论的对象。
</details></li>
</ul>
<hr>
<h2 id="Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response"><a href="#Cross-domain-feature-disentanglement-for-interpretable-modeling-of-tumor-microenvironment-impact-on-drug-response" class="headerlink" title="Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response"></a>Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09264">http://arxiv.org/abs/2311.09264</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jia Zhai, Hui Liu</li>
<li>for: 本研究旨在模拟肿瘤微环境（TME）对药物响应的影响，以提高药物治疗的效果和特点。</li>
<li>methods: 本研究使用了适应域网络进行特征分离，将源领域（cell lines）和目标领域（肿瘤）的特征分离开来，并使用了 Graph Attention Network 学习药物的潜在表示。</li>
<li>results: 研究表明，适应域网络可以 superior performance 在预测药物响应和分解肿瘤微环境对药物效果的影响。<details>
<summary>Abstract</summary>
High-throughput screening technology has facilitated the generation of large-scale drug responses across hundreds of cancer cell lines. However, there exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because of tumors comprise of complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details>
<details>
<summary>摘要</summary>
高通量屏测技术已经促进了大规模药物响应的生成 across hundreds of cancer cell lines. 然而， exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because tumors comprise complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy.
</details></li>
</ul>
<hr>
<h2 id="Auto-ICL-In-Context-Learning-without-Human-Supervision"><a href="#Auto-ICL-In-Context-Learning-without-Human-Supervision" class="headerlink" title="Auto-ICL: In-Context Learning without Human Supervision"></a>Auto-ICL: In-Context Learning without Human Supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09263">http://arxiv.org/abs/2311.09263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ecielyang/auto-icl">https://github.com/ecielyang/auto-icl</a></li>
<li>paper_authors: Jinghan Yang, Shuming Ma, Furu Wei</li>
<li>for: 这个研究旨在提高人机交互的自然语言功能，使大语言模型在各种任务上具备更高的灵活性和自主性。</li>
<li>methods: 该研究提出了一种自动启发学习框架，可以让模型自动生成示例、标签、指导路径等，以便在不同任务上进行启发学习。</li>
<li>results: 研究表明，该方法在多种任务上能够实现优秀的表现，与现有方法相比，具有更高的灵活性和自主性。<details>
<summary>Abstract</summary>
In the era of Large Language Models (LLMs), human-computer interaction has evolved towards natural language, offering unprecedented flexibility. Despite this, LLMs are heavily reliant on well-structured prompts to function efficiently within the realm of In-Context Learning. Vanilla In-Context Learning relies on human-provided contexts, such as labeled examples, explicit instructions, or other guiding mechanisms that shape the model's outputs. To address this challenge, our study presents a universal framework named Automatic In-Context Learning. Upon receiving a user's request, we ask the model to independently generate examples, including labels, instructions, or reasoning pathways. The model then leverages this self-produced context to tackle the given problem. Our approach is universally adaptable and can be implemented in any setting where vanilla In-Context Learning is applicable. We demonstrate that our method yields strong performance across a range of tasks, standing up well when compared to existing methods.
</details>
<details>
<summary>摘要</summary>
（在大语言模型（LLM）时代，人机交互发展到自然语言水平，提供了前所未有的灵活性。然而，LLMs仍然受到良好结构化提示的限制，以便在受限的上下文学习中功能 efficiently。vanilla In-Context Learning rely on人类提供的上下文，如标注的例子、显式的指令或其他引导机制，以shape模型的输出。为解决这个挑战，我们的研究提出了一个通用框架 named Automatic In-Context Learning。当接收用户的请求时，我们会让模型独立生成示例，包括标签、指令或推理路径。然后，模型会利用自己生成的上下文来解决给定的问题。我们的方法是 universally adaptable，可以在任何可以使用vanilla In-Context Learning的场景中实现。我们示出了我们的方法在多种任务上具有强大表现，与现有方法相比，表现良好。）
</details></li>
</ul>
<hr>
<h2 id="Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values"><a href="#Disentangling-the-Potential-Impacts-of-Papers-into-Diffusion-Conformity-and-Contribution-Values" class="headerlink" title="Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values"></a>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09262">http://arxiv.org/abs/2311.09262</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhikai Xue, Guoxiu He, Zhuoren Jiang, Yangyang Kang, Star Zhao, Wei Lu</li>
<li>for: 这个论文的目的是计算学术论文的潜在影响力，并分解其为三个方面：散布、遵循和贡献。</li>
<li>methods: 该论文提出了一种基于图神经网络的新方法，称为DPPDCC，用于解决这些问题。DPPDCC使用动态不同类型的图 структуры，包括时间和结构特征，以捕捉知识的流动。具体来说，它使用比较和相关的信息来捕捉知识的流动，并使用约束来避免模型之间的混淆。</li>
<li>results: 实验结果表明，DPPDCC在不同时间点的论文上表现出色，与基线模型相比，它在新发表、新鲜出版和当下发表的论文上均有显著优势。此外，DPPDCC还能够robust地处理不同类型的论文和数据集。<details>
<summary>Abstract</summary>
The potential impact of an academic paper is determined by various factors, including its popularity and contribution. Existing models usually estimate original citation counts based on static graphs and fail to differentiate values from nuanced perspectives. In this study, we propose a novel graph neural network to Disentangle the Potential impacts of Papers into Diffusion, Conformity, and Contribution values (called DPPDCC). Given a target paper, DPPDCC encodes temporal and structural features within the constructed dynamic heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize the importance of comparative and co-cited/citing information between papers and aggregate snapshots evolutionarily. To unravel popularity, we contrast augmented graphs to extract the essence of diffusion and predict the accumulated citation binning to model conformity. We further apply orthogonal constraints to encourage distinct modeling of each perspective and preserve the inherent value of contribution. To evaluate models' generalization for papers published at various times, we reformulate the problem by partitioning data based on specific time points to mirror real-world conditions. Extensive experimental results on three datasets demonstrate that DPPDCC significantly outperforms baselines for previously, freshly, and immediately published papers. Further analyses confirm its robust capabilities. We will make our datasets and codes publicly available.
</details>
<details>
<summary>摘要</summary>
科学论文的潜在影响因多种因素决定，包括其受欢迎程度和贡献。现有模型通常基于静止图计算原始引用数，而不能区分不同的观点。在这项研究中，我们提出了一种新的图神经网络，即分离论文的潜在影响值（DPPDCC）。给定目标论文，DPPDCC 编码了时间和结构特征在构建的动态 hetэроogeneous图中。特别是，为了捕捉知识的流动，我们强调在比较和引用/引用信息之间的关系中捕捉知识的流动。为了评估媒体，我们对升级图进行比较，从而提取论文的核心特征。我们还应用正交约束，以便独特地模型每个角度，并保留论文的内在价值。为了评估模型在不同时间点发表的论文的普适性，我们将数据分 partitions  according to specific time points，以模拟实际情况。我们的实验结果表明，DPPDCC 在三个数据集上显著超过基线。进一步的分析证明它的稳定性。我们将数据和代码公开。
</details></li>
</ul>
<hr>
<h2 id="Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network"><a href="#Emerging-Drug-Interaction-Prediction-Enabled-by-Flow-based-Graph-Neural-Network-with-Biomedical-Network" class="headerlink" title="Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network"></a>Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09261">http://arxiv.org/abs/2311.09261</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lars-research/emergnn">https://github.com/lars-research/emergnn</a></li>
<li>paper_authors: Yongqi Zhang, Quanming Yao, Ling Yue, Xian Wu, Ziheng Zhang, Zhenxi Lin, Yefeng Zheng</li>
<li>for: 预测新药物与新药物之间的药物交互作用，以提高病人患病经验和药物开发效率。</li>
<li>methods: 使用图 нейрон网络（GNN）来预测新药物之间的交互作用，并利用生物医学网络中的资料来提高预测的准确性。</li>
<li>results: EmerGNN比现有方法更高的准确性来预测新药物之间的交互作用，并可以快速地确定最重要的生物医学概念。<details>
<summary>Abstract</summary>
Accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network.
</details>
<details>
<summary>摘要</summary>
通过计算方法精准预测新药 drug-drug interactions (DDI)，可以提高患者护理和药物开发效率。然而，许多现有的计算方法需要大量已知 DDI 信息，而这些信息对新药来说匮乏。在这篇文章中，我们提出 EmerGNN，一种基于图神经网络 (GNN) 的方法，可以有效预测新药之间的交互。EmerGNN 通过提取药物对之间的路径，传递药物之间的信息，并 incorporate 生物医学网络上相关的概念，来学习药物对之间的对应。不同的生物医学网络边缘权重，以指示目标 DDI 预测中的重要性。总的来说，EmerGNN 比现有方法更高精度地预测新药之间的交互，并可以 Identify 生物医学网络上最重要的信息。
</details></li>
</ul>
<hr>
<h2 id="Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning"><a href="#Joint-User-Pairing-and-Beamforming-Design-of-Multi-STAR-RISs-Aided-NOMA-in-the-Indoor-Environment-via-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning"></a>Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08708">http://arxiv.org/abs/2311.08708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Min Park, Yan Kyaw Tun, Choong Seon Hong<br>for:* 6G&#x2F;B5G wireless networks with enhanced quality requirementsmethods:* NOMA technique for multiple users to share resources* STAR-RISs for improved coverage, spectral efficiency, and reliabilityresults:* Joint user pairing and beamforming design for Multi-STAR-RISs in an indoor environment* Maximum total throughput of multiple users (MUs) through optimization of decoding order, user pairing, active beamforming, and passive beamformingPlease note that the above information is in Simplified Chinese text, as requested.<details>
<summary>Abstract</summary>
The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academia and industry. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the amplitude and phase shift corresponding to reflection and transmission, which makes the existing terrestrial networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To address this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems, namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity.
</details>
<details>
<summary>摘要</summary>
6G/B5G无线网络的开发，具有超过当前5G网络的需求，已经吸引了学术界和业界的关注。然而，使得6G/B5G网络质量提高的传统Cellsular网络，受到地面基站的限制，它们的空间和经济性不足。而NOMA技术允许多个用户共享同一资源，提高系统的spectral efficiency，并且可以支持更多的用户。此外，通过智能地控制反射和发射信号的相位和幅度，STAR-RISs可以实现改善的覆盖率、增加spectral efficiency和通信可靠性。然而，STAR-RISs需要同时优化反射和发射信号的相位和幅度，这使得现有的地面网络更加复杂，并被视为主要挑战。驱动了以上，我们研究了Multi-STAR-RISs在室内环境中的用户对称对接和束缚设计。然后，我们形ulated了优化问题的目标，即通过同时优化用户对称对接、束缚、活动束缚和空转束缚来提高多个用户机（MU）的总吞吐量。然而，该问题是一个MINLP问题。为了解决这个挑战，我们首先介绍了NOMA网络中的解码顺序。然后，我们将原问题分解成两个子问题，即：1）用户对称对接问题和2）束缚优化问题。对于第一个子问题，我们采用协方差基于K-means分 clustering算法来解决用户对称对接问题。然后，为了同时处理束缚向量优化问题，我们提议MAPPO，它可以在给定环境中做出快速决策，因为它的复杂度较低。
</details></li>
</ul>
<hr>
<h2 id="Aligned-A-Platform-based-Process-for-Alignment"><a href="#Aligned-A-Platform-based-Process-for-Alignment" class="headerlink" title="Aligned: A Platform-based Process for Alignment"></a>Aligned: A Platform-based Process for Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08706">http://arxiv.org/abs/2311.08706</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/klonnet23/helloy-word">https://github.com/klonnet23/helloy-word</a></li>
<li>paper_authors: Ethan Shaotran, Ido Pesok, Sam Jones, Emi Liu</li>
<li>for: 本研究旨在提供一个公信worthy、公开的方式来保障前沿模型的安全性，并最终实现超智能。</li>
<li>methods: 本研究使用了一个 constitutional committee 框架，Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support。</li>
<li>results: 研究显示了平台的自然扩展性，使得社区参与者具有更高的信任和满意度。<details>
<summary>Abstract</summary>
We are introducing Aligned, a platform for global governance and alignment of frontier models, and eventually superintelligence. While previous efforts at the major AI labs have attempted to gather inputs for alignment, these are often conducted behind closed doors. We aim to set the foundation for a more trustworthy, public-facing approach to safety: a constitutional committee framework. Initial tests with 680 participants result in a 30-guideline constitution with 93% overall support. We show the platform naturally scales, instilling confidence and enjoyment from the community. We invite other AI labs and teams to plug and play into the Aligned ecosystem.
</details>
<details>
<summary>摘要</summary>
我们是引入了对齐平台，用于全球治理和前沿模型的对齐，最终是超智能。在大型AI室中，过去的尝试都是在关闭的门后进行集Inputs for alignment，我们想要设立一个更加可靠、公共的安全方法：一个宪法委员会框架。我们的初步测试中，680名参与者共同制定了30个指南，得到了93%的总支持。我们表明该平台自然扩展，带来了社区的信任和愉悦。我们邀请其他AI室和团队加入对齐生态系统。
</details></li>
</ul>
<hr>
<h2 id="Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains"><a href="#Can-Large-Language-Models-Follow-Concept-Annotation-Guidelines-A-Case-Study-on-Scientific-and-Financial-Domains" class="headerlink" title="Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains"></a>Can Large Language Models Follow Concept Annotation Guidelines? A Case Study on Scientific and Financial Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08704">http://arxiv.org/abs/2311.08704</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marcio Fonseca, Shay B. Cohen</li>
<li>For: The paper aims to examine the capacity of instruction-tuned large language models (LLMs) to follow in-context concept guidelines for sentence labeling tasks.* Methods: The paper uses zero-shot sentence classification tasks with different types of factual and counterfactual concept definitions as prompts to test the models’ ability to recognize new concepts.* Results: The paper finds that only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts, and that proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines. Additionally, the paper finds that Falcon-180B-chat is outperformed by Llama-2-70B-chat in most cases, indicating that careful fine-tuning is more effective than increasing model scale.Here’s the simplified Chinese version of the three key points:* For: 论文目的是检验基于叙述示例的指导下，大型自然语言模型（LLMs）是否可以学习新的概念或事实。* Methods: 论文使用零shot句式分类任务，用不同类型的事实和反事实指导来测试模型的新概念认知能力。* Results: 论文发现，只有70B参数或更多的模型才能在对应的反事实上工作，而且专有API如GPT-3.5和GPT-4可以识别无意义的指导。此外，论文发现Falcon-180B-chat在大多数情况下被Llama-2-70B-chat所超越，这表明精心调整是更有效的than增加模型scale。<details>
<summary>Abstract</summary>
Although large language models (LLMs) exhibit remarkable capacity to leverage in-context demonstrations, it is still unclear to what extent they can learn new concepts or facts from ground-truth labels. To address this question, we examine the capacity of instruction-tuned LLMs to follow in-context concept guidelines for sentence labeling tasks. We design guidelines that present different types of factual and counterfactual concept definitions, which are used as prompts for zero-shot sentence classification tasks. Our results show that although concept definitions consistently help in task performance, only the larger models (with 70B parameters or more) have limited ability to work under counterfactual contexts. Importantly, only proprietary models such as GPT-3.5 and GPT-4 can recognize nonsensical guidelines, which we hypothesize is due to more sophisticated alignment methods. Finally, we find that Falcon-180B-chat is outperformed by Llama-2-70B-chat is most cases, which indicates that careful fine-tuning is more effective than increasing model scale. Altogether, our simple evaluation method reveals significant gaps in concept understanding between the most capable open-source language models and the leading proprietary APIs.
</details>
<details>
<summary>摘要</summary>
尽管大型语言模型（LLM）具有丰富的 Context 掌握能力，但是是否可以从真实标签中学习新的概念或事实仍然不清楚。为了回答这个问题，我们研究了基于示例示范的 instruction-tuned LLM 是否能够遵循 Context 中的概念指南进行句子标签任务。我们设计了不同类型的事实和反事实概念定义，用作零容量 sentence classification 任务的提示。我们的结果表明，虽然概念定义 invariably 提高任务性能，但只有70B参数或更多的大型模型可以在对应事实上下降性能。此外，我们发现仅有专有模型 such as GPT-3.5 和 GPT-4 可以识别不合理的指南，我们假设这是因为它们使用了更加复杂的对接方法。最后，我们发现 Falcon-180B-chat 通常被 Llama-2-70B-chat 超越，这表明精细的微调更加重要于提高模型规模。总之，我们的简单评估方法 revelas 最高水平的 open-source 语言模型和主流专有 API 之间的概念理解存在显著差距。
</details></li>
</ul>
<hr>
<h2 id="Debate-Helps-Supervise-Unreliable-Experts"><a href="#Debate-Helps-Supervise-Unreliable-Experts" class="headerlink" title="Debate Helps Supervise Unreliable Experts"></a>Debate Helps Supervise Unreliable Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08702">http://arxiv.org/abs/2311.08702</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/julianmichael/debate">https://github.com/julianmichael/debate</a></li>
<li>paper_authors: Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R. Bowman</li>
<li>for: supervising unreliable AI systems to give answers that are systematically true</li>
<li>methods: using debate between two unreliable experts to help a non-expert judge more reliably identify the truth</li>
<li>results: debate performs significantly better than consultancy (a baseline approach) and is more efficient, with 84% judge accuracy compared to 74% for consultancy<details>
<summary>Abstract</summary>
As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.
</details>
<details>
<summary>摘要</summary>
Traditional Chinese:随着人工智能系统用于回答更加困难的问题，评估其输出的真实性变得更加困难和更加重要。如何监督不可靠的专家，他们有存取真理，但可能不会正确地报告它们？在这个工作中，我们显示了对两名不可靠专家进行辩论可以帮助非专家评估者更加可靠地评估真理。我们收集了一个人类写作的辩论集，其中一名专家认为正确的答案，另一名专家认为 incorrect的答案。与基准我们称之为咨询（consultancy），单一的专家 argue for 正确的答案，其中正确的答案是半数的时间。我们发现，在我们的辩论中，辩论比咨询表现更好，评估者的准确率为84%，而咨询的准确率为74%。辩论也更有效率，长度只有68%。我们对人工和人类辩论进行比较，发现随着专家的技能提高，辩论的表现也提高，而咨询的表现则下降。我们的错误分析也支持这个趋势，发现人类辩论中的错误中46%是由诚实的辩论者所引起的（这些错误可以逐渐消失），而人类咨询中的错误中52%是由辩论者对评估者隐藏重要证据所致（这些错误可以加剧）。总的来说，这些结果显示了辩论是一种可靠地监督 increasingly capable 但可能不可靠的 AI 系统的方法。
</details></li>
</ul>
<hr>
<h2 id="Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception"><a href="#Artificial-General-Intelligence-Existential-Risk-and-Human-Risk-Perception" class="headerlink" title="Artificial General Intelligence, Existential Risk, and Human Risk Perception"></a>Artificial General Intelligence, Existential Risk, and Human Risk Perception</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08698">http://arxiv.org/abs/2311.08698</a></li>
<li>repo_url: None</li>
<li>paper_authors: David R. Mandel</li>
<li>for: 这篇论文关注人工智能（AGI）的可能性，特别是AGI在未来20年内可能超过人类智能水平，然后快速超越人类智能。</li>
<li>methods: 作者基于公开可用的预测和意见数据，研究了专家和非专家对AGI的风险认知。</li>
<li>results: 研究发现，对AGI的世界大悲害或灭绝风险的认知比其他潜在存在风险（如核战或人类引起的气候变化）高，过去一年内对AGI风险的认知增长也比其他风险更快。<details>
<summary>Abstract</summary>
Artificial general intelligence (AGI) does not yet exist, but given the pace of technological development in artificial intelligence, it is projected to reach human-level intelligence within roughly the next two decades. After that, many experts expect it to far surpass human intelligence and to do so rapidly. The prospect of superintelligent AGI poses an existential risk to humans because there is no reliable method for ensuring that AGI goals stay aligned with human goals. Drawing on publicly available forecaster and opinion data, the author examines how experts and non-experts perceive risk from AGI. The findings indicate that the perceived risk of a world catastrophe or extinction from AGI is greater than for other existential risks. The increase in perceived risk over the last year is also steeper for AGI than for other existential threats (e.g., nuclear war or human-caused climate change). That AGI is a pressing existential risk is something on which experts and non-experts agree, but the basis for such agreement currently remains obscure.
</details>
<details>
<summary>摘要</summary>
人工总智能（AGI）目前还没有存在，但根据技术发展的速度，预计在下一两十年内达到人类水平的智能。之后，许多专家预计它会迅速超越人类智能。超智AGI的出现对人类存在极大的风险，因为没有可靠的方法来保证AGI目标与人类目标相对应。作者通过公开available的预测和意见数据，检查专家和非专家对AGI风险的认知。结果表明AGI世界灾难或灭绝的风险高于其他极大风险（如核战或人类引起的气候变化）。过去一年内AGI风险的增加速度也比其他极大风险更大。虽然专家和非专家都认为AGI是一种极大的存在风险，但目前这种一致的基础还未明确。
</details></li>
</ul>
<hr>
<h2 id="An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping"><a href="#An-Eye-on-Clinical-BERT-Investigating-Language-Model-Generalization-for-Diabetic-Eye-Disease-Phenotyping" class="headerlink" title="An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"></a>An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08687">http://arxiv.org/abs/2311.08687</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kharrigian/ml4h-clinical-bert">https://github.com/kharrigian/ml4h-clinical-bert</a></li>
<li>paper_authors: Keith Harrigian, Tina Tang, Anthony Gonzales, Cindy X. Cai, Mark Dredze</li>
<li>for: 本研究旨在帮助监测 диабетиче眼病的临床趋势和检测护理不足，以预防盲视。</li>
<li>methods: 本研究使用了19种临床概念相关的文本提取系统，以检测和描述 диабетиче眼病的临床特征。</li>
<li>results: 研究发现，使用BERT语言模型预训练在非临床数据上的语言模型，对于本领域来说并无显著改进。<details>
<summary>Abstract</summary>
Diabetic eye disease is a major cause of blindness worldwide. The ability to monitor relevant clinical trajectories and detect lapses in care is critical to managing the disease and preventing blindness. Alas, much of the information necessary to support these goals is found only in the free text of the electronic medical record. To fill this information gap, we introduce a system for extracting evidence from clinical text of 19 clinical concepts related to diabetic eye disease and inferring relevant attributes for each. In developing this ophthalmology phenotyping system, we are also afforded a unique opportunity to evaluate the effectiveness of clinical language models at adapting to new clinical domains. Across multiple training paradigms, we find that BERT language models pretrained on out-of-distribution clinical data offer no significant improvement over BERT language models pretrained on non-clinical data for our domain. Our study tempers recent claims that language models pretrained on clinical data are necessary for clinical NLP tasks and highlights the importance of not treating clinical language data as a single homogeneous domain.
</details>
<details>
<summary>摘要</summary>
糖尿病眼病是全球最大的失明原因之一。监测相关的临床轨迹和检测护理缺失是控制疾病和避免失明的关键。然而，大量关键信息都藏在电子医疗记录中的自由文本中，使得管理疾病困难。为了填补这个信息差距，我们介绍了一种EXTRACTING EVIDENCE FROM CLINICAL TEXT OF 19 CLINICAL CONCEPTS RELATED TO DIABETIC EYE DISEASE AND INFERRING RELEVANT ATTRIBUTES FOR EACH。在开发这种眼科phenotyping系统时，我们也获得了评估临床语言模型在新临床领域中的适应性的机会。在多种训练方法中，我们发现BERT语言模型在非临床数据上进行预训练后对我们领域没有显著提高。我们的研究抑制了最近的宣称，即临床语言数据上的语言模型预训练是临床NLP任务中必不可少的。我们的研究也 highlights the importance of not treating clinical language data as a single homogeneous domain。
</details></li>
</ul>
<hr>
<h2 id="Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data"><a href="#Safer-Instruct-Aligning-Language-Models-with-Automated-Preference-Data" class="headerlink" title="Safer-Instruct: Aligning Language Models with Automated Preference Data"></a>Safer-Instruct: Aligning Language Models with Automated Preference Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08685">http://arxiv.org/abs/2311.08685</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uscnlp-lime/safer-instruct">https://github.com/uscnlp-lime/safer-instruct</a></li>
<li>paper_authors: Taiwei Shi, Kai Chen, Jieyu Zhao</li>
<li>for: 本研究旨在提高语言模型的安全性，通过人工审核和自动生成数据来提高模型的准确率和安全性。</li>
<li>methods: 本研究提出了一种新的数据生成管道，即Safer-Instruct，它使用倒转指令调整、指令生成和专家模型评估来生成高质量的偏好数据，不需要人工纠正。</li>
<li>results: 通过LLaMA进行指令生成和GPT-4作为专家模型，生成了约10K个偏好样本。通过训练一个Alpaca模型于此数据集，可以提高模型的安全性而不会影响其对话和下游任务的性能。Safer-Instruct解决了偏好数据获取的挑战，为更安全和责任的AI系统的发展提供了新的思路。<details>
<summary>Abstract</summary>
Reinforcement Learning from Human Feedback (RLHF) is a vital strategy for enhancing model safety in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for semi-automatically constructing large-scale preference datasets. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. We evaluate Safer-Instruct using LLaMA for instruction induction and GPT-4 as an expert model, generating approximately 10K preference samples. Finetuning an Alpaca model on this dataset demonstrates improved harmlessness while maintaining competitive performance on conversation and downstream tasks. Safer-Instruct addresses the challenges in preference data acquisition, advancing the development of safer and more responsible AI systems. Our code and data are available at https://github.com/uscnlp-lime/safer-instruct
</details>
<details>
<summary>摘要</summary>
� Reinforcement Learning from Human Feedback (RLHF) 是一种重要的策略来提高语言模型的安全性。然而，为RLHF annotating偏好数据是一个资源密集且创作需求高的过程，而自动生成方法受到数据多样性和质量的限制。为此，我们提出了Safer-Instruct，一个新的管线来半自动建构大规模的偏好数据。我们的方法利用倒转指令调整、指令生成和专家模型评估，以生成高品质的偏好数据，不需要人工标注员。我们使用LLaMA进行指令生成和GPT-4作为专家模型，生成约10K偏好数据。给Alpaca模型进行调整后，示出改善了无害性，同时保持了与对话和下游任务的竞争性能。Safer-Instruct解决了偏好数据取得的挑战，推动了更安全和责任的AI系统的开发。我们的代码和数据可以在https://github.com/uscnlp-lime/safer-instruct上取得。
</details></li>
</ul>
<hr>
<h2 id="Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets"><a href="#Multi-Set-Inoculation-Assessing-Model-Robustness-Across-Multiple-Challenge-Sets" class="headerlink" title="Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets"></a>Multi-Set Inoculation: Assessing Model Robustness Across Multiple Challenge Sets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08662">http://arxiv.org/abs/2311.08662</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vatsal Gupta, Pranshu Pandya, Tushar Kataria, Vivek Gupta, Dan Roth</li>
<li>for: 这个研究旨在理解语言模型对输入异常的敏感性，以增强模型的可信度。</li>
<li>methods: 研究使用了精细调教和多个干扰的训练策略，以及一种链式思维（COT）示例来提高模型的多干扰Robustness。</li>
<li>results: 研究显示，使用提议的方法可以训练模型对不同干扰的Robustness，而不会影响模型在给定任务上的准确率。<details>
<summary>Abstract</summary>
Language models, given their black-box nature, often exhibit sensitivity to input perturbations, leading to trust issues due to hallucinations. To bolster trust, it's essential to understand these models' failure modes and devise strategies to enhance their performance. In this study, we propose a framework to study the effect of input perturbations on language models of different scales, from pre-trained models to large language models (LLMs). We use fine-tuning to train a robust model to perturbations, and we investigate whether exposure to one perturbation improves or degrades the model's performance on other perturbations. To address multi-perturbation robustness, we suggest three distinct training strategies. We also extend the framework to LLMs via a chain of thought(COT) prompting with exemplars. We instantiate our framework for the Tabular-NLI task and show that the proposed strategies train the model robust to different perturbations without losing accuracy on a given dataset.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本模型，由于其黑盒特性，经常表现出输入杂乱的敏感性，导致不信任问题由于幻觉。为了增强不信任，我们需要理解这些模型的失败模式，并设计策略来提高其性能。在这项研究中，我们提议一个框架来研究输入杂乱对不同规模的语言模型（从预训练模型到大语言模型）的影响。我们使用精度训练来适应杂乱，并研究曝光一种杂乱后，模型对其他杂乱的性能是否改善或恶化。为了解决多种杂乱的可靠性，我们提出三种不同的训练策略。此外，我们将框架扩展到大语言模型（LLMs）via一种链式思维（COT）提示法，并通过例子来实现。我们在Tabular-NLI任务上实现了我们的框架，并示出了提议的策略可以在给定数据集上训练模型抗杂乱而不失去精度。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing"><a href="#Autonomous-Large-Language-Model-Agents-Enabling-Intent-Driven-Mobile-GUI-Testing" class="headerlink" title="Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing"></a>Autonomous Large Language Model Agents Enabling Intent-Driven Mobile GUI Testing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08649">http://arxiv.org/abs/2311.08649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juyeon Yoon, Robert Feldt, Shin Yoo</li>
<li>for:  automatize GUI testing of Android apps, to increase testing efficiency and coverage</li>
<li>methods:  uses Large Language Models and support mechanisms such as long- and short-term memory to set relevant task goals and perform realistic tasks</li>
<li>results:  achieved 61% activity coverage and 317 out of 374 autonomously created tasks are realistic and relevant to app functionalities, outperforming current state-of-the-art GUI testing techniques.<details>
<summary>Abstract</summary>
GUI testing checks if a software system behaves as expected when users interact with its graphical interface, e.g., testing specific functionality or validating relevant use case scenarios. Currently, deciding what to test at this high level is a manual task since automated GUI testing tools target lower level adequacy metrics such as structural code coverage or activity coverage. We propose DroidAgent, an autonomous GUI testing agent for Android, for semantic, intent-driven automation of GUI testing. It is based on Large Language Models and support mechanisms such as long- and short-term memory. Given an Android app, DroidAgent sets relevant task goals and subsequently tries to achieve them by interacting with the app. Our empirical evaluation of DroidAgent using 15 apps from the Themis benchmark shows that it can set up and perform realistic tasks, with a higher level of autonomy. For example, when testing a messaging app, DroidAgent created a second account and added a first account as a friend, testing a realistic use case, without human intervention. On average, DroidAgent achieved 61% activity coverage, compared to 51% for current state-of-the-art GUI testing techniques. Further, manual analysis shows that 317 out of the 374 autonomously created tasks are realistic and relevant to app functionalities, and also that DroidAgent interacts deeply with the apps and covers more features.
</details>
<details>
<summary>摘要</summary>
GUI 测试检查软件系统在用户与图形界面交互时是否按预期的行为，例如测试特定功能或验证相关用例enario。目前，决定要测试的高级水平是一个手动任务，因为自动化GUI测试工具通常target lower level的充分度度量 such as 结构代码覆盖率或活动覆盖率。我们提出了DroidAgent，一个基于大型自然语言模型和支持机制such as长期和短期记忆的Android GUI测试自动化工具。给一个Android应用程序，DroidAgent会设定相关的任务目标，然后通过与应用程序交互来实现这些目标。我们对DroidAgent使用Themis测试套件中的15个应用程序进行了实验性评估，结果显示DroidAgent可以自动设置和执行真实的任务，并且比现有的GUI测试技术高一个等级。例如，当测试一个消息应用程序时，DroidAgent创建了一个第二个帐户，并将第一个帐户添加为好友，测试了一个真实的用例，没有人工干预。在average，DroidAgent achieve 61%的活动覆盖率，比现有技术的51%高。此外，手动分析结果显示DroidAgent自动创建的374个任务中，317个任务是真实有用和相关于应用程序功能，而且DroidAgent会深入与应用程序交互，覆盖更多的功能。
</details></li>
</ul>
<hr>
<h2 id="Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification"><a href="#Explore-Spurious-Correlations-at-the-Concept-Level-in-Language-Models-for-Text-Classification" class="headerlink" title="Explore Spurious Correlations at the Concept Level in Language Models for Text Classification"></a>Explore Spurious Correlations at the Concept Level in Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08648">http://arxiv.org/abs/2311.08648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang</li>
<li>for: 本研究旨在探讨语言模型（LM）在不同语言处理任务中的表现，以及如何减少LM因杂质相关性而导致的Robustness问题。</li>
<li>methods: 本研究使用语言模型（LM）对文本进行标签，并测试LM在不同文本分类任务中的表现。同时，我们还提出了一种数据重新平衡方法，通过添加LM生成的对反数据来减少杂质相关性。</li>
<li>results: 研究结果表明，存在多个概念的标签分布偏误在多个文本分类数据集中，LM会利用这些偏误来进行预测，而我们的减少方法可以有效地减少这些偏误。<details>
<summary>Abstract</summary>
Language models (LMs) have gained great achievement in various NLP tasks for both fine-tuning and in-context learning (ICL) methods. Despite its outstanding performance, evidence shows that spurious correlations caused by imbalanced label distributions in training data (or exemplars in ICL) lead to robustness issues. However, previous studies mostly focus on word- and phrase-level features and fail to tackle it from the concept level, partly due to the lack of concept labels and subtle and diverse expressions of concepts in text. In this paper, we first use the LLM to label the concept for each text and then measure the concept bias of models for fine-tuning or ICL on the test data. Second, we propose a data rebalancing method to mitigate the spurious correlations by adding the LLM-generated counterfactual data to make a balanced label distribution for each concept. We verify the effectiveness of our mitigation method and show its superiority over the token removal method. Overall, our results show that there exist label distribution biases in concepts across multiple text classification datasets, and LMs will utilize these shortcuts to make predictions in both fine-tuning and ICL methods.
</details>
<details>
<summary>摘要</summary>
语言模型（LM）在各种自然语言处理（NLP）任务中已经取得了很大的成就，包括精度训练和上下文学习（ICL）方法。尽管它们的表现很出色，但证据表明，由于训练数据中标签的不均匀分布而导致的偏见问题。然而，前一些研究主要集中在单词和短语水平的特征上，忽略了概念水平的问题，其中一个原因是缺乏概念标签，以及文本中概念的柔和和多样化表达。在本文中，我们首先使用LM来标注每个文本中的概念，然后测量模型在测试数据上的概念偏见。其次，我们提出了一种数据重新补做方法，以避免由于标签分布的偏见问题。我们证明了我们的mitigation方法的有效性，并证明它在和token移除方法相比而言更有优势。总之，我们的结果表明，存在多个文本分类 datasets中的概念标签偏见，LM在精度训练和ICL方法中都会利用这些短cuts来做预测。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations"><a href="#Interpretable-by-Design-Wrapper-Boxes-Combine-Neural-Performance-with-Faithful-Explanations" class="headerlink" title="Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations"></a>Interpretable by Design: Wrapper Boxes Combine Neural Performance with Faithful Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08644">http://arxiv.org/abs/2311.08644</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiheng Su, Juni Jessy Li, Matthew Lease</li>
<li>for: 能够保持神经网络模型的准确性while提供 faithful的解释吗？我们提出了“ wrapper boxes”，一种通用的方法来生成 faithful， example-based解释 для模型预测结果，同时保持预测性能。</li>
<li>methods: 我们首先训练了一个神经网络模型，然后将其学习的特征表示输入到一个可解释的模型中进行实际预测。这种简单的策略 surprisingly effective，results largely comparable to those of the original neural model， как shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics。</li>
<li>results: 此外，因为这些可解释模型是设计为可解释的，所以可以直接向用户显示trainig example subset That determine classic model predictions。<details>
<summary>Abstract</summary>
Can we preserve the accuracy of neural models while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details>
<details>
<summary>摘要</summary>
可以保持神经网络模型的准确性 while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.Here's the translation in Traditional Chinese:可以保持神经网络模型的准确性 while also providing faithful explanations? We present wrapper boxes, a general approach to generate faithful, example-based explanations for model predictions while maintaining predictive performance. After training a neural model as usual, its learned feature representation is input to a classic, interpretable model to perform the actual prediction. This simple strategy is surprisingly effective, with results largely comparable to those of the original neural model, as shown across three large pre-trained language models, two datasets of varying scale, four classic models, and four evaluation metrics. Moreover, because these classic models are interpretable by design, the subset of training examples that determine classic model predictions can be shown directly to users.
</details></li>
</ul>
<hr>
<h2 id="Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction"><a href="#Spatio-Temporal-Graph-Neural-Point-Process-for-Traffic-Congestion-Event-Prediction" class="headerlink" title="Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction"></a>Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08635">http://arxiv.org/abs/2311.08635</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangyin Jin, Lingbo Liu, Fuxian Li, Jincai Huang</li>
<li>for: 预测交通堵塞事件，以提高智能交通系统的效能。</li>
<li>methods: 我们提出了一种基于图 neural point process 框架的 spatial-temporal graph neural network，可以充分捕捉历史交通状态数据中的长距离空间-时间依赖关系，同时还可以模型堵塞事件的发展趋势。</li>
<li>results: 我们的方法在两个实际数据集上进行了广泛的实验，并证明了与现有状态艺术方法相比，其性能更高。<details>
<summary>Abstract</summary>
Traffic congestion event prediction is an important yet challenging task in intelligent transportation systems. Many existing works about traffic prediction integrate various temporal encoders and graph convolution networks (GCNs), called spatio-temporal graph-based neural networks, which focus on predicting dense variables such as flow, speed and demand in time snapshots, but they can hardly forecast the traffic congestion events that are sparsely distributed on the continuous time axis. In recent years, neural point process (NPP) has emerged as an appropriate framework for event prediction in continuous time scenarios. However, most conventional works about NPP cannot model the complex spatio-temporal dependencies and congestion evolution patterns. To address these limitations, we propose a spatio-temporal graph neural point process framework, named STGNPP for traffic congestion event prediction. Specifically, we first design the spatio-temporal graph learning module to fully capture the long-range spatio-temporal dependencies from the historical traffic state data along with the road network. The extracted spatio-temporal hidden representation and congestion event information are then fed into a continuous gated recurrent unit to model the congestion evolution patterns. In particular, to fully exploit the periodic information, we also improve the intensity function calculation of the point process with a periodic gated mechanism. Finally, our model simultaneously predicts the occurrence time and duration of the next congestion. Extensive experiments on two real-world datasets demonstrate that our method achieves superior performance in comparison to existing state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
traffic 堵塞事件预测是智能交通系统中的一个重要 yet 挑战性任务。许多现有的交通预测方法 integrates 多种 temporal 编码器和图像 convolution 网络（GCNs），称为 spatio-temporal 图像-based 神经网络，它们主要 focus 在 predicting 稠密变量 such as flow, speed 和 demand 在时刻戳中，但它们很难预测分布在继续时间轴上的交通堵塞事件。在过去几年，神经点过程（NPP）已经 emerge 为继续时间场景中的适用性Frameworks。然而，大多数传统的 NPP 方法无法模型 complex spatio-temporal 依赖关系和堵塞演化模式。为了解决这些局限性，我们提出了一种 spatio-temporal 图像神经点过程框架，名为 STGNPP  для交通堵塞事件预测。具体来说，我们首先设计了 spatio-temporal 图像学习模块，以全面捕捉历史交通状态数据中的长距离 spatio-temporal 依赖关系，同时与道路网络相结合。提取的 spatio-temporal 隐藏表示和堵塞事件信息然后被 fed 到一个连续闭合回归单元，以模型堵塞演化模式。特别是，为了充分利用周期信息，我们还改进了点过程中的 Intensity 函数计算方法。最后，我们的模型同时预测下一次堵塞事件的发生时间和持续时间。广泛的实验表明，我们的方法在两个真实世界数据集上表现出优于现有的状态前方法。
</details></li>
</ul>
<hr>
<h2 id="XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making"><a href="#XplainLLM-A-QA-Explanation-Dataset-for-Understanding-LLM-Decision-Making" class="headerlink" title="XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making"></a>XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08614">http://arxiv.org/abs/2311.08614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra</li>
<li>for: 本研究旨在提高大型自然语言处理模型（LLM）的决策过程的可见性，通过创建一个新的问答解释数据集（QAE），integrating知识图（KG）。</li>
<li>methods: 我们使用了知识图和图注意网络（GAT）来找到reason-elements，并将其转化为可理解的why-choose和why-not-choose解释。</li>
<li>results: 我们通过量化和质量评价表明，我们的数据集可以提高LLM在上下文学习中的性能，提高其解释性和可见性，使其更加可靠和可信worthy。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a why-choose explanation, a why-not-choose explanation, and a set of reason-elements that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the reason-elements and transform them into why-choose and why-not-choose explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: https://github.com/chen-zichen/XplainLLM_dataset.git
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures"><a href="#Navigating-the-Ocean-of-Biases-Political-Bias-Attribution-in-Language-Models-via-Causal-Structures" class="headerlink" title="Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures"></a>Navigating the Ocean of Biases: Political Bias Attribution in Language Models via Causal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08605">http://arxiv.org/abs/2311.08605</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/david-jenny/llm-political-study">https://github.com/david-jenny/llm-political-study</a></li>
<li>paper_authors: David F. Jenny, Yann Billeter, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</li>
<li>for: 本研究旨在探讨 Large Language Models (LLMs) 在政治辩论中的决策过程和内在偏见。</li>
<li>methods: 本研究使用 Activity Dependency Networks (ADNs) 抽取 LLMs 中的隐式评价标准，并 illustrate how normative values influence these perceptions。</li>
<li>results: 研究发现 LLMs 在评价 “好Arguments” 时存在偏见，并且这些偏见受到了 normative values 的影响。这些结果有关于人机同步和偏见减少的影响。<details>
<summary>Abstract</summary>
The rapid advancement of Large Language Models (LLMs) has sparked intense debate regarding their ability to perceive and interpret complex socio-political landscapes. In this study, we undertake an exploration of decision-making processes and inherent biases within LLMs, exemplified by ChatGPT, specifically contextualizing our analysis within political debates. We aim not to critique or validate LLMs' values, but rather to discern how they interpret and adjudicate "good arguments." By applying Activity Dependency Networks (ADNs), we extract the LLMs' implicit criteria for such assessments and illustrate how normative values influence these perceptions. We discuss the consequences of our findings for human-AI alignment and bias mitigation. Our code and data at https://github.com/david-jenny/LLM-Political-Study.
</details>
<details>
<summary>摘要</summary>
LLMs 的快速发展已经引发了对其能够理解和解释复杂社会政治景观的激烈讨论。在这个研究中，我们进行了 LLMS 决策过程和内在偏见的探索，以 chatGPT 为例，并在政治辩论中进行了Contextual化分析。我们的目标不是评价或验证 LLMS 的价值观，而是理解它们如何解读和评价 "好的论点"。通过应用 Activity Dependency Networks (ADNs)，我们提取了 LLMS 的隐藏标准 для这些评价，并示出了如何 normative 价值影响这些见解。我们讨论了我们发现的后果，以及如何实现人机同步和偏见缓减。我们的代码和数据可以在 <https://github.com/david-jenny/LLM-Political-Study> 找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.AI_2023_11_15/" data-id="clp89do9j0077i7889gvn5085" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_15" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/15/cs.CL_2023_11_15/" class="article-date">
  <time datetime="2023-11-15T11:00:00.000Z" itemprop="datePublished">2023-11-15</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/15/cs.CL_2023_11_15/">cs.CL - 2023-11-15</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries"><a href="#Lexical-Repetitions-Lead-to-Rote-Learning-Unveiling-the-Impact-of-Lexical-Overlap-in-Train-and-Test-Reference-Summaries" class="headerlink" title="Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries"></a>Lexical Repetitions Lead to Rote Learning: Unveiling the Impact of Lexical Overlap in Train and Test Reference Summaries</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09458">http://arxiv.org/abs/2311.09458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Prafulla Kumar Choubey, Alexander R. Fabbri, Caiming Xiong, Chien-Sheng Wu<br>for:* The paper is written to propose a fine-grained evaluation protocol for summarization models to determine their competencies in generalizing to novel summary-worthy content.methods:* The authors use a test set partitioned based on the lexical similarity of reference test summaries with training summaries to evaluate the model’s performance.* They observe a significant difference in ROUGE-2 and entity recall scores between the subsets with the lowest and highest similarity.results:* The authors show that limiting lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages can improve the model’s performance on novel test cases while retaining average performance.* Their automatic and human evaluations on novel test subsets and recent news articles demonstrate that limiting lexical repetitions can prevent rote learning and improve generalization.<details>
<summary>Abstract</summary>
Ideal summarization models should generalize to novel summary-worthy content without remembering reference training summaries by rote. However, a single average performance score on the entire test set is inadequate in determining such model competencies. We propose a fine-grained evaluation protocol by partitioning a test set based on the lexical similarity of reference test summaries with training summaries. We observe up to a 5x (1.2x) difference in ROUGE-2 (entity recall) scores between the subsets with the lowest and highest similarity. Next, we show that such training repetitions also make a model vulnerable to rote learning, reproducing data artifacts such as factual errors, especially when reference test summaries are lexically close to training summaries. Consequently, we propose to limit lexical repetitions in training summaries during both supervised fine-tuning and likelihood calibration stages to improve the performance on novel test cases while retaining average performance. Our automatic and human evaluations on novel test subsets and recent news articles show that limiting lexical repetitions in training summaries can prevent rote learning and improve generalization.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset"><a href="#Subtle-Misogyny-Detection-and-Mitigation-An-Expert-Annotated-Dataset" class="headerlink" title="Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset"></a>Subtle Misogyny Detection and Mitigation: An Expert-Annotated Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09443">http://arxiv.org/abs/2311.09443</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brooklyn Sheppard, Anna Richter, Allison Cohen, Elizabeth Allyn Smith, Tamara Kneese, Carolyne Pelletier, Ioana Baldini, Yue Dong</li>
<li>for: 本研究用于开发一个新的 dataset，捕捉女性偏见的细节和复杂性。</li>
<li>methods: 该 dataset 使用多学科专家和注释器共同建构，包括电影字幕注释，捕捉北美电影中的日常性偏见表达。</li>
<li>results: 该研究提供了偏见检测和改进的基准值，并分析了获得的注释。 hope 该工作能够推动 AI 为社会好用的 NLP 技术发展。<details>
<summary>Abstract</summary>
Using novel approaches to dataset development, the Biasly dataset captures the nuance and subtlety of misogyny in ways that are unique within the literature. Built in collaboration with multi-disciplinary experts and annotators themselves, the dataset contains annotations of movie subtitles, capturing colloquial expressions of misogyny in North American film. The dataset can be used for a range of NLP tasks, including classification, severity score regression, and text generation for rewrites. In this paper, we discuss the methodology used, analyze the annotations obtained, and provide baselines using common NLP algorithms in the context of misogyny detection and mitigation. We hope this work will promote AI for social good in NLP for bias detection, explanation, and removal.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:使用创新的数据集开发方法，Biasly数据集 capture了偏见的细节和细腻性，在文献中具有独特的表现。与多种学科专家和批注人员合作建立的数据集包含电影字幕拼音，捕捉了北美电影中的日常性偏见。该数据集可以用于多种NLP任务，包括分类、偏见度评分和文本生成重写。在这篇论文中，我们介绍了使用的方法、分析获得的拼音和使用常见NLP算法进行偏见检测和修正的基线。我们希望这项工作能够促进NLP领域的AI为社会好。
</details></li>
</ul>
<hr>
<h2 id="Labeled-Interactive-Topic-Models"><a href="#Labeled-Interactive-Topic-Models" class="headerlink" title="Labeled Interactive Topic Models"></a>Labeled Interactive Topic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09438">http://arxiv.org/abs/2311.09438</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Kyle Seelman, Mozhi Zhang, Jordan Boyd-Graber</li>
<li>for: 用于改善 neural topic model 中的主题选择</li>
<li>methods: 使用用户标签来修改主题，以更好地满足用户的信息需求</li>
<li>results: 通过人工研究，发现用户标签可以提高文档排名分数，从而更好地找到与查询有关的文档<details>
<summary>Abstract</summary>
Topic models help users understand large document collections; however, topic models do not always find the ``right'' topics. While classical probabilistic and anchor-based topic models have interactive variants to guide models toward better topics, such interactions are not available for neural topic models such as the embedded topic model (\abr{etm}). We correct this lacuna by adding an intuitive interaction to neural topic models: users can label a topic with a word, and topics are updated so that the topic words are close to the label. This allows a user to refine topics based on their information need. While, interactivity is intuitive for \abr{etm}, we extend this framework to work with other neural topic models as well. We develop an interactive interface which allows users to interact and relabel topic models as they see fit. We evaluate our method through a human study, where users can relabel topics to find relevant documents. Using our method, user labeling improves document rank scores, helping to find more relevant documents to a given query when compared to no user labeling.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers"><a href="#Striped-Attention-Faster-Ring-Attention-for-Causal-Transformers" class="headerlink" title="Striped Attention: Faster Ring Attention for Causal Transformers"></a>Striped Attention: Faster Ring Attention for Causal Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09431">http://arxiv.org/abs/2311.09431</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/exists-forall/striped_attention">https://github.com/exists-forall/striped_attention</a></li>
<li>paper_authors: William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, Jonathan Ragan-Kelley</li>
<li>for: 提高Transformer模型中的序列长度增长的能力</li>
<li>methods: 使用Ring Attention算法和Striped Attention扩展来解决每个设备内存瓶颈问题</li>
<li>results: 在 causal transformer 模型中实现1.45倍的终端通过put通过率提高，并在16个TPUv4板上实现1.65倍的速度提高，sequence length为256k和786k。<details>
<summary>Abstract</summary>
To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source
</details>
<details>
<summary>摘要</summary>
为了满足长序列的增长需求，刘等人最近提出了环形注意力算法（Ring Attention），可以在单个设备内分布自注意力，从而缓解单个设备内存瓶颈。在这篇论文中，我们研究了环形注意力在重要的 causal transformer 模型中的性能特点，并发现了一个关键的工作负荷不均。我们提出了一个简单的扩展，称为扫描注意力（Striped Attention），可以解决这一问题。在实验中，我们在 A100 GPU 和 TPUv4 上运行了扫描注意力算法，并 achieved 256k 序列长度下的最大 1.45x 终端通过puts，以及 786k 序列长度下的最大 1.65x 终端通过puts。此外，我们还发布了我们的实验代码作为开源。
</details></li>
</ul>
<hr>
<h2 id="Predicting-generalization-performance-with-correctness-discriminators"><a href="#Predicting-generalization-performance-with-correctness-discriminators" class="headerlink" title="Predicting generalization performance with correctness discriminators"></a>Predicting generalization performance with correctness discriminators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09422">http://arxiv.org/abs/2311.09422</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuekun Yao, Alexander Koller</li>
<li>for: 预测NLP模型在未看过数据上的准确率，以确保模型的可靠性。</li>
<li>methods: 提出了一种新的模型，通过训练一个推断器，来预测序列到序列模型输出是正确或错误的。</li>
<li>results: 在多种标注、分析和semantic parsing任务上，金字典准确率都在预测的上下限之间，并且这些上下限很接近。<details>
<summary>Abstract</summary>
The ability to predict an NLP model's accuracy on unseen, potentially out-of-distribution data is a prerequisite for trustworthiness. We present a novel model that establishes upper and lower bounds on the accuracy, without requiring gold labels for the unseen data. We achieve this by training a discriminator which predicts whether the output of a given sequence-to-sequence model is correct or not. We show across a variety of tagging, parsing, and semantic parsing tasks that the gold accuracy is reliably between the predicted upper and lower bounds, and that these bounds are remarkably close together.
</details>
<details>
<summary>摘要</summary>
使得预测NLP模型对未看过、可能不属于输入范围的数据的准确率是一个必要的前提，以确保模型的可靠性。我们提出了一种新的模型，可以在未看过数据上预测模型的准确率，不需要黄金标签。我们通过训练一个推断器，判断给定的序列-到-序列模型输出是否正确，来实现这一点。我们在不同的标注、分析和 semantics 解析任务上显示，黄金准确率在预测的Upper和Lower bound之间，这些 bound 很接近。</SYS>Here's the translation in Simplified Chinese:<SYS>    使得预测NLP模型对未看过、可能不属于输入范围的数据的准确率是一个必要的前提，以确保模型的可靠性。我们提出了一种新的模型，可以在未看过数据上预测模型的准确率，不需要黄金标签。我们通过训练一个推断器，判断给定的序列-到-序列模型输出是否正确，来实现这一点。我们在不同的标注、分析和 semantics 解析任务上显示，黄金准确率在预测的Upper和Lower bound之间，这些 bound 很接近。</SYS>Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. Traditional Chinese is used in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture"><a href="#Alternatives-to-the-Scaled-Dot-Product-for-Attention-in-the-Transformer-Neural-Network-Architecture" class="headerlink" title="Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture"></a>Alternatives to the Scaled Dot Product for Attention in the Transformer Neural Network Architecture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09406">http://arxiv.org/abs/2311.09406</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Bernhard</li>
<li>for: 避免权重缺失导致梯度消失的问题</li>
<li>methods: 提出一些替代缩放方法，包括将dot product除以键值Sum前应用softmax</li>
<li>results: 通过使用模拟的键和问题示例，显示了这些缩放方法在许多情况下效果更好，避免了梯度消失的问题<details>
<summary>Abstract</summary>
The transformer neural network architecture uses a form of attention in which the dot product of query and key is divided by the square root of the key dimension before applying softmax. This scaling of the dot product is designed to avoid the absolute value of the dot products becoming so large that applying softmax leads to vanishing gradients. In this paper, we propose some alternative scalings, including dividing the dot product instead by the sum of the key lengths before applying softmax. We use simulated keys and queries to show that in many situations this appears to be more effective at avoiding regions where applying softmax leads to vanishing gradients.
</details>
<details>
<summary>摘要</summary>
transformer神经网络架构使用一种叫做注意力的机制，其中查询和键的点积被除以键维度的平方根之后应用softmax。这种缩放的点积是为了避免查询和键的绝对值变得太大，使得应用softmax导致梯度消失。在这篇论文中，我们提出了一些代替的缩放方法，包括在应用softmax之前将点积除以键的总长度。我们使用模拟的查询和键来显示，在许多情况下，这些缩放方法更有效地避免应用softmax导致梯度消失的区域。
</details></li>
</ul>
<hr>
<h2 id="To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages"><a href="#To-Translate-or-Not-to-Translate-A-Systematic-Investigation-of-Translation-Based-Cross-Lingual-Transfer-to-Low-Resource-Languages" class="headerlink" title="To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages"></a>To Translate or Not to Translate: A Systematic Investigation of Translation-Based Cross-Lingual Transfer to Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09404">http://arxiv.org/abs/2311.09404</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benedikt Ebing, Goran Glavaš</li>
<li>for: 本研究旨在系统地评估现有和提出新的翻译基于的跨语言迁移（XLT）方法，以便在低资源语言上进行迁移。</li>
<li>methods: 本研究使用了翻译基于的XLT方法，包括将源语言训练数据翻译回目标语言，并将目标语言测试数据翻译回源语言。此外，还添加了其他高资源语言的可靠翻译来加强模型。</li>
<li>results: 研究发现，使用翻译基于的XLT方法可以大幅超越零极XLT方法，并且可以通过添加其他高资源语言的翻译来进一步提高实验性能。此外，研究还提出了一种能够在不支持MT系统的语言上实现XLT的效果的策略。最后，研究发现，使用MT系统生成的目标语言验证数据来选择XLT模型可以更好地提高模型性能。<details>
<summary>Abstract</summary>
Perfect machine translation (MT) would render cross-lingual transfer (XLT) by means of multilingual language models (LMs) superfluous. Given, on one hand, the large body of work on improving XLT with multilingual LMs and, on the other hand, recent advances in massively multilingual MT, in this work, we systematically evaluate existing and propose new translation-based XLT approaches for transfer to low-resource languages. We show that all translation-based approaches dramatically outperform zero-shot XLT with multilingual LMs, rendering the approach that combines the round-trip translation of the source-language training data with the translation of the target-language test instances the most effective. We next show that one can obtain further empirical gains by adding reliable translations to other high-resource languages to the training data. Moreover, we propose an effective translation-based XLT strategy even for languages not supported by the MT system. Finally, we show that model selection for XLT based on target-language validation data obtained with MT outperforms model selection based on the source-language data. We hope that our findings encourage adoption of more robust translation-based baselines in XLT research.
</details>
<details>
<summary>摘要</summary>
如果精准机器翻译（MT）能够实现语言转换（XLT），那么使用多语言模型（LM）来实现XLT将成为 redundant。在一个手上，有大量关于提高XLT的多语言LM的研究，而在另一个手上，有最近的质量翻译技术的进步。在这项工作中，我们系统地评估了现有的翻译基于XLT的方法，并提出了新的翻译基于XLT的方法。我们发现所有的翻译基于方法在零投入XLT中都表现出了很好的表现，而combined round-trip translation of the source-language training data with the translation of the target-language test instances的方法是最有效的。我们还证明可以通过添加其他高资源语言的可靠翻译到训练数据中来获得更高的实验性赢利。此外，我们提出了一种有效的翻译基于XLT策略，即使Language不支持MT系统。最后，我们发现基于MT系统 validation data 进行模型选择可以更好地than基于源语言数据。我们希望我们的发现能够激励XLT研究中更多使用更加稳定的翻译基于基准。
</details></li>
</ul>
<hr>
<h2 id="LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems"><a href="#LEEETs-Dial-Linguistic-Entrainment-in-End-to-End-Task-oriented-Dialogue-systems" class="headerlink" title="LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems"></a>LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09390">http://arxiv.org/abs/2311.09390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nalin Kumar, Ondřej Dušek</li>
<li>for: 这个研究旨在提高对话系统的自然性，通过实现对话Alignment。</li>
<li>methods: 该研究使用GPT-2基于端到端对话系统，并采用共享词汇来实现对话Alignment。试用了训练实例权重、对ignment特定的损失函数和额外conditioning来生成与用户的响应。</li>
<li>results: 通过对MultiWOZ数据集进行比较，研究发现三种 entraining 技术均可以significantly improve alignment compared to the baseline，并被自动和手动评估指标证明。<details>
<summary>Abstract</summary>
Linguistic entrainment, or alignment, represents a phenomenon where linguistic patterns employed by conversational participants converge to one another. While alignment has been shown to produce a more natural user experience, most dialogue systems do not have any provisions for it. In this work, we introduce methods for achieving dialogue alignment in a GPT-2-based end-to-end dialogue system through the utilization of shared vocabulary. We experiment with training instance weighting, alignment-specific loss, and additional conditioning to generate responses that align with the user. By comparing different entrainment techniques on the MultiWOZ dataset, we demonstrate that all three approaches produce significantly better-aligned results than the baseline, as confirmed by both automated and manual evaluation metrics.
</details>
<details>
<summary>摘要</summary>
语言同步（或对齐）现象表示对话参与者使用的语言模式相互听得一致。尽管对齐可以提供更自然的用户体验，但大多数对话系统没有相关的规定。在这项工作中，我们介绍了基于 GPT-2 的端到端对话系统中实现对话对齐的方法，通过共享词汇的使用。我们对训练实例权重、对齐特定的损失函数和附加条件进行实验，以生成与用户相对的回答。通过对 MultiWOZ 数据集的不同对齐技术进行比较，我们证明了所有三种方法均可以在自动和手动评估指标上提供显著更好的对齐效果。
</details></li>
</ul>
<hr>
<h2 id="Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing"><a href="#Neural-machine-translation-for-automated-feedback-on-children’s-early-stage-writing" class="headerlink" title="Neural machine translation for automated feedback on children’s early-stage writing"></a>Neural machine translation for automated feedback on children’s early-stage writing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09389">http://arxiv.org/abs/2311.09389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonas Vestergaard Jensen, Mikkel Jordahn, Michael Riis Andersen</li>
<li>for: 本研究旨在自动生成初级写作评估和建议，使用机器学习技术。</li>
<li>methods: 本研究提议使用序列到序列模型将初级写作翻译成正常写作，以便使用语言指标进行分析。此外，提出了一种新的强度 likelihood 来抑制数据集中的噪声影响。</li>
<li>results: 经numerical实验 validate，可以高精度预测正常写作。<details>
<summary>Abstract</summary>
In this work, we address the problem of assessing and constructing feedback for early-stage writing automatically using machine learning. Early-stage writing is typically vastly different from conventional writing due to phonetic spelling and lack of proper grammar, punctuation, spacing etc. Consequently, early-stage writing is highly non-trivial to analyze using common linguistic metrics. We propose to use sequence-to-sequence models for "translating" early-stage writing by students into "conventional" writing, which allows the translated text to be analyzed using linguistic metrics. Furthermore, we propose a novel robust likelihood to mitigate the effect of noise in the dataset. We investigate the proposed methods using a set of numerical experiments and demonstrate that the conventional text can be predicted with high accuracy.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们解决了自动使用机器学习进行早期写作评估和建构反馈的问题。早期写作通常具有不同的语音拼写和缺失正确的语法、标点、间距等等特点，因此对于常见语言指标来说非常困难分析。我们提议使用序列到序列模型将学生早期写作翻译成“常规”的写作，以便使用语言指标进行分析。此外，我们提出了一种新的稳定 likelihood 来抑制数据集中的噪声的影响。我们通过数字实验 investigate 这些方法，并证明可以高度准确地预测常规文本。
</details></li>
</ul>
<hr>
<h2 id="Banach-Tarski-Embeddings-and-Transformers"><a href="#Banach-Tarski-Embeddings-and-Transformers" class="headerlink" title="Banach-Tarski Embeddings and Transformers"></a>Banach-Tarski Embeddings and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09387">http://arxiv.org/abs/2311.09387</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jtmaher/embedding">https://github.com/jtmaher/embedding</a></li>
<li>paper_authors: Joshua Maher</li>
<li>for: 这个论文是为了提出一种将任意递归数据结构嵌入高维向量空间的新方法。</li>
<li>methods: 这个论文使用的方法包括提出一种可解释性模型，用于转换器的秘密状态向量。这个模型可以在嵌入维度够大时将嵌入vector解码回原始数据结构。此解码算法自然地实现为一种转换器。此外，这些嵌入向量还可以直接进行计算，无需解码。例如，我们提出了一种使用vector操作构建嵌入Token序列的嵌入 parse树算法。</li>
<li>results: 这个论文的结果表明，当嵌入维度够大时，这种嵌入可以准确地重建原始数据结构。此外，这种嵌入还可以 Directly manipulate the embedded vectors to perform computations on the underlying data without decoding.<details>
<summary>Abstract</summary>
We introduce a new construction of embeddings of arbitrary recursive data structures into high dimensional vectors. These embeddings provide an interpretable model for the latent state vectors of transformers. We demonstrate that these embeddings can be decoded to the original data structure when the embedding dimension is sufficiently large. This decoding algorithm has a natural implementation as a transformer. We also show that these embedding vectors can be manipulated directly to perform computations on the underlying data without decoding. As an example we present an algorithm that constructs the embedded parse tree of an embedded token sequence using only vector operations in embedding space.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的嵌入构造，用于将任意递归数据结构嵌入高维向量中。这些嵌入提供了可解释的模型 дляtransformer的latent状态向量。我们证明了这些嵌入可以在嵌入维度充分大的情况下被解码回原始数据结构。这个解码算法自然地实现为transformer。我们还证明了这些嵌入向量可以直接进行计算，而无需解码。作为示例，我们提出了一个算法，用于在嵌入空间内构建token序列的嵌入树结构。
</details></li>
</ul>
<hr>
<h2 id="Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach"><a href="#Long-form-Question-Answering-An-Iterative-Planning-Retrieval-Generation-Approach" class="headerlink" title="Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach"></a>Long-form Question Answering: An Iterative Planning-Retrieval-Generation Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09383">http://arxiv.org/abs/2311.09383</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pritom Saha Akash, Kashob Kumar Roy, Lucian Popa, Kevin Chen-Chuan Chang</li>
<li>for: 这篇论文是为了解决长形问答（LFQA）问题，旨在生成详细的回答，而不是简单的是或否答案或短要的信息。</li>
<li>methods: 该论文提出了一种基于谱计规划、检索和生成的LFQA模型，通过多次迭代的计划、检索和生成过程来生成详细的回答。</li>
<li>results: 经过广泛的实验，该模型在开放领域和技术领域的QA数据集上表现出优于现有模型，在多种文本和事实指标上具有更高的表现。<details>
<summary>Abstract</summary>
Long-form question answering (LFQA) poses a challenge as it involves generating detailed answers in the form of paragraphs, which go beyond simple yes/no responses or short factual answers. While existing QA models excel in questions with concise answers, LFQA requires handling multiple topics and their intricate relationships, demanding comprehensive explanations. Previous attempts at LFQA focused on generating long-form answers by utilizing relevant contexts from a corpus, relying solely on the question itself. However, they overlooked the possibility that the question alone might not provide sufficient information to identify the relevant contexts. Additionally, generating detailed long-form answers often entails aggregating knowledge from diverse sources. To address these limitations, we propose an LFQA model with iterative Planning, Retrieval, and Generation. This iterative process continues until a complete answer is generated for the given question. From an extensive experiment on both an open domain and a technical domain QA dataset, we find that our model outperforms the state-of-the-art models on various textual and factual metrics for the LFQA task.
</details>
<details>
<summary>摘要</summary>
长swers 问题 (LFQA) 提出了一个挑战，因为它们需要生成详细的答案，而不是单纯的是或否答案或简短的事实答案。现有的 QA 模型在问题中可以提供简短的答案，但 LFQA 需要处理多个话题和它们的复杂关系，需要详细的解释。过去的 LFQA 尝试都是通过使用相关的文本库来生成长答案，但它们忽视了问题本身可能无法提供足够的信息来定义相关的文本库。此外，生成详细的长答案通常需要从多个来源汇集知识。为解决这些限制，我们提出了一个基于迭代的计划、检索和生成的 LFQA 模型。这个迭代过程一直进行，直到为给定的问题生成完整的答案。从对开放领域和技术领域 QA 数据集的广泛实验来看，我们发现我们的模型在不同的文本和事实指标上超过了当前状态的模型。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms"><a href="#A-Survey-on-Online-User-Aggression-Content-Detection-and-Behavioural-Analysis-on-Social-Media-Platforms" class="headerlink" title="A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms"></a>A Survey on Online User Aggression: Content Detection and Behavioural Analysis on Social Media Platforms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09367">http://arxiv.org/abs/2311.09367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Swapnil Mane, Suman Kundu, Rajesh Sharma</li>
<li>for: This paper aims to bridge the gap between disparate studies on aggression content detection and behavioral analysis of aggressive users in the context of cyber-aggressive behavior.</li>
<li>methods: The paper examines the comprehensive process of aggression content detection, including dataset creation, feature selection and extraction, and detection algorithm development. It also reviews studies on behavioral analysis of aggression that explore influencing factors, consequences, and patterns associated with cyber-aggressive behavior.</li>
<li>results: The paper identifies research gaps and encourages further progress in the unified domain of socio-computational aggressive behavior analysis.Here’s the Chinese version of the three information points:</li>
<li>for: 这篇论文目标是将不同领域的侵略行为探究归并到一起，以探讨cyber-侵略行为中的社会问题。</li>
<li>methods: 论文检查了侵略内容检测的全面过程，包括数据集创建、特征选择和提取、检测算法开发。它还查看了对侵略行为的行为分析研究，探讨了侵略行为的影响因素、后果和模式。</li>
<li>results: 论文发现了研究漏洞，并促进了在统一领域内的社会计算侵略行为分析的进展。<details>
<summary>Abstract</summary>
The rise of social media platforms has led to an increase in cyber-aggressive behavior, encompassing a broad spectrum of hostile behavior, including cyberbullying, online harassment, and the dissemination of offensive and hate speech. These behaviors have been associated with significant societal consequences, ranging from online anonymity to real-world outcomes such as depression, suicidal tendencies, and, in some instances, offline violence. Recognizing the societal risks associated with unchecked aggressive content, this paper delves into the field of Aggression Content Detection and Behavioral Analysis of Aggressive Users, aiming to bridge the gap between disparate studies. In this paper, we analyzed the diversity of definitions and proposed a unified cyber-aggression definition. We examine the comprehensive process of Aggression Content Detection, spanning from dataset creation, feature selection and extraction, and detection algorithm development. Further, we review studies on Behavioral Analysis of Aggression that explore the influencing factors, consequences, and patterns associated with cyber-aggressive behavior. This systematic literature review is a cross-examination of content detection and behavioral analysis in the realm of cyber-aggression. The integrated investigation reveals the effectiveness of incorporating sociological insights into computational techniques for preventing cyber-aggressive behavior. Finally, the paper concludes by identifying research gaps and encouraging further progress in the unified domain of socio-computational aggressive behavior analysis.
</details>
<details>
<summary>摘要</summary>
“社交媒体平台的崛起导致了网络攻击性行为的增加，包括网络欺凌、网络恐吓和各种不实和恨言。这些行为与社会的后果存在联系，包括线上匿名和实际世界的抑郁、自杀倾向和，在某些情况下，网络上的暴力。本文探讨了网络攻击性行为的多元定义，并提出了统一的网络攻击定义。我们分析了各种数据集的建立、特征选择和提取，以及检测算法的发展。此外，我们审查了关于攻击者行为的行为分析研究，探讨了这些行为的影响因素、后果和模式。本文的系统性审查显示了融合社会学知识和计算技术可以预防网络攻击性行为。最后，本文总结了研究缺陷，并鼓励进一步的进展在统一的网络攻击行为分析领域。”
</details></li>
</ul>
<hr>
<h2 id="Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models"><a href="#Investigating-the-Emergent-Audio-Classification-Ability-of-ASR-Foundation-Models" class="headerlink" title="Investigating the Emergent Audio Classification Ability of ASR Foundation Models"></a>Investigating the Emergent Audio Classification Ability of ASR Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09363">http://arxiv.org/abs/2311.09363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill</li>
<li>for: 这 paper 探讨了 ASR 基础模型 Whisper 和 MMS 在零shot 设定下的语音分类能力。</li>
<li>methods: 这 paper 使用了简单的模板基于文本提示，将 decoder 的解码概率用于生成零shot 预测。无需训练模型或添加新参数，Whisper 在多个 audio-classification dataset 上表现出了有前所未有的零shot 分类性能，比前一个状态的平均精度高出 9%。</li>
<li>results: 这 paper 发现，对零shot 分类任务，Whisper 模型的性能随模型大小增长，表明随着 ASR 基础模型的扩大，其零shot 性能可能会提高。<details>
<summary>Abstract</summary>
Text and vision foundation models can perform many tasks in a zero-shot setting, a desirable property that enables these systems to be applied in general and low-resource settings. However, there has been significantly less work on the zero-shot abilities of ASR foundation models, with these systems typically fine-tuned to specific tasks or constrained to applications that match their training criterion and data annotation. In this work we investigate the ability of Whisper and MMS, ASR foundation models trained primarily for speech recognition, to perform zero-shot audio classification. We use simple template-based text prompts at the decoder and use the resulting decoding probabilities to generate zero-shot predictions. Without training the model on extra data or adding any new parameters, we demonstrate that Whisper shows promising zero-shot classification performance on a range of 8 audio-classification datasets, outperforming existing state-of-the-art zero-shot baseline's accuracy by an average of 9%. One important step to unlock the emergent ability is debiasing, where a simple unsupervised reweighting method of the class probabilities yields consistent significant performance gains. We further show that performance increases with model size, implying that as ASR foundation models scale up, they may exhibit improved zero-shot performance.
</details>
<details>
<summary>摘要</summary>
文本和视觉基础模型可以完成许多零 shot 任务，这是一个极其愉快的特性，使这些系统可以在通用和低资源环境中应用。然而，针对 ASR 基础模型的零 shot 能力的研究远未充分，这些系统通常是特定任务的精度调整或者限定到与其训练标准和数据注解相匹配的应用。在这项工作中，我们调查了 Whisper 和 MMS，这两个 ASR 基础模型是主要用于语音识别的。我们使用简单的模板基于文本提示，并使用 resulting 的解码概率来生成零 shot 预测。无需训练模型Extra 数据或添加新参数，我们示出了 Whisper 在多个 8 个音频分类数据集上的出色的零 shot 分类性能，与现有状态的平均性能提高率为 9%。一种重要的步骤是去偏见，其中一种简单的无监督重weighting 方法可以持续提供显著的性能提升。我们进一步表明，性能随模型大小增长，implying 随 ASR 基础模型的扩大，它们可能会表现出更好的零 shot 性能。
</details></li>
</ul>
<hr>
<h2 id="LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents"><a href="#LePaRD-A-Large-Scale-Dataset-of-Judges-Citing-Precedents" class="headerlink" title="LePaRD: A Large-Scale Dataset of Judges Citing Precedents"></a>LePaRD: A Large-Scale Dataset of Judges Citing Precedents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09356">http://arxiv.org/abs/2311.09356</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rmahari/lepard">https://github.com/rmahari/lepard</a></li>
<li>paper_authors: Robert Mahari, Dominik Stammbach, Elliott Ash, Alex &#96;Sandy’ Pentland</li>
<li>for: 本研究的目的是发展实用法律自然语言处理技术，帮助扩大法律研究的访问和 justice 的质量。</li>
<li>methods: 本研究使用了大量的美国联邦法院判例文献，通过Contextualized Word Embeddings 和文本分类来进行预测。</li>
<li>results: 研究发现，使用文本分类方法可以在预测法律前置文献中达到较高的准确率，但是法律预测仍然是一项具有挑战性的任务，具有很大的改进空间。<details>
<summary>Abstract</summary>
We present the Legal Passage Retrieval Dataset LePaRD. LePaRD is a massive collection of U.S. federal judicial citations to precedent in context. The dataset aims to facilitate work on legal passage prediction, a challenging practice-oriented legal retrieval and reasoning task. Legal passage prediction seeks to predict relevant passages from precedential court decisions given the context of a legal argument. We extensively evaluate various retrieval approaches on LePaRD, and find that classification appears to work best. However, we note that legal precedent prediction is a difficult task, and there remains significant room for improvement. We hope that by publishing LePaRD, we will encourage others to engage with a legal NLP task that promises to help expand access to justice by reducing the burden associated with legal research. A subset of the LePaRD dataset is freely available and the whole dataset will be released upon publication.
</details>
<details>
<summary>摘要</summary>
我们介绍了《法律段落预测数据集》（LePaRD）。LePaRD是一个庞大的美国联邦司法文献引用集，旨在促进法律段落预测任务的研究。法律段落预测是一种实践 oriented 的法律检索和逻辑任务，旨在预测基于法律 Argument 的相关部分。我们在 LePaRD 上进行了广泛的评估，发现类别方法在这些任务中表现最好。然而，我们注意到法律预测是一个具有挑战性的任务，还有很大的改进空间。我们希望通过发布 LePaRD，促进法律自然语言处理任务的研究，以扩大对正义的访问。一个 LePaRD 的子集已经公开可用，整个数据集将在发表后公开。
</details></li>
</ul>
<hr>
<h2 id="Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization"><a href="#Language-and-Task-Arithmetic-with-Parameter-Efficient-Layers-for-Zero-Shot-Summarization" class="headerlink" title="Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization"></a>Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09344">http://arxiv.org/abs/2311.09344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexandra Chronopoulou, Jonas Pfeiffer, Joshua Maynez, Xinyi Wang, Sebastian Ruder, Priyanka Agrawal</li>
<li>for: 提高大语言模型在下游任务中的性能，特别是使用 labelled task 数据进行 parameter-efficient fine-tuning (PEFT)。</li>
<li>methods: 提出了一种基于 language 或 task 特有的 parameter 的特殊化方法，通过元素 wise 加法操作来挖掘无标注数据和英语标注数据。</li>
<li>results: 经验表明，该方法可以在摘要任务上取得稳定的提升，只需要训练 PEFT 模块 minimal amount of training data。<details>
<summary>Abstract</summary>
Parameter-efficient fine-tuning (PEFT) using labeled task data can significantly improve the performance of large language models (LLMs) on the downstream task. However, there are 7000 languages in the world and many of these languages lack labeled data for real-world language generation tasks. In this paper, we propose to improve zero-shot cross-lingual transfer by composing language or task specialized parameters. Our method composes language and task PEFT modules via element-wise arithmetic operations to leverage unlabeled data and English labeled data. We extend our approach to cases where labeled data from more languages is available and propose to arithmetically compose PEFT modules trained on languages related to the target. Empirical results on summarization demonstrate that our method is an effective strategy that obtains consistent gains using minimal training of PEFT modules.
</details>
<details>
<summary>摘要</summary>
参数高效调整（PEFT）使用标注任务数据可以显著提高大语言模型（LLM）的下游任务性能。然而，世界上有7000种语言，并且许多这些语言缺乏实际语言生成任务的标注数据。在这篇论文中，我们提议通过语言或任务特化的参数进行改进零上下游语言传递。我们的方法通过语言和任务PEFT模块之间的元素积算操作来利用无标注数据和英文标注数据。我们将我们的方法扩展到有更多语言的标注数据的情况，并提议使用相关语言的PEFT模块进行加法组合。实验结果表明，我们的方法是一种有效的策略，可以通过最小的PEFT模块训练而获得常见的提升。
</details></li>
</ul>
<hr>
<h2 id="Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback"><a href="#Pinpoint-Not-Criticize-Refining-Large-Language-Models-via-Fine-Grained-Actionable-Feedback" class="headerlink" title="Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback"></a>Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09336">http://arxiv.org/abs/2311.09336</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag</li>
<li>for: 提高文本生成质量</li>
<li>methods: 使用细化的行为反馈，通过一个学习的错误定位模型来进行迭代改进</li>
<li>results: 在三个文本生成任务中，包括机器翻译、长篇问答和主题概要，观察到0.8和0.7 MetricX的提升，以及4.5和1.8 ROUGE-L的提升，单次迭代改进。使用仿生热化算法可以进一步提高质量，包括最多1.7 MetricX的提升。<details>
<summary>Abstract</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA) and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details>
<details>
<summary>摘要</summary>
Recent improvements in text generation have leveraged human feedback to improve the quality of the generated output. However, human feedback is not always available, especially during inference. In this work, we propose an inference time optimization method FITO to use fine-grained actionable feedback in the form of error type, error location, and severity level that are predicted by a learned error pinpoint model for iterative refinement. FITO starts with an initial output, then iteratively incorporates the feedback via a refinement model that generates an improved output conditioned on the feedback. Given the uncertainty of consistent refined samples at iterative steps, we formulate iterative refinement into a local search problem and develop a simulated annealing based algorithm that balances exploration of the search space and optimization for output quality. We conduct experiments on three text generation tasks, including machine translation, long-form question answering (QA), and topical summarization. We observe 0.8 and 0.7 MetricX gain on Chinese-English and English-German translation, 4.5 and 1.8 ROUGE-L gain at long form QA and topic summarization respectively, with a single iteration of refinement. With our simulated annealing algorithm, we see further quality improvements, including up to 1.7 MetricX improvements over the baseline approach.
</details></li>
</ul>
<hr>
<h2 id="Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models"><a href="#Mind’s-Mirror-Distilling-Self-Evaluation-Capability-and-Comprehensive-Thinking-from-Large-Language-Models" class="headerlink" title="Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models"></a>Mind’s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09214">http://arxiv.org/abs/2311.09214</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu, Hongxia Xu, Jintai Chen, Jian Wu</li>
<li>for: 提高小语言模型（SLM）的性能，使其更加接近人类认知。</li>
<li>methods: 提出了一种两重方法，首先是将大语言模型（LLM）中的自我评估能力抽象到 SLM 中，以减少错误的 reasoning 和幻见的影响；其次是提出了一种多种链条思维和自我评估 paradigm 的总体distillation进程，以确保更加全面和坚实地将知识传递到 SLM 中。</li>
<li>results: 实验结果表明，我们的方法可以显著提高 distilled SLM 的性能，并且突出了开发更小的模型，更 closely aligns with human cognition 的道路。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in the field of natural language processing. However, the sheer scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained contexts. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still carry over flawed reasoning or hallucinations inherited from their LLM counterparts. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability inherent in LLMs into SLMs, which aims to mitigate the adverse effects of erroneous reasoning and reduce hallucinations. Second, we advocate for a comprehensive distillation process that incorporates multiple distinct chain-of-thought and self-evaluation paradigms and ensures a more holistic and robust knowledge transfer into SLMs. Experiments on three NLP benchmarks demonstrate that our method significantly improves the performance of distilled SLMs and sheds light on the path towards developing smaller models closely aligned with human cognition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Distill the self-evaluation capability of LLMs into small language models (SLMs) to mitigate erroneous reasoning and reduce hallucinations.2. Use a comprehensive distillation process that incorporates multiple chain-of-thought and self-evaluation paradigms for a more holistic and robust knowledge transfer.Experiments on three NLP benchmarks show that our method significantly improves the performance of distilled SLMs and provides insights into developing smaller models aligned with human cognition.</details></li>
</ol>
<hr>
<h2 id="GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes"><a href="#GRIM-GRaph-based-Interactive-narrative-visualization-for-gaMes" class="headerlink" title="GRIM: GRaph-based Interactive narrative visualization for gaMes"></a>GRIM: GRaph-based Interactive narrative visualization for gaMes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09213">http://arxiv.org/abs/2311.09213</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jorge Leandro, Sudha Rao, Michael Xu, Weijia Xu, Nebosja Jojic, Chris Brockett, Bill Dolan</li>
<li>for: 帮助对话式角色扮演游戏（RPG）的故事创作。</li>
<li>methods: 使用大型生成文本模型协助创作过程。</li>
<li>results: 可以生成rich narrative graph with branching storylines，并且可以在设计者的交互下自动生成新的子图文件，以满足编辑需求。<details>
<summary>Abstract</summary>
Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The narratives of these may take years to write and typically involve a large creative team. In this work, we demonstrate the potential of large generative text models to assist this process. \textbf{GRIM}, a prototype \textbf{GR}aph-based \textbf{I}nteractive narrative visualization system for ga\textbf{M}es, generates a rich narrative graph with branching storylines that match a high-level narrative description and constraints provided by the designer. Game designers can interactively edit the graph by automatically generating new sub-graphs that fit the edits within the original narrative and constraints. We illustrate the use of \textbf{GRIM} in conjunction with GPT-4, generating branching narratives for four well-known stories with different contextual constraints.
</details>
<details>
<summary>摘要</summary>
对话式角色游戏（RPG）需要强大的故事编写。这些故事可能需要几年时间写作，通常需要一大群创作人员。在这个工作中，我们展示了大型生成文本模型如何帮助这个过程。我们开发了一个名为“GRIM”的原型，它是一个基于图的互动式narative视觉系统，可以根据设计师提供的高级剧本和约束生成丰富的剧本图。设计师可以通过交互地编辑图表，生成适应修改的新子图，以保持在原始剧本和约束之内。我们使用GPT-4和GRIM在不同的Contextual约束下生成分支剧本，以示其可用性。
</details></li>
</ul>
<hr>
<h2 id="Contrastive-Chain-of-Thought-Prompting"><a href="#Contrastive-Chain-of-Thought-Prompting" class="headerlink" title="Contrastive Chain-of-Thought Prompting"></a>Contrastive Chain-of-Thought Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09277">http://arxiv.org/abs/2311.09277</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/contrastive-cot">https://github.com/damo-nlp-sg/contrastive-cot</a></li>
<li>paper_authors: Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing</li>
<li>for: 提高语音模型的逻辑推理能力</li>
<li>methods: 使用对比链式思维法，提供有效和无效示例来引导语音模型进行步骤式推理，并提高推理错误的检测能力</li>
<li>results: 在逻辑推理benchmark上实现了对比链式思维法的普适性，并且提高了语音模型的逻辑推理能力<details>
<summary>Abstract</summary>
Despite the success of chain of thought in enhancing language model reasoning, the underlying process remains less well understood. Although logically sound reasoning appears inherently crucial for chain of thought, prior studies surprisingly reveal minimal impact when using invalid demonstrations instead. Furthermore, the conventional chain of thought does not inform language models on what mistakes to avoid, which potentially leads to more errors. Hence, inspired by how humans can learn from both positive and negative examples, we propose contrastive chain of thought to enhance language model reasoning. Compared to the conventional chain of thought, our approach provides both valid and invalid reasoning demonstrations, to guide the model to reason step-by-step while reducing reasoning mistakes. To improve generalization, we introduce an automatic method to construct contrastive demonstrations. Our experiments on reasoning benchmarks demonstrate that contrastive chain of thought can serve as a general enhancement of chain-of-thought prompting.
</details>
<details>
<summary>摘要</summary>
尽管链式思考已经提高了语言模型的逻辑能力，但它们的下面逻辑过程仍然尚不够了解。尽管逻辑正确性看起来是链式思考的核心，但是前一 Studies 显示，使用无效示例时的影响却是很小。此外， convent ional 链式思考没有告诉语言模型哪些错误需要避免，这可能会导致更多的错误。因此，我们提出了受人类学习 FROM positive 和 negative 示例的灵感，并将其应用于语言模型的逻辑reasoning。与 convent ional 链式思考相比，我们的approach 可以提供有效和无效的逻辑示例，以引导模型 step-by-step 进行逻辑reasoning，同时减少逻辑错误。为了提高泛化能力，我们提出了一种自动生成对照示例的方法。我们的实验表明，对于逻辑测试 benchmark 来说，对照链式思考可以作为一种通用的逻辑促进。
</details></li>
</ul>
<hr>
<h2 id="TableLlama-Towards-Open-Large-Generalist-Models-for-Tables"><a href="#TableLlama-Towards-Open-Large-Generalist-Models-for-Tables" class="headerlink" title="TableLlama: Towards Open Large Generalist Models for Tables"></a>TableLlama: Towards Open Large Generalist Models for Tables</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09206">http://arxiv.org/abs/2311.09206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun<br>for:This paper aims to develop open-source large language models (LLMs) as generalists for a diversity of table-based tasks.methods:The authors construct a new dataset called TableInstruct, which includes a variety of realistic tables and tasks, and fine-tune an open-source model (TableLlama) with LongLoRA to address the long context challenge.results:TableLlama achieves comparable or better performance than the state-of-the-art (SOTA) on 7 out of 8 in-domain tasks, and shows 6-48 absolute point gains on 6 out-of-domain datasets, demonstrating the model’s generalizability.Here’s the simplified Chinese text:for: 这篇论文目标是开发大量自然语言模型（LLM），用于多种表格任务。methods: 作者们构建了一个新的表格数据集（TableInstruct），包括多种真实的表格和任务，并使用LongLoRA进行微调，以解决长上下文挑战。results: TableLlama在7个域内任务中达到或超过当前最佳性能（SOTA），并在6个对于任务特定设计的数据集上显示6-48个绝对点提升，示出模型的通用性。<details>
<summary>Abstract</summary>
Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOTA for each task, despite the latter often has task-specific design. On 6 out-of-domain datasets, it achieves 6-48 absolute point gains compared with the base model, showing that training on TableInstruct enhances the model's generalizability. We will open-source our dataset and trained model to boost future work on developing open generalist models for tables.
</details>
<details>
<summary>摘要</summary>
semi-structured 表格是普遍存在的。有很多任务旨在自动理解、增强和查询表格。现有的方法frequently需要预训练表格或特定的模型体系设计，或者只能处理特定的表格类型，或者假设表格和任务过于简单。这篇论文是开发大型自然语言模型（LLM）为表格任务的第一步。为此，我们构建了一个名为 TableInstruct 的新数据集，用于训练和评估 LLM。我们还开发了首个开源的通用模型 для表格，即 TableLlama，通过长Context挑战 LongLoRA 来练习。我们在域 Setting 和 out-of-domain Setting 下进行了实验。在 7 个域 Setting 中，TableLlama 在每个任务上与 SOTA 相比，具有相似或更好的性能，即使后者具有特定的设计。在 6 个 out-of-domain 数据集上，它与基模型相比，获得了 6-48 绝对点胜。这表明训练在 TableInstruct 上增强了模型的通用性。我们将开源我们的数据集和训练模型，以便将来的开发工作。
</details></li>
</ul>
<hr>
<h2 id="When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages"><a href="#When-Is-Multilinguality-a-Curse-Language-Modeling-for-250-High-and-Low-Resource-Languages" class="headerlink" title="When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages"></a>When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09205">http://arxiv.org/abs/2311.09205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tylerachang/curse-of-multilinguality">https://github.com/tylerachang/curse-of-multilinguality</a></li>
<li>paper_authors: Tyler A. Chang, Catherine Arnett, Zhuowen Tu, Benjamin K. Bergen</li>
<li>for: 本研究旨在 investigate the effects of multilinguality on language modeling performance in individual languages.</li>
<li>methods: 研究人员采用了10,000个单语言和多语言语言模型，对250种语言进行预训练，包括一些未得到尝试的语言家族。研究人员评估了预训练语言模型性能如何随着单语言数据集大小、多语言数据集大小、预训练语言相似性和模型大小（最大45M参数）变化。</li>
<li>results: 结果表明，在一定程度上添加多语言数据可以提高低资源语言模型性能，与单语言数据集大小相似的效果。这些改进取决于预训练语言之间的语法相似性，而非词汇重叠。然而，高资源语言在多语言预训练场景下 consistently poor performance。随着数据集大小的增加，添加多语言数据开始对所有语言类型的性能产生负面影响， probable due to limited model capacity（“多语言诅咒”）。这些结果表明，大规模多语言预训练可能不适用于任何语言，但更专注的模型可以显著提高性能。<details>
<summary>Abstract</summary>
Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the "curse of multilinguality"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.
</details>
<details>
<summary>摘要</summary>
多语言语言模型广泛应用于扩展NLP系统到低资源语言。然而，具体的证据表明多语言性对语言模型性能在个体语言中的影响仍然缺乏。在这里，我们预训练了10,000多语言和多语言语言模型，涵盖250种语言，包括一些在NLP中未得到足够研究的语言家族。我们评估了在每种语言中语言模型性能如何随(1)单语言数据集大小、(2)添加多语言数据集大小、(3)添加语言家族之间的语法相似性和(4)模型大小（最多4500万参数）而变化。我们发现，在一定程度上，添加多语言数据可以提高低资源语言模型性能，类似于增加低资源数据集大小，最多提高33%。提高取决于添加的多语言数据中的语法相似性，而词汇重叠也具有有限的效果。然而，高资源语言在多语言预训练场景下一直表现差。随着数据集大小的增加，添加多语言数据开始对低资源语言和高资源语言都有负面影响，可能是因为模型容量的限制（“多语言性的咒”）。这些结果表明，大规模多语言预训练可能不适合任何语言，但更加注重的模型可以很大程度提高性能。
</details></li>
</ul>
<hr>
<h2 id="Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models"><a href="#Structural-Priming-Demonstrates-Abstract-Grammatical-Representations-in-Multilingual-Language-Models" class="headerlink" title="Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models"></a>Structural Priming Demonstrates Abstract Grammatical Representations in Multilingual Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09194">http://arxiv.org/abs/2311.09194</a></li>
<li>repo_url: None</li>
<li>paper_authors: James A. Michaelov, Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen</li>
<li>for: 这paper主要研究了大语言模型中的 grammatical knowledge 的抽象性，以及这种抽象性如何在不同语言之间具有共同的特征。</li>
<li>methods: 研究者使用了大语言模型，并对其进行了跨语言和单语言的结构预期测试，以评估模型中 grammatical knowledge 的抽象性。</li>
<li>results: 研究者发现，大语言模型中的 grammatical knowledge 具有抽象性，并且可以在不同语言之间共同影响文本生成。此外，模型的表现和人类实验结果相似，证明了模型中 grammatical knowledge 的抽象性和人类的语言知识之间的相似性。<details>
<summary>Abstract</summary>
Abstract grammatical knowledge - of parts of speech and grammatical patterns - is key to the capacity for linguistic generalization in humans. But how abstract is grammatical knowledge in large language models? In the human literature, compelling evidence for grammatical abstraction comes from structural priming. A sentence that shares the same grammatical structure as a preceding sentence is processed and produced more readily. Because confounds exist when using stimuli in a single language, evidence of abstraction is even more compelling from crosslingual structural priming, where use of a syntactic structure in one language primes an analogous structure in another language. We measure crosslingual structural priming in large language models, comparing model behavior to human experimental results from eight crosslingual experiments covering six languages, and four monolingual structural priming experiments in three non-English languages. We find evidence for abstract monolingual and crosslingual grammatical representations in the models that function similarly to those found in humans. These results demonstrate that grammatical representations in multilingual language models are not only similar across languages, but they can causally influence text produced in different languages.
</details>
<details>
<summary>摘要</summary>
抽象语法知识 - parts of speech和 grammatical patterns - 是人类语言能力的关键。但是大语言模型中的语法知识如何抽象？我们通过跨语言结构启发来证明语法抽象的存在。在人类文献中，跨语言结构启发提供了吸引人的证据，其中一句语言与之前一句语言的同一个语法结构相似时，对于语言的处理和生成而言更加容易。由于单一语言的干扰因素存在，跨语言结构启发的证据更加吸引人，其中一种语言中的语法结构在另一种语言中引起相似的结构。我们使用大语言模型测量跨语言结构启发，与人类实验结果相比，来自八种cross语言实验和四种单语言实验。我们发现大语言模型中的语法表示存在抽象的特征，与人类中的语法表示类似，并且可以影响不同语言中的文本生成。这些结果表明，多语言语言模型中的语法表示不仅在不同语言之间具有相似性，而且可以 causally 影响不同语言中的文本生成。
</details></li>
</ul>
<hr>
<h2 id="PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health"><a href="#PsyEval-A-Comprehensive-Large-Language-Model-Evaluation-Benchmark-for-Mental-Health" class="headerlink" title="PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health"></a>PsyEval: A Comprehensive Large Language Model Evaluation Benchmark for Mental Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09189">http://arxiv.org/abs/2311.09189</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoan Jin, Siyuan Chen, Mengyue Wu, Kenny Q. Zhu</li>
<li>for: 本研究旨在提供大语言模型（LLM）在心理健康领域的评价标准，填补当前该领域中LLM的评价缺乏的空白。</li>
<li>methods: 本研究使用了六个子任务，涵盖三个维度，系统地评价了八种高级LLM在心理健康领域的能力。</li>
<li>results: 实验结果表明，当前的LLM在心理健康领域仍有很大的提升空间，同时也揭示了未来模型优化的潜在方向。<details>
<summary>Abstract</summary>
Recently, there has been a growing interest in utilizing large language models (LLMs) in mental health research, with studies showcasing their remarkable capabilities, such as disease detection. However, there is currently a lack of a comprehensive benchmark for evaluating the capability of LLMs in this domain. Therefore, we address this gap by introducing the first comprehensive benchmark tailored to the unique characteristics of the mental health domain. This benchmark encompasses a total of six sub-tasks, covering three dimensions, to systematically assess the capabilities of LLMs in the realm of mental health. We have designed corresponding concise prompts for each sub-task. And we comprehensively evaluate a total of eight advanced LLMs using our benchmark. Experiment results not only demonstrate significant room for improvement in current LLMs concerning mental health but also unveil potential directions for future model optimization.
</details>
<details>
<summary>摘要</summary>
近些时间，大语言模型（LLM）在心理健康研究中的应用受到了越来越多的关注，研究显示其惊人的能力，如疾病检测。然而，当前心理健康领域中LLM的能力的全面评估 benchmark 缺乏。因此，我们填补这一空白，引入了心理健康领域的首个全面性 benchmark。这个 benchmark 涵盖了六个子任务，覆盖三个维度，用于系统地评估 LLM 在心理健康领域的能力。我们设计了每个子任务的简洁提示。我们对八个高级 LLM 进行了全面评估，实验结果表明，现有 LLM 在心理健康领域还有很大的提升空间，同时也揭示了未来模型优化的潜在方向。
</details></li>
</ul>
<hr>
<h2 id="Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization"><a href="#Benchmarking-Generation-and-Evaluation-Capabilities-of-Large-Language-Models-for-Instruction-Controllable-Summarization" class="headerlink" title="Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization"></a>Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09184">http://arxiv.org/abs/2311.09184</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yale-nlp/instrusum">https://github.com/yale-nlp/instrusum</a></li>
<li>paper_authors: Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, Arman Cohan</li>
<li>for: 这篇论文旨在研究语言模型（LLM）在更复杂的概要任务设定下的性能，特别是在指定概要特征的情况下。</li>
<li>methods: 作者使用了一组指定文章和自然语言需求来训练 LLM，并对5种基于 LLM 的概要系统进行人工评估。以及使用了4种评估协议和11种 LLM 进行自动评估。</li>
<li>results: 研究发现，对于 LLM 来说，制定概要任务仍然是一项具有挑战性的任务，因为（1）所有评估的 LLM 都会在概要中作出错误和其他类型的错误;（2）所有基于 LLM 的评估方法无法与人类标注员 achieve strong alignment 的质量评估标准;（3）不同的 LLM 在概要生成和评估方面表现出了大的性能差距。<details>
<summary>Abstract</summary>
While large language models (LLMs) already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for the desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation. We make our collected benchmark, InstruSum, publicly available to facilitate future research in this direction.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经在标准化的摘要 benchmark 上 дости得了强大的表现，但它们在更加复杂的摘要任务设定中的表现更少研究。因此，我们将 LLM  benchmark 在 instruction 控制的文本摘要任务中，其中模型输入包括来源文章和自然语言需求摘要特性。为此，我们为这个任务设定了评估对象 dataset 并进行了人类评估 five LLM 摘要系统。然后，我们对 LLM 自动评估的 benchmark 进行了四种评估协议和 eleven LLM 的评估，共计 forty 种评估方法。我们的研究发现， instruction 控制的文本摘要仍然是 LLM 的挑战，因为：1. 所有 LLM 评估都会在摘要中发生实际和其他类型的错误。2. 所有 LLM 基于的评估方法无法与人类评估者在评估候选摘要质量上实现强大的一致。3. 不同的 LLM 在摘要生成和评估中表现出大的性能差异。我们将我们收集的 benchmark， InstruSum，公开供后续研究使用。
</details></li>
</ul>
<hr>
<h2 id="ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models"><a href="#ContraDoc-Understanding-Self-Contradictions-in-Documents-with-Large-Language-Models" class="headerlink" title="ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models"></a>ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09182">http://arxiv.org/abs/2311.09182</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jierui Li, Vipul Raheja, Dhruv Kumar</li>
<li>for: 研究长文档自相矛盾的能力</li>
<li>methods: 使用四个现有的开源和商业可用的大语言模型（GPT3.5、GPT4、PaLM2、LLaMAv2）进行分析</li>
<li>results: GPT4表现最好，可以超越人类的表现，但 ainda有问题，尤其是需要更多的细节和 контекст的自相矛盾。<details>
<summary>Abstract</summary>
In recent times, large language models (LLMs) have shown impressive performance on various document-level tasks such as document classification, summarization, and question-answering. However, research on understanding their capabilities on the task of self-contradictions in long documents has been very limited. In this work, we introduce ContraDoc, the first human-annotated dataset to study self-contradictions in long documents across multiple domains, varying document lengths, self-contradictions types, and scope. We then analyze the current capabilities of four state-of-the-art open-source and commercially available LLMs: GPT3.5, GPT4, PaLM2, and LLaMAv2 on this dataset. While GPT4 performs the best and can outperform humans on this task, we find that it is still unreliable and struggles with self-contradictions that require more nuance and context. We release the dataset and all the code associated with the experiments.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:近期，大型语言模型（LLM）在各种文档级任务上表现出色，如文档分类、概要和问答等。然而，关于 LLMS 在自相矛盾任务上的能力研究却非常有限。在这项工作中，我们介绍了 ContraDoc，首个人类标注的长文档自相矛盾数据集，覆盖多个领域、文档长度、自相矛盾类型和范围。然后，我们分析了四种当前最佳的开源和商业可用 LLM：GPT3.5、GPT4、PaLM2 和 LLaMAv2 在这个数据集上的表现。虽然 GPT4 表现最佳，可以超越人类的表现，但我们发现它在自相矛盾需要更多的细节和上下文时表现不可靠。我们发布了数据集和相关实验代码。
</details></li>
</ul>
<hr>
<h2 id="PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers"><a href="#PEARL-Personalizing-Large-Language-Model-Writing-Assistants-with-Generation-Calibrated-Retrievers" class="headerlink" title="PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers"></a>PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09180">http://arxiv.org/abs/2311.09180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas Gonzalez, Jennifer Neville, Tara Safavi</li>
<li>for: 提高写作和沟通质量和效率</li>
<li>methods: 使用搜索引擎增强大型语言模型的写作助手，以提供个性化的写作 Output</li>
<li>results: 实现了个性化的社交媒体和Reddit评论生成，并且可以用作写作质量预测和优化低质量生成<details>
<summary>Abstract</summary>
Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.
</details>
<details>
<summary>摘要</summary>
强大的大语言模型已经促进了写作助手的开发，这些助手承诺可以大幅提高写作和交流的质量和效率。然而，一个阻碍效果的问题是LLM输出的不具有作者的通信风格和专业知识的个性化。在这篇论文中，我们解决这个挑战，提出了一种基于检索的LLM写作助手，即PEARL。我们的检索器通过选择历史用户自动生成的文档来补充请求，以便最大化LLM生成的个性化效果。我们提出了两项关键新特点来训练我们的检索器：1）一种用于选择可以从属性的请求和文档，以便提高个性化效果；2）一种托管KL散度目标，确保检索器与个性化生成的效果相似。我们示出PEARL在生成工作室社交媒体帖子和Reddit评论中的个性化效果。 finally，我们展示了一种基于生成检索的性能预测器，可以进一步改善低质量生成的LLM链。
</details></li>
</ul>
<hr>
<h2 id="SiRA-Sparse-Mixture-of-Low-Rank-Adaptation"><a href="#SiRA-Sparse-Mixture-of-Low-Rank-Adaptation" class="headerlink" title="SiRA: Sparse Mixture of Low Rank Adaptation"></a>SiRA: Sparse Mixture of Low Rank Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09179">http://arxiv.org/abs/2311.09179</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun Zhu, Nevan Wichers, Chu-Cheng Lin, Xinyi Wang, Tianlong Chen, Lei Shu, Han Lu, Canoee Liu, Liangchen Luo, Jindong Chen, Lei Meng</li>
<li>For: 这篇论文的目的是提出一种简单且高效的推导大型自然语言模型（LoRA），以适应下游任务。* Methods: 这篇论文使用了一种称为“简单混合”的方法，即将LoRA的所有参数都用来适应特定任务。然而，这种方法在实验中被证明是不太有效的。因此，这篇论文提出了一种新的方法，即SiRA，它使用了简单的混合来提高LoRA的性能。* Results: 这篇论文的实验结果显示，SiRA比LoRA和其他混合专家方法在不同单任务和多任务设置中表现更好。<details>
<summary>Abstract</summary>
Parameter Efficient Tuning has been an prominent approach to adapt the Large Language Model to downstream tasks. Most previous works considers adding the dense trainable parameters, where all parameters are used to adapt certain task. We found this less effective empirically using the example of LoRA that introducing more trainable parameters does not help. Motivated by this we investigate the importance of leveraging "sparse" computation and propose SiRA: sparse mixture of low rank adaption. SiRA leverages the Sparse Mixture of Expert(SMoE) to boost the performance of LoRA. Specifically it enforces the top $k$ experts routing with a capacity limit restricting the maximum number of tokens each expert can process. We propose a novel and simple expert dropout on top of gating network to reduce the over-fitting issue. Through extensive experiments, we verify SiRA performs better than LoRA and other mixture of expert approaches across different single tasks and multitask settings.
</details>
<details>
<summary>摘要</summary>
“对大型语言模型进行高效调整”（Parameter Efficient Tuning）是一种广泛使用的方法，以适应下游任务。大多数前一些研究假设添加密集可训练参数，其中所有参数都用于适应特定任务。但我们在LoRA的例子中发现，增加更多的可训练参数并不对Empirical Effective。驱动于此，我们展开了对“稀疏”计算的重要性，并提出了SiRA：稀疏混合低阶适应。SiRA利用Sparse Mixture of Expert（SMoE）来提高LoRA的性能。具体来说，它强制 Top $k$ 专家路由具有容量限制，限制每个专家处理的 Token 最多数量。我们还提出了一种新的简单的专家排除方法，以降低过滤问题。通过广泛的实验，我们证明SiRA在不同的单任务和多任务设置中表现比LoRA和其他混合专家方法更好。
</details></li>
</ul>
<hr>
<h2 id="CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models"><a href="#CLEAN-EVAL-Clean-Evaluation-on-Contaminated-Large-Language-Models" class="headerlink" title="CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models"></a>CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09154">http://arxiv.org/abs/2311.09154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, Hongyuan Lu</li>
<li>for: 评估大语言模型（LLM）的真实能力，因为数据污染导致评估结果不准确。</li>
<li>methods: 提出了一种新的评估方法——Clean-Eval，通过抽象和反编译污染数据，生成表达相同意义的不同表面形式的样本，并使用语义检测器筛选低质量样本，选择最佳样本基于BLEURT分数。</li>
<li>results: Clean-Eval可以准确地评估污染后的LLM表现，并且可以生成新的评估标准。实验表明，Clean-Eval在几个不同场景下能够重新评估污染后的LLM表现。<details>
<summary>Abstract</summary>
We are currently in an era of fierce competition among various large language models (LLMs) continuously pushing the boundaries of benchmark performance. However, genuinely assessing the capabilities of these LLMs has become a challenging and critical issue due to potential data contamination, and it wastes dozens of time and effort for researchers and engineers to download and try those contaminated models. To save our precious time, we propose a novel and useful method, Clean-Eval, which mitigates the issue of data contamination and evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to paraphrase and back-translate the contaminated data into a candidate set, generating expressions with the same meaning but in different surface forms. A semantic detector is then used to filter the generated low-quality samples to narrow down this candidate set. The best candidate is finally selected from this set based on the BLEURT score. According to human assessment, this best candidate is semantically similar to the original contamination data but expressed differently. All candidates can form a new benchmark to evaluate the model. Our experiments illustrate that Clean-Eval substantially restores the actual evaluation results on contaminated LLMs under both few-shot learning and fine-tuning scenarios.
</details>
<details>
<summary>摘要</summary>
现在是一个大型语言模型（LLM）不断推进指标性能的竞争时代。然而，评估这些 LLM 的真实能力已成为一个困难和重要的问题，因为可能存在数据污染，这会浪费研究人员和工程师们很多时间和努力来下载和尝试这些污染的模型。为了保留我们的宝贵时间，我们提出了一种新的方法，即 Clean-Eval，它解决了数据污染问题，并评估 LLM 在更加干净的环境下。Clean-Eval 使用一个 LLM 来重新表述和反翻污染数据，生成表达同一个意义，但表现在不同的表面形式中的候选集。然后，一个Semantic Detector 被用来筛选生成的低质量样本，从而缩小候选集。最后，根据 BLEURT 分数，从候选集中选择最佳候选。根据人工评估，这个最佳候选与原始污染数据具有相同的含义，但表现在不同的表面形式中。所有候选都可以组成一个新的评估标准。我们的实验表明，Clean-Eval 可以减少在污染 LLM 下的实际评估结果的损失，在少量学习和微调学习场景下。
</details></li>
</ul>
<hr>
<h2 id="Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders"><a href="#Grounding-or-Guesswork-Large-Language-Models-are-Presumptive-Grounders" class="headerlink" title="Grounding or Guesswork? Large Language Models are Presumptive Grounders"></a>Grounding or Guesswork? Large Language Models are Presumptive Grounders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09144">http://arxiv.org/abs/2311.09144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Omar Shaikh, Kristina Gligorić, Ashna Khetan, Matthias Gerstgrasser, Diyi Yang, Dan Jurafsky</li>
<li>for: 这个论文主要是研究人工智能和人的对话中的共同基础建立方面。</li>
<li>methods: 这个论文使用了一些对话动作（如 clarify 和 acknowledge）来研究人工智能是否可以成功地建立共同基础。</li>
<li>results: 研究发现现有的大型自然语言处理模型（LLMs）在建立共同基础时偏向假设共同基础的存在，而不使用对话动作来确认共同基础。<details>
<summary>Abstract</summary>
Effective conversation requires common ground: a shared understanding between the participants. Common ground, however, does not emerge spontaneously in conversation. Speakers and listeners work together to both identify and construct a shared basis while avoiding misunderstanding. To accomplish grounding, humans rely on a range of dialogue acts, like clarification (What do you mean?) and acknowledgment (I understand.). In domains like teaching and emotional support, carefully constructing grounding prevents misunderstanding. However, it is unclear whether large language models (LLMs) leverage these dialogue acts in constructing common ground. To this end, we curate a set of grounding acts and propose corresponding metrics that quantify attempted grounding. We study whether LLMs use these grounding acts, simulating them taking turns from several dialogue datasets, and comparing the results to humans. We find that current LLMs are presumptive grounders, biased towards assuming common ground without using grounding acts. To understand the roots of this behavior, we examine the role of instruction tuning and reinforcement learning with human feedback (RLHF), finding that RLHF leads to less grounding. Altogether, our work highlights the need for more research investigating grounding in human-AI interaction.
</details>
<details>
<summary>摘要</summary>
有效的对话需要共同基础：参与者之间的共同理解。然而，这些共同基础不会自然地出现在对话中。说话者和听者需要共同工作，以确定并构建共同基础，并避免错解。人类在教学和情感支持等领域中，会考虑地构建共同基础，以避免错解。然而，是否LLMs会利用对话措施来建立共同基础，这是一个未知的问题。为了解决这个问题，我们筛选了一组共同基础动作，并提出了相应的评价指标。我们研究了LLMs是否使用这些共同基础动作，通过对多个对话集进行模拟，并与人类对话进行比较。我们发现，当前的LLMs具有假设共同基础的倾向，即不使用共同基础动作来建立共同基础。为了了解这种行为的起源，我们研究了指导调整和人类反馈学习（RLHF）的作用，发现RLHF会减少共同基础的使用。总之，我们的工作强调了人机交互中共同基础的研究的重要性。
</details></li>
</ul>
<hr>
<h2 id="RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context"><a href="#RRescue-Ranking-LLM-Responses-to-Enhance-Reasoning-Over-Context" class="headerlink" title="RRescue: Ranking LLM Responses to Enhance Reasoning Over Context"></a>RRescue: Ranking LLM Responses to Enhance Reasoning Over Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09136">http://arxiv.org/abs/2311.09136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yikun Wang, Rui Zheng, Haoming Li, Qi Zhang, Tao Gui, Fei Liu</li>
<li>for: 这篇论文目的是提高大语言模型（LLM）的上下文理解能力，以便更好地应用于响应生成。</li>
<li>methods: 该论文提出了一种新的应用ranking指标来优化LLM的上下文理解，包括人工标注、规则函数和模型蒸馏等方法。</li>
<li>results: 通过使用这种新的应用ranking指标，论文的实验结果表明LLM的上下文理解能力得到了改进，并且在最新的多文档问答 dataset 上达到了更高的成绩。<details>
<summary>Abstract</summary>
Effectively using a given context is paramount for large language models. A context window can include task specifications, retrieved documents, previous conversations, and even model self-reflections, functioning similarly to episodic memory. While efforts are being made to expand the context window, studies indicate that LLMs do not use their context optimally for response generation. In this paper, we present a novel approach to optimize LLMs using ranking metrics, which teaches LLMs to rank a collection of contextually-grounded candidate responses. Rather than a traditional full ordering, we advocate for a partial ordering. This is because achieving consensus on the perfect order for system responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be acquired through human labelers, heuristic functions, or model distillation. We test our system's improved contextual understanding using the latest benchmarks, including a new multi-document question answering dataset. We conduct ablation studies to understand crucial factors, such as how to gather candidate responses, determine their most suitable order, and balance supervised fine-tuning with ranking metrics. Our approach, named RRescue, suggests a promising avenue for enhancing LLMs' contextual understanding via response ranking.
</details>
<details>
<summary>摘要</summary>
使用给定的上下文是大语言模型的关键。上下文窗口可以包括任务规范、检索到的文档、先前的对话和模型自我反思，功能类似于 episodic memory。然而，研究表明，LLMs 不使用上下文最优。在这篇论文中，我们提出了一种新的方法来优化 LLMs，使其可以 ranks 上下文化的候选答案集。而不是传统的全局排序，我们建议使用 partial ordering。这是因为实现完美的上下文排序可能是困难的。我们的 partial ordering 更加稳定， less sensitive to noise，可以通过人工标注、规则函数或模型泛化来获得。我们测试了我们的系统在最新的benchmark中的改进上下文理解，包括一个新的多文档问答数据集。我们进行了ablation study来理解关键因素，如如何收集候选答案、确定其最佳顺序和平衡upervised fine-tuning with ranking metrics。我们的方法，名为 RRescue，建议一种可能提高 LLMs 的上下文理解的 Avenues。
</details></li>
</ul>
<hr>
<h2 id="Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference"><a href="#Aligning-Neural-Machine-Translation-Models-Human-Feedback-in-Training-and-Inference" class="headerlink" title="Aligning Neural Machine Translation Models: Human Feedback in Training and Inference"></a>Aligning Neural Machine Translation Models: Human Feedback in Training and Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09132">http://arxiv.org/abs/2311.09132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miguel Moura Ramos, Patrick Fernandes, António Farinhas, André F. T. Martins</li>
<li>for: 这种技术是为了提高语言模型生成的文本质量，使其更加类似于人类生成的文本。</li>
<li>methods: 这种技术使用人类反馈来训练抽象模型，并在语言模型的训练过程中使用它来改进模型的性能。</li>
<li>results: 这个研究表明，通过integratingquality metrics into the MT pipeline可以提高翻译质量，并且 combining RL training with reranking techniques可以实现显著的提高。<details>
<summary>Abstract</summary>
Reinforcement learning from human feedback (RLHF) is a recent technique to improve the quality of the text generated by a language model, making it closer to what humans would generate. A core ingredient in RLHF's success in aligning and improving large language models (LLMs) is its reward model, trained using human feedback on model outputs. In machine translation (MT), where metrics trained from human annotations can readily be used as reward models, recent methods using minimum Bayes risk decoding and reranking have succeeded in improving the final quality of translation. In this study, we comprehensively explore and compare techniques for integrating quality metrics as reward models into the MT pipeline. This includes using the reward model for data filtering, during the training phase through RL, and at inference time by employing reranking techniques, and we assess the effects of combining these in a unified approach. Our experimental results, conducted across multiple translation tasks, underscore the crucial role of effective data filtering, based on estimated quality, in harnessing the full potential of RL in enhancing MT quality. Furthermore, our findings demonstrate the effectiveness of combining RL training with reranking techniques, showcasing substantial improvements in translation quality.
</details>
<details>
<summary>摘要</summary>
人类反馈学习（RLHF）是一种现代技术，用于改进语言模型生成的文本质量，使其更加类似于人类生成的文本。RLHF的成功一大部分归功于其奖励模型，通过人类反馈来训练。在机器翻译（MT）领域，可以 readily使用人类标注数据来训练奖励模型，最近的方法使用最小极大 Bayes风险解oding和重新排序技术，已经在提高翻译质量方面取得了 significanthy进步。本研究旨在全面探讨和比较在MT管道中 integrateQuality metrics as reward models的技术。这包括使用奖励模型来筛选数据，在训练阶段通过RL进行训练，以及在推理时使用重新排序技术，并评估这些技术的组合效果。我们的实验结果，在多个翻译任务上进行了检验，强调了有效的数据筛选，基于估计的质量，在RL中激发全部的潜力，提高翻译质量。此外，我们的发现还证明了RL训练与重新排序技术的组合可以实现显著的提高翻译质量。
</details></li>
</ul>
<hr>
<h2 id="Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes"><a href="#Social-Meme-ing-Measuring-Linguistic-Variation-in-Memes" class="headerlink" title="Social Meme-ing: Measuring Linguistic Variation in Memes"></a>Social Meme-ing: Measuring Linguistic Variation in Memes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09130">http://arxiv.org/abs/2311.09130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/naitian/semantic-memes">https://github.com/naitian/semantic-memes</a></li>
<li>paper_authors: Naitian Zhou, David Jurgens, David Bamman</li>
<li>for: This paper explores sociolinguistic variation in memes, using a computational pipeline to cluster individual instances of memes into templates and semantic variables.</li>
<li>methods: The paper uses a multimodal approach, taking advantage of the visual templates and text in memes to analyze their semantic function.</li>
<li>results: The study discovers meaningful social variation in meme usage between subreddits, and patterns of meme innovation and acculturation within these communities align with previous findings on written language.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文探索了社会语言变化在抖音中，使用计算机方法将个体抖音划分成模板和语义变量。</li>
<li>methods: 论文采用多Modal方法，利用抖音的视觉模板和文本来分析其语义功能。</li>
<li>results: 研究发现抖音在社区之间存在社会意义的变化，并发现抖音创新和同化在这些社区中与过去语言变化的趋势相吻合。<details>
<summary>Abstract</summary>
Much work in the space of NLP has used computational methods to explore sociolinguistic variation in text. In this paper, we argue that memes, as multimodal forms of language comprised of visual templates and text, also exhibit meaningful social variation. We construct a computational pipeline to cluster individual instances of memes into templates and semantic variables, taking advantage of their multimodal structure in doing so. We apply this method to a large collection of meme images from Reddit and make available the resulting \textsc{SemanticMemes} dataset of 3.8M images clustered by their semantic function. We use these clusters to analyze linguistic variation in memes, discovering not only that socially meaningful variation in meme usage exists between subreddits, but that patterns of meme innovation and acculturation within these communities align with previous findings on written language.
</details>
<details>
<summary>摘要</summary>
很多NLP领域的研究使用计算方法来探索社会语言变化。在这篇论文中，我们 argue That memes，作为Multimodal的语言形式，也存在意义的社会变化。我们构建了一个计算管道来将个体照片分为模板和semantic variable，利用其 Multimodal结构来做此。我们将这些方法应用于Reddit上的大量meme图片集合，并将结果作为\textsc{SemanticMemes}数据集，包含3.8M个图片，按Semantic功能进行分组。我们使用这些分组来分析Memes的语言变化，发现不仅在subreddit之间存在社会意义的变化，而且在这些社区中，meme创新和同化的模式与前面的文本语言发现相似。
</details></li>
</ul>
<hr>
<h2 id="Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark"><a href="#Universal-NER-A-Gold-Standard-Multilingual-Named-Entity-Recognition-Benchmark" class="headerlink" title="Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark"></a>Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09122">http://arxiv.org/abs/2311.09122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephen Mayhew, Terra Blevins, Shuheng Liu, Marek Šuppa, Hila Gonen, Joseph Marvin Imperial, Börje F. Karlsson, Peiqin Lin, Nikola Ljubešić, LJ Miranda, Barbara Plank, Arij Riabi, Yuval Pinter</li>
<li>for: 这 paper 的目的是开发一个开源、社区驱动的项目，以创建多种语言的高质量名实体识别（NER）标准 benchmark。</li>
<li>methods: 这 paper 使用了多种语言的名实体识别数据集，并对其进行了cross-lingual consistent的标注。</li>
<li>results: 这 paper 提供了多种语言的名实体识别数据集，并在不同的语言和学习环境中提供了初步的模型基线。<details>
<summary>Abstract</summary>
We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 18 datasets annotated with named entities in a cross-lingual consistent schema across 12 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We release the data, code, and fitted models to the public.
</details>
<details>
<summary>摘要</summary>
我们介绍Universal NER（UNER），一个开放、社区驱动的项目，旨在开发多种语言的高标准命名实体识别标准。UNER v1包含18个数据集，每个数据集包含多种语言的命名实体，以跨语言一致的方式进行标注。在这篇论文中，我们详细介绍了UNER数据集的创建和组合，以及在本语言和跨语言学习环境中的初步模型基线。我们将数据、代码和适应模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces"><a href="#R-Spin-Efficient-Speaker-and-Noise-invariant-Representation-Learning-with-Acoustic-Pieces" class="headerlink" title="R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces"></a>R-Spin: Efficient Speaker and Noise-invariant Representation Learning with Acoustic Pieces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09117">http://arxiv.org/abs/2311.09117</a></li>
<li>repo_url: None</li>
<li>paper_authors: Heng-Jui Chang, James Glass</li>
<li>for: 这篇论文是为了提出一种数据效率的自主supervised fine-tuning框架，以获得 speaker和噪声不变的语音表示。</li>
<li>methods: 该框架使用 speaker-invariant clustering (Spin) 学习精确的音频单元，并通过预测音频片段来强化内容表示。</li>
<li>results: 相比之前的状态艺术方法，R-Spin 可以在严重扭曲语音场景下获得更好的表示性，同时减少了计算资源的使用量，达到 12 倍的提升。<details>
<summary>Abstract</summary>
This paper introduces Robust Spin (R-Spin), a data-efficient self-supervised fine-tuning framework for speaker and noise-invariant speech representations by learning discrete acoustic units with speaker-invariant clustering (Spin). R-Spin resolves Spin's issues and enhances content representations by learning to predict acoustic pieces. R-Spin offers a 12X reduction in computational resources compared to previous state-of-the-art methods while outperforming them in severely distorted speech scenarios. This paper provides detailed analyses to show how discrete units contribute to speech encoder training and improving robustness in diverse acoustic environments.
</details>
<details>
<summary>摘要</summary>
Translation in Simplified Chinese:这篇论文介绍了一种名为Robust Spin（R-Spin）的数据精简自我超越框架，用于实现Speaker和噪声不变的语音表示。R-Spin解决了Spin的问题，并提高了语音表示的内容。R-Spin可以在严重扭曲的语音enario中具有12倍的计算资源减少，并在前一代方法中出perform。这篇论文还提供了详细的分析，以显示дискреTE Units在语音编码器训练中的贡献和提高 robustness在多种听频环境中。
</details></li>
</ul>
<hr>
<h2 id="“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context"><a href="#“We-Demand-Justice-”-Towards-Grounding-Political-Text-in-Social-Context" class="headerlink" title="“We Demand Justice!”: Towards Grounding Political Text in Social Context"></a>“We Demand Justice!”: Towards Grounding Political Text in Social Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09106">http://arxiv.org/abs/2311.09106</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajkumar Pujari, Chengfei Wu, Dan Goldwasser</li>
<li>for: 本研究旨在 Computational setting中理解ambiguous statements的语言含义，并将其与现实世界相关的实体、行为和态度关联。</li>
<li>methods: 本研究使用了两个具有挑战性的 datasets，需要理解文本的现实世界上下文才能解决 Effectively。此外，还开发了基于现有 ‘Discourse Contextualization Framework’ 和 ‘Political Actor Representation’ 模型的更结构化基elines。</li>
<li>results: 本研究通过对基elines的比较分析，提供了更深入的理解社会语言理解挑战的信息。<details>
<summary>Abstract</summary>
Social media discourse from US politicians frequently consists of 'seemingly similar language used by opposing sides of the political spectrum'. But often, it translates to starkly contrasting real-world actions. For instance, "We need to keep our students safe from mass shootings" may signal either "arming teachers to stop the shooter" or "banning guns to reduce mass shootings" depending on who says it and their political stance on the issue. In this paper, we define and characterize the context that is required to fully understand such ambiguous statements in a computational setting and ground them in real-world entities, actions, and attitudes. To that end, we propose two challenging datasets that require an understanding of the real-world context of the text to be solved effectively. We benchmark these datasets against baselines built upon large pre-trained models such as BERT, RoBERTa, GPT-3, etc. Additionally, we develop and benchmark more structured baselines building upon existing 'Discourse Contextualization Framework' and 'Political Actor Representation' models. We perform analysis of the datasets and baseline predictions to obtain further insights into the pragmatic language understanding challenges posed by the proposed social grounding tasks.
</details>
<details>
<summary>摘要</summary>
社交媒体讨论由美国政客们频繁使用"看起来相似的语言",但实际上它们可能表达出极其不同的现实世界行动。例如，"我们需要保护学生免受大规模枪击"可能表示"武装教师以阻止射手"或"禁止枪支以减少大规模枪击"，这取决于说话人的政治立场。在这篇论文中，我们定义和描述了 Computational Setting中需要完全理解这些抽象语言的上下文，并将其固定到现实世界实体、行动和态度。为此，我们提出了两个复杂的数据集，需要理解文本的现实世界上下文才能解决 effectively。我们对这些数据集进行了基线测试，并开发了基于现有"Discourse Contextualization Framework"和"Political Actor Representation"模型的更结构化基线。我们对数据集和基线预测进行分析，以获得更深入的语言理解挑战的进一步洞察。
</details></li>
</ul>
<hr>
<h2 id="MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation"><a href="#MAVEN-Arg-Completing-the-Puzzle-of-All-in-One-Event-Understanding-Dataset-with-Event-Argument-Annotation" class="headerlink" title="MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation"></a>MAVEN-Arg: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09105">http://arxiv.org/abs/2311.09105</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li</li>
<li>for: This paper is written for the purpose of introducing a new dataset, MAVEN-Arg, which supports event understanding tasks such as event detection, event argument extraction, and event relation extraction.</li>
<li>methods: The paper uses a large-scale dataset, MAVEN-Arg, which is augmented with event argument annotations, to support the development and evaluation of event understanding models.</li>
<li>results: The paper reports that MAVEN-Arg is a challenging dataset for both fine-tuned EAE models and proprietary large language models (LLMs), and demonstrates the potential benefits of an all-in-one dataset for future event prediction applications using LLMs.Here is the information in Simplified Chinese text:</li>
<li>for: 这篇论文是为了介绍一个新的数据集MAVEN-Arg，该数据集支持事件理解任务，包括事件检测、事件Argument提取和事件关系提取。</li>
<li>methods: 这篇论文使用了一个大规模的数据集MAVEN-Arg，该数据集包括事件Argument的注释，以支持事件理解模型的发展和评估。</li>
<li>results: 论文表明，MAVEN-Arg是对于both fine-tuned EAE模型和专有大语言模型（LLMs）来说是一个具有挑战性的数据集，并demonstrates该数据集的可能性用于未来事件预测应用程序。<details>
<summary>Abstract</summary>
Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from https://github.com/THU-KEG/MAVEN-Argument.
</details>
<details>
<summary>摘要</summary>
Understanding events in texts is a core goal of natural language understanding, which involves detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the challenges of annotation, a large-scale dataset covering the full process of event understanding has been lacking. In this paper, we introduce MAVEN-Arg, which adds event argument annotations to the MAVEN datasets, creating the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained through laborious human annotation; (3) exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments at the document level. Experiments show that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and our code can be obtained from <https://github.com/THU-KEG/MAVEN-Argument>.
</details></li>
</ul>
<hr>
<h2 id="Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization"><a href="#Defending-Large-Language-Models-Against-Jailbreaking-Attacks-Through-Goal-Prioritization" class="headerlink" title="Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"></a>Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09096">http://arxiv.org/abs/2311.09096</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-coai/jailbreakdefense_goalpriority">https://github.com/thu-coai/jailbreakdefense_goalpriority</a></li>
<li>paper_authors: Zhexin Zhang, Junxiao Yang, Pei Ke, Minlie Huang</li>
<li>for: 这个论文的目的是提出一种对销害攻击的防御方法，帮助保护大语言模型（LLMs）免受销害攻击。</li>
<li>methods: 该论文使用了目标优先级的思想来防御销害攻击，在训练和推理阶段都实现了目标优先级的 integrating。</li>
<li>results: 该论文的实验结果表明，通过在推理阶段实现目标优先级，可以减少销害攻击的成功率，并且不会影响大语言模型的总体性能。此外，通过在训练阶段实现目标优先级，可以更好地防止销害攻击。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks. While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of exploration into defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the inherent conflict between the goals of being helpful and ensuring safety. To counter jailbreaking attacks, we propose to integrate goal prioritization at both training and inference stages. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to 2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising general performance. Furthermore, integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half, decreasing it from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks. We hope our work could contribute to the comprehension of jailbreaking attacks and defenses, and shed light on the relationship between LLMs' capability and safety. Our code will be available at \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）继续进步，但同时也涉及到一系列的安全隐患。虽然有很多研究利用 LLM 的弱点进行攻击，但对于防御攻击的研究却受到了相对的少量关注。我们指出，在 LLM 中进行干预时存在一个重要的因素，即帮助和安全之间的矛盾。为了防御攻击，我们提议在训练和执行阶段都进行目标优先级化。在执行阶段实现目标优先级化后，可以显著减少攻击成功率（ASR），从66.4%降低至2.0% для ChatGPT，从68.2%降低至19.4% для Vicuna-33B，无需妥协总体性能。此外，在训练阶段 integrate 目标优先级化也可以降低 ASR 至6.6% для LLama2-13B。即使在没有攻击样本的情况下，我们的方法仍可以减少 ASR 的一半，从71.0%降低至34.0%。此外，我们的研究还发现，强大的 LLM 面临更大的安全隐患，但同时它们也拥有更大的防御能力。我们希望我们的工作可以对攻击和防御之间的关系提供更深入的理解，并为 LLM 的安全做出贡献。我们的代码将在 GitHub 上公开，请参考 \url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.
</details></li>
</ul>
<hr>
<h2 id="Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models"><a href="#Social-Bias-Probing-Fairness-Benchmarking-for-Language-Models" class="headerlink" title="Social Bias Probing: Fairness Benchmarking for Language Models"></a>Social Bias Probing: Fairness Benchmarking for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09090">http://arxiv.org/abs/2311.09090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein</li>
<li>for: 本研究旨在探讨语言模型中社会偏见的问题，并提出了一种新的探测方法。</li>
<li>methods: 本研究使用了一种新的词语混杂度-based fairness分数，并收集了一个大规模的探测数据集，以分析语言模型的总体协会以及社会分类、标签和刻板印象方面的偏见。</li>
<li>results: 研究发现，语言模型中的偏见更加复杂，大型模型变体具有更高度的偏见，并且发现不同religion表达的人群在所有模型中产生最大的不同待遇。<details>
<summary>Abstract</summary>
Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.
</details>
<details>
<summary>摘要</summary>
大型语言模型已经显示出了多种社会偏见，这可能导致下游危害。虽然这些偏见的影响已经被认可，但先前的偏见评估方法受限于小型数据集和二元关联测试，这只能提供社会偏见在语言模型中的压缩视图。在这篇论文中，我们提出了一种原创的语言模型偏见探测框架。我们收集了一个探测数据集，以分析语言模型的通用关联以及社会分类、标签和刻板印象的方向。为此，我们利用了一种新的折衣率基准公平分数。我们创建了一个大规模的比较数据集，以解决现有公平集的缺点和限制，扩展到不同的标签和刻板印象。与先前的工作比较，我们发现了更多的偏见在语言模型中，特别是更大的模型变体更加偏见。此外，我们发现了不同的宗教标签表达时，所有模型中的最大差异。
</details></li>
</ul>
<hr>
<h2 id="Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts"><a href="#Identifying-Self-Disclosures-of-Use-Misuse-and-Addiction-in-Community-based-Social-Media-Posts" class="headerlink" title="Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts"></a>Identifying Self-Disclosures of Use, Misuse and Addiction in Community-based Social Media Posts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09066">http://arxiv.org/abs/2311.09066</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N Slavin, Nabila El-Bassel, Smaranda Muresan</li>
<li>For: The paper aims to develop a tool to identify at-risk patients with opioid use disorder by analyzing community-based social media platforms like Reddit.* Methods: The authors use a corpus of 2500 opioid-related posts from various subreddits to annotate span-level extractive explanations and evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting.* Results: The authors find that using explanations during modeling leads to a significant boost in classification accuracy, demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum.Here are the three points in Simplified Chinese text:* For: 这篇论文目标是开发一种用于识别患有酒精使用障碍的患者的工具，通过分析社区基于的Reddit社交媒体平台上的自透露。* Methods: 作者使用2500篇关于酒精的Reddit帖子，并对它们进行分析和注释，以评估一些当前最佳的模型在不同的超级vised、几个shot和零shot设置下的表现。* Results: 作者发现，在高度关键的领域中，使用解释时期的模型会导致识别酒精使用障碍的准确率显著提高，这demonstrates解释的有利role在研究酒精使用障碍continuum中。<details>
<summary>Abstract</summary>
In the last decade, the United States has lost more than 500,000 people from an overdose involving prescription and illicit opioids (https://www.cdc.gov/drugoverdose/epidemic/index.html) making it a national public health emergency (USDHHS, 2017). To more effectively prevent unintentional opioid overdoses, medical practitioners require robust and timely tools that can effectively identify at-risk patients. Community-based social media platforms such as Reddit allow self-disclosure for users to discuss otherwise sensitive drug-related behaviors, often acting as indicators for opioid use disorder. Towards this, we present a moderate size corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details>
<details>
<summary>摘要</summary>
在过去一个十年，美国已经失去了超过500,000名人因为吸毒过量，其中包括药物和黑市药品（https://www.cdc.gov/drugoverdose/epidemic/index.html），这使得这成为一个国家紧急公共卫生问题（USDHHS, 2017）。为了更好地预防意外的毒品过量，医疗专业人员需要强大和时间相对的工具，以有效地识别有风险的病人。社区基础的社交媒体平台如Reddit，allow users to disclose themselves and discuss sensitive drug-related behaviors, often serving as indicators of opioid use disorder. 为此，我们提供了一个 Moderate-sized corpus of 2500 opioid-related posts from various subreddits spanning 6 different phases of opioid use: Medical Use, Misuse, Addiction, Recovery, Relapse, Not Using. For every post, we annotate span-level extractive explanations and crucially study their role both in annotation quality and model development. We evaluate several state-of-the-art models in a supervised, few-shot, or zero-shot setting. Experimental results and error analysis show that identifying the phases of opioid use disorder is highly contextual and challenging. However, we find that using explanations during modeling leads to a significant boost in classification accuracy, demonstrating their beneficial role in a high-stakes domain such as studying the opioid use disorder continuum. The dataset will be made available for research on Github in the formal version.
</details></li>
</ul>
<hr>
<h2 id="Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs"><a href="#Do-Localization-Methods-Actually-Localize-Memorized-Data-in-LLMs" class="headerlink" title="Do Localization Methods Actually Localize Memorized Data in LLMs?"></a>Do Localization Methods Actually Localize Memorized Data in LLMs?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09060">http://arxiv.org/abs/2311.09060</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Yun Chang, Jesse Thomason, Robin Jia</li>
<li>for: 本研究旨在找到LLMs中记忆某个序列的小量神经元。</li>
<li>methods: 本文使用两个benchmark方法评估本地化方法的效果，一个是INJ Benchmark，通过在小量神经元中插入新信息来测试本地化方法的准确性；另一个是DEL Benchmark，通过测试dropout located neurons是否会使模型忘记记忆的序列。</li>
<li>results: 本研究发现，五种本地化方法在两个benchmark上都达到了一定的成果，尤其是使用减少方法时，能够准确地本地化记忆。但是，所identified神经元不一定是特定的一个记忆序列的特征。<details>
<summary>Abstract</summary>
Large language models (LLMs) can memorize many pretrained sequences verbatim. This paper studies if we can locate a small set of neurons in LLMs responsible for memorizing a given sequence. While the concept of localization is often mentioned in prior work, methods for localization have never been systematically and directly evaluated; we address this with two benchmarking approaches. In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pretrained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model. We evaluate five localization methods on our two benchmarks, and both show similar rankings. All methods exhibit promising localization ability, especially for pruning-based methods, though the neurons they identify are not necessarily specific to a single memorized sequence.
</details>
<details>
<summary>摘要</summary>
In our INJ Benchmark, we actively inject a piece of new information into a small subset of LLM weights and measure whether localization methods can identify these "ground truth" weights. In the DEL Benchmark, we study localization of pre-trained data that LLMs have already memorized; while this setting lacks ground truth, we can still evaluate localization by measuring whether dropping out located neurons erases a memorized sequence from the model.We evaluate five localization methods on our two benchmarks, and all show promising localization ability, especially for pruning-based methods. However, the neurons they identify are not necessarily specific to a single memorized sequence.
</details></li>
</ul>
<hr>
<h2 id="GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models"><a href="#GRASP-A-novel-benchmark-for-evaluating-language-GRounding-And-Situated-Physics-understanding-in-multimodal-language-models" class="headerlink" title="GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"></a>GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09048">http://arxiv.org/abs/2311.09048</a></li>
<li>repo_url: None</li>
<li>paper_authors: Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, Elia Bruni</li>
<li>for: 评估视频基于多modal语言模型的语言固定和物理理解能力</li>
<li>methods: 使用Unity simulations进行两 tier评估，包括语言固定和直觉物理理解能力</li>
<li>results: 现有多modal语言模型具有语言固定和直觉物理理解缺陷，GRASP benchmark可以帮助监测未来模型的进步<details>
<summary>Abstract</summary>
This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The initial level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of 'Intuitive Physics' principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in current models' language grounding and intuitive physics. These identified limitations underline the importance of benchmarks like GRASP to monitor the progress of future models in developing these competencies.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了GRASP，一个新的评估语言固定和物理理解能力的视频基于多模态大语言模型（LLM）的benchmark。这种评估方式通过Unity simulate层次结构来实现。第一层测试语言固定的能力，通过将简单的文本描述与视觉信息相关联。第二层测试模型的物理理解能力，包括物体永久性和连续性。此外，我们还使用GRASP评估多种当前领先的多模态LLM。我们的评估发现当前模型的语言固定和直觉物理存在显著的缺陷。这些缺陷证明了GRASP这种benchmark的重要性，以便监测未来模型的发展。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation"><a href="#Exploring-the-Potential-of-Large-Language-Models-in-Computational-Argumentation" class="headerlink" title="Exploring the Potential of Large Language Models in Computational Argumentation"></a>Exploring the Potential of Large Language Models in Computational Argumentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09022">http://arxiv.org/abs/2311.09022</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/llm-argumentation">https://github.com/damo-nlp-sg/llm-argumentation</a></li>
<li>paper_authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing</li>
<li>for: 本研究旨在评估大语言模型（LLMs）在计算辩论领域中的表现，包括零学习和少学习 Setting下的能力。</li>
<li>methods: 本研究使用了多种任务，包括辩论挖掘和辩论生成，以评估LLMs的表现。我们还提供了一个新的对话生成测试集，以全面评估LLMs的综合性能。</li>
<li>results: 实验结果显示LLMs在大多数任务中表现出色，证明它们在计算辩论领域具有remarkable能力。然而，我们也注意到了评估计算辩论的限制，并提供了未来研究的建议。<details>
<summary>Abstract</summary>
Computational argumentation has become an essential tool in various fields, including artificial intelligence, law, and public policy. It is an emerging research field in natural language processing (NLP) that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated strong abilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on various computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models and LLaMA2 models, under zero-shot and few-shot settings within the realm of computational argumentation. We organize existing tasks into 6 main classes and standardise the format of 14 open-sourced datasets. In addition, we present a new benchmark dataset on counter speech generation, that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of these datasets, demonstrating their capabilities in the field of argumentation. We also highlight the limitations in evaluating computational argumentation and provide suggestions for future research directions in this field.
</details>
<details>
<summary>摘要</summary>
计算辩论已成为不同领域的重要工具，包括人工智能、法律和公共政策。这是自然语言处理（NLP）的一个快速发展的研究领域，吸引了更多的关注。研究计算辩论主要涉及两类任务：辩论挖掘和辩论生成。由于大语言模型（LLMs）在理解上下文和生成自然语言方面表现出色，因此值得评估LLMs在不同计算辩论任务中的表现。本工作计划在零 shot和几 shot设置下评估 ChatGPT、Flan 模型和 LLaMA2 模型在计算辩论任务中的表现。我们将现有任务分为 6 个主要类型，并标准化 datasets 的格式。此外，我们还提供了一个新的benchmark dataset，用于全面评估 LLMS 在辩论挖掘和辩论生成任务中的综合表现。广泛的实验表明 LLMS 在大多数 datasets 中表现出色，证明它们在辩论领域的能力。我们还提出了计算辩论评估的限制和未来研究方向。
</details></li>
</ul>
<hr>
<h2 id="End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions"><a href="#End-to-end-Task-oriented-Dialogue-A-Survey-of-Tasks-Methods-and-Future-Directions" class="headerlink" title="End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions"></a>End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and Future Directions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09008">http://arxiv.org/abs/2311.09008</a></li>
<li>repo_url: None</li>
<li>paper_authors: Libo Qin, Wenbo Pan, Qiguang Chen, Lizi Liao, Zhou Yu, Yue Zhang, Wanxiang Che, Min Li</li>
<li>for: 这篇论文主要针对的是End-to-end task-oriented dialogue（EToD）研究领域，旨在提供一份系统性的综述，涵盖该领域的所有方法和最新趋势。</li>
<li>methods: 该论文使用了大量的深度神经网络模型，特别是使用大型预训练模型，以实现EToD研究中的显著进步。</li>
<li>results: 该论文提供了一个综述EToD研究领域的新趋势和前沿领域，并提供了一个公共网站（<a target="_blank" rel="noopener" href="https://etods.net/">https://etods.net/</a>），以便EToD研究人员直接访问最新的进步。<details>
<summary>Abstract</summary>
End-to-end task-oriented dialogue (EToD) can directly generate responses in an end-to-end fashion without modular training, which attracts escalating popularity. The advancement of deep neural networks, especially the successful use of large pre-trained models, has further led to significant progress in EToD research in recent years. In this paper, we present a thorough review and provide a unified perspective to summarize existing approaches as well as recent trends to advance the development of EToD research. The contributions of this paper can be summarized: (1) \textbf{\textit{First survey}: to our knowledge, we take the first step to present a thorough survey of this research field; (2) \textbf{\textit{New taxonomy}: we first introduce a unified perspective for EToD, including (i) \textit{Modularly EToD} and (ii) \textit{Fully EToD}; (3) \textbf{\textit{New Frontiers}: we discuss some potential frontier areas as well as the corresponding challenges, hoping to spur breakthrough research in EToD field; (4) \textbf{\textit{Abundant resources}: we build a public website\footnote{We collect the related papers, baseline projects, and leaderboards for the community at \url{https://etods.net/}.}, where EToD researchers could directly access the recent progress. We hope this work can serve as a thorough reference for the EToD research community.
</details>
<details>
<summary>摘要</summary>
END-TO-END TASK-ORIENTED DIALOGUE (EToD) 可以直接生成响应，无需模块化训练，这已经在过去几年中吸引了越来越多的关注。深度神经网络的发展，特别是大型预训练模型的成功使用，导致了 EToD 研究领域的 significiant progress。在这篇论文中，我们提供了一份系统性的回顾和总结，旨在推动 EToD 研究的发展。本文的贡献包括：1. 首次调查：我们知道的所有文献中，我们是第一个进行这项研究的全面调查。2. 新的分类：我们首先引入了 EToD 的统一视角，包括（i）模块化 EToD 和（ii）完全 EToD。3. 新的前iers：我们讨论了一些潜在的前沿领域，以及相应的挑战，希望能够促进 EToD 领域的突破性研究。4. 充沛的资源：我们建立了一个公共网站（https://etods.net/）， где EToD 研究人员可以直接访问最新的进展。我们希望这份工作能够成为 EToD 研究社区的参考。
</details></li>
</ul>
<hr>
<h2 id="Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance"><a href="#Data-Similarity-is-Not-Enough-to-Explain-Language-Model-Performance" class="headerlink" title="Data Similarity is Not Enough to Explain Language Model Performance"></a>Data Similarity is Not Enough to Explain Language Model Performance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09006">http://arxiv.org/abs/2311.09006</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gyauney/data-similarity-is-not-enough">https://github.com/gyauney/data-similarity-is-not-enough</a></li>
<li>paper_authors: Gregory Yauney, Emily Reif, David Mimno</li>
<li>for: 这个论文旨在探讨语言模型在多种下游任务中的高性能是如何实现的？</li>
<li>methods: 该论文使用了多种同构和示例特定的相似度度量（嵌入-, 字符-和模型基于的）来衡量语言模型在下游任务中的性能。</li>
<li>results: 在多语言任务中，相似度度量与语言模型的性能显著相关，但在其他benchmark中，相似度度量与准确率或者even每个相似度度量之间没有相关性。这表明下游任务和预训练数据之间的关系比较复杂。<details>
<summary>Abstract</summary>
Large language models achieve high performance on many but not all downstream tasks. The interaction between pretraining data and task data is commonly assumed to determine this variance: a task with data that is more similar to a model's pretraining data is assumed to be easier for that model. We test whether distributional and example-specific similarity measures (embedding-, token- and model-based) correlate with language model performance through a large-scale comparison of the Pile and C4 pretraining datasets with downstream benchmarks. Similarity correlates with performance for multilingual datasets, but in other benchmarks, we surprisingly find that similarity metrics are not correlated with accuracy or even each other. This suggests that the relationship between pretraining data and downstream tasks is more complex than often assumed.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output"><a href="#Factcheck-GPT-End-to-End-Fine-Grained-Document-Level-Fact-Checking-and-Correction-of-LLM-Output" class="headerlink" title="Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output"></a>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09000">http://arxiv.org/abs/2311.09000</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yuxiaw/factcheck-gpt">https://github.com/yuxiaw/factcheck-gpt</a></li>
<li>paper_authors: Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, Preslav Nakov</li>
<li>for: 这篇论文旨在提供一个涵盖所有阶段的Annotation scheme来验证大型自然语言模型（LLM）生成的回答的实现方式，以便确保其精度和可靠性。</li>
<li>methods: 这篇论文使用了一个多阶段的Annotation scheme，让评分者能够为LLM生成的回答提供细化的标签，以捕捉回答中的可靠性和事实不一致之处。此外，这篇论文还开发了一个Annotation tool来加速评分过程，并且可以自动插入证据等自动结果。</li>
<li>results: 根据初步实验结果，FactTool、FactScore和Perplexity.ai等工具在验证false claims方面的性能不太理想，其F1分数为0.53。这篇论文提供了一个开放领域的文档级实验库，并且提供了一个网站供下载Annotation tool和代码。<details>
<summary>Abstract</summary>
The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53. Annotation tool, benchmark and code are available at https://github.com/yuxiaw/Factcheck-GPT.
</details>
<details>
<summary>摘要</summary>
通过大语言模型（LLM）在各种实际应用中的普及，需要验证其输出的事实准确性的机制。在这项工作中，我们提出了一种涵盖所有阶段的综合答案，用于标注 LLG 生成的响应中的事实准确性，并设计了一个多Stage annotation scheme，以生成细化的标签，包括 LLG 输出中的可靠性和事实不一致。我们设计了一个用于加速标注过程的标注工具，并且可以自动 incorporate 任何阶段的自动结果，例如自动检索到的证据。我们还构建了一个开放领域文档级别的事实准确性标准吗，包括声明、句子和文档三个级别。我们的初步实验表明，FacTool、FactScore 和 Perplexity.ai 在标识false声明方面的最佳 F1 值为 0.53。我们的标注工具、标准吗和代码可以在 GitHub 上获取。
</details></li>
</ul>
<hr>
<h2 id="SentAlign-Accurate-and-Scalable-Sentence-Alignment"><a href="#SentAlign-Accurate-and-Scalable-Sentence-Alignment" class="headerlink" title="SentAlign: Accurate and Scalable Sentence Alignment"></a>SentAlign: Accurate and Scalable Sentence Alignment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08982">http://arxiv.org/abs/2311.08982</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/steinst/sentalign">https://github.com/steinst/sentalign</a></li>
<li>paper_authors: Steinþór Steingrímsson, Hrafn Loftsson, Andy Way</li>
<li>for: 该论文设计了一个高精度的句子对齐工具，用于处理非常大的平行文档对。</li>
<li>methods: 该算法使用用户定义的参数，采用分治分解方法对大量句子进行对齐，并使用LaBSE双语句子表示来评分。</li>
<li>results:  SentAlign在德语-法语和英语-冰岛语两个评估集上表现出色，并在下游机器翻译任务中表现更好。<details>
<summary>Abstract</summary>
We present SentAlign, an accurate sentence alignment tool designed to handle very large parallel document pairs. Given user-defined parameters, the alignment algorithm evaluates all possible alignment paths in fairly large documents of thousands of sentences and uses a divide-and-conquer approach to align documents containing tens of thousands of sentences. The scoring function is based on LaBSE bilingual sentence representations. SentAlign outperforms five other sentence alignment tools when evaluated on two different evaluation sets, German-French and English-Icelandic, and on a downstream machine translation task.
</details>
<details>
<summary>摘要</summary>
我们介绍了 SentAlign，一款精度很高的句子对齐工具，可以处理非常大的平行文档对。通过用户定义的参数，对齐算法会评估所有可能的对齐路径，并使用分治分解方法对文档中的千余句进行对齐。对齐函数基于 LaBSE 双语句表示。 SentAlign 在两个不同的评估集上（德语-法语和英语-冰岛语）和下游机器翻译任务上表现出色，超越了五个其他句子对齐工具。
</details></li>
</ul>
<hr>
<h2 id="Speculative-Contrastive-Decoding"><a href="#Speculative-Contrastive-Decoding" class="headerlink" title="Speculative Contrastive Decoding"></a>Speculative Contrastive Decoding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08981">http://arxiv.org/abs/2311.08981</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou</li>
<li>for: 提高大语言模型（LLM）的推断质量和速度</li>
<li>methods: 使用 amateur models 预测专家模型的生成，并使用自然冲突来优化推断结果</li>
<li>results: 实验结果表明，使用 Speculative Contrastive Decoding（SCD）可以达到类似的加速因子，同时提高推断质量，并且可以减少计算资源的消耗<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown extraordinary performance in various language tasks, but high computational requirements hinder their widespread deployment. Speculative decoding, which uses amateur models to predict the generation of expert models, has been proposed as a way to accelerate LLM inference. However, speculative decoding focuses on acceleration instead of making the best use of the token distribution from amateur models. We proposed Speculative Contrastive Decoding (SCD), an accelerated decoding method leveraging the natural contrast between expert and amateur models in speculative decoding. Comprehensive evaluations on four benchmarks show that SCD can achieve similar acceleration factors as speculative decoding while further improving the generation quality as the contrastive decoding. The analysis of token probabilities further demonstrates the compatibility between speculative and contrastive decoding. Overall, SCD provides an effective approach to enhance the decoding quality of LLMs while saving computational resources.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer"><a href="#Improving-Large-scale-Deep-Biasing-with-Phoneme-Features-and-Text-only-Data-in-Streaming-Transducer" class="headerlink" title="Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer"></a>Improving Large-scale Deep Biasing with Phoneme Features and Text-only Data in Streaming Transducer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08966">http://arxiv.org/abs/2311.08966</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jin Qiu, Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma</li>
<li>for: 提高流式自动语音识别（ASR）中罕见词或上下文实体的识别性能。</li>
<li>methods: combine拟音和文本信息，以 distinguishing 同音或同字符序列的词语。</li>
<li>results: 在LibriSpeech corpus上，提出的方法实现了不同规模和偏好列表的罕见词错误率的国际先进性。<details>
<summary>Abstract</summary>
Deep biasing for the Transducer can improve the recognition performance of rare words or contextual entities, which is essential in practical applications, especially for streaming Automatic Speech Recognition (ASR). However, deep biasing with large-scale rare words remains challenging, as the performance drops significantly when more distractors exist and there are words with similar grapheme sequences in the bias list. In this paper, we combine the phoneme and textual information of rare words in Transducers to distinguish words with similar pronunciation or spelling. Moreover, the introduction of training with text-only data containing more rare words benefits large-scale deep biasing. The experiments on the LibriSpeech corpus demonstrate that the proposed method achieves state-of-the-art performance on rare word error rate for different scales and levels of bias lists.
</details>
<details>
<summary>摘要</summary>
深层偏迁对扬声器可以改善不同语言模型中的识别性能，尤其是在实时自动语音识别（ASR）应用中。然而，深层偏迁大规模罕见词仍然存在挑战，因为性能下降很快，有许多干扰符和类似的字符序列存在偏迁列表中。在这篇论文中，我们将扬声器中罕见词的音频和文本信息结合起来，以便在同音或同字符序列时分词。此外，在训练文本只含罕见词数据时，大规模深层偏迁的训练效果也得到了改进。在 LibriSpeech 数据集上进行的实验表明，提出的方法可以在不同的规模和偏迁列表水平上取得状态的词错率最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models"><a href="#Self-Improving-for-Zero-Shot-Named-Entity-Recognition-with-Large-Language-Models" class="headerlink" title="Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models"></a>Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08921">http://arxiv.org/abs/2311.08921</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang</li>
<li>for:  investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy.</li>
<li>methods:  utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER, and explore various strategies to select reliable samples from the self-annotated dataset as demonstrations.</li>
<li>results:  achieve an obvious performance improvement, and there might still be space for improvement via more advanced strategy for reliable entity selection.Here’s the full text in Simplified Chinese:</li>
<li>for: 本研究旨在探索利用大型自然语言模型（LLM）进行零shotNamed Entity Recognition（NER）任务的可能性，并提出一种无需训练的自我改进策略。</li>
<li>methods: 我们利用一个无标注语料来刺激LLM的自我学习能力，并考虑了多种策略来选择自动标注数据中的可靠示例作为示例。</li>
<li>results: 我们的研究发现，使用自我改进策略可以进一步推动零shotNER的发展，并实现显著的性能提升。此外，我们还发现，简单地增加无标注语料或 iterative self-improving 并不能保证改进。<details>
<summary>Abstract</summary>
Exploring the application of powerful large language models (LLMs) on the fundamental named entity recognition (NER) task has drawn much attention recently. This work aims to investigate the possibilities of pushing the boundary of zero-shot NER with LLM via a training-free self-improving strategy. We propose a self-improving framework, which utilize an unlabeled corpus to stimulate the self-learning ability of LLMs on NER. First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Second, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, considering the similarity, diversity and reliability of demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations. Through comprehensive experimental analysis, our study yielded the following findings: (1) The self-improving framework further pushes the boundary of zero-shot NER with LLMs, and achieves an obvious performance improvement; (2) Iterative self-improving or naively increasing the size of unlabeled corpus does not guarantee improvements; (3) There might still be space for improvement via more advanced strategy for reliable entity selection.
</details>
<details>
<summary>摘要</summary>
First, we use LLM to make predictions on the unlabeled corpus and obtain the self-annotated data. Next, we explore various strategies to select reliable samples from the self-annotated dataset as demonstrations, taking into account the similarity, diversity, and reliability of the demonstrations. Finally, we conduct inference for the test query via in-context learning with the selected self-annotated demonstrations.Our comprehensive experimental analysis yielded the following findings:1. The self-improving framework further pushes the boundary of zero-shot NER with LLMs, achieving an obvious performance improvement.2. Iterative self-improving or simply increasing the size of the unlabeled corpus does not guarantee improvements.3. There may still be room for improvement via more advanced strategies for selecting reliable entities.
</details></li>
</ul>
<hr>
<h2 id="HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence"><a href="#HELLaMA-LLaMA-based-Table-to-Text-Generation-by-Highlighting-the-Important-Evidence" class="headerlink" title="HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence"></a>HELLaMA: LLaMA-based Table to Text Generation by Highlighting the Important Evidence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08896">http://arxiv.org/abs/2311.08896</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Bian, Xiaolei Qin, Wuhe Zou, Mengzuo Huang, Weidong Zhang</li>
<li>for: 这个论文主要是为了提出一种基于大语言模型的表格转文本方法，以优化表格转文本任务的性能。</li>
<li>methods: 这个方法使用了两个模块：一个表格理解器，用于从表格中提取相关的行数据，以及一个表格摘要生成器，用于基于高亮的表格生成文本。此外， authors还提出了一种搜索策略来生成表格理解 Label。</li>
<li>results: 在FetaQA和QTSumm数据集上，该方法达到了当前最佳的STATE-OF-THE-ARTResults，并且发现高亮输入表格可以显著提高模型的性能，同时提供有价值的解释性。<details>
<summary>Abstract</summary>
Large models have demonstrated significant progress across various domains, particularly in tasks related to text generation. In the domain of Table to Text, many Large Language Model (LLM)-based methods currently resort to modifying prompts to invoke public APIs, incurring potential costs and information leaks. With the advent of open-source large models, fine-tuning LLMs has become feasible. In this study, we conducted parameter-efficient fine-tuning on the LLaMA2 model. Distinguishing itself from previous fine-tuning-based table-to-text methods, our approach involves injecting reasoning information into the input by emphasizing table-specific row data. Our model consists of two modules: 1) a table reasoner that identifies relevant row evidence, and 2) a table summarizer that generates sentences based on the highlighted table. To facilitate this, we propose a search strategy to construct reasoning labels for training the table reasoner. On both the FetaQA and QTSumm datasets, our approach achieved state-of-the-art results. Additionally, we observed that highlighting input tables significantly enhances the model's performance and provides valuable interpretability.
</details>
<details>
<summary>摘要</summary>
大型模型在不同领域的任务中已经实现了显著的进步，尤其是在文本生成相关的任务中。在表格到文本领域，许多大语言模型（LLM）基于方法通常是修改提示来访问公共API，可能会导致潜在的成本和信息泄露。随着开源大型模型的出现，细化LLM成为可能。在这项研究中，我们进行了效率高的参数调整LLaMA2模型。与前期 Fine-tuning 基于表格到文本方法不同，我们的方法是通过强调表格特定的行数据来注入逻辑信息。我们的模型包括两个模块：1）表格逻辑器，用于确定相关的行证据；2）表格概要生成器，用于基于突出的表格生成句子。为了实现这一点，我们提议一种搜索策略来构建逻辑标签用于训练表格逻辑器。在FetaQA和QTSumm数据集上，我们的方法实现了状态的最佳结果。此外，我们发现高亮输入表格会显著提高模型的性能并提供有价值的解释性。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM"><a href="#Large-Language-Models-are-legal-but-they-are-not-Making-the-case-for-a-powerful-LegalLLM" class="headerlink" title="Large Language Models are legal but they are not: Making the case for a powerful LegalLLM"></a>Large Language Models are legal but they are not: Making the case for a powerful LegalLLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08890">http://arxiv.org/abs/2311.08890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thanmay Jayakumar, Fauzan Farooqui, Luqman Farooqui</li>
<li>for: 本研究目的是评估通用语言模型在法律领域的性能，以及与专门为法律领域开发的模型进行比较。</li>
<li>methods: 本研究使用了三个通用语言模型（ChatGPT-20b、LLaMA-2-70b和Falcon-180b），对LEDGAR子集进行零shot测试，以评估这些模型在合同提供分类任务中的性能。</li>
<li>results: 研究发现，通用语言模型可以在大多数情况下正确地分类主题，但是它们的mic-F1&#x2F;mac-F1性能与特定于法律领域的小型模型相比，可能下降到19.2&#x2F;26.8％。这表明，为法律领域开发更强大的语言模型是有必要的。<details>
<summary>Abstract</summary>
Realizing the recent advances in Natural Language Processing (NLP) to the legal sector poses challenging problems such as extremely long sequence lengths, specialized vocabulary that is usually only understood by legal professionals, and high amounts of data imbalance. The recent surge of Large Language Models (LLMs) has begun to provide new opportunities to apply NLP in the legal domain due to their ability to handle lengthy, complex sequences. Moreover, the emergence of domain-specific LLMs has displayed extremely promising results on various tasks. In this study, we aim to quantify how general LLMs perform in comparison to legal-domain models (be it an LLM or otherwise). Specifically, we compare the zero-shot performance of three general-purpose LLMs (ChatGPT-20b, LLaMA-2-70b, and Falcon-180b) on the LEDGAR subset of the LexGLUE benchmark for contract provision classification. Although the LLMs were not explicitly trained on legal data, we observe that they are still able to classify the theme correctly in most cases. However, we find that their mic-F1/mac-F1 performance is up to 19.2/26.8\% lesser than smaller models fine-tuned on the legal domain, thus underscoring the need for more powerful legal-domain LLMs.
</details>
<details>
<summary>摘要</summary>
现在的自然语言处理（NLP）技术在法律领域中提供了挑战性的问题，例如非常长的序列长度、专业legal vocabulary和大量数据不均衡。最近的大语言模型（LLMs）已经开始为法律领域提供新的应用机会，因为它们可以处理长、复杂的序列。此外，域 específico LLMS 的出现已经在多个任务上显示出非常有 promise。在本研究中，我们想要量化一般 LLMS 与法律领域模型（LLM或其他）的比较。我们比较三个一般用途 LLMS（ChatGPT-20b、LLaMA-2-70b和Falcon-180b）在 LEDGAR 子集上的零shot性性能。尽管 LLMS 没有直接接触法律数据，但我们发现它们仍然可以正确地分类主题。然而，我们发现它们的 mic-F1/mac-F1 性能与小型法律领域模型 fine-tuned 的性能相比，下降到 19.2/26.8%，这emet underscore the need for more powerful legal-domain LLMS。
</details></li>
</ul>
<hr>
<h2 id="CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building"><a href="#CLIMB-Curriculum-Learning-for-Infant-inspired-Model-Building" class="headerlink" title="CLIMB: Curriculum Learning for Infant-inspired Model Building"></a>CLIMB: Curriculum Learning for Infant-inspired Model Building</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08886">http://arxiv.org/abs/2311.08886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Richard Diehl Martinez, Zebulon Goriely, Hope McGovern, Christopher Davis, Andrew Caines, Paula Buttery, Lisa Beinborn</li>
<li>for: 本研究是为了提高语言模型的性能，并 investigate cognitively-motivated curriculum learning的效果。</li>
<li>methods: 本研究使用了三种不同的认知驱动的课程学习方法，包括词汇课程、数据课程和目标课程。</li>
<li>results: 研究发现，使用不同的课程学习方法可以获得一些有限的改善，但是不一致地改善所有语言测试任务。研究还发现，选择合适的模型架构和训练参数可以获得较好的改善。<details>
<summary>Abstract</summary>
We describe our team's contribution to the STRICT-SMALL track of the BabyLM Challenge. The challenge requires training a language model from scratch using only a relatively small training dataset of ten million words. We experiment with three variants of cognitively-motivated curriculum learning and analyze their effect on the performance of the model on linguistic evaluation tasks. In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves. In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model. In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities. Our results did not yield consistent improvements over our own non-curriculum learning baseline across a range of linguistic benchmarks; however, we do find marginal gains on select tasks. Our analysis highlights key takeaways for specific combinations of tasks and settings which benefit from our proposed curricula. We moreover determine that careful selection of model architecture, and training hyper-parameters yield substantial improvements over the default baselines provided by the BabyLM challenge.
</details>
<details>
<summary>摘要</summary>
我们描述我们团队在STRICT-SMALL track上的 BabyLM 挑战中的贡献。挑战需要从头开始训练一个语言模型，只使用一个相对较小的训练集数据量为十万个单词。我们在语言评估任务中运行三种认知驱动的课程学习方法，并分析它们对模型性能的影响。在词汇课程中，我们分析了在初期训练阶段限制词汇的方法，以模拟更加认知可能的学习曲线。在数据课程实验中，我们变化了训练实例的顺序，根据i) 婴儿引发的期望和ii) 模型学习行为。在目标课程中，我们探索不同的拟合面见任务和更粗糙的词类预测任务的结合方式，以强化语言总结能力。我们的结果没有在一系列语言标准准点上得到了一致的改进，但我们发现了一些任务上的微妙改进。我们的分析强调特定任务和设置中的课程学习的优点。此外，我们发现选择模型架构和训练超参数可以提供substantial改进。
</details></li>
</ul>
<hr>
<h2 id="Enabling-Large-Language-Models-to-Learn-from-Rules"><a href="#Enabling-Large-Language-Models-to-Learn-from-Rules" class="headerlink" title="Enabling Large Language Models to Learn from Rules"></a>Enabling Large Language Models to Learn from Rules</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08883">http://arxiv.org/abs/2311.08883</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen</li>
<li>for: 本研究旨在探讨使用规则来帮助大型自然语言模型（LLM）学习新的任务或知识。</li>
<li>methods: 我们提出了一种名为规则浸泡的方法，它首先使用LLM的强 Context-Aware 能力提取规则中的知识，然后将知识Explicitly 编码到LLM 参数中，通过学习内部的信号来帮助LLM 学习。</li>
<li>results: 我们的实验结果显示，使用我们的方法可以让LLM更加快速地学习新任务或知识，并且在样本数量和泛化能力方面都比例例-based 学习更高效。<details>
<summary>Abstract</summary>
Large language models (LLMs) have shown incredible performance in completing various real-world tasks. The current knowledge learning paradigm of LLMs is mainly based on learning from examples, in which LLMs learn the internal rule implicitly from a certain number of supervised examples. However, the learning paradigm may not well learn those complicated rules, especially when the training examples are limited. We are inspired that humans can learn the new tasks or knowledge in another way by learning from rules. That is, humans can grasp the new tasks or knowledge quickly and generalize well given only a detailed rule and a few optional examples. Therefore, in this paper, we aim to explore the feasibility of this new learning paradigm, which encodes the rule-based knowledge into LLMs. We propose rule distillation, which first uses the strong in-context abilities of LLMs to extract the knowledge from the textual rules and then explicitly encode the knowledge into LLMs' parameters by learning from the above in-context signals produced inside the model. Our experiments show that making LLMs learn from rules by our method is much more efficient than example-based learning in both the sample size and generalization ability.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation"><a href="#Llamas-Know-What-GPTs-Don’t-Show-Surrogate-Models-for-Confidence-Estimation" class="headerlink" title="Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation"></a>Llamas Know What GPTs Don’t Show: Surrogate Models for Confidence Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08877">http://arxiv.org/abs/2311.08877</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vaishnavi Shrivastava, Percy Liang, Ananya Kumar</li>
<li>for: 提高LLM的可靠性，使其在问答 зада中准确地表达自己的信任度。</li>
<li>methods: 使用语言模型来描述自己的信任度，并使用一个伪装的信任模型来评估原始模型的信任度。</li>
<li>results: 使用这两种方法可以获得更高的AUC值（84.6%平均值），提高LLM的可靠性。<details>
<summary>Abstract</summary>
To maintain user trust, large language models (LLMs) should signal low confidence on examples where they are incorrect, instead of misleading the user. The standard approach of estimating confidence is to use the softmax probabilities of these models, but as of November 2023, state-of-the-art LLMs such as GPT-4 and Claude-v1.3 do not provide access to these probabilities. We first study eliciting confidence linguistically -- asking an LLM for its confidence in its answer -- which performs reasonably (80.5% AUC on GPT-4 averaged across 12 question-answering datasets -- 7% above a random baseline) but leaves room for improvement. We then explore using a surrogate confidence model -- using a model where we do have probabilities to evaluate the original model's confidence in a given question. Surprisingly, even though these probabilities come from a different and often weaker model, this method leads to higher AUC than linguistic confidences on 9 out of 12 datasets. Our best method composing linguistic confidences and surrogate model probabilities gives state-of-the-art confidence estimates on all 12 datasets (84.6% average AUC on GPT-4).
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:以维护用户信任，大型自然语言模型（LLMs）应该在错误的示例上显示低自信，而不是误导用户。标准的自信估计方法是使用这些模型的软条对应的概率，但在2023年11月，现场的LMMs如GPT-4和Claude-v1.3并不提供这些概率。我们首先研究用于描述自信的语言方法 -- 将LMM询问自己的答案中的自信度 -- 这perform reasonably well（GPT-4的80.5% AUC在12个问答dataset上的平均值上升7%），但还有改善的空间。我们然后探索使用代理自信模型 -- 使用一个拥有概率的模型来评估原始模型在特定问题上的自信度。 surprisingly，这些概率来自不同和常较弱的模型，这种方法在12个dataset上高于语言自信的AUC（84.6%的GPT-4平均值）。我们的最佳方法是融合语言自信和代理模型概率，得到了现场的自信估计（84.6%的GPT-4平均值）。
</details></li>
</ul>
<hr>
<h2 id="OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining"><a href="#OFA-A-Framework-of-Initializing-Unseen-Subword-Embeddings-for-Efficient-Large-scale-Multilingual-Continued-Pretraining" class="headerlink" title="OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining"></a>OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08849">http://arxiv.org/abs/2311.08849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Liu, Peiqin Lin, Mingyang Wang, Hinrich Schütze</li>
<li>for: 本文旨在提出一种高效的多语言语模型适应方法，以提高适应多语言语言模型的效率和可行性。</li>
<li>methods: 本文提出了一种名为\textbf{\textsc{Ofa}}的框架，它通过智能初始化目标语言中未看到的字词的embeddings来适应多语言语言模型。\textsc{Ofa}使用了外部的多语言word embeddings，并将它们的对应关系注入到新的embeddings中。此外，\textsc{Ofa}还应用了矩阵因子分解，将高维的embeddings替换为两个低维的矩阵，从而减少参数的数量。</li>
<li>results: 经过广泛的实验表明，由\textsc{Ofa}初始化的模型能够高效地适应多语言语言模型，并在多种下沉任务上表现出色。此外，\textsc{Ofa}不仅加速了继续预训的整合，还提高了零Instance cross语言传递性。<details>
<summary>Abstract</summary>
Pretraining multilingual language models from scratch requires considerable computational resources and substantial training data. Therefore, a more efficient method is to adapt existing pretrained language models (PLMs) to new languages via vocabulary extension and continued pretraining. However, this method usually randomly initializes the embeddings of new subwords and introduces substantially more embedding parameters to the language model, thus weakening the efficiency. To address these issues, we propose a novel framework: \textbf{O}ne \textbf{F}or \textbf{A}ll (\textbf{\textsc{Ofa}), which wisely initializes the embeddings of unseen subwords from target languages and thus can adapt a PLM to multiple languages efficiently and effectively. \textsc{Ofa} takes advantage of external well-aligned multilingual word embeddings and injects the alignment knowledge into the new embeddings. In addition, \textsc{Ofa} applies matrix factorization and replaces the cumbersome embeddings with two lower-dimensional matrices, which significantly reduces the number of parameters while not sacrificing the performance. Through extensive experiments, we show models initialized by \textsc{Ofa} are efficient and outperform several baselines. \textsc{Ofa} not only accelerates the convergence of continued pretraining, which is friendly to a limited computation budget, but also improves the zero-shot crosslingual transfer on a wide range of downstream tasks. We make our code and models publicly available.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>现有的多语言语言模型（PLM）预训练需要较大的计算资源和大量的训练数据。因此，一种更有效的方法是使用现有的 PLM 进行多语言适应，通过词库扩展和继续预训练。但这种方法通常会随机初始化目标语言中的新词表示，并添加大量的词表示参数到语言模型中，从而降低效率。为解决这些问题，我们提出了一个新的框架：\textbf{一个 для所有} (\textbf{\textsc{Ofa}), 它智能初始化目标语言中的未看过词表示，并可以快速和有效地将 PLM 适应多种语言。\textsc{Ofa} 利用外部的多语言Word embeddings 和注入对应关系知识，并应用矩阵分解，将繁琐的词表示替换为两个更低维度的矩阵，这样减少了参数的数量，而不会降低性能。经过广泛的实验，我们发现模型使用 \textsc{Ofa} 初始化的效果更好，并在多种下游任务上实现了零shot Cross-Lingual 传递。\textsc{Ofa} 不仅加速了继续预训练的整合，也提高了零shot Cross-Lingual 传递的性能，这对有限的计算预算是友好的。我们将代码和模型公开发布。
</details></li>
</ul>
<hr>
<h2 id="Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder"><a href="#Violet-A-Vision-Language-Model-for-Arabic-Image-Captioning-with-Gemini-Decoder" class="headerlink" title="Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder"></a>Violet: A Vision-Language Model for Arabic Image Captioning with Gemini Decoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08844">http://arxiv.org/abs/2311.08844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abdelrahman Mohamed, Fakhraddin Alwajih, El Moatez Billah Nagoudi, Alcides Alcoba Inciarte, Muhammad Abdul-Mageed</li>
<li>for: 本研究的目的是提高阿拉伯语言图像描述的水平，提供更多的泛型语言模型。</li>
<li>methods: 本研究使用了视觉编码器和 Gemini 文本解码器，以实现视觉和语言组件的融合。同时，我们还提出了一种自动从英语数据集中获取数据的新方法。</li>
<li>results: 对于我们的评估数据集，\textit{Violet} 表现出了显著的提升，例如在我们手动标注的数据集上达到了 CIDEr 分数为 61.2，并在 Flickr8k 上提高了13个点。<details>
<summary>Abstract</summary>
Although image captioning has a vast array of applications, it has not reached its full potential in languages other than English. Arabic, for instance, although the native language of more than 400 million people, remains largely underrepresented in this area. This is due to the lack of labeled data and powerful Arabic generative models. We alleviate this issue by presenting a novel vision-language model dedicated to Arabic, dubbed \textit{Violet}. Our model is based on a vision encoder and a Gemini text decoder that maintains generation fluency while allowing fusion between the vision and language components. To train our model, we introduce a new method for automatically acquiring data from available English datasets. We also manually prepare a new dataset for evaluation. \textit{Violet} performs sizeably better than our baselines on all of our evaluation datasets. For example, it reaches a CIDEr score of $61.2$ on our manually annotated dataset and achieves an improvement of $13$ points on Flickr8k.
</details>
<details>
<summary>摘要</summary>
To train our model, we introduce a new method for automatically acquiring data from existing English datasets. Additionally, we manually prepare a new dataset for evaluation. Compared to our baselines, Violet performs significantly better on all of our evaluation datasets. For example, it achieves a CIDEr score of 61.2 on our manually annotated dataset and improves by 13 points on Flickr8k.
</details></li>
</ul>
<hr>
<h2 id="Disinformation-Capabilities-of-Large-Language-Models"><a href="#Disinformation-Capabilities-of-Large-Language-Models" class="headerlink" title="Disinformation Capabilities of Large Language Models"></a>Disinformation Capabilities of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08838">http://arxiv.org/abs/2311.08838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kinit-sk/disinformation-capabilities">https://github.com/kinit-sk/disinformation-capabilities</a></li>
<li>paper_authors: Ivan Vykopal, Matúš Pikuliak, Ivan Srba, Robert Moro, Dominik Macko, Maria Bielikova</li>
<li>for: 本研究探讨了现代语言模型（LLM）可能在扩散假新闻方面的能力，以及这些能力对民主社会的影响。</li>
<li>methods: 研究使用20个假新闻narritives测试了10个LLM的能力，包括生成新闻文章的质量、与假新闻narritives的倾向度、生成安全警告等方面。</li>
<li>results: 研究发现，LLMs可以生成有力的新闻文章，并且往往同意危险的假新闻narritives。此外，检测模型也能够准确地检测LLM生成的假新闻文章。<details>
<summary>Abstract</summary>
Automated disinformation generation is often listed as one of the risks of large language models (LLMs). The theoretical ability to flood the information space with disinformation content might have dramatic consequences for democratic societies around the world. This paper presents a comprehensive study of the disinformation capabilities of the current generation of LLMs to generate false news articles in English language. In our study, we evaluated the capabilities of 10 LLMs using 20 disinformation narratives. We evaluated several aspects of the LLMs: how well they are at generating news articles, how strongly they tend to agree or disagree with the disinformation narratives, how often they generate safety warnings, etc. We also evaluated the abilities of detection models to detect these articles as LLM-generated. We conclude that LLMs are able to generate convincing news articles that agree with dangerous disinformation narratives.
</details>
<details>
<summary>摘要</summary>
自动化假信息生成是大语言模型（LLM）的风险之一。这种理论上的信息淹没能力可能对世界各地的民主社会造成巨大的影响。本文提供了现代大语言模型对英语新闻文章的假信息生成能力的全面研究。我们在这种研究中评估了10个LLM的表现，使用20个假信息 narraative。我们评估了这些LLM的新闻文章生成能力、假信息narraative的同意程度、安全警告的生成频率等方面。我们还评估了检测模型对这些文章是否能够检测出LLM生成的能力。我们结论是，LLM可以生成有力的新闻文章，并与危险的假信息narraative相符。
</details></li>
</ul>
<hr>
<h2 id="StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving"><a href="#StrategyLLM-Large-Language-Models-as-Strategy-Generators-Executors-Optimizers-and-Evaluators-for-Problem-Solving" class="headerlink" title="StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving"></a>StrategyLLM: Large Language Models as Strategy Generators, Executors, Optimizers, and Evaluators for Problem Solving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08803">http://arxiv.org/abs/2311.08803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam</li>
<li>for: 提高Chain-of-thought（CoT）提问方法的普适性和一致性，以解决现有方法的通用性和任务级别一致性问题。</li>
<li>methods: 使用LLMs的能力，提出了一个完整的框架StrategylLM，通过自动生成、评估和选择有前途的策略来解决各种任务。</li>
<li>results: StrategylLM在13个数据集和4个复杂任务上取得了无人干预的比较优秀成绩，比基elineCoT-SC提高了39.2%到43.3%，70.3%到72.5%，51.7%到62.0%和30.0%到79.2%。<details>
<summary>Abstract</summary>
Most existing chain-of-thought (CoT) prompting methods suffer from the issues of generalizability and consistency, as they often rely on instance-specific solutions that may not be applicable to other cases and lack task-level consistency in their reasoning steps. To address these limitations, we propose a comprehensive framework, StrategyLLM, harnessing the capabilities of LLMs to tackle various tasks. The framework improves generalizability by formulating general problem-solving strategies and enhances consistency by producing consistent solutions using these strategies. StrategyLLM employs four LLM-based agents: strategy generator, executor, optimizer, and evaluator, working together to generate, evaluate, and select promising strategies for a given task automatically. The experimental results demonstrate that StrategyLLM outperforms the competitive baseline CoT-SC that requires human-annotated solutions on 13 datasets across 4 challenging tasks without human involvement, including math reasoning (39.2% $\rightarrow$ 43.3%), commonsense reasoning (70.3% $\rightarrow$ 72.5%), algorithmic reasoning (51.7% $\rightarrow$ 62.0%), and symbolic reasoning (30.0% $\rightarrow$ 79.2%).
</details>
<details>
<summary>摘要</summary>
现有的链式思维（CoT）提问方法受到普适性和一致性的限制，因为它们经常依赖于特定情况的解决方案，这些解决方案可能无法应用于其他情况，而且缺乏任务水平的一致性在其思维步骤中。为了解决这些限制，我们提出了一个全面的框架，名为策略LLM，充分利用LLM的能力来解决各种任务。该框架提高了普适性，通过形ulated general problem-solving策略，并增强一致性，通过使用这些策略生成一致的解决方案。策略LLM使用四个LLM基于的代理：策略生成器、执行器、优化器和评估器，这些代理共同工作，自动生成、评估和选择有投入潜力的策略，以解决给定任务。实验结果表明，策略LLM比基线CoT-SC，需要人工标注解决方案的情况下，在13个数据集上的4个挑战任务中表现出色，无需人类参与，包括数学逻辑（39.2% $\rightarrow$ 43.3%）、通情能力（70.3% $\rightarrow$ 72.5%）、算法逻辑（51.7% $\rightarrow$ 62.0%）和符号逻辑（30.0% $\rightarrow$ 79.2%）。
</details></li>
</ul>
<hr>
<h2 id="German-FinBERT-A-German-Pre-trained-Language-Model"><a href="#German-FinBERT-A-German-Pre-trained-Language-Model" class="headerlink" title="German FinBERT: A German Pre-trained Language Model"></a>German FinBERT: A German Pre-trained Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08793">http://arxiv.org/abs/2311.08793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Moritz Scherrmann</li>
<li>for: 本研究开发了一个特有的德国语言模型，名为德国金融BERT，供金融文本数据分析使用。</li>
<li>methods: 本研究使用了广泛的预训练过程，运用了大量的金融报告、紧急公告和新闻，与德国公司相关。</li>
<li>results: 研究结果显示，德国金融BERT在下游任务中表现出色，尤其是在金融专业数据上。这表明德国金融BERT能够捕捉领域特有的特征。<details>
<summary>Abstract</summary>
This study presents German FinBERT, a novel pre-trained German language model tailored for financial textual data. The model is trained through a comprehensive pre-training process, leveraging a substantial corpus comprising financial reports, ad-hoc announcements and news related to German companies. The corpus size is comparable to the data sets commonly used for training standard BERT models. I evaluate the performance of German FinBERT on downstream tasks, specifically sentiment prediction, topic recognition and question answering against generic German language models. My results demonstrate improved performance on finance-specific data, indicating the efficacy of German FinBERT in capturing domain-specific nuances. The presented findings suggest that German FinBERT holds promise as a valuable tool for financial text analysis, potentially benefiting various applications in the financial domain.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity"><a href="#Accelerating-Toeplitz-Neural-Network-with-Constant-time-Inference-Complexity" class="headerlink" title="Accelerating Toeplitz Neural Network with Constant-time Inference Complexity"></a>Accelerating Toeplitz Neural Network with Constant-time Inference Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08756">http://arxiv.org/abs/2311.08756</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion">https://github.com/opennlplab/etsc-exact-toeplitz-to-ssm-conversion</a></li>
<li>paper_authors: Zhen Qin, Yiran Zhong</li>
<li>for: 本文旨在将 toeplitz neural networks (TNNs) 转化为 state space models (SSMs)，以便在推理过程中实现常数复杂性。</li>
<li>methods: 作者通过对 TNNs 的推理过程进行优化，将其转化为 SSMs。该过程被形式化为一个优化问题，并提供了关闭式解决方案。在解决过程中，作者使用离散傅里叶变换 (DFT) 来高效解决 Vandermonde 线性系统问题。</li>
<li>results: 作者在语言模型任务上进行了广泛的实验，证明了其方法的有效性。具体来说，作者的方法可以在不同的设定下保持数值稳定性，并且与其他梯度下降解决方案相比，具有更高的数值稳定性。<details>
<summary>Abstract</summary>
Toeplitz Neural Networks (TNNs) have exhibited outstanding performance in various sequence modeling tasks. They outperform commonly used Transformer-based models while benefiting from log-linear space-time complexities. On the other hand, State Space Models (SSMs) achieve lower performance than TNNs in language modeling but offer the advantage of constant inference complexity. In this paper, we aim to combine the strengths of TNNs and SSMs by converting TNNs to SSMs during inference, thereby enabling TNNs to achieve the same constant inference complexities as SSMs. To accomplish this, we formulate the conversion process as an optimization problem and provide a closed-form solution. We demonstrate how to transform the target equation into a Vandermonde linear system problem, which can be efficiently solved using the Discrete Fourier Transform (DFT). Notably, our method requires no training and maintains numerical stability. It can be also applied to any LongConv-based model. To assess its effectiveness, we conduct extensive experiments on language modeling tasks across various settings. Additionally, we compare our method to other gradient-descent solutions, highlighting the superior numerical stability of our approach. The source code is available at https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion.
</details>
<details>
<summary>摘要</summary>
托平论 neural network (TNN) 在不同的序列模型任务中表现出色，而且比通用的 transformer 型模型更具有 Log-linear 空间时间复杂度的优势。然而，状态空间模型 (SSM) 在语言模型中表现较差，但它具有常数推理复杂度的优点。在这篇论文中，我们想要将 TNN 转换成 SSM 以实现常数推理复杂度，而不需要训练。我们将转换过程定义为优化问题，并提供了关闭式解决方案。我们将目标方程转换成 Vandermonde 线性系统问题，可以使用离散傅立叶变换 (DFT) 高效解决。值得注意的是，我们的方法不需要训练，并且保持了数值稳定性。此外，我们的方法可以应用于任何 LongConv 基于模型。为评估其效果，我们在不同的语言模型任务上进行了广泛的实验。此外，我们与其他梯度下降解决方案进行比较，高亮了我们的方法的数值稳定性的优势。源代码可以在 GitHub 上找到：https://github.com/OpenNLPLab/ETSC-Exact-Toeplitz-to-SSM-Conversion。
</details></li>
</ul>
<hr>
<h2 id="Thread-of-Thought-Unraveling-Chaotic-Contexts"><a href="#Thread-of-Thought-Unraveling-Chaotic-Contexts" class="headerlink" title="Thread of Thought Unraveling Chaotic Contexts"></a>Thread of Thought Unraveling Chaotic Contexts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08734">http://arxiv.org/abs/2311.08734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen</li>
<li>For: The paper aims to improve the reasoning performance of large language models (LLMs) in chaotic contexts by introducing a new “Thread of Thought” (ThoT) strategy.* Methods: The ThoT strategy segments and analyzes extended contexts, selecting pertinent information to improve the reasoning performance of LLMs. The strategy is versatile and can be integrated with various LLMs and prompting techniques.* Results: The paper demonstrates the effectiveness of ThoT using three datasets (PopQA, EntityQ, and MTCR) and shows that ThoT significantly improves reasoning performance compared to other prompting techniques.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have ushered in a transformative era in the field of natural language processing, excelling in tasks related to text comprehension and generation. Nevertheless, they encounter difficulties when confronted with chaotic contexts (e.g., distractors rather than long irrelevant context), leading to the inadvertent omission of certain details within the chaotic context. In response to these challenges, we introduce the "Thread of Thought" (ThoT) strategy, which draws inspiration from human cognitive processes. ThoT systematically segments and analyzes extended contexts while adeptly selecting pertinent information. This strategy serves as a versatile "plug-and-play" module, seamlessly integrating with various LLMs and prompting techniques. In the experiments, we utilize the PopQA and EntityQ datasets, as well as a Multi-Turn Conversation Response dataset (MTCR) we collected, to illustrate that ThoT significantly improves reasoning performance compared to other prompting techniques.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models"><a href="#Enhancing-Emergency-Decision-making-with-Knowledge-Graphs-and-Large-Language-Models" class="headerlink" title="Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models"></a>Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08732">http://arxiv.org/abs/2311.08732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minze Chen, Zhenxiang Tao, Weitong Tang, Tingxin Qin, Rui Yang, Chunli Zhu<br>for: 提供可靠的紧急决策支持methods: 使用知识图和大语言模型results: 在不同的紧急情况下，与基eline模型相比，得到了显著的改善，得分9.06、9.09、9.03和9.09。<details>
<summary>Abstract</summary>
Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.
</details>
<details>
<summary>摘要</summary>
应急管理强需全面知识，同时具有跨个人认知范围的可能性。因此，基于人工智能（AI）的决策在这种情况下是非常重要的。最新的大语言模型（LLM）提供了一个新的方向来提高目标机器智能。然而，直接使用LLM会导致不可靠的输出，因为它们的内置问题包括幻觉和思维能力不足。在这项工作中，我们开发了一个系统called Enhancing Emergency decision-making with Knowledge Graph and LLM（E-KELL），它提供了基于证据的决策在不同的应急阶段。研究构建了一个结构化的应急知识图，并使用提示链导引LLM进行图上的理解。在实际评估中，E-KELL得分9.06、9.09、9.03和9.09在可读性、准确性、简洁性和指导性方面，分别从一群应急指挥官和消防员手中得到评分，表明与基eline模型相比在不同的情况下显著提高。这项工作介绍了一种可靠的应急决策支持方法。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission"><a href="#Uncertainty-Estimation-on-Sequential-Labeling-via-Uncertainty-Transmission" class="headerlink" title="Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission"></a>Uncertainty Estimation on Sequential Labeling via Uncertainty Transmission</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08726">http://arxiv.org/abs/2311.08726</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jianfeng He, Linlin Yu, Shuo Lei, Chang-Tien Lu, Feng Chen</li>
<li>for: 本研究旨在提高Named Entity Recognition（NER）预测的不确定性评估（UE-NER）。</li>
<li>methods: 本研究提出了一个Sequential Labeling Posterior Network（SLPN），用于估算NER预测结果的不确定性。SLPN考虑了ENTITY之间的连接（即一个ENTITY嵌入是基于其他ENTITY的学习），并且特别处理了WRONG-SPAN情况。</li>
<li>results: 本研究在两个数据集上实现了显著的改善，例如在MIT-Restaurant数据集上提高了AUPR指数5.54个点。<details>
<summary>Abstract</summary>
Sequential labeling is a task predicting labels for each token in a sequence, such as Named Entity Recognition (NER). NER tasks aim to extract entities and predict their labels given a text, which is important in information extraction. Although previous works have shown great progress in improving NER performance, uncertainty estimation on NER (UE-NER) is still underexplored but essential. This work focuses on UE-NER, which aims to estimate uncertainty scores for the NER predictions. Previous uncertainty estimation models often overlook two unique characteristics of NER: the connection between entities (i.e., one entity embedding is learned based on the other ones) and wrong span cases in the entity extraction subtask. Therefore, we propose a Sequential Labeling Posterior Network (SLPN) to estimate uncertainty scores for the extracted entities, considering uncertainty transmitted from other tokens. Moreover, we have defined an evaluation strategy to address the specificity of wrong-span cases. Our SLPN has achieved significant improvements on two datasets, such as a 5.54-point improvement in AUPR on the MIT-Restaurant dataset.
</details>
<details>
<summary>摘要</summary>
Sequential labeling是一个任务，它 predicts labels for each token in a sequence，如Named Entity Recognition (NER)。NER任务的目标是从文本中提取实体，并预测它们的标签，这是信息抽取中非常重要的一步。虽然之前的工作已经达到了NER性能的很大进步，但UE-NER（Named Entity Recognition uncertainty estimation）还是被忽略了，这是非常重要的。本工作关注UE-NER，它的目标是为NER预测中的实体提取 uncertainty scores。以前的uncertainty estimation模型经常忽略了NER中的两个特有特征：实体之间的连接（即一个实体嵌入是基于其他实体学习的）以及实体提取子任务中的错误案例。因此，我们提出了一个Sequential Labeling Posterior Network (SLPN)，用于估计实体预测中的uncertainty scores，考虑实体之间的uncertainty传递。此外，我们定义了一种评估策略，用于解决实体提取子任务中的特殊错误案例。我们的SLPN在两个 dataset上达到了显著的改进，如MIT-Restaurant dataset上的AUPR提高5.54点。
</details></li>
</ul>
<hr>
<h2 id="Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph"><a href="#Method-for-Text-Entity-Linking-in-Power-Distribution-Scheduling-Oriented-to-Power-Distribution-Network-Knowledge-Graph" class="headerlink" title="Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph"></a>Method for Text Entity Linking in Power Distribution Scheduling Oriented to Power Distribution Network Knowledge Graph</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08724">http://arxiv.org/abs/2311.08724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li</li>
<li>for: 本研究旨在链接发电 dispatch 文本中的实体到一个电力分配网络知识图的方法。</li>
<li>methods: 该方法利用电力分配网络知识图和发电 dispatch 文本中实体的semantic、phonetic和syntactic特征进行深入理解，并使用加强型模型——lexical semantic feature-based skip convolutional neural network (LSF-SCNN) 进行实体匹配。</li>
<li>results: 比较控制模型的实验结果表明，LSF-SCNN 模型在英语发电 dispatch 文本中高精度地链接了多种实体类型，表现了高总准确率在实体链接中。<details>
<summary>Abstract</summary>
The proposed method for linking entities in power distribution dispatch texts to a power distribution network knowledge graph is based on a deep understanding of these networks. This method leverages the unique features of entities in both the power distribution network's knowledge graph and the dispatch texts, focusing on their semantic, phonetic, and syntactic characteristics. An enhanced model, the Lexical Semantic Feature-based Skip Convolutional Neural Network (LSF-SCNN), is utilized for effectively matching dispatch text entities with those in the knowledge graph. The efficacy of this model, compared to a control model, is evaluated through cross-validation methods in real-world power distribution dispatch scenarios. The results indicate that the LSF-SCNN model excels in accurately linking a variety of entity types, demonstrating high overall accuracy in entity linking when the process is conducted in English.
</details>
<details>
<summary>摘要</summary>
“提议的方法是基于电力分配网络知识图的深入理解，该方法利用知识图和调度文本中实体的语义、语音和语法特征。使用加强模型——lexical semantic feature-based skip convolutional neural network（LSF-SCNN），可以有效地匹配调度文本中的实体与知识图中的实体。通过跨验证方法在实际电力分配调度场景中评估模型的效果，结果表明LSF-SCNN模型在英语下可以准确地连接多种实体类型，实现高精度实体连接。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text"><a href="#Token-Prediction-as-Implicit-Classification-to-Identify-LLM-Generated-Text" class="headerlink" title="Token Prediction as Implicit Classification to Identify LLM-Generated Text"></a>Token Prediction as Implicit Classification to Identify LLM-Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08723">http://arxiv.org/abs/2311.08723</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/markchenyutian/t5-sentinel-public">https://github.com/markchenyutian/t5-sentinel-public</a></li>
<li>paper_authors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj</li>
<li>for: 本研究旨在提出一种新的语言模型标识方法，以便在文本生成中识别可能的大型语言模型（LLMs）。</li>
<li>methods: 我们重新框定了分类任务为下一个字符预测任务，直接使用基础LM进行 fine-tune，而不是添加额外的分类层。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们的实验基础。</li>
<li>results: 我们的方法在文本分类任务中表现出色，表明其简单性和效率。此外，我们对模型提取的特征进行了解释性研究，发现它能够在不同的LLMs中分辨出不同的写作风格，即使没有显式的分类器。我们还收集了一个名为 OpenLLMText 的数据集，包含约 340k 的文本样本，来自人类和 LLMs，包括 GPT3.5、PaLM、LLaMA 和 GPT2。<details>
<summary>Abstract</summary>
This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了一种新的方法，用于识别文本生成过程中可能的大语言模型（LLM）。而不是添加一层分类层到基础语言模型（LM）上，我们将分类任务重新定义为下一个字符预测任务，并直接使用基础LM进行 fine-tune。我们使用 Text-to-Text Transfer Transformer（T5）模型作为我们的实验室。我们与直接使用隐藏状态进行分类的方法进行比较。评估结果表明我们的方法在文本分类任务中表现出色，强调其简单性和效率。此外，我们对我们的模型提取的特征进行了解释性研究，发现它能够在不同的LLM下 diferenciate 不同的写作风格，甚至在没有显式分类器的情况下。我们还收集了一个名为 OpenLLMText 的数据集，包含约 340k 的文本样本，来自人类和 LLM，包括 GPT3.5、PaLM、LLaMA 和 GPT2。
</details></li>
</ul>
<hr>
<h2 id="Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory"><a href="#Think-in-Memory-Recalling-and-Post-thinking-Enable-LLMs-with-Long-Term-Memory" class="headerlink" title="Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"></a>Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08719">http://arxiv.org/abs/2311.08719</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang</li>
<li>for: 提高大语言模型在长期人机交互中的表现，减少偏见的问题。</li>
<li>methods: 提出了一种新的记忆机制 called TiM，允许大语言模型在对话流中维护一个演化的记忆，并通过插入、忘记和合并操作来动态更新记忆。</li>
<li>results: 在实际和模拟对话中，通过使用 TiM 机制，大语言模型的响应表现得到了显著提高，并且可以减少偏见的问题。<details>
<summary>Abstract</summary>
Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, \textit{i.e.}, inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有增强的记忆功能，在人机交互中表现出了很高的能力。然而，在重复 recall 和推理的过程中，LLM 容易产生偏见，即不同问题时的推理结果不一致。人类可以将想法保持在记忆中，而不需要重复推理。为了解决这个问题，我们提出了一种新的记忆机制called TiM（思考在内存），允许 LLM 在对话流中维护一个演进的记忆。TiM 框架包括两个关键阶段：（1）在生成响应之前，LLM 代理检索相关的思想记忆中，（2）在生成响应后，LLM 代理在历史和新的思想之间进行后思考和融合，以更新记忆。因此，TiM 可以消除重复推理的问题，并将后思考的思想作为历史记忆保存。此外，我们采用了在 TiM 中使用 Local Sensitive Hashing 进行高效的检索，以便应对长期对话。我们在实际和模拟对话中进行了质量和量的实验，demonstrating  equip  existing LLMs with TiM 可以明显提高它们在长期交互中的响应能力。
</details></li>
</ul>
<hr>
<h2 id="Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling"><a href="#Decomposing-Uncertainty-for-Large-Language-Models-through-Input-Clarification-Ensembling" class="headerlink" title="Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling"></a>Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08718">http://arxiv.org/abs/2311.08718</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang</li>
<li>for: 这 paper aims to improve the reliability, trustworthiness, and interpretability of large language models (LLMs) by developing an uncertainty decomposition framework.</li>
<li>methods: The proposed framework, called input clarifications ensemble, generates a set of clarifications for the input and feeds them into the fixed LLMs to ensure accurate and reliable uncertainty quantification.</li>
<li>results: Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks, and the code will be made publicly available at <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty.Here's">https://github.com/UCSB-NLP-Chang/llm_uncertainty.Here&#39;s</a> the Chinese version:</li>
<li>for: 这 paper 的目的是提高大型自然语言处理模型（LLMs）的可靠性、可信度和可解释性。</li>
<li>methods: 提议的框架是输入明确集，它会生成输入的一组明确度，然后将其传递给固定的 LLMs，以确保准确和可靠的不确定量评估。</li>
<li>results: 实验证明，提议的框架可以在不同任务上提供准确和可靠的不确定量评估，代码将会在 <a target="_blank" rel="noopener" href="https://github.com/UCSB-NLP-Chang/llm_uncertainty">https://github.com/UCSB-NLP-Chang/llm_uncertainty</a> 上公开发布。<details>
<summary>Abstract</summary>
Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details>
<details>
<summary>摘要</summary>
<<SYS>> transtable("Uncertainty decomposition")uncertainty decomposition REFERS TO THE TASK OF DECOMPOSING THE TOTAL UNCERTAINTY OF A MODEL INTO DATA (aleatoric) uncertainty, RESULTING FROM THE INHERENT COMPLEXITY OR AMBIGUITY OF THE DATA, AND MODEL (epistemic) uncertainty, RESULTING FROM THE LACK OF KNOWLEDGE IN THE MODEL. PERFORMING UNCERTAINTY DECOMPOSITION FOR LARGE LANGUAGE MODELS (LLMs) IS AN IMPORTANT STEP TOWARD IMPROVING THE RELIABILITY, TRUSTWORTHINESS, AND INTERPRETABILITY OF LLMs, BUT THIS RESEARCH TASK IS VERY CHALLENGING AND REMAINS UNRESOLVED. THE EXISTING CANONICAL METHOD, BAYESIAN NEURAL NETWORK (BNN), CANNOT BE APPLIED TO LLMs, BECAUSE BNN REQUIRES TRAINING AND ENSMBLING MULTIPLE VARIANTS OF MODELS, WHICH IS INFEASIBLE OR PROHIBITIVELY EXPENSIVE FOR LLMs. IN THIS PAPER, WE INTRODUCE AN UNCERTAINTY DECOMPOSITION FRAMEWORK FOR LLMs, CALLED INPUT CLARIFICATIONS ENSEMBLE, WHICH BYPASSES THE NEED TO TRAIN NEW MODELS. RATHER THAN ENSMBLING MODELS WITH DIFFERENT PARAMETERS, OUR APPROACH GENERATES A SET OF CLARIFICATIONS FOR THE INPUT, FEEDS THEM INTO THE FIXED LLMs, AND ENSMBLES THE CORRESPONDING PREDICTIONS. WE SHOW THAT OUR FRAMEWORK SHARES A SYMMETRIC DECOMPOSITION STRUCTURE WITH BNN. EMPIRICAL EVALUATIONS DEMONSTRATE THAT THE PROPOSED FRAMEWORK PROVIDES ACCURATE AND RELIABLE UNCERTAINTY QUANTIFICATION ON VARIOUS TASKS. CODE WILL BE MADE PUBLICLY AVAILABLE AT https://github.com/UCSB-NLP-Chang/llm_uncertainty .
</details></li>
</ul>
<hr>
<h2 id="PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning"><a href="#PLUG-Leveraging-Pivot-Language-in-Cross-Lingual-Instruction-Tuning" class="headerlink" title="PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning"></a>PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08711">http://arxiv.org/abs/2311.08711</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ytyz1307zzh/plug">https://github.com/ytyz1307zzh/plug</a></li>
<li>paper_authors: Zhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu, Mengzhao Jia, Meng Jiang, Francesco Barbieri</li>
<li>for: 提高大语言模型在不同人类指令下的理解和回答能力</li>
<li>methods: 使用高资源语言（主要是英语）为核心，实现指令准备语言转化为目标语言的回答</li>
<li>results: 比直接回答目标语言alone提高了大语言模型对指令的遵从能力，增加了29%的平均提升率。<details>
<summary>Abstract</summary>
Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency.
</details>
<details>
<summary>摘要</summary>
具有杰出表现的指令调整技术已经大幅提高了大型自然语言模型（LLM）的理解和回应多样化人类指令的能力。然而，在低资源语言上应用这些技术却遇到了挑战，这主要归结于LLM在不同语言的基础能力的不均衡，这种不均衡来自于模型在它们的预训练数据中的语言分布不均。为解决这个问题，我们提出了锚语言导向生成（PLUG）方法，该方法利用高资源语言（主要是英语）作为锚点，以提高低资源语言中的指令调整能力。它将模型首先在锚语言中处理指令，然后生成回应在目标语言中。为评估我们的方法，我们提出了一个标准测试套件，名为X-AlpacaEval，该套件包含4种语言（中文、韩语、意大利语和西班牙语）的指令，每个指令由专业翻译员进行标注。我们的方法在平均上提高了LLM的指令遵循能力 by 29%，相比直接在目标语言中回应。此外，我们的实验还证明了我们的方法可以采用不同的锚语言来帮助语言，其中LLM表现较低的语言。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations"><a href="#Evaluating-Robustness-of-Dialogue-Summarization-Models-in-the-Presence-of-Naturally-Occurring-Variations" class="headerlink" title="Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations"></a>Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08705">http://arxiv.org/abs/2311.08705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ankita Gupta, Chulaka Gunasekara, Hui Wan, Jatin Ganhotra, Sachindra Joshi, Marina Danilevsky</li>
<li>for: 本研究旨在探讨对话摘要模型的Robustness Challenge，包括对各种自然语言变化和噪声的影响。</li>
<li>methods: 我们使用公开的数据集对现有的对话摘要模型进行了系统性的研究，以评估这些模型对各种语言变化和噪声的抗预测性。我们引入了两种类型的干扰：utterance-level干扰和对话-level干扰。</li>
<li>results: 我们发现，尽管使用精度级进行了微调和指令级进行了微调，但是这些模型都受到输入变化的影响，特别是对话-level干扰。我们还通过人工评估 validate our findings。此外，我们发现使用一部分干扰数据进行训练并不能解决对话摘要模型的Robustness Challenge。<details>
<summary>Abstract</summary>
Dialogue summarization task involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations) and existing dialogue summarization models suffer from performance drop on such conversations. In this study, we systematically investigate the impact of such variations on state-of-the-art dialogue summarization models using publicly available datasets. To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We conduct our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which capture different aspects of the summarization model's performance. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate if the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data and observe that this approach is insufficient to address robustness challenges with current models and thus warrants a more thorough investigation to identify better solutions. Overall, our work highlights robustness challenges in dialogue summarization and provides insights for future research.
</details>
<details>
<summary>摘要</summary>
对话摘要任务 involve 摘要长 conversations 而保留最重要信息。实际对话中经常出现自然的变化（例如重复、停顿），现有的对话摘要模型在这些对话中表现不佳。在这项研究中，我们系统地研究这些变化对现状对话摘要模型的影响。为了模拟实际变化，我们引入了两种类型的杂化：个别话语杂化（ modify 个别话语中的错误和语言变化）和对话杂化（添加无关信息的交流，例如重复、致谢）。我们按照三个维度进行分析：一致性、重要性和忠诚度，这些维度捕捉了不同的对话摘要模型表现方面。我们发现， beide fine-tuned 和 instruction-tuned 模型受到输入变化的影响，其中后者更加敏感，特别是对话杂化。我们还通过人工评估 validate 我们的发现。最后，我们 investigate 是否可以通过训练 fine-tuned 模型 WITH 一部分杂化数据来提高其robustness，并发现这种方法不足以解决当前模型的Robustness挑战，因此需要进一步的调查以找到更好的解决方案。总之，我们的工作强调对话摘要中的Robustness挑战和未来研究的需要。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA"><a href="#Attribute-Diversity-Determines-the-Systematicity-Gap-in-VQA" class="headerlink" title="Attribute Diversity Determines the Systematicity Gap in VQA"></a>Attribute Diversity Determines the Systematicity Gap in VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08695">http://arxiv.org/abs/2311.08695</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ian Berlot-Attwell, A. Michael Carrell, Kumar Krishna Agrawal, Yash Sharma, Naomi Saphra</li>
<li>for: 研究 neural network 是否可以通过将 familar concept 组合在一起来泛化到新的情况。</li>
<li>methods: 引入了一个新的诊断数据集 CLEVR-HOPE，以测试系统aticity gap 在视觉问答中的表现。</li>
<li>results: 发现尽量增加训练数据量不会减少系统aticity gap，但是增加不同类型的属性组合在未seen combination中的训练数据多样性可以减少系统aticity gap。<details>
<summary>Abstract</summary>
The degree to which neural networks can generalize to new combinations of familiar concepts, and the conditions under which they are able to do so, has long been an open question. In this work, we study the systematicity gap in visual question answering: the performance difference between reasoning on previously seen and unseen combinations of object attributes. To test, we introduce a novel diagnostic dataset, CLEVR-HOPE. We find that while increased quantity of training data does not reduce the systematicity gap, increased training data diversity of the attributes in the unseen combination does. In all, our experiments suggest that the more distinct attribute type combinations are seen during training, the more systematic we can expect the resulting model to be.
</details>
<details>
<summary>摘要</summary>
“神经网络是否能够总结新的熵合？”这个问题一直是开放的。在这个工作中，我们研究视觉问答中的系统特性差距：推理已经看过和未经看过的对象属性的组合性的表现差异。为了测试，我们引入了一个新的诊断数据集，CLEVR-HOPE。我们发现，尽管增加训练数据量不会减少系统特性差距，但是增加未经看过组合属性的训练数据多样性可以减少系统特性差距。总之，我们的实验表明，更多的独特属性类型组合被训练时，更可预期性的结果。
</details></li>
</ul>
<hr>
<h2 id="Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models"><a href="#Routing-to-the-Expert-Efficient-Reward-guided-Ensemble-of-Large-Language-Models" class="headerlink" title="Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models"></a>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou</li>
<li>for: 本研究旨在提高大语言模型（LLM）的 ensemble 性能，通过挖掘各自领域和任务中的专业知识，实现更好的 ensemble 性能。</li>
<li>methods: 本研究提出了一种名为 Zooter 的奖励导引路由方法，通过训练路由函数来精准地分配每个查询到适合的 LLM 中。此外，研究还提出了一种基于标签的抑制难以预测的噪声的技术。</li>
<li>results: 研究发现，Zooter 在一系列 benchmark 集合上表现出色，比单个模型的表现更好，并在 44% 的任务上击败了多个奖励模型排名方法。<details>
<summary>Abstract</summary>
The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the given text into Simplified Chinese.<</SYS>>LLM的补偿潜力假设市售LLM有多个领域和任务的多样化专业知识，以 ensemble 方式实现更好的性能。现有的LLM ensemble方法主要集中于奖励模型排名输出，导致计算开销增加。为解决这个问题，我们再次探讨LLM的补偿潜力，并通过挖掘缓存专业知识使用市售奖励模型。我们提议Zooter，一种奖励导航方法，通过在训练查询上分配奖励来培养路由函数，可以准确地将每个查询分配给LLM拥有相关专业知识。我们还 integra 了标签基本标签增强来降低使用奖励作为银色监督时的噪音。Zooter在推理中引入了只有市售奖励模型排名方法相对较少的计算开销。我们对一个包含26个子集的全面 benchmark 集进行了评估，Zooter在 average 上超过了最佳单个模型，并在44%的任务上排名第一。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Calibration-for-Multilingual-Question-Answering-Models"><a href="#Understanding-Calibration-for-Multilingual-Question-Answering-Models" class="headerlink" title="Understanding Calibration for Multilingual Question Answering Models"></a>Understanding Calibration for Multilingual Question Answering Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08669">http://arxiv.org/abs/2311.08669</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yahan Yang, Soham Dan, Dan Roth, Insup Lee</li>
<li>for: 这篇论文主要研究了多语言预训练语言模型在问答任务中的准确性。</li>
<li>methods: 该论文使用了多种问答模型设计和多种语言进行了广泛的实验，包括抽取式和生成式问答模型，以及高资源语言和低资源语言。它还研究了不同维度的准确性，包括在适应区、离distribution和跨语言传递设置中。</li>
<li>results: 研究发现自动翻译数据增强技术可以大幅提高模型准确性，并进行了一系列的减少实验来研究模型大小对准确性的影响和多语言模型与单语言模型的比较。<details>
<summary>Abstract</summary>
Multilingual pre-trained language models are incredibly effective at Question Answering (QA), a core task in Natural Language Understanding, achieving high accuracies on several multilingual benchmarks. However, little is known about how well they are calibrated. In this paper, we study the calibration properties of several pre-trained multilingual large language models (LLMs) on a variety of question-answering tasks. We perform extensive experiments, spanning both extractive and generative QA model designs and diverse languages, spanning both high-resource and low-resource ones. We study different dimensions of calibration in in-distribution, out-of-distribution, and cross-lingual transfer settings, and investigate strategies to improve it, including post-hoc methods and regularized fine-tuning. We demonstrate automatically translated data augmentation as a highly effective technique to improve model calibration. We also conduct a number of ablation experiments to study the effect of model size on calibration and how multilingual models compare with their monolingual counterparts for diverse tasks and languages.
</details>
<details>
<summary>摘要</summary>
多语言预训练语言模型在问答任务（QA）中表现非常出色，在多种多语言benchmark上达到了高准确率。然而，对于这些模型的准确性calibration的了解非常少。在这篇论文中，我们研究了多种预训练多语言大型语言模型（LLMs）在问答任务中的准确性calibration性。我们进行了广泛的实验，涵盖了EXTRACTIVE和生成型问答模型的设计，以及多种语言和资源量的组合。我们研究了不同的calibration维度，包括在适用范围内、外部和跨语言传递设置中的calibration性，并 investigate了提高calibration性的策略，包括后期方法和规则化的细化。我们示出了自动翻译数据增强为一种非常有效的技术来提高模型的准确性calibration。我们还进行了一些减少实验来研究模型大小对calibration的影响和多语言模型与单语言模型在多种任务和语言上的比较。
</details></li>
</ul>
<hr>
<h2 id="It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games"><a href="#It-Takes-Two-to-Negotiate-Modeling-Social-Exchange-in-Online-Multiplayer-Games" class="headerlink" title="It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games"></a>It Takes Two to Negotiate: Modeling Social Exchange in Online Multiplayer Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08666">http://arxiv.org/abs/2311.08666</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kj2013/claff-diplomacy">https://github.com/kj2013/claff-diplomacy</a></li>
<li>paper_authors: Kokil Jaidka, Hansin Ahuja, Lynnette Ng</li>
<li>for: 研究在线上战略游戏《 дипломати》中玩家之间的交互，以了解玩家如何在游戏中谈判他们的方式。</li>
<li>methods: 使用了10,000多则聊天讯息的标注数据，以分析不同谈判策略的重要性，并评估这些策略在预测短期和长期游戏结果中的影响。</li>
<li>results: 发现谈判策略可以通过语言模型化聊天讯息来预测，但是在短期内的信任性预测需要更多的资料。然而，谈判策略在图像意识强化学习方法中是非常重要的，可以预测长期游戏结果，如玩家的成功。<details>
<summary>Abstract</summary>
Online games are dynamic environments where players interact with each other, which offers a rich setting for understanding how players negotiate their way through the game to an ultimate victory. This work studies online player interactions during the turn-based strategy game, Diplomacy. We annotated a dataset of over 10,000 chat messages for different negotiation strategies and empirically examined their importance in predicting long- and short-term game outcomes. Although negotiation strategies can be predicted reasonably accurately through the linguistic modeling of the chat messages, more is needed for predicting short-term outcomes such as trustworthiness. On the other hand, they are essential in graph-aware reinforcement learning approaches to predict long-term outcomes, such as a player's success, based on their prior negotiation history. We close with a discussion of the implications and impact of our work. The dataset is available at https://github.com/kj2013/claff-diplomacy.
</details>
<details>
<summary>摘要</summary>
在线游戏是动态环境，玩家之间的互动可以提供丰富的数据来理解玩家如何在游戏中获得最终胜利。这个研究 focuses on线上玩家互动中的谈判策略，并对不同的谈判策略进行了类别标注。我们分析了超过10,000封聊天讯息，并评估了这些谈判策略对游戏的长期和短期结果的影响。虽然可以透过语言模型估计谈判策略的准确性，但是在短期内的信任性仍然是难以预测的。然而，这些谈判策略在图形意识型态的强化学习方法中是非常重要的，可以预测长期的成功。我们在结论中讨论了这个研究的影响和意义，并提供了资料集的网站地址。
</details></li>
</ul>
<hr>
<h2 id="Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models"><a href="#Multistage-Collaborative-Knowledge-Distillation-from-Large-Language-Models" class="headerlink" title="Multistage Collaborative Knowledge Distillation from Large Language Models"></a>Multistage Collaborative Knowledge Distillation from Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08640">http://arxiv.org/abs/2311.08640</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum</li>
<li>for: 这 paper 是为了解决 semi-supervised sequence prediction 任务，其中有限量的标注数据不足以有效地训练模型，而同时几何shot提示大型自然语言模型 (LLM) 的性能有限。</li>
<li>methods: 这 paper 使用了一种新的混合型知识填充方法 (MCKD)，其首先使用几何shot在 Context 中学习来生成假标签 для无标注数据。然后，在每个阶段的填充中，一对学生在不同的分区上进行训练，每个学生生成新的和改进的假标签来监督下一个阶段的学生。</li>
<li>results: 这 paper 的结果表明，在两个 constituency parsing 任务上，使用多stage collaborative knowledge distillation (MCKD) 可以提高模型的性能。在 CRAFT 生物医学解析任务上，3-stage MCKD 使用 50 个标注例可以与 supervised finetuning 使用 500 个标注例匹配的性能，并且超过提示 LL 和 vanilla KD 的性能 by 7.5% 和 3.7% 的解析 F1，分别。<details>
<summary>Abstract</summary>
We study semi-supervised sequence prediction tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from a prompted LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we propose a new distillation method, multistage collaborative knowledge distillation from an LLM (MCKD), for such tasks. MCKD first prompts an LLM using few-shot in-context learning to produce pseudolabels for unlabeled data. Then, at each stage of distillation, a pair of students are trained on disjoint partitions of the pseudolabeled data. Each student subsequently produces new and improved pseudolabels for the unseen partition to supervise the next round of student(s) with. We show the benefit of multistage cross-partition labeling on two constituency parsing tasks. On CRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the performance of supervised finetuning with 500 examples and outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.
</details>
<details>
<summary>摘要</summary>
我们研究半supervised序列预测任务，其中标签资料短缺，无法有效地调整模型。另一方面，几个shot提示大型自然语言模型（LLM）的表现有限。在这篇论文中，我们发现学生模型从提示LLM的distillation中可以对such tasks generalize更好。基于这发现，我们提出了一个新的distillation方法：多stage合作知识传递法（MCKD）。MCKD首先使用几个shot在场景学习生成pseudolabels для无标的数据。然后，在每个阶段的distillation中，一对学生被训练在不同的分区中。每个学生 subsequntially生成新的和改善的pseudolabels для未看到的分区，以便supervise the next round of student(s) with。我们显示了多stage交叉分区标签的 benefitu two constituency parsing tasks。在CRAFT生物医学分析任务上，3 stage MCKD with 50标签例和supervised fine-tuning with 500标签例的表现相似，并且超过提示LLM和vanilla KD的构造解析F1指标 by 7.5%和3.7%，对于这两个任务而言。
</details></li>
</ul>
<hr>
<h2 id="Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference"><a href="#Formal-Proofs-as-Structured-Explanations-Proposing-Several-Tasks-on-Explainable-Natural-Language-Inference" class="headerlink" title="Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference"></a>Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08637">http://arxiv.org/abs/2311.08637</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lasha Abzianidze</li>
<li>for: 提出一种使用正式证明来实现可解释的自然语言推理（NLI）任务。</li>
<li>methods: 使用可靠高性能的逻辑基于NLI系统生成正式证明，并利用生成的证明中的深入信息来定义可解释NLI任务。</li>
<li>results: 提出一系列有结构化解释的NLI任务，可以根据解释的粒度来排序。 argue that these tasks will have fewer shortcomings than existing explainable NLI tasks.<details>
<summary>Abstract</summary>
In this position paper, we propose a way of exploiting formal proofs to put forward several explainable natural language inference (NLI) tasks. The formal proofs will be produced by a reliable and high-performing logic-based NLI system. Taking advantage of the in-depth information available in the generated formal proofs, we show how it can be used to define NLI tasks with structured explanations. The proposed tasks can be ordered according to difficulty defined in terms of the granularity of explanations. We argue that the tasks will suffer with substantially fewer shortcomings than the existing explainable NLI tasks (or datasets).
</details>
<details>
<summary>摘要</summary>
在这份位置论文中，我们提出了使用正式证明来提出一些可解释的自然语言推理（NLI）任务。正式证明将由可靠高性能的逻辑基于NLI系统生成。利用生成的正式证明中的深入信息，我们显示了如何使用结构化解释来定义NLI任务。我们提出的任务可以按Difficulty进行排序，定义为证明粒度的水平。我们认为这些任务具有较少缺陷，相比现有的可解释NLI任务（或数据集）。
</details></li>
</ul>
<hr>
<h2 id="DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models"><a href="#DEED-Dynamic-Early-Exit-on-Decoder-for-Accelerating-Encoder-Decoder-Transformer-Models" class="headerlink" title="DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models"></a>DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08623">http://arxiv.org/abs/2311.08623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Tang, Pengkai Zhu, Tian Li, Srikar Appalaraju, Vijay Mahadevan, R. Manmatha</li>
<li>for: 降低encoder-decoder transformer模型的推理时间</li>
<li>methods: 使用Dynamic Early Exit on Decoder (DEED)方法，包括多出口encoder-decoder模型、深度监管和适应模块等简单 yet practical技术，以提高推理精度并降低推理时间</li>
<li>results: 对两种state-of-the-art encoder-decoder transformer模型进行评测，实现了30%-60%的总推理时间减少，同时保持与基线相当或更高的准确率<details>
<summary>Abstract</summary>
Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>预测模型Encoder-decoder transformer模型在视觉语言任务中获得了很大成功，但它们受到高速引入延迟的困扰。通常，解码器占总时间的大部分，因为解码器使用自动回归的方式。为了加速推断，我们提出了在解码器上进行动态早期离开的方法（DEED）。我们构建了多出口encoder-decoder transformer模型，该模型在每个解码层都可以生成可信度的预测。此外，我们利用了简单 yet practical的技术，包括共享生成头和适应模块，以保持精度 when exiting at shallow decoder layers。基于多出口模型，我们在推断过程中实施了Step-level动态早期离开，其中模型可以根据当前层的信息使用 fewer decoder layers。由于不同的decoder layers可能会在不同的推断步骤中使用，我们在每个推断步骤 compute deeper-layer decoder features的时候，以确保不同推断步骤的特征是协调的。我们使用了两个状态对模型在多种视觉语言任务上进行评估。我们发现，我们的方法可以降低总推断时间的30%-60%，与基eline相比，保持相对或更高的准确率。
</details></li>
</ul>
<hr>
<h2 id="Multiple-Question-Multiple-Answer-Text-VQA"><a href="#Multiple-Question-Multiple-Answer-Text-VQA" class="headerlink" title="Multiple-Question Multiple-Answer Text-VQA"></a>Multiple-Question Multiple-Answer Text-VQA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08622">http://arxiv.org/abs/2311.08622</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jha1990/VQA-Multimodal-AI">https://github.com/jha1990/VQA-Multimodal-AI</a></li>
<li>paper_authors: Peng Tang, Srikar Appalaraju, R. Manmatha, Yusheng Xie, Vijay Mahadevan<br>for:多个问题和多个答案（MQMA）是一种新的文本-VQA方法，用于在encoder-decoder transformer模型中进行文本理解和图像理解。methods:MQMA方法使用多个问题和内容作为输入，并在encoder和decoder中进行自动进程的推理，以同时预测多个答案。我们对标准encoder-decoder transformer模型进行了一些新的建模修改以支持MQMA。results:MQMA预训练模型在多个文本-VQA数据集上达到了当前最佳result，具体是OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)的绝对改进。<details>
<summary>Abstract</summary>
We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do text-VQA in encoder-decoder transformer models. The text-VQA task requires a model to answer a question by understanding multi-modal content: text (typically from OCR) and an associated image. To the best of our knowledge, almost all previous approaches for text-VQA process a single question and its associated content to predict a single answer. In order to answer multiple questions from the same image, each question and content are fed into the model multiple times. In contrast, our proposed MQMA approach takes multiple questions and content as input at the encoder and predicts multiple answers at the decoder in an auto-regressive manner at the same time. We make several novel architectural modifications to standard encoder-decoder transformers to support MQMA. We also propose a novel MQMA denoising pre-training task which is designed to teach the model to align and delineate multiple questions and content with associated answers. MQMA pre-trained model achieves state-of-the-art results on multiple text-VQA datasets, each with strong baselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%) absolute improvements over the previous state-of-the-art approaches.
</details>
<details>
<summary>摘要</summary>
我们提出了多问题多答案（MQMA），一种新的方法来实现编码器-解码器变换器模型中的文本-VQA任务。文本-VQA任务需要模型理解多Modal内容：文本（通常来自OCR）和相关的图像。根据我们所知，前一代的approaches都是处理单个问题和其相关的内容来预测单个答案。而我们提出的MQMA方法则是在编码器中输入多个问题和内容，并在解码器中预测多个答案，这些答案将在自动进行重复的情况下同时预测。我们对标准编码器-解码器变换器模型进行了一些新的建议，以支持MQMA。我们还提出了一个MQMA净化预训练任务，用于教导模型将多个问题和内容与相应的答案进行对齐和分割。MQMA预训练模型在多个文本-VQA数据集上达到了最佳状态，每个数据集都有强的基eline。具体来说，在OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%), DocVQA (+1.1%)中的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="Toucan-Token-Aware-Character-Level-Language-Modeling"><a href="#Toucan-Token-Aware-Character-Level-Language-Modeling" class="headerlink" title="Toucan: Token-Aware Character Level Language Modeling"></a>Toucan: Token-Aware Character Level Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08620">http://arxiv.org/abs/2311.08620</a></li>
<li>repo_url: None</li>
<li>paper_authors: William Fleshman, Benjamin Van Durme</li>
<li>for: 这篇论文主要是为了提高Character-level语言模型的效率，使其能够更快地生成字符串。</li>
<li>methods: 这篇论文提出了一种基于”token-aware”的修改方法，可以帮助Character-level语言模型更好地处理长字符串。这种方法通过学习将字符串转换为token来实现，而不需要额外的tokenizer。</li>
<li>results: 对比于先前的工作，这种方法可以提高字符生成速度，而无需减少语言模型的表现。此外，这种方法还可以处理更长的字符串，并且可以生成更多的长字符串。code和项目可以在<a target="_blank" rel="noopener" href="https://nlp.jhu.edu/nuggets/%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://nlp.jhu.edu/nuggets/上获取。</a><details>
<summary>Abstract</summary>
Character-level language models obviate the need for separately trained tokenizers, but efficiency suffers from longer sequence lengths. Learning to combine character representations into tokens has made training these models more efficient, but they still require decoding characters individually. We propose Toucan, an augmentation to character-level models to make them "token-aware". Comparing our method to prior work, we demonstrate significant speed-ups in character generation without a loss in language modeling performance. We then explore differences between our learned dynamic tokenization of character sequences with popular fixed vocabulary solutions such as Byte-Pair Encoding and WordPiece, finding our approach leads to a greater amount of longer sequences tokenized as single items. Our project and code are available at https://nlp.jhu.edu/nuggets/.
</details>
<details>
<summary>摘要</summary>
⟨SYS⟩文本翻译成简化中文。Character-level语言模型取消了分配单独的tokenizer的需要，但是序列长度变长会导致效率下降。学习将字符表示合并到 tokens中的方法可以使训练这些模型更加高效，但它们仍然需要解码字符个个。我们提出了Toucan，一种将字符级模型转化为“字符认识”的增强。与先前的工作进行比较，我们示出了不失语言模型表现的速度提升。然后，我们探讨了我们学习的动态tokenization和固定词库解决方案如Byte-PairEncoding和WordPiece的 diferencias，发现我们的方法可以处理更多的更长的序列。我们的项目和代码可以在https://nlp.jhu.edu/nuggets/找到。
</details></li>
</ul>
<hr>
<h2 id="Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech"><a href="#Towards-Generalizable-SER-Soft-Labeling-and-Data-Augmentation-for-Modeling-Temporal-Emotion-Shifts-in-Large-Scale-Multilingual-Speech" class="headerlink" title="Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech"></a>Towards Generalizable SER: Soft Labeling and Data Augmentation for Modeling Temporal Emotion Shifts in Large-Scale Multilingual Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.08607">http://arxiv.org/abs/2311.08607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/spaghettisystems/emotion_whisper">https://github.com/spaghettisystems/emotion_whisper</a></li>
<li>paper_authors: Mohamed Osman, Tamer Nadeem, Ghada Khoriba</li>
<li>for: 这项研究的目的是提高人机交互的进步，通过识别口头沟通中的情感。</li>
<li>methods: 这项研究使用了16个不同的数据集，共计375小时的数据，包括英语、中文和日语等语言。研究采用软标注系统来捕捉情感的渐进强度。使用了Whisper编码器和启发自对比例学习的数据增强方法，注重情感的时间动态。</li>
<li>results: 研究在四个多语言数据集上进行验证，显示出了显著的零基eline泛化性。发布了开源模型权重和初步的良好结果，并在Hume-Prosody上进行了细化调整。<details>
<summary>Abstract</summary>
Recognizing emotions in spoken communication is crucial for advanced human-machine interaction. Current emotion detection methodologies often display biases when applied cross-corpus. To address this, our study amalgamates 16 diverse datasets, resulting in 375 hours of data across languages like English, Chinese, and Japanese. We propose a soft labeling system to capture gradational emotional intensities. Using the Whisper encoder and data augmentation methods inspired by contrastive learning, our method emphasizes the temporal dynamics of emotions. Our validation on four multilingual datasets demonstrates notable zero-shot generalization. We publish our open source model weights and initial promising results after fine-tuning on Hume-Prosody.
</details>
<details>
<summary>摘要</summary>
感知情感在人机交互中的重要性。当前的情感检测方法经常在不同的文本库中显示偏见。为解决这个问题，我们的研究将16种不同的数据集融合起来，共计375小时的数据，涵盖英语、中文和日语等语言。我们提议一种柔化标签系统，以捕捉情感的柔化强度。使用Whisper编码器和基于对比学习的数据增强方法，我们的方法强调情感的时间动态。我们的验证在四种多语言数据集上表现出了显著的零shot泛化。我们将我们的开源模型 веса和初步成果发布在Hume-Prosody上。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/15/cs.CL_2023_11_15/" data-id="clp89doby00f9i7884l5h4fjq" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/97/">97</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">141</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">128</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">66</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">81</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">140</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
