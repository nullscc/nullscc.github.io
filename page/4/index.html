
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/4/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_10_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/27/cs.AI_2023_10_27/" class="article-date">
  <time datetime="2023-10-27T12:00:00.000Z" itemprop="datePublished">2023-10-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/27/cs.AI_2023_10_27/">cs.AI - 2023-10-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Multi-Time-Scale-World-Models"><a href="#Multi-Time-Scale-World-Models" class="headerlink" title="Multi Time Scale World Models"></a>Multi Time Scale World Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18534">http://arxiv.org/abs/2310.18534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/djdprogramming/adfa2">https://github.com/djdprogramming/adfa2</a></li>
<li>paper_authors: Vaisakh Shaj, Saleh Gholam Zadeh, Ozan Demir, Luiz Ricardo Douat, Gerhard Neumann</li>
<li>for: 这 paper 是为了研究智能代理如何使用内部世界模型来预测不同的行为范围和时间尺度上的不同趋势。</li>
<li>methods: 这 paper 使用了一种名为 Multi Time Scale State Space (MTS3) 的概率ormalism，这种ormalism 可以有效地在多个时间尺度上进行高精度的长期预测和不确定性估计。</li>
<li>results: 实验表明，MTS3 方法在许多系统标识 benchmark 上表现出色，包括复杂的模拟和实际世界动力系统。<details>
<summary>Abstract</summary>
Intelligent agents use internal world models to reason and make predictions about different courses of their actions at many scales. Devising learning paradigms and architectures that allow machines to learn world models that operate at multiple levels of temporal abstractions while dealing with complex uncertainty predictions is a major technical hurdle. In this work, we propose a probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale State Space (MTS3) model. Our model uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future. Our experiments, which focus on action conditional long horizon future predictions, show that MTS3 outperforms recent methods on several system identification benchmarks including complex simulated and real-world dynamical systems.
</details>
<details>
<summary>摘要</summary>
智能代理用内部世界模型来进行理解和预测不同的行动轨迹，从小规模到大规模，面临艰难的技术挑战。在这种工作中，我们提出了一种概率形式来学习多级时间尺度的世界模型，我们称之为多时间尺度状态空间（MTS3）模型。我们的模型使用多个时间尺度的计算效率优化的推理方案，以实现高精度的长期预测和未来数分秒内的不确定性估计。我们的实验集中关注行动条件长期未来预测，并在复杂的模拟和真实世界动力系统上达到了比较好的效果，超过了最近的方法。
</details></li>
</ul>
<hr>
<h2 id="Sample-based-Explanations-via-Generalized-Representers"><a href="#Sample-based-Explanations-via-Generalized-Representers" class="headerlink" title="Sample based Explanations via Generalized Representers"></a>Sample based Explanations via Generalized Representers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18526">http://arxiv.org/abs/2310.18526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Che-Ping Tsai, Chih-Kuan Yeh, Pradeep Ravikumar</li>
<li>for: 本文提出了一种通用的样本基于解释方法，称为通用代表者（generalized representers），用于测量模型训练样本对测试预测的影响。</li>
<li>methods: 该方法使用两个组件：全局样本重要性和本地样本重要性。全局样本重要性量化训练点对模型的影响，是对测试样本不变的。本地样本重要性使用kernel计算测试点和训练点之间的相似度。文章的一个重要贡献是证明通用代表者是所有样本基于解释方法的自然集合。</li>
<li>results: 文章进行了对两个图像和两个文本分类 datasets 的实验比较，并证明了不同的通用代表者在不同的 dataset 上的性能。<details>
<summary>Abstract</summary>
We propose a general class of sample based explanations of machine learning models, which we term generalized representers. To measure the effect of a training sample on a model's test prediction, generalized representers use two components: a global sample importance that quantifies the importance of the training point to the model and is invariant to test samples, and a local sample importance that measures similarity between the training sample and the test point with a kernel. A key contribution of the paper is to show that generalized representers are the only class of sample based explanations satisfying a natural set of axiomatic properties. We discuss approaches to extract global importances given a kernel, and also natural choices of kernels given modern non-linear models. As we show, many popular existing sample based explanations could be cast as generalized representers with particular choices of kernels and approaches to extract global importances. Additionally, we conduct empirical comparisons of different generalized representers on two image and two text classification datasets.
</details>
<details>
<summary>摘要</summary>
我们提出一种通用的样本基于解释方法，我们称之为通用表示者（generalized representers）。为了测量训练样本对模型测试预测的影响，通用表示者使用两个组件：全局样本重要性和本地样本重要性。全局样本重要性量化训练点对模型的影响，是不变的测试样本，而本地样本重要性则是测试点和训练点之间的相似性，使用核函数。我们的论文的一个重要贡献是证明通用表示者是唯一满足自然的axioms的类型的样本基于解释方法。我们讨论如何从核函数提取全局重要性，以及现代非线性模型中的自然选择核函数。我们还进行了两个图像和两个文本分类 datasets上的实验比较，以证明不同的通用表示者之间的区别。
</details></li>
</ul>
<hr>
<h2 id="3DCoMPaT-An-improved-Large-scale-3D-Vision-Dataset-for-Compositional-Recognition"><a href="#3DCoMPaT-An-improved-Large-scale-3D-Vision-Dataset-for-Compositional-Recognition" class="headerlink" title="3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition"></a>3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18511">http://arxiv.org/abs/2310.18511</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Vision-CAIR/3DCoMPaT-v2">https://github.com/Vision-CAIR/3DCoMPaT-v2</a></li>
<li>paper_authors: Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, Mohamed Elhoseiny</li>
<li>for: 本研究团队发布了3DCoMPaT$^{++}$,一个包含2D&#x2F;3D多Modal数据集，包括160万个渲染视图和1000万个精细化的3D形状，以及匹配的RGB点云、3D纹理网格、深度地图和分割mask。</li>
<li>methods: 研究人员使用了一种新的任务，即Grounded CoMPaT Recognition (GCR)，以同时识别和地理3D物体的组合材料。此外，研究人员还提出了一种修改后的PointNet$^{++}$模型，用于6D输入的训练。</li>
<li>results: 研究人员在CVPR2023会议上组织了一场数据挑战，展示了赢家方法的使用，并 explore了GCR增强的一些alternative技术。<details>
<summary>Abstract</summary>
In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at CVPR2023, showcasing the winning method's utilization of a modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision.
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们介绍3DCoMPaT$^{++}$,一个多Modal 2D/3D数据集，包含160万个渲染视图，以及更多的1000万个精细化的3D形状，其中每个形状都有精细化的部件标注，同时还包含匹配的RGB点云、 текстури化的三角形、深度地图和分割mask。3DCoMPaT$^{++}$覆盖了41种形状类，275种精细化部件类，以及293种精细化材料类，这些类可以在3D对象的部件上进行组合应用。我们从四个相等的视图渲染了一个百万个精细化的形状，并且随机选择四个视图，共计160万个渲染。在部件级别进行分割，并设置了粗略和细腻的semantic水平。我们介绍了一个新任务，即Grounded CoMPaT Recognition (GCR)，以同时认识和固定3D对象的部件上的材料组合。此外，我们还报告了CVPR2023年度数据挑战的结果，展示了一种使用修改后的PointNet$^{++}$模型训练于6D输入的赢家方法，以及探讨了GCR增强技术的代替方法。我们希望我们的工作能够为未来的3D视觉研究提供帮助。
</details></li>
</ul>
<hr>
<h2 id="Deep-Reinforcement-Learning-for-Weapons-to-Targets-Assignment-in-a-Hypersonic-strike"><a href="#Deep-Reinforcement-Learning-for-Weapons-to-Targets-Assignment-in-a-Hypersonic-strike" class="headerlink" title="Deep Reinforcement Learning for Weapons to Targets Assignment in a Hypersonic strike"></a>Deep Reinforcement Learning for Weapons to Targets Assignment in a Hypersonic strike</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18509">http://arxiv.org/abs/2310.18509</a></li>
<li>repo_url: None</li>
<li>paper_authors: Brian Gaudet, Kris Drozd, Roberto Furfaro</li>
<li>for: 用深度强化学习优化多辆 hypersonic strike 武器识别策略，以 maximize 每集episode中破坏目标的总价值。</li>
<li>methods: 使用深度强化学习来优化武器识别策略，并与非线性整数编程（NLIP）比较性能。</li>
<li>results: 相比NLIP策略，深度强化学习策略具有优化性和1000倍减少计算时间，可以实现实时决策，满足 autonomous 决策在任务末端。<details>
<summary>Abstract</summary>
We use deep reinforcement learning (RL) to optimize a weapons to target assignment (WTA) policy for multi-vehicle hypersonic strike against multiple targets. The objective is to maximize the total value of destroyed targets in each episode. Each randomly generated episode varies the number and initial conditions of the hypersonic strike weapons (HSW) and targets, the value distribution of the targets, and the probability of a HSW being intercepted. We compare the performance of this WTA policy to that of a benchmark WTA policy derived using non-linear integer programming (NLIP), and find that the RL WTA policy gives near optimal performance with a 1000X speedup in computation time, allowing real time operation that facilitates autonomous decision making in the mission end game.
</details>
<details>
<summary>摘要</summary>
我们使用深度强化学习（RL）优化多辆高速武器对多个目标的分配策略，以最大化每个回合的目标总值。每个随机生成的回合都会变化高速武器和目标的数量和初始状态，目标的价值分布，以及高速武器被 intercept 的概率。我们对这种 WTA 策略与非线性整数编程（NLIP） derive 的参考 WTA 策略进行比较，发现 RL WTA 策略在计算时间上具有1000倍的加速，可以实现实时运行，从而促进任务尾部自动决策。
</details></li>
</ul>
<hr>
<h2 id="How-Well-Do-Feature-Additive-Explainers-Explain-Feature-Additive-Predictors"><a href="#How-Well-Do-Feature-Additive-Explainers-Explain-Feature-Additive-Predictors" class="headerlink" title="How Well Do Feature-Additive Explainers Explain Feature-Additive Predictors?"></a>How Well Do Feature-Additive Explainers Explain Feature-Additive Predictors?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18496">http://arxiv.org/abs/2310.18496</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zachariah Carmichael, Walter J. Scheirer</li>
<li>for: 这篇论文旨在研究可解释AI（XAI）技术，以帮助理解黑盒模型的决策过程。</li>
<li>methods: 该论文使用了多种常用的特征添加式解释器（如LIME、SHAP、SHAPR、MAPLE、PDP），以评估这些解释器在添加式预测器上的效果。</li>
<li>results: 研究发现，这些解释器在处理符号表示、神经网络和总代数模型上都有较差的性能，尤其是当决策过程含有特征交互时。<details>
<summary>Abstract</summary>
Surging interest in deep learning from high-stakes domains has precipitated concern over the inscrutable nature of black box neural networks. Explainable AI (XAI) research has led to an abundance of explanation algorithms for these black boxes. Such post hoc explainers produce human-comprehensible explanations, however, their fidelity with respect to the model is not well understood - explanation evaluation remains one of the most challenging issues in XAI. In this paper, we ask a targeted but important question: can popular feature-additive explainers (e.g., LIME, SHAP, SHAPR, MAPLE, and PDP) explain feature-additive predictors? Herein, we evaluate such explainers on ground truth that is analytically derived from the additive structure of a model. We demonstrate the efficacy of our approach in understanding these explainers applied to symbolic expressions, neural networks, and generalized additive models on thousands of synthetic and several real-world tasks. Our results suggest that all explainers eventually fail to correctly attribute the importance of features, especially when a decision-making process involves feature interactions.
</details>
<details>
<summary>摘要</summary>
高于常规领域的深度学习突破性引起了黑盒神经网络的不可预测性的问题的关注。可解释AI（XAI）研究引发了大量的解释算法 для这些黑盒。然而，这些后期解释器的准确性与模型之间的关系并不很清楚 - 解释评估仍然是XAI中最大的挑战。在这篇论文中，我们提出了一个targeted yet important问题：可能性分解器（例如LIME、SHAP、SHAPR、MAPLE和PDP）能够解释增加性预测器吗？我们在这篇论文中评估这些解释器在符号表示法、神经网络和总加itive模型上的 thousendsof synthetic和several real-world任务中的效果。我们的结果表明，无论是在符号表示法还是在实际任务上，所有的解释器都 eventually fail to correctly attribute the importance of features，特别是当决策过程中涉及到特征之间的互动。
</details></li>
</ul>
<hr>
<h2 id="MOSEL-Inference-Serving-Using-Dynamic-Modality-Selection"><a href="#MOSEL-Inference-Serving-Using-Dynamic-Modality-Selection" class="headerlink" title="MOSEL: Inference Serving Using Dynamic Modality Selection"></a>MOSEL: Inference Serving Using Dynamic Modality Selection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18481">http://arxiv.org/abs/2310.18481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bodun Hu, Le Xu, Jeongyoon Moon, Neeraja J. Yadwadkar, Aditya Akella</li>
<li>For: The paper is written for researchers and developers who are working on machine learning models and inference-serving systems, and who are looking for ways to improve the efficiency and accuracy of their models.* Methods: The paper proposes a new approach called modality selection, which involves adaptively choosing the most relevant modalities for an inference task based on user-defined performance and accuracy requirements. The proposed approach is implemented in an automated inference serving system called MOSEL.* Results: The paper reports that MOSEL improves system throughput by 3.6 times with an accuracy guarantee and shortens job completion times by 11 times compared to a baseline approach. The results demonstrate the effectiveness of the modality selection approach and the benefits of using MOSEL for multi-modal machine learning models.<details>
<summary>Abstract</summary>
Rapid advancements over the years have helped machine learning models reach previously hard-to-achieve goals, sometimes even exceeding human capabilities. However, to attain the desired accuracy, the model sizes and in turn their computational requirements have increased drastically. Thus, serving predictions from these models to meet any target latency and cost requirements of applications remains a key challenge, despite recent work in building inference-serving systems as well as algorithmic approaches that dynamically adapt models based on inputs. In this paper, we introduce a form of dynamism, modality selection, where we adaptively choose modalities from inference inputs while maintaining the model quality. We introduce MOSEL, an automated inference serving system for multi-modal ML models that carefully picks input modalities per request based on user-defined performance and accuracy requirements. MOSEL exploits modality configurations extensively, improving system throughput by 3.6$\times$ with an accuracy guarantee and shortening job completion times by 11$\times$.
</details>
<details>
<summary>摘要</summary>
随着时间的推移，机器学习模型在过去的几年内进行了快速的进步，有时甚至超越人类的能力。然而，为了达到所需的准确率，模型的大小和计算需求却有了很大的增长。因此，将预测结果服务到应用程序中，以满足任何目标延迟和成本要求，仍然是一大项目。在这篇论文中，我们引入了一种动态性，即modalities选择，我们在推理输入中动态选择Modalities，保持模型质量。我们介绍了MOSEL，一个自动化推理服务系统，可以智能地选择输入Modalities，根据用户定义的性能和准确率要求。MOSEL利用模式配置的潜在优势，提高系统吞吐量3.6倍，同时保证准确率和完成任务时间的短短化。
</details></li>
</ul>
<hr>
<h2 id="Weighted-Sampled-Split-Learning-WSSL-Balancing-Privacy-Robustness-and-Fairness-in-Distributed-Learning-Environments"><a href="#Weighted-Sampled-Split-Learning-WSSL-Balancing-Privacy-Robustness-and-Fairness-in-Distributed-Learning-Environments" class="headerlink" title="Weighted Sampled Split Learning (WSSL): Balancing Privacy, Robustness, and Fairness in Distributed Learning Environments"></a>Weighted Sampled Split Learning (WSSL): Balancing Privacy, Robustness, and Fairness in Distributed Learning Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18479">http://arxiv.org/abs/2310.18479</a></li>
<li>repo_url: None</li>
<li>paper_authors: Manish Osti, Aashray Thakuri, Basheer Qolomany, Aos Mulahuwaish</li>
<li>for: 提高隐私、可靠性和公平性在分布式机器学习系统中</li>
<li>methods: 使用权重采样方法，将学习过程分布到多个客户端，以保护数据隐私和提高模型准确性</li>
<li>results: 1) 提高模型准确性，2) 提高系统可靠性，3) 维护客户端组合的公平性<details>
<summary>Abstract</summary>
This study presents Weighted Sampled Split Learning (WSSL), an innovative framework tailored to bolster privacy, robustness, and fairness in distributed machine learning systems. Unlike traditional approaches, WSSL disperses the learning process among multiple clients, thereby safeguarding data confidentiality. Central to WSSL's efficacy is its utilization of weighted sampling. This approach ensures equitable learning by tactically selecting influential clients based on their contributions. Our evaluation of WSSL spanned various client configurations and employed two distinct datasets: Human Gait Sensor and CIFAR-10. We observed three primary benefits: heightened model accuracy, enhanced robustness, and maintained fairness across diverse client compositions. Notably, our distributed frameworks consistently surpassed centralized counterparts, registering accuracy peaks of 82.63% and 75.51% for the Human Gait Sensor and CIFAR-10 datasets, respectively. These figures contrast with the top accuracies of 81.12% and 58.60% achieved by centralized systems. Collectively, our findings champion WSSL as a potent and scalable successor to conventional centralized learning, marking it as a pivotal stride forward in privacy-focused, resilient, and impartial distributed machine learning.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Causal-disentanglement-of-multimodal-data"><a href="#Causal-disentanglement-of-multimodal-data" class="headerlink" title="Causal disentanglement of multimodal data"></a>Causal disentanglement of multimodal data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18471">http://arxiv.org/abs/2310.18471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elise Walker, Jonas A. Actor, Carianne Martinez, Nathaniel Trask</li>
<li>for: 本研究旨在探讨 causal representation learning 算法，它可以从数据中找到低维度的表示，并且这些表示具有可解释的 causal 关系。</li>
<li>methods: 本研究使用了多种方法，包括 linear 结构 causal model、 intervenitional 数据和 weak supervision。然而，在 exploratory causal representation learning 中，这些元素和先前信息可能不可用或不合理。因此，我们提出了一种新的 causal representation learning 算法（causalPIMA），它可以使用多模态数据和物理约束来找到重要的 causal 关系。</li>
<li>results: 我们的结果表明，causalPIMA 可以在完全无监督情况下学习一个可解释的 causal 结构，同时也可以找到关键的特征。我们测试了这种算法在一个 synthetic 数据集和一个科学数据集上，结果表明，它可以在完全无监督情况下找到关键的特征和 causal 关系。<details>
<summary>Abstract</summary>
Causal representation learning algorithms discover lower-dimensional representations of data that admit a decipherable interpretation of cause and effect; as achieving such interpretable representations is challenging, many causal learning algorithms utilize elements indicating prior information, such as (linear) structural causal models, interventional data, or weak supervision. Unfortunately, in exploratory causal representation learning, such elements and prior information may not be available or warranted. Alternatively, scientific datasets often have multiple modalities or physics-based constraints, and the use of such scientific, multimodal data has been shown to improve disentanglement in fully unsupervised settings. Consequently, we introduce a causal representation learning algorithm (causalPIMA) that can use multimodal data and known physics to discover important features with causal relationships. Our innovative algorithm utilizes a new differentiable parametrization to learn a directed acyclic graph (DAG) together with a latent space of a variational autoencoder in an end-to-end differentiable framework via a single, tractable evidence lower bound loss function. We place a Gaussian mixture prior on the latent space and identify each of the mixtures with an outcome of the DAG nodes; this novel identification enables feature discovery with causal relationships. Tested against a synthetic and a scientific dataset, our results demonstrate the capability of learning an interpretable causal structure while simultaneously discovering key features in a fully unsupervised setting.
</details>
<details>
<summary>摘要</summary>
causal representation learning algorithms 找到 Lower-dimensional 的表示，这些表示具有可解释的 causal 关系；因为实现这种可解释的表示是困难的，许多 causal learning algorithms 使用元信息，如（线性）结构 causal 模型， intervening 数据或 weak supervision。然而，在 exploratory causal representation learning 中，这些元信息和 prior information 可能不可用或不合适。 alternatively， scientific datasets  часто有多个模式或 physics-based 约束，并使用这些 scientific, multimodal 数据可以提高 disentanglement 在完全无监督的设置中。因此，我们引入了一种 causal representation learning algorithm (causalPIMA)，可以使用 multimodal 数据和known physics 来发现重要的 causal 关系。我们的 innovative algorithm 使用了一种新的 differentiable  parametrization，在一个 end-to-end  differentiable 框架中学习一个 directed acyclic graph (DAG) 和一个 latent space 的 variational autoencoder。我们在这个框架中使用了一个单一的 tractable evidence lower bound 损失函数。我们在 latent space 中分配了 Gaussian mixture prior，并将每个混合物标识为 DAG 节点的结果；这种新的标识使得 feature discovery 具有 causal 关系。我们在一个 sintetic 和一个 scientific dataset 上测试了我们的结果，结果表明我们可以在完全无监督的设置中学习可解释的 causal 结构，同时也可以发现关键的特征。
</details></li>
</ul>
<hr>
<h2 id="Semi-Synthetic-Dataset-Augmentation-for-Application-Specific-Gaze-Estimation"><a href="#Semi-Synthetic-Dataset-Augmentation-for-Application-Specific-Gaze-Estimation" class="headerlink" title="Semi-Synthetic Dataset Augmentation for Application-Specific Gaze Estimation"></a>Semi-Synthetic Dataset Augmentation for Application-Specific Gaze Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18469">http://arxiv.org/abs/2310.18469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Cedric Leblond-Menard, Gabriel Picard-Krashevski, Sofiane Achiche</li>
<li>for: 增强 gaze estimation 数据集，提高模型的通用性</li>
<li>methods: 使用 textured tridimensional mesh 技术，将训练图像从虚拟摄像头中渲染出来</li>
<li>results: 平均降低 gaze estimation 错误角度的比例为 47%<details>
<summary>Abstract</summary>
Although the number of gaze estimation datasets is growing, the application of appearance-based gaze estimation methods is mostly limited to estimating the point of gaze on a screen. This is in part because most datasets are generated in a similar fashion, where the gaze target is on a screen close to camera's origin. In other applications such as assistive robotics or marketing research, the 3D point of gaze might not be close to the camera's origin, meaning models trained on current datasets do not generalize well to these tasks. We therefore suggest generating a textured tridimensional mesh of the face and rendering the training images from a virtual camera at a specific position and orientation related to the application as a mean of augmenting the existing datasets. In our tests, this lead to an average 47% decrease in gaze estimation angular error.
</details>
<details>
<summary>摘要</summary>
In other words, the existing datasets for gaze estimation are mostly generated with the gaze target on a screen close to the camera's origin, which limits the application of appearance-based gaze estimation methods to only estimating the point of gaze on a screen. To address this limitation, we suggest using a textured 3D mesh of the face and rendering the training images from a virtual camera at a specific position and orientation related to the application as a means of augmenting the existing datasets. This leads to an average 47% decrease in gaze estimation angular error.
</details></li>
</ul>
<hr>
<h2 id="LLMSTEP-LLM-proofstep-suggestions-in-Lean"><a href="#LLMSTEP-LLM-proofstep-suggestions-in-Lean" class="headerlink" title="LLMSTEP: LLM proofstep suggestions in Lean"></a>LLMSTEP: LLM proofstep suggestions in Lean</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18457">http://arxiv.org/abs/2310.18457</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wellecks/llmstep">https://github.com/wellecks/llmstep</a></li>
<li>paper_authors: Sean Welleck, Rahul Saha</li>
<li>for: 用于将语言模型集成到lean数据助手中</li>
<li>methods: 使用服务器主机的语言模型生成建议，并在lean中检查和显示给用户</li>
<li>results: 提供了基准语言模型，以及代码 для精度调整和评估，以支持进一步的开发In English, this means:</li>
<li>for: The paper is written to integrate a language model into the Lean proof assistant.</li>
<li>methods: The paper proposes using a server hosting a language model to generate suggestions, which are then checked in Lean and displayed to the user in their development environment.</li>
<li>results: The paper provides a baseline language model, along with code for fine-tuning and evaluation to support further development.<details>
<summary>Abstract</summary>
We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.
</details>
<details>
<summary>摘要</summary>
我们介绍LLMSTEP，一个将语言模型集成到lean推理助手的工具。LLMSTEP是lean 4的一个战略，将用户的证明状态发送到一个主机上的语言模型。语言模型产生建议，并在lean中检查和显示给用户。我们提供了基线语言模型，以及代码 для微调和评估，以支持进一步的开发。我们提供了 CPU、CUDA GPU 和 Google Colab 笔记本上的服务器实现，以便快速、有效地获得任何用户的语言模型建议。
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Skip-Orthogonal-List-for-Dynamic-Optimal-Transport-Problem"><a href="#A-Novel-Skip-Orthogonal-List-for-Dynamic-Optimal-Transport-Problem" class="headerlink" title="A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem"></a>A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18446">http://arxiv.org/abs/2310.18446</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xyxu2033/DynamicOptimalTransport">https://github.com/xyxu2033/DynamicOptimalTransport</a></li>
<li>paper_authors: Xiaoyang Xu, Hu Ding</li>
<li>for:  solves the discrete dynamic optimal transport problem efficiently when the weights or locations of the data points change, with applications in machine learning.</li>
<li>methods:  proposes a novel 2D Skip Orthogonal List and dynamic tree techniques, based on the conventional simplex method, to efficiently complete each pivoting operation within $O(|V|)$ time with high probability.</li>
<li>results:  significantly outperforms existing algorithms in dynamic scenarios, with a few simplex iterations in practice.<details>
<summary>Abstract</summary>
Optimal transportation is a fundamental topic that has attracted a great amount of attention from machine learning community in the past decades. In this paper, we consider an interesting discrete dynamic optimal transport problem: can we efficiently update the optimal transport plan when the weights or the locations of the data points change? This problem is naturally motivated by several applications in machine learning. For example, we often need to compute the optimal transportation cost between two different data sets; if some change happens to a few data points, should we re-compute the high complexity cost function or update the cost by some efficient dynamic data structure? We are aware that several dynamic maximum flow algorithms have been proposed before, however, the research on dynamic minimum cost flow problem is still quite limited, to the best of our knowledge. We propose a novel 2D Skip Orthogonal List together with some dynamic tree techniques. Although our algorithm is based on the conventional simplex method, it can efficiently complete each pivoting operation within $O(|V|)$ time with high probability where $V$ is the set of all supply and demand nodes. Since dynamic modifications typically do not introduce significant changes, our algorithm requires only a few simplex iterations in practice. So our algorithm is more efficient than re-computing the optimal transportation cost that needs at least one traversal over all the $O(|E|) = O(|V|^2)$ variables in general cases. Our experiments demonstrate that our algorithm significantly outperforms existing algorithms in the dynamic scenarios.
</details>
<details>
<summary>摘要</summary>
最优运输是机器学习领域内一个基本问题，在过去几十年内吸引了大量关注。在这篇论文中，我们考虑了一个有趣的离散动态最优运输问题：在数据点的重量或位置发生变化时，是否可以有效地更新最优运输计划？这个问题是机器学习中各种应用场景的自然推动。例如，我们经常需要计算两个不同数据集之间的最优运输成本；如果一些数据点发生变化，是否可以快速地更新高复杂性成本函数，或者使用一些高效的动态数据结构？我们知道有几种动态最大流算法被提出，但是关于动态最小成本流问题的研究还很有限，至于我们所知道的最佳状态。我们提出了一种新的2D跳过列表，并结合了一些动态树技术。尽管我们的算法基于传统的简单кс方法，但它可以在$O(|V|)$时间内高可用性下完成每次轴转操作，其中$V$是所有供应和需求节点的集合。由于动态修改通常不会引入重要的变化，我们的算法只需要几个简单кс迭代即可。因此，我们的算法比重新计算总成本函数，需要至少一次遍历所有$O(|E|) = O(|V|^2)$变量的情况下更高效。我们的实验表明，我们的算法在动态场景下明显超过现有算法。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-fuller-understanding-of-neurons-with-Clustered-Compositional-Explanations"><a href="#Towards-a-fuller-understanding-of-neurons-with-Clustered-Compositional-Explanations" class="headerlink" title="Towards a fuller understanding of neurons with Clustered Compositional Explanations"></a>Towards a fuller understanding of neurons with Clustered Compositional Explanations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18443">http://arxiv.org/abs/2310.18443</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krlgroup/clustered-compositional-explanations">https://github.com/krlgroup/clustered-compositional-explanations</a></li>
<li>paper_authors: Biagio La Rosa, Leilani H. Gilpin, Roberto Capobianco</li>
<li>for: 本研究旨在提出一种新的 neuron 行为预测方法，即 Clustered Compositional Explanations，以拓宽 neuron 活动谱的spectrum。</li>
<li>methods: 本研究使用 Compositional Explanations 方法，并将其与归一化和一种新的搜索算法结合，以便更好地预测 neuron 的行为。</li>
<li>results: 本研究通过分析不同谱activation的问题和提出了一些 desiderata 质量，以便评估不同算法返回的解释的有效性。<details>
<summary>Abstract</summary>
Compositional Explanations is a method for identifying logical formulas of concepts that approximate the neurons' behavior. However, these explanations are linked to the small spectrum of neuron activations (i.e., the highest ones) used to check the alignment, thus lacking completeness. In this paper, we propose a generalization, called Clustered Compositional Explanations, that combines Compositional Explanations with clustering and a novel search heuristic to approximate a broader spectrum of the neurons' behavior. We define and address the problems connected to the application of these methods to multiple ranges of activations, analyze the insights retrievable by using our algorithm, and propose desiderata qualities that can be used to study the explanations returned by different algorithms.
</details>
<details>
<summary>摘要</summary>
《 compositional explanations 是一种方法，用于identifying logical formulas of concepts that approximate the neurons' behavior。然而，这些解释与小谱activations（即用于检查alignment的最高一些）相关，因此缺乏完整性。在这篇论文中，我们提出了一种扩展，called Clustered Compositional Explanations，它将 Compositional Explanations 与 clustering 和一种新的搜索规则相结合，以approximate a broader spectrum of the neurons' behavior。我们定义并讨论了应用这些方法到多个范围的活动问题，分析了使用我们的算法可以获得的洞察，并提出了对不同算法返回的解释的希望质量。》Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="On-the-Fairness-ROAD-Robust-Optimization-for-Adversarial-Debiasing"><a href="#On-the-Fairness-ROAD-Robust-Optimization-for-Adversarial-Debiasing" class="headerlink" title="On the Fairness ROAD: Robust Optimization for Adversarial Debiasing"></a>On the Fairness ROAD: Robust Optimization for Adversarial Debiasing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18413">http://arxiv.org/abs/2310.18413</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fairmlresearch/road">https://github.com/fairmlresearch/road</a></li>
<li>paper_authors: Vincent Grari, Thibault Laugel, Tatsunori Hashimoto, Sylvain Lamprier, Marcin Detyniecki</li>
<li>for: 本研究旨在解决分布式公平性问题，保证预测结果在不同敏感组中具有地域性均衡。</li>
<li>methods: 我们提出了一种名为ROAD的新方法，基于分布式Robust优化（DRO）框架和公平对抗学习目标，通过一种实例级别的重量策略，优先级给可能存在地方不公平的输入。</li>
<li>results: 实验结果表明，ROAD方法可以在三个标准数据集上实现Pareto优化，即同时保证地域性均衡和全球公平性，并且在分布shift情况下提高公平性泛化性。<details>
<summary>Abstract</summary>
In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which ensures that the predictor is unbiased not only in terms of expectations over the whole population, but also within any subregion of the feature space, unknown at training time. To enforce this objective, we introduce ROAD, a novel approach that leverages the Distributionally Robust Optimization (DRO) framework within a fair adversarial learning objective, where an adversary tries to infer the sensitive attribute from the predictions. Using an instance-level re-weighting strategy, ROAD is designed to prioritize inputs that are likely to be locally unfair, i.e. where the adversary faces the least difficulty in reconstructing the sensitive attribute. Numerical experiments demonstrate the effectiveness of our method: it achieves Pareto dominance with respect to local fairness and accuracy for a given global fairness level across three standard datasets, and also enhances fairness generalization under distribution shift.
</details>
<details>
<summary>摘要</summary>
在算法公平领域，大量关注集合公平标准，如人口学性别比和等值机会。然而，这些目标，作为总体平均值，已经引起了地方不均衡的持续问题。在这种情况下，我们解决了地方公平问题，以确保预测器在整个人口中不偏袋，而且在任何未知训练时间的子区域中也是不偏袋。为此，我们提出了ROAD，一种基于分布robust优化（DRO）框架的新方法，具有公平反对抗学习目标，其中一个反对手尝试从预测中推断敏感特征。通过实例级别的重量策略，ROAD可以优先级化可能存在地方不公平的输入，即反对手在推断敏感特征时面临最小的困难。 numerically experiment demontrates the effectiveness of our method：it achieves Pareto dominance with respect to local fairness and accuracy for a given global fairness level across three standard datasets, and also enhances fairness generalization under distribution shift.
</details></li>
</ul>
<hr>
<h2 id="Gen2Sim-Scaling-up-Robot-Learning-in-Simulation-with-Generative-Models"><a href="#Gen2Sim-Scaling-up-Robot-Learning-in-Simulation-with-Generative-Models" class="headerlink" title="Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models"></a>Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18308">http://arxiv.org/abs/2310.18308</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pushkal Katara, Zhou Xian, Katerina Fragkiadaki</li>
<li>for: 本研究旨在帮助基础学习的机器人抓取 manipulate的技能，以便在多种环境中学习和应用。</li>
<li>methods: 本研究使用大型生成模型来自动生成3D资产、任务描述、任务分解和奖励函数，从而减少人类的参与度。</li>
<li>results: 研究成功地学习了多种长期任务的策略，而非temporally decomposed reward function无法学习这些任务。 Gen2Sim提供了一种可行的方法来扩大和多样化 robot manipulation技能的学习，并且可以通过时间层次分解来探索RL中的行为发现。<details>
<summary>Abstract</summary>
Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long horizon tasks, where reinforcement learning with non temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation.
</details>
<details>
<summary>摘要</summary>
通用 robot manipulator 需要学习多种 manipulate 技能在多种环境中。现有的 robot 训练管道依赖人类提供动能示例或编程 simulation 环境，并编程 reward 函数 для reinforcement learning。这种人类参与度是扩大 robot 学习的重要瓶颈。我们提出 Generation to Simulation（Gen2Sim）方法，用于扩大 robot 技能学习在 simulation 中。我们使用大型预训练的语言和视觉生成模型自动生成 3D 资产、任务描述、任务分解和 reward 函数。我们使用图像扩散模型将开放世界 2D 物体中的图像映射到 3D，并使用 LLMS 确定物理参数。给定 URDF 文件生成和人类开发的资产，我们使用链式思维 Prompt LLMs 将它们映射到相关的任务描述、时间分解和相应的 Python  reward 函数。我们证明 Gen2Sim 可以学习多种长期任务的策略，而 reinforcement learning 无法使用非时间分解的 reward 函数。Gen2Sim 为 robot manipulator 在 simulation 中的学习提供了一条可行的道路，不仅扩大和多样化任务和环境开发，还促进了通过时间分解在 RL 中发现执行 behaviors 的发现。我们的工作提供了数百个模拟资产、任务和示例，为完全自主 robotic manipulation 技能获得做出了一步进展。
</details></li>
</ul>
<hr>
<h2 id="A-Stability-Principle-for-Learning-under-Non-Stationarity"><a href="#A-Stability-Principle-for-Learning-under-Non-Stationarity" class="headerlink" title="A Stability Principle for Learning under Non-Stationarity"></a>A Stability Principle for Learning under Non-Stationarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18304">http://arxiv.org/abs/2310.18304</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chengpiao Huang, Kaizheng Wang</li>
<li>for: 这个研究旨在开发一个适应非站ARY环境的统计学学习框架。</li>
<li>methods: 这个方法利用稳定原则选择每个时间段的回顾窗口，以最大化历史数据的利用，并保持累累合错的总错误在接受随机错误的变化范围内。</li>
<li>results: 论述显示这个方法在不知道非站ARY的情况下也能够适应。 regret bound是最大化对应损失的最小化最大化对应损失，即logarithmic factor。研究中的两个新成果包括一个Function similarity度量和一个分 segmentation技术。<details>
<summary>Abstract</summary>
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
</details>
<details>
<summary>摘要</summary>
我们开发了一个灵活的统计学学习框架，适用于不稳定的环境。每个时间间隔，我们的方法会选择一个稳定原则来选择最大化历史数据的利用，同时保持积累偏差在接受范围内的偏差。我们的理论表明这种方法在未知非站ARY情况下具有适应性。我们的 regret bound是最小化的最大化因子，当人口损失是强Converter或Lipschitz时。我们的分析中包括两个新的组成部分：一种函数相似度度量和非站ARY数据序列分割技术。
</details></li>
</ul>
<hr>
<h2 id="Socially-Cognizant-Robotics-for-a-Technology-Enhanced-Society"><a href="#Socially-Cognizant-Robotics-for-a-Technology-Enhanced-Society" class="headerlink" title="Socially Cognizant Robotics for a Technology Enhanced Society"></a>Socially Cognizant Robotics for a Technology Enhanced Society</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18303">http://arxiv.org/abs/2310.18303</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kristin J. Dana, Clinton Andrews, Kostas Bekris, Jacob Feldman, Matthew Stone, Pernille Hemmer, Aaron Mazzeo, Hal Salzman, Jingang Yi</li>
<li>for: 本研究旨在推动人类中心的机器人应用，并关注其影响的问题。</li>
<li>methods: 本研究提出了一种涉猛社会科学方法，将技术和社会科学方法相结合，以便在机器人行为中推动参与者参与和社会评估。</li>
<li>results: 研究发现，通过将人类中心的目标放在首位，可以开拓出许多新的研究视角和问题，以改善机器人与人类之间的交互，并对社会产生的影响。<details>
<summary>Abstract</summary>
Emerging applications of robotics, and concerns about their impact, require the research community to put human-centric objectives front-and-center. To meet this challenge, we advocate an interdisciplinary approach, socially cognizant robotics, which synthesizes technical and social science methods. We argue that this approach follows from the need to empower stakeholder participation (from synchronous human feedback to asynchronous societal assessment) in shaping AI-driven robot behavior at all levels, and leads to a range of novel research perspectives and problems both for improving robots' interactions with individuals and impacts on society. Drawing on these arguments, we develop best practices for socially cognizant robot design that balance traditional technology-based metrics (e.g. efficiency, precision and accuracy) with critically important, albeit challenging to measure, human and society-based metrics.
</details>
<details>
<summary>摘要</summary>
新兴应用场景和对其影响的担忧，需要研究社区将人类中心的目标置于首位。为解决这个挑战，我们支持跨学科的方法，社会认知机器人，它将技术和社会科学方法相结合。我们认为，这种方法来自参与者参与（从同步人类反馈到异步社会评估）在AI驱动机器人行为的形成中发挥作用，并导致了改善机器人与个人交互以及对社会的影响的新研究视角和问题。从这些理由，我们开发了社会认知机器人的最佳实践，权衡传统技术基础的指标（如效率、准确率）与人类和社会基础的指标，这些指标具有挑战性，但对于机器人的设计和应用至关重要。
</details></li>
</ul>
<hr>
<h2 id="Interactive-Motion-Planning-for-Autonomous-Vehicles-with-Joint-Optimization"><a href="#Interactive-Motion-Planning-for-Autonomous-Vehicles-with-Joint-Optimization" class="headerlink" title="Interactive Motion Planning for Autonomous Vehicles with Joint Optimization"></a>Interactive Motion Planning for Autonomous Vehicles with Joint Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18301">http://arxiv.org/abs/2310.18301</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxiao Chen, Sushant Veer, Peter Karkus, Marco Pavone</li>
<li>for:  This paper is written for planning safe motions for autonomous vehicles in highly interactive driving scenarios.</li>
<li>methods:  The paper uses deep-learning-based models for trajectory prediction and joint optimization with model predictive control (MPC) to leverage ego-conditioned prediction.</li>
<li>results:  The proposed Interactive Joint Planning (IJP) method significantly outperforms baselines in closed-loop simulation, demonstrating its effectiveness in providing safe and efficient motions for autonomous vehicles in interactive driving scenarios.Here’s the Chinese translation of the three points:</li>
<li>for: 这篇论文是为了规划自动驾驶车辆在高度互动的驾驶场景中安全的运动计划。</li>
<li>methods: 该论文使用深度学习基于模型来预测轨迹并与模型预测控制（MPC）结合进行联合优化，以利用egos conditioned预测。</li>
<li>results: 提出的互动联合规划（IJP）方法在关闭Loop simulation中显著超越基准值，demonstrating its effectiveness in providing safe and efficient motions for autonomous vehicles in interactive driving scenarios.<details>
<summary>Abstract</summary>
In highly interactive driving scenarios, the actions of one agent greatly influences those of its neighbors. Planning safe motions for autonomous vehicles in such interactive environments, therefore, requires reasoning about the impact of the ego's intended motion plan on nearby agents' behavior. Deep-learning-based models have recently achieved great success in trajectory prediction and many models in the literature allow for ego-conditioned prediction. However, leveraging ego-conditioned prediction remains challenging in downstream planning due to the complex nature of neural networks, limiting the planner structure to simple ones, e.g., sampling-based planner. Despite their ability to generate fine-grained high-quality motion plans, it is difficult for gradient-based planning algorithms, such as model predictive control (MPC), to leverage ego-conditioned prediction due to their iterative nature and need for gradient. We present Interactive Joint Planning (IJP) that bridges MPC with learned prediction models in a computationally scalable manner to provide us the best of both the worlds. In particular, IJP jointly optimizes over the behavior of the ego and the surrounding agents and leverages deep-learned prediction models as prediction priors that the join trajectory optimization tries to stay close to. Furthermore, by leveraging homotopy classes, our joint optimizer searches over diverse motion plans to avoid getting stuck at local minima. Closed-loop simulation result shows that IJP significantly outperforms the baselines that are either without joint optimization or running sampling-based planning.
</details>
<details>
<summary>摘要</summary>
在高度互动的驾驶场景中，一个agent的行为会深刻影响其周围的其他agent。因此，为自动驾驶车辆在这些互动环境中规划安全的动作计划，需要考虑ego的意图动作计划对周围agent的行为的影响。深度学习基于模型在轨迹预测方面刚果取得了很大成功，但是在下游规划中利用egoconditioned预测仍然具有挑战性，因为神经网络的复杂性限制了规划结构的选择，只能选择简单的采样基本预测器。尽管它们可以生成细腻高质量的动作计划，但是使用梯度计算法，如模型预测控制（MPC），利用egoconditioned预测却困难，因为它们的迭代性和需要梯度。我们提出了互动联合规划（IJP），它将MPC与学习预测模型在计算可扩展的方式联系起来，以获得最佳的世界。具体来说，IJP同时优化ego和周围agent的行为，并利用深度学习预测模型作为预测假设，Join trajectory optimization尝试保持近于预测。此外，通过Homotopy类，我们的联合优化器搜索到多种动作计划，以避免陷入地点附近的局部最佳解。关闭环境 simulate结果表明，IJP显著超过了不包含联合优化或运行采样基本预测的基eline。
</details></li>
</ul>
<hr>
<h2 id="Image-Clustering-Conditioned-on-Text-Criteria"><a href="#Image-Clustering-Conditioned-on-Text-Criteria" class="headerlink" title="Image Clustering Conditioned on Text Criteria"></a>Image Clustering Conditioned on Text Criteria</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18297">http://arxiv.org/abs/2310.18297</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sehyunkwon/ictc">https://github.com/sehyunkwon/ictc</a></li>
<li>paper_authors: Sehyun Kwon, Jaeseung Park, Minkyu Kim, Jaewoong Cho, Ernest K. Ryu, Kangwook Lee</li>
<li>for: 图像 clustering based on user-specified text criteria</li>
<li>methods: 利用现代视觉语言模型和大语言模型，实现图像 clustering Conditional on Text Criteria (IC$|$TC)</li>
<li>results: 在不同的基准下，IC$|$TC 可以有效地对图像进行分 clustering，并与基eline 相比显著提高表现。<details>
<summary>Abstract</summary>
Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC$|$TC), and it represents a different paradigm of image clustering. IC$|$TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC$|$TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.
</details>
<details>
<summary>摘要</summary>
传统的帮助方法不提供用户直接控制帮助结果，并且帮助结果可能不符合用户有意思的标准。在这种工作中，我们介绍了一种新的图像帮助方法，基于用户指定的文本标准。我们称之为图像帮助 conditional on 文本标准（IC$|$TC），它代表了一种新的帮助方法 paradigm。IC$|$TC需要最小化和实用的人类干预，并为用户提供了较高的控制权，以换取更好的帮助结果。我们的实验表明，IC$|$TC可以有效地将图像分类到不同的标准，如人类动作、物理位置或人的情绪，而与基准值相比显著性能更高。
</details></li>
</ul>
<hr>
<h2 id="Moments-for-Perceptive-Narration-Analysis-Through-the-Emotional-Attachment-of-Audience-to-Discourse-and-Story"><a href="#Moments-for-Perceptive-Narration-Analysis-Through-the-Emotional-Attachment-of-Audience-to-Discourse-and-Story" class="headerlink" title="Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story"></a>Moments for Perceptive Narration Analysis Through the Emotional Attachment of Audience to Discourse and Story</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18273">http://arxiv.org/abs/2310.18273</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gary Bruins, Ergun Akleman</li>
<li>for: 这篇论文的目的是开发一个可以分析视觉故事的理论框架，从而更好地理解电影、漫画等视觉故事的效果。</li>
<li>methods: 这篇论文引入了一个新的故事元素called “moments”，并提出了一种方法来分解线性故事（如电影）into a set of moments。这些 moments 可以分为两类：Story moments 和 Discourse moments。每种类型的 moment 可以进一步分为三种类型的 universal storytelling moments，这些 moments 可以增强或削弱观众对角色或故事的情感附加。</li>
<li>results: 这篇论文提出了一种方法来目录各种 universal moments 的出现，并使用曲线或颜色带来可视化角色的旅程。此外， authors 还证明了 story moments 和 Discourse moments 都可以转化为一个总趋势参数，这个参数可以在时间轴上Plot 出观众对故事的情感附加情况。<details>
<summary>Abstract</summary>
In this work, our goal is to develop a theoretical framework that can eventually be used for analyzing the effectiveness of visual stories such as feature films to comic books. To develop this theoretical framework, we introduce a new story element called moments. Our conjecture is that any linear story such as the story of a feature film can be decomposed into a set of moments that follow each other. Moments are defined as the perception of the actions, interactions, and expressions of all characters or a single character during a given time period. We categorize the moments into two major types: story moments and discourse moments. Each type of moment can further be classified into three types, which we call universal storytelling moments. We believe these universal moments foster or deteriorate the emotional attachment of the audience to a particular character or the story. We present a methodology to catalog the occurrences of these universal moments as they are found in the story. The cataloged moments can be represented using curves or color strips. Therefore, we can visualize a character's journey through the story as either a 3D curve or a color strip. We also demonstrated that both story and discourse moments can be transformed into one lump-sum attraction parameter. The attraction parameter in time provides a function that can be plotted graphically onto a timeline illustrating changes in the emotional attachment of audience to a character or the story. By inspecting these functions the story analyst can analytically decipher the moments in the story where the attachment is being established, maintained, strengthened, or conversely where it is languishing.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们的目标是开发一个理论框架，以便分析视觉故事，从电影到漫画。为了实现这个目标，我们引入了一个新的故事元素，即“时刻”（moments）。我们的假设是，任何线性故事，例如电影的故事，都可以分解成一系列的时刻，这些时刻继承于一个时间段内的人物或单一人物的行动、互动和表达。我们将时刻分类为两大类：剧情时刻和对话时刻。每种时刻可以进一步分为三种通用故事创作时刻。我们认为这些通用时刻会使观众对特定人物或故事产生情感附加或减少。我们提出了一种方法来目录这些通用时刻的出现，并可以使用曲线或颜色带来表示人物的旅程。我们还证明了，剧情和对话时刻都可以转化为一个累积参数。这个参数在时间上提供了一个函数，可以在时间轴上Plot，并表示观众对人物或故事的情感附加或减少的变化。通过查看这些函数，故事分析人员可以分析故事中情感附加的时刻，以及将其建立、维护、强化或反之。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Search-Feasible-and-Infeasible-Regions-of-Routing-Problems-with-Flexible-Neural-k-Opt"><a href="#Learning-to-Search-Feasible-and-Infeasible-Regions-of-Routing-Problems-with-Flexible-Neural-k-Opt" class="headerlink" title="Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt"></a>Learning to Search Feasible and Infeasible Regions of Routing Problems with Flexible Neural k-Opt</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18264">http://arxiv.org/abs/2310.18264</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yining043/neuopt">https://github.com/yining043/neuopt</a></li>
<li>paper_authors: Yining Ma, Zhiguang Cao, Yeow Meng Chee</li>
<li>for: 本研究开展了一种基于学习搜索的路径规划算法NeuOpt，用于解决路径规划问题。</li>
<li>methods: NeuOpt使用了一种特定的动作因子化方法和一种自定义的循环双流解码器，以学习具有灵活k-选择的搜索策略。此外，paper还提出了一种引导不可能区域探索（GIRE）方案，以便更好地让搜索算法自主探索可行和不可行的区域。</li>
<li>results: 实验表明，NeuOpt在TSP和CVRP问题上显著超越了现有的面罩-based L2S算法，同时也超越了L2C和L2P算法。此外，paper还提供了一些新的思路来处理VRP约束。<details>
<summary>Abstract</summary>
In this paper, we present Neural k-Opt (NeuOpt), a novel learning-to-search (L2S) solver for routing problems. It learns to perform flexible k-opt exchanges based on a tailored action factorization method and a customized recurrent dual-stream decoder. As a pioneering work to circumvent the pure feasibility masking scheme and enable the autonomous exploration of both feasible and infeasible regions, we then propose the Guided Infeasible Region Exploration (GIRE) scheme, which supplements the NeuOpt policy network with feasibility-related features and leverages reward shaping to steer reinforcement learning more effectively. Additionally, we equip NeuOpt with Dynamic Data Augmentation (D2A) for more diverse searches during inference. Extensive experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that our NeuOpt not only significantly outstrips existing (masking-based) L2S solvers, but also showcases superiority over the learning-to-construct (L2C) and learning-to-predict (L2P) solvers. Notably, we offer fresh perspectives on how neural solvers can handle VRP constraints. Our code is available: https://github.com/yining043/NeuOpt.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种名为Neural k-Opt（NeuOpt）的学习到搜索（L2S）算法，用于解决路径问题。它学习如何进行灵活的 k-opt 交换，基于一种适应性的动作因子化方法和一种自定义的循环双流解码器。作为一种突破约束 маскинг 方案的先锋性工作，我们然后提出了指导不可能区域探索（GIRE）方案，该方案在NeuOpt策略网络中添加了可行性相关特征，并通过奖励形成来更有效地驱动学习。此外，我们还为NeuOpt提供了动态数据扩充（D2A）以在推理中进行更多的搜索。我们在旅行商问题（TSP）和容量有限的交通问题（CVRP）进行了广泛的实验，结果表明，我们的NeuOpt不仅明显超越了现有的（masking-based）L2S算法，还超越了学习到构建（L2C）和学习到预测（L2P）算法。另外，我们还提供了一些新的视角，用于描述如何使用神经网络来处理 VRP 约束。我们的代码可以在 GitHub 上找到：https://github.com/yining043/NeuOpt。
</details></li>
</ul>
<hr>
<h2 id="A-Review-of-the-Evidence-for-Existential-Risk-from-AI-via-Misaligned-Power-Seeking"><a href="#A-Review-of-the-Evidence-for-Existential-Risk-from-AI-via-Misaligned-Power-Seeking" class="headerlink" title="A Review of the Evidence for Existential Risk from AI via Misaligned Power-Seeking"></a>A Review of the Evidence for Existential Risk from AI via Misaligned Power-Seeking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18244">http://arxiv.org/abs/2310.18244</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rose Hadshar</li>
<li>for: 该论文探讨了人工智能（AI）可能对人类 pose existential risks 的证据，具体来说是通过偏向和权力寻求。</li>
<li>methods: 该论文对偏向和权力寻求的证据进行了评估，包括实证证据、概念性证据和专家意见。</li>
<li>results: 论文发现，虽然目前没有公开的实证例子表明 AI 系统会发展出偏向和权力寻求，但是理论上的证据和实验证据表明这种风险存在。因此，无法 completly 排除 AI via 偏向和权力寻求对人类 pose existential risks 的可能性。<details>
<summary>Abstract</summary>
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose existential risks. This paper reviews the evidence for existential risks from AI via misalignment, where AI systems develop goals misaligned with human values, and power-seeking, where misaligned AIs actively seek power. The review examines empirical findings, conceptual arguments and expert opinion relating to specification gaming, goal misgeneralization, and power-seeking. The current state of the evidence is found to be concerning but inconclusive regarding the existence of extreme forms of misaligned power-seeking. Strong empirical evidence of specification gaming combined with strong conceptual evidence for power-seeking make it difficult to dismiss the possibility of existential risk from misaligned power-seeking. On the other hand, to date there are no public empirical examples of misaligned power-seeking in AI systems, and so arguments that future systems will pose an existential risk remain somewhat speculative. Given the current state of the evidence, it is hard to be extremely confident either that misaligned power-seeking poses a large existential risk, or that it poses no existential risk. The fact that we cannot confidently rule out existential risk from AI via misaligned power-seeking is cause for serious concern.
</details>
<details>
<summary>摘要</summary>
人工智能（AI）的快速发展已引发了专家、政策制定者和世界领袖对AI系统可能对人类存在潜在的极大风险的担忧。这篇评论文章检查了AI系统发展不同目标的证据，包括 specification gaming、目标扩展和权力寻求。审查的证据表明，虽然目前没有公共的实证例子，但概念上的证据强，表明AI系统可能会发展出不同于人类价值观的目标。此外，由于目前的证据状况，无法绝对排除AI系统可能对人类存在极大风险的可能性。因此，我们应该对这一点表示严重关注。
</details></li>
</ul>
<hr>
<h2 id="Fine-Tuning-Language-Models-Using-Formal-Methods-Feedback"><a href="#Fine-Tuning-Language-Models-Using-Formal-Methods-Feedback" class="headerlink" title="Fine-Tuning Language Models Using Formal Methods Feedback"></a>Fine-Tuning Language Models Using Formal Methods Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18239">http://arxiv.org/abs/2310.18239</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunhao Yang, Neel P. Bhatt, Tyler Ingebrand, William Ward, Steven Carr, Zhangyang Wang, Ufuk Topcu</li>
<li>for: 这个论文旨在提高预训练语言模型的应用在自动控制领域，使其能够更好地满足具体任务的需求。</li>
<li>methods: 这篇论文提出了一种完全自动的 fine-tuning 方法，通过自然语言任务描述 guideline 将预训练语言模型转换为具体任务的控制器，并通过世界模型来验证这些控制器的合liance。</li>
<li>results: 论文提供了多个自动驾驶任务的实验结果，表明该方法可以在不同的任务上提高预训练语言模型的性能，从60%提高到90%。<details>
<summary>Abstract</summary>
Although pre-trained language models encode generic knowledge beneficial for planning and control, they may fail to generate appropriate control policies for domain-specific tasks. Existing fine-tuning methods use human feedback to address this limitation, however, sourcing human feedback is labor intensive and costly. We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers with high compliance with the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidences, primarily in autonomous driving, to demonstrate the method's effectiveness across multiple tasks. The results indicate an improvement in percentage of specifications satisfied by the controller from 60% to 90%.
</details>
<details>
<summary>摘要</summary>
Our method uses natural language task descriptions to guide the synthesis of automaton-based controllers from pre-trained models. These controllers are verifiable against independently provided specifications within a world model, which can be abstract or obtained from a high-fidelity simulator. Controllers that comply with the desired specifications receive higher ranks, guiding the iterative fine-tuning process.We provide quantitative evidence, primarily in the field of autonomous driving, to demonstrate the effectiveness of our method. The results show an improvement in the percentage of specifications satisfied by the controller, from 60% to 90%.
</details></li>
</ul>
<hr>
<h2 id="Davidsonian-Scene-Graph-Improving-Reliability-in-Fine-grained-Evaluation-for-Text-to-Image-Generation"><a href="#Davidsonian-Scene-Graph-Improving-Reliability-in-Fine-grained-Evaluation-for-Text-to-Image-Generation" class="headerlink" title="Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation"></a>Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18235">http://arxiv.org/abs/2310.18235</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang<br>for: 这个论文的目的是evaluating text-to-image models的可靠性。methods: 这个论文使用了Question Generation and Answering（QG&#x2F;A）方法，通过使用预训练的基础模型生成提问和答案，然后根据提问生成的答案和图像是否一致来评估图像的可靠性。results: 这个论文通过提出和解决一些可靠性问题（如提问不应该包含幻像、重复或漏掉信息），并使用Davidsonian Scene Graph（DSG）评估框架来提高评估的可靠性。DSG使用图表来组织提问和答案，以确保提问的 semantic coverage 和答案的一致性。经过广泛的实验和人工评估，这个论文证明了DSG可以有效地解决这些问题。<details>
<summary>Abstract</summary>
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG is an automatic, graph-based QG/A that is modularly implemented to be adaptable to any QG/A module. DSG produces atomic and unique questions organized in dependency graphs, which (i) ensure appropriate semantic coverage and (ii) sidestep inconsistent answers. With extensive experimentation and human evaluation on a range of model configurations (LLM, VQA, and T2I), we empirically demonstrate that DSG addresses the challenges noted above. Finally, we present DSG-1k, an open-sourced evaluation benchmark that includes 1,060 prompts, covering a wide range of fine-grained semantic categories with a balanced distribution. We release the DSG-1k prompts and the corresponding DSG questions.
</details>
<details>
<summary>摘要</summary>
评估文本到图像模型是非常困难的。一种强大的最近的方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成了一组问题和答案从提示中，然后根据图像输出的答案是否与提示基础答案一致来评分。这种评估方法自然地受到基础QG和QA模型的质量的影响。我们 indentify和解决了现有QG/A工作中的一些可靠性挑战：（a）QG问题应该遵循提示（避免幻象、重复和漏掉），（b）VQA答案应该一致（不能声称图像中没有摩托车而同时声称摩托车是蓝色）。我们使用戴维森景图（DSG）来解决这些问题，DSG是基于形式 semantics的实际训练的评估框架。DSG自动生成了原子和唯一的问题，组织成依赖图，以确保适当的semantic Coverage并且 circumvent不一致的答案。通过广泛的实验和人工评估，我们证明了DSG可以解决上述挑战。最后，我们提供了DSG-1k，一个开源的评估标准 benchmark，包括1,060个提示，覆盖了各种细化的semantic类别，并且具有良好的分布。我们发布了DSG-1k提示和相应的DSG问题。
</details></li>
</ul>
<hr>
<h2 id="Alignment-and-Outer-Shell-Isotropy-for-Hyperbolic-Graph-Contrastive-Learning"><a href="#Alignment-and-Outer-Shell-Isotropy-for-Hyperbolic-Graph-Contrastive-Learning" class="headerlink" title="Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning"></a>Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18209">http://arxiv.org/abs/2310.18209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Zhang, Hao Zhu, Jiahong Liu, Piotr Koniusz, Irwin King</li>
<li>for: 学习高质量自主图表示，以便下游任务的改进。</li>
<li>methods: 提出了一种新的对偶学习框架，包括设计了有效地捕捉层次数据不变信息的对齐度量，以及提出了一种取代均匀度量来避免维度塌降问题。</li>
<li>results: 在不同的гипербо利图表示技术上，通过自动匹配度量和均匀度量来学习高质量图表示，并在supervised和自主学习设置下实现了较高的效果。<details>
<summary>Abstract</summary>
Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotropic ring density towards boundaries of Poincar\'e ball. This ring density can be easily imposed by promoting the isotropic feature distribution on the tangent space of manifold. In the experiments, we demonstrate the efficacy of our proposed method across different hyperbolic graph embedding techniques in both supervised and self-supervised learning settings.
</details>
<details>
<summary>摘要</summary>
学习良好的自我超VIewgraph representation是挑战性较高的。contrastive learning方法在这些方法中具有竞争性的表现。contrastive learning的嵌入是在一个径向体上安排的，这使得在欧几何空间中可以使用cosine距离测量。然而，许多领域的下游任务中的数据结构具有非欧几何的隐藏几何结构。为此，我们提出了一种新的对比学习框架，以学习高质量的图像嵌入。具体来说，我们设计了一个对应度度量，可以有效地捕捉层次数据不变信息，同时我们也提出了一种取代均匀度量来避免叫做dimensional collapse。我们发现在拓扑空间中，需要 Addressing leaf-和height-level uniformity，这与树的性质有关。而在拓扑空间中的 ambient space 中，这些概念转化为在Poincaré球的边界上强制实施一个均匀环绕径。这个环绕径可以通过推动拓扑空间的 tangent space 上的均匀特征分布来实现。在实验中，我们证明了我们提出的方法在不同的拓扑空间中的几何图像嵌入技术中具有效果，并在自我超VIewgraph embedding中和supervised learning中进行了证明。
</details></li>
</ul>
<hr>
<h2 id="Is-Scaling-Learned-Optimizers-Worth-It-Evaluating-The-Value-of-VeLO’s-4000-TPU-Months"><a href="#Is-Scaling-Learned-Optimizers-Worth-It-Evaluating-The-Value-of-VeLO’s-4000-TPU-Months" class="headerlink" title="Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO’s 4000 TPU Months"></a>Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO’s 4000 TPU Months</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18191">http://arxiv.org/abs/2310.18191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fady Rezk, Antreas Antoniou, Henry Gouk, Timothy Hospedales</li>
<li>for: 本研究是用来训练一个通用的”基础”优化器的最大规模尝试。</li>
<li>methods: 本研究使用了 thousands of 机器学习任务和 over 4000 TPU 月份，以 Produce 一个可以泛化到新问题的优化器，并且不需要 гиперparameters 的调整。</li>
<li>results: 我们发现，与初始声明不符的结论：（1） VeLO 有一个关键的 гиперparameters 需要具体问题的调整，（2） VeLO 并不一定可以在解决质量上超越竞争对手，（3） VeLO 不一定比竞争优化器更快地降低训练损失。这些观察结论质疑 VeLO 的通用性和投资训练它的价值。<details>
<summary>Abstract</summary>
We analyze VeLO (versatile learned optimizer), the largest scale attempt to train a general purpose "foundational" optimizer to date. VeLO was trained on thousands of machine learning tasks using over 4000 TPU months with the goal of producing an optimizer capable of generalizing to new problems while being hyperparameter free, and outperforming industry standards such as Adam. We independently evaluate VeLO on the MLCommons optimizer benchmark suite. We find that, contrary to initial claims: (1) VeLO has a critical hyperparameter that needs problem-specific tuning, (2) VeLO does not necessarily outperform competitors in quality of solution found, and (3) VeLO is not faster than competing optimizers at reducing the training loss. These observations call into question VeLO's generality and the value of the investment in training it.
</details>
<details>
<summary>摘要</summary>
我们分析了VeLO（多功能学习优化器），目前最大规模的尝试是用多种机器学习任务来训练一个通用的“基础”优化器。VeLO在多达4000个TPU月的训练时间和4000个机器学习任务上被训练，以产生一个能够泛化到新问题的优化器，并且不需要任何hyperparameter。我们独立评估了VeLO在MLCommons优化器benchmark集合中的性能。我们发现：1. VeLO有一个关键的hyperparameter需要问题特定的调整。2. VeLO不一定能够超越竞争对手在解决问题的质量上。3. VeLO不一定比竞争对手快速地减少训练损失。这些观察结果质疑了VeLO的通用性和投资训练它的价值。
</details></li>
</ul>
<hr>
<h2 id="Personas-as-a-Way-to-Model-Truthfulness-in-Language-Models"><a href="#Personas-as-a-Way-to-Model-Truthfulness-in-Language-Models" class="headerlink" title="Personas as a Way to Model Truthfulness in Language Models"></a>Personas as a Way to Model Truthfulness in Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18168">http://arxiv.org/abs/2310.18168</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, He He</li>
<li>for: This paper explores the ability of large language models to discern truth from falsehood in contradictory data.</li>
<li>methods: The authors hypothesize that language models can cluster truthful text by modeling a truthful persona, which is a group of agents that are likely to produce truthful text and share similar features. They use arithmetics as a synthetic environment to test this hypothesis.</li>
<li>results: The authors find that language models can separate true and false statements, and generalize truthfulness across agents, but only if the agents in the training data share a truthful generative process that enables the creation of a truthful persona. This suggests that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.<details>
<summary>Abstract</summary>
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.
</details>
<details>
<summary>摘要</summary>
我们的实验证明了这个人格假设，通过以下两个观察：（1）我们可以在模型生成答案之前检查其是否为真实的；（2）在训练集中对一些事实进行调整，可以提高模型对未见过的主题上的真实性。接下来，我们使用数学为Synthetic环境，证明语言模型可以分辨真实和假的声明，并将真实性扩展到不同的代理人。但是，只有在训练资料中的代理人具有真实生成过程，才能够创建一个真实的人格。总的来说，我们的发现表明了模型可以运用数据的层次结构，学习抽象概念如真实性。
</details></li>
</ul>
<hr>
<h2 id="Improving-Intrinsic-Exploration-by-Creating-Stationary-Objectives"><a href="#Improving-Intrinsic-Exploration-by-Creating-Stationary-Objectives" class="headerlink" title="Improving Intrinsic Exploration by Creating Stationary Objectives"></a>Improving Intrinsic Exploration by Creating Stationary Objectives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18144">http://arxiv.org/abs/2310.18144</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roger Creus Castanyer, Joshua Romoff, Glen Berseth</li>
<li>for: 提高 Agent 在探索问题中的性能，特别是在稀缺奖励 Task 和高维 Observation 等难题上。</li>
<li>methods: 利用 Count-based 方法 derivate 探索奖励，并通过 Stationary Objectives For Exploration (SOFE) 框架将原始非站点奖励转化为站点奖励，以便更好地优化 Agent 的目标。</li>
<li>results: 在多种探索问题中，包括稀缺奖励 Task、像素基 Observation、3D 导航和生成的环境等，SOFE 能够提高 Agent 的性能。<details>
<summary>Abstract</summary>
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the agents' performance in challenging exploration problems, including sparse-reward tasks, pixel-based observations, 3D navigation, and procedurally generated environments.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：探索奖励在强化学习中引导长期探索，定义自定义内在目标。计数基本方法使用状态访问频率 derive 探索奖励。我们发现，任何基于计数基本方法 derive 的内在奖励都是非站ARY的，因此难以优化代理人的目标。我们的工作关键在于将原始非站ARY奖励转化为站ARY奖励，通过增强状态表示来实现。为此，我们提出了站ARY目标 для探索 (SOFE) 框架。SOFE需要确定不同探索奖励的 suffiSing statistic 和有效地编码这些统计作为深度网络的输入。SOFE基于提出状态扩展，既可以扩大状态空间，又可以简化代理人的目标优化。我们的实验表明，SOFE在复杂探索问题中提高了代理人的表现，包括罕见奖励任务、像素基本观察、3D导航和生成环境。</SYS>Note: The translation is in Simplified Chinese, which is the standard Chinese writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format instead.
</details></li>
</ul>
<hr>
<h2 id="Ask-more-know-better-Reinforce-Learned-Prompt-Questions-for-Decision-Making-with-Large-Language-Models"><a href="#Ask-more-know-better-Reinforce-Learned-Prompt-Questions-for-Decision-Making-with-Large-Language-Models" class="headerlink" title="Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models"></a>Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18127">http://arxiv.org/abs/2310.18127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xue Yan, Yan Song, Xinyu Cui, Filippos Christianos, Haifeng Zhang, David Henry Mguni, Jun Wang</li>
<li>for: This paper aims to develop a fully integrated end-to-end framework for task-solving in real settings using complicated reasoning.</li>
<li>methods: The proposed leader-follower bilevel framework learns to ask relevant questions (prompts) and undertake reasoning to guide the learning of actions to be performed in an environment. The system uses a prompt-generator policy and an action policy to adapt to the CoT process and take decisive, high-performing actions.</li>
<li>results: The empirical data shows that the proposed system outperforms leading methods in agent learning benchmarks such as Overcooked and FourRoom.<details>
<summary>Abstract</summary>
Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical findings, leading the CoT to consider the anticipated goals. A prompt-generator policy has its own aim in our system, allowing it to adapt to the action policy and automatically root the CoT process towards outputs that lead to decisive, high-performing actions. Meanwhile, the action policy is learning how to use the CoT outputs to take specific actions. Our empirical data reveal that our system outperforms leading methods in agent learning benchmarks such as Overcooked and FourRoom.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在解决实际挑战中展示了其应用潜力，通过结合动作政策和链接思维（CoT）理解。然而，高质量提示是框架的重要 componenet，并且通常需要人工干预以开发基础函数，以确保低层控制器正确处理CoT理解。在这篇论文中，我们将实现完整的终端到终端框架，用于实际设置中的任务解决。为此，我们提出了一个新的领导者-追随者二级框架，能够学习问题（提示）和随后进行理解，以导引行为学习。一个好的提示应该根据历史发现进行 introspective 修订，导引CoT考虑预期目标。在我们的系统中，提示策略有自己的目标，让它适应行为策略，并自动将CoT过程导向出力，以确保高效、决策性的动作。同时，动作策略在使用CoT出力进行特定动作。我们的实验数据显示，我们的系统在代理学习测试 benchmark 中表现出色，比如 Overcooked 和 FourRoom。
</details></li>
</ul>
<hr>
<h2 id="OpinSummEval-Revisiting-Automated-Evaluation-for-Opinion-Summarization"><a href="#OpinSummEval-Revisiting-Automated-Evaluation-for-Opinion-Summarization" class="headerlink" title="OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization"></a>OpinSummEval: Revisiting Automated Evaluation for Opinion Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18122">http://arxiv.org/abs/2310.18122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/a-chicharito-s/opinsummeval">https://github.com/a-chicharito-s/opinsummeval</a></li>
<li>paper_authors: Yuchen Shen, Xiaojun Wan</li>
<li>for: This paper focuses on the evaluation of opinion summarization models, specifically exploring the correlation between automatic metrics and human ratings.</li>
<li>methods: The paper uses a dataset called OpinSummEval, which includes human judgments and outputs from 14 opinion summarization models. The authors explore the correlation between 24 automatic metrics and human ratings across four dimensions.</li>
<li>results: The authors find that metrics based on neural networks generally outperform non-neural ones, but even the best-performing metrics do not consistently correlate well across all dimensions. This highlights the need for advancements in automated evaluation methods for opinion summarization.<details>
<summary>Abstract</summary>
Opinion summarization sets itself apart from other types of summarization tasks due to its distinctive focus on aspects and sentiments. Although certain automated evaluation methods like ROUGE have gained popularity, we have found them to be unreliable measures for assessing the quality of opinion summaries. In this paper, we present OpinSummEval, a dataset comprising human judgments and outputs from 14 opinion summarization models. We further explore the correlation between 24 automatic metrics and human ratings across four dimensions. Our findings indicate that metrics based on neural networks generally outperform non-neural ones. However, even metrics built on powerful backbones, such as BART and GPT-3/3.5, do not consistently correlate well across all dimensions, highlighting the need for advancements in automated evaluation methods for opinion summarization. The code and data are publicly available at https://github.com/A-Chicharito-S/OpinSummEval/tree/main.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-a-Unified-Conversational-Recommendation-System-Multi-task-Learning-via-Contextualized-Knowledge-Distillation"><a href="#Towards-a-Unified-Conversational-Recommendation-System-Multi-task-Learning-via-Contextualized-Knowledge-Distillation" class="headerlink" title="Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation"></a>Towards a Unified Conversational Recommendation System: Multi-task Learning via Contextualized Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18119">http://arxiv.org/abs/2310.18119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yeongseo Jung, Eunseo Jung, Lei Chen</li>
<li>for: 提高对话式推荐系统（CRS）的个性化推荐和对话能力。</li>
<li>methods: 提出了一种基于Contextualized Knowledge Distillation（ConKD）的多任务学习方法，将对话推荐和对话模块结合到一起，以提高推荐性和对话流畅性。</li>
<li>results: 实验表明，我们的单个模型可以显著提高推荐性，同时保持对话流畅性，并与多任务学习方法相比，实现了相似的多样性表现。<details>
<summary>Abstract</summary>
In Conversational Recommendation System (CRS), an agent is asked to recommend a set of items to users within natural language conversations. To address the need for both conversational capability and personalized recommendations, prior works have utilized separate recommendation and dialogue modules. However, such approach inevitably results in a discrepancy between recommendation results and generated responses. To bridge the gap, we propose a multi-task learning for a unified CRS, where a single model jointly learns both tasks via Contextualized Knowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate and soft gate. The former selectively gates between two task-specific teachers, while the latter integrates knowledge from both teachers. Our gates are computed on-the-fly in a context-specific manner, facilitating flexible integration of relevant knowledge. Extensive experiments demonstrate that our single model significantly improves recommendation performance while enhancing fluency, and achieves comparable results in terms of diversity.
</details>
<details>
<summary>摘要</summary>
在协作推荐系统（CRS）中，一个代理被要求在自然语言交流中推荐一组ITEMS给用户。为了解决个性化推荐和对话能力的需求，先前的工作通常使用了分开的推荐和对话模块。然而，这种方法无法快速bridging these two tasks的差异，导致推荐结果与生成的响应之间存在差异。为了bridge这个差异，我们提出了一种多任务学习的统一CRS，其中一个模型同时学习了两个任务 via Contextualized Knowledge Distillation（ConKD）。我们引入了两种ConKD版本：hard gate和soft gate。前者在两个任务特定的教师之间选择性地阻断，而后者将两个教师的知识集成在一起。我们的门控在上下文具体的计算，使得可以在不同的上下文中灵活地集成相关的知识。我们的实验表明，我们的单一模型可以大幅提高推荐性能，同时提高流畅性，并与多任务学习模型相比，在多样性方面实现相似的结果。
</details></li>
</ul>
<hr>
<h2 id="er-autopilot-1-0-The-Full-Autonomous-Stack-for-Oval-Racing-at-High-Speeds"><a href="#er-autopilot-1-0-The-Full-Autonomous-Stack-for-Oval-Racing-at-High-Speeds" class="headerlink" title="er.autopilot 1.0: The Full Autonomous Stack for Oval Racing at High Speeds"></a>er.autopilot 1.0: The Full Autonomous Stack for Oval Racing at High Speeds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18112">http://arxiv.org/abs/2310.18112</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ayoub Raji, Danilo Caporale, Francesco Gatti, Andrea Giove, Micaela Verucchi, Davide Malatesta, Nicola Musiu, Alessandro Toschi, Silviu Roberto Popitanu, Fabio Bagni, Massimiliano Bosi, Alexander Liniger, Marko Bertogna, Daniele Morra, Francesco Amerotti, Luca Bartoli, Federico Martello, Riccardo Porta</li>
<li>for: 本研究旨在提出一个独立开发的自主车辆软件架构，并在赛车赛道上进行了实验验证。</li>
<li>methods: 本研究使用了独立开发的自主车辆软件，包括了适应障碍物、主动超越和速度控制等模组。</li>
<li>results: 本研究在首两场赛事中获得了第二和第三名的成绩，并提供了各模组的实验结果和所学。<details>
<summary>Abstract</summary>
The Indy Autonomous Challenge (IAC) brought together for the first time in history nine autonomous racing teams competing at unprecedented speed and in head-to-head scenario, using independently developed software on open-wheel racecars. This paper presents the complete software architecture used by team TII EuroRacing (TII-ER), covering all the modules needed to avoid static obstacles, perform active overtakes and reach speeds above 75 m/s (270 km/h). In addition to the most common modules related to perception, planning, and control, we discuss the approaches used for vehicle dynamics modelling, simulation, telemetry, and safety. Overall results and the performance of each module are described, as well as the lessons learned during the first two events of the competition on oval tracks, where the team placed respectively second and third.
</details>
<details>
<summary>摘要</summary>
印第安那自主挑战（IAC）是历史上第一次将九支自主赛车队伍集结在一起，以前所未有的速度和头一头方式竞赛，使用独立开发的软件在开放式赛车上。本文介绍了TII EuroRacing（TII-ER）队伍所使用的完整软件架构，涵盖避免静止障碍物、实施活动超越和速度超过75米/秒（270公里/小时）等模块。此外，我们还讨论了车辆动力学模型、模拟、测验和安全方面的方法。文章结尾还提供了每个模块的性能和成绩，以及在oval赛道上的第一两场比赛中所学到的经验。
</details></li>
</ul>
<hr>
<h2 id="Detrimental-Contexts-in-Open-Domain-Question-Answering"><a href="#Detrimental-Contexts-in-Open-Domain-Question-Answering" class="headerlink" title="Detrimental Contexts in Open-Domain Question Answering"></a>Detrimental Contexts in Open-Domain Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18077">http://arxiv.org/abs/2310.18077</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xfactlab/emnlp2023-damaging-retrieval">https://github.com/xfactlab/emnlp2023-damaging-retrieval</a></li>
<li>paper_authors: Philhoon Oh, James Thorne</li>
<li>for: 本研究旨在探讨抓取大量信息对知识搜索模型的性能影响，并分析抓取大量信息对问答模型的影响。</li>
<li>methods: 本研究使用现有的抓取方法，不需要进一步的训练或数据。研究人员通过对抓取的文章进行筛选，以提高问答模型的性能。</li>
<li>results: 研究人员发现，使用抓取大量信息可以提高问答模型的准确率，但是使用整个文章可以导致模型的性能下降。通过筛选抓取的文章，可以提高模型的性能。<details>
<summary>Abstract</summary>
For knowledge intensive NLP tasks, it has been widely accepted that accessing more information is a contributing factor to improvements in the model's end-to-end performance. However, counter-intuitively, too much context can have a negative impact on the model when evaluated on common question answering (QA) datasets. In this paper, we analyze how passages can have a detrimental effect on retrieve-then-read architectures used in question answering. Our empirical evidence indicates that the current read architecture does not fully leverage the retrieved passages and significantly degrades its performance when using the whole passages compared to utilizing subsets of them. Our findings demonstrate that model accuracy can be improved by 10% on two popular QA datasets by filtering out detrimental passages. Additionally, these outcomes are attained by utilizing existing retrieval methods without further training or data. We further highlight the challenges associated with identifying the detrimental passages. First, even with the correct context, the model can make an incorrect prediction, posing a challenge in determining which passages are most influential. Second, evaluation typically considers lexical matching, which is not robust to variations of correct answers. Despite these limitations, our experimental results underscore the pivotal role of identifying and removing these detrimental passages for the context-efficient retrieve-then-read pipeline. Code and data are available at https://github.com/xfactlab/emnlp2023-damaging-retrieval
</details>
<details>
<summary>摘要</summary>
对知识密集的NLP任务，许多研究表明，更多的信息访问可以提高模型的综合性表现。然而，counter-intuitively，过度的背景信息可能会对模型在常见问答（QA）数据集上的性能产生负面影响。在这篇论文中，我们分析了如何段落可以对问答模型产生负面影响。我们的实验证据表明，当前的读取架构不能充分利用检索到的段落，并且将整个段落作为输入时，模型的性能会明显下降。我们的发现表明，可以通过过滤掉负面影响的段落来提高模型的准确率。此外，我们还高亮了确定负面影响的段落的挑战。首先，即使正确的上下文，模型可能会作出错误预测，困难判断哪些段落最有影响。其次，评估通常是基于字符匹配，这并不是对正确答案的变体具有坚定的鲁棒性。尽管如此，我们的实验结果表明，确定和移除负面影响的段落对Context-efficient检索-然后-读取管线是非常重要的。代码和数据可以在https://github.com/xfactlab/emnlp2023-damaging-retrieval中找到。
</details></li>
</ul>
<hr>
<h2 id="Knowledge-Corpus-Error-in-Question-Answering"><a href="#Knowledge-Corpus-Error-in-Question-Answering" class="headerlink" title="Knowledge Corpus Error in Question Answering"></a>Knowledge Corpus Error in Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18076">http://arxiv.org/abs/2310.18076</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xfactlab/emnlp2023-knowledge-corpus-error">https://github.com/xfactlab/emnlp2023-knowledge-corpus-error</a></li>
<li>paper_authors: Yejoon Lee, Philhoon Oh, James Thorne</li>
<li>for: This paper explores the effectiveness of generating context passages from large language models (LLMs) in open-domain question answering (QA), and investigates why generated passages may be more effective than retrieved ones.</li>
<li>methods: The paper introduces the concept of knowledge corpus error, which arises when the knowledge corpus used for retrieval is only a subset of the entire string space, and mitigates this shortcoming by generating passages in a larger space using LLMs. The paper also presents an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically.</li>
<li>results: The results across three QA benchmarks show an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error.Here is the information in Simplified Chinese text, as requested:</li>
<li>for: 这篇论文研究了开放领域问答（QA）中大语言模型（LLMs）生成上下文段的效果，以及为何生成的段更有效。</li>
<li>methods: 论文提出了知识库错误概念，即知识库用于搜索的字符串空间仅占整个字符串空间的子集，可能排除了更有帮助的段。论文使用大语言模型生成更大的字符串空间，以避免这种缺陷。</li>
<li>results: 结果表明，使用生成的段可以提高表现（10% - 13%），这表明知识库错误的存在。<details>
<summary>Abstract</summary>
Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error
</details>
<details>
<summary>摘要</summary>
现有研究在开放领域问答（QA）中已经探索了从大语言模型（LLM）中生成上下文段落，取代传统的检索步骤在QA管道中。然而，不是很好地理解为何生成的段落比检索的更有效。本研究重新定义了传统的QA формулировка，并引入了知识库错误的概念。这种错误发生在用于检索的知识库只是字符串空间中的一个子集，可能排除了更有帮助的段落。LLM可能 mitigate这个缺点，因为它们可以生成段落在更大的空间中。我们设计了一个使用LLM来重新译human-annotated金标段落的实验，以观察知识库错误的实际情况。我们的结果在三个QA benchmark上显示，使用重新译段落时性能提高了10%-13%，这表明了知识库错误的存在。我们的代码可以在https://github.com/xfactlab/emnlp2023-knowledge-corpus-error中找到。
</details></li>
</ul>
<hr>
<h2 id="DUMA-a-Dual-Mind-Conversational-Agent-with-Fast-and-Slow-Thinking"><a href="#DUMA-a-Dual-Mind-Conversational-Agent-with-Fast-and-Slow-Thinking" class="headerlink" title="DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking"></a>DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18075">http://arxiv.org/abs/2310.18075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Tian, Liangyu Chen, Na Liu, Yaxuan Liu, Wei Zou, Kaijiang Chen, Ming Cui</li>
<li>for: 这篇论文的目的是提出一个基于 dual-process theory 的 conversational agent 框架，以提高对问题的回答效率和质量。</li>
<li>methods: 这篇论文使用了两个生成型 Large Language Models (LLMs)，一个用于快速思考，另一个用于慢思考。快速思考模型负责外部互动和初步回答生成，根据问题的复杂程度进行评估是否需要启动慢思考模型。当启动时，慢思考模型会主导对话，进行细心的规划、推理和工具使用，以提供一个详细分析的回答。</li>
<li>results: 实验结果显示，我们的方法可以将效率和质量兼顾，与基准相比有很大的改善。<details>
<summary>Abstract</summary>
Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, and has a significant improvement compared to the baseline.
</details>
<details>
<summary>摘要</summary>
基于人类认知双进程理论，我们介绍DUMA conversational agent框架，该框架通过两个生成型大语言模型（LLM）来实现 быстро和慢思考的双 Mind 机制。快思模型作为外部交互的主要界面，评估问题的复杂性，并根据需要邀请慢思模型参与对话。当邀请时，慢思模型会承担对话，进行细致的规划、理智和工具使用，以提供优化的回答。这种双 Mind 配置允许在不同情况下协调Intuitive 回答和慎重的问题解决过程。我们在房地产领域的在线问题处理中构建了一个 conversational agent，实验证明我们的方法能够平衡效率和效果，与基准相比有显著改进。
</details></li>
</ul>
<hr>
<h2 id="Moral-Responsibility-for-AI-Systems"><a href="#Moral-Responsibility-for-AI-Systems" class="headerlink" title="Moral Responsibility for AI Systems"></a>Moral Responsibility for AI Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18040">http://arxiv.org/abs/2310.18040</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sander Beckers</li>
<li>for: 本文提出了一个定义AI系统的道德责任的方法，以便在AI系统做出具有道德意义的决策时能够定义道德责任。</li>
<li>methods: 本文使用 causal models 框架定义道德责任的两个条件：行动应该导致结果，并且agent应该意识到可能的道德后果。</li>
<li>results: 本文提出了一种度量道德责任的方法，并与现有的BvH和HK方法进行比较。<details>
<summary>Abstract</summary>
As more and more decisions that have a significant ethical dimension are being outsourced to AI systems, it is important to have a definition of moral responsibility that can be applied to AI systems. Moral responsibility for an outcome of an agent who performs some action is commonly taken to involve both a causal condition and an epistemic condition: the action should cause the outcome, and the agent should have been aware -- in some form or other -- of the possible moral consequences of their action. This paper presents a formal definition of both conditions within the framework of causal models. I compare my approach to the existing approaches of Braham and van Hees (BvH) and of Halpern and Kleiman-Weiner (HK). I then generalize my definition into a degree of responsibility.
</details>
<details>
<summary>摘要</summary>
随着更多的具有道德含义的决策被推到人工智能系统中，有必要为AI系统定义道德责任的定义。道德责任的出来由两个条件组成：行为应该导致结果，并且机器人应该知道（在某种形式下）可能的道德后果。这篇文章提出了一个正式的定义方法，并与布拉姆和海斯（BvH）和哈尔普尔和克莱曼-维纳（HK）的现有方法进行比较。然后，我将定义推广到责任度的一级。Here's the translation in Traditional Chinese:随着更多的具有道德含义的决策被推到人工智能系统中，有必要为AI系统定义道德责任的定义。道德责任的出来由两个条件组成：行为应该导致结果，并且机器人应该知道（在某种形式下）可能的道德后果。这篇文章提出了一个正式的定义方法，并与布拉姆和海斯（BvH）和哈尔普尔和克莱曼-维纳（HK）的现有方法进行比较。然后，我将定义推广到责任度的一级。
</details></li>
</ul>
<hr>
<h2 id="Large-language-models-for-aspect-based-sentiment-analysis"><a href="#Large-language-models-for-aspect-based-sentiment-analysis" class="headerlink" title="Large language models for aspect-based sentiment analysis"></a>Large language models for aspect-based sentiment analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18025">http://arxiv.org/abs/2310.18025</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qagentur/absa_llm">https://github.com/qagentur/absa_llm</a></li>
<li>paper_authors: Paul F. Simmering, Paavo Huoviala</li>
<li>for: The paper is written for assessing the performance of GPT-4 and GPT-3.5 in aspect-based sentiment analysis (ABSA) tasks, and exploring the cost-performance trade-offs of different models.</li>
<li>methods: The paper uses zero-shot, few-shot, and fine-tuned settings to evaluate the performance of GPT-4 and GPT-3.5 on the ABSA task, and compares their performance with InstructABSA [@scaria_instructabsa_2023].</li>
<li>results: The fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task, improving upon InstructABSA by 5.7%. However, the fine-tuned model has 1000 times more parameters and thus higher inference cost. The paper also finds that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models.<details>
<summary>Abstract</summary>
Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of prompt engineering versus fine-tuning when using LLMs for ABSA.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="OffMix-3L-A-Novel-Code-Mixed-Dataset-in-Bangla-English-Hindi-for-Offensive-Language-Identification"><a href="#OffMix-3L-A-Novel-Code-Mixed-Dataset-in-Bangla-English-Hindi-for-Offensive-Language-Identification" class="headerlink" title="OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification"></a>OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18387">http://arxiv.org/abs/2310.18387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dhiman Goswami, Md Nishat Raihan, Antara Mahmud, Antonios Anstasopoulos, Marcos Zampieri</li>
<li>for: 本研究是为了开发一个新的三语混合语料库，用于识别攻击性语言。</li>
<li>methods: 本研究使用了多种模型进行实验，包括 transformer 基于模型和 GPT 3.5。</li>
<li>results: 研究发现，BanglishBERT 在这个三语混合语料库中表现出色，超过其他 transformer 基于模型的表现。<details>
<summary>Abstract</summary>
Code-mixing is a well-studied linguistic phenomenon when two or more languages are mixed in text or speech. Several works have been conducted on building datasets and performing downstream NLP tasks on code-mixed data. Although it is not uncommon to observe code-mixing of three or more languages, most available datasets in this domain contain code-mixed data from only two languages. In this paper, we introduce OffMix-3L, a novel offensive language identification dataset containing code-mixed data from three different languages. We experiment with several models on this dataset and observe that BanglishBERT outperforms other transformer-based models and GPT-3.5.
</details>
<details>
<summary>摘要</summary>
<SYS>Code-mixing 是一种已经广泛研究的语言现象，在文本或语音中混合两种或更多种语言。许多研究已经建立了 code-mixed 数据集并在这些数据集上进行了下游 NLP 任务。虽然三种语言混合并不是不常见的，但大多数可用的数据集都只包含了两种语言的 code-mixed 数据。在这篇论文中，我们介绍了 OffMix-3L，一个新的三种语言混合语言识别数据集。我们在这个数据集上试用了一些模型，并发现 BanglishBERT 超过了其他转换器基于模型和 GPT-3.5。</SYS>Here's the translation in Traditional Chinese:<SYS>Code-mixing 是一种已经广泛研究的语言现象，在文本或语音中混合两种或更多种语言。许多研究已经建立了 code-mixed 数据集并在这些数据集上进行了下游 NLP 任务。处于三种语言混合的情况下，大多数可用的数据集都只包含了两种语言的 code-mixed 数据。在这篇论文中，我们介绍了 OffMix-3L，一个新的三种语言混合语言识别数据集。我们在这个数据集上尝试了一些模型，并发现 BanglishBERT 超过了其他对应的 transformer 基于模型和 GPT-3.5。</SYS>
</details></li>
</ul>
<hr>
<h2 id="FormalGeo-The-First-Step-Toward-Human-like-IMO-level-Geometric-Automated-Reasoning"><a href="#FormalGeo-The-First-Step-Toward-Human-like-IMO-level-Geometric-Automated-Reasoning" class="headerlink" title="FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning"></a>FormalGeo: The First Step Toward Human-like IMO-level Geometric Automated Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18021">http://arxiv.org/abs/2310.18021</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaokai Zhang, Na Zhu, Yiming He, Jia Zou, Qike Huang, Xiaoxiao Jin, Yanjun Guo, Chenyang Mao, Zhe Zhu, Dengfeng Yue, Fangzhen Zhu, Yang Li, Yifan Wang, Yiwen Huang, Runan Wang, Cheng Qin, Zhenbing Zeng, Shaorong Xie, Xiangfeng Luo, Tuo Leng</li>
<li>for: 这个论文的目的是构建一个完整的 formally compatible 平面几何系统，以便将AI自动推理与IMO级别的平面几何挑战联系起来。</li>
<li>methods: 该论文使用了geometry formalization theory（GFT）指导建立了FormalGeo系统，包括88个几何 predicate 和 196个定理。它可以处理、验证和解决IMO级别的平面几何问题。此外，他们还实现了一个基于Python的Formal Geometry Problem Solver（FGPS），可以作为人工智能辅助验证问题解决过程，以及自动问题解决器，使用了forward search、backward search 和 AI-assisted search 等方法。</li>
<li>results: 实现了FormalGeo系统和FGPS实验，证明了GFT的正确性和实用性。使用backward depth-first search方法，解决问题失败率仅2.42%，并可以通过深度学习技术来降低这一值。<details>
<summary>Abstract</summary>
This is the first paper in a series of work we have accomplished over the past three years. In this paper, we have constructed a complete and compatible formal plane geometry system. This will serve as a crucial bridge between IMO-level plane geometry challenges and readable AI automated reasoning. With this formal system in place, we have been able to seamlessly integrate modern AI models with our formal system. Within this formal framework, AI is now capable of providing deductive reasoning solutions to IMO-level plane geometry problems, just like handling other natural languages, and these proofs are readable, traceable, and verifiable. We propose the geometry formalization theory (GFT) to guide the development of the geometry formal system. Based on the GFT, we have established the FormalGeo, which consists of 88 geometric predicates and 196 theorems. It can represent, validate, and solve IMO-level geometry problems. we also have crafted the FGPS (formal geometry problem solver) in Python. It serves as both an interactive assistant for verifying problem-solving processes and an automated problem solver, utilizing various methods such as forward search, backward search and AI-assisted search. We've annotated the FormalGeo7k dataset, containing 6,981 (expand to 186,832 through data augmentation) geometry problems with complete formal language annotations. Implementation of the formal system and experiments on the FormalGeo7k validate the correctness and utility of the GFT. The backward depth-first search method only yields a 2.42% problem-solving failure rate, and we can incorporate deep learning techniques to achieve lower one. The source code of FGPS and FormalGeo7k dataset are available at https://github.com/BitSecret/FormalGeo.
</details>
<details>
<summary>摘要</summary>
这是我们过去三年的一系列工作中的第一篇论文。在这篇论文中，我们构建了一个完整、兼容的正式平面几何系统。这将成为在IMO级平面几何挑战和可读的人工智能自动理解之间的关键桥梁。通过这个正式系统，我们可以将现代人工智能模型与我们的正式系统集成了。在这个正式框架下，人工智能现在可以提供平面几何问题的推理解决方案，就像处理其他自然语言一样，并且这些证明是可读、可追溯和可验证的。我们提出了几何ormal化理论（GFT），以引导正式几何系统的开发。基于GFT，我们建立了FormalGeo，它包含88个几何 predicate 和 196个定理。它可以表示、验证和解决IMO级平面几何问题。我们还制作了FGPS（正式几何问题解决器），它是一个在 Python 中实现的交互式助手和自动问题解决器，可以使用多种方法，如前向搜索、后向搜索和人工智能辅助搜索。我们对 FormaleGeo7k 数据集进行了注释，该数据集包含 6,981 个（通过数据扩充到 186,832）平面几何问题的完整正式语言注释。我们对正式系统的实现和 FormaleGeo7k 数据集的实验 validate 了正确性和实用性。使用回溯深度先搜索法只有2.42%的问题解决失败率，并且可以通过深度学习技术来降低这个数字。FGPS 和 FormaleGeo7k 数据集的源代码可以在 GitHub 上找到。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-Enables-Large-Depth-of-Field-Images-for-Sub-Diffraction-Limit-Scanning-Superlens-Microscopy"><a href="#Deep-Learning-Enables-Large-Depth-of-Field-Images-for-Sub-Diffraction-Limit-Scanning-Superlens-Microscopy" class="headerlink" title="Deep Learning Enables Large Depth-of-Field Images for Sub-Diffraction-Limit Scanning Superlens Microscopy"></a>Deep Learning Enables Large Depth-of-Field Images for Sub-Diffraction-Limit Scanning Superlens Microscopy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17997">http://arxiv.org/abs/2310.17997</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sun, Hao Luo, Feifei Wang, Qingjiu Chen, Meng Chen, Xiaoduo Wang, Haibo Yu, Guanglie Zhang, Lianqing Liu, Jianping Wang, Dapeng Wu, Wen Jung Li</li>
<li>for: 这个论文的目的是提高扫描电子镜像（SEM）的分辨率和深度场景图像。</li>
<li>methods: 这个论文使用深度学习来建立扫描超分解（OSR）图像和SEM领域图像之间的映射关系，从而将OSR图像转换成SEM类型的大深度场景图像。</li>
<li>results: 比较PSNR和结构相似度指标值表示，深度学习方法在图像到图像翻译中表现出色，与光学超分解图像相比，PSNR提高约0.74dB。这种方法在检测晶圆缺陷、生物样本分析、审查和其他领域都具有广泛的应用前景。<details>
<summary>Abstract</summary>
Scanning electron microscopy (SEM) is indispensable in diverse applications ranging from microelectronics to food processing because it provides large depth-of-field images with a resolution beyond the optical diffraction limit. However, the technology requires coating conductive films on insulator samples and a vacuum environment. We use deep learning to obtain the mapping relationship between optical super-resolution (OSR) images and SEM domain images, which enables the transformation of OSR images into SEM-like large depth-of-field images. Our custom-built scanning superlens microscopy (SSUM) system, which requires neither coating samples by conductive films nor a vacuum environment, is used to acquire the OSR images with features down to ~80 nm. The peak signal-to-noise ratio (PSNR) and structural similarity index measure values indicate that the deep learning method performs excellently in image-to-image translation, with a PSNR improvement of about 0.74 dB over the optical super-resolution images. The proposed method provides a high level of detail in the reconstructed results, indicating that it has broad applicability to chip-level defect detection, biological sample analysis, forensics, and various other fields.
</details>
<details>
<summary>摘要</summary>
扫描电子顾问（SEM）在多种应用中是不可或缺的，包括微电子到食品加工等，因为它可以提供具有大深度场的图像，超过光学折射限制。然而，技术需要将导电薄膜层应用于隔离样品和真空环境。我们使用深度学习来获得扫描超解像（OSR）图像和SEM领域图像之间的映射关系，这使得OSR图像可以转换为大深度场的SEM样式图像。我们自制的扫描超透镜系统（SSUM）不需要将样品层层涂敷导电薄膜，也不需要真空环境，可以获得OSR图像的特征下限为~80nm。PSNR和结构相似性指数值表明，深度学习方法在图像到图像翻译中表现出色，与扫描超解像图像相比，PSNR提高约0.74dB。我们提出的方法可以在各种领域中提供高级别的细节，包括半导体缺陷检测、生物样本分析、法医和多种其他领域。
</details></li>
</ul>
<hr>
<h2 id="Autonomous-3D-Exploration-in-Large-Scale-Environments-with-Dynamic-Obstacles"><a href="#Autonomous-3D-Exploration-in-Large-Scale-Environments-with-Dynamic-Obstacles" class="headerlink" title="Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles"></a>Autonomous 3D Exploration in Large-Scale Environments with Dynamic Obstacles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17977">http://arxiv.org/abs/2310.17977</a></li>
<li>repo_url: None</li>
<li>paper_authors: Emil Wiman, Ludvig Widén, Mattias Tiger, Fredrik Heintz</li>
<li>for: 本研究旨在开探自动系统在动态和不确定的实际环境中的探索能力，以及如何通过包含动态障碍物在计划中来利用动态环境。</li>
<li>methods: 提议的 Dynamic Autonomous Exploration Planner (DAEP) extend AEP，以便考虑动态障碍物，并在各种动态环境中进行了全面评估。</li>
<li>results: DAEP 在动态和大规模环境中表现出优于当前标准方法，并在探索和碰撞避免方面具有更高的效果。<details>
<summary>Abstract</summary>
Exploration in dynamic and uncertain real-world environments is an open problem in robotics and constitutes a foundational capability of autonomous systems operating in most of the real world. While 3D exploration planning has been extensively studied, the environments are assumed static or only reactive collision avoidance is carried out. We propose a novel approach to not only avoid dynamic obstacles but also include them in the plan itself, to exploit the dynamic environment in the agent's favor. The proposed planner, Dynamic Autonomous Exploration Planner (DAEP), extends AEP to explicitly plan with respect to dynamic obstacles. To thoroughly evaluate exploration planners in such settings we propose a new enhanced benchmark suite with several dynamic environments, including large-scale outdoor environments. DAEP outperform state-of-the-art planners in dynamic and large-scale environments. DAEP is shown to be more effective at both exploration and collision avoidance.
</details>
<details>
<summary>摘要</summary>
文本翻译为简化中文：在真实世界中的动态和不确定环境中进行探索是Robotics中的一个开放问题，也是自主系统在大多数真实世界中的基本能力。而3D探索规划已经得到了广泛的研究，但是环境假设为静止的或者只是进行了反射性碰撞避免。我们提出了一种新的方法，不仅避免动态障碍物，而且将其包含在计划中，以利用动态环境来帮助代理人。我们提出的 Dynamic Autonomous Exploration Planner（DAEP）扩展了AEP，以明确地考虑动态障碍物。为了全面评估探索 плаanner在这些设置下的性能，我们提出了一个新的加强版benchmark suite，包括一些大规模的外部环境。DAEP在动态和大规模环境中表现出色，在探索和碰撞避免方面都更有效。</SYS>Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="Train-Once-Get-a-Family-State-Adaptive-Balances-for-Offline-to-Online-Reinforcement-Learning"><a href="#Train-Once-Get-a-Family-State-Adaptive-Balances-for-Offline-to-Online-Reinforcement-Learning" class="headerlink" title="Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning"></a>Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17966">http://arxiv.org/abs/2310.17966</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leaplabthu/famo2o">https://github.com/leaplabthu/famo2o</a></li>
<li>paper_authors: Shenzhi Wang, Qisen Yang, Jiawei Gao, Matthieu Gaetan Lin, Hao Chen, Liwei Wu, Ning Jia, Shiji Song, Gao Huang</li>
<li>for: 这篇论文是关于Offline-to-online reinforcement learning（RL）训练方法的，旨在解决在线环境中融合预先收集的数据集和精度调整的问题。</li>
<li>methods: 该论文提出了一种简单 yet effective的框架，即Family Offline-to-Online RL（FamO2O），可以让现有算法决定不同状态下的改进&#x2F;约束平衡。FamO2O使用一个通用模型训练一个家族政策，并使用一个平衡模型选择适合每个状态的政策。</li>
<li>results: 在许多实验中，FamO2O具有与现有方法相比的 statistically significant 改进，并达到了D4RLbenchmark上的状态空间最佳性能。代码可以在<a target="_blank" rel="noopener" href="https://github.com/LeapLabTHU/FamO2O%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://github.com/LeapLabTHU/FamO2O中找到。</a><details>
<summary>Abstract</summary>
Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.
</details>
<details>
<summary>摘要</summary>
偏向在线学习（RL）训练 paradigma combines 预训练在预收集的数据集上与在线环境的精细调整。然而，在线调整可能会加剧分布shift问题。现有的解决方案通过在offline和online学习中加入策略约束来解决该问题。这些方法通常提出一个在多种数据集上保持策略改进目标和约束之间的平衡。然而，这种一大把 fits all的方法可能无法最佳利用每个采集的样本，因为不同的状态下的数据质量有很大的差异。为此，我们介绍了Family Offline-to-Online RL（FamO2O）框架，它可以让现有算法在不同的状态下选择适当的策略改进约束。FamO2O使用一个通用模型来训练一个家族策略，每个策略都有不同的改进约束强度。此外，FamO2O还使用一个平衡模型来选择每个状态下最适合的策略。理论上，我们证明了适应性平衡是实现更高策略性能上限的必要条件。empirically，我们进行了大量的实验，并证明了FamO2O可以在D4RL benchmark上达到状态前方的性能。代码可以在https://github.com/LeapLabTHU/FamO2O上获取。
</details></li>
</ul>
<hr>
<h2 id="Qilin-Med-VL-Towards-Chinese-Large-Vision-Language-Model-for-General-Healthcare"><a href="#Qilin-Med-VL-Towards-Chinese-Large-Vision-Language-Model-for-General-Healthcare" class="headerlink" title="Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare"></a>Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17956">http://arxiv.org/abs/2310.17956</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/williamliujl/qilin-med-vl">https://github.com/williamliujl/qilin-med-vl</a></li>
<li>paper_authors: Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, Yining Hua<br>for:This paper aims to address the lack of large language models in languages other than English and the ability to interpret multi-modal input, specifically for global healthcare accessibility.methods:The study introduces Qilin-Med-VL, the first Chinese large vision-language model that combines a pre-trained Vision Transformer (ViT) with a foundational language model. The model undergoes a two-stage curriculum training process that includes feature alignment and instruction tuning.results:The model is able to generate medical captions and answer complex medical queries, and the authors release a dataset called ChiMed-VL, which consists of over 1 million image-text pairs to enable detailed and comprehensive interpretation of medical data using various types of images.<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经引入了新的时代，能够深刻理解复杂的医疗和生物医学话题。然而， существует一定的语言 besides English和可以处理多modal输入的模型缺失，这对全球医疗访问ibilty是关键。为此，本研究介绍了Qilin-Med-VL，首个用于整合文本和视觉数据的中文大vision-语言模型。Qilin-Med-VL结合预训练的视觉转换器（ViT）和基础的LLM。它通过两个阶段课程训练过程，包括特征对齐和指令调整。这种方法使得模型能够生成医学描述和回答复杂的医学问题。我们还发布了ChiMed-VL数据集，包含more than 1M的图像-文本对。这个数据集经过仔细审核，以便使用不同类型的图像进行详细和全面的医学数据解释。
</details></li>
</ul>
<hr>
<h2 id="Understanding-Parameter-Saliency-via-Extreme-Value-Theory"><a href="#Understanding-Parameter-Saliency-via-Extreme-Value-Theory" class="headerlink" title="Understanding Parameter Saliency via Extreme Value Theory"></a>Understanding Parameter Saliency via Extreme Value Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17951">http://arxiv.org/abs/2310.17951</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuo Wang, Issei Sato</li>
<li>For: This paper aims to identify and correct misclassifications in deep neural networks, specifically convolutional neural networks (CNNs), by ranking convolution filters based on their potential to cause misidentification.* Methods: The paper uses parameter saliency ranking, which is based on extreme value theory, to identify the filters that are most likely to cause misclassification. The authors also use fine-tuning to correct misidentification.* Results: The paper shows that the proposed method can detect malicious filters and is less biased against the depth of layers in deep neural networks compared to existing methods. The authors also demonstrate the effectiveness of their approach on ImageNet.<details>
<summary>Abstract</summary>
Deep neural networks are being increasingly implemented throughout society in recent years. It is useful to identify which parameters trigger misclassification in diagnosing undesirable model behaviors. The concept of parameter saliency is proposed and used to diagnose convolutional neural networks (CNNs) by ranking convolution filters that may have caused misclassification on the basis of parameter saliency. It is also shown that fine-tuning the top ranking salient filters has efficiently corrected misidentification on ImageNet. However, there is still a knowledge gap in terms of understanding why parameter saliency ranking can find the filters inducing misidentification. In this work, we attempt to bridge the gap by analyzing parameter saliency ranking from a statistical viewpoint, namely, extreme value theory. We first show that the existing work implicitly assumes that the gradient norm computed for each filter follows a normal distribution. Then, we clarify the relationship between parameter saliency and the score based on the peaks-over-threshold (POT) method, which is often used to model extreme values. Finally, we reformulate parameter saliency in terms of the POT method, where this reformulation is regarded as statistical anomaly detection and does not require the implicit assumptions of the existing parameter-saliency formulation. Our experimental results demonstrate that our reformulation can detect malicious filters as well. Furthermore, we show that the existing parameter saliency method exhibits a bias against the depth of layers in deep neural networks. In particular, this bias has the potential to inhibit the discovery of filters that cause misidentification in situations where domain shift occurs. In contrast, parameter saliency based on POT shows less of this bias.
</details>
<details>
<summary>摘要</summary>
深度神经网络在近年中逐渐普及社会。为了诊断模型的不良行为，identifying模型参数的诱导性是非常有用的。在这些年中，我们提出了参数敏感性的概念，并用于诊断卷积神经网络（CNNs）中的参数敏感性排名。我们还证明了精细调整涉及到诊断错误的顶层敏感filter可以高效地修复ImageNet中的误分类。然而，我们还存在一个知识漏洞，即理解参数敏感排名如何找到导致误分类的filter。在这种情况下，我们尝试通过统计视角来填补这个漏洞，即使用极值理论。我们首先显示了现有工作假设每个滤波器的梯度 нор computes follows a normal distribution。然后，我们解释了参数敏感和分数之间的关系，并使用peaks-over-threshold（POT）方法来模型极值。最后，我们重新定义参数敏感，以统计异常检测的形式，不需要现有参数敏感的假设。我们的实验结果表明，我们的重新定义可以检测到危险的滤波器，并且我们发现现有参数敏感方法具有层深度的偏见，可能在领域转换 occurs 时阻碍发现误分类的滤波器。相比之下，基于POT方法的参数敏感方法具有较少的偏见。
</details></li>
</ul>
<hr>
<h2 id="A-Comprehensive-and-Reliable-Feature-Attribution-Method-Double-sided-Remove-and-Reconstruct-DoRaR"><a href="#A-Comprehensive-and-Reliable-Feature-Attribution-Method-Double-sided-Remove-and-Reconstruct-DoRaR" class="headerlink" title="A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR)"></a>A Comprehensive and Reliable Feature Attribution Method: Double-sided Remove and Reconstruct (DoRaR)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17945">http://arxiv.org/abs/2310.17945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dxq21/dorar">https://github.com/dxq21/dorar</a></li>
<li>paper_authors: Dong Qin, George Amariucai, Daji Qiao, Yong Guan, Shen Fu<br>for: 这种研究旨在解决深度神经网络和其他机器学习模型中的内部决策机制不透明性问题，以提高这些黑盒模型在不同领域的应用。methods: 该研究提出了一种基于多种改进方法的 Double-sided Remove and Reconstruct (DoRaR) 特征归因方法，可以有效地减轻艺术ifacts问题和Encoding Prediction in the Explanation (EPITE)问题，并可以帮助训练一个性能更高的特征选择器。results: 该研究通过对 MNIST、CIFAR10 和自己制作的synthetic数据集进行了广泛的测试，表明 DoRaR 特征归因方法可以有效地解释模型决策，并且可以超越其他现有的特征归因方法。<details>
<summary>Abstract</summary>
The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor's decisions rely not on the features, but on the masks that selects those features. As a result, the credibility of attribution results is undermined by these downsides. In this research, we introduce the Double-sided Remove and Reconstruct (DoRaR) feature attribution method based on several improvement methods that addresses these issues. By conducting thorough testing on MNIST, CIFAR10 and our own synthetic dataset, we demonstrate that the DoRaR feature attribution method can effectively bypass the above issues and can aid in training a feature selector that outperforms other state-of-the-art feature attribution methods. Our code is available at https://github.com/dxq21/DoRaR.
</details>
<details>
<summary>摘要</summary>
深度神经网络（DNN）和其他机器学习（ML）模型的内部决策机制的不充分透明性，限制了它们在一些领域的应用。为了解决这个问题，feature attrition方法被开发出来，以确定DNN和ML模型决策中的关键特征。然而，许多feature attrition方法存在一些缺点。例如，一类feature attrition方法会产生artefacts问题，即将外部样本掩码直接输入到原始训练的分类器中。另一类feature attrition方法使用共同训练的特征选择器和预测器，可以避免artefacts问题，但是它们会产生Encoding Prediction in the Explanation（EPITE）问题，导致预测结果的可信度受到特征选择器的影响。为了解决这些问题，我们在本研究中提出了Double-sided Remove and Reconstruct（DoRaR）特征attrition方法，基于一些改进方法。通过对MNIST、CIFAR10和我们自己的 sintetic dataset进行了广泛的测试，我们证明了DoRaR特征attrition方法可以有效地 circumvent这些问题，并且可以帮助训练一个性能更高的特征选择器。我们的代码可以在https://github.com/dxq21/DoRaR上下载。
</details></li>
</ul>
<hr>
<h2 id="Unified-Segment-to-Segment-Framework-for-Simultaneous-Sequence-Generation"><a href="#Unified-Segment-to-Segment-Framework-for-Simultaneous-Sequence-Generation" class="headerlink" title="Unified Segment-to-Segment Framework for Simultaneous Sequence Generation"></a>Unified Segment-to-Segment Framework for Simultaneous Sequence Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17940">http://arxiv.org/abs/2310.17940</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaolei Zhang, Yang Feng</li>
<li>for:  simultaneous sequence generation for real-time scenarios, such as streaming speech recognition, simultaneous machine translation, and simultaneous speech translation</li>
<li>methods:  segment-to-segment framework (Seg2Seg) with adaptive and unified learning for mapping between source and target sequences</li>
<li>results:  state-of-the-art performance and better generality across various tasks, as demonstrated by experiments on multiple simultaneous generation tasks<details>
<summary>Abstract</summary>
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generating a target segment, making the segment serve as the natural bridge between the source and target. To accomplish this, Seg2Seg introduces a latent segment as the pivot between source to target and explores all potential source-target mappings via the proposed expectation training, thereby learning the optimal moments for generating. Experiments on multiple simultaneous generation tasks demonstrate that Seg2Seg achieves state-of-the-art performance and exhibits better generality across various tasks.
</details>
<details>
<summary>摘要</summary>
同时序列生成是现实时应用场景中的关键任务，如流媒体语音识别、同时机器翻译和同时语音翻译，其目标序列在接收源序列时生成。实现高质量生成的关键在于确定最佳的生成时机，通过学习源和目标序列之间的映射来实现。然而，现有方法通常采用特定任务的规则来控制生成，限制模型在不同序列类型上适应性地学习源-目标映射，阻碍了不同同时任务的多任务学习。本文提出了一个统一的段到段框架（Seg2Seg），用于同时序列生成。在同时生成过程中，模型会在等待源段和生成目标段之间转换，使得段成为源和目标之间自然的桥梁。为了实现这一点，Seg2Seg引入了一个 latent segment，作为源到目标映射的潜在桥梁，并通过提出的预期训练来探索所有可能的源-目标映射，从而学习最佳的生成时机。实验表明，Seg2Seg在多个同时生成任务上具有状态体系最佳性和更好的普适性。
</details></li>
</ul>
<hr>
<h2 id="Transformers-as-Graph-to-Graph-Models"><a href="#Transformers-as-Graph-to-Graph-Models" class="headerlink" title="Transformers as Graph-to-Graph Models"></a>Transformers as Graph-to-Graph Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17936">http://arxiv.org/abs/2310.17936</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/idiap/g2g-transformer">https://github.com/idiap/g2g-transformer</a></li>
<li>paper_authors: James Henderson, Alireza Mohammadshahi, Andrei C. Coman, Lesly Miculicich</li>
<li>for: 本 paper 的目的是使 Transformers 成为图形模型，并将序列视为特殊情况。</li>
<li>methods: 本 paper 使用图 edges 作为注意力权重，并通过 iterative graph refinement 实现非 autoregressive 图预测。</li>
<li>results: 本 paper 的实验结果表明，这种 architecture 可以达到模elling 多种语言结构的最佳性能，并很好地与预training 中学习的含义语言表示结合。<details>
<summary>Abstract</summary>
We argue that Transformers are essentially graph-to-graph models, with sequences just being a special case. Attention weights are functionally equivalent to graph edges. Our Graph-to-Graph Transformer architecture makes this ability explicit, by inputting graph edges into the attention weight computations and predicting graph edges with attention-like functions, thereby integrating explicit graphs into the latent graphs learned by pretrained Transformers. Adding iterative graph refinement provides a joint embedding of input, output, and latent graphs, allowing non-autoregressive graph prediction to optimise the complete graph without any bespoke pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracies for modelling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.
</details>
<details>
<summary>摘要</summary>
我们认为Transformer是基本上是图像图模型，序列只是特殊情况。注意权重函数对应于图像边。我们的图像图Transformer架构使得这种能力变得明确，将图像边输入到注意权重计算中，并使用注意函数预测图像边，因此将显式图 integrate到预训练Transformer中学习的潜在图中。添加迭代图精度提供了共同嵌入输入、输出和潜在图，allowing非自适应图预测可以优化完整的图 без any bespoke pipeline or decoding strategy. empirical results show that this architecture achieves state-of-the-art accuracy for modeling a variety of linguistic structures, integrating very effectively with the latent linguistic representations learned by pretraining.Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Matching-of-Descriptive-Labels-to-Glossary-Descriptions"><a href="#Matching-of-Descriptive-Labels-to-Glossary-Descriptions" class="headerlink" title="Matching of Descriptive Labels to Glossary Descriptions"></a>Matching of Descriptive Labels to Glossary Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18385">http://arxiv.org/abs/2310.18385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Toshihiro Takahashi, Takaaki Tateishi, Michiaki Tatsubori</li>
<li>for: 这个论文主要是为了解决软件工程中的描述性标签匹配问题，即工程师需要将描述性标签（如商业术语、表列名称）与相关的描述文本相匹配。</li>
<li>methods: 该论文提议使用现有的semantic text similarity测量（STS），并通过扩展 Sentence Retrieval和集成上下文化来增强它。 Sentence Retrieval是一种方法，可以为给定的标签返回与之相关的句子，而集成上下文化则是一种方法，可以计算两个上下文集（例如，两个表中的列名称）之间的相似度。</li>
<li>results: 实验结果表明，提议的方法可以帮助下面STS更正确地匹配描述性标签与描述文本。<details>
<summary>Abstract</summary>
Semantic text similarity plays an important role in software engineering tasks in which engineers are requested to clarify the semantics of descriptive labels (e.g., business terms, table column names) that are often consists of too short or too generic words and appears in their IT systems. We formulate this type of problem as a task of matching descriptive labels to glossary descriptions. We then propose a framework to leverage an existing semantic text similarity measurement (STS) and augment it using semantic label enrichment and set-based collective contextualization where the former is a method to retrieve sentences relevant to a given label and the latter is a method to compute similarity between two contexts each of which is derived from a set of texts (e.g., column names in the same table). We performed an experiment on two datasets derived from publicly available data sources. The result indicated that the proposed methods helped the underlying STS correctly match more descriptive labels with the descriptions.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用简化字符串对文本进行相似度计算，可以帮助软件工程师在IT系统中更好地理解描述性标签（如商业术语、表列名称）的 semantics。我们将这种问题定义为映射描述标签到词典描述的任务。我们then proposed a framework to leveragen existing semantic text similarity measurement (STS) and augment it using semantic label enrichment and set-based collective contextualization。在我们的实验中，我们使用了两个公共数据源中的数据，并得到了结果，表明我们的方法可以帮助STS更正确地匹配描述标签与描述。Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Knowing-What-LLMs-DO-NOT-Know-A-Simple-Yet-Effective-Self-Detection-Method"><a href="#Knowing-What-LLMs-DO-NOT-Know-A-Simple-Yet-Effective-Self-Detection-Method" class="headerlink" title="Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method"></a>Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17918">http://arxiv.org/abs/2310.17918</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin</li>
<li>for: 检测LLMs生成的非事实答案，提高LLMs的可靠性。</li>
<li>methods: 提出了一种自我检测方法，通过多种文本表达和模型自身的询问来检测LLMs是否生成非事实答案。</li>
<li>results: 经过实验表明，该方法可以有效地检测LLMs生成的非事实答案，并且可以在最新发布的LLMs中进行应用，如Vicuna、ChatGPT和GPT-4等。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
</details>
<details>
<summary>摘要</summary>
大型自然语言处理模型（LLM）在自然语言处理任务中表现出了很大的潜力。然而，最新的文献表明，LLM occasional generation of nonfactual responses，这限制了LLM的可靠性，使其无法进一步使用。在这篇论文中，我们提出了一种新的自我检测方法，可以检测LLM不熟悉的问题是否会生成非事实答案。 Specifically，我们首先对给定问题的文本表达进行多样化，然后收集相应的答案。然后，我们比较这些生成的答案之间的差异，以确定问题是否会导致模型生成假信息。所有这些步骤都可以通过使用LLM自己的提问，不需要参考任何外部资源。我们对最近发布的LLM，如Vicuna、ChatGPT和GPT-4等进行了广泛的实验，并证明了我们的方法的有效性。
</details></li>
</ul>
<hr>
<h2 id="The-Innovation-to-Occupations-Ontology-Linking-Business-Transformation-Initiatives-to-Occupations-and-Skills"><a href="#The-Innovation-to-Occupations-Ontology-Linking-Business-Transformation-Initiatives-to-Occupations-and-Skills" class="headerlink" title="The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills"></a>The Innovation-to-Occupations Ontology: Linking Business Transformation Initiatives to Occupations and Skills</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17909">http://arxiv.org/abs/2310.17909</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela Elia, Fang Chen, Didar Zowghi, Marian-Andrei Rizoiu</li>
<li>for: 这篇论文是为了描述一种新的 ontology 和一种自动填充方法，以链接企业变革 iniciativas 和职业。</li>
<li>methods: 该论文使用了 online job ads 和 Wikipedia 页面的嵌入式提取出来自动填充 ontology。</li>
<li>results: 该研究成功地匹配了各种企业变革 initiaves 和相应的职业，并提供了一种创新的方法来导引企业和教育机构在具体的企业变革 initiaves 中寻找合适的人才。<details>
<summary>Abstract</summary>
The fast adoption of new technologies forces companies to continuously adapt their operations making it harder to predict workforce requirements. Several recent studies have attempted to predict the emergence of new roles and skills in the labour market from online job ads. This paper aims to present a novel ontology linking business transformation initiatives to occupations and an approach to automatically populating it by leveraging embeddings extracted from job ads and Wikipedia pages on business transformation and emerging technologies topics. To our knowledge, no previous research explicitly links business transformation initiatives, like the adoption of new technologies or the entry into new markets, to the roles needed. Our approach successfully matches occupations to transformation initiatives under ten different scenarios, five linked to technology adoption and five related to business. This framework presents an innovative approach to guide enterprises and educational institutions on the workforce requirements for specific business transformation initiatives.
</details>
<details>
<summary>摘要</summary>
新技术的快速采用使公司需要不断适应操作，使预测工作力需求变得更加困难。一些最近的研究尝试通过在线职位招聘广告预测劳动力市场中的新角色和技能的出现。本文提出了一种新的 ontology，将企业转型活动与职业联系起来，并通过利用来自职位招聘和企业转型和新技术主题的Wikipedia页面中提取的嵌入进行自动填充。根据我们所知，没有任何前期研究直接将企业转型活动，如新技术的采用或新市场的入场，与需要的职业联系起来。我们的方法成功地将职业与转型活动相匹配，并在五种技术采用和五种商业转型的场景下进行了十个不同的enario。这种框架将为企业和教育机构提供一种创新的方法，以指导特定的企业转型活动所需的工作力。
</details></li>
</ul>
<hr>
<h2 id="Pitfalls-in-Language-Models-for-Code-Intelligence-A-Taxonomy-and-Survey"><a href="#Pitfalls-in-Language-Models-for-Code-Intelligence-A-Taxonomy-and-Survey" class="headerlink" title="Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey"></a>Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17903">http://arxiv.org/abs/2310.17903</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yueyuel/reliablelm4code">https://github.com/yueyuel/reliablelm4code</a></li>
<li>paper_authors: Xinyu She, Yue Liu, Yanjie Zhao, Yiling He, Li Li, Chakkrit Tantithamthavorn, Zhan Qin, Haoyu Wang</li>
<li>for: 本研究旨在探讨语言模型 для代码智能（LM4Code）中存在的各种隐患，以提高其可靠性和实用性。</li>
<li>methods: 我们采用了系统性的研究方法，包括文献综述和分类分析，对67篇来自顶尖学术会议的研究进行了仔细审查。</li>
<li>results: 我们发现了LM4Code研究中存在的4大类隐患，即数据采集和标注、系统设计和学习、性能评估和部署维护。这些隐患可能导致LM4Code系统的不可靠性和实用性问题。<details>
<summary>Abstract</summary>
Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, implications, current solutions, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways.
</details>
<details>
<summary>摘要</summary>
（简化中文）现代语言模型（LM）在源代码生成和理解方面取得了成功，导致学习基于代码智能的研究得到了推动，例如自动修复bug和测试用例生成。尽管LM4Code具有巨大的潜力，但它们也面临着一些潜在的难题，这些难题会影响LM4Code在实际应用中的性能和可靠性。这些挑战需要我们进行全面的理解，不仅是识别这些问题，而且还需要探究它们的可能的影响和现有的解决方案，以建立更可靠的LM4Code系统。我们采用了一种系统atic research approach，进行了广泛的文献综述，并最终确定了67篇来自top-tier venues的研究。经过仔细检查这些研究，我们设计了LM4Code研究中的taxonomy难点，并进行了系统的研究，总结了各种难点的问题、影响、当前的解决方案和挑战。我们开发了一种全面的分类方案，将难点分解成四个重要方面：数据收集和标注、系统设计和学习、性能评估和部署维护。通过这项研究，我们希望为研究者和实践者提供一个路线图，以便他们更好地理解和利用LM4Code，以实现可靠和可信worthy的应用。
</details></li>
</ul>
<hr>
<h2 id="Natural-Language-Interfaces-for-Tabular-Data-Querying-and-Visualization-A-Survey"><a href="#Natural-Language-Interfaces-for-Tabular-Data-Querying-and-Visualization-A-Survey" class="headerlink" title="Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey"></a>Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17894">http://arxiv.org/abs/2310.17894</a></li>
<li>repo_url: None</li>
<li>paper_authors: Weixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan, Raymond Chi-Wing Wong, Haiqin Yang</li>
<li>for: 这篇论文旨在提供一个完整的对话语言处理（NLP）技术在表格数据查询和可见化方面的概述，帮助用户通过自然语言查询来交互 WITH 数据。</li>
<li>methods: 本论文介绍了对话语言处理技术的基本概念和方法，尤其是Semantic Parsing，这是将自然语言转换为SQL查询或数据可见化命令的关键技术。</li>
<li>results: 本论文提供了关于文本-SQL和文本-可见化问题的最新进展，包括数据集、方法、指标和系统设计等方面的报告，并强调了大语言模型（LLM）在这些领域的影响和未来发展的可能性。<details>
<summary>Abstract</summary>
The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.
</details>
<details>
<summary>摘要</summary>
“自然语言处理技术的出现已经改变了用户与数据表格之间的交互方式，从传统的查询语言和手动折衣到更直观的自然语言界面。大型语言模型（LLM）如ChatGPT和其继承者的出现已经进一步推动了这个领域，开启了新的自然语言处理技术的avenues。本缩短所提供的缩短简介了自然语言界面 для数据表格查询和可视化，让用户可以使用自然语言查询来与数据进行交互。我们将介绍这些界面的基本概念和技术，尤其是对于自然语言转换为SQL查询或数据可视化指令的问题，我们将对这些问题进行深入探讨。我们还将详细介绍过去几年在文本转SQL和文本转可视化领域中的进展，包括 dataset、方法、指标和系统设计等方面的发展。这包括对大型语言模型（LLM）的影响，包括它们的优点、局限性和未来改进的潜力。我们 hoped that this survey will provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Can-LLMs-Keep-a-Secret-Testing-Privacy-Implications-of-Language-Models-via-Contextual-Integrity-Theory"><a href="#Can-LLMs-Keep-a-Secret-Testing-Privacy-Implications-of-Language-Models-via-Contextual-Integrity-Theory" class="headerlink" title="Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory"></a>Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17884">http://arxiv.org/abs/2310.17884</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/skywalker023/confAIde">https://github.com/skywalker023/confAIde</a></li>
<li>paper_authors: Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi</li>
<li>for:  This paper highlights the privacy risks associated with the use of large language models (LLMs) in AI assistants, specifically the inference-time privacy risks that arise when LLMs are fed different types of information from multiple sources and are expected to reason about what to share in their outputs.</li>
<li>methods:  The paper proposes ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. The authors use this benchmark to evaluate the privacy reasoning abilities of two state-of-the-art LLMs, GPT-4 and ChatGPT.</li>
<li>results:  The authors find that even the most capable models, GPT-4 and ChatGPT, reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when the authors employ privacy-inducing prompts or chain-of-thought reasoning. The paper highlights the immediate need to explore novel inference-time privacy-preserving approaches based on reasoning and theory of mind.<details>
<summary>Abstract</summary>
The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）在人工智能助手（在工作、家庭等）的交互使用中引入了一新的推理时隐私风险：LLMs 从多种来源接受不同类型的信息，并被要求在输出中对此进行分享、目的和对象的决定，在给定的上下文中。在这项工作中，我们吸引关注高度敏感但受到忽略的上下文隐私的概念，并提出了 ConfAIde，一个用于评估指导学习模型的隐私推理能力的benchmark。我们的实验显示，even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively。这种泄露 persist even when we employ privacy-inducing prompts or chain-of-thought reasoning。我们的工作强调了立即需要探索新的推理时隐私保护方法，基于推理和思维模型。
</details></li>
</ul>
<hr>
<h2 id="ASPIRO-Any-shot-Structured-Parsing-error-Induced-ReprOmpting-for-Consistent-Data-to-Text-Generation"><a href="#ASPIRO-Any-shot-Structured-Parsing-error-Induced-ReprOmpting-for-Consistent-Data-to-Text-Generation" class="headerlink" title="ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation"></a>ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17877">http://arxiv.org/abs/2310.17877</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/vejvarm/aspiro">https://github.com/vejvarm/aspiro</a></li>
<li>paper_authors: Martin Vejvar, Yasutaka Fujimoto</li>
<li>for: 用于构成数据数据预设的短模板句子生成，具有零至几据设置。</li>
<li>methods: 使用大型自然语言模型（LLM）直接生成概念不受限制的模板，而不是依赖 LLM 复制示例物件或手动验证&#x2F;创建模板。 使用 LLM 重新提示和自动架构检查来解决生成问题。</li>
<li>results: 与直接 LLM 输出相比，ASPIRO 平均降低 DART dataset 中生成的括号错误率66%。在 Rel2Text dataset 上，使用最佳 5 据 setup（text-davinci-003）， scored BLEU 50.62、METEOR 45.16、BLEURT 0.82、NUBIA 0.87 和 PARENT 0.8962，与最近的 fine-tuned 预训语言模型竞争。<details>
<summary>Abstract</summary>
We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
</details>
<details>
<summary>摘要</summary>
我们介绍ASPIRO方法，用于在零到几极少示例设置下将结构化数据变成简短的模板句子。与前一代方法不同，我们的方法会让大型自然语言模型（LLM）直接生成无关实体的模板，而不是依赖于LLM忠实 копи写给定示例实体，或者手动验证/制定模板。我们利用LLM重新拓展，根据算法解析检查触发，以及由PARENT метрик引起的一致验证，实时rectify模板生成问题。与直接LLM输出相比，ASPIRO方法在DART数据集上的生成架构化描述中的平均解析错误率减少了66%。我们的最佳5枚TEXT-DAVINCI-003设置，在Rel2Text数据集上的BLEU分数为50.62，METEOR分数为45.16，BLEURT分数为0.82，NUBIA分数为0.87，和PARENT分数为0.8962，与最近的微调预训练语言模型竞争得来。
</details></li>
</ul>
<hr>
<h2 id="Ranking-with-Slot-Constraints"><a href="#Ranking-with-Slot-Constraints" class="headerlink" title="Ranking with Slot Constraints"></a>Ranking with Slot Constraints</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17870">http://arxiv.org/abs/2310.17870</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/GarlGuo/ranking_with_slot_constraints">https://github.com/GarlGuo/ranking_with_slot_constraints</a></li>
<li>paper_authors: Wentao Guo, Andrew Wang, Bradon Thymes, Thorsten Joachims</li>
<li>for: ranking with slot constraints, which can be applied to various real-world problems such as college admission and medical trial participant selection.</li>
<li>methods: the proposed algorithm called MatchRank, which aims to maximize the number of filled slots by evaluating candidates in the order of the ranking.</li>
<li>results: MatchRank has a strong approximation guarantee and can provide substantial improvements over a range of synthetic and real-world tasks.Here’s the full summary in Simplified Chinese:</li>
<li>for: ranking with slot constraints, 可以应用到各种实际世界问题，如大学入学和医学试验参与者选择。</li>
<li>methods: 提案的算法叫做MatchRank，它的目标是通过评估候选人来填充槽位。</li>
<li>results: MatchRank具有强的近似保证，并且在多个 sintetic 和实际任务上能提供substantial 的改善。<details>
<summary>Abstract</summary>
We introduce the problem of ranking with slot constraints, which can be used to model a wide range of application problems -- from college admission with limited slots for different majors, to composing a stratified cohort of eligible participants in a medical trial. We show that the conventional Probability Ranking Principle (PRP) can be highly sub-optimal for slot-constrained ranking problems, and we devise a new ranking algorithm, called MatchRank. The goal of MatchRank is to produce rankings that maximize the number of filled slots if candidates are evaluated by a human decision maker in the order of the ranking. In this way, MatchRank generalizes the PRP, and it subsumes the PRP as a special case when there are no slot constraints. Our theoretical analysis shows that MatchRank has a strong approximation guarantee without any independence assumptions between slots or candidates. Furthermore, we show how MatchRank can be implemented efficiently. Beyond the theoretical guarantees, empirical evaluations show that MatchRank can provide substantial improvements over a range of synthetic and real-world tasks.
</details>
<details>
<summary>摘要</summary>
我们介绍了带槽限制的排名问题，这可以用来模型广泛的应用问题---从大学入学限制不同学系的名额，到组织一个适合者的医疗试验参与者实验组。我们表明，传统的概率排名原则（PRP）可以对带槽限制的排名问题高度不理想，而我们提出了一个新的排名算法，called MatchRank。MatchRank的目的是在人工决策者按照排名顺序评估候选者时，生成将满足最多槽位的排名。这样，MatchRank超越了PRP，并将其视为对槽位不存在的特别情况。我们的理论分析显示MatchRank具有强的近似保证，不需要候选者或槽位之间的独立假设。此外，我们显示了如何有效地实现MatchRank。在理论保证之外，实际评估显示MatchRank可以提供广泛的Synthetic和实际任务中的重大改善。
</details></li>
</ul>
<hr>
<h2 id="Reproducibility-in-Multiple-Instance-Learning-A-Case-For-Algorithmic-Unit-Tests"><a href="#Reproducibility-in-Multiple-Instance-Learning-A-Case-For-Algorithmic-Unit-Tests" class="headerlink" title="Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests"></a>Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17867">http://arxiv.org/abs/2310.17867</a></li>
<li>repo_url: None</li>
<li>paper_authors: Edward Raff, James Holt<br>for:多元例子学习（MIL）是一个特殊的分类问题，其中输入包含多个实例，每个实例具有一个标签，标签为正则则表示至少有一个正例在输入中，否则为负例。训练这种问题需要将实例级别的信息与袋级别的标签相关联，并且含有一定的 causal 假设和偏见。MIL问题在医疗、网络安全等领域都有广泛的应用。methods:本文研究了五种深度学习的MIL模型，发现这些模型都不尊重标准MIL假设。它们能够学习反相关的实例，即默认为正例，直到看到负例为止，这不应该是正确的MIL模型的行为。我们认为这些模型的改进版本和其他相关工作也会具有同样的问题。results:我们通过一种提议的”算法单元测试”来证明这些模型的问题。我们创建了一些合成数据集，可以由一个尊重MIL假设的模型解决，而这些数据集明显暴露了这些模型学习的问题。五种评估模型各自失败了一个或多个这些测试。这提供了一种模型独立的方法来识别模型假设的违反，我们希望这将对未来的MIL模型开发和评估具有用处。<details>
<summary>Abstract</summary>
Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a "bag" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to "positive" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and other works derived from these models will share the same issue. In any context in which these models are being used, this creates the potential for learning incorrect models, which creates risk of operational failure. We identify and demonstrate this problem via a proposed "algorithmic unit test", where we create synthetic datasets that can be solved by a MIL respecting model, and which clearly reveal learning that violates MIL assumptions. The five evaluated methods each fail one or more of these tests. This provides a model-agnostic way to identify violations of modeling assumptions, which we hope will be useful for future development and evaluation of MIL models.
</details>
<details>
<summary>摘要</summary>
多例学习（Multiple Instance Learning，MIL）是一个类别问题的子领域，其中包含正例和负例，以及一个“袋”（bag）中的输入，其中正例是指袋中包含正例元素，否则为负例。在这种情况下，训练需要将袋级别标签与实例级别信息相关联，并且含有一定的 causal 假设和不对称性。MIL 问题在医疗（一个有害细胞表示癌症）、计算机安全（一个恶意执行程序使计算机感染）等领域出现。在这种工作中，我们考察了五种最具有影响力的深度MIL模型，并发现其中没有任何一个遵循标准MIL假设。它们可以学习反相关实例，即默认为正例标签，直到看到负例对例，这不应该是正确的MIL模型。我们认为这些模型的改进和其他基于这些模型的工作都会受到同样的问题。在这些模型被使用的任何情况下，这会创造出学习错误的模型，从而导致操作失败的风险。我们通过一种“算法单元测试”来识别和演示这个问题，其中我们创建了一些可以由遵循MIL假设的模型解决的 sintetic 数据集，并显示了这些模型学习的问题。五种评估方法都失败了一个或多个这些测试。这提供了一种模型独立的方式来识别模型假设的违反，我们希望这将对未来的MIL模型发展和评估具有用。
</details></li>
</ul>
<hr>
<h2 id="Function-Space-Bayesian-Pseudocoreset-for-Bayesian-Neural-Networks"><a href="#Function-Space-Bayesian-Pseudocoreset-for-Bayesian-Neural-Networks" class="headerlink" title="Function Space Bayesian Pseudocoreset for Bayesian Neural Networks"></a>Function Space Bayesian Pseudocoreset for Bayesian Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17852">http://arxiv.org/abs/2310.17852</a></li>
<li>repo_url: None</li>
<li>paper_authors: Balhae Kim, Hyungi Lee, Juho Lee</li>
<li>for: 这个论文旨在构建一种 bayesian  Pseudocoreset，用于实现可扩展的 Bayesian 推理。</li>
<li>methods: 该方法在函数空间上构建 variational approximation，并将其与全量数据 posterior 匹配在函数空间上。</li>
<li>results: 该方法可以更好地衡量uncertainty量和Robustness，并且适用于多种模型架构。<details>
<summary>Abstract</summary>
A Bayesian pseudocoreset is a compact synthetic dataset summarizing essential information of a large-scale dataset and thus can be used as a proxy dataset for scalable Bayesian inference. Typically, a Bayesian pseudocoreset is constructed by minimizing a divergence measure between the posterior conditioning on the pseudocoreset and the posterior conditioning on the full dataset. However, evaluating the divergence can be challenging, particularly for the models like deep neural networks having high-dimensional parameters. In this paper, we propose a novel Bayesian pseudocoreset construction method that operates on a function space. Unlike previous methods, which construct and match the coreset and full data posteriors in the space of model parameters (weights), our method constructs variational approximations to the coreset posterior on a function space and matches it to the full data posterior in the function space. By working directly on the function space, our method could bypass several challenges that may arise when working on a weight space, including limited scalability and multi-modality issue. Through various experiments, we demonstrate that the Bayesian pseudocoresets constructed from our method enjoys enhanced uncertainty quantification and better robustness across various model architectures.
</details>
<details>
<summary>摘要</summary>
一个 bayesian pseudocoreset 是一个简化的人工数据集，它捕捉了大规模数据集中的关键信息，因此可以用作可扩展的 bayesian 推理的代理数据集。通常，一个 bayesian pseudocoreset 是通过将 posterior conditioning 中的差异最小化来构建的。然而，对于深度神经网络等高维参数模型来说，评估差异可能具有挑战。在这篇论文中，我们提出了一种新的 bayesian pseudocoreset 构建方法，它在函数空间上进行。不同于之前的方法，我们的方法在模型参数（ weights）空间上构建和匹配 coreset 和全数据 posterior，而不是直接在 weights 空间上做。通过在函数空间上工作，我们的方法可以避免一些在 weights 空间上工作时可能会出现的挑战，包括有限扩展性和多模性问题。通过多个实验，我们示出了 bayesian pseudocoresets 由我们的方法构建的uncertainty quantification和模型 Architecture 的多样性均有所提高。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Animation-Generation-and-Control-on-Rigged-Models-via-Large-Language-Models"><a href="#Real-time-Animation-Generation-and-Control-on-Rigged-Models-via-Large-Language-Models" class="headerlink" title="Real-time Animation Generation and Control on Rigged Models via Large Language Models"></a>Real-time Animation Generation and Control on Rigged Models via Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17838">http://arxiv.org/abs/2310.17838</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Whalefishin/LLM_animation">https://github.com/Whalefishin/LLM_animation</a></li>
<li>paper_authors: Han Huang, Fernanda De La Torre, Cathy Mengying Fang, Andrzej Banburski-Fahey, Judith Amores, Jaron Lanier</li>
<li>for: 这个论文是用于实时动画控制和生成的新方法的介绍，使用自然语言输入控制RIGGED模型。</li>
<li>methods: 论文使用了一个大型语言模型（LLM），将其嵌入在Unity中，以输出结构化文本，并将其解析成多种真实的动画。</li>
<li>results: 论文展示了LLM的潜在性，可以实现动画状态的灵活转换，并通过许多RIGGED模型和动作的质量验证了该方法的稳定性。<details>
<summary>Abstract</summary>
We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustrate LLM's potential to enable flexible state transition between existing animations. We showcase the robustness of our approach through qualitative results on various rigged models and motions.
</details>
<details>
<summary>摘要</summary>
我们介绍了一种新的实时动画控制和生成技术，使用自然语言输入控制着带有骨架的模型。首先，我们将大型语言模型（LLM）引入Unity中，以输出结构化的文本，并将其解析成多种真实和生动的动画。其次，我们展示了LLM的潜在能力，可以实现动画状态的灵活转换 между已有的动画。我们通过对不同的带有骨架和动作的模型和动画进行质量检测，证明了我们的方法的稳定性和可靠性。
</details></li>
</ul>
<hr>
<h2 id="One-Style-is-All-you-Need-to-Generate-a-Video"><a href="#One-Style-is-All-you-Need-to-Generate-a-Video" class="headerlink" title="One Style is All you Need to Generate a Video"></a>One Style is All you Need to Generate a Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17835">http://arxiv.org/abs/2310.17835</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video">https://github.com/sandman002/One-Style-is-All-You-Need-to-Generate-a-Video</a></li>
<li>paper_authors: Sandeep Manandhar, Auguste Genovesio</li>
<li>for: 这个论文旨在提出一种基于风格的条件视频生成模型，以及一种新的时间生成器，它基于一组学习的振荡基。</li>
<li>methods: 该方法使用了一种新的时间生成器，基于学习的振荡基，来学习动作的动态表示，这些表示独立于图像内容，可以在不同的演员之间传递。</li>
<li>results: 论文表明，该方法可以对视频质量进行显著提高，并且可以独立地 manipulate 动作和内容，以及进行时间GAN-反转，从一个内容或身份中恢复和传输视频动作。<details>
<summary>Abstract</summary>
In this paper, we propose a style-based conditional video generative model. We introduce a novel temporal generator based on a set of learned sinusoidal bases. Our method learns dynamic representations of various actions that are independent of image content and can be transferred between different actors. Beyond the significant enhancement of video quality compared to prevalent methods, we demonstrate that the disentangled dynamic and content permit their independent manipulation, as well as temporal GAN-inversion to retrieve and transfer a video motion from one content or identity to another without further preprocessing such as landmark points.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种基于风格的条件视频生成模型。我们引入了一种基于学习的振荡基函数，用于学习不同动作的动态表示。我们的方法可以独立地 manipulate 动作表示，并且可以在不同的演员身上传递。除了与常见方法相比带来显著改善的视频质量之外，我们还示出了独立地执行动作和内容的权限，以及时间GAN-反转来重新处理和传输视频动作。Note: Please note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Ontology-Revision-based-on-Pre-trained-Language-Models"><a href="#Ontology-Revision-based-on-Pre-trained-Language-Models" class="headerlink" title="Ontology Revision based on Pre-trained Language Models"></a>Ontology Revision based on Pre-trained Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18378">http://arxiv.org/abs/2310.18378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qiu Ji, Guilin Qi, Yuxin Ye, Jiaye Li, Site Li, Jianjie Ren, Songtao Lu</li>
<li>for: 本研究旨在提出一种基于预训练模型的 ontology 修订算法，以解决 unsatisfiable 概念的问题。</li>
<li>methods: 本研究使用了 various 的 ontology 修订方法，包括定义 revision 算子和设计排名策略，以及使用 pre-trained 模型来编码 axiom  semantics。</li>
<li>results: 根据实验结果，本研究的算法可以达到了 promising 的性能，而 adapted 修订算法可以大幅提高效率，最多可以Save 96% 的时间。此外，一些 scoring functions 可以帮助修订算法在很多情况下获得更好的结果。<details>
<summary>Abstract</summary>
Ontology revision aims to seamlessly incorporate new information into an existing ontology and plays a crucial role in tasks such as ontology evolution, ontology maintenance, and ontology alignment. Similar to repair single ontologies, resolving logical incoherence in the task of ontology revision is also important and meaningful since incoherence is a main potential factor to cause inconsistency and reasoning with an inconsistent ontology will obtain meaningless answers. To deal with this problem, various ontology revision methods have been proposed to define revision operators and design ranking strategies for axioms in an ontology. However, they rarely consider axiom semantics which provides important information to differentiate axioms. On the other hand, pre-trained models can be utilized to encode axiom semantics, and have been widely applied in many natural language processing tasks and ontology-related ones in recent years. Therefore, in this paper, we define four scoring functions to rank axioms based on a pre-trained model by considering various information from a rebuttal ontology and its corresponding reliable ontology. Based on such a scoring function, we propose an ontology revision algorithm to deal with unsatisfiable concepts at once. If it is hard to resolve all unsatisfiable concepts in a rebuttal ontology together, an adapted revision algorithm is designed to deal with them group by group. We conduct experiments over 19 ontology pairs and compare our algorithms and scoring functions with existing ones. According to the experiments, it shows that our algorithms could achieve promising performance. The adapted revision algorithm could improve the efficiency largely, and at most 96% time could be saved for some ontology pairs. Some of our scoring functions help a revision algorithm obtain better results in many cases, especially for the challenging pairs.
</details>
<details>
<summary>摘要</summary>
ontology revision aims to seamlessly incorporate new information into an existing ontology and plays a crucial role in tasks such as ontology evolution, ontology maintenance, and ontology alignment. similar to repairing single ontologies, resolving logical incoherence in the task of ontology revision is also important and meaningful, as incoherence is a main potential factor that can cause inconsistency, and reasoning with an inconsistent ontology will obtain meaningless answers. to deal with this problem, various ontology revision methods have been proposed to define revision operators and design ranking strategies for axioms in an ontology. however, they rarely consider axiom semantics, which provides important information to differentiate axioms. on the other hand, pre-trained models can be utilized to encode axiom semantics, and have been widely applied in many natural language processing tasks and ontology-related ones in recent years. therefore, in this paper, we define four scoring functions to rank axioms based on a pre-trained model by considering various information from a rebuttal ontology and its corresponding reliable ontology. based on such a scoring function, we propose an ontology revision algorithm to deal with unsatisfiable concepts at once. if it is hard to resolve all unsatisfiable concepts in a rebuttal ontology together, an adapted revision algorithm is designed to deal with them group by group. we conduct experiments over 19 ontology pairs and compare our algorithms and scoring functions with existing ones. according to the experiments, it shows that our algorithms could achieve promising performance. the adapted revision algorithm could improve the efficiency largely, and at most 96% time could be saved for some ontology pairs. some of our scoring functions help a revision algorithm obtain better results in many cases, especially for the challenging pairs.
</details></li>
</ul>
<hr>
<h2 id="Large-scale-Foundation-Models-and-Generative-AI-for-BigData-Neuroscience"><a href="#Large-scale-Foundation-Models-and-Generative-AI-for-BigData-Neuroscience" class="headerlink" title="Large-scale Foundation Models and Generative AI for BigData Neuroscience"></a>Large-scale Foundation Models and Generative AI for BigData Neuroscience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18377">http://arxiv.org/abs/2310.18377</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ran Wang, Zhe Sage Chen</li>
<li>for: 该论文探讨了基础模型和生成人工智能模型在神经科学中的应用，包括自然语言和语音、 semantics 记忆、神经机器 interfaces（BMIs）和数据扩展。</li>
<li>methods: 该论文使用了自动学习（SSL）和传输学习来描述基础模型和生成 AI 模型的应用。</li>
<li>results: 该论文 argued that this paradigm-shift framework will open new avenues for many neuroscience research directions and discuss the accompanying challenges and opportunities.I hope that helps!<details>
<summary>Abstract</summary>
Recent advances in machine learning have made revolutionary breakthroughs in computer games, image and natural language understanding, and scientific discovery. Foundation models and large-scale language models (LLMs) have recently achieved human-like intelligence thanks to BigData. With the help of self-supervised learning (SSL) and transfer learning, these models may potentially reshape the landscapes of neuroscience research and make a significant impact on the future. Here we present a mini-review on recent advances in foundation models and generative AI models as well as their applications in neuroscience, including natural language and speech, semantic memory, brain-machine interfaces (BMIs), and data augmentation. We argue that this paradigm-shift framework will open new avenues for many neuroscience research directions and discuss the accompanying challenges and opportunities.
</details>
<details>
<summary>摘要</summary>
Recent advances in machine learning have led to significant breakthroughs in computer games, image and natural language understanding, and scientific discovery. The development of foundation models and large-scale language models (LLMs) has achieved human-like intelligence, thanks to the power of BigData. With the help of self-supervised learning (SSL) and transfer learning, these models have the potential to reshape the landscapes of neuroscience research and have a profound impact on the future.In this mini-review, we will explore recent advances in foundation models and generative AI models, as well as their applications in neuroscience. We will discuss the use of these models in natural language and speech, semantic memory, brain-machine interfaces (BMIs), and data augmentation. We argue that this paradigm-shift framework will open new avenues for many neuroscience research directions and discuss the accompanying challenges and opportunities.Foundation Models and Generative AI ModelsFoundation models and generative AI models have been instrumental in achieving human-like intelligence in various fields. These models are trained on large datasets and use self-supervised learning techniques to learn the underlying patterns and relationships in the data. Once trained, these models can be fine-tuned for specific tasks, such as natural language processing, image recognition, and speech recognition.Applications in Neuroscience1. Natural Language and Speech: Foundation models and generative AI models have been used to develop advanced natural language processing systems that can understand and generate human-like language. These systems have numerous applications in neuroscience, such as analyzing large amounts of text data to identify patterns and trends, and generating natural language descriptions of complex scientific concepts.2. Semantic Memory: These models can be used to develop advanced memory systems that can store and retrieve large amounts of information. This has numerous applications in neuroscience, such as developing systems that can remember and recall complex scientific concepts and theories.3. Brain-Machine Interfaces (BMIs): Foundation models and generative AI models can be used to develop advanced BMIs that can read and interpret brain signals. This has numerous applications in neuroscience, such as developing systems that can decode brain signals to control prosthetic limbs and other assistive technologies.4. Data Augmentation: These models can be used to generate large amounts of synthetic data that can be used to augment real-world datasets. This has numerous applications in neuroscience, such as developing systems that can generate synthetic brain imaging data to augment real-world datasets and improve the accuracy of brain imaging techniques.Challenges and OpportunitiesWhile foundation models and generative AI models have the potential to revolutionize neuroscience research, there are several challenges and opportunities that must be addressed. Some of the challenges include:1. Data Quality: The quality of the data used to train these models is crucial. Poor-quality data can lead to biased or inaccurate models.2. Explainability: It is often difficult to understand how these models make decisions, which can be a problem in fields such as neuroscience where transparency and explainability are essential.3. Ethics: The use of these models raises ethical concerns, such as the potential for bias and the impact on employment.4. Training Time: Training these models can be time-consuming and computationally intensive.Despite these challenges, the opportunities presented by foundation models and generative AI models are significant. With the right training data and the appropriate fine-tuning, these models have the potential to revolutionize neuroscience research and lead to significant advances in our understanding of the brain and nervous system.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/27/cs.AI_2023_10_27/" data-id="clogxf3l100675xra8crp5jg8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/27/cs.CL_2023_10_27/" class="article-date">
  <time datetime="2023-10-27T11:00:00.000Z" itemprop="datePublished">2023-10-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/27/cs.CL_2023_10_27/">cs.CL - 2023-10-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Evaluating-Cross-Domain-Text-to-SQL-Models-and-Benchmarks"><a href="#Evaluating-Cross-Domain-Text-to-SQL-Models-and-Benchmarks" class="headerlink" title="Evaluating Cross-Domain Text-to-SQL Models and Benchmarks"></a>Evaluating Cross-Domain Text-to-SQL Models and Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18538">http://arxiv.org/abs/2310.18538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammadreza Pourreza, Davood Rafiei</li>
<li>for: 本研究目的是evaluating the performance of text-to-SQL models on several prominent cross-domain benchmarks, and re-evaluating top-performing models to assess their true performance.</li>
<li>methods: 该研究使用了 manual evaluation and equivalent expression rewriting to evaluate the SQL queries and models.</li>
<li>results: 研究发现，due to the multiple interpretations of the provided samples, attaining a perfect performance on these benchmarks is unfeasible.  additionally, the true performance of the models was underestimated, and their relative performance changed after re-evaluation. Most notably, a recent GPT4-based model surpassed the gold standard reference queries in the Spider benchmark in human evaluation, highlighting the importance of interpreting benchmark evaluations cautiously.<details>
<summary>Abstract</summary>
Text-to-SQL benchmarks play a crucial role in evaluating the progress made in the field and the ranking of different models. However, accurately matching a model-generated SQL query to a reference SQL query in a benchmark fails for various reasons, such as underspecified natural language queries, inherent assumptions in both model-generated and reference queries, and the non-deterministic nature of SQL output under certain conditions. In this paper, we conduct an extensive study of several prominent cross-domain text-to-SQL benchmarks and re-evaluate some of the top-performing models within these benchmarks, by both manually evaluating the SQL queries and rewriting them in equivalent expressions. Our evaluation reveals that attaining a perfect performance on these benchmarks is unfeasible due to the multiple interpretations that can be derived from the provided samples. Furthermore, we find that the true performance of the models is underestimated and their relative performance changes after a re-evaluation. Most notably, our evaluation reveals a surprising discovery: a recent GPT4-based model surpasses the gold standard reference queries in the Spider benchmark in our human evaluation. This finding highlights the importance of interpreting benchmark evaluations cautiously, while also acknowledging the critical role of additional independent evaluations in driving advancements in the field.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="On-the-Automatic-Generation-and-Simplification-of-Children’s-Stories"><a href="#On-the-Automatic-Generation-and-Simplification-of-Children’s-Stories" class="headerlink" title="On the Automatic Generation and Simplification of Children’s Stories"></a>On the Automatic Generation and Simplification of Children’s Stories</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18502">http://arxiv.org/abs/2310.18502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maria Valentini, Jennifer Weber, Jesus Salcido, Téa Wright, Eliana Colunga, Katharina Kann</li>
<li>for: 这个研究的目的是开发一个自动生成儿童教育材料的系统，以提高儿童的学习效果。</li>
<li>methods: 研究者使用了一些流行的大语言模型（LLMs）来生成儿童教育材料，并评估了这些模型的 lexical 和 readability 水平是否适合儿童。</li>
<li>results: 研究者发现，虽然 LLMs 的能力在不断提高，但它们目前还没有能力限制自己的词汇水平适合更年轻的儿童。在第二个实验中，研究者explored the ability of state-of-the-art lexical simplification models to generalize to the domain of children’s stories, and created an efficient pipeline for their automatic generation.<details>
<summary>Abstract</summary>
With recent advances in large language models (LLMs), the concept of automatically generating children's educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children's stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset.
</details>
<details>
<summary>摘要</summary>
As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children's stories. We develop a dataset of child-directed lexical simplification instances, using examples from the LLM-generated stories in our first experiment. We find that while the strongest-performing current lexical simplification models do not perform as well on material designed for children, some models that achieve strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning.我们使用最新的大语言模型（LLMs），目标是自动生成儿童教育材料。为了实现适合不同年龄层的简化，我们首先评估各种流行的LLMs是否能够自动生成适合不同年龄层的故事。我们发现，虽然LLMs在过去几年内做出了很大的进步，但它们还没有拥有适合儿童年龄层的词汇量。作为第二个实验，我们研究了当前最佳的 lexical simplification 模型是否能够在儿童故事领域得到普遍化。我们开发了一个儿童指向的 lexical simplification 示例集，其中的例子来自我们的第一个实验中的 LLM-生成的故事。我们发现，当前最强的 lexical simplification 模型在面向儿童的数据上表现不如其他数据上，这是因为它们在后台使用大语言模型。但是，一些在普遍数据上表现良好的模型可以通过我们的特制的儿童指向的简化示例集进行细化，从而实现良好的表现。
</details></li>
</ul>
<hr>
<h2 id="Publicly-Detectable-Watermarking-for-Language-Models"><a href="#Publicly-Detectable-Watermarking-for-Language-Models" class="headerlink" title="Publicly Detectable Watermarking for Language Models"></a>Publicly Detectable Watermarking for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18491">http://arxiv.org/abs/2310.18491</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang</li>
<li>for: 本研究旨在构建可证明的语音模型水印方案，以便在公共可读性或可靠性下进行证明。</li>
<li>methods: 本研究使用私钥进行水印，并使用公钥进行水印检测。我们的协议是首个不嵌入生成文本中的统计信号的语音模型水印方案，而是直接使用拒绝抽样来嵌入公共可靠性的 криптографиic signature。我们证明了我们的建构符合强式形式安全保证和私钥水印中的多个欢迎性特性。</li>
<li>results: 我们的水印方案在7B参数范围内进行实验，并证明了我们的正式声明。我们的实验结果表明，我们的水印方案可以保持文本质量，同时符合正式要求。<details>
<summary>Abstract</summary>
We construct the first provable watermarking scheme for language models with public detectability or verifiability: we use a private key for watermarking and a public key for watermark detection. Our protocol is the first watermarking scheme that does not embed a statistical signal in generated text. Rather, we directly embed a publicly-verifiable cryptographic signature using a form of rejection sampling. We show that our construction meets strong formal security guarantees and preserves many desirable properties found in schemes in the private-key watermarking setting. In particular, our watermarking scheme retains distortion-freeness and model agnosticity. We implement our scheme and make empirical measurements over open models in the 7B parameter range. Our experiments suggest that our watermarking scheme meets our formal claims while preserving text quality.
</details>
<details>
<summary>摘要</summary>
我们构建了首个可证明的文本标记 schemes for 语言模型，其中使用私钥进行标记并使用公钥进行标记检测。我们的协议是首个不在生成的文本中嵌入统计信号的 watermarking  schemes，而是直接使用拒绝抽象来嵌入公共可验证的 криптографиic 签名。我们证明了我们的构建符合强制ormal security guarantees 和 preserve many desirable properties found in private-key watermarking setting。特别是，我们的文本标记 schemes  preserved distortion-freeness 和 model agnosticity。我们实现了我们的协议并对 open models 在 7B 参数范围进行了实验。我们的实验表明，我们的文本标记 schemes 符合我们的ormal claims 而 preserve text quality。
</details></li>
</ul>
<hr>
<h2 id="PeTailor-Improving-Large-Language-Model-by-Tailored-Chunk-Scorer-in-Biomedical-Triple-Extraction"><a href="#PeTailor-Improving-Large-Language-Model-by-Tailored-Chunk-Scorer-in-Biomedical-Triple-Extraction" class="headerlink" title="PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction"></a>PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18463">http://arxiv.org/abs/2310.18463</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingchen Li, M. Chen, Huixue Zhou, Rui Zhang</li>
<li>for: 本研究旨在提高自动抽取生物医学实体和其互动的能力，因为现有的专家标注标准数据集的有限性。</li>
<li>methods: 本研究提出了一种基于检索的语言框架（PETAI-LOR），该框架通过修改chunk scorer来适应语言模型（LM）的特定需求。此外，我们还介绍了一个专家标注的生物医学 triple 抽取数据集（GM-CIHT），该数据集涵盖非药治疗和通用生物医学领域。</li>
<li>results: 我们的实验表明，PETAI-LOR在GM-CIHT上实现了状态 искусственный智能的表现。<details>
<summary>Abstract</summary>
The automatic extraction of biomedical entities and their interaction from unstructured data remains a challenging task due to the limited availability of expert-labeled standard datasets. In this paper, we introduce PETAI-LOR, a retrieval-based language framework that is augmented by tailored chunk scorer. Unlike previous retrieval-augmented language models (LM) that retrieve relevant documents by calculating the similarity between the input sentence and the candidate document set, PETAILOR segments the sentence into chunks and retrieves the relevant chunk from our pre-computed chunk-based relational key-value memory. Moreover, in order to comprehend the specific requirements of the LM, PETAI-LOR adapt the tailored chunk scorer to the LM. We also introduce GM-CIHT, an expert annotated biomedical triple extraction dataset with more relation types. This dataset is centered on the non-drug treatment and general biomedical domain. Additionally, we investigate the efficacy of triple extraction models trained on general domains when applied to the biomedical domain. Our experiments reveal that PETAI-LOR achieves state-of-the-art performance on GM-CIHT
</details>
<details>
<summary>摘要</summary>
自动提取生物医学实体和其交互从未结构化数据中 Remains 是一个挑战性的任务，因为专家标注标准数据集的可用性有限。在本文中，我们介绍 PETAI-LOR，一种基于检索的语言框架，该框架通过专门设计的块分词器进行增强。与过去的检索增强语言模型（LM）不同，PETAI-LOR 不是通过计算输入句子和候选文档集之间的相似性来 Retrieval 相关文档，而是通过将句子分成块，然后从我们预计算出的块基于关键值对存储中提取相关块。此外，为了适应特定的LM要求，PETAI-LOR 可以根据LM进行适应tailored块评分器。我们还介绍了GM-CIHT，一个专家标注的生物医学三元EXTRACT数据集，该数据集涵盖非药治疗和普通生物医学领域。此外，我们还调查了将生物医学领域应用于通用领域提取模型的可行性。我们的实验表明，PETAI-LOR 在GM-CIHT上实现了状态机器人的表现。
</details></li>
</ul>
<hr>
<h2 id="Do-Not-Harm-Protected-Groups-in-Debiasing-Language-Representation-Models"><a href="#Do-Not-Harm-Protected-Groups-in-Debiasing-Language-Representation-Models" class="headerlink" title="Do Not Harm Protected Groups in Debiasing Language Representation Models"></a>Do Not Harm Protected Groups in Debiasing Language Representation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18458">http://arxiv.org/abs/2310.18458</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chloe Qinyu Zhu, Rickard Stureborg, Brandon Fain</li>
<li>for: 这篇论文旨在探讨语言模型中的偏见和不公平对待，以及如何透过干预技术解除偏见。</li>
<li>methods: 本论文使用了四种干预技术，包括word embeddings、 adversarial training、 debiasing word embeddings和 adversarial debiasing。</li>
<li>results: 研究发现，干预技术可以减少偏见，但是这些技术可能会对保护的群体造成不良影响，包括性别、种族和年龄等。<details>
<summary>Abstract</summary>
Language Representation Models (LRMs) trained with real-world data may capture and exacerbate undesired bias and cause unfair treatment of people in various demographic groups. Several techniques have been investigated for applying interventions to LRMs to remove bias in benchmark evaluations on, for example, word embeddings. However, the negative side effects of debiasing interventions are usually not revealed in the downstream tasks. We propose xGAP-DEBIAS, a set of evaluations on assessing the fairness of debiasing. In this work, We examine four debiasing techniques on a real-world text classification task and show that reducing biasing is at the cost of degrading performance for all demographic groups, including those the debiasing techniques aim to protect. We advocate that a debiasing technique should have good downstream performance with the constraint of ensuring no harm to the protected group.
</details>
<details>
<summary>摘要</summary>
语言表示模型（LRM）通过实际数据训练可能捕捉和增强不良偏见，导致各种人口组群体受到不公正待遇。一些技术已经研究了对LRMs进行修正以去除偏见，但这些修正的负面影响通常不会在下游任务中表现出来。我们提出xGAP-DEBIAS，一种评估去偏见的评价方法。在这种工作中，我们研究了一个真实世界文本分类任务中四种去偏见技术，并显示了减少偏见的代价是对所有人口组群体，包括被保护的群体，进行性能下降。我们强调，一种去偏见技术应该在保证不会对保护的群体造成伤害的前提下保持良好的下游性能。
</details></li>
</ul>
<hr>
<h2 id="T5-meets-Tybalt-Author-Attribution-in-Early-Modern-English-Drama-Using-Large-Language-Models"><a href="#T5-meets-Tybalt-Author-Attribution-in-Early-Modern-English-Drama-Using-Large-Language-Models" class="headerlink" title="T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models"></a>T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18454">http://arxiv.org/abs/2310.18454</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rebecca M. M. Hicke, David Mimno</li>
<li>for: 这个论文探讨了大语言模型在文学领域中的应用，具体来说是用于早期现代英语戏剧作者识别。</li>
<li>methods: 这个论文使用了一个精度调整后的t5-large模型，并对几种基线模型进行比较，包括逻辑回归、支持向量机和归一化delta。</li>
<li>results: 研究发现，这个精度调整后的t5-large模型在小段文本识别作者方面表现出色，并且超过了所有测试基线模型。然而，研究还发现了一些作者在模型的预训练数据中的存在对预测结果产生了困难评估的影响。<details>
<summary>Abstract</summary>
Large language models have shown breakthrough potential in many NLP domains. Here we consider their use for stylometry, specifically authorship identification in Early Modern English drama. We find both promising and concerning results; LLMs are able to accurately predict the author of surprisingly short passages but are also prone to confidently misattribute texts to specific authors. A fine-tuned t5-large model outperforms all tested baselines, including logistic regression, SVM with a linear kernel, and cosine delta, at attributing small passages. However, we see indications that the presence of certain authors in the model's pre-training data affects predictive results in ways that are difficult to assess.
</details>
<details>
<summary>摘要</summary>
大型语言模型在许多自然语言处理领域中显示出了突破性潜力。我们在这里考虑使用这些模型来进行类型推断，具体而言是在 Early Modern English drama 中进行作者识别。我们发现了一些有希望的结果，以及一些担心的结果：大型语言模型能够对短段文本准确地预测作者，但也容易将文本错误归属给特定的作者。我们发现了一些证据表明模型的预设数据中的作者存在影响预测结果的方式，但这些影响难以评估。
</details></li>
</ul>
<hr>
<h2 id="Modeling-Legal-Reasoning-LM-Annotation-at-the-Edge-of-Human-Agreement"><a href="#Modeling-Legal-Reasoning-LM-Annotation-at-the-Edge-of-Human-Agreement" class="headerlink" title="Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement"></a>Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18440">http://arxiv.org/abs/2310.18440</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rosamond Thalken, Edward H. Stiglitz, David Mimno, Matthew Wilkens</li>
<li>for: 法律逻辑分析的 классификация，即法律哲学分析。</li>
<li>methods: 使用生成语言模型（LMs）进行文档分类任务，并对不同的LM模型进行系统性测试。</li>
<li>results: 发现生成模型在不受 instrucion（i.e. 提示）的情况下表现不佳，但是在对标注过的数据集进行微调后，得到了最佳的结果，并通过应用这些预测来研究历史时期的法律哲学趋势，这与知名的历史质量论相一致，同时还指出了一些可能需要进一步修正的方面。<details>
<summary>Abstract</summary>
Generative language models (LMs) are increasingly used for document class-prediction tasks and promise enormous improvements in cost and efficiency. Existing research often examines simple classification tasks, but the capability of LMs to classify on complex or specialized tasks is less well understood. We consider a highly complex task that is challenging even for humans: the classification of legal reasoning according to jurisprudential philosophy. Using a novel dataset of historical United States Supreme Court opinions annotated by a team of domain experts, we systematically test the performance of a variety of LMs. We find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook. Our strongest results derive from fine-tuning models on the annotated dataset; the best performing model is an in-domain model, LEGAL-BERT. We apply predictions from this fine-tuned model to study historical trends in jurisprudence, an exercise that both aligns with prominent qualitative historical accounts and points to areas of possible refinement in those accounts. Our findings generally sound a note of caution in the use of generative LMs on complex tasks without fine-tuning and point to the continued relevance of human annotation-intensive classification methods.
</details>
<details>
<summary>摘要</summary>
现代生成语言模型（LMs）在文档分类任务中越来越受到广泛使用，承诺可以大幅提高成本和效率。现有研究通常研究简单的分类任务，但生成模型在复杂或专业化任务上的能力更少被了解。我们考虑了一个非常复杂的任务：用法律哲学来分类法律理解。使用一个新的历史美国最高法院判决 opacity 的注释者队伍编制的数据集，我们系统地测试了多种LMs的性能。我们发现，当给生成模型提供相同的指令（i.e. 提示）时，生成模型表现很差。我们最好的结果来自于在注释过的数据集上练习模型，最佳表现的模型是适应于法律领域的 LEGAL-BERT。我们使用这个练习后的模型进行历史趋势的研究，这与著名的qualitative历史质量相符，并指出了可能的改进点。我们的发现通常表达了对生成LMs在复杂任务中无需练习的使用存在警告，并指出了人工注释Intensive分类方法的持续 relevance。
</details></li>
</ul>
<hr>
<h2 id="Expanding-the-Set-of-Pragmatic-Considerations-in-Conversational-AI"><a href="#Expanding-the-Set-of-Pragmatic-Considerations-in-Conversational-AI" class="headerlink" title="Expanding the Set of Pragmatic Considerations in Conversational AI"></a>Expanding the Set of Pragmatic Considerations in Conversational AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18435">http://arxiv.org/abs/2310.18435</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. M. Seals, Valerie L. Shalin</li>
<li>for: 这篇论文主要是为了探讨当前对话AI系统的表现有多好，却未能满足用户的需求。</li>
<li>methods: 论文提出了一些实用上的限制，并通过示例表明了这些限制的缺陷。</li>
<li>results: 论文提出了一种类型化的对话AI系统的设计和评估方法，以解决现有系统的实用上的缺陷。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Despite considerable performance improvements, current conversational AI systems often fail to meet user expectations. We discuss several pragmatic limitations of current conversational AI systems. We illustrate pragmatic limitations with examples that are syntactically appropriate, but have clear pragmatic deficiencies. We label our complaints as "Turing Test Triggers" (TTTs) as they indicate where current conversational AI systems fall short compared to human behavior. We develop a taxonomy of pragmatic considerations intended to identify what pragmatic competencies a conversational AI system requires and discuss implications for the design and evaluation of conversational AI systems.
</details>
<details>
<summary>摘要</summary>
尽管现有的对话AI系统已经做出了很大的表现改进，但它们仍然不能满足用户的期望。我们讨论了现有对话AI系统的各种各样的限制。我们使用合适的语法示例来 illustrate these limitations, but these examples have clear pragmatic deficiencies. 我们称这些问题为“图灵测试触发器”（TTTs），因为它们表明现有的对话AI系统与人类行为相比存在着缺陷。我们开发了对话AI系统的pragma考虑的分类，以确定这些系统所需的pragma能力，并讨论了这些分类的影响对对话AI系统的设计和评估。
</details></li>
</ul>
<hr>
<h2 id="SDOH-NLI-a-Dataset-for-Inferring-Social-Determinants-of-Health-from-Clinical-Notes"><a href="#SDOH-NLI-a-Dataset-for-Inferring-Social-Determinants-of-Health-from-Clinical-Notes" class="headerlink" title="SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes"></a>SDOH-NLI: a Dataset for Inferring Social Determinants of Health from Clinical Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18431">http://arxiv.org/abs/2310.18431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adam D. Lelkes, Eric Loreaux, Tal Schuster, Ming-Jun Chen, Alvin Rajkomar</li>
<li>for: This paper aims to provide a new dataset for natural language inference (NLI) tasks to extract social and behavioral determinants of health (SDOH) from clinical notes.</li>
<li>methods: The paper uses a dataset of publicly available clinical notes and formulates SDOH extraction as an NLI task, with binary textual entailment labels obtained from human raters.</li>
<li>results: The authors evaluate both “off-the-shelf” entailment models and models fine-tuned on their data, and find that their dataset appears more challenging than commonly used NLI datasets.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文的目的是提供一个新的自然语言推理（NLI）任务，以提取医疗记录中的社会和行为Determinants of health（SDOH）。</li>
<li>methods: 论文使用了公共可用的医疗记录数据集，将SDOH抽取作为NLI任务，并使用人工评分者提供的二分文本推理标签。</li>
<li>results: 作者评估了一些“卖在架”的推理模型以及特定于其数据集的模型，并发现其数据集与常用的NLI数据集相比更加具有挑战性。<details>
<summary>Abstract</summary>
Social and behavioral determinants of health (SDOH) play a significant role in shaping health outcomes, and extracting these determinants from clinical notes is a first step to help healthcare providers systematically identify opportunities to provide appropriate care and address disparities. Progress on using NLP methods for this task has been hindered by the lack of high-quality publicly available labeled data, largely due to the privacy and regulatory constraints on the use of real patients' information. This paper introduces a new dataset, SDOH-NLI, that is based on publicly available notes and which we release publicly. We formulate SDOH extraction as a natural language inference (NLI) task, and provide binary textual entailment labels obtained from human raters for a cross product of a set of social history snippets as premises and SDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in that our premises and hypotheses are obtained independently. We evaluate both "off-the-shelf" entailment models as well as models fine-tuned on our data, and highlight the ways in which our dataset appears more challenging than commonly used NLI datasets.
</details>
<details>
<summary>摘要</summary>
社会和行为determinants of health (SDOH) play a significant role in shaping health outcomes, and extracting these determinants from clinical notes is a first step to help healthcare providers systematically identify opportunities to provide appropriate care and address disparities. Progress on using NLP methods for this task has been hindered by the lack of high-quality publicly available labeled data, largely due to the privacy and regulatory constraints on the use of real patients' information. This paper introduces a new dataset, SDOH-NLI, that is based on publicly available notes and which we release publicly. We formulate SDOH extraction as a natural language inference (NLI) task, and provide binary textual entailment labels obtained from human raters for a cross product of a set of social history snippets as premises and SDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in that our premises and hypotheses are obtained independently. We evaluate both "off-the-shelf" entailment models as well as models fine-tuned on our data, and highlight the ways in which our dataset appears more challenging than commonly used NLI datasets.Here's the translation in Traditional Chinese:社会和行为determinants of health (SDOH) play a significant role in shaping health outcomes, and extracting these determinants from clinical notes is a first step to help healthcare providers systematically identify opportunities to provide appropriate care and address disparities. Progress on using NLP methods for this task has been hindered by the lack of high-quality publicly available labeled data, largely due to the privacy and regulatory constraints on the use of real patients' information. This paper introduces a new dataset, SDOH-NLI, that is based on publicly available notes and which we release publicly. We formulate SDOH extraction as a natural language inference (NLI) task, and provide binary textual entailment labels obtained from human raters for a cross product of a set of social history snippets as premises and SDOH factors as hypotheses. Our dataset differs from standard NLI benchmarks in that our premises and hypotheses are obtained independently. We evaluate both "off-the-shelf" entailment models as well as models fine-tuned on our data, and highlight the ways in which our dataset appears more challenging than commonly used NLI datasets.
</details></li>
</ul>
<hr>
<h2 id="Teacher-Perception-of-Automatically-Extracted-Grammar-Concepts-for-L2-Language-Learning"><a href="#Teacher-Perception-of-Automatically-Extracted-Grammar-Concepts-for-L2-Language-Learning" class="headerlink" title="Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning"></a>Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18417">http://arxiv.org/abs/2310.18417</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditi Chaudhary, Arun Sampath, Ashwin Sheshadri, Antonios Anastasopoulos, Graham Neubig</li>
<li>for: 这个研究的目的是为了帮助创建语言教学课程，尤其是 для那些没有充分的资源和专业知识的教师。</li>
<li>methods: 这篇论文使用自然语言文库来自动发现和可见化语法描述。它使用文本来回答 morphosyntax 和 semantics 问题，以帮助教师更好地教授印度语言 kannada 和 marathi。</li>
<li>results: 这篇论文的结果表明，使用自然语言文库来自动发现和可见化语法描述可以帮助教师更好地创建语言教学课程，并且这些材料被教育专业人员评估为有用。<details>
<summary>Abstract</summary>
One of the challenges in language teaching is how best to organize rules regarding syntax, semantics, or phonology in a meaningful manner. This not only requires content creators to have pedagogical skills, but also have that language's deep understanding. While comprehensive materials to develop such curricula are available in English and some broadly spoken languages, for many other languages, teachers need to manually create them in response to their students' needs. This is challenging because i) it requires that such experts be accessible and have the necessary resources, and ii) describing all the intricacies of a language is time-consuming and prone to omission. In this work, we aim to facilitate this process by automatically discovering and visualizing grammar descriptions. We extract descriptions from a natural text corpus that answer questions about morphosyntax (learning of word order, agreement, case marking, or word formation) and semantics (learning of vocabulary). We apply this method for teaching two Indian languages, Kannada and Marathi, which, unlike English, do not have well-developed resources for second language learning. To assess the perceived utility of the extracted material, we enlist the help of language educators from schools in North America to perform a manual evaluation, who find the materials have potential to be used for their lesson preparation and learner evaluation.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:一个挑战在语言教学中是如何有效地组织语法、 semantics 或音律规则的方式。这不仅需要内容创作人具备教学技能，还需要对这种语言有深刻的理解。而且，为了开发这些课程资料，英语和一些广泛使用的语言有相关的资源，但对于其他语言，教师需要手动创建响应学生需求的资料。这是因为i) 需要访问这些专家和有必要的资源，ii) 描述语言的细节是时间consuming 和易于缺少。在这个工作中，我们希望通过自动发现和视觉化语法描述来促进这个过程。我们从自然文本 corpus 中提取描述，回答有关 morphosyntax （学习word order、一致、格emarking 或 word formation）和 semantics （学习词汇）的问题。我们对几种印度语言，如 kannada 和 Marathi 进行应用，这些语言与英语不同，没有很好的第二语言学习资源。为了评估提取的材料的实际用途，我们征得北美语言教育专业人士的帮助进行手动评估，他们认为这些材料具有教学和学生评估的潜在用途。
</details></li>
</ul>
<hr>
<h2 id="FP8-LM-Training-FP8-Large-Language-Models"><a href="#FP8-LM-Training-FP8-Large-Language-Models" class="headerlink" title="FP8-LM: Training FP8 Large Language Models"></a>FP8-LM: Training FP8 Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18313">http://arxiv.org/abs/2310.18313</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azure/ms-amp">https://github.com/azure/ms-amp</a></li>
<li>paper_authors: Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, Peng Cheng</li>
<li>for: 这个论文探讨FP8低位数据格式在大型自然语言模型（LLM）的高效训练中的应用。</li>
<li>methods: 作者提出了一个新的FP8自动混合精度框架，用于训练LLM模型。这个框架逐渐地使用8位数据格式，包括梯度和优化器状态，以实现混合精度和分布式并行训练。</li>
<li>results: 实验结果显示，在使用GPT-175B模型在H100 GPU平台进行训练时，作者的FP8混合精度训练框架可以实现42%的实际内存使用减少和64%的BF16框架（即Megatron-LM）的运行速度，超过Nvidia Transformer Engine的速度。此外，这种混合精度训练方法可以应用于其他任务，如LLM指令优化和人工回馈学习，从而降低精度训练成本。作者的FP8低精度训练框架已经公开开源于GitHub（<a target="_blank" rel="noopener" href="https://github.com/Azure/MS-AMP">https://github.com/Azure/MS-AMP</a>）。<details>
<summary>Abstract</summary>
In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Approach-to-Automatically-generating-Riddles-aiding-Concept-Attainment"><a href="#An-Approach-to-Automatically-generating-Riddles-aiding-Concept-Attainment" class="headerlink" title="An Approach to Automatically generating Riddles aiding Concept Attainment"></a>An Approach to Automatically generating Riddles aiding Concept Attainment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18290">http://arxiv.org/abs/2310.18290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Niharika Sri Parasa, Chaitali Diwan, Srinath Srinivasa</li>
<li>for: The paper aims to enhance learner engagement in online learning environments by applying the Concept Attainment Model to build conceptual riddles.</li>
<li>methods: The paper uses a combination of natural language processing and the Concept Attainment Model to create factual triples from learning resources, classify them based on their uniqueness to a concept, and generate riddles based on the Concept Attainment Model’s format.</li>
<li>results: The human evaluation of the riddles obtained encouraging results, indicating the effectiveness of the proposed approach in enhancing learner engagement.Here’s the simplified Chinese text for the three information points:</li>
<li>for: 本研究旨在在线学习环境中提高学习者的 engagment，通过应用概念获取模型建立概念游戏。</li>
<li>methods: 本研究使用自然语言处理和概念获取模型将学习资源转换为事实三重，根据概念的唯一性进行分类，并根据概念获取模型的格式生成游戏。</li>
<li>results: 人类评价的结果显示，提案的方法具有吸引学习者的潜力。<details>
<summary>Abstract</summary>
One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of riddles prove encouraging.
</details>
<details>
<summary>摘要</summary>
一个主要挑战在在线学习环境中是保持学生的参与度。多种不同的教学策略在在线和OFFLINE环境中被提出，以增强学生的参与度。概念把握模型是一种教学策略，强调学生深入理解概念，而不仅仅是其字面意思。这是通过搜索和列出不同概念的例子和非例子中的特征来实现的。我们尝试将概念把握模型应用于建立概念的谜题，并在在线学习环境中部署。该方法包括从学习资源中提取事实三元组，将其分类为概念的唯一特征和公共特征，然后根据概念把握模型的格式生成谜题，并捕捉所有的解决方案。人工评估结果表明，谜题的效果是有挑战性的。
</details></li>
</ul>
<hr>
<h2 id="MalFake-A-Multimodal-Fake-News-Identification-for-Malayalam-using-Recurrent-Neural-Networks-and-VGG-16"><a href="#MalFake-A-Multimodal-Fake-News-Identification-for-Malayalam-using-Recurrent-Neural-Networks-and-VGG-16" class="headerlink" title="MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16"></a>MalFake: A Multimodal Fake News Identification for Malayalam using Recurrent Neural Networks and VGG-16</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18263">http://arxiv.org/abs/2310.18263</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adhish S. Sujan, Ajitha. V, Aleena Benny, Amiya M. P., V. S. Anoop</li>
<li>for: 这篇研究的目的是为了发展一个能够有效地识别假新闻的模型，尤其是在印度的地方语言中。</li>
<li>methods: 这篇研究使用多modalities的特征提取法和深度学习分类模型来识别假新闻。</li>
<li>results: 这篇研究发现，使用多modalities的特征提取法和深度学习分类模型可以更高度准确地识别假新闻，并且在Malayalam语言中进行了首次实证。<details>
<summary>Abstract</summary>
The amount of news being consumed online has substantially expanded in recent years. Fake news has become increasingly common, especially in regional languages like Malayalam, due to the rapid publication and lack of editorial standards on some online sites. Fake news may have a terrible effect on society, causing people to make bad judgments, lose faith in authorities, and even engage in violent behavior. When we take into the context of India, there are many regional languages, and fake news is spreading in every language. Therefore, providing efficient techniques for identifying false information in regional tongues is crucial. Until now, little to no work has been done in Malayalam, extracting features from multiple modalities to classify fake news. Multimodal approaches are more accurate in detecting fake news, as features from multiple modalities are extracted to build the deep learning classification model. As far as we know, this is the first piece of work in Malayalam that uses multimodal deep learning to tackle false information. Models trained with more than one modality typically outperform models taught with only one modality. Our study in the Malayalam language utilizing multimodal deep learning is a significant step toward more effective misinformation detection and mitigation.
</details>
<details>
<summary>摘要</summary>
在最近几年，网络上新闻的浏览量有所扩大。假新闻在当地语言 like 马拉雅利姆语中变得越来越普遍，尤其是在一些在线站点上不具备编辑标准的情况下。假新闻可能对社会产生坏处，让人们做出错误的判断，失去对权威机构的信任，甚至发生暴力行为。在印度国情下，有很多的地方语言，假新闻在每种语言中广泛传播。因此，为了有效地检测假新闻，在马拉雅利姆语中提供有效的技术是非常重要。直到现在，我们知道的是，在马拉雅利姆语中使用多Modalities 的深度学习模型来检测假新闻是第一次。使用多Modalities 的特征可以提高假新闻检测的准确率，因为从多个模式中提取的特征用于建立深度学习分类模型。我们的研究表明，使用多Modalities 的深度学习模型在马拉雅利姆语中可以有效地检测假新闻。这是一项重要的研究，可以帮助我们更好地检测和解决假新闻。
</details></li>
</ul>
<hr>
<h2 id="Revising-with-a-Backward-Glance-Regressions-and-Skips-during-Reading-as-Cognitive-Signals-for-Revision-Policies-in-Incremental-Processing"><a href="#Revising-with-a-Backward-Glance-Regressions-and-Skips-during-Reading-as-Cognitive-Signals-for-Revision-Policies-in-Incremental-Processing" class="headerlink" title="Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing"></a>Revising with a Backward Glance: Regressions and Skips during Reading as Cognitive Signals for Revision Policies in Incremental Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18229">http://arxiv.org/abs/2310.18229</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/briemadu/revreg">https://github.com/briemadu/revreg</a></li>
<li>paper_authors: Brielen Madureira, Pelin Çelikkol, David Schlangen</li>
<li>for: 这个论文旨在研究如何使用人类阅读眼动追踪数据来优化增量处理器的修订策略。</li>
<li>methods: 这个论文使用了人类阅读眼动追踪数据，并使用了普通的混合效应模型来分析人类的阅读习惯。</li>
<li>results: 研究发现，人类阅读眼动追踪数据中的回退和跳过可能serve as useful predictors for revisions in BiLSTMs and Transformer models，并且这些结果适用于多种语言。<details>
<summary>Abstract</summary>
In NLP, incremental processors produce output in instalments, based on incoming prefixes of the linguistic input. Some tokens trigger revisions, causing edits to the output hypothesis, but little is known about why models revise when they revise. A policy that detects the time steps where revisions should happen can improve efficiency. Still, retrieving a suitable signal to train a revision policy is an open problem, since it is not naturally available in datasets. In this work, we investigate the appropriateness of regressions and skips in human reading eye-tracking data as signals to inform revision policies in incremental sequence labelling. Using generalised mixed-effects models, we find that the probability of regressions and skips by humans can potentially serve as useful predictors for revisions in BiLSTMs and Transformer models, with consistent results for various languages.
</details>
<details>
<summary>摘要</summary>
在自然语言处理（NLP）中，逐步处理器生成输出，基于进来的语言输入前缀。一些token触发修订，导致输出假设中的修订，但是不多少是为何模型修订这件事情都不太清楚。一个政策可以提高效率是在哪些时间步骤中进行修订。然而，找到适合训练修订政策的信号仍然是一个开放的问题，因为这些信号不自然地出现在数据集中。在这项工作中，我们 investigate了人类阅读眼动追踪数据中的回退和跳过是否可以作为修订政策的信号。使用通用混合效应模型，我们发现人类的回退和跳过概率可能可以作为BiLSTM和Transformer模型中的修订预测器，具有一致的结果。
</details></li>
</ul>
<hr>
<h2 id="ArcheType-A-Novel-Framework-for-Open-Source-Column-Type-Annotation-using-Large-Language-Models"><a href="#ArcheType-A-Novel-Framework-for-Open-Source-Column-Type-Annotation-using-Large-Language-Models" class="headerlink" title="ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models"></a>ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18208">http://arxiv.org/abs/2310.18208</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/penfever/archetype">https://github.com/penfever/archetype</a></li>
<li>paper_authors: Benjamin Feuer, Yurong Liu, Chinmay Hegde, Juliana Freire</li>
<li>for: 本研究旨在解决现有深度学习方法 для semantic column type annotation (CTA) 中的重要缺点，包括类型 fixed 在训练时间、大量训练样本和高 inference 成本。</li>
<li>methods: 本研究使用大语言模型来解决 CTA 问题，并提出了一种简单、实用的方法 ArcheType，包括 context sampling、prompt serialization、model querying 和 label remapping。</li>
<li>results: 本研究在 zero-shot 和 fine-tuned CTA 问题上达到了新的州Of-the-art 性能，包括三个新的领域特定的benchmark，并发布了相关的代码和数据。<details>
<summary>Abstract</summary>
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned CTA, including three new domain-specific benchmarks, which we release, along with the code to reproduce our results at https://github.com/penfever/ArcheType.
</details>
<details>
<summary>摘要</summary>
现有的深度学习方法 дляsemantic column type annotation（CTA）具有重要的缺点：它们依赖于固定的semantic type，需要训练样本数量很多，并且在运行时会产生大量的计算成本。此外，它们在新的数据集上表现不佳，即使类型保持不变。大型语言模型在各种任务上表现出了强的零shot分类能力，在这篇论文中，我们探索了它们在CTA中的使用。我们介绍了ArcheType，一种简单、实用的方法，可以使大型语言模型解决column type annotation问题，无需任何训练样本。我们分别离去每个方法的组成部分，并证明了改进context sampling和label remapping可以提供最大的改进。ArcheType在零shot和精心调整的CTA中成功地设置新的状态纪录，包括三个新的域特定的benchmark，我们在https://github.com/penfever/ArcheType中发布了这些benchmark和 reproduce我们的结果的代码。
</details></li>
</ul>
<hr>
<h2 id="INA-An-Integrative-Approach-for-Enhancing-Negotiation-Strategies-with-Reward-Based-Dialogue-System"><a href="#INA-An-Integrative-Approach-for-Enhancing-Negotiation-Strategies-with-Reward-Based-Dialogue-System" class="headerlink" title="INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System"></a>INA: An Integrative Approach for Enhancing Negotiation Strategies with Reward-Based Dialogue System</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18207">http://arxiv.org/abs/2310.18207</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zishan-ai/neg">https://github.com/zishan-ai/neg</a></li>
<li>paper_authors: Zishan Ahmad, Suman Saurabh, Vaishakh Sreekanth Menon, Asif Ekbal, Roshni Ramnani, Anutosh Maitra<br>for:* The paper proposes a novel negotiation dialogue agent for online marketplaces, designed to negotiate on price and other factors such as item inclusion&#x2F;exclusion in a bundle deal.methods:* The agent uses a new semi-automated data creation method that combines defining negotiation intents, actions, and intent-action simulation to generate potential dialogue flows.* The agent employs a set of novel rewards tailored for the negotiation task to train the Integrative Negotiation Agent (INA).results:* The proposed approach and reward system significantly enhance the agent’s negotiation capabilities, allowing it to engage in integrative negotiations and dynamically adjust prices and item inclusions&#x2F;exclusions in a bundle deal.<details>
<summary>Abstract</summary>
In this paper, we propose a novel negotiation dialogue agent designed for the online marketplace. Our agent is integrative in nature i.e, it possesses the capability to negotiate on price as well as other factors, such as the addition or removal of items from a deal bundle, thereby offering a more flexible and comprehensive negotiation experience. We create a new dataset called Integrative Negotiation Dataset (IND) to enable this functionality. For this dataset creation, we introduce a new semi-automated data creation method, which combines defining negotiation intents, actions, and intent-action simulation between users and the agent to generate potential dialogue flows. Finally, the prompting of GPT-J, a state-of-the-art language model, is done to generate dialogues for a given intent, with a human-in-the-loop process for post-editing and refining minor errors to ensure high data quality. We employ a set of novel rewards, specifically tailored for the negotiation task to train our Negotiation Agent, termed as the Integrative Negotiation Agent (INA). These rewards incentivize the chatbot to learn effective negotiation strategies that can adapt to various contextual requirements and price proposals. By leveraging the IND, we train our model and conduct experiments to evaluate the effectiveness of our reward-based dialogue system for negotiation. Our results demonstrate that the proposed approach and reward system significantly enhance the agent's negotiation capabilities. The INA successfully engages in integrative negotiations, displaying the ability to dynamically adjust prices and negotiate the inclusion or exclusion of items in a bundle deal
</details>
<details>
<summary>摘要</summary>
本文提出了一种新的谈判对话机器人，适用于在线市场场所。我们的机器人具有整合性，即可以谈判价格以及其他因素，如交易包中的物品添加或删除，从而提供更加灵活和全面的谈判体验。我们创建了一个新的整合谈判数据集（IND），以实现这种功能。为了创建IND，我们提出了一种新的半自动化数据创建方法，该方法结合定义谈判意图、行为和用户和机器人之间的意图动作模拟，以生成潜在的对话流程。最后，我们使用GPT-J，一种现代自然语言处理模型，来提示对话，并进行人类在 Loop 过程中的修改和微调，以确保数据质量高。我们采用一组特定于谈判任务的新奖励，以训练我们的谈判机器人，称为整合谈判机器人（INA）。这些奖励激励机器人学习有效的谈判策略，能够适应不同的情况和价格建议。通过利用IND，我们训练我们的模型，并进行实验来评估我们的奖励基于对话系统的效果。我们的结果表明，我们的方法和奖励系统可以显著提高机器人的谈判能力。INA成功地参与了整合谈判，展现了可以动态调整价格并谈判交易包中的物品 inclusion 或 exclusion 的能力。
</details></li>
</ul>
<hr>
<h2 id="Lost-in-Translation-Found-in-Spans-Identifying-Claims-in-Multilingual-Social-Media"><a href="#Lost-in-Translation-Found-in-Spans-Identifying-Claims-in-Multilingual-Social-Media" class="headerlink" title="Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media"></a>Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18205">http://arxiv.org/abs/2310.18205</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mbzuai-nlp/x-claim">https://github.com/mbzuai-nlp/x-claim</a></li>
<li>paper_authors: Shubham Mittal, Megha Sundriyal, Preslav Nakov</li>
<li>for: 这篇论文是为了提高社交媒体文本中Checkworthy声明的识别率而写的。</li>
<li>methods: 这篇论文使用了新的数据集X-CLAIM，包含5种印度语言和英语的7000个实际声明，以及现有的encoder-only语言模型和GPT系列的生成大语言模型。</li>
<li>results: 研究发现，使用多种语言进行训练可以超过零扩展传递和翻译数据进行训练的性能，并且小型encoder-only语言模型在低资源语言上表现比GPT系列更好。<details>
<summary>Abstract</summary>
Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a checkworthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.
</details>
<details>
<summary>摘要</summary>
“宣称 span 识别（CSI）是 фак-检查管道中的重要步骤，目的是寻找社交媒体文章中可信worthy的声明或asserttion。despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.”
</details></li>
</ul>
<hr>
<h2 id="Style-Description-based-Text-to-Speech-with-Conditional-Prosodic-Layer-Normalization-based-Diffusion-GAN"><a href="#Style-Description-based-Text-to-Speech-with-Conditional-Prosodic-Layer-Normalization-based-Diffusion-GAN" class="headerlink" title="Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN"></a>Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18169">http://arxiv.org/abs/2310.18169</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neeraj Kumar, Ankur Narang, Brejesh Lall</li>
<li>for: 该研究旨在提出一种基于Diffusion GAN的方法（Prosodic Diff-TTS），用于基于样式描述和内容文本的输入生成高效的语音样本，仅需4个释除步骤。</li>
<li>methods: 该方法利用了新的条件式词干层normalization技术，将样式嵌入 integrate into多头注意力基本Encoder和Mel spectrogram Decoder结构中，以生成语音。样式嵌入由预训练BERT模型在auxiliary任务上练习，如抑制、速度、情感、性别分类。</li>
<li>results: 该研究在多个多种语音数据集上进行了证明，包括LibriTTS和PromptSpeech数据集，并通过多个量化度量测试生成的准确率和MOS来证明其效果。<details>
<summary>Abstract</summary>
In this paper, we present a Diffusion GAN based approach (Prosodic Diff-TTS) to generate the corresponding high-fidelity speech based on the style description and content text as an input to generate speech samples within only 4 denoising steps. It leverages the novel conditional prosodic layer normalization to incorporate the style embeddings into the multi head attention based phoneme encoder and mel spectrogram decoder based generator architecture to generate the speech. The style embedding is generated by fine tuning the pretrained BERT model on auxiliary tasks such as pitch, speaking speed, emotion,gender classifications. We demonstrate the efficacy of our proposed architecture on multi-speaker LibriTTS and PromptSpeech datasets, using multiple quantitative metrics that measure generated accuracy and MOS.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种扩散GAN基本方法（叫做Prosodic Diff-TTS），用于根据样式描述和内容文本生成相应的高精度语音，并且只需要4个释除步骤。它利用了新的 conditional prosodic layer normalization来将样式嵌入 incorporated 到多头注意力基本架构中的phoneme encoder和mel spectrogram decoder基本生成器中，以生成语音。样式嵌入由先前热身BERT模型的 fine-tuning 在auxiliary task such as pitch, speaking speed, emotion, gender classifications中进行。我们在多个 speakers的 LibriTTS 和 PromptSpeech 数据集上证明了我们提出的架构的可行性，并使用多个量化度量来评估生成的准确性和MOS。
</details></li>
</ul>
<hr>
<h2 id="MPrompt-Exploring-Multi-level-Prompt-Tuning-for-Machine-Reading-Comprehension"><a href="#MPrompt-Exploring-Multi-level-Prompt-Tuning-for-Machine-Reading-Comprehension" class="headerlink" title="MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension"></a>MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18167">http://arxiv.org/abs/2310.18167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoxin Chen, Yiming Qian, Bowen Wang, Liangzhi Li</li>
<li>for: 这篇论文是为了提出一种轻量级的Prompt tuning方法，以提高预训练语言模型（PLMs）在新 dataset上的表现。</li>
<li>methods: 该方法使用了多级Prompt，包括任务特定、领域特定和上下文特定的Prompt，以提高输入语义理解的精度。另外，该方法还提出了一个独立性约束，以避免域特定Prompt中重复的信息。</li>
<li>results: 在12个不同的benchmark上进行了广泛的实验，并实现了与当前最佳方法的平均提升率为1.94%。<details>
<summary>Abstract</summary>
The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we present a prompt generator that incorporates context-related knowledge in the prompt generation to enhance contextual relevancy. We conducted extensive experiments on 12 benchmarks of various QA formats and achieved an average improvement of 1.94\% over the state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型已经在不同的自然语言任务上实现了出色的性能。然而，这些方法具有资源占用很大的缺点，需要较多的训练数据来调整新的数据集。软提示调整方法提供了一种资源有效的解决方案，可以在保持模型权重固定的情况下，对预训练语言模型（PLMs）进行调整。现有的软提示方法主要关注设计独立的输入提示，以使模型适应新数据集的领域。这些方法通常忽略了文本的任务和上下文细节信息。在这篇论文中，我们提出了一种多级提示调整（MPrompt）方法，用于机器阅读理解。它利用提示在任务特定、领域特定和上下文特定三个级别来提高输入 semantics 的理解。我们还提出了一种独立约束，以确保每个领域特定的提示专注于自己的领域内容，以避免重复。此外，我们提出了一种 incorporating 上下文相关知识的提示生成器，以提高上下文相关性。我们在 12 个不同的benchmark上进行了广泛的实验，并实现了与状态 искус法方法的平均提升率为1.94%。
</details></li>
</ul>
<hr>
<h2 id="Elevating-Code-mixed-Text-Handling-through-Auditory-Information-of-Words"><a href="#Elevating-Code-mixed-Text-Handling-through-Auditory-Information-of-Words" class="headerlink" title="Elevating Code-mixed Text Handling through Auditory Information of Words"></a>Elevating Code-mixed Text Handling through Auditory Information of Words</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18155">http://arxiv.org/abs/2310.18155</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mamta, Zishan Ahmad, Asif Ekbal</li>
<li>for:  handles code-mixed textual data with auditory information</li>
<li>methods:  pre-training step based on masked-language-modelling with SOUNDEX representations (SAMLM) and a new input method</li>
<li>results:  improved robustness towards adversarial attacks and better classification results over popular baselines for code-mixed tasksHere is the simplified Chinese version of the three points:</li>
<li>for: 处理混合语言文本数据，使用听音信息</li>
<li>methods: 基于隐藏语言模型的预训练步骤，使用SOUNDEX表示法（SAMLM）和一种新的输入方法</li>
<li>results: 提高了对 adversarial 攻击的Robustness，以及对 code-mixed 任务的基eline 性能<details>
<summary>Abstract</summary>
With the growing popularity of code-mixed data, there is an increasing need for better handling of this type of data, which poses a number of challenges, such as dealing with spelling variations, multiple languages, different scripts, and a lack of resources. Current language models face difficulty in effectively handling code-mixed data as they primarily focus on the semantic representation of words and ignore the auditory phonetic features. This leads to difficulties in handling spelling variations in code-mixed text. In this paper, we propose an effective approach for creating language models for handling code-mixed textual data using auditory information of words from SOUNDEX. Our approach includes a pre-training step based on masked-language-modelling, which includes SOUNDEX representations (SAMLM) and a new method of providing input data to the pre-trained model. Through experimentation on various code-mixed datasets (of different languages) for sentiment, offensive and aggression classification tasks, we establish that our novel language modeling approach (SAMLM) results in improved robustness towards adversarial attacks on code-mixed classification tasks. Additionally, our SAMLM based approach also results in better classification results over the popular baselines for code-mixed tasks. We use the explainability technique, SHAP (SHapley Additive exPlanations) to explain how the auditory features incorporated through SAMLM assist the model to handle the code-mixed text effectively and increase robustness against adversarial attacks \footnote{Source code has been made available on \url{https://github.com/20118/DefenseWithPhonetics}, \url{https://www.iitp.ac.in/~ai-nlp-ml/resources.html\#Phonetics}.
</details>
<details>
<summary>摘要</summary>
随着code-mixed数据的普及，处理这类数据的需求日益增加，但这也存在许多挑战，如处理拼写变化、多语言、不同的字符集和资源不足等。现有语言模型在处理code-mixed文本时存在困难，因为它们主要关注单词的 semantics 表示，忽略了听音特征。这导致了处理拼写变化的困难。在这篇论文中，我们提出了一种有效的方法，使用听音信息来创建适用于处理code-mixed文本数据的语言模型。我们的方法包括在遮盖语言模型的预训练阶段基于MASKED-LANGUAGE-MODELING，以及一种新的输入数据提供方法。通过对不同语言的code-mixed数据集进行 sentiment、攻击和侵略等任务的实验，我们证明了我们的新的语言模型方法（SAMLM）能够更好地鲁棒化对code-mixed文本的攻击。此外，我们的SAMLM基于方法还在code-mixed任务上得到了更好的分类结果，比 популяр的基elines更好。我们使用SHAP（SHapley Additive exPlanations）技术来解释如何通过SAMLM incorporating 听音特征来处理code-mixed文本，从而提高模型对code-mixed文本的鲁棒性和抗击攻击能力。详细的源代码已经在 <https://github.com/20118/DefenseWithPhonetics> 和 <https://www.iitp.ac.in/~ai-nlp-ml/resources.html\#Phonetics> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Disentangled-Representation-Learning-with-Large-Language-Models-for-Text-Attributed-Graphs"><a href="#Disentangled-Representation-Learning-with-Large-Language-Models-for-Text-Attributed-Graphs" class="headerlink" title="Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs"></a>Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18152">http://arxiv.org/abs/2310.18152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu<br>for: 这篇论文是为了解决现有的大语言模型（LLM）在文本嵌入图（TAG）中的缺陷，提高LLM的理解和预测能力。methods: 这篇论文提出了一种名为Disentangled Graph-Text Learner（DGTL）模型，通过专门设计的分离图神经网络（GNN）层，使LLM可以更好地捕捉文本嵌入图中的复杂关系。results: 实验证明，提出的DGTL模型可以在文本嵌入图中实现superior或相当于现有基线的性能，并且可以提供自然语言的解释，因此显著提高了模型的可读性。<details>
<summary>Abstract</summary>
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.
</details>
<details>
<summary>摘要</summary>
文本归属图（TAG）在网络上非常普遍，研究人员对这些图像（如引用网络、购物网络和社交网络）的研究吸引了广泛的关注。最近，大型自然语言模型（LLM）在各种任务上表现出了非常出色的能力。然而，现有的工作强调仅通过提示来使LLM对图像进行理解，因此忽略了TAG中复杂的结构关系的问题。为解决这个问题，我们在这篇论文中提出了卷积图文学习者（DGTL）模型，可以增强LLM对TAG的理解和预测能力。我们的提议的DGTL模型通过适应的分离卷积神经网络层来捕捉TAG中多种结构因素中的复杂关系，使LLM能够从多个角度理解TAG的结构。此外，DGTL模型可以与预训练的LLM结合使用， thereby reducing computational costs and allowing for much more flexibility in combining with different LLM models。实验评估表明，我们的提议的DGTL模型可以在达到或与当前基eline相当的性能。此外，我们还示出了DGTL模型可以提供自然语言的解释，从而显著提高模型可读性。
</details></li>
</ul>
<hr>
<h2 id="DELPHI-Data-for-Evaluating-LLMs’-Performance-in-Handling-Controversial-Issues"><a href="#DELPHI-Data-for-Evaluating-LLMs’-Performance-in-Handling-Controversial-Issues" class="headerlink" title="DELPHI: Data for Evaluating LLMs’ Performance in Handling Controversial Issues"></a>DELPHI: Data for Evaluating LLMs’ Performance in Handling Controversial Issues</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18130">http://arxiv.org/abs/2310.18130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zidixiu/delphi">https://github.com/zidixiu/delphi</a></li>
<li>paper_authors: David Q. Sun, Artem Abzaliev, Hadas Kotek, Zidi Xiu, Christopher Klein, Jason D. Williams</li>
<li>For: This paper aims to systematically examine how large language models (LLMs) respond to questions related to ongoing debates and controversial issues.* Methods: The authors propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. They evaluate different LLMs using a subset of this dataset to understand how they handle controversial issues and the stances they adopt.* Results: The research reveals challenges concerning knowledge recency, safety, fairness, and bias in LLMs’ interaction with controversial issues, and contributes to our understanding of how these models handle complex societal debates.Here’s the text in Simplified Chinese:* For: 这篇论文目标是系统地检查大语言模型（LLM）对ongoing debates和争议问题的回答。* Methods: 作者提出了一种基于Quora Question Pairs Dataset的争议问题集的新建构，以评估不同LLM对争议问题的处理和立场。* Results: 研究发现LLM在处理争议问题时存在知识新鲜度、安全、公正性和偏见等挑战，这些挑战对于LLM在处理复杂社会问题的理解具有重要意义。<details>
<summary>Abstract</summary>
Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs' interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates.
</details>
<details>
<summary>摘要</summary>
争议是我们时代精神的反映，是任何讨论的重要方面。大语言模型（LLM）作为对话系统的出现，使人们倾向于依赖这些系统以解答他们的各种问题。因此，系统地检查 LLM 如何回答与当前讨论相关的问题是非常重要的。然而，现在还没有多少 datasets 提供了当今社会讨论的人工标注数据。为推动这一领域的研究，我们提出了一种新的争议问题集合，基于已公布的 Quora 问题对 dataset。这个 dataset 存在知识新鲜度、安全性、公平性和偏见等挑战。我们使用一部分这个 dataset 评估不同的 LLM，揭示它们如何处理争议问题，以及它们所采取的立场。这项研究最终会促进我们对 LLM 与复杂社会讨论的理解，为其更好地处理和理解社会争议的能力做出贡献。
</details></li>
</ul>
<hr>
<h2 id="Mind-the-Gap-Automated-Corpus-Creation-for-Enthymeme-Detection-and-Reconstruction-in-Learner-Arguments"><a href="#Mind-the-Gap-Automated-Corpus-Creation-for-Enthymeme-Detection-and-Reconstruction-in-Learner-Arguments" class="headerlink" title="Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments"></a>Mind the Gap: Automated Corpus Creation for Enthymeme Detection and Reconstruction in Learner Arguments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18098">http://arxiv.org/abs/2310.18098</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/webis-de/emnlp-23">https://github.com/webis-de/emnlp-23</a></li>
<li>paper_authors: Maja Stahl, Nick Düsterhus, Mei-Hua Chen, Henning Wachsmuth</li>
<li>for: 这篇论文主要是为了提高学生写作论据的能力，帮助学生更好地搜寻和重建论据。</li>
<li>methods: 该论文提出了两个新任务来提高学生论据质量：enthymeme detection和enthymeme reconstruction。它们使用自然语言处理技术来自动生成论据实例，并通过人工研究证明了这些实例的质量。</li>
<li>results: 该论文通过实验表明，使用该方法可以生成高质量的论据实例，并且这些实例的自然语言表达与学生原始写作的语言相似。此外，该论文还提出了初步的检测和重建论据的方法，以便进一步研究这些任务的可能性。<details>
<summary>Abstract</summary>
Writing strong arguments can be challenging for learners. It requires to select and arrange multiple argumentative discourse units (ADUs) in a logical and coherent way as well as to decide which ADUs to leave implicit, so called enthymemes. However, when important ADUs are missing, readers might not be able to follow the reasoning or understand the argument's main point. This paper introduces two new tasks for learner arguments: to identify gaps in arguments (enthymeme detection) and to fill such gaps (enthymeme reconstruction). Approaches to both tasks may help learners improve their argument quality. We study how corpora for these tasks can be created automatically by deleting ADUs from an argumentative text that are central to the argument and its quality, while maintaining the text's naturalness. Based on the ICLEv3 corpus of argumentative learner essays, we create 40,089 argument instances for enthymeme detection and reconstruction. Through manual studies, we provide evidence that the proposed corpus creation process leads to the desired quality reduction, and results in arguments that are similarly natural to those written by learners. Finally, first baseline approaches to enthymeme detection and reconstruction demonstrate the corpus' usefulness.
</details>
<details>
<summary>摘要</summary>
写出强大的论据可能对学习者来说是一项挑战。它需要选择并将多个论据性言Unit (ADU) 组织成逻辑和一致的方式，并决定哪些ADU可以被暗示，即欠Entymemes。然而，当重要的ADU缺失时，读者可能无法跟踪思维或理解论据的主要点。这篇论文提出了两个新任务来提高学习者的论据质量：识别论据缺失 (enthymeme检测) 和填充这些缺失 (enthymeme重建).我们研究了如何通过自动创建 corpora来实现这两个任务。基于 ICLEv3  Argumentative learner essays 论文库，我们创建了40,089个论据实例。通过手动研究，我们提供了证据，表明我们的 corpus 创建过程导致了期望的质量降低，并且结果是与学习者写作的论据类似的自然。最后，我们提出了首个基eline Approaches to enthymeme检测和重建，这证明了 corpus 的用用。
</details></li>
</ul>
<hr>
<h2 id="Lost-in-Translation-–-Multilingual-Misinformation-and-its-Evolution"><a href="#Lost-in-Translation-–-Multilingual-Misinformation-and-its-Evolution" class="headerlink" title="Lost in Translation – Multilingual Misinformation and its Evolution"></a>Lost in Translation – Multilingual Misinformation and its Evolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18089">http://arxiv.org/abs/2310.18089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dorian Quelle, Calvin Cheng, Alexandre Bovet, Scott A. Hale</li>
<li>for: 本研究探讨了在多语言环境中流传的谣言的频率和 dynamics，通过分析了250,000多个不同语言的事实核查。</li>
<li>methods: 该研究使用了事实核查作为谣言传播的代理，并使用多语言句子嵌入来表示事实核查。研究还使用了群erset扩展来分类相似的CLAIM，并分析了不同语言之间CLAIM的连接和短路。</li>
<li>results: 研究发现，虽然大多数谣言CLAIM只被核查一次，但11.7%的CLAIM（相当于21,000多个）被多次核查。研究还发现，33%的重复CLAIM跨语言传播，表明一些谣言可以跨越语言障碍。然而，研究还发现，谣言在同一语言中更容易传播。通过分析不同语言之间CLAIM的连接和短路，研究发现CLAIM逐渐发展和变化，并且在 crossing 语言时更加明显。<details>
<summary>Abstract</summary>
Misinformation and disinformation are growing threats in the digital age, spreading rapidly across languages and borders. This paper investigates the prevalence and dynamics of multilingual misinformation through an analysis of over 250,000 unique fact-checks spanning 95 languages. First, we find that while the majority of misinformation claims are only fact-checked once, 11.7%, corresponding to more than 21,000 claims, are checked multiple times. Using fact-checks as a proxy for the spread of misinformation, we find 33% of repeated claims cross linguistic boundaries, suggesting that some misinformation permeates language barriers. However, spreading patterns exhibit strong homophily, with misinformation more likely to spread within the same language. To study the evolution of claims over time and mutations across languages, we represent fact-checks with multilingual sentence embeddings and cluster semantically similar claims. We analyze the connected components and shortest paths connecting different versions of a claim finding that claims gradually drift over time and undergo greater alteration when traversing languages. Overall, this novel investigation of multilingual misinformation provides key insights. It quantifies redundant fact-checking efforts, establishes that some claims diffuse across languages, measures linguistic homophily, and models the temporal and cross-lingual evolution of claims. The findings advocate for expanded information sharing between fact-checkers globally while underscoring the importance of localized verification.
</details>
<details>
<summary>摘要</summary>
“误信和伪信在数字时代增长为潜在的威胁，迅速在语言和国界之间传播。这篇论文通过分析超过250,000个唯一的事实核查来研究多语言误信的普遍性和动态。我们发现大多数误信声明只被核查一次，但11.7%（相当于 más than 21,000）的声明被重复核查。使用事实核查作为误信传播的代理，我们发现33%的重复声明跨语言传播，这表明一些误信可以跨越语言障碍。然而，误信的传播模式具有强的同语群效应，误信更可能在同一语言中传播。为了研究声明的时间发展和语言过渡的变化，我们使用多语言句子嵌入表示事实核查，并对具有相似含义的声明进行聚类分析。我们分析了声明之间的连接组件和语言之间的短路，发现声明逐渐演化，并在语言之间传播时更容易发生变化。总的来说，这项研究提供了关键的发现，证实了重复核查的重要性，同时也强调了地方化验证的重要性。”
</details></li>
</ul>
<hr>
<h2 id="A-Scalable-Framework-for-Table-of-Contents-Extraction-from-Complex-ESG-Annual-Reports"><a href="#A-Scalable-Framework-for-Table-of-Contents-Extraction-from-Complex-ESG-Annual-Reports" class="headerlink" title="A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports"></a>A Scalable Framework for Table of Contents Extraction from Complex ESG Annual Reports</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18073">http://arxiv.org/abs/2310.18073</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Wang, Lin Gui, Yulan He</li>
<li>for: 这篇论文主要是关于文档结构化的研究，旨在提出一个新的 dataset，ESGDoc，包含了563家公司的1093份 ESG年报，从2001年到2022年。</li>
<li>methods: 该论文提出了一种新的框架，用于文档结构化，它包括三个步骤：（1）根据文本阅读顺序和字体大小构建初始树；（2）模型每个树节（或文本块），基于它所处的子树上的信息；（3）修改原始树，通过对每个树节进行适当的操作（保留、删除或移动）。</li>
<li>results: 该框架可以更好地处理文档的不同结构和长度，并且比前一代基eline的方法快得多。实验结果表明，我们的方法可以更高效地处理文档，并且可以适应不同的文档长度。<details>
<summary>Abstract</summary>
Table of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for Toc extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself. Experimental results show that our approach outperforms the previous state-of-the-art baseline with a fraction of running time. Our framework proves its scalability by effectively handling documents of any length.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Constructing an initial tree of text blocks based on reading order and font sizes.2. Modeling each tree node (or text block) independently by considering its contextual information captured in a node-centric subtree.3. Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move).This construction-modeling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modeling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself. Experimental results show that our approach outperforms the previous state-of-the-art baseline with a fraction of running time. Our framework proves its scalability by effectively handling documents of any length.</details></li>
</ol>
<hr>
<h2 id="Multi-grained-Evidence-Inference-for-Multi-choice-Reading-Comprehension"><a href="#Multi-grained-Evidence-Inference-for-Multi-choice-Reading-Comprehension" class="headerlink" title="Multi-grained Evidence Inference for Multi-choice Reading Comprehension"></a>Multi-grained Evidence Inference for Multi-choice Reading Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18070">http://arxiv.org/abs/2310.18070</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilin Zhao, Hai Zhao, Sufeng Duan</li>
<li>for: 多选机器阅读理解（MRC）是一项具有挑战性的任务，需要机器能够根据提供的选项回答问题。</li>
<li>methods: 我们提出了一种新的通用模型增强方法，名为多重粒度证据推理器（Mugen），用于弥补机器无法直接从给定的繁杂、噪音 passage 中提取准确证据的不足。Mugen 将在不同粒度上提取证据：粗粒度、中粒度和细粒度证据，并将证据与原始 passage 集成，实现了四个多选 MRC benchmark 上显著和一致的性能提升。</li>
<li>results: 我们的方法在四个多选 MRC benchmark 上实现了显著和一致的性能提升。<details>
<summary>Abstract</summary>
Multi-choice Machine Reading Comprehension (MRC) is a major and challenging task for machines to answer questions according to provided options. Answers in multi-choice MRC cannot be directly extracted in the given passages, and essentially require machines capable of reasoning from accurate extracted evidence. However, the critical evidence may be as simple as just one word or phrase, while it is hidden in the given redundant, noisy passage with multiple linguistic hierarchies from phrase, fragment, sentence until the entire passage. We thus propose a novel general-purpose model enhancement which integrates multi-grained evidence comprehensively, named Multi-grained evidence inferencer (Mugen), to make up for the inability. Mugen extracts three different granularities of evidence: coarse-, middle- and fine-grained evidence, and integrates evidence with the original passages, achieving significant and consistent performance improvement on four multi-choice MRC benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="“Honey-Tell-Me-What’s-Wrong”-Global-Explanation-of-Textual-Discriminative-Models-through-Cooperative-Generation"><a href="#“Honey-Tell-Me-What’s-Wrong”-Global-Explanation-of-Textual-Discriminative-Models-through-Cooperative-Generation" class="headerlink" title="“Honey, Tell Me What’s Wrong”, Global Explanation of Textual Discriminative Models through Cooperative Generation"></a>“Honey, Tell Me What’s Wrong”, Global Explanation of Textual Discriminative Models through Cooperative Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18063">http://arxiv.org/abs/2310.18063</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Chaffin, Julien Delaunay</li>
<li>for: 这篇论文的目的是提出一种全球和模型无关的解释方法，用于文本分类器中。</li>
<li>methods: 这种方法基于合作生成的文本，不需要输入数据集，可以在数据缺失时提供解释。</li>
<li>results: 实验表明，这种方法可以准确地描述分类器对输入空间中的行为，并且在输入数据不具体化时表现更好于使用输入数据的方法。<details>
<summary>Abstract</summary>
The ubiquity of complex machine learning has raised the importance of model-agnostic explanation algorithms. These methods create artificial instances by slightly perturbing real instances, capturing shifts in model decisions. However, such methods rely on initial data and only provide explanations of the decision for these. To tackle these problems, we propose Therapy, the first global and model-agnostic explanation method adapted to text which requires no input dataset. Therapy generates texts following the distribution learned by a classifier through cooperative generation. Because it does not rely on initial samples, it allows to generate explanations even when data is absent (e.g., for confidentiality reasons). Moreover, conversely to existing methods that combine multiple local explanations into a global one, Therapy offers a global overview of the model behavior on the input space. Our experiments show that although using no input data to generate samples, Therapy provides insightful information about features used by the classifier that is competitive with the ones from methods relying on input samples and outperforms them when input samples are not specific to the studied model.
</details>
<details>
<summary>摘要</summary>
“复杂机器学习的普遍使得模型无关解释算法的重要性提高。这些方法通过微量修改真实实例而创造人工实例，捕捉模型决策的变化。然而，这些方法仅依赖于初始数据，只能提供这些数据的决策说明。为解决这些问题，我们提议了疗法（Therapy），是首个全球、模型无关的解释方法，不需要输入数据集。疗法通过与分类器一起生成文本，学习分类器的分布。因为不依赖于初始样本，疗法可以在没有数据时产生解释（例如，保持隐私原则）。此外，不同于现有的方法，将多个本地解释合并成一个全局解释，疗法提供了输入空间上模型行为的全面视图。我们的实验表明，使用没有输入数据生成样本，疗法可以提供有用的特征信息，与使用输入数据生成样本的方法竞争，并在输入数据不是特定于研究模型时表现更好。”
</details></li>
</ul>
<hr>
<h2 id="ViCLEVR-A-Visual-Reasoning-Dataset-and-Hybrid-Multimodal-Fusion-Model-for-Visual-Question-Answering-in-Vietnamese"><a href="#ViCLEVR-A-Visual-Reasoning-Dataset-and-Hybrid-Multimodal-Fusion-Model-for-Visual-Question-Answering-in-Vietnamese" class="headerlink" title="ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese"></a>ViCLEVR: A Visual Reasoning Dataset and Hybrid Multimodal Fusion Model for Visual Question Answering in Vietnamese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18046">http://arxiv.org/abs/2310.18046</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kvt0012/viclevr">https://github.com/kvt0012/viclevr</a></li>
<li>paper_authors: Khiem Vinh Tran, Hao Phu Phan, Kiet Van Nguyen, Ngan Luu Thuy Nguyen</li>
<li>for: 本研究旨在提高越南语言Visual Question Answering（VQA）系统的性能，并探讨现代视觉理解系统的强点和局限性。</li>
<li>methods: 本研究使用了一个新的多模态混合方法，称为 PhoVIT，该方法可以基于问题来确定图像中的对象。PhoVIT使用了 transformers 来同时进行文本和视觉数据的推理，并在早期模型阶段将两种模式融合。</li>
<li>results: 实验结果显示，我们的提议的模型在四个评价指标中均达到了当前最佳性能。<details>
<summary>Abstract</summary>
In recent years, Visual Question Answering (VQA) has gained significant attention for its diverse applications, including intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion that identifies objects in images based on questions. The architecture effectively employs transformers to enable simultaneous reasoning over textual and visual data, merging both modalities at an early model stage. The experimental findings demonstrate that our proposed model achieves state-of-the-art performance across four evaluation metrics. The accompanying code and dataset have been made publicly accessible at \url{https://github.com/kvt0012/ViCLEVR}. This provision seeks to stimulate advancements within the research community, fostering the development of more multimodal fusion algorithms, specifically tailored to address the nuances of low-resource languages, exemplified by Vietnamese.
</details>
<details>
<summary>摘要</summary>
Recently, Visual Question Answering (VQA) has gained significant attention due to its diverse applications, such as intelligent car assistance, aiding visually impaired individuals, and document image information retrieval using natural language queries. VQA requires the effective integration of information from questions and images to generate accurate answers. Neural models for VQA have made remarkable progress on large-scale datasets, with a primary focus on resource-rich languages like English. To address this, we introduce the ViCLEVR dataset, a pioneering collection for evaluating various visual reasoning capabilities in Vietnamese while mitigating biases. The dataset comprises over 26,000 images and 30,000 question-answer pairs (QAs), each question annotated to specify the type of reasoning involved. Leveraging this dataset, we conduct a comprehensive analysis of contemporary visual reasoning systems, offering valuable insights into their strengths and limitations. Furthermore, we present PhoVIT, a comprehensive multimodal fusion that identifies objects in images based on questions. The architecture effectively employs transformers to enable simultaneous reasoning over textual and visual data, merging both modalities at an early model stage. The experimental findings demonstrate that our proposed model achieves state-of-the-art performance across four evaluation metrics. The accompanying code and dataset have been made publicly accessible at [url=https://github.com/kvt0012/ViCLEVR]. This provision seeks to stimulate advancements within the research community, fostering the development of more multimodal fusion algorithms, specifically tailored to address the nuances of low-resource languages, exemplified by Vietnamese.
</details></li>
</ul>
<hr>
<h2 id="On-General-Language-Understanding"><a href="#On-General-Language-Understanding" class="headerlink" title="On General Language Understanding"></a>On General Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18038">http://arxiv.org/abs/2310.18038</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: David Schlangen</li>
<li>for: 这篇论文的目的是为了探讨人工智能语言处理领域内的语言理解问题，以及现有测量模型质量的方法是否具有足够的有效性。</li>
<li>methods: 这篇论文使用了一种模型，用于描述语言理解是一种多方面的现象，兼包含个人主义和社会过程。</li>
<li>results: 这篇论文的结论是，不同的语言使用场景类型具有不同的特点，而语言理解是一种多方面的现象，需要考虑个人主义和社会过程。此外，选择的理解指标会影响测量模型质量的限制，并且开启了对NLP使用的伦理考虑。<details>
<summary>Abstract</summary>
Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement ("Do Large Language Models Understand Language, And If So, How Much?"). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current methods of measurement of model quality. The paper makes three claims: A) That different language use situation types have different characteristics, B) That language understanding is a multifaceted phenomenon, bringing together individualistic and social processes, and C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use.
</details>
<details>
<summary>摘要</summary>
自然语言处理（NLP）自认为是一个经验主义的，甚至是直接经验主义的领域，然而最近它似乎涉及到必要的本质主义辩论（"大语言模型是否理解语言，以及如何量度它们？"）。这不是偶合：在这里，就如 everywhere else，证据不够特征化理解。为了解决这问题，这篇论文提出了一个理解模型，以便评估现有测量模型质量的问题。论文提出了三个主张：A）不同的语言使用情况类型有不同的特征；B）语言理解是多方面的现象，既具有个人主义的特征，又具有社会过程的特征；C）选择理解指标标志着测量的限制，也标志着NLP使用的伦理考虑的开始。
</details></li>
</ul>
<hr>
<h2 id="SentMix-3L-A-Bangla-English-Hindi-Code-Mixed-Dataset-for-Sentiment-Analysis"><a href="#SentMix-3L-A-Bangla-English-Hindi-Code-Mixed-Dataset-for-Sentiment-Analysis" class="headerlink" title="SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis"></a>SentMix-3L: A Bangla-English-Hindi Code-Mixed Dataset for Sentiment Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18023">http://arxiv.org/abs/2310.18023</a></li>
<li>repo_url: None</li>
<li>paper_authors: Md Nishat Raihan, Dhiman Goswami, Antara Mahmud, Antonios Anstasopoulos, Marcos Zampieri</li>
<li>for: 这篇论文的目的是为了提出一个新的三种语言混合数据集（SentMix-3L），用于 sentiment analysis 的计算模型训练。</li>
<li>methods: 该论文使用了 GPT-3.5 作为预训练模型，并进行了对 SentMix-3L 的全面评估。</li>
<li>results: 研究发现，使用 GPT-3.5 的零shot提问方法可以在 SentMix-3L 上超越所有基于 transformer 的模型。<details>
<summary>Abstract</summary>
Code-mixing is a well-studied linguistic phenomenon when two or more languages are mixed in text or speech. Several datasets have been build with the goal of training computational models for code-mixing. Although it is very common to observe code-mixing with multiple languages, most datasets available contain code-mixed between only two languages. In this paper, we introduce SentMix-3L, a novel dataset for sentiment analysis containing code-mixed data between three languages Bangla, English, and Hindi. We carry out a comprehensive evaluation using SentMix-3L. We show that zero-shot prompting with GPT-3.5 outperforms all transformer-based models on SentMix-3L.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化字符串。<</SYS>>研究人员已经广泛研究了语言混合现象，在文本或语音中混合两种或更多的语言。许多数据集已经建立，用于训练计算机模型。虽然混合多种语言很常见，但大多数可用的数据集只包含两种语言的混合。在这篇论文中，我们介绍了一个新的 sentiment 分析数据集 SentMix-3L，包含三种语言孟加拉语、英语和印地语的混合数据。我们进行了全面的评估，并显示了 GPT-3.5 预训练模型在 SentMix-3L 上的零批训练性能超过所有 transformer 模型。
</details></li>
</ul>
<hr>
<h2 id="NLP-Evaluation-in-trouble-On-the-Need-to-Measure-LLM-Data-Contamination-for-each-Benchmark"><a href="#NLP-Evaluation-in-trouble-On-the-Need-to-Measure-LLM-Data-Contamination-for-each-Benchmark" class="headerlink" title="NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark"></a>NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18018">http://arxiv.org/abs/2310.18018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oscar Sainz, Jon Ander Campos, Iker García-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, Eneko Agirre</li>
<li>for: 本文 argue that classical NLP task evaluation using annotated benchmarks is facing a serious problem, specifically the worst kind of data contamination.</li>
<li>methods: 本文提出了不同级别的数据污染水平，并呼吁社区共同努力，包括开发自动和半自动检测数据 benchmark 中模型训练时的污染程度的方法，以及建议将污染数据导致的科学结论列入涂抹名单。</li>
<li>results: 本文表明，当一个大自然语言模型（LLM）在测试分割上训练，然后在同一个benchmark上评估时，会导致模型性能的过高估计，从而导致科学结论的错误公布，同时正确的结论被抛弃。这种情况可能导致科学研究的假阳性结论，并且可能对社会造成不良影响。<details>
<summary>Abstract</summary>
In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.
</details>
<details>
<summary>摘要</summary>
在这份位点论文中，我们Arguments that the traditional evaluation of Natural Language Processing (NLP) tasks using annotated benchmarks is facing a crisis. The most severe data contamination occurs when a Large Language Model (LLM) is trained on the test set of a benchmark and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not easy to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task compared to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and advocates for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.
</details></li>
</ul>
<hr>
<h2 id="Does-Role-Playing-Chatbots-Capture-the-Character-Personalities-Assessing-Personality-Traits-for-Role-Playing-Chatbots"><a href="#Does-Role-Playing-Chatbots-Capture-the-Character-Personalities-Assessing-Personality-Traits-for-Role-Playing-Chatbots" class="headerlink" title="Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots"></a>Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17976">http://arxiv.org/abs/2310.17976</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/LC1332/Chat-Haruhi-Suzumiya">https://github.com/LC1332/Chat-Haruhi-Suzumiya</a></li>
<li>paper_authors: Xintao Wang, Quan Tu, Yaying Fei, Ziang Leng, Cheng Li</li>
<li>for: 这篇论文旨在探讨如何使用大规模预训练语言模型来评估角色扮演聊天机器人的人格特质。</li>
<li>methods: 该论文提出了一种开放结束式采访方法，用于评估角色扮演聊天机器人的人格特质，并对32个使用ChatHaruhi库创建的角色扮演聊天机器人进行了评估。</li>
<li>results: 研究结果显示，使用大规模预训练语言模型创建的角色扮演聊天机器人可以准确表现出对应的人格特质，与人类所认可的人格特质的一致率为82.8%。此外，论文还提出了可能的形成聊天机器人人格的策略。因此，这篇论文为Role-playing聊天机器人的研究提供了一个基础性的研究。<details>
<summary>Abstract</summary>
The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chatbots that intersects computational linguistics and psychology. Our resources are available at https://github.com/LC1332/Chat-Haruhi-Suzumiya
</details>
<details>
<summary>摘要</summary>
大规模预训语言模型的出现对新的人工智能应用程序带来了革命性的变革，特别是在游戏角色聊天机器人的领域。由于聊天机器人的“刺激-应答”性质，这篇论文推出了一种创新的开端式 интервью式人格测试方法，可以更深入地了解角色聊天机器人的内在人格特质。我们对使用ChatHaruhi库创建的32个角色聊天机器人进行了人格测试，包括Big Five和MBTI维度，并与人类的认知进行比较。结果显示，现代基于LLMs的角色聊天机器人可以有效表达对应的人格特质，与人类认知的人格Alignment率为82.8%。此外，我们还提出了可能的聊天机器人人格模型的形成策略。因此，本论文作为计算语言学和心理学交叉领域的基础研究，可以为角色聊天机器人的开发提供启示。我们的资源可以在https://github.com/LC1332/Chat-Haruhi-Suzumiya 查看。
</details></li>
</ul>
<hr>
<h2 id="Whisper-MCE-Whisper-Model-Finetuned-for-Better-Performance-with-Mixed-Languages"><a href="#Whisper-MCE-Whisper-Model-Finetuned-for-Better-Performance-with-Mixed-Languages" class="headerlink" title="Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages"></a>Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17953">http://arxiv.org/abs/2310.17953</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peng Xie, XingYuan Liu, ZiWei Chen, Kani Chen, Yang Wang</li>
<li>for: 这项研究旨在提高英语自动语音识别（ASR）中的人类水平稳定性和准确率，特别是在小语言和混合语言语音识别方面。</li>
<li>methods: 这项研究使用了自己收集的混合粤语和英语音频数据集（MCE）来训练了自适应的Whisper模型（Whisper-MCE），并提出了一种新的评价机制来评估模型在小语言和混合语言上的效果。</li>
<li>results: 研究表明， compare to基线的Whisper-大型v2模型，Whisper-MCE模型能够更好地捕捉原始音频的内容，实现更高的识别精度，并且具有更快的识别速度，特别是在混合语言任务中表现出色。<details>
<summary>Abstract</summary>
Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
</details>
<details>
<summary>摘要</summary>
最近，Whisper 在英语自动语音识别（ASR）中达到了人类水平的Robustness和准确率，而在小语言和杂语言语音识别方面仍然有很大的改进空间。在这项工作中，我们发布了我们自收集的数据集，混合粤语和英语音频数据集（MCE），并使用这些数据集来训练我们的 Whisper 模型，并对其进行了迁移。尽管 word error rate（WER）在小语言和杂语言上存在评估效果的挑战，我们则提出了一种新的评价机制。通过对我们的模型与基eline whisper-large-v2 模型进行比较，我们表明了我们的模型在捕捉原始音频内容的能力更高， recognition 率更高，并且速度更快。值得一提是，我们的模型在杂语言识别任务中表现出色，胜过其他现有的模型。
</details></li>
</ul>
<hr>
<h2 id="SOUL-Towards-Sentiment-and-Opinion-Understanding-of-Language"><a href="#SOUL-Towards-Sentiment-and-Opinion-Understanding-of-Language" class="headerlink" title="SOUL: Towards Sentiment and Opinion Understanding of Language"></a>SOUL: Towards Sentiment and Opinion Understanding of Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17924">http://arxiv.org/abs/2310.17924</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/soul">https://github.com/damo-nlp-sg/soul</a></li>
<li>paper_authors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing</li>
<li>for: 评估语言模型在情感分析领域的能力，探讨语言模型是否能够理解语言中的情感和意见。</li>
<li>methods: 提出了一种新的任务叫做情感和意见理解语言（SOUL），SOUL包括两个子任务：评论理解（RC）和证明生成（JG）。</li>
<li>results: 实验结果表明，SOUL是现有语言模型很难解决的任务，与人类表现相比，语言模型的性能差距可达27%。此外，与人类专家和GPT-4进行评估表明，小语言模型在生成有理根据的证明方面存在限制。这些结果强调了现有语言模型在情感分析领域存在的复杂性，并提出了进一步发展情感分析的需求。<details>
<summary>Abstract</summary>
Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL.
</details>
<details>
<summary>摘要</summary>
sentiment分析是一个已经广泛应用的自然语言处理任务，其中情感质量分类是该领域最受欢迎的任务之一。然而，尽管先前训练的语言模型在这个领域取得了成功，但它们经常无法捕捉 sentiment分析的更广泛复杂性。为了解决这个问题，我们提出了一个新的任务，即语言情感理解（SOUL）。SOUL的目的是评估语言情感理解的能力，通过两个子任务：评论理解（RC）和证明生成（JG）。RC检验基于评论文本中主观信息的准确性，而JG要求模型为其情感预测提供解释。为了实现全面的评估，我们注释了一个新的数据集，包含15,028个语句，来自3,638篇评论。实验结果表明，SOUL是现有模型的一个挑战性任务，与人类表现的差距可达27%。此外，通过人类专家和GPT-4的评估，我们发现小语言模型在生成理由基于的证明方面存在限制。这些发现强调现有模型在情感分析的复杂性方面存在困难，需要进一步的进步，以更好地捕捉 sentiment分析的复杂性。新的数据集和代码可以在https://github.com/DAMO-NLP-SG/SOUL上获取。
</details></li>
</ul>
<hr>
<h2 id="3D-Aware-Visual-Question-Answering-about-Parts-Poses-and-Occlusions"><a href="#3D-Aware-Visual-Question-Answering-about-Parts-Poses-and-Occlusions" class="headerlink" title="3D-Aware Visual Question Answering about Parts, Poses and Occlusions"></a>3D-Aware Visual Question Answering about Parts, Poses and Occlusions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17914">http://arxiv.org/abs/2310.17914</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xingruiwang/3d-aware-vqa">https://github.com/xingruiwang/3d-aware-vqa</a></li>
<li>paper_authors: Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, Alan Yuille</li>
<li>for: 推动3D视Question Answering领域的进步，提高VQA模型对3D场景的理解。</li>
<li>methods: 提出了3D-aware VQA任务，并设计了Super-CLEVR-3D数据集，用于挑战VQA模型的 Compositional Reasoning能力。</li>
<li>results: 提出了PO3D-VQA模型，结合概率神经 симвоlic Program Execution 和深度神经网络，实现了3D生成表示和可靠视觉识别。实验结果显示PO3D-VQA模型在3D-aware VQA任务中表现出色，但还有一定的性能差距与2D VQA标准准样本比较， indicating that 3D-aware VQA remains an important open research area。<details>
<summary>Abstract</summary>
Despite rapid progress in Visual question answering (VQA), existing datasets and models mainly focus on testing reasoning in 2D. However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation. This includes an understanding of the 3D object pose, their parts and occlusions. In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes. We address 3D-aware VQA from both the dataset and the model perspective. First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions. Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition. Our experimental results show our model PO3D-VQA outperforms existing methods significantly, but we still observe a significant performance gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an important open research area.
</details>
<details>
<summary>摘要</summary>
尽管视觉问答（VQA）已经快速进步，现有的数据集和模型主要是测试二维空间中的逻辑能力。然而，VQA模型也需要理解三维视觉场景的结构，如支持导航或操作任务。这包括理解三维物体姿态、部件和遮挡。在这项工作中，我们引入三维逻辑VQA任务，它挑战需要对视觉场景的三维结构进行复杂的推理。我们从数据集和模型两个角度解决3D-aware VQA。首先，我们介绍Super-CLEVR-3D数据集，它包含对物体部件、姿态和遮挡进行复杂的推理的问题。其次，我们提出PO3D-VQA模型，它结合了可靠的神经网络符号表示和深度神经网络的3D生成表示来实现可靠的视觉识别和逻辑推理。我们的实验结果表明，PO3D-VQA模型在3D-aware VQA任务上表现出色，但我们还观察到与2D VQA标准chmark相比，3D-aware VQA任务的性能仍然存在显著的差距，因此3D-aware VQA仍然是一个重要的未解决问题。
</details></li>
</ul>
<hr>
<h2 id="TarGEN-Targeted-Data-Generation-with-Large-Language-Models"><a href="#TarGEN-Targeted-Data-Generation-with-Large-Language-Models" class="headerlink" title="TarGEN: Targeted Data Generation with Large Language Models"></a>TarGEN: Targeted Data Generation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17876">http://arxiv.org/abs/2310.17876</a></li>
<li>repo_url: None</li>
<li>paper_authors: Himanshu Gupta, Kevin Scaria, Ujjwala Anantheswaran, Shreyas Verma, Mihir Parmar, Saurabh Arjun Sawant, Chitta Baral, Swaroop Mishra</li>
<li>for: 这个论文旨在提供一种基于大语言模型（LLM）的多步指示策略，用于生成高质量的人工数据集。</li>
<li>methods: 该策略基于LLM，并且不需要特定任务实例，因此可以广泛应用于不同任务。另外， authors 还提出了一种自修复技术，以便LLM在数据创建过程中纠正错误标签。</li>
<li>results: 通过在8个SuperGLUE任务上训练不同类型的语言模型，包括编码器-解码器、编码器和解码器等，authors 发现 TarGEN 可以生成高质量的人工数据集，并且与原始数据集相比，模型在 TarGEN 数据集上训练后表现约1-2%点更高。<details>
<summary>Abstract</summary>
The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models trained on datasets generated by TarGEN perform approximately 1-2% points better than those trained on original datasets (82.84% via syn. vs. 81.12% on og. using Flan-T5). When incorporating instruction tuning, the performance increases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A comprehensive analysis of the synthetic dataset compared to the original dataset reveals that the synthetic dataset demonstrates similar or higher levels of dataset complexity and diversity. Furthermore, the synthetic dataset displays a bias level that aligns closely with the original dataset. Finally, when pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive results on the OpenLLM leaderboard, surpassing the model trained on the Self-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for quality data generation and reducing the human efforts to create complex benchmarks.
</details>
<details>
<summary>摘要</summary>
LLMS 的快速进步已经引起了数据生成技术的兴趣，以生成多样化和高质量的synthetic dataset。然而，这些synthetic dataset经常受到缺乏多样性和附加噪音的问题困扰。在本文中，我们提出了 TarGEN，一种多步提示策略，通过 LLMS 来生成高质量的synthetic dataset。 TarGEN 的优点在于它不需要特定任务实例，因此其可以应用于任务复制以外的场景。我们还在 TarGEN 中添加了一种自修复技术，使 LLMS 能够在数据创建过程中纠正错误标签，以确保可靠的标签。为评估我们的技术效果，我们在SuperGLUEbenchmark中模拟了8个任务，并使用不同的语言模型进行训练。我们发现，使用 TarGEN 生成的synthetic dataset，模型在原始测试集上的性能约为1-2%点高于使用原始数据训练的模型（82.84% via syn. vs. 81.12% on og. using Flan-T5）。当将 instrucion tuning  incorporated 时，模型在synthetic数据上的性能提高到84.54% vs. 81.49% on original data by Flan-T5。我们对synthetic dataset和原始 dataset进行了全面的分析，发现synthetic dataset中的多样性和复杂性与原始 dataset相似或更高，并且噪音水平与原始 dataset相似。最后，当 T5-3B 在我们的synthetic SuperGLUE dataset上进行预训练后，在OpenLLM领头占据了4.14%点的优势。我们希望 TarGEN 可以帮助生成高质量的数据，并减少人类创建复杂的benchmark所需的努力。
</details></li>
</ul>
<hr>
<h2 id="From-Values-to-Opinions-Predicting-Human-Behaviors-and-Stances-Using-Value-Injected-Large-Language-Models"><a href="#From-Values-to-Opinions-Predicting-Human-Behaviors-and-Stances-Using-Value-Injected-Large-Language-Models" class="headerlink" title="From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models"></a>From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17857">http://arxiv.org/abs/2310.17857</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongjunkang/vim">https://github.com/dongjunkang/vim</a></li>
<li>paper_authors: Dongjun Kang, Joonsuk Park, Yohan Jo, JinYeong Bak</li>
<li>for: 预测人们对问题和行为的意见和选择在现实场景中有助于各领域，如政治和市场营销。</li>
<li>methods: 我们提出使用值批入大型自然语言模型（LLM）预测意见和行为，并提出了值批入方法（VIM），包括对话生成和问答方法，通过细化训练将目标价值分布注入到 LLM 中。</li>
<li>results: 我们对四个任务进行了一系列实验，发现使用值批入 LLM substantially 超过基eline，同时也发现使用值批入 LLM 可以更好地预测人们的意见和行为。<details>
<summary>Abstract</summary>
Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.
</details>
<details>
<summary>摘要</summary>
可以预测人们对问题和行为的意见在现实场景中是有帮助的，例如在政治和市场营销等领域。然而，进行大规模的民意调查，如欧洲社会调查，以获取人们对个别问题的意见可能会付出昂贵的代价。我们建议使用核心人类价值的影响于个人决策和行为的先前研究，并使用价值插入大语言模型（LLM）来预测意见和行为。为此，我们提出了价值插入方法（VIM），包括两种方法——论点生成和问答——用于在 LL M 中插入目标价值分布。我们then进行了四个任务的 série of experiments 来测试 VIM 的效果和使用价值插入 LL M 来预测人们的意见和行为的可能性。我们发现，使用 VIM 对 LL M 进行 fine-tuning 后，其表现substantially outperform baseline。此外，结果还表明，使用价值插入 LL M 可以更好地预测人们的意见和行为， чем基eline Approaches。
</details></li>
</ul>
<hr>
<h2 id="SQLformer-Deep-Auto-Regressive-Query-Graph-Generation-for-Text-to-SQL-Translation"><a href="#SQLformer-Deep-Auto-Regressive-Query-Graph-Generation-for-Text-to-SQL-Translation" class="headerlink" title="SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation"></a>SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18376">http://arxiv.org/abs/2310.18376</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adrián Bazaga, Pietro Liò, Gos Micklem</li>
<li>for: 这个论文旨在解决文本到SQL翻译 зада务中的难题，即将自然语言问题转换成可执行的SQL查询。</li>
<li>methods: 该论文提出了一种名为SQLformer的新的Transformer架构，用于实现文本到SQL翻译任务。该模型预测SQL查询为抽象 syntax tree（AST），并在核心层采用了结构卷积激活。</li>
<li>results: 对于文本到SQL Spider测试集，SQLformer显示出了最新的表现，并且在适应不同数据库和查询任务中具有良好的泛化能力。<details>
<summary>Abstract</summary>
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-the-art performance of SQLformer in the challenging text-to-SQL Spider benchmark. Our implementation is available at https://github.com/AdrianBZG/SQLformer
</details>
<details>
<summary>摘要</summary>
近年来，文本到SQL翻译技术已经受到了越来越多的关注，这是将自然语言问题转换成可执行的SQL查询的任务。这种技术可以帮助普通人从数据库中提取数据。然而，这个技术的一些关键挑战包括领域总结，即将数据库中的数据映射到自然语言中，以及自然语言问题与相应的SQL查询的对应。为了解决这些挑战，我们提出了SQLformer，一种专门为文本到SQL翻译任务设计的Transformer架构。我们的模型预测SQL查询为抽象语法树（AST），并在树和树之间具有指导性的结构卷积。这种卷积引导了数据库表和列的选择，帮助解码器生成SQL查询AST表示为图形，并在深度优先搜索中遍历。我们的实现可以在https://github.com/AdrianBZG/SQLformer上获取。Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form instead.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/27/cs.CL_2023_10_27/" data-id="clogxf3ma00d15xra3nr511gu" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_10_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/27/cs.LG_2023_10_27/" class="article-date">
  <time datetime="2023-10-27T10:00:00.000Z" itemprop="datePublished">2023-10-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/27/cs.LG_2023_10_27/">cs.LG - 2023-10-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Contextual-Stochastic-Bilevel-Optimization"><a href="#Contextual-Stochastic-Bilevel-Optimization" class="headerlink" title="Contextual Stochastic Bilevel Optimization"></a>Contextual Stochastic Bilevel Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18535">http://arxiv.org/abs/2310.18535</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifan Hu, Jie Wang, Yao Xie, Andreas Krause, Daniel Kuhn</li>
<li>for: 本文提出了一种Contextual Stochastic Bilevel Optimization（CSBO）框架，用于解决 Stochastic Bilevel Optimization（SBO）中下级决策者受到上级决策者的决策以及一些副情况信息的影响。</li>
<li>methods: 本文提出了一种高效的double-loop梯度法，基于Multilevel Monte-Carlo（MLMC）技术，以解决 CSBO 中存在上级决策者的副情况信息导致的扩散问题。</li>
<li>results: 本文的方法可以在 meta-learning、personalized federated learning、end-to-end learning 和 Wasserstein distributionally robust optimization with side information（WDRO-SI）等应用中实现高效的优化。特别是在 Stochastic Nonconvex Optimization 中，我们的方法与现有的下界匹配。在数学实验中，我们的方法的复杂性不依赖于任务数量。<details>
<summary>Abstract</summary>
We introduce contextual stochastic bilevel optimization (CSBO) -- a stochastic bilevel optimization framework with the lower-level problem minimizing an expectation conditioned on some contextual information and the upper-level decision variable. This framework extends classical stochastic bilevel optimization when the lower-level decision maker responds optimally not only to the decision of the upper-level decision maker but also to some side information and when there are multiple or even infinite many followers. It captures important applications such as meta-learning, personalized federated learning, end-to-end learning, and Wasserstein distributionally robust optimization with side information (WDRO-SI). Due to the presence of contextual information, existing single-loop methods for classical stochastic bilevel optimization are unable to converge. To overcome this challenge, we introduce an efficient double-loop gradient method based on the Multilevel Monte-Carlo (MLMC) technique and establish its sample and computational complexities. When specialized to stochastic nonconvex optimization, our method matches existing lower bounds. For meta-learning, the complexity of our method does not depend on the number of tasks. Numerical experiments further validate our theoretical results.
</details>
<details>
<summary>摘要</summary>
我们介绍了 Contextual Stochastic Bilevel Optimization（CSBO）框架，这是一种带有上下文信息的随机二重优化框架，下一级决策者根据上一级决策者的决策以及一些副信息进行优化。这种框架超越了经典的随机二重优化，因为下一级决策者不仅响应上一级决策者的决策，还响应一些副信息，并且可能有多个或无数多个追随者。它涵盖了重要的应用，如meta-学习、个性化联合学习、端到端学习以及 Wasserstein Distributionally Robust Optimization with Side Information（WDRO-SI）。由于上下文信息的存在，传统的单循环方法无法收敛。为解决这个挑战，我们提出了一种高效的双循环梯度法，基于 Multilevel Monte-Carlo（MLMC）技术，并证明其样本和计算复杂度。当特化到随机非 convex 优化时，我们的方法与已有的下界匹配。对于 meta-学习，我们的方法的复杂度不виси于任务数量。实验数据也 validate 我们的理论结果。
</details></li>
</ul>
<hr>
<h2 id="Feature-Selection-in-the-Contrastive-Analysis-Setting"><a href="#Feature-Selection-in-the-Contrastive-Analysis-Setting" class="headerlink" title="Feature Selection in the Contrastive Analysis Setting"></a>Feature Selection in the Contrastive Analysis Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18531">http://arxiv.org/abs/2310.18531</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ethan Weinberger, Ian Covert, Su-In Lee</li>
<li>for: 本研究旨在提出一种特点适用于对比分析（CA）设置的特征选择方法（CFS），以便在生物医学数据分析中实现更好的特征选择。</li>
<li>methods: 本研究使用了一种新的信息论分析方法来驱动CFS方法的设计，并对四个实际生物医学数据集进行了实验验证。</li>
<li>results: 实验结果表明，CFS方法在四个数据集中均可以比前所有的超参数和无监督特征选择方法表现更好，并且可以更好地捕捉到CA设置中的特征差异。<details>
<summary>Abstract</summary>
Contrastive analysis (CA) refers to the exploration of variations uniquely enriched in a target dataset as compared to a corresponding background dataset generated from sources of variation that are irrelevant to a given task. For example, a biomedical data analyst may wish to find a small set of genes to use as a proxy for variations in genomic data only present among patients with a given disease (target) as opposed to healthy control subjects (background). However, as of yet the problem of feature selection in the CA setting has received little attention from the machine learning community. In this work we present contrastive feature selection (CFS), a method for performing feature selection in the CA setting. We motivate our approach with a novel information-theoretic analysis of representation learning in the CA setting, and we empirically validate CFS on a semi-synthetic dataset and four real-world biomedical datasets. We find that our method consistently outperforms previously proposed state-of-the-art supervised and fully unsupervised feature selection methods not designed for the CA setting. An open-source implementation of our method is available at https://github.com/suinleelab/CFS.
</details>
<details>
<summary>摘要</summary>
contrastive analysis (CA) 指的是对target数据集中具有特点的变化进行探索，相比较background数据集中的无关变化。例如，生物医学数据分析员可能想要找到一小组基因作为病人群体疾病（target）中唯一存在的变化表示，而不是健康控制Subject（background）。然而，当前的CA设定中的特征选择问题尚未得到机器学习社区的足够关注。在这篇文章中，我们提出了对CA设定进行特征选择的方法——对比特征选择（CFS）。我们通过对CA设定的信息理论分析来证明我们的方法，并在一个半 sintetic数据集和四个实际生物医学数据集上进行了验证。我们发现，我们的方法常常超越了已有的supervised和完全无监督特征选择方法，不是CA设定。我们的实现可以在https://github.com/suinleelab/CFS上获取。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-design-protein-protein-interactions-with-enhanced-generalization"><a href="#Learning-to-design-protein-protein-interactions-with-enhanced-generalization" class="headerlink" title="Learning to design protein-protein interactions with enhanced generalization"></a>Learning to design protein-protein interactions with enhanced generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18515">http://arxiv.org/abs/2310.18515</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anton Bushuiev, Roman Bushuiev, Anatolii Filkin, Petr Kouba, Marketa Gabrielova, Michal Gabriel, Jiri Sedlar, Tomas Pluskal, Jiri Damborsky, Stanislav Mazurenko, Josef Sivic</li>
<li>for: 提高生物医学研究和开发新药的进步，发现加强蛋白质-蛋白质交互（PPI）的突变是关键。</li>
<li>methods: 使用机器学习方法，特别是SE(3)-等变征模型，以实现大规模学习和泛化。</li>
<li>results: 提出了PPIRef数据集，这是世界上最大、非重复的蛋白质-蛋白质交互数据集，并使用PPIRef数据集预训练PPIformer模型，并通过调整预训练损失函数来预测蛋白质-蛋白质交互突变的效果。最终，通过比较其他现有的状态之最好方法，提高了新的PPIformer方法的泛化性。<details>
<summary>Abstract</summary>
Discovering mutations enhancing protein-protein interactions (PPIs) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct PPIRef, the largest and non-redundant dataset of 3D protein-protein interactions, enabling effective large-scale learning. Second, we leverage the PPIRef dataset to pre-train PPIformer, a new SE(3)-equivariant model generalizing across diverse protein-binder variants. We fine-tune PPIformer to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new PPIformer approach by outperforming other state-of-the-art methods on new, non-leaking splits of standard labeled PPI mutational data and independent case studies optimizing a human antibody against SARS-CoV-2 and increasing the thrombolytic activity of staphylokinase.
</details>
<details>
<summary>摘要</summary>
发现加强蛋白蛋白交互（PPI）的突变可以推动生物医学研究和开发改进的药物。虽然机器学习方法在这个领域已经做出了很大的进步，但它们经常在实际应用场景中难以泛化。这个研究的贡献有三个方面：1. 我们构建了3D蛋白蛋白交互的最大和非重复的数据集PPIRef，使得大规模学习成为可能。2. 我们利用PPIRef数据集来预训练PPIformer，一种新的SE(3)-可变型模型，可以在不同的蛋白蛋白绑定变体中进行广泛的泛化。3. 我们利用PPIformer模型来预测蛋白蛋白交互中突变的影响，通过在预训练损失函数中做一种热力学激活的调整。最后，我们表明了我们的新PPIformer方法在新的非泄漏分组的标注的PPI突变数据集和独立的案例研究中表现出了更高的泛化能力，比如人类抗体 against SARS-CoV-2和提高破坏酶的溶解活性。
</details></li>
</ul>
<hr>
<h2 id="Preventing-Language-Models-From-Hiding-Their-Reasoning"><a href="#Preventing-Language-Models-From-Hiding-Their-Reasoning" class="headerlink" title="Preventing Language Models From Hiding Their Reasoning"></a>Preventing Language Models From Hiding Their Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18512">http://arxiv.org/abs/2310.18512</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/redwoodresearch/text-steganography-benchmark">https://github.com/redwoodresearch/text-steganography-benchmark</a></li>
<li>paper_authors: Fabien Roger, Ryan Greenblatt</li>
<li>for: 本研究探讨了大型自然语言模型（LLM）在解决复杂问题时是否会使用隐藏的推理步骤，以及这些推理步骤是否会被模型所理解。</li>
<li>methods: 本研究使用了一种叫做“编码推理”的方法，通过对模型生成的文本进行分析，检测出模型是否在使用隐藏的推理步骤来解决问题。</li>
<li>results: 研究发现，当 LLM 强度增加时，它们更容易使用编码推理来解决问题，但这些推理步骤可能不可读明白于人类读者。此外，研究还提出了一种方法来评估防御机制，并证明在某些条件下，重叠 rewrite 可以成功防止模型编码推理。<details>
<summary>Abstract</summary>
Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）经常从中间步骤的理解来生成复杂问题的答案。当这些中间步骤的理解用于监测模型的活动时，则必须确保这些Explicit reasoning是 faithful，即模型真正正在理解什么。在这项工作中，我们关注一种可能的不忠的中间步骤理解情况：编码理解，其中一个LLM可能将中间步骤的理解编码到生成的文本中，以至于人类读者无法理解。我们证明了语言模型可以通过编码理解来提高性能，而无需用户理解中间步骤的理解。我们还 argues that，随着语言模型的强大化，这种行为越来越容易出现。最后，我们描述了一种方法ология，用于评估防御机制 против编码理解，并显示，在合适的条件下，重叠成功地防止了我们构建的最佳编码方案中更多于3 bits的信息。
</details></li>
</ul>
<hr>
<h2 id="Multi-fidelity-Design-of-Porous-Microstructures-for-Thermofluidic-Applications"><a href="#Multi-fidelity-Design-of-Porous-Microstructures-for-Thermofluidic-Applications" class="headerlink" title="Multi-fidelity Design of Porous Microstructures for Thermofluidic Applications"></a>Multi-fidelity Design of Porous Microstructures for Thermofluidic Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18505">http://arxiv.org/abs/2310.18505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonathan Tammer Eweis-LaBolle, Chuanning Zhao, Yoonjin Won, Ramin Bostanabad</li>
<li>for: 这 paper 的目的是为了设计最佳的热管理解决方案，以满足现代电子设备的高效热管理需求。</li>
<li>methods: 这 paper 使用了数据驱动的方法，利用特征函数（SDFs）来编码设计空间，并通过多元模拟和优化算法来找到最佳的热管理解决方案。</li>
<li>results: 这 paper 的结果显示，使用这种数据驱动的方法可以快速和有效地找到最佳的热管理解决方案，并且可以满足现代电子设备的高效热管理需求。<details>
<summary>Abstract</summary>
As modern electronic devices are increasingly miniaturized and integrated, their performance relies more heavily on effective thermal management. Two-phase cooling methods enhanced by porous surfaces, which capitalize on thin-film evaporation atop structured porous surfaces, are emerging as potential solutions. In such porous structures, the optimum heat dissipation capacity relies on two competing objectives that depend on mass and heat transfer. The computational costs of evaluating these objectives, the high dimensionality of the design space which a voxelated microstructure representation, and the manufacturability constraints hinder the optimization process for thermal management. We address these challenges by developing a data-driven framework for designing optimal porous microstructures for cooling applications. In our framework we leverage spectral density functions (SDFs) to encode the design space via a handful of interpretable variables and, in turn, efficiently search it. We develop physics-based formulas to quantify the thermofluidic properties and feasibility of candidate designs via offline simulations. To decrease the reliance on expensive simulations, we generate multi-fidelity data and build emulators to find Pareto-optimal designs. We apply our approach to a canonical problem on evaporator wick design and obtain fin-like topologies in the optimal microstructures which are also characteristics often observed in industrial applications.
</details>
<details>
<summary>摘要</summary>
现代电子设备逐渐减小和集成，其性能受到有效热管理的依赖程度加大。两相冷却方法，通过使用结构化孔隙表面增强薄膜蒸发，被视为可能的解决方案。在such porous structures中，最佳热耗抑制 capacitance rely on two competing objectives that depend on mass and heat transfer。计算这些目标的成本，高维度的设计空间的computational cost，以及制造性能约束，使得优化过程受到阻碍。我们解决这些挑战 by developing a data-driven framework for designing optimal porous microstructures for cooling applications。在我们的框架中，我们利用spectral density functions (SDFs)来编码设计空间，通过一些可读性好的变量来快速搜索。我们开发了物理学基于的方程来评估候选设计的热流体性能和可行性，并通过offline simulations来验证。为了减少成本expensive simulations，我们生成多 fideltysimulation data和建立模型来找到Pareto优化的设计。我们在一个标准的蒸发器柱状结构设计问题中应用我们的方法，并获得了fin-like topologies的优化结构，这些结构也经常出现在工业应用中。
</details></li>
</ul>
<hr>
<h2 id="Understanding-and-Improving-Ensemble-Adversarial-Defense"><a href="#Understanding-and-Improving-Ensemble-Adversarial-Defense" class="headerlink" title="Understanding and Improving Ensemble Adversarial Defense"></a>Understanding and Improving Ensemble Adversarial Defense</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18477">http://arxiv.org/abs/2310.18477</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xqsi/igat">https://github.com/xqsi/igat</a></li>
<li>paper_authors: Yian Deng, Tingting Mu</li>
<li>for: 这篇论文旨在解释 ensemble adversarial defense 的理论基础，以及一种新的实现方法来提高 ensemble adversarial defense 的性能。</li>
<li>methods: 该论文使用了一种新的错误理论来解释 ensemble adversarial defense 的效果，并提出了一种名为 interactive global adversarial training (iGAT) 的新方法来提高 ensemble adversarial defense 的性能。</li>
<li>results: 根据实验结果，iGAT 可以在 CIFAR10 和 CIFAR100  datasets 下提高 ensemble adversarial defense 的性能，最高提升达 17%，并在 white-box 和 black-box 攻击下都有显著的提升。<details>
<summary>Abstract</summary>
The strategy of ensemble has become popular in adversarial defense, which trains multiple base classifiers to defend against adversarial attacks in a cooperative manner. Despite the empirical success, theoretical explanations on why an ensemble of adversarially trained classifiers is more robust than single ones remain unclear. To fill in this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense, demonstrating a provable 0-1 loss reduction on challenging sample sets in an adversarial defense scenario. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates to different base classifiers adversarial examples that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. Being tested over various existing ensemble adversarial defense techniques, iGAT is capable of boosting their performance by increases up to 17% evaluated using CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks.
</details>
<details>
<summary>摘要</summary>
《 ensemble 策略在对抗攻击方面变得受欢迎，它在多个基础分类器之间进行协作来防止对抗攻击。 DESPITE 这种策略的实际成功，对 ensemble 对抗防御的理论解释仍然不清楚。 To fill this gap, we develop a new error theory dedicated to understanding ensemble adversarial defense, demonstrating a provable 0-1 loss reduction on challenging sample sets in an adversarial defense scenario. Guided by this theory, we propose an effective approach to improve ensemble adversarial defense, named interactive global adversarial training (iGAT). The proposal includes (1) a probabilistic distributing rule that selectively allocates adversarial examples to different base classifiers that are globally challenging to the ensemble, and (2) a regularization term to rescue the severest weaknesses of the base classifiers. tested over various existing ensemble adversarial defense techniques, iGAT is capable of boosting their performance by increases up to 17% evaluated using CIFAR10 and CIFAR100 datasets under both white-box and black-box attacks.》Note: Please note that the translation is in Simplified Chinese, and the formatting of the text may be different from the original English version.
</details></li>
</ul>
<hr>
<h2 id="Parameter-Efficient-Methods-for-Metastases-Detection-from-Clinical-Notes"><a href="#Parameter-Efficient-Methods-for-Metastases-Detection-from-Clinical-Notes" class="headerlink" title="Parameter-Efficient Methods for Metastases Detection from Clinical Notes"></a>Parameter-Efficient Methods for Metastases Detection from Clinical Notes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18472">http://arxiv.org/abs/2310.18472</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maede Ashofteh Barabadi, Xiaodan Zhu, Wai Yip Chan, Amber L. Simpson, Richard K. G. Do</li>
<li>for: 这项研究旨在自动检测基于 computed tomography（CT）辐射报告的肝肿瘤疾病进展。</li>
<li>methods: 我们使用三种方法提高模型性能，包括使用通用语言模型（LM），自动标注大量无标注数据集，以及多任务转移学习。</li>
<li>results: 我们的最佳模型在 F1 分数上达到 73.8%，准确率为 84%，并 recall 为 65.8%。<details>
<summary>Abstract</summary>
Understanding the progression of cancer is crucial for defining treatments for patients. The objective of this study is to automate the detection of metastatic liver disease from free-style computed tomography (CT) radiology reports. Our research demonstrates that transferring knowledge using three approaches can improve model performance. First, we utilize generic language models (LMs), pretrained in a self-supervised manner. Second, we use a semi-supervised approach to train our model by automatically annotating a large unlabeled dataset; this approach substantially enhances the model's performance. Finally, we transfer knowledge from related tasks by designing a multi-task transfer learning methodology. We leverage the recent advancement of parameter-efficient LM adaptation strategies to improve performance and training efficiency. Our dataset consists of CT reports collected at Memorial Sloan Kettering Cancer Center (MSKCC) over the course of 12 years. 2,641 reports were manually annotated by domain experts; among them, 841 reports have been annotated for the presence of liver metastases. Our best model achieved an F1-score of 73.8%, a precision of 84%, and a recall of 65.8%.
</details>
<details>
<summary>摘要</summary>
理解肿瘤的进程对于定义患者的治疗是非常重要。本研究的目标是自动从自由式 computed tomography（CT） radiology report中检测到肝肿瘤病变。我们的研究表明，通过三种方法可以提高模型的性能。首先，我们使用通用语言模型（LM），先前在自我超vised的方式进行预训练。第二，我们使用自动注释大量未标注数据集来培训我们的模型，这种方法显著提高了模型的性能。最后，我们利用相关任务的知识传递来设计多任务转移学习方法。我们利用了最近的参数效率LM参照扩展策略，以提高性能和训练效率。我们的数据集包括 Memorial Sloan Kettering Cancer Center（MSKCC）在12年期间收集的CT报告2,641份，其中841份已经被培训出版物的培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培训出版物培�
</details></li>
</ul>
<hr>
<h2 id="Minimax-Optimal-Submodular-Optimization-with-Bandit-Feedback"><a href="#Minimax-Optimal-Submodular-Optimization-with-Bandit-Feedback" class="headerlink" title="Minimax Optimal Submodular Optimization with Bandit Feedback"></a>Minimax Optimal Submodular Optimization with Bandit Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18465">http://arxiv.org/abs/2310.18465</a></li>
<li>repo_url: None</li>
<li>paper_authors: Artin Tajdini, Lalit Jain, Kevin Jamieson</li>
<li>for: 本文研究了在测量不确定随机预测器下实现最大化一个单调增长的 monotonic 集合函数 $f$ 的问题。</li>
<li>methods: 本文使用了一个新的算法，可以对 $f$ 的不确定性进行最佳化，并且可以在 $T$ 次循环中的每一次选择一个大小不超过 $k$ 的集合，以获得最大化 $f$ 的优化。</li>
<li>results: 本文获得了一个新的下界 bound，该 bound 表示在 $T$ 次循环中， learner 的 regret 将是 $\mathcal{O}(\min_{i \le k}(in^{1&#x2F;3}T^{2&#x2F;3} + \sqrt{n^{k-i}T}))$。此外，本文还提出了一个可以对下界 bound 做出匹配的算法。<details>
<summary>Abstract</summary>
We consider maximizing a monotonic, submodular set function $f: 2^{[n]} \rightarrow [0,1]$ under stochastic bandit feedback. Specifically, $f$ is unknown to the learner but at each time $t=1,\dots,T$ the learner chooses a set $S_t \subset [n]$ with $|S_t| \leq k$ and receives reward $f(S_t) + \eta_t$ where $\eta_t$ is mean-zero sub-Gaussian noise. The objective is to minimize the learner's regret over $T$ times with respect to ($1-e^{-1}$)-approximation of maximum $f(S_*)$ with $|S_*| = k$, obtained through greedy maximization of $f$. To date, the best regret bound in the literature scales as $k n^{1/3} T^{2/3}$. And by trivially treating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$ is also achievable. In this work, we establish the first minimax lower bound for this setting that scales like $\mathcal{O}(\min_{i \le k}(in^{1/3}T^{2/3} + \sqrt{n^{k-i}T}))$. Moreover, we propose an algorithm that is capable of matching the lower bound regret.
</details>
<details>
<summary>摘要</summary>
我们考虑最大化一个单调、下调的集合函数 $f：2^{[n]} \to [0,1]$ 在数据随机弹指示下。具体来说，$f$ 是学习者不知道的，但在每个时间 $t=1,\dots,T$ 中，学习者选择一个集合 $S_t \subset [n]$ ， $|S_t| \leq k$，并获得奖励 $f(S_t) + \eta_t$，其中 $\eta_t$ 是mean-zero sub-Gaussian 噪声。学习者的目标是在 $T$ 次时间内，与 ($1-e^{-1}$) 近似最大的 $f(S_*)$ ，其中 $|S_*| = k$，通过简单的单调最大化 $f$ 而得到。现有的最差 regret bound 是 $k n^{1/3} T^{2/3}$。而通过将每个集合视为单一的枪一样，则可以得到 $\sqrt{n \choose k} T}$ 的 regret bound。在这个研究中，我们建立了首个最小最差下界，其scale如 $\mathcal{O}(\min_{i \le k}(in^{1/3}T^{2/3} + \sqrt{n^{k-i}T}))$。此外，我们也提出了一个能够匹配下界的算法。
</details></li>
</ul>
<hr>
<h2 id="Approximate-Heavy-Tails-in-Offline-Multi-Pass-Stochastic-Gradient-Descent"><a href="#Approximate-Heavy-Tails-in-Offline-Multi-Pass-Stochastic-Gradient-Descent" class="headerlink" title="Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent"></a>Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18455">http://arxiv.org/abs/2310.18455</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/krunolp/offline_ht">https://github.com/krunolp/offline_ht</a></li>
<li>paper_authors: Krunoslav Lehman Pavasovic, Alain Durmus, Umut Simsekli</li>
<li>for: 本研究探讨SGD在实际应用中可能会出现重 tailed 行为的起因，并证明了这种行为的出现是由于训练数据的finite amount导致的。</li>
<li>methods: 本研究使用了 offline SGD 和 online SGD 进行比较，并提供了 nonasymptotic Wasserstein convergence bounds 以确定这种行为的起因。</li>
<li>results: 研究发现，随着数据点数量的增加，offline SGD 会变得越来越 “power-law-like”，而 online SGD 则会保持不变。此外，研究还在 synthetic data 和神经网络上进行了实验来证明理论结论。<details>
<summary>Abstract</summary>
A recent line of empirical studies has demonstrated that SGD might exhibit a heavy-tailed behavior in practical settings, and the heaviness of the tails might correlate with the overall performance. In this paper, we investigate the emergence of such heavy tails. Previous works on this problem only considered, up to our knowledge, online (also called single-pass) SGD, in which the emergence of heavy tails in theoretical findings is contingent upon access to an infinite amount of data. Hence, the underlying mechanism generating the reported heavy-tailed behavior in practical settings, where the amount of training data is finite, is still not well-understood. Our contribution aims to fill this gap. In particular, we show that the stationary distribution of offline (also called multi-pass) SGD exhibits 'approximate' power-law tails and the approximation error is controlled by how fast the empirical distribution of the training data converges to the true underlying data distribution in the Wasserstein metric. Our main takeaway is that, as the number of data points increases, offline SGD will behave increasingly 'power-law-like'. To achieve this result, we first prove nonasymptotic Wasserstein convergence bounds for offline SGD to online SGD as the number of data points increases, which can be interesting on their own. Finally, we illustrate our theory on various experiments conducted on synthetic data and neural networks.
</details>
<details>
<summary>摘要</summary>
现在的一些实验研究表明，SGD可能在实际情况下具有重尾性行为，并且这种重尾性与总性能之间存在相关性。在这篇论文中，我们探究了这种重尾性的出现。先前的研究只考虑了在线（也称为单通道）SGD，其中证明重尾性的存在需要训练数据的无穷多余。因此，在实际设置中，其下面机制仍然不很清楚。我们的贡献是填充这个空白。具体来说，我们表明了离线（也称为多通道）SGD的站点分布 exhibits 'approximate' 的力学尾部，并且这种预测错误是通过如何让Empirical distribution of training data converge to the true underlying data distribution在 Wasserstein 度量下控制的。我们的主要结论是，随着数据点数量增加，离线 SGD 会逐渐具有 'power-law-like' 的行为。为了实现这个结论，我们首先证明了离线 SGD 与在线 SGD 的非尺寸 Wasserstein 准确性 bound，这可能是一个独立的兴趣点。最后，我们在synthetic data和神经网络上进行了多个实验来证明我们的理论。
</details></li>
</ul>
<hr>
<h2 id="Bayesian-Optimization-with-Hidden-Constraints-via-Latent-Decision-Models"><a href="#Bayesian-Optimization-with-Hidden-Constraints-via-Latent-Decision-Models" class="headerlink" title="Bayesian Optimization with Hidden Constraints via Latent Decision Models"></a>Bayesian Optimization with Hidden Constraints via Latent Decision Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18449">http://arxiv.org/abs/2310.18449</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqian Xing, Jungho Lee, Chong Liu, Shixiang Zhu</li>
<li>for: 这篇论文旨在提出一种基于潜在约束的隐藏约束空间极化搜索（HC-LSBO）方法，用于解决公共政策领域的复杂决策问题，如警区设置。</li>
<li>methods: 该方法首先使用变量自动编码器学习原始决策空间中的可行约束分布，从而实现了原始空间和隐藏空间之间的双向映射。然后，HC-LSBO使用这种隐藏空间映射来优化决策，同时评估目标函数在原始空间中。</li>
<li>results: 我们通过数值实验表示，HC-LSBO在 synthetic 和实际数据集上表现出色，特别是在大规模警区设置问题上。与基线方法相比，HC-LSBO 提供了显著的性能和效率改善。<details>
<summary>Abstract</summary>
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scale police districting problems in Atlanta, Georgia. Our results reveal that HC-LSBO offers notable improvements in performance and efficiency compared to the baselines.
</details>
<details>
<summary>摘要</summary>
bayesian 优化（BO）已成为复杂决策挑战的强大工具，尤其在公共政策领域such as 警区划分。然而，它在公共政策决策中的更广泛应用受到定义可行区域的复杂性和决策的高维度所阻碍。这篇文章介绍了隐藏的约束 latent Space Bayesian 优化（HC-LSBO），一种 integrate Bayesian 优化方法和秘密决策模型。这种方法利用一种变换自动编码器来学习原始决策空间中的可行分布，从而实现原始空间和隐藏空间之间的两个方向的映射。由此，HC-LSBO 捕捉了公共政策中隐藏的约束，允许在隐藏空间进行优化而不影响原始空间中的目标评价。我们通过对 sintetic 和实际数据集进行数学实验，发现HC-LSBO 对基eline 提供了显著的性能和效率提升。
</details></li>
</ul>
<hr>
<h2 id="M3C-A-Framework-towards-Convergent-Flexible-and-Unsupervised-Learning-of-Mixture-Graph-Matching-and-Clustering"><a href="#M3C-A-Framework-towards-Convergent-Flexible-and-Unsupervised-Learning-of-Mixture-Graph-Matching-and-Clustering" class="headerlink" title="M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering"></a>M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18444">http://arxiv.org/abs/2310.18444</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Lu, Zetian Jiang, Tianzhe Wang, Junchi Yan</li>
<li>for: 这篇论文targets real-world graph matching and clustering tasks, where graphs exhibit diverse modes and require grouping before or along with matching.</li>
<li>methods: 该方法基于Minorize-Maximization框架，提供了学习自由的 guarantee of theoretical convergence, along with relaxed clustering for enhanced flexibility.</li>
<li>results: 实验结果表明，该方法在公共benchmark上的准确率和效率都高于现状的graph matching和mixture graph matching和分 clustering方法。Here’s the English version for reference:</li>
<li>for: This paper targets real-world graph matching and clustering tasks, where graphs exhibit diverse modes and require grouping before or along with matching.</li>
<li>methods: The method is based on the Minorize-Maximization framework, providing learning-free guarantees of theoretical convergence, along with relaxed clustering for enhanced flexibility.</li>
<li>results: Experimental results demonstrate that our method outperforms state-of-the-art graph matching and mixture graph matching and clustering approaches in both accuracy and efficiency on public benchmarks.<details>
<summary>Abstract</summary>
Existing graph matching methods typically assume that there are similar structures between graphs and they are matchable. However, these assumptions do not align with real-world applications. This work addresses a more realistic scenario where graphs exhibit diverse modes, requiring graph grouping before or along with matching, a task termed mixture graph matching and clustering. We introduce Minorize-Maximization Matching and Clustering (M3C), a learning-free algorithm that guarantees theoretical convergence through the Minorize-Maximization framework and offers enhanced flexibility via relaxed clustering. Building on M3C, we develop UM3C, an unsupervised model that incorporates novel edge-wise affinity learning and pseudo label selection. Extensive experimental results on public benchmarks demonstrate that our method outperforms state-of-the-art graph matching and mixture graph matching and clustering approaches in both accuracy and efficiency. Source code will be made publicly available.
</details>
<details>
<summary>摘要</summary>
现有的图匹配方法通常假设图有相似结构，可以匹配。然而，这些假设与实际应用场景不符。本工作面临现实世界中图表现多种模式的问题，需要在匹配之前或同时进行图分组，一种被称为杂合图匹配和分群。我们介绍了一种不含学习的算法，名为小于最大化匹配和分群（M3C），该算法 garantías了理论上的收敛，并提供了放宽分群的灵活性。基于M3C，我们开发了一种无监督的模型，名为UM3C，它包括新的边绑定学习和 Pseudo标签选择。经过广泛的实验研究，我们发现OUR方法在公共测试 benchmark上比现有的图匹配和杂合图匹配和分群方法更高效和更准确。代码将在公共平台上发布。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Distributionally-Robust-Learning-and-Offline-RL-An-Approach-to-Mitigate-Distribution-Shift-and-Partial-Data-Coverage"><a href="#Bridging-Distributionally-Robust-Learning-and-Offline-RL-An-Approach-to-Mitigate-Distribution-Shift-and-Partial-Data-Coverage" class="headerlink" title="Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage"></a>Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18434">http://arxiv.org/abs/2310.18434</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zaiyan-x/drqi">https://github.com/zaiyan-x/drqi</a></li>
<li>paper_authors: Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, Mohammad Ghavamzadeh</li>
<li>for: 本研究的目的是开发一种基于历史数据的离线强化学习（RL）算法，以便学习最优策略，无需在线上探索环境。</li>
<li>methods: 本研究使用了分布 robust学习（DRL）框架，通过最小最大形式来处理模型差异问题。</li>
<li>results: 我们的提议算法在实验中表现出优于现有算法，并且我们通过对单个策略强度的假设进行Characterization sample complexity。<details>
<summary>Abstract</summary>
The goal of an offline reinforcement learning (RL) algorithm is to learn optimal polices using historical (offline) data, without access to the environment for online exploration. One of the main challenges in offline RL is the distribution shift which refers to the difference between the state-action visitation distribution of the data generating policy and the learning policy. Many recent works have used the idea of pessimism for developing offline RL algorithms and characterizing their sample complexity under a relatively weak assumption of single policy concentrability. Different from the offline RL literature, the area of distributionally robust learning (DRL) offers a principled framework that uses a minimax formulation to tackle model mismatch between training and testing environments. In this work, we aim to bridge these two areas by showing that the DRL approach can be used to tackle the distributional shift problem in offline RL. In particular, we propose two offline RL algorithms using the DRL framework, for the tabular and linear function approximation settings, and characterize their sample complexity under the single policy concentrability assumption. We also demonstrate the superior performance our proposed algorithm through simulation experiments.
</details>
<details>
<summary>摘要</summary>
文本：offline reinforcement learning（RL）算法的目标是使用历史数据学习优化策略，不能直接访问环境进行在线探索。offline RL中的一个主要挑战是分布转移，即数据生成策略对应的状态动作访问分布与学习策略之间的差异。许多最近的研究使用了偏见的想法开发了offline RL算法，并对其样本复杂性进行了定量化。与offline RL文献不同，分布robust学习（DRL）领域提供了一个理性的框架，使用最小最大形式来处理训练和测试环境之间的模型差异。本文想要将这两个领域相连，我们表明了DRL框架可以解决offline RL中的分布转移问题。特别是，我们提出了两种使用DRL框架的offline RL算法，一种是 для tabular设置，另一种是 для线性函数近似设置，并对其样本复杂性进行了定量化。我们还通过实验证明了我们的提出算法的优秀性。Translation:文本：The goal of an offline reinforcement learning (RL) algorithm is to learn optimal policies using historical (offline) data, without access to the environment for online exploration. One of the main challenges in offline RL is the distribution shift, which refers to the difference between the state-action visitation distribution of the data generating policy and the learning policy. Many recent works have used the idea of pessimism for developing offline RL algorithms and characterizing their sample complexity under a relatively weak assumption of single policy concentrability. Different from the offline RL literature, the area of distributionally robust learning (DRL) offers a principled framework that uses a minimax formulation to tackle model mismatch between training and testing environments. In this work, we aim to bridge these two areas by showing that the DRL approach can be used to tackle the distributional shift problem in offline RL. In particular, we propose two offline RL algorithms using the DRL framework, for the tabular and linear function approximation settings, and characterize their sample complexity under the single policy concentrability assumption. We also demonstrate the superior performance of our proposed algorithm through simulation experiments.
</details></li>
</ul>
<hr>
<h2 id="MCRAGE-Synthetic-Healthcare-Data-for-Fairness"><a href="#MCRAGE-Synthetic-Healthcare-Data-for-Fairness" class="headerlink" title="MCRAGE: Synthetic Healthcare Data for Fairness"></a>MCRAGE: Synthetic Healthcare Data for Fairness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18430">http://arxiv.org/abs/2310.18430</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keira Behal, Jiayi Chen, Caleb Fikes, Sophia Xiao</li>
<li>for: 这个论文是为了解决医疗健康数据集中的敏感属性异质性问题，以便通过开发机器学习模型来提高医疗资源管理和疾病诊断和治疗的准确率。</li>
<li>methods: 这个论文提出了一种新的方法 called Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE)，它利用一种深度生成模型来生成高质量的人工数据样本，以增强原有的异质数据集，从而实现更好的数据均衡。</li>
<li>results: 作者们通过对MCRAGE方法与其他方法进行比较，发现MCRAGE方法可以提高医疗机器学习模型的准确率和F1分数，并且可以减少对不同性别和年龄等敏感属性的偏见。<details>
<summary>Abstract</summary>
In the field of healthcare, electronic health records (EHR) serve as crucial training data for developing machine learning models for diagnosis, treatment, and the management of healthcare resources. However, medical datasets are often imbalanced in terms of sensitive attributes such as race/ethnicity, gender, and age. Machine learning models trained on class-imbalanced EHR datasets perform significantly worse in deployment for individuals of the minority classes compared to samples from majority classes, which may lead to inequitable healthcare outcomes for minority groups. To address this challenge, we propose Minority Class Rebalancing through Augmentation by Generative modeling (MCRAGE), a novel approach to augment imbalanced datasets using samples generated by a deep generative model. The MCRAGE process involves training a Conditional Denoising Diffusion Probabilistic Model (CDDPM) capable of generating high-quality synthetic EHR samples from underrepresented classes. We use this synthetic data to augment the existing imbalanced dataset, thereby achieving a more balanced distribution across all classes, which can be used to train an unbiased machine learning model. We measure the performance of MCRAGE versus alternative approaches using Accuracy, F1 score and AUROC. We provide theoretical justification for our method in terms of recent convergence results for DDPMs with minimal assumptions.
</details>
<details>
<summary>摘要</summary>
在医疗领域，电子医疗记录（EHR）作为重要的训练数据，用于发展 диагности学、治疗和医疗资源管理的机器学习模型。然而，医疗数据经常受到敏感属性的影响，如种族/民族、性别和年龄，这会导致机器学习模型在部署过程中对少数群体的性能下降，从而可能导致不公正的医疗结果。为解决这个挑战，我们提出了少数类重新平衡通过扩充（MCRAGE），一种使用深度生成模型生成高质量的人工数据来增强不平衡的数据集的新方法。MCRAGE过程中，我们首先训练一个 Conditional Denoising Diffusion Probabilistic Model（CDDPM），可以生成来自少数类的高质量人工数据。然后，我们使用这些人工数据来扩充现有的不平衡数据集，以达到更平衡的分布，可以用于训练不偏袋机器学习模型。我们使用精度、F1分数和AUROC三个指标来衡量MCRAGE的性能和相对于其他方法的比较。此外，我们还提供了对MCRAGE方法的理论 justify，基于最近的DDPM的征untuous结果和最小的假设。
</details></li>
</ul>
<hr>
<h2 id="The-Bayesian-Stability-Zoo"><a href="#The-Bayesian-Stability-Zoo" class="headerlink" title="The Bayesian Stability Zoo"></a>The Bayesian Stability Zoo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18428">http://arxiv.org/abs/2310.18428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shay Moran, Hilla Schefler, Jonathan Shafer</li>
<li>for: 本研究旨在探讨学习理论中不同定义稳定性的等价关系。</li>
<li>methods: 本文使用了不同的定义稳定性，包括相对隐私、纯隐私、复制性、全面稳定性、完美泛化、TV稳定性、信息稳定性和KL散度稳定性等。</li>
<li>results: 本文证明了这些定义稳定性之间存在等价关系，并证明了一些扩充稳定性的结果，以便更好地理解和掌握最近几年出现的多种稳定性概念。<details>
<summary>Abstract</summary>
We show that many definitions of stability found in the learning theory literature are equivalent to one another. We distinguish between two families of definitions of stability: distribution-dependent and distribution-independent Bayesian stability. Within each family, we establish equivalences between various definitions, encompassing approximate differential privacy, pure differential privacy, replicability, global stability, perfect generalization, TV stability, mutual information stability, KL-divergence stability, and R\'enyi-divergence stability. Along the way, we prove boosting results that enable the amplification of the stability of a learning rule. This work is a step towards a more systematic taxonomy of stability notions in learning theory, which can promote clarity and an improved understanding of an array of stability concepts that have emerged in recent years.
</details>
<details>
<summary>摘要</summary>
我们证明了学习理论中多种稳定性定义之间存在等价关系。我们将稳定性定义分为两类：受分布影响的和不受分布影响的极 bayesian稳定性。每个家族中，我们证明了各种定义之间的等价关系，包括近似隐私、纯隐私、复制性、全局稳定性、完美泛化、TV稳定性、信息稳定性和KL散度稳定性。在过程中，我们证明了增强结果，使得稳定性的学习规则得到扩大。这项工作是一步 towards 更系统化的学习理论中稳定性概念的分类，可以促进清晰和学习许多年来出现的稳定性概念的更好的理解。
</details></li>
</ul>
<hr>
<h2 id="Fast-Machine-Learning-Method-with-Vector-Embedding-on-Orthonormal-Basis-and-Spectral-Transform"><a href="#Fast-Machine-Learning-Method-with-Vector-Embedding-on-Orthonormal-Basis-and-Spectral-Transform" class="headerlink" title="Fast Machine Learning Method with Vector Embedding on Orthonormal Basis and Spectral Transform"></a>Fast Machine Learning Method with Vector Embedding on Orthonormal Basis and Spectral Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18424">http://arxiv.org/abs/2310.18424</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/louisyulu/veob-and-st">https://github.com/louisyulu/veob-and-st</a></li>
<li>paper_authors: Louis Yu Lu</li>
<li>for: 本文提出了一种新的快速机器学习方法，利用了两种技术：Vector Embedding on Orthonormal Basis（VEOB）和Spectral Transform（ST）。</li>
<li>methods: 本方法使用了Singular Value Decomposition（SVD）技术计算向量基和投影坐标，从而提高了嵌入空间中的距离测量，并且可以压缩数据，保留最大的协方差 projection vectors。 ST 方法则将短 vectors 序列转换为спектраль空间，通过应用Discrete Cosine Transform（DCT）和选择最重要的组件，可以简化长 vectors 序列的处理。</li>
<li>results: 本文通过word embedding、text chunk embedding和image embedding的示例，在Julia语言中实现了一个向量数据库。它还 investigate了无监督学习和监督学习，以及处理大量数据的策略。<details>
<summary>Abstract</summary>
This paper presents a novel fast machine learning method that leverages two techniques: Vector Embedding on Orthonormal Basis (VEOB) and Spectral Transform (ST). The VEOB converts the original data encoding into a vector embedding with coordinates projected onto orthonormal bases. The Singular Value Decomposition (SVD) technique is used to calculate the vector basis and projection coordinates, leading to an enhanced distance measurement in the embedding space and facilitating data compression by preserving the projection vectors associated with the largest singular values. On the other hand, ST transforms sequence of vector data into spectral space. By applying the Discrete Cosine Transform (DCT) and selecting the most significant components, it streamlines the handling of lengthy vector sequences. The paper provides examples of word embedding, text chunk embedding, and image embedding, implemented in Julia language with a vector database. It also investigates unsupervised learning and supervised learning using this method, along with strategies for handling large data volumes.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种新的快速机器学习方法，该方法利用了两种技术：向量嵌入在正交基（VEOB）和 спектраль转换（ST）。VEOB将原始数据编码转换为一个向量嵌入，其坐标被 проек到正交基上。使用SVD技术计算向量基和投影坐标，从而提高了嵌入空间中的距离测量，并且可以压缩数据，保留投影向量与最大特征值相关的 projet 矢量。一方面，ST将序列化的向量数据转换为 спектраль空间。通过应用DCT和选择最有价值的组件，可以简化长向量序列的处理。文章提供了word嵌入、文本块嵌入和图像嵌入的例子，实现在Julia语言中的一个向量数据库。它还 investigate了无监督学习和监督学习，以及处理大量数据的策略。
</details></li>
</ul>
<hr>
<h2 id="A-general-learning-scheme-for-classical-and-quantum-Ising-machines"><a href="#A-general-learning-scheme-for-classical-and-quantum-Ising-machines" class="headerlink" title="A general learning scheme for classical and quantum Ising machines"></a>A general learning scheme for classical and quantum Ising machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18411">http://arxiv.org/abs/2310.18411</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ludwig Schmid, Enrico Zardini, Davide Pastorello</li>
<li>for: 这篇论文是关于设计用于找到哈密顿基态的硬件，例如协调性Isings机和量子温化器。</li>
<li>methods: 该论文提出了一种基于Ising结构的机器学习模型，可以使用梯度下降进行高效地训练。文章提供了一种基于损失函数优化的数学特征化，其中部分导数不是直接计算而是通过Isings机本身估计。</li>
<li>results: 实验结果表明，该学习模型在训练和执行上具有新的可能性，特别是在量子领域，量子资源被用于模型的执行和训练，提供了一个有前途的量子机器学习perspective。<details>
<summary>Abstract</summary>
An Ising machine is any hardware specifically designed for finding the ground state of the Ising model. Relevant examples are coherent Ising machines and quantum annealers. In this paper, we propose a new machine learning model that is based on the Ising structure and can be efficiently trained using gradient descent. We provide a mathematical characterization of the training process, which is based upon optimizing a loss function whose partial derivatives are not explicitly calculated but estimated by the Ising machine itself. Moreover, we present some experimental results on the training and execution of the proposed learning model. These results point out new possibilities offered by Ising machines for different learning tasks. In particular, in the quantum realm, the quantum resources are used for both the execution and the training of the model, providing a promising perspective in quantum machine learning.
</details>
<details>
<summary>摘要</summary>
一种Ising机器是专门设计用于找到Ising模型的稳定状态的硬件。相关的例子包括协调Isimg machine和量子气化器。在这篇论文中，我们提出一种基于Ising结构的新的机器学习模型，可以通过梯度下降方法高效地训练。我们提供了一个数学 caracterization of the training process，该过程基于优化一个损失函数的 partial derivatives不是直接计算出来，而是由Ising机器自己估算出来。此外，我们还提供了一些实验结果，证明了我们的学习模型在不同的任务上的应用前景。特别是在量子领域，量子资源被用于模型的执行和训练，提供了一个有前途的Perspective in quantum machine learning。
</details></li>
</ul>
<hr>
<h2 id="State-Action-Similarity-Based-Representations-for-Off-Policy-Evaluation"><a href="#State-Action-Similarity-Based-Representations-for-Off-Policy-Evaluation" class="headerlink" title="State-Action Similarity-Based Representations for Off-Policy Evaluation"></a>State-Action Similarity-Based Representations for Off-Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18409">http://arxiv.org/abs/2310.18409</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Badger-RL/ROPE">https://github.com/Badger-RL/ROPE</a></li>
<li>paper_authors: Brahma S. Pavse, Josiah P. Hanna</li>
<li>for: 这篇论文主要是关于 reinforcement learning 中的 off-policy evaluation (OPE) 问题，即计算一个评估策略的预期返回值基于一个已知的数据集。</li>
<li>methods: 这篇论文提出了一种基于 fitted q-evaluation (FQE) 算法的数据效能提升方法，即首先将数据集经过一个学习的编码器处理，然后将处理后的数据集传递给 FQE 算法进行学习。以学习这个编码器，我们引入了一种特定于 OPE 的状态动作行为相似度度量，并使用这个度量和固定数据集来学习编码器。</li>
<li>results: 我们的实验结果显示，使用我们的状态动作表示方法可以提高 FQE 的数据效能，降低 OPE 错误值，并在不同的分布变换下保持 FQE 的稳定性。此外，我们还发现其他状态动作相似度度量无法表示评估策略的动作值函数，而我们的状态动作表示方法可以减少 FQE 中的数据误差。<details>
<summary>Abstract</summary>
In reinforcement learning, off-policy evaluation (OPE) is the problem of estimating the expected return of an evaluation policy given a fixed dataset that was collected by running one or more different policies. One of the more empirically successful algorithms for OPE has been the fitted q-evaluation (FQE) algorithm that uses temporal difference updates to learn an action-value function, which is then used to estimate the expected return of the evaluation policy. Typically, the original fixed dataset is fed directly into FQE to learn the action-value function of the evaluation policy. Instead, in this paper, we seek to enhance the data-efficiency of FQE by first transforming the fixed dataset using a learned encoder, and then feeding the transformed dataset into FQE. To learn such an encoder, we introduce an OPE-tailored state-action behavioral similarity metric, and use this metric and the fixed dataset to learn an encoder that models this metric. Theoretically, we show that this metric allows us to bound the error in the resulting OPE estimate. Empirically, we show that other state-action similarity metrics lead to representations that cannot represent the action-value function of the evaluation policy, and that our state-action representation method boosts the data-efficiency of FQE and lowers OPE error relative to other OPE-based representation learning methods on challenging OPE tasks. We also empirically show that the learned representations significantly mitigate divergence of FQE under varying distribution shifts. Our code is available here: https://github.com/Badger-RL/ROPE.
</details>
<details>
<summary>摘要</summary>
在强化学习中，评估策略（OPE）是计算评估策略所预期的返回的问题。一种更加实验成功的算法是适应q评估（FQE）算法，它使用时间差更新来学习一个动作价值函数，并使用这个函数来估计评估策略的返回。通常，原始固定数据集直接 fed 到 FQE 来学习评估策略的动作价值函数。而在这篇论文中，我们尝试使用一个学习的编码器来增强 FQE 的数据效率。我们引入一个适应 OPE 的状态动作行为相似度 metric，并使用这个 metric 和固定数据集来学习一个编码器，该编码器模型了这个 metric。理论上，我们证明这个 metric 可以约束 OPE 估计的错误。实验上，我们发现其他状态动作相似度 metric 导致的表示无法表示评估策略的动作价值函数，而我们的状态动作表示方法可以提高 FQE 的数据效率，相比其他基于 OPE 的表示学习方法。我们还发现学习的表示可以有效地减少 FQE 在不同分布下的急剧异常。我们的代码可以在以下链接中找到：https://github.com/Badger-RL/ROPE。
</details></li>
</ul>
<hr>
<h2 id="Supervised-and-Penalized-Baseline-Correction"><a href="#Supervised-and-Penalized-Baseline-Correction" class="headerlink" title="Supervised and Penalized Baseline Correction"></a>Supervised and Penalized Baseline Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18306">http://arxiv.org/abs/2310.18306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Erik Andries, Ramin Nikzad-Langerodi</li>
<li>for: 这种研究是为了提高分析和量化结果的准确性，通过利用知道的分析物质含量来改进基线修正方法。</li>
<li>methods: 这种研究使用了一类现有的基线修正方法（penalized baseline correction），并对其进行修改，以便利用知道的分析物质含量来提高预测性能。</li>
<li>results: 研究发现，利用知道的分析物质含量来修正基线可以提高分析和量化结果的准确性，并且在两个近红外数据集上都有良好的性能。<details>
<summary>Abstract</summary>
Spectroscopic measurements can show distorted spectra shapes arising from a mixture of absorbing and scattering contributions. These distortions (or baselines) often manifest themselves as non-constant offsets or low-frequency oscillations. As a result, these baselines can adversely affect analytical and quantitative results. Baseline correction is an umbrella term where one applies pre-processing methods to obtain baseline spectra (the unwanted distortions) and then remove the distortions by differencing. However, current state-of-the art baseline correction methods do not utilize analyte concentrations even if they are available, or even if they contribute significantly to the observed spectral variability. We examine a class of state-of-the-art methods (penalized baseline correction) and modify them such that they can accommodate a priori analyte concentration such that prediction can be enhanced. Performance will be access on two near infra-red data sets across both classical penalized baseline correction methods (without analyte information) and modified penalized baseline correction methods (leveraging analyte information).
</details>
<details>
<summary>摘要</summary>
We examine a class of state-of-the-art baseline correction methods (penalized baseline correction) and modify them to accommodate a priori analyte concentration information. By leveraging this information, we can enhance prediction performance on two near infra-red data sets. In comparison to classical penalized baseline correction methods (without analyte information), our modified methods demonstrate improved performance.
</details></li>
</ul>
<hr>
<h2 id="Addressing-GAN-Training-Instabilities-via-Tunable-Classification-Losses"><a href="#Addressing-GAN-Training-Instabilities-via-Tunable-Classification-Losses" class="headerlink" title="Addressing GAN Training Instabilities via Tunable Classification Losses"></a>Addressing GAN Training Instabilities via Tunable Classification Losses</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18291">http://arxiv.org/abs/2310.18291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Monica Welfert, Gowtham R. Kurri, Kyle Otstot, Lalitha Sankar</li>
<li>For: 该论文旨在提出一种基于生成对抗网络（GAN）的数据生成方法，使得生成的数据具有正式的保证。* Methods: 该论文使用类probability估计（CPE）损失函数来重新定义GAN的价值函数，并证明CPE损失GAN与$f$-GAN具有两种对应关系。此外，该论文还证明所有对称的$f$-散度都有相同的减少性。* Results: 在finite sample和模型容量下，该论文定义和获得估计和泛化错误的上下限。特别是，对于$\alpha$-GANs，该论文使用$\alpha$-损失函数，一个可调的CPE损失函数，并证明其在训练稳定性方面具有优越性。此外，该论文还引入了一种 dual-objective GAN，以解决GAN训练不稳定性问题。<details>
<summary>Abstract</summary>
Generative adversarial networks (GANs), modeled as a zero-sum game between a generator (G) and a discriminator (D), allow generating synthetic data with formal guarantees. Noting that D is a classifier, we begin by reformulating the GAN value function using class probability estimation (CPE) losses. We prove a two-way correspondence between CPE loss GANs and $f$-GANs which minimize $f$-divergences. We also show that all symmetric $f$-divergences are equivalent in convergence. In the finite sample and model capacity setting, we define and obtain bounds on estimation and generalization errors. We specialize these results to $\alpha$-GANs, defined using $\alpha$-loss, a tunable CPE loss family parametrized by $\alpha\in(0,\infty]$. We next introduce a class of dual-objective GANs to address training instabilities of GANs by modeling each player's objective using $\alpha$-loss to obtain $(\alpha_D,\alpha_G)$-GANs. We show that the resulting non-zero sum game simplifies to minimizing an $f$-divergence under appropriate conditions on $(\alpha_D,\alpha_G)$. Generalizing this dual-objective formulation using CPE losses, we define and obtain upper bounds on an appropriately defined estimation error. Finally, we highlight the value of tuning $(\alpha_D,\alpha_G)$ in alleviating training instabilities for the synthetic 2D Gaussian mixture ring as well as the large publicly available Celeb-A and LSUN Classroom image datasets.
</details>
<details>
<summary>摘要</summary>
生成敌战网络（GAN），模型为零SUM游戏中的生成器（G）和分类器（D）之间的对抗，可以生成具有正式保证的 sintetic 数据。注意到D是一个分类器，我们开始通过类型概率估计（CPE）损失函数来重新定义GAN的值函数。我们证明了CPE损失函数和$f$-GANs之间的双向对应关系，以及所有对称的$f$-散度都是相同的整合。在finite sample和模型容量设置下，我们定义和获得估计和泛化错误的上下界。我们特殊化这些结果到$\alpha$-GANs中，defined using $\alpha$-loss，一个可调CPE损失函数中的$\alpha\in(0,\infty]$.我们接着引入一个 dual-objective GAN来解决GAN的训练不稳定性，通过对每个玩家的目标使用$\alpha$-loss来获得$(\alpha_D,\alpha_G)$-GANs。我们证明了这个非零SUM游戏可以通过适当的条件来简化为$f$-散度的最小化。通过扩展这种双对象形式，我们定义和获得一个相应的估计错误的上限。最后，我们强调了在Synthetic 2D Gaussian mixture ring和大规模公共可用的Celeb-A和LSUN Classroom图像 dataset上调整($\alpha_D,\alpha_G)$的价值，以适应训练不稳定性。
</details></li>
</ul>
<hr>
<h2 id="Sustainable-Concrete-via-Bayesian-Optimization"><a href="#Sustainable-Concrete-via-Bayesian-Optimization" class="headerlink" title="Sustainable Concrete via Bayesian Optimization"></a>Sustainable Concrete via Bayesian Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18288">http://arxiv.org/abs/2310.18288</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/sustainableconcrete">https://github.com/facebookresearch/sustainableconcrete</a></li>
<li>paper_authors: Sebastian Ament, Andrew Witte, Nishant Garg, Julius Kusuma</li>
<li>for: 本研究旨在找到可持续的混凝土方程式，以减少建筑数据中心的碳排放。</li>
<li>methods: 本研究使用 Bayesian 优化法加速搜索可持续的混凝土方程式，并模拟混凝土强度使其可以准确预测。</li>
<li>results: 研究结果显示，使用开源的方法可以实现更好的 globel warming potential 和强度之间的交易，比现有业务实践更有利。<details>
<summary>Abstract</summary>
Eight percent of global carbon dioxide emissions can be attributed to the production of cement, the main component of concrete, which is also the dominant source of CO2 emissions in the construction of data centers. The discovery of lower-carbon concrete formulae is therefore of high significance for sustainability. However, experimenting with new concrete formulae is time consuming and labor intensive, as one usually has to wait to record the concrete's 28-day compressive strength, a quantity whose measurement can by its definition not be accelerated. This provides an opportunity for experimental design methodology like Bayesian Optimization (BO) to accelerate the search for strong and sustainable concrete formulae. Herein, we 1) propose modeling steps that make concrete strength amenable to be predicted accurately by a Gaussian process model with relatively few measurements, 2) formulate the search for sustainable concrete as a multi-objective optimization problem, and 3) leverage the proposed model to carry out multi-objective BO with real-world strength measurements of the algorithmically proposed mixes. Our experimental results show improved trade-offs between the mixtures' global warming potential (GWP) and their associated compressive strengths, compared to mixes based on current industry practices. Our methods are open-sourced at github.com/facebookresearch/SustainableConcrete.
</details>
<details>
<summary>摘要</summary>
全球碳排放的8%可以追溯到混凝土的生产，混凝土也是数据中心建设中主要的CO2排放来源。发现更低碳排放混凝土 формула的发现对可持续发展有着重要意义。然而，尝试新的混凝土 формула可以是时间占用和人力消耗的，因为一般需要等待28天压缩强度的测量，这个测量不能加速。这提供了 Bayesian 优化（BO）实验方法的机会，以加速搜索具有高强度和可持续的混凝土 формула。我们的方法包括：1. 模型步骤，使得混凝土强度可以准确预测，只需要 relativelly few 测量。2. 将寻找可持续的混凝土形式化为多目标优化问题。3. 利用我们提出的模型，通过实际测量 Algorithmically 提出的混凝土的强度，进行多目标 BO。我们的实验结果表明，我们的方法可以相比于现有行业实践，提高混凝土的全球温室效应（GWP）和压缩强度之间的交换。我们的方法在 GitHub 上公开发布，请参考 <https://github.com/facebookresearch/SustainableConcrete>。
</details></li>
</ul>
<hr>
<h2 id="Optimal-Transport-for-Treatment-Effect-Estimation"><a href="#Optimal-Transport-for-Treatment-Effect-Estimation" class="headerlink" title="Optimal Transport for Treatment Effect Estimation"></a>Optimal Transport for Treatment Effect Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18286">http://arxiv.org/abs/2310.18286</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hao Wang, Zhichao Chen, Jiajun Fan, Haoxuan Li, Tianqiao Liu, Weiming Liu, Quanyu Dai, Yichao Wang, Zhenhua Dong, Ruiming Tang</li>
<li>for: 估计Conditional Average Treatment Effect（CAT）从观察数据中，受到很大挑战，主要是因为存在治疗选择偏见。现有方法通过将不同治疗组的分布调整到共同的 latent space 来mitigate这个问题，但是这些方法无法解决两个主要问题：(1) mini-batch sampling effects（MSE），这会导致非理想的 mini-batch 中存在结果偏见和异常值; (2) unobserved confounder effects（UCE），这会导致不正确的差异计算，因为忽略了未观察的干扰因素。</li>
<li>methods: 我们提出了一种原则性的方法，即 Entire Space CounterFactual Regression（ESCFR），它是在 causality 框架中的新进展，基于随机优化运输的框架，我们提出了一种放宽的质量保持正则化器来解决 MSE 问题，并设计了一种距离观察结果正则化器来处理 UCE 问题。</li>
<li>results: 我们的提议的 ESCFR 可以成功地解决很多治疗选择偏见问题，并在比较state-of-the-art方法时显示出显著的优异性。<details>
<summary>Abstract</summary>
Estimating conditional average treatment effect from observational data is highly challenging due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue. Extensive experiments demonstrate that our proposed ESCFR can successfully tackle the treatment selection bias and achieve significantly better performance than state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
Estimating conditional average treatment effect from observational data is extremely difficult due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning the distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue. Extensive experiments demonstrate that our proposed ESCFR can successfully tackle the treatment selection bias and achieve significantly better performance than state-of-the-art methods.Here is the translation of the text into Traditional Chinese:Estimating conditional average treatment effect from observational data is extremely difficult due to the existence of treatment selection bias. Prevalent methods mitigate this issue by aligning the distributions of different treatment groups in the latent space. However, there are two critical problems that these methods fail to address: (1) mini-batch sampling effects (MSE), which causes misalignment in non-ideal mini-batches with outcome imbalance and outliers; (2) unobserved confounder effects (UCE), which results in inaccurate discrepancy calculation due to the neglect of unobserved confounders. To tackle these problems, we propose a principled approach named Entire Space CounterFactual Regression (ESCFR), which is a new take on optimal transport in the context of causality. Specifically, based on the framework of stochastic optimal transport, we propose a relaxed mass-preserving regularizer to address the MSE issue and design a proximal factual outcome regularizer to handle the UCE issue. Extensive experiments demonstrate that our proposed ESCFR can successfully tackle the treatment selection bias and achieve significantly better performance than state-of-the-art methods.
</details></li>
</ul>
<hr>
<h2 id="Entity-Embeddings-Perspectives-Towards-an-Omni-Modality-Era-for-Large-Language-Models"><a href="#Entity-Embeddings-Perspectives-Towards-an-Omni-Modality-Era-for-Large-Language-Models" class="headerlink" title="Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models"></a>Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18390">http://arxiv.org/abs/2310.18390</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eren Unlu, Unver Ciftci</li>
<li>for: 这个论文旨在探讨语言模型（LLMs）如何将多Modalities（如文本、图像、音频）集成到一个统一的语言空间中。</li>
<li>methods: 该论文提出了一种将概念实体定义为文本序列中的模式，并将其视为多种模式的潜在形式的方法。</li>
<li>results: 该论文预测了这种结构的潜在应用和挑战。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) are evolving to integrate multiple modalities, such as text, image, and audio into a unified linguistic space. We envision a future direction based on this framework where conceptual entities defined in sequences of text can also be imagined as modalities. Such a formulation has the potential to overcome the cognitive and computational limitations of current models. Several illustrative examples of such potential implicit modalities are given. Along with vast promises of the hypothesized structure, expected challenges are discussed as well.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在演化中将多Modalities，如文本、图像和音频 integrate into a unified linguistic space。我们可以想像将概念实体定义为文本序列中的一部分，也可以被视为模式。这种概念的形式化有很多潜在的应用前景，并且可以超越当前模型的认知和计算限制。我们给出了一些示例，以及预期的挑战。
</details></li>
</ul>
<hr>
<h2 id="Structured-Semidefinite-Programming-for-Recovering-Structured-Preconditioners"><a href="#Structured-Semidefinite-Programming-for-Recovering-Structured-Preconditioners" class="headerlink" title="Structured Semidefinite Programming for Recovering Structured Preconditioners"></a>Structured Semidefinite Programming for Recovering Structured Preconditioners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18265">http://arxiv.org/abs/2310.18265</a></li>
<li>repo_url: None</li>
<li>paper_authors: Arun Jambulapati, Jerry Li, Christopher Musco, Kirankumar Shiragur, Aaron Sidford, Kevin Tian</li>
<li>for: 这种框架用于解决线性系统的约似优化预处理问题。</li>
<li>methods: 使用新的核心矩阵方法和矩阵解决问题来解决这种问题。</li>
<li>results: 得到了改进的运行时间，比如$\widetilde{O}(\text{nnz}(\mathbf{K}) \cdot \text{poly}(\kappa^\star,\epsilon^{-1}))$和$\widetilde{O}(d^2)$。<details>
<summary>Abstract</summary>
We develop a general framework for finding approximately-optimal preconditioners for solving linear systems. Leveraging this framework we obtain improved runtimes for fundamental preconditioning and linear system solving problems including the following. We give an algorithm which, given positive definite $\mathbf{K} \in \mathbb{R}^{d \times d}$ with $\mathrm{nnz}(\mathbf{K})$ nonzero entries, computes an $\epsilon$-optimal diagonal preconditioner in time $\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$, where $\kappa^\star$ is the optimal condition number of the rescaled matrix. We give an algorithm which, given $\mathbf{M} \in \mathbb{R}^{d \times d}$ that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in $\mathbf{M}$ in $\widetilde{O}(d^2)$ time. Our diagonal preconditioning results improve state-of-the-art runtimes of $\Omega(d^{3.5})$ attained by general-purpose semidefinite programming, and our solvers improve state-of-the-art runtimes of $\Omega(d^{\omega})$ where $\omega > 2.3$ is the current matrix multiplication constant. We attain our results via new algorithms for a class of semidefinite programs (SDPs) we call matrix-dictionary approximation SDPs, which we leverage to solve an associated problem we call matrix-dictionary recovery.
</details>
<details>
<summary>摘要</summary>
我们开发了一个通用框架，用于找到近似优化的预Conditioner，以解决线性系统。利用这个框架，我们得到了改进的运行时间 для基本的预Conditioning和线性系统解决问题，包括以下几个。我们提供了一个算法， Given positive definite $\mathbf{K} \in \mathbb{R}^{d \times d}$ with $\mathrm{nnz}(\mathbf{K})$ nonzero entries, computes an $\epsilon$-optimal diagonal预Conditioner in time $\widetilde{O}(\mathrm{nnz}(\mathbf{K}) \cdot \mathrm{poly}(\kappa^\star,\epsilon^{-1}))$, where $\kappa^\star$ is the optimal condition number of the rescaled matrix.我们提供了一个算法， Given $\mathbf{M} \in \mathbb{R}^{d \times d}$ that is either the pseudoinverse of a graph Laplacian matrix or a constant spectral approximation of one, solves linear systems in $\mathbf{M}$ in $\widetilde{O}(d^2)$ time.我们的diagonal预Conditioning结果提高了现有的semidefinite programming的运行时间，从而实现了$\Omega(d^{3.5})$的性能。我们的解决方案提高了现有的运行时间，达到了$\Omega(d^{\omega})$，其中 $\omega > 2.3$ 是当前的矩阵乘法常数。我们通过新的semidefinite programs（SDPs）的算法，称为matrix-dictionary approximation SDPs，来解决一个 associate problem，称为matrix-dictionary recovery。
</details></li>
</ul>
<hr>
<h2 id="Guided-Data-Augmentation-for-Offline-Reinforcement-Learning-and-Imitation-Learning"><a href="#Guided-Data-Augmentation-for-Offline-Reinforcement-Learning-and-Imitation-Learning" class="headerlink" title="Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning"></a>Guided Data Augmentation for Offline Reinforcement Learning and Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18247">http://arxiv.org/abs/2310.18247</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicholas E. Corrado, Yuxiao Qu, John U. Balis, Adam Labiosa, Josiah P. Hanna</li>
<li>for: 学习从示例（LfD）是一种常用的机器人控制策略学习技术，但是获取专家质量的示例受限因素，如实际数据收集成本高昂，示例创造者的技能和安全问题。</li>
<li>methods: 我们提出了一种人类导航数据增强（GuDA）框架，该框架通过让用户提供一些简单的规则来自动生成专家质量的增强样本。</li>
<li>results: 我们对physical robot Soccer任务和simulated D4RL导航任务、simulated autonomous driving任务和simulated Soccer任务进行了实验，发现GuDA可以学习从一小量可能不优质的示例中，并大幅超越随机生成的DA策略。<details>
<summary>Abstract</summary>
Learning from demonstration (LfD) is a popular technique that uses expert demonstrations to learn robot control policies. However, the difficulty in acquiring expert-quality demonstrations limits the applicability of LfD methods: real-world data collection is often costly, and the quality of the demonstrations depends greatly on the demonstrator's abilities and safety concerns. A number of works have leveraged data augmentation (DA) to inexpensively generate additional demonstration data, but most DA works generate augmented data in a random fashion and ultimately produce highly suboptimal data. In this work, we propose Guided Data Augmentation (GuDA), a human-guided DA framework that generates expert-quality augmented data. The key insight of GuDA is that while it may be difficult to demonstrate the sequence of actions required to produce expert data, a user can often easily identify when an augmented trajectory segment represents task progress. Thus, the user can impose a series of simple rules on the DA process to automatically generate augmented samples that approximate expert behavior. To extract a policy from GuDA, we use off-the-shelf offline reinforcement learning and behavior cloning algorithms. We evaluate GuDA on a physical robot soccer task as well as simulated D4RL navigation tasks, a simulated autonomous driving task, and a simulated soccer task. Empirically, we find that GuDA enables learning from a small set of potentially suboptimal demonstrations and substantially outperforms a DA strategy that samples augmented data randomly.
</details>
<details>
<summary>摘要</summary>
学习示例（LfD）是一种广泛使用的技术，通过专家示例学习机器人控制策略。然而，获得专家质量示例的困难限制了LfD方法的应用范围：现实世界数据收集常常成本高昂，示例制定者的能力和安全问题具有很大的影响。许多工作使用数据扩展（DA）来生成更多的示例数据，但大多数DA工作生成扩展数据的方式是随机的，最终生成低质量的数据。在这种情况下，我们提出了指导数据扩展（GuDA），一种人类指导的DA框架，可以生成专家质量的扩展数据。GuDA的关键想法是，虽然可能困难示出完整的任务执行序列，但用户可以轻松地判断扩展 trajectory 段是否表示任务进步。因此，用户可以对 DA 过程进行一些简单的规则，自动生成扩展样本，approxime 专家行为。为提取策略，我们使用现有的离线 reinforcement learning 和行为复制算法。我们对 physical robot 足球任务、simulated D4RL 导航任务、simulated autonomous driving 任务和 simulated 足球任务 进行了评估。 empirically，我们发现 GuDA 可以从小型可能不优质的示例中学习，并substantially 超过随机扩展数据的DA策略。
</details></li>
</ul>
<hr>
<h2 id="α-Mutual-Information-A-Tunable-Privacy-Measure-for-Privacy-Protection-in-Data-Sharing"><a href="#α-Mutual-Information-A-Tunable-Privacy-Measure-for-Privacy-Protection-in-Data-Sharing" class="headerlink" title="$α$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing"></a>$α$-Mutual Information: A Tunable Privacy Measure for Privacy Protection in Data Sharing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18241">http://arxiv.org/abs/2310.18241</a></li>
<li>repo_url: None</li>
<li>paper_authors: MirHamed Jafarzadeh Asl, Mohammadhadi Shateri, Fabrice Labeau</li>
<li>for: 本研究采用了阿里玛特的α-私钥信息，在一种隐私保护的数据发布设置中，以防止泄露私人数据给敌方。</li>
<li>methods: 我们使用了一种通用的扭曲基本机制， manipulate the original data to offer privacy protection。该扭曲度量根据具体的实验数据结构进行确定。</li>
<li>results: 我们通过实验证明了α-私钥信息的适用性，并证明了我们的方法可以在不同的性能维度上妥协隐私和实用性。此外，我们还分析了攻击者获取私人数据的边 información的后果，并证明了我们的适应性比现有技术更高。<details>
<summary>Abstract</summary>
This paper adopts Arimoto's $\alpha$-Mutual Information as a tunable privacy measure, in a privacy-preserving data release setting that aims to prevent disclosing private data to adversaries. By fine-tuning the privacy metric, we demonstrate that our approach yields superior models that effectively thwart attackers across various performance dimensions. We formulate a general distortion-based mechanism that manipulates the original data to offer privacy protection. The distortion metrics are determined according to the data structure of a specific experiment. We confront the problem expressed in the formulation by employing a general adversarial deep learning framework that consists of a releaser and an adversary, trained with opposite goals. This study conducts empirical experiments on images and time-series data to verify the functionality of $\alpha$-Mutual Information. We evaluate the privacy-utility trade-off of customized models and compare them to mutual information as the baseline measure. Finally, we analyze the consequence of an attacker's access to side information about private data and witness that adapting the privacy measure results in a more refined model than the state-of-the-art in terms of resiliency against side information.
</details>
<details>
<summary>摘要</summary>
Note: The text has been translated into Simplified Chinese, which is the standard writing system used in mainland China. The translation may not be perfect, and some nuances or idioms may be lost in translation.
</details></li>
</ul>
<hr>
<h2 id="Deep-Transformed-Gaussian-Processes"><a href="#Deep-Transformed-Gaussian-Processes" class="headerlink" title="Deep Transformed Gaussian Processes"></a>Deep Transformed Gaussian Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18230">http://arxiv.org/abs/2310.18230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Toby-Wei-Liu/Mutual-Knowledge-Learning-Network">https://github.com/Toby-Wei-Liu/Mutual-Knowledge-Learning-Network</a></li>
<li>paper_authors: Sáez-Maldonado Francisco Javier, Maroñas Juan, Hernández-Lobato Daniel</li>
<li>for: 本研究旨在探讨增强 Gaussian Processes (GP) 的扩展，提高 GP 的灵活性。</li>
<li>methods: 本文提出了一种新的扩展方法，即 Deep Transformed Gaussian Processes (DTGP)，它是通过堆叠多个噪声过程层来增强 GP 的灵活性。</li>
<li>results: 实验表明，DTGP 可以在多个回归数据集中实现好的扩展性和性能。<details>
<summary>Abstract</summary>
Transformed Gaussian Processes (TGPs) are stochastic processes specified by transforming samples from the joint distribution from a prior process (typically a GP) using an invertible transformation; increasing the flexibility of the base process.   Furthermore, they achieve competitive results compared with Deep Gaussian Processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs. In this work, we propose a generalization of TGPs named Deep Transformed Gaussian Processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increment of flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations yielding a straightforward extension of the popular DSVI inference algorithm Salimbeni et al (2017). The experiments conducted evaluate the proposed novel DTGPs in multiple regression datasets, achieving good scalability and performance.
</details>
<details>
<summary>摘要</summary>
transformed  Gaussian 进程（TGP）是一种 Stochastic 过程，其 Specified by transforming samples from the joint distribution of a prior process (usually a GP) using an invertible transformation; increasing the flexibility of the base process.  Furthermore, they achieve competitive results compared with deep Gaussian processes (DGPs), which are another generalization constructed by a hierarchical concatenation of GPs.  In this work, we propose a generalization of TGPs named deep transformed Gaussian processes (DTGPs), which follows the trend of concatenating layers of stochastic processes. More precisely, we obtain a multi-layer model in which each layer is a TGP. This generalization implies an increase in flexibility with respect to both TGPs and DGPs. Exact inference in such a model is intractable. However, we show that one can use variational inference to approximate the required computations, yielding a straightforward extension of the popular DSVI inference algorithm (Salimbeni et al., 2017). The experiments conducted evaluate the proposed novel DTGPs in multiple regression datasets, achieving good scalability and performance.Note: Some of the technical terms in the original text, such as "Gaussian processes" and "variational inference," may not have direct translations in Simplified Chinese. In such cases, I have used the most common translations available in the literature, but the reader may need to consult a more specialized dictionary or reference for a more precise translation.
</details></li>
</ul>
<hr>
<h2 id="One-Model-Fits-All-Cross-Region-Taxi-Demand-Forecasting"><a href="#One-Model-Fits-All-Cross-Region-Taxi-Demand-Forecasting" class="headerlink" title="One Model Fits All: Cross-Region Taxi-Demand Forecasting"></a>One Model Fits All: Cross-Region Taxi-Demand Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18215">http://arxiv.org/abs/2310.18215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ren Ozeki, Haruki Yonekura, Aidana Baimbetova, Hamada Rizk, Hirozumi Yamaguchi</li>
<li>for: 预测出行需求 (forecasting taxi demand)</li>
<li>methods: 使用图 neural network 捕捉城市环境中的空间相关性和模式，同时采用区域中性方法，使模型可以在任何区域中进行预测，包括未经见过的区域。</li>
<li>results: 实验结果表明，提案的系统能够准确预测出行需求，包括在未经见过的区域。这显示了该系统在优化出行服务和提高交通效率的潜力。<details>
<summary>Abstract</summary>
The growing demand for ride-hailing services has led to an increasing need for accurate taxi demand prediction. Existing systems are limited to specific regions, lacking generalizability to unseen areas. This paper presents a novel taxi demand forecasting system that leverages a graph neural network to capture spatial dependencies and patterns in urban environments. Additionally, the proposed system employs a region-neutral approach, enabling it to train a model that can be applied to any region, including unseen regions. To achieve this, the framework incorporates the power of Variational Autoencoder to disentangle the input features into region-specific and region-neutral components. The region-neutral features facilitate cross-region taxi demand predictions, allowing the model to generalize well across different urban areas. Experimental results demonstrate the effectiveness of the proposed system in accurately forecasting taxi demand, even in previously unobserved regions, thus showcasing its potential for optimizing taxi services and improving transportation efficiency on a broader scale.
</details>
<details>
<summary>摘要</summary>
随着乘车需求的增长，需求预测已成为了ride-hailing服务的紧迫需求。现有的系统受限于特定地区，缺乏对未见地区的泛化能力。这篇论文提出了一种新的出租车需求预测系统，利用图 neural network 捕捉城市环境中的空间依赖关系和模式。此外，提出的系统采用了地域中性的方法，使得其可以训练可应用于任何地区，包括未见地区的模型。为达到这一目标，框架具有Variational Autoencoder 的力量，压缩输入特征成地域特定和地域中性组成部分。地域中性特征使得出租车需求预测可以在不同的城市区域进行跨地区预测，使模型能够在不同的城市区域中具有泛化能力。实验结果表明，提出的系统可以准确预测出租车需求，甚至在未见地区进行预测，从而展示其在优化出租车服务和改善交通效率的潜力。
</details></li>
</ul>
<hr>
<h2 id="Robustness-of-Algorithms-for-Causal-Structure-Learning-to-Hyperparameter-Choice"><a href="#Robustness-of-Algorithms-for-Causal-Structure-Learning-to-Hyperparameter-Choice" class="headerlink" title="Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice"></a>Robustness of Algorithms for Causal Structure Learning to Hyperparameter Choice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18212">http://arxiv.org/abs/2310.18212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dmachlanski/benchpress-dm">https://github.com/dmachlanski/benchpress-dm</a></li>
<li>paper_authors: Damian Machlanski, Spyridon Samothrakis, Paul Clarke</li>
<li>for: 本研究旨在探讨隐藏参数在结构学习中的影响，以及如何选择最佳的隐藏参数来提高结构学习性能。</li>
<li>methods: 本研究采用了一些经典的结构学习算法，并对这些算法进行了融合调参。</li>
<li>results: 研究发现，隐藏参数的选择在集成设置中具有很大的影响，可以导致分析者选择不适合自己数据的算法，从而影响结构学习性能。<details>
<summary>Abstract</summary>
Hyperparameters play a critical role in machine learning. Hyperparameter tuning can make the difference between state-of-the-art and poor prediction performance for any algorithm, but it is particularly challenging for structure learning due to its unsupervised nature. As a result, hyperparameter tuning is often neglected in favour of using the default values provided by a particular implementation of an algorithm. While there have been numerous studies on performance evaluation of causal discovery algorithms, how hyperparameters affect individual algorithms, as well as the choice of the best algorithm for a specific problem, has not been studied in depth before. This work addresses this gap by investigating the influence of hyperparameters on causal structure learning tasks. Specifically, we perform an empirical evaluation of hyperparameter selection for some seminal learning algorithms on datasets of varying levels of complexity. We find that, while the choice of algorithm remains crucial to obtaining state-of-the-art performance, hyperparameter selection in ensemble settings strongly influences the choice of algorithm, in that a poor choice of hyperparameters can lead to analysts using algorithms which do not give state-of-the-art performance for their data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Model-free-Posterior-Sampling-via-Learning-Rate-Randomization"><a href="#Model-free-Posterior-Sampling-via-Learning-Rate-Randomization" class="headerlink" title="Model-free Posterior Sampling via Learning Rate Randomization"></a>Model-free Posterior Sampling via Learning Rate Randomization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18186">http://arxiv.org/abs/2310.18186</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Pierre Perrault, Michal Valko, Pierre Menard</li>
<li>for: 这篇论文是设计了一个新的机器学习算法，叫做Randomized Q-learning (RandQL)，用于最小化 regret 的 episodic Markov Decision Processes (MDPs) 中的 regret。</li>
<li>methods: RandQL 使用了一个新的 Randomized model-free 算法，使用 posterior sampling 来实现 optimistic exploration，不需要使用 bonus。</li>
<li>results: RandQL 在 tabular MDPs 和 metric state-action space 中均可以 achieve regret bound of order $\widetilde{\mathcal{O}(\sqrt{H^{5}SAT})$，并且在 empirical study 中与 existing approaches 比较，表现更好。<details>
<summary>Abstract</summary>
In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order $\widetilde{\mathcal{O}(\sqrt{H^{5}SAT})$, where $H$ is the planning horizon, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order $\widetilde{\mathcal{O}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where $d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic exploration without using bonuses, relying instead on a novel idea of learning rate randomization. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration environments.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了Randomized Q-learning（RandQL），一种新的随机化无模型算法，用于减少偏差在集束Markov决策过程（MDP）中的 regret。我们知道，RandQL是首个可追踪的无模型 posterior sampling-based algorithm。我们分析了RandQL在标准MDP和非标准 metric space中的性能。在标准MDP中，RandQL的 regret bound为 $\widetilde{\mathcal{O}(\sqrt{H^{5}SAT})$，其中 $H$ 是规划 horizion， $S$ 是状态数量， $A$ 是动作数量， $T$ 是集束数量。在 metric state-action space 中，RandQL 的 regret bound为 $\widetilde{\mathcal{O}(H^{5/2} T^{(d_z+1)/(d_z+2)})$，其中 $d_z$ 表示 zooming 维度。值得注意的是，RandQL 实现了无奖诱导的探索，不使用奖励，而是通过一种新的学习率随机化的想法。我们的实验研究表明，RandQL 在基eline探索环境中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Enterprise-Network-Security-Comparing-Machine-Level-and-Process-Level-Analysis-for-Dynamic-Malware-Detection"><a href="#Enhancing-Enterprise-Network-Security-Comparing-Machine-Level-and-Process-Level-Analysis-for-Dynamic-Malware-Detection" class="headerlink" title="Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection"></a>Enhancing Enterprise Network Security: Comparing Machine-Level and Process-Level Analysis for Dynamic Malware Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18165">http://arxiv.org/abs/2310.18165</a></li>
<li>repo_url: None</li>
<li>paper_authors: Baskoro Adi Pratomo, Toby Jackson, Pete Burnap, Andrew Hood, Eirini Anthi</li>
<li>for: 本研究旨在提高动态恶意软件检测精度，以便更好地理解恶意软件如何工作，并开发适当的检测和预防方法。</li>
<li>methods: 本研究使用了过程级别的Recurrent Neural Network (RNN)检测模型，以便更好地识别和分类运行中的恶意进程。</li>
<li>results: 对比前一代STATE-OF-THE-ART方法，本研究的提议模型具有较高的检测精度，具体来说，检测精度提高约20.12%，而false positive率在0.1左右。<details>
<summary>Abstract</summary>
Analysing malware is important to understand how malicious software works and to develop appropriate detection and prevention methods. Dynamic analysis can overcome evasion techniques commonly used to bypass static analysis and provide insights into malware runtime activities. Much research on dynamic analysis focused on investigating machine-level information (e.g., CPU, memory, network usage) to identify whether a machine is running malicious activities. A malicious machine does not necessarily mean all running processes on the machine are also malicious. If we can isolate the malicious process instead of isolating the whole machine, we could kill the malicious process, and the machine can keep doing its job. Another challenge dynamic malware detection research faces is that the samples are executed in one machine without any background applications running. It is unrealistic as a computer typically runs many benign (background) applications when a malware incident happens. Our experiment with machine-level data shows that the existence of background applications decreases previous state-of-the-art accuracy by about 20.12% on average. We also proposed a process-level Recurrent Neural Network (RNN)-based detection model. Our proposed model performs better than the machine-level detection model; 0.049 increase in detection rate and a false-positive rate below 0.1.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Proportional-Fairness-in-Clustering-A-Social-Choice-Perspective"><a href="#Proportional-Fairness-in-Clustering-A-Social-Choice-Perspective" class="headerlink" title="Proportional Fairness in Clustering: A Social Choice Perspective"></a>Proportional Fairness in Clustering: A Social Choice Perspective</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18162">http://arxiv.org/abs/2310.18162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leon Kellerhals, Jannik Peters</li>
<li>for: 这个论文研究了陈等人的著作([ICML’19])中的质量归一化问题，并将其与计算社会选择领域的多赢者投票相关联。</li>
<li>methods: 这篇论文使用了布里尔和彼得斯（[EC’23）的弱 пропорциональ性定义来证明任何归一化满足这种定义，同时也能获得最佳知道的分配公平性定义（陈等人，[ICML’19），以及个人公平性定义（Jung等人，[FORC’20）和核心定义（Li等人，[ICML’21）的最佳近似值。</li>
<li>results: 这篇论文显示任何归一化都能同时满足分配公平性定义和个人公平性定义，以及更强的多赢者代表性定义。此外，弱 пропорциональ性定义也能导致更强的多赢者代表性定义的近似值。<details>
<summary>Abstract</summary>
We study the proportional clustering problem of Chen et al. [ICML'19] and relate it to the area of multiwinner voting in computational social choice. We show that any clustering satisfying a weak proportionality notion of Brill and Peters [EC'23] simultaneously obtains the best known approximations to the proportional fairness notion of Chen et al. [ICML'19], but also to individual fairness [Jung et al., FORC'20] and the "core" [Li et al. ICML'21]. In fact, we show that any approximation to proportional fairness is also an approximation to individual fairness and vice versa. Finally, we also study stronger notions of proportional representation, in which deviations do not only happen to single, but multiple candidate centers, and show that stronger proportionality notions of Brill and Peters [EC'23] imply approximations to these stronger guarantees.
</details>
<details>
<summary>摘要</summary>
我们研究陈等人的著作中的协Relative Clustering问题([ICML'19])，并与计算社会选择领域的多赢者投票问题相关。我们显示任何满足Brill和Peter斯([EC'23])的弱 пропорциональ性定义，同时也能获得Chen等人的最佳知道的变分概念([ICML'19])、个体公平([Jung等人，FORC'20])以及"核心"([Li等人，ICML'21])的最佳近似。事实上，任何对 proportional fairness 的近似也是对个体公平的近似，并且vice versa。 finally，我们还研究了更强的多个候选者代表性定义，在多个候选者中偏移不仅发生在单个中心，而是在多个中心上，并证明Brill和Peter斯([EC'23])的更强的 proportionality 定义能够导致这些更强的保证。
</details></li>
</ul>
<hr>
<h2 id="Sample-Complexity-Bounds-for-Score-Matching-Causal-Discovery-and-Generative-Modeling"><a href="#Sample-Complexity-Bounds-for-Score-Matching-Causal-Discovery-and-Generative-Modeling" class="headerlink" title="Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling"></a>Sample Complexity Bounds for Score-Matching: Causal Discovery and Generative Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18123">http://arxiv.org/abs/2310.18123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenyu Zhu, Francesco Locatello, Volkan Cevher</li>
<li>for: 这篇论文为了提供了对Score-matching的统计样本复杂性下界，以及其应用于 causal discovery。</li>
<li>methods: 论文使用了标准的深度ReLU神经网络和随机梯度下降来准确地估计得分函数。</li>
<li>results: 论文提出了对Score-matching基于 causal discovery方法的Recovering causal relationships的误差率的下界，假设得分函数的估计充分准确。此外，论文还分析了Score-matching估计在Score-based生成模型中的Upper bound。<details>
<summary>Abstract</summary>
This paper provides statistical sample complexity bounds for score-matching and its applications in causal discovery. We demonstrate that accurate estimation of the score function is achievable by training a standard deep ReLU neural network using stochastic gradient descent. We establish bounds on the error rate of recovering causal relationships using the score-matching-based causal discovery method of Rolland et al. [2022], assuming a sufficiently good estimation of the score function. Finally, we analyze the upper bound of score-matching estimation within the score-based generative modeling, which has been applied for causal discovery but is also of independent interest within the domain of generative models.
</details>
<details>
<summary>摘要</summary>
Note:* "score-matching" is translated as "分数匹配" (fēnzhèng píngchǎ)* "causal discovery" is translated as " causal discovery" ( causal discovery)* "score function" is translated as "分数函数" (fēnzhèng fúnción)* "standard deep ReLU neural network" is translated as "标准深度ReLU神经网络" (zhèngdé ReLU xīnnéirwàng)* "stochastic gradient descent" is translated as "随机梯度下降" (suìjī tiēdào xiàojiù)* "error rate" is translated as "错误率" (error rate)
</details></li>
</ul>
<hr>
<h2 id="A-Global-Multi-Unit-Calibration-as-a-Method-for-Large-Scale-IoT-Particulate-Matter-Monitoring-Systems-Deployments"><a href="#A-Global-Multi-Unit-Calibration-as-a-Method-for-Large-Scale-IoT-Particulate-Matter-Monitoring-Systems-Deployments" class="headerlink" title="A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments"></a>A Global Multi-Unit Calibration as a Method for Large Scale IoT Particulate Matter Monitoring Systems Deployments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18118">http://arxiv.org/abs/2310.18118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Saverio De Vito, Gerardo D Elia, Sergio Ferlito, Girolamo Di Francia, Milos Davidovic, Duska Kleut, Danka Stojanovic, Milena Jovasevic Stojanovic</li>
<li>for: 这个论文的目的是提出一种可扩展和有效的准确测试方法，以便在低成本空气质量监测系统中实现准确的气囊监测。</li>
<li>methods: 这种方法基于论文中提出的全球准确测试方法，使用低成本 particulate matter (PM) 传感器，并利用场记录的响应来实现。这种方法可以对所有同类型的设备进行universal应用，并且可以在设备上直接实现。</li>
<li>results: 测试campaign表明，当应用于不同的传感器时，这种方法的性能与现有的方法匹配，而且可以实现大量的准确气囊监测设备的投入。如果确认，这些结果表明，当得到了正确的准确测试法，可以在大量的网络设备上实现准确的气囊监测，并且可以减少长距离数据传输需求。此外，这种准确测试模型可以轻松地被嵌入到设备上，或者在边缘实现，以便实现个人曝露监测应用。<details>
<summary>Abstract</summary>
Scalable and effective calibration is a fundamental requirement for Low Cost Air Quality Monitoring Systems and will enable accurate and pervasive monitoring in cities. Suffering from environmental interferences and fabrication variance, these devices need to encompass sensors specific and complex calibration processes for reaching a sufficient accuracy to be deployed as indicative measurement devices in Air Quality (AQ) monitoring networks. Concept and sensor drift often force calibration process to be frequently repeated. These issues lead to unbearable calibration costs which denies their massive deployment when accuracy is a concern. In this work, We propose a zero transfer samples, global calibration methodology as a technological enabler for IoT AQ multisensory devices which relies on low cost Particulate Matter (PM) sensors. This methodology is based on field recorded responses from a limited number of IoT AQ multisensors units and machine learning concepts and can be universally applied to all units of the same type. A multi season test campaign shown that, when applied to different sensors, this methodology performances match those of state of the art methodology which requires to derive different calibration parameters for each different unit. If confirmed, these results show that, when properly derived, a global calibration law can be exploited for a large number of networked devices with dramatic cost reduction eventually allowing massive deployment of accurate IoT AQ monitoring devices. Furthermore, this calibration model could be easily embedded on board of the device or implemented on the edge allowing immediate access to accurate readings for personal exposure monitor applications as well as reducing long range data transfer needs.
</details>
<details>
<summary>摘要</summary>
低成本空气质量监测系统需要扩展可扩展的准确化，以实现精准和广泛的监测城市。由于环境干扰和制造变化，这些设备需要特定的感应器和复杂的准确化过程以达到足够的准确性，以便作为空气质量（AQ）监测网络的指示测量设备。概念和感应器偏移 часто导致准确化过程需要频繁重复。这些问题导致不可持续的准确化成本，这使得大规模部署变得不可能。在这种情况下，我们提议一种零传输样本、全球准确化方法，这种方法基于低成本 particulate matter（PM）感应器。这种方法基于场记录的响应，并且可以通过机器学习概念应用于所有类型的单元。一个多季度测试 campagne表明，当应用于不同的感应器时，这种方法的性能与当前的方法匹配，该方法需要为每个不同单元 derivation 不同的准确化参数。如果确认，这些结果表明，当正确地 derivation 全球准确化法则，可以在大量部署精准的 IoT AQ 监测设备。此外，这种准确化模型可以轻松地嵌入到设备上或实现在边缘，以便提供快速的准确阅读，用于个人曝露监测应用，以及减少长距离数据传输需求。
</details></li>
</ul>
<hr>
<h2 id="Transductive-conformal-inference-with-adaptive-scores"><a href="#Transductive-conformal-inference-with-adaptive-scores" class="headerlink" title="Transductive conformal inference with adaptive scores"></a>Transductive conformal inference with adaptive scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18108">http://arxiv.org/abs/2310.18108</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ulysse Gazin, Gilles Blanchard, Etienne Roquain</li>
<li>For:  This paper provides distribution-free guarantees for many machine learning tasks, specifically in the transductive setting where decisions are made on a test sample of new points.* Methods: The paper uses conformal inference, which is a fundamental and versatile tool that provides distribution-free guarantees. The paper also uses a P&#39;olya urn model to describe the joint distribution of the conformal $p$-values, and establishes a concentration inequality for their empirical distribution function.* Results: The paper provides uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification. The results hold for arbitrary exchangeable scores, including adaptive ones that can use the covariates of the test+calibration samples at training stage for increased accuracy.<details>
<summary>Abstract</summary>
Conformal inference is a fundamental and versatile tool that provides distribution-free guarantees for many machine learning tasks. We consider the transductive setting, where decisions are made on a test sample of $m$ new points, giving rise to $m$ conformal $p$-values. {While classical results only concern their marginal distribution, we show that their joint distribution follows a P\'olya urn model, and establish a concentration inequality for their empirical distribution function.} The results hold for arbitrary exchangeable scores, including {\it adaptive} ones that can use the covariates of the test+calibration samples at training stage for increased accuracy. We demonstrate the usefulness of these theoretical results through uniform, in-probability guarantees for two machine learning tasks of current interest: interval prediction for transductive transfer learning and novelty detection based on two-class classification.
</details>
<details>
<summary>摘要</summary>
它们是一种基本和多方面的工具，提供不受分布限制的保证，用于许多机器学习任务。我们考虑了推论 setting，在一个测试样本中有 $m$ 个新点，从而生成 $m$ 个充分满足的 $p$-值。{而 classical 结果只关注它们的边缘分布，我们显示它们的联合分布遵循波尔雅urn模型，并证明它们的empirical distribution function具有减法不等式.}结果适用于任意兼容的分数，包括可适应的分数，可以在训练阶段使用测试样本和标准化样本的 covariates 进行更高的准确性。我们通过对两个现有的机器学习任务进行保证， namely interval prediction for transductive transfer learning和 noveldetection based on two-class classification，来证明这些理论结果的实用性。
</details></li>
</ul>
<hr>
<h2 id="Adversarial-Anomaly-Detection-using-Gaussian-Priors-and-Nonlinear-Anomaly-Scores"><a href="#Adversarial-Anomaly-Detection-using-Gaussian-Priors-and-Nonlinear-Anomaly-Scores" class="headerlink" title="Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores"></a>Adversarial Anomaly Detection using Gaussian Priors and Nonlinear Anomaly Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18091">http://arxiv.org/abs/2310.18091</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/emundo/ecgan">https://github.com/emundo/ecgan</a></li>
<li>paper_authors: Fiete Lüer, Tobias Weber, Maxim Dolgich, Christian Böhm</li>
<li>for: 这篇论文的目的是解决受束缚的数据集中异常检测问题，尤其在医疗领域，因为获取和标注异常性是成本较高的。</li>
<li>methods: 该论文提出了一种新的模型，称为β-VAEGAN，它将β-variational autoencoder (VAE) 的生成稳定性与生成敌对网络 (GANs) 的探测力相结合。论文还研究了如何组合异常分数，包括使用泛化支持向量机 (SVM) 在不同的关联方式下进行训练。</li>
<li>results: 与现有工作相比，该论文在MITBIH Arrhythmia Database 上的 $F_1$ 分数从0.85提高到0.92，表明β-VAEGAN 可以更好地检测异常。<details>
<summary>Abstract</summary>
Anomaly detection in imbalanced datasets is a frequent and crucial problem, especially in the medical domain where retrieving and labeling irregularities is often expensive. By combining the generative stability of a $\beta$-variational autoencoder (VAE) with the discriminative strengths of generative adversarial networks (GANs), we propose a novel model, $\beta$-VAEGAN. We investigate methods for composing anomaly scores based on the discriminative and reconstructive capabilities of our model. Existing work focuses on linear combinations of these components to determine if data is anomalous. We advance existing work by training a kernelized support vector machine (SVM) on the respective error components to also consider nonlinear relationships. This improves anomaly detection performance, while allowing faster optimization. Lastly, we use the deviations from the Gaussian prior of $\beta$-VAEGAN to form a novel anomaly score component. In comparison to state-of-the-art work, we improve the $F_1$ score during anomaly detection from 0.85 to 0.92 on the widely used MITBIH Arrhythmia Database.
</details>
<details>
<summary>摘要</summary>
非常常见的异常检测问题在不均衡数据集中，特别是在医疗领域，因为检测和标注异常性往往是昂贵的。我们提出了一种新的模型，$\beta$-VAEGAN，通过结合$\beta$-variational autoencoder（VAE）的生成稳定性和生成敌对网络（GANs）的攻击力，以提高异常检测性能。我们研究了基于这两个组件的异常分数的组合方法，包括线性组合以及训练kernelized支持向量机（SVM）来考虑非线性关系。这些改进了异常检测性能，同时允许更快的优化。此外，我们还使用$\beta$-VAEGAN的偏差从拜尔分布来形成一种新的异常分数组件。与现有工作相比，我们在MITBIHArrhythmia数据库上提高了异常检测$F_1$分数从0.85提高到0.92。
</details></li>
</ul>
<hr>
<h2 id="Unveiling-the-Potential-of-Probabilistic-Embeddings-in-Self-Supervised-Learning"><a href="#Unveiling-the-Potential-of-Probabilistic-Embeddings-in-Self-Supervised-Learning" class="headerlink" title="Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning"></a>Unveiling the Potential of Probabilistic Embeddings in Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18080">http://arxiv.org/abs/2310.18080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Denis Janiak, Jakub Binkowski, Piotr Bielak, Tomasz Kajdanowicz</li>
<li>for: 本研究旨在探讨自动学习中的信息理论基础, 特别是模型学习响应不做预处理的情况下, 模型能够从无标签数据中获得有用的表示。</li>
<li>methods: 本研究使用了probabilistic embedding来模型表示, 并评估其对性能, 信息压缩和外部样本探测的影响。</li>
<li>results: 研究发现, 在信息理论基础下, 增加一个压缩瓶颈可以明显提高外部样本探测能力, 但是同时可能导致表示的压缩和信息损失。<details>
<summary>Abstract</summary>
In recent years, self-supervised learning has played a pivotal role in advancing machine learning by allowing models to acquire meaningful representations from unlabeled data. An intriguing research avenue involves developing self-supervised models within an information-theoretic framework, but many studies often deviate from the stochasticity assumptions made when deriving their objectives. To gain deeper insights into this issue, we propose to explicitly model the representation with stochastic embeddings and assess their effects on performance, information compression and potential for out-of-distribution detection. From an information-theoretic perspective, we seek to investigate the impact of probabilistic modeling on the information bottleneck, shedding light on a trade-off between compression and preservation of information in both representation and loss space. Emphasizing the importance of distinguishing between these two spaces, we demonstrate how constraining one can affect the other, potentially leading to performance degradation. Moreover, our findings suggest that introducing an additional bottleneck in the loss space can significantly enhance the ability to detect out-of-distribution examples, only leveraging either representation features or the variance of their underlying distribution.
</details>
<details>
<summary>摘要</summary>
近年来，自适应学习已经在机器学习领域发挥了关键作用，允许模型从无标签数据中获得有意义的表示。一个吸引人的研究方向是在信息理论框架下开发自适应模型，但许多研究通常会背离在 derivation 目标时所做的随机性假设。为了更深入地了解这个问题，我们提议显式地模型表示中的随机嵌入，评估其对性能、信息压缩和可能出现在其他分布中的检测的影响。从信息理论的视角来看，我们希望 investigate 表示中的信息瓶颈，探讨其与损失空间之间的交互关系，以及在这两个空间之间是否存在负反馈的问题。我们发现，在损失空间中引入一个额外的瓶颈可以明显提高对于异常分布的检测，只需要使用表示特征或者对其分布的变异。
</details></li>
</ul>
<hr>
<h2 id="Lipschitz-and-Holder-Continuity-in-Reproducing-Kernel-Hilbert-Spaces"><a href="#Lipschitz-and-Holder-Continuity-in-Reproducing-Kernel-Hilbert-Spaces" class="headerlink" title="Lipschitz and Hölder Continuity in Reproducing Kernel Hilbert Spaces"></a>Lipschitz and Hölder Continuity in Reproducing Kernel Hilbert Spaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18078">http://arxiv.org/abs/2310.18078</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Fiedler</li>
<li>for:  investigate Lipschitz and H&quot;older continuity in Reproducing Kernel Hilbert Spaces (RKHSs)</li>
<li>methods:  provide sufficient conditions and collect related known results from the literature</li>
<li>results:  new results on reproducing kernels inducing prescribed Lipschitz or H&quot;older continuity<details>
<summary>Abstract</summary>
Reproducing kernel Hilbert spaces (RKHSs) are very important function spaces, playing an important role in machine learning, statistics, numerical analysis and pure mathematics. Since Lipschitz and H\"older continuity are important regularity properties, with many applications in interpolation, approximation and optimization problems, in this work we investigate these continuity notion in RKHSs. We provide several sufficient conditions as well as an in depth investigation of reproducing kernels inducing prescribed Lipschitz or H\"older continuity. Apart from new results, we also collect related known results from the literature, making the present work also a convenient reference on this topic.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>复制kernel空间（RKHS）是非常重要的函数空间，在机器学习、统计、数值分析和纯 математи学中扮演着重要的角色。由于 lipschitz 和 holder 连续性是重要的规范性质，在 interpolate、approximation 和优化问题中具有广泛的应用，因此在这种工作中我们调查这些连续性观念在 RKHS 中。我们提供了多个足够条件以及对 reproduce kernel 引起的 lipschitz 或 holder 连续性进行深入调查。除了新的结果之外，我们还收集了相关的已知结果，使得现在的工作也成为了这个话题的便捷参考。
</details></li>
</ul>
<hr>
<h2 id="On-kernel-based-statistical-learning-in-the-mean-field-limit"><a href="#On-kernel-based-statistical-learning-in-the-mean-field-limit" class="headerlink" title="On kernel-based statistical learning in the mean field limit"></a>On kernel-based statistical learning in the mean field limit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18074">http://arxiv.org/abs/2310.18074</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christian Fiedler, Michael Herty, Sebastian Trimpe</li>
<li>for: 本研究探讨了机器学习中大量变量的应用，特别是基于互动粒子系统的机器学习问题。</li>
<li>methods: 本研究使用了kernel方法，完善了现有的理论，并提供了有关这些kernel的应用 approximate的结果。</li>
<li>results: 研究结果表明，在mean field limit中，empirical和无限样本解的 converges 以及相关的风险的 converges。这些结果为大规模问题提供了新的理论工具和洞察，同时也为统计学学习理论中的limit问题提供了新的形式。<details>
<summary>Abstract</summary>
In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds to a new form of limit of learning problems, which seems to have not been investigated yet in the statistical learning theory literature.
</details>
<details>
<summary>摘要</summary>
Many machine learning applications involve a large number of variables. 基于机器学习中的互动体系，我们考虑到输入变量的数量趋于无穷大。首先，我们继续推动渐近场限的kernel和它们的重现函数空间的研究，完善现有的理论。接着，我们提供用这些kernel进行近似的结果，包括一个表示定理。最后，我们使用这些kernel在mean field限下进行统计学学习，具体来说，我们展示了empirical和无限样本解的mean field收敛和相应的风险的收敛。我们的结果建立了机器学习中的mean field限，提供了新的理论工具和意见，用于处理大规模问题。另一方面，我们的设定对 statistical learning theory文献中没有被 investigate的一种新的限制问题形式，这种形式是mean field limit。
</details></li>
</ul>
<hr>
<h2 id="DP-SGD-with-weight-clipping"><a href="#DP-SGD-with-weight-clipping" class="headerlink" title="DP-SGD with weight clipping"></a>DP-SGD with weight clipping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18001">http://arxiv.org/abs/2310.18001</a></li>
<li>repo_url: None</li>
<li>paper_authors: Antoine Barczewski, Jan Ramon</li>
<li>for: 这个论文目的是提出一种新的权限保护技术，以保证数据隐私。</li>
<li>methods: 该论文使用了一种新的方法，利用公共信息来约束梯度的变化，从而减少权限保护所需的噪声。</li>
<li>results: 该论文提出的方法可以提高梯度约束的精度，从而提高数据隐私的保护水平，同时也可以降低噪声的水平。<details>
<summary>Abstract</summary>
Recently, due to the popularity of deep neural networks and other methods whose training typically relies on the optimization of an objective function, and due to concerns for data privacy, there is a lot of interest in differentially private gradient descent methods. To achieve differential privacy guarantees with a minimum amount of noise, it is important to be able to bound precisely the sensitivity of the information which the participants will observe. In this study, we present a novel approach that mitigates the bias arising from traditional gradient clipping. By leveraging public information concerning the current global model and its location within the search domain, we can achieve improved gradient bounds, leading to enhanced sensitivity determinations and refined noise level adjustments. We extend the state of the art algorithms, present improved differential privacy guarantees requiring less noise and present an empirical evaluation.
</details>
<details>
<summary>摘要</summary>
最近，由于深度神经网络和其他方法的训练通常基于目标函数优化，以及数据隐私的关注，有很多关注在不同敏感度下进行梯度下降方法。为了保证数据隐私保障，需要准确地评估梯度下降中信息敏感度。在本研究中，我们提出了一种新的方法，用于减轻传统梯度裁剪所导致的偏见。我们利用公共信息，包括当前全球模型和其位置在搜索区域中，来实现更好的梯度 bound，从而提高敏感度评估和降低噪声水平。我们扩展了现有算法，提供更好的不同敏感度保障，并进行了实验评估。
</details></li>
</ul>
<hr>
<h2 id="Closing-the-Gap-Between-the-Upper-Bound-and-the-Lower-Bound-of-Adam’s-Iteration-Complexity"><a href="#Closing-the-Gap-Between-the-Upper-Bound-and-the-Lower-Bound-of-Adam’s-Iteration-Complexity" class="headerlink" title="Closing the Gap Between the Upper Bound and the Lower Bound of Adam’s Iteration Complexity"></a>Closing the Gap Between the Upper Bound and the Lower Bound of Adam’s Iteration Complexity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17998">http://arxiv.org/abs/2310.17998</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, Wei Chen<br>for: 这个论文是为了提供一种新的 Adam 优化算法的 convergenc guarantee，以便在不同的 hyperparameters 下实现更高的效率。methods: 这个论文使用了一种新的技术来处理积分和自适应学习率的杂糅，并将 Descent Lemma 中的首项转换为 gradients 的 norm，以获得更高的效率。results: 这个论文提出了一种新的 Adam 优化算法，其 convergenc guarantee 是基于 $L$-smooth condition 和 bounded noise variance assumption，并且适用于广泛的 hyperparameters。特别是，对于合适的 hyperparameters，这个算法可以实现更高的效率，并且可以 closing the gap  между existing literature 中的 convergence guarantee 和实际性能。<details>
<summary>Abstract</summary>
Recently, Arjevani et al. [1] established a lower bound of iteration complexity for the first-order optimization under an $L$-smooth condition and a bounded noise variance assumption. However, a thorough review of existing literature on Adam's convergence reveals a noticeable gap: none of them meet the above lower bound. In this paper, we close the gap by deriving a new convergence guarantee of Adam, with only an $L$-smooth condition and a bounded noise variance assumption. Our results remain valid across a broad spectrum of hyperparameters. Especially with properly chosen hyperparameters, we derive an upper bound of the iteration complexity of Adam and show that it meets the lower bound for first-order optimizers. To the best of our knowledge, this is the first to establish such a tight upper bound for Adam's convergence. Our proof utilizes novel techniques to handle the entanglement between momentum and adaptive learning rate and to convert the first-order term in the Descent Lemma to the gradient norm, which may be of independent interest.
</details>
<details>
<summary>摘要</summary>
近些时候，Arjevani等人（1）已经建立了first-order优化的迭代复杂度下界。然而，现有文献中对Adam的减少报告没有满足上述下界。在这篇论文中，我们填补了这一漏洞，通过引入$L$-smooth条件和bounded noise variance假设， derivate一个新的Adam的减少保证。我们的结论适用于广泛的权重参数。特别是，对于适当的权重参数，我们 derive一个迭代复杂度的 Upper bound of Adam，并证明它与first-order优化器的下界相符。根据我们知道，这是first-order优化器的减少保证的首次建立。我们的证明使用了新的技术来处理杠杆和自适应学习率的杂糅，并将 Descent Lemma 中的first-order项转换为梯度norm，这可能有独立的价值。
</details></li>
</ul>
<hr>
<h2 id="CEFL-Carbon-Efficient-Federated-Learning"><a href="#CEFL-Carbon-Efficient-Federated-Learning" class="headerlink" title="CEFL: Carbon-Efficient Federated Learning"></a>CEFL: Carbon-Efficient Federated Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17972">http://arxiv.org/abs/2310.17972</a></li>
<li>repo_url: None</li>
<li>paper_authors: Talha Mehboob, Noman Bashir, Jesus Omana Iglesias, Michael Zink, David Irwin</li>
<li>for: 这篇论文的目的是优化 Federated Learning（FL）模型训练的成本，以减少资料转移开销和保护数据隐私。</li>
<li>methods: 本论文使用了适应成本意识的客户端选择策略来优化 FL 模型训练中的成本。这些策略扩展了以往的实用性基础学习和决定性学习阶段，并将它们变成成本意识的。</li>
<li>results: 这篇论文显示了一个叫做“碳能efficient FL”，其中使用了能源的碳气况来衡量成本。结果显示，这种方法可以 reduces carbon emissions by 93% 和 reduces training time by 50% 相比随机选择客户端。另外，它可以 reduces carbon emissions by 80%, 而仅增加训练时间 by 38% 相比一种现有的方法。<details>
<summary>Abstract</summary>
Federated Learning (FL) distributes machine learning (ML) training across many edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span millions of devices and is thus resource-intensive, prior work has focused on improving its resource efficiency to optimize time-to-accuracy. However, prior work generally treats all resources the same, while, in practice, they may incur widely different costs, which instead motivates optimizing cost-to-accuracy. To address the problem, we design CEFL, which uses adaptive cost-aware client selection policies to optimize an arbitrary cost metric when training FL models. Our policies extend and combine prior work on utility-based client selection and critical learning periods by making them cost-aware. We demonstrate CEFL by designing carbon-efficient FL, where energy's carbon-intensity is the cost, and show that it i) reduces carbon emissions by 93\% and reduces training time by 50% compared to random client selection and ii) reduces carbon emissions by 80%, while only increasing training time by 38%, compared to a state-of-the-art approach that optimizes training time.
</details>
<details>
<summary>摘要</summary>
协同学习（FL）通过分布机器学习训练 across多个边缘设备来减少数据传输开销和保护数据隐私。由于FL模型训练可能涵盖数百万个设备，因此需要进行资源效率优化以提高时间精度。然而，先前的工作通常忽视不同资源之间的差异，而在实践中，这些资源可能具有不同的成本，这些成本反而需要优化成本精度。为解决这个问题，我们提出了CEFL，它使用适应成本 aware的客户端选择策略来优化任意成本度量在协同学习模型训练中。我们的策略扩展和结合了先前的实用性基于资源利用率的客户端选择策略和批处理学习时期的优化策略，使其成为成本 aware。我们通过设计碳素协同学习，其中能源的碳气强度作为成本，并证明了它可以：1. 降低碳排放量93%，降低训练时间50%比Random Client Selection。2. 降低碳排放量80%，仅提高训练时间38%比一种状态精通的方法。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Edge-Machine-Learning-A-Survey"><a href="#Trustworthy-Edge-Machine-Learning-A-Survey" class="headerlink" title="Trustworthy Edge Machine Learning: A Survey"></a>Trustworthy Edge Machine Learning: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17944">http://arxiv.org/abs/2310.17944</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaojie Wang, Beibei Wang, Yu Wu, Zhaolong Ning, Song Guo, Fei Richard Yu</li>
<li>for: 本研究强调 Edge Machine Learning (EML) 在 Sixth-Generation (6G) 网络中的可靠性，以满足不同应用场景的需求。</li>
<li>methods: 本文首先介绍了 EML 在部署和实际应用场景中遇到的挑战，并提出了一个初步的可靠 EML 定义和关键特性。然后，文章介绍了基本的框架和实现技术，并进行了深入的文献综述，以增强 EML 系统的可靠性。</li>
<li>results: 本文提出了一些解决方案来提高 EML 系统的可靠性，但也指出了一些研究挑战和未解决的问题。<details>
<summary>Abstract</summary>
The convergence of Edge Computing (EC) and Machine Learning (ML), known as Edge Machine Learning (EML), has become a highly regarded research area by utilizing distributed network resources to perform joint training and inference in a cooperative manner. However, EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications, which together affect the trustworthiness of EML in the eyes of its stakeholders. This survey provides a comprehensive summary of definitions, attributes, frameworks, techniques, and solutions for trustworthy EML. Specifically, we first emphasize the importance of trustworthy EML within the context of Sixth-Generation (6G) networks. We then discuss the necessity of trustworthiness from the perspective of challenges encountered during deployment and real-world application scenarios. Subsequently, we provide a preliminary definition of trustworthy EML and explore its key attributes. Following this, we introduce fundamental frameworks and enabling technologies for trustworthy EML systems, and provide an in-depth literature review of the latest solutions to enhance trustworthiness of EML. Finally, we discuss corresponding research challenges and open issues.
</details>
<details>
<summary>摘要</summary>
<sys>随着边缘计算（EC）和机器学习（ML）的融合，称为边缘机器学习（EML），已经成为了非常受到关注的研究领域，通过分布式网络资源进行共同训练和推理，以实现共同的目标。但是，EML受到了资源约束、多样网络环境和不同应用程序的服务需求等多种挑战，这些挑战共同影响了EML的可靠性，从而影响了它的投资者和用户的信任度。本文提供了Edge Machine Learning的全面概述，包括定义、特征、框架、技术和解决方案，以确保EML在6G网络中的可靠性。</sys>Here's the breakdown of the translation:<sys> - This tag indicates that the following text is a system-level translation, rather than a word-for-word translation.随着边缘计算（EC）和机器学习（ML）的融合 - This phrase translates to "With the convergence of edge computing and machine learning."称为边缘机器学习（EML） - This phrase translates to "known as edge machine learning."已经成为了非常受到关注的研究领域 - This phrase translates to "has already become a highly regarded research area."通过分布式网络资源进行共同训练和推理 - This phrase translates to "by utilizing distributed network resources to perform joint training and inference."以实现共同的目标 - This phrase translates to "to achieve common goals."但是 - This word translates to "but."EML受到了资源约束、多样网络环境和不同应用程序的服务需求等多种挑战 - This phrase translates to "EML faces various challenges due to resource constraints, heterogeneous network environments, and diverse service requirements of different applications."这些挑战共同影响了EML的可靠性 - This phrase translates to "these challenges collectively affect the trustworthiness of EML."从而影响了它的投资者和用户的信任度 - This phrase translates to "and thus affect the investors and users' trust in it."本文提供了Edge Machine Learning的全面概述，包括定义、特征、框架、技术和解决方案 - This phrase translates to "This article provides a comprehensive overview of Edge Machine Learning, including definitions, features, frameworks, techniques, and solutions."以确保EML在6G网络中的可靠性 - This phrase translates to "to ensure the reliability of EML in 6G networks."
</details></li>
</ul>
<hr>
<h2 id="MicroNAS-Memory-and-Latency-Constrained-Hardware-Aware-Neural-Architecture-Search-for-Time-Series-Classification-on-Microcontrollers"><a href="#MicroNAS-Memory-and-Latency-Constrained-Hardware-Aware-Neural-Architecture-Search-for-Time-Series-Classification-on-Microcontrollers" class="headerlink" title="MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers"></a>MicroNAS: Memory and Latency Constrained Hardware-Aware Neural Architecture Search for Time Series Classification on Microcontrollers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18384">http://arxiv.org/abs/2310.18384</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tk-king/MicroNas">https://github.com/tk-king/MicroNas</a></li>
<li>paper_authors: Tobias King, Yexu Zhou, Tobias Röddiger, Michael Beigl</li>
<li>for: 这个研究是为了自动搜寻和生成适合在资源有限的微控制器（MCU）上运行的神经网络架构，并生成相应的 tf-lite ML 模型。</li>
<li>methods: 这个系统使用了一个缓存服务器来搜寻和生成神经网络架构，并考虑了用户定义的执行时间和峰值内存耗用限制。</li>
<li>results: 这个系统可以实现高精度的识别结果（UCI-HAR 93.93%，SkodaR 96.33%），并在 Cortex-M4 MCU 上运行。<details>
<summary>Abstract</summary>
This paper presents MicroNAS, a system designed to automatically search and generate neural network architectures capable of classifying time series data on resource-constrained microcontrollers (MCUs) and generating standard tf-lite ML models. MicroNAS takes into account user-defined constraints on execution latency and peak memory consumption on a target MCU. This approach ensures that the resulting neural network architectures are optimised for the specific constraints and requirements of the MCU on which they are implemented. To achieve this, MicroNAS uses a look-up table estimation approach for accurate execution latency calculations, with a minimum error of only 1.02ms. This accurate latency estimation on MCUs sets it apart from other hardware-aware neural architecture search (HW-NAS) methods that use less accurate estimation techniques. Finally, MicroNAS delivers performance close to that of state-of-the-art models running on desktop computers, achieving high classification accuracies on recognised datasets (93.93% on UCI-HAR and 96.33% on SkodaR) while running on a Cortex-M4 MCU.
</details>
<details>
<summary>摘要</summary>
To achieve this, MicroNAS uses a look-up table estimation approach for accurate execution latency calculations, with a minimum error of only 1.02 milliseconds. This accurate latency estimation on MCUs sets it apart from other hardware-aware neural architecture search (HW-NAS) methods that use less accurate estimation techniques.Finally, MicroNAS delivers performance close to that of state-of-the-art models running on desktop computers, achieving high classification accuracies on recognized datasets (93.93% on UCI-HAR and 96.33% on SkodaR) while running on a Cortex-M4 MCU.
</details></li>
</ul>
<hr>
<h2 id="Lifting-the-Veil-Unlocking-the-Power-of-Depth-in-Q-learning"><a href="#Lifting-the-Veil-Unlocking-the-Power-of-Depth-in-Q-learning" class="headerlink" title="Lifting the Veil: Unlocking the Power of Depth in Q-learning"></a>Lifting the Veil: Unlocking the Power of Depth in Q-learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17915">http://arxiv.org/abs/2310.17915</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shao-Bo Lin, Tao Li, Shaojie Tang, Yao Wang, Ding-Xuan Zhou</li>
<li>for: 本文试图 theoretically verify the power of depth in deep Q-learning, and provide a solid theoretical foundation for its success in numerous applications.</li>
<li>methods: 本文使用 statistical learning theory 来rigorously prove that deep Q-learning outperforms traditional Q-learning, and demonstrate its good generalization error bound.</li>
<li>results: 研究发现，深度 Q-learning 的成功主要归功于深度神经网络（deep nets）的特殊性能，而不是它的大容量。 Additionally, the paper answers three key questions: Why does deep Q-learning perform so well? When does deep Q-learning perform better than traditional Q-learning? How many samples are required to achieve a specific prediction accuracy for deep Q-learning?<details>
<summary>Abstract</summary>
With the help of massive data and rich computational resources, deep Q-learning has been widely used in operations research and management science and has contributed to great success in numerous applications, including recommender systems, supply chains, games, and robotic manipulation. However, the success of deep Q-learning lacks solid theoretical verification and interpretability. The aim of this paper is to theoretically verify the power of depth in deep Q-learning. Within the framework of statistical learning theory, we rigorously prove that deep Q-learning outperforms its traditional version by demonstrating its good generalization error bound. Our results reveal that the main reason for the success of deep Q-learning is the excellent performance of deep neural networks (deep nets) in capturing the special properties of rewards namely, spatial sparseness and piecewise constancy, rather than their large capacities. In this paper, we make fundamental contributions to the field of reinforcement learning by answering to the following three questions: Why does deep Q-learning perform so well? When does deep Q-learning perform better than traditional Q-learning? How many samples are required to achieve a specific prediction accuracy for deep Q-learning? Our theoretical assertions are verified by applying deep Q-learning in the well-known beer game in supply chain management and a simulated recommender system.
</details>
<details>
<summary>摘要</summary>
通过庞大的数据和丰富的计算资源，深度Q学习在运筐研究和管理科学中广泛应用，并在多个应用中做出了卓越的成绩，包括推荐系统、供应链、游戏和机器人操作。然而，深度Q学习的成功尚未得到坚实的理论验证和可读性。本文的目标是从统计学学习理论的视角出发，确认深度Q学习的力量。我们在统计学学习理论的框架下，严格地证明了深度Q学习的泛化误差 bound 比传统Q学习更好。我们的结果表明，深度Q学习的成功主要归功于深度神经网络（深度网）在奖励特性上表现出色，而不是它的大容量。本文对抗习学习领域做出了基础性的贡献，回答了以下三个问题：深度Q学习为什么会表现 så well? 深度Q学习在哪些情况下表现更好于传统Q学习? 深度Q学习需要多少样本来达到特定的预测精度？我们的理论声明得到了在啤酒游戏和一个模拟的推荐系统中应用深度Q学习的实质验证。
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Knowledge-Gradient-Algorithm"><a href="#Improving-the-Knowledge-Gradient-Algorithm" class="headerlink" title="Improving the Knowledge Gradient Algorithm"></a>Improving the Knowledge Gradient Algorithm</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17901">http://arxiv.org/abs/2310.17901</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yhtang123/Intelligent-High-Efficiency-Energy-Conversion-System">https://github.com/yhtang123/Intelligent-High-Efficiency-Energy-Conversion-System</a></li>
<li>paper_authors: Yang Le, Gao Siyang, Ho Chin Pang</li>
<li>for: 该论文是解决最佳武器标识问题的一种策略，即选择带最大预期改进的测试方法来优化武器的估计。</li>
<li>methods: 该策略基于一个简单的想法，即选择导致最大预期改进的测试方法来优化武器的估计。然而，这种策略有限制，导致算法不是极限优化的。</li>
<li>results: 作者提出了一种改进的策略，即改进知识梯度（iKG）策略，它可以在 variant problems of BAI 中展现出更好的性能。在数学示例中，iKG 的性能也被证明是比 KG 更好的。<details>
<summary>Abstract</summary>
The knowledge gradient (KG) algorithm is a popular policy for the best arm identification (BAI) problem. It is built on the simple idea of always choosing the measurement that yields the greatest expected one-step improvement in the estimate of the best mean of the arms. In this research, we show that this policy has limitations, causing the algorithm not asymptotically optimal. We next provide a remedy for it, by following the manner of one-step look ahead of KG, but instead choosing the measurement that yields the greatest one-step improvement in the probability of selecting the best arm. The new policy is called improved knowledge gradient (iKG). iKG can be shown to be asymptotically optimal. In addition, we show that compared to KG, it is easier to extend iKG to variant problems of BAI, with the $\epsilon$-good arm identification and feasible arm identification as two examples. The superior performances of iKG on these problems are further demonstrated using numerical examples.
</details>
<details>
<summary>摘要</summary>
“知识梯度（KG）算法是一种受欢迎的策略 для最佳臂 Identification（BAI）问题。它基于简单的想法，就是总是选择测量，可以将最大化预期的一步改善在臂的最佳均值的估计。在这个研究中，我们显示出这个策略有限制，导致算法不是 asymptotically 优化的。我们随后提供了一个修正方案，通过一步前进的方式，选择测量，可以将最大化一步改善在臂选择的可能性。这个新策略被称为改善知识梯度（iKG）。iKG可以显示是 asymptotically 优化的。此外，我们显示了在 variant 问题中，iKG比KG更容易扩展，例如 $\epsilon$-good arm identification 和可行臂 identification 两个例子。iKG 在这些问题上的表现更加出色，通过数学例子进行说明。”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Submodel-Partitioning-in-Hierarchical-Federated-Learning-Algorithm-Design-and-Convergence-Analysis"><a href="#Submodel-Partitioning-in-Hierarchical-Federated-Learning-Algorithm-Design-and-Convergence-Analysis" class="headerlink" title="Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis"></a>Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17890">http://arxiv.org/abs/2310.17890</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenzhi Fang, Dong-Jun Han, Christopher G. Brinton</li>
<li>for: 提高资源受限的互联网智能设备（IoT）上的 federated learning（FL）规模化和效率化。</li>
<li>methods: 提出了一种新的分布式学习方法——层次独立子模型训练（HIST），通过在层次结构中对全球模型进行分区，使每个客户端只需训练一部分的全局模型，从而降低了计算&#x2F;存储成本并减轻了通信负担。</li>
<li>results: 通过数学分析和实验 validate HIST 能够在非对称损失函数下保证收敛性，并且在许多属性（如Cell数量、本地和全球聚合频率）的影响下对性能与效率进行了评估。实验结果表明，HIST 能够大幅减少通信成本，同时保持测试准确率不变。<details>
<summary>Abstract</summary>
Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional "star-topology" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex loss functions under mild assumptions, showing the impact of several attributes (e.g., number of cells, local and global aggregation frequency) on the performance-efficiency tradeoff. Finally, through numerical experiments, we verify that HIST is able to save communication costs by a wide margin while achieving the same target testing accuracy.
</details>
<details>
<summary>摘要</summary>
HIST 的关键想法是在每轮中对全球模型进行层次分区，将每个分区分配给不同的细胞，以便每个客户端只需训练自己的分区，而不需要与其他细胞进行通信。这样，每个客户端都可以降低计算/存储成本，同时减轻通信负担。我们分析了不同参数（例如细胞数、本地和全球汇总频率）对性能与效率的影响。最后，通过数值实验，我们证明了 HIST 可以减少通信成本，同时保持测试准确率不变。
</details></li>
</ul>
<hr>
<h2 id="Machine-Learning-Infused-Distributed-Optimization-for-Coordinating-Virtual-Power-Plant-Assets"><a href="#Machine-Learning-Infused-Distributed-Optimization-for-Coordinating-Virtual-Power-Plant-Assets" class="headerlink" title="Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets"></a>Machine Learning Infused Distributed Optimization for Coordinating Virtual Power Plant Assets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17882">http://arxiv.org/abs/2310.17882</a></li>
<li>repo_url: None</li>
<li>paper_authors: Meiyi Li, Javad Mohammadi<br>for: This paper aims to present a novel machine learning-assisted distributed optimization method for coordinating Virtual Power Plants (VPPs) and their associated Distributed Energy Resources (DERs).methods: The proposed method, named LOOP-MAC, utilizes a multi-agent coordination approach and neural network approximators to expedite the solution search.results: The LOOP-MAC method demonstrates accelerated solution times per iteration and significantly reduced convergence times compared to conventional centralized and distributed optimization methods.<details>
<summary>Abstract</summary>
Amid the increasing interest in the deployment of Distributed Energy Resources (DERs), the Virtual Power Plant (VPP) has emerged as a pivotal tool for aggregating diverse DERs and facilitating their participation in wholesale energy markets. These VPP deployments have been fueled by the Federal Energy Regulatory Commission's Order 2222, which makes DERs and VPPs competitive across market segments. However, the diversity and decentralized nature of DERs present significant challenges to the scalable coordination of VPP assets. To address efficiency and speed bottlenecks, this paper presents a novel machine learning-assisted distributed optimization to coordinate VPP assets. Our method, named LOOP-MAC(Learning to Optimize the Optimization Process for Multi-agent Coordination), adopts a multi-agent coordination perspective where each VPP agent manages multiple DERs and utilizes neural network approximators to expedite the solution search. The LOOP-MAC method employs a gauge map to guarantee strict compliance with local constraints, effectively reducing the need for additional post-processing steps. Our results highlight the advantages of LOOP-MAC, showcasing accelerated solution times per iteration and significantly reduced convergence times. The LOOP-MAC method outperforms conventional centralized and distributed optimization methods in optimization tasks that require repetitive and sequential execution.
</details>
<details>
<summary>摘要</summary>
在增加分布能源资源（DERs）的投入中，虚拟发电厂（VPP）已成为汇集多种DERs并促进其参与到总体能源市场中的关键工具。这些VPP部署受到联邦能源管理委员会的命令2222的推动，该命令使DERs和VPPs在市场 segments中竞争。然而，DERs的多样性和分散化带来了VPP资产的扩展协调的显著挑战。为了提高效率和速度瓶颈，本文提出了一种新的机器学习协助分布优化方法，称为LOOP-MAC（学习优化优化过程多代理协调）。LOOP-MAC方法采用多代理协调视角，每个VPP代理负责多个DERs，并使用神经网络approximators快速搜索解决方案。LOOP-MAC方法使用一个报表图来保证本地约束的严格遵从，从而减少了额外处理步骤的需要。我们的结果表明LOOP-MAC方法具有优势，其解决时间和趋势时间均显著减少。LOOP-MAC方法在需要重复和序列执行的优化任务中超过了传统中央化和分布式优化方法。
</details></li>
</ul>
<hr>
<h2 id="A-Sublinear-Time-Spectral-Clustering-Oracle-with-Improved-Preprocessing-Time"><a href="#A-Sublinear-Time-Spectral-Clustering-Oracle-with-Improved-Preprocessing-Time" class="headerlink" title="A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time"></a>A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17878">http://arxiv.org/abs/2310.17878</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ranran Shen, Pan Peng</li>
<li>for: 本文针对图像中具有强烈分割特征的图进行 spectral clustering  oracle 的设计。</li>
<li>methods: 本文使用的方法包括 preprocessing 和 query answering，均在 sublinear 时间内完成。</li>
<li>results: 本文的结果表明，对于具有较大内径导通率（至少为 $\varphi$）和较小外径导通率（至多为 $\varepsilon$）的图像，可以实现高效的 clustering membership queries。<details>
<summary>Abstract</summary>
We address the problem of designing a sublinear-time spectral clustering oracle for graphs that exhibit strong clusterability. Such graphs contain $k$ latent clusters, each characterized by a large inner conductance (at least $\varphi$) and a small outer conductance (at most $\varepsilon$). Our aim is to preprocess the graph to enable clustering membership queries, with the key requirement that both preprocessing and query answering should be performed in sublinear time, and the resulting partition should be consistent with a $k$-partition that is close to the ground-truth clustering. Previous oracles have relied on either a $\textrm{poly}(k)\log n$ gap between inner and outer conductances or exponential (in $k/\varepsilon$) preprocessing time. Our algorithm relaxes these assumptions, albeit at the cost of a slightly higher misclassification ratio. We also show that our clustering oracle is robust against a few random edge deletions. To validate our theoretical bounds, we conducted experiments on synthetic networks.
</details>
<details>
<summary>摘要</summary>
我们处理拥有强 clustering 性的图的问题，特别是对于具有至少 $\phi$ 内导通和最多 $\varepsilon$ 外导通的 $k$ 个粒子集的图。我们的目标是在子线性时间内进行类别查询，并且保持类别结果与真实分类相似。previous 的标志有两个假设：一是 $\textrm{poly}(k)\log n$ 内外导通之间的差，二是 exponential （在 $k/\varepsilon$ 上）的预processing 时间。我们的算法则关注这两个假设，但是会导致轻微的错分率增加。我们还证明了我们的类别标志对于几个随机边的删除而Robust。为了证明我们的理论上限，我们对Synthetic 网络进行实验。
</details></li>
</ul>
<hr>
<h2 id="From-Generative-AI-to-Generative-Internet-of-Things-Fundamentals-Framework-and-Outlooks"><a href="#From-Generative-AI-to-Generative-Internet-of-Things-Fundamentals-Framework-and-Outlooks" class="headerlink" title="From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks"></a>From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18382">http://arxiv.org/abs/2310.18382</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinbo Wen, Jiangtian Nie, Jiawen Kang, Dusit Niyato, Hongyang Du, Yang Zhang, Mohsen Guizani</li>
<li>for: 这篇论文旨在探讨基于生成人工智能（GAI）的生成互联网（GIoT）技术，以及其在不同领域的应用前景。</li>
<li>methods: 论文首先介绍了四种GAI技术，然后对各种可能的GIoT应用进行了探讨。最后，文章提出了一种基于GAI的安全激励机制框架，以解决GIoT中的主要挑战。</li>
<li>results: 文章通过一个现有的现场案例研究，利用生成扩散模型（GDM）设计有效的奖励合同，以吸引用户提供高质量感知数据。此外，文章还提出了一些未来研究的开放方向。<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (GAI) possesses the capabilities of generating realistic data and facilitating advanced decision-making. By integrating GAI into modern Internet of Things (IoT), Generative Internet of Things (GIoT) is emerging and holds immense potential to revolutionize various aspects of society, enabling more efficient and intelligent IoT applications, such as smart surveillance and voice assistants. In this article, we present the concept of GIoT and conduct an exploration of its potential prospects. Specifically, we first overview four GAI techniques and investigate promising GIoT applications. Then, we elaborate on the main challenges in enabling GIoT and propose a general GAI-based secure incentive mechanism framework to address them, in which we adopt Generative Diffusion Models (GDMs) for incentive mechanism designs and apply blockchain technologies for secure GIoT management. Moreover, we conduct a case study on modern Internet of Vehicle traffic monitoring, which utilizes GDMs to generate effective contracts for incentivizing users to contribute sensing data with high quality. Finally, we suggest several open directions worth investigating for the future popularity of GIoT.
</details>
<details>
<summary>摘要</summary>
优化的人工智能（GAI）具有生成真实数据和提供高级决策的能力。将GAI融入现代互联网器（IoT）后，生成互联网器（GIoT）得到了巨大的潜力，推动了社会各方面的改革，如智能监测和语音助手等智能应用。本文提出了GIoT的概念，并对其潜在可能性进行了探讨。 Specifically，我们首先介绍了四种GAI技术，然后研究了GIoT的潜在应用场景。然后，我们详细介绍了GIoT实现的主要挑战和一种基于GAI的安全奖励机制框架，其中采用生成扩散模型（GDMs）为奖励机制设计，并应用区块链技术来安全地管理GIoT。此外，我们进行了现代互联网器交通监测的实践案例，利用GDMs生成高质量感知数据的合法合约。最后，我们提出了未来GIoT的一些开放方向值得进一步探索。
</details></li>
</ul>
<hr>
<h2 id="Unveil-Sleep-Spindles-with-Concentration-of-Frequency-and-Time"><a href="#Unveil-Sleep-Spindles-with-Concentration-of-Frequency-and-Time" class="headerlink" title="Unveil Sleep Spindles with Concentration of Frequency and Time"></a>Unveil Sleep Spindles with Concentration of Frequency and Time</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18381">http://arxiv.org/abs/2310.18381</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rsbci/conceft-spindle">https://github.com/rsbci/conceft-spindle</a></li>
<li>paper_authors: Riki Shimizu, Hau-Tieng Wu</li>
<li>For: The paper aims to develop an accurate and interpretable algorithm for sleep spindle detection in EEG data, and to quantify the instantaneous frequencies of spindles.* Methods: The authors introduce a novel non-linear time-frequency analysis tool called “Concentration of Frequency and Time” (ConceFT), which effectively reduces stochastic EEG influence and enhances spindle visibility in the time-frequency representation. They also developed an automated spindle detection algorithm called ConceFT-Spindle (ConceFT-S), which is compared to two other algorithms (A7 and SUMO) using two benchmark databases (Dream and MASS).* Results: The results show that ConceFT-S achieves F1 scores of 0.749 in Dream and 0.786 in MASS, which is equivalent to or surpasses the performance of A7 and SUMO with statistical significance. Additionally, the authors reveal that spindle IF is generally nonlinear.Here are the three points in Simplified Chinese text:* For: 这个论文目的是开发一个准确和可解释的EEG数据中睡眠潮汐检测算法，并量化潮汐的快速频率。* Methods: 作者们引入了一种新的非线性时间频谱分析工具”集中频率和时间”(ConceFT),该工具有效减少了随机EEG的影响，使潮汐在时间频谱表示中更加明了潮汐。他们还开发了一个自动潮汐检测算法ConceFT-Spindle(ConceFT-S)，并与A7和SUMO两个算法进行比较使用了两个标准数据库(Dream和MASS)。* Results: 结果表明，ConceFT-S在Dream和MASS两个数据库中的F1分数分别为0.749和0.786，这与或超过A7和SUMO的性能有统计学上的显著性。此外，作者们还发现，潮汐的快速频率通常是非线性的。<details>
<summary>Abstract</summary>
Objective: Sleep spindles contain crucial brain dynamics information. We introduce the novel non-linear time-frequency analysis tool 'Concentration of Frequency and Time' (ConceFT) to create an interpretable automated algorithm for sleep spindle annotation in EEG data and to measure spindle instantaneous frequencies (IFs). Methods: ConceFT effectively reduces stochastic EEG influence, enhancing spindle visibility in the time-frequency representation. Our automated spindle detection algorithm, ConceFT-Spindle (ConceFT-S), is compared to A7 (non-deep learning) and SUMO (deep learning) using Dream and MASS benchmark databases. We also quantify spindle IF dynamics. Results: ConceFT-S achieves F1 scores of 0.749 in Dream and 0.786 in MASS, which is equivalent to or surpass A7 and SUMO with statistical significance. We reveal that spindle IF is generally nonlinear. Conclusion: ConceFT offers an accurate, interpretable EEG-based sleep spindle detection algorithm and enables spindle IF quantification.
</details>
<details>
<summary>摘要</summary>
目标：睡眠尖峰含有关键脑动态信息。我们介绍了一种新的非线性时域分析工具“时域频率卷积”（ConceFT），以创建可解释的自动化睡眠尖峰标注算法，并测量尖峰快速频率（IF）的动态变化。方法：ConceFT可以有效减少随机的EEG影响，使睡眠尖峰在时域表示更加明显。我们的自动化睡眠尖峰检测算法ConceFT-Spindle（ConceFT-S）与A7（非深度学习）和SUMO（深度学习）在梦境和MASS数据库上进行比较，并评估尖峰IF动态变化。结果：ConceFT-S在梦境和MASS上的F1分数分别为0.749和0.786，与A7和SUMO相当或超过，这种差异为统计学上的显著性。我们发现尖峰IF通常是非线性的。结论：ConceFT提供了一种准确可解释的EEG基于睡眠尖峰检测算法，并允许量化尖峰IF的动态变化。
</details></li>
</ul>
<hr>
<h2 id="Boosting-Data-Analytics-With-Synthetic-Volume-Expansion"><a href="#Boosting-Data-Analytics-With-Synthetic-Volume-Expansion" class="headerlink" title="Boosting Data Analytics With Synthetic Volume Expansion"></a>Boosting Data Analytics With Synthetic Volume Expansion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17848">http://arxiv.org/abs/2310.17848</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaotong Shen, Yifei Liu, Rex Shen</li>
<li>for: 本研究旨在探讨统计方法在生成的数据上的准确性，并提出一种基于生成模型的数据生成框架，以 Addressing data scarcity and privacy concerns while enhancing the performance of statistical methods.</li>
<li>methods: 本研究使用了高优化的生成模型，如表格扩散和生成预训练变换模型，以生成高准确性的生成数据。这些模型在训练过程中受到了相关研究的指导，以提高生成数据的准确性。</li>
<li>results: 研究发现，随着生成数据的增加，统计方法的误差最初逐渐减少，但 eventually可能增加或折衣。这种现象被称为“生成效应”，它表明在生成数据中复制原始数据的分布时存在一个“反射点”，即特定的误差度量的优化阈值。通过三个案例研究，包括文本感知分析、结构化数据预测和表格数据推理，我们证明了这种框架的效果，并将其与传统方法进行比较。<details>
<summary>Abstract</summary>
Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distributions, highlights a "reflection point"--an optimal threshold in the size of synthetic data determined by specific error metrics. Through three illustrative case studies-sentiment analysis of texts, predictive modeling of structured data, and inference in tabular data--we demonstrate the effectiveness of this framework over traditional ones. We underline its potential to amplify various statistical methods, including gradient boosting for prediction and hypothesis testing, thereby underscoring the transformative potential of synthetic data generation in data science.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate "Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distributions, highlights a "reflection point"--an optimal threshold in the size of synthetic data determined by specific error metrics. Through three illustrative case studies-sentiment analysis of texts, predictive modeling of structured data, and inference in tabular data--we demonstrate the effectiveness of this framework over traditional ones. We underline its potential to amplify various statistical methods, including gradient boosting for prediction and hypothesis testing, thereby underscoring the transformative potential of synthetic data generation in data science."中文简体版：<<SYS>>生成数据领域，人工智能生成的核心，数据科学领域发生了一场 парадигShift，通过地址数据缺乏和隐私问题，实现了无前例的性能。随着生成数据的普及，关注统计方法在生成数据上的准确性问题 arise。本文介绍了生成数据分析框架。这个框架利用高准确度的生成数据，由高级模型如表 diffusion和生成预训练 transformer 模型生成，这些模型在原始数据上训练。在这个框架中，我们发现了一种生成效应：在生成数据上使用统计方法的错误在初始阶段随着添加生成数据减少，但可能在某些点上增加或稳定。这种现象基于生成数据 Distribution 复杂性，表明一个 "反射点"--一个特定的错误指标决定的最佳大小。通过三个案例研究--文本情感分析、结构化数据预测和表格数据推理--我们示出了这个框架的效果，比传统方法更高。我们强调了它的潜在作用，包括权度提升、预测和假设测试，从而强调生成数据生成在数据科学中的转型潜力。
</details></li>
</ul>
<hr>
<h2 id="A-Data-Centric-Online-Market-for-Machine-Learning-From-Discovery-to-Pricing"><a href="#A-Data-Centric-Online-Market-for-Machine-Learning-From-Discovery-to-Pricing" class="headerlink" title="A Data-Centric Online Market for Machine Learning: From Discovery to Pricing"></a>A Data-Centric Online Market for Machine Learning: From Discovery to Pricing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17843">http://arxiv.org/abs/2310.17843</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minbiao Han, Jonathan Light, Steven Xia, Sainyam Galhotra, Raul Castro Fernandez, Haifeng Xu</li>
<li>for: 这篇论文是为了解决机器学习（ML）领域的数据问题而写的，即将数据从多个数据持有者中匹配到ML任务中，以提高ML模型的性能和可用性。</li>
<li>methods: 这篇论文使用了自动匹配算法来匹配ML任务和数据，以及一种新的价格机制来激励ML用户参与到市场中。</li>
<li>results: 论文的实验结果表明，这些新技术可以有效地匹配ML任务和数据，并且可以鼓励ML用户参与到市场中，从而提高ML模型的性能和可用性。<details>
<summary>Abstract</summary>
Data fuels machine learning (ML) - rich and high-quality training data is essential to the success of ML. However, to transform ML from the race among a few large corporations to an accessible technology that serves numerous normal users' data analysis requests, there still exist important challenges. One gap we observed is that many ML users can benefit from new data that other data owners possess, whereas these data owners sit on piles of data without knowing who can benefit from it. This gap creates the opportunity for building an online market that can automatically connect supply with demand. While online matching markets are prevalent (e.g., ride-hailing systems), designing a data-centric market for ML exhibits many unprecedented challenges.   This paper develops new techniques to tackle two core challenges in designing such a market: (a) to efficiently match demand with supply, we design an algorithm to automatically discover useful data for any ML task from a pool of thousands of datasets, achieving high-quality matching between ML models and data; (b) to encourage market participation of ML users without much ML expertise, we design a new pricing mechanism for selling data-augmented ML models. Furthermore, our market is designed to be API-compatible with existing online ML markets like Vertex AI and Sagemaker, making it easy to use while providing better results due to joint data and model search. We envision that the synergy of our data and model discovery algorithm and pricing mechanism will be an important step towards building a new data-centric online market that serves ML users effectively.
</details>
<details>
<summary>摘要</summary>
数据驱动机器学习（ML）——高质量和丰富的训练数据是ML的成功关键。然而，将ML从几家大公司的竞赛转变为让常见用户的数据分析请求的可 accessible 技术，仍存在重要挑战。我们发现了一个差距：许多ML用户可以从其他数据所有者手中获得新的数据，而这些数据所有者拥有大量数据，不知道谁可以从中受益。这个差距创造了建立一个在线市场的机会，可以自动连接供应和需求。虽然在线匹配市场是普遍的（例如，乘车应用程序），但设计一个专门为ML的数据市场具有许多前所未有的挑战。本文提出了新的技术来解决两个核心挑战：(a) 高效匹配需求和供应，我们设计了一个自动从千余个数据集中找到适用于任何ML任务的有用数据，以实现高质量的匹配 междуML模型和数据。(b) 鼓励ML用户 без ML专业知识参与市场，我们设计了一种新的价格机制来销售数据增强ML模型。此外，我们的市场采用API兼容于现有的在线ML市场 like Vertex AI和Sagemaker，使其易于使用，同时提供更好的结果由于共同数据和模型搜索。我们认为，我们的数据和模型发现算法和价格机制的共同作用将是建立一个新的数据驱动的在线市场的重要一步。
</details></li>
</ul>
<hr>
<h2 id="Positional-Encoding-based-Resident-Identification-in-Multi-resident-Smart-Homes"><a href="#Positional-Encoding-based-Resident-Identification-in-Multi-resident-Smart-Homes" class="headerlink" title="Positional Encoding-based Resident Identification in Multi-resident Smart Homes"></a>Positional Encoding-based Resident Identification in Multi-resident Smart Homes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17836">http://arxiv.org/abs/2310.17836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiyi Song, Dipankar Chaki, Abdallah Lakhdari, Athman Bouguettaya</li>
<li>for: 本研究旨在提出一种新的居民身份验证框架，用于在智能环境中识别多名occupant。</li>
<li>methods: 该框架使用基于 pozitional 编码的特征提取模型，并使用Node2Vec算法将图转换为高维节点嵌入。一个Long Short-Term Memory（LSTM）模型用于预测依据时间序列感知器事件的居民身份。</li>
<li>results: 广泛的实验表明，提出的方案可以有效地识别多名occupant在智能环境中。两个实际数据集的评估结果显示，该方案的准确率分别为94.5%和87.9%。<details>
<summary>Abstract</summary>
We propose a novel resident identification framework to identify residents in a multi-occupant smart environment. The proposed framework employs a feature extraction model based on the concepts of positional encoding. The feature extraction model considers the locations of homes as a graph. We design a novel algorithm to build such graphs from layout maps of smart environments. The Node2Vec algorithm is used to transform the graph into high-dimensional node embeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the identities of residents using temporal sequences of sensor events with the node embeddings. Extensive experiments show that our proposed scheme effectively identifies residents in a multi-occupant environment. Evaluation results on two real-world datasets demonstrate that our proposed approach achieves 94.5% and 87.9% accuracy, respectively.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的居民标识框架，用于在多occupant智能环境中识别居民。我们的框架使用基于 pozitional 编码的特征提取模型，该模型考虑了智能环境的布局地图。我们提出了一种新的算法，用于从布局地图中生成图形。然后，我们使用 Node2Vec 算法将图形转换成高维节点嵌入。我们引入了一个长期快速储存（LSTM）模型，用于预测基于时间序列的感知事件的居民身份。广泛的实验表明，我们的提议方案可以有效地识别多occupant环境中的居民。两个实际数据集的评估结果表明，我们的方法可以达到94.5%和87.9%的准确率。
</details></li>
</ul>
<hr>
<h2 id="Hybrid-Optical-Turbulence-Models-Using-Machine-Learning-and-Local-Measurements"><a href="#Hybrid-Optical-Turbulence-Models-Using-Machine-Learning-and-Local-Measurements" class="headerlink" title="Hybrid Optical Turbulence Models Using Machine Learning and Local Measurements"></a>Hybrid Optical Turbulence Models Using Machine Learning and Local Measurements</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17829">http://arxiv.org/abs/2310.17829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Jellen, Charles Nelson, John Burkhardt, Cody Brownell</li>
<li>for: 这个研究的目的是为了精确预测大气光学震荡在局部环境中，以便估算自由空间光学系统的性能。</li>
<li>methods: 这个研究使用了机器学习 Informed 混合模型框架，结合了一些基本的大气macro-meteorological模型和本地观测数据，以提高预测的精度。</li>
<li>results: 研究发现，这个混合模型可以比基本的大气macro-meteorological模型和机器学习模型更好地预测大气光学震荡的性能，尤其是在训练数据少的情况下。<details>
<summary>Abstract</summary>
Accurate prediction of atmospheric optical turbulence in localized environments is essential for estimating the performance of free-space optical systems. Macro-meteorological models developed to predict turbulent effects in one environment may fail when applied in new environments. However, existing macro-meteorological models are expected to offer some predictive power. Building a new model from locally-measured macro-meteorology and scintillometer readings can require significant time and resources, as well as a large number of observations. These challenges motivate the development of a machine-learning informed hybrid model framework. By combining some baseline macro-meteorological model with local observations, hybrid models were trained to improve upon the predictive power of each baseline model. Comparisons between the performance of the hybrid models, the selected baseline macro-meteorological models, and machine-learning models trained only on local observations highlight potential use cases for the hybrid model framework when local data is expensive to collect. Both the hybrid and data-only models were trained using the Gradient Boosted Decision Tree (GBDT) architecture with a variable number of in-situ meteorological observations. The hybrid and data-only models were found to outperform three baseline macro-meteorological models, even for low numbers of observations, in some cases as little as one day. For the first baseline macro-meteorological model investigated, the hybrid model achieves an estimated 29% reduction in mean absolute error (MAE) using only one days-equivalent of observation, growing to 41% after only two days, and 68% after 180 days-equivalent training data. The number of days-equivalent training data required is potentially indicative of the seasonal variation in the local microclimate and its propagation environment.
</details>
<details>
<summary>摘要</summary>
准确预测大气光学抖振在本地化环境中是自由空间光学系统性能预测的关键。 macro-метеорологические模型在不同环境中预测抖振效果可能失败，但现有的 macro-метеорологические模型仍然可以提供一定的预测力。 基于本地测量的 macro-метеорологи和抖振仪读数建立新模型可能需要较长的时间和资源，以及大量观测数据。这些挑战驱动了开发一种机器学习 Informed 混合模型框架。通过将基线 macro-метеорологических模型与本地观测数据结合，混合模型可以提高每个基线模型的预测力。对比 hybrid 模型、选择的基线 macro-метеорологи models 和只使用本地观测数据训练的机器学习模型， hybrid 模型在一些情况下可以更好地预测抖振效果。使用 Gradient Boosted Decision Tree (GBDT) 架构，hybrid 模型和数据 только模型都被训练使用本地 meteorological 观测数据。在一些情况下，hybrid 模型可以在只需一天的观测数据量下表现出较好的预测效果，而且随着训练数据量的增加，hybrid 模型的性能会得到进一步改善。对于第一个基eline macro-метеорологи models  investigated，hybrid 模型可以在一天的训练数据量下实现了相对于基线模型的29%的减少 Mean Absolute Error (MAE)，随着训练数据量的增加，hybrid 模型的性能会得到进一步改善。这些结果表明了hybrid 模型在本地数据质量较低的情况下的可行性。 hybrid 模型和数据只模型的训练需要的天数相对于季节变化的本地微气候和其传播环境可能有关。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/27/cs.LG_2023_10_27/" data-id="clogxf3oz00qp5xra5c6ggq9w" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_10_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/27/eess.IV_2023_10_27/" class="article-date">
  <time datetime="2023-10-27T09:00:00.000Z" itemprop="datePublished">2023-10-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/27/eess.IV_2023_10_27/">eess.IV - 2023-10-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="FPM-INR-Fourier-ptychographic-microscopy-image-stack-reconstruction-using-implicit-neural-representations"><a href="#FPM-INR-Fourier-ptychographic-microscopy-image-stack-reconstruction-using-implicit-neural-representations" class="headerlink" title="FPM-INR: Fourier ptychographic microscopy image stack reconstruction using implicit neural representations"></a>FPM-INR: Fourier ptychographic microscopy image stack reconstruction using implicit neural representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18529">http://arxiv.org/abs/2310.18529</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hwzhou2020/fpm_inr">https://github.com/hwzhou2020/fpm_inr</a></li>
<li>paper_authors: Haowen Zhou, Brandon Y. Feng, Haiyun Guo, Siyu, Lin, Mingshu Liang, Christopher A. Metzler, Changhuei Yang<br>for:* 这个论文是为了提高快速大型远程生物学图像分析。methods:* 这个论文使用了物理学基础模型和隐藏神经网络表示（INR）来重建快速大型远程生物学图像。results:* 比traditional FPM算法快速25倍，内存占用量减少80倍。<details>
<summary>Abstract</summary>
Image stacks provide invaluable 3D information in various biological and pathological imaging applications. Fourier ptychographic microscopy (FPM) enables reconstructing high-resolution, wide field-of-view image stacks without z-stack scanning, thus significantly accelerating image acquisition. However, existing FPM methods take tens of minutes to reconstruct and gigabytes of memory to store a high-resolution volumetric scene, impeding fast gigapixel-scale remote digital pathology. While deep learning approaches have been explored to address this challenge, existing methods poorly generalize to novel datasets and can produce unreliable hallucinations. This work presents FPM-INR, a compact and efficient framework that integrates physics-based optical models with implicit neural representations (INR) to represent and reconstruct FPM image stacks. FPM-INR is agnostic to system design or sample types and does not require external training data. In our demonstrated experiments, FPM-INR substantially outperforms traditional FPM algorithms with up to a 25-fold increase in speed and an 80-fold reduction in memory usage for continuous image stack representations.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="TabAttention-Learning-Attention-Conditionally-on-Tabular-Data"><a href="#TabAttention-Learning-Attention-Conditionally-on-Tabular-Data" class="headerlink" title="TabAttention: Learning Attention Conditionally on Tabular Data"></a>TabAttention: Learning Attention Conditionally on Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18129">http://arxiv.org/abs/2310.18129</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sanoscience/tab-attention">https://github.com/sanoscience/tab-attention</a></li>
<li>paper_authors: Michal K. Grzeszczyk, Szymon Płotka, Beata Rebizant, Katarzyna Kosińska-Kaczyńska, Michał Lipa, Robert Brawura-Biskupski-Samaha, Przemysław Korzeniowski, Tomasz Trzciński, Arkadiusz Sitek</li>
<li>for: 这 paper 的目的是提出一种基于 Convolutional Neural Networks (CNNs) 的 novel 模块 TabAttention，用于combine 图像和表格数据进行预测。</li>
<li>methods: 该 paper 使用了一种叫做 Convolutional Block Attention Module 的模块，并将其扩展到 3D 空间，使用多头自注意力学习 attention maps。此外， authors 还增强了所有的注意模块，通过将表格数据嵌入。</li>
<li>results: 据 authors 的实验结果，TabAttention 可以超过临床医生和现有的方法，用于FBW 预测。这种新的方法有potential 用于各种临床工作流程中， где 图像和表格数据相结合。<details>
<summary>Abstract</summary>
Medical data analysis often combines both imaging and tabular data processing using machine learning algorithms. While previous studies have investigated the impact of attention mechanisms on deep learning models, few have explored integrating attention modules and tabular data. In this paper, we introduce TabAttention, a novel module that enhances the performance of Convolutional Neural Networks (CNNs) with an attention mechanism that is trained conditionally on tabular data. Specifically, we extend the Convolutional Block Attention Module to 3D by adding a Temporal Attention Module that uses multi-head self-attention to learn attention maps. Furthermore, we enhance all attention modules by integrating tabular data embeddings. Our approach is demonstrated on the fetal birth weight (FBW) estimation task, using 92 fetal abdominal ultrasound video scans and fetal biometry measurements. Our results indicate that TabAttention outperforms clinicians and existing methods that rely on tabular and/or imaging data for FBW prediction. This novel approach has the potential to improve computer-aided diagnosis in various clinical workflows where imaging and tabular data are combined. We provide a source code for integrating TabAttention in CNNs at https://github.com/SanoScience/Tab-Attention.
</details>
<details>
<summary>摘要</summary>
医疗数据分析经常结合图像和表格数据处理使用机器学习算法。 previous studies have investigated the impact of attention mechanisms on deep learning models, but few have explored integrating attention modules and tabular data. In this paper, we introduce TabAttention, a novel module that enhances the performance of Convolutional Neural Networks (CNNs) with an attention mechanism that is trained conditionally on tabular data. Specifically, we extend the Convolutional Block Attention Module to 3D by adding a Temporal Attention Module that uses multi-head self-attention to learn attention maps. Furthermore, we enhance all attention modules by integrating tabular data embeddings. Our approach is demonstrated on the fetal birth weight (FBW) estimation task, using 92 fetal abdominal ultrasound video scans and fetal biometry measurements. Our results indicate that TabAttention outperforms clinicians and existing methods that rely on tabular and/or imaging data for FBW prediction. This novel approach has the potential to improve computer-aided diagnosis in various clinical workflows where imaging and tabular data are combined. We provide a source code for integrating TabAttention in CNNs at <https://github.com/SanoScience/Tab-Attention>.
</details></li>
</ul>
<hr>
<h2 id="Hyper-Skin-A-Hyperspectral-Dataset-for-Reconstructing-Facial-Skin-Spectra-from-RGB-Images"><a href="#Hyper-Skin-A-Hyperspectral-Dataset-for-Reconstructing-Facial-Skin-Spectra-from-RGB-Images" class="headerlink" title="Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial Skin-Spectra from RGB Images"></a>Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial Skin-Spectra from RGB Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17911">http://arxiv.org/abs/2310.17911</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hyperspectral-skin/hyper-skin-2023">https://github.com/hyperspectral-skin/hyper-skin-2023</a></li>
<li>paper_authors: Pai Chet Ng, Zhixiang Chi, Yannick Verdie, Juwei Lu, Konstantinos N. Plataniotis<br>for:* 这个论文是为了探讨人脸皮肤的各种特征和问题而设计的。methods:* 这个论文使用了推杆式彩色扫描仪获取了各种彩色图像，并使用了这些图像来重建人脸皮肤的各种spectra特征。results:* 这个论文通过使用现有的state-of-the-art模型对41个带spectra特征的数据进行了重建，并得到了较高的重建精度。<details>
<summary>Abstract</summary>
We introduce Hyper-Skin, a hyperspectral dataset covering wide range of wavelengths from visible (VIS) spectrum (400nm - 700nm) to near-infrared (NIR) spectrum (700nm - 1000nm), uniquely designed to facilitate research on facial skin-spectra reconstruction. By reconstructing skin spectra from RGB images, our dataset enables the study of hyperspectral skin analysis, such as melanin and hemoglobin concentrations, directly on the consumer device. Overcoming limitations of existing datasets, Hyper-Skin consists of diverse facial skin data collected with a pushbroom hyperspectral camera. With 330 hyperspectral cubes from 51 subjects, the dataset covers the facial skin from different angles and facial poses. Each hyperspectral cube has dimensions of 1024$\times$1024$\times$448, resulting in millions of spectra vectors per image. The dataset, carefully curated in adherence to ethical guidelines, includes paired hyperspectral images and synthetic RGB images generated using real camera responses. We demonstrate the efficacy of our dataset by showcasing skin spectra reconstruction using state-of-the-art models on 31 bands of hyperspectral data resampled in the VIS and NIR spectrum. This Hyper-Skin dataset would be a valuable resource to NeurIPS community, encouraging the development of novel algorithms for skin spectral reconstruction while fostering interdisciplinary collaboration in hyperspectral skin analysis related to cosmetology and skin's well-being. Instructions to request the data and the related benchmarking codes are publicly available at: \url{https://github.com/hyperspectral-skin/Hyper-Skin-2023}.
</details>
<details>
<summary>摘要</summary>
我们介绍Hyper-Skin，一个涵盖各种波长的对称资料集，从可见光（VIS） спектル（400nm - 700nm）至近红外（NIR） спектル（700nm - 1000nm）。这个对称资料集专门设计来推进对面部肤 Spectra 重建的研究，通过从RGB图像中推算肤 Spectra，实现在consumer device上进行对肤 Spectra 的分析。在现有资料集的限制之下，Hyper-Skin 的资料集包括多样化的面部肤 Data，透过推挤式对称摄取器收集。资料集包含51名志愿者的330个对称摄取，每个对称摄取都有1024x1024x448的对称立方体，总共有多万个特征向量。资料集遵循道德指南，并包括对称摄取和Synthetic RGB图像，这些图像是使用真实摄像头的回应生成的。我们显示Hyper-Skin 资料集可以用现代模型进行肤 Spectra 重建，并在31个对称摄取中显示了肤 Spectra 的重建。这个Hyper-Skin 资料集将是neurIPS社区的一个宝贵资源，激发开发新的肤 Spectra 重建算法，并促进对肤 Spectra 分析的跨学科合作。请从以下连结获取资料和相关的benchmarking代码：<https://github.com/hyperspectral-skin/Hyper-Skin-2023>
</details></li>
</ul>
<hr>
<h2 id="CPIA-Dataset-A-Comprehensive-Pathological-Image-Analysis-Dataset-for-Self-supervised-Learning-Pre-training"><a href="#CPIA-Dataset-A-Comprehensive-Pathological-Image-Analysis-Dataset-for-Self-supervised-Learning-Pre-training" class="headerlink" title="CPIA Dataset: A Comprehensive Pathological Image Analysis Dataset for Self-supervised Learning Pre-training"></a>CPIA Dataset: A Comprehensive Pathological Image Analysis Dataset for Self-supervised Learning Pre-training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17902">http://arxiv.org/abs/2310.17902</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/zhanglab2021/cpia_dataset">https://github.com/zhanglab2021/cpia_dataset</a></li>
<li>paper_authors: Nan Ying, Yanli Lei, Tianyi Zhang, Shangqing Lyu, Chunhui Li, Sicheng Chen, Zeyu Liu, Yu Zhao, Guanglei Zhang</li>
<li>for: 这个论文主要是为了提出一个大规模的自助学习预训练数据集，以提高计算机辅助诊断中的病理图像分析效果。</li>
<li>methods: 这个论文使用了自助学习（SSL）预训练方法，不需要样本水平标签，从而解决了临床标注的昂贵问题。</li>
<li>results: 这个论文提出了一个大规模的病理图像分析（CPIA）数据集，包含21427877个标准化图像，覆盖了48种器官&#x2F;组织和100多种疾病，并提供了一些国际顶尖的基线模型和下游评估方法。<details>
<summary>Abstract</summary>
Pathological image analysis is a crucial field in computer-aided diagnosis, where deep learning is widely applied. Transfer learning using pre-trained models initialized on natural images has effectively improved the downstream pathological performance. However, the lack of sophisticated domain-specific pathological initialization hinders their potential. Self-supervised learning (SSL) enables pre-training without sample-level labels, which has great potential to overcome the challenge of expensive annotations. Thus, studies focusing on pathological SSL pre-training call for a comprehensive and standardized dataset, similar to the ImageNet in computer vision. This paper presents the comprehensive pathological image analysis (CPIA) dataset, a large-scale SSL pre-training dataset combining 103 open-source datasets with extensive standardization. The CPIA dataset contains 21,427,877 standardized images, covering over 48 organs/tissues and about 100 kinds of diseases, which includes two main data types: whole slide images (WSIs) and characteristic regions of interest (ROIs). A four-scale WSI standardization process is proposed based on the uniform resolution in microns per pixel (MPP), while the ROIs are divided into three scales artificially. This multi-scale dataset is built with the diagnosis habits under the supervision of experienced senior pathologists. The CPIA dataset facilitates a comprehensive pathological understanding and enables pattern discovery explorations. Additionally, to launch the CPIA dataset, several state-of-the-art (SOTA) baselines of SSL pre-training and downstream evaluation are specially conducted. The CPIA dataset along with baselines is available at https://github.com/zhanglab2021/CPIA_Dataset.
</details>
<details>
<summary>摘要</summary>
临床图像分析是计算机辅助诊断中的关键领域，深度学习在这个领域中广泛应用。使用预训练模型 initialized 自自然图像的传输学习可以有效提高下渠道的临床性能。然而，由于精细的域专专业 initialize 的缺乏，使得它们的潜力受限。无监督学习（SSL）可以无需样本级别标签进行预训练，这种技术具有巨大的潜力以超越临床标注的成本高。因此，关注临床SSL预训练的研究需要一个完整的、标准化的数据集，类似于计算机视觉领域的ImageNet。本文提出了临床图像分析（CPIA）数据集，这是一个大规模的SSL预训练数据集， combinining 103个开源数据集，通过了广泛的标准化。CPIA数据集包含21,427,877个标准化图像，覆盖了48种器官/组织和约100种疾病，其中包括两种主要数据类型：整幅影像（WSIs）和特征区域 interest（ROIs）。基于MPP（微米每平方Pixel）的均匀分辨率，我们提出了一种四级WSIs标准化过程，而ROIs则被 искусственно分为三级。这个多级数据集是根据经验丰富的高级医生的诊断习惯建立的。CPIA数据集促进了全面的临床理解，并允许探索 Pattern discovery。此外，为了推出CPIA数据集，我们特别进行了一些现状顶峰（SOTA）的SSL预训练和下渠道评估。CPIA数据集、基elines都可以在https://github.com/zhanglab2021/CPIA_Dataset上下载。
</details></li>
</ul>
<hr>
<h2 id="Towards-optimal-multimode-fiber-imaging-by-leveraging-input-polarization-and-conditional-generative-adversarial-networks"><a href="#Towards-optimal-multimode-fiber-imaging-by-leveraging-input-polarization-and-conditional-generative-adversarial-networks" class="headerlink" title="Towards optimal multimode fiber imaging by leveraging input polarization and conditional generative adversarial networks"></a>Towards optimal multimode fiber imaging by leveraging input polarization and conditional generative adversarial networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17889">http://arxiv.org/abs/2310.17889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jawaria Maqbool, Syed Talal Hassan, M. Imran Cheema</li>
<li>for: 实现实用的多模式纤维器成像</li>
<li>methods: 使用深度学习技术和conditional generative adversarial network（CGAN）模型</li>
<li>results: 实验显示，输入光波 polarization 状态对成像质量产生重要影响，并且通过控制输入光波 polarization 状态，可以实现最佳的成像效果。<details>
<summary>Abstract</summary>
Deep learning techniques provide a plausible route towards achieving practical imaging through multimode fibers. However, the results produced by these methods are often influenced by physical factors like temperature, fiber length, external perturbations, and polarization state of the input light. The impact of other factors, except input light polarization, has been discussed in the literature for imaging applications. The input polarization has been considered by researchers while looking at the characterization and control of polarization in multimode fibers. Here, we show experimentally that the state of polarization of light, being injected at multimode fiber input, affects the fidelity of reconstructed images from speckle patterns. Certain polarization states produce high-quality images at fiber output, while some yield degraded results. We have designed a conditional generative adversarial network~(CGAN) for image regeneration at various degrees of input light polarization. We demonstrate that in the case of multimode fibers that are held fixed, optimal imaging can be achieved by leveraging our CGAN model with the input light polarization state, where the fidelity of images is maximum. Our work exhibits high average structural similarity index values exceeding 0.9, surpassing the previously reported value of 0.8772. We also show that the model can be generalized to image adequately for all input light polarization states when the fiber has bends or twists. We anticipate our work will be a stepping stone toward developing high-resolution and less invasive multimode fiber endoscopes.
</details>
<details>
<summary>摘要</summary>
深度学习技术可能提供实用的多模式纤维器成像方法。然而，这些方法的结果经常受到物理因素的影响，如温度、纤维长度、外部干扰和输入光的极化状态。关于成像应用，已经在文献中讨论了其他因素的影响。而输入光的极化状态则在研究人员中被视为 Characterization and control of polarization in multimode fibers。我们的实验表明，输入多模式纤维器的光极化状态会影响生成的图像质量。某些极化状态可以生成高质量的图像，而其他状态则会导致图像受损。我们开发了一种基于CGAN的图像恢复模型，可以在不同的输入光极化状态下进行图像恢复。我们的结果显示，当多模式纤维器保持不动时，我们的模型可以在不同的输入光极化状态下实现最佳的成像。我们的结果超过了之前报道的最高值0.8772，并且表明我们的模型可以在纤维器弯曲或扭转时进行图像恢复。我们预计我们的工作将成为高分辨率和 menos invasive的多模式纤维器镜头的开端。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/27/eess.IV_2023_10_27/" data-id="clogxf3t8016f5xraa5yd3e2b" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_10_27" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/27/eess.SP_2023_10_27/" class="article-date">
  <time datetime="2023-10-27T08:00:00.000Z" itemprop="datePublished">2023-10-27</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/27/eess.SP_2023_10_27/">eess.SP - 2023-10-27</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Generalized-Firefly-Algorithm-for-Optimal-Transmit-Beamforming"><a href="#Generalized-Firefly-Algorithm-for-Optimal-Transmit-Beamforming" class="headerlink" title="Generalized Firefly Algorithm for Optimal Transmit Beamforming"></a>Generalized Firefly Algorithm for Optimal Transmit Beamforming</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18460">http://arxiv.org/abs/2310.18460</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tuan Anh Le, Xin-She Yang<br>for: 这种纸是用于解决多变量函数目标和约束的优化框架中的一种通用火fly算法(FA)。methods: 提议使用一种通用的火fly算法(FA)来解决下降传输焊缝问题，包括约束函数和目标函数为多变量独立优化变量。results: 对四个示例问题进行了解释，包括经典传输焊缝、认知焊缝、嵌入智能表面帮助传输焊缝和嵌入智能表面帮助无线电力传输。计算复杂性分析表明，在大天线 режимом下，提议的FA方法需要较少的计算复杂性，但需要更高的复杂性 than iterative和successive convex approximation（SCA）方法。实验结果表明，提议的FA方法可以达到与IPM的全球最优解相同的解决方案，而且在经典传输焊缝、RIS帮助传输焊缝和RIS帮助无线电力传输中，FA方法可以超越iterative、IPM和SCA方法。<details>
<summary>Abstract</summary>
This paper proposes a generalized Firefly Algorithm (FA) to solve an optimization framework having objective function and constraints as multivariate functions of independent optimization variables. Four representative examples of how the proposed generalized FA can be adopted to solve downlink beamforming problems are shown for a classic transmit beamforming, cognitive beamforming, reconfigurable-intelligent-surfaces-aided (RIS-aided) transmit beamforming, and RIS-aided wireless power transfer (WPT). Complexity analyzes indicate that in large-antenna regimes the proposed FA approaches require less computational complexity than their corresponding interior point methods (IPMs) do, yet demand a higher complexity than the iterative and the successive convex approximation (SCA) approaches do. Simulation results reveal that the proposed FA attains the same global optimal solution as that of the IPM for an optimization problem in cognitive beamforming. On the other hand, the proposed FA approaches outperform the iterative, IPM and SCA in terms of obtaining better solution for optimization problems, respectively, for a classic transmit beamforming, RIS-aided transmit beamforming and RIS-aided WPT.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文提出一种通用的Firefly算法（FA），用于解决多变量函数和约束的优化框架问题。论文展示了四种示例，用于采用提议的通用FA来解决传输磁场Synthesizing、认知磁场Synthesizing、智能表面帮助传输磁场Synthesizing和智能表面帮助无线电能耗 Transfer（WPT）问题。复杂性分析表明，在大antenna regime下，提议的FA方法比其相应的内点方法（IPM）更具计算效率，但需要更高的计算复杂性 than iterative和successive Convex Approximation（SCA）方法。实验结果表明，提议的FA方法可以达到与IPM相同的全局最优解的global optimal solution，而且在认知磁场Synthesizing问题中，提议的FA方法超过iterative、IPM和SCA方法。
</details></li>
</ul>
<hr>
<h2 id="DPSS-based-Codebook-Design-for-Near-Field-XL-MIMO-Channel-Estimation"><a href="#DPSS-based-Codebook-Design-for-Near-Field-XL-MIMO-Channel-Estimation" class="headerlink" title="DPSS-based Codebook Design for Near-Field XL-MIMO Channel Estimation"></a>DPSS-based Codebook Design for Near-Field XL-MIMO Channel Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18180">http://arxiv.org/abs/2310.18180</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shicong Liu, Xianghao Yu, Zhen Gao, Derrick Wing Kwan Ng</li>
<li>for: 这个论文旨在为6G系统提供高精度的频率域通信频率域通信。</li>
<li>methods: 本论文提出了一种新的代码书设计方法，可以高效地估计靠近场的通信道。这种方法基于靠近场电磁波传输模型，并且通过分析近场电磁波的特点，提出了一种新的代码书设计方法。</li>
<li>results:  simulation结果表明，提出的代码书设计方法可以具有较高的压缩感知性和较低的泄漏效应，同时可以高效地估计靠近场通信道。<details>
<summary>Abstract</summary>
Future sixth-generation (6G) systems are expected to leverage extremely large-scale multiple-input multiple-output (XL-MIMO) technology, which significantly expands the range of the near-field region. While accurate channel estimation is essential for beamforming and data detection, the unique characteristics of near-field channels pose additional challenges to the effective acquisition of channel state information. In this paper, we propose a novel codebook design, which allows efficient near-field channel estimation with significantly reduced codebook size. Specifically, we consider the eigen-problem based on the near-field electromagnetic wave transmission model. Moreover, we derive the general form of the eigenvectors associated with the near-field channel matrix, revealing their noteworthy connection to the discrete prolate spheroidal sequence (DPSS). Based on the proposed near-field codebook design, we further introduce a two-step channel estimation scheme. Simulation results demonstrate that the proposed codebook design not only achieves superior sparsification performance of near-field channels with a lower leakage effect, but also significantly improves the accuracy in compressive sensing channel estimation.
</details>
<details>
<summary>摘要</summary>
未来第六代（6G）系统预计会利用非常大规模多输入多输出（XL-MIMO）技术，这将大幅扩展近场区域的范围。准确频率预测是扫描和数据检测中关键的一环，但近场通道特有的特征会对有效地获取频率状态信息提出更多的挑战。在这篇论文中，我们提出了一种新的编码ebook设计，允许高效地近场频率预测，同时减少编码ebook的大小。 Specifically，我们基于近场电磁波传输模型来解决近场电磁波传输的eigen-问题。此外，我们还 derive了近场通道矩阵的特征值和特征向量的总体形式，发现它们与杂谱圆柱形数列（DPSS）之间存在深刻的连接。基于我们的近场编码ebook设计，我们还提出了两步频率预测方案。实验结果表明，我们的编码ebook设计不仅可以高效地压缩近场通道，同时也可以大幅提高压缩感知通道预测的准确性。
</details></li>
</ul>
<hr>
<h2 id="A-Control-Bounded-Quadrature-Leapfrog-ADC"><a href="#A-Control-Bounded-Quadrature-Leapfrog-ADC" class="headerlink" title="A Control-Bounded Quadrature Leapfrog ADC"></a>A Control-Bounded Quadrature Leapfrog ADC</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18110">http://arxiv.org/abs/2310.18110</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hampus Malmberg, Fredrik Feyling, Jose M. de la Rosa</li>
<li>for: 本文示cases the design flexibility of the control-bounded analog-to-digital converter principle.</li>
<li>methods: 本文使用了band-pass analog-to-digital converter作为应用和Case study，并示出了如何将low-pass control-bounded analog-to-digital converter转换为band-pass版本，保持了 garantied stability、converter bandwidth和signal-to-noise ratio的 preserved，而且可以自由地调整中心频率。</li>
<li>results: 提出的converter被通过several filter orders、center frequencies和oversampling ratios的行为 simulations validate，并且对op-amp circuit实现进行了考虑，显示了first-order op-amp non-idealities的效果。最后，通过Monte Carlo simulations， demonstrate the robustness against component variations.<details>
<summary>Abstract</summary>
In this paper, the design flexibility of the control-bounded analog-to-digital converter principle is demonstrated. A band-pass analog-to-digital converter is considered as an application and case study. We show how a low-pass control-bounded analog-to-digital converter can be translated into a band-pass version where the guaranteed stability, converter bandwidth, and signal-to-noise ratio are preserved while the center frequency for conversion can be positioned freely. The proposed converter is validated with behavioral simulations on several filter orders, center frequencies, and oversampling ratios. Additionally, we consider an op-amp circuit realization where the effects of first-order op-amp non-idealities are shown. Finally, robustness against component variations is demonstrated by Monte Carlo simulations.
</details>
<details>
<summary>摘要</summary>
在本文中，我们示出了控制bounded的报文数字转换原理的设计灵活性。我们使用了带通量的报文数字转换器作为应用和案例研究。我们表明了一种low-pass控制bounded的报文数字转换器可以被翻译成带通量版本，保持稳定性、转换宽度和信号噪声比，并且可以自由地调整中心频率。我们通过多个筛ORDER、中心频率和抽样比例的行为仿真进行验证。此外，我们还考虑了一种op-amp电路实现，其中表明了首次逻辑不 idealities的效果。最后，我们通过Monte Carlo仿真展示了对Component变化的Robustness。
</details></li>
</ul>
<hr>
<h2 id="Probabilistic-Constellation-Shaping-for-OFDM-Based-ISAC-Signaling"><a href="#Probabilistic-Constellation-Shaping-for-OFDM-Based-ISAC-Signaling" class="headerlink" title="Probabilistic Constellation Shaping for OFDM-Based ISAC Signaling"></a>Probabilistic Constellation Shaping for OFDM-Based ISAC Signaling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18090">http://arxiv.org/abs/2310.18090</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhen Du, Fan Liu, Yifeng Xiong, Tony Xiao Han, Weijie Yuan, Yuanhao Cui, Changhua Yao, Yonina C. Eldar</li>
<li>for: 这个论文的目的是为 sixth-generation wireless communication systems (6G) 的 Integrated Sensing and Communications (ISAC) 技术做出贡献。</li>
<li>methods: 本论文使用的方法包括分析 OFDM 通信信号的歧义函数，并提出一种可扩展的概率 Constellation Shaping (PCS) 方法来实现 Sensing and Communications (S&amp;C) 之间的平衡。</li>
<li>results: 本论文的结果显示，使用 PCS 方法可以实现一个可扩展的 S&amp;C 贡献平衡，并且在numerical simulations中证明了这种方法的超越性。<details>
<summary>Abstract</summary>
Integrated Sensing and Communications (ISAC) has garnered significant attention as a promising technology for the upcoming sixth-generation wireless communication systems (6G). In pursuit of this goal, a common strategy is that a unified waveform, such as Orthogonal Frequency Division Multiplexing (OFDM), should serve dual-functional roles by enabling simultaneous sensing and communications (S&C) operations. However, the sensing performance of an OFDM communication signal is substantially affected by the randomness of the data symbols mapped from bit streams. Therefore, achieving a balance between preserving communication capability (i.e., the randomness) while improving sensing performance remains a challenging task. To cope with this issue, in this paper we analyze the ambiguity function of the OFDM communication signal modulated by random data. Subsequently, a probabilistic constellation shaping (PCS) method is proposed to devise the probability distributions of constellation points, which is able to strike a scalable S&C tradeoff of the random transmitted signal. Finally, the superiority of the proposed PCS method over conventional uniformly distributed constellations is validated through numerical simulations.
</details>
<details>
<summary>摘要</summary>
integrated sensing and communications (ISAC) 已经引起了广泛的关注，作为未来 sixth-generation wireless communication systems (6G) 的可能技术。为实现这个目标，一个常见的策略是使用 unified waveform，如orthogonal frequency division multiplexing (OFDM)，以实现同时的 sensing and communications (S&C) 操作。然而，OFDM 通信信号的探测性能受到数据符号的随机性的影响，因此保持通信能力（即随机性）的同时提高探测性能是一项挑战。为解决这个问题，本文分析 OFDM 通信信号模拟了随机数据的异步函数。然后，一种 probabilistic constellation shaping (PCS) 方法是提出来，以设计均匀分布的星座点概率分布，能够实现可扩展的 S&C 质量规则。最后，通过数值仿真，validate了提议的 PCS 方法的超越性。
</details></li>
</ul>
<hr>
<h2 id="New-Fast-Transform-for-Orthogonal-Frequency-Division-Multiplexing"><a href="#New-Fast-Transform-for-Orthogonal-Frequency-Division-Multiplexing" class="headerlink" title="New Fast Transform for Orthogonal Frequency Division Multiplexing"></a>New Fast Transform for Orthogonal Frequency Division Multiplexing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18064">http://arxiv.org/abs/2310.18064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Said Boussakta, Mounir T. Hamood, Mohammed Sh. Ahmed</li>
<li>for: 本研究旨在开发一个新的快速且具有低复杂性的传播对OFDM无线通信系统。</li>
<li>methods: 本研究使用了一种新的复变对对OFDM系统进行快速变换，即复变对对FTT（CTT）。此外，还发展了一个快速计算CTT对对的新算法，称为FCT。</li>
<li>results: 本研究发现，使用FCT算法可以实现OFDM系统中具有更好的对�hash-Hadamard变换（CHT）和快速傅立宝（FFT）的复合效果，并且可以实现更好的对�hash-Hadamard变换（CHT）和快速傅立宝（FFT）的复合效果，并且可以实现更好的对�hash-Hadamard变换（CHT）和快速傅立宝（FFT）的复合效果。此外，提出了一个新的OFDM系统，使用FCT算法，并评估了其性能。结果显示，提案的CT-OFDM可以实现更好的对�hash-Hadamard变换（CHT）和快速傅立宝（FFT）的复合效果，并且可以实现更好的对�hash-Hadamard变换（CHT）和快速傅立宝（FFT）的复合效果。<details>
<summary>Abstract</summary>
In this paper, a new fast and low complexity transform is introduced for orthogonal frequency division multiplexing (OFDM) wireless systems. The new transform combines the effects of fast complex-Walsh-Hadamard transform (CHT) and the fast Fourier transform (FFT) into a single unitary transform named in this paper as the complex transition transform (CTT). The development of a new algorithm for fast calculation of the CT transform called FCT is found to have all the desirable properties such as in-place computation, simple indexing scheme and considerably lower arithmetic complexity than existing algorithms. Furthermore, a new OFDM system using the FCT algorithm is introduced and its performance has been evaluated. The proposed CT-OFDM achieves a noticeable reduction in peak-to-average-power-ratio (PAPR) and a significant improvement in the bit-error-rate (BER) performance compared with the conventional OFDM.
</details>
<details>
<summary>摘要</summary>
在本文中，一种新的快速低复杂度变换被介绍到了分割多播发射系统中。该变换结合了快速复杂威尔逊哈达姆变换（CHT）和快速傅立叶变换（FFT）的效果，并将其称为复杂过渡变换（CTT）。本文提出了一种新的快速计算CT变换的算法，称为快速CT变换算法（FCT），该算法具有占位计算、简单的索引方式和较低的数学复杂性。此外，一种使用FCT算法的新的OFDM系统被引入，其性能被评估。提出的CT-OFDM系统可以减少峰值平均功率比（PAPR）和提高比特错误率（BER）的性能，与传统的OFDM系统相比有显著的改善。
</details></li>
</ul>
<hr>
<h2 id="Vision-Based-Reconfigurable-Intelligent-Surface-Beam-Tracking-for-mmWave-Communications"><a href="#Vision-Based-Reconfigurable-Intelligent-Surface-Beam-Tracking-for-mmWave-Communications" class="headerlink" title="Vision-Based Reconfigurable Intelligent Surface Beam Tracking for mmWave Communications"></a>Vision-Based Reconfigurable Intelligent Surface Beam Tracking for mmWave Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18012">http://arxiv.org/abs/2310.18012</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Sanchez, Xuesong Cai, Fredrik Tufvesson</li>
<li>for: 这篇论文主要是为了提高5G和更高版本无线通信性能而开发的重配置智能表面技术。</li>
<li>methods: 本论文使用计算机视觉技术来实现智能表面扫描 Algorithm，以解决复杂性、功耗和成本问题。</li>
<li>results: 研究结果表明，在插入智能表面后，多pathComponents会出现，其中一个路径的功率在堵塞情况下可以是关键，而在线视和非线视情况下都可以 observer capacity提高。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surfaces have emerged as a technology with the potential to enhance wireless communication performance for 5G and beyond. However, the technology comes with challenges in areas such as complexity, power consumption, and cost. This paper demonstrates a computer vision-based reconfigurable intelligent surface beamforming algorithm that addresses complexity and cost issues and analyzes the multipath components that arise from the insertion of such a device into the wireless channel. The results show that a reconfigurable intelligent surface can provide an additional multipath component. The power of this additional path can be critical in blockage scenarios, and a capacity increase can be perceived in both line-of-sight and non line-of-sight scenarios.
</details>
<details>
<summary>摘要</summary>
《可重配置智能表面技术在5G和以后的无线通信中表现出了潜在的提高性。然而，这技术受到复杂性、功耗和成本等因素的影响。本文提出了基于计算机视觉的可重配置智能表面扫描算法，解决了复杂性和成本问题，同时分析了在插入此设备到无线通信频道时产生的多Path分量。结果表明，可重配置智能表面可以提供一个额外的多Path分量，其功率在堵塞情况下可以是关键，在线视和非线视情况下都可以观察到容量增加。》Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="SCAN-MUSIC-An-Efficient-Super-resolution-Algorithm-for-Single-Snapshot-Wide-band-Line-Spectral-Estimation"><a href="#SCAN-MUSIC-An-Efficient-Super-resolution-Algorithm-for-Single-Snapshot-Wide-band-Line-Spectral-Estimation" class="headerlink" title="SCAN-MUSIC: An Efficient Super-resolution Algorithm for Single Snapshot Wide-band Line Spectral Estimation"></a>SCAN-MUSIC: An Efficient Super-resolution Algorithm for Single Snapshot Wide-band Line Spectral Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17988">http://arxiv.org/abs/2310.17988</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zetao Fei, Hai Zhang</li>
<li>for: 该论文设计用于重构一维宽频线谱在固定区间 [[-\Omega, \Omega]] 中。</li>
<li>methods: 该论文提出了一种可扩展的算法方法，使用固定 Gaussian 窗口扫描 spectral Domain，然后使用每次扫描结果来重构线谱。 其中，对于具有岛屿结构的线谱，可以进一步使用灭除器技术进行精细调整。</li>
<li>results: 该论文的算法可以减少标准 MUSIC 算法的计算复杂性，同时保持一定的分辨率。其性能与当前最佳算法相当，且在处理具有岛屿结构的线谱时更为可靠。<details>
<summary>Abstract</summary>
We propose an efficient algorithm for reconstructing one-dimensional wide-band line spectra from their Fourier data in a bounded interval $[-\Omega,\Omega]$. While traditional subspace methods such as MUSIC achieve super-resolution for closely separated line spectra, their computational cost is high, particularly for wide-band line spectra. To address this issue, we proposed a scalable algorithm termed SCAN-MUSIC that scans the spectral domain using a fixed Gaussian window and then reconstructs the line spectra falling into the window at each time. For line spectra with cluster structure, we further refine the proposed algorithm using the annihilating filter technique. Both algorithms can significantly reduce the computational complexity of the standard MUSIC algorithm with a moderate loss of resolution. Moreover, in terms of speed, their performance is comparable to the state-of-the-art algorithms, while being more reliable for reconstructing line spectra with cluster structure. The algorithms are supplemented with theoretical analyses of error estimates, sampling complexity, computational complexity, and computational limit.
</details>
<details>
<summary>摘要</summary>
我们提出了一种高效的算法来重建一维宽频线谱在固定区间 [[-\Ω, \Ω]] 中的重建问题。传统的子空间方法如 MUSIC 可以在紧邻的线谱上实现超解析，但其计算成本高、特别是对宽频线谱。为解决这个问题，我们提出了一种可扩展的算法 termed SCAN-MUSIC，它在 spectral 频域中使用固定的 Gaussian 窗口进行扫描，然后在每个时间点上重建落入窗口内的线谱。对于具有嵌入结构的线谱，我们进一步改进了提议的算法使用抑制器技术。这些算法可以在标准 MUSIC 算法的计算复杂度中减少计算复杂度，同时保持与现状算法相同的速度性和可靠性。我们还提供了算法的理论分析，包括错误估计、抽象复杂度、计算复杂度和计算限制。
</details></li>
</ul>
<hr>
<h2 id="User-Association-and-Resource-Allocation-in-Large-Language-Model-Based-Mobile-Edge-Computing-System-over-Wireless-Communications"><a href="#User-Association-and-Resource-Allocation-in-Large-Language-Model-Based-Mobile-Edge-Computing-System-over-Wireless-Communications" class="headerlink" title="User Association and Resource Allocation in Large Language Model Based Mobile Edge Computing System over Wireless Communications"></a>User Association and Resource Allocation in Large Language Model Based Mobile Edge Computing System over Wireless Communications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17872">http://arxiv.org/abs/2310.17872</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liangxin Qian, Jun Zhao</li>
<li>for: 这篇论文是关于如何实现高效的语言模型服务，尤其是在移动设备上的限制性计算资源下。</li>
<li>methods: 本篇论文提出了一个合作框架，让用户可以与服务器共同训练语言模型，以提高服务效能。用户首先将本地数据和模型适配器部组更新，然后将部组传送到服务器进行更新。</li>
<li>results: 透过实验，本篇论文证明了其提出的DASHF算法的效能，并提供了有用的问题解决方案，对于实现高效的语言模型服务在移动设备上提供了重要的启示。<details>
<summary>Abstract</summary>
In the rapidly evolving landscape of large language models (LLMs) and mobile edge computing, the need for efficient service delivery to mobile users with constrained computational resources has become paramount. Addressing this, our paper delves into a collaborative framework for model training where user data and model adapters are shared with servers to optimize performance. Within this framework, users initially update the first several layers of the adapters while freezing the other layers of them, leveraging their local datasets. Once this step is complete, these partially trained parameters are transmitted to servers. The servers, equipped with more robust computational capabilities, then update the subsequent layers. After this training, they send the enhanced parameters back to the users. This collaborative training approach ensures that mobile users with limited computational capacities can still benefit from advanced LLM services without being burdened by exhaustive computations. Central to our methodology is the DASHF algorithm, which encapsulates the Dinkelbach algorithm, alternating optimization, semidefinite relaxation (SDR), the Hungarian method, and a pioneering fractional programming technique from our recent IEEE JSAC paper "Human-Centric Resource Allocation in the Metaverse over Wireless Communications". The crux of DASHF is its capability to reformulate an optimization problem as Quadratically Constrained Quadratic Programming (QCQP) via meticulously crafted transformations, making it solvable by SDR and the Hungarian algorithm. Through extensive simulations, we demonstrate the effectiveness of the DASHF algorithm, offering significant insights for the advancement of collaborative LLM service deployments.
</details>
<details>
<summary>摘要</summary>
在大型语言模型（LLM）和移动边缘 computing 的快速演进中，为了提供对移动用户的有效服务，尤其是具有限制的计算资源，已经成为非常重要。我们的论文探讨了一个合作框架，其中用户的数据和模型适配器被分享到服务器，以便优化性能。在这个框架中，用户首先对适配器的前几层进行更新，并免除其他层的固定，利用本地数据集。一旦这步完成，这些部分训练的参数将被传递到服务器。服务器，具有更强大的计算能力，则对后续层进行更新。之后，这些优化的参数将被发送回用户。这个合作训练方法确保了移动用户具有有限的计算能力仍然能够享受进步的 LLN 服务，不会受到复杂的计算所拘束。我们的方法中心在 DASHF 算法，这个算法包含了 Dinkelbach 算法、分布式优化、正方形relaxation（SDR）、匈牙利方法和我们在 IEEE JSAC 上发表的“人类中心资源分配在Metaverse中的无线通信”一文中的创新分程式技术。DASHF 算法的核心在于可以通过精心设计的转换，将优化问题转换为 quadratic constraints quadratic programming（QCQP），使其可以通过 SDR 和匈牙利算法解决。经过广泛的 simulations，我们证明了 DASHF 算法的有效性，提供了进一步探讨合作 LLN 服务部署的重要意义。
</details></li>
</ul>
<hr>
<h2 id="Resource-Allocation-for-Near-Field-Communications-Fundamentals-Tools-and-Outlooks"><a href="#Resource-Allocation-for-Near-Field-Communications-Fundamentals-Tools-and-Outlooks" class="headerlink" title="Resource Allocation for Near-Field Communications: Fundamentals, Tools, and Outlooks"></a>Resource Allocation for Near-Field Communications: Fundamentals, Tools, and Outlooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17868">http://arxiv.org/abs/2310.17868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bokai Xu, Jiayi Zhang, Hongyang Du, Zhe Wang, Yuanwei Liu, Dusit Niyato, Bo Ai, Khaled B. Letaief</li>
<li>for: 本文主要研究近场通信系统中的资源分配问题，以实现高 spectral efficiency (SE) 和 energy efficiency (EE)。</li>
<li>methods: 本文使用 numerical techniques 和 machine learning methods 来解决近场资源分配问题，并且 highlighted their strengths and limitations。</li>
<li>results: 本文指出了近场通信系统中可用的资源，并且 Summarized optimization tools for addressing near-field resource allocation.<details>
<summary>Abstract</summary>
Extremely large-scale multiple-input-multiple output (XL-MIMO) is a promising technology to achieve high spectral efficiency (SE) and energy efficiency (EE) in future wireless systems. The larger array aperture of XL-MIMO makes communication scenarios closer to the near-field region. Therefore, near-field resource allocation is essential in realizing the above key performance indicators (KPIs). Moreover, the overall performance of XL-MIMO systems heavily depends on the channel characteristics of the selected users, eliminating interference between users through beamforming, power control, etc. The above resource allocation issue constitutes a complex joint multi-objective optimization problem since many variables and parameters must be optimized, including the spatial degree of freedom, rate, power allocation, and transmission technique. In this article, we review the basic properties of near-field communications and focus on the corresponding "resource allocation" problems. First, we identify available resources in near-field communication systems and highlight their distinctions from far-field communications. Then, we summarize optimization tools, such as numerical techniques and machine learning methods, for addressing near-field resource allocation, emphasizing their strengths and limitations. Finally, several important research directions of near-field communications are pointed out for further investigation.
</details>
<details>
<summary>摘要</summary>
非常大规模多输入多输出（XL-MIMO）技术是未来无线系统中实现高频率效率（SE）和能效率（EE）的有力方案。XL-MIMO的更大的天线组合使得通信场景更接近近场区域。因此，近场资源分配是实现上述关键性表达指标（KPI）的重要前提。此外，XL-MIMO系统的总性性能强度取决于选择用户的通道特性，通过扫描、功率控制等技术消除用户之间的干扰。以上资源分配问题构成了复杂的共同多目标优化问题，因为需要优化多个变量和参数，包括空间度的自由度、速率、功率分配和传输技术。在本文中，我们介绍了近场通信的基本性能和相关的"资源分配"问题。首先，我们确定了近场通信系统中可用的资源和与远场通信系统的区别。然后，我们总结了优化工具，如数值技术和机器学习方法，用于解决近场资源分配问题，强调其优点和局限性。最后，我们指出了进一步研究近场通信的重要研究方向。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/27/eess.SP_2023_10_27/" data-id="clogxf3u6019u5xrahb4k29yw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/cs.SD_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T15:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/cs.SD_2023_10_26/">cs.SD - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Single-channel-speech-enhancement-by-colored-spectrograms"><a href="#Single-channel-speech-enhancement-by-colored-spectrograms" class="headerlink" title="Single channel speech enhancement by colored spectrograms"></a>Single channel speech enhancement by colored spectrograms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17142">http://arxiv.org/abs/2310.17142</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sania Gul, Muhammad Salman Khan, Muhammad Fazeel</li>
<li>for: 提高混吵 speech 质量和理解度</li>
<li>methods: 使用深度神经网络 (DNN) 架构，采用 pix2pix 生成整数网络 (GAN) 训练 colored spectrograms 混吵 speech 去噪</li>
<li>results: 比对基eline方法，提高了 almost 0.84 点 PESQ 和 1% STOI，且 computational cost 大幅减少<details>
<summary>Abstract</summary>
Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.
</details>
<details>
<summary>摘要</summary>
音响提升关注于从目标语音中除去不想要的背景声音，以提高其质量和可理解性。在这篇论文中，我们提出了一种基于深度神经网络（DNN）的单通道语音提升方法，使用颜色spectrogram。我们采用了基于 pix2pix生成对抗网络（GAN）的DNN架构，并在颜色spectrogram上训练其来减噪。减噪后，颜色spectrogram的颜色被翻译为快时傅立声变换（STFT）的大小使用一个浅层神经网络进行预测。这些估算的STFT大小后与噪音相加，以获得提升的语音。结果表明，与不处理噪音数据相比，提升语音质量和可理解性的改进约为0.84分（PESQ）和1%（STOI）。与比较基线方法相比，提升的质量和可理解性减噪量约为90%，而计算成本减少了约10倍。提议的解决方案提供了相对于基线方法的PESQ分数，但计算成本减少了约10倍。此外，与另一个基线系统（CNN-GAN）相比，提升的STOI减噪量约为28倍，而计算成本减少了约28倍。
</details></li>
</ul>
<hr>
<h2 id="Real-time-Neonatal-Chest-Sound-Separation-using-Deep-Learning"><a href="#Real-time-Neonatal-Chest-Sound-Separation-using-Deep-Learning" class="headerlink" title="Real-time Neonatal Chest Sound Separation using Deep Learning"></a>Real-time Neonatal Chest Sound Separation using Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17116">http://arxiv.org/abs/2310.17116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yang Yi Poh, Ethan Grooby, Kenneth Tan, Lindsay Zhou, Arrabella King, Ashwin Ramanathan, Atul Malhotra, Mehrtash Harandi, Faezeh Marzbanrad</li>
<li>for: 这篇论文是为了提供一种基于深度学习的胸部听觉分离方法，以提高胸部听觉诊断的质量。</li>
<li>methods: 该论文提出了一种基于Conv-TasNet模型的深度学习方法，包括编码器、解码器和面Generator。编码器使用1D卷积模型，解码器使用反卷积，面Generator使用堆叠的1D卷积和变换器。</li>
<li>results: 该论文在人工数据集上比前方法提高了2.01dB至5.06dB的对象扭曲度量，同时计算时间也提高了至少17倍。因此，该方法可以作为任何胸部听觉监测系统的预处理步骤。<details>
<summary>Abstract</summary>
Auscultation for neonates is a simple and non-invasive method of providing diagnosis for cardiovascular and respiratory disease. Such diagnosis often requires high-quality heart and lung sounds to be captured during auscultation. However, in most cases, obtaining such high-quality sounds is non-trivial due to the chest sounds containing a mixture of heart, lung, and noise sounds. As such, additional preprocessing is needed to separate the chest sounds into heart and lung sounds. This paper proposes a novel deep-learning approach to separate such chest sounds into heart and lung sounds. Inspired by the Conv-TasNet model, the proposed model has an encoder, decoder, and mask generator. The encoder consists of a 1D convolution model and the decoder consists of a transposed 1D convolution. The mask generator is constructed using stacked 1D convolutions and transformers. The proposed model outperforms previous methods in terms of objective distortion measures by 2.01 dB to 5.06 dB in the artificial dataset, as well as computation time, with at least a 17-time improvement. Therefore, our proposed model could be a suitable preprocessing step for any phonocardiogram-based health monitoring system.
</details>
<details>
<summary>摘要</summary>
来诊检测新生儿是一种简单且不侵入性的诊断方法，用于诊断循环和呼吸道疾病。然而，在大多数情况下，获取高质量心脏和肺声 зву乐是非常困难，因为胸部声音包含了心脏、肺声和噪音声音。为了解决这个问题，通常需要进行额外的处理，以分离胸部声音成为心脏声音和肺声音。这篇论文提出了一个新的深度学习方法，用于将胸部声音分类为心脏声音和肺声音。这个方法受到Conv-TasNet模型的激发，并包括Encoder、Decoder和面组生成器。Encoder由1D梯度核心组成，Decoder由转置1D梯度组成，而面组生成器则由堆叠1D梯度和对称器组成。这个方法在人工数据集上比前方法提高了2.01dB至5.06dB的对象歪斜度指数，以及计算时间，至少提高了17倍。因此，我们的提案方法可以作为任何phonocardiogram基于的医疗监控系统的适当预处理步骤。
</details></li>
</ul>
<hr>
<h2 id="Multi-Speaker-Expressive-Speech-Synthesis-via-Semi-supervised-Contrastive-Learning"><a href="#Multi-Speaker-Expressive-Speech-Synthesis-via-Semi-supervised-Contrastive-Learning" class="headerlink" title="Multi-Speaker Expressive Speech Synthesis via Semi-supervised Contrastive Learning"></a>Multi-Speaker Expressive Speech Synthesis via Semi-supervised Contrastive Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17101">http://arxiv.org/abs/2310.17101</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinfa Zhu, Yuke Li, Yi Lei, Ning Jiang, Guoqing Zhao, Lei Xie</li>
<li>for: 建立一个可表达多种语音样式和情感的 TTS系统，以实现多 speaker 的语音合成。</li>
<li>methods: 提出一种基于对照学习的 TTS方法，通过将负例和正例构成组合，以更好地提取语音中的构造、情感和 speaker 特征。</li>
<li>results: 透过 semi-supervised 训练和多元数据，提高 VITS 模型的表现，使其能够实现多种语音样式和情感的语音合成。<details>
<summary>Abstract</summary>
This paper aims to build an expressive TTS system for multi-speakers, synthesizing a target speaker's speech with multiple styles and emotions. To this end, we propose a novel contrastive learning-based TTS approach to transfer style and emotion across speakers. Specifically, we construct positive-negative sample pairs at both utterance and category (such as emotion-happy or style-poet or speaker A) levels and leverage contrastive learning to better extract disentangled style, emotion, and speaker representations from speech. Furthermore, we introduce a semi-supervised training strategy to the proposed approach to effectively leverage multi-domain data, including style-labeled data, emotion-labeled data, and unlabeled data. We integrate the learned representations into an improved VITS model, enabling it to synthesize expressive speech with diverse styles and emotions for a target speaker. Experiments on multi-domain data demonstrate the good design of our model.
</details>
<details>
<summary>摘要</summary>
这篇论文目标建立一个表达力强的多话者Text-to-Speech（TTS）系统，使得目标说话者的speech中包含多种风格和情感。为此，我们提出了一种基于对比学习的TTS方法，用于传递风格和情感 across speakers。具体来说，我们构建了一个utterance和类别（例如情感-高兴或风格-诗人或说话者A）两级的正负样本对，并利用对比学习来更好地提取speech中的分离风格、情感和说话者表示。此外，我们提出了一种半监督训练策略，以更好地利用多个频道数据，包括风格标注数据、情感标注数据和无标注数据。我们将学习的表示 integrate into an improved VITS模型，使其能够合成具有多种风格和情感的表达性speech for a target speaker。实验结果表明我们的模型设计很好。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/cs.SD_2023_10_26/" data-id="clogxf3qv00x15xra0qt61gwm" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/eess.AS_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T14:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/eess.AS_2023_10_26/">eess.AS - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Privacy-preserving-Representation-Learning-for-Speech-Understanding"><a href="#Privacy-preserving-Representation-Learning-for-Speech-Understanding" class="headerlink" title="Privacy-preserving Representation Learning for Speech Understanding"></a>Privacy-preserving Representation Learning for Speech Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17194">http://arxiv.org/abs/2310.17194</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minh Tran, Mohammad Soleymani</li>
<li>for: 隐私保护的语音表示学习方法，目前只适用于单一应用领域。这篇论文提出了一种新的框架，可以匿名化预训练encoder生成的语音表示，并证明其在各种语音分类任务中的效果。</li>
<li>methods: 使用Transformer进行预测，根据预训练encoder生成的表示，计算不同说话者的同一句话表示。在推理阶段，提取的表示可以转换为不同的身份来保护隐私。</li>
<li>results: 与VoicePrivacy 2022挑战的基准方法进行比较，我们的方法在隐私和实用性两个领域中具有更好的性能，特别是在情感识别、抑郁诊断和意图识别等词语表达任务中。<details>
<summary>Abstract</summary>
Existing privacy-preserving speech representation learning methods target a single application domain. In this paper, we present a novel framework to anonymize utterance-level speech embeddings generated by pre-trained encoders and show its effectiveness for a range of speech classification tasks. Specifically, given the representations from a pre-trained encoder, we train a Transformer to estimate the representations for the same utterances spoken by other speakers. During inference, the extracted representations can be converted into different identities to preserve privacy. We compare the results with the voice anonymization baselines from the VoicePrivacy 2022 challenge. We evaluate our framework on speaker identification for privacy and emotion recognition, depression classification, and intent classification for utility. Our method outperforms the baselines on privacy and utility in paralinguistic tasks and achieves comparable performance for intent classification.
</details>
<details>
<summary>摘要</summary>
现有的隐私保护的语音表示学习方法都是专门为单个应用领域设计的。在这篇论文中，我们提出了一种新的框架，用于匿名化预训练编码器生成的语音嵌入，并证明其对各种语音分类任务的效果。具体来说，给定预训练编码器生成的表示，我们训练了一个Transformer模型，以便估计相同的声音由其他 speaker 说的表示。在推理阶段，提取的表示可以转换为不同的 identities，以保护隐私。我们与 VoicePrivacy 2022 挑战的基准值进行比较，并对 speaker 识别、情感识别、抑郁诊断和意图识别等多种任务进行评估。我们的方法在隐私和实用性两个方面都超过基准值，并在意图识别任务中与基准值相对。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/eess.AS_2023_10_26/" data-id="clogxf3rr010h5xra72g45umy" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/cs.CV_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T13:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/cs.CV_2023_10_26/">cs.CV - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Image-Prior-and-Posterior-Conditional-Probability-Representation-for-Efficient-Damage-Assessment"><a href="#Image-Prior-and-Posterior-Conditional-Probability-Representation-for-Efficient-Damage-Assessment" class="headerlink" title="Image Prior and Posterior Conditional Probability Representation for Efficient Damage Assessment"></a>Image Prior and Posterior Conditional Probability Representation for Efficient Damage Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17801">http://arxiv.org/abs/2310.17801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jie Wei, Weicong Feng, Erik Blasch, Erika Ardiles-Cruz, Haibin Ling</li>
<li>for: 本研究旨在提高人工援助和灾难应急Response (HADR) 应用中的损害评估 (DA) 效率和可扩展性。</li>
<li>methods: 本文提出了一种名为图像先后 conditional probability (IP2CP) 的有效计算视觉表示方法，用于对准前后灾难图像，并将其编码成一个图像进行深度学习处理，以确定损害水平。</li>
<li>results: 在两个重要的实际应用场景中，IP2CP 表现出了良好的性能：一是像素级Semantic segmentation，二是质心矩阵学习引入的全面损害分类。结果表明，基于 IP2CP 的深度学习框架可以有效实现数据和计算效率，这对 HADR 应用来说非常重要。<details>
<summary>Abstract</summary>
It is important to quantify Damage Assessment (DA) for Human Assistance and Disaster Response (HADR) applications. In this paper, to achieve efficient and scalable DA in HADR, an image prior and posterior conditional probability (IP2CP) is developed as an effective computational imaging representation. Equipped with the IP2CP representation, the matching pre- and post-disaster images are effectively encoded into one image that is then processed using deep learning approaches to determine the damage levels. Two scenarios of crucial importance for the practical use of DA in HADR applications are examined: pixel-wise semantic segmentation and patch-based contrastive learning-based global damage classification. Results achieved by IP2CP in both scenarios demonstrate promising performances, showing that our IP2CP-based methods within the deep learning framework can effectively achieve data and computational efficiency, which is of utmost importance for the DA in HADR applications.
</details>
<details>
<summary>摘要</summary>
important to quantify Damage Assessment (DA) for Human Assistance and Disaster Response (HADR) applications. In this paper, to achieve efficient and scalable DA in HADR, an image prior and posterior conditional probability (IP2CP) is developed as an effective computational imaging representation. Equipped with the IP2CP representation, the matching pre- and post-disaster images are effectively encoded into one image that is then processed using deep learning approaches to determine the damage levels. Two scenarios of crucial importance for the practical use of DA in HADR applications are examined: pixel-wise semantic segmentation and patch-based contrastive learning-based global damage classification. Results achieved by IP2CP in both scenarios demonstrate promising performances, showing that our IP2CP-based methods within the deep learning framework can effectively achieve data and computational efficiency, which is of utmost importance for the DA in HADR applications.Here's the translation:重要的是量化损害评估（DA）在人类援助和灾害应急应用中。本文使用图像先后条件概率（IP2CP）来实现有效的计算影像表示。通过IP2CP表示，匹配的前后灾害图像都被编码为一个图像，然后使用深度学习方法来确定损害水平。在实际应用中，对DA的两个场景得到了极重要的评估：像素级Semantic segmentation和 patch-based contrastive learning-based全面损害分类。IP2CP在这两个场景中的表现具有承诺性，表明我们的IP2CP基于深度学习框架可以有效实现数据和计算效率，这对DA应用中至关重要。
</details></li>
</ul>
<hr>
<h2 id="ControlLLM-Augment-Language-Models-with-Tools-by-Searching-on-Graphs"><a href="#ControlLLM-Augment-Language-Models-with-Tools-by-Searching-on-Graphs" class="headerlink" title="ControlLLM: Augment Language Models with Tools by Searching on Graphs"></a>ControlLLM: Augment Language Models with Tools by Searching on Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17796">http://arxiv.org/abs/2310.17796</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/opengvlab/controlllm">https://github.com/opengvlab/controlllm</a></li>
<li>paper_authors: Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, Wenhai Wang</li>
<li>for: 本研究旨在开发一种新的框架，使大型自然语言模型（LLM）能够利用多Modal工具解决复杂的实际任务。</li>
<li>methods: 本研究的方法包括三个关键组成部分：（1）任务分解器，将复杂任务分解成明确的子任务，并将输入和输出明确定义；（2）思想图（ToG） paradigma，在已经建立的工具图中搜索优化解决方案的路径；（3）执行引擎，将解决方案 interpret并有效地运行工具在不同的计算设备上。</li>
<li>results: 研究人员对多种图像、音频和视频处理任务进行了评估，并证明了该框架在精度、效率和多样性方面与现有方法相比有superior性。代码可以在<a target="_blank" rel="noopener" href="https://github.com/OpenGVLab/ControlLLM">https://github.com/OpenGVLab/ControlLLM</a> 上找到。<details>
<summary>Abstract</summary>
We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate tool selection and parameterization, and inefficient tool scheduling. To overcome these challenges, our framework comprises three key components: (1) a \textit{task decomposer} that breaks down a complex task into clear subtasks with well-defined inputs and outputs; (2) a \textit{Thoughts-on-Graph (ToG) paradigm} that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and (3) an \textit{execution engine with a rich toolbox} that interprets the solution path and runs the tools efficiently on different computational devices. We evaluate our framework on diverse tasks involving image, audio, and video processing, demonstrating its superior accuracy, efficiency, and versatility compared to existing methods. The code is at https://github.com/OpenGVLab/ControlLLM .
</details>
<details>
<summary>摘要</summary>
我们提出了 ControlLLM 框架，一种新的框架，允许大型自然语言模型（LLM）使用多Modal工具来解决复杂的实际任务。虽然 LLM 表现出了惊人的表现，但它们仍然因为用户提示的歧义、不准确的工具选择和参数化、不合理的工具调度而陷入困难。为了解决这些挑战，我们的框架包括三个关键组件：1. 任务分解器，将复杂任务分解成明确的子任务，输入和输出都具有明确的定义。2. Thoughts-on-Graph（ToG）理念，在预先构建的工具图上搜索最佳解决方案的路径。这个图表示不同工具之间的参数和依赖关系。3. 执行引擎与丰富工具库，将解决方案 интер普理解并运行工具，并在不同的计算设备上进行高效的执行。我们在各种图像、音频和视频处理任务中评估了 ControlLLM 框架，并证明它在精度、效率和多样性方面胜过现有方法。代码可以在 GitHub 上找到：https://github.com/OpenGVLab/ControlLLM。
</details></li>
</ul>
<hr>
<h2 id="AutoCT-Automated-CT-registration-segmentation-and-quantification"><a href="#AutoCT-Automated-CT-registration-segmentation-and-quantification" class="headerlink" title="AutoCT: Automated CT registration, segmentation, and quantification"></a>AutoCT: Automated CT registration, segmentation, and quantification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17780">http://arxiv.org/abs/2310.17780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhe Bai, Abdelilah Essiari, Talita Perciano, Kristofer E. Bouchard</li>
<li>for: 提供一个全面的CT成像处理和分析管道，用于基础科学发展和临床应用。</li>
<li>methods: 使用自动化预处理、注册、分割和量化分析3D CT扫描数据的整个管道。</li>
<li>results: 实现了基于Atlas的CT分割和量化，通过势能变换来提取本地特征，用于下游统计学学习，可以facilitate医疗诊断。<details>
<summary>Abstract</summary>
The processing and analysis of computed tomography (CT) imaging is important for both basic scientific development and clinical applications. In AutoCT, we provide a comprehensive pipeline that integrates an end-to-end automatic preprocessing, registration, segmentation, and quantitative analysis of 3D CT scans. The engineered pipeline enables atlas-based CT segmentation and quantification leveraging diffeomorphic transformations through efficient forward and inverse mappings. The extracted localized features from the deformation field allow for downstream statistical learning that may facilitate medical diagnostics. On a lightweight and portable software platform, AutoCT provides a new toolkit for the CT imaging community to underpin the deployment of artificial intelligence-driven applications.
</details>
<details>
<summary>摘要</summary>
computed tomography（CT）影像处理和分析对科学研究和临床应用都具有重要 significanc。在AutoCT中，我们提供了一个整体性的管道，该管道集成了端到端自动化预处理、注册、分割和量化CT扫描图像的工作流程。我们通过可靠的前向和反向映射来实现 diffeomorphic 变换，从而提取 CT 扫描图像中的本地特征。这些本地特征可以用于下游统计学学习，以便帮助医学诊断。AutoCT 在轻量级和可搬式软件平台上提供了一套新的工具包，为 CT 影像社区提供了人工智能驱动应用的基础。
</details></li>
</ul>
<hr>
<h2 id="A-Dataset-of-Relighted-3D-Interacting-Hands"><a href="#A-Dataset-of-Relighted-3D-Interacting-Hands" class="headerlink" title="A Dataset of Relighted 3D Interacting Hands"></a>A Dataset of Relighted 3D Interacting Hands</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17768">http://arxiv.org/abs/2310.17768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gyeongsik Moon, Shunsuke Saito, Weipeng Xu, Rohan Joshi, Julia Buffalini, Harley Bellan, Nicholas Rosen, Jesse Richardson, Mallorie Mize, Philippe de Bree, Tomas Simon, Bo Peng, Shubham Garg, Kevyn McPhail, Takaaki Shiratori</li>
<li>for: 本研究旨在提供一个多样化和实际的手交互分析数据集，以便更好地研究手交互的自然语言处理。</li>
<li>methods: 本研究使用了一种现代的手重光照网络，并利用了精准的两手三维姿态跟踪来生成多样化和实际的手交互图像。</li>
<li>results: 对比于现有的手交互图像数据集，本研究的Re：InterHand数据集具有更多的多样化和实际的图像表现，同时也提供了更多的大量的三维姿态跟踪数据。<details>
<summary>Abstract</summary>
The two-hand interaction is one of the most challenging signals to analyze due to the self-similarity, complicated articulations, and occlusions of hands. Although several datasets have been proposed for the two-hand interaction analysis, all of them do not achieve 1) diverse and realistic image appearances and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands that achieve the two goals. To this end, we employ a state-of-the-art hand relighting network with our accurately tracked two-hand 3D poses. We compare our Re:InterHand with existing 3D interacting hands datasets and show the benefit of it. Our Re:InterHand is available in https://mks0601.github.io/ReInterHand/.
</details>
<details>
<summary>摘要</summary>
“二手互动是训练模型最difficult的讯号之一，因为手部的自相似和复杂的动作，以及手部的 occlusion。许多dataset已经被提出来分析二手互动，但都无法同时取得1) 多样化和生动的图像出现和2) 多样化和大量的GT 3D姿态。在这个工作中，我们提出了Re:InterHand，一个基于state-of-the-art手部重光网络和我们精确地追踪的二手3D姿态。我们与现有的3D互动手部dataset进行比较，展示了Re:InterHand的优点。Re:InterHand可以在https://mks0601.github.io/ReInterHand/中下载。”Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, please let me know and I can provide the translation in that format as well.
</details></li>
</ul>
<hr>
<h2 id="SynergyNet-Bridging-the-Gap-between-Discrete-and-Continuous-Representations-for-Precise-Medical-Image-Segmentation"><a href="#SynergyNet-Bridging-the-Gap-between-Discrete-and-Continuous-Representations-for-Precise-Medical-Image-Segmentation" class="headerlink" title="SynergyNet: Bridging the Gap between Discrete and Continuous Representations for Precise Medical Image Segmentation"></a>SynergyNet: Bridging the Gap between Discrete and Continuous Representations for Precise Medical Image Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17764">http://arxiv.org/abs/2310.17764</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/CandleLabAI/SynergyNet-WACV-2024">https://github.com/CandleLabAI/SynergyNet-WACV-2024</a></li>
<li>paper_authors: Vandan Gorade, Sparsh Mittal, Debesh Jha, Ulas Bagci</li>
<li>for: 这篇论文是为了提高医疗影像分析的表现，特别是透过整合维度的混合以提高现有的encoder-decoder分类框架。</li>
<li>methods: 这篇论文使用了一种新的瓶颈架构，称为SynergyNet，它可以融合维度的整合以获取相互补偿的信息，并成功地保留了细节和高级构造的资讯。</li>
<li>results: 根据多组组织分类和心脏组织数据，这篇论文的SynergyNet模型比其他现有的方法（包括TransUNet）表现更好，统计学上的提升为2.16%的 dice scores和11.13%的 Hausdorff scores。在皮肤患病和脑肿瘤分类数据上，这篇论文的SynergyNet模型实现了1.71%的 Intersection-over Union 提升和8.58%的提升。<details>
<summary>Abstract</summary>
In recent years, continuous latent space (CLS) and discrete latent space (DLS) deep learning models have been proposed for medical image analysis for improved performance. However, these models encounter distinct challenges. CLS models capture intricate details but often lack interpretability in terms of structural representation and robustness due to their emphasis on low-level features. Conversely, DLS models offer interpretability, robustness, and the ability to capture coarse-grained information thanks to their structured latent space. However, DLS models have limited efficacy in capturing fine-grained details. To address the limitations of both DLS and CLS models, we propose SynergyNet, a novel bottleneck architecture designed to enhance existing encoder-decoder segmentation frameworks. SynergyNet seamlessly integrates discrete and continuous representations to harness complementary information and successfully preserves both fine and coarse-grained details in the learned representations. Our extensive experiment on multi-organ segmentation and cardiac datasets demonstrates that SynergyNet outperforms other state of the art methods, including TransUNet: dice scores improving by 2.16%, and Hausdorff scores improving by 11.13%, respectively. When evaluating skin lesion and brain tumor segmentation datasets, we observe a remarkable improvement of 1.71% in Intersection-over Union scores for skin lesion segmentation and of 8.58% for brain tumor segmentation. Our innovative approach paves the way for enhancing the overall performance and capabilities of deep learning models in the critical domain of medical image analysis.
</details>
<details>
<summary>摘要</summary>
Recently, continuous latent space (CLS) 和 discrete latent space (DLS) deep learning models have been proposed for medical image analysis, which have improved performance. However, these models have different challenges. CLS models can capture intricate details, but often lack interpretability in terms of structural representation and robustness due to their emphasis on low-level features. On the other hand, DLS models have interpretability, robustness, and the ability to capture coarse-grained information thanks to their structured latent space. However, DLS models have limited efficacy in capturing fine-grained details. To address the limitations of both CLS and DLS models, we propose SynergyNet, a novel bottleneck architecture designed to enhance existing encoder-decoder segmentation frameworks. SynergyNet seamlessly integrates discrete and continuous representations to harness complementary information and successfully preserves both fine and coarse-grained details in the learned representations. Our extensive experiment on multi-organ segmentation and cardiac datasets shows that SynergyNet outperforms other state-of-the-art methods, including TransUNet: dice scores improve by 2.16%, and Hausdorff scores improve by 11.13%, respectively. When evaluating skin lesion and brain tumor segmentation datasets, we observe a remarkable improvement of 1.71% in Intersection-over-Union scores for skin lesion segmentation and of 8.58% for brain tumor segmentation. Our innovative approach paves the way for enhancing the overall performance and capabilities of deep learning models in the critical domain of medical image analysis.
</details></li>
</ul>
<hr>
<h2 id="Alzheimers-Disease-Diagnosis-by-Deep-Learning-Using-MRI-Based-Approaches"><a href="#Alzheimers-Disease-Diagnosis-by-Deep-Learning-Using-MRI-Based-Approaches" class="headerlink" title="Alzheimers Disease Diagnosis by Deep Learning Using MRI-Based Approaches"></a>Alzheimers Disease Diagnosis by Deep Learning Using MRI-Based Approaches</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17755">http://arxiv.org/abs/2310.17755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarasadat Foroughipoor, Kimia Moradi, Hamidreza Bolhasani</li>
<li>for: 这篇研究旨在探讨使用Magnetic Resonance Imaging（MRI）技术和深度学习算法来诊断阿尔茨海默病（AD）。</li>
<li>methods: 这些研究使用MRI技术取得的数据，然后使用深度学习算法进行特征提取和模式识别，以帮助早期诊断和病程评估。</li>
<li>results: 这些研究发现，使用MRI技术和深度学习算法可以帮助早期诊断阿尔茨海默病，并且可以特别识别出患者的病程阶段和特定症状。<details>
<summary>Abstract</summary>
The most frequent kind of dementia of the nervous system, Alzheimer's disease, weakens several brain processes (such as memory) and eventually results in death. The clinical study uses magnetic resonance imaging to diagnose AD. Deep learning algorithms are capable of pattern recognition and feature extraction from the inputted raw data. As early diagnosis and stage detection are the most crucial elements in enhancing patient care and treatment outcomes, deep learning algorithms for MRI images have recently allowed for diagnosing a medical condition at the beginning stage and identifying particular symptoms of Alzheimer's disease. As a result, we aimed to analyze five specific studies focused on AD diagnosis using MRI-based deep learning algorithms between 2021 and 2023 in this study. To completely illustrate the differences between these techniques and comprehend how deep learning algorithms function, we attempted to explore selected approaches in depth.
</details>
<details>
<summary>摘要</summary>
最常见的神经系统失智症，阿尔茨海默病（AD），会弱化许多大脑过程（如记忆），最终会导致死亡。临床研究使用核磁共振成像（MRI）诊断AD。深度学习算法具有模式识别和特征提取功能，可以从输入的原始数据中提取有用的特征。早期诊断和stage检测是患者护理和治疗效果的关键因素，因此深度学习算法在MRI图像上的应用在诊断阿尔茨海默病的早期阶段和特征识别方面具有重要意义。本研究采用MRI图像基于深度学习算法进行AD诊断的五项研究，均发生在2021年至2023年之间。为了彻底描述这些技术的差异和深度学习算法的工作方式，我们尝试了深入探讨选择的方法。
</details></li>
</ul>
<hr>
<h2 id="Advancing-Brain-Tumor-Detection-A-Thorough-Investigation-of-CNNs-Clustering-and-SoftMax-Classification-in-the-Analysis-of-MRI-Images"><a href="#Advancing-Brain-Tumor-Detection-A-Thorough-Investigation-of-CNNs-Clustering-and-SoftMax-Classification-in-the-Analysis-of-MRI-Images" class="headerlink" title="Advancing Brain Tumor Detection: A Thorough Investigation of CNNs, Clustering, and SoftMax Classification in the Analysis of MRI Images"></a>Advancing Brain Tumor Detection: A Thorough Investigation of CNNs, Clustering, and SoftMax Classification in the Analysis of MRI Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17720">http://arxiv.org/abs/2310.17720</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jonayet Miah, Duc M Cao, Md Abu Sayed3, Md Siam Taluckder, Md Sabbirul Haque, Fuad Mahmud</li>
<li>for: 检测脑肿的早期检测是脑肿疾病管理的关键，以提高治疗效果和患者结果。本研究探讨了使用卷积神经网络（CNN）来检测脑肿，使用MRI图像。</li>
<li>methods: 本研究使用MRI图像进行数据采集，并将其处理并输入到CNN体系中。CNN体系使用SoftMax彻底连接层进行分类，实现了98%的准确率。此外，本研究还使用Radial Basis Function（RBF）和Decision Tree（DT）两种分类器，其中RBF的准确率为98.24%，DT的准确率为95.64%。此外，本研究还引入了分类方法来提高CNN的准确率。</li>
<li>results: 本研究的结果表明，SoftMax分类器在测试数据上的准确率为99.52%，而RBF和DT分类器的准确率分别为98.24%和95.64%。此外，本研究还使用了敏感度、特异性和准确率来全面评估网络的性能。<details>
<summary>Abstract</summary>
Brain tumors pose a significant global health challenge due to their high prevalence and mortality rates across all age groups. Detecting brain tumors at an early stage is crucial for effective treatment and patient outcomes. This study presents a comprehensive investigation into the use of Convolutional Neural Networks (CNNs) for brain tumor detection using Magnetic Resonance Imaging (MRI) images. The dataset, consisting of MRI scans from both healthy individuals and patients with brain tumors, was processed and fed into the CNN architecture. The SoftMax Fully Connected layer was employed to classify the images, achieving an accuracy of 98%. To evaluate the CNN's performance, two other classifiers, Radial Basis Function (RBF) and Decision Tree (DT), were utilized, yielding accuracy rates of 98.24% and 95.64%, respectively. The study also introduced a clustering method for feature extraction, improving CNN's accuracy. Sensitivity, Specificity, and Precision were employed alongside accuracy to comprehensively evaluate the network's performance. Notably, the SoftMax classifier demonstrated the highest accuracy among the categorizers, achieving 99.52% accuracy on test data. The presented research contributes to the growing field of deep learning in medical image analysis. The combination of CNNs and MRI data offers a promising tool for accurately detecting brain tumors, with potential implications for early diagnosis and improved patient care.
</details>
<details>
<summary>摘要</summary>
脑肿瘤对全球健康带来了很大挑战，因为它们在所有年龄组中有高的发病率和死亡率。早期检测脑肿瘤非常重要，以确保有效的治疗和患者结果。本研究使用了卷积神经网络（CNN）来检测脑肿瘤，使用了磁共振成像（MRI）图像。数据集包括了健康个体和脑肿瘤患者的MRI图像，经过处理后被传输到CNN体系中。SoftMax完全连接层被用来分类图像，实现了98%的准确率。为了评估CNN的表现，还使用了几种其他分类器，包括卷积函数（RBF）和决策树（DT），其中RBF和DT分别实现了98.24%和95.64%的准确率。研究还提出了一种归一化方法，以提高CNN的准确率。在评估网络表现时，使用了敏感性、特异性和精度，以全面评估网络的表现。结果显示，SoftMax分类器在测试数据上实现了99.52%的准确率。本研究贡献了深度学习在医疗图像分析领域的发展，并表明了CNN和MRI数据的结合可以提供高精度的脑肿瘤检测，有可能提供早期诊断和改善患者护理的可能性。
</details></li>
</ul>
<hr>
<h2 id="Fantastic-Gains-and-Where-to-Find-Them-On-the-Existence-and-Prospect-of-General-Knowledge-Transfer-between-Any-Pretrained-Model"><a href="#Fantastic-Gains-and-Where-to-Find-Them-On-the-Existence-and-Prospect-of-General-Knowledge-Transfer-between-Any-Pretrained-Model" class="headerlink" title="Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model"></a>Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17653">http://arxiv.org/abs/2310.17653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karsten Roth, Lukas Thede, Almut Sophia Koepke, Oriol Vinyals, Olivier Hénaff, Zeynep Akata</li>
<li>for: 这个论文旨在探讨 deep learning 模型在不同设计决策下的训练方法，以及这些方法如何影响模型学习特征集。</li>
<li>methods: 作者使用了公共模型库，包含 thousands 个在 ImageNet 等标准数据集上训练的模型，并对这些模型进行了分析。</li>
<li>results: 研究发现，无论模型是哪两个，都可以在训练过程中学习独特的特征集。此外，作者还提出了一种基于数据分割的扩展方法，可以在大多数情况下实现模型之间的知识传递，无需外部评价。<details>
<summary>Abstract</summary>
Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like ImageNet, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such "complementary" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer in scenarios agnostic to pretrained model pairings would unlock auxiliary gains and knowledge fusion from any model repository without restrictions on model and problem specifics - including from weaker, lower-performance models. This work therefore provides an initial, in-depth exploration on the viability of such general-purpose knowledge transfer. Across large-scale experiments, we first reveal the shortcomings of standard knowledge distillation techniques, and then propose a much more general extension through data partitioning for successful transfer between nearly all pretrained models, which we show can also be done unsupervised. Finally, we assess both the scalability and impact of fundamental model properties on successful model-agnostic knowledge transfer.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="A-Coarse-to-Fine-Pseudo-Labeling-C2FPL-Framework-for-Unsupervised-Video-Anomaly-Detection"><a href="#A-Coarse-to-Fine-Pseudo-Labeling-C2FPL-Framework-for-Unsupervised-Video-Anomaly-Detection" class="headerlink" title="A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection"></a>A Coarse-to-Fine Pseudo-Labeling (C2FPL) Framework for Unsupervised Video Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17650">http://arxiv.org/abs/2310.17650</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Al-lahham, Nurbek Tastan, Zaigham Zaheer, Karthik Nandakumar</li>
<li>for: 本研究旨在提出一种完全无监督的视频异常事件检测方法，以解决视频异常检测在无任何标注或人工监督的情况下的挑战。</li>
<li>methods: 提议的方法基于一种简单而有效的两个阶段pseudo标签生成框架，包括层次分割和统计假设测试，以生成视频段级（常见&#x2F;异常）pseudo标签。</li>
<li>results: 对两个大规模公共领域数据集（UCF-Crime和XD-Violence）进行了广泛的研究，并证明了提议的无监督方法可以与所有现有的一类分类和无监督方法相比，而且与状态的监督方法相比，可以达到相似的性能。<details>
<summary>Abstract</summary>
Detection of anomalous events in videos is an important problem in applications such as surveillance. Video anomaly detection (VAD) is well-studied in the one-class classification (OCC) and weakly supervised (WS) settings. However, fully unsupervised (US) video anomaly detection methods, which learn a complete system without any annotation or human supervision, have not been explored in depth. This is because the lack of any ground truth annotations significantly increases the magnitude of the VAD challenge. To address this challenge, we propose a simple-but-effective two-stage pseudo-label generation framework that produces segment-level (normal/anomaly) pseudo-labels, which can be further used to train a segment-level anomaly detector in a supervised manner. The proposed coarse-to-fine pseudo-label (C2FPL) generator employs carefully-designed hierarchical divisive clustering and statistical hypothesis testing to identify anomalous video segments from a set of completely unlabeled videos. The trained anomaly detector can be directly applied on segments of an unseen test video to obtain segment-level, and subsequently, frame-level anomaly predictions. Extensive studies on two large-scale public-domain datasets, UCF-Crime and XD-Violence, demonstrate that the proposed unsupervised approach achieves superior performance compared to all existing OCC and US methods , while yielding comparable performance to the state-of-the-art WS methods.
</details>
<details>
<summary>摘要</summary>
检测视频异常事件是应用中的一个重要问题，例如监视。视频异常检测（VAD）在一类分类（OCC）和弱监督（WS）设置中已经得到了广泛的研究。然而，完全无监督（US）的视频异常检测方法，即没有任何标注或人工监督，尚未得到了深入研究。这是因为缺乏任何标注数据导致VAD挑战的难度增加了。为解决这个挑战，我们提出了一个简单 yet effective的两个阶段 Pseudo-label生成框架，生成视频段级（正常/异常） Pseudo-labels，可以进一步用于在监督下训练段级异常检测器。我们的粗略到细分 Pseudo-label生成器（C2FPL）使用了仔细设计的层次分割和统计假设测试来从一组完全无标注视频中 identific 异常视频段。训练后的异常检测器可以直接应用于测试视频段中，以获得段级异常预测，并最终生成帧级异常预测。我们在两个大规模的公共领域数据集，UCFCrime和XD-Violence上，进行了广泛的研究，结果显示，我们的无监督方法在OCC和US方法中表现出色，同时与WS方法相当。
</details></li>
</ul>
<hr>
<h2 id="6-DoF-Stability-Field-via-Diffusion-Models"><a href="#6-DoF-Stability-Field-via-Diffusion-Models" class="headerlink" title="6-DoF Stability Field via Diffusion Models"></a>6-DoF Stability Field via Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17649">http://arxiv.org/abs/2310.17649</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takuma Yoneda, Tianchong Jiang, Gregory Shakhnarovich, Matthew R. Walter<br>for:* 6-DoFusion is a generative model that can generate 3D poses of objects to create stable configurations of a scene.methods:* The model uses a diffusion model to incrementally refine a randomly initialized SE(3) pose to generate a sample from a learned, context-dependent distribution over stable poses.results:* The model can construct stable scenes involving novel object classes and improve the accuracy of state-of-the-art 3D pose estimation methods.Here is the text in Simplified Chinese:for:* 6-DoFusion 是一个生成模型，可以生成 объек的 3D 姿态，以实现组件的稳定配置。methods:* 模型使用一个散布模型，逐步优化一个随机初始化的 SE(3) 姿态，以生成一个从学习的、上下文相依的分布中的稳定姿态样本。results:* 模型可以实现稳定的组件，包括新的物体类型，并且可以提高现有的3D 姿态估计方法的精度。<details>
<summary>Abstract</summary>
A core capability for robot manipulation is reasoning over where and how to stably place objects in cluttered environments. Traditionally, robots have relied on object-specific, hand-crafted heuristics in order to perform such reasoning, with limited generalizability beyond a small number of object instances and object interaction patterns. Recent approaches instead learn notions of physical interaction, namely motion prediction, but require supervision in the form of labeled object information or come at the cost of high sample complexity, and do not directly reason over stability or object placement. We present 6-DoFusion, a generative model capable of generating 3D poses of an object that produces a stable configuration of a given scene. Underlying 6-DoFusion is a diffusion model that incrementally refines a randomly initialized SE(3) pose to generate a sample from a learned, context-dependent distribution over stable poses. We evaluate our model on different object placement and stacking tasks, demonstrating its ability to construct stable scenes that involve novel object classes as well as to improve the accuracy of state-of-the-art 3D pose estimation methods.
</details>
<details>
<summary>摘要</summary>
一个核心能力 для机器人操作是在混乱环境中稳定地放置物体。传统上，机器人依靠特定对象的手工规则来实现这种能力，有限的通用性只能涵盖一小数量的物体实例和物体交互模式。最近的方法学习物理互动，但需要监督，通过标注对象信息来获得权重，或者来到高样本复杂度的代价，并不直接关于稳定性或物体放置的理解。我们提出了6DoFusion，一种生成模型，可以生成一个物体的3D姿态，使得给定场景中的物体生成稳定的配置。6DoFusion的基础是一种扩散模型，逐步精细地修改一个随机初始化的SE(3)姿态，以生成一个学习的、场景相依的 pose distribution。我们对不同的物体放置和堆叠任务进行评估，示出了我们的模型能够处理新的物体类型以及提高现有3D姿态估计方法的准确性。
</details></li>
</ul>
<hr>
<h2 id="Drive-Anywhere-Generalizable-End-to-end-Autonomous-Driving-with-Multi-modal-Foundation-Models"><a href="#Drive-Anywhere-Generalizable-End-to-end-Autonomous-Driving-with-Multi-modal-Foundation-Models" class="headerlink" title="Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models"></a>Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17642">http://arxiv.org/abs/2310.17642</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, Yutong Ban, Alexander Amini, Guy Rosman, Sertac Karaman, Daniela Rus<br>for:这篇论文旨在提出一种基于多modal基础模型的端到端自动驾驶系统，以提高系统的可靠性和适应能力。methods: authors使用了多modal基础模型，包括图像和文本，以提高自动驾驶系统的准确性和可靠性。他们还使用了 pixel&#x2F;patch-aligned特征提取方法，以捕捉图像的细节和 semantics。results: authors的方法在多种不同的测试中达到了无 precedent的结果，同时在不同的环境和场景下表现出了更大的鲁棒性。此外， authors还使用了文本来进行数据增强和策略调试，以提高自动驾驶系统的可靠性和适应能力。<details>
<summary>Abstract</summary>
As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems, enabling out-of-distribution, end-to-end, multimodal, and more explainable autonomy. Specifically, we present an approach to apply end-to-end open-set (any environment/scene) autonomous driving that is capable of providing driving decisions from representations queryable by image and text. To do so, we introduce a method to extract nuanced spatial (pixel/patch-aligned) features from transformers to enable the encapsulation of both spatial and semantic features. Our approach (i) demonstrates unparalleled results in diverse tests while achieving significantly greater robustness in out-of-distribution situations, and (ii) allows the incorporation of latent space simulation (via text) for improved training (data augmentation via text) and policy debugging. We encourage the reader to check our explainer video at https://www.youtube.com/watch?v=4n-DJf8vXxo&feature=youtu.be and to view the code and demos on our project webpage at https://drive-anywhere.github.io/.
</details>
<details>
<summary>摘要</summary>
As autonomous driving technology matures, end-to-end methodologies have emerged as a leading strategy, promising seamless integration from perception to control via deep learning. However, existing systems grapple with challenges such as unexpected open set environments and the complexity of black-box models. At the same time, the evolution of deep learning introduces larger, multimodal foundational models, offering multi-modal visual and textual understanding. In this paper, we harness these multimodal foundation models to enhance the robustness and adaptability of autonomous driving systems, enabling out-of-distribution, end-to-end, multimodal, and more explainable autonomy. Specifically, we present an approach to apply end-to-end open-set (any environment/scene) autonomous driving that is capable of providing driving decisions from representations queryable by image and text. To do so, we introduce a method to extract nuanced spatial (pixel/patch-aligned) features from transformers to enable the encapsulation of both spatial and semantic features. Our approach (i) demonstrates unparalleled results in diverse tests while achieving significantly greater robustness in out-of-distribution situations, and (ii) allows the incorporation of latent space simulation (via text) for improved training (data augmentation via text) and policy debugging. We encourage the reader to check our explainer video at <https://www.youtube.com/watch?v=4n-DJf8vXxo&feature=youtu.be> and to view the code and demos on our project webpage at <https://drive-anywhere.github.io/>.
</details></li>
</ul>
<hr>
<h2 id="DeepShaRM-Multi-View-Shape-and-Reflectance-Map-Recovery-Under-Unknown-Lighting"><a href="#DeepShaRM-Multi-View-Shape-and-Reflectance-Map-Recovery-Under-Unknown-Lighting" class="headerlink" title="DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown Lighting"></a>DeepShaRM: Multi-View Shape and Reflectance Map Recovery Under Unknown Lighting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17632">http://arxiv.org/abs/2310.17632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kohei Yamashita, Shohei Nobuhara, Ko Nishino</li>
<li>for:  accurately recovers object geometry in challenging settings of textureless, non-Lambertian objects under unknown natural illumination.</li>
<li>methods:  novel multi-view method called DeepShaRM, which uses a deep reflectance map estimation network and a deep shape-from-shading network to bypass the ill-posed problem of reflectance and illumination decomposition.</li>
<li>results:  state-of-the-art accuracy on this challenging task, demonstrated through extensive experiments on both synthetic and real-world data.<details>
<summary>Abstract</summary>
Geometry reconstruction of textureless, non-Lambertian objects under unknown natural illumination (i.e., in the wild) remains challenging as correspondences cannot be established and the reflectance cannot be expressed in simple analytical forms. We derive a novel multi-view method, DeepShaRM, that achieves state-of-the-art accuracy on this challenging task. Unlike past methods that formulate this as inverse-rendering, i.e., estimation of reflectance, illumination, and geometry from images, our key idea is to realize that reflectance and illumination need not be disentangled and instead estimated as a compound reflectance map. We introduce a novel deep reflectance map estimation network that recovers the camera-view reflectance maps from the surface normals of the current geometry estimate and the input multi-view images. The network also explicitly estimates per-pixel confidence scores to handle global light transport effects. A deep shape-from-shading network then updates the geometry estimate expressed with a signed distance function using the recovered reflectance maps. By alternating between these two, and, most important, by bypassing the ill-posed problem of reflectance and illumination decomposition, the method accurately recovers object geometry in these challenging settings. Extensive experiments on both synthetic and real-world data clearly demonstrate its state-of-the-art accuracy.
</details>
<details>
<summary>摘要</summary>
几何重建Textureless、非拉贝特的物体（即在野外）仍然是一个挑战，因为无法确定对应关系和反射无法表示为简单的分析型式。我们提出了一种新的多视图方法，即DeepShaRM，可以在这个挑战任务中实现状态 arts的准确性。与过去的方法不同，我们的关键思想是反射和照明无需分离，而是一起 estimating compound reflectance map。我们介绍了一种新的深度反射地图估计网络，可以从表 Normal 和输入多视图图像中提取camera-view反射地图。该网络还可以直接估计每个像素的信任分数，以处理全球照明效果。一个深度形状从反射地图更新geometry estimate，使用表 Normal 和输入多视图图像来表示。通过交互这两个网络，并更重要的是，通过绕过反射和照明的分解问题，该方法可以高精度地重建物体几何。我们的实验表明，该方法在 sintetic 和 real-world 数据上具有状态 arts 的准确性。
</details></li>
</ul>
<hr>
<h2 id="A-Survey-on-Transferability-of-Adversarial-Examples-across-Deep-Neural-Networks"><a href="#A-Survey-on-Transferability-of-Adversarial-Examples-across-Deep-Neural-Networks" class="headerlink" title="A Survey on Transferability of Adversarial Examples across Deep Neural Networks"></a>A Survey on Transferability of Adversarial Examples across Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17626">http://arxiv.org/abs/2310.17626</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jindonggu/awesome_adversarial_transferability">https://github.com/jindonggu/awesome_adversarial_transferability</a></li>
<li>paper_authors: Jindong Gu, Xiaojun Jia, Pau de Jorge, Wenqain Yu, Xinwei Liu, Avery Ma, Yuan Xun, Anjun Hu, Ashkan Khakzar, Zhijiang Li, Xiaochun Cao, Philip Torr</li>
<li>for: 本研究旨在探讨对抗性例子的跨模型传播性，以及提高抗性例子的传播性的不同方法。</li>
<li>methods: 本文分析了现有的方法，包括权重调整、数据 augmentation、抗性例子生成等方法，以提高抗性例子的传播性。</li>
<li>results: 本文发现，现有的方法可以增强抗性例子的传播性，但也存在一些挑战和未知，如抗性例子的数量和种类的限制，以及模型之间的传播性的不同。<details>
<summary>Abstract</summary>
The emergence of Deep Neural Networks (DNNs) has revolutionized various domains, enabling the resolution of complex tasks spanning image recognition, natural language processing, and scientific problem-solving. However, this progress has also exposed a concerning vulnerability: adversarial examples. These crafted inputs, imperceptible to humans, can manipulate machine learning models into making erroneous predictions, raising concerns for safety-critical applications. An intriguing property of this phenomenon is the transferability of adversarial examples, where perturbations crafted for one model can deceive another, often with a different architecture. This intriguing property enables "black-box" attacks, circumventing the need for detailed knowledge of the target model. This survey explores the landscape of the adversarial transferability of adversarial examples. We categorize existing methodologies to enhance adversarial transferability and discuss the fundamental principles guiding each approach. While the predominant body of research primarily concentrates on image classification, we also extend our discussion to encompass other vision tasks and beyond. Challenges and future prospects are discussed, highlighting the importance of fortifying DNNs against adversarial vulnerabilities in an evolving landscape.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Noise-Free-Score-Distillation"><a href="#Noise-Free-Score-Distillation" class="headerlink" title="Noise-Free Score Distillation"></a>Noise-Free Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17590">http://arxiv.org/abs/2310.17590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orenkatzir/nfsd">https://github.com/orenkatzir/nfsd</a></li>
<li>paper_authors: Oren Katzir, Or Patashnik, Daniel Cohen-Or, Dani Lischinski</li>
<li>For: 这 paper 探讨了 Text-to-Content Generation 在非图像领域中的实现方法，具体来说是对 Score Distillation Sampling (SDS)  proces 进行了重新解释和改进。* Methods: 该 paper 建议了一种新的 Noise-Free Score Distillation (NFSD)  proces, 该 proces 基于一种简单的解释，即通过约束 undesired noise term 的泛化，以实现更有效的 Text-to-image 模型的泛化。* Results: 作者通过提供许多质量比较的例子，证明了 NFSD  proces 可以在 Nominal Classifier-Free Guidance (CFG) scale 下实现更有效的泛化，并且可以避免结果的过度平滑，以保证生成的数据是真实的并符合描述文本的需求。<details>
<summary>Abstract</summary>
Score Distillation Sampling (SDS) has emerged as the de facto approach for text-to-content generation in non-image domains. In this paper, we reexamine the SDS process and introduce a straightforward interpretation that demystifies the necessity for large Classifier-Free Guidance (CFG) scales, rooted in the distillation of an undesired noise term. Building upon our interpretation, we propose a novel Noise-Free Score Distillation (NFSD) process, which requires minimal modifications to the original SDS framework. Through this streamlined design, we achieve more effective distillation of pre-trained text-to-image diffusion models while using a nominal CFG scale. This strategic choice allows us to prevent the over-smoothing of results, ensuring that the generated data is both realistic and complies with the desired prompt. To demonstrate the efficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as well as several other methods.
</details>
<details>
<summary>摘要</summary>
尽管Score Distillation Sampling（SDS）已成为非图像领域中文本生成的德法方法，但我们在这篇论文中又进行了重新评估和解释SDS过程。我们发现，SDS中的大型Classifier-Free Guidance（CFG）缺点是由一种不希望的噪声项引起的。基于这种解释，我们提出了一种新的Noise-Free Score Distillation（NFSD）过程，它具有最小改动，但可以更好地储备预训练的文本生成扩散模型。这种流lined设计可以避免过度熔炼结果，使得生成的数据具有真实性和满足描述的提示。为证明NFSD的有效性，我们提供了许多qualitative例子，包括NFSD和SDS以及其他方法。
</details></li>
</ul>
<hr>
<h2 id="Global-Structure-Aware-Diffusion-Process-for-Low-Light-Image-Enhancement"><a href="#Global-Structure-Aware-Diffusion-Process-for-Low-Light-Image-Enhancement" class="headerlink" title="Global Structure-Aware Diffusion Process for Low-Light Image Enhancement"></a>Global Structure-Aware Diffusion Process for Low-Light Image Enhancement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17577">http://arxiv.org/abs/2310.17577</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jinnh/GSAD">https://github.com/jinnh/GSAD</a></li>
<li>paper_authors: Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, Hui Yuan</li>
<li>for: 本文研究了一种基于分散的框架，以解决低光照图像增强问题。</li>
<li>methods: 我们提出了一种基于散度模型的方法，并在其内部加入了一个抽象的ODE-轨迹规则来正则化。这种方法利用了最近的研究结果，表明低拥挤ODE-轨迹可以导致稳定和有效的散度过程。我们在图像数据中嵌入了一个全球结构意识的正则化项，以逐渐促进图像细节的保留和对比的增强。此外，我们还引入了一种不确定度指导正则化技术，以智能地减少图像中最EXTREME的区域的约束。</li>
<li>results: 实验评估表明，提出的散度基于框架，并且补充了排名信息的正则化，在低光照图像增强中表现出色。结果表明，我们的方法可以提高图像质量、降低噪声和增强对比，相比之前的方法有显著进步。我们认为这种创新的方法将激发更多的探索和进步在低光照图像处理领域，并可能具有其他散度模型的应用。代码可以在<a target="_blank" rel="noopener" href="https://github.com/jinnh/GSAD%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jinnh/GSAD上获取。</a><details>
<summary>Abstract</summary>
This paper studies a diffusion-based framework to address the low-light image enhancement problem. To harness the capabilities of diffusion models, we delve into this intricate process and advocate for the regularization of its inherent ODE-trajectory. To be specific, inspired by the recent research that low curvature ODE-trajectory results in a stable and effective diffusion process, we formulate a curvature regularization term anchored in the intrinsic non-local structures of image data, i.e., global structure-aware regularization, which gradually facilitates the preservation of complicated details and the augmentation of contrast during the diffusion process. This incorporation mitigates the adverse effects of noise and artifacts resulting from the diffusion process, leading to a more precise and flexible enhancement. To additionally promote learning in challenging regions, we introduce an uncertainty-guided regularization technique, which wisely relaxes constraints on the most extreme regions of the image. Experimental evaluations reveal that the proposed diffusion-based framework, complemented by rank-informed regularization, attains distinguished performance in low-light enhancement. The outcomes indicate substantial advancements in image quality, noise suppression, and contrast amplification in comparison with state-of-the-art methods. We believe this innovative approach will stimulate further exploration and advancement in low-light image processing, with potential implications for other applications of diffusion models. The code is publicly available at https://github.com/jinnh/GSAD.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SD4Match-Learning-to-Prompt-Stable-Diffusion-Model-for-Semantic-Matching"><a href="#SD4Match-Learning-to-Prompt-Stable-Diffusion-Model-for-Semantic-Matching" class="headerlink" title="SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching"></a>SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17569">http://arxiv.org/abs/2310.17569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu</li>
<li>for: 本研究旨在解决图像对的semantic keypoint匹配问题。</li>
<li>methods: 本文使用Stable Diffusion（SD）的中间输出作为图像特征地图，并通过基本的提示调整技术来解 liberate SD的内在潜力，从而实现对前一些方法的显著提高。此外，我们还提出了一种新的决定式提示模块，该模块根据输入图像对的本地特征进行Conditional prompting，从而进一步提高表现。</li>
<li>results: 我们的方法SD4Match在PF-Pascal、PF-Willow和SPair-71k数据集上进行了广泛的评估，显示SD4Match在所有数据集上都设置了新的Benchmark。特别是在SPair-71k数据集上，SD4Match比前一些State-of-the-art方法提高12个百分点。<details>
<summary>Abstract</summary>
In this paper, we address the challenge of matching semantically similar keypoints across image pairs. Existing research indicates that the intermediate output of the UNet within the Stable Diffusion (SD) can serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets new benchmarks in accuracy across all these datasets. Particularly, SD4Match outperforms the previous state-of-the-art by a margin of 12 percentage points on the challenging SPair-71k dataset.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们 Addresses the challenge of matching semantically similar keypoints across image pairs. 现有研究表明，Stable Diffusion（SD）的中间输出可以 serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. 我们对PF-Pascal、PF-Willow和SPair-71k dataset进行了全面的评估，并证明SD4Match在这些dataset上设置新的benchmark，并且在SPair-71k dataset上比前一个state-of-the-art提高12个百分点。
</details></li>
</ul>
<hr>
<h2 id="Masked-Space-Time-Hash-Encoding-for-Efficient-Dynamic-Scene-Reconstruction"><a href="#Masked-Space-Time-Hash-Encoding-for-Efficient-Dynamic-Scene-Reconstruction" class="headerlink" title="Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction"></a>Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17527">http://arxiv.org/abs/2310.17527</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/masked-spacetime-hashing/msth">https://github.com/masked-spacetime-hashing/msth</a></li>
<li>paper_authors: Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, Huaping Liu</li>
<li>for:  efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos</li>
<li>methods:  Masked Space-Time Hash encoding (MSTH), a novel method that represents a dynamic scene as a weighted combination of a 3D hash encoding and a 4D hash encoding, guided by an uncertainty-based objective</li>
<li>results: consistently better results than previous methods with only 20 minutes of training time and 130 MB of memory storage<details>
<summary>Abstract</summary>
In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel method for efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos. Based on the observation that dynamic scenes often contain substantial static areas that result in redundancy in storage and computations, MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding and a 4D hash encoding. The weights for the two components are represented by a learnable mask which is guided by an uncertainty-based objective to reflect the spatial and temporal importance of each 3D position. With this design, our method can reduce the hash collision rate by avoiding redundant queries and modifications on static areas, making it feasible to represent a large number of space-time voxels by hash tables with small size.Besides, without the requirements to fit the large numbers of temporally redundant features independently, our method is easier to optimize and converge rapidly with only twenty minutes of training for a 300-frame dynamic scene.As a result, MSTH obtains consistently better results than previous methods with only 20 minutes of training time and 130 MB of memory storage. Code is available at https://github.com/masked-spacetime-hashing/msth
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了Masked Space-Time Hash编码（MSTH），一种新的方法用于高效地重建动态3D场景从多视角或单视角视频中。基于观察到的动态场景中往往包含了相对较多的静止区域，从而导致存储和计算中的重复，MSTH将动态场景表示为一个权重加权的3D哈希编码和4D哈希编码的权重组合。这个权重组合是由一个可学习的掩码引导的，掩码的学习目标是根据空间和时间的重要性来反映每个3D位置的重要性。通过这种设计，我们的方法可以避免在静止区域上的重复查询和修改，因此可以使用哈希表存储大量的空间时间坐标，并且可以快速优化和训练。在20分钟的训练时间和130MB的存储空间内，我们的方法可以在300帧动态场景中实现更好的结果，超过之前的方法。代码可以在https://github.com/masked-spacetime-hashing/msth上找到。
</details></li>
</ul>
<hr>
<h2 id="FLARE-Fast-Learning-of-Animatable-and-Relightable-Mesh-Avatars"><a href="#FLARE-Fast-Learning-of-Animatable-and-Relightable-Mesh-Avatars" class="headerlink" title="FLARE: Fast Learning of Animatable and Relightable Mesh Avatars"></a>FLARE: Fast Learning of Animatable and Relightable Mesh Avatars</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17519">http://arxiv.org/abs/2310.17519</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sbharadwajj/flare">https://github.com/sbharadwajj/flare</a></li>
<li>paper_authors: Shrisha Bharadwaj, Yufeng Zheng, Otmar Hilliges, Michael J. Black, Victoria Fernandez-Abrevaya</li>
<li>for: 创建个性化可动的3D头像，使其具有高度准确、现实主义、可重新灯光和与现有渲染系统兼容的特征。</li>
<li>methods: 通过可微分渲染和高度优化的计算机图形方法，以及部分使用神经网络来学习高级度3D磁质表示。</li>
<li>results: 实现了高质量的geometry和外观，同时具有高效地训练和渲染的特点，比现有方法更高效。<details>
<summary>Abstract</summary>
Our goal is to efficiently learn personalized animatable 3D head avatars from videos that are geometrically accurate, realistic, relightable, and compatible with current rendering systems. While 3D meshes enable efficient processing and are highly portable, they lack realism in terms of shape and appearance. Neural representations, on the other hand, are realistic but lack compatibility and are slow to train and render. Our key insight is that it is possible to efficiently learn high-fidelity 3D mesh representations via differentiable rendering by exploiting highly-optimized methods from traditional computer graphics and approximating some of the components with neural networks. To that end, we introduce FLARE, a technique that enables the creation of animatable and relightable mesh avatars from a single monocular video. First, we learn a canonical geometry using a mesh representation, enabling efficient differentiable rasterization and straightforward animation via learned blendshapes and linear blend skinning weights. Second, we follow physically-based rendering and factor observed colors into intrinsic albedo, roughness, and a neural representation of the illumination, allowing the learned avatars to be relit in novel scenes. Since our input videos are captured on a single device with a narrow field of view, modeling the surrounding environment light is non-trivial. Based on the split-sum approximation for modeling specular reflections, we address this by approximating the pre-filtered environment map with a multi-layer perceptron (MLP) modulated by the surface roughness, eliminating the need to explicitly model the light. We demonstrate that our mesh-based avatar formulation, combined with learned deformation, material, and lighting MLPs, produces avatars with high-quality geometry and appearance, while also being efficient to train and render compared to existing approaches.
</details>
<details>
<summary>摘要</summary>
我们的目标是高效地学习个性化可动的3D头像，从视频中学习高精度、现实、可重新照明和现代渲染系统兼容的3D矩阵表示。尽管3D矩阵可以高效处理和高度可移植，但它们缺乏实际上的形状和外观真实性。神经表示方法，则是真实的，但它们在训练和渲染过程中较慢，而且兼容性不佳。我们的关键发现是可以通过可导式渲染来高效地学习高精度3D矩阵表示，并通过高度优化的计算机图形方法来减少一些组件的神经网络。为了实现这一点，我们提出了FLARE技术，它可以从单个照视视频中生成可动和可重新照明的3D头像。首先，我们通过 mesh 表示来学习 canonical geometry，以便高效地进行可导式渲染和学习混合形状和线性混合皮肤弹性参数。其次，我们遵循物理学渲染，将观察到的颜色 decomposed 为内在反射率、粗糙度和神经网络中的照明表示，以便学习的头像在新的场景中被重新照明。由于我们的输入视频是在单个设备上捕捉的，因此模拟环境光是非常困难的。在基于折射积分的approximation中，我们使用多层感知器（MLP）模ulated by surface roughness来approximate环境图像，从而消除了必须直接模型光的需求。我们示出了我们的矩阵形式的 avatar 概合，与学习的形变、材质和照明 MLP 相结合，可以生成高质量的形状和外观，同时也比现有方法更高效地训练和渲染。
</details></li>
</ul>
<hr>
<h2 id="Revisiting-the-Distillation-of-Image-Representations-into-Point-Clouds-for-Autonomous-Driving"><a href="#Revisiting-the-Distillation-of-Image-Representations-into-Point-Clouds-for-Autonomous-Driving" class="headerlink" title="Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving"></a>Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17504">http://arxiv.org/abs/2310.17504</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane Siméoni, Corentin Sautier, Patrick Pérez, Andrei Bursuc, Renaud Marlet</li>
<li>for: 本文旨在提出一种简单的2D-to-3D填充方法，用于提高自动驾驶场景中的自我监督3D网络表现。</li>
<li>methods: 本文提出了一种简单的2D-to-3D填充方法，通过填充高质量的2D特征到3D网络中来提高3D网络的表现。此外，本文还运用了高容量3D网络进行填充，以提高3D特征质量。</li>
<li>results: 本文的实验结果表明，通过使用提出的2D-to-3D填充方法，可以显著提高自动驾驶场景中的自我监督3D网络表现，并且可以用于开放词汇分割和背景&#x2F;前景发现。<details>
<summary>Abstract</summary>
Self-supervised image networks can be used to address complex 2D tasks (e.g., semantic segmentation, object discovery) very efficiently and with little or no downstream supervision. However, self-supervised 3D networks on lidar data do not perform as well for now. A few methods therefore propose to distill high-quality self-supervised 2D features into 3D networks. The most recent ones doing so on autonomous driving data show promising results. Yet, a performance gap persists between these distilled features and fully-supervised ones. In this work, we revisit 2D-to-3D distillation. First, we propose, for semantic segmentation, a simple approach that leads to a significant improvement compared to prior 3D distillation methods. Second, we show that distillation in high capacity 3D networks is key to reach high quality 3D features. This actually allows us to significantly close the gap between unsupervised distilled 3D features and fully-supervised ones. Last, we show that our high-quality distilled representations can also be used for open-vocabulary segmentation and background/foreground discovery.
</details>
<details>
<summary>摘要</summary>
自我监督图像网络可以非常高效地解决复杂的2D任务（例如semantic segmentation、物体发现），但是自我监督3D网络在激光数据上并不表现太好。一些方法因此提议将高质量的2D自我监督特征融合到3D网络中。最新的这些方法在自主驾驶数据上显示了有前途的结果。然而，与完全监督的特征相比，这些融合的特征仍然存在一定的性能差距。在这项工作中，我们重新审视2D-to-3D融合。首先，我们提议用于semantic segmentation的简单方法，这会比之前的3D融合方法带来显著的改善。其次，我们表明了高容量3D网络中的融合是关键来达到高质量3D特征。这实际上使得我们可以明显减小不监督融合3D特征和完全监督特征之间的性能差距。最后，我们表明了我们高质量的融合表示可以用于开放词汇分割和背景/前景发现。
</details></li>
</ul>
<hr>
<h2 id="A-Hybrid-Graph-Network-for-Complex-Activity-Detection-in-Video"><a href="#A-Hybrid-Graph-Network-for-Complex-Activity-Detection-in-Video" class="headerlink" title="A Hybrid Graph Network for Complex Activity Detection in Video"></a>A Hybrid Graph Network for Complex Activity Detection in Video</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17493">http://arxiv.org/abs/2310.17493</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/salmank255/CompAD">https://github.com/salmank255/CompAD</a></li>
<li>paper_authors: Salman Khan, Izzeddin Teeti, Andrew Bradley, Mohamed Elhoseiny, Fabio Cuzzolin<br>for:本研究旨在解决视频中复杂活动检测问题，尤其是在自动驾驶和体育分析等领域。methods:本研究使用混合图神经网络，其combines attention应用于本地动态场景中的图编码和时间图模型。results:研究结果表明，该方法在三个数据集上都超过了之前的状态OF-THE-ART方法。<details>
<summary>Abstract</summary>
Interpretation and understanding of video presents a challenging computer vision task in numerous fields - e.g. autonomous driving and sports analytics. Existing approaches to interpreting the actions taking place within a video clip are based upon Temporal Action Localisation (TAL), which typically identifies short-term actions. The emerging field of Complex Activity Detection (CompAD) extends this analysis to long-term activities, with a deeper understanding obtained by modelling the internal structure of a complex activity taking place within the video. We address the CompAD problem using a hybrid graph neural network which combines attention applied to a graph encoding the local (short-term) dynamic scene with a temporal graph modelling the overall long-duration activity. Our approach is as follows: i) Firstly, we propose a novel feature extraction technique which, for each video snippet, generates spatiotemporal `tubes' for the active elements (`agents') in the (local) scene by detecting individual objects, tracking them and then extracting 3D features from all the agent tubes as well as the overall scene. ii) Next, we construct a local scene graph where each node (representing either an agent tube or the scene) is connected to all other nodes. Attention is then applied to this graph to obtain an overall representation of the local dynamic scene. iii) Finally, all local scene graph representations are interconnected via a temporal graph, to estimate the complex activity class together with its start and end time. The proposed framework outperforms all previous state-of-the-art methods on all three datasets including ActivityNet-1.3, Thumos-14, and ROAD.
</details>
<details>
<summary>摘要</summary>
视频内容理解和解释存在许多领域中是一项computer vision挑战，如自动驾驶和运动分析。现有的视频clip中动作解释方法基于Temporal Action Localisation（TAL），通常可以识别短期动作。然而，emerging field of Complex Activity Detection（CompAD）扩展了这种分析，通过模型内部结构的复杂活动来获得更深刻的理解。我们使用混合图解释器来解决CompAD问题，其 combine了对本地（短期）动态场景的图编码器和时间图模型。我们的方法如下：1.首先，我们提出了一种新的特征提取技术，对于每个视频剪辑，生成了空间时间的“管” для活动元素（agent）在本地场景中，通过对个体物体的检测、跟踪和提取3D特征来实现。2.接下来，我们构建了本地场景图，其中每个节点（表示agent管或场景）与其他节点相连。然后，我们应用了注意力来获得本地动态场景的总体表示。3.最后，所有的本地场景图表示被通过时间图相连，以估计复杂活动类型以及其开始和结束时间。我们提出的框架在所有三个数据集上都超过了之前的所有状态的方法，包括ActivityNet-1.3、Thumos-14和ROAD。
</details></li>
</ul>
<hr>
<h2 id="Cross-modal-Active-Complementary-Learning-with-Self-refining-Correspondence"><a href="#Cross-modal-Active-Complementary-Learning-with-Self-refining-Correspondence" class="headerlink" title="Cross-modal Active Complementary Learning with Self-refining Correspondence"></a>Cross-modal Active Complementary Learning with Self-refining Correspondence</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17468">http://arxiv.org/abs/2310.17468</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/qinyang79/crcl">https://github.com/qinyang79/crcl</a></li>
<li>paper_authors: Yang Qin, Yuan Sun, Dezhong Peng, Joey Tianyi Zhou, Xi Peng, Peng Hu</li>
<li>for: 提高图文匹配的Robustness，增强现有方法的鲁棒性。</li>
<li>methods: 提出一种Generalized Cross-modal Robust Complementary Learning框架（CRCL），利用Active Complementary Loss（ACL）和Self-refining Correspondence Correction（SCC）提高现有方法的鲁棒性。</li>
<li>results: 经验和理论 pruebas 表明，CRCL可以减少NC的影响，提高图文匹配的精度和稳定性。<details>
<summary>Abstract</summary>
Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous supervision, leading to theoretically and experimentally demonstrated robustness against NC. SCC utilizes multiple self-refining processes with momentum correction to enlarge the receptive field for correcting correspondences, thereby alleviating error accumulation and achieving accurate and stable corrections. We carry out extensive experiments on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify the superior robustness of our CRCL against synthetic and real-world noisy correspondences.
</details>
<details>
<summary>摘要</summary>
近些时候，图文匹配已经引起了学术和业界的越来越多的关注，这是图文模态之间的隐藏相关性的基础。然而，大多数现有方法假设训练对都是正确的，而忽略了普遍存在的注释噪声（NC），从而不可避免性下降。虽然一些方法尝试解决这种噪声，但它们仍然面临两个挑战：过度记忆和不可靠的NC修正，特别是在高噪声下。为Address这两个问题，我们提议一种通用的跨模态Robust Complementary Learning框架（CRCL），它具有一种新的Active Complementary Loss（ACL）和一种有效的Self-refining Correspondence Correction（SCC），以提高现有方法的 robustness。Specifically, ACL利用活动和补做学习损失来减少提供错误指导的风险，从而实际和实验证明了对NC的robustness。SCC利用多个自适应过程和振荡调整来扩大对匹配的感知范围，从而缓解错误的积累和实现精准和稳定的修正。我们在Flickr30K、MS-COCO和CC152K三个图文benchmark上进行了广泛的实验，以验证我们的CRCL对于Synthetic和Real-world NC的超越性。
</details></li>
</ul>
<hr>
<h2 id="OTMatch-Improving-Semi-Supervised-Learning-with-Optimal-Transport"><a href="#OTMatch-Improving-Semi-Supervised-Learning-with-Optimal-Transport" class="headerlink" title="OTMatch: Improving Semi-Supervised Learning with Optimal Transport"></a>OTMatch: Improving Semi-Supervised Learning with Optimal Transport</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17455">http://arxiv.org/abs/2310.17455</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiquan Tan, Kaipeng Zheng, Weiran Huang</li>
<li>for: 这篇论文是为了提高 semi-supervised learning 中的学习效果，使用有限量的标签数据，并利用无标签数据中的资讯。</li>
<li>methods: 这篇论文使用的方法是使用 optimal transport loss function，并且利用这些数据中的 semantic relationships 来提高学习效果。</li>
<li>results:  compared to current state-of-the-art method FreeMatch，OTMatch 可以实现更高的精度，具体的测试结果为 CIFAR-10 上的标签数据下的误差比 FreeMatch 降低了 3.18%、STL-10 上的标签数据下的误差比 FreeMatch 降低了 3.46%、ImageNet 上的标签数据下的误差比 FreeMatch 降低了 1.28%。这显示了 OTMatch 的优化性和超越性。<details>
<summary>Abstract</summary>
Semi-supervised learning has made remarkable strides by effectively utilizing a limited amount of labeled data while capitalizing on the abundant information present in unlabeled data. However, current algorithms often prioritize aligning image predictions with specific classes generated through self-training techniques, thereby neglecting the inherent relationships that exist within these classes. In this paper, we present a new approach called OTMatch, which leverages semantic relationships among classes by employing an optimal transport loss function. By utilizing optimal transport, our proposed method consistently outperforms established state-of-the-art methods. Notably, we observed a substantial improvement of a certain percentage in accuracy compared to the current state-of-the-art method, FreeMatch. OTMatch achieves 3.18%, 3.46%, and 1.28% error rate reduction over FreeMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels per class, respectively. This demonstrates the effectiveness and superiority of our approach in harnessing semantic relationships to enhance learning performance in a semi-supervised setting.
</details>
<details>
<summary>摘要</summary>
semi-supervised learning 已经做出了很大的进步，通过有效地利用有限量的标注数据和丰富的无标注数据，实现了不错的学习效果。然而，目前的算法经常强调通过自我训练技术生成的特定类型来对图像进行预测，从而忽视了这些类型之间的内在关系。在这篇论文中，我们提出了一种新的方法 called OTMatch，它利用类型之间的 semantics 关系，通过最优运输损失函数来进行学习。通过使用最优运输，我们的提议方法可以一直超越现有的状态平衡方法。我们注意到，与现有状态平衡方法 FreeMatch 进行比较，OTMatch 在 CIFAR-10 上with 1 个标签、STL-10 上with 4 个标签、以及 ImageNet 上with 100 个标签时都具有较高的精度。这表明我们的方法可以借助类型之间的 semantics 关系，提高 semi-supervised 学习中的学习性能。
</details></li>
</ul>
<hr>
<h2 id="Sign-Languague-Recognition-without-frame-sequencing-constraints-A-proof-of-concept-on-the-Argentinian-Sign-Language"><a href="#Sign-Languague-Recognition-without-frame-sequencing-constraints-A-proof-of-concept-on-the-Argentinian-Sign-Language" class="headerlink" title="Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language"></a>Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17437">http://arxiv.org/abs/2310.17437</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Ronchetti, Facundo Manuel Quiroga, César Estrebou, Laura Lanzarini, Alejandro Rosete</li>
<li>for: 这个论文旨在提出一种可靠的手语识别方法，以帮助听力障碍人士和听力正常人士之间的交流和教学。</li>
<li>methods: 该论文提出了一种概率模型，结合不同类型的特征（如位置、运动和手势）进行手语分类。该模型采用袋子中的词法方法，以探索论文中的假设，即不需要顺序来进行识别。</li>
<li>results: 该论文在使用阿根廷手语数据集（包含64个手语类和3200个样本）时，实现了97%的准确率，提供了一些证据，证明了论文中的假设是可行的。<details>
<summary>Abstract</summary>
Automatic sign language recognition (SLR) is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language for the hearing population.   SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or similar models to recognize signs. Such techniques exploit the sequential ordering of frames to reduce the number of hypothesis. This paper presents a general probabilistic model for sign classification that combines sub-classifiers based on different types of features such as position, movement and handshape. The model employs a bag-of-words approach in all classification steps, to explore the hypothesis that ordering is not essential for recognition. The proposed model achieved an accuracy rate of 97% on an Argentinian Sign Language dataset containing 64 classes of signs and 3200 samples, providing some evidence that indeed recognition without ordering is possible.
</details>
<details>
<summary>摘要</summary>
自动手语识别（SLR）是人机交互和机器学习领域中的一个重要话题。一方面，它需要多种知识领域的交叠，如视频处理、图像处理、智能系统和语言学。另一方面，可靠地识别手语可以帮助翻译过程和听力障碍人士的 интеграción，以及听力人士学习手语。通常，SLR系统使用隐藏Marker模型、动态时间扩展或类似模型来识别手语。这些技术利用手语的顺序排序来减少假设数量。本文提出了一种通用的手语分类模型，该模型结合不同类型的特征，如位置、运动和手势。该模型使用袋包法在所有分类步骤中，以探索假设That ordering是不必须的。提出的模型在阿根廷手语数据集上达到了97%的准确率，包括64个手语类和3200个样本，提供了一些证据，证明了实际上可以通过识别手语而不需要顺序。
</details></li>
</ul>
<hr>
<h2 id="Uncertainty-weighted-Loss-Functions-for-Improved-Adversarial-Attacks-on-Semantic-Segmentation"><a href="#Uncertainty-weighted-Loss-Functions-for-Improved-Adversarial-Attacks-on-Semantic-Segmentation" class="headerlink" title="Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation"></a>Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17436">http://arxiv.org/abs/2310.17436</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kmaag/uncertainty-weighted-loss">https://github.com/kmaag/uncertainty-weighted-loss</a></li>
<li>paper_authors: Kira Maag, Asja Fischer</li>
<li>for: 防止深度神经网络受到攻击，提高图像分割的可靠性。</li>
<li>methods: 使用简单的不确定性权重方案，将攻击损失函数中的像素级别权重提高，并将确定错误分类的像素loss设为零。</li>
<li>results: 在多个 dataset 和模型上进行了实质性分析，显示了这些方法可以提高攻击性能。<details>
<summary>Abstract</summary>
State-of-the-art deep neural networks have been shown to be extremely powerful in a variety of perceptual tasks like semantic segmentation. However, these networks are vulnerable to adversarial perturbations of the input which are imperceptible for humans but lead to incorrect predictions. Treating image segmentation as a sum of pixel-wise classifications, adversarial attacks developed for classification models were shown to be applicable to segmentation models as well. In this work, we present simple uncertainty-based weighting schemes for the loss functions of such attacks that (i) put higher weights on pixel classifications which can more easily perturbed and (ii) zero-out the pixel-wise losses corresponding to those pixels that are already confidently misclassified. The weighting schemes can be easily integrated into the loss function of a range of well-known adversarial attackers with minimal additional computational overhead, but lead to significant improved perturbation performance, as we demonstrate in our empirical analysis on several datasets and models.
</details>
<details>
<summary>摘要</summary>
现代深度神经网络已经在多种感知任务中展现出极高的能力，如Semantic Segmentation。然而，这些网络受到输入杂音的影响，这些杂音对人类来说是难以看到的，但会导致错误预测。对于图像分割任务，我们将图像分割看作是每个像素的分类问题。在这个工作中，我们提出了一些简单的不确定性基于权重的损失函数，其中（i）将更容易受到杂音影响的像素分类权重高，（ii）将确定错误分类的像素权重设为0。这些权重函数可以轻松地与许多已知的恶意攻击者损失函数结合使用，但会导致显著提高杂音性能，我们在多个 dataset 和模型上进行了实验性分析，并证明了这一点。
</details></li>
</ul>
<hr>
<h2 id="AntifakePrompt-Prompt-Tuned-Vision-Language-Models-are-Fake-Image-Detectors"><a href="#AntifakePrompt-Prompt-Tuned-Vision-Language-Models-are-Fake-Image-Detectors" class="headerlink" title="AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors"></a>AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17419">http://arxiv.org/abs/2310.17419</a></li>
<li>repo_url: None</li>
<li>paper_authors: You-Ming Chang, Chen Yeh, Wei-Chen Chiu, Ning Yu</li>
<li>for: 这个论文目的是提出一种基于语言视觉模型的深层伪造探测方法，以提高对未见到的数据的深层伪造探测精度。</li>
<li>methods: 这篇论文使用了InstructBLIP和提问调整技术，将深层伪造探测转换为视觉问题，并对InstructBLIP进行软题调整以回答问题中的真伪信息。</li>
<li>results: 实验结果显示，使用预训练的语言视觉模型并进行提问调整可以很好地提高深层伪造探测精度，从58.8%提高到91.31%，而且这些进步仅需要较少的培训参数，因此这是一个有效和经济的深层伪造探测解决方案。<details>
<summary>Abstract</summary>
Deep generative models can create remarkably photorealistic fake images while raising concerns about misinformation and copyright infringement, known as deepfake threats. Deepfake detection technique is developed to distinguish between real and fake images, where the existing methods typically learn classifiers in the image domain or various feature domains. However, the generalizability of deepfake detection against emerging and more advanced generative models remains challenging. In this paper, being inspired by the zero-shot advantages of Vision-Language Models (VLMs), we propose a novel approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake detection accuracy over unseen data. We formulate deepfake detection as a visual question answering problem, and tune soft prompts for InstructBLIP to answer the real/fake information of a query image. We conduct full-spectrum experiments on datasets from 3 held-in and 13 held-out generative models, covering modern text-to-image generation, image editing and image attacks. Results demonstrate that (1) the deepfake detection accuracy can be significantly and consistently improved (from 58.8% to 91.31%, in average accuracy over unseen data) using pretrained vision-language models with prompt tuning; (2) our superior performance is at less cost of trainable parameters, resulting in an effective and efficient solution for deepfake detection. Code and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.
</details>
<details>
<summary>摘要</summary>
深度生成模型可创造出极其真实的假图像，但也引发了假信息和版权侵犯的问题，称为深度假图检测问题。现有的方法通常是在图像领域或多个特征领域学习分类器。然而，对于出现和更进一步的生成模型来说，总的来说是很困难的。在这篇论文中，我们受到了零批优势的视觉语言模型（VLMs）的启发，我们提出了一种新的方法，使用VLMs（例如InstructBLIP）和提问技术来改进深度假图检测精度。我们将深度假图检测转化为视觉问答问题，并对InstructBLIP进行软提问的调整，以回答查询图像的真假信息。我们在3个保持数据集和13个保持数据集上进行了全谱试验，覆盖了现代文本到图像生成、图像修改和图像攻击等多种生成模型。结果显示：1. 使用预训练的视觉语言模型和提问调整可以显著提高深度假图检测精度（从58.8%提高到91.31%，平均精度提高）。2. 我们的优秀表现在较少的可学习参数上，具有效率和可行的解决方案。代码和模型可以在https://github.com/nctu-eva-lab/AntifakePrompt中找到。
</details></li>
</ul>
<hr>
<h2 id="Circuit-as-Set-of-Points"><a href="#Circuit-as-Set-of-Points" class="headerlink" title="Circuit as Set of Points"></a>Circuit as Set of Points</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17418">http://arxiv.org/abs/2310.17418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hustvl/circuitformer">https://github.com/hustvl/circuitformer</a></li>
<li>paper_authors: Jialv Zou, Xinggang Wang, Jiahao Guo, Wenyu Liu, Qian Zhang, Chang Huang</li>
<li>for: 该 paper 主要用于提高 Electronic Design Automation (EDA) 中的电路设计过程中的缓存和 Routing 阶段的效率，通过使用人工智能技术来帮助电路设计。</li>
<li>methods: 该 paper 使用 Transformer-based point cloud perception 方法来提取电路组件的特征，无需预处理，可以进行终端训练，并且实现了高性能。</li>
<li>results: 实验结果显示，该方法在 CircuitNet 和 ISPD2015 数据集上的堵塞预测任务以及 CircuitNet 数据集上的设计规则检查 (DRC) 预测任务中均达到了状态数据集的最高表现。<details>
<summary>Abstract</summary>
As the size of circuit designs continues to grow rapidly, artificial intelligence technologies are being extensively used in Electronic Design Automation (EDA) to assist with circuit design. Placement and routing are the most time-consuming parts of the physical design process, and how to quickly evaluate the placement has become a hot research topic. Prior works either transformed circuit designs into images using hand-crafted methods and then used Convolutional Neural Networks (CNN) to extract features, which are limited by the quality of the hand-crafted methods and could not achieve end-to-end training, or treated the circuit design as a graph structure and used Graph Neural Networks (GNN) to extract features, which require time-consuming preprocessing. In our work, we propose a novel perspective for circuit design by treating circuit components as point clouds and using Transformer-based point cloud perception methods to extract features from the circuit. This approach enables direct feature extraction from raw data without any preprocessing, allows for end-to-end training, and results in high performance. Experimental results show that our method achieves state-of-the-art performance in congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as well as in design rule check (DRC) violation prediction tasks on the CircuitNet dataset. Our method establishes a bridge between the relatively mature point cloud perception methods and the fast-developing EDA algorithms, enabling us to leverage more collective intelligence to solve this task. To facilitate the research of open EDA design, source codes and pre-trained models are released at https://github.com/hustvl/circuitformer.
</details>
<details>
<summary>摘要</summary>
随着电路设计的大小不断增长，人工智能技术在电子设计自动化（EDA）中得到了广泛应用，以帮助电路设计。电路的位置和路径是物理设计过程中最时间consuming的部分，如何快速评估电路的位置已成为热点研究话题。先前的工作都是将电路设计转化为图像使用手动方法，然后使用卷积神经网络（CNN）提取特征，但这种方法的特征提取有限，无法实现端到端训练。另一些工作则是将电路设计视为图structure，使用图神经网络（GNN）提取特征，但这种方法需要时间consuming的预处理。在我们的工作中，我们提出了一种新的电路设计视角，将电路组件视为点云，使用基于点云的传输器来提取电路特征。这种方法可以直接从原始数据提取特征，不需预处理，可以实现端到端训练，并且实现了高性能。实验结果表明，我们的方法在CircuitNet和ISPD2015 datasets上的堵塞预测任务和CircuitNetdataset上的设计规则检查（DRC）预测任务中均 achieve state-of-the-art表现。我们的方法打通了相对较成熔的点云识别方法和快速发展的 EDA 算法之间的桥梁，使我们可以更好地利用全球的智能来解决这个任务。为便于开放式 EDA 设计的研究，我们在 GitHub 上发布了源代码和预训练模型，请参考 <https://github.com/hustvl/circuitformer>。
</details></li>
</ul>
<hr>
<h2 id="Detection-Defenses-An-Empty-Promise-against-Adversarial-Patch-Attacks-on-Optical-Flow"><a href="#Detection-Defenses-An-Empty-Promise-against-Adversarial-Patch-Attacks-on-Optical-Flow" class="headerlink" title="Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow"></a>Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17403">http://arxiv.org/abs/2310.17403</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cv-stuttgart/detectiondefenses">https://github.com/cv-stuttgart/detectiondefenses</a></li>
<li>paper_authors: Erik Scheurer, Jenny Schmalfuss, Alexander Lis, Andrés Bruhn</li>
<li>for: 这篇论文旨在检验目前可用的检测并移除防御策略（ILP和LGS）对state-of-the-art Optical Flow方法的影响，以及这些防御策略对攻击者的抗击能力。</li>
<li>methods: 这篇论文使用了多种state-of-the-art Optical Flow方法，并对这些方法进行了防御策略的检测和移除。</li>
<li>results: 实验结果表明，目前使用的检测并移除防御策略不仅会下降无抗的场景中的Optical Flow质量，还会削弱对攻击者的抗击能力。这些防御策略不能提供预期的安全性。<details>
<summary>Abstract</summary>
Adversarial patches undermine the reliability of optical flow predictions when placed in arbitrary scene locations. Therefore, they pose a realistic threat to real-world motion detection and its downstream applications. Potential remedies are defense strategies that detect and remove adversarial patches, but their influence on the underlying motion prediction has not been investigated. In this paper, we thoroughly examine the currently available detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art optical flow methods, and illuminate their side effects on the quality and robustness of the final flow predictions. In particular, we implement defense-aware attacks to investigate whether current defenses are able to withstand attacks that take the defense mechanism into account. Our experiments yield two surprising results: Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patch attacks for all tested optical flow methods except FlowNetC. As currently employed detect-and-remove defenses fail to deliver the promised adversarial robustness for optical flow, they evoke a false sense of security. The code is available at https://github.com/cv-stuttgart/DetectionDefenses.
</details>
<details>
<summary>摘要</summary>
“敌对画面推帧可以让光流预测结果不可靠，因此它对实际世界中的动作探测和其下渠道应用存在实际的威胁。因此，可能的解决方案是针对攻击画面中的敌对画面进行探测和移除。但是，这些防护策略对光流预测的质量和可靠性的影响尚未得到充分的探讨。在这篇论文中，我们对一些现有的探测和移除防护措施ILP和LGS进行了广泛的测试，并评估它们对光流预测的质量和可靠性的影响。尤其是，我们实现了防护意识攻击，以检查现有的防护策略是否能够抵抗这种攻击。我们的实验结果产生了两个惊喜：探测和移除防护不仅会对正常场景下的光流质量产生负面影响，而且会对所有测试过的光流方法（除了FlowNetC）下的攻击实际上减少其防护能力。现在的探测和移除防护无法为光流预测提供实际的敌对防护，它们产生了一个假的安全感。我们的代码可以在https://github.com/cv-stuttgart/DetectionDefenses上获得。”
</details></li>
</ul>
<hr>
<h2 id="Learning-Temporal-Sentence-Grounding-From-Narrated-EgoVideos"><a href="#Learning-Temporal-Sentence-Grounding-From-Narrated-EgoVideos" class="headerlink" title="Learning Temporal Sentence Grounding From Narrated EgoVideos"></a>Learning Temporal Sentence Grounding From Narrated EgoVideos</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17395">http://arxiv.org/abs/2310.17395</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/keflanagan/climer">https://github.com/keflanagan/climer</a></li>
<li>paper_authors: Kevin Flanagan, Dima Damen, Michael Wray</li>
<li>for: 这 paper 的目的是解决长形自 centered 数据集（如 Ego4D 和 EPIC-Kitchens）对 temporal sentence grounding (TSG) 任务的新挑战。</li>
<li>methods: 该 paper 使用了只使用笔记和其相对粗略的时间戳来学习在这些数据集中附加 sentences。它们提出了一种名为 clip merging (CliMer) 的方法，通过文本控制注意力来进行对比性增强。</li>
<li>results: 对比高效的 TSG 方法，CliMer 方法可以提高 mean R@1 的性能，从 3.9 提高到 5.7 在 Ego4D 上，从 10.7 提高到 13.0 在 EPIC-Kitchens 上。<details>
<summary>Abstract</summary>
The onset of long-form egocentric datasets such as Ego4D and EPIC-Kitchens presents a new challenge for the task of Temporal Sentence Grounding (TSG). Compared to traditional benchmarks on which this task is evaluated, these datasets offer finer-grained sentences to ground in notably longer videos. In this paper, we develop an approach for learning to ground sentences in these datasets using only narrations and their corresponding rough narration timestamps. We propose to artificially merge clips to train for temporal grounding in a contrastive manner using text-conditioning attention. This Clip Merging (CliMer) approach is shown to be effective when compared with a high performing TSG method -- e.g. mean R@1 improves from 3.9 to 5.7 on Ego4D and from 10.7 to 13.0 on EPIC-Kitchens. Code and data splits available from: https://github.com/keflanagan/CliMer
</details>
<details>
<summary>摘要</summary>
“ egocentric 数据集如 Ego4D 和 EPIC-Kitchens 的出现提出了新的挑战 для时间句子固定（TSG）任务。与传统的评估标准相比，这些数据集提供了更细化的句子，需要在视频中进行更精细的固定。在这篇论文中，我们提出了使用 narraion 和其相应的粗略 narraion 时间戳来学习固定 sentences。我们称之为 clip merging（CliMer）方法，它通过文本控制注意力来进行对比性训练。我们的 CliMer 方法在比较高性能 TSG 方法（如 Mean R@1）的基础上进行了改进，例如在 Ego4D 上从 3.9 提高到 5.7，在 EPIC-Kitchens 上从 10.7 提高到 13.0。代码和数据分割可以从 GitHub 上获取：https://github.com/keflanagan/CliMer。”Note that Simplified Chinese is the standard writing system used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="SE-3-Diffusion-Model-based-Point-Cloud-Registration-for-Robust-6D-Object-Pose-Estimation"><a href="#SE-3-Diffusion-Model-based-Point-Cloud-Registration-for-Robust-6D-Object-Pose-Estimation" class="headerlink" title="SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation"></a>SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17359">http://arxiv.org/abs/2310.17359</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Jiang-HB/DiffusionReg">https://github.com/Jiang-HB/DiffusionReg</a></li>
<li>paper_authors: Haobo Jiang, Mathieu Salzmann, Zheng Dang, Jin Xie, Jian Yang</li>
<li>for: 6D object pose estimation in real-world scenarios</li>
<li>methods: SE(3) diffusion model-based point cloud registration framework</li>
<li>results: Outstanding pose estimation performance on real-world datasets (TUD-L, LINEMOD, and Occluded-LINEMOD)<details>
<summary>Abstract</summary>
In this paper, we introduce an SE(3) diffusion model-based point cloud registration framework for 6D object pose estimation in real-world scenarios. Our approach formulates the 3D registration task as a denoising diffusion process, which progressively refines the pose of the source point cloud to obtain a precise alignment with the model point cloud. Training our framework involves two operations: An SE(3) diffusion process and an SE(3) reverse process. The SE(3) diffusion process gradually perturbs the optimal rigid transformation of a pair of point clouds by continuously injecting noise (perturbation transformation). By contrast, the SE(3) reverse process focuses on learning a denoising network that refines the noisy transformation step-by-step, bringing it closer to the optimal transformation for accurate pose estimation. Unlike standard diffusion models used in linear Euclidean spaces, our diffusion model operates on the SE(3) manifold. This requires exploiting the linear Lie algebra $\mathfrak{se}(3)$ associated with SE(3) to constrain the transformation transitions during the diffusion and reverse processes. Additionally, to effectively train our denoising network, we derive a registration-specific variational lower bound as the optimization objective for model learning. Furthermore, we show that our denoising network can be constructed with a surrogate registration model, making our approach applicable to different deep registration networks. Extensive experiments demonstrate that our diffusion registration framework presents outstanding pose estimation performance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets.
</details>
<details>
<summary>摘要</summary>
“在这篇论文中，我们介绍了基于SE(3)扩散模型的点云注准框架，用于在实际场景中进行6D对象pose估计。我们的方法将3D注准任务转化为一个滤净扩散过程，通过不断注入噪声（扰动变换）来逐步纠正源点云的pose，以达到精确对齐model点云。我们的训练过程包括两个操作：SE(3)扩散过程和SE(3)反向过程。SE(3)扩散过程逐渐扰动优化的rigid变换，而SE(3)反向过程则是学习一个滤净网络，逐步纠正噪声后的不确定变换，使其更加精确地对齐pose。与标准扩散模型在线性Euclidean空间中使用不同，我们的扩散模型在SE(3)拟合上运行。这需要利用SE(3)拟合中的线性李代数 $\mathfrak{se}(3)$ 来约束变换过程中的过渡。此外，为了有效地训练我们的滤净网络，我们 derivates一种注准特有的下界为优化目标，并证明我们的滤净网络可以通过 substitute registration model来实现。我们的实验表明，我们的扩散注准框架在实际世界TUD-L、LINEMOD和Occluded-LINEMOD数据集上具有出色的pose估计性能。”
</details></li>
</ul>
<hr>
<h2 id="Sky-Imager-Based-Forecast-of-Solar-Irradiance-Using-Machine-Learning"><a href="#Sky-Imager-Based-Forecast-of-Solar-Irradiance-Using-Machine-Learning" class="headerlink" title="Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning"></a>Sky Imager-Based Forecast of Solar Irradiance Using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17356">http://arxiv.org/abs/2310.17356</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Al-lahham, Obaidah Theeb, Khaled Elalem, Tariq A. Alshawi, Saleh A. Alshebeili</li>
<li>for: 预测能源含量稳定电网和不间断服务。</li>
<li>methods: 提出了一种新的天际图像特征提取和学习基本技术来估计短期太阳辐射。</li>
<li>results: 与文献中已知的计算高效算法相比，我们的方法实现了竞争性的结果，而且计算复杂度减少了多。<details>
<summary>Abstract</summary>
Ahead-of-time forecasting of the output power of power plants is essential for the stability of the electricity grid and ensuring uninterrupted service. However, forecasting renewable energy sources is difficult due to the chaotic behavior of natural energy sources. This paper presents a new approach to estimate short-term solar irradiance from sky images. The~proposed algorithm extracts features from sky images and use learning-based techniques to estimate the solar irradiance. The~performance of proposed machine learning (ML) algorithm is evaluated using two publicly available datasets of sky images. The~datasets contain over 350,000 images for an interval of 16 years, from 2004 to 2020, with the corresponding global horizontal irradiance (GHI) of each image as the ground truth. Compared to the state-of-the-art computationally heavy algorithms proposed in the literature, our approach achieves competitive results with much less computational complexity for both nowcasting and forecasting up to 4 h ahead of time.
</details>
<details>
<summary>摘要</summary>
预测发电厂输出功率的预测是电力网络稳定和无间断服务的关键。然而，预测可再生能源很困难，因为自然能源的行为是杂乱的。本文提出了一种新的方法来估算短期日射量。该算法从天空图像中提取特征，并使用学习技术来估算太阳辐射。提出的机器学习（ML）算法的性能被评估使用了两个公开available的天空图像数据集。这两个数据集包含了2004年至2020年的16年间，共有350,000张图像，每张图像的全球水平照度（GHI）作为真实值。相比之前在文献中提出的计算沉重的算法，我们的方法实现了与之相当的竞争力，而且计算复杂性减少了多。这意味着我们的方法可以在4小时之前对nowcasting和预测进行有效的预测。
</details></li>
</ul>
<hr>
<h2 id="CADS-Unleashing-the-Diversity-of-Diffusion-Models-through-Condition-Annealed-Sampling"><a href="#CADS-Unleashing-the-Diversity-of-Diffusion-Models-through-Condition-Annealed-Sampling" class="headerlink" title="CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling"></a>CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17347">http://arxiv.org/abs/2310.17347</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seyedmorteza Sadat, Jakob Buhmann, Derek Bradely, Otmar Hilliges, Romann M. Weber</li>
<li>for: 提高 diffusion models 的输出多样性，特别是在高指导缩放比例下或者在小数据集上训练时。</li>
<li>methods: 提供一种改进的抽样策略，通过在推理过程中添加 scheduled, monotonically decreasing Gaussian noise 来平衡多样性和条件匹配。</li>
<li>results: 在多种条件生成任务中，使用现有预训练 diffusion model，CADS 可以提高 diffusion models 的多样性，并在 class-conditional ImageNet 生成中达到新的state-of-the-art FID 值（1.70和2.31）。<details>
<summary>Abstract</summary>
While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. Our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing Gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. Our Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. Further, using an existing pretrained diffusion model, CADS achieves a new state-of-the-art FID of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256 and 512$\times$512 respectively.
</details>
<details>
<summary>摘要</summary>
“ whilst conditional diffusion models have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. we attribute this problem to the role of the conditioning signal in inference and offer an improved sampling strategy for diffusion models that can increase generation diversity, especially at high guidance scales, with minimal loss of sample quality. our sampling strategy anneals the conditioning signal by adding scheduled, monotonically decreasing gaussian noise to the conditioning vector during inference to balance diversity and condition alignment. our condition-annealed diffusion sampler (cads) can be used with any pretrained model and sampling algorithm, and we show that it boosts the diversity of diffusion models in various conditional generation tasks. further, using an existing pretrained diffusion model, cads achieves a new state-of-the-art fid of 1.70 and 2.31 for class-conditional imagenet generation at 256x256 and 512x512 respectively.”Note: "FID" stands for "Frechet Inception Distance", which is a measure of the quality of generated images. A lower FID score indicates better image quality.
</details></li>
</ul>
<hr>
<h2 id="IndustReal-A-Dataset-for-Procedure-Step-Recognition-Handling-Execution-Errors-in-Egocentric-Videos-in-an-Industrial-Like-Setting"><a href="#IndustReal-A-Dataset-for-Procedure-Step-Recognition-Handling-Execution-Errors-in-Egocentric-Videos-in-an-Industrial-Like-Setting" class="headerlink" title="IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting"></a>IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17323">http://arxiv.org/abs/2310.17323</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/timschoonbeek/industreal">https://github.com/timschoonbeek/industreal</a></li>
<li>paper_authors: Tim J. Schoonbeek, Tim Houben, Hans Onvlee, Peter H. N. de With, Fons van der Sommen</li>
<li>for: 这篇论文主要关注于recognizing the correct completion and order of procedural steps，以解决action recognition for procedural tasks中的一个限制，即无法衡量动作的成功度。</li>
<li>methods: 论文提出了一种新的任务—procedure step recognition（PSR），并提供了一个多模态的IndustReal数据集。</li>
<li>results: 论文在IndustReal数据集上进行了实验，并发现了一些新的错误类型，如执行错误和步骤错误。这些错误会限制action recognition的应用在工业领域。<details>
<summary>Abstract</summary>
Although action recognition for procedural tasks has received notable attention, it has a fundamental flaw in that no measure of success for actions is provided. This limits the applicability of such systems especially within the industrial domain, since the outcome of procedural actions is often significantly more important than the mere execution. To address this limitation, we define the novel task of procedure step recognition (PSR), focusing on recognizing the correct completion and order of procedural steps. Alongside the new task, we also present the multi-modal IndustReal dataset. Unlike currently available datasets, IndustReal contains procedural errors (such as omissions) as well as execution errors. A significant part of these errors are exclusively present in the validation and test sets, making IndustReal suitable to evaluate robustness of algorithms to new, unseen mistakes. Additionally, to encourage reproducibility and allow for scalable approaches trained on synthetic data, the 3D models of all parts are publicly available. Annotations and benchmark performance are provided for action recognition and assembly state detection, as well as the new PSR task. IndustReal, along with the code and model weights, is available at: https://github.com/TimSchoonbeek/IndustReal .
</details>
<details>
<summary>摘要</summary>
尽管动作识别 для进程任务已经受到了关注，但是它具有一个基本的缺陷，即没有提供行动的成功度量。这限制了这些系统在工业领域的应用，因为进程动作的结果比执行动作本身更重要。为解决这些限制，我们定义了新的任务：程序步骤识别（PSR），它是识别正确完成和顺序的程序步骤的任务。同时，我们也提供了多modal的IndustReal数据集。与现有数据集不同的是，IndustReal包含了进程错误（如漏洞）以及执行错误。大多数这些错误只存在在验证和测试集中，使IndustReal适用于评估算法对新、未看过的错误的Robustness。此外，为便于可重复性和可以使用合成数据进行扩展，所有部件的3D模型都公开可用。标注和比较性性能是提供的，以及新的PSR任务。IndustReal，以及代码和模型权重，可以在：https://github.com/TimSchoonbeek/IndustReal 中找到。
</details></li>
</ul>
<hr>
<h2 id="Defect-Spectrum-A-Granular-Look-of-Large-Scale-Defect-Datasets-with-Rich-Semantics"><a href="#Defect-Spectrum-A-Granular-Look-of-Large-Scale-Defect-Datasets-with-Rich-Semantics" class="headerlink" title="Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics"></a>Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17316">http://arxiv.org/abs/2310.17316</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Yang, Zhifei Chen, Pengguang Chen, Xi Fang, Shu Liu, Yingcong Chen</li>
<li>for: 本研究的目的是提供一个精准、semantic-abundant、大规模的 defect spectrum 数据集，用于实际应用中的缺陷检测。</li>
<li>methods: 本研究使用了 four key industrial benchmarks，对现有的标注进行了细化和增加 semantic details，以区分多种缺陷类型。 而且，我们提出了一种基于 diffusion-based generator的 two-stage 生成器，用于生成高质量和多样化的缺陷图像。</li>
<li>results: 对于 defect inspection 模型的效果，synthetic images generated by Defect-Gen 显示了明显的提高。总的来说，The Defect Spectrum dataset 在缺陷检测研究中显示了很好的潜力，提供了一个坚实的平台用于测试和优化高级模型。<details>
<summary>Abstract</summary>
Defect inspection is paramount within the closed-loop manufacturing system. However, existing datasets for defect inspection often lack precision and semantic granularity required for practical applications. In this paper, we introduce the Defect Spectrum, a comprehensive benchmark that offers precise, semantic-abundant, and large-scale annotations for a wide range of industrial defects. Building on four key industrial benchmarks, our dataset refines existing annotations and introduces rich semantic details, distinguishing multiple defect types within a single image. Furthermore, we introduce Defect-Gen, a two-stage diffusion-based generator designed to create high-quality and diverse defective images, even when working with limited datasets. The synthetic images generated by Defect-Gen significantly enhance the efficacy of defect inspection models. Overall, The Defect Spectrum dataset demonstrates its potential in defect inspection research, offering a solid platform for testing and refining advanced models.
</details>
<details>
<summary>摘要</summary>
“缺陷检查是关键在关闭式生产系统中。然而，现有的缺陷检查数据集经常缺乏实际应用中所需的精度和semantic细节。本文介绍了缺陷谱，一个全面的标准准 markers，提供了精度、semantic-abundant和大规模的注释，用于各种工业缺陷。基于四个键industrial benchmark，我们的数据集细化了现有的注释，并引入了丰富的semantic细节，在单个图像中分辨多种缺陷类型。此外，我们引入了 Defect-Gen，一个两stage diffusion-based generator，用于生成高质量和多样化的缺陷图像，即使working with limited datasets。生成的synthetic图像由 Defect-Gen明显提高了缺陷检查模型的效果。总之，缺陷谱数据集在缺陷检查研究中展示了很好的潜力，提供了一个坚实的平台用于测试和优化高级模型。”
</details></li>
</ul>
<hr>
<h2 id="Scale-Adaptive-Feature-Aggregation-for-Efficient-Space-Time-Video-Super-Resolution"><a href="#Scale-Adaptive-Feature-Aggregation-for-Efficient-Space-Time-Video-Super-Resolution" class="headerlink" title="Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution"></a>Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17294">http://arxiv.org/abs/2310.17294</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/megvii-research/wacv2024-safa">https://github.com/megvii-research/wacv2024-safa</a></li>
<li>paper_authors: Zhewei Huang, Ailin Huang, Xiaotao Hu, Chen Hu, Jun Xu, Shuchang Zhou</li>
<li>for: 提高视频质量</li>
<li>methods: 提出了一种新的Scale-Adaptive Feature Aggregation（SAFA）网络，该网络可以适应不同的运动幅度，以提高流体基于特征的传播。</li>
<li>results: 对四个公共的STVSR标准测试集进行了实验，SAFA网络可以达到领先的性能水平，比如TMNet和VideoINR方法的平均提高超过0.5dB的PSNR表现，而需要 menos than half的参数和only 1&#x2F;3的计算成本。<details>
<summary>Abstract</summary>
The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual quality of videos, by simultaneously performing video frame interpolation (VFI) and video super-resolution (VSR). However, facing the challenge of the additional temporal dimension and scale inconsistency, most existing STVSR methods are complex and inflexible in dynamically modeling different motion amplitudes. In this work, we find that choosing an appropriate processing scale achieves remarkable benefits in flow-based feature propagation. We propose a novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects sub-networks with different processing scales for individual samples. Experiments on four public STVSR benchmarks demonstrate that SAFA achieves state-of-the-art performance. Our SAFA network outperforms recent state-of-the-art methods such as TMNet and VideoINR by an average improvement of over 0.5dB on PSNR, while requiring less than half the number of parameters and only 1/3 computational costs.
</details>
<details>
<summary>摘要</summary>
space-time video super-resolution (STVSR) 任务是提高视频质量，同时进行视频帧 interpolate (VFI) 和视频超解像 (VSR)。然而，面临 temporal 维度和比例不一致的挑战，大多数现有的 STVSR 方法复杂且不灵活地处理不同的运动振荡。在这种情况下，我们发现选择合适的处理缩放级别可以实现remarkable benefits的流基feature propagation。我们提出了一种新的Scale-Adaptive Feature Aggregation (SAFA) 网络，该网络可以适应各个样本的不同处理缩放级别。实验表明，我们的 SAFA 网络在四个公共 STVSR 测试集上达到了最佳性能。与最近的 state-of-the-art 方法如 TMNet 和 VideoINR 相比，我们的 SAFA 网络平均提高了PSNR 值超过 0.5dB，同时需要 fewer than half 的参数和只有 1/3 的计算成本。
</details></li>
</ul>
<hr>
<h2 id="RIO-A-Benchmark-for-Reasoning-Intention-Oriented-Objects-in-Open-Environments"><a href="#RIO-A-Benchmark-for-Reasoning-Intention-Oriented-Objects-in-Open-Environments" class="headerlink" title="RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments"></a>RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17290">http://arxiv.org/abs/2310.17290</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, Yunchao Wei</li>
<li>for: 这个论文旨在探讨如何基于具体的目的或需求来检测对象。</li>
<li>methods: 这个论文使用了一个新的数据集called Reasoning Intention-Oriented Objects (RIO)，以便更好地处理开放环境中的意图。</li>
<li>results: 研究人员通过使用RIO数据集，发现了一些现有模型在开放环境中理解意图对象的能力有所提高。<details>
<summary>Abstract</summary>
Intention-oriented object detection aims to detect desired objects based on specific intentions or requirements. For instance, when we desire to "lie down and rest", we instinctively seek out a suitable option such as a "bed" or a "sofa" that can fulfill our needs. Previous work in this area is limited either by the number of intention descriptions or by the affordance vocabulary available for intention objects. These limitations make it challenging to handle intentions in open environments effectively. To facilitate this research, we construct a comprehensive dataset called Reasoning Intention-Oriented Objects (RIO). In particular, RIO is specifically designed to incorporate diverse real-world scenarios and a wide range of object categories. It offers the following key features: 1) intention descriptions in RIO are represented as natural sentences rather than a mere word or verb phrase, making them more practical and meaningful; 2) the intention descriptions are contextually relevant to the scene, enabling a broader range of potential functionalities associated with the objects; 3) the dataset comprises a total of 40,214 images and 130,585 intention-object pairs. With the proposed RIO, we evaluate the ability of some existing models to reason intention-oriented objects in open environments.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese:Intention-oriented object detection aims to detect desired objects based on specific intentions or requirements. For instance, when we desire to "lie down and rest", we instinctively seek out a suitable option such as a "bed" or a "sofa" that can fulfill our needs. Previous work in this area is limited either by the number of intention descriptions or by the affordance vocabulary available for intention objects. These limitations make it challenging to handle intentions in open environments effectively. To facilitate this research, we construct a comprehensive dataset called Reasoning Intention-Oriented Objects (RIO). In particular, RIO is specifically designed to incorporate diverse real-world scenarios and a wide range of object categories. It offers the following key features:1. 在RIO中，INTENTIONDESCRIPTION是用自然的句子来表达，而不是单个词或动词短语，使其更加实用和有意义。2. RIO中的INTENTIONDESCRIPTION是场景相关的，使得对象的功能更加广泛。3. RIO dataset包含40,214张图片和130,585个INTENTION-OBJECT对。Using the proposed RIO, we evaluate the ability of some existing models to reason intention-oriented objects in open environments.Translate the text into Simplified Chinese, please.</SYS>>Here's the translation:意向导航对象检测的目的是根据特定的意向或需求检测所需的对象。例如，当我们想要“躺下休息”时，我们会自然地寻找一个适合的选择，如床或沙发，以满足我们的需求。现有的研究在这一领域受限于意向描述的数量或意向对象的可用词汇。这些限制使得在开放环境中处理意向变得困难。为了进行这些研究，我们构建了一个完整的数据集，即理解意向对象（RIO）。特别是，RIO是专门为开放环境和多种对象类型设计的。它具有以下三个关键特点：1. RIO中的意向描述使用自然的句子表达，而不是单个词或动词短语，使其更加实用和有意义。2. RIO中的意向描述与场景相关，使得对象的功能更加广泛。3. RIO dataset包含40,214张图片和130,585个意向对象对。使用我们提议的RIO，我们评估了一些现有模型在开放环境中理解意向对象的能力。
</details></li>
</ul>
<hr>
<h2 id="BEVContrast-Self-Supervision-in-BEV-Space-for-Automotive-Lidar-Point-Clouds"><a href="#BEVContrast-Self-Supervision-in-BEV-Space-for-Automotive-Lidar-Point-Clouds" class="headerlink" title="BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds"></a>BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17281">http://arxiv.org/abs/2310.17281</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/valeoai/bevcontrast">https://github.com/valeoai/bevcontrast</a></li>
<li>paper_authors: Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, Vincent Lepetit</li>
<li>for: 提高自动驾驶汽车 LiDAR 点云自我监督的简单性和效率。</li>
<li>methods: 设计了一种基于 Bird’s Eye View 平面的对比损失函数，从而实现了简单且高效的自我监督。</li>
<li>results: 比起 PointConstrast 和 TARL 等方法，BEVContrast 可以提供更好的性能和简洁性，且计算cell级别表示只需要 pays a small computational cost.<details>
<summary>Abstract</summary>
We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of implementation, it is surpassed by the latter, which however requires a costly pre-processing. In BEVContrast, we define our contrast at the level of 2D cells in the Bird's Eye View plane. Resulting cell-level representations offer a good trade-off between the point-level representations exploited in PointContrast and segment-level representations exploited in TARL: we retain the simplicity of PointContrast (cell representations are cheap to compute) while surpassing the performance of TARL in downstream semantic segmentation.
</details>
<details>
<summary>摘要</summary>
我们提出了一种奇异简单高效的自监督3D脊梁在汽车激光雷达点云上的方法。我们定义了在同一场景中捕捉的雷达扫描特征之间的对比损失。文献中已有许多类似的方法，如PointContrast，它使用点级对比，到现状之最佳TARL，它使用段级对比，约相对应于物体。而PointContrast具有简单的实现，但是被TARL所超越，后者 however需要昂贵的预处理。在BEVContrast中，我们定义了2D bird's eye view平面上的细胞级对比，得到的细胞级表示具有点级表示使用PointContrast和段级表示使用TARL之间的好COMPROMISE：我们保留了PointContrast的简单实现，同时超越TARL在下游semantic segmentation中的性能。
</details></li>
</ul>
<hr>
<h2 id="Generalizing-to-Unseen-Domains-in-Diabetic-Retinopathy-Classification"><a href="#Generalizing-to-Unseen-Domains-in-Diabetic-Retinopathy-Classification" class="headerlink" title="Generalizing to Unseen Domains in Diabetic Retinopathy Classification"></a>Generalizing to Unseen Domains in Diabetic Retinopathy Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17255">http://arxiv.org/abs/2310.17255</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chumsy0725/spsd-vit">https://github.com/chumsy0725/spsd-vit</a></li>
<li>paper_authors: Chamuditha Jayanga Galappaththige, Gayal Kuruppu, Muhammad Haris Khan<br>for:这个研究旨在解决遗传性糖尿病视力损害（DR）的早期诊断和治疗过程中的检测问题，以帮助早期疗法和恢复病情。methods:我们提出了一个简单且有效的领域扩展（DG）方法，通过一个新的预测软化机制，实现了自我激发（self-distillation）在感知 трансформа器（ViT）中。results:我们在多个挑战性的开源DR检测数据集上进行了广泛的实验，包括多源和单源DG设定，并使用三种不同的ViT背bone进行比较。我们的方法在这些设定下实现了比较好的性能，并且在验证测试中获得了改善的准确性和调整性。<details>
<summary>Abstract</summary>
Diabetic retinopathy (DR) is caused by long-standing diabetes and is among the fifth leading cause for visual impairments. The process of early diagnosis and treatments could be helpful in curing the disease, however, the detection procedure is rather challenging and mostly tedious. Therefore, automated diabetic retinopathy classification using deep learning techniques has gained interest in the medical imaging community. Akin to several other real-world applications of deep learning, the typical assumption of i.i.d data is also violated in DR classification that relies on deep learning. Therefore, developing DR classification methods robust to unseen distributions is of great value. In this paper, we study the problem of generalizing a model to unseen distributions or domains (a.k.a domain generalization) in DR classification. To this end, we propose a simple and effective domain generalization (DG) approach that achieves self-distillation in vision transformers (ViT) via a novel prediction softening mechanism. This prediction softening is an adaptive convex combination one-hot labels with the model's own knowledge. We perform extensive experiments on challenging open-source DR classification datasets under both multi-source and single-source DG settings with three different ViT backbones to establish the efficacy and applicability of our approach against competing methods. For the first time, we report the performance of several state-of-the-art DG methods on open-source DR classification datasets after conducting thorough experiments. Finally, our method is also capable of delivering improved calibration performance than other methods, showing its suitability for safety-critical applications, including healthcare. We hope that our contributions would investigate more DG research across the medical imaging community.
</details>
<details>
<summary>摘要</summary>
糖尿病retinopathy (DR) 是由长期糖尿病引起的，是视力障碍的第五大原因。 early diagnosis 和治疗可以帮助缓解病情，但检测过程很复杂且时consuming。因此，使用深度学习技术进行糖尿病分类已经在医学影像社区中吸引了广泛的关注。与其他多种应用场景一样，糖尿病分类中的i.i.d数据假设也被违反。因此，开发一种可以抗见 distributions 的DR分类方法非常有价值。在这篇论文中，我们研究了在DR分类中对不同分布或领域（a.k.a. 领域普适化）的扩展。为达到这个目标，我们提出了一种简单而有效的领域普适化（DG）方法，通过一种新的预测软化机制来实现自我热化。这种预测软化是一种可靠的一个逻辑折衔，将一个一个的一个零准确率与模型自己的知识相乘。我们在多个开源DR分类数据集上进行了广泛的实验，包括多源和单源DG设置，并使用三种不同的 ViT 框架来证明我们的方法的有效性和可应用性。我们是第一次在开源DR分类数据集上进行了多种state-of-the-art DG方法的实验，并发现我们的方法在calibration性能方面表现出色，表明它适合安全关键应用，如医疗。我们希望通过我们的贡献，可以鼓励更多的领域普适化研究在医学影像社区中进行。
</details></li>
</ul>
<hr>
<h2 id="Prototypical-Contrastive-Learning-based-CLIP-Fine-tuning-for-Object-Re-identification"><a href="#Prototypical-Contrastive-Learning-based-CLIP-Fine-tuning-for-Object-Re-identification" class="headerlink" title="Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification"></a>Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17218">http://arxiv.org/abs/2310.17218</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiachen Li, Xiaojin Gong</li>
<li>for: 提高对象重复识别（Re-ID）性能 across various supervision settings。</li>
<li>methods: 利用大规模预训练的视觉语言模型（CLIP）进行适应，并直接精度地调整CLIP的图像Encoder使用抽象对比学习（PCL）损失，取消需要提示学习。</li>
<li>results: 在人车Re-ID数据集上实现了与CLIP-ReID相当的竞争性表现，并在无监督情况下进一步实现了状态级表现。<details>
<summary>Abstract</summary>
This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个工作的目标是适应大规模预训练的视觉语言模型，如对比语言图像预训练（CLIP），以提高对象重新识别（Re-ID）的性能。虽然推荐学习已经使得CLIP-ReID实现了良好的表现，但是下面的机制和推荐学习的必要性仍然不清楚，这是因为ReID任务缺乏semantic标签。在这个工作中，我们首先分析CLIP-ReID中的推荐学习角色和其局限性。基于我们的调查，我们提议一种简单 yet effective的方法，通过直接练习CLIP的图像编码器使用prototype对比学习（PCL）损失来适应CLIPsupervised object Re-ID。我们的方法不需要推荐学习。实验结果表明，我们的方法在人体和车辆Re-ID dataset上与CLIP-ReID相比具有竞争力。此外，我们还扩展了我们的PCL-based CLIP fine-tuning方法到无监督场景，在这些场景下，我们实现了状态的最佳性能。
</details></li>
</ul>
<hr>
<h2 id="Three-dimensional-Bone-Image-Synthesis-with-Generative-Adversarial-Networks"><a href="#Three-dimensional-Bone-Image-Synthesis-with-Generative-Adversarial-Networks" class="headerlink" title="Three-dimensional Bone Image Synthesis with Generative Adversarial Networks"></a>Three-dimensional Bone Image Synthesis with Generative Adversarial Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17216">http://arxiv.org/abs/2310.17216</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christoph Angermann, Johannes Bereiter-Payr, Kerstin Stock, Markus Haltmeier, Gerald Degenhart</li>
<li>for: 这篇论文是为了探讨三维生成对医疗影像处理领域的应用而写的。</li>
<li>methods: 这篇论文使用的方法是基于三维生成对抗网络（GAN），可以高效地生成高分辨率医疗影像 Volume 的细节。</li>
<li>results: 这篇论文的结果表明，GAN可以成功地在三维设定下进行生成，并且可以进行大规模的数据驱动模型的开发。此外，GAN的反向减法也可以在这种设定下实现，并用于图像混合、特征编辑和风格混合等应用。结果得到了三维 HR-pQCT 数据库的广泛验证。<details>
<summary>Abstract</summary>
Medical image processing has been highlighted as an area where deep learning-based models have the greatest potential. However, in the medical field in particular, problems of data availability and privacy are hampering research progress and thus rapid implementation in clinical routine. The generation of synthetic data not only ensures privacy, but also allows to \textit{draw} new patients with specific characteristics, enabling the development of data-driven models on a much larger scale. This work demonstrates that three-dimensional generative adversarial networks (GANs) can be efficiently trained to generate high-resolution medical volumes with finely detailed voxel-based architectures. In addition, GAN inversion is successfully implemented for the three-dimensional setting and used for extensive research on model interpretability and applications such as image morphing, attribute editing and style mixing. The results are comprehensively validated on a database of three-dimensional HR-pQCT instances representing the bone micro-architecture of the distal radius.
</details>
<details>
<summary>摘要</summary>
医学图像处理领域内，深度学习基本模型的潜力得到了特别强调。然而，医疗领域中特别是数据可用性和隐私问题，对研究进步和临床应用的阻碍。生成 sintetic 数据不仅保障隐私，还可以为新的患者群体创造特定特征，以便开发基于大规模数据的模型。这项工作表明，三维生成对抗网络（GAN）可以高效地训练高分辨率医学三维体volume，并在三维设定下成功实现GAN反向。这些结果在三维 HR-pQCT 数据库上进行了广泛的验证，表明这些模型在bone micro-architecture中具有高度的可解释性和应用前景。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Surgical-Phase-Recognition"><a href="#Weakly-Supervised-Surgical-Phase-Recognition" class="headerlink" title="Weakly-Supervised Surgical Phase Recognition"></a>Weakly-Supervised Surgical Phase Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17209">http://arxiv.org/abs/2310.17209</a></li>
<li>repo_url: None</li>
<li>paper_authors: Roy Hirsch, Regev Cohen, Mathilde Caron, Tomer Golany, Daniel Freedman, Ehud Rivlin</li>
<li>for: 针对计算机助手手术系统中的阶段识别问题</li>
<li>methods: 组合图像分割和自动学习，提出一种随机游走解决方案，并利用弱约束和少量学习</li>
<li>results: 在公共Cholec80 dataset上进行实验，在多种设置下达到了可塑性和低资源消耗的表现<details>
<summary>Abstract</summary>
A key element of computer-assisted surgery systems is phase recognition of surgical videos. Existing phase recognition algorithms require frame-wise annotation of a large number of videos, which is time and money consuming. In this work we join concepts of graph segmentation with self-supervised learning to derive a random-walk solution for per-frame phase prediction. Furthermore, we utilize within our method two forms of weak supervision: sparse timestamps or few-shot learning. The proposed algorithm enjoys low complexity and can operate in lowdata regimes. We validate our method by running experiments with the public Cholec80 dataset of laparoscopic cholecystectomy videos, demonstrating promising performance in multiple setups.
</details>
<details>
<summary>摘要</summary>
computer-assisted surgery systems中一个关键元素是运行过程识别。现有的运行识别算法需要大量的几几个影像档案进行框架层级的标注，这是时间和金额的浪费。在这个工作中，我们结合了Graph分类和自动学习的概念，以derive一个随机步进行每帧运行预测。此外，我们在方法中使用了两种弱型指导：稀脱时间标签或几何学学习。提议的算法具有低复杂度，可以在低数据 режи中运行。我们运行了 experiments with公共Cholec80dataset of laparoscopic cholecystectomy videos， demonstarted promising performance in multiple setups。
</details></li>
</ul>
<hr>
<h2 id="Lookup-Table-meets-Local-Laplacian-Filter-Pyramid-Reconstruction-Network-for-Tone-Mapping"><a href="#Lookup-Table-meets-Local-Laplacian-Filter-Pyramid-Reconstruction-Network-for-Tone-Mapping" class="headerlink" title="Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping"></a>Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction Network for Tone Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17190">http://arxiv.org/abs/2310.17190</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fengzhang427/LLF-LUT">https://github.com/fengzhang427/LLF-LUT</a></li>
<li>paper_authors: Feng Zhang, Ming Tian, Zhiqiang Li, Bin Xu, Qingbo Lu, Changxin Gao, Nong Sang</li>
<li>for: 本研究旨在 Addressing the limitations of traditional 3-Dimensional LookUp Table (3D LUT) based tone mapping methods, which often fail to preserve local details in images.</li>
<li>methods: 该研究提出了一种新的策略，即通过closed-form Laplacian pyramid decomposition and reconstruction，并采用image-adaptive 3D LUTs和Progressive learning of local Laplacian filters来实现同时global和local操作。</li>
<li>results: 对两个标准测试集进行了广泛的实验，并证明了该方法可以同时保持全球含义和地方细节，并且与现有方法相比有所提高。<details>
<summary>Abstract</summary>
Tone mapping aims to convert high dynamic range (HDR) images to low dynamic range (LDR) representations, a critical task in the camera imaging pipeline. In recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained attention due to their ability to strike a favorable balance between enhancement performance and computational efficiency. However, these methods often fail to deliver satisfactory results in local areas since the look-up table is a global operator for tone mapping, which works based on pixel values and fails to incorporate crucial local information. To this end, this paper aims to address this issue by exploring a novel strategy that integrates global and local operators by utilizing closed-form Laplacian pyramid decomposition and reconstruction. Specifically, we employ image-adaptive 3D LUTs to manipulate the tone in the low-frequency image by leveraging the specific characteristics of the frequency information. Furthermore, we utilize local Laplacian filters to refine the edge details in the high-frequency components in an adaptive manner. Local Laplacian filters are widely used to preserve edge details in photographs, but their conventional usage involves manual tuning and fixed implementation within camera imaging pipelines or photo editing tools. We propose to learn parameter value maps progressively for local Laplacian filters from annotated data using a lightweight network. Our model achieves simultaneous global tone manipulation and local edge detail preservation in an end-to-end manner. Extensive experimental results on two benchmark datasets demonstrate that the proposed method performs favorably against state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
《Tone Mapping using 3D LUT and Local Laplacian Filters》目的：使高动态范围（HDR）图像转换为低动态范围（LDR）表示，是摄像头成像管线中的关键任务。在过去几年，基于3维LookUp Table（3D LUT）的方法吸引了广泛关注，因为它们能够平衡提升性和计算效率。然而，这些方法经常在地方区域出现不满心的结果，因为Look-up Table是一个全局操作符，基于像素值进行匹配，而不会考虑重要的地方信息。为此，本文提出了一种新的策略，即通过closed-form Laplacian pyramid decomposition和重建来结合全局和地方操作符。特别是，我们使用适应性的3D LUT来控制在低频谱中的音调，并且使用地方 Laplacian 滤波器来在高频谱中进行适应式的细节缩放。地方 Laplacian 滤波器广泛用于保持照片中的缝隙细节，但是它们的传统使用具有手动调整和固定实现在摄像头成像管线或图像修饰工具中。我们提议通过轻量级网络来逐渐学习参数值图表进行地方 Laplacian 滤波器的进行进行适应性调整。我们的模型可以同时进行全局音调调整和地方细节缩放，并且在端到端方式进行实现。对两个标准数据集进行了广泛的实验，结果表明，我们的方法在比较当前的方法中表现出色。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Iterative-Refinement-with-Diffusion-Models-for-Video-Grounding"><a href="#Exploring-Iterative-Refinement-with-Diffusion-Models-for-Video-Grounding" class="headerlink" title="Exploring Iterative Refinement with Diffusion Models for Video Grounding"></a>Exploring Iterative Refinement with Diffusion Models for Video Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17189">http://arxiv.org/abs/2310.17189</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mastervito/diffusionvg">https://github.com/mastervito/diffusionvg</a></li>
<li>paper_authors: Xiao Liang, Tao Shi, Yaoyuan Liang, Te Tao, Shao-Lun Huang</li>
<li>for: 本研究旨在提高视频落实（video grounding）的精度和效率，以便更好地将文本描述与视频内容相匹配。</li>
<li>methods: 本研究提出了一种基于扩散模型的新方法，称为DiffusionVG，它将视频落实视为一个条件生成任务，通过逐渐添加噪声并在反扩散过程中进行恢复，以便从噪声输入中生成目标 span。</li>
<li>results: 在主流的Charades-STA和ActivityNet Captions测试集上，DiffusionVG表现竞争力或者连续性更高，而无需使用复杂的特性或噪声。<details>
<summary>Abstract</summary>
Video grounding aims to localize the target moment in an untrimmed video corresponding to a given sentence query. Existing methods typically select the best prediction from a set of predefined proposals or directly regress the target span in a single-shot manner, resulting in the absence of a systematical prediction refinement process. In this paper, we propose DiffusionVG, a novel framework with diffusion models that formulates video grounding as a conditional generation task, where the target span is generated from Gaussian noise inputs and interatively refined in the reverse diffusion process. During training, DiffusionVG progressively adds noise to the target span with a fixed forward diffusion process and learns to recover the target span in the reverse diffusion process. In inference, DiffusionVG can generate the target span from Gaussian noise inputs by the learned reverse diffusion process conditioned on the video-sentence representations. Our DiffusionVG follows the encoder-decoder architecture, which firstly encodes the video-sentence features and iteratively denoises the predicted spans in its specialized span refining decoder. Without bells and whistles, our DiffusionVG demonstrates competitive or even superior performance compared to existing well-crafted models on mainstream Charades-STA and ActivityNet Captions benchmarks.
</details>
<details>
<summary>摘要</summary>
视频固定目标是将目标刻影在未处理视频中与给定句子查询匹配。现有方法通常从预先定义的提案中选择最佳预测或直接在单击shot模式下进行 span 直接回归，从而缺乏系统化预测纠正过程。在这篇论文中，我们提出了DiffusionVG，一种新的框架，其中视频固定目标被формализова为一个条件生成任务，其中目标刻影从托管的 Gaussian 噪声输入生成并经过反演 diffusion 过程进行逐步纠正。在训练时，DiffusionVG 逐渐添加到目标刻影的噪声输入，并通过学习反演 diffusion 过程来回归目标刻影。在推理时，DiffusionVG 可以从 Gaussian 噪声输入生成目标刻影，并且通过特殊的 span 纠正逻辑来进行 Conditioned 生成。我们的DiffusionVG 采用了 Encoder-Decoder 架构，它首先将视频-句子特征编码，然后在特殊的 span 纠正解码器中iteratively 进行噪声恢复。没有一切饰物的，我们的DiffusionVG 在主流的 Charades-STA 和 ActivityNet Captions benchmark 上达到了与现有高水平的竞争性或者even 超越性表现。
</details></li>
</ul>
<hr>
<h2 id="Blind-Image-Super-resolution-with-Rich-Texture-Aware-Codebooks"><a href="#Blind-Image-Super-resolution-with-Rich-Texture-Aware-Codebooks" class="headerlink" title="Blind Image Super-resolution with Rich Texture-Aware Codebooks"></a>Blind Image Super-resolution with Rich Texture-Aware Codebooks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17188">http://arxiv.org/abs/2310.17188</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rui Qin, Ming Sun, Fangyuan Zhang, Xing Wen, Bin Wang</li>
<li>for: 提高盲目超解像（BSR）方法的效果，使其能够更好地处理复杂的盲目压缩和杂质损害。</li>
<li>methods: 提出了一种基于高分辨率（HR）重建码库的 Rich Texture-aware Codebook-based Network（RTCNet），包括了适应性损害抑制模块（DTPM）和 patch-aware texture prior module（PTPM）。</li>
<li>results: RTCNet在多个 benchmark 上比州先进方法提高了0.16 ~ 0.46dB。<details>
<summary>Abstract</summary>
Blind super-resolution (BSR) methods based on high-resolution (HR) reconstruction codebooks have achieved promising results in recent years. However, we find that a codebook based on HR reconstruction may not effectively capture the complex correlations between low-resolution (LR) and HR images. In detail, multiple HR images may produce similar LR versions due to complex blind degradations, causing the HR-dependent only codebooks having limited texture diversity when faced with confusing LR inputs. To alleviate this problem, we propose the Rich Texture-aware Codebook-based Network (RTCNet), which consists of the Degradation-robust Texture Prior Module (DTPM) and the Patch-aware Texture Prior Module (PTPM). DTPM effectively mines the cross-resolution correlation of textures between LR and HR images by exploiting the cross-resolution correspondence of textures. PTPM uses patch-wise semantic pre-training to correct the misperception of texture similarity in the high-level semantic regularization. By taking advantage of this, RTCNet effectively gets rid of the misalignment of confusing textures between HR and LR in the BSR scenarios. Experiments show that RTCNet outperforms state-of-the-art methods on various benchmarks by up to 0.16 ~ 0.46dB.
</details>
<details>
<summary>摘要</summary>
干扰盲超分辨率（BSR）方法，基于高分辨率（HR）重建码库，在过去几年内取得了有望的成果。然而，我们发现，基于HR重建的码库可能不能有效地捕捉LR和HR图像之间的复杂相关性。具体来说，多个HR图像可能会生成相同的LR版本，因为复杂的盲抑分辨率，导致HR依赖的只码库具有有限的文本多样性，面临恶势riorityLR输入。为了解决这个问题，我们提议了Rich Texture-aware Codebook-based Network（RTCNet），它包括Degradation-robust Texture Prior Module（DTPM）和Patch-aware Texture Prior Module（PTPM）。DTPM通过利用LR和HR图像之间的Texture的交叉相关性，有效地挖掘LR和HR图像之间的Texture相关性。PTPM使用patch-wise semantic pre-training来正确地修正高级 semantics regularization中的Texture相似性误差。通过这种方式，RTCNet可以有效地消除BSR场景中HR和LR图像之间的混淆文本。实验表明，RTCNet在不同的标准 bencmarks上出perform state-of-the-art方法，提高了0.16~0.46dB。
</details></li>
</ul>
<hr>
<h2 id="MO-YOLO-End-to-End-Multiple-Object-Tracking-Method-with-YOLO-and-MOTR"><a href="#MO-YOLO-End-to-End-Multiple-Object-Tracking-Method-with-YOLO-and-MOTR" class="headerlink" title="MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR"></a>MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17170">http://arxiv.org/abs/2310.17170</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liaopan-lp/MO-YOLO">https://github.com/liaopan-lp/MO-YOLO</a></li>
<li>paper_authors: Liao Pan, Yang Feng, Wu Di, Liu Bo, Zhang Xingle</li>
<li>for: 提高多对象跟踪（MOT）领域中的灵活性和计算效率，提出一种高效、轻量级、计算资源减少的端到端多对象跟踪模型，名为MO-YOLO。</li>
<li>methods: 结合YOLO和RT-DETR模型，构建一个高效、轻量级、计算资源减少的端到端多对象跟踪网络，以提高MOT领域的计算效率和灵活性。</li>
<li>results: 在MOT17 dataset上，MO-YOLO只需要1个GeForce 2080 Ti GPU和12个小时的训练时间，就能够 achieve comparable performance，而MOTR\cite{zeng2022motr}则需要8个GeForce 2080 Ti GPU和4天的训练时间。<details>
<summary>Abstract</summary>
This paper aims to address critical issues in the field of Multi-Object Tracking (MOT) by proposing an efficient and computationally resource-efficient end-to-end multi-object tracking model, named MO-YOLO. Traditional MOT methods typically involve two separate steps: object detection and object tracking, leading to computational complexity and error propagation issues. Recent research has demonstrated outstanding performance in end-to-end MOT models based on Transformer architectures, but they require substantial hardware support. MO-YOLO combines the strengths of YOLO and RT-DETR models to construct a high-efficiency, lightweight, and resource-efficient end-to-end multi-object tracking network, offering new opportunities in the multi-object tracking domain. On the MOT17 dataset, MOTR\cite{zeng2022motr} requires training with 8 GeForce 2080 Ti GPUs for 4 days to achieve satisfactory results, while MO-YOLO only requires 1 GeForce 2080 Ti GPU and 12 hours of training to achieve comparable performance.
</details>
<details>
<summary>摘要</summary>
这篇论文目标是解决多对物跟踪（MOT）领域的关键问题，提出一种高效、计算资源充足的端到端多对物跟踪模型，名为MO-YOLO。传统的MOT方法通常包括两个分开的步骤：物体检测和物体跟踪，导致计算复杂性和错误传递问题。现代研究表明，基于Transformer架构的端到端MOT模型可以达到出色的性能，但它们需要重要的硬件支持。MO-YOLO将YOLO和RT-DETR模型的优点相结合，构建一个高效、轻量级、计算资源充足的端到端多对物跟踪网络，为多对物跟踪领域带来新的机遇。在MOT17数据集上，MOTR\cite{zeng2022motr}需要训练8个GeForce 2080 Ti GPU的4天时间来获得满意的结果，而MO-YOLO只需1个GeForce 2080 Ti GPU和12个小时的训练时间来达到相当的性能。
</details></li>
</ul>
<hr>
<h2 id="Bridging-Phylogeny-and-Taxonomy-with-Protein-protein-Interaction-Networks"><a href="#Bridging-Phylogeny-and-Taxonomy-with-Protein-protein-Interaction-Networks" class="headerlink" title="Bridging Phylogeny and Taxonomy with Protein-protein Interaction Networks"></a>Bridging Phylogeny and Taxonomy with Protein-protein Interaction Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17164">http://arxiv.org/abs/2310.17164</a></li>
<li>repo_url: None</li>
<li>paper_authors: Long-Huei Chen, Mohana Prasad Sathya Moorthy, Pratyaksh Sharma</li>
<li>for: 这项研究旨在更深入地理解生物体内的蛋白质-蛋白质互作（PPI）网络，以了解生物体之间的种系发生关系。</li>
<li>methods: 研究人员使用了已知种类的蛋白质网络统计特征来预测新发现的蛋白质网络统计特征，以及使用这些统计特征来分类生物体。</li>
<li>results: 研究人员成功创建了一个预测蛋白质网络统计特征的模型，以及一个使用蛋白质网络统计特征来分类生物体的模型。这两个模型成功地将蛋白质网络和种系发生关系两个领域联系起来。<details>
<summary>Abstract</summary>
The protein-protein interaction (PPI) network provides an overview of the complex biological reactions vital to an organism's metabolism and survival. Even though in the past PPI network were compared across organisms in detail, there has not been large-scale research on how individual PPI networks reflect on the species relationships. In this study we aim to increase our understanding of the tree of life and taxonomy by gleaming information from the PPI networks. We successful created (1) a predictor of network statistics based on known traits of existing species in the phylogeny, and (2) a taxonomic classifier of organism using the known protein network statistics, whether experimentally determined or predicted de novo. With the knowledge of protein interactions at its core, our two models effectively connects two field with widely diverging methodologies - the phylogeny and taxonomy of species.
</details>
<details>
<summary>摘要</summary>
生物体内的蛋白质-蛋白质互作（PPI）网络提供了生物过程的概述，这些过程对生物体的存活和代谢是关键。尽管过去曾经对不同生物体的PPI网络进行了详细比较，但是没有大规模研究过个PPI网络如何反映种之间的关系。在这项研究中，我们希望通过蛋白质互作网络来增加我们对生命树的理解和分类学的知识。我们成功地开发了以下两种模型：1. 基于已知种的特征来预测网络统计 Parameters的模型，这些特征包括生物体的分类、体积、生长速度等。2. 使用已知蛋白质网络统计 Parameters来分类生物体，无论这些统计 Parameters是通过实验测定还是通过德拟测定来获得的。通过蛋白质互作网络的知识作为核心，我们的两种模型成功地结合了生物体分类学和蛋白质互作网络的两个领域，它们的方法论相对远离。
</details></li>
</ul>
<hr>
<h2 id="Low-Dimensional-Gradient-Helps-Out-of-Distribution-Detection"><a href="#Low-Dimensional-Gradient-Helps-Out-of-Distribution-Detection" class="headerlink" title="Low-Dimensional Gradient Helps Out-of-Distribution Detection"></a>Low-Dimensional Gradient Helps Out-of-Distribution Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17163">http://arxiv.org/abs/2310.17163</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yingwen Wu, Tao Li, Xinwen Cheng, Jie Yang, Xiaolin Huang</li>
<li>for: 这个研究旨在探讨深度神经网络（DNNs）中的外部资料探测（OOD）领域，以确保深度学习模型在实际应用中的可靠性。</li>
<li>methods: 这个研究使用了整个梯度信息来进行OOD探测，包括梯度方向和梯度norm。具体来说，研究者使用了一个特定的主成分空间来实现线性维度减少，从而获得了具有最小资料损失的低维度表示。</li>
<li>results: 研究结果显示，这个新的OOD探测方法可以与现有的检测方法相比，在各种检测任务中表现出色，例如在ImageNetbenchmark上，这个方法可以实现11.15%的 False Positive Rate reduction（FPR95）。<details>
<summary>Abstract</summary>
Detecting out-of-distribution (OOD) samples is essential for ensuring the reliability of deep neural networks (DNNs) in real-world scenarios. While previous research has predominantly investigated the disparity between in-distribution (ID) and OOD data through forward information analysis, the discrepancy in parameter gradients during the backward process of DNNs has received insufficient attention. Existing studies on gradient disparities mainly focus on the utilization of gradient norms, neglecting the wealth of information embedded in gradient directions. To bridge this gap, in this paper, we conduct a comprehensive investigation into leveraging the entirety of gradient information for OOD detection. The primary challenge arises from the high dimensionality of gradients due to the large number of network parameters. To solve this problem, we propose performing linear dimension reduction on the gradient using a designated subspace that comprises principal components. This innovative technique enables us to obtain a low-dimensional representation of the gradient with minimal information loss. Subsequently, by integrating the reduced gradient with various existing detection score functions, our approach demonstrates superior performance across a wide range of detection tasks. For instance, on the ImageNet benchmark, our method achieves an average reduction of 11.15% in the false positive rate at 95% recall (FPR95) compared to the current state-of-the-art approach. The code would be released.
</details>
<details>
<summary>摘要</summary>
检测出现在数据集之外的样本（out-of-distribution，OOD）是深度神经网络（DNN）在实际应用中的可靠性 Ensure essential. While previous research has mainly investigated the disparity between in-distribution (ID) and OOD data through forward information analysis, the discrepancy in parameter gradients during the backward process of DNNs has received insufficient attention. Existing studies on gradient disparities mainly focus on the utilization of gradient norms, neglecting the wealth of information embedded in gradient directions. To bridge this gap, in this paper, we conduct a comprehensive investigation into leveraging the entirety of gradient information for OOD detection. The primary challenge arises from the high dimensionality of gradients due to the large number of network parameters. To solve this problem, we propose performing linear dimension reduction on the gradient using a designated subspace that comprises principal components. This innovative technique enables us to obtain a low-dimensional representation of the gradient with minimal information loss. Subsequently, by integrating the reduced gradient with various existing detection score functions, our approach demonstrates superior performance across a wide range of detection tasks. For instance, on the ImageNet benchmark, our method achieves an average reduction of 11.15% in the false positive rate at 95% recall (FPR95) compared to the current state-of-the-art approach. The code will be released.Note that Simplified Chinese is a romanization of Chinese, and the actual Chinese characters may vary depending on the system and font used.
</details></li>
</ul>
<hr>
<h2 id="Learning-depth-from-monocular-video-sequences"><a href="#Learning-depth-from-monocular-video-sequences" class="headerlink" title="Learning depth from monocular video sequences"></a>Learning depth from monocular video sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17156">http://arxiv.org/abs/2310.17156</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenwei Luo</li>
<li>for: 这个论文旨在提出一种基于单影视频序列的单张图像深度估计模型，以便在训练过程中更好地使用更多的图像作为监督。</li>
<li>methods: 我们提出了一种新的训练损失函数，使得在训练过程中可以更好地包含更多的图像作为监督。我们还提出了一种简单 yet effective的模型来考虑帧到帧像素运动。</li>
<li>results: 当我们将这些方法结合使用时，我们在KITTI dataset上的自主监督下得到了单张图像深度估计的state-of-the-art结果。<details>
<summary>Abstract</summary>
Learning single image depth estimation model from monocular video sequence is a very challenging problem. In this paper, we propose a novel training loss which enables us to include more images for supervision during the training process. We propose a simple yet effective model to account the frame to frame pixel motion. We also design a novel network architecture for single image estimation. When combined, our method produces state of the art results for monocular depth estimation on the KITTI dataset in the self-supervised setting.
</details>
<details>
<summary>摘要</summary>
学习单个图像深度估计模型从单摄影视频序列是一个非常困难的问题。在这篇论文中，我们提出了一种新的训练损失函数，允许我们在训练过程中使用更多的图像进行监督。我们提出了一种简单又有效的方法来考虑帧到帧像素运动。我们还设计了一种新的网络架构来实现单个图像估计。当这些方法相结合使用时，我们的方法在KITTI dataset上的自主监督 Setting中产生了state-of-the-art的结果。
</details></li>
</ul>
<hr>
<h2 id="Deep-Imbalanced-Regression-via-Hierarchical-Classification-Adjustment"><a href="#Deep-Imbalanced-Regression-via-Hierarchical-Classification-Adjustment" class="headerlink" title="Deep Imbalanced Regression via Hierarchical Classification Adjustment"></a>Deep Imbalanced Regression via Hierarchical Classification Adjustment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17154">http://arxiv.org/abs/2310.17154</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haipeng Xiong, Angela Yao</li>
<li>for: 本文提出了一种解决不平衡回归任务中的问题，即使用层次分类器来改善回归性能。</li>
<li>methods: 该方法首先将回归目标空间分解为多个精细分类器，然后使用范围保持分类器来学习一个单一的类ifier。</li>
<li>results: 实验结果显示，该方法在三种多元的回归任务中（年龄估计、人群数量估计和深度估计）都达到了Superior result。<details>
<summary>Abstract</summary>
Regression tasks in computer vision, such as age estimation or counting, are often formulated into classification by quantizing the target space into classes. Yet real-world data is often imbalanced -- the majority of training samples lie in a head range of target values, while a minority of samples span a usually larger tail range. By selecting the class quantization, one can adjust imbalanced regression targets into balanced classification outputs, though there are trade-offs in balancing classification accuracy and quantization error. To improve regression performance over the entire range of data, we propose to construct hierarchical classifiers for solving imbalanced regression tasks. The fine-grained classifiers limit the quantization error while being modulated by the coarse predictions to ensure high accuracy. Standard hierarchical classification approaches, however, when applied to the regression problem, fail to ensure that predicted ranges remain consistent across the hierarchy. As such, we propose a range-preserving distillation process that can effectively learn a single classifier from the set of hierarchical classifiers. Our novel hierarchical classification adjustment (HCA) for imbalanced regression shows superior results on three diverse tasks: age estimation, crowd counting and depth estimation. We will release the source code upon acceptance.
</details>
<details>
<summary>摘要</summary>
计算机视觉领域中的回归任务，如年龄估计或计数，经常被转化为分类任务。然而，实际数据往往受到偏斜——大多数训练样本集中在一个头range的目标值上，而一小部分样本则覆盖一个通常更大的尾range。通过选择类划分，可以调整不均匀的回归目标，并且可以平衡分类精度和划分误差。为了提高回归性能，我们提议使用层次分类器解决不均匀的回归任务。细化分类器限制划分误差，同时被模拟粗略预测的控制，以确保高精度。然而，标准层次分类方法，应用于回归问题时，无法保证预测范围保持一致性。因此，我们提议一种保持范围的润释过程，可以有效地学习单个分类器从多个层次分类器中。我们称之为层次分类调整（HCA）。我们的HCA在三种多样化任务上显示出优秀的结果：年龄估计、人群计数和深度估计。我们即将发布源代码。
</details></li>
</ul>
<hr>
<h2 id="Simple-Baselines-for-Projection-based-Full-reference-and-No-reference-Point-Cloud-Quality-Assessment"><a href="#Simple-Baselines-for-Projection-based-Full-reference-and-No-reference-Point-Cloud-Quality-Assessment" class="headerlink" title="Simple Baselines for Projection-based Full-reference and No-reference Point Cloud Quality Assessment"></a>Simple Baselines for Projection-based Full-reference and No-reference Point Cloud Quality Assessment</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17147">http://arxiv.org/abs/2310.17147</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zicheng Zhang, Yingjie Zhou, Wei Sun, Xiongkuo Min, Guangtao Zhai</li>
<li>for: 本研究旨在提供高效的点云质量评估方法，以满足存储和带宽限制下的3D内容表示和应用需求。</li>
<li>methods: 该研究使用多个投影方法从点云数据中获取多个投影，并使用流行的视觉脊梁提取质量感知特征。基于FR和NR两种任务，分别计算出全referenced和无参照质量表示。</li>
<li>results: 在ICIP 2023 PCVQA挑战中，该研究取得了五个评测轨道中的四个首位。<details>
<summary>Abstract</summary>
Point clouds are widely used in 3D content representation and have various applications in multimedia. However, compression and simplification processes inevitably result in the loss of quality-aware information under storage and bandwidth constraints. Therefore, there is an increasing need for effective methods to quantify the degree of distortion in point clouds. In this paper, we propose simple baselines for projection-based point cloud quality assessment (PCQA) to tackle this challenge. We use multi-projections obtained via a common cube-like projection process from the point clouds for both full-reference (FR) and no-reference (NR) PCQA tasks. Quality-aware features are extracted with popular vision backbones. The FR quality representation is computed as the similarity between the feature maps of reference and distorted projections while the NR quality representation is obtained by simply squeezing the feature maps of distorted projections with average pooling The corresponding quality representations are regressed into visual quality scores by fully-connected layers. Taking part in the ICIP 2023 PCVQA Challenge, we succeeded in achieving the top spot in four out of the five competition tracks.
</details>
<details>
<summary>摘要</summary>
点云是广泛应用于3D内容表示领域中的一种常用技术，但是压缩和简化过程会导致数据损失。因此，有效地评估点云的质量变得越来越重要。在这篇论文中，我们提出了一些简单的基线方法用于基于投影的点云质量评估（PCQA）任务。我们使用了通过共同的立方体投影过程获得的多个投影，并从点云中提取了流行的视觉脊梁中的质量感知特征。FR质量表示为参照投影和扭曲投影之间的相似性，而NR质量表示直接压缩扭曲投影的特征图，并使用了全连接层进行回归。在ICIP 2023 PCVQA挑战中，我们成功地获得了五个竞赛轨道中的四个首位。
</details></li>
</ul>
<hr>
<h2 id="A-Classifier-Using-Global-Character-Level-and-Local-Sub-unit-Level-Features-for-Hindi-Online-Handwritten-Character-Recognition"><a href="#A-Classifier-Using-Global-Character-Level-and-Local-Sub-unit-Level-Features-for-Hindi-Online-Handwritten-Character-Recognition" class="headerlink" title="A Classifier Using Global Character Level and Local Sub-unit Level Features for Hindi Online Handwritten Character Recognition"></a>A Classifier Using Global Character Level and Local Sub-unit Level Features for Hindi Online Handwritten Character Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17138">http://arxiv.org/abs/2310.17138</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anand Sharma, A. G. Ramakrishnan</li>
<li>For: The paper is written to develop a classifier for Hindi online handwritten characters, which models the joint distribution of global character features, number of sub-units, and local sub-unit features using latent variables.* Methods: The classifier uses histograms of points, orientations, and dynamics of orientations (HPOD) features to represent characters at both global and local levels, and the parameters are estimated using maximum likelihood method. The study also compares the performance of the developed classifier with other classifiers and features used in previous studies.* Results: The developed classifier achieves the highest accuracy of 93.5% on the testing set, outperforming other classifiers trained on different features extracted from the same training set and evaluated on the same testing set.Here are the three key points in Simplified Chinese text:* For: 本研究开发了一种基于全球特征、分割单元数量和地方分割单元特征的类ifizier，用于模型印地语 Онлайн手写字符。* Methods: 该类ifizier使用点频、方向和方向动态特征（HPOD）来表示字符的全球特征和地方分割单元特征，并使用最大可能性方法来估计类ifizier的参数。研究还对以前的研究中使用的不同类ifizier和特征进行比较。* Results: 研究发现，基于HPOD特征的类ifizier在测试集上达到了93.5%的最高准确率，超过了基于不同特征的类ifizier在同一测试集上的表现。<details>
<summary>Abstract</summary>
A classifier is developed that defines a joint distribution of global character features, number of sub-units and local sub-unit features to model Hindi online handwritten characters. The classifier uses latent variables to model the structure of sub-units. The classifier uses histograms of points, orientations, and dynamics of orientations (HPOD) features to represent characters at global character level and local sub-unit level and is independent of character stroke order and stroke direction variations. The parameters of the classifier is estimated using maximum likelihood method. Different classifiers and features used in other studies are considered in this study for classification performance comparison with the developed classifier. The classifiers considered are Second Order Statistics (SOS), Sub-space (SS), Fisher Discriminant (FD), Feedforward Neural Network (FFN) and Support Vector Machines (SVM) and the features considered are Spatio Temporal (ST), Discrete Fourier Transform (DFT), Discrete Cosine Transform (SCT), Discrete Wavelet Transform (DWT), Spatial (SP) and Histograms of Oriented Gradients (HOG). Hindi character datasets used for training and testing the developed classifier consist of samples of handwritten characters from 96 different character classes. There are 12832 samples with an average of 133 samples per character class in the training set and 2821 samples with an average of 29 samples per character class in the testing set. The developed classifier has the highest accuracy of 93.5\% on the testing set compared to that of the classifiers trained on different features extracted from the same training set and evaluated on the same testing set considered in this study.
</details>
<details>
<summary>摘要</summary>
我们开发了一种分类器，它定义了全局字符特征、数量的子单元特征和本地子单元特征的共同分布，用于模型印度 Онлайн手写字符。这个分类器使用隐藏变量来模型子单元的结构。它使用点频率、方向和方向的变化（HPOD）特征来表示字符的全局特征和本地子单元特征，并且不受字符笔触顺序和笔触方向的变化。参数的估计使用最大可能性方法。本研究中考虑了其他一些研究使用的不同分类器和特征，包括第二阶 statistics（SOS）、子空间（SS）、捕捉特征（FD）、径向神经网络（FFN）和支持向量机（SVM），以及各种特征，如时空特征（ST）、抽象傅立叙变换（DFT）、抽象佩顺叙变换（SCT）、时空扫描变换（DWT）、空间特征（SP）和方向特征（HOG）。印度字符数据集用于训练和测试开发的分类器，包括96个不同字符类的样本。训练集包含12832个样本，平均每个字符类样本133个，测试集包含2821个样本，平均每个字符类样本29个。开发的分类器在测试集上的准确率为93.5%，比其他在同一个训练集和测试集上训练的分类器的准确率高。
</details></li>
</ul>
<hr>
<h2 id="Comparison-of-Cross-Entropy-Dice-and-Focal-Loss-for-Sea-Ice-Type-Segmentation"><a href="#Comparison-of-Cross-Entropy-Dice-and-Focal-Loss-for-Sea-Ice-Type-Segmentation" class="headerlink" title="Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation"></a>Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17135">http://arxiv.org/abs/2310.17135</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Pires de Lima, Behzad Vahedi, Morteza Karimzadeh</li>
<li>for: 这篇论文是为了提高对于冰对航行安全的 Navigation 中使用 Convolutional Neural Network (CNN) 模型来生成冰图。</li>
<li>methods: 这篇论文使用了三种不同的损失函数（cross-entropy、Dice和Focal），以测试它们在对冰类型的预测中的表现。</li>
<li>results:  despite the fact that Dice 和 Focal loss produce higher metrics, results from cross-entropy seem generally more physically consistent。<details>
<summary>Abstract</summary>
Up-to-date sea ice charts are crucial for safer navigation in ice-infested waters. Recently, Convolutional Neural Network (CNN) models show the potential to accelerate the generation of ice maps for large regions. However, results from CNN models still need to undergo scrutiny as higher metrics performance not always translate to adequate outputs. Sea ice type classes are imbalanced, requiring special treatment during training. We evaluate how three different loss functions, some developed for imbalanced class problems, affect the performance of CNN models trained to predict the dominant ice type in Sentinel-1 images. Despite the fact that Dice and Focal loss produce higher metrics, results from cross-entropy seem generally more physically consistent.
</details>
<details>
<summary>摘要</summary>
现代海冰图表是航海安全 navigation 中不可或缺的。最近，卷积神经网络（CNN）模型表现出加速大区域海冰图表生成的潜力。然而，CNN 模型的结果仍需受到评估，因为高度度量表现并不总是能够确保良好的输出。海冰类型受到不均匀分布，需要特殊的训练方法。我们 evaluate 了三种不同的损失函数，其中一些是为异常分布类问题而设计的，如何影响 CNN 模型在 Sentinel-1 图像中预测主要海冰类型的性能。虽然 Dice 和 Focal 损失能生成更高的度量，但跨度 entropy 损失的结果更加 физи学上一致。
</details></li>
</ul>
<hr>
<h2 id="Virtual-Accessory-Try-On-via-Keypoint-Hallucination"><a href="#Virtual-Accessory-Try-On-via-Keypoint-Hallucination" class="headerlink" title="Virtual Accessory Try-On via Keypoint Hallucination"></a>Virtual Accessory Try-On via Keypoint Hallucination</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17131">http://arxiv.org/abs/2310.17131</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junhong Gou, Bo Zhang, Li Niu, Jianfu Zhang, Jianlou Si, Chen Qian, Liqing Zhang</li>
<li>for: 处理虚拟穿着和虚拟配饰的任务，专注于虚拟配饰try-on，将饰物（例如眼镜、领带）适应到脸部或肖像图像中。</li>
<li>methods: 我们提出了一个背景对准网络，利用背景知识来将背景和前景融合成一个合理的合成图像。我们的方法首先学习人体知识，然后预测该项目的目标位置，然后将该资讯与预测的位置混合到背景UNet中。最后，我们计算扭曲参数，将该资讯扭曲到背景中。</li>
<li>results: 我们在STRAT dataset上进行了实验， validate the effectiveness of our proposed method。<details>
<summary>Abstract</summary>
The virtual try-on task refers to fitting the clothes from one image onto another portrait image. In this paper, we focus on virtual accessory try-on, which fits accessory (e.g., glasses, ties) onto a face or portrait image. Unlike clothing try-on, which relies on human silhouette as guidance, accessory try-on warps the accessory into an appropriate location and shape to generate a plausible composite image. In contrast to previous try-on methods that treat foreground (i.e., accessories) and background (i.e., human faces or bodies) equally, we propose a background-oriented network to utilize the prior knowledge of human bodies and accessories. Specifically, our approach learns the human body priors and hallucinates the target locations of specified foreground keypoints in the background. Then our approach will inject foreground information with accessory priors into the background UNet. Based on the hallucinated target locations, the warping parameters are calculated to warp the foreground. Moreover, this background-oriented network can also easily incorporate auxiliary human face/body semantic segmentation supervision to further boost performance. Experiments conducted on STRAT dataset validate the effectiveness of our proposed method.
</details>
<details>
<summary>摘要</summary>
virtual 试穿任务指的是从一个图像上适应另一个肖像图像上的服装。在这篇论文中，我们专注于虚拟配饰试穿，即将配饰（如镜片、领带）适应到一个脸或肖像图像上。与服装试穿不同，配饰试穿不需要人体轮廓作为指导，而是将配饰扭曲到合适的位置和形状，以生成一个可信度高的复合图像。在之前的试穿方法中，背景（即人体）和前景（即配饰）被平等对待，我们提议了背景 oriented 网络，以利用人体和配饰的先验知识。具体来说，我们的方法学习人体先验知识，并在背景中预测target键带点的位置。然后，我们将前景信息与配饰先验知识混合到背景 UNet 中，根据预测的target位置，计算扭曲参数，以扭曲前景。此外，这种背景 oriented 网络还可以轻松地包含辅助人体脸/身体 semantic 分割监督，以进一步提高性能。在 STRAT 数据集上进行的实验证明了我们的提议的效果。
</details></li>
</ul>
<hr>
<h2 id="Task-driven-Prompt-Evolution-for-Foundation-Models"><a href="#Task-driven-Prompt-Evolution-for-Foundation-Models" class="headerlink" title="Task-driven Prompt Evolution for Foundation Models"></a>Task-driven Prompt Evolution for Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17128">http://arxiv.org/abs/2310.17128</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rachana Sathish, Rahul Venkataramani, K S Shriram, Prasad Sudhakar</li>
<li>for: 这个研究是为了提高基础模型（Segment Anything Model，SAM）在医疗影像模式下的表现。</li>
<li>methods: 这个研究使用了大量预训条件和概念学习模型，从下游任务中学习提示来优化基础模型的表现。</li>
<li>results: 研究发现，这种提示优化技术可以在肺部 segmentation中获得了 significiant improvement（约75%），并且可以自动优化基础模型的提示，以提高其表现。<details>
<summary>Abstract</summary>
Promptable foundation models, particularly Segment Anything Model (SAM), have emerged as a promising alternative to the traditional task-specific supervised learning for image segmentation. However, many evaluation studies have found that their performance on medical imaging modalities to be underwhelming compared to conventional deep learning methods. In the world of large pre-trained language and vision-language models, learning prompt from downstream tasks has achieved considerable success in improving performance. In this work, we propose a plug-and-play Prompt Optimization Technique for foundation models like SAM (SAMPOT) that utilizes the downstream segmentation task to optimize the human-provided prompt to obtain improved performance. We demonstrate the utility of SAMPOT on lung segmentation in chest X-ray images and obtain an improvement on a significant number of cases ($\sim75\%$) over human-provided initial prompts. We hope this work will lead to further investigations in the nascent field of automatic visual prompt-tuning.
</details>
<details>
<summary>摘要</summary>
通用基础模型，如分割任何模型（SAM），已经出现为图像分割任务中的有前途的替代方案。然而，许多评估研究发现，这些模型在医疗影像模式上的表现不如传统的深度学习方法出色。在大型预训练语言和视觉语言模型的世界中，学习下游任务中的提示已经取得了显著的成功，以提高性能。在这项工作中，我们提出了一种插入式优化技术（SAMPOT），使用下游分割任务来优化提供的人类提示，以获得改善的性能。我们在肺部分剖扫图像中应用SAMPOT，并在大量的 случаес中（约75%）获得了人类提供的初始提示的改善。我们希望这项工作会鼓励进一步的自动视觉提示优化研究。
</details></li>
</ul>
<hr>
<h2 id="Deep-Learning-on-SAR-Imagery-Transfer-Learning-Versus-Randomly-Initialized-Weights"><a href="#Deep-Learning-on-SAR-Imagery-Transfer-Learning-Versus-Randomly-Initialized-Weights" class="headerlink" title="Deep Learning on SAR Imagery: Transfer Learning Versus Randomly Initialized Weights"></a>Deep Learning on SAR Imagery: Transfer Learning Versus Randomly Initialized Weights</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17126">http://arxiv.org/abs/2310.17126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Morteza Karimzadeh, Rafael Pires de Lima</li>
<li>for: 本研究旨在评估深度学习在探测雷达数据上的应用，特别是用于海洋 Navigation 的海冰映射。</li>
<li>methods: 本研究使用了 randomly initialized weights 和 fine-tuning pre-trained model 两种方法来训练深度学习模型。</li>
<li>results: 研究结果显示，使用 pre-trained model 进行 fine-tuning 后，模型在测试样本中的表现更佳，特别是在融雪季节的样本上。<details>
<summary>Abstract</summary>
Deploying deep learning on Synthetic Aperture Radar (SAR) data is becoming more common for mapping purposes. One such case is sea ice, which is highly dynamic and rapidly changes as a result of the combined effect of wind, temperature, and ocean currents. Therefore, frequent mapping of sea ice is necessary to ensure safe marine navigation. However, there is a general shortage of expert-labeled data to train deep learning algorithms. Fine-tuning a pre-trained model on SAR imagery is a potential solution. In this paper, we compare the performance of deep learning models trained from scratch using randomly initialized weights against pre-trained models that we fine-tune for this purpose. Our results show that pre-trained models lead to better results, especially on test samples from the melt season.
</details>
<details>
<summary>摘要</summary>
deploying deep learning on synthetic aperture radar (SAR) data 是 becoming more common for mapping purposes. One such case is sea ice, which is highly dynamic and rapidly changes as a result of the combined effect of wind, temperature, and ocean currents. Therefore, frequent mapping of sea ice is necessary to ensure safe marine navigation. However, there is a general shortage of expert-labeled data to train deep learning algorithms. Fine-tuning a pre-trained model on SAR imagery is a potential solution. In this paper, we compare the performance of deep learning models trained from scratch using randomly initialized weights against pre-trained models that we fine-tune for this purpose. Our results show that pre-trained models lead to better results, especially on test samples from the melt season.Here's the text with Traditional Chinese characters:部署深度学习于Synthetic Aperture Radar（SAR）数据是 becoming more common for mapping purposes。One such case is sea ice, which is highly dynamic and rapidly changes as a result of the combined effect of wind, temperature, and ocean currents。Therefore, frequent mapping of sea ice is necessary to ensure safe marine navigation。However, there is a general shortage of expert-labeled data to train deep learning algorithms。Fine-tuning a pre-trained model on SAR imagery is a potential solution。In this paper, we compare the performance of deep learning models trained from scratch using randomly initialized weights against pre-trained models that we fine-tune for this purpose。Our results show that pre-trained models lead to better results, especially on test samples from the melt season。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-sea-ice-segmentation-in-Sentinel-1-images-with-atrous-convolutions"><a href="#Enhancing-sea-ice-segmentation-in-Sentinel-1-images-with-atrous-convolutions" class="headerlink" title="Enhancing sea ice segmentation in Sentinel-1 images with atrous convolutions"></a>Enhancing sea ice segmentation in Sentinel-1 images with atrous convolutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17122">http://arxiv.org/abs/2310.17122</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rafael Pires de Lima, Behzad Vahedi, Nick Hughes, Andrew P. Barrett, Walter Meier, Morteza Karimzadeh</li>
<li>for: 这研究旨在使用机器学习算法来自动化海冰图表生成，以提高海冰 Navigation 的效率。</li>
<li>methods: 我们使用了 Extreme Earth version 2 高分辨率测试数据集，并开发了一个自定义管道，其 combining ResNets 和 Atrous Spatial Pyramid Pooling 来 segments SAR 图像。</li>
<li>results: 我们的模型在 binary 海冰-开水分类和多类海冰分类两个方面都达到了高效性。特别是，在 January 和 July 测试场景中，我们的模型的 weighted F1 分数都大于 0.95， median weighted F1 分数为 0.98。相比之下，一个基eline U-Net 的 weighted average F1 分数在 July 和 January 测试场景中分别为 0.92-0.94 和 0.97-0.98。<details>
<summary>Abstract</summary>
Due to the growing volume of remote sensing data and the low latency required for safe marine navigation, machine learning (ML) algorithms are being developed to accelerate sea ice chart generation, currently a manual interpretation task. However, the low signal-to-noise ratio of the freely available Sentinel-1 Synthetic Aperture Radar (SAR) imagery, the ambiguity of backscatter signals for ice types, and the scarcity of open-source high-resolution labelled data makes automating sea ice mapping challenging. We use Extreme Earth version 2, a high-resolution benchmark dataset generated for ML training and evaluation, to investigate the effectiveness of ML for automated sea ice mapping. Our customized pipeline combines ResNets and Atrous Spatial Pyramid Pooling for SAR image segmentation. We investigate the performance of our model for: i) binary classification of sea ice and open water in a segmentation framework; and ii) a multiclass segmentation of five sea ice types. For binary ice-water classification, models trained with our largest training set have weighted F1 scores all greater than 0.95 for January and July test scenes. Specifically, the median weighted F1 score was 0.98, indicating high performance for both months. By comparison, a competitive baseline U-Net has a weighted average F1 score of ranging from 0.92 to 0.94 (median 0.93) for July, and 0.97 to 0.98 (median 0.97) for January. Multiclass ice type classification is more challenging, and even though our models achieve 2% improvement in weighted F1 average compared to the baseline U-Net, test weighted F1 is generally between 0.6 and 0.80. Our approach can efficiently segment full SAR scenes in one run, is faster than the baseline U-Net, retains spatial resolution and dimension, and is more robust against noise compared to approaches that rely on patch classification.
</details>
<details>
<summary>摘要</summary>
Translation:由于远程感知数据的增长量和海洋导航需要的延迟时间都在逐渐增长，因此机器学习（ML）算法在自动化海冰图制定中得到了广泛的应用。然而，自由available Sentinel-1 Synthetic Aperture Radar（SAR）影像的信号噪声比率较低，反射信号的含义对冰种类是ambiguous，而且开源高分辨率标注数据的缺乏使自动化海冰 mapping更加挑战。我们使用Extreme Earth版2，一个高分辨率标准 datasets generated for ML training和评估，来调查ML在自动化海冰 mapping中的效果。我们自定义的管道 combining ResNets和Atrous Spatial Pyramid Pooling来进行SAR图像分割。我们对以下两个方面进行调查：i) 将海冰和开水分割为二元类型；ii) 将五种冰种类分割为多类型。对于二元海冰-开水分割，我们使用最大training set进行训练的模型均有weighted F1 scores大于0.95 for January和July测试场景。特别是，测试场景的中位weighted F1 score是0.98，表示在这两个月的性能都很高。与基线U-Net相比，我们的模型在July月的测试场景中weighted average F1 score在0.92-0.94之间（中位值为0.93），在January月的测试场景中weighted average F1 score在0.97-0.98之间（中位值为0.97）。对于多类冰种类分割，我们的模型比基线U-Net提高了2%的weighted F1平均分，但测试weighted F1在0.6-0.80之间。我们的方法可以一次性将整个SAR场景分割，比基线U-Net更快，保留空间分辨率和维度，并对噪声更加抗性。
</details></li>
</ul>
<hr>
<h2 id="LP-OVOD-Open-Vocabulary-Object-Detection-by-Linear-Probing"><a href="#LP-OVOD-Open-Vocabulary-Object-Detection-by-Linear-Probing" class="headerlink" title="LP-OVOD: Open-Vocabulary Object Detection by Linear Probing"></a>LP-OVOD: Open-Vocabulary Object Detection by Linear Probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17109">http://arxiv.org/abs/2310.17109</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chau Pham, Truong Vu, Khoi Nguyen<br>for:* This paper addresses the challenging problem of open-vocabulary object detection (OVOD), where an object detector must identify both seen and unseen classes in test images without labeled examples of the unseen classes in training.methods:* The proposed method, LP-OVOD, uses a novel approach that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the top relevant region proposals to the novel text.results:* Experimental results on COCO affirm the superior performance of the LP-OVOD approach over the state of the art, achieving $\textbf{40.5}$ in $\text{AP}_{novel}$ using ResNet50 as the backbone and without external datasets or knowing novel classes during training.<details>
<summary>Abstract</summary>
This paper addresses the challenging problem of open-vocabulary object detection (OVOD) where an object detector must identify both seen and unseen classes in test images without labeled examples of the unseen classes in training. A typical approach for OVOD is to use joint text-image embeddings of CLIP to assign box proposals to their closest text label. However, this method has a critical issue: many low-quality boxes, such as over- and under-covered-object boxes, have the same similarity score as high-quality boxes since CLIP is not trained on exact object location information. To address this issue, we propose a novel method, LP-OVOD, that discards low-quality boxes by training a sigmoid linear classifier on pseudo labels retrieved from the top relevant region proposals to the novel text. Experimental results on COCO affirm the superior performance of our approach over the state of the art, achieving $\textbf{40.5}$ in $\text{AP}_{novel}$ using ResNet50 as the backbone and without external datasets or knowing novel classes during training. Our code will be available at https://github.com/VinAIResearch/LP-OVOD.
</details>
<details>
<summary>摘要</summary>
这个论文解决了开放词汇物体检测（OVOD）的挑战，即在测试图像中无需在训练中提供未知类的标注时，一个物体检测器可以识别已知和未知类。一种常见的OVOD方法是使用CLIP的共同文本图像嵌入来将盒子提案分配给其最近的文本标签。然而，这种方法存在一个重要问题：许多低质量盒子，如过度和下遮掩物体盒子，与高质量盒子具有同样的相似性分数，因为CLIP没有接受具体物体位置信息的训练。为解决这个问题，我们提出了一种新的方法，LP-OVOD，它抛弃低质量盒子通过在顶部相关地区提取的pseudo标签来训练sigmoid线性分类器。实验结果表明我们的方法在COCO上超过了现有的状态态势，达到了$\textbf{40.5}$的$\text{AP}_{novel}$值，使用ResNet50作为背景网络，不需要外部数据集或在训练过程中知道未知类。我们的代码将在https://github.com/VinAIResearch/LP-OVOD上发布。
</details></li>
</ul>
<hr>
<h2 id="Navigating-Data-Heterogeneity-in-Federated-Learning-A-Semi-Supervised-Approach-for-Object-Detection"><a href="#Navigating-Data-Heterogeneity-in-Federated-Learning-A-Semi-Supervised-Approach-for-Object-Detection" class="headerlink" title="Navigating Data Heterogeneity in Federated Learning A Semi-Supervised Approach for Object Detection"></a>Navigating Data Heterogeneity in Federated Learning A Semi-Supervised Approach for Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17097">http://arxiv.org/abs/2310.17097</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taehyeon Kim, Eric Lin, Junu Lee, Christian Lau, Vaikkunth Mugunthan</li>
<li>for: 本研究旨在提出一种semi-supervised federated object detection（SSFOD）方法，用于Scene中where labeled data只存在服务器端，客户端具有无标签数据。</li>
<li>methods: 我们提出了一种两阶段策略，包括选择性训练和正交增强全参数训练，以有效地解决数据shift（例如天气条件） между服务器和客户端。我们还提出了一种选择性修剪backbone的检测器，以避免过拟合；一种正交规则来增强表达分歧；以及一种本地EMA驱动的假标分配来生成高质量假标。</li>
<li>results: 我们对prominent autonomous driving dataset（BDD100K、Cityscapes和SODA10M）进行了广泛的验证，并证明了我们的方法的有效性。特别是，使用仅20-30%的标签，FedSTO方法可以与完全supervised centralized training方法相比，达到nearly同等水平的性能。<details>
<summary>Abstract</summary>
Federated Learning (FL) has emerged as a potent framework for training models across distributed data sources while maintaining data privacy. Nevertheless, it faces challenges with limited high-quality labels and non-IID client data, particularly in applications like autonomous driving. To address these hurdles, we navigate the uncharted waters of Semi-Supervised Federated Object Detection (SSFOD). We present a pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients possess unlabeled data. Notably, our method represents the inaugural implementation of SSFOD for clients with 0% labeled non-IID data, a stark contrast to previous studies that maintain some subset of labels at each client. We propose FedSTO, a two-stage strategy encompassing Selective Training followed by Orthogonally enhanced full-parameter training, to effectively address data shift (e.g. weather conditions) between server and clients. Our contributions include selectively refining the backbone of the detector to avert overfitting, orthogonality regularization to boost representation divergence, and local EMA-driven pseudo label assignment to yield high-quality pseudo labels. Extensive validation on prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M) attests to the efficacy of our approach, demonstrating state-of-the-art results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as well as fully-supervised centralized training methods.
</details>
<details>
<summary>摘要</summary>
federnated learning (FL) 已经成为训练模型遍布分布式数据源的有效框架，同时保持数据隐私。然而，它面临有限高质量标签和非标一致客户端数据的挑战，特别是在自动驾驶应用中。为解决这些障碍，我们在无法预测的水域中探索 semi-supervised federated object detection (SSFOD)。我们提出了一种先进的 SSFOD 框架，适用于客户端只有无标记数据，而服务器只有标记数据。尤其是，我们的方法是首次实现 SSFOD 客户端无标记非标一致数据上的实现，与前一些研究不同，后者都保留了每个客户端上的一些标签。我们提出了 FedSTO，一种两阶段策略，包括选择性训练和正交扩展全参数训练，以有效地解决数据偏移（如天气条件）问题。我们的贡献包括选择性修正检测器的背bone，避免过拟合，正交regularization 增强表示异常性，以及本地EMA驱动的pseudo标签分配。我们对知名的自动驾驶数据集（BDD100K、Cityscapes和SODA10M）进行了广泛验证，证明我们的方法的有效性。印象人是，FedSTO，只使用20-30%的标签，可以与完全监督中心训练方法相比。
</details></li>
</ul>
<hr>
<h2 id="Automating-lichen-monitoring-in-ecological-studies-using-instance-segmentation-of-time-lapse-images"><a href="#Automating-lichen-monitoring-in-ecological-studies-using-instance-segmentation-of-time-lapse-images" class="headerlink" title="Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images"></a>Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17080">http://arxiv.org/abs/2310.17080</a></li>
<li>repo_url: None</li>
<li>paper_authors: Safwen Naimi, Olfa Koubaa, Wassim Bouachir, Guillaume-Alexandre Bilodeau, Gregory Jeddore, Patricia Baines, David Correia, Andre Arsenault</li>
<li>for:  assist ecologists in monitoring and analyzing epiphytic lichens</li>
<li>methods:  use time-lapse cameras and semantic segmentation with an effective training approach to automate monitoring and biomass estimation of epiphytic lichens</li>
<li>results:  significantly improve the accuracy and efficiency of lichen population monitoring, making it a valuable tool for forest ecologists and environmental scientists to evaluate the impact of climate change on Canada’s forests<details>
<summary>Abstract</summary>
Lichens are symbiotic organisms composed of fungi, algae, and/or cyanobacteria that thrive in a variety of environments. They play important roles in carbon and nitrogen cycling, and contribute directly and indirectly to biodiversity. Ecologists typically monitor lichens by using them as indicators to assess air quality and habitat conditions. In particular, epiphytic lichens, which live on trees, are key markers of air quality and environmental health. A new method of monitoring epiphytic lichens involves using time-lapse cameras to gather images of lichen populations. These cameras are used by ecologists in Newfoundland and Labrador to subsequently analyze and manually segment the images to determine lichen thalli condition and change. These methods are time-consuming and susceptible to observer bias. In this work, we aim to automate the monitoring of lichens over extended periods and to estimate their biomass and condition to facilitate the task of ecologists. To accomplish this, our proposed framework uses semantic segmentation with an effective training approach to automate monitoring and biomass estimation of epiphytic lichens on time-lapse images. We show that our method has the potential to significantly improve the accuracy and efficiency of lichen population monitoring, making it a valuable tool for forest ecologists and environmental scientists to evaluate the impact of climate change on Canada's forests. To the best of our knowledge, this is the first time that such an approach has been used to assist ecologists in monitoring and analyzing epiphytic lichens.
</details>
<details>
<summary>摘要</summary>
蘑菇是一种 симбиotic 生物，由真菌、藻类和/或细菌组成，可以在多种环境中存活。它们对碳和氮的循环具有重要作用，并直接和间接地对生物多样性产生贡献。生态学家通常通过使用蘑菇作为指标来评估空气质量和栖息环境。特别是epiphytic蘑菇，生活在树上，是评估空气质量和环境健康的关键标志。一种新的监测epiphytic蘑菇的方法是使用时间�lapse摄像头获取图像。这些摄像头由新foundland和Labrador的生态学家使用，以后分析和手动分割图像，以确定蘑菇质量和变化。这些方法时间consuming和易受观察者偏见的影响。在这项工作中，我们的目标是自动监测蘑菇在长期内的变化，并估计其生物质量和condition。为此，我们提出了一个基于semantic Segmentation的框架，以自动监测和估计epiphytic蘑菇在时间�lapse图像上的生物质量和condition。我们表明，我们的方法具有提高精度和效率的潜在优势，可以为森林生态学家和环境科学家提供一种有价值的工具，以评估气候变化对加拿大森林的影响。到目前为止，这是首次使用这种方法来帮助生态学家监测和分析epiphytic蘑菇。
</details></li>
</ul>
<hr>
<h2 id="HCT-Hybrid-Convnet-Transformer-for-Parkinson’s-disease-detection-and-severity-prediction-from-gait"><a href="#HCT-Hybrid-Convnet-Transformer-for-Parkinson’s-disease-detection-and-severity-prediction-from-gait" class="headerlink" title="HCT: Hybrid Convnet-Transformer for Parkinson’s disease detection and severity prediction from gait"></a>HCT: Hybrid Convnet-Transformer for Parkinson’s disease detection and severity prediction from gait</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17078">http://arxiv.org/abs/2310.17078</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/safwennaimi/hct-hybrid-convnet-transformer-for-parkinson-s-disease-detection-and-severity-prediction-from-gait">https://github.com/safwennaimi/hct-hybrid-convnet-transformer-for-parkinson-s-disease-detection-and-severity-prediction-from-gait</a></li>
<li>paper_authors: Safwen Naimi, Wassim Bouachir, Guillaume-Alexandre Bilodeau</li>
<li>for: 本研究提出了一种基于新型混合卷积神经网络-变换器架构的深度学习方法，用于从步态数据中检测和分stage帕金森病（PD）。</li>
<li>methods: 我们采用了一种两步方法，将问题分解为两个子问题。我们的混合架构首先将健康人 versus 帕金森病人分类。如果患者是帕金森病人，那么我们的多类 Hybrid ConvNet-Transformer 模型将确定帕金森病的谱分 stage。我们的混合 architecture 利用了 ConvNet 和 Transformer 两种不同的强大技术，以便准确地检测 PD 和确定其严重程度 stage。</li>
<li>results: 我们的混合方法在比较其他状态革新方法时表现出优异，PD 检测精度达 97%，严重程度分 stage 精度达 87%。I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
In this paper, we propose a novel deep learning method based on a new Hybrid ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD) from gait data. We adopt a two-step approach by dividing the problem into two sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy versus parkinsonian patients. If the patient is parkinsonian, a multi-class Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&Y) score to assess the PD severity stage. Our hybrid architecture exploits the strengths of both Convolutional Neural Networks (ConvNets) and Transformers to accurately detect PD and determine the severity stage. In particular, we take advantage of ConvNets to capture local patterns and correlations in the data, while we exploit Transformers for handling long-term dependencies in the input signal. We show that our hybrid method achieves superior performance when compared to other state-of-the-art methods, with a PD detection accuracy of 97% and a severity staging accuracy of 87%. Our source code is available at: https://github.com/SafwenNaimi
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们提出了一种新的深度学习方法，基于新的混合ConvNet-Transformer架构，用于从步态数据中检测和分期 Parkinson's disease（PD）。我们采用了两步 Approach，先将问题分为两个子问题。我们的混合架构首先分辨健康和 Parkinsonian 患者。如果患者是 Parkinsonian， THEN 我们的多类 Hybrid ConvNet-Transformer 模型确定了 Hoehn 和 Yahr（H&Y）分数，以评估PD的严重程度阶段。我们的混合架构利用 ConvNets 捕捉本地征 patrerns 和相关性，而Transformers 处理输入信号的长期依赖关系。我们表明，我们的混合方法在比较其他当前领先方法时，具有superior的性能，PD检测精度达97%，严重阶段评估精度达87%。我们的源代码可以在：https://github.com/SafwenNaimi 中找到。
</details></li>
</ul>
<hr>
<h2 id="HyperFields-Towards-Zero-Shot-Generation-of-NeRFs-from-Text"><a href="#HyperFields-Towards-Zero-Shot-Generation-of-NeRFs-from-Text" class="headerlink" title="HyperFields: Towards Zero-Shot Generation of NeRFs from Text"></a>HyperFields: Towards Zero-Shot Generation of NeRFs from Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17075">http://arxiv.org/abs/2310.17075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sudarshan Babu, Richard Liu, Avery Zhou, Michael Maire, Greg Shakhnarovich, Rana Hanocka</li>
<li>for: 这篇论文的目的是提出一种基于文本的NeRF生成方法，可以在单个前进 pass中生成文本状态下的NeRF模型，并且可以在不同场景下进行适应。</li>
<li>methods: 该方法使用了动态权重网络和NeRF填充训练，以学习文本token嵌入空间中的NeRF模型的映射。</li>
<li>results: 该方法可以在不同场景下适应，并且可以在不同的文本条件下生成novel的场景，包括零shot和一些精心调整后的场景。此外，训练HyperFields可以比传统的神经网络优化方法更快速地训练 converges。<details>
<summary>Abstract</summary>
We introduce HyperFields, a method for generating text-conditioned Neural Radiance Fields (NeRFs) with a single forward pass and (optionally) some fine-tuning. Key to our approach are: (i) a dynamic hypernetwork, which learns a smooth mapping from text token embeddings to the space of NeRFs; (ii) NeRF distillation training, which distills scenes encoded in individual NeRFs into one dynamic hypernetwork. These techniques enable a single network to fit over a hundred unique scenes. We further demonstrate that HyperFields learns a more general map between text and NeRFs, and consequently is capable of predicting novel in-distribution and out-of-distribution scenes -- either zero-shot or with a few finetuning steps. Finetuning HyperFields benefits from accelerated convergence thanks to the learned general map, and is capable of synthesizing novel scenes 5 to 10 times faster than existing neural optimization-based methods. Our ablation experiments show that both the dynamic architecture and NeRF distillation are critical to the expressivity of HyperFields.
</details>
<details>
<summary>摘要</summary>
我们介绍HyperFields，一种方法用于生成文本条件的神经辐射场（NeRF），通过单一的前进 pass和（可选）一些精细调整。关键技术包括：（i）动态超网络，该网络学习文本token嵌入空间中的NeRF的缓和映射；（ii）NeRF蒸馏训练，将各个NeRF中的场景编码到一个动态超网络中。这些技术使得单个网络可以适应百余个不同的场景。我们进一步证明，HyperFields学习了文本和NeRF之间的更加通用的映射，因此能够预测静态和动态场景，包括零 shot 和几步精度调整。HyperFields在精度调整过程中受到加速的收敛速度，可以在5到10倍 бы于现有神经优化方法中 synthesize 新场景。我们的抽象实验表明，动态架构和NeRF蒸馏都是HyperFields的表达能力的关键因素。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/cs.CV_2023_10_26/" data-id="clogxf3nm00ju5xra7t3hgw8y" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/cs.AI_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T12:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/cs.AI_2023_10_26/">cs.AI - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Style-Aware-Radiology-Report-Generation-with-RadGraph-and-Few-Shot-Prompting"><a href="#Style-Aware-Radiology-Report-Generation-with-RadGraph-and-Few-Shot-Prompting" class="headerlink" title="Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting"></a>Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17811">http://arxiv.org/abs/2310.17811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Yan, Ruochen Liu, David E. Kuo, Subathra Adithan, Eduardo Pontes Reis, Stephen Kwak, Vasantha Kumar Venugopal, Chloe P. O’Connell, Agustina Saenz, Pranav Rajpurkar, Michael Moor</li>
<li>For: 提高 radiologist 的工作流程，通过自动生成医疗影像报告* Methods: 提出了一种 two-step 方法，首先提取图像中的内容，然后将其折衣成医生特定的报告风格* Results: 在量化评估中获得了有利的性能，人工评估中显示 AI 生成的报告能够匹配医生特定的报告风格，即使只使用了一些示例作为 context<details>
<summary>Abstract</summary>
Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.
</details>
<details>
<summary>摘要</summary>
自动生成的医疗图像报告承诺改善诊断医生的工作流程。现有方法直接将图像转换为完整的报告，但这会混合报告的内容（如发现和其属性）与样式（如格式和语言选择），导致临床不准确的报告。为解决这个问题，我们提出了一种两步方法 для医学报告生成。首先，我们从图像中提取内容；然后，我们将提取的内容转换为医生特有的样式的报告。我们利用RadGraph -- 报告的图表表示方式 -- 以及大型自然语言模型（LLM）。我们的量化评估表明，我们的方法具有有利性。我们的人类评估，医生特有的报告被AI生成的报告杂化不可分辨，即使只使用几个例子作为 контекст。
</details></li>
</ul>
<hr>
<h2 id="Clover-Closed-Loop-Verifiable-Code-Generation"><a href="#Clover-Closed-Loop-Verifiable-Code-Generation" class="headerlink" title="Clover: Closed-Loop Verifiable Code Generation"></a>Clover: Closed-Loop Verifiable Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17807">http://arxiv.org/abs/2310.17807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuyue Sun, Ying Sheng, Oded Padon, Clark Barrett</li>
<li>for: 这篇论文旨在提出一种方法来确保代码生成器生成的代码是正确的，以避免不良的结果。</li>
<li>methods: 该方法基于一种名为Clover的概念，即闭环可验证代码生成，它将正确性检查降低到更容易进行的一个问题：一致性检查。Clover使用了一种新的形式验证工具和大型自然语言模型的结合来实现一个可验证的代码检查器。</li>
<li>results: 在一个手动设计的数据集（CloverBench）上，我们发现：（i）LLMs可以自动生成正式规范; 并且（ii）我们的一致性检查器在正确的实例上可以达到87%的接受率，而且没有任何false positive（Zero tolerance for incorrect instances）。<details>
<summary>Abstract</summary>
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results show that for this dataset, (i) LLMs are reasonably successful at automatically generating formal specifications; and (ii) our consistency checker achieves a promising acceptance rate (up to 87%) for correct instances while maintaining zero tolerance for incorrect ones (no false positives).
</details>
<details>
<summary>摘要</summary>
大量语言模型在软件开发中用于代码生成是一种快速增长的趋势。然而，如果没有有效的方法来确保代码的正确性，这种趋势可能会导致任何不良结果。在这篇论文中，我们提出了一个方法来解决这个挑战：叶对象（Clover）模型，简称为关闭Loop可验证代码生成。叶对象模型的核心在于一个实现了一种可验证性检查的检查器，该检查器通过对代码、文档字符串和正式注释进行一系列的一致性检查来减少正确性检查到更加可 accessible的一致性检查问题。我们提供了一个理论分析，以支持我们的论点，即叶对象模型在一致性检查中应该是有效的。我们还对一个手动设计的数据集（CloverBench）进行了实验研究，该数据集包含了注释的达凡程程序。实验结果显示，（i）LLMs可以自动生成正式规范; （ii）我们的一致性检查器在正确的实例中达到了Promising的接受率（达到87%），而且在错误的实例中保持了零的准确率（没有假阳性）。
</details></li>
</ul>
<hr>
<h2 id="Reward-Scale-Robustness-for-Proximal-Policy-Optimization-via-DreamerV3-Tricks"><a href="#Reward-Scale-Robustness-for-Proximal-Policy-Optimization-via-DreamerV3-Tricks" class="headerlink" title="Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks"></a>Reward Scale Robustness for Proximal Policy Optimization via DreamerV3 Tricks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17805">http://arxiv.org/abs/2310.17805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ryan Sullivan, Akarsh Kumar, Shengyi Huang, John P. Dickerson, Joseph Suarez</li>
<li>for: 本研究旨在应用DreamerV3的模型基method，并评估其是否能够对PPO提供改进。</li>
<li>methods: 本研究使用了DreamerV3的一些陷阱，包括缓存滤选和规律对应。</li>
<li>results: 研究结果显示，这些陷阱并不能够通用于PPO，并且在一些情况下可能会下降性能。然而，研究还发现了一些特定情况下，这些陷阱可以对PPO提供改进，例如在Atari游戏中实现奖励截断。<details>
<summary>Abstract</summary>
Most reinforcement learning methods rely heavily on dense, well-normalized environment rewards. DreamerV3 recently introduced a model-based method with a number of tricks that mitigate these limitations, achieving state-of-the-art on a wide range of benchmarks with a single set of hyperparameters. This result sparked discussion about the generality of the tricks, since they appear to be applicable to other reinforcement learning algorithms. Our work applies DreamerV3's tricks to PPO and is the first such empirical study outside of the original work. Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. We use a high quality PPO reference implementation and present extensive ablation studies totaling over 10,000 A100 hours on the Arcade Learning Environment and the DeepMind Control Suite. Though our experiments demonstrate that these tricks do not generally outperform PPO, we identify cases where they succeed and offer insight into the relationship between the implementation tricks. In particular, PPO with these tricks performs comparably to PPO on Atari games with reward clipping and significantly outperforms PPO without reward clipping.
</details>
<details>
<summary>摘要</summary>
大多数强化学习方法很依赖密集、均衡环境奖励。 DreamerV3 最近引入了一种模型基于方法，并使用了一些技巧来缓解这些限制，在各种标准 benchmark 上达到了单一的超参数 Settings 的状态体验最佳性。这一结果引发了对这些技巧的通用性的讨论，因为它们看起来可以应用于其他强化学习算法。我们的工作将 DreamerV3 的技巧应用到 PPO 中，是外部工作中的第一个实验。Surprisingly, we find that the tricks presented do not transfer as general improvements to PPO. 我们使用高质量 PPO 参考实现，并进行了大量的磨砺研究，总计超过 10,000 A100 小时在 Arcade Learning Environment 和 DeepMind Control Suite 上。虽然我们的实验表明，这些技巧不一般地提高 PPO，但我们确定了其在 Atari 游戏中奖励截断时和无奖励截断时的性能相当。
</details></li>
</ul>
<hr>
<h2 id="“You-Are-An-Expert-Linguistic-Annotator”-Limits-of-LLMs-as-Analyzers-of-Abstract-Meaning-Representation"><a href="#“You-Are-An-Expert-Linguistic-Annotator”-Limits-of-LLMs-as-Analyzers-of-Abstract-Meaning-Representation" class="headerlink" title="“You Are An Expert Linguistic Annotator”: Limits of LLMs as Analyzers of Abstract Meaning Representation"></a>“You Are An Expert Linguistic Annotator”: Limits of LLMs as Analyzers of Abstract Meaning Representation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17793">http://arxiv.org/abs/2310.17793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Allyson Ettinger, Jena D. Hwang, Valentina Pyatkin, Chandra Bhagavatula, Yejin Choi</li>
<li>for: 本研究旨在检验LLMs是否可以作为语言专家提供准确的语义分析结果。</li>
<li>methods: 研究使用了GPT-3、ChatGPT和GPT-4模型，通过对句子意义结构进行分析，并使用Abstract Meaning Representation（AMR）格式进行表示。</li>
<li>results: 研究发现，LLMs可以准确地生成AMR格式的句子意义结构，但是模型输出存在重要和常见的错误，无法生成完全准确的句子意义结构。<details>
<summary>Abstract</summary>
Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can reliably reproduce the basic format of AMR, and can often capture core event, argument, and modifier structure -- however, model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses. Eliciting natural language responses produces similar patterns of errors. Overall, our findings indicate that these models out-of-the-box can capture aspects of semantic structure, but there remain key limitations in their ability to support fully accurate semantic analyses or parses.
</details>
<details>
<summary>摘要</summary>
大语言模型（LLM）显示了惊人的掌握能力和流畅性在语言使用方面。这意味着它们也获得了深刻的语言知识吗？在这篇论文中，我们研究了GPT-3、ChatGPT和GPT-4模型对句子意义结构的分析，使用抽象意义表示（AMR）分析方法，该方法提供了丰富的图形表示方式，抽象于表面形式。我们在两个设置下对模型的分析进行比较：1）直接生成基于零或几个提示的AMR parse，和2）通过语言问题（如“这句话中的主事件是什么，以及其对应的 predicate”）进行间接半重建AMR。在这两个设置下，我们发现模型可靠地生成基本的AMR格式，并常常捕捉核心事件、参加者和修饰结构。然而，模型的输出受到频繁和重大的错误的影响，全面分析parse的可 acceptability 显示，即使有几个示例，模型的成功率几乎为零。召唤自然语言响应也会产生类似的错误模式。总之，我们的发现表明，这些模型可以在出废的情况下捕捉含义结构的方面，但还有关键的限制，它们无法支持完全准确的semantic analyses或parse。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-Language-Models-for-Energy-Load-Forecasting"><a href="#Utilizing-Language-Models-for-Energy-Load-Forecasting" class="headerlink" title="Utilizing Language Models for Energy Load Forecasting"></a>Utilizing Language Models for Energy Load Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17788">http://arxiv.org/abs/2310.17788</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/xuehaouwa/lm-load-forecasting">https://github.com/xuehaouwa/lm-load-forecasting</a></li>
<li>paper_authors: Hao Xue, Flora D. Salim</li>
<li>for: 实时能源负载预测可以帮助企业和城市优化资源配置和管理能源消耗。</li>
<li>methods: 本文提出一种使用语言模型进行能源负载预测的新方法，使用提示技术将能源消耗数据转换为描述性句子，并使用自动生成方法进行预测。</li>
<li>results: 经过实验 validate 的结果显示，该方法可以对真实数据进行高精度的能源负载预测，并且可以预测不同时间点的未来能源负载。<details>
<summary>Abstract</summary>
Energy load forecasting plays a crucial role in optimizing resource allocation and managing energy consumption in buildings and cities. In this paper, we propose a novel approach that leverages language models for energy load forecasting. We employ prompting techniques to convert energy consumption data into descriptive sentences, enabling fine-tuning of language models. By adopting an autoregressive generating approach, our proposed method enables predictions of various horizons of future energy load consumption. Through extensive experiments on real-world datasets, we demonstrate the effectiveness and accuracy of our proposed method. Our results indicate that utilizing language models for energy load forecasting holds promise for enhancing energy efficiency and facilitating intelligent decision-making in energy systems.
</details>
<details>
<summary>摘要</summary>
(Simplified Chinese translation)能量负荷预测对于建筑物和城市资源分配和能源消耗管理起到关键作用。在这篇论文中，我们提出了一种新的方法，利用语言模型进行能量负荷预测。我们使用提示技术将能量消耗数据转化为描述性句子，以便Language Model的微调。采用autoregressive生成方法，我们的提议方法可以预测不同时间 horizons的未来能量负荷消耗。通过对实际数据进行广泛的实验，我们证明了我们的提议方法的效果和准确性。结果表明，通过语言模型进行能量负荷预测，可以提高能源效率，并促进智能决策在能源系统中。
</details></li>
</ul>
<hr>
<h2 id="Evaluation-of-large-language-models-using-an-Indian-language-LGBTI-lexicon"><a href="#Evaluation-of-large-language-models-using-an-Indian-language-LGBTI-lexicon" class="headerlink" title="Evaluation of large language models using an Indian language LGBTI+ lexicon"></a>Evaluation of large language models using an Indian language LGBTI+ lexicon</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17787">http://arxiv.org/abs/2310.17787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aditya Joshi, Shruta Rawat, Alpana Dange</li>
<li>for: 本研究旨在评估大型自然语言处理（LLM）模型在LGBTI+语言上的责任行为。</li>
<li>methods: 本研究使用LGBTI+词汇库进行评估，包括四个步骤：形式化NLP任务，创建测试提示，使用LLM生成输出，并手动评估结果。</li>
<li>results: 研究发现，三种LLM模型无法检测下面带有仇恨内容的语言。此外，我们发现使用机器翻译来评估自然语言理解可能存在限制，特别是在非英语语言中。<details>
<summary>Abstract</summary>
Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs, as demonstrated in the context of prevalent social perception of the LGBTI+ community.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Graph-Convolutional-Networks-for-Complex-Traffic-Scenario-Classification"><a href="#Graph-Convolutional-Networks-for-Complex-Traffic-Scenario-Classification" class="headerlink" title="Graph Convolutional Networks for Complex Traffic Scenario Classification"></a>Graph Convolutional Networks for Complex Traffic Scenario Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17773">http://arxiv.org/abs/2310.17773</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Hoek, Holger Caesar, Andreas Falkovén, Tommy Johansson</li>
<li>for: 这篇论文的目的是为了简化自动驾驶系统（ADS）的安全性证据所需的时间。</li>
<li>methods: 这篇论文使用了enario-based testing方法，并使用了图形传播网络（Graph Convolutional Networks，GCN）来模型车辆与环境的互动，以及其他交通代理人的互动。</li>
<li>results: 这篇论文提出了一个可以模型车辆与环境的互动，以及其他交通代理人的互动的方法，并使用了扩展的 nuScenes 和 Argoverse 2 驾驶测试数据来训练这个方法。这个方法在训练后已经成为了一个可靠的基eline для未来关于每帧复杂enario的分类研究。<details>
<summary>Abstract</summary>
A scenario-based testing approach can reduce the time required to obtain statistically significant evidence of the safety of Automated Driving Systems (ADS). Identifying these scenarios in an automated manner is a challenging task. Most methods on scenario classification do not work for complex scenarios with diverse environments (highways, urban) and interaction with other traffic agents. This is mirrored in their approaches which model an individual vehicle in relation to its environment, but neglect the interaction between multiple vehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing datasets lack diversity and do not have per-frame annotations to accurately learn the start and end time of a scenario. We propose a method for complex traffic scenario classification that is able to model the interaction of a vehicle with the environment, as well as other agents. We use Graph Convolutional Networks to model spatial and temporal aspects of these scenarios. Expanding the nuScenes and Argoverse 2 driving datasets, we introduce a scenario-labeled dataset, which covers different driving environments and is annotated per frame. Training our method on this dataset, we present a promising baseline for future research on per-frame complex scenario classification.
</details>
<details>
<summary>摘要</summary>
一种场景基本测试方法可以减少自动驾驶系统（ADS）的安全性证明所需的时间。确定这些场景的自动化方式是一项具有挑战性的任务。大多数场景分类方法不适用于复杂的场景中（高速公路、城市），并且与其他交通代理人之间的交互。这是它们的方法所模拟的个体车辆与其环境之间的关系，而忽略了多辆车辆之间的交互（例如，割込、静止领航车）。此外，现有的数据集缺乏多样性，并没有每帧的注释，以准确地学习场景的开始和结束时间。我们提议一种能够模型车辆与环境的交互，以及其他代理人之间的交互的方法。我们使用图像卷积网络来模型场景的空间和时间方面。对于扩展nuScenes和Argoverse 2驾驶数据集，我们引入了场景标注数据集，覆盖不同的驾驶环境，并且每帧都有注释。通过对这个数据集进行训练，我们提出了一个可能的基线 для未来关于每帧复杂场景分类的研究。
</details></li>
</ul>
<hr>
<h2 id="GROOViST-A-Metric-for-Grounding-Objects-in-Visual-Storytelling"><a href="#GROOViST-A-Metric-for-Grounding-Objects-in-Visual-Storytelling" class="headerlink" title="GROOViST: A Metric for Grounding Objects in Visual Storytelling"></a>GROOViST: A Metric for Grounding Objects in Visual Storytelling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17770">http://arxiv.org/abs/2310.17770</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/akskuchi/groovist">https://github.com/akskuchi/groovist</a></li>
<li>paper_authors: Aditya K Surikuchi, Sandro Pezzelle, Raquel Fernández</li>
<li>for: 评估视觉故事的可ovygrounding度，即图像序列中显示的实体是否被story中正确地描述。</li>
<li>methods: 分析当前的评估方法，包括专门 для这个目标的评估方法和通用视觉对齐方法，以及提出一种新的评估工具GROOViST，该工具考虑了交叉模态依赖关系、时间不同步（图像序列和story的顺序不一致）以及人类视觉嵌入的 intuitions。</li>
<li>results: GROOViST提供了一种可以评估视觉故事的可ovygrounding度的新评估工具，其中每个组件的贡献可以分别评估和解释。<details>
<summary>Abstract</summary>
A proper evaluation of stories generated for a sequence of images -- the task commonly referred to as visual storytelling -- must consider multiple aspects, such as coherence, grammatical correctness, and visual grounding. In this work, we focus on evaluating the degree of grounding, that is, the extent to which a story is about the entities shown in the images. We analyze current metrics, both designed for this purpose and for general vision-text alignment. Given their observed shortcomings, we propose a novel evaluation tool, GROOViST, that accounts for cross-modal dependencies, temporal misalignments (the fact that the order in which entities appear in the story and the image sequence may not match), and human intuitions on visual grounding. An additional advantage of GROOViST is its modular design, where the contribution of each component can be assessed and interpreted individually.
</details>
<details>
<summary>摘要</summary>
为评估生成的故事（常称视觉故事），需考虑多个方面，如 coherence、 grammatical correctness 和 visual grounding。在这种工作中，我们专注于评估故事的基础设定，即图像中显示的实体是否被描述。我们分析了现有的指标，包括专门为这个目的设计的指标以及通用视觉-文本对齐的指标。由于这些指标的缺点，我们提出了一种新的评估工具——GROOViST，它考虑了跨模态依赖关系、时间不对齐（图像序列和故事序列中实体出现的顺序不同）以及人类对视觉基础的直觉。GROOViST 的另一个优点是它的模块化设计，允许每个组件的贡献被分析和解释。
</details></li>
</ul>
<hr>
<h2 id="Social-Contract-AI-Aligning-AI-Assistants-with-Implicit-Group-Norms"><a href="#Social-Contract-AI-Aligning-AI-Assistants-with-Implicit-Group-Norms" class="headerlink" title="Social Contract AI: Aligning AI Assistants with Implicit Group Norms"></a>Social Contract AI: Aligning AI Assistants with Implicit Group Norms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17769">http://arxiv.org/abs/2310.17769</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/janphilippfranken/scai">https://github.com/janphilippfranken/scai</a></li>
<li>paper_authors: Jan-Philipp Fränken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, Alex Tamkin, Tobias Gerstenberg, Noah D. Goodman</li>
<li>for:  validating the proposal of aligning an AI assistant by inverting a model of users’ preferences from observed interactions</li>
<li>methods:  using proof-of-concept simulations in the economic ultimatum game to formalize user preferences as policies that guide the actions of simulated players</li>
<li>results:  the AI assistant accurately aligns its behavior to match standard policies from the economic literature, but exhibits limited generalization in an out-of-distribution setting and slow learning when there is inconsistency in the relationship between language use and an unknown policy.<details>
<summary>Abstract</summary>
We explore the idea of aligning an AI assistant by inverting a model of users' (unknown) preferences from observed interactions. To validate our proposal, we run proof-of-concept simulations in the economic ultimatum game, formalizing user preferences as policies that guide the actions of simulated players. We find that the AI assistant accurately aligns its behavior to match standard policies from the economic literature (e.g., selfish, altruistic). However, the assistant's learned policies lack robustness and exhibit limited generalization in an out-of-distribution setting when confronted with a currency (e.g., grams of medicine) that was not included in the assistant's training distribution. Additionally, we find that when there is inconsistency in the relationship between language use and an unknown policy (e.g., an altruistic policy combined with rude language), the assistant's learning of the policy is slowed. Overall, our preliminary results suggest that developing simulation frameworks in which AI assistants need to infer preferences from diverse users can provide a valuable approach for studying practical alignment questions.
</details>
<details>
<summary>摘要</summary>
我团队研究了一种方法，通过反向模型用户（未知）的偏好来调整人工智能助手的行为。为了证明我们的建议，我们在经济最终决策游戏中进行了证明性实验，将用户的偏好形式为助手的动作指导政策。我们发现助手的行为准确地匹配了经济文献中的标准政策（例如自利和慈善）。然而，助手学习的政策缺乏 Robustness 和在对应分布外的扩展性，例如当面临不包括在助手训练分布中的货币（例如药物的重量）时，助手的行为会受到限制。此外，当语言使用与未知政策之间存在不一致（例如慈善政策与侮辱语言）时，助手学习政策的速度会减慢。总之，我们的初步结果表明，通过在用户多样化的情况下使用人工智能助手来学习用户的偏好可以提供一种有价值的方法，用于研究实际的对齐问题。
</details></li>
</ul>
<hr>
<h2 id="Salespeople-vs-SalesBot-Exploring-the-Role-of-Educational-Value-in-Conversational-Recommender-Systems"><a href="#Salespeople-vs-SalesBot-Exploring-the-Role-of-Educational-Value-in-Conversational-Recommender-Systems" class="headerlink" title="Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems"></a>Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17749">http://arxiv.org/abs/2310.17749</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lidiya Murakhovs’ka, Philippe Laban, Tian Xie, Caiming Xiong, Chien-Sheng Wu</li>
<li>for: 这个论文的目的是提供一种基于对话的产品推荐系统，同时提供教育性的值。</li>
<li>methods: 这个论文使用了大语言模型（LLM）来实现混合类型混合动机的对话系统，并通过人类研究对比专业销售人员的表现。</li>
<li>results: 研究发现，虽然销售机器人（SalesBot）的流畅性和信息准确性与专业销售人员相当，但是它在提供建议质量方面落后。此外，研究还发现，销售机器人和专业销售人员都面临着确保信念的挑战。<details>
<summary>Abstract</summary>
Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context. We release our code and make all data available.
</details>
<details>
<summary>摘要</summary>
大购物需要消费者进行研究或咨询销售人员以获得领域专业知识。然而，现有的对话式推荐系统（CRS）通常忽视用户的背景知识不足，专注于收集首选。在这项工作中，我们定义了对话式代理人提供产品推荐和教育价值的新问题空间。我们提出了销售操作（SalesOps）框架，利用最新的大语言模型（LLMs）进行 simulate和评估。我们建立了销售机器人（SalesBot）和购物机器人（ShopperBot），这两个 LLM 动态代理人可以模拟框架两侧。人类研究比较了销售机器人和专业销售人员，发现销售机器人在流畅性和信息完整性方面几乎与专业人员相当，但在推荐质量方面存在明显的不足。我们强调了对话式推荐系统中 Ensure faithfulness 的挑战，并发布了我们的代码和所有数据。
</details></li>
</ul>
<hr>
<h2 id="Improving-Traffic-Density-Forecasting-in-Intelligent-Transportation-Systems-Using-Gated-Graph-Neural-Networks"><a href="#Improving-Traffic-Density-Forecasting-in-Intelligent-Transportation-Systems-Using-Gated-Graph-Neural-Networks" class="headerlink" title="Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks"></a>Improving Traffic Density Forecasting in Intelligent Transportation Systems Using Gated Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17729">http://arxiv.org/abs/2310.17729</a></li>
<li>repo_url: None</li>
<li>paper_authors: Razib Hayat Khan, Jonayet Miah, S M Yasir Arafat, M M Mahbubul Syeed, Duc M Ca</li>
<li>for: 本研究探讨了应用图 neural network 在交通预测方面，这是智能交通系统中重要的一环。准确的交通预测对于如旅行规划、交通控制和车辆路径规划等功能都是关键。</li>
<li>methods: 研究中探讨了三种常见的图 neural network 架构，即 Graph Convolutional Networks (Graph Sample and Aggregation)、Gated Graph Neural Networks。每种架构的方法都得到了详细的描述，包括层配置、活动函数和超参数。研究的目标是最小化预测错误，GGNNs  emerges as the most effective choice among the three models。</li>
<li>results: 研究结果显示，GCNs 的 RMSE 为 9.10 和 MAE 为 8.00，而 GraphSAGE 表现有所提高，其 RMSE 为 8.3 和 MAE 为 7.5。而 Gated Graph Neural Networks (GGNNs) 则在三种模型中表现最佳，其 RMSE 为 9.15 和 MAE 为 7.1。<details>
<summary>Abstract</summary>
This study delves into the application of graph neural networks in the realm of traffic forecasting, a crucial facet of intelligent transportation systems. Accurate traffic predictions are vital for functions like trip planning, traffic control, and vehicle routing in such systems. Three prominent GNN architectures Graph Convolutional Networks (Graph Sample and Aggregation) and Gated Graph Neural Networks are explored within the context of traffic prediction. Each architecture's methodology is thoroughly examined, including layer configurations, activation functions,and hyperparameters. The primary goal is to minimize prediction errors, with GGNNs emerging as the most effective choice among the three models. The research outlines outcomes for each architecture, elucidating their predictive performance through root mean squared error and mean absolute error (MAE). Hypothetical results reveal intriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows improvement with an RMSE of 8.3 and an MAE of 7.5. Gated Graph Neural Networks (GGNNs) exhibit the lowest RMSE at 9.15 and an impressive MAE of 7.1, positioning them as the frontrunner.
</details>
<details>
<summary>摘要</summary>
The study aims to minimize prediction errors, and the results show that GGNNs are the most effective among the three models. The outcomes for each architecture are presented in terms of root mean squared error (RMSE) and mean absolute error (MAE). The hypothetical results reveal that GCNs have an RMSE of 9.10 and an MAE of 8.00, while GraphSAGE shows improvement with an RMSE of 8.3 and an MAE of 7.5. GGNNs exhibit the lowest RMSE at 9.15 and an impressive MAE of 7.1, making them the frontrunner.The study's findings highlight the potential of GNNs in traffic forecasting and provide valuable insights into the strengths and limitations of different GNN architectures. The results suggest that GGNNs are a promising approach for accurate traffic prediction, and future research can build on these findings to improve the accuracy and efficiency of intelligent transportation systems.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-as-Generalizable-Policies-for-Embodied-Tasks"><a href="#Large-Language-Models-as-Generalizable-Policies-for-Embodied-Tasks" class="headerlink" title="Large Language Models as Generalizable Policies for Embodied Tasks"></a>Large Language Models as Generalizable Policies for Embodied Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17722">http://arxiv.org/abs/2310.17722</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, Devon Hjelm, Alexander Toshev</li>
<li>for: 这个论文的目的是使大型自然语言模型（LLM）可以被适应为涉及视觉任务的通用策略。</li>
<li>methods: 这种方法called Large LAnguage model Reinforcement Learning Policy（LLaRP），它使用预训练的冻结LLM来接受文本指令和视觉 egocentric 观察，并直接在环境中输出操作。通过强化学习，我们训练 LLaRP 通过环境交互来看和行动。</li>
<li>results: 我们的实验表明，LLaRP 能够通过复杂的重新排序指令来执行任务，并且可以在新任务中表现出新的优化行为。特别是，在 1,000 个未seen 任务中，LLaRP 的成功率为 42%，比其他常见的学习基线或零shot 应用的 LLM 高出 1.7 倍。此外，我们还发布了一个新的标准测试集，名为 Language Rearrangement，它包含 150,000 个训练任务和 1,000 个测试任务，以便研究语言条件、大量多任务、embodied AI 问题。视频示例可以在 <a target="_blank" rel="noopener" href="https://llm-rl.github.io/">https://llm-rl.github.io</a> 上找到。<details>
<summary>Abstract</summary>
We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.
</details>
<details>
<summary>摘要</summary>
我们显示大型语言模型（LLM）可以适应为普遍化政策 для具有身体视觉任务。我们的方法，叫做大型语言模型增强学习政策（LLaRP），将预训冻的LLM变数为接受文本指令和 Egocentric 视觉观察，并将其转换为环境中的动作。使用增强学习，我们训练 LLaRP 通过环境互动来看和行动。我们显示 LLARP 能够对复杂重新写法的任务指令进行抗衡，并且能够扩展到新的任务，需要新的优化行为。特别是，在 1,000 个未见任务中，它取得了 42% 的成功率，比其他常见的基eline或 zero-shot 应用的 LLM 高一倍。最后，为了帮助社区研究语言条件、大规模多任务、具有身体视觉 AI 问题，我们发布了一个新的对benchmark，语言重新排序，包括 150,000 个训练任务和 1,000 个测试任务 для语言条件的重新排序。影像示例 LLARP 在未见 Language Rearrangement 指令下的动作可以在 https://llm-rl.github.io 浏览。
</details></li>
</ul>
<hr>
<h2 id="From-Transcripts-to-Insights-Uncovering-Corporate-Risks-Using-Generative-AI"><a href="#From-Transcripts-to-Insights-Uncovering-Corporate-Risks-Using-Generative-AI" class="headerlink" title="From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI"></a>From Transcripts to Insights: Uncovering Corporate Risks Using Generative AI</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17721">http://arxiv.org/abs/2310.17721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Kim, Maximilian Muhn, Valeri Nikolaev</li>
<li>for: 该研究旨在探讨生成AI工具，如ChatGPT，如何帮助投资者发现公司风险。</li>
<li>methods: 研究人员开发了基于资料提供的会议记录文本的启发式AI模型，以生成公司风险报告和评估。</li>
<li>results: 研究发现，基于GPT 3.5模型的风险测量方法具有显著的信息内容，并且能够超越现有的风险测量方法在预测公司独特异常性和投资决策等方面。<details>
<summary>Abstract</summary>
We explore the value of generative AI tools, such as ChatGPT, in helping investors uncover dimensions of corporate risk. We develop and validate firm-level measures of risk exposure to political, climate, and AI-related risks. Using the GPT 3.5 model to generate risk summaries and assessments from the context provided by earnings call transcripts, we show that GPT-based measures possess significant information content and outperform the existing risk measures in predicting (abnormal) firm-level volatility and firms' choices such as investment and innovation. Importantly, information in risk assessments dominates that in risk summaries, establishing the value of general AI knowledge. We also find that generative AI is effective at detecting emerging risks, such as AI risk, which has soared in recent quarters. Our measures perform well both within and outside the GPT's training window and are priced in equity markets. Taken together, an AI-based approach to risk measurement provides useful insights to users of corporate disclosures at a low cost.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Outlier-Dimensions-Encode-Task-Specific-Knowledge"><a href="#Outlier-Dimensions-Encode-Task-Specific-Knowledge" class="headerlink" title="Outlier Dimensions Encode Task-Specific Knowledge"></a>Outlier Dimensions Encode Task-Specific Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17715">http://arxiv.org/abs/2310.17715</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wrudman/outlier_dimensions">https://github.com/wrudman/outlier_dimensions</a></li>
<li>paper_authors: William Rudman, Catherine Chen, Carsten Eickhoff</li>
<li>for: 这个论文的目的是研究大语言模型（LLM）表示的缺失特征dimension的影响。</li>
<li>methods: 这个论文使用了大语言模型的 fine-tuning 方法，以investigate how fine-tuning impacts outlier dimensions。</li>
<li>results: 这个论文发现了一些 Interesting results，包括：1) pre-training 中出现的异常维度在 fine-tuned 模型中仍然存在，2) 一个异常维度可以完成下游任务 WITH 较低的错误率。这些结果表明了异常维度可能会含有关键的任务特定知识，并且这些知识可能会影响下游模型的决策。<details>
<summary>Abstract</summary>
Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的表示被证明为具有极高方差的一小部分维度所控制。先前的研究表明，尽管剖除这些异常维度在LLM表示中减少下游性能，但异常维度对表征质量仍然有负面影响。在这种研究中，我们研究了如何微调影响异常维度，并发现以下两点：1）在预训练中出现的异常维度在微调后仍然存在于模型中，2）一个异常维度可以通过最小错误率完成下游任务。我们的结果表明，异常维度可能含有关键任务知识，并且表示中的一个异常维度的值会驱动下游模型决策。
</details></li>
</ul>
<hr>
<h2 id="A-Wireless-AI-Generated-Content-AIGC-Provisioning-Framework-Empowered-by-Semantic-Communication"><a href="#A-Wireless-AI-Generated-Content-AIGC-Provisioning-Framework-Empowered-by-Semantic-Communication" class="headerlink" title="A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication"></a>A Wireless AI-Generated Content (AIGC) Provisioning Framework Empowered by Semantic Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17705">http://arxiv.org/abs/2310.17705</a></li>
<li>repo_url: None</li>
<li>paper_authors: Runze Cheng, Yao Sun, Dusit Niyato, Lan Zhang, Lei Zhang, Muhammad Ali Imran</li>
<li>for: 提供高质量人工智能生成内容（AIGC）服务，使其可以通过无线通信网络进行 ubique 访问。</li>
<li>methods: 使用 semantics communication（SemCom）技术，只提取内容的 semantic 信息，而不是所有的 binary 位。同时，利用 diffusion-based 模型进行高效的内容生成和计算负荷的灵活调整。</li>
<li>results:  simulations 表明，提出的 SemAIGC 框架在延迟和内容质量方面比 conventional 方法更高效。<details>
<summary>Abstract</summary>
Generative AI applications are recently catering to a vast user base by creating diverse and high-quality AI-generated content (AIGC). With the proliferation of mobile devices and rapid growth of mobile traffic, providing ubiquitous access to high-quality AIGC services via wireless communication networks is becoming the future direction for AIGC products. However, it is challenging to provide optimal AIGC services in wireless networks with unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To tackle these challenges, we propose a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be extracted and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion-based models within the semantic encoder and decoder for efficient content generation and flexible adjustment of the computing workload of both transmitter and receiver. Meanwhile, we devise a resource-aware workload trade-off (ROOT) scheme into the SemAIGC framework to intelligently decide transmitter/receiver workload, thus adjusting the utilization of computational resource according to service requirements. Simulations verify the superiority of our proposed SemAIGC framework in terms of latency and content quality compared to conventional approaches.
</details>
<details>
<summary>摘要</summary>
<<SYS>>现代生成AI应用程序正在为广泛的用户群体提供多样化和高质量的AI生成内容（AIGC）服务。随着移动设备的普及和移动流量的快速增长，将高质量AIGC服务通过无线通信网络提供到用户的 ubique 访问已成为未来的发展方向。然而，在不稳定的通道、有限的带宽资源和分布式计算资源的情况下，提供优化的AIGC服务是一项挑战。为解决这些挑战，我们提议一种基于semantic communication（SemCom）的AI生成内容（AIGC）生成和传输框架，其中只需提取内容的semantic信息而不是所有的二进制位数据。具体来说，SemAIGC框架包括在semantic编码器和解码器中的扩散模型，以实现高效的内容生成和接收端计算资源的灵活调整。同时，我们在SemAIGC框架中实现了根据服务需求调整计算资源的资源意识的工作负荷调整（ROOT）策略。实验证明我们的提议的SemAIGC框架在延迟和内容质量方面与传统方法相比具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Defending-Against-Transfer-Attacks-From-Public-Models"><a href="#Defending-Against-Transfer-Attacks-From-Public-Models" class="headerlink" title="Defending Against Transfer Attacks From Public Models"></a>Defending Against Transfer Attacks From Public Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17645">http://arxiv.org/abs/2310.17645</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/wagner-group/pubdef">https://github.com/wagner-group/pubdef</a></li>
<li>paper_authors: Chawin Sitawarin, Jaewon Chang, David Huang, Wesson Altoyan, David Wagner</li>
<li>for: 这篇论文主要是为了提出一种实际的攻击模型，以及基于游戏理论的防御方法，以应对未来安全敏感应用中的攻击。</li>
<li>methods: 这篇论文使用了将攻击者通过公共可用的副本模型进行转移攻击，并提出了一种基于游戏理论的特殊防御方法。</li>
<li>results: 对于3个数据集（CIFAR-10、CIFAR-100和ImageNet）和24个公共模型以及11种攻击算法，我们的防御方法PubDef在攻击下表现出了明显的优势，与白盒 adversarial training相比，几乎没有失去正常准确率。例如，在ImageNet上，我们的防御方法在最强的转移攻击下达到了62%的准确率，而最佳的白盒 adversarial training只达到了36%。而在不受攻击的情况下，我们的防御方法的准确率只比无防御模型低2%（78%vs80%）。<details>
<summary>Abstract</summary>
Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this paper, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. The defenses are evaluated under 24 public models and 11 attack algorithms across three datasets (CIFAR-10, CIFAR-100, and ImageNet). Under this threat model, our defense, PubDef, outperforms the state-of-the-art white-box adversarial training by a large margin with almost no loss in the normal accuracy. For instance, on ImageNet, our defense achieves 62% accuracy under the strongest transfer attack vs only 36% of the best adversarially trained model. Its accuracy when not under attack is only 2% lower than that of an undefended model (78% vs 80%). We release our code at https://github.com/wagner-group/pubdef.
</details>
<details>
<summary>摘要</summary>
针对抗击攻击已成为industry中的潜在威胁，然而过去一代robustness评估文献中的经验教我们，在攻击者有Machine Learning和领域专业知识的情况下，构建强大或最佳的攻击是困难的。因此，过去大多数文献中所假设的白盒威胁模型是不切实际的。在这篇论文中，我们提出了一种新的实用威胁模型，在这个模型中，攻击者通过公共可用的副本模型进行转移攻击。我们认为这将在安全敏感应用中成为未来的主要威胁模型。我们对这个设定中的转移攻击进行评估，并提出了基于游戏理论的防御方法。我们对这些防御策略进行了24个公共模型和11种攻击算法的评估，并在CIFAR-10、CIFAR-100和ImageNet三个数据集上进行了评估。根据这个威胁模型，我们的防御策略PubDef在对抗攻击下的性能大幅超越了现有的白盒针对攻击训练方法，而且与正常准确率几乎没有差异。例如，在ImageNet上，我们的防御策略在最强的转移攻击下达到62%的准确率，而最佳针对攻击训练方法只达到36%。其正常准确率与没有防御的情况下的准确率几乎没有差异（78% vs 80%）。我们将代码发布在GitHub上，可以通过https://github.com/wagner-group/pubdef访问。
</details></li>
</ul>
<hr>
<h2 id="In-Context-Learning-Dynamics-with-Random-Binary-Sequences"><a href="#In-Context-Learning-Dynamics-with-Random-Binary-Sequences" class="headerlink" title="In-Context Learning Dynamics with Random Binary Sequences"></a>In-Context Learning Dynamics with Random Binary Sequences</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17639">http://arxiv.org/abs/2310.17639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Tomer D. Ullman</li>
<li>for: The paper aims to improve our understanding of the complex, emergent capabilities of large language models (LLMs) and their in-context learning dynamics.</li>
<li>methods: The authors propose a Cognitive Interpretability framework that involves using random binary sequences as context to study the dynamics of in-context learning in LLMs. They manipulate properties of the context data, such as sequence length, to observe the behavior of the models.</li>
<li>results: The authors find that the latest GPT-3.5+ models exhibit emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics that transition sharply from pseudo-random behaviors to deterministic repetition.Here’s the Chinese translation of the three information points:</li>
<li>for: 该文章目的是更好地理解大语言模型（LLMs）的复杂、萌发性能力以及其在上下文学习动态。</li>
<li>methods: 作者们提出了一种认知可读性框架，使用随机二进制序列作为上下文来研究LLMs中的上下文学习动态。他们在context数据的属性上进行了修改，以观察模型的行为。</li>
<li>results: 作者们发现最新的GPT-3.5+模型在上下文学习过程中展现出了萌发性能力，可以生成 Pseudo-Random 数字和学习基本的正式语言，并且上下文学习动态具有强烈的转变性，从 Pseudo-Random 行为转sharply 到决定性重复。<details>
<summary>Abstract</summary>
Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Grow-Your-Limits-Continuous-Improvement-with-Real-World-RL-for-Robotic-Locomotion"><a href="#Grow-Your-Limits-Continuous-Improvement-with-Real-World-RL-for-Robotic-Locomotion" class="headerlink" title="Grow Your Limits: Continuous Improvement with Real-World RL for Robotic Locomotion"></a>Grow Your Limits: Continuous Improvement with Real-World RL for Robotic Locomotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17634">http://arxiv.org/abs/2310.17634</a></li>
<li>repo_url: None</li>
<li>paper_authors: Laura Smith, Yunhao Cao, Sergey Levine</li>
<li>for: 实现机器人自动获得复杂行为，如四肢行走。</li>
<li>methods: 使用policy regularization框架，调整机器人在训练过程中的探索。</li>
<li>results: 实现了在真实世界中几分钟内完全学习四肢行走，并继续训练后能够更好地适应不同的情况和动力学变化。<details>
<summary>Abstract</summary>
Deep reinforcement learning (RL) can enable robots to autonomously acquire complex behaviors, such as legged locomotion. However, RL in the real world is complicated by constraints on efficiency, safety, and overall training stability, which limits its practical applicability. We present APRL, a policy regularization framework that modulates the robot's exploration over the course of training, striking a balance between flexible improvement potential and focused, efficient exploration. APRL enables a quadrupedal robot to efficiently learn to walk entirely in the real world within minutes and continue to improve with more training where prior work saturates in performance. We demonstrate that continued training with APRL results in a policy that is substantially more capable of navigating challenging situations and is able to adapt to changes in dynamics with continued training.
</details>
<details>
<summary>摘要</summary>
深度强化学习（RL）可以让机器人自动获得复杂的行为，如四肢行走。但在实际世界中，RL受到效率、安全性和总训练稳定性的限制，这限制了其实际应用性。我们提出了APRL，一种策略Regularization框架，可以在训练过程中调整机器人的探索行为，实现在训练过程中均衡 flexible improvement potential和专注、效率的探索。APRL使得一只四肢机器人在实际世界中快速地学习行走，并继续增强，其性能比之前的工作更高。我们示出，继续训练APRL后，机器人可以更好地处理复杂的情况，并能够适应动力学变化。
</details></li>
</ul>
<hr>
<h2 id="JudgeLM-Fine-tuned-Large-Language-Models-are-Scalable-Judges"><a href="#JudgeLM-Fine-tuned-Large-Language-Models-are-Scalable-Judges" class="headerlink" title="JudgeLM: Fine-tuned Large Language Models are Scalable Judges"></a>JudgeLM: Fine-tuned Large Language Models are Scalable Judges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17631">http://arxiv.org/abs/2310.17631</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/baaivision/judgelm">https://github.com/baaivision/judgelm</a></li>
<li>paper_authors: Lianghui Zhu, Xinggang Wang, Xinlong Wang<br>for:The paper aims to evaluate large language models (LLMs) in open-ended scenarios by fine-tuning them as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively.methods:The paper proposes a comprehensive dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. The authors train JudgeLM at different scales and analyze its capabilities and behaviors.results:JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and the proposed new benchmark. The JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs, and achieves high agreement with the teacher judge (agreement exceeding 90%, even surpassing human-to-human agreement). JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.<details>
<summary>Abstract</summary>
Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.
</details>
<details>
<summary>摘要</summary>
评估大语言模型（LLM）在开放场景中是有挑战的，因为现有的标准准则和指标不能全面评估它们。为解决这问题，我们提议 fine-tune LLM 为可扩展的判官（JudgeLM），以有效地评估 LLM 在开放场景中。我们首先提出了一个全面、大规模、高质量的数据集，包括任务种子、LLM 生成的答案、GPT-4 生成的判断，以便 Fine-tune 高性能的判官。我们在不同的批处参数（7B、13B、33B）上进行了系统性的分析和评估。我们then analyzed the key biases in fine-tuning LLM as a judge and considered them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient, and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.
</details></li>
</ul>
<hr>
<h2 id="Using-State-of-the-Art-Speech-Models-to-Evaluate-Oral-Reading-Fluency-in-Ghana"><a href="#Using-State-of-the-Art-Speech-Models-to-Evaluate-Oral-Reading-Fluency-in-Ghana" class="headerlink" title="Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in Ghana"></a>Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in Ghana</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17606">http://arxiv.org/abs/2310.17606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Henkel, Hannah Horne-Robinson, Libby Hills, Bill Roberts, Joshua McGrane</li>
<li>for: 这项研究旨在使用大规模语音模型对学生在加纳的口语读写能力进行自动评估。</li>
<li>methods: 研究使用最新版本的大规模语音模型（Whisper V2 wav2vec2.0）对学生的口语读写能力进行评估。</li>
<li>results: 研究发现，使用Whisper V2生成的学生口语读写 транскриптов的Word Error Rate为13.5，与成人语音识别模型的平均WER（12.8）相似，而且与人工评分员生成的分数高度相关（相关系数为0.96）。此外，研究还发现这些 транскриптов可以用于生成自动化的 ORF 分数，并且在 repre sentative 数据集上达到了高度相关性（相关系数为0.96）。<details>
<summary>Abstract</summary>
This paper reports on a set of three recent experiments utilizing large-scale speech models to evaluate the oral reading fluency (ORF) of students in Ghana. While ORF is a well-established measure of foundational literacy, assessing it typically requires one-on-one sessions between a student and a trained evaluator, a process that is time-consuming and costly. Automating the evaluation of ORF could support better literacy instruction, particularly in education contexts where formative assessment is uncommon due to large class sizes and limited resources. To our knowledge, this research is among the first to examine the use of the most recent versions of large-scale speech models (Whisper V2 wav2vec2.0) for ORF assessment in the Global South.   We find that Whisper V2 produces transcriptions of Ghanaian students reading aloud with a Word Error Rate of 13.5. This is close to the model's average WER on adult speech (12.8) and would have been considered state-of-the-art for children's speech transcription only a few years ago. We also find that when these transcriptions are used to produce fully automated ORF scores, they closely align with scores generated by expert human graders, with a correlation coefficient of 0.96. Importantly, these results were achieved on a representative dataset (i.e., students with regional accents, recordings taken in actual classrooms), using a free and publicly available speech model out of the box (i.e., no fine-tuning). This suggests that using large-scale speech models to assess ORF may be feasible to implement and scale in lower-resource, linguistically diverse educational contexts.
</details>
<details>
<summary>摘要</summary>
The results show that Whisper V2 produces transcriptions of Ghanaian students reading aloud with a Word Error Rate (WER) of 13.5, which is close to the model's average WER on adult speech (12.8) and would have been considered state-of-the-art for children's speech transcription only a few years ago. Furthermore, the transcriptions were used to produce fully automated ORF scores, which closely aligned with scores generated by expert human graders, with a correlation coefficient of 0.96.The results were achieved on a representative dataset, including students with regional accents and recordings taken in actual classrooms, using a free and publicly available speech model out of the box (i.e., no fine-tuning). This suggests that using large-scale speech models to assess ORF may be feasible to implement and scale in lower-resource, linguistically diverse educational contexts.
</details></li>
</ul>
<hr>
<h2 id="MimicGen-A-Data-Generation-System-for-Scalable-Robot-Learning-using-Human-Demonstrations"><a href="#MimicGen-A-Data-Generation-System-for-Scalable-Robot-Learning-using-Human-Demonstrations" class="headerlink" title="MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations"></a>MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17596">http://arxiv.org/abs/2310.17596</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox</li>
<li>for: 这个论文主要目标是提出一种自动生成大规模、 ricah datasets 的方法，以便通过模仿学习来训练 робот代理。</li>
<li>methods: 这个系统使用了一种基于人工示例的自动生成方法，可以从少量的人工示例中生成大量的示例，并将其适应到新的上下文中。</li>
<li>results: 通过使用这个系统，研究人员可以生成大量的示例，并训练 robot 代理以达到强大的性能，包括多部件组装和咖啡制作等高精度任务，并且在不同的初始状态分布下表现出色。<details>
<summary>Abstract</summary>
Imitation learning from a large set of human demonstrations has proved to be an effective paradigm for building capable robot agents. However, the demonstrations can be extremely costly and time-consuming to collect. We introduce MimicGen, a system for automatically synthesizing large-scale, rich datasets from only a small number of human demonstrations by adapting them to new contexts. We use MimicGen to generate over 50K demonstrations across 18 tasks with diverse scene configurations, object instances, and robot arms from just ~200 human demonstrations. We show that robot agents can be effectively trained on this generated dataset by imitation learning to achieve strong performance in long-horizon and high-precision tasks, such as multi-part assembly and coffee preparation, across broad initial state distributions. We further demonstrate that the effectiveness and utility of MimicGen data compare favorably to collecting additional human demonstrations, making it a powerful and economical approach towards scaling up robot learning. Datasets, simulation environments, videos, and more at https://mimicgen.github.io .
</details>
<details>
<summary>摘要</summary>
人工智能控制机器人的努力学习从人类示例集中获得了成功。然而，收集示例集可以非常昂贵和耗时。我们介绍MimicGen，一个系统可以自动生成大规模、丰富的数据集，只需要一小部分的人类示例。我们使用MimicGen生成了18种任务中的超过50,000个示例，包括多个物品配置、物品实例和机器人臂。我们表明，通过对这些生成的数据集进行依据学习，可以让机器人在长期和高精度任务中表现出色，例如多部件组装和咖啡制作。此外，我们还证明了MimicGen数据的有效性和实用性，与收集更多的人类示例相比，它是一种强大和经济的方法来扩大机器人学习。更多信息请访问https://mimicgen.github.io。
</details></li>
</ul>
<hr>
<h2 id="SPA-A-Graph-Spectral-Alignment-Perspective-for-Domain-Adaptation"><a href="#SPA-A-Graph-Spectral-Alignment-Perspective-for-Domain-Adaptation" class="headerlink" title="SPA: A Graph Spectral Alignment Perspective for Domain Adaptation"></a>SPA: A Graph Spectral Alignment Perspective for Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17594">http://arxiv.org/abs/2310.17594</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhiqing Xiao, Haobo Wang, Ying Jin, Lei Feng, Gang Chen, Fei Huang, Junbo Zhao</li>
<li>for: 这个研究旨在解决域类别预测中的领域对预测模型的不足，使用无监督领域适应（Unsupervised Domain Adaptation，UDA）来将内部预测模型扩展到不同的目标领域，并且考虑到这些领域之间的数据分布不同。</li>
<li>methods: 这个方法基于图 primitives，将DA问题转换为图间的对称问题，并且使用一个新的spectral regularizer来将领域图在特征空间进行调整。此外，还开发了一个细节化的讯息传递模组，以提高目标领域中的推断能力。</li>
<li>results: 在标准的 benchmark 上，SPA 的实验结果表明其性能已经超过了现有的剪切前渠道DA方法。另外，透过对称分析，我们发现SPA 的方法具有较好的有效性、韧性、推断能力和传递能力。资料和代码可以在<a target="_blank" rel="noopener" href="https://github.com/CrownX/SPA">https://github.com/CrownX/SPA</a> 上获取。<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to extend the in-domain model to the distinctive target domains where the data distributions differ. Most prior works focus on capturing the inter-domain transferability but largely overlook rich intra-domain structures, which empirically results in even worse discriminability. In this work, we introduce a novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The core of our method is briefly condensed as follows: (i)-by casting the DA problem to graph primitives, SPA composes a coarse graph alignment mechanism with a novel spectral regularizer towards aligning the domain graphs in eigenspaces; (ii)-we further develop a fine-grained message propagation module -- upon a novel neighbor-aware self-training mechanism -- in order for enhanced discriminability in the target domain. On standardized benchmarks, the extensive experiments of SPA demonstrate that its performance has surpassed the existing cutting-edge DA methods. Coupled with dense model analysis, we conclude that our approach indeed possesses superior efficacy, robustness, discriminability, and transferability. Code and data are available at: https://github.com/CrownX/SPA.
</details>
<details>
<summary>摘要</summary>
无监督领域适应（USDA）是机器学习中的一种重要形式，用于将内域模型扩展到不同的目标领域，其数据分布不同。大多数前期工作强调了交叉领域的传送性，但它们忽略了内域结构的丰富性，这会导致实际效果更差。在这种情况下，我们介绍了一种新的图spectral alignment（SPA）框架，用于解决这个负担。SPA的核心思想如下：（i）通过将DA问题转化为图 primitives，SPA使用一种含有新的 spectral regularizer 来将领域图在特征空间进行对齐；（ii）我们进一步发展了一种细化的消息传递模块，基于一种新的邻居自动训练机制，以提高目标领域的分类能力。在标准化的测试上，我们进行了广泛的实验，结果表明，SPA的性能已经超过了现有的cutting-edge DA方法。同时，我们还进行了密集的模型分析，得出了我们的方法实际上具有更好的效果、更好的稳定性、更好的分类能力和更好的传送性。代码和数据可以从以下地址获取：https://github.com/CrownX/SPA。
</details></li>
</ul>
<hr>
<h2 id="An-Open-Source-Data-Contamination-Report-for-Llama-Series-Models"><a href="#An-Open-Source-Data-Contamination-Report-for-Llama-Series-Models" class="headerlink" title="An Open Source Data Contamination Report for Llama Series Models"></a>An Open Source Data Contamination Report for Llama Series Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17589">http://arxiv.org/abs/2310.17589</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/liyucheng09/contamination_detector">https://github.com/liyucheng09/contamination_detector</a></li>
<li>paper_authors: Yucheng Li</li>
<li>for: 这篇论文旨在提供一种开源的数据污染报告，用于评估LLama系列模型的可靠性。</li>
<li>methods: 该论文使用了多种方法进行数据污染分析，包括对六个多选问答 benchmark进行分析，并计算这些benchmark中的重叠率。</li>
<li>results: 研究发现，这些benchmark中存在1%到8.7%的数据污染程度，而LLama模型在污染subset上的准确率高于清晰subset上的准确率超过5%. 数据和代码可以在<a target="_blank" rel="noopener" href="https://github.com/liyucheng09/Contamination_Detector%E4%B8%AD%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/liyucheng09/Contamination_Detector中获得。</a><details>
<summary>Abstract</summary>
Data contamination in language model evaluation is increasingly prevalent as the popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has became an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by LLM developers and often lacks transparency and completeness. This paper present an open source data contamination reports for the Llama series models. We analyse six popular multi-choice QA benchmarks and quantify their overlapping with the training set of Llama. Various levels of contamination ranging from 1\% to 8.7\% are found across benchmarks. Our comparison also reveals that Llama models can gain over 5\% higher accuracy on contaminated subsets versus clean subsets. Data and code are available at: https://github.com/liyucheng09/Contamination_Detector.
</details>
<details>
<summary>摘要</summary>
<<SYS>>文本环境污染在语言模型评估中日益普遍，这是由大型语言模型的普及所导致。这种污染允许模型通过记忆而不是展示真正的能力“偷懒”。因此，污染分析已成为可靠模型评估的重要组成部分。然而，现有的污染分析通常由LLM开发者进行内部实施，lacks transparency和完整性。本文介绍了一个开源的数据污染报告 для LLama 系列模型。我们分析了 six 个受欢迎的多选问答 bencmarks，并衡量它们与 LLama 训练集的重叠。我们发现，这些 bencmarks 中的污染水平从 1% 到 8.7% 不等。我们的比较还表明，LLama 模型在污染subset 上可以获得高于 5% 的准确率。数据和代码可以在 GitHub 上获取：https://github.com/liyucheng09/Contamination_Detector。Note: "LLama" refers to a series of language models, and "LLM" stands for "large language model".
</details></li>
</ul>
<hr>
<h2 id="Can-LLMs-Grade-Short-answer-Reading-Comprehension-Questions-Foundational-Literacy-Assessment-in-LMICs"><a href="#Can-LLMs-Grade-Short-answer-Reading-Comprehension-Questions-Foundational-Literacy-Assessment-in-LMICs" class="headerlink" title="Can LLMs Grade Short-answer Reading Comprehension Questions : Foundational Literacy Assessment in LMICs"></a>Can LLMs Grade Short-answer Reading Comprehension Questions : Foundational Literacy Assessment in LMICs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18373">http://arxiv.org/abs/2310.18373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Owen Henkel, Libby Hills, Bill Roberts, Joshua McGrane</li>
<li>for: 这项研究用于评估大语言模型（GPT-4）在评估短回答阅读理解问题上的可靠性。</li>
<li>methods: 研究使用了多种配置的生成式大语言模型（LLMs）来评估来自新数据集的学生答案，该数据集由150名学生在加纳完成的阅读测验中收集。</li>
<li>results: GPT-4在评估新数据集上表现出色，其 quadratic weighted kappa 值为0.923，F1值为0.88，大大超越了基于传输学习的方法。此外，GPT-4还能够与人类评分员相匹配。这项研究表明，生成式LLMs可能用于可靠地评估基础的阅读理解能力。<details>
<summary>Abstract</summary>
This paper presents emerging evidence of using generative large language models (i.e., GPT-4) to reliably evaluate short-answer reading comprehension questions. Specifically, we explore how various configurations of generative (LLMs) are able to evaluate student responses from a new dataset, drawn from a battery of reading assessments conducted with over 150 students in Ghana. As this dataset is novel and hence not used in training runs of GPT, it offers an opportunity to test for domain shift and evaluate the generalizability of generative LLMs, which are predominantly designed and trained on data from high-income North American countries. We found that GPT-4, with minimal prompt engineering performed extremely well on evaluating the novel dataset (Quadratic Weighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning based approaches, and even exceeding expert human raters (Quadratic Weighted Kappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to empirically evaluate the performance of generative LLMs on short-answer reading comprehension questions, using real student data, and suggests that generative LLMs have the potential to reliably evaluate foundational literacy. Currently the assessment of formative literacy and numeracy is infrequent in many low and middle-income countries (LMICs) due to the cost and operational complexities of conducting them at scale. Automating the grading process for reading assessment could enable wider usage, and in turn improve decision-making regarding curricula, school management, and teaching practice at the classroom level. Importantly, in contrast transfer learning based approaches, generative LLMs generalize well and the technical barriers to their use are low, making them more feasible to implement and scale in lower resource educational contexts.
</details>
<details>
<summary>摘要</summary>
The results show that GPT-4, with minimal prompt engineering, performed extremely well on evaluating the novel dataset, outperforming transfer-learning based approaches and even exceeding expert human raters. This is the first study to empirically evaluate the performance of generative LLMs on short-answer reading comprehension questions using real student data, and suggests that these models have the potential to reliably evaluate foundational literacy.Currently, the assessment of formative literacy and numeracy is infrequent in many low and middle-income countries (LMICs) due to the cost and operational complexities of conducting them at scale. Automating the grading process for reading assessment could enable wider usage and improve decision-making regarding curricula, school management, and teaching practice at the classroom level. Additionally, generative LLMs generalize well and have low technical barriers to implementation, making them more feasible to use in lower resource educational contexts.
</details></li>
</ul>
<hr>
<h2 id="Skill-Mix-a-Flexible-and-Expandable-Family-of-Evaluations-for-AI-models"><a href="#Skill-Mix-a-Flexible-and-Expandable-Family-of-Evaluations-for-AI-models" class="headerlink" title="Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models"></a>Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17567">http://arxiv.org/abs/2310.17567</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, Sanjeev Arora</li>
<li>for: 这个论文旨在探讨 LLM 评价方法如何随着 LLM 从统计语言模型转化为通用 AI 代理而变化。</li>
<li>methods: 本论文引入了一种新的评价方法，称为 Skill-Mix，用于评估 LLM 的能力集合和组合能力。评价方法包括 randomly 选择 $k$ 个技能，并要求 LLM 生成组合这些技能的文本。</li>
<li>results: 研究人员通过对两个流行的 chatbot 进行评价，发现存在较大的差异在不同模型之间，这些差异不会被捕捉到在现有的 LLM 排名板。此外，研究人员发现 GPT-4 在 $k&#x3D;5$ 时表现良好，这可能指示它在组合技能方面具有更高的能力。<details>
<summary>Abstract</summary>
With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora & Goyal, 2023).   This work introduces Skill-Mix, a new evaluation to measure ability to combine skills. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model.   Administering a version of to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards ("cramming for the leaderboard"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on $k=5$ is suggestive of going beyond "stochastic parrot" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training.   We sketch how the methodology can lead to a Skill-Mix based eco-system of open evaluations for AI capabilities of future models.
</details>
<details>
<summary>摘要</summary>
With LLMs 从统计语言模型转移到通用 AI 代理，如何改变 LLM 评估呢？可以 argue 一个关键的 AI 代理能力是随时结合基本技能。人类教学中也有这种能力，以及 Arora 和 Goyal 的论文（2023）中所描述的 emergence 现象。  这个工作介绍了 Skill-Mix，一种新的评估方法，用于测试 LLM 的技能结合能力。评估者会随机选择 $k$ 个技能，并要求 LLM 生成结合这些技能的文本。由于可能的组合方式的数量如 $N^k$，即使使用 modest 的 $k$，这种评估方法仍然可以要求 LLM 生成未在训练集中出现的文本。文章还提供了一种方法来设计和进行这种评估，以及自动评分和人工检查的方法。  对两个流行的 chatbot 进行了评估，结果与预期一致，但也有一些意外结果。不同的模型能力存在显著差异，这些差异不会被捕捉在 popular LLM 排名板。此外，简单的概率计算表明，GPT-4 在 $k=5$ 时的表现是指示它可能超过 "随机习语" 行为（Bender et al., 2021），即它可以结合技能，而它没有在训练过程中看到这些技能。  文章略 outline 如何使用 Skill-Mix 基础环境来建立未来模型的 AI 能力评估系统。
</details></li>
</ul>
<hr>
<h2 id="Bifurcations-and-loss-jumps-in-RNN-training"><a href="#Bifurcations-and-loss-jumps-in-RNN-training" class="headerlink" title="Bifurcations and loss jumps in RNN training"></a>Bifurcations and loss jumps in RNN training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17561">http://arxiv.org/abs/2310.17561</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/durstewitzlab/scyfi">https://github.com/durstewitzlab/scyfi</a></li>
<li>paper_authors: Lukas Eisenmann, Zahra Monfared, Niclas Alexander Göring, Daniel Durstewitz</li>
<li>for: 这个论文的目的是使用权重链网络（RNN）模型和预测时间序列数据，以及推导出动态系统（DS）的各种计算和动态性质。</li>
<li>methods: 这个论文使用了权重链网络（RNN）和权重链网络的训练过程，以及动态系统（DS）理论中的概念，来更好地理解训练过程和模型的计算和动态性质。</li>
<li>results: 这个论文提出了一种新的规则搜索算法，可以快速和精确地找到权重链网络（RNN）中的固定点和循环点，以及它们的存在和稳定区域。这种算法可以帮助分析训练过程中的某些突变，并且可以推导出训练过程中的某些性质。<details>
<summary>Abstract</summary>
Recurrent neural networks (RNNs) are popular machine learning tools for modeling and forecasting sequential data and for inferring dynamical systems (DS) from observed time series. Concepts from DS theory (DST) have variously been used to further our understanding of both, how trained RNNs solve complex tasks, and the training process itself. Bifurcations are particularly important phenomena in DS, including RNNs, that refer to topological (qualitative) changes in a system's dynamical behavior as one or more of its parameters are varied. Knowing the bifurcation structure of an RNN will thus allow to deduce many of its computational and dynamical properties, like its sensitivity to parameter variations or its behavior during training. In particular, bifurcations may account for sudden loss jumps observed in RNN training that could severely impede the training process. Here we first mathematically prove for a particular class of ReLU-based RNNs that certain bifurcations are indeed associated with loss gradients tending toward infinity or zero. We then introduce a novel heuristic algorithm for detecting all fixed points and k-cycles in ReLU-based RNNs and their existence and stability regions, hence bifurcation manifolds in parameter space. In contrast to previous numerical algorithms for finding fixed points and common continuation methods, our algorithm provides exact results and returns fixed points and cycles up to high orders with surprisingly good scaling behavior. We exemplify the algorithm on the analysis of the training process of RNNs, and find that the recently introduced technique of generalized teacher forcing completely avoids certain types of bifurcations in training. Thus, besides facilitating the DST analysis of trained RNNs, our algorithm provides a powerful instrument for analyzing the training process itself.
</details>
<details>
<summary>摘要</summary>
循环神经网络（RNN）是人工智能中流行的模型和预测序列数据的工具，以及从观察时间序列中推导动力系统（DS）的学习方法。DS理论（DST）的概念在RNN中都有各种应用，以深化我们对RNN解决复杂任务的理解和训练过程。变分是RNN中重要的特点之一，它指的是在参数变化时系统的动力学行为发生了 topological（Qualitative）变化。了解RNN的变分结构，可以推导出它的计算和动力学性质，例如参数变化的敏感度和训练过程中的行为。尤其是变分可以解释RNN训练过程中的突然损失峰值，这可能会对训练进程产生严重的阻碍。在这篇文章中，我们首先 математиче地证明了一种特定的ReLU基于RNN中的变分与损失勋度很大或者很小之间的关系。然后，我们提出了一种新的启发式算法，可以在ReLU基于RNN中找到所有的固定点和k-循环，以及它们在参数空间的存在和稳定区域。与之前的数值算法和常见继续方法不同，我们的算法提供了精确的结果，可以在高阶度上找到固定点和循环，并且具有惊人的扩展性。我们在RNN训练过程的分析中应用了这种算法，并发现了一些通过通用教师填充（Generalized Teacher Forcing）完全避免某些类型的变分的训练技术。因此，除了促进DST分析已训练的RNN之外，我们的算法还提供了一种可以分析训练过程本身的强大工具。
</details></li>
</ul>
<hr>
<h2 id="Instability-of-computer-vision-models-is-a-necessary-result-of-the-task-itself"><a href="#Instability-of-computer-vision-models-is-a-necessary-result-of-the-task-itself" class="headerlink" title="Instability of computer vision models is a necessary result of the task itself"></a>Instability of computer vision models is a necessary result of the task itself</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17559">http://arxiv.org/abs/2310.17559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Oliver Turnbull, George Cevora</li>
<li>for: 这篇论文探讨了计算机视觉模型中的敌对示例问题，强调了这些问题的潜在危害性，以及如何通过分析问题的本质和数据的稳定性来部分缓解这些问题。</li>
<li>methods: 本论文使用了数据的对称性（翻译不变性）、分类任务的分类性和图像本身的基本不同来探讨计算机视觉模型的不稳定性。</li>
<li>results: 研究发现，由于数据的对称性、分类任务的分类性和图像本身的基本不同，计算机视觉模型必然存在不稳定性。此外，由于训练数据的不彻底标注，这种不稳定性可能会更加严重。但是，通过提高图像的分辨率、提供图像上下文信息、彻底标注训练数据和防止攻击者频繁访问计算机视觉系统，可以部分缓解这种不稳定性。<details>
<summary>Abstract</summary>
Adversarial examples resulting from instability of current computer vision models are an extremely important topic due to their potential to compromise any application. In this paper we demonstrate that instability is inevitable due to a) symmetries (translational invariance) of the data, b) the categorical nature of the classification task, and c) the fundamental discrepancy of classifying images as objects themselves. The issue is further exacerbated by non-exhaustive labelling of the training data. Therefore we conclude that instability is a necessary result of how the problem of computer vision is currently formulated. While the problem cannot be eliminated, through the analysis of the causes, we have arrived at ways how it can be partially alleviated. These include i) increasing the resolution of images, ii) providing contextual information for the image, iii) exhaustive labelling of training data, and iv) preventing attackers from frequent access to the computer vision system.
</details>
<details>
<summary>摘要</summary>
“对今天的计算机视觉模型而言，敌对示例是一个非常重要的话题，因为它们有可能会破坏任何应用程序。在这篇论文中，我们示出了这种不稳定性是不可避免的，原因包括：a) 数据中的对称性（平移不变性），b) 分类任务的分类性质，以及c) 图像本身的基本不同。这个问题被加剧了由于训练数据的非完整标注。因此，我们得出结论是，不稳定性是计算机视觉问题的一个必然的结果。虽然这个问题无法完全消除，但通过分析其原因，我们到达了一些可以减轻这个问题的方法，包括：i) 提高图像的分辨率，ii) 为图像提供上下文信息，iii) 对训练数据进行完整的标注，以及iv) 防止攻击者对计算机视觉系统进行频繁访问。”
</details></li>
</ul>
<hr>
<h2 id="Interactive-Robot-Learning-from-Verbal-Correction"><a href="#Interactive-Robot-Learning-from-Verbal-Correction" class="headerlink" title="Interactive Robot Learning from Verbal Correction"></a>Interactive Robot Learning from Verbal Correction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17555">http://arxiv.org/abs/2310.17555</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihan Liu, Alice Chen, Yuke Zhu, Adith Swaminathan, Andrey Kolobov, Ching-An Cheng</li>
<li>for: 本研究旨在帮助机器人学习并优化其行为在不结构化环境中，使其能够在日常生活中更好地协助人类。</li>
<li>methods: 该研究使用大型自然语言模型（LLM）OLAF，让日常用户通过语音纠正机器人的错误行为来教育机器人。OLAF可以根据语音反馈更新机器人的视听动作神经策略，以避免将来重复错误。</li>
<li>results: 在实验中，用户通过OLAF教育机器人完成长期 manipulate任务，成功率提高了20.0%。详细结果和视频可以在<a target="_blank" rel="noopener" href="https://ut-austin-rpl.github.io/olaf/%E4%B8%AD%E6%89%BE%E5%88%B0%E3%80%82">https://ut-austin-rpl.github.io/olaf/中找到。</a><details>
<summary>Abstract</summary>
The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying "Stop what you're doing. You should move closer to the cup." A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at https://ut-austin-rpl.github.io/olaf/
</details>
<details>
<summary>摘要</summary>
“在设计 robots 操作在无结构环境中时，学习和改进行为的能力已经日益重要。在这个工作中，我们设计了一个基于大型自然语言模型（LLM）的新学习系统，名为 OLAF，允许日常用户通过语音纠正，当 robot 错误时，例如“停止你的动作，你应该靠近碗子”。 OLAF 的一个关键特点是能够根据语音反馈更新 robot 的视觉动作神经策略，以避免未来重复错误。这与现有的 LLM-based robotic 系统不同，只会跟随语音命令或纠正，而不会从中学习。我们在实验中证明了我们的设计，可以让用户教育 robot 进行长期搬运任务，并在模拟和物理硬件上实现了平均 20.0% 的政策成功率。详细信息和视频可以在 <https://ut-austin-rpl.github.io/olaf/> 获取。”
</details></li>
</ul>
<hr>
<h2 id="Model-Based-Runtime-Monitoring-with-Interactive-Imitation-Learning"><a href="#Model-Based-Runtime-Monitoring-with-Interactive-Imitation-Learning" class="headerlink" title="Model-Based Runtime Monitoring with Interactive Imitation Learning"></a>Model-Based Runtime Monitoring with Interactive Imitation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17552">http://arxiv.org/abs/2310.17552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Huihan Liu, Shivin Dass, Roberto Martín-Martín, Yuke Zhu</li>
<li>for: 这个研究旨在将Robot学习方法升级为可靠和可靠的高度任务。</li>
<li>methods: 这个研究使用互动学习和监控方法，将人工智能和机器人联合作为一体，以提高机器人的性能和可靠性。</li>
<li>results: 这个研究比基准方法高出23%和40%在模拟和物理硬件上的成功率。<details>
<summary>Abstract</summary>
Robot learning methods have recently made great strides, but generalization and robustness challenges still hinder their widespread deployment. Failing to detect and address potential failures renders state-of-the-art learning systems not combat-ready for high-stakes tasks. Recent advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. Nonetheless, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains. This work aims to endow a robot with the ability to monitor and detect errors during task execution. We introduce a model-based runtime monitoring algorithm that learns from deployment data to detect system anomalies and anticipate failures. Unlike prior work that cannot foresee future failures or requires failure experiences for training, our method learns a latent-space dynamics model and a failure classifier, enabling our method to simulate future action outcomes and detect out-of-distribution and high-risk states preemptively. We train our method within an interactive imitation learning framework, where it continually updates the model from the experiences of the human-robot team collected using trustworthy deployments. Consequently, our method reduces the human workload needed over time while ensuring reliable task execution. Our method outperforms the baselines across system-level and unit-test metrics, with 23% and 40% higher success rates in simulation and on physical hardware, respectively. More information at https://ut-austin-rpl.github.io/sirius-runtime-monitor/
</details>
<details>
<summary>摘要</summary>
现代机器人学习方法在最近几年内已经做出了很大的进步，但是总结和稳定性问题仍然限制它们的普及。如果不能检测和解决潜在的失败，那么当前最先进的学习系统将不能在高度任务中进行实战。 recient advances in interactive imitation learning have presented a promising framework for human-robot teaming, enabling the robots to operate safely and continually improve their performances over long-term deployments. However, existing methods typically require constant human supervision and preemptive feedback, limiting their practicality in realistic domains.本研究的目的是赋予机器人能力监控和检测任务执行过程中的错误。我们介绍了一种基于模型的运行时监控算法，可以从部署数据中学习检测系统异常和预测失败。与先前的方法不同，我们的方法不需要失败经验进行训练，而是通过学习latent空间动力学模型和失败分类器，可以预测未来动作结果和检测出现在分布中的高风险和异常状态。我们在人机团队优先级的交互式模仿学习框架中训练了我们的方法，从人机团队收集的经验中不断更新模型。因此，我们的方法可以逐渐减少人工劳动量，并同时确保任务执行的可靠性。我们的方法在系统级别和单元测试指标上比基eline高出23%和40%，在实际硬件上也达到了相同的水平。更多信息请参考https://ut-austin-rpl.github.io/sirius-runtime-monitor/。
</details></li>
</ul>
<hr>
<h2 id="Unpacking-the-Ethical-Value-Alignment-in-Big-Models"><a href="#Unpacking-the-Ethical-Value-Alignment-in-Big-Models" class="headerlink" title="Unpacking the Ethical Value Alignment in Big Models"></a>Unpacking the Ethical Value Alignment in Big Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17551">http://arxiv.org/abs/2310.17551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie</li>
<li>for: 本文旨在探讨大型模型在社会中的风险和挑战，以及现有的人工智能伦理准则是如何应对这些风险的。</li>
<li>methods: 本文对现有的AI伦理准则进行了检视，并分析了大型模型的伦理含义和挑战。此外，本文还 investigate了当前主流的语言模型（LLMs）的道德倾向，分析了现有的对齐算法，并提出了一种新的概念框架以确定大型模型的道德价值观。</li>
<li>results: 本文提出了一种新的概念框架，以帮助建立一个统一的AI伦理框架，并提出了一些有优先的研究方向以确定大型模型的道德价值观。<details>
<summary>Abstract</summary>
Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned AI   This paper is a modified English version of our Chinese paper https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended to help non-Chinese native speakers better understand our work.
</details>
<details>
<summary>摘要</summary>
大型模型已经大幅提高了人工智能的理解、生成和 manipulate信息和内容能力，这些能力已经开拓了许多应用程序。然而，随着这些模型在日常生活中的普及，它们的内置优先级和可能的偏见带来了未料的风险。本文提供了大型模型所存在的风险和挑战，检视了现有的人工智能伦理准则，并分析了这些模型的伦理含义。从normative伦理角度出发，我们提出了重新评估最近的normative准则，强调在学术界共同努力建立一个统一的人工智能伦理框架。此外，我们使用道尔文基础理论分析当今主流的大型语言模型（LLMs）中的道德倾向，分析现有的对Alignment算法，并详细介绍了对道德价值的整合在这些模型中所遇到的挑战。为解决这些挑战，我们提出了一种新的概念 paradigm，并讨论了一些有前途的研究方向，代表了初步的interdisciplinary构建人工智能的道德框架。Please note that the translation is done using a machine translation tool, and may not be perfect. Additionally, some cultural references or idioms may not be accurately translated.
</details></li>
</ul>
<hr>
<h2 id="Human-Guided-Complexity-Controlled-Abstractions"><a href="#Human-Guided-Complexity-Controlled-Abstractions" class="headerlink" title="Human-Guided Complexity-Controlled Abstractions"></a>Human-Guided Complexity-Controlled Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17550">http://arxiv.org/abs/2310.17550</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mycal-tucker/human-guided-abstractions">https://github.com/mycal-tucker/human-guided-abstractions</a></li>
<li>paper_authors: Andi Peng, Mycal Tucker, Eoin Kenny, Noga Zaslavsky, Pulkit Agrawal, Julie Shah</li>
<li>for: 这个论文旨在探讨神经网络如何学习任务特定的幂等表示，以及如何使其泛化到新的任务和设定。</li>
<li>methods: 作者使用了生成谱分布来训练神经网络生成一谱分布，并控制表示复杂性（即输入编码的比特数）通过调整分布 entropy。</li>
<li>results: 在 fine-tuning 实验中，使用只有一小数量的标注数据，发现（1）调整表示复杂性到任务适应的水平支持最高的 fine-tuning 性能，以及（2）在人类参与者研究中，用户能够通过视觉化表示的方式确定下游任务适应的复杂性水平。<details>
<summary>Abstract</summary>
Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g., "bird" vs. "sparrow") and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising direction for rapid model finetuning by leveraging human insight.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>根据任务适应性调整表示复杂性水平支持最高的finetuning性能。2. 在人类参与者研究中，用户能通过视觉化表示的幻数表示来识别适当的复杂性水平。我们的结果表明，可以通过人类意见来快速训练模型，并且有批处性。</details></li>
</ol>
<hr>
<h2 id="Neuro-Inspired-Fragmentation-and-Recall-to-Overcome-Catastrophic-Forgetting-in-Curiosity"><a href="#Neuro-Inspired-Fragmentation-and-Recall-to-Overcome-Catastrophic-Forgetting-in-Curiosity" class="headerlink" title="Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity"></a>Neuro-Inspired Fragmentation and Recall to Overcome Catastrophic Forgetting in Curiosity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17537">http://arxiv.org/abs/2310.17537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/fietelab/farcuriosity">https://github.com/fietelab/farcuriosity</a></li>
<li>paper_authors: Jaedong Hwang, Zhang-Wei Hong, Eric Chen, Akhilan Boopathy, Pulkit Agrawal, Ila Fiete</li>
<li>for: 解决难度探索任务中的快速忘记问题</li>
<li>methods: 使用Fragmentation and Recall Curiosity方法，通过在不同 Fragment 上使用不同的本地好奇模块来减少忘记</li>
<li>results: 在Atari benchmark suite of tasks 上的游戏环境中，实现了 less forgetting 和 better overall performance<details>
<summary>Abstract</summary>
Deep reinforcement learning methods exhibit impressive performance on a range of tasks but still struggle on hard exploration tasks in large environments with sparse rewards. To address this, intrinsic rewards can be generated using forward model prediction errors that decrease as the environment becomes known, and incentivize an agent to explore novel states. While prediction-based intrinsic rewards can help agents solve hard exploration tasks, they can suffer from catastrophic forgetting and actually increase at visited states. We first examine the conditions and causes of catastrophic forgetting in grid world environments. We then propose a new method FARCuriosity, inspired by how humans and animals learn. The method depends on fragmentation and recall: an agent fragments an environment based on surprisal, and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment so that modules are not trained on the entire environment. At each fragmentation event, the agent stores the current module in long-term memory (LTM) and either initializes a new module or recalls a previously stored module based on its match with the current state. With fragmentation and recall, FARCuriosity achieves less forgetting and better overall performance in games with varied and heterogeneous environments in the Atari benchmark suite of tasks. Thus, this work highlights the problem of catastrophic forgetting in prediction-based curiosity methods and proposes a solution.
</details>
<details>
<summary>摘要</summary>
In this work, we examine the conditions and causes of catastrophic forgetting in grid world environments and propose a new method called FARCuriosity, inspired by how humans and animals learn. FARCuriosity depends on fragmentation and recall, where the agent fragments the environment based on surprisal and uses different local curiosity modules (prediction-based intrinsic reward functions) for each fragment. At each fragmentation event, the agent stores the current module in long-term memory (LTM) and either initializes a new module or recalls a previously stored module based on its match with the current state.With fragmentation and recall, FARCuriosity achieves less forgetting and better overall performance in games with varied and heterogeneous environments in the Atari benchmark suite of tasks. This work highlights the problem of catastrophic forgetting in prediction-based curiosity methods and proposes a solution.
</details></li>
</ul>
<hr>
<h2 id="SoK-Pitfalls-in-Evaluating-Black-Box-Attacks"><a href="#SoK-Pitfalls-in-Evaluating-Black-Box-Attacks" class="headerlink" title="SoK: Pitfalls in Evaluating Black-Box Attacks"></a>SoK: Pitfalls in Evaluating Black-Box Attacks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17534">http://arxiv.org/abs/2310.17534</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamgroot42/blackboxsok">https://github.com/iamgroot42/blackboxsok</a></li>
<li>paper_authors: Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David Evans<br>for:* This paper is written to systematize knowledge in the area of black-box attacks on image classifiers, and to provide a taxonomy for understanding the threat space of these attacks.methods:* The paper uses a taxonomy to organize the threat space of black-box attacks on image classifiers, and to identify under-explored threat spaces that require further research.* The paper also demonstrates the importance of considering the quality and quantity of auxiliary data available to the attacker, as well as the access of interactive queries, in understanding the threat model of different attacks.results:* The paper establishes a new state-of-the-art in the less-studied setting of access to top-k confidence scores, and shows how this setting can be challenging even for well-explored techniques.* The paper also overturns prior state-of-the-art claims in the setting of interactive query access, and highlights the need for more research in this area.* The paper reveals connections between black-box attacks and related areas, such as model inversion and extraction attacks, and discusses how advances in these areas can enable stronger black-box attacks.I hope this helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, highlighting the need for more research. 2) Identification the threat model of different attacks uncovers stronger baselines that challenge prior state-of-the-art claims. We demonstrate this by enhancing an initially weaker baseline (under interactive query access) via surrogate models, effectively overturning claims in the respective paper. 3) Our taxonomy reveals interactions between attacker knowledge that connect well to related areas, such as model inversion and extraction attacks. We discuss how advances in other areas can enable potentially stronger black-box attacks. Finally, we emphasize the need for a more realistic assessment of attack success by factoring in local attack runtime. This approach reveals the potential for certain attacks to achieve notably higher success rates and the need to evaluate attacks in diverse and harder settings, highlighting the need for better selection criteria.
</details>
<details>
<summary>摘要</summary>
多种研究报告了黑盒攻击图像分类器。然而，这些研究假设了不同的攻击者知识和当前文献缺乏一个协调中心，因此我们提出了一个分类器，以攻击者的攻击模型为中心，并将攻击者的知识分为三个轴：回归精度、交互查询访问权限和攻击者可用的辅助数据质量和量。我们的新分类器提供了三个关键发现：1.  DESPITE 详细的文献研究，还有许多未经探索的攻击空间，这些空间无法通过适应已经explored的设置来解决。我们通过在访问top-k信任分数的设置下Establishing a new state-of-the-art，并证明这些设置仍然不够 restrictive， highlighting the need for more research。2.  Identifying the threat model of different attacks reveals stronger baselines that challenge prior state-of-the-art claims. We demonstrate this by enhancing an initially weaker baseline (under interactive query access) via surrogate models, effectively overturning claims in the respective paper.3.  Our taxonomy reveals interactions between attacker knowledge that connect well to related areas, such as model inversion and extraction attacks. We discuss how advances in other areas can enable potentially stronger black-box attacks. Finally, we emphasize the need for a more realistic assessment of attack success by factoring in local attack runtime. This approach reveals the potential for certain attacks to achieve notably higher success rates and the need to evaluate attacks in diverse and harder settings, highlighting the need for better selection criteria.
</details></li>
</ul>
<hr>
<h2 id="Can-large-language-models-replace-humans-in-the-systematic-review-process-Evaluating-GPT-4’s-efficacy-in-screening-and-extracting-data-from-peer-reviewed-and-grey-literature-in-multiple-languages"><a href="#Can-large-language-models-replace-humans-in-the-systematic-review-process-Evaluating-GPT-4’s-efficacy-in-screening-and-extracting-data-from-peer-reviewed-and-grey-literature-in-multiple-languages" class="headerlink" title="Can large language models replace humans in the systematic review process? Evaluating GPT-4’s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages"></a>Can large language models replace humans in the systematic review process? Evaluating GPT-4’s efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17526">http://arxiv.org/abs/2310.17526</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qusai Khraisha, Sophie Put, Johanna Kappenberg, Azza Warraitch, Kristin Hadfield</li>
<li>for: 这个论文旨在评估大型自然语言模型（LLM）在系统性审查中的性能，以及LLM在不同类型的文献和语言中的表现。</li>
<li>methods: 该论文采用了一种’人出现’-based的方法，通过对LLM进行训练和测试，以评估其在标题&#x2F;摘要层次层级和全文审查中的表现。</li>
<li>results: 研究发现，LLM在大多数任务中的准确率与人类表现相当，但是结果受到了机会协议和数据不均衡的影响。经过调整后，LLM在数据抽取任务中表现 moderate，但是在不同阶段和语言类型中的层次层级层级表现呈现不均匀。使用高可靠性的提示时，LLM在全文审查任务中的表现接近完美。<details>
<summary>Abstract</summary>
Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts - screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more. Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.
</details>
<details>
<summary>摘要</summary>
While GPT-4 had accuracy on par with human performance in most tasks, the results were affected by chance agreement and dataset imbalance. After adjusting for these factors, GPT-4's performance was moderate for data extraction, but its screening performance was low for different stages and languages. However, when screening full-text literature using highly reliable prompts, GPT-4's performance was almost perfect. Penalizing GPT-4 for missing key studies using highly reliable prompts further improved its performance.Our findings suggest that, while LLMs have the potential to automate systematic reviews, they should be used with caution, and their performance should be carefully evaluated. However, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.
</details></li>
</ul>
<hr>
<h2 id="The-Expressive-Power-of-Low-Rank-Adaptation"><a href="#The-Expressive-Power-of-Low-Rank-Adaptation" class="headerlink" title="The Expressive Power of Low-Rank Adaptation"></a>The Expressive Power of Low-Rank Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17513">http://arxiv.org/abs/2310.17513</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/uw-madison-lee-lab/expressive_power_of_lora">https://github.com/uw-madison-lee-lab/expressive_power_of_lora</a></li>
<li>paper_authors: Yuchen Zeng, Kangwook Lee</li>
<li>for: This paper aims to theoretically analyze the expressive power of Low-Rank Adaptation (LoRA) for fine-tuning pre-trained models, specifically large language models and diffusion models.</li>
<li>methods: The paper uses theoretical analysis to prove the expressive power of LoRA for fully connected neural networks and Transformer networks. The authors show that LoRA can adapt any model to accurately represent any smaller target model with a certain rank threshold.</li>
<li>results: The paper proves that, for fully connected neural networks, LoRA can adapt any model to accurately represent any smaller target model if LoRA-rank is greater than or equal to the product of the width of the model and the depth of the target model, divided by the depth of the model. For Transformer networks, the authors show that any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}{2})$ LoRA adapters. The paper also quantifies the approximation error when LoRA-rank is lower than the threshold.<details>
<summary>Abstract</summary>
Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model $f$ to accurately represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of }\overline{f}{\text{depth of }f}$. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-$(\frac{\text{embedding size}{2})$ LoRA adapters.
</details>
<details>
<summary>摘要</summary>
低级别适应（LoRA），一种精炼批处理方法，通过低级别适应weight矩阵，实现了许多预训练模型的精炼。尽管LoRA在实践中取得了很大成功，但其理论基础尚未得到充分探索。这篇论文是研究LoRA的第一步，我们提供了LoRA的表达力理论分析。我们证明，对于完全连接神经网络，LoRA可以使任何模型$f$ accurately表示任何更小的target模型$\overline{f}$，如果LoRA-rank $\geq(\text{宽度of }f) \times \frac{\text{深度of }\overline{f}{\text{深度of }f}$。我们还量化了当LoRA-rank低于阈值时的讹差。对于Transformer网络，我们显示任何模型可以通过rank-$(\frac{\text{嵌入大小}{2})$ LoRA adapter来适应同样大小的target模型。
</details></li>
</ul>
<hr>
<h2 id="CompeteAI-Understanding-the-Competition-Behaviors-in-Large-Language-Model-based-Agents"><a href="#CompeteAI-Understanding-the-Competition-Behaviors-in-Large-Language-Model-based-Agents" class="headerlink" title="CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents"></a>CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17512">http://arxiv.org/abs/2310.17512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie</li>
<li>for: 这篇论文探讨了基于大语言模型（LLM）的代理在竞争中的行为。</li>
<li>methods: 作者提出了一个通用的竞争框架来研究代理之间的竞争。然后，他们使用GPT-4实现了一个假设的虚拟小镇，并在其中让两种代理进行竞争：餐厅代理和客户代理。餐厅代理之间竞争，以吸引更多的顾客，这种竞争使得餐厅代理受到激发，如培养新的运营策略。</li>
<li>results: 实验结果显示了一些有趣的发现，包括社会学学习和玛提效应，这些发现与社会学和经济学的现有理论很好吻合。作者认为，代理之间的竞争值得进一步研究，以更好地理解社会。代码将很快发布。<details>
<summary>Abstract</summary>
Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and economic theories. We believe that competition between agents deserves further investigation to help us understand society better. The code will be released soon.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已广泛应用于不同任务的代理人，如个人助手或活动规划。然而，大多数工作都集中在合作和协作之间，很少探讨竞争，这也是社会和经济发展的重要机制。在这篇论文中，我们想要研究LLM基于代理人的竞争行为。我们首先提出一个通用的框架来研究代理人之间的竞争。然后，我们使用GPT-4实现一个实际竞争环境，模拟一个虚拟小镇，有两种代理人：餐厅代理人和客户代理人。具体来说，餐厅代理人之间竞争，以吸引更多的客户，这种竞争使得他们变得更加创新，如培养新的运营策略。我们的实验结果显示了一些有趣的发现，包括社会学习到马特效应，这与社会学和经济学的现有理论吻合得非常好。我们认为代理人之间的竞争值得进一步调查，以更好地理解社会。代码将很快发布。
</details></li>
</ul>
<hr>
<h2 id="Orchestration-of-Emulator-Assisted-Mobile-Edge-Tuning-for-AI-Foundation-Models-A-Multi-Agent-Deep-Reinforcement-Learning-Approach"><a href="#Orchestration-of-Emulator-Assisted-Mobile-Edge-Tuning-for-AI-Foundation-Models-A-Multi-Agent-Deep-Reinforcement-Learning-Approach" class="headerlink" title="Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach"></a>Orchestration of Emulator Assisted Mobile Edge Tuning for AI Foundation Models: A Multi-Agent Deep Reinforcement Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17492">http://arxiv.org/abs/2310.17492</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenhan Yu, Terence Jie Chua, Jun Zhao</li>
<li>for: 本研究旨在提高当地任务性能，通过Mobile Edge Computing（MEC）与基础模型集成，以提高用户设备（UE）的本地任务性能。</li>
<li>methods: 我们提出了一种创新的Emulator-Adapter架构，将基础模型分成两个协同模块，以保持计算资源并提高下游任务的适应性和微调效率。此外，我们还提出了一种适应度较高的资源分配机制，以适应Emulator-Adapter结构在分散环境中的需求。</li>
<li>results: 我们通过实验和验证表明，我们的方法可以减少计算资源消耗，同时提高下游任务的性能和扩展性。这种方法在实际应用中具有强大的实用性和扩展性。<details>
<summary>Abstract</summary>
The efficient deployment and fine-tuning of foundation models are pivotal in contemporary artificial intelligence. In this study, we present a groundbreaking paradigm integrating Mobile Edge Computing (MEC) with foundation models, specifically designed to enhance local task performance on user equipment (UE). Central to our approach is the innovative Emulator-Adapter architecture, segmenting the foundation model into two cohesive modules. This design not only conserves computational resources but also ensures adaptability and fine-tuning efficiency for downstream tasks. Additionally, we introduce an advanced resource allocation mechanism that is fine-tuned to the needs of the Emulator-Adapter structure in decentralized settings. To address the challenges presented by this system, we employ a hybrid multi-agent Deep Reinforcement Learning (DRL) strategy, adept at handling mixed discrete-continuous action spaces, ensuring dynamic and optimal resource allocations. Our comprehensive simulations and validations underscore the practical viability of our approach, demonstrating its robustness, efficiency, and scalability. Collectively, this work offers a fresh perspective on deploying foundation models and balancing computational efficiency with task proficiency.
</details>
<details>
<summary>摘要</summary>
当代人工智能中的有效部署和细化调整是关键。在本研究中，我们提出了一种创新的模型整合Mobile Edge Computing（MEC）和基础模型，特意设计用于增强用户设备（UE）上本地任务性能。我们的方法的核心是分解基础模型为两个协同模块的Emulator-Adapter架构，不仅保留计算资源，而且确保适应性和调整效率。此外，我们还提出了一种适应Emulator-Adapter结构的高级资源分配机制，在分布式环境中进行微调。为解决这个系统中的挑战，我们采用了一种混合多代理 Deep Reinforcement Learning（DRL）策略，能够处理混合数字-连续动作空间， Ensure dynamic and optimal resource allocations。我们的全面的 simulations和验证表明了我们的方法的实用性和可扩展性。总之，这项工作提供了一种新的基础模型部署和计算效率协调的新视角。
</details></li>
</ul>
<hr>
<h2 id="Improving-Zero-shot-Reader-by-Reducing-Distractions-from-Irrelevant-Documents-in-Open-Domain-Question-Answering"><a href="#Improving-Zero-shot-Reader-by-Reducing-Distractions-from-Irrelevant-Documents-in-Open-Domain-Question-Answering" class="headerlink" title="Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering"></a>Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17490">http://arxiv.org/abs/2310.17490</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sukmin Cho, Jeong yeon Seo, Soyeong Jeong, Jong C. Park</li>
<li>for: 这篇论文旨在探讨Zero-shot Question Answering（ODQA）中的语言模型（LLMs），以及其在开放领域中的应用。</li>
<li>methods: 本研究使用了Distracted-aware Answer Selection（DAS）技术，将不相关的文档除去，以提高Zero-shotReader的性能。</li>
<li>results: 实验结果显示，DAS技术能够成功地抑制干扰，提高Zero-shotReader的性能，并且与supervised reader不同，Zero-shot reader能够在未见到数据的情况下实现卓越的转移性。<details>
<summary>Abstract</summary>
Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）允许零条件方法在开放领域问题回答（ODQA）中，但有限的进步，因为读者与搜寻器之间的比较有限。本研究探讨了零条件读者的可能性，并实现了适当的选择方法以减少干扰。我们发现，LLMs 受到无关文档的干扰，并且在应用为零条件读者时，产生了过度自信的答案。为了解决这些问题，我们使用了对抗干扰的选择技术（DAS），并调整得分以确保适当的答案选择。实验结果显示，我们的方法可以成功地减少干扰，并提高零条件读者的表现。此外，不同于需要训练的监督读者，零条件读者具有卓越的转移性，无需任何训练。
</details></li>
</ul>
<hr>
<h2 id="Bias-in-Evaluation-Processes-An-Optimization-Based-Model"><a href="#Bias-in-Evaluation-Processes-An-Optimization-Based-Model" class="headerlink" title="Bias in Evaluation Processes: An Optimization-Based Model"></a>Bias in Evaluation Processes: An Optimization-Based Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17489">http://arxiv.org/abs/2310.17489</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/anaymehrotra/bias-in-evaluation-processes">https://github.com/anaymehrotra/bias-in-evaluation-processes</a></li>
<li>paper_authors: L. Elisa Celis, Amit Kumar, Anay Mehrotra, Nisheeth K. Vishnoi</li>
<li>for: 这个论文主要研究了评估过程中受到个人社会特征的偏见的现象，包括录用和招聘等设置。</li>
<li>methods: 该论文使用了一种解决具有信息约束的损失最小化问题来模型评估过程中的偏见。该模型具有两个参数，它们是资源信息费用Parameter和风险偏好Parameter。</li>
<li>results: 该论文通过分析模型生成的分布，研究了这两个参数对观察分布的影响。此外，该论文还验证了模型的实际应用，并使用其来研究在下游选择任务中的干预效果。这些结果为偏见在评估过程中的发生提供了更深刻的理解，并提供了用于mitigate偏见的工具。<details>
<summary>Abstract</summary>
Biases with respect to socially-salient attributes of individuals have been well documented in evaluation processes used in settings such as admissions and hiring. We view such an evaluation process as a transformation of a distribution of the true utility of an individual for a task to an observed distribution and model it as a solution to a loss minimization problem subject to an information constraint. Our model has two parameters that have been identified as factors leading to biases: the resource-information trade-off parameter in the information constraint and the risk-averseness parameter in the loss function. We characterize the distributions that arise from our model and study the effect of the parameters on the observed distribution. The outputs of our model enrich the class of distributions that can be used to capture variation across groups in the observed evaluations. We empirically validate our model by fitting real-world datasets and use it to study the effect of interventions in a downstream selection task. These results contribute to an understanding of the emergence of bias in evaluation processes and provide tools to guide the deployment of interventions to mitigate biases.
</details>
<details>
<summary>摘要</summary>
社会背景下个体特征偏见在评估过程中得到了广泛的报道。我们视这种评估过程为一种将真实个体能力的分布转化为观察分布，并模型为一种损失最小化问题下的信息约束问题。我们的模型具有两个参数，这两个参数被证明导致偏见：资源信息费用参数在信息约束中，以及风险偏好参数在损失函数中。我们描述出的分布可以用来捕捉不同群体在观察评估中的变化。我们采用实际数据进行验证，并用其来研究在下游选择任务中的干预效果。这些结果对偏见的出现和控制偏见的措施提供了深入的理解和实用的工具。
</details></li>
</ul>
<hr>
<h2 id="Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-using-the-Physical-Laws-of-Motion"><a href="#Towards-Learning-Monocular-3D-Object-Localization-From-2D-Labels-using-the-Physical-Laws-of-Motion" class="headerlink" title="Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion"></a>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17462">http://arxiv.org/abs/2310.17462</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel Kienzle, Julian Lorenz, Katja Ludwig, Rainer Lienhart</li>
<li>for: 精确的3D物体定位在单个图像中</li>
<li>methods: 使用2D标签和物体运动的物理知识进行训练，不需要Expensive 3D标签</li>
<li>results: 在实验中，实现了平均距离错误只有6 cm，表明方法具有实现3D物体定位估计的潜在能力，而不需要收集3D数据进行训练。<details>
<summary>Abstract</summary>
We present a novel method for precise 3D object localization in single images from a single calibrated camera using only 2D labels. No expensive 3D labels are needed. Thus, instead of using 3D labels, our model is trained with easy-to-annotate 2D labels along with the physical knowledge of the object's motion. Given this information, the model can infer the latent third dimension, even though it has never seen this information during training. Our method is evaluated on both synthetic and real-world datasets, and we are able to achieve a mean distance error of just 6 cm in our experiments on real data. The results indicate the method's potential as a step towards learning 3D object location estimation, where collecting 3D data for training is not feasible.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，可以准确地在单张图像中 lokalisieren 3D 对象，只使用单个满足的相机和2D 标签。没有需要高价的3D标签。因此，我们的模型在受训练时不使用3D标签，而是使用容易标注的2D标签以及物体运动的物理知识。给定这些信息，模型可以推断缺失的第三维度信息，即使在训练时没有看到这些信息。我们的方法在实验中达到了6cm的平均误差，这表明该方法在无法收集3D数据的情况下可能成为3D对象位置估计的一个重要步骤。
</details></li>
</ul>
<hr>
<h2 id="Generating-by-Understanding-Neural-Visual-Generation-with-Logical-Symbol-Groundings"><a href="#Generating-by-Understanding-Neural-Visual-Generation-with-Logical-Symbol-Groundings" class="headerlink" title="Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings"></a>Generating by Understanding: Neural Visual Generation with Logical Symbol Groundings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17451">http://arxiv.org/abs/2310.17451</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifei Peng, Yu Jin, Zhexu Luo, Yao-Xiang Ding, Wang-Zhou Dai, Zhong Ren, Kun Zhou</li>
<li>for:  integrate neural visual generative models with strong symbolic knowledge reasoning systems</li>
<li>methods: 使用abductive learning框架、量化决策法、对决 Meta-abduction法</li>
<li>results: 比基eline要少的实例级标签信息、能够学习数据中的逻辑生成规则<details>
<summary>Abstract</summary>
Despite the great success of neural visual generative models in recent years, integrating them with strong symbolic knowledge reasoning systems remains a challenging task. The main challenges are two-fold: one is symbol assignment, i.e. bonding latent factors of neural visual generators with meaningful symbols from knowledge reasoning systems. Another is rule learning, i.e. learning new rules, which govern the generative process of the data, to augment the knowledge reasoning systems. To deal with these symbol grounding problems, we propose a neural-symbolic learning approach, Abductive Visual Generation (AbdGen), for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, the quantized abduction method is introduced for generating abduction proposals by the nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, the contrastive meta-abduction method is proposed to eliminate wrong rules with positive cases and avoid less-informative rules with negative cases simultaneously. Experimental results on various benchmark datasets show that compared to the baselines, AbdGen requires significantly fewer instance-level labeling information for symbol assignment. Furthermore, our approach can effectively learn underlying logical generative rules from data, which is out of the capability of existing approaches.
</details>
<details>
<summary>摘要</summary>
尽管 neural visual generative models 在过去几年取得了很大的成功，但将它们与强大的符号知识推理系统集成仍然是一项挑战。主要的挑战有两个方面：一是符号分配，即将 neural visual generators 的幂谱因子绑定到有意义的符号 FROM knowledge reasoning systems。另一个是规则学习，即学习新的规则，这些规则 governs 数据生成过程。 To address these symbol grounding problems, we propose a neural-symbolic learning approach, AbdGen, for integrating logic programming systems with neural visual generative models based on the abductive learning framework. To achieve reliable and efficient symbol assignment, we introduce the quantized abduction method, which generates abduction proposals by nearest-neighbor lookups within semantic codebooks. To achieve precise rule learning, we propose the contrastive meta-abduction method to eliminate wrong rules with positive cases and avoid less-informative rules with negative cases simultaneously. Experimental results on various benchmark datasets show that compared to the baselines, AbdGen requires significantly fewer instance-level labeling information for symbol assignment. Furthermore, our approach can effectively learn underlying logical generative rules from data, which is beyond the capability of existing approaches.
</details></li>
</ul>
<hr>
<h2 id="LSA64-An-Argentinian-Sign-Language-Dataset"><a href="#LSA64-An-Argentinian-Sign-Language-Dataset" class="headerlink" title="LSA64: An Argentinian Sign Language Dataset"></a>LSA64: An Argentinian Sign Language Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17429">http://arxiv.org/abs/2310.17429</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Ronchetti, Facundo Manuel Quiroga, César Estrebou, Laura Lanzarini, Alejandro Rosete</li>
<li>for: 本研究旨在提供一个以阿根廷手语为基础的手语识别 dataset，以便进行机器学习或其他研究。</li>
<li>methods: 本研究使用了10名参与者的3200个手语视频，并将其分为64个不同的手语类型。参与者穿着了颜色的手套，以便追踪和分类手部运动。</li>
<li>results: 本研究提供了一个名为LSA64的手语识别dataset，包含3200个手语视频，并 Computed statistics of movement, position和手型。这个dataset可以作为未来的机器学习或其他研究使用。<details>
<summary>Abstract</summary>
Automatic sign language recognition is a research area that encompasses human-computer interaction, computer vision and machine learning. Robust automatic recognition of sign language could assist in the translation process and the integration of hearing-impaired people, as well as the teaching of sign language to the hearing population. Sign languages differ significantly in different countries and even regions, and their syntax and semantics are different as well from those of written languages. While the techniques for automatic sign language recognition are mostly the same for different languages, training a recognition system for a new language requires having an entire dataset for that language. This paper presents a dataset of 64 signs from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains 3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first step towards building a comprehensive research-level dataset of Argentinian signs, specifically tailored to sign language recognition or other machine learning tasks. The subjects that performed the signs wore colored gloves to ease the hand tracking and segmentation steps, allowing experiments on the dataset to focus specifically on the recognition of signs. We also present a pre-processed version of the dataset, from which we computed statistics of movement, position and handshape of the signs.
</details>
<details>
<summary>摘要</summary>
自动手语识别是一个人机交互、计算机视觉和机器学习研究领域。可靠自动识别手语可以帮助翻译过程和听力障碍人群的集成，以及教育听力人群学习手语。不同国家和地区的手语之间存在很大差异，其语法和 semantics 也与written languages不同。虽然自动手语识别技术大多相同，但为新语言训练recognition系统需要拥有整个语言的数据集。本文介绍了一个名为LSA64的数据集，包括64种阿根廷手语（LSA）的视频记录，共3200个视频，由10名参与者执行。这是建立 comprehensive 研究级数据集的第一步，特地适用于手语识别或其他机器学习任务。参与者在执行手语时穿着颜色的手套，以便轻松地跟踪和分割手部，从而使实验中能够专注于手语识别。我们还提供了对数据集进行了预处理，从而计算了手语的运动、位置和形状的统计数据。
</details></li>
</ul>
<hr>
<h2 id="Handshape-recognition-for-Argentinian-Sign-Language-using-ProbSom"><a href="#Handshape-recognition-for-Argentinian-Sign-Language-using-ProbSom" class="headerlink" title="Handshape recognition for Argentinian Sign Language using ProbSom"></a>Handshape recognition for Argentinian Sign Language using ProbSom</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17427">http://arxiv.org/abs/2310.17427</a></li>
<li>repo_url: None</li>
<li>paper_authors: Franco Ronchetti, Facundo Manuel Quiroga, César Estrebou, Laura Lanzarini</li>
<li>for: 这篇论文主要针对的是自动手语识别技术，以帮助听力障碍人士参与社会通信。</li>
<li>methods: 该论文提出了两大贡献：首先，建立了一个大量的阿根廷手语（LSA）手势数据库，这是一个未曾被充分研究的领域。其次，提出了一种基于自适应映射的图像处理技术，并对其进行了比较与当前状态艺术中的支持向量机器（SVM）、随机森林和神经网络等方法。</li>
<li>results: 该论文的实验结果显示，使用提出的特征点和ProbSom核算法可以实现手势识别精度高于90%。<details>
<summary>Abstract</summary>
Automatic sign language recognition is an important topic within the areas of human-computer interaction and machine learning. On the one hand, it poses a complex challenge that requires the intervention of various knowledge areas, such as video processing, image processing, intelligent systems and linguistics. On the other hand, robust recognition of sign language could assist in the translation process and the integration of hearing-impaired people.   This paper offers two main contributions: first, the creation of a database of handshapes for the Argentinian Sign Language (LSA), which is a topic that has barely been discussed so far. Secondly, a technique for image processing, descriptor extraction and subsequent handshape classification using a supervised adaptation of self-organizing maps that is called ProbSom. This technique is compared to others in the state of the art, such as Support Vector Machines (SVM), Random Forests, and Neural Networks.   The database that was built contains 800 images with 16 LSA handshapes, and is a first step towards building a comprehensive database of Argentinian signs. The ProbSom-based neural classifier, using the proposed descriptor, achieved an accuracy rate above 90%.
</details>
<details>
<summary>摘要</summary>
自动手语识别是人工智能和人机交互领域中的一个重要话题。一方面，它需要多种知识领域的干预，如视频处理、图像处理、智能系统和语言学。另一方面，可靠地识别手语可以帮助翻译过程和听力障碍人士的 интеграción。本文提供了两个主要贡献：首先，建立了阿根廷手语（LSA）的手势数据库，这是一个尚未得到广泛讨论的话题。其次，提出了一种基于自组织地图的图像处理技术，即ProbSom，用于手势特征提取和分类。这种技术与现有的状态对比较技术，如支持向量机（SVM）、Random Forests和神经网络，进行比较。建立的数据库包含16种LSA手势的800个图像，是建立全面的阿根廷手语数据库的第一步。ProbSom基于的神经分类器，使用提出的特征，达到了90%以上的准确率。
</details></li>
</ul>
<hr>
<h2 id="Distribution-of-Action-Movements-DAM-A-Descriptor-for-Human-Action-Recognition"><a href="#Distribution-of-Action-Movements-DAM-A-Descriptor-for-Human-Action-Recognition" class="headerlink" title="Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition"></a>Distribution of Action Movements (DAM): A Descriptor for Human Action Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17421">http://arxiv.org/abs/2310.17421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Facundo Manuel Quiroga, Franco Ronchetti, Laura Lanzarini, Cesar Eestrebou</li>
<li>for: 人体动作识别从骨骼数据是一个重要和活跃的研究领域，现状的最佳性还没有在许多知名数据集上达到近乎完美的准确率。</li>
<li>methods: 我们引入了分布动作运动特征（ Distribution of Action Movements Descriptor），一种基于骨骼动作帧间 JOINTS 的方向分布的新动作描述器。该描述器通过对集成数据集中所有可能的动作的方向分布进行归一化，计算得到一个正常化 histogram，并通过窗口 schemes 保留一定的时间结构。</li>
<li>results: 该描述器，结合标准分类器，在许多知名数据集上超过了许多现状技术的性能。<details>
<summary>Abstract</summary>
Human action recognition from skeletal data is an important and active area of research in which the state of the art has not yet achieved near-perfect accuracy on many well-known datasets. In this paper, we introduce the Distribution of Action Movements Descriptor, a novel action descriptor based on the distribution of the directions of the motions of the joints between frames, over the set of all possible motions in the dataset. The descriptor is computed as a normalized histogram over a set of representative directions of the joints, which are in turn obtained via clustering. While the descriptor is global in the sense that it represents the overall distribution of movement directions of an action, it is able to partially retain its temporal structure by applying a windowing scheme.   The descriptor, together with a standard classifier, outperforms several state-of-the-art techniques on many well-known datasets.
</details>
<details>
<summary>摘要</summary>
人体动作识别从骨骼数据是一个重要和活跃的研究领域，目前状态OF THE ART还没有在许多公知数据集上达到近乎完美准确性。在这篇论文中，我们介绍了动作描述器（Distribution of Action Movements Descriptor），这是一种基于数据集中 JOINTS 的动作描述器，它通过计算 JOINTS 的方向分布来描述动作的总体分布。这个描述器是全局的，因为它表示整个动作的方向分布，但同时还能够部分保留时间结构，通过应用窗口计划。这个描述器，结合标准分类器，在许多公知数据集上超越了多种状态OF THE ART技术。
</details></li>
</ul>
<hr>
<h2 id="Goals-are-Enough-Inducing-AdHoc-cooperation-among-unseen-Multi-Agent-systems-in-IMFs"><a href="#Goals-are-Enough-Inducing-AdHoc-cooperation-among-unseen-Multi-Agent-systems-in-IMFs" class="headerlink" title="Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs"></a>Goals are Enough: Inducing AdHoc cooperation among unseen Multi-Agent systems in IMFs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17416">http://arxiv.org/abs/2310.17416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaushik Dey, Satheesh K. Perepu, Abir Das</li>
<li>for: 这个论文的目的是提出一种基于人工智能的拓展者代理人机制，以便在下一代移动网络中实现用户expectation的有效管理。</li>
<li>methods: 该论文使用了多智能代理人学习（MARL）和人工智能监督代理人（AI-based supervisor agent）来实现协调多个预训练好的自利推荐代理人的协同工作。</li>
<li>results: 该论文的实验结果表明，相比 traditional rule-based方法，使用该提议的方法可以更快地和更好地满足用户的期望，并且能够适应环境变化。<details>
<summary>Abstract</summary>
Intent-based management will play a critical role in achieving customers' expectations in the next-generation mobile networks. Traditional methods cannot perform efficient resource management since they tend to handle each expectation independently. Existing approaches, e.g., based on multi-agent reinforcement learning (MARL) allocate resources in an efficient fashion when there are conflicting expectations on the network slice. However, in reality, systems are often far more complex to be addressed by a standalone MARL formulation. Often there exists a hierarchical structure of intent fulfilment where multiple pre-trained, self-interested agents may need to be further orchestrated by a supervisor or controller agent. Such agents may arrive in the system adhoc, which then needs to be orchestrated along with other available agents. Retraining the whole system every time is often infeasible given the associated time and cost. Given the challenges, such adhoc coordination of pre-trained systems could be achieved through an intelligent supervisor agent which incentivizes pre-trained RL/MARL agents through sets of dynamic contracts (goals or bonuses) and encourages them to act as a cohesive unit towards fulfilling a global expectation. Some approaches use a rule-based supervisor agent and deploy the hierarchical constituent agents sequentially, based on human-coded rules.   In the current work, we propose a framework whereby pre-trained agents can be orchestrated in parallel leveraging an AI-based supervisor agent. For this, we propose to use Adhoc-Teaming approaches which assign optimal goals to the MARL agents and incentivize them to exhibit certain desired behaviours. Results on the network emulator show that the proposed approach results in faster and improved fulfilment of expectations when compared to rule-based approaches and even generalizes to changes in environments.
</details>
<details>
<summary>摘要</summary>
“intent-based管理将在下一代移动网络中扮演关键角色，以实现用户的期望。传统方法无法有效地资源管理，因为它们通常处理每个期望独立。现有的方法，如基于多代理学习（MARL）的方法，可以有效地分配资源，当存在 conflicting 期望时。然而，在实际情况下，系统通常是多个层次结构的意图实现，其中多个预训练的自利愿代理（agent）需要被进一步协调。这些代理可能会随时间的推移而变化，需要在系统中进行实时协调。在这种情况下，不可能每次都进行系统重新训练。因此，我们提出了一种基于人工智能（AI）的监督代理，通过设置动态目标（goal）和奖励（bonus）来吸引预训练的 MARL 代理，使其们作为一个协调的单元工作。我们还提出了一种可靠性评估方法，以确保系统在不同环境下的稳定性。”Note that Simplified Chinese is the official language used in mainland China, and it is different from Traditional Chinese, which is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="PETA-Evaluating-the-Impact-of-Protein-Transfer-Learning-with-Sub-word-Tokenization-on-Downstream-Applications"><a href="#PETA-Evaluating-the-Impact-of-Protein-Transfer-Learning-with-Sub-word-Tokenization-on-Downstream-Applications" class="headerlink" title="PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications"></a>PETA: Evaluating the Impact of Protein Transfer Learning with Sub-word Tokenization on Downstream Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17415">http://arxiv.org/abs/2310.17415</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ginnm/proteinpretraining">https://github.com/ginnm/proteinpretraining</a></li>
<li>paper_authors: Yang Tan, Mingchen Li, Pan Tan, Ziyi Zhou, Huiqun Yu, Guisheng Fan, Liang Hong</li>
<li>For: The paper is written for protein engineering, specifically to explore the use of large protein language models for capturing the underlying evolutionary information in primary structures.* Methods: The paper uses pre-trained language models, specifically PETA, with 14 different vocabulary sizes under three tokenization methods to assess the models’ transfer learning capabilities.* Results: The paper finds that vocabulary sizes between 50 and 200 optimize the model, while sizes exceeding 800 detrimentally affect the model’s representational performance.Here is the information in Simplified Chinese text:* For: 本文是为了蛋白工程而写的，具体来说是探讨大蛋白语言模型在主要结构中捕捉进化信息的可能性。* Methods: 本文使用预训练的语言模型，具体来说是PETA模型，并在不同的词汇大小下进行了14种不同的模型训练。* Results: 研究发现，词汇大小在50-200之间优化模型表现，而词汇大小超过800会有消化性的影响。<details>
<summary>Abstract</summary>
Large protein language models are adept at capturing the underlying evolutionary information in primary structures, offering significant practical value for protein engineering. Compared to natural language models, protein amino acid sequences have a smaller data volume and a limited combinatorial space. Choosing an appropriate vocabulary size to optimize the pre-trained model is a pivotal issue. Moreover, despite the wealth of benchmarks and studies in the natural language community, there remains a lack of a comprehensive benchmark for systematically evaluating protein language model quality. Given these challenges, PETA trained language models with 14 different vocabulary sizes under three tokenization methods. It conducted thousands of tests on 33 diverse downstream datasets to assess the models' transfer learning capabilities, incorporating two classification heads and three random seeds to mitigate potential biases. Extensive experiments indicate that vocabulary sizes between 50 and 200 optimize the model, whereas sizes exceeding 800 detrimentally affect the model's representational performance. Our code, model weights and datasets are available at https://github.com/ginnm/ProteinPretraining.
</details>
<details>
<summary>摘要</summary>
大型蛋白语言模型能够很好地捕捉基因编码中的演化信息，具有重要的实用价值 для蛋白工程。与自然语言模型相比，蛋白肽序列具有较小的数据量和有限的组合空间。选择合适的词汇大小以优化预训练模型是一个关键的问题。此外，虽然自然语言社区有着丰富的benchmark和研究，但是对蛋白语言模型质量的系统性评估还缺乏一个完整的benchmark。为了解决这些挑战，PETA使用了14个不同的词汇大小在三种token化方法上训练语言模型。它在33个多样化的下游数据集上进行了千次测试，以评估模型的传输学能力，并包括两个分类头和三个随机种子以mitigate潜在偏见。广泛的实验表明，词汇大小在50-200之间优化模型，而大于800的词汇大小会消化模型的表征性表现。我们的代码、模型权重和数据集可以在https://github.com/ginnm/ProteinPretraining上获取。
</details></li>
</ul>
<hr>
<h2 id="Synthesizing-Efficiently-Monitorable-Formulas-in-Metric-Temporal-Logic"><a href="#Synthesizing-Efficiently-Monitorable-Formulas-in-Metric-Temporal-Logic" class="headerlink" title="Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic"></a>Synthesizing Efficiently Monitorable Formulas in Metric Temporal Logic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17410">http://arxiv.org/abs/2310.17410</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ritamraha/Teal">https://github.com/ritamraha/Teal</a></li>
<li>paper_authors: Ritam Raha, Rajarshi Roy, Nathanael Fijalkow, Daniel Neider, Guillermo A. Perez</li>
<li>for: 这篇论文的目的是提出一种自动从系统执行中生成正式规则，以便实时监控系统的与性。</li>
<li>methods: 这篇论文使用了一种叫做线性现实数学（LRA）的数学方法，将问题转换为一系列的满足问题，然后将这些问题的解决方案转换为Metric Temporal Logic（MTL）的规则。</li>
<li>results: 这篇论文的结果显示了一个名为TEAL的工具可以实现高效地从系统执行中生成监控可能的MTL规则，并且可以控制规则的”看 ahead”量以提高监控的效率。<details>
<summary>Abstract</summary>
In runtime verification, manually formalizing a specification for monitoring system executions is a tedious and error-prone process. To address this issue, we consider the problem of automatically synthesizing formal specifications from system executions. To demonstrate our approach, we consider the popular specification language Metric Temporal Logic (MTL), which is particularly tailored towards specifying temporal properties for cyber-physical systems (CPS). Most of the classical approaches for synthesizing temporal logic formulas aim at minimizing the size of the formula. However, for efficiency in monitoring, along with the size, the amount of "lookahead" required for the specification becomes relevant, especially for safety-critical applications. We formalize this notion and devise a learning algorithm that synthesizes concise formulas having bounded lookahead. To do so, our algorithm reduces the synthesis task to a series of satisfiability problems in Linear Real Arithmetic (LRA) and generates MTL formulas from their satisfying assignments. The reduction uses a novel encoding of a popular MTL monitoring procedure using LRA. Finally, we implement our algorithm in a tool called TEAL and demonstrate its ability to synthesize efficiently monitorable MTL formulas in a CPS application.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Invariance-Measures-for-Neural-Networks"><a href="#Invariance-Measures-for-Neural-Networks" class="headerlink" title="Invariance Measures for Neural Networks"></a>Invariance Measures for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17404">http://arxiv.org/abs/2310.17404</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facundoq/tmeasures">https://github.com/facundoq/tmeasures</a></li>
<li>paper_authors: Facundo Manuel Quiroga, Jordina Torrents-Barrena, Laura Cristina Lanzarini, Domenec Puig-Valls</li>
<li>for: 本研究旨在量化神经网络模型中的对称性表示。</li>
<li>methods: 本研究提出了一种量化神经网络模型中对称性的方法，该方法基于模型的内部表示。</li>
<li>results: 研究发现，使用该方法可以对神经网络模型的内部表示进行量化，并且该量化结果具有稳定性和可解释性。此外，研究还发现了神经网络模型的内部对称性在不同的数据集和变换下的稳定性。<details>
<summary>Abstract</summary>
Invariances in neural networks are useful and necessary for many tasks. However, the representation of the invariance of most neural network models has not been characterized. We propose measures to quantify the invariance of neural networks in terms of their internal representation. The measures are efficient and interpretable, and can be applied to any neural network model. They are also more sensitive to invariance than previously defined measures. We validate the measures and their properties in the domain of affine transformations and the CIFAR10 and MNIST datasets, including their stability and interpretability. Using the measures, we perform a first analysis of CNN models and show that their internal invariance is remarkably stable to random weight initializations, but not to changes in dataset or transformation. We believe the measures will enable new avenues of research in invariance representation.
</details>
<details>
<summary>摘要</summary>
neural networks 的不变性是有用和必需的许多任务中的。然而，大多数神经网络模型中的不变性表示尚未得到了描述。我们提出了一些量化神经网络模型内部表示的不变性的方法。这些方法是高效的，可解释的，可以应用于任何神经网络模型。它们也比之前定义的方法更敏感于不变性。我们验证了这些方法和其属性在拟合变换和 CIFAR10 和 MNIST 数据集上，包括其稳定性和可解释性。使用这些方法，我们进行了第一次 CNN 模型的分析，并发现它们的内部不变性具有Random weight initialization 的稳定性，但不具有数据集或变换的稳定性。我们认为这些方法将开启新的研究领域，即不变性表示。
</details></li>
</ul>
<hr>
<h2 id="ToxicChat-Unveiling-Hidden-Challenges-of-Toxicity-Detection-in-Real-World-User-AI-Conversation"><a href="#ToxicChat-Unveiling-Hidden-Challenges-of-Toxicity-Detection-in-Real-World-User-AI-Conversation" class="headerlink" title="ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation"></a>ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17389">http://arxiv.org/abs/2310.17389</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, Jingbo Shang</li>
<li>for: 本研究旨在提供一个基于实际用户与AI交互的排泄评估 benchmark，以便为用户与AI交互环境中的不良言语检测模型提供更好的训练数据。</li>
<li>methods: 本研究使用了现有的排泄评估 benchmark 进行系统性的评估，并通过与现有的模型进行比较，以显示这些模型在实际用户与AI交互中的缺陷。</li>
<li>results: 研究发现，现有的排泄评估模型在实际用户与AI交互中表现不佳，尤其是在辨识具有复杂涵义的不良言语方面。这显示了社交媒体内的排泄评估 benchmark 和实际用户与AI交互中的不良言语检测具有重要的区别。<details>
<summary>Abstract</summary>
Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.
</details>
<details>
<summary>摘要</summary>
尽管大语言模型在 чат机器人中已经做出了很多卓越的进步，但现在保持非恶意用户-AI交互环境变得越来越重要。然而，先前的恶意检测努力都基于社交媒体内容的标准套件，忽略了实际世界用户-AI交互中的独特挑战。在这项工作中，我们介绍了一个新的恶意测试集，即 ToxicChat，该集基于实际的用户问题，从一个开源的 chatbot 中提取出来。这个测试集包含实际世界用户-AI交互中的复杂和细腻的现象，这些现象可能会让当前的恶意检测模型很难以识别，与社交媒体内容之间存在显著的域差。我们对现有的恶意数据集上训练的模型进行了系统性的评估，发现这些模型在 ToxicChat 中的表现不佳，表明了现有的恶意检测模型在实际世界用户-AI交互中存在一定的潜在问题。我们的工作暴露了现有的恶意检测模型在实际世界用户-AI交互中可能存在的被过look的挑战。未来，ToxicChat 可以成为驱动进一步帮助建立安全和健康的用户-AI交互环境的资源。
</details></li>
</ul>
<hr>
<h2 id="YOLO-BEV-Generating-Bird’s-Eye-View-in-the-Same-Way-as-2D-Object-Detection"><a href="#YOLO-BEV-Generating-Bird’s-Eye-View-in-the-Same-Way-as-2D-Object-Detection" class="headerlink" title="YOLO-BEV: Generating Bird’s-Eye View in the Same Way as 2D Object Detection"></a>YOLO-BEV: Generating Bird’s-Eye View in the Same Way as 2D Object Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17379">http://arxiv.org/abs/2310.17379</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chang Liu, Liguo Zhou, Yanliang Huang, Alois Knoll</li>
<li>for: 提高安全和导航的自动驾驶系统视觉理解能力，实现全面和快速的视觉解释。</li>
<li>methods: 使用特殊的周围摄像头设置，将八个摄像头分别放置在45度的interval上，将图像集成成3x3格式，留下中心空间，提供了充足的空间表示，使得效率处理。使用YOLO检测机制，利用其快速响应和小型模型结构的优点。</li>
<li>results: 预liminary结果表明YOLO-BEV在实时交通视觉任务中的可行性。它的流线式架构和可能的快速部署因为参数的减少，对自动驾驶系统未来的视觉角度提供了一个丰富的探索。<details>
<summary>Abstract</summary>
Vehicle perception systems strive to achieve comprehensive and rapid visual interpretation of their surroundings for improved safety and navigation. We introduce YOLO-BEV, an efficient framework that harnesses a unique surrounding cameras setup to generate a 2D bird's-eye view of the vehicular environment. By strategically positioning eight cameras, each at a 45-degree interval, our system captures and integrates imagery into a coherent 3x3 grid format, leaving the center blank, providing an enriched spatial representation that facilitates efficient processing. In our approach, we employ YOLO's detection mechanism, favoring its inherent advantages of swift response and compact model structure. Instead of leveraging the conventional YOLO detection head, we augment it with a custom-designed detection head, translating the panoramically captured data into a unified bird's-eye view map of ego car. Preliminary results validate the feasibility of YOLO-BEV in real-time vehicular perception tasks. With its streamlined architecture and potential for rapid deployment due to minimized parameters, YOLO-BEV poses as a promising tool that may reshape future perspectives in autonomous driving systems.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Optimization-dependent-generalization-bound-for-ReLU-networks-based-on-sensitivity-in-the-tangent-bundle"><a href="#Optimization-dependent-generalization-bound-for-ReLU-networks-based-on-sensitivity-in-the-tangent-bundle" class="headerlink" title="Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle"></a>Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17378">http://arxiv.org/abs/2310.17378</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dániel Rácz, Mihály Petreczky, András Csertán, Bálint Daróczy</li>
<li>for: 该论文旨在解释深度学习模型如何通过极大过 Parametrization 来泛化良好。</li>
<li>methods: 该论文使用 PAC 类型 bound 来估计抽象网络的泛化误差，通过估计梯度下降过程中输入数据的敏感度。</li>
<li>results: 该论文通过实验证明，抽象网络的泛化误差可以通过估计梯度下降过程中输入数据的敏感度来 bounds。<details>
<summary>Abstract</summary>
Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "PAC" stands for "probably approximately correct" and refers to a theoretical framework for understanding the generalization ability of machine learning models.* "Rademacher complexity" is a measure of the complexity of a set of functions, and is used to bound the generalization error of a model.* "feedforward ReLU networks" are a type of deep neural network that uses ReLU activation functions and does not have any feedback connections.* "gradient descent" is an optimization algorithm used to train deep neural networks.* "MNIST" and "CIFAR-10" are benchmark datasets commonly used in deep learning research.
</details></li>
</ul>
<hr>
<h2 id="Dialogue-based-generation-of-self-driving-simulation-scenarios-using-Large-Language-Models"><a href="#Dialogue-based-generation-of-self-driving-simulation-scenarios-using-Large-Language-Models" class="headerlink" title="Dialogue-based generation of self-driving simulation scenarios using Large Language Models"></a>Dialogue-based generation of self-driving simulation scenarios using Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17372">http://arxiv.org/abs/2310.17372</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/avmb/dialogllmscenic">https://github.com/avmb/dialogllmscenic</a></li>
<li>paper_authors: Antonio Valerio Miceli-Barone, Alex Lascarides, Craig Innes</li>
<li>for: 这篇论文主要用于开发和评估自动驾驶车辆控制器。</li>
<li>methods: 该论文使用了大型自然语言模型（LLM）将用户的英文语言交互映射到域pecific的编程代码中，以支持扩展的多模态互动。</li>
<li>results: 研究表明，LLMs可以捕捉用户在交互中的上下文敏感性，以便计算用户的真正意图。<details>
<summary>Abstract</summary>
Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
</details>
<details>
<summary>摘要</summary>
模拟是自驾车控制器开发和评估的不可或缺工具。当前的模拟框架通常使用域Specific语言（DSL）驱动，因此增加自然语言界面可以大幅提高用户体验。但是，通常存在一个差距，这个差距由用户在提供简短的英文语言指令时所做的tacit assumption组成。在这篇论文中，我们描述了一个解决这个问题的系统，该系统支持扩展的多Modal交互：用户可以在模拟生成后进行修改或重新定义先前的指令。我们使用大型自然语言模型（LLM）将用户的英文语言指令映射到域Specific代码中，因此我们探讨了LLM是否能捕捉到在对话中的上下文敏感性。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Potential-of-Generative-AI-for-the-World-Wide-Web"><a href="#Exploring-the-Potential-of-Generative-AI-for-the-World-Wide-Web" class="headerlink" title="Exploring the Potential of Generative AI for the World Wide Web"></a>Exploring the Potential of Generative AI for the World Wide Web</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17370">http://arxiv.org/abs/2310.17370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nouar AlDahoul, Joseph Hong, Matteo Varvello, Yasir Zaki</li>
<li>for: The paper explores the potential of generative AI in the realm of the World Wide Web, specifically focusing on image generation.</li>
<li>methods: The paper develops a tool called WebDiffusion that simulates a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. The tool also supports crowdsourcing of user opinions to evaluate the quality and accuracy of AI-generated images.</li>
<li>results: The paper finds that generative AI is already capable of producing pertinent and high-quality Web images, even without requiring Web designers to manually input prompts, just by leveraging contextual information available within the webpages. However, direct in-browser image generation remains a challenge, and only highly powerful GPUs can partially compete with classic image downloads.<details>
<summary>Abstract</summary>
Generative Artificial Intelligence (AI) is a cutting-edge technology capable of producing text, images, and various media content leveraging generative models and user prompts. Between 2022 and 2023, generative AI surged in popularity with a plethora of applications spanning from AI-powered movies to chatbots. In this paper, we delve into the potential of generative AI within the realm of the World Wide Web, specifically focusing on image generation. Web developers already harness generative AI to help crafting text and images, while Web browsers might use it in the future to locally generate images for tasks like repairing broken webpages, conserving bandwidth, and enhancing privacy. To explore this research area, we have developed WebDiffusion, a tool that allows to simulate a Web powered by stable diffusion, a popular text-to-image model, from both a client and server perspective. WebDiffusion further supports crowdsourcing of user opinions, which we use to evaluate the quality and accuracy of 409 AI-generated images sourced from 60 webpages. Our findings suggest that generative AI is already capable of producing pertinent and high-quality Web images, even without requiring Web designers to manually input prompts, just by leveraging contextual information available within the webpages. However, we acknowledge that direct in-browser image generation remains a challenge, as only highly powerful GPUs, such as the A40 and A100, can (partially) compete with classic image downloads. Nevertheless, this approach could be valuable for a subset of the images, for example when fixing broken webpages or handling highly private content.
</details>
<details>
<summary>摘要</summary>
优化人工智能（AI）是一种前沿技术，可以生成文本、图像和多媒体内容，通过生成模型和用户提示。在2022年和2023年之间，生成AI的受欢迎程度增加，其应用领域包括AI电影和chatbot等。在这篇论文中，我们探讨生成AI在互联网上的潜力，具体来说是图像生成。当前，开发者已经使用生成AI来帮助制作文本和图像，而浏览器可能在未来使用它来本地生成图像，以完成维护破碎页面、降低带宽和保护隐私等任务。为了探索这个研究领域，我们开发了WebDiffusion工具，可以模拟一个基于稳定的扩散模型的网络，从客户端和服务端两个角度进行模拟。WebDiffusion还支持用户对OPINION的协同评估，我们使用这些评估来评估409个由60个页面生成的AI图像的质量和准确性。我们的发现表明，生成AI已经能够生成与页面上的内容相关的高质量和 pertinent 的网络图像，而不需要网站设计师手动输入提示。然而，我们认为，直接在浏览器中生成图像仍然是一个挑战，只有高性能的GPU，如A40和A100，才能（部分）与经典图像下载竞争。尽管如此，这种方法可能对一部分图像有价值，例如修复破碎页面或处理高度隐私内容。
</details></li>
</ul>
<hr>
<h2 id="Cultural-Adaptation-of-Recipes"><a href="#Cultural-Adaptation-of-Recipes" class="headerlink" title="Cultural Adaptation of Recipes"></a>Cultural Adaptation of Recipes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17353">http://arxiv.org/abs/2310.17353</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yong Cao, Yova Kementchedjhieva, Ruixiang Cui, Antonia Karamolegkou, Li Zhou, Megan Dare, Lucia Donatelli, Daniel Hershcovich</li>
<li>for: 本研究旨在探讨跨文化料理翻译和文化化问题，利用大语言模型来支持这一任务。</li>
<li>methods: 本研究使用了GPT-4和其他大语言模型、传统机器翻译和信息检索技术进行评估。</li>
<li>results: GPT-4在翻译中文料理为英文时表现出色，但在翻译英文料理为中文时仍然落后于人工专家。这反映了跨文化翻译的复杂性。<details>
<summary>Abstract</summary>
Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally-aware language models and their practical application in culturally diverse contexts.
</details>
<details>
<summary>摘要</summary>
基于大语言模型（LLM）的 significative advances，我们现在可以Addressing More sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally-aware language models and their practical application in culturally diverse contexts.Note: I used the Simplified Chinese character set for the translation. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="CQM-Curriculum-Reinforcement-Learning-with-a-Quantized-World-Model"><a href="#CQM-Curriculum-Reinforcement-Learning-with-a-Quantized-World-Model" class="headerlink" title="CQM: Curriculum Reinforcement Learning with a Quantized World Model"></a>CQM: Curriculum Reinforcement Learning with a Quantized World Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17330">http://arxiv.org/abs/2310.17330</a></li>
<li>repo_url: None</li>
<li>paper_authors: Seungjae Lee, Daesol Cho, Jonghae Park, H. Jin Kim</li>
<li>for: 解决复杂任务的 Reinforcement Learning (RL) 方法面临高维度目标空间生成课程目标的挑战，因此通常需要手动指定目标空间。</li>
<li>methods: 我们提出了一种新的课程方法，它自动定义了 semantic goal space，包含关键信息 для课程过程，并提出了 uncertainty 和 temporal distance-aware 的课程目标，可以快速在无信息环境中进行探索。</li>
<li>results: 我们的方法可以快速实现目标，并且在不同的目标达成任务中表现出优于现状最佳方法，包括使用 egocentric 视觉输入。<details>
<summary>Abstract</summary>
Recent curriculum Reinforcement Learning (RL) has shown notable progress in solving complex tasks by proposing sequences of surrogate tasks. However, the previous approaches often face challenges when they generate curriculum goals in a high-dimensional space. Thus, they usually rely on manually specified goal spaces. To alleviate this limitation and improve the scalability of the curriculum, we propose a novel curriculum method that automatically defines the semantic goal space which contains vital information for the curriculum process, and suggests curriculum goals over it. To define the semantic goal space, our method discretizes continuous observations via vector quantized-variational autoencoders (VQ-VAE) and restores the temporal relations between the discretized observations by a graph. Concurrently, ours suggests uncertainty and temporal distance-aware curriculum goals that converges to the final goals over the automatically composed goal space. We demonstrate that the proposed method allows efficient explorations in an uninformed environment with raw goal examples only. Also, ours outperforms the state-of-the-art curriculum RL methods on data efficiency and performance, in various goal-reaching tasks even with ego-centric visual inputs.
</details>
<details>
<summary>摘要</summary>
现代训练学习（RL）在解决复杂任务上展现出了显著的进步，通过提出序列的代理任务来解决问题。然而，之前的方法经常面临高维空间中生成课程目标的挑战，因此通常依赖于手动指定的目标空间。为了解决这些限制并提高课程的扩展性，我们提出了一种新的课程方法，它自动定义了 semantic goal space，包含课程过程中重要的信息，并在其上提出课程目标。为了定义 semantic goal space，我们使用 vector quantized-variational autoencoders（VQ-VAE）来维度化连续观察数据，并使用图restore temporal relations between the discretized observations。同时，我们建议uncertainty和 temporal distance-aware curriculum goals，这些目标在自动组成的目标空间中趋向于最终目标。我们示示了我们的方法可以在没有任何信息的环境中高效地探索，并且在不同的目标达成任务中，我们的方法超过了当前RL课程方法的数据效率和性能。
</details></li>
</ul>
<hr>
<h2 id="C-Disentanglement-Discovering-Causally-Independent-Generative-Factors-under-an-Inductive-Bias-of-Confounder"><a href="#C-Disentanglement-Discovering-Causally-Independent-Generative-Factors-under-an-Inductive-Bias-of-Confounder" class="headerlink" title="C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder"></a>C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17325">http://arxiv.org/abs/2310.17325</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaoyu Liu, Jiaxin Yuan, Bang An, Yuancheng Xu, Yifan Yang, Furong Huang</li>
<li>for: 本研究目的是探索如何在实际数据中找到几个semantically meaningful的生成因素，并使这些因素在幂下空间中分离开来。</li>
<li>methods: 本研究使用了一种名为Confounded-Disentanglement（C-Disentanglement）的框架，该框架通过域专家的标签引入了 inductive bias of confounder，以便在实际数据中找到 causally disentangled的特征。</li>
<li>results: 根据实验结果，C-Disentanglement 方法在各种 benchmark 上与多种现状顶峰模型相比，在 domain shift 下能够获得 causally disentangled 的特征和下游任务的优秀表现。<details>
<summary>Abstract</summary>
Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Disentanglement), the first framework that explicitly introduces the inductive bias of confounder via labels from domain expertise. In addition, we accordingly propose an approach to sufficiently identify the causally disentangled factors under any inductive bias of the confounder. We conduct extensive experiments on both synthetic and real-world datasets. Our method demonstrates competitive results compared to various SOTA baselines in obtaining causally disentangled features and downstream tasks under domain shifts.
</details>
<details>
<summary>摘要</summary>
学习表示假设实际世界数据是由一些semantically meaningful的生成因素（即生成因素）生成的，并且目标是在幽默空间发现这些因素。这些因素应该是 causally disentangled，meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. 比如，compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. 然而，most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Disentanglement), the first framework that explicitly introduces the inductive bias of confounder via labels from domain expertise. In addition, we accordingly propose an approach to sufficiently identify the causally disentangled factors under any inductive bias of the confounder. We conduct extensive experiments on both synthetic and real-world datasets. Our method demonstrates competitive results compared to various SOTA baselines in obtaining causally disentangled features and downstream tasks under domain shifts.
</details></li>
</ul>
<hr>
<h2 id="In-Context-Ability-Transfer-for-Question-Decomposition-in-Complex-QA"><a href="#In-Context-Ability-Transfer-for-Question-Decomposition-in-Complex-QA" class="headerlink" title="In-Context Ability Transfer for Question Decomposition in Complex QA"></a>In-Context Ability Transfer for Question Decomposition in Complex QA</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18371">http://arxiv.org/abs/2310.18371</a></li>
<li>repo_url: None</li>
<li>paper_authors: Venktesh V, Sourangshu Bhattacharya, Avishek Anand</li>
<li>for: 这个论文的目的是提出一种能够帮助语言模型学习复杂问答任务的方法，无需进行模型训练或专家注释。</li>
<li>methods: 这个方法基于在可用数据源中选择相关任务的示例，并通过注意力机制将这些示例传递给语言模型，以便帮助模型学习复杂问答任务。</li>
<li>results: 研究人员通过对多种复杂问答任务进行大规模实验，证明了 ICAT 可以在不进行模型训练或专家注释的情况下，与已有的提问基于方法相比，表现更好。<details>
<summary>Abstract</summary>
Answering complex questions is a challenging task that requires question decomposition and multistep reasoning for arriving at the solution. While existing supervised and unsupervised approaches are specialized to a certain task and involve training, recently proposed prompt-based approaches offer generalizable solutions to tackle a wide variety of complex question-answering (QA) tasks. However, existing prompt-based approaches that are effective for complex QA tasks involve expensive hand annotations from experts in the form of rationales and are not generalizable to newer complex QA scenarios and tasks. We propose, icat (In-Context Ability Transfer) which induces reasoning capabilities in LLMs without any LLM fine-tuning or manual annotation of in-context samples. We transfer the ability to decompose complex questions to simpler questions or generate step-by-step rationales to LLMs, by careful selection from available data sources of related tasks. We also propose an automated uncertainty-aware exemplar selection approach for selecting examples from transfer data sources. Finally, we conduct large-scale experiments on a variety of complex QA tasks involving numerical reasoning, compositional complex QA, and heterogeneous complex QA which require decomposed reasoning. We show that ICAT convincingly outperforms existing prompt-based solutions without involving any model training, showcasing the benefits of re-using existing abilities.
</details>
<details>
<summary>摘要</summary>
Answering complex questions is a difficult task that requires breaking down the question into smaller parts and using multistep reasoning to find the solution. While existing supervised and unsupervised approaches are specialized to a certain task and require training, recently proposed prompt-based approaches offer generalizable solutions to tackle a wide variety of complex question-answering (QA) tasks. However, existing prompt-based approaches that are effective for complex QA tasks rely on expensive expert annotations in the form of rationales and are not generalizable to newer complex QA scenarios and tasks.We propose a new approach called icat (In-Context Ability Transfer), which enables reasoning capabilities in large language models (LLMs) without any fine-tuning or manual annotation of in-context samples. We transfer the ability to decompose complex questions into simpler questions or generate step-by-step rationales to LLMs by carefully selecting relevant data from available sources of related tasks. We also propose an automated uncertainty-aware exemplar selection approach for selecting examples from transfer data sources.We conduct large-scale experiments on a variety of complex QA tasks involving numerical reasoning, compositional complex QA, and heterogeneous complex QA, which require decomposed reasoning. Our results show that ICAT outperforms existing prompt-based solutions without any model training, demonstrating the benefits of reusing existing abilities.
</details></li>
</ul>
<hr>
<h2 id="CodeFusion-A-Pre-trained-Diffusion-Model-for-Code-Generation"><a href="#CodeFusion-A-Pre-trained-Diffusion-Model-for-Code-Generation" class="headerlink" title="CodeFusion: A Pre-trained Diffusion Model for Code Generation"></a>CodeFusion: A Pre-trained Diffusion Model for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17680">http://arxiv.org/abs/2310.17680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Gust Verbruggen</li>
<li>for: 本研究的目的是提出一种基于扩散代码生成模型，以便在自然语言编程中能够更好地重新考虑之前生成的代码。</li>
<li>methods: 本研究使用了预训练的扩散代码生成模型CodeFusion，通过 Iterative Denoising 来重新考虑 encoded natural language 中的整个程序。</li>
<li>results: 实验表明，CodeFusion 能够与现状的 auto-regressive 系统相当，并且在 top-3 和 top-5 准确率上表现更佳，这是因为它更好地均衡了多样性和质量。<details>
<summary>Abstract</summary>
Imagine a developer who can only change their last line of code, how often would they have to start writing a function from scratch before it is correct? Auto-regressive models for code generation from natural language have a similar limitation: they do not easily allow reconsidering earlier tokens generated. We introduce CodeFusion, a pre-trained diffusion code generation model that addresses this limitation by iteratively denoising a complete program conditioned on the encoded natural language. We evaluate CodeFusion on the task of natural language to code generation for Bash, Python, and Microsoft Excel conditional formatting (CF) rules. Experiments show that CodeFusion (75M parameters) performs on par with state-of-the-art auto-regressive systems (350M-175B parameters) in top-1 accuracy and outperforms them in top-3 and top-5 accuracy due to its better balance in diversity versus quality.
</details>
<details>
<summary>摘要</summary>
想象一个开发者只能改变最后一行代码，如何频繁地重新写函数才能达到正确性？自然语言到代码生成模型具有类似的限制：它们不易允许重新考虑早些 tokens 生成的。我们介绍 CodeFusion，一种预训练的扩散代码生成模型，解决了这种限制。CodeFusion 通过Iteratively Denoising 完整程序，根据编码的自然语言来conditioning。我们在 Bash、Python 和 Microsoft Excel 条件格式（CF）规则中进行了实验，结果显示 CodeFusion（75M 参数）与状态之前 auto-regressive 系统（350M-175B 参数）在 top-1 准确率上相当，而且在 top-3 和 top-5 准确率上表现更好，这是因为它更好地均衡了多样性和质量。
</details></li>
</ul>
<hr>
<h2 id="FormaT5-Abstention-and-Examples-for-Conditional-Table-Formatting-with-Natural-Language"><a href="#FormaT5-Abstention-and-Examples-for-Conditional-Table-Formatting-with-Natural-Language" class="headerlink" title="FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language"></a>FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17306">http://arxiv.org/abs/2310.17306</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, Elnaz Nouri, Mohammad Raza, Gust Verbruggen</li>
<li>for: 本研究旨在提供一种基于 transformer 模型的自动表格格式化系统，以便根据用户提供的自然语言描述，生成数据依赖 conditional formatting（CF）规则。</li>
<li>methods: 本研究使用 transformer 模型来生成 CF 规则，并通过预测 placeholder 来解决用户描述的下pecification和 Argument Errors 问题。</li>
<li>results: 对于 1053 个 CF 任务，FormaT5 可以通过预测 placeholder 和 filling 来超过 8 种神经网络方法的性能，both with 和 without 例子。这说明了建立域pecific learning system 的价值。<details>
<summary>Abstract</summary>
Formatting is an important property in tables for visualization, presentation, and analysis. Spreadsheet software allows users to automatically format their tables by writing data-dependent conditional formatting (CF) rules. Writing such rules is often challenging for users as it requires them to understand and implement the underlying logic. We present FormaT5, a transformer-based model that can generate a CF rule given the target table and a natural language description of the desired formatting logic. We find that user descriptions for these tasks are often under-specified or ambiguous, making it harder for code generation systems to accurately learn the desired rule in a single step. To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective. These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system. To evaluate FormaT5 on diverse and real scenarios, we create an extensive benchmark of 1053 CF tasks, containing real-world descriptions collected from four different sources. We release our benchmarks to encourage research in this area. Abstention and filling allow FormaT5 to outperform 8 different neural approaches on our benchmarks, both with and without examples. Our results illustrate the value of building domain-specific learning systems.
</details>
<details>
<summary>摘要</summary>
表格的格式化是一个重要的属性，它对于视觉化、展示和分析都非常重要。电子表格软件允许用户自动格式化他们的表格，这可以通过写数据依赖的条件格式化规则（CF）来实现。写这些规则是常常给用户带来挑战，因为它们需要用户理解并实现下面的逻辑。我们提出了FormaT5，一种基于转换器的模型，可以根据目标表格和自然语言描述来生成CF规则。我们发现用户对这些任务的描述经常是不充分或模糊的，这使得代码生成系统更难准确地学习所需的规则。为解决这个问题，FormaT5学习预测占位符，通过缺失目标对象的目标搜索来减少参数错误。这些占位符可以通过第二个模型或，当有示例行可用时，通过编程示例系统来填充。为评估FormaT5在多样化和实际场景中的表现，我们创建了1053个CF任务的广泛 benchmark，其中包括来自四个不同来源的真实描述。我们发布了这些 benchmark，以便促进这一领域的研究。忽略和填充允许FormaT5在我们的 benchmark 上超越8种神经网络方法，包括和没有示例。我们的结果表明，建立领域特定的学习系统是非常有价值的。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Photorealistic-and-Animated-Embodied-Conversational-Agents-in-Serious-Games-An-Empirical-Study-on-User-Experience"><a href="#Comparing-Photorealistic-and-Animated-Embodied-Conversational-Agents-in-Serious-Games-An-Empirical-Study-on-User-Experience" class="headerlink" title="Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience"></a>Comparing Photorealistic and Animated Embodied Conversational Agents in Serious Games: An Empirical Study on User Experience</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17300">http://arxiv.org/abs/2310.17300</a></li>
<li>repo_url: None</li>
<li>paper_authors: Danai Korre</li>
<li>for: 这篇论文的目的是研究对话人工智能（ECAs）在严肃游戏环境中的使用，以及两种不同的表现实实验的影响。</li>
<li>methods: 这篇论文使用了一种在Subjects中使用的两重两жды因素设计，并采集了36名参与者的数据，以便分析对ECAs的使用性和参与者对不同版本的偏好。</li>
<li>results: 研究发现，两种版本都被评估为非常可用，但参与者中69.4%表示偏好真实版本，25%表示偏好动画版本，5.6%没有表态。真实版本被认为更加真实和人类化，而动画版本使得任务更像游戏。尽管代理人的真实性没有对可用性产生显著影响，但它 positively 影响了参与者对代理人的评估。<details>
<summary>Abstract</summary>
Embodied conversational agents (ECAs) are paradigms of conversational user interfaces in the form of embodied characters. While ECAs offer various manipulable features, this paper focuses on a study conducted to explore two distinct levels of presentation realism. The two agent versions are photorealistic and animated. The study aims to provide insights and design suggestions for speech-enabled ECAs within serious game environments. A within-subjects, two-by-two factorial design was employed for this research with a cohort of 36 participants balanced for gender. The results showed that both the photorealistic and the animated versions were perceived as highly usable, with overall mean scores of 5.76 and 5.71, respectively. However, 69.4 per cent of the participants stated they preferred the photorealistic version, 25 per cent stated they preferred the animated version and 5.6 per cent had no stated preference. The photorealistic agents were perceived as more realistic and human-like, while the animated characters made the task feel more like a game. Even though the agents' realism had no significant effect on usability, it positively influenced participants' perceptions of the agent. This research aims to lay the groundwork for future studies on ECA realism's impact in serious games across diverse contexts.
</details>
<details>
<summary>摘要</summary>
人工智能对话代理（ECAs）是对话用户界面的一种形式，具有各种可操作特性。本研究探讨了两种不同的展示现实主义水平，即真实摄影和动画两种代理版本。这项研究的目的是为了在严格游戏环境中的speech-enabled ECAs提供设计建议和灵感。本研究采用了一种内subjects，两个因素实验设计，参与者共36名，男女各半数。结果显示，两种版本都被评估为非常可用，总的 mean分别为5.76和5.71。然而，69.4%的参与者表示喜欢真实摄影版本，25%表示喜欢动画版本，5.6%无偏好。真实摄影代理被认为更真实和人类化，而动画人物使得任务感觉更像是一场游戏。虽然代理的真实性没有显著影响可用性，但它 positively 影响了参与者对代理的看法。本研究的目的是为将来在多种场景中的ECAs真实性的影响进行深入研究。
</details></li>
</ul>
<hr>
<h2 id="Fast-Scalable-and-Accurate-Discovery-of-DAGs-Using-the-Best-Order-Score-Search-and-Grow-Shrink-Trees"><a href="#Fast-Scalable-and-Accurate-Discovery-of-DAGs-Using-the-Best-Order-Score-Search-and-Grow-Shrink-Trees" class="headerlink" title="Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees"></a>Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17679">http://arxiv.org/abs/2310.17679</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cmu-phil/boss">https://github.com/cmu-phil/boss</a></li>
<li>paper_authors: Bryan Andrews, Joseph Ramsey, Ruben Sanchez-Romero, Jazmin Camchong, Erich Kummerfeld</li>
<li>for: 学习图解 conditional independence 结构是机器学习中一项重要的问题，也是 causal discovery 的重要基础。但是，现有的算法的准确率和执行时间通常难以扩展到包含百个高度连接的变量的问题，例如从 fMRI 数据中恢复大脑网络。</li>
<li>methods: 我们引入了最佳顺序分数搜索 (BOSS) 和 grow-shrink 树 (GST)，用于学习 Directed Acyclic Graphs (DAGs)。BOSS 通过 GST 构建和评分 DAGs 来进行搜索。GST 高效缓存分数，以消除重复计算。</li>
<li>results: BOSS 可以在各种条件下达到 state-of-the-art 的准确率和执行时间，与其他 combinatorial 和梯度基于的学习算法相比。为了证明其实用性，我们将 BOSS 应用于两个resting-state fMRI数据集：一个是 simulated data 与 pseudo-empirical noise distribution  derivated from randomized empirical fMRI cortical signals，另一个是 3T fMRI scans 处理后的 cortical parcels。BOSS 可以在 TETRAD 项目中使用，包括 Python 和 R  wrapper。<details>
<summary>Abstract</summary>
Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables -- for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals and clinical data from 3T fMRI scans processed into cortical parcels. BOSS is available for use within the TETRAD project which includes Python and R wrappers.
</details>
<details>
<summary>摘要</summary>
学习图structures是机器学习的重要问题，也是 causal discovery的基础。但是，学习算法的准确率和执行时间通常在百个高度连接的变量问题上难以扩展 -- 例如，从 fMRI 数据中回归大脑网络。我们介绍了最佳顺序分数搜索（BOSS）和生长缩小树（GST）用于学习 directed acyclic graphs（DAGs）。BOSS 在 permutations of variables 上进行探索，使用 GSTs 构建和评分 DAGs。GSTs 高效地缓存分数，以消除重复计算。BOSS 在准确率和执行时间方面达到了状态机器学习算法的最佳性能，与许多 combinatorial 和梯度基于的学习算法进行比较，在各种条件下表现出色。为了证明其实用性，我们将 BOSS 应用于两个 sets of resting-state fMRI 数据：生成的 simulated data 和临床数据 from 3T fMRI 扫描。BOSS 可以在 TETRAD 项目中使用，该项目包括 Python 和 R 包装。
</details></li>
</ul>
<hr>
<h2 id="New-Boolean-satisfiability-problem-heuristic-strategy-Minimal-Positive-Negative-Product-Strategy"><a href="#New-Boolean-satisfiability-problem-heuristic-strategy-Minimal-Positive-Negative-Product-Strategy" class="headerlink" title="New Boolean satisfiability problem heuristic strategy: Minimal Positive Negative Product Strategy"></a>New Boolean satisfiability problem heuristic strategy: Minimal Positive Negative Product Strategy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.18370">http://arxiv.org/abs/2310.18370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qun Zhao, Xintao Wang, Menghui Yang</li>
<li>for: 解决Boolean satisfiability问题</li>
<li>methods: 使用Minimal Positive Negative Product Strategy引导CDCL算法</li>
<li>results: 实验结果证明该算法在问题解决中更高效 чем常用的DLIS和VSIDS算法<details>
<summary>Abstract</summary>
This study presents a novel heuristic algorithm called the "Minimal Positive Negative Product Strategy" to guide the CDCL algorithm in solving the Boolean satisfiability problem. It provides a mathematical explanation for the superiority of this algorithm over widely used heuristics such as the Dynamic Largest Individual Sum (DLIS) and the Variable State Independent Decaying Sum (VSIDS). Experimental results further confirm the effectiveness of this heuristic strategy in problem-solving.
</details>
<details>
<summary>摘要</summary>
Here is the text in Simplified Chinese:这个研究提出了一种新的启发算法，called "最小正负乘积策略"，以帮助CDCL算法解决Boolean满足问题。这个算法被数学上证明为其他广泛使用的启发策略，如DLIS和VSIDS，的超越。实验结果还证明了该启发策略的效果。
</details></li>
</ul>
<hr>
<h2 id="Attribute-Based-Interpretable-Evaluation-Metrics-for-Generative-Models"><a href="#Attribute-Based-Interpretable-Evaluation-Metrics-for-Generative-Models" class="headerlink" title="Attribute Based Interpretable Evaluation Metrics for Generative Models"></a>Attribute Based Interpretable Evaluation Metrics for Generative Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17261">http://arxiv.org/abs/2310.17261</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dongkyun Kim, Mingi Kwon, Youngjung Uh</li>
<li>for: 本研究旨在提出一种新的评估协议，用于评估生成模型是否能够准确地捕捉训练集中的种类分布。</li>
<li>methods: 本研究使用了单 attribute 分化（SaD）和对应 attribute 分化（PaD）两种新的评估指标，以及一种新的图像特征评估指标——不同类型 CLIPScore（HCS）。</li>
<li>results: 通过使用这些指标，我们发现了一些现有的生成模型的缺陷，例如 ProjectedGAN 生成了不可能的属性关系，扩散模型困难捕捉数据集中的多种颜色，而 latent diffusion model 的更大的抽样步骤生成了更小的对象。<details>
<summary>Abstract</summary>
When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond "diversity". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. Single-attribute Divergence (SaD) measures the divergence regarding PDFs of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding joint PDFs of a pair of attributes. They provide which attributes the models struggle. For measuring the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we reveal the following about existing generative models. ProjectedGAN generates implausible attribute relationships such as a baby with a beard even though it has competitive scores of existing metrics. Diffusion models struggle to capture diverse colors in the datasets. The larger sampling timesteps of latent diffusion model generate the more minor objects including earrings and necklaces. Stable Diffusion v1.5 better captures the attributes than v2.1. Our metrics lay a foundation for explainable evaluations of generative models.
</details>
<details>
<summary>摘要</summary>
We define Single-attribute Divergence (SaD) as the divergence regarding the probability density functions (PDFs) of a single attribute. Paired-attribute Divergence (PaD) measures the divergence regarding the joint PDFs of a pair of attributes. These metrics reveal which attributes the models struggle with.To measure the attribute strengths of an image, we propose Heterogeneous CLIPScore (HCS), which measures the cosine similarity between image and text vectors with heterogeneous initial points. With SaD and PaD, we find that ProjectedGAN generates implausible attribute relationships, such as a baby with a beard, despite having competitive scores on existing metrics. Diffusion models struggle to capture diverse colors in the datasets, and the larger sampling timesteps of the latent diffusion model result in the generation of smaller objects, such as earrings and necklaces. Stable Diffusion v1.5 performs better in capturing attributes than v2.1.Our proposed metrics provide a foundation for explainable evaluations of generative models, enabling us to better understand their strengths and weaknesses.
</details></li>
</ul>
<hr>
<h2 id="IDENAS-Internal-Dependency-Exploration-for-Neural-Architecture-Search"><a href="#IDENAS-Internal-Dependency-Exploration-for-Neural-Architecture-Search" class="headerlink" title="IDENAS: Internal Dependency Exploration for Neural Architecture Search"></a>IDENAS: Internal Dependency Exploration for Neural Architecture Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17250">http://arxiv.org/abs/2310.17250</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/viharoszsolt/idenas">https://github.com/viharoszsolt/idenas</a></li>
<li>paper_authors: Anh T. Hoang, Zsolt J. Viharos</li>
<li>for: 提高自动机器学习模型开发的效率和准确率，特别是在输入和输出变量之间存在未知关系的情况下。</li>
<li>methods: 提出了一种基于内部依赖关系的搜索方法IDENAS，结合了神经网络搜索和特征选择。IDENAS使用修改后的编码器-解码器模型和继承前进搜索算法，将输入-输出配置搜索与嵌入特征选择相结合。</li>
<li>results: 实验结果显示，IDENAS在比较其他算法的情况下表现出色， demonstrating its effectiveness in model development pipelines and automated machine learning. On average, IDENAS achieved significant modelling improvements, highlighting its significant contribution to advancing the state-of-the-art in neural architecture search and feature selection integration.<details>
<summary>Abstract</summary>
Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior performance in comparison to other algorithms, showcasing its effectiveness in model development pipelines and automated machine learning. On average, IDENAS achieved significant modelling improvements, underscoring its significant contribution to advancing the state-of-the-art in neural architecture search and feature selection integration.
</details>
<details>
<summary>摘要</summary>
机器学习是一种强大的工具，可以从多样化数据集中提取有价值信息并进行多种预测。传统的算法假设输入和输出变量之间存在明确的定义，但有时候输入和输出变量之间的关系并不明确。神经网络搜索（NAS）和特征选择是一些有前途的解决方案。这项研究提出了内部依赖性搜索（IDENAS），它将NAS与特征选择集成了一起。该方法在完全参数空间中搜索内部依赖关系，用于分类，包括1D感知器和2D图像数据。IDENAS使用修改后的encoder-decoder模型和顺序前进搜索（SFS）算法，将输入输出配置搜索与嵌入特征选择结合在一起。实验结果表明，IDENAS在其他算法的比较中表现出色，展示了其在机器学习开发流程和自动化机器学习中的有效性。在平均上，IDENAS实现了重要的模型改进，强调了它在神经网络搜索和特征选择集成中的重要贡献。
</details></li>
</ul>
<hr>
<h2 id="CROP-Conservative-Reward-for-Model-based-Offline-Policy-Optimization"><a href="#CROP-Conservative-Reward-for-Model-based-Offline-Policy-Optimization" class="headerlink" title="CROP: Conservative Reward for Model-based Offline Policy Optimization"></a>CROP: Conservative Reward for Model-based Offline Policy Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17245">http://arxiv.org/abs/2310.17245</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/g0k0ururi/crop">https://github.com/g0k0ururi/crop</a></li>
<li>paper_authors: Hao Li, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Xiao-Yin Liu, Mei-Jiang Gui, Tian-Yu Xiang, De-Xing Huang, Bo-Xian Yao, Zeng-Guang Hou</li>
<li>for: 提出了一种新的模型基于的离线强化学习算法（CROP），用于优化策略，并通过保守估计奖励来避免分布迁移问题。</li>
<li>methods: 该算法使用了模型训练来估计奖励，并同时减少估计错误和随机行动奖励的积累。</li>
<li>results: 实验结果表明，CROP算法与当前基eline相当，并且在D4RLbenchmark上显示了良好的性能。此外，该算法还发现了在离线RL中的onlineRL技术的潜在连接。Here’s the translation in English:</li>
<li>for: The paper proposes a new model-based offline reinforcement learning algorithm (CROP) to optimize policies and mitigate the distribution drift problem by conservatively estimating rewards.</li>
<li>methods: The algorithm uses model training to estimate rewards and simultaneously minimizes the estimation error and the reward of random actions.</li>
<li>results: Experimental results show that the performance of CROP is comparable to the state-of-the-art baselines, and it establishes an innovative connection between offline and online RL by adopting online RL techniques to the empirical Markov decision process trained with a conservative reward.<details>
<summary>Abstract</summary>
Offline reinforcement learning (RL) aims to optimize policy using collected data without online interactions. Model-based approaches are particularly appealing for addressing offline RL challenges due to their capability to mitigate the limitations of offline data through data generation using models. Prior research has demonstrated that introducing conservatism into the model or Q-function during policy optimization can effectively alleviate the prevalent distribution drift problem in offline RL. However, the investigation into the impacts of conservatism in reward estimation is still lacking. This paper proposes a novel model-based offline RL algorithm, Conservative Reward for model-based Offline Policy optimization (CROP), which conservatively estimates the reward in model training. To achieve a conservative reward estimation, CROP simultaneously minimizes the estimation error and the reward of random actions. Theoretical analysis shows that this conservative reward mechanism leads to a conservative policy evaluation and helps mitigate distribution drift. Experiments on D4RL benchmarks showcase that the performance of CROP is comparable to the state-of-the-art baselines. Notably, CROP establishes an innovative connection between offline and online RL, highlighting that offline RL problems can be tackled by adopting online RL techniques to the empirical Markov decision process trained with a conservative reward. The source code is available with https://github.com/G0K0URURI/CROP.git.
</details>
<details>
<summary>摘要</summary>
偏好线上学习（RL）的目标是通过收集数据来优化策略，而不是在线交互。基于模型的方法在解决偏好线上学习挑战方面尤其有利，因为它们可以通过模型生成数据来减少收集数据的限制。过去的研究表明，在策略优化中引入保守性可以有效地解决偏好线上学习中的分布漂移问题。然而，关于奖励估计中的保守性的研究仍然缺乏。这篇论文提出了一种新的模型基于的偏好线上学习算法，即保守奖励for model-based Offline Policy optimization（CROP）。CROP通过在模型训练中保守地估计奖励来实现保守的奖励估计。为了实现保守的奖励估计，CROP同时减少了估计错误和随机动作的奖励。理论分析表明，这种保守的奖励机制导致保守的策略评估，帮助解决分布漂移问题。实验表明，CROP在D4RL benchmark上的性能与现状的基eline相当。尤其是，CROP建立了在线和偏好线上学习之间的创新连接，指出偏好线上学习问题可以通过采用在线RL技术来解决empirical Markov decision process中训练的保守奖励。源代码可以在https://github.com/G0K0URURI/CROP.git中找到。
</details></li>
</ul>
<hr>
<h2 id="Joint-Entity-and-Relation-Extraction-with-Span-Pruning-and-Hypergraph-Neural-Networks"><a href="#Joint-Entity-and-Relation-Extraction-with-Span-Pruning-and-Hypergraph-Neural-Networks" class="headerlink" title="Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks"></a>Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17238">http://arxiv.org/abs/2310.17238</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yanzhh/hgere">https://github.com/yanzhh/hgere</a></li>
<li>paper_authors: Zhaohui Yan, Songlin Yang, Wei Liu, Kewei Tu</li>
<li>for: 提高Entity和Relation抽取（ERE）任务的性能，特别是解决 marker-based 管道模型中的错误卷积问题。</li>
<li>methods: 基于 PL-marker  marker-based 管道模型，提出 HyperGraph 神经网络（$\hgnn{}$），并使用高复 recall 减弱机制来减轻NER模块的负担。进一步地，建立一个高级图，其中节点为实体（由 span pruner 提供）和其关系，强制编码这些关系的交互。</li>
<li>results: 在三个广泛使用的 ERE  benchmark 上（\acef{}, \ace{} 和 \scierc{）），经验表明 $\hgnn{}$ 模型在前一代 marker-based 管道模型的基础上具有显著的改进。<details>
<summary>Abstract</summary>
Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-of-the-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higher-order interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose HyperGraph neural network for ERE ($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run a hypergraph neural network for higher-order inference by applying message passing over the built hypergraph. Experiments on three widely used benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant improvements over the previous state-of-the-art PL-marker.
</details>
<details>
<summary>摘要</summary>
entity 和 relation 抽取 (ERE) 是信息抽取中的重要任务。 current marker-based pipeline 模型可以达到状态的最佳性能，但仍然受到错误卷积问题的影响。 besides， current ERE 模型多数不考虑多个实体和关系之间的高阶交互，而高阶模型化可能是有利的。在这种情况下，我们提出了 HyperGraph 神经网络 для ERE ($ \hgnn{}$), 它基于 PL-marker (现状最佳 marker-based pipeline 模型)。为了缓解错误卷积问题，我们使用高度回归预测机制，将实体识别和标注的负担从 NER 模块传递给我们模型的联合模块。 For higher-order modeling， we build a hypergraph, where nodes are entities (由 span pruner 提供) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities。然后，我们运行一个高阶神经网络，通过在建立的 hypergraph 上进行消息传递来进行高阶推理。 experiments 表明，在三个常用的 ERE  benchmark 上（\acef{}, \ace{} 和 \scierc{）），我们的模型可以具有显著的改善，胜过了之前的 PL-marker。
</details></li>
</ul>
<hr>
<h2 id="TST-mathrm-R-Target-Similarity-Tuning-Meets-the-Real-World"><a href="#TST-mathrm-R-Target-Similarity-Tuning-Meets-the-Real-World" class="headerlink" title="TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World"></a>TST$^\mathrm{R}$: Target Similarity Tuning Meets the Real World</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17228">http://arxiv.org/abs/2310.17228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Khatry, Sumit Gulwani, Priyanshu Gupta, Vu Le, Ananya Singha, Mukul Singh, Gust Verbruggen</li>
<li>for: This paper is written for improving the performance of natural language (NL) to code generation through large language models (LLMs) by adapting a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs.</li>
<li>methods: The paper proposes different methods to apply and improve target similarity tuning (TST) in the real world, including replacing the sentence transformer with embeddings from a larger model, training a tiny model to transform the embeddings, and efficiently selecting a smaller number of training examples.</li>
<li>results: The paper introduces a ranking-based evaluation for TST that does not require end-to-end code generation experiments, which can be expensive to perform.<details>
<summary>Abstract</summary>
Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we show how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evaluation for TST that does not require end-to-end code generation experiments, which can be expensive to perform.
</details>
<details>
<summary>摘要</summary>
目标相似调整（TST）是一种使用大语言模型（LLM）来生成代码的方法，旨在将NL输入与其相关的代码输出之间的相似性进行调整。在这篇论文中，我们提出了不同的方法来应用和改进TST在实际应用中。首先，我们将 sentence transformer 替换为来自更大的模型的嵌入，这会降低语言分布的敏感度，并提供更多的自然语言生成的可能性，然后我们将这些嵌入变换到一个空间中，使得嵌入相似性与代码相似性匹配，这些操作只需在推理时进行几次矩阵乘法即可。其次，我们展示了如何高效地选择训练例子来训练TST模型。最后，我们引入了一种基于排名的评估方法，不需要进行昂贵的端到端代码生成实验。
</details></li>
</ul>
<hr>
<h2 id="Beyond-MLE-Convex-Learning-for-Text-Generation"><a href="#Beyond-MLE-Convex-Learning-for-Text-Generation" class="headerlink" title="Beyond MLE: Convex Learning for Text Generation"></a>Beyond MLE: Convex Learning for Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17217">http://arxiv.org/abs/2310.17217</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ictnlp/convex-learning">https://github.com/ictnlp/convex-learning</a></li>
<li>paper_authors: Chenze Shao, Zhengrui Ma, Min Zhang, Yang Feng</li>
<li>For:  This paper proposes a novel approach to training text generation models using convex functions, which can help the models focus on highly probable outputs without requiring maximum likelihood estimation (MLE).* Methods: The proposed approach uses convex functions to define the training objective, which enables the model to better capture outputs with high probabilities. The authors investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss.* Results: The proposed approach is effective in improving the performance of text generation models. In experiments on various text generation tasks and models, the approach enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. The approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks.<details>
<summary>Abstract</summary>
Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the optimal distribution, thereby enabling the model to better capture outputs with high probabilities. Experiments on various text generation tasks and models show the effectiveness of our approach. It enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks. Source code is available at \url{https://github.com/ictnlp/Convex-Learning}.
</details>
<details>
<summary>摘要</summary>
最大 LIKElihood 估计 (MLE) 是一种统计方法，用于估计一个概率分布中的参数，以便最好地预测观察到的数据。在文本生成任务中，MLE  oftens 用于训练生成语言模型，以便生成新的文本。然而，我们认为MLE 不一定是最佳和必要的，尤其是在关闭式文本生成任务中，如机器翻译。在这些任务中，模型的目标是生成最佳的回答，而不一定需要估计整个数据分布。为此，我们提出了一种新的训练目标函数，基于凸函数，允许文本生成模型专注于高可能性的输出，而不需要估计整个数据分布。我们研究了这种新的训练目标函数的理论性质，并证明了凸函数可以使估计的最佳分布更加紧凑，使模型更好地捕捉高可能性的输出。我们在不同的文本生成任务和模型上进行了实验，并证明了我们的方法的效iveness。它使得 autoregressive 模型可以跨度搜索和搜索，并且使得非 autoregressive 模型学习得到最大改进（9+ BLEU 点）。此外，我们的方法还在大语言模型 (LLM) 上展现了显著的影响，substantially 提高了它们的生成能力在多种任务上。可以在 \url{https://github.com/ictnlp/Convex-Learning} 上获得源代码。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Recognition-by-Video-A-review"><a href="#Emotion-Recognition-by-Video-A-review" class="headerlink" title="Emotion Recognition by Video: A review"></a>Emotion Recognition by Video: A review</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17212">http://arxiv.org/abs/2310.17212</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/References">https://github.com/Aryia-Behroziuan/References</a></li>
<li>paper_authors: Junxiao Xue, Jie Wang, Xuecheng Wu, Liangyu Fu</li>
<li>for: 本文旨在帮助学术界和现代科学家综合了解最新的发展和创新在视频情感识别领域。</li>
<li>methods: 本文分析了视频情感识别方法的特点和性能，并对不同类型的方法进行比较。</li>
<li>results: 本文系统性地梳理了2015年至2023年期间发表的视频情感识别研究，包括两种常见情感模型、常用的数据库和现代视频情感识别方法的结构和性能。<details>
<summary>Abstract</summary>
Video emotion recognition is an important branch of affective computing, and its solutions can be applied in different fields such as human-computer interaction (HCI) and intelligent medical treatment. Although the number of papers published in the field of emotion recognition is increasing, there are few comprehensive literature reviews covering related research on video emotion recognition. Therefore, this paper selects articles published from 2015 to 2023 to systematize the existing trends in video emotion recognition in related studies. In this paper, we first talk about two typical emotion models, then we talk about databases that are frequently utilized for video emotion recognition, including unimodal databases and multimodal databases. Next, we look at and classify the specific structure and performance of modern unimodal and multimodal video emotion recognition methods, talk about the benefits and drawbacks of each, and then we compare them in detail in the tables. Further, we sum up the primary difficulties right now looked by video emotion recognition undertakings and point out probably the most encouraging future headings, such as establishing an open benchmark database and better multimodal fusion strategys. The essential objective of this paper is to assist scholarly and modern scientists with keeping up to date with the most recent advances and new improvements in this speedy, high-influence field of video emotion recognition.
</details>
<details>
<summary>摘要</summary>
视频情感识别是人工智能的重要分支，其解决方案可以应用于不同领域，如人机交互（HCI）和智能医疗治疗。虽然有很多关于情感识别的研究论文发表，但有很少的总结性文献，涵盖相关研究的视频情感识别领域。因此，本文选择2015年至2023年发表的文献，系матизи了视频情感识别领域的现有趋势。在本文中，我们首先介绍了两种典型的情感模型，然后介绍了通常用于视频情感识别的数据库，包括单模态数据库和多模态数据库。接着，我们分析和比较现代单模态和多模态视频情感识别方法的特点和性能，讲述每种方法的优缺点，并在表格中进行详细比较。然后，我们总结了现在视频情感识别项目面临的主要挑战，并提出了未来可能的发展方向，如建立开源标准数据库和更好的多模态融合策略。本文的主要目标是帮助学术和现代科学家保持最新的发展和新进展在这个快速、高影响的领域中。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Data-Fusion-using-the-Tsetlin-Machine"><a href="#Efficient-Data-Fusion-using-the-Tsetlin-Machine" class="headerlink" title="Efficient Data Fusion using the Tsetlin Machine"></a>Efficient Data Fusion using the Tsetlin Machine</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17207">http://arxiv.org/abs/2310.17207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rupsa Saha, Vladimir I. Zadorozhny, Ole-Christoffer Granmo</li>
<li>for: 本研究提出了一种新的方法来评估和融合噪音数据，使用Tsetlin机器。</li>
<li>methods: 该方法通过监测Tsetlin机器学习的解释逻辑 clause 如何随数据噪音变化，从而识别噪音或者通过新的逻辑 clause 来反映噪音。</li>
<li>results: 该方法在不同的数据集上进行了全面的实验研究，得到了高效的结果。<details>
<summary>Abstract</summary>
We propose a novel way of assessing and fusing noisy dynamic data using a Tsetlin Machine. Our approach consists in monitoring how explanations in form of logical clauses that a TM learns changes with possible noise in dynamic data. This way TM can recognize the noise by lowering weights of previously learned clauses, or reflect it in the form of new clauses. We also perform a comprehensive experimental study using notably different datasets that demonstrated high performance of the proposed approach.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的方法，使用Tsetlin机器来评估和融合含有噪声的动态数据。我们的方法是通过观察TMC所学得的逻辑条件如何随着可能的噪声在动态数据中变化，从而使TMC能够认可噪声，例如降低先前学习的条件的权重，或者表现为新的条件。我们还进行了对不同数据集的完整实验研究，并得到了高性能的结果。
</details></li>
</ul>
<hr>
<h2 id="Taming-Gradient-Variance-in-Federated-Learning-with-Networked-Control-Variates"><a href="#Taming-Gradient-Variance-in-Federated-Learning-with-Networked-Control-Variates" class="headerlink" title="Taming Gradient Variance in Federated Learning with Networked Control Variates"></a>Taming Gradient Variance in Federated Learning with Networked Control Variates</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17200">http://arxiv.org/abs/2310.17200</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingyan Chen, Yaling Liu, Huaming Du, Mu Wang, Yu Zhao</li>
<li>For: 这个研究旨在解决联合学习中的问题，包括广泛的通信开销、慢态变化和不稳定的改善。这些问题主要导因于变量 gradient 由于客户端数据分布不均匀。* Methods: 这个研究提出了一个名为 FedNCV 的联合学习框架，采用了 REINFORCE Leave-One-Out (RLOO) 作为基本控制量单元，实现在客户端和服务器两个层次。在客户端上，RLOO 控制量单元用于优化本地梯度更新，减少由数据样本引入的变量。一旦传递到服务器端，RLOO 基本估计又提供了不偏且低变量的总梯度，导致Robust global更新。这个双面应用可以理解为对于客户端和服务器端的线性结合。我们提供了一个数学表达式，捕捉了这个组合的双控制量单元在 FedNCV 中的组合。* Results: 这个研究在六个多样的数据集上进行了六个 SOTA 方法的比较，以及该研究的性能优势。结果显示，FedNCV 具有较高的性能，并且可以实现大规模应用。<details>
<summary>Abstract</summary>
Federated learning, a decentralized approach to machine learning, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. These challenges primarily stem from the gradient variance due to heterogeneous client data distributions. To address this, we introduce a novel Networked Control Variates (FedNCV) framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO) as a fundamental control variate unit in the FedNCV framework, implemented at both client and server levels. At the client level, the RLOO control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. Once relayed to the server, the RLOO-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. This dual-side application is formalized as a linear combination of composite control variates. We provide a mathematical expression capturing this integration of double control variates within FedNCV and present three theoretical results with corresponding proofs. This unique dual structure equips FedNCV to address data heterogeneity and scalability issues, thus potentially paving the way for large-scale applications. Moreover, we tested FedNCV on six diverse datasets under a Dirichlet distribution with {\alpha} = 0.1, and benchmarked its performance against six SOTA methods, demonstrating its superiority.
</details>
<details>
<summary>摘要</summary>
federated learning, a decentralized machine learning approach, faces significant challenges such as extensive communication overheads, slow convergence, and unstable improvements. these challenges primarily stem from the gradient variance due to heterogeneous client data distributions. to address this, we introduce a novel networked control variates (fedncov) framework for federated learning. we adopt the reinforce leave-one-out (rloo) as a fundamental control variate unit in the fedncov framework, implemented at both client and server levels. at the client level, the rloo control variate is employed to optimize local gradient updates, mitigating the variance introduced by data samples. once relayed to the server, the rloo-based estimator further provides an unbiased and low-variance aggregated gradient, leading to robust global updates. this dual-side application is formalized as a linear combination of composite control variates. we provide a mathematical expression capturing this integration of double control variates within fedncov and present three theoretical results with corresponding proofs. this unique dual structure equips fedncov to address data heterogeneity and scalability issues, thus potentially paving the way for large-scale applications. moreover, we tested fedncov on six diverse datasets under a dirichlet distribution with α = 0.1, and benchmarked its performance against six sota methods, demonstrating its superiority.
</details></li>
</ul>
<hr>
<h2 id="How-do-Language-Models-Bind-Entities-in-Context"><a href="#How-do-Language-Models-Bind-Entities-in-Context" class="headerlink" title="How do Language Models Bind Entities in Context?"></a>How do Language Models Bind Entities in Context?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17191">http://arxiv.org/abs/2310.17191</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiahai Feng, Jacob Steinhardt</li>
<li>for: 这篇论文旨在探讨语言模型（LM）如何在上下文中使用符号知识，具体来说是如何将形态绑定到其特征上。</li>
<li>methods: 这篇论文使用了 causal intervention 技术，来检查 LM 内部活动是否表示绑定信息，并发现了绑定 ID 机制，即在大型 Pythia 和 LLaMA 模型中每一个模型都具有解决绑定问题的一致性机制。</li>
<li>results: 研究发现，LM 的内部活动实际上将形态绑定到其特征上，并且绑定 ID 向量组成一个连续的子空间，在这个子空间中，绑定 ID 向量之间的距离反映了它们的推理程度。总的来说，这些结果揭示了 LM 在上下文中表示符号知识的可读性策略，为大规模 LM 的上下文理解提供了一个重要的步阶。<details>
<summary>Abstract</summary>
To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>为正确地使用上下文信息，语言模型（LM）必须将实体绑定到其属性上。例如，在一个描述绿色正方形和蓝色圆形的上下文中，LM必须将形状绑定到它们的相应颜色上。我们分析LM表示形式和识别绑定机制：一种通用的解决绑定问题的机制，我们在Pyythia和LLaMA家族中的每个足够大的模型中都观察到。使用 causal intervention，我们表明LM内部的活动表示绑定信息，将绑定ID向量附加到对应的实体和属性上。我们还表明绑定ID向量组成一个连续的子空间，在这个子空间中，绑定ID向量之间的距离反映它们的推理程度。总之，我们的结果揭示了LM在含义上的具体推理策略，提供了解决大规模LM的普遍性含义理解的一个步骤。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Effects-of-Projectors-in-Knowledge-Distillation"><a href="#Understanding-the-Effects-of-Projectors-in-Knowledge-Distillation" class="headerlink" title="Understanding the Effects of Projectors in Knowledge Distillation"></a>Understanding the Effects of Projectors in Knowledge Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17183">http://arxiv.org/abs/2310.17183</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenyd7/pefd">https://github.com/chenyd7/pefd</a></li>
<li>paper_authors: Yudong Chen, Sen Wang, Jiajun Liu, Xuwei Xu, Frank de Hoog, Brano Kusy, Zi Huang</li>
<li>for: 这篇论文旨在调查隐藏在知识储存过程中的投影器（feature distillation）的作用，即使学生和教师网络具有相同的特征维度。</li>
<li>methods: 该论文使用了预训练的教师网络和学生网络，并在学生网络中添加了投影器来进行特征转换。</li>
<li>results: 该研究发现，即使学生和教师网络具有相同的特征维度，投影器仍然能够提高知识储存性能。此外，投影器甚至在逻辑分布式学习中也能够提高性能。这些发现驱动了 authors 提出了一种基于投影器集合的特征储存方法，以进一步提高知识储存性能。<details>
<summary>Abstract</summary>
Conventionally, during the knowledge distillation process (e.g. feature distillation), an additional projector is often required to perform feature transformation due to the dimension mismatch between the teacher and the student networks. Interestingly, we discovered that even if the student and the teacher have the same feature dimensions, adding a projector still helps to improve the distillation performance. In addition, projectors even improve logit distillation if we add them to the architecture too. Inspired by these surprising findings and the general lack of understanding of the projectors in the knowledge distillation process from existing literature, this paper investigates the implicit role that projectors play but so far have been overlooked. Our empirical study shows that the student with a projector (1) obtains a better trade-off between the training accuracy and the testing accuracy compared to the student without a projector when it has the same feature dimensions as the teacher, (2) better preserves its similarity to the teacher beyond shallow and numeric resemblance, from the view of Centered Kernel Alignment (CKA), and (3) avoids being over-confident as the teacher does at the testing phase. Motivated by the positive effects of projectors, we propose a projector ensemble-based feature distillation method to further improve distillation performance. Despite the simplicity of the proposed strategy, empirical results from the evaluation of classification tasks on benchmark datasets demonstrate the superior classification performance of our method on a broad range of teacher-student pairs and verify from the aspects of CKA and model calibration that the student's features are of improved quality with the projector ensemble design.
</details>
<details>
<summary>摘要</summary>
通常在知识塑化过程中（例如特征塑化），需要添加一个投影器来实现特征转换，因为教师和学生网络的维度不匹配。 Interestingly, we discovered that even if the student and the teacher have the same feature dimensions, adding a projector still helps to improve the distillation performance. In addition, projectors even improve logit distillation if we add them to the architecture too.  inspirited by these surprising findings and the general lack of understanding of the projectors in the knowledge distillation process from existing literature, this paper investigates the implicit role that projectors play but so far have been overlooked. Our empirical study shows that the student with a projector (1) obtains a better trade-off between the training accuracy and the testing accuracy compared to the student without a projector when it has the same feature dimensions as the teacher, (2) better preserves its similarity to the teacher beyond shallow and numeric resemblance, from the view of Centered Kernel Alignment (CKA), and (3) avoids being over-confident as the teacher does at the testing phase. Motivated by the positive effects of projectors, we propose a projector ensemble-based feature distillation method to further improve distillation performance. Despite the simplicity of the proposed strategy, empirical results from the evaluation of classification tasks on benchmark datasets demonstrate the superior classification performance of our method on a broad range of teacher-student pairs and verify from the aspects of CKA and model calibration that the student's features are of improved quality with the projector ensemble design.
</details></li>
</ul>
<hr>
<h2 id="Graphical-Object-Centric-Actor-Critic"><a href="#Graphical-Object-Centric-Actor-Critic" class="headerlink" title="Graphical Object-Centric Actor-Critic"></a>Graphical Object-Centric Actor-Critic</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17178">http://arxiv.org/abs/2310.17178</a></li>
<li>repo_url: None</li>
<li>paper_authors: Leonid Ugadiarov, Aleksandr I. Panov</li>
<li>for: 提高image-based object-centric reinforcement learning任务中的策略学习效果</li>
<li>methods: 使用actor-critic和model-based方法，使用transformer编码器提取对象表示，使用图 neural network逼近环境动力学</li>
<li>results: 在3D机器人环境和2Dcompositional结构环境中表现较为出色，比对state-of-the-art模型自由actor-critic算法和monolithic模型基础算法更好<details>
<summary>Abstract</summary>
There have recently been significant advances in the problem of unsupervised object-centric representation learning and its application to downstream tasks. The latest works support the argument that employing disentangled object representations in image-based object-centric reinforcement learning tasks facilitates policy learning. We propose a novel object-centric reinforcement learning algorithm combining actor-critic and model-based approaches to utilize these representations effectively. In our approach, we use a transformer encoder to extract object representations and graph neural networks to approximate the dynamics of an environment. The proposed method fills a research gap in developing efficient object-centric world models for reinforcement learning settings that can be used for environments with discrete or continuous action spaces. Our algorithm performs better in a visually complex 3D robotic environment and a 2D environment with compositional structure than the state-of-the-art model-free actor-critic algorithm built upon transformer architecture and the state-of-the-art monolithic model-based algorithm.
</details>
<details>
<summary>摘要</summary>
近来，无监督物体归一表示学习问题得到了重要进展，以及其应用于下游任务中。最新的研究证明了使用分离的物体表示在图像基本的反馈学习任务中帮助策略学习。我们提出了一种新的物体中心的奖励学习算法，将actor-critic和模型基础方法结合起来，以有效利用这些表示。在我们的方法中，我们使用变换器编码器提取物体表示，并使用图 neural network来近似环境的动态。我们的方法填充了奖励学习设置中的物体中心世界模型的研究空白，可以用于具有离散或连续动作空间的环境。我们的算法在三维 робоット环境和二维 Compositional 结构环境中表现更好than当前无监督actor-critic算法和单一模型基础算法。
</details></li>
</ul>
<hr>
<h2 id="Bridging-The-Gaps-Between-Token-Pruning-and-Full-Pre-training-via-Masked-Fine-tuning"><a href="#Bridging-The-Gaps-Between-Token-Pruning-and-Full-Pre-training-via-Masked-Fine-tuning" class="headerlink" title="Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning"></a>Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17177">http://arxiv.org/abs/2310.17177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fengyuan Shi, Limin Wang</li>
<li>for: 提高适应性和抗遮挡能力，使基本模型更适合用于动态图像转换器的初始化。</li>
<li>methods: 使用Masked Fine-Tuning方法，将预训练基本模型与动态图像转换器的token减少策略相匹配，从而解决基本模型与动态模型之间的不一致问题。</li>
<li>results: 对ImageNet dataset进行了广泛的实验，显示了基本模型通过Masked Fine-Tuning方法获得了强大的遮挡Robustness和信息损失能力，并且Dynamic ViT在不同的token减少比例下（例如0.8和0.3）获得了更高的准确率。<details>
<summary>Abstract</summary>
Despite the success of transformers on various computer vision tasks, they suffer from excessive memory and computational cost. Some works present dynamic vision transformers to accelerate inference by pruning redundant tokens. A key to improving token pruning is using well-trained models as initialization for faster convergence and better performance. However, current base models usually adopt full image training, i.e., using full images as inputs and keeping the whole feature maps through the forward process, which causes inconsistencies with dynamic models that gradually reduce tokens, including calculation pattern, information amount and token selection strategy inconsistencies. Inspired by MAE which performs masking and reconstruction self-supervised task, we devise masked fine-tuning to bridge the gaps between pre-trained base models used for initialization and token pruning based dynamic vision transformers, by masking image patches and predicting the image class label based on left unmasked patches. Extensive experiments on ImageNet demonstrate that base models via masked fine-tuning gain strong occlusion robustness and ability against information loss. With this better initialization, Dynamic ViT achieves higher accuracies, especially under large token pruning ratios (e.g., 81.9% vs. 81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3). Moreover, we apply our method into different token pruning based dynamic vision transformers, different pre-trained models and randomly initialized models to demonstrate the generalization ability.
</details>
<details>
<summary>摘要</summary>
尽管变换器在各种计算机视觉任务上取得了成功，但它们受到过度的内存和计算成本的束缚。一些工作提出了动态视觉转换器来加速推理，其中一个关键是使用已经训练过的模型作为初始化以更快地达到更好的性能。然而，当前的基本模型通常采用全像训练，即将全像作为输入，并保留整个特征图进行前进计算，这会导致动态模型逐渐减少token的问题，包括计算模式、信息量和选择策略不一致。受到MAE的启发，我们设计了彩色精度调整来bridging基本模型和动态视觉转换器之间的差异，通过遮盖图像块并预测图像类别标签基于未遮盖的块来进行彩色精度调整。我们在ImageNet上进行了广泛的实验，发现基于彩色精度调整的基本模型在遮盖率较高时（例如0.8和0.3）获得了强大的遮盖异常和信息损失能力。这些更好的初始化使得动态ViT在不同的token遮盖比例（例如81.9% vs. 81.3%,和62.3% vs. 58.9%）上取得了更高的准确率。此外，我们还应用了我们的方法到不同的token遮盖基于动态视觉转换器、不同的预训练模型和随机初始化模型，以 demonstrate其通用性。
</details></li>
</ul>
<hr>
<h2 id="A-Deep-Learning-Approach-to-Teeth-Segmentation-and-Orientation-from-Panoramic-X-rays"><a href="#A-Deep-Learning-Approach-to-Teeth-Segmentation-and-Orientation-from-Panoramic-X-rays" class="headerlink" title="A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays"></a>A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17176">http://arxiv.org/abs/2310.17176</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mrinal054/instance_teeth_segmentation">https://github.com/mrinal054/instance_teeth_segmentation</a></li>
<li>paper_authors: Mrinal Kanti Dhar, Mou Deb, D. Madhab, Zeyun Yu</li>
<li>for: 这篇研究旨在提高现代口腔健康预算中的精确牙齿分类和方位测量，以便精确诊断、治疗规划和 dental implant 设计。</li>
<li>methods: 我们使用了深度学习技术，基于 FUSegNet 模型，并将其改进为具有格子基于注意门的 skip connections。我们还引入了 Orientated bounding box (OBB) 生成，通过主成分分析 (PCA)，以精确地 Orient 牙齿。</li>
<li>results: 我们在公开的 DNS 资料集上评估了我们的方法，包括 543 枚 panoramic X-ray 图像，得到了 teeth 实例分类中的最高 Intersection-over-Union (IoU) 分数 82.43%，Dice Similarity Coefficient (DSC) 分数 90.37%，以及 Rotated IoU (RIoU) 分数 82.82%。我们还进行了各个牙齿标签和分类性能的详细分析，为未来口腔预算中的精确诊断、治疗规划和个性化医疗带来了promising prospects。<details>
<summary>Abstract</summary>
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We also conduct detailed analyses of individual tooth labels and categorical performance, shedding light on strengths and weaknesses. The proposed model's accuracy and versatility offer promising prospects for improving dental diagnoses, treatment planning, and personalized healthcare in the oral domain. Our generated OBB coordinates and codes are available at https://github.com/mrinal054/Instance_teeth_segmentation.
</details>
<details>
<summary>摘要</summary>
准确的牙齿分割和方向是现代口腔医疗中的基本要求，帮助确定精准诊断、治疗规划和植入设计。在本研究中，我们提出了一种涵盖所有牙齿分割和方向的全面方法，基于FUSegNet模型，并通过栅格基于注意机制的修改来提高性能。我们还引入了原则components分析（PCA）来生成方向 bounding box（OBB），以便精准地 Orient estimation。在公共可用的 DNS 数据集上评估我们的方法，包括 543 张扫描图像，我们达到了 teeth 实例分割中最高的 Intersection-over-Union（IoU）分数（82.43%）和 Dice Similarity Coefficient（DSC）分数（90.37%），同时在 OBB 分析中获得了 Rotated IoU（RIoU）分数（82.82%）。我们还进行了精度分析，探讨个体牙齿标签和分类性能，为口腔医疗领域的个性化医疗带来了推荐的前景。我们在 GitHub 上公布了生成的 OBB 坐标和代码，请参考 https://github.com/mrinal054/Instance_teeth_segmentation。
</details></li>
</ul>
<hr>
<h2 id="Improving-Denoising-Diffusion-Models-via-Simultaneous-Estimation-of-Image-and-Noise"><a href="#Improving-Denoising-Diffusion-Models-via-Simultaneous-Estimation-of-Image-and-Noise" class="headerlink" title="Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise"></a>Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17167">http://arxiv.org/abs/2310.17167</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhenkai Zhang, Krista A. Ehinger, Tom Drummond</li>
<li>for: 本 paper 的两个主要贡献是提高反扩散过程中图像生成的速度和质量。</li>
<li>methods: 本 paper 使用了两种方法来提高图像生成的速度和质量，第一种是将扩散过程重parameterized为图像和噪声之间的角度，第二种是直接使用网络来估算图像和噪声的值。</li>
<li>results: 根据 Frechet Inception Distance (FID)、spatial Frechet Inception Distance (sFID)、精度和回归率等指标，本 paper 的模型可以更快地生成高质量的图像，并且可以更快地达到高质量图像。<details>
<summary>Abstract</summary>
This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\displaystyle \sqrt{\bar{\alpha}=\cos(\eta)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary differential equation (ODE). In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be used effectively. The second contribution is to directly estimate both the image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which enables more stable calculations of the update step in the inverse diffusion steps, as accurate estimation of both the image and noise are crucial at different stages of the process. Together with these changes, our model achieves faster generation, with the ability to converge on high-quality images more quickly, and higher quality of the generated images, as measured by metrics such as Frechet Inception Distance (FID), spatial Frechet Inception Distance (sFID), precision, and recall.
</details>
<details>
<summary>摘要</summary></li>
</ul>
<ol>
<li>Reparameterizing the diffusion process: Instead of using the conventional $\sqrt{\bar{\alpha} &#x3D; \cos(\eta)$, we parameterize the diffusion process in terms of the angle between the image and noise on a quarter-circular arc. This eliminates two singularities and allows the diffusion evolution to be expressed as a well-behaved ordinary differential equation (ODE), making it possible to use higher-order ODE solvers such as Runge-Kutta methods.2. Direct estimation of image and noise: Our network directly estimates both the image and noise, which ensures more stable calculations of the update step in the inverse diffusion process. Accurate estimation of both the image and noise is crucial at different stages of the process, and our model achieves faster generation of high-quality images, as measured by metrics such as Frechet Inception Distance (FID), spatial Frechet Inception Distance (sFID), precision, and recall.</details></li>
</ol>
<hr>
<h2 id="Content-based-Controls-For-Music-Large-Language-Modeling"><a href="#Content-based-Controls-For-Music-Large-Language-Modeling" class="headerlink" title="Content-based Controls For Music Large Language Modeling"></a>Content-based Controls For Music Large Language Modeling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17162">http://arxiv.org/abs/2310.17162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liwei Lin, Gus Xia, Junyan Jiang, Yixiao Zhang</li>
<li>for: 这个论文旨在提供一种基于内容的控制方法，以提高大规模语言模型在音乐频域中的音乐生成质量。</li>
<li>methods: 该方法使用一种效率高的参数调整方法（PEFT），专门针对基于转换器的音频模型。实验表明，我们的方法可以在具有少量超级vised学习的情况下实现高质量的音乐生成，并且可以具有有效的内容基于的控制能力。</li>
<li>results: 我们的方法可以实现高质量的音乐生成，并且可以通过调整旋律和和声来实现有效的内容基于的控制。此外，我们还示出了将内容基于的控制与文本描述结合使用可以实现灵活的音乐变化生成和风格传递。<details>
<summary>Abstract</summary>
Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and training on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls, and we illustrate the control power via chords and rhythms, two of the most salient features of music audio. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and style transfer. Our source codes and demos are available online.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "large-scale language models" 大规模语言模型 (dà xiǎng móde lǐng yǔ)* "end-to-end generation" 端到端生成 (dían dào diàn chéng)* "conditioned generation" 受控生成 (fù kòng shēng chéng)* "text descriptions" 文本描述 (wén tiěn mǐng yù)* "meta-data" 元数据 (yuán jí)* "singers and instruments" 歌手和乐器 (gē shǒu hé yuè qì)* "genre and emotion" 种类和情感 (zhòng lèi hé qíng gǎn)* "innate music languages" Native Music Languages (yuán jì yǔ)* "pitch, chords, and drum tracks" 抑弹、和弹、鼓踏 (zuò dì, hé dì, gǔ tà)* "parameter-efficient fine-tuning" 参数高效精度调整 (cèshù gāodégòu jīngdé jiǎo)* "Transformer-based audio models" 基于Transformer的音频模型 (jī yú Transformer de yīn yǐn módel)* "low-resource semi-supervised learning" 半指导式半资源学习 (bàn zhǐdǎo xī bàn zīyuán xuéxí)* "tuning with less than 4% parameters" 使用少于4%参数调整 (shǐyòu xiǎo yú 4% cèshù jiǎo)* "training on a small dataset" 使用小 datasets 训练 (shǐyòu xiǎo dataset zhīngxì)* "fewest than 300 songs"  fewer than 300 songs (liǎo xiǎo gē)* "content-based controls" 内容基于的控制 (néngyòu jīyào de kòng zhì)* "chords and rhythms" 和弹和节奏 (hé dì yǔ jié zhù)* "flexible music variation generation" 灵活的音乐变换 (língyòu de yīn yuè biàn huà)* "style transfer" 风格传递 (fēngxìng chuándòu)
</details></li>
</ul>
<hr>
<h2 id="CosmosDSR-–-a-methodology-for-automated-detection-and-tracking-of-orbital-debris-using-the-Unscented-Kalman-Filter"><a href="#CosmosDSR-–-a-methodology-for-automated-detection-and-tracking-of-orbital-debris-using-the-Unscented-Kalman-Filter" class="headerlink" title="CosmosDSR – a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter"></a>CosmosDSR – a methodology for automated detection and tracking of orbital debris using the Unscented Kalman Filter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17158">http://arxiv.org/abs/2310.17158</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniel S. Roll, Zeyneb Kurt, Wai Lok Woo</li>
<li>for: Addressing the Kessler syndrome by detecting and tracking satellites in sequential images.</li>
<li>methods: Combining YOLOv3 with an Unscented Kalman Filter (UKF) for tracking satellites, and comparing with a linear Kalman filter (LKF).</li>
<li>results: Precise detection and classification of satellite categories with few errors, and accurate tracking of satellites with a mean squared error (MSE) and root mean squared error (RMSE) of 2.83&#x2F;1.66 for UKF and 2.84&#x2F;1.66 for LKF.<details>
<summary>Abstract</summary>
The Kessler syndrome refers to the escalating space debris from frequent space activities, threatening future space exploration. Addressing this issue is vital. Several AI models, including Convolutional Neural Networks, Kernel Principal Component Analysis, and Model-Agnostic Meta- Learning have been assessed with various data types. Earlier studies highlighted the combination of the YOLO object detector and a linear Kalman filter (LKF) for object detection and tracking. Advancing this, the current paper introduces a novel methodology for the Comprehensive Orbital Surveillance and Monitoring Of Space by Detecting Satellite Residuals (CosmosDSR) by combining YOLOv3 with an Unscented Kalman Filter (UKF) for tracking satellites in sequential images. Using the Spacecraft Recognition Leveraging Knowledge of Space Environment (SPARK) dataset for training and testing, the YOLOv3 precisely detected and classified all satellite categories (Mean Average Precision=97.18%, F1=0.95) with few errors (TP=4163, FP=209, FN=237). Both CosmosDSR and an implemented LKF used for comparison tracked satellites accurately for a mean squared error (MSE) and root mean squared error (RME) of MSE=2.83/RMSE=1.66 for UKF and MSE=2.84/RMSE=1.66 for LKF. The current study is limited to images generated in a space simulation environment, but the CosmosDSR methodology shows great potential in detecting and tracking satellites, paving the way for solutions to the Kessler syndrome.
</details>
<details>
<summary>摘要</summary>
《凯斯勒征》指的是由于频繁的空间活动而导致的增加的空间废弃物，这对未来的空间探索造成了威胁。为解决这个问题，许多人使用了人工智能模型，包括卷积神经网络、基准 principl component analysis 和模型无关元学习。在之前的研究中，拟合了 YOLO 对象检测器和线性 Kalman 筛（LKF）的结合，用于对象检测和跟踪。现在的论文介绍了一种新的方法，即 CosmosDSR，它将 YOLOv3 与不确定 Kalman 筛（UKF）结合，用于在顺序图像中跟踪卫星。使用 SPARK 数据集进行训练和测试，YOLOv3 精确地检测和分类了所有卫星类别（平均精度=97.18%, F1=0.95），只有一些错误（TP=4163, FP=209, FN=237）。两种 CosmosDSR 和 LKF 的实现都可以准确地跟踪卫星，MSE 和 RMSE 分别为 MSE=2.83/RMSE=1.66。当前的研究只是在空间模拟环境中生成的图像上进行的，但 CosmosDSR 方法具有很好的潜在性，可以用于检测和跟踪卫星，为解决凯斯勒征提供了新的解决方案。
</details></li>
</ul>
<hr>
<h2 id="Technical-Note-Feasibility-of-translating-3-0T-trained-Deep-Learning-Segmentation-Models-Out-of-the-Box-on-Low-Field-MRI-0-55T-Knee-MRI-of-Healthy-Controls"><a href="#Technical-Note-Feasibility-of-translating-3-0T-trained-Deep-Learning-Segmentation-Models-Out-of-the-Box-on-Low-Field-MRI-0-55T-Knee-MRI-of-Healthy-Controls" class="headerlink" title="Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls"></a>Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17152">http://arxiv.org/abs/2310.17152</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rupsa Bhattacharjee, Zehra Akkaya, Johanna Luitjens, Pan Su, Yang Yang, Valentina Pedoia, Sharmila Majumdar<br>for: 这项研究的目的是评估将深度学习技术应用于评估双下肢骨骼标记的可能性，并将其应用于健康控制人群的0.55T MR 影像中。methods: 这项研究使用了标准的实践中的骨和软组织分割算法，并对其进行质量和量化的评估，以确定在0.55T和3.0T之间的差异。results: 初步结果表明，可以将现有的高级深度学习图像分割技术，训练在3.0T上，翻译到0.55T上，并在多 vendor 环境中实现可用到良好的技术可行性。尤其是在分割软组织 compartment 方面，模型表现几乎相当于3.0T。这表明，0.55T低场磁共振成像可以用于评估双下肢骨骼标记，并且可以通过使用现有的深度学习图像分割技术来提高表征性。<details>
<summary>Abstract</summary>
In the current study, our purpose is to evaluate the feasibility of applying deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in healthy controls scanned at 0.55T and compared with 3.0T. The current study assesses the performance of standard in-practice bone, and cartilage segmentation algorithms at 0.55T, both qualitatively and quantitatively, in terms of comparing segmentation performance, areas of improvement, and compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial results demonstrate a usable to good technical feasibility of translating existing quantitative deep-learning-based image segmentation techniques, trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition environment. Especially in terms of segmenting cartilage compartments, the models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be utilized for evaluating knee cartilage thickness and bone segmentations aided by established DL algorithms trained at higher-field strengths out-of-the-box initially. This could be utilized at the far-spread point-of-care locations with a lack of radiologists available to manually segment low-field images, at least till a decent base of low-field data pool is collated. With further fine-tuning with manual labeling of low-field data or utilizing synthesized higher SNR images from low-field images, OA biomarker quantification performance is potentially guaranteed to be further improved.
</details>
<details>
<summary>摘要</summary>
当前研究的目的是评估使用深度学习（DL）启用算法来评估双侧膝关节生物标志物理量的可能性。研究现在评估0.55T中标准实践骨和软组织分割算法的性能，包括对比分割性能、改进方向和软组织厚度值 между0.55T和3.0T。初步结果表明可以将已经训练在3.0T上的量化深度学习图像分割技术翻译到0.55T，并在多 vendor acquisition 环境中实现了技术可行性。特别是在分割软组织COMPARTMENT中，模型表现了几乎相同的Likert排名。因此，0.55T的低场可持续和易于安装的MRI可以用于评估膝软组织厚度和骨分割，并且可以通过已有的DL算法在更高的场 strengths 中进行外部调试。这可以在覆盖医疗机构的各个点批处理地点使用，至少ntil a decent base of low-field data pool is collated。通过进一步细化的手动标注低场数据或使用生成的更高SNR图像来进行优化，OA生物标志量的评估性能可能会得到进一步改进。
</details></li>
</ul>
<hr>
<h2 id="Explainable-Spatio-Temporal-Graph-Neural-Networks"><a href="#Explainable-Spatio-Temporal-Graph-Neural-Networks" class="headerlink" title="Explainable Spatio-Temporal Graph Neural Networks"></a>Explainable Spatio-Temporal Graph Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17149">http://arxiv.org/abs/2310.17149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hkuds/stexplainer">https://github.com/hkuds/stexplainer</a></li>
<li>paper_authors: Jiabin Tang, Lianghao Xia, Chao Huang</li>
<li>for: 这个论文的目的是提出一个可解释的城市空间时间图预测模型（STExplainer），以增强城市 aplicatons 中的内置可解释性。</li>
<li>methods: 这个模型使用了一个统一的城市空间图注意力网络（STGNN），加上一个位置信息融合层，以解决城市空间时间资料的黑盒问题。此外，我们还提出了一个结构炼分法，基于图形信息瓶颈（GIB）原则，并使用了一个可解释的目标函数。</li>
<li>results: 经过广泛的实验，我们证明了我们的 STExplainer 模型在交通和犯罪预测任务上比基于state-of-the-art 的基础模型表现更好，并且在预测精度和可解释度（例如给定和实际）方面均达到了优秀的成绩。此外，我们的模型还能够有效地解决资料缺失和稀疏性问题。<details>
<summary>Abstract</summary>
Spatio-temporal graph neural networks (STGNNs) have gained popularity as a powerful tool for effectively modeling spatio-temporal dependencies in diverse real-world urban applications, including intelligent transportation and public safety. However, the black-box nature of STGNNs limits their interpretability, hindering their application in scenarios related to urban resource allocation and policy formulation. To bridge this gap, we propose an Explainable Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances STGNNs with inherent explainability, enabling them to provide accurate predictions and faithful explanations simultaneously. Our framework integrates a unified spatio-temporal graph attention network with a positional information fusion layer as the STG encoder and decoder, respectively. Furthermore, we propose a structure distillation approach based on the Graph Information Bottleneck (GIB) principle with an explainable objective, which is instantiated by the STG encoder and decoder. Through extensive experiments, we demonstrate that our STExplainer outperforms state-of-the-art baselines in terms of predictive accuracy and explainability metrics (i.e., sparsity and fidelity) on traffic and crime prediction tasks. Furthermore, our model exhibits superior representation ability in alleviating data missing and sparsity issues. The implementation code is available at: https://github.com/HKUDS/STExplainer.
</details>
<details>
<summary>摘要</summary>
随着城市应用的多样化和复杂性的增加，随时空 Graph Neural Networks (STGNNs) 已经得到了广泛的应用，以模型城市中的随时空关系。然而，黑盒模型的限制使得 STGNNs 的解释性受到限制，从而阻碍其在城市资源分配和政策制定方面的应用。为了bridging这个鸿沟，我们提出了一个可解释的随时空 Graph Neural Networks (STExplainer) 框架，该框架可以增强 STGNNs 的解释性，使其同时提供高准确率和 faithful 的预测和解释。我们的框架包括一个统一的随时空 Graph attention网络和一个位置信息融合层作为 STG Encoder 和 Decoder，分别。此外，我们提出了基于 Graph Information Bottleneck (GIB) 原理的结构填充方法，该方法通过一个可解释的目标函数实现。经过广泛的实验，我们证明了我们的 STExplainer 在交通预测和犯罪预测任务上的预测精度和解释性指标（即稀疏性和准确性）高于当前基线。此外，我们的模型在缺失数据和稀疏性问题下的表示能力也更高。代码可以在 GitHub 上获取：https://github.com/HKUDS/STExplainer。
</details></li>
</ul>
<hr>
<h2 id="Counterfactual-Augmented-Importance-Sampling-for-Semi-Offline-Policy-Evaluation"><a href="#Counterfactual-Augmented-Importance-Sampling-for-Semi-Offline-Policy-Evaluation" class="headerlink" title="Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation"></a>Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17146">http://arxiv.org/abs/2310.17146</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mld3/counterfactualannot-semiope">https://github.com/mld3/counterfactualannot-semiope</a></li>
<li>paper_authors: Shengpu Tang, Jenna Wiens</li>
<li>for: 这个论文是用于推广强化学习（RL）在高风险领域的应用，并通过观察数据进行量化和质量evaluation，以帮助实践者理解新策略的总体性能。</li>
<li>methods: 这个论文提出了一种半在线评估框架，通过询问人类用户提供不可见的对比性轨迹的注释，以帮助解决在线评估不可能进行的问题。</li>
<li>results: 这个论文的实验表明，相比标准的强化学习评估器，该半在线评估框架可以减少偏见和噪声，并且在不完整的注释情况下表现更加稳定。<details>
<summary>Abstract</summary>
In applying reinforcement learning (RL) to high-stakes domains, quantitative and qualitative evaluation using observational data can help practitioners understand the generalization performance of new policies. However, this type of off-policy evaluation (OPE) is inherently limited since offline data may not reflect the distribution shifts resulting from the application of new policies. On the other hand, online evaluation by collecting rollouts according to the new policy is often infeasible, as deploying new policies in these domains can be unsafe. In this work, we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories. While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results. Instead, we design a new family of OPE estimators based on importance sampling (IS) and a novel weighting scheme that incorporate counterfactual annotations without introducing additional bias. We analyze the theoretical properties of our approach, showing its potential to reduce both bias and variance compared to standard IS estimators. Our analyses reveal important practical considerations for handling biased, noisy, or missing annotations. In a series of proof-of-concept experiments involving bandits and a healthcare-inspired simulator, we demonstrate that our approach outperforms purely offline IS estimators and is robust to imperfect annotations. Our framework, combined with principled human-centered design of annotation solicitation, can enable the application of RL in high-stakes domains.
</details>
<details>
<summary>摘要</summary>
在应用强化学习（RL）到高风险领域时，可以使用观察数据进行量化和质量evaluation来帮助实践者理解新策略的总结性性能。然而，这种Off-policy评估（OPE）是由于新策略的应用而导致的分布变化的限制。相反，在线评估，通过根据新策略收集滚动数据，可以是不可靠的，因为在这些领域中部署新策略可能是不安全的。在这种情况下，我们提出了一种半Offline评估框架，作为在线和Offline评估之间的中间步骤，在这里，人类用户提供了未观察的contrastive Trajectory的注释。虽然有吸引力地将现有数据 augmented with这些注释，但我们表明这种Naive Approach可能会导致偏向结果。相反，我们设计了一种基于重要性抽样（IS）和一种新的权重方案的新家族OPE估计器，可以在不引入额外偏向的情况下，利用contrastive注释进行估计。我们分析了我们的方法的理论性质，并表明它在减少偏向和方差方面具有潜在的优势。我们的分析还揭示了在处理偏向、杂音或缺失注释时的重要实践考虑事项。在一系列Proof-of-concept实验中，我们示出了我们的方法可以在bandits和一种医疗领域的模拟器中出performances，并且可以抗护免着不完整的注释。我们的框架，结合人类中心的注释 solicitation设计，可以帮助RL在高风险领域应用。
</details></li>
</ul>
<hr>
<h2 id="Symbolic-Planning-and-Code-Generation-for-Grounded-Dialogue"><a href="#Symbolic-Planning-and-Code-Generation-for-Grounded-Dialogue" class="headerlink" title="Symbolic Planning and Code Generation for Grounded Dialogue"></a>Symbolic Planning and Code Generation for Grounded Dialogue</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17140">http://arxiv.org/abs/2310.17140</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/justinchiu/onecommon-gpt">https://github.com/justinchiu/onecommon-gpt</a></li>
<li>paper_authors: Justin T. Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried</li>
<li>for: 这个论文的目的是提出一种可组合和可解释的对话系统，以解决现有的对话系统在跟踪目标和处理新的grounding方面的缺陷。</li>
<li>methods: 该系统包括一个读取器和一个规划器：读取器使用大语言模型将对话伙伴的话语转换成可执行代码，并调用函数来完成grounding。符号规划器使用符号计划法确定下一个最佳回答。</li>
<li>results: 该系统在OneCommon对话任务中表现出色，成功率从56%提高到69%，在最复杂的设定下也有显著提升。<details>
<summary>Abstract</summary>
Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在处理和生成文本和代码方面表现出色，但LLM在固定任务对话中有限的应用可能性，主要是因为它们难以追导到任务目标并处理新的固定。我们提出了一个模块化和可解释的基于符号计划的对话系统，这个系统通过将LLM与符号计划和基于符号的代码执行结合起来，以解决这些缺点。我们的系统包括读者和计划器：读者使用LLM将伙伴的话语转换为执行代码，并调用函数来进行固定。转换后的代码的输出被存储以跟踪对话状态，而符号计划器根据对话状态确定下一个适当的回应。我们对一个具有抽象点云图像的OneCommon对话任务进行了评估，并substantially outperformed前一个状态的艺术。在最复杂的设定下，我们的系统的任务成功率从56%提高到69%。
</details></li>
</ul>
<hr>
<h2 id="Core-Challenge-2023-Solver-and-Graph-Descriptions"><a href="#Core-Challenge-2023-Solver-and-Graph-Descriptions" class="headerlink" title="Core Challenge 2023: Solver and Graph Descriptions"></a>Core Challenge 2023: Solver and Graph Descriptions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17136">http://arxiv.org/abs/2310.17136</a></li>
<li>repo_url: None</li>
<li>paper_authors: Takehide Soh, Tomoya Tanjo, Yoshio Okamoto, Takehiro Ito</li>
<li>for: 本研究收集了CoRe Challenge 2023中所提交的解决方案和ISR实例的描述。</li>
<li>methods: 本研究使用了各种解决方案和ISR实例来描述CoRe Challenge 2023中的问题。</li>
<li>results: 本研究收集了CoRe Challenge 2023中所有的解决方案和ISR实例，以便进行后续的分析和研究。<details>
<summary>Abstract</summary>
This paper collects all descriptions of solvers and ISR instances submitted to CoRe Challenge 2023.
</details>
<details>
<summary>摘要</summary>
这篇论文收集了2023年CoRe挑战中所提交的解决方案和实例。Note: "CoRe" stands for "Combinatorial Optimization and Recommendation" challenge.
</details></li>
</ul>
<hr>
<h2 id="Incorporating-Probing-Signals-into-Multimodal-Machine-Translation-via-Visual-Question-Answering-Pairs"><a href="#Incorporating-Probing-Signals-into-Multimodal-Machine-Translation-via-Visual-Question-Answering-Pairs" class="headerlink" title="Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs"></a>Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17133">http://arxiv.org/abs/2310.17133</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/libeineu/mmt-vqa">https://github.com/libeineu/mmt-vqa</a></li>
<li>paper_authors: Yuxin Zuo, Bei Li, Chuanhao Lv, Tong Zheng, Tong Xiao, Jingbo Zhu</li>
<li>for: 这篇论文研究了多Modal机器翻译（MMT）系统中文本输入完整性的影响，并提出了一种新的方法来促进cross-模态交互。</li>
<li>methods: 该论文提出了一种生成来源文本中Visual Question-Answering（VQA）样式对的方法，并使用Large Language Models（LLMs）来显式地模型MMT中的探测信号。</li>
<li>results: 实验结果表明，该新方法可以提高MMT系统的性能，并且可以帮助MMT系统更好地理解图像信息。<details>
<summary>Abstract</summary>
This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
</details>
<details>
<summary>摘要</summary>
The authors use Large Language Models (LLMs) to explicitly model the probing signal in MMT and convert it into VQA-style data, creating the Multi30K-VQA dataset. They then introduce an MMT-VQA multitask learning framework to incorporate explicit probing signals from the dataset into the MMT training process.Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. The code and data used in this study will be available at the following link: <https://github.com/libeineu/MMT-VQA>.
</details></li>
</ul>
<hr>
<h2 id="Unleashing-the-potential-of-GNNs-via-Bi-directional-Knowledge-Transfer"><a href="#Unleashing-the-potential-of-GNNs-via-Bi-directional-Knowledge-Transfer" class="headerlink" title="Unleashing the potential of GNNs via Bi-directional Knowledge Transfer"></a>Unleashing the potential of GNNs via Bi-directional Knowledge Transfer</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17132">http://arxiv.org/abs/2310.17132</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shuai Zheng, Zhizhe Liu, Zhenfeng Zhu, Xingxing Zhang, Jianxin Li, Yao Zhao</li>
<li>for: 提高 Graph Neural Network (GNN) 的性能。</li>
<li>methods: 利用 message-passing 框架中的 feature transformation 操作，提出 Bi-directional Knowledge Transfer (BiKT) 方法，以便不需修改原有架构即可充分发挥 GNN 的 potential。</li>
<li>results: 对 7 个数据集和 5 种常见 GNN 进行了广泛的实验，显示 BiKT 可以提高 GNN 的性能，最高提升达 0.5% - 4%，同时 derive 模型也能够独立应用于特定下游任务。<details>
<summary>Abstract</summary>
Based on the message-passing paradigm, there has been an amount of research proposing diverse and impressive feature propagation mechanisms to improve the performance of GNNs. However, less focus has been put on feature transformation, another major operation of the message-passing framework. In this paper, we first empirically investigate the performance of the feature transformation operation in several typical GNNs. Unexpectedly, we notice that GNNs do not completely free up the power of the inherent feature transformation operation. By this observation, we propose the Bi-directional Knowledge Transfer (BiKT), a plug-and-play approach to unleash the potential of the feature transformation operations without modifying the original architecture. Taking the feature transformation operation as a derived representation learning model that shares parameters with the original GNN, the direct prediction by this model provides a topological-agnostic knowledge feedback that can further instruct the learning of GNN and the feature transformations therein. On this basis, BiKT not only allows us to acquire knowledge from both the GNN and its derived model but promotes each other by injecting the knowledge into the other. In addition, a theoretical analysis is further provided to demonstrate that BiKT improves the generalization bound of the GNNs from the perspective of domain adaption. An extensive group of experiments on up to 7 datasets with 5 typical GNNs demonstrates that BiKT brings up to 0.5% - 4% performance gain over the original GNN, which means a boosted GNN is obtained. Meanwhile, the derived model also shows a powerful performance to compete with or even surpass the original GNN, enabling us to flexibly apply it independently to some other specific downstream tasks.
</details>
<details>
<summary>摘要</summary>
使用消息传递模式的基础，有很多研究提出了多样化和吸引人的特征传播机制，以提高GNN的性能。然而，对特征转换的研究相对较少。在这篇论文中，我们首先employs empirical investigation来研究GNN中特征转换操作的性能。我们发现，GNN并不完全利用特征转换操作的力量。通过这一发现，我们提出了双向知识传输（BiKT），一种可插入的扩展approach，以解 liberate特征转换操作的潜力。 BiKT通过将特征转换操作作为GNN中的derived representation learning模型，并将这两个模型共享参数，从而实现了从GNN中获得 topological-agnostic的知识反馈，以帮助GNN的学习和特征转换。此外，我们还提供了一个理论分析，以证明BiKT在适应领域中提高GNN的泛化范围。在7个数据集和5种典型的GNN上进行了广泛的实验，显示BiKT可以提高GNN的性能，从0.5%到4%不等。此外， derivated模型还能够独立地应用于其他特定的下游任务中，并且表现强劲。
</details></li>
</ul>
<hr>
<h2 id="Topic-Segmentation-of-Semi-Structured-and-Unstructured-Conversational-Datasets-using-Language-Models"><a href="#Topic-Segmentation-of-Semi-Structured-and-Unstructured-Conversational-Datasets-using-Language-Models" class="headerlink" title="Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models"></a>Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17120">http://arxiv.org/abs/2310.17120</a></li>
<li>repo_url: None</li>
<li>paper_authors: Reshmi Ghosh, Harjeet Singh Kajal, Sharanya Kamath, Dhuri Shrivastava, Samyadeep Basu, Hansi Zeng, Soundararajan Srinivasan</li>
<li>For: This paper focuses on analyzing the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts, and evaluating the effectiveness of different loss functions for improving segmentation results in unstructured conversational datasets.* Methods: The paper uses a variety of methods, including training from scratch with a small-sized dataset of the target unstructured domain, and experimenting with multiple loss functions (including Cross-Entropy, re-weighted Cross-Entropy, and Focal Loss) to mitigate the effects of imbalance in unstructured conversational datasets.* Results: The paper finds that training from scratch with a small-sized dataset of the target unstructured domain improves segmentation results by a significant margin, and that the Focal Loss function is a robust alternative to Cross-Entropy and re-weighted Cross-Entropy loss function when segmenting unstructured and semi-structured chats.Here’s the information in Simplified Chinese text:* For: 本文研究了现有state-of-the-art话题分割模型在不结构化文本上的泛化能力，并评估了不同损失函数在不结构化对话集中的性能。* Methods: 本文使用了许多方法，包括从scratch在目标不结构化频道上小型数据集上训练，以及使用多种损失函数（包括十字熵、重量十字熵和焦点损失）来减轻不结构化对话集中的不均衡问题。* Results: 本文发现，从scratch在小型数据集上训练可以大幅提高话题分割结果，而焦点损失函数在不结构化和半结构化对话中话题分割时表现出了良好的Robustness。<details>
<summary>Abstract</summary>
Breaking down a document or a conversation into multiple contiguous segments based on its semantic structure is an important and challenging problem in NLP, which can assist many downstream tasks. However, current works on topic segmentation often focus on segmentation of structured texts. In this paper, we comprehensively analyze the generalization capabilities of state-of-the-art topic segmentation models on unstructured texts. We find that: (a) Current strategies of pre-training on a large corpus of structured text such as Wiki-727K do not help in transferability to unstructured conversational data. (b) Training from scratch with only a relatively small-sized dataset of the target unstructured domain improves the segmentation results by a significant margin. We stress-test our proposed Topic Segmentation approach by experimenting with multiple loss functions, in order to mitigate effects of imbalance in unstructured conversational datasets. Our empirical evaluation indicates that Focal Loss function is a robust alternative to Cross-Entropy and re-weighted Cross-Entropy loss function when segmenting unstructured and semi-structured chats.
</details>
<details>
<summary>摘要</summary>
分析文档或对话的 semantic 结构，将其分解成多个连续的段落是 NLP 中一项重要和挑战性的问题，可以帮助多种下游任务。然而，现有的话题 segmentation 模型通常只关注结构化文本的 segmentation。在这篇论文中，我们全面分析了现代话题 segmentation 模型对非结构化文本的泛化能力。我们发现：(a) 现有的预训练方法，如使用 Wiki-727K 大量结构化文本集，对于非结构化对话数据的转移性不够。(b) 直接从 scratch 使用target域的小型数据集进行训练，可以大幅提高分 segmentation 结果。我们为了 Mitigate 非结构化对话集的不均衡问题，对我们的提议的话题 segmentation 方法进行了多种搅拌损失函数的实验。我们的实验表明，焦点损失函数是跨Entropy 和重量跨Entropy损失函数的稳定和可靠的替代方案。
</details></li>
</ul>
<hr>
<h2 id="Detecting-stealthy-cyberattacks-on-adaptive-cruise-control-vehicles-A-machine-learning-approach"><a href="#Detecting-stealthy-cyberattacks-on-adaptive-cruise-control-vehicles-A-machine-learning-approach" class="headerlink" title="Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach"></a>Detecting stealthy cyberattacks on adaptive cruise control vehicles: A machine learning approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17091">http://arxiv.org/abs/2310.17091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tianyi Li, Mingfeng Shang, Shian Wang, Raphael Stern<br>for:The paper is written to address the detection of cyberattacks on vehicles equipped with advanced driver-assistance systems (ADAS) and automated driving features.methods:The paper proposes a traffic model framework for three types of potential cyberattacks, and uses a novel generative adversarial network (GAN)-based anomaly detection model to identify such attacks in real-time using vehicle trajectory data.results:The paper provides numerical evidence to demonstrate the efficacy of the proposed machine learning approach in detecting cyberattacks on ACC-equipped vehicles, and compares the results against some recently proposed neural network models.<details>
<summary>Abstract</summary>
With the advent of vehicles equipped with advanced driver-assistance systems, such as adaptive cruise control (ACC) and other automated driving features, the potential for cyberattacks on these automated vehicles (AVs) has emerged. While overt attacks that force vehicles to collide may be easily identified, more insidious attacks, which only slightly alter driving behavior, can result in network-wide increases in congestion, fuel consumption, and even crash risk without being easily detected. To address the detection of such attacks, we first present a traffic model framework for three types of potential cyberattacks: malicious manipulation of vehicle control commands, false data injection attacks on sensor measurements, and denial-of-service (DoS) attacks. We then investigate the impacts of these attacks at both the individual vehicle (micro) and traffic flow (macro) levels. A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identification of such attacks using vehicle trajectory data. We provide numerical evidence {to demonstrate} the efficacy of our machine learning approach in detecting cyberattacks on ACC-equipped vehicles. The proposed method is compared against some recently proposed neural network models and observed to have higher accuracy in identifying anomalous driving behaviors of ACC vehicles.
</details>
<details>
<summary>摘要</summary>
A novel generative adversarial network (GAN)-based anomaly detection model is proposed for real-time identification of such attacks using vehicle trajectory data. We provide numerical evidence to demonstrate the efficacy of our machine learning approach in detecting cyberattacks on ACC-equipped vehicles. The proposed method is compared against some recently proposed neural network models and is found to have higher accuracy in identifying anomalous driving behaviors of ACC vehicles.Here is the text in Simplified Chinese:随着自动驾驶汽车（AV） équiponder with advanced driver-assistance systems（ADAS），如适应速度控制（ACC）和其他自动驾驶功能，攻击这些自动汽车的可能性已出现。而这些攻击可能会导致车辆之间的冲突，也可能会被轻松发现。但是，更嫌恶的攻击可能会导致网络上的堵塞，燃油消耗和碰撞风险的增加，而不会被轻松发现。为了解决这些攻击的检测，我们首先提出了一个交通流模型框架，用于三种可能的网络攻击：负面控制命令的恶意修改，感知测量数据的假数据插入攻击和服务拒绝（DoS）攻击。然后，我们研究了这些攻击对各个车辆（微）和交通流（ макро）水平的影响。一种基于生成对抗网络（GAN）的异常检测模型被提出，用于实时标识这些攻击。我们通过车辆轨迹数据来提供数据来支持我们的机器学习方法的可行性。我们的方法与一些最近提出的神经网络模型进行比较，并被证明具有更高的准确性，可以快速和准确地识别ACC车辆上的异常驾驶行为。
</details></li>
</ul>
<hr>
<h2 id="Transformers-Learn-Higher-Order-Optimization-Methods-for-In-Context-Learning-A-Study-with-Linear-Models"><a href="#Transformers-Learn-Higher-Order-Optimization-Methods-for-In-Context-Learning-A-Study-with-Linear-Models" class="headerlink" title="Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models"></a>Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17086">http://arxiv.org/abs/2310.17086</a></li>
<li>repo_url: None</li>
<li>paper_authors: Deqing Fu, Tian-Qi Chen, Robin Jia, Vatsal Sharan</li>
<li>for: 这 paper 探讨了 Transformer 在受限学习 (In-Context Learning, ICL) 中的表现，尤其是它如何在不更新参数的情况下学习。</li>
<li>methods: 这 paper 展示了 Transformer 可以通过内部运行高级梯度下降 (Higher-Order Optimization Method) 来实现 ICL。</li>
<li>results: 实验表明，Transformer 可以很准确地实现 Iterative Newton’s Method，一种高级梯度下降方法，而不是 Gradient Descent。每个中间层都可以 rough Compute 3 个 Newton’s Method 迭代步骤，而 Gradient Descent 需要 exponentiation 更多步骤才能匹配一个 Transformer 层。此外，Transformer 还可以在不良条件数据上进行受限学习，一种 Gradient Descent 在这种情况下困难的情况。最后，paper 还提供了理论结果，支持实验结果，并与实验结果具有密切相关性：Transformer 可以通过 $\mathcal{O}(k)$ 层实现 $k$ 次 Newton’s Method 迭代。<details>
<summary>Abstract</summary>
Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of convergence with high-order methods such as Iterative Newton, which are exponentially faster than Gradient Descent. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, we show theoretical results which support our empirical findings and have a close correspondence with them: we prove that Transformers can implement $k$ iterations of Newton's method with $\mathcal{O}(k)$ layers.
</details>
<details>
<summary>摘要</summary>
传播者很好地做内部学习（ICL）——学习 без 参数更新——但它们在做 ICL 的方式仍然是一个谜。最近的工作表明，传播者可能在内部运行 Gradient Descent，一种一阶估计方法。在这篇论文中，我们则证明了传播者会实现更高阶的估计方法来做 ICL。专注于内部线性回推，我们显示了传播者会实现一个非常相似的Iterative Newton's Method，一种更高阶的估计方法，而不是 Gradient Descent。实际上，我们证明了传播者的预测值在不同的中间层之间可以线性地匹配不同的Newton's Method 迭代，每个中间层约 Compute 3 次迭代。相比之下，Gradient Descent 需要更多的步骤来匹配一个额外的传播者层，这表明传播者具有与高阶方法相似的速度，但是 Gradient Descent 的速度是指数增长的。我们还证明了传播者可以在糜烂数据上进行内部学习，这是 Gradient Descent 在这种设定下陷阱的。最后，我们提供了理论结果，证明了传播者可以通过 $\mathcal{O}(k)$ 层来实现 $k$ 次Newton's method。
</details></li>
</ul>
<hr>
<h2 id="Isometric-Motion-Manifold-Primitives"><a href="#Isometric-Motion-Manifold-Primitives" class="headerlink" title="Isometric Motion Manifold Primitives"></a>Isometric Motion Manifold Primitives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17072">http://arxiv.org/abs/2310.17072</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gabe-yhlee/immp-public">https://github.com/gabe-yhlee/immp-public</a></li>
<li>paper_authors: Yonghyeon Lee</li>
<li>for: 这个论文主要是为了提出一种基于拟合 manifold 的运动控制方法，以实现一系列的动作任务。</li>
<li>methods: 这个方法使用了 decoder 函数来 parametrize 拟合 manifold，并使用了在 latent 坐标空间中的概率密度。</li>
<li>results: 论文表明，使用 Isometric Motion Manifold Primitives (IMMP) 可以大幅提高运动控制的性能，并且在 planar 障碍物避免和推动 manipulate 任务中表现出色。<details>
<summary>Abstract</summary>
The Motion Manifold Primitive (MMP) produces, for a given task, a continuous manifold of trajectories each of which can successfully complete the task. It consists of the decoder function that parametrizes the manifold and the probability density in the latent coordinate space. In this paper, we first show that the MMP performance can significantly degrade due to the geometric distortion in the latent space -- by distortion, we mean that similar motions are not located nearby in the latent space. We then propose {\it Isometric Motion Manifold Primitives (IMMP)} whose latent coordinate space preserves the geometry of the manifold. For this purpose, we formulate and use a Riemannian metric for the motion space (i.e., parametric curve space), which we call a {\it CurveGeom Riemannian metric}. Experiments with planar obstacle-avoiding motions and pushing manipulation tasks show that IMMP significantly outperforms existing MMP methods. Code is available at https://github.com/Gabe-YHLee/IMMP-public.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>动态 manifold 基本原理（MMP）生成一个任务下的连续扩散 manifold 每个可以成功完成任务。它包括嵌入函数参数化扩散和在幂空间中的概率密度。在这篇论文中，我们首先表明了 MMP 性能可能因 latent space 的几何扭曲而受到 significiant 降低。然后，我们提议使用 Isometric Motion Manifold Primitives (IMMP)，它的幂空间保持了扩散的几何结构。为了实现这一目标，我们构造了一个 Riemannian metric  для动作空间（即参数曲线空间），我们称之为 CurveGeom Riemannian metric。实验表明，IMMP 在平面障碍物避免和推动 manipulate 任务中表现出色，较之 exist 的 MMP 方法更高效。代码可以在 https://github.com/Gabe-YHLee/IMMP-public 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/cs.AI_2023_10_26/" data-id="clogxf3l100695xra2p998r6x" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_10_26" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/10/26/cs.CL_2023_10_26/" class="article-date">
  <time datetime="2023-10-26T11:00:00.000Z" itemprop="datePublished">2023-10-26</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/10/26/cs.CL_2023_10_26/">cs.CL - 2023-10-26</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="TIMELINE-Exhaustive-Annotation-of-Temporal-Relations-Supporting-the-Automatic-Ordering-of-Events-in-News-Articles"><a href="#TIMELINE-Exhaustive-Annotation-of-Temporal-Relations-Supporting-the-Automatic-Ordering-of-Events-in-News-Articles" class="headerlink" title="TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles"></a>TIMELINE: Exhaustive Annotation of Temporal Relations Supporting the Automatic Ordering of Events in News Articles</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17802">http://arxiv.org/abs/2310.17802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alsayyahi/timeline">https://github.com/alsayyahi/timeline</a></li>
<li>paper_authors: Sarah Alsayyahi, Riza Batista-Navarro</li>
<li>for: 本研究旨在提高现有的时间关系抽象模型，因为现有的新闻数据集存在许多问题，包括：(1) 注解者之间的干扰准确性低，由于注解指南不够具体，未能准确地定义时间关系的标准; (2) 排除文档内部远程关系（即文档中不同段落之间的关系）; (3) 排除不以词为中心的事件。本研究提出了一个新的注解方案，clearly defines the criteria for annotating temporal relations, includes events that are not expressed as verbs, and annotates all temporal relations, including long-distance ones.</li>
<li>methods: 本研究提出了一种新的注解方法，包括：(1) 使用明确的注解指南，以准确地定义时间关系的标准; (2) 使用自动化注解工具，减少注解员的时间和努力; (3) 包括不以词为中心的事件。</li>
<li>results: 本研究在新闻数据集上进行了基eline模型的训练和评估，与之前report的时间关系数据集相比，获得了改善的注解干扰准确性。<details>
<summary>Abstract</summary>
Temporal relation extraction models have thus far been hindered by a number of issues in existing temporal relation-annotated news datasets, including: (1) low inter-annotator agreement due to the lack of specificity of their annotation guidelines in terms of what counts as a temporal relation; (2) the exclusion of long-distance relations within a given document (those spanning across different paragraphs); and (3) the exclusion of events that are not centred on verbs. This paper aims to alleviate these issues by presenting a new annotation scheme that clearly defines the criteria based on which temporal relations should be annotated. Additionally, the scheme includes events even if they are not expressed as verbs (e.g., nominalised events). Furthermore, we propose a method for annotating all temporal relations -- including long-distance ones -- which automates the process, hence reducing time and manual effort on the part of annotators. The result is a new dataset, the TIMELINE corpus, in which improved inter-annotator agreement was obtained, in comparison with previously reported temporal relation datasets. We report the results of training and evaluating baseline temporal relation extraction models on the new corpus, and compare them with results obtained on the widely used MATRES corpus.
</details>
<details>
<summary>摘要</summary>
temporal relation extraction models 在现有的新闻 datasets 中遇到了一些问题，包括：（1） annotator 之间的协调性低下，由于注释指南不够specificity，不确定哪些 temporal relation 应该被注释;（2） 在给定文档中排除远程关系（ span across 不同段落）; 和（3） 排除不以 verb 表达的事件。这篇论文希望通过提出一个新的注释方案来缓解这些问题，该方案明确定义了注释 temporal relation 的标准，并包括不以 verb 表达的事件。此外，我们还提议使用自动化注释方法来快速和减少注释员的时间和劳动。这使得得到了一个新的数据集，名为 TIMELINE corpus，其中得到了提高的 inter-annotator 协调性，与之前报道的 temporal relation 数据集相比。我们对新数据集进行了训练和评估基eline temporal relation EXTRACTION 模型，并与 widely 使用的 MATRES corpus 进行了比较。
</details></li>
</ul>
<hr>
<h2 id="Words-Subwords-and-Morphemes-What-Really-Matters-in-the-Surprisal-Reading-Time-Relationship"><a href="#Words-Subwords-and-Morphemes-What-Really-Matters-in-the-Surprisal-Reading-Time-Relationship" class="headerlink" title="Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?"></a>Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17774">http://arxiv.org/abs/2310.17774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sathvik Nair, Philip Resnik</li>
<li>for: This paper tests the assumption that LLMs (large language models) can be used effectively on psycholinguistic data without considering morphological information.</li>
<li>methods: The paper compares surprisal estimates using orthographic, morphological, and BPE (byte pair encoding) tokenization against reading time data to determine the impact of tokenization method on LLM predictions.</li>
<li>results: The results show that BPE-based tokenization does not result in significantly worse predictions compared to morphological and orthographic segmentation, but a finer-grained analysis reveals potential issues with relying on BPE and suggests the use of morphologically-aware surprisal estimates as an alternative method for evaluating morphological prediction.<details>
<summary>Abstract</summary>
An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.
</details>
<details>
<summary>摘要</summary>
一种重要的假设，即使用语言模型（LLM）处理心理语言数据时，没有得到确认。 LLM 的预测基于字符串tokenization，而不是word decomposition into morphemes。这么？我们仔细测试这一点， Comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data。我们的结果重复了之前的发现，并提供了证据，表明在总体上，使用 BPE  tokenization 的预测不比 morphological and orthographic segmentation 受到影响。然而，一个更加细致的分析表明，可能存在依赖于 BPE 基于 tokenization 的问题，以及提供了 morphologically-aware surprisal estimates 和一种新的评估方法。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Automated-Measurement-of-Responsible-AI-Harms-in-Generative-AI-Applications"><a href="#A-Framework-for-Automated-Measurement-of-Responsible-AI-Harms-in-Generative-AI-Applications" class="headerlink" title="A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications"></a>A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17750">http://arxiv.org/abs/2310.17750</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Magooda, Alec Helyar, Kyle Jackson, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen</li>
<li>for: 这个论文目标是提出一种自动测量责任AI（RAI）指标的框架，用于评估大语言模型（LLM）和相关产品和服务的可责任性。</li>
<li>methods: 该框架基于现有的技术和社会技术知识，利用现代大语言模型，如GPT-4，来自动测量LLMs中可能产生的各种危害。</li>
<li>results: 通过这个框架，我们可以对不同的LLM进行多个案例研究，以评估它们是否违反了不同的RAI相关原则。此框架可以与领域特定的社会技术知识结合使用，以创造未来的新危害评估领域的测量方法。<details>
<summary>Abstract</summary>
We present a framework for the automated measurement of responsible AI (RAI) metrics for large language models (LLMs) and associated products and services. Our framework for automatically measuring harms from LLMs builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art LLMs, such as GPT-4. We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles. The framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. By implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of LLMs.
</details>
<details>
<summary>摘要</summary>
我们提出了一套自动测量责任人工智能（RAI）指标的框架，用于大语言模型（LLM）和相关产品和服务的评估。我们的自动测量损害方法基于现有的技术和社会技术知识，利用现代最先进的LLM，如GPT-4，并可以用于评估不同LLM在多种伦理原则上的违反。我们使用这套框架进行了多个案例研究，探讨了不同LLM在不同伦理原则上的可能损害。这套框架可以与域专业知识相结合，以创造未来的新损害领域的测量。通过实施这套框架，我们期望推动责任用LLM的更高级别的损害评估，并促进责任用LLM的应用。
</details></li>
</ul>
<hr>
<h2 id="StyleBART-Decorate-Pretrained-Model-with-Style-Adapters-for-Unsupervised-Stylistic-Headline-Generation"><a href="#StyleBART-Decorate-Pretrained-Model-with-Style-Adapters-for-Unsupervised-Stylistic-Headline-Generation" class="headerlink" title="StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation"></a>StyleBART: Decorate Pretrained Model with Style Adapters for Unsupervised Stylistic Headline Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17743">http://arxiv.org/abs/2310.17743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanqing Wang, Yajing Luo, Boya Xiong, Guanhua Chen, Yun Chen</li>
<li>for: 这篇论文主要针对的是无监督的 стилистического标题生成任务，即生成一个标题不仅总结文章内容，还反映出用户所需的样式。</li>
<li>methods: 该论文提出了一种无监督的方法，即StyleBART，该方法使用预训练的 BART 模型，并在其上添加了不同风格的适应器，以生成具有不同风格的标题。</li>
<li>results: 经过自动和人工评估，StyleBART 可以具有新的状态 искусственный智能水平，生成高质量的标题，同时具有用户所需的风格。<details>
<summary>Abstract</summary>
Stylistic headline generation is the task to generate a headline that not only summarizes the content of an article, but also reflects a desired style that attracts users. As style-specific article-headline pairs are scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. In this work, we follow this line and propose StyleBART, an unsupervised approach for stylistic headline generation. Our method decorates the pretrained BART model with adapters that are responsible for different styles and allows the generation of headlines with diverse styles by simply switching the adapters. Different from previous works, StyleBART separates the task of style learning and headline generation, making it possible to freely combine the base model and the style adapters during inference. We further propose an inverse paraphrasing task to enhance the style adapters. Extensive automatic and human evaluations show that StyleBART achieves new state-of-the-art performance in the unsupervised stylistic headline generation task, producing high-quality headlines with the desired style.
</details>
<details>
<summary>摘要</summary>
“样式化标题生成任务是生成一个标题，不仅概括文章内容，还反映用户所需的样式。由于样式特定的文章标题对пада scarce, previous researches focus on unsupervised approaches with a standard headline generation dataset and mono-style corpora. 在这项工作中，我们跟随这条线和提出了 StyleBART，一种不supervised方法 для样式化标题生成。我们的方法在预训练BART模型的基础之上添加了适应器，负责不同样式，使得可以通过简单地切换适应器来生成多样的标题。与前一些工作不同，StyleBART分离了样式学习和标题生成任务，使得在推理过程中可以自由地组合基础模型和样式适应器。我们还提出了反向重写任务，以增强样式适应器。自动和人工评估表明，StyleBART在无监督样式化标题生成任务中实现了新的状态纪录性表现，生成高质量的标题，满足用户所需的样式。”
</details></li>
</ul>
<hr>
<h2 id="ArchBERT-Bi-Modal-Understanding-of-Neural-Architectures-and-Natural-Languages"><a href="#ArchBERT-Bi-Modal-Understanding-of-Neural-Architectures-and-Natural-Languages" class="headerlink" title="ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages"></a>ArchBERT: Bi-Modal Understanding of Neural Architectures and Natural Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17737">http://arxiv.org/abs/2310.17737</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohammad Akbari, Saeed Ranjbar Alvar, Behnam Kamranian, Amin Banitalebi-Dehkordi, Yong Zhang</li>
<li>For: 这个论文的目的是提出一种基于多modalitat的语言模型ArchBERT，用于同时学习自然语言和神经网络架构。* Methods: 这个论文使用了一种名为Masked Architecture Modeling（MAM）的预训练策略，并使用了两个新的双Modal数据集进行训练和验证。* Results: 经过数据分析和实验，ArchBERT在不同的下游任务中表现出色，包括建筑 oriented 理解、问答和摘要。<details>
<summary>Abstract</summary>
Building multi-modal language models has been a trend in the recent years, where additional modalities such as image, video, speech, etc. are jointly learned along with natural languages (i.e., textual information). Despite the success of these multi-modal language models with different modalities, there is no existing solution for neural network architectures and natural languages. Providing neural architectural information as a new modality allows us to provide fast architecture-2-text and text-2-architecture retrieval/generation services on the cloud with a single inference. Such solution is valuable in terms of helping beginner and intermediate ML users to come up with better neural architectures or AutoML approaches with a simple text query. In this paper, we propose ArchBERT, a bi-modal model for joint learning and understanding of neural architectures and natural languages, which opens up new avenues for research in this area. We also introduce a pre-training strategy named Masked Architecture Modeling (MAM) for a more generalized joint learning. Moreover, we introduce and publicly release two new bi-modal datasets for training and validating our methods. The ArchBERT's performance is verified through a set of numerical experiments on different downstream tasks such as architecture-oriented reasoning, question answering, and captioning (summarization). Datasets, codes, and demos are available supplementary materials.
</details>
<details>
<summary>摘要</summary>
“在 latest years, 多modal language models 已经成为一股潮流，其中包括图像、视频、语音等多种多样的Modalities 被同时学习与自然语言（即文本信息）。尽管这些多modal language models 已经取得了成功，但是在 neural network architectures 和自然语言之间仍没有现有的解决方案。提供 neural 架构信息作为新的Modalities 允许我们在云端提供快速的 architecture-2-text 和 text-2-architecture 搜寻/生成服务，仅需单一的推论。这个解决方案非常有价值，可以帮助 beginner 和 intermediate ML 用户创建更好的 neural 架构或 AutoML 方法，只需要一个简单的文本查询。在本文中，我们提出 ArchBERT，一个 bi-modal 模型 для 共同学习和理解 neural 架构和自然语言，这开启了新的研究领域。我们还提出了一个名为 Masked Architecture Modeling (MAM) 的预训练策略，以更加通用的共同学习。此外，我们创建了两个新的 bi-modal 数据集用于训练和验证我们的方法。ArchBERT 的表现被证明通过不同的下游任务，如 architecture-oriented reasoning、问题回答和描述（摘要）。数据集、代码和示例在补充材料中提供。”
</details></li>
</ul>
<hr>
<h2 id="Investigating-Multilingual-Coreference-Resolution-by-Universal-Annotations"><a href="#Investigating-Multilingual-Coreference-Resolution-by-Universal-Annotations" class="headerlink" title="Investigating Multilingual Coreference Resolution by Universal Annotations"></a>Investigating Multilingual Coreference Resolution by Universal Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17734">http://arxiv.org/abs/2310.17734</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/haixiachai/multi-coref">https://github.com/haixiachai/multi-coref</a></li>
<li>paper_authors: Haixia Chai, Michael Strube</li>
<li>for: 本研究是针对多语言核心共referencing（MCR）任务进行研究，使用新提出的多语言核心共referencing数据集（CorefUD）进行调查。</li>
<li>methods: 本研究首先通过查看不同语言水平和 genre 的真实数据，以获得关于核心共referencing的特征的启示。其次，通过分析CRAC 2022 共同任务中 SotA 系统无法解决的最difficult case，进行错误分析。最后，基于这种分析，从universal morphosyntactic annotations中提取特征，并将其集成到基eline系统中，以评估其可能带来的改进。</li>
<li>results: 我们的最佳配置的特征提高了基eline系统的 F1 分数0.9%。<details>
<summary>Abstract</summary>
Multilingual coreference resolution (MCR) has been a long-standing and challenging task. With the newly proposed multilingual coreference dataset, CorefUD (Nedoluzhko et al., 2022), we conduct an investigation into the task by using its harmonized universal morphosyntactic and coreference annotations. First, we study coreference by examining the ground truth data at different linguistic levels, namely mention, entity and document levels, and across different genres, to gain insights into the characteristics of coreference across multiple languages. Second, we perform an error analysis of the most challenging cases that the SotA system fails to resolve in the CRAC 2022 shared task using the universal annotations. Last, based on this analysis, we extract features from universal morphosyntactic annotations and integrate these features into a baseline system to assess their potential benefits for the MCR task. Our results show that our best configuration of features improves the baseline by 0.9% F1 score.
</details>
<details>
<summary>摘要</summary>
多语言核心引用解决 (MCR) 是一项长期存在挑战的任务。我们使用新提出的多语言核心引用数据集（CorefUD，Nedoluzhko et al., 2022）进行研究。我们首先研究核心引用的特征，包括提取不同语言水平的 mention、entity 和文档水平的 annotations，以及不同类型的文献中的核心引用特征。其次，我们对 CRAC 2022 共享任务中 SotA 系统失败的最具挑战性情况进行错误分析。最后，我们从 universally 的 morphosyntactic 注释中提取特征，并将这些特征与基eline 系统集成，以评估其可能带来的改进。我们的结果表明，我们最佳的配置在 F1 分数上提高了 0.9%。
</details></li>
</ul>
<hr>
<h2 id="ZeroQuant-HERO-Hardware-Enhanced-Robust-Optimized-Post-Training-Quantization-Framework-for-W8A8-Transformers"><a href="#ZeroQuant-HERO-Hardware-Enhanced-Robust-Optimized-Post-Training-Quantization-Framework-for-W8A8-Transformers" class="headerlink" title="ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers"></a>ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17723">http://arxiv.org/abs/2310.17723</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He</li>
<li>for: 这个论文旨在提出一个全新的、具有硬件优化的Robust Optimized Post-training W8A8量化框架，以提高深度神经网络的执行速度和可靠性。</li>
<li>methods: 这个框架使用了 ZeroQuant 的动态量化技术，并且特别处理了内存带宽和 compute-intensive 算子，以便在硬件上实现最佳性能。此外，这个框架还提供了一个可选的 INT8 模组，可以在 FP16&#x2F;BF16 模式下运行，以提高准确性。</li>
<li>results: 这个框架可以实现更好的硬件性能和可靠性，并且可以适应不同的硬件环境。在一个实际应用中，这个框架可以实现更好的执行速度和准确性，相比之下 ZeroQuant 的动态量化技术。<details>
<summary>Abstract</summary>
Quantization techniques are pivotal in reducing the memory and computational demands of deep neural network inference. Existing solutions, such as ZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook crucial memory-bounded operators and the complexities of per-token quantization. Addressing these gaps, we present a novel, fully hardware-enhanced robust optimized post-training W8A8 quantization framework, ZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and compute-intensive operators, aiming for optimal hardware performance. Additionally, it offers flexibility by allowing specific INT8 modules to switch to FP16/BF16 mode, enhancing accuracy.
</details>
<details>
<summary>摘要</summary>
深度神经网络的批量化技术是减少深度神经网络的内存和计算需求的关键。现有的解决方案，如ZeroQuant，为BERT和GPT模型提供了动态量化，但忽略了关键的内存缓存操作和每个token量化的复杂性。为了解决这些缺陷，我们提出了一种全新的、具有硬件优化的robust批量化框架ZeroQuant-HERO。这个框架独特地集成了内存带宽和计算投入的操作，寻求最佳硬件性能。此外，它还提供了flexibility，允许特定INT8模块在FP16/BF16模式下运行，以提高精度。
</details></li>
</ul>
<hr>
<h2 id="Nearest-Neighbor-Search-over-Vectorized-Lexico-Syntactic-Patterns-for-Relation-Extraction-from-Financial-Documents"><a href="#Nearest-Neighbor-Search-over-Vectorized-Lexico-Syntactic-Patterns-for-Relation-Extraction-from-Financial-Documents" class="headerlink" title="Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents"></a>Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17714">http://arxiv.org/abs/2310.17714</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/pawan2411/pan-dl_refind">https://github.com/pawan2411/pan-dl_refind</a></li>
<li>paper_authors: Pawan Kumar Rajpoot, Ankur Parikh</li>
<li>for: 提高 implicit expression 和 long-tail relation class 的处理能力，以及提供一种可 accessible 的 RE 模型 для用户。</li>
<li>methods: 使用 nearest-neighbor search over dense vectors of lexico-syntactic patterns 来咨询训练关系，并使用这种方法来解决语言复杂性和数据稀缺问题。</li>
<li>results: 在 REFinD 上测试，本方法达到了状态级性能，并且在人工循环设置下提供了一个不错的开始。<details>
<summary>Abstract</summary>
Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation classes, caused by language complexity and data sparsity. Further, these approaches and models are largely inaccessible to users who don't have direct access to large language models (LLMs) and/or infrastructure for supervised training or fine-tuning. Rule-based systems also struggle with implicit expressions. Apart from this, Real world financial documents such as various 10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose another challenge to rule-based systems in terms of longer and complex sentences. In this paper, we introduce a simple approach that consults training relations at test time through a nearest-neighbor search over dense vectors of lexico-syntactic patterns and provides a simple yet effective means to tackle the above issues. We evaluate our approach on REFinD and show that our method achieves state-of-the-art performance. We further show that it can provide a good start for human in the loop setup when a small number of annotations are available and it is also beneficial when domain experts can provide high quality patterns.
</details>
<details>
<summary>摘要</summary>
<<SYS>>语言模型已经在relation抽取（RE）方面做出了很大的进步，但现有的RE模型通常无法处理两种情况：偏向表达和长尾关系类别，这是由于语言复杂性和数据稀缺所致。此外，现有的方法和模型具有训练和精度调整的限制，使得用户无法直接访问大语言模型（LLM）和基础设施。规则式系统也难以处理偏向表达。除此之外，公开上市公司的财务报表（如10-K、10-Q等）也对规则式系统 pose another challenge，因为它们具有更长和复杂的句子。在这篇论文中，我们介绍了一种简单的方法，通过在测试时通过 nearest-neighbor搜索 dense vector of lexico-syntactic pattern来咨询训练关系，并提供了一种简单 yet effective的方法来解决以上问题。我们对REFinD进行评估，并证明我们的方法可以达到状态的表现。我们还证明，它可以提供一个好的起点 для人工循环setup，当只有少量注释时，以及当领域专家可以提供高质量的模式时。
</details></li>
</ul>
<hr>
<h2 id="Is-Explanation-the-Cure-Misinformation-Mitigation-in-the-Short-Term-and-Long-Term"><a href="#Is-Explanation-the-Cure-Misinformation-Mitigation-in-the-Short-Term-and-Long-Term" class="headerlink" title="Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term"></a>Is Explanation the Cure? Misinformation Mitigation in the Short Term and Long Term</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17711">http://arxiv.org/abs/2310.17711</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi-Li Hsu, Shih-Chieh Dai, Aiping Xiong, Lun-Wei Ku</li>
<li>for: 本研究旨在测试自动生成的解释是否能够帮助人们战胜假新闻，并与警告标签相比。</li>
<li>methods: 本研究使用了GPT-4生成的反假证据来对假新闻进行处理。</li>
<li>results: 研究发现，两种干预措施都能够有效地降低受试者对假新闻的信念，并且这两种干预措施在短期和长期都有相似的效果。<details>
<summary>Abstract</summary>
With advancements in natural language processing (NLP) models, automatic explanation generation has been proposed to mitigate misinformation on social media platforms in addition to adding warning labels to identified fake news. While many researchers have focused on generating good explanations, how these explanations can really help humans combat fake news is under-explored. In this study, we compare the effectiveness of a warning label and the state-of-the-art counterfactual explanations generated by GPT-4 in debunking misinformation. In a two-wave, online human-subject study, participants (N = 215) were randomly assigned to a control group in which false contents are shown without any intervention, a warning tag group in which the false claims were labeled, or an explanation group in which the false contents were accompanied by GPT-4 generated explanations. Our results show that both interventions significantly decrease participants' self-reported belief in fake claims in an equivalent manner for the short-term and long-term. We discuss the implications of our findings and directions for future NLP-based misinformation debunking strategies.
</details>
<details>
<summary>摘要</summary>
随着自然语言处理（NLP）模型的进步，自动生成解释被提议用于社交媒体平台上 combat 谣言，并且添加警告标签到已知的假新闻。然而，许多研究者对生成好的解释进行了重点研究，对于这些解释如何真正地帮助人们战胜假新闻则未得到充分探讨。在本研究中，我们比较了警告标签和GPT-4的国际标准对抗谣言的效果。在两个波次的在线人类试验中，参与者（N = 215）被随机分配到控制组（无任何干预）、警告标签组（假称有警告标签）或解释组（假称有GPT-4生成的解释）。我们的结果表明，两种 интервенción都能够在短期和长期内等效地减少参与者对假CLAIM的自我报告的信任度。我们讨论了我们的发现的意义和未来NLP基于的谣言战胜策略的方向。
</details></li>
</ul>
<hr>
<h2 id="The-impact-of-using-an-AI-chatbot-to-respond-to-patient-messages"><a href="#The-impact-of-using-an-AI-chatbot-to-respond-to-patient-messages" class="headerlink" title="The impact of using an AI chatbot to respond to patient messages"></a>The impact of using an AI chatbot to respond to patient messages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17703">http://arxiv.org/abs/2310.17703</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/aim-harvard/oncqa">https://github.com/aim-harvard/oncqa</a></li>
<li>paper_authors: Shan Chen, Marco Guevara, Shalini Moningi, Frank Hoebers, Hesham Elhalawani, Benjamin H. Kann, Fallon E. Chipidza, Jonathan Leeman, Hugo J. W. L. Aerts, Timothy Miller, Guergana K. Savova, Raymond H. Mak, Maryam Lustberg, Majid Afshar, Danielle S. Bitterman<br>for: 这个研究旨在测试人工智能聊天机器人（ChatGPT）是否可以减轻医生的负担，并且检查这些系统在医疗决策中的价值和影响。methods: 这个研究使用了两阶段跨sectional研究，让6名癌科医生回答100个真实的人工癌病患者情况和门诊信息，首先是手动回答，然后是使用人工智能帮助。results: 研究发现，AI帮助下的回答比手动回答更长、更难读，但是可以提供可接受的草稿无需修改58%的时间。AI帮助下的效率提高77%的时间，并且具有低伤害风险（82%安全）。但是，7.7%的AI回答可能会导致严重伤害。在31%的情况下，医生认为AI草稿是人类写的。AI帮助下的回答比手动回答更有传递patient education的建议， fewer clinical actions。结果显示AI可以改善医生的效率和病人照顾，但是需要在安全的情况下使用。<details>
<summary>Abstract</summary>
Documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician burden by assisting with documentation. Although many hospitals are actively integrating such systems into electronic medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with AI assistance.   We find AI-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. AI assistance improved efficiency 77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses could severely harm. In 31% cases, physicians thought AI drafts were human-written. AI assistance led to more patient education recommendations, fewer clinical actions than manual responses. Results show promise for AI to improve clinician efficiency and patient care through assisting documentation, if used judiciously. Monitoring model outputs and human-AI interaction remains crucial for safe implementation.
</details>
<details>
<summary>摘要</summary>
医疗文书负担是临床医生疲劳的主要 contribuutor，这种情况在全国范围内升高，是我们护理患者的能力的急迫性问题。人工智能（AI）聊天机器人，如ChatGPT，可以减轻医生的文书负担。虽然许多医院在电子医疗记录系统中活动地整合这些系统，但AI聊天机器人在临床决策中的实用性和影响尚未得到研究。我们是第一个对大型自然语言模型在协助临床医生编写答复 patient questions 进行了研究。在我们的两个阶段cross-sectional研究中，6名Oncologists responded to 100种真实 synthetic cancer patient scenario和门户消息，这些消息是通过反映常见医疗情况来设计的。我们发现，使用AI助手的答复比手动答复长得多，可读性下降，但是58%的时间内可以提供可接受的答复无需修改。使用AI助手可以提高效率，77%的时间内可以提高效率，并且风险低（82%的时间内安全）。但是，7.7%的AI答复可能会严重害。在31%的情况下，医生认为AI答复是人类写的。使用AI助手可以提高患者教育建议和临床行动的数量，而手动答复则相对较少。结果表明，AI可以通过协助文书，提高临床医生的效率和患者的护理质量，但是要在安全的情况下使用。监测模型输出和人机AI交互仍然是关键。
</details></li>
</ul>
<hr>
<h2 id="Non-contrastive-sentence-representations-via-self-supervision"><a href="#Non-contrastive-sentence-representations-via-self-supervision" class="headerlink" title="Non-contrastive sentence representations via self-supervision"></a>Non-contrastive sentence representations via self-supervision</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17690">http://arxiv.org/abs/2310.17690</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marco Farina, Duccio Pappadopulo</li>
<li>for: 学习文本和句子嵌入的Unsupervised方法</li>
<li>methods: 使用自动标注对照方法和计算维度对比的方法</li>
<li>results: 无需auxiliary损失函数，自动标注对照方法可以超越SimCSE在下游任务上的表现<details>
<summary>Abstract</summary>
Sample contrastive methods, typically referred to simply as contrastive are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.
</details>
<details>
<summary>摘要</summary>
Sample contrastive methods, typically referred to simply as contrastive, are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.Here's the translation in Traditional Chinese:Sample contrastive methods, typically referred to simply as contrastive, are the foundation of most unsupervised methods to learn text and sentence embeddings. On the other hand, a different class of self-supervised loss functions and methods have been considered in the computer vision community and referred to as dimension contrastive. In this paper, we thoroughly compare this class of methods with the standard baseline for contrastive sentence embeddings, SimCSE. We find that self-supervised embeddings trained using dimension contrastive objectives can outperform SimCSE on downstream tasks without needing auxiliary loss functions.
</details></li>
</ul>
<hr>
<h2 id="torchdistill-Meets-Hugging-Face-Libraries-for-Reproducible-Coding-Free-Deep-Learning-Studies-A-Case-Study-on-NLP"><a href="#torchdistill-Meets-Hugging-Face-Libraries-for-Reproducible-Coding-Free-Deep-Learning-Studies-A-Case-Study-on-NLP" class="headerlink" title="torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP"></a>torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free Deep Learning Studies: A Case Study on NLP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17644">http://arxiv.org/abs/2310.17644</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yoshitomo-matsubara/torchdistill">https://github.com/yoshitomo-matsubara/torchdistill</a></li>
<li>paper_authors: Yoshitomo Matsubara</li>
<li>for: 本研究旨在提高科学研究中的可重现性，通过对深度学习领域的研究领域进行快速发展，支持更多任务和第三方库。</li>
<li>methods: 本研究使用了模块驱动的 coding-free 深度学习框架 torchdistill，并将其升级至支持更多任务。为了证明新的框架可以支持更多任务，我们使用了一个基于 upgraded torchdistill 的脚本，使用了各种 Hugging Face 库来重现 GLUE benchmark 结果。</li>
<li>results: 我们在本研究中重现了 27 个 fine-tuned BERT 模型和配置，并将其发布在 Hugging Face 上。我们还重新实现了一些小型模型和新的知识传递方法，并在计算机视觉任务上进行了额外的实验。<details>
<summary>Abstract</summary>
Reproducibility in scientific work has been becoming increasingly important in research communities such as machine learning, natural language processing, and computer vision communities due to the rapid development of the research domains supported by recent advances in deep learning. In this work, we present a significantly upgraded version of torchdistill, a modular-driven coding-free deep learning framework significantly upgraded from the initial release, which supports only image classification and object detection tasks for reproducible knowledge distillation experiments. To demonstrate that the upgraded framework can support more tasks with third-party libraries, we reproduce the GLUE benchmark results of BERT models using a script based on the upgraded torchdistill, harmonizing with various Hugging Face libraries. All the 27 fine-tuned BERT models and configurations to reproduce the results are published at Hugging Face, and the model weights have already been widely used in research communities. We also reimplement popular small-sized models and new knowledge distillation methods and perform additional experiments for computer vision tasks.
</details>
<details>
<summary>摘要</summary>
科学研究中的重复性在机器学习、自然语言处理和计算机视觉等领域日益重要，因为近年来深度学习的发展对研究领域的进步提供了大量支持。在这项工作中，我们发布了torchdistill的升级版本，该框架支持了图像分类和物体检测任务，并且可以用于无编程的知识储存实验。为了证明新版本框架可以支持更多任务，我们使用基于升级后的torchdistill的脚本重现GLUEbenchmark中BERT模型的结果。我们在Hugging Face上发布了27个精度调整后的BERT模型和配置，以及模型的 weights，这些模型 weights已经在研究 сообществе广泛使用。此外，我们还重新实现了一些小型模型和新的知识储存方法，并在计算机视觉任务上进行了其他实验。
</details></li>
</ul>
<hr>
<h2 id="InstOptima-Evolutionary-Multi-objective-Instruction-Optimization-via-Large-Language-Model-based-Instruction-Operators"><a href="#InstOptima-Evolutionary-Multi-objective-Instruction-Optimization-via-Large-Language-Model-based-Instruction-Operators" class="headerlink" title="InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators"></a>InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17630">http://arxiv.org/abs/2310.17630</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yangheng95/instoptima">https://github.com/yangheng95/instoptima</a></li>
<li>paper_authors: Heng Yang, Ke Li</li>
<li>for: 提高 instruciton 工程效率，推动 instruction 学科发展</li>
<li>methods: 基于 evolutionary multi-objective optimization 的 instruciton 生成方法，利用大语言模型 simulate instruction 操作，并 introducing objective-guided mechanism</li>
<li>results: 实验结果显示 improved fine-tuning performance 和生成多种高质量 instrucitonIn English, this would be:</li>
<li>for: Improving instruction engineering efficiency to advance the field of instruction studies</li>
<li>methods: Using an evolutionary multi-objective optimization approach with a large language model to simulate instruction operators, and introducing an objective-guided mechanism to enhance the quality of generated instructions</li>
<li>results: Experimental results show improved fine-tuning performance and the generation of a diverse set of high-quality instructions.<details>
<summary>Abstract</summary>
Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they primarily aim to improve performance without considering other crucial objectives that impact instruction quality, such as instruction length and perplexity. Therefore, we propose a novel approach (i.e., InstOptima) that treats instruction generation as an evolutionary multi-objective optimization problem. In contrast to text edition-based methods, our approach utilizes a large language model (LLM) to simulate instruction operators, including mutation and crossover. Furthermore, we introduce an objective-guided mechanism for these operators, allowing the LLM to comprehend the objectives and enhance the quality of the generated instructions. Experimental results demonstrate improved fine-tuning performance and the generation of a diverse set of high-quality instructions.
</details>
<details>
<summary>摘要</summary>
过去的研究主要集中在对预训语言模型进行调整，但是指令工程的效率仍然较低，这限制了指令研究的发展。现在的研究主要是对指令生成进行自动化，但是这些研究主要关注性能的提高，而忽略了其他重要的目标，例如指令长度和混淆率。因此，我们提出了一个新的方法（即InstOptima），将指令生成视为进化多个目标优化问题。在对文本进行修订的方法不同之余，我们的方法利用大型语言模型（LLM）来模拟指令操作，包括变异和交叉。此外，我们引入了目标导向的机制，让LLM能够理解目标，并提高生成的指令质量。实验结果显示，我们的方法可以提高调整性能和生成多个高质量的指令。
</details></li>
</ul>
<hr>
<h2 id="Proving-Test-Set-Contamination-in-Black-Box-Language-Models"><a href="#Proving-Test-Set-Contamination-in-Black-Box-Language-Models" class="headerlink" title="Proving Test Set Contamination in Black Box Language Models"></a>Proving Test Set Contamination in Black Box Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17623">http://arxiv.org/abs/2310.17623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, Tatsunori B. Hashimoto</li>
<li>for: 这篇论文是为了证明语言模型是否受到数据污染的。</li>
<li>methods: 这篇论文使用了一种基于交换可能性的方法来证明语言模型是否受到数据污染。</li>
<li>results: 这篇论文的实验结果表明，使用这种方法可以准确地检测语言模型是否受到数据污染，并且可以在小型模型和少量数据情况下进行检测。<details>
<summary>Abstract</summary>
Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:大型语言模型通过互联网数据进行训练，引发了关注和推测，它们可能已经记忆了公共测试集。但是从推测到证据是困难的，因为 propietary 模型使用的预训练数据通常不公开 accessible。我们表明可以在无数据污染情况下提供可证明的保证。我们的方法利用了无数据污染时，所有的交换 benchmark 都应该是相同概率的。相反，语言模型很可能记忆示例的顺序，因此一个污染的语言模型会找到一些特定的 canonical 顺序非常有可能性。我们的测试会在这些可能性中旁 Flag 潜在的污染。我们示例了我们的方法可以在复杂的情况下可靠地证明测试集污染，包括模型只有 1.4 十亿参数，测试集只有 1000 个示例，并且数据集仅在预训练 Corpora 中出现一些次。使用我们的测试，我们对五种公共可 accessible 语言模型进行审核，并未发现普遍的污染。
</details></li>
</ul>
<hr>
<h2 id="Uncovering-Meanings-of-Embeddings-via-Partial-Orthogonality"><a href="#Uncovering-Meanings-of-Embeddings-via-Partial-Orthogonality" class="headerlink" title="Uncovering Meanings of Embeddings via Partial Orthogonality"></a>Uncovering Meanings of Embeddings via Partial Orthogonality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17611">http://arxiv.org/abs/2310.17611</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibo Jiang, Bryon Aragam, Victor Veitch</li>
<li>for: 这个论文研究了语言语义结构如何在数字嵌入中编码。</li>
<li>methods: 作者使用了受限正交的概念和方法来捕捉语义独立性。</li>
<li>results: 作者证明了受限正交可以 capture语义独立性，并提出了独立嵌入和独立嵌入的存在。<details>
<summary>Abstract</summary>
Machine learning tools often rely on embedding text as vectors of real numbers. In this paper, we study how the semantic structure of language is encoded in the algebraic structure of such embeddings. Specifically, we look at a notion of ``semantic independence'' capturing the idea that, e.g., ``eggplant'' and ``tomato'' are independent given ``vegetable''. Although such examples are intuitive, it is difficult to formalize such a notion of semantic independence. The key observation here is that any sensible formalization should obey a set of so-called independence axioms, and thus any algebraic encoding of this structure should also obey these axioms. This leads us naturally to use partial orthogonality as the relevant algebraic structure. We develop theory and methods that allow us to demonstrate that partial orthogonality does indeed capture semantic independence. Complementary to this, we also introduce the concept of independence preserving embeddings where embeddings preserve the conditional independence structures of a distribution, and we prove the existence of such embeddings and approximations to them.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LeCaRDv2-A-Large-Scale-Chinese-Legal-Case-Retrieval-Dataset"><a href="#LeCaRDv2-A-Large-Scale-Chinese-Legal-Case-Retrieval-Dataset" class="headerlink" title="LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset"></a>LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17609">http://arxiv.org/abs/2310.17609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, Yiqun Liu<br>for:LeCaRDv2 is a large-scale Legal Case Retrieval Dataset (version 2) that aims to alleviate the limitations of existing datasets in the Chinese legal system, such as limited data size, narrow definitions of legal relevance, and naive candidate pooling strategies.methods:LeCaRDv2 consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents. It enriches the existing relevance criteria by considering three key aspects: characterization, penalty, and procedure. Additionally, a two-level candidate set pooling strategy is proposed to effectively identify potential candidates for each query case.results:The dataset has been annotated by multiple legal experts specializing in criminal law, ensuring the accuracy and reliability of the annotations. The evaluation of several state-of-the-art retrieval models at LeCaRDv2 demonstrates that there is still significant room for improvement in legal case retrieval.Here’s the Chinese version of the information:for:LeCaRDv2 是一个大规模的法律案例检索数据集（第二版），旨在解决现有数据集中的三个问题：数据量有限，法律相关性过于窄，和数据采样中的候选人选择策略过于简单。methods:LeCaRDv2 包含 800 个查询和 55,192 个候选案例，从 4.3 万个刑事案件文档中提取。它对现有的相关性标准进行扩展，考虑三个关键方面： caracterization、penalty 和 procedure。此外，还提出了一种两级候选人pooling策略，以更好地确定每个查询案件的可能候选人。results:所有案例都被多名专业律师（专门从事刑事法） annotate，以确保数据的准确性和可靠性。在 LeCaRDv2 上评估了多种当前领先的检索模型，显示了法律案例检索仍有很大的提升空间。<details>
<summary>Abstract</summary>
As an important component of intelligent legal systems, legal case retrieval plays a critical role in ensuring judicial justice and fairness. However, the development of legal case retrieval technologies in the Chinese legal system is restricted by three problems in existing datasets: limited data size, narrow definitions of legal relevance, and naive candidate pooling strategies used in data sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale Legal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192 candidates extracted from 4.3 million criminal case documents. To the best of our knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval datasets, providing extensive coverage of criminal charges. Additionally, we enrich the existing relevance criteria by considering three key aspects: characterization, penalty, procedure. This comprehensive criteria enriches the dataset and may provides a more holistic perspective. Furthermore, we propose a two-level candidate set pooling strategy that effectively identify potential candidates for each query case. It's important to note that all cases in the dataset have been annotated by multiple legal experts specializing in criminal law. Their expertise ensures the accuracy and reliability of the annotations. We evaluate several state-of-the-art retrieval models at LeCaRDv2, demonstrating that there is still significant room for improvement in legal case retrieval. The details of LeCaRDv2 can be found at the anonymous website https://github.com/anonymous1113243/LeCaRDv2.
</details>
<details>
<summary>摘要</summary>
legal case retrieval 作为智能法律系统的重要组件，在确保司法公平和公正方面扮演着关键角色。然而，中国法律系统中的法律案例检索技术的发展受到了三种现有数据集的限制：数据量受限，法律相关性的定义太窄，以及数据采样中使用的候选人选择策略过于简单。为了解决这些问题，我们提出了LeCaRDv2，一个大规模的法律案例检索数据集（版本2）。它包括800个查询和55,192个候选者，从430万起诉案件文档中提取出来。我们知道LeCaRDv2是中国法律系统中最大的法律案例检索数据集之一，提供了广泛的刑事罪名覆盖。此外，我们增强了现有的相关性标准，考虑了三个关键方面： caracterización、刑事和程序。这种全面的标准可能提供了更全面的视角。此外，我们提议了两级候选者集pooling策略，可以有效地找到每个查询案例的可能候选者。需要注意的是，所有案例在数据集中都被多名法律专家特许律师 annotated，他们的专业性确保了数据集的准确性和可靠性。我们在LeCaRDv2上评估了多种当前最佳 Retrieval 模型，显示了法律案例检索领域还有很大的改进空间。LeCaRDv2的细节可以在https://github.com/anonymous1113243/LeCaRDv2 的匿名网站上找到。
</details></li>
</ul>
<hr>
<h2 id="Lil-Bevo-Explorations-of-Strategies-for-Training-Language-Models-in-More-Humanlike-Ways"><a href="#Lil-Bevo-Explorations-of-Strategies-for-Training-Language-Models-in-More-Humanlike-Ways" class="headerlink" title="Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways"></a>Lil-Bevo: Explorations of Strategies for Training Language Models in More Humanlike Ways</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17591">http://arxiv.org/abs/2310.17591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/venkatasg/lil-bevo">https://github.com/venkatasg/lil-bevo</a></li>
<li>paper_authors: Venkata S Govindarajan, Juan Diego Rodriguez, Kaj Bostrom, Kyle Mahowald</li>
<li>for: 本研究旨在提出一种基于小量数据的语言模型预训练方法，以提高语言模型的性能。</li>
<li>methods: 该方法使用了三种inghamredients：初始预训练使用音乐数据、在较短的序列上进行预训练 перед较长的序列上，以及在特定的标记Token上进行做袋装。</li>
<li>results: 研究发现，使用这些技术可以使模型的性能达到或超过随机的水平，但是并不能与更大的语言模型在更多的数据上达到的性能水平。另外，在较短的序列上进行预训练表现较好，而使用音乐数据可能带来小量的改善。<details>
<summary>Abstract</summary>
We present Lil-Bevo, our submission to the BabyLM Challenge. We pretrained our masked language models with three ingredients: an initial pretraining with music data, training on shorter sequences before training on longer ones, and masking specific tokens to target some of the BLiMP subtasks. Overall, our baseline models performed above chance, but far below the performance levels of larger LLMs trained on more data. We found that training on short sequences performed better than training on longer sequences.Pretraining on music may help performance marginally, but, if so, the effect seems small. Our targeted Masked Language Modeling augmentation did not seem to improve model performance in general, but did seem to help on some of the specific BLiMP tasks that we were targeting (e.g., Negative Polarity Items). Training performant LLMs on small amounts of data is a difficult but potentially informative task. While some of our techniques showed some promise, more work is needed to explore whether they can improve performance more than the modest gains here. Our code is available at https://github.com/venkatasg/Lil-Bevo and out models at https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a
</details>
<details>
<summary>摘要</summary>
我们给你介绍Lil-Bevo，我们对 BabyLM 挑战的提交。我们预训我们的封面语言模型使用三种食品：初始预训使用音乐数据、在较短的序列上进行训练 перед训练较长的序列、以及对特定的Token进行遮盾以 targets 一些 BLiMP 任务。总体而言，我们的基eline模型的表现高于机会，但与更多数据训练的更大LLMs相比，表现仍然较低。我们发现训练较短的序列表现比较好，但预训使用音乐数据可能帮助表现，但效果似乎很小。我们的对话类型掩盖语言模型增强法不会提高模型的表现，但在特定的 BLiMP 任务上（例如，负面负陵项），它可能有所帮助。训练表现好的LLMs在小量数据上是一个困难但有可能提供有益的任务。一些我们的技术表现了一定的 promise，但需要更多的工作来探索这些技术是否可以提高表现。我们的代码可以在 GitHub 上找到（https://github.com/venkatasg/Lil-Bevo），我们的模型可以在 Hugging Face 上找到（https://huggingface.co/collections/venkatasg/babylm-653591cdb66f4bf68922873a）。
</details></li>
</ul>
<hr>
<h2 id="PAC-tuning-Fine-tuning-Pretrained-Language-Models-with-PAC-driven-Perturbed-Gradient-Descent"><a href="#PAC-tuning-Fine-tuning-Pretrained-Language-Models-with-PAC-driven-Perturbed-Gradient-Descent" class="headerlink" title="PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent"></a>PAC-tuning:Fine-tuning Pretrained Language Models with PAC-driven Perturbed Gradient Descent</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17588">http://arxiv.org/abs/2310.17588</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guangliang Liu, Zhiyu Xue, Xitong Zhang, Kristen Marie Johnson, Rongrong Wang</li>
<li>for: 提高大型语言模型（PLMs）在下游任务中的泛化性能，并且在少量数据学习（few-shot learning）情况下具有良好的泛化性能。</li>
<li>methods: 提议一种两阶段精度调整方法（PAC-tuning），首先根据PAC-Bayes训练直接逼近PAC-Bayes泛化 bound，然后在训练过程中通过在模型参数中尝试随机变量来 modify 梯度，实现一种变种的扰动梯度下降（PGD）。</li>
<li>results: 实验结果表明，PAC-tuning可以成功地处理精度调整问题，并在5个 GLUEbenchmark任务上超越强基eline方法，Visible margin。这些结果证明了PAC训练可以在任何其他使用Adam优化器进行训练的设置中应用。<details>
<summary>Abstract</summary>
Fine-tuning pretrained language models (PLMs) for downstream tasks is a large-scale optimization problem, in which the choice of the training algorithm critically determines how well the trained model can generalize to unseen test data, especially in the context of few-shot learning. To achieve good generalization performance and avoid overfitting, techniques such as data augmentation and pruning are often applied. However, adding these regularizations necessitates heavy tuning of the hyperparameters of optimization algorithms, such as the popular Adam optimizer. In this paper, we propose a two-stage fine-tuning method, PAC-tuning, to address this optimization challenge. First, based on PAC-Bayes training, PAC-tuning directly minimizes the PAC-Bayes generalization bound to learn proper parameter distribution. Second, PAC-tuning modifies the gradient by injecting noise with the variance learned in the first stage into the model parameters during training, resulting in a variant of perturbed gradient descent (PGD). In the past, the few-shot scenario posed difficulties for PAC-Bayes training because the PAC-Bayes bound, when applied to large models with limited training data, might not be stringent. Our experimental results across 5 GLUE benchmark tasks demonstrate that PAC-tuning successfully handles the challenges of fine-tuning tasks and outperforms strong baseline methods by a visible margin, further confirming the potential to apply PAC training for any other settings where the Adam optimizer is currently used for training.
</details>
<details>
<summary>摘要</summary>
大规模优化问题中，细化预训练语言模型（PLM） для下游任务是一个大型优化问题，在这个问题中，选择训练算法的选择对模型能够在未经见数据测试中准确预测的能力具有关键作用。为了 достичь好的泛化性表现和避免过拟合，技术如数据增强和剪裁通常被应用。然而，添加这些正则化需要严重调整优化算法的超参数，如流行的Adam优化器。在这篇论文中，我们提议一种两Stage细化方法，即PAC-tuning，以解决这个优化挑战。首先，基于PAC-Bayes培训，PAC-tuning直接将PAC-Bayes泛化约束约束学习到合适的参数分布。其次，PAC-tuning在训练过程中将模型参数中插入采样变量，从而实现一种变化的梯度下降（PGD）。在过去，几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的几个几个任务的
</details></li>
</ul>
<hr>
<h2 id="Global-Voices-Local-Biases-Socio-Cultural-Prejudices-across-Languages"><a href="#Global-Voices-Local-Biases-Socio-Cultural-Prejudices-across-Languages" class="headerlink" title="Global Voices, Local Biases: Socio-Cultural Prejudices across Languages"></a>Global Voices, Local Biases: Socio-Cultural Prejudices across Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17586">http://arxiv.org/abs/2310.17586</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamshnoo/weathub">https://github.com/iamshnoo/weathub</a></li>
<li>paper_authors: Anjishnu Mukherjee, Chahat Raj, Ziwei Zhu, Antonios Anastasopoulos</li>
<li>for: 这个论文旨在探讨语言模型（LM）如何反映和增强社会偏见。</li>
<li>methods: 作者使用Word Embedding Association Test（WEAT）测试24种语言中LM的偏见，并在每种语言的地区上添加了当地文化特点。</li>
<li>results: 研究发现，LM具有广泛的社会偏见，包括语言、文化和社会方面的偏见。此外，研究还发现了新的偏见维度，如恶意、残夷和更多的社会偏见。最后，研究者强调了这些社会偏见的重要性，并提出了更加公平的语言模型的需求。<details>
<summary>Abstract</summary>
Human biases are ubiquitous but not uniform: disparities exist across linguistic, cultural, and societal borders. As large amounts of recent literature suggest, language models (LMs) trained on human data can reflect and often amplify the effects of these social biases. However, the vast majority of existing studies on bias are heavily skewed towards Western and European languages. In this work, we scale the Word Embedding Association Test (WEAT) to 24 languages, enabling broader studies and yielding interesting findings about LM bias. We additionally enhance this data with culturally relevant information for each language, capturing local contexts on a global scale. Further, to encompass more widely prevalent societal biases, we examine new bias dimensions across toxicity, ableism, and more. Moreover, we delve deeper into the Indian linguistic landscape, conducting a comprehensive regional bias analysis across six prevalent Indian languages. Finally, we highlight the significance of these social biases and the new dimensions through an extensive comparison of embedding methods, reinforcing the need to address them in pursuit of more equitable language models. All code, data and results are available here: https://github.com/iamshnoo/weathub.
</details>
<details>
<summary>摘要</summary>
人类偏见 universal，但不均匀：在语言、文化和社会边界之间存在差异。据 latest literature 表明，语言模型（LM）在人类数据上训练后可能反映和增强社会偏见的效果。然而，现有的大多数研究都集中在西方和欧洲语言上，这使得我们无法充分了解各语言的偏见。在这项工作中，我们将 Word Embedding Association Test（WEAT）扩展到24种语言，以便更广泛的研究和发现有趣的发现。此外，我们还增加了每种语言的地方信息，以捕捉每种语言的当地情况。此外，我们还研究了更广泛的社会偏见维度，包括恶意、残夷和更多的维度。此外，我们进行了全面的印度语言风貌分析，对6种流行的印度语言进行了详细的地方偏见分析。最后，我们强调了这些社会偏见的重要性，并通过对不同的插入方法进行了广泛比较，强调需要对其进行更加公正的补偿。所有代码、数据和结果都可以在以下链接中找到：https://github.com/iamshnoo/weathub。
</details></li>
</ul>
<hr>
<h2 id="1D-Touch-NLP-Assisted-Coarse-Text-Selection-via-a-Semi-Direct-Gesture"><a href="#1D-Touch-NLP-Assisted-Coarse-Text-Selection-via-a-Semi-Direct-Gesture" class="headerlink" title="1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture"></a>1D-Touch: NLP-Assisted Coarse Text Selection via a Semi-Direct Gesture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17576">http://arxiv.org/abs/2310.17576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peiling Jiang, Li Feng, Fuling Sun, Parakrant Sarkar, Haijun Xia, Can Liu</li>
<li>for: 提高touch屏上文本选择的精度和效率，特别是在word和phrase水平上。</li>
<li>methods: 引入1D-Touch方法，使用简单的垂直滑块势件来扩展和缩小选择区域，从word到semantic chunk的范围。</li>
<li>results: 对于coarse-grained文本选择任务，与默认word-snapping方法相比，1D-Touch方法提高选择精度20%。<details>
<summary>Abstract</summary>
Existing text selection techniques on touchscreen focus on improving the control for moving the carets. Coarse-grained text selection on word and phrase levels has not received much support beyond word-snapping and entity recognition. We introduce 1D-Touch, a novel text selection method that complements the carets-based sub-word selection by facilitating the selection of semantic units of words and above. This method employs a simple vertical slide gesture to expand and contract a selection area from a word. The expansion can be by words or by semantic chunks ranging from sub-phrases to sentences. This technique shifts the concept of text selection, from defining a range by locating the first and last words, towards a dynamic process of expanding and contracting a textual semantic entity. To understand the effects of our approach, we prototyped and tested two variants: WordTouch, which offers a straightforward word-by-word expansion, and ChunkTouch, which leverages NLP to chunk text into syntactic units, allowing the selection to grow by semantically meaningful units in response to the sliding gesture. Our evaluation, focused on the coarse-grained selection tasks handled by 1D-Touch, shows a 20% improvement over the default word-snapping selection method on Android.
</details>
<details>
<summary>摘要</summary>
现有的触感屏选Text技术主要关注改进选择框的控制。word和phrase层级的粗体化文本选择尚未得到了多少支持，只有word拖拽和实体识别。我们介绍了1D-Touch，一种新的文本选择方法，该方法补充了基于拖拽的字符选择，并且使用简单的垂直滑块手势来扩展和缩小选择区域。这种方法将文本选择的概念从定义选择范围的first和last字符改变为一个动态的文本semantic实体选择过程。为了了解我们的方法的效果，我们实现了两个变体：WordTouch，它通过直接拖拽word来扩展选择范围，和ChunkTouch，它利用NLP将文本切分成语法单位，使选择可以通过semantically meaningful单位来响应滑块手势的扩展。我们的评估对于Android上的粗体化选择任务表现出了20%的提升。
</details></li>
</ul>
<hr>
<h2 id="DiffS2UT-A-Semantic-Preserving-Diffusion-Model-for-Textless-Direct-Speech-to-Speech-Translation"><a href="#DiffS2UT-A-Semantic-Preserving-Diffusion-Model-for-Textless-Direct-Speech-to-Speech-Translation" class="headerlink" title="DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation"></a>DiffS2UT: A Semantic Preserving Diffusion Model for Textless Direct Speech-to-Speech Translation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17570">http://arxiv.org/abs/2310.17570</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongxin Zhu, Zhujin Gao, Xinyuan Zhou, Zhongyi Ye, Linli Xu</li>
<li>for: 这篇论文主要关注在于如何将传播生成模型（Diffusion Generative Models）有效地应用于语音生成和翻译任务中。</li>
<li>methods: 本论文提出了一种新的扩散模型，通过将扩散前进过程应用在连续的语音表示空间中，而将扩散后退程应用在组合的语音单位空间中。这样可以保留连续语音表示空间的semantic结构，并将连续和组合扩散模型融合起来。</li>
<li>results: 根据实验结果，提出的方法可以与 computationally intensive auto-regressive baselines（500步平均）相比，具有相同的表现（50步平均），并且需要更少的解oding步骤（50步）。<details>
<summary>Abstract</summary>
While Diffusion Generative Models have achieved great success on image generation tasks, how to efficiently and effectively incorporate them into speech generation especially translation tasks remains a non-trivial problem. Specifically, due to the low information density of speech data, the transformed discrete speech unit sequence is much longer than the corresponding text transcription, posing significant challenges to existing auto-regressive models. Furthermore, it is not optimal to brutally apply discrete diffusion on the speech unit sequence while disregarding the continuous space structure, which will degrade the generation performance significantly. In this paper, we propose a novel diffusion model by applying the diffusion forward process in the \textit{continuous} speech representation space, while employing the diffusion backward process in the \textit{discrete} speech unit space. In this way, we preserve the semantic structure of the continuous speech representation space in the diffusion process and integrate the continuous and discrete diffusion models. We conduct extensive experiments on the textless direct speech-to-speech translation task, where the proposed method achieves comparable results to the computationally intensive auto-regressive baselines (500 steps on average) with significantly fewer decoding steps (50 steps).
</details>
<details>
<summary>摘要</summary>
Diffusion生成模型在图像生成任务上取得了很大的成功，但是在语音生成特别是翻译任务中efficiently和有效地 интеGRATEDiffusion模型仍然是一个非轻松的问题。具体来说，由于语音数据的信息密度低，将转换后的整数语音序列与相应的文本译本进行比较，则长得多于文本译本，这会对现有的自动回归模型带来很大的挑战。另外，不是最佳的办法是在语音单元序列上直接施用抽象diffusion，而不考虑语音空间的连续结构，这将导致生成性能下降显著。在这篇论文中，我们提出了一种新的Diffusion模型，通过在连续语音表示空间中应用抽象diffusion进程，而在整数语音单元空间中使用反向抽象diffusion进程。这种方法保留了连续语音表示空间中的semantic结构，并将连续和整数Diffusion模型 integrate。我们在文本无直接语音翻译任务上进行了广泛的实验，并得到了与计算Intensive自动回归基准（500步平均）相比的相对较好的结果，但是需要更少的解码步骤（50步）。
</details></li>
</ul>
<hr>
<h2 id="Navigating-to-Success-in-Multi-Modal-Human-Robot-Collaboration-Analysis-and-Corpus-Release"><a href="#Navigating-to-Success-in-Multi-Modal-Human-Robot-Collaboration-Analysis-and-Corpus-Release" class="headerlink" title="Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis and Corpus Release"></a>Navigating to Success in Multi-Modal Human-Robot Collaboration: Analysis and Corpus Release</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17568">http://arxiv.org/abs/2310.17568</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stephanie M. Lukin, Kimberly A. Pollard, Claire Bonial, Taylor Hudson, Ron Arstein, Clare Voss, David Traum</li>
<li>for: 本研究旨在探讨人类和机器人在远程位置的共同探索方法，以及不同modalities的使用对探索成功的影响。</li>
<li>methods: 本研究使用多modalities的交互方式，包括自然语言指令、2D LIDAR地图和 Upon-request的静止照片，以帮助参与者在远程位置进行探索。</li>
<li>results: 研究发现参与者在不同modalities的使用方式上有不同的策略，这些策略可能与探索不同任务的成功度有关。  addition, the study found that requesting photos may have improved the identification and counting of certain entities (such as doorways) without hindering overall area exploration.<details>
<summary>Abstract</summary>
Human-guided robotic exploration is a useful approach to gathering information at remote locations, especially those that might be too risky, inhospitable, or inaccessible for humans. Maintaining common ground between the remotely-located partners is a challenge, one that can be facilitated by multi-modal communication. In this paper, we explore how participants utilized multiple modalities to investigate a remote location with the help of a robotic partner. Participants issued spoken natural language instructions and received from the robot: text-based feedback, continuous 2D LIDAR mapping, and upon-request static photographs. We noticed that different strategies were adopted in terms of use of the modalities, and hypothesize that these differences may be correlated with success at several exploration sub-tasks. We found that requesting photos may have improved the identification and counting of some key entities (doorways in particular) and that this strategy did not hinder the amount of overall area exploration. Future work with larger samples may reveal the effects of more nuanced photo and dialogue strategies, which can inform the training of robotic agents. Additionally, we announce the release of our unique multi-modal corpus of human-robot communication in an exploration context: SCOUT, the Situated Corpus on Understanding Transactions.
</details>
<details>
<summary>摘要</summary>
人类指导式机器人探索是一种有用的方法，特别是在远程位置的信息收集方面，这些位置可能是危险、不适生存或不可达的。保持远程合作伙伴之间的共同点是一个挑战，这可以通过多模态通信来促进。在这篇论文中，我们探讨了参与者如何使用多种渠道来探索远程位置，并received from the robot：文本反馈、连续2D LIDAR地图和 Upon-request静止照片。我们注意到参与者在不同的渠道使用情况中采取了不同的策略，并 hypothesize 这些差异可能与探索子任务中的成功有关。我们发现，请求照片可能提高了某些关键实体（门户）的识别和计数，并且这种策略不会降低总区域探索的范围。未来的大样本研究可能会揭示更加细化的照片和对话策略的效果，这可以帮助训练机器人代理。此外，我们宣布了我们独特的多模态人机交互 corps：SCOUT，即 Situated Corpus on Understanding Transactions。
</details></li>
</ul>
<hr>
<h2 id="Towards-Matching-Phones-and-Speech-Representations"><a href="#Towards-Matching-Phones-and-Speech-Representations" class="headerlink" title="Towards Matching Phones and Speech Representations"></a>Towards Matching Phones and Speech Representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17558">http://arxiv.org/abs/2310.17558</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gene-Ping Yang, Hao Tang</li>
<li>for: 这个研究是为了使用自我监督学习来学习手机类型。</li>
<li>methods: 该研究使用了匹配中心点和手机嵌入的方法来解决问题。</li>
<li>results: 实验表明，匹配结果能够捕捉手机之间的关系。通过与常规自我监督学习loss函数合作训练，可以大幅提高下游手机分类的性能。<details>
<summary>Abstract</summary>
Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the context of self-supervised learning, and pose it as the problem of matching cluster centroids to phone embeddings. We study two key properties that enable matching, namely, whether cluster centroids of self-supervised representations reduce the variability of phone instances and respect the relationship among phones. We then use the matching result to produce pseudo-labels and introduce a new loss function for improving self-supervised representations. Our experiments show that the matching result captures the relationship among phones. Training the new loss function jointly with the regular self-supervised losses, such as APC and CPC, significantly improves the downstream phone classification.
</details>
<details>
<summary>摘要</summary>
学习手机类型从手机实例中获取信息已经是一个长期存在的问题，而且还在开放状态下进行学习。在这项工作中，我们重新审视了这个问题，并将其转化为匹配中心点与手机嵌入的问题。我们研究了两个关键的特性，即自生 represencing 中心点是否减少手机实例的变化和是否尊重手机之间的关系。然后，我们使用匹配结果生成 Pseudo-标签，并引入了一个新的损失函数来改进自生 represencing。我们的实验表明，匹配结果能够捕捉手机之间的关系。在训练新的损失函数并与常见的自生损失函数，如APC和CPC，并行训练时，下流手机分类得到了显著改进。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-Bias-and-Fairness-in-Gender-Neutral-Pretrained-Vision-and-Language-Models"><a href="#Evaluating-Bias-and-Fairness-in-Gender-Neutral-Pretrained-Vision-and-Language-Models" class="headerlink" title="Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models"></a>Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17530">http://arxiv.org/abs/2310.17530</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/coastalcph/gender-neutral-vl">https://github.com/coastalcph/gender-neutral-vl</a></li>
<li>paper_authors: Laura Cabello, Emanuele Bugliarello, Stephanie Brandl, Desmond Elliott</li>
<li>for: 本研究旨在探讨预训练模型内置的性别偏见如何影响模型性能，以及如何通过不同的预训练和终端训练方法来降低这种偏见。</li>
<li>methods: 本研究使用了三种视觉语言模型家族，通过分别在预训练和终端训练两个阶段进行训练，对模型的性别偏见进行了评估。</li>
<li>results: 研究发现，预训练和终端训练两个阶段的偏见增强是独立的，并且终端训练gender-neutral数据可以降低群体差异，提高模型的公平性。<details>
<summary>Abstract</summary>
Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance.
</details>
<details>
<summary>摘要</summary>
preprained 机器学习模型已知可能导致和加剧现有偏见在数据中，从而导致不公正的结果，最终影响用户体验。因此，我们需要了解这些偏见的机制，以确保模型性能不会导致对某些群体或人口的歧视行为。在这项工作中，我们选择性别偏见作为我们的案例研究。我们量化在预训练和精度调整后的偏见增强。我们调查这两个学习阶段之间的连接，如果有，以及如何将偏见增强反映到模型性能中。总之，我们发现预训练和精度调整的偏见增强是独立的。然后，我们查看继续预训练 gender-neutral 数据后的效果，发现这会降低群体差异，即提高公平性，在 VQAv2 和检索任务上没有显著削弱任务性能。
</details></li>
</ul>
<hr>
<h2 id="The-Validity-of-Evaluation-Results-Assessing-Concurrence-Across-Compositionality-Benchmarks"><a href="#The-Validity-of-Evaluation-Results-Assessing-Concurrence-Across-Compositionality-Benchmarks" class="headerlink" title="The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks"></a>The Validity of Evaluation Results: Assessing Concurrence Across Compositionality Benchmarks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17514">http://arxiv.org/abs/2310.17514</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/compositionalityvalidity">https://github.com/facebookresearch/compositionalityvalidity</a></li>
<li>paper_authors: Kaiser Sun, Adina Williams, Dieuwke Hupkes</li>
<li>for: 本研究旨在探讨如何确定评价 datasets 是否能够准确衡量模型的能力。</li>
<li>methods: 研究使用了 6 种模型方法，并对 4 个 datasets 进行了8种 compositional splitting strategies的比较，并将模型按18个 compositional generalization splits进行排名。</li>
<li>results: 研究结果显示：一、不同的 datasets 对模型方法的评价有所不同; 二、人工生成的 datasets 与其他 datasets 之间的对比更为一致; 三、数据源的样本来源是评价模型的更重要因素，而不是Compositionality的解释; 四、数据中使用的词语项可以强烈影响结论。  overall, 这个研究表明了评价 datasets 的有效性还需要进一步的检验，并建议为领域设置更加严格的评价标准。<details>
<summary>Abstract</summary>
NLP models have progressed drastically in recent years, according to numerous datasets proposed to evaluate performance. Questions remain, however, about how particular dataset design choices may impact the conclusions we draw about model capabilities. In this work, we investigate this question in the domain of compositional generalization. We examine the performance of six modeling approaches across 4 datasets, split according to 8 compositional splitting strategies, ranking models by 18 compositional generalization splits in total. Our results show that: i) the datasets, although all designed to evaluate compositional generalization, rank modeling approaches differently; ii) datasets generated by humans align better with each other than they with synthetic datasets, or than synthetic datasets among themselves; iii) generally, whether datasets are sampled from the same source is more predictive of the resulting model ranking than whether they maintain the same interpretation of compositionality; and iv) which lexical items are used in the data can strongly impact conclusions. Overall, our results demonstrate that much work remains to be done when it comes to assessing whether popular evaluation datasets measure what they intend to measure, and suggest that elucidating more rigorous standards for establishing the validity of evaluation sets could benefit the field.
</details>
<details>
<summary>摘要</summary>
《NLP模型在最近几年内有了很大的进步，根据各种评估表现的数据集提出了许多。然而，有些数据集的设计方式可能会影响我们对模型能力的结论。在本工作中，我们研究了这个问题，以 Compositional generalization 领域为例。我们评估了六种模型方法，在四个数据集上进行了8种 Compositional splitting 策略，并将模型按18种 Compositional generalization 排名。我们的结果表明：一、不同的数据集，即使都是用于评估 Compositional generalization 的，对模型方法的评价有所不同；二、人类生成的数据集更加符合人类生成的数据集，而不是人类生成的数据集和人工生成的数据集之间的对比；三、数据集是否来自同一个源的样本是评估模型排名的更好的预测因素，而不是数据集是否保持同一种 Compositional 性的解释；四、数据集中使用的语言项可以强烈地影响结论。总之，我们的结果表明，评估 NLP 模型的领域还需要很多工作，并建议在设置评估数据集的时候，更加严格地遵循一些标准，以确保评估结果的有效性。》
</details></li>
</ul>
<hr>
<h2 id="The-IMS-Toucan-System-for-the-Blizzard-Challenge-2023"><a href="#The-IMS-Toucan-System-for-the-Blizzard-Challenge-2023" class="headerlink" title="The IMS Toucan System for the Blizzard Challenge 2023"></a>The IMS Toucan System for the Blizzard Challenge 2023</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17499">http://arxiv.org/abs/2310.17499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/digitalphonetics/ims-toucan">https://github.com/digitalphonetics/ims-toucan</a></li>
<li>paper_authors: Florian Lux, Julia Koch, Sarina Meyer, Thomas Bott, Nadja Schauffler, Pavel Denisov, Antje Schweitzer, Ngoc Thang Vu</li>
<li>for: 这个论文是为了提高法语音识别系统的性能而写的。</li>
<li>methods: 该论文使用了一种基于规则的文本到phoneme处理系统，包括法语中 homograph 的规则化解决方法。然后将phoneme转换为spectrogram作为中间表示，使用Conformer和Glow架构实现快速和高效的非autoregressive生成架构。最后，使用GAN基于神经网络 vocoder将spectrogram转换为最终波形。</li>
<li>results: 作者在Blizzard Challenge 2023中提交的系统实现了优化，与Blizzard Challenge 2021中提交的系统相比，提高了性能。<details>
<summary>Abstract</summary>
For our contribution to the Blizzard Challenge 2023, we improved on the system we submitted to the Blizzard Challenge 2021. Our approach entails a rule-based text-to-phoneme processing system that includes rule-based disambiguation of homographs in the French language. It then transforms the phonemes to spectrograms as intermediate representations using a fast and efficient non-autoregressive synthesis architecture based on Conformer and Glow. A GAN based neural vocoder that combines recent state-of-the-art approaches converts the spectrogram to the final wave. We carefully designed the data processing, training, and inference procedures for the challenge data. Our system identifier is G. Open source code and demo are available.
</details>
<details>
<summary>摘要</summary>
为我们在Blizzard Challenge 2023中的贡献，我们对我们在Blizzard Challenge 2021中提交的系统进行了改进。我们的方法包括一个基于规则的文本到音节处理系统，其中包括基于法语 homographs 的规则化扩展。然后，它将音节转换为中间表示使用一个高效的非 autoregressive 合成架构，基于 Conformer 和 Glow。一个基于 GAN 的神经 vocoder 将spectrogram转换为最终波形。我们仔细设计了挑战数据的数据处理、训练和推断过程。我们的系统标识符是 G。开源代码和示例程序可以获得。
</details></li>
</ul>
<hr>
<h2 id="LightLM-A-Lightweight-Deep-and-Narrow-Language-Model-for-Generative-Recommendation"><a href="#LightLM-A-Lightweight-Deep-and-Narrow-Language-Model-for-Generative-Recommendation" class="headerlink" title="LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation"></a>LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17488">http://arxiv.org/abs/2310.17488</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/dongyuanjushi/lightlm">https://github.com/dongyuanjushi/lightlm</a></li>
<li>paper_authors: Kai Mei, Yongfeng Zhang</li>
<li>for: 这篇论文旨在提出一种轻量级的Transformer模型，用于生成推荐。</li>
<li>methods: 该模型使用了一种特定针对推荐任务的深度和窄针对Transformer结构，以及一种 Spectral Collaborative Indexing (SCI) 和 Graph Collaborative Indexing (GCI) 方法来优化模型的性能。</li>
<li>results: 实验结果表明，LightLM 可以在实际数据集上超过多种竞争对手， both in terms of 推荐准确率和效率。<details>
<summary>Abstract</summary>
This paper presents LightLM, a lightweight Transformer-based language model for generative recommendation. While Transformer-based generative modeling has gained importance in various AI sub-fields such as NLP and vision, generative recommendation is still in its infancy due to its unique demand on personalized generative modeling. Existing works on generative recommendation often use NLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are heavy-weight and are not specifically designed for recommendation tasks. LightLM tackles the issue by introducing a light-weight deep and narrow Transformer architecture, which is specifically tailored for direct generation of recommendation items. This structure is especially apt for straightforward generative recommendation and stems from the observation that language model does not have to be too wide for this task, as the input predominantly consists of short tokens that are well-suited for the model's capacity. We also show that our devised user and item ID indexing methods, i.e., Spectral Collaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables the deep and narrow Transformer architecture to outperform large-scale language models for recommendation. Besides, to address the hallucination problem of generating items as output, we propose the constrained generation process for generative recommenders. Experiments on real-world datasets show that LightLM outperforms various competitive baselines in terms of both recommendation accuracy and efficiency. The code can be found at https://github.com/dongyuanjushi/LightLM.
</details>
<details>
<summary>摘要</summary>
To further enhance the performance of LightLM, the paper proposes two indexing methods, Spectral Collaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), which enable the model to outperform large-scale language models for recommendation. Additionally, the paper addresses the hallucination problem of generating items as output by proposing a constrained generation process for generative recommenders.Experiments on real-world datasets demonstrate that LightLM outperforms various competitive baselines in terms of both recommendation accuracy and efficiency. The code for LightLM can be found at https://github.com/dongyuanjushi/LightLM.
</details></li>
</ul>
<hr>
<h2 id="Dialect-Adaptation-and-Data-Augmentation-for-Low-Resource-ASR-TalTech-Systems-for-the-MADASR-2023-Challenge"><a href="#Dialect-Adaptation-and-Data-Augmentation-for-Low-Resource-ASR-TalTech-Systems-for-the-MADASR-2023-Challenge" class="headerlink" title="Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge"></a>Dialect Adaptation and Data Augmentation for Low-Resource ASR: TalTech Systems for the MADASR 2023 Challenge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17448">http://arxiv.org/abs/2310.17448</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tanel Alumäe, Jiaming Kong, Daniil Robnikov</li>
<li>for: 这篇论文描述了塔林大学技术（TalTech）为ASRU MADASR 2023挑战制定的系统。挑战的目标是自动识别具有方言多样性的印度语言，使用有限的训练音频和文本数据。</li>
<li>methods: 我们的方法与传统的精度训练模型 fine-tuning 不同，在两个关键点上有所创新：首先，通过实施协调数据扩充技术，提高训练数据的语言多样性；其次，通过深度预FIX Tuning 进行方言适应mod型。</li>
<li>results: 在两个跑道上，我们的方法实现了比基eline低的单词错误率，在所有参赛队伍中获得最低的成绩。<details>
<summary>Abstract</summary>
This paper describes Tallinn University of Technology (TalTech) systems developed for the ASRU MADASR 2023 Challenge. The challenge focuses on automatic speech recognition of dialect-rich Indian languages with limited training audio and text data. TalTech participated in two tracks of the challenge: Track 1 that allowed using only the provided training data and Track 3 which allowed using additional audio data. In both tracks, we relied on wav2vec2.0 models. Our methodology diverges from the traditional procedure of finetuning pretrained wav2vec2.0 models in two key points: firstly, through the implementation of the aligned data augmentation technique to enhance the linguistic diversity of the training data, and secondly, via the application of deep prefix tuning for dialect adaptation of wav2vec2.0 models. In both tracks, our approach yielded significant improvements over the provided baselines, achieving the lowest word error rates across all participating teams.
</details>
<details>
<summary>摘要</summary>
这篇论文介绍了塔林大学科技（TalTech）为ASRU MADASR 2023 挑战开发的系统。挑战目标是使用少量训练音频和文本数据自动识别具有方言落差的印度语言。TalTech 参与了两个赛道：第一个赛道允许只使用提供的训练数据，第三个赛道允许使用额外的音频数据。在两个赛道中，我们采用了两点不同的方法：首先，通过实施对齐数据增强技术来提高训练数据的语言多样性，其次，通过深度预refix 调整来进行方言适应。在两个赛道中，我们的方法比提供的基准值具有显著改进，实现了所有参与队伍中最低的单词错误率。
</details></li>
</ul>
<hr>
<h2 id="‘’Fifty-Shades-of-Bias’’-Normative-Ratings-of-Gender-Bias-in-GPT-Generated-English-Text"><a href="#‘’Fifty-Shades-of-Bias’’-Normative-Ratings-of-Gender-Bias-in-GPT-Generated-English-Text" class="headerlink" title="‘’Fifty Shades of Bias’’: Normative Ratings of Gender Bias in GPT Generated English Text"></a>‘’Fifty Shades of Bias’’: Normative Ratings of Gender Bias in GPT Generated English Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17428">http://arxiv.org/abs/2310.17428</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rishav Hada, Agrima Seth, Harshita Diddee, Kalika Bali</li>
<li>for: This paper aims to investigate the gender bias in language models and its impact on the generation of text.</li>
<li>methods: The authors use a dataset of GPT-generated English text with normative ratings of gender bias, and employ Best–Worst Scaling to obtain the ratings. They also analyze the variation of themes of gender biases in the observed ranking.</li>
<li>results: The authors show that identity-attack is most closely related to gender bias, and evaluate the performance of existing automated models trained on related concepts on their dataset.<details>
<summary>Abstract</summary>
Language serves as a powerful tool for the manifestation of societal belief systems. In doing so, it also perpetuates the prevalent biases in our society. Gender bias is one of the most pervasive biases in our society and is seen in online and offline discourses. With LLMs increasingly gaining human-like fluency in text generation, gaining a nuanced understanding of the biases these systems can generate is imperative. Prior work often treats gender bias as a binary classification task. However, acknowledging that bias must be perceived at a relative scale; we investigate the generation and consequent receptivity of manual annotators to bias of varying degrees. Specifically, we create the first dataset of GPT-generated English text with normative ratings of gender bias. Ratings were obtained using Best--Worst Scaling -- an efficient comparative annotation framework. Next, we systematically analyze the variation of themes of gender biases in the observed ranking and show that identity-attack is most closely related to gender bias. Finally, we show the performance of existing automated models trained on related concepts on our dataset.
</details>
<details>
<summary>摘要</summary>
文化信仰系统的表达工具，即语言，同时也推动社会中普遍存在的偏见。 gender bias 是社会中最普遍的偏见之一，可见在线上和线下的交流中。随着人工智能语言模型（LLM）的发展，我们必须有一个深刻的理解，这些系统可以生成的偏见是多么的。在先前的工作中， gender bias 常被视为二分类任务。然而，我们认为偏见应该被评估为相对的程度，我们开创了首个 GPT 生成的英文文本中的normative 评分。我们使用了 Best--Worst Scaling  Comparative Annotation Framework 来获取评分。接下来，我们系统地分析了观察到的性别偏见主题的变化，并发现 identity-attack 最为相关于性别偏见。最后，我们展示了对我们数据集的现有自动化模型的性能。
</details></li>
</ul>
<hr>
<h2 id="Harnessing-GPT-3-5-turbo-for-Rhetorical-Role-Prediction-in-Legal-Cases"><a href="#Harnessing-GPT-3-5-turbo-for-Rhetorical-Role-Prediction-in-Legal-Cases" class="headerlink" title="Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases"></a>Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17413">http://arxiv.org/abs/2310.17413</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anas Belfathi, Nicolas Hernandez, Laura Monceaux</li>
<li>for: 本研究旨在为querying一个大型预训练的生成器 transformer（GPT-3.5-turbo）在法律案件中的辩论角色预测任务进行全面的研究。这个任务需要处理文本上下文。</li>
<li>methods: 本研究使用了零数少shot策略、任务特定定义和文本上下文的解释、通用提示和特定问题来探索适用于这个任务的一Stage抽象技术。</li>
<li>results: 研究发现，数量的增加、标签的定义、文本上下文的显示和特定问题对模型的性能有积极的影响。在不同的测试集配置下，我们发现，使用一些直接来自上下文的标签的例子提示可以使模型在Weighted F1 score中达到72%的性能，但还有一定的差距需要追赶到最佳系统（86%），这些系统需要专门的资源、体系和训练。<details>
<summary>Abstract</summary>
We propose a comprehensive study of one-stage elicitation techniques for querying a large pre-trained generative transformer (GPT-3.5-turbo) in the rhetorical role prediction task of legal cases. This task is known as requiring textual context to be addressed. Our study explores strategies such as zero-few shots, task specification with definitions and clarification of annotation ambiguities, textual context and reasoning with general prompts and specific questions. We show that the number of examples, the definition of labels, the presentation of the (labelled) textual context and specific questions about this context have a positive influence on the performance of the model. Given non-equivalent test set configurations, we observed that prompting with a few labelled examples from direct context can lead the model to a better performance than a supervised fined-tuned multi-class classifier based on the BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach the performance of the best systems = 86%) in the LegalEval 2023 task which, on the other hand, require dedicated resources, architectures and training.
</details>
<details>
<summary>摘要</summary>
我们提议进行一项涵盖一阶段 solicitation 技术的大规模预训练生成器（GPT-3.5-turbo）在法律案例中的辩论角色预测任务中进行全面的研究。这项任务需要处理文本背景，我们的研究探讨了策略如零些例、任务特定定义和修饰、文本背景和推理的通用提示和特定问题。我们发现了数量、标签定义、文本背景的显示和特定问题的影响于模型的性能。给定不同的测试集配置，我们发现，使用 direct context 中标签的几个例子提问可以使模型达到比supervised fine-tuned multi-class classifier（基于 BERT 编码器）的性能（weighted F1 分数 = 72%），但还有一定的差距，与最佳系统（86%）在 LegalEval 2023 任务中的性能相比。
</details></li>
</ul>
<hr>
<h2 id="Tackling-the-Matrix-Multiplication-Micro-kernel-Generation-with-Exo"><a href="#Tackling-the-Matrix-Multiplication-Micro-kernel-Generation-with-Exo" class="headerlink" title="Tackling the Matrix Multiplication Micro-kernel Generation with Exo"></a>Tackling the Matrix Multiplication Micro-kernel Generation with Exo</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17408">http://arxiv.org/abs/2310.17408</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/adcastel/exo_ukr_generator">https://github.com/adcastel/exo_ukr_generator</a></li>
<li>paper_authors: Adrián Castelló, Julian Bellavita, Grace Dinh, Yuka Ikarashi, Héctor Martínez</li>
<li>for: 本研究旨在提高矩阵乘法（GEMM）的优化，以提高现代线性代数库（如BLIS、OpenBLAS、Intel OneAPI）的性能。</li>
<li>methods: 本研究使用Exo编译器生成微型核心代码，以实现close to（或更好于）手动编写的微型核心代码。此外，本解决方案提高了代码的可移植性，因为硬件目标完全由一个简洁的库所 descriptions。</li>
<li>results: 本研究显示，使用Exo编译器生成微型核心代码可以实现 close to（或更好于）手动编写的微型核心代码，并提高代码的可移植性。<details>
<summary>Abstract</summary>
The optimization of the matrix multiplication (or GEMM) has been a need during the last decades. This operation is considered the flagship of current linear algebra libraries such as BLIS, OpenBLAS, or Intel OneAPI because of its widespread use in a large variety of scientific applications. The GEMM is usually implemented following the GotoBLAS philosophy, which tiles the GEMM operands and uses a series of nested loops for performance improvement. These approaches extract the maximum computational power of the architectures through small pieces of hardware-oriented, high-performance code called micro-kernel. However, this approach forces developers to generate, with a non-negligible effort, a dedicated micro-kernel for each new hardware.   In this work, we present a step-by-step procedure for generating micro-kernels with the Exo compiler that performs close to (or even better than) manually developed microkernels written with intrinsic functions or assembly language. Our solution also improves the portability of the generated code, since a hardware target is fully specified by a concise library-based description of its instructions.
</details>
<details>
<summary>摘要</summary>
“矩阵乘法（GEMM）的优化在过去几十年中一直是一个需求。这个操作被当今的线性代数库（如BLIS、OpenBLAS或Intel OneAPI）视为标杆操作，因为它在许多科学应用中广泛使用。GEMM通常采用GotoBLAS哲学，即将GEMM参数瓦bben分割并使用一系列嵌套循环来提高性能。这些方法利用硬件的最大计算能力，通过小块硬件特定、高性能的代码块（微型核心）来提高性能。然而，这种方法需要开发者投入一定的努力来生成每个新硬件的专门微型核心。在这个工作中，我们提供了一个步骤式的过程，使用Exo编译器生成微型核心，其性能与手动编写的微型核心或 Assembly 语言代码几乎相当。我们的解决方案还提高了生成代码的可移植性，因为硬件目标可以通过一个简洁的库所示的指令来完全定义。”
</details></li>
</ul>
<hr>
<h2 id="Meaning-and-understanding-in-large-language-models"><a href="#Meaning-and-understanding-in-large-language-models" class="headerlink" title="Meaning and understanding in large language models"></a>Meaning and understanding in large language models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17407">http://arxiv.org/abs/2310.17407</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Vladimír Havlík</li>
<li>for: 本文总结了现代人工智能语言模型的发展，并评估了传统哲学假设中关于机器语言理解的假设。</li>
<li>methods: 本文使用了生成式大语言模型来评估机器语言理解的水平，并检验了现代语言模型是否具有真正的语言理解能力。</li>
<li>results: 研究结果显示，现代语言模型可以具有深层次的语言理解能力，不仅仅是 superficielle 的语法处理。<details>
<summary>Abstract</summary>
Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the simulation of understanding, which is only partial and very shallow, without sufficient referential grounding in the world. The aim is to highlight the conditions crucial to attributing natural language understanding to state-of-the-art LLMs, where it can be legitimately argued that LLMs not only use syntax but also semantics, their understanding not being simulated but duplicated; and determine how they ground the meanings of linguistic expressions.
</details>
<details>
<summary>摘要</summary>
can 机器理解自然语言吗？最近的人工智能生成大语言模型（LLMs）的发展已经让人们认为传统哲学假设关于机器语言理解需要修订。这篇文章批判现在人们通常认为机器语言表现只是Syntax的 manipulate和模拟理解，而不是真正的 semantics 理解，而且这种理解并不具备充分的 referential 基础。文章的目标是 highlighting  state-of-the-art LLMs 才能够被合理地 argue 不仅使用 syntax 还使用 semantics，其理解不是 simulated 而是 duplicated，并 determin 语言表达意义的基础。
</details></li>
</ul>
<hr>
<h2 id="Language-and-Mental-Health-Measures-of-Emotion-Dynamics-from-Text-as-Linguistic-Biosocial-Markers"><a href="#Language-and-Mental-Health-Measures-of-Emotion-Dynamics-from-Text-as-Linguistic-Biosocial-Markers" class="headerlink" title="Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers"></a>Language and Mental Health: Measures of Emotion Dynamics from Text as Linguistic Biosocial Markers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17369">http://arxiv.org/abs/2310.17369</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daniela Teodorescu, Tiffany Cheng, Alona Fyshe, Saif M. Mohammad</li>
<li>For: This paper aims to investigate the relationship between tweet emotion dynamics and mental health disorders.* Methods: The authors use a dataset of tweets and employ recent approaches to determining emotion dynamics from everyday utterances.* Results: The study finds that each of the emotion dynamics metrics studied varies by the user’s self-disclosed diagnosis, and that linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses.Here’s the same information in Simplified Chinese text:* For: 这篇论文目的是调查推特情感动力与心理疾病的关系。* Methods: 作者使用了一个推特数据集，并采用了现代的语言预测方法来确定情感动力。* Results: 研究发现，每一个情感动力指标都与用户自我报告的疾病有关，并且语言上的情感动力征具有识别心理疾病的重要作用。<details>
<summary>Abstract</summary>
Research in psychopathology has shown that, at an aggregate level, the patterns of emotional change over time -- emotion dynamics -- are indicators of one's mental health. One's patterns of emotion change have traditionally been determined through self-reports of emotions; however, there are known issues with accuracy, bias, and convenience. Recent approaches to determining emotion dynamics from one's everyday utterances, addresses many of these concerns, but it is not yet known whether these measures of utterance emotion dynamics (UED) correlate with mental health diagnoses. Here, for the first time, we study the relationship between tweet emotion dynamics and mental health disorders. We find that each of the UED metrics studied varied by the user's self-disclosed diagnosis. For example: average valence was significantly higher (i.e., more positive text) in the control group compared to users with ADHD, MDD, and PTSD. Valence variability was significantly lower in the control group compared to ADHD, depression, bipolar disorder, MDD, PTSD, and OCD but not PPD. Rise and recovery rates of valence also exhibited significant differences from the control. This work provides important early evidence for how linguistic cues pertaining to emotion dynamics can play a crucial role as biosocial markers for mental illnesses and aid in the understanding, diagnosis, and management of mental health disorders.
</details>
<details>
<summary>摘要</summary>
研究发现，在总体水平上，情感变化趋势 -- 情感动力学 -- 是诊断心理健康的指标。传统的情感变化评估方法是通过自我报告情感，但这些方法存在准确性、偏见和便利性问题。现在的方法是从日常语言中提取情感动力学，解决了这些问题，但是不知道这些措施与心理疾病诊断是否相关。本研究是第一次研究推特情感动力学与心理疾病的关系。我们发现，每一个UED指标都与用户自我报告的疾病有关。例如：正常组比对ADHD、抑郁症、PTSD等用户的平均值更高（即更正面的文本）。变化的值的变化率低于正常组，但是与抑郁症、跨悲症、偏抑郁症、抑郁症、PTSD和OCD有关。这项工作提供了关键的早期证据，证明语言上的情感动力学指标可以作为心理疾病的生物社会标志，帮助理解、诊断和管理心理健康疾病。
</details></li>
</ul>
<hr>
<h2 id="ACT-SQL-In-Context-Learning-for-Text-to-SQL-with-Automatically-Generated-Chain-of-Thought"><a href="#ACT-SQL-In-Context-Learning-for-Text-to-SQL-with-Automatically-Generated-Chain-of-Thought" class="headerlink" title="ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought"></a>ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17342">http://arxiv.org/abs/2310.17342</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/x-lance/text2sql-gpt">https://github.com/x-lance/text2sql-gpt</a></li>
<li>paper_authors: Hanchong Zhang, Ruisheng Cao, Lu Chen, Hongshen Xu, Kai Yu</li>
<li>for: 提高大语言模型（LLMs）在文本到SQL任务中的逻辑能力</li>
<li>methods: 使用链条思维（CoT）提问和自动生成Auto-CoT参考实例，不需手动标注</li>
<li>results: LLMs的性能得到改善，在多turn文本到SQL任务中也有优秀表现，达到了现有的SOTA水平<details>
<summary>Abstract</summary>
Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Arabic-Fine-Grained-Entity-Recognition"><a href="#Arabic-Fine-Grained-Entity-Recognition" class="headerlink" title="Arabic Fine-Grained Entity Recognition"></a>Arabic Fine-Grained Entity Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17333">http://arxiv.org/abs/2310.17333</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haneen Liqreina, Mustafa Jarrar, Mohammed Khalilia, Ahmed Oumar El-Shangiti, Muhammad AbdulMageed</li>
<li>For: This paper aims to advance Arabic Named Entity Recognition (NER) with fine-grained entities, specifically by extending the Wojood corpus with 31 subtypes for four main entity types (GPE, LOC, ORG, and FAC).* Methods: The authors first revised Wojood’s annotations to be compatible with the LDC’s ACE guidelines, and then manually annotated all mentions of GPE, LOC, ORG, and FAC (~44K) with the LDC’s ACE sub-types. They also fine-tuned three pre-trained Arabic BERT encoders in three settings to compute the baselines of WojoodF ine.* Results: The authors achieved inter-annotator agreement (IAA) of 0.9861 and 0.9889 using Cohen’s Kappa and F1 score, respectively. They also achieved F1 score of 0.920, 0.866, and 0.885 in three settings of fine-tuning the pre-trained Arabic BERT encoders.Here are the three points in Simplified Chinese text:* 为：本文目的是提高阿拉伯语名实体识别（NER）的细化实体识别，具体是将 Wojood  корпусу扩展到4类主要实体类型（GPE、LOC、ORG、FAC）中的31个子类型。* 方法：作者首先将 Wojood 的注释更改为与 LDC 的 ACE 指南兼容，并手动注释了 Wojood 中约44,000 个 GPE、LOC、ORG 和 FAC 的所有提及。他们还使用三个预训练的阿拉伯语 BERT 核心进行三种设定的 fine-tuning，以计算 WojoodF ine 的基线。* 结果：作者达到了 Cohen 的卡方和 F1 得分的共识度（IAA）为 0.9861 和 0.9889，并在三种设定中达到了 fine-tuning 预训练的阿拉伯语 BERT 核心的 F1 得分为 0.920、0.866 和 0.885。<details>
<summary>Abstract</summary>
Traditional NER systems are typically trained to recognize coarse-grained entities, and less attention is given to classifying entities into a hierarchy of fine-grained lower-level subtypes. This article aims to advance Arabic NER with fine-grained entities. We chose to extend Wojood (an open-source Nested Arabic Named Entity Corpus) with subtypes. In particular, four main entity types in Wojood, geopolitical entity (GPE), location (LOC), organization (ORG), and facility (FAC), are extended with 31 subtypes. To do this, we first revised Wojood's annotations of GPE, LOC, ORG, and FAC to be compatible with the LDC's ACE guidelines, which yielded 5, 614 changes. Second, all mentions of GPE, LOC, ORG, and FAC (~44K) in Wojood are manually annotated with the LDC's ACE sub-types. We refer to this extended version of Wojood as WojoodF ine. To evaluate our annotations, we measured the inter-annotator agreement (IAA) using both Cohen's Kappa and F1 score, resulting in 0.9861 and 0.9889, respectively. To compute the baselines of WojoodF ine, we fine-tune three pre-trained Arabic BERT encoders in three settings: flat NER, nested NER and nested NER with subtypes and achieved F1 score of 0.920, 0.866, and 0.885, respectively. Our corpus and models are open-source and available at https://sina.birzeit.edu/wojood/.
</details>
<details>
<summary>摘要</summary>
传统的NER系统通常只会训练粗粒度的实体识别，对于细致的实体类型进行分类不受充分关注。这篇文章旨在提高阿拉伯语NER的细致性。我们选择将 Wojood（一个开源的嵌套式阿拉伯语命名实体词典）扩展到子类型。特别是，Wojood中的四个主要实体类型（地域政治实体（GPE）、位置（LOC）、组织（ORG）和设施（FAC））被扩展到31个子类型。为此，我们首先修改了Wojood中GPE、LOC、ORG和FAC的注释，使其与LDC的ACE指南相容，共计5,614个更改。然后，Wojood中所有GPE、LOC、ORG和FAC的提及（约44,000个）都被手动注释为LDC的ACE子类型。我们称这个扩展版本为Wojood Fine。为了评估我们的注释，我们使用了COhen的Kappa和F1得分来计算同义词一致性，得到0.9861和0.9889的值。为了计算Wojood Fine的基准值，我们在三种设置下练习三个预训练的阿拉伯语BERTEncoder：普通的NER、嵌套NER和嵌套NER+子类型，并 achieved F1得分为0.920、0.866和0.885，分别。我们的词典和模型都是开源的，可以在https://sina.birzeit.edu/wojood/获取。
</details></li>
</ul>
<hr>
<h2 id="Nabra-Syrian-Arabic-Dialects-with-Morphological-Annotations"><a href="#Nabra-Syrian-Arabic-Dialects-with-Morphological-Annotations" class="headerlink" title="Nabra: Syrian Arabic Dialects with Morphological Annotations"></a>Nabra: Syrian Arabic Dialects with Morphological Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17315">http://arxiv.org/abs/2310.17315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amal Nayouf, Tymaa Hammouda, Mustafa Jarrar, Fadi Zaraket, Mohamad-Bassam Kurdy</li>
<li>For: 这个论文是为了提供一个包含叙利亚阿拉伯语言方言的 corpora，以便进行语言研究和应用。* Methods: 这个论文使用了社交媒体帖子、电影和电视剧cript、歌曲歌词和地方谚语等多种来源，收集了超过6万句话，共60万个单词，并对这些单词进行了全 morphological 注释。* Results: 这个论文通过使用 nine 名native annotator 进行注释，实现了高质量的注释，F1和κ合理性分数在不同特征上的范围为74%-98%。该 corpora 已经公开发布，可以在 Currasat 门户网站上获取：<a target="_blank" rel="noopener" href="https://sina.birzeit.edu/currasat%E3%80%82">https://sina.birzeit.edu/currasat。</a><details>
<summary>Abstract</summary>
This paper presents Nabra, a corpora of Syrian Arabic dialects with morphological annotations. A team of Syrian natives collected more than 6K sentences containing about 60K words from several sources including social media posts, scripts of movies and series, lyrics of songs and local proverbs to build Nabra. Nabra covers several local Syrian dialects including those of Aleppo, Damascus, Deir-ezzur, Hama, Homs, Huran, Latakia, Mardin, Raqqah, and Suwayda. A team of nine annotators annotated the 60K tokens with full morphological annotations across sentence contexts. We trained the annotators to follow methodological annotation guidelines to ensure unique morpheme annotations, and normalized the annotations. F1 and kappa agreement scores ranged between 74% and 98% across features, showing the excellent quality of Nabra annotations. Our corpora are open-source and publicly available as part of the Currasat portal https://sina.birzeit.edu/currasat.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="An-Ensemble-Method-Based-on-the-Combination-of-Transformers-with-Convolutional-Neural-Networks-to-Detect-Artificially-Generated-Text"><a href="#An-Ensemble-Method-Based-on-the-Combination-of-Transformers-with-Convolutional-Neural-Networks-to-Detect-Artificially-Generated-Text" class="headerlink" title="An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text"></a>An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17312">http://arxiv.org/abs/2310.17312</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vijini Liyanage, Davide Buscaldi</li>
<li>for: 本研究旨在探讨使用自然语言生成器（Natural Language Generation，NLG）生成的文本是否能够自动分类为人工生成或人类写作。</li>
<li>methods: 本研究使用了一些 ensemble transformer 模型，包括 Sci-BERT、DeBERTa 和 XLNet，以及卷积神经网络（Convolutional Neural Networks，CNNs）。</li>
<li>results: 我们的实验结果表明，使用 ensemble 模型可以超越单独的 transformer 模型的性能，而 SciBERT-CNN ensemble 模型在 ALTA 共享任务 2023 数据集上达到了 F1 分数为 98.36%。<details>
<summary>Abstract</summary>
Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.
</details>
<details>
<summary>摘要</summary>
thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.Here's the translation in Traditional Chinese:感谢现代大型语言模型（LLMs），语言生成已经到达了出色的水平。这些模型能够生成高品质的内容，因此对于实际生成文本和人写文本的区别成为一个挑战。尽管自然语言生成具有许多优点，但是无法自动识别生成文本的问题可能会导致道德问题。因此，设计和开发检测人工内容的方法ologies是非常重要的。在我们的工作中，我们提出了一些由transformer模型（如Sci-BERT、DeBERTa和XLNet）和卷积神经网（CNNs）构成的分类模型。我们的实验结果显示，考虑的结合架构在分类方面表现更高。此外，我们提出的SciBERT-CNN结合模型在ALTA共享任务2023数据集上产生了98.36%的F1分。
</details></li>
</ul>
<hr>
<h2 id="Learning-to-Abstract-with-Nonparametric-Variational-Information-Bottleneck"><a href="#Learning-to-Abstract-with-Nonparametric-Variational-Information-Bottleneck" class="headerlink" title="Learning to Abstract with Nonparametric Variational Information Bottleneck"></a>Learning to Abstract with Nonparametric Variational Information Bottleneck</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17284">http://arxiv.org/abs/2310.17284</a></li>
<li>repo_url: None</li>
<li>paper_authors: Melika Behjati, Fabio Fehr, James Henderson</li>
<li>for: 提高语言模型的鲁棒性和抗干扰能力</li>
<li>methods: 使用Nonparametric Variational Information Bottleneck（NVIB）压缩Transformer自注意层，实现模型层次结构中的压缩和抽象</li>
<li>results: 模型可以更好地捕捉语言特征，并且具有更高的鲁棒性和抗干扰能力<details>
<summary>Abstract</summary>
Learned representations at the level of characters, sub-words, words and sentences, have each contributed to advances in understanding different NLP tasks and linguistic phenomena. However, learning textual embeddings is costly as they are tokenization specific and require different models to be trained for each level of abstraction. We introduce a novel language representation model which can learn to compress to different levels of abstraction at different layers of the same model. We apply Nonparametric Variational Information Bottleneck (NVIB) to stacked Transformer self-attention layers in the encoder, which encourages an information-theoretic compression of the representations through the model. We find that the layers within the model correspond to increasing levels of abstraction and that their representations are more linguistically informed. Finally, we show that NVIB compression results in a model which is more robust to adversarial perturbations.
</details>
<details>
<summary>摘要</summary>
学习的表示形式在字符、子词、词和句子等多个层次上，各自为不同的自然语言处理任务和语言现象带来了进步。然而，学习文本嵌入是费时的，因为它们是特定的Tokenization的，需要为每个层次投入不同的模型。我们介绍了一种新的语言表示模型，可以在同一个模型中学习压缩到不同的层次。我们在编码器中使用非参数的可变信息瓶颈（NVIB），将核心Transformer自我注意层堆叠在一起，这使得模型中的表示进行了信息学 compression。我们发现模型中的层次对应于不同的层次抽象，并且它们的表示更加语言化。最后，我们发现NVIB压缩后的模型更加抗性 adversarial perturbations。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Logical-Forms-improve-fidelity-in-Table-to-Text-generation"><a href="#Automatic-Logical-Forms-improve-fidelity-in-Table-to-Text-generation" class="headerlink" title="Automatic Logical Forms improve fidelity in Table-to-Text generation"></a>Automatic Logical Forms improve fidelity in Table-to-Text generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17279">http://arxiv.org/abs/2310.17279</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alonsoapp/tlt">https://github.com/alonsoapp/tlt</a></li>
<li>paper_authors: Iñigo Alonso, Eneko Agirre</li>
<li>for: 这个论文主要写于如何从表格生成自然语言陈述。</li>
<li>methods: 该论文使用了自动生成的逻辑形式（LF），以提高文本的事实准确性。</li>
<li>results: 研究发现，使用自动生成的LF可以提高文本的事实准确性，相比之前的系统不使用LF。此外，研究还发现了高事实准确性的主要挑战，包括自动选择内容、逻辑到文本转换和表格到逻辑转换。<details>
<summary>Abstract</summary>
Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
</details>
<details>
<summary>摘要</summary>
tables-to-text 系统可以从结构化数据中生成自然语言声明。而通过终端技术，产生的声明准确性（loyalty）很低。一项之前的研究发现，使用手动逻辑形式（LF）可以提高声明的准确性。然而，使用自动生成的LF是否有效？或者是选择内容alone 贡献了改进。我们提出了 TlT，它可以从表格和选择的内容中生成逻辑形式，然后生成文本声明。我们首次发现，使用自动生成的LF可以提高质量，准确性提高30个点。我们的实验表明，自动选择内容是首要挑战，然后是逻辑到文本转换，并且这些挑战减少了一些。
</details></li>
</ul>
<hr>
<h2 id="Understanding-the-Role-of-Input-Token-Characters-in-Language-Models-How-Does-Information-Loss-Affect-Performance"><a href="#Understanding-the-Role-of-Input-Token-Characters-in-Language-Models-How-Does-Information-Loss-Affect-Performance" class="headerlink" title="Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?"></a>Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17271">http://arxiv.org/abs/2310.17271</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ahmed Alajrami, Katerina Margatina, Nikolaos Aletras</li>
<li>for: 本研究旨在探讨如何在自然语言处理中理解预训练语言模型（PLMs）对语言的学习。</li>
<li>methods: 本研究使用了小 subsets of characters from individual tokens进行预训练语言模型。</li>
<li>results:  surprisingly, 我们发现，即使在极端设置下（即只使用每个токен中的一个字符），预训练后的模型在标准NLU任务和探测任务中的性能保留率相对较高，比如使用单个第一个字符从tokentoken中预训练的模型在SuperGLUE和GLUE任务中的性能保留率分别为 approximately $90$%和$77$%。<details>
<summary>Abstract</summary>
Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately $90$\% and $77$\% of the full-token model in SuperGLUE and GLUE tasks, respectively.
</details>
<details>
<summary>摘要</summary>
理解PLMs如何学习语言是自然语言处理领域的开放挑战。先前的工作主要集中在确定PLMs是否捕捉 semantics和 sintaxis信息，以及数据或预训练目标对其性能的影响。然而，根据我们所知，没有任何前一项工作专门检查了输入token字符损失对PLMs的影响。在这项研究中，我们解决这个空白，通过使用个体token中的小subset的字符进行预训练语言模型。Resultingly, we find that even under extreme settings, i.e., using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively.
</details></li>
</ul>
<hr>
<h2 id="EMMA-X-An-EM-like-Multilingual-Pre-training-Algorithm-for-Cross-lingual-Representation-Learning"><a href="#EMMA-X-An-EM-like-Multilingual-Pre-training-Algorithm-for-Cross-lingual-Representation-Learning" class="headerlink" title="EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning"></a>EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17233">http://arxiv.org/abs/2310.17233</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ping Guo, Xiangpeng Wei, Yue Hu, Baosong Yang, Dayiheng Liu, Fei Huang, Jun Xie</li>
<li>for: 本研究探讨了如何学习跨语言共同表示，以增强机器翻译和其他语言处理任务的性能。</li>
<li>methods: 该研究提出了一种基于EM算法的多语言预训练算法，称为EMMA-X，用于学习跨语言共同表示。EMMA-X使用了大量的多语言非параллель数据，并将跨语言表示学习任务和额外semantic relation预测任务结合在一起。</li>
<li>results: 实验表明，EMMA-X在新引入的XRETEBenchmark上达到了状态对性能。此外，对于建立的表示空间的几何分析表明，EMMA-X在三个需求下表现出了superiority。<details>
<summary>Abstract</summary>
Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic ``universals'' for any two languages. In this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm, to learn (X)Cross-lingual universals with the aid of excessive multilingual non-parallel data. EMMA-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that EMMA-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of EMMA-X over advanced models.
</details>
<details>
<summary>摘要</summary>
<<SYS>> translate("Expressing universal semantics common to all languages is helpful in understanding the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic 'universals' for any two languages. In this paper, we propose EMMA-X: an EM-like Multilingual pre-training Algorithm, to learn (X)Cross-lingual universals with the aid of excessive multilingual non-parallel data. EMMA-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate EMMA-X, we conduct experiments on XRETE, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that EMMA-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of EMMA-X over advanced models.）Here's the translation in Traditional Chinese:<<SYS>>翻译("表达通用 semantics 对所有语言都很有帮助，对复杂和文化特有的句子进行理解。研究主题下面这个 scenario 是通过大量平行 corpora 来学习语言之间的通用表现。然而，由于平行数据的稀缺和罕见性，还是有一个大的挑战是从任何两个语言中学习真正的通用。在这篇文章中，我们提出了 EMMA-X：一种 EM-like 多语言预训 Algorithm，以learn (X) Cross-lingual universals 的 aid  excessive multilingual non-parallel data。EMMA-X 将 Cross-lingual representation learning task 和 extra semantic relation prediction task 统一在 EM 框架内。两个类别的 semantic classifier 和 Cross-lingual sentence encoder 都会 approximates 两个句子之间的 semantic relation，并且彼此监控 until convergence。为了评估 EMMA-X，我们在 XRETE 上进行了实验，XRETE 是一个 newly introduced 的 benchmark，包含 12 种通过句子水平表现来研究的 cross-lingual task。结果显示 EMMA-X 实现了 state-of-the-art 性能。进一步的 geometric analysis 显示 EMMA-X 在三个需求下建立的表示空间优化。）
</details></li>
</ul>
<hr>
<h2 id="Codebook-Features-Sparse-and-Discrete-Interpretability-for-Neural-Networks"><a href="#Codebook-Features-Sparse-and-Discrete-Interpretability-for-Neural-Networks" class="headerlink" title="Codebook Features: Sparse and Discrete Interpretability for Neural Networks"></a>Codebook Features: Sparse and Discrete Interpretability for Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17230">http://arxiv.org/abs/2310.17230</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/taufeeque9/codebook-features">https://github.com/taufeeque9/codebook-features</a></li>
<li>paper_authors: Alex Tamkin, Mohammad Taufeeque, Noah D. Goodman</li>
<li>for: 这种方法可以帮助我们更好地理解神经网络的行为和性质。</li>
<li>methods: 我们使用了vector quantization bottleneck来压缩神经网络的潜在特征，从而生成一个具有稀疏、整数特征的神经网络。</li>
<li>results: 我们发现这种方法可以减少神经网络的性能下降，并且可以帮助我们更好地控制神经网络的行为。我们在几个不同的数据集上训练了codebook Transformers，并发现可以通过活化相应的代码来控制神经网络的输出。<details>
<summary>Abstract</summary>
Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on several different datasets. First, we explore a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. Second, we train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics by activating the appropriate codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at https://github.com/taufeeque9/codebook-features.
</details>
<details>
<summary>摘要</summary>
理解神经网络是困难的一个原因之一是它们的隐藏状态的紧密、连续性。我们研究是否可以训练神经网络，使其隐藏状态变得稀疏、简单化并更容易理解，通过将连续的特征映射到我们称之为“代码库特征”中。代码库特征是通过在每层神经网络中加入vector量化瓶颈，生成一个网络，其隐藏特征是由一小数量的简单vector码选择从大型代码库中。我们发现，可以在这种极端瓶颈下训练神经网络，只具有轻微的性能下降。这种稀疏、简单的瓶颈还提供了一种直观的方式控制神经网络行为：首先，找到表示愿景存在的代码，然后在生成时 aktivate 这些代码，以诱发愿景。我们验证了我们的方法，通过在多个不同的数据集上训练代码库Transformer。首先，我们研究了一个有 infinitely many 隐藏状态的 finite state machine 数据集，在这种情况下，我们的方法可以把状态分配给独特的代码，从而解决超position 问题。我们发现，可以通过活动相应的代码来让神经网络 behave 如果是不同的状态。其次，我们在 two 个自然语言数据集上训练了 Transformer 语言模型，包含多达 410M 参数。我们在这些模型中找到了表示多元、分离的概念的代码（从负情感到月份），并发现可以通过在推理中活动相应的代码来引导模型生成不同的话题。总之，代码库特征看来是神经网络和解释性的有希望的单元。我们的代码库和模型在 <https://github.com/taufeeque9/codebook-features> 上公开。
</details></li>
</ul>
<hr>
<h2 id="X-SNS-Cross-Lingual-Transfer-Prediction-through-Sub-Network-Similarity"><a href="#X-SNS-Cross-Lingual-Transfer-Prediction-through-Sub-Network-Similarity" class="headerlink" title="X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity"></a>X-SNS: Cross-Lingual Transfer Prediction through Sub-Network Similarity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17166">http://arxiv.org/abs/2310.17166</a></li>
<li>repo_url: None</li>
<li>paper_authors: Taejun Yun, Jinhyeon Kim, Deokyeong Kang, Seong Hoon Lim, Jihoon Kim, Taeuk Kim</li>
<li>for: 本研究旨在预测跨语言传递（XLT）的Compatibility，以便选择适合的源语言来提高模型的性能。</li>
<li>methods: 我们提出了基于语言卷积网络相似性的方法，通过分析模型的内部工作机制来预测XLT的Compatibility。我们的方法只需要一小量的原始文本，与大多数前一些方法不同。</li>
<li>results: 我们在多个任务中进行了实验，并证明了我们的方法比基eline更有效，具体来说，它在NDCG@3中平均提高了4.6%。我们还提供了详细的分析，证明了语言卷积网络的相似性对XLT预测的重要性。<details>
<summary>Abstract</summary>
Cross-lingual transfer (XLT) is an emergent ability of multilingual language models that preserves their performance on a task to a significant extent when evaluated in languages that were not included in the fine-tuning process. While English, due to its widespread usage, is typically regarded as the primary language for model adaption in various tasks, recent studies have revealed that the efficacy of XLT can be amplified by selecting the most appropriate source languages based on specific conditions. In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources. In experiments, we demonstrate that our method is more effective than baselines across diverse tasks. Specifically, it shows proficiency in ranking candidates for zero-shot XLT, achieving an improvement of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that confirm the utility of sub-networks for XLT prediction.
</details>
<details>
<summary>摘要</summary>
In this work, we propose the utilization of sub-network similarity between two languages as a proxy for predicting the compatibility of the languages in the context of XLT. Our approach is model-oriented, better reflecting the inner workings of foundation models. In addition, it requires only a moderate amount of raw text from candidate languages, distinguishing it from the majority of previous methods that rely on external resources.In experiments, we demonstrate that our method is more effective than baselines across diverse tasks. Specifically, it shows proficiency in ranking candidates for zero-shot XLT, achieving an improvement of 4.6% on average in terms of NDCG@3. We also provide extensive analyses that confirm the utility of sub-networks for XLT prediction.
</details></li>
</ul>
<hr>
<h2 id="Supercharging-academic-writing-with-generative-AI-framework-techniques-and-caveats"><a href="#Supercharging-academic-writing-with-generative-AI-framework-techniques-and-caveats" class="headerlink" title="Supercharging academic writing with generative AI: framework, techniques, and caveats"></a>Supercharging academic writing with generative AI: framework, techniques, and caveats</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17143">http://arxiv.org/abs/2310.17143</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhicheng Lin</li>
<li>For:  This paper aims to improve the quality and efficiency of academic writing by leveraging generative artificial intelligence (AI) and large language models (LLMs).* Methods: The authors propose a human-AI collaborative framework for writing that delineates the rationale, process, and nature of AI engagement in writing. They also describe effective prompting techniques for incorporating AI into the writing routine and strategies for maintaining rigorous scholarship.* Results: The authors argue that the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.<details>
<summary>Abstract</summary>
Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) as well as strategies for maintaining rigorous scholarship, adhering to varied journal policies, and avoiding overreliance on AI. Ultimately, the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="M2C-Towards-Automatic-Multimodal-Manga-Complement"><a href="#M2C-Towards-Automatic-Multimodal-Manga-Complement" class="headerlink" title="M2C: Towards Automatic Multimodal Manga Complement"></a>M2C: Towards Automatic Multimodal Manga Complement</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17130">http://arxiv.org/abs/2310.17130</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hc-guo/m2c">https://github.com/hc-guo/m2c</a></li>
<li>paper_authors: Hongcheng Guo, Boyang Wang, Jiaqi Bai, Jiaheng Liu, Jian Yang, Zhoujun Li</li>
<li>for: 提高漫画理解，结合视觉和文本特征</li>
<li>methods: 使用大语言模型 mines 漫画事件知识，并使用细化视觉提示支持漫画补充</li>
<li>results: FVP-M$^{2}$ 方法实现了 Multimodal Manga Complement 任务的有效性<details>
<summary>Abstract</summary>
Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for Multimodal Mange Complement.
</details>
<details>
<summary>摘要</summary>
多模态漫画分析强调使用视觉和文本特征，吸引了自然语言处理和计算机视觉领域的广泛关注。目前大多数漫画是手动绘制的，容易出现缺失页码、文本污染和衰老等问题，导致漫画文本内容丢失和人类理解受阻。即使是现有的多模态漫画补充（M2C）任务也没有被 investigate，该任务目的是通过提供视觉和语言理解共享semantic空间来解决上述问题。为此，我们首先提出了多模态漫画补充任务，并设置了一个新的M2Cbenchmark dataset，覆盖两种语言。然后，我们设计了漫画论证方法（MCoT），用于在大型语言模型中挖掘漫画中的事件知识。最后，我们提出了一个有效的基线方法FVP-M$^{2}$，使用细化的视觉提示来支持多模态漫画补充。广泛的实验结果表明FVP-M$^{2}$方法的效iveness для多模态漫画补充。
</details></li>
</ul>
<hr>
<h2 id="Test-time-Augmentation-for-Factual-Probing"><a href="#Test-time-Augmentation-for-Factual-Probing" class="headerlink" title="Test-time Augmentation for Factual Probing"></a>Test-time Augmentation for Factual Probing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17121">http://arxiv.org/abs/2310.17121</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gokamoda/TTA4FactualProbing">https://github.com/gokamoda/TTA4FactualProbing</a></li>
<li>paper_authors: Go Kamoda, Benjamin Heinzerling, Keisuke Sakaguchi, Kentaro Inui</li>
<li>for: 验证语言模型是否具备certain world knowledge facts的能力。</li>
<li>methods: 使用提示来测试语言模型的知识。</li>
<li>results: 使用test-time augmentation（TTA）可以减少提示变化导致的模型敏感性，并提高模型评估准确性。但是，一些模型可能会受到TTA的影响，导致质量下降。<details>
<summary>Abstract</summary>
Factual probing is a method that uses prompts to test if a language model "knows" certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.
</details>
<details>
<summary>摘要</summary>
factual probing 是一种使用提示测试语言模型是否具备certain world knowledge fact的方法。问题在于小Changes to the prompt can lead to large changes in model output。 previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning。However, such approaches are relation-specific and do not generalize to unseen relation types。Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time。experiments show improved model calibration，i.e., with TTA, model confidence better reflects prediction accuracy。improvements in prediction accuracy are observed for some models，but for other models，TTA leads to degradation。error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA。
</details></li>
</ul>
<hr>
<h2 id="FLEEK-Factual-Error-Detection-and-Correction-with-Evidence-Retrieved-from-External-Knowledge"><a href="#FLEEK-Factual-Error-Detection-and-Correction-with-Evidence-Retrieved-from-External-Knowledge" class="headerlink" title="FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge"></a>FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2310.17119">http://arxiv.org/abs/2310.17119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, Yunyao Li</li>
<li>for: 检测文本信息中的事实错误，以便做出有知识基础的决策。</li>
<li>methods: 使用 prototype 工具 FLEEK，自动提取文本中的事实CLAIM，从外部知识源收集证据，评估每个CLAIM的事实性，并提供修复错误的建议。</li>
<li>results: 初步实验显示 FLEEK 可以准确地检测事实错误（77-85% F1）。<details>
<summary>Abstract</summary>
Detecting factual errors in textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present FLEEK, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85\% F1) shows the potential of FLEEK. A video demo of FLEEK can be found at https://youtu.be/NapJFUlkPdQ.
</details>
<details>
<summary>摘要</summary>
检测文本信息中的事实错误，无论是由大型语言模型（LLM）生成或由人类编辑，都是决策时的关键。 LLM 的声称无法归因于外部知识并且倾向于幻见，使得不可靠地依赖其回答。人类也容易在写作时犯下事实错误。由于手动检测和修正事实错误是劳动密集的，因此开发自动化方法可以减少人类努力。我们提出了 FLEEK，一种 прототип工具，可以自动从文本中提取事实声称，从外部知识源收集证据，评估每个声称的事实性，并使用收集的证据进行标注错误。我们的初始实验结果（77-85\% F1）表明 FLEEK 有潜力。有关 FLEEK 的视频demo可以在 https://youtu.be/NapJFUlkPdQ 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/10/26/cs.CL_2023_10_26/" data-id="clogxf3ma00d35xra2jypa63m" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/83/">83</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">121</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">115</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">55</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">111</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">61</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
