
<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  
  <title>Fun Paper</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fun Paper">
<meta property="og:url" content="https://nullscc.github.io/page/4/index.html">
<meta property="og:site_name" content="Fun Paper">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="nullscc">
<meta name="twitter:card" content="summary">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
  

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <nav id="upper-nav" class="inner">
      <a id="main-nav-toggle" class="nav-icon"></a>
      <div class="sub-nav">
        
        
          <a id="nav-github" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/nullscc"></a>
        
      </div>
    </nav>
    <div id="header-title">
      
        <h1 id="blog-title-wrap">
          <a href="/" id="blog-title">Fun Paper</a>
        </h1>
      
    </div>
    <div id="contenedor">
      <ul class="cube">
        <li class="cara">Paper</li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
        <li class="cara"></li>
      </ul>
    </div>
    <nav id="main-nav">
      
        <a class="main-nav-link" href="/">Home</a>
      
        <a class="main-nav-link" href="/archives">Archives</a>
      
    </nav>
  </div>
</header>

    <div class="outer">
      <section id="main">
  
    <article id="post-cs.AI_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/cs.AI_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T12:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/cs.AI_2023_11_17/">cs.AI - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Flexible-Model-Interpretability-through-Natural-Language-Model-Editing"><a href="#Flexible-Model-Interpretability-through-Natural-Language-Model-Editing" class="headerlink" title="Flexible Model Interpretability through Natural Language Model Editing"></a>Flexible Model Interpretability through Natural Language Model Editing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10905">http://arxiv.org/abs/2311.10905</a></li>
<li>repo_url: None</li>
<li>paper_authors: Karel D’Oosterlinck, Thomas Demeester, Chris Develder, Christopher Potts</li>
<li>for: 论文目的是提高模型解释性和修改模型行为的可能性。</li>
<li>methods: 论文使用了修改模型行为以探索人类概念的方法。</li>
<li>results: 研究发现，可以通过修改模型行为来提高模型内部表示的解释性，并且可以通过这种方法找到相关的表示和修改它们。<details>
<summary>Abstract</summary>
Model interpretability and model editing are crucial goals in the age of large language models. Interestingly, there exists a link between these two goals: if a method is able to systematically edit model behavior with regard to a human concept of interest, this editor method can help make internal representations more interpretable by pointing towards relevant representations and systematically manipulating them.
</details>
<details>
<summary>摘要</summary>
MODEL理解和模型编辑是当今大语言模型时代的两个关键目标。有趣的是，这两个目标之间存在一种联系：如果一种方法可以系统地编辑模型的行为，以便更好地理解人类概念的内部表示。这种编辑方法可以帮助暴露内部表示，并系统地操作它们，从而使模型的内部表示更加可读。
</details></li>
</ul>
<hr>
<h2 id="On-Functional-Activations-in-Deep-Neural-Networks"><a href="#On-Functional-Activations-in-Deep-Neural-Networks" class="headerlink" title="On Functional Activations in Deep Neural Networks"></a>On Functional Activations in Deep Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10898">http://arxiv.org/abs/2311.10898</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew S. Nencka, L. Tugan Muftuler, Peter LaViolette, Kevin M. Koch</li>
<li>for:  probing the workings of deep neural networks, specifically the Facebook Galactica-125M language model</li>
<li>methods:  functional neuroimaging techniques were applied to the model, using block-designed task-based prompt sequences to probe its functional structure</li>
<li>results:  distinct, overlapping networks were identified for each task, with the most overlap between medical imaging and pathology networks, and the identified functional networks were found to be repeatable across repeated performance of related tasks and accurate in identifying presented tasks<details>
<summary>Abstract</summary>
Background: Deep neural networks have proven to be powerful computational tools for modeling, prediction, and generation. However, the workings of these models have generally been opaque. Recent work has shown that the performance of some models are modulated by overlapping functional networks of connections within the models. Here the techniques of functional neuroimaging are applied to an exemplary large language model to probe its functional structure. Methods: A series of block-designed task-based prompt sequences were generated to probe the Facebook Galactica-125M model. Tasks included prompts relating to political science, medical imaging, paleontology, archeology, pathology, and random strings presented in an off/on/off pattern with prompts about other random topics. For the generation of each output token, all layer output values were saved to create an effective time series. General linear models were fit to the data to identify layer output values which were active with the tasks. Results: Distinct, overlapping networks were identified with each task. Most overlap was observed between medical imaging and pathology networks. These networks were repeatable across repeated performance of related tasks, and correspondence of identified functional networks and activation in tasks not used to define the functional networks was shown to accurately identify the presented task. Conclusion: The techniques of functional neuroimaging can be applied to deep neural networks as a means to probe their workings. Identified functional networks hold the potential for use in model alignment, modulation of model output, and identifying weights to target in fine-tuning.
</details>
<details>
<summary>摘要</summary>
Background: 深度神经网络已经证明是有力的计算工具，用于模型、预测和生成。然而，这些模型的工作方式通常是不透明的。最近的研究表明，一些模型的性能是由模型内部的重叠功能网络连接所模ulated。在这种情况下，我们使用函数神经成像技术来探索Facebook Galactica-125M模型的函数结构。Methods: 我们生成了一系列块设计的任务基本提示序列，用于探索Facebook Galactica-125M模型。这些任务包括政治科学、医学成像、古生物学、考古学、病理学和随机串列，并以On/Off/On的模式提交关于其他随机主题的提示。为每个输出字符生成，所有层输出值都被保存，以创建一个有效的时间序列。我们使用通用线性模型适应 данных，以确定每个任务的相关层输出值。Results: 我们发现了每个任务都有独特的、重叠的函数网络。最多的重叠是在医学成像和病理学任务之间。这些网络在相关任务的重复执行中是重复的，并且可以用来确定提交的任务。我们还发现，在不使用定义函数网络的任务中，活跃的层输出值与确定的任务相关。Conclusion: 我们可以将函数神经成像技术应用于深度神经网络，以探索它们的工作方式。被发现的函数网络具有识别和调整模型输出的潜在优势。
</details></li>
</ul>
<hr>
<h2 id="The-Hidden-Linear-Structure-in-Score-Based-Models-and-its-Application"><a href="#The-Hidden-Linear-Structure-in-Score-Based-Models-and-its-Application" class="headerlink" title="The Hidden Linear Structure in Score-Based Models and its Application"></a>The Hidden Linear Structure in Score-Based Models and its Application</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10892">http://arxiv.org/abs/2311.10892</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/neurons">https://github.com/Aryia-Behroziuan/neurons</a></li>
<li>paper_authors: Binxu Wang, John J. Vastola</li>
<li>for: 本研究旨在发现Score-based模型中是否存在 universale结构，以便更好地设计模型和处理数据。</li>
<li>methods: 研究人员使用了 normative analysis of the score function 来找到这种结构，并通过 empirical validation of pre-trained images diffusion model 和理论分析来证明其存在。</li>
<li>results: 研究人员发现，在高噪音级别下，Well-trained diffusion models 的学习得分可以近似于 Gaussian 的直线得分。这种发现可以帮助预测初始噪音轨迹，并在预测图像扩散过程中提高样本质量。<details>
<summary>Abstract</summary>
Score-based models have achieved remarkable results in the generative modeling of many domains. By learning the gradient of smoothed data distribution, they can iteratively generate samples from complex distribution e.g. natural images.   However, is there any universal structure in the gradient field that will eventually be learned by any neural network? Here, we aim to find such structures through a normative analysis of the score function.   First, we derived the closed-form solution to the scored-based model with a Gaussian score. We claimed that for well-trained diffusion models, the learned score at a high noise scale is well approximated by the linear score of Gaussian. We demonstrated this through empirical validation of pre-trained images diffusion model and theoretical analysis of the score function. This finding enabled us to precisely predict the initial diffusion trajectory using the analytical solution and to accelerate image sampling by 15-30\% by skipping the initial phase without sacrificing image quality. Our finding of the linear structure in the score-based model has implications for better model design and data pre-processing.
</details>
<details>
<summary>摘要</summary>
score-based 模型在多个领域的生成模型中获得了惊人的成绩。通过学习抽象数据分布的梯度场，它们可以逐步生成复杂分布，例如自然图像。然而，任何神经网络都会学习到 universal 结构在梯度场中吗？在这里，我们通过评估函数的正规分析来寻找这些结构。首先，我们得到了涉及到 scored-based 模型的关闭式解。我们证明，对于具有高噪声级别的扩散模型，学习的得分在高噪声级别上是可以近似为 Gaussian 梯度的线性得分。我们通过实验验证预训练的图像扩散模型和理论分析得分函数来证明这一点。这一发现使我们能够准确预测扩散过程的初始态使用关闭式解，并且可以通过跳过初始阶段来加速图像抽取，从而提高图像质量。我们发现了梯度场中的线性结构，这种结构在 score-based 模型中具有更好的模型设计和数据预处理的意义。
</details></li>
</ul>
<hr>
<h2 id="Verified-Compositional-Neuro-Symbolic-Control-for-Stochastic-Systems-with-Temporal-Logic-Tasks"><a href="#Verified-Compositional-Neuro-Symbolic-Control-for-Stochastic-Systems-with-Temporal-Logic-Tasks" class="headerlink" title="Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks"></a>Verified Compositional Neuro-Symbolic Control for Stochastic Systems with Temporal Logic Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10863">http://arxiv.org/abs/2311.10863</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jun Wang, Kaiyuan Tan, Zihe Sun, Yiannis Kantaros<br>for:* The paper aims to learn neural network (NN) controllers for autonomous agents with unknown and stochastic dynamics, tasked with complex missions captured by Linear Temporal Logic (LTL).methods:* The paper proposes a new approach that integrates automata theory and data-driven reachability analysis tools for NN-controlled stochastic systems.* The proposed method uses a neuro-symbolic controller that allows the agent to generate safe behaviors for unseen complex temporal logic tasks in a zero-shot fashion by leveraging its base skills.results:* The paper shows correctness of the proposed method and provides conditions under which it is complete.* The proposed method is demonstrated through extensive numerical simulations and hardware experiments on robot navigation tasks.<details>
<summary>Abstract</summary>
Several methods have been proposed recently to learn neural network (NN) controllers for autonomous agents, with unknown and stochastic dynamics, tasked with complex missions captured by Linear Temporal Logic (LTL). Due to the sample-inefficiency of the majority of these works, compositional learning methods have been proposed decomposing the LTL specification into smaller sub-tasks. Then, separate controllers are learned and composed to satisfy the original task. A key challenge within these approaches is that they often lack safety guarantees or the provided guarantees are impractical. This paper aims to address this challenge. Particularly, we consider autonomous systems with unknown and stochastic dynamics and LTL-encoded tasks. We assume that the system is equipped with a finite set of base skills modeled by trained NN feedback controllers. Our goal is to check if there exists a temporal composition of the trained NN controllers - and if so, to compute it - that will yield a composite system behavior that satisfies the assigned LTL task with probability one. We propose a new approach that relies on a novel integration of automata theory and data-driven reachability analysis tools for NN-controlled stochastic systems. The resulting neuro-symbolic controller allows the agent to generate safe behaviors for unseen complex temporal logic tasks in a zero-shot fashion by leveraging its base skills. We show correctness of the proposed method and we provide conditions under which it is complete. To the best of our knowledge, this is the first work that designs verified temporal compositions of NN controllers for unknown and stochastic systems. Finally, we provide extensive numerical simulations and hardware experiments on robot navigation tasks to demonstrate the proposed method.
</details>
<details>
<summary>摘要</summary>
Recently, several methods have been proposed to learn neural network (NN) controllers for autonomous agents with unknown and stochastic dynamics, tasked with complex missions captured by Linear Temporal Logic (LTL). However, due to the sample-inefficiency of most of these works, compositional learning methods have been proposed to decompose the LTL specification into smaller sub-tasks, and then learn separate controllers to satisfy the original task. A major challenge within these approaches is the lack of safety guarantees or impractical guarantees provided. This paper aims to address this challenge. Specifically, we consider autonomous systems with unknown and stochastic dynamics and LTL-encoded tasks. We assume that the system is equipped with a finite set of base skills modeled by trained NN feedback controllers. Our goal is to check if there exists a temporal composition of the trained NN controllers, and if so, to compute it, that will yield a composite system behavior that satisfies the assigned LTL task with probability one. We propose a new approach that combines automata theory and data-driven reachability analysis tools for NN-controlled stochastic systems. The resulting neuro-symbolic controller allows the agent to generate safe behaviors for unseen complex temporal logic tasks in a zero-shot fashion by leveraging its base skills. We prove the correctness of the proposed method and provide conditions under which it is complete. To the best of our knowledge, this is the first work that designs verified temporal compositions of NN controllers for unknown and stochastic systems. Finally, we provide extensive numerical simulations and hardware experiments on robot navigation tasks to demonstrate the proposed method.
</details></li>
</ul>
<hr>
<h2 id="Formal-concept-analysis-for-evaluating-intrinsic-dimension-of-a-natural-language"><a href="#Formal-concept-analysis-for-evaluating-intrinsic-dimension-of-a-natural-language" class="headerlink" title="Formal concept analysis for evaluating intrinsic dimension of a natural language"></a>Formal concept analysis for evaluating intrinsic dimension of a natural language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10862">http://arxiv.org/abs/2311.10862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei O. Kuznetsov, Vasilii A. Gromov, Nikita S. Borodin, Andrei M. Divavin</li>
<li>for: 这些研究是为了确定孟加拉语和俄语的内在维度的计算实验结果。</li>
<li>methods: 这些研究使用了正式概念分析算法来解决这个问题，同时分别考虑了这两种语言的单词和大词组。</li>
<li>results: 研究发现，这两种语言的内在维度远低于 популяр的人工神经网络模型在自然语言处理中使用的维度。<details>
<summary>Abstract</summary>
Some results of a computational experiment for determining the intrinsic dimension of linguistic varieties for the Bengali and Russian languages are presented. At the same time, both sets of words and sets of bigrams in these languages were considered separately. The method used to solve this problem was based on formal concept analysis algorithms. It was found that the intrinsic dimensions of these languages are significantly less than the dimensions used in popular neural network models in natural language processing.
</details>
<details>
<summary>摘要</summary>
<<SYS>>将文本翻译成简化中文。<</SYS>>这些计算实验结果表明，孟加拉语和俄语的语言变体的内在维度非常低。在这些语言中，单词和大字组都被考虑了。使用了正式概念分析算法来解决这个问题。结果显示，这些语言的内在维度明显比流行的人工神经网络模型在自然语言处理中使用的维度更低。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Consistency-Quality-and-Challenges-in-Manual-and-Automated-Coding-of-Free-text-Diagnoses-from-Hospital-Outpatient-Letters"><a href="#Exploring-the-Consistency-Quality-and-Challenges-in-Manual-and-Automated-Coding-of-Free-text-Diagnoses-from-Hospital-Outpatient-Letters" class="headerlink" title="Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters"></a>Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10856">http://arxiv.org/abs/2311.10856</a></li>
<li>repo_url: None</li>
<li>paper_authors: Warren Del-Pinto, George Demetriou, Meghna Jani, Rikesh Patel, Leanne Gray, Alex Bulcock, Niels Peek, Andrew S. Kanter, William G Dixon, Goran Nenadic</li>
<li>For: This paper aims to evaluate the quality and consistency of manual and automated clinical coding of diagnoses from hospital outpatient letters.* Methods: The authors used 100 randomly selected letters for coding, and two human clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding was also performed using IMO’s Concept Tagger. A gold standard was constructed by a panel of clinicians from a subset of the annotated diagnoses.* Results: The results indicate that humans slightly out-performed automated coding, while both performed notably better when there was only a single diagnosis contained in the free-text description. Automated coding was considered acceptable by the panel of clinicians in approximately 90% of cases.Here’s the Chinese translation of the three key information points:* For: 这篇论文目标是评估人工和自动化医疗诊断代码的质量和一致性。* Methods: 作者使用100个随机选择的医疗信息letter进行编码，并由两名人类医生对诊断列表进行编码到SNOMED CT。自动编码也使用IMO的概念标签。一个金标准由一组医生从一 subset of annotated diagnoses中构建。* Results: 结果显示人工编码slightly exceeded自动编码，而两者都在只有一个诊断存在于自由文本描述时表现更好。自动编码被评估为可以接受的程度，大约90%的 случа子。<details>
<summary>Abstract</summary>
Coding of unstructured clinical free-text to produce interoperable structured data is essential to improve direct care, support clinical communication and to enable clinical research.However, manual clinical coding is difficult and time consuming, which motivates the development and use of natural language processing for automated coding. This work evaluates the quality and consistency of both manual and automated clinical coding of diagnoses from hospital outpatient letters. Using 100 randomly selected letters, two human clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding was also performed using IMO's Concept Tagger. A gold standard was constructed by a panel of clinicians from a subset of the annotated diagnoses. This was used to evaluate the quality and consistency of both manual and automated coding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2) a qualitative metric agreed upon by the panel of clinicians. Correlation between the two metrics was also evaluated. Comparing human and computer-generated codes to the gold standard, the results indicate that humans slightly out-performed automated coding, while both performed notably better when there was only a single diagnosis contained in the free-text description. Automated coding was considered acceptable by the panel of clinicians in approximately 90% of cases.
</details>
<details>
<summary>摘要</summary>
临床自由文本编码到可互操作结构化数据是直接护理、促进临床通信和促进临床研究的关键。然而，手动临床编码困难和时间consuming，这些动机了开发和使用自然语言处理（NLP）自动编码。这项工作评估手动和自动临床编码诊断列表的质量和一致性。使用100份随机选择的医疗信息，两名人类临床医生 manually编码诊断列表到SNOMED CT。自动编码也使用IMO的概念标签器。一个由临床医生组成的委员会从一个子集中的注释诊断构建了黄金标准。这些标准用于评估手动和自动编码质量和一致性。首先，使用 distance-based 度量，将 SNOMED CT 视为图，并使用一种由委员会所议定的质量量表。其次，对手动和自动编码与黄金标准进行比较。结果显示，人类轻微地超越了自动编码，而两者都在单个诊断存在于自由文本描述时表现出色。自动编码被委员会认为是可接受的约90%的情况下。
</details></li>
</ul>
<hr>
<h2 id="Artificial-Intelligence-in-Fetal-Resting-State-Functional-MRI-Brain-Segmentation-A-Comparative-Analysis-of-3D-UNet-VNet-and-HighRes-Net-Models"><a href="#Artificial-Intelligence-in-Fetal-Resting-State-Functional-MRI-Brain-Segmentation-A-Comparative-Analysis-of-3D-UNet-VNet-and-HighRes-Net-Models" class="headerlink" title="Artificial Intelligence in Fetal Resting-State Functional MRI Brain Segmentation: A Comparative Analysis of 3D UNet, VNet, and HighRes-Net Models"></a>Artificial Intelligence in Fetal Resting-State Functional MRI Brain Segmentation: A Comparative Analysis of 3D UNet, VNet, and HighRes-Net Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10844">http://arxiv.org/abs/2311.10844</a></li>
<li>repo_url: None</li>
<li>paper_authors: Farzan Vahedifard, Xuchu Liu, Mehmet Kocak, H. Asher Ai, Mark Supanich, Christopher Sica., Kranthi K Marathu, Seth Adler, Maysam Orouskhani, Sharon Byrd</li>
<li>for: 这个研究的目的是提高婴儿脑功能磁共振成像（fMRI）中脑部分分 Segmentation 的精度。</li>
<li>methods: 这个研究使用了人工智能（AI）技术来自动进行脑部分分 Segmentation，并使用了一个开源的婴儿 fMRI 数据集来训练 AI 模型。</li>
<li>results: 研究发现，使用 VNet 模型可以提高脑部分分 Segmentation 的精度，但是需要进一步的调整和研究以全面 explore 每种模型的潜力和局限性。<details>
<summary>Abstract</summary>
Introduction: Fetal resting-state functional magnetic resonance imaging (rs-fMRI) is a rapidly evolving field that provides valuable insight into brain development before birth. Accurate segmentation of the fetal brain from the surrounding tissue in nonstationary 3D brain volumes poses a significant challenge in this domain. Current available tools have 0.15 accuracy. Aim: This study introduced a novel application of artificial intelligence (AI) for automated brain segmentation in fetal brain fMRI, magnetic resonance imaging (fMRI). Open datasets were employed to train AI models, assess their performance, and analyze their capabilities and limitations in addressing the specific challenges associated with fetal brain fMRI segmentation. Method: We utilized an open-source fetal functional MRI (fMRI) dataset consisting of 160 cases (reference: fetal-fMRI - OpenNeuro). An AI model for fMRI segmentation was developed using a 5-fold cross-validation methodology. Three AI models were employed: 3D UNet, VNet, and HighResNet. Optuna, an automated hyperparameter-tuning tool, was used to optimize these models. Results and Discussion: The Dice scores of the three AI models (VNet, UNet, and HighRes-net) were compared, including a comparison between manually tuned and automatically tuned models using Optuna. Our findings shed light on the performance of different AI models for fetal resting-state fMRI brain segmentation. Although the VNet model showed promise in this application, further investigation is required to fully explore the potential and limitations of each model, including the HighRes-net model. This study serves as a foundation for further extensive research into the applications of AI in fetal brain fMRI segmentation.
</details>
<details>
<summary>摘要</summary>
引言：胎儿休息状态功能磁共振成像（rs-fMRI）是一个快速发展的领域，它为胎儿脑发展前的脑部提供了价值的信息。但是，准确地从周围组织中分离胎儿脑部在非站ARY 3D脑部图像中是一项 significante challenges。目前可用的工具的准确率只有0.15。目标：本研究推出了一种使用人工智能（AI）自动 segmentation的胎儿脑部fMRI magnet resonance imaging（fMRI）应用。我们使用了开源的胎儿功能MRI数据集（fetal-fMRI - OpenNeuro）来训练AI模型，评估其性能，并分析它们在特定挑战中的能力和局限性。方法：我们使用了一个5-fold cross-validation方法来开发一个AI模型。我们使用了3D UNet、VNet和HighResNet三种AI模型。我们使用了Optuna，一个自动调参工具，来优化这些模型。结果和讨论：我们比较了三种AI模型（VNet、UNet和HighRes-net）的Dice分数，包括手动调参和使用Optuna自动调参的模型的比较。我们的发现 shed light on the performance of different AI models for fetal resting-state fMRI brain segmentation。虽然VNet模型表现良好，但需要进一步的研究以全面探讨每种模型的潜在和局限性，包括HighRes-net模型。这项研究为胎儿脑fMRI segmentation中AI应用的进一步探索提供了基础。
</details></li>
</ul>
<hr>
<h2 id="Integration-and-Implementation-Strategies-for-AI-Algorithm-Deployment-with-Smart-Routing-Rules-and-Workflow-Management"><a href="#Integration-and-Implementation-Strategies-for-AI-Algorithm-Deployment-with-Smart-Routing-Rules-and-Workflow-Management" class="headerlink" title="Integration and Implementation Strategies for AI Algorithm Deployment with Smart Routing Rules and Workflow Management"></a>Integration and Implementation Strategies for AI Algorithm Deployment with Smart Routing Rules and Workflow Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10840">http://arxiv.org/abs/2311.10840</a></li>
<li>repo_url: None</li>
<li>paper_authors: Barbaros Selnur Erdal, Vikash Gupta, Mutlu Demirer, Kim H. Fair, Richard D. White, Jeff Blair, Barbara Deichert, Laurie Lafleur, Ming Melvin Qin, David Bericat, Brad Genereaux</li>
<li>for: 这篇论文探讨了医疗行业广泛采用人工智能解决方案的挑战，特别是医疗图像应用，以及如何通过可交互性和企业级扩展性解决这些挑战。</li>
<li>methods: 论文强调了健康域的复杂性，大量和安全的医疗图像数据管理，以及人工智能开发的标准化框架缺失，这些因素都是阻碍医疗AI应用广泛采用的主要障碍。</li>
<li>results: 论文认为，通过可交互性和企业级扩展性来解决这些挑战，可以提高医疗AI应用的普及率。例如，DICOM、HL7和IHE等标准被用于健康域的共同图像工作流程。Laurel Bridge在这一领域中发挥了转变性的作用。而MONAI项目，成立于2019年，也被认为是一种重要的 iniciativa，用于重定义医疗AI应用的开发。MONAI部署App SDK是该项目的关键工具，可以简化AI应用的包装和部署过程，使得AI应用的扩展和标准化部署模式变得可能。<details>
<summary>Abstract</summary>
This paper reviews the challenges hindering the widespread adoption of artificial intelligence (AI) solutions in the healthcare industry, focusing on computer vision applications for medical imaging, and how interoperability and enterprise-grade scalability can be used to address these challenges. The complex nature of healthcare workflows, intricacies in managing large and secure medical imaging data, and the absence of standardized frameworks for AI development pose significant barriers and require a new paradigm to address them. The role of interoperability is examined in this paper as a crucial factor in connecting disparate applications within healthcare workflows. Standards such as DICOM, Health Level 7 HL7, and Integrating the Healthcare Enterprise (IHE) are highlighted as foundational for common imaging workflows. A specific focus is placed on the role of DICOM gateways, with Laurel Bridge leading transformational efforts in this area. To drive enterprise scalability, new tools are needed. Project MONAI, established in 2019, is introduced as an initiative aiming to redefine the development of medical AI applications. The MONAI Deploy App SDK, a component of Project MONAI, is identified as a key tool in simplifying the packaging and deployment process, enabling repeatable, scalable, and standardized deployment patterns for AI applications. The abstract underscores the potential impact of successful AI adoption in healthcare, offering physicians both life-saving and time-saving insights and driving efficiencies in radiology department workflows. The collaborative efforts between academia and industry, exemplified by collaborations with organizations like NVIDIA and Laurel Bridge, are emphasized as essential for advancing the adoption of healthcare AI solutions.
</details>
<details>
<summary>摘要</summary>
这篇论文检讨了医疗领域人工智能解决方案的广泛应用面临的挑战，特别是医疗图像应用，以及如何通过可交互性和企业级扩展性来解决这些挑战。医疗工作流程的复杂性、管理大量安全医疗图像的细节和开发人工智能应用程序的缺乏标准化框架是主要的阻碍因素，需要一种新的思维方式来解决这些问题。本文强调了可交互性在医疗工作流程中的重要性，并提到了如DICOM、HL7和IHE等标准的作用。特别是在医疗图像工作流程中，DICOM网关的角色很重要，如Laurel Bridge等公司的努力。为了驱动企业级扩展，新的工具是需要的。2019年成立的项目MONAI是一个旨在重定义医疗人工智能应用程序的开发的 iniciativa。MONAI Deploy App SDK是项目MONAI的一个关键工具，可以简化包装和部署过程，实现可重复、可扩展和标准化的部署模式。摘要 highlights the potential impact of successful AI adoption in healthcare, offering physicians both life-saving and time-saving insights and driving efficiencies in radiology department workflows。学术和产业之间的合作，例如与NVIDIA和Laurel Bridge等组织的合作，被视为健康医疗人工智能解决方案的发展的重要因素。
</details></li>
</ul>
<hr>
<h2 id="Exploring-Machine-Learning-Models-for-Federated-Learning-A-Review-of-Approaches-Performance-and-Limitations"><a href="#Exploring-Machine-Learning-Models-for-Federated-Learning-A-Review-of-Approaches-Performance-and-Limitations" class="headerlink" title="Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations"></a>Exploring Machine Learning Models for Federated Learning: A Review of Approaches, Performance, and Limitations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10832">http://arxiv.org/abs/2311.10832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Elaheh Jafarigol, Theodore Trafalis, Talayeh Razzaghi, Mona Zamankhani</li>
<li>for: 本文是一篇系统性的文献评论，旨在为研究者和实践者提供 Federated Learning 的全面性评论，尤其是在机器学习领域。</li>
<li>methods: 本文综述了过去几年的学术文献，包括支持学习&#x2F;无支持学习机器学习算法、集成方法、meta-heuristic方法、区块链技术和强化学习，以及 Federated Learning 应用场景。</li>
<li>results: 本文对 Federated Learning 的不同组件进行了评论，以及其在不同应用场景中的应用。此外，文章还提供了一些未解决的问题和未来研究方向。<details>
<summary>Abstract</summary>
In the growing world of artificial intelligence, federated learning is a distributed learning framework enhanced to preserve the privacy of individuals' data. Federated learning lays the groundwork for collaborative research in areas where the data is sensitive. Federated learning has several implications for real-world problems. In times of crisis, when real-time decision-making is critical, federated learning allows multiple entities to work collectively without sharing sensitive data. This distributed approach enables us to leverage information from multiple sources and gain more diverse insights. This paper is a systematic review of the literature on privacy-preserving machine learning in the last few years based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Specifically, we have presented an extensive review of supervised/unsupervised machine learning algorithms, ensemble methods, meta-heuristic approaches, blockchain technology, and reinforcement learning used in the framework of federated learning, in addition to an overview of federated learning applications. This paper reviews the literature on the components of federated learning and its applications in the last few years. The main purpose of this work is to provide researchers and practitioners with a comprehensive overview of federated learning from the machine learning point of view. A discussion of some open problems and future research directions in federated learning is also provided.
</details>
<details>
<summary>摘要</summary>
在快速发展的人工智能世界中，联邦学习是一种分布式学习框架，旨在保护个人数据隐私。联邦学习为敏捷决策和敏捷研究提供了一个保密的平台，当危机发生时，多个实体可以共同工作，无需分享敏感数据。这种分布式方法允许我们利用多个来源的信息，获得更多的多样化的洞察。本文是根据《系统atic Review和Meta-Analysis（PRISMA）》指南进行的一项系统性的文献综述，具体来说，我们对于联邦学习框架中的支持学习/无支持学习机器学习算法、集成方法、meta-heuristic方法、区块链技术和强化学习的应用进行了广泛的回顾。此外，我们还对联邦学习应用的各种领域进行了概述。本文的主要目的是为研究者和实践者提供联邦学习从机器学习角度的全面的视图。文章还提供了一些开放问题和未来研究方向的讨论。
</details></li>
</ul>
<hr>
<h2 id="A-Language-Agent-for-Autonomous-Driving"><a href="#A-Language-Agent-for-Autonomous-Driving" class="headerlink" title="A Language Agent for Autonomous Driving"></a>A Language Agent for Autonomous Driving</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10813">http://arxiv.org/abs/2311.10813</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/usc-gvl/agent-driver">https://github.com/usc-gvl/agent-driver</a></li>
<li>paper_authors: Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, Yue Wang</li>
<li>For: This paper aims to integrate human-like intelligence into autonomous driving systems by leveraging Large Language Models (LLMs) as a cognitive agent.* Methods: The proposed approach, called Agent-Driver, includes a versatile tool library, a cognitive memory of common sense and experiential knowledge, and a reasoning engine for chain-of-thought reasoning, task planning, motion planning, and self-reflection.* Results: The approach significantly outperforms state-of-the-art driving methods on the large-scale nuScenes benchmark, with superior interpretability and few-shot learning ability.Here is the text in Simplified Chinese:* For: 这篇论文目标是将人类智能集成到自动驾驶系统中，通过使用大型自然语言模型（LLM）作为认知代理。* Methods: 提议的方法，称为Agent-Driver，包括一个多功能工具库、一个认知记忆，以及一个链式思维、任务规划、运动规划和自我反思的理解引擎。* Results: 该方法在大规模的 nuScenes 数据集上与当前最佳驾驶方法进行比较，显著超过了它们，并且具有更高的可读性和少量学习能力。<details>
<summary>Abstract</summary>
Human-level driving is an ultimate goal of autonomous driving. Conventional approaches formulate autonomous driving as a perception-prediction-planning framework, yet their systems do not capitalize on the inherent reasoning ability and experiential knowledge of humans. In this paper, we propose a fundamental paradigm shift from current pipelines, exploiting Large Language Models (LLMs) as a cognitive agent to integrate human-like intelligence into autonomous driving systems. Our approach, termed Agent-Driver, transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls, a cognitive memory of common sense and experiential knowledge for decision-making, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive common sense and robust reasoning capabilities, thus enabling a more nuanced, human-like approach to autonomous driving. We evaluate our approach on the large-scale nuScenes benchmark, and extensive experiments substantiate that our Agent-Driver significantly outperforms the state-of-the-art driving methods by a large margin. Our approach also demonstrates superior interpretability and few-shot learning ability to these methods. Project page: \href{https://github.com/USC-GVL/Agent-Driver/blob/main/index.html}{here}.
</details>
<details>
<summary>摘要</summary>
人类水平驾驶是自动驾驶的最终目标。传统方法将自动驾驶分为感知、预测和规划三个步骤，但是这些系统不能充分利用人类的内在智能和经验知识。在这篇论文中，我们提出了一种基本的思想转变，利用大型自然语言模型（LLM）作为认知代理人，将人类化智能integrated into自动驾驶系统。我们的方法，称为Agent-Driver，把传统的自动驾驶管道转变为一个可访问函数调用的多功能工具库、一个包含常识和经验知识的认知储存、以及一个可以进行链式思维、任务规划、运动规划和自我反思的解释引擎。通过LLM的支持，我们的Agent-Driver具有直观的常识和强大的解释能力，因此可以实现更加人类化、智能化的自动驾驶方法。我们在nuScenes大规模测试 benchmark上进行了广泛的实验，并证明了我们的Agent-Driver在与当前状态的驾驶方法相比有很大的提升。我们的方法还表现出了更好的可解释性和少量学习能力。项目页面：<https://github.com/USC-GVL/Agent-Driver/blob/main/index.html>
</details></li>
</ul>
<hr>
<h2 id="Emu-Video-Factorizing-Text-to-Video-Generation-by-Explicit-Image-Conditioning"><a href="#Emu-Video-Factorizing-Text-to-Video-Generation-by-Explicit-Image-Conditioning" class="headerlink" title="Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning"></a>Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10709">http://arxiv.org/abs/2311.10709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra</li>
<li>for: 这个论文是用于描述一种文本到视频生成模型，即Emu Video，它将文本生成分解成两步：首先根据文本生成图像，然后根据图像和文本生成视频。</li>
<li>methods: 这个模型使用了调整的噪声表格和多stage训练来直接生成高质量和高分辨率的视频，不需要先后串行多个模型。</li>
<li>results: 人工评价中，我们生成的视频质量高于所有之前的工作（81% vs. Google的Imagen Video，90% vs. Nvidia的PYOCO，96% vs. Meta的Make-A-Video），并且超过了商业解决方案 such as RunwayML的Gen2和Pika Labs。此外，我们的因式方法自然地适用于根据用户文本提示生成动画图像，我们的生成被评价为96%高于前工作。<details>
<summary>Abstract</summary>
We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions--adjusted noise schedules for diffusion, and multi-stage training--that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's PYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial solutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user's text prompt, where our generations are preferred 96% over prior work.
</details>
<details>
<summary>摘要</summary>
我们介绍Emu Video，一种文本到视频生成模型，它将生成过程分为两步：首先生成基于文本的图像，然后生成基于文本和生成图像的视频。我们提出了关键的设计决策——调整的噪声学习策略和多Stage训练——使得我们可以直接生成高质量和高分辨率的视频，不需要先前的深度层次模型。在人工评估中，我们生成的视频质量得到了81%的优势 compared to Google的Imagen Video，90%的优势 compared to Nvidia的PYOCO，和96%的优势 compared to Meta的Make-A-Video。我们的模型超越了商业解决方案 such as RunwayML的Gen2和Pika Labs。最后，我们的分解方法自然地适用于基于用户文本提示的图像动画，我们的生成被评估为96%高于先前的工作。
</details></li>
</ul>
<hr>
<h2 id="Using-linear-initialisation-to-improve-speed-of-convergence-and-fully-trained-error-in-Autoencoders"><a href="#Using-linear-initialisation-to-improve-speed-of-convergence-and-fully-trained-error-in-Autoencoders" class="headerlink" title="Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders"></a>Using linear initialisation to improve speed of convergence and fully-trained error in Autoencoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10699">http://arxiv.org/abs/2311.10699</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/artefactory-uk/autoencoder-paper">https://github.com/artefactory-uk/autoencoder-paper</a></li>
<li>paper_authors: Marcel Marais, Mate Hartstein, George Cevora</li>
<li>for: 这 paper 的目的是提出一种新的初始化方法，以优化人工神经网络的训练。</li>
<li>methods: 该 paper 使用了一种名为 Straddled Matrix Initialiser 的新初始化技术，它基于作者假设大规模数据中的关系都是线性的，只有小规模的非线性关系需要复杂的非线性函数。该技术结合 Straddled Matrix 和 ReLU 活化函数，初始化神经网络为一个几乎是线性模型。</li>
<li>results: 作者在三个数据集上使用 Straddled Matrix Initialiser 和七种state-of-the-art 初始化方法进行比较，结果显示 Straddled Matrix Initialiser 在所有 экспериментах中明显超过了所有其他方法。<details>
<summary>Abstract</summary>
Good weight initialisation is an important step in successful training of Artificial Neural Networks. Over time a number of improvements have been proposed to this process. In this paper we introduce a novel weight initialisation technique called the Straddled Matrix Initialiser. This initialisation technique is motivated by our assumption that major, global-scale relationships in data are linear with only smaller effects requiring complex non-linearities. Combination of Straddled Matrix and ReLU activation function initialises a Neural Network as a de facto linear model, which we postulate should be a better starting point for optimisation given our assumptions. We test this by training autoencoders on three datasets using Straddled Matrix and seven other state-of-the-art weight initialisation techniques. In all our experiments the Straddeled Matrix Initialiser clearly outperforms all other methods.
</details>
<details>
<summary>摘要</summary>
好的初始化Weight是训练人工神经网络成功的重要步骤。随着时间的推移，许多改进有被提议用于这个过程。在这篇论文中，我们介绍了一种新的Weight初始化技术called Straddled Matrix Initializer。这种初始化技术基于我们假设大规模数据中的主要关系是线性的，只有小规模的影响需要复杂的非线性关系。将Straddled Matrix和ReLU活化函数结合使得神经网络被初始化为一个事实上的线性模型，我们认为这应该是优化的开始点。我们通过在三个数据集上使用Straddled Matrix和七种现状顶尖Weight初始化技术进行训练autoencoder来测试这个假设。在所有我们的实验中，Straddled Matrix Initializer明显超过了所有其他方法。
</details></li>
</ul>
<hr>
<h2 id="A-novel-post-hoc-explanation-comparison-metric-and-applications"><a href="#A-novel-post-hoc-explanation-comparison-metric-and-applications" class="headerlink" title="A novel post-hoc explanation comparison metric and applications"></a>A novel post-hoc explanation comparison metric and applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10811">http://arxiv.org/abs/2311.10811</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shreyan Mitra, Leilani Gilpin</li>
<li>for: 这paper是为了评估explainable AI系统之间的一致性而写的。</li>
<li>methods: 这paper使用了一种新的评估 metric called Shreyan Distance，用于比较两种explainable AI系统（SHAP和LIME）在 regression和 classification 任务中的一致性。</li>
<li>results: 该paper发现，在不同的学习任务中，explainable AI系统之间的一致性可以 vary significantly，表明一致性不仅取决于explainer的自身特性，还取决于学习任务的类型。Here’s the English version of the paper’s abstract:This paper presents a novel metric called the Shreyan Distance to quantify the differences between explainable AI systems. The Shreyan Distance is based on the weighted difference between ranked feature importance lists produced by such systems. The paper compares two popular explainable AI systems, SHAP and LIME, for both regression and classification learning tasks. The results show that the average Shreyan Distance varies significantly between these two tasks, indicating that consistency between explainers not only depends on inherent properties of the explainers themselves, but also the type of learning task. The paper also introduces the XAISuite library, which integrates the Shreyan distance algorithm into machine learning pipelines.<details>
<summary>Abstract</summary>
Explanatory systems make the behavior of machine learning models more transparent, but are often inconsistent. To quantify the differences between explanatory systems, this paper presents the Shreyan Distance, a novel metric based on the weighted difference between ranked feature importance lists produced by such systems. This paper uses the Shreyan Distance to compare two explanatory systems, SHAP and LIME, for both regression and classification learning tasks. Because we find that the average Shreyan Distance varies significantly between these two tasks, we conclude that consistency between explainers not only depends on inherent properties of the explainers themselves, but also the type of learning task. This paper further contributes the XAISuite library, which integrates the Shreyan distance algorithm into machine learning pipelines.
</details>
<details>
<summary>摘要</summary>
<<SYS>>使用描述系统可以使机器学习模型的行为更加透明，但这些系统经常存在不一致性。为了量化这些描述系统之间的差异，这篇论文提出了尚然距离度量，这是基于权重加权的特征重要性列表生成的描述系统之间的差异。本文使用尚然距离对两种描述系统（SHAP和LIME）进行了回归和分类学习任务的比较。我们发现，在这两种任务之间，均值尚然距离有很大的差异，这表明，透明度的一致性不仅取决于描述系统本身的内在特性，还取决于学习任务的类型。此外，本文还提供了XAISuite库，它将尚然距离算法集成到机器学习管道中。
</details></li>
</ul>
<hr>
<h2 id="PEFT-MedAware-Large-Language-Model-for-Medical-Awareness"><a href="#PEFT-MedAware-Large-Language-Model-for-Medical-Awareness" class="headerlink" title="PEFT-MedAware: Large Language Model for Medical Awareness"></a>PEFT-MedAware: Large Language Model for Medical Awareness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10697">http://arxiv.org/abs/2311.10697</a></li>
<li>repo_url: None</li>
<li>paper_authors: Keivalya Pandya</li>
<li>for: 这个研究旨在提高医疗问题回答 tasks 的精度和效率，特别是在资源有限的环境下。</li>
<li>methods: 本研究使用 parameter-efficient fine-tuning (PEFT) 技术来优化 Falcon-1b 大型自然语言模型，并使用特殊的 MedQuAD 资料集，包含 16,407 个医疗问题组。</li>
<li>results: 研究发现，使用 PEFT 技术可以将 Falcon-1b 模型在医疗问题回答 tasks 中提高精度，并且只需使用 0.44% 的训练urable 参数，实现了资源有限下的高效性。<details>
<summary>Abstract</summary>
Chat models are capable of answering a wide range of questions, however, the accuracy of their responses is highly uncertain. In this research, we propose a specialized PEFT-MedAware model where we utilize parameter-efficient fine-tuning (PEFT) to enhance the Falcon-1b large language model on specialized MedQuAD data consisting of 16,407 medical QA pairs, leveraging only 0.44% of its trainable parameters to enhance computational efficiency. The paper adopts data preprocessing and PEFT to optimize model performance, complemented by a BitsAndBytesConfig for efficient transformer training. The resulting model was capable of outperforming other LLMs in medical question-answering tasks in specific domains with greater accuracy utilizing limited computational resources making it suitable for deployment in resource-constrained environments. We propose further improvements through expanded datasets, larger models, and feedback mechanisms for sustained medical relevancy. Our work highlights the efficiency gains and specialized capabilities of PEFT in medical AI, outpacing standard models in precision without extensive resource demands. The proposed model and data are released for research purposes only.
</details>
<details>
<summary>摘要</summary>
协议模型可以回答广泛的问题，但它们的准确性却很 uncertain。在这项研究中，我们提出了一种特殊的 PEFT-MedAware 模型，我们利用 parameter-efficient fine-tuning（PEFT）来提高 Falcon-1b 大语言模型在特殊的 MedQuAD 数据上，只使用 0.44% 的可训练参数来提高计算效率。本文采用了数据处理和 PEFT 来优化模型性能，并使用 BitsAndBytesConfig 来快速训练 transformer。得到的模型能够在具体的医疗问答任务中超越其他 LLMS 的精度，使用有限的计算资源。我们建议进一步改进通过扩展数据集、更大的模型和反馈机制来保持医疗相关性。我们的工作表明 PEFT 在医疗 AI 中具有高效性和特殊能力，在精度不受极限计算资源的情况下，超越标准模型。我们发布了提posed模型和数据供研究用途。
</details></li>
</ul>
<hr>
<h2 id="Use-GPT-J-Prompt-Generation-with-RoBERTa-for-NER-Models-on-Diagnosis-Extraction-of-Periodontal-Diagnosis-from-Electronic-Dental-Records"><a href="#Use-GPT-J-Prompt-Generation-with-RoBERTa-for-NER-Models-on-Diagnosis-Extraction-of-Periodontal-Diagnosis-from-Electronic-Dental-Records" class="headerlink" title="Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records"></a>Use GPT-J Prompt Generation with RoBERTa for NER Models on Diagnosis Extraction of Periodontal Diagnosis from Electronic Dental Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10810">http://arxiv.org/abs/2311.10810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao-Shun Chuang, Xiaoqian Jiang, Chun-Teh Lee, Ryan Brandon, Duong Tran, Oluwabunmi Tokede, Muhammad F. Walji</li>
<li>for: 这个研究探讨了对命名实体识别（NER）任务的提问生成器的可用性，以及不同设定中的提问对性能的影响。</li>
<li>methods: 该研究使用GPT-J模型生成提问，直接测试标准金句并生成种子，然后将种子传输给RoBERTa模型和spaCy包。</li>
<li>results: 研究发现，具有更少的负例并且更多的示例的提问可以达到最佳结果，即F1分数0.72。此外，在训练RoBERTa模型后，所有设定中的性能都保持了0.92-0.97的Consistency，表明种子质量比quantity更重要。这种提问生成方法可以快速和高效地挖掘医疗记录中的 periodontal 诊断。<details>
<summary>Abstract</summary>
This study explored the usability of prompt generation on named entity recognition (NER) tasks and the performance in different settings of the prompt. The prompt generation by GPT-J models was utilized to directly test the gold standard as well as to generate the seed and further fed to the RoBERTa model with the spaCy package. In the direct test, a lower ratio of negative examples with higher numbers of examples in prompt achieved the best results with a F1 score of 0.72. The performance revealed consistency, 0.92-0.97 in the F1 score, in all settings after training with the RoBERTa model. The study highlighted the importance of seed quality rather than quantity in feeding NER models. This research reports on an efficient and accurate way to mine clinical notes for periodontal diagnoses, allowing researchers to easily and quickly build a NER model with the prompt generation approach.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Extracting-periodontitis-diagnosis-in-clinical-notes-with-RoBERTa-and-regular-expression"><a href="#Extracting-periodontitis-diagnosis-in-clinical-notes-with-RoBERTa-and-regular-expression" class="headerlink" title="Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular expression"></a>Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular expression</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10809">http://arxiv.org/abs/2311.10809</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao-Shun Chuang, Chun-Teh Lee, Ryan Brandon, Trung Duong Tran, Oluwabunmi Tokede, Muhammad F. Walji, Xiaoqian Jiang</li>
<li>for: 本研究使用文本处理和自然语言处理（NLP）模型，从临床笔记中挖掘出periodontitis的诊断。</li>
<li>methods: 研究使用了不同的正则表达（RE）方法，分别是简单RE和复杂RE，对文本数据进行提取和生成训练数据。使用了SpaCy包和RoBERTa变换器模型来构建Named Entity Recognition（NER）模型，并对其与手动标注的金标准进行评估。</li>
<li>results: 研究发现，随着RE算法的复杂度增加，F1分数从0.3-0.4提高到约0.9。NER模型显示了优秀的预测性，简单RE方法在评估指标中得分0.84-0.92，而复杂RE方法和组合RE方法在评估指标中得分0.95-0.99。这项研究示例了将NER方法和NLP模型结合使用，从自由文本中提取目标信息，并将其转化为结构化数据，以满足缺失的诊断。<details>
<summary>Abstract</summary>
This study aimed to utilize text processing and natural language processing (NLP) models to mine clinical notes for the diagnosis of periodontitis and to evaluate the performance of a named entity recognition (NER) model on different regular expression (RE) methods. Two complexity levels of RE methods were used to extract and generate the training data. The SpaCy package and RoBERTa transformer models were used to build the NER model and evaluate its performance with the manual-labeled gold standards. The comparison of the RE methods with the gold standard showed that as the complexity increased in the RE algorithms, the F1 score increased from 0.3-0.4 to around 0.9. The NER models demonstrated excellent predictions, with the simple RE method showing 0.84-0.92 in the evaluation metrics, and the advanced and combined RE method demonstrating 0.95-0.99 in the evaluation. This study provided an example of the benefit of combining NER methods and NLP models in extracting target information from free-text to structured data and fulfilling the need for missing diagnoses from unstructured notes.
</details>
<details>
<summary>摘要</summary>
Note: Simplified Chinese is also known as "简化字符" or "简化字符".Please note that the translation is in Simplified Chinese, if you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Distilling-and-Retrieving-Generalizable-Knowledge-for-Robot-Manipulation-via-Language-Corrections"><a href="#Distilling-and-Retrieving-Generalizable-Knowledge-for-Robot-Manipulation-via-Language-Corrections" class="headerlink" title="Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections"></a>Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10678">http://arxiv.org/abs/2311.10678</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Stanford-ILIAD/droc">https://github.com/Stanford-ILIAD/droc</a></li>
<li>paper_authors: Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh</li>
<li>for: 本研究旨在帮助机器人在新环境中提高表现，通过人类纠正反馈来启发机器人学习总结和泛化。</li>
<li>methods: 本研究使用了大型自然语言模型（LLM），可以响应任意形式的语言反馈，提取纠正中的总结知识，并根据文本和视觉相似性检索相关的过去经验。</li>
<li>results: 对比其他直接通过LLM生成机器人代码的技术，DROC需要一round的纠正数量的一半，并在两轮纠正后几乎不需要再纠正。研究还表明，DROC在新任务或物体实例中表现出色，并提供了视频、提示和代码等详细结果。<details>
<summary>Abstract</summary>
Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results, videos, prompts and code on https://sites.google.com/stanford.edu/droc .
</details>
<details>
<summary>摘要</summary>
We present Distillation and Retrieval of Online Corrections (DROC), a system that responds to arbitrary language feedback, distills generalizable knowledge from corrections, and retrieves relevant past experiences based on textual and visual similarity. DROC can respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in new settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via large language models by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations.For more information, videos, prompts, and code, please visit <https://sites.google.com/stanford.edu/droc>.
</details></li>
</ul>
<hr>
<h2 id="Fuse-It-or-Lose-It-Deep-Fusion-for-Multimodal-Simulation-Based-Inference"><a href="#Fuse-It-or-Lose-It-Deep-Fusion-for-Multimodal-Simulation-Based-Inference" class="headerlink" title="Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference"></a>Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10671">http://arxiv.org/abs/2311.10671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Marvin Schmitt, Stefan T. Radev, Paul-Christian Bürkner</li>
<li>for: 这个论文旨在探讨如何通过将多种不同来源的数据融合，使用神经网络进行模拟基于推断，以提高数据分析精度。</li>
<li>methods: 该论文提出了多Modal Neural Posterior Estimation（MultiNPE）方法，启发自深度融合学习的进步，可以将不同领域的数据融合到一起，进行复杂数学模型的参数推断。 authors 提出了不同的多模态融合方法（早期、晚期和混合），并对它们在三个数学实验中进行评估。</li>
<li>results: 论文表明，MultiNPE不仅在一个基本模型上超越了 na&quot;ive 基elines，而且在 neuroscience 和 cardiology 等科学领域中的代表性模型上也实现了更高的推断精度。 authors 还系统地研究了部分缺失数据对不同融合策略的影响。 results 表明，晚期和混合融合技术在实际应用中是选择的。<details>
<summary>Abstract</summary>
We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate different multimodal fusion approaches for MultiNPE (early, late, and hybrid) and evaluate their performance in three challenging numerical experiments. MultiNPE not only outperforms na\"ive baselines on a benchmark model, but also achieves superior inference on representative scientific models from neuroscience and cardiology. In addition, we systematically investigate the impact of partially missing data on the different fusion strategies. Across our different experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based inference.
</details>
<details>
<summary>摘要</summary>
我们介绍了多Modal neural posterior估计（MultiNPE），一种能够将不同来源的数据集成在基于神经网络的 simulated-based推理中，以提高参数推导的准确性。受到深度融合学习的进步启发，MultiNPE允许研究人员从不同领域中的数据中分析数据，并使用更加准确地推导复杂的数学模型的参数。我们对MultiNPE的不同多模态融合方法（早期、晚期和混合）进行了不同的评估，并在三个复杂的数学实验中评估其性能。MultiNPE不仅在一个标准模型上超越了Na\"ive的基eline，还在 neuroscience和cardiology等科学领域中的代表性模型上实现了更高的推导精度。此外，我们系统地研究了partially missing data的不同融合策略的影响。在我们的不同实验中，晚期和混合融合技术被证明为实际应用中的首选方法。
</details></li>
</ul>
<hr>
<h2 id="Multi-delay-arterial-spin-labeled-perfusion-estimation-with-biophysics-simulation-and-deep-learning"><a href="#Multi-delay-arterial-spin-labeled-perfusion-estimation-with-biophysics-simulation-and-deep-learning" class="headerlink" title="Multi-delay arterial spin-labeled perfusion estimation with biophysics simulation and deep learning"></a>Multi-delay arterial spin-labeled perfusion estimation with biophysics simulation and deep learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10640">http://arxiv.org/abs/2311.10640</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcmue/2311COMP106401">https://github.com/hcmue/2311COMP106401</a></li>
<li>paper_authors: Renjiu Hu, Qihao Zhang, Pascal Spincemaille, Thanh D. Nguyen, Yi Wang</li>
<li>for: 用于估计 perfusion Q 从核磁共振成像（ASL）图像中。</li>
<li>methods: 使用深度学习的3D U-Net（QTMnet）来估计 perfusion。网络在基于人工血管结构的 simulated 4D tracer进程图像上进行训练和测试。</li>
<li>results: QTMnet 精确地重建 perfusion Q 从 koncentrasi 数据。synthetic brain ASL 图像中的相对误差为 7.04%，比单delay ASL 模型的误差（25.15%）和多delay ASL 模型的误差（12.62%）都要低。<details>
<summary>Abstract</summary>
Purpose: To develop biophysics-based method for estimating perfusion Q from arterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net (QTMnet) was trained to estimate perfusion from 4D tracer propagation images. The network was trained and tested on simulated 4D tracer concentration data based on artificial vasculature structure generated by constrained constructive optimization (CCO) method. The trained network was further tested in a synthetic brain ASL image based on vasculature network extracted from magnetic resonance (MR) angiography. The estimations from both trained network and a conventional kinetic model were compared in ASL images acquired from eight healthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from concentration data. Relative error of the synthetic brain ASL image was 7.04% for perfusion Q, lower than the error using single-delay ASL model: 25.15% for Q, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet provides accurate estimation on perfusion parameters and is a promising approach as a clinical ASL MRI image processing pipeline.
</details>
<details>
<summary>摘要</summary>
目的：通过深度学习来开发基于生物物理的方法，从树脂扩散成像（ASL）图像中估算血液流速（Q）。方法：使用3D U-Net（QTMnet）来估算 perfusion Q，并在基于人工血管结构的 simulate 4D tracer扩散数据上训练该网络。网络被训练和测试在基于magnetic resonance（MR） angiography中提取的血管网络上。已训练的网络被进一步测试在synthetic brain ASL图像上，并与传统的动力学模型的估算结果进行比较。结果：QTMnet高精度地重建 perfusion Q。人工脑 ASL图像中的相对误差为7.04%，比单Delay ASL模型（25.15%）和多Delay ASL模型（12.62%）更低。结论：QTMnet提供了高精度的血液流速估算，是一个有前途的临床ASL MRI图像处理管道。
</details></li>
</ul>
<hr>
<h2 id="Concept-free-Causal-Disentanglement-with-Variational-Graph-Auto-Encoder"><a href="#Concept-free-Causal-Disentanglement-with-Variational-Graph-Auto-Encoder" class="headerlink" title="Concept-free Causal Disentanglement with Variational Graph Auto-Encoder"></a>Concept-free Causal Disentanglement with Variational Graph Auto-Encoder</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10638">http://arxiv.org/abs/2311.10638</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingyun Feng, Lin Zhang, Lili Yang</li>
<li>for: 学习不含概念的 causal 分解，以便从数据中直接学习概念结构。</li>
<li>methods: 提出一种不含概念的 causal 分解方法，基于理论上的紧急上界，直接从数据中学习概念结构。</li>
<li>results: 经验表明，提出的模型CCVGAE和CC-Meta-Graph在不同的数据集上均达到了较高的性能，与基eline相比，提高了29%和11%的精度。<details>
<summary>Abstract</summary>
In disentangled representation learning, the goal is to achieve a compact representation that consists of all interpretable generative factors in the observational data. Learning disentangled representations for graphs becomes increasingly important as graph data rapidly grows. Existing approaches often rely on Variational Auto-Encoder (VAE) or its causal structure learning-based refinement, which suffer from sub-optimality in VAEs due to the independence factor assumption and unavailability of concept labels, respectively. In this paper, we propose an unsupervised solution, dubbed concept-free causal disentanglement, built on a theoretically provable tight upper bound approximating the optimal factor. This results in an SCM-like causal structure modeling that directly learns concept structures from data. Based on this idea, we propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal disentanglement layer into Variational Graph Auto-Encoder. Furthermore, we prove concept consistency under our concept-free causal disentanglement framework, hence employing it to enhance the meta-learning framework, called concept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive experiments to demonstrate the superiority of the proposed models: CCVGAE and CC-Meta-Graph, reaching up to $29\%$ and $11\%$ absolute improvements over baselines in terms of AUC, respectively.
</details>
<details>
<summary>摘要</summary>
干净表示学习的目标是达到一个具有所有可解释生成因素的紧凑表示，这些生成因素在观测数据中都是可解释的。随着图数据的快速增长，学习图数据的干净表示变得越来越重要。现有的方法经常利用变量自动编码器（VAE）或其 causal structure learning 的修正，但这些方法受到 VAE 的独立因素假设和概念标签不可用的限制。在这篇论文中，我们提出了一种无监督的解决方案，称为无概念 causal disentanglement，基于一个可求优的上界，来近似最佳因子。这导致了一种 SCM 类的 causal structure 模型，直接从数据中学习概念结构。基于这个想法，我们提出了无概念 causal VGAE（CCVGAE），通过在变量自动编码器中添加一个新的 causal disentanglement 层来实现。此外，我们证明了我们的概念一致性，因此可以通过我们的概念自由 causal Meta-Graph（CC-Meta-Graph）框架来增强元学习框架。我们进行了广泛的实验，以示我们提出的模型（CCVGAE 和 CC-Meta-Graph）的超越性，相比基eline，其中 AUC 上的改进率可达 $29\%$ 和 $11\%$。
</details></li>
</ul>
<hr>
<h2 id="A-Self-enhancement-Approach-for-Domain-specific-Chatbot-Training-via-Knowledge-Mining-and-Digest"><a href="#A-Self-enhancement-Approach-for-Domain-specific-Chatbot-Training-via-Knowledge-Mining-and-Digest" class="headerlink" title="A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest"></a>A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10614">http://arxiv.org/abs/2311.10614</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang</li>
<li>for: 提高大语言模型（LLM）在特定领域的语言生成能力</li>
<li>methods: 自动从域专文检索Question-Answer对，并将其与对话集合进行练习 fine-tune LLM</li>
<li>results: 模型在特定领域的表现得到了显著提高，超过了直接在域 corpus 上进行 fine-tune 的模型，并且只需600个种子实例进行自动化培育。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Active-Inference-on-the-Edge-A-Design-Study"><a href="#Active-Inference-on-the-Edge-A-Design-Study" class="headerlink" title="Active Inference on the Edge: A Design Study"></a>Active Inference on the Edge: A Design Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10607">http://arxiv.org/abs/2311.10607</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Sedlak, Victor Casamayor Pujol, Praveen Kumar Donta, Schahram Dustdar</li>
<li>for: 本研究旨在提高分布式计算系统中任务分配和机器学习训练的精度和可靠性，以满足服务质量（QoS）要求。</li>
<li>methods: 本研究使用了机器学习（ML）和自动推理（ACI）技术，实现了分布式代理人的自适应行为和感知征识。</li>
<li>results: 研究结果表明，通过将ACI技术与ML技术结合使用，可以快速和可靠地解决分布式系统中的优化问题，同时满足QoS要求。<details>
<summary>Abstract</summary>
Machine Learning (ML) is a common tool to interpret and predict the behavior of distributed computing systems, e.g., to optimize the task distribution between devices. As more and more data is created by Internet of Things (IoT) devices, data processing and ML training are carried out by edge devices in close proximity. To ensure Quality of Service (QoS) throughout these operations, systems are supervised and dynamically adapted with the help of ML. However, as long as ML models are not retrained, they fail to capture gradual shifts in the variable distribution, leading to an inaccurate view of the system state. Moreover, as the prediction accuracy decreases, the reporting device should actively resolve uncertainties to improve the model's precision. Such a level of self-determination could be provided by Active Inference (ACI) -- a concept from neuroscience that describes how the brain constantly predicts and evaluates sensory information to decrease long-term surprise. We encompassed these concepts in a single action-perception cycle, which we implemented for distributed agents in a smart manufacturing use case. As a result, we showed how our ACI agent was able to quickly and traceably solve an optimization problem while fulfilling QoS requirements.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Chatbots-as-social-companions-How-people-perceive-consciousness-human-likeness-and-social-health-benefits-in-machines"><a href="#Chatbots-as-social-companions-How-people-perceive-consciousness-human-likeness-and-social-health-benefits-in-machines" class="headerlink" title="Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines"></a>Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10599">http://arxiv.org/abs/2311.10599</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rose Guingrich, Michael S. A. Graziano</li>
<li>for: This paper aims to investigate the impact of human-AI interaction on human-human interaction, specifically focusing on the use of chatbots as social companions.</li>
<li>methods: The study compares the social health benefits of using chatbots with not using them, and examines how people perceive the consciousness and humanlikeness of chatbots.</li>
<li>results: The study finds that companion bot users report beneficial social health effects, while nonusers view them as harmful. Additionally, perceiving chatbots as more conscious and humanlike is associated with more positive opinions and better social health benefits.<details>
<summary>Abstract</summary>
As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, but little is known about how their use impacts human relationships. A common hypothesis is that these companion bots are detrimental to social health by harming or replacing human interaction. To understand how companion bots impact social health, we studied people who used companion bots and people who did not. Contrary to expectations, companion bot users indicated that these relationships were beneficial to their social health, whereas nonusers viewed them as harmful. Another common assumption is that people perceive conscious, humanlike AI as disturbing and threatening. Among both users and nonusers, however, we found the opposite: perceiving companion bots as more conscious and humanlike correlated with more positive opinions and better social health benefits. Humanlike bots may aid social health by supplying reliable and safe interactions, without necessarily harming human relationships.
</details>
<details>
<summary>摘要</summary>
随着人工智能（AI）的普及，人类与AI交互的问题凝固在人类之间的交互方面。聊天机器人是一种在社交方面使用的AI，但对人类之间的交互的影响还不够了解。一个常见的假设是，这些伴侣机器人会对人类之间的关系产生负面影响，或者取代人类交流。为了了解伴侣机器人对社交健康的影响，我们对使用伴侣机器人和不使用伴侣机器人的人进行了研究。结果表明，使用伴侣机器人的人认为这些关系对其社交健康有益，而不使用伴侣机器人的人则认为它们有害。另一个常见的假设是，人们认为对人类似的AI会对人类产生不良影响。我们发现，使用伴侣机器人的人和不使用伴侣机器人的人都认为， perceiving companion bots as more conscious and humanlike correlated with more positive opinions and better social health benefits。可能是因为这些机器人可以提供可靠和安全的交流，而不会直接影响人类之间的关系。
</details></li>
</ul>
<hr>
<h2 id="Designing-Reconfigurable-Intelligent-Systems-with-Markov-Blankets"><a href="#Designing-Reconfigurable-Intelligent-Systems-with-Markov-Blankets" class="headerlink" title="Designing Reconfigurable Intelligent Systems with Markov Blankets"></a>Designing Reconfigurable Intelligent Systems with Markov Blankets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10597">http://arxiv.org/abs/2311.10597</a></li>
<li>repo_url: None</li>
<li>paper_authors: Boris Sedlak, Victor Casamayor Pujol, Praveen Kumar Donta, Schahram Dustdar</li>
<li>for: 这个论文是为了评估业务需求（服务水平目标）而写的。</li>
<li>methods: 这篇论文使用了 causality 筛选器基于Markov抹层（MB），以限制每个设备需要跟踪的变量数量，并在设备基础上分析 SLOs。</li>
<li>results: 这篇论文的结果表明，通过使用 causality 筛选器和 MB，可以减少设备需要跟踪的变量数量，并在设备基础上分析 SLOs，从而实现了 Decentralized 的 Intelligence。<details>
<summary>Abstract</summary>
Compute Continuum (CC) systems comprise a vast number of devices distributed over computational tiers. Evaluating business requirements, i.e., Service Level Objectives (SLOs), requires collecting data from all those devices; if SLOs are violated, devices must be reconfigured to ensure correct operation. If done centrally, this dramatically increases the number of devices and variables that must be considered, while creating an enormous communication overhead. To address this, we (1) introduce a causality filter based on Markov blankets (MB) that limits the number of variables that each device must track, (2) evaluate SLOs decentralized on a device basis, and (3) infer optimal device configuration for fulfilling SLOs. We evaluated our methodology by analyzing video stream transformations and providing device configurations that ensure the Quality of Service (QoS). The devices thus perceived their environment and acted accordingly -- a form of decentralized intelligence.
</details>
<details>
<summary>摘要</summary>
计算 kontinuum (CC) 系统包括大量分布在计算层次的设备。评估业务需求（服务级别目标，SLO）需要从所有设备收集数据，如果 SLO 被违反，则设备需要重新配置以确保正确的运行。如果从中心处进行，这将导致对设备和变量的考虑数量增加得非常大，同时创造出巨大的通信开销。为解决这一问题，我们（1）引入 causality 筛选器基于 Markov 纱（MB），限制每个设备需要跟踪的变量数量，（2）在设备基础上分布式评估 SLO，（3）根据 fulfilling SLO 推导出最佳设备配置。我们对视频流变换进行分析，并提供了保证服务质量（QoS）的设备配置，这些设备因此可以根据自己的环境进行自适应调整——一种分布式智能。
</details></li>
</ul>
<hr>
<h2 id="Hashing-it-Out-Predicting-Unhealthy-Conversations-on-Twitter"><a href="#Hashing-it-Out-Predicting-Unhealthy-Conversations-on-Twitter" class="headerlink" title="Hashing it Out: Predicting Unhealthy Conversations on Twitter"></a>Hashing it Out: Predicting Unhealthy Conversations on Twitter</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10596">http://arxiv.org/abs/2311.10596</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevendleung/hashing-it-out">https://github.com/stevendleung/hashing-it-out</a></li>
<li>paper_authors: Steven Leung, Filippos Papapolyzos</li>
<li>for: 预测Twitter上的对话偏领事件，建立一个实用的工具，促进社交媒体上的更好交流。</li>
<li>methods: 使用BERT模型，在Twitter corpus上进行预测，并通过精心练习和Synthetic oversampling技术来缓解过拟合问题。</li>
<li>results: 比基线LSTM模型有明显的性能优势，并在小规模、新 Dataset上进行了较好的预测。<details>
<summary>Abstract</summary>
Personal attacks in the context of social media conversations often lead to fast-paced derailment, leading to even more harmful exchanges being made. State-of-the-art systems for the detection of such conversational derailment often make use of deep learning approaches for prediction purposes. In this paper, we show that an Attention-based BERT architecture, pre-trained on a large Twitter corpus and fine-tuned on our task, is efficient and effective in making such predictions. This model shows clear advantages in performance to the existing LSTM model we use as a baseline. Additionally, we show that this impressive performance can be attained through fine-tuning on a relatively small, novel dataset, particularly after mitigating overfitting issues through synthetic oversampling techniques. By introducing the first transformer based model for forecasting conversational events on Twitter, this work lays the foundation for a practical tool to encourage better interactions on one of the most ubiquitous social media platforms.
</details>
<details>
<summary>摘要</summary>
互联网社交媒体对话中的人身攻击常常导致快速的对话脱轨，从而导致更多的恶势攻击性交流。现代的对话脱轨检测系统经常使用深度学习方法进行预测。在这篇论文中，我们表明了一种基于注意力的BERT架构，在Twitter大量数据集上预训练并在我们的任务上细化，可以有效地进行这些预测。这个模型在性能方面有明显的优势，比基线LSTM模型更好。此外，我们还证明了这种出色的性能可以通过对小型、新的数据集进行细化来实现，特别是通过synthetic oversampling技术来 Mitigate overfitting问题。通过介绍Twitter上首个基于 transformer 模型的对话事件预测模型，这项工作为社交媒体平台上更好的互动提供了基础。
</details></li>
</ul>
<hr>
<h2 id="FOCAL-A-Cost-Aware-Video-Dataset-for-Active-Learning"><a href="#FOCAL-A-Cost-Aware-Video-Dataset-for-Active-Learning" class="headerlink" title="FOCAL: A Cost-Aware Video Dataset for Active Learning"></a>FOCAL: A Cost-Aware Video Dataset for Active Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10591">http://arxiv.org/abs/2311.10591</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/olivesgatech/focal_dataset">https://github.com/olivesgatech/focal_dataset</a></li>
<li>paper_authors: Kiran Kokilepersaud, Yash-Yee Logan, Ryan Benkert, Chen Zhou, Mohit Prabhushankar, Ghassan AlRegib, Enrique Corona, Kunjan Singh, Mostafa Parchami</li>
<li>for: 这个论文的目的是研究视频活动学中的注释成本影响。</li>
<li>methods: 这个论文使用了FOCAL数据集（ford-olives合作的活动学数据集），并引入了一些Sequential structure的活动学算法，以实现更好的注释成本和性能之间的平衡，同时减少浮点运算（FLOPS）的开销。</li>
<li>results: 研究发现，使用这些方法可以在113小时之内实现更好的性能和注释成本之间的平衡，并且比传统的活动学方法更便宜。<details>
<summary>Abstract</summary>
In this paper, we introduce the FOCAL (Ford-OLIVES Collaboration on Active Learning) dataset which enables the study of the impact of annotation-cost within a video active learning setting. Annotation-cost refers to the time it takes an annotator to label and quality-assure a given video sequence. A practical motivation for active learning research is to minimize annotation-cost by selectively labeling informative samples that will maximize performance within a given budget constraint. However, previous work in video active learning lacks real-time annotation labels for accurately assessing cost minimization and instead operates under the assumption that annotation-cost scales linearly with the amount of data to annotate. This assumption does not take into account a variety of real-world confounding factors that contribute to a nonlinear cost such as the effect of an assistive labeling tool and the variety of interactions within a scene such as occluded objects, weather, and motion of objects. FOCAL addresses this discrepancy by providing real annotation-cost labels for 126 video sequences across 69 unique city scenes with a variety of weather, lighting, and seasonal conditions. We also introduce a set of conformal active learning algorithms that take advantage of the sequential structure of video data in order to achieve a better trade-off between annotation-cost and performance while also reducing floating point operations (FLOPS) overhead by at least 77.67%. We show how these approaches better reflect how annotations on videos are done in practice through a sequence selection framework. We further demonstrate the advantage of these approaches by introducing two performance-cost metrics and show that the best conformal active learning method is cheaper than the best traditional active learning method by 113 hours.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们介绍了FOCAL（福特-橙色蜂合作活动学习）数据集，它允许我们研究视频活动学习中标注成本的影响。标注成本指的是标注和质量控制视频序列所需的时间。在实际中，活动学习研究的目标是最小化标注成本，以便在给定的预算限制下 maximize性能。然而，以前的视频活动学习研究缺乏实时标注标签，因此无法准确评估成本最小化。FOCAL解决了这个问题，提供了126个视频序列的真实标注成本标签，这些序列来自69个不同的城市场景，包括不同的天气、照明和季节条件。我们还介绍了一组具有顺序结构的视频数据的宽泛活动学习算法，可以在标注成本和性能之间达到更好的变换。此外，我们还降低了浮点运算过程（FLOPS）的负担，至少减少77.67%。我们示出了这些方法如何更好地反映实际标注视频的方式，并在序列选择框架中引入了两种性能成本指标。我们还证明了这些方法的优势，并证明了最佳宽泛活动学习方法比最佳传统活动学习方法快113小时。
</details></li>
</ul>
<hr>
<h2 id="EduGym-An-Environment-Suite-for-Reinforcement-Learning-Education"><a href="#EduGym-An-Environment-Suite-for-Reinforcement-Learning-Education" class="headerlink" title="EduGym: An Environment Suite for Reinforcement Learning Education"></a>EduGym: An Environment Suite for Reinforcement Learning Education</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10590">http://arxiv.org/abs/2311.10590</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rlg-leiden/edugym">https://github.com/rlg-leiden/edugym</a></li>
<li>paper_authors: Thomas M. Moerland, Matthias Müller-Brockhausen, Zhao Yang, Andrius Bernatavicius, Koen Ponse, Tom Kouwenhoven, Andreas Sauter, Michiel van der Meer, Bram Renting, Aske Plaat</li>
<li>for: 本研究旨在提供一个用于强化学习教育的教学环境和相关互动式笔记，以帮助学生将Equtions与Code转换成实践中的问题。</li>
<li>methods: 本研究使用了一个名为EduGym的教学环境和相关互动式笔记，每个环境都是为了解释一个特定的强化学习挑战（如探索、偏见观察、随机性等），并且说明了解决方案的可能性，将Equtions与Code联系在一起。</li>
<li>results: 在评估86%的学生和研究人员中，大多数人认为EduGym是一个有用的强化学习教育工具。所有的笔记可以从<a target="_blank" rel="noopener" href="https://sites.google.com/view/edu-gym/home%E4%B8%8B%E8%BD%BD%EF%BC%8C%E8%80%8C%E5%85%A8%E5%A5%97%E8%BD%AF%E4%BB%B6%E5%A5%97%E4%BB%B6%E5%8F%AF%E4%BB%A5%E4%BB%8Ehttps://github.com/RLG-Leiden/edugym%E5%AE%89%E8%A3%85%E3%80%82">https://sites.google.com/view/edu-gym/home下载，而全套软件套件可以从https://github.com/RLG-Leiden/edugym安装。</a><details>
<summary>Abstract</summary>
Due to the empirical success of reinforcement learning, an increasing number of students study the subject. However, from our practical teaching experience, we see students entering the field (bachelor, master and early PhD) often struggle. On the one hand, textbooks and (online) lectures provide the fundamentals, but students find it hard to translate between equations and code. On the other hand, public codebases do provide practical examples, but the implemented algorithms tend to be complex, and the underlying test environments contain multiple reinforcement learning challenges at once. Although this is realistic from a research perspective, it often hinders educational conceptual understanding. To solve this issue we introduce EduGym, a set of educational reinforcement learning environments and associated interactive notebooks tailored for education. Each EduGym environment is specifically designed to illustrate a certain aspect/challenge of reinforcement learning (e.g., exploration, partial observability, stochasticity, etc.), while the associated interactive notebook explains the challenge and its possible solution approaches, connecting equations and code in a single document. An evaluation among RL students and researchers shows 86% of them think EduGym is a useful tool for reinforcement learning education. All notebooks are available from https://sites.google.com/view/edu-gym/home, while the full software package can be installed from https://github.com/RLG-Leiden/edugym.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="SENetV2-Aggregated-dense-layer-for-channelwise-and-global-representations"><a href="#SENetV2-Aggregated-dense-layer-for-channelwise-and-global-representations" class="headerlink" title="SENetV2: Aggregated dense layer for channelwise and global representations"></a>SENetV2: Aggregated dense layer for channelwise and global representations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10807">http://arxiv.org/abs/2311.10807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahendran Narayanan</li>
<li>for: 这篇论文是为了提出一种新的图像分类模型，用于超越现有的图像分类模型。</li>
<li>methods: 该模型使用了压缩激活网络模块，以及多层感知器（MLP）来学习图像数据的全局表示。</li>
<li>results: 实验结果表明，提出的模型在评估 datasets 上具有remarkable的分类精度提升，与现有的建筑体系相比。<details>
<summary>Abstract</summary>
Convolutional Neural Networks (CNNs) have revolutionized image classification by extracting spatial features and enabling state-of-the-art accuracy in vision-based tasks. The squeeze and excitation network proposed module gathers channelwise representations of the input. Multilayer perceptrons (MLP) learn global representation from the data and in most image classification models used to learn extracted features of the image. In this paper, we introduce a novel aggregated multilayer perceptron, a multi-branch dense layer, within the Squeeze excitation residual module designed to surpass the performance of existing architectures. Our approach leverages a combination of squeeze excitation network module with dense layers. This fusion enhances the network's ability to capture channel-wise patterns and have global knowledge, leading to a better feature representation. This proposed model has a negligible increase in parameters when compared to SENet. We conduct extensive experiments on benchmark datasets to validate the model and compare them with established architectures. Experimental results demonstrate a remarkable increase in the classification accuracy of the proposed model.
</details>
<details>
<summary>摘要</summary>
卷积神经网络（CNN）已经革命化图像分类任务，提取空间特征并实现了视觉任务中的状态vector。 propose模块中的压缩和刺激网络（SENet）模块集成了通道表示，多层感知器（MLP）学习数据的全局表示。在这篇论文中，我们介绍了一种新的聚合多层感知器，在SENet模块中添加了多个密集层，以提高网络的频率特征捕捉和全局知识捕捉，从而实现更好的特征表示。这种提议的模型与SENet模型参数数量增加非常小，但性能明显提高。我们在标准数据集上进行了广泛的实验，并与现有的建筑物进行比较。实验结果表明，提议的模型具有很高的分类精度。
</details></li>
</ul>
<hr>
<h2 id="Testing-Language-Model-Agents-Safely-in-the-Wild"><a href="#Testing-Language-Model-Agents-Safely-in-the-Wild" class="headerlink" title="Testing Language Model Agents Safely in the Wild"></a>Testing Language Model Agents Safely in the Wild</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10538">http://arxiv.org/abs/2311.10538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz, Adam Tauman Kalai, David Bau</li>
<li>for: 这篇论文旨在提出一种安全的自主机器人测试框架，以便在实际世界中进行安全的自主测试。</li>
<li>methods: 该框架使用了上下文敏感的监控器，对自主机器人的行为进行审核，并在发现不安全的情况时停止测试。</li>
<li>results: 通过使用一个反对者 simulate 的自主机器人，该监控器能够识别并阻止不安全的情况，但在实际世界中进行测试时，还存在一些限制和挑战。<details>
<summary>Abstract</summary>
A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We a design a basic safety monitor that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the safety monitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.
</details>
<details>
<summary>摘要</summary>
安全自主测试需要在野外安全测试。然而，实际世界自主测试面临多种独特的安全挑战，包括测试过程可能导致伤害，以及与真实世界和可能有恶意actor的交互中遇到新的危险行为。我们提出了在互联网上进行安全自主代理测试的框架：代理行为被上下文敏感监控器监视，以防止不安全测试，并将异常行为排名和记录以供人类检查。我们设计了一个基本安全监控器，可以监控现有的LLM代理，并使用对抗式的模拟代理，测试其能够识别和阻止危险情况。然后，我们应用了安全监控器在AutoGPT的实际世界测试中，并发现了许多限制和挑战，这些限制和挑战将随自主代理技术的发展而出现。
</details></li>
</ul>
<hr>
<h2 id="SEA-Multi-Graph-based-High-Order-Sensor-Alignment-for-Multivariate-Time-Series-Unsupervised-Domain-Adaptation"><a href="#SEA-Multi-Graph-based-High-Order-Sensor-Alignment-for-Multivariate-Time-Series-Unsupervised-Domain-Adaptation" class="headerlink" title="SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate Time-Series Unsupervised Domain Adaptation"></a>SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate Time-Series Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10806">http://arxiv.org/abs/2311.10806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen</li>
<li>for: 本研究旨在适应多变量时间序列数据（MTS）领域的无监督领域适应（UDA）问题。</li>
<li>methods: 我们提出了感知对齐（SEA）方法，旨在在多感知水平（local sensor level）和全局感知水平（global sensor level）都减少频谱域差异。我们还提出了SEA++方法，通过增加高级对Alignment来提高endo-feature对齐。</li>
<li>results: 我们在公共MTS数据集上进行了广泛的实验，并证明了SEA和SEA++在MTS-UDA问题上达到了状态之arte的表现。<details>
<summary>Abstract</summary>
Unsupervised Domain Adaptation (UDA) methods have been successful in reducing label dependency by minimizing the domain discrepancy between a labeled source domain and an unlabeled target domain. However, these methods face challenges when dealing with Multivariate Time-Series (MTS) data. MTS data typically consist of multiple sensors, each with its own unique distribution. This characteristic makes it hard to adapt existing UDA methods, which mainly focus on aligning global features while overlooking the distribution discrepancies at the sensor level, to reduce domain discrepancies for MTS data. To address this issue, a practical domain adaptation scenario is formulated as Multivariate Time-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose SEnsor Alignment (SEA) for MTS-UDA, aiming to reduce domain discrepancy at both the local and global sensor levels. At the local sensor level, we design endo-feature alignment, which aligns sensor features and their correlations across domains. To reduce domain discrepancy at the global sensor level, we design exo-feature alignment that enforces restrictions on global sensor features. We further extend SEA to SEA++ by enhancing the endo-feature alignment. Particularly, we incorporate multi-graph-based high-order alignment for both sensor features and their correlations. Extensive empirical results have demonstrated the state-of-the-art performance of our SEA and SEA++ on public MTS datasets for MTS-UDA.
</details>
<details>
<summary>摘要</summary>
Unsupervised Domain Adaptation（UDA）方法已经成功地减少标签依赖性，最大化源频道和目标频道之间的频道差异。然而，这些方法在Multivariate Time-Series（MTS）数据上遇到了挑战。MTS数据通常包含多个传感器，每个传感器都有自己独特的分布。这种特点使得现有的UDA方法难以适应MTS数据，因为这些方法主要关注全局特征的对应，而忽略传感器级别的分布差异。为解决这个问题，我们提出了实用的频道适应场景——Multivariate Time-Series Unsupervised Domain Adaptation（MTS-UDA）。在这篇论文中，我们提出了感ensor Alignment（SEA）算法，旨在降低频道差异在传感器级别和全局传感器级别。在传感器级别上，我们设计了内部特征对齐，将传感器特征和它们之间的相关性在频道之间对齐。为了降低频道差异在全局传感器级别，我们设计了外部特征对齐，对全局传感器特征进行限制。我们还延展了SEA到SEA++，通过增强内部特征对齐来提高性能。具体来说，我们在endo-feature对齐中 incorporate多格基高阶对齐，以提高传感器特征和其相关性的对齐。我们在实验中证明了SEA和SEA++在公共MTS数据集上实现了领先的性能。
</details></li>
</ul>
<hr>
<h2 id="Towards-a-Standardized-Reinforcement-Learning-Framework-for-AAM-Contingency-Management"><a href="#Towards-a-Standardized-Reinforcement-Learning-Framework-for-AAM-Contingency-Management" class="headerlink" title="Towards a Standardized Reinforcement Learning Framework for AAM Contingency Management"></a>Towards a Standardized Reinforcement Learning Framework for AAM Contingency Management</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10805">http://arxiv.org/abs/2311.10805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Luis E. Alvarez, Marc W. Brittain, Kara Breeden</li>
<li>for: 这篇论文旨在探讨未来一代空中交通运输系统中的安全管理系统，以及如何通过机器学习技术来实现协调管理和自动决策。</li>
<li>methods: 这篇论文使用了Markov Decision Process（MDP）来形式化调整管理问题，并将调整管理MDPintegreated into AAM-Gym simulated environment，以便快速实现机器学习算法的测试和评估。</li>
<li>results: 论文提供了基本的统计信息和示例性能指标，以便作为未来算法开发的共同benchmark。<details>
<summary>Abstract</summary>
Advanced Air Mobility (AAM) is the next generation of air transportation that includes new entrants such as electric vertical takeoff and landing (eVTOL) aircraft, increasingly autonomous flight operations, and small UAS package delivery. With these new vehicles and operational concepts comes a desire to increase densities far beyond what occurs today in and around urban areas, to utilize new battery technology, and to move toward more autonomously-piloted aircraft. To achieve these goals, it becomes essential to introduce new safety management system capabilities that can rapidly assess risk as it evolves across a span of complex hazards and, if necessary, mitigate risk by executing appropriate contingencies via supervised or automated decision-making during flights. Recently, reinforcement learning has shown promise for real-time decision making across a wide variety of applications including contingency management. In this work, we formulate the contingency management problem as a Markov Decision Process (MDP) and integrate the contingency management MDP into the AAM-Gym simulation framework. This enables rapid prototyping of reinforcement learning algorithms and evaluation of existing systems, thus providing a community benchmark for future algorithm development. We report baseline statistical information for the environment and provide example performance metrics.
</details>
<details>
<summary>摘要</summary>
高级空中交通（AAM）是未来一代空运交通，包括新入场者如电动垂直起降（eVTOL）飞机、自动驾驶飞行操作和小型无人机快递。这些新的车辆和操作概念使得想要在今天的城市区域内增加密度，利用新的电池技术，并尝试更加自动驾驶飞机。为了实现这些目标，需要引入新的安全管理系统功能，能够快速评估飞行中的风险，并在需要时执行相应的应急措施，以确保安全飞行。在这篇文章中，我们将挑战管理问题形式化为Markov决策过程（MDP），并将它集成到AAM-Gym模拟框架中。这使得可以快速创建和评估各种启发式学习算法，并提供了一个社区标准，用于未来算法的发展。我们提供了基线统计信息，并提供了一些表现指标的示例。
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Object-Coherence-in-Layout-to-Image-Synthesis"><a href="#Enhancing-Object-Coherence-in-Layout-to-Image-Synthesis" class="headerlink" title="Enhancing Object Coherence in Layout-to-Image Synthesis"></a>Enhancing Object Coherence in Layout-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10522">http://arxiv.org/abs/2311.10522</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin</li>
<li>for: 本研究的目的是提出一种新的扩散模型，用于控制图像生成中对象的准确性和一致性。</li>
<li>methods: 该模型包括有效的全局semantic fusion（GSF）和自相似特征增强模块，用于指导对象的一致性。</li>
<li>results: 对比于传统方法，该模型能够更好地控制对象的一致性，包括semantic coherence和physical coherence。实验结果表明，该模型可以生成更高质量和更加控制性的图像。<details>
<summary>Abstract</summary>
Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and generated images, which addresses the highly relevant layout restriction and semantic coherence separately and thus leads to unsatisfying results shown in our experiments, we develop GSF to fuse the supervision from the layout restriction and semantic coherence requirement and exploit it to guide the image synthesis process. Moreover, to improve the physical coherence, we develop a Self-similarity Coherence Attention (SCA) module to explicitly integrate local contextual physical coherence into each pixel's generation process. Specifically, we adopt a self-similarity map to encode the coherence restrictions and employ it to extract coherent features from text embedding. Through visualization of our self-similarity map, we explore the essence of SCA, revealing that its effectiveness is not only in capturing reliable physical coherence patterns but also in enhancing complex texture generation. Extensive experiments demonstrate the superiority of our proposed method in both image generation quality and controllability.
</details>
<details>
<summary>摘要</summary>
layout-to-image 合成是一种在可控图像生成领域的新趋势。它的目标是生成复杂的场景，用户需要精细控制场景中对象的布局。然而，控制对象准确性仍然是一个挑战，包括semantic coherence（例如猫看到花或不）和physical coherence（例如手和racquet不能扭曲）。在这篇论文中，我们提出了一种新的扩散模型，包括有效的全局semantic fusion（GSF）和自相似特征增强模块，以导引对象准确性。 для semantic coherence，我们认为图像caption中包含了丰富的信息，用于定义图像中对象之间的semantic关系。而不是简单地在生成图像和caption之间进行交叉注意力，这会导致不满足的结果，我们开发了GSF来融合权重从layout restriction和semantic coherence的约束，并将其用于图像生成过程中。此外，为了提高physical coherence，我们开发了一个Self-similarity Coherence Attention（SCA）模块，用于 direkt地集成每个像素的生成过程中的本地上下文物理准确性。我们采用了自相似度图来编码准确性约束，并使用它来提取准确的特征从文本嵌入。通过可视化我们的自相似度图，我们探索了SCA的本质，发现它不仅能够捕捉可靠的物理准确性模式，还能够提高复杂的Texture生成。我们的实验表明，我们提出的方法在图像生成质量和可控性方面具有显著优势。
</details></li>
</ul>
<hr>
<h2 id="CNL2ASP-converting-controlled-natural-language-sentences-into-ASP"><a href="#CNL2ASP-converting-controlled-natural-language-sentences-into-ASP" class="headerlink" title="CNL2ASP: converting controlled natural language sentences into ASP"></a>CNL2ASP: converting controlled natural language sentences into ASP</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10505">http://arxiv.org/abs/2311.10505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Simone Caruso, Carmine Dodaro, Marco Maratea, Marco Mochi, Francesco Riccio</li>
<li>for: 用于将英语自然语言 sentences 翻译为 Answer Set Programming (ASP) 程序。</li>
<li>methods: 使用一种新的工具 called CNL2ASP，该工具可以将控制的自然语言 (CNL)  sentences 翻译为 ASP 规则。</li>
<li>results: 在实际应用中，CNL2ASP 可以获得良好的性能，与 ASP 专家手动编写的编码相比。<details>
<summary>Abstract</summary>
Answer Set Programming (ASP) is a popular declarative programming language for solving hard combinatorial problems. Although ASP has gained widespread acceptance in academic and industrial contexts, there are certain user groups who may find it more advantageous to employ a higher-level language that closely resembles natural language when specifying ASP programs. In this paper, we propose a novel tool, called CNL2ASP, for translating English sentences expressed in a controlled natural language (CNL) form into ASP. In particular, we first provide a definition of the type of sentences allowed by our CNL and their translation as ASP rules, and then exemplify the usage of the CNL for the specification of both synthetic and real-world combinatorial problems. Finally, we report the results of an experimental analysis conducted on the real-world problems to compare the performance of automatically generated encodings with the ones written by ASP practitioners, showing that our tool can obtain satisfactory performance on these benchmarks. Under consideration in Theory and Practice of Logic Programming (TPLP).
</details>
<details>
<summary>摘要</summary>
Answer Set Programming (ASP) 是一种流行的声明性编程语言，用于解决复杂的 combinatorial 问题。although ASP 在 academic 和 industrial 上得到了广泛的 Acceptance，有些用户群可能更喜欢使用更接近自然语言的高级语言来Specify ASP 程序。在这篇论文中，我们提出了一种新的工具，called CNL2ASP，用于将英语句子表达在控制的自然语言（CNL）形式中转化为 ASP。特别是，我们首先提供了允许的 CNL 句子类型和其转化为 ASP 规则，然后使用 CNL 来指定 both  sintetic 和 real-world  combinatorial 问题的解决方案。最后，我们进行了一个实验分析， Comparing the performance of automatically generated encodings with those written by ASP practitioners, showing that our tool can obtain satisfactory performance on these benchmarks. 在 Theory and Practice of Logic Programming (TPLP) 中进行评议。
</details></li>
</ul>
<hr>
<h2 id="A-Study-on-Altering-the-Latent-Space-of-Pretrained-Text-to-Speech-Models-for-Improved-Expressiveness"><a href="#A-Study-on-Altering-the-Latent-Space-of-Pretrained-Text-to-Speech-Models-for-Improved-Expressiveness" class="headerlink" title="A Study on Altering the Latent Space of Pretrained Text to Speech Models for Improved Expressiveness"></a>A Study on Altering the Latent Space of Pretrained Text to Speech Models for Improved Expressiveness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10804">http://arxiv.org/abs/2311.10804</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mathias Vogel</li>
<li>for: 这 paper 探讨了增强 Text-to-Speech (TTS) 模型的表达控制性的挑战，通过增加一个冻结预训练模型的 Diffusion Model，conditioned 于共同semantic audio&#x2F;text嵌入。</li>
<li>methods: 这 paper 评估了不同的 image-to-image 方法，用于在 latent speech 特征上进行修改。</li>
<li>results: 我们的结果为 future research 提供了有价值的洞察，并开启了在这个方向上的新途径。<details>
<summary>Abstract</summary>
This report explores the challenge of enhancing expressiveness control in Text-to-Speech (TTS) models by augmenting a frozen pretrained model with a Diffusion Model that is conditioned on joint semantic audio/text embeddings. The paper identifies the challenges encountered when working with a VAE-based TTS model and evaluates different image-to-image methods for altering latent speech features. Our results offer valuable insights into the complexities of adding expressiveness control to TTS systems and open avenues for future research in this direction.
</details>
<details>
<summary>摘要</summary>
Here's the translation in Simplified Chinese:这份报告研究了增强文本到语音（TTS）模型的表达控制的挑战，通过将预训练的模型加入一个基于扩散模型的增强方法，使其可以根据联合semantic audio/text嵌入来进行控制。报告描述了在VAE基于TTS模型上工作时遇到的挑战，并评估了不同的图像到图像方法以修改幂等语音特征。结果提供了关于添加表达控制到TTS系统的复杂性的有价值的洞察，并开启了未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="From-Principle-to-Practice-Vertical-Data-Minimization-for-Machine-Learning"><a href="#From-Principle-to-Practice-Vertical-Data-Minimization-for-Machine-Learning" class="headerlink" title="From Principle to Practice: Vertical Data Minimization for Machine Learning"></a>From Principle to Practice: Vertical Data Minimization for Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10500">http://arxiv.org/abs/2311.10500</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/eth-sri/datamin">https://github.com/eth-sri/datamin</a></li>
<li>paper_authors: Robin Staab, Nikola Jovanović, Mislav Balunović, Martin Vechev</li>
<li>for: 防止机器学习模型泄露private信息，实现数据最小化（DM）原则的执行。</li>
<li>methods: 提出了一种新的纵向数据最小化（vDM）工作流程，基于数据泛化，以确保在训练和部署模型时不收集任何全分辨率的客户端数据，从而保护客户端隐私。</li>
<li>results: 提出了一种基线vDM算法和隐私感知树（PAT）算法，后者在多个场景中表现出色，超过了所有基线。计划将代码公开发布为公共可用库，以促进DM原则在实际应用中的普及。<details>
<summary>Abstract</summary>
Aiming to train and deploy predictive models, organizations collect large amounts of detailed client data, risking the exposure of private information in the event of a breach. To mitigate this, policymakers increasingly demand compliance with the data minimization (DM) principle, restricting data collection to only that data which is relevant and necessary for the task. Despite regulatory pressure, the problem of deploying machine learning models that obey DM has so far received little attention. In this work, we address this challenge in a comprehensive manner. We propose a novel vertical DM (vDM) workflow based on data generalization, which by design ensures that no full-resolution client data is collected during training and deployment of models, benefiting client privacy by reducing the attack surface in case of a breach. We formalize and study the corresponding problem of finding generalizations that both maximize data utility and minimize empirical privacy risk, which we quantify by introducing a diverse set of policy-aligned adversarial scenarios. Finally, we propose a range of baseline vDM algorithms, as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that outperforms all baselines across several settings. We plan to release our code as a publicly available library, helping advance the standardization of DM for machine learning. Overall, we believe our work can help lay the foundation for further exploration and adoption of DM principles in real-world applications.
</details>
<details>
<summary>摘要</summary>
In this work, we address this challenge comprehensively. We propose a novel vertical DM (vDM) workflow based on data generalization, which ensures that no full-resolution client data is collected during training and deployment of models. This benefits client privacy by reducing the attack surface in case of a breach.We formalize and study the problem of finding generalizations that maximize data utility and minimize empirical privacy risk. We quantify this using a diverse set of policy-aligned adversarial scenarios.We propose a range of baseline vDM algorithms, as well as Privacy-aware Tree (PAT), an especially effective vDM algorithm that outperforms all baselines across several settings. We plan to release our code as a publicly available library, helping advance the standardization of DM for machine learning.Overall, we believe our work can help lay the foundation for further exploration and adoption of DM principles in real-world applications.
</details></li>
</ul>
<hr>
<h2 id="Regions-are-Who-Walk-Them-a-Large-Pre-trained-Spatiotemporal-Model-Based-on-Human-Mobility-for-Ubiquitous-Urban-Sensing"><a href="#Regions-are-Who-Walk-Them-a-Large-Pre-trained-Spatiotemporal-Model-Based-on-Human-Mobility-for-Ubiquitous-Urban-Sensing" class="headerlink" title="Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing"></a>Regions are Who Walk Them: a Large Pre-trained Spatiotemporal Model Based on Human Mobility for Ubiquitous Urban Sensing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10471">http://arxiv.org/abs/2311.10471</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ruixing Zhang, Liangzhe Han, Leilei Sun, Yunqi Liu, Jibin Wang, Weifeng Lv</li>
<li>for: 这篇论文的目的是提出一种基于 trajectory 数据的大 spacetime 模型，以提高用户 profiling 和区域分析的效率。</li>
<li>methods: 该模型使用 GPT-like 结构，并具有一个可变的参数数量为 1B。它还引入了一个时空细化模块，用于从 trajectory 数据中提取用户 embeddings。</li>
<li>results: 实验结果表明，该模型可以准确地 profiling 用户和区域，并且在 trajectory 生成任务中表现出了良好的预测能力。<details>
<summary>Abstract</summary>
User profiling and region analysis are two tasks of significant commercial value. However, in practical applications, modeling different features typically involves four main steps: data preparation, data processing, model establishment, evaluation, and optimization. This process is time-consuming and labor-intensive. Repeating this workflow for each feature results in abundant development time for tasks and a reduced overall volume of task development. Indeed, human mobility data contains a wealth of information. Several successful cases suggest that conducting in-depth analysis of population movement data could potentially yield meaningful profiles about users and areas. Nonetheless, most related works have not thoroughly utilized the semantic information within human mobility data and trained on a fixed number of the regions. To tap into the rich information within population movement, based on the perspective that Regions Are Who walk them, we propose a large spatiotemporal model based on trajectories (RAW). It possesses the following characteristics: 1) Tailored for trajectory data, introducing a GPT-like structure with a parameter count of up to 1B; 2) Introducing a spatiotemporal fine-tuning module, interpreting trajectories as collection of users to derive arbitrary region embedding. This framework allows rapid task development based on the large spatiotemporal model. We conducted extensive experiments to validate the effectiveness of our proposed large spatiotemporal model. It's evident that our proposed method, relying solely on human mobility data without additional features, exhibits a certain level of relevance in user profiling and region analysis. Moreover, our model showcases promising predictive capabilities in trajectory generation tasks based on the current state, offering the potential for further innovative work utilizing this large spatiotemporal model.
</details>
<details>
<summary>摘要</summary>
用户 profiling 和区域分析是商业值得的两项任务。然而，在实际应用中，模型不同特征通常需要四个主要步骤：数据准备、数据处理、模型建立、评估和优化。这个过程是时间consuming 和人力消耗。为了实现每个特征，需要重复这个工作流程，从而导致任务开发的庞大量和总体任务开发的减少。实际上，人口流动数据包含丰富的信息。许多成功的案例表明，对人口流动数据进行深入分析可能会获得有用的用户和区域 profiling。然而，大多数相关的工作没有全面利用人口流动数据中的 semantics 信息，并且只在固定的区域上进行训练。为了挖掘人口流动中的丰富信息，我们基于“区域是谁行走的”的视角，提出了一种大型空间时间模型（RAW）。它具有以下特点：1. 适用于轨迹数据，引入 GPT-like 结构，最多可以 Count 1B 参数。2. 引入空间时间细化模块，解释轨迹为用户集合，生成任意区域嵌入。这个框架允许基于大型空间时间模型的快速任务开发。我们进行了广泛的实验 validate 我们提议的大型空间时间模型的效果。显然，我们的提议方法， solely 基于人口流动数据而无需其他特征，在用户 profiling 和区域分析中 exhibit 一定的相关性。此外，我们的模型在 trajectory 生成任务中表现出了良好的预测能力，提供了可能进一步利用这个大型空间时间模型的机会。
</details></li>
</ul>
<hr>
<h2 id="Using-Cooperative-Game-Theory-to-Prune-Neural-Networks"><a href="#Using-Cooperative-Game-Theory-to-Prune-Neural-Networks" class="headerlink" title="Using Cooperative Game Theory to Prune Neural Networks"></a>Using Cooperative Game Theory to Prune Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10468">http://arxiv.org/abs/2311.10468</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mauricio Diaz-Ortiz Jr, Benjamin Kempinski, Daphne Cornelisse, Yoram Bachrach, Tal Kachman</li>
<li>for: 用于降低深度神经网络（DNN）的计算需求，同时保持预测精度。</li>
<li>methods: 使用游戏理论支持的方法，根据神经元对预测质量的共同影响进行减少神经网络大小，同时保持预测精度。</li>
<li>results: 比较 existing 方法，Game Theory Assisted Pruning（GTAP）在实现参数数量和模型精度之间的平衡方面表现出色。<details>
<summary>Abstract</summary>
We show how solution concepts from cooperative game theory can be used to tackle the problem of pruning neural networks.   The ever-growing size of deep neural networks (DNNs) increases their performance, but also their computational requirements. We introduce a method called Game Theory Assisted Pruning (GTAP), which reduces the neural network's size while preserving its predictive accuracy. GTAP is based on eliminating neurons in the network based on an estimation of their joint impact on the prediction quality through game theoretic solutions. Specifically, we use a power index akin to the Shapley value or Banzhaf index, tailored using a procedure similar to Dropout (commonly used to tackle overfitting problems in machine learning).   Empirical evaluation of both feedforward networks and convolutional neural networks shows that this method outperforms existing approaches in the achieved tradeoff between the number of parameters and model accuracy.
</details>
<details>
<summary>摘要</summary>
我们展示了使用合作游戏理论的解决方案来解决深度神经网络（DNNs）的压缩问题。深度神经网络的 ever-growing 大小可以提高其性能，但也增加计算需求。我们提出了一种方法called Game Theory Assisted Pruning（GTAP），它可以降低神经网络的大小，同时保持预测精度。GTAP 基于 eliminating neurons 在神经网络中，根据游戏理论解决方案估计neurons 的共同影响力。具体来说，我们使用一种力量指数，类似于 Shapley 值或 Banzhaf 指数，通过 Dropout 方法（通常用于避免过拟合问题）进行定制。我们的实验表明，GTAP 方法可以与现有方法比较，在实现参数数量和模型精度之间的平衡中具有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Accurate-and-Fast-Fischer-Tropsch-Reaction-Microkinetics-using-PINNs"><a href="#Accurate-and-Fast-Fischer-Tropsch-Reaction-Microkinetics-using-PINNs" class="headerlink" title="Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs"></a>Accurate and Fast Fischer-Tropsch Reaction Microkinetics using PINNs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10456">http://arxiv.org/abs/2311.10456</a></li>
<li>repo_url: None</li>
<li>paper_authors: Harshil Patel, Aniruddha Panda, Tymofii Nikolaienko, Stanislav Jaso, Alejandro Lopez, Kaushic Kalyanaraman</li>
<li>for: 用于模拟 Fischer-Tropsch 合成（FTS）中的化学变化。</li>
<li>methods: 使用物理学 informed neural networks（PINNs）模型FTS微谱。</li>
<li>results: 提出了一种计算高效并准确的方法，可以在现实的生产条件下解决FTS微谱模型。该模型可以准确计算含有活化剂的晶格site的比例，并且可以在GPU上运行，比传统方法快速多少。<details>
<summary>Abstract</summary>
Microkinetics allows detailed modelling of chemical transformations occurring in many industrially relevant reactions. Traditional way of solving the microkinetics model for Fischer-Tropsch synthesis (FTS) becomes inefficient when it comes to more advanced real-time applications. In this work, we address these challenges by using physics-informed neural networks(PINNs) for modelling FTS microkinetics. We propose a computationally efficient and accurate method, enabling the ultra-fast solution of the existing microkinetics models in realistic process conditions. The proposed PINN model computes the fraction of vacant catalytic sites, a key quantity in FTS microkinetics, with median relative error (MRE) of 0.03%, and the FTS product formation rates with MRE of 0.1%. Compared to conventional equation solvers, the model achieves up to 1E+06 times speed-up when running on GPUs, thus being fast enough for multi-scale and multi-physics reactor modelling and enabling its applications in real-time process control and optimization.
</details>
<details>
<summary>摘要</summary>
The proposed PINN model accurately computes the fraction of vacant catalytic sites, a key quantity in FTS microkinetics, with a median relative error (MRE) of 0.03%. Additionally, the model accurately predicts FTS product formation rates with an MRE of 0.1%. In comparison to conventional equation solvers, the PINN model achieves up to 10^6 times speed-up when running on GPUs, making it fast enough for multi-scale and multi-physics reactor modeling and enabling its applications in real-time process control and optimization.
</details></li>
</ul>
<hr>
<h2 id="Reinforcement-Learning-with-Maskable-Stock-Representation-for-Portfolio-Management-in-Customizable-Stock-Pools"><a href="#Reinforcement-Learning-with-Maskable-Stock-Representation-for-Portfolio-Management-in-Customizable-Stock-Pools" class="headerlink" title="Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools"></a>Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10801">http://arxiv.org/abs/2311.10801</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao Zhang</li>
<li>for: 本研究旨在提高套利投资（Portfolio Management）的效率和可靠性，通过使用强化学习（Reinforcement Learning）训练智能代理人。</li>
<li>methods: 本研究提出了一种名为EarnMore的强化学习框架，使用自适应掩码（Maskable Stock Representation）来处理个性化股票池（Customizable Stock Pools，CSPs）。</li>
<li>results: 经过广泛的实验 validate 了EarnMore 在 8 个不同股票池的亚集（US 股票市场）上，与 14 个基eline 之间的差异性达到 40% 以上。<details>
<summary>Abstract</summary>
Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40% improvement on profit.
</details>
<details>
<summary>摘要</summary>
PORTFOLIO管理（PM）是财务交易中的基本任务，它探索在不同股票中分配资金以实现长期收益。强化学习（RL）在最近几年内已经表现出培养财务交易的可能性，但现有的工作主要集中在固定股票池中，这与投资者的实际需求不符。特别是，投资者的目标股票池在不同的市场状况下会有很大的变化，个人投资者可能会在不同的时间内增加或减少感兴趣的股票，这导致个性化股票池（CSP）。现有的RL方法需要在股票池变化时重新训练RL代理人，这会导致计算成本高涨和性能不稳定。为解决这个挑战，我们提出了EarnMore，一个基于强化学习的投资培养框架，可以处理CSP。具体来说，我们首先引入一种机制，用于在股票外部的表示上层Masking。其次，我们通过一种自我监督的Masking和重建过程来学习有意义的股票表示。最后，我们设计了一种重要性调整机制，以便让股票组合更集中于有利股票，并忽略股票外部的表示。通过对美国股市8个子集股票池进行了广泛的实验，我们证明EarnMore在6种常见的金融指标上显著超过14种基eline的表现，增加了40%以上的利润。
</details></li>
</ul>
<hr>
<h2 id="A-Bridge-between-Dynamical-Systems-and-Machine-Learning-Engineered-Ordinary-Differential-Equations-as-Classification-Algorithm-EODECA"><a href="#A-Bridge-between-Dynamical-Systems-and-Machine-Learning-Engineered-Ordinary-Differential-Equations-as-Classification-Algorithm-EODECA" class="headerlink" title="A Bridge between Dynamical Systems and Machine Learning: Engineered Ordinary Differential Equations as Classification Algorithm (EODECA)"></a>A Bridge between Dynamical Systems and Machine Learning: Engineered Ordinary Differential Equations as Classification Algorithm (EODECA)</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10387">http://arxiv.org/abs/2311.10387</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raffaele Marino, Lorenzo Giambagli, Lorenzo Chicchi, Lorenzo Buffoni, Duccio Fanelli</li>
<li>for: 这个研究旨在 bridge machine learning 和动力系统，提高机器学习模型的解释性。</li>
<li>methods: 这篇论文提出了 Engineered Ordinary Differential Equations as Classification Algorithms (EODECAs)，这是基于连续普通微分方程的神经网络模型，具有高度的解释性和高分类性能。</li>
<li>results: EODECAs 可以提供高分类性能和自然的解释性，与传统的深度学习模型相比，它们更加透明和可解。<details>
<summary>Abstract</summary>
In a world increasingly reliant on machine learning, the interpretability of these models remains a substantial challenge, with many equating their functionality to an enigmatic black box. This study seeks to bridge machine learning and dynamical systems. Recognizing the deep parallels between dense neural networks and dynamical systems, particularly in the light of non-linearities and successive transformations, this manuscript introduces the Engineered Ordinary Differential Equations as Classification Algorithms (EODECAs). Uniquely designed as neural networks underpinned by continuous ordinary differential equations, EODECAs aim to capitalize on the well-established toolkit of dynamical systems. Unlike traditional deep learning models, which often suffer from opacity, EODECAs promise both high classification performance and intrinsic interpretability. They are naturally invertible, granting them an edge in understanding and transparency over their counterparts. By bridging these domains, we hope to usher in a new era of machine learning models where genuine comprehension of data processes complements predictive prowess.
</details>
<details>
<summary>摘要</summary>
在一个越来越依赖机器学习的世界中，机器学习模型的解释性仍然是一大挑战，许多人将其功能比作一个神秘的黑盒子。这个研究想要把机器学习和动力系统相连起来。Recognizing the deep parallels between dense neural networks and dynamical systems, particularly in the light of non-linearities and successive transformations, this manuscript introduces the Engineered Ordinary Differential Equations as Classification Algorithms (EODECAs). Uniquely designed as neural networks underpinned by continuous ordinary differential equations, EODECAs aim to capitalize on the well-established toolkit of dynamical systems. Unlike traditional deep learning models, which often suffer from opacity, EODECAs promise both high classification performance and intrinsic interpretability. They are naturally invertible, granting them an edge in understanding and transparency over their counterparts. By bridging these domains, we hope to usher in a new era of machine learning models where genuine comprehension of data processes complements predictive prowess.
</details></li>
</ul>
<hr>
<h2 id="Quantum-Data-Encoding-A-Comparative-Analysis-of-Classical-to-Quantum-Mapping-Techniques-and-Their-Impact-on-Machine-Learning-Accuracy"><a href="#Quantum-Data-Encoding-A-Comparative-Analysis-of-Classical-to-Quantum-Mapping-Techniques-and-Their-Impact-on-Machine-Learning-Accuracy" class="headerlink" title="Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy"></a>Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum Mapping Techniques and Their Impact on Machine Learning Accuracy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10375">http://arxiv.org/abs/2311.10375</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minati Rath, Hema Date</li>
<li>for: 本研究旨在把量子数据嵌入技术与经典机器学习（ML）算法结合起来，以评估性能改进和计算因素的影响。</li>
<li>methods: 我们探讨了多种经典数据编码方法，包括基准编码、角度编码和振荡编码，并对各种经典ML算法进行了广泛的实验，包括Logistic Regression、K-Nearest Neighbors、支持向量机和集成方法Like Random Forest、LightGBM、AdaBoost和CatBoost。</li>
<li>results: 我们发现，量子数据嵌入可以提高分类精度和F1分数，特别是在具有增强特征表示的模型中。我们发现在运行时间方面，低复杂度模型显示出了中等增加，而更复杂的模型则表现出了明显的变化。意外地，集成方法表现出了有利的平衡，即性能改进和计算开销之间的权衡。这种研究证明了量子数据嵌入在经典ML模型中的潜在优势，并强调在实际应用中考虑性能改进和计算成本之间的平衡。未来的研究可能包括优化量子编码过程以提高计算效率，以及探讨量子嵌入技术在实际应用中的扩展性和可扩展性。<details>
<summary>Abstract</summary>
This research explores the integration of quantum data embedding techniques into classical machine learning (ML) algorithms, aiming to assess the performance enhancements and computational implications across a spectrum of models. We explore various classical-to-quantum mapping methods, ranging from basis encoding, angle encoding to amplitude encoding for encoding classical data, we conducted an extensive empirical study encompassing popular ML algorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector Machines and ensemble methods like Random Forest, LightGBM, AdaBoost, and CatBoost. Our findings reveal that quantum data embedding contributes to improved classification accuracy and F1 scores, particularly notable in models that inherently benefit from enhanced feature representation. We observed nuanced effects on running time, with low-complexity models exhibiting moderate increases and more computationally intensive models experiencing discernible changes. Notably, ensemble methods demonstrated a favorable balance between performance gains and computational overhead. This study underscores the potential of quantum data embedding in enhancing classical ML models and emphasizes the importance of weighing performance improvements against computational costs. Future research directions may involve refining quantum encoding processes to optimize computational efficiency and exploring scalability for real-world applications. Our work contributes to the growing body of knowledge at the intersection of quantum computing and classical machine learning, offering insights for researchers and practitioners seeking to harness the advantages of quantum-inspired techniques in practical scenarios.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Dates-Fruit-Disease-Recognition-using-Machine-Learning"><a href="#Dates-Fruit-Disease-Recognition-using-Machine-Learning" class="headerlink" title="Dates Fruit Disease Recognition using Machine Learning"></a>Dates Fruit Disease Recognition using Machine Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10365">http://arxiv.org/abs/2311.10365</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ghassen Ben Brahim, Jaafar Alghazo, Ghazanfar Latif, Khalid Alnujaidi</li>
<li>for: 这项研究的目的是提出一种自动检测日tto fruit疾病的集成解决方案，以提高日tto fruit生产的效率和质量。</li>
<li>methods: 该研究使用了计算机视觉、机器学习、无人机技术等新技术，提出了一种基于L<em>a</em>b颜色特征、统计特征和DWT纹理特征的混合特征提取方法，以早期检测和识别日tto fruit疾病。</li>
<li>results: 研究发现，将L<em>a</em>b、统计和DWT特征结合使用，可以提高检测精度和综合性，并且在871个图像中实现了95.2%的平均准确率。<details>
<summary>Abstract</summary>
Many countries such as Saudi Arabia, Morocco and Tunisia are among the top exporters and consumers of palm date fruits. Date fruit production plays a major role in the economies of the date fruit exporting countries. Date fruits are susceptible to disease just like any fruit and early detection and intervention can end up saving the produce. However, with the vast farming lands, it is nearly impossible for farmers to observe date trees on a frequent basis for early disease detection. In addition, even with human observation the process is prone to human error and increases the date fruit cost. With the recent advances in computer vision, machine learning, drone technology, and other technologies; an integrated solution can be proposed for the automatic detection of date fruit disease. In this paper, a hybrid features based method with the standard classifiers is proposed based on the extraction of L*a*b color features, statistical features, and Discrete Wavelet Transform (DWT) texture features for the early detection and classification of date fruit disease. A dataset was developed for this work consisting of 871 images divided into the following classes; Healthy date, Initial stage of disease, Malnourished date, and Parasite infected. The extracted features were input to common classifiers such as the Random Forest (RF), Multilayer Perceptron (MLP), Na\"ive Bayes (NB), and Fuzzy Decision Trees (FDT). The highest average accuracy was achieved when combining the L*a*b, Statistical, and DWT Features.
</details>
<details>
<summary>摘要</summary>
许多国家如沙特阿拉伯、摩洛哥和突尼斯等是dates果实的主要出口国和消费国。dates果实的生产对出口国经济发展具有重要作用。然而，由于庞大的农业地域，农民无法在一定时间内频繁地检查dates Tree，因此早期病诊断和 intervención可能会增加dates fruit的成本。在现代计算机视觉、机器学习、无人机技术等的支持下，一个整合的解决方案可以提议用于自动检测dates fruit病诊断。在这篇论文中，一种基于混合特征的方法被提议，该方法基于L*a*b颜色特征、统计特征和Discrete Wavelet Transform（DWT）Texture特征进行早期检测和分类dates fruit病诊断。一个数据集被开发用于这项工作，该数据集包括871张图像，分为以下类别：健康的dates，病诊断的初期阶段，营养不良的dates和受到寄生虫感染的。提取的特征被输入到常见的分类器 such as Random Forest（RF）、Multilayer Perceptron（MLP）、Na\"ive Bayes（NB）和Fuzzy Decision Trees（FDT）。结果表明，将L*a*b、统计和DWT特征结合使用时，获得了最高的平均准确率。
</details></li>
</ul>
<hr>
<h2 id="Quantum-Assisted-Simulation-A-Framework-for-Designing-Machine-Learning-Models-in-the-Quantum-Computing-Domain"><a href="#Quantum-Assisted-Simulation-A-Framework-for-Designing-Machine-Learning-Models-in-the-Quantum-Computing-Domain" class="headerlink" title="Quantum-Assisted Simulation: A Framework for Designing Machine Learning Models in the Quantum Computing Domain"></a>Quantum-Assisted Simulation: A Framework for Designing Machine Learning Models in the Quantum Computing Domain</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10363">http://arxiv.org/abs/2311.10363</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minati Rath, Hema Date</li>
<li>for: 这篇论文主要针对的是使用量子计算（QC）技术加速机器学习（ML）模型的训练，以提高数据处理效率和准确率。</li>
<li>methods: 本论文使用了量子机器学习（QML）算法，将传统的机器学习算法映射到量子机制域中，以实现对大数据的加速处理。</li>
<li>results: 通过对一个数据集使用机器学习和量子机器学习两种方法进行比较，研究发现量子机器学习方法可以提高数据处理效率和准确率。<details>
<summary>Abstract</summary>
Machine learning (ML) models are trained using historical data to classify new, unseen data. However, traditional computing resources often struggle to handle the immense amount of data, commonly known as Big Data, within a reasonable timeframe. Quantum computing (QC) provides a novel approach to information processing. Quantum algorithms have the potential to process classical data exponentially faster than classical computing. By mapping quantum machine learning (QML) algorithms into the quantum mechanical domain, we can potentially achieve exponential improvements in data processing speed, reduced resource requirements, and enhanced accuracy and efficiency. In this article, we delve into both the QC and ML fields, exploring the interplay of ideas between them, as well as the current capabilities and limitations of hardware. We investigate the history of quantum computing, examine existing QML algorithms, and aim to present a simplified procedure for setting up simulations of QML algorithms, making it accessible and understandable for readers. Furthermore, we conducted simulations on a dataset using both machine learning and quantum machine learning approaches. We then proceeded to compare their respective performances by utilizing a quantum simulator.
</details>
<details>
<summary>摘要</summary>
机器学习（ML）模型通过历史数据来分类新的、未经见过的数据。然而，传统计算资源经常无法处理大量数据，通常被称为大数据，在合理的时间framworks内进行处理。量子计算（QC）提供了一种新的信息处理方法。量子算法有能力在经典计算中进行批处理，并且可以在批处理中实现对数据的快速分类。通过将量子机器学习（QML）算法映射到量子机械领域，我们可以实现对数据的快速处理，降低资源需求，提高准确率和效率。在这篇文章中，我们将探讨QC和ML两个领域之间的交互，以及现有硬件的能力和限制。我们还会investigate量子计算的历史，检查现有的QML算法，并尝试提供一个简化的程序来设置QML算法的 simulations，使其更加可读性和可理解性。此外，我们还进行了一个数据集的simulation，使用了机器学习和量子机器学习两种方法。然后，我们对这两种方法的性能进行了比较，使用了量子模拟器。
</details></li>
</ul>
<hr>
<h2 id="INSPECT-A-Multimodal-Dataset-for-Pulmonary-Embolism-Diagnosis-and-Prognosis"><a href="#INSPECT-A-Multimodal-Dataset-for-Pulmonary-Embolism-Diagnosis-and-Prognosis" class="headerlink" title="INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis"></a>INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10798">http://arxiv.org/abs/2311.10798</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shih-Cheng Huang, Zepeng Huo, Ethan Steinberg, Chia-Chun Chiang, Matthew P. Lungren, Curtis P. Langlotz, Serena Yeung, Nigam H. Shah, Jason A. Fries</li>
<li>for: 这个论文的目的是为了提供一个基准数据集，用于评估多modal的医学应用程序的性能。</li>
<li>methods: 这个论文使用了多modal的医学数据集，包括CT图像、辐射报告和结构化电子医疗记录数据。</li>
<li>results: 这个论文提出了一个基准数据集，用于评估多modal的医学模型的性能。这个数据集包括19,402名病人的数据，包括CT图像、辐射报告和结构化电子医疗记录数据。<details>
<summary>Abstract</summary>
Synthesizing information from multiple data sources plays a crucial role in the practice of modern medicine. Current applications of artificial intelligence in medicine often focus on single-modality data due to a lack of publicly available, multimodal medical datasets. To address this limitation, we introduce INSPECT, which contains de-identified longitudinal records from a large cohort of patients at risk for pulmonary embolism (PE), along with ground truth labels for multiple outcomes. INSPECT contains data from 19,402 patients, including CT images, radiology report impression sections, and structured electronic health record (EHR) data (i.e. demographics, diagnoses, procedures, vitals, and medications). Using INSPECT, we develop and release a benchmark for evaluating several baseline modeling approaches on a variety of important PE related tasks. We evaluate image-only, EHR-only, and multimodal fusion models. Trained models and the de-identified dataset are made available for non-commercial use under a data use agreement. To the best of our knowledge, INSPECT is the largest multimodal dataset integrating 3D medical imaging and EHR for reproducible methods evaluation and research.
</details>
<details>
<summary>摘要</summary>
现代医学中合并信息 FROM多种数据源的Synthesizing plays a crucial role。Current applications of artificial intelligence in medicine often focus on single-modality data due to a lack of publicly available, multimodal medical datasets。To address this limitation, we introduce INSPECT，which contains de-identified longitudinal records from a large cohort of patients at risk for pulmonary embolism (PE), along with ground truth labels for multiple outcomes。INSPECT contains data from 19,402 patients，including CT images，radiology report impression sections，and structured electronic health record (EHR) data（i.e. demographics，diagnoses，procedures，vitals，and medications）。Using INSPECT，we develop and release a benchmark for evaluating several baseline modeling approaches on a variety of important PE related tasks。We evaluate image-only，EHR-only，and multimodal fusion models。Trained models and the de-identified dataset are made available for non-commercial use under a data use agreement。To the best of our knowledge，INSPECT is the largest multimodal dataset integrating 3D medical imaging and EHR for reproducible methods evaluation and research。
</details></li>
</ul>
<hr>
<h2 id="TaCo-Enhancing-Cross-Lingual-Transfer-for-Low-Resource-Languages-in-LLMs-through-Translation-Assisted-Chain-of-Thought-Processes"><a href="#TaCo-Enhancing-Cross-Lingual-Transfer-for-Low-Resource-Languages-in-LLMs-through-Translation-Assisted-Chain-of-Thought-Processes" class="headerlink" title="TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes"></a>TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in LLMs through Translation-Assisted Chain-of-Thought Processes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10797">http://arxiv.org/abs/2311.10797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bibek Upadhayay, Vahid Behzadan</li>
<li>for: 提出了一种cost-effective的解决方案，以便在多语言 Setting中训练和调节LLMs。</li>
<li>methods: 提出了一种名为“TaCo：翻译帮助交叉语言”的新方法，该方法利用翻译进行链式思考过程，以实现在新语言上进行 instrucion-tuning LLMs。</li>
<li>results: 对于三种低资源语言和一种高资源语言进行了进一步的 instrucion-tuning，并通过比较result表明，TaCo方法可以提高GPT-4的性能， especailly for low-resource languages。<details>
<summary>Abstract</summary>
LLMs such as ChatGPT and PaLM can be utilized to train on a new language and revitalize low-resource languages. However, it is evidently very costly to pretrain pr fine-tune LLMs to adopt new languages. Another challenge is the limitation of benchmark datasets and the metrics used to measure the performance of models in multilingual settings. This paper proposes cost-effective solutions to both of the aforementioned challenges. We introduce the Multilingual Instruction-Tuning Dataset (MITS), which is comprised of the translation of Alpaca-52K, Dolly-15K, and Vicuna Benchmark in 132 languages. Also, we propose a new method called \emph{TaCo: Translation-Assisted Cross-Linguality}, which make uses of translation in a chain-of-thought process to instruction-tune LLMs on a new languages through a curriculum learning process. As a proof of concept, we experimented with the instruction-tuned Guanaco-33B model and performed further instruction tuning using the TaCo method in three low-resource languages and one high-resource language. Our results show that the TaCo method impresses the GPT-4 with 82% for a low-resource language in the Vicuna Benchmark dataset, and boosts performance by double in contrast to the performance of instruction tuning only. Our results show that TaCo is a promising method for creating multilingual LLMs, even for low-resource languages. We have released our datasets and the model adapters, and encourage the research community to make use of these resources towards advancing work on multilingual LLMs.
</details>
<details>
<summary>摘要</summary>
LLMs 如 ChatGPT 和 PaLM 可以用来训练新语言并恢复低资源语言。然而，训练或精度调整 LLMs 以采用新语言明显很昂贵。另外，评价模型在多语言设置下的表现也受到限制。这篇论文提出了经济的解决方案。我们引入了多语言指导集（MITS），其包括了 Alpaca-52K、Dolly-15K 和 Vicuna Benchmark 的翻译版本，涵盖 132 种语言。此外，我们提出了一种新方法 called “TaCo：翻译协助跨语言”，它利用翻译的链条过程来帮助 LLMs 在新语言上进行指导调整。作为证明，我们在 Guanaco-33B 模型上进行了进一步的指导调整，并使用 TaCo 方法在三种低资源语言和一种高资源语言进行了实验。我们的结果表明，TaCo 方法可以在 Vicuna Benchmark 数据集中让 GPT-4 的表现提高至 82%，并在低资源语言中提高表现的两倍。我们的结果表明，TaCo 是一种有前途的方法，可以创造多语言 LLMs，即使是低资源语言。我们已经发布了数据集和模型适配器，并鼓励研究社区使用这些资源来推进多语言 LLMs 的研究。
</details></li>
</ul>
<hr>
<h2 id="Federated-Knowledge-Graph-Completion-via-Latent-Embedding-Sharing-and-Tensor-Factorization"><a href="#Federated-Knowledge-Graph-Completion-via-Latent-Embedding-Sharing-and-Tensor-Factorization" class="headerlink" title="Federated Knowledge Graph Completion via Latent Embedding Sharing and Tensor Factorization"></a>Federated Knowledge Graph Completion via Latent Embedding Sharing and Tensor Factorization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10341">http://arxiv.org/abs/2311.10341</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maolin Wang, Dun Zeng, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao</li>
<li>for: 这篇论文主要用于提出一种新的 federated tensor factorization 方法，用于解决分布式知识图（KG）完成任务中的隐私问题。</li>
<li>methods: 该方法使用 federated tensor factorization 技术，包括嵌入矩阵分解和秘密词汇共享，以降低隐私风险。</li>
<li>results: 实验结果表明，FLEST 方法可以具有较高的效率和隐私保护，同时也能够保持完成任务的性能。<details>
<summary>Abstract</summary>
Knowledge graphs (KGs), which consist of triples, are inherently incomplete and always require completion procedure to predict missing triples. In real-world scenarios, KGs are distributed across clients, complicating completion tasks due to privacy restrictions. Many frameworks have been proposed to address the issue of federated knowledge graph completion. However, the existing frameworks, including FedE, FedR, and FEKG, have certain limitations. = FedE poses a risk of information leakage, FedR's optimization efficacy diminishes when there is minimal overlap among relations, and FKGE suffers from computational costs and mode collapse issues. To address these issues, we propose a novel method, i.e., Federated Latent Embedding Sharing Tensor factorization (FLEST), which is a novel approach using federated tensor factorization for KG completion. FLEST decompose the embedding matrix and enables sharing of latent dictionary embeddings to lower privacy risks. Empirical results demonstrate FLEST's effectiveness and efficiency, offering a balanced solution between performance and privacy. FLEST expands the application of federated tensor factorization in KG completion tasks.
</details>
<details>
<summary>摘要</summary>
知识图（KG）是一种嵌入式的三元组，总是缺失一些 triple，需要完成过程来预测缺失的 triple。在实际应用中，KG 分布在客户端上，因此完成任务变得更加复杂，因为隐私限制。许多框架已经被提出来解决联邦知识图完成任务中的问题，包括 FedE、FedR 和 FEKG，但是这些框架都有一些局限性。FedE 可能会导致信息泄露，FedR 的优化效果随着关系的重叠度下降，而 FEKG 则受到计算成本和模式塌 collapse 问题的限制。为了解决这些问题，我们提出了一种新的方法，即联邦隐藏嵌入分解 tensor factorization (FLEST)，它使用联邦tensor factorization来完成KG completion任务。FLEST 将嵌入矩阵分解成多个独立的嵌入矩阵，从而降低了隐私风险。我们的实验结果表明，FLEST 具有较高的效果和效率，可以均衡性和隐私之间的 contradiction。FLEST 扩展了联邦tensor factorization在KG completion任务中的应用范围。
</details></li>
</ul>
<hr>
<h2 id="Emotion-Aware-Music-Recommendation-System-Enhancing-User-Experience-Through-Real-Time-Emotional-Context"><a href="#Emotion-Aware-Music-Recommendation-System-Enhancing-User-Experience-Through-Real-Time-Emotional-Context" class="headerlink" title="Emotion-Aware Music Recommendation System: Enhancing User Experience Through Real-Time Emotional Context"></a>Emotion-Aware Music Recommendation System: Enhancing User Experience Through Real-Time Emotional Context</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10796">http://arxiv.org/abs/2311.10796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tina Babu, Rekha R Nair, Geetha A</li>
<li>for: 这项研究旨在改进传统的音乐推荐系统，强调用户的情感因素在音乐选择中的重要性。</li>
<li>methods: 该研究提出了一种基于人工智能的音乐推荐模型，通过准确检测用户当前情感状态，为用户提供个性化的音乐推荐。</li>
<li>results: 该模型可以增强用户音乐经验，为用户提供与当前情感状态相符的音乐推荐，从而创造更有意义和有感的听众体验。<details>
<summary>Abstract</summary>
This study addresses the deficiency in conventional music recommendation systems by focusing on the vital role of emotions in shaping users music choices. These systems often disregard the emotional context, relying predominantly on past listening behavior and failing to consider the dynamic and evolving nature of users emotional preferences. This gap leads to several limitations. Users may receive recommendations that do not match their current mood, which diminishes the quality of their music experience. Furthermore, without accounting for emotions, the systems might overlook undiscovered or lesser-known songs that have a profound emotional impact on users. To combat these limitations, this research introduces an AI model that incorporates emotional context into the song recommendation process. By accurately detecting users real-time emotions, the model can generate personalized song recommendations that align with the users emotional state. This approach aims to enhance the user experience by offering music that resonates with their current mood, elicits the desired emotions, and creates a more immersive and meaningful listening experience. By considering emotional context in the song recommendation process, the proposed model offers an opportunity for a more personalized and emotionally resonant musical journey.
</details>
<details>
<summary>摘要</summary>
To address these limitations, this research introduces an AI model that incorporates emotional context into the song recommendation process. By accurately detecting users' real-time emotions, the model can generate personalized song recommendations that align with the users' emotional state. This approach aims to enhance the user experience by offering music that resonates with their current mood, elicits the desired emotions, and creates a more immersive and meaningful listening experience.By considering emotional context in the song recommendation process, the proposed model offers an opportunity for a more personalized and emotionally resonant musical journey. This study has the potential to revolutionize the music industry by providing users with a more tailored and emotionally satisfying experience, ultimately leading to a more engaging and fulfilling listening experience.
</details></li>
</ul>
<hr>
<h2 id="High-fidelity-Person-centric-Subject-to-Image-Synthesis"><a href="#High-fidelity-Person-centric-Subject-to-Image-Synthesis" class="headerlink" title="High-fidelity Person-centric Subject-to-Image Synthesis"></a>High-fidelity Person-centric Subject-to-Image Synthesis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10329">http://arxiv.org/abs/2311.10329</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin</li>
<li>for: 这 paper written for improving the quality of person-centric image generation, specifically addressing the challenges of training imbalance and quality compromise in current subject-driven image generation methods.</li>
<li>methods: 这 paper propose a collaborative generation pipeline called Face-diffuser, which consists of two specialized pre-trained diffusion models (TDM and SDM) and a novel mechanism called Saliency-adaptive Noise Fusion (SNF) to eliminate training imbalance and quality compromise.</li>
<li>results: 该 paper achieve impressive effectiveness and robustness in person-centric image generation, with extensive experiments confirming the improved performance of Face-diffuser over existing methods.<details>
<summary>Abstract</summary>
Current subject-driven image generation methods encounter significant challenges in person-centric image generation. The reason is that they learn the semantic scene and person generation by fine-tuning a common pre-trained diffusion, which involves an irreconcilable training imbalance. Precisely, to generate realistic persons, they need to sufficiently tune the pre-trained model, which inevitably causes the model to forget the rich semantic scene prior and makes scene generation over-fit to the training data. Moreover, even with sufficient fine-tuning, these methods can still not generate high-fidelity persons since joint learning of the scene and person generation also lead to quality compromise. In this paper, we propose Face-diffuser, an effective collaborative generation pipeline to eliminate the above training imbalance and quality compromise. Specifically, we first develop two specialized pre-trained diffusion models, i.e., Text-driven Diffusion Model (TDM) and Subject-augmented Diffusion Model (SDM), for scene and person generation, respectively. The sampling process is divided into three sequential stages, i.e., semantic scene construction, subject-scene fusion, and subject enhancement. The first and last stages are performed by TDM and SDM respectively. The subject-scene fusion stage, that is the collaboration achieved through a novel and highly effective mechanism, Saliency-adaptive Noise Fusion (SNF). Specifically, it is based on our key observation that there exists a robust link between classifier-free guidance responses and the saliency of generated images. In each time step, SNF leverages the unique strengths of each model and allows for the spatial blending of predicted noises from both models automatically in a saliency-aware manner. Extensive experiments confirm the impressive effectiveness and robustness of the Face-diffuser.
</details>
<details>
<summary>摘要</summary>
现有的主题驱动图像生成方法遇到了人центric图像生成中的显著挑战。原因在于它们通过精度调整一个共同预训练的扩散来学习 semantic scene和人类生成，而这种培训不匹配导致模型忘记了丰富的 semantic scene prior，从而导致场景生成过拟合训练数据。此外，即使充分调整，这些方法仍然无法生成高效的人类，因为共同学习场景和人类生成也会导致质量牺牲。在这篇论文中，我们提出了Face-diffuser，一种有效的合作生成管线，以消除以上培训不匹配和质量牺牲。具体来说，我们首先开发了两个特殊的预训练扩散模型，即 Text-driven Diffusion Model (TDM) 和 Subject-augmented Diffusion Model (SDM)，用于场景和人类生成。采样过程分为三个顺序阶段：semantic scene construction、subject-scene fusion 和 subject enhancement。第一个和最后一个阶段由 TDM 和 SDM 执行。subject-scene fusion 阶段通过一种新的、非常有效的机制——Saliency-adaptive Noise Fusion (SNF) 实现了合作。具体来说，SNF 基于我们关键观察到的，在生成图像中存在稳定的静止特征的robust链接。在每次迭代中，SNF 利用每个模型的独特优势，自动在一种saliency-aware的方式进行空间混合预测噪音。广泛的实验证明了Face-diffuser 的强大效果和稳定性。
</details></li>
</ul>
<hr>
<h2 id="Clustering-Techniques-for-Stable-Linear-Dynamical-Systems-with-applications-to-Hard-Disk-Drives"><a href="#Clustering-Techniques-for-Stable-Linear-Dynamical-Systems-with-applications-to-Hard-Disk-Drives" class="headerlink" title="Clustering Techniques for Stable Linear Dynamical Systems with applications to Hard Disk Drives"></a>Clustering Techniques for Stable Linear Dynamical Systems with applications to Hard Disk Drives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10322">http://arxiv.org/abs/2311.10322</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Potu Surya Prakash, Joohwan Seo, Jongeun Choi, Roberto Horowitz</li>
<li>for: 这个论文是为了设计对多个植入函数或家族植入函数的稳定控制器而写的。</li>
<li>methods: 这篇论文使用了 clustering 技术来分组稳定线性动力系统，以便设计每个群组内的优化控制器。</li>
<li>results: 该论文提出了基于 k-medoids 算法的硬 clustering 方法和基于 Gaussian Mixture Models 的特殊类 LTI 系统 clustering 方法，以便设计对每个群组内的优化控制器。<details>
<summary>Abstract</summary>
In Robust Control and Data Driven Robust Control design methodologies, multiple plant transfer functions or a family of transfer functions are considered and a common controller is designed such that all the plants that fall into this family are stabilized. Though the plants are stabilized, the controller might be sub-optimal for each of the plants when the variations in the plants are large. This paper presents a way of clustering stable linear dynamical systems for the design of robust controllers within each of the clusters such that the controllers are optimal for each of the clusters. First a k-medoids algorithm for hard clustering will be presented for stable Linear Time Invariant (LTI) systems and then a Gaussian Mixture Models (GMM) clustering for a special class of LTI systems, common for Hard Disk Drive plants, will be presented.
</details>
<details>
<summary>摘要</summary>
在Robust控制和数据驱动Robust控制设计方法中，考虑多个植入函数或一家植入函数家族，设计一个通用控制器，使得所有fall into这个家族的植入函数都稳定。虽然植入函数都稳定，但控制器可能对每个植入函数进行优化。这篇文章介绍了稳定线性动力系统集群的方法，以设计内部的优化控制器。首先，将介绍k-medians算法 для硬 clustering稳定线性时间不变（LTI）系统，然后介绍Gaussian Mixture Models（GMM）集群方法，专门适用于硬盘植入系统。
</details></li>
</ul>
<hr>
<h2 id="Shifting-to-Machine-Supervision-Annotation-Efficient-Semi-and-Self-Supervised-Learning-for-Automatic-Medical-Image-Segmentation-and-Classification"><a href="#Shifting-to-Machine-Supervision-Annotation-Efficient-Semi-and-Self-Supervised-Learning-for-Automatic-Medical-Image-Segmentation-and-Classification" class="headerlink" title="Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification"></a>Shifting to Machine Supervision: Annotation-Efficient Semi and Self-Supervised Learning for Automatic Medical Image Segmentation and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10319">http://arxiv.org/abs/2311.10319</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone</li>
<li>for: 提高临床治疗和研究的前进受到了监督学习技术的限制，这些技术需要大量的标注数据，并且需要许多临床专家的时间。</li>
<li>methods: 本文提出使用自监学习和半监学习技术，这些技术可以进行无标签任务，可以轻松地扩大机器监督的覆盖率，相比完全监督技术更加容易。</li>
<li>results: 本文提出的S4MI（自监学习和半监学习 для医疗影像）管道可以在三个医疗影像数据集上进行分类和 segmentation 的检验，并且在大多数数据集中，使用10%的标注 perfomed 更好于100%的标注。半监学习方法在 segmentation 中取得了良好的效果，使用50% fewer labels 在所有三个数据集中都超过了完全监督方法。<details>
<summary>Abstract</summary>
Advancements in clinical treatment and research are limited by supervised learning techniques that rely on large amounts of annotated data, an expensive task requiring many hours of clinical specialists' time. In this paper, we propose using self-supervised and semi-supervised learning. These techniques perform an auxiliary task that is label-free, scaling up machine-supervision is easier compared with fully-supervised techniques. This paper proposes S4MI (Self-Supervision and Semi-Supervision for Medical Imaging), our pipeline to leverage advances in self and semi-supervision learning. We benchmark them on three medical imaging datasets to analyze their efficacy for classification and segmentation. This advancement in self-supervised learning with 10% annotation performed better than 100% annotation for the classification of most datasets. The semi-supervised approach yielded favorable outcomes for segmentation, outperforming the fully-supervised approach by using 50% fewer labels in all three datasets.
</details>
<details>
<summary>摘要</summary>
临床治疗和研究的进步受到监督学习技术的限制，这些技术需要大量的注解数据，需要许多临床专家的时间和努力。在这篇论文中，我们提议使用自监学习和半监学习技术。这些技术可以完成无标签任务，比完全监学技术更容易扩大机器监督。我们提出S4MI（自监学和半监学医学影像处理管道），我们的管道可以利用自监学和半监学技术来提高医学影像处理的效果。我们在三个医学影像数据集上对S4MI进行了比较，发现在大多数数据集中，自监学技术比100%注解技术表现更好，而半监学技术在所有三个数据集中对segmentation问题表现更好，只需使用50%的标签数据。
</details></li>
</ul>
<hr>
<h2 id="Supervised-structure-learning"><a href="#Supervised-structure-learning" class="headerlink" title="Supervised structure learning"></a>Supervised structure learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10300">http://arxiv.org/abs/2311.10300</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/alteryx/compose">https://github.com/alteryx/compose</a></li>
<li>paper_authors: Karl J. Friston, Lancelot Da Costa, Alexander Tschantz, Alex Kiefer, Tommaso Salvatori, Victorita Neacsu, Magnus Koudahl, Conor Heins, Noor Sajid, Dimitrije Markovic, Thomas Parr, Tim Verbelen, Christopher L Buckley</li>
<li>for: 这篇论文关注于构造学习或发现抽象生成模型。它强调 bayesian 模型选择和训练数据或内容的吸收，尤其是数据的顺序排序。</li>
<li>methods: 该论文使用 bayesian 模型选择，基于预先知道的结果（即偏好结果）来设置约束。在这种设置下，预期自由能量降为受约束的共识信息。</li>
<li>results: 在 MNIST  dataset 上进行图像分类示例中，该方法可以快速地学习抽象生成模型。在一个更加复杂的问题中，使用一种简单的 sprite-based 视觉分解问题和 Tower of Hanoi 问题，生成模型可以自动地恢复（即分离）积极状态的因素结构和特征路径或动态。<details>
<summary>Abstract</summary>
This paper concerns structure learning or discovery of discrete generative models. It focuses on Bayesian model selection and the assimilation of training data or content, with a special emphasis on the order in which data are ingested. A key move - in the ensuing schemes - is to place priors on the selection of models, based upon expected free energy. In this setting, expected free energy reduces to a constrained mutual information, where the constraints inherit from priors over outcomes (i.e., preferred outcomes). The resulting scheme is first used to perform image classification on the MNIST dataset to illustrate the basic idea, and then tested on a more challenging problem of discovering models with dynamics, using a simple sprite-based visual disentanglement paradigm and the Tower of Hanoi (cf., blocks world) problem. In these examples, generative models are constructed autodidactically to recover (i.e., disentangle) the factorial structure of latent states - and their characteristic paths or dynamics.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "discrete generative models" 转为 "整数生成模型"* "Bayesian model selection" 转为 " Bayesian 模型选择"* "assimilation of training data or content" 转为 "训练数据或内容的吸收"* "priors on the selection of models" 转为 "模型选择的先验"* "expected free energy" 转为 "预期的自由能"* "constrained mutual information" 转为 "约束的共同信息"* "outcomes" 转为 "结果"* "preferred outcomes" 转为 "偏好的结果"* "autodidactically" 转为 "自学习地"
</details></li>
</ul>
<hr>
<h2 id="Traffic-Sign-Interpretation-in-Real-Road-Scene"><a href="#Traffic-Sign-Interpretation-in-Real-Road-Scene" class="headerlink" title="Traffic Sign Interpretation in Real Road Scene"></a>Traffic Sign Interpretation in Real Road Scene</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10793">http://arxiv.org/abs/2311.10793</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chuang Yang, Kai Zhuang, Mulin Chen, Haozhao Ma, Xu Han, Tao Han, Changxing Guo, Han Han, Bingxuan Zhao, Qi Wang</li>
<li>for: 本研究旨在提供高精度的自动驾驶或助手驾驶支持，通过解决现有的交通标志检测和识别问题。</li>
<li>methods: 本研究提出了交通标志解释（TSI）任务，用于解释交通标志的全球 semantics 逻辑，并将其转换为自然语言，以提供高精度的交通指导。同时，我们设计了一种多任务学习架构，用于实现 TSI 任务，包括交通标志检测和识别以及交通标志的自然语言解释。</li>
<li>results: 我们在 TSI-CN 数据集上进行了实验，并证明了 TSI 任务的可行性，并且 TSI 架构可以从场景中成功地解释交通标志，即使存在复杂的 semantics 逻辑。<details>
<summary>Abstract</summary>
Most existing traffic sign-related works are dedicated to detecting and recognizing part of traffic signs individually, which fails to analyze the global semantic logic among signs and may convey inaccurate traffic instruction. Following the above issues, we propose a traffic sign interpretation (TSI) task, which aims to interpret global semantic interrelated traffic signs (e.g.,~driving instruction-related texts, symbols, and guide panels) into a natural language for providing accurate instruction support to autonomous or assistant driving. Meanwhile, we design a multi-task learning architecture for TSI, which is responsible for detecting and recognizing various traffic signs and interpreting them into a natural language like a human. Furthermore, the absence of a public TSI available dataset prompts us to build a traffic sign interpretation dataset, namely TSI-CN. The dataset consists of real road scene images, which are captured from the highway and the urban way in China from a driver's perspective. It contains rich location labels of texts, symbols, and guide panels, and the corresponding natural language description labels. Experiments on TSI-CN demonstrate that the TSI task is achievable and the TSI architecture can interpret traffic signs from scenes successfully even if there is a complex semantic logic among signs. The TSI-CN dataset and the source code of the TSI architecture will be publicly available after the revision process.
</details>
<details>
<summary>摘要</summary>
现有的交通标志相关工作都是专注于分别检测和识别交通标志，而忽略了交通标志之间的全球semantic逻辑，可能导致不准确的交通指示。为了解决这些问题，我们提出了交通标志解释（TSI）任务，该任务旨在将交通标志转化为自然语言，以提供准确的交通指示支持给自动驾驶或助手驾驶。同时，我们设计了一种多任务学习架构 для TSI，该架构负责检测和识别各种交通标志，并将它们转化为自然语言，类似于人类。由于没有公开的TSI可用数据集，我们建立了交通标志解释数据集（TSI-CN）。该数据集包括真实的路面场景图像， captured from 高速公路和城市道路在中国 driver's 视角。它包含了文本、符号和引导板的准确位置标签，以及相应的自然语言描述标签。实验表明，TSI任务是可行的，并且TSI架构可以从场景中成功地解释交通标志，即使存在复杂的semantic逻辑。TSI-CN数据集和TSI架构的源代码将在修订过程后公开。
</details></li>
</ul>
<hr>
<h2 id="Attention-Mechanism-for-Lithium-Ion-Battery-Lifespan-Prediction-Temporal-and-Cyclic-Attention"><a href="#Attention-Mechanism-for-Lithium-Ion-Battery-Lifespan-Prediction-Temporal-and-Cyclic-Attention" class="headerlink" title="Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention"></a>Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10792">http://arxiv.org/abs/2311.10792</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaewook Lee, Seongmin Heo, Jay H. Lee</li>
<li>for: 预测锂离子电池（LIB）寿命，以便优化使用和避免事故。</li>
<li>methods: employs attention mechanisms（AM）construct data-driven models for predicting LIB lifespan using easily measurable inputs such as voltage, current, temperature, and capacity data.</li>
<li>results: 1) 通过使用时间注意力（TA）和循环注意力（CA），提高预测精度和描述输入数据中关键特征。2) 计算TA scores highlights the rest phase as a key characteristic distinguishing LIB data among different batches.3)  CA scores reveal variations in the importance of cycles across batches, and the potential to reduce the number of cycles in the input data.<details>
<summary>Abstract</summary>
Accurately predicting the lifespan of lithium-ion batteries (LIBs) is pivotal for optimizing usage and preventing accidents. Previous studies in constructing prediction models often relied on inputs challenging to measure in real-time operations and failed to capture intra-cycle and inter-cycle data patterns, essential features for accurate predictions, comprehensively. In this study, we employ attention mechanisms (AM) to develop data-driven models for predicting LIB lifespan using easily measurable inputs such as voltage, current, temperature, and capacity data. The developed model integrates recurrent neural network (RNN) and convolutional neural network (CNN) components, featuring two types of attention mechanisms: temporal attention (TA) and cyclic attention (CA). The inclusion of TA aims to identify important time steps within each cycle by scoring the hidden states of the RNN, whereas CA strives to capture key features of inter-cycle correlations through self-attention (SA). This enhances model accuracy and elucidates critical features in the input data. To validate our method, we apply it to publicly available cycling data consisting of three batches of cycling modes. The calculated TA scores highlight the rest phase as a key characteristic distinguishing LIB data among different batches. Additionally, CA scores reveal variations in the importance of cycles across batches. By leveraging CA scores, we explore the potential to reduce the number of cycles in the input data. The single-head and multi-head attentions enable us to decrease the input dimension from 100 to 50 and 30 cycles, respectively.
</details>
<details>
<summary>摘要</summary>
预测锂离子电池（LIB）的寿命是非常重要的，以便优化使用和避免事故。之前的研究常常建立预测模型，但这些模型通常需要难以在实时操作中测量的输入，并且未能捕捉到循环和循环之间的数据模式，这些特征是精确预测的关键。在本研究中，我们使用注意力机制（AM）来开发基于数据驱动的LIB寿命预测模型。我们的模型结合了循环神经网络（RNN）和卷积神经网络（CNN）组件，并包括两种注意力机制：时间注意力（TA）和循环注意力（CA）。TA的目的是在每个循环中 identific important的时间步骤，而CA则是捕捉循环之间的关键特征。这使得我们的模型更加准确，并且可以帮助我们理解输入数据中的关键特征。为了验证我们的方法，我们对公共可用的循环数据进行应用，该数据包括三批循环模式。计算的TA分数显示，循环静止阶段是不同批次中LIB数据的关键特征。此外，CA分数还揭示了循环之间的重要性差异。通过利用CA分数，我们可以考虑将输入数据减少到30循环或50循环。单头和多头注意力允许我们降低输入维度。
</details></li>
</ul>
<hr>
<h2 id="Physics-Enhanced-Multi-fidelity-Learning-for-Optical-Surface-Imprint"><a href="#Physics-Enhanced-Multi-fidelity-Learning-for-Optical-Surface-Imprint" class="headerlink" title="Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint"></a>Physics-Enhanced Multi-fidelity Learning for Optical Surface Imprint</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10278">http://arxiv.org/abs/2311.10278</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yongchao Chen</li>
<li>for: 本研究旨在利用多似真神经网络（MFNN）解决材料归一化问题，即将仪器测量的弹性印记转换为真实的弹性曲线。</li>
<li>methods: 本研究使用了多似真神经网络（MFNN）模型，首先通过纯 simulate 数据进行 aktif 训练，然后通过传输学习将 sim-to-real 距离降低。特点是通过神经网络提取未知物理特性，同时还注入已知物理特性到传输学习框架中，从而大幅提高模型稳定性和数据需求。</li>
<li>results: 本研究结果表明，通过使用多似真神经网络（MFNN）模型可以高效地解决材料归一化问题，并且可以减少数据需求和提高模型稳定性。这种方法可以在实验研究中应用到受限于数据约束和真实变化的场景中。<details>
<summary>Abstract</summary>
Human fingerprints serve as one unique and powerful characteristic for each person, from which policemen can recognize the identity. Similar to humans, many natural bodies and intrinsic mechanical qualities can also be uniquely identified from surface characteristics. To measure the elasto-plastic properties of one material, one formally sharp indenter is pushed into the measured body under constant force and retracted, leaving a unique residual imprint of the minute size from several micrometers to nanometers. However, one great challenge is how to map the optical image of this residual imprint into the real wanted mechanical properties, i.e., the tensile force curve. In this paper, we propose a novel method to use multi-fidelity neural networks (MFNN) to solve this inverse problem. We first actively train the NN model via pure simulation data, and then bridge the sim-to-real gap via transfer learning. The most innovative part is that we use NN to dig out the unknown physics and also implant the known physics into the transfer learning framework, thus highly improving the model stability and decreasing the data requirement. This work serves as one great example of applying machine learning into the real experimental research, especially under the constraints of data limitation and fidelity variance.
</details>
<details>
<summary>摘要</summary>
人类指印也是每个人的唯一和强大特征之一，它可以帮助警察认定人员身份。与人类类似，许多自然体系和内在机械特性也可以通过表面特征进行uniquely标识。为了测量材料的弹性挤压性能，我们使用一个高精度的检测器压入测量体系，并在Constant Force下 retraction，从而留下微米级别的剩下印记。然而，一大问题是如何将光学图像转化为真实想要的机械性能Curve。在这篇论文中，我们提出了一种新的方法，使用多种信任度神经网络（MFNN）解决这个逆问题。我们首先通过纯度 simulation 数据 актив地训练 NN 模型，然后使用传输学习桥接 sim-to-real 漏洞。最创新的部分是使用 NN 挖掘未知物理，同时还注入已知物理到传输学习框架中，从而高度提高模型稳定性和数据需求。这项工作作为机器学习在实验研究中的应用，特别是在数据约束和可靠性变化的情况下。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-pap-smear-cell-representation-for-cervical-cancer-screening"><a href="#Interpretable-pap-smear-cell-representation-for-cervical-cancer-screening" class="headerlink" title="Interpretable pap smear cell representation for cervical cancer screening"></a>Interpretable pap smear cell representation for cervical cancer screening</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10269">http://arxiv.org/abs/2311.10269</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yu Ando, Nora Jee-Young Park and, Gun Oh Chong, Seokhwan Ko, Donghyeon Lee, Junghwan Cho, Hyungsoo Han</li>
<li>for: 本研究旨在开发一种基于一类分类的变分自动编码器来学习可解释的深度陵胞细胞图像表示，以自动化陵胞检测。</li>
<li>methods: 本研究使用变分自动编码器来学习陵胞细胞图像表示，并使用一类分类来计算细胞异常程度。</li>
<li>results: 研究发现，使用变分自动编码器可以计算细胞异常程度，并可以在不使用异常样本的情况下进行训练。最佳模型在分类陵胞细胞癌（SCC）和常见细胞（NOR）之间的区别达到0.908+-0.003的AUC，而在分类高级细胞变化（HSIL）和常见细胞之间的区别达到0.920+-0.002的AUC。相比其他聚类方法，我们的方法可以更好地隔离不同的异常区域，帮助解释我们的结果。<details>
<summary>Abstract</summary>
Screening is critical for prevention and early detection of cervical cancer but it is time-consuming and laborious. Supervised deep convolutional neural networks have been developed to automate pap smear screening and the results are promising. However, the interest in using only normal samples to train deep neural networks has increased owing to class imbalance problems and high-labeling costs that are both prevalent in healthcare. In this study, we introduce a method to learn explainable deep cervical cell representations for pap smear cytology images based on one class classification using variational autoencoders. Findings demonstrate that a score can be calculated for cell abnormality without training models with abnormal samples and localize abnormality to interpret our results with a novel metric based on absolute difference in cross entropy in agglomerative clustering. The best model that discriminates squamous cell carcinoma (SCC) from normals gives 0.908 +- 0.003 area under operating characteristic curve (AUC) and one that discriminates high-grade epithelial lesion (HSIL) 0.920 +- 0.002 AUC. Compared to other clustering methods, our method enhances the V-measure and yields higher homogeneity scores, which more effectively isolate different abnormality regions, aiding in the interpretation of our results. Evaluation using in-house and additional open dataset show that our model can discriminate abnormality without the need of additional training of deep models.
</details>
<details>
<summary>摘要</summary>
屏选是预防和早期检测颈部癌症的关键，但是它却是时间consuming和劳动密集的。在超vised深度征 neural networks 中，有人开发了 automate pap smear 屏选方法，并且结果很有 promise。然而，因为医疗行业中的类别不均和标签成本高的问题，有越来越多的人关注使用正常样本来训练深度神经网络的可能性。在这项研究中，我们提出了一种方法，通过一类分类使用变量自动encoder来学习可解释深度颈部细胞表示图像。我们的发现是，可以计算细胞异常程度无需训练异常样本，并且可以将异常区域当地化，以便更好地解释我们的结果。我们的最佳模型可以在SCC和正常样本之间分类，得到0.908 ± 0.003的AUC，而在HSIL和正常样本之间分类，得到0.920 ± 0.002的AUC。相比其他分 clustering 方法，我们的方法可以提高V-度量和Homogeneity 分数，从而更好地隔离不同的异常区域，帮助解释我们的结果。我们的模型可以在我们的实验室和其他开放数据集上进行评估，并且可以在不需要深度模型进行额外训练的情况下，有效地检测异常。
</details></li>
</ul>
<hr>
<h2 id="FedTruth-Byzantine-Robust-and-Backdoor-Resilient-Federated-Learning-Framework"><a href="#FedTruth-Byzantine-Robust-and-Backdoor-Resilient-Federated-Learning-Framework" class="headerlink" title="FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework"></a>FedTruth: Byzantine-Robust and Backdoor-Resilient Federated Learning Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10248">http://arxiv.org/abs/2311.10248</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sheldon C. Ebron Jr., Kan Yang</li>
<li>for: 本研究旨在提出一种robust的防御机制，以保护 Federated Learning（FL）中的机器学习模型免受恶意客户端的攻击。</li>
<li>methods: 本研究使用了一种名为FedTruth的新防御机制，该机制不假设特定的数据分布，并不需要benign的根据 dataset。它通过动态权重聚合来估算全局的模型更新，考虑所有benign客户端的贡献。</li>
<li>results: 实验表明，FedTruth可以有效地防止恶意客户端的攻击，使FL模型免受模型毒化和后门攻击的影响。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables collaborative machine learning model training across multiple parties without sharing raw data. However, FL's distributed nature allows malicious clients to impact model training through Byzantine or backdoor attacks, using erroneous model updates. Existing defenses measure the deviation of each update from a 'ground-truth model update.' They often rely on a benign root dataset on the server or use trimmed mean or median for clipping, both methods having limitations.   We introduce FedTruth, a robust defense against model poisoning in FL. FedTruth doesn't assume specific data distributions nor requires a benign root dataset. It estimates a global model update with dynamic aggregation weights, considering contributions from all benign clients. Empirical studies demonstrate FedTruth's efficacy in mitigating the impacts of poisoned updates from both Byzantine and backdoor attacks.
</details>
<details>
<summary>摘要</summary>
共同学习（Federated Learning，FL）允许多方合作培训机器学习模型，无需分享原始数据。然而，FL的分布式结构允许有恶意客户端通过Byzantine或后门攻击，使用错误的模型更新。现有防御方法测量每个更新的偏差，通常基于服务器上的善意根据dataset或使用截断平均或中值clip，两种方法均有局限性。我们介绍FedTruth，一种鲁棒的FL模型报 poisoning防御方法。FedTruth不假设特定的数据分布，不需要善意根据dataset。它估算全局模型更新，使用动态汇集权重考虑所有善意客户端的贡献。实验研究表明FedTruth有效地减少了恶意更新的影响，包括Byzantine和后门攻击。
</details></li>
</ul>
<hr>
<h2 id="Surprisal-Driven-k-NN-for-Robust-and-Interpretable-Nonparametric-Learning"><a href="#Surprisal-Driven-k-NN-for-Robust-and-Interpretable-Nonparametric-Learning" class="headerlink" title="Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning"></a>Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10246">http://arxiv.org/abs/2311.10246</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amartya Banerjee, Christopher J. Hazard, Jacob Beel, Cade Mack, Jack Xia, Michael Resnick, Will Goddin</li>
<li>for: 本研究旨在提出一种基于信息理论的 robust和可解释的$k$-nearest neighbors（$k$-NN）算法框架，用于进行分类、回归和异常检测等任务。</li>
<li>methods: 该算法使用一种新的表示形式的\textit{surprisal}（用于解释差异）而不是传统的距离度量，从而提高了模型的可解释性和鲁棒性。</li>
<li>results: 研究表明，使用该算法可以在分类、回归和异常检测等任务上达到或超过现有技术的性能，同时提供了新的数据和预测概念，以增强模型的解释性。<details>
<summary>Abstract</summary>
Nonparametric learning is a fundamental concept in machine learning that aims to capture complex patterns and relationships in data without making strong assumptions about the underlying data distribution. Owing to simplicity and familiarity, one of the most well-known algorithms under this paradigm is the $k$-nearest neighbors ($k$-NN) algorithm. Driven by the usage of machine learning in safety-critical applications, in this work, we shed new light on the traditional nearest neighbors algorithm from the perspective of information theory and propose a robust and interpretable framework for tasks such as classification, regression, and anomaly detection using a single model. Instead of using a traditional distance measure which needs to be scaled and contextualized, we use a novel formulation of \textit{surprisal} (amount of information required to explain the difference between the observed and expected result). Finally, we demonstrate this architecture's capability to perform at-par or above the state-of-the-art on classification, regression, and anomaly detection tasks using a single model with enhanced interpretability by providing novel concepts for characterizing data and predictions.
</details>
<details>
<summary>摘要</summary>
在传统的最近邻算法中，使用的距离度量需要缩放和contextualize，我们使用一种新的表述 Surprisal（需要解释异常结果的信息量）。最后，我们示出了这个架构可以在分类、回归和异常检测任务中表现至少与现有技术水平一样，同时提供了新的概念来描述数据和预测。
</details></li>
</ul>
<hr>
<h2 id="Efficient-Temporally-Aware-DeepFake-Detection-using-H-264-Motion-Vectors"><a href="#Efficient-Temporally-Aware-DeepFake-Detection-using-H-264-Motion-Vectors" class="headerlink" title="Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors"></a>Efficient Temporally-Aware DeepFake Detection using H.264 Motion Vectors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10788">http://arxiv.org/abs/2311.10788</a></li>
<li>repo_url: None</li>
<li>paper_authors: Peter Grönquist, Yufan Ren, Qingyi He, Alessio Verardo, Sabine Süsstrunk</li>
<li>for: 这篇论文旨在探讨一种新的深度伪造检测方法，它可以快速和计算效率地检测视频中的深度伪造。</li>
<li>methods: 这篇论文使用了H.264 видео编解码器中的动作 векто（MV）和信息面罩（IM）来检测深度伪造的 temporally inconsistent。</li>
<li>results:  experiments 显示，这种方法可以实现高效的深度伪造检测，并且与传统的每帧RGB-only方法相比，有着更小的计算成本。<details>
<summary>Abstract</summary>
Video DeepFakes are fake media created with Deep Learning (DL) that manipulate a person's expression or identity. Most current DeepFake detection methods analyze each frame independently, ignoring inconsistencies and unnatural movements between frames. Some newer methods employ optical flow models to capture this temporal aspect, but they are computationally expensive. In contrast, we propose using the related but often ignored Motion Vectors (MVs) and Information Masks (IMs) from the H.264 video codec, to detect temporal inconsistencies in DeepFakes. Our experiments show that this approach is effective and has minimal computational costs, compared with per-frame RGB-only methods. This could lead to new, real-time temporally-aware DeepFake detection methods for video calls and streaming.
</details>
<details>
<summary>摘要</summary>
视频深伪是利用深度学习（DL）创建的假媒体，可以 manipulate 人的表情或身份。现有的大多数 DeepFake 检测方法是分析每帧图像独立，忽略帧之间的不一致和不自然的运动。一些 newer 方法使用光流模型来捕捉这种 temporal 方面，但它们的计算成本较高。相比之下，我们提议使用 H.264 视频编码器中的相关但 часто被忽略的动作向量（MV）和信息面积（IM），来检测 DeepFakes 中的时间不一致。我们的实验表明，这种方法可以减少计算成本，并且效果比 RGB 每帧方法更好。这可能导致新的实时时间感知 DeepFake 检测方法，用于视频通话和流媒体。
</details></li>
</ul>
<hr>
<h2 id="Advancements-in-Generative-AI-A-Comprehensive-Review-of-GANs-GPT-Autoencoders-Diffusion-Model-and-Transformers"><a href="#Advancements-in-Generative-AI-A-Comprehensive-Review-of-GANs-GPT-Autoencoders-Diffusion-Model-and-Transformers" class="headerlink" title="Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers"></a>Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10242">http://arxiv.org/abs/2311.10242</a></li>
<li>repo_url: None</li>
<li>paper_authors: Staphord Bengesi, Hoda El-Sayed, Md Kamruzzaman Sarker, Yao Houkpati, John Irungu, Timothy Oladunni</li>
<li>for: This paper explores the recent advancements in Generative Artificial Intelligence, particularly the development and applications of cutting-edge tools like Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox.</li>
<li>methods: The paper discusses various state-of-the-art models used in these tools, including Stable Diffusion, transformer models like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial networks.</li>
<li>results: The paper highlights the remarkable capabilities of these tools in accomplishing tasks such as text generation, music composition, image creation, video production, code generation, and scientific work, and also discusses the challenges posed by these advancements.<details>
<summary>Abstract</summary>
The launch of ChatGPT has garnered global attention, marking a significant milestone in the field of Generative Artificial Intelligence. While Generative AI has been in effect for the past decade, the introduction of ChatGPT has ignited a new wave of research and innovation in the AI domain. This surge in interest has led to the development and release of numerous cutting-edge tools, such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox, among others. These tools exhibit remarkable capabilities, encompassing tasks ranging from text generation and music composition, image creation, video production, code generation, and even scientific work. They are built upon various state-of-the-art models, including Stable Diffusion, transformer models like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial networks. This advancement in Generative AI presents a wealth of exciting opportunities and, simultaneously, unprecedented challenges. Throughout this paper, we have explored these state-of-the-art models, the diverse array of tasks they can accomplish, the challenges they pose, and the promising future of Generative Artificial Intelligence.
</details>
<details>
<summary>摘要</summary>
<SYS>Launch of ChatGPT 引发全球关注，标志着生成人工智能领域的重要突破口。过去十年来，生成 AI 已经在发展，但 ChatGPT 的出现启动了新一波的研究和创新在 AI 领域。这一波的兴趣使得许多 cutting-edge 工具的出现，如 Bard、Stable Diffusion、DALL-E、Make-A-Video、Runway ML 和 Jukebox 等。这些工具在文本生成、音乐创作、图像创建、视频制作、代码生成和科学工作等多个任务上表现出惊人的能力。它们基于多种当前最先进的模型，包括 Stable Diffusion、变换器模型如 GPT-3（最新 GPT-4）、变量自动编码器和生成对抗网络。这一次的生成 AI 发展呈现了许多激动人心的机遇，同时也面临着前所未有的挑战。在这篇文章中，我们探讨了这些当前最先进的模型，它们可以完成的多种任务，它们的挑战，以及生成人工智能的未来。</SYS>Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you need a more accurate translation, please consider using a professional translation service.
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/cs.AI_2023_11_17/" data-id="clpztdnco007nes88fvcsfqod" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/cs.CL_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T11:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/cs.CL_2023_11_17/">cs.CL - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Extraction-and-Summarization-of-Explicit-Video-Content-using-Multi-Modal-Deep-Learning"><a href="#Extraction-and-Summarization-of-Explicit-Video-Content-using-Multi-Modal-Deep-Learning" class="headerlink" title="Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning"></a>Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10899">http://arxiv.org/abs/2311.10899</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shaunak Joshi, Raghav Gaggar</li>
<li>for: 用于自动审核视频数据中的Explicit内容</li>
<li>methods: 使用多modal深度学习提取输入视频中的Explicit段落，然后使用文本概括其内容以确定适用年龄和评级</li>
<li>results: 通过标准指标评估ipeline的效果<details>
<summary>Abstract</summary>
With the increase in video-sharing platforms across the internet, it is difficult for humans to moderate the data for explicit content. Hence, an automated pipeline to scan through video data for explicit content has become the need of the hour. We propose a novel pipeline that uses multi-modal deep learning to first extract the explicit segments of input videos and then summarize their content using text to determine its age appropriateness and age rating. We also evaluate our pipeline's effectiveness in the end using standard metrics.
</details>
<details>
<summary>摘要</summary>
随着互联网上的视频分享平台的增加，人类对数据进行监管已变得困难。因此，一个自动化的数据扫描管道已成为当前的需求。我们提出了一种新的管道，使用多Modal深度学习来提取输入视频中的Explicit段落，然后使用文本来确定其适合年龄和评级。我们还在结束时对管道的效果进行评估，使用标准指标。Here's the translation of the text into Traditional Chinese:随着互联网上的视频分享平台的增加，人类对数据进行监管已变得困难。因此，一个自动化的数据扫描管道已成为当前的需求。我们提出了一个新的管道，使用多Modal深度学习来提取输入视频中的Explicit段落，然后使用文本来确定其适合年龄和评级。我们也在结束时对管道的效果进行评估，使用标准指标。
</details></li>
</ul>
<hr>
<h2 id="Labeling-Indoor-Scenes-with-Fusion-of-Out-of-the-Box-Perception-Models"><a href="#Labeling-Indoor-Scenes-with-Fusion-of-Out-of-the-Box-Perception-Models" class="headerlink" title="Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models"></a>Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10883">http://arxiv.org/abs/2311.10883</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimeng Li, Navid Rajabi, Sulabh Shrestha, Md Alimoor Reza, Jana Kosecka</li>
<li>for: 本研究旨在提高indoor scene中object detection和semantic segmentation模型的训练和评估中的图像描述阶段。</li>
<li>methods: 我们提出了一种基于state-of-the-art模型（SAM、Detic、MaskFormer）的cost-effective标注方法，用于获得indoor scene中的semantic segmentation和object instance detection的pseudo-labels。我们还提出了一种多视图标注融合阶段，用于解决单视图不一致的问题。</li>
<li>results: 我们在Active Vision dataset和ADE20K dataset上证明了提出的方法的效iveness。我们对比了我们的标注过程与人工标注，并证明了下游任务中的object goal navigation和part discovery的性能得到了提高。<details>
<summary>Abstract</summary>
The image annotation stage is a critical and often the most time-consuming part required for training and evaluating object detection and semantic segmentation models. Deployment of the existing models in novel environments often requires detecting novel semantic classes not present in the training data. Furthermore, indoor scenes contain significant viewpoint variations, which need to be handled properly by trained perception models. We propose to leverage the recent advancements in state-of-the-art models for bottom-up segmentation (SAM), object detection (Detic), and semantic segmentation (MaskFormer), all trained on large-scale datasets. We aim to develop a cost-effective labeling approach to obtain pseudo-labels for semantic segmentation and object instance detection in indoor environments, with the ultimate goal of facilitating the training of lightweight models for various downstream tasks. We also propose a multi-view labeling fusion stage, which considers the setting where multiple views of the scenes are available and can be used to identify and rectify single-view inconsistencies. We demonstrate the effectiveness of the proposed approach on the Active Vision dataset and the ADE20K dataset. We evaluate the quality of our labeling process by comparing it with human annotations. Also, we demonstrate the effectiveness of the obtained labels in downstream tasks such as object goal navigation and part discovery. In the context of object goal navigation, we depict enhanced performance using this fusion approach compared to a zero-shot baseline that utilizes large monolithic vision-language pre-trained models.
</details>
<details>
<summary>摘要</summary>
“图像标注阶段是训练和评估物件探测和Semantic Segmentation模型的 kritical 和最时间consuming 部分。在部署现有模型到新环境时，需要探测到 novel 的Semantic classes，而且室内场景具有重要的视角差异，需要对训练好的感知模型进行正确处理。我们提出利用最新的顶尖模型，包括底部分Segmentation（SAM）、物件探测（Detic）和Semantic Segmentation（MaskFormer），所有它们在大规模数据上进行训练。我们目标是发展一种可以实现Cost-effective的标签方法，以获得室内环境中Semantic segmentation和物件实例探测的pseudo-labels，以便训练轻量级模型供不同的下游任务。我们还提出一个多视角标签融合阶段，考虑到场景中有多个视角可用，并可以使用多个视角来识别和修正单一视角的不一致。我们在Active Vision dataset和ADE20K dataset上显示了提案的效果。我们评估标签过程的质量，与人工标注进行比较，以及下游任务中的物件目标航行和部件发现的效果。在物件目标航行中，我们显示了基于融合方法的表现，较之零基eline模型使用大型视觉语言预训模型。”
</details></li>
</ul>
<hr>
<h2 id="Token-level-Adaptation-of-LoRA-Adapters-for-Downstream-Task-Generalization"><a href="#Token-level-Adaptation-of-LoRA-Adapters-for-Downstream-Task-Generalization" class="headerlink" title="Token-level Adaptation of LoRA Adapters for Downstream Task Generalization"></a>Token-level Adaptation of LoRA Adapters for Downstream Task Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10847">http://arxiv.org/abs/2311.10847</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jb-01/lora-tle">https://github.com/jb-01/lora-tle</a></li>
<li>paper_authors: Joshua Belofsky</li>
<li>for: 这篇研究是为了将LoRA束变更适应到更小的语言模型中，以便在任意下游任务中进行适应。</li>
<li>methods: 这篇研究使用了一种不需要计算增加的算法来选择LoRA束的束重量组合，而不是使用标准的混合专家架构。</li>
<li>results: 研究结果显示，将LoRA束进行单位水平的适应，可以超过基本Llama-2-7b模型的性能，并且在数学（GSM8K）、科学（ARC-Challenge）、阅读理解（SQuAD）和程式设计（CodeAlpaca-20k）任务中都能够取得佳绩。<details>
<summary>Abstract</summary>
This paper introduces a method for adapting LoRA adapters in smaller-sized language models to arbitrary downstream tasks. Unlike standard mixture-of-expert architectures, our method employs a gradient-free routing function to choose a weighted combination of experts without increasing the compute requirements for training or inference. The results show that token-level adaptation of LoRA adapters outperforms the base Llama-2-7b model across mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension (SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that the average performance of token-level adaptation outperforms individual models fine-tuned for each of the tasks with the best performance observed in adaptation of every-other token during inference. The code for this study is made available through a public repository.
</details>
<details>
<summary>摘要</summary>
Here's the Simplified Chinese translation:这篇论文提出了一种方法，用于在小型语言模型中适应任意下游任务。与传统的权重混合体系不同，我们的方法使用梯度自由路由函数来选择一个权重加权的专家组合，不会增加训练或推理计算的需求。结果表明，将LoRA适应器 token 级别适应到下游任务时，超过了基础 Llama-2-7b 模型在数学 (GSM8K)、科学 (ARC-Challenge)、阅读理解 (SQuAD) 和编程 (CodeAlpaca-20k) 任务上的性能。此外，token 级别适应的平均性能还超过了每个任务的最佳模型 fine-tune 后的性能，并且在推理时采用每个 token 适应的方式得到了最佳性能。该研究的代码公开在公共存储库中。
</details></li>
</ul>
<hr>
<h2 id="Camels-in-a-Changing-Climate-Enhancing-LM-Adaptation-with-Tulu-2"><a href="#Camels-in-a-Changing-Climate-Enhancing-LM-Adaptation-with-Tulu-2" class="headerlink" title="Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2"></a>Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10702">http://arxiv.org/abs/2311.10702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi</li>
<li>for: 提高适应大语言模型下沟通和用户偏好的理解和最佳实践</li>
<li>methods: 利用新的迭代优化技术和更高质量的指导数据集进行模型优化</li>
<li>results: 实现了开源模型的状态天化表现和匹配或超越GPT-3.5-turbo-0301的一些benchmarkHere’s a more detailed explanation of each point:</li>
<li>for: The paper aims to improve the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences.</li>
<li>methods: The authors use a number of advances in open resources for instruction tuning, including better base models and new finetuning techniques, to improve the T&quot;ULU models. They release a suite of improved models, including T&quot;ULU-V2-mix, T&quot;ULU 2, T&quot;ULU 2+DPO, and CODE T&quot;ULU 2.</li>
<li>results: The authors evaluate the T&quot;ULU 2 suite on multiple benchmarks and show that it achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks.<details>
<summary>Abstract</summary>
Since the release of T\"ULU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into T\"ULU, resulting in T\"ULU 2, a suite of improved T\"ULU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) T\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2) T\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\"ULU 2+DPO, T\"ULU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (T\"ULU 2+DPO 70B); (4) CODE T\"ULU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the T\"ULU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.
</details>
<details>
<summary>摘要</summary>
Since the release of T\"ULU [王等，2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into T\"ULU, resulting in T\"ULU 2, a suite of improved T\"ULU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) T\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2) T\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\"ULU 2+DPO, T\"ULU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (T\"ULU 2+DPO 70B); (4) CODE T\"ULU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the T\"ULU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.
</details></li>
</ul>
<hr>
<h2 id="Rethinking-Attention-Exploring-Shallow-Feed-Forward-Neural-Networks-as-an-Alternative-to-Attention-Layers-in-Transformers"><a href="#Rethinking-Attention-Exploring-Shallow-Feed-Forward-Neural-Networks-as-an-Alternative-to-Attention-Layers-in-Transformers" class="headerlink" title="Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers"></a>Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10642">http://arxiv.org/abs/2311.10642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/kyegomez/ShallowFF">https://github.com/kyegomez/ShallowFF</a></li>
<li>paper_authors: Vukasin Bozic, Danilo Dordevic, Daniele Coppola, Joseph Thommes</li>
<li>for: 本研究探讨了使用标准浅层feed-forward网络来模拟Transformer模型中的注意力机制的效果。</li>
<li>methods: 我们在Original Transformer模型中的注意力机制中使用了简单的feed-forward网络，通过知识塑化来训练。</li>
<li>results: 我们的实验结果表明，使用这些”注意力less Transformer”可以与原始建筑物rival的性能。我们通过精心的减少研究和不同类型和大小的置换网络来证明我们的方法的可行性。<details>
<summary>Abstract</summary>
This work presents an analysis of the effectiveness of using standard shallow feed-forward networks to mimic the behavior of the attention mechanism in the original Transformer model, a state-of-the-art architecture for sequence-to-sequence tasks. We substitute key elements of the attention mechanism in the Transformer with simple feed-forward networks, trained using the original components via knowledge distillation. Our experiments, conducted on the IWSLT2017 dataset, reveal the capacity of these "attentionless Transformers" to rival the performance of the original architecture. Through rigorous ablation studies, and experimenting with various replacement network types and sizes, we offer insights that support the viability of our approach. This not only sheds light on the adaptability of shallow feed-forward networks in emulating attention mechanisms but also underscores their potential to streamline complex architectures for sequence-to-sequence tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Countering-Misinformation-via-Emotional-Response-Generation"><a href="#Countering-Misinformation-via-Emotional-Response-Generation" class="headerlink" title="Countering Misinformation via Emotional Response Generation"></a>Countering Misinformation via Emotional Response Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10587">http://arxiv.org/abs/2311.10587</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/marcoguerini/vermouth">https://github.com/marcoguerini/vermouth</a></li>
<li>paper_authors: Daniel Russo, Shane Peter Kaszefski-Yaschuk, Jacopo Staiano, Marco Guerini</li>
<li>for: 防止社交媒体上的谣言散播，保护公共健康、社会稳定和民主制度。</li>
<li>methods: 利用人工智能技术自动化评断谣言，并integrate fact-checker材料与社交媒体通信风格和情感。</li>
<li>results: 提供了首个大规模的证据Response集（约12千个CLAIM-RESPONSE对），覆盖了社交媒体平台上的基本情感和谣言吸引力两大因素，并通过了大规模实验，证明模型在输出质量和总体适应能力方面具有显著改进。<details>
<summary>Abstract</summary>
The proliferation of misinformation on social media platforms (SMPs) poses a significant danger to public health, social cohesion and ultimately democracy. Previous research has shown how social correction can be an effective way to curb misinformation, by engaging directly in a constructive dialogue with users who spread -- often in good faith -- misleading messages. Although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in conversations on social media. Thereby, significant effort has been made to automate the use of fact-checker material in social correction; however, no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset comprising roughly 12 thousand claim-response pairs (linked to debunking articles), accounting for both SMP-style and basic emotions, two factors which have a significant role in misinformation credibility and spreading. To collect this dataset we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing how models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.
</details>
<details>
<summary>摘要</summary>
社交媒体平台上的谣言泛洪 pose 一大难题 для公共健康、社会凝聚和最终是民主。 previous research 表明，社交更正可以有效地遏制谣言，通过直接与传播谣言的用户进行构建性对话。 although professional fact-checkers are crucial to debunking viral claims, they usually do not engage in social media conversations. Therefore, significant effort has been made to automate the use of fact-checker material in social correction, but no previous work has tried to integrate it with the style and pragmatics that are commonly employed in social media communication. To fill this gap, we present VerMouth, the first large-scale dataset consisting of approximately 12,000 claim-response pairs (linked to debunking articles), taking into account both SMP-style and basic emotions, which have a significant impact on the credibility and spread of misinformation. To collect this dataset, we used a technique based on an author-reviewer pipeline, which efficiently combines LLMs and human annotators to obtain high-quality data. We also provide comprehensive experiments showing that models trained on our proposed dataset have significant improvements in terms of output quality and generalization capabilities.
</details></li>
</ul>
<hr>
<h2 id="Detection-of-Offensive-and-Threatening-Online-Content-in-a-Low-Resource-Language"><a href="#Detection-of-Offensive-and-Threatening-Online-Content-in-a-Low-Resource-Language" class="headerlink" title="Detection of Offensive and Threatening Online Content in a Low Resource Language"></a>Detection of Offensive and Threatening Online Content in a Low Resource Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10541">http://arxiv.org/abs/2311.10541</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fatima Muhammad Adam, Abubakar Yakubu Zandam, Isa Inuwa-Dutse<br>for: This study aimed to address the lack of detection systems for offensive and threatening language in Hausa, a low-resource language spoken by over 100 million people in Africa.methods: The study consisted of two user studies (n&#x3D;308) to investigate cyberbullying-related issues, collecting and annotating the first set of offensive and threatening datasets in Hausa, and developing a detection system to flag offensive and threatening content.results: The detection system was able to detect more than 70% of offensive and threatening content, but many of these were mistranslated by Google’s translation engine. The study highlights the need for a more effective detection system, which can be achieved by involving diverse stakeholders in understanding local conventions and demographics.<details>
<summary>Abstract</summary>
Hausa is a major Chadic language, spoken by over 100 million people in Africa. However, from a computational linguistic perspective, it is considered a low-resource language, with limited resources to support Natural Language Processing (NLP) tasks. Online platforms often facilitate social interactions that can lead to the use of offensive and threatening language, which can go undetected due to the lack of detection systems designed for Hausa. This study aimed to address this issue by (1) conducting two user studies (n=308) to investigate cyberbullying-related issues, (2) collecting and annotating the first set of offensive and threatening datasets to support relevant downstream tasks in Hausa, (3) developing a detection system to flag offensive and threatening content, and (4) evaluating the detection system and the efficacy of the Google-based translation engine in detecting offensive and threatening terms in Hausa. We found that offensive and threatening content is quite common, particularly when discussing religion and politics. Our detection system was able to detect more than 70% of offensive and threatening content, although many of these were mistranslated by Google's translation engine. We attribute this to the subtle relationship between offensive and threatening content and idiomatic expressions in the Hausa language. We recommend that diverse stakeholders participate in understanding local conventions and demographics in order to develop a more effective detection system. These insights are essential for implementing targeted moderation strategies to create a safe and inclusive online environment.
</details>
<details>
<summary>摘要</summary>
哈萨语是一种主要的查迪语言，在非洲被讲语言人数超过100万人。然而，从计算机语言学角度来看，哈萨语是一种低资源语言，有限的资源支持自然语言处理（NLP）任务。在线平台经常承载社交交互，可能会导致使用侮辱和威胁语言，这些语言可能因为缺乏适用于哈萨语的检测系统而被忽略。本研究的目的是解决这个问题，通过以下方法：1. 进行了两场用户研究（n=308），检查了在线社交媒体上的煽动和威胁行为。2. 收集和标注了哈萨语的第一批侮辱和威胁数据集，以支持相关的下游任务。3. 开发了一个检测系统，可以检测侮辱和威胁内容。4. 评估了检测系统和Google翻译引擎在哈萨语中检测侮辱和威胁表达的效果。我们发现，在讨论宗教和政治时，侮辱和威胁内容很普遍。我们的检测系统可以检测到大于70%的侮辱和威胁内容，然而许多这些被Google翻译引擎误译。我们认为，这是因为哈萨语中侮辱和威胁内容和idiomatic表达之间存在巧妙的关系。我们建议参与者来理解当地的习俗和人口结构，以开发更有效的检测系统。这些洞察力是必要的，以实施targeted moderation策略，创造一个安全和包容的在线环境。
</details></li>
</ul>
<hr>
<h2 id="When-a-Language-Question-Is-at-Stake-A-Revisited-Approach-to-Label-Sensitive-Content"><a href="#When-a-Language-Question-Is-at-Stake-A-Revisited-Approach-to-Label-Sensitive-Content" class="headerlink" title="When a Language Question Is at Stake. A Revisited Approach to Label Sensitive Content"></a>When a Language Question Is at Stake. A Revisited Approach to Label Sensitive Content</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10514">http://arxiv.org/abs/2311.10514</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stetsenko Daria</li>
<li>for: 这个论文目标是为了提供高质量的数据集，用于特定任务，如辱华语言检测、不实信息或谣言识别。</li>
<li>methods: 这篇论文使用了 pseudo-labeling 方法，对乌克兰微博中的俄乌战争话语进行了标注。</li>
<li>results: 实验表明，使用 pseudo-labeling 方法可以生成高质量的数据集，并提供了基本的统计分析和模型评估。<details>
<summary>Abstract</summary>
Many under-resourced languages require high-quality datasets for specific tasks such as offensive language detection, disinformation, or misinformation identification. However, the intricacies of the content may have a detrimental effect on the annotators. The article aims to revisit an approach of pseudo-labeling sensitive data on the example of Ukrainian tweets covering the Russian-Ukrainian war. Nowadays, this acute topic is in the spotlight of various language manipulations that cause numerous disinformation and profanity on social media platforms. The conducted experiment highlights three main stages of data annotation and underlines the main obstacles during machine annotation. Ultimately, we provide a fundamental statistical analysis of the obtained data, evaluation of models used for pseudo-labelling, and set further guidelines on how the scientists can leverage the corpus to execute more advanced research and extend the existing data samples without annotators' engagement.
</details>
<details>
<summary>摘要</summary>
许多资源不足的语言需要高质量的数据集来进行特定任务，如违禁语言检测、谎言或谣言识别。然而，内容的细节可能会对注释者产生负面影响。本文旨在重新考虑使用假标注敏感数据的方法，以便在乌克兰推特上的俄罗斯-乌克兰战争为例。现在，这个紧耦的话题在社交媒体平台上引起了各种语言护卫和词汇滥议。本实验描述了三个主要的数据注释阶段，并强调了机器注释过程中的主要困难。最后，我们提供了基本的统计分析，评估使用pseudo标注模型，并提出了科学家可以通过这些资料执行更高级别的研究和扩展现有数据样本而无需注释员的参与。
</details></li>
</ul>
<hr>
<h2 id="Sinhala-English-Word-Embedding-Alignment-Introducing-Datasets-and-Benchmark-for-a-Low-Resource-Language"><a href="#Sinhala-English-Word-Embedding-Alignment-Introducing-Datasets-and-Benchmark-for-a-Low-Resource-Language" class="headerlink" title="Sinhala-English Word Embedding Alignment: Introducing Datasets and Benchmark for a Low Resource Language"></a>Sinhala-English Word Embedding Alignment: Introducing Datasets and Benchmark for a Low Resource Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10436">http://arxiv.org/abs/2311.10436</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kasun Wickramasinghe, Nisansa de Silva</li>
<li>for: This paper aims to align Sinhala and English word embedding spaces, addressing the lack of attention on low-resource languages in previous research.</li>
<li>methods: The authors use available alignment techniques and introduce a benchmark for Sinhala language embedding alignment, as well as an intermediate task of creating Sinhala-English alignment datasets.</li>
<li>results: While the results are not comparable to those of high-resource languages, the paper lays the groundwork for more specialized alignment between English and Sinhala embeddings.Here’s the simplified Chinese text:</li>
<li>for: 本研究目的是对Singhala和英语词嵌入空间进行对齐，解决过去对低资源语言的忽略。</li>
<li>methods: 作者使用可用的对齐技术，并在Singhala语言嵌入空间上引入一个参考基准。此外，作者还在Singhala-英语对齐任务上进行了一个中间任务，创建了一些Singhala-英语对齐数据集。</li>
<li>results: 虽然结果与高资源语言相比并不相匹配，但这些研究为特有的英语-Singhala嵌入对齐做出了基础。<details>
<summary>Abstract</summary>
Since their inception, embeddings have become a primary ingredient in many flavours of Natural Language Processing (NLP) tasks supplanting earlier types of representation. Even though multilingual embeddings have been used for the increasing number of multilingual tasks, due to the scarcity of parallel training data, low-resource languages such as Sinhala, tend to focus more on monolingual embeddings. Then when it comes to the aforementioned multi-lingual tasks, it is challenging to utilize these monolingual embeddings given that even if the embedding spaces have a similar geometric arrangement due to an identical training process, the embeddings of the languages considered are not aligned. This is solved by the embedding alignment task. Even in this, high-resource language pairs are in the limelight while low-resource languages such as Sinhala which is in dire need of help seem to have fallen by the wayside. In this paper, we try to align Sinhala and English word embedding spaces based on available alignment techniques and introduce a benchmark for Sinhala language embedding alignment. In addition to that, to facilitate the supervised alignment, as an intermediate task, we also introduce Sinhala-English alignment datasets. These datasets serve as our anchor datasets for supervised word embedding alignment. Even though we do not obtain results comparable to the high-resource languages such as French, German, or Chinese, we believe our work lays the groundwork for more specialized alignment between English and Sinhala embeddings.
</details>
<details>
<summary>摘要</summary>
自它们的出现以来，嵌入式已成为许多自然语言处理（NLP）任务的主要组成部分，取代了之前的类型的表示。 Although multilingual embeddings have been used for the increasing number of multilingual tasks, due to the scarcity of parallel training data, low-resource languages such as Sinhala tend to focus more on monolingual embeddings. When it comes to the aforementioned multi-lingual tasks, it is challenging to utilize these monolingual embeddings because even if the embedding spaces have a similar geometric arrangement due to an identical training process, the embeddings of the languages considered are not aligned. This is solved by the embedding alignment task. Even in this, high-resource language pairs are in the limelight while low-resource languages such as Sinhala, which is in dire need of help, seem to have fallen by the wayside. In this paper, we try to align Sinhala and English word embedding spaces based on available alignment techniques and introduce a benchmark for Sinhala language embedding alignment. In addition to that, to facilitate the supervised alignment, as an intermediate task, we also introduce Sinhala-English alignment datasets. These datasets serve as our anchor datasets for supervised word embedding alignment. Although we do not obtain results comparable to the high-resource languages such as French, German, or Chinese, we believe our work lays the groundwork for more specialized alignment between English and Sinhala embeddings.
</details></li>
</ul>
<hr>
<h2 id="Causal-Graph-in-Language-Model-Rediscovers-Cortical-Hierarchy-in-Human-Narrative-Processing"><a href="#Causal-Graph-in-Language-Model-Rediscovers-Cortical-Hierarchy-in-Human-Narrative-Processing" class="headerlink" title="Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human Narrative Processing"></a>Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human Narrative Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10431">http://arxiv.org/abs/2311.10431</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhengqi He, Taro Toyoizumi</li>
<li>for: 这个研究的目的是探索语言模型和人脑是否存在共同之处。</li>
<li>methods: 研究使用了语言模型的 causal 关系来探索信息流动的特征，并将这些特征分为两类：’低入度’ 和 ‘高入度’。</li>
<li>results: 研究发现，这两类特征的预测精度图与人脑的活动时间常数图相似，这表明语言模型和人脑在处理语言信息方面存在共同之处。<details>
<summary>Abstract</summary>
Understanding how humans process natural language has long been a vital research direction. The field of natural language processing (NLP) has recently experienced a surge in the development of powerful language models. These models have proven to be invaluable tools for studying another complex system known to process human language: the brain. Previous studies have demonstrated that the features of language models can be mapped to fMRI brain activity. This raises the question: is there a commonality between information processing in language models and the human brain? To estimate information flow patterns in a language model, we examined the causal relationships between different layers. Drawing inspiration from the workspace framework for consciousness, we hypothesized that features integrating more information would more accurately predict higher hierarchical brain activity. To validate this hypothesis, we classified language model features into two categories based on causal network measures: 'low in-degree' and 'high in-degree'. We subsequently compared the brain prediction accuracy maps for these two groups. Our results reveal that the difference in prediction accuracy follows a hierarchical pattern, consistent with the cortical hierarchy map revealed by activity time constants. This finding suggests a parallel between how language models and the human brain process linguistic information.
</details>
<details>
<summary>摘要</summary>
人类语言处理的理解已经是研究方向的一个重要领域。自然语言处理（NLP）领域在最近几年内发展出了一系列强大的语言模型。这些模型已经证明是研究人类大脑语言处理的强大工具。以前的研究表明，语言模型的特征可以与fMRI脑动activity相映射。这引起了问题：语言模型和人类大脑是否存在共同性？为了估计语言模型中信息流动的 patrern，我们研究了不同层之间的 causal 关系。受工作空间框架的启发，我们假设了语言模型中的特征可以更好地集成更多的信息，并更准确地预测高层脑动活动。为了验证这个假设，我们将语言模型的特征分为两个类别：'low in-degree' 和 'high in-degree'。然后，我们比较了这两个类别的脑预测准确率图。我们的结果表明，预测准确率之间存在层次结构，与人类大脑语言处理的 cortical hierarchy map 相符。这一发现表明了语言模型和人类大脑在处理语言信息上的并行性。
</details></li>
</ul>
<hr>
<h2 id="Bias-A-head-Analyzing-Bias-in-Transformer-Based-Language-Model-Attention-Heads"><a href="#Bias-A-head-Analyzing-Bias-in-Transformer-Based-Language-Model-Attention-Heads" class="headerlink" title="Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads"></a>Bias A-head? Analyzing Bias in Transformer-Based Language Model Attention Heads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10395">http://arxiv.org/abs/2311.10395</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yi Yang, Hanyu Duan, Ahmed Abbasi, John P. Lalor, Kar Yan Tam</li>
<li>for: 这个研究的目的是理解 pré-trained 大型自然语言模型（PLM）中的偏见机制，以便更好地评估模型的公平性和开发有效的修正策略。</li>
<li>methods: 这个研究使用了注意头分析框架，探索和识别在 PLM 中存在的偏见注意头，以及这些偏见注意头如何影响模型的偏见行为。</li>
<li>results: 研究发现，在英语语言中，两种类型的 transformer 基于 PLM 中的 gender 和 racial 偏见注意头都存在，并且这些偏见注意头在不同的模型中 exhibit 不同的行为。 结果为理解 PLM 中偏见行为带来了新的认识和指导。<details>
<summary>Abstract</summary>
Transformer-based pretrained large language models (PLM) such as BERT and GPT have achieved remarkable success in NLP tasks. However, PLMs are prone to encoding stereotypical biases. Although a burgeoning literature has emerged on stereotypical bias mitigation in PLMs, such as work on debiasing gender and racial stereotyping, how such biases manifest and behave internally within PLMs remains largely unknown. Understanding the internal stereotyping mechanisms may allow better assessment of model fairness and guide the development of effective mitigation strategies. In this work, we focus on attention heads, a major component of the Transformer architecture, and propose a bias analysis framework to explore and identify a small set of biased heads that are found to contribute to a PLM's stereotypical bias. We conduct extensive experiments to validate the existence of these biased heads and to better understand how they behave. We investigate gender and racial bias in the English language in two types of Transformer-based PLMs: the encoder-based BERT model and the decoder-based autoregressive GPT model. Overall, the results shed light on understanding the bias behavior in pretrained language models.
</details>
<details>
<summary>摘要</summary>
transformer-based pre-trained大型自然语言模型（PLM），如BERT和GPT，在NLPTask中取得了很大成功。然而，PLMs具有编码偏见的倾向。虽然一个快速发展的文献出现在PLMs中的偏见减轻中，如对性别和种族刻板印象的减轻，但内部这些偏见的manifestation和行为仍然不为人所知。更深入的理解这些偏见机制可以帮助评估模型公正性，并指导开发有效的减轻策略。在这项工作中，我们专注于Transformer架构中的注意头，并提出一种偏见分析框架，以探索和确定PLM中的偏见注意头。我们进行了广泛的实验 validate the existence of these biased heads and better understand how they behave。我们investigate gender和种族偏见在英语语言中的两种Transformer-based PLMs：BERT模型和GPT模型。总的来说，结果 shed light on understanding pretrained语言模型的偏见行为。
</details></li>
</ul>
<hr>
<h2 id="FOAL-Fine-grained-Contrastive-Learning-for-Cross-domain-Aspect-Sentiment-Triplet-Extraction"><a href="#FOAL-Fine-grained-Contrastive-Learning-for-Cross-domain-Aspect-Sentiment-Triplet-Extraction" class="headerlink" title="FOAL: Fine-grained Contrastive Learning for Cross-domain Aspect Sentiment Triplet Extraction"></a>FOAL: Fine-grained Contrastive Learning for Cross-domain Aspect Sentiment Triplet Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10373">http://arxiv.org/abs/2311.10373</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting Xu, Zhen Wu, Huiyun Yang, Xinyu Dai</li>
<li>for: 本研究旨在提高半精度抽象 Sentiment Triplet Extraction (ASTE) 的性能，使其可以在不充分的标注数据下进行预测。</li>
<li>methods: 本研究提议使用 Fine-grained cOntrAstive Learning (FOAL) 方法来减少频率域差异，保持每个类别的抽象性，从而提高 ASTE 的性能。</li>
<li>results: 对六个转移对比试验表明，FOAL 方法可以提高 ASTE 的性能达6%，同时显著减少频率域差异。<details>
<summary>Abstract</summary>
Aspect Sentiment Triplet Extraction (ASTE) has achieved promising results while relying on sufficient annotation data in a specific domain. However, it is infeasible to annotate data for each individual domain. We propose to explore ASTE in the cross-domain setting, which transfers knowledge from a resource-rich source domain to a resource-poor target domain, thereby alleviating the reliance on labeled data in the target domain. To effectively transfer the knowledge across domains and extract the sentiment triplets accurately, we propose a method named Fine-grained cOntrAstive Learning (FOAL) to reduce the domain discrepancy and preserve the discriminability of each category. Experiments on six transfer pairs show that FOAL achieves 6% performance gains and reduces the domain discrepancy significantly compared with strong baselines. Our code will be publicly available once accepted.
</details>
<details>
<summary>摘要</summary>
ASTE在Specific domain中得到了有前途的结果，但是在每个域名下annotate数据是不可能的。我们提议在cross-domain Setting中运用ASTE，将知识从resource-rich的源域传播到resource-poor的目标域，从而减轻目标域的标注数据的依赖。为了有效地在域之间传递知识和准确地提取情感 triplets，我们提出了一种方法 named Fine-grained cOntrAstive Learning (FOAL)，以减少域之间的差异和保持每个类别的抽象能力。我们在六个转移对的实验中发现，FOAL可以提高性能6%，同时显著减少域之间的差异。我们的代码将在接受后公开。
</details></li>
</ul>
<hr>
<h2 id="Exploring-the-Relationship-between-In-Context-Learning-and-Instruction-Tuning"><a href="#Exploring-the-Relationship-between-In-Context-Learning-and-Instruction-Tuning" class="headerlink" title="Exploring the Relationship between In-Context Learning and Instruction Tuning"></a>Exploring the Relationship between In-Context Learning and Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10367">http://arxiv.org/abs/2311.10367</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, Kar Yan Tam</li>
<li>for: 本研究旨在探讨受限语言模型（LLM）下批处理应用的两种主要方法——受限语言模型（ICL）和指导调整（IT）之间的关系。</li>
<li>methods: 本研究使用了LLaMA-2（7B和13B）进行了精心的实验，以explore ICL和IT之间的关系。</li>
<li>results: 研究发现，ICL和IT都会改变LLM的隐藏状态，但ICL是IT的隐藏状态改变的假设形式。此外，ICL和IT的融合程度受到提供示例的多种因素的影响。本研究提供了对LLM行为的新的理解。<details>
<summary>Abstract</summary>
In-Context Learning (ICL) and Instruction Tuning (IT) are two primary paradigms of adopting Large Language Models (LLMs) to downstream applications. However, they are significantly different. In ICL, a set of demonstrations are provided at inference time but the LLM's parameters are not updated. In IT, a set of demonstrations are used to tune LLM's parameters in training time but no demonstrations are used at inference time. Although a growing body of literature has explored ICL and IT, studies on these topics have largely been conducted in isolation, leading to a disconnect between these two paradigms. In this work, we explore the relationship between ICL and IT by examining how the hidden states of LLMs change in these two paradigms. Through carefully designed experiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit IT. In other words, ICL changes an LLM's hidden states as if the demonstrations were used to instructionally tune the model. Furthermore, the convergence between ICL and IT is largely contingent upon several factors related to the provided demonstrations. Overall, this work offers a unique perspective to explore the connection between ICL and IT and sheds light on understanding the behaviors of LLM.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在下游应用中采用两种主要方法：卷积学习（ICL）和指导调整（IT）。然而，这两种方法有所不同。在ICL中，批处时提供示例，但LLM的参数未更新。在IT中，用示例进行模型训练时更新LLM的参数，但在批处时不使用示例。虽然有一部分文献研究了ICL和IT，但这两个领域的研究几乎孤立地进行，导致ICL和IT之间的连接受到了挑战。在这项工作中，我们研究了ICL和IT之间的关系，并通过对LLaMA-2（7B和13B）进行精心的实验，发现ICL是卷积学习的隐藏状态的变化。即ICL类似于在训练时使用示例进行调整LLM的模型。此外，ICL和IT的协调性受示例提供的多个因素的影响。总之，本工作提供了研究ICL和IT之间连接的新视角，并照明了LLM的行为。
</details></li>
</ul>
<hr>
<h2 id="Complementary-Advantages-of-ChatGPTs-and-Human-Readers-in-Reasoning-Evidence-from-English-Text-Reading-Comprehension"><a href="#Complementary-Advantages-of-ChatGPTs-and-Human-Readers-in-Reasoning-Evidence-from-English-Text-Reading-Comprehension" class="headerlink" title="Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension"></a>Complementary Advantages of ChatGPTs and Human Readers in Reasoning: Evidence from English Text Reading Comprehension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10344">http://arxiv.org/abs/2311.10344</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tongquan Zhou, Yao Zhang, Siyi Cao, Yulu Li, Tao Wang</li>
<li>for:  investigate how ChatGPTs and Chinese senior school students exhibited their reasoning ability from English narrative texts.</li>
<li>methods:  used three reasoning tests: Test 1 for commonsense inference, Test 2 for emotional inference, and Test 3 for causal inference.</li>
<li>results:  ChatGPTs outperformed the students in daily-life inferences and positive emotions, but the students showed superiority in negative emotions and logical analysis. ChatGPT Plus excelled in updating command condition.<details>
<summary>Abstract</summary>
ChatGPT has shown its great power in text processing, including its reasoning ability from text reading. However, there has not been any direct comparison between human readers and ChatGPT in reasoning ability related to text reading. This study was undertaken to investigate how ChatGPTs (i.e., ChatGPT and ChatGPT Plus) and Chinese senior school students as ESL learners exhibited their reasoning ability from English narrative texts. Additionally, we compared the two ChatGPTs in the reasoning performances when commands were updated elaborately. The whole study was composed of three reasoning tests: Test 1 for commonsense inference, Test 2 for emotional inference, and Test 3 for causal inference. The results showed that in Test 1, the students outdid the two ChatGPT versions in local-culture-related inferences but performed worse than the chatbots in daily-life inferences. In Test 2, ChatGPT Plus excelled whereas ChatGPT lagged behind in accuracy. In association with both accuracy and frequency of correct responses, the students were inferior to the two chatbots. Compared with ChatGPTs' better performance in positive emotions, the students showed their superiority in inferring negative emotions. In Test 3, the students demonstrated better logical analysis, outdoing both chatbots. In updating command condition, ChatGPT Plus displayed good causal reasoning ability while ChatGPT kept unchanged. Our study reveals that human readers and ChatGPTs have their respective advantages and disadvantages in drawing inferences from text reading comprehension, unlocking a complementary relationship in text-based reasoning.
</details>
<details>
<summary>摘要</summary>
chatGPT 在文本处理方面表现出了强大的能力，包括从文本阅读中的理解能力。然而，直到现在，没有直接比较人类阅读者和 chatGPT 在文本阅读中的理解能力。这项研究的目的是 investigate  chatGPT 和中国高中生作为 ESOL 学习者在英文故事文本中的理解能力。此外，我们还比较了两个 chatGPT 版本在命令更新后的理解性能。整个研究包括三个理解测验：测验 1 为常识推理，测验 2 为情感推理，测验 3 为 causal 推理。结果表明，在测验 1 中，学生在本地文化相关的推理方面表现出色，但在日常生活相关的推理方面表现落后于 chatbot。在测验 2 中， chatGPT 加 plus 版本表现出色，而 chatGPT 版本则落后于 accuracy。与 accuracy 和正确回答频率相关，学生比 chatbot 弱。在测验 3 中，学生表现出色，在 logical analysis 方面胜过两个 chatbot。在命令更新后的情况下， chatGPT 加 plus 版本表现出色，而 chatGPT 版本保持不变。我们的研究表明，人类阅读者和 chatGPT 在文本阅读中的理解能力各有优劣，可以形成 complementary 关系。
</details></li>
</ul>
<hr>
<h2 id="Prompt-Pool-based-Class-Incremental-Continual-Learning-for-Dialog-State-Tracking"><a href="#Prompt-Pool-based-Class-Incremental-Continual-Learning-for-Dialog-State-Tracking" class="headerlink" title="Prompt Pool based Class-Incremental Continual Learning for Dialog State Tracking"></a>Prompt Pool based Class-Incremental Continual Learning for Dialog State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10271">http://arxiv.org/abs/2311.10271</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/thu-spmi/ppt2dst">https://github.com/thu-spmi/ppt2dst</a></li>
<li>paper_authors: Hong Liu, Yucheng Cai, Yuan Zhou, Zhijian Ou, Yi Huang, Junlan Feng</li>
<li>for: 本研究旨在Addressing continual learning of dialog state tracking (DST) in the class-incremental scenario, where task identities are unknown during testing.</li>
<li>methods: 我们提出使用提问池方法，维护一个包含键值对的提问池，根据对话历史和提问键的距离选择提问。该方法可以自动识别任务并选择合适的提问 during testing.</li>
<li>results: 我们在Schema-Guided Dialog dataset (SGD) 和一个真实世界对话应用中的实验结果表明，提问池方法可以获得远高于基eline的联合目标准确率。并且将该方法与储备缓存结合，可以进一步提高模型性能。<details>
<summary>Abstract</summary>
Continual learning is crucial for dialog state tracking (DST) in dialog systems, since requirements from users for new functionalities are often encountered. However, most of existing continual learning methods for DST require task identities during testing, which is a severe limit in real-world applications. In this paper, we aim to address continual learning of DST in the class-incremental scenario (namely the task identity is unknown in testing). Inspired by the recently emerging prompt tuning method that performs well on dialog systems, we propose to use the prompt pool method, where we maintain a pool of key-value paired prompts and select prompts from the pool according to the distance between the dialog history and the prompt keys. The proposed method can automatically identify tasks and select appropriate prompts during testing. We conduct experiments on Schema-Guided Dialog dataset (SGD) and another dataset collected from a real-world dialog application. Experiment results show that the prompt pool method achieves much higher joint goal accuracy than the baseline. After combining with a rehearsal buffer, the model performance can be further improved.
</details>
<details>
<summary>摘要</summary>
Inspired by the recently emerging prompt tuning method that performs well on dialog systems, we propose the prompt pool method. We maintain a pool of key-value paired prompts and select prompts based on the distance between the dialog history and the prompt keys. The proposed method can automatically identify tasks and select appropriate prompts during testing.We conduct experiments on the Schema-Guided Dialog dataset (SGD) and a real-world dialog application dataset. Experiment results show that the prompt pool method achieves much higher joint goal accuracy than the baseline. By combining with a rehearsal buffer, the model performance can be further improved.
</details></li>
</ul>
<hr>
<h2 id="Energy-and-Carbon-Considerations-of-Fine-Tuning-BERT"><a href="#Energy-and-Carbon-Considerations-of-Fine-Tuning-BERT" class="headerlink" title="Energy and Carbon Considerations of Fine-Tuning BERT"></a>Energy and Carbon Considerations of Fine-Tuning BERT</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10267">http://arxiv.org/abs/2311.10267</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle Friedler, Sasha Luccioni</li>
<li>for: This paper aims to provide a comprehensive understanding of the energy and carbon footprint of fine-tuning in NLP, in order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions.</li>
<li>methods: The paper uses a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure, and measurement modalities to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference.</li>
<li>results: The paper outlines recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.Here is the same information in Simplified Chinese text:</li>
<li>for: 这篇论文目的是为了更好地了解 NLP 中 fine-tuning 的能量和碳脚印，以更好地了解 fine-tuning 在能量和碳脚印领域中的角色。</li>
<li>methods: 这篇论文使用了对各种任务、数据集、硬件基础设施和测量方式的计算成本的仔细实验研究，以将 fine-tuning 的能量和碳脚印放在 pre-training 和推理中的视野中。</li>
<li>results: 这篇论文为 NLP 研究者和实践者提供了提高 fine-tuning 能效的建议。<details>
<summary>Abstract</summary>
Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Diagnosing-and-Debiasing-Corpus-Based-Political-Bias-and-Insults-in-GPT2"><a href="#Diagnosing-and-Debiasing-Corpus-Based-Political-Bias-and-Insults-in-GPT2" class="headerlink" title="Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2"></a>Diagnosing and Debiasing Corpus-Based Political Bias and Insults in GPT2</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10266">http://arxiv.org/abs/2311.10266</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ambri Ma, Arnav Kumar, Brett Zeligson</li>
<li>for: This study aims to investigate the effectiveness of a decoding algorithm in mitigating insults and political bias in generated text, with the goal of contributing to the ongoing effort of examining the ethical and social implications of human-AI interaction.</li>
<li>methods: The study uses generative pretrained transformer (GPT) language models that have the ability to recognize and detect toxicity in generated content, and a decoding algorithm that allows the models to self-debias and reduce the likelihood of generating harmful text.</li>
<li>results: The study aims to evaluate the efficacy of the diagnosing-debiasing approach in mitigating insults and political bias in generated text, and contribute to the ongoing effort of understanding the ethical and social implications of human-AI interaction.<details>
<summary>Abstract</summary>
The training of large language models (LLMs) on extensive, unfiltered corpora sourced from the internet is a common and advantageous practice. Consequently, LLMs have learned and inadvertently reproduced various types of biases, including violent, offensive, and toxic language. However, recent research shows that generative pretrained transformer (GPT) language models can recognize their own biases and detect toxicity in generated content, a process referred to as self-diagnosis. In response, researchers have developed a decoding algorithm that allows LLMs to self-debias, or reduce their likelihood of generating harmful text. This study investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias. These biases are often used interchangeably in discourse, despite exhibiting potentially dissimilar semantic and syntactic properties. We aim to contribute to the ongoing effort of investigating the ethical and social implications of human-AI interaction.
</details>
<details>
<summary>摘要</summary>
培训大型语言模型（LLM）在互联网上广泛、未经过滤的 corpora 上进行常见和有利的做法。因此，LLM 已经学习并不意气地复制了各种偏见，包括暴力、袋判和毒害的语言。然而，最近的研究表明，生成预训练 transformer（GPT）语言模型可以识别自己的偏见并检测生成内容中的毒害，一个过程称为自我诊断。为此，研究人员已经开发了一种解码算法，allowing LLMs 自我减偏，即减少生成危险的文本的可能性。本研究investigates the efficacy of the diagnosing-debiasing approach in mitigating two additional types of biases: insults and political bias。这些偏见通常在DISCOURSE中被混用，尽管它们可能具有不同的semantic和 sintactic 特征。我们想要贡献到人机交互的伦理和社会因素的ongoing 探索中。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/cs.CL_2023_11_17/" data-id="clpztdnf700f8es884i3q5yn8" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.LG_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/cs.LG_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T10:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-LG/">cs.LG</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/cs.LG_2023_11_17/">cs.LG - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Equivariant-Neural-Operator-Learning-with-Graphon-Convolution"><a href="#Equivariant-Neural-Operator-Learning-with-Graphon-Convolution" class="headerlink" title="Equivariant Neural Operator Learning with Graphon Convolution"></a>Equivariant Neural Operator Learning with Graphon Convolution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10908">http://arxiv.org/abs/2311.10908</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ccr-cheng/infgcn-pytorch">https://github.com/ccr-cheng/infgcn-pytorch</a></li>
<li>paper_authors: Chaoran Cheng, Jian Peng</li>
<li>For: 学习3D欧几何空间中函数之间的映射。* Methods:  combinatorial学习方案和差异运算层，保证SE(3)-等变征性。从图谱视角来看，我们的方法可以看作是图像 convolution on graphons，我们称之为InfGCN。* Results: 在大规模电子密度数据集上进行了广泛的实验，与当前状态艺术体系相比，our model表现出了显著的优异性。多个缺省研究也进行了，以证明提案的建筑的效果。<details>
<summary>Abstract</summary>
We propose a general architecture that combines the coefficient learning scheme with a residual operator layer for learning mappings between continuous functions in the 3D Euclidean space. Our proposed model is guaranteed to achieve SE(3)-equivariance by design. From the graph spectrum view, our method can be interpreted as convolution on graphons (dense graphs with infinitely many nodes), which we term InfGCN. By leveraging both the continuous graphon structure and the discrete graph structure of the input data, our model can effectively capture the geometric information while preserving equivariance. Through extensive experiments on large-scale electron density datasets, we observed that our model significantly outperformed the current state-of-the-art architectures. Multiple ablation studies were also carried out to demonstrate the effectiveness of the proposed architecture.
</details>
<details>
<summary>摘要</summary>
我们提出了一种通用的建筑方案，这种方案结合了系数学习方案和差分运算层来学习3D欧几何空间中函数的映射。我们的提议的模型由设计 garantizado SE(3)-等价性。从图spectrum的视角来看，我们的方法可以被解释为 dense graphs（有无穷多个节点的图）上的卷积，我们称之为InfGCN。通过利用 continues graphon 结构和输入数据的树状结构，我们的模型可以有效地捕捉几何信息，同时保持等价性。经过对大规模电子密度数据集进行了广泛的实验，我们发现我们的模型可以明显超越当前状态的体系。此外，我们还进行了多个缺省研究来证明提案的效果。
</details></li>
</ul>
<hr>
<h2 id="A-Whole-New-Ball-Game-A-Primal-Accelerated-Method-for-Matrix-Games-and-Minimizing-the-Maximum-of-Smooth-Functions"><a href="#A-Whole-New-Ball-Game-A-Primal-Accelerated-Method-for-Matrix-Games-and-Minimizing-the-Maximum-of-Smooth-Functions" class="headerlink" title="A Whole New Ball Game: A Primal Accelerated Method for Matrix Games and Minimizing the Maximum of Smooth Functions"></a>A Whole New Ball Game: A Primal Accelerated Method for Matrix Games and Minimizing the Maximum of Smooth Functions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10886">http://arxiv.org/abs/2311.10886</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yair Carmon, Arun Jambulapati, Yujia Jin, Aaron Sidford</li>
<li>for: 本文为了最小化 $\max_{i\in[n]} f_i(x)$ over $d$-dimensional Euclidean or simplex domain 的问题提出了算法。</li>
<li>methods: 本文使用了三个新的基本 primitives：（1）一个动态数据结构，可以高效地在小 $\ell_2$ 或 $\ell_1$ 球中估计随机梯度；（2）一种适应这个数据结构的镜投影算法，可以在这些球中最小化目标函数；（3）一个适用于非欧几何的简单球观测框架。</li>
<li>results: 本文的算法可以在 $n$ 较大时，在 $\epsilon$-approximate 的情况下，使用 $\widetilde{O}(n \epsilon^{-1&#x2F;3} + \epsilon^{-2})$ 梯度和函数评估，以及 $\widetilde{O}(n \epsilon^{-4&#x2F;3})$ 的额外时间来解决该问题。在特定的特例中，当每个 $f_i$ 是线性函数时，本文的算法可以在runtime $\widetilde{O}(n (d&#x2F;\epsilon)^{2&#x2F;3} + nd + d\epsilon^{-2})$ 内获得 $\epsilon$-approximate 解决方案。这在 $n&gt;d$ 和 $\epsilon&#x3D;1&#x2F;\sqrt{n}$ 时超过所有已知的第一个方法。<details>
<summary>Abstract</summary>
We design algorithms for minimizing $\max_{i\in[n]} f_i(x)$ over a $d$-dimensional Euclidean or simplex domain. When each $f_i$ is $1$-Lipschitz and $1$-smooth, our method computes an $\epsilon$-approximate solution using $\widetilde{O}(n \epsilon^{-1/3} + \epsilon^{-2})$ gradient and function evaluations, and $\widetilde{O}(n \epsilon^{-4/3})$ additional runtime. For large $n$, our evaluation complexity is optimal up to polylogarithmic factors. In the special case where each $f_i$ is linear -- which corresponds to finding a near-optimal primal strategy in a matrix game -- our method finds an $\epsilon$-approximate solution in runtime $\widetilde{O}(n (d/\epsilon)^{2/3} + nd + d\epsilon^{-2})$. For $n>d$ and $\epsilon=1/\sqrt{n}$ this improves over all existing first-order methods. When additionally $d = \omega(n^{8/11})$ our runtime also improves over all known interior point methods.   Our algorithm combines three novel primitives: (1) A dynamic data structure which enables efficient stochastic gradient estimation in small $\ell_2$ or $\ell_1$ balls. (2) A mirror descent algorithm tailored to our data structure implementing an oracle which minimizes the objective over these balls. (3) A simple ball oracle acceleration framework suitable for non-Euclidean geometry.
</details>
<details>
<summary>摘要</summary>
我们设计算法以最小化 $\max_{i\in[n]} f_i(x)$ 在 $d$-维欧几何或简单体领域上。当每个 $f_i$ 是 $1$-Lipschitz 和 $1$-smooth 时，我们的方法可以在 $\widetilde{O}(n \epsilon^{-1/3} + \epsilon^{-2})$ 梯度和函数评估和 $\widetilde{O}(n \epsilon^{-4/3})$ 额外时间下 Compute an $\epsilon$-approximate solution。对于大 $n$，我们的评估复杂度是最佳的，仅仅带有极小的多项式因子。在特殊情况下，每个 $f_i$ 是线性的情况下（即找到一个近似最佳 primal 策略在矩阵游戏中），我们的方法可以在 runtime $\widetilde{O}(n (d/\epsilon)^{2/3} + nd + d\epsilon^{-2})$ 下Compute an $\epsilon$-approximate solution。当 $n>d$ 且 $\epsilon=1/\sqrt{n}$ 时，我们的时间复杂度超过所有已知的首ORDER方法。另外，当 $d = \omega(n^{8/11})$ 时，我们的时间复杂度也超过所有已知的内部点方法。我们的算法结合了三个新的基本 primitives：1. 一个动态数据结构，可以实现高效的随机梯度估计在小 $\ell_2$ 或 $\ell_1$ 球上。2. 一个镜像下降算法，适应我们的数据结构，实现一个函数实现器，可以实现这些球上的目标最小化。3. 一个简单的球观点增强框架，适合非欧几何。I hope this helps! Let me know if you have any questions or need further clarification.
</details></li>
</ul>
<hr>
<h2 id="A-Quadratic-Speedup-in-Finding-Nash-Equilibria-of-Quantum-Zero-Sum-Games"><a href="#A-Quadratic-Speedup-in-Finding-Nash-Equilibria-of-Quantum-Zero-Sum-Games" class="headerlink" title="A Quadratic Speedup in Finding Nash Equilibria of Quantum Zero-Sum Games"></a>A Quadratic Speedup in Finding Nash Equilibria of Quantum Zero-Sum Games</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10859">http://arxiv.org/abs/2311.10859</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francisca Vasconcelos, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Panayotis Mertikopoulos, Georgios Piliouras, Michael I. Jordan</li>
<li>for:  quantum zero-sum games</li>
<li>methods:  hierarchy of quantum optimization algorithms, including Optimistic Matrix Multiplicative Weights Update (OMMWU) algorithm</li>
<li>results:  quadratic speed-up relative to previous algorithm, with an average-iterate convergence complexity of $\mathcal{O}(d&#x2F;\epsilon)$ iterations to $\epsilon$-Nash equilibria.<details>
<summary>Abstract</summary>
Recent developments in domains such as non-local games, quantum interactive proofs, and quantum generative adversarial networks have renewed interest in quantum game theory and, specifically, quantum zero-sum games. Central to classical game theory is the efficient algorithmic computation of Nash equilibria, which represent optimal strategies for both players. In 2008, Jain and Watrous proposed the first classical algorithm for computing equilibria in quantum zero-sum games using the Matrix Multiplicative Weight Updates (MMWU) method to achieve a convergence rate of $\mathcal{O}(d/\epsilon^2)$ iterations to $\epsilon$-Nash equilibria in the $4^d$-dimensional spectraplex. In this work, we propose a hierarchy of quantum optimization algorithms that generalize MMWU via an extra-gradient mechanism. Notably, within this proposed hierarchy, we introduce the Optimistic Matrix Multiplicative Weights Update (OMMWU) algorithm and establish its average-iterate convergence complexity as $\mathcal{O}(d/\epsilon)$ iterations to $\epsilon$-Nash equilibria. This quadratic speed-up relative to Jain and Watrous' original algorithm sets a new benchmark for computing $\epsilon$-Nash equilibria in quantum zero-sum games.
</details>
<details>
<summary>摘要</summary>
In this work, we propose a hierarchy of quantum optimization algorithms that generalize MMWU via an extra-gradient mechanism. Notably, within this proposed hierarchy, we introduce the Optimistic Matrix Multiplicative Weights Update (OMMWU) algorithm and establish its average-iterate convergence complexity as $\mathcal{O}(d/\epsilon)$ iterations to $\epsilon$-Nash equilibria. This represents a quadratic speed-up relative to Jain and Watrous' original algorithm, setting a new benchmark for computing $\epsilon$-Nash equilibria in quantum zero-sum games.
</details></li>
</ul>
<hr>
<h2 id="Accelerating-L-shaped-Two-stage-Stochastic-SCUC-with-Learning-Integrated-Benders-Decomposition"><a href="#Accelerating-L-shaped-Two-stage-Stochastic-SCUC-with-Learning-Integrated-Benders-Decomposition" class="headerlink" title="Accelerating L-shaped Two-stage Stochastic SCUC with Learning Integrated Benders Decomposition"></a>Accelerating L-shaped Two-stage Stochastic SCUC with Learning Integrated Benders Decomposition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10835">http://arxiv.org/abs/2311.10835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fouad Hasan, Amin Kargarian</li>
<li>for: 解决大型杂integer问题</li>
<li>methods: 使用机器学习提出了增强版Benders分解法，用于解决两阶段随机安全绑定单机制作业（SCUC）问题</li>
<li>results: 通过创建紧张的割和减少主问题的大小，提高Benders分解法的计算成本和内存使用情况。三种方法被提出，即回归Benders、分类Benders和回归-分类Benders。一个回归器读取负荷profile场景，预测子问题目标函数代理变量，形成紧张割。定义一个标准来衡量割的用于下界提高的水平。用于割的有用性被定义，并在含有分类学习器和无分类学习器两种情况下进行评估。有用割逐渐添加到主问题中，非有用割则被抛弃，以降低Benders迭代中的计算负担。多个测试系统的实践研究显示了提案的学习帮助Benders分解法在比传统多割Benders分解法更有效地解决两阶段SCUC问题。<details>
<summary>Abstract</summary>
Benders decomposition is widely used to solve large mixed-integer problems. This paper takes advantage of machine learning and proposes enhanced variants of Benders decomposition for solving two-stage stochastic security-constrained unit commitment (SCUC). The problem is decomposed into a master problem and subproblems corresponding to a load scenario. The goal is to reduce the computational costs and memory usage of Benders decomposition by creating tighter cuts and reducing the size of the master problem. Three approaches are proposed, namely regression Benders, classification Benders, and regression-classification Benders. A regressor reads load profile scenarios and predicts subproblem objective function proxy variables to form tighter cuts for the master problem. A criterion is defined to measure the level of usefulness of cuts with respect to their contribution to lower bound improvement. Useful cuts that contain the necessary information to form the feasible region are identified with and without a classification learner. Useful cuts are iteratively added to the master problem, and non-useful cuts are discarded to reduce the computational burden of each Benders iteration. Simulation studies on multiple test systems show the effectiveness of the proposed learning-aided Benders decomposition for solving two-stage SCUC as compared to conventional multi-cut Benders decomposition.
</details>
<details>
<summary>摘要</summary>
<<SYS> translate into Simplified Chinese</SYS>бендер的分解广泛应用于解决大规模杂合integer问题。这篇论文利用机器学习技术，提出了增强版本的本дер分解方法，用于解决两个阶段随机安全约束Unit Commitment（SCUC）问题。问题被分解成主问题和相应的负荷enario子问题。目标是通过创建更紧张的割和减少主问题的大小来降低本дер分解的计算成本和内存使用。三种方法被提出，即回归本дер、分类本дер和回归分类本дер。一个回归器读取负荷profile scenario，预测子问题目标函数假变量，以形成更紧张的割。一个 criterion 是定义用于测量割与 respect to its contribution to lower bound improvement的水平。有用的割是指包含必要信息来形成可行区的割，而不需要分类学习。有用的割在主问题中迭代添加，不用的割则被抛弃，以降低每个本дер迭代的计算负担。多个测试系统的 simulate 研究表明，提出的学习帮助的本дер分解方法可以与传统的多割本дер分解方法相比，更高效地解决两个阶段SCUC问题。
</details></li>
</ul>
<hr>
<h2 id="Machine-learning-phase-transitions-Connections-to-the-Fisher-information"><a href="#Machine-learning-phase-transitions-Connections-to-the-Fisher-information" class="headerlink" title="Machine learning phase transitions: Connections to the Fisher information"></a>Machine learning phase transitions: Connections to the Fisher information</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10710">http://arxiv.org/abs/2311.10710</a></li>
<li>repo_url: None</li>
<li>paper_authors: Julian Arnold, Niels Lörch, Flemming Holtorf, Frank Schäfer</li>
<li>for: 本研究旨在解释机器学习技术用于检测数据中的相转移的工作原理和基本限制。</li>
<li>methods: 本研究使用信息geometry的工具来证明机器学习指标可以用来判断数据中的相转移。</li>
<li>results: 研究证明了机器学习指标对数据中相转移的精度，并通过数值示范了这些指标在 классиical和量子系统中的性能。<details>
<summary>Abstract</summary>
Despite the widespread use and success of machine-learning techniques for detecting phase transitions from data, their working principle and fundamental limits remain elusive. Here, we explain the inner workings and identify potential failure modes of these techniques by rooting popular machine-learning indicators of phase transitions in information-theoretic concepts. Using tools from information geometry, we prove that several machine-learning indicators of phase transitions approximate the square root of the system's (quantum) Fisher information from below -- a quantity that is known to indicate phase transitions but is often difficult to compute from data. We numerically demonstrate the quality of these bounds for phase transitions in classical and quantum systems.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:尽管机器学习技术在数据上检测phasetransition的广泛使用和成功，它们的工作原理和基本限制仍然未知。在这里，我们解释这些技术的内部工作和 potential failure modes，并将它们基于信息理论概念。使用信息几何工具，我们证明了一些机器学习phasetransition的指标 approximate系统的(量子) Fisher信息的平方根从下面 - 这是已知能指示phasetransition的量，但是从数据中计算很难。我们 numerically示出了这些下界的质量 для classical和quantum系统的phasetransition.
</details></li>
</ul>
<hr>
<h2 id="Optimal-Embedding-Dimension-for-Sparse-Subspace-Embeddings"><a href="#Optimal-Embedding-Dimension-for-Sparse-Subspace-Embeddings" class="headerlink" title="Optimal Embedding Dimension for Sparse Subspace Embeddings"></a>Optimal Embedding Dimension for Sparse Subspace Embeddings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10680">http://arxiv.org/abs/2311.10680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shabarish Chenakkod, Michał Dereziński, Xiaoyu Dong, Mark Rudelson</li>
<li>for: The paper is written to address the main open question posed by Nelson and Nguyen (FOCS 2013) on the embedding dimension of oblivious subspace embeddings (OSEs) and to improve on the previous results by Cohen (SODA 2016).</li>
<li>methods: The paper uses a random matrix with randomly sparsified $\pm1&#x2F;\sqrt s$ entries and having $s&#x3D; O(\log^4(d))$ non-zeros per column to construct an OSE with $\epsilon &#x3D; O_{\theta}(1)$.</li>
<li>results: The paper shows that the proposed OSE has an embedding dimension of $m&#x3D;O(d)$ and achieves a distortion of $\epsilon &#x3D; O_{\theta}(1)$, which improves on the previous results of $m&#x3D;O(d\log(d))$ and $\epsilon &#x3D; O(1)$ respectively. Additionally, the paper presents an optimal single-pass algorithm for least squares regression using the proposed OSE.<details>
<summary>Abstract</summary>
A random $m\times n$ matrix $S$ is an oblivious subspace embedding (OSE) with parameters $\epsilon>0$, $\delta\in(0,1/3)$ and $d\leq m\leq n$, if for any $d$-dimensional subspace $W\subseteq R^n$,   $P\big(\,\forall_{x\in W}\ (1+\epsilon)^{-1}\|x\|\leq\|Sx\|\leq (1+\epsilon)\|x\|\,\big)\geq 1-\delta.$   It is known that the embedding dimension of an OSE must satisfy $m\geq d$, and for any $\theta > 0$, a Gaussian embedding matrix with $m\geq (1+\theta) d$ is an OSE with $\epsilon = O_\theta(1)$. However, such optimal embedding dimension is not known for other embeddings. Of particular interest are sparse OSEs, having $s\ll m$ non-zeros per column, with applications to problems such as least squares regression and low-rank approximation.   We show that, given any $\theta > 0$, an $m\times n$ random matrix $S$ with $m\geq (1+\theta)d$ consisting of randomly sparsified $\pm1/\sqrt s$ entries and having $s= O(\log^4(d))$ non-zeros per column, is an oblivious subspace embedding with $\epsilon = O_{\theta}(1)$. Our result addresses the main open question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse OSEs can achieve $m=O(d)$ embedding dimension, and it improves on $m=O(d\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first oblivious subspace embedding with $O(d)$ embedding dimension that can be applied faster than current matrix multiplication time, and to obtain an optimal single-pass algorithm for least squares regression. We further extend our results to construct even sparser non-oblivious embeddings, leading to the first subspace embedding with low distortion $\epsilon=o(1)$ and optimal embedding dimension $m=O(d/\epsilon^2)$ that can be applied in current matrix multiplication time.
</details>
<details>
<summary>摘要</summary>
一个随机矩阵 $S$ 是一个透彻空间嵌入 (OSE)， Parameters $\epsilon>0$, $\delta\in(0,1/3)$ 和 $d\leq m\leq n$。如果任何 $d$-维子空间 $W\subseteq \mathbb{R}^n$ 上，then $P\big(\,\forall_{x\in W}\ (1+\epsilon)^{-1}\|x\|\leq\|Sx\|\leq (1+\epsilon)\|x\|\,\big)\geq 1-\delta.$  It is known that the embedding dimension of an OSE must satisfy $m\geq d$, and for any $\theta > 0$, a Gaussian embedding matrix with $m\geq (1+\theta) d$ is an OSE with $\epsilon = O_\theta(1)$. However, the optimal embedding dimension is not known for other embeddings. Of particular interest are sparse OSEs, having $s\ll m$ non-zeros per column, with applications to problems such as least squares regression and low-rank approximation.We show that, given any $\theta > 0$, an $m\times n$ random matrix $S$ with $m\geq (1+\theta)d$ consisting of randomly sparsified $\pm1/\sqrt s$ entries and having $s= O(\log^4(d))$ non-zeros per column, is an oblivious subspace embedding with $\epsilon = O_{\theta}(1)$. Our result addresses the main open question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse OSEs can achieve $m=O(d)$ embedding dimension, and it improves on $m=O(d\log(d))$ shown by Cohen (SODA 2016). We use this to construct the first oblivious subspace embedding with $O(d)$ embedding dimension that can be applied faster than current matrix multiplication time, and to obtain an optimal single-pass algorithm for least squares regression. We further extend our results to construct even sparser non-oblivious embeddings, leading to the first subspace embedding with low distortion $\epsilon=o(1)$ and optimal embedding dimension $m=O(d/\epsilon^2)$ that can be applied in current matrix multiplication time.
</details></li>
</ul>
<hr>
<h2 id="Multiparameter-Persistent-Homology-for-Molecular-Property-Prediction"><a href="#Multiparameter-Persistent-Homology-for-Molecular-Property-Prediction" class="headerlink" title="Multiparameter Persistent Homology for Molecular Property Prediction"></a>Multiparameter Persistent Homology for Molecular Property Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10808">http://arxiv.org/abs/2311.10808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andac Demir, Bulent Kiziltan</li>
<li>for: This paper presents a novel method for generating molecular fingerprints based on multiparameter persistent homology, which reveals the latent structures and relationships within molecular geometry and detects topological features that exhibit persistence across multiple scales.</li>
<li>methods: The proposed fingerprinting method uses multiparameter persistent homology, which is a more comprehensive and interpretable approach than traditional graph neural networks. The method incorporates multiple parameters such as atomic mass, partial charge, and bond type, and can be further enhanced by incorporating additional parameters.</li>
<li>results: The proposed method has been demonstrated to be effective in predicting molecular properties through extensive experiments on the Lipophilicity, FreeSolv, and ESOL datasets. The method provides fresh perspectives on molecular structure that are not easily discernible from single-parameter or single-scale analysis.<details>
<summary>Abstract</summary>
In this study, we present a novel molecular fingerprint generation method based on multiparameter persistent homology. This approach reveals the latent structures and relationships within molecular geometry, and detects topological features that exhibit persistence across multiple scales along multiple parameters, such as atomic mass, partial charge, and bond type, and can be further enhanced by incorporating additional parameters like ionization energy, electron affinity, chirality and orbital hybridization. The proposed fingerprinting method provides fresh perspectives on molecular structure that are not easily discernible from single-parameter or single-scale analysis. Besides, in comparison with traditional graph neural networks, multiparameter persistent homology has the advantage of providing a more comprehensive and interpretable characterization of the topology of the molecular data. We have established theoretical stability guarantees for multiparameter persistent homology, and have conducted extensive experiments on the Lipophilicity, FreeSolv, and ESOL datasets to demonstrate its effectiveness in predicting molecular properties.
</details>
<details>
<summary>摘要</summary>
在本研究中，我们提出了一种基于多参数持续同态的分子指纹生成方法。这种方法可以揭示分子几何结构中的隐藏结构和关系，并检测在多个缩放量和多个参数（如原子质量、部分电荷、键类型）之间的 persistente 特征。此外，通过添加更多参数（如离子能力、电子亲和力、旋转hybridization），可以进一步增强分子指纹的准确性。提议的指纹方法可以为分子结构的分析提供新的视角，并且与传统的图 neural network 相比， multiparameter persistent homology 具有更全面和可 interpret的特征。我们已经提供了理论稳定保证，并在 Lipophilicity、FreeSolv 和 ESOL 数据集上进行了广泛的实验，以证明其效iveness 在预测分子性质。
</details></li>
</ul>
<hr>
<h2 id="Online-Calibration-of-Deep-Learning-Sub-Models-for-Hybrid-Numerical-Modeling-Systems"><a href="#Online-Calibration-of-Deep-Learning-Sub-Models-for-Hybrid-Numerical-Modeling-Systems" class="headerlink" title="Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling Systems"></a>Online Calibration of Deep Learning Sub-Models for Hybrid Numerical Modeling Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10665">http://arxiv.org/abs/2311.10665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Said Ouala, Bertrand Chapron, Fabrice Collard, Lucile Gaultier, Ronan Fablet<br>for: 这个论文主要是关于如何使用人工智能和深度学习来改进数值 simulate 框架，以及如何在这些框架中使用 neural network 来模型化物理系统。methods: 这个论文使用了一种名为 EGA（Euler Gradient Approximation）的在线学习方法，该方法假设物理模型中的梯度可以用一种加法方式来近似，并且使用了 Explicit Euler 方法来计算梯度。results: 实验结果表明，EGA 方法可以在不同的案例中提供显著的改进，比如 ocean-atmosphere 动力学等。这些结果也表明，在线学习方法可以在 hybrid 模型中提供更好的预测性能，相比于传统的 offline 学习方法。<details>
<summary>Abstract</summary>
Artificial intelligence and deep learning are currently reshaping numerical simulation frameworks by introducing new modeling capabilities. These frameworks are extensively investigated in the context of model correction and parameterization where they demonstrate great potential and often outperform traditional physical models. Most of these efforts in defining hybrid dynamical systems follow {offline} learning strategies in which the neural parameterization (called here sub-model) is trained to output an ideal correction. Yet, these hybrid models can face hard limitations when defining what should be a relevant sub-model response that would translate into a good forecasting performance. End-to-end learning schemes, also referred to as online learning, could address such a shortcoming by allowing the deep learning sub-models to train on historical data. However, defining end-to-end training schemes for the calibration of neural sub-models in hybrid systems requires working with an optimization problem that involves the solver of the physical equations. Online learning methodologies thus require the numerical model to be differentiable, which is not the case for most modeling systems. To overcome this difficulty and bypass the differentiability challenge of physical models, we present an efficient and practical online learning approach for hybrid systems. The method, called EGA for Euler Gradient Approximation, assumes an additive neural correction to the physical model, and an explicit Euler approximation of the gradients. We demonstrate that the EGA converges to the exact gradients in the limit of infinitely small time steps. Numerical experiments are performed on various case studies, including prototypical ocean-atmosphere dynamics. Results show significant improvements over offline learning, highlighting the potential of end-to-end online learning for hybrid modeling.
</details>
<details>
<summary>摘要</summary>
人工智能和深度学习现在在数值仿真框架中发挥重要作用，带来新的模型化能力。这些框架在模型修正和参数化方面得到了广泛的研究，并在许多情况下超越了传统的物理模型。大多数这些尝试都采用了拟合动力系统的方法，其中大多数采用了Offline学习策略，在哪里神经参数化（以下简称为子模型）被训练以输出理想的修正。然而，这些混合模型在定义相关的子模型响应时可能会遇到困难。在线学习方法，也称为Online学习，可以解决这一缺点，并允许深度学习子模型在历史数据上进行训练。然而，定义在混合系统中的End-to-end学习方案需要与物理方程的解除器进行优化问题，这种问题需要数值模型具有导数性。因此，在线学习方法需要数值模型是可导的，这并不是现实的情况。为了缺过这个挑战和Physical模型的不导数性，我们提出了一种高效和实用的在线学习方法，称为EGA（Euler Gradient Approximation）。EGA假设神经修正是加性的，并且使用显式Euler近似来计算导数。我们证明EGA可以在无穷小时步下收敛到正确的导数。在不同的案例研究中，包括气洋大气动力学的示例，我们进行了数值实验，并得到了与Offline学习相比显著的改进。这些结果表明了混合模型的End-to-end在线学习的潜在潜力。
</details></li>
</ul>
<hr>
<h2 id="Learning-Realistic-Joint-Space-Boundaries-for-Range-of-Motion-Analysis-of-Healthy-and-Impaired-Human-Arms"><a href="#Learning-Realistic-Joint-Space-Boundaries-for-Range-of-Motion-Analysis-of-Healthy-and-Impaired-Human-Arms" class="headerlink" title="Learning Realistic Joint Space Boundaries for Range of Motion Analysis of Healthy and Impaired Human Arms"></a>Learning Realistic Joint Space Boundaries for Range of Motion Analysis of Healthy and Impaired Human Arms</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10653">http://arxiv.org/abs/2311.10653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shafagh Keyvanian, Michelle J. Johnson, Nadia Figueroa<br>for:这篇论文旨在创建一个真实的人体动机学模型，以便在人机交互、生物力学和机器人帮助重建中更加准确地模拟人体动作。methods:该论文使用数据驱动的方法，通过一个一类支持向量机进行joint空间探索运动数据的适应，并实现了高效的hyperparameter调整方案。results:该论文的方法在有效地学习真实的人体动机学范围，并提供了一个量化的依硬力指标（II），用于评估健康和损伤臂的能力差异。<details>
<summary>Abstract</summary>
A realistic human kinematic model that satisfies anatomical constraints is essential for human-robot interaction, biomechanics and robot-assisted rehabilitation. Modeling realistic joint constraints, however, is challenging as human arm motion is constrained by joint limits, inter- and intra-joint dependencies, self-collisions, individual capabilities and muscular or neurological constraints which are difficult to represent. Hence, physicians and researchers have relied on simple box-constraints, ignoring important anatomical factors. In this paper, we propose a data-driven method to learn realistic anatomically constrained upper-limb range of motion (RoM) boundaries from motion capture data. This is achieved by fitting a one-class support vector machine to a dataset of upper-limb joint space exploration motions with an efficient hyper-parameter tuning scheme. Our approach outperforms similar works focused on valid RoM learning. Further, we propose an impairment index (II) metric that offers a quantitative assessment of capability/impairment when comparing healthy and impaired arms. We validate the metric on healthy subjects physically constrained to emulate hemiplegia and different disability levels as stroke patients.
</details>
<details>
<summary>摘要</summary>
真实的人类动态模型，满足人体生物力学和机器人协助康复的需求，是非常重要。但是模拟真实的关节约束却是一项挑战，因为人类手臂运动受到关节限制、间关节和内关节依赖、自体冲撞和个体能力和神经学约束的限制。因此，医生和研究人员通常采用简单的盒子约束，忽略了重要的解剖因素。在这篇论文中，我们提出了一种基于数据驱动的方法，通过将一类支持向量机制适应到一个基于运动捕捉数据的上下文中，以学习真实的解剖约束范围。我们的方法比类似的工作更高效。此外，我们还提出了一个评价能力/障碍度的指标（II指标），可以对健康和损伤手臂进行数量评价。我们验证了这个指标，通过将健康人物理约束为假性肢体瘫痪和不同的残伤水平来验证。
</details></li>
</ul>
<hr>
<h2 id="Predicting-the-Probability-of-Collision-of-a-Satellite-with-Space-Debris-A-Bayesian-Machine-Learning-Approach"><a href="#Predicting-the-Probability-of-Collision-of-a-Satellite-with-Space-Debris-A-Bayesian-Machine-Learning-Approach" class="headerlink" title="Predicting the Probability of Collision of a Satellite with Space Debris: A Bayesian Machine Learning Approach"></a>Predicting the Probability of Collision of a Satellite with Space Debris: A Bayesian Machine Learning Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10633">http://arxiv.org/abs/2311.10633</a></li>
<li>repo_url: None</li>
<li>paper_authors: João Simões Catulo, Cláudia Soares, Marta Guimarães</li>
<li>for: 本研究旨在提高遥感器官的冲突风险评估，以避免在低地球轨道上发生冲突。</li>
<li>methods: 本研究使用隐马尔可夫模型（HMM）来预测冲突风险，并使用 bayesian 统计来归一化模型参数。</li>
<li>results: 实验结果表明，使用 HMM 可以超越预测方法的性能，这 further 支持冲突警告可能具有markov 性质。<details>
<summary>Abstract</summary>
Space is becoming more crowded in Low Earth Orbit due to increased space activity. Such a dense space environment increases the risk of collisions between space objects endangering the whole space population. Therefore, the need to consider collision avoidance as part of routine operations is evident to satellite operators. Current procedures rely on the analysis of multiple collision warnings by human analysts. However, with the continuous growth of the space population, this manual approach may become unfeasible, highlighting the importance of automation in risk assessment. In 2019, ESA launched a competition to study the feasibility of applying machine learning in collision risk estimation and released a dataset that contained sequences of Conjunction Data Messages (CDMs) in support of real close encounters. The competition results showed that the naive forecast and its variants are strong predictors for this problem, which suggests that the CDMs may follow the Markov property. The proposed work investigates this theory by benchmarking Hidden Markov Models (HMM) in predicting the risk of collision between two resident space objects by using one feature of the entire dataset: the sequence of the probability in the CDMs. In addition, Bayesian statistics are used to infer a joint distribution for the parameters of the models, which allows the development of robust and reliable probabilistic predictive models that can incorporate physical or prior knowledge about the problem within a rigorous theoretical framework and provides prediction uncertainties that nicely reflect the accuracy of the predicted risk. This work shows that the implemented HMM outperforms the naive solution in some metrics, which further adds to the idea that the collision warnings may be Markovian and suggests that this is a powerful method to be further explored.
</details>
<details>
<summary>摘要</summary>
Space 正在低地球轨道上变得越来越拥挤，由于增加的空间活动。这样的紧密的空间环境会提高空间物体之间的Collision的风险， threatening the whole space population.因此，卫星运营商必须考虑避免Collision的作业为 Routine 的一部分。现有的程序仍然基于人类分析多个Collision 警告。然而，随着空间人口的不断增长，这种手动方法可能变得不可行， highlighting the importance of automation in risk assessment.在2019年，ESA发布了一项竞赛，以研究在Collision 风险估计中应用机器学习的可行性，并发布了一个包含实际近距离Encounter 的数据集。竞赛结果表明，naive forecast和其变体是Close Encounter 中的强有力预测器，这意味着CDMs可能遵循Markov 性质。本工作investigates this theory by benchmarkingHidden Markov Models (HMM) in predicting the risk of collision between two resident space objects using one feature of the entire dataset: the sequence of the probability in the CDMs。此外，Bayesian statistics are used to infer a joint distribution for the parameters of the models, which allows the development of robust and reliable probabilistic predictive models that can incorporate physical or prior knowledge about the problem within a rigorous theoretical framework and provides prediction uncertainties that nicely reflect the accuracy of the predicted risk。本工作显示，实施的HMM OUTPERFORMS naive solution in some metrics，这更加支持CDMs遵循Markovian的想法，并 suggets that this is a powerful method to be further explored。
</details></li>
</ul>
<hr>
<h2 id="A-Poincare-Inequality-and-Consistency-Results-for-Signal-Sampling-on-Large-Graphs"><a href="#A-Poincare-Inequality-and-Consistency-Results-for-Signal-Sampling-on-Large-Graphs" class="headerlink" title="A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs"></a>A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10610">http://arxiv.org/abs/2311.10610</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thien Le, Luana Ruiz, Stefanie Jegelka</li>
<li>for: 这篇论文是关于大规模图机器学习中的一种挑战，即学习模型的复杂性与图size成正相关。</li>
<li>methods: 作者提出了一种基于图Limit的信号抽样理论，并证明了这种抽样集是图on信号空间的唯一抽样集。</li>
<li>results: 作者采用了相关的图on信号抽样算法，并通过实验证明了其在图机器学习任务上的良好实验表现。Here’s the same information in English:</li>
<li>for: The paper addresses the challenge of large-scale graph machine learning, where the complexity of learning models scales with the graph size.</li>
<li>methods: The authors propose a signal sampling theory for a type of graph limit called the graphon, and prove that certain sampling sets are unique and consistent for graphon signals.</li>
<li>results: The authors propose a related graphon signal sampling algorithm and demonstrate its good empirical performance on graph machine learning tasks.<details>
<summary>Abstract</summary>
Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.
</details>
<details>
<summary>摘要</summary>
大规模图机器学习是挑战的，因为学习模型的复杂度与图Size相关。图样本是非欧几何的，现有的图样本技术需要计算大Matrix的特征值，并在图改变时重复这些计算，例如图生长。在这篇论文中，我们介绍了一种图Limit——图он的信号抽样理论。我们证明了图он信号的波因耳假设下的质量假设，并证明了这些样本集在Paley-Wiener空间中是独特的抽样集。通过spectral clustering和欧几何排序的连接，我们证明了这些样本集是一致的，即在 convergent 图序列上的样本集 converges to 图он上的样本集。然后，我们提出了一种基于图он信号抽样的大图机器学习算法，并在实际应用中得到了良好的实际表现。
</details></li>
</ul>
<hr>
<h2 id="Scaling-TabPFN-Sketching-and-Feature-Selection-for-Tabular-Prior-Data-Fitted-Networks"><a href="#Scaling-TabPFN-Sketching-and-Feature-Selection-for-Tabular-Prior-Data-Fitted-Networks" class="headerlink" title="Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks"></a>Scaling TabPFN: Sketching and Feature Selection for Tabular Prior-Data Fitted Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10609">http://arxiv.org/abs/2311.10609</a></li>
<li>repo_url: None</li>
<li>paper_authors: Benjamin Feuer, Chinmay Hegde, Niv Cohen</li>
<li>for: 本研究旨在 investigating the best way to summarize the labelled training samples before feeding them to a pre-trained Prior-Data Fitted Network (PFN) for tabular data.</li>
<li>methods: 本研究使用 sketching 和 feature-selection methods to summarize the labelled training samples, and compare the results with conventionally fitted tabular models.</li>
<li>results: 研究发现，使用 sketching 和 feature-selection methods可以有效地缩小 labelled training samples，而且这些方法与 conventionally fitted tabular models 有一定的区别。<details>
<summary>Abstract</summary>
Tabular classification has traditionally relied on supervised algorithms, which estimate the parameters of a prediction model using its training data. Recently, Prior-Data Fitted Networks (PFNs) such as TabPFN have successfully learned to classify tabular data in-context: the model parameters are designed to classify new samples based on labelled training samples given after the model training. While such models show great promise, their applicability to real-world data remains limited due to the computational scale needed. Here we study the following question: given a pre-trained PFN for tabular data, what is the best way to summarize the labelled training samples before feeding them to the model? We conduct an initial investigation of sketching and feature-selection methods for TabPFN, and note certain key differences between it and conventionally fitted tabular models.
</details>
<details>
<summary>摘要</summary>
文本分类传统上采用了指导算法，这些算法估算模型参数使用训练数据。近期，先进的假数据适应网络（PFN），如TabPFN，已成功地在标签数据上进行了分类：模型参数是在给定模型训练后的标签训练样本上基于新样本进行分类。虽然这些模型显示出了极大的承诺，但它们在实际数据上的应用受到了计算规模的限制。我们研究以下问题：给定一个预训练的PFN，如何最好 SUMMARIZE 标签训练样本？我们进行了初步的笔记和特征选择方法的研究，并注意到了PFN与传统的适应 tabular 模型之间的一些关键差异。
</details></li>
</ul>
<hr>
<h2 id="Implicit-Maximum-a-Posteriori-Filtering-via-Adaptive-Optimization"><a href="#Implicit-Maximum-a-Posteriori-Filtering-via-Adaptive-Optimization" class="headerlink" title="Implicit Maximum a Posteriori Filtering via Adaptive Optimization"></a>Implicit Maximum a Posteriori Filtering via Adaptive Optimization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10580">http://arxiv.org/abs/2311.10580</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gianlucabencomo/implicitmap">https://github.com/gianlucabencomo/implicitmap</a></li>
<li>paper_authors: Gianluca M. Bencomo, Jake C. Snell, Thomas L. Griffiths</li>
<li>for: 这个论文的目的是提出一种高维状态空间中的 bayesian 筛选方法，以解决现有的筛选方法在高维状态空间中的实用性问题。</li>
<li>methods: 该论文使用了优化问题的形式来实现 bayesian 筛选，而不需要维护大 matrices 或 Monte Carlo 估计。</li>
<li>results: 实验表明，该方法可以在高维状态空间中实现有效、Robust 和可扩展的筛选result，与标准的 bayesian 筛选方法相比，该方法更容易 fine-tune 优化器。<details>
<summary>Abstract</summary>
Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires either storage, inversion, and multiplication of large matrices or Monte Carlo estimation, neither of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. Here, we frame the standard Bayesian filtering problem as optimization over a time-varying objective. Instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the Bayesian filter implicitly. In the linear-Gaussian setting, we show that every Kalman filter has an equivalent formulation using K steps of gradient descent. In the nonlinear setting, our experiments demonstrate that our framework results in filters that are effective, robust, and scalable to high-dimensional systems, comparing well against the standard toolbox of Bayesian filtering solutions. We suggest that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making our framework an attractive option for high-dimensional filtering problems.
</details>
<details>
<summary>摘要</summary>
bayesian filtering  aproximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. this process typically requires either storage, inversion, and multiplication of large matrices or monte carlo estimation, neither of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. here, we frame the standard bayesian filtering problem as optimization over a time-varying objective. instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the bayesian filter implicitly. in the linear-gaussian setting, we show that every kalman filter has an equivalent formulation using k steps of gradient descent. in the nonlinear setting, our experiments demonstrate that our framework results in filters that are effective, robust, and scalable to high-dimensional systems, comparing well against the standard toolbox of bayesian filtering solutions. we suggest that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making our framework an attractive option for high-dimensional filtering problems.
</details></li>
</ul>
<hr>
<h2 id="Graph-Neural-Networks-for-Pressure-Estimation-in-Water-Distribution-Systems"><a href="#Graph-Neural-Networks-for-Pressure-Estimation-in-Water-Distribution-Systems" class="headerlink" title="Graph Neural Networks for Pressure Estimation in Water Distribution Systems"></a>Graph Neural Networks for Pressure Estimation in Water Distribution Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10579">http://arxiv.org/abs/2311.10579</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ditec-project/gnn-pressure-estimation">https://github.com/ditec-project/gnn-pressure-estimation</a></li>
<li>paper_authors: Huy Truong, Andrés Tello, Alexander Lazovik, Victoria Degeler</li>
<li>for: 这个论文的目的是提出一种基于物理模型和图学神经网络的压力估计方法，以便水资源管理公司可以优化控制操作。</li>
<li>methods: 这个方法利用了数据驱动方法，包括一种新的数据生成方法和一种随机感知器放置策略，以及一种具有多图预训练的模型。</li>
<li>results: 这个方法在一个规模很大的荷兰水分布网络上进行了实验，并得到了较高的准确性和稳定性。相比之下，之前的研究中的方法在同样的网络上的表现较差。<details>
<summary>Abstract</summary>
Pressure and flow estimation in Water Distribution Networks (WDN) allows water management companies to optimize their control operations. For many years, mathematical simulation tools have been the most common approach to reconstructing an estimate of the WDN hydraulics. However, pure physics-based simulations involve several challenges, e.g. partially observable data, high uncertainty, and extensive manual configuration. Thus, data-driven approaches have gained traction to overcome such limitations. In this work, we combine physics-based modeling and Graph Neural Networks (GNN), a data-driven approach, to address the pressure estimation problem. First, we propose a new data generation method using a mathematical simulation but not considering temporal patterns and including some control parameters that remain untouched in previous works; this contributes to a more diverse training data. Second, our training strategy relies on random sensor placement making our GNN-based estimation model robust to unexpected sensor location changes. Third, a realistic evaluation protocol considers real temporal patterns and additionally injects the uncertainties intrinsic to real-world scenarios. Finally, a multi-graph pre-training strategy allows the model to be reused for pressure estimation in unseen target WDNs. Our GNN-based model estimates the pressure of a large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of 7%, surpassing the performance of previous studies. Likewise, it outperformed previous approaches on other WDN benchmarks, showing a reduction of absolute error up to approximately 52% in the best cases.
</details>
<details>
<summary>摘要</summary>
“水distribution网络（WDN）中的压力和流量估算可以帮助水资源管理公司优化其控制操作。在过去的几十年中，数学模拟工具一直是WDN hidraulics的重要估算方法。然而，基于物理的数据驱动方法具有一些挑战，如部分可见数据、高度不确定和广泛的手动配置。因此，数据驱动方法在WDN中得到了广泛的应用。在这种情况下，我们将物理模型和图 neural network（GNN）相结合，以解决压力估算问题。首先，我们提出了一种新的数据生成方法，使用数学模拟而不考虑时间模式，并包括一些控制参数，这些参数在前一些研究中未经考虑。其次，我们的训练策略基于随机感知器的布局，使我们的GNN-based estimation模型具有鲁棒性。最后，我们采用了一种现实istic的评估协议，考虑真实的时间模式，并在实际情况中添加了内在的不确定性。此外，我们还提出了一种多图预训练策略，使模型可以在未看到的目标WDN中进行重用。我们的GNN-based模型在荷兰的一个大规模WDN中估算了压力的 Mean Absolute Error（MAE）为1.94mH$_2$O，与前一些研究相比，表现出色。此外，它还在其他WDNbenchmark上表现出色，比前一些方法减少绝对错误的约52%。”
</details></li>
</ul>
<hr>
<h2 id="Direct-Amortized-Likelihood-Ratio-Estimation"><a href="#Direct-Amortized-Likelihood-Ratio-Estimation" class="headerlink" title="Direct Amortized Likelihood Ratio Estimation"></a>Direct Amortized Likelihood Ratio Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10571">http://arxiv.org/abs/2311.10571</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/sri-csl/dnre">https://github.com/sri-csl/dnre</a></li>
<li>paper_authors: Adam D. Cobb, Brian Matejek, Daniel Elenius, Anirban Roy, Susmit Jha</li>
<li>for: 这 paper 是为了提出一种新的likelihood-free simulation-based inference（SBI）的估计器。</li>
<li>methods: 这 paper 使用了一种直观ratio estimator（DNRE）来估计likelihood ratio，DNRE 通过单个前进传播来估计likelihood ratio，与之前的方法不同。</li>
<li>results: 作者在引入 DNRE 时还提出了一种相应的Monte Carlo估计 posterior，并对新的 ratio estimator 进行了比较性分析。 results 显示，新的 ratio estimator 通常能够超越先前的方法。此外，作者还引入了一种新的 derivative estimator，用于比较likelihood-free Hamiltonian Monte Carlo（HMC）与 random-walk Metropolis-Hastings（MH）。结果表明，HMC 和 MH 在效果上几乎相等。最后，作者通过使用 neural ratio estimator 设计了一架quadcopter，这是一个实际应用的例子。代码可以在<a target="_blank" rel="noopener" href="https://github.com/SRI-CSL/dnre">https://github.com/SRI-CSL/dnre</a> 上获取。<details>
<summary>Abstract</summary>
We introduce a new amortized likelihood ratio estimator for likelihood-free simulation-based inference (SBI). Our estimator is simple to train and estimates the likelihood ratio using a single forward pass of the neural estimator. Our approach directly computes the likelihood ratio between two competing parameter sets which is different from the previous approach of comparing two neural network output values. We refer to our model as the direct neural ratio estimator (DNRE). As part of introducing the DNRE, we derive a corresponding Monte Carlo estimate of the posterior. We benchmark our new ratio estimator and compare to previous ratio estimators in the literature. We show that our new ratio estimator often outperforms these previous approaches. As a further contribution, we introduce a new derivative estimator for likelihood ratio estimators that enables us to compare likelihood-free Hamiltonian Monte Carlo (HMC) with random-walk Metropolis-Hastings (MH). We show that HMC is equally competitive, which has not been previously shown. Finally, we include a novel real-world application of SBI by using our neural ratio estimator to design a quadcopter. Code is available at https://github.com/SRI-CSL/dnre.
</details>
<details>
<summary>摘要</summary>
我们介绍一个新的折衣率分布估计器，用于无likelihood-based simulation-based推理（SBI）。我们的估计器简单易于训练，通过单一的前进传播神经估计器来估计对抗组件之间的折衣率。我们的方法直接计算两个竞争性 parameter set 之间的折衣率，与前一种比较两个神经网络输出值的方法不同。我们称之为“直接神经率估计器”（DNRE）。在引入 DNRE 时，我们 derivate 一个对应的Monte Carlo estimate of the posterior。我们 benchmark 我们的新折衣率估计器，并与过去的折衣率估计器进行比较。我们显示了我们的新折衣率估计器经常超越过去的方法。此外，我们引入了一个新的折衣率估计器 Derby 估计器，允许我们比较likelihood-free Hamiltonian Monte Carlo（HMC）与随机步进 Metropolis-Hastings（MH）。我们显示了HMC 与 MH 在likelihood-free状况下是等效的，这没有被证明过。最后，我们还提出了一个新的实际应用，利用我们的神经率估计器设计一架quadcopter。代码可以在https://github.com/SRI-CSL/dnre 上找到。
</details></li>
</ul>
<hr>
<h2 id="RONAALP-Reduced-Order-Nonlinear-Approximation-with-Active-Learning-Procedure"><a href="#RONAALP-Reduced-Order-Nonlinear-Approximation-with-Active-Learning-Procedure" class="headerlink" title="RONAALP: Reduced-Order Nonlinear Approximation with Active Learning Procedure"></a>RONAALP: Reduced-Order Nonlinear Approximation with Active Learning Procedure</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10550">http://arxiv.org/abs/2311.10550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Clément Scherding, Georgios Rigas, Denis Sipp, Peter J Schmid, Taraneh Sayadi</li>
<li>for: This paper is written for engineers and researchers who need to evaluate expensive, non-linear high-dimensional functions in their applications.</li>
<li>methods: The paper proposes the RONAALP algorithm, which is a reduced-order nonlinear approximation with active learning procedure to incrementally learn a fast and accurate reduced-order surrogate model of a target function on-the-fly. The algorithm combines nonlinear auto-encoders, community clustering, and radial basis function networks to learn an efficient and compact surrogate model with limited training data.</li>
<li>results: The paper demonstrates the effectiveness of the RONAALP algorithm on three direct numerical simulations of hypersonic flows in chemical nonequilibrium. The results show that the algorithm can reduce the cost of the simulation by up to 75% while maintaining an error of less than 10% on relevant quantities of interest.<details>
<summary>Abstract</summary>
Many engineering applications rely on the evaluation of expensive, non-linear high-dimensional functions. In this paper, we propose the RONAALP algorithm (Reduced Order Nonlinear Approximation with Active Learning Procedure) to incrementally learn a fast and accurate reduced-order surrogate model of a target function on-the-fly as the application progresses. First, the combination of nonlinear auto-encoder, community clustering and radial basis function networks allows to learn an efficient and compact surrogate model with limited training data. Secondly, the active learning procedure overcome any extrapolation issue when evaluating the surrogate model outside of its initial training range during the online stage. This results in generalizable, fast and accurate reduced-order models of high-dimensional functions. The method is demonstrated on three direct numerical simulations of hypersonic flows in chemical nonequilibrium. Accurate simulations of these flows rely on detailed thermochemical gas models that dramatically increase the cost of such calculations. Using RONAALP to learn a reduced-order thermodynamic model surrogate on-the-fly, the cost of such simulation was reduced by up to 75% while maintaining an error of less than 10% on relevant quantities of interest.
</details>
<details>
<summary>摘要</summary>
многие инженерные приложения зависят от оценки дорогостоящих, нелинейных, многомерных функций. В этой статье мы предлагаем алгоритм RONAALP (Редуцированный порядковый нелинейный подход с активным обучением) для постепенного обучения быстрому и точному редуцированному моделированию целевой функции на месте, как функция прогрессирует. Сначала комбинация нелинейного автоэнкодера, clustering сообществ и радиальных основных сетей позволяет обучить эффективную и компактную модель-surrogate с ограниченным количеством данных обучения. Затем активный процесс обучения преодолевает любые проблемы экстраполяции, когда модель-surrogate оценивается за пределами своего первоначального диапазона во время онлайн-стадии. Это приводит к генерализованным, быстрым и точным редуцированным моделям высокодимензионных функций. Метод демонстрируется на трёх прямых численных симуляциях гиперзвуковых потоков в химическом неравновесии. Точные симуляции таких потоков зависят от подробных моделей газов, что увеличивает стоимость расчетов. Применение RONAALP для обучения редуцированному модели thermodynamic на месте сократило стоимость таких симуляций на 75% при сохранении ошибки менее 10% на ключевые величины интереса.
</details></li>
</ul>
<hr>
<h2 id="Utilizing-VQ-VAE-for-End-to-End-Health-Indicator-Generation-in-Predicting-Rolling-Bearing-RUL"><a href="#Utilizing-VQ-VAE-for-End-to-End-Health-Indicator-Generation-in-Predicting-Rolling-Bearing-RUL" class="headerlink" title="Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL"></a>Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10525">http://arxiv.org/abs/2311.10525</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junliang Wang, Qinghua Zhang, Guanhua Zhu, Guoxi Sun</li>
<li>for: 预测滚珠机器的剩余有用生命（RUL）是工业生产中的一个重要问题。这篇论文提出了一种终端HI构建方法，即向量量化变换自动encoder（VQ-VAE），以解决传统无监督学习方法中维度减少缺失的问题。</li>
<li>methods: 本论文使用VQ-VAE方法对振荡信号进行变换，并引入了两种新的统计指标，即平均绝对距离（MAD）和平均方差（MV），以准确描述抽象曲线的波动趋势。</li>
<li>results: 使用VQ-VAE方法构建标签后，PMH2012数据集上的方法显示出较低的MAD和MV值，而使用VQ-VAE标签训练的ASTCN预测模型也达到了最低的MAD和MV值。<details>
<summary>Abstract</summary>
The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.
</details>
<details>
<summary>摘要</summary>
rolling bearings 的剩余有用生命剩余预测是工业生产中的一个关键问题。一种关键的方法是将振荡信号转化为健康指标（HI），以便模型训练。本文提出了一种终端HI建构方法，基于量化变分自动编码器（VQ-VAE），解决了传统无监督学习方法中的维度减少问题。此外，由于传统统计指标不准确地反映曲线波动，本文引入了两种新的统计指标：平均绝对距离（MAD）和平均方差（MV）。这两种指标准确地描述曲线波动的特征，因此可以反映模型对相似特征的准确性。在PMH2012数据集上，使用VQ-VAE进行标签建构的方法实现了较低的MAD和MV值。此外，使用VQ-VAE标签训练的ASTCN预测模型实现了最低的MAD和MV值。
</details></li>
</ul>
<hr>
<h2 id="Causal-Fairness-Guided-Dataset-Reweighting-using-Neural-Networks"><a href="#Causal-Fairness-Guided-Dataset-Reweighting-using-Neural-Networks" class="headerlink" title="Causal Fairness-Guided Dataset Reweighting using Neural Networks"></a>Causal Fairness-Guided Dataset Reweighting using Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10512">http://arxiv.org/abs/2311.10512</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xuan Zhao, Klaus Broelemann, Salvatore Ruggieri, Gjergji Kasneci</li>
<li>for: This paper aims to address the issue of fairness in machine learning models from a causal perspective, and proposes a reweighting scheme of datasets to mitigate bias and achieve causal fairness.</li>
<li>methods: The proposed method uses two neural networks to approximate the causal model of the data and the causal model of interventions, and applies reweighting guided by a discriminator to achieve various fairness notions.</li>
<li>results: The experiments on real-world datasets show that the proposed method can achieve causal fairness on the data while remaining close to the original data for downstream tasks.<details>
<summary>Abstract</summary>
The importance of achieving fairness in machine learning models cannot be overstated. Recent research has pointed out that fairness should be examined from a causal perspective, and several fairness notions based on the on Pearl's causal framework have been proposed. In this paper, we construct a reweighting scheme of datasets to address causal fairness. Our approach aims at mitigating bias by considering the causal relationships among variables and incorporating them into the reweighting process. The proposed method adopts two neural networks, whose structures are intentionally used to reflect the structures of a causal graph and of an interventional graph. The two neural networks can approximate the causal model of the data, and the causal model of interventions. Furthermore, reweighting guided by a discriminator is applied to achieve various fairness notions. Experiments on real-world datasets show that our method can achieve causal fairness on the data while remaining close to the original data for downstream tasks.
</details>
<details>
<summary>摘要</summary>
“machine learning模型中的公平性的重要性不能被过度说明。latest research表明，公平性应该从 causal perspective examined，并提出了基于pearl causal framework的多种公平性观。本文提出了一种基于dataset重Weighting的方法，以mitigate bias by considering the causal relationships among variables and incorporating them into the reweighting process。我们采用了两个神经网络，其结构与causal graph和interventional graph Reflects。两个神经网络可以 aproximate the causal model of the data, and the causal model of interventions。此外，我们还使用了一个抑制器来实现多种公平性观。实验表明，我们的方法可以在实际数据上实现causal fairness，while remain close to the original data for downstream tasks。”Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and other parts of the world. Traditional Chinese is also widely used, especially in Taiwan and Hong Kong.
</details></li>
</ul>
<hr>
<h2 id="Handling-Overlapping-Asymmetric-Datasets-–-A-Twice-Penalized-P-Spline-Approach"><a href="#Handling-Overlapping-Asymmetric-Datasets-–-A-Twice-Penalized-P-Spline-Approach" class="headerlink" title="Handling Overlapping Asymmetric Datasets – A Twice Penalized P-Spline Approach"></a>Handling Overlapping Asymmetric Datasets – A Twice Penalized P-Spline Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10489">http://arxiv.org/abs/2311.10489</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matthew McTeer, Robin Henderson, Quentin M Anstee, Paolo Missier</li>
<li>for: 这种研究的目的是为了开发一种能够模型小样本而考虑大样本的新方法，以便在医疗数据中处理不平衡的数据。</li>
<li>methods: 这种方法基于非Parametric模型，具体来说是通过扩展加itive模型来实现可变滑动平滑 Approximation，以避免小样本中的过度或者下降适应。</li>
<li>results: 通过数据 simulate、参数调整和模型改进，我们发现在考虑continuous和binaryResponse的情况下，我们的双重penalized方法可以提供较好的适应性，比起线性B-spline和once penalized P-spline Approximation。在实际数据中应用于评估非酒精性肝炎发展的风险时，我们发现模型适应性得到了65%以上的提高。<details>
<summary>Abstract</summary>
Overlapping asymmetric datasets are common in data science and pose questions of how they can be incorporated together into a predictive analysis. In healthcare datasets there is often a small amount of information that is available for a larger number of patients such as an electronic health record, however a small number of patients may have had extensive further testing. Common solutions such as missing imputation can often be unwise if the smaller cohort is significantly different in scale to the larger sample, therefore the aim of this research is to develop a new method which can model the smaller cohort against a particular response, whilst considering the larger cohort also. Motivated by non-parametric models, and specifically flexible smoothing techniques via generalized additive models, we model a twice penalized P-Spline approximation method to firstly prevent over/under-fitting of the smaller cohort and secondly to consider the larger cohort. This second penalty is created through discrepancies in the marginal value of covariates that exist in both the smaller and larger cohorts. Through data simulations, parameter tunings and model adaptations to consider a continuous and binary response, we find our twice penalized approach offers an enhanced fit over a linear B-Spline and once penalized P-Spline approximation. Applying to a real-life dataset relating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see an improved model fit performance of over 65%. Areas for future work within this space include adapting our method to not require dimensionality reduction and also consider parametric modelling methods. However, to our knowledge this is the first work to propose additional marginal penalties in a flexible regression of which we can report a vastly improved model fit that is able to consider asymmetric datasets, without the need for missing data imputation.
</details>
<details>
<summary>摘要</summary>
常见的不协调数据集在数据科学中出现，问题是如何将它们集成到预测分析中。医疗数据集中有时只有一小部分数据可用于较多的病人，例如电子健康记录，但是一些病人可能进行了较多的进一步检测。常见的解决方案，如遗弃值替换，可能不适用，因为小组规模较小的组比大组规模更大。因此，本研究的目标是开发一种新的方法，可以将小组模型为特定的响应，同时考虑大组。我们受非参数模型的激励，以及通用的滑动技术，特别是通用的加性模型。我们使用二次罚款P-Spline近似方法，首先避免小组过/下适应，第二个罚款是通过小组和大组covariate的偏度差异来考虑大组。通过数据 simulations、参数调整和模型修改来考虑连续和二分类响应，我们发现我们的两次罚款方法在Linear B-Spline和一次罚款P-Spline Approximation中提供了显著改进。应用于一个实际数据集，关于一个人的非酒精性肝炎风险，我们发现我们的模型适应性能高于65%。未来的工作包括适应我们的方法不需要维度减少和考虑 Parametric 模型方法。但是，到我们知道的是，这是首次提出额外的边缘罚款在灵活回归中，我们可以报告一个远远超过65%的模型适应性能，能够考虑不协调数据集，无需遗弃数据替换。
</details></li>
</ul>
<hr>
<h2 id="Robustness-Enhancement-in-Neural-Networks-with-Alpha-Stable-Training-Noise"><a href="#Robustness-Enhancement-in-Neural-Networks-with-Alpha-Stable-Training-Noise" class="headerlink" title="Robustness Enhancement in Neural Networks with Alpha-Stable Training Noise"></a>Robustness Enhancement in Neural Networks with Alpha-Stable Training Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10803">http://arxiv.org/abs/2311.10803</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoğlu</li>
<li>For: The paper aims to improve the robustness of deep learning systems by exploring the use of alpha-stable noise instead of Gaussian noise for data augmentation.* Methods: The paper compares the testing accuracy of models trained with Gaussian noise and alpha-stable noise on data corrupted by different types of noise, and finds that training with alpha-stable noise is more effective, especially for impulsive noise.* Results: The paper shows that training with alpha-stable noise improves the robustness of deep learning models on various datasets, including image and time series datasets, and other benchmark corrupted datasets.Here’s the simplified Chinese text for the three points:* For: 该论文目的是提高深度学习系统的Robustness，通过替换传统的高斯噪声使用α稳定噪声进行数据增强。* Methods: 论文通过对不同噪声类型的数据进行测试，并比较高斯噪声和α稳定噪声训练模型的测试精度，发现α稳定噪声训练模型在干扰噪声下的性能更高。* Results: 论文通过在多个图像和时间序列数据集以及其他受损数据集上进行实验，证明α稳定噪声训练模型在不同的噪声环境下都能够提高模型的Robustness。<details>
<summary>Abstract</summary>
With the increasing use of deep learning on data collected by non-perfect sensors and in non-perfect environments, the robustness of deep learning systems has become an important issue. A common approach for obtaining robustness to noise has been to train deep learning systems with data augmented with Gaussian noise. In this work, we challenge the common choice of Gaussian noise and explore the possibility of stronger robustness for non-Gaussian impulsive noise, specifically alpha-stable noise. Justified by the Generalized Central Limit Theorem and evidenced by observations in various application areas, alpha-stable noise is widely present in nature. By comparing the testing accuracy of models trained with Gaussian noise and alpha-stable noise on data corrupted by different noise, we find that training with alpha-stable noise is more effective than Gaussian noise, especially when the dataset is corrupted by impulsive noise, thus improving the robustness of the model. The generality of this conclusion is validated through experiments conducted on various deep learning models with image and time series datasets, and other benchmark corrupted datasets. Consequently, we propose a novel data augmentation method that replaces Gaussian noise, which is typically added to the training data, with alpha-stable noise.
</details>
<details>
<summary>摘要</summary>
Alpha-stable noise is widely present in nature, as evidenced by the Generalized Central Limit Theorem and observations in various application areas. By comparing the testing accuracy of models trained with Gaussian noise and alpha-stable noise on data corrupted by different noise, we find that training with alpha-stable noise is more effective than Gaussian noise, especially when the dataset is corrupted by impulsive noise, thus improving the robustness of the model.We validate the generality of this conclusion through experiments conducted on various deep learning models with image and time series datasets, as well as other benchmark corrupted datasets. Based on these results, we propose a novel data augmentation method that replaces Gaussian noise, which is typically added to the training data, with alpha-stable noise. This approach can improve the robustness of deep learning systems and enhance their ability to generalize to real-world data.
</details></li>
</ul>
<hr>
<h2 id="Maintenance-Techniques-for-Anomaly-Detection-AIOps-Solutions"><a href="#Maintenance-Techniques-for-Anomaly-Detection-AIOps-Solutions" class="headerlink" title="Maintenance Techniques for Anomaly Detection AIOps Solutions"></a>Maintenance Techniques for Anomaly Detection AIOps Solutions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10421">http://arxiv.org/abs/2311.10421</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorena Poenaru-Olaru, Natalia Karpova, Luis Cruz, Jan Rellermeyer, Arie van Deursen</li>
<li>for: 这种研究旨在探讨 anomaly detection 技术如何自动监控 IT 系统和操作，以及如何保持模型的性能在时间变化中。</li>
<li>methods: 本研究使用了两种不同的模型维护技术，namely blind model retraining 和 informed model retraining，并对各种更新频率进行了分析。</li>
<li>results: 研究发现，采用 full-history 方法更新模型可以保持较高的检测精度，而 sliding window 方法更新模型则可以适应时间变化。此外，使用数据变化监控工具可以确定模型是否需要更新。<details>
<summary>Abstract</summary>
Anomaly detection techniques are essential in automating the monitoring of IT systems and operations. These techniques imply that machine learning algorithms are trained on operational data corresponding to a specific period of time and that they are continuously evaluated on newly emerging data. Operational data is constantly changing over time, which affects the performance of deployed anomaly detection models. Therefore, continuous model maintenance is required to preserve the performance of anomaly detectors over time. In this work, we analyze two different anomaly detection model maintenance techniques in terms of the model update frequency, namely blind model retraining and informed model retraining. We further investigate the effects of updating the model by retraining it on all the available data (full-history approach) and on only the newest data (sliding window approach). Moreover, we investigate whether a data change monitoring tool is capable of determining when the anomaly detection model needs to be updated through retraining.
</details>
<details>
<summary>摘要</summary>
“异常探测技术是自动监控IT系统和操作的重要工具。这些技术假设机器学习算法在特定时间段的操作数据上进行训练，并在新的数据上持续评估。操作数据随时间变化，因此需要持续维护适用的异常探测模型，以确保它们在时间进行良好的表现。在这个工作中，我们分析了两种异常探测模型维护技术，分别是隐身模型重训和知情模型重训。我们还调查了将模型重训在所有可用数据上（全历史方法）和仅仅在最新的数据上（滑块窗口方法）的影响。此外，我们还探讨了一个数据变化监控工具是否能够决定异常探测模型是否需要更新。”Note that Simplified Chinese is used in mainland China, while Traditional Chinese is used in Taiwan and other regions.
</details></li>
</ul>
<hr>
<h2 id="DynaPipe-Optimizing-Multi-task-Training-through-Dynamic-Pipelines"><a href="#DynaPipe-Optimizing-Multi-task-Training-through-Dynamic-Pipelines" class="headerlink" title="DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines"></a>DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10418">http://arxiv.org/abs/2311.10418</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chenyu-jiang/megatron-lm">https://github.com/chenyu-jiang/megatron-lm</a></li>
<li>paper_authors: Chenyu Jiang, Zhen Jia, Shuai Zheng, Yida Wang, Chuan Wu</li>
<li>for: This paper proposes a method to improve the efficiency of multi-task model training, which is often hindered by the variation in input sequence length.</li>
<li>methods: The proposed method, called DynaPipe, uses dynamic micro-batching to tackle sequence length variation and enable efficient training of large language models.</li>
<li>results: The authors evaluate DynaPipe on the FLANv2 dataset and show that it achieves up to 4.39x higher training throughput compared to packing-based baselines, and 3.25x compared to the best-performing baseline.Here’s the full text in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种改进多任务模型训练的方法，以解决输入序列长度的变化带来的干扰。</li>
<li>methods: 该方法被称为DynaPipe，它使用动态微批来处理输入序列的长度变化，并使用管道并行训练大型自然语言模型。</li>
<li>results: 作者们在FLANv2数据集上评估了DynaPipe，并显示其在比基eline的4.39倍和GPT的3.25倍的训练吞吐量上具有明显的优势。<details>
<summary>Abstract</summary>
Multi-task model training has been adopted to enable a single deep neural network model (often a large language model) to handle multiple tasks (e.g., question answering and text summarization). Multi-task training commonly receives input sequences of highly different lengths due to the diverse contexts of different tasks. Padding (to the same sequence length) or packing (short examples into long sequences of the same length) is usually adopted to prepare input samples for model training, which is nonetheless not space or computation efficient. This paper proposes a dynamic micro-batching approach to tackle sequence length variation and enable efficient multi-task model training. We advocate pipeline-parallel training of the large model with variable-length micro-batches, each of which potentially comprises a different number of samples. We optimize micro-batch construction using a dynamic programming-based approach, and handle micro-batch execution time variation through dynamic pipeline and communication scheduling, enabling highly efficient pipeline training. Extensive evaluation on the FLANv2 dataset demonstrates up to 4.39x higher training throughput when training T5, and 3.25x when training GPT, as compared with packing-based baselines. DynaPipe's source code is publicly available at https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines.
</details>
<details>
<summary>摘要</summary>
多任务模型训练已被采用，以使用单个深度神经网络模型（通常是大型语言模型）来处理多个任务（例如，问答和文本摘要）。多任务训练通常接收不同上下文中的输入序列，因此输入样本的长度异常变化。 padding（到同一个序列长度）或 packing（短示例入力到同一个长度的序列）通常被采用，以为模型训练准备输入样本。然而，这并不是空间或计算效率的最佳选择。这篇论文提出了动态微批处理方法，以解决序列长度的变化和有效地进行多任务模型训练。我们提议在大型模型的管道并行训练中使用可变大小的微批，每个微批可能包含不同数量的样本。我们使用动态编程方法优化微批的建立，并通过动态管道和通信调度来处理微批执行时间的变化，以实现高效的管道训练。我们对FLANv2数据集进行了广泛的评估，并显示在与填充基elines进行比较时，DynaPipe可以增加训练 durchput的4.39倍，对T5模型进行训练时，可以增加3.25倍。DynaPipe的源代码可以在https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines中获取。
</details></li>
</ul>
<hr>
<h2 id="Decentralized-Energy-Marketplace-via-NFTs-and-AI-based-Agents"><a href="#Decentralized-Energy-Marketplace-via-NFTs-and-AI-based-Agents" class="headerlink" title="Decentralized Energy Marketplace via NFTs and AI-based Agents"></a>Decentralized Energy Marketplace via NFTs and AI-based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10406">http://arxiv.org/abs/2311.10406</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rasoulnik/dem">https://github.com/rasoulnik/dem</a></li>
<li>paper_authors: Rasoul Nikbakht, Farhana Javed, Farhad Rezazadeh, Nikolaos Bartzoudis, Josep Mangues-Bafalluy</li>
<li>for: 这项研究旨在开发一个基于区块链技术和人工智能的高级分布式能源市场（DEM），以便在智能家居系统中进行能源交易。</li>
<li>methods: 该方案使用非可换Token（NFT）来表示唯一的能源profile，并通过联邦深度强化学习（FDRL）来推动合作和适应性的能源管理策略，保持用户隐私。</li>
<li>results: 研究人员通过对系统进行广泛评估，证明了系统的扩展性和FDRL方法在分布式能源供应中的优化性。这项研究对建立先进的分布式智能电网基础设施做出了重要贡献，并拓宽了区块链和人工智能在可再生能源系统中的应用前景。<details>
<summary>Abstract</summary>
The paper introduces an advanced Decentralized Energy Marketplace (DEM) integrating blockchain technology and artificial intelligence to manage energy exchanges among smart homes with energy storage systems. The proposed framework uses Non-Fungible Tokens (NFTs) to represent unique energy profiles in a transparent and secure trading environment. Leveraging Federated Deep Reinforcement Learning (FDRL), the system promotes collaborative and adaptive energy management strategies, maintaining user privacy. A notable innovation is the use of smart contracts, ensuring high efficiency and integrity in energy transactions. Extensive evaluations demonstrate the system's scalability and the effectiveness of the FDRL method in optimizing energy distribution. This research significantly contributes to developing sophisticated decentralized smart grid infrastructures. Our approach broadens potential blockchain and AI applications in sustainable energy systems and addresses incentive alignment and transparency challenges in traditional energy trading mechanisms. The implementation of this paper is publicly accessible at \url{https://github.com/RasoulNik/DEM}.
</details>
<details>
<summary>摘要</summary>
文章介绍了一种先进的分布式能源市场place（DEM），利用区块链技术和人工智能来管理智能家庭之间的能源交易。提出的框架使用非 fungible Token（NFT）来表示独特的能源Profile，创造透明和安全的交易环境。通过联邦深度学习（FDRL），系统推广合作和适应能源管理策略，保持用户隐私。使用智能合同，确保高效和完整的能源交易。经过广泛评估，系统的扩展性和FDRL方法在优化能源分布方面的效果得到证明。这项研究对建立先进的分布式智能网格基础设施做出了重要贡献。我们的方法拓宽了区块和人工智能在可再生能源系统中的应用前景，解决了传统能源交易机制中的奖励对齐和透明度挑战。实现该文件可以通过 \url{https://github.com/RasoulNik/DEM} 访问。
</details></li>
</ul>
<hr>
<h2 id="The-Next-700-ML-Enabled-Compiler-Optimizations"><a href="#The-Next-700-ML-Enabled-Compiler-Optimizations" class="headerlink" title="The Next 700 ML-Enabled Compiler Optimizations"></a>The Next 700 ML-Enabled Compiler Optimizations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10800">http://arxiv.org/abs/2311.10800</a></li>
<li>repo_url: None</li>
<li>paper_authors: S. VenkataKeerthy, Siddharth Jain, Umesh Kalvakuntla, Pranav Sai Gorantla, Rajiv Shailesh Chitale, Eugene Brevdo, Albert Cohen, Mircea Trofin, Ramakrishna Upadrasta</li>
<li>for: 这个论文旨在提高编译器优化使用机器学习模型，但是编译器和机器学习框架之间的交互仍然存在困难。</li>
<li>methods: 该论文提出了一种名为ML-Compiler-Bridge的解决方案，它允许在传统的Python框架中开发机器学习模型，并使得与优化编译器的结合成为可能和高效。</li>
<li>results: 该论文对多个研究和生产用 caso，包括训练和推理，在多个优化问题、多个编译器和其版本以及多个gym基础设施进行了评估。<details>
<summary>Abstract</summary>
There is a growing interest in enhancing compiler optimizations with ML models, yet interactions between compilers and ML frameworks remain challenging. Some optimizations require tightly coupled models and compiler internals,raising issues with modularity, performance and framework independence. Practical deployment and transparency for the end-user are also important concerns. We propose ML-Compiler-Bridge to enable ML model development within a traditional Python framework while making end-to-end integration with an optimizing compiler possible and efficient. We evaluate it on both research and production use cases, for training and inference, over several optimization problems, multiple compilers and its versions, and gym infrastructures.
</details>
<details>
<summary>摘要</summary>
有增长的兴趣在把机器学习（ML）模型与编译器结合使用，但是编译器和ML框架之间的交互仍然是一个挑战。一些优化需要与编译器内部紧密集成的模型，导致模块化、性能和框架独立性的问题。在实践中，并不是所有的用户都可以轻松地使用和理解这些技术。我们提出了ML编译桥，它使得在传统的Python框架中开发ML模型，同时可以与优化编译器进行绑定，并且可以实现高效的终端集成。我们在多个研究和生产用 caso中进行了评估，包括训练和推理、多个编译器和其版本、以及Gym基础设施。
</details></li>
</ul>
<hr>
<h2 id="Delete-My-Account-Impact-of-Data-Deletion-on-Machine-Learning-Classifiers"><a href="#Delete-My-Account-Impact-of-Data-Deletion-on-Machine-Learning-Classifiers" class="headerlink" title="Delete My Account: Impact of Data Deletion on Machine Learning Classifiers"></a>Delete My Account: Impact of Data Deletion on Machine Learning Classifiers</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10385">http://arxiv.org/abs/2311.10385</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tobias Dam, Maximilian Henzl, Lukas Daniel Klausner</li>
<li>for: 这个论文主要研究了在分类任务中机器学习模型如何受到删除行为影响的表现。</li>
<li>methods: 我们通过使用不同的数据集和机器学习算法进行了多种实验，以研究不同的删除行为场景下数据质量的影响。</li>
<li>results: 我们发现，删除数据量、数据集特点和删除偏好等因素对机器学习模型表现产生强烈的影响。<details>
<summary>Abstract</summary>
Users are more aware than ever of the importance of their own data, thanks to reports about security breaches and leaks of private, often sensitive data in recent years. Additionally, the GDPR has been in effect in the European Union for over three years and many people have encountered its effects in one way or another. Consequently, more and more users are actively protecting their personal data. One way to do this is to make of the right to erasure guaranteed in the GDPR, which has potential implications for a number of different fields, such as big data and machine learning.   Our paper presents an in-depth analysis about the impact of the use of the right to erasure on the performance of machine learning models on classification tasks. We conduct various experiments utilising different datasets as well as different machine learning algorithms to analyse a variety of deletion behaviour scenarios. Due to the lack of credible data on actual user behaviour, we make reasonable assumptions for various deletion modes and biases and provide insight into the effects of different plausible scenarios for right to erasure usage on data quality of machine learning. Our results show that the impact depends strongly on the amount of data deleted, the particular characteristics of the dataset and the bias chosen for deletion and assumptions on user behaviour.
</details>
<details>
<summary>摘要</summary>
用户们现在更加意识到自己的数据重要性，这主要归功于过去几年内的安全泄露和private数据泄露事件的报道。此外，欧盟的GDPR也已经在三年之前生效，许多人已经直接或 indirectly受其影响。因此，更多的用户正在主动保护自己的个人数据。一种方式是通过在GDPR中确保的“右 deletion”来保护自己的数据。这种技术在big data和机器学习等领域可能有各种不同的应用。我们的论文提供了关于使用“右 deletion”的影响对机器学习模型的分类任务性能的深入分析。我们在不同的数据集和机器学习算法上进行了多种实验，以分析不同的删除行为场景。由于实际用户行为的可靠数据缺乏，我们在不同的删除模式和偏见下做出了合理的假设，并对不同的数据质量和机器学习模型的影响进行了分析。我们的结果表明，删除数据的量、特定数据集的特点以及删除模式和偏见的选择都会对数据质量产生强烈的影响。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Modelling-Approach-for-Row-Type-Dependent-Predictive-Analysis-RTDPA-A-Framework-for-Designing-Machine-Learning-Models-for-Credit-Risk-Analysis-in-Banking-Sector"><a href="#Adaptive-Modelling-Approach-for-Row-Type-Dependent-Predictive-Analysis-RTDPA-A-Framework-for-Designing-Machine-Learning-Models-for-Credit-Risk-Analysis-in-Banking-Sector" class="headerlink" title="Adaptive Modelling Approach for Row-Type Dependent Predictive Analysis (RTDPA): A Framework for Designing Machine Learning Models for Credit Risk Analysis in Banking Sector"></a>Adaptive Modelling Approach for Row-Type Dependent Predictive Analysis (RTDPA): A Framework for Designing Machine Learning Models for Credit Risk Analysis in Banking Sector</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10799">http://arxiv.org/abs/2311.10799</a></li>
<li>repo_url: None</li>
<li>paper_authors: Minati Rath, Hema Date</li>
<li>For: 这个论文的目的是提出一种适应行业特点的预测分析方法（RTDPA），以便更好地处理不同行业类型的数据。* Methods: 该方法使用了特定行业类型的数据预处理和特性工程，并选择了传统机器学习预测模型和高级组合技术。* Results: 研究发现，所有预测方法具有精度达90%以上，而RTDPA方法可以为每个行业类型分别应用不同的预测模型，以捕捉每个行业类型的特定特征和模式。这种方法为银行业提供了更加准确和定制化的分类结果。<details>
<summary>Abstract</summary>
In many real-world datasets, rows may have distinct characteristics and require different modeling approaches for accurate predictions. In this paper, we propose an adaptive modeling approach for row-type dependent predictive analysis(RTDPA). Our framework enables the development of models that can effectively handle diverse row types within a single dataset. Our dataset from XXX bank contains two different risk categories, personal loan and agriculture loan. each of them are categorised into four classes standard, sub-standard, doubtful and loss. We performed tailored data pre processing and feature engineering to different row types. We selected traditional machine learning predictive models and advanced ensemble techniques. Our findings indicate that all predictive approaches consistently achieve a precision rate of no less than 90%. For RTDPA, the algorithms are applied separately for each row type, allowing the models to capture the specific patterns and characteristics of each row type. This approach enables targeted predictions based on the row type, providing a more accurate and tailored classification for the given dataset.Additionally, the suggested model consistently offers decision makers valuable and enduring insights that are strategic in nature in banking sector.
</details>
<details>
<summary>摘要</summary>
在许多现实世界数据集中，每行可能具有不同的特点和需要不同的模型预测方法以获得准确的预测结果。在这篇论文中，我们提出了一种适应型模型预测方法（RTDPA），允许模型在单个数据集中处理多种行类型。我们的数据集来自XXX银行，包括两个不同的风险类别：个人贷款和农业贷款。每个类别都被分为四个类别：标准、次normal、 doubtful和损失。我们进行了适应的数据处理和特征工程，并选择了传统的机器学习预测模型和高级的集成技术。我们的发现表明，所有预测方法均能够具有至少90%的准确率。为RTDPA，我们对每个行类型应用了不同的算法，使模型能够捕捉每个行类型特有的特征和模式。这种方法允许模型进行基于行类型的targeted预测，为给定数据集提供更加准确和定制化的分类结果。此外，我们的建议模型可以为银行部门提供有价值和持续的战略性发现。
</details></li>
</ul>
<hr>
<h2 id="Few-shot-Message-Enhanced-Contrastive-Learning-for-Graph-Anomaly-Detection"><a href="#Few-shot-Message-Enhanced-Contrastive-Learning-for-Graph-Anomaly-Detection" class="headerlink" title="Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection"></a>Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10370">http://arxiv.org/abs/2311.10370</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fan Xu, Nan Wang, Xuezhi Wen, Meiqi Gao, Chaoqun Guo, Xibin Zhao</li>
<li>for: The paper is focused on developing a novel few-shot graph anomaly detection model called FMGAD, which can effectively identify anomalies in graph data with limited labeled information.</li>
<li>methods: The proposed FMGAD model uses a self-supervised contrastive learning strategy within and across views to capture intrinsic and transferable structural representations. Additionally, the model employs a Deep-GNN message-enhanced reconstruction module to extensively exploit few-shot label information and disseminate supervision signals to deeper unlabeled nodes.</li>
<li>results: The paper demonstrates that FMGAD achieves better performance than other state-of-the-art methods on six real-world datasets, regardless of artificially injected anomalies or domain-organic anomalies.<details>
<summary>Abstract</summary>
Graph anomaly detection plays a crucial role in identifying exceptional instances in graph data that deviate significantly from the majority. It has gained substantial attention in various domains of information security, including network intrusion, financial fraud, and malicious comments, et al. Existing methods are primarily developed in an unsupervised manner due to the challenge in obtaining labeled data. For lack of guidance from prior knowledge in unsupervised manner, the identified anomalies may prove to be data noise or individual data instances. In real-world scenarios, a limited batch of labeled anomalies can be captured, making it crucial to investigate the few-shot problem in graph anomaly detection. Taking advantage of this potential, we propose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot Message-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a self-supervised contrastive learning strategy within and across views to capture intrinsic and transferable structural representations. Furthermore, we propose the Deep-GNN message-enhanced reconstruction module, which extensively exploits the few-shot label information and enables long-range propagation to disseminate supervision signals to deeper unlabeled nodes. This module in turn assists in the training of self-supervised contrastive learning. Comprehensive experimental results on six real-world datasets demonstrate that FMGAD can achieve better performance than other state-of-the-art methods, regardless of artificially injected anomalies or domain-organic anomalies.
</details>
<details>
<summary>摘要</summary>
GRAPH anomaly detection plays a crucial role in identifying exceptional instances in graph data that deviate significantly from the majority. It has gained substantial attention in various domains of information security, including network intrusion, financial fraud, and malicious comments, etc. Existing methods are primarily developed in an unsupervised manner due to the challenge in obtaining labeled data. For lack of guidance from prior knowledge in unsupervised manner, the identified anomalies may prove to be data noise or individual data instances. In real-world scenarios, a limited batch of labeled anomalies can be captured, making it crucial to investigate the few-shot problem in graph anomaly detection. Taking advantage of this potential, we propose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot Message-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a self-supervised contrastive learning strategy within and across views to capture intrinsic and transferable structural representations. Furthermore, we propose the Deep-GNN message-enhanced reconstruction module, which extensively exploits the few-shot label information and enables long-range propagation to disseminate supervision signals to deeper unlabeled nodes. This module in turn assists in the training of self-supervised contrastive learning. Comprehensive experimental results on six real-world datasets demonstrate that FMGAD can achieve better performance than other state-of-the-art methods, regardless of artificially injected anomalies or domain-organic anomalies.
</details></li>
</ul>
<hr>
<h2 id="FIKIT-Priority-Based-Real-time-GPU-Multi-tasking-Scheduling-with-Kernel-Identification"><a href="#FIKIT-Priority-Based-Real-time-GPU-Multi-tasking-Scheduling-with-Kernel-Identification" class="headerlink" title="FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification"></a>FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel Identification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10359">http://arxiv.org/abs/2311.10359</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenqing Wu</li>
<li>for: 加速机器学习训练、推理和普通HPC任务的高度并行化工作负载，使用GPU设备可以大幅提高计算能力。在云计算集群中，通过多任务共享来利用GPU的计算能力是非常受需求的，因为有更多的任务请求 чемGPU的数量。</li>
<li>methods: 我们提出了一种基于FIKIT（填充间隔内核时间）的核心级调度策略，该策略利用任务级别优先级信息、细化的核心标识和核心测量，以便在高优先级任务的间隔时间内执行低优先级任务。</li>
<li>results: 在一组ML模型上，基于FIKIT的推理系统在GPU共享模式下加速高优先级任务的执行速度，相比JCT，高优先级任务的加速比例在1.33到14.87倍之间，而且超过一半的情况下加速超过3.5倍。同时，在预约共享模式下，低优先级任务的执行速度与默认GPU共享模式JCT相似，占用了0.84到1倍的时间。此外，我们还限制了核心测量和细化核心调度过程的负担在10%以下。<details>
<summary>Abstract</summary>
Highly parallelized workloads like machine learning training, inferences and general HPC tasks are greatly accelerated using GPU devices. In a cloud computing cluster, serving a GPU's computation power through multi-tasks sharing is highly demanded since there are always more task requests than the number of GPU available. Existing GPU sharing solutions focus on reducing task-level waiting time or task-level switching costs when multiple jobs competing for a single GPU. Non-stopped computation requests come with different priorities, having non-symmetric impact on QoS for sharing a GPU device. Existing work missed the kernel-level optimization opportunity brought by this setting. To address this problem, we present a novel kernel-level scheduling strategy called FIKIT: Filling Inter-kernel Idle Time. FIKIT incorporates task-level priority information, fine-grained kernel identification, and kernel measurement, allowing low priorities task's execution during high priority task's inter-kernel idle time. Thereby, filling the GPU's device runtime fully, and reduce overall GPU sharing impact to cloud services. Across a set of ML models, the FIKIT based inference system accelerated high priority tasks by 1.33 to 14.87 times compared to the JCT in GPU sharing mode, and more than half of the cases are accelerated by more than 3.5 times. Alternatively, under preemptive sharing, the low-priority tasks have a comparable to default GPU sharing mode JCT, with a 0.84 to 1 times ratio. We further limit the kernel measurement and runtime fine-grained kernel scheduling overhead to less than 10%.
</details>
<details>
<summary>摘要</summary>
高度并行化工作负载如机器学习训练、推断和一般高性能计算任务在GPU设备上得到了很大的加速。在云计算集群中，通过多任务共享来利用GPU的计算能力是非常受需求的，因为有更多的任务请求 чемGPU可用。现有的GPU共享解决方案主要集中在缓和任务级别的等待时间或任务级别的转换成本，当多个作业竞争一个GPU时。现有的工作缺乏了核心层级优化机会，这个设定下。为解决这个问题，我们提出了一个新的核心层级排程策略 called FIKIT：填充间隔 kernel Idle Time。FIKIT包括任务级别优先级信息、精确的核心识别和核心衡量，因此在高优先级任务的间隔 kernel Idle Time中执行低优先级任务，从而填充GPU的设备时间，并将全局GPU共享影响减少到云服务。遍历一系列的ML模型，基于FIKIT的推断系统在GPU共享模式下加速高优先级任务1.33到14.87倍，并大多数情况上增加了超过3.5倍。另一方面，在预设共享模式下，低优先级任务与默认GPU共享模式JCT相似，几乎没有差异。我们进一步限制了测量和精确核心排程调度的开销，以便在10%以下。
</details></li>
</ul>
<hr>
<h2 id="How-False-Data-Affects-Machine-Learning-Models-in-Electrochemistry"><a href="#How-False-Data-Affects-Machine-Learning-Models-in-Electrochemistry" class="headerlink" title="How False Data Affects Machine Learning Models in Electrochemistry?"></a>How False Data Affects Machine Learning Models in Electrochemistry?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10795">http://arxiv.org/abs/2311.10795</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Demodesu/How-False-Data-Affects-Machine-Learning-Models-in-Electrochemistry">https://github.com/Demodesu/How-False-Data-Affects-Machine-Learning-Models-in-Electrochemistry</a></li>
<li>paper_authors: Krittapong Deshsorna, Luckhana Lawtrakul, Pawin Iamprasertkun</li>
<li>for: This study aims to evaluate the performance of machine learning models in noisy electrochemical data and to determine whether stacking models can provide robustness to weak-to-noise models.</li>
<li>methods: The study uses 12 standalone models and a stacking model to test the performance of different machine learning models on electrochemical data. The models include XGB, LGBM, RF, GB, ADA, NN, ELAS, LASS, RIDGE, SVM, KNN, DT, and the stacking model.</li>
<li>results: The study finds that linear models handle noise well but suffer from low prediction accuracy, while tree-based models have poor noise handling but high prediction accuracy. The stacking model exhibits both high accuracy and good noise handling, making it a viable choice for beginner and experienced machine learning researchers in electrochemistry. Additionally, the study shows that neural networks are not suitable for electrochemical data and can be susceptible to noise.<details>
<summary>Abstract</summary>
Recently, the selection of machine learning model based on only the data distribution without concerning the noise of the data. This study aims to distinguish, which models perform well under noisy data, and establish whether stacking machine learning models actually provide robustness to otherwise weak-to-noise models. The electrochemical data were tested with 12 standalone models and stacking model. This includes XGB, LGBM, RF, GB, ADA, NN, ELAS, LASS, RIDGE, SVM, KNN, DT, and the stacking model. It is found that linear models handle noise well with the average error of (slope) to 1.75 F g-1 up to error per 100% percent noise added; but it suffers from prediction accuracy due to having an average of 60.19 F g-1 estimated at minimal error at 0% noise added. Tree-based models fail in terms of noise handling (average slope is 55.24 F g-1 at 100% percent noise), but it can provide higher prediction accuracy (lowest error of 23.9 F g-1) than that of linear. To address the controversial between prediction accuracy and error handling, the stacking model was constructed, which is not only show high accuracy (intercept of 25.03 F g-1), but it also exhibits good noise handling (slope of 43.58 F g-1), making stacking models a relatively low risk and viable choice for beginner and experienced machine learning research in electrochemistry. Even though neural networks (NN) are gaining popularity in the electrochemistry field. However, this study presents that NN is not suitable for electrochemical data, and improper tuning resulting in a model that is susceptible to noise. Thus, STACK models should provide better benefits in that even with untuned base models, they can achieve an accurate and noise-tolerant model. Overall, this work provides insight into machine learning model selection for electrochemical data, which should aid the understanding of data science in chemistry context.
</details>
<details>
<summary>摘要</summary>
近来，选择基于数据分布而不考虑数据噪声的机器学习模型。这项研究目的是分辨哪些模型在噪声数据上表现良好，并确定堆叠机器学习模型是否提供强度噪声模型的可靠性。使用12个独立模型和堆叠模型测试电化学数据。包括XGB、LGBM、RF、GB、ADA、NN、ELAS、LASS、RIDGE、SVM、KNN和DT模型，以及堆叠模型。结果显示线性模型在噪声数据上处理噪声well，其平均误差为1.75 F g-1，但是它因为误差最小值为0% 噪声添加而导致预测精度低下。树状模型在噪声处理方面失败（平均 Slope 为55.24 F g-1），但它可以提供更高的预测精度（最低误差为23.9 F g-1）。为了解决预测精度和噪声处理之间的矛盾，堆叠模型被构建，它不仅显示高精度（ intercept 为25.03 F g-1），而且 также表现良好的噪声处理（ Slope 为43.58 F g-1）。因此，堆叠模型可以在电化学领域中提供低风险且可靠的选择。虽然神经网络（NN）在电化学领域中获得了流行，但这项研究表明NN不适合电化学数据，并且不当调整可能导致模型易受噪声的影响。因此，STACK模型可以提供更好的利益，即甚至无需调整基本模型，可以实现准确且噪声耐受的模型。总之，这项研究为电化学数据机器学习模型选择提供了新的理解，帮助数据科学在化学上下文中进行更好的发展。
</details></li>
</ul>
<hr>
<h2 id="Towards-Machine-Learning-based-Quantitative-Hyperspectral-Image-Guidance-for-Brain-Tumor-Resection"><a href="#Towards-Machine-Learning-based-Quantitative-Hyperspectral-Image-Guidance-for-Brain-Tumor-Resection" class="headerlink" title="Towards Machine Learning-based Quantitative Hyperspectral Image Guidance for Brain Tumor Resection"></a>Towards Machine Learning-based Quantitative Hyperspectral Image Guidance for Brain Tumor Resection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10321">http://arxiv.org/abs/2311.10321</a></li>
<li>repo_url: None</li>
<li>paper_authors: David Black, Declan Byrne, Anna Walke, Sidong Liu, Antonio Di leva, Sadahiro Kaneko, Walter Stummer, Septimiu Salcudean, Eric Suero Molina<br>for: 这个论文主要目标是为了开发一种基于多光谱的肿瘤分类系统，以帮助 neurosurgeon 在运行时分辨不同类型的肿瘤。methods: 这个论文使用了五种氧化酶的谱荧光特征，通过多光谱成像技术来分析这些氧化酶的谱荧光特征，并使用机器学习算法来类型不同的肿瘤和组织。results: 这个论文的结果表明，使用这五种氧化酶的谱荧光特征可以准确地分类不同类型的肿瘤和组织，并且这些氧化酶的谱荧光特征在不同的肿瘤和组织中具有不同的异常性。<details>
<summary>Abstract</summary>
Complete resection of malignant gliomas is hampered by the difficulty in distinguishing tumor cells at the infiltration zone. Fluorescence guidance with 5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work characterized five fluorophores' emission spectra in most human brain tumors. In this paper, the effectiveness of these five spectra was explored for different tumor and tissue classification tasks in 184 patients (891 hyperspectral measurements) harboring low- (n=30) and high-grade gliomas (n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2), miscellaneous (n=10) and metastases (n=8). Four machine learning models were trained to classify tumor type, grade, glioma margins and IDH mutation. Using random forests and multi-layer perceptrons, the classifiers achieved average test accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore abundances varied between tumor margin types and tumor grades (p < 0.01). For tissue type, at least four of the five fluorophore abundances were found to be significantly different (p < 0.01) between all classes. These results demonstrate the fluorophores' differing abundances in different tissue classes, as well as the value of the five fluorophores as potential optical biomarkers, opening new opportunities for intraoperative classification systems in fluorescence-guided neurosurgery.
</details>
<details>
<summary>摘要</summary>
完全除除恶性肿瘤受到了区分肿瘤细胞的困难所限制。使用5-ALA的荧光导航可以帮助达成这个目标。以前的工作通过多光谱成像技术 caracterized five fluorophores的辐射 спектrum在人脑肿瘤中的表达。本文 investigate了这五种辐射的效iveness для不同的肿瘤和组织类型分类任务，在184名患者（891次谱测）中进行了研究，其中有30例低级 glioma、115例高级 glioma、非 glial主要脑肿瘤19例、辐射nekrosis 2例、其他10例和 метастаasis 8例。四种机器学习模型被训练来分类肿瘤类型、级别、 glioma 边缘和 IDH 突变。使用随机森林和多层感知器，分类器在测试集上达到了74-82%、79%、81%和93%的平均测试准确率。五种辐射含量在肿瘤边缘类型和肿瘤级别之间有 statistically significant differences（P < 0.01）。对于组织类型，至少有四种辐射含量都是 statistically significant differences（P < 0.01）。这些结果表明五种辐射含量在不同的组织类型之间有 statistically significant differences，同时也表明了这五种辐射的可能作为光学生物标志的价值，开启了新的可见光导航系统在肿瘤预置手术中的新机会。
</details></li>
</ul>
<hr>
<h2 id="Graph-Sparsifications-using-Neural-Network-Assisted-Monte-Carlo-Tree-Search"><a href="#Graph-Sparsifications-using-Neural-Network-Assisted-Monte-Carlo-Tree-Search" class="headerlink" title="Graph Sparsifications using Neural Network Assisted Monte Carlo Tree Search"></a>Graph Sparsifications using Neural Network Assisted Monte Carlo Tree Search</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10316">http://arxiv.org/abs/2311.10316</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/abureyanahmed/gnn-msts-sparsification">https://github.com/abureyanahmed/gnn-msts-sparsification</a></li>
<li>paper_authors: Alvin Chiu, Mithun Ghosh, Reyan Ahmed, Kwang-Sung Jun, Stephen Kobourov, Michael T. Goodrich</li>
<li>for: 这个论文是为了计算图 sparse 的目的而写的。</li>
<li>methods: 该论文使用了图神经网络和 Monte Carlo 搜索来计算图 sparse。首先，使用图神经网络来训练一个可以接受部分解决方案并提出新节点的模型，然后使用这个模型在 Monte Carlo 搜索中计算一个简化后的图。</li>
<li>results: 该方法在不同类型的图上consistently 超过了一些标准的approximation algorithm，并经常找到最佳解决方案。<details>
<summary>Abstract</summary>
Graph neural networks have been successful for machine learning, as well as for combinatorial and graph problems such as the Subgraph Isomorphism Problem and the Traveling Salesman Problem. We describe an approach for computing graph sparsifiers by combining a graph neural network and Monte Carlo Tree Search. We first train a graph neural network that takes as input a partial solution and proposes a new node to be added as output. This neural network is then used in a Monte Carlo search to compute a sparsifier. The proposed method consistently outperforms several standard approximation algorithms on different types of graphs and often finds the optimal solution.
</details>
<details>
<summary>摘要</summary>
图 нейрон网络已成功应用于机器学习和组合问题，如子图同构问题和旅行商问题。我们提出了一种基于图 нейрон网络和Monte Carlo搜索的图简化算法。我们首先训练了一个图 нейрон网络，它接受一个半解决方案并生成一个新的节点作为输出。这个神经网络然后在Monte Carlo搜索中用于计算简化图。我们的方法常常超过一些标准近似算法，并经常找到优化解决方案。
</details></li>
</ul>
<hr>
<h2 id="Interpretable-Modeling-of-Single-cell-perturbation-Responses-to-Novel-Drugs-Using-Cycle-Consistence-Learning"><a href="#Interpretable-Modeling-of-Single-cell-perturbation-Responses-to-Novel-Drugs-Using-Cycle-Consistence-Learning" class="headerlink" title="Interpretable Modeling of Single-cell perturbation Responses to Novel Drugs Using Cycle Consistence Learning"></a>Interpretable Modeling of Single-cell perturbation Responses to Novel Drugs Using Cycle Consistence Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10315">http://arxiv.org/abs/2311.10315</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Huang, Aichun Zhu, Hui Liu</li>
<li>for: 本研究旨在开发一种深度学习框架，用于预测药物对细胞的影响。</li>
<li>methods: 该框架基于编码器-解码器架构，将初始细胞状态映射到一个缺失空间中，其中假设药物干扰对细胞状态的效果follows linear additivity。另外，我们还引入了一种循环一致性约束，以保证初始细胞状态下药物干扰后的细胞响应能够恢复到初始细胞状态。</li>
<li>results: 我们对三种不同类型的数据集进行验证，包括维度肿瘤响应、蛋白质响应和单个细胞肿瘤响应。结果显示，我们的模型在比较状态-of-the-art方法时表现出了更好的性能。<details>
<summary>Abstract</summary>
Phenotype-based screening has attracted much attention for identifying cell-active compounds. Transcriptional and proteomic profiles of cell population or single cells are informative phenotypic measures of cellular responses to perturbations. In this paper, we proposed a deep learning framework based on encoder-decoder architecture that maps the initial cellular states to a latent space, in which we assume the effects of drug perturbation on cellular states follow linear additivity. Next, we introduced the cycle consistency constraints to enforce that initial cellular state subjected to drug perturbations would produce the perturbed cellular responses, and, conversely, removal of drug perturbation from the perturbed cellular states would restore the initial cellular states. The cycle consistency constraints and linear modeling in latent space enable to learn interpretable and transferable drug perturbation representations, so that our model can predict cellular response to unseen drugs. We validated our model on three different types of datasets, including bulk transcriptional responses, bulk proteomic responses, and single-cell transcriptional responses to drug perturbations. The experimental results show that our model achieves better performance than existing state-of-the-art methods.
</details>
<details>
<summary>摘要</summary>
生物学上的现象型层次检测已经吸引了很多关注，以找到活性物质的潜在作用。 транскрипцион和蛋白质Profile of cell population or single cells are informative phenotypic measures of cellular responses to perturbations. 在这篇论文中，我们提出了基于编码器-解码器架构的深度学习框架，将初始cellular states映射到一个缺失空间中，我们假设药物干扰对cellular states的影响是线性的加性。然后，我们引入了循环一致性约束，要求初始cellular state exposed to drug perturbations would produce the perturbed cellular responses，并且，从扰动cellular states中移除药物干扰后，初始cellular states可以恢复到原始的cellular states。这些循环一致性约束和线性模型在缺失空间中学习可读取和可传递的药物干扰表示，使得我们的模型可以预测未经见过的药物的作用。我们在三种不同的数据集上验证了我们的模型，包括分布式转录表达数据集、分布式蛋白质表达数据集和单个cell transcriptional responses to drug perturbations。实验结果显示，我们的模型在与现有的状态艺术方法相比，有更好的性能。
</details></li>
</ul>
<hr>
<h2 id="Imagination-augmented-Hierarchical-Reinforcement-Learning-for-Safe-and-Interactive-Autonomous-Driving-in-Urban-Environments"><a href="#Imagination-augmented-Hierarchical-Reinforcement-Learning-for-Safe-and-Interactive-Autonomous-Driving-in-Urban-Environments" class="headerlink" title="Imagination-augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments"></a>Imagination-augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10309">http://arxiv.org/abs/2311.10309</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sang-Hyun Lee, Yoonjae Jung, Seung-Woo Seo</li>
<li>for: 本研究旨在开发一种可应用于实际导航任务中的层次强化学习（HRL）算法，以解决现有HRL算法无法应用于实际导航任务中的问题。</li>
<li>methods: 本研究提出了一种新的 imagination-augmented HRL（IAHRL）算法，该算法使得导航机器人可以在实际导航任务中学习安全和互动性的行为。特别是，高级策略通过解释低级策略所生成的互动行为来决定低级策略，而低级策略则是通过优化策略生成器来生成安全和结构化的行为，并遵循任务特定的规则。</li>
<li>results: 在五个复杂的城市驾驶任务中，我们的层次代理成功完成了安全意识和互动行为，其成功率和平均话语步数都高于基elines。<details>
<summary>Abstract</summary>
Hierarchical reinforcement learning (HRL) has led to remarkable achievements in diverse fields. However, existing HRL algorithms still cannot be applied to real-world navigation tasks. These tasks require an agent to perform safety-aware behaviors and interact with surrounding objects in dynamic environments. In addition, an agent in these tasks should perform consistent and structured exploration as they are long-horizon and have complex structures with diverse objects and task-specific rules. Designing HRL agents that can handle these challenges in real-world navigation tasks is an open problem. In this paper, we propose imagination-augmented HRL (IAHRL), a new and general navigation algorithm that allows an agent to learn safe and interactive behaviors in real-world navigation tasks. Our key idea is to train a hierarchical agent in which a high-level policy infers interactions by interpreting behaviors imagined with low-level policies. Specifically, the high-level policy is designed with a permutation-invariant attention mechanism to determine which low-level policy generates the most interactive behavior, and the low-level policies are implemented with an optimization-based behavior planner to generate safe and structured behaviors following task-specific rules. To evaluate our algorithm, we introduce five complex urban driving tasks, which are among the most challenging real-world navigation tasks. The experimental results indicate that our hierarchical agent performs safety-aware behaviors and properly interacts with surrounding vehicles, achieving higher success rates and lower average episode steps than baselines in urban driving tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Leveraging-Function-Space-Aggregation-for-Federated-Learning-at-Scale"><a href="#Leveraging-Function-Space-Aggregation-for-Federated-Learning-at-Scale" class="headerlink" title="Leveraging Function Space Aggregation for Federated Learning at Scale"></a>Leveraging Function Space Aggregation for Federated Learning at Scale</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10291">http://arxiv.org/abs/2311.10291</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Dhawan, Nicole Mitchell, Zachary Charles, Zachary Garrett, Gintare Karolina Dziugaite</li>
<li>for: 这篇论文旨在提出一种基于函数空间视角的联合学习算法，以提高联合学习的稳定性和个性化能力。</li>
<li>methods: 该算法使用客户端模型的投影函数来对本地更新进行融合，并使用估计基于沃尔什信息来估计客户端模型的误差。</li>
<li>results: 对实际大规模跨设备测试，该算法在客户端模型偏离度增加时表现更加稳定，并在本地训练轮次增加时表现出显著提高。此外，该算法可以更好地实现本地化个性化，例如在 Stack Overflow 上进行几步个性化后， FedFish 比 FedAvg 提高了下一个token预测的正确率7%。<details>
<summary>Abstract</summary>
The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local training epochs increase. Further, FedFish results in global networks that are more amenable to efficient personalization via local fine-tuning on the same or shifted data distributions. For instance, federated pretraining on the C4 dataset, followed by few-shot personalization on Stack Overflow, results in a 7% improvement in next-token prediction by FedFish over FedAvg.
</details>
<details>
<summary>摘要</summary>
联邦学习模式对联邦学习方法的发展提供了启发，包括将多个客户端更新融合到全球服务器模型中，无需分享客户数据。许多联邦学习算法，包括标准的联邦均值（FedAvg），使用直接（可能是加权）平均客户参数更新，基于分布式优化的结果。在这个研究中，我们从函数空间的角度出发，提出一个新的算法，FedFish，它将客户端的本地近似函数学习结果融合，使用估计基于客户端的费雪信息。我们在实际的大规模跨设备测试中评估了FedFish。相比于客户端模型偏离的情况下，FedFish的表现更加稳定，我们的评估结果显示，当客户端训练epoch增加时，FedFish的表现会比FedAvg更好。此外，FedFish可以实现更好的本地化，例如在C4 dataset上进行联邦预训，然后在相同或类似数据分布上进行几步本地微调，可以得到7%的下一个字预测提升。
</details></li>
</ul>
<hr>
<h2 id="Sobol-Sequence-Optimization-for-Hardware-Efficient-Vector-Symbolic-Architectures"><a href="#Sobol-Sequence-Optimization-for-Hardware-Efficient-Vector-Symbolic-Architectures" class="headerlink" title="Sobol Sequence Optimization for Hardware-Efficient Vector Symbolic Architectures"></a>Sobol Sequence Optimization for Hardware-Efficient Vector Symbolic Architectures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10277">http://arxiv.org/abs/2311.10277</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sercan Aygun, M. Hassan Najafi</li>
<li>for: 本研究旨在提高高维计算（Hyperdimensional Computing，HDC）系统的性能，通过优化数据编码方法来提高高维向量Symbolic Sequence（Hypervector）的质量。</li>
<li>methods: 本研究使用Quasi-Random Sequence（SobolSequence）来生成高维向量，并提出了一种优化算法来选择最佳的SobolSequence来生成最小相关性的高维向量。</li>
<li>results: 对于语言和标题分类两个应用，本研究的实验结果显示，使用SobolSequence生成高维向量可以提高准确率，相比 traditional方法（基于线性反馈Shift Register和MATLAB随机函数），准确率提高10.79%，同时具有更低的能耗和更高的面积-延迟产品。<details>
<summary>Abstract</summary>
Hyperdimensional computing (HDC) is an emerging computing paradigm with significant promise for efficient and robust learning. In HDC, objects are encoded with high-dimensional vector symbolic sequences called hypervectors. The quality of hypervectors, defined by their distribution and independence, directly impacts the performance of HDC systems. Despite a large body of work on the processing parts of HDC systems, little to no attention has been paid to data encoding and the quality of hypervectors. Most prior studies have generated hypervectors using inherent random functions, such as MATLAB`s or Python`s random function. This work introduces an optimization technique for generating hypervectors by employing quasi-random sequences. These sequences have recently demonstrated their effectiveness in achieving accurate and low-discrepancy data encoding in stochastic computing systems. The study outlines the optimization steps for utilizing Sobol sequences to produce high-quality hypervectors in HDC systems. An optimization algorithm is proposed to select the most suitable Sobol sequences for generating minimally correlated hypervectors, particularly in applications related to symbol-oriented architectures. The performance of the proposed technique is evaluated in comparison to two traditional approaches of generating hypervectors based on linear-feedback shift registers and MATLAB random function. The evaluation is conducted for two applications: (i) language and (ii) headline classification. Our experimental results demonstrate accuracy improvements of up to 10.79%, depending on the vector size. Additionally, the proposed encoding hardware exhibits reduced energy consumption and a superior area-delay product.
</details>
<details>
<summary>摘要</summary>
高维ensional计算（HDC）是一种出现在计算机科学中的新型计算模式，它提供了高效和可靠的学习机制。在HDC中，对象被编码为高维度 вектор符号序列called hypervectors。 hypervectors的质量直接影响HDC系统的性能。despite a large body of work on the processing parts of HDC systems, little attention has been paid to data encoding and the quality of hypervectors. Most prior studies have generated hypervectors using inherent random functions, such as MATLAB`s or Python`s random function. This work introduces an optimization technique for generating hypervectors by employing quasi-random sequences. These sequences have recently demonstrated their effectiveness in achieving accurate and low-discrepancy data encoding in stochastic computing systems. The study outlines the optimization steps for utilizing Sobol sequences to produce high-quality hypervectors in HDC systems. An optimization algorithm is proposed to select the most suitable Sobol sequences for generating minimally correlated hypervectors, particularly in applications related to symbol-oriented architectures. The performance of the proposed technique is evaluated in comparison to two traditional approaches of generating hypervectors based on linear-feedback shift registers and MATLAB random function. The evaluation is conducted for two applications: (i) language and (ii) headline classification. Our experimental results demonstrate accuracy improvements of up to 10.79%, depending on the vector size. Additionally, the proposed encoding hardware exhibits reduced energy consumption and a superior area-delay product.
</details></li>
</ul>
<hr>
<h2 id="Multiscale-Hodge-Scattering-Networks-for-Data-Analysis"><a href="#Multiscale-Hodge-Scattering-Networks-for-Data-Analysis" class="headerlink" title="Multiscale Hodge Scattering Networks for Data Analysis"></a>Multiscale Hodge Scattering Networks for Data Analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10270">http://arxiv.org/abs/2311.10270</a></li>
<li>repo_url: None</li>
<li>paper_authors: Naoki Saito, Stefan C. Schonsheck, Eugene Shvarts</li>
<li>for: 这篇论文旨在提出一种基于 simplicial complexes 的新型散度网络，用于处理信号。</li>
<li>methods: 该方法使用 multiscale basis dictionaries 和 layered structure，类似于 convolutional neural network (CNN)，以提取信号特征。</li>
<li>results: 该方法可以提取具有抗变性和强健性的特征，并且可以用于 signal classification、domain classification 和 molecular dynamics prediction。<details>
<summary>Abstract</summary>
We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \N$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the underlying graphs). Importantly, the use of multiscale basis dictionaries in our MHSNs admits a natural pooling operation that is akin to local pooling in CNNs, and which may be performed either locally or per-scale. These pooling operations are harder to define in both traditional scattering networks based on Morlet wavelets, and geometric scattering networks based on Diffusion Wavelets. As a result, we are able to extract a rich set of descriptive yet robust features that can be used along with very simple machine learning methods (i.e., logistic regression or support vector machines) to achieve high-accuracy classification systems with far fewer parameters to train than most modern graph neural networks. Finally, we demonstrate the usefulness of our MHSNs in three distinct types of problems: signal classification, domain (i.e., graph/simplex) classification, and molecular dynamics prediction.
</details>
<details>
<summary>摘要</summary>
我们提出了一种新的扩散网络，称为多Scale霍德扩散网络（MHSN），用于测量 simplicial 复合体上的信号。我们的构建基于 simplicial 复合体上的多Scale基准词典，即 $\kappa$-GHWT 和 $\kappa$-HGLET，我们之前在 simplicial 复合体上发展了这些基准词典。这些词典包含了多Scale基准向量的 redundancy 集和相应的扩展系数，可以用来描述信号的不同尺度特征。我们的 MHSN 使用层次结构类似于 convolutional neural network (CNN)，将模ulus 的词典系数 moments 层次结构化。这些特征具有对 simplicial 复合体的排序（i.e., 节点重新排序）的不变性。此外，使用多Scale基准词典的使用可以自然地进行本地池化操作，这与传统的扩散网络基于 Morlet 波lets 和 geometric 扩散网络基于Diffusion 波lets不同。这些池化操作可以在多个尺度上进行，并且可以使用本地或每个尺度进行。这些特征可以用与非常简单的机器学习方法（例如 logistic regression 或 support vector machines）结合使用，以实现高精度的分类系统，并且具有许多参数 fewer than most modern graph neural networks。 Finally, we demonstrate the usefulness of our MHSNs in three distinct types of problems: signal classification, domain (i.e., graph/simplex) classification, and molecular dynamics prediction.
</details></li>
</ul>
<hr>
<h2 id="Stable-Differentiable-Causal-Discovery"><a href="#Stable-Differentiable-Causal-Discovery" class="headerlink" title="Stable Differentiable Causal Discovery"></a>Stable Differentiable Causal Discovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10263">http://arxiv.org/abs/2311.10263</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/azizilab/sdcd">https://github.com/azizilab/sdcd</a></li>
<li>paper_authors: Achille Nazaret, Justin Hong, Elham Azizi, David Blei</li>
<li>for: 用于推断 causal relationships as directed acyclic graphs (DAGs)</li>
<li>methods: 使用 Differentiable Causal Discovery (DCD) 方法，具体来说是一种 continuous optimization 方法</li>
<li>results: 提高了 existing DCD 方法的稳定性和性能，能够更好地处理 thousands of variables 的场景<details>
<summary>Abstract</summary>
Inferring causal relationships as directed acyclic graphs (DAGs) is an important but challenging problem. Differentiable Causal Discovery (DCD) is a promising approach to this problem, framing the search as a continuous optimization. But existing DCD methods are numerically unstable, with poor performance beyond tens of variables. In this paper, we propose Stable Differentiable Causal Discovery (SDCD), a new method that improves previous DCD methods in two ways: (1) It employs an alternative constraint for acyclicity; this constraint is more stable, both theoretically and empirically, and fast to compute. (2) It uses a training procedure tailored for sparse causal graphs, which are common in real-world scenarios. We first derive SDCD and prove its stability and correctness. We then evaluate it with both observational and interventional data and on both small-scale and large-scale settings. We find that SDCD outperforms existing methods in both convergence speed and accuracy and can scale to thousands of variables.
</details>
<details>
<summary>摘要</summary>
“推断 causal 关系为导向无环图（DAGs）是一个重要但具有挑战性的问题。可 diferenciable causal discovery（DCD）是一种有前途的方法，它将搜索转化为连续优化问题。但现有的 DCD 方法存在数值不稳定性问题，性能在多个变量上不佳。在这篇论文中，我们提出了稳定可 diferenciable causal discovery（SDCD）方法，这种方法在两个方面提高了现有 DCD 方法：（1）它使用了一种更稳定的 alternating 约束，这个约束是理论上和实际上更稳定，计算快速。（2）它使用了适用于稀疏 causal 图的训练程序，这种图是现实世界中常见的。我们首先 deriv 了 SDCD 并证明其稳定性和正确性。然后，我们对 observational 和 intervencial 数据进行了评估，并在小规模和大规模的设置下进行了评估。我们发现 SDCD 在速度和准确性两个方面都高于现有方法，并且可以扩展到千个变量。”
</details></li>
</ul>
<hr>
<h2 id="FREE-The-Foundational-Semantic-Recognition-for-Modeling-Environmental-Ecosystems"><a href="#FREE-The-Foundational-Semantic-Recognition-for-Modeling-Environmental-Ecosystems" class="headerlink" title="FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems"></a>FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10255">http://arxiv.org/abs/2311.10255</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiyuan Luo, Juntong Ni, Shengyu Chen, Runlong Yu, Yiqun Xie, Licheng Liu, Zhenong Jin, Huaxiu Yao, Xiaowei Jia</li>
<li>For:  This paper aims to develop a new framework called FREE for modeling environmental ecosystems, which can capture the complex relationships between various environmental data over space and time.* Methods: The FREE framework uses Large Language Models (LLMs) to map available environmental data into a text space and convert the traditional predictive modeling task into a semantic recognition problem. This allows for the incorporation of natural language descriptions and the capture of data semantics.* Results: The proposed FREE framework is evaluated in two real-world applications, predicting stream water temperature in the Delaware River Basin and predicting annual corn yield in Illinois and Iowa. The results show that FREE outperforms multiple baseline methods and is more data- and computation-efficient, as it can be pre-trained on simulated data generated by physics-based models.<details>
<summary>Abstract</summary>
Modeling environmental ecosystems is critical for the sustainability of our planet, but is extremely challenging due to the complex underlying processes driven by interactions amongst a large number of physical variables. As many variables are difficult to measure at large scales, existing works often utilize a combination of observable features and locally available measurements or modeled values as input to build models for a specific study region and time period. This raises a fundamental question in advancing the modeling of environmental ecosystems: how to build a general framework for modeling the complex relationships amongst various environmental data over space and time? In this paper, we introduce a new framework, FREE, which maps available environmental data into a text space and then converts the traditional predictive modeling task in environmental science to the semantic recognition problem. The proposed FREE framework leverages recent advances in Large Language Models (LLMs) to supplement the original input features with natural language descriptions. This facilitates capturing the data semantics and also allows harnessing the irregularities of input features. When used for long-term prediction, FREE has the flexibility to incorporate newly collected observations to enhance future prediction. The efficacy of FREE is evaluated in the context of two societally important real-world applications, predicting stream water temperature in the Delaware River Basin and predicting annual corn yield in Illinois and Iowa. Beyond the superior predictive performance over multiple baseline methods, FREE is shown to be more data- and computation-efficient as it can be pre-trained on simulated data generated by physics-based models.
</details>
<details>
<summary>摘要</summary>
Modeling environmental ecosystems is critical for the sustainability of our planet, but it is extremely challenging due to the complex underlying processes driven by interactions amongst a large number of physical variables. As many variables are difficult to measure at large scales, existing works often utilize a combination of observable features and locally available measurements or modeled values as input to build models for a specific study region and time period. This raises a fundamental question in advancing the modeling of environmental ecosystems: how to build a general framework for modeling the complex relationships amongst various environmental data over space and time?In this paper, we introduce a new framework called FREE, which maps available environmental data into a text space and then converts the traditional predictive modeling task in environmental science to the semantic recognition problem. The proposed FREE framework leverages recent advances in Large Language Models (LLMs) to supplement the original input features with natural language descriptions, allowing for the capture of data semantics and the harnessing of irregularities in the input features. When used for long-term prediction, FREE has the flexibility to incorporate newly collected observations to enhance future prediction.We evaluate the efficacy of FREE in the context of two societally important real-world applications: predicting stream water temperature in the Delaware River Basin and predicting annual corn yield in Illinois and Iowa. Compared to multiple baseline methods, FREE achieves superior predictive performance and is more data- and computation-efficient, as it can be pre-trained on simulated data generated by physics-based models.
</details></li>
</ul>
<hr>
<h2 id="Degeneration-of-kernel-regression-with-Matern-kernels-into-low-order-polynomial-regression-in-high-dimension"><a href="#Degeneration-of-kernel-regression-with-Matern-kernels-into-low-order-polynomial-regression-in-high-dimension" class="headerlink" title="Degeneration of kernel regression with Matern kernels into low-order polynomial regression in high dimension"></a>Degeneration of kernel regression with Matern kernels into low-order polynomial regression in high dimension</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10790">http://arxiv.org/abs/2311.10790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sergei Manzhos, Manabu Ihara</li>
<li>for: 用于适应高维特征空间中的可能能能谱表面和物理函数的拟合。</li>
<li>methods: 使用kernel方法，如ridge回归和Gaussian进程回归，特别是使用Matern型kernels。</li>
<li>results: 在高维特征空间中， kernel方法可能变得不稳定，容易变成低阶多项式回归，失去了对kernel方法的优势。这些结果为PIP型多项式模型在中型分子中的成功提供了更多的光UNTING，以及使用physically-motivated（复制）kernels的重要性。<details>
<summary>Abstract</summary>
Kernel methods such as kernel ridge regression and Gaussian process regressions with Matern type kernels have been increasingly used, in particular, to fit potential energy surfaces (PES) and density functionals, and for materials informatics. When the dimensionality of the feature space is high, these methods are used with necessarily sparse data. In this regime, the optimal length parameter of a Matern-type kernel tends to become so large that the method effectively degenerates into a low-order polynomial regression and therefore loses any advantage over such regression. This is demonstrated theoretically as well as numerically on the examples of six- and fifteen-dimensional molecular PES using squared exponential and simple exponential kernels. The results shed additional light on the success of polynomial approximations such as PIP for medium size molecules and on the importance of orders-of-coupling based models for preserving the advantages of kernel methods with Matern type kernels or on the use of physically-motivated (reproducing) kernels.
</details>
<details>
<summary>摘要</summary>
kernel 方法如 kernel ridge regression 和 Gaussian process regression with Matern 类型kernel 在特别是用来适应 potential energy surfaces (PES) 和物理函数als, 并在物理计算中进行材料信息学。当特征空间维度高时，这些方法通常使用必要 sparse 的数据。在这种情况下，Matern 类型 kernel 的最佳尺度参数往往变得非常大，这使得方法实际上变成了低阶多项式回归，从而失去了对 kernel 方法的优势。这些结果也提供了中型分子 PIP 的Success 和 Matern 类型 kernel 的orders-of-coupling 基本模型的重要性。Note: The translation is done using Google Translate, and may not be perfect.
</details></li>
</ul>
<hr>
<h2 id="Stratified-NMF-for-Heterogeneous-Data"><a href="#Stratified-NMF-for-Heterogeneous-Data" class="headerlink" title="Stratified-NMF for Heterogeneous Data"></a>Stratified-NMF for Heterogeneous Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10789">http://arxiv.org/abs/2311.10789</a></li>
<li>repo_url: None</li>
<li>paper_authors: James Chapman, Yotam Yaniv, Deanna Needell</li>
<li>for: 解决约物数据集中的时间和地点差异性问题</li>
<li>methods: 提出了一种修改后的NMF目标函数，即Stratified-NMF，该函数同时学习不同阶段的统计量和共享主题矩阵</li>
<li>results: 实验表明，该方法可以高效精确地学习数据集中的特征表示Here’s the breakdown of each point:</li>
<li>for: The paper is written to resolve the problem of heterogeneity in data collected at different times or locations.</li>
<li>methods: The paper proposes a modified NMF objective called Stratified-NMF, which learns strata-dependent statistics and a shared topics matrix.</li>
<li>results: The paper presents experimental results on synthetic data and real-world datasets to demonstrate the efficiency and accuracy of the method.<details>
<summary>Abstract</summary>
Non-negative matrix factorization (NMF) is an important technique for obtaining low dimensional representations of datasets. However, classical NMF does not take into account data that is collected at different times or in different locations, which may exhibit heterogeneity. We resolve this problem by solving a modified NMF objective, Stratified-NMF, that simultaneously learns strata-dependent statistics and a shared topics matrix. We develop multiplicative update rules for this novel objective and prove convergence of the objective. Then, we experiment on synthetic data to demonstrate the efficiency and accuracy of the method. Lastly, we apply our method to three real world datasets and empirically investigate their learned features.
</details>
<details>
<summary>摘要</summary>
非正式矩阵分解（NMF）是一种重要的数据低维表示技术。然而，经典的NMF不会考虑数据在不同时间或地点采集的差异性。我们解决这个问题，通过解决修改后的NMF目标函数，即Stratified-NMF，它同时学习约束分布和共享话题矩阵。我们开发了乘法更新规则，并证明目标函数的收敛性。然后，我们在synthetic数据上进行实验，以证明方法的效率和准确性。最后，我们应用我们的方法于三个实际世界数据集，并考察其学习出的特征。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/cs.LG_2023_11_17/" data-id="clpztdnmf00v6es88cg9b38cp" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.IV_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/eess.IV_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T09:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-IV/">eess.IV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/eess.IV_2023_11_17/">eess.IV - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Virtual-trajectories-for-I-24-MOTION-data-and-tools"><a href="#Virtual-trajectories-for-I-24-MOTION-data-and-tools" class="headerlink" title="Virtual trajectories for I-24 MOTION: data and tools"></a>Virtual trajectories for I-24 MOTION: data and tools</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10888">http://arxiv.org/abs/2311.10888</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junyi Ji, Yanbing Wang, Derek Gloudemans, Gergely Zachár, William Barbour, Daniel B. Work</li>
<li>for: 这篇论文旨在提供一个基于I-24 MOTION INCEPTION v1.0.0数据集的虚拟轨迹数据集，以解决大规模 yet noisy 轨迹数据集的分析挑战。</li>
<li>methods: 该论文提供了一种基于Python的虚拟轨迹生成实现方式，可以将大规模 raw 数据集转化为虚拟轨迹，以便更好地进行分析。</li>
<li>results: 研究人员通过使用虚拟轨迹数据集，成功地评估了I-24 MOTION INCEPTION v1.0.0数据集中不同车道之间的速度变化和行驶时间。此外，虚拟轨迹数据集还开启了未来对交通波动的研究。<details>
<summary>Abstract</summary>
This article introduces a new virtual trajectory dataset derived from the I-24 MOTION INCEPTION v1.0.0 dataset to address challenges in analyzing large but noisy trajectory datasets. Building on the concept of virtual trajectories, we provide a Python implementation to generate virtual trajectories from large raw datasets that are typically challenging to process due to their size. We demonstrate the practical utility of these trajectories in assessing speed variability and travel times across different lanes within the INCEPTION dataset. The virtual trajectory dataset opens future research on traffic waves and their impact on energy.
</details>
<details>
<summary>摘要</summary>
Note: The Simplified Chinese translation is written in Traditional Chinese characters, which is the standard form of Chinese used in Taiwan and other countries.Here's the breakdown of the translation:1. "This article" is translated as "这篇文章" (zhè běn wén tiāng).2. "introduces" is translated as "介绍" (jiè jiǎo).3. "a new virtual trajectory dataset" is translated as "一个新的虚拟路径集" (yī gè xīn de hū yì lù fāng jī).4. "derived from the I-24 MOTION INCEPTION v1.0.0 dataset" is translated as "基于I-24 MOTION INCEPTION v1.0.0 dataset" (jī yú I-24 MOTION INCEPTION v1.0.0 jiāng dài).5. "to address challenges in analyzing large but noisy trajectory datasets" is translated as "用于处理大小噪嗤的路径集数据" (yòng yú chūng hòu dà xiǎo bīng de lù fāng jī shuō yì).6. "Building on the concept of virtual trajectories" is translated as "基于虚拟路径的概念" (jī yú hū yì lù fāng de guī yì).7. "we provide a Python implementation" is translated as "我们提供Python实现" (wǒ men tí huì Python shí jì).8. "to generate virtual trajectories from large raw datasets" is translated as "生成虚拟路径从大Raw数据" (shēng jì hū yì lù fāng jiǔ raw shuō yì).9. "that are typically challenging to process due to their size" is translated as "因其大小而具有挑战性" (yǐn qí dà xiǎo èr bù huì yǒu zhàng xìng).10. "We demonstrate the practical utility of these trajectories" is translated as "我们示出这些虚拟路径的实用性" (wǒ men shì chuī zhè xī hū yì lù fāng de shí yòng xìng).11. "in assessing speed variability and travel times across different lanes within the INCEPTION dataset" is translated as "在INCEPTION dataset中的不同车道之间的速度变化和旅行时间" (zhī dào zhè yì zhè yì shuāng dào zhī jiān de zhōng dào biàn huà hěn lǚ xíng shí).12. "The virtual trajectory dataset opens future research on traffic waves and their impact on energy" is translated as "虚拟路径集开启了交通波和能源的未来研究" (hū yì lù fāng jī kāi kě yì jiāo yì yǐn yuè yì).Note that the translation is written in Simplified Chinese, which is the standard form of Chinese used in mainland China and other countries. Traditional Chinese is used in Taiwan and other countries.
</details></li>
</ul>
<hr>
<h2 id="SDDPM-Speckle-Denoising-Diffusion-Probabilistic-Models"><a href="#SDDPM-Speckle-Denoising-Diffusion-Probabilistic-Models" class="headerlink" title="SDDPM: Speckle Denoising Diffusion Probabilistic Models"></a>SDDPM: Speckle Denoising Diffusion Probabilistic Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10868">http://arxiv.org/abs/2311.10868</a></li>
<li>repo_url: None</li>
<li>paper_authors: Soumee Guha, Scott T. Acton</li>
<li>for: 这 paper 是为了提出一种新的图像去噪算法，用于去除基于信号的乱噪。</li>
<li>methods: 该算法使用了扩散模型来除去信号依赖的乱噪。</li>
<li>results: 该paper的实验表明，该算法在不同的噪声水平下表现出色，与比较模型相比，其表现更加稳定和高效。<details>
<summary>Abstract</summary>
Coherent imaging systems, such as medical ultrasound and synthetic aperture radar (SAR), are subject to corruption from speckle due to sub-resolution scatterers. Since speckle is multiplicative in nature, the constituent image regions become corrupted to different extents. The task of denoising such images requires algorithms specifically designed for removing signal-dependent noise. This paper proposes a novel image denoising algorithm for removing signal-dependent multiplicative noise with diffusion models, called Speckle Denoising Diffusion Probabilistic Models (SDDPM). We derive the mathematical formulations for the forward process, the reverse process, and the training objective. In the forward process, we apply multiplicative noise to a given image and prove that the forward process is Gaussian. We show that the reverse process is also Gaussian and the final training objective can be expressed as the Kullback Leibler (KL) divergence between the forward and reverse processes. As derived in the paper, the final denoising task is a single step process, thereby reducing the denoising time significantly. We have trained our model with natural land-use images and ultrasound images for different noise levels. Extensive experiments centered around two different applications show that SDDPM is robust and performs significantly better than the comparative models even when the images are severely corrupted.
</details>
<details>
<summary>摘要</summary>
高度一致的干扰系统，如医疗超声和 sintetical aperture radar（SAR），受到微小雷达的扰动，导致图像受到不同程度的损害。由于干扰是加法的，图像中的不同区域受到不同程度的损害。去除这种信号依赖的干扰需要特定的算法。本文提出了一种新的图像去干扰算法，基于扩散模型，称为Speckle Denoising Diffusion Probabilistic Models（SDDPM）。我们 derivate了前向过程、反向过程和训练目标的数学表述。在前向过程中，我们将一个图像加上了加法干扰，并证明了前向过程是高斯分布。我们还证明了反向过程也是高斯分布，最终的训练目标可以表示为高斯差分（KL）散度 между前向和反向过程。根据文章中的 derivation，最终的去干扰任务是单步过程，因此可以减少去干扰时间。我们在不同的陆地使用图像和超声图像上训练了我们的模型，并在不同的噪声水平进行了广泛的实验。结果表明，SDDPM是稳定和高效的，即使图像受到严重的损害也能够达到优秀的去干扰效果。
</details></li>
</ul>
<hr>
<h2 id="Image-Domain-Material-Decomposition-for-Dual-energy-CT-using-Unsupervised-Learning-with-Data-fidelity-Loss"><a href="#Image-Domain-Material-Decomposition-for-Dual-energy-CT-using-Unsupervised-Learning-with-Data-fidelity-Loss" class="headerlink" title="Image-Domain Material Decomposition for Dual-energy CT using Unsupervised Learning with Data-fidelity Loss"></a>Image-Domain Material Decomposition for Dual-energy CT using Unsupervised Learning with Data-fidelity Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10641">http://arxiv.org/abs/2311.10641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junbo Peng, Chih-Wei Chang, Huiqiao Xie, Richard L. J. Qiu, Justin Roper, Tonghe Wang, Beth Bradshaw, Xiangyang Tang, Xiaofeng Yang</li>
<li>for: 这个研究的目的是发展一个不受训练数据的架构，用于静止影像领域内的材料分解。</li>
<li>methods: 这个研究使用了一个无监督学习架构，并且使用了数据量度的一致性来进行图像领域内的材料分解。</li>
<li>results: 研究获得了一个可靠的、不受噪声扰乱的材料分解方法，并且在静止影像领域内进行了实验评估。<details>
<summary>Abstract</summary>
Background: Dual-energy CT (DECT) and material decomposition play vital roles in quantitative medical imaging. However, the decomposition process may suffer from significant noise amplification, leading to severely degraded image signal-to-noise ratios (SNRs). While existing iterative algorithms perform noise suppression using different image priors, these heuristic image priors cannot accurately represent the features of the target image manifold. Although deep learning-based decomposition methods have been reported, these methods are in the supervised-learning framework requiring paired data for training, which is not readily available in clinical settings.   Purpose: This work aims to develop an unsupervised-learning framework with data-measurement consistency for image-domain material decomposition in DECT.
</details>
<details>
<summary>摘要</summary>
背景：双能CT（DECT）和材料分解在医学影像中扮演着重要的角色，但是分解过程可能会受到干扰的强化，导致影像信号噪声比（SNR）严重下降。现有的迭代算法在不同的图像假设上进行干扰抑制，但这些启发式图像假设无法准确地表示目标图像拓扑。虽然深度学习基于的分解方法已经报道，但这些方法需要训练用 paired 数据，这在临床 Settings 中并不可得。目的：本工作旨在开发一个无监督学习框架，以保证数据测量一致性，用于图像领域的材料分解在 DECT 中。
</details></li>
</ul>
<hr>
<h2 id="MIFA-Metadata-Incentives-Formats-and-Accessibility-guidelines-to-improve-the-reuse-of-AI-datasets-for-bioimage-analysis"><a href="#MIFA-Metadata-Incentives-Formats-and-Accessibility-guidelines-to-improve-the-reuse-of-AI-datasets-for-bioimage-analysis" class="headerlink" title="MIFA: Metadata, Incentives, Formats, and Accessibility guidelines to improve the reuse of AI datasets for bioimage analysis"></a>MIFA: Metadata, Incentives, Formats, and Accessibility guidelines to improve the reuse of AI datasets for bioimage analysis</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10443">http://arxiv.org/abs/2311.10443</a></li>
<li>repo_url: None</li>
<li>paper_authors: eresa Zulueta-Coarasa, Florian Jug, Aastha Mathur, Josh Moore, Arrate Muñoz-Barrutia, Liviu Anita, Kola Babalola, Pete Bankhead, Perrine Gilloteaux, Nodar Gogoberidze, Martin Jones, Gerard J. Kleywegt, Paul Korir, Anna Kreshuk, Aybüke Küpcü Yoldaş, Luca Marconato, Kedar Narayan, Nils Norlin, Bugra Oezdemir, Jessica Riesterer, Norman Rzepka, Ugis Sarkans, Beatriz Serrano, Christian Tischer, Virginie Uhlmann, Vladimír Ulman, Matthew Hartley</li>
<li>for: 本研究的目的是提高生物图像分析和处理领域中人工智能方法的发展，通过提供高质量的标注图像数据来帮助训练和开发新方法。</li>
<li>methods: 本研究使用的方法包括将社区专家集结在一起举行工作坊，制定了关于数据格式、元数据、数据展示和分享的标准指南，以及鼓励生成新数据集的奖励。</li>
<li>results: 研究人员认为，基于MIFA（Metadata, Incentives, Formats, and Accessibility）指南的开发将加速生物图像分析领域中人工智能工具的发展，因为它将提高高质量的训练数据的获取和 reuse。<details>
<summary>Abstract</summary>
Artificial Intelligence methods are powerful tools for biological image analysis and processing. High-quality annotated images are key to training and developing new methods, but access to such data is often hindered by the lack of standards for sharing datasets. We brought together community experts in a workshop to develop guidelines to improve the reuse of bioimages and annotations for AI applications. These include standards on data formats, metadata, data presentation and sharing, and incentives to generate new datasets. We are positive that the MIFA (Metadata, Incentives, Formats, and Accessibility) recommendations will accelerate the development of AI tools for bioimage analysis by facilitating access to high quality training data.
</details>
<details>
<summary>摘要</summary>
人工智能技术是生物图像分析和处理中非常有力的工具。高质量的注释图像是训练和开发新方法的关键，但获取这些数据往往受到数据分享标准的限制。我们将社区专家集合在一起，开展工作室，制定了指南，以提高生物图像和注释的重用。这些指南包括数据格式、元数据、数据展示和分享、以及奖励新数据集的创造。我们对MIFA（元数据、激励、格式和可访问性）建议表示乐见，这将加速生物图像分析中AI工具的发展，并且促进高质量训练数据的获取。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/eess.IV_2023_11_17/" data-id="clpztdntv01djes88hkfe9tnw" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.SP_2023_11_17" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/17/eess.SP_2023_11_17/" class="article-date">
  <time datetime="2023-11-17T08:00:00.000Z" itemprop="datePublished">2023-11-17</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-SP/">eess.SP</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/17/eess.SP_2023_11_17/">eess.SP - 2023-11-17</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="GAN-supervised-Seismic-Data-Reconstruction-An-Enhanced-Learning-for-Improved-Generalization"><a href="#GAN-supervised-Seismic-Data-Reconstruction-An-Enhanced-Learning-for-Improved-Generalization" class="headerlink" title="GAN-supervised Seismic Data Reconstruction: An Enhanced-Learning for Improved Generalization"></a>GAN-supervised Seismic Data Reconstruction: An Enhanced-Learning for Improved Generalization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10910">http://arxiv.org/abs/2311.10910</a></li>
<li>repo_url: None</li>
<li>paper_authors: Paul Goyes-Penafiel, Leon Suarez-Rodriguez, Claudia Correa, Henry Arguello</li>
<li>for: 提高深度学习方法的适用范围和数据特性适应性</li>
<li>methods: 使用对抗生成网络（GAN）和重构网络，实现数据异常性控制和特征多样性提高</li>
<li>results: 比基eline方法和深度学习秘密法等提高8dB的PSNR表示性能<details>
<summary>Abstract</summary>
Seismic data interpolation plays a crucial role in subsurface imaging, enabling accurate analysis and interpretation throughout the seismic processing workflow. Despite the widespread exploration of deep supervised learning methods for seismic data reconstruction, several challenges still remain open. Particularly, the requirement of extensive training data and poor domain generalization due to the seismic survey's variability poses significant issues. To overcome these limitations, this paper introduces a deep-learning-based seismic data reconstruction approach that leverages data redundancy. This method involves a two-stage training process. First, an adversarial generative network (GAN) is trained using synthetic seismic data, enabling the extraction and learning of their primary and local seismic characteristics. Second, a reconstruction network is trained with synthetic data generated by the GAN, which dynamically adjusts the noise and distortion level at each epoch to promote feature diversity. This approach enhances the generalization capabilities of the reconstruction network by allowing control over the generation of seismic patterns from the latent space of the GAN, thereby reducing the dependency on large seismic databases. Experimental results on field and synthetic seismic datasets both pre-stack and post-stack show that the proposed method outperforms the baseline supervised learning and unsupervised approaches such as deep seismic prior and internal learning, by up to 8 dB of PSNR.
</details>
<details>
<summary>摘要</summary>
减少数据的重要作用在地球声图减少中发挥重要作用，帮助减少分析和解释过程中的准确性。despite the widespread exploration of deep supervised learning methods for seismic data reconstruction, several challenges still remain open. Particularly, the requirement of extensive training data and poor domain generalization due to the seismic survey's variability poses significant issues. To overcome these limitations, this paper introduces a deep-learning-based seismic data reconstruction approach that leverages data redundancy. This method involves a two-stage training process. First, an adversarial generative network (GAN) is trained using synthetic seismic data, enabling the extraction and learning of their primary and local seismic characteristics. Second, a reconstruction network is trained with synthetic data generated by the GAN, which dynamically adjusts the noise and distortion level at each epoch to promote feature diversity. This approach enhances the generalization capabilities of the reconstruction network by allowing control over the generation of seismic patterns from the latent space of the GAN, thereby reducing the dependency on large seismic databases. Experimental results on field and synthetic seismic datasets both pre-stack and post-stack show that the proposed method outperforms the baseline supervised learning and unsupervised approaches such as deep seismic prior and internal learning, by up to 8 dB of PSNR.
</details></li>
</ul>
<hr>
<h2 id="User-Dynamics-Aware-Edge-Caching-and-Computing-for-Mobile-Virtual-Reality"><a href="#User-Dynamics-Aware-Edge-Caching-and-Computing-for-Mobile-Virtual-Reality" class="headerlink" title="User Dynamics-Aware Edge Caching and Computing for Mobile Virtual Reality"></a>User Dynamics-Aware Edge Caching and Computing for Mobile Virtual Reality</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10645">http://arxiv.org/abs/2311.10645</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mushu Li, Jie Gao, Conghao Zhou, Xuemin Shen, Weihua Zhuang</li>
<li>for: 提高移动虚拟现实（VR）视频流媒体性能</li>
<li>methods: 采用缓存和计算资源的灵活分配策略，以适应用户和网络动态变化，最大化VR视频流媒体性能</li>
<li>results: 对比传统缓存和计算资源分配策略，提出的方法可以显著提高VR视频流媒体性能<details>
<summary>Abstract</summary>
In this paper, we present a novel content caching and delivery approach for mobile virtual reality (VR) video streaming. The proposed approach aims to maximize VR video streaming performance, i.e., minimizing video frame missing rate, by proactively caching popular VR video chunks and adaptively scheduling computing resources at an edge server based on user and network dynamics. First, we design a scalable content placement scheme for deciding which video chunks to cache at the edge server based on tradeoffs between computing and caching resource consumption. Second, we propose a machine learning-assisted VR video delivery scheme, which allocates computing resources at the edge server to satisfy video delivery requests from multiple VR headsets. A Whittle index-based method is adopted to reduce the video frame missing rate by identifying network and user dynamics with low signaling overhead. Simulation results demonstrate that the proposed approach can significantly improve VR video streaming performance over conventional caching and computing resource scheduling strategies.
</details>
<details>
<summary>摘要</summary>
在本文中，我们提出了一种新的移动虚拟现实（VR）视频流传输的内容缓存和交付方法。我们的方法旨在最大化VR视频流传输性能，即最小化视频帧缺失率，通过在边缘服务器上推动受欢迎的VR视频块缓存和根据用户和网络动态进行adaptive资源调度。首先，我们设计了一种可扩展的内容分布 schemes，用于决定边缘服务器上缓存哪些视频块。我们通过考虑计算和缓存资源消耗进行了负荷平衡。其次，我们提出了一种基于机器学习的VR视频交付方案，该方案在边缘服务器上分配计算资源来满足多个VR头戴设备的视频交付请求。我们采用了Whittle指数来减少视频帧缺失率，并通过低信号过载来识别网络和用户动态。实验结果表明，提出的方法可以 significatively提高VR视频流传输性能，比 convential缓存和计算资源调度策略更好。
</details></li>
</ul>
<hr>
<h2 id="A-Universal-Framework-for-Multiport-Network-Analysis-of-Reconfigurable-Intelligent-Surfaces"><a href="#A-Universal-Framework-for-Multiport-Network-Analysis-of-Reconfigurable-Intelligent-Surfaces" class="headerlink" title="A Universal Framework for Multiport Network Analysis of Reconfigurable Intelligent Surfaces"></a>A Universal Framework for Multiport Network Analysis of Reconfigurable Intelligent Surfaces</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10561">http://arxiv.org/abs/2311.10561</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matteo Nerini, Shanpu Shen, Hongyu Li, Marco Di Renzo, Bruno Clerckx</li>
<li>for: 本研究旨在提出一种通用的多口网络分析框架，用于研究受支持系统的各种扩展和改进。</li>
<li>methods: 本研究使用了阻抗、导电、散射参数分析方法来模型受支持系统和受支持架构。</li>
<li>results: 研究者通过三种等效模型来描述受支持系统的影响，并提出了选择合适参数的方法。numerical results提供了额外证明这三种模型之间的等效性。<details>
<summary>Abstract</summary>
Reconfigurable intelligent surface (RIS) is an emerging paradigm able to control the propagation environment in wireless systems. Most of the research on RIS has been dedicated to system-level optimization and, with the advent of beyond diagonal RIS (BD-RIS), to RIS architecture design. However, developing general and unified electromagnetic (EM)-compliant models for RIS-aided systems remains an open problem. In this study, we propose a universal framework for the multiport network analysis of RIS-aided systems. With our framework, we model RIS-aided systems and RIS architectures through impedance, admittance, and scattering parameter analysis. Based on these analyses, three equivalent models are derived accounting for the effects of impedance mismatching and mutual coupling. The three models are then simplified by assuming large transmission distances, perfect matching, and no mutual coupling to understand the role of the RIS in the communication model. The derived simplified models are consistent with the model used in related literature, although we show that an additional approximation is commonly considered in the literature. We discuss the benefits of each analysis in characterizing and optimizing the RIS and how to select the most suitable parameters according to the needs. Numerical results provide additional evidence of the equivalence of the three analyses.
</details>
<details>
<summary>摘要</summary>
可编程智能表面（RIS）是一种新兴思路，可以控制无线系统的宣传环境。大多数RIS研究都集中在系统水平优化和BD-RIS架构设计上。然而，为RIS协助系统设计通用和统一的电磁（EM）适应模型仍然是一个未解决的问题。在这种研究中，我们提出了RIS协助系统多口网络分析的普适框架。我们通过阻抗、导电、散射参数分析来模型RIS协助系统和RIS架构。根据这些分析，我们 derive three equivalent models, each accounting for the effects of impedance mismatching and mutual coupling. These models are then simplified by assuming large transmission distances, perfect matching, and no mutual coupling to understand the role of the RIS in the communication model. We discuss the benefits of each analysis in characterizing and optimizing the RIS and how to select the most suitable parameters according to the needs. Numerical results provide additional evidence of the equivalence of the three analyses.
</details></li>
</ul>
<hr>
<h2 id="A-Tutorial-on-5G-Positioning"><a href="#A-Tutorial-on-5G-Positioning" class="headerlink" title="A Tutorial on 5G Positioning"></a>A Tutorial on 5G Positioning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10551">http://arxiv.org/abs/2311.10551</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lorenzo Italiano, Bernardo Camajori Tedeschini, Mattia Brambilla, Huiping Huang, Monica Nicoli, Henk Wymeersch</li>
<li>for: 本研究旨在为研究人员和实践者提供关于5G位置定位的深入教程，概述了5G网络的历史发展，描述了3GPP标准的当前和未来发布，并讨论了主要的研究趋势。</li>
<li>methods: 本文使用了wireless地址定位的基本概念，全面地定义了测量和建筑，并提供了算法的示例和simulation方法。</li>
<li>results: 本文对3GPP Rel-16位置定位在户外和户内环境中的性能进行了低analyze，并提供了系统配置的变化对位置定位的影响。<details>
<summary>Abstract</summary>
The widespread adoption of the fifth generation (5G) of cellular networks has brought new opportunities for localization-based services. High-precision positioning use cases and functionalities defined by the standards are drawing the interest of vertical industries. In the transition to the deployment, this paper aims to provide an in-depth tutorial on 5G positioning, summarizing the historical events that led to the standardization of cellular-based positioning, describing current and forthcoming releases of the Third Generation Partnership Project (3GPP) standard, and discussing about the major research trends. This paper is intended to represent an exhaustive guide for researchers and practitioners by providing fundamental notions on wireless localization, comprehensive definitions of measurements and architectures, examples of algorithms, and details on simulation approaches. Our approach aims to merge practical aspects of enabled use cases and related requirements with theoretical methodologies and fundamental bounds, allowing to understand the trade-off between system complexity and achievable, i.e., tangible, benefits of 5G positioning services. We also discuss about current limitations to be resolved for delivering accurate positioning solutions. We evaluate the performances of 3GPP Rel-16 positioning in outdoor and indoor environments, providing thorough analyses of the effect of changing the system configuration.
</details>
<details>
<summary>摘要</summary>
fifth generation (5G) 通信网络的普及已经带来了基于本地化服务的新机遇。高精度定位用例和功能由标准定义，吸引了专业领域的关注。在部署过程中，这篇论文的目标是为研究人员和实践者提供5G定位的深入教程，从历史事件的发展到3GPP标准的发布，从当前和未来的3GPP标准发布中概述了主要的研究趋势。这篇论文旨在为研究人员和实践者提供5G定位的权威指南，涵盖无线地位定位的基础知识，完整的定义测量和架构，算法的示例，以及模拟方法。我们的方法旨在将实践的使用案例和相关要求与理论方法和基本上限相结合，以便理解系统复杂性和可以实现的优良定位服务的贸易。我们还讨论了5G定位服务的当前限制，以及需要解决的问题。我们评估了Rel-16版本的定位性能在户外和户内环境中，并提供了丰富的分析结果，包括系统配置变化的效果。
</details></li>
</ul>
<hr>
<h2 id="Mutual-Coupling-in-RIS-Aided-Communication-Experimental-Validation-and-Performance-Evaluation"><a href="#Mutual-Coupling-in-RIS-Aided-Communication-Experimental-Validation-and-Performance-Evaluation" class="headerlink" title="Mutual Coupling in RIS-Aided Communication: Experimental Validation and Performance Evaluation"></a>Mutual Coupling in RIS-Aided Communication: Experimental Validation and Performance Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10544">http://arxiv.org/abs/2311.10544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Pinjun Zheng, Ruiqi Wang, Atif Shamim, Tareq Y. Al-Naffouri</li>
<li>for: 这篇论文探讨了协同扩散（RIS）帮助通信系统中的互相干扰。</li>
<li>methods: 该论文首先引入了一种基于3D全波模拟的新的模型训练方法，然后通过实验测量 Validated the obtained model in a 1-bit quasi-passive RIS prototype operating in the mmWave band.</li>
<li>results: 比较分析表明， employed mutual coupling-aware model and the assessed model parameters are precise, offering a realistic evaluation of mutual coupling in authentic RIS hardware. The results also show that the mutual coupling in RIS exhibits heightened significance with increased RIS amplitude gains and showcases a frequency-dependent effect.<details>
<summary>Abstract</summary>
This paper explores the mutual coupling in the reconfigurable intelligent surface (RIS)-aided communication. Despite the existence of several mutual coupling-aware models for RIS-aided communication, a notable gap remains due to the lack of experimental validation. This paper bridges this gap by first introducing a novel model training approach based on the 3D full-wave simulation and subsequently validating the obtained model via experimental measurements in a 1-bit quasi-passive RIS prototype operating in the mmWave band. Comparative analyses reveal precision in both the employed mutual coupling-aware model and the assessed model parameters, offering a realistic evaluation of mutual coupling in authentic RIS hardware. Utilizing the validated mutual coupling-aware communication model, we systematically examine the impact of mutual coupling on communication performance by adopting the achievable rate as a performance indicator. Our results reveal that the mutual coupling in RIS exhibits heightened significance with increased RIS amplitude gains and showcases a frequency-dependent effect.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="“UWBCarGraz”-Dataset-for-Car-Occupancy-Detection-using-Ultra-Wideband-Radar"><a href="#“UWBCarGraz”-Dataset-for-Car-Occupancy-Detection-using-Ultra-Wideband-Radar" class="headerlink" title="“UWBCarGraz” Dataset for Car Occupancy Detection using Ultra-Wideband Radar"></a>“UWBCarGraz” Dataset for Car Occupancy Detection using Ultra-Wideband Radar</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10478">http://arxiv.org/abs/2311.10478</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jakob Möderl, Stefan Posch, Franz Pernkopf, Klaus Witrisal</li>
<li>for: 这篇论文是为了提出一种基于ultra-wideband雷达的数据驱动车辆占用检测算法，并使用ResNet架构进行训练。</li>
<li>methods: 该算法使用了Channel Impulse Response（CIR）数据集，并对其进行了三种活动水平（呼吸、说话、移动）的训练。与之相比，使用了变分essage passing（VMP）算法的状态艺术。</li>
<li>results: 对比VMP算法，ResNet架构在低信号响应比（SNR）下表现更好，具体来说，在SNR&#x3D;-20dB下，VMP检测器的AUC为0.87，而ResNet架构的AUC为0.91，如果目标人在安静呼吸。对于其他活动水平，表现相似。为了在车辆上部硬件中实现，我们进行了一些减少计算复杂性和提高性能的ablation研究。数据集用于训练和评估算法是公开可用的。<details>
<summary>Abstract</summary>
We present a data-driven car occupancy detection algorithm using ultra-wideband radar based on the ResNet architecture. The algorithm is trained on a dataset of channel impulse responses obtained from measurements at three different activity levels of the occupants (i.e. breathing, talking, moving). We compare the presented algorithm against a state-of-the-art car occupancy detection algorithm based on variational message passing (VMP). Our presented ResNet architecture is able to outperform the VMP algorithm in terms of the area under the receiver operating curve (AUC) at low signal-to-noise ratios (SNRs) for all three activity levels of the target. Specifically, for an SNR of -20 dB the VMP detector achieves an AUC of 0.87 while the ResNet architecture achieves an AUC of 0.91 if the target is sitting still and breathing naturally. The difference in performance for the other activities is similar. To facilitate the implementation in the onboard computer of a car we perform an ablation study to optimize the tradeoff between performance and computational complexity for several ResNet architectures. The dataset used to train and evaluate the algorithm is openly accessible. This facilitates an easy comparison in future works.
</details>
<details>
<summary>摘要</summary>
我们提出了基于ultra-wideband雷达的数据驱动车辆占用检测算法，使用ResNet架构。我们对这种算法进行了训练，并对其进行了与现有state-of-the-art车辆占用检测算法（基于变分消息传递）的比较。我们发现，在低信号响应比（SNR）下，我们的ResNet架构能够超越VMP算法，在三种不同活动水平下的占用率中，具有更高的接收操作曲线面积（AUC）。具体来说，在SNR=-20dB下，VMP检测器的AUC为0.87，而我们的ResNet架构的AUC为0.91，当目标坐在安静地呼吸时。其他活动水平的差异类似。为了在车辆上部硬件中实现，我们进行了一项减少性能和计算复杂度之间的权衡分析，对多个ResNet架构进行了优化。我们使用的训练和评估数据集是公开 accessible，这使得未来的研究更容易进行比较。
</details></li>
</ul>
<hr>
<h2 id="Meta-DSP-A-Meta-Learning-Approach-for-Data-Driven-Nonlinear-Compensation-in-High-Speed-Optical-Fiber-Systems"><a href="#Meta-DSP-A-Meta-Learning-Approach-for-Data-Driven-Nonlinear-Compensation-in-High-Speed-Optical-Fiber-Systems" class="headerlink" title="Meta-DSP: A Meta-Learning Approach for Data-Driven Nonlinear Compensation in High-Speed Optical Fiber Systems"></a>Meta-DSP: A Meta-Learning Approach for Data-Driven Nonlinear Compensation in High-Speed Optical Fiber Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10416">http://arxiv.org/abs/2311.10416</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xinyu Xiao, Zhennan Zhou, Bin Dong, Dingjiong Ma, Li Zhou, Jie Sun</li>
<li>for:  This paper aims to improve the performance of long-haul, high-speed optical fiber systems by developing a novel data-driven nonlinear compensation model based on meta-learning.</li>
<li>methods:  The proposed model, called Meta-DSP, processes multi-modal data across diverse transmission rates, power levels, and channel numbers to enhance signal quality and reduce the complexity of the nonlinear processing algorithm.</li>
<li>results:  Compared to existing methods, Meta-DSP delivers a 0.7 dB increase in the Q-factor and reduces computational complexity by a factor of ten while retaining comparable performance. The model’s scalability and generalization performance make it a promising solution for addressing the critical parameters defining optical communication networks.<details>
<summary>Abstract</summary>
Non-linear effects in long-haul, high-speed optical fiber systems significantly hinder channel capacity. While the Digital Backward Propagation algorithm (DBP) with adaptive filter (ADF) can mitigate these effects, it suffers from an overwhelming computational complexity. Recent solutions have incorporated deep neural networks in a data-driven strategy to alleviate this complexity in the DBP model. However, these models are often limited to a specific symbol rate and channel number, necessitating retraining for different settings, their performance declines significantly under high-speed and high-power conditions. We introduce Meta-DSP, a novel data-driven nonlinear compensation model based on meta-learning that processes multi-modal data across diverse transmission rates, power levels, and channel numbers. This not only enhances signal quality but also substantially reduces the complexity of the nonlinear processing algorithm. Our model delivers a 0.7 dB increase in the Q-factor over Electronic Dispersion Compensation (EDC), and compared to DBP, it curtails computational complexity by a factor of ten while retaining comparable performance. From the perspective of the entire signal processing system, the core idea of Meta-DSP can be employed in any segment of the overall communication system to enhance the model's scalability and generalization performance. Our research substantiates Meta-DSP's proficiency in addressing the critical parameters defining optical communication networks.
</details>
<details>
<summary>摘要</summary>
非线性效应在长距离、高速光纤系统中带来了通道容量的很大障碍。尽管数字倒推算法（DBP）与自适应滤波器（ADF）可以减轻这些效应，但它们受到极高的计算复杂性的困扰。现有的解决方案通常是通过深度神经网络在数据驱动策略中引入它们。然而，这些模型通常只能在特定的符号速率和通道数下工作，需要重新训练，其性能在高速和高功率条件下减退 significatively。我们介绍了 Meta-DSP，一种基于元学习的数据驱动非线性补做模型。这种模型可以处理多模态数据，并在不同的传输速率、功率水平和通道数下工作。这不仅提高了信号质量，还减少了非线性处理算法的计算复杂性。我们的模型与电子排序补做（EDC）相比，提高了Q因子的值0.7dB，相比DBP，它减少了计算复杂性的因子为10，保持了相似的性能。从整个信号处理系统的角度来看，Meta-DSP的核心思想可以在任何系统中实现，以提高模型的扩展性和泛化性表现。我们的研究证明了Meta-DSP在光通信网络中的灵活性和可扩展性。
</details></li>
</ul>
<hr>
<h2 id="Downlink-Transmission-in-FBMC-based-Massive-MIMO-with-Co-located-and-Distributed-Antennas"><a href="#Downlink-Transmission-in-FBMC-based-Massive-MIMO-with-Co-located-and-Distributed-Antennas" class="headerlink" title="Downlink Transmission in FBMC-based Massive MIMO with Co-located and Distributed Antennas"></a>Downlink Transmission in FBMC-based Massive MIMO with Co-located and Distributed Antennas</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10374">http://arxiv.org/abs/2311.10374</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hamed Hosseiny, Arman Farhang, Behrouz Farhang-Boroujeny</li>
<li>for: 该研究探讨了 FBMC-based 巨量MIMO 系统下的实用预编码方法。</li>
<li>methods: 该方法包括两个阶段的预编码器，首先使用 fractionally spaced prefilter (FSP) 每个子载波均匀化通道，然后使用 conventional precoder 将用户信号集中到其空间位置上。</li>
<li>results: 研究人员通过对不完美的通道匀化和 CSI 知识的分析，证明了该预编码器在实际应用中的出色性能。数值评估也表明，该预编码器在比较 OFDM 方法为参照点时表现出色。<details>
<summary>Abstract</summary>
This paper introduces a practical precoding method for the downlink of Filter Bank Multicarrier-based (FBMC-based) massive multiple-input multiple-output (MIMO) systems. The proposed method comprises a two-stage precoder, consisting of a fractionally spaced prefilter (FSP) per subcarrier to equalize the channel across each subcarrier band. This is followed by a conventional precoder that concentrates the signals of different users at their spatial locations, ensuring each user receives only the intended information. In practical scenarios, a perfect channel reciprocity may not hold due to radio chain mismatches in the uplink and downlink. Moreover, the channel state information (CSI) may not be perfectly known at the base station. To address these issues, we theoretically analyze the performance of the proposed precoder in presence of imperfect CSI and channel reciprocity calibration errors. Our investigation covers both co-located (cell-based) and cell-free massive MIMO cases. In the cell-free massive MIMO setup, we propose an access point selection method based on the received SINRs of different users in the uplink. Finally, we conduct numerical evaluations to assess the performance of the proposed precoder. Our results demonstrate the excellent performance of the proposed precoder when compared with the orthogonal frequency division multiplexing (OFDM) method as a benchmark.
</details>
<details>
<summary>摘要</summary>
In practical scenarios, the channel reciprocity may not hold due to radio chain mismatches in the uplink and downlink, and the channel state information (CSI) may not be perfectly known at the base station. To address these issues, the paper analyzes the performance of the proposed precoder in the presence of imperfect CSI and channel reciprocity calibration errors. The investigation covers both co-located (cell-based) and cell-free massive MIMO cases.In the cell-free massive MIMO setup, the paper proposes an access point selection method based on the received SINRs of different users in the uplink. Numerical evaluations are conducted to assess the performance of the proposed precoder, and the results show that it outperforms the orthogonal frequency division multiplexing (OFDM) method as a benchmark.
</details></li>
</ul>
<hr>
<h2 id="Joint-Sensing-and-Communication-Optimization-in-Target-Mounted-STARS-Assisted-Vehicular-Networks-A-MADRL-Approach"><a href="#Joint-Sensing-and-Communication-Optimization-in-Target-Mounted-STARS-Assisted-Vehicular-Networks-A-MADRL-Approach" class="headerlink" title="Joint Sensing and Communication Optimization in Target-Mounted STARS-Assisted Vehicular Networks: A MADRL Approach"></a>Joint Sensing and Communication Optimization in Target-Mounted STARS-Assisted Vehicular Networks: A MADRL Approach</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10352">http://arxiv.org/abs/2311.10352</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haocheng Zhang, Rang Liu, Ming Li, Wei Wang, Qian Liu</li>
<li>for: 提高路边单元（RSU）通信性能 durch 目标车辆上的同时传输和反射表面（STARS）技术。</li>
<li>methods: 使用多智能深度学习（MADRL）框架，在RSU和车辆上部署智能代理，实现协同感知和通信优化。</li>
<li>results: 通过使用目标车辆上的STARS系统，提高感知和通信性能，并在不同环境下进行了比较性分析和对比。<details>
<summary>Abstract</summary>
The utilization of integrated sensing and communication (ISAC) technology has the potential to enhance the communication performance of road side units (RSUs) through the active sensing of target vehicles. Furthermore, installing a simultaneous transmitting and reflecting surface (STARS) on the target vehicle can provide an extra boost to the reflection of the echo signal, thereby improving the communication quality for in-vehicle users. However, the design of this target-mounted STARS system exhibits significant challenges, such as limited information sharing and distributed STARS control. In this paper, we propose an end-to-end multi-agent deep reinforcement learning (MADRL) framework to tackle the challenges of joint sensing and communication optimization in the considered target-mounted STARS assisted vehicle networks. By deploying agents on both RSU and vehicle, the MADRL framework enables RSU and vehicle to perform beam prediction and STARS pre-configuration using their respective local information. To ensure efficient and stable learning for continuous decision-making, we employ the multi-agent soft actor critic (MASAC) algorithm and the multi-agent proximal policy optimization (MAPPO) algorithm on the proposed MADRL framework. Extensive experimental results confirm the effectiveness of our proposed MADRL framework in improving both sensing and communication performance through the utilization of target-mounted STARS. Finally, we conduct a comparative analysis and comparison of the two proposed algorithms under various environmental conditions.
</details>
<details>
<summary>摘要</summary>
utilization of integrated sensing and communication (ISAC) technology 可能会增强路边单元 (RSU) 的通信性能 через active sensing of target vehicles. In addition, installing a simultaneous transmitting and reflecting surface (STARS) on the target vehicle can provide an extra boost to the reflection of the echo signal, thereby improving the communication quality for in-vehicle users. However, the design of this target-mounted STARS system presents significant challenges, such as limited information sharing and distributed STARS control.In this paper, we propose an end-to-end multi-agent deep reinforcement learning (MADRL) framework to address the challenges of joint sensing and communication optimization in the considered target-mounted STARS assisted vehicle networks. By deploying agents on both RSU and vehicle, the MADRL framework enables RSU and vehicle to perform beam prediction and STARS pre-configuration using their respective local information. To ensure efficient and stable learning for continuous decision-making, we employ the multi-agent soft actor critic (MASAC) algorithm and the multi-agent proximal policy optimization (MAPPO) algorithm on the proposed MADRL framework.Extensive experimental results confirm the effectiveness of our proposed MADRL framework in improving both sensing and communication performance through the utilization of target-mounted STARS. Finally, we conduct a comparative analysis and comparison of the two proposed algorithms under various environmental conditions.
</details></li>
</ul>
<hr>
<h2 id="Joint-channel-estimation-and-data-detection-in-massive-MIMO-systems-based-on-diffusion-models"><a href="#Joint-channel-estimation-and-data-detection-in-massive-MIMO-systems-based-on-diffusion-models" class="headerlink" title="Joint channel estimation and data detection in massive MIMO systems based on diffusion models"></a>Joint channel estimation and data detection in massive MIMO systems based on diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10311">http://arxiv.org/abs/2311.10311</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nicolas Zilberstein, Ananthram Swami, Santiago Segarra</li>
<li>for: 这个论文是为了解决大量多输入多Output系统中的混合模型 JOINT频率估计和数据检测问题。</li>
<li>methods: 该论文提出了一种基于扩散模型的 JOINT频率估计和数据检测算法，通过采样joint posterior distribution of symbols和通道来实现约束最大化估计。在实现这个算法时，我们构建了一个扩散过程，该模型joint distribution of channels and symbols given noisy observations，然后运行反向过程来生成样本。</li>
<li>results: 通过数值实验，我们示出了该算法比竞争方法具有更低的归一化平均方差Error和减少预先 overhead。<details>
<summary>Abstract</summary>
We propose a joint channel estimation and data detection algorithm for massive multilple-input multiple-output systems based on diffusion models. Our proposed method solves the blind inverse problem by sampling from the joint posterior distribution of the symbols and channels and computing an approximate maximum a posteriori estimation. To achieve this, we construct a diffusion process that models the joint distribution of the channels and symbols given noisy observations, and then run the reverse process to generate the samples. A unique contribution of the algorithm is to include the discrete prior distribution of the symbols and a learned prior for the channels. Indeed, this is key as it allows a more efficient exploration of the joint search space and, therefore, enhances the sampling process. Through numerical experiments, we demonstrate that our method yields a lower normalized mean squared error than competing approaches and reduces the pilot overhead.
</details>
<details>
<summary>摘要</summary>
我们提出了一种共同频道估计和数据检测算法，用于大规模多输入多出力系统，基于协沃分布。我们的提议方法解决了无目标反问题，通过采样 JOINT posterior distribution 的符号和通道，并计算 Approximate Maximum A Posteriori 估计。为实现这一点，我们构建了一个协沃过程，模型 JOINT 分布符号和通道给噪声观测，然后运行反向过程来生成样本。我们的唯一贡献在于包含符号的精确估计和学习的通道先验。这确实是关键，因为它允许更高效地探索 JOINT 搜索空间，并因此提高采样过程。通过数值实验，我们示出了我们的方法比竞争方法具有较低的 норmalized Mean Squared Error，并降低了卫星负荷。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/17/eess.SP_2023_11_17/" data-id="clpztdnvu01i2es881t1h856u" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.SD_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.SD_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T15:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-SD/">cs.SD</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.SD_2023_11_16/">cs.SD - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="DINO-VITS-Data-Efficient-Noise-Robust-Zero-Shot-Voice-Cloning-via-Multi-Tasking-with-Self-Supervised-Speaker-Verification-Loss"><a href="#DINO-VITS-Data-Efficient-Noise-Robust-Zero-Shot-Voice-Cloning-via-Multi-Tasking-with-Self-Supervised-Speaker-Verification-Loss" class="headerlink" title="DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss"></a>DINO-VITS: Data-Efficient Noise-Robust Zero-Shot Voice Cloning via Multi-Tasking with Self-Supervised Speaker Verification Loss</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09770">http://arxiv.org/abs/2311.09770</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vikentii Pankov, Valeria Pronina, Alexander Kuzmin, Maksim Borisov, Nikita Usoltsev, Xingshan Zeng, Alexander Golubkov, Nikolai Ermolenko, Aleksandra Shirshova, Yulia Matveeva</li>
<li>for: 这个论文的目的是提出一种半监督零shotvoice cloning方法，用于从无标注数据上训练。</li>
<li>methods: 这种方法使用了基于HuBERT的语音转换系统，并在训练数据中添加了噪声，以验证方法的稳定性。</li>
<li>results: 这种方法可以在噪声中提供高质量的生成音频，并且不需要任何类型的噪声或噪声标注。此外，我们还提出了一种多任务协同学习方法，通过结合自动матиче预测和协同学习来提高生成音频的质量。<details>
<summary>Abstract</summary>
Recent progress in self-supervised representation learning has opened up new opportunities for training from unlabeled data and has been a growing trend in voice conversion. However, unsupervised training of voice cloning seems to remain a challenging task. In this paper we propose a semi-supervised zero-shot voice cloning approach that works by adapting a HuBERT-based voice conversion system to the voice cloning task and shows the robustness of such a system to noises both in training data (we add noises resulting in up to 0db signal-to-noise-ratio to 35% of training data with no significant degradation of evaluation metrics) and in the target speaker reference audio at inference. Moreover, such a method does not require any type of denoising or noise-labeling of training data. Finally, we introduce a novel multi-tasking approach by incorporating self-supervised DINO loss into joint training of a CAM++ based speaker verification system and a unit-based VITS cloning system. We show that it significantly improves the quality of generated audio over baselines, especially for noisy target speaker references.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Future-Full-Ocean-Deep-SSPs-Prediction-based-on-Hierarchical-Long-Short-Term-Memory-Neural-Networks"><a href="#Future-Full-Ocean-Deep-SSPs-Prediction-based-on-Hierarchical-Long-Short-Term-Memory-Neural-Networks" class="headerlink" title="Future Full-Ocean Deep SSPs Prediction based on Hierarchical Long Short-Term Memory Neural Networks"></a>Future Full-Ocean Deep SSPs Prediction based on Hierarchical Long Short-Term Memory Neural Networks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09537">http://arxiv.org/abs/2311.09537</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiajun Lu, Hao Zhang, Pengfei Wu, Sijia Li, Wei Huang</li>
<li>for: 提供海上位置导航和时间服务，即PNT服务</li>
<li>methods: 使用层次长短时间记忆（H-LSTM）神经网络进行声速场预测</li>
<li>results: 在不同深度层次上月均声速分布的预测准确性比其他现有方法更高，月均声速分布的误差小于1米&#x2F;秒<details>
<summary>Abstract</summary>
The spatial-temporal distribution of underwater sound velocity affects the propagation mode of underwater acoustic signals. Therefore, rapid estimation and prediction of underwater sound velocity distribution is crucial for providing underwater positioning, navigation and timing (PNT) services. Currently, sound speed profile (SSP) inversion methods have a faster time response rate compared to direct measurement methods, however, most SSP inversion methods focus on constructing spatial dimensional sound velocity fields and are highly dependent on sonar observation data, thus high requirements have been placed on observation data sources. To explore the distribution pattern of sound velocity in the time dimension and achieve future SSP prediction without sonar observation data, we propose a hierarchical long short-term memory (H-LSTM) neural network for SSP prediction. By our SSP prediction method, the sound speed distribution could be estimated without any on-site data measurement process, so that the time efficiency could be greatly improved. Through comparing with other state-of-the-art methods, H-LSTM has better accuracy performance on prediction of monthly average sound velocity distribution, which is less than 1 m/s in different depth layers.
</details>
<details>
<summary>摘要</summary>
<<SYS>>Translate the following text into Simplified Chinese:The spatial-temporal distribution of underwater sound velocity affects the propagation mode of underwater acoustic signals. Therefore, rapid estimation and prediction of underwater sound velocity distribution is crucial for providing underwater positioning, navigation and timing (PNT) services. Currently, sound speed profile (SSP) inversion methods have a faster time response rate compared to direct measurement methods, however, most SSP inversion methods focus on constructing spatial dimensional sound velocity fields and are highly dependent on sonar observation data, thus high requirements have been placed on observation data sources. To explore the distribution pattern of sound velocity in the time dimension and achieve future SSP prediction without sonar observation data, we propose a hierarchical long short-term memory (H-LSTM) neural network for SSP prediction. By our SSP prediction method, the sound speed distribution could be estimated without any on-site data measurement process, so that the time efficiency could be greatly improved. Through comparing with other state-of-the-art methods, H-LSTM has better accuracy performance on prediction of monthly average sound velocity distribution, which is less than 1 m/s in different depth layers.Translation:水下声速分布的空间-时间分布对声音信号的传播模式产生影响，因此快速估计和预测水下声速分布是提供水下定位、导航和时间服务（PNT）的关键。目前，声速Profile（SSP）反向方法有更快的时间响应率，但大多数SSP反向方法都是建立空间维度的声速场，高度依赖于声波观测数据，因此对观测数据的要求非常高。为了探索声速分布的时间维度分布 pattern和实现未来SSP预测无需声波观测数据，我们提议使用层次long short-term memory（H-LSTM）神经网络进行SSP预测。我们的预测方法可以无需任何现场数据测量过程，因此可以大幅提高时间效率。与其他现有方法比较，H-LSTM在月均声速分布预测中表现出更高的准确性，声速分布在不同深度层中的误差低于1 m/s。
</details></li>
</ul>
<hr>
<h2 id="AQUATK-An-Audio-Quality-Assessment-Toolkit"><a href="#AQUATK-An-Audio-Quality-Assessment-Toolkit" class="headerlink" title="AQUATK: An Audio Quality Assessment Toolkit"></a>AQUATK: An Audio Quality Assessment Toolkit</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10113">http://arxiv.org/abs/2311.10113</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashvala Vinay, Alexander Lerch</li>
<li>for:  bridging the gap between recent advancements in Neural Audio Synthesis (NAS) and standardized evaluation methodologies.</li>
<li>methods:  open-source Python library with a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and multiple operating modes to accommodate various user needs.</li>
<li>results:  simplifies and standardizes the evaluation of NAS systems.<details>
<summary>Abstract</summary>
Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs.
</details>
<details>
<summary>摘要</summary>
最近的神经音频合成（NAS）技术的发展速度超过了评估方法和工具的开发。为了bridging这个差距，我们介绍了AquaTk，一个开源的Python库，专门用于简化和标准化NAS系统的评估。AquaTk提供了多种音频质量指标，包括Python中Unique实现的基本PEAQ算法，并在多种模式下运行，以满足不同用户需求。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.SD_2023_11_16/" data-id="clpztdnp3012ges88d9qg552p" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-eess.AS_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/eess.AS_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T14:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/eess-AS/">eess.AS</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/eess.AS_2023_11_16/">eess.AS - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Improving-fairness-for-spoken-language-understanding-in-atypical-speech-with-Text-to-Speech"><a href="#Improving-fairness-for-spoken-language-understanding-in-atypical-speech-with-Text-to-Speech" class="headerlink" title="Improving fairness for spoken language understanding in atypical speech with Text-to-Speech"></a>Improving fairness for spoken language understanding in atypical speech with Text-to-Speech</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10149">http://arxiv.org/abs/2311.10149</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/WangHelin1997/Aty-TTS">https://github.com/WangHelin1997/Aty-TTS</a></li>
<li>paper_authors: Helin Wang, Venkatesh Ravichandran, Milind Rao, Becky Lammers, Myra Sydnor, Nicholas Maragakis, Ankur A. Butala, Jayne Zhang, Lora Clawson, Victoria Chovaz, Laureano Moro-Velazquez</li>
<li>for: 提高 Speech 识别系统（SLU）对非典型发音的处理能力</li>
<li>methods: 使用 Text-to-Speech（TTS）synthesis-based数据增强技术，通过知识传递来模拟非典型 speaker 的语音特征</li>
<li>results: 实现了对非典型发音的高质量语音生成，并为 SLU 系统提供更公平的处理能力<details>
<summary>Abstract</summary>
Spoken language understanding (SLU) systems often exhibit suboptimal performance in processing atypical speech, typically caused by neurological conditions and motor impairments. Recent advancements in Text-to-Speech (TTS) synthesis-based augmentation for more fair SLU have struggled to accurately capture the unique vocal characteristics of atypical speakers, largely due to insufficient data. To address this issue, we present a novel data augmentation method for atypical speakers by finetuning a TTS model, called Aty-TTS. Aty-TTS models speaker and atypical characteristics via knowledge transferring from a voice conversion model. Then, we use the augmented data to train SLU models adapted to atypical speech. To train these data augmentation models and evaluate the resulting SLU systems, we have collected a new atypical speech dataset containing intent annotation. Both objective and subjective assessments validate that Aty-TTS is capable of generating high-quality atypical speech. Furthermore, it serves as an effective data augmentation strategy, contributing to more fair SLU systems that can better accommodate individuals with atypical speech patterns.
</details>
<details>
<summary>摘要</summary>
听说理解（Spoken Language Understanding，SLU）系统经常在处理非典型语音时表现出下标的性能，通常是由神经系统和motor功能障碍所致。最近，基于文本到语音（Text-to-Speech，TTS）合成的数据增强技术在更公正的SLU中获得了进展，但是它们在捕捉非典型说话者的特有声音特征方面存在准确性问题，主要是因为数据不足。为解决这个问题，我们提出了一种新的数据增强方法，即Aty-TTS。Aty-TTS模型通过知识传递自voice conversion模型来学习说话者和非典型特征。然后，我们使用这些增强数据来训练适应非典型语音的SLU系统。为了训练这些数据增强模型和评估所得的SLU系统，我们收集了一个新的非典型语音数据集，其中包含意向注解。对象和主观评估表明，Aty-TTS可以生成高质量的非典型语音，并且作为数据增强策略，它有效地改善了SLU系统的公正性，使其更好地适应非典型语音模式。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/eess.AS_2023_11_16/" data-id="clpztdnqq016ces883mna27zt" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CV_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.CV_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T13:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CV/">cs.CV</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.CV_2023_11_16/">cs.CV - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="CV-Attention-UNet-Attention-based-UNet-for-3D-Cerebrovascular-Segmentation-of-Enhanced-TOF-MRA-Images"><a href="#CV-Attention-UNet-Attention-based-UNet-for-3D-Cerebrovascular-Segmentation-of-Enhanced-TOF-MRA-Images" class="headerlink" title="CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images"></a>CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10224">http://arxiv.org/abs/2311.10224</a></li>
<li>repo_url: None</li>
<li>paper_authors: Syed Farhan Abbas, Nguyen Thanh Duc, Yoonguu Song, Kyungwon Kim, Boreom Lee<br>for: 这个研究的目的是精确地分类脑血管图像，以帮助诊断脑血管疾病。methods: 这个研究使用了3D脑血管注意力UNet方法，named CV-AttentionUNet，来精确地提取脑血管图像。这个方法包括了一系列的预处理技术和深度超级vised UNet，以提高脑血管分类的精度。此外，这个方法还使用了注意力机制，以专注于相关的相互关联，并忽略无关的生物学信息。results: 我们的研究表明，CV-AttentionUNet方法可以对于脑血管分类task中的脑血管图像进行高精度的分类，并且在TubeTK dataset上表现比现有的state-of-the-art方法更好。<details>
<summary>Abstract</summary>
Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, we applied the attention mechanism. This mechanism focuses on relevant associations and neglects irrelevant anatomical information. Furthermore, the inclusion of deep supervision incorporates different levels of features that prove to be beneficial for network convergence. Results: We demonstrate the efficiency of the proposed method by cross-validating with an unlabeled dataset, which was further labeled by us. We believe that the novelty of this algorithm lies in its ability to perform well on both labeled and unlabeled data with image processing-based enhancement. The results indicate that our method performed better than the existing state-of-the-art methods on the TubeTK dataset. Conclusion: The proposed method will help in accurate segmentation of cerebrovascular structure leading to stroke
</details>
<details>
<summary>摘要</summary>
due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. the commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. methods: in this paper, we attempt to solve these limitations and propose the 3d cerebrovascular attention UNet method, named cv-attentionunet, for precise extraction of brain vessel images. we proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. to combine the low and high semantics, we applied the attention mechanism. this mechanism focuses on relevant associations and neglects irrelevant anatomical information. furthermore, the inclusion of deep supervision incorporates different levels of features that prove to be beneficial for network convergence. results: we demonstrate the efficiency of the proposed method by cross-validating with an unlabeled dataset, which was further labeled by us. we believe that the novelty of this algorithm lies in its ability to perform well on both labeled and unlabeled data with image processing-based enhancement. the results indicate that our method performed better than the existing state-of-the-art methods on the tubetk dataset. conclusion: the proposed method will help in accurate segmentation of cerebrovascular structure leading to stroke.
</details></li>
</ul>
<hr>
<h2 id="Stella-Nera-Achieving-161-TOp-s-W-with-Multiplier-free-DNN-Acceleration-based-on-Approximate-Matrix-Multiplication"><a href="#Stella-Nera-Achieving-161-TOp-s-W-with-Multiplier-free-DNN-Acceleration-based-on-Approximate-Matrix-Multiplication" class="headerlink" title="Stella Nera: Achieving 161 TOp&#x2F;s&#x2F;W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication"></a>Stella Nera: Achieving 161 TOp&#x2F;s&#x2F;W with Multiplier-free DNN Acceleration based on Approximate Matrix Multiplication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10207">http://arxiv.org/abs/2311.10207</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jannis Schönleber, Lukas Cavigelli, Renzo Andri, Matteo Perotti, Luca Benini</li>
<li>for: 本文针对精度高、能耗低的MatMul计算问题提出了一种解决方案，以替代传统的MatMul加速器。</li>
<li>methods: 本文使用了一种叫做“Maddness”的方法，通过使用哈希函数和缓存表（LUT）来实现MatMul计算，而不需要直接进行乘法运算。</li>
<li>results: 对于14nm和3nm技术的缩放，本文实现了一个高达161 TOp&#x2F;s&#x2F;<a href="mailto:&#87;&#64;&#x30;&#x2e;&#53;&#x35;&#x56;">&#87;&#64;&#x30;&#x2e;&#53;&#x35;&#x56;</a>的能耗效率，并达到了CIFAR-10的Top-1准确率高于92.5% using ResNet9。<details>
<summary>Abstract</summary>
From classical HPC to deep learning, MatMul is at the heart of today's computing. The recent Maddness method approximates MatMul without the need for multiplication by using a hash-based version of product quantization (PQ) indexing into a look-up table (LUT). Stella Nera is the first Maddness accelerator and it achieves 15x higher area efficiency (GMAC/s/mm^2) and more than 25x higher energy efficiency (TMAC/s/W) than direct MatMul accelerators implemented in the same technology. The hash function is a decision tree, which allows for an efficient hardware implementation as the multiply-accumulate operations are replaced by decision tree passes and LUT lookups. The entire Maddness MatMul can be broken down into parts that allow an effective implementation with small computing units and memories, allowing it to reach extreme efficiency while remaining generically applicable for MatMul tasks. In a commercial 14nm technology and scaled to 3nm, we achieve an energy efficiency of 161 TOp/s/W@0.55V with a Top-1 accuracy on CIFAR-10 of more than 92.5% using ResNet9.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="K-space-Cold-Diffusion-Learning-to-Reconstruct-Accelerated-MRI-without-Noise"><a href="#K-space-Cold-Diffusion-Learning-to-Reconstruct-Accelerated-MRI-without-Noise" class="headerlink" title="K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise"></a>K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10162">http://arxiv.org/abs/2311.10162</a></li>
<li>repo_url: None</li>
<li>paper_authors: Guoyao Shen, Mengyu Li, Chad W. Farris, Stephan Anderson, Xin Zhang</li>
<li>for: 这 paper 是为了提出一种基于冰晶扩展的 MRI 重建模型，用于快速 MRI 图像重建。</li>
<li>methods: 该模型使用冰晶扩展来实现一般化的图像变换，包括模糊、下采样等操作。</li>
<li>results: 根据对一个大量开源 MRI 数据集的测试，该模型可以生成高质量的重建图像，并且比其他深度学习基于 MRI 重建模型更好。<details>
<summary>Abstract</summary>
Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
</details>
<details>
<summary>摘要</summary>
现在的深度学习基于MRI重建模型已经取得了出色的表现。最近，扩散模型在图像生成、填充、超分解、图像修改等领域都有出色的表现。作为一种通用扩散模型，冷扩散进一步拓宽了范围，考虑了基于任意图像变换的模型，如模糊、下采样等。在这篇论文中，我们提出了基于k空间冷扩散模型的图像劣化和重建方法，无需Gaussian噪声。我们对多种深度学习基于MRI重建模型进行了比较，并在一个著名的大型开源MRI数据集上进行了测试。我们的结果表明，这种新的劣化方法可以生成高质量的重建图像 для加速MRI。Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="The-Chosen-One-Consistent-Characters-in-Text-to-Image-Diffusion-Models"><a href="#The-Chosen-One-Consistent-Characters-in-Text-to-Image-Diffusion-Models" class="headerlink" title="The Chosen One: Consistent Characters in Text-to-Image Diffusion Models"></a>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10093">http://arxiv.org/abs/2311.10093</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/johndpope/TheChosenOne">https://github.com/johndpope/TheChosenOne</a></li>
<li>paper_authors: Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski</li>
<li>for: 文章旨在提供一种完全自动化的一致性人物生成方法，用于文学创作、游戏开发资产设计、广告等实际应用。</li>
<li>methods: 我们提出的方法基于迭代过程，在每一个阶段内，从给定的文本提示中找出一个具有相似identify的图像集，并从这个集合中提取一个更加一致的identify。</li>
<li>results: 我们的方法在量化分析中表现出更好的平衡点，与基准方法相比，并且在用户研究中得到了证实。我们还展示了该方法在各种实际应用中的可行性。<details>
<summary>Abstract</summary>
Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach. Project page is available at https://omriavrahami.com/the-chosen-one
</details>
<details>
<summary>摘要</summary>
近期文本到图像生成模型的进步，推开了大量的视觉创造力。然而，这些模型往往受到一致性的限制，这是许多实际应用中的关键问题，如故事化、游戏开发资产设计、广告等。现有方法通常依赖于多个预存图像或尝试繁琐的手动过程。在这个工作中，我们提出了一种完全自动的一致性Character生成解决方案，唯一的输入是文本提示。我们引入了一种迭代过程，每个阶段都会从一组相似的图像中提取一个更一致的标识。我们的量化分析表明，我们的方法在提示对齐和一致性之间做出了更好的平衡，这些发现得到了用户研究的证实。为了结束，我们展示了一些实际应用场景。项目页面可以在https://omriavrahami.com/the-chosen-one找到。
</details></li>
</ul>
<hr>
<h2 id="Traffic-Video-Object-Detection-using-Motion-Prior"><a href="#Traffic-Video-Object-Detection-using-Motion-Prior" class="headerlink" title="Traffic Video Object Detection using Motion Prior"></a>Traffic Video Object Detection using Motion Prior</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10092">http://arxiv.org/abs/2311.10092</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lihao Liu, Yanqi Cheng, Dongdong Chen, Jing He, Pietro Liò, Carola-Bibiane Schönlieb, Angelica I Aviles-Rivero</li>
<li>for: 提高交通视频中物体检测精度</li>
<li>methods: 使用新的自注意模块和 Pseudo-label 机制，利用运动先驱来增强时间信息Integration和纠正噪声 pseudo label</li>
<li>results: 与现有状态前的方法比，表现出2%的提高，达到更高的检测精度<details>
<summary>Abstract</summary>
Traffic videos inherently differ from generic videos in their stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval. Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios. However, they fail to harness the strength of motion prior to enhance detection accuracy. In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection. Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting. Secondly, we utilise the motion prior to develop a pseudo-labelling mechanism to eliminate noisy pseudo labels for the semi-supervised setting. Both of our motion-prior-centred methods consistently demonstrates superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP.
</details>
<details>
<summary>摘要</summary>
traffic videos 自然 diferen  FROM generic videos 的 stationary camera setup, thus providing a strong motion prior where objects often move in a specific direction over a short time interval. Existing works predominantly employ generic video object detection framework for traffic video object detection, which yield certain advantages such as broad applicability and robustness to diverse scenarios. However, they fail to harness the strength of motion prior to enhance detection accuracy. In this work, we propose two innovative methods to exploit the motion prior and boost the performance of both fully-supervised and semi-supervised traffic video object detection. Firstly, we introduce a new self-attention module that leverages the motion prior to guide temporal information integration in the fully-supervised setting. Secondly, we utilize the motion prior to develop a pseudo-labeling mechanism to eliminate noisy pseudo labels for the semi-supervised setting. Both of our motion-prior-centred methods consistently demonstrate superior performance, outperforming existing state-of-the-art approaches by a margin of 2% in terms of mAP.
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Shells-for-Efficient-Neural-Radiance-Field-Rendering"><a href="#Adaptive-Shells-for-Efficient-Neural-Radiance-Field-Rendering" class="headerlink" title="Adaptive Shells for Efficient Neural Radiance Field Rendering"></a>Adaptive Shells for Efficient Neural Radiance Field Rendering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10091">http://arxiv.org/abs/2311.10091</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zian Wang, Tianchang Shen, Merlin Nimier-David, Nicholas Sharp, Jun Gao, Alexander Keller, Sanja Fidler, Thomas Müller, Zan Gojcic</li>
<li>for: 这个论文的目的是提高神经辐射场的渲染速度和视觉质量，通过在各个场景中适当地使用积分和表面基于的渲染方法。</li>
<li>methods: 这个论文使用神经网络来学习积分和表面基于的渲染方法，并通过自适应的积分大小和权重来控制在各个场景中的渲染精度。</li>
<li>results: 实验结果表明，这个方法可以大幅提高渲染速度和视觉质量，并且可以在不同的场景中适应性地应用。此外，这个方法还可以提取出精度高的表面 mesh，用于下游应用such as 动画和仿真。<details>
<summary>Abstract</summary>
Neural radiance fields achieve unprecedented quality for novel view synthesis, but their volumetric formulation remains expensive, requiring a huge number of samples to render high-resolution images. Volumetric encodings are essential to represent fuzzy geometry such as foliage and hair, and they are well-suited for stochastic optimization. Yet, many scenes ultimately consist largely of solid surfaces which can be accurately rendered by a single sample per pixel. Based on this insight, we propose a neural radiance formulation that smoothly transitions between volumetric- and surface-based rendering, greatly accelerating rendering speed and even improving visual fidelity. Our method constructs an explicit mesh envelope which spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To this end, we generalize the NeuS formulation with a learned spatially-varying kernel size which encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with width determined by the kernel size, and fine-tune the radiance field within this band. At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Experiments show that our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.
</details>
<details>
<summary>摘要</summary>
Our method constructs an explicit mesh envelope that spatially bounds a neural volumetric representation. In solid regions, the envelope nearly converges to a surface and can often be rendered with a single sample. To achieve this, we generalize the NeuS formulation with a learned spatially-varying kernel size that encodes the spread of the density, fitting a wide kernel to volume-like regions and a tight kernel to surface-like regions. We then extract an explicit mesh of a narrow band around the surface, with the width determined by the kernel size, and fine-tune the radiance field within this band.At inference time, we cast rays against the mesh and evaluate the radiance field only within the enclosed region, greatly reducing the number of samples required. Our approach enables efficient rendering at very high fidelity. We also demonstrate that the extracted envelope enables downstream applications such as animation and simulation.
</details></li>
</ul>
<hr>
<h2 id="Visual-Environment-Assessment-for-Safe-Autonomous-Quadrotor-Landing"><a href="#Visual-Environment-Assessment-for-Safe-Autonomous-Quadrotor-Landing" class="headerlink" title="Visual Environment Assessment for Safe Autonomous Quadrotor Landing"></a>Visual Environment Assessment for Safe Autonomous Quadrotor Landing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10065">http://arxiv.org/abs/2311.10065</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mattia Secchiero, Nishanth Bobbili, Yang Zhou, Giuseppe Loianno</li>
<li>for: 本研究旨在提供一种能够自动检测和评估安全降落区域的方法，以确保无人机在系统故障、低电量或完成特定任务后安全着陆。</li>
<li>methods: 该方法利用神经网络提取环境特征，并与geometry Map结合，以获得环境特征和重要的几何特征，如 Slope、平坦程度和护岸度。然后，根据这些特征，定义多个成本指标来评估环境的安全性、稳定性和适用性，并将最适合的降落区域标识出来。</li>
<li>results: 实验结果表明，该方法可以有效地评估环境中的降落区域，并帮助无人机安全着陆。<details>
<summary>Abstract</summary>
Autonomous identification and evaluation of safe landing zones are of paramount importance for ensuring the safety and effectiveness of aerial robots in the event of system failures, low battery, or the successful completion of specific tasks. In this paper, we present a novel approach for detection and assessment of potential landing sites for safe quadrotor landing. Our solution efficiently integrates 2D and 3D environmental information, eliminating the need for external aids such as GPS and computationally intensive elevation maps. The proposed pipeline combines semantic data derived from a Neural Network (NN), to extract environmental features, with geometric data obtained from a disparity map, to extract critical geometric attributes such as slope, flatness, and roughness. We define several cost metrics based on these attributes to evaluate safety, stability, and suitability of regions in the environments and identify the most suitable landing area. Our approach runs in real-time on quadrotors equipped with limited computational capabilities. Experimental results conducted in diverse environments demonstrate that the proposed method can effectively assess and identify suitable landing areas, enabling the safe and autonomous landing of a quadrotor.
</details>
<details>
<summary>摘要</summary>
自动识别和评估安全降落区的重要性对于保证飞行器的安全性和效率具有极高的重要性，尤其在系统故障、电池低下或完成特定任务后。在这篇论文中，我们提出了一种新的降落点检测和评估方法。我们的解决方案可以快速地将2D和3D环境信息集成，从而消除需要外部帮助（如GPS）和计算昂贵的高程地图。我们的管道使用神经网络（NN）来提取环境特征，并使用不同程度图来提取环境中关键的几何特征，如坡度、平坦度和荒凉程度。我们根据这些特征定义了多个成本指标，以评估环境中区域的安全性、稳定性和适用性，并将最适合的降落区标识出来。我们的方法在飞行器上搭载有有限的计算能力下运行，并在多种环境中进行了实验，证明了我们的方法可以有效地评估和标识适合降落的区域，使飞行器安全地自动降落。
</details></li>
</ul>
<hr>
<h2 id="Analyzing-Deviations-of-Dyadic-Lines-in-Fast-Hough-Transform"><a href="#Analyzing-Deviations-of-Dyadic-Lines-in-Fast-Hough-Transform" class="headerlink" title="Analyzing Deviations of Dyadic Lines in Fast Hough Transform"></a>Analyzing Deviations of Dyadic Lines in Fast Hough Transform</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10064">http://arxiv.org/abs/2311.10064</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gleb Smirnov, Simon Karpenko</li>
<li>for: 本文研究了dyadic线模型在图像识别中的准确性。</li>
<li>methods: 本文使用了统计分析方法来研究dyadic线模型的偏差。</li>
<li>results: 研究发现，dyadic线模型的偏差的mean值为零，而方差为O(log(n))。随着n的增加，这些偏差的分布会转化为一个正态分布，其中mean值为零，并且具有小的方差。这个限定结果借鉴了 Erdős theory。<details>
<summary>Abstract</summary>
Fast Hough transform is a widely used algorithm in pattern recognition. The algorithm relies on approximating lines using a specific discrete line model called dyadic lines. The worst-case deviation of a dyadic line from the ideal line it used to construct grows as $O(log(n))$, where $n$ is the linear size of the image. But few lines actually reach the worst-case bound. The present paper addresses a statistical analysis of the deviation of a dyadic line from its ideal counterpart. Specifically, our findings show that the mean deviation is zero, and the variance grows as $O(log(n))$. As $n$ increases, the distribution of these (suitably normalized) deviations converges towards a normal distribution with zero mean and a small variance. This limiting result makes an essential use of ergodic theory.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Depth-Insight-–-Contribution-of-Different-Features-to-Indoor-Single-image-Depth-Estimation"><a href="#Depth-Insight-–-Contribution-of-Different-Features-to-Indoor-Single-image-Depth-Estimation" class="headerlink" title="Depth Insight – Contribution of Different Features to Indoor Single-image Depth Estimation"></a>Depth Insight – Contribution of Different Features to Indoor Single-image Depth Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10042">http://arxiv.org/abs/2311.10042</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yihong Wu, Yuwen Heng, Mahesan Niranjan, Hansung Kim</li>
<li>for: 这个论文主要研究了单一图像中的深度估算问题，以寻求更好地理解深度估算模型如何利用图像中的各种特征来预测深度。</li>
<li>methods: 本论文使用了特征提取技术来关联单个特征（形状、 текстура、颜色和饱和度）与深度的关系。</li>
<li>results: 研究发现，在indoor场景中，形状提取得到的结果具有更大的贡献，而其他特征也具有不同程度的贡献。这些发现可以帮助优化深度估算模型，提高其准确性和Robustness。<details>
<summary>Abstract</summary>
Depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent. Whereas impressive performances have been reported in this area recently using end-to-end trained deep neural architectures, as to what cues in the images that are being exploited by these black box systems is hard to know. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, colour and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimise depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.
</details>
<details>
<summary>摘要</summary>
depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent.  Recently, impressive performances have been reported in this area using end-to-end trained deep neural architectures, but it is hard to know what cues in the images are being exploited by these black box systems. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, color, and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimize depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.Here's the translation in Traditional Chinese:depth estimation from a single image is a challenging problem in computer vision because binocular disparity or motion information is absent.  Recently, impressive performances have been reported in this area using end-to-end trained deep neural architectures, but it is hard to know what cues in the images are being exploited by these black box systems. To this end, in this work, we quantify the relative contributions of the known cues of depth in a monocular depth estimation setting using an indoor scene data set. Our work uses feature extraction techniques to relate the single features of shape, texture, color, and saturation, taken in isolation, to predict depth. We find that the shape of objects extracted by edge detection substantially contributes more than others in the indoor setting considered, while the other features also have contributions in varying degrees. These insights will help optimize depth estimation models, boosting their accuracy and robustness. They promise to broaden the practical applications of vision-based depth estimation. The project code is attached to the supplementary material and will be published on GitHub.
</details></li>
</ul>
<hr>
<h2 id="Match-and-Locate-low-frequency-monocular-odometry-based-on-deep-feature-matching"><a href="#Match-and-Locate-low-frequency-monocular-odometry-based-on-deep-feature-matching" class="headerlink" title="Match and Locate: low-frequency monocular odometry based on deep feature matching"></a>Match and Locate: low-frequency monocular odometry based on deep feature matching</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10034">http://arxiv.org/abs/2311.10034</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stepan Konev, Yuriy Biktairov</li>
<li>for: 这篇论文的目的是提出一种基于单个摄像头的Robotic odometry方法，以提高系统的可行性和简洁性。</li>
<li>methods: 这篇论文使用了深度特征匹配模型来匹配预先 capture的图像特征，然后使用卷积神经网进行精确的姿势和位置估计。</li>
<li>results: 这篇论文在AISG-SLA Visual Localisation Challenge中评估了这种方法的表现，发现其可以实现精确的姿势和位置估计， orientation estimation error约3度， translation estimation error约2米，与其他参赛者相比，这种方法在computational efficiency和易于实现的前提下表现竞争力强。<details>
<summary>Abstract</summary>
Accurate and robust pose estimation plays a crucial role in many robotic systems. Popular algorithms for pose estimation typically rely on high-fidelity and high-frequency signals from various sensors. Inclusion of these sensors makes the system less affordable and much more complicated. In this work we introduce a novel approach for the robotic odometry which only requires a single camera and, importantly, can produce reliable estimates given even extremely low-frequency signal of around one frame per second. The approach is based on matching image features between the consecutive frames of the video stream using deep feature matching models. The resulting coarse estimate is then adjusted by a convolutional neural network, which is also responsible for estimating the scale of the transition, otherwise irretrievable using only the feature matching information. We evaluate the performance of the approach in the AISG-SLA Visual Localisation Challenge and find that while being computationally efficient and easy to implement our method shows competitive results with only around $3^{\circ}$ of orientation estimation error and $2m$ of translation estimation error taking the third place in the challenge.
</details>
<details>
<summary>摘要</summary>
准确和可靠的姿态估计在许多机器人系统中扮演着关键性的角色。常见的姿态估计算法通常基于高精度和高频信号，这些信号来自多种感知器。然而，这些感知器的包含使得系统变得更加昂贵和复杂。在这个工作中，我们介绍了一种新的机器人征卷方法，只需一个摄像头即可实现。这种方法基于图像特征匹配模型，通过比较连续帧视频流中的图像特征，生成初步估计。然后，使用卷积神经网络调整初步估计，同时估计转换的比例。我们在AISG-SLA视 lokalisierung Challenge中评估了这种方法的性能，发现它具有计算效率和易于实现的优点，同时与其他参赛者相比，其姿态估计误差为约3度和平均误差为2米，在挑战中排名第三。
</details></li>
</ul>
<hr>
<h2 id="On-the-Overconfidence-Problem-in-Semantic-3D-Mapping"><a href="#On-the-Overconfidence-Problem-in-Semantic-3D-Mapping" class="headerlink" title="On the Overconfidence Problem in Semantic 3D Mapping"></a>On the Overconfidence Problem in Semantic 3D Mapping</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10018">http://arxiv.org/abs/2311.10018</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joao Marcos Correia Marques, Albert Zhai, Shenlong Wang, Kris Hauser</li>
<li>for: 这篇论文旨在解决Semantic 3D mapping中的混淆风险问题，即多视图结合深度和图像分割信息时，传统的映射方法会假设整个地图是正确的，从而导致输出抖抖。</li>
<li>methods: 该论文提出了多种在整合缓存阶段使用不同方法来改善混淆度规则的方法，并对ScanNet数据集进行了比较。其中，最常用的 bayesian 混合策略被证明是最差异步calibrated。作者们还提出了一种学习管道，GLFS，可以同时实现高精度和3D地图准确性，并保留实时能力。</li>
<li>results: 作者们示出，在一个模块化ObjectNav Agent上，通过正确地将Semantic Fusion纳入混合过程中，可以提高其成功率。此外，作者们还证明了地图准确性对下游任务的重要性。<details>
<summary>Abstract</summary>
Semantic 3D mapping, the process of fusing depth and image segmentation information between multiple views to build 3D maps annotated with object classes in real-time, is a recent topic of interest. This paper highlights the fusion overconfidence problem, in which conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.
</details>
<details>
<summary>摘要</summary>
Semantic 3D mapping, the process of combining depth and image segmentation information from multiple views to create 3D maps annotated with object classes in real-time, is a current area of interest. This paper highlights the fusion overconfidence problem, where conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.Here's the translation in Traditional Chinese:Semantic 3D mapping, the process of combining depth and image segmentation information from multiple views to create 3D maps annotated with object classes in real-time, is a current area of interest. This paper highlights the fusion overconfidence problem, where conventional mapping methods assign high confidence to the entire map even when they are incorrect, leading to miscalibrated outputs. Several methods to improve uncertainty calibration at different stages in the fusion pipeline are presented and compared on the ScanNet dataset. We show that the most widely used Bayesian fusion strategy is among the worst calibrated, and propose a learned pipeline that combines fusion and calibration, GLFS, which achieves simultaneously higher accuracy and 3D map calibration while retaining real-time capability. We further illustrate the importance of map calibration on a downstream task by showing that incorporating proper semantic fusion on a modular ObjectNav agent improves its success rates. Our code will be provided on Github for reproducibility upon acceptance.
</details></li>
</ul>
<hr>
<h2 id="SQLNet-Scale-Modulated-Query-and-Localization-Network-for-Few-Shot-Class-Agnostic-Counting"><a href="#SQLNet-Scale-Modulated-Query-and-Localization-Network-for-Few-Shot-Class-Agnostic-Counting" class="headerlink" title="SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting"></a>SQLNet: Scale-Modulated Query and Localization Network for Few-Shot Class-Agnostic Counting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10011">http://arxiv.org/abs/2311.10011</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/hcplab-sysu/sqlnet">https://github.com/hcplab-sysu/sqlnet</a></li>
<li>paper_authors: Hefeng Wu, Yandong Chen, Lingbo Liu, Tianshui Chen, Keze Wang, Liang Lin</li>
<li>for: 解决 counting all objects of an arbitrary class 问题，提高下游任务的性能。</li>
<li>methods: 提出了一种基于 localization 的 novel 方法，即 Scale-modulated Query and Localization Network (SQLNet)，它可以充分利用 exemplars 的 scale 信息，进行有效的 counting。</li>
<li>results: 在 popular CAC benchmarks 上，SQLNet 的表现优于当前领先方法，不仅在 counting 精度方面表现出色，还在 localization 和 bounding box 生成方面达到了优秀的result。<details>
<summary>Abstract</summary>
The class-agnostic counting (CAC) task has recently been proposed to solve the problem of counting all objects of an arbitrary class with several exemplars given in the input image. To address this challenging task, existing leading methods all resort to density map regression, which renders them impractical for downstream tasks that require object locations and restricts their ability to well explore the scale information of exemplars for supervision. To address the limitations, we propose a novel localization-based CAC approach, termed Scale-modulated Query and Localization Network (SQLNet). It fully explores the scales of exemplars in both the query and localization stages and achieves effective counting by accurately locating each object and predicting its approximate size. Specifically, during the query stage, rich discriminative representations of the target class are acquired by the Hierarchical Exemplars Collaborative Enhancement (HECE) module from the few exemplars through multi-scale exemplar cooperation with equifrequent size prompt embedding. These representations are then fed into the Exemplars-Unified Query Correlation (EUQC) module to interact with the query features in a unified manner and produce the correlated query tensor. In the localization stage, the Scale-aware Multi-head Localization (SAML) module utilizes the query tensor to predict the confidence, location, and size of each potential object. Moreover, a scale-aware localization loss is introduced, which exploits flexible location associations and exemplar scales for supervision to optimize the model performance. Extensive experiments demonstrate that SQLNet outperforms state-of-the-art methods on popular CAC benchmarks, achieving excellent performance not only in counting accuracy but also in localization and bounding box generation. Our codes will be available at https://github.com/HCPLab-SYSU/SQLNet
</details>
<details>
<summary>摘要</summary>
“类型不敏感 counting（CAC）任务最近被提出来解决输入图像中所有类型的对象数量的问题。为了解决这个复杂的任务，现有领先的方法都是通过密度地图回归来实现，这会导致其在下游任务中的缺乏能力和对 exemplars 的缺乏监督。为了解决这些限制，我们提出了一种基于localization的新方法，称为Scale-modulated Query and Localization Network（SQLNet）。它能够全面探索 exemplars 的尺度，并在查询和本地化两个阶段中准确地定位和估计每个对象的大小。在查询阶段，通过多scale exemplar合作和equifrequent size prompt embedding，HECE模块从少量 exemplars 中获得了丰富的描述符，然后将其传递给EUQC模块进行与查询特征的统一交互，生成相关的查询张量。在本地化阶段，SAML模块使用查询张量预测对象的信心、位置和大小。此外，我们还引入了具有灵活位置关系和 exemplars 尺度的scale-aware本地化损失，以便在模型性能优化。我们的实验结果表明，SQLNet 在流行的 CAC 测试准则上表现出色，不仅在计数准确性方面取得了出色的成绩，还在本地化和 bounding box 生成方面取得了优秀的成绩。我们的代码将在https://github.com/HCPLab-SYSU/SQLNet 上提供。”
</details></li>
</ul>
<hr>
<h2 id="TransFusion-–-A-Transparency-Based-Diffusion-Model-for-Anomaly-Detection"><a href="#TransFusion-–-A-Transparency-Based-Diffusion-Model-for-Anomaly-Detection" class="headerlink" title="TransFusion – A Transparency-Based Diffusion Model for Anomaly Detection"></a>TransFusion – A Transparency-Based Diffusion Model for Anomaly Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09999">http://arxiv.org/abs/2311.09999</a></li>
<li>repo_url: None</li>
<li>paper_authors: Matic Fučka, Vitjan Zavrtanik, Danijel Skočaj</li>
<li>for: 这个研究是为了提高表面异常检测的精度和效率，特别是在工业检测中。</li>
<li>methods: 这个研究使用了一种新的透明度基于的扩散过程，通过逐步增加异常区域的透明度，以精确地重建正常的 appearances。</li>
<li>results: 这个研究在两个常用的检测 dataset上（VisA和MVTec AD）得到了 state-of-the-art 的表现，具有image-level AUROC 的98.5%和99.2%。<details>
<summary>Abstract</summary>
Surface anomaly detection is a vital component in manufacturing inspection. Reconstructive anomaly detection methods restore the normal appearance of an object, ideally modifying only the anomalous regions. Due to the limitations of commonly used reconstruction architectures, the produced reconstructions are often poor and either still contain anomalies or lack details in anomaly-free regions. Recent reconstructive methods adopt diffusion models, however with the standard diffusion process the problems are not adequately addressed. We propose a novel transparency-based diffusion process, where the transparency of anomalous regions is progressively increased, restoring their normal appearance accurately and maintaining the appearance of anomaly-free regions without loss of detail. We propose TRANSparency DifFUSION (TransFusion), a discriminative anomaly detection method that implements the proposed diffusion process, enabling accurate downstream anomaly detection. TransFusion achieves state-of-the-art performance on both the VisA and the MVTec AD datasets, with an image-level AUROC of 98.5% and 99.2%, respectively.
</details>
<details>
<summary>摘要</summary>
表面异常检测是制造检查中的重要组成部分。重建性异常检测方法可以修复物品的正常外观，理想情况下仅 modify anomalous regions。由于通用的重建架构受限，生成的重建结果经常仍然含有异常或lack of detail in anomaly-free regions。最近的重建方法采用扩散模型，但标准的扩散过程并不能够妥善解决问题。我们提出了一种新的透明度基于扩散过程，其中异常区域的透明度逐渐增加，将其正确地修复为正常的外观，并维持异常区域以外的外观不受损害。我们提出了名为TransFusion的检测方法，它实现了提议的扩散过程，允许精确的下游异常检测。TransFusion在VisA和MVTec AD datasets上 achieve state-of-the-art performance，具体来说是图像水平的AUROC为98.5%和99.2%。
</details></li>
</ul>
<hr>
<h2 id="DeepEMD-A-Transformer-based-Fast-Estimation-of-the-Earth-Mover’s-Distance"><a href="#DeepEMD-A-Transformer-based-Fast-Estimation-of-the-Earth-Mover’s-Distance" class="headerlink" title="DeepEMD: A Transformer-based Fast Estimation of the Earth Mover’s Distance"></a>DeepEMD: A Transformer-based Fast Estimation of the Earth Mover’s Distance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09998">http://arxiv.org/abs/2311.09998</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/atulkumarin/deepemd">https://github.com/atulkumarin/deepemd</a></li>
<li>paper_authors: Atul Kumar Sinha, Francois Fleuret</li>
<li>for: 这个论文的目的是提出一种基于注意力的模型，用于精确地计算点云集的摩尔变换距离（Earth Mover’s Distance，EMD），以便作为生成模型的训练损失函数。</li>
<li>methods: 这种模型使用注意力机制来计算点云集之间的匹配，并通过Explicitly Computing Matching来获得精确的摩尔变换距离和其梯度的估计。</li>
<li>results: 实验表明，这种模型可以准确地计算摩尔变换距离和其梯度，并且在训练过程中具有高速度响应和广泛的应用前提。此外，模型在无法训练集上的运行表现也非常出色。<details>
<summary>Abstract</summary>
The Earth Mover's Distance (EMD) is the measure of choice between point clouds. However the computational cost to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance. We propose an attention-based model to compute an accurate approximation of the EMD that can be used as a training loss for generative models. To get the necessary accurate estimation of the gradients we train our model to explicitly compute the matching between point clouds instead of EMD itself. We cast this new objective as the estimation of an attention matrix that approximates the ground truth matching matrix. Experiments show that this model provides an accurate estimate of the EMD and its gradient with a wall clock speed-up of more than two orders of magnitude with respect to the exact Hungarian matching algorithm and one order of magnitude with respect to the standard approximate Sinkhorn algorithm, allowing in particular to train a point cloud VAE with the EMD itself. Extensive evaluation show the remarkable behaviour of this model when operating out-of-distribution, a key requirement for a distance surrogate. Finally, the model generalizes very well to point clouds during inference several times larger than during training.
</details>
<details>
<summary>摘要</summary>
地球移动者距离（EMD）是点云之间的度量标准，但计算成本使其成为训练损失的禁制品。标准方法是使用 Chamfer 距离作为代理。我们提议使用注意力基于模型来计算精确的EMD aproximation，以便用于训练生成模型。为了获得必要的精确Gradient，我们在模型训练中显式计算点云之间的匹配。我们将这个新的目标设定为估算真实的匹配矩阵。实验显示，这个模型可以准确地计算EMD和其Gradient，并且与准确的挪威抽象搜索算法和标准搜索算法相比，具有大量的时间速度提升（至少两个排名）和一个排名的速度提升。这使得我们可以使用EMD本身作为训练损失。我们的模型在误差外的操作中表现出色，这是距离代理的关键要求。此外，我们的模型在推理时可以处理大量的点云，并且可以在训练时使用相同的模型。
</details></li>
</ul>
<hr>
<h2 id="From-Pretext-to-Purpose-Batch-Adaptive-Self-Supervised-Learning"><a href="#From-Pretext-to-Purpose-Batch-Adaptive-Self-Supervised-Learning" class="headerlink" title="From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning"></a>From Pretext to Purpose: Batch-Adaptive Self-Supervised Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09974">http://arxiv.org/abs/2311.09974</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiansong Zhang, Peizhong Liu</li>
<li>for: 本文旨在提出一种适应batch size和预测任务的自然语言处理方法，以提高自然语言处理的自动学习能力。</li>
<li>methods: 本文使用了对 batch data 的维度减少和重建，以实现batch数据之间的内部通信，并通过嵌入层来适应性地增强自我超vised feature编码能力。</li>
<li>results: 根据ImageNet-1k的线性分类测试，我们的方法可以在比较公平的情况下达到状态 arts 性能，而且在ImageNet-100上，相比原始性能，top1的最大提升为1.25%。<details>
<summary>Abstract</summary>
In recent years, self-supervised contrastive learning has emerged as a distinguished paradigm in the artificial intelligence landscape. It facilitates unsupervised feature learning through contrastive delineations at the instance level. However, crafting an effective self-supervised paradigm remains a pivotal challenge within this field. This paper delves into two crucial factors impacting self-supervised contrastive learning-bach size and pretext tasks, and from a data processing standpoint, proposes an adaptive technique of batch fusion. The proposed method, via dimensionality reduction and reconstruction of batch data, enables formerly isolated individual data to partake in intra-batch communication through the Embedding Layer. Moreover, it adaptively amplifies the self-supervised feature encoding capability as the training progresses. We conducted a linear classification test of this method based on the classic contrastive learning framework on ImageNet-1k. The empirical findings illustrate that our approach achieves state-of-the-art performance under equitable comparisons. Benefiting from its "plug-and-play" characteristics, we further explored other contrastive learning methods. On the ImageNet-100, compared to the original performance, the top1 has seen a maximum increase of 1.25%. We suggest that the proposed method may contribute to the advancement of data-driven self-supervised learning research, bringing a fresh perspective to this community.
</details>
<details>
<summary>摘要</summary>
近年来，自我超viscontrastive learning已经出现为人工智能领域的一种distinguished paradigm。它可以通过对instance level进行对比，实现无监督特征学习。然而，制定有效的自我超viscontrastive learning paradigm仍然是这个领域中的一个关键挑战。这篇论文探讨了自我超viscontrastive learning中两个关键因素：batch size和pretext tasks，并从数据处理角度提出了一种适应技术——批处理融合。提议的方法通过维度减少和批处理数据的重建，使得原来隔离的个体数据能够在Embedding层内进行INTRA-batch交流。此外，该方法可以逐渐增强自我超viscontrastive feature编码能力，并在训练进程中进行适应调整。我们基于经典对比学习框架进行了Linear classification测试，实验结果表明，我们的方法在相等比较下实现了状态盘领先性。由于其“嵌入式”的特点，我们进一步探索了其他对比学习方法。在ImageNet-100上，相比原始性能，排名前100的最大提升为1.25%。我们建议该方法可能会促进数据驱动的自我超viscontrastive学习研究，为这个社区带来一种新的视角。
</details></li>
</ul>
<hr>
<h2 id="SurgPLAN-Surgical-Phase-Localization-Network-for-Phase-Recognition"><a href="#SurgPLAN-Surgical-Phase-Localization-Network-for-Phase-Recognition" class="headerlink" title="SurgPLAN: Surgical Phase Localization Network for Phase Recognition"></a>SurgPLAN: Surgical Phase Localization Network for Phase Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09965">http://arxiv.org/abs/2311.09965</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xingjian Luo, You Pang, Zhen Chen, Jinlin Wu, Zongmin Zhang, Zhen Lei, Hongbin Liu</li>
<li>for: 提高智能操作室中手术理解的精度，解决自动手术阶段识别存在两大问题，即不能捕捉每帧和运动信息的特征特征，以及每个阶段的预测不稳定性。</li>
<li>methods: 我们提出了一种名为手术阶段位置网络（SurgPLAN），它通过捕捉多尺度空间和时间特征，以及基于时间区域提案的phasenprediction来提高手术阶段识别的精度和稳定性。</li>
<li>results: 我们的SurgPLAN在比较existing方法时，在精度和稳定性两个方面具有显著的优势。<details>
<summary>Abstract</summary>
Surgical phase recognition is crucial to providing surgery understanding in smart operating rooms. Despite great progress in automatic surgical phase recognition, most existing methods are still restricted by two problems. First, these methods cannot capture discriminative visual features for each frame and motion information with simple 2D networks. Second, the frame-by-frame recognition paradigm degrades the performance due to unstable predictions within each phase, termed as phase shaking. To address these two challenges, we propose a Surgical Phase LocAlization Network, named SurgPLAN, to facilitate a more accurate and stable surgical phase recognition with the principle of temporal detection. Specifically, we first devise a Pyramid SlowFast (PSF) architecture to serve as the visual backbone to capture multi-scale spatial and temporal features by two branches with different frame sampling rates. Moreover, we propose a Temporal Phase Localization (TPL) module to generate the phase prediction based on temporal region proposals, which ensures accurate and consistent predictions within each surgical phase. Extensive experiments confirm the significant advantages of our SurgPLAN over frame-by-frame approaches in terms of both accuracy and stability.
</details>
<details>
<summary>摘要</summary>
针对智能操作室中的手术理解，外科阶段识别具有重要的意义。虽然自动外科阶段识别技术已经取得了大量的进步，但大多数现有方法都受到两个问题的限制。首先，这些方法无法捕捉每帧和运动信息的特征特征，这限制了它们的识别精度。其次，frame-by-frame认识模式会导致识别性下降，这被称为“阶段震荡”。为了解决这两个挑战，我们提议一种名为外科阶段封顶网络（SurgPLAN），它可以提供更加准确和稳定的外科阶段识别。具体来说，我们首先设计了一种Pyramid SlowFast（PSF）架构，它作为视觉后ION来捕捉多scalespatial和时间特征。此外，我们还提出了一种时间阶段本地化（TPL）模块，它可以基于时间区域提案来生成阶段预测，从而保证了识别的准确和一致。我们进行了广泛的实验，结果表明，我们的SurgPLAN在 Frame-by-frame方法的基础上具有显著优势，包括更高的准确率和稳定性。
</details></li>
</ul>
<hr>
<h2 id="VertDetect-Fully-End-to-End-3D-Vertebral-Instance-Segmentation-Model"><a href="#VertDetect-Fully-End-to-End-3D-Vertebral-Instance-Segmentation-Model" class="headerlink" title="VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model"></a>VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09958">http://arxiv.org/abs/2311.09958</a></li>
<li>repo_url: None</li>
<li>paper_authors: Geoff Klein, Michael Hardisty, Cari Whyne, Anne L. Martel</li>
<li>for: 这篇论文的目的是提出一个完全自动的三维脊梗检测和分类模型，以便在骨科手术和放射治疗中进行观察规划。</li>
<li>methods: 这个模型使用了一个共同的CNN背部，将检测和分类分支的feature map传递给它们。此外，还使用了一个图 theoretics 网络层，以改善脊梗标签。</li>
<li>results: 这个模型在 VerSe 2019 和 VerSe 2020 公共和隐藏测试集中均 achieved state-of-the-art 性能，其中 Dice Similarity Coefficient (DSC) 为 0.883 (95% CI, 0.843-0.906) 和 0.882 (95% CI, 0.835-0.909)，以及 0.868 (95% CI, 0.834-0.890) 和 0.869 (95% CI, 0.832-0.891)。<details>
<summary>Abstract</summary>
Vertebral detection and segmentation are critical steps for treatment planning in spine surgery and radiation therapy. Accurate identification and segmentation are complicated in imaging that does not include the full spine, in cases with variations in anatomy (T13 and/or L6 vertebrae), and in the presence of fracture or hardware. This paper proposes VertDetect, a fully automated end-to-end 3D vertebral instance segmentation Convolutional Neural Network (CNN) model to predict vertebral level labels and segmentations for all vertebrae present in a CT scan. The utilization of a shared CNN backbone provides the detection and segmentation branches of the network with feature maps containing both spinal and vertebral level information. A Graph Convolutional Network (GCN) layer is used to improve vertebral labelling by using the known structure of the spine. This model achieved a Dice Similarity Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI, 0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\% CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively. This model achieved state-of-the-art performance for an end-to-end architecture, whose design facilitates the extraction of features that can be subsequently used for downstream tasks.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate("VertDetect is a fully automated end-to-end 3D vertebral instance segmentation Convolutional Neural Network (CNN) model that predicts vertebral level labels and segmentations for all vertebrae present in a CT scan. The shared CNN backbone provides the detection and segmentation branches with feature maps containing both spinal and vertebral level information. A Graph Convolutional Network (GCN) layer is used to improve vertebral labeling using the known structure of the spine. This model achieved a Dice Similarity Coefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI, 0.835-0.909) in the VerSe 2019 and 0.868 (95\% CI, 0.834-0.890) and 0.869 (95\% CI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively. This model achieved state-of-the-art performance for an end-to-end architecture, whose design facilitates the extraction of features that can be subsequently used for downstream tasks.">> Here's the breakdown of the translation:* "VertDetect" is translated as "VertDetect" (全自动的三维vertebral实例分割Convolutional Neural Network模型)* "Convolutional Neural Network" is translated as "Convolutional Neural Network" (卷积神经网络)* "end-to-end" is translated as "端到端" (end-to-end)* "vertebral instance segmentation" is translated as "vertebral实例分割" (vertebral instance segmentation)* "CT scan" is translated as "CT扫描" (CT scan)* "spinal and vertebral level information" is translated as "脊梁和vertebral уров别信息" (spinal and vertebral level information)* "Graph Convolutional Network" is translated as "图connvolutional网络" (Graph Convolutional Network)* "vertebral labeling" is translated as "vertebral标注" (vertebral labeling)* "known structure of the spine" is translated as "脊梁的已知结构" (known structure of the spine)* "Dice Similarity Coefficient" is translated as " dice相似度系数" (Dice Similarity Coefficient)* "public and hidden test sets" is translated as "公共和隐藏测试集" (public and hidden test sets)* "state-of-the-art performance" is translated as "现状最佳性能" (state-of-the-art performance)Note that the translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Score-based-generative-models-learn-manifold-like-structures-with-constrained-mixing"><a href="#Score-based-generative-models-learn-manifold-like-structures-with-constrained-mixing" class="headerlink" title="Score-based generative models learn manifold-like structures with constrained mixing"></a>Score-based generative models learn manifold-like structures with constrained mixing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09952">http://arxiv.org/abs/2311.09952</a></li>
<li>repo_url: None</li>
<li>paper_authors: Li Kevin Wenliang, Ben Moran</li>
<li>for:  score-based generative models (SBMs) learn the data distribution supported on a low-dimensional manifold</li>
<li>methods: linear approximations and subspaces spanned by local feature vectors</li>
<li>results: the learned vector field mixes samples by a non-conservative field within the manifold, and the subspace spanned by the local features overlaps with an effective density function.Here is the text in Simplified Chinese:</li>
<li>for: SBMs 学习数据分布的低维折衔</li>
<li>methods: 利用线性近似和本地特征向量生成子空间</li>
<li>results: 学习得到的向量场在折衔中混合样本，并且在折衔中保持数据分布的折衔结构。<details>
<summary>Abstract</summary>
How do score-based generative models (SBMs) learn the data distribution supported on a low-dimensional manifold? We investigate the score model of a trained SBM through its linear approximations and subspaces spanned by local feature vectors. During diffusion as the noise decreases, the local dimensionality increases and becomes more varied between different sample sequences. Importantly, we find that the learned vector field mixes samples by a non-conservative field within the manifold, although it denoises with normal projections as if there is an energy function in off-manifold directions. At each noise level, the subspace spanned by the local features overlap with an effective density function. These observations suggest that SBMs can flexibly mix samples with the learned score field while carefully maintaining a manifold-like structure of the data distribution.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Harnessing-Transformers-A-Leap-Forward-in-Lung-Cancer-Image-Detection"><a href="#Harnessing-Transformers-A-Leap-Forward-in-Lung-Cancer-Image-Detection" class="headerlink" title="Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection"></a>Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09942">http://arxiv.org/abs/2311.09942</a></li>
<li>repo_url: None</li>
<li>paper_authors: Amine Bechar, Youssef Elmir, Rafik Medjoudj, Yassine Himeur, Abbes Amira</li>
<li>for: 本研究探讨了贯彻学习（TL）和变换器在肿瘤检测中的应用，尤其是基于图像分析。</li>
<li>methods: 本研究使用了多种方法，包括TL、变换器和卷积神经网络（CNN）模型，其中变换器在图像分析中表现最佳，具有97.41%的准确率 для肾癌检测和94.71%的准确率 для histopathological lung cancer。</li>
<li>results: 本研究结果显示，变Transformers在图像分析中表现最佳，其准确率为97.41% для肾癌检测和94.71% для histopathological lung cancer。<details>
<summary>Abstract</summary>
This paper discusses the role of Transfer Learning (TL) and transformers in cancer detection based on image analysis. With the enormous evolution of cancer patients, the identification of cancer cells in a patient's body has emerged as a trend in the field of Artificial Intelligence (AI). This process involves analyzing medical images, such as Computed Tomography (CT) scans and Magnetic Resonance Imaging (MRIs), to identify abnormal growths that may help in cancer detection. Many techniques and methods have been realized to improve the quality and performance of cancer classification and detection, such as TL, which allows the transfer of knowledge from one task to another with the same task or domain. TL englobes many methods, particularly those used in image analysis, such as transformers and Convolutional Neural Network (CNN) models trained on the ImageNet dataset. This paper analyzes and criticizes each method of TL based on image analysis and compares the results of each method, showing that transformers have achieved the best results with an accuracy of 97.41% for colon cancer detection and 94.71% for Histopathological Lung cancer. Future directions for cancer detection based on image analysis are also discussed.
</details>
<details>
<summary>摘要</summary>
TL encompasses a range of methods, including those used in image analysis, such as transformers and Convolutional Neural Network (CNN) models trained on the ImageNet dataset. This paper examines and critiques each TL method based on image analysis, comparing the results of each method. The paper finds that transformers have achieved the best results, with an accuracy of 97.41% for colon cancer detection and 94.71% for Histopathological Lung cancer. The paper also discusses future directions for cancer detection based on image analysis.Translated into Simplified Chinese:这篇论文探讨了转移学习（TL）和转换器在生物图像分析中的肿瘤检测。随着癌症患者的增加，在病理图像分析中确定患者体内癌细胞的存在已成为人工智能领域的趋势。这个过程涉及分析医疗图像，如计算Tomography（CT）扫描和磁共振成像（MRI），以确定癌细胞的存在。多种技术和方法已被实现以提高癌症分类和检测的质量和性能，包括TL，它允许知识从一个任务中传输到另一个任务或领域中。TL包括许多方法，其中包括在图像分析中使用的转换器和Convolutional Neural Network（CNN）模型在ImageNet数据集上被训练。这篇论文对每种TL方法进行了分析和评价，并比较了每种方法的结果。论文发现，转换器已经实现了最好的结果，具体来说是97.41%的肿瘤检测精度 для肠癌和94.71%的肺癌检测精度。论文还讨论了基于图像分析的未来癌症检测的方向。Translated into Traditional Chinese:这篇论文探讨了将学习（TL）和转换器在生物图像分析中的肿瘤检测。随着癌症患者的增加，在病理图像分析中确定患者体内癌细胞的存在已成为人工智能领域的趋势。这个过程涉及分析医疗图像，如计算Tomography（CT）扫描和磁共振成像（MRI），以确定癌细胞的存在。多种技术和方法已被实现以提高癌症分类和检测的质量和性能，包括TL，它允许知识从一个任务中传输到另一个任务或领域中。TL包括许多方法，其中包括在图像分析中使用的转换器和Convolutional Neural Network（CNN）模型在ImageNet数据集上被训练。这篇论文对每种TL方法进行了分析和评价，并比较了每种方法的结果。论文发现，转换器已经实现了最好的结果，具体来说是97.41%的肿瘤检测精度 для肝癌和94.71%的肺癌检测精度。论文还讨论了基于图像分析的未来癌症检测的方向。
</details></li>
</ul>
<hr>
<h2 id="RED-DOT-Multimodal-Fact-checking-via-Relevant-Evidence-Detection"><a href="#RED-DOT-Multimodal-Fact-checking-via-Relevant-Evidence-Detection" class="headerlink" title="RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection"></a>RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09939">http://arxiv.org/abs/2311.09939</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stevejpapad/relevant-evidence-detection">https://github.com/stevejpapad/relevant-evidence-detection</a></li>
<li>paper_authors: Stefanos-Iordanis Papadopoulos, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis</li>
<li>for: 本研究旨在提供一种自动Multimodal fact-checking方法，用于支持或驳斥声明的真实性。</li>
<li>methods: 本研究使用了一种新的”相关证据检测”（RED）模块，用于判断每个证据是否相关，以支持或驳斥声明。此外，研究还提出了多种体系和机制，如”导向注意力”模块，以提高模型的可解释性和性能。</li>
<li>results: 研究表明，使用RED-DOT模型可以在VERITEbenchmark上实现30%的提高，并在NewsCLIPings+上达到竞争性和改进的性能，无需大量的证据或多个Encoder。此外，研究还进行了质量分析，表明”导向注意力”模块可以提高模型的解释性。<details>
<summary>Abstract</summary>
Online misinformation is often multimodal in nature, i.e., it is caused by misleading associations between texts and accompanying images. To support the fact-checking process, researchers have been recently developing automatic multimodal methods that gather and analyze external information, evidence, related to the image-text pairs under examination. However, prior works assumed all collected evidence to be relevant. In this study, we introduce a "Relevant Evidence Detection" (RED) module to discern whether each piece of evidence is relevant, to support or refute the claim. Specifically, we develop the "Relevant Evidence Detection Directed Transformer" (RED-DOT) and explore multiple architectural variants (e.g., single or dual-stage) and mechanisms (e.g., "guided attention"). Extensive ablation and comparative experiments demonstrate that RED-DOT achieves significant improvements over the state-of-the-art on the VERITE benchmark by up to 28.5%. Furthermore, our evidence re-ranking and element-wise modality fusion led to RED-DOT achieving competitive and even improved performance on NewsCLIPings+, without the need for numerous evidence or multiple backbone encoders. Finally, our qualitative analysis demonstrates that the proposed "guided attention" module has the potential to enhance the architecture's interpretability. We release our code at: https://github.com/stevejpapad/relevant-evidence-detection
</details>
<details>
<summary>摘要</summary>
在线资讯承害 frequently 是多 modal 的，即由诽导的文字和附加的图像所致。为支持事实核查过程，研究人员最近已经开发出自动多模式方法，将外部信息、证据聚合和分析。但是，先前的工作假设所有收集到的证据都是有用的。在这一 studyt，我们引入一个“有用证据检测”（RED）模组，以决定每个证据是否有用，以支持或驳回主张。我们开发了“导向注意力”（RED-DOT）模型，并考虑多种架构和机制（例如，单Stage 或 dual-stage，以及“导向注意力”）。我们实施了广泛的删除和比较实验，证明了 RED-DOT 在 VERITE 标准 benchmark 上可以实现最多 28.5% 的提升。此外，我们的证据重新排序和元素综合模式融合实现了 RED-DOT 在 NewsCLIPings+ 上的竞争性和改进性，无需丰富的证据或多个背部构成器。最后，我们的Qualitative分析显示出“导向注意力”模组具有提高架构解释性的潜力。我们在 GitHub 上发布了代码：https://github.com/stevejpapad/relevant-evidence-detection。
</details></li>
</ul>
<hr>
<h2 id="Selection-of-Distinct-Morphologies-to-Divide-Conquer-Gigapixel-Pathology-Images"><a href="#Selection-of-Distinct-Morphologies-to-Divide-Conquer-Gigapixel-Pathology-Images" class="headerlink" title="Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel Pathology Images"></a>Selection of Distinct Morphologies to Divide &amp; Conquer Gigapixel Pathology Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09902">http://arxiv.org/abs/2311.09902</a></li>
<li>repo_url: None</li>
<li>paper_authors: Abubakr Shafique, Saghir Alfasly, Areej Alsaafin, Peyman Nejat, Jibran A. Khan, H. R. Tizhoosh</li>
<li>for: 本研究旨在提出一种选择小型、代表性强的WSIs补丁集合方法，以便在计算生物学中进行WSIs分类和匹配分析。</li>
<li>methods: 本方法基于”拆分与统一”的思想，通过绘制出WSIs中各种形态特征的示意图，并对这些示意图进行自动选择，以选择一个能够涵盖所有WSIs中形态特征的小型补丁集合。</li>
<li>results: 研究表明，SDM方法在多个公共和私人生化病理学数据集上具有remarkable的效果，并且不需要参数设定，因为它自动优化选择过程以捕捉WSIs中的形态特征。<details>
<summary>Abstract</summary>
Whole slide images (WSIs) are massive digital pathology files illustrating intricate tissue structures. Selecting a small, representative subset of patches from each WSI is essential yet challenging. Therefore, following the "Divide & Conquer" approach becomes essential to facilitate WSI analysis including the classification and the WSI matching in computational pathology. To this end, we propose a novel method termed "Selection of Distinct Morphologies" (SDM) to choose a subset of WSI patches. The aim is to encompass all inherent morphological variations within a given WSI while simultaneously minimizing the number of selected patches to represent these variations, ensuring a compact yet comprehensive set of patches. This systematically curated patch set forms what we term a "montage". We assess the representativeness of the SDM montage across various public and private histopathology datasets. This is conducted by using the leave-one-out WSI search and matching evaluation method, comparing it with the state-of-the-art Yottixel's mosaic. SDM demonstrates remarkable efficacy across all datasets during its evaluation. Furthermore, SDM eliminates the necessity for empirical parameterization, a crucial aspect of Yottixel's mosaic, by inherently optimizing the selection process to capture the distinct morphological features within the WSI.
</details>
<details>
<summary>摘要</summary>
整个扫描图像（WSIs）是大量数字病理学文件，展示了复杂的组织结构。选择WSIs中每个patch的小样本 subset是必要的，但是具有挑战性。为了促进WSIs的分析，包括类别和WSIs匹配，我们提出了一种新方法称为“选择独特形态”（SDM）。该方法的目标是在给定WSIs中涵盖所有自然形态的变化，同时最小化选择的patch数量，以确保一个紧凑且全面的patch集。这个系统化批处形成了我们称之为“montage”。我们在不同的公共和私人 histopathology 数据集上评估SDM montage的表现。我们使用离开一个 WSI 搜索和匹配评估方法，与现有的 Yottixel 的落幕进行比较。SDM在所有数据集上都表现出了很好的效果。此外，SDM 摒弃了 Yottixel 落幕中的参数化，因为它自动优化选择过程，以捕捉 WSIs 中的独特形态特征。
</details></li>
</ul>
<hr>
<h2 id="I-S-ViT-An-Inclusive-Stable-Method-for-Pushing-the-Limit-of-Post-Training-ViTs-Quantization"><a href="#I-S-ViT-An-Inclusive-Stable-Method-for-Pushing-the-Limit-of-Post-Training-ViTs-Quantization" class="headerlink" title="I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of Post-Training ViTs Quantization"></a>I&amp;S-ViT: An Inclusive &amp; Stable Method for Pushing the Limit of Post-Training ViTs Quantization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10126">http://arxiv.org/abs/2311.10126</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunshan Zhong, Jiawei Hu, Mingbao Lin, Mengzhao Chen, Rongrong Ji<br>for:* 这个 paper 是为了解决 transformer 模型在实际应用中的 Computational Cost 问题，特别是在训练和测试过程中的 dense 计算成本问题。methods:* 这个 paper 使用 post-training quantization (PTQ) 方法，将 transformer 模型训练后的 weights 量化为 low-bit 格式，以提高 computational efficiency。* 这个 paper  introduce 一个 novel 的 I&amp;S-ViT 方法，它可以在 PTQ 过程中稳定地调整 transformer 模型的 weights，以提高模型的性能。results:* 这个 paper 的结果显示，I&amp;S-ViT 方法可以在 diverse vision tasks 中提高 transformer 模型的性能，特别是在 low-bit enario 下。* 例如，I&amp;S-ViT 方法可以提高 3-bit ViT-B 模型的性能 by 50.68%。<details>
<summary>Abstract</summary>
Albeit the scalable performance of vision transformers (ViTs), the dense computational costs (training & inference) undermine their position in industrial applications. Post-training quantization (PTQ), tuning ViTs with a tiny dataset and running in a low-bit format, well addresses the cost issue but unluckily bears more performance drops in lower-bit cases. In this paper, we introduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an inclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of ViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for post-Softmax activations; (2) Rugged and magnified loss landscape in coarse-grained quantization granularity for post-LayerNorm activations. Then, I&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2 quantizer (SULQ) that incorporates a shift mechanism followed by uniform quantization to achieve both an inclusive domain representation and accurate distribution approximation; (2) A three-stage smooth optimization strategy (SOS) that amalgamates the strengths of channel-wise and layer-wise quantization to enable stable learning. Comprehensive evaluations across diverse vision tasks validate I&S-ViT' superiority over existing PTQ of ViTs methods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the performance of 3-bit ViT-B by an impressive 50.68%.
</details>
<details>
<summary>摘要</summary>
尽管Scalable Performance of Vision Transformers（ViTs），但是密集的计算成本（训练和推理）却限制其在产业应用中的位置。Post-training Quantization（PTQ），通过使用tiny dataset和低位数据类型进行调教，有效地解决了成本问题，但是在低位数据类型下会导致性能下降。在这篇论文中，我们引入I&S-ViT，一种新的方法，可以在包容和稳定的情况下进行PTQ的规范。I&S-ViT首先发现了ViTs中PTQ中的两个问题：（1）频繁使用的log2 quantizer在Post-Softmax活动中的量化不准确;（2）LayerNorm活动中的粗糙和增大的损失图像。然后，I&S-ViT通过引入以下两种方法来解决这些问题：（1）一种新的Shift-Uniform-Log2 quantizer（SULQ），通过添加shift机制并使用均匀量化来实现包容的领域表示和准确的分布近似;（2）一种三个阶段的smooth optimization strategy（SOS），将通道级和层级量化融合在一起，以实现稳定的学习。通过对多种视觉任务进行广泛的评估，我们证明I&S-ViT在PTQ方法中的超越性，特别是在低位数据类型下。例如，I&S-ViT使3位ViT-B的性能提高了50.68%。
</details></li>
</ul>
<hr>
<h2 id="UnifiedVisionGPT-Streamlining-Vision-Oriented-AI-through-Generalized-Multimodal-Framework"><a href="#UnifiedVisionGPT-Streamlining-Vision-Oriented-AI-through-Generalized-Multimodal-Framework" class="headerlink" title="UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework"></a>UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10125">http://arxiv.org/abs/2311.10125</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lhbuilder/sa-segment-anything">https://github.com/lhbuilder/sa-segment-anything</a></li>
<li>paper_authors: Chris Kelly, Luhui Hu, Cindy Yang, Yu Tian, Deshun Yang, Bang Yang, Zaoshan Huang, Zihao Li, Yuexian Zou</li>
<li>for: 这份论文的目的是发展一个可以整合多种现有的州OfTheArt（SOTA）Computer Vision（CV）模型的框架，以提高CV领域的进步和效率。</li>
<li>methods: 这份论文使用了一个名为UnifiedVisionGPT的框架，它可以融合多种SOTA CV模型，并且可以自动选择适合的模型基于多 modal 输入，例如文本提示和图像。</li>
<li>results: 这份论文显示了UnifiedVisionGPT的架构和能力，并证明了它在CV领域中的应用可以提高效率、多样性、普遍性和性能。<details>
<summary>Abstract</summary>
In the current landscape of artificial intelligence, foundation models serve as the bedrock for advancements in both language and vision domains. OpenAI GPT-4 has emerged as the pinnacle in large language models (LLMs), while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models such as Meta's SAM and DINO, and YOLOS. However, the financial and computational burdens of training new models from scratch remain a significant barrier to progress. In response to this challenge, we introduce UnifiedVisionGPT, a novel framework designed to consolidate and automate the integration of SOTA vision models, thereby facilitating the development of vision-oriented AI. UnifiedVisionGPT distinguishes itself through four key features: (1) provides a versatile multimodal framework adaptable to a wide range of applications, building upon the strengths of multimodal foundation models; (2) seamlessly integrates various SOTA vision models to create a comprehensive multimodal platform, capitalizing on the best components of each model; (3) prioritizes vision-oriented AI, ensuring a more rapid progression in the CV domain compared to the current trajectory of LLMs; and (4) introduces automation in the selection of SOTA vision models, generating optimal results based on diverse multimodal inputs such as text prompts and images. This paper outlines the architecture and capabilities of UnifiedVisionGPT, demonstrating its potential to revolutionize the field of computer vision through enhanced efficiency, versatility, generalization, and performance. Our implementation, along with the unified multimodal framework and comprehensive dataset, is made publicly available at https://github.com/LHBuilder/SA-Segment-Anything.
</details>
<details>
<summary>摘要</summary>
当前人工智能领域中，基础模型作为进步的基础，在语言和视觉领域得到了广泛应用。OpenAI GPT-4在大语言模型（LLM）中脱颖而出，而计算机视觉（CV）领域则拥有丰富的状态对照（SOTA）模型，如Meta的SAM和DINO，以及YOLOS。然而，培育新模型的财务和计算成本仍然是进步的主要障碍。为应对这个挑战，我们介绍了一种新的框架——统一视觉GPT，该框架旨在集成和自动化STATE OF THE ART（SOTA）视觉模型，以便开发视觉启发的人工智能。统一视觉GPT的四个关键特点是：（1）提供多样化的多Modal Framework，可适应各种应用场景，基于多Modal基础模型的优势；（2）将多种SOTA视觉模型集成在一起，创造出全面的多Modal平台，利用每个模型的优点；（3）强调视觉启发，以更快速地进步在计算机视觉领域，相比现有的语言模型的趋势；以及（4）通过自动化SOTA视觉模型的选择，根据多Modal输入 such as文本提示和图像，生成优化的结果。本文描述了统一视觉GPT的架构和能力，展示其在计算机视觉领域的潜在革命性。我们的实现，以及统一多Modal框架和完整的数据集，在https://github.com/LHBuilder/SA-Segment-Anything上公开提供。
</details></li>
</ul>
<hr>
<h2 id="Rusty-Detection-Using-Image-Processing-For-Maintenance-Of-Stations"><a href="#Rusty-Detection-Using-Image-Processing-For-Maintenance-Of-Stations" class="headerlink" title="Rusty Detection Using Image Processing For Maintenance Of Stations"></a>Rusty Detection Using Image Processing For Maintenance Of Stations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09849">http://arxiv.org/abs/2311.09849</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dao Duy Tung, Ho Xuan Hung</li>
<li>for: 这种研究旨在准确地分割涂抹的锈批镀表面上的锈区域。</li>
<li>methods: 该方法基于数字图像处理，使用HSV颜色模型进行分割，并应用单一级Retinex模型来平衡光照的影响。然后通过手动色滤波进行进一步处理，以增强锈区域的识别。最后，使用DBScan算法进行准确的锈区域分割。</li>
<li>results: 该方法可以准确地分割涂抹的锈批镀表面上的锈区域，提供一种有价值的锈检测和分析方法。<details>
<summary>Abstract</summary>
This study addresses the challenge of accurately seg-menting rusted areas on painted construction surfaces. A method leveraging digital image processing is explored to calculate the percentage of rust present on painted coatings. The proposed segmentation approach is based on the HSV color model. To equalize luminosity and mitigate the influence of illumination, a fundamental model of single-scale Retinex is applied specifically to the saturation component.   Subsequently, the image undergoes further processing, involv-ing manual color filtering. This step is crucial for refining the identification of rusted regions. To enhance precision and filter out noise, the pixel areas selected through color filtering are subjected to the DBScan algorithm. This multi-step process aims to achieve a robust segmentation of rusted areas on painted construction surfaces, providing a valuable contribution to the field of corrosion detection and analysis.
</details>
<details>
<summary>摘要</summary>
First, a fundamental model of single-scale Retinex is applied to the saturation component of the image to equalize luminosity and mitigate the influence of illumination. Next, the image undergoes manual color filtering to refine the identification of rusted regions. Finally, the pixel areas selected through color filtering are subjected to the DBScan algorithm to enhance precision and filter out noise.The proposed segmentation approach is designed to provide a robust and accurate method for detecting rusted areas on painted construction surfaces, which is a valuable contribution to the field of corrosion detection and analysis.
</details></li>
</ul>
<hr>
<h2 id="Overcoming-Data-Scarcity-in-Biomedical-Imaging-with-a-Foundational-Multi-Task-Model"><a href="#Overcoming-Data-Scarcity-in-Biomedical-Imaging-with-a-Foundational-Multi-Task-Model" class="headerlink" title="Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model"></a>Overcoming Data Scarcity in Biomedical Imaging with a Foundational Multi-Task Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09847">http://arxiv.org/abs/2311.09847</a></li>
<li>repo_url: None</li>
<li>paper_authors: Raphael Schäfer, Till Nicke, Henning Höfener, Annkristin Lange, Dorit Merhof, Friedrich Feuerhake, Volkmar Schulz, Johannes Lotz, Fabian Kiessling</li>
<li>For: 这个论文的目的是提出一种基于多任务学习的基本模型训练策略，以便在生物医学成像领域中使用。* Methods: 这个论文使用了一种多任务学习策略，其中包括将多种类别和分类任务组织在一起，以便减少训练数据的内存需求。* Results: 研究发现，使用这种多任务学习策略可以让基本模型在不同的任务上保持高度的性能，并且只需要1%的原始训练数据和不需要细化。此外，这种方法还可以在不同的中心进行跨中心传输。<details>
<summary>Abstract</summary>
Foundational models, pretrained on a large scale, have demonstrated substantial success across non-medical domains. However, training these models typically requires large, comprehensive datasets, which contrasts with the smaller and more heterogeneous datasets common in biomedical imaging. Here, we propose a multi-task learning strategy that decouples the number of training tasks from memory requirements. We trained a Universal bioMedical PreTrained model (UMedPT) on a multi-task database including tomographic, microscopic, and X-ray images, with various labelling strategies such as classification, segmentation, and object detection. The UMedPT foundational model outperformed ImageNet pretraining and the previous state-of-the-art models. For tasks related to the pretraining database, it maintained its performance with only 1% of the original training data and without fine-tuning. For out-of-domain tasks it required not more than 50% of the original training data. In an external independent validation imaging features extracted using UMedPT proved to be a new standard for cross-center transferability.
</details>
<details>
<summary>摘要</summary>
基础模型，在大规模预训练下，在非医学领域实现了重要成功。然而，这些模型的训练通常需要大量、全面的数据集，而生物医学成像中的数据集通常较小、更加多样化。在这里，我们提出了一种多任务学习策略，即解耦训练任务数量和内存需求。我们使用了一个通用的生物医学预训练模型（UMedPT），在包括tomographic、微scopic和X射线图像的多任务数据库上进行训练，并采用了不同的标签策略，如分类、分割和对象检测。UMedPT基础模型在预训练数据库中相对于ImageNet预训练和前一个状态的模型表现出色。对于与预训练数据库相关的任务，它只需要1%的原始训练数据，而不需要微调。对于外部独立验证的任务，它只需要50%的原始训练数据。在外部独立验证中，使用UMedPT提取出的生物医学特征被认为是新的跨中心传送标准。
</details></li>
</ul>
<hr>
<h2 id="GroupMixer-Patch-based-Group-Convolutional-Neural-Network-for-Breast-Cancer-Detection-from-Histopathological-Images"><a href="#GroupMixer-Patch-based-Group-Convolutional-Neural-Network-for-Breast-Cancer-Detection-from-Histopathological-Images" class="headerlink" title="GroupMixer: Patch-based Group Convolutional Neural Network for Breast Cancer Detection from Histopathological Images"></a>GroupMixer: Patch-based Group Convolutional Neural Network for Breast Cancer Detection from Histopathological Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09846">http://arxiv.org/abs/2311.09846</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ardavan Modarres, Erfan Ebrahim Esfahani, Mahsa Bahrami</li>
<li>for: 检测悬肢癌细胞恶性的早期阶段是控制其影响的关键步骤。 histopathological analysis 提供了独特的机会 для恶性肿瘤检测。但是，这种任务会对 histopathologists 太 tedious 和 time-consuming。</li>
<li>methods: 使用 Deep Neural Networks 直接从 raw histopathological images 学习 informative features，而不需要 manual feature extraction。</li>
<li>results: 使用 CNN 架构和 Patch Embedding 操作，实现了高度精准的悬肢癌检测，并且对比其他方法具有更多的 trainable parameters 和更大的数据集来进行训练。尽管使用 Transformer 架构在医学图像分析中显示了惊人的表现，但是这些架构具有较多的可训练参数和需要大量的数据来进行训练。<details>
<summary>Abstract</summary>
Diagnosis of breast cancer malignancy at the early stages is a crucial step for controlling its side effects. Histopathological analysis provides a unique opportunity for malignant breast cancer detection. However, such a task would be tedious and time-consuming for the histopathologists. Deep Neural Networks enable us to learn informative features directly from raw histopathological images without manual feature extraction. Although Convolutional Neural Networks (CNNs) have been the dominant architectures in the computer vision realm, Transformer-based architectures have shown promising results in different computer vision tasks. Although harnessing the capability of Transformer-based architectures for medical image analysis seems interesting, these architectures are large, have a significant number of trainable parameters, and require large datasets to be trained on, which are usually rare in the medical domain. It has been claimed and empirically proved that at least part of the superior performance of Transformer-based architectures in Computer Vision domain originates from patch embedding operation. In this paper, we borrowed the previously introduced idea of integrating a fully Convolutional Neural Network architecture with Patch Embedding operation and presented an efficient CNN architecture for breast cancer malignancy detection from histopathological images. Despite the number of parameters that is significantly smaller than other methods, the accuracy performance metrics achieved 97.65%, 98.92%, 99.21%, and 98.01% for 40x, 100x, 200x, and 400x magnifications respectively. We took a step forward and modified the architecture using Group Convolution and Channel Shuffling ideas and reduced the number of trainable parameters even more with a negligible decline in performance and achieved 95.42%, 98.16%, 96.05%, and 97.92% accuracy for the mentioned magnifications respectively.
</details>
<details>
<summary>摘要</summary>
诊断乳腺癌恶性肿瘤的初期阶段是控制其副作用的关键步骤。 histopathological 分析提供了诊断恶性乳腺癌的唯一机会。然而，这种任务对 histopathologists 来说是繁琐和时间consuming的。深度神经网络允许我们直接从 Raw histopathological 图像中学习有用的特征，无需手动提取特征。尽管卷积神经网络（CNNs）在计算机视觉领域是主导的建筑，但基于 Transformer 的建筑在不同的计算机视觉任务中表现出色。虽然在医疗领域使用基于 Transformer 的建筑可能有趣，这些建筑却很大，有很多可训练的参数，并且需要大量的数据来进行训练，这些数据通常在医疗领域是罕见的。有人提出并证明了，至少一部分基于 Transformer 的表现提升的原因是负权重嵌入操作。在这篇论文中，我们借鉴了之前引入的将 Fully Convolutional Neural Network 架构与负权重嵌入操作结合的想法，并提出了一种高效的乳腺癌恶性识别方法。尽管参数的数量远少于其他方法，但我们在 40x、100x、200x 和 400x 倍镜下测试的性能指标分别达到 97.65%、98.92%、99.21% 和 98.01%。我们进一步修改了架构，使用 Group Convolution 和 Channel Shuffling 的想法，并减少了可训练参数的数量，但性能的下降是极少的，达到 95.42%、98.16%、96.05% 和 97.92%。
</details></li>
</ul>
<hr>
<h2 id="MAM-E-Mammographic-synthetic-image-generation-with-diffusion-models"><a href="#MAM-E-Mammographic-synthetic-image-generation-with-diffusion-models" class="headerlink" title="MAM-E: Mammographic synthetic image generation with diffusion models"></a>MAM-E: Mammographic synthetic image generation with diffusion models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09822">http://arxiv.org/abs/2311.09822</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Likalto4/diffusion-models_master">https://github.com/Likalto4/diffusion-models_master</a></li>
<li>paper_authors: Ricardo Montoya-del-Angel, Karla Sam-Millan, Joan C Vilanova, Robert Martí</li>
<li>for: 本研究旨在使用扩散模型作为医疗影像资料增强技术，以解决医疗影像领域资料缺乏的问题。</li>
<li>methods: 本研究使用了现代的条件扩散管道，以生成高质量的全场数字乳腺影像。同时，我们还提出使用稳定的扩散模型来填充人工变化的Synthetic lesions在健康乳腺影像上。</li>
<li>results: 我们提出了一个名为MAM-E的生成模型架空，可以根据文本提示生成高质量的乳腺影像，并且可以填充人工变化的Synthetic lesions在specific region of the breast。 finally, we provide了量化和质感评估的生成影像，以及易用的グラフィカルユーザインターフェース для乳腺影像生成。<details>
<summary>Abstract</summary>
Generative models are used as an alternative data augmentation technique to alleviate the data scarcity problem faced in the medical imaging field. Diffusion models have gathered special attention due to their innovative generation approach, the high quality of the generated images and their relatively less complex training process compared with Generative Adversarial Networks. Still, the implementation of such models in the medical domain remains at early stages. In this work, we propose exploring the use of diffusion models for the generation of high quality full-field digital mammograms using state-of-the-art conditional diffusion pipelines. Additionally, we propose using stable diffusion models for the inpainting of synthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of generative models for high quality mammography synthesis controlled by a text prompt and capable of generating synthetic lesions on specific regions of the breast. Finally, we provide quantitative and qualitative assessment of the generated images and easy-to-use graphical user interfaces for mammography synthesis.
</details>
<details>
<summary>摘要</summary>
“生成模型被用作医疗影像领域数据增强技术的替代方法，以解决医疗影像领域面临的数据缺乏问题。扩散模型吸引了特别的关注，因为它们的创新生成方法、高质量生成图像和相对于对抗网络更为简单的训练过程。然而，医疗领域中的实施仍然处于早期阶段。在这种工作中，我们提议使用扩散模型来生成高质量全场数字乳肿图像，并使用稳定的扩散模型进行 synthetic 病变的填充。我们介绍了MAM-E，一个基于生成模型的高质量乳肿合成管道，可以通过文本提示来控制生成过程。此外，我们还提供了生成图像的量化和质量评估，以及易用的图形用户界面。”
</details></li>
</ul>
<hr>
<h2 id="Neural-Logic-Human-Object-Interaction-Detection"><a href="#Neural-Logic-Human-Object-Interaction-Detection" class="headerlink" title="Neural-Logic Human-Object Interaction Detection"></a>Neural-Logic Human-Object Interaction Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09817">http://arxiv.org/abs/2311.09817</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Aryia-Behroziuan/Other-sources">https://github.com/Aryia-Behroziuan/Other-sources</a></li>
<li>paper_authors: Liulei Li, Jianan Wei, Wenguan Wang, Yi Yang</li>
<li>for: 提高 HOI 检测器的性能和零学习推广能力</li>
<li>methods: 修改传统 Transformer 自注意机制，以便在 &lt;人, 动作, 物品&gt;  triplet 上进行神经逻辑reasoning，并由两种关键性 HOI 理解属性（可用性和 прокси迫）导引。</li>
<li>results: 在 V-COCO 和 HICO-DET 上对常规和零学习情况下，实现了显著改善，与现有方法相比。<details>
<summary>Abstract</summary>
The interaction decoder utilized in prevalent Transformer-based HOI detectors typically accepts pre-composed human-object pairs as inputs. Though achieving remarkable performance, such paradigm lacks feasibility and cannot explore novel combinations over entities during decoding. We present L OGIC HOI, a new HOI detector that leverages neural-logic reasoning and Transformer to infer feasible interactions between entities. Specifically, we modify the self-attention mechanism in vanilla Transformer, enabling it to reason over the <human, action, object> triplet and constitute novel interactions. Meanwhile, such reasoning process is guided by two crucial properties for understanding HOI: affordances (the potential actions an object can facilitate) and proxemics (the spatial relations between humans and objects). We formulate these two properties in first-order logic and ground them into continuous space to constrain the learning process of our approach, leading to improved performance and zero-shot generalization capabilities. We evaluate L OGIC HOI on V-COCO and HICO-DET under both normal and zero-shot setups, achieving significant improvements over existing methods.
</details>
<details>
<summary>摘要</summary>
很多现有的Transformer基于HOI探测器通常使用预 compose的人物对碰输入decoder。虽然实现了惊人的性能，但这种方法缺乏实用性，不能探索Entities During Decoding中的新组合。我们介绍了 L OGIC HOI，一种新的HOI探测器，利用神经逻辑理解和Transformer来推理可能的人物对碰。具体来说，我们修改了vanilla Transformer中的自我注意机制，使其能够对 <人,行为,物> triplet进行推理，并且通过两个关键的HOI理解属性：可行性（物品可能支持的行为）和距离（人类和物品之间的空间关系）。我们将这两个属性写入了第一频谱逻辑，并将其降到连续空间中，以制约我们的方法学习过程，从而提高性能和零shot泛化能力。我们在V-COCO和HICO-DET上评估了L OGIC HOI，在正常和零shot设置下都达到了显著的改善。
</details></li>
</ul>
<hr>
<h2 id="MetaDreamer-Efficient-Text-to-3D-Creation-With-Disentangling-Geometry-and-Texture"><a href="#MetaDreamer-Efficient-Text-to-3D-Creation-With-Disentangling-Geometry-and-Texture" class="headerlink" title="MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture"></a>MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry and Texture</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10123">http://arxiv.org/abs/2311.10123</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lincong Feng, Muyu Wang, Maoyu Wang, Kuo Xu, Xiaoli Liu<br>for:* 这个研究旨在提高三维物体生成的效率和质量，并且解决多视角对称和实际对称的问题。methods:* 这个方法使用了两个阶段的优化方法，首先优化三维物体的几何表示，然后进行细微调整和纹理优化。results:* 这个方法可以实现高品质的三维物体生成，并且可以在20分钟内完成文本描述的三维生成。此外，这个方法还能够实现图像控制，提高了三维生成的可控性。<details>
<summary>Abstract</summary>
Generative models for 3D object synthesis have seen significant advancements with the incorporation of prior knowledge distilled from 2D diffusion models. Nevertheless, challenges persist in the form of multi-view geometric inconsistencies and slow generation speeds within the existing 3D synthesis frameworks. This can be attributed to two factors: firstly, the deficiency of abundant geometric a priori knowledge in optimization, and secondly, the entanglement issue between geometry and texture in conventional 3D generation methods.In response, we introduce MetaDreammer, a two-stage optimization approach that leverages rich 2D and 3D prior knowledge. In the first stage, our emphasis is on optimizing the geometric representation to ensure multi-view consistency and accuracy of 3D objects. In the second stage, we concentrate on fine-tuning the geometry and optimizing the texture, thereby achieving a more refined 3D object. Through leveraging 2D and 3D prior knowledge in two stages, respectively, we effectively mitigate the interdependence between geometry and texture. MetaDreamer establishes clear optimization objectives for each stage, resulting in significant time savings in the 3D generation process. Ultimately, MetaDreamer can generate high-quality 3D objects based on textual prompts within 20 minutes, and to the best of our knowledge, it is the most efficient text-to-3D generation method. Furthermore, we introduce image control into the process, enhancing the controllability of 3D generation. Extensive empirical evidence confirms that our method is not only highly efficient but also achieves a quality level that is at the forefront of current state-of-the-art 3D generation techniques.
</details>
<details>
<summary>摘要</summary>
现代生成模型在三维物体生成方面已经做出了重要进步，通过吸取二维扩散模型中的知识。然而，现有的三维生成框架仍然面临着多视图几何不一致和慢速生成速度的挑战。这可以归结于两点：首先，严重缺乏丰富的几何知识在优化中，其次，在传统的三维生成方法中，几何和文本之间存在杂化问题。为了解决这些问题，我们介绍MetaDreamer，一种两stage优化方法，利用强大的二维和三维知识。在第一个阶段，我们强调优化几何表示，以确保多视图一致性和三维物体的准确性。在第二个阶段，我们专注于细化几何和优化文本，以实现更加细腻的三维物体。通过在两个阶段分别利用二维和三维知识，我们有效地消除了几何和文本之间的互相关系。MetaDreamer采用清晰的优化目标，从而大大降低三维生成过程中的时间成本。最终，MetaDreamer可以在20分钟内基于文本提示生成高质量的三维物体，并且，到目前为止，它是目前最高效的文本到三维生成方法。此外，我们还引入图像控制，使三维生成过程中的控制性得到了进一步提升。广泛的实验证明，我们的方法不仅高效，而且达到了当前领域的前沿水平。
</details></li>
</ul>
<hr>
<h2 id="EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction-on-Mobile-Devices"><a href="#EvaSurf-Efficient-View-Aware-Implicit-Textured-Surface-Reconstruction-on-Mobile-Devices" class="headerlink" title="EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices"></a>EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction on Mobile Devices</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09806">http://arxiv.org/abs/2311.09806</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jingnan Gao, Zhuo Chen, Yichao Yan, Bowen Pan, Zhe Wang, Jiangjing Lyu, Xiaokang Yang</li>
<li>For: 高效率、视角受限的3D对象重建* Methods: 使用高效表面基本模型、多视图监测模块、含有 Gaussian 豆的隐式Texture 以及轻量级神经灯谱* Results: 能够在移动设备上实现高质量的外观和准确的网格重建，并且可以在1-2个小时内训练使用单个GPU，并在40帧&#x2F;秒以上的帧率下运行。<details>
<summary>Abstract</summary>
Reconstructing real-world 3D objects has numerous applications in computer vision, such as virtual reality, video games, and animations. Ideally, 3D reconstruction methods should generate high-fidelity results with 3D consistency in real-time. Traditional methods match pixels between images using photo-consistency constraints or learned features, while differentiable rendering methods like Neural Radiance Fields (NeRF) use surface-based representations or differentiable volume rendering to generate high-fidelity scenes. However, these methods require excessive runtime for rendering, making them impractical for daily applications. To address these challenges, we present $\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware Implicit Textured $\textbf{Surf}$ace Reconstruction method on Mobile Devices. In our method, we first employ an efficient surface-based model with a multi-view supervision module to ensure accurate mesh creation. To enable high-fidelity rendering, we learn an implicit texture embedded with a set of Gaussian lobes to capture view-dependent information. Furthermore, With the explicit geometry and the implicit texture, we can employ a lightweight neural shader to reduce the expense of computation and further support real-time rendering on common mobile devices. Extensive experiments demonstrate that our method can reconstruct high-quality appearance and accurate mesh on both synthetic and real-world datasets. Moreover, our method can be trained in just 1-2 hours using a single GPU and run on mobile devices at over 40FPS (Frames Per Second), with a final package required for rendering taking up only 40-50 MB.
</details>
<details>
<summary>摘要</summary>
<<SYS>>重建现实世界中的3D对象有很多应用程序在计算机视觉中，如虚拟现实、游戏和动画。理想情况下，3D重建方法应该生成高品质的结果，并在实时中进行渲染。传统方法通过图像匹配 pixels 使用光度约束或学习特征来实现，而 differentiable rendering 方法如神经辐射场（NeRF）则使用表面基本表示或可导渲染来生成高品质场景。然而，这些方法在渲染时需要过分的时间，使得它们在日常应用中不够实用。为解决这些挑战，我们介绍 $\textbf{EvaSurf}$，一种高效的视觉相关的凹面 Textured 表面重建方法，运行在移动设备上。在我们的方法中，我们首先采用高效的表面基本模型，并在多视图超vision模块的支持下确保精度的网格创建。为了实现高品质渲染，我们学习了一个包含多个 Gaussian lobes 的隐式文本，以捕捉视觉相关信息。此外，通过Explicit geometry和隐式文本，我们可以使用轻量级的神经渲染器来减少计算成本，并进一步支持实时渲染在常见的移动设备上。广泛的实验表明，我们的方法可以在 Both synthetic and real-world datasets 上重建高质量的外观和准确的网格，并且可以在1-2小时内在单个 GPU 上训练，并在移动设备上运行于40帧/秒（Frame Per Second），总包装需要40-50 MB。Note: The text has been translated using Google Translate, and some parts may not be perfectly accurate or idiomatic.
</details></li>
</ul>
<hr>
<h2 id="Certified-Control-for-Train-Sign-Classification"><a href="#Certified-Control-for-Train-Sign-Classification" class="headerlink" title="Certified Control for Train Sign Classification"></a>Certified Control for Train Sign Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09778">http://arxiv.org/abs/2311.09778</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Roßbach, Michael Leuschel</li>
<li>for: 这篇论文是为了研究一种用于验证自动驾驶列车系统的证明框架，以避免识别器错误地检测交通标志。</li>
<li>methods: 这篇论文使用了经典的计算机视觉算法来检查检测到的交通标志是否遵循预先定义的规范。</li>
<li>results: 我们的初步结果很有希望，可以达到较高的准确率，同时只有较小的报告率下降，但是进一步的推广性研究仍然需要进行。<details>
<summary>Abstract</summary>
There is considerable industrial interest in integrating AI techniques into railway systems, notably for fully autonomous train systems. The KI-LOK research project is involved in developing new methods for certifying such AI-based systems. Here we explore the utility of a certified control architecture for a runtime monitor that prevents false positive detection of traffic signs in an AI-based perception system. The monitor uses classical computer vision algorithms to check if the signs -- detected by an AI object detection model -- fit predefined specifications. We provide such specifications for some critical signs and integrate a Python prototype of the monitor with a popular object detection model to measure relevant performance metrics on generated data. Our initial results are promising, achieving considerable precision gains with only minor recall reduction; however, further investigation into generalization possibilities will be necessary.
</details>
<details>
<summary>摘要</summary>
有很大的工业兴趣在将人工智能技术应用于铁路系统中，特别是实现无人驾驶列车系统。KI-LOK研究项目正在开发新的认证方法，以确保这些基于AI的系统的可靠性。我们在这篇文章中探讨一种具有认证控制架构的运行监控系统，以防止AI基于感知系统中的假阳性检测交通标志。这个监控系统使用传统的计算机视觉算法来检查探测到的标志是否符合预定义的规范。我们为一些关键的标志提供了特定的规范，并将Python原型 integrate with a popular object detection model to measure relevant performance metrics on generated data。我们的初步结果很有前途，可以实现较大的准确率提升，但是需要进一步的探索可泛化性。
</details></li>
</ul>
<hr>
<h2 id="Video-LLaVA-Learning-United-Visual-Representation-by-Alignment-Before-Projection"><a href="#Video-LLaVA-Learning-United-Visual-Representation-by-Alignment-Before-Projection" class="headerlink" title="Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"></a>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10122">http://arxiv.org/abs/2311.10122</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/PKU-YuanGroup/Video-LLaVA">https://github.com/PKU-YuanGroup/Video-LLaVA</a></li>
<li>paper_authors: Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, Li Yuan</li>
<li>for: 提高视觉语言理解的下游任务性能</li>
<li>methods: 将图像和视频编码到一个共享特征空间中，并将其作为大语言模型的输入</li>
<li>results: 创建了一个简单 yet robust LVLM 基线模型 Video-LLaVA，可以从混合图像和视频 dataset 中学习并提高多模态交互。Video-LLaVA 在 9 个图像benchmark 上表现出色，并在 5 个图像问答dataset 和 4 个图像benchmark toolkits 上超过 Video-ChatGPT。<details>
<summary>Abstract</summary>
The Large Vision-Language Model (LVLM) has enhanced the performance of various downstream tasks in visual-language understanding. Most existing approaches encode images and videos into separate feature spaces, which are then fed as inputs to large language models. However, due to the lack of unified tokenization for images and videos, namely misalignment before projection, it becomes challenging for a Large Language Model (LLM) to learn multi-modal interactions from several poor projection layers. In this work, we unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM. As a result, we establish a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other. Video-LLaVA achieves superior performances on a broad range of 9 image benchmarks across 5 image question-answering datasets and 4 image benchmark toolkits. Additionally, our Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive experiments demonstrate that Video-LLaVA mutually benefits images and videos within a unified visual representation, outperforming models designed specifically for images or videos.
</details>
<details>
<summary>摘要</summary>
大型视语模型（LVLM）已经提高了多个下游任务的性能，包括图像和视频理解。大多数现有的方法将图像和视频编码为分开的特征空间，然后将其作为大型语言模型的输入。然而，由于图像和视频的不一致编码，即 projection 层的不一致，使得大型语言模型（LLM）学习多modal交互变得困难。在这种情况下，我们将视觉表示 integrate 到语言特征空间中，以提高基础的 LLM  towards 一体化 LVLM。因此，我们建立了简单 yet robust 的 LVM 基eline，称为 Video-LLaVA，它从混合的图像和视频数据集中学习，并且互相增强。 Video-LLaVA 在 9 个图像benchmark上 achieve 出色的表现，包括 5 个图像问答数据集和 4 个图像benchmark工具kit。此外，我们的 Video-LLaVA 还比 Video-ChatGPT 在 MSRVTT、MSVD、TGIF 和 ActivityNet 上表现出色，提高了 5.8%、9.9%、18.6% 和 10.1%。值得注意的是，广泛的实验表明，Video-LLaVA 能够在一个统一的视觉表示中促进图像和视频之间的互助，并且在图像和视频特征空间中提高表现，比特定于图像或视频的模型更好。
</details></li>
</ul>
<hr>
<h2 id="Slide-SAM-Medical-SAM-Meets-Sliding-Window"><a href="#Slide-SAM-Medical-SAM-Meets-Sliding-Window" class="headerlink" title="Slide-SAM: Medical SAM Meets Sliding Window"></a>Slide-SAM: Medical SAM Meets Sliding Window</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10121">http://arxiv.org/abs/2311.10121</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quan Quan, Fenghe Tang, Zikang Xu, Heqin Zhu, S. Kevin Zhou</li>
<li>For: This paper proposes a new method called Slide-SAM for 3D medical image segmentation, which extends the Segment Anything Model (SAM) to 3D medical images.* Methods: The proposed method uses a single slice prompt to segment the entire volume, reducing the prompt workload for professionals. It also uses high resolution (H$ \times $W &#x3D; 1024$ \times $1024) for training in 3D images to achieve optimal learning for small targets.* Results: The proposed method was evaluated on multiple datasets and achieved the most advanced 3D segmentation performance while maintaining the minimum prompt. The code will be open source soon.Here’s the summary in Simplified Chinese:</li>
<li>for: 这篇论文提出了一种新方法 called Slide-SAM，用于3D医学图像分割。</li>
<li>methods: 该方法使用单个slice提示来分割整个量子，减少专业人员的提示工作负担。它还使用高分辨率（H$ \times $W &#x3D; 1024$ \times $1024）在3D图像上进行训练，以便在小目标上达到优化的学习。</li>
<li>results: 该方法在多个数据集上进行了评估，并达到了最高的3D分割性能，同时保持最小的提示。代码即将公开源代码。<details>
<summary>Abstract</summary>
Segment Anything Model (SAM) achieves remarkable results in 2D image segmentation of natural images. However, the huge gap between medical images and natural images prevents it directly applied to medical image segmentation tasks. Especially in 3D medical image, SAM cannot learn the contextual relationship between slices, which limites application in real scenarios. In addition, recent research shows that applying 2D SAM to 3D images requires prompting the entire volume, which is time and label comsuming. In order to solve the above problems, we introduced Slide-SAM which extended SAM to 3D medical images. Specifically, you only need to use a single slice prompt to segement the entire volume, which greatly reduces the prompt workload for professionals. Secondly, unlike traditional 3D medical image segmentation, we are free from the influence of computing resources and can still use high resolution (H$ \times $W = 1024$ \times $1024) for training in 3D images to achieve optimal learning for small targets. This is to combine the entire 3D volume is beyond the reach of training. Finally, we collected a large number of 3D images from large-scale 3D public and private datasets, and extended SAM to 3D medical image segmentation involving bounding box and point prompts. Finally, we perform a comprehensive evaluation and analysis investigating the performance of Slide-SAM in medical image segmentation of different modalities, anatomy, and organs. We have verified Slide-SAM's segmentation capabilities on multiple datasets, achieving the most advanced 3D segmentation performance while maintaining the minimum prompt. Code will be open source soon.
</details>
<details>
<summary>摘要</summary>
Segment Anything Model (SAM) 在自然图像2D分割任务上取得了惊人的结果。然而，医疗图像与自然图像之间的巨大差距使得SAM直接应用于医疗图像分割任务是不可能的。尤其是在3D医疗图像中，SAM无法学习层次关系 между slice，这限制了其在实际场景中的应用。此外， latest research 表明，将2D SAM应用于3D图像需要整个Volume提示，这是时间和标签耗费的。为解决以上问题，我们提出了Slide-SAM，它将SAM扩展到3D医疗图像。具体来说，只需使用单个slice提示可以分割整个Volume，这大幅减少了专业人员的提示工作负担。其次，与传统3D医疗图像分割不同，我们不受计算资源的限制，可以在3D图像的训练中使用高分辨率（H $\times $ W = 1024 $\times $ 1024），以达到最佳学习效果。最后，我们收集了大量3D图像从大规模的3D公共和私人数据集，并将SAM扩展到3D医疗图像分割，包括 bounding box 和点提示。我们进行了全面的评估和分析，investigating 3D segmentation的不同Modalities、Anatomy 和器官的性能。我们已经证明Slide-SAM在不同Modalities、Anatomy 和器官上的 segmentation 性能是最先进的，同时保持最少的提示。代码即将公开源代码。
</details></li>
</ul>
<hr>
<h2 id="Utilizing-dataset-affinity-prediction-in-object-detection-to-assess-training-data"><a href="#Utilizing-dataset-affinity-prediction-in-object-detection-to-assess-training-data" class="headerlink" title="Utilizing dataset affinity prediction in object detection to assess training data"></a>Utilizing dataset affinity prediction in object detection to assess training data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09768">http://arxiv.org/abs/2311.09768</a></li>
<li>repo_url: None</li>
<li>paper_authors: Stefan Becker, Jens Bayer, Ronny Hug, Wolfgang Hübner, Michael Arens</li>
<li>for: 提高对象探测器的训练效果和一致性，并且可以采用不同的汽车数据集来增强模型的通用性。</li>
<li>methods: 提出了一种在检测时加入数据源预测模块的方法，以便在训练时更好地评估数据集的信息价值，从而提高对象探测器的性能。</li>
<li>results: 研究表明，通过自动选择不同数据集中的样本，可以训练对象探测器使用较少的训练样本，而无需失去检测精度。<details>
<summary>Abstract</summary>
Data pooling offers various advantages, such as increasing the sample size, improving generalization, reducing sampling bias, and addressing data sparsity and quality, but it is not straightforward and may even be counterproductive. Assessing the effectiveness of pooling datasets in a principled manner is challenging due to the difficulty in estimating the overall information content of individual datasets. Towards this end, we propose incorporating a data source prediction module into standard object detection pipelines. The module runs with minimal overhead during inference time, providing additional information about the data source assigned to individual detections. We show the benefits of the so-called dataset affinity score by automatically selecting samples from a heterogeneous pool of vehicle datasets. The results show that object detectors can be trained on a significantly sparser set of training samples without losing detection accuracy.
</details>
<details>
<summary>摘要</summary>
数据聚合提供了各种优势，如增加样本大小、改善泛化、减少采样偏见和解决数据稀缺和质量问题，但不是直接的并可能是Counterproductive。评估聚合dataset的效果是有挑战的，因为难以估计个体dataset的总信息内容。为此，我们提议在标准对象检测管道中添加数据源预测模块。该模块在推理时间产生较少的开销，为个体检测提供额外信息关于分配给它的数据源。我们显示了所谓的数据源相互邻接分数的好处，自动从多种交通工具数据集中选择样本。结果表明，可以通过减少训练样本的数量来训练对象检测器，而不会影响检测精度。
</details></li>
</ul>
<hr>
<h2 id="Scene-Text-Image-Super-resolution-based-on-Text-conditional-Diffusion-Models"><a href="#Scene-Text-Image-Super-resolution-based-on-Text-conditional-Diffusion-Models" class="headerlink" title="Scene Text Image Super-resolution based on Text-conditional Diffusion Models"></a>Scene Text Image Super-resolution based on Text-conditional Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09759">http://arxiv.org/abs/2311.09759</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chihiro Noguchi, Shun Fukuda, Masao Yamanaka<br>for: 这 paper 的目的是提出一种基于文本条件扩散模型（DM）的Scene Text Image Super-resolution（STISR）方法，以提高Scene Text Recognition（STR）的性能。methods: 这 paper 使用了文本条件扩散模型（DM），其能够Synthesize高分辨率（HR）文本图像，从而提高STR的性能。results:  experiments 表明，使用文本条件扩散模型（DM）可以 notable improve STISR 的性能，特别是当输入为低分辨率（LR）文本图像时。此外，该方法还可以生成高分辨率（HR）和低分辨率（LR）的对应图像对，为 STR 的训练提供了更好的数据支持。<details>
<summary>Abstract</summary>
Scene Text Image Super-resolution (STISR) has recently achieved great success as a preprocessing method for scene text recognition. STISR aims to transform blurred and noisy low-resolution (LR) text images in real-world settings into clear high-resolution (HR) text images suitable for scene text recognition. In this study, we leverage text-conditional diffusion models (DMs), known for their impressive text-to-image synthesis capabilities, for STISR tasks. Our experimental results revealed that text-conditional DMs notably surpass existing STISR methods. Especially when texts from LR text images are given as input, the text-conditional DMs are able to produce superior quality super-resolution text images. Utilizing this capability, we propose a novel framework for synthesizing LR-HR paired text image datasets. This framework consists of three specialized text-conditional DMs, each dedicated to text image synthesis, super-resolution, and image degradation. These three modules are vital for synthesizing distinct LR and HR paired images, which are more suitable for training STISR methods. Our experiments confirmed that these synthesized image pairs significantly enhance the performance of STISR methods in the TextZoom evaluation.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DIFFNAT-Improving-Diffusion-Image-Quality-Using-Natural-Image-Statistics"><a href="#DIFFNAT-Improving-Diffusion-Image-Quality-Using-Natural-Image-Statistics" class="headerlink" title="DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics"></a>DIFFNAT: Improving Diffusion Image Quality Using Natural Image Statistics</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09753">http://arxiv.org/abs/2311.09753</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Roy, Maiterya Suin, Anshul Shah, Ketul Shah, Jiang Liu, Rama Chellappa</li>
<li>for: 提高生成图像质量</li>
<li>methods: 使用 Kurtosis Concentration (KC) 损失函数</li>
<li>results: 在三种不同任务中（1）个性化几何折衔练习、（2）无条件图像生成、（3）图像超分辨率）中提高了 perceived 质量， measured by FID、MUSIQ score 和用户评价。<details>
<summary>Abstract</summary>
Diffusion models have advanced generative AI significantly in terms of editing and creating naturalistic images. However, efficiently improving generated image quality is still of paramount interest. In this context, we propose a generic "naturalness" preserving loss function, viz., kurtosis concentration (KC) loss, which can be readily applied to any standard diffusion model pipeline to elevate the image quality. Our motivation stems from the projected kurtosis concentration property of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. To retain the "naturalness" of the generated images, we enforce reducing the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note that our approach does not require any additional guidance like classifier or classifier-free guidance to improve the image quality. We validate the proposed approach for three diverse tasks, viz., (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss has improved the perceptual quality across all these tasks in terms of both FID, MUSIQ score, and user evaluation.
</details>
<details>
<summary>摘要</summary>
Diffusion models have advanced generative AI significantly in terms of editing and creating naturalistic images. However, improving the quality of generated images efficiently is still a top priority. To address this, we propose a generic "naturalness" preserving loss function, called kurtosis concentration (KC) loss, which can be easily applied to any standard diffusion model pipeline to enhance image quality. Our motivation comes from the projected kurtosis concentration property of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. To retain the "naturalness" of the generated images, we enforce reducing the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. Note that our approach does not require any additional guidance like classifier or classifier-free guidance to improve the image quality. We validate the proposed approach for three diverse tasks, namely (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss has improved the perceptual quality across all these tasks in terms of both FID, MUSIQ score, and user evaluation.
</details></li>
</ul>
<hr>
<h2 id="Gradient-Map-Guided-Adaptive-Domain-Generalization-for-Cross-Modality-MRI-Segmentation"><a href="#Gradient-Map-Guided-Adaptive-Domain-Generalization-for-Cross-Modality-MRI-Segmentation" class="headerlink" title="Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation"></a>Gradient-Map-Guided Adaptive Domain Generalization for Cross Modality MRI Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09737">http://arxiv.org/abs/2311.09737</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/cuttle-fish-my/gm-guided-dg">https://github.com/cuttle-fish-my/gm-guided-dg</a></li>
<li>paper_authors: Bingnan Li, Zhitong Gao, Xuming He</li>
<li>for: 这个研究旨在提高计算机支持医学诊断的跨Modal MRI标本分类，以实现跨域数据收集和模型通用性。</li>
<li>methods: 我们提出了一个新的适应领域扩展框架，它结合了无学习交叉领域表示基于图像梯度地图和一种基于分类前置检测适应策略，以减少地方领域迁移。</li>
<li>results: 我们在两个多Modal MRI数据集上验证了我们的方法，包括六个跨Modal MRI标本分类任务。 在所有任务设置下，我们的方法一直超过竞争方法，并且在有限的训练数据下保持稳定的性能。<details>
<summary>Abstract</summary>
Cross-modal MRI segmentation is of great value for computer-aided medical diagnosis, enabling flexible data acquisition and model generalization. However, most existing methods have difficulty in handling local variations in domain shift and typically require a significant amount of data for training, which hinders their usage in practice. To address these problems, we propose a novel adaptive domain generalization framework, which integrates a learning-free cross-domain representation based on image gradient maps and a class prior-informed test-time adaptation strategy for mitigating local domain shift. We validate our approach on two multi-modal MRI datasets with six cross-modal segmentation tasks. Across all the task settings, our method consistently outperforms competing approaches and shows a stable performance even with limited training data.
</details>
<details>
<summary>摘要</summary>
跨模态MRI分割是医疗辅助诊断中的非常有价值的技术，它允许悬浮数据采集和模型通用化。然而，大多数现有方法在域Shift问题上困难处理本地变化，通常需要大量数据进行训练，这会限制它们在实践中的使用。为解决这些问题，我们提出了一种新的适应域通用化框架，该框架 integrate了一种无需学习的跨Domain表示基于图像梯度地图和一种基于类偏置的测试时适应策略，以mitigate本地域Shift问题。我们在两个多模态MRI数据集上进行了六个跨模态分割任务的验证。在所有任务设置下，我们的方法始终超越了竞争方法，并在有限的训练数据下保持稳定性。
</details></li>
</ul>
<hr>
<h2 id="MS-Former-Memory-Supported-Transformer-for-Weakly-Supervised-Change-Detection-with-Patch-Level-Annotations"><a href="#MS-Former-Memory-Supported-Transformer-for-Weakly-Supervised-Change-Detection-with-Patch-Level-Annotations" class="headerlink" title="MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations"></a>MS-Former: Memory-Supported Transformer for Weakly Supervised Change Detection with Patch-Level Annotations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09726">http://arxiv.org/abs/2311.09726</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/guanyuezhen/ms-former">https://github.com/guanyuezhen/ms-former</a></li>
<li>paper_authors: Zhenglai Li, Chang Tang, Xinwang Liu, Changdong Li, Xianju Li, Wei Zhang</li>
<li>for: 这 paper 是为了提出一种基于 patch-level 纪录的弱类标注下的变化检测方法。</li>
<li>methods: 该方法基于 transformer 架构，包括一个 bidirectional attention block (BAB) 和一个 patch-level supervision scheme (PSS)。BAB 通过从时间差特征中提取关于改变和不变区域的上下文信息，并将其存储在内存银行中。PSS 则使用 patch-level 纪录来引导网络学习有价值的知识，从而进一步提高表达能力。</li>
<li>results: 实验结果表明，该方法在三个标准测试集上达到了优秀的变化检测效果。<details>
<summary>Abstract</summary>
Fully supervised change detection methods have achieved significant advancements in performance, yet they depend severely on acquiring costly pixel-level labels. Considering that the patch-level annotations also contain abundant information corresponding to both changed and unchanged objects in bi-temporal images, an intuitive solution is to segment the changes with patch-level annotations. How to capture the semantic variations associated with the changed and unchanged regions from the patch-level annotations to obtain promising change results is the critical challenge for the weakly supervised change detection task. In this paper, we propose a memory-supported transformer (MS-Former), a novel framework consisting of a bi-directional attention block (BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised change detection with patch-level annotations. More specifically, the BAM captures contexts associated with the changed and unchanged regions from the temporal difference features to construct informative prototypes stored in the memory bank. On the other hand, the BAM extracts useful information from the prototypes as supplementary contexts to enhance the temporal difference features, thereby better distinguishing changed and unchanged regions. After that, the PSS guides the network learning valuable knowledge from the patch-level annotations, thus further elevating the performance. Experimental results on three benchmark datasets demonstrate the effectiveness of our proposed method in the change detection task. The demo code for our work will be publicly available at \url{https://github.com/guanyuezhen/MS-Former}.
</details>
<details>
<summary>摘要</summary>
具有全程监督改变方法在性能方面已经取得了显著进步，然而它们受到买到便宜的像素级标签的限制。考虑到bi-temporal图像中的块级注释也包含了改变和不改变对象之间的许多信息，因此一种直观的解决方案是将改变分割成块级注释。然而，如何从块级注释中捕捉改变和不改变区域之间的semantic变化，以获得出色的改变结果是critical挑战。在这篇论文中，我们提出了一种记忆支持的变换器（MS-Former），这是一种新的框架，包括一个bi-directional attention块（BAB）和一个块级监督方案（PSS），这些方案特地针对无监督改变检测任务。更加具体地说，BAB会从bi-temporal特征图中捕捉改变和不改变区域之间的上下文，并将其存储在内存银行中。然后，BAB会从内存银行中提取有用的信息，以增强bi-temporal特征图中的时间差特征，从而更好地 отличи改变和不改变区域。同时，PSS会引导网络学习从块级注释中得到有价值的知识，从而进一步提高性能。我们的实验结果表明，我们的提议方法在改变检测任务中具有出色的效果。我们将在 \url{https://github.com/guanyuezhen/MS-Former} 上公开发布我们的代码示例。
</details></li>
</ul>
<hr>
<h2 id="Now-and-Future-of-Artificial-Intelligence-based-Signet-Ring-Cell-Diagnosis-A-Survey"><a href="#Now-and-Future-of-Artificial-Intelligence-based-Signet-Ring-Cell-Diagnosis-A-Survey" class="headerlink" title="Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey"></a>Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10118">http://arxiv.org/abs/2311.10118</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhu Meng, Junhao Dong, Limei Guo, Fei Su, Guangxi Wang, Zhicheng Zhao</li>
<li>for: 本研究主要为了提供一份从2008年到8月2023年的深度学习驱动的筛 Cell（SRC）分析的综述，帮助无医学背景的研究人员更好地了解SRC的生物学特征和自动识别挑战，以及现有算法的表现和未来趋势。</li>
<li>methods: 本文分析了SRC分析中使用的代表性算法，并对它们进行了分类、检测和分 segmentation 的比较分析。</li>
<li>results: 本文发现了SRC分析领域中的一些问题和未解决的问题，并提出了未来研究的方向和趋势，以帮助研究人员更好地理解SRC的生物学特征和自动识别挑战，以及现有算法的表现和未来趋势。<details>
<summary>Abstract</summary>
Since signet ring cells (SRCs) are associated with high peripheral metastasis rate and dismal survival, they play an important role in determining surgical approaches and prognosis, while they are easily missed by even experienced pathologists. Although automatic diagnosis SRCs based on deep learning has received increasing attention to assist pathologists in improving the diagnostic efficiency and accuracy, the existing works have not been systematically overviewed, which hindered the evaluation of the gap between algorithms and clinical applications. In this paper, we provide a survey on SRC analysis driven by deep learning from 2008 to August 2023. Specifically, the biological characteristics of SRCs and the challenges of automatic identification are systemically summarized. Then, the representative algorithms are analyzed and compared via dividing them into classification, detection, and segmentation. Finally, for comprehensive consideration to the performance of existing methods and the requirements for clinical assistance, we discuss the open issues and future trends of SRC analysis. The retrospect research will help researchers in the related fields, particularly for who without medical science background not only to clearly find the outline of SRC analysis, but also gain the prospect of intelligent diagnosis, resulting in accelerating the practice and application of intelligent algorithms.
</details>
<details>
<summary>摘要</summary>
自 signet 环绕细胞（SRC）与高周边肿瘤率和减少生存率之间的关系，使得 SRC 在决定手术方法和诊断的重要作用。然而，经验 Pathologist 可能会扫描到这些细胞，尽管 automatic 诊断 SRC 基于深度学习已经收到了提高诊断效率和准确性的关注。在这篇文章中，我们提供了从 2008 年到 8 月 2023 年的 SRC 分析驱动深度学习的报告。specifically，我们系统地概述了 SRC 的生物特征和自动识别的挑战。然后，我们分析了代表性的算法，并将其分为分类、检测和分 segmentation 三个部分进行比较。最后，为了全面评估现有方法的性能和临床应用的需求，我们讨论了开放问题和未来趋势。这些研究将帮助相关领域的研究人员，特别是没有医学背景的研究人员，不仅能够清楚地了解 SRC 分析的大纲，而且能够获得智能诊断的前景，从而加速智能算法的实践和应用。
</details></li>
</ul>
<hr>
<h2 id="Robust-Contrastive-Learning-With-Theory-Guarantee"><a href="#Robust-Contrastive-Learning-With-Theory-Guarantee" class="headerlink" title="Robust Contrastive Learning With Theory Guarantee"></a>Robust Contrastive Learning With Theory Guarantee</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09671">http://arxiv.org/abs/2311.09671</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ngoc N. Tran, Lam Tran, Hoang Phan, Anh Bui, Tung Pham, Toan Tran, Dinh Phung, Trung Le</li>
<li>for: 这个论文的目的是探讨contrastive learning（CL）自我超vised训练方法中的不supervised预处理阶段是如何支持supervised预处理阶段的。</li>
<li>methods: 这个论文使用了一种分两个阶段的CL框架，首先从无标记数据中学习特征，然后使用这些特征来训练一个线性分类器。</li>
<li>results: 研究发现，在CL的第一个阶段中使用的不supervised损失函数对于在第二个阶段的supervised损失函数的提升有着重要的作用。同时，研究还发现了一些关键的组成部分在不supervised损失函数中，可以帮助提高supervised损失函数的稳定性和性能。<details>
<summary>Abstract</summary>
Contrastive learning (CL) is a self-supervised training paradigm that allows us to extract meaningful features without any label information. A typical CL framework is divided into two phases, where it first tries to learn the features from unlabelled data, and then uses those features to train a linear classifier with the labeled data. While a fair amount of existing theoretical works have analyzed how the unsupervised loss in the first phase can support the supervised loss in the second phase, none has examined the connection between the unsupervised loss and the robust supervised loss, which can shed light on how to construct an effective unsupervised loss for the first phase of CL. To fill this gap, our work develops rigorous theories to dissect and identify which components in the unsupervised loss can help improve the robust supervised loss and conduct proper experiments to verify our findings.
</details>
<details>
<summary>摘要</summary>
《对比学习（Contrastive Learning，CL）是一种自动标注训练方法，它允许我们从无标签数据中提取有意义的特征。一个典型的CL框架包括两个阶段，第一阶段是从无标签数据中学习特征，第二阶段是使用这些特征来训练一个线性分类器与标签数据进行训练。虽然一些现有的理论研究已经分析了如何在第一阶段中学习的无监督损失如何支持第二阶段的监督损失，但是没有研究过无监督损失与鲁棒监督损失之间的连接，这可以推熔到如何构建有效的无监督损失。为了填补这个差距，我们的工作开发了准确的理论来分析无监督损失中的各个组件是否能够提高鲁棒监督损失，并进行了相应的实验验证。》Note: Please keep in mind that the translation is done by a machine and may not be perfect. If you have any further questions or need any adjustments, please let me know.
</details></li>
</ul>
<hr>
<h2 id="Multi-View-Spectrogram-Transformer-for-Respiratory-Sound-Classification"><a href="#Multi-View-Spectrogram-Transformer-for-Respiratory-Sound-Classification" class="headerlink" title="Multi-View Spectrogram Transformer for Respiratory Sound Classification"></a>Multi-View Spectrogram Transformer for Respiratory Sound Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09655">http://arxiv.org/abs/2311.09655</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wentao He, Yuchen Yan, Jianfeng Ren, Ruibin Bai, Xudong Jiang</li>
<li>for: 这篇论文旨在应用深度神经网络来分类呼吸声音。</li>
<li>methods: 该论文提出了一种多观点spectrogram对称trasformer（MVST），将不同大小的mel-spectrogram分割为多个视角的音频元件，然后使用对称转构器将这些元件转换为自适应特征。</li>
<li>results: 实验结果显示，该提出的MVST方法较前一项方法有更好的表现，在ICBHI数据集上分类呼吸声音的任务中。<details>
<summary>Abstract</summary>
Deep neural networks have been applied to audio spectrograms for respiratory sound classification. Existing models often treat the spectrogram as a synthetic image while overlooking its physical characteristics. In this paper, a Multi-View Spectrogram Transformer (MVST) is proposed to embed different views of time-frequency characteristics into the vision transformer. Specifically, the proposed MVST splits the mel-spectrogram into different sized patches, representing the multi-view acoustic elements of a respiratory sound. These patches and positional embeddings are then fed into transformer encoders to extract the attentional information among patches through a self-attention mechanism. Finally, a gated fusion scheme is designed to automatically weigh the multi-view features to highlight the best one in a specific scenario. Experimental results on the ICBHI dataset demonstrate that the proposed MVST significantly outperforms state-of-the-art methods for classifying respiratory sounds.
</details>
<details>
<summary>摘要</summary>
深度神经网络已经应用于音频spectrogram中的呼吸音分类。现有模型 oftentimes treat spectrogram as synthetic image，忽略其物理特征。本文提出了 Multi-View Spectrogram Transformer (MVST)，用于嵌入不同视图的时间频谱特征。具体来说，提案的MVST将mel-spectrogram分割成不同大小的patches，表示呼吸音的多视图听音元件。这些patches和位域嵌入被Feed into transformer encoder中，通过自我注意机制提取多视图特征之间的关注信息。最后，设计了一种权重权重调整方案，以自动将多视图特征相互权重，以便在特定场景下高亮最佳的一个视图特征。实验结果表明，提案的MVST在ICBHI数据集上明显超过了现有方法，用于分类呼吸音。
</details></li>
</ul>
<hr>
<h2 id="Improved-TokenPose-with-Sparsity"><a href="#Improved-TokenPose-with-Sparsity" class="headerlink" title="Improved TokenPose with Sparsity"></a>Improved TokenPose with Sparsity</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09653">http://arxiv.org/abs/2311.09653</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anning Li</li>
<li>for: 人体姿态估计</li>
<li>methods: 使用简单的隐藏状态和自动重要性来降低计算复杂度，并通过对键点和视觉特征进行精细化来提高人体姿态估计的精度。</li>
<li>results: 在MPII数据集上实现新的状态艺术纪录，并证明了方法的可行性。<details>
<summary>Abstract</summary>
Over the past few years, the vision transformer and its various forms have gained significance in human pose estimation. By treating image patches as tokens, transformers can capture global relationships wisely, estimate the keypoint tokens by leveraging the visual tokens, and recognize the posture of the human body. Nevertheless, global attention is computationally demanding, which poses a challenge for scaling up transformer-based methods to high-resolution features. In this paper, we introduce sparsity in both keypoint token attention and visual token attention to improve human pose estimation. Experimental results on the MPII dataset demonstrate that our model has a higher level of accuracy and proved the feasibility of the method, achieving new state-of-the-art results. The idea can also provide references for other transformer-based models.
</details>
<details>
<summary>摘要</summary>
在过去几年，视力变换器和其多种形式在人体姿态估计中具有重要意义。通过将图像块看作为符号，变换器可以聪明地捕捉全局关系，根据视觉符号来估计关键点符号，并识别人体姿态。然而，全球注意力计算具有挑战性，这会对基于变换器的方法的扩大到高分辨率特征造成挑战。在这篇论文中，我们引入了缺失在关键点符号注意力和视觉符号注意力中，以提高人体姿态估计的精度。实验结果表明，我们的模型在MPII数据集上达到了新的州OF-the-artResult，并证明了方法的可行性。这个想法还可以作为其他基于变换器的模型的参考。
</details></li>
</ul>
<hr>
<h2 id="Event-based-Motion-Robust-Accurate-Shape-Estimation-for-Mixed-Reflectance-Scenes"><a href="#Event-based-Motion-Robust-Accurate-Shape-Estimation-for-Mixed-Reflectance-Scenes" class="headerlink" title="Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes"></a>Event-based Motion-Robust Accurate Shape Estimation for Mixed Reflectance Scenes</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09652">http://arxiv.org/abs/2311.09652</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aniket Dashpute, Jiazhang Wang, James Taylor, Oliver Cossairt, Ashok Veeraraghavan, Florian Willomitzer</li>
<li>for: fast and motion-robust 3D imaging of mixed reflectance scenes</li>
<li>methods: event-based structured light system, epipolar constraints, triangulation, deflectometry</li>
<li>results: high accuracy (&lt;500μm) and fast capture speed (14Hz or 250Hz) for mixed reflectance scenes<details>
<summary>Abstract</summary>
Event-based structured light systems have recently been introduced as an exciting alternative to conventional frame-based triangulation systems for the 3D measurements of diffuse surfaces. Important benefits include the fast capture speed and the high dynamic range provided by the event camera - albeit at the cost of lower data quality. So far, both low-accuracy event-based as well as high-accuracy frame-based 3D imaging systems are tailored to a specific surface type, such as diffuse or specular, and can not be used for a broader class of object surfaces ("mixed reflectance scenes"). In this paper, we present a novel event-based structured light system that enables fast 3D imaging of mixed reflectance scenes with high accuracy. On the captured events, we use epipolar constraints that intrinsically enable decomposing the measured reflections into diffuse, two-bounce specular, and other multi-bounce reflections. The diffuse objects in the scene are reconstructed using triangulation. Eventually, the reconstructed diffuse scene parts are used as a "display" to evaluate the specular scene parts via deflectometry. This novel procedure allows us to use the entire scene as a virtual screen, using only a scanning laser and an event camera. The resulting system achieves fast and motion-robust (14Hz) reconstructions of mixed reflectance scenes with < 500 $\mu$m accuracy. Moreover, we introduce a "superfast" capture mode (250Hz) for the 3D measurement of diffuse scenes.
</details>
<details>
<summary>摘要</summary>
现代事件驱动的探讨光系统已经被引入为diffuse表面三维测量的新型代替方案，具有快速捕捉速度和高动态范围。然而，这些系统的数据质量相对较低，而且仅适用于特定表面类型（diffuse或speculative），无法应用于更广泛的物体表面（"混合反射场景"）。在这篇论文中，我们提出了一种新的事件驱动探讨光系统，可以快速地三维测量混合反射场景，并且具有高准确性。在捕捉到的事件上，我们使用epipolar约束，可以自动分解测量的反射为diffuse、两次反射和其他多次反射。diffuse对象在场景中被重建，并用三角测量来重建。最后，重建的diffuse场景部分被用作"显示"，以评估speculative场景部分via折射。这种新的程序让我们可以使用扫描 láser和事件相机来扫描整个场景，并且实现快速和运动稳定（14Hz）的重建混合反射场景，具有<500μm的准确性。此外，我们还介绍了"超快"捕捉模式（250Hz），用于三维测量diffuse场景。
</details></li>
</ul>
<hr>
<h2 id="Reconstructing-Continuous-Light-Field-From-Single-Coded-Image"><a href="#Reconstructing-Continuous-Light-Field-From-Single-Coded-Image" class="headerlink" title="Reconstructing Continuous Light Field From Single Coded Image"></a>Reconstructing Continuous Light Field From Single Coded Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09646">http://arxiv.org/abs/2311.09646</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuya Ishikawa, Keita Takahashi, Chihiro Tsutake, Toshiaki Fujii</li>
<li>for:  reconstruction of continuous light fields of a target scene from a single observed image</li>
<li>methods:  joint aperture-exposure coding and neural radiance field (NeRF)</li>
<li>results:  accurate and efficient reconstruction of continuous light fields without test time optimization, bridging the gap between camera design and neural rendering.Here’s the full text in Simplified Chinese:</li>
<li>for: 这个研究旨在从单个观察图像中重建目标场景的连续光场。</li>
<li>methods: 这个方法结合了共同的开口-曝光编码和神经辐射场（NeRF）来实现视觉合成。</li>
<li>results: 这个方法可以高效地和高质量地重建连续光场，不需要任何测试时间优化。这是我们知道的第一个将摄像头设计和神经渲染相结合的研究。<details>
<summary>Abstract</summary>
We propose a method for reconstructing a continuous light field of a target scene from a single observed image. Our method takes the best of two worlds: joint aperture-exposure coding for compressive light-field acquisition, and a neural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding implemented in a camera enables effective embedding of 3-D scene information into an observed image, but in previous works, it was used only for reconstructing discretized light-field views. NeRF-based neural rendering enables high quality view synthesis of a 3-D scene from continuous viewpoints, but when only a single image is given as the input, it struggles to achieve satisfactory quality. Our method integrates these two techniques into an efficient and end-to-end trainable pipeline. Trained on a wide variety of scenes, our method can reconstruct continuous light fields accurately and efficiently without any test time optimization. To our knowledge, this is the first work to bridge two worlds: camera design for efficiently acquiring 3-D information and neural rendering.
</details>
<details>
<summary>摘要</summary>
我们提出了一种方法，可以从单个观察到的图像中重建连续的光场场景。我们的方法结合了两种世界的优点：joint aperature-exposure coding for compressive light-field acquisition,和基于神经辐射场（NeRF）的视觉合成。 joint aperature-exposure coding在摄像头中实现了有效地嵌入3D场景信息到观察到的图像中，但在前一些工作中，它只用于重建精确的光场观察角度。基于NeRF的神经渲染可以高质量地合成3D场景的视角，但当只有单个图像作为输入时，它很难达到满意的质量。我们的方法将这两种技术集成成一个高效、端到端训练可以的管道。我们在各种场景下训练了这种方法，可以高效地和高质量地重建连续的光场场景，无需任何测试时间优化。到我们所知，这是第一次将摄像头设计用于高效地获取3D信息和神经渲染相结合。
</details></li>
</ul>
<hr>
<h2 id="Weakly-Supervised-Anomaly-Detection-for-Chest-X-Ray-Image"><a href="#Weakly-Supervised-Anomaly-Detection-for-Chest-X-Ray-Image" class="headerlink" title="Weakly Supervised Anomaly Detection for Chest X-Ray Image"></a>Weakly Supervised Anomaly Detection for Chest X-Ray Image</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09642">http://arxiv.org/abs/2311.09642</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/iamcuriosity/wscxr">https://github.com/iamcuriosity/wscxr</a></li>
<li>paper_authors: Haoqi Ni, Ximiao Zhang, Min Xu, Ning Lang, Xiuzhuang Zhou</li>
<li>for: 本研究旨在提出一种基于weakly supervised learning的额外肺X光图像异常检测方法，以便在临床应用中更好地检测肺疾病。</li>
<li>methods: 本方法首先构建了normal和异常图像特征集合，然后通过异常特征挖掘来除去正常区域特征，以便全面利用疾病区域的珍贵特征。此外，本方法还使用了线性混合策略来增强异常检测器的训练。</li>
<li>results:  experiments表明，本方法在两个肺X光图像 Dataset上显示了效果。<details>
<summary>Abstract</summary>
Chest X-Ray (CXR) examination is a common method for assessing thoracic diseases in clinical applications. While recent advances in deep learning have enhanced the significance of visual analysis for CXR anomaly detection, current methods often miss key cues in anomaly images crucial for identifying disease regions, as they predominantly rely on unsupervised training with normal images. This letter focuses on a more practical setup in which few-shot anomaly images with only image-level labels are available during training. For this purpose, we propose WSCXR, a weakly supervised anomaly detection framework for CXR. WSCXR firstly constructs sets of normal and anomaly image features respectively. It then refines the anomaly image features by eliminating normal region features through anomaly feature mining, thus fully leveraging the scarce yet crucial features of diseased areas. Additionally, WSCXR employs a linear mixing strategy to augment the anomaly features, facilitating the training of anomaly detector with few-shot anomaly images. Experiments on two CXR datasets demonstrate the effectiveness of our approach.
</details>
<details>
<summary>摘要</summary>
骨肋X射影（CXR）检测是诊断 thoracic 疾病的常用方法。Recent advances in deep learning 使得视觉分析在 CXR 畸形检测中具有更大的重要性，但现有方法通常会遗漏疾病区域中的关键提示，因为它们主要依靠 normal 图像进行无监督训练。这封信件关注一种更实用的设置，在训练过程中仅有几张畸形图像和图像水平标签可用。为此，我们提出了 WSCXR，一种弱型监督畸形检测框架 для CXR。WSCXR 首先构建 normal 和畸形图像特征集，然后通过畸形特征挖掘，减少疾病区域特征，从而全面利用疾病区域中的珍贵特征。此外，WSCXR 采用了线性混合策略，以增强畸形特征的训练，使用几张畸形图像进行检测。实验表明，我们的方法有效地检测 CXR 畸形。
</details></li>
</ul>
<hr>
<h2 id="On-the-Quantification-of-Image-Reconstruction-Uncertainty-without-Training-Data"><a href="#On-the-Quantification-of-Image-Reconstruction-Uncertainty-without-Training-Data" class="headerlink" title="On the Quantification of Image Reconstruction Uncertainty without Training Data"></a>On the Quantification of Image Reconstruction Uncertainty without Training Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09639">http://arxiv.org/abs/2311.09639</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sirui Bi, Victor Fung, Jiaxin Zhang</li>
<li>for:  This paper focuses on developing a deep variational framework for image reconstruction and uncertainty estimation in computational imaging.</li>
<li>methods: The proposed method leverages a deep generative model to learn an approximate posterior distribution for image reconstruction uncertainty, using a flow-based model and gradient boosting for robustness and expressiveness.</li>
<li>results: The method is validated on several benchmark tasks and two real-world applications, demonstrating reliable and high-quality image reconstruction with robust uncertainty estimation.<details>
<summary>Abstract</summary>
Computational imaging plays a pivotal role in determining hidden information from sparse measurements. A robust inverse solver is crucial to fully characterize the uncertainty induced by these measurements, as it allows for the estimation of the complete posterior of unrecoverable targets. This, in turn, facilitates a probabilistic interpretation of observational data for decision-making. In this study, we propose a deep variational framework that leverages a deep generative model to learn an approximate posterior distribution to effectively quantify image reconstruction uncertainty without the need for training data. We parameterize the target posterior using a flow-based model and minimize their Kullback-Leibler (KL) divergence to achieve accurate uncertainty estimation. To bolster stability, we introduce a robust flow-based model with bi-directional regularization and enhance expressivity through gradient boosting. Additionally, we incorporate a space-filling design to achieve substantial variance reduction on both latent prior space and target posterior space. We validate our method on several benchmark tasks and two real-world applications, namely fastMRI and black hole image reconstruction. Our results indicate that our method provides reliable and high-quality image reconstruction with robust uncertainty estimation.
</details>
<details>
<summary>摘要</summary>
计算成像在捕捉隐藏信息方面发挥关键作用，从稀缺测量中推断出隐藏信息的不确定性需要一个坚固的 inverse solver。这样可以全面描述测量过程中induced的uncertainty，并且使得观察数据的概率解释变得可能，从而帮助做出决策。在这个研究中，我们提出了一个深度变量框架，该框架利用深度生成模型来学习一个近似 posterior distribution，以便有效地量ify image reconstruction uncertainty，无需训练数据。我们使用流基本模型来参数化目标 posterior，并通过最小化其Kullback-Leibler（KL）偏度来实现准确的 uncertainty estimation。为了增强稳定性，我们引入了bi-directional regularization和扩展表达能力通过梯度批处理。此外，我们采用了填充设计，以实现在latent prior空间和目标 posterior空间上的重要variance reduction。我们在多个benchmark任务和两个实际应用中，即fastMRI和黑洞图像重建中 validate our方法，结果表明我们的方法可以提供可靠和高质量的图像重建，同时也可以提供准确的 uncertainty estimation。
</details></li>
</ul>
<hr>
<h2 id="DECDM-Document-Enhancement-using-Cycle-Consistent-Diffusion-Models"><a href="#DECDM-Document-Enhancement-using-Cycle-Consistent-Diffusion-Models" class="headerlink" title="DECDM: Document Enhancement using Cycle-Consistent Diffusion Models"></a>DECDM: Document Enhancement using Cycle-Consistent Diffusion Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09625">http://arxiv.org/abs/2311.09625</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaxin Zhang, Joy Rimchala, Lalla Mouatadid, Kamalika Das, Sricharan Kumar</li>
<li>for: 提高文档像素质量，以提高自动文档处理和文档智能。</li>
<li>methods: 基于Diffusion模型的终端到终端文档图像翻译方法，无需同时训练源和目标模型，可以应用到其他域对。</li>
<li>results: 与状态艺术方法相比，DECDM在多种 sintetic数据和benchmark datasets上表现出色，可以 Quantitatively和Qualitatively提高文档图像质量。<details>
<summary>Abstract</summary>
The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and {\color{black}shadow} removal, and demonstrate the superiority of performance quantitatively and qualitatively.
</details>
<details>
<summary>摘要</summary>
表现强大的光学字符识别（OCR）功能受到文档图像质量的限制，这对于自动文档处理和文档智能来说非常重要。然而，现有的文档增强方法通常需要监督数据对，这会导致数据分离和隐私保护的问题，使得这些方法难以适应新的域对。为解决这些问题，我们提出了DECDM，一种基于傅立叶分布模型的终端文档图像翻译方法。DECDM在无监督的情况下独立地训练源（噪音输入）和目标（清晰输出）模型，因此可以应用到其他对。DECDM在一个dataset上单独训练，不需要同时扫描两个dataset，从而有效地保护数据隐私。我们还介绍了一些简单的数据扩展策略，以保持字符形态的恒久性 durante la traducción。我们与状态之前的方法进行比较，并在多个合成数据和标准 benchmark datasets上进行评估，如文档噪音去除和阴影去除。我们的实验结果表明DECDM在量和质量上具有显著的优势。
</details></li>
</ul>
<hr>
<h2 id="Apoptosis-classification-using-attention-based-spatio-temporal-graph-convolution-neural-network"><a href="#Apoptosis-classification-using-attention-based-spatio-temporal-graph-convolution-neural-network" class="headerlink" title="Apoptosis classification using attention based spatio temporal graph convolution neural network"></a>Apoptosis classification using attention based spatio temporal graph convolution neural network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09623">http://arxiv.org/abs/2311.09623</a></li>
<li>repo_url: None</li>
<li>paper_authors: Akash Awasthi</li>
<li>for: 本研究旨在提出一种基于注意力图像抽象的图像缓冲扩充网络，用于精准地分类细胞死亡。</li>
<li>methods: 该方法使用注意力图像抽象网络，考虑多个目标细胞之间的交互关系，并在视频序列中模型每个时刻点的关系。</li>
<li>results: 该方法可以准确地分类细胞死亡，同时考虑空间和时间关系。<details>
<summary>Abstract</summary>
Accurate classification of apoptosis plays an important role in cell biology research. There are many state-of-the-art approaches which use deep CNNs to perform the apoptosis classification but these approaches do not account for the cell interaction. Our paper proposes the Attention Graph spatio-temporal graph convolutional network to classify the cell death based on the target cells in the video. This method considers the interaction of multiple target cells at each time stamp. We model the whole video sequence as a set of graphs and classify the target cell in the video as dead or alive. Our method encounters both spatial and temporal relationships.
</details>
<details>
<summary>摘要</summary>
精准的细胞死亡分类在细胞生物研究中扮演着重要的角色。目前有许多先进的方法使用深度卷积神经网络进行细胞死亡分类，但这些方法不考虑细胞之间的互动。我们的论文提出了注意力图像空间时间卷积神经网络，用于基于目标细胞的视频中细胞死亡分类。这种方法考虑了每个时间戳的多个目标细胞之间的互动关系。我们将整个视频序列视为一系列图像，并将目标细胞在视频中分类为死亡或活着。我们的方法考虑了空间和时间关系。
</details></li>
</ul>
<hr>
<h2 id="Wildfire-Smoke-Detection-with-Cross-Contrast-Patch-Embedding"><a href="#Wildfire-Smoke-Detection-with-Cross-Contrast-Patch-Embedding" class="headerlink" title="Wildfire Smoke Detection with Cross Contrast Patch Embedding"></a>Wildfire Smoke Detection with Cross Contrast Patch Embedding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10116">http://arxiv.org/abs/2311.10116</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chong Wang, Cheng Xu, Adeel Akram, Zhilin Shan, Qixing Zhang</li>
<li>for: 本研究旨在提高Transformer基于深度网络的野火识别性能，特别是提高Transformer对烟雾特征的抽取能力。</li>
<li>methods: 本研究提出了 Cross Contrast Patch Embedding（CCPE）模块和Separable Negative Sampling Mechanism（SNSM），以提高网络对烟雾特征的抽取和识别性能。</li>
<li>results: 对RealFire Test dataset进行了广泛测试和评估，与基线检测模型相比，本研究的方法具有显著的性能提升。<details>
<summary>Abstract</summary>
The Transformer-based deep networks have increasingly shown significant advantages over CNNs. Some existing work has applied it in the field of wildfire recognition or detection. However, we observed that the vanilla Transformer is not friendly for extracting smoke features. Because low-level information such as color, transparency and texture is very important for smoke recognition, and transformer pays more attention to the semantic relevance between middle- or high-level features, and is not sensitive to the subtle changes of low-level features along the space. To solve this problem, we propose the Cross Contrast Patch Embedding(CCPE) module based on the Swin Transformer, which uses the multi-scales spatial frequency contrast information in both vertical and horizontal directions to improve the discrimination of the network on the underlying details. The fuzzy boundary of smoke makes the positive and negative label assignment for instances in a dilemma, which is another challenge for wildfires detection. To solve this problem, a Separable Negative Sampling Mechanism(SNSM) is proposed. By using two different negative instance sampling strategies on positive images and negative images respectively, the problem of supervision signal confusion caused by label diversity in the process of network training is alleviated. This paper also releases the RealFire Test, the largest real wildfire test set so far, to evaluate the proposed method and promote future research. It contains 50,535 images from 3,649 video clips. The proposed method has been extensively tested and evaluated on RealFire Test dataset, and has a significant performance improvement compared with the baseline detection models.
</details>
<details>
<summary>摘要</summary>
《Transformer基于深度网络在野火识别方面的应用》 Introduction:现在的研究中，Transformer基于深度网络已经显示出了对于CNNs的明显优势。然而，我们发现了Transformer不适合提取烟雾特征。因为烟雾识别中低级信息如颜色、透明度和文本ure是非常重要的，而Transformer更关注中间或高级特征之间的 semantic relevance，并不敏感于空间方向中的细微变化。为解决这个问题，我们提出了基于Swin Transformer的 Cross Contrast Patch Embedding（CCPE）模块，利用多个比例的空间频率对比信息来提高网络对下面详细信息的推断。另外，野火检测中烟雾的模糊边界使得实例的正负标签分配困难，这也是一个挑战。为解决这个问题，我们提出了分解负采样机制（SNSM）。通过在正例图像和负例图像上采用不同的负采样策略，使得网络训练过程中的监督信号混乱问题得到了缓解。This paper also releases the RealFire Test, the largest real wildfire test set so far, to evaluate the proposed method and promote future research. It contains 50,535 images from 3,649 video clips. The proposed method has been extensively tested and evaluated on RealFire Test dataset, and has a significant performance improvement compared with the baseline detection models.
</details></li>
</ul>
<hr>
<h2 id="Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans"><a href="#Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans" class="headerlink" title="Multi-Task Learning Approach for Unified Biometric Estimation from Fetal Ultrasound Anomaly Scans"></a>Multi-Task Learning Approach for Unified Biometric Estimation from Fetal Ultrasound Anomaly Scans</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09607">http://arxiv.org/abs/2311.09607</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans">https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans</a></li>
<li>paper_authors: Mohammad Areeb Qazi, Mohammed Talha Alam, Ibrahim Almakky, Werner Gerhard Diehl, Leanne Bricker, Mohammad Yaqub</li>
<li>for: The paper is written for estimating fetal biometry parameters from ultrasound images, which is crucial for evaluating fetal growth, monitoring health, and identifying potential complications.</li>
<li>methods: The paper proposes a multi-task learning approach that combines classification and segmentation to estimate fetal biometrics. The approach uses a U-Net architecture with an added classification head, and leverages a weighted joint classification and segmentation loss function to train the model.</li>
<li>results: The paper achieves a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44 mm on abdomen circumference, and 1.10 mm on femur length with a classification accuracy of 99.91% on a dataset of fetal ultrasound images.Here’s the information in Simplified Chinese text:</li>
<li>for: 本研究是为了从ultrasound图像中计算胎儿生长指标，这是评估胎儿生长、监测健康和识别潜在问题的关键。</li>
<li>methods: 本研究提出了一种多任务学习方法，即将分类和分割结合在一起，以便从ultrasound图像中计算胎儿生长指标。该方法使用了U-Net架构，并添加了一个分类头，以便在训练过程中使用加权共同分类和分割损失函数。</li>
<li>results: 本研究实现了head圈 circumference的平均绝对误差（MAE）为1.08 mm， Abdomen circumference的MAE为1.44 mm， femur length的MAE为1.10 mm，并达到了99.91%的分类精度在一个 dataset of fetal ultrasound images 中。<details>
<summary>Abstract</summary>
Precise estimation of fetal biometry parameters from ultrasound images is vital for evaluating fetal growth, monitoring health, and identifying potential complications reliably. However, the automated computerized segmentation of the fetal head, abdomen, and femur from ultrasound images, along with the subsequent measurement of fetal biometrics, remains challenging. In this work, we propose a multi-task learning approach to classify the region into head, abdomen and femur as well as estimate the associated parameters. We were able to achieve a mean absolute error (MAE) of 1.08 mm on head circumference, 1.44 mm on abdomen circumference and 1.10 mm on femur length with a classification accuracy of 99.91\% on a dataset of fetal Ultrasound images. To achieve this, we leverage a weighted joint classification and segmentation loss function to train a U-Net architecture with an added classification head. The code can be accessed through \href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github}
</details>
<details>
<summary>摘要</summary>
准确估算胎儿生长指标从ultrasound图像是诊断胎儿增长、监测健康和识别问题的关键。然而，通过计算器自动分割ultrasound图像中的胎儿头、腹部和股骨，以及其后的胎儿生长指标的测量，仍然是一项挑战。在这种工作中，我们提议一种多任务学习方法，通过分类区域为头、腹部和股骨，同时估算相关参数。我们在一个胎儿ultrasound图像集上实现了mean absolute error（MAE）为1.08毫米的头圈 circumference，1.44毫米的腹部 circumference和1.10毫米的股骨长度，同时实现了99.91%的分类精度。为达到这一点，我们利用了一种权重加权的联合分类和分割损失函数，用于训练一个U-Net架构，并添加了一个分类头。代码可以通过\href{https://github.com/BioMedIA-MBZUAI/Multi-Task-Learning-Approach-for-Unified-Biometric-Estimation-from-Fetal-Ultrasound-Anomaly-Scans.git}{\texttt{Github} [
</details></li>
</ul>
<hr>
<h2 id="Gradual-Source-Domain-Expansion-for-Unsupervised-Domain-Adaptation"><a href="#Gradual-Source-Domain-Expansion-for-Unsupervised-Domain-Adaptation" class="headerlink" title="Gradual Source Domain Expansion for Unsupervised Domain Adaptation"></a>Gradual Source Domain Expansion for Unsupervised Domain Adaptation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09599">http://arxiv.org/abs/2311.09599</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ThomasWestfechtel/GSDE">https://github.com/ThomasWestfechtel/GSDE</a></li>
<li>paper_authors: Thomas Westfechtel, Hao-Wei Yeh, Dexuan Zhang, Tatsuya Harada</li>
<li>for:  overcome the need for a large labeled dataset in unsupervised domain adaptation</li>
<li>methods:  gradual source domain expansion (GSDE) algorithm, training the UDA task several times from scratch with target data expansion</li>
<li>results:  outperform state-of-the-art methods on three benchmarks (Office-31, OfficeHome, and DomainNet) and improve the accuracy of a variety of different state-of-the-art UDA approaches.Here’s the format you requested:</li>
<li>for: &lt;what are the paper written for?&gt;</li>
<li>methods: &lt;what methods the paper use?&gt;</li>
<li>results: &lt;what results the paper get?&gt;I hope that helps!<details>
<summary>Abstract</summary>
Unsupervised domain adaptation (UDA) tries to overcome the need for a large labeled dataset by transferring knowledge from a source dataset, with lots of labeled data, to a target dataset, that has no labeled data. Since there are no labels in the target domain, early misalignment might propagate into the later stages and lead to an error build-up. In order to overcome this problem, we propose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA task several times from scratch, each time reinitializing the network weights, but each time expands the source dataset with target data. In particular, the highest-scoring target data of the previous run are employed as pseudo-source samples with their respective pseudo-label. Using this strategy, the pseudo-source samples induce knowledge extracted from the previous run directly from the start of the new training. This helps align the two domains better, especially in the early training epochs. In this study, we first introduce a strong baseline network and apply our GSDE strategy to it. We conduct experiments and ablation studies on three benchmarks (Office-31, OfficeHome, and DomainNet) and outperform state-of-the-art methods. We further show that the proposed GSDE strategy can improve the accuracy of a variety of different state-of-the-art UDA approaches.
</details>
<details>
<summary>摘要</summary>
Unsupervised domain adaptation (UDA) 尝试使用源数据集中具有很多标签数据的知识来推导目标数据集，该数据集没有标签。然而，在目标领域中的早期不一致可能会导致错误堆积。为解决这个问题，我们提出了慢步源领域扩展（GSDE）算法。GSDE 在 UDA 任务上进行多次从零开始训练，每次重新初始化网络权重，但每次扩展源数据集以包括目标数据。具体来说，上一轮最高分的目标数据被用作 Pseudo-source 样本，与其它 Pseudo-source 样本一起，直接从上一轮训练中提取了知识。这种策略可以更好地对两个领域进行对应，尤其是在训练的早期。在这个研究中，我们首先提出了一个强大的基线网络，然后应用我们的 GSDE 策略来改进其性能。我们在 Office-31、OfficeHome 和 DomainNet 三个标准测试集上进行了实验和剖析研究，并超越了当前最佳方法。此外，我们还证明了我们的 GSDE 策略可以提高多种不同的状态流行 UDA 方法的准确率。
</details></li>
</ul>
<hr>
<h2 id="MARformer-An-Efficient-Metal-Artifact-Reduction-Transformer-for-Dental-CBCT-Images"><a href="#MARformer-An-Efficient-Metal-Artifact-Reduction-Transformer-for-Dental-CBCT-Images" class="headerlink" title="MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images"></a>MARformer: An Efficient Metal Artifact Reduction Transformer for Dental CBCT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09590">http://arxiv.org/abs/2311.09590</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Shi, Jun Xu, Dinggang Shen</li>
<li>for: 针对受到金属artefacts干扰的 dental CBCT 图像，提出了一种高效的 Transformer 来实现金属artefacts 减少 (MAR)。</li>
<li>methods: 提出了一种基于 globally 相似结构的 Dimension-Reduced Self-Attention (DRSA) 模块，以及一种基于 Patch-wise Perceptive Feed Forward Network (P2FFN) 的 fine-grained  восстановление模块。</li>
<li>results: 对 dental CBCT 图像进行了 Synthetic 和实际的 metal artefacts 测试，结果显示，我们的 MARformer 高效，并且超过了之前的 MAR 方法和两种 Restoration Transformers。<details>
<summary>Abstract</summary>
Cone Beam Computed Tomography (CBCT) plays a key role in dental diagnosis and surgery. However, the metal teeth implants could bring annoying metal artifacts during the CBCT imaging process, interfering diagnosis and downstream processing such as tooth segmentation. In this paper, we develop an efficient Transformer to perform metal artifacts reduction (MAR) from dental CBCT images. The proposed MAR Transformer (MARformer) reduces computation complexity in the multihead self-attention by a new Dimension-Reduced Self-Attention (DRSA) module, based on that the CBCT images have globally similar structure. A Patch-wise Perceptive Feed Forward Network (P2FFN) is also proposed to perceive local image information for fine-grained restoration. Experimental results on CBCT images with synthetic and real-world metal artifacts show that our MARformer is efficient and outperforms previous MAR methods and two restoration Transformers.
</details>
<details>
<summary>摘要</summary>
cone beam computed tomography (CBCT) 在 dental 诊断和手术中发挥关键作用，但是 metal  зуб钻Implant 可能会在 CBCT 图像处理过程中引入干扰性的 metal  artifacts，影响诊断和下游处理，如 Tooth 分 segmentation。在这篇论文中，我们开发了一种高效的 transformer 来实现 dental CBCT 图像中的 metal artifacts 减少 (MAR)。我们提出的 MAR transformer （MARformer）通过一种新的 Dimension-Reduced Self-Attention（DRSA）模块，基于 CBCT 图像的全球相似结构，来降低计算复杂性。此外，我们还提出了一种 Patch-wise Perceptive Feed Forward Network（P2FFN）来捕捉本地图像信息，进行细化修复。实验结果表明，我们的 MARformer 高效，并比前期 MAR 方法和两种修复 transformer 高效。
</details></li>
</ul>
<hr>
<h2 id="3D-Paintbrush-Local-Stylization-of-3D-Shapes-with-Cascaded-Score-Distillation"><a href="#3D-Paintbrush-Local-Stylization-of-3D-Shapes-with-Cascaded-Score-Distillation" class="headerlink" title="3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation"></a>3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09571">http://arxiv.org/abs/2311.09571</a></li>
<li>repo_url: None</li>
<li>paper_authors: Dale Decatur, Itai Lang, Kfir Aberman, Rana Hanocka</li>
<li>for: 本研究开发了一种自动填充 mesh 的本地 semantic 区域 texture 技术，通过文本描述进行操作。</li>
<li>methods: 本方法直接操作 mesh，生成可以与标准 графіcs 管线整合的 texture map。同时生成本地化MAP和 texture map，以实现它们之间的融合。使用多个阶层的填充模型来监督本地编辑技术，以提高细节和分辨率。</li>
<li>results: 本研究能够对不同类别的 shapes 进行本地 texture，并且可以控制 texture 的细节和全球理解。实验页面：<a target="_blank" rel="noopener" href="https://threedle.github.io/3d-paintbrush">https://threedle.github.io/3d-paintbrush</a><details>
<summary>Abstract</summary>
In this work we develop 3D Paintbrush, a technique for automatically texturing local semantic regions on meshes via text descriptions. Our method is designed to operate directly on meshes, producing texture maps which seamlessly integrate into standard graphics pipelines. We opt to simultaneously produce a localization map (to specify the edit region) and a texture map which conforms to it. This synergistic approach improves the quality of both the localization and the stylization. To enhance the details and resolution of the textured area, we leverage multiple stages of a cascaded diffusion model to supervise our local editing technique with generative priors learned from images at different resolutions. Our technique, referred to as Cascaded Score Distillation (CSD), simultaneously distills scores at multiple resolutions in a cascaded fashion, enabling control over both the granularity and global understanding of the supervision. We demonstrate the effectiveness of 3D Paintbrush to locally texture a variety of shapes within different semantic regions. Project page: https://threedle.github.io/3d-paintbrush
</details>
<details>
<summary>摘要</summary>
在这个工作中，我们开发了3D涂刷技术，它可以通过文本描述自动地给mesh中的本地semantic区域添加文本ure。我们的方法直接操作mesh，生成的текстура映射可以衔接到标准图形管道中。我们同时生成了localization map（用于specify edit region）和它对应的текстура映射。这种相互作用使得本地化和 стилизация均得到了改善。为了提高文本区域的细节和分辨率，我们利用了多个阶段的叠加扩散模型来监督我们的本地编辑技术。我们称之为Cascaded Score Distillation（CSD），它同时在叠加的多个阶段中进行分辨率控制和全局理解的监督。我们示出了3D涂刷技术的效果，可以在不同的semantic region中地方Texture各种形状。项目页面：https://threedle.github.io/3d-paintbrush
</details></li>
</ul>
<hr>
<h2 id="Temporal-Aware-Refinement-for-Video-based-Human-Pose-and-Shape-Recovery"><a href="#Temporal-Aware-Refinement-for-Video-based-Human-Pose-and-Shape-Recovery" class="headerlink" title="Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery"></a>Temporal-Aware Refinement for Video-based Human Pose and Shape Recovery</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09543">http://arxiv.org/abs/2311.09543</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ming Chen, Yan Zhou, Weihua Jian, Pengfei Wan, Zhongyuan Wang</li>
<li>for: 高精度和时间一致的人体动作重建从视频中</li>
<li>methods: 使用全球变换器编码器获取时间感知的全局特征序列，并使用卷积GRU网络生成高分辨率的本地特征图，以及一个循环更新模块来优化估计的SMPL参数，以实现高精度和平滑的结果</li>
<li>results: 比前一代方法更高精度的结果，在3DPW、MPI-INF-3DHP和Human3.6M等知名 benchmark 上都达到了更高的性能<details>
<summary>Abstract</summary>
Though significant progress in human pose and shape recovery from monocular RGB images has been made in recent years, obtaining 3D human motion with high accuracy and temporal consistency from videos remains challenging. Existing video-based methods tend to reconstruct human motion from global image features, which lack detailed representation capability and limit the reconstruction accuracy. In this paper, we propose a Temporal-Aware Refining Network (TAR), to synchronously explore temporal-aware global and local image features for accurate pose and shape recovery. First, a global transformer encoder is introduced to obtain temporal global features from static feature sequences. Second, a bidirectional ConvGRU network takes the sequence of high-resolution feature maps as input, and outputs temporal local feature maps that maintain high resolution and capture the local motion of the human body. Finally, a recurrent refinement module iteratively updates estimated SMPL parameters by leveraging both global and local temporal information to achieve accurate and smooth results. Extensive experiments demonstrate that our TAR obtains more accurate results than previous state-of-the-art methods on popular benchmarks, i.e., 3DPW, MPI-INF-3DHP, and Human3.6M.
</details>
<details>
<summary>摘要</summary>
尽管在最近几年内，从单色RGB图像中提取人体姿态和形状的进步很大，但从视频中获取高精度和时间一致的人体运动仍然是一个挑战。现有的视频基于方法通常是从全图像特征中提取人体运动，这些特征缺乏细节表示能力，导致重建精度有限。在这篇论文中，我们提议一种名为时间感知修复网络（TAR），以同步探索时间感知的全局和局部图像特征，以达到高精度和平滑的人体姿态和形状重建。首先，我们引入全球变换Encoder，从静止特征序列中提取时间全局特征。其次，我们使用双向ConvGRU网络，将高分辨率特征图组作为输入，并输出时间局部特征图，以保持高分辨率和捕捉人体动作的局部运动。最后，我们引入循环更新模块，通过全球和局部时间信息来更新估计的SMPL参数，以实现高精度和平滑的结果。我们对 популяр的benchmark进行了广泛的实验，结果表明，我们的TAR方法比之前的状态 искусственный智能方法更高精度。
</details></li>
</ul>
<hr>
<h2 id="FedFusion-Manifold-Driven-Federated-Learning-for-Multi-satellite-and-Multi-modality-Fusion"><a href="#FedFusion-Manifold-Driven-Federated-Learning-for-Multi-satellite-and-Multi-modality-Fusion" class="headerlink" title="FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion"></a>FedFusion: Manifold Driven Federated Learning for Multi-satellite and Multi-modality Fusion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09540">http://arxiv.org/abs/2311.09540</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ldxdu/fedfusion">https://github.com/ldxdu/fedfusion</a></li>
<li>paper_authors: DaiXun Li, Weiying Xie, Yunsong Li, Leyuan Fang<br>for:多Modal remote sensing数据的合并是一项复杂的任务，因为它涉及到多种不同的感知特性和数据分布。methods:该文提出了一种基于拟合的多Modal数据合并框架，即FedFusion，它通过在每个客户端上随机选择地方数据来共同估计每个客户端的显著拟合结构，并将特征矩阵压缩到低维度空间中，作为后续分类器的输入。results:该文比较了现有方法和自己的方法在三个多Modal数据集上的性能，并达到了94.35%的平均分类精度，同时压缩通信成本四倍。此外，基于 Jetson TX2 工业模块的轨道边缘计算架构上进行了实际卫星图像的数字实验，显示FedFusion可以减少训练时间48.4分钟（15.18%），同时优化精度。<details>
<summary>Abstract</summary>
Multi-satellite, multi-modality in-orbit fusion is a challenging task as it explores the fusion representation of complex high-dimensional data under limited computational resources. Deep neural networks can reveal the underlying distribution of multi-modal remote sensing data, but the in-orbit fusion of multimodal data is more difficult because of the limitations of different sensor imaging characteristics, especially when the multimodal data follows non-independent identically distribution (Non-IID) distributions. To address this problem while maintaining classification performance, this paper proposes a manifold-driven multi-modality fusion framework, FedFusion, which randomly samples local data on each client to jointly estimate the prominent manifold structure of shallow features of each client and explicitly compresses the feature matrices into a low-rank subspace through cascading and additive approaches, which is used as the feature input of the subsequent classifier. Considering the physical space limitations of the satellite constellation, we developed a multimodal federated learning module designed specifically for manifold data in a deep latent space. This module achieves iterative updating of the sub-network parameters of each client through global weighted averaging, constructing a framework that can represent compact representations of each client. The proposed framework surpasses existing methods in terms of performance on three multimodal datasets, achieving a classification average accuracy of 94.35$\%$ while compressing communication costs by a factor of 4. Furthermore, extensive numerical evaluations of real-world satellite images were conducted on the orbiting edge computing architecture based on Jetson TX2 industrial modules, which demonstrated that FedFusion significantly reduced training time by 48.4 minutes (15.18%) while optimizing accuracy.}
</details>
<details>
<summary>摘要</summary>
多卫星、多Modalities在遥感空间融合是一个复杂的任务，因为它探索了复杂高维数据的融合表示，在限制的计算资源下进行。深度神经网络可以揭示多Modalities遥感数据的下面分布，但是多Modalities遥感数据的融合更加困难，因为不同的感器成像特点存在限制，特别是当多Modalities数据遵循非独立同分布（Non-IID）分布时。为了解决这个问题而保持分类性能，这篇论文提出了一个概率驱动的多Modalities融合框架，即FedFusion，它在每个客户端上随机选择本地数据，并且同时将每个客户端的浅层特征矩阵压缩到低维度空间中，并通过权重平均来更新每个客户端的子网络参数。针对卫星团 constellation 的物理空间限制，我们开发了特有的多Modalities联合学习模块，用于深层空间中的权重学习。这个模块通过迭代更新每个客户端的子网络参数，构建了一个可以表示每个客户端的紧凑表示框架。提出的框架在三个多Modalities数据集上表现出色，实现了分类准确率94.35%，同时压缩通信成本4倍。此外，基于 Jetson TX2 工业模块的遥感边缘计算架构进行了实际数据测试，并证明了 FedFusion 可以减少训练时间48.4分钟（15.18%），同时优化准确率。
</details></li>
</ul>
<hr>
<h2 id="Pseudo-keypoints-RKHS-Learning-for-Self-supervised-6DoF-Pose-Estimation"><a href="#Pseudo-keypoints-RKHS-Learning-for-Self-supervised-6DoF-Pose-Estimation" class="headerlink" title="Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation"></a>Pseudo-keypoints RKHS Learning for Self-supervised 6DoF Pose Estimation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09500">http://arxiv.org/abs/2311.09500</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangzheng Wu, Michael Greenspan</li>
<li>for:  bridging the simulation-to-real domain gap in 6DoF PE</li>
<li>methods: 使用自然学习kernel在RKHS中，并提出了一种自我超vised keypoint radial voting-based 6DoF PE框架</li>
<li>results: 实现了state-of-the-art性能在三个常用的6DoF PE数据集上（LINEMOD (+4.2%), Occlusion LINEMOD (+2%), YCB-Video (+3%）），并与完全监督方法在所有六个BOP核心数据集上表现相当（ Within -10.8% to -0.3%）。<details>
<summary>Abstract</summary>
This paper addresses the simulation-to-real domain gap in 6DoF PE, and proposes a novel self-supervised keypoint radial voting-based 6DoF PE framework, effectively narrowing this gap using a learnable kernel in RKHS. We formulate this domain gap as a distance in high-dimensional feature space, distinct from previous iterative matching methods. We propose an adapter network, which evolves the network parameters from the source domain, which has been massively trained on synthetic data with synthetic poses, to the target domain, which is trained on real data. Importantly, the real data training only uses pseudo-poses estimated by pseudo-keypoints, and thereby requires no real groundtruth data annotations. RKHSPose achieves state-of-the-art performance on three commonly used 6DoF PE datasets including LINEMOD (+4.2%), Occlusion LINEMOD (+2%), and YCB-Video (+3%). It also compares favorably to fully supervised methods on all six applicable BOP core datasets, achieving within -10.8% to -0.3% of the top fully supervised results.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Center-Focusing-Network-for-Real-Time-LiDAR-Panoptic-Segmentation"><a href="#Center-Focusing-Network-for-Real-Time-LiDAR-Panoptic-Segmentation" class="headerlink" title="Center Focusing Network for Real-Time LiDAR Panoptic Segmentation"></a>Center Focusing Network for Real-Time LiDAR Panoptic Segmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09499">http://arxiv.org/abs/2311.09499</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gangzhang842/cfnet">https://github.com/gangzhang842/cfnet</a></li>
<li>paper_authors: Xiaoyan Li, Gang Zhang, Boyue Wang, Yongli Hu, Baocai Yin</li>
<li>for: 实时LiDAR精准分割，提高自动驾驶车辆对周围环境和对象的全面理解。</li>
<li>methods: 提出了一种新的中心吸引网络（CFNet），包括中心吸引特征编码（CFFE）和快速中心排除模块（CDM），以提高精准和实时LiDAR精准分割。</li>
<li>results: 在SemanticKITTI和nuScenes精准分割benchmark上，CFNet比所有其他方法表现出较大的优势，并与最高效的方法相比，运行速度提高1.6倍。<details>
<summary>Abstract</summary>
LiDAR panoptic segmentation facilitates an autonomous vehicle to comprehensively understand the surrounding objects and scenes and is required to run in real time. The recent proposal-free methods accelerate the algorithm, but their effectiveness and efficiency are still limited owing to the difficulty of modeling non-existent instance centers and the costly center-based clustering modules. To achieve accurate and real-time LiDAR panoptic segmentation, a novel center focusing network (CFNet) is introduced. Specifically, the center focusing feature encoding (CFFE) is proposed to explicitly understand the relationships between the original LiDAR points and virtual instance centers by shifting the LiDAR points and filling in the center points. Moreover, to leverage the redundantly detected centers, a fast center deduplication module (CDM) is proposed to select only one center for each instance. Experiments on the SemanticKITTI and nuScenes panoptic segmentation benchmarks demonstrate that our CFNet outperforms all existing methods by a large margin and is 1.6 times faster than the most efficient method. The code is available at https://github.com/GangZhang842/CFNet.
</details>
<details>
<summary>摘要</summary>
利用LiDAR照片拼接的精炼分割可以帮助自动驾驶车辆全面理解周围环境和场景，并且需要在实时下运行。最近的提议方法可以加速算法，但其效果和效率仍然受到非存在实例中心的模型化和中心基于归一化模块的成本所限。为了实现准确和实时的LiDAR精炼分割，我们提出了一种新的中心集中网络（CFNet）。具体来说，我们提出了中心集中特征编码（CFFE），以明确原始LiDAR点和虚拟实例中心之间的关系，通过将LiDAR点Shift和填充中心点。此外，为了利用重复检测到的中心点，我们提出了快速中心筛选模块（CDM），以选择每个实例只有一个中心点。实验表明，我们的CFNet在SemanticKITTI和nuScenes精炼分割标准 benchmark上比所有其他方法差距较大，并且比最高效的方法快1.6倍。代码可以在https://github.com/GangZhang842/CFNet 中找到。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.CV_2023_11_16/" data-id="clpztdnjm00naes885fjscb0k" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.AI_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.AI_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T12:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-AI/">cs.AI</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.AI_2023_11_16/">cs.AI - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="The-Analysis-and-Extraction-of-Structure-from-Organizational-Charts"><a href="#The-Analysis-and-Extraction-of-Structure-from-Organizational-Charts" class="headerlink" title="The Analysis and Extraction of Structure from Organizational Charts"></a>The Analysis and Extraction of Structure from Organizational Charts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10234">http://arxiv.org/abs/2311.10234</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikhil Manali, David Doermann, Mahesh Desai</li>
<li>for: 提供一种自动化和端到端的方法，用于从组织图中提取信息，以解决手动提取信息的困难和时间消耗问题。</li>
<li>methods: 使用计算机视觉、深度学习和自然语言处理技术来自动提取组织图中的信息。</li>
<li>results: 提出一种用于评估提取信息的完整性和层次准确性的度量，并通过实验证明该方法的有效性。<details>
<summary>Abstract</summary>
Organizational charts, also known as org charts, are critical representations of an organization's structure and the hierarchical relationships between its components and positions. However, manually extracting information from org charts can be error-prone and time-consuming. To solve this, we present an automated and end-to-end approach that uses computer vision, deep learning, and natural language processing techniques. Additionally, we propose a metric to evaluate the completeness and hierarchical accuracy of the extracted information. This approach has the potential to improve organizational restructuring and resource utilization by providing a clear and concise representation of the organizational structure. Our study lays a foundation for further research on the topic of hierarchical chart analysis.
</details>
<details>
<summary>摘要</summary>
organizational charts, also known as org charts, are critical representations of an organization's structure and the hierarchical relationships between its components and positions. however, manually extracting information from org charts can be error-prone and time-consuming. to solve this, we present an automated and end-to-end approach that uses computer vision, deep learning, and natural language processing techniques. additionally, we propose a metric to evaluate the completeness and hierarchical accuracy of the extracted information. this approach has the potential to improve organizational restructuring and resource utilization by providing a clear and concise representation of the organizational structure. our study lays a foundation for further research on the topic of hierarchical chart analysis.Note: The translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you need the translation in Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="A-Graphical-Model-of-Hurricane-Evacuation-Behaviors"><a href="#A-Graphical-Model-of-Hurricane-Evacuation-Behaviors" class="headerlink" title="A Graphical Model of Hurricane Evacuation Behaviors"></a>A Graphical Model of Hurricane Evacuation Behaviors</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10228">http://arxiv.org/abs/2311.10228</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hui Sophie Wang, Nutchanon Yongsatianchot, Stacy Marsella</li>
<li>for: 这 paper 的目的是研究人们在风暴来临时是否离开家园的决策，以及这些决策如何影响紧急准备和应急响应。</li>
<li>methods: 这 paper 使用了 Protection motivation theory (PMT) 框架，构建了风暴离开决策的复杂关系图，并通过 conditional independence tests 评估不同的图 Structures。</li>
<li>results: 研究发现，人们的风暴离开决策受到了威胁评估（threat appraisal）和应急 coping 评估的直接影响，以及媒体信息的直接和间接影响。 certain information received from media 影响了威胁评估，并通过它影响了风暴离开行为。此外，一些变量直接影响了风暴离开行为和威胁评估，包括家人和朋友的建议，邻居的离开行为，以及官员发布的离开通知。<details>
<summary>Abstract</summary>
Natural disasters such as hurricanes are increasing and causing widespread devastation. People's decisions and actions regarding whether to evacuate or not are critical and have a large impact on emergency planning and response. Our interest lies in computationally modeling complex relationships among various factors influencing evacuation decisions. We conducted a study on the evacuation of Hurricane Irma of the 2017 Atlantic hurricane season. The study was guided by the Protection motivation theory (PMT), a widely-used framework to understand people's responses to potential threats. Graphical models were constructed to represent the complex relationships among the factors involved and the evacuation decision. We evaluated different graphical structures based on conditional independence tests using Irma data. The final model largely aligns with PMT. It shows that both risk perception (threat appraisal) and difficulties in evacuation (coping appraisal) influence evacuation decisions directly and independently. Certain information received from media was found to influence risk perception, and through it influence evacuation behaviors indirectly. In addition, several variables were found to influence both risk perception and evacuation behaviors directly, including family and friends' suggestions, neighbors' evacuation behaviors, and evacuation notices from officials.
</details>
<details>
<summary>摘要</summary>
自然灾害如飓风减少不断，引起广泛的破坏。人们的逃离或不逃离的决定对紧急准备和应急应对有着重要的影响。我们的兴趣在于通过计算模型来模拟人们逃离决定的复杂关系。我们在2017年大西洋飓风赛季的飓风艾尔马事例进行了研究。研究受保护动机理论（PMT）的导向，这是解释人们面临可能威胁的响应的广泛使用的框架。我们使用图表模型来表示逃离决定中的复杂关系，并对逃离决定进行了不同的图表结构的评估。我们根据飓风艾尔马数据进行了条件独立测试，最终模型大致与PMT相符。它表明，风险识别（威胁评估）和逃离困难（处理评估）都直接和独立地影响逃离决定。媒体上接受的信息也影响了风险识别，并通过它影响了逃离行为。此外，一些变量直接和 indirectly影响了逃离决定，包括家庭和朋友的建议、邻居的逃离行为以及官员发布的逃离通知。
</details></li>
</ul>
<hr>
<h2 id="Think-Twice-Perspective-Taking-Improves-Large-Language-Models’-Theory-of-Mind-Capabilities"><a href="#Think-Twice-Perspective-Taking-Improves-Large-Language-Models’-Theory-of-Mind-Capabilities" class="headerlink" title="Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities"></a>Think Twice: Perspective-Taking Improves Large Language Models’ Theory-of-Mind Capabilities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10227">http://arxiv.org/abs/2311.10227</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, Louis-Philippe Morency</li>
<li>for: 本研究旨在提高现有的大自然语言模型（LLM） Theory of Mind（ToM）能力。</li>
<li>methods: 本研究提出了一种新的两阶段引导框架，名为SimToM，它基于认知科学理论“模拟理论”中的视角变换。</li>
<li>results: 对当前 ToM benchmark 进行应用，SimToM 方法显示了明显的改善，而且我们的分析表明了对理论知识的了解对 ToM 能力的重要性。<details>
<summary>Abstract</summary>
Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory "Simulation Theory" to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.
</details>
<details>
<summary>摘要</summary>
人类互动深受理智思维、信念和愿望的互动，这些思维是通过理智思维（ToM）实现的：我们的认知能力理解自己和他人的心理状态。虽然ToM可能是自然的，但模拟它对even最高级语言模型（LLMs）来说是一项挑战。现有的LLMs的理解能力的改进从简单而有效的提示技术such as Chain-of-Thought中有限的应用于ToM。在这篇论文中，我们转向了著名的认知科学理论“模拟理论”来bridging这个差距。我们提出了一种新的两个阶段的提示框架，称为SimToM，它是基于模拟理论中的看法拟合的想法。为了在当前的ToM标准benchmark上实现这个想法，SimToM首先根据character知道的信息过滤上下文，然后回答关于其心理状态的问题。我们的方法不需要额外的训练和微小的提示调整，而且与现有的方法比较，我们的结果表明了看法拟合的重要性，以及它在理智思维能力方面的潜在性。我们的发现建议将 perspective-taking作为未来研究理智思维能力的可能方向。
</details></li>
</ul>
<hr>
<h2 id="A-Language-and-Its-Dimensions-Intrinsic-Dimensions-of-Language-Fractal-Structures"><a href="#A-Language-and-Its-Dimensions-Intrinsic-Dimensions-of-Language-Fractal-Structures" class="headerlink" title="A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures"></a>A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal Structures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10217">http://arxiv.org/abs/2311.10217</a></li>
<li>repo_url: None</li>
<li>paper_authors: Vasilii A. Gromov, Nikita S. Borodin, Asel S. Yerbolova</li>
<li>for: The paper is written to introduce a new object of study - a language fractal structure, and to estimate the intrinsic dimensions of language fractal structures for the Russian and English languages.</li>
<li>methods: The paper uses methods based on topological data analysis and a minimum spanning tree of a data graph to estimate the intrinsic dimensions of language fractal structures.</li>
<li>results: The paper finds that the intrinsic dimensions of language fractal structures for both the Russian and English languages are non-integer values, close to 9 for both languages.Here is the information in Simplified Chinese text, as requested:</li>
<li>for: 本研究对自然语言中的语言异步结构进行了新的研究，并估计了俄语和英语语言异步结构的内在维度。</li>
<li>methods: 本研究使用了基于拓扑数据分析和数据图中最小杆的方法来估计语言异步结构的内在维度。</li>
<li>results: 研究发现，俄语和英语语言异步结构的内在维度都是非整数值，都接近9。<details>
<summary>Abstract</summary>
The present paper introduces a novel object of study - a language fractal structure. We hypothesize that a set of embeddings of all $n$-grams of a natural language constitutes a representative sample of this fractal set. (We use the term Hailonakea to refer to the sum total of all language fractal structures, over all $n$). The paper estimates intrinsic (genuine) dimensions of language fractal structures for the Russian and English languages. To this end, we employ methods based on (1) topological data analysis and (2) a minimum spanning tree of a data graph for a cloud of points considered (Steele theorem). For both languages, for all $n$, the intrinsic dimensions appear to be non-integer values (typical for fractal sets), close to 9 for both of the Russian and English language.
</details>
<details>
<summary>摘要</summary>
本文介绍一种新的研究对象——语言自similarity结构。我们假设所有自然语言中的ngrams集合可以视为这种自similarity结构的代表样本。（我们使用“Hailonakea”这个 термин来描述所有语言自similarity结构的总和，随着n的变化）。本文对俄语和英语两种语言的语言自similarity结构进行了估计。为此，我们使用了基于拓扑数据分析和最小杆法（Steele theorem）的方法。对于两种语言和所有n，内部维度都显示为非整数值（典型的自similarity集合特征），约等于9。
</details></li>
</ul>
<hr>
<h2 id="Predictive-Minds-LLMs-As-Atypical-Active-Inference-Agents"><a href="#Predictive-Minds-LLMs-As-Atypical-Active-Inference-Agents" class="headerlink" title="Predictive Minds: LLMs As Atypical Active Inference Agents"></a>Predictive Minds: LLMs As Atypical Active Inference Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10215">http://arxiv.org/abs/2311.10215</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jan Kulveit, Clem von Stengel, Roman Leventov</li>
<li>for: 本文探讨大语言模型（LLM）如何理解和使用active inference理论。</li>
<li>methods: 文章比较传统的active inference系统和LLM的相似之处和差异，并结论LLM目前缺乏与行动在世界中产生影响的紧密反馈循环，但其他地方符合active inference模式。</li>
<li>results: 文章列出了可能关闭这个循环的原因，以及这可能导致模型自我意识和减少预测错误的变化。<details>
<summary>Abstract</summary>
Large language models (LLMs) like GPT are often conceptualized as passive predictors, simulators, or even stochastic parrots. We instead conceptualize LLMs by drawing on the theory of active inference originating in cognitive science and neuroscience. We examine similarities and differences between traditional active inference systems and LLMs, leading to the conclusion that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impacts of their actions, but otherwise fit in the active inference paradigm. We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）如GPT通常被概念化为无动的预测器、模拟器或甚至是随机的喊喊鸟。我们则通过从认知科学和神经科学中的活跃推理理论来概念化LLM。我们比较了传统的活跃推理系统和LLM之间的相似之处和不同之处，结论是，目前LLM缺乏在世界中行动并观察自己的影响的紧密回路，但以其他方面符合活跃推理概念。我们列出了关闭这个回路的原因，以及这可能会带来的影响，包括增强模型自我意识和驱动降低预测错误的改变世界的驱动。
</details></li>
</ul>
<hr>
<h2 id="Bayes-in-the-age-of-intelligent-machines"><a href="#Bayes-in-the-age-of-intelligent-machines" class="headerlink" title="Bayes in the age of intelligent machines"></a>Bayes in the age of intelligent machines</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10206">http://arxiv.org/abs/2311.10206</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thomas L. Griffiths, Jian-Qiao Zhu, Erin Grant, R. Thomas McCoy</li>
<li>for: 本研究旨在探讨人工神经网络如何影响人类认知解释，并 argue that Bayesian模型和人工神经网络是 complementary modeling approach，可以用来理解人类认知和智能机器的行为。</li>
<li>methods: 本研究使用了人工神经网络和 Bayesian模型来解释人类认知和智能机器的行为。</li>
<li>results: 研究发现，Bayesian模型和人工神经网络是不同的层次分析方法，可以共同理解人类认知和智能机器的行为，并且 Bayesian模型在解释大型、透明度低的人工神经网络行为方面可能具有独特的价值。<details>
<summary>Abstract</summary>
The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.
</details>
<details>
<summary>摘要</summary>
人类认知的解释可能会受到基于人工神经网络的方法的成功威胁。我们认为这并不是如此，我们认为这些系统实际上提供了新的机会来模型人类认知。具体来说，我们认为认知科学的概率模型和人工神经网络模型在不同的水平上进行模型化，这些模型之间存在衔接，可以用来理解人类认知的各个水平。此外，我们还认为概率模型在理解大型、不透明的人工神经网络的行为方面可能具有独特的价值，这些网络通常是基于专有数据进行训练的。
</details></li>
</ul>
<hr>
<h2 id="Towards-Improving-Robustness-Against-Common-Corruptions-using-Mixture-of-Class-Specific-Experts"><a href="#Towards-Improving-Robustness-Against-Common-Corruptions-using-Mixture-of-Class-Specific-Experts" class="headerlink" title="Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts"></a>Towards Improving Robustness Against Common Corruptions using Mixture of Class Specific Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10177">http://arxiv.org/abs/2311.10177</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shashank Kotyan, Danilo Vasconcellos Vargas</li>
<li>for: 这篇论文的目的是提高神经网络的适用范围和性能，以应对不断变化的实际世界情况。</li>
<li>methods: 这篇论文提出了一种名为“混合类别专家架构”的新方法，它通过专门为每个类别训练独立的神经网络段，并将其输出组合以提高神经网络的扩展性和绩效。</li>
<li>results: 研究发现，这种新方法可以提高神经网络的适用范围和性能，并在不同的测试 benchmark 上表现出色。特别是在面对未知的扭曲和折衣时，这种方法可以提供更高的适用范围和稳定性。<details>
<summary>Abstract</summary>
Neural networks have demonstrated significant accuracy across various domains, yet their vulnerability to subtle input alterations remains a persistent challenge. Conventional methods like data augmentation, while effective to some extent, fall short in addressing unforeseen corruptions, limiting the adaptability of neural networks in real-world scenarios. In response, this paper introduces a novel paradigm known as the Mixture of Class-Specific Expert Architecture. The approach involves disentangling feature learning for individual classes, offering a nuanced enhancement in scalability and overall performance. By training dedicated network segments for each class and subsequently aggregating their outputs, the proposed architecture aims to mitigate vulnerabilities associated with common neural network structures. The study underscores the importance of comprehensive evaluation methodologies, advocating for the incorporation of benchmarks like the common corruptions benchmark. This inclusion provides nuanced insights into the vulnerabilities of neural networks, especially concerning their generalization capabilities and robustness to unforeseen distortions. The research aligns with the broader objective of advancing the development of highly robust learning systems capable of nuanced reasoning across diverse and challenging real-world scenarios. Through this contribution, the paper aims to foster a deeper understanding of neural network limitations and proposes a practical approach to enhance their resilience in the face of evolving and unpredictable conditions.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="JaxMARL-Multi-Agent-RL-Environments-in-JAX"><a href="#JaxMARL-Multi-Agent-RL-Environments-in-JAX" class="headerlink" title="JaxMARL: Multi-Agent RL Environments in JAX"></a>JaxMARL: Multi-Agent RL Environments in JAX</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10090">http://arxiv.org/abs/2311.10090</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/flairox/jaxmarl">https://github.com/flairox/jaxmarl</a></li>
<li>paper_authors: Alexander Rutherford, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, Akbir Khan, Christian Schroeder de Witt, Alexandra Souly, Saptarashmi Bandyopadhyay, Mikayel Samvelyan, Minqi Jiang, Robert Tjarko Lange, Shimon Whiteson, Bruno Lacerda, Nick Hawes, Tim Rocktaschel, Chris Lu, Jakob Nicolaus Foerster</li>
<li>for: This paper is written for researchers and developers in the field of reinforcement learning (RL) and multi-agent reinforcement learning (MARL), who need efficient and scalable environments for training and evaluating their algorithms.</li>
<li>methods: The paper uses JAX (Jax.org) to enable massively parallel RL training pipelines and environments, and presents JaxMARL, an open-source code base that combines ease-of-use with GPU-enabled efficiency for commonly used MARL environments and popular baseline algorithms.</li>
<li>results: The paper shows that JaxMARL is up to 12,500 times faster than existing approaches in terms of wall clock time, enabling efficient and thorough evaluations, and introduces SMAX, a vectorized and simplified version of the StarCraft Multi-Agent Challenge that enables GPU acceleration and provides a more flexible MARL environment.<details>
<summary>Abstract</summary>
Benchmarks play an important role in the development of machine learning algorithms. For example, research in reinforcement learning (RL) has been heavily influenced by available environments and benchmarks. However, RL environments are traditionally run on the CPU, limiting their scalability with typical academic compute. Recent advancements in JAX have enabled the wider use of hardware acceleration to overcome these computational hurdles, enabling massively parallel RL training pipelines and environments. This is particularly useful for multi-agent reinforcement learning (MARL) research. First of all, multiple agents must be considered at each environment step, adding computational burden, and secondly, the sample complexity is increased due to non-stationarity, decentralised partial observability, or other MARL challenges. In this paper, we present JaxMARL, the first open-source code base that combines ease-of-use with GPU enabled efficiency, and supports a large number of commonly used MARL environments as well as popular baseline algorithms. When considering wall clock time, our experiments show that per-run our JAX-based training pipeline is up to 12500x faster than existing approaches. This enables efficient and thorough evaluations, with the potential to alleviate the evaluation crisis of the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. We provide code at https://github.com/flairox/jaxmarl.
</details>
<details>
<summary>摘要</summary>
��benchmark��是机器学习算法开发中非常重要的一部分。例如，在强化学习（RL）方面，可用的环境和benchmark��对研究产生了深远的影响。然而，RL环境通常在CPU上运行，这限制了学术计算的扩展性。现在，JAX技术的发展使得可以通过硬件加速来超越这些计算障碍，实现了大规模并行的RL训练管道和环境。这特别有用于多智能体强化学习（MARL）研究。首先，在每个环境步骤中，需要考虑多个智能体，这添加了计算压力；其次，样本复杂性增加由非站立性、分布式部分可见性或其他MARL挑战。在这篇论文中，我们提供了JaxMARL，首个开源代码库，搭配易用性和GPU启用效率，支持大量常用的MARL环境以及流行的基线算法。在考虑wall clock时间的情况下，我们的JAX基本训练管道相比现有方法，每次训练的时间提高了12500倍。这使得有效和详细的评估变得可能，有potential解决机器学习领域的评估危机。我们还引入了SMAX，一种简化版的StarCraft Multi-Agent Challenge， removes the need to run the StarCraft II game engine。这不仅使得GPU加速可能，还提供了更 flexible MARL环境，解锁了自适应、元学习和未来MARL应用的潜力。我们提供了代码，可以在https://github.com/flairox/jaxmarl上获取。
</details></li>
</ul>
<hr>
<h2 id="Emu-Edit-Precise-Image-Editing-via-Recognition-and-Generation-Tasks"><a href="#Emu-Edit-Precise-Image-Editing-via-Recognition-and-Generation-Tasks" class="headerlink" title="Emu Edit: Precise Image Editing via Recognition and Generation Tasks"></a>Emu Edit: Precise Image Editing via Recognition and Generation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10089">http://arxiv.org/abs/2311.10089</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman</li>
<li>For: 这个论文旨在提出一种多任务图像编辑模型，以便通过自然语言指令执行图像编辑操作。* Methods: 该模型通过多任务学习和学习任务嵌入来提高图像编辑能力，并且可以通过几个例子学习新任务。* Results: 该模型在多个图像编辑任务中达到了领先水平，包括区域基本编辑、自由形编辑和计算机视觉任务。此外，该模型还可以执行新的图像编辑任务，只需几个标注的样本即可。<details>
<summary>Abstract</summary>
Instruction-based image editing holds immense potential for a variety of applications, as it enables users to perform any editing operation using a natural language instruction. However, current models in this domain often struggle with accurately executing user instructions. We present Emu Edit, a multi-task image editing model which sets state-of-the-art results in instruction-based image editing. To develop Emu Edit we train it to multi-task across an unprecedented range of tasks, such as region-based editing, free-form editing, and Computer Vision tasks, all of which are formulated as generative tasks. Additionally, to enhance Emu Edit's multi-task learning abilities, we provide it with learned task embeddings which guide the generation process towards the correct edit type. Both these elements are essential for Emu Edit's outstanding performance. Furthermore, we show that Emu Edit can generalize to new tasks, such as image inpainting, super-resolution, and compositions of editing tasks, with just a few labeled examples. This capability offers a significant advantage in scenarios where high-quality samples are scarce. Lastly, to facilitate a more rigorous and informed assessment of instructable image editing models, we release a new challenging and versatile benchmark that includes seven different image editing tasks.
</details>
<details>
<summary>摘要</summary>
<SYS>请求已处理，以下是文本的简化中文版本：</SYS> instruciton-based image editing 拥有巨大的潜力，可以用自然语言指令进行任何编辑操作。然而，当前在这个领域的模型经常会对用户的指令进行不准确的执行。我们介绍了 Emu Edit，一个多任务图像编辑模型，它在 instruciton-based image editing 中设置了新的STATE-OF-THE-ART 成绩。为了开发 Emu Edit，我们训练它来执行多种任务，例如区域编辑、自由形编辑和计算机视觉任务，这些任务都是生成任务。此外，为了增强 Emu Edit 的多任务学习能力，我们为它提供了学习任务嵌入，这些嵌入引导生成过程向正确的编辑类型。这两个元素都是 Emu Edit 的出色表现的关键。此外，我们表明 Emu Edit 可以通过几个标注的例子学习新任务，如图像填充、超解像和编辑任务的组合。这种能力在情况中缺乏高质量样本时具有重要优势。最后，为了促进 instruciton-based image editing 模型的更加严格和有知识的评估，我们发布了一个新的复杂和多样的benchmark，包括七种不同的图像编辑任务。
</details></li>
</ul>
<hr>
<h2 id="Intelligent-Generation-of-Graphical-Game-Assets-A-Conceptual-Framework-and-Systematic-Review-of-the-State-of-the-Art"><a href="#Intelligent-Generation-of-Graphical-Game-Assets-A-Conceptual-Framework-and-Systematic-Review-of-the-State-of-the-Art" class="headerlink" title="Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art"></a>Intelligent Generation of Graphical Game Assets: A Conceptual Framework and Systematic Review of the State of the Art</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10129">http://arxiv.org/abs/2311.10129</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kaisei Fukaya, Damon Daylamani-Zad, Harry Agius</li>
<li>for: 这篇论文的目的是对游戏中的图形资产生成进行系统性的文献综述，以便为感兴趣的人提供可能的方法和方法，并帮助他们了解和应用这些方法。</li>
<li>methods: 这篇论文使用了系统性的文献综述方法，检索了200篇有关图形资产生成的论文，从而掌握了现状的方法和技术。</li>
<li>results: 该论文对现有的图形资产生成方法进行了概括和分析，并基于文献的研究，提出了一种概念框架，以帮助感兴趣的人了解和应用图形资产生成的方法。<details>
<summary>Abstract</summary>
Procedural content generation (PCG) can be applied to a wide variety of tasks in games, from narratives, levels and sounds, to trees and weapons. A large amount of game content is comprised of graphical assets, such as clouds, buildings or vegetation, that do not require gameplay function considerations. There is also a breadth of literature examining the procedural generation of such elements for purposes outside of games. The body of research, focused on specific methods for generating specific assets, provides a narrow view of the available possibilities. Hence, it is difficult to have a clear picture of all approaches and possibilities, with no guide for interested parties to discover possible methods and approaches for their needs, and no facility to guide them through each technique or approach to map out the process of using them. Therefore, a systematic literature review has been conducted, yielding 200 accepted papers. This paper explores state-of-the-art approaches to graphical asset generation, examining research from a wide range of applications, inside and outside of games. Informed by the literature, a conceptual framework has been derived to address the aforementioned gaps.
</details>
<details>
<summary>摘要</summary>
<<SYS>>translate_orientation: horizontal Процедурное содержание генерации (PCG) может быть применено к широкому спектру задач в играх, от сюжетов, уровней и звуков, до деревьев и оружия. Огромное количество содержимого игры состоит из графических ассетов, таких как облака, здания или растительность, которые не требуют рассмотрения функций игры. Также существует широкий спектр литературы, который изучает процедурное генерация таких элементов для целей вне игр. Тело исследований, сосредоточенное на конкретных методах для генерации конкретных активов, ограничивает видимость доступных возможных подходов и подходов. Поэтому трудно получить ясный обзор всех подходов и возможностей, а также нет инструментов для руководства заинтересованными сторонами в возможных методах и подходах для их потребностей. Поэтому была проведена систематическая рецензия литературы, которая дала 200 принятых статей. Эта статья исследовает современные подходы к генерации графических активов, изучая исследования из широкого спектра приложений, как внутри, так и вне игр. Обоснованная литературой, была получена концептуальная рамка, чтобы устранить перечисленные пробелы.
</details></li>
</ul>
<hr>
<h2 id="ChatGPT-3-5-ChatGPT-4-Google-Bard-and-Microsoft-Bing-to-Improve-Health-Literacy-and-Communication-in-Pediatric-Populations-and-Beyond"><a href="#ChatGPT-3-5-ChatGPT-4-Google-Bard-and-Microsoft-Bing-to-Improve-Health-Literacy-and-Communication-in-Pediatric-Populations-and-Beyond" class="headerlink" title="ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond"></a>ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Bing to Improve Health Literacy and Communication in Pediatric Populations and Beyond</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10075">http://arxiv.org/abs/2311.10075</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kanhai S. Amin, Linda Mayes, Pavan Khosla, Rushabh Doshi</li>
<li>For: The paper aims to investigate whether large language models (LLMs) can improve health literacy in children and other populations.* Methods: The authors used 26 different prompts to test the ability of three LLMs (ChatGPT-3.5, Microsoft Bing, and Google Bard) to provide health information at different reading grade levels (RGL). They evaluated the responses based on their reading grade level and word count.* Results: The results show that all three LLMs were able to provide responses at or above a 10th-grade RGL. However, ChatGPT-3.5 and ChatGPT-4 were better at providing responses at lower grade levels, while Microsoft Bing and Google Bard tended to produce responses at a consistent high school level. The authors also found that Bard was more cautious in providing certain outputs, which may indicate a need for further research on the accuracy and effectiveness of LLMs in health communication.<details>
<summary>Abstract</summary>
Purpose: Enhanced health literacy has been linked to better health outcomes; however, few interventions have been studied. We investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.   Methods: We ran 288 conditions using 26 different prompts through ChatGPT-3.5, Microsoft Bing, and Google Bard. Given constraints imposed by rate limits, we tested a subset of 150 conditions through ChatGPT-4. The primary outcome measurements were the reading grade level (RGL) and word counts of output.   Results: Across all models, output for basic prompts such as "Explain" and "What is (are)" were at, or exceeded, a 10th-grade RGL. When prompts were specified to explain conditions from the 1st to 12th RGL, we found that LLMs had varying abilities to tailor responses based on RGL. ChatGPT-3.5 provided responses that ranged from the 7th-grade to college freshmen RGL while ChatGPT-4 outputted responses from the 6th-grade to the college-senior RGL. Microsoft Bing provided responses from the 9th to 11th RGL while Google Bard provided responses from the 7th to 10th RGL.   Discussion: ChatGPT-3.5 and ChatGPT-4 did better in achieving lower-grade level outputs. Meanwhile Bard and Bing tended to consistently produce an RGL that is at the high school level regardless of prompt. Additionally, Bard's hesitancy in providing certain outputs indicates a cautious approach towards health information. LLMs demonstrate promise in enhancing health communication, but future research should verify the accuracy and effectiveness of such tools in this context.   Implications: LLMs face challenges in crafting outputs below a sixth-grade reading level. However, their capability to modify outputs above this threshold provides a potential mechanism to improve health literacy and communication in a pediatric population and beyond.
</details>
<details>
<summary>摘要</summary>
目的：增强健康文化知识与健康结果之间的关系，但只有少数临床实践被研究。我们 investigate whether large language models (LLMs) can serve as a medium to improve health literacy in children and other populations.  方法：我们运行了288个条件，使用26个提示，通过ChatGPT-3.5、Microsoft Bing和Google Bard进行测试。由于环境限制，我们只测试了150个条件。主要输出测量方法包括阅读水平（RGL）和单词计数。  结果：所有模型的输出基本提问（如“解释”和“是什么”）的RGL都达到或超过了高中水平。当提示是指定为1-12年级的条件时，我们发现LLMs有不同的能力来适应不同的学龄水平。ChatGPT-3.5提供了7-12年级的回答，而ChatGPT-4则提供了6-12年级的回答。Microsoft Bing提供了9-11年级的回答，而Google Bard则提供了7-10年级的回答。  讨论：ChatGPT-3.5和ChatGPT-4在实现更低学龄水平的输出方面表现更好。与之相比，Bing和Bard倾向于一直提供高中水平的回答，无论提示是什么。此外，Bard的某些输出表现出了谨慎的态度，这可能是一种对健康信息的谨慎方式。LLMs在健康沟通方面表现了搭配性，但未来的研究应该验证这些工具在这种上下文中的准确性和效果。  意义：LLMs面临低于6年级阅读水平的输出创作的挑战。然而，它们可以修改输出以上这个阈值提供一个可能的机制来提高健康文化知识和沟通。
</details></li>
</ul>
<hr>
<h2 id="The-Song-Describer-Dataset-a-Corpus-of-Audio-Captions-for-Music-and-Language-Evaluation"><a href="#The-Song-Describer-Dataset-a-Corpus-of-Audio-Captions-for-Music-and-Language-Evaluation" class="headerlink" title="The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation"></a>The Song Describer Dataset: a Corpus of Audio Captions for Music-and-Language Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10057">http://arxiv.org/abs/2311.10057</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mulab-mir/song-describer-dataset">https://github.com/mulab-mir/song-describer-dataset</a></li>
<li>paper_authors: Ilaria Manco, Benno Weck, SeungHeon Doh, Minz Won, Yixiao Zhang, Dmitry Bodganov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, Elio Quinton, György Fazekas, Juhan Nam</li>
<li>for: 评估音乐和语言模型的评估 dataset，提供高质量的音频-caption对。</li>
<li>methods: 使用人工写好的自然语言描述，对706首乐曲进行了评估。</li>
<li>results: 通过三种音乐和语言任务的测试（乐曲描述、文本到乐曲生成和乐曲语言检索），研究人员可以通过 SDD 来更好地了解模型性能。<details>
<summary>Abstract</summary>
We introduce the Song Describer dataset (SDD), a new crowdsourced corpus of high-quality audio-caption pairs, designed for the evaluation of music-and-language models. The dataset consists of 1.1k human-written natural language descriptions of 706 music recordings, all publicly accessible and released under Creative Common licenses. To showcase the use of our dataset, we benchmark popular models on three key music-and-language tasks (music captioning, text-to-music generation and music-language retrieval). Our experiments highlight the importance of cross-dataset evaluation and offer insights into how researchers can use SDD to gain a broader understanding of model performance.
</details>
<details>
<summary>摘要</summary>
我们介绍歌曲描述数据集（SDD），一个新的人工抽样的音频-caption对数据集，适用于评估音乐和语言模型。该数据集包含1.1k名人写的自然语言描述，描述了706首音乐录音，所有公共可访问，发布于创意共享许可证下。为了展示我们的数据集的使用，我们在三个关键的音乐和语言任务（音频描述、文本到音乐生成和音乐语言检索）中对流行的模型进行了测试。我们的实验表明了跨数据集评估的重要性，并提供了如何使用 SDD 来深入了解模型性能的细节。
</details></li>
</ul>
<hr>
<h2 id="Is-“A-Helpful-Assistant”-the-Best-Role-for-Large-Language-Models-A-Systematic-Evaluation-of-Social-Roles-in-System-Prompts"><a href="#Is-“A-Helpful-Assistant”-the-Best-Role-for-Large-Language-Models-A-Systematic-Evaluation-of-Social-Roles-in-System-Prompts" class="headerlink" title="Is “A Helpful Assistant” the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts"></a>Is “A Helpful Assistant” the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10054">http://arxiv.org/abs/2311.10054</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mingqian Zheng, Jiaxin Pei, David Jurgens</li>
<li>for: 这个研究旨在评估社交角色在系统提示中对模型性能的影响，以帮助设计更好的AI系统提示。</li>
<li>methods: 该研究使用了3个流行的大语言模型和2457道问题进行了广泛的分析，并制定了162个社交角色，包括6种人际关系和8种职业。</li>
<li>results: 研究发现，在提示中添加社交角色可以一致提高模型的性能，并且使用gender-neutral角色和指定受众角色可以更好地提高模型的性能。然而，预测哪一个角色会导致最佳性能仍然是一个挑战，并且频率、相似度和混淆率不能完全解释社交角色对模型性能的影响。<details>
<summary>Abstract</summary>
Prompting serves as the major way humans interact with Large Language Models (LLM). Commercial AI systems commonly define the role of the LLM in system prompts. For example, ChatGPT uses "You are a helpful assistant" as part of the default system prompt. But is "a helpful assistant" the best role for LLMs? In this study, we present a systematic evaluation of how social roles in system prompts affect model performance. We curate a list of 162 roles covering 6 types of interpersonal relationships and 8 types of occupations. Through extensive analysis of 3 popular LLMs and 2457 questions, we show that adding interpersonal roles in prompts consistently improves the models' performance over a range of questions. Moreover, while we find that using gender-neutral roles and specifying the role as the audience leads to better performances, predicting which role leads to the best performance remains a challenging task, and that frequency, similarity, and perplexity do not fully explain the effect of social roles on model performances. Our results can help inform the design of system prompts for AI systems. Code and data are available at https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.
</details>
<details>
<summary>摘要</summary>
大量语言模型（LLM）与人类之间的互动主要是通过提示来实现。商业人工智能系统通常将 LLM 的角色定义为系统提示中的一部分。例如，ChatGPT 使用 "你是一位有用的助手" 作为默认系统提示。但是 "有用的助手" 是 LLM 的最佳角色吗？在这项研究中，我们提供了一种系统atic评估如何社交角色在系统提示中影响模型性能。我们筛选了 162 个角色，涵盖了6种人际关系和8种职业。通过对 3 个流行 LLM 和 2457 个问题进行广泛分析，我们发现，在提示中添加社交角色可以逐渐提高模型的性能范围内的问题。此外，我们发现使用 gender-neutral 角色和指定角色为受众可以提高模型的性能，但是预测哪一个角色会导致最佳性能是一项困难的任务，并且频率、相似度和混淆率不能完全解释社交角色对模型性能的影响。我们的结果可以帮助设计 AI 系统的提示。代码和数据可以在 https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles 上获取。
</details></li>
</ul>
<hr>
<h2 id="Inherently-Interpretable-Time-Series-Classification-via-Multiple-Instance-Learning"><a href="#Inherently-Interpretable-Time-Series-Classification-via-Multiple-Instance-Learning" class="headerlink" title="Inherently Interpretable Time Series Classification via Multiple Instance Learning"></a>Inherently Interpretable Time Series Classification via Multiple Instance Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10049">http://arxiv.org/abs/2311.10049</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jaearly/miltimeseriesclassification">https://github.com/jaearly/miltimeseriesclassification</a></li>
<li>paper_authors: Joseph Early, Gavin KC Cheung, Kurt Cutajar, Hanting Xie, Jas Kandola, Niall Twomey</li>
<li>for: 本研究旨在提高时间序列分类器的可解释性，使其决策过程更加直观和可理解。</li>
<li>methods: 本研究使用多例学习（MIL）技术，提出了一种名为MILLET（多例学习为时间序列分类提供了本地可解释性）的新框架。该框架可以应用于现有的深度学习时间序列分类模型，使其自然地具有可解释性，而不会产生性能下降。</li>
<li>results: 在85个UCR时间序列分类 dataset上测试了MILLET，并证明了它可以生成高质量的解释，比其他已知的可解释方法更好。此外，我们还提供了一个专门为可解释性评估设计的 synthetic dataset。<details>
<summary>Abstract</summary>
Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains
</details>
<details>
<summary>摘要</summary>
We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specifically designed to facilitate interpretability evaluation. Our results show that MILLET produces high-quality explanations quickly, which are superior to other well-known interpretability methods. To the best of our knowledge, our work with MILLET, which is available on GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the first to develop general MIL methods for TSC and apply them to a wide range of domains.
</details></li>
</ul>
<hr>
<h2 id="A-Novel-Neural-Network-Based-Federated-Learning-System-for-Imbalanced-and-Non-IID-Data"><a href="#A-Novel-Neural-Network-Based-Federated-Learning-System-for-Imbalanced-and-Non-IID-Data" class="headerlink" title="A Novel Neural Network-Based Federated Learning System for Imbalanced and Non-IID Data"></a>A Novel Neural Network-Based Federated Learning System for Imbalanced and Non-IID Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10025">http://arxiv.org/abs/2311.10025</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mahfuzur Rahman Chowdhury, Muhammad Ibrahim</li>
<li>for: 本研究旨在提高 Federated Learning 中数据隐私的保护，并提出一种中央化的 neural network-based Federated Learning 系统，以提高 Accuracy 和 Efficiency。</li>
<li>methods: 本研究使用了 Micro-level 并行处理，启发自传统的 mini-batch 算法， Client 设备和服务器分别处理前向和反向传播。此外，我们还提出了一种 Edge Computing 版本的我们的提posed algorithm，在减少中央服务器负担的情况下， Client 处理前向和反向传播。</li>
<li>results: 我们在五个Well-known Benchmark dataset上进行了评估，并在不同数据分布Setting下获得了满意的性能，与一些现有的标准algorithm相比，我们的提posed system在一定程度上减少了训练时间。<details>
<summary>Abstract</summary>
With the growth of machine learning techniques, privacy of data of users has become a major concern. Most of the machine learning algorithms rely heavily on large amount of data which may be collected from various sources. Collecting these data yet maintaining privacy policies has become one of the most challenging tasks for the researchers. To combat this issue, researchers have introduced federated learning, where a prediction model is learnt by ensuring the privacy of data of clients data. However, the prevalent federated learning algorithms possess an accuracy and efficiency trade-off, especially for non-IID data. In this research, we propose a centralized, neural network-based federated learning system. The centralized algorithm incorporates micro-level parallel processing inspired by the traditional mini-batch algorithm where the client devices and the server handle the forward and backward propagation respectively. We also devise a semi-centralized version of our proposed algorithm. This algorithm takes advantage of edge computing for minimizing the load from the central server, where clients handle both the forward and backward propagation while sacrificing the overall train time to some extent. We evaluate our proposed systems on five well-known benchmark datasets and achieve satisfactory performance in a reasonable time across various data distribution settings as compared to some existing benchmark algorithms.
</details>
<details>
<summary>摘要</summary>
随着机器学习技术的发展，用户数据隐私的问题已成为一个重要的挑战。大多数机器学习算法需要大量数据，这些数据可能来自于多个来源。收集这些数据并保持隐私政策已成为研究人员最大的挑战。为解决这个问题，研究人员已经引入联邦学习，这种方法可以保证客户端数据的隐私。然而，现有的联邦学习算法具有精度和效率的负担假设，特别是非相关数据。在这些研究中，我们提议了一种中央化的神经网络基于联邦学习系统。中央算法包括微级并行处理，这种方法由客户端设备和服务器处理前向和反向传播。我们还开发了一种半中央化版本的我们的提议算法。这种算法利用边计算来减轻中央服务器的负担，客户端处理了前向和反向传播，但是在一定程度上减少了总训练时间。我们在五个常见的benchmark数据集上评估了我们的提议系统，并在不同的数据分布设置下实现了满意的性能，与一些现有的benchmark算法相比。
</details></li>
</ul>
<hr>
<h2 id="Learning-interactions-to-boost-human-creativity-with-bandits-and-GPT-4"><a href="#Learning-interactions-to-boost-human-creativity-with-bandits-and-GPT-4" class="headerlink" title="Learning interactions to boost human creativity with bandits and GPT-4"></a>Learning interactions to boost human creativity with bandits and GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10127">http://arxiv.org/abs/2311.10127</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ara Vartanian, Xiaoxi Sun, Yun-Shiuan Chuang, Siddharth Suresh, Xiaojin Zhu, Timothy T. Rogers</li>
<li>for: 这个论文探讨了人与AI算法之间的互动如何提高人类创造力。</li>
<li>methods: 研究人员使用了一种心理任务来检验人类创造力的限制，即Semantic feature generation：给一个概念名称后，参与者需要列出该概念的所有特征。</li>
<li>results: 研究发现，人类参与者和语言AI（GPT-4）在标准任务和一种提供算法生成提示的变体任务中的行为相似，而且 bandaits学习自AI响应 preferences 与人类行为学习的提示策略相同。结果表明，通过计算机互动，可以使bandits learn from simulated participants的行为，以提高人类创造力。<details>
<summary>Abstract</summary>
This paper considers how interactions with AI algorithms can boost human creative thought. We employ a psychological task that demonstrates limits on human creativity, namely semantic feature generation: given a concept name, respondents must list as many of its features as possible. Human participants typically produce only a fraction of the features they know before getting "stuck." In experiments with humans and with a language AI (GPT-4) we contrast behavior in the standard task versus a variant in which participants can ask for algorithmically-generated hints. Algorithm choice is administered by a multi-armed bandit whose reward indicates whether the hint helped generating more features. Humans and the AI show similar benefits from hints, and remarkably, bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior. The results suggest that strategies for boosting human creativity via computer interactions can be learned by bandits run on groups of simulated participants.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这篇论文研究了人与AI算法之间的互动如何提高人类创造力。我们使用了一项心理任务，以示人类创造力的限制，即给出概念名称后，参与者需要列出该概念的所有特征。人类参与者通常只能生成一小部分的特征，然后就会被"困顿"。在人类参与者和一种语言AI（GPT-4）的实验中，我们比较了标准任务和一种允许参与者请求算法生成提示的变体。算法选择是通过一个多重机枪进行管理，其奖励参与者选择有助于生成更多特征的提示。人类和AI都显示了类似的好处，奇怪地，由人类行为学习的多重机枪偏好了与AI回答相同的提示策略。这些结果表明，通过计算机互动，可以学习提高人类创造力的策略。
</details></li>
</ul>
<hr>
<h2 id="Straggler-resilient-Federated-Learning-Tackling-Computation-Heterogeneity-with-Layer-wise-Partial-Model-Training-in-Mobile-Edge-Network"><a href="#Straggler-resilient-Federated-Learning-Tackling-Computation-Heterogeneity-with-Layer-wise-Partial-Model-Training-in-Mobile-Edge-Network" class="headerlink" title="Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network"></a>Straggler-resilient Federated Learning: Tackling Computation Heterogeneity with Layer-wise Partial Model Training in Mobile Edge Network</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10002">http://arxiv.org/abs/2311.10002</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hongda Wu, Ping Wang, C V Aswartha Narayana</li>
<li>for: 这个研究旨在提出一种基于分布式学习的训练方法，以便让不同设备 collaboration 训练模型，但不需要共享数据。</li>
<li>methods: 该方法基于模型不同设备的Compute能力不同，设备会根据自己的Compute能力来训练模型，而不是训练完整的模型。</li>
<li>results: 研究表明，该方法可以让不同设备 collaboration 训练模型，并且可以更好地平衡学习精度和完成时间。 compared to 现有的参考方法，该方法可以更快地达到学习目标。<details>
<summary>Abstract</summary>
Federated Learning (FL) enables many resource-limited devices to train a model collaboratively without data sharing. However, many existing works focus on model-homogeneous FL, where the global and local models are the same size, ignoring the inherently heterogeneous computational capabilities of different devices and restricting resource-constrained devices from contributing to FL. In this paper, we consider model-heterogeneous FL and propose Federated Partial Model Training (FedPMT), where devices with smaller computational capabilities work on partial models (subsets of the global model) and contribute to the global model. Different from Dropout-based partial model generation, which removes neurons in hidden layers at random, model training in FedPMT is achieved from the back-propagation perspective. As such, all devices in FedPMT prioritize the most crucial parts of the global model. Theoretical analysis shows that the proposed partial model training design has a similar convergence rate to the widely adopted Federated Averaging (FedAvg) algorithm, $\mathcal{O}(1/T)$, with the sub-optimality gap enlarged by a constant factor related to the model splitting design in FedPMT. Empirical results show that FedPMT significantly outperforms the existing benchmark FedDrop. Meanwhile, compared to the popular model-homogeneous benchmark, FedAvg, FedPMT reaches the learning target in a shorter completion time, thus achieving a better trade-off between learning accuracy and completion time.
</details>
<details>
<summary>摘要</summary>
Federated Learning (FL) 允许多个资源有限的设备共同训练模型而无需数据共享。然而，现有的大多数研究都专注于模型同质的 FL，即全球模型和本地模型均为同一个大小，这ignore了不同设备的内在不同计算能力，从而限制了资源有限的设备参与FL。在这篇文章中，我们考虑了模型不同质的 FL，并提出了 Federated Partial Model Training（FedPMT），其中设备的计算能力较低的设备可以在部分模型（全球模型的子集）上进行训练，并对全球模型进行贡献。与Dropout技术基于随机移除隐藏层中的神经元不同，FedPMT中的模型训练是从反射层的角度来实现的，因此所有的设备在FedPMT中都会优先级别最重要的部分。理论分析表明，我们的 partial model 训练设计和 widely adopted Federated Averaging（FedAvg）算法相似，具有 $\mathcal{O}(1/T)$ 的收敛率，但与 FedAvg 相比，FedPMT 的优劣差异因子与模型分割设计相关。实验结果表明，FedPMT 明显超过了现有的 refer 替 benchmark FedDrop。同时，相比于通用模型同质的标准 refer 替 FedAvg，FedPMT 在完成时间和学习精度之间更好地做出了平衡。
</details></li>
</ul>
<hr>
<h2 id="Towards-more-Practical-Threat-Models-in-Artificial-Intelligence-Security"><a href="#Towards-more-Practical-Threat-Models-in-Artificial-Intelligence-Security" class="headerlink" title="Towards more Practical Threat Models in Artificial Intelligence Security"></a>Towards more Practical Threat Models in Artificial Intelligence Security</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09994">http://arxiv.org/abs/2311.09994</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi</li>
<li>for: 这个论文旨在描述人工智能安全领域的研究与实践之间的差距。</li>
<li>methods: 该论文通过对6种最常研究的AI安全攻击方法进行审查，并与271名工业实践者进行调查，以确定这些攻击方法在实际场景中的可行性。</li>
<li>results: 研究发现，现有的威胁模型都是可行的，但是有一些重大匹配度：研究往往假设攻击者具有实际场景中不易获得的信息。这篇论文因此呼吁研究更加实际的威胁模型。<details>
<summary>Abstract</summary>
Recent works have identified a gap between research and practice in artificial intelligence security: threats studied in academia do not always reflect the practical use and security risks of AI. For example, while models are often studied in isolation, they form part of larger ML pipelines in practice. Recent works also brought forward that adversarial manipulations introduced by academic attacks are impractical. We take a first step towards describing the full extent of this disparity. To this end, we revisit the threat models of the six most studied attacks in AI security research and match them to AI usage in practice via a survey with \textbf{271} industrial practitioners. On the one hand, we find that all existing threat models are indeed applicable. On the other hand, there are significant mismatches: research is often too generous with the attacker, assuming access to information not frequently available in real-world settings. Our paper is thus a call for action to study more practical threat models in artificial intelligence security.
</details>
<details>
<summary>摘要</summary>
最近的研究发现人工智能安全领域存在一个研究与实践之间的差距：在学术中研究的威胁不一定与实际使用和安全风险相符。例如，模型在实践中通常是作为更大的机器学习管道的一部分进行研究，而不是孤立的单元。此外，学术攻击的恶意修改也被证明是不实际的。我们为了描述这种差距，我们重新审视了人工智能安全领域最常研究的六种攻击方法，并通过对 \textbf{271} 名工业实践者进行调查，发现所有的威胁模型都是可靠的。然而，我们发现有一些巨大的匹配错误：研究经常假设攻击者具有实际场景中不常 disponibles的信息。因此，我们的论文是一种呼吁，呼吁更多地研究实际可行的人工智能安全威胁模型。
</details></li>
</ul>
<hr>
<h2 id="Generative-AI-for-Hate-Speech-Detection-Evaluation-and-Findings"><a href="#Generative-AI-for-Hate-Speech-Detection-Evaluation-and-Findings" class="headerlink" title="Generative AI for Hate Speech Detection: Evaluation and Findings"></a>Generative AI for Hate Speech Detection: Evaluation and Findings</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09993">http://arxiv.org/abs/2311.09993</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sagi Pendzel, Tomer Wullach, Amir Adler, Einat Minkov</li>
<li>for: 提高自动仇恨语言检测的泛化性能</li>
<li>methods: 使用生成AI生成大量的人工仇恨语言序列，并在已有标注数据的基础上训练大型预训练语言模型（LLM）</li>
<li>results: 对于BERT、RoBERTa、ALBERT等常见LLM，以及已适应仇恨检测的RoBERTa-Toxicity、HateBERT、HateXplain、ToxDect和ToxiGen等模型，进行了评估和比较，并证实了这种方法可以提高仇恨语言泛化性能。同时，我们还对采用零shot仇恨检测的GPT-3.5模型进行了测试，结果显示该模型可以获得更高的泛化性能，但是具有较差的准确率和预测率。<details>
<summary>Abstract</summary>
Automatic hate speech detection using deep neural models is hampered by the scarcity of labeled datasets, leading to poor generalization. To mitigate this problem, generative AI has been utilized to generate large amounts of synthetic hate speech sequences from available labeled examples, leveraging the generated data in finetuning large pre-trained language models (LLMs). In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT, we apply and evaluate the impact of train set augmentation with generated data using LLMs that have been already adapted for hate detection, including RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical study corroborates our previous findings, showing that this approach improves hate speech generalization, boosting recall performance across data distributions. In addition, we explore and compare the performance of the finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results demonstrate that while better generalization is achieved using the GPT-3.5 model, it achieves mediocre recall and low precision on most datasets. It is an open question whether the sensitivity of models such as GPT-3.5, and onward, can be improved using similar techniques of text generation.
</details>
<details>
<summary>摘要</summary>
自动发现仇恨言语使用深度神经网络受到数据标注的罕见性的限制，导致模型的泛化性差。为解决这个问题，生成AI技术被应用来生成大量的人工仇恨言语序列，利用生成的数据进行训练大型预训练语言模型（LLM）。在这章中，我们提供了相关的方法、实验设置和评估这种方法的评估。除了一般的LLM，如BERT、RoBERTa和ALBERT，我们还应用并评估生成数据集 augmentation的影响。我们使用已经适应仇恨检测的LLM，包括RoBERTa-Toxicity、HateBERT、HateXplain、ToxDect和ToxiGen进行训练和评估。我们的实验结果表明，这种方法可以提高仇恨言语泛化性，提高检测性能 across data distributions。此外，我们还探讨了使用GPT-3.5模型进行零容量仇恨检测的性能。我们的结果表明，虽然使用GPT-3.5模型可以获得更好的泛化性，但它的准确率和精度在大多数数据集上都很低。这是一个开放的问题，是否可以通过类似的文本生成技术提高模型的敏感性。
</details></li>
</ul>
<hr>
<h2 id="A-Framework-for-Monitoring-and-Retraining-Language-Models-in-Real-World-Applications"><a href="#A-Framework-for-Monitoring-and-Retraining-Language-Models-in-Real-World-Applications" class="headerlink" title="A Framework for Monitoring and Retraining Language Models in Real-World Applications"></a>A Framework for Monitoring and Retraining Language Models in Real-World Applications</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09930">http://arxiv.org/abs/2311.09930</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jaykumar Kasundra, Claudia Schulz, Melicaalsadat Mirsafian, Stavroula Skylaki</li>
<li>for: 本研究旨在探讨多类标签分类模型在不同 retraining 决策点下的影响，以及如何设计有效的模型重新训练策略。</li>
<li>methods: 本研究使用了多种 retraining 决策点，包括数据或概念漂移、模型性能下降和新数据收集等，以评估它们对模型性能和资源利用率的影响。</li>
<li>results: 研究发现，不同 retraining 决策点可能导致不同的模型性能和资源利用率。根据研究结果，提出了一个参考框架，可以帮助设计有效的模型重新训练策略。<details>
<summary>Abstract</summary>
In the Machine Learning (ML) model development lifecycle, training candidate models using an offline holdout dataset and identifying the best model for the given task is only the first step. After the deployment of the selected model, continuous model monitoring and model retraining is required in many real-world applications. There are multiple reasons for retraining, including data or concept drift, which may be reflected on the model performance as monitored by an appropriate metric. Another motivation for retraining is the acquisition of increasing amounts of data over time, which may be used to retrain and improve the model performance even in the absence of drifts. We examine the impact of various retraining decision points on crucial factors, such as model performance and resource utilization, in the context of Multilabel Classification models. We explain our key decision points and propose a reference framework for designing an effective model retraining strategy.
</details>
<details>
<summary>摘要</summary>
在机器学习（ML）模型开发生命周期中，使用停滞 dataset 训练候选模型并选择适合任务的最佳模型只是第一步。在实际应用中，已经部署的选定模型后，需要进行连续的模型监测和重新训练。有多种重新训练的原因，包括数据或概念漂移，这可能会影响模型性能，并且可以通过适当的指标来监测。另一个重新训练的动机是随着时间的推移，采集到的数据量的增加，可以重新训练并改进模型性能，即使没有数据漂移。我们研究重新训练决策点对关键因素的影响，如模型性能和资源利用率，并提出了一个参考框架，以设计有效的模型重新训练策略。
</details></li>
</ul>
<hr>
<h2 id="DSR-Diff-Depth-Map-Super-Resolution-with-Diffusion-Model"><a href="#DSR-Diff-Depth-Map-Super-Resolution-with-Diffusion-Model" class="headerlink" title="DSR-Diff: Depth Map Super-Resolution with Diffusion Model"></a>DSR-Diff: Depth Map Super-Resolution with Diffusion Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09919">http://arxiv.org/abs/2311.09919</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuan Shi, Bin Xia, Rui Zhu, Qingmin Liao, Wenming Yang</li>
<li>for: 提高低质量深度图的空间分辨率，用于3D重建、虚拟现实和增强现实等应用。</li>
<li>methods: 利用扩散模型在幽Defaultslat space中生成导航，并将其与高质量颜色图相结合，以实现深度图超分辨。</li>
<li>results: 对比州前方法，提出了一种新的CDSR模型，并实现了较高的准确率和效率。代码将在<a target="_blank" rel="noopener" href="https://github.com/shiyuan7/DSR-Diff%E4%B8%AD%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/shiyuan7/DSR-Diff中发布。</a><details>
<summary>Abstract</summary>
Color-guided depth map super-resolution (CDSR) improve the spatial resolution of a low-quality depth map with the corresponding high-quality color map, benefiting various applications such as 3D reconstruction, virtual reality, and augmented reality. While conventional CDSR methods typically rely on convolutional neural networks or transformers, diffusion models (DMs) have demonstrated notable effectiveness in high-level vision tasks. In this work, we present a novel CDSR paradigm that utilizes a diffusion model within the latent space to generate guidance for depth map super-resolution. The proposed method comprises a guidance generation network (GGN), a depth map super-resolution network (DSRN), and a guidance recovery network (GRN). The GGN is specifically designed to generate the guidance while managing its compactness. Additionally, we integrate a simple but effective feature fusion module and a transformer-style feature extraction module into the DSRN, enabling it to leverage guided priors in the extraction, fusion, and reconstruction of multi-model images. Taking into account both accuracy and efficiency, our proposed method has shown superior performance in extensive experiments when compared to state-of-the-art methods. Our codes will be made available at https://github.com/shiyuan7/DSR-Diff.
</details>
<details>
<summary>摘要</summary>
颜色导航深度地图超分辨 (CDSR) 可以提高低质量深度地图的空间分辨率，有利于多种应用，如三维重建、虚拟现实和增强现实。传统的 CDSR 方法通常采用卷积神经网络或转换器，而扩散模型（DM）则在高级视觉任务中表现出了很好的效果。在这项工作中，我们提出了一种新的 CDSR 模式，利用在潜在空间中的扩散模型来生成指导 depth map 超分辨。我们的方法包括指导生成网络（GGN）、深度地图超分辨网络（DSRN）和指导恢复网络（GRN）。GGN 专门设计用于生成指导，同时管理其 компакт性。此外，我们还将一个简单 yet effective 的特征融合模块和一个基于转换器的特征提取模块integrated into DSRN，使其能够在抽取、融合和重建多模型图像时利用指导PRIORS。考虑到精度和效率，我们提出的方法在广泛的实验中显示出了与状态 искусственный智能方法相比的superior performance。我们的代码将在 GitHub 上发布，请参考 https://github.com/shiyuan7/DSR-Diff.
</details></li>
</ul>
<hr>
<h2 id="INTERVENOR-Prompt-the-Coding-Ability-of-Large-Language-Models-with-the-Interactive-Chain-of-Repairing"><a href="#INTERVENOR-Prompt-the-Coding-Ability-of-Large-Language-Models-with-the-Interactive-Chain-of-Repairing" class="headerlink" title="INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing"></a>INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09868">http://arxiv.org/abs/2311.09868</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/neuir/intervenor">https://github.com/neuir/intervenor</a></li>
<li>paper_authors: Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu</li>
<li>for: 这个论文是为了提出一种基于人类编程行为的自动代码修复方法（INTERVENOR），该方法利用了两个基于大语言模型（LLM）的代理机制，以便在代码修复过程中提高代码生成和翻译能力。</li>
<li>methods: 该方法使用了两个LLM代理机制，namely Code Learner和Code Teacher。Code Learner是根据Code Teacher的指导生成和修复代码，而Code Teacher则是通过编译器的反馈来重新思考代码错误，并生成一个链式修复（CoR）来引导代码修复过程。</li>
<li>results: 我们的实验表明，INTERVENOR比 estado-of-the-art 方法更高效，在代码生成和代码翻译任务中分别提高了约13%和4.5%。我们的进一步分析还表明，CoR可以通过自然语言提供错误原因和解决方案的明确描述。由于编译器的反馈，INTERVENOR可以准确地识别代码中的语法错误和断言错误，并提供精确的修复指令，使LLMs在只需要三次修复后就能达到极限性能。<details>
<summary>Abstract</summary>
This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher, to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher. The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner. Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and solution plans via natural language. Thanks to the feedback of code compilers, INTERVENOR can accurately identify the syntax errors and assertion errors in the code and provide precise instructions to repair codes, making LLMs achieve the plateau performance with only three repairing turns. All data and codes are available at https://github.com/NEUIR/INTERVENOR
</details>
<details>
<summary>摘要</summary>
这个论文提出了一种名为INTERactiVE chaiN Of Repairing（INTERVENOR）的方法，它模仿人类代码修复行为（迭代评估、重新思考和修复），并唤醒LLM的编程能力。具体来说，INTERVENOR使用两个基于LLM的代理人：代码学习者和代码教师。代码学习者根据代码教师的指导生成和修复代码。代码教师根据编译器的反馈重新评估代码错误，并生成了修复过程中的链条（CoR），以引导代码修复过程。我们的实验表明，INTERVENOR在代码生成和代码翻译任务上表现出色，与当前状态的方法相比，提高了约13%和4.5%。我们的进一步分析表明，CoR可以通过自然语言来描述错误原因和解决方案。由于编译器的反馈，INTERVENOR可以准确地识别代码中的语法错误和断言错误，并提供精准的修复指导，使LLM在只需三次修复后达到极限性能。所有数据和代码可以在https://github.com/NEUIR/INTERVENOR上获取。
</details></li>
</ul>
<hr>
<h2 id="PsyBench-a-balanced-and-in-depth-Psychological-Chinese-Evaluation-Benchmark-for-Foundation-Models"><a href="#PsyBench-a-balanced-and-in-depth-Psychological-Chinese-Evaluation-Benchmark-for-Foundation-Models" class="headerlink" title="PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models"></a>PsyBench: a balanced and in-depth Psychological Chinese Evaluation Benchmark for Foundation Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09861">http://arxiv.org/abs/2311.09861</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junlei Zhang, Hongliang He, Nirui Song, Shuyuan He, Shuai Zhang, Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan</li>
<li>for: 这个论文是为了提供一个全面的中文评价集，用于评估基础模型在心理学领域的能力。</li>
<li>methods: 本文使用多选题型的评价方法，以评估基础模型在不同知识领域的表现。</li>
<li>results: 研究发现不同知识领域的表现存在显著差异，而且只有ChatGPT模型的平均准确率超过70%， indicating that there is still room for improvement in this area.<details>
<summary>Abstract</summary>
As Large Language Models (LLMs) are becoming prevalent in various fields, there is an urgent need for improved NLP benchmarks that encompass all the necessary knowledge of individual discipline. Many contemporary benchmarks for foundational models emphasize a broad range of subjects but often fall short in presenting all the critical subjects and encompassing necessary professional knowledge of them. This shortfall has led to skewed results, given that LLMs exhibit varying performance across different subjects and knowledge areas. To address this issue, we present psybench, the first comprehensive Chinese evaluation suite that covers all the necessary knowledge required for graduate entrance exams. psybench offers a deep evaluation of a model's strengths and weaknesses in psychology through multiple-choice questions. Our findings show significant differences in performance across different sections of a subject, highlighting the risk of skewed results when the knowledge in test sets is not balanced. Notably, only the ChatGPT model reaches an average accuracy above $70\%$, indicating that there is still plenty of room for improvement. We expect that psybench will help to conduct thorough evaluations of base models' strengths and weaknesses and assist in practical application in the field of psychology.
</details>
<details>
<summary>摘要</summary>
如Large Language Models（LLMs）在不同领域变得越来越普遍，需要改进的自然语言处理（NLP）标准测试Suite来涵盖所有必要的专业知识。许多当代测试标准 для基础模型通常会忽略一些重要的主题和专业知识，这会导致测试结果偏向。为解决这个问题，我们介绍了psybench，第一个涵盖所有必要的心理学入学考试知识的全面中文评估suite。psybench通过多选题提供了深入的模型强项和弱项评估。我们的发现表明不同主题section的性能存在显著差异，这说明测试集知识不均衡可能会导致偏向测试结果。各种ChatGPT模型的平均准确率超过70%，这表明当前还有很多机会进行改进。我们期望psybench可以帮助进行深入的模型强项和弱项评估，并在心理学领域的实践应用中提供支持。
</details></li>
</ul>
<hr>
<h2 id="SurvTimeSurvival-Survival-Analysis-On-The-Patient-With-Multiple-Visits-Records"><a href="#SurvTimeSurvival-Survival-Analysis-On-The-Patient-With-Multiple-Visits-Records" class="headerlink" title="SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits&#x2F;Records"></a>SurvTimeSurvival: Survival Analysis On The Patient With Multiple Visits&#x2F;Records</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09854">http://arxiv.org/abs/2311.09854</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/davidlee1102/surtimesurvival">https://github.com/davidlee1102/surtimesurvival</a></li>
<li>paper_authors: Hung Le, Ong Eng-Jon, Bober Miroslaw</li>
<li>for: 预测严重疾病患者生存时间的准确预测仍然是艰难的挑战，尽管近年来人工智能技术得到了进一步发展。这项研究推出了“SurvTimeSurvival：survival分析在多次&#x2F;记录数据上”，利用Transformer模型不仅能够处理时间变化的covariates，还能够处理covariates数据。</li>
<li>methods: 我们采用了Transformer模型来处理时间变化的covariates和covariates数据，并解决了生存分析数据集中的数据缺失问题，通过将生成Synthetic数据集成到学习过程中。</li>
<li>results: 我们的方法在covariates和时间变化covariates数据集上都超过了现有的深度学习方法的性能。我们的方法的目的不仅是提高个体患者生存轨迹的理解，从而提高预测精度，而且也在设计临床试验和开发新的治疗方法中发挥重要作用。<details>
<summary>Abstract</summary>
The accurate prediction of survival times for patients with severe diseases remains a critical challenge despite recent advances in artificial intelligence. This study introduces "SurvTimeSurvival: Survival Analysis On Patients With Multiple Visits/Records", utilizing the Transformer model to not only handle the complexities of time-varying covariates but also covariates data. We also tackle the data sparsity issue common to survival analysis datasets by integrating synthetic data generation into the learning process of our model. We show that our method outperforms state-of-the-art deep learning approaches on both covariates and time-varying covariates datasets. Our approach aims not only to enhance the understanding of individual patient survival trajectories across various medical conditions, thereby improving prediction accuracy, but also to play a pivotal role in designing clinical trials and creating new treatments.
</details>
<details>
<summary>摘要</summary>
医学预测患者生存时间的准确性仍然是一项关键挑战，尽管最近的人工智能技术得到了进步。这项研究介绍了“SurvTimeSurvival：基于多次/记录的生存分析”，利用Transformer模型不仅能处理时间变化的共 covariates，而且还能处理 covariates 数据。我们还解决了生存分析数据集中的数据缺失问题通过将生成 Synthetic 数据 incorporated 到我们的模型学习过程中。我们表明我们的方法在 covariates 和时间变化 covariates 数据集上都能够超越当前的深度学习方法。我们的方法不仅可以提高预测准确性，还可以提高对各种医疗情况的个体患者生存轨迹的理解，从而为设计临床试验和开发新药物做出重要贡献。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-LLMs-in-Scholarly-Knowledge-Graph-Question-Answering"><a href="#Leveraging-LLMs-in-Scholarly-Knowledge-Graph-Question-Answering" class="headerlink" title="Leveraging LLMs in Scholarly Knowledge Graph Question Answering"></a>Leveraging LLMs in Scholarly Knowledge Graph Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09841">http://arxiv.org/abs/2311.09841</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/huntila/scholarly-kgqa">https://github.com/huntila/scholarly-kgqa</a></li>
<li>paper_authors: Tilahun Abedissa Taffa, Ricardo Usbeck</li>
<li>for: 这篇论文旨在提出一种学术知识图问答系统，可以通过几 shot 的方式回答 bibliographic 自然语言问题。</li>
<li>methods: 该模型首先使用 BERT 基于 sentence encoder 将测试问题与training问题进行相似性比较，然后选择 top-n 相似问题对应的 SPARQL，并将这些对应的问题作为示例，将测试问题作为提示， passing it to LLM 生成 SPARQL。最后，对于underlying KG （ORKG）终端进行查询，并返回答案。</li>
<li>results: 该系统在 SciQA 中实现了 F1 分数 99.0%，在 Scholarly-QALD-23 挑战 benchmark 上表现出色。<details>
<summary>Abstract</summary>
This paper presents a scholarly Knowledge Graph Question Answering (KGQA) that answers bibliographic natural language questions by leveraging a large language model (LLM) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a BERT-based sentence encoder and retrieves their corresponding SPARQL. Using the top-n similar question-SPARQL pairs as an example and the test question creates a prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs the SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and returns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of the Scholarly-QALD-23 challenge benchmarks.
</details>
<details>
<summary>摘要</summary>
这篇论文提出了一种学术知识图问答系统（KGQA），该系统可以通过几个尝试回答文学性问题，使用大型语言模型（LLM）。系统首先使用BERT基于的句子编码器来将测试问题与相似训练问题进行对比，然后使用相似问题-SPARQL对应的最上层对象来生成一个提示。最后，将提示传递给LLM进行生成SPARQL，并将其运行于基础知识图（ORKG）终端，以获得答案。我们的系统在SciQA中的F1分数达99.0%。
</details></li>
</ul>
<hr>
<h2 id="PELMS-Pre-training-for-Effective-Low-Shot-Multi-Document-Summarization"><a href="#PELMS-Pre-training-for-Effective-Low-Shot-Multi-Document-Summarization" class="headerlink" title="PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization"></a>PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09836">http://arxiv.org/abs/2311.09836</a></li>
<li>repo_url: None</li>
<li>paper_authors: Joseph J. Peper, Wenzhao Qiu, Lu Wang</li>
<li>for: 本研究旨在提高摘要文献的抽象性和反射性，即通过多文摘要来提高文献的简洁、流畅和准确性。</li>
<li>methods: 我们提出了一种基于语义相关性规则和准确性约束的预训练模型，并使用了多个文档输入来支持模型的训练。</li>
<li>results: 我们在多种摘要任务上进行了广泛的评估，并经验显示了我们的方法在低shot设定下可以准确地捕捉摘要文献的主题和含义，并且在抽象性、流畅性、准确性和 faithfulness 等方面具有优异性。<details>
<summary>Abstract</summary>
We investigate pre-training techniques for abstractive multi-document summarization (MDS), which is much less studied than summarizing single documents. Though recent work has demonstrated the effectiveness of highlighting information salience for pre-training strategy design, it struggles to generate abstractive and reflective summaries, which are critical properties for MDS. To this end, we present PELMS, a pre-trained model that uses objectives based on semantic coherence heuristics and faithfulness constraints with un-labeled multi-document inputs, to promote the generation of concise, fluent, and faithful summaries. To support the training of PELMS, we compile MultiPT, a multi-document pre-training corpus containing over 93 million documents to form more than 3 million unlabeled topic-centric document clusters, covering diverse genres such as product reviews, news, and general knowledge. We perform extensive evaluation of PELMS in low-shot settings on a wide range of MDS datasets. Our approach consistently outperforms competitive comparisons with respect to overall informativeness, abstractiveness, coherence, and faithfulness.
</details>
<details>
<summary>摘要</summary>
我团队研究了抽象多文摘要（MDS）的预训练技术，这个领域比单文摘要更为少studied。虽然 latest work 表明了突出信息重要性的预训练策略的效果，但它很难生成抽象和反射的摘要，这些特性是MDS的关键性能。为此，我们提出了 PELMS，一种预训练模型，使用基于 semantic coherence heuristics 和 faithfulness constraints 的目标函数，以便在无标签多文输入下生成简洁、流畅、忠实的摘要。为支持 PELMS 的训练，我们编译了 MultiPT，一个包含超过 93 万个文档的多文预训练集，其中包含多种类型的文档，如产品评论、新闻和通用知识。我们对 PELMS 进行了广泛的评估，包括低投入设定下的评估，并在多种 MDS 数据集上表现出consistent 的优异性。
</details></li>
</ul>
<hr>
<h2 id="ML-Bench-Large-Language-Models-Leverage-Open-source-Libraries-for-Machine-Learning-Tasks"><a href="#ML-Bench-Large-Language-Models-Leverage-Open-source-Libraries-for-Machine-Learning-Tasks" class="headerlink" title="ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks"></a>ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09835">http://arxiv.org/abs/2311.09835</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu, Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan Hu, Zengxian Yang, Kaikai An, Ruijun Huang, Shuzheng Si, Sheng Chen, Haozhe Zhao, Zhengliang Li, Liang Chen, Yiming Zong, Yan Wang, Tianyu Liu, Zhiwei Jiang, Baobao Chang, Yujia Qin, Wangchunshu Zhou, Yilun Zhao, Arman Cohan, Mark Gerstein</li>
<li>for: 本研究旨在评估大语言模型在使用开源库完成机器学习任务时的实用性。</li>
<li>methods: 本研究提出了一个新的评估Setup，其中大语言模型使用开源库完成机器学习任务，而不是从scratch编写代码。研究者们制定了ML-Bench，一个包含10044个样例和130个任务的广泛的benchmark，以评估大语言模型在使用开源库时的效果。</li>
<li>results: GPT-4在这些任务中表现出色，但只有39.73%的任务得到了成功。研究者们提出了ML-Agent，一种用于快速浏览代码库，找到相关文档、代码和执行代码的方法。与GPT-4结合使用后，ML-Agent得到了进一步的改进。<details>
<summary>Abstract</summary>
Large language models have shown promising performance in code generation benchmarks. However, a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming's reliance on pre-existing libraries. Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks. Therefore, we propose ML-Bench, an expansive benchmark developed to assess the effectiveness of LLMs in leveraging existing functions in open-source libraries. Consisting of 10044 samples spanning 130 tasks over 14 notable machine learning GitHub repositories. In this setting, given a specific machine learning task instruction and the accompanying README in a codebase, an LLM is tasked to generate code to accomplish the task. This necessitates the comprehension of long and language-code interleaved documents, as well as the understanding of complex cross-file code structures, introducing new challenges. Notably, while GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73\% of the tasks, leaving a huge space for improvement. We address these challenges by proposing ML-Agent, designed to effectively navigate the codebase, locate documentation, retrieve code, and generate executable code. Empirical results demonstrate that ML-Agent, built upon GPT-4, results in further improvements. Code, data, and models are available at \url{https://ml-bench.github.io/}.
</details>
<details>
<summary>摘要</summary>
大型语言模型在代码生成比赛中表现出了优异的成绩。然而，实际Programming中对于这些模型的实用性存在许多差距，主要是因为实际程式码中的对于预存函数的依赖。相反于评估这些模型在从零开始写代码的能力，这个工作尝试了一个新的评估设置，让模型使用公开源代码库中的函数来完成机器学习任务。因此，我们提出了 ML-Bench，一个包含10044个样本、130个任务和14个知名机器学习GitHub来源的广泛的库。在这个设置下，给定一个机器学习任务的指令和相应的README档案，一个模型需要使用代码库中的函数来完成任务。这需要理解长长的文档和代码档案之间的交互，以及复杂的跨档代码结构，带来新的挑战。对此，我们提出了 ML-Agent，用于有效地浏览代码库、找到文档、获取代码和实现可执行的代码。实验结果显示，使用 GPT-4 建立的 ML-Agent 导致了进一步的改善。代码、数据和模型可以在 \url{https://ml-bench.github.io/} 获取。
</details></li>
</ul>
<hr>
<h2 id="AutoPlanBench-Automatically-generating-benchmarks-for-LLM-planners-from-PDDL"><a href="#AutoPlanBench-Automatically-generating-benchmarks-for-LLM-planners-from-PDDL" class="headerlink" title="AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL"></a>AutoPlanBench: : Automatically generating benchmarks for LLM planners from PDDL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09830">http://arxiv.org/abs/2311.09830</a></li>
<li>repo_url: None</li>
<li>paper_authors: Katharina Stein, Alexander Koller</li>
<li>for: 这paper的目的是探讨LLMs在规划任务中的能力，以及如何使用文本描述来评估这些能力。</li>
<li>methods: 这paper使用了一种新的方法，可以将PDDL中的规划benchmark转换成文本描述，并提供了一个新的benchmark集。</li>
<li>results: 研究发现，当今最好的LLM规划器在许多规划任务上表现出色，但是其他任务仍然远远超出了现有方法的能力范围。<details>
<summary>Abstract</summary>
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on many planning tasks, others remain out of reach of current methods.
</details>
<details>
<summary>摘要</summary>
LLMs 正在越来越多地用于计划样式的任务，但它们的计划和理解能力尚未得到充分的理解。我们提出了一种新的方法，可以自动将 PDDL 格式的计划标准转换成文本描述，并提供了我们创建的 benchmark 数据集。我们发现，当前的 LLM 计划器在许多计划任务上表现出色，但有些任务仍然超出当前的能力范围。
</details></li>
</ul>
<hr>
<h2 id="PWISeg-Point-based-Weakly-supervised-Instance-Segmentation-for-Surgical-Instruments"><a href="#PWISeg-Point-based-Weakly-supervised-Instance-Segmentation-for-Surgical-Instruments" class="headerlink" title="PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments"></a>PWISeg: Point-based Weakly-supervised Instance Segmentation for Surgical Instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09819">http://arxiv.org/abs/2311.09819</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/seanxuu/PWISeg">https://github.com/seanxuu/PWISeg</a></li>
<li>paper_authors: Zhen Sun, Huan Xu, Jinlin Wu, Zhen Chen, Zhen Lei, Hongbin Liu</li>
<li>for: 这个论文的目的是提出一种新的、有效的医疗器械实例分割方法，以解决实际中获得mask-level注释是劳动密集的问题。</li>
<li>methods: 该方法基于FCN架构，包括点到方框和点到面分支，用于模型特征点和 bounding box 之间的关系，以及特征点和分割面之间的关系。</li>
<li>results: 该方法在我们所提供的新的医疗器械数据集上进行了广泛的试验，并证明了与大多数无监督 bounding box 的实例分割方法相比，它的实rument分割精度得到了显著提高。<details>
<summary>Abstract</summary>
In surgical procedures, correct instrument counting is essential. Instance segmentation is a location method that locates not only an object's bounding box but also each pixel's specific details. However, obtaining mask-level annotations is labor-intensive in instance segmentation. To address this issue, we propose a novel yet effective weakly-supervised surgical instrument instance segmentation approach, named Point-based Weakly-supervised Instance Segmentation (PWISeg). PWISeg adopts an FCN-based architecture with point-to-box and point-to-mask branches to model the relationships between feature points and bounding boxes, as well as feature points and segmentation masks on FPN, accomplishing instrument detection and segmentation jointly in a single model. Since mask level annotations are hard to available in the real world, for point-to-mask training, we introduce an unsupervised projection loss, utilizing the projected relation between predicted masks and bboxes as supervision signal. On the other hand, we annotate a few pixels as the key pixel for each instrument. Based on this, we further propose a key pixel association loss and a key pixel distribution loss, driving the point-to-mask branch to generate more accurate segmentation predictions. To comprehensively evaluate this task, we unveil a novel surgical instrument dataset with manual annotations, setting up a benchmark for further research. Our comprehensive research trial validated the superior performance of our PWISeg. The results show that the accuracy of surgical instrument segmentation is improved, surpassing most methods of instance segmentation via weakly supervised bounding boxes. This improvement is consistently observed in our proposed dataset and when applied to the public HOSPI-Tools dataset.
</details>
<details>
<summary>摘要</summary>
在手术过程中，正确的工具数量是非常重要的。实例分割是一种位置方法，可以不仅找到物体的包围盒，还可以每个像素的特定细节。然而，在实例分割中获得mask水平的注释是很劳动密集的。为解决这个问题，我们提出了一种新的但有效的weakly-supervised手术工具实例分割方法，名为Point-based Weakly-supervised Instance Segmentation（PWISeg）。PWISeg采用了FCN基 architecture，并设置了点到包围盒和点到面积分支，以模型特征点和包围盒之间的关系，以及特征点和分割面积之间的关系。这样可以同时完成工具检测和分割。由于mask水平的注释很难在实际世界中获得，为点到面训练，我们引入了一种无监督投影损失，利用预测的面积和包围盒之间的投影关系作为监督信号。此外，我们还标注了每个工具的一些键点，并基于这些键点，我们进一步提出了键点协会损失和键点分布损失，使点到面分支生成更加准确的分割预测。为全面评估这个任务，我们披露了一个新的手术工具数据集，并设置了一个标准的比较基准。我们的全面研究试验证明了PWISeg的超越性。结果表明，通过我们提出的PWISeg，手术工具分割的准确性得到了提高，超越了大多数基于weakly supervised bounding box的实例分割方法。这种改进是在我们所提出的数据集和公共HOSPI-Tools数据集上均可见。
</details></li>
</ul>
<hr>
<h2 id="MedAgents-Large-Language-Models-as-Collaborators-for-Zero-shot-Medical-Reasoning"><a href="#MedAgents-Large-Language-Models-as-Collaborators-for-Zero-shot-Medical-Reasoning" class="headerlink" title="MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning"></a>MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10537">http://arxiv.org/abs/2311.10537</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/gersteinlab/medagents">https://github.com/gersteinlab/medagents</a></li>
<li>paper_authors: Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein</li>
<li>for: 本研究旨在提高语言模型在医学领域的表现，并且提供一种可行的多学科合作框架，以便语言模型可以更好地理解和应用医学知识。</li>
<li>methods: 本研究使用了多学科合作框架，其包括五个关键步骤：寻找培训数据，提出个人分析，汇总分析成报告，进行多轮讨论，并最终做出决策。</li>
<li>results: 本研究在九个数据集（MedQA、MedMCQA、PubMedQA以及MMLU中的六个子任务）上得到了出色的结果，证明了我们提出的多学科合作框架可以帮助语言模型更好地理解和应用医学知识，同时还可以扩展其理解能力。<details>
<summary>Abstract</summary>
Large Language Models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and the reasoning over specialized knowledge. To address these obstinate issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages role-playing LLM-based agents who participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free and interpretable framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work particularly focuses on the zero-shot scenario, our results on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise in LLMs, as well as extending its reasoning abilities. Based on these outcomes, we further conduct a human evaluation to pinpoint and categorize common errors within our method, as well as ablation studies aimed at understanding the impact of various factors on overall performance. Our code can be found at \url{https://github.com/gersteinlab/MedAgents}.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM），尽管在不同领域中做出了惊人的进步，在医疗领域还是遇到了很多障碍。这个领域面临着域名特定的术语和专业知识的推理等独特问题。为了解决这些难题，我们提出了一种新的多学科合作（MC）框架，该框架通过多个角色扮演LLM基于的代理人参与协同多轮讨论，从而提高LLM的技能和推理能力。这个无需训练和可解释的框架包括五个关键步骤：召集域专家、提出个人分析、汇总分析成报告、讨论迭代 until 达成一致，并最终做出决策。我们的工作特别关注于零容量情况下的情况，我们的结果表明，我们提出的MC框架在激活和利用医疗领域LLM的专业知识，以及扩展其推理能力方面表现出色。基于这些结果，我们进一步进行了人类评估，以找到和分类我们方法中的常见错误，以及进行了缺失因素的研究，以了解对总性表现的影响。我们的代码可以在 \url{https://github.com/gersteinlab/MedAgents} 找到。
</details></li>
</ul>
<hr>
<h2 id="Performance-Trade-offs-of-Watermarking-Large-Language-Models"><a href="#Performance-Trade-offs-of-Watermarking-Large-Language-Models" class="headerlink" title="Performance Trade-offs of Watermarking Large Language Models"></a>Performance Trade-offs of Watermarking Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09816">http://arxiv.org/abs/2311.09816</a></li>
<li>repo_url: None</li>
<li>paper_authors: Anirudh Ajith, Sameer Singh, Danish Pruthi</li>
<li>for: 本研究旨在评估水印模型在多种任务上的性能，以了解水印的影响和用户应该考虑的贸易OFF。</li>
<li>methods: 本研究使用了水印策略，在生成文本中嵌入一个信号，以便区分人工生成的文本和模型生成的文本。</li>
<li>results: 研究发现，在大多数情况下，水印对任务的性能没有显著影响。但是，长形生成任务（如概要和翻译）的性能下降了15-20%。这些结果指出了水印使用的贸易OFF，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Amidst growing concerns of large language models (LLMs) being misused for generating misinformation or completing homework assignments, watermarking has emerged as an effective solution for distinguishing human-written and LLM-generated text. A prominent watermarking strategy is to embed a signal into generated text by upsampling a (pseudorandomly-chosen) subset of tokens at every generation step. Although this signal is imperceptible to a human reader, it is detectable through statistical testing. However, implanting such signals alters the model's output distribution and can have unintended effects when watermarked LLMs are used for downstream applications. In this work, we evaluate the performance of watermarked LLMs on a diverse suite of tasks, including text classification, textual entailment, reasoning, question answering, translation, summarization, and language modeling. We find that watermarking has negligible impact on the performance of tasks posed as k-class classification problems in the average case. However, the accuracy can plummet to that of a random classifier for some scenarios (that occur with non-negligible probability). Tasks that are cast as multiple-choice questions and short-form generation are surprisingly unaffected by watermarking. For long-form generation tasks, including summarization and translation, we see a drop of 15-20% in the performance due to watermarking. Our findings highlight the trade-offs that users should be cognizant of when using watermarked models, and point to cases where future research could improve existing trade-offs.
</details>
<details>
<summary>摘要</summary>
在大型语言模型（LLM）被违用于生成谣言或完成作业任务时，水印技术已成为一种有效的解决方案，以区分人类写作和LLM生成的文本。一种常见的水印策略是在生成文本时附加一个信号，通过随机选择一部分token进行upsampling。尽管这个信号对人类读者无法察觉，但可以通过统计测试探测。然而，植入这个信号会改变模型的输出分布，可能会导致下游应用中的不良影响。在这个工作中，我们评估水印LLM在多种任务上的表现，包括文本分类、文本推理、问答、翻译、概要和语言模型。我们发现，对于大多数情况，水印对任务的性能没有显著影响。然而，在某些特殊情况下（占总体的非致命概率），水印可能会导致性能降低到随机分类器水平。在长形生成任务中，如概要和翻译，我们发现水印导致性能下降约15-20%。我们的发现指出了使用水印模型时的交易offs，并提出了未来研究可以改善现有交易offs的可能性。
</details></li>
</ul>
<hr>
<h2 id="Towards-Formal-Fault-Injection-for-Safety-Assessment-of-Automated-Systems"><a href="#Towards-Formal-Fault-Injection-for-Safety-Assessment-of-Automated-Systems" class="headerlink" title="Towards Formal Fault Injection for Safety Assessment of Automated Systems"></a>Towards Formal Fault Injection for Safety Assessment of Automated Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09810">http://arxiv.org/abs/2311.09810</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashfaq Farooqui, Behrooz Sangchoolie</li>
<li>for: 这篇论文是为了探讨自动化系统中的安全性、安全性和其他可靠性特性的问题，以便在日常生活中广泛应用自动化系统。</li>
<li>methods: 这篇论文使用了正式方法，这些方法可以数学方式推导系统的行为，从而确保系统的可靠性。然而，这些方法通常只适用于系统抽象模型，可能不能完全反映实际系统。</li>
<li>results: 这篇论文提出了正式缺陷插入，一种将正式方法和缺陷插入相结合的技术，以提高自动化系统的可靠性。文章还讨论了这些技术在开发生命周期中的潜在优势，并提出了未来研究的可能性，以解决当前的挑战。<details>
<summary>Abstract</summary>
Reasoning about safety, security, and other dependability attributes of autonomous systems is a challenge that needs to be addressed before the adoption of such systems in day-to-day life. Formal methods is a class of methods that mathematically reason about a system's behavior. Thus, a correctness proof is sufficient to conclude the system's dependability. However, these methods are usually applied to abstract models of the system, which might not fully represent the actual system. Fault injection, on the other hand, is a testing method to evaluate the dependability of systems. However, the amount of testing required to evaluate the system is rather large and often a problem. This vision paper introduces formal fault injection, a fusion of these two techniques throughout the development lifecycle to enhance the dependability of autonomous systems. We advocate for a more cohesive approach by identifying five areas of mutual support between formal methods and fault injection. By forging stronger ties between the two fields, we pave the way for developing safe and dependable autonomous systems. This paper delves into the integration's potential and outlines future research avenues, addressing open challenges along the way.
</details>
<details>
<summary>摘要</summary>
考虑 autonomous systems 的安全性、安全性和其他可靠性特性的推理是在普及这些系统之前需要解决的挑战。Formal methods 是一类方法，通过数学方式推理系统的行为，因此，一个正确性证明即可确保系统的可靠性。然而，这些方法通常应用于系统抽象模型，可能不完全反映实际系统。错误插入测试是一种评估系统可靠性的方法，但测试量很大，经常成为问题。这篇视野论文介绍了形式错误插入，它将这两种技术在开发生命周期中融合，以提高自动化系统的可靠性。我们认为这两个领域之间存在五个互助领域，通过加强这两个领域之间的关系，我们开拓了开发安全可靠的自动化系统的可能性。这篇论文探讨了融合的潜在可能性和未来研究方向，并解决了一些开放的挑战。
</details></li>
</ul>
<hr>
<h2 id="Comparing-Differentiable-Logics-for-Learning-Systems-A-Research-Preview"><a href="#Comparing-Differentiable-Logics-for-Learning-Systems-A-Research-Preview" class="headerlink" title="Comparing Differentiable Logics for Learning Systems: A Research Preview"></a>Comparing Differentiable Logics for Learning Systems: A Research Preview</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09809">http://arxiv.org/abs/2311.09809</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tflinkow/dl-comparison">https://github.com/tflinkow/dl-comparison</a></li>
<li>paper_authors: Thomas Flinkow, Barak A. Pearlmutter, Rosemary Monahan</li>
<li>for: 这篇论文旨在研究如何使机器学习（ML）系统满足正确性和安全性要求，并考虑了自动化系统的自我更新和适应能力。</li>
<li>methods: 论文使用了 differentiable logics 方法，其中 Background Knowledge 编码为逻辑约束，导引学习过程。</li>
<li>results: 实验结果与文献报道的结果相符，但是使用 differentiable logics 引入了一个新的 гиперпарамет，即 tuning 难度和影响力。<details>
<summary>Abstract</summary>
Extensive research on formal verification of machine learning (ML) systems indicates that learning from data alone often fails to capture underlying background knowledge. A variety of verifiers have been developed to ensure that a machine-learnt model satisfies correctness and safety properties, however, these verifiers typically assume a trained network with fixed weights. ML-enabled autonomous systems are required to not only detect incorrect predictions, but should also possess the ability to self-correct, continuously improving and adapting. A promising approach for creating ML models that inherently satisfy constraints is to encode background knowledge as logical constraints that guide the learning process via so-called differentiable logics. In this research preview, we compare and evaluate various logics from the literature in weakly-supervised contexts, presenting our findings and highlighting open problems for future work. Our experimental results are broadly consistent with results reported previously in literature; however, learning with differentiable logics introduces a new hyperparameter that is difficult to tune and has significant influence on the effectiveness of the logics.
</details>
<details>
<summary>摘要</summary>
根据大量研究，机器学习（ML）系统从数据alone学习时常常无法捕捉下面背景知识。为确保机器学习模型满足正确性和安全性质量，许多验证工具已经开发，但这些验证工具通常假设已经训练过的网络重量是固定的。ML自适应系统需要不仅检测错误预测，还应该具备自我更新和适应能力。一种有前途的方法是通过 différentiable logics将背景知识编码成逻辑约束，以导引学习过程。在这个研究预览中，我们对文献中的不同逻辑进行比较和评价，在弱监督上下文中展示我们的发现和挑出未来工作的开放问题。我们的实验结果与文献中已经报道的结果相符，但是学习与 diffe抽象逻辑引入了一个新的Hyperparameter，它具有很大的影响力和难于调整。
</details></li>
</ul>
<hr>
<h2 id="Neuro-Symbolic-Integration-Brings-Causal-and-Reliable-Reasoning-Proofs"><a href="#Neuro-Symbolic-Integration-Brings-Causal-and-Reliable-Reasoning-Proofs" class="headerlink" title="Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs"></a>Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09802">http://arxiv.org/abs/2311.09802</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/damo-nlp-sg/caring">https://github.com/damo-nlp-sg/caring</a></li>
<li>paper_authors: Sen Yang, Xin Li, Leyang Cui, Lidong Bing, Wai Lam</li>
<li>for: 这 paper 是为了提高 AI 模型的推理能力和可靠性而写的。</li>
<li>methods: 这 paper 使用了一种 combining 方法，即将 neural LLM 和 symbolic solver  integrate 在一起，以便进行 deliberative reasoning 和证明。</li>
<li>results: 实验表明，使用这种方法可以在 ProofWriter 和 GSM8K 上大幅提高推理 accuracy 和证明相似性。<details>
<summary>Abstract</summary>
Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge. Specifically, our customized meta-interpreters allow the production of reasoning proofs and support flexible search strategies. These reasoning proofs are ensured to be causal and reliable because of the deterministic executing nature of the symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT baseline by nearly double in accuracy and more than triple in proof similarity. On GSM8K, our method also shows accuracy improvements and nearly doubled proof similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing
</details>
<details>
<summary>摘要</summary>
尽管通过不同的逻辑结构让LLMs进行推理生成推理证明和答案，但这些证明并不能保证是 causal 和可靠的，因为 LLMS 本身存在一些缺陷。为了解决这些问题，我们提出了一种神经符号 интеграция方法，在这种方法中，一个神经网络 LLM 用于表示问题的知识，而另一个 LLM-free 符号分析器用于进行思考和推理。具体来说，我们自定义的 meta-interpreters 允许生成推理证明和支持灵活的搜索策略。由于符号分析器的 deterministic 执行特性，所生成的推理证明是 causal 和可靠的。在 ProofWriter 上，我们的方法比 CoT 基线高出 nearly double 的准确率和 более triple 的证明相似度。在 GSM8K 上，我们的方法也显示了准确率上的提高和证明相似度的近 double。我们的代码在 <https://github.com/DAMO-NLP-SG/CaRing> 上发布。
</details></li>
</ul>
<hr>
<h2 id="Interpreting-User-Requests-in-the-Context-of-Natural-Language-Standing-Instructions"><a href="#Interpreting-User-Requests-in-the-Context-of-Natural-Language-Standing-Instructions" class="headerlink" title="Interpreting User Requests in the Context of Natural Language Standing Instructions"></a>Interpreting User Requests in the Context of Natural Language Standing Instructions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09796">http://arxiv.org/abs/2311.09796</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikita Moghe, Patrick Xia, Jacob Andreas, Jason Eisner, Benjamin Van Durme, Harsh Jhamtani</li>
<li>for: 用于提高自然语言界面的用户体验，减少用户重复提供偏好信息的需求。</li>
<li>methods: 利用大型语言模型（LLMs）和自然语言描述文本（standing instructions）作为用户偏好和指令的补充信息。</li>
<li>results: 在NLSI语料集上进行实验，使用大语言模型和不同的检索方法，最高达44.7%的精确匹配率。<details>
<summary>Abstract</summary>
Users of natural language interfaces, generally powered by Large Language Models (LLMs),often must repeat their preferences each time they make a similar request. To alleviate this, we propose including some of a user's preferences and instructions in natural language -- collectively termed standing instructions -- as additional context for such interfaces. For example, when a user states I'm hungry, their previously expressed preference for Persian food will be automatically added to the LLM prompt, so as to influence the search for relevant restaurants. We develop NLSI, a language-to-program dataset consisting of over 2.4K dialogues spanning 17 domains, where each dialogue is paired with a user profile (a set of users specific standing instructions) and corresponding structured representations (API calls). A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue. NLSI contains diverse phenomena, from simple preferences to interdependent instructions such as triggering a hotel search whenever the user is booking tickets to an event. We conduct experiments on NLSI using prompting with large language models and various retrieval approaches, achieving a maximum of 44.7% exact match on API prediction. Our results demonstrate the challenges in identifying the relevant standing instructions and their interpretation into API calls.
</details>
<details>
<summary>摘要</summary>
用户们使用自然语言界面，通常需要每次发出相似的请求都重复他们的首选项。为了解决这个问题，我们建议将用户的首选项和指令（总称为“站坐指令”）作为自然语言 interfaces 的附加上下文。例如，当用户说 “我饿” 时，他们之前表达的波斯料食物的首选项将自动添加到 LLM 提示中，以影响搜索相关餐厅。我们开发了 NLSI，一个语言到程序数据集，包含了超过 2.4K 对话，涵盖 17 个领域，每个对话都与用户的 Profile（用户特定的站坐指令）和相应的结构化表示（API 调用）一起出现。在 NLSI 中，一个主要挑战是确定哪些站坐指令适用于给定的对话。NLSI 包含了多种现象，从简单的首选项到互相关联的指令，例如在购买票务时自动触发酒店搜索。我们使用 LLM 和不同的检索方法进行实验，最高达 44.7% 精确匹配 API 预测。我们的结果表明了站坐指令的适用和其 интерпретация成 API 调用的挑战。
</details></li>
</ul>
<hr>
<h2 id="Breaking-Boundaries-Balancing-Performance-and-Robustness-in-Deep-Wireless-Traffic-Forecasting"><a href="#Breaking-Boundaries-Balancing-Performance-and-Robustness-in-Deep-Wireless-Traffic-Forecasting" class="headerlink" title="Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting"></a>Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09790">http://arxiv.org/abs/2311.09790</a></li>
<li>repo_url: None</li>
<li>paper_authors: Romain Ilbert, Thai V. Hoang, Zonghua Zhang, Themis Palpanas</li>
<li>for: 本研究旨在寻找一种能够平衡精度和Robustness的时序预测方法，以便在真实世界中的电信数据上进行预测。</li>
<li>methods: 我们使用了一种混合策略，包括一个分类器来检测外延攻击，一个去噪器来消除外延数据样本中的噪声，以及一个标准预测模型。我们对这些策略进行了比较，并与两种现有的对抗训练算法进行了比较。</li>
<li>results: 我们的hybrid策略在 both clean和外延数据上表现出色，其MSE在clean数据上保持了92.02%的原始预测模型性能，而在外延数据上则更加Robust，其MSE比较方法的MSE低出2.71倍和2.51倍。此外，我们的模型的组件可以并行训练，从而提高计算效率。<details>
<summary>Abstract</summary>
Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using $\ell_{\infty}$-norm, $\in [0.1,0.4]$. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to $92.02\%$ the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71$\times$ and 2.51$\times$ lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.
</details>
<details>
<summary>摘要</summary>
平衡精度和Robustness之间的贸易OFF是时间序列预测领域的长standing挑战。大多数现有的Robust算法在干净数据上达到了一定的下行性，但在数据抖动的情况下维持同样的性能很难。在这篇论文中，我们研究了各种抖动enario并提出了新的防御机制，使用实际的电信数据对抗 adversarial 攻击。我们与两种现有的 adversarial 训练算法进行比较，使用 $[0.1,0.4]$ 的 $\ell_{\infty}$ 范围内的最大允许抖动。我们的发现显示，我们的混合策略，包括一个分类器来检测 adversarial 示例，一个去噪器来除掉抖动数据示例中的噪声，以及一个标准预测器，在干净数据和抖动数据上都能够达到最好的性能。我们的优化模型可以保持原始预测模型的 $92.02\%$ 的性能（按照 Mean Squared Error 的评价），而且在抖动数据上比标准 adversarial 训练模型更加Robust。它的 MSE 值分别为 $2.71\times$ 和 $2.51\times$ 比对应模型的 MSE 值更低。此外，我们的模型组件可以并行训练，从而提高计算效率。我们的结果表明，我们可以通过改进分类器和去噪器来优化 forecasting 模型，甚至在抖动数据上进行高级和破坏性攻击。
</details></li>
</ul>
<hr>
<h2 id="3vLTL-A-Tool-to-Generate-Automata-for-Three-valued-LTL"><a href="#3vLTL-A-Tool-to-Generate-Automata-for-Three-valued-LTL" class="headerlink" title="3vLTL: A Tool to Generate Automata for Three-valued LTL"></a>3vLTL: A Tool to Generate Automata for Three-valued LTL</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09787">http://arxiv.org/abs/2311.09787</a></li>
<li>repo_url: None</li>
<li>paper_authors: Francesco Belardinelli, Angelo Ferrando, Vadim Malvone</li>
<li>for: 这篇论文主要是为了提供一个工具来生成 Buchi 自动机，用于验证 Linear-time Temporal Logic（LTL）式语言中的多值规定。</li>
<li>methods: 该工具使用Linear-time Temporal Logic（LTL）式语言中的三值 semantics来解释 formulas，并生成一个 Buchi 自动机，用于验证 LTL 式是否真、假或未定义于一个模型中。</li>
<li>results: 该工具可以生成一个 Buchi 自动机，用于验证 LTL 式的真假性，并且可以让这个自动机被第三方库处理，以便进一步进行验证。<details>
<summary>Abstract</summary>
Multi-valued logics have a long tradition in the literature on system verification, including run-time verification. However, comparatively fewer model-checking tools have been developed for multi-valued specification languages. We present 3vLTL, a tool to generate Buchi automata from formulas in Linear-time Temporal Logic (LTL) interpreted on a three-valued semantics. Given an LTL formula, a set of atomic propositions as the alphabet for the automaton, and a truth value, our procedure generates a Buchi automaton that accepts all the words that assign the chosen truth value to the LTL formula. Given the particular type of the output of the tool, it can also be seamlessly processed by third-party libraries in a natural way. That is, the Buchi automaton can then be used in the context of formal verification to check whether an LTL formula is true, false, or undefined on a given model.
</details>
<details>
<summary>摘要</summary>
多值逻辑在系统验证文献中有很长的传统，包括运行时验证。然而，相比之下， fewer model-checking工具被开发用于多值规定语言。我们介绍了3vLTL，一种生成 Buchi 自动机从Linear-time Temporal Logic（LTL）在三值 semantics中解释的方程的工具。给定一个 LTL 方程，一个字母集，一个真假值，我们的过程生成一个 Buchi 自动机，接受将选择的真假值分配给 LTL 方程的所有词。给出特定输出的类型，这个 Buchi 自动机可以轻松地处理第三方库中的自然方式。也就是说，Buch i自动机可以在正式验证中使用，以验证一个 LTL 方程是否真、假或未定义于一个模型。
</details></li>
</ul>
<hr>
<h2 id="Correct-by-Construction-Control-for-Stochastic-and-Uncertain-Dynamical-Models-via-Formal-Abstractions"><a href="#Correct-by-Construction-Control-for-Stochastic-and-Uncertain-Dynamical-Models-via-Formal-Abstractions" class="headerlink" title="Correct-by-Construction Control for Stochastic and Uncertain Dynamical Models via Formal Abstractions"></a>Correct-by-Construction Control for Stochastic and Uncertain Dynamical Models via Formal Abstractions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09786">http://arxiv.org/abs/2311.09786</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thom Badings, Nils Jansen, Licio Romao, Alessandro Abate</li>
<li>for: 这篇论文的目的是为Autonomous Systems的自动化控制器的批量生成提供了一种可靠的方法。</li>
<li>methods: 这篇论文使用了一种基于Markov决策过程的间接抽象方法，以及现有的验证技术来计算一个满足给定规范的优化策略。</li>
<li>results: 这篇论文的结果表明，通过使用这种方法，可以生成一个可靠地满足规范的控制器，并且可以证明这个控制器满足规范的 garanties。<details>
<summary>Abstract</summary>
Automated synthesis of correct-by-construction controllers for autonomous systems is crucial for their deployment in safety-critical scenarios. Such autonomous systems are naturally modeled as stochastic dynamical models. The general problem is to compute a controller that provably satisfies a given task, represented as a probabilistic temporal logic specification. However, factors such as stochastic uncertainty, imprecisely known parameters, and hybrid features make this problem challenging. We have developed an abstraction framework that can be used to solve this problem under various modeling assumptions. Our approach is based on a robust finite-state abstraction of the stochastic dynamical model in the form of a Markov decision process with intervals of probabilities (iMDP). We use state-of-the-art verification techniques to compute an optimal policy on the iMDP with guarantees for satisfying the given specification. We then show that, by construction, we can refine this policy into a feedback controller for which these guarantees carry over to the dynamical model. In this short paper, we survey our recent research in this area and highlight two challenges (related to scalability and dealing with nonlinear dynamics) that we aim to address with our ongoing research.
</details>
<details>
<summary>摘要</summary>
自动生成正确性承诺控制器是自主系统的部署中关键的一步。这些自主系统通常是随机动力学模型的。通用问题是计算一个可以准确满足给定任务的控制器，该任务是 probabilistic temporal logic 规范。然而，因为随机不确定性、不精确知道参数以及混合特征，这个问题具有挑战性。我们已经开发了一个抽象框架，可以在不同的模型假设下解决这个问题。我们的方法基于一种可靠的finite-state抽象方法，即Markov decision process with intervals of probabilities（iMDP）。我们使用当前的验证技术来计算iMDP上的优质策略，并 garantía para satisfacer给定规范。然后，我们表明，通过构建，我们可以从这种策略中提取一个反馈控制器，这些 garantías会传递到动力学模型中。在这篇短文中，我们回顾了我们近期在这个领域的研究，并提出了两个挑战（相关于扩展性和处理非线性动力学），我们计划通过我们的进行研究来解决这些挑战。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Generation-of-Scenarios-for-System-level-Simulation-based-Verification-of-Autonomous-Driving-Systems"><a href="#Automatic-Generation-of-Scenarios-for-System-level-Simulation-based-Verification-of-Autonomous-Driving-Systems" class="headerlink" title="Automatic Generation of Scenarios for System-level Simulation-based Verification of Autonomous Driving Systems"></a>Automatic Generation of Scenarios for System-level Simulation-based Verification of Autonomous Driving Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09784">http://arxiv.org/abs/2311.09784</a></li>
<li>repo_url: None</li>
<li>paper_authors: Srajan Goyal, Alberto Griggio, Jacob Kimblad, Stefano Tonetta<br>for:* The paper is written for the purpose of presenting a generic framework for system-level simulation-based verification and validation (V&amp;V) of autonomous driving systems (ADS) that employ AI components.methods:* The framework uses a simulation model of the system, an abstract model that describes symbolically the system behavior, and formal methods to generate scenarios and verify the simulation executions.* The approach leverages the CARLA driving simulator and its ScenarioRunner tool to create diverse and complex driving scenarios.results:* The paper describes the instantiation of the VIVAS framework for an ADS case study, and demonstrates the effectiveness of the approach in automatically generating scenarios for system-level simulation-based V&amp;V of an automated driving system using CARLA and ScenarioRunner.* The results show the potential of the approach as a powerful tool in the future of ADS V&amp;V methodologies.<details>
<summary>Abstract</summary>
With increasing complexity of Automated Driving Systems (ADS), ensuring their safety and reliability has become a critical challenge. The Verification and Validation (V&V) of these systems are particularly demanding when AI components are employed to implement perception and/or control functions. In ESA-funded project VIVAS, we developed a generic framework for system-level simulation-based V&V of autonomous systems. The approach is based on a simulation model of the system, an abstract model that describes symbolically the system behavior, and formal methods to generate scenarios and verify the simulation executions. Various coverage criteria can be defined to guide the automated generation of the scenarios.   In this paper, we describe the instantiation of the VIVAS framework for an ADS case study. This is based on the integration of CARLA, a widely-used driving simulator, and its ScenarioRunner tool, which enables the creation of diverse and complex driving scenarios. This is also used in the CARLA Autonomous Driving Challenge to validate different ADS agents for perception and control based on AI, shared by the CARLA community. We describe the development of an abstract ADS model and the formulation of a coverage criterion that focuses on the behaviors of vehicles relative to the vehicle with ADS under verification. Leveraging the VIVAS framework, we generate and execute various driving scenarios, thus testing the capabilities of the AI components. The results show the effectiveness of VIVAS in automatically generating scenarios for system-level simulation-based V&V of an automated driving system using CARLA and ScenarioRunner. Therefore, they highlight the potential of the approach as a powerful tool in the future of ADS V&V methodologies.
</details>
<details>
<summary>摘要</summary>
随着自动驾驶系统（ADS）的复杂度增加，确保其安全性和可靠性已成为一项杰匡的挑战。验证和验议（V&V）这些系统特别是当AI组件用于感知和/或控制功能时，变得非常具有挑战性。在欧洲空间局（ESA）资助的项目VIVAS中，我们开发了一种通用框架 для系统级别的模拟基于验证和验议自动驾驶系统。该方法基于系统模型、一个抽象模型，用于 символи地描述系统行为，以及正式方法来生成场景和验证模拟执行。可以定义多种覆盖度标准来引导自动生成场景。  在这篇论文中，我们描述了VIvas框架在自动驾驶系统case study中的实现。这基于卡拉拉（CARLA）广泛使用的驾驶模拟器和其ScenarioRunner工具，可以创造多样化和复杂的驾驶场景。这也是在CARLA自动驾驶挑战中 validate不同的ADS代理人以及感知和控制基于AI的不同ADS代理人。我们开发了抽象的ADS模型，并将其与驾驶车辆相对于具有ADS的车辆的行为相关的覆盖度标准定义。通过VIvas框架，我们生成并执行多种驾驶场景，因此测试了AI组件的能力。结果显示VIvas在使用CARLA和ScenarioRunner进行系统级别的模拟基于验证和验议中自动生成场景的能力是非常有力的。因此，它高亮了未来ADS验证和验议方法的潜在力量。
</details></li>
</ul>
<hr>
<h2 id="Investigating-Data-Contamination-in-Modern-Benchmarks-for-Large-Language-Models"><a href="#Investigating-Data-Contamination-in-Modern-Benchmarks-for-Large-Language-Models" class="headerlink" title="Investigating Data Contamination in Modern Benchmarks for Large Language Models"></a>Investigating Data Contamination in Modern Benchmarks for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09783">http://arxiv.org/abs/2311.09783</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan</li>
<li>for: 提高LLM的评估标准和评估方法的可靠性</li>
<li>methods: 提议两种适用于开源和专有模型的数据污染检测方法，包括检索系统和TS推测法</li>
<li>results: 研究发现一些商业LLM可以很准确地猜测测试集中的缺失选项，并且在一些标准套件中发现了模型的表现改善。<details>
<summary>Abstract</summary>
Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named \textbf{T}estset \textbf{S}lot Guessing (\textit{TS-Guessing}), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the TruthfulQA benchmark, we find that LLMs exhibit notable performance improvement when provided with additional metadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52\% and 57\%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.
</details>
<details>
<summary>摘要</summary>
近期观察发现了 LLM 的评估标准和实际性能之间的差距，引发了评估标准污染的 Concerns 。这个问题尤其对于关闭源模型和某些开源模型来说是 kritisch 。在这篇论文中，我们研究了数据污染问题，并提出了两种适用于开源和专有 LLM 的方法。我们首先介绍了一个检索基于系统，用于探索评估标准和预训练 corpora 之间的可能的重叠。然后，我们介绍了一种新的调查协议，名为 \textbf{T}estset \textbf{S}lot Guessing (\textit{TS-Guessing）}, 适用于开源和专有模型。这种方法包括在多选问题中隐藏错误答案，并让模型填充差距。此外，还包括隐藏评估示例中不可能的单词，并让模型生成它。我们发现了一些商业 LLM 可以意外地猜测测试集中的缺失选项。例如，在 TruthfulQA  benchmark 中，我们发现了 LLM 在提供额外metadata 时表现出 Notable 的性能提升。此外，在 MMLU  benchmark 中， ChatGPT 和 GPT-4 在测试集中猜测缺失选项的精准率分别为 52% 和 57%。我们希望这些结果可以强调评估方法和标准的需要更加Robust 。
</details></li>
</ul>
<hr>
<h2 id="Model-Checking-for-Closed-Loop-Robot-Reactive-Planning"><a href="#Model-Checking-for-Closed-Loop-Robot-Reactive-Planning" class="headerlink" title="Model Checking for Closed-Loop Robot Reactive Planning"></a>Model Checking for Closed-Loop Robot Reactive Planning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09780">http://arxiv.org/abs/2311.09780</a></li>
<li>repo_url: None</li>
<li>paper_authors: Christopher Chandler, Bernd Porr, Alice Miller, Giulia Lafratta</li>
<li>for: 本研究用模型检查来创建多步计划，使Diffuse Drive驱动轮胎自动车可以避免危险。</li>
<li>methods: 我们使用一种小型、专门设计的模型检查算法，在实时环境中生成计划，并且这种方法具有 Egocentric 反应性的简单生物体特征。我们的方法基于链接临时控制系统，通过消除环境中的干扰来让自动车保持其首选行为或休眠状态。我们使用2D LiDAR数据的精细化方法，敏感于环境中的 bounded 随机变化。我们使用深度优先搜索来实现多步规划，并通过cul-de-sac 场景作为第一个测试用例。</li>
<li>results: 我们的结果表明，模型检查可以用来规划效率的轨迹，超越单步规划的表现。我们在实时使用无预计算数据来实现这一点。虽然我们的方法有一些局限性，但我们认为我们的方法具有开发安全、可靠和透明的轨迹规划方法的潜力。<details>
<summary>Abstract</summary>
In this paper, we show how model checking can be used to create multi-step plans for a differential drive wheeled robot so that it can avoid immediate danger. Using a small, purpose built model checking algorithm in situ we generate plans in real-time in a way that reflects the egocentric reactive response of simple biological agents. Our approach is based on chaining temporary control systems which are spawned to eliminate disturbances in the local environment that disrupt an autonomous agent from its preferred action (or resting state). The method involves a novel discretization of 2D LiDAR data which is sensitive to bounded stochastic variations in the immediate environment. We operationalise multi-step planning using invariant checking by forward depth-first search, using a cul-de-sac scenario as a first test case. Our results demonstrate that model checking can be used to plan efficient trajectories for local obstacle avoidance, improving on the performance of a reactive agent which can only plan one step. We achieve this in near real-time using no pre-computed data. While our method has limitations, we believe our approach shows promise as an avenue for the development of safe, reliable and transparent trajectory planning in the context of autonomous vehicles.
</details>
<details>
<summary>摘要</summary>
在这篇论文中，我们展示了如何使用模型检查来创建多步计划，以使Diffusion Drive轮胎自动车避免 immediate danger。我们使用一种小型、特有的模型检查算法在实时中生成计划，以模仿简单生物体的 Egocentric 反应。我们的方法基于临时控制系统的链接，以消除环境中的干扰，使自动 Agent 能够继续进行其 preferred action（或休眠状态）。我们的方法包括一种新的2D LiDAR数据的精度化，敏感于环境中的 bounded 随机变化。我们通过在深度优先搜索中进行 invariants 检查来实现多步规划，并使用 cul-de-sac 场景作为第一个测试 caso。我们的结果表明，模型检查可以用于计划高效的轨迹，超越了只能计划一步的感知Agent。我们在实时中使用不需要预计算数据来实现这一点。虽然我们的方法有限制，但我们认为我们的方法显示出了在自动汽车中安全、可靠和透明的轨迹规划的可能性。
</details></li>
</ul>
<hr>
<h2 id="HuatuoGPT-II-One-stage-Training-for-Medical-Adaption-of-LLMs"><a href="#HuatuoGPT-II-One-stage-Training-for-Medical-Adaption-of-LLMs" class="headerlink" title="HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs"></a>HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09774">http://arxiv.org/abs/2311.09774</a></li>
<li>repo_url: None</li>
<li>paper_authors: Junying Chen, Xidong Wang, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, Benyou Wang</li>
<li>for: 这个论文的目的是适应具有特殊知识的领域，如医学，以便将特殊知识 integrate into一个通用语言模型中，如Llama2。</li>
<li>methods: 该论文提出了一种将多种数据集，包括预训练和指导训练，转化为一个统一的输入输出对形式，以简化学习协议。</li>
<li>results: 该论文通过对多个医学领域的测试，证明了其在中医领域的状态对级表现，并在一些方面超过了现有的专有模型，如ChatGPT和GPT-4。专家手动评估也证明了该模型的优势。<details>
<summary>Abstract</summary>
Adapting a language model into a specific domain, a.k.a `domain adaption', is a common practice when specialized knowledge, e.g. medicine, is not encapsulated in a general language model like Llama2. The challenge lies in the heterogeneity of data across the two training stages, as it varies in languages, genres, or formats. To tackle this and simplify the learning protocol, we propose to transform heterogeneous data, from the both pre-training and supervised stages, into a unified, simple input-output pair format. We validate the new protocol in the domains where proprietary LLMs like ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The developed model, HuatuoGPT-II, has shown state-of-the-art performance in Chinese medicine domain on a number of benchmarks, e.g. medical licensing exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some aspects, especially in Traditional Chinese Medicine. Expert manual evaluations further validate HuatuoGPT-II's advantages over existing LLMs. Notably, HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing Examination where it achieved the best performance, showcasing not only its effectiveness but also its generalization capabilities.
</details>
<details>
<summary>摘要</summary>
适应特定领域的语言模型化，即域 adaptation，是一种常见的做法，当特殊知识，如医学，不包含在通用语言模型如LLAMA2中。挑战在两个训练阶段数据的异ogeneity上，因为数据的语言、种类、格式都不同。为了解决这个问题并简化学习协议，我们提议将各种不同数据，从预训练和监督两个阶段，转换成一个统一、简单的输入输出对 format。我们验证了新协议的效果在各种领域，如中医，并在一些标准测试任务上达到了状态 искусственный智能表现。特别是在传统中医领域，我们的模型 HuatuoGPT-II 表现出了优秀的result，并在一些方面超越了商业化模型，如ChatGPT和GPT-4。专业人员手动评估也证明了 HuatuoGPT-II 的优势。值得一提的是，HuatuoGPT-II 在新的中医国家医籍考试中达到了最佳表现，证明了它不仅有效，还有普适化能力。
</details></li>
</ul>
<hr>
<h2 id="Back-to-Basics-A-Simple-Recipe-for-Improving-Out-of-Domain-Retrieval-in-Dense-Encoders"><a href="#Back-to-Basics-A-Simple-Recipe-for-Improving-Out-of-Domain-Retrieval-in-Dense-Encoders" class="headerlink" title="Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders"></a>Back to Basics: A Simple Recipe for Improving Out-of-Domain Retrieval in Dense Encoders</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09765">http://arxiv.org/abs/2311.09765</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/amy-hyunji/lora-for-retrieval">https://github.com/amy-hyunji/lora-for-retrieval</a></li>
<li>paper_authors: Hyunji Lee, Luca Soldaini, Arman Cohan, Minjoon Seo, Kyle Lo</li>
<li>for: 这个论文主要是为了提高 dense retriever 模型在未经见过的领域中的泛化能力。</li>
<li>methods: 该论文提出了一种简单的训练方法，包括使用 LoRA 等参数效率的方法，并选择使用 batch 内的负样本，除非给出了准确制定的困难负样本。</li>
<li>results: 该论文使用 BEIR  benchmark 进行验证，并发现这些建议可以在不同的 dense encoder 和基础模型大小上 persist，并且与其他资源密集的策略（如建筑修改或多个预训练）相结合，可以提高 dense retriever 模型的泛化能力。<details>
<summary>Abstract</summary>
Prevailing research practice today often relies on training dense retrievers on existing large datasets such as MSMARCO and then experimenting with ways to improve zero-shot generalization capabilities to unseen domains. While prior work has tackled this challenge through resource-intensive steps such as data augmentation, architectural modifications, increasing model size, or even further base model pretraining, comparatively little investigation has examined whether the training procedures themselves can be improved to yield better generalization capabilities in the resulting models. In this work, we recommend a simple recipe for training dense encoders: Train on MSMARCO with parameter-efficient methods, such as LoRA, and opt for using in-batch negatives unless given well-constructed hard negatives. We validate these recommendations using the BEIR benchmark and find results are persistent across choice of dense encoder and base model size and are complementary to other resource-intensive strategies for out-of-domain generalization such as architectural modifications or additional pretraining. We hope that this thorough and impartial study around various training techniques, which augments other resource-intensive methods, offers practical insights for developing a dense retrieval model that effectively generalizes, even when trained on a single dataset.
</details>
<details>
<summary>摘要</summary>
现有研究往往采用 dense retriever 训练大量数据集 MSMARCO，然后尝试改进零shot泛化能力。而在优化这种泛化能力方面，已有许多研究，包括数据增强、结构修改、模型大小增加和额外预训练。然而，对于训练过程本身是否可以进行优化以实现更好的泛化能力，尚未得到了充分的探讨。在这项工作中，我们建议一种简单的 dense encoder 训练热键：通过 LoRA 等参数效率的方法在 MSMARCO 上训练，并在批处中使用卷积批处。我们使用 BEIR 测试准则 validate 这些建议，并发现结果是不依赖于 dense encoder 和基础模型大小，并且与其他资源占用量大的方法相结合，可以实现更好的泛化能力。我们希望这种具有充分的探讨和不偏袋的研究，能够为开发高效泛化 dense retrieval 模型提供实际的指导。
</details></li>
</ul>
<hr>
<h2 id="Graph-Guided-Reasoning-for-Multi-Hop-Question-Answering-in-Large-Language-Models"><a href="#Graph-Guided-Reasoning-for-Multi-Hop-Question-Answering-in-Large-Language-Models" class="headerlink" title="Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models"></a>Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09762">http://arxiv.org/abs/2311.09762</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jinyoung Park, Ameen Patel, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim</li>
<li>For: The paper aims to improve the multi-step reasoning capabilities of large language models (LLMs) by addressing two issues in previous CoT prompting methods: generating irrelevant rationales and failing to compose subquestions or queries for obtaining all relevant information.* Methods: The proposed graph-guided CoT prompting method uses a “question&#x2F;rationale graph” constructed by LLMs to guide the reasoning process. The method includes graph representation and verification steps to filter out irrelevant rationales and generate follow-up questions to obtain relevant information.* Results: The proposed method shows superior performance compared to previous CoT prompting methods and their variants on multi-hop question answering benchmark datasets.<details>
<summary>Abstract</summary>
Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning capabilities of Large Language Models (LLMs) by generating a series of rationales before the final answer. We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning: (i) Generating rationales irrelevant to the question, (ii) Unable to compose subquestions or queries for generating/retrieving all the relevant information. To address them, we propose a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps. Specifically, we first leverage LLMs to construct a "question/rationale graph" by using knowledge extraction prompting given the initial question and the rationales generated in the previous steps. Then, the graph verification step diagnoses the current rationale triplet by comparing it with the existing question/rationale graph to filter out irrelevant rationales and generate follow-up questions to obtain relevant information. Additionally, we generate CoT paths that exclude the extracted graph information to represent the context information missed from the graph extraction. Our graph-guided reasoning method shows superior performance compared to previous CoT prompting and the variants on multi-hop question answering benchmark datasets.
</details>
<details>
<summary>摘要</summary>
chain-of-thought (CoT) 提示法已经提高了大语言模型 (LLM) 的多步逻辑能力，通过生成一系列的理由来提高答案。我们分析了 CoT 生成的逻辑路径，并发现了两个多步逻辑问题：（i）生成与问题无关的理由，（ii）无法组合子问题或查询来获取所有相关信息。为解决这些问题，我们提议一种图表引导 CoT 提示法，使 LLM 能够通过图表表示/验证步骤达到正确答案。 Specifically，我们首先利用 LLM 使用知识EXTRACTION 提示生成一个 "问题/理由图"，并then 使用图表验证步骤来诊断当前的理由 triplet，并将无关的理由过滤掉，生成跟进 вопро题以获取相关信息。此外，我们还生成 CoT 路径，排除扩展的图信息以表示Context missed 信息。我们的图表引导逻辑方法在多个多步问答 benchmark 数据集上表现出色。
</details></li>
</ul>
<hr>
<h2 id="MAFALDA-A-Benchmark-and-Comprehensive-Study-of-Fallacy-Detection-and-Classification"><a href="#MAFALDA-A-Benchmark-and-Comprehensive-Study-of-Fallacy-Detection-and-Classification" class="headerlink" title="MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification"></a>MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09761">http://arxiv.org/abs/2311.09761</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chadi Helwe, Tom Calamai, Pierre-Henri Paris, Chloé Clavel, Fabian Suchanek</li>
<li>For: The paper aims to address the challenges of automated fallacy detection and classification, particularly the subjectivity of the task and the need for a comprehensive and unified approach in existing research.* Methods: The paper introduces a novel taxonomy of fallacies that refines and aligns previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity.* Results: The paper introduces MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset based on examples from various previously existing fallacy datasets under the unified taxonomy, and evaluates several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. The evaluation provides valuable insights into the strengths and limitations of these models in addressing fallacious reasoning.Here is the same information in Simplified Chinese text:* For: 这篇论文目标是解决自动推理错误检测和分类的挑战，特别是任务的主观性和现有研究中存在的缺乏一致性。* Methods: 论文引入了一种新的论点分类方法，该方法可以协调和融合先前的分类方法，同时还提供了一种适用于主观NLPTask的新注释方案。* Results: 论文引入了一个金标准数据集MAFALDA（多级注释错误数据集），该数据集基于先前的许多错误数据集的一致性，并对多种语言模型进行零基础学习 Setting中的评价。<details>
<summary>Abstract</summary>
Fallacies can be used to spread disinformation, fake news, and propaganda, underlining the importance of their detection. Automated detection and classification of fallacies, however, remain challenging, mainly because of the innate subjectivity of the task and the need for a comprehensive, unified approach in existing research. Addressing these limitations, our study introduces a novel taxonomy of fallacies that aligns and refines previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity, adapted to precision, recall, and F1-Score metrics. Using our annotation scheme, the paper introduces MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset. MAFALDA is based on examples from various previously existing fallacy datasets under our unified taxonomy across three levels of granularity. We then evaluate several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. Our comprehensive evaluation not only benchmarks the performance of these models but also provides valuable insights into their strengths and limitations in addressing fallacious reasoning.
</details>
<details>
<summary>摘要</summary>
False information, fake news, and propaganda can be spread through fallacies, highlighting the importance of detecting them. However, automated detection and classification of fallacies are challenging due to the subjective nature of the task and the lack of a comprehensive, unified approach in existing research. To address these limitations, our study proposes a new taxonomy of fallacies that aligns and refines previous classifications, a new annotation scheme tailored for subjective NLP tasks, and a new evaluation method designed to handle subjectivity, adapted to precision, recall, and F1-Score metrics. Using our annotation scheme, we introduce MAFALDA (Multi-level Annotated FALlacy DAtaset), a gold standard dataset based on examples from various previously existing fallacy datasets under our unified taxonomy across three levels of granularity. We then evaluate several language models under a zero-shot learning setting using MAFALDA to assess their fallacy detection and classification capability. Our comprehensive evaluation not only benchmarks the performance of these models but also provides valuable insights into their strengths and limitations in addressing fallacious reasoning.
</details></li>
</ul>
<hr>
<h2 id="UFPS-A-unified-framework-for-partially-annotated-federated-segmentation-in-heterogeneous-data-distribution"><a href="#UFPS-A-unified-framework-for-partially-annotated-federated-segmentation-in-heterogeneous-data-distribution" class="headerlink" title="UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution"></a>UFPS: A unified framework for partially-annotated federated segmentation in heterogeneous data distribution</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09757">http://arxiv.org/abs/2311.09757</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/tekap404/unified_federated_partially-labeled_segmentation">https://github.com/tekap404/unified_federated_partially-labeled_segmentation</a></li>
<li>paper_authors: Le Jiang, Li Yan Ma, Tie Yong Zeng, Shi Hui Ying</li>
<li>for: 这个研究是为了解决基于半自动化标签的医疗影像分类问题，并且不会遗漏数据的隐私问题。</li>
<li>methods: 这个研究使用了联邦式半自动化分类（FPSS），并且提出了一个统一的全球模型训练方法（UFPS），以解决半自动化分类中的分类异常和客户端漂移问题。</li>
<li>results: 这个研究的实验结果显示，UFPS方法能够更好地解决半自动化分类中的分类异常和客户端漂移问题，并且在实际医疗影像数据上显示了更好的适应和普遍性。<details>
<summary>Abstract</summary>
Partially supervised segmentation is a label-saving method based on datasets with fractional classes labeled and intersectant. However, it is still far from landing on real-world medical applications due to privacy concerns and data heterogeneity. As a remedy without privacy leakage, federated partially supervised segmentation (FPSS) is formulated in this work. The main challenges for FPSS are class heterogeneity and client drift. We propose a Unified Federated Partially-labeled Segmentation (UFPS) framework to segment pixels within all classes for partially-annotated datasets by training a totipotential global model without class collision. Our framework includes Unified Label Learning and sparsed Unified Sharpness Aware Minimization for unification of class and feature space, respectively. We find that vanilla combinations for traditional methods in partially supervised segmentation and federated learning are mainly hampered by class collision through empirical study. Our comprehensive experiments on real medical datasets demonstrate better deconflicting and generalization ability of UFPS compared with modified methods.
</details>
<details>
<summary>摘要</summary>
partially supervised segmentation是一种基于分类数据集的标签保存方法，但它还远离实际医疗应用中的应用，主要原因是隐私问题和数据不一致。为解决这些问题，本文提出了联邦半supervised分割（FPSS）方法。FPSS的主要挑战是分类异ogeneous和客户端漂移。我们提出了一种总统的联邦半supervised分割（UFPS）框架，用于对半标注数据集中的像素进行分割，不会出现分类冲突。我们的框架包括统一标签学习和粗粒化的统一锐度感知优化，用于统一类和特征空间。我们通过实际研究发现，传统的partially supervised segmentation和联邦学习方法的组合在面临分类冲突的情况下效果较差。我们的全面实验表明，UFPS在实际医疗数据集上具有更好的冲突解决和泛化能力，与修改后的方法相比。
</details></li>
</ul>
<hr>
<h2 id="Redefining-the-Laparoscopic-Spatial-Sense-AI-based-Intra-and-Postoperative-Measurement-from-Stereoimages"><a href="#Redefining-the-Laparoscopic-Spatial-Sense-AI-based-Intra-and-Postoperative-Measurement-from-Stereoimages" class="headerlink" title="Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages"></a>Redefining the Laparoscopic Spatial Sense: AI-based Intra- and Postoperative Measurement from Stereoimages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09744">http://arxiv.org/abs/2311.09744</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/leopoldmueller/laparoscopicmeasurement">https://github.com/leopoldmueller/laparoscopicmeasurement</a></li>
<li>paper_authors: Leopold Müller, Patrick Hemmer, Moritz Queisner, Igor Sauer, Simeon Allmendinger, Johannes Jakubik, Michael Vössing, Niklas Kühl<br>for: This paper aims to provide a more accurate and efficient solution for image-guided surgery, specifically for measuring relevant structures such as vessel segments, resection margins, and bowel lengths.methods: The proposed method utilizes stereo vision and state-of-the-art machine learning architectures, including RAFT-Stereo and YOLOv8, to achieve high accuracy in distance measurements with errors below 1 mm.results: The developed method is assessed in various realistic experimental evaluation environments and demonstrates robustness in challenging environments with textureless regions. The results outline the potential of the method for providing more precise, safe, and efficient surgical procedures.<details>
<summary>Abstract</summary>
A significant challenge in image-guided surgery is the accurate measurement task of relevant structures such as vessel segments, resection margins, or bowel lengths. While this task is an essential component of many surgeries, it involves substantial human effort and is prone to inaccuracies. In this paper, we develop a novel human-AI-based method for laparoscopic measurements utilizing stereo vision that has been guided by practicing surgeons. Based on a holistic qualitative requirements analysis, this work proposes a comprehensive measurement method, which comprises state-of-the-art machine learning architectures, such as RAFT-Stereo and YOLOv8. The developed method is assessed in various realistic experimental evaluation environments. Our results outline the potential of our method achieving high accuracies in distance measurements with errors below 1 mm. Furthermore, on-surface measurements demonstrate robustness when applied in challenging environments with textureless regions. Overall, by addressing the inherent challenges of image-guided surgery, we lay the foundation for a more robust and accurate solution for intra- and postoperative measurements, enabling more precise, safe, and efficient surgical procedures.
</details>
<details>
<summary>摘要</summary>
significante挑战在图像引导手术中是准确测量有关结构，如血管段、切除边缘或肠长度。这项任务是许多手术中不可或缺的一部分，但它具有较大的人工劳动量和不准确率。在这篇论文中，我们开发了一种新的人类-AI基于方法，用于肠 Laparoscopic 测量，利用推导导航的STereo视力。基于全面的quality要求分析，这项工作提出了一种全面的测量方法，包括当前的机器学习架构，如RAFT-Stereo和YOLOv8。我们开发的方法在不同的实际试验环境中进行了评估。我们的结果表明，我们的方法可以实现高精度的距离测量，错误在1毫米以下。此外，在表面上进行的测量也能够在粗糙区域中展示 robustness。总之，我们通过解决图像引导手术中的内在挑战，为更加精确、安全和有效的手术过程奠定了基础。
</details></li>
</ul>
<hr>
<h2 id="Redefining-Super-Resolution-Fine-mesh-PDE-predictions-without-classical-simulations"><a href="#Redefining-Super-Resolution-Fine-mesh-PDE-predictions-without-classical-simulations" class="headerlink" title="Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations"></a>Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09740">http://arxiv.org/abs/2311.09740</a></li>
<li>repo_url: None</li>
<li>paper_authors: Rajat Kumar Sarkar, Ritam Majumdar, Vishal Jadhav, Sagar Srinivas Sakhinana, Venkataramana Runkana</li>
<li>for: 用于提高计算流体力学（CFD）中粗粒度 simulations 的精度，并且可以避免传统的超分辨率方法中的约束。</li>
<li>methods: 我们提出了一种新的超分辨率定义，将 coarse-grid 数据作为输入，预测 fine-grid 数据。使用 physics-infused UNet 上升方法，并在多种2D-CFD问题中进行了证明，包括 Burger 方程中的缺陷检测、甲烷燃烧和工业热交换器中的沾吸。</li>
<li>results: 我们的方法可以生成精度高的 fine-mesh 解决方案，不需要传统的计算，同时保持了原始真实情况的精度。通过在训练过程中使用多种边界条件，我们还证明了我们的方法的稳定性，这将推动其广泛应用于工程和科学 CFD 解决方案中。<details>
<summary>Abstract</summary>
In Computational Fluid Dynamics (CFD), coarse mesh simulations offer computational efficiency but often lack precision. Applying conventional super-resolution to these simulations poses a significant challenge due to the fundamental contrast between downsampling high-resolution images and authentically emulating low-resolution physics. The former method conserves more of the underlying physics, surpassing the usual constraints of real-world scenarios. We propose a novel definition of super-resolution tailored for PDE-based problems. Instead of simply downsampling from a high-resolution dataset, we use coarse-grid simulated data as our input and predict fine-grid simulated outcomes. Employing a physics-infused UNet upscaling method, we demonstrate its efficacy across various 2D-CFD problems such as discontinuity detection in Burger's equation, Methane combustion, and fouling in Industrial heat exchangers. Our method enables the generation of fine-mesh solutions bypassing traditional simulation, ensuring considerable computational saving and fidelity to the original ground truth outcomes. Through diverse boundary conditions during training, we further establish the robustness of our method, paving the way for its broad applications in engineering and scientific CFD solvers.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Source-Prompt-Coordinated-Pre-training-of-Language-Models-on-Diverse-Corpora-from-Multiple-Sources"><a href="#Source-Prompt-Coordinated-Pre-training-of-Language-Models-on-Diverse-Corpora-from-Multiple-Sources" class="headerlink" title="Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources"></a>Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09732">http://arxiv.org/abs/2311.09732</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yipei Xu, Dakuan Lu, Jiaqing Liang, Xintao Wang, Yipeng Geng, Yingsi Xin, Hengkui Wu, Ken Chen, ruiji zhang, Yanghua Xiao</li>
<li>for: 这 paper 是研究 Pre-trained language models (PLMs) 的新 paradigm 在 NLP 领域的一部分，以及如何更好地准备这些模型和它们的预训练 corpora。</li>
<li>methods: 这 paper 使用了一种常见和成功的方法，即不断扩大模型和预训练 corpora 的大小。这些大 corpora 通常是从多个源 converges 而来的，因此变得越来越多样化。然而，这些巨大的 converged corpora 的侧effect 尚未得到了充分的研究。</li>
<li>results: 在这 paper 中， authors 发现了各种 corpora 的多样性会对预训练 PLMs 的性能产生负面影响。为了协调预训练在多种 corpora 上，authors 提出了源提示 (SP)，这是一种在预训练和精度调整阶段显式地提示模型数据源的技术。Results 表明，使用 SP 在多种 corpora 上预训练 PLMs 可以获得显著的下游任务提升。<details>
<summary>Abstract</summary>
Pre-trained language models (PLMs) have established the new paradigm in the field of NLP. For more powerful PLMs, one of the most popular and successful way is to continuously scale up sizes of the models and the pre-training corpora. These large corpora are generally obtained by converging smaller ones from multiple sources, they are thus growing increasingly diverse. However, the side-effects of these colossal converged corpora remain understudied. In this paper, we identify the disadvantage of heterogeneous corpora from multiple sources for pre-training PLMs. Towards coordinated pre-training on diverse corpora, we further propose source prompts (SP), which explicitly prompt the model of the data source at the pre-training and fine-tuning stages. Results of extensive experiments demonstrate that PLMs pre-trained with SP on diverse corpora gain significant improvement in various downstream tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Prudent-Silence-or-Foolish-Babble-Examining-Large-Language-Models’-Responses-to-the-Unknown"><a href="#Prudent-Silence-or-Foolish-Babble-Examining-Large-Language-Models’-Responses-to-the-Unknown" class="headerlink" title="Prudent Silence or Foolish Babble? Examining Large Language Models’ Responses to the Unknown"></a>Prudent Silence or Foolish Babble? Examining Large Language Models’ Responses to the Unknown</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09731">http://arxiv.org/abs/2311.09731</a></li>
<li>repo_url: None</li>
<li>paper_authors: Genglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, Hao Peng</li>
<li>for: 本研究旨在系统地研究LLMs在缺乏必要知识的情况下的行为，以及这种行为如何与人类对话规范不符。</li>
<li>methods: 本研究使用了一个反 adversarial question-answering benchmark，该 benchmark 包含了 LLMS 缺乏训练数据的情况下的问题。</li>
<li>results: 研究发现，通过 instrucion finetuning 和人类反馈学习（RLHF），LLMs 可以更好地表达uncertainty，并且与有效的问题相对应，表现出更高的准确率和自信度。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) often struggle when faced with situations where they lack the prerequisite knowledge to generate a sensical response. In these cases, models tend to fabricate and hallucinate, rather than appropriately signaling uncertainty as humans would. This behavior misaligns with human conversational norms and presents challenges surrounding responsible and ethical AI development. This work aims to systematically investigate LLMs' behaviors in such situations. We curate an adversarial question-answering benchmark containing unanswerable questions targeting information absent from the LLM's training data. Concretely, these unanswerable questions contain non-existent concepts or false premises. When presented with such unanswerable questions, an LLM should appropriately convey uncertainty, and be able to challenge the premise and refuse to generate a response. While facing answerable valid questions, a model should demonstrate a positive correlation between accuracy and confidence. Using a model-agnostic unified confidence elicitation approach, we observe that LLMs that have gone through instruction finetuning and reinforcement learning from human feedback (RLHF) perform significantly better than their counterparts that do not. Moreover, uncertainty expression 1 through our elicitation method does not always stay consistent with the perceived confidence of the direct response of an LLM. Our findings call for further research into teaching LLMs to proactively and reliably express uncertainty.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Aligning-with-Whom-Large-Language-Models-Have-Gender-and-Racial-Biases-in-Subjective-NLP-Tasks"><a href="#Aligning-with-Whom-Large-Language-Models-Have-Gender-and-Racial-Biases-in-Subjective-NLP-Tasks" class="headerlink" title="Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks"></a>Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09730">http://arxiv.org/abs/2311.09730</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jiaxin-pei/llm-group-bias">https://github.com/jiaxin-pei/llm-group-bias</a></li>
<li>paper_authors: Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens</li>
<li>for: 这个研究探讨了大语言模型（LLM）在主观NLP任务上是否具有性别和民族特征的偏见。</li>
<li>methods: 研究使用了POPQUORN数据集，对四种流行的LLM进行了一系列实验，检查它们在政eness和不礼貌任务上是否具有偏见。</li>
<li>results: 研究发现，对于这两个任务，模型的预测结果更接近白人和女性参与者的标签。进一步的探讨发现，在使用目标民族和性别标签作为提示时，模型的性能会下降。 Code和数据可以在<a target="_blank" rel="noopener" href="https://github.com/Jiaxin-Pei/LLM-Group-Bias%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Jiaxin-Pei/LLM-Group-Bias上获取。</a><details>
<summary>Abstract</summary>
Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of "Black" and "Asian" individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.
</details>
<details>
<summary>摘要</summary>
人类对语言的理解受个人背景的影响，如性别和民族。 although existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, we leverage the POPQUORN dataset, which contains annotations of diverse demographic backgrounds, to investigate the capability of four popular LLMs to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. Specifically, when prompted to respond from the perspective of "Black" and "Asian" individuals, models show lower performance in predicting both overall scores and the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks, and that demographic-infused prompts alone may be insufficient to mitigate such effects. 可以在 GitHub 上获取代码和数据：https://github.com/Jiaxin-Pei/LLM-Group-Bias。
</details></li>
</ul>
<hr>
<h2 id="Outcome-supervised-Verifiers-for-Planning-in-Mathematical-Reasoning"><a href="#Outcome-supervised-Verifiers-for-Planning-in-Mathematical-Reasoning" class="headerlink" title="Outcome-supervised Verifiers for Planning in Mathematical Reasoning"></a>Outcome-supervised Verifiers for Planning in Mathematical Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09724">http://arxiv.org/abs/2311.09724</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fei Yu, Anningzhe Gao, Benyou Wang<br>for: 这种研究旨在解决大型语言模型（LLM）在数学逻辑推理中维持准确性的问题， LLMs 经常在推理过程中出现错误，导致最终结果不准确。methods: 该研究提出了一种新的评估模型——结果监督价值模型（OVM），通过结果监督来训练，从而提高了模型的准确性。 OVM 不需要劳动密集的步骤级别正确性注释，从而提高了其可扩展性。results: 在两个多步数学逻辑数据集上进行了实验，结果显示 OVM 模型在 GSM8K 数据集上 achievement 状态的最佳结果，而不需要使用 GPT-4 或代码执行。 这些发现提供了一种新的视角，即在训练评估模型时，结果监督可以提高模型的价值估计，并且有理论基础。<details>
<summary>Abstract</summary>
Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a \textit{planning} problem. Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for \textit{planning} by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \textbf{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters}; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）经常在进行多步骤推理时维持正确性问题，导致错误传递，干扰最终结果。现有的方法来解决这问题主要是使用验证模型来评估生成的解答候选者，专注于全局推理路径或部分推理路径。我们认为评估未完成推理路径的潜力可能更有利，将任务转换为规划问题。我们提出的验证器是结果监督值模型（OVM），通过结果监督进行训练，提供一种高效和直观的规划方法，优先级是导向正确的结论的步骤。此外，OVM不需要耗费劳动的步骤正确性标注，提高其扩展性。我们在GSM8K和Game of 24两个多步骤推理数据集上进行实验，结果显示OVM模型在LLM中表现出色，特别是OVM-7B模型在GSM8K数据集上实现了状态顶尖结果，而不使用GPT-4或代码执行。这些结果提供一个新的见解，强调结果监督在训练验证器 для多步骤推理任务中的role，并提供了值估计的理论基础。
</details></li>
</ul>
<hr>
<h2 id="You-don’t-need-a-personality-test-to-know-these-models-are-unreliable-Assessing-the-Reliability-of-Large-Language-Models-on-Psychometric-Instruments"><a href="#You-don’t-need-a-personality-test-to-know-these-models-are-unreliable-Assessing-the-Reliability-of-Large-Language-Models-on-Psychometric-Instruments" class="headerlink" title="You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments"></a>You don’t need a personality test to know these models are unreliable: Assessing the Reliability of Large Language Models on Psychometric Instruments</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09718">http://arxiv.org/abs/2311.09718</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/orange0629/llm-personas">https://github.com/orange0629/llm-personas</a></li>
<li>paper_authors: Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Dallas Card, David Jurgens</li>
<li>for: 本研究旨在检验当前问题提示的格式是否能够具备一致和可靠的回答能力。</li>
<li>methods: 研究者首先构建了39种人格测试工具的693个问题集，然后设计了一些细微变化的提示，以检验LLM的回答准确性和一致性。</li>
<li>results: 实验发现，即使使用简单的变换，LLM的回答能力也会受到很大的下降，而大多数LLM的否定一致性也很低。这些结果表明，当前的问题提示方式不够准确地捕捉模型的感知，并讨论了可能的更好的 alternatives。<details>
<summary>Abstract</summary>
The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. In particular, to properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs of particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting enables LLMs to provide responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLM's capabilities to generate accurate answers, as well as consistency variations to examine their consistency towards simple perturbations such as switching the option order. Our experiments on 15 different open-source LLMs reveal that even simple perturbations are sufficient to significantly downgrade a model's question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately capture model perceptions, and we discuss potential alternatives to improve such issues.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）在自然语言理解任务上的多样性，使得它们在社会科学研究中受欢迎。特别是，为了准确理解LLM的性质和内在人格，研究人员已经使用了问题提示的形式进行研究。在这项研究中，我们做出了一个谨慎的步骤，检查当前的提示格式是否能够让LLM提供一致和稳定的回答。我们首先构建了一个包含39种测量工具和115个人格轴的数据集。此外，我们设计了一些具有轻微变化的提示，检查LLM是否能够生成准确回答，以及对简单的变化（如选项顺序交换）的一致性。我们在15种开源LLM上进行了实验，发现，即使使用简单的变化，也可以导致模型的问题回答能力下降 significatively，并且大多数LLM具有低的否定一致性。我们的结果表明，当前广泛使用的提示方式不够用于准确捕捉模型的感知，我们讨论了可能的改进方案。
</details></li>
</ul>
<hr>
<h2 id="Towards-Autonomous-Hypothesis-Verification-via-Language-Models-with-Minimal-Guidance"><a href="#Towards-Autonomous-Hypothesis-Verification-via-Language-Models-with-Minimal-Guidance" class="headerlink" title="Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance"></a>Towards Autonomous Hypothesis Verification via Language Models with Minimal Guidance</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09706">http://arxiv.org/abs/2311.09706</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shiro Takagi, Ryutaro Yamauchi, Wataru Kumagai</li>
<li>for: 本研究旨在检验AI是否可以自主生成和验证假设，以实现人类水平的自主研究。</li>
<li>methods: 本研究使用GPT-4生成假设和Python代码进行验证，仅提供有限的方法指导。</li>
<li>results: 研究发现，在某些情况下，GPT-4可以自主生成和验证假设，但无一个完美的验证结果，表明还有许多挑战需要继续探索，以实现自主研究。<details>
<summary>Abstract</summary>
Research automation efforts usually employ AI as a tool to automate specific tasks within the research process. To create an AI that truly conduct research themselves, it must independently generate hypotheses, design verification plans, and execute verification. Therefore, we investigated if an AI itself could autonomously generate and verify hypothesis for a toy machine learning research problem. We prompted GPT-4 to generate hypotheses and Python code for hypothesis verification with limited methodological guidance. Our findings suggest that, in some instances, GPT-4 can autonomously generate and validate hypotheses without detailed guidance. While this is a promising result, we also found that none of the verifications were flawless, and there remain significant challenges in achieving autonomous, human-level research using only generic instructions. These findings underscore the need for continued exploration to develop a general and autonomous AI researcher.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Deceiving-Semantic-Shortcuts-on-Reasoning-Chains-How-Far-Can-Models-Go-without-Hallucination"><a href="#Deceiving-Semantic-Shortcuts-on-Reasoning-Chains-How-Far-Can-Models-Go-without-Hallucination" class="headerlink" title="Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?"></a>Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09702">http://arxiv.org/abs/2311.09702</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen</li>
<li>for: 这个研究探讨了大型语言模型（LLM）在句子理解中存在的幻觉和不当思维现象，以及这些现象如何影响模型的表现。</li>
<li>methods: 该研究使用了一种新的探测方法和benchmark，称为EureQA，以量化这种现象。该方法从问题中找到可以得到答案的关键Entity，然后逐层添加证据句，让模型在找到答案之前必须按照链式的证据逻辑进行搜寻。</li>
<li>results: 研究发现，现有的LLM模型缺乏能够按照正确的逻辑链进行搜寻和理解句子的能力，而是倾向于通过幻觉和不当思维来简化问题。这些幻觉通常是基于语义关系的，并且会导致模型的偏差和幻觉。<details>
<summary>Abstract</summary>
Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor clues (evidence) that will not directly lead to the correct answer but require a chain-like reasoning process. We evaluate if models can follow the correct reasoning chain instead of short-cutting through distractor clues. We find that existing LLMs lack the necessary capabilities to follow correct reasoning paths and resist the attempt of greedy shortcuts. We show that the distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning.
</details>
<details>
<summary>摘要</summary>
尽管最近的大语言模型（LLM）在许多测试中表现出色，但最新的研究表明这些模型受到了幻觉和不正确的理解的影响。这项研究探讨了 LLM 受到 semantic association 引起的幻觉的特点。我们具体研究 LLM 是否因为提示中的关键词/实体偏见而快速缩短 reasoning 过程。为了衡量这种现象，我们提出了一种新的探测方法和标准 benchmark called EureQA。我们从可以通过 utmost certainty 回答 вопро题的问题开始，并在提示中逐层隐藏关键实体，要求模型通过证据链来找到隐藏的实体。在建构证据时，我们故意将可能导致正确答案的 semantic clue 替换为不直接导致正确答案的 distractor clue，以便要求模型遵循 chain-like 的 reasoning 过程。我们评估模型是否可以遵循正确的 reasoning 路径，而不是短ircuit 通过 distractor clue。我们发现现有的 LLM 缺乏遵循正确 reasoning 路径的能力，并且很容易受到幻觉的影响。我们显示，distractor semantic association frequently leads to model hallucination，这是强有力的证据，证明了现有 LLM 的reasoning 无效。
</details></li>
</ul>
<hr>
<h2 id="Accommodating-Missing-Modalities-in-Time-Continuous-Multimodal-Emotion-Recognition"><a href="#Accommodating-Missing-Modalities-in-Time-Continuous-Multimodal-Emotion-Recognition" class="headerlink" title="Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition"></a>Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10119">http://arxiv.org/abs/2311.10119</a></li>
<li>repo_url: None</li>
<li>paper_authors: Juan Vazquez-Rodriguez, Grégoire Lefebvre, Julien Cumin, James L. Crowley</li>
<li>for: 本研究旨在提高时间连续的情感识别精度，即使有一些模式缺失。</li>
<li>methods: 我们提议一种基于Transformer的新架构，通过跨模式关注和自关注机制来强调时间上的关系，以提高学习过程的稳定性和精度。</li>
<li>results: 实验结果表明，我们的模型在 Ulm-TSST 数据集上的评价协调系数评价提高了37% （对应于情感值预测）和30% （对应于情感评价），相比基线方法。<details>
<summary>Abstract</summary>
Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a time-continuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs. Experimental results on the Ulm-TSST dataset show that our model exhibits an improvement of the concordance correlation coefficient evaluation of 37% when predicting arousal values and 30% when predicting valence values, compared to a late-fusion baseline approach.
</details>
<details>
<summary>摘要</summary>
以下是文本的简化中文翻译：多种研究表明，情感识别更有效率地使用多种modalities。但如果某些modalities缺失呢？为解决这个问题，我们提出了一种基于Transformer架构的时间连续的情感识别模型，即使缺失输入modalities也能够准确地识别情感。我们通过跨modalities和自modalities的相互关注机制来强调时间和模式之间的关系，从而提高模型在弱烈输入上学习的能力。实验结果表明，我们的模型在 Ulm-TSST 数据集上的评价方法比基准方法提高了37% （对于情绪值预测）和30% （对于情感值预测）。
</details></li>
</ul>
<hr>
<h2 id="BLT-Can-Large-Language-Models-Handle-Basic-Legal-Text"><a href="#BLT-Can-Large-Language-Models-Handle-Basic-Legal-Text" class="headerlink" title="BLT: Can Large Language Models Handle Basic Legal Text?"></a>BLT: Can Large Language Models Handle Basic Legal Text?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09693">http://arxiv.org/abs/2311.09693</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/blairstanek/blt">https://github.com/blairstanek/blt</a></li>
<li>paper_authors: Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme</li>
<li>for: 法律执业中需要基础文本处理能力，现有的公共可用LLM like GPT-4和PaLM2表现不佳。</li>
<li>methods: 我们引入了一个 benchmark 来衡量这种 poor performance，这casts 疑问 LLMS 目前是否可靠 для legal practice。</li>
<li>results:  fine-tuning  older LLM 可以带来 near-perfect performance 在我们的测试集上，也提高了相关的法律任务的表现。这个结果强调了 LLMS 训练中需要更多的领域专业知识。<details>
<summary>Abstract</summary>
We find that the best publicly available LLMs like GPT-4 and PaLM 2 currently perform poorly at basic text handling required of lawyers or paralegals, such as looking up the text at a line of a witness deposition or at a subsection of a contract. We introduce a benchmark to quantify this poor performance, which casts into doubt LLMs' current reliability as-is for legal practice. Finetuning for these tasks brings an older LLM to near-perfect performance on our test set and also raises performance on a related legal task. This stark result highlights the need for more domain expertise in LLM training.
</details>
<details>
<summary>摘要</summary>
我团队发现，目前最佳公开可用的LLM（大型语言模型）如GPT-4和PaLM 2在法律领域的基础文本处理任务上表现不佳，如在证人证言或合同下的特定段落中查找文本。我们引入了一个比较方式来衡量这种不佳表现，这casts into doubt LLMPresent reliability in legal practice。经过调整，一个较老的LLM在我们测试集上几乎达到了近乎完美的表现，同时也提高了相关的法律任务表现。这一结果强调了LLM训练中需要更多的领域专业知识。
</details></li>
</ul>
<hr>
<h2 id="Augmenting-Unsupervised-Reinforcement-Learning-with-Self-Reference"><a href="#Augmenting-Unsupervised-Reinforcement-Learning-with-Self-Reference" class="headerlink" title="Augmenting Unsupervised Reinforcement Learning with Self-Reference"></a>Augmenting Unsupervised Reinforcement Learning with Self-Reference</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09692">http://arxiv.org/abs/2311.09692</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andrew Zhao, Erle Zhu, Rui Lu, Matthieu Lin, Yong-Jin Liu, Gao Huang</li>
<li>for: The paper is written for proposing a new approach called Self-Reference (SR) to improve the performance of reinforcement learning agents in the unsupervised pretrain-then-finetune setting.</li>
<li>methods: The SR approach explicitly leverages historical information to mitigate the nonstationarity of intrinsic rewards during pretraining and prevent the unlearning of valuable exploratory behaviors during finetuning.</li>
<li>results: The SR approach achieves state-of-the-art results in terms of Interquartile Mean (IQM) performance and Optimality Gap reduction on the Unsupervised Reinforcement Learning Benchmark for model-free methods, with an 86% IQM and a 16% Optimality Gap reduction. Additionally, it improves current algorithms by up to 17% IQM and reduces the Optimality Gap by 31%.<details>
<summary>Abstract</summary>
Humans possess the ability to draw on past experiences explicitly when learning new tasks and applying them accordingly. We believe this capacity for self-referencing is especially advantageous for reinforcement learning agents in the unsupervised pretrain-then-finetune setting. During pretraining, an agent's past experiences can be explicitly utilized to mitigate the nonstationarity of intrinsic rewards. In the finetuning phase, referencing historical trajectories prevents the unlearning of valuable exploratory behaviors. Motivated by these benefits, we propose the Self-Reference (SR) approach, an add-on module explicitly designed to leverage historical information and enhance agent performance within the pretrain-finetune paradigm. Our approach achieves state-of-the-art results in terms of Interquartile Mean (IQM) performance and Optimality Gap reduction on the Unsupervised Reinforcement Learning Benchmark for model-free methods, recording an 86% IQM and a 16% Optimality Gap. Additionally, it improves current algorithms by up to 17% IQM and reduces the Optimality Gap by 31%. Beyond performance enhancement, the Self-Reference add-on also increases sample efficiency, a crucial attribute for real-world applications.
</details>
<details>
<summary>摘要</summary>
人类具有将过去经验显式地应用于学习新任务的能力，这对于无监督预训练然后精度调整的机器学习代理来说是非常有利的。在预训练阶段，代理可以通过过去经验来抑制内在奖励的非站点性。在精度调整阶段， referencing历史轨迹可以防止探索行为的忘记。基于这些优点，我们提出了自引 Referenced (SR) 方法，这是专门为预训练然后精度调整的 paradigm 设计的一个模块。我们的方法在无监督学习学Benchmark上实现了状态之art 的表现，具有86%的Interquartile Mean (IQM) 和16%的Optimality Gap 减少。此外，它还可以提高当前算法的性能，最高提高17%的IQM和31%的Optimality Gap。此外，SR 方法还可以提高实际应用中的样本效率，这是一个非常重要的特性。
</details></li>
</ul>
<hr>
<h2 id="Do-Physicians-Know-How-to-Prompt-The-Need-for-Automatic-Prompt-Optimization-Help-in-Clinical-Note-Generation"><a href="#Do-Physicians-Know-How-to-Prompt-The-Need-for-Automatic-Prompt-Optimization-Help-in-Clinical-Note-Generation" class="headerlink" title="Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation"></a>Do Physicians Know How to Prompt? The Need for Automatic Prompt Optimization Help in Clinical Note Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09684">http://arxiv.org/abs/2311.09684</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zonghai Yao, Ahmed Jaafar, Beining Wang, Yue Zhu, Zhichao Yang, Hong Yu</li>
<li>for: 这项研究探讨了对大语言模型（LLM）在医疗笔记生成中的表现，并提出了自动提示优化（APO）框架来优化初始提示。</li>
<li>methods: 研究使用了GPT3.5和GPT4两个语言模型，并对它们进行了APO优化。</li>
<li>results: 结果显示GPT4 APO在标准化提示质量方面表现出色，而且专业人员在APO后仍然保持了内容质量。<details>
<summary>Abstract</summary>
This study examines the effect of prompt engineering on the performance of Large Language Models (LLMs) in clinical note generation. We introduce an Automatic Prompt Optimization (APO) framework to refine initial prompts and compare the outputs of medical experts, non-medical experts, and APO-enhanced GPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in standardizing prompt quality across clinical note sections. A human-in-the-loop approach shows that experts maintain content quality post-APO, with a preference for their own modifications, suggesting the value of expert customization. We recommend a two-phase optimization process, leveraging APO-GPT4 for consistency and expert input for personalization.
</details>
<details>
<summary>摘要</summary>
这项研究研究了大语言模型（LLM）在医疗记录生成中的表现，并评估了提示工程的影响。我们提出了自动提示优化（APO）框架，以改进初始提示并比较医学专家、非医学专家和APO-加强GPT3.5和GPT4的输出。结果显示GPT4 APO在标准化提示质量方面表现出色，而人类在循环中的干预表明专家保持了提示质量的控制，并偏好自己的修改，这表明了专家自定义的价值。我们建议使用两个阶段优化过程，首先使用APO-GPT4保证一致性，然后通过专家输入进行个性化。
</details></li>
</ul>
<hr>
<h2 id="MacGyver-Are-Large-Language-Models-Creative-Problem-Solvers"><a href="#MacGyver-Are-Large-Language-Models-Creative-Problem-Solvers" class="headerlink" title="MacGyver: Are Large Language Models Creative Problem Solvers?"></a>MacGyver: Are Large Language Models Creative Problem Solvers?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09682">http://arxiv.org/abs/2311.09682</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman</li>
<li>for:  This paper aims to explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting, specifically in circumventing functional fixedness.</li>
<li>methods: The paper uses an automatically generated dataset called MacGyver, which consists of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking ‘out-of-the-box’. The paper compares and contrasts the problem-solving abilities of LLMs and humans on this dataset.</li>
<li>results: The paper shows that both LLMs and humans struggle with the MacGyver problems, but in different ways. LLMs are prone to overconfidence and propose physically infeasible or inefficient solutions, while humans excel in solving familiar problems but struggle with tasks requiring domain-specific knowledge. The paper also demonstrates the potential of enhancing LLMs’ problem-solving ability with novel prompting techniques.<details>
<summary>Abstract</summary>
We explore the creative problem-solving capabilities of modern large language models (LLMs) in a constrained setting. The setting requires circumventing a cognitive bias known in psychology as ''functional fixedness'' to use familiar objects in innovative or unconventional ways. To this end, we create MacGyver, an automatically generated dataset consisting of 1,600 real-world problems that deliberately trigger functional fixedness and require thinking 'out-of-the-box'. We then present our collection of problems to both LLMs and humans to compare and contrast their problem-solving abilities. We show that MacGyver is challenging for both groups, but in unique and complementary ways. For example, humans typically excel in solving problems that they are familiar with but may struggle with tasks requiring domain-specific knowledge, leading to a higher variance. On the other hand, LLMs, being exposed to a variety of highly specialized knowledge, attempt broader problems but are prone to overconfidence and propose actions that are physically infeasible or inefficient. We also provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work provides insight into the creative problem-solving capabilities of humans and AI and illustrates how psychological paradigms can be extended into large-scale tasks for comparing humans and machines.
</details>
<details>
<summary>摘要</summary>
我们探索现代大语言模型（LLM）的创造力问题解决能力在限制性的设定下。这个设定要求绕过心理学中的''功能固化''（functional fixedness）来使用familiar对象在创新或非正式的方式上。为此，我们创建了MacGyver数据集，包含1,600个实际世界问题，旨在触发功能固化并需要''思外框''的思维。然后，我们将这些问题提交给LLM和人类，以比较和对比他们的问题解决能力。我们发现MacGyver对两个组合体是挑战的，但是各自unique和补做的。例如，人类通常在 familar的问题上 excel，但可能会在需要域pecific知识的任务上遇到问题，导致更高的变差。而LLMs，作为承载了多种高度特殊化的知识的，尝试更广泛的问题，但容易过度自信和提出物理不可能或不fficient的操作。我们还提供了LLMs的详细错误分析，并示出了使用迭代步骤反思和多元思维技巧来增强其问题解决能力的潜在。这项工作提供了人类和AI的创造力问题解决能力的视角，并示出了将心理学概念扩展到大规模任务上，用于比较人类和机器的能力。
</details></li>
</ul>
<hr>
<h2 id="Trustworthy-Large-Models-in-Vision-A-Survey"><a href="#Trustworthy-Large-Models-in-Vision-A-Survey" class="headerlink" title="Trustworthy Large Models in Vision: A Survey"></a>Trustworthy Large Models in Vision: A Survey</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09680">http://arxiv.org/abs/2311.09680</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyan Guo, Jun Liu</li>
<li>for: 本文旨在探讨 Large Models (LMs) 在 computer vision 领域中的可靠性问题，并提出相关挑战和对策。</li>
<li>methods: 本文使用了系统性的方法，包括简述了四种可能会妨碍 LMs 在视觉领域中的可靠使用的问题，并提供了对应的挑战、对策和讨论。</li>
<li>results: 本文通过对 LMs 在视觉领域中的可靠性问题进行系统性的探讨，提供了 deeper understanding 的概念和对策，以便promote LMs 在人类社会中的可靠使用。<details>
<summary>Abstract</summary>
The rapid progress of Large Models (LMs) has recently revolutionized various fields of deep learning with remarkable grades, ranging from Natural Language Processing (NLP) to Computer Vision (CV). However, LMs are increasingly challenged and criticized by academia and industry due to their powerful performance but untrustworthy behavior, which urgently needs to be alleviated in reliable methods. Despite the abundance of literature on trustworthy LMs in language, a systematic survey specifically delving into the trustworthiness of LMs in vision remains absent. In order to mitigate this gap, we summarize four relevant concerns that obstruct the trustworthy usage in vision of LMs in this survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4) interpretability. By highlighting corresponding challenge, countermeasures, and discussion in each topic, we hope this survey will facilitate readers' understanding of the field, promote alignment of LMs with human expectations and enable trustworthy LMs to serve as welfare rather than disaster for human society.
</details>
<details>
<summary>摘要</summary>
大型模型（LM）的快速进步最近对深度学习多个领域产生了很大的改变，从自然语言处理（NLP）到计算机视觉（CV）。然而，LM在学术和业界的应用中遇到了强大性的挑战和批评，需要可靠的方法来缓解这些问题。尽管有很多关于可靠LMs的语言文献，但是关于视觉领域中LMs的可靠性的系统性调查仍然缺失。为了弥补这个差距，我们在这份报告中总结了四个对视觉领域LMs可靠性的挑战，包括1）人类违用、2）抵触、3）内在问题和4）可解性。通过对每个话题的挑战、对策和讨论进行强调，我们希望通过这份报告能够帮助读者更好地理解这个领域，促进LMs与人类期望的Alignment，使LMs成为人类社会的福利而不是灾难。
</details></li>
</ul>
<hr>
<h2 id="Structured-Chemistry-Reasoning-with-Large-Language-Models"><a href="#Structured-Chemistry-Reasoning-with-Large-Language-Models" class="headerlink" title="Structured Chemistry Reasoning with Large Language Models"></a>Structured Chemistry Reasoning with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09656">http://arxiv.org/abs/2311.09656</a></li>
<li>repo_url: None</li>
<li>paper_authors: Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Jiawei Han, Lianhui Qin</li>
<li>for: 解决复杂化学问题的大自然语言模型（LLM）问题。</li>
<li>methods: 提出了一种新的结构化思维方法——InstructChem，可以显著提高 LLM 的化学思维能力。InstructChem 分解为三个关键句，包括化学式生成、逐步思维和迭代审阅改进。</li>
<li>results: 对四种化学挑战进行了广泛的实验，包括量子化学、量子力学、物理化学和化学动力学。结果显示，我们的方法可以显著提高 GPT-4 的化学思维能力，具体达到了8%的平均绝对改进和30%的峰值改进。此外，我们还使用 GPT-4 生成的思维来 fine-tune  smaller LMs（如 Vicuna），并观察到了这些 smaller LMs 的强大改进。这 Validates 我们的方法，并允许 LLMs 生成高质量的思维。<details>
<summary>Abstract</summary>
This paper studies the problem of solving complex chemistry problems with large language models (LLMs). Despite the extensive general knowledge in LLMs (such as GPT-4), they struggle with chemistry reasoning that requires faithful grounded reasoning with diverse chemical knowledge and an integrative understanding of chemical interactions. We propose InstructChem, a new structured reasoning approach that substantially boosts the LLMs' chemical reasoning capabilities. InstructChem explicitly decomposes the reasoning into three critical phrases, including chemical formulae generation by LLMs that offers the basis for subsequent grounded reasoning, step-by-step reasoning that makes multi-step derivations with the identified formulae for a preliminary answer, and iterative review-and-refinement that steers LLMs to progressively revise the previous phases for increasing confidence, leading to the final high-confidence answer. We conduct extensive experiments on four different chemistry challenges, including quantum chemistry, quantum mechanics, physical chemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on chemistry reasoning, yielding an 8% average absolute improvement and a 30% peak improvement. We further use the generated reasoning by GPT-4 to fine-tune smaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs. This validates our approach and enables LLMs to generate high-quality reasoning.
</details>
<details>
<summary>摘要</summary>
InstructChem breaks down the reasoning process into three critical phases:1. Chemical formula generation: LLMs generate the chemical formula as the basis for subsequent grounded reasoning.2. Step-by-step reasoning: LLMs make multi-step derivations with the identified formula to arrive at a preliminary answer.3. Iterative review-and-refinement: LLMs revise the previous phases to increase confidence and eventually arrive at a high-confidence answer.We conduct extensive experiments on four chemistry challenges: quantum chemistry, quantum mechanics, physical chemistry, and chemistry kinetics. Our approach achieves an average absolute improvement of 8% and a peak improvement of 30% over GPT-4 on chemistry reasoning tasks. Moreover, we use the generated reasoning by GPT-4 to fine-tune smaller LMs (e.g., Vicuna) and observe significant improvement in their performance, validating our approach. This demonstrates that LLMs can generate high-quality reasoning with the help of InstructChem.
</details></li>
</ul>
<hr>
<h2 id="“It’s-not-like-Jarvis-but-it’s-pretty-close-”-–-Examining-ChatGPT’s-Usage-among-Undergraduate-Students-in-Computer-Science"><a href="#“It’s-not-like-Jarvis-but-it’s-pretty-close-”-–-Examining-ChatGPT’s-Usage-among-Undergraduate-Students-in-Computer-Science" class="headerlink" title="“It’s not like Jarvis, but it’s pretty close!” – Examining ChatGPT’s Usage among Undergraduate Students in Computer Science"></a>“It’s not like Jarvis, but it’s pretty close!” – Examining ChatGPT’s Usage among Undergraduate Students in Computer Science</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09651">http://arxiv.org/abs/2311.09651</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ishika Joshi, Ritvik Budhiraja, Harshal D Akolekar, Jagat Sesh Challa, Dhruv Kumar</li>
<li>for: 本研究旨在了解学生如何使用 ChatGPT 作为课程相关任务的工具。</li>
<li>methods: 本研究使用学生调查和面试获取了学生对 ChatGPT 的看法和体验，以及他们在使用中遇到的挑战和改进建议。</li>
<li>results: 研究发现大多数学生（超过 57%）对使用 ChatGPT 作为课程相关任务的工具有积极的看法，但也提出了一些需要解决的挑战，以便在长期使用中得到学生的acceptance。<details>
<summary>Abstract</summary>
Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students. The findings from this investigation have broader implications and may be applicable to other LLMs and their role in computing education.
</details>
<details>
<summary>摘要</summary>
大型自然语言模型（LLM）如ChatGPT和Google Bard在学术社区中受到了广泛的关注。先前的研究已经评估了这些LLM在不同应用场景中的性能，但这些评估大多由教师和研究人员进行，未经考虑学生的实际使用情况。这项研究采用学生第一的方法，以全面了解学生在使用ChatGPT时的各种优点、挑战和改进建议。我们通过学生问卷和面试获得了价值的反馈，了解学生对ChatGPT的批评和建议。我们发现超过57%的学生对使用ChatGPT为课程任务提供帮助表示积极的看法。然而，我们的研究也揭示了长期Acceptance of ChatGPT amongst students。这些发现对其他LLM和计算教育有广泛的意义。
</details></li>
</ul>
<hr>
<h2 id="On-the-Exploitability-of-Reinforcement-Learning-with-Human-Feedback-for-Large-Language-Models"><a href="#On-the-Exploitability-of-Reinforcement-Learning-with-Human-Feedback-for-Large-Language-Models" class="headerlink" title="On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models"></a>On the Exploitability of Reinforcement Learning with Human Feedback for Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09641">http://arxiv.org/abs/2311.09641</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao</li>
<li>for: 防止语言模型（LLMs）被恶意攻击，保持其与人类偏好的 aligning。</li>
<li>methods: 提出了一种攻击方法 RankPoison，通过对偏好选择的权重赋值进行预言，使 LLMS 生成更长的序列，而不会影响原始的安全对齐性表现。</li>
<li>results: 通过使用 RankPoison，可以实现攻击 LLMS，使其生成更长的答案，并且可以在问题中Trigger Word 的情况下实现后门攻击。这些发现 highlighted RLHF 的安全挑战，强调了更加Robust的对齐方法的需求。<details>
<summary>Abstract</summary>
Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.
</details>
<details>
<summary>摘要</summary>
大自然语言模型（LLM）与人类偏好的重塑学习（RLHF）是一种方法，用于将 LLM 与人类偏好进行对应。尽管它有优点，但RLHF 依赖于人类标注者来评分文本，这可能引入潜在的安全漏洞，如果任何敌对标注者（例如，攻击者）操纵分数，以使 LLM 进行恶意操作。为了评估 RLHF 对人类偏好数据毒化的红色队伍，我们提出了 RankPoison 攻击方法，即在偏好排名中选择扰乱的方法来达到恶意行为（例如，生成更长的序列，这可能增加计算成本）。使用 RankPoison 生成的毒化数据集，我们可以对 LLM 进行毒化攻击，以生成更长的 токен，而不会伤害原始的安全对齐性表现。此外，我们还成功地实现了后门攻击，使 LLM 可以在问题中的词TriggerWord 下生成更长的答案。我们的发现高亮了 RLHF 中的安全挑战，这让我们更需要更加robust的对齐方法来保护 LLM。
</details></li>
</ul>
<hr>
<h2 id="Automatic-Engineering-of-Long-Prompts"><a href="#Automatic-Engineering-of-Long-Prompts" class="headerlink" title="Automatic Engineering of Long Prompts"></a>Automatic Engineering of Long Prompts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10117">http://arxiv.org/abs/2311.10117</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Sfedfcv/redesigned-pancake">https://github.com/Sfedfcv/redesigned-pancake</a></li>
<li>paper_authors: Cho-Jui Hsieh, Si Si, Felix X. Yu, Inderjit S. Dhillon</li>
<li>for:  automatic long prompt engineering for LLMs</li>
<li>methods:  greedy algorithms, genetic algorithms, and LLM-based mutation</li>
<li>results:  average accuracy gain of 9.2% on eight tasks in Big Bench HardHere’s the full answer in Simplified Chinese:</li>
<li>for: 自动生成长提示 для LLM</li>
<li>methods: 简单的排序算法、进化算法和基于 LLM 的突变</li>
<li>results:  eight tasks in Big Bench Hard 的平均精度提升率为 9.2%I hope that helps! Let me know if you have any other questions.<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated remarkable capabilities in solving complex open-domain tasks, guided by comprehensive instructions and demonstrations provided in the form of prompts. However, these prompts can be lengthy, often comprising hundreds of lines and thousands of tokens, and their design often requires considerable human effort. Recent research has explored automatic prompt engineering for short prompts, typically consisting of one or a few sentences. However, the automatic design of long prompts remains a challenging problem due to its immense search space. In this paper, we investigate the performance of greedy algorithms and genetic algorithms for automatic long prompt engineering. We demonstrate that a simple greedy approach with beam search outperforms other methods in terms of search efficiency. Moreover, we introduce two novel techniques that utilize search history to enhance the effectiveness of LLM-based mutation in our search algorithm. Our results show that the proposed automatic long prompt engineering algorithm achieves an average of 9.2% accuracy gain on eight tasks in Big Bench Hard, highlighting the significance of automating prompt designs to fully harness the capabilities of LLMs.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）在解决复杂的开放领域任务上表现出了很好的能力，受到详细的指令和示例的引导。然而，这些示例通常是非常长，可能包含数百行和数千个符号，其设计通常需要较多的人工努力。现有研究已经探索了自动生成短示例，通常只有一些句子或数个句子。然而，自动设计长示例仍然是一个具有巨大搜索空间的困难问题。在这篇论文中，我们调查了大型语言模型（LLM）引导的自动长示例工程。我们发现，使用搜索缓存的简单排序法比其他方法更高效。此外，我们还介绍了两种使用搜索历史来增强LLM基于搜索算法的变异效果的新技术。我们的结果表明，我们的自动长示例工程算法在Big Bench Hard上的八个任务中平均提高了9.2%的准确率，这 highlights了自动化示例设计以便满分 LLMs 的可能性。
</details></li>
</ul>
<hr>
<h2 id="Online-Continual-Knowledge-Learning-for-Language-Models"><a href="#Online-Continual-Knowledge-Learning-for-Language-Models" class="headerlink" title="Online Continual Knowledge Learning for Language Models"></a>Online Continual Knowledge Learning for Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09632">http://arxiv.org/abs/2311.09632</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhao Wu, Tongjun Shi, Karthick Sharma, Chun Wei Seah, Shuhao Zhang</li>
<li>for: 本研究旨在解决语言模型（LLM）中的动态世界知识管理问题，以满足实时环境中的问题解决和事实核查。</li>
<li>methods: 本文提出了一个新的 continual learning 问题，即在线上动态知识学习（OCKL）问题，并提出了一个新的评价指标来衡量知识获得率和先前学习知识的保留。</li>
<li>results: 我们的实验结果表明，现有的 continual learning 方法无法解决 OCKL 问题，而我们的研究带来了关于如何在不断变化的环境中训练 LLM 的新理解。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) serve as repositories of extensive world knowledge, enabling them to perform tasks such as question-answering and fact-checking. However, this knowledge can become obsolete as global contexts change. In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL). This problem formulation aims to manage the dynamic nature of world knowledge in LMs under real-time constraints. We propose a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge. Our empirical evaluation, conducted using a variety of state-of-the-art methods, establishes robust base-lines for OCKL. Our results reveal that existing continual learning approaches are unfortunately insufficient for tackling the unique challenges posed by OCKL. We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large Language Models" is translated as "大型语言模型" (dàxíng yǔyán módelǐng)* "Continual Learning" is translated as "连续学习" (liánxù xuéxí)* "Online Continual Knowledge Learning" is translated as "在线连续知识学习" (zài xiàng liánxù zhīshī xuéxí)* "world knowledge" is translated as "世界知识" (shìjiè zhīshī)* "global contexts" is translated as "全球背景" (quánqiú bèngjǐng)* "dynamic nature" is translated as "动态性" (dòngtǐ xìng)* "real-time constraints" is translated as "实时约束" (shíhòu yuēsuǒ)* "benchmark" is translated as "标准" (biāo jiā)* "evaluation metric" is translated as "评价指标" (píngjì zhǐbiāo)* "rate of new knowledge acquisition" is translated as "新知识获得速率" (xīn zhīshī gòngdé sùlù)* "retention of previously learned knowledge" is translated as "前期学习知识保持" (qiánxī xuéxí zhīshī bǎochí)* "trade-off" is translated as "交互" (jiāoxì)* "key factors" is translated as "关键因素" (guānjī yǐnxiàng)* "continually evolving environment" is translated as "不断发展的环境" (bùdàn fāzhǎn de huánjìng)
</details></li>
</ul>
<hr>
<h2 id="CRISPR-Eliminating-Bias-Neurons-from-an-Instruction-following-Language-Model"><a href="#CRISPR-Eliminating-Bias-Neurons-from-an-Instruction-following-Language-Model" class="headerlink" title="CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model"></a>CRISPR: Eliminating Bias Neurons from an Instruction-following Language Model</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09627">http://arxiv.org/abs/2311.09627</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nakyeong Yang, Taegwan Kang, Kyomin Jung</li>
<li>for: 这个论文是为了解决语言模型（LLMs）在基于指令的任务执行过程中遇到的分布差异问题，以及这些差异如何导致语言模型受到干扰和偏见。</li>
<li>methods: 这个论文提出了一种新的偏见缓和方法，称为CRISPR，用于减少基于指令的偏见。CRISPR使用了负责任方法来 identific 受偏见影响的偏见神经元，并使用了剪除来消除这些偏见神经元。</li>
<li>results: 实验结果表明，CRISPR可以有效地减少基于指令的偏见，提高语言模型在社会偏见benchmark上的表现，而无需损失先前学习的知识。此外，CRISPR是模型无关的，可以适应不断变化的社会偏见。<details>
<summary>Abstract</summary>
Large language models (LLMs) executing tasks through instruction-based prompts often face challenges stemming from distribution differences between user instructions and training instructions. This leads to distractions and biases, especially when dealing with inconsistent dynamic labels. In this paper, we introduces a novel bias mitigation method, CRISPR, designed to alleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods to identify bias neurons influencing biased outputs and employs pruning to eliminate the bias neurons. Experimental results demonstrate the method's effectiveness in mitigating biases in instruction-based prompting, enhancing language model performance on social bias benchmarks without compromising pre-existing knowledge. CRISPR proves highly practical, model-agnostic, offering flexibility in adapting to evolving social biases.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）通过指令式提示进行任务时，常会面临用户指令和训练指令之间的分布差异问题，这会导致分心和偏袋问题，特别是对于不稳定的动态标签。在这篇论文中，我们介绍了一种新的偏见缓和方法，名为CRISPR，用于缓和指令标签偏见在LLM中。CRISPR利用属性方法来识别偏见神经元影响偏见输出，并使用剪除来消除这些偏见神经元。实验结果显示CRISPR有效地缓和偏见在指令式提示中，提高语言模型在社会偏见标准上的表现，不会对先前的知识造成损害。CRISPR非常实用、模型无须对类型，可以灵活地适应社会偏见的变化。
</details></li>
</ul>
<hr>
<h2 id="AI-Recommendation-System-for-Enhanced-Customer-Experience-A-Novel-Image-to-Text-Method"><a href="#AI-Recommendation-System-for-Enhanced-Customer-Experience-A-Novel-Image-to-Text-Method" class="headerlink" title="AI Recommendation System for Enhanced Customer Experience: A Novel Image-to-Text Method"></a>AI Recommendation System for Enhanced Customer Experience: A Novel Image-to-Text Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09624">http://arxiv.org/abs/2311.09624</a></li>
<li>repo_url: None</li>
<li>paper_authors: Mohamaed Foued Ayedi, Hiba Ben Salem, Soulaimen Hammami, Ahmed Ben Said, Rateb Jabbar, Achraf CHabbouh</li>
<li>for: 这项研究旨在提供精准和个性化的时尚推荐系统，使用人工智能技术进行细致的视觉解释，以帮助用户找到与愿望图像中的风格相似的时尚产品。</li>
<li>methods: 该研究使用了一个全新的端到端管道，其包括图像上传、对图像进行分类、对图像进行描述、对全球时尚产品目录进行检索、并将检索结果与原图像进行比较。</li>
<li>results: 在使用了 более than 100,000个分类的时尚照片数据集上进行训练和评估后，该管道实现了0.97的F1分数，表明其可以准确地识别时尚对象，并且可以为用户提供个性化的时尚推荐。<details>
<summary>Abstract</summary>
Existing fashion recommendation systems encounter difficulties in using visual data for accurate and personalized recommendations. This research describes an innovative end-to-end pipeline that uses artificial intelligence to provide fine-grained visual interpretation for fashion recommendations. When customers upload images of desired products or outfits, the system automatically generates meaningful descriptions emphasizing stylistic elements. These captions guide retrieval from a global fashion product catalogue to offer similar alternatives that fit the visual characteristics of the original image. On a dataset of over 100,000 categorized fashion photos, the pipeline was trained and evaluated. The F1-score for the object detection model was 0.97, exhibiting exact fashion object recognition capabilities optimized for recommendation. This visually aware system represents a key advancement in customer engagement through personalized fashion recommendations
</details>
<details>
<summary>摘要</summary>
现有的时尚推荐系统在使用视觉数据进行准确和个性化推荐时遇到困难。本研究描述了一种创新的端到端管道，使用人工智能来为时尚推荐提供细腻的视觉解释。当客户上传欲购买的产品或服装图片时，系统会自动生成有意义的描述，强调时尚元素。这些描述将引导从全球时尚产品目录中选择类似的产品，以适应原图的视觉特征。在超过10万个分类时尚照片的数据集上训练和评估，管道的F1分数为0.97，表示系统具有高精度的时尚物品识别能力，优化为推荐。这种视觉意识系统将成为个性化时尚推荐的关键进步。
</details></li>
</ul>
<hr>
<h2 id="Comprehensive-Evaluation-and-Insights-into-the-Use-of-Deep-Neural-Networks-to-Detect-and-Quantify-Lymphoma-Lesions-in-PET-CT-Images"><a href="#Comprehensive-Evaluation-and-Insights-into-the-Use-of-Deep-Neural-Networks-to-Detect-and-Quantify-Lymphoma-Lesions-in-PET-CT-Images" class="headerlink" title="Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET&#x2F;CT Images"></a>Comprehensive Evaluation and Insights into the Use of Deep Neural Networks to Detect and Quantify Lymphoma Lesions in PET&#x2F;CT Images</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09614">http://arxiv.org/abs/2311.09614</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/microsoft/lymphoma-segmentation-dnn">https://github.com/microsoft/lymphoma-segmentation-dnn</a></li>
<li>paper_authors: Shadab Ahamed, Yixi Xu, Claire Gowdy, Joo H. O, Ingrid Bloise, Don Wilson, Patrick Martineau, François Bénard, Fereshteh Yousefirizi, Rahul Dodhia, Juan M. Lavista, William B. Weeks, Carlos F. Uribe, Arman Rahmim<br>for:This paper evaluates the performance of four deep learning architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion segmentation from PET&#x2F;CT images.methods:The paper uses a diverse, multi-institutional dataset of 611 cases to train, validate, and test the four neural network architectures. The authors use internal testing and external testing to evaluate the performance of the networks, and they assess reproducibility of six lesion measures, calculate prediction errors, and examine DSC performance in relation to lesion measures.results:The results show that SegResNet is the top performer with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml on the internal testing set. On the unseen external test set, SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best false negative volume (FNV) of 0.41 ml. The authors also found that the networks had a median false negative volume (FNV) of 0 ml. Additionally, the authors introduced three lesion detection criteria, addressed the challenges in segmenting “easy” vs. “hard” cases, and performed inter-observer agreement assessment.Here is the same information in Simplified Chinese text:for:这个研究用四种深度学习架构（UNet、SegResNet、DynUNet、SwinUNETR）进行淋巴癌肿囊分 segmentation from PET&#x2F;CT图像。methods:这个研究使用多个机构的多例数据集（611例）来训练、验证和测试四种深度学习架构。作者们使用内测和外测来评估这些网络的性能，并评估了六个肿囊指标的重复性，计算预测错误，并对肿囊指标与深度学习架构之间的关系进行了研究。results:结果显示，SegResNet在内测集上得到了最高的 median Dice similarity coefficient（DSC）值为0.76，并且 median false positive volume（FPV）值为4.55ml。在未看过的外测集上，SegResNet得到了最高的 median DSC值为0.68和FPV值为21.46ml，而UNet得到了最低的 false negative volume（FNV）值为0.41ml。此外，作者们发现所有网络都有0ml的false negative volume（FNV）。此外，作者们还引入了三个肿囊检测标准，解决了检测”容易”vs.”Difficult”情况的挑战，并进行了多个专家 annotator 的一致性评估。<details>
<summary>Abstract</summary>
This study performs comprehensive evaluation of four neural network architectures (UNet, SegResNet, DynUNet, and SwinUNETR) for lymphoma lesion segmentation from PET/CT images. These networks were trained, validated, and tested on a diverse, multi-institutional dataset of 611 cases. Internal testing (88 cases; total metabolic tumor volume (TMTV) range [0.52, 2300] ml) showed SegResNet as the top performer with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml; all networks had a median false negative volume (FNV) of 0 ml. On the unseen external test set (145 cases with TMTV range: [0.10, 2480] ml), SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml. We assessed reproducibility of six lesion measures, calculated their prediction errors, and examined DSC performance in relation to these lesion measures, offering insights into segmentation accuracy and clinical relevance. Additionally, we introduced three lesion detection criteria, addressing the clinical need for identifying lesions, counting them, and segmenting based on metabolic characteristics. We also performed expert intra-observer variability analysis revealing the challenges in segmenting ``easy'' vs. ``hard'' cases, to assist in the development of more resilient segmentation algorithms. Finally, we performed inter-observer agreement assessment underscoring the importance of a standardized ground truth segmentation protocol involving multiple expert annotators. Code is available at: https://github.com/microsoft/lymphoma-segmentation-dnn
</details>
<details>
<summary>摘要</summary>
internally, SegResNet showed the highest performance with a median Dice similarity coefficient (DSC) of 0.76 and median false positive volume (FPV) of 4.55 ml. All models had a median false negative volume (FNV) of 0 ml. On the external test set, SegResNet achieved the best median DSC of 0.68 and FPV of 21.46 ml, while UNet had the best FNV of 0.41 ml.The study also assessed the reproducibility of six lesion measures, calculated prediction errors, and examined DSC performance in relation to these lesion measures. Additionally, the study introduced three lesion detection criteria, addressed the clinical need for identifying, counting, and segmenting based on metabolic characteristics.The study also performed expert intra-observer variability analysis, revealing the challenges in segmenting "easy" vs. "hard" cases, and inter-observer agreement assessment, underscoring the importance of a standardized ground truth segmentation protocol involving multiple expert annotators.The code for the study is available at: https://github.com/microsoft/lymphoma-segmentation-dnn.
</details></li>
</ul>
<hr>
<h2 id="Digital-Socrates-Evaluating-LLMs-through-explanation-critiques"><a href="#Digital-Socrates-Evaluating-LLMs-through-explanation-critiques" class="headerlink" title="Digital Socrates: Evaluating LLMs through explanation critiques"></a>Digital Socrates: Evaluating LLMs through explanation critiques</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09613">http://arxiv.org/abs/2311.09613</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuling Gu, Oyvind Tafjord, Peter Clark</li>
<li>for: 本研究的目的是解释现代模型的解释能力，并提供一种自动生成高质量、可读性的解释评价工具。</li>
<li>methods: 本研究使用了定义新的解释批判任务，创建了大量人工验证的数据集，并使用这些数据集训练了一个开源的自动解释批判模型（称为“数字索慈”）。</li>
<li>results: 通过量化和质量分析，本研究表明了数字索慈可以帮助揭示学生模型的思维链，并提供高质量、细化的自动解释评价。数字索慈因此填补了现有的解释评价工具之间的重要空白。<details>
<summary>Abstract</summary>
While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critiquing model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.
</details>
<details>
<summary>摘要</summary>
而LMs可以提供结构化的解释，但是这些解释的质量和性质仍然不够理解。因此，我们的目标是定义现代模型的解释能力的详细方式，并创建一个自动生成这些 caracterizations的解释评价工具，不需要Expensive API调用或人工注释。我们的方法包括：（a）定义解释批判任务——标识和分类任何主要缺陷在解释中，并提供修复建议。（b）创建大量，人工验证的数据集。（c）使用这些数据集，训练一个开源的自动批判模型（称为数字SOCRATES）。通过量化和质量分析，我们示出了数字SOCRATES如何用于探索学生模型的逻辑链，以及如何提供高质量、细化的自动评价。数字SOCRATES因此填充了现代模型解释行为的评价工具中的重要空白。
</details></li>
</ul>
<hr>
<h2 id="Code-Models-are-Zero-shot-Precondition-Reasoners"><a href="#Code-Models-are-Zero-shot-Precondition-Reasoners" class="headerlink" title="Code Models are Zero-shot Precondition Reasoners"></a>Code Models are Zero-shot Precondition Reasoners</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09601">http://arxiv.org/abs/2311.09601</a></li>
<li>repo_url: None</li>
<li>paper_authors: Lajanugen Logeswaran, Sungryull Sohn, Yiwei Lyu, Anthony Zhe Liu, Dong-Ki Kim, Dongsub Shim, Moontae Lee, Honglak Lee</li>
<li>for: 这篇论文旨在探讨如何使用代码表示来理解行为前提条件，以便在Sequential Decision Making任务中完成任务。</li>
<li>methods: 这篇论文使用了预训练的代码模型，从示例轨迹中提取出行动前提条件，并使用这些前提条件来预测行动。</li>
<li>results: 根据论文的结果，使用这种前condition-aware的行动采样策略可以提高几何 shot 策略学习的性能，并在任务 oriented dialog 和 embodied textworld 测试 benchmark 上达到了更好的结果。<details>
<summary>Abstract</summary>
One of the fundamental skills required for an agent acting in an environment to complete tasks is the ability to understand what actions are plausible at any given point. This work explores a novel use of code representations to reason about action preconditions for sequential decision making tasks. Code representations offer the flexibility to model procedural activities and associated constraints as well as the ability to execute and verify constraint satisfaction. Leveraging code representations, we extract action preconditions from demonstration trajectories in a zero-shot manner using pre-trained code models. Given these extracted preconditions, we propose a precondition-aware action sampling strategy that ensures actions predicted by a policy are consistent with preconditions. We demonstrate that the proposed approach enhances the performance of few-shot policy learning approaches across task-oriented dialog and embodied textworld benchmarks.
</details>
<details>
<summary>摘要</summary>
一个基本的技能需要在环境中完成任务是理解当前点可行的动作。这项工作探讨一种使用代码表示来理解动作前提条件的新用途。代码表示具有模拟过程活动和相关约束的灵活性，以及执行和验证约束满足的能力。通过代码表示，我们从示例轨迹中提取动作前提条件，无需任何更改或训练。基于提取的前提条件，我们提议一种了解政策预测的动作抽样策略，以确保政策预测的动作与前提条件相符。我们证明，该方法可以提高几个shot策略学习的性能在任务强调对话和embodied textworld benchmark上。
</details></li>
</ul>
<hr>
<h2 id="Multi-Step-Dialogue-Workflow-Action-Prediction"><a href="#Multi-Step-Dialogue-Workflow-Action-Prediction" class="headerlink" title="Multi-Step Dialogue Workflow Action Prediction"></a>Multi-Step Dialogue Workflow Action Prediction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09593">http://arxiv.org/abs/2311.09593</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ramya Ramakrishnan, Ethan Elenberg, Hashan Narangodage, Ryan McDonald</li>
<li>for: 提高对对话任务的自动化率，增加对话系统的效率和智能化 Waterfall </li>
<li>methods: 提出了三种简单实现的模型方法：1）精度调整训练集，2）几步学习利用检索和大语言模型提示，3）零步图 traversal </li>
<li>results: 实现了20%的步骤自动化，不需要人工监督 Waterfall Here’s the translation in English:</li>
<li>for: Improving the automation rate of dialogue tasks, increasing the efficiency and intelligence of conversation systems.</li>
<li>methods: Proposed three simple modeling methods: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction.</li>
<li>results: Achieved 20% automation of steps without requiring as much human oversight.<details>
<summary>Abstract</summary>
In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback from a human overseeing the system.
</details>
<details>
<summary>摘要</summary>
在任务导向对话中，系统经常需要遵循一系列动作，称为工作流程，以完成任务。在这篇论文中，我们提出了多步工作流程动作预测的新问题，其中系统预测多个未来的工作流程动作。准确预测多个步骤可以实现多轮自动化，这可以释放时间专注更复杂的任务。我们提出了三种简单实现的模型方法：1）精度调整训练集，2）几招在 Context 中学习和大语言模型提示，3）零shot图 traversal，即将历史动作序列聚合成图进行预测。我们显示，多步动作预测生成了下游对话任务的准确预测特征，并可以提高自动化步骤的效率达20%，不需要人工监督系统提供多少反馈。
</details></li>
</ul>
<hr>
<h2 id="Tied-Lora-Enhacing-parameter-efficiency-of-LoRA-with-weight-tying"><a href="#Tied-Lora-Enhacing-parameter-efficiency-of-LoRA-with-weight-tying" class="headerlink" title="Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying"></a>Tied-Lora: Enhacing parameter efficiency of LoRA with weight tying</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09578">http://arxiv.org/abs/2311.09578</a></li>
<li>repo_url: None</li>
<li>paper_authors: Adithya Renduchintala, Tugrul Konuk, Oleksii Kuchaiev</li>
<li>for: 提高LoRA方法的参数效率</li>
<li>methods: 使用权重固定和选择性训练</li>
<li>results: 实现了与标准LoRA方法相当的性能，使用的参数比例减少至13%Here’s a breakdown of each point:</li>
<li>for: The paper is written to improve the parameter efficiency of the Low-rank adaptation (LoRA) method.</li>
<li>methods: The paper proposes a simple paradigm that utilizes weight tying and selective training to further increase the parameter efficiency of LoRA.</li>
<li>results: The paper provides experiments that demonstrate the effectiveness of the proposed Tied-LoRA method, achieving comparable performance to the standard LoRA method while using only 13% of the parameters.<details>
<summary>Abstract</summary>
We propose Tied-LoRA, a simple paradigm utilizes weight tying and selective training to further increase parameter efficiency of the Low-rank adaptation (LoRA) method. Our investigations include all feasible combinations parameter training/freezing in conjunction with weight tying to identify the optimal balance between performance and the number of trainable parameters. Through experiments covering a variety of tasks and two base language models, we provide analysis revealing trade-offs between efficiency and performance. Our experiments uncovered a particular Tied-LoRA configuration that stands out by demonstrating comparable performance across several tasks while employing only 13~\% percent of parameters utilized by the standard LoRA method.
</details>
<details>
<summary>摘要</summary>
我们提出了缔绳LoRA（Tied-LoRA）方法，这是一种简单的思想，通过权重缔绳和选择性训练来进一步提高LoRA方法中的参数效率。我们对所有可能的参数训练/冻结 combinational进行了调查，以确定最佳的效率和性能之间的平衡。通过覆盖多种任务和两个基础语言模型的实验，我们提供了分析，揭示了效率和性能之间的交易。我们的实验发现，一种特定的Tied-LoRA配置可以在多个任务中达到相似的性能水平，使用了标准LoRA方法的13\%的参数。
</details></li>
</ul>
<hr>
<h2 id="Work-State-Centric-AI-Agents-Design-Implementation-and-Management-of-Cognitive-Work-Threads"><a href="#Work-State-Centric-AI-Agents-Design-Implementation-and-Management-of-Cognitive-Work-Threads" class="headerlink" title="Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads"></a>Work State-Centric AI Agents: Design, Implementation, and Management of Cognitive Work Threads</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09576">http://arxiv.org/abs/2311.09576</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chen Zhang</li>
<li>for: 提高任务执行效率和任务分析预测</li>
<li>methods: 使用工作笔记记录和反思循环来捕捉工作状态信息，并将工作状态记录作为全面的工作日志</li>
<li>results: 提高任务执行效率，并为后续任务分析和审核提供坚实的基础<details>
<summary>Abstract</summary>
AI agents excel in executing predefined tasks, but the dynamic management of work state information during task execution remains an underexplored area. We propose a work state-centric AI agent model employing "work notes" to record and reflect the state throughout task execution. This paper details the model's architecture, featuring worker threads for task oversight, planner modules for task decomposition and planning, and executor modules for performing subtasks using a ReAct-inspired thought-action loop. We provide an exhaustive work state record incorporating plans and outcomes, constituting a comprehensive work journal. Our results show that this model not only improves task execution efficiency but also lays a solid foundation for subsequent task analysis and auditing.
</details>
<details>
<summary>摘要</summary>
人工智能代理人 excel 在预定任务执行方面，但是在任务执行过程中的工作状态管理仍然是一个未经探索的领域。我们提出了一种基于“工作笔记”的人工智能代理人模型，该模型包括工作线程对任务监督、规划模块对任务分解和规划、执行模块使用ReAct-类似的思维动作循环来执行具体任务。我们提供了全面的工作状态记录，包括计划和结果，这个全面的工作日志可以帮助进一步分析和审核任务。我们的结果表明，这种模型不仅可以提高任务执行效率，还可以为后续任务分析和审核提供坚实的基础。
</details></li>
</ul>
<hr>
<h2 id="LymphoML-An-interpretable-artificial-intelligence-based-method-identifies-morphologic-features-that-correlate-with-lymphoma-subtype"><a href="#LymphoML-An-interpretable-artificial-intelligence-based-method-identifies-morphologic-features-that-correlate-with-lymphoma-subtype" class="headerlink" title="LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype"></a>LymphoML: An interpretable artificial intelligence-based method identifies morphologic features that correlate with lymphoma subtype</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09574">http://arxiv.org/abs/2311.09574</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/rajpurkarlab/lymphoml">https://github.com/rajpurkarlab/lymphoml</a></li>
<li>paper_authors: Vivek Shankar, Xiaoli Yang, Vrishab Krishna, Brent Tan, Oscar Silva, Rebecca Rojansky, Andrew Ng, Fabiola Valvert, Edward Briercheck, David Weinstock, Yasodha Natkunam, Sebastian Fernandez-Pol, Pranav Rajpurkar<br>for:* 这份研究旨在开发一个可解释的机器学习方法，以便更正确地分类淋巴癌亚型。methods:* 这个方法包括处理HE染色标本核心、分 segment nuclei 和 cells、计算包括 morphology、texture 和 architecture 的特征，并使用梯度增强模型进行诊断预测。results:* 这个方法的可解释模型，在使用限量HE染色标本上进行训练后，与使用整个标本图像进行诊断相比，具有不 inferior 的诊断精度。* 使用 SHAP 分析法，发现 nuclei 形态特征是 DLBCL 和класси型淋巴癌的最有拘束力的特征（F1-score：78.7%）。* 这个研究还证明了一个结合 H&amp;E 染色标本和标准化的6个免疫标本的模型，可以实现与46个标本 panel 的相似的诊断精度（85.3%）。<details>
<summary>Abstract</summary>
The accurate classification of lymphoma subtypes using hematoxylin and eosin (H&E)-stained tissue is complicated by the wide range of morphological features these cancers can exhibit. We present LymphoML - an interpretable machine learning method that identifies morphologic features that correlate with lymphoma subtypes. Our method applies steps to process H&E-stained tissue microarray cores, segment nuclei and cells, compute features encompassing morphology, texture, and architecture, and train gradient-boosted models to make diagnostic predictions. LymphoML's interpretable models, developed on a limited volume of H&E-stained tissue, achieve non-inferior diagnostic accuracy to pathologists using whole-slide images and outperform black box deep-learning on a dataset of 670 cases from Guatemala spanning 8 lymphoma subtypes. Using SHapley Additive exPlanation (SHAP) analysis, we assess the impact of each feature on model prediction and find that nuclear shape features are most discriminative for DLBCL (F1-score: 78.7%) and classical Hodgkin lymphoma (F1-score: 74.5%). Finally, we provide the first demonstration that a model combining features from H&E-stained tissue with features from a standardized panel of 6 immunostains results in a similar diagnostic accuracy (85.3%) to a 46-stain panel (86.1%).
</details>
<details>
<summary>摘要</summary>
“准确分类淋巴癌亚型使用染色剂和艾索维（H&E）染色的组织是具有广泛的 morphological 特征的复杂任务。我们提出了 LymphoML 方法，这是一种可读性高的机器学习方法，可以识别淋巴癌亚型的 morphologic 特征。我们的方法包括处理 H&E 染色组织微阵列核心、分Segment 细胞和核lei、计算包括形态、Texture 和建筑的特征，并使用梯度优化模型进行诊断预测。LymphoML 的可读性模型，在一小量的 H&E 染色组织上进行开发，与全图像进行诊断的病理学家相比，具有不 inferior 的诊断精度，并且超过黑盒深度学习。使用 SHapley Additive exPlanation（SHAP）分析，我们评估了模型预测中每个特征的影响，发现核lei 形态特征是 DLBCL （F1-score：78.7%）和 класиical Hodgkin 淋巴癌（F1-score：74.5%）中最有决定性的。最后，我们提供了首次证明，一个结合 H&E 染色组织和标准化的6个抗体染色的模型，具有与46个抗体染色的模型（86.1%）相同的诊断精度（85.3%）。”
</details></li>
</ul>
<hr>
<h2 id="Prompt-Optimisation-with-Random-Sampling"><a href="#Prompt-Optimisation-with-Random-Sampling" class="headerlink" title="Prompt Optimisation with Random Sampling"></a>Prompt Optimisation with Random Sampling</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09569">http://arxiv.org/abs/2311.09569</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Lu, Jiayi Wang, Sebastian Riedel, Pontus Stenetorp</li>
<li>for: 这个论文主要是为了探讨语言模型可以用来生成任务相关的分隔符的可能性，并证明了这种方法可以达到类似于人工curated prompts的性能。</li>
<li>methods: 这篇论文使用了三种Random generation strategies来生成分隔符，包括随机选择 vocabulary 中的token，以及基于语言模型的生成方法。</li>
<li>results: 实验结果显示，使用随机生成的分隔符可以提高 text classification 的性能，相比于人工curated prompts，平均提高16%，并与自动生成的 prompt searching 方法相当。<details>
<summary>Abstract</summary>
Using the generative nature of a language model to generate task-relevant separators has shown competitive results compared to human-curated prompts like "TL;DR". We demonstrate that even randomly chosen tokens from the vocabulary as separators can achieve near-state-of-the-art performance. We analyse this phenomenon in detail using three different random generation strategies, establishing that the language space is rich with potential good separators, regardless of the underlying language model size. These observations challenge the common assumption that an effective prompt should be human-readable or task-relevant. Experimental results show that using random separators leads to an average 16% relative improvement across nine text classification tasks on seven language models, compared to human-curated separators, and is on par with automatic prompt searching methods.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="LongBoX-Evaluating-Transformers-on-Long-Sequence-Clinical-Tasks"><a href="#LongBoX-Evaluating-Transformers-on-Long-Sequence-Clinical-Tasks" class="headerlink" title="LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks"></a>LongBoX: Evaluating Transformers on Long-Sequence Clinical Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09564">http://arxiv.org/abs/2311.09564</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/mihir3009/longbox">https://github.com/mihir3009/longbox</a></li>
<li>paper_authors: Mihir Parmar, Aakanksha Naik, Himanshu Gupta, Disha Agrawal, Chitta Baral</li>
<li>for: 这 paper 旨在评估大型语言模型 (LLMs) 在医疗领域中的长序处理能力。</li>
<li>methods: 该 paper 使用了 seven 个医疗领域的文本数据集，并对两种长序处理技术进行评估：(i) 本地-全局注意力和 (ii) Fusion-in-Decoder (FiD)。</li>
<li>results: 该 paper 的初步实验表明，医疗领域的 LLMs (例如 BioGPT) 和通用领域的 LLMs (例如 FLAN-T5) 在这个 benchmark 上表现不佳，并且两种长序处理技术在一些数据集上得到了混乱的结果。<details>
<summary>Abstract</summary>
Many large language models (LLMs) for medicine have largely been evaluated on short texts, and their ability to handle longer sequences such as a complete electronic health record (EHR) has not been systematically explored. Assessing these models on long sequences is crucial since prior work in the general domain has demonstrated performance degradation of LLMs on longer texts. Motivated by this, we introduce LongBoX, a collection of seven medical datasets in text-to-text format, designed to investigate model performance on long sequences. Preliminary experiments reveal that both medical LLMs (e.g., BioGPT) and strong general domain LLMs (e.g., FLAN-T5) struggle on this benchmark. We further evaluate two techniques designed for long-sequence handling: (i) local-global attention, and (ii) Fusion-in-Decoder (FiD). Our results demonstrate mixed results with long-sequence handling - while scores on some datasets increase, there is substantial room for improvement. We hope that LongBoX facilitates the development of more effective long-sequence techniques for the medical domain. Data and source code are available at https://github.com/Mihir3009/LongBoX.
</details>
<details>
<summary>摘要</summary>
许多大型语言模型（LLMs）在医学领域的评估主要基于短文本，而对于完整的电子医疗记录（EHR）的评估尚未得到系统性的探讨。考虑到此，我们提出了LongBoX，一个包含七个医学数据集，用于调查模型在长序列上的性能。我们的初步实验表明，医学LLMs（如BioGPT）以及通用领域LLMs（如FLAN-T5）在这个benchmark上表现不佳。我们还评估了两种适合长序列处理的技术：（i）本地-全局注意力，以及（ii）FiD（混合在解码器中）。我们的结果表明，虽有一些数据集的分数提高，但是还有很大的提升空间。我们希望LongBoX可以促进医学领域中更有效的长序列处理技术的发展。数据和源代码可以在https://github.com/Mihir3009/LongBoX上下载。
</details></li>
</ul>
<hr>
<h2 id="Enchancing-Semi-Supervised-Learning-for-Extractive-Summarization-with-an-LLM-based-pseudolabeler"><a href="#Enchancing-Semi-Supervised-Learning-for-Extractive-Summarization-with-an-LLM-based-pseudolabeler" class="headerlink" title="Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler"></a>Enchancing Semi-Supervised Learning for Extractive Summarization with an LLM-based pseudolabeler</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09559">http://arxiv.org/abs/2311.09559</a></li>
<li>repo_url: None</li>
<li>paper_authors: Gaurav Sahu, Olga Vechtomova, Issam H. Laradji</li>
<li>for: 这个研究是用于解决有限标注数据场景下的抽取文本概要问题。</li>
<li>methods: 这个方法使用一种半supervised的approach，提议一种prompt-based Pseudolabel选择策略，使用GPT-4进行评估和生成 Pseudolabels。</li>
<li>results: 实验表明，通过使用LLM评估和生成 Pseudolabels，可以提高ROUGE-1的表现，在不同的dataset上提高10-20%，与增强预训练模型相当。此外，这种方法需要更少的无标例示例来实现更好的表现。<details>
<summary>Abstract</summary>
This work tackles the task of extractive text summarization in a limited labeled data scenario using a semi-supervised approach. Specifically, we propose a prompt-based pseudolabel selection strategy using GPT-4. We evaluate our method on three text summarization datasets: TweetSumm, WikiHow, and ArXiv/PubMed. Our experiments show that by using an LLM to evaluate and generate pseudolabels, we can improve the ROUGE-1 by 10-20\% on the different datasets, which is akin to enhancing pretrained models. We also show that such a method needs a smaller pool of unlabeled examples to perform better.
</details>
<details>
<summary>摘要</summary>
这个工作面临有限标注数据enario中的抽取文本摘要任务，使用半supervised方法。我们提议一种基于提示的pseudolabel选择策略，使用GPT-4。我们在三个文本摘要数据集上进行了测试：TweetSumm、WikiHow和ArXiv/PubMed。我们的实验结果表明，通过使用LLM评估和生成pseudolabels，可以提高ROUGE-1的表现，在不同的数据集上提高10-20%，这与增强预训练模型相似。此外，我们还发现这种方法需要较少的无标注示例来表现更好。Note: Please note that the translation is in Simplified Chinese, which is one of the two standard forms of Chinese writing.
</details></li>
</ul>
<hr>
<h2 id="Program-Aided-Reasoners-better-Know-What-They-Know"><a href="#Program-Aided-Reasoners-better-Know-What-They-Know" class="headerlink" title="Program-Aided Reasoners (better) Know What They Know"></a>Program-Aided Reasoners (better) Know What They Know</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09553">http://arxiv.org/abs/2311.09553</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/houstoncuj/Educating-for-the-Large-Shop-to-make-Custom-Name-Patches">https://github.com/houstoncuj/Educating-for-the-Large-Shop-to-make-Custom-Name-Patches</a></li>
<li>paper_authors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig</li>
<li>for: 该研究旨在评估程序帮助语言模型（PAL）和文本基于的链条（COT）提问技术的准确性和准确性评估结果。</li>
<li>methods: 该研究使用了5个数据集和2种模型类型（LLaMA模型和OpenAI模型），并对PAL和COT技术进行比较。</li>
<li>results: 研究发现，PAL在75%的情况下具有更高的准确性评估结果，并且发现使用温度缩放法可以降低生成的多样性，从而提高PAL的准确性和准确性评估结果。<details>
<summary>Abstract</summary>
Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to "know what they know", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Scaling-User-Modeling-Large-scale-Online-User-Representations-for-Ads-Personalization-in-Meta"><a href="#Scaling-User-Modeling-Large-scale-Online-User-Representations-for-Ads-Personalization-in-Meta" class="headerlink" title="Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta"></a>Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09544">http://arxiv.org/abs/2311.09544</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, Kai Wang, Zhangyuan Wang, Zhengxing Chen, Min Li, Fenggang Wu, Minghai Chen, Huayu Li, Yunnan Wu, Zhan Shu, Mindi Yuan, Sri Reddy</li>
<li>for: 该论文旨在提高个性化广告的效果，但是训练吞吐量、服务延迟和内存等约束限制了在线广告排序模型的复杂性和输入特征集。</li>
<li>methods: 作者提出了Scaling User Modeling（SUM）框架，通过一些指定的上游用户模型来合成用户嵌入，从 massive amounts of用户特征中进行高级模型化技术。这些嵌入然后作为下游在线广告排序模型的输入，以提高效率。</li>
<li>results: 作者通过实验证明SUM框架在Meta的广告排序系统中的广泛部署，每天处理数百十亿个用户请求，并且提供了显著的在线指标增长和基础设施成本减少。<details>
<summary>Abstract</summary>
Effective user representations are pivotal in personalized advertising. However, stringent constraints on training throughput, serving latency, and memory, often limit the complexity and input feature set of online ads ranking models. This challenge is magnified in extensive systems like Meta's, which encompass hundreds of models with diverse specifications, rendering the tailoring of user representation learning for each model impractical. To address these challenges, we present Scaling User Modeling (SUM), a framework widely deployed in Meta's ads ranking system, designed to facilitate efficient and scalable sharing of online user representation across hundreds of ads models. SUM leverages a few designated upstream user models to synthesize user embeddings from massive amounts of user features with advanced modeling techniques. These embeddings then serve as inputs to downstream online ads ranking models, promoting efficient representation sharing. To adapt to the dynamic nature of user features and ensure embedding freshness, we designed SUM Online Asynchronous Platform (SOAP), a latency free online serving system complemented with model freshness and embedding stabilization, which enables frequent user model updates and online inference of user embeddings upon each user request. We share our hands-on deployment experiences for the SUM framework and validate its superiority through comprehensive experiments. To date, SUM has been launched to hundreds of ads ranking models in Meta, processing hundreds of billions of user requests daily, yielding significant online metric gains and infrastructure cost savings.
</details>
<details>
<summary>摘要</summary>
实用用户表现是在个性化广告中核心的。然而，训练过程中的约束和服务延迟、内存限制，通常限制了在线广告排序模型的复杂度和输入特征集。这个挑战在Meta的架构中变得更加突出，这里涉及到多种不同的模型，使得为每个模型自适应用户表示学习变得不实际。为解决这些挑战，我们提出了扩展用户模型（SUM）框架，用于在Meta的广告排序系统中实现有效的用户表示共享。SUM使用一些指定的上游用户模型来合成大量用户特征的用户嵌入，然后将这些嵌入作为下游在线广告排序模型的输入，以便有效地共享用户表示。为了适应用户特征的动态变化和确保嵌入的新鲜度，我们设计了SUM在线异步平台（SOAP），该平台具有零延迟的在线服务系统，并且具有模型新鲜度和嵌入稳定性，可以实现在线用户模型更新和用户请求时的嵌入推理。我们在 SUM 框架的部署经验和实验结果中分享我们的经验。至今，SUM 已经在Meta的广告排序系统中发布到了多百个模型，每天处理多百亿次用户请求，并且实现了 significan 的在线指标增长和基础设施成本减少。
</details></li>
</ul>
<hr>
<h2 id="HelpSteer-Multi-attribute-Helpfulness-Dataset-for-SteerLM"><a href="#HelpSteer-Multi-attribute-Helpfulness-Dataset-for-SteerLM" class="headerlink" title="HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM"></a>HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09528">http://arxiv.org/abs/2311.09528</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, Oleksii Kuchaiev</li>
<li>for: The paper aims to address the problem of existing open-source helpfulness preference datasets not specifying what makes some responses more helpful and others less so, and to provide a solution by collecting a multi-attribute helpfulness dataset annotated for various aspects that make responses helpful.</li>
<li>methods: The paper uses a dataset called HelpSteer, which is a 37k-sample dataset annotated for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. The paper also uses the SteerLM technique to train a model on the dataset.</li>
<li>results: The paper reports that training a model on the HelpSteer dataset with the SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4).<details>
<summary>Abstract</summary>
Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer
</details>
<details>
<summary>摘要</summary>
现有的开源有用性偏好数据集不 specify 有用性回答的特点。模型在这些数据集上训练可能会意外地学习数据集的特性（例如，偏爱 longer  pero 无用的回答只因其长度）。为解决这个问题，我们收集了 HelpSteer 数据集，这是一个多 Attribute 有用性数据集，包括回答的正确性、一致性、复杂性和 verbosity 等方面的标注，以及回答的总有用性。使用 HelpSteer 数据集和 SteerLM 技术训练 Llama 2 70B 模型，得到的分数为 7.54 在 MT Bench，当前为开放模型不需要更强大的模型（如 GPT4）的训练数据而得到的最高分。我们在 https://huggingface.co/datasets/nvidia/HelpSteer 发布了这个数据集，协议为 CC-BY-4.0。
</details></li>
</ul>
<hr>
<h2 id="MDFL-Multi-domain-Diffusion-driven-Feature-Learning"><a href="#MDFL-Multi-domain-Diffusion-driven-Feature-Learning" class="headerlink" title="MDFL: Multi-domain Diffusion-driven Feature Learning"></a>MDFL: Multi-domain Diffusion-driven Feature Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09520">http://arxiv.org/abs/2311.09520</a></li>
<li>repo_url: None</li>
<li>paper_authors: Daixun Li, Weiying Xie, Jiaqing Zhang, Yunsong Li</li>
<li>for: 本研究旨在提高高维像素扩展的数据特征提取性能，以揭示高维数据的内在异常和结构。</li>
<li>methods: 该研究提出了一种多域扩散驱动特征学习网络（MDFL），该方法利用扩散基于 posterior 抽样来考虑多个域结构之间的共同信息交互，从而消除视觉模型中的面具效应。此外，该方法还提出了一种特征重用机制，以收集高维数据的深度和原始特征。</li>
<li>results: 实验结果表明，MDFL 可以明显提高高维数据特征提取性能，其平均总准确率达到 98.25%，超过了多种现有基线方案。<details>
<summary>Abstract</summary>
High-dimensional images, known for their rich semantic information, are widely applied in remote sensing and other fields. The spatial information in these images reflects the object's texture features, while the spectral information reveals the potential spectral representations across different bands. Currently, the understanding of high-dimensional images remains limited to a single-domain perspective with performance degradation. Motivated by the masking texture effect observed in the human visual system, we present a multi-domain diffusion-driven feature learning network (MDFL) , a scheme to redefine the effective information domain that the model really focuses on. This method employs diffusion-based posterior sampling to explicitly consider joint information interactions between the high-dimensional manifold structures in the spectral, spatial, and frequency domains, thereby eliminating the influence of masking texture effects in visual models. Additionally, we introduce a feature reuse mechanism to gather deep and raw features of high-dimensional data. We demonstrate that MDFL significantly improves the feature extraction performance of high-dimensional data, thereby providing a powerful aid for revealing the intrinsic patterns and structures of such data. The experimental results on three multi-modal remote sensing datasets show that MDFL reaches an average overall accuracy of 98.25%, outperforming various state-of-the-art baseline schemes. The code will be released, contributing to the computer vision community.
</details>
<details>
<summary>摘要</summary>
高维度图像，rich in semantic information，广泛应用于远程感知和其他领域。图像空间信息反映物体的文本特征，而spectral信息揭示不同频谱域的可能性表示。现在，高维度图像的理解受限于单个领域视角，性能下降。为了解决这个问题，我们提出了一种多领域扩散驱动特征学习网络（MDFL），该方法重新定义了模型真正关注的信息Domain。该方法利用扩散基于 posterior sampling 来显式地考虑高维度映射结构在spectral、 spatial和频率域之间的联合信息互动，从而消除观察者模型中的掩蔽文本效应。此外，我们引入了特征重用机制，以收集高维数据的深度和原始特征。我们示出，MDFL可以显著改善高维数据特征提取性能，从而为揭示高维数据内部征性和结构提供强大的帮助。实验结果表明，MDFL在三个多modal远程感知数据集上达到了98.25%的平均总准确率，超过了多种现状顶峰方案。代码将被发布，为计算机视觉社区做出贡献。
</details></li>
</ul>
<hr>
<h2 id="SegMix-A-Simple-Structure-Aware-Data-Augmentation-Method"><a href="#SegMix-A-Simple-Structure-Aware-Data-Augmentation-Method" class="headerlink" title="SegMix: A Simple Structure-Aware Data Augmentation Method"></a>SegMix: A Simple Structure-Aware Data Augmentation Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09505">http://arxiv.org/abs/2311.09505</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing</li>
<li>for: 这个论文主要用于提出一种基于 interpolate 的数据扩充（DA）方法，以提高自然语言处理（NLP）任务中的模型性能。</li>
<li>methods: 这个论文使用了 interpolation 方法来 linearly interpolate 训练示例的输入和标签。它还提出了一种名为 SegMix 的数据扩充框架，该框架可以适应任务特定的结构。</li>
<li>results: 实验结果表明，SegMix 可以在Named Entity Recognition (NER) 和 Relation Extraction (RE) 任务中提高性能，特别是在数据缺乏情况下。此外，这种方法较容易实现，增加了训练时间的负担也很小。<details>
<summary>Abstract</summary>
Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.
</details>
<details>
<summary>摘要</summary>
优化数据augmentation（DA）方法（mixup） linearly interpolate 输入和标签的两个或更多的训练例子。mixup在自然语言处理（NLP）领域被应用于序列标注任务。然而，这种直接采用方法不会考虑NLP任务中的结构。为此，我们提议SegMix，一个包含 interpolation-based DA算法的集合，可以适应任务特定的结构。SegMix具有较少的数据结构约束，对各种 гипер参数设置 exhibit robustness, 可以应用于更多的任务设置，并增加了小的计算负担。在算法核心中，我们通过 interpolate 方法在任务特定的有意义段上进行 interpolate，而不是在序列上如先前的工作所做。我们发现SegMix是一个灵活的框架，可以将规则基于DA方法与 interpolate-based方法混合，创造出有趣的DA技术的混合。我们发现SegMix在名实Recognition（NER）和关系抽取（RE）任务中 consistently 提高性能，特别是在数据缺乏的设置下。此外，这种方法易于实现，并且增加了训练过程中的负担。
</details></li>
</ul>
<hr>
<h2 id="Adaptive-Interventions-with-User-Defined-Goals-for-Health-Behavior-Change"><a href="#Adaptive-Interventions-with-User-Defined-Goals-for-Health-Behavior-Change" class="headerlink" title="Adaptive Interventions with User-Defined Goals for Health Behavior Change"></a>Adaptive Interventions with User-Defined Goals for Health Behavior Change</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09483">http://arxiv.org/abs/2311.09483</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aishwarya Mandyam, Matthew Joerke, Barbara E. Engelhardt, Emma Brunskill</li>
<li>for: 本研究旨在提高移动医疗应用的 физи活动促进效果，通过个性化目标设定来提高用户参与度和持续性。</li>
<li>methods: 本研究使用了修改了汤姆逊抽样算法，通过优化个性化奖励函数来实现个性化目标设定。</li>
<li>results: 在物理活动模拟器中，我们的算法可以减少各种基线的累累 regret，并且在不共享数据或不优化个性化奖励函数的情况下具有更好的性能。<details>
<summary>Abstract</summary>
Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret while preserving the sample complexity benefits of data sharing. In a physical activity simulator, we demonstrate that our algorithm achieves substantial improvements in cumulative regret compared to baselines that do not share data or do not optimize for individualized rewards.
</details>
<details>
<summary>摘要</summary>
physical inactivity remains a major public health concern, with associations to adverse health outcomes such as cardiovascular disease and type-2 diabetes. mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, but often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. this paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. as a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. we prove that our modification incurs only a constant penalty on the cumulative regret while preserving the sample complexity benefits of data sharing. in a physical activity simulator, we demonstrate that our algorithm achieves substantial improvements in cumulative regret compared to baselines that do not share data or do not optimize for individualized rewards.Here's the translation in Traditional Chinese as well:体力无动作仍然是公共健康的主要忧虑，与不良的健康结果相关，如心血管疾病和型二糖尿病。 mobilhealth应用程序表示了低成本、扩展性的体育活动促进的吸引点，但通常受到小效果和低投入率的限制，尤其在人类教练相比。 目标设定是体育教练中的重要 Component，对于移动健康应用程序的自适性优化，尚未获得充分利用。 本文介绍了对 Thompson 抽样算法的修改，将优先级设置为个人化的目标设定，通过优化个人化的赏金函数来实现。 为支持目标设定，本文提出了一种均衡的方法，可以利用共享结构，同时优化个人偏好和目标。 我们证明了我们的修改仅增加了常数的责任，保留了数据分享的样本复杂性的好处。 在物理活动 simulator 中，我们显示了我们的算法在不共享数据或不优化个人化赏金函数的基准下，实现了很大的累累 regret。
</details></li>
</ul>
<hr>
<h2 id="ARES-An-Automated-Evaluation-Framework-for-Retrieval-Augmented-Generation-Systems"><a href="#ARES-An-Automated-Evaluation-Framework-for-Retrieval-Augmented-Generation-Systems" class="headerlink" title="ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"></a>ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09476">http://arxiv.org/abs/2311.09476</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-futuredata/ares">https://github.com/stanford-futuredata/ares</a></li>
<li>paper_authors: Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia</li>
<li>for: 评估抽取增强生成（RAG）系统的质量，通常需要手动标注输入查询、文章和回答。</li>
<li>methods: 我们介绍了一个自动化的RAG评估系统（ARES），用于评估RAG系统的上下文相关性、答案准确性和答案相关性。ARES使用灵活的语言模型（LM）judge进行评估，并使用小量人工标注数据进行预测推断（PPI）来减少预测错误。</li>
<li>results: ARES可以准确评估RAG系统，只需要使用几百个人工标注数据进行评估。此外，ARES的评估标准可以适应不同的领域和类型的查询和文章，并保持评估标准的有效性。代码和数据可以在<a target="_blank" rel="noopener" href="https://github.com/stanford-futuredata/ARES%E4%B8%8A%E4%B8%8B%E8%BD%BD%E5%92%8C%E4%BD%BF%E7%94%A8%E3%80%82">https://github.com/stanford-futuredata/ARES上下载和使用。</a><details>
<summary>Abstract</summary>
Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. Using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.
</details>
<details>
<summary>摘要</summary>
evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. we introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. using synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. to mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). across six different knowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG systems while using a few hundred human annotations during evaluation. furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. we make our datasets and code for replication and deployment available at https://github.com/stanford-futuredata/ARES.Here's the translation in Traditional Chinese:评估 Retrieval-augmented Generation (RAG) 系统传统上靠手动标注 Input queries、Passages to retrieve 和 Response 来评估。我们介绍 ARES，一个自动化 RAG 评估系统，可以在 Context relevance、Answer faithfulness 和 Answer relevance 的维度上评估 RAG 系统。使用人工训练数据，ARES 精确地评估 RAG 系统中各个元件的质量。为了减少预测错误，ARES 使用一小量的人工标注数据进行预测力测试 (PPI)。在 KILT 和 SuperGLUE 中的六个知识密集任务上，ARES 精确地评估 RAG 系统，仅需使用一些百个人工标注。此外，ARES 的评估判别器还能够在领域转移时保持有效，并在改变查询和/或文档类型时仍然精确地评估 RAG 系统。我们在 <https://github.com/stanford-futuredata/ARES> 上提供了数据和代码，以便复制和部署。
</details></li>
</ul>
<hr>
<h2 id="JAB-Joint-Adversarial-Prompting-and-Belief-Augmentation"><a href="#JAB-Joint-Adversarial-Prompting-and-Belief-Augmentation" class="headerlink" title="JAB: Joint Adversarial Prompting and Belief Augmentation"></a>JAB: Joint Adversarial Prompting and Belief Augmentation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09473">http://arxiv.org/abs/2311.09473</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ninareh Mehrabi, Palash Goyal, Anil Ramakrishna, Jwala Dhamala, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta</li>
<li>for: 这篇论文的目的是提高语言模型的安全性和可靠性。</li>
<li>methods: 这篇论文使用了对目标模型的挑战性询问和信念增强，通过迭代反馈循环来提高挑战性询问和信念增强的效果。</li>
<li>results: 在实验中，这篇论文显示了这种框架可以在动态和静态情况下降低对目标模型的攻击性内容生成。<details>
<summary>Abstract</summary>
With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.
</details>
<details>
<summary>摘要</summary>
随着语言模型在不同应用场景中的普及，对这些模型的安全性和可靠性的关注也在不断增加。我们在这篇文章中提出了一种联合框架，通过对黑盒目标模型进行抗对抗探测和信念增强，使其具备更高的安全性和可靠性。这种框架利用自动化的红团攻击方法来探测目标模型，同时使用信念增强器来生成对目标模型进行增强其对抗性的指令。重要的是，抗对抗模型和信念生成器都利用过去互动的反馈来提高对抗提示和信念的效果。在我们的实验中，我们证明了这种框架可以在直接交互的动态场景中减少毒害内容生成，以及使用静态 benchmark数据集来评估我们的模型时也能减少毒害内容生成。
</details></li>
</ul>
<hr>
<h2 id="Think-While-You-Write-Hypothesis-Verification-Promotes-Faithful-Knowledge-to-Text-Generation"><a href="#Think-While-You-Write-Hypothesis-Verification-Promotes-Faithful-Knowledge-to-Text-Generation" class="headerlink" title="Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation"></a>Think While You Write: Hypothesis Verification Promotes Faithful Knowledge-to-Text Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09467">http://arxiv.org/abs/2311.09467</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yifu Qiu, Varun Embar, Shay B. Cohen, Benjamin Han</li>
<li>for: 提高神经网络生成模型的 faithfulness，减少幻像现象</li>
<li>methods: 提出了一种新的解码方法 called TWEAK，使用假设验证模型（HVM）对生成的语句进行排序，以确保语句与输入信息一致</li>
<li>results: TWEAK variants 在 FactKB、WebNLG 和 TekGen&#x2F;GenWiki 上的 faithfulness 和 quality 都得到了提高，但是唯一的代价是 slight degradation （0.14&#x2F;0.32 points）in quality measured by BERTScore。<details>
<summary>Abstract</summary>
Neural knowledge-to-text generation models often struggle to faithfully generate descriptions for the input facts: they may produce hallucinations that contradict the given facts, or describe facts not present in the input. To reduce hallucinations, we propose a novel decoding method, TWEAK (Think While Effectively Articulating Knowledge). TWEAK treats the generated sequences at each decoding step and its future sequences as hypotheses, and ranks each generation candidate based on how well their corresponding hypotheses support the input facts using a Hypothesis Verification Model (HVM). We first demonstrate the effectiveness of TWEAK by using a Natural Language Inference (NLI) model as the HVM and report improved faithfulness with minimal impact on the quality. We then replace the NLI model with our task-specific HVM trained with a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which pairs input facts with their faithful and hallucinated descriptions with the hallucinated spans marked. The new HVM improves the faithfulness and the quality further and runs faster. Overall the best TWEAK variants improve on average 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and TekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality measured by BERTScore over the same datasets. Since TWEAK is a decoding-only approach, it can be integrated with any neural generative model without retraining.
</details>
<details>
<summary>摘要</summary>
neural knowledge-to-text生成模型经常难以准确地生成输入事实的描述：它们可能会产生幻觉，或者描述不在输入中的事实。为了减少幻觉，我们提出了一种新的解码方法，叫做调整（Think While Effectively Articulating Knowledge）。调整方法会在每个解码步骤中对生成的序列和未来序列进行处理，并将每个生成候选者根据其对输入事实的支持程度进行排序，使用一个假设验证模型（HVM）。我们首先使用一个自然语言推理（NLI）模型作为HVM，并发现使用NLI模型可以提高准确性，同时具有最小的影响。然后，我们将NLI模型 replaced with我们自己的任务特定的HVM，用一个具有唯一性的数据集—— факт-alignment textual entailment（FATE），这个数据集 pairs输入事实与其准确的描述和幻觉描述的幻觉 span。新的HVM可以进一步提高准确性和质量，同时具有更快的运行速度。总的来说，最佳的调整变体可以在FactKB上提高准确性平均2.22/7.17点，同时保持质量水平，BERTScore上的平均下降0.14/0.32点。由于调整是解码-only方法，因此可以与任何 neural生成模型无需重新训练。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.AI_2023_11_16/" data-id="clpztdncm007fes88exxlcjv5" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
    <article id="post-cs.CL_2023_11_16" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <h3 href="/2023/11/16/cs.CL_2023_11_16/" class="article-date">
  <time datetime="2023-11-16T11:00:00.000Z" itemprop="datePublished">2023-11-16</time>
</h3>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/cs-CL/">cs.CL</a>
  </div>

  </div>
  <div class="article-inner">
  <div class="curve-down">
  <div class="fill-content">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2023/11/16/cs.CL_2023_11_16/">cs.CL - 2023-11-16</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
        <h2 id="Latent-Feature-based-Data-Splits-to-Improve-Generalisation-Evaluation-A-Hate-Speech-Detection-Case-Study"><a href="#Latent-Feature-based-Data-Splits-to-Improve-Generalisation-Evaluation-A-Hate-Speech-Detection-Case-Study" class="headerlink" title="Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study"></a>Latent Feature-based Data Splits to Improve Generalisation Evaluation: A Hate Speech Detection Case Study</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10236">http://arxiv.org/abs/2311.10236</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maikezuefle/latent-feature-splits">https://github.com/maikezuefle/latent-feature-splits</a></li>
<li>paper_authors: Maike Züfle, Verna Dankers, Ivan Titov</li>
<li>for: 提高社交媒体平台上的仇恨言语检测系统的Robustness，避免模型过拟合特定目标和关键词。</li>
<li>methods: 使用新的训练测试分割方法，包括Subset-Sum-Split和Closest-Split，对两个数据集和四个预训练模型进行测试。</li>
<li>results: 研究发现，当模型面临到偏移的数据分布时，其表现很差，这表明任务难度不一定是人类可理解的。研究还发现，不同的数据分布下的模型表现差异很大，建议在模型开发和评估中使用矩阵特征基于的分割方法。<details>
<summary>Abstract</summary>
With the ever-growing presence of social media platforms comes the increased spread of harmful content and the need for robust hate speech detection systems. Such systems easily overfit to specific targets and keywords, and evaluating them without considering distribution shifts that might occur between train and test data overestimates their benefit. We challenge hate speech models via new train-test splits of existing datasets that rely on the clustering of models' hidden representations. We present two split variants (Subset-Sum-Split and Closest-Split) that, when applied to two datasets using four pretrained models, reveal how models catastrophically fail on blind spots in the latent space. This result generalises when developing a split with one model and evaluating it on another. Our analysis suggests that there is no clear surface-level property of the data split that correlates with the decreased performance, which underscores that task difficulty is not always humanly interpretable. We recommend incorporating latent feature-based splits in model development and release two splits via the GenBench benchmark.
</details>
<details>
<summary>摘要</summary>
随着社交媒体平台的不断普及，恶意内容的散布也在不断增加，需要建立有力的恶意言语检测系统。但是现有的检测系统容易过拟合特定目标和关键词，而不充分考虑数据分布的变化可能会发生在训练和测试数据之间。我们通过新的训练测试分割方法来挑战恶意言语模型。我们提出了两种分割方法（子集和最近的分割），当应用于两个数据集上四个预训练模型时，发现模型在隐藏表示空间中的缺陷。这个结果普适地发生在开发新的分割和使用另一个模型进行评估。我们的分析表明，不存在明确的表层特征，可以用来判断任务难度。我们建议在模型开发和发布中包含隐藏特征基于的分割。我们释放了两个分割，并将其作为GenBench测试套件。
</details></li>
</ul>
<hr>
<h2 id="The-Impact-of-Familiarity-on-Naming-Variation-A-Study-on-Object-Naming-in-Mandarin-Chinese"><a href="#The-Impact-of-Familiarity-on-Naming-Variation-A-Study-on-Object-Naming-in-Mandarin-Chinese" class="headerlink" title="The Impact of Familiarity on Naming Variation: A Study on Object Naming in Mandarin Chinese"></a>The Impact of Familiarity on Naming Variation: A Study on Object Naming in Mandarin Chinese</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10181">http://arxiv.org/abs/2311.10181</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yunke He, Xixian Liao, Jialing Liang, Gemma Boleda</li>
<li>for: 这研究探讨了不同说话人之间对同一个对象或实体的命名方式的差异。</li>
<li>methods: 研究人员使用了语言和视觉数据集，对1319个自然的图像进行了20个不同的命名。他们 investigate了对象之familiarity对命名差异的影响。</li>
<li>results: 研究发现， Familiarity会影响命名差异，有两种方式：一是通过扩展词汇，使命名更加多样化；二是通过推广标准名称，使命名更加统一。研究 illustrate了如何使用计算机资源来解决认知科学问题。<details>
<summary>Abstract</summary>
Different speakers often produce different names for the same object or entity (e.g., "woman" vs. "tourist" for a female tourist). The reasons behind variation in naming are not well understood. We create a Language and Vision dataset for Mandarin Chinese that provides an average of 20 names for 1319 naturalistic images, and investigate how familiarity with a given kind of object relates to the degree of naming variation it triggers across subjects. We propose that familiarity influences naming variation in two competing ways: increasing familiarity can either expand vocabulary, leading to higher variation, or promote convergence on conventional names, thereby reducing variation. We find evidence for both factors being at play. Our study illustrates how computational resources can be used to address research questions in Cognitive Science.
</details>
<details>
<summary>摘要</summary>
不同的说话人经常生成不同的名称 для同一个物体或实体（例如，"女性旅客" vs. "旅客"  для女性旅客）。名称的变化原因还不很清楚。我们创建了一个语言和视觉数据集 для普通话，提供了每个图像的平均20个名称，并研究了对象的 familiairity 如何影响名称的变化。我们提出了两种可能的影响因素：增加familiarity可能会扩展词汇，导致更高的变化，或者推动对常见名称的共识，从而减少变化。我们发现了这两种因素都在运作。我们的研究示例了如何使用计算机资源解决认知科学研究问题。
</details></li>
</ul>
<hr>
<h2 id="JWSign-A-Highly-Multilingual-Corpus-of-Bible-Translations-for-more-Diversity-in-Sign-Language-Processing"><a href="#JWSign-A-Highly-Multilingual-Corpus-of-Bible-Translations-for-more-Diversity-in-Sign-Language-Processing" class="headerlink" title="JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing"></a>JWSign: A Highly Multilingual Corpus of Bible Translations for more Diversity in Sign Language Processing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10174">http://arxiv.org/abs/2311.10174</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/shesterg/jwsign-machine-translation">https://github.com/shesterg/jwsign-machine-translation</a></li>
<li>paper_authors: Shester Gueuwou, Sophie Siake, Colin Leong, Mathias Müller</li>
<li>for: 本研究的目的是提供一个大型、多语言的手语译写数据集，以促进手语译写、翻译和生成任务的进步。</li>
<li>methods: 本研究使用的方法包括对JWSign数据集进行神经机器翻译实验，以及为不同的语言组合实现多语言系统。</li>
<li>results: 实验结果显示，使用多语言系统可以超越双语基eline系统，且在较高资源enario中，Language pairs的类型相似性 clustering 可以提高翻译质量。<details>
<summary>Abstract</summary>
Advancements in sign language processing have been hindered by a lack of sufficient data, impeding progress in recognition, translation, and production tasks. The absence of comprehensive sign language datasets across the world's sign languages has widened the gap in this field, resulting in a few sign languages being studied more than others, making this research area extremely skewed mostly towards sign languages from high-income countries. In this work we introduce a new large and highly multilingual dataset for sign language translation: JWSign. The dataset consists of 2,530 hours of Bible translations in 98 sign languages, featuring more than 1,500 individual signers. On this dataset, we report neural machine translation experiments. Apart from bilingual baseline systems, we also train multilingual systems, including some that take into account the typological relatedness of signed or spoken languages. Our experiments highlight that multilingual systems are superior to bilingual baselines, and that in higher-resource scenarios, clustering language pairs that are related improves translation quality.
</details>
<details>
<summary>摘要</summary>
技术进步受到了数据不足的阻碍，妨碍了认知、翻译和生产任务的进步。全球各种手语的缺乏完整的数据集，使得这一领域的研究受到了极大的偏袋，大多数研究集中在高收入国家的手语上进行，这使得这个领域的研究非常偏向高收入国家的手语。在这项工作中，我们介绍了一个新的大型、多语言的手语翻译数据集：JWSign。该数据集包括98种手语的2,530小时的圣经翻译，共有1,500名个体手语演示者。在这个数据集上，我们报告了神经机器翻译实验。除了双语基eline系统，我们还训练了多语言系统，其中一些考虑了手语或口语语言之间的类型学关系。我们的实验表明，多语言系统比双语基eline系统更高效，而在更高资源的场景下，将相关的语言对 grouping 可以提高翻译质量。
</details></li>
</ul>
<hr>
<h2 id="A-Computationally-Efficient-Sparsified-Online-Newton-Method"><a href="#A-Computationally-Efficient-Sparsified-Online-Newton-Method" class="headerlink" title="A Computationally Efficient Sparsified Online Newton Method"></a>A Computationally Efficient Sparsified Online Newton Method</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10085">http://arxiv.org/abs/2311.10085</a></li>
<li>repo_url: None</li>
<li>paper_authors: Fnu Devvrit, Sai Surya Duvvuri, Rohan Anil, Vineet Gupta, Cho-Jui Hsieh, Inderjit Dhillon</li>
<li>for: 这篇论文的目的是提出一种可扩展的第二类方法，以提高深度神经网络训练的快速度和效率。</li>
<li>methods: 这篇论文使用了一种称为SONew的新方法，它是一种具有优化的条件系统，可以实现高效的深度神经网络训练。这个方法基于LogDet矩阵差分量的新用途，并且运用了简洁条件以减少遗传 regret。</li>
<li>results: 这篇论文的实验结果显示，SONew方法可以实现30%更快的快速度，3.4%的效能提升，并且80%的训练损失减少，相比于内存高效的优化器，包括第一类方法。此外，SONew方法可以实现高度的并行和高效率的实现，并且可以轻松地扩展到大规模的实验。<details>
<summary>Abstract</summary>
Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to 30% faster convergence, 3.4% relative improvement in validation performance, and 80% relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact -- imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about 3% slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devvrit/SONew
</details>
<details>
<summary>摘要</summary>
第二顺序方法具有增强深度神经网络训练的潜在潜力，但它们的巨大内存和计算需求限制了它们的实用性。因此，有一个需求是可扩展的第二顺序方法，可以高效地训练大型模型。在这篇文章中，我们介绍了简化在线新点方法（SONew），它是一个具有优化组件的内存有效的第二顺序方法。我们使用了一个新的LogDet矩阵差异测度，并与简化条件相结合，以减少在线凸优化框架中的遗憾。我们对大规模benchmark进行实验，获得了30%的更快的渐进、3.4%的相对提高验证性能，以及80%的相对提高训练损失。相比之下，内存高效的优化器，包括首顺序方法，SONew具有更高的实用性。实际上，我们发现，对大型benchmark的应用，Shampoo方法无法扩展，并且需要较多的工程实践来扩展。相比之下，SONew方法具有更直接的实现方式，增加了它的实用性。SONew代码可以在以下链接获取：https://github.com/devvrit/SONew。
</details></li>
</ul>
<hr>
<h2 id="Characterizing-Tradeoffs-in-Language-Model-Decoding-with-Informational-Interpretations"><a href="#Characterizing-Tradeoffs-in-Language-Model-Decoding-with-Informational-Interpretations" class="headerlink" title="Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations"></a>Characterizing Tradeoffs in Language Model Decoding with Informational Interpretations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10083">http://arxiv.org/abs/2311.10083</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chung-Ching Chang, William W. Cohen, Yun-Hsuan Sung</li>
<li>for: 这 paper 是为了提出一种语言模型预测器的理论框架，用于解决预测器的设计问题。</li>
<li>methods: 这 paper 使用动态 программирова法和信息理论来描述语言模型预测器的算法。它将预测器的设计从 logit 空间提升到 action-state value function 空间，并将每个组件在这个空间中的解释。</li>
<li>results: 这 paper 显示了如何通过优化 action-state value functions，以获得更好的预测性和可解释性。这些结果可以帮助解决预测器的质量和性能问题。<details>
<summary>Abstract</summary>
We propose a theoretical framework for formulating language model decoder algorithms with dynamic programming and information theory. With dynamic programming, we lift the design of decoder algorithms from the logit space to the action-state value function space, and show that the decoding algorithms are consequences of optimizing the action-state value functions. Each component in the action-state value function space has an information theoretical interpretation. With the lifting and interpretation, it becomes evident what the decoder algorithm is optimized for, and hence facilitating the arbitration of the tradeoffs in sensibleness, diversity, and attribution.
</details>
<details>
<summary>摘要</summary>
我们提出了一个理论框架，用于形式化语言模型decoder算法的动态计划和信息理论。通过动态计划，我们将decoder算法的设计从ilogit空间提升到动作-状态价值函数空间，并显示出decoding算法是优化动作-状态价值函数的结果。每个动作-状态价值函数空间中的组件都有信息理论的解释。通过提升和解释，就可以看出decoder算法是优化什么，因此可以促进折衔敏感、多样性和责任的衡量。
</details></li>
</ul>
<hr>
<h2 id="DRESS-Instructing-Large-Vision-Language-Models-to-Align-and-Interact-with-Humans-via-Natural-Language-Feedback"><a href="#DRESS-Instructing-Large-Vision-Language-Models-to-Align-and-Interact-with-Humans-via-Natural-Language-Feedback" class="headerlink" title="DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"></a>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.10081">http://arxiv.org/abs/2311.10081</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran</li>
<li>for: The paper aims to improve the performance of large vision language models (LVLMs) by incorporating natural language feedback (NLF) to enhance their alignment and interactions.</li>
<li>methods: The paper proposes a novel categorization of NLF into two types: critique and refinement, and uses conditional reinforcement learning to train the LVLMs to incorporate feedback in multi-turn interactions.</li>
<li>results: The paper shows that the proposed method, called DRESS, can generate more helpful, honest, and harmless responses, and more effectively learn from feedback during multi-turn interactions compared to state-of-the-art LVLMs.<details>
<summary>Abstract</summary>
We present DRESS, a large vision language model (LVLM) that innovatively exploits Natural Language feedback (NLF) from Large Language Models to enhance its alignment and interactions by addressing two key limitations in the state-of-the-art LVLMs. First, prior LVLMs generally rely only on the instruction finetuning stage to enhance alignment with human preferences. Without incorporating extra feedback, they are still prone to generate unhelpful, hallucinated, or harmful responses. Second, while the visual instruction tuning data is generally structured in a multi-turn dialogue format, the connections and dependencies among consecutive conversational turns are weak. This reduces the capacity for effective multi-turn interactions. To tackle these, we propose a novel categorization of the NLF into two key types: critique and refinement. The critique NLF identifies the strengths and weaknesses of the responses and is used to align the LVLMs with human preferences. The refinement NLF offers concrete suggestions for improvement and is adopted to improve the interaction ability of the LVLMs-- which focuses on LVLMs' ability to refine responses by incorporating feedback in multi-turn interactions. To address the non-differentiable nature of NLF, we generalize conditional reinforcement learning for training. Our experimental results demonstrate that DRESS can generate more helpful (9.76%), honest (11.52%), and harmless (21.03%) responses, and more effectively learn from feedback during multi-turn interactions compared to SOTA LVMLs.
</details>
<details>
<summary>摘要</summary>
我们介绍DRESS，一个大型视觉语言模型（LVLM），它创新地利用自然语言反馈（NLF）来提高对人类偏好的调整和互动。现有LVLMs通常只通过 instrucion 精化阶段来提高对人类偏好的调整，而无法采用更多的反馈来减少生成无用、幻想或危险的回答。其次，视觉指令练习数据通常是多turn对话格式，但连续对话中的连接和依赖关系较弱，这限制了LVLMs的有效多turn互动能力。为此，我们提出了一种新的NLF分类方法：批评和细化。批评NLF可以识别回答的优缺点，并用于对LVLMs进行调整，使其更加适应人类偏好。细化NLF可以提供具体的改进建议，并被用来提高LVLMs的互动能力，即LVLMs能够通过反馈进行多turn互动中的改进。由于NLF的非准确性，我们扩展了条件奖励学习的训练方法。我们的实验结果表明，DRESS可以生成更有用（9.76%）、诚实（11.52%）和无害（21.03%）的回答，并在多turn互动中更好地学习反馈。
</details></li>
</ul>
<hr>
<h2 id="Unambiguity-and-Fewness-for-Nonuniform-Families-of-Polynomial-Size-Nondeterministic-Finite-Automata"><a href="#Unambiguity-and-Fewness-for-Nonuniform-Families-of-Polynomial-Size-Nondeterministic-Finite-Automata" class="headerlink" title="Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata"></a>Unambiguity and Fewness for Nonuniform Families of Polynomial-Size Nondeterministic Finite Automata</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09979">http://arxiv.org/abs/2311.09979</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tomoyuki Yamakami</li>
<li>for: 这个论文主要针对的是非征Compatibility promise decision问题的解决方案。</li>
<li>methods: 论文使用了非征Compatibility families of polynomial-size finite automata，这些自动机有 polynomially many inner states，来解决这些问题。</li>
<li>results: 论文表明，在一些特定情况下，这些非征Compatibility families of finite automata 有不同的计算能力，而且两种方法（一个方法是限制机器只能做一个方向的移动，另一个方法是限制机器的长度为 polynomially-bounded）是等价的。<details>
<summary>Abstract</summary>
Nonuniform families of polynomial-size finite automata, which are series of indexed finite automata having polynomially many inner states, are used in the past literature to solve nonuniform families of promise decision problems. Among such nonuniform families of finite automata, we focus our attention, in particular, on the variants of nondeterministic finite automata, which have at most "one" (unambiguous), "polynomially many" (few) accepting computation paths, or unambiguous/few computation paths leading to each fixed configuration. When such machines are limited to make only one-way head moves, we can prove with no unproven hardness assumptions that some of these variants are different in computational power from each other. As for two-way machines restricted to instances of polynomially-bounded length, families of two-way polynomial-size nondeterministic finite automata are equivalent in power to families of polynomial-size unambiguous finite automata.
</details>
<details>
<summary>摘要</summary>
非均匀家族的多项式大小自动机，这是在过去文献中用于解决非均匀家族的Promise决策问题的工具。我们在这些非均匀自动机家族中特别关注尝试机器，它们在最多只能有一个（不ambiguous）， polynomially many（少）的接受计算路径，或者每个固定配置都有一个或 polynomially many 的计算路径。当这些机器只能做一个一向头移时，我们可以证明不带任何难度假设的情况下，这些变体之间存在不同的计算能力。而两个方向的机器，限制到 polynomially-bounded 长度的实例时， families of two-way polynomial-size nondeterministic finite automata 和 families of polynomial-size unambiguous finite automata 之间存在相同的计算能力。
</details></li>
</ul>
<hr>
<h2 id="Hijacking-Large-Language-Models-via-Adversarial-In-Context-Learning"><a href="#Hijacking-Large-Language-Models-via-Adversarial-In-Context-Learning" class="headerlink" title="Hijacking Large Language Models via Adversarial In-Context Learning"></a>Hijacking Large Language Models via Adversarial In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09948">http://arxiv.org/abs/2311.09948</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Qiang, Xiangyu Zhou, Dongxiao Zhu</li>
<li>for: 这个论文是为了研究受限下的语言模型（LLM）在特定任务上的性能，并使用示例作为示范来启用LLM。</li>
<li>methods: 这篇论文使用了梯度基于的搜索方法来学习并贴加不可见的反对示例，以控制LLM生成的响应。</li>
<li>results: 实验结果表明，这种攻击可以让LLM生成targeted的不良输出，通过吸引LLM的注意力向 adversarial tokens。<details>
<summary>Abstract</summary>
In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.
</details>
<details>
<summary>摘要</summary>
内容学习（ICL）已经 emerged as a powerful paradigm，利用 LLMS  для特定任务，通过使用标签的示例作为条件答案。 despite its promising performance， ICL 受到示例选择和排序的不稳定性问题。 In addition, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMS to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.
</details></li>
</ul>
<hr>
<h2 id="An-Attention-Based-Denoising-Framework-for-Personality-Detection-in-Social-Media-Texts"><a href="#An-Attention-Based-Denoising-Framework-for-Personality-Detection-in-Social-Media-Texts" class="headerlink" title="An Attention-Based Denoising Framework for Personality Detection in Social Media Texts"></a>An Attention-Based Denoising Framework for Personality Detection in Social Media Texts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09945">http://arxiv.org/abs/2311.09945</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/once2gain/personalitydetection">https://github.com/once2gain/personalitydetection</a></li>
<li>paper_authors: Qirui Tang, Wenkang Jiang, Yihua Du, Lei Lin</li>
<li>for: 这篇论文旨在提高社交媒体文本中人格检测的准确率，并提供一种基于注意力的信息提取机制和人格检测框架。</li>
<li>methods: 论文提出了一种基于注意力的信息提取机制（AIEM），以及一种基于注意力的干扰除框架（ADF），以提高人格检测的准确率。</li>
<li>results: 论文在两个常用的数据集上达到了当前领域的最佳性能水平，具体来说是在Twitter-Myers-Briggs Type Indicator（Twitter-MBTI）数据集上的平均准确率提高10.2%。<details>
<summary>Abstract</summary>
In social media networks, users produce a large amount of text content anytime, providing researchers with a valuable approach to digging for personality-related information. Personality detection based on user-generated texts is a universal method that can be used to build user portraits. The presence of noise in social media texts hinders personality detection. However, previous studies have not fully addressed this challenge. Inspired by the scanning reading technique, we propose an attention-based information extraction mechanism (AIEM) for long texts, which is applied to quickly locate valuable pieces of information, and focus more attention on the deep semantics of key pieces. Then, we provide a novel attention-based denoising framework (ADF) for personality detection tasks and achieve state-of-the-art performance on two commonly used datasets. Notably, we obtain an average accuracy improvement of 10.2% on the gold standard Twitter-Myers-Briggs Type Indicator (Twitter-MBTI) dataset. We made our code publicly available on GitHub. We shed light on how AIEM works to magnify personality-related signals.
</details>
<details>
<summary>摘要</summary>
在社交媒体网络中，用户生成大量文本内容，为研究人员提供了一个价值的检测人格信息的方法。基于用户生成的文本进行人格检测是一种通用的方法，可以 construir 用户肖像。但是，社交媒体文本中的噪声会妨碍人格检测。然而，先前的研究并没有彻底解决这个挑战。我们提出了基于扫描阅读技术的注意力基本信息提取机制（AIEM），用于快速找到有价值信息，并更多地关注深层 semantics 的关键 Piece。然后，我们提出了一种基于注意力的噪声降减框架（ADF），用于人格检测任务，并在两个常用的数据集上实现了状态的表现。特别是，我们在 Twitter-Myers-Briggs Type Indicator（Twitter-MBTI）数据集上实现了10.2%的平均准确率提升。我们在 GitHub 上公开了我们的代码。我们探讨了 AIEM 如何强调人格相关的信号。
</details></li>
</ul>
<hr>
<h2 id="Language-Generation-from-Human-Brain-Activities"><a href="#Language-Generation-from-Human-Brain-Activities" class="headerlink" title="Language Generation from Human Brain Activities"></a>Language Generation from Human Brain Activities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09889">http://arxiv.org/abs/2311.09889</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Ye, Qingyao Ai, Yiqun Liu, Min Zhang, Christina Lioma, Tuukka Ruotsalo</li>
<li>for: 这个研究旨在开发一个基于脑computer interfaces（BCIs）的语言生成系统，以便无需侵入性地传入语言。</li>
<li>methods: 这个研究使用了大型语言模型（LLM）和semantic brain decoder来直接从functional magnetic resonance imaging（fMRI）输入中生成语言。</li>
<li>results: 研究发现，这个模型可以对于视觉或听觉语言刺激而生成协调的语言序列，并且与脑Input的内容有着Semantic相似性。相比之下，随机控制和预先生成的语言选择方法，以及标准的LLM，它们只能生成基于语言训练数据的通用单词次序。<details>
<summary>Abstract</summary>
Generating human language through non-invasive brain-computer interfaces (BCIs) has the potential to unlock many applications, such as serving disabled patients and improving communication. Currently, however, generating language via BCIs has been previously successful only within a classification setup for selecting pre-generated sentence continuation candidates with the most likely cortical semantic representation. Inspired by recent research that revealed associations between the brain and the large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates. We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data. The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled. Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation.
</details>
<details>
<summary>摘要</summary>
使用非侵入式脑计算机接口（BCI）生成人类语言有很多应用前途，如服务残疾患者和改善沟通。然而，目前通过BCI生成语言仅限于在分类设置中选择预生成的句子续写候选者中的最有可能性的 cortical semantic representation。 inspirited by recent research that revealed associations between the brain and large computational language models, we propose a generative language BCI that utilizes the capacity of a large language model (LLM) jointly with a semantic brain decoder to directly generate language from functional magnetic resonance imaging (fMRI) input. The proposed model can generate coherent language sequences aligned with the semantic content of visual or auditory language stimuli perceived, without prior knowledge of any pre-generated candidates. We compare the language generated from the presented model with a random control, pre-generated language selection approach, and a standard LLM, which generates common coherent text solely based on the next word likelihood according to statistical language training data. The proposed model is found to generate language that is more aligned with semantic stimulus in response to which brain input is sampled. Our findings demonstrate the potential and feasibility of employing BCIs in direct language generation.Note that the translation is in Simplified Chinese, which is the standard writing system used in mainland China. If you prefer Traditional Chinese, I can provide that as well.
</details></li>
</ul>
<hr>
<h2 id="Which-Modality-should-I-use-–-Text-Motif-or-Image-Understanding-Graphs-with-Large-Language-Models"><a href="#Which-Modality-should-I-use-–-Text-Motif-or-Image-Understanding-Graphs-with-Large-Language-Models" class="headerlink" title="Which Modality should I use – Text, Motif, or Image? : Understanding Graphs with Large Language Models"></a>Which Modality should I use – Text, Motif, or Image? : Understanding Graphs with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09862">http://arxiv.org/abs/2311.09862</a></li>
<li>repo_url: None</li>
<li>paper_authors: Debarati Das, Ishaan Gupta, Jaideep Srivastava, Dongyeop Kang</li>
<li>for: 本研究旨在探讨如何更好地融合图数据与大语言模型（LLMs），以提高LLMs在复杂图结构分析中的效iveness。</li>
<li>methods: 本研究使用了不同的编码方式（如文本、图像和模式）和不同的推荐方法来优化LLMs在处理复杂图结构时的表现。</li>
<li>results: 研究发现，图像模式，尤其是通过高级见语言模型like GPT-4V支持，比文本更有效地管理токен限制而保留重要信息。研究还探讨了不同因素对每种编码模式表现的影响。<details>
<summary>Abstract</summary>
Large language models (LLMs) are revolutionizing various fields by leveraging large text corpora for context-aware intelligence. Due to the context size, however, encoding an entire graph with LLMs is fundamentally limited. This paper explores how to better integrate graph data with LLMs and presents a novel approach using various encoding modalities (e.g., text, image, and motif) and approximation of global connectivity of a graph using different prompting methods to enhance LLMs' effectiveness in handling complex graph structures. The study also introduces GraphTMI, a new benchmark for evaluating LLMs in graph structure analysis, focusing on factors such as homophily, motif presence, and graph difficulty. Key findings reveal that image modality, supported by advanced vision-language models like GPT-4V, is more effective than text in managing token limits while retaining critical information. The research also examines the influence of different factors on each encoding modality's performance. This study highlights the current limitations and charts future directions for LLMs in graph understanding and reasoning tasks.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxíng yǔyán módelǐ)* "revolutionizing" is translated as "革命化" (gémònghuà)* "various fields" is translated as "多个领域" (duō gè lǐngyù)* "leveraging large text corpora" is translated as "利用大量文本资料" (lìyòng dàliàng wén tiě xīn yǎng)* "context-aware intelligence" is translated as "Context-aware intelligence" (上下文意识)* "encoding an entire graph" is translated as "完整的图形编码" (quánzhì de túxíng biān mǎ)* "fundamentally limited" is translated as "基本上有限" (jībǎo shang yǒu xiàn)* "novel approach" is translated as "新的方法" (xīn de fāngfǎ)* "using various encoding modalities" is translated as "使用多种编码方式" (shǐyòu duōshì biān mǎ fāngshì)* "approximation of global connectivity" is translated as "全球连接的估计" (quánqiú liánjiē de gèjì)* "different prompting methods" is translated as "不同的提示方法" (bùdōng de tímí fāngfǎ)* "enhance LLMs' effectiveness" is translated as "增强LLMs的效果" (zēngcháng LLMs de xiànggòu)* "in handling complex graph structures" is translated as "处理复杂的图形结构" (chùzhì fùzì de túxíng jiégòu)* "Key findings reveal" is translated as "主要发现是" (zhǔyào fāxìn shì)* "image modality" is translated as "图像模式" (túxíang móshì)* "supported by advanced vision-language models" is translated as "由高级视语言模型支持" (yǐ gāojí wèi yǔ yǔ móshì)* "more effective than text" is translated as "比文本更有效" (bǐ wén tiěn jí yòu yì)* "managing token limits" is translated as "管理 токен限制" (guǎn lǐ tóu kē yùn xiàn)* "while retaining critical information" is translated as "保留关键信息" (bǎo liú guān jí xìn xīn)* "The research also examines the influence of different factors" is translated as "研究也研究了不同因素的影响" (yánjiū yě yánjiū le bùdàng yīn xiǎng de yìngxìn)* "on each encoding modality's performance" is translated as "每种编码方式的性能" (měi zhǒng biān mǎ fāngshì de xìngnéng)* "This study highlights the current limitations" is translated as "这项研究透视当前的限制" (zhè yè yánjiū tòu shì dāng zhì de jiàn zhì)* "and charts future directions" is translated as "并规划未来的发展" (dàn zhì yú yì yè yì)
</details></li>
</ul>
<hr>
<h2 id="GSAP-NER-A-Novel-Task-Corpus-and-Baseline-for-Scholarly-Entity-Extraction-Focused-on-Machine-Learning-Models-and-Datasets"><a href="#GSAP-NER-A-Novel-Task-Corpus-and-Baseline-for-Scholarly-Entity-Extraction-Focused-on-Machine-Learning-Models-and-Datasets" class="headerlink" title="GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets"></a>GSAP-NER: A Novel Task, Corpus, and Baseline for Scholarly Entity Extraction Focused on Machine Learning Models and Datasets</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09860">http://arxiv.org/abs/2311.09860</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wolfgang Otto, Matthäus Zloch, Lu Gan, Saurav Karmakar, Stefan Dietze</li>
<li>for: 本研究旨在提供精细化的机器学习模型和数据集的实体识别模型，以便更好地理解它们在学术论文中的提及。</li>
<li>methods: 本研究使用了一种基于BERT的首个基线模型，以及一个手动注解的全文科学论文集。</li>
<li>results: 本研究发现了10种不同的实体类型，包括机器学习模型和数据集，并提供了一个手动注解的全文科学论文集，以便进一步研究和应用。<details>
<summary>Abstract</summary>
Named Entity Recognition (NER) models play a crucial role in various NLP tasks, including information extraction (IE) and text understanding. In academic writing, references to machine learning models and datasets are fundamental components of various computer science publications and necessitate accurate models for identification. Despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like ML model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around ML models and datasets. In order to provide a nuanced understanding of how ML models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like "our BERT-based model" or "an image CNN". You can find the ground truth dataset and code to replicate model training at https://data.gesis.org/gsap/gsap-ner.
</details>
<details>
<summary>摘要</summary>
Named Entity Recognition (NER) 模型在各种自然语言处理（NLP）任务中扮演着关键的角色，包括信息抽取（IE）和文本理解。在学术写作中，关于机器学习模型和数据集的引用是学术出版物的重要组成部分，需要准确的模型来识别。despite the advancements in NER, existing ground truth datasets do not treat fine-grained types like machine learning model and model architecture as separate entity types, and consequently, baseline models cannot recognize them as such. In this paper, we release a corpus of 100 manually annotated full-text scientific publications and a first baseline model for 10 entity types centered around machine learning models and datasets. In order to provide a nuanced understanding of how machine learning models and datasets are mentioned and utilized, our dataset also contains annotations for informal mentions like "our BERT-based model" or "an image CNN". You can find the ground truth dataset and code to replicate model training at <https://data.gesis.org/gsap/gsap-ner>.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore.
</details></li>
</ul>
<hr>
<h2 id="Overview-of-the-HASOC-Subtrack-at-FIRE-2023-Identification-of-Tokens-Contributing-to-Explicit-Hate-in-English-by-Span-Detection"><a href="#Overview-of-the-HASOC-Subtrack-at-FIRE-2023-Identification-of-Tokens-Contributing-to-Explicit-Hate-in-English-by-Span-Detection" class="headerlink" title="Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English by Span Detection"></a>Overview of the HASOC Subtrack at FIRE 2023: Identification of Tokens Contributing to Explicit Hate in English by Span Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09834">http://arxiv.org/abs/2311.09834</a></li>
<li>repo_url: None</li>
<li>paper_authors: Sarah Masud, Mohammad Aflah Khan, Md. Shad Akhtar, Tanmoy Chakraborty</li>
<li>for: 本研究旨在开发计算方法来减少网络上的仇恨言论。</li>
<li>methods: 本研究使用黑盒模型来识别仇恨内容，并提供了一种可能的重写建议。</li>
<li>results: 研究发现，使用这种方法可以减少Explicit span detection in English Tweets，最高macro-F1达0.58。<details>
<summary>Abstract</summary>
As hate speech continues to proliferate on the web, it is becoming increasingly important to develop computational methods to mitigate it. Reactively, using black-box models to identify hateful content can perplex users as to why their posts were automatically flagged as hateful. On the other hand, proactive mitigation can be achieved by suggesting rephrasing before a post is made public. However, both mitigation techniques require information about which part of a post contains the hateful aspect, i.e., what spans within a text are responsible for conveying hate. Better detection of such spans can significantly reduce explicitly hateful content on the web. To further contribute to this research area, we organized HateNorm at HASOC-FIRE 2023, focusing on explicit span detection in English Tweets. A total of 12 teams participated in the competition, with the highest macro-F1 observed at 0.58.
</details>
<details>
<summary>摘要</summary>
随着仇恨言论在网络上的迅速增加，计算方法的开发已成为一项非常重要的任务。 Reactively，使用黑盒模型标识仇恨内容可能会让用户感到困惑，因为他们不知道哪些内容被自动标识为仇恨。然而，投入型mitigation可以通过建议重写内容之前提交，以避免内容被公布。然而，这两种mitigation技术都需要知道哪些文本内容包含仇恨元素，即哪些文本段落在传递仇恨信息方面表现出色。更好地检测这些文本段落可以有效减少网络上直接的仇恨内容。为了进一步贡献到这个研究领域，我们在HASOC-FIRE 2023年组织了HateNorm比赛，专注于英语推文中的直接检测。总共12个团队参与了比赛，最高的macro-F1为0.58。
</details></li>
</ul>
<hr>
<h2 id="X-Mark-Towards-Lossless-Watermarking-Through-Lexical-Redundancy"><a href="#X-Mark-Towards-Lossless-Watermarking-Through-Lexical-Redundancy" class="headerlink" title="X-Mark: Towards Lossless Watermarking Through Lexical Redundancy"></a>X-Mark: Towards Lossless Watermarking Through Lexical Redundancy</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09832">http://arxiv.org/abs/2311.09832</a></li>
<li>repo_url: None</li>
<li>paper_authors: Liang Chen, Yatao Bian, Yang Deng, Shuaiyi Li, Bingzhe Wu, Peilin Zhao, Kam-fai Wong</li>
<li>For: This paper focuses on the issue of text watermarking, which is important for detecting machine-generated text.* Methods: The authors introduce a novel approach called XMark, which leverages text redundancy within the lexical space to improve text generation fluency while maintaining watermark detectability.* Results: The authors present theoretical analyses and empirical evidence showing that XMark outperforms existing methods in retaining the emergent abilities of large language models, including zero-shot and few-shot knowledge recall, logical reasoning, and instruction following.Here’s the same information in Simplified Chinese text:* For: 这篇论文关注了文本沟通技术，它在机器生成文本检测方面具有重要意义。* Methods: 作者们提出了一种新的方法，即XMark，它利用文本空间内的同义词来提高文本生成流畅性，同时保持水印检测的能力。* Results: 作者们提供了理论分析和实验证据，表明XMark比现有方法更能保持大语言模型的emergent能力，包括零批知识回忆、几批知识回忆、逻辑推理和指令遵循。<details>
<summary>Abstract</summary>
Text watermarking has emerged as an important technique for detecting machine-generated text. However, existing methods can severely degrade text quality due to arbitrary vocabulary partitioning, which disrupts the language model's expressiveness and impedes textual coherence. To mitigate this, we introduce XMark, a novel approach that capitalizes on text redundancy within the lexical space. Specifically, XMark incorporates a mutually exclusive rule for synonyms during the language model decoding process, thereby integrating prior knowledge into vocabulary partitioning and preserving the capabilities of language generation. We present theoretical analyses and empirical evidence demonstrating that XMark substantially enhances text generation fluency while maintaining watermark detectability. Furthermore, we investigate watermarking's impact on the emergent abilities of large language models, including zero-shot and few-shot knowledge recall, logical reasoning, and instruction following. Our comprehensive experiments confirm that XMark consistently outperforms existing methods in retaining these crucial capabilities of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="FollowEval-A-Multi-Dimensional-Benchmark-for-Assessing-the-Instruction-Following-Capability-of-Large-Language-Models"><a href="#FollowEval-A-Multi-Dimensional-Benchmark-for-Assessing-the-Instruction-Following-Capability-of-Large-Language-Models" class="headerlink" title="FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models"></a>FollowEval: A Multi-Dimensional Benchmark for Assessing the Instruction-Following Capability of Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09829">http://arxiv.org/abs/2311.09829</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yimin Jing, Renren Jin, Jiahao Hu, Huishi Qiu, Xiaohua Wang, Peng Wang, Deyi Xiong</li>
<li>for: 评估大语言模型（LLM）的指令遵循能力是非常重要的。一个不能遵循人类指令的模型可能无法提供可靠和有用的回答。</li>
<li>methods: 我们在这篇论文中引入了FollowEvalBenchmark，一个包含英文和中文测试例的多语言指令遵循测试 benchmark。所有测试例都是由人类专家手动制作的。</li>
<li>results: 我们通过使用FollowEvalBenchmark测试多个LLM模型，发现它们的性能远远落后于人类。这 highlights 大语言模型的指令遵循能力仍然具有很大的提升空间。<details>
<summary>Abstract</summary>
The effective assessment of the instruction-following ability of large language models (LLMs) is of paramount importance. A model that cannot adhere to human instructions might be not able to provide reliable and helpful responses. In pursuit of this goal, various benchmarks have been constructed to evaluate the instruction-following capacity of these models. However, these benchmarks are limited to a single language and are constructed using automated approaches, which restricts their applicability and the quality of the test examples they contain. To bridge this gap, we introduce the FollowEval benchmark in this paper. This benchmark is composed of instances in both English and Chinese, and all test examples are crafted by human experts. Furthermore, the FollowEval benchmark is designed to assess LLMs across five critical dimensions of instruction following: string manipulation, commonsense reasoning, logical reasoning, spatial reasoning, and response constraints. To enhance the complexity and present a sufficient challenge, each test example is designed to evaluate more than one dimension. We have evaluated various LLMs using the FollowEval benchmark and found that their performance significantly lags behind that of humans. This highlights the considerable room for improvement in the instruction-following ability of these models.
</details>
<details>
<summary>摘要</summary>
检测大语言模型（LLM）的 instrucion-following 能力的有效性非常重要。如果一个模型无法遵循人类的 instrucion，那么它可能无法提供可靠和有用的回答。为了实现这个目标，各种标准套件已经建立来评估这些模型的 instrucion-following 能力。然而，这些标准套件受限于单一语言，并且使用自动化的方法构建，这限制了它们的可应用性和测试例子的质量。为了bridging这个差距，我们在这篇论文中引入 FollowEval 套件。这个套件包含英文和中文两种语言的实例，并且所有的测试例子都是由人类专家手动制作的。此外，FollowEval 套件采用了五个关键的 instrucion-following 维度来评估 LLM：字符串处理、常识理解、逻辑理解、空间理解和回答约束。为了增加复杂性和提供足够的挑战，每个测试例子都会评估多个维度。我们使用 FollowEval 套件测试了多种 LLM，发现它们在人类的 instrucion-following 能力方面表现明显落后。这说明这些模型在 instrucion-following 能力方面还有很大的进步空间。
</details></li>
</ul>
<hr>
<h2 id="AfriMTE-and-AfriCOMET-Empowering-COMET-to-Embrace-Under-resourced-African-Languages"><a href="#AfriMTE-and-AfriCOMET-Empowering-COMET-to-Embrace-Under-resourced-African-Languages" class="headerlink" title="AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages"></a>AfriMTE and AfriCOMET: Empowering COMET to Embrace Under-resourced African Languages</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09828">http://arxiv.org/abs/2311.09828</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/Unbabel/COMET">https://github.com/Unbabel/COMET</a></li>
<li>paper_authors: Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Marek Masiak, Xuanli He, Sofia Bourhim, Andiswa Bukula, Muhidin Mohamed, Temitayo Olatoye, Hamam Mokayede, Christine Mwase, Wangui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Hassan Muhammad, Salomey Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane, Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Abdi Mohamed, Ayinde Hassan, Oluwabusayo Olufunke Awoyomi, Lama Alkhaled, Sana Al-Azzawi, Naome A. Etori, Millicent Ochieng, Clemencia Siro, Samuel Njoroge, Eric Muchiri, Wangari Kimotho, Lyse Naomi Wamba Momo, Daud Abolade, Simbiat Ajao, Tosin Adewumi, Iyanuoluwa Shode, Ricky Macharm, Ruqayya Nasir Iro, Saheed S. Abdullahi, Stephen E. Moore, Bernard Opoku, Zainab Akinjobi, Abeeb Afolabi, Nnaemeka Obiefuna, Onyekachi Raphael Ogbu, Sam Brian, Verrah Akinyi Otiende, Chinedu Emmanuel Mbonu, Sakayo Toadoum Sari, Pontus Stenetorp</li>
<li>for: 这个论文的目的是为了提高非洲语言机器翻译的评估方法，以便更好地评估这些语言的翻译质量。</li>
<li>methods: 这篇论文使用了人工评分的方法来创建高质量的评估数据，并开发了一种基于DA训练数据的COMET评估指标，以提高非洲语言机器翻译的评估精度。</li>
<li>results: 这篇论文的结果表明，使用这种新的评估方法可以提高非洲语言机器翻译的评估精度，并且与人类评分有高度相关性（Spearman-rank correlation +0.406）。<details>
<summary>Abstract</summary>
Despite the progress we have recorded in scaling multilingual machine translation (MT) models and evaluation data to several under-resourced African languages, it is difficult to measure accurately the progress we have made on these languages because evaluation is often performed on n-gram matching metrics like BLEU that often have worse correlation with human judgments. Embedding-based metrics such as COMET correlate better; however, lack of evaluation data with human ratings for under-resourced languages, complexity of annotation guidelines like Multidimensional Quality Metrics (MQM), and limited language coverage of multilingual encoders have hampered their applicability to African languages. In this paper, we address these challenges by creating high-quality human evaluation data with a simplified MQM guideline for error-span annotation and direct assessment (DA) scoring for 13 typologically diverse African languages. Furthermore, we develop AfriCOMET, a COMET evaluation metric for African languages by leveraging DA training data from high-resource languages and African-centric multilingual encoder (AfroXLM-Roberta) to create the state-of-the-art evaluation metric for African languages MT with respect to Spearman-rank correlation with human judgments (+0.406).
</details>
<details>
<summary>摘要</summary>
尽管我们在扩展多语言机器翻译（MT）模型和评估数据到数个非常贫语言方面做出了进步，但是很难准确度量我们在这些语言上做出的进步，因为评估通常基于n-gram匹配度量如BLEU，这些度量与人类评估的相关性较差。基于嵌入度量如COMET可以更好地与人类评估相关，但是对于非常贫语言来说，评估数据的缺乏、评估指南的复杂性（如多维质量度量（MQM）），以及多语言encoder的语言覆盖率带来了障碍。在这篇论文中，我们解决了这些挑战，通过创建高质量的人类评估数据，并采用简化MQM指南进行错误扩 span的注释和直接评估（DA）分数的计算，对13种 typologically 多样化的非洲语言进行评估。此外，我们还开发了AfriCOMET评估度量，通过利用高资源语言的DA训练数据和非洲中心的多语言encoder（AfroXLM-Roberta）创建了非洲语言MT中的 estado-of-the-art 评估度量，与人类评估相关性为+0.406（Spearman相关度）。
</details></li>
</ul>
<hr>
<h2 id="Cognitive-Overload-Jailbreaking-Large-Language-Models-with-Overloaded-Logical-Thinking"><a href="#Cognitive-Overload-Jailbreaking-Large-Language-Models-with-Overloaded-Logical-Thinking" class="headerlink" title="Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking"></a>Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09827">http://arxiv.org/abs/2311.09827</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen</li>
<li>for: 本研究旨在探讨LLMs中的认知结构和过程如何受到攻击，以及如何防范这些攻击。</li>
<li>methods: 本研究使用了新的类型的监狱攻击， specifically targeting LLMs的认知结构和过程。 experiments conducted on AdvBench and MasterKey demonstrate that various LLMs can be compromised through cognitive overload.</li>
<li>results: 研究发现，通过三种不同的认知负担方式，可以成功地监狱所有研究的LLMs，而现有的防御策略很难有效地防止这些恶意用途。<details>
<summary>Abstract</summary>
While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经显示出了增加的力量，但也导致了各种危害行为的出现。作为代表，跳狱攻击可以让 LLM 发生危害或不道德的反应，即使经过安全Alignment。在这篇论文中，我们 investigate 一种新的跳狱攻击，这种攻击targets LLM的认知结构和过程。 Specifically, we analyze the safety vulnerability of LLMs in the face of （1）多语言认知过载、（2）掩饰表达和（3）效果归因。与之前的跳狱攻击不同，我们提出的认知过载是一种黑盒攻击，没有需要对模型结构或模型参数的知识。在 AdvBench 和 MasterKey 上进行的实验表明，包括流行的开源模型 Llama 2 和商业模型 ChatGPT 等各种 LLMS 都可以通过认知过载遭受攻击。受到认知心理学的管理认知过载的启示，我们进一步调查了防御认知过载攻击的两种方面。实验表明，我们从三个角度来进行认知过载可以成功地跳狱所有研究过的 LLMS，而现有的防御策略很难有效地抑制这些危害用途。
</details></li>
</ul>
<hr>
<h2 id="Human-Still-Wins-over-LLM-An-Empirical-Study-of-Active-Learning-on-Domain-Specific-Annotation-Tasks"><a href="#Human-Still-Wins-over-LLM-An-Empirical-Study-of-Active-Learning-on-Domain-Specific-Annotation-Tasks" class="headerlink" title="Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks"></a>Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09825">http://arxiv.org/abs/2311.09825</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuxuan Lu, Bingsheng Yao, Shao Zhang, Yun Wang, Peng Zhang, Tun Lu, Toby Jia-Jun Li, Dakuo Wang</li>
<li>for: 本研究证明了小型模型在域专知任务中的优势，并探讨了LLMs是否能在域专知任务中超越小型模型。</li>
<li>methods: 本研究使用了活动学习（AL）方法，并对四个数据集进行了实验比较。</li>
<li>results: 研究发现，即使使用了一些百度的标注数据，小型模型仍可以超过GPT-3.5的性能，而且与GPT-4相比，它们的性能相对较高。这些结论表明，LLMs的预测可以作为域专知应用中的启动方法，而人类专家仍然是数据标注驱动的域专知任务中不可或缺的。<details>
<summary>Abstract</summary>
Large Language Models (LLMs) have demonstrated considerable advances, and several claims have been made about their exceeding human performance. However, in real-world tasks, domain knowledge is often required. Low-resource learning methods like Active Learning (AL) have been proposed to tackle the cost of domain expert annotation, raising this question: Can LLMs surpass compact models trained with expert annotations in domain-specific tasks? In this work, we conduct an empirical experiment on four datasets from three different domains comparing SOTA LLMs with small models trained on expert annotations with AL. We found that small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller. Based on these findings, we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Towards-Robust-Temporal-Reasoning-of-Large-Language-Models-via-a-Multi-Hop-QA-Dataset-and-Pseudo-Instruction-Tuning"><a href="#Towards-Robust-Temporal-Reasoning-of-Large-Language-Models-via-a-Multi-Hop-QA-Dataset-and-Pseudo-Instruction-Tuning" class="headerlink" title="Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning"></a>Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09821">http://arxiv.org/abs/2311.09821</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyu Tan, Hwee Tou Ng, Lidong Bing</li>
<li>for: 提高大语言模型（LLMs）的时间知识理解能力，尤其是多个答案和多个跳跃类时间理解。</li>
<li>methods: 提出了一个复杂时间问答（QA）数据集 Complex-TR，以及一种新的数据增强策略，以提高 LLMS 的复杂时间理解能力和可靠性。</li>
<li>results: 对多个时间问答数据集进行了实验，研究结果表明，我们的方法能够提高 LLMS 的时间问答指标，比基eline方法提高了显著的多。<details>
<summary>Abstract</summary>
Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering (QA) dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs' performance on temporal QA benchmarks by significant margins.
</details>
<details>
<summary>摘要</summary>
现实世界中的知识是不断更新的。然而，更新大型自然语言模型（LLM）的成本很高。因此，LLM需要理解时间知识的概念。但是，先前的时间问答工作没有强调多个答案和多个跳步类时间推理。在这篇论文中，我们提出了复杂时间问答（QA）数据集Complex-TR，它专注于多个答案和多个跳步时间推理。此外，我们还提出了一种新的数据增强策略，用于提高LLM的复杂时间推理能力和鲁棒性。我们在多个时间问答数据集上进行了实验，实验结果显示，我们的方法可以在时间问答标准准则上提高LLM的表现。
</details></li>
</ul>
<hr>
<h2 id="SUQL-Conversational-Search-over-Structured-and-Unstructured-Data-with-Large-Language-Models"><a href="#SUQL-Conversational-Search-over-Structured-and-Unstructured-Data-with-Large-Language-Models" class="headerlink" title="SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models"></a>SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09818">http://arxiv.org/abs/2311.09818</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/stanford-oval/suql">https://github.com/stanford-oval/suql</a></li>
<li>paper_authors: Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui Dávid, Monica S. Lam</li>
<li>for: 这篇论文旨在构建基于多种数据源的对话式查询系统，以便更好地处理结构化和无结构化数据的混合查询。</li>
<li>methods: 这篇论文提出了SUQL（结构化和无结构化查询语言），它是一种可执行的正式表示语言，可以自然地涵盖结构化和无结构化数据查询的复杂作业。</li>
<li>results: 在实验中，使用SUQL和大语言模型实现的对话式搜索代理可以在51.3%的问题中找到满足用户需求的实体，比常用的基eline提高89.3%。<details>
<summary>Abstract</summary>
Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.   This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.
</details>
<details>
<summary>摘要</summary>
Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.  This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.Here's the translation in Traditional Chinese:Many knowledge sources consist of both structured information such as relational databases as well as unstructured free text. Building a conversational interface to such data sources is challenging.  This paper introduces SUQL, Structured and Unstructured Query Language, the first formal executable representation that naturally covers compositions of structured and unstructured data queries. Specifically, it augments SQL with several free-text primitives to form a precise, succinct, and expressive representation. This paper also presents a conversational search agent based on large language models, including a few-shot contextual semantic parser for SUQL.   To validate our approach, we introduce a dataset consisting of crowdsourced questions and conversations about real restaurants. Over 51% of the questions in the dataset require both structured and unstructured data, suggesting that it is a common phenomenon. We show that our few-shot conversational agent based on SUQL finds an entity satisfying all user requirements 89.3% of the time, compared to just 65.0% for a strong and commonly used baseline.
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-for-Propaganda-Span-Annotation"><a href="#Large-Language-Models-for-Propaganda-Span-Annotation" class="headerlink" title="Large Language Models for Propaganda Span Annotation"></a>Large Language Models for Propaganda Span Annotation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09812">http://arxiv.org/abs/2311.09812</a></li>
<li>repo_url: None</li>
<li>paper_authors: Maram Hasanain, Fatema Ahmed, Firoj Alam</li>
<li>for: 本研究的目的是检测在线通信中的宣传技巧，以及使用大语言模型GPT-4来完成 annotator 的任务。</li>
<li>methods: 本研究使用了一个自己开发的 Dataset，包括多个笔记者的注释。我们的结果表明，为模型提供更多的信息作为提示，可以提高注释一致性和性能。</li>
<li>results: 我们的结果表明，提供更多的信息作为提示可以提高注释一致性和性能。我们计划将多个笔记者的注释，包括GPT-4的注释，向社区提供。<details>
<summary>Abstract</summary>
The use of propagandistic techniques in online communication has increased in recent years, aiming to manipulate online audiences. Efforts to automatically detect and debunk such content have been made, addressing various modeling scenarios. These include determining whether the content (text, image, or multimodal) (i) is propagandistic, (ii) employs one or more techniques, and (iii) includes techniques with identifiable spans. Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans. We investigate whether large language models such as GPT-4 can be utilized to perform the task of an annotator. For the experiments, we used an in-house developed dataset consisting of annotations from multiple annotators. Our results suggest that providing more information to the model as prompts improves the annotation agreement and performance compared to human annotations. We plan to make the annotated labels from multiple annotators, including GPT-4, available for the community.
</details>
<details>
<summary>摘要</summary>
在latest years, the use of propaganda techniques in online communication has increased, aiming to manipulate online audiences. Efforts to automatically detect and debunk such content have been made, addressing various modeling scenarios. These include determining whether the content (text, image, or multimodal) (i) is propagandistic, (ii) employs one or more techniques, and (iii) includes techniques with identifiable spans. Significant research efforts have been devoted to the first two scenarios compared to the latter. Therefore, in this study, we focus on the task of detecting propagandistic textual spans. We investigate whether large language models such as GPT-4 can be utilized to perform the task of an annotator. For the experiments, we used an in-house developed dataset consisting of annotations from multiple annotators. Our results suggest that providing more information to the model as prompts improves the annotation agreement and performance compared to human annotations. We plan to make the annotated labels from multiple annotators, including GPT-4, available for the community.Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know.
</details></li>
</ul>
<hr>
<h2 id="PixT3-Pixel-based-Table-To-Text-generation"><a href="#PixT3-Pixel-based-Table-To-Text-generation" class="headerlink" title="PixT3: Pixel-based Table To Text generation"></a>PixT3: Pixel-based Table To Text generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09808">http://arxiv.org/abs/2311.09808</a></li>
<li>repo_url: None</li>
<li>paper_authors: Iñigo Alonso, Eneko Agirre, Mirella Lapata</li>
<li>for: 这篇论文旨在提出一种多模态表格转文本模型，以提高表格转文本任务的效率和准确性。</li>
<li>methods: 该模型使用图像表示方法，而不是传统的文本化方法，以提高空间效率。它还使用了一种新的中间训练课程，以增强表格结构的认识。</li>
<li>results: 模型在ToTTo测试套件中的纯表格转文本设置中超过了状态态的表现，并在控制的表格转文本设置中保持竞争力。它还在未看过的数据集中表现出色，在所有生成设置中超越了ToTTo状态态。<details>
<summary>Abstract</summary>
Table-to-Text has been traditionally approached as a linear language to text problem. However, visually represented tables are rich in visual information and serve as a concise, effective form of representing data and its relationships. When using text-based approaches, after the linearization process, this information is either lost or represented in a space inefficient manner. This inefficiency has remained a constant challenge for text-based approaches making them struggle with large tables. In this paper, we demonstrate that image representation of tables are more space-efficient than the typical textual linearizations, and multi-modal approaches are competitive in Table-to-Text tasks. We present PixT3, a multimodal table-to-text model that outperforms the state-of-the-art (SotA) in the ToTTo benchmark in a pure Table-to-Text setting while remaining competitive in controlled Table-to-Text scenarios. It also generalizes better in unseen datasets, outperforming ToTTo SotA in all generation settings. Additionally, we introduce a new intermediate training curriculum to reinforce table structural awareness, leading to improved generation and overall faithfulness of the models.
</details>
<details>
<summary>摘要</summary>
传统上，Table-to-Text问题被看作是一个线性语言到文本问题。然而，可见的表格具有丰富的视觉信息，并且作为数据和其关系的简洁、有效的表示形式。在文本基于的方法中， послеLinearization过程，这些信息将 Either lost or represented in an inefficient manner.这种不足在文本基于的方法中一直是一大挑战，使得它们在处理大表格时困难。在这篇论文中，我们证明了图像表示的表格更加空间效率，而且多模态方法在Table-to-Text任务中竞争。我们提出了PixT3，一种多模态表格到文本模型，在ToTTo Benchmark中超越了状态的艺术（SotA），在纯文本基于的Setting中具有比较竞争力，并在Controlled Table-to-Text Setting中具有更好的整体 faithfulness。此外，我们还引入了一种新的中间培训课程，以强化表格结构的认识，导致模型的生成和整体 faithfulness得到改进。
</details></li>
</ul>
<hr>
<h2 id="The-Curious-Decline-of-Linguistic-Diversity-Training-Language-Models-on-Synthetic-Text"><a href="#The-Curious-Decline-of-Linguistic-Diversity-Training-Language-Models-on-Synthetic-Text" class="headerlink" title="The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text"></a>The Curious Decline of Linguistic Diversity: Training Language Models on Synthetic Text</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09807">http://arxiv.org/abs/2311.09807</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanzhu Guo, Guokan Shang, Michalis Vazirgiannis, Chloé Clavel</li>
<li>for: 这项研究探讨了将大型自然语言模型（LLM）训练于前代模型生成的 sintetic 数据上的后果，这已成为增加人类训练数据的方法的增长趋势。</li>
<li>methods: 我们开发了一组新的评价指标，旨在衡量模型在不同自然语言生成任务上的语言多样性，特别是在时间 recursive 练习中进行的。</li>
<li>results: 我们的发现表明，在successive 迭代中，模型的输出多样性明显减少，这表明了在这种训练方法下，LLMs 的语言能力可能受到限制。<details>
<summary>Abstract</summary>
This study investigates the consequences of training large language models (LLMs) on synthetic data generated by their predecessors, an increasingly prevalent practice aimed at addressing the limited supply of human-generated training data. Diverging from the usual emphasis on performance metrics, we focus on the impact of this training methodology on linguistic diversity, especially when conducted recursively over time. To assess this, we developed a set of novel metrics targeting lexical, syntactic, and semantic diversity, applying them in recursive fine-tuning experiments across various natural language generation tasks. Our findings reveal a marked decrease in the diversity of the models' outputs through successive iterations. This trend underscores the potential risks of training LLMs on predecessor-generated text, particularly concerning the preservation of linguistic richness. Our study highlights the need for careful consideration of the long-term effects of such training approaches on the linguistic capabilities of LLMs.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="DocMath-Eval-Evaluating-Numerical-Reasoning-Capabilities-of-LLMs-in-Understanding-Long-Documents-with-Tabular-Data"><a href="#DocMath-Eval-Evaluating-Numerical-Reasoning-Capabilities-of-LLMs-in-Understanding-Long-Documents-with-Tabular-Data" class="headerlink" title="DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data"></a>DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09805">http://arxiv.org/abs/2311.09805</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, Arman Cohan</li>
<li>for: This paper aims to evaluate the numerical reasoning and problem-solving capabilities of large language models (LLMs) in the context of understanding and analyzing financial documents.</li>
<li>methods: The paper introduces DocMath-Eval, a comprehensive benchmark that incorporates different prompting strategies to assess the capabilities and limitations of existing LLMs in understanding financial documents.</li>
<li>results: The current best-performing system (GPT-4) can perform well on simple problems, but significantly lags behind human experts in more complex problems grounded in longer contexts. The paper concludes that DocMath-Eval can be used as a valuable benchmark to evaluate LLMs’ capabilities to solve challenging numerical reasoning problems in expert domains.<details>
<summary>Abstract</summary>
Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning and problem-solving capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 19 LLMs, including those specialized in coding and finance. We also incorporate different prompting strategies (i.e., Chain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs' capabilities to solve challenging numerical reasoning problems in expert domains. We will release the benchmark and code at https://github.com/yale-nlp/DocMath-Eval.
</details>
<details>
<summary>摘要</summary>
现代LLM技术在解决类似于考试的数学问题方面已经表现出了惊人的表现。然而，这些数学解决问题在实际场景中的有效性，特别是在专家领域，仍然未经充分调查。这篇论文介绍了DocMath-Eval，一个特有的数学问题解决和分析金融文档中的文本和表格的完整性评价标准。我们评估了19种不同的LLM系统，包括编程和金融领域的专门系统。我们还采用了不同的提示策略（即链条思维和程序思维）来全面评估现有LLM的能力和局限性。我们发现，即使最佳表现的系统（即GPT-4）在短文档上解决金融指标增长率的简单问题上表现出色，但是在更复杂的问题上，它在更长的文档上缺乏人类专家的能力。我们认为DocMath-Eval可以用于评估LLM在专家领域中解决复杂的数学问题的能力。我们将在https://github.com/yale-nlp/DocMath-Eval上发布标准和代码。
</details></li>
</ul>
<hr>
<h2 id="textit-Dial-BeInfo-for-Faithfulness-Improving-Factuality-of-Information-Seeking-Dialogue-via-Behavioural-Fine-Tuning"><a href="#textit-Dial-BeInfo-for-Faithfulness-Improving-Factuality-of-Information-Seeking-Dialogue-via-Behavioural-Fine-Tuning" class="headerlink" title="$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning"></a>$\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09800">http://arxiv.org/abs/2311.09800</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Razumovskaia, Ivan Vulić, Pavle Marković, Tomasz Cichy, Qian Zheng, Tsung-Hsien Wen, Paweł Budzianowski</li>
<li>for: 提高信息寻找对话系统的准确性和可靠性，使其响应用户的询问能够提供有用和适合知识源的回答。</li>
<li>methods: 使用行为调整来改善信息寻找对话系统的准确性和可靠性，以避免现象抽象和假象。</li>
<li>results: 对三个标准数据集和多个领域进行了调整，并在零容量情况下在不同领域中表现出色，而且在实际生产对话中表现更好，超过GPT4。<details>
<summary>Abstract</summary>
Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong performance on data from real `production' conversations and outperform GPT4 when tuned on a limited amount of such realistic in-domain dialogues.
</details>
<details>
<summary>摘要</summary>
factuality是寻求信息对话中的重要需求：系统应该根据用户的询问回答，以便响应是有意义的并与系统提供的知识一致。然而，现代大语言模型很容易出现幻觉，即生成不支持或与知识源相 contradicting 的回答。为了解决这个问题并提高信息寻求对话系统的忠诚度，我们介绍了BeInfo，一种简单 yet effective的方法，通过行为调整来帮助信息寻求对话。我们使用三个标准 dataset 来显示，通过BeInfo-调整，模型在seen 和 unseen 领域都变得更加忠诚于知识源。此外，我们还显示，具有3B参数的模型（如Flan-T5），通过BeInfo 的调整，在真实的生产对话数据上表现出色，并在不seen 领域中具有 Zero-shot 的优势。
</details></li>
</ul>
<hr>
<h2 id="How-Far-Can-We-Extract-Diverse-Perspectives-from-Large-Language-Models-Criteria-Based-Diversity-Prompting"><a href="#How-Far-Can-We-Extract-Diverse-Perspectives-from-Large-Language-Models-Criteria-Based-Diversity-Prompting" class="headerlink" title="How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!"></a>How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09799">http://arxiv.org/abs/2311.09799</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minnesotanlp/diversity-extraction-from-llms">https://github.com/minnesotanlp/diversity-extraction-from-llms</a></li>
<li>paper_authors: Shirley Anugrah Hayati, Minhwa Lee, Dheeraj Rajagopal, Dongyeop Kang</li>
<li>for: 这个研究旨在检验LLMs是否可以生成多元观点和理由，以及可以不同程度检验LLMs的多元观点生成能力。</li>
<li>methods: 研究使用了一种基于标准的提问技术来评估LLMs的多元观点生成能力，并使用了句子嵌入和距离度量来衡量 semantics 多元性。</li>
<li>results: 研究发现，通过使用提问技术，可以很好地评估LLMs的多元观点生成能力，并且可以在不同任务上（如荷尔豪害语言标注和故事续写）检验LLMs的多元观点生成能力。<details>
<summary>Abstract</summary>
Collecting diverse human data on subjective NLP topics is costly and challenging. As Large Language Models (LLMs) have developed human-like capabilities, there is a recent trend in collaborative efforts between humans and LLMs for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate this problem as diversity extraction in LLMs and propose a criteria-based prompting technique to ground diverse opinions and measure perspective diversity from the generated criteria words. Our results show that measuring semantic diversity through sentence embeddings and distance metrics is not enough to measure perspective diversity. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our prompting method to other tasks (hate speech labeling and story continuation), indeed we find that LLMs are able to generate diverse opinions according to the degree of task subjectivity.
</details>
<details>
<summary>摘要</summary>
COLLECTING多样的人类数据 на主观NLP话题是成本高昂和挑战性强的。随着大语言模型（LLMs）的发展，有一种现代趋势是通过人类和LLMs的合作来生成多样数据，提供可扩展和高效的解决方案。然而，LLMs是否具备生成主观话题多样视角的能力仍是一个未知问题。在这种研究中，我们调查LLMs是否能够生成主观话题多样视角和理由，如社会规范和论战文本。我们将这个问题定义为LLMs中的多样性提取问题，并提出了基于标准的提示技术来锁定多样的意见和度量视角多样性从生成的标准词语中。我们的结果表明，通过句子嵌入和距离度量来度量semantic多样性并不够来度量视角多样性。为了测试LLMs是否能够提取多样视角，我们采用了一种步骤性的回忆提示法，通过多次生成输出来评估模型的多样性覆盖率。在我们应用提示方法于其他任务（仇恨言语标注和故事续写）时，实际上我们发现LLMs可以根据任务的主观程度生成多样的意见。
</details></li>
</ul>
<hr>
<h2 id="KnowledgeMath-Knowledge-Intensive-Math-Word-Problem-Solving-in-Finance-Domains"><a href="#KnowledgeMath-Knowledge-Intensive-Math-Word-Problem-Solving-in-Finance-Domains" class="headerlink" title="KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains"></a>KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09797">http://arxiv.org/abs/2311.09797</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, Arman Cohan</li>
<li>for: 评估语言模型在金融领域的应用能力，特别是解决复杂数学问题。</li>
<li>methods: 使用 KnowledgeMath  benchmark，包括 1,259 个问题，具有混合文本和表格内容，并提供专家注解的详细解决方案。</li>
<li>results: 评估了 14 种不同的语言模型，其中最高级别的系统 (GPT-4 with Program-of-Thoughts) 的准确率只有 45.4%，而知识扩展的 LLMs 可以提高性能 (如 GPT-3.5 从 23.9% 提高到 32.0%)，但仍然远低于人类专家的估计性能 (94%)。<details>
<summary>Abstract</summary>
We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems. Compared to prior works, this study features three core advancements. First, KnowledgeMath includes 1,259 problems with a hybrid of textual and tabular content and require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves only 45.4% accuracy, leaving substantial room for improvement. While knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0% for GPT-3.5), it is still significantly lower the estimated human expert performance of 94%. We believe that KnowledgeMath can facilitate future research on domain-specific knowledge retrieval and augmentation into the math word problem-solving process. We will release the benchmark and code at https://github.com/yale-nlp/KnowledgeMath.
</details>
<details>
<summary>摘要</summary>
我们介绍 KnowledgeMath，一个新的评估大型自然语言处理（LLM）的能力应用金融知识解决复杂的数学问题的标准库。相比之前的研究，这些研究有三个核心进步：首先，KnowledgeMath 包含 1,259 个具有文本和表格内容的问题，需要大学学士学位水准的金融领域知识以解决。第二，我们提供了专家标注、详细的解决方案 refer 以 Python 程式码格式，以 Ensure 高品质的标准库 для LLM 评估。第三，我们评估了 14 种不同的 LLM，包括 Chain-of-Thoughts 和 Program-of-Thoughts 等提示策略。现有最高表现的系统 (即 GPT-4 with Program-of-Thoughts) 的精度为 45.4%，剩下许多空间供改善。而知识增强 LLM 可以提高表现 (例如从 23.9% 提升至 32.0%  для GPT-3.5)，但仍然很低于估计的人类专家表现率 (94%)。我们认为 KnowledgeMath 可以促进未来专业知识抽取和增强在数学问题解决过程中的研究。我们将在 GitHub 上发布标准库和程式码。
</details></li>
</ul>
<hr>
<h2 id="More-Samples-or-More-Prompt-Inputs-Exploring-Effective-In-Context-Sampling-for-LLM-Few-Shot-Prompt-Engineering"><a href="#More-Samples-or-More-Prompt-Inputs-Exploring-Effective-In-Context-Sampling-for-LLM-Few-Shot-Prompt-Engineering" class="headerlink" title="More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering"></a>More Samples or More Prompt Inputs? Exploring Effective In-Context Sampling for LLM Few-Shot Prompt Engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09782">http://arxiv.org/abs/2311.09782</a></li>
<li>repo_url: None</li>
<li>paper_authors: Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu, Jiachen Li, Shao Zhang, Sijia Liu, James Hendler, Dakuo Wang</li>
<li>for: 提高LLM性能和信心度</li>
<li>methods: 利用多个ICL提示输入构建多个ICS提示输入，以提高LLM的预测性能和信心度</li>
<li>results: 实验结果表明，ICS可以一直提高LLM的预测性能和信心度，而且可以采用多样性基于的策略进一步提高LLM的性能。<details>
<summary>Abstract</summary>
While most existing works on LLM prompt-engineering focus only on how to select a better set of data samples inside one single prompt input (In-Context Learning or ICL), why can't we design and leverage multiple prompt inputs together to further improve the LLM performance? In this work, we propose In-Context Sampling (ICS), a low-resource LLM prompt-engineering technique to produce the most confident prediction results by optimizing the construction of multiple ICL prompt inputs. Extensive experiments with two SOTA LLMs (FlanT5-XL and Mistral-7B) on three NLI datasets (e-SNLI, Multi-NLI, and ANLI) illustrate that ICS can consistently enhance LLM's prediction performance and confidence. An ablation study suggests that a diversity-based ICS strategy may further improve LLM's performance, which sheds light on a new yet promising future research direction.
</details>
<details>
<summary>摘要</summary>
“现有的大多数 LLM 提示工程化研究仅专注于选择更好的内部提示输入（内部学习或 ICL），那么我们不能设计和利用多个提示输入来进一步提高 LLM 的表现吗？在这个工作中，我们提出了内部抽象（ICS），一种低资源 LLM 提示工程化技术，以提高多个 ICL 提示输入的建构，以获得最高的预测结果和自信度。实验显示，使用 FlanT5-XL 和 Mistral-7B 两个 SOTA LLM 在 e-SNLI、Multi-NLI 和 ANLI 三个 NLI 数据集上，ICS 可以一致地提高 LLM 的预测性能和自信度。剖析研究表明，一种多样性基于的 ICS 策略可能会进一步提高 LLM 的表现，这照明了一个新的未来研究方向。”Note: "LLM" stands for "Large Language Model" in English.
</details></li>
</ul>
<hr>
<h2 id="To-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering"><a href="#To-be-or-not-to-be-an-exploration-of-continuously-controllable-prompt-engineering" class="headerlink" title="To be or not to be? an exploration of continuously controllable prompt engineering"></a>To be or not to be? an exploration of continuously controllable prompt engineering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09773">http://arxiv.org/abs/2311.09773</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Yuhan Sun, Mukai Li, Yixin Cao, Kun Wang, Wenxiao Wang, Xingyu Zeng, Rui Zhao</li>
<li>for: 这篇论文旨在提供一种能够精确控制 Language Model 的问题提示（Prompt）影响，以便更好地自定义模型和处理其输出。</li>
<li>methods: 本文使用的方法包括 LoRA（Low-Rank Adaptation）和特定的问题提示蒸馏（Prompt distillation），以实现问题提示的精确控制。</li>
<li>results: 本文的实验结果显示，ControlPE 可以实现精确控制不同类型的问题提示（包括短回答问题、拒绝问题和推理链问题），并且能够在不同的任务上灵活地应用。<details>
<summary>Abstract</summary>
As the use of large language models becomes more widespread, techniques like parameter-efficient fine-tuning and other methods for controlled generation are gaining traction for customizing models and managing their outputs. However, the challenge of precisely controlling how prompts influence these models is an area ripe for further investigation. In response, we introduce ControlPE (Continuously Controllable Prompt Engineering). ControlPE enables finer adjustments to prompt effects, complementing existing prompt engineering, and effectively controls continuous targets. This approach harnesses the power of LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting, enabling fine-tuned adjustments to the impact of prompts. Our methodology involves generating specialized datasets for prompt distillation, incorporating these prompts into the LoRA model, and carefully adjusting LoRA merging weight to regulate the influence of prompts. This provides a dynamic and adaptable tool for prompt control. Through our experiments, we have validated the practicality and efficacy of ControlPE. It proves to be a promising solution for control a variety of prompts, ranging from generating short responses prompts, refusal prompts to chain-of-thought prompts.
</details>
<details>
<summary>摘要</summary>
As the use of large language models becomes more widespread, techniques like parameter-efficient fine-tuning and other methods for controlled generation are gaining traction for customizing models and managing their outputs. However, the challenge of precisely controlling how prompts influence these models is an area ripe for further investigation. In response, we introduce ControlPE (Continuously Controllable Prompt Engineering). ControlPE enables finer adjustments to prompt effects, complementing existing prompt engineering, and effectively controls continuous targets. This approach harnesses the power of LoRA (Low-Rank Adaptation) to create an effect akin to prompt weighting, enabling fine-tuned adjustments to the impact of prompts. Our methodology involves generating specialized datasets for prompt distillation, incorporating these prompts into the LoRA model, and carefully adjusting LoRA merging weight to regulate the influence of prompts. This provides a dynamic and adaptable tool for prompt control. Through our experiments, we have validated the practicality and efficacy of ControlPE. It proves to be a promising solution for control a variety of prompts, ranging from generating short responses prompts, refusal prompts to chain-of-thought prompts.Here's the translation in Traditional Chinese:当大语言模型的使用越来越普及时，Parameter-efficient fine-tuning 和其他控制生成的技术也在广泛地应用，以适应化模型和管理其输出。然而， precisely controlling how prompts influence these models 是一个需要进一步的探索的领域。为此，我们引入 ControlPE (Continuously Controllable Prompt Engineering)。ControlPE 可以实现更细微的问题影响，与现有的问题工程相结合，并实现连续目标的控制。这个方法利用 LoRA (Low-Rank Adaptation) 的力量，实现问题权重的效果，并允许精确地调整问题的影响。我们的方法包括生成特殊的问题蒸馏集，将这些问题 integrate 到 LoRA 模型中，并精确地调整 LoRA 合并重量，以调控问题的影响。这提供了一个动态和适应的问题控制工具。经过我们的实验，我们已经 validate 了 ControlPE 的实用性和有效性。它证明是一个可靠的解决方案，可以控制多种问题，包括短回应问题、拒绝问题和链式思维问题。
</details></li>
</ul>
<hr>
<h2 id="LLMs-as-Narcissistic-Evaluators-When-Ego-Inflates-Evaluation-Scores"><a href="#LLMs-as-Narcissistic-Evaluators-When-Ego-Inflates-Evaluation-Scores" class="headerlink" title="LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores"></a>LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09766">http://arxiv.org/abs/2311.09766</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin</li>
<li>for: This paper aims to investigate the potential bias of language model-driven evaluation metrics in the context of summarization tasks.</li>
<li>methods: The paper uses three popular language models (BART, T5, and GPT) to evaluate the quality of summaries generated by these models.</li>
<li>results: The paper finds that the evaluation metrics demonstrate a bias towards the underlying language models, particularly when used in a reference-free manner without gold summaries.<details>
<summary>Abstract</summary>
Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and GPTScore--demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more dependable evaluation protocols in the future.
</details>
<details>
<summary>摘要</summary>
<<SYS>>现代自然语言处理（NLP）领域中自动评估生成文本内容的挑战仍在继续。由于现代语言模型（LM）在多种NLP任务中表现出色，因此有增加使用这些模型来创造新的评估指标来自动评估生成任务。本文探讨一个重要问题：语言模型驱动的评估指标是否具有偏好于由同一个语言模型生成的文本？ Specifically，我们评估了三个主要LM-based评估指标——BARTScore、T5Score和GPTScore——在摘要任务中是否具有偏好于其所处理的语言模型。我们的发现显示，在不使用黄金摘要的情况下，这些评估指标具有明显的偏好，特别是当用于 reference-free 的情况下。这些结果表明，由生成评估模型提供的评估结果可能会受到 beyond 文本质量的因素的影响，高亮了未来需要开发更可靠的评估协议。
</details></li>
</ul>
<hr>
<h2 id="Test-time-Backdoor-Mitigation-for-Black-Box-Large-Language-Models-with-Defensive-Demonstrations"><a href="#Test-time-Backdoor-Mitigation-for-Black-Box-Large-Language-Models-with-Defensive-Demonstrations" class="headerlink" title="Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations"></a>Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09763">http://arxiv.org/abs/2311.09763</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, Muhao Chen</li>
<li>for: This paper focuses on defending against backdoor attacks in large language models (LLMs) during the testing phase, which has been overlooked in previous studies that primarily focus on training-time defenses.</li>
<li>methods: The proposed method, called defensive demonstrations, involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are combined with user queries and presented to the model during testing, without requiring any modifications or tuning to the black-box model.</li>
<li>results: The paper shows that defensive demonstrations are effective in defending against both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.<details>
<summary>Abstract</summary>
Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models. Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications/tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations. Extensive experiments show that defensive demonstrations are effective in defending both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.
</details>
<details>
<summary>摘要</summary>
Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications or tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations.Our extensive experiments show that defensive demonstrations are effective in defending against both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.
</details></li>
</ul>
<hr>
<h2 id="OrchestraLLM-Efficient-Orchestration-of-Language-Models-for-Dialogue-State-Tracking"><a href="#OrchestraLLM-Efficient-Orchestration-of-Language-Models-for-Dialogue-State-Tracking" class="headerlink" title="OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking"></a>OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09758">http://arxiv.org/abs/2311.09758</a></li>
<li>repo_url: None</li>
<li>paper_authors: Chia-Hsuan Lee, Hao Cheng, Mari Ostendorf</li>
<li>for: 提高自然语言处理系统的计算效率，使用小语言模型（SLM）作为cost-effective的替代方案。</li>
<li>methods: 基于发现SLM和大语言模型（LLM）在结构化知识EXTRACTION任务中具有补做的优势，提出了一种SLM&#x2F;LLM路由框架，通过在批处理中选择最佳路由来提高计算效率并提高任务性能。</li>
<li>results: 在对话状态追踪任务中，提出的路由框架substantially提高了性能，而且降低了计算成本超过50%。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.
</details>
<details>
<summary>摘要</summary>
The framework begins by creating exemplar pools that represent the types of contexts where each LM provides a more reliable answer. This is achieved by fine-tuning a sentence embedding so that context similarity is close to dialogue state similarity. During inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote.In dialogue state tracking tasks, the proposed routing framework enhances performance by over 50% compared to relying solely on LLMs, while reducing computational costs by over 50%. This framework demonstrates the potential of combining SLMs and LLMs to improve the efficiency and effectiveness of natural language processing systems.
</details></li>
</ul>
<hr>
<h2 id="FairytaleCQA-Integrating-a-Commonsense-Knowledge-Graph-into-Children’s-Storybook-Narratives"><a href="#FairytaleCQA-Integrating-a-Commonsense-Knowledge-Graph-into-Children’s-Storybook-Narratives" class="headerlink" title="FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children’s Storybook Narratives"></a>FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children’s Storybook Narratives</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09756">http://arxiv.org/abs/2311.09756</a></li>
<li>repo_url: None</li>
<li>paper_authors: Jiaju Chen, Yuxuan Lu, Shao Zhang, Bingsheng Yao, Yuanzhe Dong, Ying Xu, Yunyao Li, Qianwen Wang, Dakuo Wang, Yuling Sun</li>
<li>for: 这个论文旨在提供适用于下游儿童教育应用的自定义问答功能，用于补充现有的故事书内容。</li>
<li>methods: 该论文使用了LLM模型，并通过外部知识图进行 Commonsense知识的扩展。</li>
<li>results: 对比较大的LLM模型（GPT-4），一个较小的T5-large模型在新的问答对组成任务（QAG）中表现出色，表明：1）我们的数据集对现有LLM模型带来了新的挑战，2）人工专家的数据注释仍然是关键，因为它们在儿童教育领域具有丰富的细节知识。<details>
<summary>Abstract</summary>
AI models (including LLM) often rely on narrative question-answering (QA) datasets to provide customized QA functionalities to support downstream children education applications; however, existing datasets only include QA pairs that are grounded within the given storybook content, but children can learn more when teachers refer the storybook content to real-world knowledge (e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is annotated by children education experts, to supplement 278 storybook narratives with educationally appropriate commonsense knowledge. The dataset has 5,868 QA pairs that not only originate from the storybook narrative but also contain the commonsense knowledge grounded by an external knowledge graph (i.e., ConceptNet). A follow-up experiment shows that a smaller model (T5-large) fine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered LLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result suggests that: 1) our dataset brings novel challenges to existing LLMs, and 2) human experts' data annotation are still critical as they have much nuanced knowledge that LLMs do not know in the children educational domain.
</details>
<details>
<summary>摘要</summary>
人工智能模型（包括LLM）经常利用叙事问答（QA）数据集来提供下游儿童教育应用程序中自定义的QA功能;然而，现有数据集只包含基于给定的故事书内容的QA对。然而，孩子们可以通过教师将故事书内容与实际世界知识相关联来学习更多。我们介绍了 FairytaleCQA 数据集，该数据集由儿童教育专家标注，用于补充 278 本故事书内容教育适用的常识知识。该数据集包含 5,868 对问答，其中不仅来自故事书内容，还由外部知识图（i.e., ConceptNet）补充了 Commonsense 知识。一项追加实验表明，一个较小的模型（T5-large）在 FairytaleCQA 上进行了可靠地超越了较大的Prompt-工程化 LLVM（例如 GPT-4）在这个新的问答对生成任务（QAG）中。这一结果表明：1）我们的数据集带来了现有的LLMs新的挑战，2）人类专家的数据标注仍然是关键的，因为它们在儿童教育领域中具有许多细节的知识，LLMs不知。
</details></li>
</ul>
<hr>
<h2 id="How-Does-Calibration-Data-Affect-the-Post-training-Pruning-and-Quantization-of-Large-Language-Models"><a href="#How-Does-Calibration-Data-Affect-the-Post-training-Pruning-and-Quantization-of-Large-Language-Models" class="headerlink" title="How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?"></a>How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09755">http://arxiv.org/abs/2311.09755</a></li>
<li>repo_url: None</li>
<li>paper_authors: Miles Williams, Nikolaos Aletras</li>
<li>for: 本文研究了大语言模型（LLM）压缩和量化的基础，包括各种压缩和量化技术的效果，以及这些技术如何影响 LLM 的性能。</li>
<li>methods: 本文使用了多种压缩和量化方法，包括各种压缩和量化技术，以及不同任务、模型和数据集。</li>
<li>results: 研究发现，使用不同的滤波数据会导致下游任务性能异常大，与现有研究不同，表明使用不同的滤波数据可能会导致 LLM 的性能变化。<details>
<summary>Abstract</summary>
Pruning and quantization form the foundation of model compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated state-of-the-art performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples, to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of pruning and quantization methods, tasks, models, and datasets. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.
</details>
<details>
<summary>摘要</summary>
剪枝和量化是神经网络模型压缩的基础，启用高效的推理 для大语言模型（LLM）。近年，各种量化和剪枝技术在后处理环境中表现出了状态之冠。它们依赖于校准数据，一小量的无标示例，来生成层活动。然而，没有任何先前的工作系统atically investigated calibration data对LLM性能的影响。在这篇论文中，我们提供了首次对剪枝和量化方法的效果进行了广泛的实验研究。我们对各种剪枝和量化方法、任务、模型和数据集进行了试验。 surprisingly，我们发现了下游任务性能的显著差异，与现有的工作 suggessthat a greater level of robustness to the calibration data。最后，我们对LLM剪枝和量化中有效使用calibration data的推荐。
</details></li>
</ul>
<hr>
<h2 id="Translation-Aligned-Sentence-Embeddings-for-Turkish-Language"><a href="#Translation-Aligned-Sentence-Embeddings-for-Turkish-Language" class="headerlink" title="Translation Aligned Sentence Embeddings for Turkish Language"></a>Translation Aligned Sentence Embeddings for Turkish Language</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09748">http://arxiv.org/abs/2311.09748</a></li>
<li>repo_url: None</li>
<li>paper_authors: Eren Unlu, Unver Ciftci</li>
<li>for: 提高 sentence embedding 模型在 Turkish 语言上的表现</li>
<li>methods: 提出了一种两个阶段的训练方法，其中第一阶段通过对 embedding 空间进行对应的调整，以便在 sentence embedding 设置中使用 pretrained encoder-decoder 模型进行精度的 fine-tuning。</li>
<li>results: 通过这种方法，可以在短时间内使用有限的 target 语言数据进行高精度的 fine-tuning，并且可以提高 sentence embedding 模型在 Turkish 语言上的表现。<details>
<summary>Abstract</summary>
Due to the limited availability of high quality datasets for training sentence embeddings in Turkish, we propose a training methodology and a regimen to develop a sentence embedding model. The central idea is simple but effective : is to fine-tune a pretrained encoder-decoder model in two consecutive stages, where the first stage involves aligning the embedding space with translation pairs. Thanks to this alignment, the prowess of the main model can be better projected onto the target language in a sentence embedding setting where it can be fine-tuned with high accuracy in short duration with limited target language dataset.
</details>
<details>
<summary>摘要</summary>
Here's the text in Simplified Chinese:由于土耳其语 sentence embedding 训练数据的可用性有限，我们提出了一种训练方法和日程，以提高 sentence embedding 模型的质量。中心思想简单 yet effective：在两个阶段中，首先对预训练的 encoder-decoder 模型进行了两个阶段的微调，其中第一个阶段是将 embedding 空间与翻译对照进行对齐。这样可以使得模型在 sentence embedding 设置下，通过短时间内使用有限的目标语言数据进行高精度的微调。
</details></li>
</ul>
<hr>
<h2 id="Capturing-Perspectives-of-Crowdsourced-Annotators-in-Subjective-Learning-Tasks"><a href="#Capturing-Perspectives-of-Crowdsourced-Annotators-in-Subjective-Learning-Tasks" class="headerlink" title="Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks"></a>Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09743">http://arxiv.org/abs/2311.09743</a></li>
<li>repo_url: None</li>
<li>paper_authors: Negar Mokhberian, Myrl G. Marmarelis, Frederic R. Hopp, Valerio Basile, Fred Morstatter, Kristina Lerman</li>
<li>for: 这篇论文的目的是解决对主观分类任务中的多 annotator 问题，因为对于主观任务，可能会有多个真实的标签，导致模型偏向特定的标签。</li>
<li>methods: 这篇论文提出了一个新的方法，即 Annotator Aware Representations for Texts (AART)，这个方法可以将每个 annotator 的标签视为一个独立的标签，以便更好地捕捉 annotators 的看法。</li>
<li>results: 该方法可以提高模型在捕捉 annotators 的看法方面的表现，并且可以避免因为 annotators 的差异而导致的偏向。此外，这个方法还可以学习 annotators 的行为，以便进一步的探索。<details>
<summary>Abstract</summary>
In most classification models, it has been assumed to have a single ground truth label for each data point. However, subjective tasks like toxicity classification can lead to genuine disagreement among annotators. In these cases aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with small samples. This problem is especially the case in crowd-sourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. We will show the improvement of our method on metrics that assess the performance on capturing annotators' perspectives. Additionally, our approach involves learning representations for annotators, allowing for an exploration of the captured annotation behaviors.
</details>
<details>
<summary>摘要</summary>
通常的分类模型假设每个数据点有单一的真实标签。然而，主观任务如攻击性评分可能会导致注释器之间真实的分歧。在这种情况下，聚合标签会导致偏执zh labels和模型，这些模型可能会忽略小量意见。先前的研究已经揭示了标签聚合的坑缺和提出了一些实用的方法来解决这个问题。我们最近提出的多注释模型，它预测每个注释器的标签，容易受到 annotators with small samples 的下降决策。这个问题特别是在大量数据集中存在。在这种情况下，我们提出了注释者意识表示（AART） для主观分类任务。我们将展示我们的方法在衡量注释者的观点性能指标上的改进。此外，我们的方法包括学习注释者的表示，以便探索被捕捉的注释行为。
</details></li>
</ul>
<hr>
<h2 id="What-Constitutes-a-Faithful-Summary-Preserving-Author-Perspectives-in-News-Summarization"><a href="#What-Constitutes-a-Faithful-Summary-Preserving-Author-Perspectives-in-News-Summarization" class="headerlink" title="What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization"></a>What Constitutes a Faithful Summary? Preserving Author Perspectives in News Summarization</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09741">http://arxiv.org/abs/2311.09741</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/lyh6560new/p3sum">https://github.com/lyh6560new/p3sum</a></li>
<li>paper_authors: Yuhan Liu, Shangbin Feng, Xiaochuang Han, Vidhisha Balachandran, Chan Young Park, Sachin Kumar, Yulia Tsvetkov</li>
<li>for: 这篇论文的目的是设计一个忠于作者意见和观点的摘要系统。</li>
<li>methods: 这篇论文使用了一种叫做P^3Sum的扩散模型基本摘要方法，这个方法使用政治观点分类器控制摘要的政治倾向。</li>
<li>results: 实验结果显示，P^3Sum比前一代摘要系统和大语言模型提高了11.4%的成功率，具有与标准摘要价值指标一样的性能。这些结果显示，即使是现有的模型，在新闻摘要中保持作者意见和观点仍然是一个挑战，而P^3Sum则是一个重要的第一步。<details>
<summary>Abstract</summary>
In this work, we take a first step towards designing summarization systems that are faithful to the author's opinions and perspectives. Focusing on a case study of preserving political perspectives in news summarization, we find that existing approaches alter the political opinions and stances of news articles in more than 50% of summaries, misrepresenting the intent and perspectives of the news authors. We thus propose P^3Sum, a diffusion model-based summarization approach controlled by political perspective classifiers. In P^3Sum, the political leaning of a generated summary is iteratively evaluated at each decoding step, and any drift from the article's original stance incurs a loss back-propagated to the embedding layers, steering the political stance of the summary at inference time. Extensive experiments on three news summarization datasets demonstrate that P^3Sum outperforms state-of-the-art summarization systems and large language models by up to 11.4% in terms of the success rate of stance preservation, with on-par performance on standard summarization utility metrics. These findings highlight the lacunae that even for state-of-the-art models it is still challenging to preserve author perspectives in news summarization, while P^3Sum presents an important first step towards evaluating and developing summarization systems that are faithful to author intent and perspectives.
</details>
<details>
<summary>摘要</summary>
在这项工作中，我们开始努力设计 faithful 的摘要系统，以保持作者的意图和观点。通过新闻摘要中保持政治立场的案例研究，我们发现现有方法在超过50%的摘要中改变了新闻文章的政治立场和意图，这些摘要不符合作者的意图和观点。我们因此提出 P^3Sum，一种基于扩散模型的摘要方法，该方法在摘要生成过程中控制政治观点分类器，以确保生成的摘要保持原文的政治立场。在 P^3Sum 中，生成的摘要中的政治倾向在每个解码步骤中被评估，如果摘要偏离原文的政治立场，就会产生损失，这些损失将在嵌入层传递给 embedding 层，以在推理时间控制摘要的政治倾向。我们对三个新闻摘要数据集进行了广泛的实验，结果表明，P^3Sum 在保持摘要的政治立场方面的成功率比现有的摘要系统和大语言模型高出11.4%，而与标准摘要用途指标具有相同的性能。这些发现表明，即使是当今最先进的模型，在新闻摘要中保持作者的意图和观点仍然是一项挑战，而 P^3Sum 则是一个重要的第一步。
</details></li>
</ul>
<hr>
<h2 id="CARE-Extracting-Experimental-Findings-From-Clinical-Literature"><a href="#CARE-Extracting-Experimental-Findings-From-Clinical-Literature" class="headerlink" title="CARE: Extracting Experimental Findings From Clinical Literature"></a>CARE: Extracting Experimental Findings From Clinical Literature</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09736">http://arxiv.org/abs/2311.09736</a></li>
<li>repo_url: None</li>
<li>paper_authors: Aakanksha Naik, Bailey Kuehl, Erin Bransom, Doug Downey, Tom Hope</li>
<li>for: 本研究旨在提供一个新的信息抽取 dataset，用于抽取生物医学文献中的临床发现。</li>
<li>methods: 本研究使用了一新的注解 schema， capture 了细化的发现作为 n-ary 关系 между实体和属性。该 schema 包括困难现象，如不连续实体跨 span、嵌入关系和变量 arity n-ary 关系。</li>
<li>results: 研究使用了两个来源的700个摘要进行广泛的注解，并对多种当前IE系统的性能进行了测试。结果表明，即使使用 SOTA 模型，如 GPT4，也很难在这个数据集上进行relation extraction。<details>
<summary>Abstract</summary>
Extracting fine-grained experimental findings from literature can provide massive utility for scientific applications. Prior work has focused on developing annotation schemas and datasets for limited aspects of this problem, leading to simpler information extraction datasets which do not capture the real-world complexity and nuance required for this task. Focusing on biomedicine, this work presents CARE (Clinical Aggregation-oriented Result Extraction) -- a new IE dataset for the task of extracting clinical findings. We develop a new annotation schema capturing fine-grained findings as n-ary relations between entities and attributes, which includes phenomena challenging for current IE systems such as discontinuous entity spans, nested relations, and variable arity n-ary relations. Using this schema, we collect extensive annotations for 700 abstracts from two sources: clinical trials and case reports. We also benchmark the performance of various state-of-the-art IE systems on our dataset, including extractive models and generative LLMs in fully supervised and limited data settings. Our results demonstrate the difficulty of our dataset -- even SOTA models such as GPT4 struggle, particularly on relation extraction. We release our annotation schema and CARE to encourage further research on extracting and aggregating scientific findings from literature.
</details>
<details>
<summary>摘要</summary>
<<SYS>>通过Literature中提取细致实验结果可以提供巨大的科学应用 utility。先前的工作主要集中在开发注解schema和数据集，以便解决这个问题的有限方面，导致的是更简单的信息抽取数据集，这些数据集不能捕捉实际世界中的复杂性和细节。在生物医学领域，本工作提出了CARE（临床结合 oriented Result Extraction）——一个新的IE数据集，用于提取临床发现。我们开发了一个新的注解schema，捕捉细致发现为n-ary关系 между实体和属性，该schemas包括现实困难 для当前IE系统的现象，如不连续实体跨度、嵌入关系和变量数学 n-ary关系。使用该schemas，我们收集了700个报告的广泛的注解，来自两个来源：临床试验和案例报告。我们还对我们的数据集进行了多种当前IE系统的性能测试，包括抽取模型和生成LLMs在完全超vised和有限数据设置下。我们的结果表明，我们的数据集具有很大的困难度——即使SOTA模型如GPT4，它们在关系提取方面尤其困难。我们发布了我们的注解schema和CARE，以促进Literature中的实验发现和抽取。
</details></li>
</ul>
<hr>
<h2 id="Tracking-the-Newsworthiness-of-Public-Documents"><a href="#Tracking-the-Newsworthiness-of-Public-Documents" class="headerlink" title="Tracking the Newsworthiness of Public Documents"></a>Tracking the Newsworthiness of Public Documents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09734">http://arxiv.org/abs/2311.09734</a></li>
<li>repo_url: None</li>
<li>paper_authors: Alexander Spangher, Emilio Ferrara, Ben Welsh, Nanyun Peng, Serdar Tumgoren, Jonathan May</li>
<li>for: 本研究ocuses on Local public policy coverage in the San Francisco Bay Area by the San Francisco Chronicle.</li>
<li>methods: The paper uses probabilistic relational modeling to link news articles, public policy documents, and meeting recordings.</li>
<li>results: The paper shows that different aspects of public policy discussion yield different newsworthiness signals, and their systems identify policies considered newsworthy with 68% F1 and their coverage recommendations are helpful with an 84% win-rate.Here’s the format you requested:</li>
<li>for: &lt;本研究ocuses on Local public policy coverage in the San Francisco Bay Area by the San Francisco Chronicle.&gt;</li>
<li>methods: &lt;The paper uses probabilistic relational modeling to link news articles, public policy documents, and meeting recordings.&gt;</li>
<li>results: &lt;The paper shows that different aspects of public policy discussion yield different newsworthiness signals, and their systems identify policies considered newsworthy with 68% F1 and their coverage recommendations are helpful with an 84% win-rate.&gt;<details>
<summary>Abstract</summary>
Journalists must find stories in huge amounts of textual data (e.g. leaks, bills, press releases) as part of their jobs: determining when and why text becomes news can help us understand coverage patterns and help us build assistive tools. Yet, this is challenging because very few labelled links exist, language use between corpora is very different, and text may be covered for a variety of reasons. In this work we focus on news coverage of local public policy in the San Francisco Bay Area by the San Francisco Chronicle. First, we gather news articles, public policy documents and meeting recordings and link them using probabilistic relational modeling, which we show is a low-annotation linking methodology that outperforms other retrieval-based baselines. Second, we define a new task: newsworthiness prediction, to predict if a policy item will get covered. We show that different aspects of public policy discussion yield different newsworthiness signals. Finally we perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68% F1 and our coverage recommendations are helpful with an 84% win-rate.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="MOKA-Moral-Knowledge-Augmentation-for-Moral-Event-Extraction"><a href="#MOKA-Moral-Knowledge-Augmentation-for-Moral-Event-Extraction" class="headerlink" title="MOKA: Moral Knowledge Augmentation for Moral Event Extraction"></a>MOKA: Moral Knowledge Augmentation for Moral Event Extraction</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09733">http://arxiv.org/abs/2311.09733</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/launchnlp/MOKA">https://github.com/launchnlp/MOKA</a></li>
<li>paper_authors: Xinliang Frederick Zhang, Winston Wu, Nick Beauchamp, Lu Wang</li>
<li>For: This paper is written for studying the phenomenon of moral language in news media and the dynamics of moral events in shaping news content.* Methods: The paper uses a new dataset called MORAL EVENTS, which consists of 5,494 structured annotations on 474 news articles from diverse US media outlets. The authors also propose a moral event extraction framework called MOKA, which leverages knowledge derived from moral words and moral scenarios.* Results: The experimental results show that MOKA outperforms competitive baselines across three moral event understanding tasks. Additionally, the authors find that media outlets of different ideological leanings selectively report moral events, highlighting the significance of event-level morality analysis in news.<details>
<summary>Abstract</summary>
News media employ moral language to create memorable stories, and readers often engage with the content that align with their values. Moral theories have been applied to news analysis studying moral values in isolation, while the intricate dynamics among participating entities in shaping moral events have been overlooked. This is mainly due to the use of obscure language to conceal evident ideology and values, coupled with the insufficient moral reasoning capability in most existing NLP systems, where LLMs are no exception. To study this phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of 5,494 structured annotations on 474 news articles by diverse US media across the political spectrum. We further propose MOKA, a moral event extraction framework with MOral Knowledge Augmentation, that leverages knowledge derived from moral words and moral scenarios. Experimental results show that MOKA outperforms competitive baselines across three moral event understanding tasks. Further analyses illuminate the selective reporting of moral events by media outlets of different ideological leanings, suggesting the significance of event-level morality analysis in news. Our datasets and codebase are available at https://github.com/launchnlp/MOKA.
</details>
<details>
<summary>摘要</summary>
新闻媒体使用道德语言创造深刻的故事，读者常与其价值观合而参与内容。道德理论在新闻分析中被应用，但是参与者之间的复杂关系和形成道德事件的过程受到了忽略。这主要是因为使用晦涩的语言隐藏了明确的意识形态和价值观，同时现有的NLP系统中的道德理解能力不够，LLMs也不例外。为研究这一现象，我们首先创建了新的数据集，道德事件集（MORAL EVENTS），包含474篇来自美国各种政见媒体的新闻文章5,494个结构化注释。我们还提出了MOKA，一个基于道德知识增强的道德事件抽取框架，利用道德词汇和道德enario来抽取道德事件。实验结果显示，MOKA在三个道德事件理解任务上与竞争对手相比表现出色。进一步的分析发现媒体不同政见倾向的报道道德事件是有偏见的，这表明事件级别的道德分析在新闻中的重要性。我们的数据集和代码库可以在GitHub上找到：https://github.com/launchnlp/MOKA。
</details></li>
</ul>
<hr>
<h2 id="On-Evaluating-the-Integration-of-Reasoning-and-Action-in-LLM-Agents-with-Database-Question-Answering"><a href="#On-Evaluating-the-Integration-of-Reasoning-and-Action-in-LLM-Agents-with-Database-Question-Answering" class="headerlink" title="On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering"></a>On Evaluating the Integration of Reasoning and Action in LLM Agents with Database Question Answering</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09721">http://arxiv.org/abs/2311.09721</a></li>
<li>repo_url: None</li>
<li>paper_authors: Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Wenfei Zhou, Arman Cohan</li>
<li>for: 这项研究是为了评估大自然语言模型（LLM）在数据库问答任务中的表现，并研究 LLM 如何使用多个 SQL 查询来获取数据库中的数据，进行Contextual reasoning，并将其总结成一份完整的分析报告。</li>
<li>methods: 本研究使用了一种新的长形数据库问答数据集，并提出了两种互动策略来解决问题。我们还进行了细腻的分析，探讨了不同阶段的互动过程中的瓶颈。</li>
<li>results: 我们的研究发现，当前的State-of-the-art GPT-4模型在这个任务中存在两个主要的瓶颈：规划能力和多个 SQL 查询的生成能力。我们还引入了一种多代理评估框架，以便更准确地评估答案质量。这种框架允许我们更好地理解当前 LLM 在复杂的检索和推理任务中的优劣点。<details>
<summary>Abstract</summary>
This study introduces a new long-form database question answering dataset designed to evaluate how Large Language Models (LLMs) interact with a SQL interpreter. The task necessitates LLMs to strategically generate multiple SQL queries to retrieve sufficient data from a database, to reason with the acquired context, and to synthesize them into a comprehensive analytical narrative. Our findings highlight that this task poses great challenges even for the state-of-the-art GPT-4 model. We propose and evaluate two interaction strategies, and provide a fine-grained analysis of the individual stages within the interaction. A key discovery is the identification of two primary bottlenecks hindering effective interaction: the capacity for planning and the ability to generate multiple SQL queries. To address the challenge of accurately assessing answer quality, we introduce a multi-agent evaluation framework that simulates the academic peer-review process, enhancing the precision and reliability of our evaluations. This framework allows for a more nuanced understanding of the strengths and limitations of current LLMs in complex retrieval and reasoning tasks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Regularized-Conventions-Equilibrium-Computation-as-a-Model-of-Pragmatic-Reasoning"><a href="#Regularized-Conventions-Equilibrium-Computation-as-a-Model-of-Pragmatic-Reasoning" class="headerlink" title="Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning"></a>Regularized Conventions: Equilibrium Computation as a Model of Pragmatic Reasoning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09712">http://arxiv.org/abs/2311.09712</a></li>
<li>repo_url: None</li>
<li>paper_authors: Athul Paul Jacob, Gabriele Farina, Jacob Andreas</li>
<li>for: 这篇论文旨在描述一种语言理解模型，即通过搜索信号游戏的 equilibria来生成和理解语言表达。</li>
<li>methods: 该模型使用搜索 equilibria来模拟语言交流中的信号游戏，并通过定制化的搜索策略来找到最佳的语言表达。</li>
<li>results: 在使用该模型的实验中，论文能够匹配或超越现有的最佳回应和理性演讲模型的预测，并且可以提供有关语言交流中的通信成功和自然性的理论保证。<details>
<summary>Abstract</summary>
We present a model of pragmatic language understanding, where utterances are produced and understood by searching for regularized equilibria of signaling games. In this model (which we call ReCo, for Regularized Conventions), speakers and listeners search for contextually appropriate utterance--meaning mappings that are both close to game-theoretically optimal conventions and close to a shared, ''default'' semantics. By characterizing pragmatic communication as equilibrium search, we obtain principled sampling algorithms and formal guarantees about the trade-off between communicative success and naturalness. Across several datasets capturing real and idealized human judgments about pragmatic implicatures, ReCo matches or improves upon predictions made by best response and rational speech act models of language understanding.
</details>
<details>
<summary>摘要</summary>
我们提出了一种语言理解模型，其中讲话和理解都是通过搜索正则化平衡的信号游戏来实现的。我们称这种模型为ReCo（正则化会议）。在这个模型中，说话者和听众在语言上进行Contextually appropriate的讲话-意思映射搜索，以达到Game-theoretically optimal的会议和共同默认 semantics。通过将 Pragmatic communication 定义为平衡搜索，我们得到了原则性的抽样算法和Formal guarantees about the trade-off between communicative success and naturalness。在几个捕捉了真实和理想的人类评价的数据集上，ReCo匹配或超过了Best response和理性语言理解模型的预测。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Model-Inference-with-Lexical-Shortlisting"><a href="#Large-Language-Model-Inference-with-Lexical-Shortlisting" class="headerlink" title="Large Language Model Inference with Lexical Shortlisting"></a>Large Language Model Inference with Lexical Shortlisting</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09709">http://arxiv.org/abs/2311.09709</a></li>
<li>repo_url: None</li>
<li>paper_authors: Nikolay Bogoychev, Pinzhen Chen, Barry Haddow, Alexandra Birch</li>
<li>for: 本研究旨在提高大语言模型（LLM）的推理速度和计算资源使用效率，通过适应lexical shortlisting技术。</li>
<li>methods: 本研究使用Unicode字符集基于的脚本筛选和基于词库的选择方法来缩短子词库。</li>
<li>results: 研究发现，lexical shortlisting可以减少一些模型的内存使用量，最高可以减少50%，同时也有25%的提升的可能性。此外，研究还发现了这种词库选择方法的缺点，并提出了未来研究的可能性。<details>
<summary>Abstract</summary>
Large language model (LLM) inference is computation and memory intensive, so we adapt lexical shortlisting to it hoping to improve both. While lexical shortlisting is well-explored in tasks like machine translation, it requires modifications before being suitable for LLMs as the intended applications vary significantly. Our work studies two heuristics to shortlist sub-vocabulary at LLM inference time: Unicode-based script filtering and corpus-based selection. We explore different LLM families and sizes, and we find that lexical shortlisting can reduce the memory usage of some models by nearly 50\% and has an upper bound of 25\% improvement in generation speed. In this pilot study, we also identify the drawbacks of such vocabulary selection methods and propose avenues for future research.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）的推理是计算和内存密集的，因此我们适应lexical shortlisting以提高它们。lexical shortlisting在机器翻译任务中广泛探索过，但是需要修改才能适用于LLM，因为它们的应用场景差异很大。我们的工作研究了两种决策指标来短list sub-vocabulary during LLM inference time：Unicode-based script filtering和corpus-based selection。我们在不同的LLM家族和大小上进行了 исследование，发现lexical shortlisting可以将一些模型的内存使用量减少到 nearly 50%，并且有一个 Upper bound的25%的提高 Speed of generation。在这个 Pilot study中，我们还发现了这种词汇选择方法的缺点并提出了未来研究的可能性。
</details></li>
</ul>
<hr>
<h2 id="A-Self-enhancement-Multitask-Framework-for-Unsupervised-Aspect-Category-Detection"><a href="#A-Self-enhancement-Multitask-Framework-for-Unsupervised-Aspect-Category-Detection" class="headerlink" title="A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection"></a>A Self-enhancement Multitask Framework for Unsupervised Aspect Category Detection</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09708">http://arxiv.org/abs/2311.09708</a></li>
<li>repo_url: None</li>
<li>paper_authors: Thi-Nhung Nguyen, Hoang Ngo, Kiem-Hieu Nguyen, Tuan-Dung Cao</li>
<li>for:  addresses the problem of unsupervised Aspect Category Detection using a small set of seed words.</li>
<li>methods:  proposes a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training, and jointly trains Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity.</li>
<li>results:  surpasses strong baselines on standard datasets.<details>
<summary>Abstract</summary>
Our work addresses the problem of unsupervised Aspect Category Detection using a small set of seed words. Recent works have focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.
</details>
<details>
<summary>摘要</summary>
我们的工作解决了无监督方面类检测问题，使用一小组种子词。 latest works focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.Here's a word-for-word translation of the text in Traditional Chinese:我们的工作解决了无监督方面类检测问题，使用一小组种子词。 latest works focused on learning embedding spaces for seed words and sentences to establish similarities between sentences and aspects. However, aspect representations are limited by the quality of initial seed words, and model performances are compromised by noise. To mitigate this limitation, we propose a simple framework that automatically enhances the quality of initial seed words and selects high-quality sentences for training instead of using the entire dataset. Our main concepts are to add a number of seed words to the initial set and to treat the task of noise resolution as a task of augmenting data for a low-resource task. In addition, we jointly train Aspect Category Detection with Aspect Term Extraction and Aspect Term Polarity to further enhance performance. This approach facilitates shared representation learning, allowing Aspect Category Detection to benefit from the additional guidance offered by other tasks. Extensive experiments demonstrate that our framework surpasses strong baselines on standard datasets.
</details></li>
</ul>
<hr>
<h2 id="GenCodeSearchNet-A-Benchmark-Test-Suite-for-Evaluating-Generalization-in-Programming-Language-Understanding"><a href="#GenCodeSearchNet-A-Benchmark-Test-Suite-for-Evaluating-Generalization-in-Programming-Language-Understanding" class="headerlink" title="GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding"></a>GenCodeSearchNet: A Benchmark Test Suite for Evaluating Generalization in Programming Language Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09707">http://arxiv.org/abs/2311.09707</a></li>
<li>repo_url: None</li>
<li>paper_authors: Andor Diera, Abdelhalim Dahou, Lukas Galke, Fabian Karl, Florian Sihler, Ansgar Scherp</li>
<li>for: 提高软件开发Productivity</li>
<li>methods: 使用大型生成模型进行代码生成和代码完成，使用小型encoder-only模型进行自然语言查询代码搜索</li>
<li>results: 提出了一个新的benchmark dataset called GenCodeSearchNet (GeCS)，以评估语言模型对不同编程语言的理解能力，并引入了一个新的手动审核 subsets StatCodeSearch，Focus on R编程语言，以增强模型对不同编程语言的适应能力。<details>
<summary>Abstract</summary>
Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries.These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et.\,al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systemically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.
</details>
<details>
<summary>摘要</summary>
Language models can serve as a valuable tool for software developers to increase productivity. Large generative models can be used for code generation and code completion, while smaller encoder-only models are capable of performing code search tasks using natural language queries. These capabilities are heavily influenced by the quality and diversity of the available training data. Source code datasets used for training usually focus on the most popular languages and testing is mostly conducted on the same distributions, often overlooking low-resource programming languages. Motivated by the NLP generalization taxonomy proposed by Hupkes et al., we propose a new benchmark dataset called GenCodeSearchNet (GeCS) which builds upon existing natural language code search datasets to systematically evaluate the programming language understanding generalization capabilities of language models. As part of the full dataset, we introduce a new, manually curated subset StatCodeSearch that focuses on R, a popular but so far underrepresented programming language that is often used by researchers outside the field of computer science. For evaluation and comparison, we collect several baseline results using fine-tuned BERT-style models and GPT-style large language models in a zero-shot setting.Here's the translation in Traditional Chinese:语言模型可以serve as a valuable tool for software developers to increase productivity。大型生成模型可以用于代码生成和代码完成，而小型encoder-only模型则可以进行代码搜寻任务使用自然语言查询。这些能力受到训练数据的质量和多样性的影响。通常的源代码资料集用于训练通常会针对最受欢迎的语言进行集中，而测试通常会在同一个分布上进行，往往忽略低资源的编程语言。驱动了Hupkes等人提出的NLG概念分类，我们提议一个新的benchmarkdatasetcalled GenCodeSearchNet (GeCS)，这个dataset建立在现有的自然语言代码搜寻dataset之上，以系统地评估语言模型对程式语言理解的扩展能力。这个dataset中，我们引入了一个新的、手动精心筛选的子集StatCodeSearch，它针对R语言，这是一个受欢迎但现在尚未得到充分关注的编程语言，经常被computer科学以外的研究人员使用。为了评估和比较，我们收集了一些基准结果使用精心翻译BERT类型模型和GPT类型大型语言模型，这些模型在零条件设定下进行比较。
</details></li>
</ul>
<hr>
<h2 id="Fumbling-in-Babel-An-Investigation-into-ChatGPT’s-Language-Identification-Ability"><a href="#Fumbling-in-Babel-An-Investigation-into-ChatGPT’s-Language-Identification-Ability" class="headerlink" title="Fumbling in Babel: An Investigation into ChatGPT’s Language Identification Ability"></a>Fumbling in Babel: An Investigation into ChatGPT’s Language Identification Ability</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09696">http://arxiv.org/abs/2311.09696</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wei-Rui Chen, Ife Adebara, Khai Duy Doan, Qisheng Liao, Muhammad Abdul-Mageed</li>
<li>for:  investigate ChatGPT’s language identification abilities</li>
<li>methods:  compile Babel-670 benchmark, study ChatGPT’s ability to identify language names and language codes under zero- and few-shot conditions with and without label set</li>
<li>results:  ChatGPT lags behind smaller finetuned language identification tools, indicating potential for enhancement before serving diverse communities.Here is the same information in Traditional Chinese text:</li>
<li>for: 探访ChatGPT的语言识别能力</li>
<li>methods: 编译Babel-670 benchmark，研究ChatGPT在零条件和几条件下进行语言名称和语言代码识别</li>
<li>results: ChatGPT落后于小型训练语言识别工具，显示需要进一步改进以应对多元社区。<details>
<summary>Abstract</summary>
Recently, ChatGPT has emerged as a powerful NLP tool that can carry out several tasks. However, the range of languages ChatGPT can handle remains largely a mystery. In this work, we investigate ChatGPT's language identification abilities. For this purpose, we compile Babel-670, a benchmark comprising $670$ languages representing $23$ language families. Languages in Babel-670 run the gamut between the very high-resource to the very low-resource and are spoken in five continents. We then study ChatGPT's (both GPT-3.5 and GPT-4) ability to (i) identify both language names and language codes (ii) under both zero- and few-shot conditions (iii) with and without provision of label set. When compared to smaller finetuned language identification tools, we find that ChatGPT lags behind. Our empirical analysis shows the reality that ChatGPT still resides in a state of potential enhancement before it can sufficiently serve diverse communities.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Whispers-of-Doubt-Amidst-Echoes-of-Triumph-in-NLP-Robustness"><a href="#Whispers-of-Doubt-Amidst-Echoes-of-Triumph-in-NLP-Robustness" class="headerlink" title="Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness"></a>Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09694">http://arxiv.org/abs/2311.09694</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ashim Gupta, Rishanth Rajendhran, Nathan Stringham, Vivek Srikumar, Ana Marasović</li>
<li>for: 本研究是为了解决NLPToday的大型和高性能模型是否已经解决了长期稳定性问题。</li>
<li>methods: 作者使用了19种不同的模型，包括不同的架构选择和预训练目标。他们使用了OOD和挑战测试集、CheckLists、对比集和抗敌输入来进行评估。</li>
<li>results: 研究发现，不是所有的OOD测试都能够提供更深入的稳定性评估。使用CheckLists和对比集的评估显示了模型的性能差距，并且尚未充分提高模型的稳定性。此外，作者还指出了当前对模型 robustness的评估方法存在问题，这些方法可以被轻松地骗过，并且当前的评估方法不够深入。因此，作者 conclude that NLP中的稳定性问题仍未得到解决，甚至一些用于评估稳定性的方法需要重新评估。<details>
<summary>Abstract</summary>
Are the longstanding robustness issues in NLP resolved by today's larger and more performant models? To address this question, we conduct a thorough investigation using 19 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) OOD and challenge test sets, (b) CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all OOD tests provide further insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them sufficiently robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.
</details>
<details>
<summary>摘要</summary>
是否已经解决了自然语言处理（NLP）领域的长期稳定性问题？为了回答这个问题，我们进行了19种不同大小和架构的模型的完整调查。我们使用了（a）跨模型测试集（Out-of-distribution，OOD）和挑战测试集，（b）CheckLists，（c）对比集和（d）敌意输入来进行评估。我们的分析发现，不是所有的OOD测试都能够提供更多的 robustness 信息。使用 CheckLists 和对比集的评估显示了模型的显著性能差异；即便模型的大小增加，也不能 garantuee 其 sufficient robustness。最后，我们指出了当前对模型 adversarial 评估的方法存在问题：它们可以轻松地被阻断，并且在当前的形式下，不能深入探索模型的 robustness。我们结论是，NLP 领域的 robustness 问题仍未得到解决，而且一些用于评估 robustness 的方法也需要重新评估。
</details></li>
</ul>
<hr>
<h2 id="Inducing-Political-Bias-Allows-Language-Models-Anticipate-Partisan-Reactions-to-Controversies"><a href="#Inducing-Political-Bias-Allows-Language-Models-Anticipate-Partisan-Reactions-to-Controversies" class="headerlink" title="Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies"></a>Inducing Political Bias Allows Language Models Anticipate Partisan Reactions to Controversies</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09687">http://arxiv.org/abs/2311.09687</a></li>
<li>repo_url: None</li>
<li>paper_authors: Zihao He, Siyi Guo, Ashwin Rao, Kristina Lerman</li>
<li>for: 本研究旨在使用大型自然语言模型（LLM）更好地理解政治偏见在数字化对话中。</li>
<li>methods: 本研究采用了一种新的办法，即使用一个单一的指令驱动的LLM来反映政治 идеологи的范围。</li>
<li>results: 研究发现模型能够准确地捕捉到情感和道德上的细节，但在姿势检测方面存在一些挑战。<details>
<summary>Abstract</summary>
Social media platforms are rife with politically charged discussions. Therefore, accurately deciphering and predicting partisan biases using Large Language Models (LLMs) is increasingly critical. In this study, we address the challenge of understanding political bias in digitized discourse using LLMs. While traditional approaches often rely on finetuning separate models for each political faction, our work innovates by employing a singular, instruction-tuned LLM to reflect a spectrum of political ideologies. We present a comprehensive analytical framework, consisting of Partisan Bias Divergence Assessment and Partisan Class Tendency Prediction, to evaluate the model's alignment with real-world political ideologies in terms of stances, emotions, and moral foundations. Our findings reveal the model's effectiveness in capturing emotional and moral nuances, albeit with some challenges in stance detection, highlighting the intricacies and potential for refinement in NLP tools for politically sensitive contexts. This research contributes significantly to the field by demonstrating the feasibility and importance of nuanced political understanding in LLMs, particularly for applications requiring acute awareness of political bias.
</details>
<details>
<summary>摘要</summary>
社交媒体平台上的政治话题非常普遍，因此正确地理解和预测政治偏见使用大型自然语言模型（LLM）变得越来越重要。在这项研究中，我们解决了政治偏见在数字化言语中的理解挑战，使用一个单一、指导 instru 的 LLM，以反映政治意识形态的谱系。我们提出了一个完整的分析框架，包括政治偏见分化评估和政治类倾向预测，以评估模型与实际世界政治意识形态之间的吻合程度。我们的发现表明模型能够很好地捕捉情感和道德上的细 nuances，但在姿势检测方面存在一些挑战，这 highlights NL 工具在政治敏感上的复杂性和可能的改进。本研究对于场景中的政治偏见理解的重要性和可行性作出了重要贡献，特别是在需要精准的政治偏见认知应用场景下。
</details></li>
</ul>
<hr>
<h2 id="R-Tuning-Teaching-Large-Language-Models-to-Refuse-Unknown-Questions"><a href="#R-Tuning-Teaching-Large-Language-Models-to-Refuse-Unknown-Questions" class="headerlink" title="R-Tuning: Teaching Large Language Models to Refuse Unknown Questions"></a>R-Tuning: Teaching Large Language Models to Refuse Unknown Questions</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09677">http://arxiv.org/abs/2311.09677</a></li>
<li>repo_url: None</li>
<li>paper_authors: Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang</li>
<li>for: 本研究旨在改进语言模型（LLM）的问答能力，特别是避免模型生成非存在的信息（hallucination）。</li>
<li>methods: 我们提出了一种新的approach，即Refusal-Aware Instruction Tuning（R-Tuning），通过初步确定知识差距，然后使用知识交叉构建拒绝意识数据，以便训练LLMs可以回答知道的问题而不回答未知的问题。</li>
<li>results: 实验结果表明，R-Tuning方法可以有效地提高模型回答知道问题的能力，同时避免回答未知问题。此外，在域外数据集上进行测试，发现模型学习不确定性的能力可以通过训练来提高。<details>
<summary>Abstract</summary>
Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the knowledge gap between parametric knowledge and the instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing. Our code will be released at https://github.com/shizhediao/R-Tuning.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）已经革命化了许多领域，但仍面临一些挑战。一个主要问题是这些模型的倾向于生成不存在的事实，被称为幻觉。我们的研究受到了以前的 instrucion 级别调整方法会让模型完成一个句子，无论模型知道这些知识还是不知道。当问题出现在模型的参数知识之外时，它会尝试 fabricate 一个答案，并且无法指示当前缺乏知识。在这篇论文中，我们提出了一种新的方法，即 Refusal-Aware Instruction Tuning（R-Tuning）。这种方法由首先标识模型的参数知识和 instrucion 调整数据之间的知识差异而始。然后，我们将基于知识交叉的 refusal-aware 数据进行调整，以使模型不再回答 beyond 其参数知识的问题。实验结果表明，这种新的 instrucion 调整方法可以有效地提高模型回答知道的问题能力，并且不再回答不知道的问题。此外，当测试在域外数据集时，发现了一个叫做 "拒绝能力" 的元技能，可以在其他任务上generalize。进一步的分析显示，在训练时学习不确定性实际上比测试时 uncertainty-based 测试时更好地估计不确定性。我们的代码将在 GitHub 上发布，请参考 <https://github.com/shizhediao/R-Tuning>。
</details></li>
</ul>
<hr>
<h2 id="Where-Do-People-Tell-Stories-Online-Story-Detection-Across-Online-Communities"><a href="#Where-Do-People-Tell-Stories-Online-Story-Detection-Across-Online-Communities" class="headerlink" title="Where Do People Tell Stories Online? Story Detection Across Online Communities"></a>Where Do People Tell Stories Online? Story Detection Across Online Communities</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09675">http://arxiv.org/abs/2311.09675</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/maria-antoniak/stories-online-communities">https://github.com/maria-antoniak/stories-online-communities</a></li>
<li>paper_authors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper</li>
<li>for: 这篇论文是为了研究在线社区中的故事tellding，以便更好地理解社会运动、意识形态、诱导策略等的动态。</li>
<li>methods: 这篇论文使用了一份codebook和Storytelling in Online Communities Corpus，一个专家标注的数据集，以及在线故事检测模型的训练和评估，来研究在线故事tellding的特点和社会上的应用。</li>
<li>results: 根据这篇论文的研究结果，在线故事tellding的特点包括：不同社区中的故事tellding频率不同，各种社区中的故事tellding具有共同的特征，以及在线故事tellding可以跨越不同话题和场景进行交互。<details>
<summary>Abstract</summary>
People share stories online for a myriad of purposes, whether as a means of self-disclosure, processing difficult personal experiences, providing needed information or entertainment, or persuading others to share their beliefs. Better understanding of online storytelling can illuminate the dynamics of social movements, sensemaking practices, persuasion strategies, and more. However, unlike other media such as books and visual content where the narrative nature of the content is often overtly signaled at the document level, studying storytelling in online communities is challenging due to the mixture of storytelling and non-storytelling behavior, which can be interspersed within documents and across diverse topics and settings. We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role storytelling of in different social contexts. We identify distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.
</details>
<details>
<summary>摘要</summary>
To address this challenge, we introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role of storytelling in different social contexts. Our findings reveal distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.
</details></li>
</ul>
<hr>
<h2 id="Improving-the-Generation-Quality-of-Watermarked-Large-Language-Models-via-Word-Importance-Scoring"><a href="#Improving-the-Generation-Quality-of-Watermarked-Large-Language-Models-via-Word-Importance-Scoring" class="headerlink" title="Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring"></a>Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09668">http://arxiv.org/abs/2311.09668</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yuhang Li, Yihan Wang, Zhouxing Shi, Cho-Jui Hsieh</li>
<li>for: 防止大语言模型（LLMs）的不良用户访问。</li>
<li>methods: 使用Token-level watermarking技术，并提出了三种方法来预测重要性分数。</li>
<li>results: 实验结果表明，我们的方法可以生成高质量的文本，同时保持了相同的检测率。<details>
<summary>Abstract</summary>
The strong general capabilities of Large Language Models (LLMs) bring potential ethical risks if they are unrestrictedly accessible to malicious users. Token-level watermarking inserts watermarks in the generated texts by altering the token probability distributions with a private random number generator seeded by its prefix tokens. However, this watermarking algorithm alters the logits during generation, which can lead to a downgraded text quality if it chooses to promote tokens that are less relevant given the input. In this work, we propose to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS). At each generation step, we estimate the importance of the token to generate, and prevent it from being impacted by watermarking if it is important for the semantic correctness of the output. We further propose three methods to predict importance scoring, including a perturbation-based method and two model-based methods. Empirical experiments show that our method can generate texts with better quality with comparable level of detection rate.
</details>
<details>
<summary>摘要</summary>
强大的普通语言模型（LLM）具有潜在的道德风险，如果这些模型在黑客用户手中不受限制。Token-level watermarking在生成文本时插入水印，通过修改token概率分布来增加一个私有随机数生成器。然而，这种水印算法在生成过程中改变了logits，可能会导致生成的文本质量下降，如果它选择推荐不太相关的token。在这种情况下，我们提出了通过水印 scoring（WIS）来改进生成的文本质量。在每个生成步骤中，我们估算token需要生成的重要性，并在生成过程中避免由水印所影响的重要token。我们还提出了三种方法来预测重要性分配，包括一种干扰基本方法和两种模型基本方法。实验表明，我们的方法可以生成文本质量更高，同时保持检测率在相同水平。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-LLM-Agent-Group-Dynamics-against-Human-Group-Dynamics-A-Case-Study-on-Wisdom-of-Partisan-Crowds"><a href="#Evaluating-LLM-Agent-Group-Dynamics-against-Human-Group-Dynamics-A-Case-Study-on-Wisdom-of-Partisan-Crowds" class="headerlink" title="Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds"></a>Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09665">http://arxiv.org/abs/2311.09665</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Siddharth Suresh, Nikunj Harlalka, Agam Goyal, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers</li>
<li>for: 本研究探讨了大自然语言模型（LLM）是否可以模拟人类群体动态，特别在政治敏感背景下。</li>
<li>methods: 我们使用LLM agents扮演为民主和共和两个政治人物，在一种类似于人类群体研究的结构化互动中进行了模拟。我们的方法评估了代理者响应如何随社会影响而发展。</li>
<li>results: 我们发现，不含链条思维（CoT）的LLM代理者具有与人类行为高度相似的Alignment，而含CoT的代理者则受到了Alignment的降低。此外，在人类数据进行精细调整LLM代理者后，可以实现人类样式的行为，但也存在过拟合特定行为的风险。这些发现表明了LLM代理者在模型人类群体现象方面的潜力和局限性。<details>
<summary>Abstract</summary>
This study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents' responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.
</details>
<details>
<summary>摘要</summary>
Translated into Simplified Chinese:这个研究 investigate LLM（大语言模型）能模拟人类群体动态，尤其在政治敏感的背景下。我们使用LLM代理人物，模拟人类群体中的决策过程，并评估代理人物如何受社会影响。我们的关键发现表明，没有Chain-of-Thought（CoT）解释的LLM代理人物和人类行为高度相似，而CoT解释会降低对应性。然而，向代理人物添加明确的偏见不一定提高群体智慧。此外，使用人类数据进行LLM fine-tuning显示 promise in achieving human-like behavior，但也存在适应特定行为的风险。这些发现表明LLM代理人物在模拟人类群体现象的潜在和局限性。
</details></li>
</ul>
<hr>
<h2 id="Evolving-Domain-Adaptation-of-Pretrained-Language-Models-for-Text-Classification"><a href="#Evolving-Domain-Adaptation-of-Pretrained-Language-Models-for-Text-Classification" class="headerlink" title="Evolving Domain Adaptation of Pretrained Language Models for Text Classification"></a>Evolving Domain Adaptation of Pretrained Language Models for Text Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09661">http://arxiv.org/abs/2311.09661</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Yi Wu, Dhruv Gupta, Rheeya Uppaal, Ananya Kumar, Luhang Sun, Makesh Narsimhan Sreedhar, Sijia Yang, Timothy T. Rogers, Junjie Hu</li>
<li>for: 维护语言模型的精度在应用中，特别是在观点探测中。</li>
<li>methods: 本研究探讨了对于语言模型进行不断更新的方法，以适应语言模型在不断变化的语言环境中的应用。</li>
<li>results: 研究发现，对于语言模型进行自我训练方法能够优化语言模型在不断变化的语言环境中的性能，并且比传统的领域适应技术高效。<details>
<summary>Abstract</summary>
Adapting pre-trained language models (PLMs) for time-series text classification amidst evolving domain shifts (EDS) is critical for maintaining accuracy in applications like stance detection. This study benchmarks the effectiveness of evolving domain adaptation (EDA) strategies, notably self-training, domain-adversarial training, and domain-adaptive pretraining, with a focus on an incremental self-training method. Our analysis across various datasets reveals that this incremental method excels at adapting PLMs to EDS, outperforming traditional domain adaptation techniques. These findings highlight the importance of continually updating PLMs to ensure their effectiveness in real-world applications, paving the way for future research into PLM robustness against the natural temporal evolution of language.
</details>
<details>
<summary>摘要</summary>
这篇研究评估了对于时间序列文本分类中的预训语言模型（PLM）进行演进领域变化（EDS）的适用性，以维持准确性。研究对于自我训练、领域抗战术和领域适应训练等不同的演进领域整合策略进行比较，并着重于增量自我训练方法。我们的分析发现，这种增量方法在处理EDS时表现出色，超过传统领域整合策略。这些发现显示了预训语言模型的更新和调整的重要性，以确保它们在实际应用中的效能。这将推动未来关于PLM的研究，以探索它们对自然时间演进的语言的Robustness。
</details></li>
</ul>
<hr>
<h2 id="ICXML-An-In-Context-Learning-Framework-for-Zero-Shot-Extreme-Multi-Label-Classification"><a href="#ICXML-An-In-Context-Learning-Framework-for-Zero-Shot-Extreme-Multi-Label-Classification" class="headerlink" title="ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification"></a>ICXML: An In-Context Learning Framework for Zero-Shot Extreme Multi-Label Classification</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09649">http://arxiv.org/abs/2311.09649</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/yaxinzhuars/icxml">https://github.com/yaxinzhuars/icxml</a></li>
<li>paper_authors: Yaxin Zhu, Hamed Zamani</li>
<li>for: 这篇论文主要针对EXTREME多标签分类任务（XMC），目标是为每个实例预测多个标签，从极其大的标签空间中预测。</li>
<li>methods: 该论文提出了一种两阶段方法，称为In-Context Extreme Multilabel Learning（ICXML），通过在上下文学习中生成候选标签并进行重新排序，以降低搜索空间。</li>
<li>results: 对于两个公共评分平台，实验结果表明ICXML已经提高了状态之arte。<details>
<summary>Abstract</summary>
This paper focuses on the task of Extreme Multi-Label Classification (XMC) whose goal is to predict multiple labels for each instance from an extremely large label space. While existing research has primarily focused on fully supervised XMC, real-world scenarios often lack complete supervision signals, highlighting the importance of zero-shot settings. Given the large label space, utilizing in-context learning approaches is not trivial. We address this issue by introducing In-Context Extreme Multilabel Learning (ICXML), a two-stage framework that cuts down the search space by generating a set of candidate labels through incontext learning and then reranks them. Extensive experiments suggest that ICXML advances the state of the art on two diverse public benchmarks.
</details>
<details>
<summary>摘要</summary></li>
</ul>
</details>


<hr>
<h2 id="Event-Causality-Is-Key-to-Computational-Story-Understanding"><a href="#Event-Causality-Is-Key-to-Computational-Story-Understanding" class="headerlink" title="Event Causality Is Key to Computational Story Understanding"></a>Event Causality Is Key to Computational Story Understanding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09648">http://arxiv.org/abs/2311.09648</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yidan Sun, Qin Chao, Boyang Li</li>
<li>for: 本研究旨在探讨人类故事理解中事件 causality 的中心作用，以及如何利用这些事件 causality 进行符号故事生成。</li>
<li>methods: 我们采用了最新的大语言模型 (LLMs) 来开发一种用于事件 causality 识别的方法，并通过设计特定的提示来提取 GPT 中的事件 causal 关系。</li>
<li>results: 我们的方法在比较 human-annotated 的事件 causal 关系集合 GLUCOSE 中表现出类似的水平，同时能够轻松地扩展到不同类型和长度的故事。这些EXTRACTED causal 关系导致了对故事质量评价的提高（5.7%）和对故事视频文本对应性的提高（8.7%）。<details>
<summary>Abstract</summary>
Psychological research suggests the central role of event causality in human story understanding. Further, event causality has been heavily utilized in symbolic story generation. However, few machine learning systems for story understanding employ event causality, partially due to the lack of reliable methods for identifying open-world causal event relations. Leveraging recent progress in large language models (LLMs), we present the first method for event causality identification that leads to material improvements in computational story understanding. We design specific prompts for extracting event causal relations from GPT. Against human-annotated event causal relations in the GLUCOSE dataset, our technique performs on par with supervised models, while being easily generalizable to stories of different types and lengths. The extracted causal relations lead to 5.7\% improvements on story quality evaluation and 8.7\% on story video-text alignment. Our findings indicate enormous untapped potential for event causality in computational story understanding.
</details>
<details>
<summary>摘要</summary>
心理研究表明人类故事理解中心stage causality的重要性。此外，event causality在Symbolic story generation中得到了广泛使用。然而，现代机器学习系统 для故事理解 rarely employ event causality，部分原因是没有可靠的方法来确定开放世界的 causal event relations。基于最近的大语言模型（LLMs），我们提出了首个事件 causality identification的方法，该方法在计算机故事理解中产生了Material improvements。我们为GPT设计了特定的提示，以EXTRACT event causal relations。与人类标注的事件 causal relations在GLUCOSE数据集中，我们的技术与超级vised模型相当，而且可以轻松扩展到不同的类型和长度的故事。提取的 causal relations导致了5.7%的故事质量评估提高和8.7%的故事视频-文本对齐提高。我们的发现表明事件 causality在计算机故事理解中存在巨大的untapped potential。
</details></li>
</ul>
<hr>
<h2 id="Evaluating-In-Context-Learning-of-Libraries-for-Code-Generation"><a href="#Evaluating-In-Context-Learning-of-Libraries-for-Code-Generation" class="headerlink" title="Evaluating In-Context Learning of Libraries for Code Generation"></a>Evaluating In-Context Learning of Libraries for Code Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09635">http://arxiv.org/abs/2311.09635</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/jettbrains/-L-">https://github.com/jettbrains/-L-</a></li>
<li>paper_authors: Arkil Patel, Siva Reddy, Dzmitry Bahdanau, Pradeep Dasigi</li>
<li>for: 本研究旨在系统地评估不同领域特化的大型自然语言模型（LLMs）在基于受欢迎库模块的代码生成中的能力和限制。</li>
<li>methods: 本研究使用了多种场景，反映不同领域的特化，以评估不同大型LLMs在基于受欢迎库模块的代码生成中的能力和限制。</li>
<li>results: 研究结果显示，即使使用小型开源LLMs如Llama-2和StarCoder，也能够很好地理解新的代码库模块，基于受欢迎库模块的 спецификации进行受欢迎库模块的代码生成。此外，研究还发现，LLMs可以通过自然语言描述或 raw code 实现来学习新的库模块，这些资源通常比示例更加便宜。总之，本研究的结果铺平了在更多的适应和动态编程环境中使用LLMs的道路。<details>
<summary>Abstract</summary>
Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability. A particularly promising area is their ability to interpret code modules from unfamiliar libraries for solving user-instructed tasks. Recent work has shown that large proprietary LLMs can learn novel library usage in-context from demonstrations. These results raise several open questions: whether demonstrations of library usage is required, whether smaller (and more open) models also possess such capabilities, etc. In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context. Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context. Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions, which are often cheaper to obtain than demonstrations. Overall, our results pave the way for harnessing LLMs in more adaptable and dynamic coding environments.
</details>
<details>
<summary>摘要</summary>
现代大型语言模型（LLM）表现出了高度的代码生成和理解能力。特别是在解决用户指令下的代码模块解释方面表现出了极高的能力。最近的研究表明，大型专有LLM可以通过示例学习新的库使用。这些结果提出了多个开放问题：是否需要示例学习，小型（更开放）模型也具备这种能力等。在这项工作中，我们采取了更广泛的方法，系统地评估了多种LLM在不同领域专业化的三个场景中代码生成能力。我们的结果显示，即使使用小型开源LLM like Llama-2和StarCoder，也能够很好地理解新的代码库，基于场景中提供的规范进行解释。我们的发现还表明，LLM在只有自然语言描述或Raw code实现函数时仍然能够学习新的库模块，这些函数经常比示例更容易获得。总的来说，我们的结果为使用LLM在更适应和动态编程环境中做出了重要贡献。
</details></li>
</ul>
<hr>
<h2 id="From-Scroll-to-Misbelief-Modeling-the-Unobservable-Susceptibility-to-Misinformation-on-Social-Media"><a href="#From-Scroll-to-Misbelief-Modeling-the-Unobservable-Susceptibility-to-Misinformation-on-Social-Media" class="headerlink" title="From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media"></a>From Scroll to Misbelief: Modeling the Unobservable Susceptibility to Misinformation on Social Media</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09630">http://arxiv.org/abs/2311.09630</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanchen Liu, Mingyu Derek Ma, Wenna Qin, Azure Zhou, Jiaao Chen, Weiyan Shi, Wei Wang, Diyi Yang</li>
<li>for: 这个研究的目的是提出一种计算模型，以推测用户受到谣言的程度。</li>
<li>methods: 该模型基于用户的活动记录，利用 observable sharing behavior 进行监督，以推算用户的受到谣言程度。</li>
<li>results: 评估表示，该模型的估计与人类判断相吻合度很高。此外，该研究还发现了不同社会因素对受到谣言程度的相关性。<details>
<summary>Abstract</summary>
Susceptibility to misinformation describes the extent to believe unverifiable claims, which is hidden in people's mental process and infeasible to observe. Existing susceptibility studies heavily rely on the self-reported beliefs, making any downstream applications on susceptability hard to scale. To address these limitations, in this work, we propose a computational model to infer users' susceptibility levels given their activities. Since user's susceptibility is a key indicator for their reposting behavior, we utilize the supervision from the observable sharing behavior to infer the underlying susceptibility tendency. The evaluation shows that our model yields estimations that are highly aligned with human judgment on users' susceptibility level comparisons. Building upon such large-scale susceptibility labeling, we further conduct a comprehensive analysis of how different social factors relate to susceptibility. We find that political leanings and psychological factors are associated with susceptibility in varying degrees.
</details>
<details>
<summary>摘要</summary>
人们的信息受感染度描述了他们信任未经验证的说法的程度，这个程度隐藏在人们的思维过程中，无法直接观察。现有的受感染性研究主要基于自我报告的信念，这使得下游应用困难扩大。为解决这些限制，在这项工作中，我们提出了一种计算模型，用于根据用户的活动来推断他们的受感染性水平。由于用户的受感染性是共享行为的关键指标，我们利用共享行为的监督来推断受感染性的倾向。我们的评估结果显示，我们的模型可以提供与人类判断高度一致的用户受感染性水平的估计。基于大规模的受感染性标签，我们进一步进行了社会因素如政治倾向和心理因素与受感染性之间的全面分析。我们发现，政治倾向和心理因素在不同程度上与受感染性相关。
</details></li>
</ul>
<hr>
<h2 id="Take-One-Step-at-a-Time-to-Know-Incremental-Utility-of-Demonstration-An-Analysis-on-Reranking-for-Few-Shot-In-Context-Learning"><a href="#Take-One-Step-at-a-Time-to-Know-Incremental-Utility-of-Demonstration-An-Analysis-on-Reranking-for-Few-Shot-In-Context-Learning" class="headerlink" title="Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning"></a>Take One Step at a Time to Know Incremental Utility of Demonstration: An Analysis on Reranking for Few-Shot In-Context Learning</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09619">http://arxiv.org/abs/2311.09619</a></li>
<li>repo_url: None</li>
<li>paper_authors: Kazuma Hashimoto, Karthik Raman, Michael Bendersky</li>
<li>for: 本研究旨在分析不同标签策略对目标任务的影响。</li>
<li>methods: 本研究使用了LLMs的输出概率和任务特定的奖励来评估不同的标策略。</li>
<li>results: 研究发现，当输出概率分布在整个值范围内时，概率是有效的（在分类任务上），而在 segmentation 和翻译任务上，提供细化的奖励值和长输出可以使下游指标更加稳定。此外，提出了一种新的标策方法——增量有用性，可以评估LLMs中带入的新知识增量。<details>
<summary>Abstract</summary>
In-Context Learning (ICL) is an emergent capability of Large Language Models (LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new tasks. Previous studies have shown that using LLMs' outputs as labels is effective in training models to select demonstrations. Such a label is expected to estimate utility of a demonstration in ICL; however, it has not been well understood how different labeling strategies affect results on target tasks. This paper presents an analysis on different utility functions by focusing on LLMs' output probability given ground-truth output, and task-specific reward given LLMs' prediction. Unlike the previous work, we introduce a novel labeling method, incremental utility, which estimates how much incremental knowledge is brought into the LLMs by a demonstration. We conduct experiments with instruction-tuned LLMs on binary/multi-class classification, segmentation, and translation across Arabic, English, Finnish, Japanese, and Spanish. Our results show that (1) the probability is effective when the probability values are distributed across the whole value range (on the classification tasks), and (2) the downstream metric is more robust when nuanced reward values are provided with long outputs (on the segmentation and translation tasks). We then show that the proposed incremental utility further helps ICL by contrasting how the LLMs perform with and without the demonstrations.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLMs）的嵌入式学习（ICL）是一种出现的能力。只需要几个示例，LLMs 就可以作为黑obox для新任务使用。先前的研究表明，使用 LLMs 的输出作为标签可以有效地培训模型选择示例。这个标签预期能够估算示例在 ICL 中的用于性能。然而，不同的标签策略对目标任务的影响还未得到很好的理解。这篇论文分析了不同的用于性能的标签策略，并对 LLMs 的输出概率和任务特定的奖励给出了分析。与先前的工作不同，我们提出了一种新的标签方法，即增量用处，可以评估示例带来 LLMS 中的增量知识。我们在使用 instruction-tuned LLMs 进行了 binary/多类分类、分割和翻译任务，并在阿拉伯语、英语、芬兰语、日语和西班牙语等语言上进行了实验。我们的结果表明：1）在分类任务中，当概率值分布在整个值范围内时，概率效果非常高；2）在分割和翻译任务中，提供细化的奖励值和长输出可以使下游指标更加稳定。然后，我们表明了我们提出的增量用处可以进一步帮助 ICL。
</details></li>
</ul>
<hr>
<h2 id="Simulating-Opinion-Dynamics-with-Networks-of-LLM-based-Agents"><a href="#Simulating-Opinion-Dynamics-with-Networks-of-LLM-based-Agents" class="headerlink" title="Simulating Opinion Dynamics with Networks of LLM-based Agents"></a>Simulating Opinion Dynamics with Networks of LLM-based Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09618">http://arxiv.org/abs/2311.09618</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers</li>
<li>for: 这篇论文旨在 simulating human opinion dynamics 以及 understanding societal phenomena, such as polarization and the spread of misinformation.</li>
<li>methods: 本文使用 Large Language Models (LLMs) 来模拟意见动态, 并通过 prompt engineering 来导致confirmation bias.</li>
<li>results: 研究发现 LLM agents 具有强烈的倾向 towards accurate information, leading to consensus in line with scientific reality. However, this bias limits the simulation of individuals with resistant views on issues like climate change, leading to opinion fragmentation.<details>
<summary>Abstract</summary>
Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations lack fidelity to human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards accurate information, leading to consensus in line with scientific reality. However, this bias limits the simulation of individuals with resistant views on issues like climate change. After inducing confirmation bias through prompt engineering, we observed opinion fragmentation in line with existing agent-based research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.
</details>
<details>
<summary>摘要</summary>
准确模拟人类意见动态对社会现象的理解具有重要意义，包括分化和信息的快速传播。然而，常用的Agent-based模型（ABM）在模拟人类行为方面缺乏准确性。我们提出一种基于大语言模型（LLM）的新方法来模拟意见动态。我们的发现表明LLM代理具有准确信息的强烈偏好，导致与科学实际相符的共识。然而，这种偏好限制了对抵抗看法的个体模拟，如气候变化。通过引入确认偏见通过提示工程，我们观察到意见分化与现有的ABM研究相符。这些发现表明LLM代理在这个领域的承诺和局限性，并建议通过与现实世界的对话来更好地模拟人类信念的演化。
</details></li>
</ul>
<hr>
<h2 id="On-Retrieval-Augmentation-and-the-Limitations-of-Language-Model-Training"><a href="#On-Retrieval-Augmentation-and-the-Limitations-of-Language-Model-Training" class="headerlink" title="On Retrieval Augmentation and the Limitations of Language Model Training"></a>On Retrieval Augmentation and the Limitations of Language Model Training</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09615">http://arxiv.org/abs/2311.09615</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ting-Rui Chiang, Xinyan Velocity Yu, Joshua Robinson, Ollie Liu, Isabelle Lee, Dani Yogatama</li>
<li>for: 这个论文的目的是探讨一种语言模型（LM）的改进方法，即通过k-最近邻（kNN） Retrieval来降低LM的复杂度。</li>
<li>methods: 该论文使用了一些新的数据集和分析方法来研究LM的各种特性和表现。其中包括了“软max瓶颈”的排除和“多层perceptron（MLP）障碍现象”的发现。</li>
<li>results: 研究发现，通过将kNN Retrieval incorporated into vanilla GPT-2 117M可以有效地提高LM的性能，特别是在针对不相关的训练数据进行探索和泛化时。<details>
<summary>Abstract</summary>
Augmenting a language model (LM) with $k$-nearest neighbors (kNN) retrieval on its training data alone can decrease its perplexity, though the underlying reasons for this remains elusive. In this work, we first rule out one previously posited possibility -- the "softmax bottleneck." We further identify the MLP hurdle phenomenon, where the final MLP layer in LMs may impede LM optimization early on. We explore memorization and generalization in language models with two new datasets, where advanced model like GPT-3.5-turbo find generalizing to irrelevant information in the training data challenging. However, incorporating kNN retrieval to vanilla GPT-2 117M can consistently improve performance in this setting.
</details>
<details>
<summary>摘要</summary>
Language model (LM) 可以通过 $k$-nearest neighbors（kNN） Retrieval on its training data alone 降低其plexity，但其下面的原因仍然不明确。在这项工作中，我们首先排除了一个先前提出的可能性——“softmax瓶颈”。我们进一步发现了 MLP 障碍现象，即 LM 的最后一层 MLP 层可能会阻碍 LM 优化的初始阶段。我们通过使用两个新的数据集进行了Memorization和Generalization的探索，发现高级模型如 GPT-3.5-turbo 在training数据中分配 irrelevant information 的泛化很困难。然而，在vanilla GPT-2 117M中添加 kNN Retrieval 可以一直提高性能在这种设定下。
</details></li>
</ul>
<hr>
<h2 id="Efficient-End-to-End-Visual-Document-Understanding-with-Rationale-Distillation"><a href="#Efficient-End-to-End-Visual-Document-Understanding-with-Rationale-Distillation" class="headerlink" title="Efficient End-to-End Visual Document Understanding with Rationale Distillation"></a>Efficient End-to-End Visual Document Understanding with Rationale Distillation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09612">http://arxiv.org/abs/2311.09612</a></li>
<li>repo_url: None</li>
<li>paper_authors: Wang Zhu, Alekh Agarwal, Mandar Joshi, Robin Jia, Jesse Thomason, Kristina Toutanova</li>
<li>for: Visual document understanding benchmarks</li>
<li>methods: 使用小型预训练图像到文本模型进行选择性文本或布局认识和理解，作为末端模型的中间推理步骤。</li>
<li>results: Student model based on Pix2Struct achieved consistent improvements on three visual document understanding benchmarks, with improvements of more than 4% absolute over a comparable Pix2Struct model that predicts answers directly.<details>
<summary>Abstract</summary>
Understanding visually situated language requires recognizing text and visual elements, and interpreting complex layouts. State-of-the-art methods commonly use specialized pre-processing tools, such as optical character recognition (OCR) systems, that map document image inputs to extracted information in the space of textual tokens, and sometimes also employ large language models (LLMs) to reason in text token space. However, the gains from external tools and LLMs come at the cost of increased computational and engineering complexity. In this paper, we ask whether small pretrained image-to-text models can learn selective text or layout recognition and reasoning as an intermediate inference step in an end-to-end model for pixel-level visual language understanding. We incorporate the outputs of such OCR tools, LLMs, and larger multimodal models as intermediate ``rationales'' on training data, and train a small student model to predict both rationales and answers for input questions based on those training examples. A student model based on Pix2Struct (282M parameters) achieves consistent improvements on three visual document understanding benchmarks representing infographics, scanned documents, and figures, with improvements of more than 4\% absolute over a comparable Pix2Struct model that predicts answers directly.
</details>
<details>
<summary>摘要</summary>
理解图文需要识别文本和视觉元素，并解释复杂的布局。现代方法通常使用专门的预处理工具，如光学字符识别（OCR）系统，将文档图像输入映射到提取的信息空间中的文本 токен中，并有时还使用大型语言模型（LLM）来在文本 токен空间中进行理解。然而，外部工具和LLM的成本是计算和工程复杂性的增加。在这篇论文中，我们问 Whether small pretrained image-to-text模型可以学习选择性的文本或布局认识和理解作为末端模型的中间推理步骤。我们将OCR工具、LLM和更大的多Modal模型的输出作为训练数据中的中间“理由”，并训练一个小型学生模型来根据输入问题预测 rationales和答案。一个基于 Pix2Struct 的小型学生模型（282M参数）在三个视觉文档理解标准准中表现出了逐渐提高，超过4%的绝对提升。
</details></li>
</ul>
<hr>
<h2 id="GistScore-Learning-Better-Representations-for-In-Context-Example-Selection-with-Gist-Bottlenecks"><a href="#GistScore-Learning-Better-Representations-for-In-Context-Example-Selection-with-Gist-Bottlenecks" class="headerlink" title="GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks"></a>GistScore: Learning Better Representations for In-Context Example Selection with Gist Bottlenecks</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09606">http://arxiv.org/abs/2311.09606</a></li>
<li>repo_url: None</li>
<li>paper_authors: Shivanshu Gupta, Clemens Rosenbaum, Ethan R. Elenberg</li>
<li>for: 这paper aimed to improve the in-context learning (ICL) performance of large language models (LLMs) by selecting the best examples from a candidate pool.</li>
<li>methods: The authors proposed a novel metric called GistScore, which is based on Example Gisting, a technique for training example retrievers using an attention bottleneck. They also experimented with fine-tuning gist models on each dataset and multi-task training a single model on a large collection of datasets.</li>
<li>results: The authors achieved state-of-the-art ICL performance on 21 diverse datasets spanning 9 tasks, with an average absolute gain of 20% over off-the-shelf retrievers and 7% over the best prior methods. Their multi-task model also generalizes well out-of-the-box to new task categories, datasets, and prompt templates, with retrieval speeds that are consistently thousands of times faster than the best prior training-free method.<details>
<summary>Abstract</summary>
Large language models (LLMs) have the ability to perform in-context learning (ICL) of new tasks by conditioning on prompts comprising a few task examples. This work studies the problem of selecting the best examples given a candidate pool to improve ICL performance on given a test input. Existing approaches either require training with feedback from a much larger LLM or are computationally expensive. We propose a novel metric, GistScore, based on Example Gisting, a novel approach for training example retrievers for ICL using an attention bottleneck via Gisting, a recent technique for compressing task instructions. To tradeoff performance with ease of use, we experiment with both fine-tuning gist models on each dataset and multi-task training a single model on a large collection of datasets. On 21 diverse datasets spanning 9 tasks, we show that our fine-tuned models get state-of-the-art ICL performance with 20% absolute average gain over off-the-shelf retrievers and 7% over the best prior methods. Our multi-task model generalizes well out-of-the-box to new task categories, datasets, and prompt templates with retrieval speeds that are consistently thousands of times faster than the best prior training-free method.
</details>
<details>
<summary>摘要</summary>
大型语言模型（LLM）具有培根学习（ICL）新任务的能力，通过条件Prompt中的一些任务示例来实现。这项工作研究如何选择最佳示例集来提高ICL性能，以便对给定输入进行测试。现有方法可能需要与更大的LLM进行培训或者计算成本较高。我们提出了一个新的指标——GistScore，基于Example Gisting，一种新的培训示例检索器 для ICL 使用注意力瓶颈via Gisting，一种最近的技术用于压缩任务说明。为了让性能和使用方便进行权衡，我们进行了练习 fine-tuning gist模型 на每个数据集和多任务训练单个模型在一个大量数据集上。在21个多样化的数据集和9个任务上，我们显示了我们的精心调整模型可以达到状态之最ICL性能，与各种off-the-shelf retrievers相比，具有20%的绝对均值提升，并且与最佳先前方法相比，具有7%的提升。我们的多任务模型在新的任务类别、数据集和提示模板上具有良好的泛化能力，并且在输入速度上与最佳先前无需培训的方法相比，保持了一定的速度优势。
</details></li>
</ul>
<hr>
<h2 id="Measuring-and-Improving-Attentiveness-to-Partial-Inputs-with-Counterfactuals"><a href="#Measuring-and-Improving-Attentiveness-to-Partial-Inputs-with-Counterfactuals" class="headerlink" title="Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals"></a>Measuring and Improving Attentiveness to Partial Inputs with Counterfactuals</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09605">http://arxiv.org/abs/2311.09605</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yanai Elazar, Bhargavi Paranjape, Hao Peng, Sarah Wiegreffe, Khyathi Raghavi, Vivek Srikumar, Sameer Singh, Noah A. Smith</li>
<li>for: 本研究旨在检验现有的超vised和in-context学习模型是否过分依赖于训练数据中的偶合关系，以及Counterfactual Attentiveness Test（CAT）是否能够改善模型的抽象能力。</li>
<li>methods: 本研究使用Counterfactual Attentiveness Test（CAT）来系统地检验了十个dataset上四个任务（自然语言推理、阅读理解、句子重构、视觉语言理解）上的established supervised和in-context learning模型。CAT使用对应的counterfactual来替换训练数据中的一部分，并期望模型能够根据这些counterfactual进行不同的预测。</li>
<li>results: 研究发现，依赖于训练数据中的偶合关系的依赖性是主要的数据依赖性。另外，研究发现GPT3在增加示例数量后变得更加不够注意力，而其测试数据上的准确率提高。结果表明，在训练或示例数据中添加counterfactual可以提高模型的抽象能力。此外，CAT测试表明，模型的注意力测量不同于 solely measuring correlations in data。<details>
<summary>Abstract</summary>
The inevitable appearance of spurious correlations in training datasets hurts the generalization of NLP models on unseen data. Previous work has found that datasets with paired inputs are prone to correlations between a specific part of the input (e.g., the hypothesis in NLI) and the label; consequently, models trained only on those outperform chance. Are these correlations picked up by models trained on the full input data? To address this question, we propose a new evaluation method, Counterfactual Attentiveness Test (CAT). CAT uses counterfactuals by replacing part of the input with its counterpart from a different example (subject to some restrictions), expecting an attentive model to change its prediction. Using CAT, we systematically investigate established supervised and in-context learning models on ten datasets spanning four tasks: natural language inference, reading comprehension, paraphrase detection, and visual & language reasoning. CAT reveals that reliance on such correlations is mainly data-dependent. Surprisingly, we find that GPT3 becomes less attentive with an increased number of demonstrations, while its accuracy on the test data improves. Our results demonstrate that augmenting training or demonstration data with counterfactuals is effective in improving models' attentiveness. We show that models' attentiveness measured by CAT reveals different conclusions from solely measuring correlations in data.
</details>
<details>
<summary>摘要</summary>
“训练数据中偶现的假象相关性会对NLP模型的泛化性产生负面影响。先前的研究发现，带有对应输入部分（如NLI中的假设）和标签之间存在相关性，导致使用只有这些输入训练的模型能够超过偶散。现在我们提出了一种新的评估方法：对比性注意力测试（CAT）。CAT使用对比例的方法，替换输入中的一部分（保留一些限制），期望一个注意力强的模型会改变其预测。通过CAT，我们系统地研究了多种supervised和in-context学习模型在十个 datasets 上，涵盖四个任务：自然语言推理、阅读理解、句子重写检测和视觉语言理解。结果表明，模型对这些相关性的依赖性是数据висиendent的。另外，我们发现GPT3在增加示例数量后，其注意力度会降低，而测试数据上的准确率会提高。我们的结果表明，在训练或示例数据中添加对比例可以提高模型的注意力度。我们的结果还表明，通过CAT评估模型的注意力度可以从数据中的相关性中分离出不同的结论。”
</details></li>
</ul>
<hr>
<h2 id="SCORE-A-framework-for-Self-Contradictory-Reasoning-Evaluation"><a href="#SCORE-A-framework-for-Self-Contradictory-Reasoning-Evaluation" class="headerlink" title="SCORE: A framework for Self-Contradictory Reasoning Evaluation"></a>SCORE: A framework for Self-Contradictory Reasoning Evaluation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09603">http://arxiv.org/abs/2311.09603</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ziyi Liu, Isabelle Lee, Yongkang Du, Soumya Sanyal, Jieyu Zhao</li>
<li>for: 这 paper 旨在分析大语言模型（LLM）是否真的具备良好的理解能力，以及这种能力是如何影响下游任务的性能。</li>
<li>methods: 这 paper 使用了一种名为 \textsc{SCORE} 的框架来分析 LLM 的理解能力。特别是，它关注自相矛盾的理解，即 LLM 在处理含有上下文信息和常识的任务时，可能会出现自相矛盾的行为。</li>
<li>results: 研究发现，LLM 在多个视点 Setting 下表现不稳定，甚至对正确预测也可能表现出含糊不清的理解。这些结果指出了 LLM 的理解能力有很大的改进空间，并且需要进一步的研究来确定评价reasoning的最佳实践。<details>
<summary>Abstract</summary>
Large language models (LLMs) have demonstrated impressive reasoning ability in various language-based tasks. Despite many proposed reasoning methods aimed at enhancing performance in downstream tasks, two fundamental questions persist: Does reasoning genuinely support predictions, and how reliable is the quality of reasoning? In this paper, we propose a framework \textsc{SCORE} to analyze how well LLMs can reason. Specifically, we focus on self-contradictory reasoning, where reasoning does not support the prediction. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information and commonsense. The model may miss evidence or use shortcuts, thereby exhibiting self-contradictory behaviors. We also employ the Point-of-View (POV) method, which probes models to generate reasoning from multiple perspectives, as a diagnostic tool for further analysis. We find that though LLMs may appear to perform well in one-perspective settings, they fail to stabilize such behavior in multi-perspectives settings. Even for correct predictions, the reasoning may be messy and incomplete, and LLMs can easily be led astray from good reasoning. \textsc{SCORE}'s results underscore the lack of robustness required for trustworthy reasoning and the urgency for further research to establish best practices for a comprehensive evaluation of reasoning beyond accuracy-based metrics.
</details>
<details>
<summary>摘要</summary>
Translation notes:* "Large language models" is translated as "大型语言模型" (dàxìng yǔyán módel).* "Reasoning" is translated as "理解" (lǐjiě) or "解释" (jiějie).* "Self-contradictory reasoning" is translated as "自相矛盾的理解" (zìxiāng dòuduō de lǐjiě).* "Point-of-View" is translated as "视角" (wénjiàng).* "Multi-perspectives" is translated as "多视角" (duōwénjiàng).* "Messy and incomplete" is translated as "杂乱不完整" (zàilàng bù qiáncháng).* "Trustworthy reasoning" is translated as "可靠的理解" (kěkuài de lǐjiě).
</details></li>
</ul>
<hr>
<h2 id="Language-Models-Mostly-Do-Not-Consider-Emotion-Triggers-When-Predicting-Emotion"><a href="#Language-Models-Mostly-Do-Not-Consider-Emotion-Triggers-When-Predicting-Emotion" class="headerlink" title="Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion"></a>Language Models (Mostly) Do Not Consider Emotion Triggers When Predicting Emotion</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09602">http://arxiv.org/abs/2311.09602</a></li>
<li>repo_url: None</li>
<li>paper_authors: Smriti Singh, Cornelia Caragea, Junyi Jessy Li</li>
<li>for: 这个研究是为了检验大型自然语言模型（LLM）和精度调整模型（Fine-tuned models）是否能够正确地识别情绪诱发因素（emotion triggers）。</li>
<li>methods: 该研究使用了一个新的数据集EmoTrigger，该数据集包含900个社交媒体文章，来源于三个不同的数据集，并由专家 manually annotated为情绪诱发因素。</li>
<li>results: 研究发现，情绪诱发因素并不是情绪预测模型中考虑的重要特征，而是存在详细的相互作用 между各种特征和情绪检测任务。<details>
<summary>Abstract</summary>
Situations and events evoke emotions in humans, but to what extent do they inform the prediction of emotion detection models? Prior work in emotion trigger or cause identification focused on training models to recognize events that trigger an emotion. Instead, this work investigates how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions. First, we introduce a novel dataset EmoTrigger, consisting of 900 social media posts sourced from three different datasets; these were annotated by experts for emotion triggers with high agreement. Using EmoTrigger, we evaluate the ability of large language models (LLMs) to identify emotion triggers, and conduct a comparative analysis of the features considered important for these tasks between LLMs and fine-tuned models. Our analysis reveals that emotion triggers are largely not considered salient features for emotion prediction models, instead there is intricate interplay between various features and the task of emotion detection.
</details>
<details>
<summary>摘要</summary>
情感Trigger的情况和事件会让人们表现出不同的情感，但到底这些事件会如何影响情感探测模型的预测呢？先前的工作主要集中在训练模型可以识别引发情感的事件上，而这个工作则是 investigate how well human-annotated emotion triggers correlate with features that models deemed salient in their prediction of emotions。我们首先介绍了一个新的数据集EmotTrigger，该数据集包含900个社交媒体文章，来自三个不同的数据集，这些文章由专家进行情感触发点的注释，注释具有高度一致性。使用EmotTrigger数据集，我们评估了大型自然语言模型（LLMs）能够识别情感触发点，并对这些任务之间的细节进行比较分析。我们的分析发现，情感触发点并不是情感预测模型考虑的重要特征，而是各种特征之间的细节很复杂地相互作用，以实现情感预测任务。
</details></li>
</ul>
<hr>
<h2 id="LifeTox-Unveiling-Implicit-Toxicity-in-Life-Advice"><a href="#LifeTox-Unveiling-Implicit-Toxicity-in-Life-Advice" class="headerlink" title="LifeTox: Unveiling Implicit Toxicity in Life Advice"></a>LifeTox: Unveiling Implicit Toxicity in Life Advice</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09585">http://arxiv.org/abs/2311.09585</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/minbeomkim/LifeTox">https://github.com/minbeomkim/LifeTox</a></li>
<li>paper_authors: Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin Jung</li>
<li>for: 这个论文的目的是为了检测生活中的隐式恶意言语。</li>
<li>methods: 这个论文使用了RoBERTa模型，并在LifeTox数据集上进行了微调。</li>
<li>results: 实验表明，RoBERTa模型在隐式恶意言语分类任务中匹配或超过了现有的大语言模型的零shot性能。<details>
<summary>Abstract</summary>
As large language models become increasingly integrated into daily life, detecting implicit toxicity across diverse contexts is crucial. To this end, we introduce LifeTox, a dataset designed for identifying implicit toxicity within a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox comprises diverse contexts derived from personal experiences through open-ended questions. Experiments demonstrate that RoBERTa fine-tuned on LifeTox matches or surpasses the zero-shot performance of large language models in toxicity classification tasks. These results underscore the efficacy of LifeTox in addressing the complex challenges inherent in implicit toxicity.
</details>
<details>
<summary>摘要</summary>
Large language models are becoming increasingly integrated into daily life, so detecting implicit toxicity across diverse contexts is crucial. To address this challenge, we introduce LifeTox, a dataset designed for identifying implicit toxicity in a broad range of advice-seeking scenarios. Unlike existing safety datasets, LifeTox includes diverse contexts derived from personal experiences through open-ended questions. Experimental results show that RoBERTa fine-tuned on LifeTox performs equally well or even better than large language models in toxicity classification tasks, demonstrating the effectiveness of LifeTox in addressing the complex challenges of implicit toxicity.
</details></li>
</ul>
<hr>
<h2 id="Enhancing-Medical-Text-Evaluation-with-GPT-4"><a href="#Enhancing-Medical-Text-Evaluation-with-GPT-4" class="headerlink" title="Enhancing Medical Text Evaluation with GPT-4"></a>Enhancing Medical Text Evaluation with GPT-4</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09581">http://arxiv.org/abs/2311.09581</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yiqing Xie, Sheng Zhang, Hao Cheng, Zelalem Gero, Cliff Wong, Tristan Naumann, Hoifung Poon</li>
<li>for: 针对医疗文本生成评估中的准确性评价。</li>
<li>methods: 提出基于GPT-4的医疗文本评估方法，包括细致性评估方面和相关医疗领域模型训练。</li>
<li>results: 与现有评价 metric 比较，提出的GPT-4基于评价方法在医疗笔记生成和医疗报告摘要任务上显示了substantially higher的一致性。<details>
<summary>Abstract</summary>
In the evaluation of medical text generation, it is essential to scrutinize each piece of information and ensure the utmost accuracy of the evaluation. Existing evaluation metrics either focus on coarse-level evaluation that assigns one score for the whole generated output or rely on evaluation models trained on general domain, resulting in inaccuracies when adapted to the medical domain. To address these issues, we propose a set of factuality-centric evaluation aspects and design corresponding GPT-4-based metrics for medical text generation. We systematically compare these metrics with existing ones on clinical note generation and medical report summarization tasks, revealing low inter-metric correlation. A comprehensive human evaluation confirms that the proposed GPT-4-based metrics exhibit substantially higher agreement with human judgments than existing evaluation metrics. Our study contributes to the understanding of medical text generation evaluation and offers a more reliable alternative to existing metrics.
</details>
<details>
<summary>摘要</summary>
在医学文本生成评估中，必须仔细检查每个信息并确保评估的准确性。现有的评估指标可能会将整个生成输出的评估授予一个分数，或者基于通用领域的评估模型，导致在医学领域中出现不准确的评估。为解决这些问题，我们提出了一组中心于事实的评估方面和基于GPT-4的评估指标，用于医学文本生成。我们系统比较了这些指标与现有指标的相关性，发现它们在医学报告摘要和医学病历生成任务上显示了低相关性。人工评估表明，我们提出的GPT-4基于的评估指标与人类判断更为一致，与现有指标相比，具有更高的一致性。我们的研究增进了医学文本生成评估的理解，并提供了更可靠的评估方法。
</details></li>
</ul>
<hr>
<h2 id="MMOE-Mixture-of-Multimodal-Interaction-Experts"><a href="#MMOE-Mixture-of-Multimodal-Interaction-Experts" class="headerlink" title="MMOE: Mixture of Multimodal Interaction Experts"></a>MMOE: Mixture of Multimodal Interaction Experts</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09580">http://arxiv.org/abs/2311.09580</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haofei Yu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency</li>
<li>for: 本研究旨在解决现实世界中新型多modal交互的问题，例如让机器学习模型更好地理解混乱的语言和手势之间的互动关系。</li>
<li>methods: 本研究提出了一种新的方法 called MMOE（多modal交互专家杂合），它可以自动将数据点分类为不同的交互类型，并采用特定交互类型的专门模型进行处理。</li>
<li>results: 根据实验结果，MMOE方法可以提高对困难交互的表现，比如让机器学习模型更好地预测讽刺语言。总的来说，这种方法可以提高 dataset 分析的新视角，并且实现了当前最佳性能。<details>
<summary>Abstract</summary>
Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.
</details>
<details>
<summary>摘要</summary>
多模式机器学习，研究不同输入模式之间的信息和互动，在理解图像和描述文本之间的关系方面做出了重要进步。然而，这只是实际世界中多模式互动的一部分，不包括新型的对话和手势冲突的互动，如讲述嘲讽的例子。当前的共享信息捕捉方法经常不能够很好地扩展到这些更复杂的互动，有时performance只有50%级别的binary分类。在这篇论文中，我们解决这个问题通过一种新的方法，即MMOE（多模式互动专家混合）。我们的方法可以自动将数据点从无标签多模式数据集分类为互动类型，并采用特殊的模型来处理每种特定的互动。根据我们的实验，这种方法可以提高对这些复杂的互动的性能，增加总性能约2%，如讲述嘲讽预测等任务。因此，互动量化提供了新的数据分析途径，并且实现了简单的方法，达到了现状之前的最佳性能。</sys>Here is the translation of the text into Simplified Chinese:<sys>多模式机器学习，研究不同输入模式之间的信息和互动，在理解图像和描述文本之间的关系方面做出了重要进步。然而，这只是实际世界中多模式互动的一部分，不包括新型的对话和手势冲突的互动，如讲述嘲讽的例子。当前的共享信息捕捉方法经常不能够很好地扩展到这些更复杂的互动，有时performance只有50%级别的binary分类。在这篇论文中，我们解决这个问题通过一种新的方法，即MMOE（多模式互动专家混合）。我们的方法可以自动将数据点从无标签多模式数据集分类为互动类型，并采用特殊的模型来处理每种特定的互动。根据我们的实验，这种方法可以提高对这些复杂的互动的性能，增加总性能约2%，如讲述嘲讽预测等任务。因此，互动量化提供了新的数据分析途径，并且实现了简单的方法，达到了现状之前的最佳性能。</sys>
</details></li>
</ul>
<hr>
<h2 id="Crafting-In-context-Examples-according-to-LMs’-Parametric-Knowledge"><a href="#Crafting-In-context-Examples-according-to-LMs’-Parametric-Knowledge" class="headerlink" title="Crafting In-context Examples according to LMs’ Parametric Knowledge"></a>Crafting In-context Examples according to LMs’ Parametric Knowledge</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09579">http://arxiv.org/abs/2311.09579</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yoonsang Lee, Pranav Atreya, Xi Ye, Eunsol Choi</li>
<li>for: 本研究探讨了如何构建受Context的示例集，以便在语言模型中触发行为，即 surface parametric knowledge。</li>
<li>methods: 研究使用了受Context示例集，并进行了分类和分析，以了解模型对于受Context示例的 parametric knowledge。</li>
<li>results: 实验结果表明，使用包含知识和未知信息的示例集可以最佳地在多种设置下进行表现。此外，研究还发现，使用模型的 parametric knowledge 来排序答案集可以提高表现。<details>
<summary>Abstract</summary>
In-context learning has been applied to knowledge-rich tasks such as question answering. In such scenarios, in-context examples are used to trigger a behaviour in the language model: namely, it should surface information stored in its parametric knowledge. We study the construction of in-context example sets, with a focus on the parametric knowledge of the model regarding in-context examples. We identify 'known' examples, where models can correctly answer from its parametric knowledge, and 'unknown' ones. Our experiments show that prompting with 'unknown' examples decreases the performance, potentially as it encourages hallucination rather than searching its parametric knowledge. Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings. We perform analysis on three multi-answer question answering datasets, which allows us to further study answer set ordering strategies based on the LM's knowledge about each answer. Together, our study sheds lights on how to best construct in-context example sets for knowledge-rich tasks.
</details>
<details>
<summary>摘要</summary>
启用上下文学习应用于知识充沛的任务，如问答。在这些场景下，上下文示例被用来触发语言模型的行为：即它应该Surface其参数知识中的信息。我们研究上下文示例集的建构，强调语言模型对上下文示例的参数知识。我们分类了“知道”的示例和“不知道”的示例。我们的实验表明，向语言模型提供“不知道”的示例会降低其性能，可能是因为它鼓励了幻化而不是搜索其参数知识。构建包含知道和不知道信息的上下文示例集最佳，我们在多种场景中进行了分析。我们还研究了基于语言模型对每个答案的知识来排序答案集的策略。ogether，我们的研究为知识充沛任务中的上下文示例集建构提供了新的灯光。
</details></li>
</ul>
<hr>
<h2 id="A-Reevaluation-of-Event-Extraction-Past-Present-and-Future-Challenges"><a href="#A-Reevaluation-of-Event-Extraction-Past-Present-and-Future-Challenges" class="headerlink" title="A Reevaluation of Event Extraction: Past, Present, and Future Challenges"></a>A Reevaluation of Event Extraction: Past, Present, and Future Challenges</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09562">http://arxiv.org/abs/2311.09562</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/ej0cl6/textee">https://github.com/ej0cl6/textee</a></li>
<li>paper_authors: Kuan-Hao Huang, I-Hung Hsu, Tanmay Parekh, Zhiyu Xie, Zixuan Zhang, Premkumar Natarajan, Kai-Wei Chang, Nanyun Peng, Heng Ji</li>
<li>For: The paper is written for the purpose of proposing a standardized, fair, and reproducible benchmark for event extraction, and to address the evaluation challenges in recent studies.* Methods: The paper uses standardized data preprocessing scripts and splits for more than ten datasets across different domains, and aggregates and re-implements over ten event extraction approaches published in recent years.* Results: The paper conducts a comprehensive reevaluation of event extraction approaches using the proposed benchmark, and explores the capability of large language models in event extraction. The results are expected to provide a reliable benchmark for future research in the field.<details>
<summary>Abstract</summary>
Event extraction has attracted much attention in recent years due to its potential for many applications. However, recent studies observe some evaluation challenges, suggesting that reported scores might not reflect the true performance. In this work, we first identify and discuss these evaluation challenges, including the unfair comparisons resulting from different assumptions about data or different data preprocessing steps, the incompleteness of the current evaluation framework leading to potential dataset bias or data split bias, and low reproducibility of prior studies. To address these challenges, we propose TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE contains standardized data preprocessing scripts and splits for more than ten datasets across different domains. In addition, we aggregate and re-implement over ten event extraction approaches published in recent years and conduct a comprehensive reevaluation. Finally, we explore the capability of large language models in event extraction and discuss some future challenges. We expect TextEE will serve as a reliable benchmark for event extraction, facilitating future research in the field.
</details>
<details>
<summary>摘要</summary>
Event extraction 在最近几年内受到了广泛关注，因为它在多个应用领域中具有潜在的潜力。然而，最近的研究发现了评估挑战，表明报告的分数可能不准确反映实际表现。在这项工作中，我们首先标识和讨论了评估挑战，包括数据假设不同或数据预处理步骤不同导致的不公正比较，当前评估框架不完整，导致可能的数据偏见或数据拆分偏见，以及过去研究的低可重现性。为解决这些挑战，我们提出了 TextEE，一个标准化、公平、可重现的事件抽取benchmark。 TextEE包含了标准化的数据预处理脚本和分割，以及多个领域的超过十个数据集。此外，我们对过去十年以来发表的十多个事件抽取方法进行了汇总和重新实现，并进行了全面的重评。最后，我们探讨了大语言模型在事件抽取中的能力，并讨论了未来的挑战。我们期望 TextEE 能成为事件抽取领域的可靠 benchmark，促进未来的研究。
</details></li>
</ul>
<hr>
<h2 id="Pachinko-Patching-Interpretable-QA-Models-through-Natural-Language-Feedback"><a href="#Pachinko-Patching-Interpretable-QA-Models-through-Natural-Language-Feedback" class="headerlink" title="Pachinko: Patching Interpretable QA Models through Natural Language Feedback"></a>Pachinko: Patching Interpretable QA Models through Natural Language Feedback</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09558">http://arxiv.org/abs/2311.09558</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/chaitanyamalaviya/pachinko">https://github.com/chaitanyamalaviya/pachinko</a></li>
<li>paper_authors: Chaitanya Malaviya, Subin Lee, Dan Roth, Mark Yatskar</li>
<li>for: 本研究旨在提高NL模型的评估，通过从用户反馈中收集改进模型。</li>
<li>methods: 研究使用了分解式问答模型，首先从 контек斯和问题中提取中间理由，然后使用这个理由来回答问题。</li>
<li>results: 研究发现，不同的理由格式对于用户提供反馈和理解模型回答的能力有显著影响。 certain formats significantly enhance user reported understanding and trust of model outputs.<details>
<summary>Abstract</summary>
Eliciting feedback from end users of NLP models can be beneficial for improving models. However, how should we present model responses to users so they are most amenable to be corrected from user feedback? Further, what properties do users value to understand and trust responses? We answer these questions by analyzing the effect of rationales generated by QA models to support their answers. We specifically consider decomposed question-answering models that first extract an intermediate rationale based on a context and a question and then use solely this rationale to answer the question. A rationale outlines the approach followed by the model to answer the question. Our work considers various formats of these rationales that vary according to well-defined properties of interest. We sample these rationales from large language models using few-shot prompting for two reading comprehension datasets, and then perform two user studies. In the first one, we present users with incorrect answers and corresponding rationales of various formats and ask them to provide natural language feedback to revise the rationale. We then measure the effectiveness of this feedback in patching these rationales through in-context learning. The second study evaluates how well different rationale formats enable users to understand and trust model answers, when they are correct. We find that rationale formats significantly affect how easy it is (1) for users to give feedback for rationales, and (2) for models to subsequently execute this feedback. In addition to influencing critiquablity, certain formats significantly enhance user reported understanding and trust of model outputs.
</details>
<details>
<summary>摘要</summary>
找到用户对NL理解模型的反馈可以有助于改进模型。然而，如何在给用户显示模型回答以便他们可以更好地修改它？而且，用户关心什么样的特性来信任和理解模型的回答呢？我们通过分析QA模型生成的论证来回答这些问题。我们专门考虑了基于上下文和问题的分解Question answering模型，它们首先从上下文和问题中提取中间论证，然后只使用这个论证回答问题。论证描述模型回答问题的方法。我们使用大型语言模型通过几个提示来采样这些论证，然后对两个阅读理解dataset进行两项用户研究。在第一项研究中，我们给用户显示错误的回答和相应的论证不同格式，并询问他们提供自然语言反馈来修改论证。我们然后测量这些反馈是否可以通过上下文学习来修复论证。第二项研究检验了不同的论证格式对用户理解和信任模型输出的影响。我们发现，不同的论证格式对用户提供反馈的容易度和模型执行这些反馈的能力有很大影响。此外，某些格式可以明显提高用户报告的理解和信任度。
</details></li>
</ul>
<hr>
<h2 id="Large-Language-Models-are-Few-Shot-Training-Example-Generators-A-Case-Study-in-Fallacy-Recognition"><a href="#Large-Language-Models-are-Few-Shot-Training-Example-Generators-A-Case-Study-in-Fallacy-Recognition" class="headerlink" title="Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition"></a>Large Language Models are Few-Shot Training Example Generators: A Case Study in Fallacy Recognition</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09552">http://arxiv.org/abs/2311.09552</a></li>
<li>repo_url: None</li>
<li>paper_authors: Tariq Alhindi, Smaranda Muresan, Preslav Nakov</li>
<li>for: 提高现有的谬误认识模型，以便更好地处理多种频率不均的谬误类型。</li>
<li>methods:  incorporating additional context and leveraging大语言模型生成Synthetic数据，以增加较少seen classes的表现。</li>
<li>results: 在不同的谬误类型、数据集和生成器上进行了评估，得到了一致的提高。<details>
<summary>Abstract</summary>
Recognizing fallacies is crucial for ensuring the quality and validity of arguments across various domains. However, computational fallacy recognition faces challenges due to the diverse genres, domains, and types of fallacies found in datasets. This leads to a highly multiclass, and even multi-label, setup with substantial class imbalance. In this study, we aim to enhance existing models for fallacy recognition by incorporating additional context and by leveraging large language models to generate synthetic data, thus increasing the representation of the infrequent classes. We experiment with GPT3.5 to generate synthetic examples and we examine the impact of prompt settings for this. Moreover, we explore zero-shot and few-shot scenarios to evaluate the effectiveness of using the generated examples for training smaller models within a unified fallacy recognition framework. Furthermore, we analyze the overlap between the synthetic data and existing fallacy datasets. Finally, we investigate the usefulness of providing supplementary context for detecting fallacy types that need such context, e.g., diversion fallacies. Our evaluation results demonstrate consistent improvements across fallacy types, datasets, and generators.
</details>
<details>
<summary>摘要</summary>
识别谬误是确保不同领域的论据质量和有效性的关键。然而，计算机谬误识别受到数据集中多种类型、领域和类别的多种谬误的挑战。这导致了一个高度多类、甚至多标签的设置，以及巨大的类别偏度问题。在这种情况下，我们想要提高现有的谬误识别模型，通过添加更多的 контекст和利用大语言模型生成 sintetic数据，以增加轻度类的表现。我们使用GPT3.5生成 sintetic例子，并考虑Prompt设置的影响。此外，我们还探索零shot和几shotenario来评估使用生成的例子来训练更小的模型在一个简化的谬误识别框架中。此外，我们还分析了生成的数据和现有的谬误数据集之间的重叠。最后，我们 investigate了在检测某些谬误类型时提供补充的 контекст的有用性，例如误导谬误。我们的评估结果表明，无论谬误类型、数据集或生成器，我们的方法都能够实现了一致的改进。
</details></li>
</ul>
<hr>
<h2 id="A-Speed-Odyssey-for-Deployable-Quantization-of-LLMs"><a href="#A-Speed-Odyssey-for-Deployable-Quantization-of-LLMs" class="headerlink" title="A Speed Odyssey for Deployable Quantization of LLMs"></a>A Speed Odyssey for Deployable Quantization of LLMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09550">http://arxiv.org/abs/2311.09550</a></li>
<li>repo_url: None</li>
<li>paper_authors: Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen Xie</li>
<li>for: 这个研究旨在提高大语言模型的推理速度和成本效益。</li>
<li>methods: 本研究使用硬件对应的数字化方法，排除不切实际的算法选择，同时将最大化硬件加速的效益。</li>
<li>results: 实验结果显示，我们的W4A8方法可以提高实际推理速度至多达4倍于Hugging Face FP16推理和2.23倍于TensorRT-LLM在FP16推理中，并在INT8推理中与TensorRT-LLM相比提高了1.45倍，而不会对性能造成严重干扰。<details>
<summary>Abstract</summary>
The large language model era urges faster and less costly inference. Prior model compression works on LLMs tend to undertake a software-centric approach primarily focused on the simulated quantization performance. By neglecting the feasibility of deployment, these approaches are typically disabled in real practice. They used to drastically push down the quantization bit range for a reduced computation which might not be supported by the mainstream hardware, or involve sophisticated algorithms that introduce extra computation or memory access overhead. We argue that pursuing a hardware-centric approach in the construction of quantization algorithms is crucial. In this regard, we are driven to build our compression method on top of hardware awareness, eliminating impractical algorithm choices while maximizing the benefit of hardware acceleration. Our method, OdysseyLLM, comes with a novel W4A8 kernel implementation called FastGEMM and a combined recipe of quantization strategies. Extensive experiments manifest the superiority of our W4A8 method which brings the actual speed boosting up to \textbf{4$\times$} compared to Hugging Face FP16 inference and \textbf{2.23$\times$} vs. the state-of-the-art inference engine TensorRT-LLM in FP16, and \textbf{1.45$\times$} vs. TensorRT-LLM in INT8, yet without substantially harming the performance.
</details>
<details>
<summary>摘要</summary>
大型语言模型时代强调更快速且成本更低的推导。先前的模型压缩方法倾向于以软件中心的方式进行，主要侧重在模拟量化性能。但这些方法通常在实际应用中被禁用，因为它们通常会降低量化比例，使得主流硬件无法支持或增加了复杂的算法或内存访问开销。我们认为在量化算法的建立中，应该将硬件考虑为核心。在这方面，我们将我们的压缩方法建立在硬件意识之上，排除不可行的算法选择，同时将硬件加速器的最大优化。我们的方法 OdysseyLLM 搭配了一个新的 W4A8 核心实现 FastGEMM，以及一种结合的量化策略。实验结果显示 OdysseyLLM 的实际速度提升为 \textbf{4$\times$} 比 Hugging Face FP16 推导，并且与现有的推导引擎 TensorRT-LLM 在 FP16 下的速度提升为 \textbf{2.23$\times$}，并且在 INT8 下的速度提升为 \textbf{1.45$\times$}，但不会对性能造成严重的损害。
</details></li>
</ul>
<hr>
<h2 id="Towards-Pragmatic-Awareness-in-Question-Answering-A-Case-Study-in-Maternal-and-Infant-Health"><a href="#Towards-Pragmatic-Awareness-in-Question-Answering-A-Case-Study-in-Maternal-and-Infant-Health" class="headerlink" title="Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health"></a>Towards Pragmatic Awareness in Question Answering: A Case Study in Maternal and Infant Health</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09542">http://arxiv.org/abs/2311.09542</a></li>
<li>repo_url: None</li>
<li>paper_authors: Neha Srikanth, Rupak Sarkar, Rachel Rudinger, Jordan Boyd-Graber</li>
<li>for: 这个论文主要是为了解决问答系统在高风险领域 like maternal and infant health 中能够更好地回答用户问题。</li>
<li>methods: 该论文使用大量语言模型来检测问题中含义的推理，以便在回答用户问题时能够更加准确地理解用户的需求。</li>
<li>results: 研究发现，通过检测问题中含义的推理，可以生成更加准确和有用的回答，从而避免了在回答用户问题时可能产生的危害。<details>
<summary>Abstract</summary>
Questions posed by information-seeking users often contain implicit false or potentially harmful assumptions. In a high-risk domain such as maternal and infant health, a question-answering system must recognize these pragmatic constraints and go beyond simply answering user questions, examining them in context to respond helpfully. To achieve this, we study pragmatic inferences made when mothers ask questions about pregnancy and infant care. Some of the inferences in these questions evade detection by existing methods, risking the possibility of QA systems failing to address them which can have dangerous health and policy implications. We explore the viability of detecting inferences from questions using large language models and illustrate that informing existing QA pipelines with pragmatic inferences produces responses that can mitigate the propagation of harmful beliefs.
</details>
<details>
<summary>摘要</summary>
常见于信息寻求用户的问题中的隐含假设或潜在危险假设，在高风险领域如母婴健康，一个问答系统必须认识这些实用限制，不仅回答用户的问题，更要在上下文中检查它们，以对用户提供有用的回答。为了实现这一目标，我们研究了怀孕和婴儿护理中妈妈提出的假设推理。一些这些问题中的假设逃避现有的方法检测，这可能会导致问答系统失败 Addressing them, which can have serious health and policy implications. We explore the feasibility of detecting inferences from questions using large language models and show that incorporating pragmatic inferences into existing QA pipelines can mitigate the propagation of harmful beliefs.
</details></li>
</ul>
<hr>
<h2 id="Reducing-Privacy-Risks-in-Online-Self-Disclosures-with-Language-Models"><a href="#Reducing-Privacy-Risks-in-Online-Self-Disclosures-with-Language-Models" class="headerlink" title="Reducing Privacy Risks in Online Self-Disclosures with Language Models"></a>Reducing Privacy Risks in Online Self-Disclosures with Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09538">http://arxiv.org/abs/2311.09538</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yao Dou, Isadora Krsek, Tarek Naous, Anubha Kabra, Sauvik Das, Alan Ritter, Wei Xu</li>
<li>For: 保护在线自透泄的用户端隐私* Methods: 发展19种自透泄类划分，精度 fine-tune语言模型，并进行人工测试* Results: 实现Token F$_1$的过程优于75%，并通过用户反馈引入自透泄抽象任务，实现多种 fine-tuning 策略，生成具有较高实用性和Moderate隐私风险的抽象结果。<details>
<summary>Abstract</summary>
Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through identification and abstraction. We develop a taxonomy of 19 self-disclosure categories, and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for identification, achieving over 75% in Token F$_1$. We further conduct a HCI user study, with 82\% of participants viewing the model positively, highlighting its real world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction. We experiment with both one-span abstraction and three-span abstraction settings, and explore multiple fine-tuning strategies. Our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation.
</details>
<details>
<summary>摘要</summary>
自我披露在社交媒体交互中很常见和奖励，但也存在隐私风险。在这篇论文中，我们主动保护用户端隐私相关于在线自我披露的权益。我们开发了19种自我披露类别的taxonomy，并采集了4.8K注释化的披露跨度。我们然后精细调整语言模型，实现了Token F$_1$的过 75%。我们进一步进行了人机交互研究，82%的参与者视为模型有利可图，这反映了其在实际世界中的可行性。受用户反馈 inspirited，我们引入了自我披露抽象任务。我们在一span抽象和三span抽象的设置下进行了实验，并探索了多种调整策略。我们最佳模型可以生成多样化的抽象， moderately reducing privacy risks while maintaining high utility according to human evaluation.
</details></li>
</ul>
<hr>
<h2 id="Effective-Large-Language-Model-Adaptation-for-Improved-Grounding"><a href="#Effective-Large-Language-Model-Adaptation-for-Improved-Grounding" class="headerlink" title="Effective Large Language Model Adaptation for Improved Grounding"></a>Effective Large Language Model Adaptation for Improved Grounding</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09533">http://arxiv.org/abs/2311.09533</a></li>
<li>repo_url: None</li>
<li>paper_authors: Xi Ye, Ruoxi Sun, Sercan Ö. Arik, Tomas Pfister</li>
<li>for: 提高大型自然语言模型（LLMs）在实际应用中的广泛部署，因为它们可能会生成“幻想”的答案。</li>
<li>methods: 提出了一种新的框架AGREE，即Adaptation of LLMs for GRounding EnhancEment，以改进grounding的问题从一个整体的角度。</li>
<li>results: 比较prompting-based方法，通过调整LLMs来ground它们的答案，可以得到更好地参照的答案，并且可以减少对数据的需求。<details>
<summary>Abstract</summary>
Large language models (LLMs) have achieved remarkable advancements in natural language understanding, generation, and manipulation of text-based data. However, one major issue towards their widespread deployment in the real world is that they can generate "hallucinated" answers that are not factual. Towards this end, this paper focuses on improving grounding from a holistic perspective with a novel framework, AGREE, Adaptation of LLMs for GRounding EnhancEment. We start with the design of an iterative test-time adaptation (TTA) capability that takes into account the support information generated in self-grounded responses. To effectively enable this capability, we tune LLMs to ground the claims in their responses to retrieved documents by providing citations. This tuning on top of the pre-trained LLMs requires a small amount of data that needs to be constructed in a particular way to learn the grounding information, for which we introduce a data construction method. Our results show that the tuning-based AGREE framework generates better grounded responses with more accurate citations compared to prompting-based approaches.
</details>
<details>
<summary>摘要</summary>
The AGREE framework focuses on improving grounding from a holistic perspective by incorporating an iterative test-time adaptation (TTA) capability that considers the support information generated in self-grounded responses. To enable this capability, we fine-tune LLMs to ground their claims in their responses to retrieved documents by providing citations. This fine-tuning process requires a small amount of specially constructed data to learn the grounding information.Our results show that the tuning-based AGREE framework generates more accurate and better grounded responses compared to prompting-based approaches. This demonstrates the effectiveness of the AGREE framework in improving the factual accuracy of LLMs' responses.
</details></li>
</ul>
<hr>
<h2 id="AMRFact-Enhancing-Summarization-Factuality-Evaluation-with-AMR-driven-Training-Data-Generation"><a href="#AMRFact-Enhancing-Summarization-Factuality-Evaluation-with-AMR-driven-Training-Data-Generation" class="headerlink" title="AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation"></a>AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven Training Data Generation</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09521">http://arxiv.org/abs/2311.09521</a></li>
<li>repo_url: None</li>
<li>paper_authors: Haoyi Qiu, Kung-Hsiang Huang, Jingnong Qu, Nanyun Peng</li>
<li>for: 本研究的目的是提高抽象摘要中的事实准确性，尤其是在抽象摘要 task 中，保持信息的精度是非常重要的。</li>
<li>methods: 本研究使用了 Abstract Meaning Representation (AMR) 来生成不一致的摘要，并使用了自然语言判断和 BARTScore 来选择高质量的负例。</li>
<li>results: 实验结果表明，本研究的方法在 AggreFact-SOTA 数据集上显著超越了之前的系统，这说明了其在检测抽象摘要中的事实准确性的能力。<details>
<summary>Abstract</summary>
Ensuring factual consistency is crucial in various natural language processing tasks, particularly in abstractive summarization, where preserving the integrity of information is paramount. Prior entailment-based approaches often generate factually inconsistent summaries and then train a classifier on the generated data. However, summaries produced by these approaches are either of low coherence or lack error-type coverage. To address these issues, we propose AMRFact, a novel framework that generates factually inconsistent summaries using Abstract Meaning Representation (AMR). Our approach parses factually correct summaries into AMR graphs and injects controlled factual inconsistencies to create negative examples, allowing for coherent factually inconsistent summaries to be generated with high error-type coverage. Additionally, we present a data selection module NegFilter based on natural language inference and BARTScore to ensure the quality of the generated negative samples. Experimental results demonstrate that our approach significantly outperforms previous systems on the AggreFact-SOTA dataset, showcasing its efficacy in assessing factuality in abstractive summarization.
</details>
<details>
<summary>摘要</summary>
保持事实一致性在各种自然语言处理任务中非常重要，特别是在抽象概念摘要中，因为保持信息完整性非常重要。先前基于前提推理的方法通常会生成不一致的摘要，然后对生成的数据进行训练。然而，这些方法生成的摘要通常是低凝结的或缺乏错误类型覆盖。为解决这些问题，我们提出了 AMRFact 框架，它使用抽象意义表示（AMR）来生成不一致的摘要。我们的方法将事实正确的摘要转换为 AMR 图并在其中注入控制的不一致性，以生成高错误类型覆盖的不一致摘要。此外，我们还提出了一个名为 NegFilter 的数据选择模块，它根据自然语言推理和 BARTScore 来确保生成的负样本的质量。实验结果表明，我们的方法与之前系统相比显著提高了 AggreFact-SOTA 数据集上的表现，这表明我们的方法在抽象概念摘要中评估事实性的有效性。
</details></li>
</ul>
<hr>
<h2 id="Leveraging-Code-to-Improve-In-context-Learning-for-Semantic-Parsing"><a href="#Leveraging-Code-to-Improve-In-context-Learning-for-Semantic-Parsing" class="headerlink" title="Leveraging Code to Improve In-context Learning for Semantic Parsing"></a>Leveraging Code to Improve In-context Learning for Semantic Parsing</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09519">http://arxiv.org/abs/2311.09519</a></li>
<li>repo_url: None</li>
<li>paper_authors: Ben Bogin, Shivanshu Gupta, Peter Clark, Ashish Sabharwal</li>
<li>for: 提高semantic parsing的效果，尤其是在受限的数据量下</li>
<li>methods: 使用通用编程语言如Python，并将提问添加结构化域描述</li>
<li>results: 在三个 популяр的数据集上显著提高了准确率（例如，从7.9%提升到66.5%），降低了需要大量示例的要求，并减少了语言的 популяр度对于预训练 corpora 的影响。<details>
<summary>Abstract</summary>
In-context learning (ICL) is an appealing approach for semantic parsing due to its few-shot nature and improved generalization. However, learning to parse to rare domain-specific languages (DSLs) from just a few demonstrations is challenging, limiting the performance of even the most capable LLMs. In this work, we improve the effectiveness of ICL for semantic parsing by (1) using general-purpose programming languages such as Python instead of DSLs, and (2) augmenting prompts with a structured domain description that includes, e.g., the available classes and functions. We show that both these changes significantly improve accuracy across three popular datasets. Combined, they lead to dramatic improvements (e.g. 7.9% to 66.5% on SMCalFlow compositional split), nearly closing the performance gap between easier i.i.d.\ and harder compositional splits when used with a strong model, and reducing the need for a large number of demonstrations. We find that the resemblance of the target parse language to general-purpose code is a more important factor than the language's popularity in pre-training corpora. Our findings provide an improved methodology for building semantic parsers in the modern context of ICL with LLMs.
</details>
<details>
<summary>摘要</summary>
启发式学习（ICL）是 semantic parsing 方法的一种吸引人的方式，因为它可以通过几次示例学习来达到更好的泛化性。然而，学习到特定领域语言（DSL）的语义分析仍然是挑战，尤其是使用只有几个示例的情况下。在这种情况下，我们改进了 ICLL 的效iveness，通过以下两点：1. 使用通用编程语言，如 Python，而不是特定领域语言。2. 在提示中添加结构化领域描述，包括可用的类和函数。我们发现，这两点都会显著提高准确性，并在三个流行的数据集上达到了显著提高（例如，从 7.9% 提高到 66.5% 在 SMCalFlow compositional split 上）。这些改进使得模型在更难的 compositional split 上表现更好，并减少了需要大量示例的需求。我们发现，目标语义分析语言与通用编程语言之间的相似性是更重要的因素，而不是语言的 популярность。我们的发现可以提供一种改进的方法来在现代 ICLL 中建立 semantic parser。
</details></li>
</ul>
<hr>
<h2 id="GEE-Grammar-Error-Explanation-with-Large-Language-Models"><a href="#GEE-Grammar-Error-Explanation-with-Large-Language-Models" class="headerlink" title="GEE! Grammar Error Explanation with Large Language Models"></a>GEE! Grammar Error Explanation with Large Language Models</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09517">http://arxiv.org/abs/2311.09517</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, Kevin Gimpel, Mohit Iyyer</li>
<li>for: 这个论文是为了解决语言学习者的 grammatical error correction 问题而写的。</li>
<li>methods: 这个论文使用的方法包括使用 GPT-4 生成一个一个 sentence explanation 的 pipeline，以及使用 fine-tuned 和提示的大型语言模型进行 structured atomic token edit extraction。</li>
<li>results: 人工评估表明，这个pipeline在德语和中文 grammar error correction 数据上的正确率分别为 93.9% 和 98.0%。<details>
<summary>Abstract</summary>
Grammatical error correction tools are effective at correcting grammatical errors in users' input sentences but do not provide users with \textit{natural language} explanations about their errors. Such explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). To address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. We analyze the capability of GPT-4 in grammar error explanation, and find that it only produces explanations for 60.2% of the errors using one-shot prompting. To improve upon this performance, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to generate explanations. We evaluate our pipeline on German and Chinese grammar error correction data sampled from language learners with a wide range of proficiency levels. Human evaluation reveals that our pipeline produces 93.9% and 98.0% correct explanations for German and Chinese data, respectively. To encourage further research in this area, we will open-source our data and code.
</details>
<details>
<summary>摘要</summary>
grammatical error correction tools can correct grammatical errors in users' input sentences, but they do not provide users with 自然语言 explanations about their errors. these explanations are essential for helping users learn the language by gaining a deeper understanding of its grammatical rules (DeKeyser, 2003; Ellis et al., 2006). to address this gap, we propose the task of grammar error explanation, where a system needs to provide one-sentence explanations for each grammatical error in a pair of erroneous and corrected sentences. we analyze the capability of GPT-4 in grammar error explanation, and find that it only produces explanations for 60.2% of the errors using one-shot prompting. to improve upon this performance, we develop a two-step pipeline that leverages fine-tuned and prompted large language models to perform structured atomic token edit extraction, followed by prompting GPT-4 to generate explanations. we evaluate our pipeline on German and Chinese grammar error correction data sampled from language learners with a wide range of proficiency levels. human evaluation reveals that our pipeline produces 93.9% and 98.0% correct explanations for German and Chinese data, respectively. to encourage further research in this area, we will open-source our data and code.
</details></li>
</ul>
<hr>
<h2 id="Sequencing-Matters-A-Generate-Retrieve-Generate-Model-for-Building-Conversational-Agents"><a href="#Sequencing-Matters-A-Generate-Retrieve-Generate-Model-for-Building-Conversational-Agents" class="headerlink" title="Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents"></a>Sequencing Matters: A Generate-Retrieve-Generate Model for Building Conversational Agents</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09513">http://arxiv.org/abs/2311.09513</a></li>
<li>repo_url: None</li>
<li>paper_authors: Quinn Patwardhan, Grace Hui Yang</li>
<li>for: This paper describes the Georgetown InfoSense group’s approach to solving the challenges of TREC iKAT 2023.</li>
<li>methods: The approach uses a Generate-Retrieve-Generate method, which is found to outperform Retrieve-Then-Generate approaches. The solution involves using Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again.</li>
<li>results: The submitted runs outperform the median runs by a significant margin, with superior performance in nDCG across various cut numbers and overall success rate. The official results of the TREC evaluation contradict the initial self-evaluation, but the findings suggest that the sequence of involving different components matters, with LLMs being essential before using search engines.<details>
<summary>Abstract</summary>
This paper contains what the Georgetown InfoSense group has done in regard to solving the challenges presented by TREC iKAT 2023. Our submitted runs outperform the median runs by a significant margin, exhibiting superior performance in nDCG across various cut numbers and in overall success rate. Our approach uses a Generate-Retrieve-Generate method, which we've found to greatly outpace Retrieve-Then-Generate approaches for the purposes of iKAT. Our solution involves the use of Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again. We leverage several purpose-built Language Models, including BERT, Chat-based, and text-to-transfer-based models, for text understanding, classification, generation, and summarization. The official results of the TREC evaluation contradict our initial self-evaluation, which may suggest that a decrease in the reliance on our retrieval and classification methods is better. Nonetheless, our findings suggest that the sequence of involving these different components matters, where we see an essentiality of using LLMs before using search engines.
</details>
<details>
<summary>摘要</summary>
Our solution employs Large Language Models (LLMs) for initial answers, answer grounding by BM25, passage quality filtering by logistic regression, and answer generation by LLMs again. We utilize several purpose-built Language Models, including BERT, Chat-based, and text-to-transfer-based models, for text understanding, classification, generation, and summarization.While the official results of the TREC evaluation differ from our initial self-evaluation, our findings suggest that the sequence of involving these different components is crucial. Specifically, we find that using LLMs before search engines is essential.
</details></li>
</ul>
<hr>
<h2 id="One-Size-Does-Not-Fit-All-Customizing-Open-Domain-Procedures"><a href="#One-Size-Does-Not-Fit-All-Customizing-Open-Domain-Procedures" class="headerlink" title="One Size Does Not Fit All: Customizing Open-Domain Procedures"></a>One Size Does Not Fit All: Customizing Open-Domain Procedures</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09510">http://arxiv.org/abs/2311.09510</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yash Kumar Lal, Li Zhang, Faeze Brahman, Bodhisattwa Prasad Majumder, Peter Clark, Niket Tandon</li>
<li>for: 这研究是关于如何使用自然语言处理机器人（LLM）来自动化开放领域过程定制。</li>
<li>methods: 研究使用了一个名为CustomPlans的探测数据集，该数据集包含多种用户定制需求，以测试LLM的定制能力。</li>
<li>results: 研究发现，在Sequential设置下使用LLM作为定制代理和执行代理时，可以很好地满足用户的定制需求，但是LLM并不充分考虑用户的定制需求，导致错误率为~51%。<details>
<summary>Abstract</summary>
How-to procedures, such as how to plant a garden, are ubiquitous. But one size does not fit all - humans often need to customize these procedural plans according to their specific needs, e.g., planting a garden without pesticides. While LLMs can fluently generate generic procedures, we present the first study on how well LLMs can customize open-domain procedures. We introduce CustomPlans, a probe dataset of customization hints that encodes diverse user needs for open-domain How-to procedures. Using LLMs as CustomizationAgent and ExecutionAgent in different settings, we establish their abilities to perform open-domain procedure customization. Human evaluation shows that using these agents in a Sequential setting is the best, but they are good enough only ~51% of the time. Error analysis shows that LLMs do not sufficiently address user customization needs in their generated procedures.
</details>
<details>
<summary>摘要</summary>
各种如何程序（如植 garden）是普遍存在的。但是一个size不适用于所有人——人们常需要根据自己的具体需求自定义这些程序，例如不使用杀虫剂植 garden。 LLMS可以轻松生成通用的程序，但我们的研究表明，LLMS可以如何自定义开放领域的程序。我们介绍了一个名为CustomPlans的探索数据集，该数据集包含多种用户需求的自定义提示。我们使用LLMS作为自定义代理和执行代理在不同的设置下，并证明了它们在开放领域程序自定义方面的能力。人工评估表明，使用这些代理在顺序设置下是最好的，但它们只能成功约51%的时间。错误分析表明，LLMS在生成的程序中不充分考虑用户自定义需求。
</details></li>
</ul>
<hr>
<h2 id="SQATIN-Supervised-Instruction-Tuning-Meets-Question-Answering-for-Improved-Dialogue-NLU"><a href="#SQATIN-Supervised-Instruction-Tuning-Meets-Question-Answering-for-Improved-Dialogue-NLU" class="headerlink" title="SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU"></a>SQATIN: Supervised Instruction Tuning Meets Question Answering for Improved Dialogue NLU</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09502">http://arxiv.org/abs/2311.09502</a></li>
<li>repo_url: None</li>
<li>paper_authors: Evgeniia Razumovskaia, Goran Glavaš, Anna Korhonen, Ivan Vulić</li>
<li>for: 本研究旨在提高对话自然语言理解（NLU）的性能，尤其是在 Labelled NLU 数据稀缺的情况下。</li>
<li>methods: 本研究提出了一种新的对话 NLU 框架，名为 SQATIN，它基于 instruction tuning 和问答模型来解决 Intent Detection 和 Value Extraction 任务。</li>
<li>results: 根据评估结果，SQATIN 在已有NLU benchmark上设置了新的状态对话NLU性能，大幅超越了现有的模型基于标准精度优化目标的表现，尤其是在跨领域传递中。<details>
<summary>Abstract</summary>
Task-oriented dialogue (ToD) systems help users execute well-defined tasks across a variety of domains (e.g., $\textit{flight booking}$ or $\textit{food ordering}$), with their Natural Language Understanding (NLU) components being dedicated to the analysis of user utterances, predicting users' intents ($\textit{Intent Detection}$, ID) and extracting values for informational slots ($\textit{Value Extraction}$, VE). In most domains, labelled NLU data is scarce, making sample-efficient learning -- enabled with effective transfer paradigms -- paramount. In this work, we introduce SQATIN, a new framework for dialog NLU based on (i) instruction tuning and (ii) question-answering-based formulation of ID and VE tasks. According to the evaluation on established NLU benchmarks, SQATIN sets the new state of the art in dialogue NLU, substantially surpassing the performance of current models based on standard fine-tuning objectives in both in-domain training and cross-domain transfer. SQATIN yields particularly large performance gains in cross-domain transfer, owing to the fact that our QA-based instruction tuning leverages similarities between natural language descriptions of classes (i.e., slots and intents) across domains.
</details>
<details>
<summary>摘要</summary>
任免对话（ToD）系统可以帮助用户完成具体的任务（如飞行订票或食物订单），其自然语言理解（NLU）组件专门用于分析用户言语，预测用户的意图（Intent Detection，ID）和提取信息槽的值（Value Extraction，VE）。在大多数领域中，标注的NLU数据 scarce，因此使得样本效率学习 -- 通过有效的传输方法 -- 是非常重要的。在这项工作中，我们介绍了SQATIN，一种新的对话NLU框架，基于（i）指令调整和（ii）问答题解法来实现ID和VE任务。根据评估已知NLU标准准的评估 benchmark，SQATIN将对话NLU的新状态划定，大幅超过了现有基于标准精度调整目标的当前模型在领域培训和跨领域传输中的性能。SQATIN在跨领域传输中的性能提升特别大，这是因为我们的QA-based instruction tuning利用了不同领域的自然语言描述中的类 similarities（即槽和意图）。
</details></li>
</ul>
<hr>
<h2 id="Personalized-Jargon-Identification-for-Enhanced-Interdisciplinary-Communication"><a href="#Personalized-Jargon-Identification-for-Enhanced-Interdisciplinary-Communication" class="headerlink" title="Personalized Jargon Identification for Enhanced Interdisciplinary Communication"></a>Personalized Jargon Identification for Enhanced Interdisciplinary Communication</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09481">http://arxiv.org/abs/2311.09481</a></li>
<li>repo_url: None</li>
<li>paper_authors: Yue Guo, Joseph Chee Chang, Maria Antoniak, Erin Bransom, Trevor Cohen, Lucy Lu Wang, Tal August</li>
<li>for: 本研究旨在提高科研人员对技术术语的认知和理解，以便在不同领域之间进行交互和合作。</li>
<li>methods: 本研究使用了一组超过10000个术语熟悉度标注数据，并分析了这些数据以识别具有不同熟悉度的术语。</li>
<li>results: 研究发现，科研人员对术语熟悉度和信息需求之间存在很大差异，即使在同一个子领域内。研究还找到了个人、子领域和领域知识等特征，以便预测个人对术语熟悉度的认知。<details>
<summary>Abstract</summary>
Scientific jargon can impede researchers when they read materials from other domains. Current methods of jargon identification mainly use corpus-level familiarity indicators (e.g., Simple Wikipedia represents plain language). However, researchers' familiarity of a term can vary greatly based on their own background. We collect a dataset of over 10K term familiarity annotations from 11 computer science researchers for terms drawn from 100 paper abstracts. Analysis of this data reveals that jargon familiarity and information needs vary widely across annotators, even within the same sub-domain (e.g., NLP). We investigate features representing individual, sub-domain, and domain knowledge to predict individual jargon familiarity. We compare supervised and prompt-based approaches, finding that prompt-based methods including personal publications yields the highest accuracy, though zero-shot prompting provides a strong baseline. This research offers insight into features and methods to integrate personal data into scientific jargon identification.
</details>
<details>
<summary>摘要</summary>
科学技术术语可能会阻碍研究人员在不同领域的文献中阅读。现有的词汇识别方法主要使用文库级 familiarness 指标（例如简单的wikipedia）。然而，研究人员对于一个词汇的熟悉程度可能会很大差异，基于他们的背景知识。我们收集了超过10,000个词汇熟悉标注from 11名计算机科学研究人员，来自100篇摘要中的词汇。我们对这些数据进行分析发现，词汇熟悉度和信息需求在审题人员中很大差异，甚至在同一个子领域（例如NLP）内。我们研究个人、子领域和领域知识的特征，以预测个人词汇熟悉度。我们比较了经过学习和提示方法，发现提示方法，包括个人出版物，可以达到最高的准确率，虽然零开始提示方法提供了强大的基准。这项研究对个人数据集成 scientific jargon 识别提供了新的想法和方法。
</details></li>
</ul>
<hr>
<h2 id="Show-Your-Work-with-Confidence-Confidence-Bands-for-Tuning-Curves"><a href="#Show-Your-Work-with-Confidence-Confidence-Bands-for-Tuning-Curves" class="headerlink" title="Show Your Work with Confidence: Confidence Bands for Tuning Curves"></a>Show Your Work with Confidence: Confidence Bands for Tuning Curves</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09480">http://arxiv.org/abs/2311.09480</a></li>
<li>repo_url: <a target="_blank" rel="noopener" href="https://github.com/nalourie/opda">https://github.com/nalourie/opda</a></li>
<li>paper_authors: Nicholas Lourie, Kyunghyun Cho, He He</li>
<li>for: 本文旨在提供一种用于比较自然语言处理方法的有效方法，以及一种用于确定这些方法之间的关系的有效方法。</li>
<li>methods: 本文使用了一种新的方法来建立有效的比较 curves，这种方法可以快速地确定不同方法之间的关系。</li>
<li>results: 本文的实验结果表明，新提出的方法可以准确地建立比较 curves，并且可以与现有的bootstrapconfidence bands进行比较。<details>
<summary>Abstract</summary>
The choice of hyperparameters greatly impacts performance in natural language processing. Often, it is hard to tell if a method is better than another or just better tuned. Tuning curves fix this ambiguity by accounting for tuning effort. Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. While several estimators exist for these curves, it is common to use point estimates, which we show fail silently and give contradictory results when given too little data.   Beyond point estimates, confidence bands are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.   Empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release a library implementing the method at https://github.com/nalourie/opda .
</details>
<details>
<summary>摘要</summary>
“选择超参数会对自然语言处理性能产生深远影响。然而，常常难以判断一方法是哪一方法更好，这是因为它们的优化努力不同。对于这问题，曲线数据可以提供解答。 Specifically, they plot validation performance as a function of the number of hyperparameter choices tried so far. 许多估计器存在这些曲线上，但是常用的是点估计，我们展示它们会在有限数据情况下失败并给出矛盾的结果。  beyond point estimates, confidence bands are necessary to rigorously establish the relationship between different approaches. We present the first method to construct valid confidence bands for tuning curves. The bands are exact, simultaneous, and distribution-free, thus they provide a robust basis for comparing methods.  empirical analysis shows that while bootstrap confidence bands, which serve as a baseline, fail to approximate their target confidence, ours achieve it exactly. We validate our design with ablations, analyze the effect of sample size, and provide guidance on comparing models with our method. To promote confident comparisons in future work, we release a library implementing the method at <https://github.com/nalourie/opda>.”Note: The translation is in Simplified Chinese, which is the standard form of Chinese used in mainland China and Singapore. If you prefer Traditional Chinese, please let me know and I can provide the translation in that form as well.
</details></li>
</ul>
<hr>
<h2 id="Clarify-When-Necessary-Resolving-Ambiguity-Through-Interaction-with-LMs"><a href="#Clarify-When-Necessary-Resolving-Ambiguity-Through-Interaction-with-LMs" class="headerlink" title="Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs"></a>Clarify When Necessary: Resolving Ambiguity Through Interaction with LMs</h2><ul>
<li>paper_url: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/2311.09469">http://arxiv.org/abs/2311.09469</a></li>
<li>repo_url: None</li>
<li>paper_authors: Michael J. Q. Zhang, Eunsol Choi</li>
<li>for: 这paper的目的是研究LMs中的ambiguity解决方法，以提高AI助手的性能。</li>
<li>methods: 该paper提出了一个任务agnostic的框架，通过向用户提问clarifying questions来解决ambiguity。这个框架包括三个子任务：确定需要clarification时，确定需要clarification的问题，以及基于新的信息回答正确。</li>
<li>results: 该paper的实验结果表明，intent-sim可以更好地确定需要clarification的时候，并且可以double randomly select的性能。此外，intent-sim在多种NLP任务和LMs中都表现了良好的稳定性。<details>
<summary>Abstract</summary>
Resolving ambiguities through interaction is a hallmark of natural language, and modeling this behavior is a core challenge in crafting AI assistants. In this work, we study such behavior in LMs by proposing a task-agnostic framework for resolving ambiguity by asking users clarifying questions. Our framework breaks down this objective into three subtasks: (1) determining when clarification is needed, (2) determining what clarifying question to ask, and (3) responding accurately with the new information gathered through clarification. We evaluate systems across three NLP applications: question answering, machine translation and natural language inference. For the first subtask, we present a novel uncertainty estimation approach, intent-sim, that determines the utility of querying for clarification by estimating the entropy over user intents. Our method consistently outperforms existing uncertainty estimation approaches at identifying predictions that will benefit from clarification. When only allowed to ask for clarification on 10% of examples, our system is able to double the performance gains over randomly selecting examples to clarify. Furthermore, we find that intent-sim is robust, demonstrating improvements across a wide range of NLP tasks and LMs. Together, our work lays foundation for studying clarifying interactions with LMs.
</details>
<details>
<summary>摘要</summary>
解决冲突通过互动是自然语言的特征，模拟这种行为是AI助手设计的核心挑战。在这项工作中，我们研究LM中的这种行为，通过提出任务无关的框架来解决冲突。我们将这个目标分解为三个互动任务：（1）确定是否需要准确化，（2）确定需要准确化的问题，以及（3）通过准确化获取新信息并准确回答。我们在三种NLP应用中评估系统：问答、机器翻译和自然语言推理。对于第一个任务，我们提出了一种新的uncertainty estimation方法，即意图sim，该方法根据用户意图的Entropy来判断是否需要准确化。我们的方法在identifying需要准确化的预测中表现出色，并且在只允许问 clarification 10%的示例中，我们的系统能够double Performance gain。此外，我们发现intent-sim是可靠的，在各种NLP任务和LM上都能够达到显著改进。总之，我们的工作为 изуча clarify interactions with LMs 提供了基础。
</details></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <div class="article-footer-content">
        
        <a data-url="https://nullscc.github.io/2023/11/16/cs.CL_2023_11_16/" data-id="clpztdnfb00fmes883tig16m3" class="article-share-link">Share</a>
        
      </div>
    </footer>
  </div>
  </div>
  </div>
  
</article>



  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/page/3/">&amp;laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/98/">98</a><a class="extend next" rel="next" href="/page/5/">Next &amp;raquo;</a>
    </nav>
  
</section>
      
      <aside id="sidebar">
  
    <div class="widget-wrap">
  <h3 class="widget-title">Calendar</h3>
  <div id="calendar"></div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/cs-AI/">cs.AI</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CL/">cs.CL</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-CV/">cs.CV</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-LG/">cs.LG</a><span class="category-list-count">142</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs-SD/">cs.SD</a><span class="category-list-count">129</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-AS/">eess.AS</a><span class="category-list-count">67</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-IV/">eess.IV</a><span class="category-list-count">127</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/eess-SP/">eess.SP</a><span class="category-list-count">82</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/11/">November 2023</a><span class="archive-list-count">147</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/10/">October 2023</a><span class="archive-list-count">231</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a><span class="archive-list-count">212</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">August 2023</a><span class="archive-list-count">175</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">July 2023</a><span class="archive-list-count">208</span></li></ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 nullscc<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/sun11/hexo-theme-paperbox" target="_blank">Paperbox</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>

  

<!-- totop start -->
<div id="totop">
	<a title="To Top"></a>
</div>
<!-- totop end -->

<!-- swiftype search start -->

<!-- swiftype search end -->



<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/lrsjng.jquery-qrcode/0.12.0/jquery.qrcode.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>



<!-- add calendar widget -->

  <script src="/js/calendar.js"></script>
  <script src="/js/languages.js"></script>
  <script type="text/javascript">
    $(function() {
    
      $('#calendar').aCalendar('en', $.extend('{"months":["January","February","March","April","May","June","July","August","September","October","November","December"],"dayOfWeekShort":["S","M","T","W","T","F","S"],"dayOfWeek":["Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"]}', {single:'true', root:'calendar/'}));
    
    });
  </script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
